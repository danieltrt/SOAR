file_path,api_count,code
tf_upgrade.py,85,"b'# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Upgrader for Python scripts from pre-1.0 TensorFlow to 1.0 TensorFlow.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nimport argparse\nimport ast\nimport collections\nimport os\nimport shutil\nimport sys\nimport tempfile\nimport traceback\n\n\nclass APIChangeSpec(object):\n  """"""List of maps that describe what changed in the API.""""""\n\n  def __init__(self):\n    # Maps from a function name to a dictionary that describes how to\n    # map from an old argument keyword to the new argument keyword.\n    self.function_keyword_renames = {\n        ""tf.count_nonzero"": {\n            ""reduction_indices"": ""axis""\n        },\n        ""tf.reduce_all"": {\n            ""reduction_indices"": ""axis""\n        },\n        ""tf.reduce_any"": {\n            ""reduction_indices"": ""axis""\n        },\n        ""tf.reduce_max"": {\n            ""reduction_indices"": ""axis""\n        },\n        ""tf.reduce_mean"": {\n            ""reduction_indices"": ""axis""\n        },\n        ""tf.reduce_min"": {\n            ""reduction_indices"": ""axis""\n        },\n        ""tf.reduce_prod"": {\n            ""reduction_indices"": ""axis""\n        },\n        ""tf.reduce_sum"": {\n            ""reduction_indices"": ""axis""\n        },\n        ""tf.reduce_logsumexp"": {\n            ""reduction_indices"": ""axis""\n        },\n        ""tf.expand_dims"": {\n            ""dim"": ""axis""\n        },\n        ""tf.argmax"": {\n            ""dimension"": ""axis""\n        },\n        ""tf.argmin"": {\n            ""dimension"": ""axis""\n        },\n        ""tf.reduce_join"": {\n            ""reduction_indices"": ""axis""\n        },\n        ""tf.sparse_concat"": {\n            ""concat_dim"": ""axis""\n        },\n        ""tf.sparse_split"": {\n            ""split_dim"": ""axis""\n        },\n        ""tf.sparse_reduce_sum"": {\n            ""reduction_axes"": ""axis""\n        },\n        ""tf.reverse_sequence"": {\n            ""seq_dim"": ""seq_axis"",\n            ""batch_dim"": ""batch_axis""\n        },\n        ""tf.sparse_reduce_sum_sparse"": {\n            ""reduction_axes"": ""axis""\n        },\n        ""tf.squeeze"": {\n            ""squeeze_dims"": ""axis""\n        },\n        ""tf.split"": {\n            ""split_dim"": ""axis"",\n            ""num_split"": ""num_or_size_splits""\n        },\n        ""tf.concat"": {\n            ""concat_dim"": ""axis""\n        },\n    }\n\n    # Mapping from function to the new name of the function\n    self.function_renames = {\n        ""tf.inv"": ""tf.reciprocal"",\n        ""tf.contrib.deprecated.scalar_summary"": ""tf.summary.scalar"",\n        ""tf.contrib.deprecated.histogram_summary"": ""tf.summary.histogram"",\n        ""tf.listdiff"": ""tf.setdiff1d"",\n        ""tf.list_diff"": ""tf.setdiff1d"",\n        ""tf.mul"": ""tf.multiply"",\n        ""tf.neg"": ""tf.negative"",\n        ""tf.sub"": ""tf.subtract"",\n        ""tf.train.SummaryWriter"": ""tf.summary.FileWriter"",\n        ""tf.scalar_summary"": ""tf.summary.scalar"",\n        ""tf.histogram_summary"": ""tf.summary.histogram"",\n        ""tf.audio_summary"": ""tf.summary.audio"",\n        ""tf.image_summary"": ""tf.summary.image"",\n        ""tf.merge_summary"": ""tf.summary.merge"",\n        ""tf.merge_all_summaries"": ""tf.summary.merge_all"",\n        ""tf.image.per_image_whitening"": ""tf.image.per_image_standardization"",\n        ""tf.all_variables"": ""tf.global_variables"",\n        ""tf.VARIABLES"": ""tf.GLOBAL_VARIABLES"",\n        ""tf.initialize_all_variables"": ""tf.global_variables_initializer"",\n        ""tf.initialize_variables"": ""tf.variables_initializer"",\n        ""tf.initialize_local_variables"": ""tf.local_variables_initializer"",\n        ""tf.batch_matrix_diag"": ""tf.matrix_diag"",\n        ""tf.batch_band_part"": ""tf.band_part"",\n        ""tf.batch_set_diag"": ""tf.set_diag"",\n        ""tf.batch_matrix_transpose"": ""tf.matrix_transpose"",\n        ""tf.batch_matrix_determinant"": ""tf.matrix_determinant"",\n        ""tf.batch_matrix_inverse"": ""tf.matrix_inverse"",\n        ""tf.batch_cholesky"": ""tf.cholesky"",\n        ""tf.batch_cholesky_solve"": ""tf.cholesky_solve"",\n        ""tf.batch_matrix_solve"": ""tf.matrix_solve"",\n        ""tf.batch_matrix_triangular_solve"": ""tf.matrix_triangular_solve"",\n        ""tf.batch_matrix_solve_ls"": ""tf.matrix_solve_ls"",\n        ""tf.batch_self_adjoint_eig"": ""tf.self_adjoint_eig"",\n        ""tf.batch_self_adjoint_eigvals"": ""tf.self_adjoint_eigvals"",\n        ""tf.batch_svd"": ""tf.svd"",\n        ""tf.batch_fft"": ""tf.fft"",\n        ""tf.batch_ifft"": ""tf.ifft"",\n        ""tf.batch_ifft2d"": ""tf.ifft2d"",\n        ""tf.batch_fft3d"": ""tf.fft3d"",\n        ""tf.batch_ifft3d"": ""tf.ifft3d"",\n        ""tf.select"": ""tf.where"",\n        ""tf.complex_abs"": ""tf.abs"",\n        ""tf.batch_matmul"": ""tf.matmul"",\n        ""tf.pack"": ""tf.stack"",\n        ""tf.unpack"": ""tf.unstack"",\n    }\n\n    self.change_to_function = {\n        ""tf.ones_initializer"",\n        ""tf.zeros_initializer"",\n    }\n\n    # Functions that were reordered should be changed to the new keyword args\n    # for safety, if positional arguments are used. If you have reversed the\n    # positional arguments yourself, this could do the wrong thing.\n    self.function_reorders = {\n        ""tf.split"": [""axis"", ""num_or_size_splits"", ""value"", ""name""],\n        ""tf.sparse_split"": [""axis"", ""num_or_size_splits"", ""value"", ""name""],\n        ""tf.concat"": [""concat_dim"", ""values"", ""name""],\n        ""tf.svd"": [""tensor"", ""compute_uv"", ""full_matrices"", ""name""],\n        ""tf.nn.softmax_cross_entropy_with_logits"": [\n            ""logits"", ""labels"", ""dim"", ""name""],\n        ""tf.nn.sparse_softmax_cross_entropy_with_logits"": [\n            ""logits"", ""labels"", ""name""],\n        ""tf.nn.sigmoid_cross_entropy_with_logits"": [\n            ""logits"", ""labels"", ""name""]\n    }\n\n    # Specially handled functions.\n    self.function_handle = {""tf.reverse"": self._reverse_handler}\n\n  @staticmethod\n  def _reverse_handler(file_edit_recorder, node):\n    # TODO(aselle): Could check for a literal list of bools and try to convert\n    # them to indices.\n    comment = (""ERROR: tf.reverse has had its argument semantics changed\\n""\n               ""significantly the converter cannot detect this reliably, so you""\n               ""need to inspect this usage manually.\\n"")\n    file_edit_recorder.add(comment,\n                           node.lineno,\n                           node.col_offset,\n                           ""tf.reverse"",\n                           ""tf.reverse"",\n                           error=""tf.reverse requires manual check."")\n\n\nclass FileEditTuple(collections.namedtuple(\n    ""FileEditTuple"", [""comment"", ""line"", ""start"", ""old"", ""new""])):\n  """"""Each edit that is recorded by a FileEditRecorder.\n  Fields:\n    comment: A description of the edit and why it was made.\n    line: The line number in the file where the edit occurs (1-indexed).\n    start: The line number in the file where the edit occurs (0-indexed).\n    old: text string to remove (this must match what was in file).\n    new: text string to add in place of `old`.\n  """"""\n\n  __slots__ = ()\n\n\nclass FileEditRecorder(object):\n  """"""Record changes that need to be done to the file.""""""\n\n  def __init__(self, filename):\n    # all edits are lists of chars\n    self._filename = filename\n\n    self._line_to_edit = collections.defaultdict(list)\n    self._errors = []\n\n  def process(self, text):\n    """"""Process a list of strings, each corresponding to the recorded changes.\n    Args:\n      text: A list of lines of text (assumed to contain newlines)\n    Returns:\n      A tuple of the modified text and a textual description of what is done.\n    Raises:\n      ValueError: if substitution source location does not have expected text.\n    """"""\n\n    change_report = """"\n\n    # Iterate of each line\n    for line, edits in self._line_to_edit.items():\n      offset = 0\n      # sort by column so that edits are processed in order in order to make\n      # indexing adjustments cumulative for changes that change the string\n      # length\n      edits.sort(key=lambda x: x.start)\n\n      # Extract each line to a list of characters, because mutable lists\n      # are editable, unlike immutable strings.\n      char_array = list(text[line - 1])\n\n      # Record a description of the change\n      change_report += ""%r Line %d\\n"" % (self._filename, line)\n      change_report += ""-"" * 80 + ""\\n\\n""\n      for e in edits:\n        change_report += ""%s\\n"" % e.comment\n      change_report += ""\\n    Old: %s"" % (text[line - 1])\n\n      # Make underscore buffers for underlining where in the line the edit was\n      change_list = ["" ""] * len(text[line - 1])\n      change_list_new = ["" ""] * len(text[line - 1])\n\n      # Iterate for each edit\n      for e in edits:\n        # Create effective start, end by accounting for change in length due\n        # to previous edits\n        start_eff = e.start + offset\n        end_eff = start_eff + len(e.old)\n\n        # Make sure the edit is changing what it should be changing\n        old_actual = """".join(char_array[start_eff:end_eff])\n        if old_actual != e.old:\n          raise ValueError(""Expected text %r but got %r"" %\n                           ("""".join(e.old), """".join(old_actual)))\n        # Make the edit\n        char_array[start_eff:end_eff] = list(e.new)\n\n        # Create the underline highlighting of the before and after\n        change_list[e.start:e.start + len(e.old)] = ""~"" * len(e.old)\n        change_list_new[start_eff:end_eff] = ""~"" * len(e.new)\n\n        # Keep track of how to generate effective ranges\n        offset += len(e.new) - len(e.old)\n\n      # Finish the report comment\n      change_report += ""         %s\\n"" % """".join(change_list)\n      text[line - 1] = """".join(char_array)\n      change_report += ""    New: %s"" % (text[line - 1])\n      change_report += ""         %s\\n\\n"" % """".join(change_list_new)\n    return """".join(text), change_report, self._errors\n\n  def add(self, comment, line, start, old, new, error=None):\n    """"""Add a new change that is needed.\n    Args:\n      comment: A description of what was changed\n      line: Line number (1 indexed)\n      start: Column offset (0 indexed)\n      old: old text\n      new: new text\n      error: this ""edit"" is something that cannot be fixed automatically\n    Returns:\n      None\n    """"""\n\n    self._line_to_edit[line].append(\n        FileEditTuple(comment, line, start, old, new))\n    if error:\n      self._errors.append(""%s:%d: %s"" % (self._filename, line, error))\n\n\nclass TensorFlowCallVisitor(ast.NodeVisitor):\n  """"""AST Visitor that finds TensorFlow Function calls.\n  Updates function calls from old API version to new API version.\n  """"""\n\n  def __init__(self, filename, lines):\n    self._filename = filename\n    self._file_edit = FileEditRecorder(filename)\n    self._lines = lines\n    self._api_change_spec = APIChangeSpec()\n\n  def process(self, lines):\n    return self._file_edit.process(lines)\n\n  def generic_visit(self, node):\n    ast.NodeVisitor.generic_visit(self, node)\n\n  def _rename_functions(self, node, full_name):\n    function_renames = self._api_change_spec.function_renames\n    try:\n      new_name = function_renames[full_name]\n      self._file_edit.add(""Renamed function %r to %r"" % (full_name,\n                                                         new_name),\n                          node.lineno, node.col_offset, full_name, new_name)\n    except KeyError:\n      pass\n\n  def _get_attribute_full_path(self, node):\n    """"""Traverse an attribute to generate a full name e.g. tf.foo.bar.\n    Args:\n      node: A Node of type Attribute.\n    Returns:\n      a \'.\'-delimited full-name or None if the tree was not a simple form.\n      i.e. `foo()+b).bar` returns None, while `a.b.c` would return ""a.b.c"".\n    """"""\n    curr = node\n    items = []\n    while not isinstance(curr, ast.Name):\n      if not isinstance(curr, ast.Attribute):\n        return None\n      items.append(curr.attr)\n      curr = curr.value\n    items.append(curr.id)\n    return ""."".join(reversed(items))\n\n  def _find_true_position(self, node):\n    """"""Return correct line number and column offset for a given node.\n    This is necessary mainly because ListComp\'s location reporting reports\n    the next token after the list comprehension list opening.\n    Args:\n      node: Node for which we wish to know the lineno and col_offset\n    """"""\n    import re\n    find_open = re.compile(""^\\s*(\\\\[).*$"")\n    find_string_chars = re.compile(""[\'\\""]"")\n\n    if isinstance(node, ast.ListComp):\n      # Strangely, ast.ListComp returns the col_offset of the first token\n      # after the \'[\' token which appears to be a bug. Workaround by\n      # explicitly finding the real start of the list comprehension.\n      line = node.lineno\n      col = node.col_offset\n      # loop over lines\n      while 1:\n        # Reverse the text to and regular expression search for whitespace\n        text = self._lines[line-1]\n        reversed_preceding_text = text[:col][::-1]\n        # First find if a [ can be found with only whitespace between it and\n        # col.\n        m = find_open.match(reversed_preceding_text)\n        if m:\n          new_col_offset = col - m.start(1) - 1\n          return line, new_col_offset\n        else:\n          if (reversed_preceding_text=="""" or\n             reversed_preceding_text.isspace()):\n            line = line - 1\n            prev_line = self._lines[line - 1]\n            # TODO(aselle):\n            # this is poor comment detection, but it is good enough for\n            # cases where the comment does not contain string literal starting/\n            # ending characters. If ast gave us start and end locations of the\n            # ast nodes rather than just start, we could use string literal\n            # node ranges to filter out spurious #\'s that appear in string\n            # literals.\n            comment_start = prev_line.find(""#"")\n            if comment_start ==  -1:\n              col = len(prev_line) -1\n            elif find_string_chars.search(prev_line[comment_start:]) is None:\n              col = comment_start\n            else:\n              return None, None\n          else:\n            return None, None\n    # Most other nodes return proper locations (with notably does not), but\n    # it is not possible to use that in an argument.\n    return node.lineno, node.col_offset\n\n\n  def visit_Call(self, node):  # pylint: disable=invalid-name\n    """"""Handle visiting a call node in the AST.\n    Args:\n      node: Current Node\n    """"""\n\n\n    # Find a simple attribute name path e.g. ""tf.foo.bar""\n    full_name = self._get_attribute_full_path(node.func)\n\n    # Make sure the func is marked as being part of a call\n    node.func.is_function_for_call = True\n\n    if full_name and full_name.startswith(""tf.""):\n      # Call special handlers\n      function_handles = self._api_change_spec.function_handle\n      if full_name in function_handles:\n        function_handles[full_name](self._file_edit, node)\n\n      # Examine any non-keyword argument and make it into a keyword argument\n      # if reordering required.\n      function_reorders = self._api_change_spec.function_reorders\n      function_keyword_renames = (\n          self._api_change_spec.function_keyword_renames)\n\n      if full_name in function_reorders:\n        reordered = function_reorders[full_name]\n        for idx, arg in enumerate(node.args):\n          lineno, col_offset = self._find_true_position(arg)\n          if lineno is None or col_offset is None:\n            self._file_edit.add(\n                ""Failed to add keyword %r to reordered function %r""\n                % (reordered[idx], full_name), arg.lineno, arg.col_offset,\n                """", """",\n                error=""A necessary keyword argument failed to be inserted."")\n          else:\n            keyword_arg = reordered[idx]\n            if (full_name in function_keyword_renames and\n                keyword_arg in function_keyword_renames[full_name]):\n              keyword_arg = function_keyword_renames[full_name][keyword_arg]\n            self._file_edit.add(""Added keyword %r to reordered function %r""\n                                % (reordered[idx], full_name), lineno,\n                                col_offset, """", keyword_arg + ""="")\n\n      # Examine each keyword argument and convert it to the final renamed form\n      renamed_keywords = ({} if full_name not in function_keyword_renames else\n                          function_keyword_renames[full_name])\n      for keyword in node.keywords:\n        argkey = keyword.arg\n        argval = keyword.value\n\n        if argkey in renamed_keywords:\n          argval_lineno, argval_col_offset = self._find_true_position(argval)\n          if (argval_lineno is not None and argval_col_offset is not None):\n            # TODO(aselle): We should scan backward to find the start of the\n            # keyword key. Unfortunately ast does not give you the location of\n            # keyword keys, so we are forced to infer it from the keyword arg\n            # value.\n            key_start = argval_col_offset - len(argkey) - 1\n            key_end = key_start + len(argkey) + 1\n            if self._lines[argval_lineno - 1][key_start:key_end] == argkey + ""="":\n              self._file_edit.add(""Renamed keyword argument from %r to %r"" %\n                              (argkey, renamed_keywords[argkey]),\n                              argval_lineno,\n                              argval_col_offset - len(argkey) - 1,\n                              argkey + ""="", renamed_keywords[argkey] + ""="")\n              continue\n          self._file_edit.add(\n              ""Failed to rename keyword argument from %r to %r"" %\n              (argkey, renamed_keywords[argkey]),\n              argval.lineno,\n              argval.col_offset - len(argkey) - 1,\n              """", """",\n              error=""Failed to find keyword lexographically. Fix manually."")\n\n    ast.NodeVisitor.generic_visit(self, node)\n\n  def visit_Attribute(self, node):  # pylint: disable=invalid-name\n    """"""Handle bare Attributes i.e. [tf.foo, tf.bar].\n    Args:\n      node: Node that is of type ast.Attribute\n    """"""\n    full_name = self._get_attribute_full_path(node)\n    if full_name and full_name.startswith(""tf.""):\n      self._rename_functions(node, full_name)\n    if full_name in self._api_change_spec.change_to_function:\n      if not hasattr(node, ""is_function_for_call""):\n        new_text = full_name + ""()""\n        self._file_edit.add(""Changed %r to %r""%(full_name, new_text),\n                            node.lineno, node.col_offset, full_name, new_text)\n\n    ast.NodeVisitor.generic_visit(self, node)\n\n\nclass TensorFlowCodeUpgrader(object):\n  """"""Class that handles upgrading a set of Python files to TensorFlow 1.0.""""""\n\n  def __init__(self):\n    pass\n\n  def process_file(self, in_filename, out_filename):\n    """"""Process the given python file for incompatible changes.\n    Args:\n      in_filename: filename to parse\n      out_filename: output file to write to\n    Returns:\n      A tuple representing number of files processed, log of actions, errors\n    """"""\n\n    # Write to a temporary file, just in case we are doing an implace modify.\n    with open(in_filename, ""r"") as in_file, \\\n        tempfile.NamedTemporaryFile(""w"", delete=False) as temp_file:\n      ret = self.process_opened_file(\n          in_filename, in_file, out_filename, temp_file)\n\n    shutil.move(temp_file.name, out_filename)\n    return ret\n\n  # Broad exceptions are required here because ast throws whatever it wants.\n  # pylint: disable=broad-except\n  def process_opened_file(self, in_filename, in_file, out_filename, out_file):\n    """"""Process the given python file for incompatible changes.\n    This function is split out to facilitate StringIO testing from\n    tf_upgrade_test.py.\n    Args:\n      in_filename: filename to parse\n      in_file: opened file (or StringIO)\n      out_filename: output file to write to\n      out_file: opened file (or StringIO)\n    Returns:\n      A tuple representing number of files processed, log of actions, errors\n    """"""\n    process_errors = []\n    text = ""-"" * 80 + ""\\n""\n    text += ""Processing file %r\\n outputting to %r\\n"" % (in_filename,\n                                                         out_filename)\n    text += ""-"" * 80 + ""\\n\\n""\n\n    parsed_ast = None\n    lines = in_file.readlines()\n    try:\n      parsed_ast = ast.parse("""".join(lines))\n    except Exception:\n      text += ""Failed to parse %r\\n\\n"" % in_filename\n      text += traceback.format_exc()\n    if parsed_ast:\n      visitor = TensorFlowCallVisitor(in_filename, lines)\n      visitor.visit(parsed_ast)\n      out_text, new_text, process_errors = visitor.process(lines)\n      text += new_text\n      if out_file:\n        out_file.write(out_text)\n    text += ""\\n""\n    return 1, text, process_errors\n  # pylint: enable=broad-except\n\n  def process_tree(self, root_directory, output_root_directory):\n    """"""Processes upgrades on an entire tree of python files in place.\n    Note that only Python files. If you have custom code in other languages,\n    you will need to manually upgrade those.\n    Args:\n      root_directory: Directory to walk and process.\n      output_root_directory: Directory to use as base\n    Returns:\n      A tuple of files processed, the report string ofr all files, and errors\n    """"""\n\n    # make sure output directory doesn\'t exist\n    if output_root_directory and os.path.exists(output_root_directory):\n      print(""Output directory %r must not already exist."" % (\n          output_root_directory))\n      sys.exit(1)\n\n    # make sure output directory does not overlap with root_directory\n    norm_root = os.path.split(os.path.normpath(root_directory))\n    norm_output = os.path.split(os.path.normpath(output_root_directory))\n    if norm_root == norm_output:\n      print(""Output directory %r same as input directory %r"" % (\n          root_directory, output_root_directory))\n      sys.exit(1)\n\n    # Collect list of files to process (we do this to correctly handle if the\n    # user puts the output directory in some sub directory of the input dir)\n    files_to_process = []\n    for dir_name, _, file_list in os.walk(root_directory):\n      py_files = [f for f in file_list if f.endswith("".py"")]\n      for filename in py_files:\n        fullpath = os.path.join(dir_name, filename)\n        fullpath_output = os.path.join(\n            output_root_directory, os.path.relpath(fullpath, root_directory))\n        files_to_process.append((fullpath, fullpath_output))\n\n    file_count = 0\n    tree_errors = []\n    report = """"\n    report += (""="" * 80) + ""\\n""\n    report += ""Input tree: %r\\n"" % root_directory\n    report += (""="" * 80) + ""\\n""\n\n    for input_path, output_path in files_to_process:\n      output_directory = os.path.dirname(output_path)\n      if not os.path.isdir(output_directory):\n        os.makedirs(output_directory)\n      file_count += 1\n      _, l_report, l_errors = self.process_file(input_path, output_path)\n      tree_errors += l_errors\n      report += l_report\n    return file_count, report, tree_errors\n\n\nif __name__ == ""__main__"":\n  parser = argparse.ArgumentParser(\n      formatter_class=argparse.RawDescriptionHelpFormatter,\n      description=""""""Convert a TensorFlow Python file to 1.0\nSimple usage:\n  tf_convert.py --infile foo.py --outfile bar.py\n  tf_convert.py --intree ~/code/old --outtree ~/code/new\n"""""")\n  parser.add_argument(\n      ""--infile"",\n      dest=""input_file"",\n      help=""If converting a single file, the name of the file ""\n      ""to convert"")\n  parser.add_argument(\n      ""--outfile"",\n      dest=""output_file"",\n      help=""If converting a single file, the output filename."")\n  parser.add_argument(\n      ""--intree"",\n      dest=""input_tree"",\n      help=""If converting a whole tree of files, the directory ""\n      ""to read from (relative or absolute)."")\n  parser.add_argument(\n      ""--outtree"",\n      dest=""output_tree"",\n      help=""If converting a whole tree of files, the output ""\n      ""directory (relative or absolute)."")\n  parser.add_argument(\n      ""--reportfile"",\n      dest=""report_filename"",\n      help=(""The name of the file where the report log is ""\n            ""stored.""\n            ""(default: %(default)s)""),\n      default=""report.txt"")\n  args = parser.parse_args()\n\n  upgrade = TensorFlowCodeUpgrader()\n  report_text = None\n  report_filename = args.report_filename\n  files_processed = 0\n  if args.input_file:\n    files_processed, report_text, errors = upgrade.process_file(\n        args.input_file, args.output_file)\n    files_processed = 1\n  elif args.input_tree:\n    files_processed, report_text, errors = upgrade.process_tree(\n        args.input_tree, args.output_tree)\n  else:\n    parser.print_help()\n  if report_text:\n    open(report_filename, ""w"").write(report_text)\n    print(""TensorFlow 1.0 Upgrade Script"")\n    print(""-----------------------------"")\n    print(""Converted %d files\\n"" % files_processed)\n    print(""Detected %d errors that require attention"" % len(errors))\n    print(""-"" * 80)\n    print(""\\n"".join(errors))\n    print(""\\nMake sure to read the detailed log %r\\n"" % report_filename)'"
algorithm/__init__.py,0,b''
algorithm/input_data.py,0,"b'""""""Functions for downloading and reading MNIST data.""""""\nfrom __future__ import print_function\nimport gzip\nimport os\nimport urllib\nimport numpy\nSOURCE_URL = \'http://yann.lecun.com/exdb/mnist/\'\ndef maybe_download(filename, work_directory):\n  """"""Download the data from Yann\'s website, unless it\'s already here.""""""\n  if not os.path.exists(work_directory):\n    os.mkdir(work_directory)\n  filepath = os.path.join(work_directory, filename)\n  if not os.path.exists(filepath):\n    filepath, _ = urllib.urlretrieve(SOURCE_URL + filename, filepath)\n    statinfo = os.stat(filepath)\n    print(\'Succesfully downloaded\', filename, statinfo.st_size, \'bytes.\')\n  return filepath\ndef _read32(bytestream):\n  dt = numpy.dtype(numpy.uint32).newbyteorder(\'>\')\n  return numpy.frombuffer(bytestream.read(4), dtype=dt)\ndef extract_images(filename):\n  """"""Extract the images into a 4D uint8 numpy array [index, y, x, depth].""""""\n  print(\'Extracting\', filename)\n  with gzip.open(filename) as bytestream:\n    magic = _read32(bytestream)\n    if magic != 2051:\n      raise ValueError(\n          \'Invalid magic number %d in MNIST image file: %s\' %\n          (magic, filename))\n    num_images = _read32(bytestream)\n    rows = _read32(bytestream)\n    cols = _read32(bytestream)\n    buf = bytestream.read(rows * cols * num_images)\n    data = numpy.frombuffer(buf, dtype=numpy.uint8)\n    data = data.reshape(num_images, rows, cols, 1)\n    return data\ndef dense_to_one_hot(labels_dense, num_classes=10):\n  """"""Convert class labels from scalars to one-hot vectors.""""""\n  num_labels = labels_dense.shape[0]\n  index_offset = numpy.arange(num_labels) * num_classes\n  labels_one_hot = numpy.zeros((num_labels, num_classes))\n  labels_one_hot.flat[index_offset + labels_dense.ravel()] = 1\n  return labels_one_hot\ndef extract_labels(filename, one_hot=False):\n  """"""Extract the labels into a 1D uint8 numpy array [index].""""""\n  print(\'Extracting\', filename)\n  with gzip.open(filename) as bytestream:\n    magic = _read32(bytestream)\n    if magic != 2049:\n      raise ValueError(\n          \'Invalid magic number %d in MNIST label file: %s\' %\n          (magic, filename))\n    num_items = _read32(bytestream)\n    buf = bytestream.read(num_items)\n    labels = numpy.frombuffer(buf, dtype=numpy.uint8)\n    if one_hot:\n      return dense_to_one_hot(labels)\n    return labels\nclass DataSet(object):\n  def __init__(self, images, labels, fake_data=False):\n    if fake_data:\n      self._num_examples = 10000\n    else:\n      assert images.shape[0] == labels.shape[0], (\n          ""images.shape: %s labels.shape: %s"" % (images.shape,\n                                                 labels.shape))\n      self._num_examples = images.shape[0]\n      # Convert shape from [num examples, rows, columns, depth]\n      # to [num examples, rows*columns] (assuming depth == 1)\n      assert images.shape[3] == 1\n      images = images.reshape(images.shape[0],\n                              images.shape[1] * images.shape[2])\n      # Convert from [0, 255] -> [0.0, 1.0].\n      images = images.astype(numpy.float32)\n      images = numpy.multiply(images, 1.0 / 255.0)\n    self._images = images\n    self._labels = labels\n    self._epochs_completed = 0\n    self._index_in_epoch = 0\n  @property\n  def images(self):\n    return self._images\n  @property\n  def labels(self):\n    return self._labels\n  @property\n  def num_examples(self):\n    return self._num_examples\n  @property\n  def epochs_completed(self):\n    return self._epochs_completed\n  def next_batch(self, batch_size, fake_data=False):\n    """"""Return the next `batch_size` examples from this data set.""""""\n    if fake_data:\n      fake_image = [1.0 for _ in xrange(784)]\n      fake_label = 0\n      return [fake_image for _ in xrange(batch_size)], [\n          fake_label for _ in xrange(batch_size)]\n    start = self._index_in_epoch\n    self._index_in_epoch += batch_size\n    if self._index_in_epoch > self._num_examples:\n      # Finished epoch\n      self._epochs_completed += 1\n      # Shuffle the data\n      perm = numpy.arange(self._num_examples)\n      numpy.random.shuffle(perm)\n      self._images = self._images[perm]\n      self._labels = self._labels[perm]\n      # Start next epoch\n      start = 0\n      self._index_in_epoch = batch_size\n      assert batch_size <= self._num_examples\n    end = self._index_in_epoch\n    return self._images[start:end], self._labels[start:end]\ndef read_data_sets(train_dir, fake_data=False, one_hot=False):\n  class DataSets(object):\n    pass\n  data_sets = DataSets()\n  if fake_data:\n    data_sets.train = DataSet([], [], fake_data=True)\n    data_sets.validation = DataSet([], [], fake_data=True)\n    data_sets.test = DataSet([], [], fake_data=True)\n    return data_sets\n  TRAIN_IMAGES = \'train-images-idx3-ubyte.gz\'\n  TRAIN_LABELS = \'train-labels-idx1-ubyte.gz\'\n  TEST_IMAGES = \'t10k-images-idx3-ubyte.gz\'\n  TEST_LABELS = \'t10k-labels-idx1-ubyte.gz\'\n  VALIDATION_SIZE = 5000\n  local_file = maybe_download(TRAIN_IMAGES, train_dir)\n  train_images = extract_images(local_file)\n  local_file = maybe_download(TRAIN_LABELS, train_dir)\n  train_labels = extract_labels(local_file, one_hot=one_hot)\n  local_file = maybe_download(TEST_IMAGES, train_dir)\n  test_images = extract_images(local_file)\n  local_file = maybe_download(TEST_LABELS, train_dir)\n  test_labels = extract_labels(local_file, one_hot=one_hot)\n  validation_images = train_images[:VALIDATION_SIZE]\n  validation_labels = train_labels[:VALIDATION_SIZE]\n  train_images = train_images[VALIDATION_SIZE:]\n  train_labels = train_labels[VALIDATION_SIZE:]\n  data_sets.train = DataSet(train_images, train_labels)\n  data_sets.validation = DataSet(validation_images, validation_labels)\n  data_sets.test = DataSet(test_images, test_labels)\n  return data_sets'"
algorithm/train_mnist_multi_perceptron.py,20,"b'from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport argparse\nimport sys\nfrom tensorflow.contrib.learn.python.learn.datasets.mnist import read_data_sets\nimport tensorflow as tf\nimport time\n\nFLAGS = tf.app.flags.FLAGS\n\ntf.flags.DEFINE_string(""data_dir"", ""./mnist"",\n                       ""mnist data_dir"")\n\n\ndef main(_):\n  mnist = read_data_sets(FLAGS.data_dir, one_hot=True)\n  x = tf.placeholder(tf.float32, [None, 784])\n  W1 = tf.Variable(tf.random_normal([784, 256]))\n  b1 = tf.Variable(tf.random_normal([256]))\n  W2 = tf.Variable(tf.random_normal([256, 256]))\n  b2 = tf.Variable(tf.random_normal([256]))\n  W3 = tf.Variable(tf.random_normal([256,10]))\n  b3 = tf.Variable(tf.random_normal([10]))\n\n  lay1 = tf.nn.relu(tf.add(tf.matmul(x, W1),b1))\n  lay2 = tf.nn.relu(tf.add(tf.matmul(lay1, W2), b2))\n  y = tf.add(tf.matmul(lay2, W3),b3)\n\n  y_ = tf.placeholder(tf.float32, [None, 10])\n  cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=y, labels=y_))\n  train_step = tf.train.GradientDescentOptimizer(0.0095).minimize(cross_entropy)\n\n  sess = tf.InteractiveSession()\n  tf.global_variables_initializer().run()\n  for index in range(100000): \n    # print(\'process the {}th batch\'.format(index))\n    start_train = time.time()\n    batch_xs, batch_ys = mnist.train.next_batch(100)\n    sess.run(train_step, feed_dict={x: batch_xs, y_: batch_ys})\n    # print(\'the {0} batch takes time: {1}\'.format(index, time.time()-start_train))\n\n  correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(y_, 1))\n  accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n  print(sess.run(accuracy, feed_dict={x: mnist.test.images,\n                                      y_: mnist.test.labels}))\n\nif __name__ == \'__main__\':\n  tf.app.run()'"
algorithm/train_mnist_single_perceptron.py,17,"b'from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport argparse\nimport sys\nfrom tensorflow.contrib.learn.python.learn.datasets.mnist import read_data_sets\nimport tensorflow as tf\nimport time\n\nFLAGS = tf.app.flags.FLAGS\n\ntf.flags.DEFINE_string(""data_dir"", ""./mnist"",\n                       ""mnist data_dir"")\n\n\ndef main(_):\n  mnist = read_data_sets(FLAGS.data_dir, one_hot=True)\n  x = tf.placeholder(tf.float32, [None, 784])\n  W1 = tf.Variable(tf.random_normal([784, 256]))\n  b1 = tf.Variable(tf.random_normal([256]))\n  W2 = tf.Variable(tf.random_normal([256, 10]))\n  b2 = tf.Variable(tf.random_normal([10]))\n  lay1 = tf.nn.relu(tf.matmul(x, W1) + b1)\n  y = tf.add(tf.matmul(lay1, W2),b2)\n\n  y_ = tf.placeholder(tf.float32, [None, 10])\n  cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=y, labels=y_))\n  train_step = tf.train.GradientDescentOptimizer(0.5).minimize(cross_entropy)\n\n  sess = tf.InteractiveSession()\n  tf.global_variables_initializer().run()\n  for index in range(10000): \n    # print(\'process the {}th batch\'.format(index))\n    start_train = time.time()\n    batch_xs, batch_ys = mnist.train.next_batch(100)\n    sess.run(train_step, feed_dict={x: batch_xs, y_: batch_ys})\n    # print(\'the {0} batch takes time: {1}\'.format(index, time.time()-start_train))\n\n  correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(y_, 1))\n  accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n  print(sess.run(accuracy, feed_dict={x: mnist.test.images,\n                                      y_: mnist.test.labels}))\n\nif __name__ == \'__main__\':\n  tf.app.run()'"
algorithm/train_mnist_softmax.py,14,"b'from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport argparse\nimport sys\nfrom tensorflow.contrib.learn.python.learn.datasets.mnist import read_data_sets\nimport tensorflow as tf\nimport time\n\nFLAGS = tf.app.flags.FLAGS\n\ntf.flags.DEFINE_string(""data_dir"", ""./mnist"",\n                       ""mnist data_dir"")\n\n\ndef main(_):\n  mnist = read_data_sets(FLAGS.data_dir, one_hot=True)\n  x = tf.placeholder(tf.float32, [None, 784])\n  W = tf.Variable(tf.random_normal([784, 10]))\n  b = tf.Variable(tf.random_normal([10]))\n  y = tf.matmul(x, W) + b\n\n  y_ = tf.placeholder(tf.float32, [None, 10])\n  cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=y, labels=y_))\n  train_step = tf.train.GradientDescentOptimizer(0.2).minimize(cross_entropy)\n\n  sess = tf.InteractiveSession()\n  tf.global_variables_initializer().run()\n  for index in range(10000): \n    print(\'process the {}th batch\'.format(index))\n    start_train = time.time()\n    batch_xs, batch_ys = mnist.train.next_batch(100)\n    sess.run(train_step, feed_dict={x: batch_xs, y_: batch_ys})\n    print(\'the {0} batch takes time: {1}\'.format(index, time.time()-start_train))\n\n  correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(y_, 1))\n  accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n  print(sess.run(accuracy, feed_dict={x: mnist.test.images,\n                                      y_: mnist.test.labels}))\n\nif __name__ == \'__main__\':\n  tf.app.run()'"
covert_to_tfrecord/covert_datasets_tfrecord.py,15,"b'#-*-coding:utf-8-*-\n\n""""""\nThis script is used to convert the images dataset of folder to tfrecord.\nThe image data set is expected to reside in JPEG files located in the following directory structure.\n\n    data_dir/label_0/image0.jpg\n    data_dir/label_1/image1.jpg\n    ...\nAnd this script will converts the traning and eval data into a sharded data set consisting of TFRecord files\n\n    train_dir/train-00000-of-01024\n    train_dir/train-00001-of-01024\n    ...\nand\n    val_dir/validation-00000-of-00128\n    val_dir/validation-00001-0f-00128\n    ...\n""""""\nimport tensorflow as tf\nimport os\nimport random\nimport math\nimport sys\n\n_NUM_SHARDS = 2\n_RANDOM_SEED = 0\n_NUM_VALIDATION = 1000\nLABELS_FILENAME = \'labels.txt\'\n\n\nclass ImageReader(object):\n  """"""Helper class that provides TensorFlow image coding utilities.""""""\n\n  def __init__(self):\n    # Initializes function that decodes RGB JPEG data.\n    self._decode_jpeg_data = tf.placeholder(dtype=tf.string)\n    self._decode_jpeg = tf.image.decode_jpeg(self._decode_jpeg_data, channels=3)\n\n  def read_image_dims(self, sess, image_data):\n    image = self.decode_jpeg(sess, image_data)\n    return image.shape[0], image.shape[1]\n\n  def decode_jpeg(self, sess, image_data):\n    image = sess.run(self._decode_jpeg,\n                     feed_dict={self._decode_jpeg_data: image_data})\n    assert len(image.shape) == 3\n    assert image.shape[2] == 3\n    return image\n\n\n\ndef int64_feature(values):\n  """"""Returns a TF-Feature of int64s.\n\n  Args:\n    values: A scalar or list of values.\n\n  Returns:\n    a TF-Feature.\n  """"""\n  if not isinstance(values, (tuple, list)):\n    values = [values]\n  return tf.train.Feature(int64_list=tf.train.Int64List(value=values))\n\n\ndef bytes_feature(values):\n  """"""Returns a TF-Feature of bytes.\n\n  Args:\n    values: A string.\n\n  Returns:\n    a TF-Feature.\n  """"""\n  return tf.train.Feature(bytes_list=tf.train.BytesList(value=[values]))\n\ndef _get_dataset_filename(dataset_dir, split_name, shard_id):\n    output_filename = ""102flowers_%s_%05d-of-%05d.tfrecord""%(\n        split_name, shard_id, _NUM_SHARDS\n    )\n    return os.path.join(dataset_dir, output_filename)\n\ndef _dataset_exists(dataset_dir):\n    for split_name in [\'train\', \'validation\']:\n        for shard_id in range(_NUM_SHARDS):\n            output_filename = _get_dataset_filename(\n                dataset_dir, split_name, shard_id\n            )\n            if not tf.gfile.Exists(output_filename):\n                return False\n    return True\n\ndef _get_filenames_and_classes(dataset_dir, dataset_name):\n    sex_detection_root = os.path.join(dataset_dir, dataset_name)\n    direcotries = []\n    class_names = []\n    for filename in os.listdir(sex_detection_root):\n        path = os.path.join(sex_detection_root, filename)\n        if os.path.isdir(path):\n            direcotries.append(path)\n            class_names.append(filename)\n    \n    image_filenames = []\n    for dir in direcotries:\n        for filename in os.listdir(dir):\n            path = os.path.join(dir, filename)\n            image_filenames.append(path)\n    return image_filenames, sorted(class_names)\n\ndef image_to_tfexample(image_data, image_format, height, width, class_id):\n  return tf.train.Example(features=tf.train.Features(feature={\n      \'image/encoded\': bytes_feature(image_data),\n      \'image/format\': bytes_feature(image_format),\n      \'image/class/label\': int64_feature(class_id),\n      \'image/height\': int64_feature(height),\n      \'image/width\': int64_feature(width),\n  }))\n\n# def _clean_up_temporary_files(dataset_dir, dataset_name):\n#   """"""Removes temporary files used to create the dataset.\n\n#   Args:\n#     dataset_dir: The directory where the temporary files are stored.\n#   """"""\n#   filename = _DATA_URL.split(\'/\')[-1]\n#   filepath = os.path.join(dataset_dir, filename)\n#   tf.gfile.Remove(filepath)\n\n#   tmp_dir = os.path.join(dataset_dir, dataset_name)\n#   tf.gfile.DeleteRecursively(tmp_dir)\n\ndef write_label_file(labels_to_class_names, dataset_dir,\n                     filename=LABELS_FILENAME):\n  """"""Writes a file with the list of class names.\n\n  Args:\n    labels_to_class_names: A map of (integer) labels to class names.\n    dataset_dir: The directory in which the labels file should be written.\n    filename: The filename where the class names are written.\n  """"""\n  labels_filename = os.path.join(dataset_dir, filename)\n  with tf.gfile.Open(labels_filename, \'w\') as f:\n    for label in labels_to_class_names:\n      class_name = labels_to_class_names[label]\n      f.write(\'%d:%s\\n\' % (label, class_name))\n\ndef _convert_dataset(split_name, filenames, class_names_to_ids, dataset_dir):\n    assert split_name in [\'train\', \'validation\']\n    num_per_shard = int(math.ceil(len(filenames)/float(_NUM_SHARDS)))\n    with tf.Graph().as_default():\n        image_reader = ImageReader()\n        with tf.Session(\'\') as sess:\n            for shard_id in range(_NUM_SHARDS):\n                output_filename = _get_dataset_filename(\n                    dataset_dir, split_name, shard_id\n                )\n                with tf.python_io.TFRecordWriter(output_filename) as tfrecord_writer:\n                    start_ndx = shard_id * num_per_shard\n                    end_ndx = min((shard_id+1)*num_per_shard, len(filenames))\n                    for i in range(start_ndx, end_ndx):\n                        # print(filenames[i+1])\n                        sys.stdout.write(\'\\r>> Converting image %d/%d shard %d\'%(\n                            i+1, len(filenames), shard_id\n                        ))\n                        sys.stdout.flush()\n                        image_data = tf.gfile.FastGFile(filenames[i], \'r\').read()\n                        height, width = image_reader.read_image_dims(sess, image_data)\n                        class_name = os.path.basename(os.path.dirname(filenames[i]))\n                        class_id = class_names_to_ids[class_name]\n\n                        example = image_to_tfexample(image_data, \'jpg\', height, width, class_id)\n                        tfrecord_writer.write(example.SerializeToString())\n    sys.stdout.write(\'\\n\')\n    sys.stdout.flush()\n\ndef run(dataset_dir, dataset_name):\n    """"""\n    Args: dataset_dir : the dataset dir where the dataset is stored.\n    """"""\n    if not tf.gfile.Exists(dataset_dir):\n        tf.gfile.MakeDirs(dataset_dir)\n    \n    if _dataset_exists(dataset_dir):\n        print(""Dataset files already exist. Existing without re-creating them."")\n    \n    images_filenames, class_names = _get_filenames_and_classes(dataset_dir,dataset_name)\n    class_names_to_ids = dict(zip(class_names, range(len(class_names))))\n\n    random.seed(_RANDOM_SEED)\n    random.shuffle(images_filenames)\n    training_filenames = images_filenames[_NUM_VALIDATION:]\n    validation_filenames = images_filenames[:_NUM_VALIDATION]\n    _convert_dataset(\'train\', training_filenames, class_names_to_ids, dataset_dir)\n    _convert_dataset(\'validation\', validation_filenames, class_names_to_ids, dataset_dir)\n\n    labels_to_class_names = dict(zip(range(len(class_names)), class_names))\n    write_label_file(labels_to_class_names, dataset_dir)\n\n    # _clean_up_temporary_files(dataset_dir,dataset_name)\n    print(\'\\n Finised convering the sex detection dataset!\')\n\n\n\n\n\n\n'"
covert_to_tfrecord/covert_sex_to_tfrecord.py,4,"b'from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\nimport covert_datasets_tfrecord\n\nFLAGS = tf.app.flags.FLAGS\n\ntf.app.flags.DEFINE_string(\n    \'dataset_name\',\n    None,\n    \'The name of the dataset to convert, one of ""cifar10"", ""flowers"", ""mnist"".\')\n\ntf.app.flags.DEFINE_string(\n    \'dataset_dir\',\n    None,\n    \'The directory where the output TFRecords and temporary files are saved.\')\n\n\ndef main(_):\n    if not FLAGS.dataset_name:\n        raise ValueError(\'You must supply the dataset name with --dataset_name\')\n    if not FLAGS.dataset_dir:\n        raise ValueError(\'You must supply the dataset name with --dataset_dir\')\n    \n    covert_datasets_tfrecord.run(FLAGS.dataset_dir, FLAGS.dataset_name)\n\n\nif __name__ == \'__main__\':\n    tf.app.run()'"
covert_to_tfrecord/covert_somedata_to_tfrecord.py,4,"b'from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\nimport covert_datasets_tfrecord\n\nFLAGS = tf.app.flags.FLAGS\n\ntf.app.flags.DEFINE_string(\n    \'dataset_name\',\n    None,\n    \'The name of the dataset to convert, one of ""cifar10"", ""flowers"", ""mnist"".\')\n\ntf.app.flags.DEFINE_string(\n    \'dataset_dir\',\n    None,\n    \'The directory where the output TFRecords and temporary files are saved.\')\n\n\ndef main(_):\n    if not FLAGS.dataset_name:\n        raise ValueError(\'You must supply the dataset name with --dataset_name\')\n    if not FLAGS.dataset_dir:\n        raise ValueError(\'You must supply the dataset name with --dataset_dir\')\n    \n    covert_datasets_tfrecord.run(FLAGS.dataset_dir, FLAGS.dataset_name)\n\n\nif __name__ == \'__main__\':\n    tf.app.run()'"
deepcolor/download_images.py,0,"b'import urllib2\nimport urllib\nimport json\nimport numpy as np\nimport cv2\nimport untangle\n\nmaxsize = 512\n\n# tags = [""asu_tora"",""puuakachan"",""mankun"",""hammer_%28sunset_beach%29"",""""]\n\n# for tag in tags:\n\ncount = 0\n\nfor i in xrange(299,10000):\n    header = {\'Referer\':\'http://safebooru.org/index.php?page=post&s=list\',\'User-Agent\' : \'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_12_3) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/56.0.2924.87 Safari/537.36\'}\n    url = ""http://safebooru.org/index.php?page=dapi&s=post&q=index&tags=1girl%20solo&pid=""+str(i+5000)\n    request = urllib2.Request(url, headers=header)\n    stringreturn = urllib2.urlopen(request).read()\n    print stringreturn\n    xmlreturn = untangle.parse(stringreturn)\n    for post in xmlreturn.posts.post:\n        imgurl = ""http:"" + post[""sample_url""]\n        print imgurl\n        if (""png"" in imgurl) or (""jpg"" in imgurl):\n\n            resp = urllib.urlopen(imgurl)\n            image = np.asarray(bytearray(resp.read()), dtype=""uint8"")\n            image = cv2.imdecode(image, cv2.IMREAD_COLOR)\n            height, width = image.shape[:2]\n            if height > width:\n                scalefactor = (maxsize*1.0) / width\n                res = cv2.resize(image,(int(width * scalefactor), int(height*scalefactor)), interpolation = cv2.INTER_CUBIC)\n                cropped = res[0:maxsize,0:maxsize]\n            if width > height:\n                scalefactor = (maxsize*1.0) / height\n                res = cv2.resize(image,(int(width * scalefactor), int(height*scalefactor)), interpolation = cv2.INTER_CUBIC)\n                center_x = int(round(width*scalefactor*0.5))\n                print center_x\n                cropped = res[0:maxsize,center_x - maxsize/2:center_x + maxsize/2]\n\n            # img_edge = cv2.adaptiveThreshold(cropped, 255,\n            #                                  cv2.ADAPTIVE_THRESH_MEAN_C,\n            #                                  cv2.THRESH_BINARY,\n            #                                  blockSize=9,\n            #                                  C=2)\n\n            count += 1\n            cv2.imwrite(""imgs-valid/""+str(count)+"".jpg"",cropped)\n            # cv2.imwrite(""imgs/""+str(post[""id""])+""-edge.jpg"",img_edge)\n'"
deepcolor/download_images_multithread.py,0,"b'import os\nimport Queue\nfrom threading import Thread\nfrom time import time\nfrom itertools import chain\nimport urllib2\nimport untangle\nimport numpy as np\nimport cv2\n\ndef download_imgs(url):\n    # count = 0\n    maxsize = 512\n    file_name = url.split(\'=\')[-1]\n    header = {\'Referer\':\'http://safebooru.org/index.php?page=post&s=list\',\'User-Agent\' : \'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_12_3) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/56.0.2924.87 Safari/537.36\'}\n    request = urllib2.Request(url, headers=header)\n    stringreturn = urllib2.urlopen(request).read()\n    xmlreturn = untangle.parse(stringreturn)\n    count = 0\n    print xmlreturn.posts[0][\'sample_url\']\n    try:\n        for post in xmlreturn.posts.post:\n            try:\n                imgurl = ""http:"" + post[""sample_url""]\n                print imgurl\n                if (""png"" in imgurl) or (""jpg"" in imgurl):\n                    resp = urllib2.urlopen(imgurl)\n                    image = np.asarray(bytearray(resp.read()), dtype=""uint8"")\n                    image = cv2.imdecode(image, cv2.IMREAD_COLOR)\n                    height, width = image.shape[:2]\n                    if height > width:\n                        scalefactor = (maxsize*1.0) / width\n                        res = cv2.resize(image,(int(width * scalefactor), int(height*scalefactor)), interpolation = cv2.INTER_CUBIC)\n                        cropped = res[0:maxsize,0:maxsize]\n                    if width >= height:\n                        scalefactor = (maxsize*1.0) / height\n                        res = cv2.resize(image,(int(width * scalefactor), int(height*scalefactor)), interpolation = cv2.INTER_CUBIC)\n                        center_x = int(round(width*scalefactor*0.5))\n                        print center_x\n                        cropped = res[0:maxsize,center_x - maxsize/2:center_x + maxsize/2]\n                    count += 1\n                    cv2.imwrite(""imgs-valid/""+file_name+\'_\'+str(count)+\'.jpg\',cropped)\n            except:\n                continue\n    except:\n        print ""no post in xml""\n        return\n\nclass DownloadWorker(Thread):\n    def __init__(self, queue):\n        Thread.__init__(self)\n        self.queue = queue\n\n    def run(self):\n        while True:\n            # Get the work from the queue and expand the tuple\n            url = self.queue.get()\n            if url is None:\n                break\n            # download_link(directory, link)\n            download_imgs(url)\n            self.queue.task_done()\n\nif __name__ == \'__main__\':\n    start = time()\n    download_queue = Queue.Queue(maxsize=100)\n    for x in range(8):\n        worker = DownloadWorker(download_queue)\n        worker.daemon = True\n        worker.start()\n    \n    url_links = [""http://safebooru.org/index.php?page=dapi&s=post&q=index&tags=1girl%20solo&pid=""+str(i+5000) for i in xrange(299,10000)]\n    # print url_links[:10]\n\n    for link in url_links:\n        download_queue.put(link)\n    download_queue.join()\n    print ""the images num is {0}"".format(len(url_links))\n    print ""took time : {0}"".format(time() - start)\n\n\n\n'"
deepcolor/download_images_v2.py,0,"b'import os\nimport Queue\nfrom threading import Thread\nfrom time import time\nfrom itertools import chain\nimport urllib2\nimport untangle\nimport numpy as np\nimport cv2\n\ndef download_imgs(url):\n    # count = 0\n    maxsize = 512\n    file_name = url.split(\'=\')[-1]\n    header = {\'Referer\':\'http://safebooru.org/index.php?page=post&s=list\',\'User-Agent\' : \'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_12_3) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/56.0.2924.87 Safari/537.36\'}\n    def get_links(self) :\n        return\n\nclass DownloadWorker(Thread):\n    def __init__(self, queue):\n        Thread.__init__(self)\n        self.queue = queue\n\n    def run(self):\n        while True:\n            # Get the work from the queue and expand the tuple\n            url = self.queue.get()\n            if url is None:\n                break\n            # download_link(directory, link)\n            download_imgs(url)\n            self.queue.task_done()\n\nif __name__ == \'__main__\':\n    start = time()\n    download_queue = Queue.Queue(maxsize=100)\n    for x in range(8):\n        worker = DownloadWorker(download_queue)\n        worker.daemon = True\n        worker.start()\n    \n    url_links = [""http://safebooru.org/index.php?page=post&s=view&id={0}"".format(string(i)) for i in xrange(299,10000)]\n    # print url_links[:10]\n\n    for link in url_links:\n        download_queue.put(link)\n    download_queue.join()\n    print ""the images num is {0}"".format(len(url_links))\n    print ""took time : {0}"".format(time() - start)\n\n\n\n'"
deepcolor/lines.py,0,"b'import cv2\nimport numpy as np\nfrom matplotlib import pyplot as plt\nfrom glob import glob\nfrom random import randint\n\ndata = glob(""imgs-valid/*.jpg"")\nfor imname in data:\n\n    cimg = cv2.imread(imname,1)\n    cimg = np.fliplr(cimg.reshape(-1,3)).reshape(cimg.shape)\n    cimg = cv2.resize(cimg, (256,256))\n\n    img = cv2.imread(imname,0)\n\n    # kernel = np.ones((5,5),np.float32)/25\n    for i in xrange(30):\n        randx = randint(0,205)\n        randy = randint(0,205)\n        cimg[randx:randx+50, randy:randy+50] = 255\n    blur = cv2.blur(cimg,(100,100))\n\n\n    # img_gray = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n    img_edge = cv2.adaptiveThreshold(img, 255,\n                                     cv2.ADAPTIVE_THRESH_MEAN_C,\n                                     cv2.THRESH_BINARY,\n                                     blockSize=9,\n                                     C=2)\n    # img_edge = cv2.cvtColor(img_edge, cv2.COLOR_GRAY2RGB)\n    # img_cartoon = cv2.bitwise_and(img, img_edge)\n\n    plt.subplot(131),plt.imshow(cimg)\n    plt.title(\'Original Image\'), plt.xticks([]), plt.yticks([])\n\n    plt.subplot(132),plt.imshow(blur)\n    plt.title(\'Edge Image\'), plt.xticks([]), plt.yticks([])\n\n    plt.subplot(133),plt.imshow(img_edge,cmap = \'gray\')\n    plt.title(\'Edge Image\'), plt.xticks([]), plt.yticks([])\n\n    plt.show()\n'"
deepcolor/main.py,38,"b'import tensorflow as tf\nimport numpy as np\nimport os\nfrom glob import glob\nimport sys\nimport math\nfrom random import randint\n\nimport ops\nfrom ops import *\nfrom utils import *\n\ndef imageblur(cimg, sampling=False):\n    if sampling:\n        cimg = cimg * 0.3 + np.ones_like(cimg) * 0.7 * 255\n    else:\n        for i in xrange(30):\n            randx = randint(0,205)\n            randy = randint(0,205)\n            cimg[randx:randx+50, randy:randy+50] = 255\n    return cv2.blur(cimg,(100,100))\n\nclass Color():\n    def __init__(self, imgsize=256, batchsize=4):\n        self.batch_size = batchsize\n        self.batch_size_sqrt = int(math.sqrt(self.batch_size))\n        self.image_size = imgsize\n        self.output_size = imgsize\n\n        self.gf_dim = 64\n        self.df_dim = 64\n\n        self.input_colors = 1\n        self.input_colors2 = 3\n        self.output_colors = 3\n\n        self.l1_scaling = 100\n\n        self.d_bn1 = batch_norm(name=\'d_bn1\')\n        self.d_bn2 = batch_norm(name=\'d_bn2\')\n        self.d_bn3 = batch_norm(name=\'d_bn3\')\n\n        self.line_images = tf.placeholder(tf.float32, [self.batch_size, self.image_size, self.image_size, self.input_colors])\n        self.color_images = tf.placeholder(tf.float32, [self.batch_size, self.image_size, self.image_size, self.input_colors2])\n        self.real_images = tf.placeholder(tf.float32, [self.batch_size, self.image_size, self.image_size, self.output_colors])\n\n        combined_preimage = tf.concat(axis=3, values=[self.line_images, self.color_images])\n        # combined_preimage = tf.concat([self.line_images, self.color_images], 3)\n        # combined_preimage = self.line_images\n\n        self.generated_images = self.generator(combined_preimage)\n\n        self.real_AB = tf.concat(axis=3, values=[combined_preimage, self.real_images])\n        self.fake_AB = tf.concat(axis=3, values=[combined_preimage, self.generated_images])\n\n        # self.real_AB = tf.concat([combined_preimage, self.real_images], 3)\n        # self.fake_AB = tf.concat([combined_preimage, self.generated_images], 3)\n\n        self.disc_true, disc_true_logits = self.discriminator(self.real_AB, reuse=False)\n        self.disc_fake, disc_fake_logits = self.discriminator(self.fake_AB, reuse=True)\n\n        self.d_loss_real = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=disc_true_logits, labels=tf.ones_like(disc_true_logits)))\n        self.d_loss_fake = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=disc_fake_logits, labels=tf.zeros_like(disc_fake_logits)))\n        self.d_loss = self.d_loss_real + self.d_loss_fake\n\n        self.g_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=disc_fake_logits, labels=tf.ones_like(disc_fake_logits))) \\\n                        + self.l1_scaling * tf.reduce_mean(tf.abs(self.real_images - self.generated_images))\n\n        t_vars = tf.trainable_variables()\n        self.d_vars = [var for var in t_vars if \'d_\' in var.name]\n        self.g_vars = [var for var in t_vars if \'g_\' in var.name]\n\n        self.d_optim = tf.train.AdamOptimizer(0.0002, beta1=0.5).minimize(self.d_loss, var_list=self.d_vars)\n        self.g_optim = tf.train.AdamOptimizer(0.0002, beta1=0.5).minimize(self.g_loss, var_list=self.g_vars)\n\n\n    def discriminator(self, image, y=None, reuse=False):\n        # image is 256 x 256 x (input_c_dim + output_c_dim)\n        with tf.variable_scope(""discriminator"") as scope:\n            if reuse:\n                tf.get_variable_scope().reuse_variables()\n            else:\n                assert tf.get_variable_scope().reuse == False\n\n            h0 = lrelu(conv2d(image, self.df_dim, name=\'d_h0_conv\')) # h0 is (128 x 128 x self.df_dim)\n            h1 = lrelu(self.d_bn1(conv2d(h0, self.df_dim*2, name=\'d_h1_conv\'))) # h1 is (64 x 64 x self.df_dim*2)\n            h2 = lrelu(self.d_bn2(conv2d(h1, self.df_dim*4, name=\'d_h2_conv\'))) # h2 is (32 x 32 x self.df_dim*4)\n            h3 = lrelu(self.d_bn3(conv2d(h2, self.df_dim*8, d_h=1, d_w=1, name=\'d_h3_conv\'))) # h3 is (16 x 16 x self.df_dim*8)\n            h4 = linear(tf.reshape(h3, [self.batch_size, -1]), 1, \'d_h3_lin\')\n            return tf.nn.sigmoid(h4), h4\n\n    def generator(self, img_in):\n        with tf.variable_scope(""generator"") as scope:\n            s = self.output_size\n            s2, s4, s8, s16, s32, s64, s128 = int(s/2), int(s/4), int(s/8), int(s/16), int(s/32), int(s/64), int(s/128)\n            # image is (256 x 256 x input_c_dim)\n            e1 = conv2d(img_in, self.gf_dim, name=\'g_e1_conv\') # e1 is (128 x 128 x self.gf_dim)\n            e2 = bn(conv2d(lrelu(e1), self.gf_dim*2, name=\'g_e2_conv\')) # e2 is (64 x 64 x self.gf_dim*2)\n            e3 = bn(conv2d(lrelu(e2), self.gf_dim*4, name=\'g_e3_conv\')) # e3 is (32 x 32 x self.gf_dim*4)\n            e4 = bn(conv2d(lrelu(e3), self.gf_dim*8, name=\'g_e4_conv\')) # e4 is (16 x 16 x self.gf_dim*8)\n            e5 = bn(conv2d(lrelu(e4), self.gf_dim*8, name=\'g_e5_conv\')) # e5 is (8 x 8 x self.gf_dim*8)\n\n\n            self.d4, self.d4_w, self.d4_b = deconv2d(tf.nn.relu(e5), [self.batch_size, s16, s16, self.gf_dim*8], name=\'g_d4\', with_w=True)\n            d4 = bn(self.d4)\n            d4 = tf.concat(axis=3, values=[d4, e4])\n            # d4 is (16 x 16 x self.gf_dim*8*2)\n\n            self.d5, self.d5_w, self.d5_b = deconv2d(tf.nn.relu(d4), [self.batch_size, s8, s8, self.gf_dim*4], name=\'g_d5\', with_w=True)\n            d5 = bn(self.d5)\n            d5 = tf.concat(axis=3, values=[d5, e3])\n            # d5 is (32 x 32 x self.gf_dim*4*2)\n\n            self.d6, self.d6_w, self.d6_b = deconv2d(tf.nn.relu(d5), [self.batch_size, s4, s4, self.gf_dim*2], name=\'g_d6\', with_w=True)\n            d6 = bn(self.d6)\n            d6 = tf.concat(axis=3, values=[d6, e2])\n            # d6 is (64 x 64 x self.gf_dim*2*2)\n\n            self.d7, self.d7_w, self.d7_b = deconv2d(tf.nn.relu(d6), [self.batch_size, s2, s2, self.gf_dim], name=\'g_d7\', with_w=True)\n            d7 = bn(self.d7)\n            d7 = tf.concat(axis=3, values=[d7, e1])\n            # d7 is (128 x 128 x self.gf_dim*1*2)\n\n            self.d8, self.d8_w, self.d8_b = deconv2d(tf.nn.relu(d7), [self.batch_size, s, s, self.output_colors], name=\'g_d8\', with_w=True)\n            # d8 is (256 x 256 x output_c_dim)\n\n        return tf.nn.tanh(self.d8)\n\n\n    def imageblur(self, cimg, sampling=False):\n        if sampling:\n            cimg = cimg * 0.3 + np.ones_like(cimg) * 0.7 * 255\n        else:\n            for i in xrange(30):\n                randx = randint(0,205)\n                randy = randint(0,205)\n                cimg[randx:randx+50, randy:randy+50] = 255\n        return cv2.blur(cimg,(100,100))\n\n    def train(self):\n        self.loadmodel()\n\n        data = glob(os.path.join(""imgs"", ""*.jpg""))\n        print data[0]\n        base = np.array([get_image(sample_file) for sample_file in data[0:self.batch_size]])\n        base_normalized = base/255.0\n\n        base_edge = np.array([cv2.adaptiveThreshold(cv2.cvtColor(ba, cv2.COLOR_BGR2GRAY), 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, blockSize=9, C=2) for ba in base]) / 255.0\n        base_edge = np.expand_dims(base_edge, 3)\n\n        base_colors = np.array([self.imageblur(ba) for ba in base]) / 255.0\n\n        ims(""results/base.png"",merge_color(base_normalized, [self.batch_size_sqrt, self.batch_size_sqrt]))\n        ims(""results/base_line.jpg"",merge(base_edge, [self.batch_size_sqrt, self.batch_size_sqrt]))\n        ims(""results/base_colors.jpg"",merge_color(base_colors, [self.batch_size_sqrt, self.batch_size_sqrt]))\n\n        datalen = len(data)\n\n        for e in xrange(20000):\n            for i in range(datalen / self.batch_size):\n                batch_files = data[i*self.batch_size:(i+1)*self.batch_size]\n                batch = np.array([get_image(batch_file) for batch_file in batch_files])\n                batch_normalized = batch/255.0\n\n                batch_edge = np.array([cv2.adaptiveThreshold(cv2.cvtColor(ba, cv2.COLOR_BGR2GRAY), 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, blockSize=9, C=2) for ba in batch]) / 255.0\n                batch_edge = np.expand_dims(batch_edge, 3)\n\n                batch_colors = np.array([self.imageblur(ba) for ba in batch]) / 255.0\n\n                d_loss, _ = self.sess.run([self.d_loss, self.d_optim], feed_dict={self.real_images: batch_normalized, self.line_images: batch_edge, self.color_images: batch_colors})\n                g_loss, _ = self.sess.run([self.g_loss, self.g_optim], feed_dict={self.real_images: batch_normalized, self.line_images: batch_edge, self.color_images: batch_colors})\n\n                print ""%d: [%d / %d] d_loss %f, g_loss %f"" % (e, i, (datalen/self.batch_size), d_loss, g_loss)\n\n                if i % 100 == 0:\n                    recreation = self.sess.run(self.generated_images, feed_dict={self.real_images: base_normalized, self.line_images: base_edge, self.color_images: base_colors})\n                    ims(""results/""+str(e*100000 + i)+"".jpg"",merge_color(recreation, [self.batch_size_sqrt, self.batch_size_sqrt]))\n\n                if i % 500 == 0:\n                    self.save(""./checkpoint"", e*100000 + i)\n\n    def loadmodel(self, load_discrim=True):\n        self.sess = tf.Session()\n        try:\n            self.sess.run(tf.global_variables_initializer())\n        except AttributeError:\n            self.sess.run(tf.initialize_all_variables())\n\n\n        if load_discrim:\n            self.saver = tf.train.Saver()\n        else:\n            self.saver = tf.train.Saver(self.g_vars)\n\n        if self.load(""./checkpoint""):\n            print ""Loaded""\n        else:\n            print ""Load failed""\n\n    def sample(self):\n        self.loadmodel()\n\n        data = glob(os.path.join(""imgs"", ""*.jpg""))\n\n        datalen = len(data)\n\n        for i in range(min(100,datalen / self.batch_size)):\n            batch_files = data[i*self.batch_size:(i+1)*self.batch_size]\n            batch = np.array([get_image(batch_file) for batch_file in batch_files])\n            batch_normalized = batch/255.0\n\n            batch_edge = np.array([cv2.adaptiveThreshold(cv2.cvtColor(ba, cv2.COLOR_BGR2GRAY), 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, blockSize=9, C=2) for ba in batch]) / 255.0\n            batch_edge = np.expand_dims(batch_edge, 3)\n\n            batch_colors = np.array([self.imageblur(ba,True) for ba in batch]) / 255.0\n\n            recreation = self.sess.run(self.generated_images, feed_dict={self.real_images: batch_normalized, self.line_images: batch_edge, self.color_images: batch_colors})\n            ims(""results/sample_""+str(i)+"".jpg"",merge_color(recreation, [self.batch_size_sqrt, self.batch_size_sqrt]))\n            ims(""results/sample_""+str(i)+""_origin.jpg"",merge_color(batch_normalized, [self.batch_size_sqrt, self.batch_size_sqrt]))\n            ims(""results/sample_""+str(i)+""_line.jpg"",merge_color(batch_edge, [self.batch_size_sqrt, self.batch_size_sqrt]))\n            ims(""results/sample_""+str(i)+""_color.jpg"",merge_color(batch_colors, [self.batch_size_sqrt, self.batch_size_sqrt]))\n\n\n    def save(self, checkpoint_dir, step):\n        model_name = ""model""\n        model_dir = ""tr""\n        checkpoint_dir = os.path.join(checkpoint_dir, model_dir)\n\n        if not os.path.exists(checkpoint_dir):\n            os.makedirs(checkpoint_dir)\n\n        self.saver.save(self.sess,\n                        os.path.join(checkpoint_dir, model_name),\n                        global_step=step)\n\n    def load(self, checkpoint_dir):\n        print("" [*] Reading checkpoint..."")\n\n        model_dir = ""tr""\n        checkpoint_dir = os.path.join(checkpoint_dir, model_dir)\n\n        ckpt = tf.train.get_checkpoint_state(checkpoint_dir)\n        if ckpt and ckpt.model_checkpoint_path:\n            ckpt_name = os.path.basename(ckpt.model_checkpoint_path)\n            self.saver.restore(self.sess, os.path.join(checkpoint_dir, ckpt_name))\n            return True\n        else:\n            return False\n\n\nif __name__ == \'__main__\':\n    if len(sys.argv) < 2:\n        print ""Usage: python main.py [train, sample]""\n    else:\n        cmd = sys.argv[1]\n        if cmd == ""train"":\n            c = Color()\n            c.train()\n        elif cmd == ""sample"":\n            c = Color()\n            c.sample()\n        else:\n            print ""Usage: python main.py [train, sample]""\n'"
deepcolor/ops.py,0,b''
deepcolor/server.py,0,"b'from bottle import route, run, template, static_file, get, post, request, BaseRequest\nimport urllib2\nimport cv2\nimport numpy as np\nimport re\nimport base64\n\nimport main\nfrom main import *\n\nBaseRequest.MEMFILE_MAX = 1000 * 1000\n\nc = Color(512, 1)\nc.loadmodel(False)\n\n@route(\'/<filename:path>\')\ndef send_static(filename):\n    return static_file(filename, root=\'web/\')\n\n@route(\'/draw\')\ndef send_static():\n    return static_file(""draw.html"", root=\'web/\')\n\n@route(\'/\')\ndef send_static():\n    return static_file(""index.html"", root=\'web/\')\n\ndef imageblur(cimg, sampling=False):\n    if sampling:\n        cimg = cimg * 0.3 + np.ones_like(cimg) * 0.7 * 255\n    else:\n        for i in xrange(30):\n            randx = randint(0,205)\n            randy = randint(0,205)\n            cimg[randx:randx+50, randy:randy+50] = 255\n    return cv2.blur(cimg,(100,100))\n\n@route(""/standard_sanae"", method=""POST"")\ndef do_uploadtl():\n    lines_img = cv2.imread(""web/image_examples/sanae.png"", 1)\n    lines_img = np.array(cv2.resize(lines_img, (512,512)))\n    lines_img = cv2.adaptiveThreshold(cv2.cvtColor(lines_img, cv2.COLOR_BGR2GRAY), 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, blockSize=9, C=2)\n    lines_img = cv2.merge((lines_img,lines_img,lines_img,255 - lines_img))\n    cnt = cv2.imencode("".png"",lines_img)[1]\n    return base64.b64encode(cnt)\n\n@route(""/standard_armscross"", method=""POST"")\ndef do_uploadtl():\n    lines_img = cv2.imread(""web/image_examples/armscross.png"", 1)\n    lines_img = np.array(cv2.resize(lines_img, (512,512)))\n    lines_img = cv2.adaptiveThreshold(cv2.cvtColor(lines_img, cv2.COLOR_BGR2GRAY), 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, blockSize=9, C=2)\n    lines_img = cv2.merge((lines_img,lines_img,lines_img,255 - lines_img))\n    cnt = cv2.imencode("".png"",lines_img)[1]\n    return base64.b64encode(cnt)\n\n@route(""/standard_picasso"", method=""POST"")\ndef do_uploadtl():\n    lines_img = cv2.imread(""web/image_examples/picasso.png"", 1)\n    lines_img = np.array(cv2.resize(lines_img, (512,512)))\n    lines_img = cv2.adaptiveThreshold(cv2.cvtColor(lines_img, cv2.COLOR_BGR2GRAY), 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, blockSize=9, C=2)\n    lines_img = cv2.merge((lines_img,lines_img,lines_img,255 - lines_img))\n    cnt = cv2.imencode("".png"",lines_img)[1]\n    return base64.b64encode(cnt)\n\n@route(""/upload_toline"", method=""POST"")\ndef do_uploadtl():\n    print ""Parsing line""\n    img = request.files.get(\'img\')\n    lines_img = cv2.imdecode(np.fromstring(img.file.read(), np.uint8), cv2.CV_LOAD_IMAGE_UNCHANGED)\n    lines_img = np.array(cv2.resize(lines_img, (512,512)))\n    lines_img = cv2.adaptiveThreshold(cv2.cvtColor(lines_img, cv2.COLOR_BGR2GRAY), 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, blockSize=9, C=2)\n    lines_img = cv2.merge((lines_img,lines_img,lines_img,255 - lines_img))\n    cnt = cv2.imencode("".png"",lines_img)[1]\n    return base64.b64encode(cnt)\n\ndef imageblur(cimg, sampling=False):\n    if sampling:\n        cimg = cimg * 0.3 + np.ones_like(cimg) * 0.7 * 255\n    else:\n        for i in xrange(30):\n            randx = randint(0,205)\n            randy = randint(0,205)\n            cimg[randx:randx+50, randy:randy+50] = 255\n    return cv2.blur(cimg,(100,100))\n\n@route(\'/upload_canvas\', method=\'POST\')\ndef do_uploadc():\n    print ""Got it""\n    # lines = request.files.get(\'lines\')\n    # colors = request.files.get(\'colors\')\n    line_data = request.forms.get(""lines"")\n    line_data = re.sub(\'^data:image/.+;base64,\', \'\', line_data)\n    line_s = base64.b64decode(line_data)\n    line_img = np.fromstring(line_s, dtype=np.uint8)\n    line_img = cv2.imdecode(line_img, -1)\n\n    color_data = request.forms.get(""colors"")\n    color_data = re.sub(\'^data:image/.+;base64,\', \'\', color_data)\n    color_s = base64.b64decode(color_data)\n    color_img = np.fromstring(color_s, dtype=np.uint8)\n    color_img = cv2.imdecode(color_img, -1)\n\n    # for c in range(0,3):\n    color_img = color_img * (line_img[:,:] / 255.0)\n\n    lines_img = np.array(cv2.resize(line_img, (512,512)))\n    # lines_img = cv2.adaptiveThreshold(lines_img, 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, blockSize=9, C=2)\n    lines_img = np.array([lines_img]) / 255.0\n    lines_img = lines_img[:,:,:,0]\n    lines_img = np.expand_dims(lines_img, 3)\n\n    colors_img = np.array(cv2.resize(color_img, (512,512)))\n    # colors_img = cv2.blur(colors_img, (100, 100))\n    colors_img = imageblur(colors_img, True)\n    colors_img = np.array([colors_img]) / 255.0\n    colors_img = colors_img[:,:,:,0:3]\n    generated = c.sess.run(c.generated_images, feed_dict={c.line_images: lines_img, c.color_images: colors_img})\n    cnt = cv2.imencode("".png"",generated[0]*255)[1]\n    return base64.b64encode(cnt)\n\n@route(\'/upload_origin\', method=\'POST\')\ndef do_uploado():\n    lines = request.files.get(\'lines\')\n    colors = request.files.get(\'colors\')\n\n    lines_img = cv2.imdecode(np.fromstring(lines.file.read(), np.uint8), cv2.CV_LOAD_IMAGE_UNCHANGED)\n    lines_img = np.array(cv2.resize(lines_img, (512,512)))\n    lines_img = cv2.adaptiveThreshold(cv2.cvtColor(lines_img, cv2.COLOR_BGR2GRAY), 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, blockSize=9, C=2)\n    lines_img = np.array([lines_img]) / 255.0\n    lines_img = np.expand_dims(lines_img, 3)\n\n    colors_img = cv2.imdecode(np.fromstring(colors.file.read(), np.uint8), cv2.CV_LOAD_IMAGE_UNCHANGED)\n    colors_img = np.array(cv2.resize(colors_img, (512,512)))\n    colors_img = cv2.blur(colors_img, (100, 100))\n    colors_img = np.array([colors_img]) / 255.0\n\n    cv2.imwrite(""uploaded/lines.jpg"", lines_img[0]*255)\n    cv2.imwrite(""uploaded/colors.jpg"", colors_img[0]*255)\n\n    generated = c.sess.run(c.generated_images, feed_dict={c.line_images: lines_img, c.color_images: colors_img})\n\n    cv2.imwrite(""uploaded/gen.jpg"", generated[0]*255)\n\n    return static_file(""uploaded/gen.jpg"",\n                       root=""."",\n                       mimetype=\'image/jpg\')\n\nrun(host=""0.0.0.0"", port=8000)\n'"
deepcolor/testserver.py,0,"b'from bottle import route, run\n\n@route(\'/hello\')\ndef hello():\n        return ""Hello World!""\nrun(host=\'0.0.0.0\', port=8080, debug=True)\n'"
deepcolor/tf_upgrade.py,85,"b'# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Upgrader for Python scripts from pre-1.0 TensorFlow to 1.0 TensorFlow.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nimport argparse\nimport ast\nimport collections\nimport os\nimport shutil\nimport sys\nimport tempfile\nimport traceback\n\n\nclass APIChangeSpec(object):\n  """"""List of maps that describe what changed in the API.""""""\n\n  def __init__(self):\n    # Maps from a function name to a dictionary that describes how to\n    # map from an old argument keyword to the new argument keyword.\n    self.function_keyword_renames = {\n        ""tf.count_nonzero"": {\n            ""reduction_indices"": ""axis""\n        },\n        ""tf.reduce_all"": {\n            ""reduction_indices"": ""axis""\n        },\n        ""tf.reduce_any"": {\n            ""reduction_indices"": ""axis""\n        },\n        ""tf.reduce_max"": {\n            ""reduction_indices"": ""axis""\n        },\n        ""tf.reduce_mean"": {\n            ""reduction_indices"": ""axis""\n        },\n        ""tf.reduce_min"": {\n            ""reduction_indices"": ""axis""\n        },\n        ""tf.reduce_prod"": {\n            ""reduction_indices"": ""axis""\n        },\n        ""tf.reduce_sum"": {\n            ""reduction_indices"": ""axis""\n        },\n        ""tf.reduce_logsumexp"": {\n            ""reduction_indices"": ""axis""\n        },\n        ""tf.expand_dims"": {\n            ""dim"": ""axis""\n        },\n        ""tf.argmax"": {\n            ""dimension"": ""axis""\n        },\n        ""tf.argmin"": {\n            ""dimension"": ""axis""\n        },\n        ""tf.reduce_join"": {\n            ""reduction_indices"": ""axis""\n        },\n        ""tf.sparse_concat"": {\n            ""concat_dim"": ""axis""\n        },\n        ""tf.sparse_split"": {\n            ""split_dim"": ""axis""\n        },\n        ""tf.sparse_reduce_sum"": {\n            ""reduction_axes"": ""axis""\n        },\n        ""tf.reverse_sequence"": {\n            ""seq_dim"": ""seq_axis"",\n            ""batch_dim"": ""batch_axis""\n        },\n        ""tf.sparse_reduce_sum_sparse"": {\n            ""reduction_axes"": ""axis""\n        },\n        ""tf.squeeze"": {\n            ""squeeze_dims"": ""axis""\n        },\n        ""tf.split"": {\n            ""split_dim"": ""axis"",\n            ""num_split"": ""num_or_size_splits""\n        },\n        ""tf.concat"": {\n            ""concat_dim"": ""axis""\n        },\n    }\n\n    # Mapping from function to the new name of the function\n    self.function_renames = {\n        ""tf.inv"": ""tf.reciprocal"",\n        ""tf.contrib.deprecated.scalar_summary"": ""tf.summary.scalar"",\n        ""tf.contrib.deprecated.histogram_summary"": ""tf.summary.histogram"",\n        ""tf.listdiff"": ""tf.setdiff1d"",\n        ""tf.list_diff"": ""tf.setdiff1d"",\n        ""tf.mul"": ""tf.multiply"",\n        ""tf.neg"": ""tf.negative"",\n        ""tf.sub"": ""tf.subtract"",\n        ""tf.train.SummaryWriter"": ""tf.summary.FileWriter"",\n        ""tf.scalar_summary"": ""tf.summary.scalar"",\n        ""tf.histogram_summary"": ""tf.summary.histogram"",\n        ""tf.audio_summary"": ""tf.summary.audio"",\n        ""tf.image_summary"": ""tf.summary.image"",\n        ""tf.merge_summary"": ""tf.summary.merge"",\n        ""tf.merge_all_summaries"": ""tf.summary.merge_all"",\n        ""tf.image.per_image_whitening"": ""tf.image.per_image_standardization"",\n        ""tf.all_variables"": ""tf.global_variables"",\n        ""tf.VARIABLES"": ""tf.GLOBAL_VARIABLES"",\n        ""tf.initialize_all_variables"": ""tf.global_variables_initializer"",\n        ""tf.initialize_variables"": ""tf.variables_initializer"",\n        ""tf.initialize_local_variables"": ""tf.local_variables_initializer"",\n        ""tf.batch_matrix_diag"": ""tf.matrix_diag"",\n        ""tf.batch_band_part"": ""tf.band_part"",\n        ""tf.batch_set_diag"": ""tf.set_diag"",\n        ""tf.batch_matrix_transpose"": ""tf.matrix_transpose"",\n        ""tf.batch_matrix_determinant"": ""tf.matrix_determinant"",\n        ""tf.batch_matrix_inverse"": ""tf.matrix_inverse"",\n        ""tf.batch_cholesky"": ""tf.cholesky"",\n        ""tf.batch_cholesky_solve"": ""tf.cholesky_solve"",\n        ""tf.batch_matrix_solve"": ""tf.matrix_solve"",\n        ""tf.batch_matrix_triangular_solve"": ""tf.matrix_triangular_solve"",\n        ""tf.batch_matrix_solve_ls"": ""tf.matrix_solve_ls"",\n        ""tf.batch_self_adjoint_eig"": ""tf.self_adjoint_eig"",\n        ""tf.batch_self_adjoint_eigvals"": ""tf.self_adjoint_eigvals"",\n        ""tf.batch_svd"": ""tf.svd"",\n        ""tf.batch_fft"": ""tf.fft"",\n        ""tf.batch_ifft"": ""tf.ifft"",\n        ""tf.batch_ifft2d"": ""tf.ifft2d"",\n        ""tf.batch_fft3d"": ""tf.fft3d"",\n        ""tf.batch_ifft3d"": ""tf.ifft3d"",\n        ""tf.select"": ""tf.where"",\n        ""tf.complex_abs"": ""tf.abs"",\n        ""tf.batch_matmul"": ""tf.matmul"",\n        ""tf.pack"": ""tf.stack"",\n        ""tf.unpack"": ""tf.unstack"",\n    }\n\n    self.change_to_function = {\n        ""tf.ones_initializer"",\n        ""tf.zeros_initializer"",\n    }\n\n    # Functions that were reordered should be changed to the new keyword args\n    # for safety, if positional arguments are used. If you have reversed the\n    # positional arguments yourself, this could do the wrong thing.\n    self.function_reorders = {\n        ""tf.split"": [""axis"", ""num_or_size_splits"", ""value"", ""name""],\n        ""tf.sparse_split"": [""axis"", ""num_or_size_splits"", ""value"", ""name""],\n        ""tf.concat"": [""concat_dim"", ""values"", ""name""],\n        ""tf.svd"": [""tensor"", ""compute_uv"", ""full_matrices"", ""name""],\n        ""tf.nn.softmax_cross_entropy_with_logits"": [\n            ""logits"", ""labels"", ""dim"", ""name""],\n        ""tf.nn.sparse_softmax_cross_entropy_with_logits"": [\n            ""logits"", ""labels"", ""name""],\n        ""tf.nn.sigmoid_cross_entropy_with_logits"": [\n            ""logits"", ""labels"", ""name""]\n    }\n\n    # Specially handled functions.\n    self.function_handle = {""tf.reverse"": self._reverse_handler}\n\n  @staticmethod\n  def _reverse_handler(file_edit_recorder, node):\n    # TODO(aselle): Could check for a literal list of bools and try to convert\n    # them to indices.\n    comment = (""ERROR: tf.reverse has had its argument semantics changed\\n""\n               ""significantly the converter cannot detect this reliably, so you""\n               ""need to inspect this usage manually.\\n"")\n    file_edit_recorder.add(comment,\n                           node.lineno,\n                           node.col_offset,\n                           ""tf.reverse"",\n                           ""tf.reverse"",\n                           error=""tf.reverse requires manual check."")\n\n\nclass FileEditTuple(collections.namedtuple(\n    ""FileEditTuple"", [""comment"", ""line"", ""start"", ""old"", ""new""])):\n  """"""Each edit that is recorded by a FileEditRecorder.\n  Fields:\n    comment: A description of the edit and why it was made.\n    line: The line number in the file where the edit occurs (1-indexed).\n    start: The line number in the file where the edit occurs (0-indexed).\n    old: text string to remove (this must match what was in file).\n    new: text string to add in place of `old`.\n  """"""\n\n  __slots__ = ()\n\n\nclass FileEditRecorder(object):\n  """"""Record changes that need to be done to the file.""""""\n\n  def __init__(self, filename):\n    # all edits are lists of chars\n    self._filename = filename\n\n    self._line_to_edit = collections.defaultdict(list)\n    self._errors = []\n\n  def process(self, text):\n    """"""Process a list of strings, each corresponding to the recorded changes.\n    Args:\n      text: A list of lines of text (assumed to contain newlines)\n    Returns:\n      A tuple of the modified text and a textual description of what is done.\n    Raises:\n      ValueError: if substitution source location does not have expected text.\n    """"""\n\n    change_report = """"\n\n    # Iterate of each line\n    for line, edits in self._line_to_edit.items():\n      offset = 0\n      # sort by column so that edits are processed in order in order to make\n      # indexing adjustments cumulative for changes that change the string\n      # length\n      edits.sort(key=lambda x: x.start)\n\n      # Extract each line to a list of characters, because mutable lists\n      # are editable, unlike immutable strings.\n      char_array = list(text[line - 1])\n\n      # Record a description of the change\n      change_report += ""%r Line %d\\n"" % (self._filename, line)\n      change_report += ""-"" * 80 + ""\\n\\n""\n      for e in edits:\n        change_report += ""%s\\n"" % e.comment\n      change_report += ""\\n    Old: %s"" % (text[line - 1])\n\n      # Make underscore buffers for underlining where in the line the edit was\n      change_list = ["" ""] * len(text[line - 1])\n      change_list_new = ["" ""] * len(text[line - 1])\n\n      # Iterate for each edit\n      for e in edits:\n        # Create effective start, end by accounting for change in length due\n        # to previous edits\n        start_eff = e.start + offset\n        end_eff = start_eff + len(e.old)\n\n        # Make sure the edit is changing what it should be changing\n        old_actual = """".join(char_array[start_eff:end_eff])\n        if old_actual != e.old:\n          raise ValueError(""Expected text %r but got %r"" %\n                           ("""".join(e.old), """".join(old_actual)))\n        # Make the edit\n        char_array[start_eff:end_eff] = list(e.new)\n\n        # Create the underline highlighting of the before and after\n        change_list[e.start:e.start + len(e.old)] = ""~"" * len(e.old)\n        change_list_new[start_eff:end_eff] = ""~"" * len(e.new)\n\n        # Keep track of how to generate effective ranges\n        offset += len(e.new) - len(e.old)\n\n      # Finish the report comment\n      change_report += ""         %s\\n"" % """".join(change_list)\n      text[line - 1] = """".join(char_array)\n      change_report += ""    New: %s"" % (text[line - 1])\n      change_report += ""         %s\\n\\n"" % """".join(change_list_new)\n    return """".join(text), change_report, self._errors\n\n  def add(self, comment, line, start, old, new, error=None):\n    """"""Add a new change that is needed.\n    Args:\n      comment: A description of what was changed\n      line: Line number (1 indexed)\n      start: Column offset (0 indexed)\n      old: old text\n      new: new text\n      error: this ""edit"" is something that cannot be fixed automatically\n    Returns:\n      None\n    """"""\n\n    self._line_to_edit[line].append(\n        FileEditTuple(comment, line, start, old, new))\n    if error:\n      self._errors.append(""%s:%d: %s"" % (self._filename, line, error))\n\n\nclass TensorFlowCallVisitor(ast.NodeVisitor):\n  """"""AST Visitor that finds TensorFlow Function calls.\n  Updates function calls from old API version to new API version.\n  """"""\n\n  def __init__(self, filename, lines):\n    self._filename = filename\n    self._file_edit = FileEditRecorder(filename)\n    self._lines = lines\n    self._api_change_spec = APIChangeSpec()\n\n  def process(self, lines):\n    return self._file_edit.process(lines)\n\n  def generic_visit(self, node):\n    ast.NodeVisitor.generic_visit(self, node)\n\n  def _rename_functions(self, node, full_name):\n    function_renames = self._api_change_spec.function_renames\n    try:\n      new_name = function_renames[full_name]\n      self._file_edit.add(""Renamed function %r to %r"" % (full_name,\n                                                         new_name),\n                          node.lineno, node.col_offset, full_name, new_name)\n    except KeyError:\n      pass\n\n  def _get_attribute_full_path(self, node):\n    """"""Traverse an attribute to generate a full name e.g. tf.foo.bar.\n    Args:\n      node: A Node of type Attribute.\n    Returns:\n      a \'.\'-delimited full-name or None if the tree was not a simple form.\n      i.e. `foo()+b).bar` returns None, while `a.b.c` would return ""a.b.c"".\n    """"""\n    curr = node\n    items = []\n    while not isinstance(curr, ast.Name):\n      if not isinstance(curr, ast.Attribute):\n        return None\n      items.append(curr.attr)\n      curr = curr.value\n    items.append(curr.id)\n    return ""."".join(reversed(items))\n\n  def _find_true_position(self, node):\n    """"""Return correct line number and column offset for a given node.\n    This is necessary mainly because ListComp\'s location reporting reports\n    the next token after the list comprehension list opening.\n    Args:\n      node: Node for which we wish to know the lineno and col_offset\n    """"""\n    import re\n    find_open = re.compile(""^\\s*(\\\\[).*$"")\n    find_string_chars = re.compile(""[\'\\""]"")\n\n    if isinstance(node, ast.ListComp):\n      # Strangely, ast.ListComp returns the col_offset of the first token\n      # after the \'[\' token which appears to be a bug. Workaround by\n      # explicitly finding the real start of the list comprehension.\n      line = node.lineno\n      col = node.col_offset\n      # loop over lines\n      while 1:\n        # Reverse the text to and regular expression search for whitespace\n        text = self._lines[line-1]\n        reversed_preceding_text = text[:col][::-1]\n        # First find if a [ can be found with only whitespace between it and\n        # col.\n        m = find_open.match(reversed_preceding_text)\n        if m:\n          new_col_offset = col - m.start(1) - 1\n          return line, new_col_offset\n        else:\n          if (reversed_preceding_text=="""" or\n             reversed_preceding_text.isspace()):\n            line = line - 1\n            prev_line = self._lines[line - 1]\n            # TODO(aselle):\n            # this is poor comment detection, but it is good enough for\n            # cases where the comment does not contain string literal starting/\n            # ending characters. If ast gave us start and end locations of the\n            # ast nodes rather than just start, we could use string literal\n            # node ranges to filter out spurious #\'s that appear in string\n            # literals.\n            comment_start = prev_line.find(""#"")\n            if comment_start ==  -1:\n              col = len(prev_line) -1\n            elif find_string_chars.search(prev_line[comment_start:]) is None:\n              col = comment_start\n            else:\n              return None, None\n          else:\n            return None, None\n    # Most other nodes return proper locations (with notably does not), but\n    # it is not possible to use that in an argument.\n    return node.lineno, node.col_offset\n\n\n  def visit_Call(self, node):  # pylint: disable=invalid-name\n    """"""Handle visiting a call node in the AST.\n    Args:\n      node: Current Node\n    """"""\n\n\n    # Find a simple attribute name path e.g. ""tf.foo.bar""\n    full_name = self._get_attribute_full_path(node.func)\n\n    # Make sure the func is marked as being part of a call\n    node.func.is_function_for_call = True\n\n    if full_name and full_name.startswith(""tf.""):\n      # Call special handlers\n      function_handles = self._api_change_spec.function_handle\n      if full_name in function_handles:\n        function_handles[full_name](self._file_edit, node)\n\n      # Examine any non-keyword argument and make it into a keyword argument\n      # if reordering required.\n      function_reorders = self._api_change_spec.function_reorders\n      function_keyword_renames = (\n          self._api_change_spec.function_keyword_renames)\n\n      if full_name in function_reorders:\n        reordered = function_reorders[full_name]\n        for idx, arg in enumerate(node.args):\n          lineno, col_offset = self._find_true_position(arg)\n          if lineno is None or col_offset is None:\n            self._file_edit.add(\n                ""Failed to add keyword %r to reordered function %r""\n                % (reordered[idx], full_name), arg.lineno, arg.col_offset,\n                """", """",\n                error=""A necessary keyword argument failed to be inserted."")\n          else:\n            keyword_arg = reordered[idx]\n            if (full_name in function_keyword_renames and\n                keyword_arg in function_keyword_renames[full_name]):\n              keyword_arg = function_keyword_renames[full_name][keyword_arg]\n            self._file_edit.add(""Added keyword %r to reordered function %r""\n                                % (reordered[idx], full_name), lineno,\n                                col_offset, """", keyword_arg + ""="")\n\n      # Examine each keyword argument and convert it to the final renamed form\n      renamed_keywords = ({} if full_name not in function_keyword_renames else\n                          function_keyword_renames[full_name])\n      for keyword in node.keywords:\n        argkey = keyword.arg\n        argval = keyword.value\n\n        if argkey in renamed_keywords:\n          argval_lineno, argval_col_offset = self._find_true_position(argval)\n          if (argval_lineno is not None and argval_col_offset is not None):\n            # TODO(aselle): We should scan backward to find the start of the\n            # keyword key. Unfortunately ast does not give you the location of\n            # keyword keys, so we are forced to infer it from the keyword arg\n            # value.\n            key_start = argval_col_offset - len(argkey) - 1\n            key_end = key_start + len(argkey) + 1\n            if self._lines[argval_lineno - 1][key_start:key_end] == argkey + ""="":\n              self._file_edit.add(""Renamed keyword argument from %r to %r"" %\n                              (argkey, renamed_keywords[argkey]),\n                              argval_lineno,\n                              argval_col_offset - len(argkey) - 1,\n                              argkey + ""="", renamed_keywords[argkey] + ""="")\n              continue\n          self._file_edit.add(\n              ""Failed to rename keyword argument from %r to %r"" %\n              (argkey, renamed_keywords[argkey]),\n              argval.lineno,\n              argval.col_offset - len(argkey) - 1,\n              """", """",\n              error=""Failed to find keyword lexographically. Fix manually."")\n\n    ast.NodeVisitor.generic_visit(self, node)\n\n  def visit_Attribute(self, node):  # pylint: disable=invalid-name\n    """"""Handle bare Attributes i.e. [tf.foo, tf.bar].\n    Args:\n      node: Node that is of type ast.Attribute\n    """"""\n    full_name = self._get_attribute_full_path(node)\n    if full_name and full_name.startswith(""tf.""):\n      self._rename_functions(node, full_name)\n    if full_name in self._api_change_spec.change_to_function:\n      if not hasattr(node, ""is_function_for_call""):\n        new_text = full_name + ""()""\n        self._file_edit.add(""Changed %r to %r""%(full_name, new_text),\n                            node.lineno, node.col_offset, full_name, new_text)\n\n    ast.NodeVisitor.generic_visit(self, node)\n\n\nclass TensorFlowCodeUpgrader(object):\n  """"""Class that handles upgrading a set of Python files to TensorFlow 1.0.""""""\n\n  def __init__(self):\n    pass\n\n  def process_file(self, in_filename, out_filename):\n    """"""Process the given python file for incompatible changes.\n    Args:\n      in_filename: filename to parse\n      out_filename: output file to write to\n    Returns:\n      A tuple representing number of files processed, log of actions, errors\n    """"""\n\n    # Write to a temporary file, just in case we are doing an implace modify.\n    with open(in_filename, ""r"") as in_file, \\\n        tempfile.NamedTemporaryFile(""w"", delete=False) as temp_file:\n      ret = self.process_opened_file(\n          in_filename, in_file, out_filename, temp_file)\n\n    shutil.move(temp_file.name, out_filename)\n    return ret\n\n  # Broad exceptions are required here because ast throws whatever it wants.\n  # pylint: disable=broad-except\n  def process_opened_file(self, in_filename, in_file, out_filename, out_file):\n    """"""Process the given python file for incompatible changes.\n    This function is split out to facilitate StringIO testing from\n    tf_upgrade_test.py.\n    Args:\n      in_filename: filename to parse\n      in_file: opened file (or StringIO)\n      out_filename: output file to write to\n      out_file: opened file (or StringIO)\n    Returns:\n      A tuple representing number of files processed, log of actions, errors\n    """"""\n    process_errors = []\n    text = ""-"" * 80 + ""\\n""\n    text += ""Processing file %r\\n outputting to %r\\n"" % (in_filename,\n                                                         out_filename)\n    text += ""-"" * 80 + ""\\n\\n""\n\n    parsed_ast = None\n    lines = in_file.readlines()\n    try:\n      parsed_ast = ast.parse("""".join(lines))\n    except Exception:\n      text += ""Failed to parse %r\\n\\n"" % in_filename\n      text += traceback.format_exc()\n    if parsed_ast:\n      visitor = TensorFlowCallVisitor(in_filename, lines)\n      visitor.visit(parsed_ast)\n      out_text, new_text, process_errors = visitor.process(lines)\n      text += new_text\n      if out_file:\n        out_file.write(out_text)\n    text += ""\\n""\n    return 1, text, process_errors\n  # pylint: enable=broad-except\n\n  def process_tree(self, root_directory, output_root_directory):\n    """"""Processes upgrades on an entire tree of python files in place.\n    Note that only Python files. If you have custom code in other languages,\n    you will need to manually upgrade those.\n    Args:\n      root_directory: Directory to walk and process.\n      output_root_directory: Directory to use as base\n    Returns:\n      A tuple of files processed, the report string ofr all files, and errors\n    """"""\n\n    # make sure output directory doesn\'t exist\n    if output_root_directory and os.path.exists(output_root_directory):\n      print(""Output directory %r must not already exist."" % (\n          output_root_directory))\n      sys.exit(1)\n\n    # make sure output directory does not overlap with root_directory\n    norm_root = os.path.split(os.path.normpath(root_directory))\n    norm_output = os.path.split(os.path.normpath(output_root_directory))\n    if norm_root == norm_output:\n      print(""Output directory %r same as input directory %r"" % (\n          root_directory, output_root_directory))\n      sys.exit(1)\n\n    # Collect list of files to process (we do this to correctly handle if the\n    # user puts the output directory in some sub directory of the input dir)\n    files_to_process = []\n    for dir_name, _, file_list in os.walk(root_directory):\n      py_files = [f for f in file_list if f.endswith("".py"")]\n      for filename in py_files:\n        fullpath = os.path.join(dir_name, filename)\n        fullpath_output = os.path.join(\n            output_root_directory, os.path.relpath(fullpath, root_directory))\n        files_to_process.append((fullpath, fullpath_output))\n\n    file_count = 0\n    tree_errors = []\n    report = """"\n    report += (""="" * 80) + ""\\n""\n    report += ""Input tree: %r\\n"" % root_directory\n    report += (""="" * 80) + ""\\n""\n\n    for input_path, output_path in files_to_process:\n      output_directory = os.path.dirname(output_path)\n      if not os.path.isdir(output_directory):\n        os.makedirs(output_directory)\n      file_count += 1\n      _, l_report, l_errors = self.process_file(input_path, output_path)\n      tree_errors += l_errors\n      report += l_report\n    return file_count, report, tree_errors\n\n\nif __name__ == ""__main__"":\n  parser = argparse.ArgumentParser(\n      formatter_class=argparse.RawDescriptionHelpFormatter,\n      description=""""""Convert a TensorFlow Python file to 1.0\nSimple usage:\n  tf_convert.py --infile foo.py --outfile bar.py\n  tf_convert.py --intree ~/code/old --outtree ~/code/new\n"""""")\n  parser.add_argument(\n      ""--infile"",\n      dest=""input_file"",\n      help=""If converting a single file, the name of the file ""\n      ""to convert"")\n  parser.add_argument(\n      ""--outfile"",\n      dest=""output_file"",\n      help=""If converting a single file, the output filename."")\n  parser.add_argument(\n      ""--intree"",\n      dest=""input_tree"",\n      help=""If converting a whole tree of files, the directory ""\n      ""to read from (relative or absolute)."")\n  parser.add_argument(\n      ""--outtree"",\n      dest=""output_tree"",\n      help=""If converting a whole tree of files, the output ""\n      ""directory (relative or absolute)."")\n  parser.add_argument(\n      ""--reportfile"",\n      dest=""report_filename"",\n      help=(""The name of the file where the report log is ""\n            ""stored.""\n            ""(default: %(default)s)""),\n      default=""report.txt"")\n  args = parser.parse_args()\n\n  upgrade = TensorFlowCodeUpgrader()\n  report_text = None\n  report_filename = args.report_filename\n  files_processed = 0\n  if args.input_file:\n    files_processed, report_text, errors = upgrade.process_file(\n        args.input_file, args.output_file)\n    files_processed = 1\n  elif args.input_tree:\n    files_processed, report_text, errors = upgrade.process_tree(\n        args.input_tree, args.output_tree)\n  else:\n    parser.print_help()\n  if report_text:\n    open(report_filename, ""w"").write(report_text)\n    print(""TensorFlow 1.0 Upgrade Script"")\n    print(""-----------------------------"")\n    print(""Converted %d files\\n"" % files_processed)\n    print(""Detected %d errors that require attention"" % len(errors))\n    print(""-"" * 80)\n    print(""\\n"".join(errors))\n    print(""\\nMake sure to read the detailed log %r\\n"" % report_filename)'"
deepcolor/utils.py,22,"b'import tensorflow as tf\nimport numpy as np\nimport cv2\n\nclass batch_norm(object):\n            # h1 = lrelu(tf.contrib.layers.batch_norm(conv2d(h0, self.df_dim*2, name=\'d_h1_conv\'),decay=0.9,updates_collections=None,epsilon=0.00001,scale=True,scope=""d_h1_conv""))\n    def __init__(self, epsilon=1e-5, momentum = 0.9, name=""batch_norm""):\n        with tf.variable_scope(name):\n            self.epsilon = epsilon\n            self.momentum = momentum\n            self.name = name\n\n    def __call__(self, x, train=True):\n        return tf.contrib.layers.batch_norm(x, decay=self.momentum, updates_collections=None, epsilon=self.epsilon, scale=True, scope=self.name)\n\nbatchnorm_count = 0\ndef bn(x):\n    global batchnorm_count\n    batch_object = batch_norm(name=(""bn"" + str(batchnorm_count)))\n    batchnorm_count += 1\n    return batch_object(x)\n\ndef conv2d(input_, output_dim,\n           k_h=5, k_w=5, d_h=2, d_w=2, stddev=0.02,\n           name=""conv2d""):\n    with tf.variable_scope(name):\n        w = tf.get_variable(\'w\', [k_h, k_w, input_.get_shape()[-1], output_dim],\n                            initializer=tf.truncated_normal_initializer(stddev=stddev))\n        conv = tf.nn.conv2d(input_, w, strides=[1, d_h, d_w, 1], padding=\'SAME\')\n\n        biases = tf.get_variable(\'biases\', [output_dim], initializer=tf.constant_initializer(0.0))\n        conv = tf.reshape(tf.nn.bias_add(conv, biases), conv.get_shape())\n\n        return conv\n\ndef deconv2d(input_, output_shape,\n             k_h=5, k_w=5, d_h=2, d_w=2, stddev=0.02,\n             name=""deconv2d"", with_w=False):\n    with tf.variable_scope(name):\n        # filter : [height, width, output_channels, in_channels]\n        w = tf.get_variable(\'w\', [k_h, k_w, output_shape[-1], input_.get_shape()[-1]], initializer=tf.random_normal_initializer(stddev=stddev))\n        deconv = tf.nn.conv2d_transpose(input_, w, output_shape=output_shape, strides=[1, d_h, d_w, 1])\n        biases = tf.get_variable(\'biases\', [output_shape[-1]], initializer=tf.constant_initializer(0.0))\n        deconv = tf.reshape(tf.nn.bias_add(deconv, biases), deconv.get_shape())\n        if with_w:\n            return deconv, w, biases\n        else:\n            return deconv\n\n\ndef lrelu(x, leak=0.2, name=""lrelu""):\n  return tf.maximum(x, leak*x)\n\ndef linear(input_, output_size, scope=None, stddev=0.02, bias_start=0.0, with_w=False):\n    shape = input_.get_shape().as_list()\n    with tf.variable_scope(scope or ""Linear""):\n        matrix = tf.get_variable(""Matrix"", [shape[1], output_size], tf.float32,\n                                 tf.random_normal_initializer(stddev=stddev))\n        bias = tf.get_variable(""bias"", [output_size],\n            initializer=tf.constant_initializer(bias_start))\n        if with_w:\n            return tf.matmul(input_, matrix) + bias, matrix, bias\n        else:\n            return tf.matmul(input_, matrix) + bias\n\ndef get_image(image_path):\n    return transform(imread(image_path))\n\ndef transform(image, npx=512, is_crop=True):\n    cropped_image = cv2.resize(image, (256,256))\n\n    return np.array(cropped_image)\n\ndef imread(path):\n    readimage = cv2.imread(path, 1)\n    return readimage\n\ndef merge_color(images, size):\n    h, w = images.shape[1], images.shape[2]\n    img = np.zeros((h * size[0], w * size[1], 3))\n\n    for idx, image in enumerate(images):\n        i = idx % size[1]\n        j = idx / size[1]\n        img[j*h:j*h+h, i*w:i*w+w, :] = image\n\n    return img\n\ndef merge(images, size):\n    h, w = images.shape[1], images.shape[2]\n    img = np.zeros((h * size[0], w * size[1], 1))\n\n    for idx, image in enumerate(images):\n        i = idx % size[1]\n        j = idx / size[1]\n        img[j*h:j*h+h, i*w:i*w+w] = image\n\n    return img[:,:,0]\n\ndef ims(name, img):\n    print ""saving img "" + name\n    cv2.imwrite(name, img*255)\n'"
finetuning/eval_image_classifier.py,30,"b'# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Generic evaluation script that evaluates a model using a given dataset.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport math\nimport tensorflow as tf\n\nfrom datasets import dataset_factory\nfrom nets import nets_factory\nfrom preprocessing import preprocessing_factory\n\nslim = tf.contrib.slim\n\ntf.app.flags.DEFINE_integer(\'batch_size\', 100,\n                            \'The number of samples in each batch.\')\n\ntf.app.flags.DEFINE_integer(\n    \'max_num_batches\', None,\n    \'Max number of batches to evaluate by default use all.\')\n\ntf.app.flags.DEFINE_string(\'master\', \'\',\n                           \'The address of the TensorFlow master to use.\')\n\ntf.app.flags.DEFINE_string(\n    \'checkpoint_path\', \'/tmp/tfmodel/\',\n    \'The directory where the model was written to or an absolute path to a \'\n    \'checkpoint file.\')\n\ntf.app.flags.DEFINE_string(\'eval_dir\', \'/tmp/tfmodel/\',\n                           \'Directory where the results are saved to.\')\n\ntf.app.flags.DEFINE_integer(\n    \'num_preprocessing_threads\', 4,\n    \'The number of threads used to create the batches.\')\n\ntf.app.flags.DEFINE_string(\'dataset_name\', \'imagenet\',\n                           \'The name of the dataset to load.\')\n\ntf.app.flags.DEFINE_string(\'dataset_split_name\', \'test\',\n                           \'The name of the train/test split.\')\n\ntf.app.flags.DEFINE_string(\'dataset_dir\', None,\n                           \'The directory where the dataset files are stored.\')\n\ntf.app.flags.DEFINE_integer(\n    \'labels_offset\', 0,\n    \'An offset for the labels in the dataset. This flag is primarily used to \'\n    \'evaluate the VGG and ResNet architectures which do not use a background \'\n    \'class for the ImageNet dataset.\')\n\ntf.app.flags.DEFINE_string(\'model_name\', \'inception_v3\',\n                           \'The name of the architecture to evaluate.\')\n\ntf.app.flags.DEFINE_string(\'preprocessing_name\', None,\n                           \'The name of the preprocessing to use. If left \'\n                           \'as `None`, then the model_name flag is used.\')\n\ntf.app.flags.DEFINE_float(\n    \'moving_average_decay\', None, \'The decay to use for the moving average.\'\n    \'If left as None, then moving averages are not used.\')\n\ntf.app.flags.DEFINE_integer(\'eval_image_size\', None, \'Eval image size\')\n\nFLAGS = tf.app.flags.FLAGS\n\n\ndef main(_):\n    if not FLAGS.dataset_dir:\n        raise ValueError(\n            \'You must supply the dataset directory with --dataset_dir\')\n\n    tf.logging.set_verbosity(tf.logging.INFO)\n    with tf.Graph().as_default():\n        tf_global_step = slim.get_or_create_global_step()\n\n        ######################\n        # Select the dataset #\n        ######################\n        dataset = dataset_factory.get_dataset(\n            FLAGS.dataset_name, FLAGS.dataset_split_name, FLAGS.dataset_dir)\n\n        ####################\n        # Select the model #\n        ####################\n        network_fn = nets_factory.get_network_fn(\n            FLAGS.model_name,\n            num_classes=(dataset.num_classes - FLAGS.labels_offset),\n            is_training=False)\n\n        ##############################################################\n        # Create a dataset provider that loads data from the dataset #\n        ##############################################################\n        provider = slim.dataset_data_provider.DatasetDataProvider(\n            dataset,\n            shuffle=False,\n            common_queue_capacity=2 * FLAGS.batch_size,\n            common_queue_min=FLAGS.batch_size)\n        [image, label] = provider.get([\'image\', \'label\'])\n        label -= FLAGS.labels_offset\n\n        #####################################\n        # Select the preprocessing function #\n        #####################################\n        preprocessing_name = FLAGS.preprocessing_name or FLAGS.model_name\n        image_preprocessing_fn = preprocessing_factory.get_preprocessing(\n            preprocessing_name, is_training=False)\n\n        eval_image_size = FLAGS.eval_image_size or network_fn.default_image_size\n\n        image = image_preprocessing_fn(image, eval_image_size, eval_image_size)\n\n        images, labels = tf.train.batch(\n            [image, label],\n            batch_size=FLAGS.batch_size,\n            num_threads=FLAGS.num_preprocessing_threads,\n            capacity=5 * FLAGS.batch_size)\n\n        ####################\n        # Define the model #\n        ####################\n        logits, _ = network_fn(images)\n\n        if FLAGS.moving_average_decay:\n            variable_averages = tf.train.ExponentialMovingAverage(\n                FLAGS.moving_average_decay, tf_global_step)\n            variables_to_restore = variable_averages.variables_to_restore(\n                slim.get_model_variables())\n            variables_to_restore[tf_global_step.op.name] = tf_global_step\n        else:\n            variables_to_restore = slim.get_variables_to_restore()\n\n        predictions = tf.argmax(logits, 1)\n        labels = tf.squeeze(labels)\n        # label = tf.reshape(labels, [-1, -1, 1])\n        # Define the metrics:\n        names_to_values, names_to_updates = slim.metrics.aggregate_metric_map({\n            \'Accuracy\': slim.metrics.streaming_accuracy(predictions, labels),\n            # \'Recall@5\': slim.metrics.streaming_recall_at_k(logits, labels, 2),\n            # \'Recall@5\':\n            # slim.metrics.streaming_sparse_recall_at_k(logits, labels, 5),\n        })\n\n        # Print the summaries to screen.\n        for name, value in names_to_values.iteritems():\n            summary_name = \'eval/%s\' % name\n            op = tf.summary.scalar(summary_name, value, collections=[])\n            op = tf.Print(op, [value], summary_name)\n            tf.add_to_collection(tf.GraphKeys.SUMMARIES, op)\n\n        # TODO(sguada) use num_epochs=1\n        if FLAGS.max_num_batches:\n            num_batches = FLAGS.max_num_batches\n        else:\n            # This ensures that we make a single pass over all of the data.\n            num_batches = math.ceil(dataset.num_samples /\n                                    float(FLAGS.batch_size))\n\n        if tf.gfile.IsDirectory(FLAGS.checkpoint_path):\n            checkpoint_path = tf.train.latest_checkpoint(FLAGS.checkpoint_path)\n        else:\n            checkpoint_path = FLAGS.checkpoint_path\n\n        tf.logging.info(\'Evaluating %s\' % checkpoint_path)\n\n        slim.evaluation.evaluate_once(\n            master=FLAGS.master,\n            checkpoint_path=checkpoint_path,\n            logdir=FLAGS.eval_dir,\n            num_evals=num_batches,\n            eval_op=names_to_updates.values(),\n            variables_to_restore=variables_to_restore)\n\n\nif __name__ == \'__main__\':\n    tf.app.run()\n'"
finetuning/train_image_classifier.py,103,"b'# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Generic training script that trains a model using a given dataset.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\n\nfrom tensorflow.python.ops import control_flow_ops\n# from datasets import dataset_factory\nfrom deployment import model_deploy\nfrom nets import nets_factory\nfrom preprocessing import preprocessing_factory\nfrom datasets import dataset_factory\n\nslim = tf.contrib.slim\n\ntf.app.flags.DEFINE_string(\'master\', \'\',\n                           \'The address of the TensorFlow master to use.\')\n\ntf.app.flags.DEFINE_string(\n    \'train_dir\', \'/tmp/tfmodel/\',\n    \'Directory where checkpoints and event logs are written to.\')\n\ntf.app.flags.DEFINE_integer(\'num_clones\', 1,\n                            \'Number of model clones to deploy.\')\n\ntf.app.flags.DEFINE_boolean(\'clone_on_cpu\', False,\n                            \'Use CPUs to deploy clones.\')\n\ntf.app.flags.DEFINE_integer(\'worker_replicas\', 1, \'Number of worker replicas.\')\n\ntf.app.flags.DEFINE_integer(\n    \'num_ps_tasks\', 0,\n    \'The number of parameter servers. If the value is 0, then the parameters \'\n    \'are handled locally by the worker.\')\n\ntf.app.flags.DEFINE_integer(\n    \'num_readers\', 4,\n    \'The number of parallel readers that read data from the dataset.\')\n\ntf.app.flags.DEFINE_integer(\n    \'num_preprocessing_threads\', 4,\n    \'The number of threads used to create the batches.\')\n\ntf.app.flags.DEFINE_integer(\'log_every_n_steps\', 10,\n                            \'The frequency with which logs are print.\')\n\ntf.app.flags.DEFINE_integer(\n    \'save_summaries_secs\', 600,\n    \'The frequency with which summaries are saved, in seconds.\')\n\ntf.app.flags.DEFINE_integer(\n    \'save_interval_secs\', 600,\n    \'The frequency with which the model is saved, in seconds.\')\n\ntf.app.flags.DEFINE_integer(\'task\', 0,\n                            \'Task id of the replica running the training.\')\n\n######################\n# Optimization Flags #\n######################\n\ntf.app.flags.DEFINE_float(\'weight_decay\', 0.00004,\n                          \'The weight decay on the model weights.\')\n\ntf.app.flags.DEFINE_string(\n    \'optimizer\', \'rmsprop\',\n    \'The name of the optimizer, one of ""adadelta"", ""adagrad"", ""adam"",\'\n    \'""ftrl"", ""momentum"", ""sgd"" or ""rmsprop"".\')\n\ntf.app.flags.DEFINE_float(\'adadelta_rho\', 0.95, \'The decay rate for adadelta.\')\n\ntf.app.flags.DEFINE_float(\'adagrad_initial_accumulator_value\', 0.1,\n                          \'Starting value for the AdaGrad accumulators.\')\n\ntf.app.flags.DEFINE_float(\n    \'adam_beta1\', 0.9,\n    \'The exponential decay rate for the 1st moment estimates.\')\n\ntf.app.flags.DEFINE_float(\n    \'adam_beta2\', 0.999,\n    \'The exponential decay rate for the 2nd moment estimates.\')\n\ntf.app.flags.DEFINE_float(\'opt_epsilon\', 1.0,\n                          \'Epsilon term for the optimizer.\')\n\ntf.app.flags.DEFINE_float(\'ftrl_learning_rate_power\', -0.5,\n                          \'The learning rate power.\')\n\ntf.app.flags.DEFINE_float(\'ftrl_initial_accumulator_value\', 0.1,\n                          \'Starting value for the FTRL accumulators.\')\n\ntf.app.flags.DEFINE_float(\'ftrl_l1\', 0.0,\n                          \'The FTRL l1 regularization strength.\')\n\ntf.app.flags.DEFINE_float(\'ftrl_l2\', 0.0,\n                          \'The FTRL l2 regularization strength.\')\n\ntf.app.flags.DEFINE_float(\n    \'momentum\', 0.9,\n    \'The momentum for the MomentumOptimizer and RMSPropOptimizer.\')\n\ntf.app.flags.DEFINE_float(\'rmsprop_momentum\', 0.9, \'Momentum.\')\n\ntf.app.flags.DEFINE_float(\'rmsprop_decay\', 0.9, \'Decay term for RMSProp.\')\n\n#######################\n# Learning Rate Flags #\n#######################\n\ntf.app.flags.DEFINE_string(\n    \'learning_rate_decay_type\', \'exponential\',\n    \'Specifies how the learning rate is decayed. One of ""fixed"", ""exponential"",\'\n    \' or ""polynomial""\')\n\ntf.app.flags.DEFINE_float(\'learning_rate\', 0.01, \'Initial learning rate.\')\n\ntf.app.flags.DEFINE_float(\n    \'end_learning_rate\', 0.0001,\n    \'The minimal end learning rate used by a polynomial decay learning rate.\')\n\ntf.app.flags.DEFINE_float(\'label_smoothing\', 0.0,\n                          \'The amount of label smoothing.\')\n\ntf.app.flags.DEFINE_float(\'learning_rate_decay_factor\', 0.94,\n                          \'Learning rate decay factor.\')\n\ntf.app.flags.DEFINE_float(\'num_epochs_per_decay\', 2.0,\n                          \'Number of epochs after which learning rate decays.\')\n\ntf.app.flags.DEFINE_bool(\n    \'sync_replicas\', False,\n    \'Whether or not to synchronize the replicas during training.\')\n\ntf.app.flags.DEFINE_integer(\n    \'replicas_to_aggregate\', 1,\n    \'The Number of gradients to collect before updating params.\')\n\ntf.app.flags.DEFINE_float(\n    \'moving_average_decay\', None, \'The decay to use for the moving average.\'\n    \'If left as None, then moving averages are not used.\')\n\n#######################\n# Dataset Flags #\n#######################\n\ntf.app.flags.DEFINE_string(\'dataset_name\', \'imagenet\',\n                           \'The name of the dataset to load.\')\n\ntf.app.flags.DEFINE_string(\'dataset_split_name\', \'train\',\n                           \'The name of the train/test split.\')\n\ntf.app.flags.DEFINE_string(\'dataset_dir\', None,\n                           \'The directory where the dataset files are stored.\')\n\ntf.app.flags.DEFINE_integer(\n    \'labels_offset\', 0,\n    \'An offset for the labels in the dataset. This flag is primarily used to \'\n    \'evaluate the VGG and ResNet architectures which do not use a background \'\n    \'class for the ImageNet dataset.\')\n\ntf.app.flags.DEFINE_string(\'model_name\', \'inception_v3\',\n                           \'The name of the architecture to train.\')\n\ntf.app.flags.DEFINE_string(\'preprocessing_name\', None,\n                           \'The name of the preprocessing to use. If left \'\n                           \'as `None`, then the model_name flag is used.\')\n\ntf.app.flags.DEFINE_integer(\'batch_size\', 32,\n                            \'The number of samples in each batch.\')\n\ntf.app.flags.DEFINE_integer(\'train_image_size\', None, \'Train image size\')\n\ntf.app.flags.DEFINE_integer(\'max_number_of_steps\', None,\n                            \'The maximum number of training steps.\')\n\n#####################\n# Fine-Tuning Flags #\n#####################\n\ntf.app.flags.DEFINE_string(\'checkpoint_path\', None,\n                           \'The path to a checkpoint from which to fine-tune.\')\n\ntf.app.flags.DEFINE_string(\n    \'checkpoint_exclude_scopes\', None,\n    \'Comma-separated list of scopes of variables to exclude when restoring \'\n    \'from a checkpoint.\')\n\ntf.app.flags.DEFINE_string(\n    \'trainable_scopes\', None,\n    \'Comma-separated list of scopes to filter the set of variables to train.\'\n    \'By default, None would train all the variables.\')\n\ntf.app.flags.DEFINE_boolean(\n    \'ignore_missing_vars\', False,\n    \'When restoring a checkpoint would ignore missing variables.\')\n\nFLAGS = tf.app.flags.FLAGS\n\n\ndef _configure_learning_rate(num_samples_per_epoch, global_step):\n    """"""Configures the learning rate.\n\n  Args:\n    num_samples_per_epoch: The number of samples in each epoch of training.\n    global_step: The global_step tensor.\n\n  Returns:\n    A `Tensor` representing the learning rate.\n\n  Raises:\n    ValueError: if\n  """"""\n    decay_steps = int(num_samples_per_epoch / FLAGS.batch_size *\n                      FLAGS.num_epochs_per_decay)\n    if FLAGS.sync_replicas:\n        decay_steps /= FLAGS.replicas_to_aggregate\n\n    if FLAGS.learning_rate_decay_type == \'exponential\':\n        return tf.train.exponential_decay(\n            FLAGS.learning_rate,\n            global_step,\n            decay_steps,\n            FLAGS.learning_rate_decay_factor,\n            staircase=True,\n            name=\'exponential_decay_learning_rate\')\n    elif FLAGS.learning_rate_decay_type == \'fixed\':\n        return tf.constant(FLAGS.learning_rate, name=\'fixed_learning_rate\')\n    elif FLAGS.learning_rate_decay_type == \'polynomial\':\n        return tf.train.polynomial_decay(\n            FLAGS.learning_rate,\n            global_step,\n            decay_steps,\n            FLAGS.end_learning_rate,\n            power=1.0,\n            cycle=False,\n            name=\'polynomial_decay_learning_rate\')\n    else:\n        raise ValueError(\'learning_rate_decay_type [%s] was not recognized\',\n                         FLAGS.learning_rate_decay_type)\n\n\ndef _configure_optimizer(learning_rate):\n    """"""Configures the optimizer used for training.\n\n  Args:\n    learning_rate: A scalar or `Tensor` learning rate.\n\n  Returns:\n    An instance of an optimizer.\n\n  Raises:\n    ValueError: if FLAGS.optimizer is not recognized.\n  """"""\n    if FLAGS.optimizer == \'adadelta\':\n        optimizer = tf.train.AdadeltaOptimizer(\n            learning_rate, rho=FLAGS.adadelta_rho, epsilon=FLAGS.opt_epsilon)\n    elif FLAGS.optimizer == \'adagrad\':\n        optimizer = tf.train.AdagradOptimizer(\n            learning_rate,\n            initial_accumulator_value=FLAGS.adagrad_initial_accumulator_value)\n    elif FLAGS.optimizer == \'adam\':\n        optimizer = tf.train.AdamOptimizer(\n            learning_rate,\n            beta1=FLAGS.adam_beta1,\n            beta2=FLAGS.adam_beta2,\n            epsilon=FLAGS.opt_epsilon)\n    elif FLAGS.optimizer == \'ftrl\':\n        optimizer = tf.train.FtrlOptimizer(\n            learning_rate,\n            learning_rate_power=FLAGS.ftrl_learning_rate_power,\n            initial_accumulator_value=FLAGS.ftrl_initial_accumulator_value,\n            l1_regularization_strength=FLAGS.ftrl_l1,\n            l2_regularization_strength=FLAGS.ftrl_l2)\n    elif FLAGS.optimizer == \'momentum\':\n        optimizer = tf.train.MomentumOptimizer(\n            learning_rate, momentum=FLAGS.momentum, name=\'Momentum\')\n    elif FLAGS.optimizer == \'rmsprop\':\n        optimizer = tf.train.RMSPropOptimizer(\n            learning_rate,\n            decay=FLAGS.rmsprop_decay,\n            momentum=FLAGS.rmsprop_momentum,\n            epsilon=FLAGS.opt_epsilon)\n    elif FLAGS.optimizer == \'sgd\':\n        optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n    else:\n        raise ValueError(\'Optimizer [%s] was not recognized\', FLAGS.optimizer)\n    return optimizer\n\n\ndef _add_variables_summaries(learning_rate):\n    summaries = []\n    for variable in slim.get_model_variables():\n        try:\n            summaries.append(tf.summary.histogram(variable.op.name, variable))\n            summaries.append(\n                tf.summary.scalar(\'training/Learning Rate\', learning_rate))\n        except AttributeError:\n            summaries.append(tf.histogram_summary(variable.op.name, variable))\n            summaries.append(\n                tf.scalar_summary(\'training/Learning Rate\', learning_rate))\n    return summaries\n\n\ndef _get_init_fn():\n    """"""Returns a function run by the chief worker to warm-start the training.\n\n  Note that the init_fn is only run when initializing the model during the very\n  first global step.\n\n  Returns:\n    An init function run by the supervisor.\n  """"""\n    if FLAGS.checkpoint_path is None:\n        return None\n\n    # Warn the user if a checkpoint exists in the train_dir. Then we\'ll be\n    # ignoring the checkpoint anyway.\n    if tf.train.latest_checkpoint(FLAGS.train_dir):\n        tf.logging.info(\n            \'Ignoring --checkpoint_path because a checkpoint already exists in %s\'\n            % FLAGS.train_dir)\n        return None\n\n    exclusions = []\n    if FLAGS.checkpoint_exclude_scopes:\n        exclusions = [\n            scope.strip()\n            for scope in FLAGS.checkpoint_exclude_scopes.split(\',\')\n        ]\n\n    # TODO(sguada) variables.filter_variables()\n    variables_to_restore = []\n    for var in slim.get_model_variables():\n        excluded = False\n        for exclusion in exclusions:\n            if var.op.name.startswith(exclusion):\n                excluded = True\n                break\n        if not excluded:\n            variables_to_restore.append(var)\n\n    if tf.gfile.IsDirectory(FLAGS.checkpoint_path):\n        checkpoint_path = tf.train.latest_checkpoint(FLAGS.checkpoint_path)\n    else:\n        checkpoint_path = FLAGS.checkpoint_path\n\n    tf.logging.info(\'Fine-tuning from %s\' % checkpoint_path)\n\n    return slim.assign_from_checkpoint_fn(\n        checkpoint_path,\n        variables_to_restore,\n        ignore_missing_vars=FLAGS.ignore_missing_vars)\n\n\ndef _get_variables_to_train():\n    """"""Returns a list of variables to train.\n\n  Returns:\n    A list of variables to train by the optimizer.\n  """"""\n    if FLAGS.trainable_scopes is None:\n        return tf.trainable_variables()\n    else:\n        scopes = [scope.strip() for scope in FLAGS.trainable_scopes.split(\',\')]\n\n    variables_to_train = []\n    for scope in scopes:\n        variables = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope)\n        variables_to_train.extend(variables)\n    return variables_to_train\n\n\ndef main(_):\n    if not FLAGS.dataset_dir:\n        raise ValueError(\n            \'You must supply the dataset directory with --dataset_dir\')\n\n    tf.logging.set_verbosity(tf.logging.INFO)\n    with tf.Graph().as_default():\n        ######################\n        # Config model_deploy#\n        ######################\n        deploy_config = model_deploy.DeploymentConfig(\n            num_clones=FLAGS.num_clones,\n            clone_on_cpu=FLAGS.clone_on_cpu,\n            replica_id=FLAGS.task,\n            num_replicas=FLAGS.worker_replicas,\n            num_ps_tasks=FLAGS.num_ps_tasks)\n\n        # Create global_step\n        with tf.device(deploy_config.variables_device()):\n            global_step = slim.create_global_step()\n\n        ######################\n        # Select the dataset #\n        ######################\n        print(FLAGS.dataset_name)\n        dataset = dataset_factory.get_dataset(\n            FLAGS.dataset_name, FLAGS.dataset_split_name, FLAGS.dataset_dir)\n\n        ####################\n        # Select the network #\n        ####################\n        network_fn = nets_factory.get_network_fn(\n            FLAGS.model_name,\n            num_classes=(dataset.num_classes - FLAGS.labels_offset),\n            weight_decay=FLAGS.weight_decay,\n            is_training=True)\n\n        #####################################\n        # Select the preprocessing function #\n        #####################################\n        preprocessing_name = FLAGS.preprocessing_name or FLAGS.model_name\n        image_preprocessing_fn = preprocessing_factory.get_preprocessing(\n            preprocessing_name, is_training=True)\n\n        ##############################################################\n        # Create a dataset provider that loads data from the dataset #\n        ##############################################################\n        with tf.device(deploy_config.inputs_device()):\n            provider = slim.dataset_data_provider.DatasetDataProvider(\n                dataset,\n                num_readers=FLAGS.num_readers,\n                common_queue_capacity=20 * FLAGS.batch_size,\n                common_queue_min=10 * FLAGS.batch_size)\n            [image, label] = provider.get([\'image\', \'label\'])\n            label -= FLAGS.labels_offset\n\n            train_image_size = FLAGS.train_image_size or network_fn.default_image_size\n\n            image = image_preprocessing_fn(image, train_image_size,\n                                           train_image_size)\n\n            images, labels = tf.train.batch(\n                [image, label],\n                batch_size=FLAGS.batch_size,\n                num_threads=FLAGS.num_preprocessing_threads,\n                capacity=5 * FLAGS.batch_size)\n            labels = slim.one_hot_encoding(\n                labels, dataset.num_classes - FLAGS.labels_offset)\n            batch_queue = slim.prefetch_queue.prefetch_queue(\n                [images, labels], capacity=2 * deploy_config.num_clones)\n\n        ####################\n        # Define the model #\n        ####################\n        def clone_fn(batch_queue):\n            """"""Allows data parallelism by creating multiple clones of network_fn.""""""\n            images, labels = batch_queue.dequeue()\n            logits, end_points = network_fn(images)\n\n            #############################\n            # Specify the loss function #\n            #############################\n            if \'AuxLogits\' in end_points:\n                slim.losses.softmax_cross_entropy(\n                    end_points[\'AuxLogits\'],\n                    labels,\n                    label_smoothing=FLAGS.label_smoothing,\n                    weight=0.4,\n                    scope=\'aux_loss\')\n            slim.losses.softmax_cross_entropy(\n                logits,\n                labels,\n                label_smoothing=FLAGS.label_smoothing,\n                weight=1.0)\n            return end_points\n\n        # Gather initial summaries.\n        summaries = set(tf.get_collection(tf.GraphKeys.SUMMARIES))\n\n        clones = model_deploy.create_clones(deploy_config, clone_fn,\n                                            [batch_queue])\n        first_clone_scope = deploy_config.clone_scope(0)\n        # Gather update_ops from the first clone. These contain, for example,\n        # the updates for the batch_norm variables created by network_fn.\n        update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS,\n                                       first_clone_scope)\n\n        # Add summaries for end_points.\n        end_points = clones[0].outputs\n        for end_point in end_points:\n            x = end_points[end_point]\n            try:\n                summaries.add(tf.summary.histogram(\'activations/\' + end_point, x))\n                summaries.add(\n                    tf.summary.scalar(\'sparsity/\' + end_point,\n                                    tf.nn.zero_fraction(x)))\n            except AttributeError:\n                summaries.add(tf.histogram_summary(\'activations/\' + end_point, x))\n                summaries.add(\n                    tf.scalar_summary(\'sparsity/\' + end_point,\n                                    tf.nn.zero_fraction(x)))\n\n                \n\n        # Add summaries for losses.\n        for loss in tf.get_collection(tf.GraphKeys.LOSSES, first_clone_scope):\n            try:\n                summaries.add(tf.summary.scalar(\'losses/%s\' % loss.op.name, loss))\n            except:\n                summaries.add(tf.scalar_summary(\'losses/%s\' % loss.op.name, loss))\n\n        # Add summaries for variables.\n        for variable in slim.get_model_variables():\n            try:\n                summaries.add(tf.summary.histogram(variable.op.name, variable))\n            except AttributeError:\n                summaries.add(tf.histogram_summary(variable.op.name, variable))\n\n        #################################\n        # Configure the moving averages #\n        #################################\n        if FLAGS.moving_average_decay:\n            moving_average_variables = slim.get_model_variables()\n            variable_averages = tf.train.ExponentialMovingAverage(\n                FLAGS.moving_average_decay, global_step)\n        else:\n            moving_average_variables, variable_averages = None, None\n\n        #########################################\n        # Configure the optimization procedure. #\n        #########################################\n        with tf.device(deploy_config.optimizer_device()):\n            learning_rate = _configure_learning_rate(dataset.num_samples,\n                                                     global_step)\n            optimizer = _configure_optimizer(learning_rate)\n            try:\n                summaries.add(\n                    tf.summary.scalar(\n                        \'learning_rate\', learning_rate, name=\'learning_rate\'))\n            except:\n                summaries.add(\n                    tf.scalar_summary(\n                        \'learning_rate\', learning_rate, name=\'learning_rate\'))\n\n        if FLAGS.sync_replicas:\n            # If sync_replicas is enabled, the averaging will be done in the chief\n            # queue runner.\n            optimizer = tf.train.SyncReplicasOptimizer(\n                opt=optimizer,\n                replicas_to_aggregate=FLAGS.replicas_to_aggregate,\n                variable_averages=variable_averages,\n                variables_to_average=moving_average_variables,\n                replica_id=tf.constant(\n                    FLAGS.task, tf.int32, shape=()),\n                total_num_replicas=FLAGS.worker_replicas)\n        elif FLAGS.moving_average_decay:\n            # Update ops executed locally by trainer.\n            update_ops.append(\n                variable_averages.apply(moving_average_variables))\n\n        # Variables to train.\n        variables_to_train = _get_variables_to_train()\n\n        #  and returns a train_tensor and summary_op\n        total_loss, clones_gradients = model_deploy.optimize_clones(\n            clones, optimizer, var_list=variables_to_train)\n        # Add total_loss to summary.\n        try:\n            summaries.add(\n                tf.summary.scalar(\n                    \'total_loss\', total_loss, name=\'total_loss\'))\n        except:\n            summaries.add(\n                tf.scalar_summary(\n                    \'total_loss\', total_loss, name=\'total_loss\'))\n\n        # Create gradient updates.\n        grad_updates = optimizer.apply_gradients(\n            clones_gradients, global_step=global_step)\n        update_ops.append(grad_updates)\n\n        update_op = tf.group(*update_ops)\n        train_tensor = control_flow_ops.with_dependencies(\n            [update_op], total_loss, name=\'train_op\')\n\n        # Add the summaries from the first clone. These contain the summaries\n        # created by model_fn and either optimize_clones() or _gather_clone_loss().\n        summaries |= set(\n            tf.get_collection(tf.GraphKeys.SUMMARIES, first_clone_scope))\n\n        # Merge all summaries together.\n        try:\n            summary_op = tf.summary.merge(list(summaries), name=\'summary_op\')\n        except AttributeError:\n            summary_op = tf.merge_summary(list(summaries), name=\'summary_op\')\n\n        ###########################\n        # Kicks off the training. #\n        ###########################\n        slim.learning.train(\n            train_tensor,\n            logdir=FLAGS.train_dir,\n            master=FLAGS.master,\n            is_chief=(FLAGS.task == 0),\n            init_fn=_get_init_fn(),\n            summary_op=summary_op,\n            number_of_steps=FLAGS.max_number_of_steps,\n            log_every_n_steps=FLAGS.log_every_n_steps,\n            save_summaries_secs=FLAGS.save_summaries_secs,\n            save_interval_secs=FLAGS.save_interval_secs,\n            sync_optimizer=optimizer if FLAGS.sync_replicas else None)\n\n\nif __name__ == \'__main__\':\n    tf.app.run()\n'"
serving/mnist_client.py,0,"b'#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n\nfrom __future__ import print_function\n\nimport grpc\nimport json\nimport numpy as np\nfrom tensorflow.python.framework import tensor_util\n\nimport predict_pb2\nfrom PIL import Image\n\n\ndef main():\n    # Connect with the gRPC server\n    server_address = ""127.0.0.1:50051""\n    request_timeout = 5.0\n    channel = grpc.insecure_channel(server_address)\n    stub = predict_pb2.PredictionServiceStub(channel)\n\n    # Make request data\n    request = predict_pb2.PredictRequest()\n    image = Image.open(\'../mnist_jpgs/4/pic_test1010.png\')\n    array = np.array(image)/(255*1.0)\n    samples_features =  array.reshape([-1,784])\n\n    # samples_features = np.array(\n    #     [[10, 10, 10, 8, 6, 1, 8, 9, 1], [10, 10, 10, 8, 6, 1, 8, 9, 1]])\n    samples_keys = np.array([1])\n    # Convert numpy to TensorProto\n    request.inputs[""features""].CopyFrom(tensor_util.make_tensor_proto(\n        samples_features))\n    request.inputs[""key""].CopyFrom(tensor_util.make_tensor_proto(samples_keys))\n\n    # Invoke gRPC request\n    response = stub.Predict(request, request_timeout)\n\n    # Convert TensorProto to numpy\n    result = {}\n    for k, v in response.outputs.items():\n        result[k] = tensor_util.MakeNdarray(v)\n    print(result)\n\n\nif __name__ == \'__main__\':\n    main()\n'"
serving/mnist_server.py,5,"b'#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n\nfrom concurrent import futures\nimport time\nimport json\nimport grpc\nimport numpy as np\nimport tensorflow as tf\nimport logging\nfrom tensorflow.python.framework import tensor_util\n\nimport predict_pb2\n\nlogging.basicConfig(level=logging.DEBUG)\n\n_ONE_DAY_IN_SECONDS = 60 * 60 * 24\n\n\nclass PredictionService(predict_pb2.PredictionServiceServicer):\n    def __init__(self, checkpoint_file, graph_file):\n        self.checkpoint_file = checkpoint_file\n        self.graph_file = graph_file\n        self.sess = None\n        self.inputs = None\n        self.outputs = None\n\n        self.init_session_handler()\n\n    def init_session_handler(self):\n        self.sess = tf.Session()\n\n        # Restore graph and weights from the model file\n        ckpt = tf.train.get_checkpoint_state(self.checkpoint_file)\n        if ckpt and ckpt.model_checkpoint_path:\n            logging.info(""Use the model: {}"".format(\n                ckpt.model_checkpoint_path))\n            saver = tf.train.import_meta_graph(self.graph_file)\n            saver.restore(self.sess, ckpt.model_checkpoint_path)\n\n            self.inputs = json.loads(tf.get_collection(\'input\')[0])\n            self.outputs = json.loads(tf.get_collection(\'output\')[0])\n        else:\n            logging.error(""No model found, exit"")\n            exit()\n\n    def Predict(self, request, context):\n        """"""Run predict op for each request.\n        \n        Args:\n          request: The TensorProto which contains the map of ""inputs"". The request.inputs looks like {\'features\': dtype: DT_FLOAT tensor_shape { dim { size: 2 } } tensor_content: ""\\000\\000 A\\000\\000?"" }.\n          context: The grpc.beta._server_adaptations._FaceServicerContext object.\n\n        Returns:\n          The TensorProto which contains the map of ""outputs"". The response.outputs looks like {\'softmax\': dtype: DT_FLOAT tensor_shape { dim { size: 2 } } tensor_content: ""\\\\\\326\\242=4\\245k?\\\\\\326\\242=4\\245k?"" }\n        """"""\n        request_map = request.inputs\n        feed_dict = {}\n        for k, v in self.inputs.items():\n            # Convert TensorProto objects to numpy\n            feed_dict[v] = tensor_util.MakeNdarray(request_map[k])\n\n        # Example result: {\'key\': array([ 2.,  2.], dtype=float32), \'prediction\': array([1, 1]), \'softmax\': array([[ 0.07951042,  0.92048955], [ 0.07951042,  0.92048955]], dtype=float32)}\n        predict_result = self.sess.run(self.outputs, feed_dict=feed_dict)\n\n        response = predict_pb2.PredictResponse()\n        for k, v in predict_result.items():\n            # Convert numpy objects to TensorProto\n            response.outputs[k].CopyFrom(tensor_util.make_tensor_proto(v))\n        return response\n\n\ndef serve(prediction_service):\n    """"""Start the gRPC service.""""""\n    logging.info(""Start gRPC server with PredictionService: {}"".format(vars(\n        prediction_service)))\n\n    # TODO: not able to use ThreadPoolExecutor\n    #server = grpc.server(futures.ThreadPoolExecutor(max_workers=10))\n    #inference_pb2.add_InferenceServiceService_to_server(InferenceService(), server)\n    server = predict_pb2.beta_create_PredictionService_server(\n        prediction_service)\n    server.add_insecure_port(\'[::]:50051\')\n    server.start()\n    try:\n        while True:\n            time.sleep(_ONE_DAY_IN_SECONDS)\n    except KeyboardInterrupt:\n        server.stop(0)\n\n\nif __name__ == \'__main__\':\n    # Specify the model files\n    checkpoint_file = ""./checkpoint/""\n    graph_file = ""./checkpoint/checkpoint.ckpt.meta""\n    prediction_service = PredictionService(checkpoint_file, graph_file)\n\n    serve(prediction_service)\n'"
serving/predict_pb2.py,0,"b'# Generated by the protocol buffer compiler.  DO NOT EDIT!\n# source: predict.proto\n\nimport sys\n_b=sys.version_info[0]<3 and (lambda x:x) or (lambda x:x.encode(\'latin1\'))\nfrom google.protobuf import descriptor as _descriptor\nfrom google.protobuf import message as _message\nfrom google.protobuf import reflection as _reflection\nfrom google.protobuf import symbol_database as _symbol_database\nfrom google.protobuf import descriptor_pb2\n# @@protoc_insertion_point(imports)\n\n_sym_db = _symbol_database.Default()\n\n\nfrom tensorflow.core.framework import tensor_pb2 as tensorflow_dot_core_dot_framework_dot_tensor__pb2\n\n\nDESCRIPTOR = _descriptor.FileDescriptor(\n  name=\'predict.proto\',\n  package=\'tensorflow.serving\',\n  syntax=\'proto3\',\n  serialized_pb=_b(\'\\n\\rpredict.proto\\x12\\x12tensorflow.serving\\x1a&tensorflow/core/framework/tensor.proto\\""\\x98\\x01\\n\\x0ePredictRequest\\x12>\\n\\x06inputs\\x18\\x02 \\x03(\\x0b\\x32..tensorflow.serving.PredictRequest.InputsEntry\\x1a\\x46\\n\\x0bInputsEntry\\x12\\x0b\\n\\x03key\\x18\\x01 \\x01(\\t\\x12&\\n\\x05value\\x18\\x02 \\x01(\\x0b\\x32\\x17.tensorflow.TensorProto:\\x02\\x38\\x01\\""\\x9d\\x01\\n\\x0fPredictResponse\\x12\\x41\\n\\x07outputs\\x18\\x01 \\x03(\\x0b\\x32\\x30.tensorflow.serving.PredictResponse.OutputsEntry\\x1aG\\n\\x0cOutputsEntry\\x12\\x0b\\n\\x03key\\x18\\x01 \\x01(\\t\\x12&\\n\\x05value\\x18\\x02 \\x01(\\x0b\\x32\\x17.tensorflow.TensorProto:\\x02\\x38\\x01\\x32g\\n\\x11PredictionService\\x12R\\n\\x07Predict\\x12\\"".tensorflow.serving.PredictRequest\\x1a#.tensorflow.serving.PredictResponseB\\x03\\xf8\\x01\\x01\\x62\\x06proto3\')\n  ,\n  dependencies=[tensorflow_dot_core_dot_framework_dot_tensor__pb2.DESCRIPTOR,])\n_sym_db.RegisterFileDescriptor(DESCRIPTOR)\n\n\n\n\n_PREDICTREQUEST_INPUTSENTRY = _descriptor.Descriptor(\n  name=\'InputsEntry\',\n  full_name=\'tensorflow.serving.PredictRequest.InputsEntry\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'key\', full_name=\'tensorflow.serving.PredictRequest.InputsEntry.key\', index=0,\n      number=1, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=_b("""").decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'value\', full_name=\'tensorflow.serving.PredictRequest.InputsEntry.value\', index=1,\n      number=2, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=_descriptor._ParseOptions(descriptor_pb2.MessageOptions(), _b(\'8\\001\')),\n  is_extendable=False,\n  syntax=\'proto3\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=160,\n  serialized_end=230,\n)\n\n_PREDICTREQUEST = _descriptor.Descriptor(\n  name=\'PredictRequest\',\n  full_name=\'tensorflow.serving.PredictRequest\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'inputs\', full_name=\'tensorflow.serving.PredictRequest.inputs\', index=0,\n      number=2, type=11, cpp_type=10, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[_PREDICTREQUEST_INPUTSENTRY, ],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto3\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=78,\n  serialized_end=230,\n)\n\n\n_PREDICTRESPONSE_OUTPUTSENTRY = _descriptor.Descriptor(\n  name=\'OutputsEntry\',\n  full_name=\'tensorflow.serving.PredictResponse.OutputsEntry\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'key\', full_name=\'tensorflow.serving.PredictResponse.OutputsEntry.key\', index=0,\n      number=1, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=_b("""").decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'value\', full_name=\'tensorflow.serving.PredictResponse.OutputsEntry.value\', index=1,\n      number=2, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=_descriptor._ParseOptions(descriptor_pb2.MessageOptions(), _b(\'8\\001\')),\n  is_extendable=False,\n  syntax=\'proto3\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=319,\n  serialized_end=390,\n)\n\n_PREDICTRESPONSE = _descriptor.Descriptor(\n  name=\'PredictResponse\',\n  full_name=\'tensorflow.serving.PredictResponse\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'outputs\', full_name=\'tensorflow.serving.PredictResponse.outputs\', index=0,\n      number=1, type=11, cpp_type=10, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[_PREDICTRESPONSE_OUTPUTSENTRY, ],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto3\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=233,\n  serialized_end=390,\n)\n\n_PREDICTREQUEST_INPUTSENTRY.fields_by_name[\'value\'].message_type = tensorflow_dot_core_dot_framework_dot_tensor__pb2._TENSORPROTO\n_PREDICTREQUEST_INPUTSENTRY.containing_type = _PREDICTREQUEST\n_PREDICTREQUEST.fields_by_name[\'inputs\'].message_type = _PREDICTREQUEST_INPUTSENTRY\n_PREDICTRESPONSE_OUTPUTSENTRY.fields_by_name[\'value\'].message_type = tensorflow_dot_core_dot_framework_dot_tensor__pb2._TENSORPROTO\n_PREDICTRESPONSE_OUTPUTSENTRY.containing_type = _PREDICTRESPONSE\n_PREDICTRESPONSE.fields_by_name[\'outputs\'].message_type = _PREDICTRESPONSE_OUTPUTSENTRY\nDESCRIPTOR.message_types_by_name[\'PredictRequest\'] = _PREDICTREQUEST\nDESCRIPTOR.message_types_by_name[\'PredictResponse\'] = _PREDICTRESPONSE\n\nPredictRequest = _reflection.GeneratedProtocolMessageType(\'PredictRequest\', (_message.Message,), dict(\n\n  InputsEntry = _reflection.GeneratedProtocolMessageType(\'InputsEntry\', (_message.Message,), dict(\n    DESCRIPTOR = _PREDICTREQUEST_INPUTSENTRY,\n    __module__ = \'predict_pb2\'\n    # @@protoc_insertion_point(class_scope:tensorflow.serving.PredictRequest.InputsEntry)\n    ))\n  ,\n  DESCRIPTOR = _PREDICTREQUEST,\n  __module__ = \'predict_pb2\'\n  # @@protoc_insertion_point(class_scope:tensorflow.serving.PredictRequest)\n  ))\n_sym_db.RegisterMessage(PredictRequest)\n_sym_db.RegisterMessage(PredictRequest.InputsEntry)\n\nPredictResponse = _reflection.GeneratedProtocolMessageType(\'PredictResponse\', (_message.Message,), dict(\n\n  OutputsEntry = _reflection.GeneratedProtocolMessageType(\'OutputsEntry\', (_message.Message,), dict(\n    DESCRIPTOR = _PREDICTRESPONSE_OUTPUTSENTRY,\n    __module__ = \'predict_pb2\'\n    # @@protoc_insertion_point(class_scope:tensorflow.serving.PredictResponse.OutputsEntry)\n    ))\n  ,\n  DESCRIPTOR = _PREDICTRESPONSE,\n  __module__ = \'predict_pb2\'\n  # @@protoc_insertion_point(class_scope:tensorflow.serving.PredictResponse)\n  ))\n_sym_db.RegisterMessage(PredictResponse)\n_sym_db.RegisterMessage(PredictResponse.OutputsEntry)\n\n\nDESCRIPTOR.has_options = True\nDESCRIPTOR._options = _descriptor._ParseOptions(descriptor_pb2.FileOptions(), _b(\'\\370\\001\\001\'))\n_PREDICTREQUEST_INPUTSENTRY.has_options = True\n_PREDICTREQUEST_INPUTSENTRY._options = _descriptor._ParseOptions(descriptor_pb2.MessageOptions(), _b(\'8\\001\'))\n_PREDICTRESPONSE_OUTPUTSENTRY.has_options = True\n_PREDICTRESPONSE_OUTPUTSENTRY._options = _descriptor._ParseOptions(descriptor_pb2.MessageOptions(), _b(\'8\\001\'))\nimport grpc\nfrom grpc.beta import implementations as beta_implementations\nfrom grpc.beta import interfaces as beta_interfaces\nfrom grpc.framework.common import cardinality\nfrom grpc.framework.interfaces.face import utilities as face_utilities\n\n\nclass PredictionServiceStub(object):\n\n  def __init__(self, channel):\n    """"""Constructor.\n\n    Args:\n      channel: A grpc.Channel.\n    """"""\n    self.Predict = channel.unary_unary(\n        \'/tensorflow.serving.PredictionService/Predict\',\n        request_serializer=PredictRequest.SerializeToString,\n        response_deserializer=PredictResponse.FromString,\n        )\n\n\nclass PredictionServiceServicer(object):\n\n  def Predict(self, request, context):\n    context.set_code(grpc.StatusCode.UNIMPLEMENTED)\n    context.set_details(\'Method not implemented!\')\n    raise NotImplementedError(\'Method not implemented!\')\n\n\ndef add_PredictionServiceServicer_to_server(servicer, server):\n  rpc_method_handlers = {\n      \'Predict\': grpc.unary_unary_rpc_method_handler(\n          servicer.Predict,\n          request_deserializer=PredictRequest.FromString,\n          response_serializer=PredictResponse.SerializeToString,\n      ),\n  }\n  generic_handler = grpc.method_handlers_generic_handler(\n      \'tensorflow.serving.PredictionService\', rpc_method_handlers)\n  server.add_generic_rpc_handlers((generic_handler,))\n\n\nclass BetaPredictionServiceServicer(object):\n  def Predict(self, request, context):\n    context.code(beta_interfaces.StatusCode.UNIMPLEMENTED)\n\n\nclass BetaPredictionServiceStub(object):\n  def Predict(self, request, timeout, metadata=None, with_call=False, protocol_options=None):\n    raise NotImplementedError()\n  Predict.future = None\n\n\ndef beta_create_PredictionService_server(servicer, pool=None, pool_size=None, default_timeout=None, maximum_timeout=None):\n  request_deserializers = {\n    (\'tensorflow.serving.PredictionService\', \'Predict\'): PredictRequest.FromString,\n  }\n  response_serializers = {\n    (\'tensorflow.serving.PredictionService\', \'Predict\'): PredictResponse.SerializeToString,\n  }\n  method_implementations = {\n    (\'tensorflow.serving.PredictionService\', \'Predict\'): face_utilities.unary_unary_inline(servicer.Predict),\n  }\n  server_options = beta_implementations.server_options(request_deserializers=request_deserializers, response_serializers=response_serializers, thread_pool=pool, thread_pool_size=pool_size, default_timeout=default_timeout, maximum_timeout=maximum_timeout)\n  return beta_implementations.server(method_implementations, options=server_options)\n\n\ndef beta_create_PredictionService_stub(channel, host=None, metadata_transformer=None, pool=None, pool_size=None):\n  request_serializers = {\n    (\'tensorflow.serving.PredictionService\', \'Predict\'): PredictRequest.SerializeToString,\n  }\n  response_deserializers = {\n    (\'tensorflow.serving.PredictionService\', \'Predict\'): PredictResponse.FromString,\n  }\n  cardinalities = {\n    \'Predict\': cardinality.Cardinality.UNARY_UNARY,\n  }\n  stub_options = beta_implementations.stub_options(host=host, metadata_transformer=metadata_transformer, request_serializers=request_serializers, response_deserializers=response_deserializers, thread_pool=pool, thread_pool_size=pool_size)\n  return beta_implementations.dynamic_stub(channel, \'tensorflow.serving.PredictionService\', cardinalities, options=stub_options)\n# @@protoc_insertion_point(module_scope)\n'"
serving/train_mnist_softmax4serving.py,41,"b'from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport argparse\n\n\nimport tensorflow as tf\nimport time\nimport json\nimport sys\nsys.path.append(\'../\')\nfrom algorithm import input_data\nFLAGS = tf.app.flags.FLAGS\n\ntf.flags.DEFINE_string(""data_dir"", ""../algorithm/mnist"",\n                       ""mnist data_dir"")\ntf.flags.DEFINE_string(\'tensorboard_dir\',""./tensorboard"", ""log dir"")\ntf.flags.DEFINE_string(\'checkpoint_dir\',\'./checkpoint\', \'checkpoint dir\')\n\n\n\n\ndef inference(inputs):\n    # W = tf.Variable(tf.random_normal([784, 10]))\n    # b = tf.Variable(tf.random_normal([10]))\n    # y = tf.matmul(inputs, W) + b\n    W1 = tf.Variable(tf.random_normal([784, 256]))\n    b1 = tf.Variable(tf.random_normal([256]))\n    W2 = tf.Variable(tf.random_normal([256, 10]))\n    b2 = tf.Variable(tf.random_normal([10]))\n    lay1 = tf.nn.relu(tf.matmul(inputs, W1) + b1)\n    y = tf.add(tf.matmul(lay1, W2),b2)\n    return y\n\n\n\ndef main(_):\n    mnist = input_data.read_data_sets(FLAGS.data_dir, one_hot=True)\n    input = tf.placeholder(tf.float32, [None, 784])\n    y = inference(input)\n    y_ = tf.placeholder(tf.float32, [None, 10])\n    global_step = tf.Variable(0, name=\'global_step\', trainable=False)\n    cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=y, labels=y_))\n    train_step = tf.train.GradientDescentOptimizer(0.2).minimize(cross_entropy, global_step=global_step)\n    correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(y_, 1))\n    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n    \n    inference_features = tf.placeholder(tf.float32, [None, 784])\n    inference_logits = inference(inference_features)\n    inference_softmax = tf.nn.softmax(inference_logits)\n    inference_result = tf.argmax(inference_softmax, 1)\n    checkpoint_dir = FLAGS.checkpoint_dir\n    checkpoint_file = checkpoint_dir + ""/checkpoint.ckpt""\n    try:\n      init_op = tf.global_variables_initializer()\n    except AttributeError:\n      init_op = tf.initialize_all_variables()\n    # add the tensors to the tensorboard logs\n    try:\n      tf.summary.scalar(\'loss\', cross_entropy)\n      tf.summary.scalar(\'accuracy\', accuracy)\n    except AttributeError:\n      tf.scalar_summary(\'loss\', cross_entropy)\n      tf.scalar_summary(\'accuracy\', accuracy)\n    \n    saver = tf.train.Saver()\n    keys_placeholder = tf.placeholder(""float"")\n    keys = tf.identity(keys_placeholder)\n    tf.add_to_collection(""input"", json.dumps({\'key\': keys_placeholder.name, \'features\': inference_features.name}))\n    tf.add_to_collection(\'output\', json.dumps({\'key\': keys.name, \'softmax\': inference_softmax.name, \'prediction\': inference_result.name}))\n    with tf.Session() as sess:\n        try:\n          summary_op = tf.summary.merge_all()\n        except AttributeError:\n          summary_op = tf.merge_all_summaries()\n        tensorboard_dir = FLAGS.tensorboard_dir\n        writer = tf.summary.FileWriter(tensorboard_dir, sess.graph)\n        try:\n          tf.global_variables_initializer().run()\n        except AttributeError:\n          tf.initialize_all_variables().run()\n        for index in range(10000):\n            print(\'process the {}th batch\'.format(index))\n            start_train = time.time()\n            batch_xs, batch_ys = mnist.train.next_batch(100)\n            _,summary_val, step  = sess.run([train_step, summary_op, global_step], feed_dict={input: batch_xs, y_: batch_ys})\n            writer.add_summary(summary_val, step)\n            print(\'the {0} batch takes time: {1}\'.format(index, time.time()-start_train))\n        print(\'the test dataset acc: \', sess.run(accuracy, feed_dict={input: mnist.test.images,y_: mnist.test.labels}))\n        saver.save(sess, checkpoint_file)    \n\n\nif __name__ == \'__main__\':\n  tf.app.run()'"
GAN/AC-GAN/discriminator.py,4,"b'import tensorflow as tf\nimport tensorflow.contrib.slim as slim\nfrom tensorflow.python.ops import variable_scope\n\ndef leaky_relu(x):\n     return tf.where(tf.greater(x, 0), x, 0.01 * x)\n\ndef discriminator(tensor, num_category=10, batch_size=32, num_cont=2):\n    """"""\n    """"""\n    \n    reuse = len([t for t in tf.global_variables() if t.name.startswith(\'discriminator\')]) > 0\n    print reuse\n    print tensor.get_shape()\n    with variable_scope.variable_scope(\'discriminator\', reuse=reuse):\n        tensor = slim.conv2d(tensor, num_outputs = 64, kernel_size=[4,4], stride=2, activation_fn=leaky_relu)\n        tensor = slim.conv2d(tensor, num_outputs=128, kernel_size=[4,4], stride=2, activation_fn=leaky_relu)\n        tensor = slim.flatten(tensor)\n        shared_tensor = slim.fully_connected(tensor, num_outputs=1024, activation_fn = leaky_relu)\n        recog_shared = slim.fully_connected(shared_tensor, num_outputs=128, activation_fn = leaky_relu)\n        disc = slim.fully_connected(shared_tensor, num_outputs=1, activation_fn=None)\n        disc = tf.squeeze(disc, -1)\n        recog_cat = slim.fully_connected(recog_shared, num_outputs=num_category, activation_fn=None)\n        recog_cont = slim.fully_connected(recog_shared, num_outputs=num_cont, activation_fn=tf.nn.sigmoid)\n    return disc, recog_cat, recog_cont\n'"
GAN/AC-GAN/generate.py,13,"b'import tensorflow as tf\nimport numpy as np\nfrom generator import generator\nimport matplotlib\nmatplotlib.use(\'Agg\')\nimport matplotlib.pyplot as plt\nimport os\n\n_logger = tf.logging._logger\n_logger.setLevel(0)\n\nbatch_size = 100   # batch size\ncat_dim = 10   # total categorical factor\ncon_dim = 2    # total continuous factor\nrand_dim = 38  # total random latent dimension\n\n\ntarget_num = tf.placeholder(dtype=tf.int32, shape=batch_size)\ntarget_cval_1 = tf.placeholder(dtype=tf.float32, shape=batch_size)\ntarget_cval_2 = tf.placeholder(dtype=tf.float32, shape=batch_size)\n\nz = tf.one_hot(tf.ones(batch_size, dtype=tf.int32) * target_num, depth=cat_dim)\nz = tf.concat(axis=z.get_shape().ndims-1, values=[z, tf.expand_dims(target_cval_1, -1), tf.expand_dims(target_cval_2, -1)])\n\nz = tf.concat(axis=z.get_shape().ndims-1, values=[z, tf.random_normal((batch_size, rand_dim))])\n\ngen = tf.squeeze(generator(z), -1)\n\ndef run_generator(num, x1, x2, fig_name=\'sample.png\'):\n    with tf.Session() as sess:\n        sess.run(tf.group(tf.global_variables_initializer(),\n                      tf.local_variables_initializer()))\n        saver = tf.train.Saver()\n        saver.restore(sess, tf.train.latest_checkpoint(\'checkpoint_dir\'))\n        imgs = sess.run(gen, {target_num: num, target_cval_1: x1, target_cval_2:x2})\n\n        _, ax = plt.subplots(10,10, sharex=True, sharey=True)\n        for i in range(10):\n            for j in range(10):\n                ax[i][j].imshow(imgs[i*10+j], \'gray\')\n                ax[i][j].set_axis_off()\n        plt.savefig(os.path.join(\'result/\',fig_name), dpi=600)\n        print \'Sample image save to ""result/{0}""\'.format(fig_name)\n        plt.close()\n\na = np.random.randint(0, cat_dim, batch_size)\nprint a\nrun_generator(a,\n              np.random.uniform(0, 1, batch_size), np.random.uniform(0, 1, batch_size),\n              fig_name=\'fake.png\')\n\n# classified image\nrun_generator(np.arange(10).repeat(10), np.linspace(0, 1, 10).repeat(10), np.expand_dims(np.linspace(0, 1, 10), axis=1).repeat(10, axis=1).T.flatten(),)\n'"
GAN/AC-GAN/generator.py,6,"b""import tensorflow as tf\nimport tensorflow.contrib.slim as slim\nfrom tensorflow.python.ops import variable_scope\n\ndef generator(tensor):\n    reuse = len([t for t in tf.global_variables() if t.name.startswith('generator')]) > 0\n    print tensor.get_shape()\n    with variable_scope.variable_scope('generator', reuse = reuse):\n        tensor = slim.fully_connected(tensor, 1024)\n        print tensor\n        tensor = slim.batch_norm(tensor, activation_fn=tf.nn.relu)\n        tensor = slim.fully_connected(tensor, 7*7*128)\n        tensor = slim.batch_norm(tensor, activation_fn=tf.nn.relu)\n        tensor = tf.reshape(tensor, [-1, 7, 7, 128])\n        # print '22',tensor.get_shape()\n        tensor = slim.conv2d_transpose(tensor, 64, kernel_size=[4,4], stride=2, activation_fn = None)\n        print 'gen',tensor.get_shape()\n        tensor = slim.batch_norm(tensor, activation_fn = tf.nn.relu)\n        tensor = slim.conv2d_transpose(tensor, 1, kernel_size=[4, 4], stride=2, activation_fn=tf.nn.sigmoid)\n    return tensor"""
GAN/AC-GAN/mnist.py,0,"b'from tensorflow.examples.tutorials.mnist import input_data\nfrom utils import data_to_tensor, Opt\n\n\n\n\n\nclass Mnist(object):\n    r""""""Downloads Mnist datasets and puts them in queues.\n    """"""\n    _data_dir = \'./asset/data/mnist\'\n\n    def __init__(self, batch_size=128, num_epochs=30, reshape=False, one_hot=False):\n\n        # load sg_data set\n        data_set = input_data.read_data_sets(Mnist._data_dir, reshape=reshape, one_hot=one_hot)\n\n        self.batch_size = batch_size\n\n        # save each sg_data set\n        _train = data_set.train\n        _valid = data_set.validation\n        _test = data_set.test\n\n        # member initialize\n        self.train, self.valid, self.test = Opt(), Opt(), Opt()\n\n        # convert to tensor queue\n        self.train.image, self.train.label = \\\n            data_to_tensor([_train.images, _train.labels.astype(\'int32\')], batch_size, name=\'train\')\n        self.valid.image, self.valid.label = \\\n            data_to_tensor([_valid.images, _valid.labels.astype(\'int32\')], batch_size, name=\'valid\')\n        self.test.image, self.test.label = \\\n            data_to_tensor([_test.images, _test.labels.astype(\'int32\')], batch_size, name=\'test\')\n\n        # calc total batch count\n        self.train.num_batch = _train.labels.shape[0] // batch_size\n        self.valid.num_batch = _valid.labels.shape[0] // batch_size\n        self.test.num_batch = _test.labels.shape[0] // batch_size\n'"
GAN/AC-GAN/optimizer.py,9,"b'from utils import Opt\nimport tensorflow as tf\n\ndef optim(loss, **kwargs):\n    r""""""Applies gradients to variables.\n\n    Args:\n        loss: A 0-D `Tensor` containing the value to minimize.\n        kwargs:\n          optim: A name for optimizer. \'MaxProp\' (default), \'AdaMax\', \'Adam\', or \'sgd\'.\n          lr: A Python Scalar (optional). Learning rate. Default is .001.\n          beta1: A Python Scalar (optional). Default is .9.\n          beta2: A Python Scalar (optional). Default is .99.\n          category: A string or string list. Specifies the variables that should be trained (optional).\n            Only if the name of a trainable variable starts with `category`, it\'s value is updated.\n            Default is \'\', which means all trainable variables are updated.\n    """"""\n    opt = Opt(kwargs)\n    # opt += Opt(optim=\'MaxProp\', lr=0.001, beta1=0.9, beta2=0.99, category=\'\')\n\n    # default training options\n    opt += Opt(optim=\'MaxProp\', lr=0.001, beta1=0.9, beta2=0.99, category=\'\')\n\n    # select optimizer\n    # if opt.optim == \'MaxProp\':\n        # optim = tf.sg_optimize.MaxPropOptimizer(learning_rate=opt.lr, beta2=opt.beta2)\n    # elif opt.optim == \'AdaMax\':\n        # optim = tf.sg_optimize.AdaMaxOptimizer(learning_rate=opt.lr, beta1=opt.beta1, beta2=opt.beta2)\n    # elif opt.optim == \'Adam\':\n    if opt.optim == \'Adm\':\n        optim = tf.train.AdamOptimizer(learning_rate=opt.lr, beta1=opt.beta1, beta2=opt.beta2)\n    else:\n        optim = tf.train.GradientDescentOptimizer(learning_rate=opt.lr)\n\n    # get trainable variables\n    if isinstance(opt.category, (tuple, list)):\n        var_list = []\n        for cat in opt.category:\n            var_list.extend([t for t in tf.trainable_variables() if t.name.startswith(cat)])\n    else:\n        var_list = [t for t in tf.trainable_variables() if t.name.startswith(opt.category)]\n\n    # calc gradient\n    gradient = optim.compute_gradients(loss, var_list=var_list)\n\n    # add summary\n    for v, g in zip(var_list, gradient):\n        # exclude batch normal statics\n        if \'mean\' not in v.name and \'variance\' not in v.name \\\n                and \'beta\' not in v.name and \'gamma\' not in v.name:\n                prefix = \'\'\n                # summary name\n                name = prefix + \'\'.join(v.name.split(\':\')[:-1])\n                # summary statistics\n                # noinspection PyBroadException\n                try:\n                    tf.summary.scalar(name + \'/grad\', tf.global_norm([g]))\n                    tf.summary.histogram(name + \'/grad-h\', g)\n                except:\n                    pass\n    global_step = tf.Variable(0, name=\'global_step\', trainable=False)\n    # gradient update op\n    return optim.apply_gradients(gradient, global_step=global_step), global_step'"
GAN/AC-GAN/queue_context.py,3,"b'import tensorflow as tf\n\ndef queue_context(sess=None):\n    r""""""Context helper for queue routines.\n\n    Args:\n      sess: A session to open queues. If not specified, a new session is created.\n\n    Returns:\n      None\n    """"""\n\n    # default session\n    sess = tf.get_default_session() if sess is None else sess\n\n    # thread coordinator\n    coord = tf.train.Coordinator()\n    threads = tf.train.start_queue_runners(sess=sess, coord=coord)\n    return coord, threads'"
GAN/AC-GAN/train.py,25,"b'# -*- coding: utf-8 -*-\nimport tensorflow as tf\nimport numpy as np\nimport logging\nfrom mnist import Mnist\nimport tensorflow.contrib.slim as slim\nfrom generator import generator\nfrom discriminator import discriminator\nfrom optimizer import optim\nfrom queue_context import queue_context\nimport os\nimport sys\n\n\ntf.logging.set_verbosity(0)\n\n\n#\n# hyper parameters\n#\n\nbatch_size = 32   # batch size\ncat_dim = 10  # total categorical factor\ncon_dim = 2  # total continuous factor\nrand_dim = 38  \nnum_epochs = 30\ndebug_max_steps = 1000\nsave_epoch = 5\nmax_epochs = 50\n\n#\n# inputs\n#\n\n# MNIST input tensor ( with QueueRunner )\ndata = Mnist(batch_size=batch_size, num_epochs=num_epochs)\nnum_batch_per_epoch = data.train.num_batch\n\n\n# input images and labels\nx = data.train.image\ny = data.train.label\n\n# labels for discriminator\ny_real = tf.ones(batch_size)\ny_fake = tf.zeros(batch_size)\n\n\n# discriminator labels ( half 1s, half 0s )\ny_disc = tf.concat(axis=0, values=[y, y * 0])\n\n#\n# create generator\n#\n\n# get random class number\nif(int(tf.__version__.split(""."")[1])<13 and int(tf.__version__.split(""."")[0])<2): ### tf version < 1.13 \n    z_cat = tf.multinomial(tf.ones((batch_size, cat_dim), dtype=tf.float32) / cat_dim, 1)\nelse: ### tf versioin >= 1.13\n    z_cat = tf.random.categorical(tf.ones((batch_size, cat_dim), dtype=tf.float32) / cat_dim, 1)\n \nz_cat = tf.squeeze(z_cat, -1)\nz_cat = tf.cast(z_cat, tf.int32)\n\n# continuous latent variable\nz_con = tf.random_normal((batch_size, con_dim))\nz_rand = tf.random_normal((batch_size, rand_dim))\n\nz = tf.concat(axis=1, values=[tf.one_hot(z_cat, depth = cat_dim), z_con, z_rand])\n\n\n# generator network\ngen = generator(z)\n\n# add image summary\n# tf.sg_summary_image(gen)\ntf.summary.image(\'real\', x)\ntf.summary.image(\'fake\', gen)\n\n#\n# discriminator\ndisc_real, cat_real, _ = discriminator(x)\ndisc_fake, cat_fake, con_fake = discriminator(gen)\n\n# discriminator loss\nloss_d_r = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=disc_real, labels=y_real))\nloss_d_f = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=disc_fake, labels=y_fake))\nloss_d = (loss_d_r + loss_d_f) / 2\nprint \'loss_d\', loss_d.get_shape()\n# generator loss\nloss_g = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=disc_fake, labels=y_real))\n\n# categorical factor loss\nloss_c_r = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits=cat_real, labels=y))\nloss_c_d = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits=cat_fake, labels=z_cat))\nloss_c = (loss_c_r + loss_c_d) / 2\nprint \'loss_c\', loss_c.get_shape()\n# continuous factor loss\nloss_con =tf.reduce_mean(tf.square(con_fake-z_con))\nprint \'loss_con\', loss_con.get_shape()\n\n\n\ntrain_disc, disc_global_step = optim(loss_d + loss_c + loss_con, lr=0.0001, optim = \'Adm\', category=\'discriminator\')\ntrain_gen, gen_global_step = optim(loss_g + loss_c + loss_con, lr=0.001, optim = \'Adm\', category=\'generator\')\ninit = tf.global_variables_initializer()\nsaver = tf.train.Saver()\nprint train_gen\n\ncur_epoch = 0\ncur_step = 0\n\n\nwith tf.Session() as sess:\n    sess.run(init)\n    coord, threads = queue_context(sess)\n    try:\n        while not coord.should_stop():\n            cur_step += 1\n            dis_part = cur_step*1.0/num_batch_per_epoch\n            dis_part = int(dis_part*50)\n            sys.stdout.write(""process bar ::|""+""<""* dis_part+\'|\'+str(cur_step*1.0/num_batch_per_epoch*100)+\'%\'+\'\\r\')\n            sys.stdout.flush()\n            l_disc, _, l_d_step = sess.run([loss_d, train_disc, disc_global_step])\n            l_gen, _, l_g_step = sess.run([loss_g, train_gen, gen_global_step])\n            last_epoch = cur_epoch\n            cur_epoch = l_d_step / num_batch_per_epoch\n            if cur_epoch > max_epochs:\n                break\n\n            if cur_epoch> last_epoch:\n                cur_step = 0\n                print \'cur epoch {0} update l_d step {1}, loss_disc {2}, loss_gen {3}\'.format(cur_epoch, l_d_step, l_disc, l_gen)\n                if cur_epoch % save_epoch == 0:\n                    # save\n                    saver.save(sess, os.path.join(\'./checkpoint_dir\', \'ac_gan\'), global_step=l_d_step)\n    except tf.errors.OutOfRangeError:\n        print \'Train Finished\'\n    finally:\n        coord.request_stop()\n'"
GAN/AC-GAN/utils.py,7,"b'import collections\nimport six\nimport tensorflow as tf\nimport sys\n\n\n\n\nclass Opt(collections.MutableMapping):\n    r""""""Option utility class.\n\n    This class is only internally used for sg_opt.\n    """"""\n\n    def __init__(self, *args, **kwargs):\n        self.__dict__.update(*args, **kwargs)\n\n    def __setitem__(self, key, value):\n        self.__dict__[key] = value\n\n    def __getitem__(self, key):\n        return self.__dict__[key]\n\n    def __delitem__(self, key):\n        del self.__dict__[key]\n\n    # noinspection PyUnusedLocal,PyUnusedLocal\n    def __getattr__(self, key):\n        return None\n\n    def __iter__(self):\n        return iter(self.__dict__)\n\n    def __len__(self):\n        return len(self.__dict__)\n\n    def __str__(self):\n        return str(self.__dict__)\n\n    def __repr__(self):\n        return self.__dict__.__repr__()\n\n    def __add__(self, other):\n        r""""""Overloads `+` operator.\n        \n        It does NOT overwrite the existing item.\n        \n        For example,\n        \n        ```python\n        import sugartensor as tf\n\n        opt = tf.sg_opt(size=1)\n        opt += tf.sg_opt(size=2)\n        print(opt) # Should be {\'size\': 1}\n        ```\n        """"""\n        res = Opt(self.__dict__)\n        for k, v in six.iteritems(other):\n            if k not in res.__dict__ or res.__dict__[k] is None:\n                res.__dict__[k] = v\n        return res\n\n    def __mul__(self, other):\n        r""""""Overloads `*` operator.\n        \n        It overwrites the existing item.\n        \n        For example,\n        \n        ```python\n        import sugartensor as tf\n\n        opt = tf.sg_opt(size=1)\n        opt *= tf.sg_opt(size=2)\n        print(opt) # Should be {\'size\': 2}\n        ```\n        """"""\n        res = Opt(self.__dict__)\n        for k, v in six.iteritems(other):\n            res.__dict__[k] = v\n        return res\n\n\ndef data_to_tensor(data_list, batch_size,  name=None):\n    r""""""Returns batch queues from the whole data. \n    \n    Args:\n      data_list: A list of ndarrays. Every array must have the same size in the first dimension.\n      batch_size: An integer.\n      name: A name for the operations (optional).\n      \n    Returns:\n      A list of tensors of `batch_size`.\n    """"""\n    # convert to constant tensor\n    const_list = [tf.constant(data) for data in data_list]\n\n    # create queue from constant tensor\n    queue_list = tf.train.slice_input_producer(const_list, capacity=batch_size*128, name=name)\n\n    # create batch queue\n    return tf.train.shuffle_batch(queue_list, batch_size, capacity=batch_size*128,\n                                  min_after_dequeue=batch_size*32, name=name)\n'"
GAN/Info-GAN/discriminator.py,4,"b'import tensorflow as tf\nimport tensorflow.contrib.slim as slim\nfrom tensorflow.python.ops import variable_scope\n\ndef leaky_relu(x):\n     return tf.where(tf.greater(x, 0), x, 0.01 * x)\n\ndef discriminator(tensor, num_category=10, batch_size=32, num_cont=2):\n    """"""\n    """"""\n    \n    reuse = len([t for t in tf.global_variables() if t.name.startswith(\'discriminator\')]) > 0\n    print reuse\n    print tensor.get_shape()\n    with variable_scope.variable_scope(\'discriminator\', reuse=reuse):\n        tensor = slim.conv2d(tensor, num_outputs = 64, kernel_size=[4,4], stride=2, activation_fn=leaky_relu)\n        tensor = slim.conv2d(tensor, num_outputs=128, kernel_size=[4,4], stride=2, activation_fn=leaky_relu)\n        tensor = slim.flatten(tensor)\n        shared_tensor = slim.fully_connected(tensor, num_outputs=1024, activation_fn = leaky_relu)\n        recog_shared = slim.fully_connected(shared_tensor, num_outputs=128, activation_fn = leaky_relu)\n        disc = slim.fully_connected(shared_tensor, num_outputs=1, activation_fn=None)\n        disc = tf.squeeze(disc, -1)\n        recog_cat = slim.fully_connected(recog_shared, num_outputs=num_category, activation_fn=None)\n        recog_cont = slim.fully_connected(recog_shared, num_outputs=num_cont, activation_fn=tf.nn.sigmoid)\n    return disc, recog_cat, recog_cont\n'"
GAN/Info-GAN/generate.py,15,"b'import tensorflow as tf\nimport numpy as np\nfrom generator import generator\nimport matplotlib\nmatplotlib.use(\'Agg\')\nimport matplotlib.pyplot as plt\nimport os\n\n_logger = tf.logging._logger\n_logger.setLevel(0)\n\nbatch_size = 100   # batch size\ncat_dim = 10   # total categorical factor\ncon_dim = 2    # total continuous factor\nrand_dim = 38  # total random latent dimension\n\n\ntarget_num = tf.placeholder(dtype=tf.int32, shape=batch_size)\ntarget_cval_1 = tf.placeholder(dtype=tf.float32, shape=batch_size)\ntarget_cval_2 = tf.placeholder(dtype=tf.float32, shape=batch_size)\n\nz = tf.one_hot(tf.ones(batch_size, dtype=tf.int32) * target_num, depth=cat_dim)\nz = tf.concat(axis=z.get_shape().ndims-1, values=[z, tf.expand_dims(target_cval_1, -1), tf.expand_dims(target_cval_2, -1)])\n\nz = tf.concat(axis=z.get_shape().ndims-1, values=[z, tf.random_normal((batch_size, rand_dim))])\n\ngen = tf.squeeze(generator(z), -1)\n\ndef run_generator(num, x1, x2, fig_name=\'sample.png\'):\n    with tf.Session() as sess:\n        try:\n                sess.run(tf.group(tf.global_variables_initializer(),\n                        tf.local_variables_initializer()))\n        except AttributeError:\n                sess.run(tf.group(tf.initialize_all_variables(),\n                        tf.initialize_local_variables()))\n        saver = tf.train.Saver()\n        saver.restore(sess, tf.train.latest_checkpoint(\'checkpoint_dir\'))\n        imgs = sess.run(gen, {target_num: num, target_cval_1: x1, target_cval_2:x2})\n\n        _, ax = plt.subplots(10,10, sharex=True, sharey=True)\n        for i in range(10):\n            for j in range(10):\n                ax[i][j].imshow(imgs[i*10+j], \'gray\')\n                ax[i][j].set_axis_off()\n        plt.savefig(os.path.join(\'result/\',fig_name), dpi=600)\n        print \'Sample image save to ""result/{0}""\'.format(fig_name)\n        plt.close()\n\na = np.random.randint(0, cat_dim, batch_size)\nprint a\nrun_generator(a,\n              np.random.uniform(0, 1, batch_size), np.random.uniform(0, 1, batch_size),\n              fig_name=\'fake.png\')\n\n# classified image\nrun_generator(np.arange(10).repeat(10), np.linspace(0, 1, 10).repeat(10), np.expand_dims(np.linspace(0, 1, 10), axis=1).repeat(10, axis=1).T.flatten(),)\n'"
GAN/Info-GAN/generator.py,6,"b""import tensorflow as tf\nimport tensorflow.contrib.slim as slim\nfrom tensorflow.python.ops import variable_scope\n\ndef generator(tensor):\n    reuse = len([t for t in tf.global_variables() if t.name.startswith('generator')]) > 0\n    print tensor.get_shape()\n    with variable_scope.variable_scope('generator', reuse = reuse):\n        tensor = slim.fully_connected(tensor, 1024)\n        print tensor\n        tensor = slim.batch_norm(tensor, activation_fn=tf.nn.relu)\n        tensor = slim.fully_connected(tensor, 7*7*128)\n        tensor = slim.batch_norm(tensor, activation_fn=tf.nn.relu)\n        tensor = tf.reshape(tensor, [-1, 7, 7, 128])\n        # print '22',tensor.get_shape()\n        tensor = slim.conv2d_transpose(tensor, 64, kernel_size=[4,4], stride=2, activation_fn = None)\n        print 'gen',tensor.get_shape()\n        tensor = slim.batch_norm(tensor, activation_fn = tf.nn.relu)\n        tensor = slim.conv2d_transpose(tensor, 1, kernel_size=[4, 4], stride=2, activation_fn=tf.nn.sigmoid)\n    return tensor"""
GAN/Info-GAN/mnist.py,0,"b'from tensorflow.examples.tutorials.mnist import input_data\nfrom utils import data_to_tensor, Opt\n\n\n\n\n\nclass Mnist(object):\n    r""""""Downloads Mnist datasets and puts them in queues.\n    """"""\n    _data_dir = \'./asset/data/mnist\'\n\n    def __init__(self, batch_size=128, num_epochs=30, reshape=False, one_hot=False):\n\n        # load sg_data set\n        data_set = input_data.read_data_sets(Mnist._data_dir, reshape=reshape, one_hot=one_hot)\n\n        self.batch_size = batch_size\n\n        # save each sg_data set\n        _train = data_set.train\n        _valid = data_set.validation\n        _test = data_set.test\n\n        # member initialize\n        self.train, self.valid, self.test = Opt(), Opt(), Opt()\n\n        # convert to tensor queue\n        self.train.image, self.train.label = \\\n            data_to_tensor([_train.images, _train.labels.astype(\'int32\')], batch_size, name=\'train\')\n        self.valid.image, self.valid.label = \\\n            data_to_tensor([_valid.images, _valid.labels.astype(\'int32\')], batch_size, name=\'valid\')\n        self.test.image, self.test.label = \\\n            data_to_tensor([_test.images, _test.labels.astype(\'int32\')], batch_size, name=\'test\')\n\n        # calc total batch count\n        self.train.num_batch = _train.labels.shape[0] // batch_size\n        self.valid.num_batch = _valid.labels.shape[0] // batch_size\n        self.test.num_batch = _test.labels.shape[0] // batch_size\n'"
GAN/Info-GAN/optimizer.py,11,"b'from utils import Opt\nimport tensorflow as tf\n\ndef optim(loss, **kwargs):\n    r""""""Applies gradients to variables.\n\n    Args:\n        loss: A 0-D `Tensor` containing the value to minimize.\n        kwargs:\n          optim: A name for optimizer. \'MaxProp\' (default), \'AdaMax\', \'Adam\', or \'sgd\'.\n          lr: A Python Scalar (optional). Learning rate. Default is .001.\n          beta1: A Python Scalar (optional). Default is .9.\n          beta2: A Python Scalar (optional). Default is .99.\n          category: A string or string list. Specifies the variables that should be trained (optional).\n            Only if the name of a trainable variable starts with `category`, it\'s value is updated.\n            Default is \'\', which means all trainable variables are updated.\n    """"""\n    opt = Opt(kwargs)\n    # opt += Opt(optim=\'MaxProp\', lr=0.001, beta1=0.9, beta2=0.99, category=\'\')\n\n    # default training options\n    opt += Opt(optim=\'MaxProp\', lr=0.001, beta1=0.9, beta2=0.99, category=\'\')\n\n    # select optimizer\n    # if opt.optim == \'MaxProp\':\n        # optim = tf.sg_optimize.MaxPropOptimizer(learning_rate=opt.lr, beta2=opt.beta2)\n    # elif opt.optim == \'AdaMax\':\n        # optim = tf.sg_optimize.AdaMaxOptimizer(learning_rate=opt.lr, beta1=opt.beta1, beta2=opt.beta2)\n    # elif opt.optim == \'Adam\':\n    if opt.optim == \'Adm\':\n        optim = tf.train.AdamOptimizer(learning_rate=opt.lr, beta1=opt.beta1, beta2=opt.beta2)\n    else:\n        optim = tf.train.GradientDescentOptimizer(learning_rate=opt.lr)\n\n    # get trainable variables\n    if isinstance(opt.category, (tuple, list)):\n        var_list = []\n        for cat in opt.category:\n            var_list.extend([t for t in tf.trainable_variables() if t.name.startswith(cat)])\n    else:\n        var_list = [t for t in tf.trainable_variables() if t.name.startswith(opt.category)]\n\n    # calc gradient\n    gradient = optim.compute_gradients(loss, var_list=var_list)\n\n    # add summary\n    for v, g in zip(var_list, gradient):\n        # exclude batch normal statics\n        if \'mean\' not in v.name and \'variance\' not in v.name \\\n                and \'beta\' not in v.name and \'gamma\' not in v.name:\n                prefix = \'\'\n                # summary name\n                name = prefix + \'\'.join(v.name.split(\':\')[:-1])\n                # summary statistics\n                # noinspection PyBroadException\n                try:\n                    tf.summary.scalar(name + \'/grad\', tf.global_norm([g]))\n                    tf.summary.histogram(name + \'/grad-h\', g)\n                except AttributeError:\n                    tf.scalar_summary(name + \'/grad\', tf.global_norm([g]))\n                    tf.histogram_summary(name + \'/grad-h\', g)\n                except:\n                    pass\n    global_step = tf.Variable(0, name=\'global_step\', trainable=False)\n    # gradient update op\n    return optim.apply_gradients(gradient, global_step=global_step), global_step'"
GAN/Info-GAN/queue_context.py,3,"b'import tensorflow as tf\n\ndef queue_context(sess=None):\n    r""""""Context helper for queue routines.\n\n    Args:\n      sess: A session to open queues. If not specified, a new session is created.\n\n    Returns:\n      None\n    """"""\n\n    # default session\n    sess = tf.get_default_session() if sess is None else sess\n\n    # thread coordinator\n    coord = tf.train.Coordinator()\n    threads = tf.train.start_queue_runners(sess=sess, coord=coord)\n    return coord, threads'"
GAN/Info-GAN/train.py,27,"b'# -*- coding: utf-8 -*-\nimport tensorflow as tf\nimport numpy as np\nimport logging\nfrom mnist import Mnist\nimport tensorflow.contrib.slim as slim\nfrom generator import generator\nfrom discriminator import discriminator\nfrom optimizer import optim\nfrom queue_context import queue_context\nimport os\nimport sys\n\n\ntf.logging.set_verbosity(0)\n\n\n#\n# hyper parameters\n#\n\nbatch_size = 32   # batch size\ncat_dim = 10  # total categorical factor\ncon_dim = 2  # total continuous factor\nrand_dim = 38  \nnum_epochs = 30\ndebug_max_steps = 1000\nsave_epoch = 5\nmax_epochs = 50\n\n#\n# inputs\n#\n\n# MNIST input tensor ( with QueueRunner )\ndata = Mnist(batch_size=batch_size, num_epochs=num_epochs)\nnum_batch_per_epoch = data.train.num_batch\n\n\n# input images and labels\nx = data.train.image\ny = data.train.label\n\n# labels for discriminator\ny_real = tf.ones(batch_size)\ny_fake = tf.zeros(batch_size)\n\n\n# discriminator labels ( half 1s, half 0s )\ny_disc = tf.concat(axis=0, values=[y, y * 0])\n\n#\n# create generator\n#\n\n# get random class number\nif(int(tf.__version__.split(""."")[1])<13 and int(tf.__version__.split(""."")[0])<2): ### tf version < 1.13 \n    z_cat = tf.multinomial(tf.ones((batch_size, cat_dim), dtype=tf.float32) / cat_dim, 1)\nelse: ### tf version >= 1.13\n    z_cat = tf.random.categorical(tf.ones((batch_size, cat_dim), dtype=tf.float32) / cat_dim, 1)\n \nz_cat = tf.squeeze(z_cat, -1)\nz_cat = tf.cast(z_cat, tf.int32)\n\n# continuous latent variable\nz_con = tf.random_normal((batch_size, con_dim))\nz_rand = tf.random_normal((batch_size, rand_dim))\n\nz = tf.concat(axis=1, values=[tf.one_hot(z_cat, depth = cat_dim), z_con, z_rand])\n\n\n# generator network\ngen = generator(z)\n\n# add image summary\n# tf.sg_summary_image(gen)\ntry:\n    tf.summary.image(\'real\', x)\n    tf.summary.image(\'fake\', gen)\nexcept AttributeError:\n    tf.image_summary(\'real\', x)\n    tf.image_summary(\'fake\', gen)\n\n#\n# discriminator\ndisc_real, _, _ = discriminator(x)\ndisc_fake, cat_fake, con_fake = discriminator(gen)\n\n# discriminator loss\nloss_d_r = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=disc_real, labels=y_real))\nloss_d_f = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=disc_fake, labels=y_fake))\nloss_d = (loss_d_r + loss_d_f) / 2\nprint \'loss_d\', loss_d.get_shape()\n# generator loss\nloss_g = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=disc_fake, labels=y_real))\n\n# categorical factor loss\nloss_c = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits=cat_fake, labels=z_cat))\n\n# continuous factor loss\nloss_con =tf.reduce_mean(tf.square(con_fake-z_con))\n\ntrain_disc, disc_global_step = optim(loss_d + loss_c + loss_con, lr=0.0001, optim = \'Adm\', category=\'discriminator\')\ntrain_gen, gen_global_step = optim(loss_g + loss_c + loss_con, lr=0.001, optim = \'Adm\', category=\'generator\')\ntry:\n    init = tf.global_variables_initializer()\nexcept AttributeError:\n    init = tf.initialize_all_variables()\nsaver = tf.train.Saver()\nprint train_gen\n\ncur_epoch = 0\ncur_step = 0\n\n\nwith tf.Session() as sess:\n    sess.run(init)\n    coord, threads = queue_context(sess)\n    try:\n        while not coord.should_stop():\n            cur_step += 1\n            dis_part = cur_step*1.0/num_batch_per_epoch\n            dis_part = int(dis_part*50)\n            sys.stdout.write(""process bar ::|""+""<""* dis_part+\'|\'+str(cur_step*1.0/num_batch_per_epoch*100)+\'%\'+\'\\r\')\n            sys.stdout.flush()\n            l_disc, _, l_d_step = sess.run([loss_d, train_disc, disc_global_step])\n            l_gen, _, l_g_step = sess.run([loss_g, train_gen, gen_global_step])\n            last_epoch = cur_epoch\n            cur_epoch = l_d_step / num_batch_per_epoch\n            if cur_epoch > max_epochs:\n                break\n\n            if cur_epoch> last_epoch:\n                cur_step = 0\n                print \'cur epoch {0} update l_d step {1}, loss_disc {2}, loss_gen {3}\'.format(cur_epoch, l_d_step, l_disc, l_gen)\n                if cur_epoch % save_epoch == 0:\n                    # save\n                    saver.save(sess, os.path.join(\'./checkpoint_dir\', \'ac_gan\'), global_step=l_d_step)\n    except tf.errors.OutOfRangeError:\n        print \'Train Finished\'\n    finally:\n        coord.request_stop()\n'"
GAN/Info-GAN/utils.py,7,"b'import collections\nimport six\nimport tensorflow as tf\nimport sys\n\n\n\n\nclass Opt(collections.MutableMapping):\n    r""""""Option utility class.\n\n    This class is only internally used for sg_opt.\n    """"""\n\n    def __init__(self, *args, **kwargs):\n        self.__dict__.update(*args, **kwargs)\n\n    def __setitem__(self, key, value):\n        self.__dict__[key] = value\n\n    def __getitem__(self, key):\n        return self.__dict__[key]\n\n    def __delitem__(self, key):\n        del self.__dict__[key]\n\n    # noinspection PyUnusedLocal,PyUnusedLocal\n    def __getattr__(self, key):\n        return None\n\n    def __iter__(self):\n        return iter(self.__dict__)\n\n    def __len__(self):\n        return len(self.__dict__)\n\n    def __str__(self):\n        return str(self.__dict__)\n\n    def __repr__(self):\n        return self.__dict__.__repr__()\n\n    def __add__(self, other):\n        r""""""Overloads `+` operator.\n        \n        It does NOT overwrite the existing item.\n        \n        For example,\n        \n        ```python\n        import sugartensor as tf\n\n        opt = tf.sg_opt(size=1)\n        opt += tf.sg_opt(size=2)\n        print(opt) # Should be {\'size\': 1}\n        ```\n        """"""\n        res = Opt(self.__dict__)\n        for k, v in six.iteritems(other):\n            if k not in res.__dict__ or res.__dict__[k] is None:\n                res.__dict__[k] = v\n        return res\n\n    def __mul__(self, other):\n        r""""""Overloads `*` operator.\n        \n        It overwrites the existing item.\n        \n        For example,\n        \n        ```python\n        import sugartensor as tf\n\n        opt = tf.sg_opt(size=1)\n        opt *= tf.sg_opt(size=2)\n        print(opt) # Should be {\'size\': 2}\n        ```\n        """"""\n        res = Opt(self.__dict__)\n        for k, v in six.iteritems(other):\n            res.__dict__[k] = v\n        return res\n\n\ndef data_to_tensor(data_list, batch_size,  name=None):\n    r""""""Returns batch queues from the whole data. \n    \n    Args:\n      data_list: A list of ndarrays. Every array must have the same size in the first dimension.\n      batch_size: An integer.\n      name: A name for the operations (optional).\n      \n    Returns:\n      A list of tensors of `batch_size`.\n    """"""\n    # convert to constant tensor\n    const_list = [tf.constant(data) for data in data_list]\n\n    # create queue from constant tensor\n    queue_list = tf.train.slice_input_producer(const_list, capacity=batch_size*128, name=name)\n\n    # create batch queue\n    return tf.train.shuffle_batch(queue_list, batch_size, capacity=batch_size*128,\n                                  min_after_dequeue=batch_size*32, name=name)\n'"
chinese_hand_write_rec/src/chinese_rec.py,73,"b'import tensorflow as tf\nimport os\nimport random\nimport tensorflow.contrib.slim as slim\nimport time\nimport logging\nimport numpy as np\nimport pickle\nfrom PIL import Image\n\n\nlogger = logging.getLogger(\'Training a chinese write char recognition\')\nlogger.setLevel(logging.INFO)\n# formatter = logging.Formatter(\'%(asctime)s - %(name)s - %(levelname)s - %(message)s\')\nch = logging.StreamHandler()\nch.setLevel(logging.INFO)\nlogger.addHandler(ch)\n\n\ntf.app.flags.DEFINE_boolean(\'random_flip_up_down\', False, ""Whether to random flip up down"")\ntf.app.flags.DEFINE_boolean(\'random_brightness\', True, ""whether to adjust brightness"")\ntf.app.flags.DEFINE_boolean(\'random_contrast\', True, ""whether to random constrast"")\n\ntf.app.flags.DEFINE_integer(\'charset_size\', 3755, ""Choose the first `charset_size` character to conduct our experiment."")\ntf.app.flags.DEFINE_integer(\'image_size\', 64, ""Needs to provide same value as in training."")\ntf.app.flags.DEFINE_boolean(\'gray\', True, ""whether to change the rbg to gray"")\ntf.app.flags.DEFINE_integer(\'max_steps\', 12002, \'the max training steps \')\ntf.app.flags.DEFINE_integer(\'eval_steps\', 50, ""the step num to eval"")\ntf.app.flags.DEFINE_integer(\'save_steps\', 2000, ""the steps to save"")\n\ntf.app.flags.DEFINE_string(\'checkpoint_dir\', \'./checkpoint/\', \'the checkpoint dir\')\ntf.app.flags.DEFINE_string(\'train_data_dir\', \'../data/train/\', \'the train dataset dir\')\ntf.app.flags.DEFINE_string(\'test_data_dir\', \'../data/test/\', \'the test dataset dir\')\ntf.app.flags.DEFINE_string(\'log_dir\', \'./log\', \'the logging dir\')\n\ntf.app.flags.DEFINE_boolean(\'restore\', False, \'whether to restore from checkpoint\')\ntf.app.flags.DEFINE_boolean(\'epoch\', 1, \'Number of epoches\')\ntf.app.flags.DEFINE_boolean(\'batch_size\', 128, \'Validation batch size\')\ntf.app.flags.DEFINE_string(\'mode\', \'train\', \'Running mode. One of {""train"", ""valid"", ""test""}\')\nFLAGS = tf.app.flags.FLAGS\n\n\nclass DataIterator:\n    def __init__(self, data_dir):\n        # Set FLAGS.charset_size to a small value if available computation power is limited.\n        truncate_path = data_dir + (\'%05d\' % FLAGS.charset_size)\n        print(truncate_path)\n        self.image_names = []\n        for root, sub_folder, file_list in os.walk(data_dir):\n            if root < truncate_path:\n                self.image_names += [os.path.join(root, file_path) for file_path in file_list]\n        random.shuffle(self.image_names)\n        self.labels = [int(file_name[len(data_dir):].split(os.sep)[0]) for file_name in self.image_names]\n\n    @property\n    def size(self):\n        return len(self.labels)\n\n    @staticmethod\n    def data_augmentation(images):\n        if FLAGS.random_flip_up_down:\n            images = tf.image.random_flip_up_down(images)\n        if FLAGS.random_brightness:\n            images = tf.image.random_brightness(images, max_delta=0.3)\n        if FLAGS.random_contrast:\n            images = tf.image.random_contrast(images, 0.8, 1.2)\n        return images\n\n    def input_pipeline(self, batch_size, num_epochs=None, aug=False):\n        images_tensor = tf.convert_to_tensor(self.image_names, dtype=tf.string)\n        labels_tensor = tf.convert_to_tensor(self.labels, dtype=tf.int64)\n        input_queue = tf.train.slice_input_producer([images_tensor, labels_tensor], num_epochs=num_epochs)\n\n        labels = input_queue[1]\n        images_content = tf.read_file(input_queue[0])\n        images = tf.image.convert_image_dtype(tf.image.decode_png(images_content, channels=1), tf.float32)\n        if aug:\n            images = self.data_augmentation(images)\n        new_size = tf.constant([FLAGS.image_size, FLAGS.image_size], dtype=tf.int32)\n        images = tf.image.resize_images(images, new_size)\n        image_batch, label_batch = tf.train.shuffle_batch([images, labels], batch_size=batch_size, capacity=50000,\n                                                          min_after_dequeue=10000)\n        return image_batch, label_batch\n\n\ndef build_graph(top_k):\n    # with tf.device(\'/cpu:0\'):\n    keep_prob = tf.placeholder(dtype=tf.float32, shape=[], name=\'keep_prob\')\n    images = tf.placeholder(dtype=tf.float32, shape=[None, 64, 64, 1], name=\'image_batch\')\n    labels = tf.placeholder(dtype=tf.int64, shape=[None], name=\'label_batch\')\n\n    conv_1 = slim.conv2d(images, 64, [3, 3], 1, padding=\'SAME\', scope=\'conv1\')\n    max_pool_1 = slim.max_pool2d(conv_1, [2, 2], [2, 2], padding=\'SAME\')\n    conv_2 = slim.conv2d(max_pool_1, 128, [3, 3], padding=\'SAME\', scope=\'conv2\')\n    max_pool_2 = slim.max_pool2d(conv_2, [2, 2], [2, 2], padding=\'SAME\')\n    conv_3 = slim.conv2d(max_pool_2, 256, [3, 3], padding=\'SAME\', scope=\'conv3\')\n    max_pool_3 = slim.max_pool2d(conv_3, [2, 2], [2, 2], padding=\'SAME\')\n\n    flatten = slim.flatten(max_pool_3)\n    fc1 = slim.fully_connected(slim.dropout(flatten, keep_prob), 1024, activation_fn=tf.nn.tanh, scope=\'fc1\')\n    logits = slim.fully_connected(slim.dropout(fc1, keep_prob), FLAGS.charset_size, activation_fn=None, scope=\'fc2\')\n        # logits = slim.fully_connected(flatten, FLAGS.charset_size, activation_fn=None, reuse=reuse, scope=\'fc\')\n    loss = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits, labels=labels))\n    accuracy = tf.reduce_mean(tf.cast(tf.equal(tf.argmax(logits, 1), labels), tf.float32))\n\n    global_step = tf.get_variable(""step"", [], initializer=tf.constant_initializer(0.0), trainable=False)\n    rate = tf.train.exponential_decay(2e-4, global_step, decay_steps=2000, decay_rate=0.97, staircase=True)\n    train_op = tf.train.AdamOptimizer(learning_rate=rate).minimize(loss, global_step=global_step)\n    probabilities = tf.nn.softmax(logits)\n\n    try:\n        tf.summary.scalar(\'loss\', loss)\n        tf.summary.scalar(\'accuracy\', accuracy)\n        merged_summary_op = tf.summary.merge_all()\n    except AttributeError:\n        tf.scalar_summary(\'loss\', loss)\n        tf.scalar_summary(\'accuracy\', accuracy)\n        merged_summary_op = tf.merge_all_summary()\n    predicted_val_top_k, predicted_index_top_k = tf.nn.top_k(probabilities, k=top_k)\n    accuracy_in_top_k = tf.reduce_mean(tf.cast(tf.nn.in_top_k(probabilities, labels, top_k), tf.float32))\n\n    return {\'images\': images,\n            \'labels\': labels,\n            \'keep_prob\': keep_prob,\n            \'top_k\': top_k,\n            \'global_step\': global_step,\n            \'train_op\': train_op,\n            \'loss\': loss,\n            \'accuracy\': accuracy,\n            \'accuracy_top_k\': accuracy_in_top_k,\n            \'merged_summary_op\': merged_summary_op,\n            \'predicted_distribution\': probabilities,\n            \'predicted_index_top_k\': predicted_index_top_k,\n            \'predicted_val_top_k\': predicted_val_top_k}\n\n\ndef train():\n    print(\'Begin training\')\n    train_feeder = DataIterator(data_dir=\'../data/train/\')\n    test_feeder = DataIterator(data_dir=\'../data/test/\')\n    with tf.Session() as sess:\n        train_images, train_labels = train_feeder.input_pipeline(batch_size=FLAGS.batch_size, aug=True)\n        test_images, test_labels = test_feeder.input_pipeline(batch_size=FLAGS.batch_size)\n        graph = build_graph(top_k=1)\n        try:\n            sess.run(tf.global_variables_initializer())\n        except AttributeError:\n            sess.run(tf.initialize_all_variables())\n\n        coord = tf.train.Coordinator()\n        threads = tf.train.start_queue_runners(sess=sess, coord=coord)\n        saver = tf.train.Saver()\n\n        train_writer = tf.summary.FileWriter(FLAGS.log_dir + \'/train\', sess.graph)\n        test_writer = tf.summary.FileWriter(FLAGS.log_dir + \'/val\')\n        start_step = 0\n        if FLAGS.restore:\n            ckpt = tf.train.latest_checkpoint(FLAGS.checkpoint_dir)\n            if ckpt:\n                saver.restore(sess, ckpt)\n                print(""restore from the checkpoint {0}"".format(ckpt))\n                start_step += int(ckpt.split(\'-\')[-1])\n\n        logger.info(\':::Training Start:::\')\n        try:\n            while not coord.should_stop():\n                start_time = time.time()\n                train_images_batch, train_labels_batch = sess.run([train_images, train_labels])\n                feed_dict = {graph[\'images\']: train_images_batch,\n                             graph[\'labels\']: train_labels_batch,\n                             graph[\'keep_prob\']: 0.8}\n                _, loss_val, train_summary, step = sess.run(\n                    [graph[\'train_op\'], graph[\'loss\'], graph[\'merged_summary_op\'], graph[\'global_step\']],\n                    feed_dict=feed_dict)\n                train_writer.add_summary(train_summary, step)\n                end_time = time.time()\n                logger.info(""the step {0} takes {1} loss {2}"".format(step, end_time - start_time, loss_val))\n                if step > FLAGS.max_steps:\n                    break\n                if step % FLAGS.eval_steps == 1:\n                    test_images_batch, test_labels_batch = sess.run([test_images, test_labels])\n                    feed_dict = {graph[\'images\']: test_images_batch,\n                                 graph[\'labels\']: test_labels_batch,\n                                 graph[\'keep_prob\']: 1.0}\n                    accuracy_test, test_summary = sess.run(\n                        [graph[\'accuracy\'], graph[\'merged_summary_op\']],\n                        feed_dict=feed_dict)\n                    test_writer.add_summary(test_summary, step)\n                    logger.info(\'===============Eval a batch=======================\')\n                    logger.info(\'the step {0} test accuracy: {1}\'\n                                .format(step, accuracy_test))\n                    logger.info(\'===============Eval a batch=======================\')\n                if step % FLAGS.save_steps == 1:\n                    logger.info(\'Save the ckpt of {0}\'.format(step))\n                    saver.save(sess, os.path.join(FLAGS.checkpoint_dir, \'my-model\'),\n                               global_step=graph[\'global_step\'])\n        except tf.errors.OutOfRangeError:\n            logger.info(\'==================Train Finished================\')\n            saver.save(sess, os.path.join(FLAGS.checkpoint_dir, \'my-model\'), global_step=graph[\'global_step\'])\n        finally:\n            coord.request_stop()\n        coord.join(threads)\n\n\ndef validation():\n    print(\'validation\')\n    test_feeder = DataIterator(data_dir=\'../data/test/\')\n\n    final_predict_val = []\n    final_predict_index = []\n    groundtruth = []\n\n    with tf.Session() as sess:\n        test_images, test_labels = test_feeder.input_pipeline(batch_size=FLAGS.batch_size, num_epochs=1)\n        graph = build_graph(3)\n        try:\n            sess.run(tf.global_variables_initializer())\n            sess.run(tf.local_variables_initializer())  # initialize test_feeder\'s inside state\n        except AttributeError:\n            sess.run(tf.initialize_all_variables())\n            sess.run(tf.initialize_local_variables())  # initialize test_feeder\'s inside state\n\n        coord = tf.train.Coordinator()\n        threads = tf.train.start_queue_runners(sess=sess, coord=coord)\n\n        saver = tf.train.Saver()\n        ckpt = tf.train.latest_checkpoint(FLAGS.checkpoint_dir)\n        if ckpt:\n            saver.restore(sess, ckpt)\n            print(""restore from the checkpoint {0}"".format(ckpt))\n\n        logger.info(\':::Start validation:::\')\n        try:\n            i = 0\n            acc_top_1, acc_top_k = 0.0, 0.0\n            while not coord.should_stop():\n                i += 1\n                start_time = time.time()\n                test_images_batch, test_labels_batch = sess.run([test_images, test_labels])\n                feed_dict = {graph[\'images\']: test_images_batch,\n                             graph[\'labels\']: test_labels_batch,\n                             graph[\'keep_prob\']: 1.0}\n                batch_labels, probs, indices, acc_1, acc_k = sess.run([graph[\'labels\'],\n                                                                       graph[\'predicted_val_top_k\'],\n                                                                       graph[\'predicted_index_top_k\'],\n                                                                       graph[\'accuracy\'],\n                                                                       graph[\'accuracy_top_k\']], feed_dict=feed_dict)\n                final_predict_val += probs.tolist()\n                final_predict_index += indices.tolist()\n                groundtruth += batch_labels.tolist()\n                acc_top_1 += acc_1\n                acc_top_k += acc_k\n                end_time = time.time()\n                logger.info(""the batch {0} takes {1} seconds, accuracy = {2}(top_1) {3}(top_k)""\n                            .format(i, end_time - start_time, acc_1, acc_k))\n\n        except tf.errors.OutOfRangeError:\n            logger.info(\'==================Validation Finished================\')\n            acc_top_1 = acc_top_1 * FLAGS.batch_size / test_feeder.size\n            acc_top_k = acc_top_k * FLAGS.batch_size / test_feeder.size\n            logger.info(\'top 1 accuracy {0} top k accuracy {1}\'.format(acc_top_1, acc_top_k))\n        finally:\n            coord.request_stop()\n        coord.join(threads)\n    return {\'prob\': final_predict_val, \'indices\': final_predict_index, \'groundtruth\': groundtruth}\n\n\ndef inference(image):\n    print(\'inference\')\n    temp_image = Image.open(image).convert(\'L\')\n    temp_image = temp_image.resize((FLAGS.image_size, FLAGS.image_size), Image.ANTIALIAS)\n    temp_image = np.asarray(temp_image) / 255.0\n    temp_image = temp_image.reshape([-1, 64, 64, 1])\n    with tf.Session() as sess:\n        logger.info(\'========start inference============\')\n        # images = tf.placeholder(dtype=tf.float32, shape=[None, 64, 64, 1])\n        # Pass a shadow label 0. This label will not affect the computation graph.\n        graph = build_graph(top_k=3)\n        saver = tf.train.Saver()\n        ckpt = tf.train.latest_checkpoint(FLAGS.checkpoint_dir)\n        if ckpt:\n            saver.restore(sess, ckpt)\n        predict_val, predict_index = sess.run([graph[\'predicted_val_top_k\'], graph[\'predicted_index_top_k\']],\n                                              feed_dict={graph[\'images\']: temp_image, graph[\'keep_prob\']: 1.0})\n    return predict_val, predict_index\n\n\ndef main(_):\n    print(FLAGS.mode)\n    if FLAGS.mode == ""train"":\n        train()\n    elif FLAGS.mode == \'validation\':\n        dct = validation()\n        result_file = \'result.dict\'\n        logger.info(\'Write result into {0}\'.format(result_file))\n        with open(result_file, \'wb\') as f:\n            pickle.dump(dct, f)\n        logger.info(\'Write file ends\')\n    elif FLAGS.mode == \'inference\':\n        image_path = \'../data/test/00190/13320.png\'\n        final_predict_val, final_predict_index = inference(image_path)\n        logger.info(\'the result info label {0} predict index {1} predict_val {2}\'.format(190, final_predict_index,\n                                                                                         final_predict_val))\n\nif __name__ == ""__main__"":\n    tf.app.run()\n'"
chinese_hand_write_rec/src/data_to_png.py,0,"b'import os\nimport numpy as np\nimport struct\nfrom PIL import Image\n\n    \ndata_dir = \'../data\'\n# train_data_dir = ""../data/HWDB1.1trn_gnt""\ntrain_data_dir = os.path.join(data_dir, \'HWDB1.1trn_gnt\')\ntest_data_dir = os.path.join(data_dir, \'HWDB1.1tst_gnt\')\n\n\ndef read_from_gnt_dir(gnt_dir=train_data_dir):\n    def one_file(f):\n        header_size = 10\n        while True:\n            header = np.fromfile(f, dtype=\'uint8\', count=header_size)\n            if not header.size: break\n            sample_size = header[0] + (header[1]<<8) + (header[2]<<16) + (header[3]<<24)\n            tagcode = header[5] + (header[4]<<8)\n            width = header[6] + (header[7]<<8)\n            height = header[8] + (header[9]<<8)\n            if header_size + width*height != sample_size:\n                break\n            image = np.fromfile(f, dtype=\'uint8\', count=width*height).reshape((height, width))\n            yield image, tagcode\n    for file_name in os.listdir(gnt_dir):\n        if file_name.endswith(\'.gnt\'):\n            file_path = os.path.join(gnt_dir, file_name)\n            with open(file_path, \'rb\') as f:\n                for image, tagcode in one_file(f):\n                    yield image, tagcode\nchar_set = set()\nfor _, tagcode in read_from_gnt_dir(gnt_dir=train_data_dir):\n    tagcode_unicode = struct.pack(\'>H\', tagcode).decode(\'gb2312\')\n    char_set.add(tagcode_unicode)\nchar_list = list(char_set)\nchar_dict = dict(zip(sorted(char_list), range(len(char_list))))\nprint len(char_dict)\nimport pickle\nf = open(\'char_dict\', \'wb\')\npickle.dump(char_dict, f)\nf.close()\ntrain_counter = 0\ntest_counter = 0\nfor image, tagcode in read_from_gnt_dir(gnt_dir=train_data_dir):\n    tagcode_unicode = struct.pack(\'>H\', tagcode).decode(\'gb2312\')\n    im = Image.fromarray(image)\n    dir_name = \'../data/train/\' + \'%0.5d\'%char_dict[tagcode_unicode]\n    if not os.path.exists(dir_name):\n        os.mkdir(dir_name)\n    im.convert(\'RGB\').save(dir_name+\'/\' + str(train_counter) + \'.png\')\n    train_counter += 1\nfor image, tagcode in read_from_gnt_dir(gnt_dir=test_data_dir):\n    tagcode_unicode = struct.pack(\'>H\', tagcode).decode(\'gb2312\')\n    im = Image.fromarray(image)\n    dir_name = \'../data/test/\' + \'%0.5d\'%char_dict[tagcode_unicode]\n    if not os.path.exists(dir_name):\n        os.mkdir(dir_name)\n    im.convert(\'RGB\').save(dir_name+\'/\' + str(test_counter) + \'.png\')\n    test_counter += 1\n'"
finetuning/convert_pys/covert_datasets_tfrecord.py,15,"b'#-*-coding:utf-8-*-\n""""""\nThis script is used to convert the images dataset of folder to tfrecord.\nThe image data set is expected to reside in JPEG files located in the following directory structure.\n\n    data_dir/label_0/image0.jpg\n    data_dir/label_1/image1.jpg\n    ...\nAnd this script will converts the traning and eval data into a sharded data set consisting of TFRecord files\n\n    train_dir/train-00000-of-01024\n    train_dir/train-00001-of-01024\n    ...\nand\n    val_dir/validation-00000-of-00128\n    val_dir/validation-00001-0f-00128\n    ...\n""""""\nimport tensorflow as tf\nimport os\nimport random\nimport math\nimport sys\n\n_NUM_SHARDS = 1\n_RANDOM_SEED = 0\nLABELS_FILENAME = \'labels.txt\'\n\n\nclass ImageReader(object):\n    """"""Helper class that provides TensorFlow image coding utilities.""""""\n\n    def __init__(self):\n        # Initializes function that decodes RGB JPEG data.\n        self._decode_jpeg_data = tf.placeholder(dtype=tf.string)\n        self._decode_jpeg = tf.image.decode_jpeg(\n            self._decode_jpeg_data, channels=3)\n\n    def read_image_dims(self, sess, image_data):\n        image = self.decode_jpeg(sess, image_data)\n        return image.shape[0], image.shape[1]\n\n    def decode_jpeg(self, sess, image_data):\n        image = sess.run(self._decode_jpeg,\n                         feed_dict={self._decode_jpeg_data: image_data})\n        assert len(image.shape) == 3\n        assert image.shape[2] == 3\n        return image\n\n\ndef int64_feature(values):\n    """"""Returns a TF-Feature of int64s.\n\n  Args:\n    values: A scalar or list of values.\n\n  Returns:\n    a TF-Feature.\n  """"""\n    if not isinstance(values, (tuple, list)):\n        values = [values]\n    return tf.train.Feature(int64_list=tf.train.Int64List(value=values))\n\n\ndef bytes_feature(values):\n    """"""Returns a TF-Feature of bytes.\n\n  Args:\n    values: A string.\n\n  Returns:\n    a TF-Feature.\n  """"""\n    return tf.train.Feature(bytes_list=tf.train.BytesList(value=[values]))\n\n\ndef _get_dataset_filename(dataset_dir, split_name, shard_id, nFold):\n    output_filename = ""fisher_%s_%05d-of-nFold-%d-%05d.tfrecord"" % (\n        split_name, shard_id, nFold, _NUM_SHARDS)\n    return os.path.join(dataset_dir, output_filename)\n\n\ndef _dataset_exists(dataset_dir, nFold):\n    for split_name in [\'train\', \'validation\']:\n        for shard_id in range(_NUM_SHARDS):\n            output_filename = _get_dataset_filename(dataset_dir, split_name,\n                                                    nFold, shard_id)\n            if not tf.gfile.Exists(output_filename):\n                return False\n    return True\n\n\ndef _get_filenames_and_classes(dataset_dir, dataset_name):\n    sex_detection_root = os.path.join(dataset_dir, dataset_name)\n    direcotries = []\n    class_names = []\n    for filename in os.listdir(sex_detection_root):\n        path = os.path.join(sex_detection_root, filename)\n        if os.path.isdir(path):\n            direcotries.append(path)\n            class_names.append(filename)\n\n    image_filenames = []\n    for dir in direcotries:\n        for filename in os.listdir(dir):\n            path = os.path.join(dir, filename)\n            image_filenames.append(path)\n    return image_filenames, sorted(class_names)\n\n\ndef image_to_tfexample(image_data, image_format, height, width, class_id):\n    return tf.train.Example(features=tf.train.Features(feature={\n        \'image/encoded\': bytes_feature(image_data),\n        \'image/format\': bytes_feature(image_format),\n        \'image/class/label\': int64_feature(class_id),\n        \'image/height\': int64_feature(height),\n        \'image/width\': int64_feature(width),\n    }))\n\n\n# def _clean_up_temporary_files(dataset_dir, dataset_name):\n#   """"""Removes temporary files used to create the dataset.\n\n#   Args:\n#     dataset_dir: The directory where the temporary files are stored.\n#   """"""\n#   filename = _DATA_URL.split(\'/\')[-1]\n#   filepath = os.path.join(dataset_dir, filename)\n#   tf.gfile.Remove(filepath)\n\n#   tmp_dir = os.path.join(dataset_dir, dataset_name)\n#   tf.gfile.DeleteRecursively(tmp_dir)\n\n\ndef write_label_file(labels_to_class_names,\n                     dataset_dir,\n                     filename=LABELS_FILENAME):\n    """"""Writes a file with the list of class names.\n\n  Args:\n    labels_to_class_names: A map of (integer) labels to class names.\n    dataset_dir: The directory in which the labels file should be written.\n    filename: The filename where the class names are written.\n  """"""\n    labels_filename = os.path.join(dataset_dir, filename)\n    with tf.gfile.Open(labels_filename, \'w\') as f:\n        for label in labels_to_class_names:\n            class_name = labels_to_class_names[label]\n            f.write(\'%d:%s\\n\' % (label, class_name))\n\n\ndef _convert_dataset(split_name, filenames, class_names_to_ids, dataset_dir,\n                     nFold):\n    assert split_name in [\'train\', \'validation\']\n    num_per_shard = int(math.ceil(len(filenames) / float(_NUM_SHARDS)))\n    with tf.Graph().as_default():\n        image_reader = ImageReader()\n        with tf.Session(\'\') as sess:\n            for shard_id in range(_NUM_SHARDS):\n                output_filename = _get_dataset_filename(\n                    dataset_dir, split_name, shard_id, nFold)\n                print \'output_file: %s\' % output_filename\n                with tf.python_io.TFRecordWriter(\n                        output_filename) as tfrecord_writer:\n                    start_ndx = shard_id * num_per_shard\n                    end_ndx = min((shard_id + 1) * num_per_shard,\n                                  len(filenames))\n                    for i in range(start_ndx, end_ndx):\n                        # print(filenames[i+1])\n                        sys.stdout.write(\'\\r>> Converting image %d/%d shard %d\'\n                                         % (i + 1, len(filenames), shard_id))\n                        sys.stdout.flush()\n                        image_data = tf.gfile.FastGFile(filenames[i],\n                                                        \'r\').read()\n                        height, width = image_reader.read_image_dims(\n                            sess, image_data)\n                        class_name = os.path.basename(\n                            os.path.dirname(filenames[i]))\n                        class_id = class_names_to_ids[class_name]\n\n                        example = image_to_tfexample(image_data, \'jpg\', height,\n                                                     width, class_id)\n                        tfrecord_writer.write(example.SerializeToString())\n    sys.stdout.write(\'\\n\')\n    sys.stdout.flush()\n\n\ndef run(FLAGS):\n    """"""\n    Args: dataset_dir : the dataset dir where the dataset is stored.\n    """"""\n    dataset_dir = FLAGS.dataset_dir\n    dataset_name = FLAGS.dataset_name\n    nFold = FLAGS.nFold\n\n    if not tf.gfile.Exists(dataset_dir):\n        tf.gfile.MakeDirs(dataset_dir)\n\n    if _dataset_exists(dataset_dir, nFold):\n        print(\n            ""Dataset files already exist. Existing without re-creating them."")\n\n    images_filenames, class_names = _get_filenames_and_classes(dataset_dir,\n                                                               dataset_name)\n    class_names_to_ids = dict(zip(class_names, range(len(class_names))))\n\n    random.seed(_RANDOM_SEED)\n    random.shuffle(images_filenames)\n    _NUM_VALIDATION = len(images_filenames) / nFold\n    for i in range(nFold):\n        start = i * _NUM_VALIDATION\n        end = (1 + i) * _NUM_VALIDATION\n        tmp_validation_filenames = images_filenames[start:end]\n        tmp_training_filenames = images_filenames[:start] + images_filenames[\n            end:]\n        _convert_dataset(\'train\', tmp_training_filenames, class_names_to_ids,\n                         dataset_dir, i)\n        _convert_dataset(\'validation\', tmp_validation_filenames,\n                         class_names_to_ids, dataset_dir, i)\n\n    labels_to_class_names = dict(zip(range(len(class_names)), class_names))\n    write_label_file(labels_to_class_names, dataset_dir)\n\n    # _clean_up_temporary_files(dataset_dir,dataset_name)\n    print(\'\\n Finised convering the sex detection dataset!\')\n'"
finetuning/convert_pys/covert_somedata_to_tfrecord.py,5,"b'from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\nimport covert_datasets_tfrecord\n\nFLAGS = tf.app.flags.FLAGS\n\ntf.app.flags.DEFINE_string(\n    \'dataset_name\', None,\n    \'The name of the dataset to convert, one of ""cifar10"", ""flowers"", ""mnist"".\')\n\ntf.app.flags.DEFINE_string(\n    \'dataset_dir\', None,\n    \'The directory where the output TFRecords and temporary files are saved.\')\n\ntf.app.flags.DEFINE_integer(\'nFold\', 1, ""The nFold of Cross validation."")\n\n\ndef main(_):\n    if not FLAGS.dataset_name:\n        raise ValueError(\n            \'You must supply the dataset name with --dataset_name\')\n    if not FLAGS.dataset_dir:\n        raise ValueError(\'You must supply the dataset name with --dataset_dir\')\n\n    covert_datasets_tfrecord.run(FLAGS)\n\n\nif __name__ == \'__main__\':\n    tf.app.run()'"
finetuning/datasets/__init__.py,0,b''
finetuning/datasets/dataset_factory.py,1,"b'from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom datasets import fisher\n\ndatasets_map = {\'fisher\': fisher}\n\n\ndef get_dataset(name, split_name, dataset_dir, file_pattern=None, reader=None):\n    """"""Given a dataset name and a split_name returns a Dataset.\n\n  Args:\n    name: String, the name of the dataset.\n    split_name: A train/test split name.\n    dataset_dir: The directory where the dataset files are stored.\n    file_pattern: The file pattern to use for matching the dataset source files.\n    reader: The subclass of tf.ReaderBase. If left as `None`, then the default\n      reader defined by each dataset is used.\n\n  Returns:\n    A `Dataset` class.\n\n  Raises:\n    ValueError: If the dataset `name` is unknown.\n  """"""\n    if name not in datasets_map:\n        raise ValueError(\'Name of dataset unknown %s\' % name)\n    return datasets_map[name].get_split(split_name, dataset_dir, file_pattern,\n                                        reader)\n'"
finetuning/datasets/dataset_utils.py,6,"b'# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Contains utilities for downloading and converting datasets.""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport os\nimport sys\nimport tarfile\n\nfrom six.moves import urllib\nimport tensorflow as tf\n\nLABELS_FILENAME = \'labels.txt\'\n\n\ndef int64_feature(values):\n    """"""Returns a TF-Feature of int64s.\n\n  Args:\n    values: A scalar or list of values.\n\n  Returns:\n    a TF-Feature.\n  """"""\n    if not isinstance(values, (tuple, list)):\n        values = [values]\n    return tf.train.Feature(int64_list=tf.train.Int64List(value=values))\n\n\ndef bytes_feature(values):\n    """"""Returns a TF-Feature of bytes.\n\n  Args:\n    values: A string.\n\n  Returns:\n    a TF-Feature.\n  """"""\n    return tf.train.Feature(bytes_list=tf.train.BytesList(value=[values]))\n\n\ndef image_to_tfexample(image_data, image_format, height, width, class_id):\n    return tf.train.Example(features=tf.train.Features(feature={\n        \'image/encoded\': bytes_feature(image_data),\n        \'image/format\': bytes_feature(image_format),\n        \'image/class/label\': int64_feature(class_id),\n        \'image/height\': int64_feature(height),\n        \'image/width\': int64_feature(width),\n    }))\n\n\ndef download_and_uncompress_tarball(tarball_url, dataset_dir):\n    """"""Downloads the `tarball_url` and uncompresses it locally.\n\n  Args:\n    tarball_url: The URL of a tarball file.\n    dataset_dir: The directory where the temporary files are stored.\n  """"""\n    filename = tarball_url.split(\'/\')[-1]\n    filepath = os.path.join(dataset_dir, filename)\n\n    def _progress(count, block_size, total_size):\n        sys.stdout.write(\'\\r>> Downloading %s %.1f%%\' % (\n            filename, float(count * block_size) / float(total_size) * 100.0))\n        sys.stdout.flush()\n\n    filepath, _ = urllib.request.urlretrieve(tarball_url, filepath, _progress)\n    print()\n    statinfo = os.stat(filepath)\n    print(\'Successfully downloaded\', filename, statinfo.st_size, \'bytes.\')\n    tarfile.open(filepath, \'r:gz\').extractall(dataset_dir)\n\n\ndef write_label_file(labels_to_class_names,\n                     dataset_dir,\n                     filename=LABELS_FILENAME):\n    """"""Writes a file with the list of class names.\n\n  Args:\n    labels_to_class_names: A map of (integer) labels to class names.\n    dataset_dir: The directory in which the labels file should be written.\n    filename: The filename where the class names are written.\n  """"""\n    labels_filename = os.path.join(dataset_dir, filename)\n    with tf.gfile.Open(labels_filename, \'w\') as f:\n        for label in labels_to_class_names:\n            class_name = labels_to_class_names[label]\n            f.write(\'%d:%s\\n\' % (label, class_name))\n\n\ndef has_labels(dataset_dir, filename=LABELS_FILENAME):\n    """"""Specifies whether or not the dataset directory contains a label map file.\n\n  Args:\n    dataset_dir: The directory in which the labels file is found.\n    filename: The filename where the class names are written.\n\n  Returns:\n    `True` if the labels file exists and `False` otherwise.\n  """"""\n    return tf.gfile.Exists(os.path.join(dataset_dir, filename))\n\n\ndef read_label_file(dataset_dir, filename=LABELS_FILENAME):\n    """"""Reads the labels file and returns a mapping from ID to class name.\n\n  Args:\n    dataset_dir: The directory in which the labels file is found.\n    filename: The filename where the class names are written.\n\n  Returns:\n    A map from a label (integer) to class name.\n  """"""\n    labels_filename = os.path.join(dataset_dir, filename)\n    with tf.gfile.Open(labels_filename, \'r\') as f:\n        lines = f.read().decode()\n    lines = lines.split(\'\\n\')\n    lines = filter(None, lines)\n\n    labels_to_class_names = {}\n    for line in lines:\n        index = line.index(\':\')\n        labels_to_class_names[int(line[:index])] = line[index + 1:]\n    return labels_to_class_names\n'"
finetuning/datasets/fisher.py,9,"b'# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Provides data for the Cifar10 dataset.\n\nThe dataset scripts used to create the dataset can be found at:\ntensorflow/models/slim/data/create_cifar10_dataset.py\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport os\nimport tensorflow as tf\n\nfrom datasets import dataset_utils\n\nslim = tf.contrib.slim\n\n_FILE_PATTERN = \'fisher_%s_*.tfrecord\'\n\nSPLITS_TO_SIZES = {\'train\': 3320, \'validation\': 350}\n\n_NUM_CLASSES = 8\n\n_ITEMS_TO_DESCRIPTIONS = {\n    \'image\': \'A color image of varying size.\',\n    \'label\': \'A single integer between 0 and 7\',\n}\n\n\ndef get_split(split_name, dataset_dir, file_pattern=None, reader=None):\n    """"""Gets a dataset tuple with instructions for reading cifar10.\n\n  Args:\n    split_name: A train/validation split name.\n    dataset_dir: The base directory of the dataset sources.\n    file_pattern: The file pattern to use when matching the dataset sources.\n      It is assumed that the pattern contains a \'%s\' string so that the split\n      name can be inserted.\n    reader: The TensorFlow reader type.\n\n  Returns:\n    A `Dataset` namedtuple.\n\n  Raises:\n    ValueError: if `split_name` is not a valid train/validation split.\n  """"""\n    if split_name not in SPLITS_TO_SIZES:\n        raise ValueError(\'split name %s was not recognized.\' % split_name)\n\n    if not file_pattern:\n        file_pattern = _FILE_PATTERN\n    file_pattern = os.path.join(dataset_dir, file_pattern % split_name)\n\n    # Allowing None in the signature so that dataset_factory can use the default.\n    if reader is None:\n        reader = tf.TFRecordReader\n\n    keys_to_features = {\n        \'image/encoded\': tf.FixedLenFeature(\n            (), tf.string, default_value=\'\'),\n        \'image/format\': tf.FixedLenFeature(\n            (), tf.string, default_value=\'png\'),\n        \'image/class/label\': tf.FixedLenFeature(\n            [], tf.int64, default_value=tf.zeros(\n                [], dtype=tf.int64)),\n    }\n\n    items_to_handlers = {\n        \'image\': slim.tfexample_decoder.Image(),\n        \'label\': slim.tfexample_decoder.Tensor(\'image/class/label\'),\n    }\n\n    decoder = slim.tfexample_decoder.TFExampleDecoder(keys_to_features,\n                                                      items_to_handlers)\n\n    labels_to_names = None\n    if dataset_utils.has_labels(dataset_dir):\n        labels_to_names = dataset_utils.read_label_file(dataset_dir)\n\n    return slim.dataset.Dataset(\n        data_sources=file_pattern,\n        reader=reader,\n        decoder=decoder,\n        num_samples=SPLITS_TO_SIZES[split_name],\n        items_to_descriptions=_ITEMS_TO_DESCRIPTIONS,\n        num_classes=_NUM_CLASSES,\n        labels_to_names=labels_to_names)\n'"
finetuning/deployment/__init__.py,0,b''
finetuning/deployment/model_deploy.py,60,"b'# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Deploy Slim models across multiple clones and replicas.\n\n# TODO(sguada) docstring paragraph by (a) motivating the need for the file and\n# (b) defining clones.\n\n# TODO(sguada) describe the high-level components of model deployment.\n# E.g. ""each model deployment is composed of several parts: a DeploymentConfig,\n# which captures A, B and C, an input_fn which loads data.. etc\n\nTo easily train a model on multiple GPUs or across multiple machines this\nmodule provides a set of helper functions: `create_clones`,\n`optimize_clones` and `deploy`.\n\nUsage:\n\n  g = tf.Graph()\n\n  # Set up DeploymentConfig\n  config = model_deploy.DeploymentConfig(num_clones=2, clone_on_cpu=True)\n\n  # Create the global step on the device storing the variables.\n  with tf.device(config.variables_device()):\n    global_step = slim.create_global_step()\n\n  # Define the inputs\n  with tf.device(config.inputs_device()):\n    images, labels = LoadData(...)\n    inputs_queue = slim.data.prefetch_queue((images, labels))\n\n  # Define the optimizer.\n  with tf.device(config.optimizer_device()):\n    optimizer = tf.train.MomentumOptimizer(FLAGS.learning_rate, FLAGS.momentum)\n\n  # Define the model including the loss.\n  def model_fn(inputs_queue):\n    images, labels = inputs_queue.dequeue()\n    predictions = CreateNetwork(images)\n    slim.losses.log_loss(predictions, labels)\n\n  model_dp = model_deploy.deploy(config, model_fn, [inputs_queue],\n                                 optimizer=optimizer)\n\n  # Run training.\n  slim.learning.train(model_dp.train_op, my_log_dir,\n                      summary_op=model_dp.summary_op)\n\nThe Clone namedtuple holds together the values associated with each call to\nmodel_fn:\n  * outputs: The return values of the calls to `model_fn()`.\n  * scope: The scope used to create the clone.\n  * device: The device used to create the clone.\n\nDeployedModel namedtuple, holds together the values needed to train multiple\nclones:\n  * train_op: An operation that run the optimizer training op and include\n    all the update ops created by `model_fn`. Present only if an optimizer\n    was specified.\n  * summary_op: An operation that run the summaries created by `model_fn`\n    and process_gradients.\n  * total_loss: A `Tensor` that contains the sum of all losses created by\n    `model_fn` plus the regularization losses.\n  * clones: List of `Clone` tuples returned by `create_clones()`.\n\nDeploymentConfig parameters:\n  * num_clones: Number of model clones to deploy in each replica.\n  * clone_on_cpu: True if clones should be placed on CPU.\n  * replica_id: Integer.  Index of the replica for which the model is\n      deployed.  Usually 0 for the chief replica.\n  * num_replicas: Number of replicas to use.\n  * num_ps_tasks: Number of tasks for the `ps` job. 0 to not use replicas.\n  * worker_job_name: A name for the worker job.\n  * ps_job_name: A name for the parameter server job.\n\nTODO(sguada):\n  - describe side effect to the graph.\n  - what happens to summaries and update_ops.\n  - which graph collections are altered.\n  - write a tutorial on how to use this.\n  - analyze the possibility of calling deploy more than once.\n\n\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport collections\n\nimport tensorflow as tf\n\nfrom tensorflow.python.ops import control_flow_ops\n\nslim = tf.contrib.slim\n\n__all__ = [\n    \'create_clones\',\n    \'deploy\',\n    \'optimize_clones\',\n    \'DeployedModel\',\n    \'DeploymentConfig\',\n    \'Clone\',\n]\n\n# Namedtuple used to represent a clone during deployment.\nClone = collections.namedtuple(\n    \'Clone\',\n    [\n        \'outputs\',  # Whatever model_fn() returned.\n        \'scope\',  # The scope used to create it.\n        \'device\',  # The device used to create.\n    ])\n\n# Namedtuple used to represent a DeployedModel, returned by deploy().\nDeployedModel = collections.namedtuple(\n    \'DeployedModel\',\n    [\n        \'train_op\',  # The `train_op`\n        \'summary_op\',  # The `summary_op`\n        \'total_loss\',  # The loss `Tensor`\n        \'clones\',  # A list of `Clones` tuples.\n    ])\n\n# Default parameters for DeploymentConfig\n_deployment_params = {\n    \'num_clones\': 1,\n    \'clone_on_cpu\': False,\n    \'replica_id\': 0,\n    \'num_replicas\': 1,\n    \'num_ps_tasks\': 0,\n    \'worker_job_name\': \'worker\',\n    \'ps_job_name\': \'ps\'\n}\n\n\ndef create_clones(config, model_fn, args=None, kwargs=None):\n    """"""Creates multiple clones according to config using a `model_fn`.\n\n  The returned values of `model_fn(*args, **kwargs)` are collected along with\n  the scope and device used to created it in a namedtuple\n  `Clone(outputs, scope, device)`\n\n  Note: it is assumed that any loss created by `model_fn` is collected at\n  the tf.GraphKeys.LOSSES collection.\n\n  To recover the losses, summaries or update_ops created by the clone use:\n  ```python\n    losses = tf.get_collection(tf.GraphKeys.LOSSES, clone.scope)\n    summaries = tf.get_collection(tf.GraphKeys.SUMMARIES, clone.scope)\n    update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS, clone.scope)\n  ```\n\n  The deployment options are specified by the config object and support\n  deploying one or several clones on different GPUs and one or several replicas\n  of such clones.\n\n  The argument `model_fn` is called `config.num_clones` times to create the\n  model clones as `model_fn(*args, **kwargs)`.\n\n  If `config` specifies deployment on multiple replicas then the default\n  tensorflow device is set appropriatly for each call to `model_fn` and for the\n  slim variable creation functions: model and global variables will be created\n  on the `ps` device, the clone operations will be on the `worker` device.\n\n  Args:\n    config: A DeploymentConfig object.\n    model_fn: A callable. Called as `model_fn(*args, **kwargs)`\n    args: Optional list of arguments to pass to `model_fn`.\n    kwargs: Optional list of keyword arguments to pass to `model_fn`.\n\n  Returns:\n    A list of namedtuples `Clone`.\n  """"""\n    clones = []\n    args = args or []\n    kwargs = kwargs or {}\n    with slim.arg_scope(\n        [slim.model_variable, slim.variable],\n            device=config.variables_device()):\n        # Create clones.\n        for i in range(0, config.num_clones):\n            with tf.name_scope(config.clone_scope(i)) as clone_scope:\n                clone_device = config.clone_device(i)\n                with tf.device(clone_device):\n                    with tf.variable_scope(\n                            tf.get_variable_scope(), reuse=True\n                            if i > 0 else None):\n                        outputs = model_fn(*args, **kwargs)\n                    clones.append(Clone(outputs, clone_scope, clone_device))\n    return clones\n\n\ndef _gather_clone_loss(clone, num_clones, regularization_losses):\n    """"""Gather the loss for a single clone.\n\n  Args:\n    clone: A Clone namedtuple.\n    num_clones: The number of clones being deployed.\n    regularization_losses: Possibly empty list of regularization_losses\n      to add to the clone losses.\n\n  Returns:\n    A tensor for the total loss for the clone.  Can be None.\n  """"""\n    # The return value.\n    sum_loss = None\n    # Individual components of the loss that will need summaries.\n    clone_loss = None\n    regularization_loss = None\n    # Compute and aggregate losses on the clone device.\n    with tf.device(clone.device):\n        all_losses = []\n        clone_losses = tf.get_collection(tf.GraphKeys.LOSSES, clone.scope)\n        if clone_losses:\n            clone_loss = tf.add_n(clone_losses, name=\'clone_loss\')\n            if num_clones > 1:\n                clone_loss = tf.div(clone_loss,\n                                    1.0 * num_clones,\n                                    name=\'scaled_clone_loss\')\n            all_losses.append(clone_loss)\n        if regularization_losses:\n            regularization_loss = tf.add_n(\n                regularization_losses, name=\'regularization_loss\')\n            all_losses.append(regularization_loss)\n        if all_losses:\n            sum_loss = tf.add_n(all_losses)\n    # Add the summaries out of the clone device block.\n    if clone_loss is not None:\n        try:\n          tf.summary.scalar(\n              clone.scope + \'/clone_loss\', clone_loss, name=\'clone_loss\')\n        except AttributeError:\n          tf.scalar_summary(\n              clone.scope + \'/clone_loss\', clone_loss, name=\'clone_loss\')\n    if regularization_loss is not None:\n        try:\n            tf.summary.scalar(\n                \'regularization_loss\',\n                regularization_loss,\n                name=\'regularization_loss\')\n        except AttributeError:\n            tf.scalar_summary(\n                \'regularization_loss\',\n                regularization_loss,\n                name=\'regularization_loss\')\n    return sum_loss\n\n\ndef _optimize_clone(optimizer, clone, num_clones, regularization_losses,\n                    **kwargs):\n    """"""Compute losses and gradients for a single clone.\n\n  Args:\n    optimizer: A tf.Optimizer  object.\n    clone: A Clone namedtuple.\n    num_clones: The number of clones being deployed.\n    regularization_losses: Possibly empty list of regularization_losses\n      to add to the clone losses.\n    **kwargs: Dict of kwarg to pass to compute_gradients().\n\n  Returns:\n    A tuple (clone_loss, clone_grads_and_vars).\n      - clone_loss: A tensor for the total loss for the clone.  Can be None.\n      - clone_grads_and_vars: List of (gradient, variable) for the clone.\n        Can be empty.\n  """"""\n    sum_loss = _gather_clone_loss(clone, num_clones, regularization_losses)\n    clone_grad = None\n    if sum_loss is not None:\n        with tf.device(clone.device):\n            clone_grad = optimizer.compute_gradients(sum_loss, **kwargs)\n    return sum_loss, clone_grad\n\n\ndef optimize_clones(clones, optimizer, regularization_losses=None, **kwargs):\n    """"""Compute clone losses and gradients for the given list of `Clones`.\n\n  Note: The regularization_losses are added to the first clone losses.\n\n  Args:\n   clones: List of `Clones` created by `create_clones()`.\n   optimizer: An `Optimizer` object.\n   regularization_losses: Optional list of regularization losses. If None it\n     will gather them from tf.GraphKeys.REGULARIZATION_LOSSES. Pass `[]` to\n     exclude them.\n   **kwargs: Optional list of keyword arguments to pass to `compute_gradients`.\n\n  Returns:\n   A tuple (total_loss, grads_and_vars).\n     - total_loss: A Tensor containing the average of the clone losses including\n       the regularization loss.\n     - grads_and_vars: A List of tuples (gradient, variable) containing the sum\n       of the gradients for each variable.\n\n  """"""\n    grads_and_vars = []\n    clones_losses = []\n    num_clones = len(clones)\n    if regularization_losses is None:\n        regularization_losses = tf.get_collection(\n            tf.GraphKeys.REGULARIZATION_LOSSES)\n    for clone in clones:\n        with tf.name_scope(clone.scope):\n            clone_loss, clone_grad = _optimize_clone(\n                optimizer, clone, num_clones, regularization_losses, **kwargs)\n            if clone_loss is not None:\n                clones_losses.append(clone_loss)\n                grads_and_vars.append(clone_grad)\n            # Only use regularization_losses for the first clone\n            regularization_losses = None\n    # Compute the total_loss summing all the clones_losses.\n    total_loss = tf.add_n(clones_losses, name=\'total_loss\')\n    # Sum the gradients accross clones.\n    grads_and_vars = _sum_clones_gradients(grads_and_vars)\n    return total_loss, grads_and_vars\n\n\ndef deploy(config,\n           model_fn,\n           args=None,\n           kwargs=None,\n           optimizer=None,\n           summarize_gradients=False):\n    """"""Deploys a Slim-constructed model across multiple clones.\n\n  The deployment options are specified by the config object and support\n  deploying one or several clones on different GPUs and one or several replicas\n  of such clones.\n\n  The argument `model_fn` is called `config.num_clones` times to create the\n  model clones as `model_fn(*args, **kwargs)`.\n\n  The optional argument `optimizer` is an `Optimizer` object.  If not `None`,\n  the deployed model is configured for training with that optimizer.\n\n  If `config` specifies deployment on multiple replicas then the default\n  tensorflow device is set appropriatly for each call to `model_fn` and for the\n  slim variable creation functions: model and global variables will be created\n  on the `ps` device, the clone operations will be on the `worker` device.\n\n  Args:\n    config: A `DeploymentConfig` object.\n    model_fn: A callable. Called as `model_fn(*args, **kwargs)`\n    args: Optional list of arguments to pass to `model_fn`.\n    kwargs: Optional list of keyword arguments to pass to `model_fn`.\n    optimizer: Optional `Optimizer` object.  If passed the model is deployed\n      for training with that optimizer.\n    summarize_gradients: Whether or not add summaries to the gradients.\n\n  Returns:\n    A `DeployedModel` namedtuple.\n\n  """"""\n    # Gather initial summaries.\n    summaries = set(tf.get_collection(tf.GraphKeys.SUMMARIES))\n\n    # Create Clones.\n    clones = create_clones(config, model_fn, args, kwargs)\n    first_clone = clones[0]\n\n    # Gather update_ops from the first clone. These contain, for example,\n    # the updates for the batch_norm variables created by model_fn.\n    update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS, first_clone.scope)\n\n    train_op = None\n    total_loss = None\n    with tf.device(config.optimizer_device()):\n        if optimizer:\n            # Place the global step on the device storing the variables.\n            with tf.device(config.variables_device()):\n                global_step = slim.get_or_create_global_step()\n\n            # Compute the gradients for the clones.\n            total_loss, clones_gradients = optimize_clones(clones, optimizer)\n\n            if clones_gradients:\n                if summarize_gradients:\n                    # Add summaries to the gradients.\n                    summaries |= set(\n                        _add_gradients_summaries(clones_gradients))\n\n                # Create gradient updates.\n                grad_updates = optimizer.apply_gradients(\n                    clones_gradients, global_step=global_step)\n                update_ops.append(grad_updates)\n\n                update_op = tf.group(*update_ops)\n                train_op = control_flow_ops.with_dependencies(\n                    [update_op], total_loss, name=\'train_op\')\n        else:\n            clones_losses = []\n            regularization_losses = tf.get_collection(\n                tf.GraphKeys.REGULARIZATION_LOSSES)\n            for clone in clones:\n                with tf.name_scope(clone.scope):\n                    clone_loss = _gather_clone_loss(clone,\n                                                    len(clones),\n                                                    regularization_losses)\n                    if clone_loss is not None:\n                        clones_losses.append(clone_loss)\n                    # Only use regularization_losses for the first clone\n                    regularization_losses = None\n            if clones_losses:\n                total_loss = tf.add_n(clones_losses, name=\'total_loss\')\n\n        # Add the summaries from the first clone. These contain the summaries\n        # created by model_fn and either optimize_clones() or _gather_clone_loss().\n        summaries |= set(\n            tf.get_collection(tf.GraphKeys.SUMMARIES, first_clone.scope))\n\n        if total_loss is not None:\n            # Add total_loss to summary.\n            try:\n                summaries.add(\n                    tf.summary.scalar(\n                        \'total_loss\', total_loss, name=\'total_loss\'))\n            except AttributeError:\n                summaries.add(\n                    tf.scalar_summary(\n                        \'total_loss\', total_loss, name=\'total_loss\'))\n\n        if summaries:\n            # Merge all summaries together.\n            try:\n                summary_op = tf.summary.merge(list(summaries), name=\'summary_op\')\n            except AttributeError:\n                summary_op = tf.merge_summary(list(summaries), name=\'summary_op\')\n\n        else:\n            summary_op = None\n\n    return DeployedModel(train_op, summary_op, total_loss, clones)\n\n\ndef _sum_clones_gradients(clone_grads):\n    """"""Calculate the sum gradient for each shared variable across all clones.\n\n  This function assumes that the clone_grads has been scaled appropriately by\n  1 / num_clones.\n\n  Args:\n    clone_grads: A List of List of tuples (gradient, variable), one list per\n    `Clone`.\n\n  Returns:\n     List of tuples of (gradient, variable) where the gradient has been summed\n     across all clones.\n  """"""\n    sum_grads = []\n    for grad_and_vars in zip(*clone_grads):\n        # Note that each grad_and_vars looks like the following:\n        #   ((grad_var0_clone0, var0), ... (grad_varN_cloneN, varN))\n        grads = []\n        var = grad_and_vars[0][1]\n        for g, v in grad_and_vars:\n            assert v == var\n            if g is not None:\n                grads.append(g)\n        if grads:\n            if len(grads) > 1:\n                sum_grad = tf.add_n(grads, name=var.op.name + \'/sum_grads\')\n            else:\n                sum_grad = grads[0]\n            sum_grads.append((sum_grad, var))\n    return sum_grads\n\n\ndef _add_gradients_summaries(grads_and_vars):\n    """"""Add histogram summaries to gradients.\n\n  Note: The summaries are also added to the SUMMARIES collection.\n\n  Args:\n    grads_and_vars: A list of gradient to variable pairs (tuples).\n\n  Returns:\n    The _list_ of the added summaries for grads_and_vars.\n  """"""\n    summaries = []\n    for grad, var in grads_and_vars:\n        if grad is not None:\n            if isinstance(grad, tf.IndexedSlices):\n                grad_values = grad.values\n            else:\n                grad_values = grad\n            try:\n                summaries.append(\n                    tf.summary.histogram(var.op.name + \':gradient\', grad_values))\n                summaries.append(\n                    tf.summary.histogram(var.op.name + \':gradient_norm\',\n                                        tf.global_norm([grad_values])))\n            except AttributeError:\n                summaries.append(\n                    tf.histogram_summary(var.op.name + \':gradient\', grad_values))\n                summaries.append(\n                    tf.histogram_summary(var.op.name + \':gradient_norm\',\n                                        tf.global_norm([grad_values])))\n        else:\n            tf.logging.info(\'Var %s has no gradient\', var.op.name)\n    return summaries\n\n\nclass DeploymentConfig(object):\n    """"""Configuration for deploying a model with `deploy()`.\n\n  You can pass an instance of this class to `deploy()` to specify exactly\n  how to deploy the model to build.  If you do not pass one, an instance built\n  from the default deployment_hparams will be used.\n  """"""\n\n    def __init__(self,\n                 num_clones=1,\n                 clone_on_cpu=False,\n                 replica_id=0,\n                 num_replicas=1,\n                 num_ps_tasks=0,\n                 worker_job_name=\'worker\',\n                 ps_job_name=\'ps\'):\n        """"""Create a DeploymentConfig.\n\n    The config describes how to deploy a model across multiple clones and\n    replicas.  The model will be replicated `num_clones` times in each replica.\n    If `clone_on_cpu` is True, each clone will placed on CPU.\n\n    If `num_replicas` is 1, the model is deployed via a single process.  In that\n    case `worker_device`, `num_ps_tasks`, and `ps_device` are ignored.\n\n    If `num_replicas` is greater than 1, then `worker_device` and `ps_device`\n    must specify TensorFlow devices for the `worker` and `ps` jobs and\n    `num_ps_tasks` must be positive.\n\n    Args:\n      num_clones: Number of model clones to deploy in each replica.\n      clone_on_cpu: If True clones would be placed on CPU.\n      replica_id: Integer.  Index of the replica for which the model is\n        deployed.  Usually 0 for the chief replica.\n      num_replicas: Number of replicas to use.\n      num_ps_tasks: Number of tasks for the `ps` job. 0 to not use replicas.\n      worker_job_name: A name for the worker job.\n      ps_job_name: A name for the parameter server job.\n\n    Raises:\n      ValueError: If the arguments are invalid.\n    """"""\n        if num_replicas > 1:\n            if num_ps_tasks < 1:\n                raise ValueError(\n                    \'When using replicas num_ps_tasks must be positive\')\n        if num_replicas > 1 or num_ps_tasks > 0:\n            if not worker_job_name:\n                raise ValueError(\n                    \'Must specify worker_job_name when using replicas\')\n            if not ps_job_name:\n                raise ValueError(\n                    \'Must specify ps_job_name when using parameter server\')\n        if replica_id >= num_replicas:\n            raise ValueError(\'replica_id must be less than num_replicas\')\n        self._num_clones = num_clones\n        self._clone_on_cpu = clone_on_cpu\n        self._replica_id = replica_id\n        self._num_replicas = num_replicas\n        self._num_ps_tasks = num_ps_tasks\n        self._ps_device = \'/job:\' + ps_job_name if num_ps_tasks > 0 else \'\'\n        self._worker_device = \'/job:\' + worker_job_name if num_ps_tasks > 0 else \'\'\n\n    @property\n    def num_clones(self):\n        return self._num_clones\n\n    @property\n    def clone_on_cpu(self):\n        return self._clone_on_cpu\n\n    @property\n    def replica_id(self):\n        return self._replica_id\n\n    @property\n    def num_replicas(self):\n        return self._num_replicas\n\n    @property\n    def num_ps_tasks(self):\n        return self._num_ps_tasks\n\n    @property\n    def ps_device(self):\n        return self._ps_device\n\n    @property\n    def worker_device(self):\n        return self._worker_device\n\n    def caching_device(self):\n        """"""Returns the device to use for caching variables.\n\n    Variables are cached on the worker CPU when using replicas.\n\n    Returns:\n      A device string or None if the variables do not need to be cached.\n    """"""\n        if self._num_ps_tasks > 0:\n            return lambda op: op.device\n        else:\n            return None\n\n    def clone_device(self, clone_index):\n        """"""Device used to create the clone and all the ops inside the clone.\n\n    Args:\n      clone_index: Int, representing the clone_index.\n\n    Returns:\n      A value suitable for `tf.device()`.\n\n    Raises:\n      ValueError: if `clone_index` is greater or equal to the number of clones"".\n    """"""\n        if clone_index >= self._num_clones:\n            raise ValueError(\'clone_index must be less than num_clones\')\n        device = \'\'\n        if self._num_ps_tasks > 0:\n            device += self._worker_device\n        if self._clone_on_cpu:\n            device += \'/device:CPU:0\'\n        else:\n            if self._num_clones > 1:\n                device += \'/device:GPU:%d\' % clone_index\n        return device\n\n    def clone_scope(self, clone_index):\n        """"""Name scope to create the clone.\n\n    Args:\n      clone_index: Int, representing the clone_index.\n\n    Returns:\n      A name_scope suitable for `tf.name_scope()`.\n\n    Raises:\n      ValueError: if `clone_index` is greater or equal to the number of clones"".\n    """"""\n        if clone_index >= self._num_clones:\n            raise ValueError(\'clone_index must be less than num_clones\')\n        scope = \'\'\n        if self._num_clones > 1:\n            scope = \'clone_%d\' % clone_index\n        return scope\n\n    def optimizer_device(self):\n        """"""Device to use with the optimizer.\n\n    Returns:\n      A value suitable for `tf.device()`.\n    """"""\n        if self._num_ps_tasks > 0 or self._num_clones > 0:\n            return self._worker_device + \'/device:CPU:0\'\n        else:\n            return \'\'\n\n    def inputs_device(self):\n        """"""Device to use to build the inputs.\n\n    Returns:\n      A value suitable for `tf.device()`.\n    """"""\n        device = \'\'\n        if self._num_ps_tasks > 0:\n            device += self._worker_device\n        device += \'/device:CPU:0\'\n        return device\n\n    def variables_device(self):\n        """"""Returns the device to use for variables created inside the clone.\n\n    Returns:\n      A value suitable for `tf.device()`.\n    """"""\n        device = \'\'\n        if self._num_ps_tasks > 0:\n            device += self._ps_device\n        device += \'/device:CPU:0\'\n\n        class _PSDeviceChooser(object):\n            """"""Slim device chooser for variables when using PS.""""""\n\n            def __init__(self, device, tasks):\n                self._device = device\n                self._tasks = tasks\n                self._task = 0\n\n            def choose(self, op):\n                if op.device:\n                    return op.device\n                node_def = op if isinstance(op, tf.NodeDef) else op.node_def\n                if node_def.op == \'Variable\':\n                    t = self._task\n                    self._task = (self._task + 1) % self._tasks\n                    d = \'%s/task:%d\' % (self._device, t)\n                    return d\n                else:\n                    return op.device\n\n        if not self._num_ps_tasks:\n            return device\n        else:\n            chooser = _PSDeviceChooser(device, self._num_ps_tasks)\n            return chooser.choose\n'"
finetuning/flask/flask_inference.py,5,"b'import tensorflow as tf\nfrom functools import wraps\nfrom flask import Flask, request, jsonify\nslim = tf.contrib.slim\nfrom PIL import Image\nimport sys\nsys.path.append("".."")\nfrom nets.inception_v3 import *\nimport numpy as np\nimport os\nimport time\n""""""\nLoad a tensorflow model and make it available as a REST service\n""""""\napp = Flask(__name__)\n\n\nclass myTfModel(object):\n    def __init__(self, model_dir, prefix):\n        self.model_dir = model_dir\n        self.prefix = prefix\n        self.output = {}\n        self.load_model()\n\n    def load_model(self):\n        sess = tf.Session()\n        input_tensor = tf.placeholder(tf.float32, [None, 299, 299, 3])\n        arg_scope = inception_v3_arg_scope()\n        with slim.arg_scope(arg_scope):\n            logits, end_points = inception_v3(\n                input_tensor, is_training=False, num_classes=8)\n            saver = tf.train.Saver()\n        params_file = tf.train.latest_checkpoint(self.model_dir)\n        saver.restore(sess, params_file)\n        self.output[\'sess\'] = sess\n        self.output[\'input_tensor\'] = input_tensor\n        self.output[\'logits\'] = logits\n        self.output[\'end_points\'] = end_points\n        # return sess, input_tensor, logits, end_points\n\n    def execute(self, data, **kwargs):\n        sess = self.output[\'sess\']\n        input_tensor = self.output[\'input_tensor\']\n        logits = self.output[\'logits\']\n        end_points = self.output[\'end_points\']\n        # ims = []\n        # for i in range(kwargs[\'batch_size\']):\n        im = Image.open(data).resize((299, 299))\n        im = np.array(im) / 255.0\n        im = im.reshape(-1, 299, 299, 3)\n        # ims.append(im)\n        # ims = np.array(ims)\n        # print ims.shape\n        start = time.time()\n        predict_values, logit_values = sess.run(\n            [end_points[\'Predictions\'], logits], feed_dict={input_tensor: im})\n        return predict_values\n        # print \'the porn score with the {0} is {1} \'.format(\n\n    # data, predict_values[1][1])\n    # print \'a image take time {0}\'.format(time.time() - start)\n\n\nmymodel = myTfModel(\'./train_log\', \'model.ckpt\')\n\n\n@app.route(\'/model\', methods=[\'GET\', \'POST\'])\ndef apply_model():\n    image = request.args.get(\'image\')\n    predict_values = mymodel.execute(image, batch_size=1)\n    predicted_class = np.argmax(predict_values[0])\n    return jsonify(output=int(predicted_class))\n\n\nif __name__ == \'__main__\':\n    app.run(debug=True)'"
finetuning/flask/flask_inference_1000.py,5,"b'import tensorflow as tf\nfrom functools import wraps\nfrom flask import Flask, render_template, request, redirect, url_for, send_from_directory, jsonify\nfrom werkzeug.utils import secure_filename\nslim = tf.contrib.slim\nfrom PIL import Image\nimport numpy as np\nimport os\nimport time\nimport uuid\nimport sys\nsys.path.append("".."")\nfrom nets.inception_v3 import *\n\n""""""\nLoad a tensorflow model and make it available as a REST service\n""""""\napp = Flask(__name__)\nUPLOAD_FOLDER = \'./uploads\'\nALLOWED_EXTENSIONS = set([\'jpg\', \'png\', \'jpeg\'])\napp.config[\'UPLOAD_FOLDER\'] = UPLOAD_FOLDER\n\n\nclass myTfModel(object):\n    def __init__(self, checkpoint_file):\n        self.checkpoint_file = checkpoint_file\n        self.output = {}\n        self.load_model()\n\n    def load_model(self):\n        sess = tf.Session()\n        input_tensor = tf.placeholder(tf.float32, [None, 299, 299, 3])\n        arg_scope = inception_v3_arg_scope()\n        with slim.arg_scope(arg_scope):\n            logits, end_points = inception_v3(\n                input_tensor, is_training=False, num_classes=1001)\n            saver = tf.train.Saver()\n        # params_file = tf.train.latest_checkpoint(self.model_dir)\n        saver.restore(sess, self.checkpoint_file)\n        self.output[\'sess\'] = sess\n        self.output[\'input_tensor\'] = input_tensor\n        self.output[\'logits\'] = logits\n        self.output[\'end_points\'] = end_points\n        # return sess, input_tensor, logits, end_points\n\n    def execute(self, data, **kwargs):\n        sess = self.output[\'sess\']\n        input_tensor = self.output[\'input_tensor\']\n        logits = self.output[\'logits\']\n        end_points = self.output[\'end_points\']\n        # ims = []\n        # for i in range(kwargs[\'batch_size\']):\n        im = Image.open(data).resize((299, 299))\n        im = np.array(im) / 255.0\n        im = im.reshape(-1, 299, 299, 3)\n        start = time.time()\n        predict_values, logit_values = sess.run(\n            [end_points[\'Predictions\'], logits], feed_dict={input_tensor: im})\n        return predict_values\n\nmymodel = myTfModel(\'../pretrain_model/inception_v3.ckpt\')\n\n\ndef get_label(sysnet_file, metadata_file):\n    index_sysnet = []\n    with open(sysnet_file, \'r\') as fread:\n        for line in fread.readlines():\n            line = line.strip(\'\\n\')\n            index_sysnet.append(line)\n    sys_label = {}\n    with open(metadata_file, \'r\') as fread:\n        for line in fread.readlines():\n            index = line.strip(\'\\n\').split(\'\\t\')[0]\n            val = line.strip(\'\\n\').split(\'\\t\')[1]\n            sys_label[index] = val\n\n    index_label = [sys_label[i] for i in index_sysnet]\n    index_label.append(""i don\'t know"")\n    return index_label\n\ndef allowed_file(filename):\n    return \'.\' in filename and filename.rsplit(\'.\', 1)[1] in ALLOWED_EXTENSIONS\n\n@app.route(\'/\', methods=[\'GET\', \'POST\'])\ndef upload_file():\n    if request.method == \'POST\':\n        file = request.files[\'file\']\n        if file and allowed_file(file.filename):\n            filename = secure_filename(file.filename)\n            # add the filename to uuid\n            file_type = filename.split(\'.\')[1]\n            file_name_ori = filename.split(\'.\')[0]\n            filename = str(uuid.uuid4()) + \'.\' + file_type\n\n            file.save(os.path.join(app.config[\'UPLOAD_FOLDER\'], filename))\n            return redirect(url_for(\'result\', filename=filename))\n    return \'\'\'\n    <!doctype html>\n    <title>Upload new File</title>\n    <h1>Upload new File</h1>\n    <form action="""" method=post enctype=multipart/form-data>\n      <p><input type=file name=file>\n         <input type=submit value=Upload>\n    </form>    \n    \'\'\'\n\n\nfrom flask import send_from_directory\n\n\n@app.route(\'/uploads/<filename>\', methods=[\'GET\', \'POST\'])\ndef uploaded_file(filename):\n    return send_from_directory(app.config[\'UPLOAD_FOLDER\'], filename)\n\n\n@app.route(\'/result/<filename>\', methods=[\'GET\'])\ndef result(filename):\n    image = os.path.join(UPLOAD_FOLDER, filename)\n    predict_values = mymodel.execute(image, batch_size=1)\n    predicted_class_top5 = reversed(np.argsort(predict_values[0])[-5:])\n    index_label = get_label(\'./sysnet.txt\', \'imagenet_metadata.txt\')\n    labels = [index_label[i - 1] for i in predicted_class_top5]\n\n    result = \'inference result: \' + \'\\n\'.join(labels)\n    return ""<!doctype html><title>Upload new File</title><h1>Result</h1><img height =\'400\', width=\'400\' src=\'/uploads/{0}\'></img></br>{1}"".format(\n        filename, result)\nfrom werkzeug.wsgi import LimitedStream\n\n\nclass StreamConsumingMiddleware(object):\n    def __init__(self, app):\n        self.app = app\n\n    def __call__(self, environ, start_response):\n        stream = LimitedStream(environ[\'wsgi.input\'],\n                               int(environ[\'CONTENT_LENGTH\'] or 0))\n        environ[\'wsgi.input\'] = stream\n        app_iter = self.app(environ, start_response)\n        try:\n            stream.exhaust()\n            for event in app_iter:\n                yield event\n        finally:\n            if hasattr(app_iter, \'close\'):\n                app_iter.close()\n\n\napp.wsgi_app = StreamConsumingMiddleware(app.wsgi_app)\n\nif __name__ == \'__main__\':\n    app.run(debug=True, threaded=True, host=\'0.0.0.0\', port=80)\n'"
finetuning/inference/fish_inference.py,5,"b'import tensorflow as tf\nslim = tf.contrib.slim\nfrom PIL import Image\nimport sys\nsys.path.append("".."")\nfrom nets.inception_v3 import *\nimport numpy as np\nimport os\nimport time\n\n\ndef inference(image):\n    checkpoint_dir = \'../train_log\'\n    input_tensor = tf.placeholder(tf.float32, [None, 299, 299, 3])\n    sess = tf.Session()\n    arg_scope = inception_v3_arg_scope()\n    with slim.arg_scope(arg_scope):\n        logits, end_points = inception_v3(\n            input_tensor, is_training=False, num_classes=8)\n        saver = tf.train.Saver()\n    ckpt = tf.train.get_checkpoint_state(checkpoint_dir)\n    if ckpt and ckpt.model_checkpoint_path:\n        saver.restore(sess, ckpt.model_checkpoint_path)\n    im = Image.open(image).resize((299, 299))\n    im = np.array(im) / 255.0\n    im = im.reshape(-1, 299, 299, 3)\n    start = time.time()\n    predict_values, logit_values = sess.run(\n        [end_points[\'Predictions\'], logits], feed_dict={input_tensor: im})\n    print \'a image take time {0}\'.format(time.time() - start)\n    return image, predict_values\n\n\nif __name__ == ""__main__"":\n    sample_images = \'train/ALB/img_00003.jpg\'\n    image, predict = inference(sample_images)\n    print \'the porn score with the {0} is {1} \'.format(image, predict)\n'"
finetuning/inference/inference_1000.py,5,"b'import tensorflow as tf\nslim = tf.contrib.slim\nfrom PIL import Image\nimport sys\nsys.path.append("".."")\nfrom nets.inception_v3 import *\nimport numpy as np\nimport os\nimport time\n\n\ndef get_label(sysnet_file, metadata_file):\n    index_sysnet = []\n    with open(sysnet_file, \'r\') as fread:\n        for line in fread.readlines():\n            line = line.strip(\'\\n\')\n            index_sysnet.append(line)\n    sys_label = {}\n    with open(metadata_file, \'r\') as fread:\n        for line in fread.readlines():\n            index = line.strip(\'\\n\').split(\'\\t\')[0]\n            val = line.strip(\'\\n\').split(\'\\t\')[1]\n            sys_label[index] = val\n\n    index_label = [sys_label[i] for i in index_sysnet]\n    index_label.append(""i don\'t know"")\n    return index_label\n\n\ndef inference(image):\n    checkpoint_dir = \'../pretrain_model\'\n    checkpoint_file = \'../pretrain_model/inception_v3.ckpt\'\n    input_tensor = tf.placeholder(tf.float32, [None, 299, 299, 3])\n    sess = tf.Session()\n    arg_scope = inception_v3_arg_scope()\n    with slim.arg_scope(arg_scope):\n        logits, end_points = inception_v3(\n            input_tensor, is_training=False, num_classes=1001)\n        saver = tf.train.Saver()\n    # ckpt = tf.train.get_checkpoint_state(checkpoint_dir)\n    saver.restore(sess, checkpoint_file)\n    im = Image.open(image).resize((299, 299))\n    im = np.array(im) / 255.0\n    im = im.reshape(-1, 299, 299, 3)\n    start = time.time()\n    predict_values, logit_values = sess.run(\n        [end_points[\'Predictions\'], logits], feed_dict={input_tensor: im})\n    print \'a image take time {0}\'.format(time.time() - start)\n    return image, predict_values\n\n\nif __name__ == ""__main__"":\n    sample_images = \'./cat.jpeg\'\n    image, predict = inference(sample_images)\n\n    print np.argmax(predict[0])\n    index_label = get_label(\'./sysnet.txt\', \'imagenet_metadata.txt\')\n    print \'the image {0}, predict label is {1}\'.format(\n        sample_images, index_label[np.argmax(predict[0] - 1)])\n'"
finetuning/nets/__init__.py,0,b'\n'
finetuning/nets/alexnet.py,8,"b'# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Contains a model definition for AlexNet.\n\nThis work was first described in:\n  ImageNet Classification with Deep Convolutional Neural Networks\n  Alex Krizhevsky, Ilya Sutskever and Geoffrey E. Hinton\n\nand later refined in:\n  One weird trick for parallelizing convolutional neural networks\n  Alex Krizhevsky, 2014\n\nHere we provide the implementation proposed in ""One weird trick"" and not\n""ImageNet Classification"", as per the paper, the LRN layers have been removed.\n\nUsage:\n  with slim.arg_scope(alexnet.alexnet_v2_arg_scope()):\n    outputs, end_points = alexnet.alexnet_v2(inputs)\n\n@@alexnet_v2\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\n\nslim = tf.contrib.slim\ntrunc_normal = lambda stddev: tf.truncated_normal_initializer(0.0, stddev)\n\n\ndef alexnet_v2_arg_scope(weight_decay=0.0005):\n  with slim.arg_scope([slim.conv2d, slim.fully_connected],\n                      activation_fn=tf.nn.relu,\n                      biases_initializer=tf.constant_initializer(0.1),\n                      weights_regularizer=slim.l2_regularizer(weight_decay)):\n    with slim.arg_scope([slim.conv2d], padding=\'SAME\'):\n      with slim.arg_scope([slim.max_pool2d], padding=\'VALID\') as arg_sc:\n        return arg_sc\n\n\ndef alexnet_v2(inputs,\n               num_classes=1000,\n               is_training=True,\n               dropout_keep_prob=0.5,\n               spatial_squeeze=True,\n               scope=\'alexnet_v2\'):\n  """"""AlexNet version 2.\n\n  Described in: http://arxiv.org/pdf/1404.5997v2.pdf\n  Parameters from:\n  github.com/akrizhevsky/cuda-convnet2/blob/master/layers/\n  layers-imagenet-1gpu.cfg\n\n  Note: All the fully_connected layers have been transformed to conv2d layers.\n        To use in classification mode, resize input to 224x224. To use in fully\n        convolutional mode, set spatial_squeeze to false.\n        The LRN layers have been removed and change the initializers from\n        random_normal_initializer to xavier_initializer.\n\n  Args:\n    inputs: a tensor of size [batch_size, height, width, channels].\n    num_classes: number of predicted classes.\n    is_training: whether or not the model is being trained.\n    dropout_keep_prob: the probability that activations are kept in the dropout\n      layers during training.\n    spatial_squeeze: whether or not should squeeze the spatial dimensions of the\n      outputs. Useful to remove unnecessary dimensions for classification.\n    scope: Optional scope for the variables.\n\n  Returns:\n    the last op containing the log predictions and end_points dict.\n  """"""\n  with tf.variable_scope(scope, \'alexnet_v2\', [inputs]) as sc:\n    end_points_collection = sc.name + \'_end_points\'\n    # Collect outputs for conv2d, fully_connected and max_pool2d.\n    with slim.arg_scope([slim.conv2d, slim.fully_connected, slim.max_pool2d],\n                        outputs_collections=[end_points_collection]):\n      net = slim.conv2d(inputs, 64, [11, 11], 4, padding=\'VALID\',\n                        scope=\'conv1\')\n      net = slim.max_pool2d(net, [3, 3], 2, scope=\'pool1\')\n      net = slim.conv2d(net, 192, [5, 5], scope=\'conv2\')\n      net = slim.max_pool2d(net, [3, 3], 2, scope=\'pool2\')\n      net = slim.conv2d(net, 384, [3, 3], scope=\'conv3\')\n      net = slim.conv2d(net, 384, [3, 3], scope=\'conv4\')\n      net = slim.conv2d(net, 256, [3, 3], scope=\'conv5\')\n      net = slim.max_pool2d(net, [3, 3], 2, scope=\'pool5\')\n\n      # Use conv2d instead of fully_connected layers.\n      with slim.arg_scope([slim.conv2d],\n                          weights_initializer=trunc_normal(0.005),\n                          biases_initializer=tf.constant_initializer(0.1)):\n        net = slim.conv2d(net, 4096, [5, 5], padding=\'VALID\',\n                          scope=\'fc6\')\n        net = slim.dropout(net, dropout_keep_prob, is_training=is_training,\n                           scope=\'dropout6\')\n        net = slim.conv2d(net, 4096, [1, 1], scope=\'fc7\')\n        net = slim.dropout(net, dropout_keep_prob, is_training=is_training,\n                           scope=\'dropout7\')\n        net = slim.conv2d(net, num_classes, [1, 1],\n                          activation_fn=None,\n                          normalizer_fn=None,\n                          biases_initializer=tf.zeros_initializer(),\n                          scope=\'fc8\')\n\n      # Convert end_points_collection into a end_point dict.\n      end_points = slim.utils.convert_collection_to_dict(end_points_collection)\n      if spatial_squeeze:\n        net = tf.squeeze(net, [1, 2], name=\'fc8/squeezed\')\n        end_points[sc.name + \'/fc8\'] = net\n      return net, end_points\nalexnet_v2.default_image_size = 224\n'"
finetuning/nets/alexnet_test.py,17,"b'# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Tests for slim.nets.alexnet.""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\n\nfrom nets import alexnet\n\nslim = tf.contrib.slim\n\n\nclass AlexnetV2Test(tf.test.TestCase):\n\n  def testBuild(self):\n    batch_size = 5\n    height, width = 224, 224\n    num_classes = 1000\n    with self.test_session():\n      inputs = tf.random_uniform((batch_size, height, width, 3))\n      logits, _ = alexnet.alexnet_v2(inputs, num_classes)\n      self.assertEquals(logits.op.name, \'alexnet_v2/fc8/squeezed\')\n      self.assertListEqual(logits.get_shape().as_list(),\n                           [batch_size, num_classes])\n\n  def testFullyConvolutional(self):\n    batch_size = 1\n    height, width = 300, 400\n    num_classes = 1000\n    with self.test_session():\n      inputs = tf.random_uniform((batch_size, height, width, 3))\n      logits, _ = alexnet.alexnet_v2(inputs, num_classes, spatial_squeeze=False)\n      self.assertEquals(logits.op.name, \'alexnet_v2/fc8/BiasAdd\')\n      self.assertListEqual(logits.get_shape().as_list(),\n                           [batch_size, 4, 7, num_classes])\n\n  def testEndPoints(self):\n    batch_size = 5\n    height, width = 224, 224\n    num_classes = 1000\n    with self.test_session():\n      inputs = tf.random_uniform((batch_size, height, width, 3))\n      _, end_points = alexnet.alexnet_v2(inputs, num_classes)\n      expected_names = [\'alexnet_v2/conv1\',\n                        \'alexnet_v2/pool1\',\n                        \'alexnet_v2/conv2\',\n                        \'alexnet_v2/pool2\',\n                        \'alexnet_v2/conv3\',\n                        \'alexnet_v2/conv4\',\n                        \'alexnet_v2/conv5\',\n                        \'alexnet_v2/pool5\',\n                        \'alexnet_v2/fc6\',\n                        \'alexnet_v2/fc7\',\n                        \'alexnet_v2/fc8\'\n                       ]\n      self.assertSetEqual(set(end_points.keys()), set(expected_names))\n\n  def testModelVariables(self):\n    batch_size = 5\n    height, width = 224, 224\n    num_classes = 1000\n    with self.test_session():\n      inputs = tf.random_uniform((batch_size, height, width, 3))\n      alexnet.alexnet_v2(inputs, num_classes)\n      expected_names = [\'alexnet_v2/conv1/weights\',\n                        \'alexnet_v2/conv1/biases\',\n                        \'alexnet_v2/conv2/weights\',\n                        \'alexnet_v2/conv2/biases\',\n                        \'alexnet_v2/conv3/weights\',\n                        \'alexnet_v2/conv3/biases\',\n                        \'alexnet_v2/conv4/weights\',\n                        \'alexnet_v2/conv4/biases\',\n                        \'alexnet_v2/conv5/weights\',\n                        \'alexnet_v2/conv5/biases\',\n                        \'alexnet_v2/fc6/weights\',\n                        \'alexnet_v2/fc6/biases\',\n                        \'alexnet_v2/fc7/weights\',\n                        \'alexnet_v2/fc7/biases\',\n                        \'alexnet_v2/fc8/weights\',\n                        \'alexnet_v2/fc8/biases\',\n                       ]\n      model_variables = [v.op.name for v in slim.get_model_variables()]\n      self.assertSetEqual(set(model_variables), set(expected_names))\n\n  def testEvaluation(self):\n    batch_size = 2\n    height, width = 224, 224\n    num_classes = 1000\n    with self.test_session():\n      eval_inputs = tf.random_uniform((batch_size, height, width, 3))\n      logits, _ = alexnet.alexnet_v2(eval_inputs, is_training=False)\n      self.assertListEqual(logits.get_shape().as_list(),\n                           [batch_size, num_classes])\n      predictions = tf.argmax(logits, 1)\n      self.assertListEqual(predictions.get_shape().as_list(), [batch_size])\n\n  def testTrainEvalWithReuse(self):\n    train_batch_size = 2\n    eval_batch_size = 1\n    train_height, train_width = 224, 224\n    eval_height, eval_width = 300, 400\n    num_classes = 1000\n    with self.test_session():\n      train_inputs = tf.random_uniform(\n          (train_batch_size, train_height, train_width, 3))\n      logits, _ = alexnet.alexnet_v2(train_inputs)\n      self.assertListEqual(logits.get_shape().as_list(),\n                           [train_batch_size, num_classes])\n      tf.get_variable_scope().reuse_variables()\n      eval_inputs = tf.random_uniform(\n          (eval_batch_size, eval_height, eval_width, 3))\n      logits, _ = alexnet.alexnet_v2(eval_inputs, is_training=False,\n                                     spatial_squeeze=False)\n      self.assertListEqual(logits.get_shape().as_list(),\n                           [eval_batch_size, 4, 7, num_classes])\n      logits = tf.reduce_mean(logits, [1, 2])\n      predictions = tf.argmax(logits, 1)\n      self.assertEquals(predictions.get_shape().as_list(), [eval_batch_size])\n\n  def testForward(self):\n    batch_size = 1\n    height, width = 224, 224\n    with self.test_session() as sess:\n      inputs = tf.random_uniform((batch_size, height, width, 3))\n      logits, _ = alexnet.alexnet_v2(inputs)\n      try:\n        sess.run(tf.global_variables_initializer())\n      except:\n        sess.run(tf.initialize_all_variables())\n      output = sess.run(logits)\n      self.assertTrue(output.any())\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
finetuning/nets/cifarnet.py,12,"b'# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Contains a variant of the CIFAR-10 model definition.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\n\nslim = tf.contrib.slim\n\ntrunc_normal = lambda stddev: tf.truncated_normal_initializer(stddev=stddev)\n\n\ndef cifarnet(images, num_classes=10, is_training=False,\n             dropout_keep_prob=0.5,\n             prediction_fn=slim.softmax,\n             scope=\'CifarNet\'):\n  """"""Creates a variant of the CifarNet model.\n\n  Note that since the output is a set of \'logits\', the values fall in the\n  interval of (-infinity, infinity). Consequently, to convert the outputs to a\n  probability distribution over the characters, one will need to convert them\n  using the softmax function:\n\n        logits = cifarnet.cifarnet(images, is_training=False)\n        probabilities = tf.nn.softmax(logits)\n        predictions = tf.argmax(logits, 1)\n\n  Args:\n    images: A batch of `Tensors` of size [batch_size, height, width, channels].\n    num_classes: the number of classes in the dataset.\n    is_training: specifies whether or not we\'re currently training the model.\n      This variable will determine the behaviour of the dropout layer.\n    dropout_keep_prob: the percentage of activation values that are retained.\n    prediction_fn: a function to get predictions out of logits.\n    scope: Optional variable_scope.\n\n  Returns:\n    logits: the pre-softmax activations, a tensor of size\n      [batch_size, `num_classes`]\n    end_points: a dictionary from components of the network to the corresponding\n      activation.\n  """"""\n  end_points = {}\n\n  with tf.variable_scope(scope, \'CifarNet\', [images, num_classes]):\n    net = slim.conv2d(images, 64, [5, 5], scope=\'conv1\')\n    end_points[\'conv1\'] = net\n    net = slim.max_pool2d(net, [2, 2], 2, scope=\'pool1\')\n    end_points[\'pool1\'] = net\n    net = tf.nn.lrn(net, 4, bias=1.0, alpha=0.001/9.0, beta=0.75, name=\'norm1\')\n    net = slim.conv2d(net, 64, [5, 5], scope=\'conv2\')\n    end_points[\'conv2\'] = net\n    net = tf.nn.lrn(net, 4, bias=1.0, alpha=0.001/9.0, beta=0.75, name=\'norm2\')\n    net = slim.max_pool2d(net, [2, 2], 2, scope=\'pool2\')\n    end_points[\'pool2\'] = net\n    net = slim.flatten(net)\n    end_points[\'Flatten\'] = net\n    net = slim.fully_connected(net, 384, scope=\'fc3\')\n    end_points[\'fc3\'] = net\n    net = slim.dropout(net, dropout_keep_prob, is_training=is_training,\n                       scope=\'dropout3\')\n    net = slim.fully_connected(net, 192, scope=\'fc4\')\n    end_points[\'fc4\'] = net\n    logits = slim.fully_connected(net, num_classes,\n                                  biases_initializer=tf.zeros_initializer(),\n                                  weights_initializer=trunc_normal(1/192.0),\n                                  weights_regularizer=None,\n                                  activation_fn=None,\n                                  scope=\'logits\')\n\n    end_points[\'Logits\'] = logits\n    end_points[\'Predictions\'] = prediction_fn(logits, scope=\'Predictions\')\n\n  return logits, end_points\ncifarnet.default_image_size = 32\n\n\ndef cifarnet_arg_scope(weight_decay=0.004):\n  """"""Defines the default cifarnet argument scope.\n\n  Args:\n    weight_decay: The weight decay to use for regularizing the model.\n\n  Returns:\n    An `arg_scope` to use for the inception v3 model.\n  """"""\n  with slim.arg_scope(\n      [slim.conv2d],\n      weights_initializer=tf.truncated_normal_initializer(stddev=5e-2),\n      activation_fn=tf.nn.relu):\n    with slim.arg_scope(\n        [slim.fully_connected],\n        biases_initializer=tf.constant_initializer(0.1),\n        weights_initializer=trunc_normal(0.04),\n        weights_regularizer=slim.l2_regularizer(weight_decay),\n        activation_fn=tf.nn.relu) as sc:\n      return sc\n'"
finetuning/nets/inception.py,0,"b'# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Brings all inception models under one namespace.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\n# pylint: disable=unused-import\nfrom nets.inception_resnet_v2 import inception_resnet_v2\nfrom nets.inception_resnet_v2 import inception_resnet_v2_arg_scope\nfrom nets.inception_v1 import inception_v1\nfrom nets.inception_v1 import inception_v1_arg_scope\nfrom nets.inception_v1 import inception_v1_base\nfrom nets.inception_v2 import inception_v2\nfrom nets.inception_v2 import inception_v2_arg_scope\nfrom nets.inception_v2 import inception_v2_base\nfrom nets.inception_v3 import inception_v3\nfrom nets.inception_v3 import inception_v3_arg_scope\nfrom nets.inception_v3 import inception_v3_base\nfrom nets.inception_v4 import inception_v4\nfrom nets.inception_v4 import inception_v4_arg_scope\nfrom nets.inception_v4 import inception_v4_base\n# pylint: enable=unused-import\n'"
finetuning/nets/inception_resnet_v2.py,39,"b'# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Contains the definition of the Inception Resnet V2 architecture.\n\nAs described in http://arxiv.org/abs/1602.07261.\n\n  Inception-v4, Inception-ResNet and the Impact of Residual Connections\n    on Learning\n  Christian Szegedy, Sergey Ioffe, Vincent Vanhoucke, Alex Alemi\n""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\n\nimport tensorflow as tf\n\nslim = tf.contrib.slim\n\n\ndef block35(net, scale=1.0, activation_fn=tf.nn.relu, scope=None, reuse=None):\n  """"""Builds the 35x35 resnet block.""""""\n  with tf.variable_scope(scope, \'Block35\', [net], reuse=reuse):\n    with tf.variable_scope(\'Branch_0\'):\n      tower_conv = slim.conv2d(net, 32, 1, scope=\'Conv2d_1x1\')\n    with tf.variable_scope(\'Branch_1\'):\n      tower_conv1_0 = slim.conv2d(net, 32, 1, scope=\'Conv2d_0a_1x1\')\n      tower_conv1_1 = slim.conv2d(tower_conv1_0, 32, 3, scope=\'Conv2d_0b_3x3\')\n    with tf.variable_scope(\'Branch_2\'):\n      tower_conv2_0 = slim.conv2d(net, 32, 1, scope=\'Conv2d_0a_1x1\')\n      tower_conv2_1 = slim.conv2d(tower_conv2_0, 48, 3, scope=\'Conv2d_0b_3x3\')\n      tower_conv2_2 = slim.conv2d(tower_conv2_1, 64, 3, scope=\'Conv2d_0c_3x3\')\n    mixed = tf.concat(axis=3, values=[tower_conv, tower_conv1_1, tower_conv2_2])\n    up = slim.conv2d(mixed, net.get_shape()[3], 1, normalizer_fn=None,\n                     activation_fn=None, scope=\'Conv2d_1x1\')\n    net += scale * up\n    if activation_fn:\n      net = activation_fn(net)\n  return net\n\n\ndef block17(net, scale=1.0, activation_fn=tf.nn.relu, scope=None, reuse=None):\n  """"""Builds the 17x17 resnet block.""""""\n  with tf.variable_scope(scope, \'Block17\', [net], reuse=reuse):\n    with tf.variable_scope(\'Branch_0\'):\n      tower_conv = slim.conv2d(net, 192, 1, scope=\'Conv2d_1x1\')\n    with tf.variable_scope(\'Branch_1\'):\n      tower_conv1_0 = slim.conv2d(net, 128, 1, scope=\'Conv2d_0a_1x1\')\n      tower_conv1_1 = slim.conv2d(tower_conv1_0, 160, [1, 7],\n                                  scope=\'Conv2d_0b_1x7\')\n      tower_conv1_2 = slim.conv2d(tower_conv1_1, 192, [7, 1],\n                                  scope=\'Conv2d_0c_7x1\')\n    mixed = tf.concat(axis=3, values=[tower_conv, tower_conv1_2])\n    up = slim.conv2d(mixed, net.get_shape()[3], 1, normalizer_fn=None,\n                     activation_fn=None, scope=\'Conv2d_1x1\')\n    net += scale * up\n    if activation_fn:\n      net = activation_fn(net)\n  return net\n\n\ndef block8(net, scale=1.0, activation_fn=tf.nn.relu, scope=None, reuse=None):\n  """"""Builds the 8x8 resnet block.""""""\n  with tf.variable_scope(scope, \'Block8\', [net], reuse=reuse):\n    with tf.variable_scope(\'Branch_0\'):\n      tower_conv = slim.conv2d(net, 192, 1, scope=\'Conv2d_1x1\')\n    with tf.variable_scope(\'Branch_1\'):\n      tower_conv1_0 = slim.conv2d(net, 192, 1, scope=\'Conv2d_0a_1x1\')\n      tower_conv1_1 = slim.conv2d(tower_conv1_0, 224, [1, 3],\n                                  scope=\'Conv2d_0b_1x3\')\n      tower_conv1_2 = slim.conv2d(tower_conv1_1, 256, [3, 1],\n                                  scope=\'Conv2d_0c_3x1\')\n    mixed = tf.concat(axis=3, values=[tower_conv, tower_conv1_2])\n    up = slim.conv2d(mixed, net.get_shape()[3], 1, normalizer_fn=None,\n                     activation_fn=None, scope=\'Conv2d_1x1\')\n    net += scale * up\n    if activation_fn:\n      net = activation_fn(net)\n  return net\n\n\ndef inception_resnet_v2(inputs, num_classes=1001, is_training=True,\n                        dropout_keep_prob=0.8,\n                        reuse=None,\n                        scope=\'InceptionResnetV2\'):\n  """"""Creates the Inception Resnet V2 model.\n\n  Args:\n    inputs: a 4-D tensor of size [batch_size, height, width, 3].\n    num_classes: number of predicted classes.\n    is_training: whether is training or not.\n    dropout_keep_prob: float, the fraction to keep before final layer.\n    reuse: whether or not the network and its variables should be reused. To be\n      able to reuse \'scope\' must be given.\n    scope: Optional variable_scope.\n\n  Returns:\n    logits: the logits outputs of the model.\n    end_points: the set of end_points from the inception model.\n  """"""\n  end_points = {}\n\n  with tf.variable_scope(scope, \'InceptionResnetV2\', [inputs], reuse=reuse):\n    with slim.arg_scope([slim.batch_norm, slim.dropout],\n                        is_training=is_training):\n      with slim.arg_scope([slim.conv2d, slim.max_pool2d, slim.avg_pool2d],\n                          stride=1, padding=\'SAME\'):\n\n        # 149 x 149 x 32\n        net = slim.conv2d(inputs, 32, 3, stride=2, padding=\'VALID\',\n                          scope=\'Conv2d_1a_3x3\')\n        end_points[\'Conv2d_1a_3x3\'] = net\n        # 147 x 147 x 32\n        net = slim.conv2d(net, 32, 3, padding=\'VALID\',\n                          scope=\'Conv2d_2a_3x3\')\n        end_points[\'Conv2d_2a_3x3\'] = net\n        # 147 x 147 x 64\n        net = slim.conv2d(net, 64, 3, scope=\'Conv2d_2b_3x3\')\n        end_points[\'Conv2d_2b_3x3\'] = net\n        # 73 x 73 x 64\n        net = slim.max_pool2d(net, 3, stride=2, padding=\'VALID\',\n                              scope=\'MaxPool_3a_3x3\')\n        end_points[\'MaxPool_3a_3x3\'] = net\n        # 73 x 73 x 80\n        net = slim.conv2d(net, 80, 1, padding=\'VALID\',\n                          scope=\'Conv2d_3b_1x1\')\n        end_points[\'Conv2d_3b_1x1\'] = net\n        # 71 x 71 x 192\n        net = slim.conv2d(net, 192, 3, padding=\'VALID\',\n                          scope=\'Conv2d_4a_3x3\')\n        end_points[\'Conv2d_4a_3x3\'] = net\n        # 35 x 35 x 192\n        net = slim.max_pool2d(net, 3, stride=2, padding=\'VALID\',\n                              scope=\'MaxPool_5a_3x3\')\n        end_points[\'MaxPool_5a_3x3\'] = net\n\n        # 35 x 35 x 320\n        with tf.variable_scope(\'Mixed_5b\'):\n          with tf.variable_scope(\'Branch_0\'):\n            tower_conv = slim.conv2d(net, 96, 1, scope=\'Conv2d_1x1\')\n          with tf.variable_scope(\'Branch_1\'):\n            tower_conv1_0 = slim.conv2d(net, 48, 1, scope=\'Conv2d_0a_1x1\')\n            tower_conv1_1 = slim.conv2d(tower_conv1_0, 64, 5,\n                                        scope=\'Conv2d_0b_5x5\')\n          with tf.variable_scope(\'Branch_2\'):\n            tower_conv2_0 = slim.conv2d(net, 64, 1, scope=\'Conv2d_0a_1x1\')\n            tower_conv2_1 = slim.conv2d(tower_conv2_0, 96, 3,\n                                        scope=\'Conv2d_0b_3x3\')\n            tower_conv2_2 = slim.conv2d(tower_conv2_1, 96, 3,\n                                        scope=\'Conv2d_0c_3x3\')\n          with tf.variable_scope(\'Branch_3\'):\n            tower_pool = slim.avg_pool2d(net, 3, stride=1, padding=\'SAME\',\n                                         scope=\'AvgPool_0a_3x3\')\n            tower_pool_1 = slim.conv2d(tower_pool, 64, 1,\n                                       scope=\'Conv2d_0b_1x1\')\n          net = tf.concat(axis=3, values=[tower_conv, tower_conv1_1,\n                              tower_conv2_2, tower_pool_1])\n\n        end_points[\'Mixed_5b\'] = net\n        net = slim.repeat(net, 10, block35, scale=0.17)\n\n        # 17 x 17 x 1024\n        with tf.variable_scope(\'Mixed_6a\'):\n          with tf.variable_scope(\'Branch_0\'):\n            tower_conv = slim.conv2d(net, 384, 3, stride=2, padding=\'VALID\',\n                                     scope=\'Conv2d_1a_3x3\')\n          with tf.variable_scope(\'Branch_1\'):\n            tower_conv1_0 = slim.conv2d(net, 256, 1, scope=\'Conv2d_0a_1x1\')\n            tower_conv1_1 = slim.conv2d(tower_conv1_0, 256, 3,\n                                        scope=\'Conv2d_0b_3x3\')\n            tower_conv1_2 = slim.conv2d(tower_conv1_1, 384, 3,\n                                        stride=2, padding=\'VALID\',\n                                        scope=\'Conv2d_1a_3x3\')\n          with tf.variable_scope(\'Branch_2\'):\n            tower_pool = slim.max_pool2d(net, 3, stride=2, padding=\'VALID\',\n                                         scope=\'MaxPool_1a_3x3\')\n          net = tf.concat(axis=3, values=[tower_conv, tower_conv1_2, tower_pool])\n\n        end_points[\'Mixed_6a\'] = net\n        net = slim.repeat(net, 20, block17, scale=0.10)\n\n        # # Auxillary tower\n        # with tf.variable_scope(\'AuxLogits\'):\n        #   aux = slim.avg_pool2d(net, 5, stride=3, padding=\'VALID\',\n        #                         scope=\'Conv2d_1a_3x3\')\n        #   aux = slim.conv2d(aux, 128, 1, scope=\'Conv2d_1b_1x1\')\n        #   aux = slim.conv2d(aux, 768, aux.get_shape()[1:3],\n        #                     padding=\'VALID\', scope=\'Conv2d_2a_5x5\')\n        #   aux = slim.flatten(aux)\n        #   aux = slim.fully_connected(aux, num_classes, activation_fn=None,\n        #                              scope=\'Logits\')\n        #   end_points[\'AuxLogits\'] = aux\n\n        with tf.variable_scope(\'Mixed_7a\'):\n          with tf.variable_scope(\'Branch_0\'):\n            tower_conv = slim.conv2d(net, 256, 1, scope=\'Conv2d_0a_1x1\')\n            tower_conv_1 = slim.conv2d(tower_conv, 384, 3, stride=2,\n                                       padding=\'VALID\', scope=\'Conv2d_1a_3x3\')\n          with tf.variable_scope(\'Branch_1\'):\n            tower_conv1 = slim.conv2d(net, 256, 1, scope=\'Conv2d_0a_1x1\')\n            tower_conv1_1 = slim.conv2d(tower_conv1, 288, 3, stride=2,\n                                        padding=\'VALID\', scope=\'Conv2d_1a_3x3\')\n          with tf.variable_scope(\'Branch_2\'):\n            tower_conv2 = slim.conv2d(net, 256, 1, scope=\'Conv2d_0a_1x1\')\n            tower_conv2_1 = slim.conv2d(tower_conv2, 288, 3,\n                                        scope=\'Conv2d_0b_3x3\')\n            tower_conv2_2 = slim.conv2d(tower_conv2_1, 320, 3, stride=2,\n                                        padding=\'VALID\', scope=\'Conv2d_1a_3x3\')\n          with tf.variable_scope(\'Branch_3\'):\n            tower_pool = slim.max_pool2d(net, 3, stride=2, padding=\'VALID\',\n                                         scope=\'MaxPool_1a_3x3\')\n          net = tf.concat(axis=3, values=[tower_conv_1, tower_conv1_1,\n                              tower_conv2_2, tower_pool])\n\n        end_points[\'Mixed_7a\'] = net\n\n        net = slim.repeat(net, 9, block8, scale=0.20)\n        net = block8(net, activation_fn=None)\n\n        net = slim.conv2d(net, 1536, 1, scope=\'Conv2d_7b_1x1\')\n        end_points[\'Conv2d_7b_1x1\'] = net\n\n        with tf.variable_scope(\'Logits\'):\n          end_points[\'PrePool\'] = net\n          net = slim.avg_pool2d(net, net.get_shape()[1:3], padding=\'VALID\',\n                                scope=\'AvgPool_1a_8x8\')\n          net = slim.flatten(net)\n\n          net = slim.dropout(net, dropout_keep_prob, is_training=is_training,\n                             scope=\'Dropout\')\n\n          end_points[\'PreLogitsFlatten\'] = net\n          logits = slim.fully_connected(net, num_classes, activation_fn=None,\n                                        scope=\'Logits\')\n          end_points[\'Logits\'] = logits\n          end_points[\'Predictions\'] = tf.nn.softmax(logits, name=\'Predictions\')\n\n    return logits, end_points\ninception_resnet_v2.default_image_size = 299\n\n\ndef inception_resnet_v2_arg_scope(weight_decay=0.00004,\n                                  batch_norm_decay=0.9997,\n                                  batch_norm_epsilon=0.001):\n  """"""Yields the scope with the default parameters for inception_resnet_v2.\n\n  Args:\n    weight_decay: the weight decay for weights variables.\n    batch_norm_decay: decay for the moving average of batch_norm momentums.\n    batch_norm_epsilon: small float added to variance to avoid dividing by zero.\n\n  Returns:\n    a arg_scope with the parameters needed for inception_resnet_v2.\n  """"""\n  # Set weight_decay for weights in conv2d and fully_connected layers.\n  with slim.arg_scope([slim.conv2d, slim.fully_connected],\n                      weights_regularizer=slim.l2_regularizer(weight_decay),\n                      biases_regularizer=slim.l2_regularizer(weight_decay)):\n\n    batch_norm_params = {\n        \'decay\': batch_norm_decay,\n        \'epsilon\': batch_norm_epsilon,\n    }\n    # Set activation_fn and parameters for batch_norm.\n    with slim.arg_scope([slim.conv2d], activation_fn=tf.nn.relu,\n                        normalizer_fn=slim.batch_norm,\n                        normalizer_params=batch_norm_params) as scope:\n      return scope\n'"
finetuning/nets/inception_resnet_v2_test.py,23,"b'# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Tests for slim.inception_resnet_v2.""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\n\nfrom nets import inception\n\n\nclass InceptionTest(tf.test.TestCase):\n\n  def testBuildLogits(self):\n    batch_size = 5\n    height, width = 299, 299\n    num_classes = 1000\n    with self.test_session():\n      inputs = tf.random_uniform((batch_size, height, width, 3))\n      logits, _ = inception.inception_resnet_v2(inputs, num_classes)\n      self.assertTrue(logits.op.name.startswith(\'InceptionResnetV2/Logits\'))\n      self.assertListEqual(logits.get_shape().as_list(),\n                           [batch_size, num_classes])\n\n  def testBuildEndPoints(self):\n    batch_size = 5\n    height, width = 299, 299\n    num_classes = 1000\n    with self.test_session():\n      inputs = tf.random_uniform((batch_size, height, width, 3))\n      _, end_points = inception.inception_resnet_v2(inputs, num_classes)\n      self.assertTrue(\'Logits\' in end_points)\n      logits = end_points[\'Logits\']\n      self.assertListEqual(logits.get_shape().as_list(),\n                           [batch_size, num_classes])\n      self.assertTrue(\'AuxLogits\' in end_points)\n      aux_logits = end_points[\'AuxLogits\']\n      self.assertListEqual(aux_logits.get_shape().as_list(),\n                           [batch_size, num_classes])\n      pre_pool = end_points[\'PrePool\']\n      self.assertListEqual(pre_pool.get_shape().as_list(),\n                           [batch_size, 8, 8, 1536])\n\n  def testVariablesSetDevice(self):\n    batch_size = 5\n    height, width = 299, 299\n    num_classes = 1000\n    with self.test_session():\n      inputs = tf.random_uniform((batch_size, height, width, 3))\n      # Force all Variables to reside on the device.\n      with tf.variable_scope(\'on_cpu\'), tf.device(\'/cpu:0\'):\n        inception.inception_resnet_v2(inputs, num_classes)\n      with tf.variable_scope(\'on_gpu\'), tf.device(\'/gpu:0\'):\n        inception.inception_resnet_v2(inputs, num_classes)\n      for v in tf.get_collection(tf.GraphKeys.VARIABLES, scope=\'on_cpu\'):\n        self.assertDeviceEqual(v.device, \'/cpu:0\')\n      for v in tf.get_collection(tf.GraphKeys.VARIABLES, scope=\'on_gpu\'):\n        self.assertDeviceEqual(v.device, \'/gpu:0\')\n\n  def testHalfSizeImages(self):\n    batch_size = 5\n    height, width = 150, 150\n    num_classes = 1000\n    with self.test_session():\n      inputs = tf.random_uniform((batch_size, height, width, 3))\n      logits, end_points = inception.inception_resnet_v2(inputs, num_classes)\n      self.assertTrue(logits.op.name.startswith(\'InceptionResnetV2/Logits\'))\n      self.assertListEqual(logits.get_shape().as_list(),\n                           [batch_size, num_classes])\n      pre_pool = end_points[\'PrePool\']\n      self.assertListEqual(pre_pool.get_shape().as_list(),\n                           [batch_size, 3, 3, 1536])\n\n  def testUnknownBatchSize(self):\n    batch_size = 1\n    height, width = 299, 299\n    num_classes = 1000\n    with self.test_session() as sess:\n      inputs = tf.placeholder(tf.float32, (None, height, width, 3))\n      logits, _ = inception.inception_resnet_v2(inputs, num_classes)\n      self.assertTrue(logits.op.name.startswith(\'InceptionResnetV2/Logits\'))\n      self.assertListEqual(logits.get_shape().as_list(),\n                           [None, num_classes])\n      images = tf.random_uniform((batch_size, height, width, 3))\n      try:\n        sess.run(tf.global_variables_initializer())\n      except AttributeError:\n        sess.run(tf.initialize_all_variables())\n      output = sess.run(logits, {inputs: images.eval()})\n      self.assertEquals(output.shape, (batch_size, num_classes))\n\n  def testEvaluation(self):\n    batch_size = 2\n    height, width = 299, 299\n    num_classes = 1000\n    with self.test_session() as sess:\n      eval_inputs = tf.random_uniform((batch_size, height, width, 3))\n      logits, _ = inception.inception_resnet_v2(eval_inputs,\n                                                num_classes,\n                                                is_training=False)\n      predictions = tf.argmax(logits, 1)\n      try:\n        sess.run(tf.global_variables_initializer())\n      except AttributeError:\n        sess.run(tf.initialize_all_variables())\n      output = sess.run(predictions)\n      self.assertEquals(output.shape, (batch_size,))\n\n  def testTrainEvalWithReuse(self):\n    train_batch_size = 5\n    eval_batch_size = 2\n    height, width = 150, 150\n    num_classes = 1000\n    with self.test_session() as sess:\n      train_inputs = tf.random_uniform((train_batch_size, height, width, 3))\n      inception.inception_resnet_v2(train_inputs, num_classes)\n      eval_inputs = tf.random_uniform((eval_batch_size, height, width, 3))\n      logits, _ = inception.inception_resnet_v2(eval_inputs,\n                                                num_classes,\n                                                is_training=False,\n                                                reuse=True)\n      predictions = tf.argmax(logits, 1)\n      try:\n        sess.run(tf.global_variables_initializer())\n      except AttributeError:\n        sess.run(tf.initialize_all_variables())\n      output = sess.run(predictions)\n      self.assertEquals(output.shape, (eval_batch_size,))\n\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
finetuning/nets/inception_utils.py,3,"b'# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Contains common code shared by all inception models.\n\nUsage of arg scope:\n  with slim.arg_scope(inception_arg_scope()):\n    logits, end_points = inception.inception_v3(images, num_classes,\n                                                is_training=is_training)\n\n""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\n\nslim = tf.contrib.slim\n\n\ndef inception_arg_scope(weight_decay=0.00004,\n                        use_batch_norm=True,\n                        batch_norm_decay=0.9997,\n                        batch_norm_epsilon=0.001):\n  """"""Defines the default arg scope for inception models.\n\n  Args:\n    weight_decay: The weight decay to use for regularizing the model.\n    use_batch_norm: ""If `True`, batch_norm is applied after each convolution.\n    batch_norm_decay: Decay for batch norm moving average.\n    batch_norm_epsilon: Small float added to variance to avoid dividing by zero\n      in batch norm.\n\n  Returns:\n    An `arg_scope` to use for the inception models.\n  """"""\n  batch_norm_params = {\n      # Decay for the moving averages.\n      \'decay\': batch_norm_decay,\n      # epsilon to prevent 0s in variance.\n      \'epsilon\': batch_norm_epsilon,\n      # collection containing update_ops.\n      \'updates_collections\': tf.GraphKeys.UPDATE_OPS,\n  }\n  if use_batch_norm:\n    normalizer_fn = slim.batch_norm\n    normalizer_params = batch_norm_params\n  else:\n    normalizer_fn = None\n    normalizer_params = {}\n  # Set weight_decay for weights in Conv and FC layers.\n  with slim.arg_scope([slim.conv2d, slim.fully_connected],\n                      weights_regularizer=slim.l2_regularizer(weight_decay)):\n    with slim.arg_scope(\n        [slim.conv2d],\n        weights_initializer=slim.variance_scaling_initializer(),\n        activation_fn=tf.nn.relu,\n        normalizer_fn=normalizer_fn,\n        normalizer_params=normalizer_params) as sc:\n      return sc\n'"
finetuning/nets/inception_v1.py,60,"b'# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Contains the definition for inception v1 classification network.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\n\nfrom nets import inception_utils\n\nslim = tf.contrib.slim\ntrunc_normal = lambda stddev: tf.truncated_normal_initializer(0.0, stddev)\n\n\ndef inception_v1_base(inputs,\n                      final_endpoint=\'Mixed_5c\',\n                      scope=\'InceptionV1\'):\n  """"""Defines the Inception V1 base architecture.\n\n  This architecture is defined in:\n    Going deeper with convolutions\n    Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed,\n    Dragomir Anguelov, Dumitru Erhan, Vincent Vanhoucke, Andrew Rabinovich.\n    http://arxiv.org/pdf/1409.4842v1.pdf.\n\n  Args:\n    inputs: a tensor of size [batch_size, height, width, channels].\n    final_endpoint: specifies the endpoint to construct the network up to. It\n      can be one of [\'Conv2d_1a_7x7\', \'MaxPool_2a_3x3\', \'Conv2d_2b_1x1\',\n      \'Conv2d_2c_3x3\', \'MaxPool_3a_3x3\', \'Mixed_3b\', \'Mixed_3c\',\n      \'MaxPool_4a_3x3\', \'Mixed_4b\', \'Mixed_4c\', \'Mixed_4d\', \'Mixed_4e\',\n      \'Mixed_4f\', \'MaxPool_5a_2x2\', \'Mixed_5b\', \'Mixed_5c\']\n    scope: Optional variable_scope.\n\n  Returns:\n    A dictionary from components of the network to the corresponding activation.\n\n  Raises:\n    ValueError: if final_endpoint is not set to one of the predefined values.\n  """"""\n  end_points = {}\n  with tf.variable_scope(scope, \'InceptionV1\', [inputs]):\n    with slim.arg_scope(\n        [slim.conv2d, slim.fully_connected],\n        weights_initializer=trunc_normal(0.01)):\n      with slim.arg_scope([slim.conv2d, slim.max_pool2d],\n                          stride=1, padding=\'SAME\'):\n        end_point = \'Conv2d_1a_7x7\'\n        net = slim.conv2d(inputs, 64, [7, 7], stride=2, scope=end_point)\n        end_points[end_point] = net\n        if final_endpoint == end_point: return net, end_points\n        end_point = \'MaxPool_2a_3x3\'\n        net = slim.max_pool2d(net, [3, 3], stride=2, scope=end_point)\n        end_points[end_point] = net\n        if final_endpoint == end_point: return net, end_points\n        end_point = \'Conv2d_2b_1x1\'\n        net = slim.conv2d(net, 64, [1, 1], scope=end_point)\n        end_points[end_point] = net\n        if final_endpoint == end_point: return net, end_points\n        end_point = \'Conv2d_2c_3x3\'\n        net = slim.conv2d(net, 192, [3, 3], scope=end_point)\n        end_points[end_point] = net\n        if final_endpoint == end_point: return net, end_points\n        end_point = \'MaxPool_3a_3x3\'\n        net = slim.max_pool2d(net, [3, 3], stride=2, scope=end_point)\n        end_points[end_point] = net\n        if final_endpoint == end_point: return net, end_points\n\n        end_point = \'Mixed_3b\'\n        with tf.variable_scope(end_point):\n          with tf.variable_scope(\'Branch_0\'):\n            branch_0 = slim.conv2d(net, 64, [1, 1], scope=\'Conv2d_0a_1x1\')\n          with tf.variable_scope(\'Branch_1\'):\n            branch_1 = slim.conv2d(net, 96, [1, 1], scope=\'Conv2d_0a_1x1\')\n            branch_1 = slim.conv2d(branch_1, 128, [3, 3], scope=\'Conv2d_0b_3x3\')\n          with tf.variable_scope(\'Branch_2\'):\n            branch_2 = slim.conv2d(net, 16, [1, 1], scope=\'Conv2d_0a_1x1\')\n            branch_2 = slim.conv2d(branch_2, 32, [3, 3], scope=\'Conv2d_0b_3x3\')\n          with tf.variable_scope(\'Branch_3\'):\n            branch_3 = slim.max_pool2d(net, [3, 3], scope=\'MaxPool_0a_3x3\')\n            branch_3 = slim.conv2d(branch_3, 32, [1, 1], scope=\'Conv2d_0b_1x1\')\n          net = tf.concat(axis=3, values=[branch_0, branch_1, branch_2, branch_3])\n        end_points[end_point] = net\n        if final_endpoint == end_point: return net, end_points\n\n        end_point = \'Mixed_3c\'\n        with tf.variable_scope(end_point):\n          with tf.variable_scope(\'Branch_0\'):\n            branch_0 = slim.conv2d(net, 128, [1, 1], scope=\'Conv2d_0a_1x1\')\n          with tf.variable_scope(\'Branch_1\'):\n            branch_1 = slim.conv2d(net, 128, [1, 1], scope=\'Conv2d_0a_1x1\')\n            branch_1 = slim.conv2d(branch_1, 192, [3, 3], scope=\'Conv2d_0b_3x3\')\n          with tf.variable_scope(\'Branch_2\'):\n            branch_2 = slim.conv2d(net, 32, [1, 1], scope=\'Conv2d_0a_1x1\')\n            branch_2 = slim.conv2d(branch_2, 96, [3, 3], scope=\'Conv2d_0b_3x3\')\n          with tf.variable_scope(\'Branch_3\'):\n            branch_3 = slim.max_pool2d(net, [3, 3], scope=\'MaxPool_0a_3x3\')\n            branch_3 = slim.conv2d(branch_3, 64, [1, 1], scope=\'Conv2d_0b_1x1\')\n          net = tf.concat(axis=3, values=[branch_0, branch_1, branch_2, branch_3])\n        end_points[end_point] = net\n        if final_endpoint == end_point: return net, end_points\n\n        end_point = \'MaxPool_4a_3x3\'\n        net = slim.max_pool2d(net, [3, 3], stride=2, scope=end_point)\n        end_points[end_point] = net\n        if final_endpoint == end_point: return net, end_points\n\n        end_point = \'Mixed_4b\'\n        with tf.variable_scope(end_point):\n          with tf.variable_scope(\'Branch_0\'):\n            branch_0 = slim.conv2d(net, 192, [1, 1], scope=\'Conv2d_0a_1x1\')\n          with tf.variable_scope(\'Branch_1\'):\n            branch_1 = slim.conv2d(net, 96, [1, 1], scope=\'Conv2d_0a_1x1\')\n            branch_1 = slim.conv2d(branch_1, 208, [3, 3], scope=\'Conv2d_0b_3x3\')\n          with tf.variable_scope(\'Branch_2\'):\n            branch_2 = slim.conv2d(net, 16, [1, 1], scope=\'Conv2d_0a_1x1\')\n            branch_2 = slim.conv2d(branch_2, 48, [3, 3], scope=\'Conv2d_0b_3x3\')\n          with tf.variable_scope(\'Branch_3\'):\n            branch_3 = slim.max_pool2d(net, [3, 3], scope=\'MaxPool_0a_3x3\')\n            branch_3 = slim.conv2d(branch_3, 64, [1, 1], scope=\'Conv2d_0b_1x1\')\n          net = tf.concat(axis=3, values=[branch_0, branch_1, branch_2, branch_3])\n        end_points[end_point] = net\n        if final_endpoint == end_point: return net, end_points\n\n        end_point = \'Mixed_4c\'\n        with tf.variable_scope(end_point):\n          with tf.variable_scope(\'Branch_0\'):\n            branch_0 = slim.conv2d(net, 160, [1, 1], scope=\'Conv2d_0a_1x1\')\n          with tf.variable_scope(\'Branch_1\'):\n            branch_1 = slim.conv2d(net, 112, [1, 1], scope=\'Conv2d_0a_1x1\')\n            branch_1 = slim.conv2d(branch_1, 224, [3, 3], scope=\'Conv2d_0b_3x3\')\n          with tf.variable_scope(\'Branch_2\'):\n            branch_2 = slim.conv2d(net, 24, [1, 1], scope=\'Conv2d_0a_1x1\')\n            branch_2 = slim.conv2d(branch_2, 64, [3, 3], scope=\'Conv2d_0b_3x3\')\n          with tf.variable_scope(\'Branch_3\'):\n            branch_3 = slim.max_pool2d(net, [3, 3], scope=\'MaxPool_0a_3x3\')\n            branch_3 = slim.conv2d(branch_3, 64, [1, 1], scope=\'Conv2d_0b_1x1\')\n          net = tf.concat(axis=3, values=[branch_0, branch_1, branch_2, branch_3])\n        end_points[end_point] = net\n        if final_endpoint == end_point: return net, end_points\n\n        end_point = \'Mixed_4d\'\n        with tf.variable_scope(end_point):\n          with tf.variable_scope(\'Branch_0\'):\n            branch_0 = slim.conv2d(net, 128, [1, 1], scope=\'Conv2d_0a_1x1\')\n          with tf.variable_scope(\'Branch_1\'):\n            branch_1 = slim.conv2d(net, 128, [1, 1], scope=\'Conv2d_0a_1x1\')\n            branch_1 = slim.conv2d(branch_1, 256, [3, 3], scope=\'Conv2d_0b_3x3\')\n          with tf.variable_scope(\'Branch_2\'):\n            branch_2 = slim.conv2d(net, 24, [1, 1], scope=\'Conv2d_0a_1x1\')\n            branch_2 = slim.conv2d(branch_2, 64, [3, 3], scope=\'Conv2d_0b_3x3\')\n          with tf.variable_scope(\'Branch_3\'):\n            branch_3 = slim.max_pool2d(net, [3, 3], scope=\'MaxPool_0a_3x3\')\n            branch_3 = slim.conv2d(branch_3, 64, [1, 1], scope=\'Conv2d_0b_1x1\')\n          net = tf.concat(axis=3, values=[branch_0, branch_1, branch_2, branch_3])\n        end_points[end_point] = net\n        if final_endpoint == end_point: return net, end_points\n\n        end_point = \'Mixed_4e\'\n        with tf.variable_scope(end_point):\n          with tf.variable_scope(\'Branch_0\'):\n            branch_0 = slim.conv2d(net, 112, [1, 1], scope=\'Conv2d_0a_1x1\')\n          with tf.variable_scope(\'Branch_1\'):\n            branch_1 = slim.conv2d(net, 144, [1, 1], scope=\'Conv2d_0a_1x1\')\n            branch_1 = slim.conv2d(branch_1, 288, [3, 3], scope=\'Conv2d_0b_3x3\')\n          with tf.variable_scope(\'Branch_2\'):\n            branch_2 = slim.conv2d(net, 32, [1, 1], scope=\'Conv2d_0a_1x1\')\n            branch_2 = slim.conv2d(branch_2, 64, [3, 3], scope=\'Conv2d_0b_3x3\')\n          with tf.variable_scope(\'Branch_3\'):\n            branch_3 = slim.max_pool2d(net, [3, 3], scope=\'MaxPool_0a_3x3\')\n            branch_3 = slim.conv2d(branch_3, 64, [1, 1], scope=\'Conv2d_0b_1x1\')\n          net = tf.concat(axis=3, values=[branch_0, branch_1, branch_2, branch_3])\n        end_points[end_point] = net\n        if final_endpoint == end_point: return net, end_points\n\n        end_point = \'Mixed_4f\'\n        with tf.variable_scope(end_point):\n          with tf.variable_scope(\'Branch_0\'):\n            branch_0 = slim.conv2d(net, 256, [1, 1], scope=\'Conv2d_0a_1x1\')\n          with tf.variable_scope(\'Branch_1\'):\n            branch_1 = slim.conv2d(net, 160, [1, 1], scope=\'Conv2d_0a_1x1\')\n            branch_1 = slim.conv2d(branch_1, 320, [3, 3], scope=\'Conv2d_0b_3x3\')\n          with tf.variable_scope(\'Branch_2\'):\n            branch_2 = slim.conv2d(net, 32, [1, 1], scope=\'Conv2d_0a_1x1\')\n            branch_2 = slim.conv2d(branch_2, 128, [3, 3], scope=\'Conv2d_0b_3x3\')\n          with tf.variable_scope(\'Branch_3\'):\n            branch_3 = slim.max_pool2d(net, [3, 3], scope=\'MaxPool_0a_3x3\')\n            branch_3 = slim.conv2d(branch_3, 128, [1, 1], scope=\'Conv2d_0b_1x1\')\n          net = tf.concat(axis=3, values=[branch_0, branch_1, branch_2, branch_3])\n        end_points[end_point] = net\n        if final_endpoint == end_point: return net, end_points\n\n        end_point = \'MaxPool_5a_2x2\'\n        net = slim.max_pool2d(net, [2, 2], stride=2, scope=end_point)\n        end_points[end_point] = net\n        if final_endpoint == end_point: return net, end_points\n\n        end_point = \'Mixed_5b\'\n        with tf.variable_scope(end_point):\n          with tf.variable_scope(\'Branch_0\'):\n            branch_0 = slim.conv2d(net, 256, [1, 1], scope=\'Conv2d_0a_1x1\')\n          with tf.variable_scope(\'Branch_1\'):\n            branch_1 = slim.conv2d(net, 160, [1, 1], scope=\'Conv2d_0a_1x1\')\n            branch_1 = slim.conv2d(branch_1, 320, [3, 3], scope=\'Conv2d_0b_3x3\')\n          with tf.variable_scope(\'Branch_2\'):\n            branch_2 = slim.conv2d(net, 32, [1, 1], scope=\'Conv2d_0a_1x1\')\n            branch_2 = slim.conv2d(branch_2, 128, [3, 3], scope=\'Conv2d_0a_3x3\')\n          with tf.variable_scope(\'Branch_3\'):\n            branch_3 = slim.max_pool2d(net, [3, 3], scope=\'MaxPool_0a_3x3\')\n            branch_3 = slim.conv2d(branch_3, 128, [1, 1], scope=\'Conv2d_0b_1x1\')\n          net = tf.concat(axis=3, values=[branch_0, branch_1, branch_2, branch_3])\n        end_points[end_point] = net\n        if final_endpoint == end_point: return net, end_points\n\n        end_point = \'Mixed_5c\'\n        with tf.variable_scope(end_point):\n          with tf.variable_scope(\'Branch_0\'):\n            branch_0 = slim.conv2d(net, 384, [1, 1], scope=\'Conv2d_0a_1x1\')\n          with tf.variable_scope(\'Branch_1\'):\n            branch_1 = slim.conv2d(net, 192, [1, 1], scope=\'Conv2d_0a_1x1\')\n            branch_1 = slim.conv2d(branch_1, 384, [3, 3], scope=\'Conv2d_0b_3x3\')\n          with tf.variable_scope(\'Branch_2\'):\n            branch_2 = slim.conv2d(net, 48, [1, 1], scope=\'Conv2d_0a_1x1\')\n            branch_2 = slim.conv2d(branch_2, 128, [3, 3], scope=\'Conv2d_0b_3x3\')\n          with tf.variable_scope(\'Branch_3\'):\n            branch_3 = slim.max_pool2d(net, [3, 3], scope=\'MaxPool_0a_3x3\')\n            branch_3 = slim.conv2d(branch_3, 128, [1, 1], scope=\'Conv2d_0b_1x1\')\n          net = tf.concat(axis=3, values=[branch_0, branch_1, branch_2, branch_3])\n        end_points[end_point] = net\n        if final_endpoint == end_point: return net, end_points\n    raise ValueError(\'Unknown final endpoint %s\' % final_endpoint)\n\n\ndef inception_v1(inputs,\n                 num_classes=1000,\n                 is_training=True,\n                 dropout_keep_prob=0.8,\n                 prediction_fn=slim.softmax,\n                 spatial_squeeze=True,\n                 reuse=None,\n                 scope=\'InceptionV1\'):\n  """"""Defines the Inception V1 architecture.\n\n  This architecture is defined in:\n\n    Going deeper with convolutions\n    Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed,\n    Dragomir Anguelov, Dumitru Erhan, Vincent Vanhoucke, Andrew Rabinovich.\n    http://arxiv.org/pdf/1409.4842v1.pdf.\n\n  The default image size used to train this network is 224x224.\n\n  Args:\n    inputs: a tensor of size [batch_size, height, width, channels].\n    num_classes: number of predicted classes.\n    is_training: whether is training or not.\n    dropout_keep_prob: the percentage of activation values that are retained.\n    prediction_fn: a function to get predictions out of logits.\n    spatial_squeeze: if True, logits is of shape is [B, C], if false logits is\n        of shape [B, 1, 1, C], where B is batch_size and C is number of classes.\n    reuse: whether or not the network and its variables should be reused. To be\n      able to reuse \'scope\' must be given.\n    scope: Optional variable_scope.\n\n  Returns:\n    logits: the pre-softmax activations, a tensor of size\n      [batch_size, num_classes]\n    end_points: a dictionary from components of the network to the corresponding\n      activation.\n  """"""\n  # Final pooling and prediction\n  with tf.variable_scope(scope, \'InceptionV1\', [inputs, num_classes],\n                         reuse=reuse) as scope:\n    with slim.arg_scope([slim.batch_norm, slim.dropout],\n                        is_training=is_training):\n      net, end_points = inception_v1_base(inputs, scope=scope)\n      with tf.variable_scope(\'Logits\'):\n        net = slim.avg_pool2d(net, [7, 7], stride=1, scope=\'MaxPool_0a_7x7\')\n        net = slim.dropout(net,\n                           dropout_keep_prob, scope=\'Dropout_0b\')\n        logits = slim.conv2d(net, num_classes, [1, 1], activation_fn=None,\n                             normalizer_fn=None, scope=\'Conv2d_0c_1x1\')\n        if spatial_squeeze:\n          logits = tf.squeeze(logits, [1, 2], name=\'SpatialSqueeze\')\n\n        end_points[\'Logits\'] = logits\n        end_points[\'Predictions\'] = prediction_fn(logits, scope=\'Predictions\')\n  return logits, end_points\ninception_v1.default_image_size = 224\n\ninception_v1_arg_scope = inception_utils.inception_arg_scope\n'"
finetuning/nets/inception_v1_test.py,25,"b'# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Tests for nets.inception_v1.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport numpy as np\nimport tensorflow as tf\n\nfrom nets import inception\n\nslim = tf.contrib.slim\n\n\nclass InceptionV1Test(tf.test.TestCase):\n\n  def testBuildClassificationNetwork(self):\n    batch_size = 5\n    height, width = 224, 224\n    num_classes = 1000\n\n    inputs = tf.random_uniform((batch_size, height, width, 3))\n    logits, end_points = inception.inception_v1(inputs, num_classes)\n    self.assertTrue(logits.op.name.startswith(\'InceptionV1/Logits\'))\n    self.assertListEqual(logits.get_shape().as_list(),\n                         [batch_size, num_classes])\n    self.assertTrue(\'Predictions\' in end_points)\n    self.assertListEqual(end_points[\'Predictions\'].get_shape().as_list(),\n                         [batch_size, num_classes])\n\n  def testBuildBaseNetwork(self):\n    batch_size = 5\n    height, width = 224, 224\n\n    inputs = tf.random_uniform((batch_size, height, width, 3))\n    mixed_6c, end_points = inception.inception_v1_base(inputs)\n    self.assertTrue(mixed_6c.op.name.startswith(\'InceptionV1/Mixed_5c\'))\n    self.assertListEqual(mixed_6c.get_shape().as_list(),\n                         [batch_size, 7, 7, 1024])\n    expected_endpoints = [\'Conv2d_1a_7x7\', \'MaxPool_2a_3x3\', \'Conv2d_2b_1x1\',\n                          \'Conv2d_2c_3x3\', \'MaxPool_3a_3x3\', \'Mixed_3b\',\n                          \'Mixed_3c\', \'MaxPool_4a_3x3\', \'Mixed_4b\', \'Mixed_4c\',\n                          \'Mixed_4d\', \'Mixed_4e\', \'Mixed_4f\', \'MaxPool_5a_2x2\',\n                          \'Mixed_5b\', \'Mixed_5c\']\n    self.assertItemsEqual(end_points.keys(), expected_endpoints)\n\n  def testBuildOnlyUptoFinalEndpoint(self):\n    batch_size = 5\n    height, width = 224, 224\n    endpoints = [\'Conv2d_1a_7x7\', \'MaxPool_2a_3x3\', \'Conv2d_2b_1x1\',\n                 \'Conv2d_2c_3x3\', \'MaxPool_3a_3x3\', \'Mixed_3b\', \'Mixed_3c\',\n                 \'MaxPool_4a_3x3\', \'Mixed_4b\', \'Mixed_4c\', \'Mixed_4d\',\n                 \'Mixed_4e\', \'Mixed_4f\', \'MaxPool_5a_2x2\', \'Mixed_5b\',\n                 \'Mixed_5c\']\n    for index, endpoint in enumerate(endpoints):\n      with tf.Graph().as_default():\n        inputs = tf.random_uniform((batch_size, height, width, 3))\n        out_tensor, end_points = inception.inception_v1_base(\n            inputs, final_endpoint=endpoint)\n        self.assertTrue(out_tensor.op.name.startswith(\n            \'InceptionV1/\' + endpoint))\n        self.assertItemsEqual(endpoints[:index+1], end_points)\n\n  def testBuildAndCheckAllEndPointsUptoMixed5c(self):\n    batch_size = 5\n    height, width = 224, 224\n\n    inputs = tf.random_uniform((batch_size, height, width, 3))\n    _, end_points = inception.inception_v1_base(inputs,\n                                                final_endpoint=\'Mixed_5c\')\n    endpoints_shapes = {\'Conv2d_1a_7x7\': [5, 112, 112, 64],\n                        \'MaxPool_2a_3x3\': [5, 56, 56, 64],\n                        \'Conv2d_2b_1x1\': [5, 56, 56, 64],\n                        \'Conv2d_2c_3x3\': [5, 56, 56, 192],\n                        \'MaxPool_3a_3x3\': [5, 28, 28, 192],\n                        \'Mixed_3b\': [5, 28, 28, 256],\n                        \'Mixed_3c\': [5, 28, 28, 480],\n                        \'MaxPool_4a_3x3\': [5, 14, 14, 480],\n                        \'Mixed_4b\': [5, 14, 14, 512],\n                        \'Mixed_4c\': [5, 14, 14, 512],\n                        \'Mixed_4d\': [5, 14, 14, 512],\n                        \'Mixed_4e\': [5, 14, 14, 528],\n                        \'Mixed_4f\': [5, 14, 14, 832],\n                        \'MaxPool_5a_2x2\': [5, 7, 7, 832],\n                        \'Mixed_5b\': [5, 7, 7, 832],\n                        \'Mixed_5c\': [5, 7, 7, 1024]}\n\n    self.assertItemsEqual(endpoints_shapes.keys(), end_points.keys())\n    for endpoint_name in endpoints_shapes:\n      expected_shape = endpoints_shapes[endpoint_name]\n      self.assertTrue(endpoint_name in end_points)\n      self.assertListEqual(end_points[endpoint_name].get_shape().as_list(),\n                           expected_shape)\n\n  def testModelHasExpectedNumberOfParameters(self):\n    batch_size = 5\n    height, width = 224, 224\n    inputs = tf.random_uniform((batch_size, height, width, 3))\n    with slim.arg_scope(inception.inception_v1_arg_scope()):\n      inception.inception_v1_base(inputs)\n    total_params, _ = slim.model_analyzer.analyze_vars(\n        slim.get_model_variables())\n    self.assertAlmostEqual(5607184, total_params)\n\n  def testHalfSizeImages(self):\n    batch_size = 5\n    height, width = 112, 112\n\n    inputs = tf.random_uniform((batch_size, height, width, 3))\n    mixed_5c, _ = inception.inception_v1_base(inputs)\n    self.assertTrue(mixed_5c.op.name.startswith(\'InceptionV1/Mixed_5c\'))\n    self.assertListEqual(mixed_5c.get_shape().as_list(),\n                         [batch_size, 4, 4, 1024])\n\n  def testUnknownImageShape(self):\n    tf.reset_default_graph()\n    batch_size = 2\n    height, width = 224, 224\n    num_classes = 1000\n    input_np = np.random.uniform(0, 1, (batch_size, height, width, 3))\n    with self.test_session() as sess:\n      inputs = tf.placeholder(tf.float32, shape=(batch_size, None, None, 3))\n      logits, end_points = inception.inception_v1(inputs, num_classes)\n      self.assertTrue(logits.op.name.startswith(\'InceptionV1/Logits\'))\n      self.assertListEqual(logits.get_shape().as_list(),\n                           [batch_size, num_classes])\n      pre_pool = end_points[\'Mixed_5c\']\n      feed_dict = {inputs: input_np}\n      tf.global_variables_initializer().run()\n      pre_pool_out = sess.run(pre_pool, feed_dict=feed_dict)\n      self.assertListEqual(list(pre_pool_out.shape), [batch_size, 7, 7, 1024])\n\n  def testUnknowBatchSize(self):\n    batch_size = 1\n    height, width = 224, 224\n    num_classes = 1000\n\n    inputs = tf.placeholder(tf.float32, (None, height, width, 3))\n    logits, _ = inception.inception_v1(inputs, num_classes)\n    self.assertTrue(logits.op.name.startswith(\'InceptionV1/Logits\'))\n    self.assertListEqual(logits.get_shape().as_list(),\n                         [None, num_classes])\n    images = tf.random_uniform((batch_size, height, width, 3))\n\n    with self.test_session() as sess:\n      sess.run(tf.global_variables_initializer())\n      output = sess.run(logits, {inputs: images.eval()})\n      self.assertEquals(output.shape, (batch_size, num_classes))\n\n  def testEvaluation(self):\n    batch_size = 2\n    height, width = 224, 224\n    num_classes = 1000\n\n    eval_inputs = tf.random_uniform((batch_size, height, width, 3))\n    logits, _ = inception.inception_v1(eval_inputs, num_classes,\n                                       is_training=False)\n    predictions = tf.argmax(logits, 1)\n\n    with self.test_session() as sess:\n      sess.run(tf.global_variables_initializer())\n      output = sess.run(predictions)\n      self.assertEquals(output.shape, (batch_size,))\n\n  def testTrainEvalWithReuse(self):\n    train_batch_size = 5\n    eval_batch_size = 2\n    height, width = 224, 224\n    num_classes = 1000\n\n    train_inputs = tf.random_uniform((train_batch_size, height, width, 3))\n    inception.inception_v1(train_inputs, num_classes)\n    eval_inputs = tf.random_uniform((eval_batch_size, height, width, 3))\n    logits, _ = inception.inception_v1(eval_inputs, num_classes, reuse=True)\n    predictions = tf.argmax(logits, 1)\n\n    with self.test_session() as sess:\n      sess.run(tf.global_variables_initializer())\n      output = sess.run(predictions)\n      self.assertEquals(output.shape, (eval_batch_size,))\n\n  def testLogitsNotSqueezed(self):\n    num_classes = 25\n    images = tf.random_uniform([1, 224, 224, 3])\n    logits, _ = inception.inception_v1(images,\n                                       num_classes=num_classes,\n                                       spatial_squeeze=False)\n\n    with self.test_session() as sess:\n      tf.global_variables_initializer().run()\n      logits_out = sess.run(logits)\n      self.assertListEqual(list(logits_out.shape), [1, 1, 1, num_classes])\n\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
finetuning/nets/inception_v2.py,68,"b'# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Contains the definition for inception v2 classification network.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\n\nfrom nets import inception_utils\n\nslim = tf.contrib.slim\ntrunc_normal = lambda stddev: tf.truncated_normal_initializer(0.0, stddev)\n\n\ndef inception_v2_base(inputs,\n                      final_endpoint=\'Mixed_5c\',\n                      min_depth=16,\n                      depth_multiplier=1.0,\n                      scope=None):\n  """"""Inception v2 (6a2).\n\n  Constructs an Inception v2 network from inputs to the given final endpoint.\n  This method can construct the network up to the layer inception(5b) as\n  described in http://arxiv.org/abs/1502.03167.\n\n  Args:\n    inputs: a tensor of shape [batch_size, height, width, channels].\n    final_endpoint: specifies the endpoint to construct the network up to. It\n      can be one of [\'Conv2d_1a_7x7\', \'MaxPool_2a_3x3\', \'Conv2d_2b_1x1\',\n      \'Conv2d_2c_3x3\', \'MaxPool_3a_3x3\', \'Mixed_3b\', \'Mixed_3c\', \'Mixed_4a\',\n      \'Mixed_4b\', \'Mixed_4c\', \'Mixed_4d\', \'Mixed_4e\', \'Mixed_5a\', \'Mixed_5b\',\n      \'Mixed_5c\'].\n    min_depth: Minimum depth value (number of channels) for all convolution ops.\n      Enforced when depth_multiplier < 1, and not an active constraint when\n      depth_multiplier >= 1.\n    depth_multiplier: Float multiplier for the depth (number of channels)\n      for all convolution ops. The value must be greater than zero. Typical\n      usage will be to set this value in (0, 1) to reduce the number of\n      parameters or computation cost of the model.\n    scope: Optional variable_scope.\n\n  Returns:\n    tensor_out: output tensor corresponding to the final_endpoint.\n    end_points: a set of activations for external use, for example summaries or\n                losses.\n\n  Raises:\n    ValueError: if final_endpoint is not set to one of the predefined values,\n                or depth_multiplier <= 0\n  """"""\n\n  # end_points will collect relevant activations for external use, for example\n  # summaries or losses.\n  end_points = {}\n\n  # Used to find thinned depths for each layer.\n  if depth_multiplier <= 0:\n    raise ValueError(\'depth_multiplier is not greater than zero.\')\n  depth = lambda d: max(int(d * depth_multiplier), min_depth)\n\n  with tf.variable_scope(scope, \'InceptionV2\', [inputs]):\n    with slim.arg_scope(\n        [slim.conv2d, slim.max_pool2d, slim.avg_pool2d, slim.separable_conv2d],\n        stride=1, padding=\'SAME\'):\n\n      # Note that sizes in the comments below assume an input spatial size of\n      # 224x224, however, the inputs can be of any size greater 32x32.\n\n      # 224 x 224 x 3\n      end_point = \'Conv2d_1a_7x7\'\n      # depthwise_multiplier here is different from depth_multiplier.\n      # depthwise_multiplier determines the output channels of the initial\n      # depthwise conv (see docs for tf.nn.separable_conv2d), while\n      # depth_multiplier controls the # channels of the subsequent 1x1\n      # convolution. Must have\n      #   in_channels * depthwise_multipler <= out_channels\n      # so that the separable convolution is not overparameterized.\n      depthwise_multiplier = min(int(depth(64) / 3), 8)\n      net = slim.separable_conv2d(\n          inputs, depth(64), [7, 7], depth_multiplier=depthwise_multiplier,\n          stride=2, weights_initializer=trunc_normal(1.0),\n          scope=end_point)\n      end_points[end_point] = net\n      if end_point == final_endpoint: return net, end_points\n      # 112 x 112 x 64\n      end_point = \'MaxPool_2a_3x3\'\n      net = slim.max_pool2d(net, [3, 3], scope=end_point, stride=2)\n      end_points[end_point] = net\n      if end_point == final_endpoint: return net, end_points\n      # 56 x 56 x 64\n      end_point = \'Conv2d_2b_1x1\'\n      net = slim.conv2d(net, depth(64), [1, 1], scope=end_point,\n                        weights_initializer=trunc_normal(0.1))\n      end_points[end_point] = net\n      if end_point == final_endpoint: return net, end_points\n      # 56 x 56 x 64\n      end_point = \'Conv2d_2c_3x3\'\n      net = slim.conv2d(net, depth(192), [3, 3], scope=end_point)\n      end_points[end_point] = net\n      if end_point == final_endpoint: return net, end_points\n      # 56 x 56 x 192\n      end_point = \'MaxPool_3a_3x3\'\n      net = slim.max_pool2d(net, [3, 3], scope=end_point, stride=2)\n      end_points[end_point] = net\n      if end_point == final_endpoint: return net, end_points\n      # 28 x 28 x 192\n      # Inception module.\n      end_point = \'Mixed_3b\'\n      with tf.variable_scope(end_point):\n        with tf.variable_scope(\'Branch_0\'):\n          branch_0 = slim.conv2d(net, depth(64), [1, 1], scope=\'Conv2d_0a_1x1\')\n        with tf.variable_scope(\'Branch_1\'):\n          branch_1 = slim.conv2d(\n              net, depth(64), [1, 1],\n              weights_initializer=trunc_normal(0.09),\n              scope=\'Conv2d_0a_1x1\')\n          branch_1 = slim.conv2d(branch_1, depth(64), [3, 3],\n                                 scope=\'Conv2d_0b_3x3\')\n        with tf.variable_scope(\'Branch_2\'):\n          branch_2 = slim.conv2d(\n              net, depth(64), [1, 1],\n              weights_initializer=trunc_normal(0.09),\n              scope=\'Conv2d_0a_1x1\')\n          branch_2 = slim.conv2d(branch_2, depth(96), [3, 3],\n                                 scope=\'Conv2d_0b_3x3\')\n          branch_2 = slim.conv2d(branch_2, depth(96), [3, 3],\n                                 scope=\'Conv2d_0c_3x3\')\n        with tf.variable_scope(\'Branch_3\'):\n          branch_3 = slim.avg_pool2d(net, [3, 3], scope=\'AvgPool_0a_3x3\')\n          branch_3 = slim.conv2d(\n              branch_3, depth(32), [1, 1],\n              weights_initializer=trunc_normal(0.1),\n              scope=\'Conv2d_0b_1x1\')\n        net = tf.concat(axis=3, values=[branch_0, branch_1, branch_2, branch_3])\n        end_points[end_point] = net\n        if end_point == final_endpoint: return net, end_points\n      # 28 x 28 x 256\n      end_point = \'Mixed_3c\'\n      with tf.variable_scope(end_point):\n        with tf.variable_scope(\'Branch_0\'):\n          branch_0 = slim.conv2d(net, depth(64), [1, 1], scope=\'Conv2d_0a_1x1\')\n        with tf.variable_scope(\'Branch_1\'):\n          branch_1 = slim.conv2d(\n              net, depth(64), [1, 1],\n              weights_initializer=trunc_normal(0.09),\n              scope=\'Conv2d_0a_1x1\')\n          branch_1 = slim.conv2d(branch_1, depth(96), [3, 3],\n                                 scope=\'Conv2d_0b_3x3\')\n        with tf.variable_scope(\'Branch_2\'):\n          branch_2 = slim.conv2d(\n              net, depth(64), [1, 1],\n              weights_initializer=trunc_normal(0.09),\n              scope=\'Conv2d_0a_1x1\')\n          branch_2 = slim.conv2d(branch_2, depth(96), [3, 3],\n                                 scope=\'Conv2d_0b_3x3\')\n          branch_2 = slim.conv2d(branch_2, depth(96), [3, 3],\n                                 scope=\'Conv2d_0c_3x3\')\n        with tf.variable_scope(\'Branch_3\'):\n          branch_3 = slim.avg_pool2d(net, [3, 3], scope=\'AvgPool_0a_3x3\')\n          branch_3 = slim.conv2d(\n              branch_3, depth(64), [1, 1],\n              weights_initializer=trunc_normal(0.1),\n              scope=\'Conv2d_0b_1x1\')\n        net = tf.concat(axis=3, values=[branch_0, branch_1, branch_2, branch_3])\n        end_points[end_point] = net\n        if end_point == final_endpoint: return net, end_points\n      # 28 x 28 x 320\n      end_point = \'Mixed_4a\'\n      with tf.variable_scope(end_point):\n        with tf.variable_scope(\'Branch_0\'):\n          branch_0 = slim.conv2d(\n              net, depth(128), [1, 1],\n              weights_initializer=trunc_normal(0.09),\n              scope=\'Conv2d_0a_1x1\')\n          branch_0 = slim.conv2d(branch_0, depth(160), [3, 3], stride=2,\n                                 scope=\'Conv2d_1a_3x3\')\n        with tf.variable_scope(\'Branch_1\'):\n          branch_1 = slim.conv2d(\n              net, depth(64), [1, 1],\n              weights_initializer=trunc_normal(0.09),\n              scope=\'Conv2d_0a_1x1\')\n          branch_1 = slim.conv2d(\n              branch_1, depth(96), [3, 3], scope=\'Conv2d_0b_3x3\')\n          branch_1 = slim.conv2d(\n              branch_1, depth(96), [3, 3], stride=2, scope=\'Conv2d_1a_3x3\')\n        with tf.variable_scope(\'Branch_2\'):\n          branch_2 = slim.max_pool2d(\n              net, [3, 3], stride=2, scope=\'MaxPool_1a_3x3\')\n        net = tf.concat(axis=3, values=[branch_0, branch_1, branch_2])\n        end_points[end_point] = net\n        if end_point == final_endpoint: return net, end_points\n      # 14 x 14 x 576\n      end_point = \'Mixed_4b\'\n      with tf.variable_scope(end_point):\n        with tf.variable_scope(\'Branch_0\'):\n          branch_0 = slim.conv2d(net, depth(224), [1, 1], scope=\'Conv2d_0a_1x1\')\n        with tf.variable_scope(\'Branch_1\'):\n          branch_1 = slim.conv2d(\n              net, depth(64), [1, 1],\n              weights_initializer=trunc_normal(0.09),\n              scope=\'Conv2d_0a_1x1\')\n          branch_1 = slim.conv2d(\n              branch_1, depth(96), [3, 3], scope=\'Conv2d_0b_3x3\')\n        with tf.variable_scope(\'Branch_2\'):\n          branch_2 = slim.conv2d(\n              net, depth(96), [1, 1],\n              weights_initializer=trunc_normal(0.09),\n              scope=\'Conv2d_0a_1x1\')\n          branch_2 = slim.conv2d(branch_2, depth(128), [3, 3],\n                                 scope=\'Conv2d_0b_3x3\')\n          branch_2 = slim.conv2d(branch_2, depth(128), [3, 3],\n                                 scope=\'Conv2d_0c_3x3\')\n        with tf.variable_scope(\'Branch_3\'):\n          branch_3 = slim.avg_pool2d(net, [3, 3], scope=\'AvgPool_0a_3x3\')\n          branch_3 = slim.conv2d(\n              branch_3, depth(128), [1, 1],\n              weights_initializer=trunc_normal(0.1),\n              scope=\'Conv2d_0b_1x1\')\n        net = tf.concat(axis=3, values=[branch_0, branch_1, branch_2, branch_3])\n        end_points[end_point] = net\n        if end_point == final_endpoint: return net, end_points\n      # 14 x 14 x 576\n      end_point = \'Mixed_4c\'\n      with tf.variable_scope(end_point):\n        with tf.variable_scope(\'Branch_0\'):\n          branch_0 = slim.conv2d(net, depth(192), [1, 1], scope=\'Conv2d_0a_1x1\')\n        with tf.variable_scope(\'Branch_1\'):\n          branch_1 = slim.conv2d(\n              net, depth(96), [1, 1],\n              weights_initializer=trunc_normal(0.09),\n              scope=\'Conv2d_0a_1x1\')\n          branch_1 = slim.conv2d(branch_1, depth(128), [3, 3],\n                                 scope=\'Conv2d_0b_3x3\')\n        with tf.variable_scope(\'Branch_2\'):\n          branch_2 = slim.conv2d(\n              net, depth(96), [1, 1],\n              weights_initializer=trunc_normal(0.09),\n              scope=\'Conv2d_0a_1x1\')\n          branch_2 = slim.conv2d(branch_2, depth(128), [3, 3],\n                                 scope=\'Conv2d_0b_3x3\')\n          branch_2 = slim.conv2d(branch_2, depth(128), [3, 3],\n                                 scope=\'Conv2d_0c_3x3\')\n        with tf.variable_scope(\'Branch_3\'):\n          branch_3 = slim.avg_pool2d(net, [3, 3], scope=\'AvgPool_0a_3x3\')\n          branch_3 = slim.conv2d(\n              branch_3, depth(128), [1, 1],\n              weights_initializer=trunc_normal(0.1),\n              scope=\'Conv2d_0b_1x1\')\n        net = tf.concat(axis=3, values=[branch_0, branch_1, branch_2, branch_3])\n        end_points[end_point] = net\n        if end_point == final_endpoint: return net, end_points\n      # 14 x 14 x 576\n      end_point = \'Mixed_4d\'\n      with tf.variable_scope(end_point):\n        with tf.variable_scope(\'Branch_0\'):\n          branch_0 = slim.conv2d(net, depth(160), [1, 1], scope=\'Conv2d_0a_1x1\')\n        with tf.variable_scope(\'Branch_1\'):\n          branch_1 = slim.conv2d(\n              net, depth(128), [1, 1],\n              weights_initializer=trunc_normal(0.09),\n              scope=\'Conv2d_0a_1x1\')\n          branch_1 = slim.conv2d(branch_1, depth(160), [3, 3],\n                                 scope=\'Conv2d_0b_3x3\')\n        with tf.variable_scope(\'Branch_2\'):\n          branch_2 = slim.conv2d(\n              net, depth(128), [1, 1],\n              weights_initializer=trunc_normal(0.09),\n              scope=\'Conv2d_0a_1x1\')\n          branch_2 = slim.conv2d(branch_2, depth(160), [3, 3],\n                                 scope=\'Conv2d_0b_3x3\')\n          branch_2 = slim.conv2d(branch_2, depth(160), [3, 3],\n                                 scope=\'Conv2d_0c_3x3\')\n        with tf.variable_scope(\'Branch_3\'):\n          branch_3 = slim.avg_pool2d(net, [3, 3], scope=\'AvgPool_0a_3x3\')\n          branch_3 = slim.conv2d(\n              branch_3, depth(96), [1, 1],\n              weights_initializer=trunc_normal(0.1),\n              scope=\'Conv2d_0b_1x1\')\n        net = tf.concat(axis=3, values=[branch_0, branch_1, branch_2, branch_3])\n        end_points[end_point] = net\n        if end_point == final_endpoint: return net, end_points\n\n      # 14 x 14 x 576\n      end_point = \'Mixed_4e\'\n      with tf.variable_scope(end_point):\n        with tf.variable_scope(\'Branch_0\'):\n          branch_0 = slim.conv2d(net, depth(96), [1, 1], scope=\'Conv2d_0a_1x1\')\n        with tf.variable_scope(\'Branch_1\'):\n          branch_1 = slim.conv2d(\n              net, depth(128), [1, 1],\n              weights_initializer=trunc_normal(0.09),\n              scope=\'Conv2d_0a_1x1\')\n          branch_1 = slim.conv2d(branch_1, depth(192), [3, 3],\n                                 scope=\'Conv2d_0b_3x3\')\n        with tf.variable_scope(\'Branch_2\'):\n          branch_2 = slim.conv2d(\n              net, depth(160), [1, 1],\n              weights_initializer=trunc_normal(0.09),\n              scope=\'Conv2d_0a_1x1\')\n          branch_2 = slim.conv2d(branch_2, depth(192), [3, 3],\n                                 scope=\'Conv2d_0b_3x3\')\n          branch_2 = slim.conv2d(branch_2, depth(192), [3, 3],\n                                 scope=\'Conv2d_0c_3x3\')\n        with tf.variable_scope(\'Branch_3\'):\n          branch_3 = slim.avg_pool2d(net, [3, 3], scope=\'AvgPool_0a_3x3\')\n          branch_3 = slim.conv2d(\n              branch_3, depth(96), [1, 1],\n              weights_initializer=trunc_normal(0.1),\n              scope=\'Conv2d_0b_1x1\')\n        net = tf.concat(axis=3, values=[branch_0, branch_1, branch_2, branch_3])\n        end_points[end_point] = net\n        if end_point == final_endpoint: return net, end_points\n      # 14 x 14 x 576\n      end_point = \'Mixed_5a\'\n      with tf.variable_scope(end_point):\n        with tf.variable_scope(\'Branch_0\'):\n          branch_0 = slim.conv2d(\n              net, depth(128), [1, 1],\n              weights_initializer=trunc_normal(0.09),\n              scope=\'Conv2d_0a_1x1\')\n          branch_0 = slim.conv2d(branch_0, depth(192), [3, 3], stride=2,\n                                 scope=\'Conv2d_1a_3x3\')\n        with tf.variable_scope(\'Branch_1\'):\n          branch_1 = slim.conv2d(\n              net, depth(192), [1, 1],\n              weights_initializer=trunc_normal(0.09),\n              scope=\'Conv2d_0a_1x1\')\n          branch_1 = slim.conv2d(branch_1, depth(256), [3, 3],\n                                 scope=\'Conv2d_0b_3x3\')\n          branch_1 = slim.conv2d(branch_1, depth(256), [3, 3], stride=2,\n                                 scope=\'Conv2d_1a_3x3\')\n        with tf.variable_scope(\'Branch_2\'):\n          branch_2 = slim.max_pool2d(net, [3, 3], stride=2,\n                                     scope=\'MaxPool_1a_3x3\')\n        net = tf.concat(axis=3, values=[branch_0, branch_1, branch_2])\n        end_points[end_point] = net\n        if end_point == final_endpoint: return net, end_points\n      # 7 x 7 x 1024\n      end_point = \'Mixed_5b\'\n      with tf.variable_scope(end_point):\n        with tf.variable_scope(\'Branch_0\'):\n          branch_0 = slim.conv2d(net, depth(352), [1, 1], scope=\'Conv2d_0a_1x1\')\n        with tf.variable_scope(\'Branch_1\'):\n          branch_1 = slim.conv2d(\n              net, depth(192), [1, 1],\n              weights_initializer=trunc_normal(0.09),\n              scope=\'Conv2d_0a_1x1\')\n          branch_1 = slim.conv2d(branch_1, depth(320), [3, 3],\n                                 scope=\'Conv2d_0b_3x3\')\n        with tf.variable_scope(\'Branch_2\'):\n          branch_2 = slim.conv2d(\n              net, depth(160), [1, 1],\n              weights_initializer=trunc_normal(0.09),\n              scope=\'Conv2d_0a_1x1\')\n          branch_2 = slim.conv2d(branch_2, depth(224), [3, 3],\n                                 scope=\'Conv2d_0b_3x3\')\n          branch_2 = slim.conv2d(branch_2, depth(224), [3, 3],\n                                 scope=\'Conv2d_0c_3x3\')\n        with tf.variable_scope(\'Branch_3\'):\n          branch_3 = slim.avg_pool2d(net, [3, 3], scope=\'AvgPool_0a_3x3\')\n          branch_3 = slim.conv2d(\n              branch_3, depth(128), [1, 1],\n              weights_initializer=trunc_normal(0.1),\n              scope=\'Conv2d_0b_1x1\')\n        net = tf.concat(axis=3, values=[branch_0, branch_1, branch_2, branch_3])\n        end_points[end_point] = net\n        if end_point == final_endpoint: return net, end_points\n\n      # 7 x 7 x 1024\n      end_point = \'Mixed_5c\'\n      with tf.variable_scope(end_point):\n        with tf.variable_scope(\'Branch_0\'):\n          branch_0 = slim.conv2d(net, depth(352), [1, 1], scope=\'Conv2d_0a_1x1\')\n        with tf.variable_scope(\'Branch_1\'):\n          branch_1 = slim.conv2d(\n              net, depth(192), [1, 1],\n              weights_initializer=trunc_normal(0.09),\n              scope=\'Conv2d_0a_1x1\')\n          branch_1 = slim.conv2d(branch_1, depth(320), [3, 3],\n                                 scope=\'Conv2d_0b_3x3\')\n        with tf.variable_scope(\'Branch_2\'):\n          branch_2 = slim.conv2d(\n              net, depth(192), [1, 1],\n              weights_initializer=trunc_normal(0.09),\n              scope=\'Conv2d_0a_1x1\')\n          branch_2 = slim.conv2d(branch_2, depth(224), [3, 3],\n                                 scope=\'Conv2d_0b_3x3\')\n          branch_2 = slim.conv2d(branch_2, depth(224), [3, 3],\n                                 scope=\'Conv2d_0c_3x3\')\n        with tf.variable_scope(\'Branch_3\'):\n          branch_3 = slim.max_pool2d(net, [3, 3], scope=\'MaxPool_0a_3x3\')\n          branch_3 = slim.conv2d(\n              branch_3, depth(128), [1, 1],\n              weights_initializer=trunc_normal(0.1),\n              scope=\'Conv2d_0b_1x1\')\n        net = tf.concat(axis=3, values=[branch_0, branch_1, branch_2, branch_3])\n        end_points[end_point] = net\n        if end_point == final_endpoint: return net, end_points\n    raise ValueError(\'Unknown final endpoint %s\' % final_endpoint)\n\n\ndef inception_v2(inputs,\n                 num_classes=1000,\n                 is_training=True,\n                 dropout_keep_prob=0.8,\n                 min_depth=16,\n                 depth_multiplier=1.0,\n                 prediction_fn=slim.softmax,\n                 spatial_squeeze=True,\n                 reuse=None,\n                 scope=\'InceptionV2\'):\n  """"""Inception v2 model for classification.\n\n  Constructs an Inception v2 network for classification as described in\n  http://arxiv.org/abs/1502.03167.\n\n  The default image size used to train this network is 224x224.\n\n  Args:\n    inputs: a tensor of shape [batch_size, height, width, channels].\n    num_classes: number of predicted classes.\n    is_training: whether is training or not.\n    dropout_keep_prob: the percentage of activation values that are retained.\n    min_depth: Minimum depth value (number of channels) for all convolution ops.\n      Enforced when depth_multiplier < 1, and not an active constraint when\n      depth_multiplier >= 1.\n    depth_multiplier: Float multiplier for the depth (number of channels)\n      for all convolution ops. The value must be greater than zero. Typical\n      usage will be to set this value in (0, 1) to reduce the number of\n      parameters or computation cost of the model.\n    prediction_fn: a function to get predictions out of logits.\n    spatial_squeeze: if True, logits is of shape is [B, C], if false logits is\n        of shape [B, 1, 1, C], where B is batch_size and C is number of classes.\n    reuse: whether or not the network and its variables should be reused. To be\n      able to reuse \'scope\' must be given.\n    scope: Optional variable_scope.\n\n  Returns:\n    logits: the pre-softmax activations, a tensor of size\n      [batch_size, num_classes]\n    end_points: a dictionary from components of the network to the corresponding\n      activation.\n\n  Raises:\n    ValueError: if final_endpoint is not set to one of the predefined values,\n                or depth_multiplier <= 0\n  """"""\n  if depth_multiplier <= 0:\n    raise ValueError(\'depth_multiplier is not greater than zero.\')\n\n  # Final pooling and prediction\n  with tf.variable_scope(scope, \'InceptionV2\', [inputs, num_classes],\n                         reuse=reuse) as scope:\n    with slim.arg_scope([slim.batch_norm, slim.dropout],\n                        is_training=is_training):\n      net, end_points = inception_v2_base(\n          inputs, scope=scope, min_depth=min_depth,\n          depth_multiplier=depth_multiplier)\n      with tf.variable_scope(\'Logits\'):\n        kernel_size = _reduced_kernel_size_for_small_input(net, [7, 7])\n        net = slim.avg_pool2d(net, kernel_size, padding=\'VALID\',\n                              scope=\'AvgPool_1a_{}x{}\'.format(*kernel_size))\n        # 1 x 1 x 1024\n        net = slim.dropout(net, keep_prob=dropout_keep_prob, scope=\'Dropout_1b\')\n        logits = slim.conv2d(net, num_classes, [1, 1], activation_fn=None,\n                             normalizer_fn=None, scope=\'Conv2d_1c_1x1\')\n        if spatial_squeeze:\n          logits = tf.squeeze(logits, [1, 2], name=\'SpatialSqueeze\')\n      end_points[\'Logits\'] = logits\n      end_points[\'Predictions\'] = prediction_fn(logits, scope=\'Predictions\')\n  return logits, end_points\ninception_v2.default_image_size = 224\n\n\ndef _reduced_kernel_size_for_small_input(input_tensor, kernel_size):\n  """"""Define kernel size which is automatically reduced for small input.\n\n  If the shape of the input images is unknown at graph construction time this\n  function assumes that the input images are is large enough.\n\n  Args:\n    input_tensor: input tensor of size [batch_size, height, width, channels].\n    kernel_size: desired kernel size of length 2: [kernel_height, kernel_width]\n\n  Returns:\n    a tensor with the kernel size.\n\n  TODO(jrru): Make this function work with unknown shapes. Theoretically, this\n  can be done with the code below. Problems are two-fold: (1) If the shape was\n  known, it will be lost. (2) inception.slim.ops._two_element_tuple cannot\n  handle tensors that define the kernel size.\n      shape = tf.shape(input_tensor)\n      return = tf.pack([tf.minimum(shape[1], kernel_size[0]),\n                        tf.minimum(shape[2], kernel_size[1])])\n\n  """"""\n  shape = input_tensor.get_shape().as_list()\n  if shape[1] is None or shape[2] is None:\n    kernel_size_out = kernel_size\n  else:\n    kernel_size_out = [min(shape[1], kernel_size[0]),\n                       min(shape[2], kernel_size[1])]\n  return kernel_size_out\n\n\ninception_v2_arg_scope = inception_utils.inception_arg_scope\n'"
finetuning/nets/inception_v2_test.py,28,"b'# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Tests for nets.inception_v2.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport numpy as np\nimport tensorflow as tf\n\nfrom nets import inception\n\nslim = tf.contrib.slim\n\n\nclass InceptionV2Test(tf.test.TestCase):\n\n  def testBuildClassificationNetwork(self):\n    batch_size = 5\n    height, width = 224, 224\n    num_classes = 1000\n\n    inputs = tf.random_uniform((batch_size, height, width, 3))\n    logits, end_points = inception.inception_v2(inputs, num_classes)\n    self.assertTrue(logits.op.name.startswith(\'InceptionV2/Logits\'))\n    self.assertListEqual(logits.get_shape().as_list(),\n                         [batch_size, num_classes])\n    self.assertTrue(\'Predictions\' in end_points)\n    self.assertListEqual(end_points[\'Predictions\'].get_shape().as_list(),\n                         [batch_size, num_classes])\n\n  def testBuildBaseNetwork(self):\n    batch_size = 5\n    height, width = 224, 224\n\n    inputs = tf.random_uniform((batch_size, height, width, 3))\n    mixed_5c, end_points = inception.inception_v2_base(inputs)\n    self.assertTrue(mixed_5c.op.name.startswith(\'InceptionV2/Mixed_5c\'))\n    self.assertListEqual(mixed_5c.get_shape().as_list(),\n                         [batch_size, 7, 7, 1024])\n    expected_endpoints = [\'Mixed_3b\', \'Mixed_3c\', \'Mixed_4a\', \'Mixed_4b\',\n                          \'Mixed_4c\', \'Mixed_4d\', \'Mixed_4e\', \'Mixed_5a\',\n                          \'Mixed_5b\', \'Mixed_5c\', \'Conv2d_1a_7x7\',\n                          \'MaxPool_2a_3x3\', \'Conv2d_2b_1x1\', \'Conv2d_2c_3x3\',\n                          \'MaxPool_3a_3x3\']\n    self.assertItemsEqual(end_points.keys(), expected_endpoints)\n\n  def testBuildOnlyUptoFinalEndpoint(self):\n    batch_size = 5\n    height, width = 224, 224\n    endpoints = [\'Conv2d_1a_7x7\', \'MaxPool_2a_3x3\', \'Conv2d_2b_1x1\',\n                 \'Conv2d_2c_3x3\', \'MaxPool_3a_3x3\', \'Mixed_3b\', \'Mixed_3c\',\n                 \'Mixed_4a\', \'Mixed_4b\', \'Mixed_4c\', \'Mixed_4d\', \'Mixed_4e\',\n                 \'Mixed_5a\', \'Mixed_5b\', \'Mixed_5c\']\n    for index, endpoint in enumerate(endpoints):\n      with tf.Graph().as_default():\n        inputs = tf.random_uniform((batch_size, height, width, 3))\n        out_tensor, end_points = inception.inception_v2_base(\n            inputs, final_endpoint=endpoint)\n        self.assertTrue(out_tensor.op.name.startswith(\n            \'InceptionV2/\' + endpoint))\n        self.assertItemsEqual(endpoints[:index+1], end_points)\n\n  def testBuildAndCheckAllEndPointsUptoMixed5c(self):\n    batch_size = 5\n    height, width = 224, 224\n\n    inputs = tf.random_uniform((batch_size, height, width, 3))\n    _, end_points = inception.inception_v2_base(inputs,\n                                                final_endpoint=\'Mixed_5c\')\n    endpoints_shapes = {\'Mixed_3b\': [batch_size, 28, 28, 256],\n                        \'Mixed_3c\': [batch_size, 28, 28, 320],\n                        \'Mixed_4a\': [batch_size, 14, 14, 576],\n                        \'Mixed_4b\': [batch_size, 14, 14, 576],\n                        \'Mixed_4c\': [batch_size, 14, 14, 576],\n                        \'Mixed_4d\': [batch_size, 14, 14, 576],\n                        \'Mixed_4e\': [batch_size, 14, 14, 576],\n                        \'Mixed_5a\': [batch_size, 7, 7, 1024],\n                        \'Mixed_5b\': [batch_size, 7, 7, 1024],\n                        \'Mixed_5c\': [batch_size, 7, 7, 1024],\n                        \'Conv2d_1a_7x7\': [batch_size, 112, 112, 64],\n                        \'MaxPool_2a_3x3\': [batch_size, 56, 56, 64],\n                        \'Conv2d_2b_1x1\': [batch_size, 56, 56, 64],\n                        \'Conv2d_2c_3x3\': [batch_size, 56, 56, 192],\n                        \'MaxPool_3a_3x3\': [batch_size, 28, 28, 192]}\n    self.assertItemsEqual(endpoints_shapes.keys(), end_points.keys())\n    for endpoint_name in endpoints_shapes:\n      expected_shape = endpoints_shapes[endpoint_name]\n      self.assertTrue(endpoint_name in end_points)\n      self.assertListEqual(end_points[endpoint_name].get_shape().as_list(),\n                           expected_shape)\n\n  def testModelHasExpectedNumberOfParameters(self):\n    batch_size = 5\n    height, width = 224, 224\n    inputs = tf.random_uniform((batch_size, height, width, 3))\n    with slim.arg_scope(inception.inception_v2_arg_scope()):\n      inception.inception_v2_base(inputs)\n    total_params, _ = slim.model_analyzer.analyze_vars(\n        slim.get_model_variables())\n    self.assertAlmostEqual(10173112, total_params)\n\n  def testBuildEndPointsWithDepthMultiplierLessThanOne(self):\n    batch_size = 5\n    height, width = 224, 224\n    num_classes = 1000\n\n    inputs = tf.random_uniform((batch_size, height, width, 3))\n    _, end_points = inception.inception_v2(inputs, num_classes)\n\n    endpoint_keys = [key for key in end_points.keys()\n                     if key.startswith(\'Mixed\') or key.startswith(\'Conv\')]\n\n    _, end_points_with_multiplier = inception.inception_v2(\n        inputs, num_classes, scope=\'depth_multiplied_net\',\n        depth_multiplier=0.5)\n\n    for key in endpoint_keys:\n      original_depth = end_points[key].get_shape().as_list()[3]\n      new_depth = end_points_with_multiplier[key].get_shape().as_list()[3]\n      self.assertEqual(0.5 * original_depth, new_depth)\n\n  def testBuildEndPointsWithDepthMultiplierGreaterThanOne(self):\n    batch_size = 5\n    height, width = 224, 224\n    num_classes = 1000\n\n    inputs = tf.random_uniform((batch_size, height, width, 3))\n    _, end_points = inception.inception_v2(inputs, num_classes)\n\n    endpoint_keys = [key for key in end_points.keys()\n                     if key.startswith(\'Mixed\') or key.startswith(\'Conv\')]\n\n    _, end_points_with_multiplier = inception.inception_v2(\n        inputs, num_classes, scope=\'depth_multiplied_net\',\n        depth_multiplier=2.0)\n\n    for key in endpoint_keys:\n      original_depth = end_points[key].get_shape().as_list()[3]\n      new_depth = end_points_with_multiplier[key].get_shape().as_list()[3]\n      self.assertEqual(2.0 * original_depth, new_depth)\n\n  def testRaiseValueErrorWithInvalidDepthMultiplier(self):\n    batch_size = 5\n    height, width = 224, 224\n    num_classes = 1000\n\n    inputs = tf.random_uniform((batch_size, height, width, 3))\n    with self.assertRaises(ValueError):\n      _ = inception.inception_v2(inputs, num_classes, depth_multiplier=-0.1)\n    with self.assertRaises(ValueError):\n      _ = inception.inception_v2(inputs, num_classes, depth_multiplier=0.0)\n\n  def testHalfSizeImages(self):\n    batch_size = 5\n    height, width = 112, 112\n    num_classes = 1000\n\n    inputs = tf.random_uniform((batch_size, height, width, 3))\n    logits, end_points = inception.inception_v2(inputs, num_classes)\n    self.assertTrue(logits.op.name.startswith(\'InceptionV2/Logits\'))\n    self.assertListEqual(logits.get_shape().as_list(),\n                         [batch_size, num_classes])\n    pre_pool = end_points[\'Mixed_5c\']\n    self.assertListEqual(pre_pool.get_shape().as_list(),\n                         [batch_size, 4, 4, 1024])\n\n  def testUnknownImageShape(self):\n    tf.reset_default_graph()\n    batch_size = 2\n    height, width = 224, 224\n    num_classes = 1000\n    input_np = np.random.uniform(0, 1, (batch_size, height, width, 3))\n    with self.test_session() as sess:\n      inputs = tf.placeholder(tf.float32, shape=(batch_size, None, None, 3))\n      logits, end_points = inception.inception_v2(inputs, num_classes)\n      self.assertTrue(logits.op.name.startswith(\'InceptionV2/Logits\'))\n      self.assertListEqual(logits.get_shape().as_list(),\n                           [batch_size, num_classes])\n      pre_pool = end_points[\'Mixed_5c\']\n      feed_dict = {inputs: input_np}\n      tf.global_variables_initializer().run()\n      pre_pool_out = sess.run(pre_pool, feed_dict=feed_dict)\n      self.assertListEqual(list(pre_pool_out.shape), [batch_size, 7, 7, 1024])\n\n  def testUnknowBatchSize(self):\n    batch_size = 1\n    height, width = 224, 224\n    num_classes = 1000\n\n    inputs = tf.placeholder(tf.float32, (None, height, width, 3))\n    logits, _ = inception.inception_v2(inputs, num_classes)\n    self.assertTrue(logits.op.name.startswith(\'InceptionV2/Logits\'))\n    self.assertListEqual(logits.get_shape().as_list(),\n                         [None, num_classes])\n    images = tf.random_uniform((batch_size, height, width, 3))\n\n    with self.test_session() as sess:\n      sess.run(tf.global_variables_initializer())\n      output = sess.run(logits, {inputs: images.eval()})\n      self.assertEquals(output.shape, (batch_size, num_classes))\n\n  def testEvaluation(self):\n    batch_size = 2\n    height, width = 224, 224\n    num_classes = 1000\n\n    eval_inputs = tf.random_uniform((batch_size, height, width, 3))\n    logits, _ = inception.inception_v2(eval_inputs, num_classes,\n                                       is_training=False)\n    predictions = tf.argmax(logits, 1)\n\n    with self.test_session() as sess:\n      sess.run(tf.global_variables_initializer())\n      output = sess.run(predictions)\n      self.assertEquals(output.shape, (batch_size,))\n\n  def testTrainEvalWithReuse(self):\n    train_batch_size = 5\n    eval_batch_size = 2\n    height, width = 150, 150\n    num_classes = 1000\n\n    train_inputs = tf.random_uniform((train_batch_size, height, width, 3))\n    inception.inception_v2(train_inputs, num_classes)\n    eval_inputs = tf.random_uniform((eval_batch_size, height, width, 3))\n    logits, _ = inception.inception_v2(eval_inputs, num_classes, reuse=True)\n    predictions = tf.argmax(logits, 1)\n\n    with self.test_session() as sess:\n      sess.run(tf.global_variables_initializer())\n      output = sess.run(predictions)\n      self.assertEquals(output.shape, (eval_batch_size,))\n\n  def testLogitsNotSqueezed(self):\n    num_classes = 25\n    images = tf.random_uniform([1, 224, 224, 3])\n    logits, _ = inception.inception_v2(images,\n                                       num_classes=num_classes,\n                                       spatial_squeeze=False)\n\n    with self.test_session() as sess:\n      tf.global_variables_initializer().run()\n      logits_out = sess.run(logits)\n      self.assertListEqual(list(logits_out.shape), [1, 1, 1, num_classes])\n\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
finetuning/nets/inception_v3.py,79,"b'# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Contains the definition for inception v3 classification network.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\n\nfrom nets import inception_utils\n\nslim = tf.contrib.slim\ntrunc_normal = lambda stddev: tf.truncated_normal_initializer(0.0, stddev)\n\n\ndef inception_v3_base(inputs,\n                      final_endpoint=\'Mixed_7c\',\n                      min_depth=16,\n                      depth_multiplier=1.0,\n                      scope=None):\n  """"""Inception model from http://arxiv.org/abs/1512.00567.\n\n  Constructs an Inception v3 network from inputs to the given final endpoint.\n  This method can construct the network up to the final inception block\n  Mixed_7c.\n\n  Note that the names of the layers in the paper do not correspond to the names\n  of the endpoints registered by this function although they build the same\n  network.\n\n  Here is a mapping from the old_names to the new names:\n  Old name          | New name\n  =======================================\n  conv0             | Conv2d_1a_3x3\n  conv1             | Conv2d_2a_3x3\n  conv2             | Conv2d_2b_3x3\n  pool1             | MaxPool_3a_3x3\n  conv3             | Conv2d_3b_1x1\n  conv4             | Conv2d_4a_3x3\n  pool2             | MaxPool_5a_3x3\n  mixed_35x35x256a  | Mixed_5b\n  mixed_35x35x288a  | Mixed_5c\n  mixed_35x35x288b  | Mixed_5d\n  mixed_17x17x768a  | Mixed_6a\n  mixed_17x17x768b  | Mixed_6b\n  mixed_17x17x768c  | Mixed_6c\n  mixed_17x17x768d  | Mixed_6d\n  mixed_17x17x768e  | Mixed_6e\n  mixed_8x8x1280a   | Mixed_7a\n  mixed_8x8x2048a   | Mixed_7b\n  mixed_8x8x2048b   | Mixed_7c\n\n  Args:\n    inputs: a tensor of size [batch_size, height, width, channels].\n    final_endpoint: specifies the endpoint to construct the network up to. It\n      can be one of [\'Conv2d_1a_3x3\', \'Conv2d_2a_3x3\', \'Conv2d_2b_3x3\',\n      \'MaxPool_3a_3x3\', \'Conv2d_3b_1x1\', \'Conv2d_4a_3x3\', \'MaxPool_5a_3x3\',\n      \'Mixed_5b\', \'Mixed_5c\', \'Mixed_5d\', \'Mixed_6a\', \'Mixed_6b\', \'Mixed_6c\',\n      \'Mixed_6d\', \'Mixed_6e\', \'Mixed_7a\', \'Mixed_7b\', \'Mixed_7c\'].\n    min_depth: Minimum depth value (number of channels) for all convolution ops.\n      Enforced when depth_multiplier < 1, and not an active constraint when\n      depth_multiplier >= 1.\n    depth_multiplier: Float multiplier for the depth (number of channels)\n      for all convolution ops. The value must be greater than zero. Typical\n      usage will be to set this value in (0, 1) to reduce the number of\n      parameters or computation cost of the model.\n    scope: Optional variable_scope.\n\n  Returns:\n    tensor_out: output tensor corresponding to the final_endpoint.\n    end_points: a set of activations for external use, for example summaries or\n                losses.\n\n  Raises:\n    ValueError: if final_endpoint is not set to one of the predefined values,\n                or depth_multiplier <= 0\n  """"""\n  # end_points will collect relevant activations for external use, for example\n  # summaries or losses.\n  end_points = {}\n\n  if depth_multiplier <= 0:\n    raise ValueError(\'depth_multiplier is not greater than zero.\')\n  depth = lambda d: max(int(d * depth_multiplier), min_depth)\n\n  with tf.variable_scope(scope, \'InceptionV3\', [inputs]):\n    with slim.arg_scope([slim.conv2d, slim.max_pool2d, slim.avg_pool2d],\n                        stride=1, padding=\'VALID\'):\n      # 299 x 299 x 3\n      end_point = \'Conv2d_1a_3x3\'\n      net = slim.conv2d(inputs, depth(32), [3, 3], stride=2, scope=end_point)\n      end_points[end_point] = net\n      if end_point == final_endpoint: return net, end_points\n      # 149 x 149 x 32\n      end_point = \'Conv2d_2a_3x3\'\n      net = slim.conv2d(net, depth(32), [3, 3], scope=end_point)\n      end_points[end_point] = net\n      if end_point == final_endpoint: return net, end_points\n      # 147 x 147 x 32\n      end_point = \'Conv2d_2b_3x3\'\n      net = slim.conv2d(net, depth(64), [3, 3], padding=\'SAME\', scope=end_point)\n      end_points[end_point] = net\n      if end_point == final_endpoint: return net, end_points\n      # 147 x 147 x 64\n      end_point = \'MaxPool_3a_3x3\'\n      net = slim.max_pool2d(net, [3, 3], stride=2, scope=end_point)\n      end_points[end_point] = net\n      if end_point == final_endpoint: return net, end_points\n      # 73 x 73 x 64\n      end_point = \'Conv2d_3b_1x1\'\n      net = slim.conv2d(net, depth(80), [1, 1], scope=end_point)\n      end_points[end_point] = net\n      if end_point == final_endpoint: return net, end_points\n      # 73 x 73 x 80.\n      end_point = \'Conv2d_4a_3x3\'\n      net = slim.conv2d(net, depth(192), [3, 3], scope=end_point)\n      end_points[end_point] = net\n      if end_point == final_endpoint: return net, end_points\n      # 71 x 71 x 192.\n      end_point = \'MaxPool_5a_3x3\'\n      net = slim.max_pool2d(net, [3, 3], stride=2, scope=end_point)\n      end_points[end_point] = net\n      if end_point == final_endpoint: return net, end_points\n      # 35 x 35 x 192.\n\n    # Inception blocks\n    with slim.arg_scope([slim.conv2d, slim.max_pool2d, slim.avg_pool2d],\n                        stride=1, padding=\'SAME\'):\n      # mixed: 35 x 35 x 256.\n      end_point = \'Mixed_5b\'\n      with tf.variable_scope(end_point):\n        with tf.variable_scope(\'Branch_0\'):\n          branch_0 = slim.conv2d(net, depth(64), [1, 1], scope=\'Conv2d_0a_1x1\')\n        with tf.variable_scope(\'Branch_1\'):\n          branch_1 = slim.conv2d(net, depth(48), [1, 1], scope=\'Conv2d_0a_1x1\')\n          branch_1 = slim.conv2d(branch_1, depth(64), [5, 5],\n                                 scope=\'Conv2d_0b_5x5\')\n        with tf.variable_scope(\'Branch_2\'):\n          branch_2 = slim.conv2d(net, depth(64), [1, 1], scope=\'Conv2d_0a_1x1\')\n          branch_2 = slim.conv2d(branch_2, depth(96), [3, 3],\n                                 scope=\'Conv2d_0b_3x3\')\n          branch_2 = slim.conv2d(branch_2, depth(96), [3, 3],\n                                 scope=\'Conv2d_0c_3x3\')\n        with tf.variable_scope(\'Branch_3\'):\n          branch_3 = slim.avg_pool2d(net, [3, 3], scope=\'AvgPool_0a_3x3\')\n          branch_3 = slim.conv2d(branch_3, depth(32), [1, 1],\n                                 scope=\'Conv2d_0b_1x1\')\n        net = tf.concat(axis=3, values=[branch_0, branch_1, branch_2, branch_3])\n      end_points[end_point] = net\n      if end_point == final_endpoint: return net, end_points\n\n      # mixed_1: 35 x 35 x 288.\n      end_point = \'Mixed_5c\'\n      with tf.variable_scope(end_point):\n        with tf.variable_scope(\'Branch_0\'):\n          branch_0 = slim.conv2d(net, depth(64), [1, 1], scope=\'Conv2d_0a_1x1\')\n        with tf.variable_scope(\'Branch_1\'):\n          branch_1 = slim.conv2d(net, depth(48), [1, 1], scope=\'Conv2d_0b_1x1\')\n          branch_1 = slim.conv2d(branch_1, depth(64), [5, 5],\n                                 scope=\'Conv_1_0c_5x5\')\n        with tf.variable_scope(\'Branch_2\'):\n          branch_2 = slim.conv2d(net, depth(64), [1, 1],\n                                 scope=\'Conv2d_0a_1x1\')\n          branch_2 = slim.conv2d(branch_2, depth(96), [3, 3],\n                                 scope=\'Conv2d_0b_3x3\')\n          branch_2 = slim.conv2d(branch_2, depth(96), [3, 3],\n                                 scope=\'Conv2d_0c_3x3\')\n        with tf.variable_scope(\'Branch_3\'):\n          branch_3 = slim.avg_pool2d(net, [3, 3], scope=\'AvgPool_0a_3x3\')\n          branch_3 = slim.conv2d(branch_3, depth(64), [1, 1],\n                                 scope=\'Conv2d_0b_1x1\')\n        net = tf.concat(axis=3, values=[branch_0, branch_1, branch_2, branch_3])\n      end_points[end_point] = net\n      if end_point == final_endpoint: return net, end_points\n\n      # mixed_2: 35 x 35 x 288.\n      end_point = \'Mixed_5d\'\n      with tf.variable_scope(end_point):\n        with tf.variable_scope(\'Branch_0\'):\n          branch_0 = slim.conv2d(net, depth(64), [1, 1], scope=\'Conv2d_0a_1x1\')\n        with tf.variable_scope(\'Branch_1\'):\n          branch_1 = slim.conv2d(net, depth(48), [1, 1], scope=\'Conv2d_0a_1x1\')\n          branch_1 = slim.conv2d(branch_1, depth(64), [5, 5],\n                                 scope=\'Conv2d_0b_5x5\')\n        with tf.variable_scope(\'Branch_2\'):\n          branch_2 = slim.conv2d(net, depth(64), [1, 1], scope=\'Conv2d_0a_1x1\')\n          branch_2 = slim.conv2d(branch_2, depth(96), [3, 3],\n                                 scope=\'Conv2d_0b_3x3\')\n          branch_2 = slim.conv2d(branch_2, depth(96), [3, 3],\n                                 scope=\'Conv2d_0c_3x3\')\n        with tf.variable_scope(\'Branch_3\'):\n          branch_3 = slim.avg_pool2d(net, [3, 3], scope=\'AvgPool_0a_3x3\')\n          branch_3 = slim.conv2d(branch_3, depth(64), [1, 1],\n                                 scope=\'Conv2d_0b_1x1\')\n        net = tf.concat(axis=3, values=[branch_0, branch_1, branch_2, branch_3])\n      end_points[end_point] = net\n      if end_point == final_endpoint: return net, end_points\n\n      # mixed_3: 17 x 17 x 768.\n      end_point = \'Mixed_6a\'\n      with tf.variable_scope(end_point):\n        with tf.variable_scope(\'Branch_0\'):\n          branch_0 = slim.conv2d(net, depth(384), [3, 3], stride=2,\n                                 padding=\'VALID\', scope=\'Conv2d_1a_1x1\')\n        with tf.variable_scope(\'Branch_1\'):\n          branch_1 = slim.conv2d(net, depth(64), [1, 1], scope=\'Conv2d_0a_1x1\')\n          branch_1 = slim.conv2d(branch_1, depth(96), [3, 3],\n                                 scope=\'Conv2d_0b_3x3\')\n          branch_1 = slim.conv2d(branch_1, depth(96), [3, 3], stride=2,\n                                 padding=\'VALID\', scope=\'Conv2d_1a_1x1\')\n        with tf.variable_scope(\'Branch_2\'):\n          branch_2 = slim.max_pool2d(net, [3, 3], stride=2, padding=\'VALID\',\n                                     scope=\'MaxPool_1a_3x3\')\n        net = tf.concat(axis=3, values=[branch_0, branch_1, branch_2])\n      end_points[end_point] = net\n      if end_point == final_endpoint: return net, end_points\n\n      # mixed4: 17 x 17 x 768.\n      end_point = \'Mixed_6b\'\n      with tf.variable_scope(end_point):\n        with tf.variable_scope(\'Branch_0\'):\n          branch_0 = slim.conv2d(net, depth(192), [1, 1], scope=\'Conv2d_0a_1x1\')\n        with tf.variable_scope(\'Branch_1\'):\n          branch_1 = slim.conv2d(net, depth(128), [1, 1], scope=\'Conv2d_0a_1x1\')\n          branch_1 = slim.conv2d(branch_1, depth(128), [1, 7],\n                                 scope=\'Conv2d_0b_1x7\')\n          branch_1 = slim.conv2d(branch_1, depth(192), [7, 1],\n                                 scope=\'Conv2d_0c_7x1\')\n        with tf.variable_scope(\'Branch_2\'):\n          branch_2 = slim.conv2d(net, depth(128), [1, 1], scope=\'Conv2d_0a_1x1\')\n          branch_2 = slim.conv2d(branch_2, depth(128), [7, 1],\n                                 scope=\'Conv2d_0b_7x1\')\n          branch_2 = slim.conv2d(branch_2, depth(128), [1, 7],\n                                 scope=\'Conv2d_0c_1x7\')\n          branch_2 = slim.conv2d(branch_2, depth(128), [7, 1],\n                                 scope=\'Conv2d_0d_7x1\')\n          branch_2 = slim.conv2d(branch_2, depth(192), [1, 7],\n                                 scope=\'Conv2d_0e_1x7\')\n        with tf.variable_scope(\'Branch_3\'):\n          branch_3 = slim.avg_pool2d(net, [3, 3], scope=\'AvgPool_0a_3x3\')\n          branch_3 = slim.conv2d(branch_3, depth(192), [1, 1],\n                                 scope=\'Conv2d_0b_1x1\')\n        net = tf.concat(axis=3, values=[branch_0, branch_1, branch_2, branch_3])\n      end_points[end_point] = net\n      if end_point == final_endpoint: return net, end_points\n\n      # mixed_5: 17 x 17 x 768.\n      end_point = \'Mixed_6c\'\n      with tf.variable_scope(end_point):\n        with tf.variable_scope(\'Branch_0\'):\n          branch_0 = slim.conv2d(net, depth(192), [1, 1], scope=\'Conv2d_0a_1x1\')\n        with tf.variable_scope(\'Branch_1\'):\n          branch_1 = slim.conv2d(net, depth(160), [1, 1], scope=\'Conv2d_0a_1x1\')\n          branch_1 = slim.conv2d(branch_1, depth(160), [1, 7],\n                                 scope=\'Conv2d_0b_1x7\')\n          branch_1 = slim.conv2d(branch_1, depth(192), [7, 1],\n                                 scope=\'Conv2d_0c_7x1\')\n        with tf.variable_scope(\'Branch_2\'):\n          branch_2 = slim.conv2d(net, depth(160), [1, 1], scope=\'Conv2d_0a_1x1\')\n          branch_2 = slim.conv2d(branch_2, depth(160), [7, 1],\n                                 scope=\'Conv2d_0b_7x1\')\n          branch_2 = slim.conv2d(branch_2, depth(160), [1, 7],\n                                 scope=\'Conv2d_0c_1x7\')\n          branch_2 = slim.conv2d(branch_2, depth(160), [7, 1],\n                                 scope=\'Conv2d_0d_7x1\')\n          branch_2 = slim.conv2d(branch_2, depth(192), [1, 7],\n                                 scope=\'Conv2d_0e_1x7\')\n        with tf.variable_scope(\'Branch_3\'):\n          branch_3 = slim.avg_pool2d(net, [3, 3], scope=\'AvgPool_0a_3x3\')\n          branch_3 = slim.conv2d(branch_3, depth(192), [1, 1],\n                                 scope=\'Conv2d_0b_1x1\')\n        net = tf.concat(axis=3, values=[branch_0, branch_1, branch_2, branch_3])\n      end_points[end_point] = net\n      if end_point == final_endpoint: return net, end_points\n      # mixed_6: 17 x 17 x 768.\n      end_point = \'Mixed_6d\'\n      with tf.variable_scope(end_point):\n        with tf.variable_scope(\'Branch_0\'):\n          branch_0 = slim.conv2d(net, depth(192), [1, 1], scope=\'Conv2d_0a_1x1\')\n        with tf.variable_scope(\'Branch_1\'):\n          branch_1 = slim.conv2d(net, depth(160), [1, 1], scope=\'Conv2d_0a_1x1\')\n          branch_1 = slim.conv2d(branch_1, depth(160), [1, 7],\n                                 scope=\'Conv2d_0b_1x7\')\n          branch_1 = slim.conv2d(branch_1, depth(192), [7, 1],\n                                 scope=\'Conv2d_0c_7x1\')\n        with tf.variable_scope(\'Branch_2\'):\n          branch_2 = slim.conv2d(net, depth(160), [1, 1], scope=\'Conv2d_0a_1x1\')\n          branch_2 = slim.conv2d(branch_2, depth(160), [7, 1],\n                                 scope=\'Conv2d_0b_7x1\')\n          branch_2 = slim.conv2d(branch_2, depth(160), [1, 7],\n                                 scope=\'Conv2d_0c_1x7\')\n          branch_2 = slim.conv2d(branch_2, depth(160), [7, 1],\n                                 scope=\'Conv2d_0d_7x1\')\n          branch_2 = slim.conv2d(branch_2, depth(192), [1, 7],\n                                 scope=\'Conv2d_0e_1x7\')\n        with tf.variable_scope(\'Branch_3\'):\n          branch_3 = slim.avg_pool2d(net, [3, 3], scope=\'AvgPool_0a_3x3\')\n          branch_3 = slim.conv2d(branch_3, depth(192), [1, 1],\n                                 scope=\'Conv2d_0b_1x1\')\n        net = tf.concat(axis=3, values=[branch_0, branch_1, branch_2, branch_3])\n      end_points[end_point] = net\n      if end_point == final_endpoint: return net, end_points\n\n      # mixed_7: 17 x 17 x 768.\n      end_point = \'Mixed_6e\'\n      with tf.variable_scope(end_point):\n        with tf.variable_scope(\'Branch_0\'):\n          branch_0 = slim.conv2d(net, depth(192), [1, 1], scope=\'Conv2d_0a_1x1\')\n        with tf.variable_scope(\'Branch_1\'):\n          branch_1 = slim.conv2d(net, depth(192), [1, 1], scope=\'Conv2d_0a_1x1\')\n          branch_1 = slim.conv2d(branch_1, depth(192), [1, 7],\n                                 scope=\'Conv2d_0b_1x7\')\n          branch_1 = slim.conv2d(branch_1, depth(192), [7, 1],\n                                 scope=\'Conv2d_0c_7x1\')\n        with tf.variable_scope(\'Branch_2\'):\n          branch_2 = slim.conv2d(net, depth(192), [1, 1], scope=\'Conv2d_0a_1x1\')\n          branch_2 = slim.conv2d(branch_2, depth(192), [7, 1],\n                                 scope=\'Conv2d_0b_7x1\')\n          branch_2 = slim.conv2d(branch_2, depth(192), [1, 7],\n                                 scope=\'Conv2d_0c_1x7\')\n          branch_2 = slim.conv2d(branch_2, depth(192), [7, 1],\n                                 scope=\'Conv2d_0d_7x1\')\n          branch_2 = slim.conv2d(branch_2, depth(192), [1, 7],\n                                 scope=\'Conv2d_0e_1x7\')\n        with tf.variable_scope(\'Branch_3\'):\n          branch_3 = slim.avg_pool2d(net, [3, 3], scope=\'AvgPool_0a_3x3\')\n          branch_3 = slim.conv2d(branch_3, depth(192), [1, 1],\n                                 scope=\'Conv2d_0b_1x1\')\n        net = tf.concat(axis=3, values=[branch_0, branch_1, branch_2, branch_3])\n      end_points[end_point] = net\n      if end_point == final_endpoint: return net, end_points\n\n      # mixed_8: 8 x 8 x 1280.\n      end_point = \'Mixed_7a\'\n      with tf.variable_scope(end_point):\n        with tf.variable_scope(\'Branch_0\'):\n          branch_0 = slim.conv2d(net, depth(192), [1, 1], scope=\'Conv2d_0a_1x1\')\n          branch_0 = slim.conv2d(branch_0, depth(320), [3, 3], stride=2,\n                                 padding=\'VALID\', scope=\'Conv2d_1a_3x3\')\n        with tf.variable_scope(\'Branch_1\'):\n          branch_1 = slim.conv2d(net, depth(192), [1, 1], scope=\'Conv2d_0a_1x1\')\n          branch_1 = slim.conv2d(branch_1, depth(192), [1, 7],\n                                 scope=\'Conv2d_0b_1x7\')\n          branch_1 = slim.conv2d(branch_1, depth(192), [7, 1],\n                                 scope=\'Conv2d_0c_7x1\')\n          branch_1 = slim.conv2d(branch_1, depth(192), [3, 3], stride=2,\n                                 padding=\'VALID\', scope=\'Conv2d_1a_3x3\')\n        with tf.variable_scope(\'Branch_2\'):\n          branch_2 = slim.max_pool2d(net, [3, 3], stride=2, padding=\'VALID\',\n                                     scope=\'MaxPool_1a_3x3\')\n        net = tf.concat(axis=3, values=[branch_0, branch_1, branch_2])\n      end_points[end_point] = net\n      if end_point == final_endpoint: return net, end_points\n      # mixed_9: 8 x 8 x 2048.\n      end_point = \'Mixed_7b\'\n      with tf.variable_scope(end_point):\n        with tf.variable_scope(\'Branch_0\'):\n          branch_0 = slim.conv2d(net, depth(320), [1, 1], scope=\'Conv2d_0a_1x1\')\n        with tf.variable_scope(\'Branch_1\'):\n          branch_1 = slim.conv2d(net, depth(384), [1, 1], scope=\'Conv2d_0a_1x1\')\n          branch_1 = tf.concat(axis=3, values=[\n              slim.conv2d(branch_1, depth(384), [1, 3], scope=\'Conv2d_0b_1x3\'),\n              slim.conv2d(branch_1, depth(384), [3, 1], scope=\'Conv2d_0b_3x1\')])\n        with tf.variable_scope(\'Branch_2\'):\n          branch_2 = slim.conv2d(net, depth(448), [1, 1], scope=\'Conv2d_0a_1x1\')\n          branch_2 = slim.conv2d(\n              branch_2, depth(384), [3, 3], scope=\'Conv2d_0b_3x3\')\n          branch_2 = tf.concat(axis=3, values=[\n              slim.conv2d(branch_2, depth(384), [1, 3], scope=\'Conv2d_0c_1x3\'),\n              slim.conv2d(branch_2, depth(384), [3, 1], scope=\'Conv2d_0d_3x1\')])\n        with tf.variable_scope(\'Branch_3\'):\n          branch_3 = slim.avg_pool2d(net, [3, 3], scope=\'AvgPool_0a_3x3\')\n          branch_3 = slim.conv2d(\n              branch_3, depth(192), [1, 1], scope=\'Conv2d_0b_1x1\')\n        net = tf.concat(axis=3, values=[branch_0, branch_1, branch_2, branch_3])\n      end_points[end_point] = net\n      if end_point == final_endpoint: return net, end_points\n\n      # mixed_10: 8 x 8 x 2048.\n      end_point = \'Mixed_7c\'\n      with tf.variable_scope(end_point):\n        with tf.variable_scope(\'Branch_0\'):\n          branch_0 = slim.conv2d(net, depth(320), [1, 1], scope=\'Conv2d_0a_1x1\')\n        with tf.variable_scope(\'Branch_1\'):\n          branch_1 = slim.conv2d(net, depth(384), [1, 1], scope=\'Conv2d_0a_1x1\')\n          branch_1 = tf.concat(axis=3, values=[\n              slim.conv2d(branch_1, depth(384), [1, 3], scope=\'Conv2d_0b_1x3\'),\n              slim.conv2d(branch_1, depth(384), [3, 1], scope=\'Conv2d_0c_3x1\')])\n        with tf.variable_scope(\'Branch_2\'):\n          branch_2 = slim.conv2d(net, depth(448), [1, 1], scope=\'Conv2d_0a_1x1\')\n          branch_2 = slim.conv2d(\n              branch_2, depth(384), [3, 3], scope=\'Conv2d_0b_3x3\')\n          branch_2 = tf.concat(axis=3, values=[\n              slim.conv2d(branch_2, depth(384), [1, 3], scope=\'Conv2d_0c_1x3\'),\n              slim.conv2d(branch_2, depth(384), [3, 1], scope=\'Conv2d_0d_3x1\')])\n        with tf.variable_scope(\'Branch_3\'):\n          branch_3 = slim.avg_pool2d(net, [3, 3], scope=\'AvgPool_0a_3x3\')\n          branch_3 = slim.conv2d(\n              branch_3, depth(192), [1, 1], scope=\'Conv2d_0b_1x1\')\n        net = tf.concat(axis=3, values=[branch_0, branch_1, branch_2, branch_3])\n      end_points[end_point] = net\n      if end_point == final_endpoint: return net, end_points\n    raise ValueError(\'Unknown final endpoint %s\' % final_endpoint)\n\n\ndef inception_v3(inputs,\n                 num_classes=1000,\n                 is_training=True,\n                 dropout_keep_prob=0.8,\n                 min_depth=16,\n                 depth_multiplier=1.0,\n                 prediction_fn=slim.softmax,\n                 spatial_squeeze=True,\n                 reuse=None,\n                 scope=\'InceptionV3\'):\n  """"""Inception model from http://arxiv.org/abs/1512.00567.\n\n  ""Rethinking the Inception Architecture for Computer Vision""\n\n  Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens,\n  Zbigniew Wojna.\n\n  With the default arguments this method constructs the exact model defined in\n  the paper. However, one can experiment with variations of the inception_v3\n  network by changing arguments dropout_keep_prob, min_depth and\n  depth_multiplier.\n\n  The default image size used to train this network is 299x299.\n\n  Args:\n    inputs: a tensor of size [batch_size, height, width, channels].\n    num_classes: number of predicted classes.\n    is_training: whether is training or not.\n    dropout_keep_prob: the percentage of activation values that are retained.\n    min_depth: Minimum depth value (number of channels) for all convolution ops.\n      Enforced when depth_multiplier < 1, and not an active constraint when\n      depth_multiplier >= 1.\n    depth_multiplier: Float multiplier for the depth (number of channels)\n      for all convolution ops. The value must be greater than zero. Typical\n      usage will be to set this value in (0, 1) to reduce the number of\n      parameters or computation cost of the model.\n    prediction_fn: a function to get predictions out of logits.\n    spatial_squeeze: if True, logits is of shape is [B, C], if false logits is\n        of shape [B, 1, 1, C], where B is batch_size and C is number of classes.\n    reuse: whether or not the network and its variables should be reused. To be\n      able to reuse \'scope\' must be given.\n    scope: Optional variable_scope.\n\n  Returns:\n    logits: the pre-softmax activations, a tensor of size\n      [batch_size, num_classes]\n    end_points: a dictionary from components of the network to the corresponding\n      activation.\n\n  Raises:\n    ValueError: if \'depth_multiplier\' is less than or equal to zero.\n  """"""\n  if depth_multiplier <= 0:\n    raise ValueError(\'depth_multiplier is not greater than zero.\')\n  depth = lambda d: max(int(d * depth_multiplier), min_depth)\n\n  with tf.variable_scope(scope, \'InceptionV3\', [inputs, num_classes],\n                         reuse=reuse) as scope:\n    with slim.arg_scope([slim.batch_norm, slim.dropout],\n                        is_training=is_training):\n      net, end_points = inception_v3_base(\n          inputs, scope=scope, min_depth=min_depth,\n          depth_multiplier=depth_multiplier)\n\n      # Auxiliary Head logits\n      with slim.arg_scope([slim.conv2d, slim.max_pool2d, slim.avg_pool2d],\n                          stride=1, padding=\'SAME\'):\n        aux_logits = end_points[\'Mixed_6e\']\n        with tf.variable_scope(\'AuxLogits\'):\n          aux_logits = slim.avg_pool2d(\n              aux_logits, [5, 5], stride=3, padding=\'VALID\',\n              scope=\'AvgPool_1a_5x5\')\n          aux_logits = slim.conv2d(aux_logits, depth(128), [1, 1],\n                                   scope=\'Conv2d_1b_1x1\')\n\n          # Shape of feature map before the final layer.\n          kernel_size = _reduced_kernel_size_for_small_input(\n              aux_logits, [5, 5])\n          aux_logits = slim.conv2d(\n              aux_logits, depth(768), kernel_size,\n              weights_initializer=trunc_normal(0.01),\n              padding=\'VALID\', scope=\'Conv2d_2a_{}x{}\'.format(*kernel_size))\n          aux_logits = slim.conv2d(\n              aux_logits, num_classes, [1, 1], activation_fn=None,\n              normalizer_fn=None, weights_initializer=trunc_normal(0.001),\n              scope=\'Conv2d_2b_1x1\')\n          if spatial_squeeze:\n            aux_logits = tf.squeeze(aux_logits, [1, 2], name=\'SpatialSqueeze\')\n          end_points[\'AuxLogits\'] = aux_logits\n\n      # Final pooling and prediction\n      with tf.variable_scope(\'Logits\'):\n        kernel_size = _reduced_kernel_size_for_small_input(net, [8, 8])\n        net = slim.avg_pool2d(net, kernel_size, padding=\'VALID\',\n                              scope=\'AvgPool_1a_{}x{}\'.format(*kernel_size))\n        # 1 x 1 x 2048\n        net = slim.dropout(net, keep_prob=dropout_keep_prob, scope=\'Dropout_1b\')\n        end_points[\'PreLogits\'] = net\n        # 2048\n        logits = slim.conv2d(net, num_classes, [1, 1], activation_fn=None,\n                             normalizer_fn=None, scope=\'Conv2d_1c_1x1\')\n        if spatial_squeeze:\n          logits = tf.squeeze(logits, [1, 2], name=\'SpatialSqueeze\')\n        # 1000\n      end_points[\'Logits\'] = logits\n      end_points[\'Predictions\'] = prediction_fn(logits, scope=\'Predictions\')\n  return logits, end_points\ninception_v3.default_image_size = 299\n\n\ndef _reduced_kernel_size_for_small_input(input_tensor, kernel_size):\n  """"""Define kernel size which is automatically reduced for small input.\n\n  If the shape of the input images is unknown at graph construction time this\n  function assumes that the input images are is large enough.\n\n  Args:\n    input_tensor: input tensor of size [batch_size, height, width, channels].\n    kernel_size: desired kernel size of length 2: [kernel_height, kernel_width]\n\n  Returns:\n    a tensor with the kernel size.\n\n  TODO(jrru): Make this function work with unknown shapes. Theoretically, this\n  can be done with the code below. Problems are two-fold: (1) If the shape was\n  known, it will be lost. (2) inception.slim.ops._two_element_tuple cannot\n  handle tensors that define the kernel size.\n      shape = tf.shape(input_tensor)\n      return = tf.pack([tf.minimum(shape[1], kernel_size[0]),\n                        tf.minimum(shape[2], kernel_size[1])])\n\n  """"""\n  shape = input_tensor.get_shape().as_list()\n  if shape[1] is None or shape[2] is None:\n    kernel_size_out = kernel_size\n  else:\n    kernel_size_out = [min(shape[1], kernel_size[0]),\n                       min(shape[2], kernel_size[1])]\n  return kernel_size_out\n\n\ninception_v3_arg_scope = inception_utils.inception_arg_scope\n'"
finetuning/nets/inception_v3_test.py,29,"b'# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Tests for nets.inception_v1.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport numpy as np\nimport tensorflow as tf\n\nfrom nets import inception\n\nslim = tf.contrib.slim\n\n\nclass InceptionV3Test(tf.test.TestCase):\n\n  def testBuildClassificationNetwork(self):\n    batch_size = 5\n    height, width = 299, 299\n    num_classes = 1000\n\n    inputs = tf.random_uniform((batch_size, height, width, 3))\n    logits, end_points = inception.inception_v3(inputs, num_classes)\n    self.assertTrue(logits.op.name.startswith(\'InceptionV3/Logits\'))\n    self.assertListEqual(logits.get_shape().as_list(),\n                         [batch_size, num_classes])\n    self.assertTrue(\'Predictions\' in end_points)\n    self.assertListEqual(end_points[\'Predictions\'].get_shape().as_list(),\n                         [batch_size, num_classes])\n\n  def testBuildBaseNetwork(self):\n    batch_size = 5\n    height, width = 299, 299\n\n    inputs = tf.random_uniform((batch_size, height, width, 3))\n    final_endpoint, end_points = inception.inception_v3_base(inputs)\n    self.assertTrue(final_endpoint.op.name.startswith(\n        \'InceptionV3/Mixed_7c\'))\n    self.assertListEqual(final_endpoint.get_shape().as_list(),\n                         [batch_size, 8, 8, 2048])\n    expected_endpoints = [\'Conv2d_1a_3x3\', \'Conv2d_2a_3x3\', \'Conv2d_2b_3x3\',\n                          \'MaxPool_3a_3x3\', \'Conv2d_3b_1x1\', \'Conv2d_4a_3x3\',\n                          \'MaxPool_5a_3x3\', \'Mixed_5b\', \'Mixed_5c\', \'Mixed_5d\',\n                          \'Mixed_6a\', \'Mixed_6b\', \'Mixed_6c\', \'Mixed_6d\',\n                          \'Mixed_6e\', \'Mixed_7a\', \'Mixed_7b\', \'Mixed_7c\']\n    self.assertItemsEqual(end_points.keys(), expected_endpoints)\n\n  def testBuildOnlyUptoFinalEndpoint(self):\n    batch_size = 5\n    height, width = 299, 299\n    endpoints = [\'Conv2d_1a_3x3\', \'Conv2d_2a_3x3\', \'Conv2d_2b_3x3\',\n                 \'MaxPool_3a_3x3\', \'Conv2d_3b_1x1\', \'Conv2d_4a_3x3\',\n                 \'MaxPool_5a_3x3\', \'Mixed_5b\', \'Mixed_5c\', \'Mixed_5d\',\n                 \'Mixed_6a\', \'Mixed_6b\', \'Mixed_6c\', \'Mixed_6d\',\n                 \'Mixed_6e\', \'Mixed_7a\', \'Mixed_7b\', \'Mixed_7c\']\n\n    for index, endpoint in enumerate(endpoints):\n      with tf.Graph().as_default():\n        inputs = tf.random_uniform((batch_size, height, width, 3))\n        out_tensor, end_points = inception.inception_v3_base(\n            inputs, final_endpoint=endpoint)\n        self.assertTrue(out_tensor.op.name.startswith(\n            \'InceptionV3/\' + endpoint))\n        self.assertItemsEqual(endpoints[:index+1], end_points)\n\n  def testBuildAndCheckAllEndPointsUptoMixed7c(self):\n    batch_size = 5\n    height, width = 299, 299\n\n    inputs = tf.random_uniform((batch_size, height, width, 3))\n    _, end_points = inception.inception_v3_base(\n        inputs, final_endpoint=\'Mixed_7c\')\n    endpoints_shapes = {\'Conv2d_1a_3x3\': [batch_size, 149, 149, 32],\n                        \'Conv2d_2a_3x3\': [batch_size, 147, 147, 32],\n                        \'Conv2d_2b_3x3\': [batch_size, 147, 147, 64],\n                        \'MaxPool_3a_3x3\': [batch_size, 73, 73, 64],\n                        \'Conv2d_3b_1x1\': [batch_size, 73, 73, 80],\n                        \'Conv2d_4a_3x3\': [batch_size, 71, 71, 192],\n                        \'MaxPool_5a_3x3\': [batch_size, 35, 35, 192],\n                        \'Mixed_5b\': [batch_size, 35, 35, 256],\n                        \'Mixed_5c\': [batch_size, 35, 35, 288],\n                        \'Mixed_5d\': [batch_size, 35, 35, 288],\n                        \'Mixed_6a\': [batch_size, 17, 17, 768],\n                        \'Mixed_6b\': [batch_size, 17, 17, 768],\n                        \'Mixed_6c\': [batch_size, 17, 17, 768],\n                        \'Mixed_6d\': [batch_size, 17, 17, 768],\n                        \'Mixed_6e\': [batch_size, 17, 17, 768],\n                        \'Mixed_7a\': [batch_size, 8, 8, 1280],\n                        \'Mixed_7b\': [batch_size, 8, 8, 2048],\n                        \'Mixed_7c\': [batch_size, 8, 8, 2048]}\n    self.assertItemsEqual(endpoints_shapes.keys(), end_points.keys())\n    for endpoint_name in endpoints_shapes:\n      expected_shape = endpoints_shapes[endpoint_name]\n      self.assertTrue(endpoint_name in end_points)\n      self.assertListEqual(end_points[endpoint_name].get_shape().as_list(),\n                           expected_shape)\n\n  def testModelHasExpectedNumberOfParameters(self):\n    batch_size = 5\n    height, width = 299, 299\n    inputs = tf.random_uniform((batch_size, height, width, 3))\n    with slim.arg_scope(inception.inception_v3_arg_scope()):\n      inception.inception_v3_base(inputs)\n    total_params, _ = slim.model_analyzer.analyze_vars(\n        slim.get_model_variables())\n    self.assertAlmostEqual(21802784, total_params)\n\n  def testBuildEndPoints(self):\n    batch_size = 5\n    height, width = 299, 299\n    num_classes = 1000\n\n    inputs = tf.random_uniform((batch_size, height, width, 3))\n    _, end_points = inception.inception_v3(inputs, num_classes)\n    self.assertTrue(\'Logits\' in end_points)\n    logits = end_points[\'Logits\']\n    self.assertListEqual(logits.get_shape().as_list(),\n                         [batch_size, num_classes])\n    self.assertTrue(\'AuxLogits\' in end_points)\n    aux_logits = end_points[\'AuxLogits\']\n    self.assertListEqual(aux_logits.get_shape().as_list(),\n                         [batch_size, num_classes])\n    self.assertTrue(\'Mixed_7c\' in end_points)\n    pre_pool = end_points[\'Mixed_7c\']\n    self.assertListEqual(pre_pool.get_shape().as_list(),\n                         [batch_size, 8, 8, 2048])\n    self.assertTrue(\'PreLogits\' in end_points)\n    pre_logits = end_points[\'PreLogits\']\n    self.assertListEqual(pre_logits.get_shape().as_list(),\n                         [batch_size, 1, 1, 2048])\n\n  def testBuildEndPointsWithDepthMultiplierLessThanOne(self):\n    batch_size = 5\n    height, width = 299, 299\n    num_classes = 1000\n\n    inputs = tf.random_uniform((batch_size, height, width, 3))\n    _, end_points = inception.inception_v3(inputs, num_classes)\n\n    endpoint_keys = [key for key in end_points.keys()\n                     if key.startswith(\'Mixed\') or key.startswith(\'Conv\')]\n\n    _, end_points_with_multiplier = inception.inception_v3(\n        inputs, num_classes, scope=\'depth_multiplied_net\',\n        depth_multiplier=0.5)\n\n    for key in endpoint_keys:\n      original_depth = end_points[key].get_shape().as_list()[3]\n      new_depth = end_points_with_multiplier[key].get_shape().as_list()[3]\n      self.assertEqual(0.5 * original_depth, new_depth)\n\n  def testBuildEndPointsWithDepthMultiplierGreaterThanOne(self):\n    batch_size = 5\n    height, width = 299, 299\n    num_classes = 1000\n\n    inputs = tf.random_uniform((batch_size, height, width, 3))\n    _, end_points = inception.inception_v3(inputs, num_classes)\n\n    endpoint_keys = [key for key in end_points.keys()\n                     if key.startswith(\'Mixed\') or key.startswith(\'Conv\')]\n\n    _, end_points_with_multiplier = inception.inception_v3(\n        inputs, num_classes, scope=\'depth_multiplied_net\',\n        depth_multiplier=2.0)\n\n    for key in endpoint_keys:\n      original_depth = end_points[key].get_shape().as_list()[3]\n      new_depth = end_points_with_multiplier[key].get_shape().as_list()[3]\n      self.assertEqual(2.0 * original_depth, new_depth)\n\n  def testRaiseValueErrorWithInvalidDepthMultiplier(self):\n    batch_size = 5\n    height, width = 299, 299\n    num_classes = 1000\n\n    inputs = tf.random_uniform((batch_size, height, width, 3))\n    with self.assertRaises(ValueError):\n      _ = inception.inception_v3(inputs, num_classes, depth_multiplier=-0.1)\n    with self.assertRaises(ValueError):\n      _ = inception.inception_v3(inputs, num_classes, depth_multiplier=0.0)\n\n  def testHalfSizeImages(self):\n    batch_size = 5\n    height, width = 150, 150\n    num_classes = 1000\n\n    inputs = tf.random_uniform((batch_size, height, width, 3))\n    logits, end_points = inception.inception_v3(inputs, num_classes)\n    self.assertTrue(logits.op.name.startswith(\'InceptionV3/Logits\'))\n    self.assertListEqual(logits.get_shape().as_list(),\n                         [batch_size, num_classes])\n    pre_pool = end_points[\'Mixed_7c\']\n    self.assertListEqual(pre_pool.get_shape().as_list(),\n                         [batch_size, 3, 3, 2048])\n\n  def testUnknownImageShape(self):\n    tf.reset_default_graph()\n    batch_size = 2\n    height, width = 299, 299\n    num_classes = 1000\n    input_np = np.random.uniform(0, 1, (batch_size, height, width, 3))\n    with self.test_session() as sess:\n      inputs = tf.placeholder(tf.float32, shape=(batch_size, None, None, 3))\n      logits, end_points = inception.inception_v3(inputs, num_classes)\n      self.assertListEqual(logits.get_shape().as_list(),\n                           [batch_size, num_classes])\n      pre_pool = end_points[\'Mixed_7c\']\n      feed_dict = {inputs: input_np}\n      tf.global_variables_initializer().run()\n      pre_pool_out = sess.run(pre_pool, feed_dict=feed_dict)\n      self.assertListEqual(list(pre_pool_out.shape), [batch_size, 8, 8, 2048])\n\n  def testUnknowBatchSize(self):\n    batch_size = 1\n    height, width = 299, 299\n    num_classes = 1000\n\n    inputs = tf.placeholder(tf.float32, (None, height, width, 3))\n    logits, _ = inception.inception_v3(inputs, num_classes)\n    self.assertTrue(logits.op.name.startswith(\'InceptionV3/Logits\'))\n    self.assertListEqual(logits.get_shape().as_list(),\n                         [None, num_classes])\n    images = tf.random_uniform((batch_size, height, width, 3))\n\n    with self.test_session() as sess:\n      sess.run(tf.global_variables_initializer())\n      output = sess.run(logits, {inputs: images.eval()})\n      self.assertEquals(output.shape, (batch_size, num_classes))\n\n  def testEvaluation(self):\n    batch_size = 2\n    height, width = 299, 299\n    num_classes = 1000\n\n    eval_inputs = tf.random_uniform((batch_size, height, width, 3))\n    logits, _ = inception.inception_v3(eval_inputs, num_classes,\n                                       is_training=False)\n    predictions = tf.argmax(logits, 1)\n\n    with self.test_session() as sess:\n      sess.run(tf.global_variables_initializer())\n      output = sess.run(predictions)\n      self.assertEquals(output.shape, (batch_size,))\n\n  def testTrainEvalWithReuse(self):\n    train_batch_size = 5\n    eval_batch_size = 2\n    height, width = 150, 150\n    num_classes = 1000\n\n    train_inputs = tf.random_uniform((train_batch_size, height, width, 3))\n    inception.inception_v3(train_inputs, num_classes)\n    eval_inputs = tf.random_uniform((eval_batch_size, height, width, 3))\n    logits, _ = inception.inception_v3(eval_inputs, num_classes,\n                                       is_training=False, reuse=True)\n    predictions = tf.argmax(logits, 1)\n\n    with self.test_session() as sess:\n      sess.run(tf.global_variables_initializer())\n      output = sess.run(predictions)\n      self.assertEquals(output.shape, (eval_batch_size,))\n\n  def testLogitsNotSqueezed(self):\n    num_classes = 25\n    images = tf.random_uniform([1, 299, 299, 3])\n    logits, _ = inception.inception_v3(images,\n                                       num_classes=num_classes,\n                                       spatial_squeeze=False)\n\n    with self.test_session() as sess:\n      tf.global_variables_initializer().run()\n      logits_out = sess.run(logits)\n      self.assertListEqual(list(logits_out.shape), [1, 1, 1, num_classes])\n\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
finetuning/nets/inception_v4.py,48,"b'# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Contains the definition of the Inception V4 architecture.\n\nAs described in http://arxiv.org/abs/1602.07261.\n\n  Inception-v4, Inception-ResNet and the Impact of Residual Connections\n    on Learning\n  Christian Szegedy, Sergey Ioffe, Vincent Vanhoucke, Alex Alemi\n""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\n\nfrom nets import inception_utils\n\nslim = tf.contrib.slim\n\n\ndef block_inception_a(inputs, scope=None, reuse=None):\n  """"""Builds Inception-A block for Inception v4 network.""""""\n  # By default use stride=1 and SAME padding\n  with slim.arg_scope([slim.conv2d, slim.avg_pool2d, slim.max_pool2d],\n                      stride=1, padding=\'SAME\'):\n    with tf.variable_scope(scope, \'BlockInceptionA\', [inputs], reuse=reuse):\n      with tf.variable_scope(\'Branch_0\'):\n        branch_0 = slim.conv2d(inputs, 96, [1, 1], scope=\'Conv2d_0a_1x1\')\n      with tf.variable_scope(\'Branch_1\'):\n        branch_1 = slim.conv2d(inputs, 64, [1, 1], scope=\'Conv2d_0a_1x1\')\n        branch_1 = slim.conv2d(branch_1, 96, [3, 3], scope=\'Conv2d_0b_3x3\')\n      with tf.variable_scope(\'Branch_2\'):\n        branch_2 = slim.conv2d(inputs, 64, [1, 1], scope=\'Conv2d_0a_1x1\')\n        branch_2 = slim.conv2d(branch_2, 96, [3, 3], scope=\'Conv2d_0b_3x3\')\n        branch_2 = slim.conv2d(branch_2, 96, [3, 3], scope=\'Conv2d_0c_3x3\')\n      with tf.variable_scope(\'Branch_3\'):\n        branch_3 = slim.avg_pool2d(inputs, [3, 3], scope=\'AvgPool_0a_3x3\')\n        branch_3 = slim.conv2d(branch_3, 96, [1, 1], scope=\'Conv2d_0b_1x1\')\n      return tf.concat(axis=3, values=[branch_0, branch_1, branch_2, branch_3])\n\n\ndef block_reduction_a(inputs, scope=None, reuse=None):\n  """"""Builds Reduction-A block for Inception v4 network.""""""\n  # By default use stride=1 and SAME padding\n  with slim.arg_scope([slim.conv2d, slim.avg_pool2d, slim.max_pool2d],\n                      stride=1, padding=\'SAME\'):\n    with tf.variable_scope(scope, \'BlockReductionA\', [inputs], reuse=reuse):\n      with tf.variable_scope(\'Branch_0\'):\n        branch_0 = slim.conv2d(inputs, 384, [3, 3], stride=2, padding=\'VALID\',\n                               scope=\'Conv2d_1a_3x3\')\n      with tf.variable_scope(\'Branch_1\'):\n        branch_1 = slim.conv2d(inputs, 192, [1, 1], scope=\'Conv2d_0a_1x1\')\n        branch_1 = slim.conv2d(branch_1, 224, [3, 3], scope=\'Conv2d_0b_3x3\')\n        branch_1 = slim.conv2d(branch_1, 256, [3, 3], stride=2,\n                               padding=\'VALID\', scope=\'Conv2d_1a_3x3\')\n      with tf.variable_scope(\'Branch_2\'):\n        branch_2 = slim.max_pool2d(inputs, [3, 3], stride=2, padding=\'VALID\',\n                                   scope=\'MaxPool_1a_3x3\')\n      return tf.concat(axis=3, values=[branch_0, branch_1, branch_2])\n\n\ndef block_inception_b(inputs, scope=None, reuse=None):\n  """"""Builds Inception-B block for Inception v4 network.""""""\n  # By default use stride=1 and SAME padding\n  with slim.arg_scope([slim.conv2d, slim.avg_pool2d, slim.max_pool2d],\n                      stride=1, padding=\'SAME\'):\n    with tf.variable_scope(scope, \'BlockInceptionB\', [inputs], reuse=reuse):\n      with tf.variable_scope(\'Branch_0\'):\n        branch_0 = slim.conv2d(inputs, 384, [1, 1], scope=\'Conv2d_0a_1x1\')\n      with tf.variable_scope(\'Branch_1\'):\n        branch_1 = slim.conv2d(inputs, 192, [1, 1], scope=\'Conv2d_0a_1x1\')\n        branch_1 = slim.conv2d(branch_1, 224, [1, 7], scope=\'Conv2d_0b_1x7\')\n        branch_1 = slim.conv2d(branch_1, 256, [7, 1], scope=\'Conv2d_0c_7x1\')\n      with tf.variable_scope(\'Branch_2\'):\n        branch_2 = slim.conv2d(inputs, 192, [1, 1], scope=\'Conv2d_0a_1x1\')\n        branch_2 = slim.conv2d(branch_2, 192, [7, 1], scope=\'Conv2d_0b_7x1\')\n        branch_2 = slim.conv2d(branch_2, 224, [1, 7], scope=\'Conv2d_0c_1x7\')\n        branch_2 = slim.conv2d(branch_2, 224, [7, 1], scope=\'Conv2d_0d_7x1\')\n        branch_2 = slim.conv2d(branch_2, 256, [1, 7], scope=\'Conv2d_0e_1x7\')\n      with tf.variable_scope(\'Branch_3\'):\n        branch_3 = slim.avg_pool2d(inputs, [3, 3], scope=\'AvgPool_0a_3x3\')\n        branch_3 = slim.conv2d(branch_3, 128, [1, 1], scope=\'Conv2d_0b_1x1\')\n      return tf.concat(axis=3, values=[branch_0, branch_1, branch_2, branch_3])\n\n\ndef block_reduction_b(inputs, scope=None, reuse=None):\n  """"""Builds Reduction-B block for Inception v4 network.""""""\n  # By default use stride=1 and SAME padding\n  with slim.arg_scope([slim.conv2d, slim.avg_pool2d, slim.max_pool2d],\n                      stride=1, padding=\'SAME\'):\n    with tf.variable_scope(scope, \'BlockReductionB\', [inputs], reuse=reuse):\n      with tf.variable_scope(\'Branch_0\'):\n        branch_0 = slim.conv2d(inputs, 192, [1, 1], scope=\'Conv2d_0a_1x1\')\n        branch_0 = slim.conv2d(branch_0, 192, [3, 3], stride=2,\n                               padding=\'VALID\', scope=\'Conv2d_1a_3x3\')\n      with tf.variable_scope(\'Branch_1\'):\n        branch_1 = slim.conv2d(inputs, 256, [1, 1], scope=\'Conv2d_0a_1x1\')\n        branch_1 = slim.conv2d(branch_1, 256, [1, 7], scope=\'Conv2d_0b_1x7\')\n        branch_1 = slim.conv2d(branch_1, 320, [7, 1], scope=\'Conv2d_0c_7x1\')\n        branch_1 = slim.conv2d(branch_1, 320, [3, 3], stride=2,\n                               padding=\'VALID\', scope=\'Conv2d_1a_3x3\')\n      with tf.variable_scope(\'Branch_2\'):\n        branch_2 = slim.max_pool2d(inputs, [3, 3], stride=2, padding=\'VALID\',\n                                   scope=\'MaxPool_1a_3x3\')\n      return tf.concat(axis=3, values=[branch_0, branch_1, branch_2])\n\n\ndef block_inception_c(inputs, scope=None, reuse=None):\n  """"""Builds Inception-C block for Inception v4 network.""""""\n  # By default use stride=1 and SAME padding\n  with slim.arg_scope([slim.conv2d, slim.avg_pool2d, slim.max_pool2d],\n                      stride=1, padding=\'SAME\'):\n    with tf.variable_scope(scope, \'BlockInceptionC\', [inputs], reuse=reuse):\n      with tf.variable_scope(\'Branch_0\'):\n        branch_0 = slim.conv2d(inputs, 256, [1, 1], scope=\'Conv2d_0a_1x1\')\n      with tf.variable_scope(\'Branch_1\'):\n        branch_1 = slim.conv2d(inputs, 384, [1, 1], scope=\'Conv2d_0a_1x1\')\n        branch_1 = tf.concat(axis=3, values=[\n            slim.conv2d(branch_1, 256, [1, 3], scope=\'Conv2d_0b_1x3\'),\n            slim.conv2d(branch_1, 256, [3, 1], scope=\'Conv2d_0c_3x1\')])\n      with tf.variable_scope(\'Branch_2\'):\n        branch_2 = slim.conv2d(inputs, 384, [1, 1], scope=\'Conv2d_0a_1x1\')\n        branch_2 = slim.conv2d(branch_2, 448, [3, 1], scope=\'Conv2d_0b_3x1\')\n        branch_2 = slim.conv2d(branch_2, 512, [1, 3], scope=\'Conv2d_0c_1x3\')\n        branch_2 = tf.concat(axis=3, values=[\n            slim.conv2d(branch_2, 256, [1, 3], scope=\'Conv2d_0d_1x3\'),\n            slim.conv2d(branch_2, 256, [3, 1], scope=\'Conv2d_0e_3x1\')])\n      with tf.variable_scope(\'Branch_3\'):\n        branch_3 = slim.avg_pool2d(inputs, [3, 3], scope=\'AvgPool_0a_3x3\')\n        branch_3 = slim.conv2d(branch_3, 256, [1, 1], scope=\'Conv2d_0b_1x1\')\n      return tf.concat(axis=3, values=[branch_0, branch_1, branch_2, branch_3])\n\n\ndef inception_v4_base(inputs, final_endpoint=\'Mixed_7d\', scope=None):\n  """"""Creates the Inception V4 network up to the given final endpoint.\n\n  Args:\n    inputs: a 4-D tensor of size [batch_size, height, width, 3].\n    final_endpoint: specifies the endpoint to construct the network up to.\n      It can be one of [ \'Conv2d_1a_3x3\', \'Conv2d_2a_3x3\', \'Conv2d_2b_3x3\',\n      \'Mixed_3a\', \'Mixed_4a\', \'Mixed_5a\', \'Mixed_5b\', \'Mixed_5c\', \'Mixed_5d\',\n      \'Mixed_5e\', \'Mixed_6a\', \'Mixed_6b\', \'Mixed_6c\', \'Mixed_6d\', \'Mixed_6e\',\n      \'Mixed_6f\', \'Mixed_6g\', \'Mixed_6h\', \'Mixed_7a\', \'Mixed_7b\', \'Mixed_7c\',\n      \'Mixed_7d\']\n    scope: Optional variable_scope.\n\n  Returns:\n    logits: the logits outputs of the model.\n    end_points: the set of end_points from the inception model.\n\n  Raises:\n    ValueError: if final_endpoint is not set to one of the predefined values,\n  """"""\n  end_points = {}\n\n  def add_and_check_final(name, net):\n    end_points[name] = net\n    return name == final_endpoint\n\n  with tf.variable_scope(scope, \'InceptionV4\', [inputs]):\n    with slim.arg_scope([slim.conv2d, slim.max_pool2d, slim.avg_pool2d],\n                        stride=1, padding=\'SAME\'):\n      # 299 x 299 x 3\n      net = slim.conv2d(inputs, 32, [3, 3], stride=2,\n                        padding=\'VALID\', scope=\'Conv2d_1a_3x3\')\n      if add_and_check_final(\'Conv2d_1a_3x3\', net): return net, end_points\n      # 149 x 149 x 32\n      net = slim.conv2d(net, 32, [3, 3], padding=\'VALID\',\n                        scope=\'Conv2d_2a_3x3\')\n      if add_and_check_final(\'Conv2d_2a_3x3\', net): return net, end_points\n      # 147 x 147 x 32\n      net = slim.conv2d(net, 64, [3, 3], scope=\'Conv2d_2b_3x3\')\n      if add_and_check_final(\'Conv2d_2b_3x3\', net): return net, end_points\n      # 147 x 147 x 64\n      with tf.variable_scope(\'Mixed_3a\'):\n        with tf.variable_scope(\'Branch_0\'):\n          branch_0 = slim.max_pool2d(net, [3, 3], stride=2, padding=\'VALID\',\n                                     scope=\'MaxPool_0a_3x3\')\n        with tf.variable_scope(\'Branch_1\'):\n          branch_1 = slim.conv2d(net, 96, [3, 3], stride=2, padding=\'VALID\',\n                                 scope=\'Conv2d_0a_3x3\')\n        net = tf.concat(axis=3, values=[branch_0, branch_1])\n        if add_and_check_final(\'Mixed_3a\', net): return net, end_points\n\n      # 73 x 73 x 160\n      with tf.variable_scope(\'Mixed_4a\'):\n        with tf.variable_scope(\'Branch_0\'):\n          branch_0 = slim.conv2d(net, 64, [1, 1], scope=\'Conv2d_0a_1x1\')\n          branch_0 = slim.conv2d(branch_0, 96, [3, 3], padding=\'VALID\',\n                                 scope=\'Conv2d_1a_3x3\')\n        with tf.variable_scope(\'Branch_1\'):\n          branch_1 = slim.conv2d(net, 64, [1, 1], scope=\'Conv2d_0a_1x1\')\n          branch_1 = slim.conv2d(branch_1, 64, [1, 7], scope=\'Conv2d_0b_1x7\')\n          branch_1 = slim.conv2d(branch_1, 64, [7, 1], scope=\'Conv2d_0c_7x1\')\n          branch_1 = slim.conv2d(branch_1, 96, [3, 3], padding=\'VALID\',\n                                 scope=\'Conv2d_1a_3x3\')\n        net = tf.concat(axis=3, values=[branch_0, branch_1])\n        if add_and_check_final(\'Mixed_4a\', net): return net, end_points\n\n      # 71 x 71 x 192\n      with tf.variable_scope(\'Mixed_5a\'):\n        with tf.variable_scope(\'Branch_0\'):\n          branch_0 = slim.conv2d(net, 192, [3, 3], stride=2, padding=\'VALID\',\n                                 scope=\'Conv2d_1a_3x3\')\n        with tf.variable_scope(\'Branch_1\'):\n          branch_1 = slim.max_pool2d(net, [3, 3], stride=2, padding=\'VALID\',\n                                     scope=\'MaxPool_1a_3x3\')\n        net = tf.concat(axis=3, values=[branch_0, branch_1])\n        if add_and_check_final(\'Mixed_5a\', net): return net, end_points\n\n      # 35 x 35 x 384\n      # 4 x Inception-A blocks\n      for idx in xrange(4):\n        block_scope = \'Mixed_5\' + chr(ord(\'b\') + idx)\n        net = block_inception_a(net, block_scope)\n        if add_and_check_final(block_scope, net): return net, end_points\n\n      # 35 x 35 x 384\n      # Reduction-A block\n      net = block_reduction_a(net, \'Mixed_6a\')\n      if add_and_check_final(\'Mixed_6a\', net): return net, end_points\n\n      # 17 x 17 x 1024\n      # 7 x Inception-B blocks\n      for idx in xrange(7):\n        block_scope = \'Mixed_6\' + chr(ord(\'b\') + idx)\n        net = block_inception_b(net, block_scope)\n        if add_and_check_final(block_scope, net): return net, end_points\n\n      # 17 x 17 x 1024\n      # Reduction-B block\n      net = block_reduction_b(net, \'Mixed_7a\')\n      if add_and_check_final(\'Mixed_7a\', net): return net, end_points\n\n      # 8 x 8 x 1536\n      # 3 x Inception-C blocks\n      for idx in xrange(3):\n        block_scope = \'Mixed_7\' + chr(ord(\'b\') + idx)\n        net = block_inception_c(net, block_scope)\n        if add_and_check_final(block_scope, net): return net, end_points\n  raise ValueError(\'Unknown final endpoint %s\' % final_endpoint)\n\n\ndef inception_v4(inputs, num_classes=1001, is_training=True,\n                 dropout_keep_prob=0.8,\n                 reuse=None,\n                 scope=\'InceptionV4\',\n                 create_aux_logits=True):\n  """"""Creates the Inception V4 model.\n\n  Args:\n    inputs: a 4-D tensor of size [batch_size, height, width, 3].\n    num_classes: number of predicted classes.\n    is_training: whether is training or not.\n    dropout_keep_prob: float, the fraction to keep before final layer.\n    reuse: whether or not the network and its variables should be reused. To be\n      able to reuse \'scope\' must be given.\n    scope: Optional variable_scope.\n    create_aux_logits: Whether to include the auxilliary logits.\n\n  Returns:\n    logits: the logits outputs of the model.\n    end_points: the set of end_points from the inception model.\n  """"""\n  end_points = {}\n  with tf.variable_scope(scope, \'InceptionV4\', [inputs], reuse=reuse) as scope:\n    with slim.arg_scope([slim.batch_norm, slim.dropout],\n                        is_training=is_training):\n      net, end_points = inception_v4_base(inputs, scope=scope)\n\n      with slim.arg_scope([slim.conv2d, slim.max_pool2d, slim.avg_pool2d],\n                          stride=1, padding=\'SAME\'):\n        # Auxiliary Head logits\n        if create_aux_logits:\n          with tf.variable_scope(\'AuxLogits\'):\n            # 17 x 17 x 1024\n            aux_logits = end_points[\'Mixed_6h\']\n            aux_logits = slim.avg_pool2d(aux_logits, [5, 5], stride=3,\n                                         padding=\'VALID\',\n                                         scope=\'AvgPool_1a_5x5\')\n            aux_logits = slim.conv2d(aux_logits, 128, [1, 1],\n                                     scope=\'Conv2d_1b_1x1\')\n            aux_logits = slim.conv2d(aux_logits, 768,\n                                     aux_logits.get_shape()[1:3],\n                                     padding=\'VALID\', scope=\'Conv2d_2a\')\n            aux_logits = slim.flatten(aux_logits)\n            aux_logits = slim.fully_connected(aux_logits, num_classes,\n                                              activation_fn=None,\n                                              scope=\'Aux_logits\')\n            end_points[\'AuxLogits\'] = aux_logits\n\n        # Final pooling and prediction\n        with tf.variable_scope(\'Logits\'):\n          # 8 x 8 x 1536\n          net = slim.avg_pool2d(net, net.get_shape()[1:3], padding=\'VALID\',\n                                scope=\'AvgPool_1a\')\n          # 1 x 1 x 1536\n          net = slim.dropout(net, dropout_keep_prob, scope=\'Dropout_1b\')\n          net = slim.flatten(net, scope=\'PreLogitsFlatten\')\n          end_points[\'PreLogitsFlatten\'] = net\n          # 1536\n          logits = slim.fully_connected(net, num_classes, activation_fn=None,\n                                        scope=\'Logits\')\n          end_points[\'Logits\'] = logits\n          end_points[\'Predictions\'] = tf.nn.softmax(logits, name=\'Predictions\')\n    return logits, end_points\ninception_v4.default_image_size = 299\n\n\ninception_v4_arg_scope = inception_utils.inception_arg_scope\n'"
finetuning/nets/inception_v4_test.py,27,"b'# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Tests for slim.inception_v4.""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\n\nfrom nets import inception\n\n\nclass InceptionTest(tf.test.TestCase):\n\n  def testBuildLogits(self):\n    batch_size = 5\n    height, width = 299, 299\n    num_classes = 1000\n    inputs = tf.random_uniform((batch_size, height, width, 3))\n    logits, end_points = inception.inception_v4(inputs, num_classes)\n    auxlogits = end_points[\'AuxLogits\']\n    predictions = end_points[\'Predictions\']\n    self.assertTrue(auxlogits.op.name.startswith(\'InceptionV4/AuxLogits\'))\n    self.assertListEqual(auxlogits.get_shape().as_list(),\n                         [batch_size, num_classes])\n    self.assertTrue(logits.op.name.startswith(\'InceptionV4/Logits\'))\n    self.assertListEqual(logits.get_shape().as_list(),\n                         [batch_size, num_classes])\n    self.assertTrue(predictions.op.name.startswith(\n        \'InceptionV4/Logits/Predictions\'))\n    self.assertListEqual(predictions.get_shape().as_list(),\n                         [batch_size, num_classes])\n\n  def testBuildWithoutAuxLogits(self):\n    batch_size = 5\n    height, width = 299, 299\n    num_classes = 1000\n    inputs = tf.random_uniform((batch_size, height, width, 3))\n    logits, endpoints = inception.inception_v4(inputs, num_classes,\n                                               create_aux_logits=False)\n    self.assertFalse(\'AuxLogits\' in endpoints)\n    self.assertTrue(logits.op.name.startswith(\'InceptionV4/Logits\'))\n    self.assertListEqual(logits.get_shape().as_list(),\n                         [batch_size, num_classes])\n\n  def testAllEndPointsShapes(self):\n    batch_size = 5\n    height, width = 299, 299\n    num_classes = 1000\n    inputs = tf.random_uniform((batch_size, height, width, 3))\n    _, end_points = inception.inception_v4(inputs, num_classes)\n    endpoints_shapes = {\'Conv2d_1a_3x3\': [batch_size, 149, 149, 32],\n                        \'Conv2d_2a_3x3\': [batch_size, 147, 147, 32],\n                        \'Conv2d_2b_3x3\': [batch_size, 147, 147, 64],\n                        \'Mixed_3a\': [batch_size, 73, 73, 160],\n                        \'Mixed_4a\': [batch_size, 71, 71, 192],\n                        \'Mixed_5a\': [batch_size, 35, 35, 384],\n                        # 4 x Inception-A blocks\n                        \'Mixed_5b\': [batch_size, 35, 35, 384],\n                        \'Mixed_5c\': [batch_size, 35, 35, 384],\n                        \'Mixed_5d\': [batch_size, 35, 35, 384],\n                        \'Mixed_5e\': [batch_size, 35, 35, 384],\n                        # Reduction-A block\n                        \'Mixed_6a\': [batch_size, 17, 17, 1024],\n                        # 7 x Inception-B blocks\n                        \'Mixed_6b\': [batch_size, 17, 17, 1024],\n                        \'Mixed_6c\': [batch_size, 17, 17, 1024],\n                        \'Mixed_6d\': [batch_size, 17, 17, 1024],\n                        \'Mixed_6e\': [batch_size, 17, 17, 1024],\n                        \'Mixed_6f\': [batch_size, 17, 17, 1024],\n                        \'Mixed_6g\': [batch_size, 17, 17, 1024],\n                        \'Mixed_6h\': [batch_size, 17, 17, 1024],\n                        # Reduction-A block\n                        \'Mixed_7a\': [batch_size, 8, 8, 1536],\n                        # 3 x Inception-C blocks\n                        \'Mixed_7b\': [batch_size, 8, 8, 1536],\n                        \'Mixed_7c\': [batch_size, 8, 8, 1536],\n                        \'Mixed_7d\': [batch_size, 8, 8, 1536],\n                        # Logits and predictions\n                        \'AuxLogits\': [batch_size, num_classes],\n                        \'PreLogitsFlatten\': [batch_size, 1536],\n                        \'Logits\': [batch_size, num_classes],\n                        \'Predictions\': [batch_size, num_classes]}\n    self.assertItemsEqual(endpoints_shapes.keys(), end_points.keys())\n    for endpoint_name in endpoints_shapes:\n      expected_shape = endpoints_shapes[endpoint_name]\n      self.assertTrue(endpoint_name in end_points)\n      self.assertListEqual(end_points[endpoint_name].get_shape().as_list(),\n                           expected_shape)\n\n  def testBuildBaseNetwork(self):\n    batch_size = 5\n    height, width = 299, 299\n    inputs = tf.random_uniform((batch_size, height, width, 3))\n    net, end_points = inception.inception_v4_base(inputs)\n    self.assertTrue(net.op.name.startswith(\n        \'InceptionV4/Mixed_7d\'))\n    self.assertListEqual(net.get_shape().as_list(), [batch_size, 8, 8, 1536])\n    expected_endpoints = [\n        \'Conv2d_1a_3x3\', \'Conv2d_2a_3x3\', \'Conv2d_2b_3x3\', \'Mixed_3a\',\n        \'Mixed_4a\', \'Mixed_5a\', \'Mixed_5b\', \'Mixed_5c\', \'Mixed_5d\',\n        \'Mixed_5e\', \'Mixed_6a\', \'Mixed_6b\', \'Mixed_6c\', \'Mixed_6d\',\n        \'Mixed_6e\', \'Mixed_6f\', \'Mixed_6g\', \'Mixed_6h\', \'Mixed_7a\',\n        \'Mixed_7b\', \'Mixed_7c\', \'Mixed_7d\']\n    self.assertItemsEqual(end_points.keys(), expected_endpoints)\n    for name, op in end_points.iteritems():\n      self.assertTrue(op.name.startswith(\'InceptionV4/\' + name))\n\n  def testBuildOnlyUpToFinalEndpoint(self):\n    batch_size = 5\n    height, width = 299, 299\n    all_endpoints = [\n        \'Conv2d_1a_3x3\', \'Conv2d_2a_3x3\', \'Conv2d_2b_3x3\', \'Mixed_3a\',\n        \'Mixed_4a\', \'Mixed_5a\', \'Mixed_5b\', \'Mixed_5c\', \'Mixed_5d\',\n        \'Mixed_5e\', \'Mixed_6a\', \'Mixed_6b\', \'Mixed_6c\', \'Mixed_6d\',\n        \'Mixed_6e\', \'Mixed_6f\', \'Mixed_6g\', \'Mixed_6h\', \'Mixed_7a\',\n        \'Mixed_7b\', \'Mixed_7c\', \'Mixed_7d\']\n    for index, endpoint in enumerate(all_endpoints):\n      with tf.Graph().as_default():\n        inputs = tf.random_uniform((batch_size, height, width, 3))\n        out_tensor, end_points = inception.inception_v4_base(\n            inputs, final_endpoint=endpoint)\n        self.assertTrue(out_tensor.op.name.startswith(\n            \'InceptionV4/\' + endpoint))\n        self.assertItemsEqual(all_endpoints[:index+1], end_points)\n\n  def testVariablesSetDevice(self):\n    batch_size = 5\n    height, width = 299, 299\n    num_classes = 1000\n    inputs = tf.random_uniform((batch_size, height, width, 3))\n    # Force all Variables to reside on the device.\n    with tf.variable_scope(\'on_cpu\'), tf.device(\'/cpu:0\'):\n      inception.inception_v4(inputs, num_classes)\n    with tf.variable_scope(\'on_gpu\'), tf.device(\'/gpu:0\'):\n      inception.inception_v4(inputs, num_classes)\n    for v in tf.get_collection(tf.GraphKeys.VARIABLES, scope=\'on_cpu\'):\n      self.assertDeviceEqual(v.device, \'/cpu:0\')\n    for v in tf.get_collection(tf.GraphKeys.VARIABLES, scope=\'on_gpu\'):\n      self.assertDeviceEqual(v.device, \'/gpu:0\')\n\n  def testHalfSizeImages(self):\n    batch_size = 5\n    height, width = 150, 150\n    num_classes = 1000\n    inputs = tf.random_uniform((batch_size, height, width, 3))\n    logits, end_points = inception.inception_v4(inputs, num_classes)\n    self.assertTrue(logits.op.name.startswith(\'InceptionV4/Logits\'))\n    self.assertListEqual(logits.get_shape().as_list(),\n                         [batch_size, num_classes])\n    pre_pool = end_points[\'Mixed_7d\']\n    self.assertListEqual(pre_pool.get_shape().as_list(),\n                         [batch_size, 3, 3, 1536])\n\n  def testUnknownBatchSize(self):\n    batch_size = 1\n    height, width = 299, 299\n    num_classes = 1000\n    with self.test_session() as sess:\n      inputs = tf.placeholder(tf.float32, (None, height, width, 3))\n      logits, _ = inception.inception_v4(inputs, num_classes)\n      self.assertTrue(logits.op.name.startswith(\'InceptionV4/Logits\'))\n      self.assertListEqual(logits.get_shape().as_list(),\n                           [None, num_classes])\n      images = tf.random_uniform((batch_size, height, width, 3))\n      try:\n        sess.run(tf.global_variables_initializer())\n      except AttributeError:\n        sess.run(tf.initialize_all_variables())\n      output = sess.run(logits, {inputs: images.eval()})\n      self.assertEquals(output.shape, (batch_size, num_classes))\n\n  def testEvaluation(self):\n    batch_size = 2\n    height, width = 299, 299\n    num_classes = 1000\n    with self.test_session() as sess:\n      eval_inputs = tf.random_uniform((batch_size, height, width, 3))\n      logits, _ = inception.inception_v4(eval_inputs,\n                                         num_classes,\n                                         is_training=False)\n      predictions = tf.argmax(logits, 1)\n      try:\n        sess.run(tf.global_variables_initializer())\n      except AttributeError:\n        sess.run(tf.initialize_all_variables())\n      output = sess.run(predictions)\n      self.assertEquals(output.shape, (batch_size,))\n\n  def testTrainEvalWithReuse(self):\n    train_batch_size = 5\n    eval_batch_size = 2\n    height, width = 150, 150\n    num_classes = 1000\n    with self.test_session() as sess:\n      train_inputs = tf.random_uniform((train_batch_size, height, width, 3))\n      inception.inception_v4(train_inputs, num_classes)\n      eval_inputs = tf.random_uniform((eval_batch_size, height, width, 3))\n      logits, _ = inception.inception_v4(eval_inputs,\n                                         num_classes,\n                                         is_training=False,\n                                         reuse=True)\n      predictions = tf.argmax(logits, 1)\n      try:\n        sess.run(tf.global_variables_initializer())\n      except AttributeError:\n        sess.run(tf.initialize_all_variables())\n      output = sess.run(predictions)\n      self.assertEquals(output.shape, (eval_batch_size,))\n\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
finetuning/nets/lenet.py,6,"b'# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Contains a variant of the LeNet model definition.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\n\nslim = tf.contrib.slim\n\n\ndef lenet(images, num_classes=10, is_training=False,\n          dropout_keep_prob=0.5,\n          prediction_fn=slim.softmax,\n          scope=\'LeNet\'):\n  """"""Creates a variant of the LeNet model.\n\n  Note that since the output is a set of \'logits\', the values fall in the\n  interval of (-infinity, infinity). Consequently, to convert the outputs to a\n  probability distribution over the characters, one will need to convert them\n  using the softmax function:\n\n        logits = lenet.lenet(images, is_training=False)\n        probabilities = tf.nn.softmax(logits)\n        predictions = tf.argmax(logits, 1)\n\n  Args:\n    images: A batch of `Tensors` of size [batch_size, height, width, channels].\n    num_classes: the number of classes in the dataset.\n    is_training: specifies whether or not we\'re currently training the model.\n      This variable will determine the behaviour of the dropout layer.\n    dropout_keep_prob: the percentage of activation values that are retained.\n    prediction_fn: a function to get predictions out of logits.\n    scope: Optional variable_scope.\n\n  Returns:\n    logits: the pre-softmax activations, a tensor of size\n      [batch_size, `num_classes`]\n    end_points: a dictionary from components of the network to the corresponding\n      activation.\n  """"""\n  end_points = {}\n\n  with tf.variable_scope(scope, \'LeNet\', [images, num_classes]):\n    net = slim.conv2d(images, 32, [5, 5], scope=\'conv1\')\n    net = slim.max_pool2d(net, [2, 2], 2, scope=\'pool1\')\n    net = slim.conv2d(net, 64, [5, 5], scope=\'conv2\')\n    net = slim.max_pool2d(net, [2, 2], 2, scope=\'pool2\')\n    net = slim.flatten(net)\n    end_points[\'Flatten\'] = net\n\n    net = slim.fully_connected(net, 1024, scope=\'fc3\')\n    net = slim.dropout(net, dropout_keep_prob, is_training=is_training,\n                       scope=\'dropout3\')\n    logits = slim.fully_connected(net, num_classes, activation_fn=None,\n                                  scope=\'fc4\')\n\n  end_points[\'Logits\'] = logits\n  end_points[\'Predictions\'] = prediction_fn(logits, scope=\'Predictions\')\n\n  return logits, end_points\nlenet.default_image_size = 28\n\n\ndef lenet_arg_scope(weight_decay=0.0):\n  """"""Defines the default lenet argument scope.\n\n  Args:\n    weight_decay: The weight decay to use for regularizing the model.\n\n  Returns:\n    An `arg_scope` to use for the inception v3 model.\n  """"""\n  with slim.arg_scope(\n      [slim.conv2d, slim.fully_connected],\n      weights_regularizer=slim.l2_regularizer(weight_decay),\n      weights_initializer=tf.truncated_normal_initializer(stddev=0.1),\n      activation_fn=tf.nn.relu) as sc:\n    return sc\n'"
finetuning/nets/nets_factory.py,1,"b'# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Contains a factory for building various models.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nimport functools\n\nimport tensorflow as tf\n\nfrom nets import alexnet\nfrom nets import cifarnet\nfrom nets import inception\nfrom nets import lenet\nfrom nets import overfeat\nfrom nets import resnet_v1\nfrom nets import resnet_v2\nfrom nets import vgg\n\nslim = tf.contrib.slim\n\nnetworks_map = {\'alexnet_v2\': alexnet.alexnet_v2,\n                \'cifarnet\': cifarnet.cifarnet,\n                \'overfeat\': overfeat.overfeat,\n                \'vgg_a\': vgg.vgg_a,\n                \'vgg_16\': vgg.vgg_16,\n                \'vgg_19\': vgg.vgg_19,\n                \'inception_v1\': inception.inception_v1,\n                \'inception_v2\': inception.inception_v2,\n                \'inception_v3\': inception.inception_v3,\n                \'inception_v4\': inception.inception_v4,\n                \'inception_resnet_v2\': inception.inception_resnet_v2,\n                \'lenet\': lenet.lenet,\n                \'resnet_v1_50\': resnet_v1.resnet_v1_50,\n                \'resnet_v1_101\': resnet_v1.resnet_v1_101,\n                \'resnet_v1_152\': resnet_v1.resnet_v1_152,\n                \'resnet_v1_200\': resnet_v1.resnet_v1_200,\n                \'resnet_v2_50\': resnet_v2.resnet_v2_50,\n                \'resnet_v2_101\': resnet_v2.resnet_v2_101,\n                \'resnet_v2_152\': resnet_v2.resnet_v2_152,\n                \'resnet_v2_200\': resnet_v2.resnet_v2_200,\n               }\n\narg_scopes_map = {\'alexnet_v2\': alexnet.alexnet_v2_arg_scope,\n                  \'cifarnet\': cifarnet.cifarnet_arg_scope,\n                  \'overfeat\': overfeat.overfeat_arg_scope,\n                  \'vgg_a\': vgg.vgg_arg_scope,\n                  \'vgg_16\': vgg.vgg_arg_scope,\n                  \'vgg_19\': vgg.vgg_arg_scope,\n                  \'inception_v1\': inception.inception_v3_arg_scope,\n                  \'inception_v2\': inception.inception_v3_arg_scope,\n                  \'inception_v3\': inception.inception_v3_arg_scope,\n                  \'inception_v4\': inception.inception_v4_arg_scope,\n                  \'inception_resnet_v2\':\n                  inception.inception_resnet_v2_arg_scope,\n                  \'lenet\': lenet.lenet_arg_scope,\n                  \'resnet_v1_50\': resnet_v1.resnet_arg_scope,\n                  \'resnet_v1_101\': resnet_v1.resnet_arg_scope,\n                  \'resnet_v1_152\': resnet_v1.resnet_arg_scope,\n                  \'resnet_v1_200\': resnet_v1.resnet_arg_scope,\n                  \'resnet_v2_50\': resnet_v2.resnet_arg_scope,\n                  \'resnet_v2_101\': resnet_v2.resnet_arg_scope,\n                  \'resnet_v2_152\': resnet_v2.resnet_arg_scope,\n                  \'resnet_v2_200\': resnet_v2.resnet_arg_scope,\n                 }\n\n\ndef get_network_fn(name, num_classes, weight_decay=0.0, is_training=False):\n  """"""Returns a network_fn such as `logits, end_points = network_fn(images)`.\n\n  Args:\n    name: The name of the network.\n    num_classes: The number of classes to use for classification.\n    weight_decay: The l2 coefficient for the model weights.\n    is_training: `True` if the model is being used for training and `False`\n      otherwise.\n\n  Returns:\n    network_fn: A function that applies the model to a batch of images. It has\n      the following signature:\n        logits, end_points = network_fn(images)\n  Raises:\n    ValueError: If network `name` is not recognized.\n  """"""\n  if name not in networks_map:\n    raise ValueError(\'Name of network unknown %s\' % name)\n  arg_scope = arg_scopes_map[name](weight_decay=weight_decay)\n  func = networks_map[name]\n  @functools.wraps(func)\n  def network_fn(images):\n    with slim.arg_scope(arg_scope):\n      return func(images, num_classes, is_training=is_training)\n  if hasattr(func, \'default_image_size\'):\n    network_fn.default_image_size = func.default_image_size\n\n  return network_fn\n'"
finetuning/nets/nets_factory_test.py,4,"b'# Copyright 2016 Google Inc. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n""""""Tests for slim.inception.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\n\nimport tensorflow as tf\n\nfrom nets import nets_factory\n\n\nclass NetworksTest(tf.test.TestCase):\n\n  def testGetNetworkFn(self):\n    batch_size = 5\n    num_classes = 1000\n    for net in nets_factory.networks_map:\n      with self.test_session():\n        net_fn = nets_factory.get_network_fn(net, num_classes)\n        # Most networks use 224 as their default_image_size\n        image_size = getattr(net_fn, \'default_image_size\', 224)\n        inputs = tf.random_uniform((batch_size, image_size, image_size, 3))\n        logits, end_points = net_fn(inputs)\n        self.assertTrue(isinstance(logits, tf.Tensor))\n        self.assertTrue(isinstance(end_points, dict))\n        self.assertEqual(logits.get_shape().as_list()[0], batch_size)\n        self.assertEqual(logits.get_shape().as_list()[-1], num_classes)\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
finetuning/nets/overfeat.py,8,"b'# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Contains the model definition for the OverFeat network.\n\nThe definition for the network was obtained from:\n  OverFeat: Integrated Recognition, Localization and Detection using\n  Convolutional Networks\n  Pierre Sermanet, David Eigen, Xiang Zhang, Michael Mathieu, Rob Fergus and\n  Yann LeCun, 2014\n  http://arxiv.org/abs/1312.6229\n\nUsage:\n  with slim.arg_scope(overfeat.overfeat_arg_scope()):\n    outputs, end_points = overfeat.overfeat(inputs)\n\n@@overfeat\n""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\n\nslim = tf.contrib.slim\ntrunc_normal = lambda stddev: tf.truncated_normal_initializer(0.0, stddev)\n\n\ndef overfeat_arg_scope(weight_decay=0.0005):\n  with slim.arg_scope([slim.conv2d, slim.fully_connected],\n                      activation_fn=tf.nn.relu,\n                      weights_regularizer=slim.l2_regularizer(weight_decay),\n                      biases_initializer=tf.zeros_initializer()):\n    with slim.arg_scope([slim.conv2d], padding=\'SAME\'):\n      with slim.arg_scope([slim.max_pool2d], padding=\'VALID\') as arg_sc:\n        return arg_sc\n\n\ndef overfeat(inputs,\n             num_classes=1000,\n             is_training=True,\n             dropout_keep_prob=0.5,\n             spatial_squeeze=True,\n             scope=\'overfeat\'):\n  """"""Contains the model definition for the OverFeat network.\n\n  The definition for the network was obtained from:\n    OverFeat: Integrated Recognition, Localization and Detection using\n    Convolutional Networks\n    Pierre Sermanet, David Eigen, Xiang Zhang, Michael Mathieu, Rob Fergus and\n    Yann LeCun, 2014\n    http://arxiv.org/abs/1312.6229\n\n  Note: All the fully_connected layers have been transformed to conv2d layers.\n        To use in classification mode, resize input to 231x231. To use in fully\n        convolutional mode, set spatial_squeeze to false.\n\n  Args:\n    inputs: a tensor of size [batch_size, height, width, channels].\n    num_classes: number of predicted classes.\n    is_training: whether or not the model is being trained.\n    dropout_keep_prob: the probability that activations are kept in the dropout\n      layers during training.\n    spatial_squeeze: whether or not should squeeze the spatial dimensions of the\n      outputs. Useful to remove unnecessary dimensions for classification.\n    scope: Optional scope for the variables.\n\n  Returns:\n    the last op containing the log predictions and end_points dict.\n\n  """"""\n  with tf.variable_scope(scope, \'overfeat\', [inputs]) as sc:\n    end_points_collection = sc.name + \'_end_points\'\n    # Collect outputs for conv2d, fully_connected and max_pool2d\n    with slim.arg_scope([slim.conv2d, slim.fully_connected, slim.max_pool2d],\n                        outputs_collections=end_points_collection):\n      net = slim.conv2d(inputs, 64, [11, 11], 4, padding=\'VALID\',\n                        scope=\'conv1\')\n      net = slim.max_pool2d(net, [2, 2], scope=\'pool1\')\n      net = slim.conv2d(net, 256, [5, 5], padding=\'VALID\', scope=\'conv2\')\n      net = slim.max_pool2d(net, [2, 2], scope=\'pool2\')\n      net = slim.conv2d(net, 512, [3, 3], scope=\'conv3\')\n      net = slim.conv2d(net, 1024, [3, 3], scope=\'conv4\')\n      net = slim.conv2d(net, 1024, [3, 3], scope=\'conv5\')\n      net = slim.max_pool2d(net, [2, 2], scope=\'pool5\')\n      with slim.arg_scope([slim.conv2d],\n                          weights_initializer=trunc_normal(0.005),\n                          biases_initializer=tf.constant_initializer(0.1)):\n        # Use conv2d instead of fully_connected layers.\n        net = slim.conv2d(net, 3072, [6, 6], padding=\'VALID\', scope=\'fc6\')\n        net = slim.dropout(net, dropout_keep_prob, is_training=is_training,\n                           scope=\'dropout6\')\n        net = slim.conv2d(net, 4096, [1, 1], scope=\'fc7\')\n        net = slim.dropout(net, dropout_keep_prob, is_training=is_training,\n                           scope=\'dropout7\')\n        net = slim.conv2d(net, num_classes, [1, 1],\n                          activation_fn=None,\n                          normalizer_fn=None,\n                          biases_initializer=tf.zeros_initializer(),\n                          scope=\'fc8\')\n      # Convert end_points_collection into a end_point dict.\n      end_points = slim.utils.convert_collection_to_dict(end_points_collection)\n      if spatial_squeeze:\n        net = tf.squeeze(net, [1, 2], name=\'fc8/squeezed\')\n        end_points[sc.name + \'/fc8\'] = net\n      return net, end_points\noverfeat.default_image_size = 231\n'"
finetuning/nets/overfeat_test.py,17,"b'# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Tests for slim.nets.overfeat.""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\n\nfrom nets import overfeat\n\nslim = tf.contrib.slim\n\n\nclass OverFeatTest(tf.test.TestCase):\n\n  def testBuild(self):\n    batch_size = 5\n    height, width = 231, 231\n    num_classes = 1000\n    with self.test_session():\n      inputs = tf.random_uniform((batch_size, height, width, 3))\n      logits, _ = overfeat.overfeat(inputs, num_classes)\n      self.assertEquals(logits.op.name, \'overfeat/fc8/squeezed\')\n      self.assertListEqual(logits.get_shape().as_list(),\n                           [batch_size, num_classes])\n\n  def testFullyConvolutional(self):\n    batch_size = 1\n    height, width = 281, 281\n    num_classes = 1000\n    with self.test_session():\n      inputs = tf.random_uniform((batch_size, height, width, 3))\n      logits, _ = overfeat.overfeat(inputs, num_classes, spatial_squeeze=False)\n      self.assertEquals(logits.op.name, \'overfeat/fc8/BiasAdd\')\n      self.assertListEqual(logits.get_shape().as_list(),\n                           [batch_size, 2, 2, num_classes])\n\n  def testEndPoints(self):\n    batch_size = 5\n    height, width = 231, 231\n    num_classes = 1000\n    with self.test_session():\n      inputs = tf.random_uniform((batch_size, height, width, 3))\n      _, end_points = overfeat.overfeat(inputs, num_classes)\n      expected_names = [\'overfeat/conv1\',\n                        \'overfeat/pool1\',\n                        \'overfeat/conv2\',\n                        \'overfeat/pool2\',\n                        \'overfeat/conv3\',\n                        \'overfeat/conv4\',\n                        \'overfeat/conv5\',\n                        \'overfeat/pool5\',\n                        \'overfeat/fc6\',\n                        \'overfeat/fc7\',\n                        \'overfeat/fc8\'\n                       ]\n      self.assertSetEqual(set(end_points.keys()), set(expected_names))\n\n  def testModelVariables(self):\n    batch_size = 5\n    height, width = 231, 231\n    num_classes = 1000\n    with self.test_session():\n      inputs = tf.random_uniform((batch_size, height, width, 3))\n      overfeat.overfeat(inputs, num_classes)\n      expected_names = [\'overfeat/conv1/weights\',\n                        \'overfeat/conv1/biases\',\n                        \'overfeat/conv2/weights\',\n                        \'overfeat/conv2/biases\',\n                        \'overfeat/conv3/weights\',\n                        \'overfeat/conv3/biases\',\n                        \'overfeat/conv4/weights\',\n                        \'overfeat/conv4/biases\',\n                        \'overfeat/conv5/weights\',\n                        \'overfeat/conv5/biases\',\n                        \'overfeat/fc6/weights\',\n                        \'overfeat/fc6/biases\',\n                        \'overfeat/fc7/weights\',\n                        \'overfeat/fc7/biases\',\n                        \'overfeat/fc8/weights\',\n                        \'overfeat/fc8/biases\',\n                       ]\n      model_variables = [v.op.name for v in slim.get_model_variables()]\n      self.assertSetEqual(set(model_variables), set(expected_names))\n\n  def testEvaluation(self):\n    batch_size = 2\n    height, width = 231, 231\n    num_classes = 1000\n    with self.test_session():\n      eval_inputs = tf.random_uniform((batch_size, height, width, 3))\n      logits, _ = overfeat.overfeat(eval_inputs, is_training=False)\n      self.assertListEqual(logits.get_shape().as_list(),\n                           [batch_size, num_classes])\n      predictions = tf.argmax(logits, 1)\n      self.assertListEqual(predictions.get_shape().as_list(), [batch_size])\n\n  def testTrainEvalWithReuse(self):\n    train_batch_size = 2\n    eval_batch_size = 1\n    train_height, train_width = 231, 231\n    eval_height, eval_width = 281, 281\n    num_classes = 1000\n    with self.test_session():\n      train_inputs = tf.random_uniform(\n          (train_batch_size, train_height, train_width, 3))\n      logits, _ = overfeat.overfeat(train_inputs)\n      self.assertListEqual(logits.get_shape().as_list(),\n                           [train_batch_size, num_classes])\n      tf.get_variable_scope().reuse_variables()\n      eval_inputs = tf.random_uniform(\n          (eval_batch_size, eval_height, eval_width, 3))\n      logits, _ = overfeat.overfeat(eval_inputs, is_training=False,\n                                    spatial_squeeze=False)\n      self.assertListEqual(logits.get_shape().as_list(),\n                           [eval_batch_size, 2, 2, num_classes])\n      logits = tf.reduce_mean(logits, [1, 2])\n      predictions = tf.argmax(logits, 1)\n      self.assertEquals(predictions.get_shape().as_list(), [eval_batch_size])\n\n  def testForward(self):\n    batch_size = 1\n    height, width = 231, 231\n    with self.test_session() as sess:\n      inputs = tf.random_uniform((batch_size, height, width, 3))\n      logits, _ = overfeat.overfeat(inputs)\n      try:\n        sess.run(tf.global_variables_initializer())\n      except AttributeError:\n        sess.run(tf.initialize_all_variables())\n      output = sess.run(logits)\n      self.assertTrue(output.any())\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
finetuning/nets/resnet_utils.py,6,"b'# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Contains building blocks for various versions of Residual Networks.\n\nResidual networks (ResNets) were proposed in:\n  Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun\n  Deep Residual Learning for Image Recognition. arXiv:1512.03385, 2015\n\nMore variants were introduced in:\n  Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun\n  Identity Mappings in Deep Residual Networks. arXiv: 1603.05027, 2016\n\nWe can obtain different ResNet variants by changing the network depth, width,\nand form of residual unit. This module implements the infrastructure for\nbuilding them. Concrete ResNet units and full ResNet networks are implemented in\nthe accompanying resnet_v1.py and resnet_v2.py modules.\n\nCompared to https://github.com/KaimingHe/deep-residual-networks, in the current\nimplementation we subsample the output activations in the last residual unit of\neach block, instead of subsampling the input activations in the first residual\nunit of each block. The two implementations give identical results but our\nimplementation is more memory efficient.\n""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport collections\nimport tensorflow as tf\n\nslim = tf.contrib.slim\n\n\nclass Block(collections.namedtuple(\'Block\', [\'scope\', \'unit_fn\', \'args\'])):\n  """"""A named tuple describing a ResNet block.\n\n  Its parts are:\n    scope: The scope of the `Block`.\n    unit_fn: The ResNet unit function which takes as input a `Tensor` and\n      returns another `Tensor` with the output of the ResNet unit.\n    args: A list of length equal to the number of units in the `Block`. The list\n      contains one (depth, depth_bottleneck, stride) tuple for each unit in the\n      block to serve as argument to unit_fn.\n  """"""\n\n\ndef subsample(inputs, factor, scope=None):\n  """"""Subsamples the input along the spatial dimensions.\n\n  Args:\n    inputs: A `Tensor` of size [batch, height_in, width_in, channels].\n    factor: The subsampling factor.\n    scope: Optional variable_scope.\n\n  Returns:\n    output: A `Tensor` of size [batch, height_out, width_out, channels] with the\n      input, either intact (if factor == 1) or subsampled (if factor > 1).\n  """"""\n  if factor == 1:\n    return inputs\n  else:\n    return slim.max_pool2d(inputs, [1, 1], stride=factor, scope=scope)\n\n\ndef conv2d_same(inputs, num_outputs, kernel_size, stride, rate=1, scope=None):\n  """"""Strided 2-D convolution with \'SAME\' padding.\n\n  When stride > 1, then we do explicit zero-padding, followed by conv2d with\n  \'VALID\' padding.\n\n  Note that\n\n     net = conv2d_same(inputs, num_outputs, 3, stride=stride)\n\n  is equivalent to\n\n     net = slim.conv2d(inputs, num_outputs, 3, stride=1, padding=\'SAME\')\n     net = subsample(net, factor=stride)\n\n  whereas\n\n     net = slim.conv2d(inputs, num_outputs, 3, stride=stride, padding=\'SAME\')\n\n  is different when the input\'s height or width is even, which is why we add the\n  current function. For more details, see ResnetUtilsTest.testConv2DSameEven().\n\n  Args:\n    inputs: A 4-D tensor of size [batch, height_in, width_in, channels].\n    num_outputs: An integer, the number of output filters.\n    kernel_size: An int with the kernel_size of the filters.\n    stride: An integer, the output stride.\n    rate: An integer, rate for atrous convolution.\n    scope: Scope.\n\n  Returns:\n    output: A 4-D tensor of size [batch, height_out, width_out, channels] with\n      the convolution output.\n  """"""\n  if stride == 1:\n    return slim.conv2d(inputs, num_outputs, kernel_size, stride=1, rate=rate,\n                       padding=\'SAME\', scope=scope)\n  else:\n    kernel_size_effective = kernel_size + (kernel_size - 1) * (rate - 1)\n    pad_total = kernel_size_effective - 1\n    pad_beg = pad_total // 2\n    pad_end = pad_total - pad_beg\n    inputs = tf.pad(inputs,\n                    [[0, 0], [pad_beg, pad_end], [pad_beg, pad_end], [0, 0]])\n    return slim.conv2d(inputs, num_outputs, kernel_size, stride=stride,\n                       rate=rate, padding=\'VALID\', scope=scope)\n\n\n@slim.add_arg_scope\ndef stack_blocks_dense(net, blocks, output_stride=None,\n                       outputs_collections=None):\n  """"""Stacks ResNet `Blocks` and controls output feature density.\n\n  First, this function creates scopes for the ResNet in the form of\n  \'block_name/unit_1\', \'block_name/unit_2\', etc.\n\n  Second, this function allows the user to explicitly control the ResNet\n  output_stride, which is the ratio of the input to output spatial resolution.\n  This is useful for dense prediction tasks such as semantic segmentation or\n  object detection.\n\n  Most ResNets consist of 4 ResNet blocks and subsample the activations by a\n  factor of 2 when transitioning between consecutive ResNet blocks. This results\n  to a nominal ResNet output_stride equal to 8. If we set the output_stride to\n  half the nominal network stride (e.g., output_stride=4), then we compute\n  responses twice.\n\n  Control of the output feature density is implemented by atrous convolution.\n\n  Args:\n    net: A `Tensor` of size [batch, height, width, channels].\n    blocks: A list of length equal to the number of ResNet `Blocks`. Each\n      element is a ResNet `Block` object describing the units in the `Block`.\n    output_stride: If `None`, then the output will be computed at the nominal\n      network stride. If output_stride is not `None`, it specifies the requested\n      ratio of input to output spatial resolution, which needs to be equal to\n      the product of unit strides from the start up to some level of the ResNet.\n      For example, if the ResNet employs units with strides 1, 2, 1, 3, 4, 1,\n      then valid values for the output_stride are 1, 2, 6, 24 or None (which\n      is equivalent to output_stride=24).\n    outputs_collections: Collection to add the ResNet block outputs.\n\n  Returns:\n    net: Output tensor with stride equal to the specified output_stride.\n\n  Raises:\n    ValueError: If the target output_stride is not valid.\n  """"""\n  # The current_stride variable keeps track of the effective stride of the\n  # activations. This allows us to invoke atrous convolution whenever applying\n  # the next residual unit would result in the activations having stride larger\n  # than the target output_stride.\n  current_stride = 1\n\n  # The atrous convolution rate parameter.\n  rate = 1\n\n  for block in blocks:\n    with tf.variable_scope(block.scope, \'block\', [net]) as sc:\n      for i, unit in enumerate(block.args):\n        if output_stride is not None and current_stride > output_stride:\n          raise ValueError(\'The target output_stride cannot be reached.\')\n\n        with tf.variable_scope(\'unit_%d\' % (i + 1), values=[net]):\n          unit_depth, unit_depth_bottleneck, unit_stride = unit\n\n          # If we have reached the target output_stride, then we need to employ\n          # atrous convolution with stride=1 and multiply the atrous rate by the\n          # current unit\'s stride for use in subsequent layers.\n          if output_stride is not None and current_stride == output_stride:\n            net = block.unit_fn(net,\n                                depth=unit_depth,\n                                depth_bottleneck=unit_depth_bottleneck,\n                                stride=1,\n                                rate=rate)\n            rate *= unit_stride\n\n          else:\n            net = block.unit_fn(net,\n                                depth=unit_depth,\n                                depth_bottleneck=unit_depth_bottleneck,\n                                stride=unit_stride,\n                                rate=1)\n            current_stride *= unit_stride\n      net = slim.utils.collect_named_outputs(outputs_collections, sc.name, net)\n\n  if output_stride is not None and current_stride != output_stride:\n    raise ValueError(\'The target output_stride cannot be reached.\')\n\n  return net\n\n\ndef resnet_arg_scope(weight_decay=0.0001,\n                     batch_norm_decay=0.997,\n                     batch_norm_epsilon=1e-5,\n                     batch_norm_scale=True):\n  """"""Defines the default ResNet arg scope.\n\n  TODO(gpapan): The batch-normalization related default values above are\n    appropriate for use in conjunction with the reference ResNet models\n    released at https://github.com/KaimingHe/deep-residual-networks. When\n    training ResNets from scratch, they might need to be tuned.\n\n  Args:\n    weight_decay: The weight decay to use for regularizing the model.\n    batch_norm_decay: The moving average decay when estimating layer activation\n      statistics in batch normalization.\n    batch_norm_epsilon: Small constant to prevent division by zero when\n      normalizing activations by their variance in batch normalization.\n    batch_norm_scale: If True, uses an explicit `gamma` multiplier to scale the\n      activations in the batch normalization layer.\n\n  Returns:\n    An `arg_scope` to use for the resnet models.\n  """"""\n  batch_norm_params = {\n      \'decay\': batch_norm_decay,\n      \'epsilon\': batch_norm_epsilon,\n      \'scale\': batch_norm_scale,\n      \'updates_collections\': tf.GraphKeys.UPDATE_OPS,\n  }\n\n  with slim.arg_scope(\n      [slim.conv2d],\n      weights_regularizer=slim.l2_regularizer(weight_decay),\n      weights_initializer=slim.variance_scaling_initializer(),\n      activation_fn=tf.nn.relu,\n      normalizer_fn=slim.batch_norm,\n      normalizer_params=batch_norm_params):\n    with slim.arg_scope([slim.batch_norm], **batch_norm_params):\n      # The following implies padding=\'SAME\' for pool1, which makes feature\n      # alignment easier for dense prediction tasks. This is also used in\n      # https://github.com/facebook/fb.resnet.torch. However the accompanying\n      # code of \'Deep Residual Learning for Image Recognition\' uses\n      # padding=\'VALID\' for pool1. You can switch to that choice by setting\n      # slim.arg_scope([slim.max_pool2d], padding=\'VALID\').\n      with slim.arg_scope([slim.max_pool2d], padding=\'SAME\') as arg_sc:\n        return arg_sc\n'"
finetuning/nets/resnet_v1.py,5,"b'# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Contains definitions for the original form of Residual Networks.\n\nThe \'v1\' residual networks (ResNets) implemented in this module were proposed\nby:\n[1] Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun\n    Deep Residual Learning for Image Recognition. arXiv:1512.03385\n\nOther variants were introduced in:\n[2] Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun\n    Identity Mappings in Deep Residual Networks. arXiv: 1603.05027\n\nThe networks defined in this module utilize the bottleneck building block of\n[1] with projection shortcuts only for increasing depths. They employ batch\nnormalization *after* every weight layer. This is the architecture used by\nMSRA in the Imagenet and MSCOCO 2016 competition models ResNet-101 and\nResNet-152. See [2; Fig. 1a] for a comparison between the current \'v1\'\narchitecture and the alternative \'v2\' architecture of [2] which uses batch\nnormalization *before* every weight layer in the so-called full pre-activation\nunits.\n\nTypical use:\n\n   from tensorflow.contrib.slim.nets import resnet_v1\n\nResNet-101 for image classification into 1000 classes:\n\n   # inputs has shape [batch, 224, 224, 3]\n   with slim.arg_scope(resnet_v1.resnet_arg_scope()):\n      net, end_points = resnet_v1.resnet_v1_101(inputs, 1000, is_training=False)\n\nResNet-101 for semantic segmentation into 21 classes:\n\n   # inputs has shape [batch, 513, 513, 3]\n   with slim.arg_scope(resnet_v1.resnet_arg_scope()):\n      net, end_points = resnet_v1.resnet_v1_101(inputs,\n                                                21,\n                                                is_training=False,\n                                                global_pool=False,\n                                                output_stride=16)\n""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\n\nfrom nets import resnet_utils\n\n\nresnet_arg_scope = resnet_utils.resnet_arg_scope\nslim = tf.contrib.slim\n\n\n@slim.add_arg_scope\ndef bottleneck(inputs, depth, depth_bottleneck, stride, rate=1,\n               outputs_collections=None, scope=None):\n  """"""Bottleneck residual unit variant with BN after convolutions.\n\n  This is the original residual unit proposed in [1]. See Fig. 1(a) of [2] for\n  its definition. Note that we use here the bottleneck variant which has an\n  extra bottleneck layer.\n\n  When putting together two consecutive ResNet blocks that use this unit, one\n  should use stride = 2 in the last unit of the first block.\n\n  Args:\n    inputs: A tensor of size [batch, height, width, channels].\n    depth: The depth of the ResNet unit output.\n    depth_bottleneck: The depth of the bottleneck layers.\n    stride: The ResNet unit\'s stride. Determines the amount of downsampling of\n      the units output compared to its input.\n    rate: An integer, rate for atrous convolution.\n    outputs_collections: Collection to add the ResNet unit output.\n    scope: Optional variable_scope.\n\n  Returns:\n    The ResNet unit\'s output.\n  """"""\n  with tf.variable_scope(scope, \'bottleneck_v1\', [inputs]) as sc:\n    depth_in = slim.utils.last_dimension(inputs.get_shape(), min_rank=4)\n    if depth == depth_in:\n      shortcut = resnet_utils.subsample(inputs, stride, \'shortcut\')\n    else:\n      shortcut = slim.conv2d(inputs, depth, [1, 1], stride=stride,\n                             activation_fn=None, scope=\'shortcut\')\n\n    residual = slim.conv2d(inputs, depth_bottleneck, [1, 1], stride=1,\n                           scope=\'conv1\')\n    residual = resnet_utils.conv2d_same(residual, depth_bottleneck, 3, stride,\n                                        rate=rate, scope=\'conv2\')\n    residual = slim.conv2d(residual, depth, [1, 1], stride=1,\n                           activation_fn=None, scope=\'conv3\')\n\n    output = tf.nn.relu(shortcut + residual)\n\n    return slim.utils.collect_named_outputs(outputs_collections,\n                                            sc.original_name_scope,\n                                            output)\n\n\ndef resnet_v1(inputs,\n              blocks,\n              num_classes=None,\n              is_training=True,\n              global_pool=True,\n              output_stride=None,\n              include_root_block=True,\n              reuse=None,\n              scope=None):\n  """"""Generator for v1 ResNet models.\n\n  This function generates a family of ResNet v1 models. See the resnet_v1_*()\n  methods for specific model instantiations, obtained by selecting different\n  block instantiations that produce ResNets of various depths.\n\n  Training for image classification on Imagenet is usually done with [224, 224]\n  inputs, resulting in [7, 7] feature maps at the output of the last ResNet\n  block for the ResNets defined in [1] that have nominal stride equal to 32.\n  However, for dense prediction tasks we advise that one uses inputs with\n  spatial dimensions that are multiples of 32 plus 1, e.g., [321, 321]. In\n  this case the feature maps at the ResNet output will have spatial shape\n  [(height - 1) / output_stride + 1, (width - 1) / output_stride + 1]\n  and corners exactly aligned with the input image corners, which greatly\n  facilitates alignment of the features to the image. Using as input [225, 225]\n  images results in [8, 8] feature maps at the output of the last ResNet block.\n\n  For dense prediction tasks, the ResNet needs to run in fully-convolutional\n  (FCN) mode and global_pool needs to be set to False. The ResNets in [1, 2] all\n  have nominal stride equal to 32 and a good choice in FCN mode is to use\n  output_stride=16 in order to increase the density of the computed features at\n  small computational and memory overhead, cf. http://arxiv.org/abs/1606.00915.\n\n  Args:\n    inputs: A tensor of size [batch, height_in, width_in, channels].\n    blocks: A list of length equal to the number of ResNet blocks. Each element\n      is a resnet_utils.Block object describing the units in the block.\n    num_classes: Number of predicted classes for classification tasks. If None\n      we return the features before the logit layer.\n    is_training: whether is training or not.\n    global_pool: If True, we perform global average pooling before computing the\n      logits. Set to True for image classification, False for dense prediction.\n    output_stride: If None, then the output will be computed at the nominal\n      network stride. If output_stride is not None, it specifies the requested\n      ratio of input to output spatial resolution.\n    include_root_block: If True, include the initial convolution followed by\n      max-pooling, if False excludes it.\n    reuse: whether or not the network and its variables should be reused. To be\n      able to reuse \'scope\' must be given.\n    scope: Optional variable_scope.\n\n  Returns:\n    net: A rank-4 tensor of size [batch, height_out, width_out, channels_out].\n      If global_pool is False, then height_out and width_out are reduced by a\n      factor of output_stride compared to the respective height_in and width_in,\n      else both height_out and width_out equal one. If num_classes is None, then\n      net is the output of the last ResNet block, potentially after global\n      average pooling. If num_classes is not None, net contains the pre-softmax\n      activations.\n    end_points: A dictionary from components of the network to the corresponding\n      activation.\n\n  Raises:\n    ValueError: If the target output_stride is not valid.\n  """"""\n  with tf.variable_scope(scope, \'resnet_v1\', [inputs], reuse=reuse) as sc:\n    end_points_collection = sc.name + \'_end_points\'\n    with slim.arg_scope([slim.conv2d, bottleneck,\n                         resnet_utils.stack_blocks_dense],\n                        outputs_collections=end_points_collection):\n      with slim.arg_scope([slim.batch_norm], is_training=is_training):\n        net = inputs\n        if include_root_block:\n          if output_stride is not None:\n            if output_stride % 4 != 0:\n              raise ValueError(\'The output_stride needs to be a multiple of 4.\')\n            output_stride /= 4\n          net = resnet_utils.conv2d_same(net, 64, 7, stride=2, scope=\'conv1\')\n          net = slim.max_pool2d(net, [3, 3], stride=2, scope=\'pool1\')\n        net = resnet_utils.stack_blocks_dense(net, blocks, output_stride)\n        if global_pool:\n          # Global average pooling.\n          net = tf.reduce_mean(net, [1, 2], name=\'pool5\', keep_dims=True)\n        if num_classes is not None:\n          net = slim.conv2d(net, num_classes, [1, 1], activation_fn=None,\n                            normalizer_fn=None, scope=\'logits\')\n        # Convert end_points_collection into a dictionary of end_points.\n        end_points = slim.utils.convert_collection_to_dict(end_points_collection)\n        if num_classes is not None:\n          end_points[\'predictions\'] = slim.softmax(net, scope=\'predictions\')\n        return net, end_points\nresnet_v1.default_image_size = 224\n\n\ndef resnet_v1_50(inputs,\n                 num_classes=None,\n                 is_training=True,\n                 global_pool=True,\n                 output_stride=None,\n                 reuse=None,\n                 scope=\'resnet_v1_50\'):\n  """"""ResNet-50 model of [1]. See resnet_v1() for arg and return description.""""""\n  blocks = [\n      resnet_utils.Block(\n          \'block1\', bottleneck, [(256, 64, 1)] * 2 + [(256, 64, 2)]),\n      resnet_utils.Block(\n          \'block2\', bottleneck, [(512, 128, 1)] * 3 + [(512, 128, 2)]),\n      resnet_utils.Block(\n          \'block3\', bottleneck, [(1024, 256, 1)] * 5 + [(1024, 256, 2)]),\n      resnet_utils.Block(\n          \'block4\', bottleneck, [(2048, 512, 1)] * 3)\n  ]\n  return resnet_v1(inputs, blocks, num_classes, is_training,\n                   global_pool=global_pool, output_stride=output_stride,\n                   include_root_block=True, reuse=reuse, scope=scope)\n\n\ndef resnet_v1_101(inputs,\n                  num_classes=None,\n                  is_training=True,\n                  global_pool=True,\n                  output_stride=None,\n                  reuse=None,\n                  scope=\'resnet_v1_101\'):\n  """"""ResNet-101 model of [1]. See resnet_v1() for arg and return description.""""""\n  blocks = [\n      resnet_utils.Block(\n          \'block1\', bottleneck, [(256, 64, 1)] * 2 + [(256, 64, 2)]),\n      resnet_utils.Block(\n          \'block2\', bottleneck, [(512, 128, 1)] * 3 + [(512, 128, 2)]),\n      resnet_utils.Block(\n          \'block3\', bottleneck, [(1024, 256, 1)] * 22 + [(1024, 256, 2)]),\n      resnet_utils.Block(\n          \'block4\', bottleneck, [(2048, 512, 1)] * 3)\n  ]\n  return resnet_v1(inputs, blocks, num_classes, is_training,\n                   global_pool=global_pool, output_stride=output_stride,\n                   include_root_block=True, reuse=reuse, scope=scope)\n\n\ndef resnet_v1_152(inputs,\n                  num_classes=None,\n                  is_training=True,\n                  global_pool=True,\n                  output_stride=None,\n                  reuse=None,\n                  scope=\'resnet_v1_152\'):\n  """"""ResNet-152 model of [1]. See resnet_v1() for arg and return description.""""""\n  blocks = [\n      resnet_utils.Block(\n          \'block1\', bottleneck, [(256, 64, 1)] * 2 + [(256, 64, 2)]),\n      resnet_utils.Block(\n          \'block2\', bottleneck, [(512, 128, 1)] * 7 + [(512, 128, 2)]),\n      resnet_utils.Block(\n          \'block3\', bottleneck, [(1024, 256, 1)] * 35 + [(1024, 256, 2)]),\n      resnet_utils.Block(\n          \'block4\', bottleneck, [(2048, 512, 1)] * 3)]\n  return resnet_v1(inputs, blocks, num_classes, is_training,\n                   global_pool=global_pool, output_stride=output_stride,\n                   include_root_block=True, reuse=reuse, scope=scope)\n\n\ndef resnet_v1_200(inputs,\n                  num_classes=None,\n                  is_training=True,\n                  global_pool=True,\n                  output_stride=None,\n                  reuse=None,\n                  scope=\'resnet_v1_200\'):\n  """"""ResNet-200 model of [2]. See resnet_v1() for arg and return description.""""""\n  blocks = [\n      resnet_utils.Block(\n          \'block1\', bottleneck, [(256, 64, 1)] * 2 + [(256, 64, 2)]),\n      resnet_utils.Block(\n          \'block2\', bottleneck, [(512, 128, 1)] * 23 + [(512, 128, 2)]),\n      resnet_utils.Block(\n          \'block3\', bottleneck, [(1024, 256, 1)] * 35 + [(1024, 256, 2)]),\n      resnet_utils.Block(\n          \'block4\', bottleneck, [(2048, 512, 1)] * 3)]\n  return resnet_v1(inputs, blocks, num_classes, is_training,\n                   global_pool=global_pool, output_stride=output_stride,\n                   include_root_block=True, reuse=reuse, scope=scope)\n'"
finetuning/nets/resnet_v1_test.py,52,"b'# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Tests for slim.nets.resnet_v1.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport numpy as np\nimport tensorflow as tf\n\nfrom nets import resnet_utils\nfrom nets import resnet_v1\n\nslim = tf.contrib.slim\n\n\ndef create_test_input(batch_size, height, width, channels):\n  """"""Create test input tensor.\n\n  Args:\n    batch_size: The number of images per batch or `None` if unknown.\n    height: The height of each image or `None` if unknown.\n    width: The width of each image or `None` if unknown.\n    channels: The number of channels per image or `None` if unknown.\n\n  Returns:\n    Either a placeholder `Tensor` of dimension\n      [batch_size, height, width, channels] if any of the inputs are `None` or a\n    constant `Tensor` with the mesh grid values along the spatial dimensions.\n  """"""\n  if None in [batch_size, height, width, channels]:\n    return tf.placeholder(tf.float32, (batch_size, height, width, channels))\n  else:\n    return tf.to_float(\n        np.tile(\n            np.reshape(\n                np.reshape(np.arange(height), [height, 1]) +\n                np.reshape(np.arange(width), [1, width]),\n                [1, height, width, 1]),\n            [batch_size, 1, 1, channels]))\n\n\nclass ResnetUtilsTest(tf.test.TestCase):\n\n  def testSubsampleThreeByThree(self):\n    x = tf.reshape(tf.to_float(tf.range(9)), [1, 3, 3, 1])\n    x = resnet_utils.subsample(x, 2)\n    expected = tf.reshape(tf.constant([0, 2, 6, 8]), [1, 2, 2, 1])\n    with self.test_session():\n      self.assertAllClose(x.eval(), expected.eval())\n\n  def testSubsampleFourByFour(self):\n    x = tf.reshape(tf.to_float(tf.range(16)), [1, 4, 4, 1])\n    x = resnet_utils.subsample(x, 2)\n    expected = tf.reshape(tf.constant([0, 2, 8, 10]), [1, 2, 2, 1])\n    with self.test_session():\n      self.assertAllClose(x.eval(), expected.eval())\n\n  def testConv2DSameEven(self):\n    n, n2 = 4, 2\n\n    # Input image.\n    x = create_test_input(1, n, n, 1)\n\n    # Convolution kernel.\n    w = create_test_input(1, 3, 3, 1)\n    w = tf.reshape(w, [3, 3, 1, 1])\n\n    tf.get_variable(\'Conv/weights\', initializer=w)\n    tf.get_variable(\'Conv/biases\', initializer=tf.zeros([1]))\n    tf.get_variable_scope().reuse_variables()\n\n    y1 = slim.conv2d(x, 1, [3, 3], stride=1, scope=\'Conv\')\n    y1_expected = tf.to_float([[14, 28, 43, 26],\n                               [28, 48, 66, 37],\n                               [43, 66, 84, 46],\n                               [26, 37, 46, 22]])\n    y1_expected = tf.reshape(y1_expected, [1, n, n, 1])\n\n    y2 = resnet_utils.subsample(y1, 2)\n    y2_expected = tf.to_float([[14, 43],\n                               [43, 84]])\n    y2_expected = tf.reshape(y2_expected, [1, n2, n2, 1])\n\n    y3 = resnet_utils.conv2d_same(x, 1, 3, stride=2, scope=\'Conv\')\n    y3_expected = y2_expected\n\n    y4 = slim.conv2d(x, 1, [3, 3], stride=2, scope=\'Conv\')\n    y4_expected = tf.to_float([[48, 37],\n                               [37, 22]])\n    y4_expected = tf.reshape(y4_expected, [1, n2, n2, 1])\n\n    with self.test_session() as sess:\n      try:\n        sess.run(tf.global_variables_initializer())\n      except AttributeError:\n        sess.run(tf.initialize_all_variables())\n      self.assertAllClose(y1.eval(), y1_expected.eval())\n      self.assertAllClose(y2.eval(), y2_expected.eval())\n      self.assertAllClose(y3.eval(), y3_expected.eval())\n      self.assertAllClose(y4.eval(), y4_expected.eval())\n\n  def testConv2DSameOdd(self):\n    n, n2 = 5, 3\n\n    # Input image.\n    x = create_test_input(1, n, n, 1)\n\n    # Convolution kernel.\n    w = create_test_input(1, 3, 3, 1)\n    w = tf.reshape(w, [3, 3, 1, 1])\n\n    tf.get_variable(\'Conv/weights\', initializer=w)\n    tf.get_variable(\'Conv/biases\', initializer=tf.zeros([1]))\n    tf.get_variable_scope().reuse_variables()\n\n    y1 = slim.conv2d(x, 1, [3, 3], stride=1, scope=\'Conv\')\n    y1_expected = tf.to_float([[14, 28, 43, 58, 34],\n                               [28, 48, 66, 84, 46],\n                               [43, 66, 84, 102, 55],\n                               [58, 84, 102, 120, 64],\n                               [34, 46, 55, 64, 30]])\n    y1_expected = tf.reshape(y1_expected, [1, n, n, 1])\n\n    y2 = resnet_utils.subsample(y1, 2)\n    y2_expected = tf.to_float([[14, 43, 34],\n                               [43, 84, 55],\n                               [34, 55, 30]])\n    y2_expected = tf.reshape(y2_expected, [1, n2, n2, 1])\n\n    y3 = resnet_utils.conv2d_same(x, 1, 3, stride=2, scope=\'Conv\')\n    y3_expected = y2_expected\n\n    y4 = slim.conv2d(x, 1, [3, 3], stride=2, scope=\'Conv\')\n    y4_expected = y2_expected\n\n    with self.test_session() as sess:\n      try:\n        sess.run(tf.global_variables_initializer())\n      except AttributeError:\n        sess.run(tf.initialize_all_variables())\n      self.assertAllClose(y1.eval(), y1_expected.eval())\n      self.assertAllClose(y2.eval(), y2_expected.eval())\n      self.assertAllClose(y3.eval(), y3_expected.eval())\n      self.assertAllClose(y4.eval(), y4_expected.eval())\n\n  def _resnet_plain(self, inputs, blocks, output_stride=None, scope=None):\n    """"""A plain ResNet without extra layers before or after the ResNet blocks.""""""\n    with tf.variable_scope(scope, values=[inputs]):\n      with slim.arg_scope([slim.conv2d], outputs_collections=\'end_points\'):\n        net = resnet_utils.stack_blocks_dense(inputs, blocks, output_stride)\n        end_points = dict(tf.get_collection(\'end_points\'))\n        return net, end_points\n\n  def testEndPointsV1(self):\n    """"""Test the end points of a tiny v1 bottleneck network.""""""\n    bottleneck = resnet_v1.bottleneck\n    blocks = [resnet_utils.Block(\'block1\', bottleneck, [(4, 1, 1), (4, 1, 2)]),\n              resnet_utils.Block(\'block2\', bottleneck, [(8, 2, 1), (8, 2, 1)])]\n    inputs = create_test_input(2, 32, 16, 3)\n    with slim.arg_scope(resnet_utils.resnet_arg_scope()):\n      _, end_points = self._resnet_plain(inputs, blocks, scope=\'tiny\')\n    expected = [\n        \'tiny/block1/unit_1/bottleneck_v1/shortcut\',\n        \'tiny/block1/unit_1/bottleneck_v1/conv1\',\n        \'tiny/block1/unit_1/bottleneck_v1/conv2\',\n        \'tiny/block1/unit_1/bottleneck_v1/conv3\',\n        \'tiny/block1/unit_2/bottleneck_v1/conv1\',\n        \'tiny/block1/unit_2/bottleneck_v1/conv2\',\n        \'tiny/block1/unit_2/bottleneck_v1/conv3\',\n        \'tiny/block2/unit_1/bottleneck_v1/shortcut\',\n        \'tiny/block2/unit_1/bottleneck_v1/conv1\',\n        \'tiny/block2/unit_1/bottleneck_v1/conv2\',\n        \'tiny/block2/unit_1/bottleneck_v1/conv3\',\n        \'tiny/block2/unit_2/bottleneck_v1/conv1\',\n        \'tiny/block2/unit_2/bottleneck_v1/conv2\',\n        \'tiny/block2/unit_2/bottleneck_v1/conv3\']\n    self.assertItemsEqual(expected, end_points)\n\n  def _stack_blocks_nondense(self, net, blocks):\n    """"""A simplified ResNet Block stacker without output stride control.""""""\n    for block in blocks:\n      with tf.variable_scope(block.scope, \'block\', [net]):\n        for i, unit in enumerate(block.args):\n          depth, depth_bottleneck, stride = unit\n          with tf.variable_scope(\'unit_%d\' % (i + 1), values=[net]):\n            net = block.unit_fn(net,\n                                depth=depth,\n                                depth_bottleneck=depth_bottleneck,\n                                stride=stride,\n                                rate=1)\n    return net\n\n  def _atrousValues(self, bottleneck):\n    """"""Verify the values of dense feature extraction by atrous convolution.\n\n    Make sure that dense feature extraction by stack_blocks_dense() followed by\n    subsampling gives identical results to feature extraction at the nominal\n    network output stride using the simple self._stack_blocks_nondense() above.\n\n    Args:\n      bottleneck: The bottleneck function.\n    """"""\n    blocks = [\n        resnet_utils.Block(\'block1\', bottleneck, [(4, 1, 1), (4, 1, 2)]),\n        resnet_utils.Block(\'block2\', bottleneck, [(8, 2, 1), (8, 2, 2)]),\n        resnet_utils.Block(\'block3\', bottleneck, [(16, 4, 1), (16, 4, 2)]),\n        resnet_utils.Block(\'block4\', bottleneck, [(32, 8, 1), (32, 8, 1)])\n    ]\n    nominal_stride = 8\n\n    # Test both odd and even input dimensions.\n    height = 30\n    width = 31\n    with slim.arg_scope(resnet_utils.resnet_arg_scope()):\n      with slim.arg_scope([slim.batch_norm], is_training=False):\n        for output_stride in [1, 2, 4, 8, None]:\n          with tf.Graph().as_default():\n            with self.test_session() as sess:\n              tf.set_random_seed(0)\n              inputs = create_test_input(1, height, width, 3)\n              # Dense feature extraction followed by subsampling.\n              output = resnet_utils.stack_blocks_dense(inputs,\n                                                       blocks,\n                                                       output_stride)\n              if output_stride is None:\n                factor = 1\n              else:\n                factor = nominal_stride // output_stride\n\n              output = resnet_utils.subsample(output, factor)\n              # Make the two networks use the same weights.\n              tf.get_variable_scope().reuse_variables()\n              # Feature extraction at the nominal network rate.\n              expected = self._stack_blocks_nondense(inputs, blocks)\n              try:\n                sess.run(tf.global_variables_initializer())\n              except AttributeError:\n                sess.run(tf.initialize_all_variables())\n              output, expected = sess.run([output, expected])\n              self.assertAllClose(output, expected, atol=1e-4, rtol=1e-4)\n\n  def testAtrousValuesBottleneck(self):\n    self._atrousValues(resnet_v1.bottleneck)\n\n\nclass ResnetCompleteNetworkTest(tf.test.TestCase):\n  """"""Tests with complete small ResNet v1 networks.""""""\n\n  def _resnet_small(self,\n                    inputs,\n                    num_classes=None,\n                    is_training=True,\n                    global_pool=True,\n                    output_stride=None,\n                    include_root_block=True,\n                    reuse=None,\n                    scope=\'resnet_v1_small\'):\n    """"""A shallow and thin ResNet v1 for faster tests.""""""\n    bottleneck = resnet_v1.bottleneck\n    blocks = [\n        resnet_utils.Block(\n            \'block1\', bottleneck, [(4, 1, 1)] * 2 + [(4, 1, 2)]),\n        resnet_utils.Block(\n            \'block2\', bottleneck, [(8, 2, 1)] * 2 + [(8, 2, 2)]),\n        resnet_utils.Block(\n            \'block3\', bottleneck, [(16, 4, 1)] * 2 + [(16, 4, 2)]),\n        resnet_utils.Block(\n            \'block4\', bottleneck, [(32, 8, 1)] * 2)]\n    return resnet_v1.resnet_v1(inputs, blocks, num_classes,\n                               is_training=is_training,\n                               global_pool=global_pool,\n                               output_stride=output_stride,\n                               include_root_block=include_root_block,\n                               reuse=reuse,\n                               scope=scope)\n\n  def testClassificationEndPoints(self):\n    global_pool = True\n    num_classes = 10\n    inputs = create_test_input(2, 224, 224, 3)\n    with slim.arg_scope(resnet_utils.resnet_arg_scope()):\n      logits, end_points = self._resnet_small(inputs, num_classes,\n                                              global_pool=global_pool,\n                                              scope=\'resnet\')\n    self.assertTrue(logits.op.name.startswith(\'resnet/logits\'))\n    self.assertListEqual(logits.get_shape().as_list(), [2, 1, 1, num_classes])\n    self.assertTrue(\'predictions\' in end_points)\n    self.assertListEqual(end_points[\'predictions\'].get_shape().as_list(),\n                         [2, 1, 1, num_classes])\n\n  def testClassificationShapes(self):\n    global_pool = True\n    num_classes = 10\n    inputs = create_test_input(2, 224, 224, 3)\n    with slim.arg_scope(resnet_utils.resnet_arg_scope()):\n      _, end_points = self._resnet_small(inputs, num_classes,\n                                         global_pool=global_pool,\n                                         scope=\'resnet\')\n      endpoint_to_shape = {\n          \'resnet/block1\': [2, 28, 28, 4],\n          \'resnet/block2\': [2, 14, 14, 8],\n          \'resnet/block3\': [2, 7, 7, 16],\n          \'resnet/block4\': [2, 7, 7, 32]}\n      for endpoint in endpoint_to_shape:\n        shape = endpoint_to_shape[endpoint]\n        self.assertListEqual(end_points[endpoint].get_shape().as_list(), shape)\n\n  def testFullyConvolutionalEndpointShapes(self):\n    global_pool = False\n    num_classes = 10\n    inputs = create_test_input(2, 321, 321, 3)\n    with slim.arg_scope(resnet_utils.resnet_arg_scope()):\n      _, end_points = self._resnet_small(inputs, num_classes,\n                                         global_pool=global_pool,\n                                         scope=\'resnet\')\n      endpoint_to_shape = {\n          \'resnet/block1\': [2, 41, 41, 4],\n          \'resnet/block2\': [2, 21, 21, 8],\n          \'resnet/block3\': [2, 11, 11, 16],\n          \'resnet/block4\': [2, 11, 11, 32]}\n      for endpoint in endpoint_to_shape:\n        shape = endpoint_to_shape[endpoint]\n        self.assertListEqual(end_points[endpoint].get_shape().as_list(), shape)\n\n  def testRootlessFullyConvolutionalEndpointShapes(self):\n    global_pool = False\n    num_classes = 10\n    inputs = create_test_input(2, 128, 128, 3)\n    with slim.arg_scope(resnet_utils.resnet_arg_scope()):\n      _, end_points = self._resnet_small(inputs, num_classes,\n                                         global_pool=global_pool,\n                                         include_root_block=False,\n                                         scope=\'resnet\')\n      endpoint_to_shape = {\n          \'resnet/block1\': [2, 64, 64, 4],\n          \'resnet/block2\': [2, 32, 32, 8],\n          \'resnet/block3\': [2, 16, 16, 16],\n          \'resnet/block4\': [2, 16, 16, 32]}\n      for endpoint in endpoint_to_shape:\n        shape = endpoint_to_shape[endpoint]\n        self.assertListEqual(end_points[endpoint].get_shape().as_list(), shape)\n\n  def testAtrousFullyConvolutionalEndpointShapes(self):\n    global_pool = False\n    num_classes = 10\n    output_stride = 8\n    inputs = create_test_input(2, 321, 321, 3)\n    with slim.arg_scope(resnet_utils.resnet_arg_scope()):\n      _, end_points = self._resnet_small(inputs,\n                                         num_classes,\n                                         global_pool=global_pool,\n                                         output_stride=output_stride,\n                                         scope=\'resnet\')\n      endpoint_to_shape = {\n          \'resnet/block1\': [2, 41, 41, 4],\n          \'resnet/block2\': [2, 41, 41, 8],\n          \'resnet/block3\': [2, 41, 41, 16],\n          \'resnet/block4\': [2, 41, 41, 32]}\n      for endpoint in endpoint_to_shape:\n        shape = endpoint_to_shape[endpoint]\n        self.assertListEqual(end_points[endpoint].get_shape().as_list(), shape)\n\n  def testAtrousFullyConvolutionalValues(self):\n    """"""Verify dense feature extraction with atrous convolution.""""""\n    nominal_stride = 32\n    for output_stride in [4, 8, 16, 32, None]:\n      with slim.arg_scope(resnet_utils.resnet_arg_scope()):\n        with tf.Graph().as_default():\n          with self.test_session() as sess:\n            tf.set_random_seed(0)\n            inputs = create_test_input(2, 81, 81, 3)\n            # Dense feature extraction followed by subsampling.\n            output, _ = self._resnet_small(inputs, None, is_training=False,\n                                           global_pool=False,\n                                           output_stride=output_stride)\n            if output_stride is None:\n              factor = 1\n            else:\n              factor = nominal_stride // output_stride\n            output = resnet_utils.subsample(output, factor)\n            # Make the two networks use the same weights.\n            tf.get_variable_scope().reuse_variables()\n            # Feature extraction at the nominal network rate.\n            expected, _ = self._resnet_small(inputs, None, is_training=False,\n                                             global_pool=False)\n            try:\n              sess.run(tf.global_variables_initializer())\n            except AttributeError:\n              sess.run(tf.initialize_all_variables())\n            self.assertAllClose(output.eval(), expected.eval(),\n                                atol=1e-4, rtol=1e-4)\n\n  def testUnknownBatchSize(self):\n    batch = 2\n    height, width = 65, 65\n    global_pool = True\n    num_classes = 10\n    inputs = create_test_input(None, height, width, 3)\n    with slim.arg_scope(resnet_utils.resnet_arg_scope()):\n      logits, _ = self._resnet_small(inputs, num_classes,\n                                     global_pool=global_pool,\n                                     scope=\'resnet\')\n    self.assertTrue(logits.op.name.startswith(\'resnet/logits\'))\n    self.assertListEqual(logits.get_shape().as_list(),\n                         [None, 1, 1, num_classes])\n    images = create_test_input(batch, height, width, 3)\n    with self.test_session() as sess:\n      try:\n        sess.run(tf.global_variables_initializer())\n      except AttributeError:\n        sess.run(tf.initialize_all_variables())\n      output = sess.run(logits, {inputs: images.eval()})\n      self.assertEqual(output.shape, (batch, 1, 1, num_classes))\n\n  def testFullyConvolutionalUnknownHeightWidth(self):\n    batch = 2\n    height, width = 65, 65\n    global_pool = False\n    inputs = create_test_input(batch, None, None, 3)\n    with slim.arg_scope(resnet_utils.resnet_arg_scope()):\n      output, _ = self._resnet_small(inputs, None, global_pool=global_pool)\n    self.assertListEqual(output.get_shape().as_list(),\n                         [batch, None, None, 32])\n    images = create_test_input(batch, height, width, 3)\n    with self.test_session() as sess:\n      try:\n        sess.run(tf.global_variables_initializer())\n      except AttributeError:\n        sess.run(tf.initialize_all_variables())\n      output = sess.run(output, {inputs: images.eval()})\n      self.assertEqual(output.shape, (batch, 3, 3, 32))\n\n  def testAtrousFullyConvolutionalUnknownHeightWidth(self):\n    batch = 2\n    height, width = 65, 65\n    global_pool = False\n    output_stride = 8\n    inputs = create_test_input(batch, None, None, 3)\n    with slim.arg_scope(resnet_utils.resnet_arg_scope()):\n      output, _ = self._resnet_small(inputs,\n                                     None,\n                                     global_pool=global_pool,\n                                     output_stride=output_stride)\n    self.assertListEqual(output.get_shape().as_list(),\n                         [batch, None, None, 32])\n    images = create_test_input(batch, height, width, 3)\n    with self.test_session() as sess:\n      try:\n        sess.run(tf.global_variables_initializer())\n      except AttributeError:\n        sess.run(tf.initialize_all_variables())\n      output = sess.run(output, {inputs: images.eval()})\n      self.assertEqual(output.shape, (batch, 9, 9, 32))\n\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
finetuning/nets/resnet_v2.py,6,"b'# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Contains definitions for the preactivation form of Residual Networks.\n\nResidual networks (ResNets) were originally proposed in:\n[1] Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun\n    Deep Residual Learning for Image Recognition. arXiv:1512.03385\n\nThe full preactivation \'v2\' ResNet variant implemented in this module was\nintroduced by:\n[2] Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun\n    Identity Mappings in Deep Residual Networks. arXiv: 1603.05027\n\nThe key difference of the full preactivation \'v2\' variant compared to the\n\'v1\' variant in [1] is the use of batch normalization before every weight layer.\nAnother difference is that \'v2\' ResNets do not include an activation function in\nthe main pathway. Also see [2; Fig. 4e].\n\nTypical use:\n\n   from tensorflow.contrib.slim.nets import resnet_v2\n\nResNet-101 for image classification into 1000 classes:\n\n   # inputs has shape [batch, 224, 224, 3]\n   with slim.arg_scope(resnet_v2.resnet_arg_scope()):\n      net, end_points = resnet_v2.resnet_v2_101(inputs, 1000, is_training=False)\n\nResNet-101 for semantic segmentation into 21 classes:\n\n   # inputs has shape [batch, 513, 513, 3]\n   with slim.arg_scope(resnet_v2.resnet_arg_scope(is_training)):\n      net, end_points = resnet_v2.resnet_v2_101(inputs,\n                                                21,\n                                                is_training=False,\n                                                global_pool=False,\n                                                output_stride=16)\n""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\n\nfrom nets import resnet_utils\n\nslim = tf.contrib.slim\nresnet_arg_scope = resnet_utils.resnet_arg_scope\n\n\n@slim.add_arg_scope\ndef bottleneck(inputs, depth, depth_bottleneck, stride, rate=1,\n               outputs_collections=None, scope=None):\n  """"""Bottleneck residual unit variant with BN before convolutions.\n\n  This is the full preactivation residual unit variant proposed in [2]. See\n  Fig. 1(b) of [2] for its definition. Note that we use here the bottleneck\n  variant which has an extra bottleneck layer.\n\n  When putting together two consecutive ResNet blocks that use this unit, one\n  should use stride = 2 in the last unit of the first block.\n\n  Args:\n    inputs: A tensor of size [batch, height, width, channels].\n    depth: The depth of the ResNet unit output.\n    depth_bottleneck: The depth of the bottleneck layers.\n    stride: The ResNet unit\'s stride. Determines the amount of downsampling of\n      the units output compared to its input.\n    rate: An integer, rate for atrous convolution.\n    outputs_collections: Collection to add the ResNet unit output.\n    scope: Optional variable_scope.\n\n  Returns:\n    The ResNet unit\'s output.\n  """"""\n  with tf.variable_scope(scope, \'bottleneck_v2\', [inputs]) as sc:\n    depth_in = slim.utils.last_dimension(inputs.get_shape(), min_rank=4)\n    preact = slim.batch_norm(inputs, activation_fn=tf.nn.relu, scope=\'preact\')\n    if depth == depth_in:\n      shortcut = resnet_utils.subsample(inputs, stride, \'shortcut\')\n    else:\n      shortcut = slim.conv2d(preact, depth, [1, 1], stride=stride,\n                             normalizer_fn=None, activation_fn=None,\n                             scope=\'shortcut\')\n\n    residual = slim.conv2d(preact, depth_bottleneck, [1, 1], stride=1,\n                           scope=\'conv1\')\n    residual = resnet_utils.conv2d_same(residual, depth_bottleneck, 3, stride,\n                                        rate=rate, scope=\'conv2\')\n    residual = slim.conv2d(residual, depth, [1, 1], stride=1,\n                           normalizer_fn=None, activation_fn=None,\n                           scope=\'conv3\')\n\n    output = shortcut + residual\n\n    return slim.utils.collect_named_outputs(outputs_collections,\n                                            sc.original_name_scope,\n                                            output)\n\n\ndef resnet_v2(inputs,\n              blocks,\n              num_classes=None,\n              is_training=True,\n              global_pool=True,\n              output_stride=None,\n              include_root_block=True,\n              reuse=None,\n              scope=None):\n  """"""Generator for v2 (preactivation) ResNet models.\n\n  This function generates a family of ResNet v2 models. See the resnet_v2_*()\n  methods for specific model instantiations, obtained by selecting different\n  block instantiations that produce ResNets of various depths.\n\n  Training for image classification on Imagenet is usually done with [224, 224]\n  inputs, resulting in [7, 7] feature maps at the output of the last ResNet\n  block for the ResNets defined in [1] that have nominal stride equal to 32.\n  However, for dense prediction tasks we advise that one uses inputs with\n  spatial dimensions that are multiples of 32 plus 1, e.g., [321, 321]. In\n  this case the feature maps at the ResNet output will have spatial shape\n  [(height - 1) / output_stride + 1, (width - 1) / output_stride + 1]\n  and corners exactly aligned with the input image corners, which greatly\n  facilitates alignment of the features to the image. Using as input [225, 225]\n  images results in [8, 8] feature maps at the output of the last ResNet block.\n\n  For dense prediction tasks, the ResNet needs to run in fully-convolutional\n  (FCN) mode and global_pool needs to be set to False. The ResNets in [1, 2] all\n  have nominal stride equal to 32 and a good choice in FCN mode is to use\n  output_stride=16 in order to increase the density of the computed features at\n  small computational and memory overhead, cf. http://arxiv.org/abs/1606.00915.\n\n  Args:\n    inputs: A tensor of size [batch, height_in, width_in, channels].\n    blocks: A list of length equal to the number of ResNet blocks. Each element\n      is a resnet_utils.Block object describing the units in the block.\n    num_classes: Number of predicted classes for classification tasks. If None\n      we return the features before the logit layer.\n    is_training: whether is training or not.\n    global_pool: If True, we perform global average pooling before computing the\n      logits. Set to True for image classification, False for dense prediction.\n    output_stride: If None, then the output will be computed at the nominal\n      network stride. If output_stride is not None, it specifies the requested\n      ratio of input to output spatial resolution.\n    include_root_block: If True, include the initial convolution followed by\n      max-pooling, if False excludes it. If excluded, `inputs` should be the\n      results of an activation-less convolution.\n    reuse: whether or not the network and its variables should be reused. To be\n      able to reuse \'scope\' must be given.\n    scope: Optional variable_scope.\n\n\n  Returns:\n    net: A rank-4 tensor of size [batch, height_out, width_out, channels_out].\n      If global_pool is False, then height_out and width_out are reduced by a\n      factor of output_stride compared to the respective height_in and width_in,\n      else both height_out and width_out equal one. If num_classes is None, then\n      net is the output of the last ResNet block, potentially after global\n      average pooling. If num_classes is not None, net contains the pre-softmax\n      activations.\n    end_points: A dictionary from components of the network to the corresponding\n      activation.\n\n  Raises:\n    ValueError: If the target output_stride is not valid.\n  """"""\n  with tf.variable_scope(scope, \'resnet_v2\', [inputs], reuse=reuse) as sc:\n    end_points_collection = sc.name + \'_end_points\'\n    with slim.arg_scope([slim.conv2d, bottleneck,\n                         resnet_utils.stack_blocks_dense],\n                        outputs_collections=end_points_collection):\n      with slim.arg_scope([slim.batch_norm], is_training=is_training):\n        net = inputs\n        if include_root_block:\n          if output_stride is not None:\n            if output_stride % 4 != 0:\n              raise ValueError(\'The output_stride needs to be a multiple of 4.\')\n            output_stride /= 4\n          # We do not include batch normalization or activation functions in\n          # conv1 because the first ResNet unit will perform these. Cf.\n          # Appendix of [2].\n          with slim.arg_scope([slim.conv2d],\n                              activation_fn=None, normalizer_fn=None):\n            net = resnet_utils.conv2d_same(net, 64, 7, stride=2, scope=\'conv1\')\n          net = slim.max_pool2d(net, [3, 3], stride=2, scope=\'pool1\')\n        net = resnet_utils.stack_blocks_dense(net, blocks, output_stride)\n        # This is needed because the pre-activation variant does not have batch\n        # normalization or activation functions in the residual unit output. See\n        # Appendix of [2].\n        net = slim.batch_norm(net, activation_fn=tf.nn.relu, scope=\'postnorm\')\n        if global_pool:\n          # Global average pooling.\n          net = tf.reduce_mean(net, [1, 2], name=\'pool5\', keep_dims=True)\n        if num_classes is not None:\n          net = slim.conv2d(net, num_classes, [1, 1], activation_fn=None,\n                            normalizer_fn=None, scope=\'logits\')\n        # Convert end_points_collection into a dictionary of end_points.\n        end_points = slim.utils.convert_collection_to_dict(end_points_collection)\n        if num_classes is not None:\n          end_points[\'predictions\'] = slim.softmax(net, scope=\'predictions\')\n        return net, end_points\nresnet_v2.default_image_size = 224\n\n\ndef resnet_v2_50(inputs,\n                 num_classes=None,\n                 is_training=True,\n                 global_pool=True,\n                 output_stride=None,\n                 reuse=None,\n                 scope=\'resnet_v2_50\'):\n  """"""ResNet-50 model of [1]. See resnet_v2() for arg and return description.""""""\n  blocks = [\n      resnet_utils.Block(\n          \'block1\', bottleneck, [(256, 64, 1)] * 2 + [(256, 64, 2)]),\n      resnet_utils.Block(\n          \'block2\', bottleneck, [(512, 128, 1)] * 3 + [(512, 128, 2)]),\n      resnet_utils.Block(\n          \'block3\', bottleneck, [(1024, 256, 1)] * 5 + [(1024, 256, 2)]),\n      resnet_utils.Block(\n          \'block4\', bottleneck, [(2048, 512, 1)] * 3)]\n  return resnet_v2(inputs, blocks, num_classes, is_training=is_training,\n                   global_pool=global_pool, output_stride=output_stride,\n                   include_root_block=True, reuse=reuse, scope=scope)\n\n\ndef resnet_v2_101(inputs,\n                  num_classes=None,\n                  is_training=True,\n                  global_pool=True,\n                  output_stride=None,\n                  reuse=None,\n                  scope=\'resnet_v2_101\'):\n  """"""ResNet-101 model of [1]. See resnet_v2() for arg and return description.""""""\n  blocks = [\n      resnet_utils.Block(\n          \'block1\', bottleneck, [(256, 64, 1)] * 2 + [(256, 64, 2)]),\n      resnet_utils.Block(\n          \'block2\', bottleneck, [(512, 128, 1)] * 3 + [(512, 128, 2)]),\n      resnet_utils.Block(\n          \'block3\', bottleneck, [(1024, 256, 1)] * 22 + [(1024, 256, 2)]),\n      resnet_utils.Block(\n          \'block4\', bottleneck, [(2048, 512, 1)] * 3)]\n  return resnet_v2(inputs, blocks, num_classes, is_training=is_training,\n                   global_pool=global_pool, output_stride=output_stride,\n                   include_root_block=True, reuse=reuse, scope=scope)\n\n\ndef resnet_v2_152(inputs,\n                  num_classes=None,\n                  is_training=True,\n                  global_pool=True,\n                  output_stride=None,\n                  reuse=None,\n                  scope=\'resnet_v2_152\'):\n  """"""ResNet-152 model of [1]. See resnet_v2() for arg and return description.""""""\n  blocks = [\n      resnet_utils.Block(\n          \'block1\', bottleneck, [(256, 64, 1)] * 2 + [(256, 64, 2)]),\n      resnet_utils.Block(\n          \'block2\', bottleneck, [(512, 128, 1)] * 7 + [(512, 128, 2)]),\n      resnet_utils.Block(\n          \'block3\', bottleneck, [(1024, 256, 1)] * 35 + [(1024, 256, 2)]),\n      resnet_utils.Block(\n          \'block4\', bottleneck, [(2048, 512, 1)] * 3)]\n  return resnet_v2(inputs, blocks, num_classes, is_training=is_training,\n                   global_pool=global_pool, output_stride=output_stride,\n                   include_root_block=True, reuse=reuse, scope=scope)\n\n\ndef resnet_v2_200(inputs,\n                  num_classes=None,\n                  is_training=True,\n                  global_pool=True,\n                  output_stride=None,\n                  reuse=None,\n                  scope=\'resnet_v2_200\'):\n  """"""ResNet-200 model of [2]. See resnet_v2() for arg and return description.""""""\n  blocks = [\n      resnet_utils.Block(\n          \'block1\', bottleneck, [(256, 64, 1)] * 2 + [(256, 64, 2)]),\n      resnet_utils.Block(\n          \'block2\', bottleneck, [(512, 128, 1)] * 23 + [(512, 128, 2)]),\n      resnet_utils.Block(\n          \'block3\', bottleneck, [(1024, 256, 1)] * 35 + [(1024, 256, 2)]),\n      resnet_utils.Block(\n          \'block4\', bottleneck, [(2048, 512, 1)] * 3)]\n  return resnet_v2(inputs, blocks, num_classes, is_training=is_training,\n                   global_pool=global_pool, output_stride=output_stride,\n                   include_root_block=True, reuse=reuse, scope=scope)\n'"
finetuning/nets/resnet_v2_test.py,52,"b'# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Tests for slim.nets.resnet_v2.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport numpy as np\nimport tensorflow as tf\n\nfrom nets import resnet_utils\nfrom nets import resnet_v2\n\nslim = tf.contrib.slim\n\n\ndef create_test_input(batch_size, height, width, channels):\n  """"""Create test input tensor.\n\n  Args:\n    batch_size: The number of images per batch or `None` if unknown.\n    height: The height of each image or `None` if unknown.\n    width: The width of each image or `None` if unknown.\n    channels: The number of channels per image or `None` if unknown.\n\n  Returns:\n    Either a placeholder `Tensor` of dimension\n      [batch_size, height, width, channels] if any of the inputs are `None` or a\n    constant `Tensor` with the mesh grid values along the spatial dimensions.\n  """"""\n  if None in [batch_size, height, width, channels]:\n    return tf.placeholder(tf.float32, (batch_size, height, width, channels))\n  else:\n    return tf.to_float(\n        np.tile(\n            np.reshape(\n                np.reshape(np.arange(height), [height, 1]) +\n                np.reshape(np.arange(width), [1, width]),\n                [1, height, width, 1]),\n            [batch_size, 1, 1, channels]))\n\n\nclass ResnetUtilsTest(tf.test.TestCase):\n\n  def testSubsampleThreeByThree(self):\n    x = tf.reshape(tf.to_float(tf.range(9)), [1, 3, 3, 1])\n    x = resnet_utils.subsample(x, 2)\n    expected = tf.reshape(tf.constant([0, 2, 6, 8]), [1, 2, 2, 1])\n    with self.test_session():\n      self.assertAllClose(x.eval(), expected.eval())\n\n  def testSubsampleFourByFour(self):\n    x = tf.reshape(tf.to_float(tf.range(16)), [1, 4, 4, 1])\n    x = resnet_utils.subsample(x, 2)\n    expected = tf.reshape(tf.constant([0, 2, 8, 10]), [1, 2, 2, 1])\n    with self.test_session():\n      self.assertAllClose(x.eval(), expected.eval())\n\n  def testConv2DSameEven(self):\n    n, n2 = 4, 2\n\n    # Input image.\n    x = create_test_input(1, n, n, 1)\n\n    # Convolution kernel.\n    w = create_test_input(1, 3, 3, 1)\n    w = tf.reshape(w, [3, 3, 1, 1])\n\n    tf.get_variable(\'Conv/weights\', initializer=w)\n    tf.get_variable(\'Conv/biases\', initializer=tf.zeros([1]))\n    tf.get_variable_scope().reuse_variables()\n\n    y1 = slim.conv2d(x, 1, [3, 3], stride=1, scope=\'Conv\')\n    y1_expected = tf.to_float([[14, 28, 43, 26],\n                               [28, 48, 66, 37],\n                               [43, 66, 84, 46],\n                               [26, 37, 46, 22]])\n    y1_expected = tf.reshape(y1_expected, [1, n, n, 1])\n\n    y2 = resnet_utils.subsample(y1, 2)\n    y2_expected = tf.to_float([[14, 43],\n                               [43, 84]])\n    y2_expected = tf.reshape(y2_expected, [1, n2, n2, 1])\n\n    y3 = resnet_utils.conv2d_same(x, 1, 3, stride=2, scope=\'Conv\')\n    y3_expected = y2_expected\n\n    y4 = slim.conv2d(x, 1, [3, 3], stride=2, scope=\'Conv\')\n    y4_expected = tf.to_float([[48, 37],\n                               [37, 22]])\n    y4_expected = tf.reshape(y4_expected, [1, n2, n2, 1])\n\n    with self.test_session() as sess:\n      try:\n        sess.run(tf.global_variables_initializer())\n      except AttributeError:\n        sess.run(tf.initialize_all_variables())\n      self.assertAllClose(y1.eval(), y1_expected.eval())\n      self.assertAllClose(y2.eval(), y2_expected.eval())\n      self.assertAllClose(y3.eval(), y3_expected.eval())\n      self.assertAllClose(y4.eval(), y4_expected.eval())\n\n  def testConv2DSameOdd(self):\n    n, n2 = 5, 3\n\n    # Input image.\n    x = create_test_input(1, n, n, 1)\n\n    # Convolution kernel.\n    w = create_test_input(1, 3, 3, 1)\n    w = tf.reshape(w, [3, 3, 1, 1])\n\n    tf.get_variable(\'Conv/weights\', initializer=w)\n    tf.get_variable(\'Conv/biases\', initializer=tf.zeros([1]))\n    tf.get_variable_scope().reuse_variables()\n\n    y1 = slim.conv2d(x, 1, [3, 3], stride=1, scope=\'Conv\')\n    y1_expected = tf.to_float([[14, 28, 43, 58, 34],\n                               [28, 48, 66, 84, 46],\n                               [43, 66, 84, 102, 55],\n                               [58, 84, 102, 120, 64],\n                               [34, 46, 55, 64, 30]])\n    y1_expected = tf.reshape(y1_expected, [1, n, n, 1])\n\n    y2 = resnet_utils.subsample(y1, 2)\n    y2_expected = tf.to_float([[14, 43, 34],\n                               [43, 84, 55],\n                               [34, 55, 30]])\n    y2_expected = tf.reshape(y2_expected, [1, n2, n2, 1])\n\n    y3 = resnet_utils.conv2d_same(x, 1, 3, stride=2, scope=\'Conv\')\n    y3_expected = y2_expected\n\n    y4 = slim.conv2d(x, 1, [3, 3], stride=2, scope=\'Conv\')\n    y4_expected = y2_expected\n\n    with self.test_session() as sess:\n      try:\n        sess.run(tf.global_variables_initializer())\n      except AttributeError:\n        sess.run(tf.initialize_all_variables())\n      self.assertAllClose(y1.eval(), y1_expected.eval())\n      self.assertAllClose(y2.eval(), y2_expected.eval())\n      self.assertAllClose(y3.eval(), y3_expected.eval())\n      self.assertAllClose(y4.eval(), y4_expected.eval())\n\n  def _resnet_plain(self, inputs, blocks, output_stride=None, scope=None):\n    """"""A plain ResNet without extra layers before or after the ResNet blocks.""""""\n    with tf.variable_scope(scope, values=[inputs]):\n      with slim.arg_scope([slim.conv2d], outputs_collections=\'end_points\'):\n        net = resnet_utils.stack_blocks_dense(inputs, blocks, output_stride)\n        end_points = dict(tf.get_collection(\'end_points\'))\n        return net, end_points\n\n  def testEndPointsV2(self):\n    """"""Test the end points of a tiny v2 bottleneck network.""""""\n    bottleneck = resnet_v2.bottleneck\n    blocks = [resnet_utils.Block(\'block1\', bottleneck, [(4, 1, 1), (4, 1, 2)]),\n              resnet_utils.Block(\'block2\', bottleneck, [(8, 2, 1), (8, 2, 1)])]\n    inputs = create_test_input(2, 32, 16, 3)\n    with slim.arg_scope(resnet_utils.resnet_arg_scope()):\n      _, end_points = self._resnet_plain(inputs, blocks, scope=\'tiny\')\n    expected = [\n        \'tiny/block1/unit_1/bottleneck_v2/shortcut\',\n        \'tiny/block1/unit_1/bottleneck_v2/conv1\',\n        \'tiny/block1/unit_1/bottleneck_v2/conv2\',\n        \'tiny/block1/unit_1/bottleneck_v2/conv3\',\n        \'tiny/block1/unit_2/bottleneck_v2/conv1\',\n        \'tiny/block1/unit_2/bottleneck_v2/conv2\',\n        \'tiny/block1/unit_2/bottleneck_v2/conv3\',\n        \'tiny/block2/unit_1/bottleneck_v2/shortcut\',\n        \'tiny/block2/unit_1/bottleneck_v2/conv1\',\n        \'tiny/block2/unit_1/bottleneck_v2/conv2\',\n        \'tiny/block2/unit_1/bottleneck_v2/conv3\',\n        \'tiny/block2/unit_2/bottleneck_v2/conv1\',\n        \'tiny/block2/unit_2/bottleneck_v2/conv2\',\n        \'tiny/block2/unit_2/bottleneck_v2/conv3\']\n    self.assertItemsEqual(expected, end_points)\n\n  def _stack_blocks_nondense(self, net, blocks):\n    """"""A simplified ResNet Block stacker without output stride control.""""""\n    for block in blocks:\n      with tf.variable_scope(block.scope, \'block\', [net]):\n        for i, unit in enumerate(block.args):\n          depth, depth_bottleneck, stride = unit\n          with tf.variable_scope(\'unit_%d\' % (i + 1), values=[net]):\n            net = block.unit_fn(net,\n                                depth=depth,\n                                depth_bottleneck=depth_bottleneck,\n                                stride=stride,\n                                rate=1)\n    return net\n\n  def _atrousValues(self, bottleneck):\n    """"""Verify the values of dense feature extraction by atrous convolution.\n\n    Make sure that dense feature extraction by stack_blocks_dense() followed by\n    subsampling gives identical results to feature extraction at the nominal\n    network output stride using the simple self._stack_blocks_nondense() above.\n\n    Args:\n      bottleneck: The bottleneck function.\n    """"""\n    blocks = [\n        resnet_utils.Block(\'block1\', bottleneck, [(4, 1, 1), (4, 1, 2)]),\n        resnet_utils.Block(\'block2\', bottleneck, [(8, 2, 1), (8, 2, 2)]),\n        resnet_utils.Block(\'block3\', bottleneck, [(16, 4, 1), (16, 4, 2)]),\n        resnet_utils.Block(\'block4\', bottleneck, [(32, 8, 1), (32, 8, 1)])\n    ]\n    nominal_stride = 8\n\n    # Test both odd and even input dimensions.\n    height = 30\n    width = 31\n    with slim.arg_scope(resnet_utils.resnet_arg_scope()):\n      with slim.arg_scope([slim.batch_norm], is_training=False):\n        for output_stride in [1, 2, 4, 8, None]:\n          with tf.Graph().as_default():\n            with self.test_session() as sess:\n              tf.set_random_seed(0)\n              inputs = create_test_input(1, height, width, 3)\n              # Dense feature extraction followed by subsampling.\n              output = resnet_utils.stack_blocks_dense(inputs,\n                                                       blocks,\n                                                       output_stride)\n              if output_stride is None:\n                factor = 1\n              else:\n                factor = nominal_stride // output_stride\n\n              output = resnet_utils.subsample(output, factor)\n              # Make the two networks use the same weights.\n              tf.get_variable_scope().reuse_variables()\n              # Feature extraction at the nominal network rate.\n              expected = self._stack_blocks_nondense(inputs, blocks)\n              try:\n                sess.run(tf.global_variables_initializer())\n              except AttributeError:\n                sess.run(tf.initialize_all_variables())\n              output, expected = sess.run([output, expected])\n              self.assertAllClose(output, expected, atol=1e-4, rtol=1e-4)\n\n  def testAtrousValuesBottleneck(self):\n    self._atrousValues(resnet_v2.bottleneck)\n\n\nclass ResnetCompleteNetworkTest(tf.test.TestCase):\n  """"""Tests with complete small ResNet v2 networks.""""""\n\n  def _resnet_small(self,\n                    inputs,\n                    num_classes=None,\n                    is_training=True,\n                    global_pool=True,\n                    output_stride=None,\n                    include_root_block=True,\n                    reuse=None,\n                    scope=\'resnet_v2_small\'):\n    """"""A shallow and thin ResNet v2 for faster tests.""""""\n    bottleneck = resnet_v2.bottleneck\n    blocks = [\n        resnet_utils.Block(\n            \'block1\', bottleneck, [(4, 1, 1)] * 2 + [(4, 1, 2)]),\n        resnet_utils.Block(\n            \'block2\', bottleneck, [(8, 2, 1)] * 2 + [(8, 2, 2)]),\n        resnet_utils.Block(\n            \'block3\', bottleneck, [(16, 4, 1)] * 2 + [(16, 4, 2)]),\n        resnet_utils.Block(\n            \'block4\', bottleneck, [(32, 8, 1)] * 2)]\n    return resnet_v2.resnet_v2(inputs, blocks, num_classes,\n                               is_training=is_training,\n                               global_pool=global_pool,\n                               output_stride=output_stride,\n                               include_root_block=include_root_block,\n                               reuse=reuse,\n                               scope=scope)\n\n  def testClassificationEndPoints(self):\n    global_pool = True\n    num_classes = 10\n    inputs = create_test_input(2, 224, 224, 3)\n    with slim.arg_scope(resnet_utils.resnet_arg_scope()):\n      logits, end_points = self._resnet_small(inputs, num_classes,\n                                              global_pool=global_pool,\n                                              scope=\'resnet\')\n    self.assertTrue(logits.op.name.startswith(\'resnet/logits\'))\n    self.assertListEqual(logits.get_shape().as_list(), [2, 1, 1, num_classes])\n    self.assertTrue(\'predictions\' in end_points)\n    self.assertListEqual(end_points[\'predictions\'].get_shape().as_list(),\n                         [2, 1, 1, num_classes])\n\n  def testClassificationShapes(self):\n    global_pool = True\n    num_classes = 10\n    inputs = create_test_input(2, 224, 224, 3)\n    with slim.arg_scope(resnet_utils.resnet_arg_scope()):\n      _, end_points = self._resnet_small(inputs, num_classes,\n                                         global_pool=global_pool,\n                                         scope=\'resnet\')\n      endpoint_to_shape = {\n          \'resnet/block1\': [2, 28, 28, 4],\n          \'resnet/block2\': [2, 14, 14, 8],\n          \'resnet/block3\': [2, 7, 7, 16],\n          \'resnet/block4\': [2, 7, 7, 32]}\n      for endpoint in endpoint_to_shape:\n        shape = endpoint_to_shape[endpoint]\n        self.assertListEqual(end_points[endpoint].get_shape().as_list(), shape)\n\n  def testFullyConvolutionalEndpointShapes(self):\n    global_pool = False\n    num_classes = 10\n    inputs = create_test_input(2, 321, 321, 3)\n    with slim.arg_scope(resnet_utils.resnet_arg_scope()):\n      _, end_points = self._resnet_small(inputs, num_classes,\n                                         global_pool=global_pool,\n                                         scope=\'resnet\')\n      endpoint_to_shape = {\n          \'resnet/block1\': [2, 41, 41, 4],\n          \'resnet/block2\': [2, 21, 21, 8],\n          \'resnet/block3\': [2, 11, 11, 16],\n          \'resnet/block4\': [2, 11, 11, 32]}\n      for endpoint in endpoint_to_shape:\n        shape = endpoint_to_shape[endpoint]\n        self.assertListEqual(end_points[endpoint].get_shape().as_list(), shape)\n\n  def testRootlessFullyConvolutionalEndpointShapes(self):\n    global_pool = False\n    num_classes = 10\n    inputs = create_test_input(2, 128, 128, 3)\n    with slim.arg_scope(resnet_utils.resnet_arg_scope()):\n      _, end_points = self._resnet_small(inputs, num_classes,\n                                         global_pool=global_pool,\n                                         include_root_block=False,\n                                         scope=\'resnet\')\n      endpoint_to_shape = {\n          \'resnet/block1\': [2, 64, 64, 4],\n          \'resnet/block2\': [2, 32, 32, 8],\n          \'resnet/block3\': [2, 16, 16, 16],\n          \'resnet/block4\': [2, 16, 16, 32]}\n      for endpoint in endpoint_to_shape:\n        shape = endpoint_to_shape[endpoint]\n        self.assertListEqual(end_points[endpoint].get_shape().as_list(), shape)\n\n  def testAtrousFullyConvolutionalEndpointShapes(self):\n    global_pool = False\n    num_classes = 10\n    output_stride = 8\n    inputs = create_test_input(2, 321, 321, 3)\n    with slim.arg_scope(resnet_utils.resnet_arg_scope()):\n      _, end_points = self._resnet_small(inputs,\n                                         num_classes,\n                                         global_pool=global_pool,\n                                         output_stride=output_stride,\n                                         scope=\'resnet\')\n      endpoint_to_shape = {\n          \'resnet/block1\': [2, 41, 41, 4],\n          \'resnet/block2\': [2, 41, 41, 8],\n          \'resnet/block3\': [2, 41, 41, 16],\n          \'resnet/block4\': [2, 41, 41, 32]}\n      for endpoint in endpoint_to_shape:\n        shape = endpoint_to_shape[endpoint]\n        self.assertListEqual(end_points[endpoint].get_shape().as_list(), shape)\n\n  def testAtrousFullyConvolutionalValues(self):\n    """"""Verify dense feature extraction with atrous convolution.""""""\n    nominal_stride = 32\n    for output_stride in [4, 8, 16, 32, None]:\n      with slim.arg_scope(resnet_utils.resnet_arg_scope()):\n        with tf.Graph().as_default():\n          with self.test_session() as sess:\n            tf.set_random_seed(0)\n            inputs = create_test_input(2, 81, 81, 3)\n            # Dense feature extraction followed by subsampling.\n            output, _ = self._resnet_small(inputs, None,\n                                           is_training=False,\n                                           global_pool=False,\n                                           output_stride=output_stride)\n            if output_stride is None:\n              factor = 1\n            else:\n              factor = nominal_stride // output_stride\n            output = resnet_utils.subsample(output, factor)\n            # Make the two networks use the same weights.\n            tf.get_variable_scope().reuse_variables()\n            # Feature extraction at the nominal network rate.\n            expected, _ = self._resnet_small(inputs, None,\n                                             is_training=False,\n                                             global_pool=False)\n            try:\n              sess.run(tf.global_variables_initializer())\n            except AttributeError:\n              sess.run(tf.initialize_all_variables())\n            self.assertAllClose(output.eval(), expected.eval(),\n                                atol=1e-4, rtol=1e-4)\n\n  def testUnknownBatchSize(self):\n    batch = 2\n    height, width = 65, 65\n    global_pool = True\n    num_classes = 10\n    inputs = create_test_input(None, height, width, 3)\n    with slim.arg_scope(resnet_utils.resnet_arg_scope()):\n      logits, _ = self._resnet_small(inputs, num_classes,\n                                     global_pool=global_pool,\n                                     scope=\'resnet\')\n    self.assertTrue(logits.op.name.startswith(\'resnet/logits\'))\n    self.assertListEqual(logits.get_shape().as_list(),\n                         [None, 1, 1, num_classes])\n    images = create_test_input(batch, height, width, 3)\n    with self.test_session() as sess:\n      try:\n        sess.run(tf.global_variables_initializer())\n      except AttributeError:\n        sess.run(tf.initialize_all_variables())\n      output = sess.run(logits, {inputs: images.eval()})\n      self.assertEqual(output.shape, (batch, 1, 1, num_classes))\n\n  def testFullyConvolutionalUnknownHeightWidth(self):\n    batch = 2\n    height, width = 65, 65\n    global_pool = False\n    inputs = create_test_input(batch, None, None, 3)\n    with slim.arg_scope(resnet_utils.resnet_arg_scope()):\n      output, _ = self._resnet_small(inputs, None,\n                                     global_pool=global_pool)\n    self.assertListEqual(output.get_shape().as_list(),\n                         [batch, None, None, 32])\n    images = create_test_input(batch, height, width, 3)\n    with self.test_session() as sess:\n      try:\n        sess.run(tf.global_variables_initializer())\n      except AttributeError:\n        sess.run(tf.initialize_all_variables())\n      output = sess.run(output, {inputs: images.eval()})\n      self.assertEqual(output.shape, (batch, 3, 3, 32))\n\n  def testAtrousFullyConvolutionalUnknownHeightWidth(self):\n    batch = 2\n    height, width = 65, 65\n    global_pool = False\n    output_stride = 8\n    inputs = create_test_input(batch, None, None, 3)\n    with slim.arg_scope(resnet_utils.resnet_arg_scope()):\n      output, _ = self._resnet_small(inputs,\n                                     None,\n                                     global_pool=global_pool,\n                                     output_stride=output_stride)\n    self.assertListEqual(output.get_shape().as_list(),\n                         [batch, None, None, 32])\n    images = create_test_input(batch, height, width, 3)\n    with self.test_session() as sess:\n      try:\n        sess.run(tf.global_variables_initializer())\n      except AttributeError:\n        sess.run(tf.initialize_all_variables())\n      output = sess.run(output, {inputs: images.eval()})\n      self.assertEqual(output.shape, (batch, 9, 9, 32))\n\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
finetuning/nets/vgg.py,9,"b'# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Contains model definitions for versions of the Oxford VGG network.\n\nThese model definitions were introduced in the following technical report:\n\n  Very Deep Convolutional Networks For Large-Scale Image Recognition\n  Karen Simonyan and Andrew Zisserman\n  arXiv technical report, 2015\n  PDF: http://arxiv.org/pdf/1409.1556.pdf\n  ILSVRC 2014 Slides: http://www.robots.ox.ac.uk/~karen/pdf/ILSVRC_2014.pdf\n  CC-BY-4.0\n\nMore information can be obtained from the VGG website:\nwww.robots.ox.ac.uk/~vgg/research/very_deep/\n\nUsage:\n  with slim.arg_scope(vgg.vgg_arg_scope()):\n    outputs, end_points = vgg.vgg_a(inputs)\n\n  with slim.arg_scope(vgg.vgg_arg_scope()):\n    outputs, end_points = vgg.vgg_16(inputs)\n\n@@vgg_a\n@@vgg_16\n@@vgg_19\n""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\n\nslim = tf.contrib.slim\n\n\ndef vgg_arg_scope(weight_decay=0.0005):\n  """"""Defines the VGG arg scope.\n\n  Args:\n    weight_decay: The l2 regularization coefficient.\n\n  Returns:\n    An arg_scope.\n  """"""\n  with slim.arg_scope([slim.conv2d, slim.fully_connected],\n                      activation_fn=tf.nn.relu,\n                      weights_regularizer=slim.l2_regularizer(weight_decay),\n                      biases_initializer=tf.zeros_initializer()):\n    with slim.arg_scope([slim.conv2d], padding=\'SAME\') as arg_sc:\n      return arg_sc\n\n\ndef vgg_a(inputs,\n          num_classes=1000,\n          is_training=True,\n          dropout_keep_prob=0.5,\n          spatial_squeeze=True,\n          scope=\'vgg_a\'):\n  """"""Oxford Net VGG 11-Layers version A Example.\n\n  Note: All the fully_connected layers have been transformed to conv2d layers.\n        To use in classification mode, resize input to 224x224.\n\n  Args:\n    inputs: a tensor of size [batch_size, height, width, channels].\n    num_classes: number of predicted classes.\n    is_training: whether or not the model is being trained.\n    dropout_keep_prob: the probability that activations are kept in the dropout\n      layers during training.\n    spatial_squeeze: whether or not should squeeze the spatial dimensions of the\n      outputs. Useful to remove unnecessary dimensions for classification.\n    scope: Optional scope for the variables.\n\n  Returns:\n    the last op containing the log predictions and end_points dict.\n  """"""\n  with tf.variable_scope(scope, \'vgg_a\', [inputs]) as sc:\n    end_points_collection = sc.name + \'_end_points\'\n    # Collect outputs for conv2d, fully_connected and max_pool2d.\n    with slim.arg_scope([slim.conv2d, slim.max_pool2d],\n                        outputs_collections=end_points_collection):\n      net = slim.repeat(inputs, 1, slim.conv2d, 64, [3, 3], scope=\'conv1\')\n      net = slim.max_pool2d(net, [2, 2], scope=\'pool1\')\n      net = slim.repeat(net, 1, slim.conv2d, 128, [3, 3], scope=\'conv2\')\n      net = slim.max_pool2d(net, [2, 2], scope=\'pool2\')\n      net = slim.repeat(net, 2, slim.conv2d, 256, [3, 3], scope=\'conv3\')\n      net = slim.max_pool2d(net, [2, 2], scope=\'pool3\')\n      net = slim.repeat(net, 2, slim.conv2d, 512, [3, 3], scope=\'conv4\')\n      net = slim.max_pool2d(net, [2, 2], scope=\'pool4\')\n      net = slim.repeat(net, 2, slim.conv2d, 512, [3, 3], scope=\'conv5\')\n      net = slim.max_pool2d(net, [2, 2], scope=\'pool5\')\n      # Use conv2d instead of fully_connected layers.\n      net = slim.conv2d(net, 4096, [7, 7], padding=\'VALID\', scope=\'fc6\')\n      net = slim.dropout(net, dropout_keep_prob, is_training=is_training,\n                         scope=\'dropout6\')\n      net = slim.conv2d(net, 4096, [1, 1], scope=\'fc7\')\n      net = slim.dropout(net, dropout_keep_prob, is_training=is_training,\n                         scope=\'dropout7\')\n      net = slim.conv2d(net, num_classes, [1, 1],\n                        activation_fn=None,\n                        normalizer_fn=None,\n                        scope=\'fc8\')\n      # Convert end_points_collection into a end_point dict.\n      end_points = slim.utils.convert_collection_to_dict(end_points_collection)\n      if spatial_squeeze:\n        net = tf.squeeze(net, [1, 2], name=\'fc8/squeezed\')\n        end_points[sc.name + \'/fc8\'] = net\n      return net, end_points\nvgg_a.default_image_size = 224\n\n\ndef vgg_16(inputs,\n           num_classes=1000,\n           is_training=True,\n           dropout_keep_prob=0.5,\n           spatial_squeeze=True,\n           scope=\'vgg_16\'):\n  """"""Oxford Net VGG 16-Layers version D Example.\n\n  Note: All the fully_connected layers have been transformed to conv2d layers.\n        To use in classification mode, resize input to 224x224.\n\n  Args:\n    inputs: a tensor of size [batch_size, height, width, channels].\n    num_classes: number of predicted classes.\n    is_training: whether or not the model is being trained.\n    dropout_keep_prob: the probability that activations are kept in the dropout\n      layers during training.\n    spatial_squeeze: whether or not should squeeze the spatial dimensions of the\n      outputs. Useful to remove unnecessary dimensions for classification.\n    scope: Optional scope for the variables.\n\n  Returns:\n    the last op containing the log predictions and end_points dict.\n  """"""\n  with tf.variable_scope(scope, \'vgg_16\', [inputs]) as sc:\n    end_points_collection = sc.name + \'_end_points\'\n    # Collect outputs for conv2d, fully_connected and max_pool2d.\n    with slim.arg_scope([slim.conv2d, slim.fully_connected, slim.max_pool2d],\n                        outputs_collections=end_points_collection):\n      net = slim.repeat(inputs, 2, slim.conv2d, 64, [3, 3], scope=\'conv1\')\n      net = slim.max_pool2d(net, [2, 2], scope=\'pool1\')\n      net = slim.repeat(net, 2, slim.conv2d, 128, [3, 3], scope=\'conv2\')\n      net = slim.max_pool2d(net, [2, 2], scope=\'pool2\')\n      net = slim.repeat(net, 3, slim.conv2d, 256, [3, 3], scope=\'conv3\')\n      net = slim.max_pool2d(net, [2, 2], scope=\'pool3\')\n      net = slim.repeat(net, 3, slim.conv2d, 512, [3, 3], scope=\'conv4\')\n      net = slim.max_pool2d(net, [2, 2], scope=\'pool4\')\n      net = slim.repeat(net, 3, slim.conv2d, 512, [3, 3], scope=\'conv5\')\n      net = slim.max_pool2d(net, [2, 2], scope=\'pool5\')\n      # Use conv2d instead of fully_connected layers.\n      net = slim.conv2d(net, 4096, [7, 7], padding=\'VALID\', scope=\'fc6\')\n      net = slim.dropout(net, dropout_keep_prob, is_training=is_training,\n                         scope=\'dropout6\')\n      net = slim.conv2d(net, 4096, [1, 1], scope=\'fc7\')\n      net = slim.dropout(net, dropout_keep_prob, is_training=is_training,\n                         scope=\'dropout7\')\n      net = slim.conv2d(net, num_classes, [1, 1],\n                        activation_fn=None,\n                        normalizer_fn=None,\n                        scope=\'fc8\')\n      # Convert end_points_collection into a end_point dict.\n      end_points = slim.utils.convert_collection_to_dict(end_points_collection)\n      if spatial_squeeze:\n        net = tf.squeeze(net, [1, 2], name=\'fc8/squeezed\')\n        end_points[sc.name + \'/fc8\'] = net\n      return net, end_points\nvgg_16.default_image_size = 224\n\n\ndef vgg_19(inputs,\n           num_classes=1000,\n           is_training=True,\n           dropout_keep_prob=0.5,\n           spatial_squeeze=True,\n           scope=\'vgg_19\'):\n  """"""Oxford Net VGG 19-Layers version E Example.\n\n  Note: All the fully_connected layers have been transformed to conv2d layers.\n        To use in classification mode, resize input to 224x224.\n\n  Args:\n    inputs: a tensor of size [batch_size, height, width, channels].\n    num_classes: number of predicted classes.\n    is_training: whether or not the model is being trained.\n    dropout_keep_prob: the probability that activations are kept in the dropout\n      layers during training.\n    spatial_squeeze: whether or not should squeeze the spatial dimensions of the\n      outputs. Useful to remove unnecessary dimensions for classification.\n    scope: Optional scope for the variables.\n\n  Returns:\n    the last op containing the log predictions and end_points dict.\n  """"""\n  with tf.variable_scope(scope, \'vgg_19\', [inputs]) as sc:\n    end_points_collection = sc.name + \'_end_points\'\n    # Collect outputs for conv2d, fully_connected and max_pool2d.\n    with slim.arg_scope([slim.conv2d, slim.fully_connected, slim.max_pool2d],\n                        outputs_collections=end_points_collection):\n      net = slim.repeat(inputs, 2, slim.conv2d, 64, [3, 3], scope=\'conv1\')\n      net = slim.max_pool2d(net, [2, 2], scope=\'pool1\')\n      net = slim.repeat(net, 2, slim.conv2d, 128, [3, 3], scope=\'conv2\')\n      net = slim.max_pool2d(net, [2, 2], scope=\'pool2\')\n      net = slim.repeat(net, 4, slim.conv2d, 256, [3, 3], scope=\'conv3\')\n      net = slim.max_pool2d(net, [2, 2], scope=\'pool3\')\n      net = slim.repeat(net, 4, slim.conv2d, 512, [3, 3], scope=\'conv4\')\n      net = slim.max_pool2d(net, [2, 2], scope=\'pool4\')\n      net = slim.repeat(net, 4, slim.conv2d, 512, [3, 3], scope=\'conv5\')\n      net = slim.max_pool2d(net, [2, 2], scope=\'pool5\')\n      # Use conv2d instead of fully_connected layers.\n      net = slim.conv2d(net, 4096, [7, 7], padding=\'VALID\', scope=\'fc6\')\n      net = slim.dropout(net, dropout_keep_prob, is_training=is_training,\n                         scope=\'dropout6\')\n      net = slim.conv2d(net, 4096, [1, 1], scope=\'fc7\')\n      net = slim.dropout(net, dropout_keep_prob, is_training=is_training,\n                         scope=\'dropout7\')\n      net = slim.conv2d(net, num_classes, [1, 1],\n                        activation_fn=None,\n                        normalizer_fn=None,\n                        scope=\'fc8\')\n      # Convert end_points_collection into a end_point dict.\n      end_points = slim.utils.convert_collection_to_dict(end_points_collection)\n      if spatial_squeeze:\n        net = tf.squeeze(net, [1, 2], name=\'fc8/squeezed\')\n        end_points[sc.name + \'/fc8\'] = net\n      return net, end_points\nvgg_19.default_image_size = 224\n\n# Alias\nvgg_d = vgg_16\nvgg_e = vgg_19\n'"
finetuning/nets/vgg_test.py,47,"b'# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Tests for slim.nets.vgg.""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\n\nfrom nets import vgg\n\nslim = tf.contrib.slim\n\n\nclass VGGATest(tf.test.TestCase):\n\n  def testBuild(self):\n    batch_size = 5\n    height, width = 224, 224\n    num_classes = 1000\n    with self.test_session():\n      inputs = tf.random_uniform((batch_size, height, width, 3))\n      logits, _ = vgg.vgg_a(inputs, num_classes)\n      self.assertEquals(logits.op.name, \'vgg_a/fc8/squeezed\')\n      self.assertListEqual(logits.get_shape().as_list(),\n                           [batch_size, num_classes])\n\n  def testFullyConvolutional(self):\n    batch_size = 1\n    height, width = 256, 256\n    num_classes = 1000\n    with self.test_session():\n      inputs = tf.random_uniform((batch_size, height, width, 3))\n      logits, _ = vgg.vgg_a(inputs, num_classes, spatial_squeeze=False)\n      self.assertEquals(logits.op.name, \'vgg_a/fc8/BiasAdd\')\n      self.assertListEqual(logits.get_shape().as_list(),\n                           [batch_size, 2, 2, num_classes])\n\n  def testEndPoints(self):\n    batch_size = 5\n    height, width = 224, 224\n    num_classes = 1000\n    with self.test_session():\n      inputs = tf.random_uniform((batch_size, height, width, 3))\n      _, end_points = vgg.vgg_a(inputs, num_classes)\n      expected_names = [\'vgg_a/conv1/conv1_1\',\n                        \'vgg_a/pool1\',\n                        \'vgg_a/conv2/conv2_1\',\n                        \'vgg_a/pool2\',\n                        \'vgg_a/conv3/conv3_1\',\n                        \'vgg_a/conv3/conv3_2\',\n                        \'vgg_a/pool3\',\n                        \'vgg_a/conv4/conv4_1\',\n                        \'vgg_a/conv4/conv4_2\',\n                        \'vgg_a/pool4\',\n                        \'vgg_a/conv5/conv5_1\',\n                        \'vgg_a/conv5/conv5_2\',\n                        \'vgg_a/pool5\',\n                        \'vgg_a/fc6\',\n                        \'vgg_a/fc7\',\n                        \'vgg_a/fc8\'\n                       ]\n      self.assertSetEqual(set(end_points.keys()), set(expected_names))\n\n  def testModelVariables(self):\n    batch_size = 5\n    height, width = 224, 224\n    num_classes = 1000\n    with self.test_session():\n      inputs = tf.random_uniform((batch_size, height, width, 3))\n      vgg.vgg_a(inputs, num_classes)\n      expected_names = [\'vgg_a/conv1/conv1_1/weights\',\n                        \'vgg_a/conv1/conv1_1/biases\',\n                        \'vgg_a/conv2/conv2_1/weights\',\n                        \'vgg_a/conv2/conv2_1/biases\',\n                        \'vgg_a/conv3/conv3_1/weights\',\n                        \'vgg_a/conv3/conv3_1/biases\',\n                        \'vgg_a/conv3/conv3_2/weights\',\n                        \'vgg_a/conv3/conv3_2/biases\',\n                        \'vgg_a/conv4/conv4_1/weights\',\n                        \'vgg_a/conv4/conv4_1/biases\',\n                        \'vgg_a/conv4/conv4_2/weights\',\n                        \'vgg_a/conv4/conv4_2/biases\',\n                        \'vgg_a/conv5/conv5_1/weights\',\n                        \'vgg_a/conv5/conv5_1/biases\',\n                        \'vgg_a/conv5/conv5_2/weights\',\n                        \'vgg_a/conv5/conv5_2/biases\',\n                        \'vgg_a/fc6/weights\',\n                        \'vgg_a/fc6/biases\',\n                        \'vgg_a/fc7/weights\',\n                        \'vgg_a/fc7/biases\',\n                        \'vgg_a/fc8/weights\',\n                        \'vgg_a/fc8/biases\',\n                       ]\n      model_variables = [v.op.name for v in slim.get_model_variables()]\n      self.assertSetEqual(set(model_variables), set(expected_names))\n\n  def testEvaluation(self):\n    batch_size = 2\n    height, width = 224, 224\n    num_classes = 1000\n    with self.test_session():\n      eval_inputs = tf.random_uniform((batch_size, height, width, 3))\n      logits, _ = vgg.vgg_a(eval_inputs, is_training=False)\n      self.assertListEqual(logits.get_shape().as_list(),\n                           [batch_size, num_classes])\n      predictions = tf.argmax(logits, 1)\n      self.assertListEqual(predictions.get_shape().as_list(), [batch_size])\n\n  def testTrainEvalWithReuse(self):\n    train_batch_size = 2\n    eval_batch_size = 1\n    train_height, train_width = 224, 224\n    eval_height, eval_width = 256, 256\n    num_classes = 1000\n    with self.test_session():\n      train_inputs = tf.random_uniform(\n          (train_batch_size, train_height, train_width, 3))\n      logits, _ = vgg.vgg_a(train_inputs)\n      self.assertListEqual(logits.get_shape().as_list(),\n                           [train_batch_size, num_classes])\n      tf.get_variable_scope().reuse_variables()\n      eval_inputs = tf.random_uniform(\n          (eval_batch_size, eval_height, eval_width, 3))\n      logits, _ = vgg.vgg_a(eval_inputs, is_training=False,\n                            spatial_squeeze=False)\n      self.assertListEqual(logits.get_shape().as_list(),\n                           [eval_batch_size, 2, 2, num_classes])\n      logits = tf.reduce_mean(logits, [1, 2])\n      predictions = tf.argmax(logits, 1)\n      self.assertEquals(predictions.get_shape().as_list(), [eval_batch_size])\n\n  def testForward(self):\n    batch_size = 1\n    height, width = 224, 224\n    with self.test_session() as sess:\n      inputs = tf.random_uniform((batch_size, height, width, 3))\n      logits, _ = vgg.vgg_a(inputs)\n      try:\n        sess.run(tf.global_variables_initializer())\n      except AttributeError:\n        sess.run(tf.initialize_all_variables())\n      output = sess.run(logits)\n      self.assertTrue(output.any())\n\n\nclass VGG16Test(tf.test.TestCase):\n\n  def testBuild(self):\n    batch_size = 5\n    height, width = 224, 224\n    num_classes = 1000\n    with self.test_session():\n      inputs = tf.random_uniform((batch_size, height, width, 3))\n      logits, _ = vgg.vgg_16(inputs, num_classes)\n      self.assertEquals(logits.op.name, \'vgg_16/fc8/squeezed\')\n      self.assertListEqual(logits.get_shape().as_list(),\n                           [batch_size, num_classes])\n\n  def testFullyConvolutional(self):\n    batch_size = 1\n    height, width = 256, 256\n    num_classes = 1000\n    with self.test_session():\n      inputs = tf.random_uniform((batch_size, height, width, 3))\n      logits, _ = vgg.vgg_16(inputs, num_classes, spatial_squeeze=False)\n      self.assertEquals(logits.op.name, \'vgg_16/fc8/BiasAdd\')\n      self.assertListEqual(logits.get_shape().as_list(),\n                           [batch_size, 2, 2, num_classes])\n\n  def testEndPoints(self):\n    batch_size = 5\n    height, width = 224, 224\n    num_classes = 1000\n    with self.test_session():\n      inputs = tf.random_uniform((batch_size, height, width, 3))\n      _, end_points = vgg.vgg_16(inputs, num_classes)\n      expected_names = [\'vgg_16/conv1/conv1_1\',\n                        \'vgg_16/conv1/conv1_2\',\n                        \'vgg_16/pool1\',\n                        \'vgg_16/conv2/conv2_1\',\n                        \'vgg_16/conv2/conv2_2\',\n                        \'vgg_16/pool2\',\n                        \'vgg_16/conv3/conv3_1\',\n                        \'vgg_16/conv3/conv3_2\',\n                        \'vgg_16/conv3/conv3_3\',\n                        \'vgg_16/pool3\',\n                        \'vgg_16/conv4/conv4_1\',\n                        \'vgg_16/conv4/conv4_2\',\n                        \'vgg_16/conv4/conv4_3\',\n                        \'vgg_16/pool4\',\n                        \'vgg_16/conv5/conv5_1\',\n                        \'vgg_16/conv5/conv5_2\',\n                        \'vgg_16/conv5/conv5_3\',\n                        \'vgg_16/pool5\',\n                        \'vgg_16/fc6\',\n                        \'vgg_16/fc7\',\n                        \'vgg_16/fc8\'\n                       ]\n      self.assertSetEqual(set(end_points.keys()), set(expected_names))\n\n  def testModelVariables(self):\n    batch_size = 5\n    height, width = 224, 224\n    num_classes = 1000\n    with self.test_session():\n      inputs = tf.random_uniform((batch_size, height, width, 3))\n      vgg.vgg_16(inputs, num_classes)\n      expected_names = [\'vgg_16/conv1/conv1_1/weights\',\n                        \'vgg_16/conv1/conv1_1/biases\',\n                        \'vgg_16/conv1/conv1_2/weights\',\n                        \'vgg_16/conv1/conv1_2/biases\',\n                        \'vgg_16/conv2/conv2_1/weights\',\n                        \'vgg_16/conv2/conv2_1/biases\',\n                        \'vgg_16/conv2/conv2_2/weights\',\n                        \'vgg_16/conv2/conv2_2/biases\',\n                        \'vgg_16/conv3/conv3_1/weights\',\n                        \'vgg_16/conv3/conv3_1/biases\',\n                        \'vgg_16/conv3/conv3_2/weights\',\n                        \'vgg_16/conv3/conv3_2/biases\',\n                        \'vgg_16/conv3/conv3_3/weights\',\n                        \'vgg_16/conv3/conv3_3/biases\',\n                        \'vgg_16/conv4/conv4_1/weights\',\n                        \'vgg_16/conv4/conv4_1/biases\',\n                        \'vgg_16/conv4/conv4_2/weights\',\n                        \'vgg_16/conv4/conv4_2/biases\',\n                        \'vgg_16/conv4/conv4_3/weights\',\n                        \'vgg_16/conv4/conv4_3/biases\',\n                        \'vgg_16/conv5/conv5_1/weights\',\n                        \'vgg_16/conv5/conv5_1/biases\',\n                        \'vgg_16/conv5/conv5_2/weights\',\n                        \'vgg_16/conv5/conv5_2/biases\',\n                        \'vgg_16/conv5/conv5_3/weights\',\n                        \'vgg_16/conv5/conv5_3/biases\',\n                        \'vgg_16/fc6/weights\',\n                        \'vgg_16/fc6/biases\',\n                        \'vgg_16/fc7/weights\',\n                        \'vgg_16/fc7/biases\',\n                        \'vgg_16/fc8/weights\',\n                        \'vgg_16/fc8/biases\',\n                       ]\n      model_variables = [v.op.name for v in slim.get_model_variables()]\n      self.assertSetEqual(set(model_variables), set(expected_names))\n\n  def testEvaluation(self):\n    batch_size = 2\n    height, width = 224, 224\n    num_classes = 1000\n    with self.test_session():\n      eval_inputs = tf.random_uniform((batch_size, height, width, 3))\n      logits, _ = vgg.vgg_16(eval_inputs, is_training=False)\n      self.assertListEqual(logits.get_shape().as_list(),\n                           [batch_size, num_classes])\n      predictions = tf.argmax(logits, 1)\n      self.assertListEqual(predictions.get_shape().as_list(), [batch_size])\n\n  def testTrainEvalWithReuse(self):\n    train_batch_size = 2\n    eval_batch_size = 1\n    train_height, train_width = 224, 224\n    eval_height, eval_width = 256, 256\n    num_classes = 1000\n    with self.test_session():\n      train_inputs = tf.random_uniform(\n          (train_batch_size, train_height, train_width, 3))\n      logits, _ = vgg.vgg_16(train_inputs)\n      self.assertListEqual(logits.get_shape().as_list(),\n                           [train_batch_size, num_classes])\n      tf.get_variable_scope().reuse_variables()\n      eval_inputs = tf.random_uniform(\n          (eval_batch_size, eval_height, eval_width, 3))\n      logits, _ = vgg.vgg_16(eval_inputs, is_training=False,\n                             spatial_squeeze=False)\n      self.assertListEqual(logits.get_shape().as_list(),\n                           [eval_batch_size, 2, 2, num_classes])\n      logits = tf.reduce_mean(logits, [1, 2])\n      predictions = tf.argmax(logits, 1)\n      self.assertEquals(predictions.get_shape().as_list(), [eval_batch_size])\n\n  def testForward(self):\n    batch_size = 1\n    height, width = 224, 224\n    with self.test_session() as sess:\n      inputs = tf.random_uniform((batch_size, height, width, 3))\n      logits, _ = vgg.vgg_16(inputs)\n      try:\n        sess.run(tf.global_variables_initializer())\n      except AttributeError:\n        sess.run(tf.initialize_all_variables())\n      output = sess.run(logits)\n      self.assertTrue(output.any())\n\n\nclass VGG19Test(tf.test.TestCase):\n\n  def testBuild(self):\n    batch_size = 5\n    height, width = 224, 224\n    num_classes = 1000\n    with self.test_session():\n      inputs = tf.random_uniform((batch_size, height, width, 3))\n      logits, _ = vgg.vgg_19(inputs, num_classes)\n      self.assertEquals(logits.op.name, \'vgg_19/fc8/squeezed\')\n      self.assertListEqual(logits.get_shape().as_list(),\n                           [batch_size, num_classes])\n\n  def testFullyConvolutional(self):\n    batch_size = 1\n    height, width = 256, 256\n    num_classes = 1000\n    with self.test_session():\n      inputs = tf.random_uniform((batch_size, height, width, 3))\n      logits, _ = vgg.vgg_19(inputs, num_classes, spatial_squeeze=False)\n      self.assertEquals(logits.op.name, \'vgg_19/fc8/BiasAdd\')\n      self.assertListEqual(logits.get_shape().as_list(),\n                           [batch_size, 2, 2, num_classes])\n\n  def testEndPoints(self):\n    batch_size = 5\n    height, width = 224, 224\n    num_classes = 1000\n    with self.test_session():\n      inputs = tf.random_uniform((batch_size, height, width, 3))\n      _, end_points = vgg.vgg_19(inputs, num_classes)\n      expected_names = [\n          \'vgg_19/conv1/conv1_1\',\n          \'vgg_19/conv1/conv1_2\',\n          \'vgg_19/pool1\',\n          \'vgg_19/conv2/conv2_1\',\n          \'vgg_19/conv2/conv2_2\',\n          \'vgg_19/pool2\',\n          \'vgg_19/conv3/conv3_1\',\n          \'vgg_19/conv3/conv3_2\',\n          \'vgg_19/conv3/conv3_3\',\n          \'vgg_19/conv3/conv3_4\',\n          \'vgg_19/pool3\',\n          \'vgg_19/conv4/conv4_1\',\n          \'vgg_19/conv4/conv4_2\',\n          \'vgg_19/conv4/conv4_3\',\n          \'vgg_19/conv4/conv4_4\',\n          \'vgg_19/pool4\',\n          \'vgg_19/conv5/conv5_1\',\n          \'vgg_19/conv5/conv5_2\',\n          \'vgg_19/conv5/conv5_3\',\n          \'vgg_19/conv5/conv5_4\',\n          \'vgg_19/pool5\',\n          \'vgg_19/fc6\',\n          \'vgg_19/fc7\',\n          \'vgg_19/fc8\'\n      ]\n      self.assertSetEqual(set(end_points.keys()), set(expected_names))\n\n  def testModelVariables(self):\n    batch_size = 5\n    height, width = 224, 224\n    num_classes = 1000\n    with self.test_session():\n      inputs = tf.random_uniform((batch_size, height, width, 3))\n      vgg.vgg_19(inputs, num_classes)\n      expected_names = [\n          \'vgg_19/conv1/conv1_1/weights\',\n          \'vgg_19/conv1/conv1_1/biases\',\n          \'vgg_19/conv1/conv1_2/weights\',\n          \'vgg_19/conv1/conv1_2/biases\',\n          \'vgg_19/conv2/conv2_1/weights\',\n          \'vgg_19/conv2/conv2_1/biases\',\n          \'vgg_19/conv2/conv2_2/weights\',\n          \'vgg_19/conv2/conv2_2/biases\',\n          \'vgg_19/conv3/conv3_1/weights\',\n          \'vgg_19/conv3/conv3_1/biases\',\n          \'vgg_19/conv3/conv3_2/weights\',\n          \'vgg_19/conv3/conv3_2/biases\',\n          \'vgg_19/conv3/conv3_3/weights\',\n          \'vgg_19/conv3/conv3_3/biases\',\n          \'vgg_19/conv3/conv3_4/weights\',\n          \'vgg_19/conv3/conv3_4/biases\',\n          \'vgg_19/conv4/conv4_1/weights\',\n          \'vgg_19/conv4/conv4_1/biases\',\n          \'vgg_19/conv4/conv4_2/weights\',\n          \'vgg_19/conv4/conv4_2/biases\',\n          \'vgg_19/conv4/conv4_3/weights\',\n          \'vgg_19/conv4/conv4_3/biases\',\n          \'vgg_19/conv4/conv4_4/weights\',\n          \'vgg_19/conv4/conv4_4/biases\',\n          \'vgg_19/conv5/conv5_1/weights\',\n          \'vgg_19/conv5/conv5_1/biases\',\n          \'vgg_19/conv5/conv5_2/weights\',\n          \'vgg_19/conv5/conv5_2/biases\',\n          \'vgg_19/conv5/conv5_3/weights\',\n          \'vgg_19/conv5/conv5_3/biases\',\n          \'vgg_19/conv5/conv5_4/weights\',\n          \'vgg_19/conv5/conv5_4/biases\',\n          \'vgg_19/fc6/weights\',\n          \'vgg_19/fc6/biases\',\n          \'vgg_19/fc7/weights\',\n          \'vgg_19/fc7/biases\',\n          \'vgg_19/fc8/weights\',\n          \'vgg_19/fc8/biases\',\n      ]\n      model_variables = [v.op.name for v in slim.get_model_variables()]\n      self.assertSetEqual(set(model_variables), set(expected_names))\n\n  def testEvaluation(self):\n    batch_size = 2\n    height, width = 224, 224\n    num_classes = 1000\n    with self.test_session():\n      eval_inputs = tf.random_uniform((batch_size, height, width, 3))\n      logits, _ = vgg.vgg_19(eval_inputs, is_training=False)\n      self.assertListEqual(logits.get_shape().as_list(),\n                           [batch_size, num_classes])\n      predictions = tf.argmax(logits, 1)\n      self.assertListEqual(predictions.get_shape().as_list(), [batch_size])\n\n  def testTrainEvalWithReuse(self):\n    train_batch_size = 2\n    eval_batch_size = 1\n    train_height, train_width = 224, 224\n    eval_height, eval_width = 256, 256\n    num_classes = 1000\n    with self.test_session():\n      train_inputs = tf.random_uniform(\n          (train_batch_size, train_height, train_width, 3))\n      logits, _ = vgg.vgg_19(train_inputs)\n      self.assertListEqual(logits.get_shape().as_list(),\n                           [train_batch_size, num_classes])\n      tf.get_variable_scope().reuse_variables()\n      eval_inputs = tf.random_uniform(\n          (eval_batch_size, eval_height, eval_width, 3))\n      logits, _ = vgg.vgg_19(eval_inputs, is_training=False,\n                             spatial_squeeze=False)\n      self.assertListEqual(logits.get_shape().as_list(),\n                           [eval_batch_size, 2, 2, num_classes])\n      logits = tf.reduce_mean(logits, [1, 2])\n      predictions = tf.argmax(logits, 1)\n      self.assertEquals(predictions.get_shape().as_list(), [eval_batch_size])\n\n  def testForward(self):\n    batch_size = 1\n    height, width = 224, 224\n    with self.test_session() as sess:\n      inputs = tf.random_uniform((batch_size, height, width, 3))\n      logits, _ = vgg.vgg_19(inputs)\n      try:\n        sess.run(tf.global_variables_initializer())\n      except AttributeError:\n        sess.run(tf.initialize_all_variables())\n      output = sess.run(logits)\n      self.assertTrue(output.any())\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
finetuning/preprocessing/__init__.py,0,b'\n'
finetuning/preprocessing/inception_preprocessing.py,68,"b'# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Provides utilities to preprocess images for the Inception networks.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\n\nfrom tensorflow.python.ops import control_flow_ops\n\n\ndef apply_with_random_selector(x, func, num_cases):\n    """"""Computes func(x, sel), with sel sampled from [0...num_cases-1].\n\n  Args:\n    x: input Tensor.\n    func: Python function to apply.\n    num_cases: Python int32, number of cases to sample sel from.\n\n  Returns:\n    The result of func(x, sel), where func receives the value of the\n    selector as a python integer, but sel is sampled dynamically.\n  """"""\n    sel = tf.random_uniform([], maxval=num_cases, dtype=tf.int32)\n    # Pass the real x only to one of the func calls.\n    return control_flow_ops.merge([\n        func(control_flow_ops.switch(x, tf.equal(sel, case))[1], case)\n        for case in range(num_cases)\n    ])[0]\n\n\ndef distort_color(image, color_ordering=0, fast_mode=True, scope=None):\n    """"""Distort the color of a Tensor image.\n\n  Each color distortion is non-commutative and thus ordering of the color ops\n  matters. Ideally we would randomly permute the ordering of the color ops.\n  Rather then adding that level of complication, we select a distinct ordering\n  of color ops for each preprocessing thread.\n\n  Args:\n    image: 3-D Tensor containing single image in [0, 1].\n    color_ordering: Python int, a type of distortion (valid values: 0-3).\n    fast_mode: Avoids slower ops (random_hue and random_contrast)\n    scope: Optional scope for name_scope.\n  Returns:\n    3-D Tensor color-distorted image on range [0, 1]\n  Raises:\n    ValueError: if color_ordering not in [0, 3]\n  """"""\n    with tf.name_scope(scope, \'distort_color\', [image]):\n        if fast_mode:\n            if color_ordering == 0:\n                image = tf.image.random_brightness(image, max_delta=32. / 255.)\n                image = tf.image.random_saturation(image, lower=0.5, upper=1.5)\n            else:\n                image = tf.image.random_saturation(image, lower=0.5, upper=1.5)\n                image = tf.image.random_brightness(image, max_delta=32. / 255.)\n        else:\n            if color_ordering == 0:\n                image = tf.image.random_brightness(image, max_delta=32. / 255.)\n                image = tf.image.random_saturation(image, lower=0.5, upper=1.5)\n                image = tf.image.random_hue(image, max_delta=0.2)\n                image = tf.image.random_contrast(image, lower=0.5, upper=1.5)\n            elif color_ordering == 1:\n                image = tf.image.random_saturation(image, lower=0.5, upper=1.5)\n                image = tf.image.random_brightness(image, max_delta=32. / 255.)\n                image = tf.image.random_contrast(image, lower=0.5, upper=1.5)\n                image = tf.image.random_hue(image, max_delta=0.2)\n            elif color_ordering == 2:\n                image = tf.image.random_contrast(image, lower=0.5, upper=1.5)\n                image = tf.image.random_hue(image, max_delta=0.2)\n                image = tf.image.random_brightness(image, max_delta=32. / 255.)\n                image = tf.image.random_saturation(image, lower=0.5, upper=1.5)\n            elif color_ordering == 3:\n                image = tf.image.random_hue(image, max_delta=0.2)\n                image = tf.image.random_saturation(image, lower=0.5, upper=1.5)\n                image = tf.image.random_contrast(image, lower=0.5, upper=1.5)\n                image = tf.image.random_brightness(image, max_delta=32. / 255.)\n            else:\n                raise ValueError(\'color_ordering must be in [0, 3]\')\n\n        # The random_* ops do not necessarily clamp.\n        return tf.clip_by_value(image, 0.0, 1.0)\n\n\ndef distorted_bounding_box_crop(image,\n                                bbox,\n                                min_object_covered=0.1,\n                                aspect_ratio_range=(0.75, 1.33),\n                                area_range=(0.05, 1.0),\n                                max_attempts=100,\n                                scope=None):\n    """"""Generates cropped_image using a one of the bboxes randomly distorted.\n\n  See `tf.image.sample_distorted_bounding_box` for more documentation.\n\n  Args:\n    image: 3-D Tensor of image (it will be converted to floats in [0, 1]).\n    bbox: 3-D float Tensor of bounding boxes arranged [1, num_boxes, coords]\n      where each coordinate is [0, 1) and the coordinates are arranged\n      as [ymin, xmin, ymax, xmax]. If num_boxes is 0 then it would use the whole\n      image.\n    min_object_covered: An optional `float`. Defaults to `0.1`. The cropped\n      area of the image must contain at least this fraction of any bounding box\n      supplied.\n    aspect_ratio_range: An optional list of `floats`. The cropped area of the\n      image must have an aspect ratio = width / height within this range.\n    area_range: An optional list of `floats`. The cropped area of the image\n      must contain a fraction of the supplied image within in this range.\n    max_attempts: An optional `int`. Number of attempts at generating a cropped\n      region of the image of the specified constraints. After `max_attempts`\n      failures, return the entire image.\n    scope: Optional scope for name_scope.\n  Returns:\n    A tuple, a 3-D Tensor cropped_image and the distorted bbox\n  """"""\n    with tf.name_scope(scope, \'distorted_bounding_box_crop\', [image, bbox]):\n        # Each bounding box has shape [1, num_boxes, box coords] and\n        # the coordinates are ordered [ymin, xmin, ymax, xmax].\n\n        # A large fraction of image datasets contain a human-annotated bounding\n        # box delineating the region of the image containing the object of interest.\n        # We choose to create a new bounding box for the object which is a randomly\n        # distorted version of the human-annotated bounding box that obeys an\n        # allowed range of aspect ratios, sizes and overlap with the human-annotated\n        # bounding box. If no box is supplied, then we assume the bounding box is\n        # the entire image.\n        sample_distorted_bounding_box = tf.image.sample_distorted_bounding_box(\n            tf.shape(image),\n            bounding_boxes=bbox,\n            min_object_covered=min_object_covered,\n            aspect_ratio_range=aspect_ratio_range,\n            area_range=area_range,\n            max_attempts=max_attempts,\n            use_image_if_no_bounding_boxes=True)\n        bbox_begin, bbox_size, distort_bbox = sample_distorted_bounding_box\n\n        # Crop the image to the specified bounding box.\n        cropped_image = tf.slice(image, bbox_begin, bbox_size)\n        return cropped_image, distort_bbox\n\n\ndef preprocess_for_train(image,\n                         height,\n                         width,\n                         bbox,\n                         fast_mode=True,\n                         scope=None):\n    """"""Distort one image for training a network.\n\n  Distorting images provides a useful technique for augmenting the data\n  set during training in order to make the network invariant to aspects\n  of the image that do not effect the label.\n\n  Additionally it would create image_summaries to display the different\n  transformations applied to the image.\n\n  Args:\n    image: 3-D Tensor of image. If dtype is tf.float32 then the range should be\n      [0, 1], otherwise it would converted to tf.float32 assuming that the range\n      is [0, MAX], where MAX is largest positive representable number for\n      int(8/16/32) data type (see `tf.image.convert_image_dtype` for details).\n    height: integer\n    width: integer\n    bbox: 3-D float Tensor of bounding boxes arranged [1, num_boxes, coords]\n      where each coordinate is [0, 1) and the coordinates are arranged\n      as [ymin, xmin, ymax, xmax].\n    fast_mode: Optional boolean, if True avoids slower transformations (i.e.\n      bi-cubic resizing, random_hue or random_contrast).\n    scope: Optional scope for name_scope.\n  Returns:\n    3-D float Tensor of distorted image used for training with range [-1, 1].\n  """"""\n    with tf.name_scope(scope, \'distort_image\', [image, height, width, bbox]):\n        if bbox is None:\n            bbox = tf.constant(\n                [0.0, 0.0, 1.0, 1.0], dtype=tf.float32, shape=[1, 1, 4])\n        if image.dtype != tf.float32:\n            image = tf.image.convert_image_dtype(image, dtype=tf.float32)\n        # Each bounding box has shape [1, num_boxes, box coords] and\n        # the coordinates are ordered [ymin, xmin, ymax, xmax].\n        image_with_box = tf.image.draw_bounding_boxes(\n            tf.expand_dims(image, 0), bbox)\n        try:\n            tf.summary.image(\'image_with_bounding_boxes\', image_with_box)\n        except AttributeError:\n            tf.image_summary(\'image_with_bounding_boxes\', image_with_box)\n          \n\n        # distorted_image, distorted_bbox = distorted_bounding_box_crop(image, bbox)\n        # Restore the shape since the dynamic slice based upon the bbox_size loses\n        # the third dimension.\n        # distorted_image.set_shape([None, None, 3])\n        # image_with_distorted_box = tf.image.draw_bounding_boxes(\n        # tf.expand_dims(image, 0), distorted_bbox)\n        # tf.image_summary(\'images_with_distorted_bounding_box\',\n        #  image_with_distorted_box)\n\n        # This resizing operation may distort the images because the aspect\n        # ratio is not respected. We select a resize method in a round robin\n        # fashion based on the thread number.\n        # Note that ResizeMethod contains 4 enumerated resizing methods.\n\n        # We select only 1 case for fast_mode bilinear.\n        num_resize_cases = 1 if fast_mode else 4\n        distorted_image = apply_with_random_selector(\n            image,\n            lambda x, method: tf.image.resize_images(x, [height, width], method=method),\n            num_cases=num_resize_cases)\n        try:\n            tf.summary.image(\'cropped_resized_image\',\n                            tf.expand_dims(distorted_image, 0))\n        except AttributeError:\n            tf.image_summary(\'cropped_resized_image\',\n                            tf.expand_dims(distorted_image, 0))\n\n        # Randomly flip the image horizontally.\n        distorted_image = tf.image.random_flip_left_right(distorted_image)\n\n        # Randomly distort the colors. There are 4 ways to do it.\n        distorted_image = apply_with_random_selector(\n            distorted_image,\n            lambda x, ordering: distort_color(x, ordering, fast_mode),\n            num_cases=4)\n        try:\n            tf.summary.image(\'final_distorted_image\',\n                            tf.expand_dims(distorted_image, 0))\n        except AttributeError:\n            tf.image_summary(\'final_distorted_image\',\n                            tf.expand_dims(distorted_image, 0))\n        distorted_image = tf.subtract(distorted_image, 0.5)\n        distorted_image = tf.multiply(distorted_image, 2.0)\n        return distorted_image\n\n\ndef preprocess_for_eval(image, height, width, central_fraction=1.0,\n                        scope=None):\n    """"""Prepare one image for evaluation.\n\n  If height and width are specified it would output an image with that size by\n  applying resize_bilinear.\n\n  If central_fraction is specified it would cropt the central fraction of the\n  input image.\n\n  Args:\n    image: 3-D Tensor of image. If dtype is tf.float32 then the range should be\n      [0, 1], otherwise it would converted to tf.float32 assuming that the range\n      is [0, MAX], where MAX is largest positive representable number for\n      int(8/16/32) data type (see `tf.image.convert_image_dtype` for details)\n    height: integer\n    width: integer\n    central_fraction: Optional Float, fraction of the image to crop.\n    scope: Optional scope for name_scope.\n  Returns:\n    3-D float Tensor of prepared image.\n  """"""\n    with tf.name_scope(scope, \'eval_image\', [image, height, width]):\n        if image.dtype != tf.float32:\n            image = tf.image.convert_image_dtype(image, dtype=tf.float32)\n        # Crop the central region of the image with an area containing 87.5% of\n        # the original image.\n        if central_fraction:\n            image = tf.image.central_crop(\n                image, central_fraction=central_fraction)\n\n        if height and width:\n            # Resize the image to the specified height and width.\n            image = tf.expand_dims(image, 0)\n            image = tf.image.resize_bilinear(\n                image, [height, width], align_corners=False)\n            image = tf.squeeze(image, [0])\n        image = tf.subtract(image, 0.5)\n        image = tf.multiply(image, 2.0)\n        return image\n\n\ndef preprocess_image(image,\n                     height,\n                     width,\n                     is_training=False,\n                     bbox=None,\n                     fast_mode=True):\n    """"""Pre-process one image for training or evaluation.\n\n  Args:\n    image: 3-D Tensor [height, width, channels] with the image.\n    height: integer, image expected height.\n    width: integer, image expected width.\n    is_training: Boolean. If true it would transform an image for train,\n      otherwise it would transform it for evaluation.\n    bbox: 3-D float Tensor of bounding boxes arranged [1, num_boxes, coords]\n      where each coordinate is [0, 1) and the coordinates are arranged as\n      [ymin, xmin, ymax, xmax].\n    fast_mode: Optional boolean, if True avoids slower transformations.\n\n  Returns:\n    3-D float Tensor containing an appropriately scaled image\n\n  Raises:\n    ValueError: if user does not provide bounding box\n  """"""\n    if is_training:\n        return preprocess_for_train(image, height, width, bbox, fast_mode)\n    else:\n        return preprocess_for_eval(image, height, width)\n'"
finetuning/preprocessing/preprocessing_factory.py,1,"b'# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Contains a factory for building various models.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\n\nfrom preprocessing import inception_preprocessing\n\nslim = tf.contrib.slim\n\n\ndef get_preprocessing(name, is_training=False):\n    """"""Returns preprocessing_fn(image, height, width, **kwargs).\n\n  Args:\n    name: The name of the preprocessing function.\n    is_training: `True` if the model is being used for training and `False`\n      otherwise.\n\n  Returns:\n    preprocessing_fn: A function that preprocessing a single image (pre-batch).\n      It has the following signature:\n        image = preprocessing_fn(image, output_height, output_width, ...).\n\n  Raises:\n    ValueError: If Preprocessing `name` is not recognized.\n  """"""\n    preprocessing_fn_map = {\n        \'inception\': inception_preprocessing,\n        \'inception_v1\': inception_preprocessing,\n        \'inception_v2\': inception_preprocessing,\n        \'inception_v3\': inception_preprocessing,\n        \'inception_v4\': inception_preprocessing,\n        \'inception_resnet_v2\': inception_preprocessing\n    }\n\n    if name not in preprocessing_fn_map:\n        raise ValueError(\'Preprocessing name [%s] was not recognized\' % name)\n\n    def preprocessing_fn(image, output_height, output_width, **kwargs):\n        return preprocessing_fn_map[name].preprocess_image(\n            image,\n            output_height,\n            output_width,\n            is_training=is_training,\n            **kwargs)\n\n    return preprocessing_fn\n'"
machinelearning_toolkit/scripts/linear_classifier.py,48,"b'import tensorflow as tf\nimport pandas as pd\nfrom tensorflow.contrib.learn.python.learn.estimators import svm\n\nclass_of_worker = tf.contrib.layers.sparse_column_with_hash_bucket(\n    column_name=\'class_of_worker\', hash_bucket_size=1000)\n\ndetailed_industry_recode = tf.contrib.layers.sparse_column_with_hash_bucket(\n    column_name=\'detailed_industry_recode\', hash_bucket_size=1000)\n\ndetailed_occupation_recode = tf.contrib.layers.sparse_column_with_hash_bucket(\n    column_name=\'detailed_occupation_recode\', hash_bucket_size=1000)\n\neducation = tf.contrib.layers.sparse_column_with_hash_bucket(\n    column_name=\'education\', hash_bucket_size=1000)\n\nenroll_in_edu_inst_last_wk = tf.contrib.layers.sparse_column_with_hash_bucket(\n    column_name=\'enroll_in_edu_inst_last_wk\', hash_bucket_size=1000)\n\nmarital_stat = tf.contrib.layers.sparse_column_with_hash_bucket(\n    column_name=\'marital_stat\', hash_bucket_size=1000)\n\nmajor_industry_code = tf.contrib.layers.sparse_column_with_hash_bucket(\n    column_name=\'major_industry_code\', hash_bucket_size=1000)\n\nmajor_occupation_code = tf.contrib.layers.sparse_column_with_hash_bucket(\n    column_name=\'major_occupation_code\', hash_bucket_size=1000)\n\nrace = tf.contrib.layers.sparse_column_with_hash_bucket(\n    column_name=\'race\', hash_bucket_size=1000)\n\nhispanic_origin = tf.contrib.layers.sparse_column_with_hash_bucket(\n    column_name=\'hispanic_origin\', hash_bucket_size=1000)\nsex = tf.contrib.layers.sparse_column_with_keys(\n    column_name=\'sex\', keys=[\'Female\', \'Male\'])\n\nmember_of_labor_union = tf.contrib.layers.sparse_column_with_hash_bucket(\n    column_name=\'member_of_labor_union\', hash_bucket_size=1000)\nreason_for_unemployment = tf.contrib.layers.sparse_column_with_hash_bucket(\n    column_name=\'reason_for_unemployment\', hash_bucket_size=1000)\n\nfull_or_part_time_employment_stat = tf.contrib.layers.sparse_column_with_hash_bucket(\n    column_name=\'full_or_part_time_employment_stat\', hash_bucket_size=1000)\n\ntax_filer_stat = tf.contrib.layers.sparse_column_with_hash_bucket(\n    column_name=\'tax_filer_stat\', hash_bucket_size=1000)\n\nregion_of_previous_residence = tf.contrib.layers.sparse_column_with_hash_bucket(\n    column_name=\'region_of_previous_residence\', hash_bucket_size=1000)\n\nstate_of_previous_residence = tf.contrib.layers.sparse_column_with_hash_bucket(\n    column_name=\'state_of_previous_residence\', hash_bucket_size=1000)\n\ndetailed_household_and_family_stat = tf.contrib.layers.sparse_column_with_hash_bucket(\n    column_name=\'detailed_household_and_family_stat\', hash_bucket_size=1000)\n\ndetailed_household_summary_in_household = tf.contrib.layers.sparse_column_with_hash_bucket(\n    column_name=\'detailed_household_summary_in_household\',\n    hash_bucket_size=1000)\n\nmigration_code_change_in_msa = tf.contrib.layers.sparse_column_with_hash_bucket(\n    column_name=\'migration_code_change_in_msa\', hash_bucket_size=1000)\n\nmigration_code_change_in_msa = tf.contrib.layers.sparse_column_with_hash_bucket(\n    column_name=\'migration_code_change_in_msa\', hash_bucket_size=1000)\n\nmigration_code_change_in_reg = tf.contrib.layers.sparse_column_with_hash_bucket(\n    column_name=\'migration_code_change_in_reg\', hash_bucket_size=1000)\n\nmigration_code_move_within_reg = tf.contrib.layers.sparse_column_with_hash_bucket(\n    column_name=\'migration_code_move_within_reg\', hash_bucket_size=1000)\n\nlive_in_this_house_1year_ago = tf.contrib.layers.sparse_column_with_hash_bucket(\n    column_name=\'live_in_this_house_1year_ago\', hash_bucket_size=1000)\n\nmigration_prev_res_in_sunbelt = tf.contrib.layers.sparse_column_with_hash_bucket(\n    column_name=\'migration_prev_res_in_sunbelt\', hash_bucket_size=1000)\n\nfamily_members_under18 = tf.contrib.layers.sparse_column_with_hash_bucket(\n    column_name=\'family_members_under18\', hash_bucket_size=1000)\n\ncountry_of_birth_father = tf.contrib.layers.sparse_column_with_hash_bucket(\n    column_name=\'country_of_birth_father\', hash_bucket_size=1000)\n\ncountry_of_birth_mother = tf.contrib.layers.sparse_column_with_hash_bucket(\n    column_name=\'country_of_birth_mother\', hash_bucket_size=1000)\n\ncountry_of_birth_self = tf.contrib.layers.sparse_column_with_hash_bucket(\n    column_name=\'country_of_birth_self\', hash_bucket_size=1000)\n\ncitizenship = tf.contrib.layers.sparse_column_with_hash_bucket(\n    column_name=\'citizenship\', hash_bucket_size=1000)\n\nown_business_or_self_employed = tf.contrib.layers.sparse_column_with_hash_bucket(\n    column_name=\'own_business_or_self_employed\', hash_bucket_size=1000)\n\nfill_inc_questionnaire_for_veteran_admin = tf.contrib.layers.sparse_column_with_hash_bucket(\n    column_name=\'fill_inc_questionnaire_for_veteran_admin\',\n    hash_bucket_size=1000)\n\nveterans_benefits = tf.contrib.layers.sparse_column_with_hash_bucket(\n    column_name=\'veterans_benefits\', hash_bucket_size=1000)\nyear = tf.contrib.layers.sparse_column_with_keys(\n    column_name=\'year\', keys=[\'94\', \'95\'])\n\n# Continuous base columns\nage = tf.contrib.layers.real_valued_column(\'age\')\nage_buckets = tf.contrib.layers.bucketized_column(\n    age, boundaries=[18, 25, 30, 35, 40, 45, 50, 55, 60, 65])\nwage_per_hour = tf.contrib.layers.real_valued_column(\'wage_per_hour\')\ncapital_gains = tf.contrib.layers.real_valued_column(\'capital_gains\')\ncapital_losses = tf.contrib.layers.real_valued_column(\'capital_losses\')\ndividends_from_stocks = tf.contrib.layers.real_valued_column(\n    \'dividends_from_stocks\')\ninstance_weight = tf.contrib.layers.real_valued_column(\'instance_weight\')\nweeks_worked_in_year = tf.contrib.layers.real_valued_column(\n    \'weeks_worked_in_year\')\nnum_persons_worked_for_employer = tf.contrib.layers.real_valued_column(\n    \'num_persons_worked_for_employer\')\n\nCOLUMNS = [\n    \'age\', \'class_of_worker\', \'detailed_industry_recode\',\n    \'detailed_occupation_recode\', \'education\', \'wage_per_hour\',\n    \'enroll_in_edu_inst_last_wk\', \'marital_stat\', \'major_industry_code\',\n    \'major_occupation_code\', \'race\', \'hispanic_origin\', \'sex\',\n    \'member_of_labor_union\', \'reason_for_unemployment\',\n    \'full_or_part_time_employment_stat\', \'capital_gains\', \'capital_losses\',\n    \'dividends_from_stocks\', \'tax_filer_stat\', \'region_of_previous_residence\',\n    \'state_of_previous_residence\', \'detailed_household_and_family_stat\',\n    \'detailed_household_summary_in_household\', \'instance_weight\',\n    \'migration_code_change_in_msa\', \'migration_code_change_in_reg\',\n    \'migration_code_move_within_reg\', \'live_in_this_house_1year_ago\',\n    \'migration_prev_res_in_sunbelt\', \'num_persons_worked_for_employer\',\n    \'family_members_under18\', \'country_of_birth_father\',\n    \'country_of_birth_mother\', \'country_of_birth_self\', \'citizenship\',\n    \'own_business_or_self_employed\',\n    \'fill_inc_questionnaire_for_veteran_admin\', \'veterans_benefits\',\n    \'weeks_worked_in_year\', \'year\', \'label\'\n]\nFEATURE_COLUMNS = [\n    age, age_buckets, class_of_worker, detailed_industry_recode,\n    detailed_occupation_recode, education, wage_per_hour,\n    enroll_in_edu_inst_last_wk, marital_stat, major_industry_code,\n    major_occupation_code, race, hispanic_origin, sex, member_of_labor_union,\n    reason_for_unemployment, full_or_part_time_employment_stat, capital_gains,\n    capital_losses, dividends_from_stocks, tax_filer_stat,\n    region_of_previous_residence, state_of_previous_residence,\n    detailed_household_and_family_stat,\n    detailed_household_summary_in_household, instance_weight,\n    migration_code_change_in_msa, migration_code_change_in_reg,\n    migration_code_move_within_reg, live_in_this_house_1year_ago,\n    migration_prev_res_in_sunbelt, num_persons_worked_for_employer,\n    family_members_under18, country_of_birth_father, country_of_birth_mother,\n    country_of_birth_self, citizenship, own_business_or_self_employed,\n    fill_inc_questionnaire_for_veteran_admin, veterans_benefits,\n    weeks_worked_in_year, year\n]\n\nLABEL_COLUMN = \'label\'\nCONTINUOUS_COLUMNS = [\n    \'age\', \'wage_per_hour\', \'capital_gains\', \'capital_losses\',\n    \'dividends_from_stocks\', \'instance_weight\', \'weeks_worked_in_year\',\n    \'num_persons_worked_for_employer\'\n]\nCATEGORICAL_COLUMNS = [\n    \'class_of_worker\', \'detailed_industry_recode\',\n    \'detailed_occupation_recode\', \'education\', \'enroll_in_edu_inst_last_wk\',\n    \'marital_stat\', \'major_industry_code\', \'major_occupation_code\', \'race\',\n    \'hispanic_origin\', \'sex\', \'member_of_labor_union\',\n    \'reason_for_unemployment\', \'full_or_part_time_employment_stat\',\n    \'tax_filer_stat\', \'region_of_previous_residence\',\n    \'state_of_previous_residence\', \'detailed_household_and_family_stat\',\n    \'detailed_household_summary_in_household\', \'migration_code_change_in_msa\',\n    \'migration_code_change_in_reg\', \'migration_code_move_within_reg\',\n    \'live_in_this_house_1year_ago\', \'migration_prev_res_in_sunbelt\',\n    \'family_members_under18\', \'country_of_birth_father\',\n    \'country_of_birth_mother\', \'country_of_birth_self\', \'citizenship\',\n    \'own_business_or_self_employed\',\n    \'fill_inc_questionnaire_for_veteran_admin\', \'veterans_benefits\', \'year\'\n]\n\nTRAIN_FILE = \'../data/census/census-income.data\'\nTEST_FILE = \'../data/census/census-income.test\'\n\ndf_train = pd.read_csv(TRAIN_FILE, names=COLUMNS, skipinitialspace=True)\ndf_test = pd.read_csv(TEST_FILE, names=COLUMNS, skipinitialspace=True)\ndf_train = df_train.dropna(how=\'any\', axis=0)\ndf_test = df_test.dropna(how=\'any\', axis=0)\ndf_train[[\n    \'detailed_industry_recode\', \'detailed_occupation_recode\', \'year\',\n    \'own_business_or_self_employed\', \'veterans_benefits\'\n]] = df_train[[\n    \'detailed_industry_recode\', \'detailed_occupation_recode\', \'year\',\n    \'own_business_or_self_employed\', \'veterans_benefits\'\n]].astype(str)\ndf_test[[\n    \'detailed_industry_recode\', \'detailed_occupation_recode\', \'year\',\n    \'own_business_or_self_employed\', \'veterans_benefits\'\n]] = df_test[[\n    \'detailed_industry_recode\', \'detailed_occupation_recode\', \'year\',\n    \'own_business_or_self_employed\', \'veterans_benefits\'\n]].astype(str)\n\ndf_train[LABEL_COLUMN] = (\n    df_train[LABEL_COLUMN].apply(lambda x: \'+\' in x)).astype(int)\ndf_test[LABEL_COLUMN] = (\n    df_test[LABEL_COLUMN].apply(lambda x: \'+\' in x)).astype(int)\ndtypess = df_train.dtypes\n\n\ndef input_fn(df):\n    # Creates a dictionary mapping from each continuous feature column name (k) to\n    # # the values of that column stored in a constant Tensor.\n    continuous_cols = {\n        k: tf.constant(df[k].values)\n        for k in CONTINUOUS_COLUMNS\n    }\n    # Creates a dictionary mapping from each categorical feature column name (k)\n    # to the values of that column stored in a tf.SparseTensor.\n    categorical_cols = {\n        k: tf.SparseTensor(\n            indices=[[i, 0] for i in range(df[k].size)],\n            values=df[k].values,\n            dense_shape=[df[k].size, 1])\n        for k in CATEGORICAL_COLUMNS\n    }\n    # Merges the two dictionaries into one.\n    feature_cols = dict(continuous_cols.items() + categorical_cols.items())\n    # Converts the label column into a constant Tensor.\n    label = tf.constant(df[LABEL_COLUMN].values)\n    # Returns the feature columns and the label.\n    return feature_cols, label\n\n\ndef train_input_fn():\n    return input_fn(df_train)\n\n\ndef eval_input_fn():\n    return input_fn(df_test)\n\n\nmodel_dir = \'../model_dir\'\n\nmodel = tf.contrib.learn.LinearClassifier(\n    feature_columns=FEATURE_COLUMNS, model_dir=model_dir)\nmodel.fit(input_fn=train_input_fn, steps=200)\nresults = model.evaluate(input_fn=eval_input_fn, steps=1)\nfor key in sorted(results):\n    print(""%s: %s"" % (key, results[key]))\n'"
machinelearning_toolkit/scripts/simple-tf-rf.py,9,"b'import tensorflow as tf\nimport pandas as pd\nfrom tensorflow.contrib.tensor_forest.client import random_forest\n\ndetailed_occupation_recode = tf.contrib.layers.sparse_column_with_hash_bucket(\n    column_name=\'detailed_occupation_recode\', hash_bucket_size=1000)\neducation = tf.contrib.layers.sparse_column_with_hash_bucket(\n    column_name=\'education\', hash_bucket_size=1000)\n# Continuous base columns\nage = tf.contrib.layers.real_valued_column(\'age\')\nwage_per_hour = tf.contrib.layers.real_valued_column(\'wage_per_hour\')\n\ncolumns = [\n    \'age\', \'detailed_occupation_recode\', \'education\', \'wage_per_hour\', \'label\'\n]\nFEATURE_COLUMNS = [\n    # age, age_buckets, class_of_worker, detailed_industry_recode,\n    age,\n    detailed_occupation_recode,\n    education,\n    wage_per_hour\n]\n\nLABEL_COLUMN = \'label\'\n\nCONTINUOUS_COLUMNS = [\'age\', \'wage_per_hour\']\n\nCATEGORICAL_COLUMNS = [\'detailed_occupation_recode\', \'education\']\n\ndf_train = pd.DataFrame(\n    [[12, \'12\', \'7th and 8th grade\', 40, \'- 50000\'],\n     [40, \'45\', \'7th and 8th grade\', 40, \'50000+\'],\n     [50, \'50\', \'10th grade\', 40, \'50000+\'],\n     [60, \'30\', \'7th and 8th grade\', 40, \'- 50000\']],\n    columns=[\n        \'age\', \'detailed_occupation_recode\', \'education\', \'wage_per_hour\',\n        \'label\'\n    ])\n\ndf_test = pd.DataFrame(\n    [[12, \'12\', \'7th and 8th grade\', 40, \'- 50000\'],\n     [40, \'45\', \'7th and 8th grade\', 40, \'50000+\'],\n     [50, \'50\', \'10th grade\', 40, \'50000+\'],\n     [60, \'30\', \'7th and 8th grade\', 40, \'- 50000\']],\n    columns=[\n        \'age\', \'detailed_occupation_recode\', \'education\', \'wage_per_hour\',\n        \'label\'\n    ])\ndf_train[LABEL_COLUMN] = (\n    df_train[LABEL_COLUMN].apply(lambda x: \'+\' in x)).astype(int)\ndf_test[LABEL_COLUMN] = (\n    df_test[LABEL_COLUMN].apply(lambda x: \'+\' in x)).astype(int)\ndtypess = df_train.dtypes\n\nprint df_train\nprint df_test\n\n\ndef input_fn(df):\n    continuous_cols = {\n        k: tf.expand_dims(tf.constant(df[k].values), 1)\n        for k in CONTINUOUS_COLUMNS\n    }\n    # continuous_cols = {\n    #     k: tf.constant(df[k].values)\n    #     for k in CONTINUOUS_COLUMNS\n    # }\n    categorical_cols = {\n        k: tf.SparseTensor(\n            indices=[[i, 0] for i in range(df[k].size)],\n            values=df[k].values,\n            dense_shape=[df[k].size, 1])\n        for k in CATEGORICAL_COLUMNS\n    }\n    feature_cols = dict(continuous_cols.items() + categorical_cols.items())\n    label = tf.constant(df[LABEL_COLUMN].values)\n    return feature_cols, label\n\n\ndef train_input_fn():\n    return input_fn(df_train)\n\n\ndef eval_input_fn():\n    return input_fn(df_test)\n\n\nmodel_dir = \'../rf_model_dir\'\n\nhparams = tf.contrib.tensor_forest.python.tensor_forest.ForestHParams(\n    num_trees=10, max_nodes=1000, num_classes=2, num_features=4)\nclassifier = random_forest.TensorForestEstimator(hparams, model_dir=model_dir)\nclassifier.fit(input_fn=train_input_fn, steps=100)\nresults = classifier.evaluate(input_fn=eval_input_fn, steps=1)\nprint results\nfor key in sorted(results):\n    print(""%s: %s"" % (key, results[key]))\n'"
machinelearning_toolkit/scripts/simple-tf-svm.py,9,"b'import tensorflow as tf\nimport pandas as pd\nfrom tensorflow.contrib.learn.python.learn.estimators import svm\n\ndetailed_occupation_recode = tf.contrib.layers.sparse_column_with_hash_bucket(\n    column_name=\'detailed_occupation_recode\', hash_bucket_size=1000)\neducation = tf.contrib.layers.sparse_column_with_hash_bucket(\n    column_name=\'education\', hash_bucket_size=1000)\n# Continuous base columns\nage = tf.contrib.layers.real_valued_column(\'age\')\nwage_per_hour = tf.contrib.layers.real_valued_column(\'wage_per_hour\')\n\ncolumns = [\n    \'age\', \'detailed_occupation_recode\', \'education\', \'wage_per_hour\', \'label\'\n]\nFEATURE_COLUMNS = [\n    # age, age_buckets, class_of_worker, detailed_industry_recode,\n    age,\n    detailed_occupation_recode,\n    education,\n    wage_per_hour\n]\n\nLABEL_COLUMN = \'label\'\n\nCONTINUOUS_COLUMNS = [\'age\', \'wage_per_hour\']\n\nCATEGORICAL_COLUMNS = [\'detailed_occupation_recode\', \'education\']\n\ndf_train = pd.DataFrame(\n    [[12, \'12\', \'7th and 8th grade\', 40, \'- 50000\'],\n     [40, \'45\', \'7th and 8th grade\', 40, \'50000+\'],\n     [50, \'50\', \'10th grade\', 40, \'50000+\'],\n     [60, \'30\', \'7th and 8th grade\', 40, \'- 50000\']],\n    columns=[\n        \'age\', \'detailed_occupation_recode\', \'education\', \'wage_per_hour\',\n        \'label\'\n    ])\n\ndf_test = pd.DataFrame(\n    [[12, \'12\', \'7th and 8th grade\', 40, \'- 50000\'],\n     [40, \'45\', \'7th and 8th grade\', 40, \'50000+\'],\n     [50, \'50\', \'10th grade\', 40, \'50000+\'],\n     [60, \'30\', \'7th and 8th grade\', 40, \'- 50000\']],\n    columns=[\n        \'age\', \'detailed_occupation_recode\', \'education\', \'wage_per_hour\',\n        \'label\'\n    ])\ndf_train[LABEL_COLUMN] = (\n    df_train[LABEL_COLUMN].apply(lambda x: \'+\' in x)).astype(int)\ndf_test[LABEL_COLUMN] = (\n    df_test[LABEL_COLUMN].apply(lambda x: \'+\' in x)).astype(int)\ndtypess = df_train.dtypes\n\n\ndef input_fn(df):\n    # continuous_cols = {k: tf.constant(df[k].values) for k in CONTINUOUS_COLUMNS}\n    continuous_cols = {\n        k: tf.expand_dims(tf.constant(df[k].values), 1)\n        for k in CONTINUOUS_COLUMNS\n    }\n    categorical_cols = {\n        k: tf.SparseTensor(\n            indices=[[i, 0] for i in range(df[k].size)],\n            values=df[k].values,\n            dense_shape=[df[k].size, 1])\n        for k in CATEGORICAL_COLUMNS\n    }\n    feature_cols = dict(continuous_cols.items() + categorical_cols.items())\n    feature_cols[\'example_id\'] = tf.constant(\n        [str(i + 1) for i in range(df[\'age\'].size)])\n    label = tf.constant(df[LABEL_COLUMN].values)\n    return feature_cols, label\n\n\ndef train_input_fn():\n    return input_fn(df_train)\n\n\ndef eval_input_fn():\n    return input_fn(df_test)\n\n\nmodel_dir = \'../svm_model_dir\'\n\nmodel = svm.SVM(example_id_column=\'example_id\',\n                feature_columns=FEATURE_COLUMNS,\n                model_dir=model_dir)\nmodel.fit(input_fn=train_input_fn, steps=10)\nresults = model.evaluate(input_fn=eval_input_fn, steps=1)\nfor key in sorted(results):\n    print(""%s: %s"" % (key, results[key]))\n'"
machinelearning_toolkit/scripts/tf-rf.py,56,"b'import tensorflow as tf\nimport pandas as pd\nimport numpy as np\nfrom tensorflow.contrib.tensor_forest.client import random_forest\ntf.logging.set_verbosity(tf.logging.INFO)\n\n# categorical base columns\nclass_of_worker = tf.contrib.layers.sparse_column_with_hash_bucket(\n    column_name=\'class_of_worker\', hash_bucket_size=1000)\ndetailed_industry_recode = tf.contrib.layers.sparse_column_with_hash_bucket(\n    column_name=\'detailed_industry_recode\', hash_bucket_size=1000)\n\ndetailed_occupation_recode = tf.contrib.layers.sparse_column_with_hash_bucket(\n    column_name=\'detailed_occupation_recode\', hash_bucket_size=1000)\n\neducation = tf.contrib.layers.sparse_column_with_hash_bucket(\n    column_name=\'education\', hash_bucket_size=1000)\n\nenroll_in_edu_inst_last_wk = tf.contrib.layers.sparse_column_with_hash_bucket(\n    column_name=\'enroll_in_edu_inst_last_wk\', hash_bucket_size=1000)\n\nmarital_stat = tf.contrib.layers.sparse_column_with_hash_bucket(\n    column_name=\'marital_stat\', hash_bucket_size=1000)\n\nmajor_industry_code = tf.contrib.layers.sparse_column_with_hash_bucket(\n    column_name=\'major_industry_code\', hash_bucket_size=1000)\n\nmajor_occupation_code = tf.contrib.layers.sparse_column_with_hash_bucket(\n    column_name=\'major_occupation_code\', hash_bucket_size=1000)\n\nrace = tf.contrib.layers.sparse_column_with_hash_bucket(\n    column_name=\'race\', hash_bucket_size=1000)\n\nhispanic_origin = tf.contrib.layers.sparse_column_with_hash_bucket(\n    column_name=\'hispanic_origin\', hash_bucket_size=1000)\nsex = tf.contrib.layers.sparse_column_with_keys(\n    column_name=\'sex\', keys=[\'Female\', \'Male\'])\n\nmember_of_labor_union = tf.contrib.layers.sparse_column_with_hash_bucket(\n    column_name=\'member_of_labor_union\', hash_bucket_size=1000)\n\nreason_for_unemployment = tf.contrib.layers.sparse_column_with_hash_bucket(\n    column_name=\'reason_for_unemployment\', hash_bucket_size=1000)\n\nfull_or_part_time_employment_stat = tf.contrib.layers.sparse_column_with_hash_bucket(\n    column_name=\'full_or_part_time_employment_stat\', hash_bucket_size=1000)\n\ntax_filer_stat = tf.contrib.layers.sparse_column_with_hash_bucket(\n    column_name=\'tax_filer_stat\', hash_bucket_size=1000)\n\nregion_of_previous_residence = tf.contrib.layers.sparse_column_with_hash_bucket(\n    column_name=\'region_of_previous_residence\', hash_bucket_size=1000)\n\nstate_of_previous_residence = tf.contrib.layers.sparse_column_with_hash_bucket(\n    column_name=\'state_of_previous_residence\', hash_bucket_size=1000)\n\ndetailed_household_and_family_stat = tf.contrib.layers.sparse_column_with_hash_bucket(\n    column_name=\'detailed_household_and_family_stat\', hash_bucket_size=1000)\n\ndetailed_household_summary_in_household = tf.contrib.layers.sparse_column_with_hash_bucket(\n    column_name=\'detailed_household_summary_in_household\',\n    hash_bucket_size=1000)\n\nmigration_code_change_in_msa = tf.contrib.layers.sparse_column_with_hash_bucket(\n    column_name=\'migration_code_change_in_msa\', hash_bucket_size=1000)\n\nmigration_code_change_in_msa = tf.contrib.layers.sparse_column_with_hash_bucket(\n    column_name=\'migration_code_change_in_msa\', hash_bucket_size=1000)\n\nmigration_code_change_in_reg = tf.contrib.layers.sparse_column_with_hash_bucket(\n    column_name=\'migration_code_change_in_reg\', hash_bucket_size=1000)\n\nmigration_code_move_within_reg = tf.contrib.layers.sparse_column_with_hash_bucket(\n    column_name=\'migration_code_move_within_reg\', hash_bucket_size=1000)\n\nlive_in_this_house_1year_ago = tf.contrib.layers.sparse_column_with_hash_bucket(\n    column_name=\'live_in_this_house_1year_ago\', hash_bucket_size=1000)\n\nmigration_prev_res_in_sunbelt = tf.contrib.layers.sparse_column_with_hash_bucket(\n    column_name=\'migration_prev_res_in_sunbelt\', hash_bucket_size=1000)\n\nfamily_members_under18 = tf.contrib.layers.sparse_column_with_hash_bucket(\n    column_name=\'family_members_under18\', hash_bucket_size=1000)\n\ncountry_of_birth_father = tf.contrib.layers.sparse_column_with_hash_bucket(\n    column_name=\'country_of_birth_father\', hash_bucket_size=1000)\n\ncountry_of_birth_mother = tf.contrib.layers.sparse_column_with_hash_bucket(\n    column_name=\'country_of_birth_mother\', hash_bucket_size=1000)\n\ncountry_of_birth_self = tf.contrib.layers.sparse_column_with_hash_bucket(\n    column_name=\'country_of_birth_self\', hash_bucket_size=1000)\n\ncitizenship = tf.contrib.layers.sparse_column_with_hash_bucket(\n    column_name=\'citizenship\', hash_bucket_size=1000)\n\nown_business_or_self_employed = tf.contrib.layers.sparse_column_with_hash_bucket(\n    column_name=\'own_business_or_self_employed\', hash_bucket_size=1000)\n\nfill_inc_questionnaire_for_veteran_admin = tf.contrib.layers.sparse_column_with_hash_bucket(\n    column_name=\'fill_inc_questionnaire_for_veteran_admin\',\n    hash_bucket_size=1000)\n\nveterans_benefits = tf.contrib.layers.sparse_column_with_hash_bucket(\n    column_name=\'veterans_benefits\', hash_bucket_size=1000)\nyear = tf.contrib.layers.sparse_column_with_keys(\n    column_name=\'year\', keys=[\'94\', \'95\'])\n\n# Continuous base columns\nage = tf.contrib.layers.real_valued_column(\'age\')\nage_buckets = tf.contrib.layers.bucketized_column(\n    age, boundaries=[18, 25, 30, 35, 40, 45, 50, 55, 60, 65])\nwage_per_hour = tf.contrib.layers.real_valued_column(\'wage_per_hour\')\ncapital_gains = tf.contrib.layers.real_valued_column(\'capital_gains\')\ncapital_losses = tf.contrib.layers.real_valued_column(\'capital_losses\')\ndividends_from_stocks = tf.contrib.layers.real_valued_column(\n    \'dividends_from_stocks\')\ninstance_weight = tf.contrib.layers.real_valued_column(\'instance_weight\')\nweeks_worked_in_year = tf.contrib.layers.real_valued_column(\n    \'weeks_worked_in_year\')\nnum_persons_worked_for_employer = tf.contrib.layers.real_valued_column(\n    \'num_persons_worked_for_employer\')\n\nCOLUMNS = [\n    \'age\', \'class_of_worker\', \'detailed_industry_recode\',\n    \'detailed_occupation_recode\', \'education\', \'wage_per_hour\',\n    \'enroll_in_edu_inst_last_wk\', \'marital_stat\', \'major_industry_code\',\n    \'major_occupation_code\', \'race\', \'hispanic_origin\', \'sex\',\n    \'member_of_labor_union\', \'reason_for_unemployment\',\n    \'full_or_part_time_employment_stat\', \'capital_gains\', \'capital_losses\',\n    \'dividends_from_stocks\', \'tax_filer_stat\', \'region_of_previous_residence\',\n    \'state_of_previous_residence\', \'detailed_household_and_family_stat\',\n    \'detailed_household_summary_in_household\', \'instance_weight\',\n    \'migration_code_change_in_msa\', \'migration_code_change_in_reg\',\n    \'migration_code_move_within_reg\', \'live_in_this_house_1year_ago\',\n    \'migration_prev_res_in_sunbelt\', \'num_persons_worked_for_employer\',\n    \'family_members_under18\', \'country_of_birth_father\',\n    \'country_of_birth_mother\', \'country_of_birth_self\', \'citizenship\',\n    \'own_business_or_self_employed\',\n    \'fill_inc_questionnaire_for_veteran_admin\', \'veterans_benefits\',\n    \'weeks_worked_in_year\', \'year\', \'label\'\n]\nFEATURE_COLUMNS = [\n    age, age_buckets, class_of_worker, detailed_industry_recode,\n    detailed_occupation_recode, education, wage_per_hour,\n    enroll_in_edu_inst_last_wk, marital_stat, major_industry_code,\n    major_occupation_code, race, hispanic_origin, sex, member_of_labor_union,\n    reason_for_unemployment, full_or_part_time_employment_stat, capital_gains,\n    capital_losses, dividends_from_stocks, tax_filer_stat,\n    region_of_previous_residence, state_of_previous_residence,\n    detailed_household_and_family_stat,\n    detailed_household_summary_in_household, instance_weight,\n    migration_code_change_in_msa, migration_code_change_in_reg,\n    migration_code_move_within_reg, live_in_this_house_1year_ago,\n    migration_prev_res_in_sunbelt, num_persons_worked_for_employer,\n    family_members_under18, country_of_birth_father, country_of_birth_mother,\n    country_of_birth_self, citizenship, own_business_or_self_employed,\n    fill_inc_questionnaire_for_veteran_admin, veterans_benefits,\n    weeks_worked_in_year, year\n]\n# FEATURE_COLUMNS = [age, detailed_occupation_recode, education, wage_per_hour]\n\nLABEL_COLUMN = \'label\'\nCONTINUOUS_COLUMNS = [\n    \'age\', \'wage_per_hour\', \'capital_gains\', \'capital_losses\',\n    \'dividends_from_stocks\', \'instance_weight\', \'weeks_worked_in_year\',\n    \'num_persons_worked_for_employer\'\n]\nCATEGORICAL_COLUMNS = [\n    \'class_of_worker\', \'detailed_industry_recode\',\n    \'detailed_occupation_recode\', \'education\', \'enroll_in_edu_inst_last_wk\',\n    \'marital_stat\', \'major_industry_code\', \'major_occupation_code\', \'race\',\n    \'hispanic_origin\', \'sex\', \'member_of_labor_union\',\n    \'reason_for_unemployment\', \'full_or_part_time_employment_stat\',\n    \'tax_filer_stat\', \'region_of_previous_residence\',\n    \'state_of_previous_residence\', \'detailed_household_and_family_stat\',\n    \'detailed_household_summary_in_household\', \'migration_code_change_in_msa\',\n    \'migration_code_change_in_reg\', \'migration_code_move_within_reg\',\n    \'live_in_this_house_1year_ago\', \'migration_prev_res_in_sunbelt\',\n    \'family_members_under18\', \'country_of_birth_father\',\n    \'country_of_birth_mother\', \'country_of_birth_self\', \'citizenship\',\n    \'own_business_or_self_employed\',\n    \'fill_inc_questionnaire_for_veteran_admin\', \'veterans_benefits\', \'year\'\n]\n\nTRAIN_FILE = \'../data/census/census-income.data\'\nTEST_FILE = \'../data/census/census-income.test\'\n\ndf_train = pd.read_csv(TRAIN_FILE, names=COLUMNS, skipinitialspace=True)\ndf_test = pd.read_csv(TEST_FILE, names=COLUMNS, skipinitialspace=True)\ndf_train = df_train.dropna(how=\'any\', axis=0)\ndf_test = df_test.dropna(how=\'any\', axis=0)\ndf_train[[\n    \'detailed_industry_recode\', \'detailed_occupation_recode\', \'year\',\n    \'own_business_or_self_employed\', \'veterans_benefits\'\n]] = df_train[[\n    \'detailed_industry_recode\', \'detailed_occupation_recode\', \'year\',\n    \'own_business_or_self_employed\', \'veterans_benefits\'\n]].astype(str)\ndf_test[[\n    \'detailed_industry_recode\', \'detailed_occupation_recode\', \'year\',\n    \'own_business_or_self_employed\', \'veterans_benefits\'\n]] = df_test[[\n    \'detailed_industry_recode\', \'detailed_occupation_recode\', \'year\',\n    \'own_business_or_self_employed\', \'veterans_benefits\'\n]].astype(str)\n\ndf_train[LABEL_COLUMN] = (\n    df_train[LABEL_COLUMN].apply(lambda x: \'+\' in x)).astype(int)\ndf_test[LABEL_COLUMN] = (\n    df_test[LABEL_COLUMN].apply(lambda x: \'+\' in x)).astype(int)\n# print df_train.dtypes\ndtypess = df_train.dtypes\n\n# print dtypess[CATEGORICAL_COLUMNS]\n\nprint df_train.head(5)\nprint df_test.head(5)\n\n\ndef input_fn(df):\n    continuous_cols = {\n        k: tf.expand_dims(tf.constant(df[k].astype(np.float32).values), 1)\n        for k in CONTINUOUS_COLUMNS\n    }\n    # Creates a dictionary mapping from each categorical feature column name (k)\n    # to the values of that column stored in a tf.SparseTensor.\n    categorical_cols = {\n        k: tf.SparseTensor(\n            indices=[[i, 0] for i in range(df[k].size)],\n            values=df[k].values,\n            dense_shape=[df[k].size, 1])\n        for k in CATEGORICAL_COLUMNS\n    }\n    # Merges the two dictionaries into one.\n    feature_cols = dict(continuous_cols.items() + categorical_cols.items())\n    # Add example id list\n    # Converts the label column into a constant Tensor.\n    label = tf.constant(df[LABEL_COLUMN].values)\n    # Returns the feature columns and the label.\n    return feature_cols, label\n\n\ndef train_input_fn():\n    return input_fn(df_train)\n\n\ndef eval_input_fn():\n    return input_fn(df_test)\n\n\nmodel_dir = \'../rf_model_dir\'\nvalidation_metrics = {\n    ""accuracy"":\n    tf.contrib.learn.MetricSpec(\n        metric_fn=tf.contrib.metrics.streaming_accuracy,\n        prediction_key=\'probabilities\'\n        ),\n    ""precision"":\n    tf.contrib.learn.MetricSpec(\n         metric_fn=tf.contrib.metrics.streaming_precision,\n         prediction_key=\'probabilities\'\n         ),\n    ""recall"":\n    tf.contrib.learn.MetricSpec(\n        metric_fn=tf.contrib.metrics.streaming_recall,\n        prediction_key=\'probabilities\'\n        )\n    }\n\nhparams = tf.contrib.tensor_forest.python.tensor_forest.ForestHParams(\n    num_trees=10,\n    max_nodes=1000,\n    num_classes=2,\n    num_features=len(CONTINUOUS_COLUMNS) + len(CATEGORICAL_COLUMNS))\nclassifier = random_forest.TensorForestEstimator(hparams, model_dir=model_dir, config=tf.contrib.learn.RunConfig(save_checkpoints_secs=60))\n\nclassifier.fit(input_fn=train_input_fn, steps=200)\nresults = classifier.evaluate(\n    input_fn=eval_input_fn, steps=1, metrics=validation_metrics)\nprint results\nfor key in sorted(results):\n    print(""%s: %s"" % (key, results[key]))\n'"
machinelearning_toolkit/scripts/tf-svm.py,49,"b'import tensorflow as tf\nimport pandas as pd\nfrom tensorflow.contrib.learn.python.learn.estimators import svm\n\nclass_of_worker = tf.contrib.layers.sparse_column_with_hash_bucket(\n    column_name=\'class_of_worker\', hash_bucket_size=1000)\n\ndetailed_industry_recode = tf.contrib.layers.sparse_column_with_hash_bucket(\n    column_name=\'detailed_industry_recode\', hash_bucket_size=1000)\n\ndetailed_occupation_recode = tf.contrib.layers.sparse_column_with_hash_bucket(\n    column_name=\'detailed_occupation_recode\', hash_bucket_size=1000)\n\neducation = tf.contrib.layers.sparse_column_with_hash_bucket(\n    column_name=\'education\', hash_bucket_size=1000)\n\nenroll_in_edu_inst_last_wk = tf.contrib.layers.sparse_column_with_hash_bucket(\n    column_name=\'enroll_in_edu_inst_last_wk\', hash_bucket_size=1000)\n\nmarital_stat = tf.contrib.layers.sparse_column_with_hash_bucket(\n    column_name=\'marital_stat\', hash_bucket_size=1000)\n\nmajor_industry_code = tf.contrib.layers.sparse_column_with_hash_bucket(\n    column_name=\'major_industry_code\', hash_bucket_size=1000)\n\nmajor_occupation_code = tf.contrib.layers.sparse_column_with_hash_bucket(\n    column_name=\'major_occupation_code\', hash_bucket_size=1000)\n\nrace = tf.contrib.layers.sparse_column_with_hash_bucket(\n    column_name=\'race\', hash_bucket_size=1000)\n\nhispanic_origin = tf.contrib.layers.sparse_column_with_hash_bucket(\n    column_name=\'hispanic_origin\', hash_bucket_size=1000)\nsex = tf.contrib.layers.sparse_column_with_keys(\n    column_name=\'sex\', keys=[\'Female\', \'Male\'])\n\n\nmember_of_labor_union = tf.contrib.layers.sparse_column_with_hash_bucket(\n    column_name=\'member_of_labor_union\', hash_bucket_size=1000)\n\nreason_for_unemployment = tf.contrib.layers.sparse_column_with_hash_bucket(\n    column_name=\'reason_for_unemployment\', hash_bucket_size=1000)\n\nfull_or_part_time_employment_stat = tf.contrib.layers.sparse_column_with_hash_bucket(\n    column_name=\'full_or_part_time_employment_stat\', hash_bucket_size=1000)\n\ntax_filer_stat = tf.contrib.layers.sparse_column_with_hash_bucket(\n    column_name=\'tax_filer_stat\', hash_bucket_size=1000)\n\nregion_of_previous_residence = tf.contrib.layers.sparse_column_with_hash_bucket(\n    column_name=\'region_of_previous_residence\', hash_bucket_size=1000)\n\nstate_of_previous_residence = tf.contrib.layers.sparse_column_with_hash_bucket(\n    column_name=\'state_of_previous_residence\', hash_bucket_size=1000)\n\ndetailed_household_and_family_stat = tf.contrib.layers.sparse_column_with_hash_bucket(\n    column_name=\'detailed_household_and_family_stat\', hash_bucket_size=1000)\n\ndetailed_household_summary_in_household = tf.contrib.layers.sparse_column_with_hash_bucket(\n    column_name=\'detailed_household_summary_in_household\',\n    hash_bucket_size=1000)\n\nmigration_code_change_in_msa = tf.contrib.layers.sparse_column_with_hash_bucket(\n    column_name=\'migration_code_change_in_msa\', hash_bucket_size=1000)\n\nmigration_code_change_in_msa = tf.contrib.layers.sparse_column_with_hash_bucket(\n    column_name=\'migration_code_change_in_msa\', hash_bucket_size=1000)\n\nmigration_code_change_in_reg = tf.contrib.layers.sparse_column_with_hash_bucket(\n    column_name=\'migration_code_change_in_reg\', hash_bucket_size=1000)\n\nmigration_code_move_within_reg = tf.contrib.layers.sparse_column_with_hash_bucket(\n    column_name=\'migration_code_move_within_reg\', hash_bucket_size=1000)\n\nlive_in_this_house_1year_ago = tf.contrib.layers.sparse_column_with_hash_bucket(\n    column_name=\'live_in_this_house_1year_ago\', hash_bucket_size=1000)\n\nmigration_prev_res_in_sunbelt = tf.contrib.layers.sparse_column_with_hash_bucket(\n    column_name=\'migration_prev_res_in_sunbelt\', hash_bucket_size=1000)\n\nfamily_members_under18 = tf.contrib.layers.sparse_column_with_hash_bucket(\n    column_name=\'family_members_under18\', hash_bucket_size=1000)\n\ncountry_of_birth_father = tf.contrib.layers.sparse_column_with_hash_bucket(\n    column_name=\'country_of_birth_father\', hash_bucket_size=1000)\n\ncountry_of_birth_mother = tf.contrib.layers.sparse_column_with_hash_bucket(\n    column_name=\'country_of_birth_mother\', hash_bucket_size=1000)\n\ncountry_of_birth_self = tf.contrib.layers.sparse_column_with_hash_bucket(\n    column_name=\'country_of_birth_self\', hash_bucket_size=1000)\n\ncitizenship = tf.contrib.layers.sparse_column_with_hash_bucket(\n    column_name=\'citizenship\', hash_bucket_size=1000)\n\nown_business_or_self_employed = tf.contrib.layers.sparse_column_with_hash_bucket(\n    column_name=\'own_business_or_self_employed\', hash_bucket_size=1000)\n\nfill_inc_questionnaire_for_veteran_admin = tf.contrib.layers.sparse_column_with_hash_bucket(\n    column_name=\'fill_inc_questionnaire_for_veteran_admin\',\n    hash_bucket_size=1000)\n\nveterans_benefits = tf.contrib.layers.sparse_column_with_hash_bucket(\n    column_name=\'veterans_benefits\', hash_bucket_size=1000)\nyear = tf.contrib.layers.sparse_column_with_keys(\n    column_name=\'year\', keys=[\'94\', \'95\'])\n\n# Continuous base columns\nage = tf.contrib.layers.real_valued_column(\'age\')\nage_buckets = tf.contrib.layers.bucketized_column(\n    age, boundaries=[18, 25, 30, 35, 40, 45, 50, 55, 60, 65])\nwage_per_hour = tf.contrib.layers.real_valued_column(\'wage_per_hour\')\ncapital_gains = tf.contrib.layers.real_valued_column(\'capital_gains\')\ncapital_losses = tf.contrib.layers.real_valued_column(\'capital_losses\')\ndividends_from_stocks = tf.contrib.layers.real_valued_column(\n    \'dividends_from_stocks\')\ninstance_weight = tf.contrib.layers.real_valued_column(\'instance_weight\')\nweeks_worked_in_year = tf.contrib.layers.real_valued_column(\n    \'weeks_worked_in_year\')\nnum_persons_worked_for_employer = tf.contrib.layers.real_valued_column(\n    \'num_persons_worked_for_employer\')\n\nCOLUMNS = [\n    \'age\', \'class_of_worker\', \'detailed_industry_recode\',\n    \'detailed_occupation_recode\', \'education\', \'wage_per_hour\',\n    \'enroll_in_edu_inst_last_wk\', \'marital_stat\', \'major_industry_code\',\n    \'major_occupation_code\', \'race\', \'hispanic_origin\', \'sex\',\n    \'member_of_labor_union\', \'reason_for_unemployment\',\n    \'full_or_part_time_employment_stat\', \'capital_gains\', \'capital_losses\',\n    \'dividends_from_stocks\', \'tax_filer_stat\', \'region_of_previous_residence\',\n    \'state_of_previous_residence\', \'detailed_household_and_family_stat\',\n    \'detailed_household_summary_in_household\', \'instance_weight\',\n    \'migration_code_change_in_msa\', \'migration_code_change_in_reg\',\n    \'migration_code_move_within_reg\', \'live_in_this_house_1year_ago\',\n    \'migration_prev_res_in_sunbelt\', \'num_persons_worked_for_employer\',\n    \'family_members_under18\', \'country_of_birth_father\',\n    \'country_of_birth_mother\', \'country_of_birth_self\', \'citizenship\',\n    \'own_business_or_self_employed\',\n    \'fill_inc_questionnaire_for_veteran_admin\', \'veterans_benefits\',\n    \'weeks_worked_in_year\', \'year\', \'label\'\n]\nFEATURE_COLUMNS = [\n    age, age_buckets, class_of_worker, detailed_industry_recode,\n    detailed_occupation_recode, education, wage_per_hour,\n    enroll_in_edu_inst_last_wk, marital_stat, major_industry_code,\n    major_occupation_code, race, hispanic_origin, sex, member_of_labor_union,\n    reason_for_unemployment, full_or_part_time_employment_stat, capital_gains,\n    capital_losses, dividends_from_stocks, tax_filer_stat,\n    region_of_previous_residence, state_of_previous_residence,\n    detailed_household_and_family_stat,\n    detailed_household_summary_in_household, instance_weight,\n    migration_code_change_in_msa, migration_code_change_in_reg,\n    migration_code_move_within_reg, live_in_this_house_1year_ago,\n    migration_prev_res_in_sunbelt, num_persons_worked_for_employer,\n    family_members_under18, country_of_birth_father, country_of_birth_mother,\n    country_of_birth_self, citizenship, own_business_or_self_employed,\n    fill_inc_questionnaire_for_veteran_admin, veterans_benefits,\n    weeks_worked_in_year, year\n]\n\nLABEL_COLUMN = \'label\'\nCONTINUOUS_COLUMNS = [\n    \'age\', \'wage_per_hour\', \'capital_gains\', \'capital_losses\',\n    \'dividends_from_stocks\', \'instance_weight\', \'weeks_worked_in_year\',\n    \'num_persons_worked_for_employer\'\n]\nCATEGORICAL_COLUMNS = [\n    \'class_of_worker\', \'detailed_industry_recode\',\n    \'detailed_occupation_recode\', \'education\', \'enroll_in_edu_inst_last_wk\',\n    \'marital_stat\', \'major_industry_code\', \'major_occupation_code\', \'race\',\n    \'hispanic_origin\', \'sex\', \'member_of_labor_union\',\n    \'reason_for_unemployment\', \'full_or_part_time_employment_stat\',\n    \'tax_filer_stat\', \'region_of_previous_residence\',\n    \'state_of_previous_residence\', \'detailed_household_and_family_stat\',\n    \'detailed_household_summary_in_household\', \'migration_code_change_in_msa\',\n    \'migration_code_change_in_reg\', \'migration_code_move_within_reg\',\n    \'live_in_this_house_1year_ago\', \'migration_prev_res_in_sunbelt\',\n    \'family_members_under18\', \'country_of_birth_father\',\n    \'country_of_birth_mother\', \'country_of_birth_self\', \'citizenship\',\n    \'own_business_or_self_employed\',\n    \'fill_inc_questionnaire_for_veteran_admin\', \'veterans_benefits\', \'year\'\n]\n\nTRAIN_FILE = \'../data/census/census-income.data\'\nTEST_FILE = \'../data/census/census-income.test\'\n\ndf_train = pd.read_csv(TRAIN_FILE, names=COLUMNS, skipinitialspace=True)\ndf_test = pd.read_csv(TEST_FILE, names=COLUMNS, skipinitialspace=True)\ndf_train = df_train.dropna(how=\'any\', axis=0)\ndf_test = df_test.dropna(how=\'any\', axis=0)\ndf_train[[\n    \'detailed_industry_recode\', \'detailed_occupation_recode\', \'year\',\n    \'own_business_or_self_employed\', \'veterans_benefits\'\n]] = df_train[[\n    \'detailed_industry_recode\', \'detailed_occupation_recode\', \'year\',\n    \'own_business_or_self_employed\', \'veterans_benefits\'\n]].astype(str)\ndf_test[[\n    \'detailed_industry_recode\', \'detailed_occupation_recode\', \'year\',\n    \'own_business_or_self_employed\', \'veterans_benefits\'\n]] = df_test[[\n    \'detailed_industry_recode\', \'detailed_occupation_recode\', \'year\',\n    \'own_business_or_self_employed\', \'veterans_benefits\'\n]].astype(str)\n\ndf_train[LABEL_COLUMN] = (\n    df_train[LABEL_COLUMN].apply(lambda x: \'+\' in x)).astype(int)\ndf_test[LABEL_COLUMN] = (\n    df_test[LABEL_COLUMN].apply(lambda x: \'+\' in x)).astype(int)\n\ndef input_fn(df):\n    # Creates a dictionary mapping from each continuous feature column name (k) to\n    # # the values of that column stored in a constant Tensor.\n    # continuous_cols = {k: tf.constant(df[k].values) for k in CONTINUOUS_COLUMNS}\n    continuous_cols = {\n        k: tf.expand_dims(tf.constant(df[k].values), 1)\n        for k in CONTINUOUS_COLUMNS\n    }\n    # Creates a dictionary mapping from each categorical feature column name (k)\n    # to the values of that column stored in a tf.SparseTensor.\n    categorical_cols = {\n        k: tf.SparseTensor(\n            indices=[[i, 0] for i in range(df[k].size)],\n            values=df[k].values,\n            dense_shape=[df[k].size, 1])\n        for k in CATEGORICAL_COLUMNS\n    }\n    # Merges the two dictionaries into one.\n    feature_cols = dict(continuous_cols.items() + categorical_cols.items())\n    # Add example id list\n    feature_cols[\'example_id\'] = tf.constant(\n        [str(i + 1) for i in range(df[\'age\'].size)])\n    # Converts the label column into a constant Tensor.\n    label = tf.constant(df[LABEL_COLUMN].values)\n    # Returns the feature columns and the label.\n    return feature_cols, label\n\n\ndef train_input_fn():\n    return input_fn(df_train)\n\n\ndef eval_input_fn():\n    return input_fn(df_test)\n\n\nmodel_dir = \'../svm_model_dir\'\n\nmodel = svm.SVM(example_id_column=\'example_id\',\n                feature_columns=FEATURE_COLUMNS,\n                model_dir=model_dir)\nmodel.fit(input_fn=train_input_fn, steps=100)\nresults = model.evaluate(input_fn=eval_input_fn, steps=1)\nfor key in sorted(results):\n    print(""%s: %s"" % (key, results[key]))\n'"
machinelearning_toolkit/scripts/tf_wide_deep.py,107,"b'import tensorflow as tf\nimport pandas as pd\nfrom tensorflow.contrib.learn.python.learn.estimators import svm\ntf.logging.set_verbosity(tf.logging.INFO)\n\n\n# logger = logging.getLogger(\'Training a classifier using wide and/or deep method\')\n# logger.setLevel(logging.INFO)\n# formatter = logging.Formatter(\'%(asctime)s - %(name)s - %(levelname)s - %(message)s\')\n# ch = logging.StreamHandler()\n# ch.setLevel(logging.INFO)\n# logger.addHandler(ch)\n\n\ntf.app.flags.DEFINE_string(\'classifier_mode\', \'wide\', \'Running mode. One of {""wide"", ""deep"", ""all""}\')\ntf.app.flags.DEFINE_integer(\'train_steps\', 200, \'the step of train the model\')\ntf.app.flags.DEFINE_string(\'model_dir\', \'../wide_model_dir\', \'the model dir\')\nFLAGS = tf.app.flags.FLAGS\n\n\nclass_of_worker = tf.contrib.layers.sparse_column_with_hash_bucket(\n    column_name=\'class_of_worker\', hash_bucket_size=1000)\n\ndetailed_industry_recode = tf.contrib.layers.sparse_column_with_hash_bucket(\n    column_name=\'detailed_industry_recode\', hash_bucket_size=1000)\n\ndetailed_occupation_recode = tf.contrib.layers.sparse_column_with_hash_bucket(\n    column_name=\'detailed_occupation_recode\', hash_bucket_size=1000)\n\neducation = tf.contrib.layers.sparse_column_with_hash_bucket(\n    column_name=\'education\', hash_bucket_size=1000)\n\nenroll_in_edu_inst_last_wk = tf.contrib.layers.sparse_column_with_hash_bucket(\n    column_name=\'enroll_in_edu_inst_last_wk\', hash_bucket_size=1000)\n\nmarital_stat = tf.contrib.layers.sparse_column_with_hash_bucket(\n    column_name=\'marital_stat\', hash_bucket_size=1000)\n\nmajor_industry_code = tf.contrib.layers.sparse_column_with_hash_bucket(\n    column_name=\'major_industry_code\', hash_bucket_size=1000)\n\nmajor_occupation_code = tf.contrib.layers.sparse_column_with_hash_bucket(\n    column_name=\'major_occupation_code\', hash_bucket_size=1000)\n\nrace = tf.contrib.layers.sparse_column_with_hash_bucket(\n    column_name=\'race\', hash_bucket_size=1000)\n\nhispanic_origin = tf.contrib.layers.sparse_column_with_hash_bucket(\n    column_name=\'hispanic_origin\', hash_bucket_size=1000)\nsex = tf.contrib.layers.sparse_column_with_keys(\n    column_name=\'sex\', keys=[\'Female\', \'Male\'])\n\nmember_of_labor_union = tf.contrib.layers.sparse_column_with_hash_bucket(\n    column_name=\'member_of_labor_union\', hash_bucket_size=1000)\nreason_for_unemployment = tf.contrib.layers.sparse_column_with_hash_bucket(\n    column_name=\'reason_for_unemployment\', hash_bucket_size=1000)\n\nfull_or_part_time_employment_stat = tf.contrib.layers.sparse_column_with_hash_bucket(\n    column_name=\'full_or_part_time_employment_stat\', hash_bucket_size=1000)\n\ntax_filer_stat = tf.contrib.layers.sparse_column_with_hash_bucket(\n    column_name=\'tax_filer_stat\', hash_bucket_size=1000)\n\nregion_of_previous_residence = tf.contrib.layers.sparse_column_with_hash_bucket(\n    column_name=\'region_of_previous_residence\', hash_bucket_size=1000)\n\nstate_of_previous_residence = tf.contrib.layers.sparse_column_with_hash_bucket(\n    column_name=\'state_of_previous_residence\', hash_bucket_size=1000)\n\ndetailed_household_and_family_stat = tf.contrib.layers.sparse_column_with_hash_bucket(\n    column_name=\'detailed_household_and_family_stat\', hash_bucket_size=1000)\n\ndetailed_household_summary_in_household = tf.contrib.layers.sparse_column_with_hash_bucket(\n    column_name=\'detailed_household_summary_in_household\',\n    hash_bucket_size=1000)\n\nmigration_code_change_in_msa = tf.contrib.layers.sparse_column_with_hash_bucket(\n    column_name=\'migration_code_change_in_msa\', hash_bucket_size=1000)\n\nmigration_code_change_in_msa = tf.contrib.layers.sparse_column_with_hash_bucket(\n    column_name=\'migration_code_change_in_msa\', hash_bucket_size=1000)\n\nmigration_code_change_in_reg = tf.contrib.layers.sparse_column_with_hash_bucket(\n    column_name=\'migration_code_change_in_reg\', hash_bucket_size=1000)\n\nmigration_code_move_within_reg = tf.contrib.layers.sparse_column_with_hash_bucket(\n    column_name=\'migration_code_move_within_reg\', hash_bucket_size=1000)\n\nlive_in_this_house_1year_ago = tf.contrib.layers.sparse_column_with_hash_bucket(\n    column_name=\'live_in_this_house_1year_ago\', hash_bucket_size=1000)\n\nmigration_prev_res_in_sunbelt = tf.contrib.layers.sparse_column_with_hash_bucket(\n    column_name=\'migration_prev_res_in_sunbelt\', hash_bucket_size=1000)\n\nfamily_members_under18 = tf.contrib.layers.sparse_column_with_hash_bucket(\n    column_name=\'family_members_under18\', hash_bucket_size=1000)\n\ncountry_of_birth_father = tf.contrib.layers.sparse_column_with_hash_bucket(\n    column_name=\'country_of_birth_father\', hash_bucket_size=1000)\n\ncountry_of_birth_mother = tf.contrib.layers.sparse_column_with_hash_bucket(\n    column_name=\'country_of_birth_mother\', hash_bucket_size=1000)\n\ncountry_of_birth_self = tf.contrib.layers.sparse_column_with_hash_bucket(\n    column_name=\'country_of_birth_self\', hash_bucket_size=1000)\n\ncitizenship = tf.contrib.layers.sparse_column_with_hash_bucket(\n    column_name=\'citizenship\', hash_bucket_size=1000)\n\nown_business_or_self_employed = tf.contrib.layers.sparse_column_with_hash_bucket(\n    column_name=\'own_business_or_self_employed\', hash_bucket_size=1000)\n\nfill_inc_questionnaire_for_veteran_admin = tf.contrib.layers.sparse_column_with_hash_bucket(\n    column_name=\'fill_inc_questionnaire_for_veteran_admin\',\n    hash_bucket_size=1000)\n\nveterans_benefits = tf.contrib.layers.sparse_column_with_hash_bucket(\n    column_name=\'veterans_benefits\', hash_bucket_size=1000)\nyear = tf.contrib.layers.sparse_column_with_keys(\n    column_name=\'year\', keys=[\'94\', \'95\'])\n\n# Continuous base columns\nage = tf.contrib.layers.real_valued_column(\'age\')\nage_buckets = tf.contrib.layers.bucketized_column(\n    age, boundaries=[18, 25, 30, 35, 40, 45, 50, 55, 60, 65])\nwage_per_hour = tf.contrib.layers.real_valued_column(\'wage_per_hour\')\ncapital_gains = tf.contrib.layers.real_valued_column(\'capital_gains\')\ncapital_losses = tf.contrib.layers.real_valued_column(\'capital_losses\')\ndividends_from_stocks = tf.contrib.layers.real_valued_column(\n    \'dividends_from_stocks\')\ninstance_weight = tf.contrib.layers.real_valued_column(\'instance_weight\')\nweeks_worked_in_year = tf.contrib.layers.real_valued_column(\n    \'weeks_worked_in_year\')\nnum_persons_worked_for_employer = tf.contrib.layers.real_valued_column(\n    \'num_persons_worked_for_employer\')\n\nCOLUMNS = [\n    \'age\', \'class_of_worker\', \'detailed_industry_recode\',\n    \'detailed_occupation_recode\', \'education\', \'wage_per_hour\',\n    \'enroll_in_edu_inst_last_wk\', \'marital_stat\', \'major_industry_code\',\n    \'major_occupation_code\', \'race\', \'hispanic_origin\', \'sex\',\n    \'member_of_labor_union\', \'reason_for_unemployment\',\n    \'full_or_part_time_employment_stat\', \'capital_gains\', \'capital_losses\',\n    \'dividends_from_stocks\', \'tax_filer_stat\', \'region_of_previous_residence\',\n    \'state_of_previous_residence\', \'detailed_household_and_family_stat\',\n    \'detailed_household_summary_in_household\', \'instance_weight\',\n    \'migration_code_change_in_msa\', \'migration_code_change_in_reg\',\n    \'migration_code_move_within_reg\', \'live_in_this_house_1year_ago\',\n    \'migration_prev_res_in_sunbelt\', \'num_persons_worked_for_employer\',\n    \'family_members_under18\', \'country_of_birth_father\',\n    \'country_of_birth_mother\', \'country_of_birth_self\', \'citizenship\',\n    \'own_business_or_self_employed\',\n    \'fill_inc_questionnaire_for_veteran_admin\', \'veterans_benefits\',\n    \'weeks_worked_in_year\', \'year\', \'label\'\n]\nFEATURE_COLUMNS = [\n    age, age_buckets, class_of_worker, detailed_industry_recode,\n    detailed_occupation_recode, education, wage_per_hour,\n    enroll_in_edu_inst_last_wk, marital_stat, major_industry_code,\n    major_occupation_code, race, hispanic_origin, sex, member_of_labor_union,\n    reason_for_unemployment, full_or_part_time_employment_stat, capital_gains,\n    capital_losses, dividends_from_stocks, tax_filer_stat,\n    region_of_previous_residence, state_of_previous_residence,\n    detailed_household_and_family_stat,\n    detailed_household_summary_in_household, instance_weight,\n    migration_code_change_in_msa, migration_code_change_in_reg,\n    migration_code_move_within_reg, live_in_this_house_1year_ago,\n    migration_prev_res_in_sunbelt, num_persons_worked_for_employer,\n    family_members_under18, country_of_birth_father, country_of_birth_mother,\n    country_of_birth_self, citizenship, own_business_or_self_employed,\n    fill_inc_questionnaire_for_veteran_admin, veterans_benefits,\n    weeks_worked_in_year, year\n]\n\nLABEL_COLUMN = \'label\'\nCONTINUOUS_COLUMNS = [\n    \'age\', \'wage_per_hour\', \'capital_gains\', \'capital_losses\',\n    \'dividends_from_stocks\', \'instance_weight\', \'weeks_worked_in_year\',\n    \'num_persons_worked_for_employer\'\n]\nCATEGORICAL_COLUMNS = [\n    \'class_of_worker\', \'detailed_industry_recode\',\n    \'detailed_occupation_recode\', \'education\', \'enroll_in_edu_inst_last_wk\',\n    \'marital_stat\', \'major_industry_code\', \'major_occupation_code\', \'race\',\n    \'hispanic_origin\', \'sex\', \'member_of_labor_union\',\n    \'reason_for_unemployment\', \'full_or_part_time_employment_stat\',\n    \'tax_filer_stat\', \'region_of_previous_residence\',\n    \'state_of_previous_residence\', \'detailed_household_and_family_stat\',\n    \'detailed_household_summary_in_household\', \'migration_code_change_in_msa\',\n    \'migration_code_change_in_reg\', \'migration_code_move_within_reg\',\n    \'live_in_this_house_1year_ago\', \'migration_prev_res_in_sunbelt\',\n    \'family_members_under18\', \'country_of_birth_father\',\n    \'country_of_birth_mother\', \'country_of_birth_self\', \'citizenship\',\n    \'own_business_or_self_employed\',\n    \'fill_inc_questionnaire_for_veteran_admin\', \'veterans_benefits\', \'year\'\n]\n\nTRAIN_FILE = \'../data/census/census-income.data\'\nTEST_FILE = \'../data/census/census-income.test\'\n\ndf_train = pd.read_csv(TRAIN_FILE, names=COLUMNS, skipinitialspace=True)\ndf_test = pd.read_csv(TEST_FILE, names=COLUMNS, skipinitialspace=True)\ndf_train = df_train.dropna(how=\'any\', axis=0)\ndf_test = df_test.dropna(how=\'any\', axis=0)\ndf_train[[\n    \'detailed_industry_recode\', \'detailed_occupation_recode\', \'year\',\n    \'own_business_or_self_employed\', \'veterans_benefits\'\n]] = df_train[[\n    \'detailed_industry_recode\', \'detailed_occupation_recode\', \'year\',\n    \'own_business_or_self_employed\', \'veterans_benefits\'\n]].astype(str)\ndf_test[[\n    \'detailed_industry_recode\', \'detailed_occupation_recode\', \'year\',\n    \'own_business_or_self_employed\', \'veterans_benefits\'\n]] = df_test[[\n    \'detailed_industry_recode\', \'detailed_occupation_recode\', \'year\',\n    \'own_business_or_self_employed\', \'veterans_benefits\'\n]].astype(str)\n\ndf_train[LABEL_COLUMN] = (\n    df_train[LABEL_COLUMN].apply(lambda x: \'+\' in x)).astype(int)\ndf_test[LABEL_COLUMN] = (\n    df_test[LABEL_COLUMN].apply(lambda x: \'+\' in x)).astype(int)\n# print df_train.dtypes\n# dtypes = df_train.dtypes\n\n# print dtypess[CATEGORICAL_COLUMNS]\n\n# print df_train.head(5)\n# print df_test.head(5)\n\n\ndef input_fn(df):\n    # Creates a dictionary mapping from each continuous feature column name (k) to\n    # # the values of that column stored in a constant Tensor.\n    continuous_cols = {\n        k: tf.constant(df[k].values)\n        for k in CONTINUOUS_COLUMNS\n    }\n    # Creates a dictionary mapping from each categorical feature column name (k)\n    # to the values of that column stored in a tf.SparseTensor.\n    categorical_cols = {\n        k: tf.SparseTensor(\n            indices=[[i, 0] for i in range(df[k].size)],\n            values=df[k].values,\n            dense_shape=[df[k].size, 1])\n        for k in CATEGORICAL_COLUMNS\n    }\n    # Merges the two dictionaries into one.\n    feature_cols = dict(continuous_cols.items() + categorical_cols.items())\n    # Converts the label column into a constant Tensor.\n    label = tf.constant(df[LABEL_COLUMN].values)\n    # Returns the feature columns and the label.\n    return feature_cols, label\n\n\ndef train_input_fn():\n    return input_fn(df_train)\n\n\ndef eval_input_fn():\n    return input_fn(df_test)\n\n\n\n# deep columns\n""""""\nage = tf.contrib.layers.real_valued_column(\'age\')\nage_buckets = tf.contrib.layers.bucketized_column(\n    age, boundaries=[18, 25, 30, 35, 40, 45, 50, 55, 60, 65])\nwage_per_hour = tf.contrib.layers.real_valued_column(\'wage_per_hour\')\ncapital_gains = tf.contrib.layers.real_valued_column(\'capital_gains\')\ncapital_losses = tf.contrib.layers.real_valued_column(\'capital_losses\')\ndividends_from_stocks = tf.contrib.layers.real_valued_column(\n    \'dividends_from_stocks\')\ninstance_weight = tf.contrib.layers.real_valued_column(\'instance_weight\')\nweeks_worked_in_year = tf.contrib.layers.real_valued_column(\n    \'weeks_worked_in_year\')\nnum_persons_worked_for_employer = tf.contrib.layers.real_valued_column(\n    \'num_persons_worked_for_employer\')\n\n\nCATEGORICAL_COLUMNS = [\n    \'class_of_worker\', \'detailed_industry_recode\',\n    \'detailed_occupation_recode\', \'education\', \'enroll_in_edu_inst_last_wk\',\n    \'marital_stat\', \'major_industry_code\', \'major_occupation_code\', \'race\',\n    \'hispanic_origin\', \'sex\', \'member_of_labor_union\',\n    \'reason_for_unemployment\', \'full_or_part_time_employment_stat\',\n    \'tax_filer_stat\', \'region_of_previous_residence\',\n    \'state_of_previous_residence\', \'detailed_household_and_family_stat\',\n    \'detailed_household_summary_in_household\', \'migration_code_change_in_msa\',\n    \'migration_code_change_in_reg\', \'migration_code_move_within_reg\',\n    \'live_in_this_house_1year_ago\', \'migration_prev_res_in_sunbelt\',\n    \'family_members_under18\', \'country_of_birth_father\',\n    \'country_of_birth_mother\', \'country_of_birth_self\', \'citizenship\',\n    \'own_business_or_self_employed\',\n    \'fill_inc_questionnaire_for_veteran_admin\', \'veterans_benefits\', \'year\'\n]\n""""""\nwide_columns = [\n    class_of_worker, detailed_industry_recode,\n    detailed_occupation_recode, education, enroll_in_edu_inst_last_wk,\n    marital_stat, major_industry_code, major_occupation_code, race,\n    hispanic_origin, sex, member_of_labor_union,\n    reason_for_unemployment, full_or_part_time_employment_stat,\n    tax_filer_stat, region_of_previous_residence,\n    state_of_previous_residence, detailed_household_and_family_stat,\n    detailed_household_summary_in_household, migration_code_change_in_msa,\n    migration_code_change_in_reg, migration_code_move_within_reg,\n    live_in_this_house_1year_ago, migration_prev_res_in_sunbelt,\n    family_members_under18, country_of_birth_father,\n    country_of_birth_mother, country_of_birth_self, citizenship,\n    own_business_or_self_employed,\n    fill_inc_questionnaire_for_veteran_admin, veterans_benefits, year,\n    age_buckets, tf.contrib.layers.crossed_column(columns=[age_buckets, class_of_worker], hash_bucket_size=1000),\n    tf.contrib.layers.crossed_column(columns=[age_buckets, education], hash_bucket_size=1000)\n]\ndeep_columns = [\n    age, wage_per_hour, capital_gains, capital_losses, dividends_from_stocks,\n    instance_weight, weeks_worked_in_year, num_persons_worked_for_employer,\n    tf.contrib.layers.embedding_column(detailed_industry_recode, dimension=8),\n    tf.contrib.layers.embedding_column(detailed_occupation_recode, dimension=8),\n    tf.contrib.layers.embedding_column(education, dimension=8),\n    tf.contrib.layers.embedding_column(enroll_in_edu_inst_last_wk, dimension=8),\n    tf.contrib.layers.embedding_column(marital_stat, dimension=8),\n    tf.contrib.layers.embedding_column(major_industry_code, dimension=8),\n    tf.contrib.layers.embedding_column(major_occupation_code, dimension=8),\n    tf.contrib.layers.embedding_column(race, dimension=8),\n    tf.contrib.layers.embedding_column(hispanic_origin, dimension=8),\n    tf.contrib.layers.embedding_column(member_of_labor_union, dimension=8),\n    tf.contrib.layers.embedding_column(reason_for_unemployment, dimension=8),\n    tf.contrib.layers.embedding_column(full_or_part_time_employment_stat, dimension=8),\n    tf.contrib.layers.embedding_column(tax_filer_stat, dimension=8),\n    tf.contrib.layers.embedding_column(region_of_previous_residence, dimension=8),\n    tf.contrib.layers.embedding_column(state_of_previous_residence, dimension=8),\n    tf.contrib.layers.embedding_column(detailed_household_and_family_stat, dimension=8),\n    tf.contrib.layers.embedding_column(detailed_household_summary_in_household, dimension=8),\n    tf.contrib.layers.embedding_column(migration_code_change_in_msa, dimension=8),\n    tf.contrib.layers.embedding_column(migration_code_change_in_reg, dimension=8),\n    tf.contrib.layers.embedding_column(migration_code_move_within_reg, dimension=8),\n    tf.contrib.layers.embedding_column(live_in_this_house_1year_ago, dimension=8),\n    tf.contrib.layers.embedding_column(migration_prev_res_in_sunbelt, dimension=8),\n    tf.contrib.layers.embedding_column(family_members_under18, dimension=8),\n    tf.contrib.layers.embedding_column(country_of_birth_father, dimension=8),\n    tf.contrib.layers.embedding_column(country_of_birth_mother, dimension=8),\n    tf.contrib.layers.embedding_column(country_of_birth_self, dimension=8),\n    tf.contrib.layers.embedding_column(citizenship, dimension=8),\n    tf.contrib.layers.embedding_column(own_business_or_self_employed, dimension=8),\n    tf.contrib.layers.embedding_column(fill_inc_questionnaire_for_veteran_admin, dimension=8),\n    tf.contrib.layers.embedding_column(veterans_benefits, dimension=8),\n    tf.contrib.layers.one_hot_column(sex),\n    tf.contrib.layers.one_hot_column(year)\n]\nmodel_dir = FLAGS.model_dir\ntrain_step = FLAGS.train_steps\nvalidation_metrics = {\n    ""accuracy"":\n    tf.contrib.learn.MetricSpec(\n        metric_fn=tf.contrib.metrics.streaming_accuracy,\n        prediction_key=""classes""),\n    ""precision"":\n    tf.contrib.learn.MetricSpec(\n         metric_fn=tf.contrib.metrics.streaming_precision,\n         prediction_key=""classes""),\n    ""recall"":\n    tf.contrib.learn.MetricSpec(\n        metric_fn=tf.contrib.metrics.streaming_recall,\n        prediction_key=""classes"")\n    }\nvalidation_monitor = tf.contrib.learn.monitors.ValidationMonitor(input_fn=eval_input_fn, \n    every_n_steps=10, metrics=validation_metrics, eval_steps=1)\nif FLAGS.classifier_mode == \'wide\':\n    model = tf.contrib.learn.LinearClassifier(model_dir=model_dir, \n        feature_columns=wide_columns, config=tf.contrib.learn.RunConfig(save_checkpoints_secs=60))\nelif FLAGS.classifier_mode == \'deep\':\n    model = tf.contrib.learn.DNNClassifier(model_dir=model_dir, feature_columns=deep_columns, hidden_units=[128, 64], config=tf.contrib.learn.RunConfig(save_checkpoints_secs=60))\nelse:\n    model = tf.contrib.learn.DNNLinearCombinedClassifier(\n        model_dir=model_dir,\n        linear_feature_columns=wide_columns,\n        dnn_feature_columns=deep_columns,\n        dnn_hidden_units=[128, 64],\n        fix_global_step_increment_bug=True,\n        config=tf.contrib.learn.RunConfig(save_checkpoints_secs=60))\n\nmodel.fit(input_fn=train_input_fn, steps=train_step, monitors=[validation_monitor])\nresults = model.evaluate(input_fn=eval_input_fn, steps=1)\nfor key in results:\n    print ""%s: %s"" % (key, results[key])'"
machinelearning_toolkit/wide_deep_scripts/linear_classifier.py,48,"b'import tensorflow as tf\nimport pandas as pd\nfrom tensorflow.contrib.learn.python.learn.estimators import svm\n\nclass_of_worker = tf.contrib.layers.sparse_column_with_hash_bucket(\n    column_name=\'class_of_worker\', hash_bucket_size=1000)\n\ndetailed_industry_recode = tf.contrib.layers.sparse_column_with_hash_bucket(\n    column_name=\'detailed_industry_recode\', hash_bucket_size=1000)\n\ndetailed_occupation_recode = tf.contrib.layers.sparse_column_with_hash_bucket(\n    column_name=\'detailed_occupation_recode\', hash_bucket_size=1000)\n\neducation = tf.contrib.layers.sparse_column_with_hash_bucket(\n    column_name=\'education\', hash_bucket_size=1000)\n\nenroll_in_edu_inst_last_wk = tf.contrib.layers.sparse_column_with_hash_bucket(\n    column_name=\'enroll_in_edu_inst_last_wk\', hash_bucket_size=1000)\n\nmarital_stat = tf.contrib.layers.sparse_column_with_hash_bucket(\n    column_name=\'marital_stat\', hash_bucket_size=1000)\n\nmajor_industry_code = tf.contrib.layers.sparse_column_with_hash_bucket(\n    column_name=\'major_industry_code\', hash_bucket_size=1000)\n\nmajor_occupation_code = tf.contrib.layers.sparse_column_with_hash_bucket(\n    column_name=\'major_occupation_code\', hash_bucket_size=1000)\n\nrace = tf.contrib.layers.sparse_column_with_hash_bucket(\n    column_name=\'race\', hash_bucket_size=1000)\n\nhispanic_origin = tf.contrib.layers.sparse_column_with_hash_bucket(\n    column_name=\'hispanic_origin\', hash_bucket_size=1000)\nsex = tf.contrib.layers.sparse_column_with_keys(\n    column_name=\'sex\', keys=[\'Female\', \'Male\'])\n\nmember_of_labor_union = tf.contrib.layers.sparse_column_with_hash_bucket(\n    column_name=\'member_of_labor_union\', hash_bucket_size=1000)\nreason_for_unemployment = tf.contrib.layers.sparse_column_with_hash_bucket(\n    column_name=\'reason_for_unemployment\', hash_bucket_size=1000)\n\nfull_or_part_time_employment_stat = tf.contrib.layers.sparse_column_with_hash_bucket(\n    column_name=\'full_or_part_time_employment_stat\', hash_bucket_size=1000)\n\ntax_filer_stat = tf.contrib.layers.sparse_column_with_hash_bucket(\n    column_name=\'tax_filer_stat\', hash_bucket_size=1000)\n\nregion_of_previous_residence = tf.contrib.layers.sparse_column_with_hash_bucket(\n    column_name=\'region_of_previous_residence\', hash_bucket_size=1000)\n\nstate_of_previous_residence = tf.contrib.layers.sparse_column_with_hash_bucket(\n    column_name=\'state_of_previous_residence\', hash_bucket_size=1000)\n\ndetailed_household_and_family_stat = tf.contrib.layers.sparse_column_with_hash_bucket(\n    column_name=\'detailed_household_and_family_stat\', hash_bucket_size=1000)\n\ndetailed_household_summary_in_household = tf.contrib.layers.sparse_column_with_hash_bucket(\n    column_name=\'detailed_household_summary_in_household\',\n    hash_bucket_size=1000)\n\nmigration_code_change_in_msa = tf.contrib.layers.sparse_column_with_hash_bucket(\n    column_name=\'migration_code_change_in_msa\', hash_bucket_size=1000)\n\nmigration_code_change_in_msa = tf.contrib.layers.sparse_column_with_hash_bucket(\n    column_name=\'migration_code_change_in_msa\', hash_bucket_size=1000)\n\nmigration_code_change_in_reg = tf.contrib.layers.sparse_column_with_hash_bucket(\n    column_name=\'migration_code_change_in_reg\', hash_bucket_size=1000)\n\nmigration_code_move_within_reg = tf.contrib.layers.sparse_column_with_hash_bucket(\n    column_name=\'migration_code_move_within_reg\', hash_bucket_size=1000)\n\nlive_in_this_house_1year_ago = tf.contrib.layers.sparse_column_with_hash_bucket(\n    column_name=\'live_in_this_house_1year_ago\', hash_bucket_size=1000)\n\nmigration_prev_res_in_sunbelt = tf.contrib.layers.sparse_column_with_hash_bucket(\n    column_name=\'migration_prev_res_in_sunbelt\', hash_bucket_size=1000)\n\nfamily_members_under18 = tf.contrib.layers.sparse_column_with_hash_bucket(\n    column_name=\'family_members_under18\', hash_bucket_size=1000)\n\ncountry_of_birth_father = tf.contrib.layers.sparse_column_with_hash_bucket(\n    column_name=\'country_of_birth_father\', hash_bucket_size=1000)\n\ncountry_of_birth_mother = tf.contrib.layers.sparse_column_with_hash_bucket(\n    column_name=\'country_of_birth_mother\', hash_bucket_size=1000)\n\ncountry_of_birth_self = tf.contrib.layers.sparse_column_with_hash_bucket(\n    column_name=\'country_of_birth_self\', hash_bucket_size=1000)\n\ncitizenship = tf.contrib.layers.sparse_column_with_hash_bucket(\n    column_name=\'citizenship\', hash_bucket_size=1000)\n\nown_business_or_self_employed = tf.contrib.layers.sparse_column_with_hash_bucket(\n    column_name=\'own_business_or_self_employed\', hash_bucket_size=1000)\n\nfill_inc_questionnaire_for_veteran_admin = tf.contrib.layers.sparse_column_with_hash_bucket(\n    column_name=\'fill_inc_questionnaire_for_veteran_admin\',\n    hash_bucket_size=1000)\n\nveterans_benefits = tf.contrib.layers.sparse_column_with_hash_bucket(\n    column_name=\'veterans_benefits\', hash_bucket_size=1000)\nyear = tf.contrib.layers.sparse_column_with_keys(\n    column_name=\'year\', keys=[\'94\', \'95\'])\n\n# Continuous base columns\nage = tf.contrib.layers.real_valued_column(\'age\')\nage_buckets = tf.contrib.layers.bucketized_column(\n    age, boundaries=[18, 25, 30, 35, 40, 45, 50, 55, 60, 65])\nwage_per_hour = tf.contrib.layers.real_valued_column(\'wage_per_hour\')\ncapital_gains = tf.contrib.layers.real_valued_column(\'capital_gains\')\ncapital_losses = tf.contrib.layers.real_valued_column(\'capital_losses\')\ndividends_from_stocks = tf.contrib.layers.real_valued_column(\n    \'dividends_from_stocks\')\ninstance_weight = tf.contrib.layers.real_valued_column(\'instance_weight\')\nweeks_worked_in_year = tf.contrib.layers.real_valued_column(\n    \'weeks_worked_in_year\')\nnum_persons_worked_for_employer = tf.contrib.layers.real_valued_column(\n    \'num_persons_worked_for_employer\')\n\nCOLUMNS = [\n    \'age\', \'class_of_worker\', \'detailed_industry_recode\',\n    \'detailed_occupation_recode\', \'education\', \'wage_per_hour\',\n    \'enroll_in_edu_inst_last_wk\', \'marital_stat\', \'major_industry_code\',\n    \'major_occupation_code\', \'race\', \'hispanic_origin\', \'sex\',\n    \'member_of_labor_union\', \'reason_for_unemployment\',\n    \'full_or_part_time_employment_stat\', \'capital_gains\', \'capital_losses\',\n    \'dividends_from_stocks\', \'tax_filer_stat\', \'region_of_previous_residence\',\n    \'state_of_previous_residence\', \'detailed_household_and_family_stat\',\n    \'detailed_household_summary_in_household\', \'instance_weight\',\n    \'migration_code_change_in_msa\', \'migration_code_change_in_reg\',\n    \'migration_code_move_within_reg\', \'live_in_this_house_1year_ago\',\n    \'migration_prev_res_in_sunbelt\', \'num_persons_worked_for_employer\',\n    \'family_members_under18\', \'country_of_birth_father\',\n    \'country_of_birth_mother\', \'country_of_birth_self\', \'citizenship\',\n    \'own_business_or_self_employed\',\n    \'fill_inc_questionnaire_for_veteran_admin\', \'veterans_benefits\',\n    \'weeks_worked_in_year\', \'year\', \'label\'\n]\nFEATURE_COLUMNS = [\n    age, age_buckets, class_of_worker, detailed_industry_recode,\n    detailed_occupation_recode, education, wage_per_hour,\n    enroll_in_edu_inst_last_wk, marital_stat, major_industry_code,\n    major_occupation_code, race, hispanic_origin, sex, member_of_labor_union,\n    reason_for_unemployment, full_or_part_time_employment_stat, capital_gains,\n    capital_losses, dividends_from_stocks, tax_filer_stat,\n    region_of_previous_residence, state_of_previous_residence,\n    detailed_household_and_family_stat,\n    detailed_household_summary_in_household, instance_weight,\n    migration_code_change_in_msa, migration_code_change_in_reg,\n    migration_code_move_within_reg, live_in_this_house_1year_ago,\n    migration_prev_res_in_sunbelt, num_persons_worked_for_employer,\n    family_members_under18, country_of_birth_father, country_of_birth_mother,\n    country_of_birth_self, citizenship, own_business_or_self_employed,\n    fill_inc_questionnaire_for_veteran_admin, veterans_benefits,\n    weeks_worked_in_year, year\n]\n\nLABEL_COLUMN = \'label\'\nCONTINUOUS_COLUMNS = [\n    \'age\', \'wage_per_hour\', \'capital_gains\', \'capital_losses\',\n    \'dividends_from_stocks\', \'instance_weight\', \'weeks_worked_in_year\',\n    \'num_persons_worked_for_employer\'\n]\nCATEGORICAL_COLUMNS = [\n    \'class_of_worker\', \'detailed_industry_recode\',\n    \'detailed_occupation_recode\', \'education\', \'enroll_in_edu_inst_last_wk\',\n    \'marital_stat\', \'major_industry_code\', \'major_occupation_code\', \'race\',\n    \'hispanic_origin\', \'sex\', \'member_of_labor_union\',\n    \'reason_for_unemployment\', \'full_or_part_time_employment_stat\',\n    \'tax_filer_stat\', \'region_of_previous_residence\',\n    \'state_of_previous_residence\', \'detailed_household_and_family_stat\',\n    \'detailed_household_summary_in_household\', \'migration_code_change_in_msa\',\n    \'migration_code_change_in_reg\', \'migration_code_move_within_reg\',\n    \'live_in_this_house_1year_ago\', \'migration_prev_res_in_sunbelt\',\n    \'family_members_under18\', \'country_of_birth_father\',\n    \'country_of_birth_mother\', \'country_of_birth_self\', \'citizenship\',\n    \'own_business_or_self_employed\',\n    \'fill_inc_questionnaire_for_veteran_admin\', \'veterans_benefits\', \'year\'\n]\n\nTRAIN_FILE = \'../data/census/census-income.data\'\nTEST_FILE = \'../data/census/census-income.test\'\n\ndf_train = pd.read_csv(TRAIN_FILE, names=COLUMNS, skipinitialspace=True)\ndf_test = pd.read_csv(TEST_FILE, names=COLUMNS, skipinitialspace=True)\ndf_train = df_train.dropna(how=\'any\', axis=0)\ndf_test = df_test.dropna(how=\'any\', axis=0)\ndf_train[[\n    \'detailed_industry_recode\', \'detailed_occupation_recode\', \'year\',\n    \'own_business_or_self_employed\', \'veterans_benefits\'\n]] = df_train[[\n    \'detailed_industry_recode\', \'detailed_occupation_recode\', \'year\',\n    \'own_business_or_self_employed\', \'veterans_benefits\'\n]].astype(str)\ndf_test[[\n    \'detailed_industry_recode\', \'detailed_occupation_recode\', \'year\',\n    \'own_business_or_self_employed\', \'veterans_benefits\'\n]] = df_test[[\n    \'detailed_industry_recode\', \'detailed_occupation_recode\', \'year\',\n    \'own_business_or_self_employed\', \'veterans_benefits\'\n]].astype(str)\n\ndf_train[LABEL_COLUMN] = (\n    df_train[LABEL_COLUMN].apply(lambda x: \'+\' in x)).astype(int)\ndf_test[LABEL_COLUMN] = (\n    df_test[LABEL_COLUMN].apply(lambda x: \'+\' in x)).astype(int)\ndtypess = df_train.dtypes\n\n\ndef input_fn(df):\n    # Creates a dictionary mapping from each continuous feature column name (k) to\n    # # the values of that column stored in a constant Tensor.\n    continuous_cols = {\n        k: tf.constant(df[k].values)\n        for k in CONTINUOUS_COLUMNS\n    }\n    # Creates a dictionary mapping from each categorical feature column name (k)\n    # to the values of that column stored in a tf.SparseTensor.\n    categorical_cols = {\n        k: tf.SparseTensor(\n            indices=[[i, 0] for i in range(df[k].size)],\n            values=df[k].values,\n            dense_shape=[df[k].size, 1])\n        for k in CATEGORICAL_COLUMNS\n    }\n    # Merges the two dictionaries into one.\n    feature_cols = dict(continuous_cols.items() + categorical_cols.items())\n    # Converts the label column into a constant Tensor.\n    label = tf.constant(df[LABEL_COLUMN].values)\n    # Returns the feature columns and the label.\n    return feature_cols, label\n\n\ndef train_input_fn():\n    return input_fn(df_train)\n\n\ndef eval_input_fn():\n    return input_fn(df_test)\n\n\nmodel_dir = \'../model_dir\'\n\nmodel = tf.contrib.learn.LinearClassifier(\n    feature_columns=FEATURE_COLUMNS, model_dir=model_dir)\nmodel.fit(input_fn=train_input_fn, steps=200)\nresults = model.evaluate(input_fn=eval_input_fn, steps=1)\nfor key in sorted(results):\n    print(""%s: %s"" % (key, results[key]))\n'"
machinelearning_toolkit/wide_deep_scripts/simple-tf-rf.py,9,"b'import tensorflow as tf\nimport pandas as pd\nfrom tensorflow.contrib.tensor_forest.client import random_forest\n\ndetailed_occupation_recode = tf.contrib.layers.sparse_column_with_hash_bucket(\n    column_name=\'detailed_occupation_recode\', hash_bucket_size=1000)\neducation = tf.contrib.layers.sparse_column_with_hash_bucket(\n    column_name=\'education\', hash_bucket_size=1000)\n# Continuous base columns\nage = tf.contrib.layers.real_valued_column(\'age\')\nwage_per_hour = tf.contrib.layers.real_valued_column(\'wage_per_hour\')\n\ncolumns = [\n    \'age\', \'detailed_occupation_recode\', \'education\', \'wage_per_hour\', \'label\'\n]\nFEATURE_COLUMNS = [\n    # age, age_buckets, class_of_worker, detailed_industry_recode,\n    age,\n    detailed_occupation_recode,\n    education,\n    wage_per_hour\n]\n\nLABEL_COLUMN = \'label\'\n\nCONTINUOUS_COLUMNS = [\'age\', \'wage_per_hour\']\n\nCATEGORICAL_COLUMNS = [\'detailed_occupation_recode\', \'education\']\n\ndf_train = pd.DataFrame(\n    [[12, \'12\', \'7th and 8th grade\', 40, \'- 50000\'],\n     [40, \'45\', \'7th and 8th grade\', 40, \'50000+\'],\n     [50, \'50\', \'10th grade\', 40, \'50000+\'],\n     [60, \'30\', \'7th and 8th grade\', 40, \'- 50000\']],\n    columns=[\n        \'age\', \'detailed_occupation_recode\', \'education\', \'wage_per_hour\',\n        \'label\'\n    ])\n\ndf_test = pd.DataFrame(\n    [[12, \'12\', \'7th and 8th grade\', 40, \'- 50000\'],\n     [40, \'45\', \'7th and 8th grade\', 40, \'50000+\'],\n     [50, \'50\', \'10th grade\', 40, \'50000+\'],\n     [60, \'30\', \'7th and 8th grade\', 40, \'- 50000\']],\n    columns=[\n        \'age\', \'detailed_occupation_recode\', \'education\', \'wage_per_hour\',\n        \'label\'\n    ])\ndf_train[LABEL_COLUMN] = (\n    df_train[LABEL_COLUMN].apply(lambda x: \'+\' in x)).astype(int)\ndf_test[LABEL_COLUMN] = (\n    df_test[LABEL_COLUMN].apply(lambda x: \'+\' in x)).astype(int)\ndtypess = df_train.dtypes\n\nprint df_train\nprint df_test\n\n\ndef input_fn(df):\n    continuous_cols = {\n        k: tf.expand_dims(tf.constant(df[k].values), 1)\n        for k in CONTINUOUS_COLUMNS\n    }\n    # continuous_cols = {\n    #     k: tf.constant(df[k].values)\n    #     for k in CONTINUOUS_COLUMNS\n    # }\n    categorical_cols = {\n        k: tf.SparseTensor(\n            indices=[[i, 0] for i in range(df[k].size)],\n            values=df[k].values,\n            dense_shape=[df[k].size, 1])\n        for k in CATEGORICAL_COLUMNS\n    }\n    feature_cols = dict(continuous_cols.items() + categorical_cols.items())\n    label = tf.constant(df[LABEL_COLUMN].values)\n    return feature_cols, label\n\n\ndef train_input_fn():\n    return input_fn(df_train)\n\n\ndef eval_input_fn():\n    return input_fn(df_test)\n\n\nmodel_dir = \'../rf_model_dir\'\n\nhparams = tf.contrib.tensor_forest.python.tensor_forest.ForestHParams(\n    num_trees=10, max_nodes=1000, num_classes=2, num_features=4)\nclassifier = random_forest.TensorForestEstimator(hparams, model_dir=model_dir)\nclassifier.fit(input_fn=train_input_fn, steps=100)\nresults = classifier.evaluate(input_fn=eval_input_fn, steps=1)\nprint results\nfor key in sorted(results):\n    print(""%s: %s"" % (key, results[key]))\n'"
machinelearning_toolkit/wide_deep_scripts/simple-tf-svm.py,9,"b'import tensorflow as tf\nimport pandas as pd\nfrom tensorflow.contrib.learn.python.learn.estimators import svm\n\ndetailed_occupation_recode = tf.contrib.layers.sparse_column_with_hash_bucket(\n    column_name=\'detailed_occupation_recode\', hash_bucket_size=1000)\neducation = tf.contrib.layers.sparse_column_with_hash_bucket(\n    column_name=\'education\', hash_bucket_size=1000)\n# Continuous base columns\nage = tf.contrib.layers.real_valued_column(\'age\')\nwage_per_hour = tf.contrib.layers.real_valued_column(\'wage_per_hour\')\n\ncolumns = [\n    \'age\', \'detailed_occupation_recode\', \'education\', \'wage_per_hour\', \'label\'\n]\nFEATURE_COLUMNS = [\n    # age, age_buckets, class_of_worker, detailed_industry_recode,\n    age,\n    detailed_occupation_recode,\n    education,\n    wage_per_hour\n]\n\nLABEL_COLUMN = \'label\'\n\nCONTINUOUS_COLUMNS = [\'age\', \'wage_per_hour\']\n\nCATEGORICAL_COLUMNS = [\'detailed_occupation_recode\', \'education\']\n\ndf_train = pd.DataFrame(\n    [[12, \'12\', \'7th and 8th grade\', 40, \'- 50000\'],\n     [40, \'45\', \'7th and 8th grade\', 40, \'50000+\'],\n     [50, \'50\', \'10th grade\', 40, \'50000+\'],\n     [60, \'30\', \'7th and 8th grade\', 40, \'- 50000\']],\n    columns=[\n        \'age\', \'detailed_occupation_recode\', \'education\', \'wage_per_hour\',\n        \'label\'\n    ])\n\ndf_test = pd.DataFrame(\n    [[12, \'12\', \'7th and 8th grade\', 40, \'- 50000\'],\n     [40, \'45\', \'7th and 8th grade\', 40, \'50000+\'],\n     [50, \'50\', \'10th grade\', 40, \'50000+\'],\n     [60, \'30\', \'7th and 8th grade\', 40, \'- 50000\']],\n    columns=[\n        \'age\', \'detailed_occupation_recode\', \'education\', \'wage_per_hour\',\n        \'label\'\n    ])\ndf_train[LABEL_COLUMN] = (\n    df_train[LABEL_COLUMN].apply(lambda x: \'+\' in x)).astype(int)\ndf_test[LABEL_COLUMN] = (\n    df_test[LABEL_COLUMN].apply(lambda x: \'+\' in x)).astype(int)\ndtypess = df_train.dtypes\n\n\ndef input_fn(df):\n    # continuous_cols = {k: tf.constant(df[k].values) for k in CONTINUOUS_COLUMNS}\n    continuous_cols = {\n        k: tf.expand_dims(tf.constant(df[k].values), 1)\n        for k in CONTINUOUS_COLUMNS\n    }\n    categorical_cols = {\n        k: tf.SparseTensor(\n            indices=[[i, 0] for i in range(df[k].size)],\n            values=df[k].values,\n            dense_shape=[df[k].size, 1])\n        for k in CATEGORICAL_COLUMNS\n    }\n    feature_cols = dict(continuous_cols.items() + categorical_cols.items())\n    feature_cols[\'example_id\'] = tf.constant(\n        [str(i + 1) for i in range(df[\'age\'].size)])\n    label = tf.constant(df[LABEL_COLUMN].values)\n    return feature_cols, label\n\n\ndef train_input_fn():\n    return input_fn(df_train)\n\n\ndef eval_input_fn():\n    return input_fn(df_test)\n\n\nmodel_dir = \'../svm_model_dir\'\n\nmodel = svm.SVM(example_id_column=\'example_id\',\n                feature_columns=FEATURE_COLUMNS,\n                model_dir=model_dir)\nmodel.fit(input_fn=train_input_fn, steps=10)\nresults = model.evaluate(input_fn=eval_input_fn, steps=1)\nfor key in sorted(results):\n    print(""%s: %s"" % (key, results[key]))\n'"
machinelearning_toolkit/wide_deep_scripts/simple_tf_wide_deep.py,28,"b'import tensorflow as tf\nimport pandas as pd\nfrom tensorflow.contrib.learn.python.learn.estimators import svm\ntf.logging.set_verbosity(tf.logging.INFO)\n\n\n# logger = logging.getLogger(\'Training a classifier using wide and/or deep method\')\n# logger.setLevel(logging.INFO)\n# formatter = logging.Formatter(\'%(asctime)s - %(name)s - %(levelname)s - %(message)s\')\n# ch = logging.StreamHandler()\n# ch.setLevel(logging.INFO)\n# logger.addHandler(ch)\n\n\ntf.app.flags.DEFINE_string(\'classifier_mode\', \'wide\', \'Running mode. One of {""wide"", ""deep"", ""all""}\')\ntf.app.flags.DEFINE_integer(\'train_steps\', 200, \'the step of train the model\')\ntf.app.flags.DEFINE_string(\'model_dir\', \'../wide_model_dir\', \'the model dir\')\nFLAGS = tf.app.flags.FLAGS\n\n\ndetailed_occupation_recode = tf.contrib.layers.sparse_column_with_hash_bucket(\n    column_name=\'detailed_occupation_recode\', hash_bucket_size=1000)\neducation = tf.contrib.layers.sparse_column_with_hash_bucket(\n    column_name=\'education\', hash_bucket_size=1000)\n# Continuous base columns\nage = tf.contrib.layers.real_valued_column(\'age\')\nwage_per_hour = tf.contrib.layers.real_valued_column(\'wage_per_hour\')\n\ncolumns = [\n    \'age\', \'detailed_occupation_recode\', \'education\', \'wage_per_hour\', \'label\'\n]\nFEATURE_COLUMNS = [\n    # age, age_buckets, class_of_worker, detailed_industry_recode,\n    age,\n    detailed_occupation_recode,\n    education,\n    wage_per_hour\n]\n\nLABEL_COLUMN = \'label\'\n\nCONTINUOUS_COLUMNS = [\'age\', \'wage_per_hour\']\n\nCATEGORICAL_COLUMNS = [\'detailed_occupation_recode\', \'education\']\n\ndf_train = pd.DataFrame(\n    [[12, \'12\', \'7th and 8th grade\', 40, \'- 50000\'],\n     [40, \'45\', \'7th and 8th grade\', 40, \'50000+\'],\n     [50, \'50\', \'10th grade\', 40, \'50000+\'],\n     [60, \'30\', \'7th and 8th grade\', 40, \'- 50000\']],\n    columns=[\n        \'age\', \'detailed_occupation_recode\', \'education\', \'wage_per_hour\',\n        \'label\'\n    ])\n\ndf_test = pd.DataFrame(\n    [[12, \'12\', \'7th and 8th grade\', 40, \'- 50000\'],\n     [40, \'45\', \'7th and 8th grade\', 40, \'50000+\'],\n     [50, \'50\', \'10th grade\', 40, \'50000+\'],\n     [60, \'30\', \'7th and 8th grade\', 40, \'- 50000\']],\n    columns=[\n        \'age\', \'detailed_occupation_recode\', \'education\', \'wage_per_hour\',\n        \'label\'\n    ])\ndf_train[LABEL_COLUMN] = (\n    df_train[LABEL_COLUMN].apply(lambda x: \'+\' in x)).astype(int)\ndf_test[LABEL_COLUMN] = (\n    df_test[LABEL_COLUMN].apply(lambda x: \'+\' in x)).astype(int)\ndtypess = df_train.dtypes\n\nprint df_train\nprint df_test\n\n# print df_train.dtypes\n# dtypes = df_train.dtypes\n\n# print dtypess[CATEGORICAL_COLUMNS]\n\n# print df_train.head(5)\n# print df_test.head(5)\n\n\ndef input_fn(df):\n    # Creates a dictionary mapping from each continuous feature column name (k) to\n    # # the values of that column stored in a constant Tensor.\n    continuous_cols = {\n        k: tf.expand_dims(tf.constant(df[k].values), 1)\n        for k in CONTINUOUS_COLUMNS\n    }\n    # Creates a dictionary mapping from each categorical feature column name (k)\n    # to the values of that column stored in a tf.SparseTensor.\n    categorical_cols = {\n        k: tf.SparseTensor(\n            indices=[[i, 0] for i in range(df[k].size)],\n            values=df[k].values,\n            dense_shape=[df[k].size, 1])\n        for k in CATEGORICAL_COLUMNS\n    }\n    # Merges the two dictionaries into one.\n    feature_cols = dict(continuous_cols.items() + categorical_cols.items())\n    # Converts the label column into a constant Tensor.\n    label = tf.constant(df[LABEL_COLUMN].values)\n    # Returns the feature columns and the label.\n    return feature_cols, label\n\n\ndef train_input_fn():\n    return input_fn(df_train)\n\n\ndef eval_input_fn():\n    return input_fn(df_test)\n\n\n\n# deep columns\n""""""\nage = tf.contrib.layers.real_valued_column(\'age\')\nage_buckets = tf.contrib.layers.bucketized_column(\n    age, boundaries=[18, 25, 30, 35, 40, 45, 50, 55, 60, 65])\nwage_per_hour = tf.contrib.layers.real_valued_column(\'wage_per_hour\')\ncapital_gains = tf.contrib.layers.real_valued_column(\'capital_gains\')\ncapital_losses = tf.contrib.layers.real_valued_column(\'capital_losses\')\ndividends_from_stocks = tf.contrib.layers.real_valued_column(\n    \'dividends_from_stocks\')\ninstance_weight = tf.contrib.layers.real_valued_column(\'instance_weight\')\nweeks_worked_in_year = tf.contrib.layers.real_valued_column(\n    \'weeks_worked_in_year\')\nnum_persons_worked_for_employer = tf.contrib.layers.real_valued_column(\n    \'num_persons_worked_for_employer\')\n\n\nCATEGORICAL_COLUMNS = [\n    \'class_of_worker\', \'detailed_industry_recode\',\n    \'detailed_occupation_recode\', \'education\', \'enroll_in_edu_inst_last_wk\',\n    \'marital_stat\', \'major_industry_code\', \'major_occupation_code\', \'race\',\n    \'hispanic_origin\', \'sex\', \'member_of_labor_union\',\n    \'reason_for_unemployment\', \'full_or_part_time_employment_stat\',\n    \'tax_filer_stat\', \'region_of_previous_residence\',\n    \'state_of_previous_residence\', \'detailed_household_and_family_stat\',\n    \'detailed_household_summary_in_household\', \'migration_code_change_in_msa\',\n    \'migration_code_change_in_reg\', \'migration_code_move_within_reg\',\n    \'live_in_this_house_1year_ago\', \'migration_prev_res_in_sunbelt\',\n    \'family_members_under18\', \'country_of_birth_father\',\n    \'country_of_birth_mother\', \'country_of_birth_self\', \'citizenship\',\n    \'own_business_or_self_employed\',\n    \'fill_inc_questionnaire_for_veteran_admin\', \'veterans_benefits\', \'year\'\n]\n""""""\nwide_columns = [detailed_occupation_recode, education]\ndeep_columns = [age, wage_per_hour]\n\nmodel_dir = FLAGS.model_dir\ntrain_step = FLAGS.train_steps\nvalidation_monitor = tf.contrib.learn.monitors.ValidationMonitor(\n    input_fn = eval_input_fn,\n    every_n_steps=100,\n    eval_steps=1)\nif FLAGS.classifier_mode == \'wide\':\n    model = tf.contrib.learn.LinearClassifier(model_dir=model_dir, \n        feature_columns=wide_columns, config=tf.contrib.learn.RunConfig(save_checkpoints_secs=60))\nelif FLAGS.classifier_mode == \'deep\':\n    model = tf.contrib.learn.DNNClassifier(model_dir=model_dir, feature_columns=deep_columns, hidden_units=[128, 64], config=tf.contrib.learn.RunConfig(save_checkpoints_secs=1))\nelse:\n    model = tf.contrib.learn.DNNLinearCombinedClassifier(\n        model_dir=model_dir,\n        linear_feature_columns=wide_columns,\n        dnn_feature_columns=deep_columns,\n        dnn_hidden_units=[128, 64],\n        fix_global_step_increment_bug=True,\n        config=tf.contrib.learn.RunConfig(save_checkpoints_secs=60))\n\nmodel.fit(input_fn=train_input_fn, steps=train_step, monitors=[validation_monitor])\nresults = model.evaluate(input_fn=eval_input_fn, steps=1)\nfor key in results:\n    print ""%s: %s"" % (key, results[key])'"
machinelearning_toolkit/wide_deep_scripts/tf-rf.py,56,"b'import tensorflow as tf\nimport pandas as pd\nimport numpy as np\nfrom tensorflow.contrib.tensor_forest.client import random_forest\ntf.logging.set_verbosity(tf.logging.INFO)\n\n# categorical base columns\nclass_of_worker = tf.contrib.layers.sparse_column_with_hash_bucket(\n    column_name=\'class_of_worker\', hash_bucket_size=1000)\ndetailed_industry_recode = tf.contrib.layers.sparse_column_with_hash_bucket(\n    column_name=\'detailed_industry_recode\', hash_bucket_size=1000)\n\ndetailed_occupation_recode = tf.contrib.layers.sparse_column_with_hash_bucket(\n    column_name=\'detailed_occupation_recode\', hash_bucket_size=1000)\n\neducation = tf.contrib.layers.sparse_column_with_hash_bucket(\n    column_name=\'education\', hash_bucket_size=1000)\n\nenroll_in_edu_inst_last_wk = tf.contrib.layers.sparse_column_with_hash_bucket(\n    column_name=\'enroll_in_edu_inst_last_wk\', hash_bucket_size=1000)\n\nmarital_stat = tf.contrib.layers.sparse_column_with_hash_bucket(\n    column_name=\'marital_stat\', hash_bucket_size=1000)\n\nmajor_industry_code = tf.contrib.layers.sparse_column_with_hash_bucket(\n    column_name=\'major_industry_code\', hash_bucket_size=1000)\n\nmajor_occupation_code = tf.contrib.layers.sparse_column_with_hash_bucket(\n    column_name=\'major_occupation_code\', hash_bucket_size=1000)\n\nrace = tf.contrib.layers.sparse_column_with_hash_bucket(\n    column_name=\'race\', hash_bucket_size=1000)\n\nhispanic_origin = tf.contrib.layers.sparse_column_with_hash_bucket(\n    column_name=\'hispanic_origin\', hash_bucket_size=1000)\nsex = tf.contrib.layers.sparse_column_with_keys(\n    column_name=\'sex\', keys=[\'Female\', \'Male\'])\n\nmember_of_labor_union = tf.contrib.layers.sparse_column_with_hash_bucket(\n    column_name=\'member_of_labor_union\', hash_bucket_size=1000)\n\nreason_for_unemployment = tf.contrib.layers.sparse_column_with_hash_bucket(\n    column_name=\'reason_for_unemployment\', hash_bucket_size=1000)\n\nfull_or_part_time_employment_stat = tf.contrib.layers.sparse_column_with_hash_bucket(\n    column_name=\'full_or_part_time_employment_stat\', hash_bucket_size=1000)\n\ntax_filer_stat = tf.contrib.layers.sparse_column_with_hash_bucket(\n    column_name=\'tax_filer_stat\', hash_bucket_size=1000)\n\nregion_of_previous_residence = tf.contrib.layers.sparse_column_with_hash_bucket(\n    column_name=\'region_of_previous_residence\', hash_bucket_size=1000)\n\nstate_of_previous_residence = tf.contrib.layers.sparse_column_with_hash_bucket(\n    column_name=\'state_of_previous_residence\', hash_bucket_size=1000)\n\ndetailed_household_and_family_stat = tf.contrib.layers.sparse_column_with_hash_bucket(\n    column_name=\'detailed_household_and_family_stat\', hash_bucket_size=1000)\n\ndetailed_household_summary_in_household = tf.contrib.layers.sparse_column_with_hash_bucket(\n    column_name=\'detailed_household_summary_in_household\',\n    hash_bucket_size=1000)\n\nmigration_code_change_in_msa = tf.contrib.layers.sparse_column_with_hash_bucket(\n    column_name=\'migration_code_change_in_msa\', hash_bucket_size=1000)\n\nmigration_code_change_in_msa = tf.contrib.layers.sparse_column_with_hash_bucket(\n    column_name=\'migration_code_change_in_msa\', hash_bucket_size=1000)\n\nmigration_code_change_in_reg = tf.contrib.layers.sparse_column_with_hash_bucket(\n    column_name=\'migration_code_change_in_reg\', hash_bucket_size=1000)\n\nmigration_code_move_within_reg = tf.contrib.layers.sparse_column_with_hash_bucket(\n    column_name=\'migration_code_move_within_reg\', hash_bucket_size=1000)\n\nlive_in_this_house_1year_ago = tf.contrib.layers.sparse_column_with_hash_bucket(\n    column_name=\'live_in_this_house_1year_ago\', hash_bucket_size=1000)\n\nmigration_prev_res_in_sunbelt = tf.contrib.layers.sparse_column_with_hash_bucket(\n    column_name=\'migration_prev_res_in_sunbelt\', hash_bucket_size=1000)\n\nfamily_members_under18 = tf.contrib.layers.sparse_column_with_hash_bucket(\n    column_name=\'family_members_under18\', hash_bucket_size=1000)\n\ncountry_of_birth_father = tf.contrib.layers.sparse_column_with_hash_bucket(\n    column_name=\'country_of_birth_father\', hash_bucket_size=1000)\n\ncountry_of_birth_mother = tf.contrib.layers.sparse_column_with_hash_bucket(\n    column_name=\'country_of_birth_mother\', hash_bucket_size=1000)\n\ncountry_of_birth_self = tf.contrib.layers.sparse_column_with_hash_bucket(\n    column_name=\'country_of_birth_self\', hash_bucket_size=1000)\n\ncitizenship = tf.contrib.layers.sparse_column_with_hash_bucket(\n    column_name=\'citizenship\', hash_bucket_size=1000)\n\nown_business_or_self_employed = tf.contrib.layers.sparse_column_with_hash_bucket(\n    column_name=\'own_business_or_self_employed\', hash_bucket_size=1000)\n\nfill_inc_questionnaire_for_veteran_admin = tf.contrib.layers.sparse_column_with_hash_bucket(\n    column_name=\'fill_inc_questionnaire_for_veteran_admin\',\n    hash_bucket_size=1000)\n\nveterans_benefits = tf.contrib.layers.sparse_column_with_hash_bucket(\n    column_name=\'veterans_benefits\', hash_bucket_size=1000)\nyear = tf.contrib.layers.sparse_column_with_keys(\n    column_name=\'year\', keys=[\'94\', \'95\'])\n\n# Continuous base columns\nage = tf.contrib.layers.real_valued_column(\'age\')\nage_buckets = tf.contrib.layers.bucketized_column(\n    age, boundaries=[18, 25, 30, 35, 40, 45, 50, 55, 60, 65])\nwage_per_hour = tf.contrib.layers.real_valued_column(\'wage_per_hour\')\ncapital_gains = tf.contrib.layers.real_valued_column(\'capital_gains\')\ncapital_losses = tf.contrib.layers.real_valued_column(\'capital_losses\')\ndividends_from_stocks = tf.contrib.layers.real_valued_column(\n    \'dividends_from_stocks\')\ninstance_weight = tf.contrib.layers.real_valued_column(\'instance_weight\')\nweeks_worked_in_year = tf.contrib.layers.real_valued_column(\n    \'weeks_worked_in_year\')\nnum_persons_worked_for_employer = tf.contrib.layers.real_valued_column(\n    \'num_persons_worked_for_employer\')\n\nCOLUMNS = [\n    \'age\', \'class_of_worker\', \'detailed_industry_recode\',\n    \'detailed_occupation_recode\', \'education\', \'wage_per_hour\',\n    \'enroll_in_edu_inst_last_wk\', \'marital_stat\', \'major_industry_code\',\n    \'major_occupation_code\', \'race\', \'hispanic_origin\', \'sex\',\n    \'member_of_labor_union\', \'reason_for_unemployment\',\n    \'full_or_part_time_employment_stat\', \'capital_gains\', \'capital_losses\',\n    \'dividends_from_stocks\', \'tax_filer_stat\', \'region_of_previous_residence\',\n    \'state_of_previous_residence\', \'detailed_household_and_family_stat\',\n    \'detailed_household_summary_in_household\', \'instance_weight\',\n    \'migration_code_change_in_msa\', \'migration_code_change_in_reg\',\n    \'migration_code_move_within_reg\', \'live_in_this_house_1year_ago\',\n    \'migration_prev_res_in_sunbelt\', \'num_persons_worked_for_employer\',\n    \'family_members_under18\', \'country_of_birth_father\',\n    \'country_of_birth_mother\', \'country_of_birth_self\', \'citizenship\',\n    \'own_business_or_self_employed\',\n    \'fill_inc_questionnaire_for_veteran_admin\', \'veterans_benefits\',\n    \'weeks_worked_in_year\', \'year\', \'label\'\n]\nFEATURE_COLUMNS = [\n    age, age_buckets, class_of_worker, detailed_industry_recode,\n    detailed_occupation_recode, education, wage_per_hour,\n    enroll_in_edu_inst_last_wk, marital_stat, major_industry_code,\n    major_occupation_code, race, hispanic_origin, sex, member_of_labor_union,\n    reason_for_unemployment, full_or_part_time_employment_stat, capital_gains,\n    capital_losses, dividends_from_stocks, tax_filer_stat,\n    region_of_previous_residence, state_of_previous_residence,\n    detailed_household_and_family_stat,\n    detailed_household_summary_in_household, instance_weight,\n    migration_code_change_in_msa, migration_code_change_in_reg,\n    migration_code_move_within_reg, live_in_this_house_1year_ago,\n    migration_prev_res_in_sunbelt, num_persons_worked_for_employer,\n    family_members_under18, country_of_birth_father, country_of_birth_mother,\n    country_of_birth_self, citizenship, own_business_or_self_employed,\n    fill_inc_questionnaire_for_veteran_admin, veterans_benefits,\n    weeks_worked_in_year, year\n]\n# FEATURE_COLUMNS = [age, detailed_occupation_recode, education, wage_per_hour]\n\nLABEL_COLUMN = \'label\'\nCONTINUOUS_COLUMNS = [\n    \'age\', \'wage_per_hour\', \'capital_gains\', \'capital_losses\',\n    \'dividends_from_stocks\', \'instance_weight\', \'weeks_worked_in_year\',\n    \'num_persons_worked_for_employer\'\n]\nCATEGORICAL_COLUMNS = [\n    \'class_of_worker\', \'detailed_industry_recode\',\n    \'detailed_occupation_recode\', \'education\', \'enroll_in_edu_inst_last_wk\',\n    \'marital_stat\', \'major_industry_code\', \'major_occupation_code\', \'race\',\n    \'hispanic_origin\', \'sex\', \'member_of_labor_union\',\n    \'reason_for_unemployment\', \'full_or_part_time_employment_stat\',\n    \'tax_filer_stat\', \'region_of_previous_residence\',\n    \'state_of_previous_residence\', \'detailed_household_and_family_stat\',\n    \'detailed_household_summary_in_household\', \'migration_code_change_in_msa\',\n    \'migration_code_change_in_reg\', \'migration_code_move_within_reg\',\n    \'live_in_this_house_1year_ago\', \'migration_prev_res_in_sunbelt\',\n    \'family_members_under18\', \'country_of_birth_father\',\n    \'country_of_birth_mother\', \'country_of_birth_self\', \'citizenship\',\n    \'own_business_or_self_employed\',\n    \'fill_inc_questionnaire_for_veteran_admin\', \'veterans_benefits\', \'year\'\n]\n\nTRAIN_FILE = \'../data/census/census-income.data\'\nTEST_FILE = \'../data/census/census-income.test\'\n\ndf_train = pd.read_csv(TRAIN_FILE, names=COLUMNS, skipinitialspace=True)\ndf_test = pd.read_csv(TEST_FILE, names=COLUMNS, skipinitialspace=True)\ndf_train = df_train.dropna(how=\'any\', axis=0)\ndf_test = df_test.dropna(how=\'any\', axis=0)\ndf_train[[\n    \'detailed_industry_recode\', \'detailed_occupation_recode\', \'year\',\n    \'own_business_or_self_employed\', \'veterans_benefits\'\n]] = df_train[[\n    \'detailed_industry_recode\', \'detailed_occupation_recode\', \'year\',\n    \'own_business_or_self_employed\', \'veterans_benefits\'\n]].astype(str)\ndf_test[[\n    \'detailed_industry_recode\', \'detailed_occupation_recode\', \'year\',\n    \'own_business_or_self_employed\', \'veterans_benefits\'\n]] = df_test[[\n    \'detailed_industry_recode\', \'detailed_occupation_recode\', \'year\',\n    \'own_business_or_self_employed\', \'veterans_benefits\'\n]].astype(str)\n\ndf_train[LABEL_COLUMN] = (\n    df_train[LABEL_COLUMN].apply(lambda x: \'+\' in x)).astype(int)\ndf_test[LABEL_COLUMN] = (\n    df_test[LABEL_COLUMN].apply(lambda x: \'+\' in x)).astype(int)\n# print df_train.dtypes\ndtypess = df_train.dtypes\n\n# print dtypess[CATEGORICAL_COLUMNS]\n\nprint df_train.head(5)\nprint df_test.head(5)\n\n\ndef input_fn(df):\n    continuous_cols = {\n        k: tf.expand_dims(tf.constant(df[k].astype(np.float32).values), 1)\n        for k in CONTINUOUS_COLUMNS\n    }\n    # Creates a dictionary mapping from each categorical feature column name (k)\n    # to the values of that column stored in a tf.SparseTensor.\n    categorical_cols = {\n        k: tf.SparseTensor(\n            indices=[[i, 0] for i in range(df[k].size)],\n            values=df[k].values,\n            dense_shape=[df[k].size, 1])\n        for k in CATEGORICAL_COLUMNS\n    }\n    # Merges the two dictionaries into one.\n    feature_cols = dict(continuous_cols.items() + categorical_cols.items())\n    # Add example id list\n    # Converts the label column into a constant Tensor.\n    label = tf.constant(df[LABEL_COLUMN].values)\n    # Returns the feature columns and the label.\n    return feature_cols, label\n\n\ndef train_input_fn():\n    return input_fn(df_train)\n\n\ndef eval_input_fn():\n    return input_fn(df_test)\n\n\nmodel_dir = \'../rf_model_dir\'\nvalidation_metrics = {\n    ""accuracy"":\n    tf.contrib.learn.MetricSpec(\n        metric_fn=tf.contrib.metrics.streaming_accuracy,\n        prediction_key=\'probabilities\'\n        ),\n    ""precision"":\n    tf.contrib.learn.MetricSpec(\n         metric_fn=tf.contrib.metrics.streaming_precision,\n         prediction_key=\'probabilities\'\n         ),\n    ""recall"":\n    tf.contrib.learn.MetricSpec(\n        metric_fn=tf.contrib.metrics.streaming_recall,\n        prediction_key=\'probabilities\'\n        )\n    }\n\nhparams = tf.contrib.tensor_forest.python.tensor_forest.ForestHParams(\n    num_trees=10,\n    max_nodes=1000,\n    num_classes=2,\n    num_features=len(CONTINUOUS_COLUMNS) + len(CATEGORICAL_COLUMNS))\nclassifier = random_forest.TensorForestEstimator(hparams, model_dir=model_dir, config=tf.contrib.learn.RunConfig(save_checkpoints_secs=60))\n\nclassifier.fit(input_fn=train_input_fn, steps=200)\nresults = classifier.evaluate(\n    input_fn=eval_input_fn, steps=1, metrics=validation_metrics)\nprint results\nfor key in sorted(results):\n    print(""%s: %s"" % (key, results[key]))\n'"
machinelearning_toolkit/wide_deep_scripts/tf-svm.py,49,"b'import tensorflow as tf\nimport pandas as pd\nfrom tensorflow.contrib.learn.python.learn.estimators import svm\n\nclass_of_worker = tf.contrib.layers.sparse_column_with_hash_bucket(\n    column_name=\'class_of_worker\', hash_bucket_size=1000)\n\ndetailed_industry_recode = tf.contrib.layers.sparse_column_with_hash_bucket(\n    column_name=\'detailed_industry_recode\', hash_bucket_size=1000)\n\ndetailed_occupation_recode = tf.contrib.layers.sparse_column_with_hash_bucket(\n    column_name=\'detailed_occupation_recode\', hash_bucket_size=1000)\n\neducation = tf.contrib.layers.sparse_column_with_hash_bucket(\n    column_name=\'education\', hash_bucket_size=1000)\n\nenroll_in_edu_inst_last_wk = tf.contrib.layers.sparse_column_with_hash_bucket(\n    column_name=\'enroll_in_edu_inst_last_wk\', hash_bucket_size=1000)\n\nmarital_stat = tf.contrib.layers.sparse_column_with_hash_bucket(\n    column_name=\'marital_stat\', hash_bucket_size=1000)\n\nmajor_industry_code = tf.contrib.layers.sparse_column_with_hash_bucket(\n    column_name=\'major_industry_code\', hash_bucket_size=1000)\n\nmajor_occupation_code = tf.contrib.layers.sparse_column_with_hash_bucket(\n    column_name=\'major_occupation_code\', hash_bucket_size=1000)\n\nrace = tf.contrib.layers.sparse_column_with_hash_bucket(\n    column_name=\'race\', hash_bucket_size=1000)\n\nhispanic_origin = tf.contrib.layers.sparse_column_with_hash_bucket(\n    column_name=\'hispanic_origin\', hash_bucket_size=1000)\nsex = tf.contrib.layers.sparse_column_with_keys(\n    column_name=\'sex\', keys=[\'Female\', \'Male\'])\n\n\nmember_of_labor_union = tf.contrib.layers.sparse_column_with_hash_bucket(\n    column_name=\'member_of_labor_union\', hash_bucket_size=1000)\n\nreason_for_unemployment = tf.contrib.layers.sparse_column_with_hash_bucket(\n    column_name=\'reason_for_unemployment\', hash_bucket_size=1000)\n\nfull_or_part_time_employment_stat = tf.contrib.layers.sparse_column_with_hash_bucket(\n    column_name=\'full_or_part_time_employment_stat\', hash_bucket_size=1000)\n\ntax_filer_stat = tf.contrib.layers.sparse_column_with_hash_bucket(\n    column_name=\'tax_filer_stat\', hash_bucket_size=1000)\n\nregion_of_previous_residence = tf.contrib.layers.sparse_column_with_hash_bucket(\n    column_name=\'region_of_previous_residence\', hash_bucket_size=1000)\n\nstate_of_previous_residence = tf.contrib.layers.sparse_column_with_hash_bucket(\n    column_name=\'state_of_previous_residence\', hash_bucket_size=1000)\n\ndetailed_household_and_family_stat = tf.contrib.layers.sparse_column_with_hash_bucket(\n    column_name=\'detailed_household_and_family_stat\', hash_bucket_size=1000)\n\ndetailed_household_summary_in_household = tf.contrib.layers.sparse_column_with_hash_bucket(\n    column_name=\'detailed_household_summary_in_household\',\n    hash_bucket_size=1000)\n\nmigration_code_change_in_msa = tf.contrib.layers.sparse_column_with_hash_bucket(\n    column_name=\'migration_code_change_in_msa\', hash_bucket_size=1000)\n\nmigration_code_change_in_msa = tf.contrib.layers.sparse_column_with_hash_bucket(\n    column_name=\'migration_code_change_in_msa\', hash_bucket_size=1000)\n\nmigration_code_change_in_reg = tf.contrib.layers.sparse_column_with_hash_bucket(\n    column_name=\'migration_code_change_in_reg\', hash_bucket_size=1000)\n\nmigration_code_move_within_reg = tf.contrib.layers.sparse_column_with_hash_bucket(\n    column_name=\'migration_code_move_within_reg\', hash_bucket_size=1000)\n\nlive_in_this_house_1year_ago = tf.contrib.layers.sparse_column_with_hash_bucket(\n    column_name=\'live_in_this_house_1year_ago\', hash_bucket_size=1000)\n\nmigration_prev_res_in_sunbelt = tf.contrib.layers.sparse_column_with_hash_bucket(\n    column_name=\'migration_prev_res_in_sunbelt\', hash_bucket_size=1000)\n\nfamily_members_under18 = tf.contrib.layers.sparse_column_with_hash_bucket(\n    column_name=\'family_members_under18\', hash_bucket_size=1000)\n\ncountry_of_birth_father = tf.contrib.layers.sparse_column_with_hash_bucket(\n    column_name=\'country_of_birth_father\', hash_bucket_size=1000)\n\ncountry_of_birth_mother = tf.contrib.layers.sparse_column_with_hash_bucket(\n    column_name=\'country_of_birth_mother\', hash_bucket_size=1000)\n\ncountry_of_birth_self = tf.contrib.layers.sparse_column_with_hash_bucket(\n    column_name=\'country_of_birth_self\', hash_bucket_size=1000)\n\ncitizenship = tf.contrib.layers.sparse_column_with_hash_bucket(\n    column_name=\'citizenship\', hash_bucket_size=1000)\n\nown_business_or_self_employed = tf.contrib.layers.sparse_column_with_hash_bucket(\n    column_name=\'own_business_or_self_employed\', hash_bucket_size=1000)\n\nfill_inc_questionnaire_for_veteran_admin = tf.contrib.layers.sparse_column_with_hash_bucket(\n    column_name=\'fill_inc_questionnaire_for_veteran_admin\',\n    hash_bucket_size=1000)\n\nveterans_benefits = tf.contrib.layers.sparse_column_with_hash_bucket(\n    column_name=\'veterans_benefits\', hash_bucket_size=1000)\nyear = tf.contrib.layers.sparse_column_with_keys(\n    column_name=\'year\', keys=[\'94\', \'95\'])\n\n# Continuous base columns\nage = tf.contrib.layers.real_valued_column(\'age\')\nage_buckets = tf.contrib.layers.bucketized_column(\n    age, boundaries=[18, 25, 30, 35, 40, 45, 50, 55, 60, 65])\nwage_per_hour = tf.contrib.layers.real_valued_column(\'wage_per_hour\')\ncapital_gains = tf.contrib.layers.real_valued_column(\'capital_gains\')\ncapital_losses = tf.contrib.layers.real_valued_column(\'capital_losses\')\ndividends_from_stocks = tf.contrib.layers.real_valued_column(\n    \'dividends_from_stocks\')\ninstance_weight = tf.contrib.layers.real_valued_column(\'instance_weight\')\nweeks_worked_in_year = tf.contrib.layers.real_valued_column(\n    \'weeks_worked_in_year\')\nnum_persons_worked_for_employer = tf.contrib.layers.real_valued_column(\n    \'num_persons_worked_for_employer\')\n\nCOLUMNS = [\n    \'age\', \'class_of_worker\', \'detailed_industry_recode\',\n    \'detailed_occupation_recode\', \'education\', \'wage_per_hour\',\n    \'enroll_in_edu_inst_last_wk\', \'marital_stat\', \'major_industry_code\',\n    \'major_occupation_code\', \'race\', \'hispanic_origin\', \'sex\',\n    \'member_of_labor_union\', \'reason_for_unemployment\',\n    \'full_or_part_time_employment_stat\', \'capital_gains\', \'capital_losses\',\n    \'dividends_from_stocks\', \'tax_filer_stat\', \'region_of_previous_residence\',\n    \'state_of_previous_residence\', \'detailed_household_and_family_stat\',\n    \'detailed_household_summary_in_household\', \'instance_weight\',\n    \'migration_code_change_in_msa\', \'migration_code_change_in_reg\',\n    \'migration_code_move_within_reg\', \'live_in_this_house_1year_ago\',\n    \'migration_prev_res_in_sunbelt\', \'num_persons_worked_for_employer\',\n    \'family_members_under18\', \'country_of_birth_father\',\n    \'country_of_birth_mother\', \'country_of_birth_self\', \'citizenship\',\n    \'own_business_or_self_employed\',\n    \'fill_inc_questionnaire_for_veteran_admin\', \'veterans_benefits\',\n    \'weeks_worked_in_year\', \'year\', \'label\'\n]\nFEATURE_COLUMNS = [\n    age, age_buckets, class_of_worker, detailed_industry_recode,\n    detailed_occupation_recode, education, wage_per_hour,\n    enroll_in_edu_inst_last_wk, marital_stat, major_industry_code,\n    major_occupation_code, race, hispanic_origin, sex, member_of_labor_union,\n    reason_for_unemployment, full_or_part_time_employment_stat, capital_gains,\n    capital_losses, dividends_from_stocks, tax_filer_stat,\n    region_of_previous_residence, state_of_previous_residence,\n    detailed_household_and_family_stat,\n    detailed_household_summary_in_household, instance_weight,\n    migration_code_change_in_msa, migration_code_change_in_reg,\n    migration_code_move_within_reg, live_in_this_house_1year_ago,\n    migration_prev_res_in_sunbelt, num_persons_worked_for_employer,\n    family_members_under18, country_of_birth_father, country_of_birth_mother,\n    country_of_birth_self, citizenship, own_business_or_self_employed,\n    fill_inc_questionnaire_for_veteran_admin, veterans_benefits,\n    weeks_worked_in_year, year\n]\n\nLABEL_COLUMN = \'label\'\nCONTINUOUS_COLUMNS = [\n    \'age\', \'wage_per_hour\', \'capital_gains\', \'capital_losses\',\n    \'dividends_from_stocks\', \'instance_weight\', \'weeks_worked_in_year\',\n    \'num_persons_worked_for_employer\'\n]\nCATEGORICAL_COLUMNS = [\n    \'class_of_worker\', \'detailed_industry_recode\',\n    \'detailed_occupation_recode\', \'education\', \'enroll_in_edu_inst_last_wk\',\n    \'marital_stat\', \'major_industry_code\', \'major_occupation_code\', \'race\',\n    \'hispanic_origin\', \'sex\', \'member_of_labor_union\',\n    \'reason_for_unemployment\', \'full_or_part_time_employment_stat\',\n    \'tax_filer_stat\', \'region_of_previous_residence\',\n    \'state_of_previous_residence\', \'detailed_household_and_family_stat\',\n    \'detailed_household_summary_in_household\', \'migration_code_change_in_msa\',\n    \'migration_code_change_in_reg\', \'migration_code_move_within_reg\',\n    \'live_in_this_house_1year_ago\', \'migration_prev_res_in_sunbelt\',\n    \'family_members_under18\', \'country_of_birth_father\',\n    \'country_of_birth_mother\', \'country_of_birth_self\', \'citizenship\',\n    \'own_business_or_self_employed\',\n    \'fill_inc_questionnaire_for_veteran_admin\', \'veterans_benefits\', \'year\'\n]\n\nTRAIN_FILE = \'../data/census/census-income.data\'\nTEST_FILE = \'../data/census/census-income.test\'\n\ndf_train = pd.read_csv(TRAIN_FILE, names=COLUMNS, skipinitialspace=True)\ndf_test = pd.read_csv(TEST_FILE, names=COLUMNS, skipinitialspace=True)\ndf_train = df_train.dropna(how=\'any\', axis=0)\ndf_test = df_test.dropna(how=\'any\', axis=0)\ndf_train[[\n    \'detailed_industry_recode\', \'detailed_occupation_recode\', \'year\',\n    \'own_business_or_self_employed\', \'veterans_benefits\'\n]] = df_train[[\n    \'detailed_industry_recode\', \'detailed_occupation_recode\', \'year\',\n    \'own_business_or_self_employed\', \'veterans_benefits\'\n]].astype(str)\ndf_test[[\n    \'detailed_industry_recode\', \'detailed_occupation_recode\', \'year\',\n    \'own_business_or_self_employed\', \'veterans_benefits\'\n]] = df_test[[\n    \'detailed_industry_recode\', \'detailed_occupation_recode\', \'year\',\n    \'own_business_or_self_employed\', \'veterans_benefits\'\n]].astype(str)\n\ndf_train[LABEL_COLUMN] = (\n    df_train[LABEL_COLUMN].apply(lambda x: \'+\' in x)).astype(int)\ndf_test[LABEL_COLUMN] = (\n    df_test[LABEL_COLUMN].apply(lambda x: \'+\' in x)).astype(int)\n\ndef input_fn(df):\n    # Creates a dictionary mapping from each continuous feature column name (k) to\n    # # the values of that column stored in a constant Tensor.\n    # continuous_cols = {k: tf.constant(df[k].values) for k in CONTINUOUS_COLUMNS}\n    continuous_cols = {\n        k: tf.expand_dims(tf.constant(df[k].values), 1)\n        for k in CONTINUOUS_COLUMNS\n    }\n    # Creates a dictionary mapping from each categorical feature column name (k)\n    # to the values of that column stored in a tf.SparseTensor.\n    categorical_cols = {\n        k: tf.SparseTensor(\n            indices=[[i, 0] for i in range(df[k].size)],\n            values=df[k].values,\n            dense_shape=[df[k].size, 1])\n        for k in CATEGORICAL_COLUMNS\n    }\n    # Merges the two dictionaries into one.\n    feature_cols = dict(continuous_cols.items() + categorical_cols.items())\n    # Add example id list\n    feature_cols[\'example_id\'] = tf.constant(\n        [str(i + 1) for i in range(df[\'age\'].size)])\n    # Converts the label column into a constant Tensor.\n    label = tf.constant(df[LABEL_COLUMN].values)\n    # Returns the feature columns and the label.\n    return feature_cols, label\n\n\ndef train_input_fn():\n    return input_fn(df_train)\n\n\ndef eval_input_fn():\n    return input_fn(df_test)\n\n\nmodel_dir = \'../svm_model_dir\'\n\nmodel = svm.SVM(example_id_column=\'example_id\',\n                feature_columns=FEATURE_COLUMNS,\n                model_dir=model_dir)\nmodel.fit(input_fn=train_input_fn, steps=100)\nresults = model.evaluate(input_fn=eval_input_fn, steps=1)\nfor key in sorted(results):\n    print(""%s: %s"" % (key, results[key]))\n'"
machinelearning_toolkit/wide_deep_scripts/tf_wide_deep.py,107,"b'import tensorflow as tf\nimport pandas as pd\nfrom tensorflow.contrib.learn.python.learn.estimators import svm\ntf.logging.set_verbosity(tf.logging.INFO)\n\n\n# logger = logging.getLogger(\'Training a classifier using wide and/or deep method\')\n# logger.setLevel(logging.INFO)\n# formatter = logging.Formatter(\'%(asctime)s - %(name)s - %(levelname)s - %(message)s\')\n# ch = logging.StreamHandler()\n# ch.setLevel(logging.INFO)\n# logger.addHandler(ch)\n\n\ntf.app.flags.DEFINE_string(\'classifier_mode\', \'wide\', \'Running mode. One of {""wide"", ""deep"", ""all""}\')\ntf.app.flags.DEFINE_integer(\'train_steps\', 200, \'the step of train the model\')\ntf.app.flags.DEFINE_string(\'model_dir\', \'../wide_model_dir\', \'the model dir\')\nFLAGS = tf.app.flags.FLAGS\n\n\nclass_of_worker = tf.contrib.layers.sparse_column_with_hash_bucket(\n    column_name=\'class_of_worker\', hash_bucket_size=1000)\n\ndetailed_industry_recode = tf.contrib.layers.sparse_column_with_hash_bucket(\n    column_name=\'detailed_industry_recode\', hash_bucket_size=1000)\n\ndetailed_occupation_recode = tf.contrib.layers.sparse_column_with_hash_bucket(\n    column_name=\'detailed_occupation_recode\', hash_bucket_size=1000)\n\neducation = tf.contrib.layers.sparse_column_with_hash_bucket(\n    column_name=\'education\', hash_bucket_size=1000)\n\nenroll_in_edu_inst_last_wk = tf.contrib.layers.sparse_column_with_hash_bucket(\n    column_name=\'enroll_in_edu_inst_last_wk\', hash_bucket_size=1000)\n\nmarital_stat = tf.contrib.layers.sparse_column_with_hash_bucket(\n    column_name=\'marital_stat\', hash_bucket_size=1000)\n\nmajor_industry_code = tf.contrib.layers.sparse_column_with_hash_bucket(\n    column_name=\'major_industry_code\', hash_bucket_size=1000)\n\nmajor_occupation_code = tf.contrib.layers.sparse_column_with_hash_bucket(\n    column_name=\'major_occupation_code\', hash_bucket_size=1000)\n\nrace = tf.contrib.layers.sparse_column_with_hash_bucket(\n    column_name=\'race\', hash_bucket_size=1000)\n\nhispanic_origin = tf.contrib.layers.sparse_column_with_hash_bucket(\n    column_name=\'hispanic_origin\', hash_bucket_size=1000)\nsex = tf.contrib.layers.sparse_column_with_keys(\n    column_name=\'sex\', keys=[\'Female\', \'Male\'])\n\nmember_of_labor_union = tf.contrib.layers.sparse_column_with_hash_bucket(\n    column_name=\'member_of_labor_union\', hash_bucket_size=1000)\nreason_for_unemployment = tf.contrib.layers.sparse_column_with_hash_bucket(\n    column_name=\'reason_for_unemployment\', hash_bucket_size=1000)\n\nfull_or_part_time_employment_stat = tf.contrib.layers.sparse_column_with_hash_bucket(\n    column_name=\'full_or_part_time_employment_stat\', hash_bucket_size=1000)\n\ntax_filer_stat = tf.contrib.layers.sparse_column_with_hash_bucket(\n    column_name=\'tax_filer_stat\', hash_bucket_size=1000)\n\nregion_of_previous_residence = tf.contrib.layers.sparse_column_with_hash_bucket(\n    column_name=\'region_of_previous_residence\', hash_bucket_size=1000)\n\nstate_of_previous_residence = tf.contrib.layers.sparse_column_with_hash_bucket(\n    column_name=\'state_of_previous_residence\', hash_bucket_size=1000)\n\ndetailed_household_and_family_stat = tf.contrib.layers.sparse_column_with_hash_bucket(\n    column_name=\'detailed_household_and_family_stat\', hash_bucket_size=1000)\n\ndetailed_household_summary_in_household = tf.contrib.layers.sparse_column_with_hash_bucket(\n    column_name=\'detailed_household_summary_in_household\',\n    hash_bucket_size=1000)\n\nmigration_code_change_in_msa = tf.contrib.layers.sparse_column_with_hash_bucket(\n    column_name=\'migration_code_change_in_msa\', hash_bucket_size=1000)\n\nmigration_code_change_in_msa = tf.contrib.layers.sparse_column_with_hash_bucket(\n    column_name=\'migration_code_change_in_msa\', hash_bucket_size=1000)\n\nmigration_code_change_in_reg = tf.contrib.layers.sparse_column_with_hash_bucket(\n    column_name=\'migration_code_change_in_reg\', hash_bucket_size=1000)\n\nmigration_code_move_within_reg = tf.contrib.layers.sparse_column_with_hash_bucket(\n    column_name=\'migration_code_move_within_reg\', hash_bucket_size=1000)\n\nlive_in_this_house_1year_ago = tf.contrib.layers.sparse_column_with_hash_bucket(\n    column_name=\'live_in_this_house_1year_ago\', hash_bucket_size=1000)\n\nmigration_prev_res_in_sunbelt = tf.contrib.layers.sparse_column_with_hash_bucket(\n    column_name=\'migration_prev_res_in_sunbelt\', hash_bucket_size=1000)\n\nfamily_members_under18 = tf.contrib.layers.sparse_column_with_hash_bucket(\n    column_name=\'family_members_under18\', hash_bucket_size=1000)\n\ncountry_of_birth_father = tf.contrib.layers.sparse_column_with_hash_bucket(\n    column_name=\'country_of_birth_father\', hash_bucket_size=1000)\n\ncountry_of_birth_mother = tf.contrib.layers.sparse_column_with_hash_bucket(\n    column_name=\'country_of_birth_mother\', hash_bucket_size=1000)\n\ncountry_of_birth_self = tf.contrib.layers.sparse_column_with_hash_bucket(\n    column_name=\'country_of_birth_self\', hash_bucket_size=1000)\n\ncitizenship = tf.contrib.layers.sparse_column_with_hash_bucket(\n    column_name=\'citizenship\', hash_bucket_size=1000)\n\nown_business_or_self_employed = tf.contrib.layers.sparse_column_with_hash_bucket(\n    column_name=\'own_business_or_self_employed\', hash_bucket_size=1000)\n\nfill_inc_questionnaire_for_veteran_admin = tf.contrib.layers.sparse_column_with_hash_bucket(\n    column_name=\'fill_inc_questionnaire_for_veteran_admin\',\n    hash_bucket_size=1000)\n\nveterans_benefits = tf.contrib.layers.sparse_column_with_hash_bucket(\n    column_name=\'veterans_benefits\', hash_bucket_size=1000)\nyear = tf.contrib.layers.sparse_column_with_keys(\n    column_name=\'year\', keys=[\'94\', \'95\'])\n\n# Continuous base columns\nage = tf.contrib.layers.real_valued_column(\'age\')\nage_buckets = tf.contrib.layers.bucketized_column(\n    age, boundaries=[18, 25, 30, 35, 40, 45, 50, 55, 60, 65])\nwage_per_hour = tf.contrib.layers.real_valued_column(\'wage_per_hour\')\ncapital_gains = tf.contrib.layers.real_valued_column(\'capital_gains\')\ncapital_losses = tf.contrib.layers.real_valued_column(\'capital_losses\')\ndividends_from_stocks = tf.contrib.layers.real_valued_column(\n    \'dividends_from_stocks\')\ninstance_weight = tf.contrib.layers.real_valued_column(\'instance_weight\')\nweeks_worked_in_year = tf.contrib.layers.real_valued_column(\n    \'weeks_worked_in_year\')\nnum_persons_worked_for_employer = tf.contrib.layers.real_valued_column(\n    \'num_persons_worked_for_employer\')\n\nCOLUMNS = [\n    \'age\', \'class_of_worker\', \'detailed_industry_recode\',\n    \'detailed_occupation_recode\', \'education\', \'wage_per_hour\',\n    \'enroll_in_edu_inst_last_wk\', \'marital_stat\', \'major_industry_code\',\n    \'major_occupation_code\', \'race\', \'hispanic_origin\', \'sex\',\n    \'member_of_labor_union\', \'reason_for_unemployment\',\n    \'full_or_part_time_employment_stat\', \'capital_gains\', \'capital_losses\',\n    \'dividends_from_stocks\', \'tax_filer_stat\', \'region_of_previous_residence\',\n    \'state_of_previous_residence\', \'detailed_household_and_family_stat\',\n    \'detailed_household_summary_in_household\', \'instance_weight\',\n    \'migration_code_change_in_msa\', \'migration_code_change_in_reg\',\n    \'migration_code_move_within_reg\', \'live_in_this_house_1year_ago\',\n    \'migration_prev_res_in_sunbelt\', \'num_persons_worked_for_employer\',\n    \'family_members_under18\', \'country_of_birth_father\',\n    \'country_of_birth_mother\', \'country_of_birth_self\', \'citizenship\',\n    \'own_business_or_self_employed\',\n    \'fill_inc_questionnaire_for_veteran_admin\', \'veterans_benefits\',\n    \'weeks_worked_in_year\', \'year\', \'label\'\n]\nFEATURE_COLUMNS = [\n    age, age_buckets, class_of_worker, detailed_industry_recode,\n    detailed_occupation_recode, education, wage_per_hour,\n    enroll_in_edu_inst_last_wk, marital_stat, major_industry_code,\n    major_occupation_code, race, hispanic_origin, sex, member_of_labor_union,\n    reason_for_unemployment, full_or_part_time_employment_stat, capital_gains,\n    capital_losses, dividends_from_stocks, tax_filer_stat,\n    region_of_previous_residence, state_of_previous_residence,\n    detailed_household_and_family_stat,\n    detailed_household_summary_in_household, instance_weight,\n    migration_code_change_in_msa, migration_code_change_in_reg,\n    migration_code_move_within_reg, live_in_this_house_1year_ago,\n    migration_prev_res_in_sunbelt, num_persons_worked_for_employer,\n    family_members_under18, country_of_birth_father, country_of_birth_mother,\n    country_of_birth_self, citizenship, own_business_or_self_employed,\n    fill_inc_questionnaire_for_veteran_admin, veterans_benefits,\n    weeks_worked_in_year, year\n]\n\nLABEL_COLUMN = \'label\'\nCONTINUOUS_COLUMNS = [\n    \'age\', \'wage_per_hour\', \'capital_gains\', \'capital_losses\',\n    \'dividends_from_stocks\', \'instance_weight\', \'weeks_worked_in_year\',\n    \'num_persons_worked_for_employer\'\n]\nCATEGORICAL_COLUMNS = [\n    \'class_of_worker\', \'detailed_industry_recode\',\n    \'detailed_occupation_recode\', \'education\', \'enroll_in_edu_inst_last_wk\',\n    \'marital_stat\', \'major_industry_code\', \'major_occupation_code\', \'race\',\n    \'hispanic_origin\', \'sex\', \'member_of_labor_union\',\n    \'reason_for_unemployment\', \'full_or_part_time_employment_stat\',\n    \'tax_filer_stat\', \'region_of_previous_residence\',\n    \'state_of_previous_residence\', \'detailed_household_and_family_stat\',\n    \'detailed_household_summary_in_household\', \'migration_code_change_in_msa\',\n    \'migration_code_change_in_reg\', \'migration_code_move_within_reg\',\n    \'live_in_this_house_1year_ago\', \'migration_prev_res_in_sunbelt\',\n    \'family_members_under18\', \'country_of_birth_father\',\n    \'country_of_birth_mother\', \'country_of_birth_self\', \'citizenship\',\n    \'own_business_or_self_employed\',\n    \'fill_inc_questionnaire_for_veteran_admin\', \'veterans_benefits\', \'year\'\n]\n\nTRAIN_FILE = \'../data/census/census-income.data\'\nTEST_FILE = \'../data/census/census-income.test\'\n\ndf_train = pd.read_csv(TRAIN_FILE, names=COLUMNS, skipinitialspace=True)\ndf_test = pd.read_csv(TEST_FILE, names=COLUMNS, skipinitialspace=True)\ndf_train = df_train.dropna(how=\'any\', axis=0)\ndf_test = df_test.dropna(how=\'any\', axis=0)\ndf_train[[\n    \'detailed_industry_recode\', \'detailed_occupation_recode\', \'year\',\n    \'own_business_or_self_employed\', \'veterans_benefits\'\n]] = df_train[[\n    \'detailed_industry_recode\', \'detailed_occupation_recode\', \'year\',\n    \'own_business_or_self_employed\', \'veterans_benefits\'\n]].astype(str)\ndf_test[[\n    \'detailed_industry_recode\', \'detailed_occupation_recode\', \'year\',\n    \'own_business_or_self_employed\', \'veterans_benefits\'\n]] = df_test[[\n    \'detailed_industry_recode\', \'detailed_occupation_recode\', \'year\',\n    \'own_business_or_self_employed\', \'veterans_benefits\'\n]].astype(str)\n\ndf_train[LABEL_COLUMN] = (\n    df_train[LABEL_COLUMN].apply(lambda x: \'+\' in x)).astype(int)\ndf_test[LABEL_COLUMN] = (\n    df_test[LABEL_COLUMN].apply(lambda x: \'+\' in x)).astype(int)\n# print df_train.dtypes\n# dtypes = df_train.dtypes\n\n# print dtypess[CATEGORICAL_COLUMNS]\n\n# print df_train.head(5)\n# print df_test.head(5)\n\n\ndef input_fn(df):\n    # Creates a dictionary mapping from each continuous feature column name (k) to\n    # # the values of that column stored in a constant Tensor.\n    continuous_cols = {\n        k: tf.constant(df[k].values)\n        for k in CONTINUOUS_COLUMNS\n    }\n    # Creates a dictionary mapping from each categorical feature column name (k)\n    # to the values of that column stored in a tf.SparseTensor.\n    categorical_cols = {\n        k: tf.SparseTensor(\n            indices=[[i, 0] for i in range(df[k].size)],\n            values=df[k].values,\n            dense_shape=[df[k].size, 1])\n        for k in CATEGORICAL_COLUMNS\n    }\n    # Merges the two dictionaries into one.\n    feature_cols = dict(continuous_cols.items() + categorical_cols.items())\n    # Converts the label column into a constant Tensor.\n    label = tf.constant(df[LABEL_COLUMN].values)\n    # Returns the feature columns and the label.\n    return feature_cols, label\n\n\ndef train_input_fn():\n    return input_fn(df_train)\n\n\ndef eval_input_fn():\n    return input_fn(df_test)\n\n\n\n# deep columns\n""""""\nage = tf.contrib.layers.real_valued_column(\'age\')\nage_buckets = tf.contrib.layers.bucketized_column(\n    age, boundaries=[18, 25, 30, 35, 40, 45, 50, 55, 60, 65])\nwage_per_hour = tf.contrib.layers.real_valued_column(\'wage_per_hour\')\ncapital_gains = tf.contrib.layers.real_valued_column(\'capital_gains\')\ncapital_losses = tf.contrib.layers.real_valued_column(\'capital_losses\')\ndividends_from_stocks = tf.contrib.layers.real_valued_column(\n    \'dividends_from_stocks\')\ninstance_weight = tf.contrib.layers.real_valued_column(\'instance_weight\')\nweeks_worked_in_year = tf.contrib.layers.real_valued_column(\n    \'weeks_worked_in_year\')\nnum_persons_worked_for_employer = tf.contrib.layers.real_valued_column(\n    \'num_persons_worked_for_employer\')\n\n\nCATEGORICAL_COLUMNS = [\n    \'class_of_worker\', \'detailed_industry_recode\',\n    \'detailed_occupation_recode\', \'education\', \'enroll_in_edu_inst_last_wk\',\n    \'marital_stat\', \'major_industry_code\', \'major_occupation_code\', \'race\',\n    \'hispanic_origin\', \'sex\', \'member_of_labor_union\',\n    \'reason_for_unemployment\', \'full_or_part_time_employment_stat\',\n    \'tax_filer_stat\', \'region_of_previous_residence\',\n    \'state_of_previous_residence\', \'detailed_household_and_family_stat\',\n    \'detailed_household_summary_in_household\', \'migration_code_change_in_msa\',\n    \'migration_code_change_in_reg\', \'migration_code_move_within_reg\',\n    \'live_in_this_house_1year_ago\', \'migration_prev_res_in_sunbelt\',\n    \'family_members_under18\', \'country_of_birth_father\',\n    \'country_of_birth_mother\', \'country_of_birth_self\', \'citizenship\',\n    \'own_business_or_self_employed\',\n    \'fill_inc_questionnaire_for_veteran_admin\', \'veterans_benefits\', \'year\'\n]\n""""""\nwide_columns = [\n    class_of_worker, detailed_industry_recode,\n    detailed_occupation_recode, education, enroll_in_edu_inst_last_wk,\n    marital_stat, major_industry_code, major_occupation_code, race,\n    hispanic_origin, sex, member_of_labor_union,\n    reason_for_unemployment, full_or_part_time_employment_stat,\n    tax_filer_stat, region_of_previous_residence,\n    state_of_previous_residence, detailed_household_and_family_stat,\n    detailed_household_summary_in_household, migration_code_change_in_msa,\n    migration_code_change_in_reg, migration_code_move_within_reg,\n    live_in_this_house_1year_ago, migration_prev_res_in_sunbelt,\n    family_members_under18, country_of_birth_father,\n    country_of_birth_mother, country_of_birth_self, citizenship,\n    own_business_or_self_employed,\n    fill_inc_questionnaire_for_veteran_admin, veterans_benefits, year,\n    age_buckets, tf.contrib.layers.crossed_column(columns=[age_buckets, class_of_worker], hash_bucket_size=1000),\n    tf.contrib.layers.crossed_column(columns=[age_buckets, education], hash_bucket_size=1000)\n]\ndeep_columns = [\n    age, wage_per_hour, capital_gains, capital_losses, dividends_from_stocks,\n    instance_weight, weeks_worked_in_year, num_persons_worked_for_employer,\n    tf.contrib.layers.embedding_column(detailed_industry_recode, dimension=8),\n    tf.contrib.layers.embedding_column(detailed_occupation_recode, dimension=8),\n    tf.contrib.layers.embedding_column(education, dimension=8),\n    tf.contrib.layers.embedding_column(enroll_in_edu_inst_last_wk, dimension=8),\n    tf.contrib.layers.embedding_column(marital_stat, dimension=8),\n    tf.contrib.layers.embedding_column(major_industry_code, dimension=8),\n    tf.contrib.layers.embedding_column(major_occupation_code, dimension=8),\n    tf.contrib.layers.embedding_column(race, dimension=8),\n    tf.contrib.layers.embedding_column(hispanic_origin, dimension=8),\n    tf.contrib.layers.embedding_column(member_of_labor_union, dimension=8),\n    tf.contrib.layers.embedding_column(reason_for_unemployment, dimension=8),\n    tf.contrib.layers.embedding_column(full_or_part_time_employment_stat, dimension=8),\n    tf.contrib.layers.embedding_column(tax_filer_stat, dimension=8),\n    tf.contrib.layers.embedding_column(region_of_previous_residence, dimension=8),\n    tf.contrib.layers.embedding_column(state_of_previous_residence, dimension=8),\n    tf.contrib.layers.embedding_column(detailed_household_and_family_stat, dimension=8),\n    tf.contrib.layers.embedding_column(detailed_household_summary_in_household, dimension=8),\n    tf.contrib.layers.embedding_column(migration_code_change_in_msa, dimension=8),\n    tf.contrib.layers.embedding_column(migration_code_change_in_reg, dimension=8),\n    tf.contrib.layers.embedding_column(migration_code_move_within_reg, dimension=8),\n    tf.contrib.layers.embedding_column(live_in_this_house_1year_ago, dimension=8),\n    tf.contrib.layers.embedding_column(migration_prev_res_in_sunbelt, dimension=8),\n    tf.contrib.layers.embedding_column(family_members_under18, dimension=8),\n    tf.contrib.layers.embedding_column(country_of_birth_father, dimension=8),\n    tf.contrib.layers.embedding_column(country_of_birth_mother, dimension=8),\n    tf.contrib.layers.embedding_column(country_of_birth_self, dimension=8),\n    tf.contrib.layers.embedding_column(citizenship, dimension=8),\n    tf.contrib.layers.embedding_column(own_business_or_self_employed, dimension=8),\n    tf.contrib.layers.embedding_column(fill_inc_questionnaire_for_veteran_admin, dimension=8),\n    tf.contrib.layers.embedding_column(veterans_benefits, dimension=8),\n    tf.contrib.layers.one_hot_column(sex),\n    tf.contrib.layers.one_hot_column(year)\n]\nmodel_dir = FLAGS.model_dir\ntrain_step = FLAGS.train_steps\nvalidation_metrics = {\n    ""accuracy"":\n    tf.contrib.learn.MetricSpec(\n        metric_fn=tf.contrib.metrics.streaming_accuracy,\n        prediction_key=""classes""),\n    ""precision"":\n    tf.contrib.learn.MetricSpec(\n         metric_fn=tf.contrib.metrics.streaming_precision,\n         prediction_key=""classes""),\n    ""recall"":\n    tf.contrib.learn.MetricSpec(\n        metric_fn=tf.contrib.metrics.streaming_recall,\n        prediction_key=""classes"")\n    }\nvalidation_monitor = tf.contrib.learn.monitors.ValidationMonitor(input_fn=eval_input_fn, \n    every_n_steps=10, metrics=validation_metrics, eval_steps=1)\nif FLAGS.classifier_mode == \'wide\':\n    model = tf.contrib.learn.LinearClassifier(model_dir=model_dir, \n        feature_columns=wide_columns, config=tf.contrib.learn.RunConfig(save_checkpoints_secs=60))\nelif FLAGS.classifier_mode == \'deep\':\n    model = tf.contrib.learn.DNNClassifier(model_dir=model_dir, feature_columns=deep_columns, hidden_units=[128, 64], config=tf.contrib.learn.RunConfig(save_checkpoints_secs=60))\nelse:\n    model = tf.contrib.learn.DNNLinearCombinedClassifier(\n        model_dir=model_dir,\n        linear_feature_columns=wide_columns,\n        dnn_feature_columns=deep_columns,\n        dnn_hidden_units=[128, 64],\n        fix_global_step_increment_bug=True,\n        config=tf.contrib.learn.RunConfig(save_checkpoints_secs=60))\n\nmodel.fit(input_fn=train_input_fn, steps=train_step, monitors=[validation_monitor])\nresults = model.evaluate(input_fn=eval_input_fn, steps=1)\nfor key in results:\n    print ""%s: %s"" % (key, results[key])'"
zhihu_code/src/gen_verification.py,0,"b'# -*- coding: utf-8 -*-\nfrom PIL import Image,ImageDraw,ImageFont\nimport random\nimport math, string\nimport logging\n# logger = logging.Logger(name=\'gen verification\')\n\nclass RandomChar():\n    @staticmethod\n    def Unicode():\n        val = random.randint(0x4E00, 0x9FBF)\n        return unichr(val)    \n\n    @staticmethod\n    def GB2312():\n        head = random.randint(0xB0, 0xCF)\n        body = random.randint(0xA, 0xF)\n        tail = random.randint(0, 0xF)\n        val = ( head << 8 ) | (body << 4) | tail\n        str = ""%x"" % val\n        return str.decode(\'hex\').decode(\'gb2312\')    \n\nclass ImageChar():\n    def __init__(self, fontColor = (0, 0, 0),\n     size = (100, 40),\n     fontPath = \'/Library/Fonts/Arial Unicode.ttf\',\n     bgColor = (255, 255, 255),\n     fontSize = 20):\n        self.size = size\n        self.fontPath = fontPath\n        self.bgColor = bgColor\n        self.fontSize = fontSize\n        self.fontColor = fontColor\n        self.font = ImageFont.truetype(self.fontPath, self.fontSize)\n        self.image = Image.new(\'RGB\', size, bgColor)\n\n    def drawText(self, pos, txt, fill):\n        draw = ImageDraw.Draw(self.image)\n        draw.text(pos, txt, font=self.font, fill=fill)\n        del draw    \n    \n    def drawTextV2(self, pos, txt, fill, angle=180):\n        image=Image.new(\'RGB\', (25,25), (255,255,255))\n        draw = ImageDraw.Draw(image)\n        draw.text( (0, -3), txt,  font=self.font, fill=fill)\n        w=image.rotate(angle,  expand=1)\n        self.image.paste(w, box=pos)\n        del draw\n\n    def randRGB(self):\n        return (0,0,0)\n\n    def randChinese(self, num, num_flip):\n        gap = 1\n        start = 0\n        num_flip_list = random.sample(range(num), num_flip)\n        # logger.info(\'num flip list:{0}\'.format(num_flip_list))\n        print \'num flip list:{0}\'.format(num_flip_list)\n        char_list = []\n        for i in range(0, num):\n            char = RandomChar().GB2312()\n            char_list.append(char)\n            x = start + self.fontSize * i + gap + gap * i\n            if i in num_flip_list:\n                self.drawTextV2((x, 6), char, self.randRGB())\n            else:\n                self.drawText((x, 0), char, self.randRGB())\n        return char_list, num_flip_list\n    def save(self, path):\n        self.image.save(path)\n\n\n\nerr_num = 0\nfor i in range(10):\n    try:\n        ic = ImageChar(fontColor=(100,211, 90), size=(280,28), fontSize = 25)\n        num_flip = random.randint(3,6)\n        char_list, num_flip_list = ic.randChinese(10, num_flip)\n        ic.save(\'\'.join(char_list)+\'_\'+\'\'.join(str(i) for i in num_flip_list)+"".jpeg"")\n    except:\n        err_num += 1\n        continue\n# print \'\'.join(char_list)\n# print \'\'.join(str(i) for i in num_flip_list)'"
zhihu_code/src/train.py,81,"b'import tensorflow as tf\nimport random\nimport os\nimport numpy as np\nimport tensorflow.contrib.slim as slim\nimport time\nimport logging\nfrom PIL import Image\n\n\nlogger = logging.getLogger(\'Training a chiness write char recognition\')\nlogger.setLevel(logging.INFO)\n# formatter = logging.Formatter(\'%(asctime)s - %(name)s - %(levelname)s - %(message)s\')\n# fh = logging.FileHandler(\'recogniiton.log\')\n# fh.setFormatter(formatter)\nch = logging.StreamHandler()\nch.setLevel(logging.INFO)\n# logger.addHandler(fh)\nlogger.addHandler(ch)\n\ntf.app.flags.DEFINE_boolean(\'random_brightness\', True, ""whether to adjust brightness"")\n# tf.app.flags.DEFINE_boolean(\'random_contrast\', True, ""whether to random constrast"")\n\ntf.app.flags.DEFINE_integer(\'image_width\',280,\n                            ""the width of image "")\ntf.app.flags.DEFINE_integer(\'image_height\', 28, \'the height of image\')\ntf.app.flags.DEFINE_boolean(\'gray\', True, ""whethet to change the rbg to gray"")\ntf.app.flags.DEFINE_integer(\'max_steps\', 100000, \'the max training steps \')\ntf.app.flags.DEFINE_integer(\'eval_steps\', 100, ""the step num to eval"")\ntf.app.flags.DEFINE_integer(\'save_steps\', 10000, ""the steps to save"")\ntf.app.flags.DEFINE_string(\'checkpoint_dir\', \'./checkpoint\', \'the checkpoint dir\')\ntf.app.flags.DEFINE_string(\'train_data_dir\',\'../data/train_data\',\'the train dataset dir\')\ntf.app.flags.DEFINE_string(\'test_data_dir\',\'../data/test_data\',\'the test dataset dir\')\ntf.app.flags.DEFINE_boolean(\'restore\',False, \'whether to restore from checkpoint\')\ntf.app.flags.DEFINE_boolean(\'batch_size\', 128, \'the batch size during train and val\')\ntf.app.flags.DEFINE_string(\'mode\', \'train\', \'the run mode\')\n\nFLAGS = tf.app.flags.FLAGS\n\nclass DataIterator:\n    def __init__(self, data_dir):\n        self.image_names = []\n        for root, sub_folder, file_list in os.walk(data_dir):\n            self.image_names += [os.path.join(root, file_path) for file_path in file_list]\n        random.shuffle(self.image_names)\n        self.labels = [self.get_label(file_name.split(\'/\')[-1].split(\'.\')[0].split(\'_\')[-1]) for file_name in self.image_names]\n        print len(self.labels)\n    @property\n    def size(self):\n        return len(self.labels)\n\n    def get_label(self, str_label):\n        """"""\n        Convert the str_label to 10 binary code, 385 to 0001010010\n        """"""\n        result = [0]*10\n        for i in str_label:\n            result[int(i)] = 1\n        return result\n\n    @staticmethod\n    def data_augmentation(images):\n        if FLAGS.random_brightness:\n            images = tf.image.random_brightness(images, max_delta=0.3)\n        return images\n\n    def input_pipeline(self, batch_size, num_epochs=None, aug=False):\n        images_tensor = tf.convert_to_tensor(self.image_names, dtype=tf.string)\n        labels_tensor = tf.convert_to_tensor(self.labels, dtype=tf.int64)\n        input_queue = tf.train.slice_input_producer([images_tensor, labels_tensor], num_epochs=num_epochs)\n\n        labels = input_queue[1]\n        images_content = tf.read_file(input_queue[0])\n        images = tf.image.convert_image_dtype(tf.image.decode_jpeg(images_content, channels=1), tf.float32)\n        if aug:\n            images = self.data_augmentation(images)\n        new_size = tf.constant([FLAGS.image_height, FLAGS.image_width], dtype=tf.int32)\n        images = tf.image.resize_images(images, new_size)\n        image_batch, label_batch = tf.train.shuffle_batch([images, labels], batch_size=batch_size, capacity=50000,\n                                                          min_after_dequeue=10000)\n        return image_batch, label_batch\n\n\n\ndef network():\n    images = tf.placeholder(dtype=tf.float32, shape=[None, 28, 280, 1], name=\'image_batch\')\n    labels = tf.placeholder(dtype=tf.int32, shape=[None, 10], name=\'label_batch\')\n    endpoints = {}\n    conv_1 = slim.conv2d(images, 32, [5,5],1, padding=\'SAME\')\n    avg_pool_1 = slim.avg_pool2d(conv_1, [2,2],[1,1], padding=\'SAME\')\n    conv_2 = slim.conv2d(avg_pool_1, 32, [5,5], 1,padding=\'SAME\')\n    avg_pool_2 = slim.avg_pool2d(conv_2, [2,2],[1,1], padding=\'SAME\')\n    conv_3 = slim.conv2d(avg_pool_2, 32, [3,3])\n    avg_pool_3 = slim.avg_pool2d(conv_3, [2,2], [1,1])\n    flatten = slim.flatten(avg_pool_3)\n    fc1 = slim.fully_connected(flatten, 512, activation_fn=None)\n    out0 = slim.fully_connected(fc1,2, activation_fn=None)\n    out1 = slim.fully_connected(fc1,2, activation_fn=None)\n    out2 = slim.fully_connected(fc1,2, activation_fn=None)\n    out3 = slim.fully_connected(fc1,2, activation_fn=None)\n    out4 = slim.fully_connected(fc1,2, activation_fn=None)\n    out5 = slim.fully_connected(fc1,2, activation_fn=None)\n    out6 = slim.fully_connected(fc1,2, activation_fn=None)\n    out7 = slim.fully_connected(fc1,2, activation_fn=None)\n    out8 = slim.fully_connected(fc1,2, activation_fn=None)\n    out9 = slim.fully_connected(fc1,2, activation_fn=None)\n    global_step = tf.Variable(initial_value=0)\n    out0_argmax = tf.expand_dims(tf.argmax(out0, 1), 1)\n    out1_argmax = tf.expand_dims(tf.argmax(out1, 1), 1)\n    out2_argmax = tf.expand_dims(tf.argmax(out2, 1), 1)\n    out3_argmax = tf.expand_dims(tf.argmax(out3, 1), 1)\n    out4_argmax = tf.expand_dims(tf.argmax(out4, 1), 1)\n    out5_argmax = tf.expand_dims(tf.argmax(out5, 1), 1)\n    out6_argmax = tf.expand_dims(tf.argmax(out6, 1), 1)\n    out7_argmax = tf.expand_dims(tf.argmax(out7, 1), 1)\n    out8_argmax = tf.expand_dims(tf.argmax(out8, 1), 1)\n    out9_argmax = tf.expand_dims(tf.argmax(out9, 1), 1)\n    out_score = tf.concat([out0, out1, out2, out3, out4, out5, out6, out7, out8, out9], axis=1)\n    out_final = tf.cast(tf.concat([out0_argmax, out1_argmax, out2_argmax, out3_argmax, out4_argmax, out5_argmax, out6_argmax, out7_argmax, out8_argmax, out9_argmax], axis=1), tf.int32)\n\n    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=out0, labels=tf.one_hot(labels[:,0],depth=2)))\n    loss1 = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=out1, labels=tf.one_hot(labels[:,1],depth=2)))\n    loss2 = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=out2, labels=tf.one_hot(labels[:,2],depth=2)))\n    loss3 = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=out3, labels=tf.one_hot(labels[:,3],depth=2)))\n    loss4 = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=out4, labels=tf.one_hot(labels[:,4],depth=2)))\n    loss5 = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=out5, labels=tf.one_hot(labels[:,5],depth=2)))\n    loss6 = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=out6, labels=tf.one_hot(labels[:,6],depth=2)))\n    loss7 = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=out7, labels=tf.one_hot(labels[:,7],depth=2)))\n    loss8 = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=out8, labels=tf.one_hot(labels[:,8],depth=2)))\n    loss9 = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=out9, labels=tf.one_hot(labels[:,9],depth=2)))\n    loss_list= [loss, loss1, loss2, loss3,loss4, loss5, loss6, loss7, loss8, loss9]\n    loss_sum = tf.reduce_sum(loss_list)\n    train_op = tf.train.AdamOptimizer(learning_rate=0.0001).minimize(loss_sum, global_step=global_step)\n    accuracy = tf.reduce_mean(tf.cast(tf.reduce_all(tf.equal(out_final, labels), axis=1), tf.float32))\n    try:\n        tf.summary.scalar(\'loss_sum\', loss_sum)\n        tf.summary.scalar(\'accuracy\', accuracy)\n        merged_summary_op = tf.summary.merge_all()\n    except AttributeError:\n        tf.scalar_summary(\'loss_sum\', loss_sum)\n        tf.scalar_summary(\'accuracy\', accuracy)\n        merged_summary_op = tf.merge_all_summaries()\n\n    endpoints[\'global_step\'] = global_step\n    endpoints[\'images\'] = images\n    endpoints[\'labels\'] = labels\n    endpoints[\'train_op\'] = train_op\n    endpoints[\'loss_sum\'] = loss_sum\n    endpoints[\'accuracy\'] = accuracy\n    endpoints[\'merged_summary_op\'] = merged_summary_op\n    endpoints[\'out_final\'] = out_final\n    endpoints[\'out_score\'] = out_score\n    return endpoints\n\ndef validation():\n    # it should be fixed by using placeholder with epoch num in train stage\n    logger.info(""=======Validation Beigin======="")\n    test_feeder = DataIterator(data_dir=\'../data/test_data/\')\n    predict_labels_list = []\n    groundtruth = []\n    with tf.Session() as sess:\n        test_images, test_labels = test_feeder.input_pipeline(batch_size=FLAGS.batch_size,num_epochs=1)\n        endpoints = network()\n        try:\n            sess.run(tf.global_variables_initializer())\n            sess.run(tf.local_variables_initializer())\n        except:            \n            sess.run(tf.initialize_all_variables())\n            sess.run(tf.initialize_local_variables())\n\n\n        coord = tf.train.Coordinator()\n        threads = tf.train.start_queue_runners(sess=sess, coord=coord)\n        saver = tf.train.Saver()\n        ckpt = tf.train.latest_checkpoint(FLAGS.checkpoint_dir)\n        if ckpt:\n            saver.restore(sess, ckpt)\n            logger.info(\'restore from the checkpoint {0}\'.format(ckpt))\n        logger.info(\'======Start Validation=======\')\n        try:\n            i = 0\n            acc_sum = 0.0\n            while not coord.should_stop():\n                i += 1\n                start_time = time.time()\n                test_images_batch, test_labels_batch = sess.run([test_images, test_labels])\n                feed_dict = {endpoints[\'images\']:test_images_batch, endpoints[\'labels\']: test_labels_batch}\n                labels_batch, predict_labels_batch, acc = sess.run([endpoints[\'labels\'],endpoints[\'out_final\'], endpoints[\'accuracy\']], feed_dict=feed_dict)\n                predict_labels_list += predict_labels_batch.tolist()\n                groundtruth += labels_batch.tolist()\n                acc_sum += acc\n                logger.info(\'the batch {0} takes {1} seconds, accuracy {2}\'.format(i, time.time()-start_time, acc))\n        except tf.errors.OutOfRangeError:\n            logger.info(\'==================Validation Finished===================\')\n            logger.info(\'The finally accuracy {0}\'.format(acc_sum/i))\n        finally:\n            coord.request_stop()\n        coord.join(threads)\n    return {\'predictions\':predict_labels_list, \'gt_labels\':groundtruth}\n\ndef inference(image):\n    logger.info(\'============inference==========\')\n    temp_image = Image.open(image).convert(\'L\')\n    # temp_image = temp_image.resize((FLAGS.image_height, FLAGS.image_size),Image.ANTIALIAS)\n    temp_image = np.asarray(temp_image) / 255.0\n    temp_image = temp_image.reshape([-1, 28, 280, 1])\n    sess = tf.Session()\n    logger.info(\'========start inference============\')\n    # images = tf.placeholder(dtype=tf.float32, shape=[None, 280, 28, 1])\n    endpoints = network()\n    saver = tf.train.Saver()\n    ckpt = tf.train.latest_checkpoint(FLAGS.checkpoint_dir)\n    if ckpt:\n        saver.restore(sess, ckpt)\n    feed_dict = {endpoints[\'images\']: temp_image}\n    predict_val, predict_index = sess.run([endpoints[\'out_score\'],endpoints[\'out_final\']], feed_dict=feed_dict)\n    sess.close()\n    return predict_val, predict_index\n    \n    \n\n\ndef train():\n    train_feeder = DataIterator(data_dir=FLAGS.train_data_dir)\n    test_feeder = DataIterator(data_dir=FLAGS.test_data_dir)\n    with tf.Session() as sess:\n        train_images, train_labels = train_feeder.input_pipeline(batch_size=FLAGS.batch_size, aug=True)\n        test_images, test_labels = test_feeder.input_pipeline(batch_size=FLAGS.batch_size)\n        endpoints = network()\n        sess.run(tf.global_variables_initializer())\n        coord = tf.train.Coordinator()\n        threads = tf.train.start_queue_runners(sess=sess, coord=coord)\n        saver = tf.train.Saver()\n\n        train_writer = tf.summary.FileWriter(\'./log\' + \'/train\',sess.graph)\n        test_writer = tf.summary.FileWriter(\'./log\' + \'/val\')\n        start_step = 0\n        if FLAGS.restore:\n            ckpt = tf.train.latest_checkpoint(FLAGS.checkpoint_dir)\n            if ckpt:\n                saver.restore(sess, ckpt)\n                print ""restore from the checkpoint {0}"".format(ckpt)\n                start_step += int(ckpt.split(\'-\')[-1])\n        logger.info(\':::Training Start:::\')\n        try:\n            while not coord.should_stop():\n                start_time = time.time()\n                train_images_batch, train_labels_batch = sess.run([train_images, train_labels])\n                feed_dict = {endpoints[\'images\']: train_images_batch, endpoints[\'labels\']: train_labels_batch}\n                _, loss_val, train_summary, step = sess.run([endpoints[\'train_op\'], endpoints[\'loss_sum\'], endpoints[\'merged_summary_op\'], endpoints[\'global_step\']], feed_dict=feed_dict)\n                train_writer.add_summary(train_summary, step)\n                end_time = time.time()\n                logger.info(""[train] the step {0} takes {1} loss {2}"".format(step, end_time-start_time, loss_val))\n                if step > FLAGS.max_steps:\n                    break\n                if step % FLAGS.eval_steps == 1:\n                    logger.info(\'========Begin eval stage =========\')\n                    start_time = time.time()\n                    # can\'t run \n                    test_images_batch, test_labels_batch = sess.run([test_images, test_labels])\n                    logger.info(\'[test] gen test batch spend {0}\'.format(time.time()-start_time))\n                    feed_dict = {\n                        endpoints[\'images\']: test_images_batch,\n                        endpoints[\'labels\']: test_labels_batch\n                    }\n                    accuracy_val,test_summary = sess.run([endpoints[\'accuracy\'], endpoints[\'merged_summary_op\']], feed_dict=feed_dict)\n                    end_time = time.time()\n                    test_writer.add_summary(test_summary, step)\n                    logger.info( \'[test] the step {0} accuracy {1} spend time {2}\'.format(step, accuracy_val, (end_time-start_time)))\n                if step % FLAGS.save_steps == 1:\n                    logger.info(\'Save the ckpt of {0}\'.format(step))\n                    saver.save(sess, os.path.join(FLAGS.checkpoint_dir, \'my-model\'), global_step=endpoints[\'global_step\'])\n        except tf.errors.OutOfRangeError:\n            # print ""============train finished=========""\n            logger.info(\'==================Train Finished================\')\n            saver.save(sess, os.path.join(FLAGS.checkpoint_dir, \'my-model\'), global_step=endpoints[\'global_step\'])\n        finally:\n            coord.request_stop()\n        coord.join(threads)\n\ndef run():\n    print FLAGS.mode\n    if FLAGS.mode == ""train"":\n        train()\n    elif FLAGS.mode == \'validation\':\n        result_dict = validation()\n        result_file = \'result.dict\'\n        logger.info(\'Write result into {0}\')\n        import pickle\n        f = open(result_file, \'wb\')\n        pickle.dump(result_dict, f)\n        f.close()\n        logger.info(\'Write file ends\')\n    elif FLAGS.mode == \'inference\':\n        print \'inference\'\n        image_file = \'../data/test_data/092e9ae8-ee91-11e6-91c1-525400551618_69128.jpeg\'\n        final_predict_val, final_predict_index = inference(image_file)\n        logger.info(\'the result info: predict index {0} predict_val {1}\'.format( final_predict_index, final_predict_val))\n\nif __name__ == \'__main__\':\n    run()\n'"
nlp/NMT/scripts/__init__.py,0,b''
nlp/NMT/scripts/config.py,0,"b'import re\nimport logging\nimport sys\n\nNON_ALPHA_PAT = re.compile(\'[\\.,-]\')\nCOPUS_TYPE = ""middle""\nTRAIN_X = ""train.en""\nTRAIN_Y = ""train.vi""\nTEST_X_2012 = ""tst2012.en""\nTEST_Y_2012 = ""tst2012.cs""\nINPUR_SEQ_LENGTH = 30\n\nlogger = logging.getLogger(""Neural Machine Translator"")\nformatter = logging.Formatter(\'%(asctime)s %(levelname)-8s: %(message)s\')\nfile_handler = logging.FileHandler(""test.log"")\nfile_handler.setFormatter(formatter)\nconsole_handler = logging.StreamHandler(sys.stdout)\nconsole_handler.formatter = formatter\nlogger.addHandler(file_handler)\nlogger.addHandler(console_handler)\nlogger.setLevel(logging.INFO)'"
nlp/Tag2Vec/scripts/gen_w2v.py,0,"b""#-*-coding:utf-8-*-\nfrom gensim.models import word2vec\n# from config import *\nimport logging\nlogging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\nsentence = word2vec.LineSentence(\n    '../data/tag_day_ok.csv'\n)\nmodel = word2vec.Word2Vec(sentences=sentence, size=50, workers=4, min_count=5)\nnews_w2v = '../data/tag_word2vec.model'\nmodel.save(news_w2v)\n"""
nlp/Tag2Vec/scripts/visual_embeddings.py,0,"b'from sklearn.manifold import TSNE\nimport matplotlib.pyplot as plt\nimport pickle\nimport numpy as np\nplt.rcParams[\'font.sans-serif\'] = [\'SimHei\']\nplt.rcParams[\'axes.unicode_minus\'] = False\nnp.random.seed(1)\n\n# load final_embeddings\nfinal_embeddings = pickle.load(open(""../data/final_embeddings.model"", ""r""))\nreverse_dictionary = pickle.load(open(""../data/reverse_dictionary.dict"", ""r""))\ndictionary = dict(zip(reverse_dictionary.values(), reverse_dictionary.keys()))\n\n# read from t_tag_infos.csv and save in a dict\ntag_id_name = {\'UNK\': ""UNk""}\nwith open(""../data/t_tag_infos.csv"", ""r"") as fread:\n    for line in fread.readlines():\n        tag_id_name_list = line.split(""\\t"")\n        tag_id = tag_id_name_list[0]\n        tag_name = tag_id_name_list[1].strip()\n        tag_id_name[tag_id] = tag_name\n\n\ndef plot_with_labels(low_dim_embs, labels, filename=\'tsne.png\'):\n    assert low_dim_embs.shape[0] >= len(labels), \'More labels than embeddings\'\n    plt.figure(figsize=(18, 18))  # in inches\n    for i, label in enumerate(labels):\n        x, y = low_dim_embs[i, :]\n        plt.scatter(x, y)\n        plt.annotate(\n            label,\n            xy=(x, y),\n            xytext=(5, 2),\n            textcoords=\'offset points\',\n            ha=\'right\',\n            va=\'bottom\')\n\n    plt.savefig(filename)\n\n\ntsne = TSNE(\n    perplexity=30,\n    n_components=2,\n    init=\'pca\',\n    random_state=1,\n    n_iter=5000,\n    method=\'exact\')\nplot_only = 500\nprint(""final_embeddings size "" + str(len(final_embeddings)))\nvalid_embeddings = final_embeddings[:plot_only, :]\nvalid_index = []\nfor index in xrange(plot_only):\n    key = reverse_dictionary[index]\n    if tag_id_name.has_key(key):\n        valid_index.append(index)\n\nlow_dim_embs = tsne.fit_transform(valid_embeddings[valid_index])\nlabels = [\n    tag_id_name[reverse_dictionary[i]].decode(\'utf-8\') for i in valid_index\n]\nplot_with_labels(low_dim_embs, labels)\n\n\ndef get_topk(index, final_embeddings, k=10):\n    print index\n    presentation_labels = []\n    similarity = np.matmul(final_embeddings, np.transpose(final_embeddings))\n    nearest = (-similarity[index, :]).argsort()[1:10 + 1]\n    print nearest\n    for k in nearest:\n        presentation_labels.append(tag_id_name[reverse_dictionary[k]])\n\n    print ""{0} nearest labels : {1}"".format(\n        tag_id_name[reverse_dictionary[index]], \' \'.join(presentation_labels))\n\n\n# 1000629\nprint dictionary[\'1000121\']\nget_topk(dictionary[\'1000121\'], final_embeddings, k=10)'"
nlp/Tag2Vec/scripts/visual_word2vec.py,0,"b'#-*-coding:utf-8-*-\nimport gensim\nimport matplotlib.pyplot as plt\nfrom sklearn.manifold import TSNE\nplt.rcParams[\'font.sans-serif\'] = [\'SimHei\']\nplt.rcParams[\'axes.unicode_minus\'] = False\nmodel = gensim.models.Word2Vec.load(""../data/tag_word2vec.model"")\ntag_id_name = {\'UNK\': ""UNk""}\n# tag_id=>tag_name\nwith open(""../data/t_tag_infos.csv"", ""r"") as fread:\n    for line in fread.readlines():\n        tag_id_name_list = line.split(""\\t"")\n        tag_id = tag_id_name_list[0]\n        tag_name = tag_id_name_list[1].strip()\n        tag_id_name[tag_id] = tag_name\n\ntsne = TSNE(\n    perplexity=30,\n    n_components=2,\n    init=\'pca\',\n    random_state=1,\n    n_iter=5000,\n    method=\'exact\')\n\n\ndef plot_with_labels(low_dim_embs, labels, filename=\'tsne.png\'):\n    assert low_dim_embs.shape[0] >= len(labels), \'More labels than embeddings\'\n    plt.figure(figsize=(18, 18))  # in inches\n    for i, label in enumerate(labels):\n        x, y = low_dim_embs[i, :]\n        plt.scatter(x, y)\n        plt.annotate(\n            label,\n            xy=(x, y),\n            xytext=(5, 2),\n            textcoords=\'offset points\',\n            ha=\'right\',\n            va=\'bottom\')\n\n    plt.savefig(filename)\n    # plt.imshow()\n\n\nX = model[model.wv.vocab]\nX_tsne = tsne.fit_transform(X[:500])\nlabels = model.wv.vocab.keys()[:500]\nlabels = [tag_id_name[i].decode(\'utf-8\') for i in labels]\nplot_with_labels(X_tsne, labels)\n\ntag_name_id = dict(zip(tag_id_name.values(), tag_id_name.keys()))\n\n\ndef get_topk(tag_word, model, topk=50):\n    nearest_list = model.wv.similar_by_word(tag_name_id[tag_word], topn=topk)\n    nearest_words = [tag_id_name[i[0]] for i in nearest_list]\n    # nearest_words_score = [tag_id_name[i] for i in nearest_list]\n    print ""near the {0}, the top {1} words are {2}"".format(\n        tag_word, topk, \' \'.join(nearest_words))\n\n\nget_topk(""\xe7\x9f\xa5\xe4\xb9\x8e"", model)\n'"
nlp/Tag2Vec/scripts/word2vec.py,52,"b'""""""Multi-threaded word2vec mini-batched skip-gram model.\nTrains the model described in:\n(Mikolov, et. al.) Efficient Estimation of Word Representations in Vector Space\nICLR 2013.\nhttp://arxiv.org/abs/1301.3781\nThis model does traditional minibatching.\nThe key ops used are:\n* placeholder for feeding in tensors for each example.\n* embedding_lookup for fetching rows from the embedding matrix.\n* sigmoid_cross_entropy_with_logits to calculate the loss.\n* GradientDescentOptimizer for optimizing the loss.\n* skipgram custom op that does input processing.\n""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport os\nimport sys\nimport threading\nimport time\n\nfrom six.moves import xrange  # pylint: disable=redefined-builtin\n\nimport numpy as np\nimport tensorflow as tf\n\nword2vec = tf.load_op_library(\n    os.path.join(\n        os.path.dirname(os.path.realpath(__file__)), \'word2vec_ops.so\'))\n\nflags = tf.app.flags\n\nflags.DEFINE_string(""save_path"", None, ""Directory to write the model and ""\n                    ""training summaries."")\nflags.DEFINE_string(""train_data"", None, ""Training text file. ""\n                    ""E.g., unzipped file http://mattmahoney.net/dc/text8.zip."")\nflags.DEFINE_string(""eval_data"", None,\n                    ""File consisting of analogies of four tokens.""\n                    ""embedding 2 - embedding 1 + embedding 3 should be close ""\n                    ""to embedding 4.""\n                    ""See README.md for how to get \'questions-words.txt\'."")\nflags.DEFINE_integer(""embedding_size"", 200, ""The embedding dimension size."")\nflags.DEFINE_integer(\n    ""epochs_to_train"", 15,\n    ""Number of epochs to train. Each epoch processes the training data once ""\n    ""completely."")\nflags.DEFINE_float(""learning_rate"", 0.2, ""Initial learning rate."")\nflags.DEFINE_integer(""num_neg_samples"", 100,\n                     ""Negative samples per training example."")\nflags.DEFINE_integer(""batch_size"", 16,\n                     ""Number of training examples processed per step ""\n                     ""(size of a minibatch)."")\nflags.DEFINE_integer(""concurrent_steps"", 12,\n                     ""The number of concurrent training steps."")\nflags.DEFINE_integer(""window_size"", 5,\n                     ""The number of words to predict to the left and right ""\n                     ""of the target word."")\nflags.DEFINE_integer(""min_count"", 5,\n                     ""The minimum number of word occurrences for it to be ""\n                     ""included in the vocabulary."")\nflags.DEFINE_float(\n    ""subsample"", 1e-3,\n    ""Subsample threshold for word occurrence. Words that appear ""\n    ""with higher frequency will be randomly down-sampled. Set ""\n    ""to 0 to disable."")\nflags.DEFINE_boolean(\n    ""interactive"", False,\n    ""If true, enters an IPython interactive session to play with the trained ""\n    ""model. E.g., try model.analogy(b\'france\', b\'paris\', b\'russia\') and ""\n    ""model.nearby([b\'proton\', b\'elephant\', b\'maxwell\'])"")\nflags.DEFINE_integer(""statistics_interval"", 5,\n                     ""Print statistics every n seconds."")\nflags.DEFINE_integer(""summary_interval"", 5,\n                     ""Save training summary to file every n seconds (rounded ""\n                     ""up to statistics interval)."")\nflags.DEFINE_integer(""checkpoint_interval"", 600,\n                     ""Checkpoint the model (i.e. save the parameters) every n ""\n                     ""seconds (rounded up to statistics interval)."")\n\nFLAGS = flags.FLAGS\n\n\nclass Options(object):\n    """"""Options used by our word2vec model.""""""\n\n    def __init__(self):\n        # Model options.\n\n        # Embedding dimension.\n        self.emb_dim = FLAGS.embedding_size\n\n        # Training options.\n        # The training text file.\n        self.train_data = FLAGS.train_data\n\n        # Number of negative samples per example.\n        self.num_samples = FLAGS.num_neg_samples\n\n        # The initial learning rate.\n        self.learning_rate = FLAGS.learning_rate\n\n        # Number of epochs to train. After these many epochs, the learning\n        # rate decays linearly to zero and the training stops.\n        self.epochs_to_train = FLAGS.epochs_to_train\n\n        # Concurrent training steps.\n        self.concurrent_steps = FLAGS.concurrent_steps\n\n        # Number of examples for one training step.\n        self.batch_size = FLAGS.batch_size\n\n        # The number of words to predict to the left and right of the target word.\n        self.window_size = FLAGS.window_size\n\n        # The minimum number of word occurrences for it to be included in the\n        # vocabulary.\n        self.min_count = FLAGS.min_count\n\n        # Subsampling threshold for word occurrence.\n        self.subsample = FLAGS.subsample\n\n        # How often to print statistics.\n        self.statistics_interval = FLAGS.statistics_interval\n\n        # How often to write to the summary file (rounds up to the nearest\n        # statistics_interval).\n        self.summary_interval = FLAGS.summary_interval\n\n        # How often to write checkpoints (rounds up to the nearest statistics\n        # interval).\n        self.checkpoint_interval = FLAGS.checkpoint_interval\n\n        # Where to write out summaries.\n        self.save_path = FLAGS.save_path\n        if not os.path.exists(self.save_path):\n            os.makedirs(self.save_path)\n\n        # Eval options.\n        # The text file for eval.\n        self.eval_data = FLAGS.eval_data\n\n\nclass Word2Vec(object):\n    """"""Word2Vec model (Skipgram).""""""\n\n    def __init__(self, options, session):\n        self._options = options\n        self._session = session\n        self._word2id = {}\n        self._id2word = []\n        self.build_graph()\n        self.build_eval_graph()\n        self.save_vocab()\n\n    def read_analogies(self):\n        """"""Reads through the analogy question file.\n    Returns:\n      questions: a [n, 4] numpy array containing the analogy question\'s\n                 word ids.\n      questions_skipped: questions skipped due to unknown words.\n    """"""\n        questions = []\n        questions_skipped = 0\n        with open(self._options.eval_data, ""rb"") as analogy_f:\n            for line in analogy_f:\n                if line.startswith(b"":""):  # Skip comments.\n                    continue\n                words = line.strip().lower().split(b"" "")\n                ids = [self._word2id.get(w.strip()) for w in words]\n                if None in ids or len(ids) != 4:\n                    questions_skipped += 1\n                else:\n                    questions.append(np.array(ids))\n        print(""Eval analogy file: "", self._options.eval_data)\n        print(""Questions: "", len(questions))\n        print(""Skipped: "", questions_skipped)\n        self._analogy_questions = np.array(questions, dtype=np.int32)\n\n    def forward(self, examples, labels):\n        """"""Build the graph for the forward pass.""""""\n        opts = self._options\n\n        # Declare all variables we need.\n        # Embedding: [vocab_size, emb_dim]\n        init_width = 0.5 / opts.emb_dim\n        emb = tf.Variable(\n            tf.random_uniform([opts.vocab_size, opts.emb_dim], -init_width,\n                              init_width),\n            name=""emb"")\n        self._emb = emb\n\n        # Softmax weight: [vocab_size, emb_dim]. Transposed.\n        sm_w_t = tf.Variable(\n            tf.zeros([opts.vocab_size, opts.emb_dim]), name=""sm_w_t"")\n\n        # Softmax bias: [vocab_size].\n        sm_b = tf.Variable(tf.zeros([opts.vocab_size]), name=""sm_b"")\n\n        # Global step: scalar, i.e., shape [].\n        self.global_step = tf.Variable(0, name=""global_step"")\n\n        # Nodes to compute the nce loss w/ candidate sampling.\n        labels_matrix = tf.reshape(\n            tf.cast(\n                labels, dtype=tf.int64), [opts.batch_size, 1])\n\n        # Negative sampling.\n        sampled_ids, _, _ = (tf.nn.fixed_unigram_candidate_sampler(\n            true_classes=labels_matrix,\n            num_true=1,\n            num_sampled=opts.num_samples,\n            unique=True,\n            range_max=opts.vocab_size,\n            distortion=0.75,\n            unigrams=opts.vocab_counts.tolist()))\n\n        # Embeddings for examples: [batch_size, emb_dim]\n        example_emb = tf.nn.embedding_lookup(emb, examples)\n\n        # Weights for labels: [batch_size, emb_dim]\n        true_w = tf.nn.embedding_lookup(sm_w_t, labels)\n        # Biases for labels: [batch_size, 1]\n        true_b = tf.nn.embedding_lookup(sm_b, labels)\n\n        # Weights for sampled ids: [num_sampled, emb_dim]\n        sampled_w = tf.nn.embedding_lookup(sm_w_t, sampled_ids)\n        # Biases for sampled ids: [num_sampled, 1]\n        sampled_b = tf.nn.embedding_lookup(sm_b, sampled_ids)\n\n        # True logits: [batch_size, 1]\n        true_logits = tf.reduce_sum(tf.multiply(example_emb, true_w),\n                                    1) + true_b\n\n        # Sampled logits: [batch_size, num_sampled]\n        # We replicate sampled noise labels for all examples in the batch\n        # using the matmul.\n        sampled_b_vec = tf.reshape(sampled_b, [opts.num_samples])\n        sampled_logits = tf.matmul(\n            example_emb, sampled_w, transpose_b=True) + sampled_b_vec\n        return true_logits, sampled_logits\n\n    def nce_loss(self, true_logits, sampled_logits):\n        """"""Build the graph for the NCE loss.""""""\n\n        # cross-entropy(logits, labels)\n        opts = self._options\n        true_xent = tf.nn.sigmoid_cross_entropy_with_logits(\n            labels=tf.ones_like(true_logits), logits=true_logits)\n        sampled_xent = tf.nn.sigmoid_cross_entropy_with_logits(\n            labels=tf.zeros_like(sampled_logits), logits=sampled_logits)\n\n        # NCE-loss is the sum of the true and noise (sampled words)\n        # contributions, averaged over the batch.\n        nce_loss_tensor = (\n            tf.reduce_sum(true_xent) + tf.reduce_sum(sampled_xent)\n        ) / opts.batch_size\n        return nce_loss_tensor\n\n    def optimize(self, loss):\n        """"""Build the graph to optimize the loss function.""""""\n\n        # Optimizer nodes.\n        # Linear learning rate decay.\n        opts = self._options\n        words_to_train = float(opts.words_per_epoch * opts.epochs_to_train)\n        lr = opts.learning_rate * tf.maximum(\n            0.0001, 1.0 - tf.cast(self._words, tf.float32) / words_to_train)\n        self._lr = lr\n        optimizer = tf.train.GradientDescentOptimizer(lr)\n        train = optimizer.minimize(\n            loss,\n            global_step=self.global_step,\n            gate_gradients=optimizer.GATE_NONE)\n        self._train = train\n\n    def build_eval_graph(self):\n        """"""Build the eval graph.""""""\n        # Eval graph\n\n        # Each analogy task is to predict the 4th word (d) given three\n        # words: a, b, c.  E.g., a=italy, b=rome, c=france, we should\n        # predict d=paris.\n\n        # The eval feeds three vectors of word ids for a, b, c, each of\n        # which is of size N, where N is the number of analogies we want to\n        # evaluate in one batch.\n        analogy_a = tf.placeholder(dtype=tf.int32)  # [N]\n        analogy_b = tf.placeholder(dtype=tf.int32)  # [N]\n        analogy_c = tf.placeholder(dtype=tf.int32)  # [N]\n\n        # Normalized word embeddings of shape [vocab_size, emb_dim].\n        nemb = tf.nn.l2_normalize(self._emb, 1)\n\n        # Each row of a_emb, b_emb, c_emb is a word\'s embedding vector.\n        # They all have the shape [N, emb_dim]\n        a_emb = tf.gather(nemb, analogy_a)  # a\'s embs\n        b_emb = tf.gather(nemb, analogy_b)  # b\'s embs\n        c_emb = tf.gather(nemb, analogy_c)  # c\'s embs\n\n        # We expect that d\'s embedding vectors on the unit hyper-sphere is\n        # near: c_emb + (b_emb - a_emb), which has the shape [N, emb_dim].\n        target = c_emb + (b_emb - a_emb)\n\n        # Compute cosine distance between each pair of target and vocab.\n        # dist has shape [N, vocab_size].\n        dist = tf.matmul(target, nemb, transpose_b=True)\n\n        # For each question (row in dist), find the top 4 words.\n        _, pred_idx = tf.nn.top_k(dist, 4)\n\n        # Nodes for computing neighbors for a given word according to\n        # their cosine distance.\n        nearby_word = tf.placeholder(dtype=tf.int32)  # word id\n        nearby_emb = tf.gather(nemb, nearby_word)\n        nearby_dist = tf.matmul(nearby_emb, nemb, transpose_b=True)\n        nearby_val, nearby_idx = tf.nn.top_k(\n            nearby_dist, min(1000, self._options.vocab_size))\n\n        # Nodes in the construct graph which are used by training and\n        # evaluation to run/feed/fetch.\n        self._analogy_a = analogy_a\n        self._analogy_b = analogy_b\n        self._analogy_c = analogy_c\n        self._analogy_pred_idx = pred_idx\n        self._nearby_word = nearby_word\n        self._nearby_val = nearby_val\n        self._nearby_idx = nearby_idx\n\n    def build_graph(self):\n        """"""Build the graph for the full model.""""""\n        opts = self._options\n        # The training data. A text file.\n        (words, counts, words_per_epoch, self._epoch, self._words, examples,\n         labels) = word2vec.skipgram_word2vec(\n             filename=opts.train_data,\n             batch_size=opts.batch_size,\n             window_size=opts.window_size,\n             min_count=opts.min_count,\n             subsample=opts.subsample)\n        (opts.vocab_words, opts.vocab_counts,\n         opts.words_per_epoch) = self._session.run(\n             [words, counts, words_per_epoch])\n        opts.vocab_size = len(opts.vocab_words)\n        print(""Data file: "", opts.train_data)\n        print(""Vocab size: "", opts.vocab_size - 1, "" + UNK"")\n        print(""Words per epoch: "", opts.words_per_epoch)\n        self._examples = examples\n        self._labels = labels\n        self._id2word = opts.vocab_words\n        for i, w in enumerate(self._id2word):\n            self._word2id[w] = i\n        true_logits, sampled_logits = self.forward(examples, labels)\n        loss = self.nce_loss(true_logits, sampled_logits)\n        try:\n            tf.summary.scalar(""NCE loss"", loss)\n        except AttributeError:\n            tf.scalar_summary(""NCE loss"", loss)\n        self._loss = loss\n        self.optimize(loss)\n\n        # Properly initialize all variables.\n        tf.global_variables_initializer().run()\n\n        self.saver = tf.train.Saver()\n\n    def save_vocab(self):\n        """"""Save the vocabulary to a file so the model can be reloaded.""""""\n        opts = self._options\n        with open(os.path.join(opts.save_path, ""vocab.txt""), ""w"") as f:\n            for i in xrange(opts.vocab_size):\n                vocab_word = tf.compat.as_text(opts.vocab_words[i]).encode(\n                    ""utf-8"")\n                f.write(""%s %d\\n"" % (vocab_word, opts.vocab_counts[i]))\n\n    def _train_thread_body(self):\n        initial_epoch, = self._session.run([self._epoch])\n        while True:\n            _, epoch = self._session.run([self._train, self._epoch])\n            if epoch != initial_epoch:\n                break\n\n    def train(self):\n        """"""Train the model.""""""\n        opts = self._options\n\n        initial_epoch, initial_words = self._session.run(\n            [self._epoch, self._words])\n        try:\n            summary_op = tf.summary.merge_all()\n        except AttributeError:\n            summary_op = tf.merge_all_summary()\n\n        summary_writer = tf.summary.FileWriter(opts.save_path,\n                                               self._session.graph)\n        workers = []\n        for _ in xrange(opts.concurrent_steps):\n            t = threading.Thread(target=self._train_thread_body)\n            t.start()\n            workers.append(t)\n\n        last_words, last_time, last_summary_time = initial_words, time.time(\n        ), 0\n        last_checkpoint_time = 0\n        while True:\n            time.sleep(\n                opts.statistics_interval)  # Reports our progress once a while.\n            (epoch, step, loss, words, lr) = self._session.run([\n                self._epoch, self.global_step, self._loss, self._words,\n                self._lr\n            ])\n            now = time.time()\n            last_words, last_time, rate = words, now, (words - last_words) / (\n                now - last_time)\n            print(\n                ""Epoch %4d Step %8d: lr = %5.3f loss = %6.2f words/sec = %8.0f\\r""\n                % (epoch, step, lr, loss, rate),\n                end="""")\n            sys.stdout.flush()\n            if now - last_summary_time > opts.summary_interval:\n                summary_str = self._session.run(summary_op)\n                summary_writer.add_summary(summary_str, step)\n                last_summary_time = now\n            if now - last_checkpoint_time > opts.checkpoint_interval:\n                self.saver.save(\n                    self._session,\n                    os.path.join(opts.save_path, ""model.ckpt""),\n                    global_step=step.astype(int))\n                last_checkpoint_time = now\n            if epoch != initial_epoch:\n                break\n\n        for t in workers:\n            t.join()\n\n        return epoch\n\n    def _predict(self, analogy):\n        """"""Predict the top 4 answers for analogy questions.""""""\n        idx, = self._session.run([self._analogy_pred_idx], {\n            self._analogy_a: analogy[:, 0],\n            self._analogy_b: analogy[:, 1],\n            self._analogy_c: analogy[:, 2]\n        })\n        return idx\n\n    def eval(self):\n        """"""Evaluate analogy questions and reports accuracy.""""""\n\n        # How many questions we get right at precision@1.\n        correct = 0\n\n        try:\n            total = self._analogy_questions.shape[0]\n        except AttributeError as e:\n            raise AttributeError(""Need to read analogy questions."")\n\n        start = 0\n        while start < total:\n            limit = start + 2500\n            sub = self._analogy_questions[start:limit, :]\n            idx = self._predict(sub)\n            start = limit\n            for question in xrange(sub.shape[0]):\n                for j in xrange(4):\n                    if idx[question, j] == sub[question, 3]:\n                        # Bingo! We predicted correctly. E.g., [italy, rome, france, paris].\n                        correct += 1\n                        break\n                    elif idx[question, j] in sub[question, :3]:\n                        # We need to skip words already in the question.\n                        continue\n                    else:\n                        # The correct label is not the precision@1\n                        break\n        print()\n        print(""Eval %4d/%d accuracy = %4.1f%%"" % (correct, total,\n                                                  correct * 100.0 / total))\n\n    def analogy(self, w0, w1, w2):\n        """"""Predict word w3 as in w0:w1 vs w2:w3.""""""\n        wid = np.array([[self._word2id.get(w, 0) for w in [w0, w1, w2]]])\n        idx = self._predict(wid)\n        for c in [self._id2word[i] for i in idx[0, :]]:\n            if c not in [w0, w1, w2]:\n                print(c)\n                break\n        print(""unknown"")\n\n    def nearby(self, words, num=20):\n        """"""Prints out nearby words given a list of words.""""""\n        ids = np.array([self._word2id.get(x, 0) for x in words])\n        vals, idx = self._session.run([self._nearby_val, self._nearby_idx],\n                                      {self._nearby_word: ids})\n        for i in xrange(len(words)):\n            print(""\\n%s\\n====================================="" % (words[i]))\n            for (neighbor, distance) in zip(idx[i, :num], vals[i, :num]):\n                print(""%-20s %6.4f"" % (self._id2word[neighbor], distance))\n\n\ndef _start_shell(local_ns=None):\n    # An interactive shell is useful for debugging/development.\n    import IPython\n    user_ns = {}\n    if local_ns:\n        user_ns.update(local_ns)\n    user_ns.update(globals())\n    IPython.start_ipython(argv=[], user_ns=user_ns)\n\n\ndef main(_):\n    """"""Train a word2vec model.""""""\n    if not FLAGS.train_data or not FLAGS.eval_data or not FLAGS.save_path:\n        print(""--train_data --eval_data and --save_path must be specified."")\n        sys.exit(1)\n    opts = Options()\n    with tf.Graph().as_default(), tf.Session() as session:\n        with tf.device(""/cpu:0""):\n            model = Word2Vec(opts, session)\n            # model.read_analogies()  # Read analogy questions\n        for _ in xrange(opts.epochs_to_train):\n            model.train()  # Process one epoch\n            # model.eval()  # Eval analogies.\n        # Perform a final save.\n        model.saver.save(\n            session,\n            os.path.join(opts.save_path, ""model.ckpt""),\n            global_step=model.global_step)\n        if FLAGS.interactive:\n            # E.g.,\n            # [0]: model.analogy(b\'france\', b\'paris\', b\'russia\')\n            # [1]: model.nearby([b\'proton\', b\'elephant\', b\'maxwell\'])\n            _start_shell(locals())\n\n\nif __name__ == ""__main__"":\n    tf.app.run()\n'"
nlp/Tag2Vec/scripts/word2vec_basic.py,22,"b'from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport collections\nimport math\nimport os\nimport random\nimport zipfile\n\nimport numpy as np\nfrom six.moves import urllib\nfrom six.moves import xrange  # pylint: disable=redefined-builtin\nimport tensorflow as tf\n\n# Step 1: Download the data.\nurl = \'http://mattmahoney.net/dc/\'\n\n\ndef maybe_download(filename, expected_bytes):\n    """"""Download a file if not present, and make sure it\'s the right size.""""""\n    if not os.path.exists(filename):\n        filename, _ = urllib.request.urlretrieve(url + filename, filename)\n    statinfo = os.stat(filename)\n    if statinfo.st_size == expected_bytes:\n        print(\'Found and verified\', filename)\n    else:\n        print(statinfo.st_size)\n        raise Exception(\'Failed to verify \' + filename +\n                        \'. Can you get to it with a browser?\')\n    return filename\n\n\n# filename = maybe_download(\'text8.zip\', 31344016)\nfilename = ""../data/text8.zip""\n\n\n# Read the data into a list of strings.\ndef read_data(filename):\n    """"""Extract the first file enclosed in a zip file as a list of words.""""""\n    with zipfile.ZipFile(filename) as f:\n        data = tf.compat.as_str(f.read(f.namelist()[0])).split()\n    return data\n\n\nvocabulary = read_data(filename)\nprint(\'Data size\', len(vocabulary))\n\n# Step 2: Build the dictionary and replace rare words with UNK token.\nvocabulary_size = 50000\n\n\ndef build_dataset(words, n_words):\n    """"""Process raw inputs into a dataset.""""""\n    count = [[\'UNK\', -1]]\n    count.extend(collections.Counter(words).most_common(n_words - 1))\n    dictionary = dict()\n    for word, _ in count:\n        dictionary[word] = len(dictionary)\n    data = list()\n    unk_count = 0\n    for word in words:\n        if word in dictionary:\n            index = dictionary[word]\n        else:\n            index = 0  # dictionary[\'UNK\']\n            unk_count += 1\n        data.append(index)\n    count[0][1] = unk_count\n    reversed_dictionary = dict(zip(dictionary.values(), dictionary.keys()))\n    return data, count, dictionary, reversed_dictionary\n\n\ndata, count, dictionary, reverse_dictionary = build_dataset(vocabulary,\n                                                            vocabulary_size)\ndel vocabulary  # Hint to reduce memory.\nprint(\'Most common words (+UNK)\', count[:5])\nprint(\'Sample data\', data[:10], [reverse_dictionary[i] for i in data[:10]])\n\ndata_index = 0\n\n\n# Step 3: Function to generate a training batch for the skip-gram model.\ndef generate_batch(batch_size, num_skips, skip_window):\n    global data_index\n    assert batch_size % num_skips == 0\n    assert num_skips <= 2 * skip_window\n    batch = np.ndarray(shape=(batch_size), dtype=np.int32)\n    labels = np.ndarray(shape=(batch_size, 1), dtype=np.int32)\n    span = 2 * skip_window + 1  # [ skip_window target skip_window ]\n    buffer = collections.deque(maxlen=span)\n    for _ in range(span):\n        buffer.append(data[data_index])\n        data_index = (data_index + 1) % len(data)\n    for i in range(batch_size // num_skips):\n        target = skip_window  # target label at the center of the buffer\n        targets_to_avoid = [skip_window]\n        for j in range(num_skips):\n            while target in targets_to_avoid:\n                target = random.randint(0, span - 1)\n            targets_to_avoid.append(target)\n            batch[i * num_skips + j] = buffer[skip_window]\n            labels[i * num_skips + j, 0] = buffer[target]\n        buffer.append(data[data_index])\n        data_index = (data_index + 1) % len(data)\n    # Backtrack a little bit to avoid skipping words in the end of a batch\n    data_index = (data_index + len(data) - span) % len(data)\n    return batch, labels\n\n\nbatch, labels = generate_batch(batch_size=8, num_skips=2, skip_window=1)\nfor i in range(8):\n    print(batch[i], reverse_dictionary[batch[i]], \'->\', labels[i, 0],\n          reverse_dictionary[labels[i, 0]])\n\n# Step 4: Build and train a skip-gram model.\n\nbatch_size = 128\nembedding_size = 128  # Dimension of the embedding vector.\nskip_window = 1  # How many words to consider left and right.\nnum_skips = 2  # How many times to reuse an input to generate a label.\n\n# We pick a random validation set to sample nearest neighbors. Here we limit the\n# validation samples to the words that have a low numeric ID, which by\n# construction are also the most frequent.\nvalid_size = 16  # Random set of words to evaluate similarity on.\nvalid_window = 100  # Only pick dev samples in the head of the distribution.\nvalid_examples = np.random.choice(valid_window, valid_size, replace=False)\nnum_sampled = 64  # Number of negative examples to sample.\n\ngraph = tf.Graph()\n\nwith graph.as_default():\n\n    # Input data.\n    train_inputs = tf.placeholder(tf.int32, shape=[batch_size])\n    train_labels = tf.placeholder(tf.int32, shape=[batch_size, 1])\n    valid_dataset = tf.constant(valid_examples, dtype=tf.int32)\n\n    # Ops and variables pinned to the CPU because of missing GPU implementation\n    with tf.device(\'/cpu:0\'):\n        # Look up embeddings for inputs.\n        embeddings = tf.Variable(\n            tf.random_uniform([vocabulary_size, embedding_size], -1.0, 1.0))\n        embed = tf.nn.embedding_lookup(embeddings, train_inputs)\n\n        # Construct the variables for the NCE loss\n        nce_weights = tf.Variable(\n            tf.truncated_normal(\n                [vocabulary_size, embedding_size],\n                stddev=1.0 / math.sqrt(embedding_size)))\n        nce_biases = tf.Variable(tf.zeros([vocabulary_size]))\n\n    # Compute the average NCE loss for the batch.\n    # tf.nce_loss automatically draws a new sample of the negative labels each\n    # time we evaluate the loss.\n    loss = tf.reduce_mean(\n        tf.nn.nce_loss(\n            weights=nce_weights,\n            biases=nce_biases,\n            labels=train_labels,\n            inputs=embed,\n            num_sampled=num_sampled,\n            num_classes=vocabulary_size))\n\n    # Construct the SGD optimizer using a learning rate of 1.0.\n    optimizer = tf.train.GradientDescentOptimizer(1.0).minimize(loss)\n\n    # Compute the cosine similarity between minibatch examples and all embeddings.\n    norm = tf.sqrt(tf.reduce_sum(tf.square(embeddings), 1, keep_dims=True))\n    normalized_embeddings = embeddings / norm\n    valid_embeddings = tf.nn.embedding_lookup(normalized_embeddings,\n                                              valid_dataset)\n    similarity = tf.matmul(\n        valid_embeddings, normalized_embeddings, transpose_b=True)\n\n    # Add variable initializer.\n    try:\n        init = tf.global_variables_initializer()\n    except AttributeError:\n        init = tf.initialize_all_variables()\n\n# Step 5: Begin training.\nnum_steps = 100001\n\nwith tf.Session(graph=graph) as session:\n    # We must initialize all variables before we use them.\n    init.run()\n    print(\'Initialized\')\n\n    average_loss = 0\n    for step in xrange(num_steps):\n        batch_inputs, batch_labels = generate_batch(batch_size, num_skips,\n                                                    skip_window)\n        feed_dict = {train_inputs: batch_inputs, train_labels: batch_labels}\n\n        # We perform one update step by evaluating the optimizer op (including it\n        # in the list of returned values for session.run()\n        _, loss_val = session.run([optimizer, loss], feed_dict=feed_dict)\n        average_loss += loss_val\n\n        if step % 2000 == 0:\n            if step > 0:\n                average_loss /= 2000\n            # The average loss is an estimate of the loss over the last 2000 batches.\n            print(\'Average loss at step \', step, \': \', average_loss)\n            average_loss = 0\n\n        # Note that this is expensive (~20% slowdown if computed every 500 steps)\n        if step % 10000 == 0:\n            sim = similarity.eval()\n            for i in xrange(valid_size):\n                valid_word = reverse_dictionary[valid_examples[i]]\n                top_k = 8  # number of nearest neighbors\n                similarity = (-sim[i, :]).argsort()[1:top_k + 1]\n                log_str = \'Nearest to %s:\' % valid_word\n                for k in xrange(top_k):\n                    close_word = reverse_dictionary[nearest[k]]\n                    log_str = \'%s %s,\' % (log_str, close_word)\n                print(log_str)\n    final_embeddings = normalized_embeddings.eval()\n\n# Step 6: Visualize the embeddings.\n\n\ndef plot_with_labels(low_dim_embs, labels, filename=\'tsne.png\'):\n    assert low_dim_embs.shape[0] >= len(labels), \'More labels than embeddings\'\n    plt.figure(figsize=(18, 18))  # in inches\n    for i, label in enumerate(labels):\n        x, y = low_dim_embs[i, :]\n        plt.scatter(x, y)\n        plt.annotate(\n            label,\n            xy=(x, y),\n            xytext=(5, 2),\n            textcoords=\'offset points\',\n            ha=\'right\',\n            va=\'bottom\')\n\n    plt.savefig(filename)\n\n\ntry:\n    # pylint: disable=g-import-not-at-top\n    from sklearn.manifold import TSNE\n    import matplotlib.pyplot as plt\n    np.random.seed(1)\n\n    tsne = TSNE(\n        perplexity=30,\n        n_components=2,\n        init=\'pca\',\n        random_state=1,\n        n_iter=5000,\n        method=\'exact\')\n    plot_only = 500\n    print(""final_embeddings size"" + str(len(final_embeddings)))\n    low_dim_embs = tsne.fit_transform(final_embeddings[:plot_only, :])\n    labels = [reverse_dictionary[i] for i in xrange(plot_only)]\n    plot_with_labels(low_dim_embs, labels)\n\nexcept ImportError:\n    print(\'Please install sklearn, matplotlib, and scipy to show embeddings.\')\n'"
nlp/text_classifier/scripts/__init__.py,0,b''
nlp/text_classifier/scripts/bow_text_classifier.py,0,"b'from dataset_helpers.cut_doc import cutDoc\nimport numpy as np\nfrom gensim import corpora,models\nfrom pprint import pprint\nimport traceback\nimport sys\nimport cPickle as pickle\nfrom scipy.sparse import *\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.externals import joblib\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import confusion_matrix\n\nreload(sys)\nsys.setdefaultencoding(\'utf-8\')\n\nclass bow_text_classifier:\n    """""" bow_text_classifier: a text classifier of bag of word\n    """"""\n    def __init__(self, data_path):\n        self.data_path = data_path\n        self.dictionary = corpora.Dictionary()\n        self.corpus = []\n        self.labels = []\n        self.cut_doc_obj = cutDoc()\n\n    def __get_all_tokens(self):\n        """""" get all tokens of the corpus\n        """"""\n        fwrite = open(self.data_path.replace(""all_title.csv"",""all_token.csv""), \'w\')\n        with open(self.data_path, ""r"") as fread:\n            i = 0\n            # while True:\n            for line in fread.readlines():\n                try:\n                    line_list = line.strip().split(""\\t"")\n                    label = line_list[0]\n                    self.labels.append(label)\n                    text = line_list[1]\n                    text_tokens = self.cut_doc_obj.run(text)\n                    self.corpus.append(text_tokens)\n                    self.dictionary.add_documents([text_tokens])\n                    fwrite.write(label+""\\t""+""\\\\"".join(text_tokens)+""\\n"")\n                    i+=1\n                except BaseException as e:\n                    msg = traceback.format_exc()\n                    print msg\n                    print ""=====>Read Done<======""\n                    break\n        self.token_len = self.dictionary.__len__()\n        print ""all token len ""+ str(self.token_len)\n        self.num_data = i\n        fwrite.close()\n\n    def __filter_tokens(self, threshold_num=10):\n        small_freq_ids = [tokenid for tokenid, docfreq in self.dictionary.dfs.items() if docfreq < threshold_num ]\n        self.dictionary.filter_tokens(small_freq_ids)\n        self.dictionary.compactify()\n\n    def vec(self):\n        """""" vec: get a vec representation of bow\n        """"""\n        self.__get_all_tokens()\n        print ""before filter, the tokens len: {0}"".format(self.dictionary.__len__())\n        self.__filter_tokens()\n        print ""After filter, the tokens len: {0}"".format(self.dictionary.__len__())\n        self.bow = []\n        for file_token in self.corpus:\n            file_bow = self.dictionary.doc2bow(file_token)\n            self.bow.append(file_bow)\n        # write the bow vec into a file\n        bow_vec_file = open(self.data_path.replace(""all_title.csv"",""bow_vec.pl""), \'wb\')\n        pickle.dump(self.bow,bow_vec_file)\n        bow_vec_file.close()\n        bow_label_file = open(self.data_path.replace(""all_title.csv"",""bow_label.pl""), \'wb\')\n        pickle.dump(self.labels,bow_label_file)\n        bow_label_file.close()\n\n    def to_csr(self):\n        self.bow = pickle.load(open(self.data_path.replace(""all_title.csv"",""bow_vec.pl""), \'rb\'))\n        self.labels = pickle.load(open(self.data_path.replace(""all_title.csv"",""bow_label.pl""), \'rb\'))\n        data = []\n        rows = []\n        cols = []\n        line_count = 0\n        for line in self.bow:\n            for elem in line:\n                rows.append(line_count)\n                cols.append(elem[0])\n                data.append(elem[1])\n            line_count += 1\n        print ""dictionary shape ({0},{1})"".format(line_count, self.dictionary.__len__())\n        bow_sparse_matrix = csr_matrix((data,(rows,cols)), shape=[line_count, self.dictionary.__len__()])\n        print ""bow_sparse matrix shape: ""\n        print bow_sparse_matrix.shape\n        # rarray=np.random.random(size=line_count)\n        self.train_set, self.test_set, self.train_tag, self.test_tag = train_test_split(bow_sparse_matrix, self.labels, test_size=0.2)\n        print ""train set shape: ""\n        print self.train_set.shape\n        train_set_file = open(self.data_path.replace(""all_title.csv"",""bow_train_set.pl""), \'wb\')\n        pickle.dump(self.train_set,train_set_file)\n        train_tag_file = open(self.data_path.replace(""all_title.csv"",""bow_train_tag.pl""), \'wb\')\n        pickle.dump(self.train_tag,train_tag_file)\n        test_set_file = open(self.data_path.replace(""all_title.csv"",""bow_test_set.pl""), \'wb\')\n        pickle.dump(self.test_set,test_set_file)\n        test_tag_file = open(self.data_path.replace(""all_title.csv"",""bow_test_tag.pl""), \'wb\')\n        pickle.dump(self.test_tag,test_tag_file)\n    \n    def train(self):\n        print ""Beigin to Train the model""\n        lr_model = LogisticRegression()\n        lr_model.fit(self.train_set, self.train_tag)\n        print ""End Now, and evalution the model with test dataset""\n        # print ""mean accuracy: {0}"".format(lr_model.score(self.test_set, self.test_tag))\n        y_pred = lr_model.predict(self.test_set)\n        print classification_report(self.test_tag, y_pred)\n        print confusion_matrix(self.test_tag, y_pred)\n        print lr_model.score(self.test_set, self.test_tag)\n        print ""save the trained model to lr_model.pl""\n        joblib.dump(lr_model, self.data_path.replace(""all_title.csv"",""bow_lr_model.pl"")) \n\n\nif __name__ == ""__main__"":\n    bow_text_classifier_obj = bow_text_classifier(""../data/origin_data/all_title.csv"")\n    bow_text_classifier_obj.vec()\n    bow_text_classifier_obj.to_csr()\n    # print len(bow_text_classifier_obj.train_set)\n    # print len(bow_text_classifier_obj.test_set)\n    bow_text_classifier_obj.train()\n    # print bow_text_classifier_obj.all_words_len\n\n\n\n\n\n'"
nlp/text_classifier/scripts/cnn_lstm_text_classifier.py,0,"b'#-*-coding:utf-8-*-\nfrom dataset_helpers.cut_doc import cutDoc\nimport numpy as np\nfrom gensim import corpora,models\nfrom pprint import pprint\nimport traceback\nimport sys\nimport cPickle as pickle\nfrom config import *\nfrom scipy.sparse import *\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.externals import joblib\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import TfidfTransformer\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import confusion_matrix\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.utils.np_utils import to_categorical\nfrom keras.layers import Dense, Input, Flatten, Dropout\nfrom keras.layers import LSTM, MaxPooling1D, Embedding, Convolution1D\nfrom keras.models import Model\nfrom keras.callbacks import TensorBoard, ModelCheckpoint, LearningRateScheduler\n# from keras.layers.merge import Concatenate\n# from keras.engine.topology import Merge\nfrom keras.optimizers import RMSprop,SGD\n\nreload(sys)\nsys.setdefaultencoding(\'utf-8\')\n\nclass cnn_text_classifier:\n    """""" tf_idf_text_classifier: a text classifier of tfidf\n    """"""\n    def __init__(self, data_path):\n        self.data_path = data_path\n        self.dictionary = corpora.Dictionary()\n        self.corpus = []\n        self.labels = []\n        self.cut_doc_obj = cutDoc()\n        self.w2v_file = W2V_FILE\n        self.class_num = CLASS_NUM\n        self.filter_sizes = (3, 8)\n        self.num_filters = 10\n        self.hidden_dims = 64\n    \n    def load_word2vec(self):\n        """""" load_word2vec: load the w2v model\n        """"""\n        print ""Start load word2vec model""\n        self.w2vec = {}\n        with open(self.w2v_file, ""r"") as fread:\n            for line in fread.readlines():\n                # print line\n                line_list = line.strip().split("" "")\n                word = line_list[0]\n                word_vec = np.fromstring(\' \'.join(line_list[1:]), dtype=float, sep=\' \')\n                # print len(word_vec)\n                self.w2vec[word] = word_vec\n        print ""Done load word2vec model""\n\n    def __get_all_tokens_v2(self):\n        """""" get all tokens from file\n        """"""\n        print ""load the tokens from file ""\n        with open(self.data_path.replace(""all.csv"",""all_token.csv""), \'r\') as fread:\n            for line in fread.readlines():\n                # print line\n                try:\n                    line_list = line.strip().split(""\\t"")\n                    label = line_list[0]\n                    # print label\n                    text_token = line_list[1].split(""\\\\"")\n                    self.dictionary.add_documents([text_token])\n                    self.labels.append(label)\n                    self.corpus.append(text_token)\n                except BaseException as e:\n                    print e\n                    continue\n                \n        # print ""load dictionary fron file""\n        # self.dictionary.load(self.data_path.replace(""all.csv"",""cnn.dict""))\n\n    def __get_all_tokens(self):\n        """""" get all tokens of the corpus\n        """"""\n        fwrite = open(self.data_path.replace(""all.csv"",""all_token.csv""), \'w\')\n        with open(self.data_path, ""r"") as fread:\n            i = 0\n            for line in fread.readlines():\n                try:\n                    line_list = line.strip().split(""\\t"")\n                    label = line_list[0]\n                    self.labels.append(label)\n                    text = line_list[1]\n                    text_tokens = self.cut_doc_obj.run(text)\n                    self.corpus.append(text_tokens)\n                    self.dictionary.add_documents([text_tokens])\n                    fwrite.write(label+""\\t""+""\\\\"".join(text_tokens)+""\\n"")\n                    i+=1\n                except BaseException as e:\n                    msg = traceback.format_exc()\n                    print msg\n                    print ""=====>Read Done<======""\n                    break\n        self.token_len = self.dictionary.__len__()\n        print ""all token len ""+ str(self.token_len)\n        print ""save the dictionary""\n        self.dictionary.save(self.data_path.replace(""all.csv"",""cnn.dict""))\n        self.num_data = i\n        fwrite.close()\n\n    def __filter_tokens(self, threshold_num=10):\n        small_freq_ids = [tokenid for tokenid, docfreq in self.dictionary.dfs.items() if docfreq < threshold_num ]\n        self.dictionary.filter_tokens(small_freq_ids)\n        self.dictionary.compactify()\n\n    def gen_embedding_matrix(self, load4file=True):\n        """""" gen_embedding_matrix: generate the embedding matrix\n        """"""\n        if load4file:\n            self.__get_all_tokens_v2()\n        else:\n            self.__get_all_tokens()\n        print ""before filter, the tokens len: {0}"".format(self.dictionary.__len__())\n        self.__filter_tokens()\n        print ""after filter, the tokens len: {0}"".format(self.dictionary.__len__())\n        self.sequence = []\n        for file_token in self.corpus:\n            temp_sequence = [x for x, y in self.dictionary.doc2bow(file_token)]\n            self.sequence.append(temp_sequence)\n\n        # bow_vec_file = open(self.data_path.replace(""all.csv"",""bow_vec.pl""), \'wb\')\n        # print ""after filter, the tokens len: {0}"".format(self.dictionary.__len__())\n        self.corpus_size = len(self.dictionary.token2id)\n        self.embedding_matrix = np.zeros((self.corpus_size, EMBEDDING_DIM)) \n        print ""corpus size: {0}"".format(len(self.dictionary.token2id))\n        for key, v in self.dictionary.token2id.items():\n            key_vec = self.w2vec.get(key)\n            if key_vec is not None:\n                self.embedding_matrix[v] = key_vec\n            else:\n                self.embedding_matrix[v] = np.random.rand(EMBEDDING_DIM) - 0.5\n    def step_decay(self, epoch):\n        drop_every = 5\n        decay_rate = (0.001*np.power(0.5, np.floor((1+drop_every)/drop_every))).astype(\'float32\')\n        return decay_rate\n\n    def __build_network(self):\n        embedding_layer = Embedding(self.corpus_size,\n                            EMBEDDING_DIM,\n                            weights=[self.embedding_matrix],\n                            input_length=MAX_SEQUENCE_LENGTH,\n                            trainable=False)\n        # train a 1D convnet with global maxpooling\n        sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype=\'int32\')\n        embedded_sequences = embedding_layer(sequence_input)\n        x = Convolution1D(self.num_filters, 5, activation=""relu"")(embedded_sequences)\n        x = MaxPooling1D(5)(x)\n        x = Convolution1D(self.num_filters, 5, activation=""relu"")(x)\n        x = MaxPooling1D(5)(x)\n        x = LSTM(64, dropout_W=0.2, dropout_U=0.2)(x)\n        preds = Dense(self.class_num, activation=\'softmax\')(x)\n        print preds.get_shape()\n        rmsprop = RMSprop(lr=0.01)\n        self.model = Model(sequence_input, preds)\n        self.model.compile(loss=\'categorical_crossentropy\', optimizer=rmsprop, metrics=[\'acc\'])\n\n    def train(self):\n        self.__split_train_test()\n        self.__build_network()\n        tensorboard = TensorBoard(log_dir=""./logs/cnn_lstm/"")\n        ckpt_file = ""weights.{epoch:02d}-{val_loss:.2f}.hdf5""\n        model_checkpoint = ModelCheckpoint(ckpt_file)\n        # print self.train_set[:128]\n        # print \'train labels: \',self.train_tag[:128]\n        lrate = LearningRateScheduler(self.step_decay)\n        self.model.fit(self.train_set, self.train_tag, validation_data=(self.test_set, self.test_tag),nb_epoch=100, batch_size=64, callbacks=[tensorboard, model_checkpoint,lrate])\n        self.model.save(self.data_path.replace(""all.csv"",""cnn.model""))\n\n    def __split_train_test(self):\n        # print len(self.sequence)\n        # print self.sequence[0]\n        self.data = pad_sequences(self.sequence, maxlen=MAX_SEQUENCE_LENGTH)\n        #indices = np.arange(self.data.shape[0])\n        #np.random.shuffle(indices)\n        #self.data = self.data[indices]\n        #self.labels = self.labels[indices]\n        # print ""__split_train_test {0}"".format(self.data.shape)\n        self.train_set, self.test_set, self.train_tag, self.test_tag = train_test_split(self.data, self.labels, test_size=0.2)\n        print ""train_tag {0}"", \' \'.join(self.train_tag)[0:1000]\n        self.train_tag = to_categorical(np.asarray(self.train_tag))\n        self.test_tag = to_categorical(np.asarray(self.test_tag))\n        # print np.asarray(self.train_tag).shape\n\n\n\n\nif __name__ == ""__main__"":\n    cnn_text_classifier_obj = cnn_text_classifier(""../data/origin_data/all.csv"")\n    cnn_text_classifier_obj.load_word2vec()\n    cnn_text_classifier_obj.gen_embedding_matrix(load4file=True)\n    cnn_text_classifier_obj.train()\n\n\n\n\n\n\n'"
nlp/text_classifier/scripts/cnn_text_classifier.py,0,"b'#-*-coding:utf-8-*-\nfrom dataset_helpers.cut_doc import cutDoc\nimport numpy as np\nfrom gensim import corpora,models\nfrom pprint import pprint\nimport traceback\nimport sys\nimport cPickle as pickle\nfrom config import *\nfrom scipy.sparse import *\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.externals import joblib\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import TfidfTransformer\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import confusion_matrix\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.utils.np_utils import to_categorical\nfrom keras.layers import Dense, Input, Flatten, Dropout\nfrom keras.layers import Convolution1D, MaxPooling1D, Embedding\nfrom keras.models import Model\nfrom keras.callbacks import TensorBoard, ModelCheckpoint, LearningRateScheduler\nfrom keras.layers.normalization import BatchNormalization\n# from keras.layers.merge import Concatenate\n# from keras.engine.topology import Merge\nfrom keras.optimizers import RMSprop\nfrom keras.layers import Activation\n\nreload(sys)\nsys.setdefaultencoding(\'utf-8\')\n\nclass cnn_text_classifier:\n    """""" tf_idf_text_classifier: a text classifier of tfidf\n    """"""\n    def __init__(self, data_path):\n        self.data_path = data_path\n        self.dictionary = corpora.Dictionary()\n        self.corpus = []\n        self.labels = []\n        self.cut_doc_obj = cutDoc()\n        self.w2v_file = W2V_FILE\n        self.class_num = CLASS_NUM\n        self.filter_sizes = (3, 8)\n        self.num_filters = 10\n        self.hidden_dims = 64\n    \n    def load_word2vec(self):\n        """""" load_word2vec: load the w2v model\n        """"""\n        print ""Start load word2vec model""\n        self.w2vec = {}\n        with open(self.w2v_file, ""r"") as fread:\n            for line in fread.readlines():\n                # print line\n                line_list = line.strip().split("" "")\n                word = line_list[0]\n                word_vec = np.fromstring(\' \'.join(line_list[1:]), dtype=float, sep=\' \')\n                # print len(word_vec)\n                self.w2vec[word] = word_vec\n        print ""Done load word2vec model""\n\n    def __get_all_tokens_v2(self):\n        """""" get all tokens from file\n        """"""\n        print ""load the tokens from file ""\n        with open(self.data_path.replace(""all.csv"",""all_token.csv""), \'r\') as fread:\n            for line in fread.readlines():\n                # print line\n                try:\n                    line_list = line.strip().split(""\\t"")\n                    label = line_list[0]\n                    # print label\n                    text_token = line_list[1].split(""\\\\"")\n                    self.dictionary.add_documents([text_token])\n                    self.labels.append(label)\n                    self.corpus.append(text_token)\n                except BaseException as e:\n                    print e\n                    continue\n                \n        # print ""load dictionary fron file""\n        # self.dictionary.load(self.data_path.replace(""all.csv"",""cnn.dict""))\n\n    def __get_all_tokens(self):\n        """""" get all tokens of the corpus\n        """"""\n        fwrite = open(self.data_path.replace(""all.csv"",""all_token.csv""), \'w\')\n        with open(self.data_path, ""r"") as fread:\n            i = 0\n            for line in fread.readlines():\n                try:\n                    line_list = line.strip().split(""\\t"")\n                    label = line_list[0]\n                    self.labels.append(label)\n                    text = line_list[1]\n                    text_tokens = self.cut_doc_obj.run(text)\n                    self.corpus.append(text_tokens)\n                    self.dictionary.add_documents([text_tokens])\n                    fwrite.write(label+""\\t""+""\\\\"".join(text_tokens)+""\\n"")\n                    i+=1\n                except BaseException as e:\n                    msg = traceback.format_exc()\n                    print msg\n                    print ""=====>Read Done<======""\n                    break\n        self.token_len = self.dictionary.__len__()\n        print ""all token len ""+ str(self.token_len)\n        print ""save the dictionary""\n        self.dictionary.save(self.data_path.replace(""all.csv"",""cnn.dict""))\n        self.num_data = i\n        fwrite.close()\n\n    def __filter_tokens(self, threshold_num=10):\n        small_freq_ids = [tokenid for tokenid, docfreq in self.dictionary.dfs.items() if docfreq < threshold_num ]\n        self.dictionary.filter_tokens(small_freq_ids)\n        self.dictionary.compactify()\n\n    def gen_embedding_matrix(self, load4file=True):\n        """""" gen_embedding_matrix: generate the embedding matrix\n        """"""\n        if load4file:\n            self.__get_all_tokens_v2()\n        else:\n            self.__get_all_tokens()\n        print ""before filter, the tokens len: {0}"".format(self.dictionary.__len__())\n        self.__filter_tokens()\n        print ""after filter, the tokens len: {0}"".format(self.dictionary.__len__())\n        self.sequence = []\n        for file_token in self.corpus:\n            temp_sequence = [x for x, y in self.dictionary.doc2bow(file_token)]\n            self.sequence.append(temp_sequence)\n\n        # bow_vec_file = open(self.data_path.replace(""all.csv"",""bow_vec.pl""), \'wb\')\n        # print ""after filter, the tokens len: {0}"".format(self.dictionary.__len__())\n        self.corpus_size = len(self.dictionary.token2id)\n        self.embedding_matrix = np.zeros((self.corpus_size, EMBEDDING_DIM)) \n        print ""corpus size: {0}"".format(len(self.dictionary.token2id))\n        for key, v in self.dictionary.token2id.items():\n            key_vec = self.w2vec.get(key)\n            if key_vec is not None:\n                self.embedding_matrix[v] = key_vec\n            else:\n                self.embedding_matrix[v] = np.random.rand(EMBEDDING_DIM) - 0.5\n        print ""enbedding_matrix len {0}"".format(len(self.embedding_matrix))\n        \n    def step_decay(self, epoch):\n        drop_every = 5\n        decay_rate = (0.001*np.power(0.5, np.floor((1+drop_every)/drop_every))).astype(\'float32\')\n        return decay_rate\n\n    def __build_network(self):\n        embedding_layer = Embedding(self.corpus_size,\n                            EMBEDDING_DIM,\n                            weights=[self.embedding_matrix],\n                            input_length=MAX_SEQUENCE_LENGTH,\n                            trainable=False)\n        # train a 1D convnet with global maxpooling\n        sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype=\'int32\')\n        embedded_sequences = embedding_layer(sequence_input)\n        # sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype=\'int32\')\n        # embedded_sequences = embedding_layer(sequence_input)\n        x = Convolution1D(128, 5)(embedded_sequences)\n        x = BatchNormalization()(x)\n        x = Activation(\'relu\')(x)\n        x = MaxPooling1D(5)(x)\n        x = Convolution1D(128, 5)(x)\n        x = BatchNormalization()(x)\n        x = Activation(\'relu\')(x)\n        x = MaxPooling1D(5)(x)\n        print ""before 256"", x.get_shape()\n        x = Convolution1D(128, 5)(x)\n        x = BatchNormalization()(x)\n        x = Activation(\'relu\')(x)\n        print ""before 35 "", x.get_shape()\n        x = MaxPooling1D(35)(x)\n        x = Flatten()(x)\n        # print x.shape()\n\n        x = Dense(128, activation=\'relu\')(x)\n        print x.get_shape()\n        x = Dropout(0.5)(x)\n        print x.get_shape()\n        preds = Dense(self.class_num, activation=\'softmax\')(x)\n        print preds.get_shape()\n        # conv_blocks = []\n        # for sz in self.filter_sizes:\n        #     conv = Convolution1D(filters=self.num_filters, kernel_size=sz, activation=""relu"", padding=\'valid\', strides=1)(embedded_sequences)\n        #     conv = MaxPooling1D(pool_size=2)(conv)\n        #     conv = Flatten()(conv)\n        #     conv_blocks.append(conv)\n        # z = Concatenate()(conv_blocks) if len(conv_blocks) > 1 else conv_blocks[0]\n        # z = Dropout(rate=0.5)(z)\n        # z = Dense(units=self.hidden_dims, activation=""relu"")(z)\n        # preds = Dense(self.class_num, activation=""softmax"")(z)\n        rmsprop = RMSprop(lr=0.001)\n        self.model = Model(sequence_input, preds)\n        self.model.compile(loss=\'categorical_crossentropy\', optimizer=rmsprop, metrics=[\'acc\'])\n\n    def train(self):\n        self.__split_train_test()\n        self.__build_network()\n        tensorboard = TensorBoard()\n        ckpt_file = ""weights.{epoch:02d}-{val_loss:.2f}.hdf5""\n        model_checkpoint = ModelCheckpoint(ckpt_file)\n        lrate = LearningRateScheduler(self.step_decay)\n        self.model.fit(self.train_set, self.train_tag, validation_data=(self.test_set, self.test_tag),nb_epoch=20, batch_size=128, callbacks=[tensorboard, model_checkpoint, lrate])\n        self.model.save(self.data_path.replace(""all.csv"",""cnn.model""))\n\n    def __split_train_test(self):\n        print len(self.sequence)\n        # print self.sequence[0]\n        self.data = pad_sequences(self.sequence, maxlen=MAX_SEQUENCE_LENGTH)\n        # print ""__split_train_test {0}"".format(self.data.shape)\n        self.train_set, self.test_set, self.train_tag, self.test_tag = train_test_split(self.data, self.labels, test_size=0.2)\n        print ""train_tag {0}"", \' \'.join(self.train_tag)[0:1000]\n        self.train_tag = to_categorical(np.asarray(self.train_tag))\n        self.test_tag = to_categorical(np.asarray(self.test_tag))\n        # print np.asarray(self.train_tag).shape\n\n\n\n\nif __name__ == ""__main__"":\n    cnn_text_classifier_obj = cnn_text_classifier(""../data/origin_data/all.csv"")\n    cnn_text_classifier_obj.load_word2vec()\n    cnn_text_classifier_obj.gen_embedding_matrix(load4file=True)\n    cnn_text_classifier_obj.train()\n\n\n\n\n\n\n'"
nlp/text_classifier/scripts/cnn_text_classifier_v2.py,0,"b'#-*-coding:utf-8-*-\nfrom dataset_helpers.cut_doc import cutDoc\nimport numpy as np\nfrom gensim import corpora, models\nfrom pprint import pprint\nimport traceback\nimport sys\nimport cPickle as pickle\nfrom config import *\nfrom scipy.sparse import *\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.externals import joblib\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import TfidfTransformer\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import confusion_matrix\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.utils.np_utils import to_categorical\nfrom keras.layers import Dense, Input, Flatten, Dropout\nfrom keras.layers import Convolution1D, MaxPooling1D, Embedding\nfrom keras.models import Model\nfrom keras.callbacks import TensorBoard, ModelCheckpoint, LearningRateScheduler\nfrom keras.layers.normalization import BatchNormalization\n# from keras.layers.merge import Concatenate\n# from keras.engine.topology import Merge\nfrom keras.optimizers import RMSprop\nfrom keras.layers import Activation\nfrom keras.engine.topology import Merge\n\nreload(sys)\nsys.setdefaultencoding(\'utf-8\')\n\n\nclass cnn_text_classifier:\n    """""" tf_idf_text_classifier: a text classifier of tfidf\n    """"""\n\n    def __init__(self, data_path):\n        self.data_path = data_path\n        self.dictionary = corpora.Dictionary()\n        self.corpus = []\n        self.labels = []\n        self.cut_doc_obj = cutDoc()\n        self.w2v_file = W2V_FILE\n        self.class_num = CLASS_NUM\n        self.filter_sizes = (3, 8)\n        self.num_filters = 10\n        self.hidden_dims = 64\n\n    def load_word2vec(self):\n        """""" load_word2vec: load the w2v model\n        """"""\n        print ""Start load word2vec model""\n        self.w2vec = {}\n        with open(self.w2v_file, ""r"") as fread:\n            for line in fread.readlines():\n                # print line\n                line_list = line.strip().split("" "")\n                word = line_list[0]\n                word_vec = np.fromstring(\n                    \' \'.join(line_list[1:]), dtype=float, sep=\' \')\n                # print len(word_vec)\n                self.w2vec[word] = word_vec\n        print ""Done load word2vec model""\n\n    def __get_all_tokens_v2(self):\n        """""" get all tokens from file\n        """"""\n        print ""load the tokens from file ""\n        with open(self.data_path.replace(""all.csv"", ""all_token.csv""),\n                  \'r\') as fread:\n            for line in fread.readlines():\n                # print line\n                try:\n                    line_list = line.strip().split(""\\t"")\n                    label = line_list[0]\n                    # print label\n                    text_token = line_list[1].split(""\\\\"")\n                    self.dictionary.add_documents([text_token])\n                    self.labels.append(label)\n                    self.corpus.append(text_token)\n                except BaseException as e:\n                    print e\n                    continue\n\n        # print ""load dictionary fron file""\n        # self.dictionary.load(self.data_path.replace(""all.csv"",""cnn.dict""))\n\n    def __get_all_tokens(self):\n        """""" get all tokens of the corpus\n        """"""\n        fwrite = open(self.data_path.replace(""all.csv"", ""all_token.csv""), \'w\')\n        with open(self.data_path, ""r"") as fread:\n            i = 0\n            for line in fread.readlines():\n                try:\n                    line_list = line.strip().split(""\\t"")\n                    label = line_list[0]\n                    self.labels.append(label)\n                    text = line_list[1]\n                    text_tokens = self.cut_doc_obj.run(text)\n                    self.corpus.append(text_tokens)\n                    self.dictionary.add_documents([text_tokens])\n                    fwrite.write(label + ""\\t"" + ""\\\\"".join(text_tokens) + ""\\n"")\n                    i += 1\n                except BaseException as e:\n                    msg = traceback.format_exc()\n                    print msg\n                    print ""=====>Read Done<======""\n                    break\n        self.token_len = self.dictionary.__len__()\n        print ""all token len "" + str(self.token_len)\n        print ""save the dictionary""\n        self.dictionary.save(self.data_path.replace(""all.csv"", ""cnn.dict""))\n        self.num_data = i\n        fwrite.close()\n\n    def __filter_tokens(self, threshold_num=10):\n        small_freq_ids = [\n            tokenid for tokenid, docfreq in self.dictionary.dfs.items()\n            if docfreq < threshold_num\n        ]\n        self.dictionary.filter_tokens(small_freq_ids)\n        self.dictionary.compactify()\n\n    def gen_embedding_matrix(self, load4file=True):\n        """""" gen_embedding_matrix: generate the embedding matrix\n        """"""\n        if load4file:\n            self.__get_all_tokens_v2()\n        else:\n            self.__get_all_tokens()\n        print ""before filter, the tokens len: {0}"".format(\n            self.dictionary.__len__())\n        self.__filter_tokens()\n        print ""after filter, the tokens len: {0}"".format(\n            self.dictionary.__len__())\n        self.sequence = []\n        for file_token in self.corpus:\n            temp_sequence = [x for x, y in self.dictionary.doc2bow(file_token)]\n            self.sequence.append(temp_sequence)\n\n        self.corpus_size = len(self.dictionary.token2id)\n        self.embedding_matrix = np.zeros((self.corpus_size, EMBEDDING_DIM))\n        print ""corpus size: {0}"".format(len(self.dictionary.token2id))\n        for key, v in self.dictionary.token2id.items():\n            key_vec = self.w2vec.get(key)\n            if key_vec is not None:\n                self.embedding_matrix[v] = key_vec\n            else:\n                self.embedding_matrix[v] = np.random.rand(EMBEDDING_DIM) - 0.5\n        print ""enbedding_matrix len {0}"".format(len(self.embedding_matrix))\n\n    def step_decay(self, epoch):\n        drop_every = 40\n        decay_rate = (0.001 * np.power(\n            0.5, np.floor((1 + drop_every) / drop_every))).astype(\'float32\')\n        return decay_rate\n\n    def __build_network(self):\n        embedding_layer = Embedding(\n            self.corpus_size,\n            EMBEDDING_DIM,\n            weights=[self.embedding_matrix],\n            input_length=MAX_SEQUENCE_LENGTH,\n            trainable=False)\n        # train a 1D convnet with global maxpooling\n        sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH, ), dtype=\'int32\')\n        embedded_sequences = embedding_layer(sequence_input)\n        conv_blocks = []\n        for sz in self.filter_sizes:\n            conv = Convolution1D(\n                self.num_filters, sz, activation=""relu"")(embedded_sequences)\n            conv = MaxPooling1D(2)(conv)\n            conv = Flatten()(conv)\n            conv_blocks.append(conv)\n        z = Merge(\n            mode=\'concat\', concat_axis=1)(conv_blocks) if len(\n                conv_blocks) > 1 else conv_blocks[0]\n        z = Dropout(0.5)(z)\n        z = Dense(self.hidden_dims, activation=""relu"")(z)\n        preds = Dense(self.class_num, activation=""softmax"")(z)\n        rmsprop = RMSprop(lr=0.001)\n        self.model = Model(sequence_input, preds)\n        self.model.compile(\n            loss=\'categorical_crossentropy\',\n            optimizer=rmsprop,\n            metrics=[\'acc\'])\n\n    def train(self):\n        self.__split_train_test()\n        self.__build_network()\n        tensorboard = TensorBoard(log_dir=""./logs/cnn_v2/"")\n        ckpt_file = ""weights.{epoch:02d}-{val_loss:.2f}.hdf5""\n        model_checkpoint = ModelCheckpoint(ckpt_file, period=5)\n        lrate = LearningRateScheduler(self.step_decay)\n        self.model.fit(self.train_set,\n                       self.train_tag,\n                       validation_data=(self.test_set, self.test_tag),\n                       nb_epoch=100,\n                       batch_size=128,\n                       callbacks=[tensorboard, model_checkpoint, lrate])\n        self.model.save(self.data_path.replace(""all.csv"", ""cnn.model""))\n\n    def __split_train_test(self):\n        self.data = pad_sequences(self.sequence, maxlen=MAX_SEQUENCE_LENGTH)\n        self.train_set, self.test_set, self.train_tag, self.test_tag = train_test_split(\n            self.data, self.labels, test_size=0.2)\n        self.train_tag = to_categorical(np.asarray(self.train_tag))\n        self.test_tag = to_categorical(np.asarray(self.test_tag))\n\n\nif __name__ == ""__main__"":\n    cnn_text_classifier_obj = cnn_text_classifier(\n        ""../data/origin_data/all.csv"")\n    cnn_text_classifier_obj.load_word2vec()\n    cnn_text_classifier_obj.gen_embedding_matrix(load4file=True)\n    cnn_text_classifier_obj.train()\n'"
nlp/text_classifier/scripts/cnn_text_classifier_v3.py,0,"b'#-*-coding:utf-8-*-\nfrom dataset_helpers.cut_doc import cutDoc\nimport numpy as np\nfrom gensim import corpora,models\nfrom pprint import pprint\nimport traceback\nimport sys\nimport cPickle as pickle\nfrom config import *\nfrom scipy.sparse import *\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.externals import joblib\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import TfidfTransformer\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import confusion_matrix\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.utils.np_utils import to_categorical\nfrom keras.layers import Dense, Input, Flatten, Dropout\nfrom keras.layers import Convolution1D, MaxPooling1D, Embedding\nfrom keras.models import Model\nfrom keras.callbacks import TensorBoard, ModelCheckpoint, LearningRateScheduler\nfrom keras.layers.normalization import BatchNormalization\nfrom keras.optimizers import RMSprop\nfrom keras.layers import Activation\nfrom keras.engine.topology import Merge\n\nreload(sys)\nsys.setdefaultencoding(\'utf-8\')\n\nclass cnn_text_classifier:\n    """""" tf_idf_text_classifier: a text classifier of tfidf\n    """"""\n    def __init__(self, data_path):\n        self.data_path = data_path\n        self.dictionary = corpora.Dictionary()\n        self.corpus = []\n        self.labels = []\n        self.cut_doc_obj = cutDoc()\n        self.w2v_file = W2V_FILE\n        self.class_num = CLASS_NUM\n        self.filter_sizes = (3, 4,5)\n        self.num_filters = 10\n        self.hidden_dims = 256\n    \n    def load_word2vec(self):\n        """""" load_word2vec: load the w2v model\n        """"""\n        print ""Start load word2vec model""\n        self.w2vec = {}\n        with open(self.w2v_file, ""r"") as fread:\n            for line in fread.readlines():\n                # print line\n                line_list = line.strip().split("" "")\n                word = line_list[0]\n                word_vec = np.fromstring(\' \'.join(line_list[1:]), dtype=float, sep=\' \')\n                # print len(word_vec)\n                self.w2vec[word] = word_vec\n        print ""Done load word2vec model""\n    \n    def load_word2vec_gensim(self):\n        print ""Start load word2vec model""\n        model = gensim.models.Word2Vec.load(\'/Users/burness/git_repository/dl_opensource/tensorflow-101/nlp/text_classifier/data/origin_data/news_w2v.model\')\n        self.w2vec = dict(zip(model.index2word, model.syn0))\n        \n\n    def __get_all_tokens_v2(self):\n        """""" get all tokens from file\n        """"""\n        print ""load the tokens from file ""\n        with open(self.data_path.replace(""all.csv"",""all_token.csv""), \'r\') as fread:\n            for line in fread.readlines():\n                # print line\n                try:\n                    line_list = line.strip().split(""\\t"")\n                    label = line_list[0]\n                    # print label\n                    text_token = line_list[1].split(""\\\\"")\n                    self.dictionary.add_documents([text_token])\n                    self.labels.append(label)\n                    self.corpus.append(text_token)\n                except BaseException as e:\n                    print e\n                    continue\n                \n        # print ""load dictionary fron file""\n        # self.dictionary.load(self.data_path.replace(""all.csv"",""cnn.dict""))\n\n    def __get_all_tokens(self):\n        """""" get all tokens of the corpus\n        """"""\n        fwrite = open(self.data_path.replace(""all.csv"",""all_token.csv""), \'w\')\n        with open(self.data_path, ""r"") as fread:\n            i = 0\n            for line in fread.readlines():\n                try:\n                    line_list = line.strip().split(""\\t"")\n                    label = line_list[0]\n                    self.labels.append(label)\n                    text = line_list[1]\n                    text_tokens = self.cut_doc_obj.run(text)\n                    self.corpus.append(text_tokens)\n                    self.dictionary.add_documents([text_tokens])\n                    fwrite.write(label+""\\t""+""\\\\"".join(text_tokens)+""\\n"")\n                    i+=1\n                except BaseException as e:\n                    msg = traceback.format_exc()\n                    print msg\n                    print ""=====>Read Done<======""\n                    break\n        self.token_len = self.dictionary.__len__()\n        print ""all token len ""+ str(self.token_len)\n        print ""save the dictionary""\n        self.dictionary.save(self.data_path.replace(""all.csv"",""cnn.dict""))\n        self.num_data = i\n        fwrite.close()\n\n    def __filter_tokens(self, threshold_num=10):\n        small_freq_ids = [tokenid for tokenid, docfreq in self.dictionary.dfs.items() if docfreq < threshold_num ]\n        self.dictionary.filter_tokens(small_freq_ids)\n        self.dictionary.compactify()\n\n    def gen_embedding_matrix(self, load4file=True):\n        """""" gen_embedding_matrix: generate the embedding matrix\n        """"""\n        if load4file:\n            self.__get_all_tokens_v2()\n        else:\n            self.__get_all_tokens()\n        print ""before filter, the tokens len: {0}"".format(self.dictionary.__len__())\n        self.__filter_tokens(threshold_num=5)\n        print ""after filter, the tokens len: {0}"".format(self.dictionary.__len__())\n        self.sequence = []\n        for file_token in self.corpus:\n            temp_sequence = [x for x, y in self.dictionary.doc2bow(file_token)]\n            self.sequence.append(temp_sequence)\n\n        self.corpus_size = len(self.dictionary.token2id)\n        self.embedding_matrix = np.zeros((self.corpus_size, EMBEDDING_DIM)) \n        print ""corpus size: {0}"".format(len(self.dictionary.token2id))\n        for key, v in self.dictionary.token2id.items():\n            key_vec = self.w2vec.get(key)\n            if key_vec is not None:\n                self.embedding_matrix[v] = key_vec\n            else:\n                self.embedding_matrix[v] = np.random.rand(EMBEDDING_DIM) - 0.5\n        print ""enbedding_matrix len {0}"".format(len(self.embedding_matrix))\n        \n    def step_decay(self, epoch):\n        drop_every = 40\n        decay_rate = (0.001*np.power(0.5, np.floor((1+drop_every)/drop_every))).astype(\'float32\')\n        return decay_rate\n\n    def __build_network(self):\n        embedding_layer = Embedding(self.corpus_size,\n                            EMBEDDING_DIM,\n                            weights=[self.embedding_matrix],\n                            input_length=MAX_SEQUENCE_LENGTH,\n                            trainable=False)\n        # train a 1D convnet with global maxpooling\n        sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype=\'int32\')\n        embedded_sequences = embedding_layer(sequence_input)\n        conv_blocks = []\n        for sz in self.filter_sizes:\n            conv = Convolution1D(self.num_filters, sz, activation=""relu"")(embedded_sequences)\n            conv = MaxPooling1D(sz)(conv)\n            conv = Flatten()(conv)\n            conv_blocks.append(conv)\n        z = Merge(mode=\'concat\', concat_axis=1)(conv_blocks) if len(conv_blocks) > 1 else conv_blocks[0]\n        z = Dropout(0.25)(z)\n        z = Dense(self.hidden_dims, activation=""relu"")(z)\n        z = Dropout(0.5)(z)\n        preds = Dense(self.class_num, activation=""softmax"")(z)\n        rmsprop = RMSprop(lr=0.001)\n        self.model = Model(sequence_input, preds)\n        self.model.compile(loss=\'categorical_crossentropy\', optimizer=rmsprop, metrics=[\'acc\'])\n\n    def train(self):\n        self.__split_train_test()\n        self.__build_network()\n        tensorboard = TensorBoard(log_dir=""./logs/cnn_hidden=256_dp1=25pd2=50/"")\n        ckpt_file = ""weights.{epoch:02d}-{val_loss:.2f}.hdf5""\n        model_checkpoint = ModelCheckpoint(ckpt_file)\n        lrate = LearningRateScheduler(self.step_decay)\n        self.model.fit(self.train_set, self.train_tag, validation_data=(self.test_set, self.test_tag),nb_epoch=100, batch_size=128, callbacks=[tensorboard, model_checkpoint, lrate])\n        self.model.save(self.data_path.replace(""all.csv"",""cnn.model""))\n\n    def __split_train_test(self):\n        self.data = pad_sequences(self.sequence, maxlen=MAX_SEQUENCE_LENGTH)\n        self.train_set, self.test_set, self.train_tag, self.test_tag = train_test_split(self.data, self.labels, test_size=0.2)\n        self.train_tag = to_categorical(np.asarray(self.train_tag))\n        self.test_tag = to_categorical(np.asarray(self.test_tag))\n\n\n\n\nif __name__ == ""__main__"":\n    cnn_text_classifier_obj = cnn_text_classifier(""../data/origin_data/all.csv"")\n    cnn_text_classifier_obj.load_word2vec_gensim()\n    cnn_text_classifier_obj.gen_embedding_matrix(load4file=True)\n    cnn_text_classifier_obj.train()\n\n\n\n\n\n\n'"
nlp/text_classifier/scripts/config.py,0,"b'import os\nDATA_DIR = ""../../data/origin_data""\nDATA_DIR = os.path.abspath(DATA_DIR)\nprint DATA_DIR\nfilename = [\n    100,\n    103,\n    104,\n    105,\n    106,\n    107,\n    108,\n    109,\n    110,\n    111,\n    112,\n    115,\n    116,\n    118,\n    119,\n    121,\n    122,\n    123,\n    124,\n    148,\n]\nall_text_filename = os.path.join(DATA_DIR, \'all.csv\')\nfilename_label = dict(zip(filename, range(len(filename))))\n# W2V_FILE = ""../../data/pretrain_w2v/zh/test.tsv""\nW2V_FILE = ""/Users/burness/git_repository/dl_opensource/nlp/oxford-cs-deepnlp-2017/practical-2/data/pretrain_w2v/w2vgood_20170209.model""\n# W2V_FILE = os.path.abspath(W2V_FILE)\nEMBEDDING_DIM = 200\nMAX_SEQUENCE_LENGTH = 1500\nCLASS_NUM = 2\nWORD_DICT = ""/Users/burness/git_repository/dl_opensource/nlp/oxford-cs-deepnlp-2017/practical-2/data/origin_data/t_tag_infos.txt""\nall_title_filename = os.path.join(DATA_DIR, \'all_title.csv\')\nMAX_TITLE_LENGTH = 20\n'"
nlp/text_classifier/scripts/lstm_text_classifier.py,0,"b'#-*-coding:utf-8-*-\nfrom dataset_helpers.cut_doc import cutDoc\nimport numpy as np\nfrom gensim import corpora, models\nfrom pprint import pprint\nimport traceback\nimport sys\nimport cPickle as pickle\nfrom config import *\nfrom scipy.sparse import *\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.externals import joblib\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import TfidfTransformer\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import confusion_matrix\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.utils.np_utils import to_categorical\nfrom keras.layers import Dense, Input, Flatten, Dropout\nfrom keras.layers import LSTM, MaxPooling1D, Embedding, Convolution1D\nfrom keras.models import Model\nfrom keras.callbacks import TensorBoard, ModelCheckpoint, LearningRateScheduler\n# from keras.layers.merge import Concatenate\n# from keras.engine.topology import Merge\nfrom keras.optimizers import RMSprop, SGD, Adam\n\nfrom keras import regularizers\n\nreload(sys)\nsys.setdefaultencoding(\'utf-8\')\n\n\nclass lstm_text_classifier:\n    """""" tf_idf_text_classifier: a text classifier of tfidf\n    """"""\n\n    def __init__(self, data_path, lr=0.001, drop_every=20, optimizer=""adam""):\n        self.data_path = data_path\n        self.dictionary = corpora.Dictionary()\n        self.corpus = []\n        self.labels = []\n        self.cut_doc_obj = cutDoc()\n        self.w2v_file = W2V_FILE\n        self.class_num = CLASS_NUM\n        self.filter_sizes = (3, 8)\n        self.num_filters = 10\n        self.hidden_dims = 64\n        self.lr = lr\n        self.drop_every = drop_every\n        self.optimizer = optimizer\n\n    def load_word2vec(self):\n        """""" load_word2vec: load the w2v model\n        """"""\n        print ""Start load word2vec model""\n        self.w2vec = {}\n        with open(self.w2v_file, ""r"") as fread:\n            for line in fread.readlines():\n                # print line\n                line_list = line.strip().split("" "")\n                word = line_list[0]\n                word_vec = np.fromstring(\n                    \' \'.join(line_list[1:]), dtype=float, sep=\' \')\n                # print len(word_vec)\n                self.w2vec[word] = word_vec\n        print ""Done load word2vec model""\n\n    def __get_all_tokens_v2(self):\n        """""" get all tokens from file\n        """"""\n        print ""load the tokens from file ""\n        with open(\n                self.data_path.replace(""all_title.csv"", ""all_token.csv""),\n                \'r\') as fread:\n            for line in fread.readlines():\n                # print line\n                try:\n                    line_list = line.strip().split(""\\t"")\n                    label = line_list[0]\n                    # print label\n                    text_token = line_list[1].split(""\\\\"")\n                    self.dictionary.add_documents([text_token])\n                    self.labels.append(label)\n                    self.corpus.append(text_token)\n                except BaseException as e:\n                    print e\n                    continue\n\n        # print ""load dictionary fron file""\n        # self.dictionary.load(self.data_path.replace(""all_title.csv"",""cnn.dict""))\n\n    def __get_all_tokens(self):\n        """""" get all tokens of the corpus\n        """"""\n        fwrite = open(\n            self.data_path.replace(""all_title.csv"", ""all_token.csv""), \'w\')\n        with open(self.data_path, ""r"") as fread:\n            i = 0\n            for line in fread.readlines():\n                try:\n                    line_list = line.strip().split(""\\t"")\n                    label = line_list[0]\n                    self.labels.append(label)\n                    text = line_list[1]\n                    text_tokens = self.cut_doc_obj.run(text)\n                    self.corpus.append(text_tokens)\n                    self.dictionary.add_documents([text_tokens])\n                    fwrite.write(label + ""\\t"" + ""\\\\"".join(text_tokens) + ""\\n"")\n                    i += 1\n                except BaseException as e:\n                    msg = traceback.format_exc()\n                    print msg\n                    print ""=====>Read Done<======""\n                    break\n        self.token_len = self.dictionary.__len__()\n        print ""all token len "" + str(self.token_len)\n        print ""save the dictionary""\n        self.dictionary.save(\n            self.data_path.replace(""all_title.csv"", ""cnn.dict""))\n        self.num_data = i\n        fwrite.close()\n\n    def __filter_tokens(self, threshold_num=10):\n        small_freq_ids = [\n            tokenid for tokenid, docfreq in self.dictionary.dfs.items()\n            if docfreq < threshold_num\n        ]\n        self.dictionary.filter_tokens(small_freq_ids)\n        self.dictionary.compactify()\n\n    def gen_embedding_matrix(self, load4file=True):\n        """""" gen_embedding_matrix: generate the embedding matrix\n        """"""\n        if load4file:\n            self.__get_all_tokens_v2()\n        else:\n            self.__get_all_tokens()\n        print ""before filter, the tokens len: {0}"".format(\n            self.dictionary.__len__())\n        self.__filter_tokens()\n        print ""after filter, the tokens len: {0}"".format(\n            self.dictionary.__len__())\n        self.sequence = []\n        for file_token in self.corpus:\n            temp_sequence = [x for x, y in self.dictionary.doc2bow(file_token)]\n            self.sequence.append(temp_sequence)\n\n        # bow_vec_file = open(self.data_path.replace(""all_title.csv"",""bow_vec.pl""), \'wb\')\n        # print ""after filter, the tokens len: {0}"".format(self.dictionary.__len__())\n        self.corpus_size = len(self.dictionary.token2id)\n        self.embedding_matrix = np.zeros((self.corpus_size, EMBEDDING_DIM))\n        print ""corpus size: {0}"".format(len(self.dictionary.token2id))\n        for key, v in self.dictionary.token2id.items():\n            key_vec = self.w2vec.get(key)\n            if key_vec is not None:\n                self.embedding_matrix[v] = key_vec\n            else:\n                self.embedding_matrix[v] = np.random.rand(EMBEDDING_DIM) - 0.5\n\n    def step_decay(self, epoch):\n        drop_every = 5\n        decay_rate = (self.lr * np.power(\n            0.5, np.floor(\n                (1 + self.drop_every) / self.drop_every))).astype(\'float32\')\n        return decay_rate\n\n    def __build_network(self):\n        embedding_layer = Embedding(\n            self.corpus_size,\n            EMBEDDING_DIM,\n            weights=[self.embedding_matrix],\n            input_length=MAX_TITLE_LENGTH,\n            trainable=False)\n        # train a 1D convnet with global maxpooling\n        sequence_input = Input(shape=(MAX_TITLE_LENGTH, ), dtype=\'int32\')\n        embedded_sequences = embedding_layer(sequence_input)\n        x = LSTM(\n            128,\n            dropout_W=0.2,\n            dropout_U=0.2,\n            W_regularizer=regularizers.l2(0.01),\n            b_regularizer=regularizers.l2(0.01))(embedded_sequences)\n        x = Dropout(0.5)(x)\n        preds = Dense(self.class_num, activation=\'softmax\')(x)\n        print preds.get_shape()\n        if self.optimizer == \'adam\':\n            self.optimizer = Adam(lr=self.lr)\n        if self.optimizer == \'rmsprop\':\n            self.optimizer = RMSprop(lr=self.lr)\n\n        # rmsprop = RMSprop(lr=self.lr)\n        self.model = Model(sequence_input, preds)\n        self.model.compile(\n            loss=\'categorical_crossentropy\',\n            optimizer=self.optimizer,\n            metrics=[\'acc\'])\n\n    def train(self):\n        self.__split_train_test()\n        self.__build_network()\n        tensorboard = TensorBoard(log_dir=""./logs/lstm/"")\n        ckpt_file = ""weights.{epoch:02d}-{val_loss:.2f}.hdf5""\n        model_checkpoint = ModelCheckpoint(ckpt_file, period=10)\n        lrate = LearningRateScheduler(self.step_decay)\n        self.model.fit(self.train_set,\n                       self.train_tag,\n                       validation_data=(self.test_set, self.test_tag),\n                       nb_epoch=100,\n                       batch_size=64,\n                       callbacks=[tensorboard, model_checkpoint, lrate])\n        self.model.save(self.data_path.replace(""all_title.csv"", ""cnn.model""))\n\n    def __split_train_test(self):\n        # print len(self.sequence)\n        # print self.sequence[0]\n        self.data = pad_sequences(self.sequence, maxlen=MAX_TITLE_LENGTH)\n        self.train_set, self.test_set, self.train_tag, self.test_tag = train_test_split(\n            self.data, self.labels, test_size=0.2)\n        print ""train_tag {0}"", \' \'.join(self.train_tag)[0:1000]\n        self.train_tag = to_categorical(np.asarray(self.train_tag))\n        self.test_tag = to_categorical(np.asarray(self.test_tag))\n        # print np.asarray(self.train_tag).shape\n\n\nif __name__ == ""__main__"":\n    lstm_text_classifier_obj = lstm_text_classifier(\n        ""../data/origin_data/all_title.csv"")\n    lstm_text_classifier_obj.load_word2vec()\n    lstm_text_classifier_obj.gen_embedding_matrix(load4file=True)\n    lstm_text_classifier_obj.train()\n'"
nlp/text_classifier/scripts/tfidf_text_classifier.py,0,"b'from dataset_helpers.cut_doc import cutDoc\nimport numpy as np\nfrom gensim import corpora, models\nfrom pprint import pprint\nimport traceback\nimport sys\nimport cPickle as pickle\nfrom scipy.sparse import *\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.externals import joblib\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import TfidfTransformer\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import confusion_matrix\n\nreload(sys)\nsys.setdefaultencoding(\'utf-8\')\n\n\nclass tfidf_text_classifier:\n    """""" tf_idf_text_classifier: a text classifier of tfidf\n    """"""\n\n    def __init__(self, data_path):\n        self.data_path = data_path\n        self.dictionary = corpora.Dictionary()\n        self.corpus = []\n        self.labels = []\n        self.cut_doc_obj = cutDoc()\n\n    def __get_all_tokens(self):\n        """""" get all tokens of the corpus\n        """"""\n        fwrite = open(\n            self.data_path.replace(""all_title.csv"", ""all_token.csv""), \'w\')\n        with open(self.data_path, ""r"") as fread:\n            i = 0\n            # while True:\n            for line in fread.readlines():\n                try:\n                    line_list = line.strip().split(""\\t"")\n                    label = line_list[0]\n                    self.labels.append(label)\n                    text = line_list[1]\n                    text_tokens = self.cut_doc_obj.run(text)\n                    self.corpus.append(\' \'.join(text_tokens))\n                    self.dictionary.add_documents([text_tokens])\n                    fwrite.write(label + ""\\t"" + ""\\\\"".join(text_tokens) + ""\\n"")\n                    i += 1\n                except BaseException as e:\n                    msg = traceback.format_exc()\n                    print msg\n                    print ""=====>Read Done<======""\n                    break\n        self.token_len = self.dictionary.__len__()\n        print ""all token len "" + str(self.token_len)\n        self.num_data = i\n        fwrite.close()\n\n    def __filter_tokens(self, threshold_num=10):\n        small_freq_ids = [\n            tokenid for tokenid, docfreq in self.dictionary.dfs.items()\n            if docfreq < threshold_num\n        ]\n        self.dictionary.filter_tokens(small_freq_ids)\n        self.dictionary.compactify()\n\n    def vec(self):\n        """""" vec: get a vec representation of bow\n        """"""\n        self.__get_all_tokens()\n        print ""before filter, the tokens len: {0}"".format(\n            self.dictionary.__len__())\n        vectorizer = CountVectorizer(min_df=1e-5)\n        transformer = TfidfTransformer()\n        # sparse matrix\n        self.tfidf = transformer.fit_transform(\n            vectorizer.fit_transform(self.corpus))\n        words = vectorizer.get_feature_names()\n        print ""word len: {0}"".format(len(words))\n        # print self.tfidf[0]\n        print ""tfidf shape ({0},{1})"".format(self.tfidf.shape[0],\n                                             self.tfidf.shape[1])\n\n        # write the tfidf vec into a file\n        tfidf_vec_file = open(\n            self.data_path.replace(""all_title.csv"", ""tfidf_vec.pl""), \'wb\')\n        pickle.dump(self.tfidf, tfidf_vec_file)\n        tfidf_vec_file.close()\n        tfidf_label_file = open(\n            self.data_path.replace(""all_title.csv"", ""tfidf_label.pl""), \'wb\')\n        pickle.dump(self.labels, tfidf_label_file)\n        tfidf_label_file.close()\n\n    def split_train_test(self):\n        self.train_set, self.test_set, self.train_tag, self.test_tag = train_test_split(\n            self.tfidf, self.labels, test_size=0.2)\n        print ""train set shape: ""\n        print self.train_set.shape\n        train_set_file = open(\n            self.data_path.replace(""all_title.csv"", ""tfidf_train_set.pl""),\n            \'wb\')\n        pickle.dump(self.train_set, train_set_file)\n        train_tag_file = open(\n            self.data_path.replace(""all_title.csv"", ""tfidf_train_tag.pl""),\n            \'wb\')\n        pickle.dump(self.train_tag, train_tag_file)\n        test_set_file = open(\n            self.data_path.replace(""all_title.csv"", ""tfidf_test_set.pl""), \'wb\')\n        pickle.dump(self.test_set, test_set_file)\n        test_tag_file = open(\n            self.data_path.replace(""all_title.csv"", ""tfidf_test_tag.pl""), \'wb\')\n        pickle.dump(self.test_tag, test_tag_file)\n\n    def train(self):\n        print ""Beigin to Train the model""\n        lr_model = LogisticRegression()\n        lr_model.fit(self.train_set, self.train_tag)\n        print ""End Now, and evalution the model with test dataset""\n        # print ""mean accuracy: {0}"".format(lr_model.score(self.test_set, self.test_tag))\n        y_pred = lr_model.predict(self.test_set)\n        print classification_report(self.test_tag, y_pred)\n        print confusion_matrix(self.test_tag, y_pred)\n        print ""save the trained model to tfidf_lr_model.pl""\n        joblib.dump(lr_model,\n                    self.data_path.replace(""all_title.csv"",\n                                           ""tfidf_lr_model.pl""))\n\n\nif __name__ == ""__main__"":\n    bow_text_classifier_obj = tfidf_text_classifier(\n        ""../data/origin_data/all_title.csv"")\n    bow_text_classifier_obj.vec()\n    bow_text_classifier_obj.split_train_test()\n    bow_text_classifier_obj.train()\n'"
nlp/NMT/scripts/dataset_helpers/Constants.py,0,"b""PAD = 0\nUNK = 1\nBOS = 2\nEOS = 3\n\nPAD_WORD = '<blank>'\nUNK_WORD = '<unk>'\nBOS_WORD = '<s>'\nEOS_WORD = '</s>'\n"""
nlp/NMT/scripts/dataset_helpers/Dict.py,0,"b'import codecs\nimport Constants\n\n\nclass Dict(object):\n    def __init__(self, data=None, lower=False):\n        self.idxToLabel = {}\n        self.labelToIdx = {}\n        self.frequencies = {}\n        self.lower = lower\n\n        # Special entries will not be pruned.\n        self.special = []\n\n        if data is not None:\n            if type(data) == str:\n                self.loadFile(data)\n            else:\n                self.addSpecials(data)\n\n    def size(self):\n        return len(self.idxToLabel)\n\n    def loadFile(self, filename):\n        ""Load entries from a file.""\n        for line in codecs.open(filename, \'r\', \'utf-8\'):\n            fields = line.split()\n            label = fields[0]\n            idx = int(fields[1])\n            self.add(label, idx)\n\n    def writeFile(self, filename):\n        ""Write entries to a file.""\n        with codecs.open(filename, \'w\', \'utf-8\') as file:\n            for i in range(self.size()):\n                label = self.idxToLabel[i]\n                file.write(\'%s %d\\n\' % (label, i))\n\n        file.close()\n\n    def lookup(self, key, default=None):\n        key = key.lower() if self.lower else key\n        try:\n            return self.labelToIdx[key]\n        except KeyError:\n            return default\n\n    def align(self, other):\n        ""Find the id of each label in other dict.""\n        alignment = [Constants.PAD] * self.size()\n        for idx, label in self.idxToLabel.items():\n            if label in other.labelToIdx:\n                alignment[idx] = other.labelToIdx[label]\n        return alignment\n\n    def getLabel(self, idx, default=None):\n        try:\n            return self.idxToLabel[idx]\n        except KeyError:\n            return default\n\n    def addSpecial(self, label, idx=None):\n        ""Mark this `label` and `idx` as special (i.e. will not be pruned).""\n        idx = self.add(label, idx)\n        self.special += [idx]\n\n    def addSpecials(self, labels):\n        ""Mark all labels in `labels` as specials (i.e. will not be pruned).""\n        for label in labels:\n            self.addSpecial(label)\n\n    def add(self, label, idx=None):\n        ""Add `label` in the dictionary. Use `idx` as its index if given.""\n        label = label.lower() if self.lower else label\n        if idx is not None:\n            self.idxToLabel[idx] = label\n            self.labelToIdx[label] = idx\n        else:\n            if label in self.labelToIdx:\n                idx = self.labelToIdx[label]\n            else:\n                idx = len(self.idxToLabel)\n                self.idxToLabel[idx] = label\n                self.labelToIdx[label] = idx\n\n        if idx not in self.frequencies:\n            self.frequencies[idx] = 1\n        else:\n            self.frequencies[idx] += 1\n\n        return idx\n\n    def prune(self, size):\n        ""Return a new dictionary with the `size` most frequent entries.""\n        if size >= self.size():\n            return self\n\n        # Only keep the `size` most frequent entries.\n        freq = [self.frequencies[i] for i in range(len(self.frequencies))]\n        print freq[:100]\n        idx = sorted(range(len(freq)), key=lambda k: freq[k], reverse=True)\n        print idx[:100]\n\n        newDict = Dict()\n        newDict.lower = self.lower\n\n        # Add special entries in all cases.\n        for i in self.special:\n            newDict.addSpecial(self.idxToLabel[i])\n\n        for i in idx[:size]:\n            newDict.add(self.idxToLabel[i])\n\n        return newDict\n\n    def convertToIdx(self, labels, unkWord, bosWord=None, eosWord=None):\n        """"""\n        Convert `labels` to indices. Use `unkWord` if not found.\n        Optionally insert `bosWord` at the beginning and `eosWord` at the .\n        """"""\n        vec = []\n\n        if bosWord is not None:\n            vec += [self.lookup(bosWord)]\n\n        unk = self.lookup(unkWord)\n        vec += [self.lookup(label, default=unk) for label in labels]\n\n        if eosWord is not None:\n            vec += [self.lookup(eosWord)]\n\n        return vec\n\n    def convertToLabels(self, idx, stop):\n        """"""\n        Convert `idx` to labels.\n        If index `stop` is reached, convert it and return.\n        """"""\n\n        labels = []\n\n        for i in idx:\n            labels += [self.getLabel(i)]\n            if i == stop:\n                break\n\n        return labels\n'"
nlp/NMT/scripts/dataset_helpers/IO.py,0,"b'# -*- coding: utf-8 -*-\nimport Constants\nimport numpy as np\n\n\ndef align(src_tokens, tgt_tokens):\n    """"""\n    Given two sequences of tokens, return\n    a mask of where there is overlap.\n\n    Returns:\n        mask: tgt_len x src_len\n    """"""\n    mask = np.zeros([len(src_tokens), len(tgt_tokens)])\n\n    for i in range(len(src_tokens)):\n        for j in range(len(tgt_tokens)):\n            if src_tokens[i] == tgt_tokens[j]:\n                mask[i][j] = 1\n    return mask\n\n\ndef readSrcLine(src_line,\n                src_dict,\n                src_feature_dicts,\n                _type=""text"",\n                src_img_dir=""""):\n    srcFeats = None\n    if _type == ""text"":\n        srcWords, srcFeatures, _ = extractFeatures(src_line)\n        srcData = src_dict.convertToIdx(srcWords, Constants.UNK_WORD)\n        if src_feature_dicts:\n            srcFeats = [\n                src_feature_dicts[j].convertToIdx(srcFeatures[j],\n                                                  Constants.UNK_WORD)\n                for j in range(len(src_feature_dicts))\n            ]\n    # elif _type == ""img"":\n    #     if not transforms:\n    #         loadImageLibs()\n    #     srcData = transforms.ToTensor()(Image.open(src_img_dir + ""/"" +\n    #                                                srcWords[0]))\n\n    return srcWords, srcData, srcFeats\n\n\ndef readTgtLine(tgt_line, tgt_dict, tgt_feature_dicts, _type=""text""):\n    tgtFeats = None\n    tgtWords, tgtFeatures, _ = extractFeatures(tgt_line)\n    tgtData = tgt_dict.convertToIdx(tgtWords, Constants.UNK_WORD,\n                                    Constants.BOS_WORD, Constants.EOS_WORD)\n    if tgt_feature_dicts:\n        tgtFeats = [\n            tgt_feature_dicts[j].convertToIdx(tgtFeatures[j],\n                                              Constants.UNK_WORD)\n            for j in range(len(tgt_feature_dicts))\n        ]\n\n    return tgtWords, tgtData, tgtFeats\n\n\ndef extractFeatures(tokens):\n    ""Given a list of token separate out words and features (if any).""\n    words = []\n    features = []\n    numFeatures = None\n\n    for t in range(len(tokens)):\n        field = tokens[t].split(u""\xef\xbf\xa8"")\n        word = field[0]\n        if len(word) > 0:\n            words.append(word)\n\n            if numFeatures is None:\n                numFeatures = len(field) - 1\n            else:\n                assert (len(field) - 1 == numFeatures), \\\n                    ""all words must have the same number of features""\n\n            if len(field) > 1:\n                for i in range(1, len(field)):\n                    if len(features) <= i - 1:\n                        features.append([])\n                    features[i - 1].append(field[i])\n                    assert (len(features[i - 1]) == len(words))\n    return words, features, numFeatures if numFeatures else 0\n'"
nlp/NMT/scripts/dataset_helpers/__init__.py,0,b''
nlp/NMT/scripts/dataset_helpers/copus.py,0,"b'from keras.preprocessing.text import Tokenizer\nimport os\nimport operator\nimport sys\nsys.path.append("".."")\nfrom config import *\n\n\nclass copus:\n    def __init__(self, train_data_path, test_data_path, test_type=""2013""):\n        self.train_data_path = train_data_path\n        self.test_data_path = test_data_path\n        self.__build_dataset(test_type)\n\n    def __build_dataset(self, test_type=""2013""):\n        self.train_source_file = os.path.join(self.train_data_path, TRAIN_X)\n        self.train_target_file = os.path.join(self.train_data_path, TRAIN_Y)\n        if test_type == ""2013"":\n            self.test_source_file = os.path.join(self.test_data_path,\n                                                 TEST_X_2013)\n            self.test_target_file = os.path.join(self.test_data_path,\n                                                 TEST_Y_2013)\n        if test_type == ""2014"":\n            self.test_source_file = os.path.join(self.test_data_path,\n                                                 TEST_X_2014)\n            self.test_target_file = os.path.join(self.test_data_path,\n                                                 TEST_Y_2014)\n        if test_type == ""2015"":\n            self.test_source_file = os.path.join(self.test_data_path,\n                                                 TEST_X_2015)\n            self.test_target_file = os.path.join(self.test_data_path,\n                                                 TEST_Y_2015)\n\n    def read_copus_generator(self, batch_size=64):\n        """""" return a generator with the specified batch_size\n        """"""\n        logger.info(""Beigin read copus {0}"".format(file_name))\n        data = []\n        index = 0\n        with open(file_name, \'r\') as fread:\n            while True:\n                try:\n                    line = fread.readline()\n                    data.append(line)\n                    index += 1\n                    if index % 100000 == 0:\n                        logger.info(""The program has processed {0} lines "".\n                                    format(index))\n                except:\n                    logger.info(""Read End"")\n                    break\n        tokenizer = Tokenizer(nb_words=30000)\n        tokenizer.fit_on_texts(data)\n        logger.info(""word num: {0}"".format(len(tokenizer.word_counts)))\n        sorted_word_counts = sorted(\n            tokenizer.word_counts.items(),\n            key=operator.itemgetter(1),\n            reverse=True)\n        # save the word_counts to the meta\n        with open(file_name.replace(""train."", ""meta.""), ""w"") as fwrite:\n            for word_cnt in sorted_word_counts:\n                key = word_cnt[0]\n                val = word_cnt[1]\n                line = key + "":"" + str(val) + ""\\n""\n                fwrite.write(line)\n        vectorize_data = tokenizer.texts_to_matrix(data)\n        return vectorize_data\n\n\nif __name__ == ""__main__"":\n    copus_obj = copus(""../../datasets/stanford/train"",\n                      ""../../datasets/stanford/test"")\n    logger.info(copus_obj.train_source_data[0])\n    logger.info(""train copus shape {0}"".format(\n        copus_obj.train_source_data.shape))\n'"
nlp/NMT/scripts/dataset_helpers/preprocess.py,0,"b'# -*- coding: utf-8 -*-\n\nimport argparse\nimport codecs\nfrom Dict import Dict\nimport numpy as np\nimport Constants\nimport IO\nimport pickle\n\nparser = argparse.ArgumentParser(description=\'preprocess.py\')\n\n# **Preprocess Options**\n\nparser.add_argument(\'-config\', help=""Read options from this file"")\n\nparser.add_argument(\n    \'-src_type\',\n    default=""text"",\n    help=""Type of the source input. Options are [text]."")\n\nparser.add_argument(\n    \'-train_src\', required=True, help=""Path to the training source data"")\nparser.add_argument(\n    \'-train_tgt\', required=True, help=""Path to the training target data"")\nparser.add_argument(\n    \'-valid_src\', required=True, help=""Path to the validation source data"")\nparser.add_argument(\n    \'-valid_tgt\', required=True, help=""Path to the validation target data"")\n\nparser.add_argument(\n    \'-save_data\', required=True, help=""Output file for the prepared data"")\n\nparser.add_argument(\n    \'-src_vocab_size\',\n    type=int,\n    default=50000,\n    help=""Size of the source vocabulary"")\nparser.add_argument(\n    \'-tgt_vocab_size\',\n    type=int,\n    default=50000,\n    help=""Size of the target vocabulary"")\nparser.add_argument(\'-src_vocab\', help=""Path to an existing source vocabulary"")\nparser.add_argument(\'-tgt_vocab\', help=""Path to an existing target vocabulary"")\nparser.add_argument(\n    \'-features_vocabs_prefix\',\n    type=str,\n    default=\'\',\n    help=""Path prefix to existing features vocabularies"")\nparser.add_argument(\n    \'-src_seq_length\',\n    type=int,\n    default=50,\n    help=""Maximum source sequence length"")\nparser.add_argument(\n    \'-src_seq_length_trunc\',\n    type=int,\n    default=0,\n    help=""Truncate source sequence length."")\nparser.add_argument(\n    \'-tgt_seq_length\',\n    type=int,\n    default=50,\n    help=""Maximum target sequence length to keep."")\nparser.add_argument(\n    \'-tgt_seq_length_trunc\',\n    type=int,\n    default=0,\n    help=""Truncate target sequence length."")\n\nparser.add_argument(\'-shuffle\', type=int, default=1, help=""Shuffle data"")\nparser.add_argument(\'-seed\', type=int, default=3435, help=""Random seed"")\n\nparser.add_argument(\'-lower\', action=\'store_true\', help=\'lowercase data\')\n\nparser.add_argument(\n    \'-report_every\',\n    type=int,\n    default=100000,\n    help=""Report status every this many sentences"")\n\nopt = parser.parse_args()\n\n\ndef makeVocabulary(filename, size):\n    ""Construct the word and feature vocabs.""\n    vocab = Dict(\n        [\n            Constants.PAD_WORD, Constants.UNK_WORD, Constants.BOS_WORD,\n            Constants.EOS_WORD\n        ],\n        lower=opt.lower)\n    featuresVocabs = []\n    with codecs.open(filename, ""r"", ""utf-8"") as f:\n        for sent in f.readlines():\n            words, features, numFeatures \\\n                = IO.extractFeatures(sent.split())\n\n            if len(featuresVocabs) == 0 and numFeatures > 0:\n                for j in range(numFeatures):\n                    featuresVocabs.append(\n                        Dict([\n                            Constants.PAD_WORD, Constants.UNK_WORD,\n                            Constants.BOS_WORD, Constants.EOS_WORD\n                        ]))\n            else:\n                assert len(featuresVocabs) == numFeatures, \\\n                    ""all sentences must have the same number of features""\n\n            for i in range(len(words)):\n                vocab.add(words[i])\n                for j in range(numFeatures):\n                    featuresVocabs[j].add(features[j][i])\n\n    originalSize = vocab.size()\n    vocab = vocab.prune(size)\n    print(\'Created dictionary of size %d (pruned from %d)\' %\n          (vocab.size(), originalSize))\n\n    return vocab, featuresVocabs\n\n\ndef initVocabulary(name, dataFile, vocabFile, vocabSize):\n    """"""If `vocabFile` exists, read it in,\n    Else, generate from data.""""""\n    vocab = None\n    if vocabFile is not None:\n        # If given, load existing word dictionary.\n        print(\'Reading \' + name + \' vocabulary from \\\'\' + vocabFile + \'\\\'...\')\n        vocab = Dict()\n        vocab.loadFile(vocabFile)\n        print(\'Loaded \' + str(vocab.size()) + \' \' + name + \' words\')\n\n    if vocab is None:\n        # If a dictionary is still missing, generate it.\n        print(\'Building \' + name + \' vocabulary...\')\n        genWordVocab, genFeaturesVocabs = makeVocabulary(dataFile, vocabSize)\n        vocab = genWordVocab\n        featuresVocabs = genFeaturesVocabs\n\n    print()\n    return vocab, featuresVocabs\n\n\ndef saveVocabulary(name, vocab, file):\n    print(\'Saving \' + name + \' vocabulary to \\\'\' + file + \'\\\'...\')\n    vocab.writeFile(file)\n\n\ndef saveFeaturesVocabularies(name, vocabs, prefix):\n    for j in range(len(vocabs)):\n        file = prefix + \'.\' + name + \'_feature_\' + str(j) + \'.dict\'\n        print(\'Saving \' + name + \' feature \' + str(j) + \' vocabulary to \\\'\' +\n              file + \'\\\'...\')\n        vocabs[j].writeFile(file)\n\n\ndef makeData(srcFile, tgtFile, srcDicts, tgtDicts, srcFeatureDicts,\n             tgtFeatureDicts):\n    src, tgt = [], []\n    srcFeats = [[] for i in range(len(srcFeatureDicts))]\n    tgtFeats = [[] for i in range(len(tgtFeatureDicts))]\n    alignments = []\n    sizes = []\n    count, ignored = 0, 0\n\n    print(\'Processing %s & %s ...\' % (srcFile, tgtFile))\n    srcF = codecs.open(srcFile, ""r"", ""utf-8"")\n    tgtF = codecs.open(tgtFile, ""r"", ""utf-8"")\n\n    while True:\n        sline = srcF.readline()\n        tline = tgtF.readline()\n\n        # normal end of file\n        if sline == """" and tline == """":\n            break\n\n        # source or target does not have same number of lines\n        if sline == """" or tline == """":\n            print(\'WARNING: src and tgt do not have the same # of sentences\')\n            break\n\n        sline = sline.strip()\n        tline = tline.strip()\n\n        # source and/or target are empty\n        if sline == """" or tline == """":\n            print(\'WARNING: ignoring an empty line (\' + str(count + 1) + \')\')\n            continue\n\n        srcLine = sline.split()\n        tgtLine = tline.split()\n\n        if len(srcLine) <= opt.src_seq_length \\\n           and len(tgtLine) <= opt.tgt_seq_length:\n\n            # Check truncation condition.\n            if opt.src_seq_length_trunc != 0:\n                srcLine = srcLine[:opt.src_seq_length_trunc]\n\n            if opt.tgt_seq_length_trunc != 0:\n                tgtLine = tgtLine[:opt.tgt_seq_length_trunc]\n\n            srcWords, srcData, srcFeat \\\n                = IO.readSrcLine(srcLine, srcDicts,\n                                      srcFeatureDicts,\n                                      _type=opt.src_type,\n                                      )\n            src += [srcData]\n            for i in range(len(srcFeats)):\n                srcFeats[i] += [srcFeat[i]]\n\n            tgtWords, tgtData, tgtFeat = IO.readTgtLine(tgtLine, tgtDicts,\n                                                        tgtFeatureDicts)\n            tgt += [tgtData]\n            for i in range(len(tgtFeats)):\n                tgtFeats[i] += [tgtFeat[i]]\n\n            alignments += [IO.align(srcWords, tgtWords)]\n            sizes += [len(srcData)]\n        else:\n            ignored += 1\n\n        count += 1\n\n        if count % opt.report_every == 0:\n            print(\'... %d sentences prepared\' % count)\n\n    srcF.close()\n    tgtF.close()\n\n    if opt.shuffle == 1:\n        print(\'... shuffling sentences\')\n        perm = np.arange(len(src))\n        np.random.shuffle(perm)\n        src = [src[idx] for idx in perm]\n        tgt = [tgt[idx] for idx in perm]\n        alignments = [alignments[idx] for idx in perm]\n        for j in range(len(srcFeatureDicts)):\n            srcFeats[j] = [srcFeats[j][idx] for idx in perm]\n        for j in range(len(tgtFeatureDicts)):\n            tgtFeats[j] = [tgtFeats[j][idx] for idx in perm]\n        sizes = [sizes[idx] for idx in perm]\n\n    print(\'... sorting sentences by size\')\n    # _, perm = torch.sort(torch.Tensor(sizes))\n\n    perm = sorted(range(len(sizes)), key=lambda k: sizes[k])\n\n    src = [src[idx] for idx in perm]\n    tgt = [tgt[idx] for idx in perm]\n    alignments = [alignments[idx] for idx in perm]\n    for j in range(len(srcFeatureDicts)):\n        srcFeats[j] = [srcFeats[j][idx] for idx in perm]\n    for j in range(len(tgtFeatureDicts)):\n        tgtFeats[j] = [tgtFeats[j][idx] for idx in perm]\n\n    print((\'Prepared %d sentences \' +\n           \'(%d ignored due to length == 0 or src len > %d or tgt len > %d)\') %\n          (len(src), ignored, opt.src_seq_length, opt.tgt_seq_length))\n\n    return src, tgt, srcFeats, tgtFeats, alignments\n\n\ndef main():\n\n    dicts = {}\n    dicts[\'src\'] = Dict()\n    if opt.src_type == ""text"":\n        dicts[\'src\'], dicts[\'src_features\'] = \\\n                initVocabulary(\'source\', opt.train_src, opt.src_vocab,\n                               opt.src_vocab_size)\n\n    dicts[\'tgt\'], dicts[\'tgt_features\'] = \\\n        initVocabulary(\'target\',\n                       opt.train_tgt,\n                       opt.tgt_vocab,\n                       opt.tgt_vocab_size)\n\n    print(\'Preparing training ...\')\n    train = {}\n    train[\'src\'], train[\'tgt\'], \\\n        train[\'src_features\'], train[\'tgt_features\'], \\\n        train[\'alignments\'] \\\n        = makeData(opt.train_src, opt.train_tgt,\n                   dicts[\'src\'], dicts[\'tgt\'],\n                   dicts[\'src_features\'], dicts[\'tgt_features\'])\n    print(\'Preparing validation ...\')\n    valid = {}\n    valid[\'src\'], valid[\'tgt\'], \\\n        valid[\'src_features\'], valid[\'tgt_features\'], \\\n        valid[\'alignments\'] \\\n        = makeData(opt.valid_src, opt.valid_tgt,\n                   dicts[\'src\'], dicts[\'tgt\'],\n                   dicts[\'src_features\'], dicts[\'tgt_features\'])\n\n    if opt.src_vocab is None:\n        saveVocabulary(\'source\', dicts[\'src\'], opt.save_data + \'.src.dict\')\n    if opt.tgt_vocab is None:\n        saveVocabulary(\'target\', dicts[\'tgt\'], opt.save_data + \'.tgt.dict\')\n    if opt.features_vocabs_prefix:\n        saveFeaturesVocabularies(\'source\', dicts[\'src_features\'],\n                                 opt.save_data)\n        saveFeaturesVocabularies(\'target\', dicts[\'tgt_features\'],\n                                 opt.save_data)\n\n    print(\'Saving data to \\\'\' + opt.save_data + \'.train.pt\\\'...\')\n    save_data = {\n        \'dicts\': dicts,\n        \'type\': opt.src_type,\n        \'train\': train,\n        \'valid\': valid\n    }\n    # torch.save(save_data, opt.save_data + \'.train.pt\')\n    with open(opt.save_data + \'.train.pt\', \'wb\') as fwrite:\n        pickle.dump(save_data, fwrite)\n\n\nif __name__ == ""__main__"":\n    main()\n'"
nlp/NMT/scripts/models/models.py,0,"b'from keras.optimizers import Adam, RMSprop, Nadam, Adadelta, SGD, Adagrad, Adamax\nfrom ..config import *\nfrom keras.layers import *\n\n\nclass TranslationModel():\n    def __init__(self, optimizer_type, lr, loss_type):\n        self.optimizer_type = optimizer_type\n        self.lr = lr\n        self.loss_type = loss_type\n\n    def setOptimizer(self):\n        logger.info(""Preparing optimizer: {0}, [LR: {1} - LOSS: {2}.]"".format(\n            self.optimizer_type, self.lr, self.loss_type))\n        if self.optimizer_type.lower() == ""sgd"":\n            self.optimizer = SGD(lr=self.lr, )\n        elif self.optimizer_type.lower() == ""rsmprop"":\n            self.optimizer = RMSprop(lr=self.lr)\n        elif self.optimizer_type.lower() == ""adagrad"":\n            self.optimizer = Adagrad(lr=self.lr)\n        elif self.optimizer_type.lower() == ""adam"":\n            self.optimizer = Adam(lr=self.lr)\n        elif self.optimizer_type.lower() == ""adamax"":\n            self.optimizer = Adamax(lr=self.lr)\n        elif self.optimizer_type.lower() == ""nadam"":\n            self.optimizer = Nadam(lr=self.lr)\n        else:\n            logger.info(""\\t WARNING: Not supported Now"")\n\n    def setLoss(self):\n        pass\n\n    def buildModel(self):\n        src_text = Input(\n            name=""NMT_input"", batch_shape=tuple([None, None]), dtype=""int32"")\n'"
nlp/text_classifier/scripts/dataset_helpers/__init__.py,0,b''
nlp/text_classifier/scripts/dataset_helpers/cut_doc.py,0,"b'#-*-coding:utf-8-*-\nimport jieba\nimport logging\nlogger = logging.getLogger(__name__)\nhandler = logging.StreamHandler()\nformatter = logging.Formatter(\n    \'%(asctime)s %(name)-12s %(levelname)-8s %(message)s\')\nhandler.setFormatter(formatter)\nlogger.addHandler(handler)\nlogger.setLevel(logging.DEBUG)\nimport re\nimport traceback\nimport os\nimport sys\nreload(sys)\nsys.setdefaultencoding(\'utf-8\')\n# from ..config import *\nWORD_DICT = ""/Users/burness/git_repository/dl_opensource/nlp/oxford-cs-deepnlp-2017/practical-2/data/origin_data/t_tag_infos.txt""\n\n\nclass cutDoc:\n    """""" cut_doc: cut the document\n    """"""\n\n    def __init__(self, mode=""default""):\n        self.mode = mode\n        self.stop_words = [\n            u""\xe4\xb8\x80"",u""\xe4\xb8\x80\xe4\xb8\x8b"",u""\xe4\xb8\x80\xe4\xba\x9b"",u""\xe4\xb8\x80\xe5\x88\x87"",u""\xe4\xb8\x80\xe5\x88\x99"",u""\xe4\xb8\x80\xe5\xa4\xa9"",u""\xe4\xb8\x80\xe5\xae\x9a"",u""\xe4\xb8\x80\xe6\x96\xb9\xe9\x9d\xa2"",u""\xe4\xb8\x80\xe6\x97\xa6"",u""\xe4\xb8\x80\xe6\x97\xb6"",u""\xe4\xb8\x80\xe6\x9d\xa5"",\n            u""\xe4\xb8\x80\xe6\xa0\xb7"",u""\xe4\xb8\x80\xe6\xac\xa1"",u""\xe4\xb8\x80\xe7\x89\x87"",u""\xe4\xb8\x80\xe7\x9b\xb4"",u""\xe4\xb8\x80\xe8\x87\xb4"",u""\xe4\xb8\x80\xe8\x88\xac"",u""\xe4\xb8\x80\xe8\xb5\xb7"",u""\xe4\xb8\x80\xe8\xbe\xb9"",u""\xe4\xb8\x80\xe9\x9d\xa2"",u""\xe4\xb8\x87\xe4\xb8\x80"",u""\xe4\xb8\x8a\xe4\xb8\x8b"",\n            u""\xe4\xb8\x8a\xe5\x8d\x87"",u""\xe4\xb8\x8a\xe5\x8e\xbb"",u""\xe4\xb8\x8a\xe6\x9d\xa5"",u""\xe4\xb8\x8a\xe8\xbf\xb0"",u""\xe4\xb8\x8a\xe9\x9d\xa2"",u""\xe4\xb8\x8b\xe5\x88\x97"",u""\xe4\xb8\x8b\xe5\x8e\xbb"",u""\xe4\xb8\x8b\xe6\x9d\xa5"",u""\xe4\xb8\x8b\xe9\x9d\xa2"",u""\xe4\xb8\x8d\xe4\xb8\x80"",u""\xe4\xb8\x8d\xe4\xb9\x85"",\n            u""\xe4\xb8\x8d\xe4\xbb\x85"",u""\xe4\xb8\x8d\xe4\xbc\x9a"",u""\xe4\xb8\x8d\xe4\xbd\x86"",u""\xe4\xb8\x8d\xe5\x85\x89"",u""\xe4\xb8\x8d\xe5\x8d\x95"",u""\xe4\xb8\x8d\xe5\x8f\x98"",u""\xe4\xb8\x8d\xe5\x8f\xaa"",u""\xe4\xb8\x8d\xe5\x8f\xaf"",u""\xe4\xb8\x8d\xe5\x90\x8c"",u""\xe4\xb8\x8d\xe5\xa4\x9f"",u""\xe4\xb8\x8d\xe5\xa6\x82"",\n            u""\xe4\xb8\x8d\xe5\xbe\x97"",u""\xe4\xb8\x8d\xe6\x80\x95"",u""\xe4\xb8\x8d\xe6\x83\x9f"",u""\xe4\xb8\x8d\xe6\x88\x90"",u""\xe4\xb8\x8d\xe6\x8b\x98"",u""\xe4\xb8\x8d\xe6\x95\xa2"",u""\xe4\xb8\x8d\xe6\x96\xad"",u""\xe4\xb8\x8d\xe6\x98\xaf"",u""\xe4\xb8\x8d\xe6\xaf\x94"",u""\xe4\xb8\x8d\xe7\x84\xb6"",u""\xe4\xb8\x8d\xe7\x89\xb9"",\n            u""\xe4\xb8\x8d\xe7\x8b\xac"",u""\xe4\xb8\x8d\xe7\xae\xa1"",u""\xe4\xb8\x8d\xe8\x83\xbd"",u""\xe4\xb8\x8d\xe8\xa6\x81"",u""\xe4\xb8\x8d\xe8\xae\xba"",u""\xe4\xb8\x8d\xe8\xb6\xb3"",u""\xe4\xb8\x8d\xe8\xbf\x87"",u""\xe4\xb8\x8d\xe9\x97\xae"",u""\xe4\xb8\x8e"",u""\xe4\xb8\x8e\xe5\x85\xb6"",u""\xe4\xb8\x8e\xe5\x90\xa6"",\n            u""\xe4\xb8\x8e\xe6\xad\xa4\xe5\x90\x8c\xe6\x97\xb6"",u""\xe4\xb8\x93\xe9\x97\xa8"",u""\xe4\xb8\x94"",u""\xe4\xb8\xa4\xe8\x80\x85"",u""\xe4\xb8\xa5\xe6\xa0\xbc"",u""\xe4\xb8\xa5\xe9\x87\x8d"",u""\xe4\xb8\xaa"",u""\xe4\xb8\xaa\xe4\xba\xba"",u""\xe4\xb8\xaa\xe5\x88\xab"",u""\xe4\xb8\xad\xe5\xb0\x8f"",u""\xe4\xb8\xad\xe9\x97\xb4"",\n            u""\xe4\xb8\xb0\xe5\xaf\x8c"",u""\xe4\xb8\xb4"",u""\xe4\xb8\xba"",u""\xe4\xb8\xba\xe4\xb8\xbb"",u""\xe4\xb8\xba\xe4\xba\x86"",u""\xe4\xb8\xba\xe4\xbb\x80\xe4\xb9\x88"",u""\xe4\xb8\xba\xe4\xbb\x80\xe9\xba\xbd"",u""\xe4\xb8\xba\xe4\xbd\x95"",u""\xe4\xb8\xba\xe7\x9d\x80"",u""\xe4\xb8\xbb\xe5\xbc\xa0"",u""\xe4\xb8\xbb\xe8\xa6\x81"",\n            u""\xe4\xb8\xbe\xe8\xa1\x8c"",u""\xe4\xb9\x83"",u""\xe4\xb9\x83\xe8\x87\xb3"",u""\xe4\xb9\x88"",u""\xe4\xb9\x8b"",u""\xe4\xb9\x8b\xe4\xb8\x80"",u""\xe4\xb9\x8b\xe5\x89\x8d"",u""\xe4\xb9\x8b\xe5\x90\x8e"",u""\xe4\xb9\x8b\xe5\xbe\x8c"",u""\xe4\xb9\x8b\xe6\x89\x80\xe4\xbb\xa5"",u""\xe4\xb9\x8b\xe7\xb1\xbb"",\n            u""\xe4\xb9\x8c\xe4\xb9\x8e"",u""\xe4\xb9\x8e"",u""\xe4\xb9\x98"",u""\xe4\xb9\x9f"",u""\xe4\xb9\x9f\xe5\xa5\xbd"",u""\xe4\xb9\x9f\xe6\x98\xaf"",u""\xe4\xb9\x9f\xe7\xbd\xa2"",u""\xe4\xba\x86"",u""\xe4\xba\x86\xe8\xa7\xa3"",u""\xe4\xba\x89\xe5\x8f\x96"",u""\xe4\xba\x8e"",u""\xe4\xba\x8e\xe6\x98\xaf"",\n            u""\xe4\xba\x8e\xe6\x98\xaf\xe4\xb9\x8e"",u""\xe4\xba\x91\xe4\xba\x91"",u""\xe4\xba\x92\xe7\x9b\xb8"",u""\xe4\xba\xa7\xe7\x94\x9f"",u""\xe4\xba\xba\xe4\xbb\xac"",u""\xe4\xba\xba\xe5\xae\xb6"",u""\xe4\xbb\x80\xe4\xb9\x88"",u""\xe4\xbb\x80\xe4\xb9\x88\xe6\xa0\xb7"",u""\xe4\xbb\x80\xe9\xba\xbd"",u""\xe4\xbb\x8a\xe5\x90\x8e"",u""\xe4\xbb\x8a\xe5\xa4\xa9"",\n            u""\xe4\xbb\x8a\xe5\xb9\xb4"",u""\xe4\xbb\x8a\xe5\xbe\x8c"",u""\xe4\xbb\x8d\xe7\x84\xb6"",u""\xe4\xbb\x8e"",u""\xe4\xbb\x8e\xe4\xba\x8b"",u""\xe4\xbb\x8e\xe8\x80\x8c"",u""\xe4\xbb\x96"",u""\xe4\xbb\x96\xe4\xba\xba"",u""\xe4\xbb\x96\xe4\xbb\xac"",u""\xe4\xbb\x96\xe7\x9a\x84"",u""\xe4\xbb\xa3\xe6\x9b\xbf"",\n            u""\xe4\xbb\xa5"",u""\xe4\xbb\xa5\xe4\xb8\x8a"",u""\xe4\xbb\xa5\xe4\xb8\x8b"",u""\xe4\xbb\xa5\xe4\xb8\xba"",u""\xe4\xbb\xa5\xe4\xbe\xbf"",u""\xe4\xbb\xa5\xe5\x85\x8d"",u""\xe4\xbb\xa5\xe5\x89\x8d"",u""\xe4\xbb\xa5\xe5\x8f\x8a"",u""\xe4\xbb\xa5\xe5\x90\x8e"",u""\xe4\xbb\xa5\xe5\xa4\x96"",u""\xe4\xbb\xa5\xe5\xbe\x8c"",\n            u""\xe4\xbb\xa5\xe6\x9d\xa5"",u""\xe4\xbb\xa5\xe8\x87\xb3"",u""\xe4\xbb\xa5\xe8\x87\xb3\xe4\xba\x8e"",u""\xe4\xbb\xa5\xe8\x87\xb4"",u""\xe4\xbb\xac"",u""\xe4\xbb\xbb"",u""\xe4\xbb\xbb\xe4\xbd\x95"",u""\xe4\xbb\xbb\xe5\x87\xad"",u""\xe4\xbb\xbb\xe5\x8a\xa1"",u""\xe4\xbc\x81\xe5\x9b\xbe"",u""\xe4\xbc\x9f\xe5\xa4\xa7"",\n            u""\xe4\xbc\xbc\xe4\xb9\x8e"",u""\xe4\xbc\xbc\xe7\x9a\x84"",u""\xe4\xbd\x86"",u""\xe4\xbd\x86\xe6\x98\xaf"",u""\xe4\xbd\x95"",u""\xe4\xbd\x95\xe5\x86\xb5"",u""\xe4\xbd\x95\xe5\xa4\x84"",u""\xe4\xbd\x95\xe6\x97\xb6"",u""\xe4\xbd\x9c\xe4\xb8\xba"",u""\xe4\xbd\xa0"",u""\xe4\xbd\xa0\xe4\xbb\xac"",\n            u""\xe4\xbd\xa0\xe7\x9a\x84"",u""\xe4\xbd\xbf\xe5\xbe\x97"",u""\xe4\xbd\xbf\xe7\x94\xa8"",u""\xe4\xbe\x8b\xe5\xa6\x82"",u""\xe4\xbe\x9d"",u""\xe4\xbe\x9d\xe7\x85\xa7"",u""\xe4\xbe\x9d\xe9\x9d\xa0"",u""\xe4\xbf\x83\xe8\xbf\x9b"",u""\xe4\xbf\x9d\xe6\x8c\x81"",u""\xe4\xbf\xba"",u""\xe4\xbf\xba\xe4\xbb\xac"",\n            u""\xe5\x80\x98"",u""\xe5\x80\x98\xe4\xbd\xbf"",u""\xe5\x80\x98\xe6\x88\x96"",u""\xe5\x80\x98\xe7\x84\xb6"",u""\xe5\x80\x98\xe8\x8b\xa5"",u""\xe5\x81\x87\xe4\xbd\xbf"",u""\xe5\x81\x87\xe5\xa6\x82"",u""\xe5\x81\x87\xe8\x8b\xa5"",u""\xe5\x81\x9a\xe5\x88\xb0"",u""\xe5\x83\x8f"",u""\xe5\x85\x81\xe8\xae\xb8"",\n            u""\xe5\x85\x85\xe5\x88\x86"",u""\xe5\x85\x88\xe5\x90\x8e"",u""\xe5\x85\x88\xe5\xbe\x8c"",u""\xe5\x85\x88\xe7\x94\x9f"",u""\xe5\x85\xa8\xe9\x83\xa8"",u""\xe5\x85\xa8\xe9\x9d\xa2"",u""\xe5\x85\xae"",u""\xe5\x85\xb1\xe5\x90\x8c"",u""\xe5\x85\xb3\xe4\xba\x8e"",u""\xe5\x85\xb6"",u""\xe5\x85\xb6\xe4\xb8\x80"",\n            u""\xe5\x85\xb6\xe4\xb8\xad"",u""\xe5\x85\xb6\xe4\xba\x8c"",u""\xe5\x85\xb6\xe4\xbb\x96"",u""\xe5\x85\xb6\xe4\xbd\x99"",u""\xe5\x85\xb6\xe5\xae\x83"",u""\xe5\x85\xb6\xe5\xae\x9e"",u""\xe5\x85\xb6\xe6\xac\xa1"",u""\xe5\x85\xb7\xe4\xbd\x93"",u""\xe5\x85\xb7\xe4\xbd\x93\xe5\x9c\xb0\xe8\xaf\xb4"",u""\xe5\x85\xb7\xe4\xbd\x93\xe8\xaf\xb4\xe6\x9d\xa5"",\n            u""\xe5\x85\xb7\xe6\x9c\x89"",u""\xe5\x86\x8d\xe8\x80\x85"",u""\xe5\x86\x8d\xe8\xaf\xb4"",u""\xe5\x86\x92"",u""\xe5\x86\xb2"",u""\xe5\x86\xb3\xe5\xae\x9a"",u""\xe5\x86\xb5\xe4\xb8\x94"",u""\xe5\x87\x86\xe5\xa4\x87"",u""\xe5\x87\xa0"",u""\xe5\x87\xa0\xe4\xb9\x8e"",u""\xe5\x87\xa0\xe6\x97\xb6"",u""\xe5\x87\xad"",\n            u""\xe5\x87\xad\xe5\x80\x9f"",u""\xe5\x87\xba\xe5\x8e\xbb"",u""\xe5\x87\xba\xe6\x9d\xa5"",u""\xe5\x87\xba\xe7\x8e\xb0"",u""\xe5\x88\x86\xe5\x88\xab"",u""\xe5\x88\x99"",u""\xe5\x88\xab"",u""\xe5\x88\xab\xe7\x9a\x84"",u""\xe5\x88\xab\xe8\xaf\xb4"",u""\xe5\x88\xb0"",u""\xe5\x89\x8d\xe5\x90\x8e"",\n            u""\xe5\x89\x8d\xe8\x80\x85"",u""\xe5\x89\x8d\xe8\xbf\x9b"",u""\xe5\x89\x8d\xe9\x9d\xa2"",u""\xe5\x8a\xa0\xe4\xb9\x8b"",u""\xe5\x8a\xa0\xe4\xbb\xa5"",u""\xe5\x8a\xa0\xe5\x85\xa5"",u""\xe5\x8a\xa0\xe5\xbc\xba"",u""\xe5\x8d\x81\xe5\x88\x86"",u""\xe5\x8d\xb3"",u""\xe5\x8d\xb3\xe4\xbb\xa4"",u""\xe5\x8d\xb3\xe4\xbd\xbf"",\n            u""\xe5\x8d\xb3\xe4\xbe\xbf"",u""\xe5\x8d\xb3\xe6\x88\x96"",u""\xe5\x8d\xb3\xe8\x8b\xa5"",u""\xe5\x8d\xb4\xe4\xb8\x8d"",u""\xe5\x8e\x9f\xe6\x9d\xa5"",u""\xe5\x8f\x88"",u""\xe5\x8f\x8a"",u""\xe5\x8f\x8a\xe5\x85\xb6"",u""\xe5\x8f\x8a\xe6\x97\xb6"",u""\xe5\x8f\x8a\xe8\x87\xb3"",u""\xe5\x8f\x8c\xe6\x96\xb9"",\n            u""\xe5\x8f\x8d\xe4\xb9\x8b"",u""\xe5\x8f\x8d\xe5\xba\x94"",u""\xe5\x8f\x8d\xe6\x98\xa0"",u""\xe5\x8f\x8d\xe8\xbf\x87\xe6\x9d\xa5"",u""\xe5\x8f\x8d\xe8\xbf\x87\xe6\x9d\xa5\xe8\xaf\xb4"",u""\xe5\x8f\x96\xe5\xbe\x97"",u""\xe5\x8f\x97\xe5\x88\xb0"",u""\xe5\x8f\x98\xe6\x88\x90"",u""\xe5\x8f\xa6"",u""\xe5\x8f\xa6\xe4\xb8\x80\xe6\x96\xb9\xe9\x9d\xa2"",\n            u""\xe5\x8f\xa6\xe5\xa4\x96"",u""\xe5\x8f\xaa\xe6\x98\xaf"",u""\xe5\x8f\xaa\xe6\x9c\x89"",u""\xe5\x8f\xaa\xe8\xa6\x81"",u""\xe5\x8f\xaa\xe9\x99\x90"",u""\xe5\x8f\xab"",u""\xe5\x8f\xab\xe5\x81\x9a"",u""\xe5\x8f\xac\xe5\xbc\x80"",u""\xe5\x8f\xae\xe5\x92\x9a"",u""\xe5\x8f\xaf"",u""\xe5\x8f\xaf\xe4\xbb\xa5"",\n            u""\xe5\x8f\xaf\xe6\x98\xaf"",u""\xe5\x8f\xaf\xe8\x83\xbd"",u""\xe5\x8f\xaf\xe8\xa7\x81"",u""\xe5\x90\x84"",u""\xe5\x90\x84\xe4\xb8\xaa"",u""\xe5\x90\x84\xe4\xba\xba"",u""\xe5\x90\x84\xe4\xbd\x8d"",u""\xe5\x90\x84\xe5\x9c\xb0"",u""\xe5\x90\x84\xe7\xa7\x8d"",u""\xe5\x90\x84\xe7\xba\xa7"",u""\xe5\x90\x84\xe8\x87\xaa"",\n            u""\xe5\x90\x88\xe7\x90\x86"",u""\xe5\x90\x8c"",u""\xe5\x90\x8c\xe4\xb8\x80"",u""\xe5\x90\x8c\xe6\x97\xb6"",u""\xe5\x90\x8c\xe6\xa0\xb7"",u""\xe5\x90\x8e\xe6\x9d\xa5"",u""\xe5\x90\x8e\xe9\x9d\xa2"",u""\xe5\x90\x91"",u""\xe5\x90\x91\xe7\x9d\x80"",u""\xe5\x90\x93"",u""\xe5\x90\x97"",u""\xe5\x90\xa6\xe5\x88\x99"",\n            u""\xe5\x90\xa7"",u""\xe5\x90\xa7\xe5\x93\x92"",u""\xe5\x90\xb1"",u""\xe5\x91\x80"",u""\xe5\x91\x83"",u""\xe5\x91\x95"",u""\xe5\x91\x97"",u""\xe5\x91\x9c"",u""\xe5\x91\x9c\xe5\x91\xbc"",u""\xe5\x91\xa2"",u""\xe5\x91\xa8\xe5\x9b\xb4"",u""\xe5\x91\xb5"",u""\xe5\x91\xb8"",\n            u""\xe5\x91\xbc\xe5\x93\xa7"",u""\xe5\x92\x8b"",u""\xe5\x92\x8c"",u""\xe5\x92\x9a"",u""\xe5\x92\xa6"",u""\xe5\x92\xb1"",u""\xe5\x92\xb1\xe4\xbb\xac"",u""\xe5\x92\xb3"",u""\xe5\x93\x87"",u""\xe5\x93\x88"",u""\xe5\x93\x88\xe5\x93\x88"",u""\xe5\x93\x89"",u""\xe5\x93\x8e"",\n            u""\xe5\x93\x8e\xe5\x91\x80"",u""\xe5\x93\x8e\xe5\x93\x9f"",u""\xe5\x93\x97"",u""\xe5\x93\x9f"",u""\xe5\x93\xa6"",u""\xe5\x93\xa9"",u""\xe5\x93\xaa"",u""\xe5\x93\xaa\xe4\xb8\xaa"",u""\xe5\x93\xaa\xe4\xba\x9b"",u""\xe5\x93\xaa\xe5\x84\xbf"",u""\xe5\x93\xaa\xe5\xa4\xa9"",u""\xe5\x93\xaa\xe5\xb9\xb4"",\n            u""\xe5\x93\xaa\xe6\x80\x95"",u""\xe5\x93\xaa\xe6\xa0\xb7"",u""\xe5\x93\xaa\xe8\xbe\xb9"",u""\xe5\x93\xaa\xe9\x87\x8c"",u""\xe5\x93\xbc"",u""\xe5\x93\xbc\xe5\x94\xb7"",u""\xe5\x94\x89"",u""\xe5\x95\x8a"",u""\xe5\x95\x90"",u""\xe5\x95\xa5"",u""\xe5\x95\xa6"",u""\xe5\x95\xaa\xe8\xbe\xbe"",\n            u""\xe5\x96\x82"",u""\xe5\x96\x8f"",u""\xe5\x96\x94\xe5\x94\xb7"",u""\xe5\x97\xa1\xe5\x97\xa1"",u""\xe5\x97\xac"",u""\xe5\x97\xaf"",u""\xe5\x97\xb3"",u""\xe5\x98\x8e"",u""\xe5\x98\x8e\xe7\x99\xbb"",u""\xe5\x98\x98"",u""\xe5\x98\x9b"",u""\xe5\x98\xbb"",u""\xe5\x98\xbf"",\n            u""\xe5\x9b\xa0"",u""\xe5\x9b\xa0\xe4\xb8\xba"",u""\xe5\x9b\xa0\xe6\xad\xa4"",u""\xe5\x9b\xa0\xe8\x80\x8c"",u""\xe5\x9b\xba\xe7\x84\xb6"",u""\xe5\x9c\xa8"",u""\xe5\x9c\xa8\xe4\xb8\x8b"",u""\xe5\x9c\xb0"",u""\xe5\x9d\x9a\xe5\x86\xb3"",u""\xe5\x9d\x9a\xe6\x8c\x81"",u""\xe5\x9f\xba\xe6\x9c\xac"",\n            u""\xe5\xa4\x84\xe7\x90\x86"",u""\xe5\xa4\x8d\xe6\x9d\x82"",u""\xe5\xa4\x9a"",u""\xe5\xa4\x9a\xe5\xb0\x91"",u""\xe5\xa4\x9a\xe6\x95\xb0"",u""\xe5\xa4\x9a\xe6\xac\xa1"",u""\xe5\xa4\xa7\xe5\x8a\x9b"",u""\xe5\xa4\xa7\xe5\xa4\x9a\xe6\x95\xb0"",u""\xe5\xa4\xa7\xe5\xa4\xa7"",u""\xe5\xa4\xa7\xe5\xae\xb6"",u""\xe5\xa4\xa7\xe6\x89\xb9"",\n            u""\xe5\xa4\xa7\xe7\xba\xa6"",u""\xe5\xa4\xa7\xe9\x87\x8f"",u""\xe5\xa4\xb1\xe5\x8e\xbb"",u""\xe5\xa5\xb9"",u""\xe5\xa5\xb9\xe4\xbb\xac"",u""\xe5\xa5\xb9\xe7\x9a\x84"",u""\xe5\xa5\xbd\xe7\x9a\x84"",u""\xe5\xa5\xbd\xe8\xb1\xa1"",u""\xe5\xa6\x82"",u""\xe5\xa6\x82\xe4\xb8\x8a\xe6\x89\x80\xe8\xbf\xb0"",u""\xe5\xa6\x82\xe4\xb8\x8b"",\n            u""\xe5\xa6\x82\xe4\xbd\x95"",u""\xe5\xa6\x82\xe5\x85\xb6"",u""\xe5\xa6\x82\xe6\x9e\x9c"",u""\xe5\xa6\x82\xe6\xad\xa4"",u""\xe5\xa6\x82\xe8\x8b\xa5"",u""\xe5\xad\x98\xe5\x9c\xa8"",u""\xe5\xae\x81"",u""\xe5\xae\x81\xe5\x8f\xaf"",u""\xe5\xae\x81\xe6\x84\xbf"",u""\xe5\xae\x81\xe8\x82\xaf"",u""\xe5\xae\x83"",\n            u""\xe5\xae\x83\xe4\xbb\xac"",u""\xe5\xae\x83\xe4\xbb\xac\xe7\x9a\x84"",u""\xe5\xae\x83\xe7\x9a\x84"",u""\xe5\xae\x89\xe5\x85\xa8"",u""\xe5\xae\x8c\xe5\x85\xa8"",u""\xe5\xae\x8c\xe6\x88\x90"",u""\xe5\xae\x9e\xe7\x8e\xb0"",u""\xe5\xae\x9e\xe9\x99\x85"",u""\xe5\xae\xa3\xe5\xb8\x83"",u""\xe5\xae\xb9\xe6\x98\x93"",u""\xe5\xaf\x86\xe5\x88\x87"",\n            u""\xe5\xaf\xb9"",u""\xe5\xaf\xb9\xe4\xba\x8e"",u""\xe5\xaf\xb9\xe5\xba\x94"",u""\xe5\xb0\x86"",u""\xe5\xb0\x91\xe6\x95\xb0"",u""\xe5\xb0\x94\xe5\x90\x8e"",u""\xe5\xb0\x9a\xe4\xb8\x94"",u""\xe5\xb0\xa4\xe5\x85\xb6"",u""\xe5\xb0\xb1"",u""\xe5\xb0\xb1\xe6\x98\xaf"",u""\xe5\xb0\xb1\xe6\x98\xaf\xe8\xaf\xb4"",\n            u""\xe5\xb0\xbd"",u""\xe5\xb0\xbd\xe7\xae\xa1"",u""\xe5\xb1\x9e\xe4\xba\x8e"",u""\xe5\xb2\x82\xe4\xbd\x86"",u""\xe5\xb7\xa6\xe5\x8f\xb3"",u""\xe5\xb7\xa8\xe5\xa4\xa7"",u""\xe5\xb7\xa9\xe5\x9b\xba"",u""\xe5\xb7\xb1"",u""\xe5\xb7\xb2\xe7\xbb\x8f"",u""\xe5\xb8\xae\xe5\x8a\xa9"",u""\xe5\xb8\xb8\xe5\xb8\xb8"",\n            u""\xe5\xb9\xb6"",u""\xe5\xb9\xb6\xe4\xb8\x8d"",u""\xe5\xb9\xb6\xe4\xb8\x8d\xe6\x98\xaf"",u""\xe5\xb9\xb6\xe4\xb8\x94"",u""\xe5\xb9\xb6\xe6\xb2\xa1\xe6\x9c\x89"",u""\xe5\xb9\xbf\xe5\xa4\xa7"",u""\xe5\xb9\xbf\xe6\xb3\x9b"",u""\xe5\xba\x94\xe5\xbd\x93"",u""\xe5\xba\x94\xe7\x94\xa8"",u""\xe5\xba\x94\xe8\xaf\xa5"",u""\xe5\xbc\x80\xe5\xa4\x96"",\n            u""\xe5\xbc\x80\xe5\xa7\x8b"",u""\xe5\xbc\x80\xe5\xb1\x95"",u""\xe5\xbc\x95\xe8\xb5\xb7"",u""\xe5\xbc\xba\xe7\x83\x88"",u""\xe5\xbc\xba\xe8\xb0\x83"",u""\xe5\xbd\x92"",u""\xe5\xbd\x93"",u""\xe5\xbd\x93\xe5\x89\x8d"",u""\xe5\xbd\x93\xe6\x97\xb6"",u""\xe5\xbd\x93\xe7\x84\xb6"",u""\xe5\xbd\x93\xe7\x9d\x80"",\n            u""\xe5\xbd\xa2\xe6\x88\x90"",u""\xe5\xbd\xbb\xe5\xba\x95"",u""\xe5\xbd\xbc"",u""\xe5\xbd\xbc\xe6\xad\xa4"",u""\xe5\xbe\x80"",u""\xe5\xbe\x80\xe5\xbe\x80"",u""\xe5\xbe\x85"",u""\xe5\xbe\x8c\xe6\x9d\xa5"",u""\xe5\xbe\x8c\xe9\x9d\xa2"",u""\xe5\xbe\x97"",u""\xe5\xbe\x97\xe5\x87\xba"",u""\xe5\xbe\x97\xe5\x88\xb0"",\n            u""\xe5\xbf\x83\xe9\x87\x8c"",u""\xe5\xbf\x85\xe7\x84\xb6"",u""\xe5\xbf\x85\xe8\xa6\x81"",u""\xe5\xbf\x85\xe9\xa1\xbb"",u""\xe6\x80\x8e"",u""\xe6\x80\x8e\xe4\xb9\x88"",u""\xe6\x80\x8e\xe4\xb9\x88\xe5\x8a\x9e"",u""\xe6\x80\x8e\xe4\xb9\x88\xe6\xa0\xb7"",u""\xe6\x80\x8e\xe6\xa0\xb7"",u""\xe6\x80\x8e\xe9\xba\xbd"",u""\xe6\x80\xbb\xe4\xb9\x8b"",\n            u""\xe6\x80\xbb\xe6\x98\xaf"",u""\xe6\x80\xbb\xe7\x9a\x84\xe6\x9d\xa5\xe7\x9c\x8b"",u""\xe6\x80\xbb\xe7\x9a\x84\xe6\x9d\xa5\xe8\xaf\xb4"",u""\xe6\x80\xbb\xe7\x9a\x84\xe8\xaf\xb4\xe6\x9d\xa5"",u""\xe6\x80\xbb\xe7\xbb\x93"",u""\xe6\x80\xbb\xe8\x80\x8c\xe8\xa8\x80\xe4\xb9\x8b"",u""\xe6\x81\xb0\xe6\x81\xb0\xe7\x9b\xb8\xe5\x8f\x8d"",u""\xe6\x82\xa8"",u""\xe6\x84\x8f\xe6\x80\x9d"",\n            u""\xe6\x84\xbf\xe6\x84\x8f"",u""\xe6\x85\xa2\xe8\xaf\xb4"",u""\xe6\x88\x90\xe4\xb8\xba"",u""\xe6\x88\x91"",u""\xe6\x88\x91\xe4\xbb\xac"",u""\xe6\x88\x91\xe7\x9a\x84"",u""\xe6\x88\x96"",u""\xe6\x88\x96\xe6\x98\xaf"",u""\xe6\x88\x96\xe8\x80\x85"",u""\xe6\x88\x98\xe6\x96\x97"",u""\xe6\x89\x80"",\n            u""\xe6\x89\x80\xe4\xbb\xa5"",u""\xe6\x89\x80\xe6\x9c\x89"",u""\xe6\x89\x80\xe8\xb0\x93"",u""\xe6\x89\x93"",u""\xe6\x89\xa9\xe5\xa4\xa7"",u""\xe6\x8a\x8a"",u""\xe6\x8a\x91\xe6\x88\x96"",u""\xe6\x8b\xbf"",u""\xe6\x8c\x89"",u""\xe6\x8c\x89\xe7\x85\xa7"",u""\xe6\x8d\xa2\xe5\x8f\xa5\xe8\xaf\x9d\xe8\xaf\xb4"",\n            u""\xe6\x8d\xa2\xe8\xa8\x80\xe4\xb9\x8b"",u""\xe6\x8d\xae"",u""\xe6\x8e\x8c\xe6\x8f\xa1"",u""\xe6\x8e\xa5\xe7\x9d\x80"",u""\xe6\x8e\xa5\xe8\x91\x97"",u""\xe6\x95\x85"",u""\xe6\x95\x85\xe6\xad\xa4"",u""\xe6\x95\xb4\xe4\xb8\xaa"",u""\xe6\x96\xb9\xe4\xbe\xbf"",u""\xe6\x96\xb9\xe9\x9d\xa2"",u""\xe6\x97\x81\xe4\xba\xba"",\n            u""\xe6\x97\xa0\xe5\xae\x81"",u""\xe6\x97\xa0\xe6\xb3\x95"",u""\xe6\x97\xa0\xe8\xae\xba"",u""\xe6\x97\xa2"",u""\xe6\x97\xa2\xe6\x98\xaf"",u""\xe6\x97\xa2\xe7\x84\xb6"",u""\xe6\x97\xb6\xe5\x80\x99"",u""\xe6\x98\x8e\xe6\x98\xbe"",u""\xe6\x98\x8e\xe7\xa1\xae"", u""\xe6\x98\xaf"",u""\xe6\x98\xaf\xe5\x90\xa6"",\n            u""\xe6\x98\xaf\xe7\x9a\x84"",u""\xe6\x98\xbe\xe7\x84\xb6"",u""\xe6\x98\xbe\xe8\x91\x97"",u""\xe6\x99\xae\xe9\x80\x9a"",u""\xe6\x99\xae\xe9\x81\x8d"",u""\xe6\x9b\xb4\xe5\x8a\xa0"",u""\xe6\x9b\xbe\xe7\xbb\x8f"",u""\xe6\x9b\xbf"",u""\xe6\x9c\x80\xe5\x90\x8e"",u""\xe6\x9c\x80\xe5\xa4\xa7"",u""\xe6\x9c\x80\xe5\xa5\xbd"",\n            u""\xe6\x9c\x80\xe5\xbe\x8c"",u""\xe6\x9c\x80\xe8\xbf\x91"",u""\xe6\x9c\x80\xe9\xab\x98"",u""\xe6\x9c\x89"",u""\xe6\x9c\x89\xe4\xba\x9b"",u""\xe6\x9c\x89\xe5\x85\xb3"",u""\xe6\x9c\x89\xe5\x88\xa9"",u""\xe6\x9c\x89\xe5\x8a\x9b"",u""\xe6\x9c\x89\xe6\x89\x80"",u""\xe6\x9c\x89\xe6\x95\x88"",u""\xe6\x9c\x89\xe6\x97\xb6"",\n            u""\xe6\x9c\x89\xe7\x82\xb9"",u""\xe6\x9c\x89\xe7\x9a\x84"",u""\xe6\x9c\x89\xe7\x9d\x80"",u""\xe6\x9c\x89\xe8\x91\x97"",u""\xe6\x9c\x9b"",u""\xe6\x9c\x9d"",u""\xe6\x9c\x9d\xe7\x9d\x80"",u""\xe6\x9c\xac"",u""\xe6\x9c\xac\xe7\x9d\x80"",u""\xe6\x9d\xa5"",u""\xe6\x9d\xa5\xe7\x9d\x80"",u""\xe6\x9e\x81\xe4\xba\x86"",\n            u""\xe6\x9e\x84\xe6\x88\x90"",u""\xe6\x9e\x9c\xe7\x84\xb6"",u""\xe6\x9e\x9c\xe7\x9c\x9f"",u""\xe6\x9f\x90"",u""\xe6\x9f\x90\xe4\xb8\xaa"",u""\xe6\x9f\x90\xe4\xba\x9b"",u""\xe6\xa0\xb9\xe6\x8d\xae"",u""\xe6\xa0\xb9\xe6\x9c\xac"",u""\xe6\xac\xa2\xe8\xbf\x8e"",u""\xe6\xad\xa3\xe5\x9c\xa8"",u""\xe6\xad\xa3\xe5\xa6\x82"",\n            u""\xe6\xad\xa3\xe5\xb8\xb8"",u""\xe6\xad\xa4"",u""\xe6\xad\xa4\xe5\xa4\x96"",u""\xe6\xad\xa4\xe6\x97\xb6"",u""\xe6\xad\xa4\xe9\x97\xb4"",u""\xe6\xaf\x8b\xe5\xae\x81"",u""\xe6\xaf\x8f"",u""\xe6\xaf\x8f\xe4\xb8\xaa"",u""\xe6\xaf\x8f\xe5\xa4\xa9"",u""\xe6\xaf\x8f\xe5\xb9\xb4"",u""\xe6\xaf\x8f\xe5\xbd\x93"",\n            u""\xe6\xaf\x94"",u""\xe6\xaf\x94\xe5\xa6\x82"",u""\xe6\xaf\x94\xe6\x96\xb9"",u""\xe6\xaf\x94\xe8\xbe\x83"",u""\xe6\xaf\xab\xe4\xb8\x8d"",u""\xe6\xb2\xa1\xe6\x9c\x89"",u""\xe6\xb2\xbf"",u""\xe6\xb2\xbf\xe7\x9d\x80"",u""\xe6\xb3\xa8\xe6\x84\x8f"",u""\xe6\xb7\xb1\xe5\x85\xa5"",u""\xe6\xb8\x85\xe6\xa5\x9a"",\n            u""\xe6\xbb\xa1\xe8\xb6\xb3"",u""\xe6\xbc\xab\xe8\xaf\xb4"",u""\xe7\x84\x89"",u""\xe7\x84\xb6\xe5\x88\x99"",u""\xe7\x84\xb6\xe5\x90\x8e"",u""\xe7\x84\xb6\xe5\xbe\x8c"",u""\xe7\x84\xb6\xe8\x80\x8c"",u""\xe7\x85\xa7"",u""\xe7\x85\xa7\xe7\x9d\x80"",u""\xe7\x89\xb9\xe5\x88\xab\xe6\x98\xaf"",u""\xe7\x89\xb9\xe6\xae\x8a"",\n            u""\xe7\x89\xb9\xe7\x82\xb9"",u""\xe7\x8e\xb0\xe4\xbb\xa3"",u""\xe7\x8e\xb0\xe5\x9c\xa8"",u""\xe7\x94\x9a\xe4\xb9\x88"",u""\xe7\x94\x9a\xe8\x80\x8c"",u""\xe7\x94\x9a\xe8\x87\xb3"",u""\xe7\x94\xa8"",u""\xe7\x94\xb1"",u""\xe7\x94\xb1\xe4\xba\x8e"",u""\xe7\x94\xb1\xe6\xad\xa4\xe5\x8f\xaf\xe8\xa7\x81"",u""\xe7\x9a\x84"",\n            u""\xe7\x9a\x84\xe8\xaf\x9d"",u""\xe7\x9b\xae\xe5\x89\x8d"",u""\xe7\x9b\xb4\xe5\x88\xb0"",u""\xe7\x9b\xb4\xe6\x8e\xa5"",u""\xe7\x9b\xb8\xe4\xbc\xbc"",u""\xe7\x9b\xb8\xe4\xbf\xa1"",u""\xe7\x9b\xb8\xe5\x8f\x8d"",u""\xe7\x9b\xb8\xe5\x90\x8c"",u""\xe7\x9b\xb8\xe5\xaf\xb9"",u""\xe7\x9b\xb8\xe5\xaf\xb9\xe8\x80\x8c\xe8\xa8\x80"",u""\xe7\x9b\xb8\xe5\xba\x94"",\n            u""\xe7\x9b\xb8\xe5\xbd\x93"",u""\xe7\x9b\xb8\xe7\xad\x89"",u""\xe7\x9c\x81\xe5\xbe\x97"",u""\xe7\x9c\x8b\xe5\x87\xba"",u""\xe7\x9c\x8b\xe5\x88\xb0"",u""\xe7\x9c\x8b\xe6\x9d\xa5"",u""\xe7\x9c\x8b\xe7\x9c\x8b"",u""\xe7\x9c\x8b\xe8\xa7\x81"",u""\xe7\x9c\x9f\xe6\x98\xaf"",u""\xe7\x9c\x9f\xe6\xad\xa3"",u""\xe7\x9d\x80"",\n            u""\xe7\x9d\x80\xe5\x91\xa2"",u""\xe7\x9f\xa3"",u""\xe7\x9f\xa5\xe9\x81\x93"",u""\xe7\xa1\xae\xe5\xae\x9a"",u""\xe7\xa6\xbb"",u""\xe7\xa7\xaf\xe6\x9e\x81"",u""\xe7\xa7\xbb\xe5\x8a\xa8"",u""\xe7\xaa\x81\xe5\x87\xba"",u""\xe7\xaa\x81\xe7\x84\xb6"",u""\xe7\xab\x8b\xe5\x8d\xb3"",u""\xe7\xac\xac"",u""\xe7\xad\x89"",\n            u""\xe7\xad\x89\xe7\xad\x89"",u""\xe7\xae\xa1"",u""\xe7\xb4\xa7\xe6\x8e\xa5\xe7\x9d\x80"",u""\xe7\xba\xb5"",u""\xe7\xba\xb5\xe4\xbb\xa4"",u""\xe7\xba\xb5\xe4\xbd\xbf"",u""\xe7\xba\xb5\xe7\x84\xb6"",u""\xe7\xbb\x83\xe4\xb9\xa0"",u""\xe7\xbb\x84\xe6\x88\x90"",u""\xe7\xbb\x8f"",u""\xe7\xbb\x8f\xe5\xb8\xb8"",\n            u""\xe7\xbb\x8f\xe8\xbf\x87"",u""\xe7\xbb\x93\xe5\x90\x88"",u""\xe7\xbb\x93\xe6\x9e\x9c"",u""\xe7\xbb\x99"",u""\xe7\xbb\x9d\xe5\xaf\xb9"",u""\xe7\xbb\xa7\xe7\xbb\xad"",u""\xe7\xbb\xa7\xe8\x80\x8c"",u""\xe7\xbb\xb4\xe6\x8c\x81"",u""\xe7\xbb\xbc\xe4\xb8\x8a\xe6\x89\x80\xe8\xbf\xb0"",u""\xe7\xbd\xa2\xe4\xba\x86"",u""\xe8\x80\x83\xe8\x99\x91"",\n            u""\xe8\x80\x85"",u""\xe8\x80\x8c"",u""\xe8\x80\x8c\xe4\xb8\x94"",u""\xe8\x80\x8c\xe5\x86\xb5"",u""\xe8\x80\x8c\xe5\xa4\x96"",u""\xe8\x80\x8c\xe5\xb7\xb2"",u""\xe8\x80\x8c\xe6\x98\xaf"",u""\xe8\x80\x8c\xe8\xa8\x80"",u""\xe8\x81\x94\xe7\xb3\xbb"",u""\xe8\x83\xbd"",u""\xe8\x83\xbd\xe5\x90\xa6"",\n            u""\xe8\x83\xbd\xe5\xa4\x9f"",u""\xe8\x85\xbe"",u""\xe8\x87\xaa"",u""\xe8\x87\xaa\xe4\xb8\xaa\xe5\x84\xbf"",u""\xe8\x87\xaa\xe4\xbb\x8e"",u""\xe8\x87\xaa\xe5\x90\x84\xe5\x84\xbf"",u""\xe8\x87\xaa\xe5\xae\xb6"",u""\xe8\x87\xaa\xe5\xb7\xb1"",u""\xe8\x87\xaa\xe8\xba\xab"",u""\xe8\x87\xb3"",u""\xe8\x87\xb3\xe4\xba\x8e"",\n            u""\xe8\x89\xaf\xe5\xa5\xbd"",u""\xe8\x8b\xa5"",u""\xe8\x8b\xa5\xe6\x98\xaf"",u""\xe8\x8b\xa5\xe9\x9d\x9e"",u""\xe8\x8c\x83\xe5\x9b\xb4"",u""\xe8\x8e\xab\xe8\x8b\xa5"",u""\xe8\x8e\xb7\xe5\xbe\x97"",u""\xe8\x99\xbd"",u""\xe8\x99\xbd\xe5\x88\x99"",u""\xe8\x99\xbd\xe7\x84\xb6"",u""\xe8\x99\xbd\xe8\xaf\xb4"",\n            u""\xe8\xa1\x8c\xe4\xb8\xba"",u""\xe8\xa1\x8c\xe5\x8a\xa8"",u""\xe8\xa1\xa8\xe6\x98\x8e"",u""\xe8\xa1\xa8\xe7\xa4\xba"",u""\xe8\xa2\xab"",u""\xe8\xa6\x81"",u""\xe8\xa6\x81\xe4\xb8\x8d"",u""\xe8\xa6\x81\xe4\xb8\x8d\xe6\x98\xaf"",u""\xe8\xa6\x81\xe4\xb8\x8d\xe7\x84\xb6"",u""\xe8\xa6\x81\xe4\xb9\x88"",u""\xe8\xa6\x81\xe6\x98\xaf"",\n            u""\xe8\xa6\x81\xe6\xb1\x82"",u""\xe8\xa7\x84\xe5\xae\x9a"",u""\xe8\xa7\x89\xe5\xbe\x97"",u""\xe8\xae\xa4\xe4\xb8\xba"",u""\xe8\xae\xa4\xe7\x9c\x9f"",u""\xe8\xae\xa4\xe8\xaf\x86"",u""\xe8\xae\xa9"",u""\xe8\xae\xb8\xe5\xa4\x9a"",u""\xe8\xae\xba"",u""\xe8\xae\xbe\xe4\xbd\xbf"",u""\xe8\xae\xbe\xe8\x8b\xa5"",\n            u""\xe8\xaf\xa5"",u""\xe8\xaf\xb4\xe6\x98\x8e"",u""\xe8\xaf\xb8\xe4\xbd\x8d"",u""\xe8\xb0\x81"",u""\xe8\xb0\x81\xe7\x9f\xa5"",u""\xe8\xb5\xb6"",u""\xe8\xb5\xb7"",u""\xe8\xb5\xb7\xe6\x9d\xa5"",u""\xe8\xb5\xb7\xe8\xa7\x81"",u""\xe8\xb6\x81"",u""\xe8\xb6\x81\xe7\x9d\x80"",u""\xe8\xb6\x8a\xe6\x98\xaf"",\n            u""\xe8\xb7\x9f"",u""\xe8\xbd\xac\xe5\x8a\xa8"",u""\xe8\xbd\xac\xe5\x8f\x98"",u""\xe8\xbd\xac\xe8\xb4\xb4"",u""\xe8\xbe\x83"",u""\xe8\xbe\x83\xe4\xb9\x8b"",u""\xe8\xbe\xb9"",u""\xe8\xbe\xbe\xe5\x88\xb0"",u""\xe8\xbf\x85\xe9\x80\x9f"",u""\xe8\xbf\x87"",u""\xe8\xbf\x87\xe5\x8e\xbb"",u""\xe8\xbf\x87\xe6\x9d\xa5"",\n            u""\xe8\xbf\x90\xe7\x94\xa8"",u""\xe8\xbf\x98\xe6\x98\xaf"",u""\xe8\xbf\x98\xe6\x9c\x89"",u""\xe8\xbf\x99"",u""\xe8\xbf\x99\xe4\xb8\xaa"",u""\xe8\xbf\x99\xe4\xb9\x88"",u""\xe8\xbf\x99\xe4\xb9\x88\xe4\xba\x9b"",u""\xe8\xbf\x99\xe4\xb9\x88\xe6\xa0\xb7"",u""\xe8\xbf\x99\xe4\xb9\x88\xe7\x82\xb9\xe5\x84\xbf"",u""\xe8\xbf\x99\xe4\xba\x9b"",\n            u""\xe8\xbf\x99\xe4\xbc\x9a\xe5\x84\xbf"",u""\xe8\xbf\x99\xe5\x84\xbf"",u""\xe8\xbf\x99\xe5\xb0\xb1\xe6\x98\xaf\xe8\xaf\xb4"",u""\xe8\xbf\x99\xe6\x97\xb6"",u""\xe8\xbf\x99\xe6\xa0\xb7"",u""\xe8\xbf\x99\xe7\x82\xb9"",u""\xe8\xbf\x99\xe7\xa7\x8d"",u""\xe8\xbf\x99\xe8\xbe\xb9"",u""\xe8\xbf\x99\xe9\x87\x8c"",u""\xe8\xbf\x99\xe9\xba\xbd"",\n            u""\xe8\xbf\x9b\xe5\x85\xa5"",u""\xe8\xbf\x9b\xe6\xad\xa5"",u""\xe8\xbf\x9b\xe8\x80\x8c"",u""\xe8\xbf\x9b\xe8\xa1\x8c"",u""\xe8\xbf\x9e"",u""\xe8\xbf\x9e\xe5\x90\x8c"",u""\xe9\x80\x82\xe5\xba\x94"",u""\xe9\x80\x82\xe5\xbd\x93"",u""\xe9\x80\x82\xe7\x94\xa8"",u""\xe9\x80\x90\xe6\xad\xa5"",u""\xe9\x80\x90\xe6\xb8\x90"",\n            u""\xe9\x80\x9a\xe5\xb8\xb8"",u""\xe9\x80\x9a\xe8\xbf\x87"",u""\xe9\x80\xa0\xe6\x88\x90"",u""\xe9\x81\x87\xe5\x88\xb0"",u""\xe9\x81\xad\xe5\x88\xb0"",u""\xe9\x81\xbf\xe5\x85\x8d"",u""\xe9\x82\xa3"",u""\xe9\x82\xa3\xe4\xb8\xaa"",u""\xe9\x82\xa3\xe4\xb9\x88"",u""\xe9\x82\xa3\xe4\xb9\x88\xe4\xba\x9b"",u""\xe9\x82\xa3\xe4\xb9\x88\xe6\xa0\xb7"",\n            u""\xe9\x82\xa3\xe4\xba\x9b"",u""\xe9\x82\xa3\xe4\xbc\x9a\xe5\x84\xbf"",u""\xe9\x82\xa3\xe5\x84\xbf"",u""\xe9\x82\xa3\xe6\x97\xb6"",u""\xe9\x82\xa3\xe6\xa0\xb7"",u""\xe9\x82\xa3\xe8\xbe\xb9"",u""\xe9\x82\xa3\xe9\x87\x8c"",u""\xe9\x82\xa3\xe9\xba\xbd"",u""\xe9\x83\xa8\xe5\x88\x86"",u""\xe9\x84\x99\xe4\xba\xba"",u""\xe9\x87\x87\xe5\x8f\x96"",\n            u""\xe9\x87\x8c\xe9\x9d\xa2"",u""\xe9\x87\x8d\xe5\xa4\xa7"",u""\xe9\x87\x8d\xe6\x96\xb0"",u""\xe9\x87\x8d\xe8\xa6\x81"",u""\xe9\x89\xb4\xe4\xba\x8e"",u""\xe9\x97\xae\xe9\xa2\x98"",u""\xe9\x98\xb2\xe6\xad\xa2"",u""\xe9\x98\xbf"",u""\xe9\x99\x84\xe8\xbf\x91"",u""\xe9\x99\x90\xe5\x88\xb6"",u""\xe9\x99\xa4"",\n            u""\xe9\x99\xa4\xe4\xba\x86"",u""\xe9\x99\xa4\xe6\xad\xa4\xe4\xb9\x8b\xe5\xa4\x96"",u""\xe9\x99\xa4\xe9\x9d\x9e"",u""\xe9\x9a\x8f"",u""\xe9\x9a\x8f\xe7\x9d\x80"",u""\xe9\x9a\x8f\xe8\x91\x97"",u""\xe9\x9b\x86\xe4\xb8\xad"",u""\xe9\x9c\x80\xe8\xa6\x81"",u""\xe9\x9d\x9e\xe4\xbd\x86"",u""\xe9\x9d\x9e\xe5\xb8\xb8"",u""\xe9\x9d\x9e\xe5\xbe\x92"",\n            u""\xe9\x9d\xa0"",u""\xe9\xa1\xba"",u""\xe9\xa1\xba\xe7\x9d\x80"",u""\xe9\xa6\x96\xe5\x85\x88"",u""\xe9\xab\x98\xe5\x85\xb4"",u""\xe6\x98\xaf\xe4\xb8\x8d\xe6\x98\xaf"",u""\xe8\xaf\xb4\xe8\xaf\xb4"",u""\xef\xbc\x8c"",u""\xe3\x80\x82"",u""\xe3\x80\x8a"",u""\xe3\x80\x8b"",u""\xef\xbc\x9f"",u""\xe3\x80\x8e"",u""\xef\xbc\x81"","","",""."",""!"",""?"",u""\xe3\x80\x81"",""\\"""",""\\"""",\n            u""\xe6\x9c\x88"",u""\xe6\x97\xa5"",u""\xe5\xb9\xb4"",""\xe2\x80\x9c"",""\xe2\x80\xa6"",""\xe2\x80\x9d"",u""\xe3\x80\x91"",u""\xe3\x80\x90"",u""\xef\xbc\x88"",u""\xef\xbc\x89""\n        ]\n        jieba.load_userdict(WORD_DICT)\n\n    def del_stopwords(self, do=True):\n        """"""\n        delete the stopwords\n        """"""\n        if do:\n            for word in self.cut_text:\n                # print word, word in self.stop_words\n                if word not in self.stop_words:\n                    self.tokens.append(word)\n        else:\n            for word in self.cut_text:\n                self.tokens.append(word)\n    \n    def is_digit(self, word):\n        value = re.compile(r""^[-+]?[0-9]+[\\.0-9]*$"")\n        result = value.match(word)\n        if result:\n            return False\n        else:\n            return True\n\n    def del_digit(self):\n        self.tokens = filter(self.is_digit, self.tokens)\n    \n    def is_alpha(self, word):\n        result = all(ord(c) < 128 for c in word)\n        return not result\n\n    def del_alpha(self):\n        self.tokens = filter(self.is_alpha, self.tokens)\n\n    def cut(self, origin_text):\n        """"""\n        text : String\n        return: generator\n        """"""\n        cut_text = jieba.cut(origin_text)\n        self.cut_text = cut_text\n\n    def run(self, origin_text):\n        """"""\n        origin_text: String\n        return: a list of tokens\n        """"""\n        self.tokens = []\n        self.cut(origin_text)\n        self.del_stopwords()\n        self.del_digit()\n        self.del_alpha()\n        return self.tokens\n\n\nif __name__ == ""__main__"":\n    cut_doc_obj = cutDoc()\n    DATA_DIR = ""../../data/origin_data""\n    DATA_DIR = os.path.abspath(DATA_DIR)\n    data_path = os.path.join(DATA_DIR, ""all.csv"")\n    print data_path\n    fwrite = open(data_path.replace(""all.csv"",""all_token.csv""), \'w\')\n    with open(data_path, ""r"") as fread:\n        i = 0\n        # while True:\n        for line in fread.readlines():\n            try:\n                line_list = line.strip().split(""\\t"")\n                print len(line_list)\n                label = line_list[0]\n                text = line_list[1]\n                # print len(text)\n                text_tokens = cut_doc_obj.run(text)\n                # print text_tokens\n                fwrite.write(\' \'.join(text_tokens))\n                print ""processing {0}th line"".format(i)\n                i+=1\n            except BaseException as e:\n                msg = traceback.format_exc()\n                print msg\n                print ""=====>Read Done<======""\n                break\n    fwrite.close()'"
nlp/text_classifier/scripts/dataset_helpers/doc_dataset.py,0,"b'# -*- coding:utf-8 -*-\nimport sys\nsys.path.append(""../"")\nfrom config import *\nimport os\nimport logging\nimport operator\nlogger = logging.getLogger(__name__)\nhandler = logging.StreamHandler()\nformatter = logging.Formatter(\n    \'%(asctime)s %(name)-12s %(levelname)-8s %(message)s\')\nhandler.setFormatter(formatter)\nlogger.addHandler(handler)\nlogger.setLevel(logging.DEBUG)\n\n\ndef prepare_dataset():\n    dataset_info = {}\n    files = os.listdir(os.path.join(DATA_DIR.replace(\'origin_data\', \'title\')))\n    with open(all_text_filename, \'w\') as fwrite:\n        for file in files:\n            i = 0\n            file_path = os.path.join(\n                DATA_DIR.replace(\'origin_data\', \'title\'), file)\n            logger.info(""Process file {0}"".format(file_path))\n            with open(file_path, \'r\') as fread:\n                for line in fread.readlines():\n                    i += 1\n                    line_list = line.split(""|"")\n                    # print line\n                    if len(line_list) >= 3:\n                        doc_text = line.split(""|"")[2]\n                        w_line = str(filename_label[int(\n                            file)]) + ""\\t"" + doc_text\n                        fwrite.write(w_line)\n                dataset_info[file] = i\n    print dataset_info\n    sorted_dataset_info = sorted(\n        dataset_info.items(), key=operator.itemgetter(1), reverse=True)\n    print sorted_dataset_info\n\n\ndef prepare_title_dataset():\n    files = os.listdir(DATA_DIR.replace(\'origin_data\', \'title\'))\n    with open(all_title_filename, \'w\') as fwrite:\n        for file in files:\n            i = 0\n            file_path = os.path.join(\n                DATA_DIR.replace(\'origin_data\', \'title\'), file)\n            logger.info(""Process file {0}"".format(file_path))\n            with open(file_path, \'r\') as fread:\n                for line in fread.readlines():\n                    i += 1\n                    line_list = line.split(""|"")\n                    if len(line_list) >= 3:\n                        doc_title = line.split(""|"")[1]\n                        w_line = str(filename_label[int(\n                            file)]) + ""\\t"" + doc_title + \'\\n\'\n                        fwrite.write(w_line)\n                    # data\n\n\nif __name__ == ""__main__"":\n    prepare_dataset()\n    # prepare_title_dataset()\n'"
nlp/text_classifier/scripts/dataset_helpers/gen_w2v.py,0,"b'#-*-coding:utf-8-*-\nfrom gensim.models import word2vec\n# from config import *\nimport logging\nlogging.basicConfig(format=\'%(asctime)s : %(levelname)s : %(message)s\', level=logging.INFO)\nsentence = word2vec.LineSentence(\n    \'/Users/burness/git_repository/dl_opensource/tensorflow-101/nlp/text_classifier/data/origin_data/all_token.csv\'\n)\nmodel = word2vec.Word2Vec(sentences=sentence, size=50, workers=4, min_count=5)\n# model.most_similar()\nnews_w2v = \'/Users/burness/git_repository/dl_opensource/tensorflow-101/nlp/text_classifier/data/origin_data/news_w2v.model\'\nmodel.save(news_w2v)\n# model.save\n# model.wv.similar_by_word(u""\xe4\xb9\xa0\xe8\xbf\x91\xe5\xb9\xb3"", topn=10)'"
