file_path,api_count,code
src/action_recognition.py,4,"b'#!/usr/bin/env python\n""""""\nA ROS node to get action recognition results by using i3d model in tensorflow\nhub. All the code is based on example colab provided by tensorflow team. Here\nyou can find the link to notebook:\nhttps://colab.research.google.com/github/tensorflow/hub/blob/master/examples/colab/action_recognition_with_tf_hub.ipynb\n\nAuthor:\n    Cagatay Odabasi -- cagatay.odabasi@ipa.fraunhofer.de\n""""""\n\nimport rospy\n\nimport numpy as np\n\nfrom cob_perception_msgs.msg import ActionRecognitionmsg\n\nimport cv2\n\nfrom cv_bridge import CvBridge\n\nfrom sensor_msgs.msg import Image\n\nimport rospkg\n\nfrom six.moves import urllib\n\nimport tensorflow as tf\n\nimport tensorflow_hub as hub\n\n# Get the package directory\nrospack = rospkg.RosPack()\n\ncd = rospack.get_path(\'cob_people_object_detection_tensorflow\')\n\n\nclass ActionRecognitionNode(object):\n    """"""A ROS node to get action recognition prediction by using models in\n    tensorflow hub\n\n    """"""\n    def __init__(self):\n        super(ActionRecognitionNode, self).__init__()\n\n        # init the node\n        rospy.init_node(\'action_recognition_node\', anonymous=False)\n\n        # Get the parameters\n        (image_topic, output_topic, recognition_interval,\n            buffer_size) \\\n            = self.get_parameters()\n\n        self._bridge = CvBridge()\n\n        # Advertise the result of Action Recognition\n        self.pub_rec = rospy.Publisher(output_topic, \\\n            ActionRecognitionmsg, queue_size=1)\n\n        # This amount of break will be given between each prediction\n        self.recognition_interval = recognition_interval\n\n        # Counter for number of frames\n        self.frame_count = 0\n\n        # frame buffer size\n        self.buffer_size = buffer_size\n\n        # Frame buffer\n        self.frame_buffer = []\n\n        # Get the labels\n        self.labels = self.initialize_database()\n\n        # Create the graph object\n        self._graph = tf.Graph()\n\n        # Initialite the model\n        self.initialize_model()\n\n        self.sub_image =rospy.Subscriber(image_topic, Image, self.rgb_callback)\n\n        # spin\n        rospy.spin()\n\n    def get_parameters(self):\n        """"""\n        Gets the necessary parameters from parameter server\n\n        Args:\n\n        Returns:\n        (tuple) (camera_topic,  output_topic, recognition_interval,\n        buffer_size)\n\n        """"""\n\n        camera_topic = rospy.get_param(""~camera_topic"")\n        output_topic = rospy.get_param(""~output_topic"")\n        recognition_interval = rospy.get_param(""~recognition_interval"")\n        buffer_size = rospy.get_param(""~buffer_size"")\n\n        return (camera_topic, output_topic,\n            recognition_interval, buffer_size)\n\n\n    def shutdown(self):\n        """"""\n        Shuts down the node\n        """"""\n        rospy.signal_shutdown(""See ya!"")\n\n    def rgb_callback(self, image):\n        """"""\n        Callback for RGB images\n\n        Args:\n        image (sensor_msgs/Image): RGB image from camera\n\n        """"""\n\n        cv_rgb = self._bridge.imgmsg_to_cv2(image, ""passthrough"")[:, :, ::-1]\n\n        cv_rgb = cv2.resize(cv_rgb, (224, 224))\n\n        cv_rgb = cv_rgb.astype(np.uint8)\n\n        cv_rgb = cv_rgb[:, :, [2, 1, 0]]\n\n        cv_rgb = cv_rgb/255\n\n        # Add new frame to buffer\n        self.frame_buffer.append(cv_rgb)\n\n        if self.frame_count > self.buffer_size:\n            self.frame_buffer.pop(0)\n\n            if self.frame_count%self.recognition_interval == 0:\n                print(self.frame_count)\n                print(self.recognition_interval)\n                # Perform action recognition\n                probs = self.recognize()\n\n                self.publish(image, probs)\n\n        self.frame_count += 1\n\n\n    def recognize(self):\n        """"""\n        Recognize the actions performed in the scene\n\n        Args:\n        (numpy.ndarray) image: incoming image\n\n        Returns:\n\n        (numpy.array) probabilities returned by i3d model\n\n        """"""\n\n        frames = np.asarray(self.frame_buffer)\n\n        print(frames.shape)\n\n        model_input = np.expand_dims(frames, axis=0)\n\n        # Perform the inference\n        with self._graph.as_default():\n            with tf.train.MonitoredSession() as session:\n                [ps] = session.run(self.probabilities,\n                       feed_dict={self.input_placeholder: model_input})\n\n\n        print(""Top 5 actions:"")\n        for i in np.argsort(ps)[::-1][:5]:\n            print(""%-22s %.2f%%"" % (self.labels[i], ps[i] * 100))\n\n        return ps\n\n    def publish(self, image, probs):\n        """"""\n        Creates the ros messages and publishes them\n\n        Args:\n        detections (cob_perception_msgs/DetectionArray): incoming detections\n        """"""\n\n        msg = ActionRecognitionmsg()\n\n        msg.header = image.header\n\n        msg.probabilities = probs.tolist()\n\n        msg.labels = self.labels\n\n        self.pub_rec.publish(msg)\n\n    def initialize_database(self):\n        """"""\n        Reads the action labels from the text file\n\n        The names of the image files are considered as their\n\n        Returns:\n        (list) labels\n\n        """"""\n\n        labels = None\n\n        with open(cd + ""/action_recognition/labels.txt"", ""rb"") as f:\n            labels = [line.decode(""utf-8"").strip() for line in f.readlines()]\n\n        return labels\n\n    def initialize_model(self):\n        """"""\n        Downloads the model from tensorflow hub and create the model object for\n        inference\n\n        """"""\n        with self._graph.as_default():\n            self._module = hub.Module(""http://tfhub.dev/deepmind/i3d-kinetics-600/1"")\n            self.input_placeholder = tf.placeholder(shape=(None, None, 224, 224, 3), dtype=tf.float32)\n            logits = self._module(self.input_placeholder)\n            self.probabilities = tf.nn.softmax(logits)\n\ndef main():\n    """""" main function\n    """"""\n    node = ActionRecognitionNode()\n\nif __name__ == \'__main__\':\n    main()\n'"
src/cob_people_object_detection_tensorflow.py,0,"b'#!/usr/bin/env python\n""""""\nA ROS node to detect objects via TensorFlow Object Detection API.\n\nAuthor:\n    Cagatay Odabasi -- cagatay.odabasi@ipa.fraunhofer.de\n""""""\n\n# ROS\nimport rospy\n\nimport cv2\n\nfrom cob_perception_msgs.msg import Detection, DetectionArray, Rect\n\nfrom sensor_msgs.msg import Image\n\nfrom cv_bridge import CvBridge, CvBridgeError\n\nfrom cob_people_object_detection_tensorflow.detector import Detector\n\nfrom cob_people_object_detection_tensorflow import utils\n\nclass PeopleObjectDetectionNode(object):\n    """"""docstring for PeopleObjectDetectionNode.""""""\n    def __init__(self):\n        super(PeopleObjectDetectionNode, self).__init__()\n\n        # init the node\n        rospy.init_node(\'people_object_detection\', anonymous=False)\n\n        # Get the parameters\n        (model_name, num_of_classes, label_file, camera_namespace, video_name,\n            num_workers) \\\n        = self.get_parameters()\n\n        # Create Detector\n        self._detector = Detector(model_name, num_of_classes, label_file,\n            num_workers)\n\n        self._bridge = CvBridge()\n\n        # Advertise the result of Object Detector\n        self.pub_detections = rospy.Publisher(\'/object_detection/detections\', \\\n            DetectionArray, queue_size=1)\n\n        # Advertise the result of Object Detector\n        self.pub_detections_image = rospy.Publisher(\\\n            \'/object_detection/detections_image\', Image, queue_size=1)\n\n        if video_name == ""no"":\n            # Subscribe to the face positions\n            self.sub_rgb = rospy.Subscriber(camera_namespace,\\\n                Image, self.rgb_callback, queue_size=1, buff_size=2**24)\n        else:\n            self.read_from_video(video_name)\n        # spin\n        rospy.spin()\n\n    def read_from_video(self, video_name):\n        """"""\n        Applies object detection to a video\n\n        Args:\n            Name of the video with full path\n\n        Returns:\n\n        """"""\n\n        cap = cv2.VideoCapture(video_name)\n\n        while(cap.isOpened()):\n            ret, frame = cap.read()\n\n            if frame is not None:\n                image_message = \\\n                    self._bridge.cv2_to_imgmsg(frame, ""bgr8"")\n\n                self.rgb_callback(image_message)\n\n            else:\n                break\n\n        cap.release()\n        cv2.destroyAllWindows()\n\n        print ""Video has been processed!""\n\n        self.shutdown()\n\n    def get_parameters(self):\n        """"""\n        Gets the necessary parameters from parameter server\n\n        Args:\n\n        Returns:\n        (tuple) (model name, num_of_classes, label_file)\n\n        """"""\n\n        model_name  = rospy.get_param(""~model_name"")\n        num_of_classes  = rospy.get_param(""~num_of_classes"")\n        label_file  = rospy.get_param(""~label_file"")\n        camera_namespace  = rospy.get_param(""~camera_namespace"")\n        video_name = rospy.get_param(""~video_name"")\n        num_workers = rospy.get_param(""~num_workers"")\n\n        return (model_name, num_of_classes, label_file, \\\n                camera_namespace, video_name, num_workers)\n\n\n    def shutdown(self):\n        """"""\n        Shuts down the node\n        """"""\n        rospy.signal_shutdown(""See ya!"")\n\n    def rgb_callback(self, data):\n        """"""\n        Callback for RGB images\n        """"""\n        try:\n            # Convert image to numpy array\n            cv_image = self._bridge.imgmsg_to_cv2(data, ""bgr8"")\n\n            # Detect\n            (output_dict, category_index) = \\\n                self._detector.detect(cv_image)\n\n\n            # Create the message\n            msg = \\\n                utils.create_detection_msg(\\\n                data, output_dict, category_index, self._bridge)\n\n            # Draw bounding boxes\n            image_processed = \\\n                self._detector.visualize(cv_image, output_dict)\n\n            # Convert numpy image into sensor img\n            msg_im = \\\n                self._bridge.cv2_to_imgmsg(\\\n                image_processed, encoding=""passthrough"")\n\n            # Publish the messages\n            self.pub_detections.publish(msg)\n            self.pub_detections_image.publish(msg_im)\n\n        except CvBridgeError as e:\n            print(e)\n\ndef main():\n    """""" main function\n    """"""\n    node = PeopleObjectDetectionNode()\n\nif __name__ == \'__main__\':\n    main()\n'"
src/face_recognizer.py,0,"b'#!/usr/bin/env python\n""""""\nA ROS node to get face bounding boxes inside of person bounding boxes returned\nby object detection node.\n\nThis node gets the people bounding boxes and applies face searching and face\ncomparison by using face_recognition Python library.\n\nThe people who are placed in /people directory will be automatically fetched\nand their face features will be compared to incoming face images. If a similar\nface is found, the name of the closest face image will be assigned to that\nbounding box.\n\nAuthor:\n    Cagatay Odabasi -- cagatay.odabasi@ipa.fraunhofer.de\n""""""\n\nimport glob\n\nimport rospy\n\nimport message_filters\n\nimport numpy as np\n\nimport cv2\n\nfrom cv_bridge import CvBridge\n\nfrom sensor_msgs.msg import Image\n\nfrom cob_perception_msgs.msg import DetectionArray\n\nimport face_recognition as fr\n\nimport rospkg\n\n\n# Get the package directory\nrospack = rospkg.RosPack()\n\ncd = rospack.get_path(\'cob_people_object_detection_tensorflow\')\n\n\nclass FaceRecognitionNode(object):\n    """"""A ROS node to get face bounding boxes inside of person bounding boxes\n\n    _bridge (CvBridge): Bridge between ROS and CV image\n    pub_det (Publisher): Publisher object for detections (bounding box, labels)\n    pub_det_rgb (Publisher): Publisher object for detection image\n    sub_detection (Subscriber): Subscriber object for object_detection\n    sub_image (Subscriber): Subscriber object for RGB image from camera\n    scaling_factor (Float): Input image will be scaled down with this\n    database (List): Contains face features of people inside /people folder\n\n    """"""\n    def __init__(self):\n        super(FaceRecognitionNode, self).__init__()\n\n        # init the node\n        rospy.init_node(\'face_recognition_node\', anonymous=False)\n\n        # Get the parameters\n        (image_topic, detection_topic, output_topic, output_topic_rgb) \\\n            = self.get_parameters()\n\n        self._bridge = CvBridge()\n\n        # Advertise the result of Object Tracker\n        self.pub_det = rospy.Publisher(output_topic, \\\n            DetectionArray, queue_size=1)\n\n        self.pub_det_rgb = rospy.Publisher(output_topic_rgb, \\\n            Image, queue_size=1)\n\n\n        self.sub_detection = message_filters.Subscriber(detection_topic, \\\n            DetectionArray)\n        self.sub_image = message_filters.Subscriber(image_topic, Image)\n\n        # Scaling factor for face recognition image\n        self.scaling_factor = 1.0\n\n        # Read the images from folder and create a database\n        self.database = self.initialize_database()\n\n        ts = message_filters.ApproximateTimeSynchronizer(\\\n            [self.sub_detection, self.sub_image], 2, 0.3)\n\n        ts.registerCallback(self.detection_callback)\n\n        # spin\n        rospy.spin()\n\n    def get_parameters(self):\n        """"""\n        Gets the necessary parameters from parameter server\n\n        Args:\n\n        Returns:\n        (tuple) (camera_topic, detection_topic, output_topic)\n\n        """"""\n\n        camera_topic = rospy.get_param(""~camera_topic"")\n        detection_topic = rospy.get_param(""~detection_topic"")\n        output_topic = rospy.get_param(""~output_topic"")\n        output_topic_rgb = rospy.get_param(""~output_topic_rgb"")\n\n        return (camera_topic, detection_topic, output_topic, output_topic_rgb)\n\n\n    def shutdown(self):\n        """"""\n        Shuts down the node\n        """"""\n        rospy.signal_shutdown(""See ya!"")\n\n    def detection_callback(self, detections, image):\n        """"""\n        Callback for RGB images and detections\n\n        Args:\n        detections (cob_perception_msgs/DetectionArray) : detections array\n        image (sensor_msgs/Image): RGB image from camera\n\n        """"""\n\n        cv_rgb = self._bridge.imgmsg_to_cv2(image, ""passthrough"")[:, :, ::-1]\n\n        cv_rgb = cv2.resize(cv_rgb, (0, 0), fx=self.scaling_factor, fy=self.scaling_factor)\n\n        cv_rgb=cv_rgb.astype(np.uint8)\n\n        (cv_rgb, detections) = self.recognize(detections, cv_rgb)\n\n        image_outgoing = self._bridge.cv2_to_imgmsg(cv_rgb, encoding=""passthrough"")\n\n        self.publish(detections, image_outgoing)\n\n\n    def recognize(self, detections, image):\n        """"""\n        Main face recognition logic, it gets the incoming detection message and\n        modifies the person labeled detections according to the face info.\n\n        For example:\n         (label) person, bounding_box -> (label) Mario, bounding_box of face\n\n        Args:\n        (DetectionArray) detections: detections array message from cob package\n        (numpy.ndarray) image: incoming people image\n\n        Returns:\n\n        (numpy.ndarray): image with labels and bounding boxes\n        (cob_perception_msgs/DetectionArray): detections with labeled faces\n\n        """"""\n\n        detections_out = DetectionArray()\n        detections_out.header = detections.header\n\n        for i, detection in enumerate(detections.detections):\n\n            if detection.label == ""person"":\n\n                x =  int(detection.mask.roi.x * self.scaling_factor)\n                y = int(detection.mask.roi.y * self.scaling_factor)\n                width = int(detection.mask.roi.width * self.scaling_factor)\n                height = int(detection.mask.roi.height * self.scaling_factor)\n                score = detection.score\n\n                try:\n                    # Crop detection image\n                    detection_image = image[y:y+height, x:x+width]\n\n                    face_locations = fr.face_locations(detection_image)\n\n                    face_features = fr.face_encodings(detection_image, \\\n                        face_locations)\n\n                    for features, (top, right, bottom, left) in \\\n                        zip(face_features, face_locations):\n                        matches = fr.compare_faces(self.database[0], features)\n\n                        l = x + left\n                        t = y + top\n                        r = x + right\n                        b = y + bottom\n\n                        detection.label = ""Unknown""\n\n\n                        if True in matches:\n                            ind = matches.index(True)\n                            detection.label = self.database[1][ind]\n\n                        # Modify the message\n                        detection.mask.roi.x  = l/self.scaling_factor\n                        detection.mask.roi.y = t/self.scaling_factor\n                        detection.mask.roi.width = (r-l)/self.scaling_factor\n                        detection.mask.roi.height = (b-t)/self.scaling_factor\n\n                        # Draw bounding boxes on current image\n\n                        cv2.rectangle(image, (l, t), \\\n                        (r, b), (0, 0, 255), 2)\n\n                        cv2.rectangle(image, (x, y), \\\n                        (x + width, y + height), (255, 0, 0), 3)\n\n                        cv2.putText(image, detection.label, \\\n                        (l + 2, t + 2), \\\n                        cv2.FONT_HERSHEY_DUPLEX, 1.0, (0, 0, 0), 1)\n\n                        detections_out.detections.append(detection)\n\n\n                except Exception as e:\n                    print e\n\n        return (image, detections_out)\n\n    def publish(self, detections, image_outgoing):\n        """"""\n        Creates the ros messages and publishes them\n\n        Args:\n        detections (cob_perception_msgs/DetectionArray): incoming detections\n        image_outgoing (sensor_msgs/Image): with face bounding boxes and labels\n\n        """"""\n\n        self.pub_det.publish(detections)\n        self.pub_det_rgb.publish(image_outgoing)\n\n\n\n    def initialize_database(self):\n        """"""\n        Reads the PNG images from ./people folder and\n        creates a list of peoples\n\n        The names of the image files are considered as their\n        real names.\n\n        For example;\n        /people\n          - mario.png\n          - jennifer.png\n          - melanie.png\n\n        Returns:\n        (tuple) (people_list, name_list) (features of people, names of people)\n\n        """"""\n        filenames = glob.glob(cd + \'/people/*.png\')\n\n        people_list = []\n        name_list = []\n\n        for f in filenames:\n            im = cv2.imread(f, 1)\n\n            #cv2.imshow(""Database Image"", im)\n\n            #cv2.waitKey(500)\n\n            im = im.astype(np.uint8)\n\n            people_list.append(fr.face_encodings(im)[0])\n\n            name_list.append(f.split(\'/\')[-1].split(\'.\')[0])\n\n            #cv2.destroyAllWindows()\n\n        return (people_list, name_list)\n\ndef main():\n    """""" main function\n    """"""\n    node = FaceRecognitionNode()\n\nif __name__ == \'__main__\':\n    main()\n'"
src/projection.py,0,"b'#!/usr/bin/env python\n""""""\nA ROS node to get 3D values of bounding boxes returned by face_recognizer node.\n\nThis node gets the face bounding boxes and gets the real world coordinates of\nthem by using depth values. It simply gets the x and y values of center point\nand gets the median value of face depth values as z value of face.\n\nAuthor:\n    Cagatay Odabasi -- cagatay.odabasi@ipa.fraunhofer.de\n""""""\n\nimport rospy\n\nimport message_filters\n\nimport numpy as np\n\nfrom cv_bridge import CvBridge\n\nfrom sensor_msgs.msg import Image\n\nfrom cob_perception_msgs.msg import DetectionArray\n\n\nclass ProjectionNode(object):\n    """"""Get 3D values of bounding boxes returned by face_recognizer node.\n\n    _bridge (CvBridge): Bridge between ROS and CV image\n    pub (Publisher): Publisher object for face depth results\n    f (Float): Focal Length\n    cx (Int): Principle Point Horizontal\n    cy (Int): Principle Point Vertical\n\n    """"""\n    def __init__(self):\n        super(ProjectionNode, self).__init__()\n\n        # init the node\n        rospy.init_node(\'projection_node\', anonymous=False)\n\n        self._bridge = CvBridge()\n\n        (depth_topic, face_topic, output_topic, f, cx, cy) = \\\n            self.get_parameters()\n\n         # Subscribe to the face positions\n        sub_obj = message_filters.Subscriber(face_topic,\\\n            DetectionArray)\n\n        sub_depth = message_filters.Subscriber(depth_topic,\\\n            Image)\n\n        # Advertise the result of Face Depths\n        self.pub = rospy.Publisher(output_topic, \\\n            DetectionArray, queue_size=1)\n\n        # Create the message filter\n        ts = message_filters.ApproximateTimeSynchronizer(\\\n            [sub_obj, sub_depth], \\\n            2, \\\n            0.9)\n\n        ts.registerCallback(self.detection_callback)\n\n        self.f = f\n        self.cx = cx\n        self.cy = cy\n\n        # spin\n        rospy.spin()\n\n\n    def shutdown(self):\n        """"""\n        Shuts down the node\n        """"""\n        rospy.signal_shutdown(""See ya!"")\n\n    def detection_callback(self, msg, depth):\n        """"""\n        Callback for RGB images: The main logic is applied here\n\n        Args:\n        msg (cob_perception_msgs/DetectionArray): detections array\n        depth (sensor_msgs/PointCloud2): depth image from camera\n\n        """"""\n\n        cv_depth = self._bridge.imgmsg_to_cv2(depth, ""passthrough"")\n\n        # get the number of detections\n        no_of_detections = len(msg.detections)\n\n        # Check if there is a detection\n        if no_of_detections > 0:\n            for i, detection in enumerate(msg.detections):\n\n                x =  detection.mask.roi.x\n                y = detection.mask.roi.y\n                width =  detection.mask.roi.width\n                height = detection.mask.roi.height\n\n                cv_depth_bounding_box = cv_depth[y:y+height,x:x+width]\n\n                try:\n\n                    depth_mean = np.nanmedian(\\\n                       cv_depth_bounding_box[np.nonzero(cv_depth_bounding_box)])\n\n                    real_x = (x + width/2-self.cx)*(depth_mean*0.001)/self.f\n\n                    real_y = (y + height/2-self.cy)*(depth_mean*0.001)/self.f\n\n                    msg.detections[i].pose.pose.position.x = real_x\n                    msg.detections[i].pose.pose.position.y = real_y\n                    msg.detections[i].pose.pose.position.z = depth_mean*0.001\n\n                except Exception as e:\n                    print e\n\n        self.pub.publish(msg)\n\n    def get_parameters(self):\n        """"""\n        Gets the necessary parameters from parameter server\n\n        Returns:\n        (tuple) :\n            depth_topic (String): Incoming depth topic name\n            face_topic (String): Incoming face bounding box topic name\n            output_topic (String): Outgoing depth topic name\n            f (Float): Focal Length\n            cx (Int): Principle Point Horizontal\n            cy (Int): Principle Point Vertical\n        """"""\n\n        depth_topic  = rospy.get_param(""~depth_topic"")\n        face_topic = rospy.get_param(\'~face_topic\')\n        output_topic = rospy.get_param(\'~output_topic\')\n        f = rospy.get_param(\'~focal_length\')\n        cx = rospy.get_param(\'~cx\')\n        cy = rospy.get_param(\'~cy\')\n\n        return (depth_topic, face_topic, output_topic, f, cx, cy)\n\n\ndef main():\n    """""" main function\n    """"""\n    node = ProjectionNode()\n\nif __name__ == \'__main__\':\n    main()\n'"
src/tracker.py,0,"b'#!/usr/bin/env python\n""""""\nA ROS node to detect objects via TensorFlow Object Detection API.\n\nAuthor:\n    Cagatay Odabasi -- cagatay.odabasi@ipa.fraunhofer.de\n""""""\n\nimport rospy\n\nimport numpy\n\nfrom scipy.optimize import linear_sum_assignment\n\nfrom cv_bridge import CvBridge\n\nfrom cob_perception_msgs.msg import DetectionArray\n\nfrom sort import sort\n\n\nclass PeopleObjectTrackerNode(object):\n    """"""docstring for PeopleObjectDetectionNode.""""""\n    def __init__(self):\n        super(PeopleObjectTrackerNode, self).__init__()\n\n        # init the node\n        rospy.init_node(\'people_object_tracker\', anonymous=False)\n\n        # Get the parameters\n        (detection_topic, tracker_topic, cost_threshold, \\\n            max_age, min_hits) = \\\n            self.get_parameters()\n\n        self._bridge = CvBridge()\n\n        self.tracker = sort.Sort(max_age=max_age, min_hits=min_hits)\n\n        self.cost_threshold = cost_threshold\n\n        # Advertise the result of Object Tracker\n        self.pub_trackers = rospy.Publisher(tracker_topic, \\\n            DetectionArray, queue_size=1)\n\n        self.sub_detection = rospy.Subscriber(detection_topic, \\\n            DetectionArray,\\\n            self.detection_callback)\n\n        # spin\n        rospy.spin()\n\n    def get_parameters(self):\n        """"""\n        Gets the necessary parameters from parameter server\n\n        Args:\n\n        Returns:\n        (tuple) (camera_topic, detection_topic, tracker_topic, cost_threhold)\n\n        """"""\n\n        detection_topic = rospy.get_param(""~detection_topic"")\n        tracker_topic = rospy.get_param(\'~tracker_topic\')\n        cost_threhold = rospy.get_param(\'~cost_threhold\')\n        min_hits = rospy.get_param(\'~min_hits\')\n        max_age = rospy.get_param(\'~max_age\')\n\n        return (detection_topic, tracker_topic, cost_threhold, \\\n            max_age, min_hits)\n\n\n    def shutdown(self):\n        """"""\n        Shuts down the node\n        """"""\n        rospy.signal_shutdown(""See ya!"")\n\n    def detection_callback(self, detections):\n        """"""\n        Callback for RGB images\n        Args:\n        detections: detections array message from cob package\n\n        image: rgb frame in openCV format\n\n        """"""\n\n        #cv_rgb = self._bridge.imgmsg_to_cv2(image, ""passthrough"")\n\n        # Dummy detection\n        # TODO: Find a better way\n        det_list = numpy.array([[0, 0, 1, 1, 0.01]])\n\n        if len(detections.detections) > 0:\n            for i, detection in enumerate(detections.detections):\n\n                if True:\n                    x =  detection.mask.roi.x\n                    y = detection.mask.roi.y\n                    width = detection.mask.roi.width\n                    height = detection.mask.roi.height\n                    score = detection.score\n\n                    det_list = numpy.vstack((det_list, \\\n                        [x, y, x+width, y+height, score]))\n                else:\n                    del detections.detections[i]\n\n        # Call the tracker\n        tracks = self.tracker.update(det_list)\n\n        # Copy the detections\n        detections_copy = detections.detections\n\n        detections.detections = []\n\n        if len(det_list) > 0:\n\n            # Create cost matrix\n            # Double for in Python :(\n            C = numpy.zeros((len(tracks), len(det_list)))\n            for i, track in enumerate(tracks):\n                for j, det in enumerate(det_list):\n                    C[i, j] = numpy.linalg.norm(det[0:-2] - track[0:-2])\n\n            # apply linear assignment\n            row_ind, col_ind = linear_sum_assignment(C)\n\n            for i, j in zip(row_ind, col_ind):\n                if C[i, j] < self.cost_threshold and j != 0:\n                    print(""{} -> {} with cost {}"".\\\n                        format(tracks[i, 4], detections_copy[j-1].label,\\\n                        C[i,j]))\n\n                    detections_copy[j-1].id = int(tracks[i, 4])\n\n                    detections.detections.append(detections_copy[j-1])\n\n            print(""------------"")\n\n\n        else:\n            print ""No tracked objects!""\n\n        self.pub_trackers.publish(detections)\n\n\ndef main():\n    """""" main function\n    """"""\n    PeopleObjectTrackerNode()\n\nif __name__ == \'__main__\':\n    main()\n'"
src/cob_people_object_detection_tensorflow/__init__.py,0,b''
src/cob_people_object_detection_tensorflow/detector.py,18,"b'#!/usr/bin/env python\n\n""""""\nDetector class to use TensorFlow detection API\n\nThe codes are from TensorFlow/Models Repo. I just transferred the code to\nROS.\n\nCagatay Odabasi\n""""""\n\nimport numpy as np\nimport tensorflow as tf\nimport time\n\nimport cv2\n\nfrom object_detection.utils import label_map_util\nfrom object_detection.utils import visualization_utils as vis_util\n\nfrom object_detection.utils import ops as utils_ops\n\nimport rospkg\n\nclass Detector(object):\n    """"""docstring for Detector.""""""\n    def __init__(self, \\\n        model_name=\'ssd_mobilenet_v1_coco_11_06_2017\',\\\n        num_of_classes=90,\\\n        label_file=\'mscoco_label_map.pbtxt\',\\\n        num_workers=-1\n        ):\n\n        super(Detector, self).__init__()\n        # What model to download.\n        self._model_name = model_name\n        # ssd_inception_v2_coco_11_06_2017\n\n        self._num_classes = num_of_classes\n\n        self._detection_graph = None\n\n        self._sess = None\n\n        self.category_index = None\n\n        self._label_file = label_file\n\n        self._num_workers = num_workers\n\n        # get an instance of RosPack with the default search paths\n        rospack = rospkg.RosPack()\n\n        self._tf_object_detection_path = \\\n            rospack.get_path(\'cob_people_object_detection_tensorflow\') + \\\n            \'/src/object_detection\'\n\n        self._path_to_ckpt = self._tf_object_detection_path + \'/\' + \\\n            self._model_name + \'/frozen_inference_graph.pb\'\n\n        # List of the strings that is used to add correct label for each box.\n        self._path_to_labels = self._tf_object_detection_path + \'/\' + \\\n            \'data/\' + self._label_file\n\n        # Prepare the model for detection\n        self.prepare()\n\n    def load_model(self):\n        """"""\n        Loads the detection model\n\n        Args:\n\n        Returns:\n\n        """"""\n\n        with self._detection_graph.as_default():\n            od_graph_def = tf.GraphDef()\n            with tf.gfile.GFile(self._path_to_ckpt, \'rb\') as fid:\n                serialized_graph = fid.read()\n                od_graph_def.ParseFromString(serialized_graph)\n                tf.import_graph_def(od_graph_def, name=\'\')\n\n        label_map = label_map_util.load_labelmap(self._path_to_labels)\n        categories = label_map_util.convert_label_map_to_categories(\\\n            label_map, max_num_classes=self._num_classes, use_display_name=True)\n        self.category_index = label_map_util.create_category_index(categories)\n\n    def prepare(self):\n        """"""\n        Prepares the model for detection\n\n        Args:\n\n        Returns:\n\n        """"""\n\n        self._detection_graph = tf.Graph()\n\n        self.load_model()\n\n        # Set the number of workers of TensorFlow\n        if self._num_workers == -1:\n            self._sess = tf.Session(graph=self._detection_graph)\n        else:\n            session_conf = tf.ConfigProto(\n                intra_op_parallelism_threads=self._num_workers,\n                inter_op_parallelism_threads=self._num_workers,\n            )\n\n            self._sess = tf.Session(graph=self._detection_graph,\n                config=session_conf)\n\n\n    def detect(self, image):\n        """"""\n        Detects objects in the image given\n\n        Args:\n        image: (numpy array) input image\n\n        Returns:\n        output_dict (dictionary) Contains boxes, scores, masks etc.\n        """"""\n        with self._detection_graph.as_default():\n         # Get handles to input and output tensors\n            ops = tf.get_default_graph().get_operations()\n            all_tensor_names = {output.name for op in ops for output in op.outputs}\n            tensor_dict = {}\n            for key in [\n               \'num_detections\', \'detection_boxes\', \'detection_scores\',\n               \'detection_classes\', \'detection_masks\'\n               ]:\n                tensor_name = key + \':0\'\n                if tensor_name in all_tensor_names:\n                     tensor_dict[key] = tf.get_default_graph().get_tensor_by_name(\n                     tensor_name)\n            if \'detection_masks\' in tensor_dict:\n                # The following processing is only for single image\n                detection_boxes = tf.squeeze(tensor_dict[\'detection_boxes\'], [0])\n                detection_masks = tf.squeeze(tensor_dict[\'detection_masks\'], [0])\n                # Reframe is required to translate mask from box coordinates to image coordinates and fit the image size.\n                real_num_detection = tf.cast(tensor_dict[\'num_detections\'][0], tf.int32)\n                detection_boxes = tf.slice(detection_boxes, [0, 0], [real_num_detection, -1])\n                detection_masks = tf.slice(detection_masks, [0, 0, 0], [real_num_detection, -1, -1])\n                detection_masks_reframed = utils_ops.reframe_box_masks_to_image_masks(\n                    detection_masks, detection_boxes, image.shape[0], image.shape[1])\n                detection_masks_reframed = tf.cast(\n                    tf.greater(detection_masks_reframed, 0.5), tf.uint8)\n                # Follow the convention by adding back the batch dimension\n                tensor_dict[\'detection_masks\'] = tf.expand_dims(\n                    detection_masks_reframed, 0)\n            image_tensor = tf.get_default_graph().get_tensor_by_name(\'image_tensor:0\')\n\n            start = time.time()\n\n             # Run inference\n            output_dict = self._sess.run(tensor_dict,\n                     feed_dict={image_tensor: np.expand_dims(image, 0)})\n\n            end = time.time()\n\n            #print end-start\n\n            # all outputs are float32 numpy arrays, so convert types as appropriate\n            output_dict[\'num_detections\'] = int(output_dict[\'num_detections\'][0])\n            output_dict[\'detection_classes\'] = output_dict[\n                \'detection_classes\'][0].astype(np.uint8)\n            output_dict[\'detection_boxes\'] = output_dict[\'detection_boxes\'][0]\n            output_dict[\'detection_scores\'] = output_dict[\'detection_scores\'][0]\n            if \'detection_masks\' in output_dict:\n                output_dict[\'detection_masks\'] = output_dict[\'detection_masks\'][0]\n        return (output_dict, self.category_index)\n\n    def visualize(self, image, output_dict):\n        """"""\n        Draws the bounding boxes, labels and scores of each detection\n\n        Args:\n        image: (numpy array) input image\n        output_dict (dictionary) output of object detection model\n\n        Returns:\n        image: (numpy array) image with drawings\n        """"""\n        # Draw the bounding boxes\n        vis_util.visualize_boxes_and_labels_on_image_array(\n            image,\n            output_dict[""detection_boxes""],\n            output_dict[""detection_classes""],\n            output_dict[""detection_scores""],\n            self.category_index,\n            instance_masks=output_dict.get(\'detection_masks\'),\n            use_normalized_coordinates=True,\n            line_thickness=5)\n\n        return image\n'"
src/cob_people_object_detection_tensorflow/utils.py,0,"b'#!/usr/bin/env python\n\n""""""\nHelper functions and classes will be placed here.\n""""""\n\nimport os\nimport tarfile\nimport six.moves.urllib as urllib\n\nimport numpy as np\n\nfrom cob_perception_msgs.msg import Detection, DetectionArray, Rect\n\n\ndef download_model(\\\n    download_base=\'http://download.tensorflow.org/models/object_detection/\', \\\n    model_name=\'ssd_mobilenet_v1_coco_11_06_2017\'\\\n    ):\n    """"""\n    Downloads the detection model from tensorflow servers\n\n    Args:\n    download_base: base url where the object detection model is downloaded from\n\n    model_name: name of the object detection model\n\n    Returns:\n\n    """"""\n\n    # add tar gz to the end of file name\n    model_file = model_name + \'.tar.gz\'\n\n    try:\n        opener = urllib.request.URLopener()\n        opener.retrieve(download_base + model_file, \\\n            model_file)\n        tar_file = tarfile.open(model_file)\n        for f in tar_file.getmembers():\n            file_name = os.path.basename(f.name)\n            if \'frozen_inference_graph.pb\' in file_name:\n                tar_file.extract(f, os.getcwd())\n    except Exception as e:\n        raise\n\ndef create_detection_msg(im, output_dict, category_index, bridge):\n    """"""\n    Creates the detection array message\n\n    Args:\n    im: (std_msgs_Image) incomming message\n\n    output_dict (dictionary) output of object detection model\n\n    category_index: dictionary of labels (like a lookup table)\n\n    bridge (cv_bridge) : cv bridge object for converting\n\n    Returns:\n\n    msg (cob_perception_msgs/DetectionArray) The message to be sent\n\n    """"""\n\n    boxes = output_dict[""detection_boxes""]\n    scores = output_dict[""detection_scores""]\n    classes = output_dict[""detection_classes""]\n    masks = None\n\n    if \'detection_masks\' in output_dict:\n        masks = output_dict[""detection_masks""]\n\n    msg = DetectionArray()\n\n    msg.header = im.header\n\n    scores_above_threshold = np.where(scores > 0.5)[0]\n\n    for s in scores_above_threshold:\n        # Get the properties\n\n        bb = boxes[s,:]\n        sc = scores[s]\n        cl = classes[s]\n\n        # Create the detection message\n        detection = Detection()\n        detection.header = im.header\n        detection.label = category_index[int(cl)][\'name\']\n        detection.id = cl\n        detection.score = sc\n        detection.detector = \'Tensorflow object detector\'\n        detection.mask.roi.x = int((im.width-1) * bb[1])\n        detection.mask.roi.y = int((im.height-1) * bb[0])\n        detection.mask.roi.width = int((im.width-1) * (bb[3]-bb[1]))\n        detection.mask.roi.height = int((im.height-1) * (bb[2]-bb[0]))\n\n        if \'detection_masks\' in output_dict:\n            detection.mask.mask = \\\n                bridge.cv2_to_imgmsg(masks[s], ""mono8"")\n\n            print detection.mask.mask.width\n\n\n        msg.detections.append(detection)\n\n    return msg\n'"
src/object_detection/__init__.py,0,b''
src/object_detection/create_pascal_tf_record.py,8,"b'# Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nr""""""Convert raw PASCAL dataset to TFRecord for object_detection.\n\nExample usage:\n    ./create_pascal_tf_record --data_dir=/home/user/VOCdevkit \\\n        --year=VOC2012 \\\n        --output_path=/home/user/pascal.record\n""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport hashlib\nimport io\nimport logging\nimport os\n\nfrom lxml import etree\nimport PIL.Image\nimport tensorflow as tf\n\nfrom object_detection.utils import dataset_util\nfrom object_detection.utils import label_map_util\n\n\nflags = tf.app.flags\nflags.DEFINE_string(\'data_dir\', \'\', \'Root directory to raw PASCAL VOC dataset.\')\nflags.DEFINE_string(\'set\', \'train\', \'Convert training set, validation set or \'\n                    \'merged set.\')\nflags.DEFINE_string(\'annotations_dir\', \'Annotations\',\n                    \'(Relative) path to annotations directory.\')\nflags.DEFINE_string(\'year\', \'VOC2007\', \'Desired challenge year.\')\nflags.DEFINE_string(\'output_path\', \'\', \'Path to output TFRecord\')\nflags.DEFINE_string(\'label_map_path\', \'data/pascal_label_map.pbtxt\',\n                    \'Path to label map proto\')\nflags.DEFINE_boolean(\'ignore_difficult_instances\', False, \'Whether to ignore \'\n                     \'difficult instances\')\nFLAGS = flags.FLAGS\n\nSETS = [\'train\', \'val\', \'trainval\', \'test\']\nYEARS = [\'VOC2007\', \'VOC2012\', \'merged\']\n\n\ndef dict_to_tf_example(data,\n                       dataset_directory,\n                       label_map_dict,\n                       ignore_difficult_instances=False,\n                       image_subdirectory=\'JPEGImages\'):\n  """"""Convert XML derived dict to tf.Example proto.\n\n  Notice that this function normalizes the bounding box coordinates provided\n  by the raw data.\n\n  Args:\n    data: dict holding PASCAL XML fields for a single image (obtained by\n      running dataset_util.recursive_parse_xml_to_dict)\n    dataset_directory: Path to root directory holding PASCAL dataset\n    label_map_dict: A map from string label names to integers ids.\n    ignore_difficult_instances: Whether to skip difficult instances in the\n      dataset  (default: False).\n    image_subdirectory: String specifying subdirectory within the\n      PASCAL dataset directory holding the actual image data.\n\n  Returns:\n    example: The converted tf.Example.\n\n  Raises:\n    ValueError: if the image pointed to by data[\'filename\'] is not a valid JPEG\n  """"""\n  img_path = os.path.join(data[\'folder\'], image_subdirectory, data[\'filename\'])\n  full_path = os.path.join(dataset_directory, img_path)\n  with tf.gfile.GFile(full_path, \'rb\') as fid:\n    encoded_jpg = fid.read()\n  encoded_jpg_io = io.BytesIO(encoded_jpg)\n  image = PIL.Image.open(encoded_jpg_io)\n  if image.format != \'JPEG\':\n    raise ValueError(\'Image format not JPEG\')\n  key = hashlib.sha256(encoded_jpg).hexdigest()\n\n  width = int(data[\'size\'][\'width\'])\n  height = int(data[\'size\'][\'height\'])\n\n  xmin = []\n  ymin = []\n  xmax = []\n  ymax = []\n  classes = []\n  classes_text = []\n  truncated = []\n  poses = []\n  difficult_obj = []\n  for obj in data[\'object\']:\n    difficult = bool(int(obj[\'difficult\']))\n    if ignore_difficult_instances and difficult:\n      continue\n\n    difficult_obj.append(int(difficult))\n\n    xmin.append(float(obj[\'bndbox\'][\'xmin\']) / width)\n    ymin.append(float(obj[\'bndbox\'][\'ymin\']) / height)\n    xmax.append(float(obj[\'bndbox\'][\'xmax\']) / width)\n    ymax.append(float(obj[\'bndbox\'][\'ymax\']) / height)\n    classes_text.append(obj[\'name\'].encode(\'utf8\'))\n    classes.append(label_map_dict[obj[\'name\']])\n    truncated.append(int(obj[\'truncated\']))\n    poses.append(obj[\'pose\'].encode(\'utf8\'))\n\n  example = tf.train.Example(features=tf.train.Features(feature={\n      \'image/height\': dataset_util.int64_feature(height),\n      \'image/width\': dataset_util.int64_feature(width),\n      \'image/filename\': dataset_util.bytes_feature(\n          data[\'filename\'].encode(\'utf8\')),\n      \'image/source_id\': dataset_util.bytes_feature(\n          data[\'filename\'].encode(\'utf8\')),\n      \'image/key/sha256\': dataset_util.bytes_feature(key.encode(\'utf8\')),\n      \'image/encoded\': dataset_util.bytes_feature(encoded_jpg),\n      \'image/format\': dataset_util.bytes_feature(\'jpeg\'.encode(\'utf8\')),\n      \'image/object/bbox/xmin\': dataset_util.float_list_feature(xmin),\n      \'image/object/bbox/xmax\': dataset_util.float_list_feature(xmax),\n      \'image/object/bbox/ymin\': dataset_util.float_list_feature(ymin),\n      \'image/object/bbox/ymax\': dataset_util.float_list_feature(ymax),\n      \'image/object/class/text\': dataset_util.bytes_list_feature(classes_text),\n      \'image/object/class/label\': dataset_util.int64_list_feature(classes),\n      \'image/object/difficult\': dataset_util.int64_list_feature(difficult_obj),\n      \'image/object/truncated\': dataset_util.int64_list_feature(truncated),\n      \'image/object/view\': dataset_util.bytes_list_feature(poses),\n  }))\n  return example\n\n\ndef main(_):\n  if FLAGS.set not in SETS:\n    raise ValueError(\'set must be in : {}\'.format(SETS))\n  if FLAGS.year not in YEARS:\n    raise ValueError(\'year must be in : {}\'.format(YEARS))\n\n  data_dir = FLAGS.data_dir\n  years = [\'VOC2007\', \'VOC2012\']\n  if FLAGS.year != \'merged\':\n    years = [FLAGS.year]\n\n  writer = tf.python_io.TFRecordWriter(FLAGS.output_path)\n\n  label_map_dict = label_map_util.get_label_map_dict(FLAGS.label_map_path)\n\n  for year in years:\n    logging.info(\'Reading from PASCAL %s dataset.\', year)\n    examples_path = os.path.join(data_dir, year, \'ImageSets\', \'Main\',\n                                 \'aeroplane_\' + FLAGS.set + \'.txt\')\n    annotations_dir = os.path.join(data_dir, year, FLAGS.annotations_dir)\n    examples_list = dataset_util.read_examples_list(examples_path)\n    for idx, example in enumerate(examples_list):\n      if idx % 100 == 0:\n        logging.info(\'On image %d of %d\', idx, len(examples_list))\n      path = os.path.join(annotations_dir, example + \'.xml\')\n      with tf.gfile.GFile(path, \'r\') as fid:\n        xml_str = fid.read()\n      xml = etree.fromstring(xml_str)\n      data = dataset_util.recursive_parse_xml_to_dict(xml)[\'annotation\']\n\n      tf_example = dict_to_tf_example(data, FLAGS.data_dir, label_map_dict,\n                                      FLAGS.ignore_difficult_instances)\n      writer.write(tf_example.SerializeToString())\n\n  writer.close()\n\n\nif __name__ == \'__main__\':\n  tf.app.run()\n'"
src/object_detection/create_pascal_tf_record_test.py,2,"b'# Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n""""""Test for create_pascal_tf_record.py.""""""\n\nimport os\n\nimport numpy as np\nimport PIL.Image\nimport tensorflow as tf\n\nfrom object_detection import create_pascal_tf_record\n\n\nclass DictToTFExampleTest(tf.test.TestCase):\n\n  def _assertProtoEqual(self, proto_field, expectation):\n    """"""Helper function to assert if a proto field equals some value.\n\n    Args:\n      proto_field: The protobuf field to compare.\n      expectation: The expected value of the protobuf field.\n    """"""\n    proto_list = [p for p in proto_field]\n    self.assertListEqual(proto_list, expectation)\n\n  def test_dict_to_tf_example(self):\n    image_file_name = \'tmp_image.jpg\'\n    image_data = np.random.rand(256, 256, 3)\n    save_path = os.path.join(self.get_temp_dir(), image_file_name)\n    image = PIL.Image.fromarray(image_data, \'RGB\')\n    image.save(save_path)\n\n    data = {\n        \'folder\': \'\',\n        \'filename\': image_file_name,\n        \'size\': {\n            \'height\': 256,\n            \'width\': 256,\n        },\n        \'object\': [\n            {\n                \'difficult\': 1,\n                \'bndbox\': {\n                    \'xmin\': 64,\n                    \'ymin\': 64,\n                    \'xmax\': 192,\n                    \'ymax\': 192,\n                },\n                \'name\': \'person\',\n                \'truncated\': 0,\n                \'pose\': \'\',\n            },\n        ],\n    }\n\n    label_map_dict = {\n        \'background\': 0,\n        \'person\': 1,\n        \'notperson\': 2,\n    }\n\n    example = create_pascal_tf_record.dict_to_tf_example(\n        data, self.get_temp_dir(), label_map_dict, image_subdirectory=\'\')\n    self._assertProtoEqual(\n        example.features.feature[\'image/height\'].int64_list.value, [256])\n    self._assertProtoEqual(\n        example.features.feature[\'image/width\'].int64_list.value, [256])\n    self._assertProtoEqual(\n        example.features.feature[\'image/filename\'].bytes_list.value,\n        [image_file_name])\n    self._assertProtoEqual(\n        example.features.feature[\'image/source_id\'].bytes_list.value,\n        [image_file_name])\n    self._assertProtoEqual(\n        example.features.feature[\'image/format\'].bytes_list.value, [\'jpeg\'])\n    self._assertProtoEqual(\n        example.features.feature[\'image/object/bbox/xmin\'].float_list.value,\n        [0.25])\n    self._assertProtoEqual(\n        example.features.feature[\'image/object/bbox/ymin\'].float_list.value,\n        [0.25])\n    self._assertProtoEqual(\n        example.features.feature[\'image/object/bbox/xmax\'].float_list.value,\n        [0.75])\n    self._assertProtoEqual(\n        example.features.feature[\'image/object/bbox/ymax\'].float_list.value,\n        [0.75])\n    self._assertProtoEqual(\n        example.features.feature[\'image/object/class/text\'].bytes_list.value,\n        [\'person\'])\n    self._assertProtoEqual(\n        example.features.feature[\'image/object/class/label\'].int64_list.value,\n        [1])\n    self._assertProtoEqual(\n        example.features.feature[\'image/object/difficult\'].int64_list.value,\n        [1])\n    self._assertProtoEqual(\n        example.features.feature[\'image/object/truncated\'].int64_list.value,\n        [0])\n    self._assertProtoEqual(\n        example.features.feature[\'image/object/view\'].bytes_list.value, [\'\'])\n\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
src/object_detection/create_pet_tf_record.py,9,"b'# Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nr""""""Convert the Oxford pet dataset to TFRecord for object_detection.\n\nSee: O. M. Parkhi, A. Vedaldi, A. Zisserman, C. V. Jawahar\n     Cats and Dogs\n     IEEE Conference on Computer Vision and Pattern Recognition, 2012\n     http://www.robots.ox.ac.uk/~vgg/data/pets/\n\nExample usage:\n    ./create_pet_tf_record --data_dir=/home/user/pet \\\n        --output_dir=/home/user/pet/output\n""""""\n\nimport hashlib\nimport io\nimport logging\nimport os\nimport random\nimport re\n\nfrom lxml import etree\nimport PIL.Image\nimport tensorflow as tf\n\nfrom object_detection.utils import dataset_util\nfrom object_detection.utils import label_map_util\n\nflags = tf.app.flags\nflags.DEFINE_string(\'data_dir\', \'\', \'Root directory to raw pet dataset.\')\nflags.DEFINE_string(\'output_dir\', \'\', \'Path to directory to output TFRecords.\')\nflags.DEFINE_string(\'label_map_path\', \'data/pet_label_map.pbtxt\',\n                    \'Path to label map proto\')\nFLAGS = flags.FLAGS\n\n\ndef get_class_name_from_filename(file_name):\n  """"""Gets the class name from a file.\n\n  Args:\n    file_name: The file name to get the class name from.\n               ie. ""american_pit_bull_terrier_105.jpg""\n\n  Returns:\n    example: The converted tf.Example.\n  """"""\n  match = re.match(r\'([A-Za-z_]+)(_[0-9]+\\.jpg)\', file_name, re.I)\n  return match.groups()[0]\n\n\ndef dict_to_tf_example(data,\n                       label_map_dict,\n                       image_subdirectory,\n                       ignore_difficult_instances=False):\n  """"""Convert XML derived dict to tf.Example proto.\n\n  Notice that this function normalizes the bounding box coordinates provided\n  by the raw data.\n\n  Args:\n    data: dict holding PASCAL XML fields for a single image (obtained by\n      running dataset_util.recursive_parse_xml_to_dict)\n    label_map_dict: A map from string label names to integers ids.\n    image_subdirectory: String specifying subdirectory within the\n      Pascal dataset directory holding the actual image data.\n    ignore_difficult_instances: Whether to skip difficult instances in the\n      dataset  (default: False).\n\n  Returns:\n    example: The converted tf.Example.\n\n  Raises:\n    ValueError: if the image pointed to by data[\'filename\'] is not a valid JPEG\n  """"""\n  img_path = os.path.join(image_subdirectory, data[\'filename\'])\n  with tf.gfile.GFile(img_path, \'rb\') as fid:\n    encoded_jpg = fid.read()\n  encoded_jpg_io = io.BytesIO(encoded_jpg)\n  image = PIL.Image.open(encoded_jpg_io)\n  if image.format != \'JPEG\':\n    raise ValueError(\'Image format not JPEG\')\n  key = hashlib.sha256(encoded_jpg).hexdigest()\n\n  width = int(data[\'size\'][\'width\'])\n  height = int(data[\'size\'][\'height\'])\n\n  xmin = []\n  ymin = []\n  xmax = []\n  ymax = []\n  classes = []\n  classes_text = []\n  truncated = []\n  poses = []\n  difficult_obj = []\n  for obj in data[\'object\']:\n    difficult = bool(int(obj[\'difficult\']))\n    if ignore_difficult_instances and difficult:\n      continue\n\n    difficult_obj.append(int(difficult))\n\n    xmin.append(float(obj[\'bndbox\'][\'xmin\']) / width)\n    ymin.append(float(obj[\'bndbox\'][\'ymin\']) / height)\n    xmax.append(float(obj[\'bndbox\'][\'xmax\']) / width)\n    ymax.append(float(obj[\'bndbox\'][\'ymax\']) / height)\n    class_name = get_class_name_from_filename(data[\'filename\'])\n    classes_text.append(class_name.encode(\'utf8\'))\n    classes.append(label_map_dict[class_name])\n    truncated.append(int(obj[\'truncated\']))\n    poses.append(obj[\'pose\'].encode(\'utf8\'))\n\n  example = tf.train.Example(features=tf.train.Features(feature={\n      \'image/height\': dataset_util.int64_feature(height),\n      \'image/width\': dataset_util.int64_feature(width),\n      \'image/filename\': dataset_util.bytes_feature(\n          data[\'filename\'].encode(\'utf8\')),\n      \'image/source_id\': dataset_util.bytes_feature(\n          data[\'filename\'].encode(\'utf8\')),\n      \'image/key/sha256\': dataset_util.bytes_feature(key.encode(\'utf8\')),\n      \'image/encoded\': dataset_util.bytes_feature(encoded_jpg),\n      \'image/format\': dataset_util.bytes_feature(\'jpeg\'.encode(\'utf8\')),\n      \'image/object/bbox/xmin\': dataset_util.float_list_feature(xmin),\n      \'image/object/bbox/xmax\': dataset_util.float_list_feature(xmax),\n      \'image/object/bbox/ymin\': dataset_util.float_list_feature(ymin),\n      \'image/object/bbox/ymax\': dataset_util.float_list_feature(ymax),\n      \'image/object/class/text\': dataset_util.bytes_list_feature(classes_text),\n      \'image/object/class/label\': dataset_util.int64_list_feature(classes),\n      \'image/object/difficult\': dataset_util.int64_list_feature(difficult_obj),\n      \'image/object/truncated\': dataset_util.int64_list_feature(truncated),\n      \'image/object/view\': dataset_util.bytes_list_feature(poses),\n  }))\n  return example\n\n\ndef create_tf_record(output_filename,\n                     label_map_dict,\n                     annotations_dir,\n                     image_dir,\n                     examples):\n  """"""Creates a TFRecord file from examples.\n\n  Args:\n    output_filename: Path to where output file is saved.\n    label_map_dict: The label map dictionary.\n    annotations_dir: Directory where annotation files are stored.\n    image_dir: Directory where image files are stored.\n    examples: Examples to parse and save to tf record.\n  """"""\n  writer = tf.python_io.TFRecordWriter(output_filename)\n  for idx, example in enumerate(examples):\n    if idx % 100 == 0:\n      logging.info(\'On image %d of %d\', idx, len(examples))\n    path = os.path.join(annotations_dir, \'xmls\', example + \'.xml\')\n\n    if not os.path.exists(path):\n      logging.warning(\'Could not find %s, ignoring example.\', path)\n      continue\n    with tf.gfile.GFile(path, \'r\') as fid:\n      xml_str = fid.read()\n    xml = etree.fromstring(xml_str)\n    data = dataset_util.recursive_parse_xml_to_dict(xml)[\'annotation\']\n\n    tf_example = dict_to_tf_example(data, label_map_dict, image_dir)\n    writer.write(tf_example.SerializeToString())\n\n  writer.close()\n\n\n# TODO: Add test for pet/PASCAL main files.\ndef main(_):\n  data_dir = FLAGS.data_dir\n  label_map_dict = label_map_util.get_label_map_dict(FLAGS.label_map_path)\n\n  logging.info(\'Reading from Pet dataset.\')\n  image_dir = os.path.join(data_dir, \'images\')\n  annotations_dir = os.path.join(data_dir, \'annotations\')\n  examples_path = os.path.join(annotations_dir, \'trainval.txt\')\n  examples_list = dataset_util.read_examples_list(examples_path)\n\n  # Test images are not included in the downloaded data set, so we shall perform\n  # our own split.\n  random.seed(42)\n  random.shuffle(examples_list)\n  num_examples = len(examples_list)\n  num_train = int(0.7 * num_examples)\n  train_examples = examples_list[:num_train]\n  val_examples = examples_list[num_train:]\n  logging.info(\'%d training and %d validation examples.\',\n               len(train_examples), len(val_examples))\n\n  train_output_path = os.path.join(FLAGS.output_dir, \'pet_train.record\')\n  val_output_path = os.path.join(FLAGS.output_dir, \'pet_val.record\')\n  create_tf_record(train_output_path, label_map_dict, annotations_dir,\n                   image_dir, train_examples)\n  create_tf_record(val_output_path, label_map_dict, annotations_dir,\n                   image_dir, val_examples)\n\nif __name__ == \'__main__\':\n  tf.app.run()\n'"
src/object_detection/eval.py,6,"b'# Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nr""""""Evaluation executable for detection models.\n\nThis executable is used to evaluate DetectionModels. There are two ways of\nconfiguring the eval job.\n\n1) A single pipeline_pb2.TrainEvalPipelineConfig file maybe specified instead.\nIn this mode, the --eval_training_data flag may be given to force the pipeline\nto evaluate on training data instead.\n\nExample usage:\n    ./eval \\\n        --logtostderr \\\n        --checkpoint_dir=path/to/checkpoint_dir \\\n        --eval_dir=path/to/eval_dir \\\n        --pipeline_config_path=pipeline_config.pbtxt\n\n2) Three configuration files may be provided: a model_pb2.DetectionModel\nconfiguration file to define what type of DetectionModel is being evaluated, an\ninput_reader_pb2.InputReader file to specify what data the model is evaluating\nand an eval_pb2.EvalConfig file to configure evaluation parameters.\n\nExample usage:\n    ./eval \\\n        --logtostderr \\\n        --checkpoint_dir=path/to/checkpoint_dir \\\n        --eval_dir=path/to/eval_dir \\\n        --eval_config_path=eval_config.pbtxt \\\n        --model_config_path=model_config.pbtxt \\\n        --input_config_path=eval_input_config.pbtxt\n""""""\nimport functools\nimport os\nimport tensorflow as tf\n\nfrom object_detection import evaluator\nfrom object_detection.builders import dataset_builder\nfrom object_detection.builders import model_builder\nfrom object_detection.utils import config_util\nfrom object_detection.utils import dataset_util\nfrom object_detection.utils import label_map_util\n\n\ntf.logging.set_verbosity(tf.logging.INFO)\n\nflags = tf.app.flags\nflags.DEFINE_boolean(\'eval_training_data\', False,\n                     \'If training data should be evaluated for this job.\')\nflags.DEFINE_string(\'checkpoint_dir\', \'\',\n                    \'Directory containing checkpoints to evaluate, typically \'\n                    \'set to `train_dir` used in the training job.\')\nflags.DEFINE_string(\'eval_dir\', \'\',\n                    \'Directory to write eval summaries to.\')\nflags.DEFINE_string(\'pipeline_config_path\', \'\',\n                    \'Path to a pipeline_pb2.TrainEvalPipelineConfig config \'\n                    \'file. If provided, other configs are ignored\')\nflags.DEFINE_string(\'eval_config_path\', \'\',\n                    \'Path to an eval_pb2.EvalConfig config file.\')\nflags.DEFINE_string(\'input_config_path\', \'\',\n                    \'Path to an input_reader_pb2.InputReader config file.\')\nflags.DEFINE_string(\'model_config_path\', \'\',\n                    \'Path to a model_pb2.DetectionModel config file.\')\nflags.DEFINE_boolean(\'run_once\', False, \'Option to only run a single pass of \'\n                     \'evaluation. Overrides the `max_evals` parameter in the \'\n                     \'provided config.\')\nFLAGS = flags.FLAGS\n\n\ndef main(unused_argv):\n  assert FLAGS.checkpoint_dir, \'`checkpoint_dir` is missing.\'\n  assert FLAGS.eval_dir, \'`eval_dir` is missing.\'\n  tf.gfile.MakeDirs(FLAGS.eval_dir)\n  if FLAGS.pipeline_config_path:\n    configs = config_util.get_configs_from_pipeline_file(\n        FLAGS.pipeline_config_path)\n    tf.gfile.Copy(FLAGS.pipeline_config_path,\n                  os.path.join(FLAGS.eval_dir, \'pipeline.config\'),\n                  overwrite=True)\n  else:\n    configs = config_util.get_configs_from_multiple_files(\n        model_config_path=FLAGS.model_config_path,\n        eval_config_path=FLAGS.eval_config_path,\n        eval_input_config_path=FLAGS.input_config_path)\n    for name, config in [(\'model.config\', FLAGS.model_config_path),\n                         (\'eval.config\', FLAGS.eval_config_path),\n                         (\'input.config\', FLAGS.input_config_path)]:\n      tf.gfile.Copy(config,\n                    os.path.join(FLAGS.eval_dir, name),\n                    overwrite=True)\n\n  model_config = configs[\'model\']\n  eval_config = configs[\'eval_config\']\n  input_config = configs[\'eval_input_config\']\n  if FLAGS.eval_training_data:\n    input_config = configs[\'train_input_config\']\n\n  model_fn = functools.partial(\n      model_builder.build,\n      model_config=model_config,\n      is_training=False)\n\n  def get_next(config):\n    return dataset_util.make_initializable_iterator(\n        dataset_builder.build(config)).get_next()\n\n  create_input_dict_fn = functools.partial(get_next, input_config)\n\n  label_map = label_map_util.load_labelmap(input_config.label_map_path)\n  max_num_classes = max([item.id for item in label_map.item])\n  categories = label_map_util.convert_label_map_to_categories(\n      label_map, max_num_classes)\n\n  if FLAGS.run_once:\n    eval_config.max_evals = 1\n\n  evaluator.evaluate(create_input_dict_fn, model_fn, eval_config, categories,\n                     FLAGS.checkpoint_dir, FLAGS.eval_dir)\n\n\nif __name__ == \'__main__\':\n  tf.app.run()\n'"
src/object_detection/eval_util.py,39,"b'# Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Common utility functions for evaluation.""""""\nimport collections\nimport logging\nimport os\nimport time\n\nimport numpy as np\nimport tensorflow as tf\n\nfrom object_detection.core import box_list\nfrom object_detection.core import box_list_ops\nfrom object_detection.core import keypoint_ops\nfrom object_detection.core import standard_fields as fields\nfrom object_detection.metrics import coco_evaluation\nfrom object_detection.utils import label_map_util\nfrom object_detection.utils import ops\nfrom object_detection.utils import visualization_utils as vis_utils\n\nslim = tf.contrib.slim\n\n\ndef write_metrics(metrics, global_step, summary_dir):\n  """"""Write metrics to a summary directory.\n\n  Args:\n    metrics: A dictionary containing metric names and values.\n    global_step: Global step at which the metrics are computed.\n    summary_dir: Directory to write tensorflow summaries to.\n  """"""\n  logging.info(\'Writing metrics to tf summary.\')\n  summary_writer = tf.summary.FileWriterCache.get(summary_dir)\n  for key in sorted(metrics):\n    summary = tf.Summary(value=[\n        tf.Summary.Value(tag=key, simple_value=metrics[key]),\n    ])\n    summary_writer.add_summary(summary, global_step)\n    logging.info(\'%s: %f\', key, metrics[key])\n  logging.info(\'Metrics written to tf summary.\')\n\n\n# TODO(rathodv): Add tests.\ndef visualize_detection_results(result_dict,\n                                tag,\n                                global_step,\n                                categories,\n                                summary_dir=\'\',\n                                export_dir=\'\',\n                                agnostic_mode=False,\n                                show_groundtruth=False,\n                                groundtruth_box_visualization_color=\'black\',\n                                min_score_thresh=.5,\n                                max_num_predictions=20,\n                                skip_scores=False,\n                                skip_labels=False,\n                                keep_image_id_for_visualization_export=False):\n  """"""Visualizes detection results and writes visualizations to image summaries.\n\n  This function visualizes an image with its detected bounding boxes and writes\n  to image summaries which can be viewed on tensorboard.  It optionally also\n  writes images to a directory. In the case of missing entry in the label map,\n  unknown class name in the visualization is shown as ""N/A"".\n\n  Args:\n    result_dict: a dictionary holding groundtruth and detection\n      data corresponding to each image being evaluated.  The following keys\n      are required:\n        \'original_image\': a numpy array representing the image with shape\n          [1, height, width, 3]\n        \'detection_boxes\': a numpy array of shape [N, 4]\n        \'detection_scores\': a numpy array of shape [N]\n        \'detection_classes\': a numpy array of shape [N]\n      The following keys are optional:\n        \'groundtruth_boxes\': a numpy array of shape [N, 4]\n        \'groundtruth_keypoints\': a numpy array of shape [N, num_keypoints, 2]\n      Detections are assumed to be provided in decreasing order of score and for\n      display, and we assume that scores are probabilities between 0 and 1.\n    tag: tensorboard tag (string) to associate with image.\n    global_step: global step at which the visualization are generated.\n    categories: a list of dictionaries representing all possible categories.\n      Each dict in this list has the following keys:\n          \'id\': (required) an integer id uniquely identifying this category\n          \'name\': (required) string representing category name\n            e.g., \'cat\', \'dog\', \'pizza\'\n          \'supercategory\': (optional) string representing the supercategory\n            e.g., \'animal\', \'vehicle\', \'food\', etc\n    summary_dir: the output directory to which the image summaries are written.\n    export_dir: the output directory to which images are written.  If this is\n      empty (default), then images are not exported.\n    agnostic_mode: boolean (default: False) controlling whether to evaluate in\n      class-agnostic mode or not.\n    show_groundtruth: boolean (default: False) controlling whether to show\n      groundtruth boxes in addition to detected boxes\n    groundtruth_box_visualization_color: box color for visualizing groundtruth\n      boxes\n    min_score_thresh: minimum score threshold for a box to be visualized\n    max_num_predictions: maximum number of detections to visualize\n    skip_scores: whether to skip score when drawing a single detection\n    skip_labels: whether to skip label when drawing a single detection\n    keep_image_id_for_visualization_export: whether to keep image identifier in\n      filename when exported to export_dir\n  Raises:\n    ValueError: if result_dict does not contain the expected keys (i.e.,\n      \'original_image\', \'detection_boxes\', \'detection_scores\',\n      \'detection_classes\')\n  """"""\n  detection_fields = fields.DetectionResultFields\n  input_fields = fields.InputDataFields\n  if not set([\n      input_fields.original_image,\n      detection_fields.detection_boxes,\n      detection_fields.detection_scores,\n      detection_fields.detection_classes,\n  ]).issubset(set(result_dict.keys())):\n    raise ValueError(\'result_dict does not contain all expected keys.\')\n  if show_groundtruth and input_fields.groundtruth_boxes not in result_dict:\n    raise ValueError(\'If show_groundtruth is enabled, result_dict must contain \'\n                     \'groundtruth_boxes.\')\n  logging.info(\'Creating detection visualizations.\')\n  category_index = label_map_util.create_category_index(categories)\n\n  image = np.squeeze(result_dict[input_fields.original_image], axis=0)\n  detection_boxes = result_dict[detection_fields.detection_boxes]\n  detection_scores = result_dict[detection_fields.detection_scores]\n  detection_classes = np.int32((result_dict[\n      detection_fields.detection_classes]))\n  detection_keypoints = result_dict.get(detection_fields.detection_keypoints)\n  detection_masks = result_dict.get(detection_fields.detection_masks)\n  detection_boundaries = result_dict.get(detection_fields.detection_boundaries)\n\n  # Plot groundtruth underneath detections\n  if show_groundtruth:\n    groundtruth_boxes = result_dict[input_fields.groundtruth_boxes]\n    groundtruth_keypoints = result_dict.get(input_fields.groundtruth_keypoints)\n    vis_utils.visualize_boxes_and_labels_on_image_array(\n        image=image,\n        boxes=groundtruth_boxes,\n        classes=None,\n        scores=None,\n        category_index=category_index,\n        keypoints=groundtruth_keypoints,\n        use_normalized_coordinates=False,\n        max_boxes_to_draw=None,\n        groundtruth_box_visualization_color=groundtruth_box_visualization_color)\n  vis_utils.visualize_boxes_and_labels_on_image_array(\n      image,\n      detection_boxes,\n      detection_classes,\n      detection_scores,\n      category_index,\n      instance_masks=detection_masks,\n      instance_boundaries=detection_boundaries,\n      keypoints=detection_keypoints,\n      use_normalized_coordinates=False,\n      max_boxes_to_draw=max_num_predictions,\n      min_score_thresh=min_score_thresh,\n      agnostic_mode=agnostic_mode,\n      skip_scores=skip_scores,\n      skip_labels=skip_labels)\n\n  if export_dir:\n    if keep_image_id_for_visualization_export and result_dict[fields.\n                                                              InputDataFields()\n                                                              .key]:\n      export_path = os.path.join(export_dir, \'export-{}-{}.png\'.format(\n          tag, result_dict[fields.InputDataFields().key]))\n    else:\n      export_path = os.path.join(export_dir, \'export-{}.png\'.format(tag))\n    vis_utils.save_image_array_as_png(image, export_path)\n\n  summary = tf.Summary(value=[\n      tf.Summary.Value(\n          tag=tag,\n          image=tf.Summary.Image(\n              encoded_image_string=vis_utils.encode_image_array_as_png_str(\n                  image)))\n  ])\n  summary_writer = tf.summary.FileWriterCache.get(summary_dir)\n  summary_writer.add_summary(summary, global_step)\n\n  logging.info(\'Detection visualizations written to summary with tag %s.\', tag)\n\n\ndef _run_checkpoint_once(tensor_dict,\n                         evaluators=None,\n                         batch_processor=None,\n                         checkpoint_dirs=None,\n                         variables_to_restore=None,\n                         restore_fn=None,\n                         num_batches=1,\n                         master=\'\',\n                         save_graph=False,\n                         save_graph_dir=\'\',\n                         losses_dict=None):\n  """"""Evaluates metrics defined in evaluators and returns summaries.\n\n  This function loads the latest checkpoint in checkpoint_dirs and evaluates\n  all metrics defined in evaluators. The metrics are processed in batch by the\n  batch_processor.\n\n  Args:\n    tensor_dict: a dictionary holding tensors representing a batch of detections\n      and corresponding groundtruth annotations.\n    evaluators: a list of object of type DetectionEvaluator to be used for\n      evaluation. Note that the metric names produced by different evaluators\n      must be unique.\n    batch_processor: a function taking four arguments:\n      1. tensor_dict: the same tensor_dict that is passed in as the first\n        argument to this function.\n      2. sess: a tensorflow session\n      3. batch_index: an integer representing the index of the batch amongst\n        all batches\n      By default, batch_processor is None, which defaults to running:\n        return sess.run(tensor_dict)\n      To skip an image, it suffices to return an empty dictionary in place of\n      result_dict.\n    checkpoint_dirs: list of directories to load into an EnsembleModel. If it\n      has only one directory, EnsembleModel will not be used --\n        a DetectionModel\n      will be instantiated directly. Not used if restore_fn is set.\n    variables_to_restore: None, or a dictionary mapping variable names found in\n      a checkpoint to model variables. The dictionary would normally be\n      generated by creating a tf.train.ExponentialMovingAverage object and\n      calling its variables_to_restore() method. Not used if restore_fn is set.\n    restore_fn: None, or a function that takes a tf.Session object and correctly\n      restores all necessary variables from the correct checkpoint file. If\n      None, attempts to restore from the first directory in checkpoint_dirs.\n    num_batches: the number of batches to use for evaluation.\n    master: the location of the Tensorflow session.\n    save_graph: whether or not the Tensorflow graph is stored as a pbtxt file.\n    save_graph_dir: where to store the Tensorflow graph on disk. If save_graph\n      is True this must be non-empty.\n    losses_dict: optional dictionary of scalar detection losses.\n\n  Returns:\n    global_step: the count of global steps.\n    all_evaluator_metrics: A dictionary containing metric names and values.\n\n  Raises:\n    ValueError: if restore_fn is None and checkpoint_dirs doesn\'t have at least\n      one element.\n    ValueError: if save_graph is True and save_graph_dir is not defined.\n  """"""\n  if save_graph and not save_graph_dir:\n    raise ValueError(\'`save_graph_dir` must be defined.\')\n  sess = tf.Session(master, graph=tf.get_default_graph())\n  sess.run(tf.global_variables_initializer())\n  sess.run(tf.local_variables_initializer())\n  sess.run(tf.tables_initializer())\n  if restore_fn:\n    restore_fn(sess)\n  else:\n    if not checkpoint_dirs:\n      raise ValueError(\'`checkpoint_dirs` must have at least one entry.\')\n    checkpoint_file = tf.train.latest_checkpoint(checkpoint_dirs[0])\n    saver = tf.train.Saver(variables_to_restore)\n    saver.restore(sess, checkpoint_file)\n\n  if save_graph:\n    tf.train.write_graph(sess.graph_def, save_graph_dir, \'eval.pbtxt\')\n\n  counters = {\'skipped\': 0, \'success\': 0}\n  aggregate_result_losses_dict = collections.defaultdict(list)\n  with tf.contrib.slim.queues.QueueRunners(sess):\n    try:\n      for batch in range(int(num_batches)):\n        if (batch + 1) % 100 == 0:\n          logging.info(\'Running eval ops batch %d/%d\', batch + 1, num_batches)\n        if not batch_processor:\n          try:\n            if not losses_dict:\n              losses_dict = {}\n            result_dict, result_losses_dict = sess.run([tensor_dict,\n                                                        losses_dict])\n            counters[\'success\'] += 1\n          except tf.errors.InvalidArgumentError:\n            logging.info(\'Skipping image\')\n            counters[\'skipped\'] += 1\n            result_dict = {}\n        else:\n          result_dict, result_losses_dict = batch_processor(\n              tensor_dict, sess, batch, counters, losses_dict=losses_dict)\n        if not result_dict:\n          continue\n        for key, value in iter(result_losses_dict.items()):\n          aggregate_result_losses_dict[key].append(value)\n        for evaluator in evaluators:\n          # TODO(b/65130867): Use image_id tensor once we fix the input data\n          # decoders to return correct image_id.\n          # TODO(akuznetsa): result_dict contains batches of images, while\n          # add_single_ground_truth_image_info expects a single image. Fix\n          evaluator.add_single_ground_truth_image_info(\n              image_id=batch, groundtruth_dict=result_dict)\n          evaluator.add_single_detected_image_info(\n              image_id=batch, detections_dict=result_dict)\n      logging.info(\'Running eval batches done.\')\n    except tf.errors.OutOfRangeError:\n      logging.info(\'Done evaluating -- epoch limit reached\')\n    finally:\n      # When done, ask the threads to stop.\n      logging.info(\'# success: %d\', counters[\'success\'])\n      logging.info(\'# skipped: %d\', counters[\'skipped\'])\n      all_evaluator_metrics = {}\n      for evaluator in evaluators:\n        metrics = evaluator.evaluate()\n        evaluator.clear()\n        if any(key in all_evaluator_metrics for key in metrics):\n          raise ValueError(\'Metric names between evaluators must not collide.\')\n        all_evaluator_metrics.update(metrics)\n      global_step = tf.train.global_step(sess, tf.train.get_global_step())\n\n      for key, value in iter(aggregate_result_losses_dict.items()):\n        all_evaluator_metrics[\'Losses/\' + key] = np.mean(value)\n  sess.close()\n  return (global_step, all_evaluator_metrics)\n\n\n# TODO(rathodv): Add tests.\ndef repeated_checkpoint_run(tensor_dict,\n                            summary_dir,\n                            evaluators,\n                            batch_processor=None,\n                            checkpoint_dirs=None,\n                            variables_to_restore=None,\n                            restore_fn=None,\n                            num_batches=1,\n                            eval_interval_secs=120,\n                            max_number_of_evaluations=None,\n                            master=\'\',\n                            save_graph=False,\n                            save_graph_dir=\'\',\n                            losses_dict=None):\n  """"""Periodically evaluates desired tensors using checkpoint_dirs or restore_fn.\n\n  This function repeatedly loads a checkpoint and evaluates a desired\n  set of tensors (provided by tensor_dict) and hands the resulting numpy\n  arrays to a function result_processor which can be used to further\n  process/save/visualize the results.\n\n  Args:\n    tensor_dict: a dictionary holding tensors representing a batch of detections\n      and corresponding groundtruth annotations.\n    summary_dir: a directory to write metrics summaries.\n    evaluators: a list of object of type DetectionEvaluator to be used for\n      evaluation. Note that the metric names produced by different evaluators\n      must be unique.\n    batch_processor: a function taking three arguments:\n      1. tensor_dict: the same tensor_dict that is passed in as the first\n        argument to this function.\n      2. sess: a tensorflow session\n      3. batch_index: an integer representing the index of the batch amongst\n        all batches\n      By default, batch_processor is None, which defaults to running:\n        return sess.run(tensor_dict)\n    checkpoint_dirs: list of directories to load into a DetectionModel or an\n      EnsembleModel if restore_fn isn\'t set. Also used to determine when to run\n      next evaluation. Must have at least one element.\n    variables_to_restore: None, or a dictionary mapping variable names found in\n      a checkpoint to model variables. The dictionary would normally be\n      generated by creating a tf.train.ExponentialMovingAverage object and\n      calling its variables_to_restore() method. Not used if restore_fn is set.\n    restore_fn: a function that takes a tf.Session object and correctly restores\n      all necessary variables from the correct checkpoint file.\n    num_batches: the number of batches to use for evaluation.\n    eval_interval_secs: the number of seconds between each evaluation run.\n    max_number_of_evaluations: the max number of iterations of the evaluation.\n      If the value is left as None the evaluation continues indefinitely.\n    master: the location of the Tensorflow session.\n    save_graph: whether or not the Tensorflow graph is saved as a pbtxt file.\n    save_graph_dir: where to save on disk the Tensorflow graph. If store_graph\n      is True this must be non-empty.\n    losses_dict: optional dictionary of scalar detection losses.\n\n  Returns:\n    metrics: A dictionary containing metric names and values in the latest\n      evaluation.\n\n  Raises:\n    ValueError: if max_num_of_evaluations is not None or a positive number.\n    ValueError: if checkpoint_dirs doesn\'t have at least one element.\n  """"""\n  if max_number_of_evaluations and max_number_of_evaluations <= 0:\n    raise ValueError(\n        \'`number_of_steps` must be either None or a positive number.\')\n\n  if not checkpoint_dirs:\n    raise ValueError(\'`checkpoint_dirs` must have at least one entry.\')\n\n  last_evaluated_model_path = None\n  number_of_evaluations = 0\n  while True:\n    start = time.time()\n    logging.info(\'Starting evaluation at \' + time.strftime(\n        \'%Y-%m-%d-%H:%M:%S\', time.gmtime()))\n    model_path = tf.train.latest_checkpoint(checkpoint_dirs[0])\n    if not model_path:\n      logging.info(\'No model found in %s. Will try again in %d seconds\',\n                   checkpoint_dirs[0], eval_interval_secs)\n    elif model_path == last_evaluated_model_path:\n      logging.info(\'Found already evaluated checkpoint. Will try again in %d \'\n                   \'seconds\', eval_interval_secs)\n    else:\n      last_evaluated_model_path = model_path\n      global_step, metrics = _run_checkpoint_once(tensor_dict, evaluators,\n                                                  batch_processor,\n                                                  checkpoint_dirs,\n                                                  variables_to_restore,\n                                                  restore_fn, num_batches,\n                                                  master, save_graph,\n                                                  save_graph_dir,\n                                                  losses_dict=losses_dict)\n      write_metrics(metrics, global_step, summary_dir)\n    number_of_evaluations += 1\n\n    if (max_number_of_evaluations and\n        number_of_evaluations >= max_number_of_evaluations):\n      logging.info(\'Finished evaluation!\')\n      break\n    time_to_next_eval = start + eval_interval_secs - time.time()\n    if time_to_next_eval > 0:\n      time.sleep(time_to_next_eval)\n\n  return metrics\n\n\ndef result_dict_for_single_example(image,\n                                   key,\n                                   detections,\n                                   groundtruth=None,\n                                   class_agnostic=False,\n                                   scale_to_absolute=False):\n  """"""Merges all detection and groundtruth information for a single example.\n\n  Note that evaluation tools require classes that are 1-indexed, and so this\n  function performs the offset. If `class_agnostic` is True, all output classes\n  have label 1.\n\n  Args:\n    image: A single 4D uint8 image tensor of shape [1, H, W, C].\n    key: A single string tensor identifying the image.\n    detections: A dictionary of detections, returned from\n      DetectionModel.postprocess().\n    groundtruth: (Optional) Dictionary of groundtruth items, with fields:\n      \'groundtruth_boxes\': [num_boxes, 4] float32 tensor of boxes, in\n        normalized coordinates.\n      \'groundtruth_classes\': [num_boxes] int64 tensor of 1-indexed classes.\n      \'groundtruth_area\': [num_boxes] float32 tensor of bbox area. (Optional)\n      \'groundtruth_is_crowd\': [num_boxes] int64 tensor. (Optional)\n      \'groundtruth_difficult\': [num_boxes] int64 tensor. (Optional)\n      \'groundtruth_group_of\': [num_boxes] int64 tensor. (Optional)\n      \'groundtruth_instance_masks\': 3D int64 tensor of instance masks\n        (Optional).\n    class_agnostic: Boolean indicating whether the detections are class-agnostic\n      (i.e. binary). Default False.\n    scale_to_absolute: Boolean indicating whether boxes and keypoints should be\n      scaled to absolute coordinates. Note that for IoU based evaluations, it\n      does not matter whether boxes are expressed in absolute or relative\n      coordinates. Default False.\n\n  Returns:\n    A dictionary with:\n    \'original_image\': A [1, H, W, C] uint8 image tensor.\n    \'key\': A string tensor with image identifier.\n    \'detection_boxes\': [max_detections, 4] float32 tensor of boxes, in\n      normalized or absolute coordinates, depending on the value of\n      `scale_to_absolute`.\n    \'detection_scores\': [max_detections] float32 tensor of scores.\n    \'detection_classes\': [max_detections] int64 tensor of 1-indexed classes.\n    \'detection_masks\': [max_detections, H, W] float32 tensor of binarized\n      masks, reframed to full image masks.\n    \'groundtruth_boxes\': [num_boxes, 4] float32 tensor of boxes, in\n      normalized or absolute coordinates, depending on the value of\n      `scale_to_absolute`. (Optional)\n    \'groundtruth_classes\': [num_boxes] int64 tensor of 1-indexed classes.\n      (Optional)\n    \'groundtruth_area\': [num_boxes] float32 tensor of bbox area. (Optional)\n    \'groundtruth_is_crowd\': [num_boxes] int64 tensor. (Optional)\n    \'groundtruth_difficult\': [num_boxes] int64 tensor. (Optional)\n    \'groundtruth_group_of\': [num_boxes] int64 tensor. (Optional)\n    \'groundtruth_instance_masks\': 3D int64 tensor of instance masks\n      (Optional).\n\n  """"""\n  label_id_offset = 1  # Applying label id offset (b/63711816)\n\n  input_data_fields = fields.InputDataFields\n  output_dict = {\n      input_data_fields.original_image: image,\n      input_data_fields.key: key,\n  }\n\n  detection_fields = fields.DetectionResultFields\n  detection_boxes = detections[detection_fields.detection_boxes][0]\n  image_shape = tf.shape(image)\n  detection_scores = detections[detection_fields.detection_scores][0]\n\n  if class_agnostic:\n    detection_classes = tf.ones_like(detection_scores, dtype=tf.int64)\n  else:\n    detection_classes = (\n        tf.to_int64(detections[detection_fields.detection_classes][0]) +\n        label_id_offset)\n\n  num_detections = tf.to_int32(detections[detection_fields.num_detections][0])\n  detection_boxes = tf.slice(\n      detection_boxes, begin=[0, 0], size=[num_detections, -1])\n  detection_classes = tf.slice(\n      detection_classes, begin=[0], size=[num_detections])\n  detection_scores = tf.slice(\n      detection_scores, begin=[0], size=[num_detections])\n\n  if scale_to_absolute:\n    absolute_detection_boxlist = box_list_ops.to_absolute_coordinates(\n        box_list.BoxList(detection_boxes), image_shape[1], image_shape[2])\n    output_dict[detection_fields.detection_boxes] = (\n        absolute_detection_boxlist.get())\n  else:\n    output_dict[detection_fields.detection_boxes] = detection_boxes\n  output_dict[detection_fields.detection_classes] = detection_classes\n  output_dict[detection_fields.detection_scores] = detection_scores\n\n  if detection_fields.detection_masks in detections:\n    detection_masks = detections[detection_fields.detection_masks][0]\n    # TODO(rathodv): This should be done in model\'s postprocess\n    # function ideally.\n    detection_masks = tf.slice(\n        detection_masks, begin=[0, 0, 0], size=[num_detections, -1, -1])\n    detection_masks_reframed = ops.reframe_box_masks_to_image_masks(\n        detection_masks, detection_boxes, image_shape[1], image_shape[2])\n    detection_masks_reframed = tf.cast(\n        tf.greater(detection_masks_reframed, 0.5), tf.uint8)\n    output_dict[detection_fields.detection_masks] = detection_masks_reframed\n  if detection_fields.detection_keypoints in detections:\n    detection_keypoints = detections[detection_fields.detection_keypoints][0]\n    output_dict[detection_fields.detection_keypoints] = detection_keypoints\n    if scale_to_absolute:\n      absolute_detection_keypoints = keypoint_ops.scale(\n          detection_keypoints, image_shape[1], image_shape[2])\n      output_dict[detection_fields.detection_keypoints] = (\n          absolute_detection_keypoints)\n\n  if groundtruth:\n    if input_data_fields.groundtruth_instance_masks in groundtruth:\n      groundtruth[input_data_fields.groundtruth_instance_masks] = tf.cast(\n          groundtruth[input_data_fields.groundtruth_instance_masks], tf.uint8)\n    output_dict.update(groundtruth)\n    if scale_to_absolute:\n      groundtruth_boxes = groundtruth[input_data_fields.groundtruth_boxes]\n      absolute_gt_boxlist = box_list_ops.to_absolute_coordinates(\n          box_list.BoxList(groundtruth_boxes), image_shape[1], image_shape[2])\n      output_dict[input_data_fields.groundtruth_boxes] = (\n          absolute_gt_boxlist.get())\n    # For class-agnostic models, groundtruth classes all become 1.\n    if class_agnostic:\n      groundtruth_classes = groundtruth[input_data_fields.groundtruth_classes]\n      groundtruth_classes = tf.ones_like(groundtruth_classes, dtype=tf.int64)\n      output_dict[input_data_fields.groundtruth_classes] = groundtruth_classes\n\n  return output_dict\n\n\ndef get_eval_metric_ops_for_evaluators(evaluation_metrics,\n                                       categories,\n                                       eval_dict,\n                                       include_metrics_per_category=False):\n  """"""Returns a dictionary of eval metric ops to use with `tf.EstimatorSpec`.\n\n  Args:\n    evaluation_metrics: List of evaluation metric names. Current options are\n      \'coco_detection_metrics\' and \'coco_mask_metrics\'.\n    categories: A list of dicts, each of which has the following keys -\n        \'id\': (required) an integer id uniquely identifying this category.\n        \'name\': (required) string representing category name e.g., \'cat\', \'dog\'.\n    eval_dict: An evaluation dictionary, returned from\n      result_dict_for_single_example().\n    include_metrics_per_category: If True, include metrics for each category.\n\n  Returns:\n    A dictionary of metric names to tuple of value_op and update_op that can be\n    used as eval metric ops in tf.EstimatorSpec.\n\n  Raises:\n    ValueError: If any of the metrics in `evaluation_metric` is not\n    \'coco_detection_metrics\' or \'coco_mask_metrics\'.\n  """"""\n  evaluation_metrics = list(set(evaluation_metrics))\n\n  input_data_fields = fields.InputDataFields\n  detection_fields = fields.DetectionResultFields\n  eval_metric_ops = {}\n  for metric in evaluation_metrics:\n    if metric == \'coco_detection_metrics\':\n      coco_evaluator = coco_evaluation.CocoDetectionEvaluator(\n          categories, include_metrics_per_category=include_metrics_per_category)\n      eval_metric_ops.update(\n          coco_evaluator.get_estimator_eval_metric_ops(\n              image_id=eval_dict[input_data_fields.key],\n              groundtruth_boxes=eval_dict[input_data_fields.groundtruth_boxes],\n              groundtruth_classes=eval_dict[\n                  input_data_fields.groundtruth_classes],\n              detection_boxes=eval_dict[detection_fields.detection_boxes],\n              detection_scores=eval_dict[detection_fields.detection_scores],\n              detection_classes=eval_dict[detection_fields.detection_classes]))\n    elif metric == \'coco_mask_metrics\':\n      coco_mask_evaluator = coco_evaluation.CocoMaskEvaluator(\n          categories, include_metrics_per_category=include_metrics_per_category)\n      eval_metric_ops.update(\n          coco_mask_evaluator.get_estimator_eval_metric_ops(\n              image_id=eval_dict[input_data_fields.key],\n              groundtruth_boxes=eval_dict[input_data_fields.groundtruth_boxes],\n              groundtruth_classes=eval_dict[\n                  input_data_fields.groundtruth_classes],\n              groundtruth_instance_masks=eval_dict[\n                  input_data_fields.groundtruth_instance_masks],\n              detection_scores=eval_dict[detection_fields.detection_scores],\n              detection_classes=eval_dict[detection_fields.detection_classes],\n              detection_masks=eval_dict[detection_fields.detection_masks]))\n    else:\n      raise ValueError(\'The only evaluation metrics supported are \'\n                       \'""coco_detection_metrics"" and ""coco_mask_metrics"". \'\n                       \'Found {} in the evaluation metrics\'.format(metric))\n\n  return eval_metric_ops\n\n\n'"
src/object_detection/eval_util_test.py,12,"b'# Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Tests for eval_util.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\n\n\nfrom object_detection import eval_util\nfrom object_detection.core import standard_fields as fields\n\n\nclass EvalUtilTest(tf.test.TestCase):\n\n  def _get_categories_list(self):\n    return [{\'id\': 0, \'name\': \'person\'},\n            {\'id\': 1, \'name\': \'dog\'},\n            {\'id\': 2, \'name\': \'cat\'}]\n\n  def _make_evaluation_dict(self):\n    input_data_fields = fields.InputDataFields\n    detection_fields = fields.DetectionResultFields\n\n    image = tf.zeros(shape=[1, 20, 20, 3], dtype=tf.uint8)\n    key = tf.constant(\'image1\')\n    detection_boxes = tf.constant([[[0., 0., 1., 1.]]])\n    detection_scores = tf.constant([[0.8]])\n    detection_classes = tf.constant([[0]])\n    detection_masks = tf.ones(shape=[1, 1, 20, 20], dtype=tf.float32)\n    num_detections = tf.constant([1])\n    groundtruth_boxes = tf.constant([[0., 0., 1., 1.]])\n    groundtruth_classes = tf.constant([1])\n    groundtruth_instance_masks = tf.ones(shape=[1, 20, 20], dtype=tf.uint8)\n    detections = {\n        detection_fields.detection_boxes: detection_boxes,\n        detection_fields.detection_scores: detection_scores,\n        detection_fields.detection_classes: detection_classes,\n        detection_fields.detection_masks: detection_masks,\n        detection_fields.num_detections: num_detections\n    }\n    groundtruth = {\n        input_data_fields.groundtruth_boxes: groundtruth_boxes,\n        input_data_fields.groundtruth_classes: groundtruth_classes,\n        input_data_fields.groundtruth_instance_masks: groundtruth_instance_masks\n    }\n    return eval_util.result_dict_for_single_example(image, key, detections,\n                                                    groundtruth)\n\n  def test_get_eval_metric_ops_for_coco_detections(self):\n    evaluation_metrics = [\'coco_detection_metrics\']\n    categories = self._get_categories_list()\n    eval_dict = self._make_evaluation_dict()\n    metric_ops = eval_util.get_eval_metric_ops_for_evaluators(\n        evaluation_metrics, categories, eval_dict)\n    _, update_op = metric_ops[\'DetectionBoxes_Precision/mAP\']\n\n    with self.test_session() as sess:\n      metrics = {}\n      for key, (value_op, _) in metric_ops.iteritems():\n        metrics[key] = value_op\n      sess.run(update_op)\n      metrics = sess.run(metrics)\n      print(metrics)\n      self.assertAlmostEqual(1.0, metrics[\'DetectionBoxes_Precision/mAP\'])\n      self.assertNotIn(\'DetectionMasks_Precision/mAP\', metrics)\n\n  def test_get_eval_metric_ops_for_coco_detections_and_masks(self):\n    evaluation_metrics = [\'coco_detection_metrics\',\n                          \'coco_mask_metrics\']\n    categories = self._get_categories_list()\n    eval_dict = self._make_evaluation_dict()\n    metric_ops = eval_util.get_eval_metric_ops_for_evaluators(\n        evaluation_metrics, categories, eval_dict)\n    _, update_op_boxes = metric_ops[\'DetectionBoxes_Precision/mAP\']\n    _, update_op_masks = metric_ops[\'DetectionMasks_Precision/mAP\']\n\n    with self.test_session() as sess:\n      metrics = {}\n      for key, (value_op, _) in metric_ops.iteritems():\n        metrics[key] = value_op\n      sess.run(update_op_boxes)\n      sess.run(update_op_masks)\n      metrics = sess.run(metrics)\n      self.assertAlmostEqual(1.0, metrics[\'DetectionBoxes_Precision/mAP\'])\n      self.assertAlmostEqual(1.0, metrics[\'DetectionMasks_Precision/mAP\'])\n\n  def test_get_eval_metric_ops_raises_error_with_unsupported_metric(self):\n    evaluation_metrics = [\'unsupported_metrics\']\n    categories = self._get_categories_list()\n    eval_dict = self._make_evaluation_dict()\n    with self.assertRaises(ValueError):\n      eval_util.get_eval_metric_ops_for_evaluators(\n          evaluation_metrics, categories, eval_dict)\n\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
src/object_detection/evaluator.py,10,"b'# Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Detection model evaluator.\n\nThis file provides a generic evaluation method that can be used to evaluate a\nDetectionModel.\n""""""\n\nimport logging\nimport tensorflow as tf\n\nfrom object_detection import eval_util\nfrom object_detection.core import prefetcher\nfrom object_detection.core import standard_fields as fields\nfrom object_detection.metrics import coco_evaluation\nfrom object_detection.utils import object_detection_evaluation\n\n# A dictionary of metric names to classes that implement the metric. The classes\n# in the dictionary must implement\n# utils.object_detection_evaluation.DetectionEvaluator interface.\nEVAL_METRICS_CLASS_DICT = {\n    \'pascal_voc_detection_metrics\':\n        object_detection_evaluation.PascalDetectionEvaluator,\n    \'weighted_pascal_voc_detection_metrics\':\n        object_detection_evaluation.WeightedPascalDetectionEvaluator,\n    \'pascal_voc_instance_segmentation_metrics\':\n        object_detection_evaluation.PascalInstanceSegmentationEvaluator,\n    \'weighted_pascal_voc_instance_segmentation_metrics\':\n        object_detection_evaluation.WeightedPascalInstanceSegmentationEvaluator,\n    \'open_images_detection_metrics\':\n        object_detection_evaluation.OpenImagesDetectionEvaluator,\n    \'coco_detection_metrics\':\n        coco_evaluation.CocoDetectionEvaluator,\n    \'coco_mask_metrics\':\n        coco_evaluation.CocoMaskEvaluator,\n}\n\nEVAL_DEFAULT_METRIC = \'pascal_voc_detection_metrics\'\n\n\ndef _extract_predictions_and_losses(model,\n                                    create_input_dict_fn,\n                                    ignore_groundtruth=False):\n  """"""Constructs tensorflow detection graph and returns output tensors.\n\n  Args:\n    model: model to perform predictions with.\n    create_input_dict_fn: function to create input tensor dictionaries.\n    ignore_groundtruth: whether groundtruth should be ignored.\n\n  Returns:\n    prediction_groundtruth_dict: A dictionary with postprocessed tensors (keyed\n      by standard_fields.DetectionResultsFields) and optional groundtruth\n      tensors (keyed by standard_fields.InputDataFields).\n    losses_dict: A dictionary containing detection losses. This is empty when\n      ignore_groundtruth is true.\n  """"""\n  input_dict = create_input_dict_fn()\n  prefetch_queue = prefetcher.prefetch(input_dict, capacity=500)\n  input_dict = prefetch_queue.dequeue()\n  original_image = tf.expand_dims(input_dict[fields.InputDataFields.image], 0)\n  preprocessed_image, true_image_shapes = model.preprocess(\n      tf.to_float(original_image))\n  prediction_dict = model.predict(preprocessed_image, true_image_shapes)\n  detections = model.postprocess(prediction_dict, true_image_shapes)\n\n  groundtruth = None\n  losses_dict = {}\n  if not ignore_groundtruth:\n    groundtruth = {\n        fields.InputDataFields.groundtruth_boxes:\n            input_dict[fields.InputDataFields.groundtruth_boxes],\n        fields.InputDataFields.groundtruth_classes:\n            input_dict[fields.InputDataFields.groundtruth_classes],\n        fields.InputDataFields.groundtruth_area:\n            input_dict[fields.InputDataFields.groundtruth_area],\n        fields.InputDataFields.groundtruth_is_crowd:\n            input_dict[fields.InputDataFields.groundtruth_is_crowd],\n        fields.InputDataFields.groundtruth_difficult:\n            input_dict[fields.InputDataFields.groundtruth_difficult]\n    }\n    if fields.InputDataFields.groundtruth_group_of in input_dict:\n      groundtruth[fields.InputDataFields.groundtruth_group_of] = (\n          input_dict[fields.InputDataFields.groundtruth_group_of])\n    if fields.DetectionResultFields.detection_masks in detections:\n      groundtruth[fields.InputDataFields.groundtruth_instance_masks] = (\n          input_dict[fields.InputDataFields.groundtruth_instance_masks])\n    label_id_offset = 1\n    model.provide_groundtruth(\n        [input_dict[fields.InputDataFields.groundtruth_boxes]],\n        [tf.one_hot(input_dict[fields.InputDataFields.groundtruth_classes]\n                    - label_id_offset, depth=model.num_classes)])\n    losses_dict.update(model.loss(prediction_dict, true_image_shapes))\n\n  result_dict = eval_util.result_dict_for_single_example(\n      original_image,\n      input_dict[fields.InputDataFields.source_id],\n      detections,\n      groundtruth,\n      class_agnostic=(\n          fields.DetectionResultFields.detection_classes not in detections),\n      scale_to_absolute=True)\n  return result_dict, losses_dict\n\n\ndef get_evaluators(eval_config, categories):\n  """"""Returns the evaluator class according to eval_config, valid for categories.\n\n  Args:\n    eval_config: evaluation configurations.\n    categories: a list of categories to evaluate.\n  Returns:\n    An list of instances of DetectionEvaluator.\n\n  Raises:\n    ValueError: if metric is not in the metric class dictionary.\n  """"""\n  eval_metric_fn_keys = eval_config.metrics_set\n  if not eval_metric_fn_keys:\n    eval_metric_fn_keys = [EVAL_DEFAULT_METRIC]\n  evaluators_list = []\n  for eval_metric_fn_key in eval_metric_fn_keys:\n    if eval_metric_fn_key not in EVAL_METRICS_CLASS_DICT:\n      raise ValueError(\'Metric not found: {}\'.format(eval_metric_fn_key))\n    evaluators_list.append(\n        EVAL_METRICS_CLASS_DICT[eval_metric_fn_key](categories=categories))\n  return evaluators_list\n\n\ndef evaluate(create_input_dict_fn, create_model_fn, eval_config, categories,\n             checkpoint_dir, eval_dir, graph_hook_fn=None, evaluator_list=None):\n  """"""Evaluation function for detection models.\n\n  Args:\n    create_input_dict_fn: a function to create a tensor input dictionary.\n    create_model_fn: a function that creates a DetectionModel.\n    eval_config: a eval_pb2.EvalConfig protobuf.\n    categories: a list of category dictionaries. Each dict in the list should\n                have an integer \'id\' field and string \'name\' field.\n    checkpoint_dir: directory to load the checkpoints to evaluate from.\n    eval_dir: directory to write evaluation metrics summary to.\n    graph_hook_fn: Optional function that is called after the training graph is\n      completely built. This is helpful to perform additional changes to the\n      training graph such as optimizing batchnorm. The function should modify\n      the default graph.\n    evaluator_list: Optional list of instances of DetectionEvaluator. If not\n      given, this list of metrics is created according to the eval_config.\n\n  Returns:\n    metrics: A dictionary containing metric names and values from the latest\n      run.\n  """"""\n\n  model = create_model_fn()\n\n  if eval_config.ignore_groundtruth and not eval_config.export_path:\n    logging.fatal(\'If ignore_groundtruth=True then an export_path is \'\n                  \'required. Aborting!!!\')\n\n  tensor_dict, losses_dict = _extract_predictions_and_losses(\n      model=model,\n      create_input_dict_fn=create_input_dict_fn,\n      ignore_groundtruth=eval_config.ignore_groundtruth)\n\n  def _process_batch(tensor_dict, sess, batch_index, counters,\n                     losses_dict=None):\n    """"""Evaluates tensors in tensor_dict, losses_dict and visualizes examples.\n\n    This function calls sess.run on tensor_dict, evaluating the original_image\n    tensor only on the first K examples and visualizing detections overlaid\n    on this original_image.\n\n    Args:\n      tensor_dict: a dictionary of tensors\n      sess: tensorflow session\n      batch_index: the index of the batch amongst all batches in the run.\n      counters: a dictionary holding \'success\' and \'skipped\' fields which can\n        be updated to keep track of number of successful and failed runs,\n        respectively.  If these fields are not updated, then the success/skipped\n        counter values shown at the end of evaluation will be incorrect.\n      losses_dict: Optional dictonary of scalar loss tensors.\n\n    Returns:\n      result_dict: a dictionary of numpy arrays\n      result_losses_dict: a dictionary of scalar losses. This is empty if input\n        losses_dict is None.\n    """"""\n    try:\n      if not losses_dict:\n        losses_dict = {}\n      result_dict, result_losses_dict = sess.run([tensor_dict, losses_dict])\n      counters[\'success\'] += 1\n    except tf.errors.InvalidArgumentError:\n      logging.info(\'Skipping image\')\n      counters[\'skipped\'] += 1\n      return {}\n    global_step = tf.train.global_step(sess, tf.train.get_global_step())\n    if batch_index < eval_config.num_visualizations:\n      tag = \'image-{}\'.format(batch_index)\n      eval_util.visualize_detection_results(\n          result_dict,\n          tag,\n          global_step,\n          categories=categories,\n          summary_dir=eval_dir,\n          export_dir=eval_config.visualization_export_dir,\n          show_groundtruth=eval_config.visualize_groundtruth_boxes,\n          groundtruth_box_visualization_color=eval_config.\n          groundtruth_box_visualization_color,\n          min_score_thresh=eval_config.min_score_threshold,\n          max_num_predictions=eval_config.max_num_boxes_to_visualize,\n          skip_scores=eval_config.skip_scores,\n          skip_labels=eval_config.skip_labels,\n          keep_image_id_for_visualization_export=eval_config.\n          keep_image_id_for_visualization_export)\n    return result_dict, result_losses_dict\n\n  variables_to_restore = tf.global_variables()\n  global_step = tf.train.get_or_create_global_step()\n  variables_to_restore.append(global_step)\n\n  if graph_hook_fn: graph_hook_fn()\n\n  if eval_config.use_moving_averages:\n    variable_averages = tf.train.ExponentialMovingAverage(0.0)\n    variables_to_restore = variable_averages.variables_to_restore()\n  saver = tf.train.Saver(variables_to_restore)\n\n  def _restore_latest_checkpoint(sess):\n    latest_checkpoint = tf.train.latest_checkpoint(checkpoint_dir)\n    saver.restore(sess, latest_checkpoint)\n\n  if not evaluator_list:\n    evaluator_list = get_evaluators(eval_config, categories)\n\n  metrics = eval_util.repeated_checkpoint_run(\n      tensor_dict=tensor_dict,\n      summary_dir=eval_dir,\n      evaluators=evaluator_list,\n      batch_processor=_process_batch,\n      checkpoint_dirs=[checkpoint_dir],\n      variables_to_restore=None,\n      restore_fn=_restore_latest_checkpoint,\n      num_batches=eval_config.num_examples,\n      eval_interval_secs=eval_config.eval_interval_secs,\n      max_number_of_evaluations=(1 if eval_config.ignore_groundtruth else\n                                 eval_config.max_evals\n                                 if eval_config.max_evals else None),\n      master=eval_config.eval_master,\n      save_graph=eval_config.save_graph,\n      save_graph_dir=(eval_dir if eval_config.save_graph else \'\'),\n      losses_dict=losses_dict)\n\n  return metrics\n'"
src/object_detection/export_inference_graph.py,7,"b'# Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nr""""""Tool to export an object detection model for inference.\n\nPrepares an object detection tensorflow graph for inference using model\nconfiguration and an optional trained checkpoint. Outputs inference\ngraph, associated checkpoint files, a frozen inference graph and a\nSavedModel (https://tensorflow.github.io/serving/serving_basic.html).\n\nThe inference graph contains one of three input nodes depending on the user\nspecified option.\n  * `image_tensor`: Accepts a uint8 4-D tensor of shape [None, None, None, 3]\n  * `encoded_image_string_tensor`: Accepts a 1-D string tensor of shape [None]\n    containing encoded PNG or JPEG images. Image resolutions are expected to be\n    the same if more than 1 image is provided.\n  * `tf_example`: Accepts a 1-D string tensor of shape [None] containing\n    serialized TFExample protos. Image resolutions are expected to be the same\n    if more than 1 image is provided.\n\nand the following output nodes returned by the model.postprocess(..):\n  * `num_detections`: Outputs float32 tensors of the form [batch]\n      that specifies the number of valid boxes per image in the batch.\n  * `detection_boxes`: Outputs float32 tensors of the form\n      [batch, num_boxes, 4] containing detected boxes.\n  * `detection_scores`: Outputs float32 tensors of the form\n      [batch, num_boxes] containing class scores for the detections.\n  * `detection_classes`: Outputs float32 tensors of the form\n      [batch, num_boxes] containing classes for the detections.\n  * `detection_masks`: Outputs float32 tensors of the form\n      [batch, num_boxes, mask_height, mask_width] containing predicted instance\n      masks for each box if its present in the dictionary of postprocessed\n      tensors returned by the model.\n\nNotes:\n * This tool uses `use_moving_averages` from eval_config to decide which\n   weights to freeze.\n\nExample Usage:\n--------------\npython export_inference_graph \\\n    --input_type image_tensor \\\n    --pipeline_config_path path/to/ssd_inception_v2.config \\\n    --trained_checkpoint_prefix path/to/model.ckpt \\\n    --output_directory path/to/exported_model_directory\n\nThe expected output would be in the directory\npath/to/exported_model_directory (which is created if it does not exist)\nwith contents:\n - graph.pbtxt\n - model.ckpt.data-00000-of-00001\n - model.ckpt.info\n - model.ckpt.meta\n - frozen_inference_graph.pb\n + saved_model (a directory)\n\nConfig overrides (see the `config_override` flag) are text protobufs\n(also of type pipeline_pb2.TrainEvalPipelineConfig) which are used to override\ncertain fields in the provided pipeline_config_path.  These are useful for\nmaking small changes to the inference graph that differ from the training or\neval config.\n\nExample Usage (in which we change the second stage post-processing score\nthreshold to be 0.5):\n\npython export_inference_graph \\\n    --input_type image_tensor \\\n    --pipeline_config_path path/to/ssd_inception_v2.config \\\n    --trained_checkpoint_prefix path/to/model.ckpt \\\n    --output_directory path/to/exported_model_directory \\\n    --config_override "" \\\n            model{ \\\n              faster_rcnn { \\\n                second_stage_post_processing { \\\n                  batch_non_max_suppression { \\\n                    score_threshold: 0.5 \\\n                  } \\\n                } \\\n              } \\\n            }""\n""""""\nimport tensorflow as tf\nfrom google.protobuf import text_format\nfrom object_detection import exporter\nfrom object_detection.protos import pipeline_pb2\n\nslim = tf.contrib.slim\nflags = tf.app.flags\n\nflags.DEFINE_string(\'input_type\', \'image_tensor\', \'Type of input node. Can be \'\n                    \'one of [`image_tensor`, `encoded_image_string_tensor`, \'\n                    \'`tf_example`]\')\nflags.DEFINE_string(\'input_shape\', None,\n                    \'If input_type is `image_tensor`, this can explicitly set \'\n                    \'the shape of this input tensor to a fixed size. The \'\n                    \'dimensions are to be provided as a comma-separated list \'\n                    \'of integers. A value of -1 can be used for unknown \'\n                    \'dimensions. If not specified, for an `image_tensor, the \'\n                    \'default shape will be partially specified as \'\n                    \'`[None, None, None, 3]`.\')\nflags.DEFINE_string(\'pipeline_config_path\', None,\n                    \'Path to a pipeline_pb2.TrainEvalPipelineConfig config \'\n                    \'file.\')\nflags.DEFINE_string(\'trained_checkpoint_prefix\', None,\n                    \'Path to trained checkpoint, typically of the form \'\n                    \'path/to/model.ckpt\')\nflags.DEFINE_string(\'output_directory\', None, \'Path to write outputs.\')\nflags.DEFINE_string(\'config_override\', \'\',\n                    \'pipeline_pb2.TrainEvalPipelineConfig \'\n                    \'text proto to override pipeline_config_path.\')\ntf.app.flags.mark_flag_as_required(\'pipeline_config_path\')\ntf.app.flags.mark_flag_as_required(\'trained_checkpoint_prefix\')\ntf.app.flags.mark_flag_as_required(\'output_directory\')\nFLAGS = flags.FLAGS\n\n\ndef main(_):\n  pipeline_config = pipeline_pb2.TrainEvalPipelineConfig()\n  with tf.gfile.GFile(FLAGS.pipeline_config_path, \'r\') as f:\n    text_format.Merge(f.read(), pipeline_config)\n  text_format.Merge(FLAGS.config_override, pipeline_config)\n  if FLAGS.input_shape:\n    input_shape = [\n        int(dim) if dim != \'-1\' else None\n        for dim in FLAGS.input_shape.split(\',\')\n    ]\n  else:\n    input_shape = None\n  exporter.export_inference_graph(FLAGS.input_type, pipeline_config,\n                                  FLAGS.trained_checkpoint_prefix,\n                                  FLAGS.output_directory, input_shape)\n\n\nif __name__ == \'__main__\':\n  tf.app.run()\n'"
src/object_detection/exporter.py,45,"b'# Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n""""""Functions to export object detection inference graph.""""""\nimport logging\nimport os\nimport tempfile\nimport tensorflow as tf\nfrom google.protobuf import text_format\nfrom tensorflow.core.protobuf import saver_pb2\nfrom tensorflow.python import pywrap_tensorflow\nfrom tensorflow.python.client import session\nfrom tensorflow.python.framework import graph_util\nfrom tensorflow.python.platform import gfile\nfrom tensorflow.python.saved_model import signature_constants\nfrom tensorflow.python.training import saver as saver_lib\nfrom object_detection.builders import model_builder\nfrom object_detection.core import standard_fields as fields\nfrom object_detection.data_decoders import tf_example_decoder\n\nslim = tf.contrib.slim\n\n\n# TODO(derekjchow): Replace with freeze_graph.freeze_graph_with_def_protos when\n# newer version of Tensorflow becomes more common.\ndef freeze_graph_with_def_protos(\n    input_graph_def,\n    input_saver_def,\n    input_checkpoint,\n    output_node_names,\n    restore_op_name,\n    filename_tensor_name,\n    clear_devices,\n    initializer_nodes,\n    variable_names_blacklist=\'\'):\n  """"""Converts all variables in a graph and checkpoint into constants.""""""\n  del restore_op_name, filename_tensor_name  # Unused by updated loading code.\n\n  # \'input_checkpoint\' may be a prefix if we\'re using Saver V2 format\n  if not saver_lib.checkpoint_exists(input_checkpoint):\n    raise ValueError(\n        \'Input checkpoint ""\' + input_checkpoint + \'"" does not exist!\')\n\n  if not output_node_names:\n    raise ValueError(\n        \'You must supply the name of a node to --output_node_names.\')\n\n  # Remove all the explicit device specifications for this node. This helps to\n  # make the graph more portable.\n  if clear_devices:\n    for node in input_graph_def.node:\n      node.device = \'\'\n\n  with tf.Graph().as_default():\n    tf.import_graph_def(input_graph_def, name=\'\')\n    config = tf.ConfigProto(graph_options=tf.GraphOptions())\n    with session.Session(config=config) as sess:\n      if input_saver_def:\n        saver = saver_lib.Saver(saver_def=input_saver_def)\n        saver.restore(sess, input_checkpoint)\n      else:\n        var_list = {}\n        reader = pywrap_tensorflow.NewCheckpointReader(input_checkpoint)\n        var_to_shape_map = reader.get_variable_to_shape_map()\n        for key in var_to_shape_map:\n          try:\n            tensor = sess.graph.get_tensor_by_name(key + \':0\')\n          except KeyError:\n            # This tensor doesn\'t exist in the graph (for example it\'s\n            # \'global_step\' or a similar housekeeping element) so skip it.\n            continue\n          var_list[key] = tensor\n        saver = saver_lib.Saver(var_list=var_list)\n        saver.restore(sess, input_checkpoint)\n        if initializer_nodes:\n          sess.run(initializer_nodes)\n\n      variable_names_blacklist = (variable_names_blacklist.split(\',\') if\n                                  variable_names_blacklist else None)\n      output_graph_def = graph_util.convert_variables_to_constants(\n          sess,\n          input_graph_def,\n          output_node_names.split(\',\'),\n          variable_names_blacklist=variable_names_blacklist)\n\n  return output_graph_def\n\n\ndef replace_variable_values_with_moving_averages(graph,\n                                                 current_checkpoint_file,\n                                                 new_checkpoint_file):\n  """"""Replaces variable values in the checkpoint with their moving averages.\n\n  If the current checkpoint has shadow variables maintaining moving averages of\n  the variables defined in the graph, this function generates a new checkpoint\n  where the variables contain the values of their moving averages.\n\n  Args:\n    graph: a tf.Graph object.\n    current_checkpoint_file: a checkpoint containing both original variables and\n      their moving averages.\n    new_checkpoint_file: file path to write a new checkpoint.\n  """"""\n  with graph.as_default():\n    variable_averages = tf.train.ExponentialMovingAverage(0.0)\n    ema_variables_to_restore = variable_averages.variables_to_restore()\n    with tf.Session() as sess:\n      read_saver = tf.train.Saver(ema_variables_to_restore)\n      read_saver.restore(sess, current_checkpoint_file)\n      write_saver = tf.train.Saver()\n      write_saver.save(sess, new_checkpoint_file)\n\n\ndef _image_tensor_input_placeholder(input_shape=None):\n  """"""Returns input placeholder and a 4-D uint8 image tensor.""""""\n  if input_shape is None:\n    input_shape = (None, None, None, 3)\n  input_tensor = tf.placeholder(\n      dtype=tf.uint8, shape=input_shape, name=\'image_tensor\')\n  return input_tensor, input_tensor\n\n\ndef _tf_example_input_placeholder():\n  """"""Returns input that accepts a batch of strings with tf examples.\n\n  Returns:\n    a tuple of input placeholder and the output decoded images.\n  """"""\n  batch_tf_example_placeholder = tf.placeholder(\n      tf.string, shape=[None], name=\'tf_example\')\n  def decode(tf_example_string_tensor):\n    tensor_dict = tf_example_decoder.TfExampleDecoder().decode(\n        tf_example_string_tensor)\n    image_tensor = tensor_dict[fields.InputDataFields.image]\n    return image_tensor\n  return (batch_tf_example_placeholder,\n          tf.map_fn(decode,\n                    elems=batch_tf_example_placeholder,\n                    dtype=tf.uint8,\n                    parallel_iterations=32,\n                    back_prop=False))\n\n\ndef _encoded_image_string_tensor_input_placeholder():\n  """"""Returns input that accepts a batch of PNG or JPEG strings.\n\n  Returns:\n    a tuple of input placeholder and the output decoded images.\n  """"""\n  batch_image_str_placeholder = tf.placeholder(\n      dtype=tf.string,\n      shape=[None],\n      name=\'encoded_image_string_tensor\')\n  def decode(encoded_image_string_tensor):\n    image_tensor = tf.image.decode_image(encoded_image_string_tensor,\n                                         channels=3)\n    image_tensor.set_shape((None, None, 3))\n    return image_tensor\n  return (batch_image_str_placeholder,\n          tf.map_fn(\n              decode,\n              elems=batch_image_str_placeholder,\n              dtype=tf.uint8,\n              parallel_iterations=32,\n              back_prop=False))\n\n\ninput_placeholder_fn_map = {\n    \'image_tensor\': _image_tensor_input_placeholder,\n    \'encoded_image_string_tensor\':\n    _encoded_image_string_tensor_input_placeholder,\n    \'tf_example\': _tf_example_input_placeholder,\n}\n\n\ndef _add_output_tensor_nodes(postprocessed_tensors,\n                             output_collection_name=\'inference_op\'):\n  """"""Adds output nodes for detection boxes and scores.\n\n  Adds the following nodes for output tensors -\n    * num_detections: float32 tensor of shape [batch_size].\n    * detection_boxes: float32 tensor of shape [batch_size, num_boxes, 4]\n      containing detected boxes.\n    * detection_scores: float32 tensor of shape [batch_size, num_boxes]\n      containing scores for the detected boxes.\n    * detection_classes: float32 tensor of shape [batch_size, num_boxes]\n      containing class predictions for the detected boxes.\n    * detection_masks: (Optional) float32 tensor of shape\n      [batch_size, num_boxes, mask_height, mask_width] containing masks for each\n      detection box.\n\n  Args:\n    postprocessed_tensors: a dictionary containing the following fields\n      \'detection_boxes\': [batch, max_detections, 4]\n      \'detection_scores\': [batch, max_detections]\n      \'detection_classes\': [batch, max_detections]\n      \'detection_masks\': [batch, max_detections, mask_height, mask_width]\n        (optional).\n      \'num_detections\': [batch]\n    output_collection_name: Name of collection to add output tensors to.\n\n  Returns:\n    A tensor dict containing the added output tensor nodes.\n  """"""\n  detection_fields = fields.DetectionResultFields\n  label_id_offset = 1\n  boxes = postprocessed_tensors.get(detection_fields.detection_boxes)\n  scores = postprocessed_tensors.get(detection_fields.detection_scores)\n  classes = postprocessed_tensors.get(\n      detection_fields.detection_classes) + label_id_offset\n  masks = postprocessed_tensors.get(detection_fields.detection_masks)\n  num_detections = postprocessed_tensors.get(detection_fields.num_detections)\n  outputs = {}\n  outputs[detection_fields.detection_boxes] = tf.identity(\n      boxes, name=detection_fields.detection_boxes)\n  outputs[detection_fields.detection_scores] = tf.identity(\n      scores, name=detection_fields.detection_scores)\n  outputs[detection_fields.detection_classes] = tf.identity(\n      classes, name=detection_fields.detection_classes)\n  outputs[detection_fields.num_detections] = tf.identity(\n      num_detections, name=detection_fields.num_detections)\n  if masks is not None:\n    outputs[detection_fields.detection_masks] = tf.identity(\n        masks, name=detection_fields.detection_masks)\n  for output_key in outputs:\n    tf.add_to_collection(output_collection_name, outputs[output_key])\n  if masks is not None:\n    tf.add_to_collection(output_collection_name,\n                         outputs[detection_fields.detection_masks])\n  return outputs\n\n\ndef write_frozen_graph(frozen_graph_path, frozen_graph_def):\n  """"""Writes frozen graph to disk.\n\n  Args:\n    frozen_graph_path: Path to write inference graph.\n    frozen_graph_def: tf.GraphDef holding frozen graph.\n  """"""\n  with gfile.GFile(frozen_graph_path, \'wb\') as f:\n    f.write(frozen_graph_def.SerializeToString())\n  logging.info(\'%d ops in the final graph.\', len(frozen_graph_def.node))\n\n\ndef write_saved_model(saved_model_path,\n                      frozen_graph_def,\n                      inputs,\n                      outputs):\n  """"""Writes SavedModel to disk.\n\n  If checkpoint_path is not None bakes the weights into the graph thereby\n  eliminating the need of checkpoint files during inference. If the model\n  was trained with moving averages, setting use_moving_averages to true\n  restores the moving averages, otherwise the original set of variables\n  is restored.\n\n  Args:\n    saved_model_path: Path to write SavedModel.\n    frozen_graph_def: tf.GraphDef holding frozen graph.\n    inputs: The input image tensor to use for detection.\n    outputs: A tensor dictionary containing the outputs of a DetectionModel.\n  """"""\n  with tf.Graph().as_default():\n    with session.Session() as sess:\n\n      tf.import_graph_def(frozen_graph_def, name=\'\')\n\n      builder = tf.saved_model.builder.SavedModelBuilder(saved_model_path)\n\n      tensor_info_inputs = {\n          \'inputs\': tf.saved_model.utils.build_tensor_info(inputs)}\n      tensor_info_outputs = {}\n      for k, v in outputs.items():\n        tensor_info_outputs[k] = tf.saved_model.utils.build_tensor_info(v)\n\n      detection_signature = (\n          tf.saved_model.signature_def_utils.build_signature_def(\n              inputs=tensor_info_inputs,\n              outputs=tensor_info_outputs,\n              method_name=signature_constants.PREDICT_METHOD_NAME))\n\n      builder.add_meta_graph_and_variables(\n          sess, [tf.saved_model.tag_constants.SERVING],\n          signature_def_map={\n              signature_constants.DEFAULT_SERVING_SIGNATURE_DEF_KEY:\n                  detection_signature,\n          },\n      )\n      builder.save()\n\n\ndef write_graph_and_checkpoint(inference_graph_def,\n                               model_path,\n                               input_saver_def,\n                               trained_checkpoint_prefix):\n  """"""Writes the graph and the checkpoint into disk.""""""\n  for node in inference_graph_def.node:\n    node.device = \'\'\n  with tf.Graph().as_default():\n    tf.import_graph_def(inference_graph_def, name=\'\')\n    with session.Session() as sess:\n      saver = saver_lib.Saver(saver_def=input_saver_def,\n                              save_relative_paths=True)\n      saver.restore(sess, trained_checkpoint_prefix)\n      saver.save(sess, model_path)\n\n\ndef _get_outputs_from_inputs(input_tensors, detection_model,\n                             output_collection_name):\n  inputs = tf.to_float(input_tensors)\n  preprocessed_inputs, true_image_shapes = detection_model.preprocess(inputs)\n  output_tensors = detection_model.predict(\n      preprocessed_inputs, true_image_shapes)\n  postprocessed_tensors = detection_model.postprocess(\n      output_tensors, true_image_shapes)\n  return _add_output_tensor_nodes(postprocessed_tensors,\n                                  output_collection_name)\n\n\ndef _build_detection_graph(input_type, detection_model, input_shape,\n                           output_collection_name, graph_hook_fn):\n  """"""Build the detection graph.""""""\n  if input_type not in input_placeholder_fn_map:\n    raise ValueError(\'Unknown input type: {}\'.format(input_type))\n  placeholder_args = {}\n  if input_shape is not None:\n    if input_type != \'image_tensor\':\n      raise ValueError(\'Can only specify input shape for `image_tensor` \'\n                       \'inputs.\')\n    placeholder_args[\'input_shape\'] = input_shape\n  placeholder_tensor, input_tensors = input_placeholder_fn_map[input_type](\n      **placeholder_args)\n  outputs = _get_outputs_from_inputs(\n      input_tensors=input_tensors,\n      detection_model=detection_model,\n      output_collection_name=output_collection_name)\n\n  # Add global step to the graph.\n  slim.get_or_create_global_step()\n\n  if graph_hook_fn: graph_hook_fn()\n\n  return outputs, placeholder_tensor\n\n\ndef _export_inference_graph(input_type,\n                            detection_model,\n                            use_moving_averages,\n                            trained_checkpoint_prefix,\n                            output_directory,\n                            additional_output_tensor_names=None,\n                            input_shape=None,\n                            output_collection_name=\'inference_op\',\n                            graph_hook_fn=None):\n  """"""Export helper.""""""\n  tf.gfile.MakeDirs(output_directory)\n  frozen_graph_path = os.path.join(output_directory,\n                                   \'frozen_inference_graph.pb\')\n  saved_model_path = os.path.join(output_directory, \'saved_model\')\n  model_path = os.path.join(output_directory, \'model.ckpt\')\n\n  outputs, placeholder_tensor = _build_detection_graph(\n      input_type=input_type,\n      detection_model=detection_model,\n      input_shape=input_shape,\n      output_collection_name=output_collection_name,\n      graph_hook_fn=graph_hook_fn)\n\n  saver_kwargs = {}\n  if use_moving_averages:\n    # This check is to be compatible with both version of SaverDef.\n    if os.path.isfile(trained_checkpoint_prefix):\n      saver_kwargs[\'write_version\'] = saver_pb2.SaverDef.V1\n      temp_checkpoint_prefix = tempfile.NamedTemporaryFile().name\n    else:\n      temp_checkpoint_prefix = tempfile.mkdtemp()\n    replace_variable_values_with_moving_averages(\n        tf.get_default_graph(), trained_checkpoint_prefix,\n        temp_checkpoint_prefix)\n    checkpoint_to_use = temp_checkpoint_prefix\n  else:\n    checkpoint_to_use = trained_checkpoint_prefix\n\n  saver = tf.train.Saver(**saver_kwargs)\n  input_saver_def = saver.as_saver_def()\n\n  write_graph_and_checkpoint(\n      inference_graph_def=tf.get_default_graph().as_graph_def(),\n      model_path=model_path,\n      input_saver_def=input_saver_def,\n      trained_checkpoint_prefix=checkpoint_to_use)\n\n  if additional_output_tensor_names is not None:\n    output_node_names = \',\'.join(outputs.keys()+additional_output_tensor_names)\n  else:\n    output_node_names = \',\'.join(outputs.keys())\n\n  frozen_graph_def = freeze_graph_with_def_protos(\n      input_graph_def=tf.get_default_graph().as_graph_def(),\n      input_saver_def=input_saver_def,\n      input_checkpoint=checkpoint_to_use,\n      output_node_names=output_node_names,\n      restore_op_name=\'save/restore_all\',\n      filename_tensor_name=\'save/Const:0\',\n      clear_devices=True,\n      initializer_nodes=\'\')\n  write_frozen_graph(frozen_graph_path, frozen_graph_def)\n  write_saved_model(saved_model_path, frozen_graph_def,\n                    placeholder_tensor, outputs)\n\n\ndef export_inference_graph(input_type,\n                           pipeline_config,\n                           trained_checkpoint_prefix,\n                           output_directory,\n                           input_shape=None,\n                           output_collection_name=\'inference_op\',\n                           additional_output_tensor_names=None):\n  """"""Exports inference graph for the model specified in the pipeline config.\n\n  Args:\n    input_type: Type of input for the graph. Can be one of [`image_tensor`,\n      `tf_example`].\n    pipeline_config: pipeline_pb2.TrainAndEvalPipelineConfig proto.\n    trained_checkpoint_prefix: Path to the trained checkpoint file.\n    output_directory: Path to write outputs.\n    input_shape: Sets a fixed shape for an `image_tensor` input. If not\n      specified, will default to [None, None, None, 3].\n    output_collection_name: Name of collection to add output tensors to.\n      If None, does not add output tensors to a collection.\n    additional_output_tensor_names: list of additional output\n      tensors to include in the frozen graph.\n  """"""\n  detection_model = model_builder.build(pipeline_config.model,\n                                        is_training=False)\n  _export_inference_graph(input_type, detection_model,\n                          pipeline_config.eval_config.use_moving_averages,\n                          trained_checkpoint_prefix,\n                          output_directory, additional_output_tensor_names,\n                          input_shape, output_collection_name,\n                          graph_hook_fn=None)\n  pipeline_config.eval_config.use_moving_averages = False\n  config_text = text_format.MessageToString(pipeline_config)\n  with tf.gfile.Open(\n      os.path.join(output_directory, \'pipeline.config\'), \'wb\') as f:\n    f.write(config_text)\n'"
src/object_detection/exporter_test.py,58,"b'# Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n""""""Tests for object_detection.export_inference_graph.""""""\nimport os\nimport numpy as np\nimport six\nimport tensorflow as tf\nfrom google.protobuf import text_format\nfrom object_detection import exporter\nfrom object_detection.builders import model_builder\nfrom object_detection.core import model\nfrom object_detection.protos import pipeline_pb2\n\nif six.PY2:\n  import mock  # pylint: disable=g-import-not-at-top\nelse:\n  from unittest import mock  # pylint: disable=g-import-not-at-top\n\nslim = tf.contrib.slim\n\n\nclass FakeModel(model.DetectionModel):\n\n  def __init__(self, add_detection_masks=False):\n    self._add_detection_masks = add_detection_masks\n\n  def preprocess(self, inputs):\n    true_image_shapes = []  # Doesn\'t matter for the fake model.\n    return tf.identity(inputs), true_image_shapes\n\n  def predict(self, preprocessed_inputs, true_image_shapes):\n    return {\'image\': tf.layers.conv2d(preprocessed_inputs, 3, 1)}\n\n  def postprocess(self, prediction_dict, true_image_shapes):\n    with tf.control_dependencies(prediction_dict.values()):\n      postprocessed_tensors = {\n          \'detection_boxes\': tf.constant([[[0.0, 0.0, 0.5, 0.5],\n                                           [0.5, 0.5, 0.8, 0.8]],\n                                          [[0.5, 0.5, 1.0, 1.0],\n                                           [0.0, 0.0, 0.0, 0.0]]], tf.float32),\n          \'detection_scores\': tf.constant([[0.7, 0.6],\n                                           [0.9, 0.0]], tf.float32),\n          \'detection_classes\': tf.constant([[0, 1],\n                                            [1, 0]], tf.float32),\n          \'num_detections\': tf.constant([2, 1], tf.float32)\n      }\n      if self._add_detection_masks:\n        postprocessed_tensors[\'detection_masks\'] = tf.constant(\n            np.arange(64).reshape([2, 2, 4, 4]), tf.float32)\n    return postprocessed_tensors\n\n  def restore_map(self, checkpoint_path, fine_tune_checkpoint_type):\n    pass\n\n  def loss(self, prediction_dict, true_image_shapes):\n    pass\n\n\nclass ExportInferenceGraphTest(tf.test.TestCase):\n\n  def _save_checkpoint_from_mock_model(self, checkpoint_path,\n                                       use_moving_averages):\n    g = tf.Graph()\n    with g.as_default():\n      mock_model = FakeModel()\n      preprocessed_inputs, true_image_shapes = mock_model.preprocess(\n          tf.placeholder(tf.float32, shape=[None, None, None, 3]))\n      predictions = mock_model.predict(preprocessed_inputs, true_image_shapes)\n      mock_model.postprocess(predictions, true_image_shapes)\n      if use_moving_averages:\n        tf.train.ExponentialMovingAverage(0.0).apply()\n      slim.get_or_create_global_step()\n      saver = tf.train.Saver()\n      init = tf.global_variables_initializer()\n      with self.test_session() as sess:\n        sess.run(init)\n        saver.save(sess, checkpoint_path)\n\n  def _load_inference_graph(self, inference_graph_path):\n    od_graph = tf.Graph()\n    with od_graph.as_default():\n      od_graph_def = tf.GraphDef()\n      with tf.gfile.GFile(inference_graph_path) as fid:\n        serialized_graph = fid.read()\n        od_graph_def.ParseFromString(serialized_graph)\n        tf.import_graph_def(od_graph_def, name=\'\')\n    return od_graph\n\n  def _create_tf_example(self, image_array):\n    with self.test_session():\n      encoded_image = tf.image.encode_jpeg(tf.constant(image_array)).eval()\n    def _bytes_feature(value):\n      return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))\n    example = tf.train.Example(features=tf.train.Features(feature={\n        \'image/encoded\': _bytes_feature(encoded_image),\n        \'image/format\': _bytes_feature(\'jpg\'),\n        \'image/source_id\': _bytes_feature(\'image_id\')\n    })).SerializeToString()\n    return example\n\n  def test_export_graph_with_image_tensor_input(self):\n    tmp_dir = self.get_temp_dir()\n    trained_checkpoint_prefix = os.path.join(tmp_dir, \'model.ckpt\')\n    self._save_checkpoint_from_mock_model(trained_checkpoint_prefix,\n                                          use_moving_averages=False)\n    with mock.patch.object(\n        model_builder, \'build\', autospec=True) as mock_builder:\n      mock_builder.return_value = FakeModel()\n      output_directory = os.path.join(tmp_dir, \'output\')\n      pipeline_config = pipeline_pb2.TrainEvalPipelineConfig()\n      pipeline_config.eval_config.use_moving_averages = False\n      exporter.export_inference_graph(\n          input_type=\'image_tensor\',\n          pipeline_config=pipeline_config,\n          trained_checkpoint_prefix=trained_checkpoint_prefix,\n          output_directory=output_directory)\n      self.assertTrue(os.path.exists(os.path.join(\n          output_directory, \'saved_model\', \'saved_model.pb\')))\n\n  def test_export_graph_with_fixed_size_image_tensor_input(self):\n    input_shape = [1, 320, 320, 3]\n\n    tmp_dir = self.get_temp_dir()\n    trained_checkpoint_prefix = os.path.join(tmp_dir, \'model.ckpt\')\n    self._save_checkpoint_from_mock_model(\n        trained_checkpoint_prefix, use_moving_averages=False)\n    with mock.patch.object(\n        model_builder, \'build\', autospec=True) as mock_builder:\n      mock_builder.return_value = FakeModel()\n      output_directory = os.path.join(tmp_dir, \'output\')\n      pipeline_config = pipeline_pb2.TrainEvalPipelineConfig()\n      pipeline_config.eval_config.use_moving_averages = False\n      exporter.export_inference_graph(\n          input_type=\'image_tensor\',\n          pipeline_config=pipeline_config,\n          trained_checkpoint_prefix=trained_checkpoint_prefix,\n          output_directory=output_directory,\n          input_shape=input_shape)\n      saved_model_path = os.path.join(output_directory, \'saved_model\')\n      self.assertTrue(\n          os.path.exists(os.path.join(saved_model_path, \'saved_model.pb\')))\n\n    with tf.Graph().as_default() as od_graph:\n      with self.test_session(graph=od_graph) as sess:\n        meta_graph = tf.saved_model.loader.load(\n            sess, [tf.saved_model.tag_constants.SERVING], saved_model_path)\n        signature = meta_graph.signature_def[\'serving_default\']\n        input_tensor_name = signature.inputs[\'inputs\'].name\n        image_tensor = od_graph.get_tensor_by_name(input_tensor_name)\n        self.assertSequenceEqual(image_tensor.get_shape().as_list(),\n                                 input_shape)\n\n  def test_export_graph_with_tf_example_input(self):\n    tmp_dir = self.get_temp_dir()\n    trained_checkpoint_prefix = os.path.join(tmp_dir, \'model.ckpt\')\n    self._save_checkpoint_from_mock_model(trained_checkpoint_prefix,\n                                          use_moving_averages=False)\n    with mock.patch.object(\n        model_builder, \'build\', autospec=True) as mock_builder:\n      mock_builder.return_value = FakeModel()\n      output_directory = os.path.join(tmp_dir, \'output\')\n      pipeline_config = pipeline_pb2.TrainEvalPipelineConfig()\n      pipeline_config.eval_config.use_moving_averages = False\n      exporter.export_inference_graph(\n          input_type=\'tf_example\',\n          pipeline_config=pipeline_config,\n          trained_checkpoint_prefix=trained_checkpoint_prefix,\n          output_directory=output_directory)\n      self.assertTrue(os.path.exists(os.path.join(\n          output_directory, \'saved_model\', \'saved_model.pb\')))\n\n  def test_export_graph_with_encoded_image_string_input(self):\n    tmp_dir = self.get_temp_dir()\n    trained_checkpoint_prefix = os.path.join(tmp_dir, \'model.ckpt\')\n    self._save_checkpoint_from_mock_model(trained_checkpoint_prefix,\n                                          use_moving_averages=False)\n    with mock.patch.object(\n        model_builder, \'build\', autospec=True) as mock_builder:\n      mock_builder.return_value = FakeModel()\n      output_directory = os.path.join(tmp_dir, \'output\')\n      pipeline_config = pipeline_pb2.TrainEvalPipelineConfig()\n      pipeline_config.eval_config.use_moving_averages = False\n      exporter.export_inference_graph(\n          input_type=\'encoded_image_string_tensor\',\n          pipeline_config=pipeline_config,\n          trained_checkpoint_prefix=trained_checkpoint_prefix,\n          output_directory=output_directory)\n      self.assertTrue(os.path.exists(os.path.join(\n          output_directory, \'saved_model\', \'saved_model.pb\')))\n\n  def _get_variables_in_checkpoint(self, checkpoint_file):\n    return set([\n        var_name\n        for var_name, _ in tf.train.list_variables(checkpoint_file)])\n\n  def test_replace_variable_values_with_moving_averages(self):\n    tmp_dir = self.get_temp_dir()\n    trained_checkpoint_prefix = os.path.join(tmp_dir, \'model.ckpt\')\n    new_checkpoint_prefix = os.path.join(tmp_dir, \'new.ckpt\')\n    self._save_checkpoint_from_mock_model(trained_checkpoint_prefix,\n                                          use_moving_averages=True)\n    graph = tf.Graph()\n    with graph.as_default():\n      fake_model = FakeModel()\n      preprocessed_inputs, true_image_shapes = fake_model.preprocess(\n          tf.placeholder(dtype=tf.float32, shape=[None, None, None, 3]))\n      predictions = fake_model.predict(preprocessed_inputs, true_image_shapes)\n      fake_model.postprocess(predictions, true_image_shapes)\n      exporter.replace_variable_values_with_moving_averages(\n          graph, trained_checkpoint_prefix, new_checkpoint_prefix)\n\n    expected_variables = set([\'conv2d/bias\', \'conv2d/kernel\'])\n    variables_in_old_ckpt = self._get_variables_in_checkpoint(\n        trained_checkpoint_prefix)\n    self.assertIn(\'conv2d/bias/ExponentialMovingAverage\',\n                  variables_in_old_ckpt)\n    self.assertIn(\'conv2d/kernel/ExponentialMovingAverage\',\n                  variables_in_old_ckpt)\n    variables_in_new_ckpt = self._get_variables_in_checkpoint(\n        new_checkpoint_prefix)\n    self.assertTrue(expected_variables.issubset(variables_in_new_ckpt))\n    self.assertNotIn(\'conv2d/bias/ExponentialMovingAverage\',\n                     variables_in_new_ckpt)\n    self.assertNotIn(\'conv2d/kernel/ExponentialMovingAverage\',\n                     variables_in_new_ckpt)\n\n  def test_export_graph_with_moving_averages(self):\n    tmp_dir = self.get_temp_dir()\n    trained_checkpoint_prefix = os.path.join(tmp_dir, \'model.ckpt\')\n    self._save_checkpoint_from_mock_model(trained_checkpoint_prefix,\n                                          use_moving_averages=True)\n    output_directory = os.path.join(tmp_dir, \'output\')\n    with mock.patch.object(\n        model_builder, \'build\', autospec=True) as mock_builder:\n      mock_builder.return_value = FakeModel()\n      pipeline_config = pipeline_pb2.TrainEvalPipelineConfig()\n      pipeline_config.eval_config.use_moving_averages = True\n      exporter.export_inference_graph(\n          input_type=\'image_tensor\',\n          pipeline_config=pipeline_config,\n          trained_checkpoint_prefix=trained_checkpoint_prefix,\n          output_directory=output_directory)\n      self.assertTrue(os.path.exists(os.path.join(\n          output_directory, \'saved_model\', \'saved_model.pb\')))\n    expected_variables = set([\'conv2d/bias\', \'conv2d/kernel\', \'global_step\'])\n    actual_variables = set(\n        [var_name for var_name, _ in tf.train.list_variables(output_directory)])\n    self.assertTrue(expected_variables.issubset(actual_variables))\n\n  def test_export_model_with_all_output_nodes(self):\n    tmp_dir = self.get_temp_dir()\n    trained_checkpoint_prefix = os.path.join(tmp_dir, \'model.ckpt\')\n    self._save_checkpoint_from_mock_model(trained_checkpoint_prefix,\n                                          use_moving_averages=True)\n    output_directory = os.path.join(tmp_dir, \'output\')\n    inference_graph_path = os.path.join(output_directory,\n                                        \'frozen_inference_graph.pb\')\n    with mock.patch.object(\n        model_builder, \'build\', autospec=True) as mock_builder:\n      mock_builder.return_value = FakeModel(add_detection_masks=True)\n      pipeline_config = pipeline_pb2.TrainEvalPipelineConfig()\n      exporter.export_inference_graph(\n          input_type=\'image_tensor\',\n          pipeline_config=pipeline_config,\n          trained_checkpoint_prefix=trained_checkpoint_prefix,\n          output_directory=output_directory)\n    inference_graph = self._load_inference_graph(inference_graph_path)\n    with self.test_session(graph=inference_graph):\n      inference_graph.get_tensor_by_name(\'image_tensor:0\')\n      inference_graph.get_tensor_by_name(\'detection_boxes:0\')\n      inference_graph.get_tensor_by_name(\'detection_scores:0\')\n      inference_graph.get_tensor_by_name(\'detection_classes:0\')\n      inference_graph.get_tensor_by_name(\'detection_masks:0\')\n      inference_graph.get_tensor_by_name(\'num_detections:0\')\n\n  def test_export_model_with_detection_only_nodes(self):\n    tmp_dir = self.get_temp_dir()\n    trained_checkpoint_prefix = os.path.join(tmp_dir, \'model.ckpt\')\n    self._save_checkpoint_from_mock_model(trained_checkpoint_prefix,\n                                          use_moving_averages=True)\n    output_directory = os.path.join(tmp_dir, \'output\')\n    inference_graph_path = os.path.join(output_directory,\n                                        \'frozen_inference_graph.pb\')\n    with mock.patch.object(\n        model_builder, \'build\', autospec=True) as mock_builder:\n      mock_builder.return_value = FakeModel(add_detection_masks=False)\n      pipeline_config = pipeline_pb2.TrainEvalPipelineConfig()\n      exporter.export_inference_graph(\n          input_type=\'image_tensor\',\n          pipeline_config=pipeline_config,\n          trained_checkpoint_prefix=trained_checkpoint_prefix,\n          output_directory=output_directory)\n    inference_graph = self._load_inference_graph(inference_graph_path)\n    with self.test_session(graph=inference_graph):\n      inference_graph.get_tensor_by_name(\'image_tensor:0\')\n      inference_graph.get_tensor_by_name(\'detection_boxes:0\')\n      inference_graph.get_tensor_by_name(\'detection_scores:0\')\n      inference_graph.get_tensor_by_name(\'detection_classes:0\')\n      inference_graph.get_tensor_by_name(\'num_detections:0\')\n      with self.assertRaises(KeyError):\n        inference_graph.get_tensor_by_name(\'detection_masks:0\')\n\n  def test_export_and_run_inference_with_image_tensor(self):\n    tmp_dir = self.get_temp_dir()\n    trained_checkpoint_prefix = os.path.join(tmp_dir, \'model.ckpt\')\n    self._save_checkpoint_from_mock_model(trained_checkpoint_prefix,\n                                          use_moving_averages=True)\n    output_directory = os.path.join(tmp_dir, \'output\')\n    inference_graph_path = os.path.join(output_directory,\n                                        \'frozen_inference_graph.pb\')\n    with mock.patch.object(\n        model_builder, \'build\', autospec=True) as mock_builder:\n      mock_builder.return_value = FakeModel(add_detection_masks=True)\n      pipeline_config = pipeline_pb2.TrainEvalPipelineConfig()\n      pipeline_config.eval_config.use_moving_averages = False\n      exporter.export_inference_graph(\n          input_type=\'image_tensor\',\n          pipeline_config=pipeline_config,\n          trained_checkpoint_prefix=trained_checkpoint_prefix,\n          output_directory=output_directory)\n\n    inference_graph = self._load_inference_graph(inference_graph_path)\n    with self.test_session(graph=inference_graph) as sess:\n      image_tensor = inference_graph.get_tensor_by_name(\'image_tensor:0\')\n      boxes = inference_graph.get_tensor_by_name(\'detection_boxes:0\')\n      scores = inference_graph.get_tensor_by_name(\'detection_scores:0\')\n      classes = inference_graph.get_tensor_by_name(\'detection_classes:0\')\n      masks = inference_graph.get_tensor_by_name(\'detection_masks:0\')\n      num_detections = inference_graph.get_tensor_by_name(\'num_detections:0\')\n      (boxes_np, scores_np, classes_np, masks_np, num_detections_np) = sess.run(\n          [boxes, scores, classes, masks, num_detections],\n          feed_dict={image_tensor: np.ones((2, 4, 4, 3)).astype(np.uint8)})\n      self.assertAllClose(boxes_np, [[[0.0, 0.0, 0.5, 0.5],\n                                      [0.5, 0.5, 0.8, 0.8]],\n                                     [[0.5, 0.5, 1.0, 1.0],\n                                      [0.0, 0.0, 0.0, 0.0]]])\n      self.assertAllClose(scores_np, [[0.7, 0.6],\n                                      [0.9, 0.0]])\n      self.assertAllClose(classes_np, [[1, 2],\n                                       [2, 1]])\n      self.assertAllClose(masks_np, np.arange(64).reshape([2, 2, 4, 4]))\n      self.assertAllClose(num_detections_np, [2, 1])\n\n  def _create_encoded_image_string(self, image_array_np, encoding_format):\n    od_graph = tf.Graph()\n    with od_graph.as_default():\n      if encoding_format == \'jpg\':\n        encoded_string = tf.image.encode_jpeg(image_array_np)\n      elif encoding_format == \'png\':\n        encoded_string = tf.image.encode_png(image_array_np)\n      else:\n        raise ValueError(\'Supports only the following formats: `jpg`, `png`\')\n    with self.test_session(graph=od_graph):\n      return encoded_string.eval()\n\n  def test_export_and_run_inference_with_encoded_image_string_tensor(self):\n    tmp_dir = self.get_temp_dir()\n    trained_checkpoint_prefix = os.path.join(tmp_dir, \'model.ckpt\')\n    self._save_checkpoint_from_mock_model(trained_checkpoint_prefix,\n                                          use_moving_averages=True)\n    output_directory = os.path.join(tmp_dir, \'output\')\n    inference_graph_path = os.path.join(output_directory,\n                                        \'frozen_inference_graph.pb\')\n    with mock.patch.object(\n        model_builder, \'build\', autospec=True) as mock_builder:\n      mock_builder.return_value = FakeModel(add_detection_masks=True)\n      pipeline_config = pipeline_pb2.TrainEvalPipelineConfig()\n      pipeline_config.eval_config.use_moving_averages = False\n      exporter.export_inference_graph(\n          input_type=\'encoded_image_string_tensor\',\n          pipeline_config=pipeline_config,\n          trained_checkpoint_prefix=trained_checkpoint_prefix,\n          output_directory=output_directory)\n\n    inference_graph = self._load_inference_graph(inference_graph_path)\n    jpg_image_str = self._create_encoded_image_string(\n        np.ones((4, 4, 3)).astype(np.uint8), \'jpg\')\n    png_image_str = self._create_encoded_image_string(\n        np.ones((4, 4, 3)).astype(np.uint8), \'png\')\n    with self.test_session(graph=inference_graph) as sess:\n      image_str_tensor = inference_graph.get_tensor_by_name(\n          \'encoded_image_string_tensor:0\')\n      boxes = inference_graph.get_tensor_by_name(\'detection_boxes:0\')\n      scores = inference_graph.get_tensor_by_name(\'detection_scores:0\')\n      classes = inference_graph.get_tensor_by_name(\'detection_classes:0\')\n      masks = inference_graph.get_tensor_by_name(\'detection_masks:0\')\n      num_detections = inference_graph.get_tensor_by_name(\'num_detections:0\')\n      for image_str in [jpg_image_str, png_image_str]:\n        image_str_batch_np = np.hstack([image_str]* 2)\n        (boxes_np, scores_np, classes_np, masks_np,\n         num_detections_np) = sess.run(\n             [boxes, scores, classes, masks, num_detections],\n             feed_dict={image_str_tensor: image_str_batch_np})\n        self.assertAllClose(boxes_np, [[[0.0, 0.0, 0.5, 0.5],\n                                        [0.5, 0.5, 0.8, 0.8]],\n                                       [[0.5, 0.5, 1.0, 1.0],\n                                        [0.0, 0.0, 0.0, 0.0]]])\n        self.assertAllClose(scores_np, [[0.7, 0.6],\n                                        [0.9, 0.0]])\n        self.assertAllClose(classes_np, [[1, 2],\n                                         [2, 1]])\n        self.assertAllClose(masks_np, np.arange(64).reshape([2, 2, 4, 4]))\n        self.assertAllClose(num_detections_np, [2, 1])\n\n  def test_raise_runtime_error_on_images_with_different_sizes(self):\n    tmp_dir = self.get_temp_dir()\n    trained_checkpoint_prefix = os.path.join(tmp_dir, \'model.ckpt\')\n    self._save_checkpoint_from_mock_model(trained_checkpoint_prefix,\n                                          use_moving_averages=True)\n    output_directory = os.path.join(tmp_dir, \'output\')\n    inference_graph_path = os.path.join(output_directory,\n                                        \'frozen_inference_graph.pb\')\n    with mock.patch.object(\n        model_builder, \'build\', autospec=True) as mock_builder:\n      mock_builder.return_value = FakeModel(add_detection_masks=True)\n      pipeline_config = pipeline_pb2.TrainEvalPipelineConfig()\n      pipeline_config.eval_config.use_moving_averages = False\n      exporter.export_inference_graph(\n          input_type=\'encoded_image_string_tensor\',\n          pipeline_config=pipeline_config,\n          trained_checkpoint_prefix=trained_checkpoint_prefix,\n          output_directory=output_directory)\n\n    inference_graph = self._load_inference_graph(inference_graph_path)\n    large_image = self._create_encoded_image_string(\n        np.ones((4, 4, 3)).astype(np.uint8), \'jpg\')\n    small_image = self._create_encoded_image_string(\n        np.ones((2, 2, 3)).astype(np.uint8), \'jpg\')\n\n    image_str_batch_np = np.hstack([large_image, small_image])\n    with self.test_session(graph=inference_graph) as sess:\n      image_str_tensor = inference_graph.get_tensor_by_name(\n          \'encoded_image_string_tensor:0\')\n      boxes = inference_graph.get_tensor_by_name(\'detection_boxes:0\')\n      scores = inference_graph.get_tensor_by_name(\'detection_scores:0\')\n      classes = inference_graph.get_tensor_by_name(\'detection_classes:0\')\n      masks = inference_graph.get_tensor_by_name(\'detection_masks:0\')\n      num_detections = inference_graph.get_tensor_by_name(\'num_detections:0\')\n      with self.assertRaisesRegexp(tf.errors.InvalidArgumentError,\n                                   \'TensorArray.*shape\'):\n        sess.run([boxes, scores, classes, masks, num_detections],\n                 feed_dict={image_str_tensor: image_str_batch_np})\n\n  def test_export_and_run_inference_with_tf_example(self):\n    tmp_dir = self.get_temp_dir()\n    trained_checkpoint_prefix = os.path.join(tmp_dir, \'model.ckpt\')\n    self._save_checkpoint_from_mock_model(trained_checkpoint_prefix,\n                                          use_moving_averages=True)\n    output_directory = os.path.join(tmp_dir, \'output\')\n    inference_graph_path = os.path.join(output_directory,\n                                        \'frozen_inference_graph.pb\')\n    with mock.patch.object(\n        model_builder, \'build\', autospec=True) as mock_builder:\n      mock_builder.return_value = FakeModel(add_detection_masks=True)\n      pipeline_config = pipeline_pb2.TrainEvalPipelineConfig()\n      pipeline_config.eval_config.use_moving_averages = False\n      exporter.export_inference_graph(\n          input_type=\'tf_example\',\n          pipeline_config=pipeline_config,\n          trained_checkpoint_prefix=trained_checkpoint_prefix,\n          output_directory=output_directory)\n\n    inference_graph = self._load_inference_graph(inference_graph_path)\n    tf_example_np = np.expand_dims(self._create_tf_example(\n        np.ones((4, 4, 3)).astype(np.uint8)), axis=0)\n    with self.test_session(graph=inference_graph) as sess:\n      tf_example = inference_graph.get_tensor_by_name(\'tf_example:0\')\n      boxes = inference_graph.get_tensor_by_name(\'detection_boxes:0\')\n      scores = inference_graph.get_tensor_by_name(\'detection_scores:0\')\n      classes = inference_graph.get_tensor_by_name(\'detection_classes:0\')\n      masks = inference_graph.get_tensor_by_name(\'detection_masks:0\')\n      num_detections = inference_graph.get_tensor_by_name(\'num_detections:0\')\n      (boxes_np, scores_np, classes_np, masks_np, num_detections_np) = sess.run(\n          [boxes, scores, classes, masks, num_detections],\n          feed_dict={tf_example: tf_example_np})\n      self.assertAllClose(boxes_np, [[[0.0, 0.0, 0.5, 0.5],\n                                      [0.5, 0.5, 0.8, 0.8]],\n                                     [[0.5, 0.5, 1.0, 1.0],\n                                      [0.0, 0.0, 0.0, 0.0]]])\n      self.assertAllClose(scores_np, [[0.7, 0.6],\n                                      [0.9, 0.0]])\n      self.assertAllClose(classes_np, [[1, 2],\n                                       [2, 1]])\n      self.assertAllClose(masks_np, np.arange(64).reshape([2, 2, 4, 4]))\n      self.assertAllClose(num_detections_np, [2, 1])\n\n  def test_write_frozen_graph(self):\n    tmp_dir = self.get_temp_dir()\n    trained_checkpoint_prefix = os.path.join(tmp_dir, \'model.ckpt\')\n    self._save_checkpoint_from_mock_model(trained_checkpoint_prefix,\n                                          use_moving_averages=True)\n    output_directory = os.path.join(tmp_dir, \'output\')\n    inference_graph_path = os.path.join(output_directory,\n                                        \'frozen_inference_graph.pb\')\n    tf.gfile.MakeDirs(output_directory)\n    with mock.patch.object(\n        model_builder, \'build\', autospec=True) as mock_builder:\n      mock_builder.return_value = FakeModel(add_detection_masks=True)\n      pipeline_config = pipeline_pb2.TrainEvalPipelineConfig()\n      pipeline_config.eval_config.use_moving_averages = False\n      detection_model = model_builder.build(pipeline_config.model,\n                                            is_training=False)\n      outputs, _ = exporter._build_detection_graph(\n          input_type=\'tf_example\',\n          detection_model=detection_model,\n          input_shape=None,\n          output_collection_name=\'inference_op\',\n          graph_hook_fn=None)\n      output_node_names = \',\'.join(outputs.keys())\n      saver = tf.train.Saver()\n      input_saver_def = saver.as_saver_def()\n      frozen_graph_def = exporter.freeze_graph_with_def_protos(\n          input_graph_def=tf.get_default_graph().as_graph_def(),\n          input_saver_def=input_saver_def,\n          input_checkpoint=trained_checkpoint_prefix,\n          output_node_names=output_node_names,\n          restore_op_name=\'save/restore_all\',\n          filename_tensor_name=\'save/Const:0\',\n          clear_devices=True,\n          initializer_nodes=\'\')\n      exporter.write_frozen_graph(inference_graph_path, frozen_graph_def)\n\n    inference_graph = self._load_inference_graph(inference_graph_path)\n    tf_example_np = np.expand_dims(self._create_tf_example(\n        np.ones((4, 4, 3)).astype(np.uint8)), axis=0)\n    with self.test_session(graph=inference_graph) as sess:\n      tf_example = inference_graph.get_tensor_by_name(\'tf_example:0\')\n      boxes = inference_graph.get_tensor_by_name(\'detection_boxes:0\')\n      scores = inference_graph.get_tensor_by_name(\'detection_scores:0\')\n      classes = inference_graph.get_tensor_by_name(\'detection_classes:0\')\n      masks = inference_graph.get_tensor_by_name(\'detection_masks:0\')\n      num_detections = inference_graph.get_tensor_by_name(\'num_detections:0\')\n      (boxes_np, scores_np, classes_np, masks_np, num_detections_np) = sess.run(\n          [boxes, scores, classes, masks, num_detections],\n          feed_dict={tf_example: tf_example_np})\n      self.assertAllClose(boxes_np, [[[0.0, 0.0, 0.5, 0.5],\n                                      [0.5, 0.5, 0.8, 0.8]],\n                                     [[0.5, 0.5, 1.0, 1.0],\n                                      [0.0, 0.0, 0.0, 0.0]]])\n      self.assertAllClose(scores_np, [[0.7, 0.6],\n                                      [0.9, 0.0]])\n      self.assertAllClose(classes_np, [[1, 2],\n                                       [2, 1]])\n      self.assertAllClose(masks_np, np.arange(64).reshape([2, 2, 4, 4]))\n      self.assertAllClose(num_detections_np, [2, 1])\n\n  def test_export_graph_saves_pipeline_file(self):\n    tmp_dir = self.get_temp_dir()\n    trained_checkpoint_prefix = os.path.join(tmp_dir, \'model.ckpt\')\n    self._save_checkpoint_from_mock_model(trained_checkpoint_prefix,\n                                          use_moving_averages=True)\n    output_directory = os.path.join(tmp_dir, \'output\')\n    with mock.patch.object(\n        model_builder, \'build\', autospec=True) as mock_builder:\n      mock_builder.return_value = FakeModel()\n      pipeline_config = pipeline_pb2.TrainEvalPipelineConfig()\n      exporter.export_inference_graph(\n          input_type=\'image_tensor\',\n          pipeline_config=pipeline_config,\n          trained_checkpoint_prefix=trained_checkpoint_prefix,\n          output_directory=output_directory)\n      expected_pipeline_path = os.path.join(\n          output_directory, \'pipeline.config\')\n      self.assertTrue(os.path.exists(expected_pipeline_path))\n\n      written_pipeline_config = pipeline_pb2.TrainEvalPipelineConfig()\n      with tf.gfile.GFile(expected_pipeline_path, \'r\') as f:\n        proto_str = f.read()\n        text_format.Merge(proto_str, written_pipeline_config)\n        self.assertProtoEquals(pipeline_config, written_pipeline_config)\n\n  def test_export_saved_model_and_run_inference(self):\n    tmp_dir = self.get_temp_dir()\n    trained_checkpoint_prefix = os.path.join(tmp_dir, \'model.ckpt\')\n    self._save_checkpoint_from_mock_model(trained_checkpoint_prefix,\n                                          use_moving_averages=False)\n    output_directory = os.path.join(tmp_dir, \'output\')\n    saved_model_path = os.path.join(output_directory, \'saved_model\')\n\n    with mock.patch.object(\n        model_builder, \'build\', autospec=True) as mock_builder:\n      mock_builder.return_value = FakeModel(add_detection_masks=True)\n      pipeline_config = pipeline_pb2.TrainEvalPipelineConfig()\n      pipeline_config.eval_config.use_moving_averages = False\n      exporter.export_inference_graph(\n          input_type=\'tf_example\',\n          pipeline_config=pipeline_config,\n          trained_checkpoint_prefix=trained_checkpoint_prefix,\n          output_directory=output_directory)\n\n    tf_example_np = np.hstack([self._create_tf_example(\n        np.ones((4, 4, 3)).astype(np.uint8))] * 2)\n    with tf.Graph().as_default() as od_graph:\n      with self.test_session(graph=od_graph) as sess:\n        meta_graph = tf.saved_model.loader.load(\n            sess, [tf.saved_model.tag_constants.SERVING], saved_model_path)\n\n        signature = meta_graph.signature_def[\'serving_default\']\n        input_tensor_name = signature.inputs[\'inputs\'].name\n        tf_example = od_graph.get_tensor_by_name(input_tensor_name)\n\n        boxes = od_graph.get_tensor_by_name(\n            signature.outputs[\'detection_boxes\'].name)\n        scores = od_graph.get_tensor_by_name(\n            signature.outputs[\'detection_scores\'].name)\n        classes = od_graph.get_tensor_by_name(\n            signature.outputs[\'detection_classes\'].name)\n        masks = od_graph.get_tensor_by_name(\n            signature.outputs[\'detection_masks\'].name)\n        num_detections = od_graph.get_tensor_by_name(\n            signature.outputs[\'num_detections\'].name)\n\n        (boxes_np, scores_np, classes_np, masks_np,\n         num_detections_np) = sess.run(\n             [boxes, scores, classes, masks, num_detections],\n             feed_dict={tf_example: tf_example_np})\n        self.assertAllClose(boxes_np, [[[0.0, 0.0, 0.5, 0.5],\n                                        [0.5, 0.5, 0.8, 0.8]],\n                                       [[0.5, 0.5, 1.0, 1.0],\n                                        [0.0, 0.0, 0.0, 0.0]]])\n        self.assertAllClose(scores_np, [[0.7, 0.6],\n                                        [0.9, 0.0]])\n        self.assertAllClose(classes_np, [[1, 2],\n                                         [2, 1]])\n        self.assertAllClose(masks_np, np.arange(64).reshape([2, 2, 4, 4]))\n        self.assertAllClose(num_detections_np, [2, 1])\n\n  def test_write_saved_model(self):\n    tmp_dir = self.get_temp_dir()\n    trained_checkpoint_prefix = os.path.join(tmp_dir, \'model.ckpt\')\n    self._save_checkpoint_from_mock_model(trained_checkpoint_prefix,\n                                          use_moving_averages=False)\n    output_directory = os.path.join(tmp_dir, \'output\')\n    saved_model_path = os.path.join(output_directory, \'saved_model\')\n    tf.gfile.MakeDirs(output_directory)\n    with mock.patch.object(\n        model_builder, \'build\', autospec=True) as mock_builder:\n      mock_builder.return_value = FakeModel(add_detection_masks=True)\n      pipeline_config = pipeline_pb2.TrainEvalPipelineConfig()\n      pipeline_config.eval_config.use_moving_averages = False\n      detection_model = model_builder.build(pipeline_config.model,\n                                            is_training=False)\n      outputs, placeholder_tensor = exporter._build_detection_graph(\n          input_type=\'tf_example\',\n          detection_model=detection_model,\n          input_shape=None,\n          output_collection_name=\'inference_op\',\n          graph_hook_fn=None)\n      output_node_names = \',\'.join(outputs.keys())\n      saver = tf.train.Saver()\n      input_saver_def = saver.as_saver_def()\n      frozen_graph_def = exporter.freeze_graph_with_def_protos(\n          input_graph_def=tf.get_default_graph().as_graph_def(),\n          input_saver_def=input_saver_def,\n          input_checkpoint=trained_checkpoint_prefix,\n          output_node_names=output_node_names,\n          restore_op_name=\'save/restore_all\',\n          filename_tensor_name=\'save/Const:0\',\n          clear_devices=True,\n          initializer_nodes=\'\')\n      exporter.write_saved_model(\n          saved_model_path=saved_model_path,\n          frozen_graph_def=frozen_graph_def,\n          inputs=placeholder_tensor,\n          outputs=outputs)\n\n    tf_example_np = np.hstack([self._create_tf_example(\n        np.ones((4, 4, 3)).astype(np.uint8))] * 2)\n    with tf.Graph().as_default() as od_graph:\n      with self.test_session(graph=od_graph) as sess:\n        meta_graph = tf.saved_model.loader.load(\n            sess, [tf.saved_model.tag_constants.SERVING], saved_model_path)\n\n        signature = meta_graph.signature_def[\'serving_default\']\n        input_tensor_name = signature.inputs[\'inputs\'].name\n        tf_example = od_graph.get_tensor_by_name(input_tensor_name)\n\n        boxes = od_graph.get_tensor_by_name(\n            signature.outputs[\'detection_boxes\'].name)\n        scores = od_graph.get_tensor_by_name(\n            signature.outputs[\'detection_scores\'].name)\n        classes = od_graph.get_tensor_by_name(\n            signature.outputs[\'detection_classes\'].name)\n        masks = od_graph.get_tensor_by_name(\n            signature.outputs[\'detection_masks\'].name)\n        num_detections = od_graph.get_tensor_by_name(\n            signature.outputs[\'num_detections\'].name)\n\n        (boxes_np, scores_np, classes_np, masks_np,\n         num_detections_np) = sess.run(\n             [boxes, scores, classes, masks, num_detections],\n             feed_dict={tf_example: tf_example_np})\n        self.assertAllClose(boxes_np, [[[0.0, 0.0, 0.5, 0.5],\n                                        [0.5, 0.5, 0.8, 0.8]],\n                                       [[0.5, 0.5, 1.0, 1.0],\n                                        [0.0, 0.0, 0.0, 0.0]]])\n        self.assertAllClose(scores_np, [[0.7, 0.6],\n                                        [0.9, 0.0]])\n        self.assertAllClose(classes_np, [[1, 2],\n                                         [2, 1]])\n        self.assertAllClose(masks_np, np.arange(64).reshape([2, 2, 4, 4]))\n        self.assertAllClose(num_detections_np, [2, 1])\n\n  def test_export_checkpoint_and_run_inference(self):\n    tmp_dir = self.get_temp_dir()\n    trained_checkpoint_prefix = os.path.join(tmp_dir, \'model.ckpt\')\n    self._save_checkpoint_from_mock_model(trained_checkpoint_prefix,\n                                          use_moving_averages=False)\n    output_directory = os.path.join(tmp_dir, \'output\')\n    model_path = os.path.join(output_directory, \'model.ckpt\')\n    meta_graph_path = model_path + \'.meta\'\n\n    with mock.patch.object(\n        model_builder, \'build\', autospec=True) as mock_builder:\n      mock_builder.return_value = FakeModel(add_detection_masks=True)\n      pipeline_config = pipeline_pb2.TrainEvalPipelineConfig()\n      pipeline_config.eval_config.use_moving_averages = False\n      exporter.export_inference_graph(\n          input_type=\'tf_example\',\n          pipeline_config=pipeline_config,\n          trained_checkpoint_prefix=trained_checkpoint_prefix,\n          output_directory=output_directory)\n\n    tf_example_np = np.hstack([self._create_tf_example(\n        np.ones((4, 4, 3)).astype(np.uint8))] * 2)\n    with tf.Graph().as_default() as od_graph:\n      with self.test_session(graph=od_graph) as sess:\n        new_saver = tf.train.import_meta_graph(meta_graph_path)\n        new_saver.restore(sess, model_path)\n\n        tf_example = od_graph.get_tensor_by_name(\'tf_example:0\')\n        boxes = od_graph.get_tensor_by_name(\'detection_boxes:0\')\n        scores = od_graph.get_tensor_by_name(\'detection_scores:0\')\n        classes = od_graph.get_tensor_by_name(\'detection_classes:0\')\n        masks = od_graph.get_tensor_by_name(\'detection_masks:0\')\n        num_detections = od_graph.get_tensor_by_name(\'num_detections:0\')\n        (boxes_np, scores_np, classes_np, masks_np,\n         num_detections_np) = sess.run(\n             [boxes, scores, classes, masks, num_detections],\n             feed_dict={tf_example: tf_example_np})\n        self.assertAllClose(boxes_np, [[[0.0, 0.0, 0.5, 0.5],\n                                        [0.5, 0.5, 0.8, 0.8]],\n                                       [[0.5, 0.5, 1.0, 1.0],\n                                        [0.0, 0.0, 0.0, 0.0]]])\n        self.assertAllClose(scores_np, [[0.7, 0.6],\n                                        [0.9, 0.0]])\n        self.assertAllClose(classes_np, [[1, 2],\n                                         [2, 1]])\n        self.assertAllClose(masks_np, np.arange(64).reshape([2, 2, 4, 4]))\n        self.assertAllClose(num_detections_np, [2, 1])\n\n  def test_write_graph_and_checkpoint(self):\n    tmp_dir = self.get_temp_dir()\n    trained_checkpoint_prefix = os.path.join(tmp_dir, \'model.ckpt\')\n    self._save_checkpoint_from_mock_model(trained_checkpoint_prefix,\n                                          use_moving_averages=False)\n    output_directory = os.path.join(tmp_dir, \'output\')\n    model_path = os.path.join(output_directory, \'model.ckpt\')\n    meta_graph_path = model_path + \'.meta\'\n    tf.gfile.MakeDirs(output_directory)\n    with mock.patch.object(\n        model_builder, \'build\', autospec=True) as mock_builder:\n      mock_builder.return_value = FakeModel(add_detection_masks=True)\n      pipeline_config = pipeline_pb2.TrainEvalPipelineConfig()\n      pipeline_config.eval_config.use_moving_averages = False\n      detection_model = model_builder.build(pipeline_config.model,\n                                            is_training=False)\n      exporter._build_detection_graph(\n          input_type=\'tf_example\',\n          detection_model=detection_model,\n          input_shape=None,\n          output_collection_name=\'inference_op\',\n          graph_hook_fn=None)\n      saver = tf.train.Saver()\n      input_saver_def = saver.as_saver_def()\n      exporter.write_graph_and_checkpoint(\n          inference_graph_def=tf.get_default_graph().as_graph_def(),\n          model_path=model_path,\n          input_saver_def=input_saver_def,\n          trained_checkpoint_prefix=trained_checkpoint_prefix)\n\n    tf_example_np = np.hstack([self._create_tf_example(\n        np.ones((4, 4, 3)).astype(np.uint8))] * 2)\n    with tf.Graph().as_default() as od_graph:\n      with self.test_session(graph=od_graph) as sess:\n        new_saver = tf.train.import_meta_graph(meta_graph_path)\n        new_saver.restore(sess, model_path)\n\n        tf_example = od_graph.get_tensor_by_name(\'tf_example:0\')\n        boxes = od_graph.get_tensor_by_name(\'detection_boxes:0\')\n        scores = od_graph.get_tensor_by_name(\'detection_scores:0\')\n        classes = od_graph.get_tensor_by_name(\'detection_classes:0\')\n        masks = od_graph.get_tensor_by_name(\'detection_masks:0\')\n        num_detections = od_graph.get_tensor_by_name(\'num_detections:0\')\n        (boxes_np, scores_np, classes_np, masks_np,\n         num_detections_np) = sess.run(\n             [boxes, scores, classes, masks, num_detections],\n             feed_dict={tf_example: tf_example_np})\n        self.assertAllClose(boxes_np, [[[0.0, 0.0, 0.5, 0.5],\n                                        [0.5, 0.5, 0.8, 0.8]],\n                                       [[0.5, 0.5, 1.0, 1.0],\n                                        [0.0, 0.0, 0.0, 0.0]]])\n        self.assertAllClose(scores_np, [[0.7, 0.6],\n                                        [0.9, 0.0]])\n        self.assertAllClose(classes_np, [[1, 2],\n                                         [2, 1]])\n        self.assertAllClose(masks_np, np.arange(64).reshape([2, 2, 4, 4]))\n        self.assertAllClose(num_detections_np, [2, 1])\n\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
src/object_detection/inputs.py,20,"b'# Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Model input function for tf-learn object detection model.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport functools\n\nimport tensorflow as tf\nfrom object_detection.builders import dataset_builder\nfrom object_detection.builders import image_resizer_builder\nfrom object_detection.builders import model_builder\nfrom object_detection.builders import preprocessor_builder\nfrom object_detection.core import preprocessor\nfrom object_detection.core import standard_fields as fields\nfrom object_detection.data_decoders import tf_example_decoder\nfrom object_detection.protos import eval_pb2\nfrom object_detection.protos import input_reader_pb2\nfrom object_detection.protos import model_pb2\nfrom object_detection.protos import train_pb2\nfrom object_detection.utils import config_util\nfrom object_detection.utils import dataset_util\nfrom object_detection.utils import ops as util_ops\n\nHASH_KEY = \'hash\'\nHASH_BINS = 1 << 31\nSERVING_FED_EXAMPLE_KEY = \'serialized_example\'\n\n# A map of names to methods that help build the input pipeline.\nINPUT_BUILDER_UTIL_MAP = {\n    \'dataset_build\': dataset_builder.build,\n}\n\n\ndef transform_input_data(tensor_dict,\n                         model_preprocess_fn,\n                         image_resizer_fn,\n                         num_classes,\n                         data_augmentation_fn=None,\n                         merge_multiple_boxes=False,\n                         retain_original_image=False):\n  """"""A single function that is responsible for all input data transformations.\n\n  Data transformation functions are applied in the following order.\n  1. data_augmentation_fn (optional): applied on tensor_dict.\n  2. model_preprocess_fn: applied only on image tensor in tensor_dict.\n  3. image_resizer_fn: applied only on instance mask tensor in tensor_dict.\n  4. one_hot_encoding: applied to classes tensor in tensor_dict.\n  5. merge_multiple_boxes (optional): when groundtruth boxes are exactly the\n     same they can be merged into a single box with an associated k-hot class\n     label.\n\n  Args:\n    tensor_dict: dictionary containing input tensors keyed by\n      fields.InputDataFields.\n    model_preprocess_fn: model\'s preprocess function to apply on image tensor.\n      This function must take in a 4-D float tensor and return a 4-D preprocess\n      float tensor and a tensor containing the true image shape.\n    image_resizer_fn: image resizer function to apply on groundtruth instance\n      masks. This function must take a 4-D float tensor of image and a 4-D\n      tensor of instances masks and return resized version of these along with\n      the true shapes.\n    num_classes: number of max classes to one-hot (or k-hot) encode the class\n      labels.\n    data_augmentation_fn: (optional) data augmentation function to apply on\n      input `tensor_dict`.\n    merge_multiple_boxes: (optional) whether to merge multiple groundtruth boxes\n      and classes for a given image if the boxes are exactly the same.\n    retain_original_image: (optional) whether to retain original image in the\n      output dictionary.\n\n  Returns:\n    A dictionary keyed by fields.InputDataFields containing the tensors obtained\n    after applying all the transformations.\n  """"""\n  if retain_original_image:\n    tensor_dict[fields.InputDataFields.\n                original_image] = tensor_dict[fields.InputDataFields.image]\n\n  # Apply data augmentation ops.\n  if data_augmentation_fn is not None:\n    tensor_dict = data_augmentation_fn(tensor_dict)\n\n  # Apply model preprocessing ops and resize instance masks.\n  image = tf.expand_dims(\n      tf.to_float(tensor_dict[fields.InputDataFields.image]), axis=0)\n  preprocessed_resized_image, true_image_shape = model_preprocess_fn(image)\n  tensor_dict[fields.InputDataFields.image] = tf.squeeze(\n      preprocessed_resized_image, axis=0)\n  tensor_dict[fields.InputDataFields.true_image_shape] = tf.squeeze(\n      true_image_shape, axis=0)\n  if fields.InputDataFields.groundtruth_instance_masks in tensor_dict:\n    masks = tensor_dict[fields.InputDataFields.groundtruth_instance_masks]\n    _, resized_masks, _ = image_resizer_fn(image, masks)\n    tensor_dict[fields.InputDataFields.\n                groundtruth_instance_masks] = resized_masks\n\n  # Transform groundtruth classes to one hot encodings.\n  label_offset = 1\n  zero_indexed_groundtruth_classes = tensor_dict[\n      fields.InputDataFields.groundtruth_classes] - label_offset\n  tensor_dict[fields.InputDataFields.groundtruth_classes] = tf.one_hot(\n      zero_indexed_groundtruth_classes, num_classes)\n\n  if merge_multiple_boxes:\n    merged_boxes, merged_classes, _ = util_ops.merge_boxes_with_multiple_labels(\n        tensor_dict[fields.InputDataFields.groundtruth_boxes],\n        zero_indexed_groundtruth_classes, num_classes)\n    tensor_dict[fields.InputDataFields.groundtruth_boxes] = merged_boxes\n    tensor_dict[fields.InputDataFields.groundtruth_classes] = merged_classes\n\n  return tensor_dict\n\n\ndef augment_input_data(tensor_dict, data_augmentation_options):\n  """"""Applies data augmentation ops to input tensors.\n\n  Args:\n    tensor_dict: A dictionary of input tensors keyed by fields.InputDataFields.\n    data_augmentation_options: A list of tuples, where each tuple contains a\n      function and a dictionary that contains arguments and their values.\n      Usually, this is the output of core/preprocessor.build.\n\n  Returns:\n    A dictionary of tensors obtained by applying data augmentation ops to the\n    input tensor dictionary.\n  """"""\n  tensor_dict[fields.InputDataFields.image] = tf.expand_dims(\n      tf.to_float(tensor_dict[fields.InputDataFields.image]), 0)\n\n  include_instance_masks = (fields.InputDataFields.groundtruth_instance_masks\n                            in tensor_dict)\n  include_keypoints = (fields.InputDataFields.groundtruth_keypoints\n                       in tensor_dict)\n  tensor_dict = preprocessor.preprocess(\n      tensor_dict, data_augmentation_options,\n      func_arg_map=preprocessor.get_default_func_arg_map(\n          include_instance_masks=include_instance_masks,\n          include_keypoints=include_keypoints))\n  tensor_dict[fields.InputDataFields.image] = tf.squeeze(\n      tensor_dict[fields.InputDataFields.image], axis=0)\n  return tensor_dict\n\n\ndef create_train_input_fn(train_config, train_input_config,\n                          model_config):\n  """"""Creates a train `input` function for `Estimator`.\n\n  Args:\n    train_config: A train_pb2.TrainConfig.\n    train_input_config: An input_reader_pb2.InputReader.\n    model_config: A model_pb2.DetectionModel.\n\n  Returns:\n    `input_fn` for `Estimator` in TRAIN mode.\n  """"""\n\n  def _train_input_fn(params=None):\n    """"""Returns `features` and `labels` tensor dictionaries for training.\n\n    Args:\n      params: Parameter dictionary passed from the estimator.\n\n    Returns:\n      features: Dictionary of feature tensors.\n        features[fields.InputDataFields.image] is a [batch_size, H, W, C]\n          float32 tensor with preprocessed images.\n        features[HASH_KEY] is a [batch_size] int32 tensor representing unique\n          identifiers for the images.\n        features[fields.InputDataFields.true_image_shape] is a [batch_size, 3]\n          int32 tensor representing the true image shapes, as preprocessed\n          images could be padded.\n      labels: Dictionary of groundtruth tensors.\n        labels[fields.InputDataFields.num_groundtruth_boxes] is a [batch_size]\n          int32 tensor indicating the number of groundtruth boxes.\n        labels[fields.InputDataFields.groundtruth_boxes] is a\n          [batch_size, num_boxes, 4] float32 tensor containing the corners of\n          the groundtruth boxes.\n        labels[fields.InputDataFields.groundtruth_classes] is a\n          [batch_size, num_boxes, num_classes] float32 one-hot tensor of\n          classes.\n        labels[fields.InputDataFields.groundtruth_weights] is a\n          [batch_size, num_boxes] float32 tensor containing groundtruth weights\n          for the boxes.\n        -- Optional --\n        labels[fields.InputDataFields.groundtruth_instance_masks] is a\n          [batch_size, num_boxes, H, W] float32 tensor containing only binary\n          values, which represent instance masks for objects.\n        labels[fields.InputDataFields.groundtruth_keypoints] is a\n          [batch_size, num_boxes, num_keypoints, 2] float32 tensor containing\n          keypoints for each box.\n\n    Raises:\n      TypeError: if the `train_config`, `train_input_config` or `model_config`\n        are not of the correct type.\n    """"""\n    if not isinstance(train_config, train_pb2.TrainConfig):\n      raise TypeError(\'For training mode, the `train_config` must be a \'\n                      \'train_pb2.TrainConfig.\')\n    if not isinstance(train_input_config, input_reader_pb2.InputReader):\n      raise TypeError(\'The `train_input_config` must be a \'\n                      \'input_reader_pb2.InputReader.\')\n    if not isinstance(model_config, model_pb2.DetectionModel):\n      raise TypeError(\'The `model_config` must be a \'\n                      \'model_pb2.DetectionModel.\')\n\n    data_augmentation_options = [\n        preprocessor_builder.build(step)\n        for step in train_config.data_augmentation_options\n    ]\n    data_augmentation_fn = functools.partial(\n        augment_input_data, data_augmentation_options=data_augmentation_options)\n\n    model = model_builder.build(model_config, is_training=True)\n    image_resizer_config = config_util.get_image_resizer_config(model_config)\n    image_resizer_fn = image_resizer_builder.build(image_resizer_config)\n\n    transform_data_fn = functools.partial(\n        transform_input_data, model_preprocess_fn=model.preprocess,\n        image_resizer_fn=image_resizer_fn,\n        num_classes=config_util.get_number_of_classes(model_config),\n        data_augmentation_fn=data_augmentation_fn)\n    dataset = INPUT_BUILDER_UTIL_MAP[\'dataset_build\'](\n        train_input_config,\n        transform_input_data_fn=transform_data_fn,\n        batch_size=params[\'batch_size\'] if params else train_config.batch_size,\n        max_num_boxes=train_config.max_number_of_boxes,\n        num_classes=config_util.get_number_of_classes(model_config),\n        spatial_image_shape=config_util.get_spatial_image_size(\n            image_resizer_config))\n    tensor_dict = dataset_util.make_initializable_iterator(dataset).get_next()\n\n    hash_from_source_id = tf.string_to_hash_bucket_fast(\n        tensor_dict[fields.InputDataFields.source_id], HASH_BINS)\n    features = {\n        fields.InputDataFields.image: tensor_dict[fields.InputDataFields.image],\n        HASH_KEY: tf.cast(hash_from_source_id, tf.int32),\n        fields.InputDataFields.true_image_shape: tensor_dict[\n            fields.InputDataFields.true_image_shape]\n    }\n\n    labels = {\n        fields.InputDataFields.num_groundtruth_boxes: tensor_dict[\n            fields.InputDataFields.num_groundtruth_boxes],\n        fields.InputDataFields.groundtruth_boxes: tensor_dict[\n            fields.InputDataFields.groundtruth_boxes],\n        fields.InputDataFields.groundtruth_classes: tensor_dict[\n            fields.InputDataFields.groundtruth_classes],\n        fields.InputDataFields.groundtruth_weights: tensor_dict[\n            fields.InputDataFields.groundtruth_weights]\n    }\n    if fields.InputDataFields.groundtruth_keypoints in tensor_dict:\n      labels[fields.InputDataFields.groundtruth_keypoints] = tensor_dict[\n          fields.InputDataFields.groundtruth_keypoints]\n    if fields.InputDataFields.groundtruth_instance_masks in tensor_dict:\n      labels[fields.InputDataFields.groundtruth_instance_masks] = tensor_dict[\n          fields.InputDataFields.groundtruth_instance_masks]\n\n    return features, labels\n\n  return _train_input_fn\n\n\ndef create_eval_input_fn(eval_config, eval_input_config, model_config):\n  """"""Creates an eval `input` function for `Estimator`.\n\n  Args:\n    eval_config: An eval_pb2.EvalConfig.\n    eval_input_config: An input_reader_pb2.InputReader.\n    model_config: A model_pb2.DetectionModel.\n\n  Returns:\n    `input_fn` for `Estimator` in EVAL mode.\n  """"""\n\n  def _eval_input_fn(params=None):\n    """"""Returns `features` and `labels` tensor dictionaries for evaluation.\n\n    Args:\n      params: Parameter dictionary passed from the estimator.\n\n    Returns:\n      features: Dictionary of feature tensors.\n        features[fields.InputDataFields.image] is a [1, H, W, C] float32 tensor\n          with preprocessed images.\n        features[HASH_KEY] is a [1] int32 tensor representing unique\n          identifiers for the images.\n        features[fields.InputDataFields.true_image_shape] is a [1, 3]\n          int32 tensor representing the true image shapes, as preprocessed\n          images could be padded.\n        features[fields.InputDataFields.original_image] is a [1, H\', W\', C]\n          float32 tensor with the original image.\n      labels: Dictionary of groundtruth tensors.\n        labels[fields.InputDataFields.groundtruth_boxes] is a [1, num_boxes, 4]\n          float32 tensor containing the corners of the groundtruth boxes.\n        labels[fields.InputDataFields.groundtruth_classes] is a\n          [num_boxes, num_classes] float32 one-hot tensor of classes.\n        labels[fields.InputDataFields.groundtruth_area] is a [1, num_boxes]\n          float32 tensor containing object areas.\n        labels[fields.InputDataFields.groundtruth_is_crowd] is a [1, num_boxes]\n          bool tensor indicating if the boxes enclose a crowd.\n        labels[fields.InputDataFields.groundtruth_difficult] is a [1, num_boxes]\n          int32 tensor indicating if the boxes represent difficult instances.\n        -- Optional --\n        labels[fields.InputDataFields.groundtruth_instance_masks] is a\n          [1, num_boxes, H, W] float32 tensor containing only binary values,\n          which represent instance masks for objects.\n\n    Raises:\n      TypeError: if the `eval_config`, `eval_input_config` or `model_config`\n        are not of the correct type.\n    """"""\n    del params\n    if not isinstance(eval_config, eval_pb2.EvalConfig):\n      raise TypeError(\'For eval mode, the `eval_config` must be a \'\n                      \'train_pb2.EvalConfig.\')\n    if not isinstance(eval_input_config, input_reader_pb2.InputReader):\n      raise TypeError(\'The `eval_input_config` must be a \'\n                      \'input_reader_pb2.InputReader.\')\n    if not isinstance(model_config, model_pb2.DetectionModel):\n      raise TypeError(\'The `model_config` must be a \'\n                      \'model_pb2.DetectionModel.\')\n\n    num_classes = config_util.get_number_of_classes(model_config)\n    model = model_builder.build(model_config, is_training=False)\n    image_resizer_config = config_util.get_image_resizer_config(model_config)\n    image_resizer_fn = image_resizer_builder.build(image_resizer_config)\n\n    transform_data_fn = functools.partial(\n        transform_input_data, model_preprocess_fn=model.preprocess,\n        image_resizer_fn=image_resizer_fn,\n        num_classes=num_classes,\n        data_augmentation_fn=None,\n        retain_original_image=True)\n    dataset = INPUT_BUILDER_UTIL_MAP[\'dataset_build\'](\n        eval_input_config,\n        transform_input_data_fn=transform_data_fn,\n        batch_size=1,\n        num_classes=config_util.get_number_of_classes(model_config),\n        spatial_image_shape=config_util.get_spatial_image_size(\n            image_resizer_config))\n    input_dict = dataset_util.make_initializable_iterator(dataset).get_next()\n\n    hash_from_source_id = tf.string_to_hash_bucket_fast(\n        input_dict[fields.InputDataFields.source_id], HASH_BINS)\n    features = {\n        fields.InputDataFields.image:\n            input_dict[fields.InputDataFields.image],\n        fields.InputDataFields.original_image:\n            input_dict[fields.InputDataFields.original_image],\n        HASH_KEY: tf.cast(hash_from_source_id, tf.int32),\n        fields.InputDataFields.true_image_shape:\n            input_dict[fields.InputDataFields.true_image_shape]\n    }\n\n    labels = {\n        fields.InputDataFields.groundtruth_boxes:\n            input_dict[fields.InputDataFields.groundtruth_boxes],\n        fields.InputDataFields.groundtruth_classes:\n            input_dict[fields.InputDataFields.groundtruth_classes],\n        fields.InputDataFields.groundtruth_area:\n            input_dict[fields.InputDataFields.groundtruth_area],\n        fields.InputDataFields.groundtruth_is_crowd:\n            input_dict[fields.InputDataFields.groundtruth_is_crowd],\n        fields.InputDataFields.groundtruth_difficult:\n            tf.cast(input_dict[fields.InputDataFields.groundtruth_difficult],\n                    tf.int32)\n    }\n    if fields.InputDataFields.groundtruth_instance_masks in input_dict:\n      labels[fields.InputDataFields.groundtruth_instance_masks] = input_dict[\n          fields.InputDataFields.groundtruth_instance_masks]\n\n    return features, labels\n\n  return _eval_input_fn\n\n\ndef create_predict_input_fn(model_config):\n  """"""Creates a predict `input` function for `Estimator`.\n\n  Args:\n    model_config: A model_pb2.DetectionModel.\n\n  Returns:\n    `input_fn` for `Estimator` in PREDICT mode.\n  """"""\n\n  def _predict_input_fn(params=None):\n    """"""Decodes serialized tf.Examples and returns `ServingInputReceiver`.\n\n    Args:\n      params: Parameter dictionary passed from the estimator.\n\n    Returns:\n      `ServingInputReceiver`.\n    """"""\n    del params\n    example = tf.placeholder(dtype=tf.string, shape=[], name=\'input_feature\')\n\n    num_classes = config_util.get_number_of_classes(model_config)\n    model = model_builder.build(model_config, is_training=False)\n    image_resizer_config = config_util.get_image_resizer_config(model_config)\n    image_resizer_fn = image_resizer_builder.build(image_resizer_config)\n\n    transform_fn = functools.partial(\n        transform_input_data, model_preprocess_fn=model.preprocess,\n        image_resizer_fn=image_resizer_fn,\n        num_classes=num_classes,\n        data_augmentation_fn=None)\n\n    decoder = tf_example_decoder.TfExampleDecoder(load_instance_masks=False)\n    input_dict = transform_fn(decoder.decode(example))\n    images = tf.to_float(input_dict[fields.InputDataFields.image])\n    images = tf.expand_dims(images, axis=0)\n    true_image_shape = tf.expand_dims(\n        input_dict[fields.InputDataFields.true_image_shape], axis=0)\n\n    return tf.estimator.export.ServingInputReceiver(\n        features={\n            fields.InputDataFields.image: images,\n            fields.InputDataFields.true_image_shape: true_image_shape},\n        receiver_tensors={SERVING_FED_EXAMPLE_KEY: example})\n\n  return _predict_input_fn\n'"
src/object_detection/inputs_test.py,67,"b'# Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Tests for object_detection.tflearn.inputs.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport functools\nimport os\n\nimport numpy as np\nimport tensorflow as tf\n\nfrom object_detection import inputs\nfrom object_detection.core import preprocessor\nfrom object_detection.core import standard_fields as fields\nfrom object_detection.utils import config_util\n\nFLAGS = tf.flags.FLAGS\n\n\ndef _get_configs_for_model(model_name):\n  """"""Returns configurations for model.""""""\n  fname = os.path.join(\n      FLAGS.test_srcdir,\n      (\'google3/third_party/tensorflow_models/\'\n       \'object_detection/samples/configs/\' + model_name + \'.config\'))\n  label_map_path = os.path.join(FLAGS.test_srcdir,\n                                (\'google3/third_party/tensorflow_models/\'\n                                 \'object_detection/data/pet_label_map.pbtxt\'))\n  data_path = os.path.join(FLAGS.test_srcdir,\n                           (\'google3/third_party/tensorflow_models/\'\n                            \'object_detection/test_data/pets_examples.record\'))\n  configs = config_util.get_configs_from_pipeline_file(fname)\n  return config_util.merge_external_params_with_configs(\n      configs,\n      train_input_path=data_path,\n      eval_input_path=data_path,\n      label_map_path=label_map_path)\n\n\nclass InputsTest(tf.test.TestCase):\n\n  def test_faster_rcnn_resnet50_train_input(self):\n    """"""Tests the training input function for FasterRcnnResnet50.""""""\n    configs = _get_configs_for_model(\'faster_rcnn_resnet50_pets\')\n    configs[\'train_config\'].unpad_groundtruth_tensors = True\n    model_config = configs[\'model\']\n    model_config.faster_rcnn.num_classes = 37\n    train_input_fn = inputs.create_train_input_fn(\n        configs[\'train_config\'], configs[\'train_input_config\'], model_config)\n    features, labels = train_input_fn()\n\n    self.assertAllEqual([1, None, None, 3],\n                        features[fields.InputDataFields.image].shape.as_list())\n    self.assertEqual(tf.float32, features[fields.InputDataFields.image].dtype)\n    self.assertAllEqual([1],\n                        features[inputs.HASH_KEY].shape.as_list())\n    self.assertEqual(tf.int32, features[inputs.HASH_KEY].dtype)\n    self.assertAllEqual(\n        [1, 50, 4],\n        labels[fields.InputDataFields.groundtruth_boxes].shape.as_list())\n    self.assertEqual(tf.float32,\n                     labels[fields.InputDataFields.groundtruth_boxes].dtype)\n    self.assertAllEqual(\n        [1, 50, model_config.faster_rcnn.num_classes],\n        labels[fields.InputDataFields.groundtruth_classes].shape.as_list())\n    self.assertEqual(tf.float32,\n                     labels[fields.InputDataFields.groundtruth_classes].dtype)\n    self.assertAllEqual(\n        [1, 50],\n        labels[fields.InputDataFields.groundtruth_weights].shape.as_list())\n    self.assertEqual(tf.float32,\n                     labels[fields.InputDataFields.groundtruth_weights].dtype)\n\n  def test_faster_rcnn_resnet50_eval_input(self):\n    """"""Tests the eval input function for FasterRcnnResnet50.""""""\n    configs = _get_configs_for_model(\'faster_rcnn_resnet50_pets\')\n    model_config = configs[\'model\']\n    model_config.faster_rcnn.num_classes = 37\n    eval_input_fn = inputs.create_eval_input_fn(\n        configs[\'eval_config\'], configs[\'eval_input_config\'], model_config)\n    features, labels = eval_input_fn()\n\n    self.assertAllEqual([1, None, None, 3],\n                        features[fields.InputDataFields.image].shape.as_list())\n    self.assertEqual(tf.float32, features[fields.InputDataFields.image].dtype)\n    self.assertAllEqual(\n        [1, None, None, 3],\n        features[fields.InputDataFields.original_image].shape.as_list())\n    self.assertEqual(tf.uint8,\n                     features[fields.InputDataFields.original_image].dtype)\n    self.assertAllEqual([1], features[inputs.HASH_KEY].shape.as_list())\n    self.assertEqual(tf.int32, features[inputs.HASH_KEY].dtype)\n    self.assertAllEqual(\n        [1, None, 4],\n        labels[fields.InputDataFields.groundtruth_boxes].shape.as_list())\n    self.assertEqual(tf.float32,\n                     labels[fields.InputDataFields.groundtruth_boxes].dtype)\n    self.assertAllEqual(\n        [1, None, model_config.faster_rcnn.num_classes],\n        labels[fields.InputDataFields.groundtruth_classes].shape.as_list())\n    self.assertEqual(tf.float32,\n                     labels[fields.InputDataFields.groundtruth_classes].dtype)\n    self.assertAllEqual(\n        [1, None],\n        labels[fields.InputDataFields.groundtruth_area].shape.as_list())\n    self.assertEqual(tf.float32,\n                     labels[fields.InputDataFields.groundtruth_area].dtype)\n    self.assertAllEqual(\n        [1, None],\n        labels[fields.InputDataFields.groundtruth_is_crowd].shape.as_list())\n    self.assertEqual(\n        tf.bool, labels[fields.InputDataFields.groundtruth_is_crowd].dtype)\n    self.assertAllEqual(\n        [1, None],\n        labels[fields.InputDataFields.groundtruth_difficult].shape.as_list())\n    self.assertEqual(\n        tf.int32, labels[fields.InputDataFields.groundtruth_difficult].dtype)\n\n  def test_ssd_inceptionV2_train_input(self):\n    """"""Tests the training input function for SSDInceptionV2.""""""\n    configs = _get_configs_for_model(\'ssd_inception_v2_pets\')\n    model_config = configs[\'model\']\n    model_config.ssd.num_classes = 37\n    batch_size = configs[\'train_config\'].batch_size\n    train_input_fn = inputs.create_train_input_fn(\n        configs[\'train_config\'], configs[\'train_input_config\'], model_config)\n    features, labels = train_input_fn()\n\n    self.assertAllEqual([batch_size, 300, 300, 3],\n                        features[fields.InputDataFields.image].shape.as_list())\n    self.assertEqual(tf.float32, features[fields.InputDataFields.image].dtype)\n    self.assertAllEqual([batch_size],\n                        features[inputs.HASH_KEY].shape.as_list())\n    self.assertEqual(tf.int32, features[inputs.HASH_KEY].dtype)\n    self.assertAllEqual(\n        [batch_size],\n        labels[fields.InputDataFields.num_groundtruth_boxes].shape.as_list())\n    self.assertEqual(tf.int32,\n                     labels[fields.InputDataFields.num_groundtruth_boxes].dtype)\n    self.assertAllEqual(\n        [batch_size, 50, 4],\n        labels[fields.InputDataFields.groundtruth_boxes].shape.as_list())\n    self.assertEqual(tf.float32,\n                     labels[fields.InputDataFields.groundtruth_boxes].dtype)\n    self.assertAllEqual(\n        [batch_size, 50, model_config.ssd.num_classes],\n        labels[fields.InputDataFields.groundtruth_classes].shape.as_list())\n    self.assertEqual(tf.float32,\n                     labels[fields.InputDataFields.groundtruth_classes].dtype)\n    self.assertAllEqual(\n        [batch_size, 50],\n        labels[fields.InputDataFields.groundtruth_weights].shape.as_list())\n    self.assertEqual(tf.float32,\n                     labels[fields.InputDataFields.groundtruth_weights].dtype)\n\n  def test_ssd_inceptionV2_eval_input(self):\n    """"""Tests the eval input function for SSDInceptionV2.""""""\n    configs = _get_configs_for_model(\'ssd_inception_v2_pets\')\n    model_config = configs[\'model\']\n    model_config.ssd.num_classes = 37\n    eval_input_fn = inputs.create_eval_input_fn(\n        configs[\'eval_config\'], configs[\'eval_input_config\'], model_config)\n    features, labels = eval_input_fn()\n\n    self.assertAllEqual([1, 300, 300, 3],\n                        features[fields.InputDataFields.image].shape.as_list())\n    self.assertEqual(tf.float32, features[fields.InputDataFields.image].dtype)\n    self.assertAllEqual(\n        [1, None, None, 3],\n        features[fields.InputDataFields.original_image].shape.as_list())\n    self.assertEqual(tf.uint8,\n                     features[fields.InputDataFields.original_image].dtype)\n    self.assertAllEqual([1], features[inputs.HASH_KEY].shape.as_list())\n    self.assertEqual(tf.int32, features[inputs.HASH_KEY].dtype)\n    self.assertAllEqual(\n        [1, None, 4],\n        labels[fields.InputDataFields.groundtruth_boxes].shape.as_list())\n    self.assertEqual(tf.float32,\n                     labels[fields.InputDataFields.groundtruth_boxes].dtype)\n    self.assertAllEqual(\n        [1, None, model_config.ssd.num_classes],\n        labels[fields.InputDataFields.groundtruth_classes].shape.as_list())\n    self.assertEqual(tf.float32,\n                     labels[fields.InputDataFields.groundtruth_classes].dtype)\n    self.assertAllEqual(\n        [1, None],\n        labels[fields.InputDataFields.groundtruth_area].shape.as_list())\n    self.assertEqual(tf.float32,\n                     labels[fields.InputDataFields.groundtruth_area].dtype)\n    self.assertAllEqual(\n        [1, None],\n        labels[fields.InputDataFields.groundtruth_is_crowd].shape.as_list())\n    self.assertEqual(\n        tf.bool, labels[fields.InputDataFields.groundtruth_is_crowd].dtype)\n    self.assertAllEqual(\n        [1, None],\n        labels[fields.InputDataFields.groundtruth_difficult].shape.as_list())\n    self.assertEqual(\n        tf.int32, labels[fields.InputDataFields.groundtruth_difficult].dtype)\n\n  def test_predict_input(self):\n    """"""Tests the predict input function.""""""\n    configs = _get_configs_for_model(\'ssd_inception_v2_pets\')\n    predict_input_fn = inputs.create_predict_input_fn(\n        model_config=configs[\'model\'])\n    serving_input_receiver = predict_input_fn()\n\n    image = serving_input_receiver.features[fields.InputDataFields.image]\n    receiver_tensors = serving_input_receiver.receiver_tensors[\n        inputs.SERVING_FED_EXAMPLE_KEY]\n    self.assertEqual([1, 300, 300, 3], image.shape.as_list())\n    self.assertEqual(tf.float32, image.dtype)\n    self.assertEqual(tf.string, receiver_tensors.dtype)\n\n  def test_error_with_bad_train_config(self):\n    """"""Tests that a TypeError is raised with improper train config.""""""\n    configs = _get_configs_for_model(\'ssd_inception_v2_pets\')\n    configs[\'model\'].ssd.num_classes = 37\n    train_input_fn = inputs.create_train_input_fn(\n        train_config=configs[\'eval_config\'],  # Expecting `TrainConfig`.\n        train_input_config=configs[\'train_input_config\'],\n        model_config=configs[\'model\'])\n    with self.assertRaises(TypeError):\n      train_input_fn()\n\n  def test_error_with_bad_train_input_config(self):\n    """"""Tests that a TypeError is raised with improper train input config.""""""\n    configs = _get_configs_for_model(\'ssd_inception_v2_pets\')\n    configs[\'model\'].ssd.num_classes = 37\n    train_input_fn = inputs.create_train_input_fn(\n        train_config=configs[\'train_config\'],\n        train_input_config=configs[\'model\'],  # Expecting `InputReader`.\n        model_config=configs[\'model\'])\n    with self.assertRaises(TypeError):\n      train_input_fn()\n\n  def test_error_with_bad_train_model_config(self):\n    """"""Tests that a TypeError is raised with improper train model config.""""""\n    configs = _get_configs_for_model(\'ssd_inception_v2_pets\')\n    configs[\'model\'].ssd.num_classes = 37\n    train_input_fn = inputs.create_train_input_fn(\n        train_config=configs[\'train_config\'],\n        train_input_config=configs[\'train_input_config\'],\n        model_config=configs[\'train_config\'])  # Expecting `DetectionModel`.\n    with self.assertRaises(TypeError):\n      train_input_fn()\n\n  def test_error_with_bad_eval_config(self):\n    """"""Tests that a TypeError is raised with improper eval config.""""""\n    configs = _get_configs_for_model(\'ssd_inception_v2_pets\')\n    configs[\'model\'].ssd.num_classes = 37\n    eval_input_fn = inputs.create_eval_input_fn(\n        eval_config=configs[\'train_config\'],  # Expecting `EvalConfig`.\n        eval_input_config=configs[\'eval_input_config\'],\n        model_config=configs[\'model\'])\n    with self.assertRaises(TypeError):\n      eval_input_fn()\n\n  def test_error_with_bad_eval_input_config(self):\n    """"""Tests that a TypeError is raised with improper eval input config.""""""\n    configs = _get_configs_for_model(\'ssd_inception_v2_pets\')\n    configs[\'model\'].ssd.num_classes = 37\n    eval_input_fn = inputs.create_eval_input_fn(\n        eval_config=configs[\'eval_config\'],\n        eval_input_config=configs[\'model\'],  # Expecting `InputReader`.\n        model_config=configs[\'model\'])\n    with self.assertRaises(TypeError):\n      eval_input_fn()\n\n  def test_error_with_bad_eval_model_config(self):\n    """"""Tests that a TypeError is raised with improper eval model config.""""""\n    configs = _get_configs_for_model(\'ssd_inception_v2_pets\')\n    configs[\'model\'].ssd.num_classes = 37\n    eval_input_fn = inputs.create_eval_input_fn(\n        eval_config=configs[\'eval_config\'],\n        eval_input_config=configs[\'eval_input_config\'],\n        model_config=configs[\'eval_config\'])  # Expecting `DetectionModel`.\n    with self.assertRaises(TypeError):\n      eval_input_fn()\n\n\nclass DataAugmentationFnTest(tf.test.TestCase):\n\n  def test_apply_image_and_box_augmentation(self):\n    data_augmentation_options = [\n        (preprocessor.resize_image, {\n            \'new_height\': 20,\n            \'new_width\': 20,\n            \'method\': tf.image.ResizeMethod.NEAREST_NEIGHBOR\n        }),\n        (preprocessor.scale_boxes_to_pixel_coordinates, {}),\n    ]\n    data_augmentation_fn = functools.partial(\n        inputs.augment_input_data,\n        data_augmentation_options=data_augmentation_options)\n    tensor_dict = {\n        fields.InputDataFields.image:\n            tf.constant(np.random.rand(10, 10, 3).astype(np.float32)),\n        fields.InputDataFields.groundtruth_boxes:\n            tf.constant(np.array([[.5, .5, 1., 1.]], np.float32))\n    }\n    augmented_tensor_dict = data_augmentation_fn(tensor_dict=tensor_dict)\n    with self.test_session() as sess:\n      augmented_tensor_dict_out = sess.run(augmented_tensor_dict)\n\n    self.assertAllEqual(\n        augmented_tensor_dict_out[fields.InputDataFields.image].shape,\n        [20, 20, 3]\n    )\n    self.assertAllClose(\n        augmented_tensor_dict_out[fields.InputDataFields.groundtruth_boxes],\n        [[10, 10, 20, 20]]\n    )\n\n  def test_include_masks_in_data_augmentation(self):\n    data_augmentation_options = [\n        (preprocessor.resize_image, {\n            \'new_height\': 20,\n            \'new_width\': 20,\n            \'method\': tf.image.ResizeMethod.NEAREST_NEIGHBOR\n        })\n    ]\n    data_augmentation_fn = functools.partial(\n        inputs.augment_input_data,\n        data_augmentation_options=data_augmentation_options)\n    tensor_dict = {\n        fields.InputDataFields.image:\n            tf.constant(np.random.rand(10, 10, 3).astype(np.float32)),\n        fields.InputDataFields.groundtruth_instance_masks:\n            tf.constant(np.zeros([2, 10, 10], np.uint8))\n    }\n    augmented_tensor_dict = data_augmentation_fn(tensor_dict=tensor_dict)\n    with self.test_session() as sess:\n      augmented_tensor_dict_out = sess.run(augmented_tensor_dict)\n\n    self.assertAllEqual(\n        augmented_tensor_dict_out[fields.InputDataFields.image].shape,\n        [20, 20, 3])\n    self.assertAllEqual(augmented_tensor_dict_out[\n        fields.InputDataFields.groundtruth_instance_masks].shape, [2, 20, 20])\n\n  def test_include_keypoints_in_data_augmentation(self):\n    data_augmentation_options = [\n        (preprocessor.resize_image, {\n            \'new_height\': 20,\n            \'new_width\': 20,\n            \'method\': tf.image.ResizeMethod.NEAREST_NEIGHBOR\n        }),\n        (preprocessor.scale_boxes_to_pixel_coordinates, {}),\n    ]\n    data_augmentation_fn = functools.partial(\n        inputs.augment_input_data,\n        data_augmentation_options=data_augmentation_options)\n    tensor_dict = {\n        fields.InputDataFields.image:\n            tf.constant(np.random.rand(10, 10, 3).astype(np.float32)),\n        fields.InputDataFields.groundtruth_boxes:\n            tf.constant(np.array([[.5, .5, 1., 1.]], np.float32)),\n        fields.InputDataFields.groundtruth_keypoints:\n            tf.constant(np.array([[[0.5, 1.0], [0.5, 0.5]]], np.float32))\n    }\n    augmented_tensor_dict = data_augmentation_fn(tensor_dict=tensor_dict)\n    with self.test_session() as sess:\n      augmented_tensor_dict_out = sess.run(augmented_tensor_dict)\n\n    self.assertAllEqual(\n        augmented_tensor_dict_out[fields.InputDataFields.image].shape,\n        [20, 20, 3]\n    )\n    self.assertAllClose(\n        augmented_tensor_dict_out[fields.InputDataFields.groundtruth_boxes],\n        [[10, 10, 20, 20]]\n    )\n    self.assertAllClose(\n        augmented_tensor_dict_out[fields.InputDataFields.groundtruth_keypoints],\n        [[[10, 20], [10, 10]]]\n    )\n\n\ndef _fake_model_preprocessor_fn(image):\n  return (image, tf.expand_dims(tf.shape(image)[1:], axis=0))\n\n\ndef _fake_image_resizer_fn(image, mask):\n  return (image, mask, tf.shape(image))\n\n\nclass DataTransformationFnTest(tf.test.TestCase):\n\n  def test_returns_correct_class_label_encodings(self):\n    tensor_dict = {\n        fields.InputDataFields.image:\n            tf.constant(np.random.rand(4, 4, 3).astype(np.float32)),\n        fields.InputDataFields.groundtruth_boxes:\n            tf.constant(np.array([[0, 0, 1, 1], [.5, .5, 1, 1]], np.float32)),\n        fields.InputDataFields.groundtruth_classes:\n            tf.constant(np.array([3, 1], np.int32))\n    }\n    num_classes = 3\n    input_transformation_fn = functools.partial(\n        inputs.transform_input_data,\n        model_preprocess_fn=_fake_model_preprocessor_fn,\n        image_resizer_fn=_fake_image_resizer_fn,\n        num_classes=num_classes)\n    with self.test_session() as sess:\n      transformed_inputs = sess.run(\n          input_transformation_fn(tensor_dict=tensor_dict))\n\n    self.assertAllClose(\n        transformed_inputs[fields.InputDataFields.groundtruth_classes],\n        [[0, 0, 1], [1, 0, 0]])\n\n  def test_returns_correct_merged_boxes(self):\n    tensor_dict = {\n        fields.InputDataFields.image:\n            tf.constant(np.random.rand(4, 4, 3).astype(np.float32)),\n        fields.InputDataFields.groundtruth_boxes:\n            tf.constant(np.array([[.5, .5, 1, 1], [.5, .5, 1, 1]], np.float32)),\n        fields.InputDataFields.groundtruth_classes:\n            tf.constant(np.array([3, 1], np.int32))\n    }\n\n    num_classes = 3\n    input_transformation_fn = functools.partial(\n        inputs.transform_input_data,\n        model_preprocess_fn=_fake_model_preprocessor_fn,\n        image_resizer_fn=_fake_image_resizer_fn,\n        num_classes=num_classes,\n        merge_multiple_boxes=True)\n\n    with self.test_session() as sess:\n      transformed_inputs = sess.run(\n          input_transformation_fn(tensor_dict=tensor_dict))\n    self.assertAllClose(\n        transformed_inputs[fields.InputDataFields.groundtruth_boxes],\n        [[.5, .5, 1., 1.]])\n    self.assertAllClose(\n        transformed_inputs[fields.InputDataFields.groundtruth_classes],\n        [[1, 0, 1]])\n\n  def test_returns_resized_masks(self):\n    tensor_dict = {\n        fields.InputDataFields.image:\n            tf.constant(np.random.rand(4, 4, 3).astype(np.float32)),\n        fields.InputDataFields.groundtruth_instance_masks:\n            tf.constant(np.random.rand(2, 4, 4).astype(np.float32)),\n        fields.InputDataFields.groundtruth_classes:\n            tf.constant(np.array([3, 1], np.int32))\n    }\n    def fake_image_resizer_fn(image, masks):\n      resized_image = tf.image.resize_images(image, [8, 8])\n      resized_masks = tf.transpose(\n          tf.image.resize_images(tf.transpose(masks, [1, 2, 0]), [8, 8]),\n          [2, 0, 1])\n      return resized_image, resized_masks, tf.shape(resized_image)\n\n    num_classes = 3\n    input_transformation_fn = functools.partial(\n        inputs.transform_input_data,\n        model_preprocess_fn=_fake_model_preprocessor_fn,\n        image_resizer_fn=fake_image_resizer_fn,\n        num_classes=num_classes)\n    with self.test_session() as sess:\n      transformed_inputs = sess.run(\n          input_transformation_fn(tensor_dict=tensor_dict))\n    self.assertAllEqual(transformed_inputs[\n        fields.InputDataFields.groundtruth_instance_masks].shape, [2, 8, 8])\n\n  def test_applies_model_preprocess_fn_to_image_tensor(self):\n    np_image = np.random.randint(256, size=(4, 4, 3))\n    tensor_dict = {\n        fields.InputDataFields.image:\n            tf.constant(np_image),\n        fields.InputDataFields.groundtruth_classes:\n            tf.constant(np.array([3, 1], np.int32))\n    }\n    def fake_model_preprocessor_fn(image):\n      return (image / 255., tf.expand_dims(tf.shape(image)[1:], axis=0))\n\n    num_classes = 3\n    input_transformation_fn = functools.partial(\n        inputs.transform_input_data,\n        model_preprocess_fn=fake_model_preprocessor_fn,\n        image_resizer_fn=_fake_image_resizer_fn,\n        num_classes=num_classes)\n\n    with self.test_session() as sess:\n      transformed_inputs = sess.run(\n          input_transformation_fn(tensor_dict=tensor_dict))\n    self.assertAllClose(transformed_inputs[fields.InputDataFields.image],\n                        np_image / 255.)\n    self.assertAllClose(transformed_inputs[fields.InputDataFields.\n                                           true_image_shape],\n                        [4, 4, 3])\n\n  def test_applies_data_augmentation_fn_to_tensor_dict(self):\n    np_image = np.random.randint(256, size=(4, 4, 3))\n    tensor_dict = {\n        fields.InputDataFields.image:\n            tf.constant(np_image),\n        fields.InputDataFields.groundtruth_classes:\n            tf.constant(np.array([3, 1], np.int32))\n    }\n    def add_one_data_augmentation_fn(tensor_dict):\n      return {key: value + 1 for key, value in tensor_dict.items()}\n\n    num_classes = 4\n    input_transformation_fn = functools.partial(\n        inputs.transform_input_data,\n        model_preprocess_fn=_fake_model_preprocessor_fn,\n        image_resizer_fn=_fake_image_resizer_fn,\n        num_classes=num_classes,\n        data_augmentation_fn=add_one_data_augmentation_fn)\n    with self.test_session() as sess:\n      augmented_tensor_dict = sess.run(\n          input_transformation_fn(tensor_dict=tensor_dict))\n\n    self.assertAllEqual(augmented_tensor_dict[fields.InputDataFields.image],\n                        np_image + 1)\n    self.assertAllEqual(\n        augmented_tensor_dict[fields.InputDataFields.groundtruth_classes],\n        [[0, 0, 0, 1], [0, 1, 0, 0]])\n\n  def test_applies_data_augmentation_fn_before_model_preprocess_fn(self):\n    np_image = np.random.randint(256, size=(4, 4, 3))\n    tensor_dict = {\n        fields.InputDataFields.image:\n            tf.constant(np_image),\n        fields.InputDataFields.groundtruth_classes:\n            tf.constant(np.array([3, 1], np.int32))\n    }\n    def mul_two_model_preprocessor_fn(image):\n      return (image * 2, tf.expand_dims(tf.shape(image)[1:], axis=0))\n    def add_five_to_image_data_augmentation_fn(tensor_dict):\n      tensor_dict[fields.InputDataFields.image] += 5\n      return tensor_dict\n\n    num_classes = 4\n    input_transformation_fn = functools.partial(\n        inputs.transform_input_data,\n        model_preprocess_fn=mul_two_model_preprocessor_fn,\n        image_resizer_fn=_fake_image_resizer_fn,\n        num_classes=num_classes,\n        data_augmentation_fn=add_five_to_image_data_augmentation_fn)\n    with self.test_session() as sess:\n      augmented_tensor_dict = sess.run(\n          input_transformation_fn(tensor_dict=tensor_dict))\n\n    self.assertAllEqual(augmented_tensor_dict[fields.InputDataFields.image],\n                        (np_image + 5) * 2)\n\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
src/object_detection/model.py,49,"b'# Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\nr""""""Creates and runs `Experiment` for object detection model.\n\nThis uses the TF.learn framework to define and run an object detection model\nwrapped in an `Estimator`.\nNote that this module is only compatible with SSD Meta architecture at the\nmoment.\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport functools\nimport os\n\nimport tensorflow as tf\n\nfrom google.protobuf import text_format\nfrom tensorflow.contrib.learn.python.learn import learn_runner\nfrom tensorflow.contrib.tpu.python.tpu import tpu_optimizer\nfrom tensorflow.python.lib.io import file_io\nfrom object_detection import eval_util\nfrom object_detection import inputs\nfrom object_detection import model_hparams\nfrom object_detection.builders import model_builder\nfrom object_detection.builders import optimizer_builder\nfrom object_detection.core import standard_fields as fields\nfrom object_detection.utils import config_util\nfrom object_detection.utils import label_map_util\nfrom object_detection.utils import shape_utils\nfrom object_detection.utils import variables_helper\nfrom object_detection.utils import visualization_utils as vis_utils\n\ntf.flags.DEFINE_string(\'model_dir\', None, \'Path to output model directory \'\n                       \'where event and checkpoint files will be written.\')\ntf.flags.DEFINE_string(\'pipeline_config_path\', None, \'Path to pipeline config \'\n                       \'file.\')\ntf.flags.DEFINE_integer(\'num_train_steps\', 500000, \'Number of train steps.\')\ntf.flags.DEFINE_integer(\'num_eval_steps\', 10000, \'Number of train steps.\')\nFLAGS = tf.flags.FLAGS\n\n\n# A map of names to methods that help build the model.\nMODEL_BUILD_UTIL_MAP = {\n    \'get_configs_from_pipeline_file\':\n        config_util.get_configs_from_pipeline_file,\n    \'create_pipeline_proto_from_configs\':\n        config_util.create_pipeline_proto_from_configs,\n    \'merge_external_params_with_configs\':\n        config_util.merge_external_params_with_configs,\n    \'create_train_input_fn\': inputs.create_train_input_fn,\n    \'create_eval_input_fn\': inputs.create_eval_input_fn,\n    \'create_predict_input_fn\': inputs.create_predict_input_fn,\n}\n\n\ndef _get_groundtruth_data(detection_model, class_agnostic):\n  """"""Extracts groundtruth data from detection_model.\n\n  Args:\n    detection_model: A `DetectionModel` object.\n    class_agnostic: Whether the detections are class_agnostic.\n\n  Returns:\n    A tuple of:\n    groundtruth: Dictionary with the following fields:\n      \'groundtruth_boxes\': [num_boxes, 4] float32 tensor of boxes, in\n        normalized coordinates.\n      \'groundtruth_classes\': [num_boxes] int64 tensor of 1-indexed classes.\n      \'groundtruth_masks\': 3D float32 tensor of instance masks (if provided in\n        groundtruth)\n    class_agnostic: Boolean indicating whether detections are class agnostic.\n  """"""\n  input_data_fields = fields.InputDataFields()\n  groundtruth_boxes = detection_model.groundtruth_lists(\n      fields.BoxListFields.boxes)[0]\n  # For class-agnostic models, groundtruth one-hot encodings collapse to all\n  # ones.\n  if class_agnostic:\n    groundtruth_boxes_shape = tf.shape(groundtruth_boxes)\n    groundtruth_classes_one_hot = tf.ones([groundtruth_boxes_shape[0], 1])\n  else:\n    groundtruth_classes_one_hot = detection_model.groundtruth_lists(\n        fields.BoxListFields.classes)[0]\n  label_id_offset = 1  # Applying label id offset (b/63711816)\n  groundtruth_classes = (\n      tf.argmax(groundtruth_classes_one_hot, axis=1) + label_id_offset)\n  groundtruth = {\n      input_data_fields.groundtruth_boxes: groundtruth_boxes,\n      input_data_fields.groundtruth_classes: groundtruth_classes\n  }\n  if detection_model.groundtruth_has_field(fields.BoxListFields.masks):\n    groundtruth[input_data_fields.groundtruth_instance_masks] = (\n        detection_model.groundtruth_lists(fields.BoxListFields.masks)[0])\n  return groundtruth\n\n\ndef unstack_batch(tensor_dict, unpad_groundtruth_tensors=True):\n  """"""Unstacks all tensors in `tensor_dict` along 0th dimension.\n\n  Unstacks tensor from the tensor dict along 0th dimension and returns a\n  tensor_dict containing values that are lists of unstacked tensors.\n\n  Tensors in the `tensor_dict` are expected to be of one of the three shapes:\n  1. [batch_size]\n  2. [batch_size, height, width, channels]\n  3. [batch_size, num_boxes, d1, d2, ... dn]\n\n  When unpad_groundtruth_tensors is set to true, unstacked tensors of form 3\n  above are sliced along the `num_boxes` dimension using the value in tensor\n  field.InputDataFields.num_groundtruth_boxes.\n\n  Note that this function has a static list of input data fields and has to be\n  kept in sync with the InputDataFields defined in core/standard_fields.py\n\n  Args:\n    tensor_dict: A dictionary of batched groundtruth tensors.\n    unpad_groundtruth_tensors: Whether to remove padding along `num_boxes`\n      dimension of the groundtruth tensors.\n\n  Returns:\n    A dictionary where the keys are from fields.InputDataFields and values are\n    a list of unstacked (optionally unpadded) tensors.\n\n  Raises:\n    ValueError: If unpad_tensors is True and `tensor_dict` does not contain\n      `num_groundtruth_boxes` tensor.\n  """"""\n  unbatched_tensor_dict = {key: tf.unstack(tensor)\n                           for key, tensor in tensor_dict.items()}\n  if unpad_groundtruth_tensors:\n    if (fields.InputDataFields.num_groundtruth_boxes not in\n        unbatched_tensor_dict):\n      raise ValueError(\'`num_groundtruth_boxes` not found in tensor_dict. \'\n                       \'Keys available: {}\'.format(\n                           unbatched_tensor_dict.keys()))\n    unbatched_unpadded_tensor_dict = {}\n    unpad_keys = set([\n        # List of input data fields that are padded along the num_boxes\n        # dimension. This list has to be kept in sync with InputDataFields in\n        # standard_fields.py.\n        fields.InputDataFields.groundtruth_instance_masks,\n        fields.InputDataFields.groundtruth_classes,\n        fields.InputDataFields.groundtruth_boxes,\n        fields.InputDataFields.groundtruth_keypoints,\n        fields.InputDataFields.groundtruth_group_of,\n        fields.InputDataFields.groundtruth_difficult,\n        fields.InputDataFields.groundtruth_is_crowd,\n        fields.InputDataFields.groundtruth_area,\n        fields.InputDataFields.groundtruth_weights\n    ]).intersection(set(unbatched_tensor_dict.keys()))\n\n    for key in unpad_keys:\n      unpadded_tensor_list = []\n      for num_gt, padded_tensor in zip(\n          unbatched_tensor_dict[fields.InputDataFields.num_groundtruth_boxes],\n          unbatched_tensor_dict[key]):\n        tensor_shape = shape_utils.combined_static_and_dynamic_shape(\n            padded_tensor)\n        slice_begin = tf.zeros([len(tensor_shape)], dtype=tf.int32)\n        slice_size = tf.stack(\n            [num_gt] + [-1 if dim is None else dim for dim in tensor_shape[1:]])\n        unpadded_tensor = tf.slice(padded_tensor, slice_begin, slice_size)\n        unpadded_tensor_list.append(unpadded_tensor)\n      unbatched_unpadded_tensor_dict[key] = unpadded_tensor_list\n    unbatched_tensor_dict.update(unbatched_unpadded_tensor_dict)\n\n  return unbatched_tensor_dict\n\n\ndef create_model_fn(detection_model_fn, configs, hparams, use_tpu=False):\n  """"""Creates a model function for `Estimator`.\n\n  Args:\n    detection_model_fn: Function that returns a `DetectionModel` instance.\n    configs: Dictionary of pipeline config objects.\n    hparams: `HParams` object.\n    use_tpu: Boolean indicating whether model should be constructed for\n        use on TPU.\n\n  Returns:\n    `model_fn` for `Estimator`.\n  """"""\n  train_config = configs[\'train_config\']\n  eval_input_config = configs[\'eval_input_config\']\n  eval_config = configs[\'eval_config\']\n\n  def model_fn(features, labels, mode, params=None):\n    """"""Constructs the object detection model.\n\n    Args:\n      features: Dictionary of feature tensors, returned from `input_fn`.\n      labels: Dictionary of groundtruth tensors if mode is TRAIN or EVAL,\n        otherwise None.\n      mode: Mode key from tf.estimator.ModeKeys.\n      params: Parameter dictionary passed from the estimator.\n\n    Returns:\n      An `EstimatorSpec` that encapsulates the model and its serving\n        configurations.\n    """"""\n    params = params or {}\n    total_loss, train_op, detections, export_outputs = None, None, None, None\n    is_training = mode == tf.estimator.ModeKeys.TRAIN\n    detection_model = detection_model_fn(is_training=is_training,\n                                         add_summaries=(not use_tpu))\n    scaffold_fn = None\n\n    if mode == tf.estimator.ModeKeys.TRAIN:\n      labels = unstack_batch(\n          labels,\n          unpad_groundtruth_tensors=train_config.unpad_groundtruth_tensors)\n    elif mode == tf.estimator.ModeKeys.EVAL:\n      labels = unstack_batch(labels, unpad_groundtruth_tensors=False)\n\n    if mode in (tf.estimator.ModeKeys.TRAIN, tf.estimator.ModeKeys.EVAL):\n      gt_boxes_list = labels[fields.InputDataFields.groundtruth_boxes]\n      gt_classes_list = labels[fields.InputDataFields.groundtruth_classes]\n      gt_masks_list = None\n      if fields.InputDataFields.groundtruth_instance_masks in labels:\n        gt_masks_list = labels[\n            fields.InputDataFields.groundtruth_instance_masks]\n      gt_keypoints_list = None\n      if fields.InputDataFields.groundtruth_keypoints in labels:\n        gt_keypoints_list = labels[fields.InputDataFields.groundtruth_keypoints]\n      detection_model.provide_groundtruth(\n          groundtruth_boxes_list=gt_boxes_list,\n          groundtruth_classes_list=gt_classes_list,\n          groundtruth_masks_list=gt_masks_list,\n          groundtruth_keypoints_list=gt_keypoints_list)\n\n    preprocessed_images = features[fields.InputDataFields.image]\n    prediction_dict = detection_model.predict(\n        preprocessed_images, features[fields.InputDataFields.true_image_shape])\n    detections = detection_model.postprocess(\n        prediction_dict, features[fields.InputDataFields.true_image_shape])\n\n    if mode == tf.estimator.ModeKeys.TRAIN:\n      if not train_config.fine_tune_checkpoint_type:\n        # train_config.from_detection_checkpoint field is deprecated. For\n        # backward compatibility, sets finetune_checkpoint_type based on\n        # from_detection_checkpoint.\n        if train_config.from_detection_checkpoint:\n          train_config.fine_tune_checkpoint_type = \'detection\'\n        else:\n          train_config.fine_tune_checkpoint_type = \'classification\'\n      if train_config.fine_tune_checkpoint and hparams.load_pretrained:\n        if not train_config.fine_tune_checkpoint_type:\n          # train_config.from_detection_checkpoint field is deprecated. For\n          # backward compatibility, set train_config.fine_tune_checkpoint_type\n          # based on train_config.from_detection_checkpoint.\n          if train_config.from_detection_checkpoint:\n            train_config.fine_tune_checkpoint_type = \'detection\'\n          else:\n            train_config.fine_tune_checkpoint_type = \'classification\'\n        asg_map = detection_model.restore_map(\n            fine_tune_checkpoint_type=train_config.fine_tune_checkpoint_type,\n            load_all_detection_checkpoint_vars=(\n                train_config.load_all_detection_checkpoint_vars))\n        available_var_map = (\n            variables_helper.get_variables_available_in_checkpoint(\n                asg_map, train_config.fine_tune_checkpoint,\n                include_global_step=False))\n        if use_tpu:\n          def tpu_scaffold():\n            tf.train.init_from_checkpoint(train_config.fine_tune_checkpoint,\n                                          available_var_map)\n            return tf.train.Scaffold()\n          scaffold_fn = tpu_scaffold\n        else:\n          tf.train.init_from_checkpoint(train_config.fine_tune_checkpoint,\n                                        available_var_map)\n\n    if mode in (tf.estimator.ModeKeys.TRAIN, tf.estimator.ModeKeys.EVAL):\n      losses_dict = detection_model.loss(\n          prediction_dict, features[fields.InputDataFields.true_image_shape])\n      losses = [loss_tensor for loss_tensor in losses_dict.itervalues()]\n      if train_config.add_regularization_loss:\n        regularization_losses = tf.get_collection(\n            tf.GraphKeys.REGULARIZATION_LOSSES)\n        if regularization_losses:\n          regularization_loss = tf.add_n(regularization_losses,\n                                         name=\'regularization_loss\')\n          losses.append(regularization_loss)\n          if not use_tpu:\n            tf.summary.scalar(\'regularization_loss\', regularization_loss)\n      total_loss = tf.add_n(losses, name=\'total_loss\')\n\n    if mode == tf.estimator.ModeKeys.TRAIN:\n      global_step = tf.train.get_or_create_global_step()\n      training_optimizer, optimizer_summary_vars = optimizer_builder.build(\n          train_config.optimizer)\n\n      if use_tpu:\n        training_optimizer = tpu_optimizer.CrossShardOptimizer(\n            training_optimizer)\n\n      # Optionally freeze some layers by setting their gradients to be zero.\n      trainable_variables = None\n      if train_config.freeze_variables:\n        trainable_variables = tf.contrib.framework.filter_variables(\n            tf.trainable_variables(),\n            exclude_patterns=train_config.freeze_variables)\n\n      clip_gradients_value = None\n      if train_config.gradient_clipping_by_norm > 0:\n        clip_gradients_value = train_config.gradient_clipping_by_norm\n\n      if not use_tpu:\n        for var in optimizer_summary_vars:\n          tf.summary.scalar(var.op.name, var)\n      summaries = [] if use_tpu else None\n      train_op = tf.contrib.layers.optimize_loss(\n          loss=total_loss,\n          global_step=global_step,\n          learning_rate=None,\n          clip_gradients=clip_gradients_value,\n          optimizer=training_optimizer,\n          variables=trainable_variables,\n          summaries=summaries,\n          name=\'\')  # Preventing scope prefix on all variables.\n\n    if mode == tf.estimator.ModeKeys.PREDICT:\n      export_outputs = {\n          tf.saved_model.signature_constants.PREDICT_METHOD_NAME:\n              tf.estimator.export.PredictOutput(detections)\n      }\n\n    eval_metric_ops = None\n    if mode == tf.estimator.ModeKeys.EVAL:\n      # Detection summaries during eval.\n      class_agnostic = (fields.DetectionResultFields.detection_classes\n                        not in detections)\n      groundtruth = _get_groundtruth_data(detection_model, class_agnostic)\n      use_original_images = fields.InputDataFields.original_image in features\n      eval_images = (\n          features[fields.InputDataFields.original_image] if use_original_images\n          else features[fields.InputDataFields.image])\n      eval_dict = eval_util.result_dict_for_single_example(\n          eval_images[0:1],\n          features[inputs.HASH_KEY][0],\n          detections,\n          groundtruth,\n          class_agnostic=class_agnostic,\n          scale_to_absolute=False)\n\n      if class_agnostic:\n        category_index = label_map_util.create_class_agnostic_category_index()\n      else:\n        category_index = label_map_util.create_category_index_from_labelmap(\n            eval_input_config.label_map_path)\n      if not use_tpu and use_original_images:\n        detection_and_groundtruth = (\n            vis_utils.draw_side_by_side_evaluation_image(\n                eval_dict, category_index, max_boxes_to_draw=20,\n                min_score_thresh=0.2))\n        tf.summary.image(\'Detections_Left_Groundtruth_Right\',\n                         detection_and_groundtruth)\n\n      # Eval metrics on a single image.\n      eval_metrics = eval_config.metrics_set\n      if not eval_metrics:\n        eval_metrics = [\'coco_detection_metrics\']\n      eval_metric_ops = eval_util.get_eval_metric_ops_for_evaluators(\n          eval_metrics, category_index.values(), eval_dict,\n          include_metrics_per_category=False)\n\n    if use_tpu:\n      return tf.contrib.tpu.TPUEstimatorSpec(\n          mode=mode,\n          scaffold_fn=scaffold_fn,\n          predictions=detections,\n          loss=total_loss,\n          train_op=train_op,\n          eval_metrics=eval_metric_ops,\n          export_outputs=export_outputs)\n    else:\n      return tf.estimator.EstimatorSpec(\n          mode=mode,\n          predictions=detections,\n          loss=total_loss,\n          train_op=train_op,\n          eval_metric_ops=eval_metric_ops,\n          export_outputs=export_outputs)\n\n  return model_fn\n\n\ndef build_experiment_fn(train_steps, eval_steps):\n  """"""Returns a function that creates an `Experiment`.""""""\n\n  def build_experiment(run_config, hparams):\n    """"""Builds an `Experiment` from configuration and hyperparameters.\n\n    Args:\n      run_config: A `RunConfig`.\n      hparams: A `HParams`.\n\n    Returns:\n      An `Experiment` object.\n    """"""\n    return populate_experiment(run_config, hparams, FLAGS.pipeline_config_path,\n                               train_steps, eval_steps)\n\n  return build_experiment\n\n\ndef populate_experiment(run_config,\n                        hparams,\n                        pipeline_config_path,\n                        train_steps=None,\n                        eval_steps=None,\n                        model_fn_creator=create_model_fn,\n                        **kwargs):\n  """"""Populates an `Experiment` object.\n\n  Args:\n    run_config: A `RunConfig`.\n    hparams: A `HParams`.\n    pipeline_config_path: A path to a pipeline config file.\n    train_steps: Number of training steps. If None, the number of training steps\n      is set from the `TrainConfig` proto.\n    eval_steps: Number of evaluation steps per evaluation cycle. If None, the\n      number of evaluation steps is set from the `EvalConfig` proto.\n    model_fn_creator: A function that creates a `model_fn` for `Estimator`.\n      Follows the signature:\n\n      * Args:\n        * `detection_model_fn`: Function that returns `DetectionModel` instance.\n        * `configs`: Dictionary of pipeline config objects.\n        * `hparams`: `HParams` object.\n      * Returns:\n        `model_fn` for `Estimator`.\n\n    **kwargs: Additional keyword arguments for configuration override.\n\n  Returns:\n    An `Experiment` that defines all aspects of training, evaluation, and\n    export.\n  """"""\n  get_configs_from_pipeline_file = MODEL_BUILD_UTIL_MAP[\n      \'get_configs_from_pipeline_file\']\n  create_pipeline_proto_from_configs = MODEL_BUILD_UTIL_MAP[\n      \'create_pipeline_proto_from_configs\']\n  merge_external_params_with_configs = MODEL_BUILD_UTIL_MAP[\n      \'merge_external_params_with_configs\']\n  create_train_input_fn = MODEL_BUILD_UTIL_MAP[\'create_train_input_fn\']\n  create_eval_input_fn = MODEL_BUILD_UTIL_MAP[\'create_eval_input_fn\']\n  create_predict_input_fn = MODEL_BUILD_UTIL_MAP[\'create_predict_input_fn\']\n\n  configs = get_configs_from_pipeline_file(pipeline_config_path)\n  configs = merge_external_params_with_configs(\n      configs,\n      hparams,\n      train_steps=train_steps,\n      eval_steps=eval_steps,\n      **kwargs)\n  model_config = configs[\'model\']\n  train_config = configs[\'train_config\']\n  train_input_config = configs[\'train_input_config\']\n  eval_config = configs[\'eval_config\']\n  eval_input_config = configs[\'eval_input_config\']\n\n  if train_steps is None and train_config.num_steps:\n    train_steps = train_config.num_steps\n\n  if eval_steps is None and eval_config.num_examples:\n    eval_steps = eval_config.num_examples\n\n  detection_model_fn = functools.partial(\n      model_builder.build, model_config=model_config)\n\n  # Create the input functions for TRAIN/EVAL.\n  train_input_fn = create_train_input_fn(\n      train_config=train_config,\n      train_input_config=train_input_config,\n      model_config=model_config)\n  eval_input_fn = create_eval_input_fn(\n      eval_config=eval_config,\n      eval_input_config=eval_input_config,\n      model_config=model_config)\n\n  export_strategies = [\n      tf.contrib.learn.utils.saved_model_export_utils.make_export_strategy(\n          serving_input_fn=create_predict_input_fn(\n              model_config=model_config))\n  ]\n\n  estimator = tf.estimator.Estimator(\n      model_fn=model_fn_creator(detection_model_fn, configs, hparams),\n      config=run_config)\n\n  if run_config.is_chief:\n    # Store the final pipeline config for traceability.\n    pipeline_config_final = create_pipeline_proto_from_configs(\n        configs)\n    if not file_io.file_exists(estimator.model_dir):\n      file_io.recursive_create_dir(estimator.model_dir)\n    pipeline_config_final_path = os.path.join(estimator.model_dir,\n                                              \'pipeline.config\')\n    config_text = text_format.MessageToString(pipeline_config_final)\n    with tf.gfile.Open(pipeline_config_final_path, \'wb\') as f:\n      tf.logging.info(\'Writing as-run pipeline config file to %s\',\n                      pipeline_config_final_path)\n      f.write(config_text)\n\n  return tf.contrib.learn.Experiment(\n      estimator=estimator,\n      train_input_fn=train_input_fn,\n      eval_input_fn=eval_input_fn,\n      train_steps=train_steps,\n      eval_steps=eval_steps,\n      export_strategies=export_strategies,\n      eval_delay_secs=120,)\n\n\ndef main(unused_argv):\n  tf.flags.mark_flag_as_required(\'model_dir\')\n  tf.flags.mark_flag_as_required(\'pipeline_config_path\')\n  config = tf.contrib.learn.RunConfig(model_dir=FLAGS.model_dir)\n  learn_runner.run(\n      experiment_fn=build_experiment_fn(FLAGS.num_train_steps,\n                                        FLAGS.num_eval_steps),\n      run_config=config,\n      hparams=model_hparams.create_hparams())\n\n\nif __name__ == \'__main__\':\n  tf.app.run()\n'"
src/object_detection/model_hparams.py,2,"b'# Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Hyperparameters for the object detection model in TF.learn.\n\nThis file consolidates and documents the hyperparameters used by the model.\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\n\n\ndef create_hparams(hparams_overrides=None):\n  """"""Returns hyperparameters, including any flag value overrides.\n\n  Args:\n    hparams_overrides: Optional hparams overrides, represented as a\n      string containing comma-separated hparam_name=value pairs.\n\n  Returns:\n    The hyperparameters as a tf.HParams object.\n  """"""\n  hparams = tf.contrib.training.HParams(\n      # Whether a fine tuning checkpoint (provided in the pipeline config)\n      # should be loaded for training.\n      load_pretrained=True)\n  # Override any of the preceding hyperparameter values.\n  if hparams_overrides:\n    hparams = hparams.parse(hparams_overrides)\n  return hparams\n'"
src/object_detection/model_test.py,27,"b'# Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Tests for object detection model.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport functools\nimport os\n\nimport numpy as np\nimport tensorflow as tf\n\nfrom object_detection import inputs\nfrom object_detection import model\nfrom object_detection import model_hparams\nfrom object_detection import model_test_util\nfrom object_detection.builders import model_builder\nfrom object_detection.core import standard_fields as fields\nfrom object_detection.utils import config_util\n\nFLAGS = tf.flags.FLAGS\n\nMODEL_NAME_FOR_TEST = model_test_util.SSD_INCEPTION_MODEL_NAME\n\n\ndef _get_data_path():\n  """"""Returns an absolute path to TFRecord file.""""""\n  return os.path.join(FLAGS.test_srcdir, model_test_util.PATH_BASE, \'test_data\',\n                      \'pets_examples.record\')\n\n\ndef _get_labelmap_path():\n  """"""Returns an absolute path to label map file.""""""\n  return os.path.join(FLAGS.test_srcdir, model_test_util.PATH_BASE, \'data\',\n                      \'pet_label_map.pbtxt\')\n\n\ndef _get_configs_for_model(model_name):\n  """"""Returns configurations for model.""""""\n  filename = model_test_util.GetPipelineConfigPath(model_name)\n  data_path = _get_data_path()\n  label_map_path = _get_labelmap_path()\n  configs = config_util.get_configs_from_pipeline_file(filename)\n  configs = config_util.merge_external_params_with_configs(\n      configs,\n      train_input_path=data_path,\n      eval_input_path=data_path,\n      label_map_path=label_map_path)\n  return configs\n\n\ndef setUpModule():\n  model_test_util.InitializeFlags(MODEL_NAME_FOR_TEST)\n\n\nclass ModelTflearnTest(tf.test.TestCase):\n\n  @classmethod\n  def setUpClass(cls):\n    tf.reset_default_graph()\n\n  def _assert_outputs_for_train_eval(self, configs, mode, class_agnostic=False):\n    model_config = configs[\'model\']\n    train_config = configs[\'train_config\']\n    with tf.Graph().as_default():\n      if mode == tf.estimator.ModeKeys.TRAIN:\n        features, labels = inputs.create_train_input_fn(\n            configs[\'train_config\'],\n            configs[\'train_input_config\'],\n            configs[\'model\'])()\n        batch_size = train_config.batch_size\n      else:\n        features, labels = inputs.create_eval_input_fn(\n            configs[\'eval_config\'],\n            configs[\'eval_input_config\'],\n            configs[\'model\'])()\n        batch_size = 1\n\n      detection_model_fn = functools.partial(\n          model_builder.build, model_config=model_config, is_training=True)\n\n      hparams = model_hparams.create_hparams(\n          hparams_overrides=\'load_pretrained=false\')\n\n      model_fn = model.create_model_fn(detection_model_fn, configs, hparams)\n      estimator_spec = model_fn(features, labels, mode)\n\n      self.assertIsNotNone(estimator_spec.loss)\n      self.assertIsNotNone(estimator_spec.predictions)\n      if class_agnostic:\n        self.assertNotIn(\'detection_classes\', estimator_spec.predictions)\n      else:\n        detection_classes = estimator_spec.predictions[\'detection_classes\']\n        self.assertEqual(batch_size, detection_classes.shape.as_list()[0])\n        self.assertEqual(tf.float32, detection_classes.dtype)\n      detection_boxes = estimator_spec.predictions[\'detection_boxes\']\n      detection_scores = estimator_spec.predictions[\'detection_scores\']\n      num_detections = estimator_spec.predictions[\'num_detections\']\n      self.assertEqual(batch_size, detection_boxes.shape.as_list()[0])\n      self.assertEqual(tf.float32, detection_boxes.dtype)\n      self.assertEqual(batch_size, detection_scores.shape.as_list()[0])\n      self.assertEqual(tf.float32, detection_scores.dtype)\n      self.assertEqual(tf.float32, num_detections.dtype)\n      if mode == tf.estimator.ModeKeys.TRAIN:\n        self.assertIsNotNone(estimator_spec.train_op)\n      return estimator_spec\n\n  def _assert_outputs_for_predict(self, configs):\n    model_config = configs[\'model\']\n\n    with tf.Graph().as_default():\n      features, _ = inputs.create_eval_input_fn(\n          configs[\'eval_config\'],\n          configs[\'eval_input_config\'],\n          configs[\'model\'])()\n      detection_model_fn = functools.partial(\n          model_builder.build, model_config=model_config, is_training=False)\n\n      hparams = model_hparams.create_hparams(\n          hparams_overrides=\'load_pretrained=false\')\n\n      model_fn = model.create_model_fn(detection_model_fn, configs, hparams)\n      estimator_spec = model_fn(features, None, tf.estimator.ModeKeys.PREDICT)\n\n      self.assertIsNone(estimator_spec.loss)\n      self.assertIsNone(estimator_spec.train_op)\n      self.assertIsNotNone(estimator_spec.predictions)\n      self.assertIsNotNone(estimator_spec.export_outputs)\n      self.assertIn(tf.saved_model.signature_constants.PREDICT_METHOD_NAME,\n                    estimator_spec.export_outputs)\n\n  def testModelFnInTrainMode(self):\n    """"""Tests the model function in TRAIN mode.""""""\n    configs = _get_configs_for_model(MODEL_NAME_FOR_TEST)\n    self._assert_outputs_for_train_eval(configs, tf.estimator.ModeKeys.TRAIN)\n\n  def testModelFnInEvalMode(self):\n    """"""Tests the model function in EVAL mode.""""""\n    configs = _get_configs_for_model(MODEL_NAME_FOR_TEST)\n    self._assert_outputs_for_train_eval(configs, tf.estimator.ModeKeys.EVAL)\n\n  def testModelFnInPredictMode(self):\n    """"""Tests the model function in PREDICT mode.""""""\n    configs = _get_configs_for_model(MODEL_NAME_FOR_TEST)\n    self._assert_outputs_for_predict(configs)\n\n  def testExperiment(self):\n    """"""Tests that the `Experiment` object is constructed correctly.""""""\n    experiment = model_test_util.BuildExperiment()\n    model_dir = experiment.estimator.model_dir\n    pipeline_config_path = os.path.join(model_dir, \'pipeline.config\')\n    self.assertTrue(tf.gfile.Exists(pipeline_config_path))\n\n\nclass UnbatchTensorsTest(tf.test.TestCase):\n\n  def test_unbatch_without_unpadding(self):\n    image_placeholder = tf.placeholder(tf.float32, [2, None, None, None])\n    groundtruth_boxes_placeholder = tf.placeholder(tf.float32, [2, None, None])\n    groundtruth_classes_placeholder = tf.placeholder(tf.float32,\n                                                     [2, None, None])\n    groundtruth_weights_placeholder = tf.placeholder(tf.float32, [2, None])\n\n    tensor_dict = {\n        fields.InputDataFields.image:\n            image_placeholder,\n        fields.InputDataFields.groundtruth_boxes:\n            groundtruth_boxes_placeholder,\n        fields.InputDataFields.groundtruth_classes:\n            groundtruth_classes_placeholder,\n        fields.InputDataFields.groundtruth_weights:\n            groundtruth_weights_placeholder\n    }\n    unbatched_tensor_dict = model.unstack_batch(\n        tensor_dict, unpad_groundtruth_tensors=False)\n\n    with self.test_session() as sess:\n      unbatched_tensor_dict_out = sess.run(\n          unbatched_tensor_dict,\n          feed_dict={\n              image_placeholder:\n                  np.random.rand(2, 4, 4, 3).astype(np.float32),\n              groundtruth_boxes_placeholder:\n                  np.random.rand(2, 5, 4).astype(np.float32),\n              groundtruth_classes_placeholder:\n                  np.random.rand(2, 5, 6).astype(np.float32),\n              groundtruth_weights_placeholder:\n                  np.random.rand(2, 5).astype(np.float32)\n          })\n    for image_out in unbatched_tensor_dict_out[fields.InputDataFields.image]:\n      self.assertAllEqual(image_out.shape, [4, 4, 3])\n    for groundtruth_boxes_out in unbatched_tensor_dict_out[\n        fields.InputDataFields.groundtruth_boxes]:\n      self.assertAllEqual(groundtruth_boxes_out.shape, [5, 4])\n    for groundtruth_classes_out in unbatched_tensor_dict_out[\n        fields.InputDataFields.groundtruth_classes]:\n      self.assertAllEqual(groundtruth_classes_out.shape, [5, 6])\n    for groundtruth_weights_out in unbatched_tensor_dict_out[\n        fields.InputDataFields.groundtruth_weights]:\n      self.assertAllEqual(groundtruth_weights_out.shape, [5])\n\n  def test_unbatch_and_unpad_groundtruth_tensors(self):\n    image_placeholder = tf.placeholder(tf.float32, [2, None, None, None])\n    groundtruth_boxes_placeholder = tf.placeholder(tf.float32, [2, 5, None])\n    groundtruth_classes_placeholder = tf.placeholder(tf.float32, [2, 5, None])\n    groundtruth_weights_placeholder = tf.placeholder(tf.float32, [2, 5])\n    num_groundtruth_placeholder = tf.placeholder(tf.int32, [2])\n\n    tensor_dict = {\n        fields.InputDataFields.image:\n            image_placeholder,\n        fields.InputDataFields.groundtruth_boxes:\n            groundtruth_boxes_placeholder,\n        fields.InputDataFields.groundtruth_classes:\n            groundtruth_classes_placeholder,\n        fields.InputDataFields.groundtruth_weights:\n            groundtruth_weights_placeholder,\n        fields.InputDataFields.num_groundtruth_boxes:\n            num_groundtruth_placeholder\n    }\n    unbatched_tensor_dict = model.unstack_batch(\n        tensor_dict, unpad_groundtruth_tensors=True)\n    with self.test_session() as sess:\n      unbatched_tensor_dict_out = sess.run(\n          unbatched_tensor_dict,\n          feed_dict={\n              image_placeholder:\n                  np.random.rand(2, 4, 4, 3).astype(np.float32),\n              groundtruth_boxes_placeholder:\n                  np.random.rand(2, 5, 4).astype(np.float32),\n              groundtruth_classes_placeholder:\n                  np.random.rand(2, 5, 6).astype(np.float32),\n              groundtruth_weights_placeholder:\n                  np.random.rand(2, 5).astype(np.float32),\n              num_groundtruth_placeholder:\n                  np.array([3, 3], np.int32)\n          })\n    for image_out in unbatched_tensor_dict_out[fields.InputDataFields.image]:\n      self.assertAllEqual(image_out.shape, [4, 4, 3])\n    for groundtruth_boxes_out in unbatched_tensor_dict_out[\n        fields.InputDataFields.groundtruth_boxes]:\n      self.assertAllEqual(groundtruth_boxes_out.shape, [3, 4])\n    for groundtruth_classes_out in unbatched_tensor_dict_out[\n        fields.InputDataFields.groundtruth_classes]:\n      self.assertAllEqual(groundtruth_classes_out.shape, [3, 6])\n    for groundtruth_weights_out in unbatched_tensor_dict_out[\n        fields.InputDataFields.groundtruth_weights]:\n      self.assertAllEqual(groundtruth_weights_out.shape, [3])\n\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
src/object_detection/model_test_util.py,2,"b'# Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Common utils for tests for object detection tflearn model.""""""\n\nfrom __future__ import absolute_import\n\nimport os\nimport tempfile\nimport tensorflow as tf\n\n\nfrom object_detection import model\nfrom object_detection import model_hparams\n\nFLAGS = tf.flags.FLAGS\n\nFASTER_RCNN_MODEL_NAME = \'faster_rcnn_resnet50_pets\'\nSSD_INCEPTION_MODEL_NAME = \'ssd_inception_v2_pets\'\nPATH_BASE = \'google3/third_party/tensorflow_models/object_detection/\'\n\n\ndef GetPipelineConfigPath(model_name):\n  """"""Returns path to the local pipeline config file.""""""\n  return os.path.join(FLAGS.test_srcdir, PATH_BASE, \'samples\', \'configs\',\n                      model_name + \'.config\')\n\n\ndef InitializeFlags(model_name_for_test):\n  FLAGS.model_dir = tempfile.mkdtemp()\n  FLAGS.pipeline_config_path = GetPipelineConfigPath(model_name_for_test)\n\n\ndef BuildExperiment():\n  """"""Builds an Experiment object for testing purposes.""""""\n  run_config = tf.contrib.learn.RunConfig()\n  hparams = model_hparams.create_hparams(\n      hparams_overrides=\'load_pretrained=false\')\n\n  # pylint: disable=protected-access\n  experiment_fn = model.build_experiment_fn(10, 10)\n  # pylint: enable=protected-access\n  return experiment_fn(run_config, hparams)\n'"
src/object_detection/model_tpu.py,26,"b'# Copyright 2018 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\nr""""""Creates and runs `Estimator` for object detection model on TPUs.\n\nThis uses the TPUEstimator API to define and run a model in TRAIN/EVAL modes.\n""""""\n# pylint: enable=line-too-long\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport functools\nimport os\n\nimport tensorflow as tf\n\nfrom tensorflow.contrib.tpu.python.tpu import tpu_config\nfrom tensorflow.contrib.tpu.python.tpu import tpu_estimator\nfrom tensorflow.contrib.training.python.training import evaluation\n\nfrom object_detection import inputs\nfrom object_detection import model\nfrom object_detection import model_hparams\nfrom object_detection.builders import model_builder\nfrom object_detection.utils import config_util\n\ntf.flags.DEFINE_bool(\'use_tpu\', True, \'Use TPUs rather than plain CPUs\')\n\n# Cloud TPU Cluster Resolvers\ntf.flags.DEFINE_string(\n    \'gcp_project\',\n    default=None,\n    help=\'Project name for the Cloud TPU-enabled project. If not specified, we \'\n    \'will attempt to automatically detect the GCE project from metadata.\')\ntf.flags.DEFINE_string(\n    \'tpu_zone\',\n    default=None,\n    help=\'GCE zone where the Cloud TPU is located in. If not specified, we \'\n    \'will attempt to automatically detect the GCE project from metadata.\')\ntf.flags.DEFINE_string(\n    \'tpu_name\',\n    default=None,\n    help=\'Name of the Cloud TPU for Cluster Resolvers. You must specify either \'\n    \'this flag or --master.\')\n\ntf.flags.DEFINE_string(\n    \'master\', default=None,\n    help=\'GRPC URL of the master (e.g. grpc://ip.address.of.tpu:8470). You \'\n    \'must specify either this flag or --tpu_name.\')\n\ntf.flags.DEFINE_integer(\'num_shards\', 8, \'Number of shards (TPU cores).\')\ntf.flags.DEFINE_integer(\'iterations_per_loop\', 100,\n                        \'Number of iterations per TPU training loop.\')\n# For mode=train_and_eval, evaluation occurs after training is finished.\n# Note: independently of steps_per_checkpoint, estimator will save the most\n# recent checkpoint every 10 minutes by default for train_and_eval\ntf.flags.DEFINE_string(\'mode\', \'train_and_eval\',\n                       \'Mode to run: train, eval, train_and_eval\')\ntf.flags.DEFINE_integer(\'train_batch_size\', 32 * 8, \'Batch size for training.\')\n\n# For EVAL.\ntf.flags.DEFINE_integer(\'min_eval_interval_secs\', 180,\n                        \'Minimum seconds between evaluations.\')\ntf.flags.DEFINE_integer(\n    \'eval_timeout_secs\', None,\n    \'Maximum seconds between checkpoints before evaluation terminates.\')\ntf.flags.DEFINE_string(\'hparams_overrides\', None, \'Comma-separated list of \'\n                       \'hyperparameters to override defaults.\')\ntf.flags.DEFINE_boolean(\'eval_training_data\', False,\n                        \'If training data should be evaluated for this job.\')\n\nFLAGS = tf.flags.FLAGS\n\n\ndef create_estimator(run_config,\n                     hparams,\n                     pipeline_config_path,\n                     train_steps=None,\n                     eval_steps=None,\n                     train_batch_size=None,\n                     model_fn_creator=model.create_model_fn,\n                     use_tpu=False,\n                     num_shards=1,\n                     params=None,\n                     **kwargs):\n  """"""Creates an `Estimator` object.\n\n  Args:\n    run_config: A `RunConfig`.\n    hparams: A `HParams`.\n    pipeline_config_path: A path to a pipeline config file.\n    train_steps: Number of training steps. If None, the number of training steps\n      is set from the `TrainConfig` proto.\n    eval_steps: Number of evaluation steps per evaluation cycle. If None, the\n      number of evaluation steps is set from the `EvalConfig` proto.\n    train_batch_size: Training batch size. If none, use batch size from\n      `TrainConfig` proto.\n    model_fn_creator: A function that creates a `model_fn` for `Estimator`.\n      Follows the signature:\n\n      * Args:\n        * `detection_model_fn`: Function that returns `DetectionModel` instance.\n        * `configs`: Dictionary of pipeline config objects.\n        * `hparams`: `HParams` object.\n      * Returns:\n        `model_fn` for `Estimator`.\n\n    use_tpu: Boolean, whether training and evaluation should run on TPU.\n    num_shards: Number of shards (TPU cores).\n    params: Parameter dictionary passed from the estimator.\n    **kwargs: Additional keyword arguments for configuration override.\n\n  Returns:\n    Estimator: A estimator object used for training and evaluation\n    train_input_fn: Input function for the training loop\n    eval_validation_input_fn: Input function to run for evaluation on\n      validation data.\n    eval_training_input_fn: Input function to run for evaluation on\n      training data.\n    train_steps: Number of training steps either from arg `train_steps` or\n      `TrainConfig` proto\n    eval_steps: Number of evaluation steps either from arg `eval_steps` or\n      `EvalConfig` proto\n  """"""\n  configs = config_util.get_configs_from_pipeline_file(pipeline_config_path)\n  configs = config_util.merge_external_params_with_configs(\n      configs,\n      hparams,\n      train_steps=train_steps,\n      eval_steps=eval_steps,\n      batch_size=train_batch_size,\n      **kwargs)\n  model_config = configs[\'model\']\n  train_config = configs[\'train_config\']\n  train_input_config = configs[\'train_input_config\']\n  eval_config = configs[\'eval_config\']\n  eval_input_config = configs[\'eval_input_config\']\n  if FLAGS.eval_training_data:\n    eval_input_config = configs[\'train_input_config\']\n\n  if params is None:\n    params = {}\n\n  if train_steps is None and train_config.num_steps:\n    train_steps = train_config.num_steps\n\n  if eval_steps is None and eval_config.num_examples:\n    eval_steps = eval_config.num_examples\n\n  detection_model_fn = functools.partial(\n      model_builder.build, model_config=model_config)\n\n  # Create the input functions for TRAIN/EVAL.\n  train_input_fn = inputs.create_train_input_fn(\n      train_config=train_config,\n      train_input_config=train_input_config,\n      model_config=model_config)\n  eval_validation_input_fn = inputs.create_eval_input_fn(\n      eval_config=eval_config,\n      eval_input_config=eval_input_config,\n      model_config=model_config)\n  eval_training_input_fn = inputs.create_eval_input_fn(\n      eval_config=eval_config,\n      eval_input_config=train_input_config,\n      model_config=model_config)\n\n  estimator = tpu_estimator.TPUEstimator(\n      model_fn=model_fn_creator(detection_model_fn, configs, hparams,\n                                use_tpu),\n      train_batch_size=train_config.batch_size,\n      # For each core, only batch size 1 is supported for eval.\n      eval_batch_size=num_shards * 1 if use_tpu else 1,\n      use_tpu=use_tpu,\n      config=run_config,\n      params=params)\n  return (estimator, train_input_fn, eval_validation_input_fn,\n          eval_training_input_fn, train_steps, eval_steps)\n\n\ndef main(unused_argv):\n  tf.flags.mark_flag_as_required(\'model_dir\')\n  tf.flags.mark_flag_as_required(\'pipeline_config_path\')\n\n  if FLAGS.master is None and FLAGS.tpu_name is None:\n    raise RuntimeError(\'You must specify either --master or --tpu_name.\')\n\n  if FLAGS.master is not None:\n    if FLAGS.tpu_name is not None:\n      tf.logging.warn(\'Both --master and --tpu_name are set. Ignoring \'\n                      \'--tpu_name and using --master.\')\n    tpu_grpc_url = FLAGS.master\n  else:\n    tpu_cluster_resolver = (\n        tf.contrib.cluster_resolver.python.training.TPUClusterResolver(\n            tpu_names=[FLAGS.tpu_name],\n            zone=FLAGS.tpu_zone,\n            project=FLAGS.gcp_project))\n    tpu_grpc_url = tpu_cluster_resolver.get_master()\n\n  config = tpu_config.RunConfig(\n      master=tpu_grpc_url,\n      evaluation_master=tpu_grpc_url,\n      model_dir=FLAGS.model_dir,\n      tpu_config=tpu_config.TPUConfig(\n          iterations_per_loop=FLAGS.iterations_per_loop,\n          num_shards=FLAGS.num_shards))\n  params = {}\n  (estimator, train_input_fn, eval_validation_input_fn, eval_training_input_fn,\n   train_steps, eval_steps) = (\n       create_estimator(\n           config,\n           model_hparams.create_hparams(\n               hparams_overrides=FLAGS.hparams_overrides),\n           FLAGS.pipeline_config_path,\n           train_steps=FLAGS.num_train_steps,\n           eval_steps=FLAGS.num_eval_steps,\n           train_batch_size=FLAGS.train_batch_size,\n           use_tpu=FLAGS.use_tpu,\n           num_shards=FLAGS.num_shards,\n           params=params))\n\n  if FLAGS.mode in [\'train\', \'train_and_eval\']:\n    estimator.train(input_fn=train_input_fn, max_steps=train_steps)\n\n  if FLAGS.mode == \'train_and_eval\':\n    # Eval one time.\n    eval_results = estimator.evaluate(\n        input_fn=eval_validation_input_fn, steps=eval_steps)\n    tf.logging.info(\'Eval results: %s\' % eval_results)\n\n  # Continuously evaluating.\n  if FLAGS.mode == \'eval\':\n    def terminate_eval():\n      tf.logging.info(\'Terminating eval after %d seconds of no checkpoints\' %\n                      FLAGS.eval_timeout_secs)\n      return True\n\n    # Run evaluation when there\'s a new checkpoint.\n    for ckpt in evaluation.checkpoints_iterator(\n        FLAGS.model_dir,\n        min_interval_secs=FLAGS.min_eval_interval_secs,\n        timeout=FLAGS.eval_timeout_secs,\n        timeout_fn=terminate_eval):\n\n      tf.logging.info(\'Starting to evaluate.\')\n      if FLAGS.eval_training_data:\n        name = \'training_data\'\n        input_fn = eval_training_input_fn\n      else:\n        name = \'validation_data\'\n        input_fn = eval_validation_input_fn\n      try:\n        eval_results = estimator.evaluate(\n            input_fn=input_fn,\n            steps=eval_steps,\n            checkpoint_path=ckpt,\n            name=name)\n        tf.logging.info(\'Eval results: %s\' % eval_results)\n\n        # Terminate eval job when final checkpoint is reached\n        current_step = int(os.path.basename(ckpt).split(\'-\')[1])\n        if current_step >= train_steps:\n          tf.logging.info(\n              \'Evaluation finished after training step %d\' % current_step)\n          break\n\n      except tf.errors.NotFoundError:\n        tf.logging.info(\n            \'Checkpoint %s no longer exists, skipping checkpoint\' % ckpt)\n\n\nif __name__ == \'__main__\':\n  tf.app.run()\n'"
src/object_detection/train.py,8,"b'# Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nr""""""Training executable for detection models.\n\nThis executable is used to train DetectionModels. There are two ways of\nconfiguring the training job:\n\n1) A single pipeline_pb2.TrainEvalPipelineConfig configuration file\ncan be specified by --pipeline_config_path.\n\nExample usage:\n    ./train \\\n        --logtostderr \\\n        --train_dir=path/to/train_dir \\\n        --pipeline_config_path=pipeline_config.pbtxt\n\n2) Three configuration files can be provided: a model_pb2.DetectionModel\nconfiguration file to define what type of DetectionModel is being trained, an\ninput_reader_pb2.InputReader file to specify what training data will be used and\na train_pb2.TrainConfig file to configure training parameters.\n\nExample usage:\n    ./train \\\n        --logtostderr \\\n        --train_dir=path/to/train_dir \\\n        --model_config_path=model_config.pbtxt \\\n        --train_config_path=train_config.pbtxt \\\n        --input_config_path=train_input_config.pbtxt\n""""""\n\nimport functools\nimport json\nimport os\nimport tensorflow as tf\n\nfrom object_detection import trainer\nfrom object_detection.builders import dataset_builder\nfrom object_detection.builders import model_builder\nfrom object_detection.utils import config_util\nfrom object_detection.utils import dataset_util\n\ntf.logging.set_verbosity(tf.logging.INFO)\n\nflags = tf.app.flags\nflags.DEFINE_string(\'master\', \'\', \'Name of the TensorFlow master to use.\')\nflags.DEFINE_integer(\'task\', 0, \'task id\')\nflags.DEFINE_integer(\'num_clones\', 1, \'Number of clones to deploy per worker.\')\nflags.DEFINE_boolean(\'clone_on_cpu\', False,\n                     \'Force clones to be deployed on CPU.  Note that even if \'\n                     \'set to False (allowing ops to run on gpu), some ops may \'\n                     \'still be run on the CPU if they have no GPU kernel.\')\nflags.DEFINE_integer(\'worker_replicas\', 1, \'Number of worker+trainer \'\n                     \'replicas.\')\nflags.DEFINE_integer(\'ps_tasks\', 0,\n                     \'Number of parameter server tasks. If None, does not use \'\n                     \'a parameter server.\')\nflags.DEFINE_string(\'train_dir\', \'\',\n                    \'Directory to save the checkpoints and training summaries.\')\n\nflags.DEFINE_string(\'pipeline_config_path\', \'\',\n                    \'Path to a pipeline_pb2.TrainEvalPipelineConfig config \'\n                    \'file. If provided, other configs are ignored\')\n\nflags.DEFINE_string(\'train_config_path\', \'\',\n                    \'Path to a train_pb2.TrainConfig config file.\')\nflags.DEFINE_string(\'input_config_path\', \'\',\n                    \'Path to an input_reader_pb2.InputReader config file.\')\nflags.DEFINE_string(\'model_config_path\', \'\',\n                    \'Path to a model_pb2.DetectionModel config file.\')\n\nFLAGS = flags.FLAGS\n\n\ndef main(_):\n  assert FLAGS.train_dir, \'`train_dir` is missing.\'\n  if FLAGS.task == 0: tf.gfile.MakeDirs(FLAGS.train_dir)\n  if FLAGS.pipeline_config_path:\n    configs = config_util.get_configs_from_pipeline_file(\n        FLAGS.pipeline_config_path)\n    if FLAGS.task == 0:\n      tf.gfile.Copy(FLAGS.pipeline_config_path,\n                    os.path.join(FLAGS.train_dir, \'pipeline.config\'),\n                    overwrite=True)\n  else:\n    configs = config_util.get_configs_from_multiple_files(\n        model_config_path=FLAGS.model_config_path,\n        train_config_path=FLAGS.train_config_path,\n        train_input_config_path=FLAGS.input_config_path)\n    if FLAGS.task == 0:\n      for name, config in [(\'model.config\', FLAGS.model_config_path),\n                           (\'train.config\', FLAGS.train_config_path),\n                           (\'input.config\', FLAGS.input_config_path)]:\n        tf.gfile.Copy(config, os.path.join(FLAGS.train_dir, name),\n                      overwrite=True)\n\n  model_config = configs[\'model\']\n  train_config = configs[\'train_config\']\n  input_config = configs[\'train_input_config\']\n\n  model_fn = functools.partial(\n      model_builder.build,\n      model_config=model_config,\n      is_training=True)\n\n  def get_next(config):\n    return dataset_util.make_initializable_iterator(\n        dataset_builder.build(config)).get_next()\n\n  create_input_dict_fn = functools.partial(get_next, input_config)\n\n  env = json.loads(os.environ.get(\'TF_CONFIG\', \'{}\'))\n  cluster_data = env.get(\'cluster\', None)\n  cluster = tf.train.ClusterSpec(cluster_data) if cluster_data else None\n  task_data = env.get(\'task\', None) or {\'type\': \'master\', \'index\': 0}\n  task_info = type(\'TaskSpec\', (object,), task_data)\n\n  # Parameters for a single worker.\n  ps_tasks = 0\n  worker_replicas = 1\n  worker_job_name = \'lonely_worker\'\n  task = 0\n  is_chief = True\n  master = \'\'\n\n  if cluster_data and \'worker\' in cluster_data:\n    # Number of total worker replicas include ""worker""s and the ""master"".\n    worker_replicas = len(cluster_data[\'worker\']) + 1\n  if cluster_data and \'ps\' in cluster_data:\n    ps_tasks = len(cluster_data[\'ps\'])\n\n  if worker_replicas > 1 and ps_tasks < 1:\n    raise ValueError(\'At least 1 ps task is needed for distributed training.\')\n\n  if worker_replicas >= 1 and ps_tasks > 0:\n    # Set up distributed training.\n    server = tf.train.Server(tf.train.ClusterSpec(cluster), protocol=\'grpc\',\n                             job_name=task_info.type,\n                             task_index=task_info.index)\n    if task_info.type == \'ps\':\n      server.join()\n      return\n\n    worker_job_name = \'%s/task:%d\' % (task_info.type, task_info.index)\n    task = task_info.index\n    is_chief = (task_info.type == \'master\')\n    master = server.target\n\n  trainer.train(create_input_dict_fn, model_fn, train_config, master, task,\n                FLAGS.num_clones, worker_replicas, FLAGS.clone_on_cpu, ps_tasks,\n                worker_job_name, is_chief, FLAGS.train_dir)\n\n\nif __name__ == \'__main__\':\n  tf.app.run()\n'"
src/object_detection/trainer.py,32,"b'# Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n""""""Detection model trainer.\n\nThis file provides a generic training method that can be used to train a\nDetectionModel.\n""""""\n\nimport functools\n\nimport tensorflow as tf\n\nfrom object_detection.builders import optimizer_builder\nfrom object_detection.builders import preprocessor_builder\nfrom object_detection.core import batcher\nfrom object_detection.core import preprocessor\nfrom object_detection.core import standard_fields as fields\nfrom object_detection.utils import ops as util_ops\nfrom object_detection.utils import variables_helper\nfrom deployment import model_deploy\n\nslim = tf.contrib.slim\n\n\ndef create_input_queue(batch_size_per_clone, create_tensor_dict_fn,\n                       batch_queue_capacity, num_batch_queue_threads,\n                       prefetch_queue_capacity, data_augmentation_options):\n  """"""Sets up reader, prefetcher and returns input queue.\n\n  Args:\n    batch_size_per_clone: batch size to use per clone.\n    create_tensor_dict_fn: function to create tensor dictionary.\n    batch_queue_capacity: maximum number of elements to store within a queue.\n    num_batch_queue_threads: number of threads to use for batching.\n    prefetch_queue_capacity: maximum capacity of the queue used to prefetch\n                             assembled batches.\n    data_augmentation_options: a list of tuples, where each tuple contains a\n      data augmentation function and a dictionary containing arguments and their\n      values (see preprocessor.py).\n\n  Returns:\n    input queue: a batcher.BatchQueue object holding enqueued tensor_dicts\n      (which hold images, boxes and targets).  To get a batch of tensor_dicts,\n      call input_queue.Dequeue().\n  """"""\n  tensor_dict = create_tensor_dict_fn()\n\n  tensor_dict[fields.InputDataFields.image] = tf.expand_dims(\n      tensor_dict[fields.InputDataFields.image], 0)\n\n  images = tensor_dict[fields.InputDataFields.image]\n  float_images = tf.to_float(images)\n  tensor_dict[fields.InputDataFields.image] = float_images\n\n  include_instance_masks = (fields.InputDataFields.groundtruth_instance_masks\n                            in tensor_dict)\n  include_keypoints = (fields.InputDataFields.groundtruth_keypoints\n                       in tensor_dict)\n  if data_augmentation_options:\n    tensor_dict = preprocessor.preprocess(\n        tensor_dict, data_augmentation_options,\n        func_arg_map=preprocessor.get_default_func_arg_map(\n            include_instance_masks=include_instance_masks,\n            include_keypoints=include_keypoints))\n\n  input_queue = batcher.BatchQueue(\n      tensor_dict,\n      batch_size=batch_size_per_clone,\n      batch_queue_capacity=batch_queue_capacity,\n      num_batch_queue_threads=num_batch_queue_threads,\n      prefetch_queue_capacity=prefetch_queue_capacity)\n  return input_queue\n\n\ndef get_inputs(input_queue, num_classes, merge_multiple_label_boxes=False):\n  """"""Dequeues batch and constructs inputs to object detection model.\n\n  Args:\n    input_queue: BatchQueue object holding enqueued tensor_dicts.\n    num_classes: Number of classes.\n    merge_multiple_label_boxes: Whether to merge boxes with multiple labels\n      or not. Defaults to false. Merged boxes are represented with a single\n      box and a k-hot encoding of the multiple labels associated with the\n      boxes.\n\n  Returns:\n    images: a list of 3-D float tensor of images.\n    image_keys: a list of string keys for the images.\n    locations_list: a list of tensors of shape [num_boxes, 4]\n      containing the corners of the groundtruth boxes.\n    classes_list: a list of padded one-hot tensors containing target classes.\n    masks_list: a list of 3-D float tensors of shape [num_boxes, image_height,\n      image_width] containing instance masks for objects if present in the\n      input_queue. Else returns None.\n    keypoints_list: a list of 3-D float tensors of shape [num_boxes,\n      num_keypoints, 2] containing keypoints for objects if present in the\n      input queue. Else returns None.\n    weights_lists: a list of 1-D float32 tensors of shape [num_boxes]\n      containing groundtruth weight for each box.\n  """"""\n  read_data_list = input_queue.dequeue()\n  label_id_offset = 1\n  def extract_images_and_targets(read_data):\n    """"""Extract images and targets from the input dict.""""""\n    image = read_data[fields.InputDataFields.image]\n    key = \'\'\n    if fields.InputDataFields.source_id in read_data:\n      key = read_data[fields.InputDataFields.source_id]\n    location_gt = read_data[fields.InputDataFields.groundtruth_boxes]\n    classes_gt = tf.cast(read_data[fields.InputDataFields.groundtruth_classes],\n                         tf.int32)\n    classes_gt -= label_id_offset\n    if merge_multiple_label_boxes:\n      location_gt, classes_gt, _ = util_ops.merge_boxes_with_multiple_labels(\n          location_gt, classes_gt, num_classes)\n    else:\n      classes_gt = util_ops.padded_one_hot_encoding(\n          indices=classes_gt, depth=num_classes, left_pad=0)\n    masks_gt = read_data.get(fields.InputDataFields.groundtruth_instance_masks)\n    keypoints_gt = read_data.get(fields.InputDataFields.groundtruth_keypoints)\n    if (merge_multiple_label_boxes and (\n        masks_gt is not None or keypoints_gt is not None)):\n      raise NotImplementedError(\'Multi-label support is only for boxes.\')\n    weights_gt = read_data.get(\n        fields.InputDataFields.groundtruth_weights)\n    return (image, key, location_gt, classes_gt, masks_gt, keypoints_gt,\n            weights_gt)\n\n  return zip(*map(extract_images_and_targets, read_data_list))\n\n\ndef _create_losses(input_queue, create_model_fn, train_config):\n  """"""Creates loss function for a DetectionModel.\n\n  Args:\n    input_queue: BatchQueue object holding enqueued tensor_dicts.\n    create_model_fn: A function to create the DetectionModel.\n    train_config: a train_pb2.TrainConfig protobuf.\n  """"""\n  detection_model = create_model_fn()\n  (images, _, groundtruth_boxes_list, groundtruth_classes_list,\n   groundtruth_masks_list, groundtruth_keypoints_list, _) = get_inputs(\n       input_queue,\n       detection_model.num_classes,\n       train_config.merge_multiple_label_boxes)\n\n  preprocessed_images = []\n  true_image_shapes = []\n  for image in images:\n    resized_image, true_image_shape = detection_model.preprocess(image)\n    preprocessed_images.append(resized_image)\n    true_image_shapes.append(true_image_shape)\n\n  images = tf.concat(preprocessed_images, 0)\n  true_image_shapes = tf.concat(true_image_shapes, 0)\n\n  if any(mask is None for mask in groundtruth_masks_list):\n    groundtruth_masks_list = None\n  if any(keypoints is None for keypoints in groundtruth_keypoints_list):\n    groundtruth_keypoints_list = None\n\n  detection_model.provide_groundtruth(groundtruth_boxes_list,\n                                      groundtruth_classes_list,\n                                      groundtruth_masks_list,\n                                      groundtruth_keypoints_list)\n  prediction_dict = detection_model.predict(images, true_image_shapes)\n\n  losses_dict = detection_model.loss(prediction_dict, true_image_shapes)\n  for loss_tensor in losses_dict.values():\n    tf.losses.add_loss(loss_tensor)\n\n\ndef train(create_tensor_dict_fn, create_model_fn, train_config, master, task,\n          num_clones, worker_replicas, clone_on_cpu, ps_tasks, worker_job_name,\n          is_chief, train_dir, graph_hook_fn=None):\n  """"""Training function for detection models.\n\n  Args:\n    create_tensor_dict_fn: a function to create a tensor input dictionary.\n    create_model_fn: a function that creates a DetectionModel and generates\n                     losses.\n    train_config: a train_pb2.TrainConfig protobuf.\n    master: BNS name of the TensorFlow master to use.\n    task: The task id of this training instance.\n    num_clones: The number of clones to run per machine.\n    worker_replicas: The number of work replicas to train with.\n    clone_on_cpu: True if clones should be forced to run on CPU.\n    ps_tasks: Number of parameter server tasks.\n    worker_job_name: Name of the worker job.\n    is_chief: Whether this replica is the chief replica.\n    train_dir: Directory to write checkpoints and training summaries to.\n    graph_hook_fn: Optional function that is called after the training graph is\n      completely built. This is helpful to perform additional changes to the\n      training graph such as optimizing batchnorm. The function should modify\n      the default graph.\n  """"""\n\n  detection_model = create_model_fn()\n  data_augmentation_options = [\n      preprocessor_builder.build(step)\n      for step in train_config.data_augmentation_options]\n\n  with tf.Graph().as_default():\n    # Build a configuration specifying multi-GPU and multi-replicas.\n    deploy_config = model_deploy.DeploymentConfig(\n        num_clones=num_clones,\n        clone_on_cpu=clone_on_cpu,\n        replica_id=task,\n        num_replicas=worker_replicas,\n        num_ps_tasks=ps_tasks,\n        worker_job_name=worker_job_name)\n\n    # Place the global step on the device storing the variables.\n    with tf.device(deploy_config.variables_device()):\n      global_step = slim.create_global_step()\n\n    with tf.device(deploy_config.inputs_device()):\n      input_queue = create_input_queue(\n          train_config.batch_size // num_clones, create_tensor_dict_fn,\n          train_config.batch_queue_capacity,\n          train_config.num_batch_queue_threads,\n          train_config.prefetch_queue_capacity, data_augmentation_options)\n\n    # Gather initial summaries.\n    # TODO(rathodv): See if summaries can be added/extracted from global tf\n    # collections so that they don\'t have to be passed around.\n    summaries = set(tf.get_collection(tf.GraphKeys.SUMMARIES))\n    global_summaries = set([])\n\n    model_fn = functools.partial(_create_losses,\n                                 create_model_fn=create_model_fn,\n                                 train_config=train_config)\n    clones = model_deploy.create_clones(deploy_config, model_fn, [input_queue])\n    first_clone_scope = clones[0].scope\n\n    # Gather update_ops from the first clone. These contain, for example,\n    # the updates for the batch_norm variables created by model_fn.\n    update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS, first_clone_scope)\n\n    with tf.device(deploy_config.optimizer_device()):\n      training_optimizer, optimizer_summary_vars = optimizer_builder.build(\n          train_config.optimizer)\n      for var in optimizer_summary_vars:\n        tf.summary.scalar(var.op.name, var, family=\'LearningRate\')\n\n    sync_optimizer = None\n    if train_config.sync_replicas:\n      training_optimizer = tf.train.SyncReplicasOptimizer(\n          training_optimizer,\n          replicas_to_aggregate=train_config.replicas_to_aggregate,\n          total_num_replicas=worker_replicas)\n      sync_optimizer = training_optimizer\n\n    # Create ops required to initialize the model from a given checkpoint.\n    init_fn = None\n    if train_config.fine_tune_checkpoint:\n      if not train_config.fine_tune_checkpoint_type:\n        # train_config.from_detection_checkpoint field is deprecated. For\n        # backward compatibility, fine_tune_checkpoint_type is set based on\n        # from_detection_checkpoint.\n        if train_config.from_detection_checkpoint:\n          train_config.fine_tune_checkpoint_type = \'detection\'\n        else:\n          train_config.fine_tune_checkpoint_type = \'classification\'\n      var_map = detection_model.restore_map(\n          fine_tune_checkpoint_type=train_config.fine_tune_checkpoint_type,\n          load_all_detection_checkpoint_vars=(\n              train_config.load_all_detection_checkpoint_vars))\n      available_var_map = (variables_helper.\n                           get_variables_available_in_checkpoint(\n                               var_map, train_config.fine_tune_checkpoint))\n      init_saver = tf.train.Saver(available_var_map)\n      def initializer_fn(sess):\n        init_saver.restore(sess, train_config.fine_tune_checkpoint)\n      init_fn = initializer_fn\n\n    with tf.device(deploy_config.optimizer_device()):\n      regularization_losses = (None if train_config.add_regularization_loss\n                               else [])\n      total_loss, grads_and_vars = model_deploy.optimize_clones(\n          clones, training_optimizer,\n          regularization_losses=regularization_losses)\n      total_loss = tf.check_numerics(total_loss, \'LossTensor is inf or nan.\')\n\n      # Optionally multiply bias gradients by train_config.bias_grad_multiplier.\n      if train_config.bias_grad_multiplier:\n        biases_regex_list = [\'.*/biases\']\n        grads_and_vars = variables_helper.multiply_gradients_matching_regex(\n            grads_and_vars,\n            biases_regex_list,\n            multiplier=train_config.bias_grad_multiplier)\n\n      # Optionally freeze some layers by setting their gradients to be zero.\n      if train_config.freeze_variables:\n        grads_and_vars = variables_helper.freeze_gradients_matching_regex(\n            grads_and_vars, train_config.freeze_variables)\n\n      # Optionally clip gradients\n      if train_config.gradient_clipping_by_norm > 0:\n        with tf.name_scope(\'clip_grads\'):\n          grads_and_vars = slim.learning.clip_gradient_norms(\n              grads_and_vars, train_config.gradient_clipping_by_norm)\n\n      # Create gradient updates.\n      grad_updates = training_optimizer.apply_gradients(grads_and_vars,\n                                                        global_step=global_step)\n      update_ops.append(grad_updates)\n      update_op = tf.group(*update_ops, name=\'update_barrier\')\n      with tf.control_dependencies([update_op]):\n        train_tensor = tf.identity(total_loss, name=\'train_op\')\n\n    if graph_hook_fn:\n      with tf.device(deploy_config.variables_device()):\n        graph_hook_fn()\n\n    # Add summaries.\n    for model_var in slim.get_model_variables():\n      global_summaries.add(tf.summary.histogram(\'ModelVars/\' +\n                                                model_var.op.name, model_var))\n    for loss_tensor in tf.losses.get_losses():\n      global_summaries.add(tf.summary.scalar(\'Losses/\' + loss_tensor.op.name,\n                                             loss_tensor))\n    global_summaries.add(\n        tf.summary.scalar(\'Losses/TotalLoss\', tf.losses.get_total_loss()))\n\n    # Add the summaries from the first clone. These contain the summaries\n    # created by model_fn and either optimize_clones() or _gather_clone_loss().\n    summaries |= set(tf.get_collection(tf.GraphKeys.SUMMARIES,\n                                       first_clone_scope))\n    summaries |= global_summaries\n\n    # Merge all summaries together.\n    summary_op = tf.summary.merge(list(summaries), name=\'summary_op\')\n\n    # Soft placement allows placing on CPU ops without GPU implementation.\n    session_config = tf.ConfigProto(allow_soft_placement=True,\n                                    log_device_placement=False)\n\n    # Save checkpoints regularly.\n    keep_checkpoint_every_n_hours = train_config.keep_checkpoint_every_n_hours\n    saver = tf.train.Saver(\n        keep_checkpoint_every_n_hours=keep_checkpoint_every_n_hours)\n\n    slim.learning.train(\n        train_tensor,\n        logdir=train_dir,\n        master=master,\n        is_chief=is_chief,\n        session_config=session_config,\n        startup_delay_steps=train_config.startup_delay_steps,\n        init_fn=init_fn,\n        summary_op=summary_op,\n        number_of_steps=(\n            train_config.num_steps if train_config.num_steps else None),\n        save_summaries_secs=120,\n        sync_optimizer=sync_optimizer,\n        saver=saver)\n'"
src/object_detection/trainer_test.py,21,"b'# Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n""""""Tests for object_detection.trainer.""""""\n\nimport tensorflow as tf\n\nfrom google.protobuf import text_format\n\nfrom object_detection import trainer\nfrom object_detection.core import losses\nfrom object_detection.core import model\nfrom object_detection.core import standard_fields as fields\nfrom object_detection.protos import train_pb2\n\n\nNUMBER_OF_CLASSES = 2\n\n\ndef get_input_function():\n  """"""A function to get test inputs. Returns an image with one box.""""""\n  image = tf.random_uniform([32, 32, 3], dtype=tf.float32)\n  key = tf.constant(\'image_000000\')\n  class_label = tf.random_uniform(\n      [1], minval=0, maxval=NUMBER_OF_CLASSES, dtype=tf.int32)\n  box_label = tf.random_uniform(\n      [1, 4], minval=0.4, maxval=0.6, dtype=tf.float32)\n\n  return {\n      fields.InputDataFields.image: image,\n      fields.InputDataFields.key: key,\n      fields.InputDataFields.groundtruth_classes: class_label,\n      fields.InputDataFields.groundtruth_boxes: box_label\n  }\n\n\nclass FakeDetectionModel(model.DetectionModel):\n  """"""A simple (and poor) DetectionModel for use in test.""""""\n\n  def __init__(self):\n    super(FakeDetectionModel, self).__init__(num_classes=NUMBER_OF_CLASSES)\n    self._classification_loss = losses.WeightedSigmoidClassificationLoss()\n    self._localization_loss = losses.WeightedSmoothL1LocalizationLoss()\n\n  def preprocess(self, inputs):\n    """"""Input preprocessing, resizes images to 28x28.\n\n    Args:\n      inputs: a [batch, height_in, width_in, channels] float32 tensor\n        representing a batch of images with values between 0 and 255.0.\n\n    Returns:\n      preprocessed_inputs: a [batch, 28, 28, channels] float32 tensor.\n      true_image_shapes: int32 tensor of shape [batch, 3] where each row is\n        of the form [height, width, channels] indicating the shapes\n        of true images in the resized images, as resized images can be padded\n        with zeros.\n    """"""\n    true_image_shapes = [inputs.shape[:-1].as_list()\n                         for _ in range(inputs.shape[-1])]\n    return tf.image.resize_images(inputs, [28, 28]), true_image_shapes\n\n  def predict(self, preprocessed_inputs, true_image_shapes):\n    """"""Prediction tensors from inputs tensor.\n\n    Args:\n      preprocessed_inputs: a [batch, 28, 28, channels] float32 tensor.\n      true_image_shapes: int32 tensor of shape [batch, 3] where each row is\n        of the form [height, width, channels] indicating the shapes\n        of true images in the resized images, as resized images can be padded\n        with zeros.\n\n    Returns:\n      prediction_dict: a dictionary holding prediction tensors to be\n        passed to the Loss or Postprocess functions.\n    """"""\n    flattened_inputs = tf.contrib.layers.flatten(preprocessed_inputs)\n    class_prediction = tf.contrib.layers.fully_connected(\n        flattened_inputs, self._num_classes)\n    box_prediction = tf.contrib.layers.fully_connected(flattened_inputs, 4)\n\n    return {\n        \'class_predictions_with_background\': tf.reshape(\n            class_prediction, [-1, 1, self._num_classes]),\n        \'box_encodings\': tf.reshape(box_prediction, [-1, 1, 4])\n    }\n\n  def postprocess(self, prediction_dict, true_image_shapes, **params):\n    """"""Convert predicted output tensors to final detections. Unused.\n\n    Args:\n      prediction_dict: a dictionary holding prediction tensors.\n      true_image_shapes: int32 tensor of shape [batch, 3] where each row is\n        of the form [height, width, channels] indicating the shapes\n        of true images in the resized images, as resized images can be padded\n        with zeros.\n      **params: Additional keyword arguments for specific implementations of\n        DetectionModel.\n\n    Returns:\n      detections: a dictionary with empty fields.\n    """"""\n    return {\n        \'detection_boxes\': None,\n        \'detection_scores\': None,\n        \'detection_classes\': None,\n        \'num_detections\': None\n    }\n\n  def loss(self, prediction_dict, true_image_shapes):\n    """"""Compute scalar loss tensors with respect to provided groundtruth.\n\n    Calling this function requires that groundtruth tensors have been\n    provided via the provide_groundtruth function.\n\n    Args:\n      prediction_dict: a dictionary holding predicted tensors\n      true_image_shapes: int32 tensor of shape [batch, 3] where each row is\n        of the form [height, width, channels] indicating the shapes\n        of true images in the resized images, as resized images can be padded\n        with zeros.\n\n    Returns:\n      a dictionary mapping strings (loss names) to scalar tensors representing\n        loss values.\n    """"""\n    batch_reg_targets = tf.stack(\n        self.groundtruth_lists(fields.BoxListFields.boxes))\n    batch_cls_targets = tf.stack(\n        self.groundtruth_lists(fields.BoxListFields.classes))\n    weights = tf.constant(\n        1.0, dtype=tf.float32,\n        shape=[len(self.groundtruth_lists(fields.BoxListFields.boxes)), 1])\n\n    location_losses = self._localization_loss(\n        prediction_dict[\'box_encodings\'], batch_reg_targets,\n        weights=weights)\n    cls_losses = self._classification_loss(\n        prediction_dict[\'class_predictions_with_background\'], batch_cls_targets,\n        weights=weights)\n\n    loss_dict = {\n        \'localization_loss\': tf.reduce_sum(location_losses),\n        \'classification_loss\': tf.reduce_sum(cls_losses),\n    }\n    return loss_dict\n\n  def restore_map(self, fine_tune_checkpoint_type=\'detection\'):\n    """"""Returns a map of variables to load from a foreign checkpoint.\n\n    Args:\n      fine_tune_checkpoint_type: whether to restore from a full detection\n        checkpoint (with compatible variable names) or to restore from a\n        classification checkpoint for initialization prior to training.\n        Valid values: `detection`, `classification`. Default \'detection\'.\n\n    Returns:\n      A dict mapping variable names to variables.\n    """"""\n    return {var.op.name: var for var in tf.global_variables()}\n\n\nclass TrainerTest(tf.test.TestCase):\n\n  def test_configure_trainer_and_train_two_steps(self):\n    train_config_text_proto = """"""\n    optimizer {\n      adam_optimizer {\n        learning_rate {\n          constant_learning_rate {\n            learning_rate: 0.01\n          }\n        }\n      }\n    }\n    data_augmentation_options {\n      random_adjust_brightness {\n        max_delta: 0.2\n      }\n    }\n    data_augmentation_options {\n      random_adjust_contrast {\n        min_delta: 0.7\n        max_delta: 1.1\n      }\n    }\n    num_steps: 2\n    """"""\n    train_config = train_pb2.TrainConfig()\n    text_format.Merge(train_config_text_proto, train_config)\n\n    train_dir = self.get_temp_dir()\n\n    trainer.train(create_tensor_dict_fn=get_input_function,\n                  create_model_fn=FakeDetectionModel,\n                  train_config=train_config,\n                  master=\'\',\n                  task=0,\n                  num_clones=1,\n                  worker_replicas=1,\n                  clone_on_cpu=True,\n                  ps_tasks=0,\n                  worker_job_name=\'worker\',\n                  is_chief=True,\n                  train_dir=train_dir)\n\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
src/object_detection/anchor_generators/__init__.py,0,b''
src/object_detection/anchor_generators/grid_anchor_generator.py,14,"b'# Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n""""""Generates grid anchors on the fly as used in Faster RCNN.\n\nGenerates grid anchors on the fly as described in:\n""Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks""\nShaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun.\n""""""\n\nimport tensorflow as tf\n\nfrom object_detection.core import anchor_generator\nfrom object_detection.core import box_list\nfrom object_detection.utils import ops\n\n\nclass GridAnchorGenerator(anchor_generator.AnchorGenerator):\n  """"""Generates a grid of anchors at given scales and aspect ratios.""""""\n\n  def __init__(self,\n               scales=(0.5, 1.0, 2.0),\n               aspect_ratios=(0.5, 1.0, 2.0),\n               base_anchor_size=None,\n               anchor_stride=None,\n               anchor_offset=None):\n    """"""Constructs a GridAnchorGenerator.\n\n    Args:\n      scales: a list of (float) scales, default=(0.5, 1.0, 2.0)\n      aspect_ratios: a list of (float) aspect ratios, default=(0.5, 1.0, 2.0)\n      base_anchor_size: base anchor size as height, width (\n                        (length-2 float32 list or tensor, default=[256, 256])\n      anchor_stride: difference in centers between base anchors for adjacent\n                     grid positions (length-2 float32 list or tensor,\n                     default=[16, 16])\n      anchor_offset: center of the anchor with scale and aspect ratio 1 for the\n                     upper left element of the grid, this should be zero for\n                     feature networks with only VALID padding and even receptive\n                     field size, but may need additional calculation if other\n                     padding is used (length-2 float32 list or tensor,\n                     default=[0, 0])\n    """"""\n    # Handle argument defaults\n    if base_anchor_size is None:\n      base_anchor_size = [256, 256]\n    base_anchor_size = tf.to_float(tf.convert_to_tensor(base_anchor_size))\n    if anchor_stride is None:\n      anchor_stride = [16, 16]\n    anchor_stride = tf.to_float(tf.convert_to_tensor(anchor_stride))\n    if anchor_offset is None:\n      anchor_offset = [0, 0]\n    anchor_offset = tf.to_float(tf.convert_to_tensor(anchor_offset))\n\n    self._scales = scales\n    self._aspect_ratios = aspect_ratios\n    self._base_anchor_size = base_anchor_size\n    self._anchor_stride = anchor_stride\n    self._anchor_offset = anchor_offset\n\n  def name_scope(self):\n    return \'GridAnchorGenerator\'\n\n  def num_anchors_per_location(self):\n    """"""Returns the number of anchors per spatial location.\n\n    Returns:\n      a list of integers, one for each expected feature map to be passed to\n      the `generate` function.\n    """"""\n    return [len(self._scales) * len(self._aspect_ratios)]\n\n  def _generate(self, feature_map_shape_list):\n    """"""Generates a collection of bounding boxes to be used as anchors.\n\n    Args:\n      feature_map_shape_list: list of pairs of convnet layer resolutions in the\n        format [(height_0, width_0)].  For example, setting\n        feature_map_shape_list=[(8, 8)] asks for anchors that correspond\n        to an 8x8 layer.  For this anchor generator, only lists of length 1 are\n        allowed.\n\n    Returns:\n      boxes_list: a list of BoxLists each holding anchor boxes corresponding to\n        the input feature map shapes.\n\n    Raises:\n      ValueError: if feature_map_shape_list, box_specs_list do not have the same\n        length.\n      ValueError: if feature_map_shape_list does not consist of pairs of\n        integers\n    """"""\n    if not (isinstance(feature_map_shape_list, list)\n            and len(feature_map_shape_list) == 1):\n      raise ValueError(\'feature_map_shape_list must be a list of length 1.\')\n    if not all([isinstance(list_item, tuple) and len(list_item) == 2\n                for list_item in feature_map_shape_list]):\n      raise ValueError(\'feature_map_shape_list must be a list of pairs.\')\n    grid_height, grid_width = feature_map_shape_list[0]\n    scales_grid, aspect_ratios_grid = ops.meshgrid(self._scales,\n                                                   self._aspect_ratios)\n    scales_grid = tf.reshape(scales_grid, [-1])\n    aspect_ratios_grid = tf.reshape(aspect_ratios_grid, [-1])\n    anchors = tile_anchors(grid_height,\n                           grid_width,\n                           scales_grid,\n                           aspect_ratios_grid,\n                           self._base_anchor_size,\n                           self._anchor_stride,\n                           self._anchor_offset)\n\n    num_anchors = anchors.num_boxes_static()\n    if num_anchors is None:\n      num_anchors = anchors.num_boxes()\n    anchor_indices = tf.zeros([num_anchors])\n    anchors.add_field(\'feature_map_index\', anchor_indices)\n    return [anchors]\n\n\ndef tile_anchors(grid_height,\n                 grid_width,\n                 scales,\n                 aspect_ratios,\n                 base_anchor_size,\n                 anchor_stride,\n                 anchor_offset):\n  """"""Create a tiled set of anchors strided along a grid in image space.\n\n  This op creates a set of anchor boxes by placing a ""basis"" collection of\n  boxes with user-specified scales and aspect ratios centered at evenly\n  distributed points along a grid.  The basis collection is specified via the\n  scale and aspect_ratios arguments.  For example, setting scales=[.1, .2, .2]\n  and aspect ratios = [2,2,1/2] means that we create three boxes: one with scale\n  .1, aspect ratio 2, one with scale .2, aspect ratio 2, and one with scale .2\n  and aspect ratio 1/2.  Each box is multiplied by ""base_anchor_size"" before\n  placing it over its respective center.\n\n  Grid points are specified via grid_height, grid_width parameters as well as\n  the anchor_stride and anchor_offset parameters.\n\n  Args:\n    grid_height: size of the grid in the y direction (int or int scalar tensor)\n    grid_width: size of the grid in the x direction (int or int scalar tensor)\n    scales: a 1-d  (float) tensor representing the scale of each box in the\n      basis set.\n    aspect_ratios: a 1-d (float) tensor representing the aspect ratio of each\n      box in the basis set.  The length of the scales and aspect_ratios tensors\n      must be equal.\n    base_anchor_size: base anchor size as [height, width]\n      (float tensor of shape [2])\n    anchor_stride: difference in centers between base anchors for adjacent grid\n                   positions (float tensor of shape [2])\n    anchor_offset: center of the anchor with scale and aspect ratio 1 for the\n                   upper left element of the grid, this should be zero for\n                   feature networks with only VALID padding and even receptive\n                   field size, but may need some additional calculation if other\n                   padding is used (float tensor of shape [2])\n  Returns:\n    a BoxList holding a collection of N anchor boxes\n  """"""\n  ratio_sqrts = tf.sqrt(aspect_ratios)\n  heights = scales / ratio_sqrts * base_anchor_size[0]\n  widths = scales * ratio_sqrts * base_anchor_size[1]\n\n  # Get a grid of box centers\n  y_centers = tf.to_float(tf.range(grid_height))\n  y_centers = y_centers * anchor_stride[0] + anchor_offset[0]\n  x_centers = tf.to_float(tf.range(grid_width))\n  x_centers = x_centers * anchor_stride[1] + anchor_offset[1]\n  x_centers, y_centers = ops.meshgrid(x_centers, y_centers)\n\n  widths_grid, x_centers_grid = ops.meshgrid(widths, x_centers)\n  heights_grid, y_centers_grid = ops.meshgrid(heights, y_centers)\n  bbox_centers = tf.stack([y_centers_grid, x_centers_grid], axis=3)\n  bbox_sizes = tf.stack([heights_grid, widths_grid], axis=3)\n  bbox_centers = tf.reshape(bbox_centers, [-1, 2])\n  bbox_sizes = tf.reshape(bbox_sizes, [-1, 2])\n  bbox_corners = _center_size_bbox_to_corners_bbox(bbox_centers, bbox_sizes)\n  return box_list.BoxList(bbox_corners)\n\n\ndef _center_size_bbox_to_corners_bbox(centers, sizes):\n  """"""Converts bbox center-size representation to corners representation.\n\n  Args:\n    centers: a tensor with shape [N, 2] representing bounding box centers\n    sizes: a tensor with shape [N, 2] representing bounding boxes\n\n  Returns:\n    corners: tensor with shape [N, 4] representing bounding boxes in corners\n      representation\n  """"""\n  return tf.concat([centers - .5 * sizes, centers + .5 * sizes], 1)\n'"
src/object_detection/anchor_generators/grid_anchor_generator_test.py,1,"b'# Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n""""""Tests for object_detection.grid_anchor_generator.""""""\nimport numpy as np\nimport tensorflow as tf\n\nfrom object_detection.anchor_generators import grid_anchor_generator\nfrom object_detection.utils import test_case\n\n\nclass GridAnchorGeneratorTest(test_case.TestCase):\n\n  def test_construct_single_anchor(self):\n    """"""Builds a 1x1 anchor grid to test the size of the output boxes.""""""\n    def graph_fn():\n      scales = [0.5, 1.0, 2.0]\n      aspect_ratios = [0.25, 1.0, 4.0]\n      anchor_offset = [7, -3]\n      anchor_generator = grid_anchor_generator.GridAnchorGenerator(\n          scales, aspect_ratios, anchor_offset=anchor_offset)\n      anchors_list = anchor_generator.generate(feature_map_shape_list=[(1, 1)])\n      anchor_corners = anchors_list[0].get()\n      return (anchor_corners,)\n    exp_anchor_corners = [[-121, -35, 135, 29], [-249, -67, 263, 61],\n                          [-505, -131, 519, 125], [-57, -67, 71, 61],\n                          [-121, -131, 135, 125], [-249, -259, 263, 253],\n                          [-25, -131, 39, 125], [-57, -259, 71, 253],\n                          [-121, -515, 135, 509]]\n    anchor_corners_out = self.execute(graph_fn, [])\n    self.assertAllClose(anchor_corners_out, exp_anchor_corners)\n\n  def test_construct_anchor_grid(self):\n    def graph_fn():\n      base_anchor_size = [10, 10]\n      anchor_stride = [19, 19]\n      anchor_offset = [0, 0]\n      scales = [0.5, 1.0, 2.0]\n      aspect_ratios = [1.0]\n\n      anchor_generator = grid_anchor_generator.GridAnchorGenerator(\n          scales,\n          aspect_ratios,\n          base_anchor_size=base_anchor_size,\n          anchor_stride=anchor_stride,\n          anchor_offset=anchor_offset)\n\n      anchors_list = anchor_generator.generate(feature_map_shape_list=[(2, 2)])\n      anchor_corners = anchors_list[0].get()\n      return (anchor_corners,)\n    exp_anchor_corners = [[-2.5, -2.5, 2.5, 2.5], [-5., -5., 5., 5.],\n                          [-10., -10., 10., 10.], [-2.5, 16.5, 2.5, 21.5],\n                          [-5., 14., 5, 24], [-10., 9., 10, 29],\n                          [16.5, -2.5, 21.5, 2.5], [14., -5., 24, 5],\n                          [9., -10., 29, 10], [16.5, 16.5, 21.5, 21.5],\n                          [14., 14., 24, 24], [9., 9., 29, 29]]\n    anchor_corners_out = self.execute(graph_fn, [])\n    self.assertAllClose(anchor_corners_out, exp_anchor_corners)\n\n  def test_construct_anchor_grid_with_dynamic_feature_map_shapes(self):\n    def graph_fn(feature_map_height, feature_map_width):\n      base_anchor_size = [10, 10]\n      anchor_stride = [19, 19]\n      anchor_offset = [0, 0]\n      scales = [0.5, 1.0, 2.0]\n      aspect_ratios = [1.0]\n      anchor_generator = grid_anchor_generator.GridAnchorGenerator(\n          scales,\n          aspect_ratios,\n          base_anchor_size=base_anchor_size,\n          anchor_stride=anchor_stride,\n          anchor_offset=anchor_offset)\n\n      anchors_list = anchor_generator.generate(\n          feature_map_shape_list=[(feature_map_height, feature_map_width)])\n      anchor_corners = anchors_list[0].get()\n      return (anchor_corners,)\n\n    exp_anchor_corners = [[-2.5, -2.5, 2.5, 2.5], [-5., -5., 5., 5.],\n                          [-10., -10., 10., 10.], [-2.5, 16.5, 2.5, 21.5],\n                          [-5., 14., 5, 24], [-10., 9., 10, 29],\n                          [16.5, -2.5, 21.5, 2.5], [14., -5., 24, 5],\n                          [9., -10., 29, 10], [16.5, 16.5, 21.5, 21.5],\n                          [14., 14., 24, 24], [9., 9., 29, 29]]\n    anchor_corners_out = self.execute_cpu(graph_fn,\n                                          [np.array(2, dtype=np.int32),\n                                           np.array(2, dtype=np.int32)])\n    self.assertAllClose(anchor_corners_out, exp_anchor_corners)\n\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
src/object_detection/anchor_generators/multiple_grid_anchor_generator.py,11,"b'# Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n""""""Generates grid anchors on the fly corresponding to multiple CNN layers.\n\nGenerates grid anchors on the fly corresponding to multiple CNN layers as\ndescribed in:\n""SSD: Single Shot MultiBox Detector""\nWei Liu, Dragomir Anguelov, Dumitru Erhan, Christian Szegedy, Scott Reed,\nCheng-Yang Fu, Alexander C. Berg\n(see Section 2.2: Choosing scales and aspect ratios for default boxes)\n""""""\n\nimport numpy as np\n\nimport tensorflow as tf\n\nfrom object_detection.anchor_generators import grid_anchor_generator\nfrom object_detection.core import anchor_generator\nfrom object_detection.core import box_list_ops\n\n\nclass MultipleGridAnchorGenerator(anchor_generator.AnchorGenerator):\n  """"""Generate a grid of anchors for multiple CNN layers.""""""\n\n  def __init__(self,\n               box_specs_list,\n               base_anchor_size=None,\n               anchor_strides=None,\n               anchor_offsets=None,\n               clip_window=None):\n    """"""Constructs a MultipleGridAnchorGenerator.\n\n    To construct anchors, at multiple grid resolutions, one must provide a\n    list of feature_map_shape_list (e.g., [(8, 8), (4, 4)]), and for each grid\n    size, a corresponding list of (scale, aspect ratio) box specifications.\n\n    For example:\n    box_specs_list = [[(.1, 1.0), (.1, 2.0)],  # for 8x8 grid\n                      [(.2, 1.0), (.3, 1.0), (.2, 2.0)]]  # for 4x4 grid\n\n    To support the fully convolutional setting, we pass grid sizes in at\n    generation time, while scale and aspect ratios are fixed at construction\n    time.\n\n    Args:\n      box_specs_list: list of list of (scale, aspect ratio) pairs with the\n        outside list having the same number of entries as feature_map_shape_list\n        (which is passed in at generation time).\n      base_anchor_size: base anchor size as [height, width]\n                        (length-2 float tensor, default=[1.0, 1.0]).\n                        The height and width values are normalized to the\n                        minimum dimension of the input height and width, so that\n                        when the base anchor height equals the base anchor\n                        width, the resulting anchor is square even if the input\n                        image is not square.\n      anchor_strides: list of pairs of strides in pixels (in y and x directions\n        respectively). For example, setting anchor_strides=[(25, 25), (50, 50)]\n        means that we want the anchors corresponding to the first layer to be\n        strided by 25 pixels and those in the second layer to be strided by 50\n        pixels in both y and x directions. If anchor_strides=None, they are set\n        to be the reciprocal of the corresponding feature map shapes.\n      anchor_offsets: list of pairs of offsets in pixels (in y and x directions\n        respectively). The offset specifies where we want the center of the\n        (0, 0)-th anchor to lie for each layer. For example, setting\n        anchor_offsets=[(10, 10), (20, 20)]) means that we want the\n        (0, 0)-th anchor of the first layer to lie at (10, 10) in pixel space\n        and likewise that we want the (0, 0)-th anchor of the second layer to\n        lie at (25, 25) in pixel space. If anchor_offsets=None, then they are\n        set to be half of the corresponding anchor stride.\n      clip_window: a tensor of shape [4] specifying a window to which all\n        anchors should be clipped. If clip_window is None, then no clipping\n        is performed.\n\n    Raises:\n      ValueError: if box_specs_list is not a list of list of pairs\n      ValueError: if clip_window is not either None or a tensor of shape [4]\n    """"""\n    if isinstance(box_specs_list, list) and all(\n        [isinstance(list_item, list) for list_item in box_specs_list]):\n      self._box_specs = box_specs_list\n    else:\n      raise ValueError(\'box_specs_list is expected to be a \'\n                       \'list of lists of pairs\')\n    if base_anchor_size is None:\n      base_anchor_size = tf.constant([256, 256], dtype=tf.float32)\n    self._base_anchor_size = base_anchor_size\n    self._anchor_strides = anchor_strides\n    self._anchor_offsets = anchor_offsets\n    if clip_window is not None and clip_window.get_shape().as_list() != [4]:\n      raise ValueError(\'clip_window must either be None or a shape [4] tensor\')\n    self._clip_window = clip_window\n    self._scales = []\n    self._aspect_ratios = []\n    for box_spec in self._box_specs:\n      if not all([isinstance(entry, tuple) and len(entry) == 2\n                  for entry in box_spec]):\n        raise ValueError(\'box_specs_list is expected to be a \'\n                         \'list of lists of pairs\')\n      scales, aspect_ratios = zip(*box_spec)\n      self._scales.append(scales)\n      self._aspect_ratios.append(aspect_ratios)\n\n    for arg, arg_name in zip([self._anchor_strides, self._anchor_offsets],\n                             [\'anchor_strides\', \'anchor_offsets\']):\n      if arg and not (isinstance(arg, list) and\n                      len(arg) == len(self._box_specs)):\n        raise ValueError(\'%s must be a list with the same length \'\n                         \'as self._box_specs\' % arg_name)\n      if arg and not all([\n          isinstance(list_item, tuple) and len(list_item) == 2\n          for list_item in arg\n      ]):\n        raise ValueError(\'%s must be a list of pairs.\' % arg_name)\n\n  def name_scope(self):\n    return \'MultipleGridAnchorGenerator\'\n\n  def num_anchors_per_location(self):\n    """"""Returns the number of anchors per spatial location.\n\n    Returns:\n      a list of integers, one for each expected feature map to be passed to\n      the Generate function.\n    """"""\n    return [len(box_specs) for box_specs in self._box_specs]\n\n  def _generate(self, feature_map_shape_list, im_height=1, im_width=1):\n    """"""Generates a collection of bounding boxes to be used as anchors.\n\n    The number of anchors generated for a single grid with shape MxM where we\n    place k boxes over each grid center is k*M^2 and thus the total number of\n    anchors is the sum over all grids. In our box_specs_list example\n    (see the constructor docstring), we would place two boxes over each grid\n    point on an 8x8 grid and three boxes over each grid point on a 4x4 grid and\n    thus end up with 2*8^2 + 3*4^2 = 176 anchors in total. The layout of the\n    output anchors follows the order of how the grid sizes and box_specs are\n    specified (with box_spec index varying the fastest, followed by width\n    index, then height index, then grid index).\n\n    Args:\n      feature_map_shape_list: list of pairs of convnet layer resolutions in the\n        format [(height_0, width_0), (height_1, width_1), ...]. For example,\n        setting feature_map_shape_list=[(8, 8), (7, 7)] asks for anchors that\n        correspond to an 8x8 layer followed by a 7x7 layer.\n      im_height: the height of the image to generate the grid for. If both\n        im_height and im_width are 1, the generated anchors default to\n        normalized coordinates, otherwise absolute coordinates are used for the\n        grid.\n      im_width: the width of the image to generate the grid for. If both\n        im_height and im_width are 1, the generated anchors default to\n        normalized coordinates, otherwise absolute coordinates are used for the\n        grid.\n\n    Returns:\n      boxes_list: a list of BoxLists each holding anchor boxes corresponding to\n        the input feature map shapes.\n\n    Raises:\n      ValueError: if feature_map_shape_list, box_specs_list do not have the same\n        length.\n      ValueError: if feature_map_shape_list does not consist of pairs of\n        integers\n    """"""\n    if not (isinstance(feature_map_shape_list, list)\n            and len(feature_map_shape_list) == len(self._box_specs)):\n      raise ValueError(\'feature_map_shape_list must be a list with the same \'\n                       \'length as self._box_specs\')\n    if not all([isinstance(list_item, tuple) and len(list_item) == 2\n                for list_item in feature_map_shape_list]):\n      raise ValueError(\'feature_map_shape_list must be a list of pairs.\')\n\n    im_height = tf.to_float(im_height)\n    im_width = tf.to_float(im_width)\n\n    if not self._anchor_strides:\n      anchor_strides = [(1.0 / tf.to_float(pair[0]), 1.0 / tf.to_float(pair[1]))\n                        for pair in feature_map_shape_list]\n    else:\n      anchor_strides = [(tf.to_float(stride[0]) / im_height,\n                         tf.to_float(stride[1]) / im_width)\n                        for stride in self._anchor_strides]\n    if not self._anchor_offsets:\n      anchor_offsets = [(0.5 * stride[0], 0.5 * stride[1])\n                        for stride in anchor_strides]\n    else:\n      anchor_offsets = [(tf.to_float(offset[0]) / im_height,\n                         tf.to_float(offset[1]) / im_width)\n                        for offset in self._anchor_offsets]\n\n    for arg, arg_name in zip([anchor_strides, anchor_offsets],\n                             [\'anchor_strides\', \'anchor_offsets\']):\n      if not (isinstance(arg, list) and len(arg) == len(self._box_specs)):\n        raise ValueError(\'%s must be a list with the same length \'\n                         \'as self._box_specs\' % arg_name)\n      if not all([isinstance(list_item, tuple) and len(list_item) == 2\n                  for list_item in arg]):\n        raise ValueError(\'%s must be a list of pairs.\' % arg_name)\n\n    anchor_grid_list = []\n    min_im_shape = tf.minimum(im_height, im_width)\n    scale_height = min_im_shape / im_height\n    scale_width = min_im_shape / im_width\n    base_anchor_size = [\n        scale_height * self._base_anchor_size[0],\n        scale_width * self._base_anchor_size[1]\n    ]\n    for feature_map_index, (grid_size, scales, aspect_ratios, stride,\n                            offset) in enumerate(\n                                zip(feature_map_shape_list, self._scales,\n                                    self._aspect_ratios, anchor_strides,\n                                    anchor_offsets)):\n      tiled_anchors = grid_anchor_generator.tile_anchors(\n          grid_height=grid_size[0],\n          grid_width=grid_size[1],\n          scales=scales,\n          aspect_ratios=aspect_ratios,\n          base_anchor_size=base_anchor_size,\n          anchor_stride=stride,\n          anchor_offset=offset)\n      if self._clip_window is not None:\n        tiled_anchors = box_list_ops.clip_to_window(\n            tiled_anchors, self._clip_window, filter_nonoverlapping=False)\n      num_anchors_in_layer = tiled_anchors.num_boxes_static()\n      if num_anchors_in_layer is None:\n        num_anchors_in_layer = tiled_anchors.num_boxes()\n      anchor_indices = feature_map_index * tf.ones([num_anchors_in_layer])\n      tiled_anchors.add_field(\'feature_map_index\', anchor_indices)\n      anchor_grid_list.append(tiled_anchors)\n\n    return anchor_grid_list\n\n\ndef create_ssd_anchors(num_layers=6,\n                       min_scale=0.2,\n                       max_scale=0.95,\n                       scales=None,\n                       aspect_ratios=(1.0, 2.0, 3.0, 1.0 / 2, 1.0 / 3),\n                       interpolated_scale_aspect_ratio=1.0,\n                       base_anchor_size=None,\n                       anchor_strides=None,\n                       anchor_offsets=None,\n                       reduce_boxes_in_lowest_layer=True):\n  """"""Creates MultipleGridAnchorGenerator for SSD anchors.\n\n  This function instantiates a MultipleGridAnchorGenerator that reproduces\n  ``default box`` construction proposed by Liu et al in the SSD paper.\n  See Section 2.2 for details. Grid sizes are assumed to be passed in\n  at generation time from finest resolution to coarsest resolution --- this is\n  used to (linearly) interpolate scales of anchor boxes corresponding to the\n  intermediate grid sizes.\n\n  Anchors that are returned by calling the `generate` method on the returned\n  MultipleGridAnchorGenerator object are always in normalized coordinates\n  and clipped to the unit square: (i.e. all coordinates lie in [0, 1]x[0, 1]).\n\n  Args:\n    num_layers: integer number of grid layers to create anchors for (actual\n      grid sizes passed in at generation time)\n    min_scale: scale of anchors corresponding to finest resolution (float)\n    max_scale: scale of anchors corresponding to coarsest resolution (float)\n    scales: As list of anchor scales to use. When not None and not empty,\n      min_scale and max_scale are not used.\n    aspect_ratios: list or tuple of (float) aspect ratios to place on each\n      grid point.\n    interpolated_scale_aspect_ratio: An additional anchor is added with this\n      aspect ratio and a scale interpolated between the scale for a layer\n      and the scale for the next layer (1.0 for the last layer).\n      This anchor is not included if this value is 0.\n    base_anchor_size: base anchor size as [height, width].\n      The height and width values are normalized to the minimum dimension of the\n      input height and width, so that when the base anchor height equals the\n      base anchor width, the resulting anchor is square even if the input image\n      is not square.\n    anchor_strides: list of pairs of strides in pixels (in y and x directions\n      respectively). For example, setting anchor_strides=[(25, 25), (50, 50)]\n      means that we want the anchors corresponding to the first layer to be\n      strided by 25 pixels and those in the second layer to be strided by 50\n      pixels in both y and x directions. If anchor_strides=None, they are set to\n      be the reciprocal of the corresponding feature map shapes.\n    anchor_offsets: list of pairs of offsets in pixels (in y and x directions\n      respectively). The offset specifies where we want the center of the\n      (0, 0)-th anchor to lie for each layer. For example, setting\n      anchor_offsets=[(10, 10), (20, 20)]) means that we want the\n      (0, 0)-th anchor of the first layer to lie at (10, 10) in pixel space\n      and likewise that we want the (0, 0)-th anchor of the second layer to lie\n      at (25, 25) in pixel space. If anchor_offsets=None, then they are set to\n      be half of the corresponding anchor stride.\n    reduce_boxes_in_lowest_layer: a boolean to indicate whether the fixed 3\n      boxes per location is used in the lowest layer.\n\n  Returns:\n    a MultipleGridAnchorGenerator\n  """"""\n  if base_anchor_size is None:\n    base_anchor_size = [1.0, 1.0]\n  base_anchor_size = tf.constant(base_anchor_size, dtype=tf.float32)\n  box_specs_list = []\n  if scales is None or not scales:\n    scales = [min_scale + (max_scale - min_scale) * i / (num_layers - 1)\n              for i in range(num_layers)] + [1.0]\n  else:\n    # Add 1.0 to the end, which will only be used in scale_next below and used\n    # for computing an interpolated scale for the largest scale in the list.\n    scales += [1.0]\n\n  for layer, scale, scale_next in zip(\n      range(num_layers), scales[:-1], scales[1:]):\n    layer_box_specs = []\n    if layer == 0 and reduce_boxes_in_lowest_layer:\n      layer_box_specs = [(0.1, 1.0), (scale, 2.0), (scale, 0.5)]\n    else:\n      for aspect_ratio in aspect_ratios:\n        layer_box_specs.append((scale, aspect_ratio))\n      # Add one more anchor, with a scale between the current scale, and the\n      # scale for the next layer, with a specified aspect ratio (1.0 by\n      # default).\n      if interpolated_scale_aspect_ratio > 0.0:\n        layer_box_specs.append((np.sqrt(scale*scale_next),\n                                interpolated_scale_aspect_ratio))\n    box_specs_list.append(layer_box_specs)\n\n  return MultipleGridAnchorGenerator(box_specs_list, base_anchor_size,\n                                     anchor_strides, anchor_offsets)\n'"
src/object_detection/anchor_generators/multiple_grid_anchor_generator_test.py,21,"b'# Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n""""""Tests for anchor_generators.multiple_grid_anchor_generator_test.py.""""""\n\nimport numpy as np\n\nimport tensorflow as tf\n\nfrom object_detection.anchor_generators import multiple_grid_anchor_generator as ag\nfrom object_detection.utils import test_case\n\n\nclass MultipleGridAnchorGeneratorTest(test_case.TestCase):\n\n  def test_construct_single_anchor_grid(self):\n    """"""Builds a 1x1 anchor grid to test the size of the output boxes.""""""\n    def graph_fn():\n\n      box_specs_list = [[(.5, .25), (1.0, .25), (2.0, .25),\n                         (.5, 1.0), (1.0, 1.0), (2.0, 1.0),\n                         (.5, 4.0), (1.0, 4.0), (2.0, 4.0)]]\n      anchor_generator = ag.MultipleGridAnchorGenerator(\n          box_specs_list,\n          base_anchor_size=tf.constant([256, 256], dtype=tf.float32),\n          anchor_strides=[(16, 16)],\n          anchor_offsets=[(7, -3)])\n      anchors_list = anchor_generator.generate(feature_map_shape_list=[(1, 1)])\n      return anchors_list[0].get()\n    exp_anchor_corners = [[-121, -35, 135, 29], [-249, -67, 263, 61],\n                          [-505, -131, 519, 125], [-57, -67, 71, 61],\n                          [-121, -131, 135, 125], [-249, -259, 263, 253],\n                          [-25, -131, 39, 125], [-57, -259, 71, 253],\n                          [-121, -515, 135, 509]]\n\n    anchor_corners_out = self.execute(graph_fn, [])\n    self.assertAllClose(anchor_corners_out, exp_anchor_corners)\n\n  def test_construct_anchor_grid(self):\n    def graph_fn():\n      box_specs_list = [[(0.5, 1.0), (1.0, 1.0), (2.0, 1.0)]]\n\n      anchor_generator = ag.MultipleGridAnchorGenerator(\n          box_specs_list,\n          base_anchor_size=tf.constant([10, 10], dtype=tf.float32),\n          anchor_strides=[(19, 19)],\n          anchor_offsets=[(0, 0)])\n      anchors_list = anchor_generator.generate(feature_map_shape_list=[(2, 2)])\n      return anchors_list[0].get()\n    exp_anchor_corners = [[-2.5, -2.5, 2.5, 2.5], [-5., -5., 5., 5.],\n                          [-10., -10., 10., 10.], [-2.5, 16.5, 2.5, 21.5],\n                          [-5., 14., 5, 24], [-10., 9., 10, 29],\n                          [16.5, -2.5, 21.5, 2.5], [14., -5., 24, 5],\n                          [9., -10., 29, 10], [16.5, 16.5, 21.5, 21.5],\n                          [14., 14., 24, 24], [9., 9., 29, 29]]\n\n    anchor_corners_out = self.execute(graph_fn, [])\n    self.assertAllClose(anchor_corners_out, exp_anchor_corners)\n\n  def test_construct_anchor_grid_non_square(self):\n\n    def graph_fn():\n      box_specs_list = [[(1.0, 1.0)]]\n      anchor_generator = ag.MultipleGridAnchorGenerator(\n          box_specs_list, base_anchor_size=tf.constant([1, 1],\n                                                       dtype=tf.float32))\n      anchors_list = anchor_generator.generate(feature_map_shape_list=[(\n          tf.constant(1, dtype=tf.int32), tf.constant(2, dtype=tf.int32))])\n      return anchors_list[0].get()\n\n    exp_anchor_corners = [[0., -0.25, 1., 0.75], [0., 0.25, 1., 1.25]]\n    anchor_corners_out = self.execute(graph_fn, [])\n    self.assertAllClose(anchor_corners_out, exp_anchor_corners)\n\n  def test_construct_dynamic_size_anchor_grid(self):\n\n    def graph_fn(height, width):\n      box_specs_list = [[(1.0, 1.0)]]\n      anchor_generator = ag.MultipleGridAnchorGenerator(\n          box_specs_list, base_anchor_size=tf.constant([1, 1],\n                                                       dtype=tf.float32))\n      anchors_list = anchor_generator.generate(feature_map_shape_list=[(height,\n                                                                        width)])\n      return anchors_list[0].get()\n\n    exp_anchor_corners = [[0., -0.25, 1., 0.75], [0., 0.25, 1., 1.25]]\n\n    anchor_corners_out = self.execute_cpu(graph_fn,\n                                          [np.array(1, dtype=np.int32),\n                                           np.array(2, dtype=np.int32)])\n    self.assertAllClose(anchor_corners_out, exp_anchor_corners)\n\n  def test_construct_anchor_grid_normalized(self):\n    def graph_fn():\n      box_specs_list = [[(1.0, 1.0)]]\n\n      anchor_generator = ag.MultipleGridAnchorGenerator(\n          box_specs_list, base_anchor_size=tf.constant([1, 1],\n                                                       dtype=tf.float32))\n      anchors_list = anchor_generator.generate(\n          feature_map_shape_list=[(tf.constant(1, dtype=tf.int32), tf.constant(\n              2, dtype=tf.int32))],\n          im_height=320,\n          im_width=640)\n      return anchors_list[0].get()\n\n    exp_anchor_corners = [[0., 0., 1., 0.5], [0., 0.5, 1., 1.]]\n    anchor_corners_out = self.execute(graph_fn, [])\n    self.assertAllClose(anchor_corners_out, exp_anchor_corners)\n\n  def test_construct_multiple_grids(self):\n\n    def graph_fn():\n      box_specs_list = [[(1.0, 1.0), (2.0, 1.0), (1.0, 0.5)],\n                        [(1.0, 1.0), (1.0, 0.5)]]\n\n      anchor_generator = ag.MultipleGridAnchorGenerator(\n          box_specs_list,\n          base_anchor_size=tf.constant([1.0, 1.0], dtype=tf.float32),\n          anchor_strides=[(.25, .25), (.5, .5)],\n          anchor_offsets=[(.125, .125), (.25, .25)])\n      anchors_list = anchor_generator.generate(feature_map_shape_list=[(4, 4), (\n          2, 2)])\n      return [anchors.get() for anchors in anchors_list]\n    # height and width of box with .5 aspect ratio\n    h = np.sqrt(2)\n    w = 1.0/np.sqrt(2)\n    exp_small_grid_corners = [[-.25, -.25, .75, .75],\n                              [.25-.5*h, .25-.5*w, .25+.5*h, .25+.5*w],\n                              [-.25, .25, .75, 1.25],\n                              [.25-.5*h, .75-.5*w, .25+.5*h, .75+.5*w],\n                              [.25, -.25, 1.25, .75],\n                              [.75-.5*h, .25-.5*w, .75+.5*h, .25+.5*w],\n                              [.25, .25, 1.25, 1.25],\n                              [.75-.5*h, .75-.5*w, .75+.5*h, .75+.5*w]]\n    # only test first entry of larger set of anchors\n    exp_big_grid_corners = [[.125-.5, .125-.5, .125+.5, .125+.5],\n                            [.125-1.0, .125-1.0, .125+1.0, .125+1.0],\n                            [.125-.5*h, .125-.5*w, .125+.5*h, .125+.5*w],]\n\n    anchor_corners_out = np.concatenate(self.execute(graph_fn, []), axis=0)\n    self.assertEquals(anchor_corners_out.shape, (56, 4))\n    big_grid_corners = anchor_corners_out[0:3, :]\n    small_grid_corners = anchor_corners_out[48:, :]\n    self.assertAllClose(small_grid_corners, exp_small_grid_corners)\n    self.assertAllClose(big_grid_corners, exp_big_grid_corners)\n\n  def test_construct_multiple_grids_with_clipping(self):\n\n    def graph_fn():\n      box_specs_list = [[(1.0, 1.0), (2.0, 1.0), (1.0, 0.5)],\n                        [(1.0, 1.0), (1.0, 0.5)]]\n\n      clip_window = tf.constant([0, 0, 1, 1], dtype=tf.float32)\n      anchor_generator = ag.MultipleGridAnchorGenerator(\n          box_specs_list,\n          base_anchor_size=tf.constant([1.0, 1.0], dtype=tf.float32),\n          clip_window=clip_window)\n      anchors_list = anchor_generator.generate(feature_map_shape_list=[(4, 4), (\n          2, 2)])\n      return [anchors.get() for anchors in anchors_list]\n    # height and width of box with .5 aspect ratio\n    h = np.sqrt(2)\n    w = 1.0/np.sqrt(2)\n    exp_small_grid_corners = [[0, 0, .75, .75],\n                              [0, 0, .25+.5*h, .25+.5*w],\n                              [0, .25, .75, 1],\n                              [0, .75-.5*w, .25+.5*h, 1],\n                              [.25, 0, 1, .75],\n                              [.75-.5*h, 0, 1, .25+.5*w],\n                              [.25, .25, 1, 1],\n                              [.75-.5*h, .75-.5*w, 1, 1]]\n\n    anchor_corners_out = np.concatenate(self.execute(graph_fn, []), axis=0)\n    small_grid_corners = anchor_corners_out[48:, :]\n    self.assertAllClose(small_grid_corners, exp_small_grid_corners)\n\n  def test_invalid_box_specs(self):\n    # not all box specs are pairs\n    box_specs_list = [[(1.0, 1.0), (2.0, 1.0), (1.0, 0.5)],\n                      [(1.0, 1.0), (1.0, 0.5, .3)]]\n    with self.assertRaises(ValueError):\n      ag.MultipleGridAnchorGenerator(box_specs_list)\n\n    # box_specs_list is not a list of lists\n    box_specs_list = [(1.0, 1.0), (2.0, 1.0), (1.0, 0.5)]\n    with self.assertRaises(ValueError):\n      ag.MultipleGridAnchorGenerator(box_specs_list)\n\n  def test_invalid_generate_arguments(self):\n    box_specs_list = [[(1.0, 1.0), (2.0, 1.0), (1.0, 0.5)],\n                      [(1.0, 1.0), (1.0, 0.5)]]\n\n    # incompatible lengths with box_specs_list\n    with self.assertRaises(ValueError):\n      anchor_generator = ag.MultipleGridAnchorGenerator(\n          box_specs_list,\n          base_anchor_size=tf.constant([1.0, 1.0], dtype=tf.float32),\n          anchor_strides=[(.25, .25)],\n          anchor_offsets=[(.125, .125), (.25, .25)])\n      anchor_generator.generate(feature_map_shape_list=[(4, 4), (2, 2)])\n    with self.assertRaises(ValueError):\n      anchor_generator = ag.MultipleGridAnchorGenerator(\n          box_specs_list,\n          base_anchor_size=tf.constant([1.0, 1.0], dtype=tf.float32),\n          anchor_strides=[(.25, .25), (.5, .5)],\n          anchor_offsets=[(.125, .125), (.25, .25)])\n      anchor_generator.generate(feature_map_shape_list=[(4, 4), (2, 2), (1, 1)])\n    with self.assertRaises(ValueError):\n      anchor_generator = ag.MultipleGridAnchorGenerator(\n          box_specs_list,\n          base_anchor_size=tf.constant([1.0, 1.0], dtype=tf.float32),\n          anchor_strides=[(.5, .5)],\n          anchor_offsets=[(.25, .25)])\n      anchor_generator.generate(feature_map_shape_list=[(4, 4), (2, 2)])\n\n    # not pairs\n    with self.assertRaises(ValueError):\n      anchor_generator = ag.MultipleGridAnchorGenerator(\n          box_specs_list,\n          base_anchor_size=tf.constant([1.0, 1.0], dtype=tf.float32),\n          anchor_strides=[(.25, .25), (.5, .5)],\n          anchor_offsets=[(.125, .125), (.25, .25)])\n      anchor_generator.generate(feature_map_shape_list=[(4, 4, 4), (2, 2)])\n    with self.assertRaises(ValueError):\n      anchor_generator = ag.MultipleGridAnchorGenerator(\n          box_specs_list,\n          base_anchor_size=tf.constant([1.0, 1.0], dtype=tf.float32),\n          anchor_strides=[(.25, .25, .1), (.5, .5)],\n          anchor_offsets=[(.125, .125), (.25, .25)])\n      anchor_generator.generate(feature_map_shape_list=[(4, 4), (2, 2)])\n    with self.assertRaises(ValueError):\n      anchor_generator = ag.MultipleGridAnchorGenerator(\n          box_specs_list,\n          base_anchor_size=tf.constant([1.0, 1.0], dtype=tf.float32),\n          anchor_strides=[(.25, .25), (.5, .5)],\n          anchor_offsets=[(.125, .125), (.25, .25)])\n      anchor_generator.generate(feature_map_shape_list=[(4), (2, 2)])\n\n\nclass CreateSSDAnchorsTest(test_case.TestCase):\n\n  def test_create_ssd_anchors_returns_correct_shape(self):\n\n    def graph_fn1():\n      anchor_generator = ag.create_ssd_anchors(\n          num_layers=6,\n          min_scale=0.2,\n          max_scale=0.95,\n          aspect_ratios=(1.0, 2.0, 3.0, 1.0 / 2, 1.0 / 3),\n          reduce_boxes_in_lowest_layer=True)\n\n      feature_map_shape_list = [(38, 38), (19, 19), (10, 10),\n                                (5, 5), (3, 3), (1, 1)]\n      anchors_list = anchor_generator.generate(\n          feature_map_shape_list=feature_map_shape_list)\n      return [anchors.get() for anchors in anchors_list]\n    anchor_corners_out = np.concatenate(self.execute(graph_fn1, []), axis=0)\n    self.assertEquals(anchor_corners_out.shape, (7308, 4))\n\n    def graph_fn2():\n      anchor_generator = ag.create_ssd_anchors(\n          num_layers=6, min_scale=0.2, max_scale=0.95,\n          aspect_ratios=(1.0, 2.0, 3.0, 1.0/2, 1.0/3),\n          reduce_boxes_in_lowest_layer=False)\n\n      feature_map_shape_list = [(38, 38), (19, 19), (10, 10),\n                                (5, 5), (3, 3), (1, 1)]\n      anchors_list = anchor_generator.generate(\n          feature_map_shape_list=feature_map_shape_list)\n      return [anchors.get() for anchors in anchors_list]\n    anchor_corners_out = np.concatenate(self.execute(graph_fn2, []), axis=0)\n    self.assertEquals(anchor_corners_out.shape, (11640, 4))\n\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
src/object_detection/anchor_generators/multiscale_grid_anchor_generator.py,0,"b'# Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Generates grid anchors on the fly corresponding to multiple CNN layers.\n\nGenerates grid anchors on the fly corresponding to multiple CNN layers as\ndescribed in:\n""Focal Loss for Dense Object Detection"" (https://arxiv.org/abs/1708.02002)\nT.-Y. Lin, P. Goyal, R. Girshick, K. He, P. Dollar\n""""""\n\nfrom object_detection.anchor_generators import grid_anchor_generator\nfrom object_detection.core import anchor_generator\nfrom object_detection.core import box_list_ops\n\n\nclass MultiscaleGridAnchorGenerator(anchor_generator.AnchorGenerator):\n  """"""Generate a grid of anchors for multiple CNN layers of different scale.""""""\n\n  def __init__(self, min_level, max_level, anchor_scale, aspect_ratios,\n               scales_per_octave, normalize_coordinates=True):\n    """"""Constructs a MultiscaleGridAnchorGenerator.\n\n    To construct anchors, at multiple scale resolutions, one must provide a\n    the minimum level and maximum levels on a scale pyramid. To define the size\n    of anchor, the anchor scale is provided to decide the size relatively to the\n    stride of the corresponding feature map. The generator allows one pixel\n    location on feature map maps to multiple anchors, that have different aspect\n    ratios and intermediate scales.\n\n    Args:\n      min_level: minimum level in feature pyramid.\n      max_level: maximum level in feature pyramid.\n      anchor_scale: anchor scale and feature stride define the size of the base\n        anchor on an image. For example, given a feature pyramid with strides\n        [2^3, ..., 2^7] and anchor scale 4. The base anchor size is\n        4 * [2^3, ..., 2^7].\n      aspect_ratios: list or tuple of (float) aspect ratios to place on each\n        grid point.\n      scales_per_octave: integer number of intermediate scales per scale octave.\n      normalize_coordinates: whether to produce anchors in normalized\n        coordinates. (defaults to True).\n    """"""\n    self._anchor_grid_info = []\n    self._aspect_ratios = aspect_ratios\n    self._scales_per_octave = scales_per_octave\n    self._normalize_coordinates = normalize_coordinates\n\n    for level in range(min_level, max_level + 1):\n      anchor_stride = [2**level, 2**level]\n      scales = []\n      aspects = []\n      for scale in range(scales_per_octave):\n        scales.append(2**(float(scale) / scales_per_octave))\n      for aspect_ratio in aspect_ratios:\n        aspects.append(aspect_ratio)\n      base_anchor_size = [2**level * anchor_scale, 2**level * anchor_scale]\n      self._anchor_grid_info.append({\n          \'level\': level,\n          \'info\': [scales, aspects, base_anchor_size, anchor_stride]\n      })\n\n  def name_scope(self):\n    return \'MultiscaleGridAnchorGenerator\'\n\n  def num_anchors_per_location(self):\n    """"""Returns the number of anchors per spatial location.\n\n    Returns:\n      a list of integers, one for each expected feature map to be passed to\n      the Generate function.\n    """"""\n    return len(self._anchor_grid_info) * [\n        len(self._aspect_ratios) * self._scales_per_octave]\n\n  def _generate(self, feature_map_shape_list, im_height, im_width):\n    """"""Generates a collection of bounding boxes to be used as anchors.\n\n    Currently we require the input image shape to be statically defined.  That\n    is, im_height and im_width should be integers rather than tensors.\n\n    Args:\n      feature_map_shape_list: list of pairs of convnet layer resolutions in the\n        format [(height_0, width_0), (height_1, width_1), ...]. For example,\n        setting feature_map_shape_list=[(8, 8), (7, 7)] asks for anchors that\n        correspond to an 8x8 layer followed by a 7x7 layer.\n      im_height: the height of the image to generate the grid for.\n      im_width: the width of the image to generate the grid for.\n\n    Returns:\n      boxes_list: a list of BoxLists each holding anchor boxes corresponding to\n        the input feature map shapes.\n    Raises:\n      ValueError: if im_height and im_width are not integers.\n    """"""\n    if not isinstance(im_height, int) or not isinstance(im_width, int):\n      raise ValueError(\'MultiscaleGridAnchorGenerator currently requires \'\n                       \'input image shape to be statically defined.\')\n    anchor_grid_list = []\n    for feat_shape, grid_info in zip(feature_map_shape_list,\n                                     self._anchor_grid_info):\n      # TODO(rathodv) check the feature_map_shape_list is consistent with\n      # self._anchor_grid_info\n      level = grid_info[\'level\']\n      stride = 2**level\n      scales, aspect_ratios, base_anchor_size, anchor_stride = grid_info[\'info\']\n      feat_h = feat_shape[0]\n      feat_w = feat_shape[1]\n      anchor_offset = [0, 0]\n      if im_height % 2.0**level == 0:\n        anchor_offset[0] = stride / 2.0\n      if im_width % 2.0**level == 0:\n        anchor_offset[1] = stride / 2.0\n      ag = grid_anchor_generator.GridAnchorGenerator(\n          scales,\n          aspect_ratios,\n          base_anchor_size=base_anchor_size,\n          anchor_stride=anchor_stride,\n          anchor_offset=anchor_offset)\n      (anchor_grid,) = ag.generate(feature_map_shape_list=[(feat_h, feat_w)])\n\n      if self._normalize_coordinates:\n        anchor_grid = box_list_ops.to_normalized_coordinates(\n            anchor_grid, im_height, im_width, check_range=False)\n      anchor_grid_list.append(anchor_grid)\n\n    return anchor_grid_list\n'"
src/object_detection/anchor_generators/multiscale_grid_anchor_generator_test.py,3,"b'# Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n""""""Tests for anchor_generators.multiscale_grid_anchor_generator_test.py.""""""\nimport numpy as np\nimport tensorflow as tf\n\nfrom object_detection.anchor_generators import multiscale_grid_anchor_generator as mg\nfrom object_detection.utils import test_case\n\n\nclass MultiscaleGridAnchorGeneratorTest(test_case.TestCase):\n\n  def test_construct_single_anchor(self):\n    min_level = 5\n    max_level = 5\n    anchor_scale = 4.0\n    aspect_ratios = [1.0]\n    scales_per_octave = 1\n    im_height = 64\n    im_width = 64\n    feature_map_shape_list = [(2, 2)]\n    exp_anchor_corners = [[-48, -48, 80, 80],\n                          [-48, -16, 80, 112],\n                          [-16, -48, 112, 80],\n                          [-16, -16, 112, 112]]\n    anchor_generator = mg.MultiscaleGridAnchorGenerator(\n        min_level, max_level, anchor_scale, aspect_ratios, scales_per_octave,\n        normalize_coordinates=False)\n    anchors_list = anchor_generator.generate(\n        feature_map_shape_list, im_height=im_height, im_width=im_width)\n    anchor_corners = anchors_list[0].get()\n\n    with self.test_session():\n      anchor_corners_out = anchor_corners.eval()\n      self.assertAllClose(anchor_corners_out, exp_anchor_corners)\n\n  def test_construct_single_anchor_in_normalized_coordinates(self):\n    min_level = 5\n    max_level = 5\n    anchor_scale = 4.0\n    aspect_ratios = [1.0]\n    scales_per_octave = 1\n    im_height = 64\n    im_width = 128\n    feature_map_shape_list = [(2, 2)]\n    exp_anchor_corners = [[-48./64, -48./128, 80./64, 80./128],\n                          [-48./64, -16./128, 80./64, 112./128],\n                          [-16./64, -48./128, 112./64, 80./128],\n                          [-16./64, -16./128, 112./64, 112./128]]\n    anchor_generator = mg.MultiscaleGridAnchorGenerator(\n        min_level, max_level, anchor_scale, aspect_ratios, scales_per_octave,\n        normalize_coordinates=True)\n    anchors_list = anchor_generator.generate(\n        feature_map_shape_list, im_height=im_height, im_width=im_width)\n    anchor_corners = anchors_list[0].get()\n\n    with self.test_session():\n      anchor_corners_out = anchor_corners.eval()\n      self.assertAllClose(anchor_corners_out, exp_anchor_corners)\n\n  def test_num_anchors_per_location(self):\n    min_level = 5\n    max_level = 6\n    anchor_scale = 4.0\n    aspect_ratios = [1.0, 2.0]\n    scales_per_octave = 3\n    anchor_generator = mg.MultiscaleGridAnchorGenerator(\n        min_level, max_level, anchor_scale, aspect_ratios, scales_per_octave,\n        normalize_coordinates=False)\n    self.assertEqual(anchor_generator.num_anchors_per_location(), [6, 6])\n\n  def test_construct_single_anchor_fails_with_tensor_image_size(self):\n    min_level = 5\n    max_level = 5\n    anchor_scale = 4.0\n    aspect_ratios = [1.0]\n    scales_per_octave = 1\n    im_height = tf.constant(64)\n    im_width = tf.constant(64)\n    feature_map_shape_list = [(2, 2)]\n    anchor_generator = mg.MultiscaleGridAnchorGenerator(\n        min_level, max_level, anchor_scale, aspect_ratios, scales_per_octave,\n        normalize_coordinates=False)\n    with self.assertRaises(ValueError):\n      anchor_generator.generate(\n          feature_map_shape_list, im_height=im_height, im_width=im_width)\n\n  def test_construct_single_anchor_with_odd_input_dimension(self):\n\n    def graph_fn():\n      min_level = 5\n      max_level = 5\n      anchor_scale = 4.0\n      aspect_ratios = [1.0]\n      scales_per_octave = 1\n      im_height = 65\n      im_width = 65\n      feature_map_shape_list = [(3, 3)]\n      anchor_generator = mg.MultiscaleGridAnchorGenerator(\n          min_level, max_level, anchor_scale, aspect_ratios, scales_per_octave,\n          normalize_coordinates=False)\n      anchors_list = anchor_generator.generate(\n          feature_map_shape_list, im_height=im_height, im_width=im_width)\n      anchor_corners = anchors_list[0].get()\n      return (anchor_corners,)\n    anchor_corners_out = self.execute(graph_fn, [])\n    exp_anchor_corners = [[-64, -64, 64, 64],\n                          [-64, -32, 64, 96],\n                          [-64, 0, 64, 128],\n                          [-32, -64, 96, 64],\n                          [-32, -32, 96, 96],\n                          [-32, 0, 96, 128],\n                          [0, -64, 128, 64],\n                          [0, -32, 128, 96],\n                          [0, 0, 128, 128]]\n    self.assertAllClose(anchor_corners_out, exp_anchor_corners)\n\n  def test_construct_single_anchor_on_two_feature_maps(self):\n\n    def graph_fn():\n      min_level = 5\n      max_level = 6\n      anchor_scale = 4.0\n      aspect_ratios = [1.0]\n      scales_per_octave = 1\n      im_height = 64\n      im_width = 64\n      feature_map_shape_list = [(2, 2), (1, 1)]\n      anchor_generator = mg.MultiscaleGridAnchorGenerator(\n          min_level, max_level, anchor_scale, aspect_ratios, scales_per_octave,\n          normalize_coordinates=False)\n      anchors_list = anchor_generator.generate(feature_map_shape_list,\n                                               im_height=im_height,\n                                               im_width=im_width)\n      anchor_corners = [anchors.get() for anchors in anchors_list]\n      return anchor_corners\n\n    anchor_corners_out = np.concatenate(self.execute(graph_fn, []), axis=0)\n    exp_anchor_corners = [[-48, -48, 80, 80],\n                          [-48, -16, 80, 112],\n                          [-16, -48, 112, 80],\n                          [-16, -16, 112, 112],\n                          [-96, -96, 160, 160]]\n    self.assertAllClose(anchor_corners_out, exp_anchor_corners)\n\n  def test_construct_single_anchor_with_two_scales_per_octave(self):\n\n    def graph_fn():\n      min_level = 6\n      max_level = 6\n      anchor_scale = 4.0\n      aspect_ratios = [1.0]\n      scales_per_octave = 2\n      im_height = 64\n      im_width = 64\n      feature_map_shape_list = [(1, 1)]\n\n      anchor_generator = mg.MultiscaleGridAnchorGenerator(\n          min_level, max_level, anchor_scale, aspect_ratios, scales_per_octave,\n          normalize_coordinates=False)\n      anchors_list = anchor_generator.generate(feature_map_shape_list,\n                                               im_height=im_height,\n                                               im_width=im_width)\n      anchor_corners = [anchors.get() for anchors in anchors_list]\n      return anchor_corners\n    # There are 4 set of anchors in this configuration. The order is:\n    # [[2**0.0 intermediate scale + 1.0 aspect],\n    #  [2**0.5 intermediate scale + 1.0 aspect]]\n    exp_anchor_corners = [[-96., -96., 160., 160.],\n                          [-149.0193, -149.0193, 213.0193, 213.0193]]\n\n    anchor_corners_out = self.execute(graph_fn, [])\n    self.assertAllClose(anchor_corners_out, exp_anchor_corners)\n\n  def test_construct_single_anchor_with_two_scales_per_octave_and_aspect(self):\n    def graph_fn():\n      min_level = 6\n      max_level = 6\n      anchor_scale = 4.0\n      aspect_ratios = [1.0, 2.0]\n      scales_per_octave = 2\n      im_height = 64\n      im_width = 64\n      feature_map_shape_list = [(1, 1)]\n      anchor_generator = mg.MultiscaleGridAnchorGenerator(\n          min_level, max_level, anchor_scale, aspect_ratios, scales_per_octave,\n          normalize_coordinates=False)\n      anchors_list = anchor_generator.generate(feature_map_shape_list,\n                                               im_height=im_height,\n                                               im_width=im_width)\n      anchor_corners = [anchors.get() for anchors in anchors_list]\n      return anchor_corners\n    # There are 4 set of anchors in this configuration. The order is:\n    # [[2**0.0 intermediate scale + 1.0 aspect],\n    #  [2**0.5 intermediate scale + 1.0 aspect],\n    #  [2**0.0 intermediate scale + 2.0 aspect],\n    #  [2**0.5 intermediate scale + 2.0 aspect]]\n\n    exp_anchor_corners = [[-96., -96., 160., 160.],\n                          [-149.0193, -149.0193, 213.0193, 213.0193],\n                          [-58.50967, -149.0193, 122.50967, 213.0193],\n                          [-96., -224., 160., 288.]]\n    anchor_corners_out = self.execute(graph_fn, [])\n    self.assertAllClose(anchor_corners_out, exp_anchor_corners)\n\n  def test_construct_single_anchors_on_feature_maps_with_dynamic_shape(self):\n\n    def graph_fn(feature_map1_height, feature_map1_width, feature_map2_height,\n                 feature_map2_width):\n      min_level = 5\n      max_level = 6\n      anchor_scale = 4.0\n      aspect_ratios = [1.0]\n      scales_per_octave = 1\n      im_height = 64\n      im_width = 64\n      feature_map_shape_list = [(feature_map1_height, feature_map1_width),\n                                (feature_map2_height, feature_map2_width)]\n      anchor_generator = mg.MultiscaleGridAnchorGenerator(\n          min_level, max_level, anchor_scale, aspect_ratios, scales_per_octave,\n          normalize_coordinates=False)\n      anchors_list = anchor_generator.generate(feature_map_shape_list,\n                                               im_height=im_height,\n                                               im_width=im_width)\n      anchor_corners = [anchors.get() for anchors in anchors_list]\n      return anchor_corners\n\n    anchor_corners_out = np.concatenate(\n        self.execute_cpu(graph_fn, [\n            np.array(2, dtype=np.int32),\n            np.array(2, dtype=np.int32),\n            np.array(1, dtype=np.int32),\n            np.array(1, dtype=np.int32)\n        ]),\n        axis=0)\n    exp_anchor_corners = [[-48, -48, 80, 80],\n                          [-48, -16, 80, 112],\n                          [-16, -48, 112, 80],\n                          [-16, -16, 112, 112],\n                          [-96, -96, 160, 160]]\n    self.assertAllClose(anchor_corners_out, exp_anchor_corners)\n\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
src/object_detection/box_coders/__init__.py,0,b''
src/object_detection/box_coders/faster_rcnn_box_coder.py,7,"b'# Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n""""""Faster RCNN box coder.\n\nFaster RCNN box coder follows the coding schema described below:\n  ty = (y - ya) / ha\n  tx = (x - xa) / wa\n  th = log(h / ha)\n  tw = log(w / wa)\n  where x, y, w, h denote the box\'s center coordinates, width and height\n  respectively. Similarly, xa, ya, wa, ha denote the anchor\'s center\n  coordinates, width and height. tx, ty, tw and th denote the anchor-encoded\n  center, width and height respectively.\n\n  See http://arxiv.org/abs/1506.01497 for details.\n""""""\n\nimport tensorflow as tf\n\nfrom object_detection.core import box_coder\nfrom object_detection.core import box_list\n\nEPSILON = 1e-8\n\n\nclass FasterRcnnBoxCoder(box_coder.BoxCoder):\n  """"""Faster RCNN box coder.""""""\n\n  def __init__(self, scale_factors=None):\n    """"""Constructor for FasterRcnnBoxCoder.\n\n    Args:\n      scale_factors: List of 4 positive scalars to scale ty, tx, th and tw.\n        If set to None, does not perform scaling. For Faster RCNN,\n        the open-source implementation recommends using [10.0, 10.0, 5.0, 5.0].\n    """"""\n    if scale_factors:\n      assert len(scale_factors) == 4\n      for scalar in scale_factors:\n        assert scalar > 0\n    self._scale_factors = scale_factors\n\n  @property\n  def code_size(self):\n    return 4\n\n  def _encode(self, boxes, anchors):\n    """"""Encode a box collection with respect to anchor collection.\n\n    Args:\n      boxes: BoxList holding N boxes to be encoded.\n      anchors: BoxList of anchors.\n\n    Returns:\n      a tensor representing N anchor-encoded boxes of the format\n      [ty, tx, th, tw].\n    """"""\n    # Convert anchors to the center coordinate representation.\n    ycenter_a, xcenter_a, ha, wa = anchors.get_center_coordinates_and_sizes()\n    ycenter, xcenter, h, w = boxes.get_center_coordinates_and_sizes()\n    # Avoid NaN in division and log below.\n    ha += EPSILON\n    wa += EPSILON\n    h += EPSILON\n    w += EPSILON\n\n    tx = (xcenter - xcenter_a) / wa\n    ty = (ycenter - ycenter_a) / ha\n    tw = tf.log(w / wa)\n    th = tf.log(h / ha)\n    # Scales location targets as used in paper for joint training.\n    if self._scale_factors:\n      ty *= self._scale_factors[0]\n      tx *= self._scale_factors[1]\n      th *= self._scale_factors[2]\n      tw *= self._scale_factors[3]\n    return tf.transpose(tf.stack([ty, tx, th, tw]))\n\n  def _decode(self, rel_codes, anchors):\n    """"""Decode relative codes to boxes.\n\n    Args:\n      rel_codes: a tensor representing N anchor-encoded boxes.\n      anchors: BoxList of anchors.\n\n    Returns:\n      boxes: BoxList holding N bounding boxes.\n    """"""\n    ycenter_a, xcenter_a, ha, wa = anchors.get_center_coordinates_and_sizes()\n\n    ty, tx, th, tw = tf.unstack(tf.transpose(rel_codes))\n    if self._scale_factors:\n      ty /= self._scale_factors[0]\n      tx /= self._scale_factors[1]\n      th /= self._scale_factors[2]\n      tw /= self._scale_factors[3]\n    w = tf.exp(tw) * wa\n    h = tf.exp(th) * ha\n    ycenter = ty * ha + ycenter_a\n    xcenter = tx * wa + xcenter_a\n    ymin = ycenter - h / 2.\n    xmin = xcenter - w / 2.\n    ymax = ycenter + h / 2.\n    xmax = xcenter + w / 2.\n    return box_list.BoxList(tf.transpose(tf.stack([ymin, xmin, ymax, xmax])))\n'"
src/object_detection/box_coders/faster_rcnn_box_coder_test.py,10,"b'# Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n""""""Tests for object_detection.box_coder.faster_rcnn_box_coder.""""""\n\nimport tensorflow as tf\n\nfrom object_detection.box_coders import faster_rcnn_box_coder\nfrom object_detection.core import box_list\n\n\nclass FasterRcnnBoxCoderTest(tf.test.TestCase):\n\n  def test_get_correct_relative_codes_after_encoding(self):\n    boxes = [[10.0, 10.0, 20.0, 15.0], [0.2, 0.1, 0.5, 0.4]]\n    anchors = [[15.0, 12.0, 30.0, 18.0], [0.1, 0.0, 0.7, 0.9]]\n    expected_rel_codes = [[-0.5, -0.416666, -0.405465, -0.182321],\n                          [-0.083333, -0.222222, -0.693147, -1.098612]]\n    boxes = box_list.BoxList(tf.constant(boxes))\n    anchors = box_list.BoxList(tf.constant(anchors))\n    coder = faster_rcnn_box_coder.FasterRcnnBoxCoder()\n    rel_codes = coder.encode(boxes, anchors)\n    with self.test_session() as sess:\n      rel_codes_out, = sess.run([rel_codes])\n      self.assertAllClose(rel_codes_out, expected_rel_codes)\n\n  def test_get_correct_relative_codes_after_encoding_with_scaling(self):\n    boxes = [[10.0, 10.0, 20.0, 15.0], [0.2, 0.1, 0.5, 0.4]]\n    anchors = [[15.0, 12.0, 30.0, 18.0], [0.1, 0.0, 0.7, 0.9]]\n    scale_factors = [2, 3, 4, 5]\n    expected_rel_codes = [[-1., -1.25, -1.62186, -0.911608],\n                          [-0.166667, -0.666667, -2.772588, -5.493062]]\n    boxes = box_list.BoxList(tf.constant(boxes))\n    anchors = box_list.BoxList(tf.constant(anchors))\n    coder = faster_rcnn_box_coder.FasterRcnnBoxCoder(\n        scale_factors=scale_factors)\n    rel_codes = coder.encode(boxes, anchors)\n    with self.test_session() as sess:\n      rel_codes_out, = sess.run([rel_codes])\n      self.assertAllClose(rel_codes_out, expected_rel_codes)\n\n  def test_get_correct_boxes_after_decoding(self):\n    anchors = [[15.0, 12.0, 30.0, 18.0], [0.1, 0.0, 0.7, 0.9]]\n    rel_codes = [[-0.5, -0.416666, -0.405465, -0.182321],\n                 [-0.083333, -0.222222, -0.693147, -1.098612]]\n    expected_boxes = [[10.0, 10.0, 20.0, 15.0], [0.2, 0.1, 0.5, 0.4]]\n    anchors = box_list.BoxList(tf.constant(anchors))\n    coder = faster_rcnn_box_coder.FasterRcnnBoxCoder()\n    boxes = coder.decode(rel_codes, anchors)\n    with self.test_session() as sess:\n      boxes_out, = sess.run([boxes.get()])\n      self.assertAllClose(boxes_out, expected_boxes)\n\n  def test_get_correct_boxes_after_decoding_with_scaling(self):\n    anchors = [[15.0, 12.0, 30.0, 18.0], [0.1, 0.0, 0.7, 0.9]]\n    rel_codes = [[-1., -1.25, -1.62186, -0.911608],\n                 [-0.166667, -0.666667, -2.772588, -5.493062]]\n    scale_factors = [2, 3, 4, 5]\n    expected_boxes = [[10.0, 10.0, 20.0, 15.0], [0.2, 0.1, 0.5, 0.4]]\n    anchors = box_list.BoxList(tf.constant(anchors))\n    coder = faster_rcnn_box_coder.FasterRcnnBoxCoder(\n        scale_factors=scale_factors)\n    boxes = coder.decode(rel_codes, anchors)\n    with self.test_session() as sess:\n      boxes_out, = sess.run([boxes.get()])\n      self.assertAllClose(boxes_out, expected_boxes)\n\n  def test_very_small_Width_nan_after_encoding(self):\n    boxes = [[10.0, 10.0, 10.0000001, 20.0]]\n    anchors = [[15.0, 12.0, 30.0, 18.0]]\n    expected_rel_codes = [[-0.833333, 0., -21.128731, 0.510826]]\n    boxes = box_list.BoxList(tf.constant(boxes))\n    anchors = box_list.BoxList(tf.constant(anchors))\n    coder = faster_rcnn_box_coder.FasterRcnnBoxCoder()\n    rel_codes = coder.encode(boxes, anchors)\n    with self.test_session() as sess:\n      rel_codes_out, = sess.run([rel_codes])\n      self.assertAllClose(rel_codes_out, expected_rel_codes)\n\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
src/object_detection/box_coders/keypoint_box_coder.py,23,"b'# Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n""""""Keypoint box coder.\n\nThe keypoint box coder follows the coding schema described below (this is\nsimilar to the FasterRcnnBoxCoder, except that it encodes keypoints in addition\nto box coordinates):\n  ty = (y - ya) / ha\n  tx = (x - xa) / wa\n  th = log(h / ha)\n  tw = log(w / wa)\n  tky0 = (ky0 - ya) / ha\n  tkx0 = (kx0 - xa) / wa\n  tky1 = (ky1 - ya) / ha\n  tkx1 = (kx1 - xa) / wa\n  ...\n  where x, y, w, h denote the box\'s center coordinates, width and height\n  respectively. Similarly, xa, ya, wa, ha denote the anchor\'s center\n  coordinates, width and height. tx, ty, tw and th denote the anchor-encoded\n  center, width and height respectively. ky0, kx0, ky1, kx1, ... denote the\n  keypoints\' coordinates, and tky0, tkx0, tky1, tkx1, ... denote the\n  anchor-encoded keypoint coordinates.\n""""""\n\nimport tensorflow as tf\n\nfrom object_detection.core import box_coder\nfrom object_detection.core import box_list\nfrom object_detection.core import standard_fields as fields\n\nEPSILON = 1e-8\n\n\nclass KeypointBoxCoder(box_coder.BoxCoder):\n  """"""Keypoint box coder.""""""\n\n  def __init__(self, num_keypoints, scale_factors=None):\n    """"""Constructor for KeypointBoxCoder.\n\n    Args:\n      num_keypoints: Number of keypoints to encode/decode.\n      scale_factors: List of 4 positive scalars to scale ty, tx, th and tw.\n        In addition to scaling ty and tx, the first 2 scalars are used to scale\n        the y and x coordinates of the keypoints as well. If set to None, does\n        not perform scaling.\n    """"""\n    self._num_keypoints = num_keypoints\n\n    if scale_factors:\n      assert len(scale_factors) == 4\n      for scalar in scale_factors:\n        assert scalar > 0\n    self._scale_factors = scale_factors\n    self._keypoint_scale_factors = None\n    if scale_factors is not None:\n      self._keypoint_scale_factors = tf.expand_dims(tf.tile(\n          [tf.to_float(scale_factors[0]), tf.to_float(scale_factors[1])],\n          [num_keypoints]), 1)\n\n  @property\n  def code_size(self):\n    return 4 + self._num_keypoints * 2\n\n  def _encode(self, boxes, anchors):\n    """"""Encode a box and keypoint collection with respect to anchor collection.\n\n    Args:\n      boxes: BoxList holding N boxes and keypoints to be encoded. Boxes are\n        tensors with the shape [N, 4], and keypoints are tensors with the shape\n        [N, num_keypoints, 2].\n      anchors: BoxList of anchors.\n\n    Returns:\n      a tensor representing N anchor-encoded boxes of the format\n      [ty, tx, th, tw, tky0, tkx0, tky1, tkx1, ...] where tky0 and tkx0\n      represent the y and x coordinates of the first keypoint, tky1 and tkx1\n      represent the y and x coordinates of the second keypoint, and so on.\n    """"""\n    # Convert anchors to the center coordinate representation.\n    ycenter_a, xcenter_a, ha, wa = anchors.get_center_coordinates_and_sizes()\n    ycenter, xcenter, h, w = boxes.get_center_coordinates_and_sizes()\n    keypoints = boxes.get_field(fields.BoxListFields.keypoints)\n    keypoints = tf.transpose(tf.reshape(keypoints,\n                                        [-1, self._num_keypoints * 2]))\n    num_boxes = boxes.num_boxes()\n\n    # Avoid NaN in division and log below.\n    ha += EPSILON\n    wa += EPSILON\n    h += EPSILON\n    w += EPSILON\n\n    tx = (xcenter - xcenter_a) / wa\n    ty = (ycenter - ycenter_a) / ha\n    tw = tf.log(w / wa)\n    th = tf.log(h / ha)\n\n    tiled_anchor_centers = tf.tile(\n        tf.stack([ycenter_a, xcenter_a]), [self._num_keypoints, 1])\n    tiled_anchor_sizes = tf.tile(\n        tf.stack([ha, wa]), [self._num_keypoints, 1])\n    tkeypoints = (keypoints - tiled_anchor_centers) / tiled_anchor_sizes\n\n    # Scales location targets as used in paper for joint training.\n    if self._scale_factors:\n      ty *= self._scale_factors[0]\n      tx *= self._scale_factors[1]\n      th *= self._scale_factors[2]\n      tw *= self._scale_factors[3]\n      tkeypoints *= tf.tile(self._keypoint_scale_factors, [1, num_boxes])\n\n    tboxes = tf.stack([ty, tx, th, tw])\n    return tf.transpose(tf.concat([tboxes, tkeypoints], 0))\n\n  def _decode(self, rel_codes, anchors):\n    """"""Decode relative codes to boxes and keypoints.\n\n    Args:\n      rel_codes: a tensor with shape [N, 4 + 2 * num_keypoints] representing N\n        anchor-encoded boxes and keypoints\n      anchors: BoxList of anchors.\n\n    Returns:\n      boxes: BoxList holding N bounding boxes and keypoints.\n    """"""\n    ycenter_a, xcenter_a, ha, wa = anchors.get_center_coordinates_and_sizes()\n\n    num_codes = tf.shape(rel_codes)[0]\n    result = tf.unstack(tf.transpose(rel_codes))\n    ty, tx, th, tw = result[:4]\n    tkeypoints = result[4:]\n    if self._scale_factors:\n      ty /= self._scale_factors[0]\n      tx /= self._scale_factors[1]\n      th /= self._scale_factors[2]\n      tw /= self._scale_factors[3]\n      tkeypoints /= tf.tile(self._keypoint_scale_factors, [1, num_codes])\n\n    w = tf.exp(tw) * wa\n    h = tf.exp(th) * ha\n    ycenter = ty * ha + ycenter_a\n    xcenter = tx * wa + xcenter_a\n    ymin = ycenter - h / 2.\n    xmin = xcenter - w / 2.\n    ymax = ycenter + h / 2.\n    xmax = xcenter + w / 2.\n    decoded_boxes_keypoints = box_list.BoxList(\n        tf.transpose(tf.stack([ymin, xmin, ymax, xmax])))\n\n    tiled_anchor_centers = tf.tile(\n        tf.stack([ycenter_a, xcenter_a]), [self._num_keypoints, 1])\n    tiled_anchor_sizes = tf.tile(\n        tf.stack([ha, wa]), [self._num_keypoints, 1])\n    keypoints = tkeypoints * tiled_anchor_sizes + tiled_anchor_centers\n    keypoints = tf.reshape(tf.transpose(keypoints),\n                           [-1, self._num_keypoints, 2])\n    decoded_boxes_keypoints.add_field(fields.BoxListFields.keypoints, keypoints)\n    return decoded_boxes_keypoints\n'"
src/object_detection/box_coders/keypoint_box_coder_test.py,13,"b'# Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n""""""Tests for object_detection.box_coder.keypoint_box_coder.""""""\n\nimport tensorflow as tf\n\nfrom object_detection.box_coders import keypoint_box_coder\nfrom object_detection.core import box_list\nfrom object_detection.core import standard_fields as fields\n\n\nclass KeypointBoxCoderTest(tf.test.TestCase):\n\n  def test_get_correct_relative_codes_after_encoding(self):\n    boxes = [[10., 10., 20., 15.],\n             [0.2, 0.1, 0.5, 0.4]]\n    keypoints = [[[15., 12.], [10., 15.]],\n                 [[0.5, 0.3], [0.2, 0.4]]]\n    num_keypoints = len(keypoints[0])\n    anchors = [[15., 12., 30., 18.],\n               [0.1, 0.0, 0.7, 0.9]]\n    expected_rel_codes = [\n        [-0.5, -0.416666, -0.405465, -0.182321,\n         -0.5, -0.5, -0.833333, 0.],\n        [-0.083333, -0.222222, -0.693147, -1.098612,\n         0.166667, -0.166667, -0.333333, -0.055556]\n    ]\n    boxes = box_list.BoxList(tf.constant(boxes))\n    boxes.add_field(fields.BoxListFields.keypoints, tf.constant(keypoints))\n    anchors = box_list.BoxList(tf.constant(anchors))\n    coder = keypoint_box_coder.KeypointBoxCoder(num_keypoints)\n    rel_codes = coder.encode(boxes, anchors)\n    with self.test_session() as sess:\n      rel_codes_out, = sess.run([rel_codes])\n      self.assertAllClose(rel_codes_out, expected_rel_codes)\n\n  def test_get_correct_relative_codes_after_encoding_with_scaling(self):\n    boxes = [[10., 10., 20., 15.],\n             [0.2, 0.1, 0.5, 0.4]]\n    keypoints = [[[15., 12.], [10., 15.]],\n                 [[0.5, 0.3], [0.2, 0.4]]]\n    num_keypoints = len(keypoints[0])\n    anchors = [[15., 12., 30., 18.],\n               [0.1, 0.0, 0.7, 0.9]]\n    scale_factors = [2, 3, 4, 5]\n    expected_rel_codes = [\n        [-1., -1.25, -1.62186, -0.911608,\n         -1.0, -1.5, -1.666667, 0.],\n        [-0.166667, -0.666667, -2.772588, -5.493062,\n         0.333333, -0.5, -0.666667, -0.166667]\n    ]\n    boxes = box_list.BoxList(tf.constant(boxes))\n    boxes.add_field(fields.BoxListFields.keypoints, tf.constant(keypoints))\n    anchors = box_list.BoxList(tf.constant(anchors))\n    coder = keypoint_box_coder.KeypointBoxCoder(\n        num_keypoints, scale_factors=scale_factors)\n    rel_codes = coder.encode(boxes, anchors)\n    with self.test_session() as sess:\n      rel_codes_out, = sess.run([rel_codes])\n      self.assertAllClose(rel_codes_out, expected_rel_codes)\n\n  def test_get_correct_boxes_after_decoding(self):\n    anchors = [[15., 12., 30., 18.],\n               [0.1, 0.0, 0.7, 0.9]]\n    rel_codes = [\n        [-0.5, -0.416666, -0.405465, -0.182321,\n         -0.5, -0.5, -0.833333, 0.],\n        [-0.083333, -0.222222, -0.693147, -1.098612,\n         0.166667, -0.166667, -0.333333, -0.055556]\n    ]\n    expected_boxes = [[10., 10., 20., 15.],\n                      [0.2, 0.1, 0.5, 0.4]]\n    expected_keypoints = [[[15., 12.], [10., 15.]],\n                          [[0.5, 0.3], [0.2, 0.4]]]\n    num_keypoints = len(expected_keypoints[0])\n    anchors = box_list.BoxList(tf.constant(anchors))\n    coder = keypoint_box_coder.KeypointBoxCoder(num_keypoints)\n    boxes = coder.decode(rel_codes, anchors)\n    with self.test_session() as sess:\n      boxes_out, keypoints_out = sess.run(\n          [boxes.get(), boxes.get_field(fields.BoxListFields.keypoints)])\n      self.assertAllClose(boxes_out, expected_boxes)\n      self.assertAllClose(keypoints_out, expected_keypoints)\n\n  def test_get_correct_boxes_after_decoding_with_scaling(self):\n    anchors = [[15., 12., 30., 18.],\n               [0.1, 0.0, 0.7, 0.9]]\n    rel_codes = [\n        [-1., -1.25, -1.62186, -0.911608,\n         -1.0, -1.5, -1.666667, 0.],\n        [-0.166667, -0.666667, -2.772588, -5.493062,\n         0.333333, -0.5, -0.666667, -0.166667]\n    ]\n    scale_factors = [2, 3, 4, 5]\n    expected_boxes = [[10., 10., 20., 15.],\n                      [0.2, 0.1, 0.5, 0.4]]\n    expected_keypoints = [[[15., 12.], [10., 15.]],\n                          [[0.5, 0.3], [0.2, 0.4]]]\n    num_keypoints = len(expected_keypoints[0])\n    anchors = box_list.BoxList(tf.constant(anchors))\n    coder = keypoint_box_coder.KeypointBoxCoder(\n        num_keypoints, scale_factors=scale_factors)\n    boxes = coder.decode(rel_codes, anchors)\n    with self.test_session() as sess:\n      boxes_out, keypoints_out = sess.run(\n          [boxes.get(), boxes.get_field(fields.BoxListFields.keypoints)])\n      self.assertAllClose(boxes_out, expected_boxes)\n      self.assertAllClose(keypoints_out, expected_keypoints)\n\n  def test_very_small_width_nan_after_encoding(self):\n    boxes = [[10., 10., 10.0000001, 20.]]\n    keypoints = [[[10., 10.], [10.0000001, 20.]]]\n    anchors = [[15., 12., 30., 18.]]\n    expected_rel_codes = [[-0.833333, 0., -21.128731, 0.510826,\n                           -0.833333, -0.833333, -0.833333, 0.833333]]\n    boxes = box_list.BoxList(tf.constant(boxes))\n    boxes.add_field(fields.BoxListFields.keypoints, tf.constant(keypoints))\n    anchors = box_list.BoxList(tf.constant(anchors))\n    coder = keypoint_box_coder.KeypointBoxCoder(2)\n    rel_codes = coder.encode(boxes, anchors)\n    with self.test_session() as sess:\n      rel_codes_out, = sess.run([rel_codes])\n      self.assertAllClose(rel_codes_out, expected_rel_codes)\n\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
src/object_detection/box_coders/mean_stddev_box_coder.py,0,"b'# Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n""""""Mean stddev box coder.\n\nThis box coder use the following coding schema to encode boxes:\nrel_code = (box_corner - anchor_corner_mean) / anchor_corner_stddev.\n""""""\nfrom object_detection.core import box_coder\nfrom object_detection.core import box_list\n\n\nclass MeanStddevBoxCoder(box_coder.BoxCoder):\n  """"""Mean stddev box coder.""""""\n\n  @property\n  def code_size(self):\n    return 4\n\n  def _encode(self, boxes, anchors):\n    """"""Encode a box collection with respect to anchor collection.\n\n    Args:\n      boxes: BoxList holding N boxes to be encoded.\n      anchors: BoxList of N anchors.  We assume that anchors has an associated\n        stddev field.\n\n    Returns:\n      a tensor representing N anchor-encoded boxes\n    Raises:\n      ValueError: if the anchors BoxList does not have a stddev field\n    """"""\n    if not anchors.has_field(\'stddev\'):\n      raise ValueError(\'anchors must have a stddev field\')\n    box_corners = boxes.get()\n    means = anchors.get()\n    stddev = anchors.get_field(\'stddev\')\n    return (box_corners - means) / stddev\n\n  def _decode(self, rel_codes, anchors):\n    """"""Decode.\n\n    Args:\n      rel_codes: a tensor representing N anchor-encoded boxes.\n      anchors: BoxList of anchors.  We assume that anchors has an associated\n        stddev field.\n\n    Returns:\n      boxes: BoxList holding N bounding boxes\n    Raises:\n      ValueError: if the anchors BoxList does not have a stddev field\n    """"""\n    if not anchors.has_field(\'stddev\'):\n      raise ValueError(\'anchors must have a stddev field\')\n    means = anchors.get()\n    stddevs = anchors.get_field(\'stddev\')\n    box_corners = rel_codes * stddevs + means\n    return box_list.BoxList(box_corners)\n'"
src/object_detection/box_coders/mean_stddev_box_coder_test.py,8,"b'# Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n""""""Tests for object_detection.box_coder.mean_stddev_boxcoder.""""""\n\nimport tensorflow as tf\n\nfrom object_detection.box_coders import mean_stddev_box_coder\nfrom object_detection.core import box_list\n\n\nclass MeanStddevBoxCoderTest(tf.test.TestCase):\n\n  def testGetCorrectRelativeCodesAfterEncoding(self):\n    box_corners = [[0.0, 0.0, 0.5, 0.5], [0.0, 0.0, 0.5, 0.5]]\n    boxes = box_list.BoxList(tf.constant(box_corners))\n    expected_rel_codes = [[0.0, 0.0, 0.0, 0.0], [-5.0, -5.0, -5.0, -3.0]]\n    prior_means = tf.constant([[0.0, 0.0, 0.5, 0.5], [0.5, 0.5, 1.0, 0.8]])\n    prior_stddevs = tf.constant(2 * [4 * [.1]])\n    priors = box_list.BoxList(prior_means)\n    priors.add_field(\'stddev\', prior_stddevs)\n\n    coder = mean_stddev_box_coder.MeanStddevBoxCoder()\n    rel_codes = coder.encode(boxes, priors)\n    with self.test_session() as sess:\n      rel_codes_out = sess.run(rel_codes)\n      self.assertAllClose(rel_codes_out, expected_rel_codes)\n\n  def testGetCorrectBoxesAfterDecoding(self):\n    rel_codes = tf.constant([[0.0, 0.0, 0.0, 0.0], [-5.0, -5.0, -5.0, -3.0]])\n    expected_box_corners = [[0.0, 0.0, 0.5, 0.5], [0.0, 0.0, 0.5, 0.5]]\n    prior_means = tf.constant([[0.0, 0.0, 0.5, 0.5], [0.5, 0.5, 1.0, 0.8]])\n    prior_stddevs = tf.constant(2 * [4 * [.1]])\n    priors = box_list.BoxList(prior_means)\n    priors.add_field(\'stddev\', prior_stddevs)\n\n    coder = mean_stddev_box_coder.MeanStddevBoxCoder()\n    decoded_boxes = coder.decode(rel_codes, priors)\n    decoded_box_corners = decoded_boxes.get()\n    with self.test_session() as sess:\n      decoded_out = sess.run(decoded_box_corners)\n      self.assertAllClose(decoded_out, expected_box_corners)\n\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
src/object_detection/box_coders/square_box_coder.py,8,"b'# Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n""""""Square box coder.\n\nSquare box coder follows the coding schema described below:\nl = sqrt(h * w)\nla = sqrt(ha * wa)\nty = (y - ya) / la\ntx = (x - xa) / la\ntl = log(l / la)\nwhere x, y, w, h denote the box\'s center coordinates, width, and height,\nrespectively. Similarly, xa, ya, wa, ha denote the anchor\'s center\ncoordinates, width and height. tx, ty, tl denote the anchor-encoded\ncenter, and length, respectively. Because the encoded box is a square, only\none length is encoded.\n\nThis has shown to provide performance improvements over the Faster RCNN box\ncoder when the objects being detected tend to be square (e.g. faces) and when\nthe input images are not distorted via resizing.\n""""""\n\nimport tensorflow as tf\n\nfrom object_detection.core import box_coder\nfrom object_detection.core import box_list\n\nEPSILON = 1e-8\n\n\nclass SquareBoxCoder(box_coder.BoxCoder):\n  """"""Encodes a 3-scalar representation of a square box.""""""\n\n  def __init__(self, scale_factors=None):\n    """"""Constructor for SquareBoxCoder.\n\n    Args:\n      scale_factors: List of 3 positive scalars to scale ty, tx, and tl.\n        If set to None, does not perform scaling. For faster RCNN,\n        the open-source implementation recommends using [10.0, 10.0, 5.0].\n\n    Raises:\n      ValueError: If scale_factors is not length 3 or contains values less than\n        or equal to 0.\n    """"""\n    if scale_factors:\n      if len(scale_factors) != 3:\n        raise ValueError(\'The argument scale_factors must be a list of length \'\n                         \'3.\')\n      if any(scalar <= 0 for scalar in scale_factors):\n        raise ValueError(\'The values in scale_factors must all be greater \'\n                         \'than 0.\')\n    self._scale_factors = scale_factors\n\n  @property\n  def code_size(self):\n    return 3\n\n  def _encode(self, boxes, anchors):\n    """"""Encodes a box collection with respect to an anchor collection.\n\n    Args:\n      boxes: BoxList holding N boxes to be encoded.\n      anchors: BoxList of anchors.\n\n    Returns:\n      a tensor representing N anchor-encoded boxes of the format\n      [ty, tx, tl].\n    """"""\n    # Convert anchors to the center coordinate representation.\n    ycenter_a, xcenter_a, ha, wa = anchors.get_center_coordinates_and_sizes()\n    la = tf.sqrt(ha * wa)\n    ycenter, xcenter, h, w = boxes.get_center_coordinates_and_sizes()\n    l = tf.sqrt(h * w)\n    # Avoid NaN in division and log below.\n    la += EPSILON\n    l += EPSILON\n\n    tx = (xcenter - xcenter_a) / la\n    ty = (ycenter - ycenter_a) / la\n    tl = tf.log(l / la)\n    # Scales location targets for joint training.\n    if self._scale_factors:\n      ty *= self._scale_factors[0]\n      tx *= self._scale_factors[1]\n      tl *= self._scale_factors[2]\n    return tf.transpose(tf.stack([ty, tx, tl]))\n\n  def _decode(self, rel_codes, anchors):\n    """"""Decodes relative codes to boxes.\n\n    Args:\n      rel_codes: a tensor representing N anchor-encoded boxes.\n      anchors: BoxList of anchors.\n\n    Returns:\n      boxes: BoxList holding N bounding boxes.\n    """"""\n    ycenter_a, xcenter_a, ha, wa = anchors.get_center_coordinates_and_sizes()\n    la = tf.sqrt(ha * wa)\n\n    ty, tx, tl = tf.unstack(tf.transpose(rel_codes))\n    if self._scale_factors:\n      ty /= self._scale_factors[0]\n      tx /= self._scale_factors[1]\n      tl /= self._scale_factors[2]\n    l = tf.exp(tl) * la\n    ycenter = ty * la + ycenter_a\n    xcenter = tx * la + xcenter_a\n    ymin = ycenter - l / 2.\n    xmin = xcenter - l / 2.\n    ymax = ycenter + l / 2.\n    xmax = xcenter + l / 2.\n    return box_list.BoxList(tf.transpose(tf.stack([ymin, xmin, ymax, xmax])))\n'"
src/object_detection/box_coders/square_box_coder_test.py,10,"b'# Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n""""""Tests for object_detection.box_coder.square_box_coder.""""""\n\nimport tensorflow as tf\n\nfrom object_detection.box_coders import square_box_coder\nfrom object_detection.core import box_list\n\n\nclass SquareBoxCoderTest(tf.test.TestCase):\n\n  def test_correct_relative_codes_with_default_scale(self):\n    boxes = [[10.0, 10.0, 20.0, 15.0], [0.2, 0.1, 0.5, 0.4]]\n    anchors = [[15.0, 12.0, 30.0, 18.0], [0.1, 0.0, 0.7, 0.9]]\n    scale_factors = None\n    expected_rel_codes = [[-0.790569, -0.263523, -0.293893],\n                          [-0.068041, -0.272166, -0.89588]]\n\n    boxes = box_list.BoxList(tf.constant(boxes))\n    anchors = box_list.BoxList(tf.constant(anchors))\n    coder = square_box_coder.SquareBoxCoder(scale_factors=scale_factors)\n    rel_codes = coder.encode(boxes, anchors)\n    with self.test_session() as sess:\n      (rel_codes_out,) = sess.run([rel_codes])\n      self.assertAllClose(rel_codes_out, expected_rel_codes)\n\n  def test_correct_relative_codes_with_non_default_scale(self):\n    boxes = [[10.0, 10.0, 20.0, 15.0], [0.2, 0.1, 0.5, 0.4]]\n    anchors = [[15.0, 12.0, 30.0, 18.0], [0.1, 0.0, 0.7, 0.9]]\n    scale_factors = [2, 3, 4]\n    expected_rel_codes = [[-1.581139, -0.790569, -1.175573],\n                          [-0.136083, -0.816497, -3.583519]]\n    boxes = box_list.BoxList(tf.constant(boxes))\n    anchors = box_list.BoxList(tf.constant(anchors))\n    coder = square_box_coder.SquareBoxCoder(scale_factors=scale_factors)\n    rel_codes = coder.encode(boxes, anchors)\n    with self.test_session() as sess:\n      (rel_codes_out,) = sess.run([rel_codes])\n      self.assertAllClose(rel_codes_out, expected_rel_codes)\n\n  def test_correct_relative_codes_with_small_width(self):\n    boxes = [[10.0, 10.0, 10.0000001, 20.0]]\n    anchors = [[15.0, 12.0, 30.0, 18.0]]\n    scale_factors = None\n    expected_rel_codes = [[-1.317616, 0., -20.670586]]\n    boxes = box_list.BoxList(tf.constant(boxes))\n    anchors = box_list.BoxList(tf.constant(anchors))\n    coder = square_box_coder.SquareBoxCoder(scale_factors=scale_factors)\n    rel_codes = coder.encode(boxes, anchors)\n    with self.test_session() as sess:\n      (rel_codes_out,) = sess.run([rel_codes])\n      self.assertAllClose(rel_codes_out, expected_rel_codes)\n\n  def test_correct_boxes_with_default_scale(self):\n    anchors = [[15.0, 12.0, 30.0, 18.0], [0.1, 0.0, 0.7, 0.9]]\n    rel_codes = [[-0.5, -0.416666, -0.405465],\n                 [-0.083333, -0.222222, -0.693147]]\n    scale_factors = None\n    expected_boxes = [[14.594306, 7.884875, 20.918861, 14.209432],\n                      [0.155051, 0.102989, 0.522474, 0.470412]]\n    anchors = box_list.BoxList(tf.constant(anchors))\n    coder = square_box_coder.SquareBoxCoder(scale_factors=scale_factors)\n    boxes = coder.decode(rel_codes, anchors)\n    with self.test_session() as sess:\n      (boxes_out,) = sess.run([boxes.get()])\n      self.assertAllClose(boxes_out, expected_boxes)\n\n  def test_correct_boxes_with_non_default_scale(self):\n    anchors = [[15.0, 12.0, 30.0, 18.0], [0.1, 0.0, 0.7, 0.9]]\n    rel_codes = [[-1., -1.25, -1.62186], [-0.166667, -0.666667, -2.772588]]\n    scale_factors = [2, 3, 4]\n    expected_boxes = [[14.594306, 7.884875, 20.918861, 14.209432],\n                      [0.155051, 0.102989, 0.522474, 0.470412]]\n    anchors = box_list.BoxList(tf.constant(anchors))\n    coder = square_box_coder.SquareBoxCoder(scale_factors=scale_factors)\n    boxes = coder.decode(rel_codes, anchors)\n    with self.test_session() as sess:\n      (boxes_out,) = sess.run([boxes.get()])\n      self.assertAllClose(boxes_out, expected_boxes)\n\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
src/object_detection/builders/__init__.py,0,b''
src/object_detection/builders/anchor_generator_builder.py,0,"b'# Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n""""""A function to build an object detection anchor generator from config.""""""\n\nfrom object_detection.anchor_generators import grid_anchor_generator\nfrom object_detection.anchor_generators import multiple_grid_anchor_generator\nfrom object_detection.anchor_generators import multiscale_grid_anchor_generator\nfrom object_detection.protos import anchor_generator_pb2\n\n\ndef build(anchor_generator_config):\n  """"""Builds an anchor generator based on the config.\n\n  Args:\n    anchor_generator_config: An anchor_generator.proto object containing the\n      config for the desired anchor generator.\n\n  Returns:\n    Anchor generator based on the config.\n\n  Raises:\n    ValueError: On empty anchor generator proto.\n  """"""\n  if not isinstance(anchor_generator_config,\n                    anchor_generator_pb2.AnchorGenerator):\n    raise ValueError(\'anchor_generator_config not of type \'\n                     \'anchor_generator_pb2.AnchorGenerator\')\n  if anchor_generator_config.WhichOneof(\n      \'anchor_generator_oneof\') == \'grid_anchor_generator\':\n    grid_anchor_generator_config = anchor_generator_config.grid_anchor_generator\n    return grid_anchor_generator.GridAnchorGenerator(\n        scales=[float(scale) for scale in grid_anchor_generator_config.scales],\n        aspect_ratios=[float(aspect_ratio)\n                       for aspect_ratio\n                       in grid_anchor_generator_config.aspect_ratios],\n        base_anchor_size=[grid_anchor_generator_config.height,\n                          grid_anchor_generator_config.width],\n        anchor_stride=[grid_anchor_generator_config.height_stride,\n                       grid_anchor_generator_config.width_stride],\n        anchor_offset=[grid_anchor_generator_config.height_offset,\n                       grid_anchor_generator_config.width_offset])\n  elif anchor_generator_config.WhichOneof(\n      \'anchor_generator_oneof\') == \'ssd_anchor_generator\':\n    ssd_anchor_generator_config = anchor_generator_config.ssd_anchor_generator\n    anchor_strides = None\n    if ssd_anchor_generator_config.height_stride:\n      anchor_strides = zip(ssd_anchor_generator_config.height_stride,\n                           ssd_anchor_generator_config.width_stride)\n    anchor_offsets = None\n    if ssd_anchor_generator_config.height_offset:\n      anchor_offsets = zip(ssd_anchor_generator_config.height_offset,\n                           ssd_anchor_generator_config.width_offset)\n    return multiple_grid_anchor_generator.create_ssd_anchors(\n        num_layers=ssd_anchor_generator_config.num_layers,\n        min_scale=ssd_anchor_generator_config.min_scale,\n        max_scale=ssd_anchor_generator_config.max_scale,\n        scales=[float(scale) for scale in ssd_anchor_generator_config.scales],\n        aspect_ratios=ssd_anchor_generator_config.aspect_ratios,\n        interpolated_scale_aspect_ratio=(\n            ssd_anchor_generator_config.interpolated_scale_aspect_ratio),\n        base_anchor_size=[\n            ssd_anchor_generator_config.base_anchor_height,\n            ssd_anchor_generator_config.base_anchor_width\n        ],\n        anchor_strides=anchor_strides,\n        anchor_offsets=anchor_offsets,\n        reduce_boxes_in_lowest_layer=(\n            ssd_anchor_generator_config.reduce_boxes_in_lowest_layer))\n  elif anchor_generator_config.WhichOneof(\n      \'anchor_generator_oneof\') == \'multiscale_anchor_generator\':\n    cfg = anchor_generator_config.multiscale_anchor_generator\n    return multiscale_grid_anchor_generator.MultiscaleGridAnchorGenerator(\n        cfg.min_level,\n        cfg.max_level,\n        cfg.anchor_scale,\n        [float(aspect_ratio) for aspect_ratio in cfg.aspect_ratios],\n        cfg.scales_per_octave,\n        cfg.normalize_coordinates\n    )\n  else:\n    raise ValueError(\'Empty anchor generator.\')\n'"
src/object_detection/builders/anchor_generator_builder_test.py,2,"b'# Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n""""""Tests for anchor_generator_builder.""""""\n\nimport math\n\nimport tensorflow as tf\n\nfrom google.protobuf import text_format\nfrom object_detection.anchor_generators import grid_anchor_generator\nfrom object_detection.anchor_generators import multiple_grid_anchor_generator\nfrom object_detection.anchor_generators import multiscale_grid_anchor_generator\nfrom object_detection.builders import anchor_generator_builder\nfrom object_detection.protos import anchor_generator_pb2\n\n\nclass AnchorGeneratorBuilderTest(tf.test.TestCase):\n\n  def assert_almost_list_equal(self, expected_list, actual_list, delta=None):\n    self.assertEqual(len(expected_list), len(actual_list))\n    for expected_item, actual_item in zip(expected_list, actual_list):\n      self.assertAlmostEqual(expected_item, actual_item, delta=delta)\n\n  def test_build_grid_anchor_generator_with_defaults(self):\n    anchor_generator_text_proto = """"""\n      grid_anchor_generator {\n      }\n     """"""\n    anchor_generator_proto = anchor_generator_pb2.AnchorGenerator()\n    text_format.Merge(anchor_generator_text_proto, anchor_generator_proto)\n    anchor_generator_object = anchor_generator_builder.build(\n        anchor_generator_proto)\n    self.assertTrue(isinstance(anchor_generator_object,\n                               grid_anchor_generator.GridAnchorGenerator))\n    self.assertListEqual(anchor_generator_object._scales, [])\n    self.assertListEqual(anchor_generator_object._aspect_ratios, [])\n    with self.test_session() as sess:\n      base_anchor_size, anchor_offset, anchor_stride = sess.run(\n          [anchor_generator_object._base_anchor_size,\n           anchor_generator_object._anchor_offset,\n           anchor_generator_object._anchor_stride])\n    self.assertAllEqual(anchor_offset, [0, 0])\n    self.assertAllEqual(anchor_stride, [16, 16])\n    self.assertAllEqual(base_anchor_size, [256, 256])\n\n  def test_build_grid_anchor_generator_with_non_default_parameters(self):\n    anchor_generator_text_proto = """"""\n      grid_anchor_generator {\n        height: 128\n        width: 512\n        height_stride: 10\n        width_stride: 20\n        height_offset: 30\n        width_offset: 40\n        scales: [0.4, 2.2]\n        aspect_ratios: [0.3, 4.5]\n      }\n     """"""\n    anchor_generator_proto = anchor_generator_pb2.AnchorGenerator()\n    text_format.Merge(anchor_generator_text_proto, anchor_generator_proto)\n    anchor_generator_object = anchor_generator_builder.build(\n        anchor_generator_proto)\n    self.assertTrue(isinstance(anchor_generator_object,\n                               grid_anchor_generator.GridAnchorGenerator))\n    self.assert_almost_list_equal(anchor_generator_object._scales,\n                                  [0.4, 2.2])\n    self.assert_almost_list_equal(anchor_generator_object._aspect_ratios,\n                                  [0.3, 4.5])\n    with self.test_session() as sess:\n      base_anchor_size, anchor_offset, anchor_stride = sess.run(\n          [anchor_generator_object._base_anchor_size,\n           anchor_generator_object._anchor_offset,\n           anchor_generator_object._anchor_stride])\n    self.assertAllEqual(anchor_offset, [30, 40])\n    self.assertAllEqual(anchor_stride, [10, 20])\n    self.assertAllEqual(base_anchor_size, [128, 512])\n\n  def test_build_ssd_anchor_generator_with_defaults(self):\n    anchor_generator_text_proto = """"""\n      ssd_anchor_generator {\n        aspect_ratios: [1.0]\n      }\n    """"""\n    anchor_generator_proto = anchor_generator_pb2.AnchorGenerator()\n    text_format.Merge(anchor_generator_text_proto, anchor_generator_proto)\n    anchor_generator_object = anchor_generator_builder.build(\n        anchor_generator_proto)\n    self.assertTrue(isinstance(anchor_generator_object,\n                               multiple_grid_anchor_generator.\n                               MultipleGridAnchorGenerator))\n    for actual_scales, expected_scales in zip(\n        list(anchor_generator_object._scales),\n        [(0.1, 0.2, 0.2),\n         (0.35, 0.418),\n         (0.499, 0.570),\n         (0.649, 0.721),\n         (0.799, 0.871),\n         (0.949, 0.974)]):\n      self.assert_almost_list_equal(expected_scales, actual_scales, delta=1e-2)\n    for actual_aspect_ratio, expected_aspect_ratio in zip(\n        list(anchor_generator_object._aspect_ratios),\n        [(1.0, 2.0, 0.5)] + 5 * [(1.0, 1.0)]):\n      self.assert_almost_list_equal(expected_aspect_ratio, actual_aspect_ratio)\n\n    with self.test_session() as sess:\n      base_anchor_size = sess.run(anchor_generator_object._base_anchor_size)\n    self.assertAllClose(base_anchor_size, [1.0, 1.0])\n\n  def test_build_ssd_anchor_generator_with_custom_scales(self):\n    anchor_generator_text_proto = """"""\n      ssd_anchor_generator {\n        aspect_ratios: [1.0]\n        scales: [0.1, 0.15, 0.2, 0.4, 0.6, 0.8]\n        reduce_boxes_in_lowest_layer: false\n      }\n    """"""\n    anchor_generator_proto = anchor_generator_pb2.AnchorGenerator()\n    text_format.Merge(anchor_generator_text_proto, anchor_generator_proto)\n    anchor_generator_object = anchor_generator_builder.build(\n        anchor_generator_proto)\n    self.assertTrue(isinstance(anchor_generator_object,\n                               multiple_grid_anchor_generator.\n                               MultipleGridAnchorGenerator))\n    for actual_scales, expected_scales in zip(\n        list(anchor_generator_object._scales),\n        [(0.1, math.sqrt(0.1 * 0.15)),\n         (0.15, math.sqrt(0.15 * 0.2)),\n         (0.2, math.sqrt(0.2 * 0.4)),\n         (0.4, math.sqrt(0.4 * 0.6)),\n         (0.6, math.sqrt(0.6 * 0.8)),\n         (0.8, math.sqrt(0.8 * 1.0))]):\n      self.assert_almost_list_equal(expected_scales, actual_scales, delta=1e-2)\n\n  def test_build_ssd_anchor_generator_with_custom_interpolated_scale(self):\n    anchor_generator_text_proto = """"""\n      ssd_anchor_generator {\n        aspect_ratios: [0.5]\n        interpolated_scale_aspect_ratio: 0.5\n        reduce_boxes_in_lowest_layer: false\n      }\n    """"""\n    anchor_generator_proto = anchor_generator_pb2.AnchorGenerator()\n    text_format.Merge(anchor_generator_text_proto, anchor_generator_proto)\n    anchor_generator_object = anchor_generator_builder.build(\n        anchor_generator_proto)\n    self.assertTrue(isinstance(anchor_generator_object,\n                               multiple_grid_anchor_generator.\n                               MultipleGridAnchorGenerator))\n    for actual_aspect_ratio, expected_aspect_ratio in zip(\n        list(anchor_generator_object._aspect_ratios),\n        6 * [(0.5, 0.5)]):\n      self.assert_almost_list_equal(expected_aspect_ratio, actual_aspect_ratio)\n\n  def test_build_ssd_anchor_generator_without_reduced_boxes(self):\n    anchor_generator_text_proto = """"""\n      ssd_anchor_generator {\n        aspect_ratios: [1.0]\n        reduce_boxes_in_lowest_layer: false\n      }\n    """"""\n    anchor_generator_proto = anchor_generator_pb2.AnchorGenerator()\n    text_format.Merge(anchor_generator_text_proto, anchor_generator_proto)\n    anchor_generator_object = anchor_generator_builder.build(\n        anchor_generator_proto)\n    self.assertTrue(isinstance(anchor_generator_object,\n                               multiple_grid_anchor_generator.\n                               MultipleGridAnchorGenerator))\n\n    for actual_scales, expected_scales in zip(\n        list(anchor_generator_object._scales),\n        [(0.2, 0.264),\n         (0.35, 0.418),\n         (0.499, 0.570),\n         (0.649, 0.721),\n         (0.799, 0.871),\n         (0.949, 0.974)]):\n      self.assert_almost_list_equal(expected_scales, actual_scales, delta=1e-2)\n\n    for actual_aspect_ratio, expected_aspect_ratio in zip(\n        list(anchor_generator_object._aspect_ratios),\n        6 * [(1.0, 1.0)]):\n      self.assert_almost_list_equal(expected_aspect_ratio, actual_aspect_ratio)\n\n    with self.test_session() as sess:\n      base_anchor_size = sess.run(anchor_generator_object._base_anchor_size)\n    self.assertAllClose(base_anchor_size, [1.0, 1.0])\n\n  def test_build_ssd_anchor_generator_with_non_default_parameters(self):\n    anchor_generator_text_proto = """"""\n      ssd_anchor_generator {\n        num_layers: 2\n        min_scale: 0.3\n        max_scale: 0.8\n        aspect_ratios: [2.0]\n        height_stride: 16\n        height_stride: 32\n        width_stride: 20\n        width_stride: 30\n        height_offset: 8\n        height_offset: 16\n        width_offset: 0\n        width_offset: 10\n      }\n    """"""\n    anchor_generator_proto = anchor_generator_pb2.AnchorGenerator()\n    text_format.Merge(anchor_generator_text_proto, anchor_generator_proto)\n    anchor_generator_object = anchor_generator_builder.build(\n        anchor_generator_proto)\n    self.assertTrue(isinstance(anchor_generator_object,\n                               multiple_grid_anchor_generator.\n                               MultipleGridAnchorGenerator))\n\n    for actual_scales, expected_scales in zip(\n        list(anchor_generator_object._scales),\n        [(0.1, 0.3, 0.3), (0.8, 0.894)]):\n      self.assert_almost_list_equal(expected_scales, actual_scales, delta=1e-2)\n\n    for actual_aspect_ratio, expected_aspect_ratio in zip(\n        list(anchor_generator_object._aspect_ratios),\n        [(1.0, 2.0, 0.5), (2.0, 1.0)]):\n      self.assert_almost_list_equal(expected_aspect_ratio, actual_aspect_ratio)\n\n    for actual_strides, expected_strides in zip(\n        list(anchor_generator_object._anchor_strides), [(16, 20), (32, 30)]):\n      self.assert_almost_list_equal(expected_strides, actual_strides)\n\n    for actual_offsets, expected_offsets in zip(\n        list(anchor_generator_object._anchor_offsets), [(8, 0), (16, 10)]):\n      self.assert_almost_list_equal(expected_offsets, actual_offsets)\n\n    with self.test_session() as sess:\n      base_anchor_size = sess.run(anchor_generator_object._base_anchor_size)\n    self.assertAllClose(base_anchor_size, [1.0, 1.0])\n\n  def test_raise_value_error_on_empty_anchor_genertor(self):\n    anchor_generator_text_proto = """"""\n    """"""\n    anchor_generator_proto = anchor_generator_pb2.AnchorGenerator()\n    text_format.Merge(anchor_generator_text_proto, anchor_generator_proto)\n    with self.assertRaises(ValueError):\n      anchor_generator_builder.build(anchor_generator_proto)\n\n  def test_build_multiscale_anchor_generator_custom_aspect_ratios(self):\n    anchor_generator_text_proto = """"""\n      multiscale_anchor_generator {\n        aspect_ratios: [1.0]\n      }\n    """"""\n    anchor_generator_proto = anchor_generator_pb2.AnchorGenerator()\n    text_format.Merge(anchor_generator_text_proto, anchor_generator_proto)\n    anchor_generator_object = anchor_generator_builder.build(\n        anchor_generator_proto)\n    self.assertTrue(isinstance(anchor_generator_object,\n                               multiscale_grid_anchor_generator.\n                               MultiscaleGridAnchorGenerator))\n    for level, anchor_grid_info in zip(\n        range(3, 8), anchor_generator_object._anchor_grid_info):\n      self.assertEqual(set(anchor_grid_info.keys()), set([\'level\', \'info\']))\n      self.assertTrue(level, anchor_grid_info[\'level\'])\n      self.assertEqual(len(anchor_grid_info[\'info\']), 4)\n      self.assertAllClose(anchor_grid_info[\'info\'][0], [2**0, 2**0.5])\n      self.assertTrue(anchor_grid_info[\'info\'][1], 1.0)\n      self.assertAllClose(anchor_grid_info[\'info\'][2],\n                          [4.0 * 2**level, 4.0 * 2**level])\n      self.assertAllClose(anchor_grid_info[\'info\'][3], [2**level, 2**level])\n      self.assertTrue(anchor_generator_object._normalize_coordinates)\n\n  def test_build_multiscale_anchor_generator_with_anchors_in_pixel_coordinates(\n      self):\n    anchor_generator_text_proto = """"""\n      multiscale_anchor_generator {\n        aspect_ratios: [1.0]\n        normalize_coordinates: false\n      }\n    """"""\n    anchor_generator_proto = anchor_generator_pb2.AnchorGenerator()\n    text_format.Merge(anchor_generator_text_proto, anchor_generator_proto)\n    anchor_generator_object = anchor_generator_builder.build(\n        anchor_generator_proto)\n    self.assertTrue(isinstance(anchor_generator_object,\n                               multiscale_grid_anchor_generator.\n                               MultiscaleGridAnchorGenerator))\n    self.assertFalse(anchor_generator_object._normalize_coordinates)\n\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
src/object_detection/builders/box_coder_builder.py,0,"b'# Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n""""""A function to build an object detection box coder from configuration.""""""\nfrom object_detection.box_coders import faster_rcnn_box_coder\nfrom object_detection.box_coders import keypoint_box_coder\nfrom object_detection.box_coders import mean_stddev_box_coder\nfrom object_detection.box_coders import square_box_coder\nfrom object_detection.protos import box_coder_pb2\n\n\ndef build(box_coder_config):\n  """"""Builds a box coder object based on the box coder config.\n\n  Args:\n    box_coder_config: A box_coder.proto object containing the config for the\n      desired box coder.\n\n  Returns:\n    BoxCoder based on the config.\n\n  Raises:\n    ValueError: On empty box coder proto.\n  """"""\n  if not isinstance(box_coder_config, box_coder_pb2.BoxCoder):\n    raise ValueError(\'box_coder_config not of type box_coder_pb2.BoxCoder.\')\n\n  if box_coder_config.WhichOneof(\'box_coder_oneof\') == \'faster_rcnn_box_coder\':\n    return faster_rcnn_box_coder.FasterRcnnBoxCoder(scale_factors=[\n        box_coder_config.faster_rcnn_box_coder.y_scale,\n        box_coder_config.faster_rcnn_box_coder.x_scale,\n        box_coder_config.faster_rcnn_box_coder.height_scale,\n        box_coder_config.faster_rcnn_box_coder.width_scale\n    ])\n  if box_coder_config.WhichOneof(\'box_coder_oneof\') == \'keypoint_box_coder\':\n    return keypoint_box_coder.KeypointBoxCoder(\n        box_coder_config.keypoint_box_coder.num_keypoints,\n        scale_factors=[\n            box_coder_config.keypoint_box_coder.y_scale,\n            box_coder_config.keypoint_box_coder.x_scale,\n            box_coder_config.keypoint_box_coder.height_scale,\n            box_coder_config.keypoint_box_coder.width_scale\n        ])\n  if (box_coder_config.WhichOneof(\'box_coder_oneof\') ==\n      \'mean_stddev_box_coder\'):\n    return mean_stddev_box_coder.MeanStddevBoxCoder()\n  if box_coder_config.WhichOneof(\'box_coder_oneof\') == \'square_box_coder\':\n    return square_box_coder.SquareBoxCoder(scale_factors=[\n        box_coder_config.square_box_coder.y_scale,\n        box_coder_config.square_box_coder.x_scale,\n        box_coder_config.square_box_coder.length_scale\n    ])\n  raise ValueError(\'Empty box coder.\')\n'"
src/object_detection/builders/box_coder_builder_test.py,2,"b'# Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n""""""Tests for box_coder_builder.""""""\n\nimport tensorflow as tf\n\nfrom google.protobuf import text_format\nfrom object_detection.box_coders import faster_rcnn_box_coder\nfrom object_detection.box_coders import keypoint_box_coder\nfrom object_detection.box_coders import mean_stddev_box_coder\nfrom object_detection.box_coders import square_box_coder\nfrom object_detection.builders import box_coder_builder\nfrom object_detection.protos import box_coder_pb2\n\n\nclass BoxCoderBuilderTest(tf.test.TestCase):\n\n  def test_build_faster_rcnn_box_coder_with_defaults(self):\n    box_coder_text_proto = """"""\n      faster_rcnn_box_coder {\n      }\n    """"""\n    box_coder_proto = box_coder_pb2.BoxCoder()\n    text_format.Merge(box_coder_text_proto, box_coder_proto)\n    box_coder_object = box_coder_builder.build(box_coder_proto)\n    self.assertIsInstance(box_coder_object,\n                          faster_rcnn_box_coder.FasterRcnnBoxCoder)\n    self.assertEqual(box_coder_object._scale_factors, [10.0, 10.0, 5.0, 5.0])\n\n  def test_build_faster_rcnn_box_coder_with_non_default_parameters(self):\n    box_coder_text_proto = """"""\n      faster_rcnn_box_coder {\n        y_scale: 6.0\n        x_scale: 3.0\n        height_scale: 7.0\n        width_scale: 8.0\n      }\n    """"""\n    box_coder_proto = box_coder_pb2.BoxCoder()\n    text_format.Merge(box_coder_text_proto, box_coder_proto)\n    box_coder_object = box_coder_builder.build(box_coder_proto)\n    self.assertIsInstance(box_coder_object,\n                          faster_rcnn_box_coder.FasterRcnnBoxCoder)\n    self.assertEqual(box_coder_object._scale_factors, [6.0, 3.0, 7.0, 8.0])\n\n  def test_build_keypoint_box_coder_with_defaults(self):\n    box_coder_text_proto = """"""\n      keypoint_box_coder {\n      }\n    """"""\n    box_coder_proto = box_coder_pb2.BoxCoder()\n    text_format.Merge(box_coder_text_proto, box_coder_proto)\n    box_coder_object = box_coder_builder.build(box_coder_proto)\n    self.assertIsInstance(box_coder_object, keypoint_box_coder.KeypointBoxCoder)\n    self.assertEqual(box_coder_object._scale_factors, [10.0, 10.0, 5.0, 5.0])\n\n  def test_build_keypoint_box_coder_with_non_default_parameters(self):\n    box_coder_text_proto = """"""\n      keypoint_box_coder {\n        num_keypoints: 6\n        y_scale: 6.0\n        x_scale: 3.0\n        height_scale: 7.0\n        width_scale: 8.0\n      }\n    """"""\n    box_coder_proto = box_coder_pb2.BoxCoder()\n    text_format.Merge(box_coder_text_proto, box_coder_proto)\n    box_coder_object = box_coder_builder.build(box_coder_proto)\n    self.assertIsInstance(box_coder_object, keypoint_box_coder.KeypointBoxCoder)\n    self.assertEqual(box_coder_object._num_keypoints, 6)\n    self.assertEqual(box_coder_object._scale_factors, [6.0, 3.0, 7.0, 8.0])\n\n  def test_build_mean_stddev_box_coder(self):\n    box_coder_text_proto = """"""\n      mean_stddev_box_coder {\n      }\n    """"""\n    box_coder_proto = box_coder_pb2.BoxCoder()\n    text_format.Merge(box_coder_text_proto, box_coder_proto)\n    box_coder_object = box_coder_builder.build(box_coder_proto)\n    self.assertTrue(\n        isinstance(box_coder_object,\n                   mean_stddev_box_coder.MeanStddevBoxCoder))\n\n  def test_build_square_box_coder_with_defaults(self):\n    box_coder_text_proto = """"""\n      square_box_coder {\n      }\n    """"""\n    box_coder_proto = box_coder_pb2.BoxCoder()\n    text_format.Merge(box_coder_text_proto, box_coder_proto)\n    box_coder_object = box_coder_builder.build(box_coder_proto)\n    self.assertTrue(\n        isinstance(box_coder_object, square_box_coder.SquareBoxCoder))\n    self.assertEqual(box_coder_object._scale_factors, [10.0, 10.0, 5.0])\n\n  def test_build_square_box_coder_with_non_default_parameters(self):\n    box_coder_text_proto = """"""\n      square_box_coder {\n        y_scale: 6.0\n        x_scale: 3.0\n        length_scale: 7.0\n      }\n    """"""\n    box_coder_proto = box_coder_pb2.BoxCoder()\n    text_format.Merge(box_coder_text_proto, box_coder_proto)\n    box_coder_object = box_coder_builder.build(box_coder_proto)\n    self.assertTrue(\n        isinstance(box_coder_object, square_box_coder.SquareBoxCoder))\n    self.assertEqual(box_coder_object._scale_factors, [6.0, 3.0, 7.0])\n\n  def test_raise_error_on_empty_box_coder(self):\n    box_coder_text_proto = """"""\n    """"""\n    box_coder_proto = box_coder_pb2.BoxCoder()\n    text_format.Merge(box_coder_text_proto, box_coder_proto)\n    with self.assertRaises(ValueError):\n      box_coder_builder.build(box_coder_proto)\n\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
src/object_detection/builders/box_predictor_builder.py,0,"b'# Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n""""""Function to build box predictor from configuration.""""""\n\nfrom object_detection.core import box_predictor\nfrom object_detection.protos import box_predictor_pb2\n\n\ndef build(argscope_fn, box_predictor_config, is_training, num_classes):\n  """"""Builds box predictor based on the configuration.\n\n  Builds box predictor based on the configuration. See box_predictor.proto for\n  configurable options. Also, see box_predictor.py for more details.\n\n  Args:\n    argscope_fn: A function that takes the following inputs:\n        * hyperparams_pb2.Hyperparams proto\n        * a boolean indicating if the model is in training mode.\n      and returns a tf slim argscope for Conv and FC hyperparameters.\n    box_predictor_config: box_predictor_pb2.BoxPredictor proto containing\n      configuration.\n    is_training: Whether the models is in training mode.\n    num_classes: Number of classes to predict.\n\n  Returns:\n    box_predictor: box_predictor.BoxPredictor object.\n\n  Raises:\n    ValueError: On unknown box predictor.\n  """"""\n  if not isinstance(box_predictor_config, box_predictor_pb2.BoxPredictor):\n    raise ValueError(\'box_predictor_config not of type \'\n                     \'box_predictor_pb2.BoxPredictor.\')\n\n  box_predictor_oneof = box_predictor_config.WhichOneof(\'box_predictor_oneof\')\n\n  if  box_predictor_oneof == \'convolutional_box_predictor\':\n    conv_box_predictor = box_predictor_config.convolutional_box_predictor\n    conv_hyperparams = argscope_fn(conv_box_predictor.conv_hyperparams,\n                                   is_training)\n    box_predictor_object = box_predictor.ConvolutionalBoxPredictor(\n        is_training=is_training,\n        num_classes=num_classes,\n        conv_hyperparams=conv_hyperparams,\n        min_depth=conv_box_predictor.min_depth,\n        max_depth=conv_box_predictor.max_depth,\n        num_layers_before_predictor=(conv_box_predictor.\n                                     num_layers_before_predictor),\n        use_dropout=conv_box_predictor.use_dropout,\n        dropout_keep_prob=conv_box_predictor.dropout_keep_probability,\n        kernel_size=conv_box_predictor.kernel_size,\n        box_code_size=conv_box_predictor.box_code_size,\n        apply_sigmoid_to_scores=conv_box_predictor.apply_sigmoid_to_scores,\n        class_prediction_bias_init=(conv_box_predictor.\n                                    class_prediction_bias_init),\n        use_depthwise=conv_box_predictor.use_depthwise\n    )\n    return box_predictor_object\n\n  if  box_predictor_oneof == \'weight_shared_convolutional_box_predictor\':\n    conv_box_predictor = (box_predictor_config.\n                          weight_shared_convolutional_box_predictor)\n    conv_hyperparams = argscope_fn(conv_box_predictor.conv_hyperparams,\n                                   is_training)\n    box_predictor_object = box_predictor.WeightSharedConvolutionalBoxPredictor(\n        is_training=is_training,\n        num_classes=num_classes,\n        conv_hyperparams=conv_hyperparams,\n        depth=conv_box_predictor.depth,\n        num_layers_before_predictor=(conv_box_predictor.\n                                     num_layers_before_predictor),\n        kernel_size=conv_box_predictor.kernel_size,\n        box_code_size=conv_box_predictor.box_code_size,\n        class_prediction_bias_init=conv_box_predictor.class_prediction_bias_init\n    )\n    return box_predictor_object\n\n  if box_predictor_oneof == \'mask_rcnn_box_predictor\':\n    mask_rcnn_box_predictor = box_predictor_config.mask_rcnn_box_predictor\n    fc_hyperparams = argscope_fn(mask_rcnn_box_predictor.fc_hyperparams,\n                                 is_training)\n    conv_hyperparams = None\n    if mask_rcnn_box_predictor.HasField(\'conv_hyperparams\'):\n      conv_hyperparams = argscope_fn(mask_rcnn_box_predictor.conv_hyperparams,\n                                     is_training)\n    box_predictor_object = box_predictor.MaskRCNNBoxPredictor(\n        is_training=is_training,\n        num_classes=num_classes,\n        fc_hyperparams=fc_hyperparams,\n        use_dropout=mask_rcnn_box_predictor.use_dropout,\n        dropout_keep_prob=mask_rcnn_box_predictor.dropout_keep_probability,\n        box_code_size=mask_rcnn_box_predictor.box_code_size,\n        conv_hyperparams=conv_hyperparams,\n        predict_instance_masks=mask_rcnn_box_predictor.predict_instance_masks,\n        mask_height=mask_rcnn_box_predictor.mask_height,\n        mask_width=mask_rcnn_box_predictor.mask_width,\n        mask_prediction_num_conv_layers=(\n            mask_rcnn_box_predictor.mask_prediction_num_conv_layers),\n        mask_prediction_conv_depth=(\n            mask_rcnn_box_predictor.mask_prediction_conv_depth),\n        predict_keypoints=mask_rcnn_box_predictor.predict_keypoints)\n    return box_predictor_object\n\n  if box_predictor_oneof == \'rfcn_box_predictor\':\n    rfcn_box_predictor = box_predictor_config.rfcn_box_predictor\n    conv_hyperparams = argscope_fn(rfcn_box_predictor.conv_hyperparams,\n                                   is_training)\n    box_predictor_object = box_predictor.RfcnBoxPredictor(\n        is_training=is_training,\n        num_classes=num_classes,\n        conv_hyperparams=conv_hyperparams,\n        crop_size=[rfcn_box_predictor.crop_height,\n                   rfcn_box_predictor.crop_width],\n        num_spatial_bins=[rfcn_box_predictor.num_spatial_bins_height,\n                          rfcn_box_predictor.num_spatial_bins_width],\n        depth=rfcn_box_predictor.depth,\n        box_code_size=rfcn_box_predictor.box_code_size)\n    return box_predictor_object\n  raise ValueError(\'Unknown box predictor: {}\'.format(box_predictor_oneof))\n'"
src/object_detection/builders/box_predictor_builder_test.py,5,"b'# Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n""""""Tests for box_predictor_builder.""""""\nimport mock\nimport tensorflow as tf\n\nfrom google.protobuf import text_format\nfrom object_detection.builders import box_predictor_builder\nfrom object_detection.builders import hyperparams_builder\nfrom object_detection.protos import box_predictor_pb2\nfrom object_detection.protos import hyperparams_pb2\n\n\nclass ConvolutionalBoxPredictorBuilderTest(tf.test.TestCase):\n\n  def test_box_predictor_calls_conv_argscope_fn(self):\n    conv_hyperparams_text_proto = """"""\n      regularizer {\n        l1_regularizer {\n          weight: 0.0003\n        }\n      }\n      initializer {\n        truncated_normal_initializer {\n          mean: 0.0\n          stddev: 0.3\n        }\n      }\n      activation: RELU_6\n    """"""\n    hyperparams_proto = hyperparams_pb2.Hyperparams()\n    text_format.Merge(conv_hyperparams_text_proto, hyperparams_proto)\n    def mock_conv_argscope_builder(conv_hyperparams_arg, is_training):\n      return (conv_hyperparams_arg, is_training)\n\n    box_predictor_proto = box_predictor_pb2.BoxPredictor()\n    box_predictor_proto.convolutional_box_predictor.conv_hyperparams.CopyFrom(\n        hyperparams_proto)\n    box_predictor = box_predictor_builder.build(\n        argscope_fn=mock_conv_argscope_builder,\n        box_predictor_config=box_predictor_proto,\n        is_training=False,\n        num_classes=10)\n    (conv_hyperparams_actual, is_training) = box_predictor._conv_hyperparams\n    self.assertAlmostEqual((hyperparams_proto.regularizer.\n                            l1_regularizer.weight),\n                           (conv_hyperparams_actual.regularizer.l1_regularizer.\n                            weight))\n    self.assertAlmostEqual((hyperparams_proto.initializer.\n                            truncated_normal_initializer.stddev),\n                           (conv_hyperparams_actual.initializer.\n                            truncated_normal_initializer.stddev))\n    self.assertAlmostEqual((hyperparams_proto.initializer.\n                            truncated_normal_initializer.mean),\n                           (conv_hyperparams_actual.initializer.\n                            truncated_normal_initializer.mean))\n    self.assertEqual(hyperparams_proto.activation,\n                     conv_hyperparams_actual.activation)\n    self.assertFalse(is_training)\n\n  def test_construct_non_default_conv_box_predictor(self):\n    box_predictor_text_proto = """"""\n      convolutional_box_predictor {\n        min_depth: 2\n        max_depth: 16\n        num_layers_before_predictor: 2\n        use_dropout: false\n        dropout_keep_probability: 0.4\n        kernel_size: 3\n        box_code_size: 3\n        apply_sigmoid_to_scores: true\n        class_prediction_bias_init: 4.0\n        use_depthwise: true\n      }\n    """"""\n    conv_hyperparams_text_proto = """"""\n      regularizer {\n        l1_regularizer {\n        }\n      }\n      initializer {\n        truncated_normal_initializer {\n        }\n      }\n    """"""\n    hyperparams_proto = hyperparams_pb2.Hyperparams()\n    text_format.Merge(conv_hyperparams_text_proto, hyperparams_proto)\n    def mock_conv_argscope_builder(conv_hyperparams_arg, is_training):\n      return (conv_hyperparams_arg, is_training)\n\n    box_predictor_proto = box_predictor_pb2.BoxPredictor()\n    text_format.Merge(box_predictor_text_proto, box_predictor_proto)\n    box_predictor_proto.convolutional_box_predictor.conv_hyperparams.CopyFrom(\n        hyperparams_proto)\n    box_predictor = box_predictor_builder.build(\n        argscope_fn=mock_conv_argscope_builder,\n        box_predictor_config=box_predictor_proto,\n        is_training=False,\n        num_classes=10)\n    self.assertEqual(box_predictor._min_depth, 2)\n    self.assertEqual(box_predictor._max_depth, 16)\n    self.assertEqual(box_predictor._num_layers_before_predictor, 2)\n    self.assertFalse(box_predictor._use_dropout)\n    self.assertAlmostEqual(box_predictor._dropout_keep_prob, 0.4)\n    self.assertTrue(box_predictor._apply_sigmoid_to_scores)\n    self.assertAlmostEqual(box_predictor._class_prediction_bias_init, 4.0)\n    self.assertEqual(box_predictor.num_classes, 10)\n    self.assertFalse(box_predictor._is_training)\n    self.assertTrue(box_predictor._use_depthwise)\n\n  def test_construct_default_conv_box_predictor(self):\n    box_predictor_text_proto = """"""\n      convolutional_box_predictor {\n        conv_hyperparams {\n          regularizer {\n            l1_regularizer {\n            }\n          }\n          initializer {\n            truncated_normal_initializer {\n            }\n          }\n        }\n      }""""""\n    box_predictor_proto = box_predictor_pb2.BoxPredictor()\n    text_format.Merge(box_predictor_text_proto, box_predictor_proto)\n    box_predictor = box_predictor_builder.build(\n        argscope_fn=hyperparams_builder.build,\n        box_predictor_config=box_predictor_proto,\n        is_training=True,\n        num_classes=90)\n    self.assertEqual(box_predictor._min_depth, 0)\n    self.assertEqual(box_predictor._max_depth, 0)\n    self.assertEqual(box_predictor._num_layers_before_predictor, 0)\n    self.assertTrue(box_predictor._use_dropout)\n    self.assertAlmostEqual(box_predictor._dropout_keep_prob, 0.8)\n    self.assertFalse(box_predictor._apply_sigmoid_to_scores)\n    self.assertEqual(box_predictor.num_classes, 90)\n    self.assertTrue(box_predictor._is_training)\n    self.assertFalse(box_predictor._use_depthwise)\n\n\nclass WeightSharedConvolutionalBoxPredictorBuilderTest(tf.test.TestCase):\n\n  def test_box_predictor_calls_conv_argscope_fn(self):\n    conv_hyperparams_text_proto = """"""\n      regularizer {\n        l1_regularizer {\n          weight: 0.0003\n        }\n      }\n      initializer {\n        truncated_normal_initializer {\n          mean: 0.0\n          stddev: 0.3\n        }\n      }\n      activation: RELU_6\n    """"""\n    hyperparams_proto = hyperparams_pb2.Hyperparams()\n    text_format.Merge(conv_hyperparams_text_proto, hyperparams_proto)\n    def mock_conv_argscope_builder(conv_hyperparams_arg, is_training):\n      return (conv_hyperparams_arg, is_training)\n\n    box_predictor_proto = box_predictor_pb2.BoxPredictor()\n    (box_predictor_proto.weight_shared_convolutional_box_predictor\n     .conv_hyperparams.CopyFrom(hyperparams_proto))\n    box_predictor = box_predictor_builder.build(\n        argscope_fn=mock_conv_argscope_builder,\n        box_predictor_config=box_predictor_proto,\n        is_training=False,\n        num_classes=10)\n    (conv_hyperparams_actual, is_training) = box_predictor._conv_hyperparams\n    self.assertAlmostEqual((hyperparams_proto.regularizer.\n                            l1_regularizer.weight),\n                           (conv_hyperparams_actual.regularizer.l1_regularizer.\n                            weight))\n    self.assertAlmostEqual((hyperparams_proto.initializer.\n                            truncated_normal_initializer.stddev),\n                           (conv_hyperparams_actual.initializer.\n                            truncated_normal_initializer.stddev))\n    self.assertAlmostEqual((hyperparams_proto.initializer.\n                            truncated_normal_initializer.mean),\n                           (conv_hyperparams_actual.initializer.\n                            truncated_normal_initializer.mean))\n    self.assertEqual(hyperparams_proto.activation,\n                     conv_hyperparams_actual.activation)\n    self.assertFalse(is_training)\n\n  def test_construct_non_default_conv_box_predictor(self):\n    box_predictor_text_proto = """"""\n      weight_shared_convolutional_box_predictor {\n        depth: 2\n        num_layers_before_predictor: 2\n        kernel_size: 7\n        box_code_size: 3\n        class_prediction_bias_init: 4.0\n      }\n    """"""\n    conv_hyperparams_text_proto = """"""\n      regularizer {\n        l1_regularizer {\n        }\n      }\n      initializer {\n        truncated_normal_initializer {\n        }\n      }\n    """"""\n    hyperparams_proto = hyperparams_pb2.Hyperparams()\n    text_format.Merge(conv_hyperparams_text_proto, hyperparams_proto)\n    def mock_conv_argscope_builder(conv_hyperparams_arg, is_training):\n      return (conv_hyperparams_arg, is_training)\n\n    box_predictor_proto = box_predictor_pb2.BoxPredictor()\n    text_format.Merge(box_predictor_text_proto, box_predictor_proto)\n    (box_predictor_proto.weight_shared_convolutional_box_predictor.\n     conv_hyperparams.CopyFrom(hyperparams_proto))\n    box_predictor = box_predictor_builder.build(\n        argscope_fn=mock_conv_argscope_builder,\n        box_predictor_config=box_predictor_proto,\n        is_training=False,\n        num_classes=10)\n    self.assertEqual(box_predictor._depth, 2)\n    self.assertEqual(box_predictor._num_layers_before_predictor, 2)\n    self.assertAlmostEqual(box_predictor._class_prediction_bias_init, 4.0)\n    self.assertEqual(box_predictor.num_classes, 10)\n    self.assertFalse(box_predictor._is_training)\n\n  def test_construct_default_conv_box_predictor(self):\n    box_predictor_text_proto = """"""\n      weight_shared_convolutional_box_predictor {\n        conv_hyperparams {\n          regularizer {\n            l1_regularizer {\n            }\n          }\n          initializer {\n            truncated_normal_initializer {\n            }\n          }\n        }\n      }""""""\n    box_predictor_proto = box_predictor_pb2.BoxPredictor()\n    text_format.Merge(box_predictor_text_proto, box_predictor_proto)\n    box_predictor = box_predictor_builder.build(\n        argscope_fn=hyperparams_builder.build,\n        box_predictor_config=box_predictor_proto,\n        is_training=True,\n        num_classes=90)\n    self.assertEqual(box_predictor._depth, 0)\n    self.assertEqual(box_predictor._num_layers_before_predictor, 0)\n    self.assertEqual(box_predictor.num_classes, 90)\n    self.assertTrue(box_predictor._is_training)\n\n\nclass MaskRCNNBoxPredictorBuilderTest(tf.test.TestCase):\n\n  def test_box_predictor_builder_calls_fc_argscope_fn(self):\n    fc_hyperparams_text_proto = """"""\n      regularizer {\n        l1_regularizer {\n          weight: 0.0003\n        }\n      }\n      initializer {\n        truncated_normal_initializer {\n          mean: 0.0\n          stddev: 0.3\n        }\n      }\n      activation: RELU_6\n      op: FC\n    """"""\n    hyperparams_proto = hyperparams_pb2.Hyperparams()\n    text_format.Merge(fc_hyperparams_text_proto, hyperparams_proto)\n    box_predictor_proto = box_predictor_pb2.BoxPredictor()\n    box_predictor_proto.mask_rcnn_box_predictor.fc_hyperparams.CopyFrom(\n        hyperparams_proto)\n    mock_argscope_fn = mock.Mock(return_value=\'arg_scope\')\n    box_predictor = box_predictor_builder.build(\n        argscope_fn=mock_argscope_fn,\n        box_predictor_config=box_predictor_proto,\n        is_training=False,\n        num_classes=10)\n    mock_argscope_fn.assert_called_with(hyperparams_proto, False)\n    self.assertEqual(box_predictor._fc_hyperparams, \'arg_scope\')\n\n  def test_non_default_mask_rcnn_box_predictor(self):\n    fc_hyperparams_text_proto = """"""\n      regularizer {\n        l1_regularizer {\n        }\n      }\n      initializer {\n        truncated_normal_initializer {\n        }\n      }\n      activation: RELU_6\n      op: FC\n    """"""\n    box_predictor_text_proto = """"""\n      mask_rcnn_box_predictor {\n        use_dropout: true\n        dropout_keep_probability: 0.8\n        box_code_size: 3\n      }\n    """"""\n    hyperparams_proto = hyperparams_pb2.Hyperparams()\n    text_format.Merge(fc_hyperparams_text_proto, hyperparams_proto)\n    def mock_fc_argscope_builder(fc_hyperparams_arg, is_training):\n      return (fc_hyperparams_arg, is_training)\n\n    box_predictor_proto = box_predictor_pb2.BoxPredictor()\n    text_format.Merge(box_predictor_text_proto, box_predictor_proto)\n    box_predictor_proto.mask_rcnn_box_predictor.fc_hyperparams.CopyFrom(\n        hyperparams_proto)\n    box_predictor = box_predictor_builder.build(\n        argscope_fn=mock_fc_argscope_builder,\n        box_predictor_config=box_predictor_proto,\n        is_training=True,\n        num_classes=90)\n    self.assertTrue(box_predictor._use_dropout)\n    self.assertAlmostEqual(box_predictor._dropout_keep_prob, 0.8)\n    self.assertEqual(box_predictor.num_classes, 90)\n    self.assertTrue(box_predictor._is_training)\n    self.assertEqual(box_predictor._box_code_size, 3)\n\n  def test_build_default_mask_rcnn_box_predictor(self):\n    box_predictor_proto = box_predictor_pb2.BoxPredictor()\n    box_predictor_proto.mask_rcnn_box_predictor.fc_hyperparams.op = (\n        hyperparams_pb2.Hyperparams.FC)\n    box_predictor = box_predictor_builder.build(\n        argscope_fn=mock.Mock(return_value=\'arg_scope\'),\n        box_predictor_config=box_predictor_proto,\n        is_training=True,\n        num_classes=90)\n    self.assertFalse(box_predictor._use_dropout)\n    self.assertAlmostEqual(box_predictor._dropout_keep_prob, 0.5)\n    self.assertEqual(box_predictor.num_classes, 90)\n    self.assertTrue(box_predictor._is_training)\n    self.assertEqual(box_predictor._box_code_size, 4)\n    self.assertFalse(box_predictor._predict_instance_masks)\n    self.assertFalse(box_predictor._predict_keypoints)\n\n  def test_build_box_predictor_with_mask_branch(self):\n    box_predictor_proto = box_predictor_pb2.BoxPredictor()\n    box_predictor_proto.mask_rcnn_box_predictor.fc_hyperparams.op = (\n        hyperparams_pb2.Hyperparams.FC)\n    box_predictor_proto.mask_rcnn_box_predictor.conv_hyperparams.op = (\n        hyperparams_pb2.Hyperparams.CONV)\n    box_predictor_proto.mask_rcnn_box_predictor.predict_instance_masks = True\n    box_predictor_proto.mask_rcnn_box_predictor.mask_prediction_conv_depth = 512\n    box_predictor_proto.mask_rcnn_box_predictor.mask_height = 16\n    box_predictor_proto.mask_rcnn_box_predictor.mask_width = 16\n    mock_argscope_fn = mock.Mock(return_value=\'arg_scope\')\n    box_predictor = box_predictor_builder.build(\n        argscope_fn=mock_argscope_fn,\n        box_predictor_config=box_predictor_proto,\n        is_training=True,\n        num_classes=90)\n    mock_argscope_fn.assert_has_calls(\n        [mock.call(box_predictor_proto.mask_rcnn_box_predictor.fc_hyperparams,\n                   True),\n         mock.call(box_predictor_proto.mask_rcnn_box_predictor.conv_hyperparams,\n                   True)], any_order=True)\n    self.assertFalse(box_predictor._use_dropout)\n    self.assertAlmostEqual(box_predictor._dropout_keep_prob, 0.5)\n    self.assertEqual(box_predictor.num_classes, 90)\n    self.assertTrue(box_predictor._is_training)\n    self.assertEqual(box_predictor._box_code_size, 4)\n    self.assertTrue(box_predictor._predict_instance_masks)\n    self.assertEqual(box_predictor._mask_prediction_conv_depth, 512)\n    self.assertFalse(box_predictor._predict_keypoints)\n\n\nclass RfcnBoxPredictorBuilderTest(tf.test.TestCase):\n\n  def test_box_predictor_calls_fc_argscope_fn(self):\n    conv_hyperparams_text_proto = """"""\n      regularizer {\n        l1_regularizer {\n          weight: 0.0003\n        }\n      }\n      initializer {\n        truncated_normal_initializer {\n          mean: 0.0\n          stddev: 0.3\n        }\n      }\n      activation: RELU_6\n    """"""\n    hyperparams_proto = hyperparams_pb2.Hyperparams()\n    text_format.Merge(conv_hyperparams_text_proto, hyperparams_proto)\n    def mock_conv_argscope_builder(conv_hyperparams_arg, is_training):\n      return (conv_hyperparams_arg, is_training)\n\n    box_predictor_proto = box_predictor_pb2.BoxPredictor()\n    box_predictor_proto.rfcn_box_predictor.conv_hyperparams.CopyFrom(\n        hyperparams_proto)\n    box_predictor = box_predictor_builder.build(\n        argscope_fn=mock_conv_argscope_builder,\n        box_predictor_config=box_predictor_proto,\n        is_training=False,\n        num_classes=10)\n    (conv_hyperparams_actual, is_training) = box_predictor._conv_hyperparams\n    self.assertAlmostEqual((hyperparams_proto.regularizer.\n                            l1_regularizer.weight),\n                           (conv_hyperparams_actual.regularizer.l1_regularizer.\n                            weight))\n    self.assertAlmostEqual((hyperparams_proto.initializer.\n                            truncated_normal_initializer.stddev),\n                           (conv_hyperparams_actual.initializer.\n                            truncated_normal_initializer.stddev))\n    self.assertAlmostEqual((hyperparams_proto.initializer.\n                            truncated_normal_initializer.mean),\n                           (conv_hyperparams_actual.initializer.\n                            truncated_normal_initializer.mean))\n    self.assertEqual(hyperparams_proto.activation,\n                     conv_hyperparams_actual.activation)\n    self.assertFalse(is_training)\n\n  def test_non_default_rfcn_box_predictor(self):\n    conv_hyperparams_text_proto = """"""\n      regularizer {\n        l1_regularizer {\n        }\n      }\n      initializer {\n        truncated_normal_initializer {\n        }\n      }\n      activation: RELU_6\n    """"""\n    box_predictor_text_proto = """"""\n      rfcn_box_predictor {\n        num_spatial_bins_height: 4\n        num_spatial_bins_width: 4\n        depth: 4\n        box_code_size: 3\n        crop_height: 16\n        crop_width: 16\n      }\n    """"""\n    hyperparams_proto = hyperparams_pb2.Hyperparams()\n    text_format.Merge(conv_hyperparams_text_proto, hyperparams_proto)\n    def mock_conv_argscope_builder(conv_hyperparams_arg, is_training):\n      return (conv_hyperparams_arg, is_training)\n\n    box_predictor_proto = box_predictor_pb2.BoxPredictor()\n    text_format.Merge(box_predictor_text_proto, box_predictor_proto)\n    box_predictor_proto.rfcn_box_predictor.conv_hyperparams.CopyFrom(\n        hyperparams_proto)\n    box_predictor = box_predictor_builder.build(\n        argscope_fn=mock_conv_argscope_builder,\n        box_predictor_config=box_predictor_proto,\n        is_training=True,\n        num_classes=90)\n    self.assertEqual(box_predictor.num_classes, 90)\n    self.assertTrue(box_predictor._is_training)\n    self.assertEqual(box_predictor._box_code_size, 3)\n    self.assertEqual(box_predictor._num_spatial_bins, [4, 4])\n    self.assertEqual(box_predictor._crop_size, [16, 16])\n\n  def test_default_rfcn_box_predictor(self):\n    conv_hyperparams_text_proto = """"""\n      regularizer {\n        l1_regularizer {\n        }\n      }\n      initializer {\n        truncated_normal_initializer {\n        }\n      }\n      activation: RELU_6\n    """"""\n    hyperparams_proto = hyperparams_pb2.Hyperparams()\n    text_format.Merge(conv_hyperparams_text_proto, hyperparams_proto)\n    def mock_conv_argscope_builder(conv_hyperparams_arg, is_training):\n      return (conv_hyperparams_arg, is_training)\n\n    box_predictor_proto = box_predictor_pb2.BoxPredictor()\n    box_predictor_proto.rfcn_box_predictor.conv_hyperparams.CopyFrom(\n        hyperparams_proto)\n    box_predictor = box_predictor_builder.build(\n        argscope_fn=mock_conv_argscope_builder,\n        box_predictor_config=box_predictor_proto,\n        is_training=True,\n        num_classes=90)\n    self.assertEqual(box_predictor.num_classes, 90)\n    self.assertTrue(box_predictor._is_training)\n    self.assertEqual(box_predictor._box_code_size, 4)\n    self.assertEqual(box_predictor._num_spatial_bins, [3, 3])\n    self.assertEqual(box_predictor._crop_size, [12, 12])\n\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
src/object_detection/builders/dataset_builder.py,7,"b'# Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""tf.data.Dataset builder.\n\nCreates data sources for DetectionModels from an InputReader config. See\ninput_reader.proto for options.\n\nNote: If users wishes to also use their own InputReaders with the Object\nDetection configuration framework, they should define their own builder function\nthat wraps the build function.\n""""""\nimport functools\nimport tensorflow as tf\n\nfrom object_detection.core import standard_fields as fields\nfrom object_detection.data_decoders import tf_example_decoder\nfrom object_detection.protos import input_reader_pb2\nfrom object_detection.utils import dataset_util\n\n\ndef _get_padding_shapes(dataset, max_num_boxes=None, num_classes=None,\n                        spatial_image_shape=None):\n  """"""Returns shapes to pad dataset tensors to before batching.\n\n  Args:\n    dataset: tf.data.Dataset object.\n    max_num_boxes: Max number of groundtruth boxes needed to computes shapes for\n      padding.\n    num_classes: Number of classes in the dataset needed to compute shapes for\n      padding.\n    spatial_image_shape: A list of two integers of the form [height, width]\n      containing expected spatial shape of the image.\n\n  Returns:\n    A dictionary keyed by fields.InputDataFields containing padding shapes for\n    tensors in the dataset.\n\n  Raises:\n    ValueError: If groundtruth classes is neither rank 1 nor rank 2.\n  """"""\n\n  if not spatial_image_shape or spatial_image_shape == [-1, -1]:\n    height, width = None, None\n  else:\n    height, width = spatial_image_shape  # pylint: disable=unpacking-non-sequence\n\n  padding_shapes = {\n      fields.InputDataFields.image: [height, width, 3],\n      fields.InputDataFields.source_id: [],\n      fields.InputDataFields.filename: [],\n      fields.InputDataFields.key: [],\n      fields.InputDataFields.groundtruth_difficult: [max_num_boxes],\n      fields.InputDataFields.groundtruth_boxes: [max_num_boxes, 4],\n      fields.InputDataFields.groundtruth_instance_masks: [max_num_boxes, height,\n                                                          width],\n      fields.InputDataFields.groundtruth_is_crowd: [max_num_boxes],\n      fields.InputDataFields.groundtruth_group_of: [max_num_boxes],\n      fields.InputDataFields.groundtruth_area: [max_num_boxes],\n      fields.InputDataFields.groundtruth_weights: [max_num_boxes],\n      fields.InputDataFields.num_groundtruth_boxes: [],\n      fields.InputDataFields.groundtruth_label_types: [max_num_boxes],\n      fields.InputDataFields.groundtruth_label_scores: [max_num_boxes],\n      fields.InputDataFields.true_image_shape: [3]\n  }\n  # Determine whether groundtruth_classes are integers or one-hot encodings, and\n  # apply batching appropriately.\n  classes_shape = dataset.output_shapes[\n      fields.InputDataFields.groundtruth_classes]\n  if len(classes_shape) == 1:  # Class integers.\n    padding_shapes[fields.InputDataFields.groundtruth_classes] = [max_num_boxes]\n  elif len(classes_shape) == 2:  # One-hot or k-hot encoding.\n    padding_shapes[fields.InputDataFields.groundtruth_classes] = [\n        max_num_boxes, num_classes]\n  else:\n    raise ValueError(\'Groundtruth classes must be a rank 1 tensor (classes) or \'\n                     \'rank 2 tensor (one-hot encodings)\')\n\n  if fields.InputDataFields.original_image in dataset.output_shapes:\n    padding_shapes[fields.InputDataFields.original_image] = [None, None, 3]\n  if fields.InputDataFields.groundtruth_keypoints in dataset.output_shapes:\n    tensor_shape = dataset.output_shapes[fields.InputDataFields.\n                                         groundtruth_keypoints]\n    padding_shape = [max_num_boxes, tensor_shape[1].value,\n                     tensor_shape[2].value]\n    padding_shapes[fields.InputDataFields.groundtruth_keypoints] = padding_shape\n  if (fields.InputDataFields.groundtruth_keypoint_visibilities\n      in dataset.output_shapes):\n    tensor_shape = dataset.output_shapes[fields.InputDataFields.\n                                         groundtruth_keypoint_visibilities]\n    padding_shape = [max_num_boxes, tensor_shape[1].value]\n    padding_shapes[fields.InputDataFields.\n                   groundtruth_keypoint_visibilities] = padding_shape\n  return {tensor_key: padding_shapes[tensor_key]\n          for tensor_key, _ in dataset.output_shapes.items()}\n\n\ndef build(input_reader_config, transform_input_data_fn=None,\n          batch_size=None, max_num_boxes=None, num_classes=None,\n          spatial_image_shape=None):\n  """"""Builds a tf.data.Dataset.\n\n  Builds a tf.data.Dataset by applying the `transform_input_data_fn` on all\n  records. Applies a padded batch to the resulting dataset.\n\n  Args:\n    input_reader_config: A input_reader_pb2.InputReader object.\n    transform_input_data_fn: Function to apply to all records, or None if\n      no extra decoding is required.\n    batch_size: Batch size. If None, batching is not performed.\n    max_num_boxes: Max number of groundtruth boxes needed to compute shapes for\n      padding. If None, will use a dynamic shape.\n    num_classes: Number of classes in the dataset needed to compute shapes for\n      padding. If None, will use a dynamic shape.\n    spatial_image_shape: A list of two integers of the form [height, width]\n      containing expected spatial shape of the image after applying\n      transform_input_data_fn. If None, will use dynamic shapes.\n\n  Returns:\n    A tf.data.Dataset based on the input_reader_config.\n\n  Raises:\n    ValueError: On invalid input reader proto.\n    ValueError: If no input paths are specified.\n  """"""\n  if not isinstance(input_reader_config, input_reader_pb2.InputReader):\n    raise ValueError(\'input_reader_config not of type \'\n                     \'input_reader_pb2.InputReader.\')\n\n  if input_reader_config.WhichOneof(\'input_reader\') == \'tf_record_input_reader\':\n    config = input_reader_config.tf_record_input_reader\n    if not config.input_path:\n      raise ValueError(\'At least one input path must be specified in \'\n                       \'`input_reader_config`.\')\n\n    label_map_proto_file = None\n    if input_reader_config.HasField(\'label_map_path\'):\n      label_map_proto_file = input_reader_config.label_map_path\n    decoder = tf_example_decoder.TfExampleDecoder(\n        load_instance_masks=input_reader_config.load_instance_masks,\n        instance_mask_type=input_reader_config.mask_type,\n        label_map_proto_file=label_map_proto_file)\n\n    def process_fn(value):\n      processed = decoder.decode(value)\n      if transform_input_data_fn is not None:\n        return transform_input_data_fn(processed)\n      return processed\n\n    dataset = dataset_util.read_dataset(\n        functools.partial(tf.data.TFRecordDataset, buffer_size=8 * 1000 * 1000),\n        process_fn, config.input_path[:], input_reader_config)\n\n    if batch_size:\n      padding_shapes = _get_padding_shapes(dataset, max_num_boxes, num_classes,\n                                           spatial_image_shape)\n      dataset = dataset.apply(\n          tf.contrib.data.padded_batch_and_drop_remainder(batch_size,\n                                                          padding_shapes))\n    return dataset\n\n  raise ValueError(\'Unsupported input_reader_config.\')\n'"
src/object_detection/builders/dataset_builder_test.py,10,"b'# Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Tests for dataset_builder.""""""\n\nimport os\nimport numpy as np\nimport tensorflow as tf\n\nfrom google.protobuf import text_format\n\nfrom tensorflow.core.example import example_pb2\nfrom tensorflow.core.example import feature_pb2\nfrom object_detection.builders import dataset_builder\nfrom object_detection.core import standard_fields as fields\nfrom object_detection.protos import input_reader_pb2\nfrom object_detection.utils import dataset_util\n\n\nclass DatasetBuilderTest(tf.test.TestCase):\n\n  def create_tf_record(self):\n    path = os.path.join(self.get_temp_dir(), \'tfrecord\')\n    writer = tf.python_io.TFRecordWriter(path)\n\n    image_tensor = np.random.randint(255, size=(4, 5, 3)).astype(np.uint8)\n    flat_mask = (4 * 5) * [1.0]\n    with self.test_session():\n      encoded_jpeg = tf.image.encode_jpeg(tf.constant(image_tensor)).eval()\n    example = example_pb2.Example(\n        features=feature_pb2.Features(\n            feature={\n                \'image/encoded\':\n                    feature_pb2.Feature(\n                        bytes_list=feature_pb2.BytesList(value=[encoded_jpeg])),\n                \'image/format\':\n                    feature_pb2.Feature(\n                        bytes_list=feature_pb2.BytesList(\n                            value=[\'jpeg\'.encode(\'utf-8\')])),\n                \'image/height\':\n                    feature_pb2.Feature(\n                        int64_list=feature_pb2.Int64List(value=[4])),\n                \'image/width\':\n                    feature_pb2.Feature(\n                        int64_list=feature_pb2.Int64List(value=[5])),\n                \'image/object/bbox/xmin\':\n                    feature_pb2.Feature(\n                        float_list=feature_pb2.FloatList(value=[0.0])),\n                \'image/object/bbox/xmax\':\n                    feature_pb2.Feature(\n                        float_list=feature_pb2.FloatList(value=[1.0])),\n                \'image/object/bbox/ymin\':\n                    feature_pb2.Feature(\n                        float_list=feature_pb2.FloatList(value=[0.0])),\n                \'image/object/bbox/ymax\':\n                    feature_pb2.Feature(\n                        float_list=feature_pb2.FloatList(value=[1.0])),\n                \'image/object/class/label\':\n                    feature_pb2.Feature(\n                        int64_list=feature_pb2.Int64List(value=[2])),\n                \'image/object/mask\':\n                    feature_pb2.Feature(\n                        float_list=feature_pb2.FloatList(value=flat_mask)),\n            }))\n    writer.write(example.SerializeToString())\n    writer.close()\n\n    return path\n\n  def test_build_tf_record_input_reader(self):\n    tf_record_path = self.create_tf_record()\n\n    input_reader_text_proto = """"""\n      shuffle: false\n      num_readers: 1\n      tf_record_input_reader {{\n        input_path: \'{0}\'\n      }}\n    """""".format(tf_record_path)\n    input_reader_proto = input_reader_pb2.InputReader()\n    text_format.Merge(input_reader_text_proto, input_reader_proto)\n    tensor_dict = dataset_util.make_initializable_iterator(\n        dataset_builder.build(input_reader_proto, batch_size=1)).get_next()\n\n    sv = tf.train.Supervisor(logdir=self.get_temp_dir())\n    with sv.prepare_or_wait_for_session() as sess:\n      sv.start_queue_runners(sess)\n      output_dict = sess.run(tensor_dict)\n\n    self.assertTrue(\n        fields.InputDataFields.groundtruth_instance_masks not in output_dict)\n    self.assertEquals((1, 4, 5, 3),\n                      output_dict[fields.InputDataFields.image].shape)\n    self.assertAllEqual([[2]],\n                        output_dict[fields.InputDataFields.groundtruth_classes])\n    self.assertEquals(\n        (1, 1, 4), output_dict[fields.InputDataFields.groundtruth_boxes].shape)\n    self.assertAllEqual(\n        [0.0, 0.0, 1.0, 1.0],\n        output_dict[fields.InputDataFields.groundtruth_boxes][0][0])\n\n  def test_build_tf_record_input_reader_and_load_instance_masks(self):\n    tf_record_path = self.create_tf_record()\n\n    input_reader_text_proto = """"""\n      shuffle: false\n      num_readers: 1\n      load_instance_masks: true\n      tf_record_input_reader {{\n        input_path: \'{0}\'\n      }}\n    """""".format(tf_record_path)\n    input_reader_proto = input_reader_pb2.InputReader()\n    text_format.Merge(input_reader_text_proto, input_reader_proto)\n    tensor_dict = dataset_util.make_initializable_iterator(\n        dataset_builder.build(input_reader_proto, batch_size=1)).get_next()\n\n    sv = tf.train.Supervisor(logdir=self.get_temp_dir())\n    with sv.prepare_or_wait_for_session() as sess:\n      sv.start_queue_runners(sess)\n      output_dict = sess.run(tensor_dict)\n    self.assertAllEqual(\n        (1, 1, 4, 5),\n        output_dict[fields.InputDataFields.groundtruth_instance_masks].shape)\n\n  def test_build_tf_record_input_reader_with_batch_size_two(self):\n    tf_record_path = self.create_tf_record()\n\n    input_reader_text_proto = """"""\n      shuffle: false\n      num_readers: 1\n      tf_record_input_reader {{\n        input_path: \'{0}\'\n      }}\n    """""".format(tf_record_path)\n    input_reader_proto = input_reader_pb2.InputReader()\n    text_format.Merge(input_reader_text_proto, input_reader_proto)\n\n    def one_hot_class_encoding_fn(tensor_dict):\n      tensor_dict[fields.InputDataFields.groundtruth_classes] = tf.one_hot(\n          tensor_dict[fields.InputDataFields.groundtruth_classes] - 1, depth=3)\n      return tensor_dict\n\n    tensor_dict = dataset_util.make_initializable_iterator(\n        dataset_builder.build(\n            input_reader_proto,\n            transform_input_data_fn=one_hot_class_encoding_fn,\n            batch_size=2,\n            max_num_boxes=2,\n            num_classes=3,\n            spatial_image_shape=[4, 5])).get_next()\n\n    sv = tf.train.Supervisor(logdir=self.get_temp_dir())\n    with sv.prepare_or_wait_for_session() as sess:\n      sv.start_queue_runners(sess)\n      output_dict = sess.run(tensor_dict)\n\n    self.assertAllEqual([2, 4, 5, 3],\n                        output_dict[fields.InputDataFields.image].shape)\n    self.assertAllEqual([2, 2, 3],\n                        output_dict[fields.InputDataFields.groundtruth_classes].\n                        shape)\n    self.assertAllEqual([2, 2, 4],\n                        output_dict[fields.InputDataFields.groundtruth_boxes].\n                        shape)\n    self.assertAllEqual(\n        [[[0.0, 0.0, 1.0, 1.0],\n          [0.0, 0.0, 0.0, 0.0]],\n         [[0.0, 0.0, 1.0, 1.0],\n          [0.0, 0.0, 0.0, 0.0]]],\n        output_dict[fields.InputDataFields.groundtruth_boxes])\n\n  def test_build_tf_record_input_reader_with_batch_size_two_and_masks(self):\n    tf_record_path = self.create_tf_record()\n\n    input_reader_text_proto = """"""\n      shuffle: false\n      num_readers: 1\n      load_instance_masks: true\n      tf_record_input_reader {{\n        input_path: \'{0}\'\n      }}\n    """""".format(tf_record_path)\n    input_reader_proto = input_reader_pb2.InputReader()\n    text_format.Merge(input_reader_text_proto, input_reader_proto)\n\n    def one_hot_class_encoding_fn(tensor_dict):\n      tensor_dict[fields.InputDataFields.groundtruth_classes] = tf.one_hot(\n          tensor_dict[fields.InputDataFields.groundtruth_classes] - 1, depth=3)\n      return tensor_dict\n\n    tensor_dict = dataset_util.make_initializable_iterator(\n        dataset_builder.build(\n            input_reader_proto,\n            transform_input_data_fn=one_hot_class_encoding_fn,\n            batch_size=2,\n            max_num_boxes=2,\n            num_classes=3,\n            spatial_image_shape=[4, 5])).get_next()\n\n    sv = tf.train.Supervisor(logdir=self.get_temp_dir())\n    with sv.prepare_or_wait_for_session() as sess:\n      sv.start_queue_runners(sess)\n      output_dict = sess.run(tensor_dict)\n\n    self.assertAllEqual(\n        [2, 2, 4, 5],\n        output_dict[fields.InputDataFields.groundtruth_instance_masks].shape)\n\n  def test_raises_error_with_no_input_paths(self):\n    input_reader_text_proto = """"""\n      shuffle: false\n      num_readers: 1\n      load_instance_masks: true\n    """"""\n    input_reader_proto = input_reader_pb2.InputReader()\n    text_format.Merge(input_reader_text_proto, input_reader_proto)\n    with self.assertRaises(ValueError):\n      dataset_builder.build(input_reader_proto)\n\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
src/object_detection/builders/hyperparams_builder.py,5,"b'# Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n""""""Builder function to construct tf-slim arg_scope for convolution, fc ops.""""""\nimport tensorflow as tf\n\nfrom object_detection.protos import hyperparams_pb2\n\nslim = tf.contrib.slim\n\n\ndef build(hyperparams_config, is_training):\n  """"""Builds tf-slim arg_scope for convolution ops based on the config.\n\n  Returns an arg_scope to use for convolution ops containing weights\n  initializer, weights regularizer, activation function, batch norm function\n  and batch norm parameters based on the configuration.\n\n  Note that if the batch_norm parameteres are not specified in the config\n  (i.e. left to default) then batch norm is excluded from the arg_scope.\n\n  The batch norm parameters are set for updates based on `is_training` argument\n  and conv_hyperparams_config.batch_norm.train parameter. During training, they\n  are updated only if batch_norm.train parameter is true. However, during eval,\n  no updates are made to the batch norm variables. In both cases, their current\n  values are used during forward pass.\n\n  Args:\n    hyperparams_config: hyperparams.proto object containing\n      hyperparameters.\n    is_training: Whether the network is in training mode.\n\n  Returns:\n    arg_scope: tf-slim arg_scope containing hyperparameters for ops.\n\n  Raises:\n    ValueError: if hyperparams_config is not of type hyperparams.Hyperparams.\n  """"""\n  if not isinstance(hyperparams_config,\n                    hyperparams_pb2.Hyperparams):\n    raise ValueError(\'hyperparams_config not of type \'\n                     \'hyperparams_pb.Hyperparams.\')\n\n  batch_norm = None\n  batch_norm_params = None\n  if hyperparams_config.HasField(\'batch_norm\'):\n    batch_norm = slim.batch_norm\n    batch_norm_params = _build_batch_norm_params(\n        hyperparams_config.batch_norm, is_training)\n\n  affected_ops = [slim.conv2d, slim.separable_conv2d, slim.conv2d_transpose]\n  if hyperparams_config.HasField(\'op\') and (\n      hyperparams_config.op == hyperparams_pb2.Hyperparams.FC):\n    affected_ops = [slim.fully_connected]\n  with slim.arg_scope(\n      affected_ops,\n      weights_regularizer=_build_regularizer(\n          hyperparams_config.regularizer),\n      weights_initializer=_build_initializer(\n          hyperparams_config.initializer),\n      activation_fn=_build_activation_fn(hyperparams_config.activation),\n      normalizer_fn=batch_norm,\n      normalizer_params=batch_norm_params) as sc:\n    return sc\n\n\ndef _build_activation_fn(activation_fn):\n  """"""Builds a callable activation from config.\n\n  Args:\n    activation_fn: hyperparams_pb2.Hyperparams.activation\n\n  Returns:\n    Callable activation function.\n\n  Raises:\n    ValueError: On unknown activation function.\n  """"""\n  if activation_fn == hyperparams_pb2.Hyperparams.NONE:\n    return None\n  if activation_fn == hyperparams_pb2.Hyperparams.RELU:\n    return tf.nn.relu\n  if activation_fn == hyperparams_pb2.Hyperparams.RELU_6:\n    return tf.nn.relu6\n  raise ValueError(\'Unknown activation function: {}\'.format(activation_fn))\n\n\ndef _build_regularizer(regularizer):\n  """"""Builds a tf-slim regularizer from config.\n\n  Args:\n    regularizer: hyperparams_pb2.Hyperparams.regularizer proto.\n\n  Returns:\n    tf-slim regularizer.\n\n  Raises:\n    ValueError: On unknown regularizer.\n  """"""\n  regularizer_oneof = regularizer.WhichOneof(\'regularizer_oneof\')\n  if  regularizer_oneof == \'l1_regularizer\':\n    return slim.l1_regularizer(scale=float(regularizer.l1_regularizer.weight))\n  if regularizer_oneof == \'l2_regularizer\':\n    return slim.l2_regularizer(scale=float(regularizer.l2_regularizer.weight))\n  raise ValueError(\'Unknown regularizer function: {}\'.format(regularizer_oneof))\n\n\ndef _build_initializer(initializer):\n  """"""Build a tf initializer from config.\n\n  Args:\n    initializer: hyperparams_pb2.Hyperparams.regularizer proto.\n\n  Returns:\n    tf initializer.\n\n  Raises:\n    ValueError: On unknown initializer.\n  """"""\n  initializer_oneof = initializer.WhichOneof(\'initializer_oneof\')\n  if initializer_oneof == \'truncated_normal_initializer\':\n    return tf.truncated_normal_initializer(\n        mean=initializer.truncated_normal_initializer.mean,\n        stddev=initializer.truncated_normal_initializer.stddev)\n  if initializer_oneof == \'random_normal_initializer\':\n    return tf.random_normal_initializer(\n        mean=initializer.random_normal_initializer.mean,\n        stddev=initializer.random_normal_initializer.stddev)\n  if initializer_oneof == \'variance_scaling_initializer\':\n    enum_descriptor = (hyperparams_pb2.VarianceScalingInitializer.\n                       DESCRIPTOR.enum_types_by_name[\'Mode\'])\n    mode = enum_descriptor.values_by_number[initializer.\n                                            variance_scaling_initializer.\n                                            mode].name\n    return slim.variance_scaling_initializer(\n        factor=initializer.variance_scaling_initializer.factor,\n        mode=mode,\n        uniform=initializer.variance_scaling_initializer.uniform)\n  raise ValueError(\'Unknown initializer function: {}\'.format(\n      initializer_oneof))\n\n\ndef _build_batch_norm_params(batch_norm, is_training):\n  """"""Build a dictionary of batch_norm params from config.\n\n  Args:\n    batch_norm: hyperparams_pb2.ConvHyperparams.batch_norm proto.\n    is_training: Whether the models is in training mode.\n\n  Returns:\n    A dictionary containing batch_norm parameters.\n  """"""\n  batch_norm_params = {\n      \'decay\': batch_norm.decay,\n      \'center\': batch_norm.center,\n      \'scale\': batch_norm.scale,\n      \'epsilon\': batch_norm.epsilon,\n      \'is_training\': is_training and batch_norm.train,\n  }\n  return batch_norm_params\n'"
src/object_detection/builders/hyperparams_builder_test.py,11,"b'# Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n""""""Tests object_detection.core.hyperparams_builder.""""""\n\nimport numpy as np\nimport tensorflow as tf\n\nfrom google.protobuf import text_format\n\nfrom object_detection.builders import hyperparams_builder\nfrom object_detection.protos import hyperparams_pb2\n\nslim = tf.contrib.slim\n\n\nclass HyperparamsBuilderTest(tf.test.TestCase):\n\n  # TODO(rathodv): Make this a public api in slim arg_scope.py.\n  def _get_scope_key(self, op):\n    return getattr(op, \'_key_op\', str(op))\n\n  def test_default_arg_scope_has_conv2d_op(self):\n    conv_hyperparams_text_proto = """"""\n      regularizer {\n        l1_regularizer {\n        }\n      }\n      initializer {\n        truncated_normal_initializer {\n        }\n      }\n    """"""\n    conv_hyperparams_proto = hyperparams_pb2.Hyperparams()\n    text_format.Merge(conv_hyperparams_text_proto, conv_hyperparams_proto)\n    scope = hyperparams_builder.build(conv_hyperparams_proto, is_training=True)\n    self.assertTrue(self._get_scope_key(slim.conv2d) in scope)\n\n  def test_default_arg_scope_has_separable_conv2d_op(self):\n    conv_hyperparams_text_proto = """"""\n      regularizer {\n        l1_regularizer {\n        }\n      }\n      initializer {\n        truncated_normal_initializer {\n        }\n      }\n    """"""\n    conv_hyperparams_proto = hyperparams_pb2.Hyperparams()\n    text_format.Merge(conv_hyperparams_text_proto, conv_hyperparams_proto)\n    scope = hyperparams_builder.build(conv_hyperparams_proto, is_training=True)\n    self.assertTrue(self._get_scope_key(slim.separable_conv2d) in scope)\n\n  def test_default_arg_scope_has_conv2d_transpose_op(self):\n    conv_hyperparams_text_proto = """"""\n      regularizer {\n        l1_regularizer {\n        }\n      }\n      initializer {\n        truncated_normal_initializer {\n        }\n      }\n    """"""\n    conv_hyperparams_proto = hyperparams_pb2.Hyperparams()\n    text_format.Merge(conv_hyperparams_text_proto, conv_hyperparams_proto)\n    scope = hyperparams_builder.build(conv_hyperparams_proto, is_training=True)\n    self.assertTrue(self._get_scope_key(slim.conv2d_transpose) in scope)\n\n  def test_explicit_fc_op_arg_scope_has_fully_connected_op(self):\n    conv_hyperparams_text_proto = """"""\n      op: FC\n      regularizer {\n        l1_regularizer {\n        }\n      }\n      initializer {\n        truncated_normal_initializer {\n        }\n      }\n    """"""\n    conv_hyperparams_proto = hyperparams_pb2.Hyperparams()\n    text_format.Merge(conv_hyperparams_text_proto, conv_hyperparams_proto)\n    scope = hyperparams_builder.build(conv_hyperparams_proto, is_training=True)\n    self.assertTrue(self._get_scope_key(slim.fully_connected) in scope)\n\n  def test_separable_conv2d_and_conv2d_and_transpose_have_same_parameters(self):\n    conv_hyperparams_text_proto = """"""\n      regularizer {\n        l1_regularizer {\n        }\n      }\n      initializer {\n        truncated_normal_initializer {\n        }\n      }\n    """"""\n    conv_hyperparams_proto = hyperparams_pb2.Hyperparams()\n    text_format.Merge(conv_hyperparams_text_proto, conv_hyperparams_proto)\n    scope = hyperparams_builder.build(conv_hyperparams_proto, is_training=True)\n    kwargs_1, kwargs_2, kwargs_3 = scope.values()\n    self.assertDictEqual(kwargs_1, kwargs_2)\n    self.assertDictEqual(kwargs_1, kwargs_3)\n\n  def test_return_l1_regularized_weights(self):\n    conv_hyperparams_text_proto = """"""\n      regularizer {\n        l1_regularizer {\n          weight: 0.5\n        }\n      }\n      initializer {\n        truncated_normal_initializer {\n        }\n      }\n    """"""\n    conv_hyperparams_proto = hyperparams_pb2.Hyperparams()\n    text_format.Merge(conv_hyperparams_text_proto, conv_hyperparams_proto)\n    scope = hyperparams_builder.build(conv_hyperparams_proto, is_training=True)\n    conv_scope_arguments = scope.values()[0]\n    regularizer = conv_scope_arguments[\'weights_regularizer\']\n    weights = np.array([1., -1, 4., 2.])\n    with self.test_session() as sess:\n      result = sess.run(regularizer(tf.constant(weights)))\n    self.assertAllClose(np.abs(weights).sum() * 0.5, result)\n\n  def test_return_l2_regularizer_weights(self):\n    conv_hyperparams_text_proto = """"""\n      regularizer {\n        l2_regularizer {\n          weight: 0.42\n        }\n      }\n      initializer {\n        truncated_normal_initializer {\n        }\n      }\n    """"""\n    conv_hyperparams_proto = hyperparams_pb2.Hyperparams()\n    text_format.Merge(conv_hyperparams_text_proto, conv_hyperparams_proto)\n    scope = hyperparams_builder.build(conv_hyperparams_proto, is_training=True)\n    conv_scope_arguments = scope.values()[0]\n\n    regularizer = conv_scope_arguments[\'weights_regularizer\']\n    weights = np.array([1., -1, 4., 2.])\n    with self.test_session() as sess:\n      result = sess.run(regularizer(tf.constant(weights)))\n    self.assertAllClose(np.power(weights, 2).sum() / 2.0 * 0.42, result)\n\n  def test_return_non_default_batch_norm_params_with_train_during_train(self):\n    conv_hyperparams_text_proto = """"""\n      regularizer {\n        l2_regularizer {\n        }\n      }\n      initializer {\n        truncated_normal_initializer {\n        }\n      }\n      batch_norm {\n        decay: 0.7\n        center: false\n        scale: true\n        epsilon: 0.03\n        train: true\n      }\n    """"""\n    conv_hyperparams_proto = hyperparams_pb2.Hyperparams()\n    text_format.Merge(conv_hyperparams_text_proto, conv_hyperparams_proto)\n    scope = hyperparams_builder.build(conv_hyperparams_proto, is_training=True)\n    conv_scope_arguments = scope.values()[0]\n    self.assertEqual(conv_scope_arguments[\'normalizer_fn\'], slim.batch_norm)\n    batch_norm_params = conv_scope_arguments[\'normalizer_params\']\n    self.assertAlmostEqual(batch_norm_params[\'decay\'], 0.7)\n    self.assertAlmostEqual(batch_norm_params[\'epsilon\'], 0.03)\n    self.assertFalse(batch_norm_params[\'center\'])\n    self.assertTrue(batch_norm_params[\'scale\'])\n    self.assertTrue(batch_norm_params[\'is_training\'])\n\n  def test_return_batch_norm_params_with_notrain_during_eval(self):\n    conv_hyperparams_text_proto = """"""\n      regularizer {\n        l2_regularizer {\n        }\n      }\n      initializer {\n        truncated_normal_initializer {\n        }\n      }\n      batch_norm {\n        decay: 0.7\n        center: false\n        scale: true\n        epsilon: 0.03\n        train: true\n      }\n    """"""\n    conv_hyperparams_proto = hyperparams_pb2.Hyperparams()\n    text_format.Merge(conv_hyperparams_text_proto, conv_hyperparams_proto)\n    scope = hyperparams_builder.build(conv_hyperparams_proto, is_training=False)\n    conv_scope_arguments = scope.values()[0]\n    self.assertEqual(conv_scope_arguments[\'normalizer_fn\'], slim.batch_norm)\n    batch_norm_params = conv_scope_arguments[\'normalizer_params\']\n    self.assertAlmostEqual(batch_norm_params[\'decay\'], 0.7)\n    self.assertAlmostEqual(batch_norm_params[\'epsilon\'], 0.03)\n    self.assertFalse(batch_norm_params[\'center\'])\n    self.assertTrue(batch_norm_params[\'scale\'])\n    self.assertFalse(batch_norm_params[\'is_training\'])\n\n  def test_return_batch_norm_params_with_notrain_when_train_is_false(self):\n    conv_hyperparams_text_proto = """"""\n      regularizer {\n        l2_regularizer {\n        }\n      }\n      initializer {\n        truncated_normal_initializer {\n        }\n      }\n      batch_norm {\n        decay: 0.7\n        center: false\n        scale: true\n        epsilon: 0.03\n        train: false\n      }\n    """"""\n    conv_hyperparams_proto = hyperparams_pb2.Hyperparams()\n    text_format.Merge(conv_hyperparams_text_proto, conv_hyperparams_proto)\n    scope = hyperparams_builder.build(conv_hyperparams_proto, is_training=True)\n    conv_scope_arguments = scope.values()[0]\n    self.assertEqual(conv_scope_arguments[\'normalizer_fn\'], slim.batch_norm)\n    batch_norm_params = conv_scope_arguments[\'normalizer_params\']\n    self.assertAlmostEqual(batch_norm_params[\'decay\'], 0.7)\n    self.assertAlmostEqual(batch_norm_params[\'epsilon\'], 0.03)\n    self.assertFalse(batch_norm_params[\'center\'])\n    self.assertTrue(batch_norm_params[\'scale\'])\n    self.assertFalse(batch_norm_params[\'is_training\'])\n\n  def test_do_not_use_batch_norm_if_default(self):\n    conv_hyperparams_text_proto = """"""\n      regularizer {\n        l2_regularizer {\n        }\n      }\n      initializer {\n        truncated_normal_initializer {\n        }\n      }\n    """"""\n    conv_hyperparams_proto = hyperparams_pb2.Hyperparams()\n    text_format.Merge(conv_hyperparams_text_proto, conv_hyperparams_proto)\n    scope = hyperparams_builder.build(conv_hyperparams_proto, is_training=True)\n    conv_scope_arguments = scope.values()[0]\n    self.assertEqual(conv_scope_arguments[\'normalizer_fn\'], None)\n    self.assertEqual(conv_scope_arguments[\'normalizer_params\'], None)\n\n  def test_use_none_activation(self):\n    conv_hyperparams_text_proto = """"""\n      regularizer {\n        l2_regularizer {\n        }\n      }\n      initializer {\n        truncated_normal_initializer {\n        }\n      }\n      activation: NONE\n    """"""\n    conv_hyperparams_proto = hyperparams_pb2.Hyperparams()\n    text_format.Merge(conv_hyperparams_text_proto, conv_hyperparams_proto)\n    scope = hyperparams_builder.build(conv_hyperparams_proto, is_training=True)\n    conv_scope_arguments = scope.values()[0]\n    self.assertEqual(conv_scope_arguments[\'activation_fn\'], None)\n\n  def test_use_relu_activation(self):\n    conv_hyperparams_text_proto = """"""\n      regularizer {\n        l2_regularizer {\n        }\n      }\n      initializer {\n        truncated_normal_initializer {\n        }\n      }\n      activation: RELU\n    """"""\n    conv_hyperparams_proto = hyperparams_pb2.Hyperparams()\n    text_format.Merge(conv_hyperparams_text_proto, conv_hyperparams_proto)\n    scope = hyperparams_builder.build(conv_hyperparams_proto, is_training=True)\n    conv_scope_arguments = scope.values()[0]\n    self.assertEqual(conv_scope_arguments[\'activation_fn\'], tf.nn.relu)\n\n  def test_use_relu_6_activation(self):\n    conv_hyperparams_text_proto = """"""\n      regularizer {\n        l2_regularizer {\n        }\n      }\n      initializer {\n        truncated_normal_initializer {\n        }\n      }\n      activation: RELU_6\n    """"""\n    conv_hyperparams_proto = hyperparams_pb2.Hyperparams()\n    text_format.Merge(conv_hyperparams_text_proto, conv_hyperparams_proto)\n    scope = hyperparams_builder.build(conv_hyperparams_proto, is_training=True)\n    conv_scope_arguments = scope.values()[0]\n    self.assertEqual(conv_scope_arguments[\'activation_fn\'], tf.nn.relu6)\n\n  def _assert_variance_in_range(self, initializer, shape, variance,\n                                tol=1e-2):\n    with tf.Graph().as_default() as g:\n      with self.test_session(graph=g) as sess:\n        var = tf.get_variable(\n            name=\'test\',\n            shape=shape,\n            dtype=tf.float32,\n            initializer=initializer)\n        sess.run(tf.global_variables_initializer())\n        values = sess.run(var)\n        self.assertAllClose(np.var(values), variance, tol, tol)\n\n  def test_variance_in_range_with_variance_scaling_initializer_fan_in(self):\n    conv_hyperparams_text_proto = """"""\n      regularizer {\n        l2_regularizer {\n        }\n      }\n      initializer {\n        variance_scaling_initializer {\n          factor: 2.0\n          mode: FAN_IN\n          uniform: false\n        }\n      }\n    """"""\n    conv_hyperparams_proto = hyperparams_pb2.Hyperparams()\n    text_format.Merge(conv_hyperparams_text_proto, conv_hyperparams_proto)\n    scope = hyperparams_builder.build(conv_hyperparams_proto, is_training=True)\n    conv_scope_arguments = scope.values()[0]\n    initializer = conv_scope_arguments[\'weights_initializer\']\n    self._assert_variance_in_range(initializer, shape=[100, 40],\n                                   variance=2. / 100.)\n\n  def test_variance_in_range_with_variance_scaling_initializer_fan_out(self):\n    conv_hyperparams_text_proto = """"""\n      regularizer {\n        l2_regularizer {\n        }\n      }\n      initializer {\n        variance_scaling_initializer {\n          factor: 2.0\n          mode: FAN_OUT\n          uniform: false\n        }\n      }\n    """"""\n    conv_hyperparams_proto = hyperparams_pb2.Hyperparams()\n    text_format.Merge(conv_hyperparams_text_proto, conv_hyperparams_proto)\n    scope = hyperparams_builder.build(conv_hyperparams_proto, is_training=True)\n    conv_scope_arguments = scope.values()[0]\n    initializer = conv_scope_arguments[\'weights_initializer\']\n    self._assert_variance_in_range(initializer, shape=[100, 40],\n                                   variance=2. / 40.)\n\n  def test_variance_in_range_with_variance_scaling_initializer_fan_avg(self):\n    conv_hyperparams_text_proto = """"""\n      regularizer {\n        l2_regularizer {\n        }\n      }\n      initializer {\n        variance_scaling_initializer {\n          factor: 2.0\n          mode: FAN_AVG\n          uniform: false\n        }\n      }\n    """"""\n    conv_hyperparams_proto = hyperparams_pb2.Hyperparams()\n    text_format.Merge(conv_hyperparams_text_proto, conv_hyperparams_proto)\n    scope = hyperparams_builder.build(conv_hyperparams_proto, is_training=True)\n    conv_scope_arguments = scope.values()[0]\n    initializer = conv_scope_arguments[\'weights_initializer\']\n    self._assert_variance_in_range(initializer, shape=[100, 40],\n                                   variance=4. / (100. + 40.))\n\n  def test_variance_in_range_with_variance_scaling_initializer_uniform(self):\n    conv_hyperparams_text_proto = """"""\n      regularizer {\n        l2_regularizer {\n        }\n      }\n      initializer {\n        variance_scaling_initializer {\n          factor: 2.0\n          mode: FAN_IN\n          uniform: true\n        }\n      }\n    """"""\n    conv_hyperparams_proto = hyperparams_pb2.Hyperparams()\n    text_format.Merge(conv_hyperparams_text_proto, conv_hyperparams_proto)\n    scope = hyperparams_builder.build(conv_hyperparams_proto, is_training=True)\n    conv_scope_arguments = scope.values()[0]\n    initializer = conv_scope_arguments[\'weights_initializer\']\n    self._assert_variance_in_range(initializer, shape=[100, 40],\n                                   variance=2. / 100.)\n\n  def test_variance_in_range_with_truncated_normal_initializer(self):\n    conv_hyperparams_text_proto = """"""\n      regularizer {\n        l2_regularizer {\n        }\n      }\n      initializer {\n        truncated_normal_initializer {\n          mean: 0.0\n          stddev: 0.8\n        }\n      }\n    """"""\n    conv_hyperparams_proto = hyperparams_pb2.Hyperparams()\n    text_format.Merge(conv_hyperparams_text_proto, conv_hyperparams_proto)\n    scope = hyperparams_builder.build(conv_hyperparams_proto, is_training=True)\n    conv_scope_arguments = scope.values()[0]\n    initializer = conv_scope_arguments[\'weights_initializer\']\n    self._assert_variance_in_range(initializer, shape=[100, 40],\n                                   variance=0.49, tol=1e-1)\n\n  def test_variance_in_range_with_random_normal_initializer(self):\n    conv_hyperparams_text_proto = """"""\n      regularizer {\n        l2_regularizer {\n        }\n      }\n      initializer {\n        random_normal_initializer {\n          mean: 0.0\n          stddev: 0.8\n        }\n      }\n    """"""\n    conv_hyperparams_proto = hyperparams_pb2.Hyperparams()\n    text_format.Merge(conv_hyperparams_text_proto, conv_hyperparams_proto)\n    scope = hyperparams_builder.build(conv_hyperparams_proto, is_training=True)\n    conv_scope_arguments = scope.values()[0]\n    initializer = conv_scope_arguments[\'weights_initializer\']\n    self._assert_variance_in_range(initializer, shape=[100, 40],\n                                   variance=0.64, tol=1e-1)\n\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
src/object_detection/builders/image_resizer_builder.py,5,"b'# Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Builder function for image resizing operations.""""""\nimport functools\nimport tensorflow as tf\n\nfrom object_detection.core import preprocessor\nfrom object_detection.protos import image_resizer_pb2\n\n\ndef _tf_resize_method(resize_method):\n  """"""Maps image resize method from enumeration type to TensorFlow.\n\n  Args:\n    resize_method: The resize_method attribute of keep_aspect_ratio_resizer or\n      fixed_shape_resizer.\n\n  Returns:\n    method: The corresponding TensorFlow ResizeMethod.\n\n  Raises:\n    ValueError: if `resize_method` is of unknown type.\n  """"""\n  dict_method = {\n      image_resizer_pb2.BILINEAR:\n          tf.image.ResizeMethod.BILINEAR,\n      image_resizer_pb2.NEAREST_NEIGHBOR:\n          tf.image.ResizeMethod.NEAREST_NEIGHBOR,\n      image_resizer_pb2.BICUBIC:\n          tf.image.ResizeMethod.BICUBIC,\n      image_resizer_pb2.AREA:\n          tf.image.ResizeMethod.AREA\n  }\n  if resize_method in dict_method:\n    return dict_method[resize_method]\n  else:\n    raise ValueError(\'Unknown resize_method\')\n\n\ndef build(image_resizer_config):\n  """"""Builds callable for image resizing operations.\n\n  Args:\n    image_resizer_config: image_resizer.proto object containing parameters for\n      an image resizing operation.\n\n  Returns:\n    image_resizer_fn: Callable for image resizing.  This callable always takes\n      a rank-3 image tensor (corresponding to a single image) and returns a\n      rank-3 image tensor, possibly with new spatial dimensions.\n\n  Raises:\n    ValueError: if `image_resizer_config` is of incorrect type.\n    ValueError: if `image_resizer_config.image_resizer_oneof` is of expected\n      type.\n    ValueError: if min_dimension > max_dimension when keep_aspect_ratio_resizer\n      is used.\n  """"""\n  if not isinstance(image_resizer_config, image_resizer_pb2.ImageResizer):\n    raise ValueError(\'image_resizer_config not of type \'\n                     \'image_resizer_pb2.ImageResizer.\')\n\n  image_resizer_oneof = image_resizer_config.WhichOneof(\'image_resizer_oneof\')\n  if image_resizer_oneof == \'keep_aspect_ratio_resizer\':\n    keep_aspect_ratio_config = image_resizer_config.keep_aspect_ratio_resizer\n    if not (keep_aspect_ratio_config.min_dimension <=\n            keep_aspect_ratio_config.max_dimension):\n      raise ValueError(\'min_dimension > max_dimension\')\n    method = _tf_resize_method(keep_aspect_ratio_config.resize_method)\n    image_resizer_fn = functools.partial(\n        preprocessor.resize_to_range,\n        min_dimension=keep_aspect_ratio_config.min_dimension,\n        max_dimension=keep_aspect_ratio_config.max_dimension,\n        method=method,\n        pad_to_max_dimension=keep_aspect_ratio_config.pad_to_max_dimension)\n    if not keep_aspect_ratio_config.convert_to_grayscale:\n      return image_resizer_fn\n  elif image_resizer_oneof == \'fixed_shape_resizer\':\n    fixed_shape_resizer_config = image_resizer_config.fixed_shape_resizer\n    method = _tf_resize_method(fixed_shape_resizer_config.resize_method)\n    image_resizer_fn = functools.partial(\n        preprocessor.resize_image,\n        new_height=fixed_shape_resizer_config.height,\n        new_width=fixed_shape_resizer_config.width,\n        method=method)\n    if not fixed_shape_resizer_config.convert_to_grayscale:\n      return image_resizer_fn\n  else:\n    raise ValueError(\n        \'Invalid image resizer option: \\\'%s\\\'.\' % image_resizer_oneof)\n\n  def grayscale_image_resizer(image):\n    [resized_image, resized_image_shape] = image_resizer_fn(image)\n    grayscale_image = preprocessor.rgb_to_gray(resized_image)\n    grayscale_image_shape = tf.concat([resized_image_shape[:-1], [1]], 0)\n    return [grayscale_image, grayscale_image_shape]\n\n  return functools.partial(grayscale_image_resizer)\n'"
src/object_detection/builders/image_resizer_builder_test.py,5,"b'# Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Tests for object_detection.builders.image_resizer_builder.""""""\nimport numpy as np\nimport tensorflow as tf\nfrom google.protobuf import text_format\nfrom object_detection.builders import image_resizer_builder\nfrom object_detection.protos import image_resizer_pb2\n\n\nclass ImageResizerBuilderTest(tf.test.TestCase):\n\n  def _shape_of_resized_random_image_given_text_proto(self, input_shape,\n                                                      text_proto):\n    image_resizer_config = image_resizer_pb2.ImageResizer()\n    text_format.Merge(text_proto, image_resizer_config)\n    image_resizer_fn = image_resizer_builder.build(image_resizer_config)\n    images = tf.to_float(\n        tf.random_uniform(input_shape, minval=0, maxval=255, dtype=tf.int32))\n    resized_images, _ = image_resizer_fn(images)\n    with self.test_session() as sess:\n      return sess.run(resized_images).shape\n\n  def test_build_keep_aspect_ratio_resizer_returns_expected_shape(self):\n    image_resizer_text_proto = """"""\n      keep_aspect_ratio_resizer {\n        min_dimension: 10\n        max_dimension: 20\n      }\n    """"""\n    input_shape = (50, 25, 3)\n    expected_output_shape = (20, 10, 3)\n    output_shape = self._shape_of_resized_random_image_given_text_proto(\n        input_shape, image_resizer_text_proto)\n    self.assertEqual(output_shape, expected_output_shape)\n\n  def test_build_keep_aspect_ratio_resizer_with_padding(self):\n    image_resizer_text_proto = """"""\n      keep_aspect_ratio_resizer {\n        min_dimension: 10\n        max_dimension: 20\n        pad_to_max_dimension: true\n      }\n    """"""\n    input_shape = (50, 25, 3)\n    expected_output_shape = (20, 20, 3)\n    output_shape = self._shape_of_resized_random_image_given_text_proto(\n        input_shape, image_resizer_text_proto)\n    self.assertEqual(output_shape, expected_output_shape)\n\n  def test_built_fixed_shape_resizer_returns_expected_shape(self):\n    image_resizer_text_proto = """"""\n      fixed_shape_resizer {\n        height: 10\n        width: 20\n      }\n    """"""\n    input_shape = (50, 25, 3)\n    expected_output_shape = (10, 20, 3)\n    output_shape = self._shape_of_resized_random_image_given_text_proto(\n        input_shape, image_resizer_text_proto)\n    self.assertEqual(output_shape, expected_output_shape)\n\n  def test_raises_error_on_invalid_input(self):\n    invalid_input = \'invalid_input\'\n    with self.assertRaises(ValueError):\n      image_resizer_builder.build(invalid_input)\n\n  def _resized_image_given_text_proto(self, image, text_proto):\n    image_resizer_config = image_resizer_pb2.ImageResizer()\n    text_format.Merge(text_proto, image_resizer_config)\n    image_resizer_fn = image_resizer_builder.build(image_resizer_config)\n    image_placeholder = tf.placeholder(tf.uint8, [1, None, None, 3])\n    resized_image, _ = image_resizer_fn(image_placeholder)\n    with self.test_session() as sess:\n      return sess.run(resized_image, feed_dict={image_placeholder: image})\n\n  def test_fixed_shape_resizer_nearest_neighbor_method(self):\n    image_resizer_text_proto = """"""\n      fixed_shape_resizer {\n        height: 1\n        width: 1\n        resize_method: NEAREST_NEIGHBOR\n      }\n    """"""\n    image = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n    image = np.expand_dims(image, axis=2)\n    image = np.tile(image, (1, 1, 3))\n    image = np.expand_dims(image, axis=0)\n    resized_image = self._resized_image_given_text_proto(\n        image, image_resizer_text_proto)\n    vals = np.unique(resized_image).tolist()\n    self.assertEqual(len(vals), 1)\n    self.assertEqual(vals[0], 1)\n\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
src/object_detection/builders/input_reader_builder.py,3,"b'# Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n""""""Input reader builder.\n\nCreates data sources for DetectionModels from an InputReader config. See\ninput_reader.proto for options.\n\nNote: If users wishes to also use their own InputReaders with the Object\nDetection configuration framework, they should define their own builder function\nthat wraps the build function.\n""""""\n\nimport tensorflow as tf\n\nfrom object_detection.data_decoders import tf_example_decoder\nfrom object_detection.protos import input_reader_pb2\n\nparallel_reader = tf.contrib.slim.parallel_reader\n\n\ndef build(input_reader_config):\n  """"""Builds a tensor dictionary based on the InputReader config.\n\n  Args:\n    input_reader_config: A input_reader_pb2.InputReader object.\n\n  Returns:\n    A tensor dict based on the input_reader_config.\n\n  Raises:\n    ValueError: On invalid input reader proto.\n    ValueError: If no input paths are specified.\n  """"""\n  if not isinstance(input_reader_config, input_reader_pb2.InputReader):\n    raise ValueError(\'input_reader_config not of type \'\n                     \'input_reader_pb2.InputReader.\')\n\n  if input_reader_config.WhichOneof(\'input_reader\') == \'tf_record_input_reader\':\n    config = input_reader_config.tf_record_input_reader\n    if not config.input_path:\n      raise ValueError(\'At least one input path must be specified in \'\n                       \'`input_reader_config`.\')\n    _, string_tensor = parallel_reader.parallel_read(\n        config.input_path[:],  # Convert `RepeatedScalarContainer` to list.\n        reader_class=tf.TFRecordReader,\n        num_epochs=(input_reader_config.num_epochs\n                    if input_reader_config.num_epochs else None),\n        num_readers=input_reader_config.num_readers,\n        shuffle=input_reader_config.shuffle,\n        dtypes=[tf.string, tf.string],\n        capacity=input_reader_config.queue_capacity,\n        min_after_dequeue=input_reader_config.min_after_dequeue)\n\n    label_map_proto_file = None\n    if input_reader_config.HasField(\'label_map_path\'):\n      label_map_proto_file = input_reader_config.label_map_path\n    decoder = tf_example_decoder.TfExampleDecoder(\n        load_instance_masks=input_reader_config.load_instance_masks,\n        instance_mask_type=input_reader_config.mask_type,\n        label_map_proto_file=label_map_proto_file)\n    return decoder.decode(string_tensor)\n\n  raise ValueError(\'Unsupported input_reader_config.\')\n'"
src/object_detection/builders/input_reader_builder_test.py,6,"b'# Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n""""""Tests for input_reader_builder.""""""\n\nimport os\nimport numpy as np\nimport tensorflow as tf\n\nfrom google.protobuf import text_format\n\nfrom tensorflow.core.example import example_pb2\nfrom tensorflow.core.example import feature_pb2\nfrom object_detection.builders import input_reader_builder\nfrom object_detection.core import standard_fields as fields\nfrom object_detection.protos import input_reader_pb2\n\n\nclass InputReaderBuilderTest(tf.test.TestCase):\n\n  def create_tf_record(self):\n    path = os.path.join(self.get_temp_dir(), \'tfrecord\')\n    writer = tf.python_io.TFRecordWriter(path)\n\n    image_tensor = np.random.randint(255, size=(4, 5, 3)).astype(np.uint8)\n    flat_mask = (4 * 5) * [1.0]\n    with self.test_session():\n      encoded_jpeg = tf.image.encode_jpeg(tf.constant(image_tensor)).eval()\n    example = example_pb2.Example(features=feature_pb2.Features(feature={\n        \'image/encoded\': feature_pb2.Feature(\n            bytes_list=feature_pb2.BytesList(value=[encoded_jpeg])),\n        \'image/format\': feature_pb2.Feature(\n            bytes_list=feature_pb2.BytesList(value=[\'jpeg\'.encode(\'utf-8\')])),\n        \'image/height\': feature_pb2.Feature(\n            int64_list=feature_pb2.Int64List(value=[4])),\n        \'image/width\': feature_pb2.Feature(\n            int64_list=feature_pb2.Int64List(value=[5])),\n        \'image/object/bbox/xmin\': feature_pb2.Feature(\n            float_list=feature_pb2.FloatList(value=[0.0])),\n        \'image/object/bbox/xmax\': feature_pb2.Feature(\n            float_list=feature_pb2.FloatList(value=[1.0])),\n        \'image/object/bbox/ymin\': feature_pb2.Feature(\n            float_list=feature_pb2.FloatList(value=[0.0])),\n        \'image/object/bbox/ymax\': feature_pb2.Feature(\n            float_list=feature_pb2.FloatList(value=[1.0])),\n        \'image/object/class/label\': feature_pb2.Feature(\n            int64_list=feature_pb2.Int64List(value=[2])),\n        \'image/object/mask\': feature_pb2.Feature(\n            float_list=feature_pb2.FloatList(value=flat_mask)),\n    }))\n    writer.write(example.SerializeToString())\n    writer.close()\n\n    return path\n\n  def test_build_tf_record_input_reader(self):\n    tf_record_path = self.create_tf_record()\n\n    input_reader_text_proto = """"""\n      shuffle: false\n      num_readers: 1\n      tf_record_input_reader {{\n        input_path: \'{0}\'\n      }}\n    """""".format(tf_record_path)\n    input_reader_proto = input_reader_pb2.InputReader()\n    text_format.Merge(input_reader_text_proto, input_reader_proto)\n    tensor_dict = input_reader_builder.build(input_reader_proto)\n\n    sv = tf.train.Supervisor(logdir=self.get_temp_dir())\n    with sv.prepare_or_wait_for_session() as sess:\n      sv.start_queue_runners(sess)\n      output_dict = sess.run(tensor_dict)\n\n    self.assertTrue(fields.InputDataFields.groundtruth_instance_masks\n                    not in output_dict)\n    self.assertEquals(\n        (4, 5, 3), output_dict[fields.InputDataFields.image].shape)\n    self.assertEquals(\n        [2], output_dict[fields.InputDataFields.groundtruth_classes])\n    self.assertEquals(\n        (1, 4), output_dict[fields.InputDataFields.groundtruth_boxes].shape)\n    self.assertAllEqual(\n        [0.0, 0.0, 1.0, 1.0],\n        output_dict[fields.InputDataFields.groundtruth_boxes][0])\n\n  def test_build_tf_record_input_reader_and_load_instance_masks(self):\n    tf_record_path = self.create_tf_record()\n\n    input_reader_text_proto = """"""\n      shuffle: false\n      num_readers: 1\n      load_instance_masks: true\n      tf_record_input_reader {{\n        input_path: \'{0}\'\n      }}\n    """""".format(tf_record_path)\n    input_reader_proto = input_reader_pb2.InputReader()\n    text_format.Merge(input_reader_text_proto, input_reader_proto)\n    tensor_dict = input_reader_builder.build(input_reader_proto)\n\n    sv = tf.train.Supervisor(logdir=self.get_temp_dir())\n    with sv.prepare_or_wait_for_session() as sess:\n      sv.start_queue_runners(sess)\n      output_dict = sess.run(tensor_dict)\n\n    self.assertEquals(\n        (4, 5, 3), output_dict[fields.InputDataFields.image].shape)\n    self.assertEquals(\n        [2], output_dict[fields.InputDataFields.groundtruth_classes])\n    self.assertEquals(\n        (1, 4), output_dict[fields.InputDataFields.groundtruth_boxes].shape)\n    self.assertAllEqual(\n        [0.0, 0.0, 1.0, 1.0],\n        output_dict[fields.InputDataFields.groundtruth_boxes][0])\n    self.assertAllEqual(\n        (1, 4, 5),\n        output_dict[fields.InputDataFields.groundtruth_instance_masks].shape)\n\n  def test_raises_error_with_no_input_paths(self):\n    input_reader_text_proto = """"""\n      shuffle: false\n      num_readers: 1\n      load_instance_masks: true\n    """"""\n    input_reader_proto = input_reader_pb2.InputReader()\n    text_format.Merge(input_reader_text_proto, input_reader_proto)\n    with self.assertRaises(ValueError):\n      input_reader_builder.build(input_reader_proto)\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
src/object_detection/builders/losses_builder.py,0,"b'# Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n""""""A function to build localization and classification losses from config.""""""\n\nfrom object_detection.core import losses\nfrom object_detection.protos import losses_pb2\n\n\ndef build(loss_config):\n  """"""Build losses based on the config.\n\n  Builds classification, localization losses and optionally a hard example miner\n  based on the config.\n\n  Args:\n    loss_config: A losses_pb2.Loss object.\n\n  Returns:\n    classification_loss: Classification loss object.\n    localization_loss: Localization loss object.\n    classification_weight: Classification loss weight.\n    localization_weight: Localization loss weight.\n    hard_example_miner: Hard example miner object.\n\n  Raises:\n    ValueError: If hard_example_miner is used with sigmoid_focal_loss.\n  """"""\n  classification_loss = _build_classification_loss(\n      loss_config.classification_loss)\n  localization_loss = _build_localization_loss(\n      loss_config.localization_loss)\n  classification_weight = loss_config.classification_weight\n  localization_weight = loss_config.localization_weight\n  hard_example_miner = None\n  if loss_config.HasField(\'hard_example_miner\'):\n    if (loss_config.classification_loss.WhichOneof(\'classification_loss\') ==\n        \'weighted_sigmoid_focal\'):\n      raise ValueError(\'HardExampleMiner should not be used with sigmoid focal \'\n                       \'loss\')\n    hard_example_miner = build_hard_example_miner(\n        loss_config.hard_example_miner,\n        classification_weight,\n        localization_weight)\n  return (classification_loss, localization_loss,\n          classification_weight,\n          localization_weight, hard_example_miner)\n\n\ndef build_hard_example_miner(config,\n                             classification_weight,\n                             localization_weight):\n  """"""Builds hard example miner based on the config.\n\n  Args:\n    config: A losses_pb2.HardExampleMiner object.\n    classification_weight: Classification loss weight.\n    localization_weight: Localization loss weight.\n\n  Returns:\n    Hard example miner.\n\n  """"""\n  loss_type = None\n  if config.loss_type == losses_pb2.HardExampleMiner.BOTH:\n    loss_type = \'both\'\n  if config.loss_type == losses_pb2.HardExampleMiner.CLASSIFICATION:\n    loss_type = \'cls\'\n  if config.loss_type == losses_pb2.HardExampleMiner.LOCALIZATION:\n    loss_type = \'loc\'\n\n  max_negatives_per_positive = None\n  num_hard_examples = None\n  if config.max_negatives_per_positive > 0:\n    max_negatives_per_positive = config.max_negatives_per_positive\n  if config.num_hard_examples > 0:\n    num_hard_examples = config.num_hard_examples\n  hard_example_miner = losses.HardExampleMiner(\n      num_hard_examples=num_hard_examples,\n      iou_threshold=config.iou_threshold,\n      loss_type=loss_type,\n      cls_loss_weight=classification_weight,\n      loc_loss_weight=localization_weight,\n      max_negatives_per_positive=max_negatives_per_positive,\n      min_negatives_per_image=config.min_negatives_per_image)\n  return hard_example_miner\n\n\ndef build_faster_rcnn_classification_loss(loss_config):\n  """"""Builds a classification loss for Faster RCNN based on the loss config.\n\n  Args:\n    loss_config: A losses_pb2.ClassificationLoss object.\n\n  Returns:\n    Loss based on the config.\n\n  Raises:\n    ValueError: On invalid loss_config.\n  """"""\n  if not isinstance(loss_config, losses_pb2.ClassificationLoss):\n    raise ValueError(\'loss_config not of type losses_pb2.ClassificationLoss.\')\n\n  loss_type = loss_config.WhichOneof(\'classification_loss\')\n\n  if loss_type == \'weighted_sigmoid\':\n    return losses.WeightedSigmoidClassificationLoss()\n  if loss_type == \'weighted_softmax\':\n    config = loss_config.weighted_softmax\n    return losses.WeightedSoftmaxClassificationLoss(\n        logit_scale=config.logit_scale)\n\n  # By default, Faster RCNN second stage classifier uses Softmax loss\n  # with anchor-wise outputs.\n  config = loss_config.weighted_softmax\n  return losses.WeightedSoftmaxClassificationLoss(\n      logit_scale=config.logit_scale)\n\n\ndef _build_localization_loss(loss_config):\n  """"""Builds a localization loss based on the loss config.\n\n  Args:\n    loss_config: A losses_pb2.LocalizationLoss object.\n\n  Returns:\n    Loss based on the config.\n\n  Raises:\n    ValueError: On invalid loss_config.\n  """"""\n  if not isinstance(loss_config, losses_pb2.LocalizationLoss):\n    raise ValueError(\'loss_config not of type losses_pb2.LocalizationLoss.\')\n\n  loss_type = loss_config.WhichOneof(\'localization_loss\')\n\n  if loss_type == \'weighted_l2\':\n    return losses.WeightedL2LocalizationLoss()\n\n  if loss_type == \'weighted_smooth_l1\':\n    return losses.WeightedSmoothL1LocalizationLoss(\n        loss_config.weighted_smooth_l1.delta)\n\n  if loss_type == \'weighted_iou\':\n    return losses.WeightedIOULocalizationLoss()\n\n  raise ValueError(\'Empty loss config.\')\n\n\ndef _build_classification_loss(loss_config):\n  """"""Builds a classification loss based on the loss config.\n\n  Args:\n    loss_config: A losses_pb2.ClassificationLoss object.\n\n  Returns:\n    Loss based on the config.\n\n  Raises:\n    ValueError: On invalid loss_config.\n  """"""\n  if not isinstance(loss_config, losses_pb2.ClassificationLoss):\n    raise ValueError(\'loss_config not of type losses_pb2.ClassificationLoss.\')\n\n  loss_type = loss_config.WhichOneof(\'classification_loss\')\n\n  if loss_type == \'weighted_sigmoid\':\n    return losses.WeightedSigmoidClassificationLoss()\n\n  if loss_type == \'weighted_sigmoid_focal\':\n    config = loss_config.weighted_sigmoid_focal\n    alpha = None\n    if config.HasField(\'alpha\'):\n      alpha = config.alpha\n    return losses.SigmoidFocalClassificationLoss(\n        gamma=config.gamma,\n        alpha=alpha)\n\n  if loss_type == \'weighted_softmax\':\n    config = loss_config.weighted_softmax\n    return losses.WeightedSoftmaxClassificationLoss(\n        logit_scale=config.logit_scale)\n\n  if loss_type == \'bootstrapped_sigmoid\':\n    config = loss_config.bootstrapped_sigmoid\n    return losses.BootstrappedSigmoidClassificationLoss(\n        alpha=config.alpha,\n        bootstrap_type=(\'hard\' if config.hard_bootstrap else \'soft\'))\n\n  raise ValueError(\'Empty loss config.\')\n'"
src/object_detection/builders/losses_builder_test.py,12,"b'# Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n""""""Tests for losses_builder.""""""\n\nimport tensorflow as tf\n\nfrom google.protobuf import text_format\nfrom object_detection.builders import losses_builder\nfrom object_detection.core import losses\nfrom object_detection.protos import losses_pb2\n\n\nclass LocalizationLossBuilderTest(tf.test.TestCase):\n\n  def test_build_weighted_l2_localization_loss(self):\n    losses_text_proto = """"""\n      localization_loss {\n        weighted_l2 {\n        }\n      }\n      classification_loss {\n        weighted_softmax {\n        }\n      }\n    """"""\n    losses_proto = losses_pb2.Loss()\n    text_format.Merge(losses_text_proto, losses_proto)\n    _, localization_loss, _, _, _ = losses_builder.build(losses_proto)\n    self.assertTrue(isinstance(localization_loss,\n                               losses.WeightedL2LocalizationLoss))\n\n  def test_build_weighted_smooth_l1_localization_loss_default_delta(self):\n    losses_text_proto = """"""\n      localization_loss {\n        weighted_smooth_l1 {\n        }\n      }\n      classification_loss {\n        weighted_softmax {\n        }\n      }\n    """"""\n    losses_proto = losses_pb2.Loss()\n    text_format.Merge(losses_text_proto, losses_proto)\n    _, localization_loss, _, _, _ = losses_builder.build(losses_proto)\n    self.assertTrue(isinstance(localization_loss,\n                               losses.WeightedSmoothL1LocalizationLoss))\n    self.assertAlmostEqual(localization_loss._delta, 1.0)\n\n  def test_build_weighted_smooth_l1_localization_loss_non_default_delta(self):\n    losses_text_proto = """"""\n      localization_loss {\n        weighted_smooth_l1 {\n          delta: 0.1\n        }\n      }\n      classification_loss {\n        weighted_softmax {\n        }\n      }\n    """"""\n    losses_proto = losses_pb2.Loss()\n    text_format.Merge(losses_text_proto, losses_proto)\n    _, localization_loss, _, _, _ = losses_builder.build(losses_proto)\n    self.assertTrue(isinstance(localization_loss,\n                               losses.WeightedSmoothL1LocalizationLoss))\n    self.assertAlmostEqual(localization_loss._delta, 0.1)\n\n  def test_build_weighted_iou_localization_loss(self):\n    losses_text_proto = """"""\n      localization_loss {\n        weighted_iou {\n        }\n      }\n      classification_loss {\n        weighted_softmax {\n        }\n      }\n    """"""\n    losses_proto = losses_pb2.Loss()\n    text_format.Merge(losses_text_proto, losses_proto)\n    _, localization_loss, _, _, _ = losses_builder.build(losses_proto)\n    self.assertTrue(isinstance(localization_loss,\n                               losses.WeightedIOULocalizationLoss))\n\n  def test_anchorwise_output(self):\n    losses_text_proto = """"""\n      localization_loss {\n        weighted_smooth_l1 {\n        }\n      }\n      classification_loss {\n        weighted_softmax {\n        }\n      }\n    """"""\n    losses_proto = losses_pb2.Loss()\n    text_format.Merge(losses_text_proto, losses_proto)\n    _, localization_loss, _, _, _ = losses_builder.build(losses_proto)\n    self.assertTrue(isinstance(localization_loss,\n                               losses.WeightedSmoothL1LocalizationLoss))\n    predictions = tf.constant([[[0.0, 0.0, 1.0, 1.0], [0.0, 0.0, 1.0, 1.0]]])\n    targets = tf.constant([[[0.0, 0.0, 1.0, 1.0], [0.0, 0.0, 1.0, 1.0]]])\n    weights = tf.constant([[1.0, 1.0]])\n    loss = localization_loss(predictions, targets, weights=weights)\n    self.assertEqual(loss.shape, [1, 2])\n\n  def test_raise_error_on_empty_localization_config(self):\n    losses_text_proto = """"""\n      classification_loss {\n        weighted_softmax {\n        }\n      }\n    """"""\n    losses_proto = losses_pb2.Loss()\n    text_format.Merge(losses_text_proto, losses_proto)\n    with self.assertRaises(ValueError):\n      losses_builder._build_localization_loss(losses_proto)\n\n\nclass ClassificationLossBuilderTest(tf.test.TestCase):\n\n  def test_build_weighted_sigmoid_classification_loss(self):\n    losses_text_proto = """"""\n      classification_loss {\n        weighted_sigmoid {\n        }\n      }\n      localization_loss {\n        weighted_l2 {\n        }\n      }\n    """"""\n    losses_proto = losses_pb2.Loss()\n    text_format.Merge(losses_text_proto, losses_proto)\n    classification_loss, _, _, _, _ = losses_builder.build(losses_proto)\n    self.assertTrue(isinstance(classification_loss,\n                               losses.WeightedSigmoidClassificationLoss))\n\n  def test_build_weighted_sigmoid_focal_classification_loss(self):\n    losses_text_proto = """"""\n      classification_loss {\n        weighted_sigmoid_focal {\n        }\n      }\n      localization_loss {\n        weighted_l2 {\n        }\n      }\n    """"""\n    losses_proto = losses_pb2.Loss()\n    text_format.Merge(losses_text_proto, losses_proto)\n    classification_loss, _, _, _, _ = losses_builder.build(losses_proto)\n    self.assertTrue(isinstance(classification_loss,\n                               losses.SigmoidFocalClassificationLoss))\n    self.assertAlmostEqual(classification_loss._alpha, None)\n    self.assertAlmostEqual(classification_loss._gamma, 2.0)\n\n  def test_build_weighted_sigmoid_focal_loss_non_default(self):\n    losses_text_proto = """"""\n      classification_loss {\n        weighted_sigmoid_focal {\n          alpha: 0.25\n          gamma: 3.0\n        }\n      }\n      localization_loss {\n        weighted_l2 {\n        }\n      }\n    """"""\n    losses_proto = losses_pb2.Loss()\n    text_format.Merge(losses_text_proto, losses_proto)\n    classification_loss, _, _, _, _ = losses_builder.build(losses_proto)\n    self.assertTrue(isinstance(classification_loss,\n                               losses.SigmoidFocalClassificationLoss))\n    self.assertAlmostEqual(classification_loss._alpha, 0.25)\n    self.assertAlmostEqual(classification_loss._gamma, 3.0)\n\n  def test_build_weighted_softmax_classification_loss(self):\n    losses_text_proto = """"""\n      classification_loss {\n        weighted_softmax {\n        }\n      }\n      localization_loss {\n        weighted_l2 {\n        }\n      }\n    """"""\n    losses_proto = losses_pb2.Loss()\n    text_format.Merge(losses_text_proto, losses_proto)\n    classification_loss, _, _, _, _ = losses_builder.build(losses_proto)\n    self.assertTrue(isinstance(classification_loss,\n                               losses.WeightedSoftmaxClassificationLoss))\n\n  def test_build_weighted_softmax_classification_loss_with_logit_scale(self):\n    losses_text_proto = """"""\n      classification_loss {\n        weighted_softmax {\n          logit_scale: 2.0\n        }\n      }\n      localization_loss {\n        weighted_l2 {\n        }\n      }\n    """"""\n    losses_proto = losses_pb2.Loss()\n    text_format.Merge(losses_text_proto, losses_proto)\n    classification_loss, _, _, _, _ = losses_builder.build(losses_proto)\n    self.assertTrue(isinstance(classification_loss,\n                               losses.WeightedSoftmaxClassificationLoss))\n\n  def test_build_bootstrapped_sigmoid_classification_loss(self):\n    losses_text_proto = """"""\n      classification_loss {\n        bootstrapped_sigmoid {\n          alpha: 0.5\n        }\n      }\n      localization_loss {\n        weighted_l2 {\n        }\n      }\n    """"""\n    losses_proto = losses_pb2.Loss()\n    text_format.Merge(losses_text_proto, losses_proto)\n    classification_loss, _, _, _, _ = losses_builder.build(losses_proto)\n    self.assertTrue(isinstance(classification_loss,\n                               losses.BootstrappedSigmoidClassificationLoss))\n\n  def test_anchorwise_output(self):\n    losses_text_proto = """"""\n      classification_loss {\n        weighted_sigmoid {\n          anchorwise_output: true\n        }\n      }\n      localization_loss {\n        weighted_l2 {\n        }\n      }\n    """"""\n    losses_proto = losses_pb2.Loss()\n    text_format.Merge(losses_text_proto, losses_proto)\n    classification_loss, _, _, _, _ = losses_builder.build(losses_proto)\n    self.assertTrue(isinstance(classification_loss,\n                               losses.WeightedSigmoidClassificationLoss))\n    predictions = tf.constant([[[0.0, 1.0, 0.0], [0.0, 0.5, 0.5]]])\n    targets = tf.constant([[[0.0, 1.0, 0.0], [0.0, 0.0, 1.0]]])\n    weights = tf.constant([[1.0, 1.0]])\n    loss = classification_loss(predictions, targets, weights=weights)\n    self.assertEqual(loss.shape, [1, 2, 3])\n\n  def test_raise_error_on_empty_config(self):\n    losses_text_proto = """"""\n      localization_loss {\n        weighted_l2 {\n        }\n      }\n    """"""\n    losses_proto = losses_pb2.Loss()\n    text_format.Merge(losses_text_proto, losses_proto)\n    with self.assertRaises(ValueError):\n      losses_builder.build(losses_proto)\n\n\nclass HardExampleMinerBuilderTest(tf.test.TestCase):\n\n  def test_do_not_build_hard_example_miner_by_default(self):\n    losses_text_proto = """"""\n      localization_loss {\n        weighted_l2 {\n        }\n      }\n      classification_loss {\n        weighted_softmax {\n        }\n      }\n    """"""\n    losses_proto = losses_pb2.Loss()\n    text_format.Merge(losses_text_proto, losses_proto)\n    _, _, _, _, hard_example_miner = losses_builder.build(losses_proto)\n    self.assertEqual(hard_example_miner, None)\n\n  def test_build_hard_example_miner_for_classification_loss(self):\n    losses_text_proto = """"""\n      localization_loss {\n        weighted_l2 {\n        }\n      }\n      classification_loss {\n        weighted_softmax {\n        }\n      }\n      hard_example_miner {\n        loss_type: CLASSIFICATION\n      }\n    """"""\n    losses_proto = losses_pb2.Loss()\n    text_format.Merge(losses_text_proto, losses_proto)\n    _, _, _, _, hard_example_miner = losses_builder.build(losses_proto)\n    self.assertTrue(isinstance(hard_example_miner, losses.HardExampleMiner))\n    self.assertEqual(hard_example_miner._loss_type, \'cls\')\n\n  def test_build_hard_example_miner_for_localization_loss(self):\n    losses_text_proto = """"""\n      localization_loss {\n        weighted_l2 {\n        }\n      }\n      classification_loss {\n        weighted_softmax {\n        }\n      }\n      hard_example_miner {\n        loss_type: LOCALIZATION\n      }\n    """"""\n    losses_proto = losses_pb2.Loss()\n    text_format.Merge(losses_text_proto, losses_proto)\n    _, _, _, _, hard_example_miner = losses_builder.build(losses_proto)\n    self.assertTrue(isinstance(hard_example_miner, losses.HardExampleMiner))\n    self.assertEqual(hard_example_miner._loss_type, \'loc\')\n\n  def test_build_hard_example_miner_with_non_default_values(self):\n    losses_text_proto = """"""\n      localization_loss {\n        weighted_l2 {\n        }\n      }\n      classification_loss {\n        weighted_softmax {\n        }\n      }\n      hard_example_miner {\n        num_hard_examples: 32\n        iou_threshold: 0.5\n        loss_type: LOCALIZATION\n        max_negatives_per_positive: 10\n        min_negatives_per_image: 3\n      }\n    """"""\n    losses_proto = losses_pb2.Loss()\n    text_format.Merge(losses_text_proto, losses_proto)\n    _, _, _, _, hard_example_miner = losses_builder.build(losses_proto)\n    self.assertTrue(isinstance(hard_example_miner, losses.HardExampleMiner))\n    self.assertEqual(hard_example_miner._num_hard_examples, 32)\n    self.assertAlmostEqual(hard_example_miner._iou_threshold, 0.5)\n    self.assertEqual(hard_example_miner._max_negatives_per_positive, 10)\n    self.assertEqual(hard_example_miner._min_negatives_per_image, 3)\n\n\nclass LossBuilderTest(tf.test.TestCase):\n\n  def test_build_all_loss_parameters(self):\n    losses_text_proto = """"""\n      localization_loss {\n        weighted_l2 {\n        }\n      }\n      classification_loss {\n        weighted_softmax {\n        }\n      }\n      hard_example_miner {\n      }\n      classification_weight: 0.8\n      localization_weight: 0.2\n    """"""\n    losses_proto = losses_pb2.Loss()\n    text_format.Merge(losses_text_proto, losses_proto)\n    (classification_loss, localization_loss,\n     classification_weight, localization_weight,\n     hard_example_miner) = losses_builder.build(losses_proto)\n    self.assertTrue(isinstance(hard_example_miner, losses.HardExampleMiner))\n    self.assertTrue(isinstance(classification_loss,\n                               losses.WeightedSoftmaxClassificationLoss))\n    self.assertTrue(isinstance(localization_loss,\n                               losses.WeightedL2LocalizationLoss))\n    self.assertAlmostEqual(classification_weight, 0.8)\n    self.assertAlmostEqual(localization_weight, 0.2)\n\n  def test_raise_error_when_both_focal_loss_and_hard_example_miner(self):\n    losses_text_proto = """"""\n      localization_loss {\n        weighted_l2 {\n        }\n      }\n      classification_loss {\n        weighted_sigmoid_focal {\n        }\n      }\n      hard_example_miner {\n      }\n      classification_weight: 0.8\n      localization_weight: 0.2\n    """"""\n    losses_proto = losses_pb2.Loss()\n    text_format.Merge(losses_text_proto, losses_proto)\n    with self.assertRaises(ValueError):\n      losses_builder.build(losses_proto)\n\n\nclass FasterRcnnClassificationLossBuilderTest(tf.test.TestCase):\n\n  def test_build_sigmoid_loss(self):\n    losses_text_proto = """"""\n      weighted_sigmoid {\n      }\n    """"""\n    losses_proto = losses_pb2.ClassificationLoss()\n    text_format.Merge(losses_text_proto, losses_proto)\n    classification_loss = losses_builder.build_faster_rcnn_classification_loss(\n        losses_proto)\n    self.assertTrue(isinstance(classification_loss,\n                               losses.WeightedSigmoidClassificationLoss))\n\n  def test_build_softmax_loss(self):\n    losses_text_proto = """"""\n      weighted_softmax {\n      }\n    """"""\n    losses_proto = losses_pb2.ClassificationLoss()\n    text_format.Merge(losses_text_proto, losses_proto)\n    classification_loss = losses_builder.build_faster_rcnn_classification_loss(\n        losses_proto)\n    self.assertTrue(isinstance(classification_loss,\n                               losses.WeightedSoftmaxClassificationLoss))\n\n  def test_build_softmax_loss_by_default(self):\n    losses_text_proto = """"""\n    """"""\n    losses_proto = losses_pb2.ClassificationLoss()\n    text_format.Merge(losses_text_proto, losses_proto)\n    classification_loss = losses_builder.build_faster_rcnn_classification_loss(\n        losses_proto)\n    self.assertTrue(isinstance(classification_loss,\n                               losses.WeightedSoftmaxClassificationLoss))\n\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
src/object_detection/builders/matcher_builder.py,0,"b'# Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n""""""A function to build an object detection matcher from configuration.""""""\n\nfrom object_detection.matchers import argmax_matcher\nfrom object_detection.matchers import bipartite_matcher\nfrom object_detection.protos import matcher_pb2\n\n\ndef build(matcher_config):\n  """"""Builds a matcher object based on the matcher config.\n\n  Args:\n    matcher_config: A matcher.proto object containing the config for the desired\n      Matcher.\n\n  Returns:\n    Matcher based on the config.\n\n  Raises:\n    ValueError: On empty matcher proto.\n  """"""\n  if not isinstance(matcher_config, matcher_pb2.Matcher):\n    raise ValueError(\'matcher_config not of type matcher_pb2.Matcher.\')\n  if matcher_config.WhichOneof(\'matcher_oneof\') == \'argmax_matcher\':\n    matcher = matcher_config.argmax_matcher\n    matched_threshold = unmatched_threshold = None\n    if not matcher.ignore_thresholds:\n      matched_threshold = matcher.matched_threshold\n      unmatched_threshold = matcher.unmatched_threshold\n    return argmax_matcher.ArgMaxMatcher(\n        matched_threshold=matched_threshold,\n        unmatched_threshold=unmatched_threshold,\n        negatives_lower_than_unmatched=matcher.negatives_lower_than_unmatched,\n        force_match_for_each_row=matcher.force_match_for_each_row,\n        use_matmul_gather=matcher.use_matmul_gather)\n  if matcher_config.WhichOneof(\'matcher_oneof\') == \'bipartite_matcher\':\n    matcher = matcher_config.bipartite_matcher\n    return bipartite_matcher.GreedyBipartiteMatcher(matcher.use_matmul_gather)\n  raise ValueError(\'Empty matcher.\')\n'"
src/object_detection/builders/matcher_builder_test.py,2,"b'# Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n""""""Tests for matcher_builder.""""""\n\nimport tensorflow as tf\n\nfrom google.protobuf import text_format\nfrom object_detection.builders import matcher_builder\nfrom object_detection.matchers import argmax_matcher\nfrom object_detection.matchers import bipartite_matcher\nfrom object_detection.protos import matcher_pb2\n\n\nclass MatcherBuilderTest(tf.test.TestCase):\n\n  def test_build_arg_max_matcher_with_defaults(self):\n    matcher_text_proto = """"""\n      argmax_matcher {\n      }\n    """"""\n    matcher_proto = matcher_pb2.Matcher()\n    text_format.Merge(matcher_text_proto, matcher_proto)\n    matcher_object = matcher_builder.build(matcher_proto)\n    self.assertTrue(isinstance(matcher_object, argmax_matcher.ArgMaxMatcher))\n    self.assertAlmostEqual(matcher_object._matched_threshold, 0.5)\n    self.assertAlmostEqual(matcher_object._unmatched_threshold, 0.5)\n    self.assertTrue(matcher_object._negatives_lower_than_unmatched)\n    self.assertFalse(matcher_object._force_match_for_each_row)\n\n  def test_build_arg_max_matcher_without_thresholds(self):\n    matcher_text_proto = """"""\n      argmax_matcher {\n        ignore_thresholds: true\n      }\n    """"""\n    matcher_proto = matcher_pb2.Matcher()\n    text_format.Merge(matcher_text_proto, matcher_proto)\n    matcher_object = matcher_builder.build(matcher_proto)\n    self.assertTrue(isinstance(matcher_object, argmax_matcher.ArgMaxMatcher))\n    self.assertEqual(matcher_object._matched_threshold, None)\n    self.assertEqual(matcher_object._unmatched_threshold, None)\n    self.assertTrue(matcher_object._negatives_lower_than_unmatched)\n    self.assertFalse(matcher_object._force_match_for_each_row)\n\n  def test_build_arg_max_matcher_with_non_default_parameters(self):\n    matcher_text_proto = """"""\n      argmax_matcher {\n        matched_threshold: 0.7\n        unmatched_threshold: 0.3\n        negatives_lower_than_unmatched: false\n        force_match_for_each_row: true\n        use_matmul_gather: true\n      }\n    """"""\n    matcher_proto = matcher_pb2.Matcher()\n    text_format.Merge(matcher_text_proto, matcher_proto)\n    matcher_object = matcher_builder.build(matcher_proto)\n    self.assertTrue(isinstance(matcher_object, argmax_matcher.ArgMaxMatcher))\n    self.assertAlmostEqual(matcher_object._matched_threshold, 0.7)\n    self.assertAlmostEqual(matcher_object._unmatched_threshold, 0.3)\n    self.assertFalse(matcher_object._negatives_lower_than_unmatched)\n    self.assertTrue(matcher_object._force_match_for_each_row)\n    self.assertTrue(matcher_object._use_matmul_gather)\n\n  def test_build_bipartite_matcher(self):\n    matcher_text_proto = """"""\n      bipartite_matcher {\n      }\n    """"""\n    matcher_proto = matcher_pb2.Matcher()\n    text_format.Merge(matcher_text_proto, matcher_proto)\n    matcher_object = matcher_builder.build(matcher_proto)\n    self.assertTrue(\n        isinstance(matcher_object, bipartite_matcher.GreedyBipartiteMatcher))\n\n  def test_raise_error_on_empty_matcher(self):\n    matcher_text_proto = """"""\n    """"""\n    matcher_proto = matcher_pb2.Matcher()\n    text_format.Merge(matcher_text_proto, matcher_proto)\n    with self.assertRaises(ValueError):\n      matcher_builder.build(matcher_proto)\n\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
src/object_detection/builders/model_builder.py,0,"b'# Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n""""""A function to build a DetectionModel from configuration.""""""\nfrom object_detection.builders import anchor_generator_builder\nfrom object_detection.builders import box_coder_builder\nfrom object_detection.builders import box_predictor_builder\nfrom object_detection.builders import hyperparams_builder\nfrom object_detection.builders import image_resizer_builder\nfrom object_detection.builders import losses_builder\nfrom object_detection.builders import matcher_builder\nfrom object_detection.builders import post_processing_builder\nfrom object_detection.builders import region_similarity_calculator_builder as sim_calc\nfrom object_detection.core import box_predictor\nfrom object_detection.meta_architectures import faster_rcnn_meta_arch\nfrom object_detection.meta_architectures import rfcn_meta_arch\nfrom object_detection.meta_architectures import ssd_meta_arch\nfrom object_detection.models import faster_rcnn_inception_resnet_v2_feature_extractor as frcnn_inc_res\nfrom object_detection.models import faster_rcnn_inception_v2_feature_extractor as frcnn_inc_v2\nfrom object_detection.models import faster_rcnn_nas_feature_extractor as frcnn_nas\nfrom object_detection.models import faster_rcnn_resnet_v1_feature_extractor as frcnn_resnet_v1\nfrom object_detection.models import ssd_resnet_v1_fpn_feature_extractor as ssd_resnet_v1_fpn\nfrom object_detection.models.embedded_ssd_mobilenet_v1_feature_extractor import EmbeddedSSDMobileNetV1FeatureExtractor\nfrom object_detection.models.ssd_inception_v2_feature_extractor import SSDInceptionV2FeatureExtractor\nfrom object_detection.models.ssd_inception_v3_feature_extractor import SSDInceptionV3FeatureExtractor\nfrom object_detection.models.ssd_mobilenet_v1_feature_extractor import SSDMobileNetV1FeatureExtractor\nfrom object_detection.models.ssd_mobilenet_v2_feature_extractor import SSDMobileNetV2FeatureExtractor\nfrom object_detection.protos import model_pb2\n\n# A map of names to SSD feature extractors.\nSSD_FEATURE_EXTRACTOR_CLASS_MAP = {\n    \'ssd_inception_v2\': SSDInceptionV2FeatureExtractor,\n    \'ssd_inception_v3\': SSDInceptionV3FeatureExtractor,\n    \'ssd_mobilenet_v1\': SSDMobileNetV1FeatureExtractor,\n    \'ssd_mobilenet_v2\': SSDMobileNetV2FeatureExtractor,\n    \'ssd_resnet50_v1_fpn\': ssd_resnet_v1_fpn.SSDResnet50V1FpnFeatureExtractor,\n    \'ssd_resnet101_v1_fpn\': ssd_resnet_v1_fpn.SSDResnet101V1FpnFeatureExtractor,\n    \'ssd_resnet152_v1_fpn\': ssd_resnet_v1_fpn.SSDResnet152V1FpnFeatureExtractor,\n    \'embedded_ssd_mobilenet_v1\': EmbeddedSSDMobileNetV1FeatureExtractor,\n}\n\n# A map of names to Faster R-CNN feature extractors.\nFASTER_RCNN_FEATURE_EXTRACTOR_CLASS_MAP = {\n    \'faster_rcnn_nas\':\n    frcnn_nas.FasterRCNNNASFeatureExtractor,\n    \'faster_rcnn_inception_resnet_v2\':\n    frcnn_inc_res.FasterRCNNInceptionResnetV2FeatureExtractor,\n    \'faster_rcnn_inception_v2\':\n    frcnn_inc_v2.FasterRCNNInceptionV2FeatureExtractor,\n    \'faster_rcnn_resnet50\':\n    frcnn_resnet_v1.FasterRCNNResnet50FeatureExtractor,\n    \'faster_rcnn_resnet101\':\n    frcnn_resnet_v1.FasterRCNNResnet101FeatureExtractor,\n    \'faster_rcnn_resnet152\':\n    frcnn_resnet_v1.FasterRCNNResnet152FeatureExtractor,\n}\n\n\ndef build(model_config, is_training, add_summaries=True):\n  """"""Builds a DetectionModel based on the model config.\n\n  Args:\n    model_config: A model.proto object containing the config for the desired\n      DetectionModel.\n    is_training: True if this model is being built for training purposes.\n    add_summaries: Whether to add tensorflow summaries in the model graph.\n\n  Returns:\n    DetectionModel based on the config.\n\n  Raises:\n    ValueError: On invalid meta architecture or model.\n  """"""\n  if not isinstance(model_config, model_pb2.DetectionModel):\n    raise ValueError(\'model_config not of type model_pb2.DetectionModel.\')\n  meta_architecture = model_config.WhichOneof(\'model\')\n  if meta_architecture == \'ssd\':\n    return _build_ssd_model(model_config.ssd, is_training, add_summaries)\n  if meta_architecture == \'faster_rcnn\':\n    return _build_faster_rcnn_model(model_config.faster_rcnn, is_training,\n                                    add_summaries)\n  raise ValueError(\'Unknown meta architecture: {}\'.format(meta_architecture))\n\n\ndef _build_ssd_feature_extractor(feature_extractor_config, is_training,\n                                 reuse_weights=None):\n  """"""Builds a ssd_meta_arch.SSDFeatureExtractor based on config.\n\n  Args:\n    feature_extractor_config: A SSDFeatureExtractor proto config from ssd.proto.\n    is_training: True if this feature extractor is being built for training.\n    reuse_weights: if the feature extractor should reuse weights.\n\n  Returns:\n    ssd_meta_arch.SSDFeatureExtractor based on config.\n\n  Raises:\n    ValueError: On invalid feature extractor type.\n  """"""\n  feature_type = feature_extractor_config.type\n  depth_multiplier = feature_extractor_config.depth_multiplier\n  min_depth = feature_extractor_config.min_depth\n  pad_to_multiple = feature_extractor_config.pad_to_multiple\n  batch_norm_trainable = feature_extractor_config.batch_norm_trainable\n  use_explicit_padding = feature_extractor_config.use_explicit_padding\n  use_depthwise = feature_extractor_config.use_depthwise\n  conv_hyperparams = hyperparams_builder.build(\n      feature_extractor_config.conv_hyperparams, is_training)\n\n  if feature_type not in SSD_FEATURE_EXTRACTOR_CLASS_MAP:\n    raise ValueError(\'Unknown ssd feature_extractor: {}\'.format(feature_type))\n\n  feature_extractor_class = SSD_FEATURE_EXTRACTOR_CLASS_MAP[feature_type]\n  return feature_extractor_class(is_training, depth_multiplier, min_depth,\n                                 pad_to_multiple, conv_hyperparams,\n                                 batch_norm_trainable, reuse_weights,\n                                 use_explicit_padding, use_depthwise)\n\n\ndef _build_ssd_model(ssd_config, is_training, add_summaries):\n  """"""Builds an SSD detection model based on the model config.\n\n  Args:\n    ssd_config: A ssd.proto object containing the config for the desired\n      SSDMetaArch.\n    is_training: True if this model is being built for training purposes.\n    add_summaries: Whether to add tf summaries in the model.\n\n  Returns:\n    SSDMetaArch based on the config.\n  Raises:\n    ValueError: If ssd_config.type is not recognized (i.e. not registered in\n      model_class_map).\n  """"""\n  num_classes = ssd_config.num_classes\n\n  # Feature extractor\n  feature_extractor = _build_ssd_feature_extractor(ssd_config.feature_extractor,\n                                                   is_training)\n\n  box_coder = box_coder_builder.build(ssd_config.box_coder)\n  matcher = matcher_builder.build(ssd_config.matcher)\n  region_similarity_calculator = sim_calc.build(\n      ssd_config.similarity_calculator)\n  encode_background_as_zeros = ssd_config.encode_background_as_zeros\n  negative_class_weight = ssd_config.negative_class_weight\n  ssd_box_predictor = box_predictor_builder.build(hyperparams_builder.build,\n                                                  ssd_config.box_predictor,\n                                                  is_training, num_classes)\n  anchor_generator = anchor_generator_builder.build(\n      ssd_config.anchor_generator)\n  image_resizer_fn = image_resizer_builder.build(ssd_config.image_resizer)\n  non_max_suppression_fn, score_conversion_fn = post_processing_builder.build(\n      ssd_config.post_processing)\n  (classification_loss, localization_loss, classification_weight,\n   localization_weight,\n   hard_example_miner) = losses_builder.build(ssd_config.loss)\n  normalize_loss_by_num_matches = ssd_config.normalize_loss_by_num_matches\n  normalize_loc_loss_by_codesize = ssd_config.normalize_loc_loss_by_codesize\n\n  return ssd_meta_arch.SSDMetaArch(\n      is_training,\n      anchor_generator,\n      ssd_box_predictor,\n      box_coder,\n      feature_extractor,\n      matcher,\n      region_similarity_calculator,\n      encode_background_as_zeros,\n      negative_class_weight,\n      image_resizer_fn,\n      non_max_suppression_fn,\n      score_conversion_fn,\n      classification_loss,\n      localization_loss,\n      classification_weight,\n      localization_weight,\n      normalize_loss_by_num_matches,\n      hard_example_miner,\n      add_summaries=add_summaries,\n      normalize_loc_loss_by_codesize=normalize_loc_loss_by_codesize)\n\n\ndef _build_faster_rcnn_feature_extractor(\n    feature_extractor_config, is_training, reuse_weights=None):\n  """"""Builds a faster_rcnn_meta_arch.FasterRCNNFeatureExtractor based on config.\n\n  Args:\n    feature_extractor_config: A FasterRcnnFeatureExtractor proto config from\n      faster_rcnn.proto.\n    is_training: True if this feature extractor is being built for training.\n    reuse_weights: if the feature extractor should reuse weights.\n\n  Returns:\n    faster_rcnn_meta_arch.FasterRCNNFeatureExtractor based on config.\n\n  Raises:\n    ValueError: On invalid feature extractor type.\n  """"""\n  feature_type = feature_extractor_config.type\n  first_stage_features_stride = (\n      feature_extractor_config.first_stage_features_stride)\n  batch_norm_trainable = feature_extractor_config.batch_norm_trainable\n\n  if feature_type not in FASTER_RCNN_FEATURE_EXTRACTOR_CLASS_MAP:\n    raise ValueError(\'Unknown Faster R-CNN feature_extractor: {}\'.format(\n        feature_type))\n  feature_extractor_class = FASTER_RCNN_FEATURE_EXTRACTOR_CLASS_MAP[\n      feature_type]\n  return feature_extractor_class(\n      is_training, first_stage_features_stride,\n      batch_norm_trainable, reuse_weights)\n\n\ndef _build_faster_rcnn_model(frcnn_config, is_training, add_summaries):\n  """"""Builds a Faster R-CNN or R-FCN detection model based on the model config.\n\n  Builds R-FCN model if the second_stage_box_predictor in the config is of type\n  `rfcn_box_predictor` else builds a Faster R-CNN model.\n\n  Args:\n    frcnn_config: A faster_rcnn.proto object containing the config for the\n      desired FasterRCNNMetaArch or RFCNMetaArch.\n    is_training: True if this model is being built for training purposes.\n    add_summaries: Whether to add tf summaries in the model.\n\n  Returns:\n    FasterRCNNMetaArch based on the config.\n  Raises:\n    ValueError: If frcnn_config.type is not recognized (i.e. not registered in\n      model_class_map).\n  """"""\n  num_classes = frcnn_config.num_classes\n  image_resizer_fn = image_resizer_builder.build(frcnn_config.image_resizer)\n\n  feature_extractor = _build_faster_rcnn_feature_extractor(\n      frcnn_config.feature_extractor, is_training)\n\n  number_of_stages = frcnn_config.number_of_stages\n  first_stage_anchor_generator = anchor_generator_builder.build(\n      frcnn_config.first_stage_anchor_generator)\n\n  first_stage_atrous_rate = frcnn_config.first_stage_atrous_rate\n  first_stage_box_predictor_arg_scope = hyperparams_builder.build(\n      frcnn_config.first_stage_box_predictor_conv_hyperparams, is_training)\n  first_stage_box_predictor_kernel_size = (\n      frcnn_config.first_stage_box_predictor_kernel_size)\n  first_stage_box_predictor_depth = frcnn_config.first_stage_box_predictor_depth\n  first_stage_minibatch_size = frcnn_config.first_stage_minibatch_size\n  first_stage_positive_balance_fraction = (\n      frcnn_config.first_stage_positive_balance_fraction)\n  first_stage_nms_score_threshold = frcnn_config.first_stage_nms_score_threshold\n  first_stage_nms_iou_threshold = frcnn_config.first_stage_nms_iou_threshold\n  first_stage_max_proposals = frcnn_config.first_stage_max_proposals\n  first_stage_loc_loss_weight = (\n      frcnn_config.first_stage_localization_loss_weight)\n  first_stage_obj_loss_weight = frcnn_config.first_stage_objectness_loss_weight\n\n  initial_crop_size = frcnn_config.initial_crop_size\n  maxpool_kernel_size = frcnn_config.maxpool_kernel_size\n  maxpool_stride = frcnn_config.maxpool_stride\n\n  second_stage_box_predictor = box_predictor_builder.build(\n      hyperparams_builder.build,\n      frcnn_config.second_stage_box_predictor,\n      is_training=is_training,\n      num_classes=num_classes)\n  second_stage_batch_size = frcnn_config.second_stage_batch_size\n  second_stage_balance_fraction = frcnn_config.second_stage_balance_fraction\n  (second_stage_non_max_suppression_fn, second_stage_score_conversion_fn\n  ) = post_processing_builder.build(frcnn_config.second_stage_post_processing)\n  second_stage_localization_loss_weight = (\n      frcnn_config.second_stage_localization_loss_weight)\n  second_stage_classification_loss = (\n      losses_builder.build_faster_rcnn_classification_loss(\n          frcnn_config.second_stage_classification_loss))\n  second_stage_classification_loss_weight = (\n      frcnn_config.second_stage_classification_loss_weight)\n  second_stage_mask_prediction_loss_weight = (\n      frcnn_config.second_stage_mask_prediction_loss_weight)\n\n  hard_example_miner = None\n  if frcnn_config.HasField(\'hard_example_miner\'):\n    hard_example_miner = losses_builder.build_hard_example_miner(\n        frcnn_config.hard_example_miner,\n        second_stage_classification_loss_weight,\n        second_stage_localization_loss_weight)\n\n  common_kwargs = {\n      \'is_training\': is_training,\n      \'num_classes\': num_classes,\n      \'image_resizer_fn\': image_resizer_fn,\n      \'feature_extractor\': feature_extractor,\n      \'number_of_stages\': number_of_stages,\n      \'first_stage_anchor_generator\': first_stage_anchor_generator,\n      \'first_stage_atrous_rate\': first_stage_atrous_rate,\n      \'first_stage_box_predictor_arg_scope\':\n      first_stage_box_predictor_arg_scope,\n      \'first_stage_box_predictor_kernel_size\':\n      first_stage_box_predictor_kernel_size,\n      \'first_stage_box_predictor_depth\': first_stage_box_predictor_depth,\n      \'first_stage_minibatch_size\': first_stage_minibatch_size,\n      \'first_stage_positive_balance_fraction\':\n      first_stage_positive_balance_fraction,\n      \'first_stage_nms_score_threshold\': first_stage_nms_score_threshold,\n      \'first_stage_nms_iou_threshold\': first_stage_nms_iou_threshold,\n      \'first_stage_max_proposals\': first_stage_max_proposals,\n      \'first_stage_localization_loss_weight\': first_stage_loc_loss_weight,\n      \'first_stage_objectness_loss_weight\': first_stage_obj_loss_weight,\n      \'second_stage_batch_size\': second_stage_batch_size,\n      \'second_stage_balance_fraction\': second_stage_balance_fraction,\n      \'second_stage_non_max_suppression_fn\':\n      second_stage_non_max_suppression_fn,\n      \'second_stage_score_conversion_fn\': second_stage_score_conversion_fn,\n      \'second_stage_localization_loss_weight\':\n      second_stage_localization_loss_weight,\n      \'second_stage_classification_loss\':\n      second_stage_classification_loss,\n      \'second_stage_classification_loss_weight\':\n      second_stage_classification_loss_weight,\n      \'hard_example_miner\': hard_example_miner,\n      \'add_summaries\': add_summaries}\n\n  if isinstance(second_stage_box_predictor, box_predictor.RfcnBoxPredictor):\n    return rfcn_meta_arch.RFCNMetaArch(\n        second_stage_rfcn_box_predictor=second_stage_box_predictor,\n        **common_kwargs)\n  else:\n    return faster_rcnn_meta_arch.FasterRCNNMetaArch(\n        initial_crop_size=initial_crop_size,\n        maxpool_kernel_size=maxpool_kernel_size,\n        maxpool_stride=maxpool_stride,\n        second_stage_mask_rcnn_box_predictor=second_stage_box_predictor,\n        second_stage_mask_prediction_loss_weight=(\n            second_stage_mask_prediction_loss_weight),\n        **common_kwargs)\n'"
src/object_detection/builders/model_builder_test.py,2,"b'# Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n""""""Tests for object_detection.models.model_builder.""""""\n\nimport tensorflow as tf\n\nfrom google.protobuf import text_format\nfrom object_detection.builders import model_builder\nfrom object_detection.meta_architectures import faster_rcnn_meta_arch\nfrom object_detection.meta_architectures import rfcn_meta_arch\nfrom object_detection.meta_architectures import ssd_meta_arch\nfrom object_detection.models import faster_rcnn_inception_resnet_v2_feature_extractor as frcnn_inc_res\nfrom object_detection.models import faster_rcnn_inception_v2_feature_extractor as frcnn_inc_v2\nfrom object_detection.models import faster_rcnn_nas_feature_extractor as frcnn_nas\nfrom object_detection.models import faster_rcnn_resnet_v1_feature_extractor as frcnn_resnet_v1\nfrom object_detection.models import ssd_resnet_v1_fpn_feature_extractor as ssd_resnet_v1_fpn\nfrom object_detection.models.embedded_ssd_mobilenet_v1_feature_extractor import EmbeddedSSDMobileNetV1FeatureExtractor\nfrom object_detection.models.ssd_inception_v2_feature_extractor import SSDInceptionV2FeatureExtractor\nfrom object_detection.models.ssd_inception_v3_feature_extractor import SSDInceptionV3FeatureExtractor\nfrom object_detection.models.ssd_mobilenet_v1_feature_extractor import SSDMobileNetV1FeatureExtractor\nfrom object_detection.models.ssd_mobilenet_v2_feature_extractor import SSDMobileNetV2FeatureExtractor\nfrom object_detection.protos import model_pb2\n\nFRCNN_RESNET_FEAT_MAPS = {\n    \'faster_rcnn_resnet50\':\n    frcnn_resnet_v1.FasterRCNNResnet50FeatureExtractor,\n    \'faster_rcnn_resnet101\':\n    frcnn_resnet_v1.FasterRCNNResnet101FeatureExtractor,\n    \'faster_rcnn_resnet152\':\n    frcnn_resnet_v1.FasterRCNNResnet152FeatureExtractor\n}\n\nSSD_RESNET_V1_FPN_FEAT_MAPS = {\n    \'ssd_resnet50_v1_fpn\':\n    ssd_resnet_v1_fpn.SSDResnet50V1FpnFeatureExtractor,\n    \'ssd_resnet101_v1_fpn\':\n    ssd_resnet_v1_fpn.SSDResnet101V1FpnFeatureExtractor,\n    \'ssd_resnet152_v1_fpn\':\n    ssd_resnet_v1_fpn.SSDResnet152V1FpnFeatureExtractor\n}\n\n\nclass ModelBuilderTest(tf.test.TestCase):\n\n  def create_model(self, model_config):\n    """"""Builds a DetectionModel based on the model config.\n\n    Args:\n      model_config: A model.proto object containing the config for the desired\n        DetectionModel.\n\n    Returns:\n      DetectionModel based on the config.\n    """"""\n    return model_builder.build(model_config, is_training=True)\n\n  def test_create_ssd_inception_v2_model_from_config(self):\n    model_text_proto = """"""\n      ssd {\n        feature_extractor {\n          type: \'ssd_inception_v2\'\n          conv_hyperparams {\n            regularizer {\n                l2_regularizer {\n                }\n              }\n              initializer {\n                truncated_normal_initializer {\n                }\n              }\n          }\n        }\n        box_coder {\n          faster_rcnn_box_coder {\n          }\n        }\n        matcher {\n          argmax_matcher {\n          }\n        }\n        similarity_calculator {\n          iou_similarity {\n          }\n        }\n        anchor_generator {\n          ssd_anchor_generator {\n            aspect_ratios: 1.0\n          }\n        }\n        image_resizer {\n          fixed_shape_resizer {\n            height: 320\n            width: 320\n          }\n        }\n        box_predictor {\n          convolutional_box_predictor {\n            conv_hyperparams {\n              regularizer {\n                l2_regularizer {\n                }\n              }\n              initializer {\n                truncated_normal_initializer {\n                }\n              }\n            }\n          }\n        }\n        loss {\n          classification_loss {\n            weighted_softmax {\n            }\n          }\n          localization_loss {\n            weighted_smooth_l1 {\n            }\n          }\n        }\n      }""""""\n    model_proto = model_pb2.DetectionModel()\n    text_format.Merge(model_text_proto, model_proto)\n    model = self.create_model(model_proto)\n    self.assertIsInstance(model, ssd_meta_arch.SSDMetaArch)\n    self.assertIsInstance(model._feature_extractor,\n                          SSDInceptionV2FeatureExtractor)\n\n  def test_create_ssd_inception_v3_model_from_config(self):\n    model_text_proto = """"""\n      ssd {\n        feature_extractor {\n          type: \'ssd_inception_v3\'\n          conv_hyperparams {\n            regularizer {\n                l2_regularizer {\n                }\n              }\n              initializer {\n                truncated_normal_initializer {\n                }\n              }\n          }\n        }\n        box_coder {\n          faster_rcnn_box_coder {\n          }\n        }\n        matcher {\n          argmax_matcher {\n          }\n        }\n        similarity_calculator {\n          iou_similarity {\n          }\n        }\n        anchor_generator {\n          ssd_anchor_generator {\n            aspect_ratios: 1.0\n          }\n        }\n        image_resizer {\n          fixed_shape_resizer {\n            height: 320\n            width: 320\n          }\n        }\n        box_predictor {\n          convolutional_box_predictor {\n            conv_hyperparams {\n              regularizer {\n                l2_regularizer {\n                }\n              }\n              initializer {\n                truncated_normal_initializer {\n                }\n              }\n            }\n          }\n        }\n        loss {\n          classification_loss {\n            weighted_softmax {\n            }\n          }\n          localization_loss {\n            weighted_smooth_l1 {\n            }\n          }\n        }\n      }""""""\n    model_proto = model_pb2.DetectionModel()\n    text_format.Merge(model_text_proto, model_proto)\n    model = self.create_model(model_proto)\n    self.assertIsInstance(model, ssd_meta_arch.SSDMetaArch)\n    self.assertIsInstance(model._feature_extractor,\n                          SSDInceptionV3FeatureExtractor)\n\n  def test_create_ssd_resnet_v1_fpn_model_from_config(self):\n    model_text_proto = """"""\n      ssd {\n        feature_extractor {\n          type: \'ssd_resnet50_v1_fpn\'\n          conv_hyperparams {\n            regularizer {\n                l2_regularizer {\n                }\n              }\n              initializer {\n                truncated_normal_initializer {\n                }\n              }\n          }\n          batch_norm_trainable: true\n        }\n        box_coder {\n          faster_rcnn_box_coder {\n          }\n        }\n        matcher {\n          argmax_matcher {\n          }\n        }\n        similarity_calculator {\n          iou_similarity {\n          }\n        }\n        encode_background_as_zeros: true\n        anchor_generator {\n          multiscale_anchor_generator {\n            aspect_ratios: [1.0, 2.0, 0.5]\n            scales_per_octave: 2\n          }\n        }\n        image_resizer {\n          fixed_shape_resizer {\n            height: 320\n            width: 320\n          }\n        }\n        box_predictor {\n          weight_shared_convolutional_box_predictor {\n            depth: 32\n            conv_hyperparams {\n              regularizer {\n                l2_regularizer {\n                }\n              }\n              initializer {\n                random_normal_initializer {\n                }\n              }\n            }\n            num_layers_before_predictor: 1\n          }\n        }\n        normalize_loss_by_num_matches: true\n        normalize_loc_loss_by_codesize: true\n        loss {\n          classification_loss {\n            weighted_sigmoid_focal {\n              alpha: 0.25\n              gamma: 2.0\n            }\n          }\n          localization_loss {\n            weighted_smooth_l1 {\n              delta: 0.1\n            }\n          }\n          classification_weight: 1.0\n          localization_weight: 1.0\n        }\n      }""""""\n    model_proto = model_pb2.DetectionModel()\n    text_format.Merge(model_text_proto, model_proto)\n\n    for extractor_type, extractor_class in SSD_RESNET_V1_FPN_FEAT_MAPS.items():\n      model_proto.ssd.feature_extractor.type = extractor_type\n      model = model_builder.build(model_proto, is_training=True)\n      self.assertIsInstance(model, ssd_meta_arch.SSDMetaArch)\n      self.assertIsInstance(model._feature_extractor, extractor_class)\n\n  def test_create_ssd_mobilenet_v1_model_from_config(self):\n    model_text_proto = """"""\n      ssd {\n        feature_extractor {\n          type: \'ssd_mobilenet_v1\'\n          conv_hyperparams {\n            regularizer {\n                l2_regularizer {\n                }\n              }\n              initializer {\n                truncated_normal_initializer {\n                }\n              }\n          }\n          batch_norm_trainable: true\n        }\n        box_coder {\n          faster_rcnn_box_coder {\n          }\n        }\n        matcher {\n          argmax_matcher {\n          }\n        }\n        similarity_calculator {\n          iou_similarity {\n          }\n        }\n        anchor_generator {\n          ssd_anchor_generator {\n            aspect_ratios: 1.0\n          }\n        }\n        image_resizer {\n          fixed_shape_resizer {\n            height: 320\n            width: 320\n          }\n        }\n        box_predictor {\n          convolutional_box_predictor {\n            conv_hyperparams {\n              regularizer {\n                l2_regularizer {\n                }\n              }\n              initializer {\n                truncated_normal_initializer {\n                }\n              }\n            }\n          }\n        }\n        normalize_loc_loss_by_codesize: true\n        loss {\n          classification_loss {\n            weighted_softmax {\n            }\n          }\n          localization_loss {\n            weighted_smooth_l1 {\n            }\n          }\n        }\n      }""""""\n    model_proto = model_pb2.DetectionModel()\n    text_format.Merge(model_text_proto, model_proto)\n    model = self.create_model(model_proto)\n    self.assertIsInstance(model, ssd_meta_arch.SSDMetaArch)\n    self.assertIsInstance(model._feature_extractor,\n                          SSDMobileNetV1FeatureExtractor)\n    self.assertTrue(model._feature_extractor._batch_norm_trainable)\n    self.assertTrue(model._normalize_loc_loss_by_codesize)\n\n  def test_create_ssd_mobilenet_v2_model_from_config(self):\n    model_text_proto = """"""\n      ssd {\n        feature_extractor {\n          type: \'ssd_mobilenet_v2\'\n          conv_hyperparams {\n            regularizer {\n                l2_regularizer {\n                }\n              }\n              initializer {\n                truncated_normal_initializer {\n                }\n              }\n          }\n          batch_norm_trainable: true\n        }\n        box_coder {\n          faster_rcnn_box_coder {\n          }\n        }\n        matcher {\n          argmax_matcher {\n          }\n        }\n        similarity_calculator {\n          iou_similarity {\n          }\n        }\n        anchor_generator {\n          ssd_anchor_generator {\n            aspect_ratios: 1.0\n          }\n        }\n        image_resizer {\n          fixed_shape_resizer {\n            height: 320\n            width: 320\n          }\n        }\n        box_predictor {\n          convolutional_box_predictor {\n            conv_hyperparams {\n              regularizer {\n                l2_regularizer {\n                }\n              }\n              initializer {\n                truncated_normal_initializer {\n                }\n              }\n            }\n          }\n        }\n        normalize_loc_loss_by_codesize: true\n        loss {\n          classification_loss {\n            weighted_softmax {\n            }\n          }\n          localization_loss {\n            weighted_smooth_l1 {\n            }\n          }\n        }\n      }""""""\n    model_proto = model_pb2.DetectionModel()\n    text_format.Merge(model_text_proto, model_proto)\n    model = self.create_model(model_proto)\n    self.assertIsInstance(model, ssd_meta_arch.SSDMetaArch)\n    self.assertIsInstance(model._feature_extractor,\n                          SSDMobileNetV2FeatureExtractor)\n    self.assertTrue(model._feature_extractor._batch_norm_trainable)\n    self.assertTrue(model._normalize_loc_loss_by_codesize)\n\n  def test_create_embedded_ssd_mobilenet_v1_model_from_config(self):\n    model_text_proto = """"""\n      ssd {\n        feature_extractor {\n          type: \'embedded_ssd_mobilenet_v1\'\n          conv_hyperparams {\n            regularizer {\n                l2_regularizer {\n                }\n              }\n              initializer {\n                truncated_normal_initializer {\n                }\n              }\n          }\n          batch_norm_trainable: true\n        }\n        box_coder {\n          faster_rcnn_box_coder {\n          }\n        }\n        matcher {\n          argmax_matcher {\n          }\n        }\n        similarity_calculator {\n          iou_similarity {\n          }\n        }\n        anchor_generator {\n          ssd_anchor_generator {\n            aspect_ratios: 1.0\n          }\n        }\n        image_resizer {\n          fixed_shape_resizer {\n            height: 256\n            width: 256\n          }\n        }\n        box_predictor {\n          convolutional_box_predictor {\n            conv_hyperparams {\n              regularizer {\n                l2_regularizer {\n                }\n              }\n              initializer {\n                truncated_normal_initializer {\n                }\n              }\n            }\n          }\n        }\n        loss {\n          classification_loss {\n            weighted_softmax {\n            }\n          }\n          localization_loss {\n            weighted_smooth_l1 {\n            }\n          }\n        }\n      }""""""\n    model_proto = model_pb2.DetectionModel()\n    text_format.Merge(model_text_proto, model_proto)\n    model = self.create_model(model_proto)\n    self.assertIsInstance(model, ssd_meta_arch.SSDMetaArch)\n    self.assertIsInstance(model._feature_extractor,\n                          EmbeddedSSDMobileNetV1FeatureExtractor)\n\n  def test_create_faster_rcnn_resnet_v1_models_from_config(self):\n    model_text_proto = """"""\n      faster_rcnn {\n        num_classes: 3\n        image_resizer {\n          keep_aspect_ratio_resizer {\n            min_dimension: 600\n            max_dimension: 1024\n          }\n        }\n        feature_extractor {\n          type: \'faster_rcnn_resnet101\'\n        }\n        first_stage_anchor_generator {\n          grid_anchor_generator {\n            scales: [0.25, 0.5, 1.0, 2.0]\n            aspect_ratios: [0.5, 1.0, 2.0]\n            height_stride: 16\n            width_stride: 16\n          }\n        }\n        first_stage_box_predictor_conv_hyperparams {\n          regularizer {\n            l2_regularizer {\n            }\n          }\n          initializer {\n            truncated_normal_initializer {\n            }\n          }\n        }\n        initial_crop_size: 14\n        maxpool_kernel_size: 2\n        maxpool_stride: 2\n        second_stage_box_predictor {\n          mask_rcnn_box_predictor {\n            fc_hyperparams {\n              op: FC\n              regularizer {\n                l2_regularizer {\n                }\n              }\n              initializer {\n                truncated_normal_initializer {\n                }\n              }\n            }\n          }\n        }\n        second_stage_post_processing {\n          batch_non_max_suppression {\n            score_threshold: 0.01\n            iou_threshold: 0.6\n            max_detections_per_class: 100\n            max_total_detections: 300\n          }\n          score_converter: SOFTMAX\n        }\n      }""""""\n    model_proto = model_pb2.DetectionModel()\n    text_format.Merge(model_text_proto, model_proto)\n    for extractor_type, extractor_class in FRCNN_RESNET_FEAT_MAPS.items():\n      model_proto.faster_rcnn.feature_extractor.type = extractor_type\n      model = model_builder.build(model_proto, is_training=True)\n      self.assertIsInstance(model, faster_rcnn_meta_arch.FasterRCNNMetaArch)\n      self.assertIsInstance(model._feature_extractor, extractor_class)\n\n  def test_create_faster_rcnn_resnet101_with_mask_prediction_enabled(self):\n    model_text_proto = """"""\n      faster_rcnn {\n        num_classes: 3\n        image_resizer {\n          keep_aspect_ratio_resizer {\n            min_dimension: 600\n            max_dimension: 1024\n          }\n        }\n        feature_extractor {\n          type: \'faster_rcnn_resnet101\'\n        }\n        first_stage_anchor_generator {\n          grid_anchor_generator {\n            scales: [0.25, 0.5, 1.0, 2.0]\n            aspect_ratios: [0.5, 1.0, 2.0]\n            height_stride: 16\n            width_stride: 16\n          }\n        }\n        first_stage_box_predictor_conv_hyperparams {\n          regularizer {\n            l2_regularizer {\n            }\n          }\n          initializer {\n            truncated_normal_initializer {\n            }\n          }\n        }\n        initial_crop_size: 14\n        maxpool_kernel_size: 2\n        maxpool_stride: 2\n        second_stage_box_predictor {\n          mask_rcnn_box_predictor {\n            fc_hyperparams {\n              op: FC\n              regularizer {\n                l2_regularizer {\n                }\n              }\n              initializer {\n                truncated_normal_initializer {\n                }\n              }\n            }\n            conv_hyperparams {\n              regularizer {\n                l2_regularizer {\n                }\n              }\n              initializer {\n                truncated_normal_initializer {\n                }\n              }\n            }\n            predict_instance_masks: true\n          }\n        }\n        second_stage_mask_prediction_loss_weight: 3.0\n        second_stage_post_processing {\n          batch_non_max_suppression {\n            score_threshold: 0.01\n            iou_threshold: 0.6\n            max_detections_per_class: 100\n            max_total_detections: 300\n          }\n          score_converter: SOFTMAX\n        }\n      }""""""\n    model_proto = model_pb2.DetectionModel()\n    text_format.Merge(model_text_proto, model_proto)\n    model = model_builder.build(model_proto, is_training=True)\n    self.assertAlmostEqual(model._second_stage_mask_loss_weight, 3.0)\n\n  def test_create_faster_rcnn_nas_model_from_config(self):\n    model_text_proto = """"""\n      faster_rcnn {\n        num_classes: 3\n        image_resizer {\n          keep_aspect_ratio_resizer {\n            min_dimension: 600\n            max_dimension: 1024\n          }\n        }\n        feature_extractor {\n          type: \'faster_rcnn_nas\'\n        }\n        first_stage_anchor_generator {\n          grid_anchor_generator {\n            scales: [0.25, 0.5, 1.0, 2.0]\n            aspect_ratios: [0.5, 1.0, 2.0]\n            height_stride: 16\n            width_stride: 16\n          }\n        }\n        first_stage_box_predictor_conv_hyperparams {\n          regularizer {\n            l2_regularizer {\n            }\n          }\n          initializer {\n            truncated_normal_initializer {\n            }\n          }\n        }\n        initial_crop_size: 17\n        maxpool_kernel_size: 1\n        maxpool_stride: 1\n        second_stage_box_predictor {\n          mask_rcnn_box_predictor {\n            fc_hyperparams {\n              op: FC\n              regularizer {\n                l2_regularizer {\n                }\n              }\n              initializer {\n                truncated_normal_initializer {\n                }\n              }\n            }\n          }\n        }\n        second_stage_post_processing {\n          batch_non_max_suppression {\n            score_threshold: 0.01\n            iou_threshold: 0.6\n            max_detections_per_class: 100\n            max_total_detections: 300\n          }\n          score_converter: SOFTMAX\n        }\n      }""""""\n    model_proto = model_pb2.DetectionModel()\n    text_format.Merge(model_text_proto, model_proto)\n    model = model_builder.build(model_proto, is_training=True)\n    self.assertIsInstance(model, faster_rcnn_meta_arch.FasterRCNNMetaArch)\n    self.assertIsInstance(\n        model._feature_extractor,\n        frcnn_nas.FasterRCNNNASFeatureExtractor)\n\n  def test_create_faster_rcnn_inception_resnet_v2_model_from_config(self):\n    model_text_proto = """"""\n      faster_rcnn {\n        num_classes: 3\n        image_resizer {\n          keep_aspect_ratio_resizer {\n            min_dimension: 600\n            max_dimension: 1024\n          }\n        }\n        feature_extractor {\n          type: \'faster_rcnn_inception_resnet_v2\'\n        }\n        first_stage_anchor_generator {\n          grid_anchor_generator {\n            scales: [0.25, 0.5, 1.0, 2.0]\n            aspect_ratios: [0.5, 1.0, 2.0]\n            height_stride: 16\n            width_stride: 16\n          }\n        }\n        first_stage_box_predictor_conv_hyperparams {\n          regularizer {\n            l2_regularizer {\n            }\n          }\n          initializer {\n            truncated_normal_initializer {\n            }\n          }\n        }\n        initial_crop_size: 17\n        maxpool_kernel_size: 1\n        maxpool_stride: 1\n        second_stage_box_predictor {\n          mask_rcnn_box_predictor {\n            fc_hyperparams {\n              op: FC\n              regularizer {\n                l2_regularizer {\n                }\n              }\n              initializer {\n                truncated_normal_initializer {\n                }\n              }\n            }\n          }\n        }\n        second_stage_post_processing {\n          batch_non_max_suppression {\n            score_threshold: 0.01\n            iou_threshold: 0.6\n            max_detections_per_class: 100\n            max_total_detections: 300\n          }\n          score_converter: SOFTMAX\n        }\n      }""""""\n    model_proto = model_pb2.DetectionModel()\n    text_format.Merge(model_text_proto, model_proto)\n    model = model_builder.build(model_proto, is_training=True)\n    self.assertIsInstance(model, faster_rcnn_meta_arch.FasterRCNNMetaArch)\n    self.assertIsInstance(\n        model._feature_extractor,\n        frcnn_inc_res.FasterRCNNInceptionResnetV2FeatureExtractor)\n\n  def test_create_faster_rcnn_inception_v2_model_from_config(self):\n    model_text_proto = """"""\n      faster_rcnn {\n        num_classes: 3\n        image_resizer {\n          keep_aspect_ratio_resizer {\n            min_dimension: 600\n            max_dimension: 1024\n          }\n        }\n        feature_extractor {\n          type: \'faster_rcnn_inception_v2\'\n        }\n        first_stage_anchor_generator {\n          grid_anchor_generator {\n            scales: [0.25, 0.5, 1.0, 2.0]\n            aspect_ratios: [0.5, 1.0, 2.0]\n            height_stride: 16\n            width_stride: 16\n          }\n        }\n        first_stage_box_predictor_conv_hyperparams {\n          regularizer {\n            l2_regularizer {\n            }\n          }\n          initializer {\n            truncated_normal_initializer {\n            }\n          }\n        }\n        initial_crop_size: 14\n        maxpool_kernel_size: 2\n        maxpool_stride: 2\n        second_stage_box_predictor {\n          mask_rcnn_box_predictor {\n            fc_hyperparams {\n              op: FC\n              regularizer {\n                l2_regularizer {\n                }\n              }\n              initializer {\n                truncated_normal_initializer {\n                }\n              }\n            }\n          }\n        }\n        second_stage_post_processing {\n          batch_non_max_suppression {\n            score_threshold: 0.01\n            iou_threshold: 0.6\n            max_detections_per_class: 100\n            max_total_detections: 300\n          }\n          score_converter: SOFTMAX\n        }\n      }""""""\n    model_proto = model_pb2.DetectionModel()\n    text_format.Merge(model_text_proto, model_proto)\n    model = model_builder.build(model_proto, is_training=True)\n    self.assertIsInstance(model, faster_rcnn_meta_arch.FasterRCNNMetaArch)\n    self.assertIsInstance(model._feature_extractor,\n                          frcnn_inc_v2.FasterRCNNInceptionV2FeatureExtractor)\n\n  def test_create_faster_rcnn_model_from_config_with_example_miner(self):\n    model_text_proto = """"""\n      faster_rcnn {\n        num_classes: 3\n        feature_extractor {\n          type: \'faster_rcnn_inception_resnet_v2\'\n        }\n        image_resizer {\n          keep_aspect_ratio_resizer {\n            min_dimension: 600\n            max_dimension: 1024\n          }\n        }\n        first_stage_anchor_generator {\n          grid_anchor_generator {\n            scales: [0.25, 0.5, 1.0, 2.0]\n            aspect_ratios: [0.5, 1.0, 2.0]\n            height_stride: 16\n            width_stride: 16\n          }\n        }\n        first_stage_box_predictor_conv_hyperparams {\n          regularizer {\n            l2_regularizer {\n            }\n          }\n          initializer {\n            truncated_normal_initializer {\n            }\n          }\n        }\n        second_stage_box_predictor {\n          mask_rcnn_box_predictor {\n            fc_hyperparams {\n              op: FC\n              regularizer {\n                l2_regularizer {\n                }\n              }\n              initializer {\n                truncated_normal_initializer {\n                }\n              }\n            }\n          }\n        }\n        hard_example_miner {\n          num_hard_examples: 10\n          iou_threshold: 0.99\n        }\n      }""""""\n    model_proto = model_pb2.DetectionModel()\n    text_format.Merge(model_text_proto, model_proto)\n    model = model_builder.build(model_proto, is_training=True)\n    self.assertIsNotNone(model._hard_example_miner)\n\n  def test_create_rfcn_resnet_v1_model_from_config(self):\n    model_text_proto = """"""\n      faster_rcnn {\n        num_classes: 3\n        image_resizer {\n          keep_aspect_ratio_resizer {\n            min_dimension: 600\n            max_dimension: 1024\n          }\n        }\n        feature_extractor {\n          type: \'faster_rcnn_resnet101\'\n        }\n        first_stage_anchor_generator {\n          grid_anchor_generator {\n            scales: [0.25, 0.5, 1.0, 2.0]\n            aspect_ratios: [0.5, 1.0, 2.0]\n            height_stride: 16\n            width_stride: 16\n          }\n        }\n        first_stage_box_predictor_conv_hyperparams {\n          regularizer {\n            l2_regularizer {\n            }\n          }\n          initializer {\n            truncated_normal_initializer {\n            }\n          }\n        }\n        initial_crop_size: 14\n        maxpool_kernel_size: 2\n        maxpool_stride: 2\n        second_stage_box_predictor {\n          rfcn_box_predictor {\n            conv_hyperparams {\n              op: CONV\n              regularizer {\n                l2_regularizer {\n                }\n              }\n              initializer {\n                truncated_normal_initializer {\n                }\n              }\n            }\n          }\n        }\n        second_stage_post_processing {\n          batch_non_max_suppression {\n            score_threshold: 0.01\n            iou_threshold: 0.6\n            max_detections_per_class: 100\n            max_total_detections: 300\n          }\n          score_converter: SOFTMAX\n        }\n      }""""""\n    model_proto = model_pb2.DetectionModel()\n    text_format.Merge(model_text_proto, model_proto)\n    for extractor_type, extractor_class in FRCNN_RESNET_FEAT_MAPS.items():\n      model_proto.faster_rcnn.feature_extractor.type = extractor_type\n      model = model_builder.build(model_proto, is_training=True)\n      self.assertIsInstance(model, rfcn_meta_arch.RFCNMetaArch)\n      self.assertIsInstance(model._feature_extractor, extractor_class)\n\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
src/object_detection/builders/optimizer_builder.py,9,"b'# Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n""""""Functions to build DetectionModel training optimizers.""""""\n\nimport tensorflow as tf\nfrom object_detection.utils import learning_schedules\n\n\ndef build(optimizer_config):\n  """"""Create optimizer based on config.\n\n  Args:\n    optimizer_config: A Optimizer proto message.\n\n  Returns:\n    An optimizer and a list of variables for summary.\n\n  Raises:\n    ValueError: when using an unsupported input data type.\n  """"""\n  optimizer_type = optimizer_config.WhichOneof(\'optimizer\')\n  optimizer = None\n\n  summary_vars = []\n  if optimizer_type == \'rms_prop_optimizer\':\n    config = optimizer_config.rms_prop_optimizer\n    learning_rate = _create_learning_rate(config.learning_rate)\n    summary_vars.append(learning_rate)\n    optimizer = tf.train.RMSPropOptimizer(\n        learning_rate,\n        decay=config.decay,\n        momentum=config.momentum_optimizer_value,\n        epsilon=config.epsilon)\n\n  if optimizer_type == \'momentum_optimizer\':\n    config = optimizer_config.momentum_optimizer\n    learning_rate = _create_learning_rate(config.learning_rate)\n    summary_vars.append(learning_rate)\n    optimizer = tf.train.MomentumOptimizer(\n        learning_rate,\n        momentum=config.momentum_optimizer_value)\n\n  if optimizer_type == \'adam_optimizer\':\n    config = optimizer_config.adam_optimizer\n    learning_rate = _create_learning_rate(config.learning_rate)\n    summary_vars.append(learning_rate)\n    optimizer = tf.train.AdamOptimizer(learning_rate)\n\n  if optimizer is None:\n    raise ValueError(\'Optimizer %s not supported.\' % optimizer_type)\n\n  if optimizer_config.use_moving_average:\n    optimizer = tf.contrib.opt.MovingAverageOptimizer(\n        optimizer, average_decay=optimizer_config.moving_average_decay)\n\n  return optimizer, summary_vars\n\n\ndef _create_learning_rate(learning_rate_config):\n  """"""Create optimizer learning rate based on config.\n\n  Args:\n    learning_rate_config: A LearningRate proto message.\n\n  Returns:\n    A learning rate.\n\n  Raises:\n    ValueError: when using an unsupported input data type.\n  """"""\n  learning_rate = None\n  learning_rate_type = learning_rate_config.WhichOneof(\'learning_rate\')\n  if learning_rate_type == \'constant_learning_rate\':\n    config = learning_rate_config.constant_learning_rate\n    learning_rate = tf.constant(config.learning_rate, dtype=tf.float32,\n                                name=\'learning_rate\')\n\n  if learning_rate_type == \'exponential_decay_learning_rate\':\n    config = learning_rate_config.exponential_decay_learning_rate\n    learning_rate = tf.train.exponential_decay(\n        config.initial_learning_rate,\n        tf.train.get_or_create_global_step(),\n        config.decay_steps,\n        config.decay_factor,\n        staircase=config.staircase, name=\'learning_rate\')\n\n  if learning_rate_type == \'manual_step_learning_rate\':\n    config = learning_rate_config.manual_step_learning_rate\n    if not config.schedule:\n      raise ValueError(\'Empty learning rate schedule.\')\n    learning_rate_step_boundaries = [x.step for x in config.schedule]\n    learning_rate_sequence = [config.initial_learning_rate]\n    learning_rate_sequence += [x.learning_rate for x in config.schedule]\n    learning_rate = learning_schedules.manual_stepping(\n        tf.train.get_or_create_global_step(), learning_rate_step_boundaries,\n        learning_rate_sequence, config.warmup)\n\n  if learning_rate_type == \'cosine_decay_learning_rate\':\n    config = learning_rate_config.cosine_decay_learning_rate\n    learning_rate = learning_schedules.cosine_decay_with_warmup(\n        tf.train.get_or_create_global_step(),\n        config.learning_rate_base,\n        config.total_steps,\n        config.warmup_learning_rate,\n        config.warmup_steps,\n        config.hold_base_rate_steps)\n\n  if learning_rate is None:\n    raise ValueError(\'Learning_rate %s not supported.\' % learning_rate_type)\n\n  return learning_rate\n'"
src/object_detection/builders/optimizer_builder_test.py,11,"b'# Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n""""""Tests for optimizer_builder.""""""\n\nimport tensorflow as tf\n\nfrom google.protobuf import text_format\n\nfrom object_detection.builders import optimizer_builder\nfrom object_detection.protos import optimizer_pb2\n\n\nclass LearningRateBuilderTest(tf.test.TestCase):\n\n  def testBuildConstantLearningRate(self):\n    learning_rate_text_proto = """"""\n      constant_learning_rate {\n        learning_rate: 0.004\n      }\n    """"""\n    learning_rate_proto = optimizer_pb2.LearningRate()\n    text_format.Merge(learning_rate_text_proto, learning_rate_proto)\n    learning_rate = optimizer_builder._create_learning_rate(\n        learning_rate_proto)\n    self.assertTrue(learning_rate.op.name.endswith(\'learning_rate\'))\n    with self.test_session():\n      learning_rate_out = learning_rate.eval()\n    self.assertAlmostEqual(learning_rate_out, 0.004)\n\n  def testBuildExponentialDecayLearningRate(self):\n    learning_rate_text_proto = """"""\n      exponential_decay_learning_rate {\n        initial_learning_rate: 0.004\n        decay_steps: 99999\n        decay_factor: 0.85\n        staircase: false\n      }\n    """"""\n    learning_rate_proto = optimizer_pb2.LearningRate()\n    text_format.Merge(learning_rate_text_proto, learning_rate_proto)\n    learning_rate = optimizer_builder._create_learning_rate(\n        learning_rate_proto)\n    self.assertTrue(learning_rate.op.name.endswith(\'learning_rate\'))\n    self.assertTrue(isinstance(learning_rate, tf.Tensor))\n\n  def testBuildManualStepLearningRate(self):\n    learning_rate_text_proto = """"""\n      manual_step_learning_rate {\n        initial_learning_rate: 0.002\n        schedule {\n          step: 100\n          learning_rate: 0.006\n        }\n        schedule {\n          step: 90000\n          learning_rate: 0.00006\n        }\n        warmup: true\n      }\n    """"""\n    learning_rate_proto = optimizer_pb2.LearningRate()\n    text_format.Merge(learning_rate_text_proto, learning_rate_proto)\n    learning_rate = optimizer_builder._create_learning_rate(\n        learning_rate_proto)\n    self.assertTrue(isinstance(learning_rate, tf.Tensor))\n\n  def testBuildCosineDecayLearningRate(self):\n    learning_rate_text_proto = """"""\n      cosine_decay_learning_rate {\n        learning_rate_base: 0.002\n        total_steps: 20000\n        warmup_learning_rate: 0.0001\n        warmup_steps: 1000\n        hold_base_rate_steps: 20000\n      }\n    """"""\n    learning_rate_proto = optimizer_pb2.LearningRate()\n    text_format.Merge(learning_rate_text_proto, learning_rate_proto)\n    learning_rate = optimizer_builder._create_learning_rate(\n        learning_rate_proto)\n    self.assertTrue(isinstance(learning_rate, tf.Tensor))\n\n  def testRaiseErrorOnEmptyLearningRate(self):\n    learning_rate_text_proto = """"""\n    """"""\n    learning_rate_proto = optimizer_pb2.LearningRate()\n    text_format.Merge(learning_rate_text_proto, learning_rate_proto)\n    with self.assertRaises(ValueError):\n      optimizer_builder._create_learning_rate(learning_rate_proto)\n\n\nclass OptimizerBuilderTest(tf.test.TestCase):\n\n  def testBuildRMSPropOptimizer(self):\n    optimizer_text_proto = """"""\n      rms_prop_optimizer: {\n        learning_rate: {\n          exponential_decay_learning_rate {\n            initial_learning_rate: 0.004\n            decay_steps: 800720\n            decay_factor: 0.95\n          }\n        }\n        momentum_optimizer_value: 0.9\n        decay: 0.9\n        epsilon: 1.0\n      }\n      use_moving_average: false\n    """"""\n    optimizer_proto = optimizer_pb2.Optimizer()\n    text_format.Merge(optimizer_text_proto, optimizer_proto)\n    optimizer, _ = optimizer_builder.build(optimizer_proto)\n    self.assertTrue(isinstance(optimizer, tf.train.RMSPropOptimizer))\n\n  def testBuildMomentumOptimizer(self):\n    optimizer_text_proto = """"""\n      momentum_optimizer: {\n        learning_rate: {\n          constant_learning_rate {\n            learning_rate: 0.001\n          }\n        }\n        momentum_optimizer_value: 0.99\n      }\n      use_moving_average: false\n    """"""\n    optimizer_proto = optimizer_pb2.Optimizer()\n    text_format.Merge(optimizer_text_proto, optimizer_proto)\n    optimizer, _ = optimizer_builder.build(optimizer_proto)\n    self.assertTrue(isinstance(optimizer, tf.train.MomentumOptimizer))\n\n  def testBuildAdamOptimizer(self):\n    optimizer_text_proto = """"""\n      adam_optimizer: {\n        learning_rate: {\n          constant_learning_rate {\n            learning_rate: 0.002\n          }\n        }\n      }\n      use_moving_average: false\n    """"""\n    optimizer_proto = optimizer_pb2.Optimizer()\n    text_format.Merge(optimizer_text_proto, optimizer_proto)\n    optimizer, _ = optimizer_builder.build(optimizer_proto)\n    self.assertTrue(isinstance(optimizer, tf.train.AdamOptimizer))\n\n  def testBuildMovingAverageOptimizer(self):\n    optimizer_text_proto = """"""\n      adam_optimizer: {\n        learning_rate: {\n          constant_learning_rate {\n            learning_rate: 0.002\n          }\n        }\n      }\n      use_moving_average: True\n    """"""\n    optimizer_proto = optimizer_pb2.Optimizer()\n    text_format.Merge(optimizer_text_proto, optimizer_proto)\n    optimizer, _ = optimizer_builder.build(optimizer_proto)\n    self.assertTrue(\n        isinstance(optimizer, tf.contrib.opt.MovingAverageOptimizer))\n\n  def testBuildMovingAverageOptimizerWithNonDefaultDecay(self):\n    optimizer_text_proto = """"""\n      adam_optimizer: {\n        learning_rate: {\n          constant_learning_rate {\n            learning_rate: 0.002\n          }\n        }\n      }\n      use_moving_average: True\n      moving_average_decay: 0.2\n    """"""\n    optimizer_proto = optimizer_pb2.Optimizer()\n    text_format.Merge(optimizer_text_proto, optimizer_proto)\n    optimizer, _ = optimizer_builder.build(optimizer_proto)\n    self.assertTrue(\n        isinstance(optimizer, tf.contrib.opt.MovingAverageOptimizer))\n    # TODO(rathodv): Find a way to not depend on the private members.\n    self.assertAlmostEqual(optimizer._ema._decay, 0.2)\n\n  def testBuildEmptyOptimizer(self):\n    optimizer_text_proto = """"""\n    """"""\n    optimizer_proto = optimizer_pb2.Optimizer()\n    text_format.Merge(optimizer_text_proto, optimizer_proto)\n    with self.assertRaises(ValueError):\n      optimizer_builder.build(optimizer_proto)\n\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
src/object_detection/builders/post_processing_builder.py,6,"b'# Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n""""""Builder function for post processing operations.""""""\nimport functools\n\nimport tensorflow as tf\nfrom object_detection.core import post_processing\nfrom object_detection.protos import post_processing_pb2\n\n\ndef build(post_processing_config):\n  """"""Builds callables for post-processing operations.\n\n  Builds callables for non-max suppression and score conversion based on the\n  configuration.\n\n  Non-max suppression callable takes `boxes`, `scores`, and optionally\n  `clip_window`, `parallel_iterations` `masks, and `scope` as inputs. It returns\n  `nms_boxes`, `nms_scores`, `nms_classes` `nms_masks` and `num_detections`. See\n  post_processing.batch_multiclass_non_max_suppression for the type and shape\n  of these tensors.\n\n  Score converter callable should be called with `input` tensor. The callable\n  returns the output from one of 3 tf operations based on the configuration -\n  tf.identity, tf.sigmoid or tf.nn.softmax. See tensorflow documentation for\n  argument and return value descriptions.\n\n  Args:\n    post_processing_config: post_processing.proto object containing the\n      parameters for the post-processing operations.\n\n  Returns:\n    non_max_suppressor_fn: Callable for non-max suppression.\n    score_converter_fn: Callable for score conversion.\n\n  Raises:\n    ValueError: if the post_processing_config is of incorrect type.\n  """"""\n  if not isinstance(post_processing_config, post_processing_pb2.PostProcessing):\n    raise ValueError(\'post_processing_config not of type \'\n                     \'post_processing_pb2.Postprocessing.\')\n  non_max_suppressor_fn = _build_non_max_suppressor(\n      post_processing_config.batch_non_max_suppression)\n  score_converter_fn = _build_score_converter(\n      post_processing_config.score_converter,\n      post_processing_config.logit_scale)\n  return non_max_suppressor_fn, score_converter_fn\n\n\ndef _build_non_max_suppressor(nms_config):\n  """"""Builds non-max suppresson based on the nms config.\n\n  Args:\n    nms_config: post_processing_pb2.PostProcessing.BatchNonMaxSuppression proto.\n\n  Returns:\n    non_max_suppressor_fn: Callable non-max suppressor.\n\n  Raises:\n    ValueError: On incorrect iou_threshold or on incompatible values of\n      max_total_detections and max_detections_per_class.\n  """"""\n  if nms_config.iou_threshold < 0 or nms_config.iou_threshold > 1.0:\n    raise ValueError(\'iou_threshold not in [0, 1.0].\')\n  if nms_config.max_detections_per_class > nms_config.max_total_detections:\n    raise ValueError(\'max_detections_per_class should be no greater than \'\n                     \'max_total_detections.\')\n\n  non_max_suppressor_fn = functools.partial(\n      post_processing.batch_multiclass_non_max_suppression,\n      score_thresh=nms_config.score_threshold,\n      iou_thresh=nms_config.iou_threshold,\n      max_size_per_class=nms_config.max_detections_per_class,\n      max_total_size=nms_config.max_total_detections)\n  return non_max_suppressor_fn\n\n\ndef _score_converter_fn_with_logit_scale(tf_score_converter_fn, logit_scale):\n  """"""Create a function to scale logits then apply a Tensorflow function.""""""\n  def score_converter_fn(logits):\n    scaled_logits = tf.divide(logits, logit_scale, name=\'scale_logits\')\n    return tf_score_converter_fn(scaled_logits, name=\'convert_scores\')\n  score_converter_fn.__name__ = \'%s_with_logit_scale\' % (\n      tf_score_converter_fn.__name__)\n  return score_converter_fn\n\n\ndef _build_score_converter(score_converter_config, logit_scale):\n  """"""Builds score converter based on the config.\n\n  Builds one of [tf.identity, tf.sigmoid, tf.softmax] score converters based on\n  the config.\n\n  Args:\n    score_converter_config: post_processing_pb2.PostProcessing.score_converter.\n    logit_scale: temperature to use for SOFTMAX score_converter.\n\n  Returns:\n    Callable score converter op.\n\n  Raises:\n    ValueError: On unknown score converter.\n  """"""\n  if score_converter_config == post_processing_pb2.PostProcessing.IDENTITY:\n    return _score_converter_fn_with_logit_scale(tf.identity, logit_scale)\n  if score_converter_config == post_processing_pb2.PostProcessing.SIGMOID:\n    return _score_converter_fn_with_logit_scale(tf.sigmoid, logit_scale)\n  if score_converter_config == post_processing_pb2.PostProcessing.SOFTMAX:\n    return _score_converter_fn_with_logit_scale(tf.nn.softmax, logit_scale)\n  raise ValueError(\'Unknown score converter.\')\n'"
src/object_detection/builders/post_processing_builder_test.py,5,"b'# Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n""""""Tests for post_processing_builder.""""""\n\nimport tensorflow as tf\nfrom google.protobuf import text_format\nfrom object_detection.builders import post_processing_builder\nfrom object_detection.protos import post_processing_pb2\n\n\nclass PostProcessingBuilderTest(tf.test.TestCase):\n\n  def test_build_non_max_suppressor_with_correct_parameters(self):\n    post_processing_text_proto = """"""\n      batch_non_max_suppression {\n        score_threshold: 0.7\n        iou_threshold: 0.6\n        max_detections_per_class: 100\n        max_total_detections: 300\n      }\n    """"""\n    post_processing_config = post_processing_pb2.PostProcessing()\n    text_format.Merge(post_processing_text_proto, post_processing_config)\n    non_max_suppressor, _ = post_processing_builder.build(\n        post_processing_config)\n    self.assertEqual(non_max_suppressor.keywords[\'max_size_per_class\'], 100)\n    self.assertEqual(non_max_suppressor.keywords[\'max_total_size\'], 300)\n    self.assertAlmostEqual(non_max_suppressor.keywords[\'score_thresh\'], 0.7)\n    self.assertAlmostEqual(non_max_suppressor.keywords[\'iou_thresh\'], 0.6)\n\n  def test_build_identity_score_converter(self):\n    post_processing_text_proto = """"""\n      score_converter: IDENTITY\n    """"""\n    post_processing_config = post_processing_pb2.PostProcessing()\n    text_format.Merge(post_processing_text_proto, post_processing_config)\n    _, score_converter = post_processing_builder.build(post_processing_config)\n    self.assertEqual(score_converter.__name__, \'identity_with_logit_scale\')\n\n    inputs = tf.constant([1, 1], tf.float32)\n    outputs = score_converter(inputs)\n    with self.test_session() as sess:\n      converted_scores = sess.run(outputs)\n      expected_converted_scores = sess.run(inputs)\n      self.assertAllClose(converted_scores, expected_converted_scores)\n\n  def test_build_identity_score_converter_with_logit_scale(self):\n    post_processing_text_proto = """"""\n      score_converter: IDENTITY\n      logit_scale: 2.0\n    """"""\n    post_processing_config = post_processing_pb2.PostProcessing()\n    text_format.Merge(post_processing_text_proto, post_processing_config)\n    _, score_converter = post_processing_builder.build(post_processing_config)\n    self.assertEqual(score_converter.__name__, \'identity_with_logit_scale\')\n\n    inputs = tf.constant([1, 1], tf.float32)\n    outputs = score_converter(inputs)\n    with self.test_session() as sess:\n      converted_scores = sess.run(outputs)\n      expected_converted_scores = sess.run(tf.constant([.5, .5], tf.float32))\n      self.assertAllClose(converted_scores, expected_converted_scores)\n\n  def test_build_sigmoid_score_converter(self):\n    post_processing_text_proto = """"""\n      score_converter: SIGMOID\n    """"""\n    post_processing_config = post_processing_pb2.PostProcessing()\n    text_format.Merge(post_processing_text_proto, post_processing_config)\n    _, score_converter = post_processing_builder.build(post_processing_config)\n    self.assertEqual(score_converter.__name__, \'sigmoid_with_logit_scale\')\n\n  def test_build_softmax_score_converter(self):\n    post_processing_text_proto = """"""\n      score_converter: SOFTMAX\n    """"""\n    post_processing_config = post_processing_pb2.PostProcessing()\n    text_format.Merge(post_processing_text_proto, post_processing_config)\n    _, score_converter = post_processing_builder.build(post_processing_config)\n    self.assertEqual(score_converter.__name__, \'softmax_with_logit_scale\')\n\n  def test_build_softmax_score_converter_with_temperature(self):\n    post_processing_text_proto = """"""\n      score_converter: SOFTMAX\n      logit_scale: 2.0\n    """"""\n    post_processing_config = post_processing_pb2.PostProcessing()\n    text_format.Merge(post_processing_text_proto, post_processing_config)\n    _, score_converter = post_processing_builder.build(post_processing_config)\n    self.assertEqual(score_converter.__name__, \'softmax_with_logit_scale\')\n\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
src/object_detection/builders/preprocessor_builder.py,5,"b'# Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n""""""Builder for preprocessing steps.""""""\n\nimport tensorflow as tf\n\nfrom object_detection.core import preprocessor\nfrom object_detection.protos import preprocessor_pb2\n\n\ndef _get_step_config_from_proto(preprocessor_step_config, step_name):\n  """"""Returns the value of a field named step_name from proto.\n\n  Args:\n    preprocessor_step_config: A preprocessor_pb2.PreprocessingStep object.\n    step_name: Name of the field to get value from.\n\n  Returns:\n    result_dict: a sub proto message from preprocessor_step_config which will be\n                 later converted to a dictionary.\n\n  Raises:\n    ValueError: If field does not exist in proto.\n  """"""\n  for field, value in preprocessor_step_config.ListFields():\n    if field.name == step_name:\n      return value\n\n  raise ValueError(\'Could not get field %s from proto!\', step_name)\n\n\ndef _get_dict_from_proto(config):\n  """"""Helper function to put all proto fields into a dictionary.\n\n  For many preprocessing steps, there\'s an trivial 1-1 mapping from proto fields\n  to function arguments. This function automatically populates a dictionary with\n  the arguments from the proto.\n\n  Protos that CANNOT be trivially populated include:\n  * nested messages.\n  * steps that check if an optional field is set (ie. where None != 0).\n  * protos that don\'t map 1-1 to arguments (ie. list should be reshaped).\n  * fields requiring additional validation (ie. repeated field has n elements).\n\n  Args:\n    config: A protobuf object that does not violate the conditions above.\n\n  Returns:\n    result_dict: |config| converted into a python dictionary.\n  """"""\n  result_dict = {}\n  for field, value in config.ListFields():\n    result_dict[field.name] = value\n  return result_dict\n\n\n# A map from a PreprocessingStep proto config field name to the preprocessing\n# function that should be used. The PreprocessingStep proto should be parsable\n# with _get_dict_from_proto.\nPREPROCESSING_FUNCTION_MAP = {\n    \'normalize_image\': preprocessor.normalize_image,\n    \'random_pixel_value_scale\': preprocessor.random_pixel_value_scale,\n    \'random_image_scale\': preprocessor.random_image_scale,\n    \'random_rgb_to_gray\': preprocessor.random_rgb_to_gray,\n    \'random_adjust_brightness\': preprocessor.random_adjust_brightness,\n    \'random_adjust_contrast\': preprocessor.random_adjust_contrast,\n    \'random_adjust_hue\': preprocessor.random_adjust_hue,\n    \'random_adjust_saturation\': preprocessor.random_adjust_saturation,\n    \'random_distort_color\': preprocessor.random_distort_color,\n    \'random_jitter_boxes\': preprocessor.random_jitter_boxes,\n    \'random_crop_to_aspect_ratio\': preprocessor.random_crop_to_aspect_ratio,\n    \'random_black_patches\': preprocessor.random_black_patches,\n    \'rgb_to_gray\': preprocessor.rgb_to_gray,\n    \'scale_boxes_to_pixel_coordinates\': (\n        preprocessor.scale_boxes_to_pixel_coordinates),\n    \'subtract_channel_mean\': preprocessor.subtract_channel_mean,\n}\n\n\n# A map to convert from preprocessor_pb2.ResizeImage.Method enum to\n# tf.image.ResizeMethod.\nRESIZE_METHOD_MAP = {\n    preprocessor_pb2.ResizeImage.AREA: tf.image.ResizeMethod.AREA,\n    preprocessor_pb2.ResizeImage.BICUBIC: tf.image.ResizeMethod.BICUBIC,\n    preprocessor_pb2.ResizeImage.BILINEAR: tf.image.ResizeMethod.BILINEAR,\n    preprocessor_pb2.ResizeImage.NEAREST_NEIGHBOR: (\n        tf.image.ResizeMethod.NEAREST_NEIGHBOR),\n}\n\n\ndef build(preprocessor_step_config):\n  """"""Builds preprocessing step based on the configuration.\n\n  Args:\n    preprocessor_step_config: PreprocessingStep configuration proto.\n\n  Returns:\n    function, argmap: A callable function and an argument map to call function\n                      with.\n\n  Raises:\n    ValueError: On invalid configuration.\n  """"""\n  step_type = preprocessor_step_config.WhichOneof(\'preprocessing_step\')\n\n  if step_type in PREPROCESSING_FUNCTION_MAP:\n    preprocessing_function = PREPROCESSING_FUNCTION_MAP[step_type]\n    step_config = _get_step_config_from_proto(preprocessor_step_config,\n                                              step_type)\n    function_args = _get_dict_from_proto(step_config)\n    return (preprocessing_function, function_args)\n\n  if step_type == \'random_horizontal_flip\':\n    config = preprocessor_step_config.random_horizontal_flip\n    return (preprocessor.random_horizontal_flip,\n            {\n                \'keypoint_flip_permutation\': tuple(\n                    config.keypoint_flip_permutation),\n            })\n\n  if step_type == \'random_vertical_flip\':\n    config = preprocessor_step_config.random_vertical_flip\n    return (preprocessor.random_vertical_flip,\n            {\n                \'keypoint_flip_permutation\': tuple(\n                    config.keypoint_flip_permutation),\n            })\n\n  if step_type == \'random_rotation90\':\n    return (preprocessor.random_rotation90, {})\n\n  if step_type == \'random_crop_image\':\n    config = preprocessor_step_config.random_crop_image\n    return (preprocessor.random_crop_image,\n            {\n                \'min_object_covered\': config.min_object_covered,\n                \'aspect_ratio_range\': (config.min_aspect_ratio,\n                                       config.max_aspect_ratio),\n                \'area_range\': (config.min_area, config.max_area),\n                \'overlap_thresh\': config.overlap_thresh,\n                \'random_coef\': config.random_coef,\n            })\n\n  if step_type == \'random_pad_image\':\n    config = preprocessor_step_config.random_pad_image\n    min_image_size = None\n    if (config.HasField(\'min_image_height\') !=\n        config.HasField(\'min_image_width\')):\n      raise ValueError(\'min_image_height and min_image_width should be either \'\n                       \'both set or both unset.\')\n    if config.HasField(\'min_image_height\'):\n      min_image_size = (config.min_image_height, config.min_image_width)\n\n    max_image_size = None\n    if (config.HasField(\'max_image_height\') !=\n        config.HasField(\'max_image_width\')):\n      raise ValueError(\'max_image_height and max_image_width should be either \'\n                       \'both set or both unset.\')\n    if config.HasField(\'max_image_height\'):\n      max_image_size = (config.max_image_height, config.max_image_width)\n\n    pad_color = config.pad_color\n    if pad_color and len(pad_color) != 3:\n      raise ValueError(\'pad_color should have 3 elements (RGB) if set!\')\n    if not pad_color:\n      pad_color = None\n    return (preprocessor.random_pad_image,\n            {\n                \'min_image_size\': min_image_size,\n                \'max_image_size\': max_image_size,\n                \'pad_color\': pad_color,\n            })\n\n  if step_type == \'random_crop_pad_image\':\n    config = preprocessor_step_config.random_crop_pad_image\n    min_padded_size_ratio = config.min_padded_size_ratio\n    if min_padded_size_ratio and len(min_padded_size_ratio) != 2:\n      raise ValueError(\'min_padded_size_ratio should have 2 elements if set!\')\n    max_padded_size_ratio = config.max_padded_size_ratio\n    if max_padded_size_ratio and len(max_padded_size_ratio) != 2:\n      raise ValueError(\'max_padded_size_ratio should have 2 elements if set!\')\n    pad_color = config.pad_color\n    if pad_color and len(pad_color) != 3:\n      raise ValueError(\'pad_color should have 3 elements if set!\')\n    kwargs = {\n        \'min_object_covered\': config.min_object_covered,\n        \'aspect_ratio_range\': (config.min_aspect_ratio,\n                               config.max_aspect_ratio),\n        \'area_range\': (config.min_area, config.max_area),\n        \'overlap_thresh\': config.overlap_thresh,\n        \'random_coef\': config.random_coef,\n    }\n    if min_padded_size_ratio:\n      kwargs[\'min_padded_size_ratio\'] = tuple(min_padded_size_ratio)\n    if max_padded_size_ratio:\n      kwargs[\'max_padded_size_ratio\'] = tuple(max_padded_size_ratio)\n    if pad_color:\n      kwargs[\'pad_color\'] = tuple(pad_color)\n    return (preprocessor.random_crop_pad_image, kwargs)\n\n  if step_type == \'random_resize_method\':\n    config = preprocessor_step_config.random_resize_method\n    return (preprocessor.random_resize_method,\n            {\n                \'target_size\': [config.target_height, config.target_width],\n            })\n\n  if step_type == \'resize_image\':\n    config = preprocessor_step_config.resize_image\n    method = RESIZE_METHOD_MAP[config.method]\n    return (preprocessor.resize_image,\n            {\n                \'new_height\': config.new_height,\n                \'new_width\': config.new_width,\n                \'method\': method\n            })\n\n  if step_type == \'ssd_random_crop\':\n    config = preprocessor_step_config.ssd_random_crop\n    if config.operations:\n      min_object_covered = [op.min_object_covered for op in config.operations]\n      aspect_ratio_range = [(op.min_aspect_ratio, op.max_aspect_ratio)\n                            for op in config.operations]\n      area_range = [(op.min_area, op.max_area) for op in config.operations]\n      overlap_thresh = [op.overlap_thresh for op in config.operations]\n      random_coef = [op.random_coef for op in config.operations]\n      return (preprocessor.ssd_random_crop,\n              {\n                  \'min_object_covered\': min_object_covered,\n                  \'aspect_ratio_range\': aspect_ratio_range,\n                  \'area_range\': area_range,\n                  \'overlap_thresh\': overlap_thresh,\n                  \'random_coef\': random_coef,\n              })\n    return (preprocessor.ssd_random_crop, {})\n\n  if step_type == \'ssd_random_crop_pad\':\n    config = preprocessor_step_config.ssd_random_crop_pad\n    if config.operations:\n      min_object_covered = [op.min_object_covered for op in config.operations]\n      aspect_ratio_range = [(op.min_aspect_ratio, op.max_aspect_ratio)\n                            for op in config.operations]\n      area_range = [(op.min_area, op.max_area) for op in config.operations]\n      overlap_thresh = [op.overlap_thresh for op in config.operations]\n      random_coef = [op.random_coef for op in config.operations]\n      min_padded_size_ratio = [tuple(op.min_padded_size_ratio)\n                               for op in config.operations]\n      max_padded_size_ratio = [tuple(op.max_padded_size_ratio)\n                               for op in config.operations]\n      pad_color = [(op.pad_color_r, op.pad_color_g, op.pad_color_b)\n                   for op in config.operations]\n      return (preprocessor.ssd_random_crop_pad,\n              {\n                  \'min_object_covered\': min_object_covered,\n                  \'aspect_ratio_range\': aspect_ratio_range,\n                  \'area_range\': area_range,\n                  \'overlap_thresh\': overlap_thresh,\n                  \'random_coef\': random_coef,\n                  \'min_padded_size_ratio\': min_padded_size_ratio,\n                  \'max_padded_size_ratio\': max_padded_size_ratio,\n                  \'pad_color\': pad_color,\n              })\n    return (preprocessor.ssd_random_crop_pad, {})\n\n  if step_type == \'ssd_random_crop_fixed_aspect_ratio\':\n    config = preprocessor_step_config.ssd_random_crop_fixed_aspect_ratio\n    if config.operations:\n      min_object_covered = [op.min_object_covered for op in config.operations]\n      area_range = [(op.min_area, op.max_area) for op in config.operations]\n      overlap_thresh = [op.overlap_thresh for op in config.operations]\n      random_coef = [op.random_coef for op in config.operations]\n      return (preprocessor.ssd_random_crop_fixed_aspect_ratio,\n              {\n                  \'min_object_covered\': min_object_covered,\n                  \'aspect_ratio\': config.aspect_ratio,\n                  \'area_range\': area_range,\n                  \'overlap_thresh\': overlap_thresh,\n                  \'random_coef\': random_coef,\n              })\n    return (preprocessor.ssd_random_crop_fixed_aspect_ratio, {})\n\n  if step_type == \'ssd_random_crop_pad_fixed_aspect_ratio\':\n    config = preprocessor_step_config.ssd_random_crop_pad_fixed_aspect_ratio\n    kwargs = {}\n    aspect_ratio = config.aspect_ratio\n    if aspect_ratio:\n      kwargs[\'aspect_ratio\'] = aspect_ratio\n    min_padded_size_ratio = config.min_padded_size_ratio\n    if min_padded_size_ratio:\n      if len(min_padded_size_ratio) != 2:\n        raise ValueError(\'min_padded_size_ratio should have 2 elements if set!\')\n      kwargs[\'min_padded_size_ratio\'] = tuple(min_padded_size_ratio)\n    max_padded_size_ratio = config.max_padded_size_ratio\n    if max_padded_size_ratio:\n      if len(max_padded_size_ratio) != 2:\n        raise ValueError(\'max_padded_size_ratio should have 2 elements if set!\')\n      kwargs[\'max_padded_size_ratio\'] = tuple(max_padded_size_ratio)\n    if config.operations:\n      kwargs[\'min_object_covered\'] = [op.min_object_covered\n                                      for op in config.operations]\n      kwargs[\'aspect_ratio_range\'] = [(op.min_aspect_ratio, op.max_aspect_ratio)\n                                      for op in config.operations]\n      kwargs[\'area_range\'] = [(op.min_area, op.max_area)\n                              for op in config.operations]\n      kwargs[\'overlap_thresh\'] = [op.overlap_thresh for op in config.operations]\n      kwargs[\'random_coef\'] = [op.random_coef for op in config.operations]\n    return (preprocessor.ssd_random_crop_pad_fixed_aspect_ratio, kwargs)\n\n  raise ValueError(\'Unknown preprocessing step.\')\n'"
src/object_detection/builders/preprocessor_builder_test.py,3,"b'# Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n""""""Tests for preprocessor_builder.""""""\n\nimport tensorflow as tf\n\nfrom google.protobuf import text_format\n\nfrom object_detection.builders import preprocessor_builder\nfrom object_detection.core import preprocessor\nfrom object_detection.protos import preprocessor_pb2\n\n\nclass PreprocessorBuilderTest(tf.test.TestCase):\n\n  def assert_dictionary_close(self, dict1, dict2):\n    """"""Helper to check if two dicts with floatst or integers are close.""""""\n    self.assertEqual(sorted(dict1.keys()), sorted(dict2.keys()))\n    for key in dict1:\n      value = dict1[key]\n      if isinstance(value, float):\n        self.assertAlmostEqual(value, dict2[key])\n      else:\n        self.assertEqual(value, dict2[key])\n\n  def test_build_normalize_image(self):\n    preprocessor_text_proto = """"""\n    normalize_image {\n      original_minval: 0.0\n      original_maxval: 255.0\n      target_minval: -1.0\n      target_maxval: 1.0\n    }\n    """"""\n    preprocessor_proto = preprocessor_pb2.PreprocessingStep()\n    text_format.Merge(preprocessor_text_proto, preprocessor_proto)\n    function, args = preprocessor_builder.build(preprocessor_proto)\n    self.assertEqual(function, preprocessor.normalize_image)\n    self.assertEqual(args, {\n        \'original_minval\': 0.0,\n        \'original_maxval\': 255.0,\n        \'target_minval\': -1.0,\n        \'target_maxval\': 1.0,\n    })\n\n  def test_build_random_horizontal_flip(self):\n    preprocessor_text_proto = """"""\n    random_horizontal_flip {\n      keypoint_flip_permutation: 1\n      keypoint_flip_permutation: 0\n      keypoint_flip_permutation: 2\n      keypoint_flip_permutation: 3\n      keypoint_flip_permutation: 5\n      keypoint_flip_permutation: 4\n    }\n    """"""\n    preprocessor_proto = preprocessor_pb2.PreprocessingStep()\n    text_format.Merge(preprocessor_text_proto, preprocessor_proto)\n    function, args = preprocessor_builder.build(preprocessor_proto)\n    self.assertEqual(function, preprocessor.random_horizontal_flip)\n    self.assertEqual(args, {\'keypoint_flip_permutation\': (1, 0, 2, 3, 5, 4)})\n\n  def test_build_random_vertical_flip(self):\n    preprocessor_text_proto = """"""\n    random_vertical_flip {\n      keypoint_flip_permutation: 1\n      keypoint_flip_permutation: 0\n      keypoint_flip_permutation: 2\n      keypoint_flip_permutation: 3\n      keypoint_flip_permutation: 5\n      keypoint_flip_permutation: 4\n    }\n    """"""\n    preprocessor_proto = preprocessor_pb2.PreprocessingStep()\n    text_format.Merge(preprocessor_text_proto, preprocessor_proto)\n    function, args = preprocessor_builder.build(preprocessor_proto)\n    self.assertEqual(function, preprocessor.random_vertical_flip)\n    self.assertEqual(args, {\'keypoint_flip_permutation\': (1, 0, 2, 3, 5, 4)})\n\n  def test_build_random_rotation90(self):\n    preprocessor_text_proto = """"""\n    random_rotation90 {}\n    """"""\n    preprocessor_proto = preprocessor_pb2.PreprocessingStep()\n    text_format.Merge(preprocessor_text_proto, preprocessor_proto)\n    function, args = preprocessor_builder.build(preprocessor_proto)\n    self.assertEqual(function, preprocessor.random_rotation90)\n    self.assertEqual(args, {})\n\n  def test_build_random_pixel_value_scale(self):\n    preprocessor_text_proto = """"""\n    random_pixel_value_scale {\n      minval: 0.8\n      maxval: 1.2\n    }\n    """"""\n    preprocessor_proto = preprocessor_pb2.PreprocessingStep()\n    text_format.Merge(preprocessor_text_proto, preprocessor_proto)\n    function, args = preprocessor_builder.build(preprocessor_proto)\n    self.assertEqual(function, preprocessor.random_pixel_value_scale)\n    self.assert_dictionary_close(args, {\'minval\': 0.8, \'maxval\': 1.2})\n\n  def test_build_random_image_scale(self):\n    preprocessor_text_proto = """"""\n    random_image_scale {\n      min_scale_ratio: 0.8\n      max_scale_ratio: 2.2\n    }\n    """"""\n    preprocessor_proto = preprocessor_pb2.PreprocessingStep()\n    text_format.Merge(preprocessor_text_proto, preprocessor_proto)\n    function, args = preprocessor_builder.build(preprocessor_proto)\n    self.assertEqual(function, preprocessor.random_image_scale)\n    self.assert_dictionary_close(args, {\'min_scale_ratio\': 0.8,\n                                        \'max_scale_ratio\': 2.2})\n\n  def test_build_random_rgb_to_gray(self):\n    preprocessor_text_proto = """"""\n    random_rgb_to_gray {\n      probability: 0.8\n    }\n    """"""\n    preprocessor_proto = preprocessor_pb2.PreprocessingStep()\n    text_format.Merge(preprocessor_text_proto, preprocessor_proto)\n    function, args = preprocessor_builder.build(preprocessor_proto)\n    self.assertEqual(function, preprocessor.random_rgb_to_gray)\n    self.assert_dictionary_close(args, {\'probability\': 0.8})\n\n  def test_build_random_adjust_brightness(self):\n    preprocessor_text_proto = """"""\n    random_adjust_brightness {\n      max_delta: 0.2\n    }\n    """"""\n    preprocessor_proto = preprocessor_pb2.PreprocessingStep()\n    text_format.Merge(preprocessor_text_proto, preprocessor_proto)\n    function, args = preprocessor_builder.build(preprocessor_proto)\n    self.assertEqual(function, preprocessor.random_adjust_brightness)\n    self.assert_dictionary_close(args, {\'max_delta\': 0.2})\n\n  def test_build_random_adjust_contrast(self):\n    preprocessor_text_proto = """"""\n    random_adjust_contrast {\n      min_delta: 0.7\n      max_delta: 1.1\n    }\n    """"""\n    preprocessor_proto = preprocessor_pb2.PreprocessingStep()\n    text_format.Merge(preprocessor_text_proto, preprocessor_proto)\n    function, args = preprocessor_builder.build(preprocessor_proto)\n    self.assertEqual(function, preprocessor.random_adjust_contrast)\n    self.assert_dictionary_close(args, {\'min_delta\': 0.7, \'max_delta\': 1.1})\n\n  def test_build_random_adjust_hue(self):\n    preprocessor_text_proto = """"""\n    random_adjust_hue {\n      max_delta: 0.01\n    }\n    """"""\n    preprocessor_proto = preprocessor_pb2.PreprocessingStep()\n    text_format.Merge(preprocessor_text_proto, preprocessor_proto)\n    function, args = preprocessor_builder.build(preprocessor_proto)\n    self.assertEqual(function, preprocessor.random_adjust_hue)\n    self.assert_dictionary_close(args, {\'max_delta\': 0.01})\n\n  def test_build_random_adjust_saturation(self):\n    preprocessor_text_proto = """"""\n    random_adjust_saturation {\n      min_delta: 0.75\n      max_delta: 1.15\n    }\n    """"""\n    preprocessor_proto = preprocessor_pb2.PreprocessingStep()\n    text_format.Merge(preprocessor_text_proto, preprocessor_proto)\n    function, args = preprocessor_builder.build(preprocessor_proto)\n    self.assertEqual(function, preprocessor.random_adjust_saturation)\n    self.assert_dictionary_close(args, {\'min_delta\': 0.75, \'max_delta\': 1.15})\n\n  def test_build_random_distort_color(self):\n    preprocessor_text_proto = """"""\n    random_distort_color {\n      color_ordering: 1\n    }\n    """"""\n    preprocessor_proto = preprocessor_pb2.PreprocessingStep()\n    text_format.Merge(preprocessor_text_proto, preprocessor_proto)\n    function, args = preprocessor_builder.build(preprocessor_proto)\n    self.assertEqual(function, preprocessor.random_distort_color)\n    self.assertEqual(args, {\'color_ordering\': 1})\n\n  def test_build_random_jitter_boxes(self):\n    preprocessor_text_proto = """"""\n    random_jitter_boxes {\n      ratio: 0.1\n    }\n    """"""\n    preprocessor_proto = preprocessor_pb2.PreprocessingStep()\n    text_format.Merge(preprocessor_text_proto, preprocessor_proto)\n    function, args = preprocessor_builder.build(preprocessor_proto)\n    self.assertEqual(function, preprocessor.random_jitter_boxes)\n    self.assert_dictionary_close(args, {\'ratio\': 0.1})\n\n  def test_build_random_crop_image(self):\n    preprocessor_text_proto = """"""\n    random_crop_image {\n      min_object_covered: 0.75\n      min_aspect_ratio: 0.75\n      max_aspect_ratio: 1.5\n      min_area: 0.25\n      max_area: 0.875\n      overlap_thresh: 0.5\n      random_coef: 0.125\n    }\n    """"""\n    preprocessor_proto = preprocessor_pb2.PreprocessingStep()\n    text_format.Merge(preprocessor_text_proto, preprocessor_proto)\n    function, args = preprocessor_builder.build(preprocessor_proto)\n    self.assertEqual(function, preprocessor.random_crop_image)\n    self.assertEqual(args, {\n        \'min_object_covered\': 0.75,\n        \'aspect_ratio_range\': (0.75, 1.5),\n        \'area_range\': (0.25, 0.875),\n        \'overlap_thresh\': 0.5,\n        \'random_coef\': 0.125,\n    })\n\n  def test_build_random_pad_image(self):\n    preprocessor_text_proto = """"""\n    random_pad_image {\n    }\n    """"""\n    preprocessor_proto = preprocessor_pb2.PreprocessingStep()\n    text_format.Merge(preprocessor_text_proto, preprocessor_proto)\n    function, args = preprocessor_builder.build(preprocessor_proto)\n    self.assertEqual(function, preprocessor.random_pad_image)\n    self.assertEqual(args, {\n        \'min_image_size\': None,\n        \'max_image_size\': None,\n        \'pad_color\': None,\n    })\n\n  def test_build_random_crop_pad_image(self):\n    preprocessor_text_proto = """"""\n    random_crop_pad_image {\n      min_object_covered: 0.75\n      min_aspect_ratio: 0.75\n      max_aspect_ratio: 1.5\n      min_area: 0.25\n      max_area: 0.875\n      overlap_thresh: 0.5\n      random_coef: 0.125\n    }\n    """"""\n    preprocessor_proto = preprocessor_pb2.PreprocessingStep()\n    text_format.Merge(preprocessor_text_proto, preprocessor_proto)\n    function, args = preprocessor_builder.build(preprocessor_proto)\n    self.assertEqual(function, preprocessor.random_crop_pad_image)\n    self.assertEqual(args, {\n        \'min_object_covered\': 0.75,\n        \'aspect_ratio_range\': (0.75, 1.5),\n        \'area_range\': (0.25, 0.875),\n        \'overlap_thresh\': 0.5,\n        \'random_coef\': 0.125,\n    })\n\n  def test_build_random_crop_pad_image_with_optional_parameters(self):\n    preprocessor_text_proto = """"""\n    random_crop_pad_image {\n      min_object_covered: 0.75\n      min_aspect_ratio: 0.75\n      max_aspect_ratio: 1.5\n      min_area: 0.25\n      max_area: 0.875\n      overlap_thresh: 0.5\n      random_coef: 0.125\n      min_padded_size_ratio: 0.5\n      min_padded_size_ratio: 0.75\n      max_padded_size_ratio: 0.5\n      max_padded_size_ratio: 0.75\n      pad_color: 0.5\n      pad_color: 0.5\n      pad_color: 1.0\n    }\n    """"""\n    preprocessor_proto = preprocessor_pb2.PreprocessingStep()\n    text_format.Merge(preprocessor_text_proto, preprocessor_proto)\n    function, args = preprocessor_builder.build(preprocessor_proto)\n    self.assertEqual(function, preprocessor.random_crop_pad_image)\n    self.assertEqual(args, {\n        \'min_object_covered\': 0.75,\n        \'aspect_ratio_range\': (0.75, 1.5),\n        \'area_range\': (0.25, 0.875),\n        \'overlap_thresh\': 0.5,\n        \'random_coef\': 0.125,\n        \'min_padded_size_ratio\': (0.5, 0.75),\n        \'max_padded_size_ratio\': (0.5, 0.75),\n        \'pad_color\': (0.5, 0.5, 1.0)\n    })\n\n  def test_build_random_crop_to_aspect_ratio(self):\n    preprocessor_text_proto = """"""\n    random_crop_to_aspect_ratio {\n      aspect_ratio: 0.85\n      overlap_thresh: 0.35\n    }\n    """"""\n    preprocessor_proto = preprocessor_pb2.PreprocessingStep()\n    text_format.Merge(preprocessor_text_proto, preprocessor_proto)\n    function, args = preprocessor_builder.build(preprocessor_proto)\n    self.assertEqual(function, preprocessor.random_crop_to_aspect_ratio)\n    self.assert_dictionary_close(args, {\'aspect_ratio\': 0.85,\n                                        \'overlap_thresh\': 0.35})\n\n  def test_build_random_black_patches(self):\n    preprocessor_text_proto = """"""\n    random_black_patches {\n      max_black_patches: 20\n      probability: 0.95\n      size_to_image_ratio: 0.12\n    }\n    """"""\n    preprocessor_proto = preprocessor_pb2.PreprocessingStep()\n    text_format.Merge(preprocessor_text_proto, preprocessor_proto)\n    function, args = preprocessor_builder.build(preprocessor_proto)\n    self.assertEqual(function, preprocessor.random_black_patches)\n    self.assert_dictionary_close(args, {\'max_black_patches\': 20,\n                                        \'probability\': 0.95,\n                                        \'size_to_image_ratio\': 0.12})\n\n  def test_build_random_resize_method(self):\n    preprocessor_text_proto = """"""\n    random_resize_method {\n      target_height: 75\n      target_width: 100\n    }\n    """"""\n    preprocessor_proto = preprocessor_pb2.PreprocessingStep()\n    text_format.Merge(preprocessor_text_proto, preprocessor_proto)\n    function, args = preprocessor_builder.build(preprocessor_proto)\n    self.assertEqual(function, preprocessor.random_resize_method)\n    self.assert_dictionary_close(args, {\'target_size\': [75, 100]})\n\n  def test_build_scale_boxes_to_pixel_coordinates(self):\n    preprocessor_text_proto = """"""\n    scale_boxes_to_pixel_coordinates {}\n    """"""\n    preprocessor_proto = preprocessor_pb2.PreprocessingStep()\n    text_format.Merge(preprocessor_text_proto, preprocessor_proto)\n    function, args = preprocessor_builder.build(preprocessor_proto)\n    self.assertEqual(function, preprocessor.scale_boxes_to_pixel_coordinates)\n    self.assertEqual(args, {})\n\n  def test_build_resize_image(self):\n    preprocessor_text_proto = """"""\n    resize_image {\n      new_height: 75\n      new_width: 100\n      method: BICUBIC\n    }\n    """"""\n    preprocessor_proto = preprocessor_pb2.PreprocessingStep()\n    text_format.Merge(preprocessor_text_proto, preprocessor_proto)\n    function, args = preprocessor_builder.build(preprocessor_proto)\n    self.assertEqual(function, preprocessor.resize_image)\n    self.assertEqual(args, {\'new_height\': 75,\n                            \'new_width\': 100,\n                            \'method\': tf.image.ResizeMethod.BICUBIC})\n\n  def test_build_rgb_to_gray(self):\n    preprocessor_text_proto = """"""\n    rgb_to_gray {}\n    """"""\n    preprocessor_proto = preprocessor_pb2.PreprocessingStep()\n    text_format.Merge(preprocessor_text_proto, preprocessor_proto)\n    function, args = preprocessor_builder.build(preprocessor_proto)\n    self.assertEqual(function, preprocessor.rgb_to_gray)\n    self.assertEqual(args, {})\n\n  def test_build_subtract_channel_mean(self):\n    preprocessor_text_proto = """"""\n    subtract_channel_mean {\n      means: [1.0, 2.0, 3.0]\n    }\n    """"""\n    preprocessor_proto = preprocessor_pb2.PreprocessingStep()\n    text_format.Merge(preprocessor_text_proto, preprocessor_proto)\n    function, args = preprocessor_builder.build(preprocessor_proto)\n    self.assertEqual(function, preprocessor.subtract_channel_mean)\n    self.assertEqual(args, {\'means\': [1.0, 2.0, 3.0]})\n\n  def test_build_ssd_random_crop(self):\n    preprocessor_text_proto = """"""\n    ssd_random_crop {\n      operations {\n        min_object_covered: 0.0\n        min_aspect_ratio: 0.875\n        max_aspect_ratio: 1.125\n        min_area: 0.5\n        max_area: 1.0\n        overlap_thresh: 0.0\n        random_coef: 0.375\n      }\n      operations {\n        min_object_covered: 0.25\n        min_aspect_ratio: 0.75\n        max_aspect_ratio: 1.5\n        min_area: 0.5\n        max_area: 1.0\n        overlap_thresh: 0.25\n        random_coef: 0.375\n      }\n    }\n    """"""\n    preprocessor_proto = preprocessor_pb2.PreprocessingStep()\n    text_format.Merge(preprocessor_text_proto, preprocessor_proto)\n    function, args = preprocessor_builder.build(preprocessor_proto)\n    self.assertEqual(function, preprocessor.ssd_random_crop)\n    self.assertEqual(args, {\'min_object_covered\': [0.0, 0.25],\n                            \'aspect_ratio_range\': [(0.875, 1.125), (0.75, 1.5)],\n                            \'area_range\': [(0.5, 1.0), (0.5, 1.0)],\n                            \'overlap_thresh\': [0.0, 0.25],\n                            \'random_coef\': [0.375, 0.375]})\n\n  def test_build_ssd_random_crop_empty_operations(self):\n    preprocessor_text_proto = """"""\n    ssd_random_crop {\n    }\n    """"""\n    preprocessor_proto = preprocessor_pb2.PreprocessingStep()\n    text_format.Merge(preprocessor_text_proto, preprocessor_proto)\n    function, args = preprocessor_builder.build(preprocessor_proto)\n    self.assertEqual(function, preprocessor.ssd_random_crop)\n    self.assertEqual(args, {})\n\n  def test_build_ssd_random_crop_pad(self):\n    preprocessor_text_proto = """"""\n    ssd_random_crop_pad {\n      operations {\n        min_object_covered: 0.0\n        min_aspect_ratio: 0.875\n        max_aspect_ratio: 1.125\n        min_area: 0.5\n        max_area: 1.0\n        overlap_thresh: 0.0\n        random_coef: 0.375\n        min_padded_size_ratio: [1.0, 1.0]\n        max_padded_size_ratio: [2.0, 2.0]\n        pad_color_r: 0.5\n        pad_color_g: 0.5\n        pad_color_b: 0.5\n      }\n      operations {\n        min_object_covered: 0.25\n        min_aspect_ratio: 0.75\n        max_aspect_ratio: 1.5\n        min_area: 0.5\n        max_area: 1.0\n        overlap_thresh: 0.25\n        random_coef: 0.375\n        min_padded_size_ratio: [1.0, 1.0]\n        max_padded_size_ratio: [2.0, 2.0]\n        pad_color_r: 0.5\n        pad_color_g: 0.5\n        pad_color_b: 0.5\n      }\n    }\n    """"""\n    preprocessor_proto = preprocessor_pb2.PreprocessingStep()\n    text_format.Merge(preprocessor_text_proto, preprocessor_proto)\n    function, args = preprocessor_builder.build(preprocessor_proto)\n    self.assertEqual(function, preprocessor.ssd_random_crop_pad)\n    self.assertEqual(args, {\'min_object_covered\': [0.0, 0.25],\n                            \'aspect_ratio_range\': [(0.875, 1.125), (0.75, 1.5)],\n                            \'area_range\': [(0.5, 1.0), (0.5, 1.0)],\n                            \'overlap_thresh\': [0.0, 0.25],\n                            \'random_coef\': [0.375, 0.375],\n                            \'min_padded_size_ratio\': [(1.0, 1.0), (1.0, 1.0)],\n                            \'max_padded_size_ratio\': [(2.0, 2.0), (2.0, 2.0)],\n                            \'pad_color\': [(0.5, 0.5, 0.5), (0.5, 0.5, 0.5)]})\n\n  def test_build_ssd_random_crop_fixed_aspect_ratio(self):\n    preprocessor_text_proto = """"""\n    ssd_random_crop_fixed_aspect_ratio {\n      operations {\n        min_object_covered: 0.0\n        min_area: 0.5\n        max_area: 1.0\n        overlap_thresh: 0.0\n        random_coef: 0.375\n      }\n      operations {\n        min_object_covered: 0.25\n        min_area: 0.5\n        max_area: 1.0\n        overlap_thresh: 0.25\n        random_coef: 0.375\n      }\n      aspect_ratio: 0.875\n    }\n    """"""\n    preprocessor_proto = preprocessor_pb2.PreprocessingStep()\n    text_format.Merge(preprocessor_text_proto, preprocessor_proto)\n    function, args = preprocessor_builder.build(preprocessor_proto)\n    self.assertEqual(function, preprocessor.ssd_random_crop_fixed_aspect_ratio)\n    self.assertEqual(args, {\'min_object_covered\': [0.0, 0.25],\n                            \'aspect_ratio\': 0.875,\n                            \'area_range\': [(0.5, 1.0), (0.5, 1.0)],\n                            \'overlap_thresh\': [0.0, 0.25],\n                            \'random_coef\': [0.375, 0.375]})\n\n  def test_build_ssd_random_crop_pad_fixed_aspect_ratio(self):\n    preprocessor_text_proto = """"""\n    ssd_random_crop_pad_fixed_aspect_ratio {\n      operations {\n        min_object_covered: 0.0\n        min_aspect_ratio: 0.875\n        max_aspect_ratio: 1.125\n        min_area: 0.5\n        max_area: 1.0\n        overlap_thresh: 0.0\n        random_coef: 0.375\n      }\n      operations {\n        min_object_covered: 0.25\n        min_aspect_ratio: 0.75\n        max_aspect_ratio: 1.5\n        min_area: 0.5\n        max_area: 1.0\n        overlap_thresh: 0.25\n        random_coef: 0.375\n      }\n      aspect_ratio: 0.875\n      min_padded_size_ratio: [1.0, 1.0]\n      max_padded_size_ratio: [2.0, 2.0]\n    }\n    """"""\n    preprocessor_proto = preprocessor_pb2.PreprocessingStep()\n    text_format.Merge(preprocessor_text_proto, preprocessor_proto)\n    function, args = preprocessor_builder.build(preprocessor_proto)\n    self.assertEqual(function,\n                     preprocessor.ssd_random_crop_pad_fixed_aspect_ratio)\n    self.assertEqual(args, {\'min_object_covered\': [0.0, 0.25],\n                            \'aspect_ratio\': 0.875,\n                            \'aspect_ratio_range\': [(0.875, 1.125), (0.75, 1.5)],\n                            \'area_range\': [(0.5, 1.0), (0.5, 1.0)],\n                            \'overlap_thresh\': [0.0, 0.25],\n                            \'random_coef\': [0.375, 0.375],\n                            \'min_padded_size_ratio\': (1.0, 1.0),\n                            \'max_padded_size_ratio\': (2.0, 2.0)})\n\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
src/object_detection/builders/region_similarity_calculator_builder.py,0,"b'# Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n""""""Builder for region similarity calculators.""""""\n\nfrom object_detection.core import region_similarity_calculator\nfrom object_detection.protos import region_similarity_calculator_pb2\n\n\ndef build(region_similarity_calculator_config):\n  """"""Builds region similarity calculator based on the configuration.\n\n  Builds one of [IouSimilarity, IoaSimilarity, NegSqDistSimilarity] objects. See\n  core/region_similarity_calculator.proto for details.\n\n  Args:\n    region_similarity_calculator_config: RegionSimilarityCalculator\n      configuration proto.\n\n  Returns:\n    region_similarity_calculator: RegionSimilarityCalculator object.\n\n  Raises:\n    ValueError: On unknown region similarity calculator.\n  """"""\n\n  if not isinstance(\n      region_similarity_calculator_config,\n      region_similarity_calculator_pb2.RegionSimilarityCalculator):\n    raise ValueError(\n        \'region_similarity_calculator_config not of type \'\n        \'region_similarity_calculator_pb2.RegionsSimilarityCalculator\')\n\n  similarity_calculator = region_similarity_calculator_config.WhichOneof(\n      \'region_similarity\')\n  if similarity_calculator == \'iou_similarity\':\n    return region_similarity_calculator.IouSimilarity()\n  if similarity_calculator == \'ioa_similarity\':\n    return region_similarity_calculator.IoaSimilarity()\n  if similarity_calculator == \'neg_sq_dist_similarity\':\n    return region_similarity_calculator.NegSqDistSimilarity()\n\n  raise ValueError(\'Unknown region similarity calculator.\')\n\n'"
src/object_detection/builders/region_similarity_calculator_builder_test.py,2,"b'# Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n""""""Tests for region_similarity_calculator_builder.""""""\n\nimport tensorflow as tf\n\nfrom google.protobuf import text_format\nfrom object_detection.builders import region_similarity_calculator_builder\nfrom object_detection.core import region_similarity_calculator\nfrom object_detection.protos import region_similarity_calculator_pb2 as sim_calc_pb2\n\n\nclass RegionSimilarityCalculatorBuilderTest(tf.test.TestCase):\n\n  def testBuildIoaSimilarityCalculator(self):\n    similarity_calc_text_proto = """"""\n      ioa_similarity {\n      }\n    """"""\n    similarity_calc_proto = sim_calc_pb2.RegionSimilarityCalculator()\n    text_format.Merge(similarity_calc_text_proto, similarity_calc_proto)\n    similarity_calc = region_similarity_calculator_builder.build(\n        similarity_calc_proto)\n    self.assertTrue(isinstance(similarity_calc,\n                               region_similarity_calculator.IoaSimilarity))\n\n  def testBuildIouSimilarityCalculator(self):\n    similarity_calc_text_proto = """"""\n      iou_similarity {\n      }\n    """"""\n    similarity_calc_proto = sim_calc_pb2.RegionSimilarityCalculator()\n    text_format.Merge(similarity_calc_text_proto, similarity_calc_proto)\n    similarity_calc = region_similarity_calculator_builder.build(\n        similarity_calc_proto)\n    self.assertTrue(isinstance(similarity_calc,\n                               region_similarity_calculator.IouSimilarity))\n\n  def testBuildNegSqDistSimilarityCalculator(self):\n    similarity_calc_text_proto = """"""\n      neg_sq_dist_similarity {\n      }\n    """"""\n    similarity_calc_proto = sim_calc_pb2.RegionSimilarityCalculator()\n    text_format.Merge(similarity_calc_text_proto, similarity_calc_proto)\n    similarity_calc = region_similarity_calculator_builder.build(\n        similarity_calc_proto)\n    self.assertTrue(isinstance(similarity_calc,\n                               region_similarity_calculator.\n                               NegSqDistSimilarity))\n\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
src/object_detection/core/__init__.py,0,b'\n'
src/object_detection/core/anchor_generator.py,4,"b'# Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n""""""Base anchor generator.\n\nThe job of the anchor generator is to create (or load) a collection\nof bounding boxes to be used as anchors.\n\nGenerated anchors are assumed to match some convolutional grid or list of grid\nshapes.  For example, we might want to generate anchors matching an 8x8\nfeature map and a 4x4 feature map.  If we place 3 anchors per grid location\non the first feature map and 6 anchors per grid location on the second feature\nmap, then 3*8*8 + 6*4*4 = 288 anchors are generated in total.\n\nTo support fully convolutional settings, feature map shapes are passed\ndynamically at generation time.  The number of anchors to place at each location\nis static --- implementations of AnchorGenerator must always be able return\nthe number of anchors that it uses per location for each feature map.\n""""""\nfrom abc import ABCMeta\nfrom abc import abstractmethod\n\nimport tensorflow as tf\n\n\nclass AnchorGenerator(object):\n  """"""Abstract base class for anchor generators.""""""\n  __metaclass__ = ABCMeta\n\n  @abstractmethod\n  def name_scope(self):\n    """"""Name scope.\n\n    Must be defined by implementations.\n\n    Returns:\n      a string representing the name scope of the anchor generation operation.\n    """"""\n    pass\n\n  @property\n  def check_num_anchors(self):\n    """"""Whether to dynamically check the number of anchors generated.\n\n    Can be overridden by implementations that would like to disable this\n    behavior.\n\n    Returns:\n      a boolean controlling whether the Generate function should dynamically\n      check the number of anchors generated against the mathematically\n      expected number of anchors.\n    """"""\n    return True\n\n  @abstractmethod\n  def num_anchors_per_location(self):\n    """"""Returns the number of anchors per spatial location.\n\n    Returns:\n      a list of integers, one for each expected feature map to be passed to\n      the `generate` function.\n    """"""\n    pass\n\n  def generate(self, feature_map_shape_list, **params):\n    """"""Generates a collection of bounding boxes to be used as anchors.\n\n    TODO(rathodv): remove **params from argument list and make stride and\n      offsets (for multiple_grid_anchor_generator) constructor arguments.\n\n    Args:\n      feature_map_shape_list: list of (height, width) pairs in the format\n        [(height_0, width_0), (height_1, width_1), ...] that the generated\n        anchors must align with.  Pairs can be provided as 1-dimensional\n        integer tensors of length 2 or simply as tuples of integers.\n      **params: parameters for anchor generation op\n\n    Returns:\n      boxes_list: a list of BoxLists each holding anchor boxes corresponding to\n        the input feature map shapes.\n\n    Raises:\n      ValueError: if the number of feature map shapes does not match the length\n        of NumAnchorsPerLocation.\n    """"""\n    if self.check_num_anchors and (\n        len(feature_map_shape_list) != len(self.num_anchors_per_location())):\n      raise ValueError(\'Number of feature maps is expected to equal the length \'\n                       \'of `num_anchors_per_location`.\')\n    with tf.name_scope(self.name_scope()):\n      anchors_list = self._generate(feature_map_shape_list, **params)\n      if self.check_num_anchors:\n        with tf.control_dependencies([\n            self._assert_correct_number_of_anchors(\n                anchors_list, feature_map_shape_list)]):\n          for item in anchors_list:\n            item.set(tf.identity(item.get()))\n      return anchors_list\n\n  @abstractmethod\n  def _generate(self, feature_map_shape_list, **params):\n    """"""To be overridden by implementations.\n\n    Args:\n      feature_map_shape_list: list of (height, width) pairs in the format\n        [(height_0, width_0), (height_1, width_1), ...] that the generated\n        anchors must align with.\n      **params: parameters for anchor generation op\n\n    Returns:\n      boxes_list: a list of BoxList, each holding a collection of N anchor\n        boxes.\n    """"""\n    pass\n\n  def _assert_correct_number_of_anchors(self, anchors_list,\n                                        feature_map_shape_list):\n    """"""Assert that correct number of anchors was generated.\n\n    Args:\n      anchors_list: A list of box_list.BoxList object holding anchors generated.\n      feature_map_shape_list: list of (height, width) pairs in the format\n        [(height_0, width_0), (height_1, width_1), ...] that the generated\n        anchors must align with.\n    Returns:\n      Op that raises InvalidArgumentError if the number of anchors does not\n        match the number of expected anchors.\n    """"""\n    expected_num_anchors = 0\n    actual_num_anchors = 0\n    for num_anchors_per_location, feature_map_shape, anchors in zip(\n        self.num_anchors_per_location(), feature_map_shape_list, anchors_list):\n      expected_num_anchors += (num_anchors_per_location\n                               * feature_map_shape[0]\n                               * feature_map_shape[1])\n      actual_num_anchors += anchors.num_boxes()\n    return tf.assert_equal(expected_num_anchors, actual_num_anchors)\n\n'"
src/object_detection/core/balanced_positive_negative_sampler.py,7,"b'# Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n""""""Class to subsample minibatches by balancing positives and negatives.\n\nSubsamples minibatches based on a pre-specified positive fraction in range\n[0,1]. The class presumes there are many more negatives than positive examples:\nif the desired batch_size cannot be achieved with the pre-specified positive\nfraction, it fills the rest with negative examples. If this is not sufficient\nfor obtaining the desired batch_size, it returns fewer examples.\n\nThe main function to call is Subsample(self, indicator, labels). For convenience\none can also call SubsampleWeights(self, weights, labels) which is defined in\nthe minibatch_sampler base class.\n""""""\n\nimport tensorflow as tf\n\nfrom object_detection.core import minibatch_sampler\n\n\nclass BalancedPositiveNegativeSampler(minibatch_sampler.MinibatchSampler):\n  """"""Subsamples minibatches to a desired balance of positives and negatives.""""""\n\n  def __init__(self, positive_fraction=0.5):\n    """"""Constructs a minibatch sampler.\n\n    Args:\n      positive_fraction: desired fraction of positive examples (scalar in [0,1])\n\n    Raises:\n      ValueError: if positive_fraction < 0, or positive_fraction > 1\n    """"""\n    if positive_fraction < 0 or positive_fraction > 1:\n      raise ValueError(\'positive_fraction should be in range [0,1]. \'\n                       \'Received: %s.\' % positive_fraction)\n    self._positive_fraction = positive_fraction\n\n  def subsample(self, indicator, batch_size, labels):\n    """"""Returns subsampled minibatch.\n\n    Args:\n      indicator: boolean tensor of shape [N] whose True entries can be sampled.\n      batch_size: desired batch size.\n      labels: boolean tensor of shape [N] denoting positive(=True) and negative\n          (=False) examples.\n\n    Returns:\n      is_sampled: boolean tensor of shape [N], True for entries which are\n          sampled.\n\n    Raises:\n      ValueError: if labels and indicator are not 1D boolean tensors.\n    """"""\n    if len(indicator.get_shape().as_list()) != 1:\n      raise ValueError(\'indicator must be 1 dimensional, got a tensor of \'\n                       \'shape %s\' % indicator.get_shape())\n    if len(labels.get_shape().as_list()) != 1:\n      raise ValueError(\'labels must be 1 dimensional, got a tensor of \'\n                       \'shape %s\' % labels.get_shape())\n    if labels.dtype != tf.bool:\n      raise ValueError(\'labels should be of type bool. Received: %s\' %\n                       labels.dtype)\n    if indicator.dtype != tf.bool:\n      raise ValueError(\'indicator should be of type bool. Received: %s\' %\n                       indicator.dtype)\n\n    # Only sample from indicated samples\n    negative_idx = tf.logical_not(labels)\n    positive_idx = tf.logical_and(labels, indicator)\n    negative_idx = tf.logical_and(negative_idx, indicator)\n\n    # Sample positive and negative samples separately\n    max_num_pos = int(self._positive_fraction * batch_size)\n    sampled_pos_idx = self.subsample_indicator(positive_idx, max_num_pos)\n    max_num_neg = batch_size - tf.reduce_sum(tf.cast(sampled_pos_idx, tf.int32))\n    sampled_neg_idx = self.subsample_indicator(negative_idx, max_num_neg)\n\n    sampled_idx = tf.logical_or(sampled_pos_idx, sampled_neg_idx)\n    return sampled_idx\n'"
src/object_detection/core/balanced_positive_negative_sampler_test.py,10,"b'# Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n""""""Tests for object_detection.core.balanced_positive_negative_sampler.""""""\n\nimport numpy as np\nimport tensorflow as tf\n\nfrom object_detection.core import balanced_positive_negative_sampler\n\n\nclass BalancedPositiveNegativeSamplerTest(tf.test.TestCase):\n\n  def test_subsample_all_examples(self):\n    numpy_labels = np.random.permutation(300)\n    indicator = tf.constant(np.ones(300) == 1)\n    numpy_labels = (numpy_labels - 200) > 0\n\n    labels = tf.constant(numpy_labels)\n\n    sampler = (balanced_positive_negative_sampler.\n               BalancedPositiveNegativeSampler())\n    is_sampled = sampler.subsample(indicator, 64, labels)\n    with self.test_session() as sess:\n      is_sampled = sess.run(is_sampled)\n      self.assertTrue(sum(is_sampled) == 64)\n      self.assertTrue(sum(np.logical_and(numpy_labels, is_sampled)) == 32)\n      self.assertTrue(sum(np.logical_and(\n          np.logical_not(numpy_labels), is_sampled)) == 32)\n\n  def test_subsample_selection(self):\n    # Test random sampling when only some examples can be sampled:\n    # 100 samples, 20 positives, 10 positives cannot be sampled\n    numpy_labels = np.arange(100)\n    numpy_indicator = numpy_labels < 90\n    indicator = tf.constant(numpy_indicator)\n    numpy_labels = (numpy_labels - 80) >= 0\n\n    labels = tf.constant(numpy_labels)\n\n    sampler = (balanced_positive_negative_sampler.\n               BalancedPositiveNegativeSampler())\n    is_sampled = sampler.subsample(indicator, 64, labels)\n    with self.test_session() as sess:\n      is_sampled = sess.run(is_sampled)\n      self.assertTrue(sum(is_sampled) == 64)\n      self.assertTrue(sum(np.logical_and(numpy_labels, is_sampled)) == 10)\n      self.assertTrue(sum(np.logical_and(\n          np.logical_not(numpy_labels), is_sampled)) == 54)\n      self.assertAllEqual(is_sampled, np.logical_and(is_sampled,\n                                                     numpy_indicator))\n\n  def test_raises_error_with_incorrect_label_shape(self):\n    labels = tf.constant([[True, False, False]])\n    indicator = tf.constant([True, False, True])\n    sampler = (balanced_positive_negative_sampler.\n               BalancedPositiveNegativeSampler())\n    with self.assertRaises(ValueError):\n      sampler.subsample(indicator, 64, labels)\n\n  def test_raises_error_with_incorrect_indicator_shape(self):\n    labels = tf.constant([True, False, False])\n    indicator = tf.constant([[True, False, True]])\n    sampler = (balanced_positive_negative_sampler.\n               BalancedPositiveNegativeSampler())\n    with self.assertRaises(ValueError):\n      sampler.subsample(indicator, 64, labels)\n\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
src/object_detection/core/batcher.py,5,"b'# Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n""""""Provides functions to batch a dictionary of input tensors.""""""\nimport collections\n\nimport tensorflow as tf\n\nfrom object_detection.core import prefetcher\n\nrt_shape_str = \'_runtime_shapes\'\n\n\nclass BatchQueue(object):\n  """"""BatchQueue class.\n\n  This class creates a batch queue to asynchronously enqueue tensors_dict.\n  It also adds a FIFO prefetcher so that the batches are readily available\n  for the consumers.  Dequeue ops for a BatchQueue object can be created via\n  the Dequeue method which evaluates to a batch of tensor_dict.\n\n  Example input pipeline with batching:\n  ------------------------------------\n  key, string_tensor = slim.parallel_reader.parallel_read(...)\n  tensor_dict = decoder.decode(string_tensor)\n  tensor_dict = preprocessor.preprocess(tensor_dict, ...)\n  batch_queue = batcher.BatchQueue(tensor_dict,\n                                   batch_size=32,\n                                   batch_queue_capacity=2000,\n                                   num_batch_queue_threads=8,\n                                   prefetch_queue_capacity=20)\n  tensor_dict = batch_queue.dequeue()\n  outputs = Model(tensor_dict)\n  ...\n  -----------------------------------\n\n  Notes:\n  -----\n  This class batches tensors of unequal sizes by zero padding and unpadding\n  them after generating a batch. This can be computationally expensive when\n  batching tensors (such as images) that are of vastly different sizes. So it is\n  recommended that the shapes of such tensors be fully defined in tensor_dict\n  while other lightweight tensors such as bounding box corners and class labels\n  can be of varying sizes. Use either crop or resize operations to fully define\n  the shape of an image in tensor_dict.\n\n  It is also recommended to perform any preprocessing operations on tensors\n  before passing to BatchQueue and subsequently calling the Dequeue method.\n\n  Another caveat is that this class does not read the last batch if it is not\n  full. The current implementation makes it hard to support that use case. So,\n  for evaluation, when it is critical to run all the examples through your\n  network use the input pipeline example mentioned in core/prefetcher.py.\n  """"""\n\n  def __init__(self, tensor_dict, batch_size, batch_queue_capacity,\n               num_batch_queue_threads, prefetch_queue_capacity):\n    """"""Constructs a batch queue holding tensor_dict.\n\n    Args:\n      tensor_dict: dictionary of tensors to batch.\n      batch_size: batch size.\n      batch_queue_capacity: max capacity of the queue from which the tensors are\n        batched.\n      num_batch_queue_threads: number of threads to use for batching.\n      prefetch_queue_capacity: max capacity of the queue used to prefetch\n        assembled batches.\n    """"""\n    # Remember static shapes to set shapes of batched tensors.\n    static_shapes = collections.OrderedDict(\n        {key: tensor.get_shape() for key, tensor in tensor_dict.items()})\n    # Remember runtime shapes to unpad tensors after batching.\n    runtime_shapes = collections.OrderedDict(\n        {(key + rt_shape_str): tf.shape(tensor)\n         for key, tensor in tensor_dict.items()})\n\n    all_tensors = tensor_dict\n    all_tensors.update(runtime_shapes)\n    batched_tensors = tf.train.batch(\n        all_tensors,\n        capacity=batch_queue_capacity,\n        batch_size=batch_size,\n        dynamic_pad=True,\n        num_threads=num_batch_queue_threads)\n\n    self._queue = prefetcher.prefetch(batched_tensors,\n                                      prefetch_queue_capacity)\n    self._static_shapes = static_shapes\n    self._batch_size = batch_size\n\n  def dequeue(self):\n    """"""Dequeues a batch of tensor_dict from the BatchQueue.\n\n    TODO: use allow_smaller_final_batch to allow running over the whole eval set\n\n    Returns:\n      A list of tensor_dicts of the requested batch_size.\n    """"""\n    batched_tensors = self._queue.dequeue()\n    # Separate input tensors from tensors containing their runtime shapes.\n    tensors = {}\n    shapes = {}\n    for key, batched_tensor in batched_tensors.items():\n      unbatched_tensor_list = tf.unstack(batched_tensor)\n      for i, unbatched_tensor in enumerate(unbatched_tensor_list):\n        if rt_shape_str in key:\n          shapes[(key[:-len(rt_shape_str)], i)] = unbatched_tensor\n        else:\n          tensors[(key, i)] = unbatched_tensor\n\n    # Undo that padding using shapes and create a list of size `batch_size` that\n    # contains tensor dictionaries.\n    tensor_dict_list = []\n    batch_size = self._batch_size\n    for batch_id in range(batch_size):\n      tensor_dict = {}\n      for key in self._static_shapes:\n        tensor_dict[key] = tf.slice(tensors[(key, batch_id)],\n                                    tf.zeros_like(shapes[(key, batch_id)]),\n                                    shapes[(key, batch_id)])\n        tensor_dict[key].set_shape(self._static_shapes[key])\n      tensor_dict_list.append(tensor_dict)\n\n    return tensor_dict_list\n'"
src/object_detection/core/batcher_test.py,22,"b'# Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n""""""Tests for object_detection.core.batcher.""""""\n\nimport numpy as np\nimport tensorflow as tf\n\nfrom object_detection.core import batcher\n\nslim = tf.contrib.slim\n\n\nclass BatcherTest(tf.test.TestCase):\n\n  def test_batch_and_unpad_2d_tensors_of_different_sizes_in_1st_dimension(self):\n    with self.test_session() as sess:\n      batch_size = 3\n      num_batches = 2\n      examples = tf.Variable(tf.constant(2, dtype=tf.int32))\n      counter = examples.count_up_to(num_batches * batch_size + 2)\n      boxes = tf.tile(\n          tf.reshape(tf.range(4), [1, 4]), tf.stack([counter, tf.constant(1)]))\n      batch_queue = batcher.BatchQueue(\n          tensor_dict={\'boxes\': boxes},\n          batch_size=batch_size,\n          batch_queue_capacity=100,\n          num_batch_queue_threads=1,\n          prefetch_queue_capacity=100)\n      batch = batch_queue.dequeue()\n\n      for tensor_dict in batch:\n        for tensor in tensor_dict.values():\n          self.assertAllEqual([None, 4], tensor.get_shape().as_list())\n\n      tf.initialize_all_variables().run()\n      with slim.queues.QueueRunners(sess):\n        i = 2\n        for _ in range(num_batches):\n          batch_np = sess.run(batch)\n          for tensor_dict in batch_np:\n            for tensor in tensor_dict.values():\n              self.assertAllEqual(tensor, np.tile(np.arange(4), (i, 1)))\n              i += 1\n        with self.assertRaises(tf.errors.OutOfRangeError):\n          sess.run(batch)\n\n  def test_batch_and_unpad_2d_tensors_of_different_sizes_in_all_dimensions(\n      self):\n    with self.test_session() as sess:\n      batch_size = 3\n      num_batches = 2\n      examples = tf.Variable(tf.constant(2, dtype=tf.int32))\n      counter = examples.count_up_to(num_batches * batch_size + 2)\n      image = tf.reshape(\n          tf.range(counter * counter), tf.stack([counter, counter]))\n      batch_queue = batcher.BatchQueue(\n          tensor_dict={\'image\': image},\n          batch_size=batch_size,\n          batch_queue_capacity=100,\n          num_batch_queue_threads=1,\n          prefetch_queue_capacity=100)\n      batch = batch_queue.dequeue()\n\n      for tensor_dict in batch:\n        for tensor in tensor_dict.values():\n          self.assertAllEqual([None, None], tensor.get_shape().as_list())\n\n      tf.initialize_all_variables().run()\n      with slim.queues.QueueRunners(sess):\n        i = 2\n        for _ in range(num_batches):\n          batch_np = sess.run(batch)\n          for tensor_dict in batch_np:\n            for tensor in tensor_dict.values():\n              self.assertAllEqual(tensor, np.arange(i * i).reshape((i, i)))\n              i += 1\n        with self.assertRaises(tf.errors.OutOfRangeError):\n          sess.run(batch)\n\n  def test_batch_and_unpad_2d_tensors_of_same_size_in_all_dimensions(self):\n    with self.test_session() as sess:\n      batch_size = 3\n      num_batches = 2\n      examples = tf.Variable(tf.constant(1, dtype=tf.int32))\n      counter = examples.count_up_to(num_batches * batch_size + 1)\n      image = tf.reshape(tf.range(1, 13), [4, 3]) * counter\n      batch_queue = batcher.BatchQueue(\n          tensor_dict={\'image\': image},\n          batch_size=batch_size,\n          batch_queue_capacity=100,\n          num_batch_queue_threads=1,\n          prefetch_queue_capacity=100)\n      batch = batch_queue.dequeue()\n\n      for tensor_dict in batch:\n        for tensor in tensor_dict.values():\n          self.assertAllEqual([4, 3], tensor.get_shape().as_list())\n\n      tf.initialize_all_variables().run()\n      with slim.queues.QueueRunners(sess):\n        i = 1\n        for _ in range(num_batches):\n          batch_np = sess.run(batch)\n          for tensor_dict in batch_np:\n            for tensor in tensor_dict.values():\n              self.assertAllEqual(tensor, np.arange(1, 13).reshape((4, 3)) * i)\n              i += 1\n        with self.assertRaises(tf.errors.OutOfRangeError):\n          sess.run(batch)\n\n  def test_batcher_when_batch_size_is_one(self):\n    with self.test_session() as sess:\n      batch_size = 1\n      num_batches = 2\n      examples = tf.Variable(tf.constant(2, dtype=tf.int32))\n      counter = examples.count_up_to(num_batches * batch_size + 2)\n      image = tf.reshape(\n          tf.range(counter * counter), tf.stack([counter, counter]))\n      batch_queue = batcher.BatchQueue(\n          tensor_dict={\'image\': image},\n          batch_size=batch_size,\n          batch_queue_capacity=100,\n          num_batch_queue_threads=1,\n          prefetch_queue_capacity=100)\n      batch = batch_queue.dequeue()\n\n      for tensor_dict in batch:\n        for tensor in tensor_dict.values():\n          self.assertAllEqual([None, None], tensor.get_shape().as_list())\n\n      tf.initialize_all_variables().run()\n      with slim.queues.QueueRunners(sess):\n        i = 2\n        for _ in range(num_batches):\n          batch_np = sess.run(batch)\n          for tensor_dict in batch_np:\n            for tensor in tensor_dict.values():\n              self.assertAllEqual(tensor, np.arange(i * i).reshape((i, i)))\n              i += 1\n        with self.assertRaises(tf.errors.OutOfRangeError):\n          sess.run(batch)\n\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
src/object_detection/core/box_coder.py,4,"b'# Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n""""""Base box coder.\n\nBox coders convert between coordinate frames, namely image-centric\n(with (0,0) on the top left of image) and anchor-centric (with (0,0) being\ndefined by a specific anchor).\n\nUsers of a BoxCoder can call two methods:\n encode: which encodes a box with respect to a given anchor\n  (or rather, a tensor of boxes wrt a corresponding tensor of anchors) and\n decode: which inverts this encoding with a decode operation.\nIn both cases, the arguments are assumed to be in 1-1 correspondence already;\nit is not the job of a BoxCoder to perform matching.\n""""""\nfrom abc import ABCMeta\nfrom abc import abstractmethod\nfrom abc import abstractproperty\n\nimport tensorflow as tf\n\n\n# Box coder types.\nFASTER_RCNN = \'faster_rcnn\'\nKEYPOINT = \'keypoint\'\nMEAN_STDDEV = \'mean_stddev\'\nSQUARE = \'square\'\n\n\nclass BoxCoder(object):\n  """"""Abstract base class for box coder.""""""\n  __metaclass__ = ABCMeta\n\n  @abstractproperty\n  def code_size(self):\n    """"""Return the size of each code.\n\n    This number is a constant and should agree with the output of the `encode`\n    op (e.g. if rel_codes is the output of self.encode(...), then it should have\n    shape [N, code_size()]).  This abstractproperty should be overridden by\n    implementations.\n\n    Returns:\n      an integer constant\n    """"""\n    pass\n\n  def encode(self, boxes, anchors):\n    """"""Encode a box list relative to an anchor collection.\n\n    Args:\n      boxes: BoxList holding N boxes to be encoded\n      anchors: BoxList of N anchors\n\n    Returns:\n      a tensor representing N relative-encoded boxes\n    """"""\n    with tf.name_scope(\'Encode\'):\n      return self._encode(boxes, anchors)\n\n  def decode(self, rel_codes, anchors):\n    """"""Decode boxes that are encoded relative to an anchor collection.\n\n    Args:\n      rel_codes: a tensor representing N relative-encoded boxes\n      anchors: BoxList of anchors\n\n    Returns:\n      boxlist: BoxList holding N boxes encoded in the ordinary way (i.e.,\n        with corners y_min, x_min, y_max, x_max)\n    """"""\n    with tf.name_scope(\'Decode\'):\n      return self._decode(rel_codes, anchors)\n\n  @abstractmethod\n  def _encode(self, boxes, anchors):\n    """"""Method to be overriden by implementations.\n\n    Args:\n      boxes: BoxList holding N boxes to be encoded\n      anchors: BoxList of N anchors\n\n    Returns:\n      a tensor representing N relative-encoded boxes\n    """"""\n    pass\n\n  @abstractmethod\n  def _decode(self, rel_codes, anchors):\n    """"""Method to be overriden by implementations.\n\n    Args:\n      rel_codes: a tensor representing N relative-encoded boxes\n      anchors: BoxList of anchors\n\n    Returns:\n      boxlist: BoxList holding N boxes encoded in the ordinary way (i.e.,\n        with corners y_min, x_min, y_max, x_max)\n    """"""\n    pass\n\n\ndef batch_decode(encoded_boxes, box_coder, anchors):\n  """"""Decode a batch of encoded boxes.\n\n  This op takes a batch of encoded bounding boxes and transforms\n  them to a batch of bounding boxes specified by their corners in\n  the order of [y_min, x_min, y_max, x_max].\n\n  Args:\n    encoded_boxes: a float32 tensor of shape [batch_size, num_anchors,\n      code_size] representing the location of the objects.\n    box_coder: a BoxCoder object.\n    anchors: a BoxList of anchors used to encode `encoded_boxes`.\n\n  Returns:\n    decoded_boxes: a float32 tensor of shape [batch_size, num_anchors,\n      coder_size] representing the corners of the objects in the order\n      of [y_min, x_min, y_max, x_max].\n\n  Raises:\n    ValueError: if batch sizes of the inputs are inconsistent, or if\n    the number of anchors inferred from encoded_boxes and anchors are\n    inconsistent.\n  """"""\n  encoded_boxes.get_shape().assert_has_rank(3)\n  if encoded_boxes.get_shape()[1].value != anchors.num_boxes_static():\n    raise ValueError(\'The number of anchors inferred from encoded_boxes\'\n                     \' and anchors are inconsistent: shape[1] of encoded_boxes\'\n                     \' %s should be equal to the number of anchors: %s.\' %\n                     (encoded_boxes.get_shape()[1].value,\n                      anchors.num_boxes_static()))\n\n  decoded_boxes = tf.stack([\n      box_coder.decode(boxes, anchors).get()\n      for boxes in tf.unstack(encoded_boxes)\n  ])\n  return decoded_boxes\n'"
src/object_detection/core/box_coder_test.py,6,"b'# Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n""""""Tests for object_detection.core.box_coder.""""""\n\nimport tensorflow as tf\n\nfrom object_detection.core import box_coder\nfrom object_detection.core import box_list\n\n\nclass MockBoxCoder(box_coder.BoxCoder):\n  """"""Test BoxCoder that encodes/decodes using the multiply-by-two function.""""""\n\n  def code_size(self):\n    return 4\n\n  def _encode(self, boxes, anchors):\n    return 2.0 * boxes.get()\n\n  def _decode(self, rel_codes, anchors):\n    return box_list.BoxList(rel_codes / 2.0)\n\n\nclass BoxCoderTest(tf.test.TestCase):\n\n  def test_batch_decode(self):\n    mock_anchor_corners = tf.constant(\n        [[0, 0.1, 0.2, 0.3], [0.2, 0.4, 0.4, 0.6]], tf.float32)\n    mock_anchors = box_list.BoxList(mock_anchor_corners)\n    mock_box_coder = MockBoxCoder()\n\n    expected_boxes = [[[0.0, 0.1, 0.5, 0.6], [0.5, 0.6, 0.7, 0.8]],\n                      [[0.1, 0.2, 0.3, 0.4], [0.7, 0.8, 0.9, 1.0]]]\n\n    encoded_boxes_list = [mock_box_coder.encode(\n        box_list.BoxList(tf.constant(boxes)), mock_anchors)\n                          for boxes in expected_boxes]\n    encoded_boxes = tf.stack(encoded_boxes_list)\n    decoded_boxes = box_coder.batch_decode(\n        encoded_boxes, mock_box_coder, mock_anchors)\n\n    with self.test_session() as sess:\n      decoded_boxes_result = sess.run(decoded_boxes)\n      self.assertAllClose(expected_boxes, decoded_boxes_result)\n\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
src/object_detection/core/box_list.py,8,"b'# Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n""""""Bounding Box List definition.\n\nBoxList represents a list of bounding boxes as tensorflow\ntensors, where each bounding box is represented as a row of 4 numbers,\n[y_min, x_min, y_max, x_max].  It is assumed that all bounding boxes\nwithin a given list correspond to a single image.  See also\nbox_list_ops.py for common box related operations (such as area, iou, etc).\n\nOptionally, users can add additional related fields (such as weights).\nWe assume the following things to be true about fields:\n* they correspond to boxes in the box_list along the 0th dimension\n* they have inferrable rank at graph construction time\n* all dimensions except for possibly the 0th can be inferred\n  (i.e., not None) at graph construction time.\n\nSome other notes:\n  * Following tensorflow conventions, we use height, width ordering,\n  and correspondingly, y,x (or ymin, xmin, ymax, xmax) ordering\n  * Tensors are always provided as (flat) [N, 4] tensors.\n""""""\n\nimport tensorflow as tf\n\n\nclass BoxList(object):\n  """"""Box collection.""""""\n\n  def __init__(self, boxes):\n    """"""Constructs box collection.\n\n    Args:\n      boxes: a tensor of shape [N, 4] representing box corners\n\n    Raises:\n      ValueError: if invalid dimensions for bbox data or if bbox data is not in\n          float32 format.\n    """"""\n    if len(boxes.get_shape()) != 2 or boxes.get_shape()[-1] != 4:\n      raise ValueError(\'Invalid dimensions for box data.\')\n    if boxes.dtype != tf.float32:\n      raise ValueError(\'Invalid tensor type: should be tf.float32\')\n    self.data = {\'boxes\': boxes}\n\n  def num_boxes(self):\n    """"""Returns number of boxes held in collection.\n\n    Returns:\n      a tensor representing the number of boxes held in the collection.\n    """"""\n    return tf.shape(self.data[\'boxes\'])[0]\n\n  def num_boxes_static(self):\n    """"""Returns number of boxes held in collection.\n\n    This number is inferred at graph construction time rather than run-time.\n\n    Returns:\n      Number of boxes held in collection (integer) or None if this is not\n        inferrable at graph construction time.\n    """"""\n    return self.data[\'boxes\'].get_shape()[0].value\n\n  def get_all_fields(self):\n    """"""Returns all fields.""""""\n    return self.data.keys()\n\n  def get_extra_fields(self):\n    """"""Returns all non-box fields (i.e., everything not named \'boxes\').""""""\n    return [k for k in self.data.keys() if k != \'boxes\']\n\n  def add_field(self, field, field_data):\n    """"""Add field to box list.\n\n    This method can be used to add related box data such as\n    weights/labels, etc.\n\n    Args:\n      field: a string key to access the data via `get`\n      field_data: a tensor containing the data to store in the BoxList\n    """"""\n    self.data[field] = field_data\n\n  def has_field(self, field):\n    return field in self.data\n\n  def get(self):\n    """"""Convenience function for accessing box coordinates.\n\n    Returns:\n      a tensor with shape [N, 4] representing box coordinates.\n    """"""\n    return self.get_field(\'boxes\')\n\n  def set(self, boxes):\n    """"""Convenience function for setting box coordinates.\n\n    Args:\n      boxes: a tensor of shape [N, 4] representing box corners\n\n    Raises:\n      ValueError: if invalid dimensions for bbox data\n    """"""\n    if len(boxes.get_shape()) != 2 or boxes.get_shape()[-1] != 4:\n      raise ValueError(\'Invalid dimensions for box data.\')\n    self.data[\'boxes\'] = boxes\n\n  def get_field(self, field):\n    """"""Accesses a box collection and associated fields.\n\n    This function returns specified field with object; if no field is specified,\n    it returns the box coordinates.\n\n    Args:\n      field: this optional string parameter can be used to specify\n        a related field to be accessed.\n\n    Returns:\n      a tensor representing the box collection or an associated field.\n\n    Raises:\n      ValueError: if invalid field\n    """"""\n    if not self.has_field(field):\n      raise ValueError(\'field \' + str(field) + \' does not exist\')\n    return self.data[field]\n\n  def set_field(self, field, value):\n    """"""Sets the value of a field.\n\n    Updates the field of a box_list with a given value.\n\n    Args:\n      field: (string) name of the field to set value.\n      value: the value to assign to the field.\n\n    Raises:\n      ValueError: if the box_list does not have specified field.\n    """"""\n    if not self.has_field(field):\n      raise ValueError(\'field %s does not exist\' % field)\n    self.data[field] = value\n\n  def get_center_coordinates_and_sizes(self, scope=None):\n    """"""Computes the center coordinates, height and width of the boxes.\n\n    Args:\n      scope: name scope of the function.\n\n    Returns:\n      a list of 4 1-D tensors [ycenter, xcenter, height, width].\n    """"""\n    with tf.name_scope(scope, \'get_center_coordinates_and_sizes\'):\n      box_corners = self.get()\n      ymin, xmin, ymax, xmax = tf.unstack(tf.transpose(box_corners))\n      width = xmax - xmin\n      height = ymax - ymin\n      ycenter = ymin + height / 2.\n      xcenter = xmin + width / 2.\n      return [ycenter, xcenter, height, width]\n\n  def transpose_coordinates(self, scope=None):\n    """"""Transpose the coordinate representation in a boxlist.\n\n    Args:\n      scope: name scope of the function.\n    """"""\n    with tf.name_scope(scope, \'transpose_coordinates\'):\n      y_min, x_min, y_max, x_max = tf.split(\n          value=self.get(), num_or_size_splits=4, axis=1)\n      self.set(tf.concat([x_min, y_min, x_max, y_max], 1))\n\n  def as_tensor_dict(self, fields=None):\n    """"""Retrieves specified fields as a dictionary of tensors.\n\n    Args:\n      fields: (optional) list of fields to return in the dictionary.\n        If None (default), all fields are returned.\n\n    Returns:\n      tensor_dict: A dictionary of tensors specified by fields.\n\n    Raises:\n      ValueError: if specified field is not contained in boxlist.\n    """"""\n    tensor_dict = {}\n    if fields is None:\n      fields = self.get_all_fields()\n    for field in fields:\n      if not self.has_field(field):\n        raise ValueError(\'boxlist must contain all specified fields\')\n      tensor_dict[field] = self.get_field(field)\n    return tensor_dict\n'"
src/object_detection/core/box_list_ops.py,165,"b'# Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n""""""Bounding Box List operations.\n\nExample box operations that are supported:\n  * areas: compute bounding box areas\n  * iou: pairwise intersection-over-union scores\n  * sq_dist: pairwise distances between bounding boxes\n\nWhenever box_list_ops functions output a BoxList, the fields of the incoming\nBoxList are retained unless documented otherwise.\n""""""\nimport tensorflow as tf\n\nfrom object_detection.core import box_list\nfrom object_detection.utils import shape_utils\n\n\nclass SortOrder(object):\n  """"""Enum class for sort order.\n\n  Attributes:\n    ascend: ascend order.\n    descend: descend order.\n  """"""\n  ascend = 1\n  descend = 2\n\n\ndef area(boxlist, scope=None):\n  """"""Computes area of boxes.\n\n  Args:\n    boxlist: BoxList holding N boxes\n    scope: name scope.\n\n  Returns:\n    a tensor with shape [N] representing box areas.\n  """"""\n  with tf.name_scope(scope, \'Area\'):\n    y_min, x_min, y_max, x_max = tf.split(\n        value=boxlist.get(), num_or_size_splits=4, axis=1)\n    return tf.squeeze((y_max - y_min) * (x_max - x_min), [1])\n\n\ndef height_width(boxlist, scope=None):\n  """"""Computes height and width of boxes in boxlist.\n\n  Args:\n    boxlist: BoxList holding N boxes\n    scope: name scope.\n\n  Returns:\n    Height: A tensor with shape [N] representing box heights.\n    Width: A tensor with shape [N] representing box widths.\n  """"""\n  with tf.name_scope(scope, \'HeightWidth\'):\n    y_min, x_min, y_max, x_max = tf.split(\n        value=boxlist.get(), num_or_size_splits=4, axis=1)\n    return tf.squeeze(y_max - y_min, [1]), tf.squeeze(x_max - x_min, [1])\n\n\ndef scale(boxlist, y_scale, x_scale, scope=None):\n  """"""scale box coordinates in x and y dimensions.\n\n  Args:\n    boxlist: BoxList holding N boxes\n    y_scale: (float) scalar tensor\n    x_scale: (float) scalar tensor\n    scope: name scope.\n\n  Returns:\n    boxlist: BoxList holding N boxes\n  """"""\n  with tf.name_scope(scope, \'Scale\'):\n    y_scale = tf.cast(y_scale, tf.float32)\n    x_scale = tf.cast(x_scale, tf.float32)\n    y_min, x_min, y_max, x_max = tf.split(\n        value=boxlist.get(), num_or_size_splits=4, axis=1)\n    y_min = y_scale * y_min\n    y_max = y_scale * y_max\n    x_min = x_scale * x_min\n    x_max = x_scale * x_max\n    scaled_boxlist = box_list.BoxList(\n        tf.concat([y_min, x_min, y_max, x_max], 1))\n    return _copy_extra_fields(scaled_boxlist, boxlist)\n\n\ndef clip_to_window(boxlist, window, filter_nonoverlapping=True, scope=None):\n  """"""Clip bounding boxes to a window.\n\n  This op clips any input bounding boxes (represented by bounding box\n  corners) to a window, optionally filtering out boxes that do not\n  overlap at all with the window.\n\n  Args:\n    boxlist: BoxList holding M_in boxes\n    window: a tensor of shape [4] representing the [y_min, x_min, y_max, x_max]\n      window to which the op should clip boxes.\n    filter_nonoverlapping: whether to filter out boxes that do not overlap at\n      all with the window.\n    scope: name scope.\n\n  Returns:\n    a BoxList holding M_out boxes where M_out <= M_in\n  """"""\n  with tf.name_scope(scope, \'ClipToWindow\'):\n    y_min, x_min, y_max, x_max = tf.split(\n        value=boxlist.get(), num_or_size_splits=4, axis=1)\n    win_y_min, win_x_min, win_y_max, win_x_max = tf.unstack(window)\n    y_min_clipped = tf.maximum(tf.minimum(y_min, win_y_max), win_y_min)\n    y_max_clipped = tf.maximum(tf.minimum(y_max, win_y_max), win_y_min)\n    x_min_clipped = tf.maximum(tf.minimum(x_min, win_x_max), win_x_min)\n    x_max_clipped = tf.maximum(tf.minimum(x_max, win_x_max), win_x_min)\n    clipped = box_list.BoxList(\n        tf.concat([y_min_clipped, x_min_clipped, y_max_clipped, x_max_clipped],\n                  1))\n    clipped = _copy_extra_fields(clipped, boxlist)\n    if filter_nonoverlapping:\n      areas = area(clipped)\n      nonzero_area_indices = tf.cast(\n          tf.reshape(tf.where(tf.greater(areas, 0.0)), [-1]), tf.int32)\n      clipped = gather(clipped, nonzero_area_indices)\n    return clipped\n\n\ndef prune_outside_window(boxlist, window, scope=None):\n  """"""Prunes bounding boxes that fall outside a given window.\n\n  This function prunes bounding boxes that even partially fall outside the given\n  window. See also clip_to_window which only prunes bounding boxes that fall\n  completely outside the window, and clips any bounding boxes that partially\n  overflow.\n\n  Args:\n    boxlist: a BoxList holding M_in boxes.\n    window: a float tensor of shape [4] representing [ymin, xmin, ymax, xmax]\n      of the window\n    scope: name scope.\n\n  Returns:\n    pruned_corners: a tensor with shape [M_out, 4] where M_out <= M_in\n    valid_indices: a tensor with shape [M_out] indexing the valid bounding boxes\n     in the input tensor.\n  """"""\n  with tf.name_scope(scope, \'PruneOutsideWindow\'):\n    y_min, x_min, y_max, x_max = tf.split(\n        value=boxlist.get(), num_or_size_splits=4, axis=1)\n    win_y_min, win_x_min, win_y_max, win_x_max = tf.unstack(window)\n    coordinate_violations = tf.concat([\n        tf.less(y_min, win_y_min), tf.less(x_min, win_x_min),\n        tf.greater(y_max, win_y_max), tf.greater(x_max, win_x_max)\n    ], 1)\n    valid_indices = tf.reshape(\n        tf.where(tf.logical_not(tf.reduce_any(coordinate_violations, 1))), [-1])\n    return gather(boxlist, valid_indices), valid_indices\n\n\ndef prune_completely_outside_window(boxlist, window, scope=None):\n  """"""Prunes bounding boxes that fall completely outside of the given window.\n\n  The function clip_to_window prunes bounding boxes that fall\n  completely outside the window, but also clips any bounding boxes that\n  partially overflow. This function does not clip partially overflowing boxes.\n\n  Args:\n    boxlist: a BoxList holding M_in boxes.\n    window: a float tensor of shape [4] representing [ymin, xmin, ymax, xmax]\n      of the window\n    scope: name scope.\n\n  Returns:\n    pruned_boxlist: a new BoxList with all bounding boxes partially or fully in\n      the window.\n    valid_indices: a tensor with shape [M_out] indexing the valid bounding boxes\n     in the input tensor.\n  """"""\n  with tf.name_scope(scope, \'PruneCompleteleyOutsideWindow\'):\n    y_min, x_min, y_max, x_max = tf.split(\n        value=boxlist.get(), num_or_size_splits=4, axis=1)\n    win_y_min, win_x_min, win_y_max, win_x_max = tf.unstack(window)\n    coordinate_violations = tf.concat([\n        tf.greater_equal(y_min, win_y_max), tf.greater_equal(x_min, win_x_max),\n        tf.less_equal(y_max, win_y_min), tf.less_equal(x_max, win_x_min)\n    ], 1)\n    valid_indices = tf.reshape(\n        tf.where(tf.logical_not(tf.reduce_any(coordinate_violations, 1))), [-1])\n    return gather(boxlist, valid_indices), valid_indices\n\n\ndef intersection(boxlist1, boxlist2, scope=None):\n  """"""Compute pairwise intersection areas between boxes.\n\n  Args:\n    boxlist1: BoxList holding N boxes\n    boxlist2: BoxList holding M boxes\n    scope: name scope.\n\n  Returns:\n    a tensor with shape [N, M] representing pairwise intersections\n  """"""\n  with tf.name_scope(scope, \'Intersection\'):\n    y_min1, x_min1, y_max1, x_max1 = tf.split(\n        value=boxlist1.get(), num_or_size_splits=4, axis=1)\n    y_min2, x_min2, y_max2, x_max2 = tf.split(\n        value=boxlist2.get(), num_or_size_splits=4, axis=1)\n    all_pairs_min_ymax = tf.minimum(y_max1, tf.transpose(y_max2))\n    all_pairs_max_ymin = tf.maximum(y_min1, tf.transpose(y_min2))\n    intersect_heights = tf.maximum(0.0, all_pairs_min_ymax - all_pairs_max_ymin)\n    all_pairs_min_xmax = tf.minimum(x_max1, tf.transpose(x_max2))\n    all_pairs_max_xmin = tf.maximum(x_min1, tf.transpose(x_min2))\n    intersect_widths = tf.maximum(0.0, all_pairs_min_xmax - all_pairs_max_xmin)\n    return intersect_heights * intersect_widths\n\n\ndef matched_intersection(boxlist1, boxlist2, scope=None):\n  """"""Compute intersection areas between corresponding boxes in two boxlists.\n\n  Args:\n    boxlist1: BoxList holding N boxes\n    boxlist2: BoxList holding N boxes\n    scope: name scope.\n\n  Returns:\n    a tensor with shape [N] representing pairwise intersections\n  """"""\n  with tf.name_scope(scope, \'MatchedIntersection\'):\n    y_min1, x_min1, y_max1, x_max1 = tf.split(\n        value=boxlist1.get(), num_or_size_splits=4, axis=1)\n    y_min2, x_min2, y_max2, x_max2 = tf.split(\n        value=boxlist2.get(), num_or_size_splits=4, axis=1)\n    min_ymax = tf.minimum(y_max1, y_max2)\n    max_ymin = tf.maximum(y_min1, y_min2)\n    intersect_heights = tf.maximum(0.0, min_ymax - max_ymin)\n    min_xmax = tf.minimum(x_max1, x_max2)\n    max_xmin = tf.maximum(x_min1, x_min2)\n    intersect_widths = tf.maximum(0.0, min_xmax - max_xmin)\n    return tf.reshape(intersect_heights * intersect_widths, [-1])\n\n\ndef iou(boxlist1, boxlist2, scope=None):\n  """"""Computes pairwise intersection-over-union between box collections.\n\n  Args:\n    boxlist1: BoxList holding N boxes\n    boxlist2: BoxList holding M boxes\n    scope: name scope.\n\n  Returns:\n    a tensor with shape [N, M] representing pairwise iou scores.\n  """"""\n  with tf.name_scope(scope, \'IOU\'):\n    intersections = intersection(boxlist1, boxlist2)\n    areas1 = area(boxlist1)\n    areas2 = area(boxlist2)\n    unions = (\n        tf.expand_dims(areas1, 1) + tf.expand_dims(areas2, 0) - intersections)\n    return tf.where(\n        tf.equal(intersections, 0.0),\n        tf.zeros_like(intersections), tf.truediv(intersections, unions))\n\n\ndef matched_iou(boxlist1, boxlist2, scope=None):\n  """"""Compute intersection-over-union between corresponding boxes in boxlists.\n\n  Args:\n    boxlist1: BoxList holding N boxes\n    boxlist2: BoxList holding N boxes\n    scope: name scope.\n\n  Returns:\n    a tensor with shape [N] representing pairwise iou scores.\n  """"""\n  with tf.name_scope(scope, \'MatchedIOU\'):\n    intersections = matched_intersection(boxlist1, boxlist2)\n    areas1 = area(boxlist1)\n    areas2 = area(boxlist2)\n    unions = areas1 + areas2 - intersections\n    return tf.where(\n        tf.equal(intersections, 0.0),\n        tf.zeros_like(intersections), tf.truediv(intersections, unions))\n\n\ndef ioa(boxlist1, boxlist2, scope=None):\n  """"""Computes pairwise intersection-over-area between box collections.\n\n  intersection-over-area (IOA) between two boxes box1 and box2 is defined as\n  their intersection area over box2\'s area. Note that ioa is not symmetric,\n  that is, ioa(box1, box2) != ioa(box2, box1).\n\n  Args:\n    boxlist1: BoxList holding N boxes\n    boxlist2: BoxList holding M boxes\n    scope: name scope.\n\n  Returns:\n    a tensor with shape [N, M] representing pairwise ioa scores.\n  """"""\n  with tf.name_scope(scope, \'IOA\'):\n    intersections = intersection(boxlist1, boxlist2)\n    areas = tf.expand_dims(area(boxlist2), 0)\n    return tf.truediv(intersections, areas)\n\n\ndef prune_non_overlapping_boxes(\n    boxlist1, boxlist2, min_overlap=0.0, scope=None):\n  """"""Prunes the boxes in boxlist1 that overlap less than thresh with boxlist2.\n\n  For each box in boxlist1, we want its IOA to be more than minoverlap with\n  at least one of the boxes in boxlist2. If it does not, we remove it.\n\n  Args:\n    boxlist1: BoxList holding N boxes.\n    boxlist2: BoxList holding M boxes.\n    min_overlap: Minimum required overlap between boxes, to count them as\n                overlapping.\n    scope: name scope.\n\n  Returns:\n    new_boxlist1: A pruned boxlist with size [N\', 4].\n    keep_inds: A tensor with shape [N\'] indexing kept bounding boxes in the\n      first input BoxList `boxlist1`.\n  """"""\n  with tf.name_scope(scope, \'PruneNonOverlappingBoxes\'):\n    ioa_ = ioa(boxlist2, boxlist1)  # [M, N] tensor\n    ioa_ = tf.reduce_max(ioa_, reduction_indices=[0])  # [N] tensor\n    keep_bool = tf.greater_equal(ioa_, tf.constant(min_overlap))\n    keep_inds = tf.squeeze(tf.where(keep_bool), squeeze_dims=[1])\n    new_boxlist1 = gather(boxlist1, keep_inds)\n    return new_boxlist1, keep_inds\n\n\ndef prune_small_boxes(boxlist, min_side, scope=None):\n  """"""Prunes small boxes in the boxlist which have a side smaller than min_side.\n\n  Args:\n    boxlist: BoxList holding N boxes.\n    min_side: Minimum width AND height of box to survive pruning.\n    scope: name scope.\n\n  Returns:\n    A pruned boxlist.\n  """"""\n  with tf.name_scope(scope, \'PruneSmallBoxes\'):\n    height, width = height_width(boxlist)\n    is_valid = tf.logical_and(tf.greater_equal(width, min_side),\n                              tf.greater_equal(height, min_side))\n    return gather(boxlist, tf.reshape(tf.where(is_valid), [-1]))\n\n\ndef change_coordinate_frame(boxlist, window, scope=None):\n  """"""Change coordinate frame of the boxlist to be relative to window\'s frame.\n\n  Given a window of the form [ymin, xmin, ymax, xmax],\n  changes bounding box coordinates from boxlist to be relative to this window\n  (e.g., the min corner maps to (0,0) and the max corner maps to (1,1)).\n\n  An example use case is data augmentation: where we are given groundtruth\n  boxes (boxlist) and would like to randomly crop the image to some\n  window (window). In this case we need to change the coordinate frame of\n  each groundtruth box to be relative to this new window.\n\n  Args:\n    boxlist: A BoxList object holding N boxes.\n    window: A rank 1 tensor [4].\n    scope: name scope.\n\n  Returns:\n    Returns a BoxList object with N boxes.\n  """"""\n  with tf.name_scope(scope, \'ChangeCoordinateFrame\'):\n    win_height = window[2] - window[0]\n    win_width = window[3] - window[1]\n    boxlist_new = scale(box_list.BoxList(\n        boxlist.get() - [window[0], window[1], window[0], window[1]]),\n                        1.0 / win_height, 1.0 / win_width)\n    boxlist_new = _copy_extra_fields(boxlist_new, boxlist)\n    return boxlist_new\n\n\ndef sq_dist(boxlist1, boxlist2, scope=None):\n  """"""Computes the pairwise squared distances between box corners.\n\n  This op treats each box as if it were a point in a 4d Euclidean space and\n  computes pairwise squared distances.\n\n  Mathematically, we are given two matrices of box coordinates X and Y,\n  where X(i,:) is the i\'th row of X, containing the 4 numbers defining the\n  corners of the i\'th box in boxlist1. Similarly Y(j,:) corresponds to\n  boxlist2.  We compute\n  Z(i,j) = ||X(i,:) - Y(j,:)||^2\n         = ||X(i,:)||^2 + ||Y(j,:)||^2 - 2 X(i,:)\' * Y(j,:),\n\n  Args:\n    boxlist1: BoxList holding N boxes\n    boxlist2: BoxList holding M boxes\n    scope: name scope.\n\n  Returns:\n    a tensor with shape [N, M] representing pairwise distances\n  """"""\n  with tf.name_scope(scope, \'SqDist\'):\n    sqnorm1 = tf.reduce_sum(tf.square(boxlist1.get()), 1, keep_dims=True)\n    sqnorm2 = tf.reduce_sum(tf.square(boxlist2.get()), 1, keep_dims=True)\n    innerprod = tf.matmul(boxlist1.get(), boxlist2.get(),\n                          transpose_a=False, transpose_b=True)\n    return sqnorm1 + tf.transpose(sqnorm2) - 2.0 * innerprod\n\n\ndef boolean_mask(boxlist, indicator, fields=None, scope=None):\n  """"""Select boxes from BoxList according to indicator and return new BoxList.\n\n  `boolean_mask` returns the subset of boxes that are marked as ""True"" by the\n  indicator tensor. By default, `boolean_mask` returns boxes corresponding to\n  the input index list, as well as all additional fields stored in the boxlist\n  (indexing into the first dimension).  However one can optionally only draw\n  from a subset of fields.\n\n  Args:\n    boxlist: BoxList holding N boxes\n    indicator: a rank-1 boolean tensor\n    fields: (optional) list of fields to also gather from.  If None (default),\n      all fields are gathered from.  Pass an empty fields list to only gather\n      the box coordinates.\n    scope: name scope.\n\n  Returns:\n    subboxlist: a BoxList corresponding to the subset of the input BoxList\n      specified by indicator\n  Raises:\n    ValueError: if `indicator` is not a rank-1 boolean tensor.\n  """"""\n  with tf.name_scope(scope, \'BooleanMask\'):\n    if indicator.shape.ndims != 1:\n      raise ValueError(\'indicator should have rank 1\')\n    if indicator.dtype != tf.bool:\n      raise ValueError(\'indicator should be a boolean tensor\')\n    subboxlist = box_list.BoxList(tf.boolean_mask(boxlist.get(), indicator))\n    if fields is None:\n      fields = boxlist.get_extra_fields()\n    for field in fields:\n      if not boxlist.has_field(field):\n        raise ValueError(\'boxlist must contain all specified fields\')\n      subfieldlist = tf.boolean_mask(boxlist.get_field(field), indicator)\n      subboxlist.add_field(field, subfieldlist)\n    return subboxlist\n\n\ndef gather(boxlist, indices, fields=None, scope=None):\n  """"""Gather boxes from BoxList according to indices and return new BoxList.\n\n  By default, `gather` returns boxes corresponding to the input index list, as\n  well as all additional fields stored in the boxlist (indexing into the\n  first dimension).  However one can optionally only gather from a\n  subset of fields.\n\n  Args:\n    boxlist: BoxList holding N boxes\n    indices: a rank-1 tensor of type int32 / int64\n    fields: (optional) list of fields to also gather from.  If None (default),\n      all fields are gathered from.  Pass an empty fields list to only gather\n      the box coordinates.\n    scope: name scope.\n\n  Returns:\n    subboxlist: a BoxList corresponding to the subset of the input BoxList\n    specified by indices\n  Raises:\n    ValueError: if specified field is not contained in boxlist or if the\n      indices are not of type int32\n  """"""\n  with tf.name_scope(scope, \'Gather\'):\n    if len(indices.shape.as_list()) != 1:\n      raise ValueError(\'indices should have rank 1\')\n    if indices.dtype != tf.int32 and indices.dtype != tf.int64:\n      raise ValueError(\'indices should be an int32 / int64 tensor\')\n    subboxlist = box_list.BoxList(tf.gather(boxlist.get(), indices))\n    if fields is None:\n      fields = boxlist.get_extra_fields()\n    for field in fields:\n      if not boxlist.has_field(field):\n        raise ValueError(\'boxlist must contain all specified fields\')\n      subfieldlist = tf.gather(boxlist.get_field(field), indices)\n      subboxlist.add_field(field, subfieldlist)\n    return subboxlist\n\n\ndef concatenate(boxlists, fields=None, scope=None):\n  """"""Concatenate list of BoxLists.\n\n  This op concatenates a list of input BoxLists into a larger BoxList.  It also\n  handles concatenation of BoxList fields as long as the field tensor shapes\n  are equal except for the first dimension.\n\n  Args:\n    boxlists: list of BoxList objects\n    fields: optional list of fields to also concatenate.  By default, all\n      fields from the first BoxList in the list are included in the\n      concatenation.\n    scope: name scope.\n\n  Returns:\n    a BoxList with number of boxes equal to\n      sum([boxlist.num_boxes() for boxlist in BoxList])\n  Raises:\n    ValueError: if boxlists is invalid (i.e., is not a list, is empty, or\n      contains non BoxList objects), or if requested fields are not contained in\n      all boxlists\n  """"""\n  with tf.name_scope(scope, \'Concatenate\'):\n    if not isinstance(boxlists, list):\n      raise ValueError(\'boxlists should be a list\')\n    if not boxlists:\n      raise ValueError(\'boxlists should have nonzero length\')\n    for boxlist in boxlists:\n      if not isinstance(boxlist, box_list.BoxList):\n        raise ValueError(\'all elements of boxlists should be BoxList objects\')\n    concatenated = box_list.BoxList(\n        tf.concat([boxlist.get() for boxlist in boxlists], 0))\n    if fields is None:\n      fields = boxlists[0].get_extra_fields()\n    for field in fields:\n      first_field_shape = boxlists[0].get_field(field).get_shape().as_list()\n      first_field_shape[0] = -1\n      if None in first_field_shape:\n        raise ValueError(\'field %s must have fully defined shape except for the\'\n                         \' 0th dimension.\' % field)\n      for boxlist in boxlists:\n        if not boxlist.has_field(field):\n          raise ValueError(\'boxlist must contain all requested fields\')\n        field_shape = boxlist.get_field(field).get_shape().as_list()\n        field_shape[0] = -1\n        if field_shape != first_field_shape:\n          raise ValueError(\'field %s must have same shape for all boxlists \'\n                           \'except for the 0th dimension.\' % field)\n      concatenated_field = tf.concat(\n          [boxlist.get_field(field) for boxlist in boxlists], 0)\n      concatenated.add_field(field, concatenated_field)\n    return concatenated\n\n\ndef sort_by_field(boxlist, field, order=SortOrder.descend, scope=None):\n  """"""Sort boxes and associated fields according to a scalar field.\n\n  A common use case is reordering the boxes according to descending scores.\n\n  Args:\n    boxlist: BoxList holding N boxes.\n    field: A BoxList field for sorting and reordering the BoxList.\n    order: (Optional) descend or ascend. Default is descend.\n    scope: name scope.\n\n  Returns:\n    sorted_boxlist: A sorted BoxList with the field in the specified order.\n\n  Raises:\n    ValueError: if specified field does not exist\n    ValueError: if the order is not either descend or ascend\n  """"""\n  with tf.name_scope(scope, \'SortByField\'):\n    if order != SortOrder.descend and order != SortOrder.ascend:\n      raise ValueError(\'Invalid sort order\')\n\n    field_to_sort = boxlist.get_field(field)\n    if len(field_to_sort.shape.as_list()) != 1:\n      raise ValueError(\'Field should have rank 1\')\n\n    num_boxes = boxlist.num_boxes()\n    num_entries = tf.size(field_to_sort)\n    length_assert = tf.Assert(\n        tf.equal(num_boxes, num_entries),\n        [\'Incorrect field size: actual vs expected.\', num_entries, num_boxes])\n\n    with tf.control_dependencies([length_assert]):\n      # TODO(derekjchow): Remove with tf.device when top_k operation runs\n      # correctly on GPU.\n      with tf.device(\'/cpu:0\'):\n        _, sorted_indices = tf.nn.top_k(field_to_sort, num_boxes, sorted=True)\n\n    if order == SortOrder.ascend:\n      sorted_indices = tf.reverse_v2(sorted_indices, [0])\n\n    return gather(boxlist, sorted_indices)\n\n\ndef visualize_boxes_in_image(image, boxlist, normalized=False, scope=None):\n  """"""Overlay bounding box list on image.\n\n  Currently this visualization plots a 1 pixel thick red bounding box on top\n  of the image.  Note that tf.image.draw_bounding_boxes essentially is\n  1 indexed.\n\n  Args:\n    image: an image tensor with shape [height, width, 3]\n    boxlist: a BoxList\n    normalized: (boolean) specify whether corners are to be interpreted\n      as absolute coordinates in image space or normalized with respect to the\n      image size.\n    scope: name scope.\n\n  Returns:\n    image_and_boxes: an image tensor with shape [height, width, 3]\n  """"""\n  with tf.name_scope(scope, \'VisualizeBoxesInImage\'):\n    if not normalized:\n      height, width, _ = tf.unstack(tf.shape(image))\n      boxlist = scale(boxlist,\n                      1.0 / tf.cast(height, tf.float32),\n                      1.0 / tf.cast(width, tf.float32))\n    corners = tf.expand_dims(boxlist.get(), 0)\n    image = tf.expand_dims(image, 0)\n    return tf.squeeze(tf.image.draw_bounding_boxes(image, corners), [0])\n\n\ndef filter_field_value_equals(boxlist, field, value, scope=None):\n  """"""Filter to keep only boxes with field entries equal to the given value.\n\n  Args:\n    boxlist: BoxList holding N boxes.\n    field: field name for filtering.\n    value: scalar value.\n    scope: name scope.\n\n  Returns:\n    a BoxList holding M boxes where M <= N\n\n  Raises:\n    ValueError: if boxlist not a BoxList object or if it does not have\n      the specified field.\n  """"""\n  with tf.name_scope(scope, \'FilterFieldValueEquals\'):\n    if not isinstance(boxlist, box_list.BoxList):\n      raise ValueError(\'boxlist must be a BoxList\')\n    if not boxlist.has_field(field):\n      raise ValueError(\'boxlist must contain the specified field\')\n    filter_field = boxlist.get_field(field)\n    gather_index = tf.reshape(tf.where(tf.equal(filter_field, value)), [-1])\n    return gather(boxlist, gather_index)\n\n\ndef filter_greater_than(boxlist, thresh, scope=None):\n  """"""Filter to keep only boxes with score exceeding a given threshold.\n\n  This op keeps the collection of boxes whose corresponding scores are\n  greater than the input threshold.\n\n  TODO(jonathanhuang): Change function name to filter_scores_greater_than\n\n  Args:\n    boxlist: BoxList holding N boxes.  Must contain a \'scores\' field\n      representing detection scores.\n    thresh: scalar threshold\n    scope: name scope.\n\n  Returns:\n    a BoxList holding M boxes where M <= N\n\n  Raises:\n    ValueError: if boxlist not a BoxList object or if it does not\n      have a scores field\n  """"""\n  with tf.name_scope(scope, \'FilterGreaterThan\'):\n    if not isinstance(boxlist, box_list.BoxList):\n      raise ValueError(\'boxlist must be a BoxList\')\n    if not boxlist.has_field(\'scores\'):\n      raise ValueError(\'input boxlist must have \\\'scores\\\' field\')\n    scores = boxlist.get_field(\'scores\')\n    if len(scores.shape.as_list()) > 2:\n      raise ValueError(\'Scores should have rank 1 or 2\')\n    if len(scores.shape.as_list()) == 2 and scores.shape.as_list()[1] != 1:\n      raise ValueError(\'Scores should have rank 1 or have shape \'\n                       \'consistent with [None, 1]\')\n    high_score_indices = tf.cast(tf.reshape(\n        tf.where(tf.greater(scores, thresh)),\n        [-1]), tf.int32)\n    return gather(boxlist, high_score_indices)\n\n\ndef non_max_suppression(boxlist, thresh, max_output_size, scope=None):\n  """"""Non maximum suppression.\n\n  This op greedily selects a subset of detection bounding boxes, pruning\n  away boxes that have high IOU (intersection over union) overlap (> thresh)\n  with already selected boxes.  Note that this only works for a single class ---\n  to apply NMS to multi-class predictions, use MultiClassNonMaxSuppression.\n\n  Args:\n    boxlist: BoxList holding N boxes.  Must contain a \'scores\' field\n      representing detection scores.\n    thresh: scalar threshold\n    max_output_size: maximum number of retained boxes\n    scope: name scope.\n\n  Returns:\n    a BoxList holding M boxes where M <= max_output_size\n  Raises:\n    ValueError: if thresh is not in [0, 1]\n  """"""\n  with tf.name_scope(scope, \'NonMaxSuppression\'):\n    if not 0 <= thresh <= 1.0:\n      raise ValueError(\'thresh must be between 0 and 1\')\n    if not isinstance(boxlist, box_list.BoxList):\n      raise ValueError(\'boxlist must be a BoxList\')\n    if not boxlist.has_field(\'scores\'):\n      raise ValueError(\'input boxlist must have \\\'scores\\\' field\')\n    selected_indices = tf.image.non_max_suppression(\n        boxlist.get(), boxlist.get_field(\'scores\'),\n        max_output_size, iou_threshold=thresh)\n    return gather(boxlist, selected_indices)\n\n\ndef _copy_extra_fields(boxlist_to_copy_to, boxlist_to_copy_from):\n  """"""Copies the extra fields of boxlist_to_copy_from to boxlist_to_copy_to.\n\n  Args:\n    boxlist_to_copy_to: BoxList to which extra fields are copied.\n    boxlist_to_copy_from: BoxList from which fields are copied.\n\n  Returns:\n    boxlist_to_copy_to with extra fields.\n  """"""\n  for field in boxlist_to_copy_from.get_extra_fields():\n    boxlist_to_copy_to.add_field(field, boxlist_to_copy_from.get_field(field))\n  return boxlist_to_copy_to\n\n\ndef to_normalized_coordinates(boxlist, height, width,\n                              check_range=True, scope=None):\n  """"""Converts absolute box coordinates to normalized coordinates in [0, 1].\n\n  Usually one uses the dynamic shape of the image or conv-layer tensor:\n    boxlist = box_list_ops.to_normalized_coordinates(boxlist,\n                                                     tf.shape(images)[1],\n                                                     tf.shape(images)[2]),\n\n  This function raises an assertion failed error at graph execution time when\n  the maximum coordinate is smaller than 1.01 (which means that coordinates are\n  already normalized). The value 1.01 is to deal with small rounding errors.\n\n  Args:\n    boxlist: BoxList with coordinates in terms of pixel-locations.\n    height: Maximum value for height of absolute box coordinates.\n    width: Maximum value for width of absolute box coordinates.\n    check_range: If True, checks if the coordinates are normalized or not.\n    scope: name scope.\n\n  Returns:\n    boxlist with normalized coordinates in [0, 1].\n  """"""\n  with tf.name_scope(scope, \'ToNormalizedCoordinates\'):\n    height = tf.cast(height, tf.float32)\n    width = tf.cast(width, tf.float32)\n\n    if check_range:\n      max_val = tf.reduce_max(boxlist.get())\n      max_assert = tf.Assert(tf.greater(max_val, 1.01),\n                             [\'max value is lower than 1.01: \', max_val])\n      with tf.control_dependencies([max_assert]):\n        width = tf.identity(width)\n\n    return scale(boxlist, 1 / height, 1 / width)\n\n\ndef to_absolute_coordinates(boxlist,\n                            height,\n                            width,\n                            check_range=True,\n                            maximum_normalized_coordinate=1.01,\n                            scope=None):\n  """"""Converts normalized box coordinates to absolute pixel coordinates.\n\n  This function raises an assertion failed error when the maximum box coordinate\n  value is larger than maximum_normalized_coordinate (in which case coordinates\n  are already absolute).\n\n  Args:\n    boxlist: BoxList with coordinates in range [0, 1].\n    height: Maximum value for height of absolute box coordinates.\n    width: Maximum value for width of absolute box coordinates.\n    check_range: If True, checks if the coordinates are normalized or not.\n    maximum_normalized_coordinate: Maximum coordinate value to be considered\n      as normalized, default to 1.01.\n    scope: name scope.\n\n  Returns:\n    boxlist with absolute coordinates in terms of the image size.\n\n  """"""\n  with tf.name_scope(scope, \'ToAbsoluteCoordinates\'):\n    height = tf.cast(height, tf.float32)\n    width = tf.cast(width, tf.float32)\n\n    # Ensure range of input boxes is correct.\n    if check_range:\n      box_maximum = tf.reduce_max(boxlist.get())\n      max_assert = tf.Assert(\n          tf.greater_equal(maximum_normalized_coordinate, box_maximum),\n          [\'maximum box coordinate value is larger \'\n           \'than %f: \' % maximum_normalized_coordinate, box_maximum])\n      with tf.control_dependencies([max_assert]):\n        width = tf.identity(width)\n\n    return scale(boxlist, height, width)\n\n\ndef refine_boxes_multi_class(pool_boxes,\n                             num_classes,\n                             nms_iou_thresh,\n                             nms_max_detections,\n                             voting_iou_thresh=0.5):\n  """"""Refines a pool of boxes using non max suppression and box voting.\n\n  Box refinement is done independently for each class.\n\n  Args:\n    pool_boxes: (BoxList) A collection of boxes to be refined. pool_boxes must\n      have a rank 1 \'scores\' field and a rank 1 \'classes\' field.\n    num_classes: (int scalar) Number of classes.\n    nms_iou_thresh: (float scalar) iou threshold for non max suppression (NMS).\n    nms_max_detections: (int scalar) maximum output size for NMS.\n    voting_iou_thresh: (float scalar) iou threshold for box voting.\n\n  Returns:\n    BoxList of refined boxes.\n\n  Raises:\n    ValueError: if\n      a) nms_iou_thresh or voting_iou_thresh is not in [0, 1].\n      b) pool_boxes is not a BoxList.\n      c) pool_boxes does not have a scores and classes field.\n  """"""\n  if not 0.0 <= nms_iou_thresh <= 1.0:\n    raise ValueError(\'nms_iou_thresh must be between 0 and 1\')\n  if not 0.0 <= voting_iou_thresh <= 1.0:\n    raise ValueError(\'voting_iou_thresh must be between 0 and 1\')\n  if not isinstance(pool_boxes, box_list.BoxList):\n    raise ValueError(\'pool_boxes must be a BoxList\')\n  if not pool_boxes.has_field(\'scores\'):\n    raise ValueError(\'pool_boxes must have a \\\'scores\\\' field\')\n  if not pool_boxes.has_field(\'classes\'):\n    raise ValueError(\'pool_boxes must have a \\\'classes\\\' field\')\n\n  refined_boxes = []\n  for i in range(num_classes):\n    boxes_class = filter_field_value_equals(pool_boxes, \'classes\', i)\n    refined_boxes_class = refine_boxes(boxes_class, nms_iou_thresh,\n                                       nms_max_detections, voting_iou_thresh)\n    refined_boxes.append(refined_boxes_class)\n  return sort_by_field(concatenate(refined_boxes), \'scores\')\n\n\ndef refine_boxes(pool_boxes,\n                 nms_iou_thresh,\n                 nms_max_detections,\n                 voting_iou_thresh=0.5):\n  """"""Refines a pool of boxes using non max suppression and box voting.\n\n  Args:\n    pool_boxes: (BoxList) A collection of boxes to be refined. pool_boxes must\n      have a rank 1 \'scores\' field.\n    nms_iou_thresh: (float scalar) iou threshold for non max suppression (NMS).\n    nms_max_detections: (int scalar) maximum output size for NMS.\n    voting_iou_thresh: (float scalar) iou threshold for box voting.\n\n  Returns:\n    BoxList of refined boxes.\n\n  Raises:\n    ValueError: if\n      a) nms_iou_thresh or voting_iou_thresh is not in [0, 1].\n      b) pool_boxes is not a BoxList.\n      c) pool_boxes does not have a scores field.\n  """"""\n  if not 0.0 <= nms_iou_thresh <= 1.0:\n    raise ValueError(\'nms_iou_thresh must be between 0 and 1\')\n  if not 0.0 <= voting_iou_thresh <= 1.0:\n    raise ValueError(\'voting_iou_thresh must be between 0 and 1\')\n  if not isinstance(pool_boxes, box_list.BoxList):\n    raise ValueError(\'pool_boxes must be a BoxList\')\n  if not pool_boxes.has_field(\'scores\'):\n    raise ValueError(\'pool_boxes must have a \\\'scores\\\' field\')\n\n  nms_boxes = non_max_suppression(\n      pool_boxes, nms_iou_thresh, nms_max_detections)\n  return box_voting(nms_boxes, pool_boxes, voting_iou_thresh)\n\n\ndef box_voting(selected_boxes, pool_boxes, iou_thresh=0.5):\n  """"""Performs box voting as described in S. Gidaris and N. Komodakis, ICCV 2015.\n\n  Performs box voting as described in \'Object detection via a multi-region &\n  semantic segmentation-aware CNN model\', Gidaris and Komodakis, ICCV 2015. For\n  each box \'B\' in selected_boxes, we find the set \'S\' of boxes in pool_boxes\n  with iou overlap >= iou_thresh. The location of B is set to the weighted\n  average location of boxes in S (scores are used for weighting). And the score\n  of B is set to the average score of boxes in S.\n\n  Args:\n    selected_boxes: BoxList containing a subset of boxes in pool_boxes. These\n      boxes are usually selected from pool_boxes using non max suppression.\n    pool_boxes: BoxList containing a set of (possibly redundant) boxes.\n    iou_thresh: (float scalar) iou threshold for matching boxes in\n      selected_boxes and pool_boxes.\n\n  Returns:\n    BoxList containing averaged locations and scores for each box in\n    selected_boxes.\n\n  Raises:\n    ValueError: if\n      a) selected_boxes or pool_boxes is not a BoxList.\n      b) if iou_thresh is not in [0, 1].\n      c) pool_boxes does not have a scores field.\n  """"""\n  if not 0.0 <= iou_thresh <= 1.0:\n    raise ValueError(\'iou_thresh must be between 0 and 1\')\n  if not isinstance(selected_boxes, box_list.BoxList):\n    raise ValueError(\'selected_boxes must be a BoxList\')\n  if not isinstance(pool_boxes, box_list.BoxList):\n    raise ValueError(\'pool_boxes must be a BoxList\')\n  if not pool_boxes.has_field(\'scores\'):\n    raise ValueError(\'pool_boxes must have a \\\'scores\\\' field\')\n\n  iou_ = iou(selected_boxes, pool_boxes)\n  match_indicator = tf.to_float(tf.greater(iou_, iou_thresh))\n  num_matches = tf.reduce_sum(match_indicator, 1)\n  # TODO(kbanoop): Handle the case where some boxes in selected_boxes do not\n  # match to any boxes in pool_boxes. For such boxes without any matches, we\n  # should return the original boxes without voting.\n  match_assert = tf.Assert(\n      tf.reduce_all(tf.greater(num_matches, 0)),\n      [\'Each box in selected_boxes must match with at least one box \'\n       \'in pool_boxes.\'])\n\n  scores = tf.expand_dims(pool_boxes.get_field(\'scores\'), 1)\n  scores_assert = tf.Assert(\n      tf.reduce_all(tf.greater_equal(scores, 0)),\n      [\'Scores must be non negative.\'])\n\n  with tf.control_dependencies([scores_assert, match_assert]):\n    sum_scores = tf.matmul(match_indicator, scores)\n  averaged_scores = tf.reshape(sum_scores, [-1]) / num_matches\n\n  box_locations = tf.matmul(match_indicator,\n                            pool_boxes.get() * scores) / sum_scores\n  averaged_boxes = box_list.BoxList(box_locations)\n  _copy_extra_fields(averaged_boxes, selected_boxes)\n  averaged_boxes.add_field(\'scores\', averaged_scores)\n  return averaged_boxes\n\n\ndef pad_or_clip_box_list(boxlist, num_boxes, scope=None):\n  """"""Pads or clips all fields of a BoxList.\n\n  Args:\n    boxlist: A BoxList with arbitrary of number of boxes.\n    num_boxes: First num_boxes in boxlist are kept.\n      The fields are zero-padded if num_boxes is bigger than the\n      actual number of boxes.\n    scope: name scope.\n\n  Returns:\n    BoxList with all fields padded or clipped.\n  """"""\n  with tf.name_scope(scope, \'PadOrClipBoxList\'):\n    subboxlist = box_list.BoxList(shape_utils.pad_or_clip_tensor(\n        boxlist.get(), num_boxes))\n    for field in boxlist.get_extra_fields():\n      subfield = shape_utils.pad_or_clip_tensor(\n          boxlist.get_field(field), num_boxes)\n      subboxlist.add_field(field, subfield)\n    return subboxlist\n\n\ndef select_random_box(boxlist,\n                      default_box=None,\n                      seed=None,\n                      scope=None):\n  """"""Selects a random bounding box from a `BoxList`.\n\n  Args:\n    boxlist: A BoxList.\n    default_box: A [1, 4] float32 tensor. If no boxes are present in `boxlist`,\n      this default box will be returned. If None, will use a default box of\n      [[-1., -1., -1., -1.]].\n    seed: Random seed.\n    scope: Name scope.\n\n  Returns:\n    bbox: A [1, 4] tensor with a random bounding box.\n    valid: A bool tensor indicating whether a valid bounding box is returned\n      (True) or whether the default box is returned (False).\n  """"""\n  with tf.name_scope(scope, \'SelectRandomBox\'):\n    bboxes = boxlist.get()\n    combined_shape = shape_utils.combined_static_and_dynamic_shape(bboxes)\n    number_of_boxes = combined_shape[0]\n    default_box = default_box or tf.constant([[-1., -1., -1., -1.]])\n\n    def select_box():\n      random_index = tf.random_uniform([],\n                                       maxval=number_of_boxes,\n                                       dtype=tf.int32,\n                                       seed=seed)\n      return tf.expand_dims(bboxes[random_index], axis=0), tf.constant(True)\n\n  return tf.cond(\n      tf.greater_equal(number_of_boxes, 1),\n      true_fn=select_box,\n      false_fn=lambda: (default_box, tf.constant(False)))\n\n\ndef get_minimal_coverage_box(boxlist,\n                             default_box=None,\n                             scope=None):\n  """"""Creates a single bounding box which covers all boxes in the boxlist.\n\n  Args:\n    boxlist: A Boxlist.\n    default_box: A [1, 4] float32 tensor. If no boxes are present in `boxlist`,\n      this default box will be returned. If None, will use a default box of\n      [[0., 0., 1., 1.]].\n    scope: Name scope.\n\n  Returns:\n    A [1, 4] float32 tensor with a bounding box that tightly covers all the\n    boxes in the box list. If the boxlist does not contain any boxes, the\n    default box is returned.\n  """"""\n  with tf.name_scope(scope, \'CreateCoverageBox\'):\n    num_boxes = boxlist.num_boxes()\n\n    def coverage_box(bboxes):\n      y_min, x_min, y_max, x_max = tf.split(\n          value=bboxes, num_or_size_splits=4, axis=1)\n      y_min_coverage = tf.reduce_min(y_min, axis=0)\n      x_min_coverage = tf.reduce_min(x_min, axis=0)\n      y_max_coverage = tf.reduce_max(y_max, axis=0)\n      x_max_coverage = tf.reduce_max(x_max, axis=0)\n      return tf.stack(\n          [y_min_coverage, x_min_coverage, y_max_coverage, x_max_coverage],\n          axis=1)\n\n    default_box = default_box or tf.constant([[0., 0., 1., 1.]])\n    return tf.cond(\n        tf.greater_equal(num_boxes, 1),\n        true_fn=lambda: coverage_box(boxlist.get()),\n        false_fn=lambda: default_box)\n'"
src/object_detection/core/box_list_ops_test.py,189,"b'# Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n""""""Tests for object_detection.core.box_list_ops.""""""\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow.python.framework import errors\n\nfrom object_detection.core import box_list\nfrom object_detection.core import box_list_ops\n\n\nclass BoxListOpsTest(tf.test.TestCase):\n  """"""Tests for common bounding box operations.""""""\n\n  def test_area(self):\n    corners = tf.constant([[0.0, 0.0, 10.0, 20.0], [1.0, 2.0, 3.0, 4.0]])\n    exp_output = [200.0, 4.0]\n    boxes = box_list.BoxList(corners)\n    areas = box_list_ops.area(boxes)\n    with self.test_session() as sess:\n      areas_output = sess.run(areas)\n      self.assertAllClose(areas_output, exp_output)\n\n  def test_height_width(self):\n    corners = tf.constant([[0.0, 0.0, 10.0, 20.0], [1.0, 2.0, 3.0, 4.0]])\n    exp_output_heights = [10., 2.]\n    exp_output_widths = [20., 2.]\n    boxes = box_list.BoxList(corners)\n    heights, widths = box_list_ops.height_width(boxes)\n    with self.test_session() as sess:\n      output_heights, output_widths = sess.run([heights, widths])\n      self.assertAllClose(output_heights, exp_output_heights)\n      self.assertAllClose(output_widths, exp_output_widths)\n\n  def test_scale(self):\n    corners = tf.constant([[0, 0, 100, 200], [50, 120, 100, 140]],\n                          dtype=tf.float32)\n    boxes = box_list.BoxList(corners)\n    boxes.add_field(\'extra_data\', tf.constant([[1], [2]]))\n\n    y_scale = tf.constant(1.0/100)\n    x_scale = tf.constant(1.0/200)\n    scaled_boxes = box_list_ops.scale(boxes, y_scale, x_scale)\n    exp_output = [[0, 0, 1, 1], [0.5, 0.6, 1.0, 0.7]]\n    with self.test_session() as sess:\n      scaled_corners_out = sess.run(scaled_boxes.get())\n      self.assertAllClose(scaled_corners_out, exp_output)\n      extra_data_out = sess.run(scaled_boxes.get_field(\'extra_data\'))\n      self.assertAllEqual(extra_data_out, [[1], [2]])\n\n  def test_clip_to_window_filter_boxes_which_fall_outside_the_window(\n      self):\n    window = tf.constant([0, 0, 9, 14], tf.float32)\n    corners = tf.constant([[5.0, 5.0, 6.0, 6.0],\n                           [-1.0, -2.0, 4.0, 5.0],\n                           [2.0, 3.0, 5.0, 9.0],\n                           [0.0, 0.0, 9.0, 14.0],\n                           [-100.0, -100.0, 300.0, 600.0],\n                           [-10.0, -10.0, -9.0, -9.0]])\n    boxes = box_list.BoxList(corners)\n    boxes.add_field(\'extra_data\', tf.constant([[1], [2], [3], [4], [5], [6]]))\n    exp_output = [[5.0, 5.0, 6.0, 6.0], [0.0, 0.0, 4.0, 5.0],\n                  [2.0, 3.0, 5.0, 9.0], [0.0, 0.0, 9.0, 14.0],\n                  [0.0, 0.0, 9.0, 14.0]]\n    pruned = box_list_ops.clip_to_window(\n        boxes, window, filter_nonoverlapping=True)\n    with self.test_session() as sess:\n      pruned_output = sess.run(pruned.get())\n      self.assertAllClose(pruned_output, exp_output)\n      extra_data_out = sess.run(pruned.get_field(\'extra_data\'))\n      self.assertAllEqual(extra_data_out, [[1], [2], [3], [4], [5]])\n\n  def test_clip_to_window_without_filtering_boxes_which_fall_outside_the_window(\n      self):\n    window = tf.constant([0, 0, 9, 14], tf.float32)\n    corners = tf.constant([[5.0, 5.0, 6.0, 6.0],\n                           [-1.0, -2.0, 4.0, 5.0],\n                           [2.0, 3.0, 5.0, 9.0],\n                           [0.0, 0.0, 9.0, 14.0],\n                           [-100.0, -100.0, 300.0, 600.0],\n                           [-10.0, -10.0, -9.0, -9.0]])\n    boxes = box_list.BoxList(corners)\n    boxes.add_field(\'extra_data\', tf.constant([[1], [2], [3], [4], [5], [6]]))\n    exp_output = [[5.0, 5.0, 6.0, 6.0], [0.0, 0.0, 4.0, 5.0],\n                  [2.0, 3.0, 5.0, 9.0], [0.0, 0.0, 9.0, 14.0],\n                  [0.0, 0.0, 9.0, 14.0], [0.0, 0.0, 0.0, 0.0]]\n    pruned = box_list_ops.clip_to_window(\n        boxes, window, filter_nonoverlapping=False)\n    with self.test_session() as sess:\n      pruned_output = sess.run(pruned.get())\n      self.assertAllClose(pruned_output, exp_output)\n      extra_data_out = sess.run(pruned.get_field(\'extra_data\'))\n      self.assertAllEqual(extra_data_out, [[1], [2], [3], [4], [5], [6]])\n\n  def test_prune_outside_window_filters_boxes_which_fall_outside_the_window(\n      self):\n    window = tf.constant([0, 0, 9, 14], tf.float32)\n    corners = tf.constant([[5.0, 5.0, 6.0, 6.0],\n                           [-1.0, -2.0, 4.0, 5.0],\n                           [2.0, 3.0, 5.0, 9.0],\n                           [0.0, 0.0, 9.0, 14.0],\n                           [-10.0, -10.0, -9.0, -9.0],\n                           [-100.0, -100.0, 300.0, 600.0]])\n    boxes = box_list.BoxList(corners)\n    boxes.add_field(\'extra_data\', tf.constant([[1], [2], [3], [4], [5], [6]]))\n    exp_output = [[5.0, 5.0, 6.0, 6.0],\n                  [2.0, 3.0, 5.0, 9.0],\n                  [0.0, 0.0, 9.0, 14.0]]\n    pruned, keep_indices = box_list_ops.prune_outside_window(boxes, window)\n    with self.test_session() as sess:\n      pruned_output = sess.run(pruned.get())\n      self.assertAllClose(pruned_output, exp_output)\n      keep_indices_out = sess.run(keep_indices)\n      self.assertAllEqual(keep_indices_out, [0, 2, 3])\n      extra_data_out = sess.run(pruned.get_field(\'extra_data\'))\n      self.assertAllEqual(extra_data_out, [[1], [3], [4]])\n\n  def test_prune_completely_outside_window(self):\n    window = tf.constant([0, 0, 9, 14], tf.float32)\n    corners = tf.constant([[5.0, 5.0, 6.0, 6.0],\n                           [-1.0, -2.0, 4.0, 5.0],\n                           [2.0, 3.0, 5.0, 9.0],\n                           [0.0, 0.0, 9.0, 14.0],\n                           [-10.0, -10.0, -9.0, -9.0],\n                           [-100.0, -100.0, 300.0, 600.0]])\n    boxes = box_list.BoxList(corners)\n    boxes.add_field(\'extra_data\', tf.constant([[1], [2], [3], [4], [5], [6]]))\n    exp_output = [[5.0, 5.0, 6.0, 6.0],\n                  [-1.0, -2.0, 4.0, 5.0],\n                  [2.0, 3.0, 5.0, 9.0],\n                  [0.0, 0.0, 9.0, 14.0],\n                  [-100.0, -100.0, 300.0, 600.0]]\n    pruned, keep_indices = box_list_ops.prune_completely_outside_window(boxes,\n                                                                        window)\n    with self.test_session() as sess:\n      pruned_output = sess.run(pruned.get())\n      self.assertAllClose(pruned_output, exp_output)\n      keep_indices_out = sess.run(keep_indices)\n      self.assertAllEqual(keep_indices_out, [0, 1, 2, 3, 5])\n      extra_data_out = sess.run(pruned.get_field(\'extra_data\'))\n      self.assertAllEqual(extra_data_out, [[1], [2], [3], [4], [6]])\n\n  def test_prune_completely_outside_window_with_empty_boxlist(self):\n    window = tf.constant([0, 0, 9, 14], tf.float32)\n    corners = tf.zeros(shape=[0, 4], dtype=tf.float32)\n    boxes = box_list.BoxList(corners)\n    boxes.add_field(\'extra_data\', tf.zeros(shape=[0], dtype=tf.int32))\n    pruned, keep_indices = box_list_ops.prune_completely_outside_window(boxes,\n                                                                        window)\n    pruned_boxes = pruned.get()\n    extra = pruned.get_field(\'extra_data\')\n\n    exp_pruned_boxes = np.zeros(shape=[0, 4], dtype=np.float32)\n    exp_extra = np.zeros(shape=[0], dtype=np.int32)\n    with self.test_session() as sess:\n      pruned_boxes_out, keep_indices_out, extra_out = sess.run(\n          [pruned_boxes, keep_indices, extra])\n      self.assertAllClose(exp_pruned_boxes, pruned_boxes_out)\n      self.assertAllEqual([], keep_indices_out)\n      self.assertAllEqual(exp_extra, extra_out)\n\n  def test_intersection(self):\n    corners1 = tf.constant([[4.0, 3.0, 7.0, 5.0], [5.0, 6.0, 10.0, 7.0]])\n    corners2 = tf.constant([[3.0, 4.0, 6.0, 8.0], [14.0, 14.0, 15.0, 15.0],\n                            [0.0, 0.0, 20.0, 20.0]])\n    exp_output = [[2.0, 0.0, 6.0], [1.0, 0.0, 5.0]]\n    boxes1 = box_list.BoxList(corners1)\n    boxes2 = box_list.BoxList(corners2)\n    intersect = box_list_ops.intersection(boxes1, boxes2)\n    with self.test_session() as sess:\n      intersect_output = sess.run(intersect)\n      self.assertAllClose(intersect_output, exp_output)\n\n  def test_matched_intersection(self):\n    corners1 = tf.constant([[4.0, 3.0, 7.0, 5.0], [5.0, 6.0, 10.0, 7.0]])\n    corners2 = tf.constant([[3.0, 4.0, 6.0, 8.0], [14.0, 14.0, 15.0, 15.0]])\n    exp_output = [2.0, 0.0]\n    boxes1 = box_list.BoxList(corners1)\n    boxes2 = box_list.BoxList(corners2)\n    intersect = box_list_ops.matched_intersection(boxes1, boxes2)\n    with self.test_session() as sess:\n      intersect_output = sess.run(intersect)\n      self.assertAllClose(intersect_output, exp_output)\n\n  def test_iou(self):\n    corners1 = tf.constant([[4.0, 3.0, 7.0, 5.0], [5.0, 6.0, 10.0, 7.0]])\n    corners2 = tf.constant([[3.0, 4.0, 6.0, 8.0], [14.0, 14.0, 15.0, 15.0],\n                            [0.0, 0.0, 20.0, 20.0]])\n    exp_output = [[2.0 / 16.0, 0, 6.0 / 400.0], [1.0 / 16.0, 0.0, 5.0 / 400.0]]\n    boxes1 = box_list.BoxList(corners1)\n    boxes2 = box_list.BoxList(corners2)\n    iou = box_list_ops.iou(boxes1, boxes2)\n    with self.test_session() as sess:\n      iou_output = sess.run(iou)\n      self.assertAllClose(iou_output, exp_output)\n\n  def test_matched_iou(self):\n    corners1 = tf.constant([[4.0, 3.0, 7.0, 5.0], [5.0, 6.0, 10.0, 7.0]])\n    corners2 = tf.constant([[3.0, 4.0, 6.0, 8.0], [14.0, 14.0, 15.0, 15.0]])\n    exp_output = [2.0 / 16.0, 0]\n    boxes1 = box_list.BoxList(corners1)\n    boxes2 = box_list.BoxList(corners2)\n    iou = box_list_ops.matched_iou(boxes1, boxes2)\n    with self.test_session() as sess:\n      iou_output = sess.run(iou)\n      self.assertAllClose(iou_output, exp_output)\n\n  def test_iouworks_on_empty_inputs(self):\n    corners1 = tf.constant([[4.0, 3.0, 7.0, 5.0], [5.0, 6.0, 10.0, 7.0]])\n    corners2 = tf.constant([[3.0, 4.0, 6.0, 8.0], [14.0, 14.0, 15.0, 15.0],\n                            [0.0, 0.0, 20.0, 20.0]])\n    boxes1 = box_list.BoxList(corners1)\n    boxes2 = box_list.BoxList(corners2)\n    boxes_empty = box_list.BoxList(tf.zeros((0, 4)))\n    iou_empty_1 = box_list_ops.iou(boxes1, boxes_empty)\n    iou_empty_2 = box_list_ops.iou(boxes_empty, boxes2)\n    iou_empty_3 = box_list_ops.iou(boxes_empty, boxes_empty)\n    with self.test_session() as sess:\n      iou_output_1, iou_output_2, iou_output_3 = sess.run(\n          [iou_empty_1, iou_empty_2, iou_empty_3])\n      self.assertAllEqual(iou_output_1.shape, (2, 0))\n      self.assertAllEqual(iou_output_2.shape, (0, 3))\n      self.assertAllEqual(iou_output_3.shape, (0, 0))\n\n  def test_ioa(self):\n    corners1 = tf.constant([[4.0, 3.0, 7.0, 5.0], [5.0, 6.0, 10.0, 7.0]])\n    corners2 = tf.constant([[3.0, 4.0, 6.0, 8.0], [14.0, 14.0, 15.0, 15.0],\n                            [0.0, 0.0, 20.0, 20.0]])\n    exp_output_1 = [[2.0 / 12.0, 0, 6.0 / 400.0],\n                    [1.0 / 12.0, 0.0, 5.0 / 400.0]]\n    exp_output_2 = [[2.0 / 6.0, 1.0 / 5.0],\n                    [0, 0],\n                    [6.0 / 6.0, 5.0 / 5.0]]\n    boxes1 = box_list.BoxList(corners1)\n    boxes2 = box_list.BoxList(corners2)\n    ioa_1 = box_list_ops.ioa(boxes1, boxes2)\n    ioa_2 = box_list_ops.ioa(boxes2, boxes1)\n    with self.test_session() as sess:\n      ioa_output_1, ioa_output_2 = sess.run([ioa_1, ioa_2])\n      self.assertAllClose(ioa_output_1, exp_output_1)\n      self.assertAllClose(ioa_output_2, exp_output_2)\n\n  def test_prune_non_overlapping_boxes(self):\n    corners1 = tf.constant([[4.0, 3.0, 7.0, 5.0], [5.0, 6.0, 10.0, 7.0]])\n    corners2 = tf.constant([[3.0, 4.0, 6.0, 8.0], [14.0, 14.0, 15.0, 15.0],\n                            [0.0, 0.0, 20.0, 20.0]])\n    boxes1 = box_list.BoxList(corners1)\n    boxes2 = box_list.BoxList(corners2)\n    minoverlap = 0.5\n\n    exp_output_1 = boxes1\n    exp_output_2 = box_list.BoxList(tf.constant(0.0, shape=[0, 4]))\n    output_1, keep_indices_1 = box_list_ops.prune_non_overlapping_boxes(\n        boxes1, boxes2, min_overlap=minoverlap)\n    output_2, keep_indices_2 = box_list_ops.prune_non_overlapping_boxes(\n        boxes2, boxes1, min_overlap=minoverlap)\n    with self.test_session() as sess:\n      (output_1_, keep_indices_1_, output_2_, keep_indices_2_, exp_output_1_,\n       exp_output_2_) = sess.run(\n           [output_1.get(), keep_indices_1,\n            output_2.get(), keep_indices_2,\n            exp_output_1.get(), exp_output_2.get()])\n      self.assertAllClose(output_1_, exp_output_1_)\n      self.assertAllClose(output_2_, exp_output_2_)\n      self.assertAllEqual(keep_indices_1_, [0, 1])\n      self.assertAllEqual(keep_indices_2_, [])\n\n  def test_prune_small_boxes(self):\n    boxes = tf.constant([[4.0, 3.0, 7.0, 5.0],\n                         [5.0, 6.0, 10.0, 7.0],\n                         [3.0, 4.0, 6.0, 8.0],\n                         [14.0, 14.0, 15.0, 15.0],\n                         [0.0, 0.0, 20.0, 20.0]])\n    exp_boxes = [[3.0, 4.0, 6.0, 8.0],\n                 [0.0, 0.0, 20.0, 20.0]]\n    boxes = box_list.BoxList(boxes)\n    pruned_boxes = box_list_ops.prune_small_boxes(boxes, 3)\n    with self.test_session() as sess:\n      pruned_boxes = sess.run(pruned_boxes.get())\n      self.assertAllEqual(pruned_boxes, exp_boxes)\n\n  def test_prune_small_boxes_prunes_boxes_with_negative_side(self):\n    boxes = tf.constant([[4.0, 3.0, 7.0, 5.0],\n                         [5.0, 6.0, 10.0, 7.0],\n                         [3.0, 4.0, 6.0, 8.0],\n                         [14.0, 14.0, 15.0, 15.0],\n                         [0.0, 0.0, 20.0, 20.0],\n                         [2.0, 3.0, 1.5, 7.0],  # negative height\n                         [2.0, 3.0, 5.0, 1.7]])  # negative width\n    exp_boxes = [[3.0, 4.0, 6.0, 8.0],\n                 [0.0, 0.0, 20.0, 20.0]]\n    boxes = box_list.BoxList(boxes)\n    pruned_boxes = box_list_ops.prune_small_boxes(boxes, 3)\n    with self.test_session() as sess:\n      pruned_boxes = sess.run(pruned_boxes.get())\n      self.assertAllEqual(pruned_boxes, exp_boxes)\n\n  def test_change_coordinate_frame(self):\n    corners = tf.constant([[0.25, 0.5, 0.75, 0.75], [0.5, 0.0, 1.0, 1.0]])\n    window = tf.constant([0.25, 0.25, 0.75, 0.75])\n    boxes = box_list.BoxList(corners)\n\n    expected_corners = tf.constant([[0, 0.5, 1.0, 1.0], [0.5, -0.5, 1.5, 1.5]])\n    expected_boxes = box_list.BoxList(expected_corners)\n    output = box_list_ops.change_coordinate_frame(boxes, window)\n\n    with self.test_session() as sess:\n      output_, expected_boxes_ = sess.run([output.get(), expected_boxes.get()])\n      self.assertAllClose(output_, expected_boxes_)\n\n  def test_ioaworks_on_empty_inputs(self):\n    corners1 = tf.constant([[4.0, 3.0, 7.0, 5.0], [5.0, 6.0, 10.0, 7.0]])\n    corners2 = tf.constant([[3.0, 4.0, 6.0, 8.0], [14.0, 14.0, 15.0, 15.0],\n                            [0.0, 0.0, 20.0, 20.0]])\n    boxes1 = box_list.BoxList(corners1)\n    boxes2 = box_list.BoxList(corners2)\n    boxes_empty = box_list.BoxList(tf.zeros((0, 4)))\n    ioa_empty_1 = box_list_ops.ioa(boxes1, boxes_empty)\n    ioa_empty_2 = box_list_ops.ioa(boxes_empty, boxes2)\n    ioa_empty_3 = box_list_ops.ioa(boxes_empty, boxes_empty)\n    with self.test_session() as sess:\n      ioa_output_1, ioa_output_2, ioa_output_3 = sess.run(\n          [ioa_empty_1, ioa_empty_2, ioa_empty_3])\n      self.assertAllEqual(ioa_output_1.shape, (2, 0))\n      self.assertAllEqual(ioa_output_2.shape, (0, 3))\n      self.assertAllEqual(ioa_output_3.shape, (0, 0))\n\n  def test_pairwise_distances(self):\n    corners1 = tf.constant([[0.0, 0.0, 0.0, 0.0],\n                            [1.0, 1.0, 0.0, 2.0]])\n    corners2 = tf.constant([[3.0, 4.0, 1.0, 0.0],\n                            [-4.0, 0.0, 0.0, 3.0],\n                            [0.0, 0.0, 0.0, 0.0]])\n    exp_output = [[26, 25, 0], [18, 27, 6]]\n    boxes1 = box_list.BoxList(corners1)\n    boxes2 = box_list.BoxList(corners2)\n    dist_matrix = box_list_ops.sq_dist(boxes1, boxes2)\n    with self.test_session() as sess:\n      dist_output = sess.run(dist_matrix)\n      self.assertAllClose(dist_output, exp_output)\n\n  def test_boolean_mask(self):\n    corners = tf.constant(\n        [4 * [0.0], 4 * [1.0], 4 * [2.0], 4 * [3.0], 4 * [4.0]])\n    indicator = tf.constant([True, False, True, False, True], tf.bool)\n    expected_subset = [4 * [0.0], 4 * [2.0], 4 * [4.0]]\n    boxes = box_list.BoxList(corners)\n    subset = box_list_ops.boolean_mask(boxes, indicator)\n    with self.test_session() as sess:\n      subset_output = sess.run(subset.get())\n      self.assertAllClose(subset_output, expected_subset)\n\n  def test_boolean_mask_with_field(self):\n    corners = tf.constant(\n        [4 * [0.0], 4 * [1.0], 4 * [2.0], 4 * [3.0], 4 * [4.0]])\n    indicator = tf.constant([True, False, True, False, True], tf.bool)\n    weights = tf.constant([[.1], [.3], [.5], [.7], [.9]], tf.float32)\n    expected_subset = [4 * [0.0], 4 * [2.0], 4 * [4.0]]\n    expected_weights = [[.1], [.5], [.9]]\n\n    boxes = box_list.BoxList(corners)\n    boxes.add_field(\'weights\', weights)\n    subset = box_list_ops.boolean_mask(boxes, indicator, [\'weights\'])\n    with self.test_session() as sess:\n      subset_output, weights_output = sess.run(\n          [subset.get(), subset.get_field(\'weights\')])\n      self.assertAllClose(subset_output, expected_subset)\n      self.assertAllClose(weights_output, expected_weights)\n\n  def test_gather(self):\n    corners = tf.constant(\n        [4 * [0.0], 4 * [1.0], 4 * [2.0], 4 * [3.0], 4 * [4.0]])\n    indices = tf.constant([0, 2, 4], tf.int32)\n    expected_subset = [4 * [0.0], 4 * [2.0], 4 * [4.0]]\n    boxes = box_list.BoxList(corners)\n    subset = box_list_ops.gather(boxes, indices)\n    with self.test_session() as sess:\n      subset_output = sess.run(subset.get())\n      self.assertAllClose(subset_output, expected_subset)\n\n  def test_gather_with_field(self):\n    corners = tf.constant([4*[0.0], 4*[1.0], 4*[2.0], 4*[3.0], 4*[4.0]])\n    indices = tf.constant([0, 2, 4], tf.int32)\n    weights = tf.constant([[.1], [.3], [.5], [.7], [.9]], tf.float32)\n    expected_subset = [4 * [0.0], 4 * [2.0], 4 * [4.0]]\n    expected_weights = [[.1], [.5], [.9]]\n\n    boxes = box_list.BoxList(corners)\n    boxes.add_field(\'weights\', weights)\n    subset = box_list_ops.gather(boxes, indices, [\'weights\'])\n    with self.test_session() as sess:\n      subset_output, weights_output = sess.run(\n          [subset.get(), subset.get_field(\'weights\')])\n      self.assertAllClose(subset_output, expected_subset)\n      self.assertAllClose(weights_output, expected_weights)\n\n  def test_gather_with_invalid_field(self):\n    corners = tf.constant([4 * [0.0], 4 * [1.0]])\n    indices = tf.constant([0, 1], tf.int32)\n    weights = tf.constant([[.1], [.3]], tf.float32)\n\n    boxes = box_list.BoxList(corners)\n    boxes.add_field(\'weights\', weights)\n    with self.assertRaises(ValueError):\n      box_list_ops.gather(boxes, indices, [\'foo\', \'bar\'])\n\n  def test_gather_with_invalid_inputs(self):\n    corners = tf.constant(\n        [4 * [0.0], 4 * [1.0], 4 * [2.0], 4 * [3.0], 4 * [4.0]])\n    indices_float32 = tf.constant([0, 2, 4], tf.float32)\n    boxes = box_list.BoxList(corners)\n    with self.assertRaises(ValueError):\n      _ = box_list_ops.gather(boxes, indices_float32)\n    indices_2d = tf.constant([[0, 2, 4]], tf.int32)\n    boxes = box_list.BoxList(corners)\n    with self.assertRaises(ValueError):\n      _ = box_list_ops.gather(boxes, indices_2d)\n\n  def test_gather_with_dynamic_indexing(self):\n    corners = tf.constant([4 * [0.0], 4 * [1.0], 4 * [2.0], 4 * [3.0], 4 * [4.0]\n                          ])\n    weights = tf.constant([.5, .3, .7, .1, .9], tf.float32)\n    indices = tf.reshape(tf.where(tf.greater(weights, 0.4)), [-1])\n    expected_subset = [4 * [0.0], 4 * [2.0], 4 * [4.0]]\n    expected_weights = [.5, .7, .9]\n\n    boxes = box_list.BoxList(corners)\n    boxes.add_field(\'weights\', weights)\n    subset = box_list_ops.gather(boxes, indices, [\'weights\'])\n    with self.test_session() as sess:\n      subset_output, weights_output = sess.run([subset.get(), subset.get_field(\n          \'weights\')])\n      self.assertAllClose(subset_output, expected_subset)\n      self.assertAllClose(weights_output, expected_weights)\n\n  def test_sort_by_field_ascending_order(self):\n    exp_corners = [[0, 0, 1, 1], [0, 0.1, 1, 1.1], [0, -0.1, 1, 0.9],\n                   [0, 10, 1, 11], [0, 10.1, 1, 11.1], [0, 100, 1, 101]]\n    exp_scores = [.95, .9, .75, .6, .5, .3]\n    exp_weights = [.2, .45, .6, .75, .8, .92]\n    shuffle = [2, 4, 0, 5, 1, 3]\n    corners = tf.constant([exp_corners[i] for i in shuffle], tf.float32)\n    boxes = box_list.BoxList(corners)\n    boxes.add_field(\'scores\', tf.constant(\n        [exp_scores[i] for i in shuffle], tf.float32))\n    boxes.add_field(\'weights\', tf.constant(\n        [exp_weights[i] for i in shuffle], tf.float32))\n    sort_by_weight = box_list_ops.sort_by_field(\n        boxes,\n        \'weights\',\n        order=box_list_ops.SortOrder.ascend)\n    with self.test_session() as sess:\n      corners_out, scores_out, weights_out = sess.run([\n          sort_by_weight.get(),\n          sort_by_weight.get_field(\'scores\'),\n          sort_by_weight.get_field(\'weights\')])\n      self.assertAllClose(corners_out, exp_corners)\n      self.assertAllClose(scores_out, exp_scores)\n      self.assertAllClose(weights_out, exp_weights)\n\n  def test_sort_by_field_descending_order(self):\n    exp_corners = [[0, 0, 1, 1], [0, 0.1, 1, 1.1], [0, -0.1, 1, 0.9],\n                   [0, 10, 1, 11], [0, 10.1, 1, 11.1], [0, 100, 1, 101]]\n    exp_scores = [.95, .9, .75, .6, .5, .3]\n    exp_weights = [.2, .45, .6, .75, .8, .92]\n    shuffle = [2, 4, 0, 5, 1, 3]\n\n    corners = tf.constant([exp_corners[i] for i in shuffle], tf.float32)\n    boxes = box_list.BoxList(corners)\n    boxes.add_field(\'scores\', tf.constant(\n        [exp_scores[i] for i in shuffle], tf.float32))\n    boxes.add_field(\'weights\', tf.constant(\n        [exp_weights[i] for i in shuffle], tf.float32))\n\n    sort_by_score = box_list_ops.sort_by_field(boxes, \'scores\')\n    with self.test_session() as sess:\n      corners_out, scores_out, weights_out = sess.run([sort_by_score.get(\n      ), sort_by_score.get_field(\'scores\'), sort_by_score.get_field(\'weights\')])\n      self.assertAllClose(corners_out, exp_corners)\n      self.assertAllClose(scores_out, exp_scores)\n      self.assertAllClose(weights_out, exp_weights)\n\n  def test_sort_by_field_invalid_inputs(self):\n    corners = tf.constant([4 * [0.0], 4 * [0.5], 4 * [1.0], 4 * [2.0], 4 *\n                           [3.0], 4 * [4.0]])\n    misc = tf.constant([[.95, .9], [.5, .3]], tf.float32)\n    weights = tf.constant([.1, .2], tf.float32)\n    boxes = box_list.BoxList(corners)\n    boxes.add_field(\'misc\', misc)\n    boxes.add_field(\'weights\', weights)\n\n    with self.test_session() as sess:\n      with self.assertRaises(ValueError):\n        box_list_ops.sort_by_field(boxes, \'area\')\n\n      with self.assertRaises(ValueError):\n        box_list_ops.sort_by_field(boxes, \'misc\')\n\n      with self.assertRaisesWithPredicateMatch(errors.InvalidArgumentError,\n                                               \'Incorrect field size\'):\n        sess.run(box_list_ops.sort_by_field(boxes, \'weights\').get())\n\n  def test_visualize_boxes_in_image(self):\n    image = tf.zeros((6, 4, 3))\n    corners = tf.constant([[0, 0, 5, 3],\n                           [0, 0, 3, 2]], tf.float32)\n    boxes = box_list.BoxList(corners)\n    image_and_boxes = box_list_ops.visualize_boxes_in_image(image, boxes)\n    image_and_boxes_bw = tf.to_float(\n        tf.greater(tf.reduce_sum(image_and_boxes, 2), 0.0))\n    exp_result = [[1, 1, 1, 0],\n                  [1, 1, 1, 0],\n                  [1, 1, 1, 0],\n                  [1, 0, 1, 0],\n                  [1, 1, 1, 0],\n                  [0, 0, 0, 0]]\n    with self.test_session() as sess:\n      output = sess.run(image_and_boxes_bw)\n      self.assertAllEqual(output.astype(int), exp_result)\n\n  def test_filter_field_value_equals(self):\n    corners = tf.constant([[0, 0, 1, 1],\n                           [0, 0.1, 1, 1.1],\n                           [0, -0.1, 1, 0.9],\n                           [0, 10, 1, 11],\n                           [0, 10.1, 1, 11.1],\n                           [0, 100, 1, 101]], tf.float32)\n    boxes = box_list.BoxList(corners)\n    boxes.add_field(\'classes\', tf.constant([1, 2, 1, 2, 2, 1]))\n    exp_output1 = [[0, 0, 1, 1], [0, -0.1, 1, 0.9], [0, 100, 1, 101]]\n    exp_output2 = [[0, 0.1, 1, 1.1], [0, 10, 1, 11], [0, 10.1, 1, 11.1]]\n\n    filtered_boxes1 = box_list_ops.filter_field_value_equals(\n        boxes, \'classes\', 1)\n    filtered_boxes2 = box_list_ops.filter_field_value_equals(\n        boxes, \'classes\', 2)\n    with self.test_session() as sess:\n      filtered_output1, filtered_output2 = sess.run([filtered_boxes1.get(),\n                                                     filtered_boxes2.get()])\n      self.assertAllClose(filtered_output1, exp_output1)\n      self.assertAllClose(filtered_output2, exp_output2)\n\n  def test_filter_greater_than(self):\n    corners = tf.constant([[0, 0, 1, 1],\n                           [0, 0.1, 1, 1.1],\n                           [0, -0.1, 1, 0.9],\n                           [0, 10, 1, 11],\n                           [0, 10.1, 1, 11.1],\n                           [0, 100, 1, 101]], tf.float32)\n    boxes = box_list.BoxList(corners)\n    boxes.add_field(\'scores\', tf.constant([.1, .75, .9, .5, .5, .8]))\n    thresh = .6\n    exp_output = [[0, 0.1, 1, 1.1], [0, -0.1, 1, 0.9], [0, 100, 1, 101]]\n\n    filtered_boxes = box_list_ops.filter_greater_than(boxes, thresh)\n    with self.test_session() as sess:\n      filtered_output = sess.run(filtered_boxes.get())\n      self.assertAllClose(filtered_output, exp_output)\n\n  def test_clip_box_list(self):\n    boxlist = box_list.BoxList(\n        tf.constant([[0.1, 0.1, 0.4, 0.4], [0.1, 0.1, 0.5, 0.5],\n                     [0.6, 0.6, 0.8, 0.8], [0.2, 0.2, 0.3, 0.3]], tf.float32))\n    boxlist.add_field(\'classes\', tf.constant([0, 0, 1, 1]))\n    boxlist.add_field(\'scores\', tf.constant([0.75, 0.65, 0.3, 0.2]))\n    num_boxes = 2\n    clipped_boxlist = box_list_ops.pad_or_clip_box_list(boxlist, num_boxes)\n\n    expected_boxes = [[0.1, 0.1, 0.4, 0.4], [0.1, 0.1, 0.5, 0.5]]\n    expected_classes = [0, 0]\n    expected_scores = [0.75, 0.65]\n    with self.test_session() as sess:\n      boxes_out, classes_out, scores_out = sess.run(\n          [clipped_boxlist.get(), clipped_boxlist.get_field(\'classes\'),\n           clipped_boxlist.get_field(\'scores\')])\n\n      self.assertAllClose(expected_boxes, boxes_out)\n      self.assertAllEqual(expected_classes, classes_out)\n      self.assertAllClose(expected_scores, scores_out)\n\n  def test_pad_box_list(self):\n    boxlist = box_list.BoxList(\n        tf.constant([[0.1, 0.1, 0.4, 0.4], [0.1, 0.1, 0.5, 0.5]], tf.float32))\n    boxlist.add_field(\'classes\', tf.constant([0, 1]))\n    boxlist.add_field(\'scores\', tf.constant([0.75, 0.2]))\n    num_boxes = 4\n    padded_boxlist = box_list_ops.pad_or_clip_box_list(boxlist, num_boxes)\n\n    expected_boxes = [[0.1, 0.1, 0.4, 0.4], [0.1, 0.1, 0.5, 0.5],\n                      [0, 0, 0, 0], [0, 0, 0, 0]]\n    expected_classes = [0, 1, 0, 0]\n    expected_scores = [0.75, 0.2, 0, 0]\n    with self.test_session() as sess:\n      boxes_out, classes_out, scores_out = sess.run(\n          [padded_boxlist.get(), padded_boxlist.get_field(\'classes\'),\n           padded_boxlist.get_field(\'scores\')])\n\n      self.assertAllClose(expected_boxes, boxes_out)\n      self.assertAllEqual(expected_classes, classes_out)\n      self.assertAllClose(expected_scores, scores_out)\n\n  def test_select_random_box(self):\n    boxes = [[0., 0., 1., 1.],\n             [0., 1., 2., 3.],\n             [0., 2., 3., 4.]]\n\n    corners = tf.constant(boxes, dtype=tf.float32)\n    boxlist = box_list.BoxList(corners)\n    random_bbox, valid = box_list_ops.select_random_box(boxlist)\n    with self.test_session() as sess:\n      random_bbox_out, valid_out = sess.run([random_bbox, valid])\n\n    norm_small = any(\n        [np.linalg.norm(random_bbox_out - box) < 1e-6 for box in boxes])\n\n    self.assertTrue(norm_small)\n    self.assertTrue(valid_out)\n\n  def test_select_random_box_with_empty_boxlist(self):\n    corners = tf.constant([], shape=[0, 4], dtype=tf.float32)\n    boxlist = box_list.BoxList(corners)\n    random_bbox, valid = box_list_ops.select_random_box(boxlist)\n    with self.test_session() as sess:\n      random_bbox_out, valid_out = sess.run([random_bbox, valid])\n\n    expected_bbox_out = np.array([[-1., -1., -1., -1.]], dtype=np.float32)\n    self.assertAllEqual(expected_bbox_out, random_bbox_out)\n    self.assertFalse(valid_out)\n\n  def test_get_minimal_coverage_box(self):\n    boxes = [[0., 0., 1., 1.],\n             [-1., 1., 2., 3.],\n             [0., 2., 3., 4.]]\n\n    expected_coverage_box = [[-1., 0., 3., 4.]]\n\n    corners = tf.constant(boxes, dtype=tf.float32)\n    boxlist = box_list.BoxList(corners)\n    coverage_box = box_list_ops.get_minimal_coverage_box(boxlist)\n    with self.test_session() as sess:\n      coverage_box_out = sess.run(coverage_box)\n\n    self.assertAllClose(expected_coverage_box, coverage_box_out)\n\n  def test_get_minimal_coverage_box_with_empty_boxlist(self):\n    corners = tf.constant([], shape=[0, 4], dtype=tf.float32)\n    boxlist = box_list.BoxList(corners)\n    coverage_box = box_list_ops.get_minimal_coverage_box(boxlist)\n    with self.test_session() as sess:\n      coverage_box_out = sess.run(coverage_box)\n\n    self.assertAllClose([[0.0, 0.0, 1.0, 1.0]], coverage_box_out)\n\n\nclass ConcatenateTest(tf.test.TestCase):\n\n  def test_invalid_input_box_list_list(self):\n    with self.assertRaises(ValueError):\n      box_list_ops.concatenate(None)\n    with self.assertRaises(ValueError):\n      box_list_ops.concatenate([])\n    with self.assertRaises(ValueError):\n      corners = tf.constant([[0, 0, 0, 0]], tf.float32)\n      boxlist = box_list.BoxList(corners)\n      box_list_ops.concatenate([boxlist, 2])\n\n  def test_concatenate_with_missing_fields(self):\n    corners1 = tf.constant([[0, 0, 0, 0], [1, 2, 3, 4]], tf.float32)\n    scores1 = tf.constant([1.0, 2.1])\n    corners2 = tf.constant([[0, 3, 1, 6], [2, 4, 3, 8]], tf.float32)\n    boxlist1 = box_list.BoxList(corners1)\n    boxlist1.add_field(\'scores\', scores1)\n    boxlist2 = box_list.BoxList(corners2)\n    with self.assertRaises(ValueError):\n      box_list_ops.concatenate([boxlist1, boxlist2])\n\n  def test_concatenate_with_incompatible_field_shapes(self):\n    corners1 = tf.constant([[0, 0, 0, 0], [1, 2, 3, 4]], tf.float32)\n    scores1 = tf.constant([1.0, 2.1])\n    corners2 = tf.constant([[0, 3, 1, 6], [2, 4, 3, 8]], tf.float32)\n    scores2 = tf.constant([[1.0, 1.0], [2.1, 3.2]])\n    boxlist1 = box_list.BoxList(corners1)\n    boxlist1.add_field(\'scores\', scores1)\n    boxlist2 = box_list.BoxList(corners2)\n    boxlist2.add_field(\'scores\', scores2)\n    with self.assertRaises(ValueError):\n      box_list_ops.concatenate([boxlist1, boxlist2])\n\n  def test_concatenate_is_correct(self):\n    corners1 = tf.constant([[0, 0, 0, 0], [1, 2, 3, 4]], tf.float32)\n    scores1 = tf.constant([1.0, 2.1])\n    corners2 = tf.constant([[0, 3, 1, 6], [2, 4, 3, 8], [1, 0, 5, 10]],\n                           tf.float32)\n    scores2 = tf.constant([1.0, 2.1, 5.6])\n\n    exp_corners = [[0, 0, 0, 0],\n                   [1, 2, 3, 4],\n                   [0, 3, 1, 6],\n                   [2, 4, 3, 8],\n                   [1, 0, 5, 10]]\n    exp_scores = [1.0, 2.1, 1.0, 2.1, 5.6]\n\n    boxlist1 = box_list.BoxList(corners1)\n    boxlist1.add_field(\'scores\', scores1)\n    boxlist2 = box_list.BoxList(corners2)\n    boxlist2.add_field(\'scores\', scores2)\n    result = box_list_ops.concatenate([boxlist1, boxlist2])\n    with self.test_session() as sess:\n      corners_output, scores_output = sess.run(\n          [result.get(), result.get_field(\'scores\')])\n      self.assertAllClose(corners_output, exp_corners)\n      self.assertAllClose(scores_output, exp_scores)\n\n\nclass NonMaxSuppressionTest(tf.test.TestCase):\n\n  def test_select_from_three_clusters(self):\n    corners = tf.constant([[0, 0, 1, 1],\n                           [0, 0.1, 1, 1.1],\n                           [0, -0.1, 1, 0.9],\n                           [0, 10, 1, 11],\n                           [0, 10.1, 1, 11.1],\n                           [0, 100, 1, 101]], tf.float32)\n    boxes = box_list.BoxList(corners)\n    boxes.add_field(\'scores\', tf.constant([.9, .75, .6, .95, .5, .3]))\n    iou_thresh = .5\n    max_output_size = 3\n\n    exp_nms = [[0, 10, 1, 11],\n               [0, 0, 1, 1],\n               [0, 100, 1, 101]]\n    nms = box_list_ops.non_max_suppression(\n        boxes, iou_thresh, max_output_size)\n    with self.test_session() as sess:\n      nms_output = sess.run(nms.get())\n      self.assertAllClose(nms_output, exp_nms)\n\n  def test_select_at_most_two_boxes_from_three_clusters(self):\n    corners = tf.constant([[0, 0, 1, 1],\n                           [0, 0.1, 1, 1.1],\n                           [0, -0.1, 1, 0.9],\n                           [0, 10, 1, 11],\n                           [0, 10.1, 1, 11.1],\n                           [0, 100, 1, 101]], tf.float32)\n    boxes = box_list.BoxList(corners)\n    boxes.add_field(\'scores\', tf.constant([.9, .75, .6, .95, .5, .3]))\n    iou_thresh = .5\n    max_output_size = 2\n\n    exp_nms = [[0, 10, 1, 11],\n               [0, 0, 1, 1]]\n    nms = box_list_ops.non_max_suppression(\n        boxes, iou_thresh, max_output_size)\n    with self.test_session() as sess:\n      nms_output = sess.run(nms.get())\n      self.assertAllClose(nms_output, exp_nms)\n\n  def test_select_at_most_thirty_boxes_from_three_clusters(self):\n    corners = tf.constant([[0, 0, 1, 1],\n                           [0, 0.1, 1, 1.1],\n                           [0, -0.1, 1, 0.9],\n                           [0, 10, 1, 11],\n                           [0, 10.1, 1, 11.1],\n                           [0, 100, 1, 101]], tf.float32)\n    boxes = box_list.BoxList(corners)\n    boxes.add_field(\'scores\', tf.constant([.9, .75, .6, .95, .5, .3]))\n    iou_thresh = .5\n    max_output_size = 30\n\n    exp_nms = [[0, 10, 1, 11],\n               [0, 0, 1, 1],\n               [0, 100, 1, 101]]\n    nms = box_list_ops.non_max_suppression(\n        boxes, iou_thresh, max_output_size)\n    with self.test_session() as sess:\n      nms_output = sess.run(nms.get())\n      self.assertAllClose(nms_output, exp_nms)\n\n  def test_select_single_box(self):\n    corners = tf.constant([[0, 0, 1, 1]], tf.float32)\n    boxes = box_list.BoxList(corners)\n    boxes.add_field(\'scores\', tf.constant([.9]))\n    iou_thresh = .5\n    max_output_size = 3\n\n    exp_nms = [[0, 0, 1, 1]]\n    nms = box_list_ops.non_max_suppression(\n        boxes, iou_thresh, max_output_size)\n    with self.test_session() as sess:\n      nms_output = sess.run(nms.get())\n      self.assertAllClose(nms_output, exp_nms)\n\n  def test_select_from_ten_identical_boxes(self):\n    corners = tf.constant(10 * [[0, 0, 1, 1]], tf.float32)\n    boxes = box_list.BoxList(corners)\n    boxes.add_field(\'scores\', tf.constant(10 * [.9]))\n    iou_thresh = .5\n    max_output_size = 3\n\n    exp_nms = [[0, 0, 1, 1]]\n    nms = box_list_ops.non_max_suppression(\n        boxes, iou_thresh, max_output_size)\n    with self.test_session() as sess:\n      nms_output = sess.run(nms.get())\n      self.assertAllClose(nms_output, exp_nms)\n\n  def test_copy_extra_fields(self):\n    corners = tf.constant([[0, 0, 1, 1],\n                           [0, 0.1, 1, 1.1]], tf.float32)\n    boxes = box_list.BoxList(corners)\n    tensor1 = np.array([[1], [4]])\n    tensor2 = np.array([[1, 1], [2, 2]])\n    boxes.add_field(\'tensor1\', tf.constant(tensor1))\n    boxes.add_field(\'tensor2\', tf.constant(tensor2))\n    new_boxes = box_list.BoxList(tf.constant([[0, 0, 10, 10],\n                                              [1, 3, 5, 5]], tf.float32))\n    new_boxes = box_list_ops._copy_extra_fields(new_boxes, boxes)\n    with self.test_session() as sess:\n      self.assertAllClose(tensor1, sess.run(new_boxes.get_field(\'tensor1\')))\n      self.assertAllClose(tensor2, sess.run(new_boxes.get_field(\'tensor2\')))\n\n\nclass CoordinatesConversionTest(tf.test.TestCase):\n\n  def test_to_normalized_coordinates(self):\n    coordinates = tf.constant([[0, 0, 100, 100],\n                               [25, 25, 75, 75]], tf.float32)\n    img = tf.ones((128, 100, 100, 3))\n    boxlist = box_list.BoxList(coordinates)\n    normalized_boxlist = box_list_ops.to_normalized_coordinates(\n        boxlist, tf.shape(img)[1], tf.shape(img)[2])\n    expected_boxes = [[0, 0, 1, 1],\n                      [0.25, 0.25, 0.75, 0.75]]\n\n    with self.test_session() as sess:\n      normalized_boxes = sess.run(normalized_boxlist.get())\n      self.assertAllClose(normalized_boxes, expected_boxes)\n\n  def test_to_normalized_coordinates_already_normalized(self):\n    coordinates = tf.constant([[0, 0, 1, 1],\n                               [0.25, 0.25, 0.75, 0.75]], tf.float32)\n    img = tf.ones((128, 100, 100, 3))\n    boxlist = box_list.BoxList(coordinates)\n    normalized_boxlist = box_list_ops.to_normalized_coordinates(\n        boxlist, tf.shape(img)[1], tf.shape(img)[2])\n\n    with self.test_session() as sess:\n      with self.assertRaisesOpError(\'assertion failed\'):\n        sess.run(normalized_boxlist.get())\n\n  def test_to_absolute_coordinates(self):\n    coordinates = tf.constant([[0, 0, 1, 1],\n                               [0.25, 0.25, 0.75, 0.75]], tf.float32)\n    img = tf.ones((128, 100, 100, 3))\n    boxlist = box_list.BoxList(coordinates)\n    absolute_boxlist = box_list_ops.to_absolute_coordinates(boxlist,\n                                                            tf.shape(img)[1],\n                                                            tf.shape(img)[2])\n    expected_boxes = [[0, 0, 100, 100],\n                      [25, 25, 75, 75]]\n\n    with self.test_session() as sess:\n      absolute_boxes = sess.run(absolute_boxlist.get())\n      self.assertAllClose(absolute_boxes, expected_boxes)\n\n  def test_to_absolute_coordinates_already_abolute(self):\n    coordinates = tf.constant([[0, 0, 100, 100],\n                               [25, 25, 75, 75]], tf.float32)\n    img = tf.ones((128, 100, 100, 3))\n    boxlist = box_list.BoxList(coordinates)\n    absolute_boxlist = box_list_ops.to_absolute_coordinates(boxlist,\n                                                            tf.shape(img)[1],\n                                                            tf.shape(img)[2])\n\n    with self.test_session() as sess:\n      with self.assertRaisesOpError(\'assertion failed\'):\n        sess.run(absolute_boxlist.get())\n\n  def test_convert_to_normalized_and_back(self):\n    coordinates = np.random.uniform(size=(100, 4))\n    coordinates = np.round(np.sort(coordinates) * 200)\n    coordinates[:, 2:4] += 1\n    coordinates[99, :] = [0, 0, 201, 201]\n    img = tf.ones((128, 202, 202, 3))\n\n    boxlist = box_list.BoxList(tf.constant(coordinates, tf.float32))\n    boxlist = box_list_ops.to_normalized_coordinates(boxlist,\n                                                     tf.shape(img)[1],\n                                                     tf.shape(img)[2])\n    boxlist = box_list_ops.to_absolute_coordinates(boxlist,\n                                                   tf.shape(img)[1],\n                                                   tf.shape(img)[2])\n\n    with self.test_session() as sess:\n      out = sess.run(boxlist.get())\n      self.assertAllClose(out, coordinates)\n\n  def test_convert_to_absolute_and_back(self):\n    coordinates = np.random.uniform(size=(100, 4))\n    coordinates = np.sort(coordinates)\n    coordinates[99, :] = [0, 0, 1, 1]\n    img = tf.ones((128, 202, 202, 3))\n\n    boxlist = box_list.BoxList(tf.constant(coordinates, tf.float32))\n    boxlist = box_list_ops.to_absolute_coordinates(boxlist,\n                                                   tf.shape(img)[1],\n                                                   tf.shape(img)[2])\n    boxlist = box_list_ops.to_normalized_coordinates(boxlist,\n                                                     tf.shape(img)[1],\n                                                     tf.shape(img)[2])\n\n    with self.test_session() as sess:\n      out = sess.run(boxlist.get())\n      self.assertAllClose(out, coordinates)\n\n\nclass BoxRefinementTest(tf.test.TestCase):\n\n  def test_box_voting(self):\n    candidates = box_list.BoxList(\n        tf.constant([[0.1, 0.1, 0.4, 0.4], [0.6, 0.6, 0.8, 0.8]], tf.float32))\n    candidates.add_field(\'ExtraField\', tf.constant([1, 2]))\n    pool = box_list.BoxList(\n        tf.constant([[0.1, 0.1, 0.4, 0.4], [0.1, 0.1, 0.5, 0.5],\n                     [0.6, 0.6, 0.8, 0.8]], tf.float32))\n    pool.add_field(\'scores\', tf.constant([0.75, 0.25, 0.3]))\n    averaged_boxes = box_list_ops.box_voting(candidates, pool)\n    expected_boxes = [[0.1, 0.1, 0.425, 0.425], [0.6, 0.6, 0.8, 0.8]]\n    expected_scores = [0.5, 0.3]\n    with self.test_session() as sess:\n      boxes_out, scores_out, extra_field_out = sess.run(\n          [averaged_boxes.get(), averaged_boxes.get_field(\'scores\'),\n           averaged_boxes.get_field(\'ExtraField\')])\n\n      self.assertAllClose(expected_boxes, boxes_out)\n      self.assertAllClose(expected_scores, scores_out)\n      self.assertAllEqual(extra_field_out, [1, 2])\n\n  def test_box_voting_fails_with_negative_scores(self):\n    candidates = box_list.BoxList(\n        tf.constant([[0.1, 0.1, 0.4, 0.4]], tf.float32))\n    pool = box_list.BoxList(tf.constant([[0.1, 0.1, 0.4, 0.4]], tf.float32))\n    pool.add_field(\'scores\', tf.constant([-0.2]))\n    averaged_boxes = box_list_ops.box_voting(candidates, pool)\n\n    with self.test_session() as sess:\n      with self.assertRaisesOpError(\'Scores must be non negative\'):\n        sess.run([averaged_boxes.get()])\n\n  def test_box_voting_fails_when_unmatched(self):\n    candidates = box_list.BoxList(\n        tf.constant([[0.1, 0.1, 0.4, 0.4]], tf.float32))\n    pool = box_list.BoxList(tf.constant([[0.6, 0.6, 0.8, 0.8]], tf.float32))\n    pool.add_field(\'scores\', tf.constant([0.2]))\n    averaged_boxes = box_list_ops.box_voting(candidates, pool)\n\n    with self.test_session() as sess:\n      with self.assertRaisesOpError(\'Each box in selected_boxes must match \'\n                                    \'with at least one box in pool_boxes.\'):\n        sess.run([averaged_boxes.get()])\n\n  def test_refine_boxes(self):\n    pool = box_list.BoxList(\n        tf.constant([[0.1, 0.1, 0.4, 0.4], [0.1, 0.1, 0.5, 0.5],\n                     [0.6, 0.6, 0.8, 0.8]], tf.float32))\n    pool.add_field(\'ExtraField\', tf.constant([1, 2, 3]))\n    pool.add_field(\'scores\', tf.constant([0.75, 0.25, 0.3]))\n    refined_boxes = box_list_ops.refine_boxes(pool, 0.5, 10)\n\n    expected_boxes = [[0.1, 0.1, 0.425, 0.425], [0.6, 0.6, 0.8, 0.8]]\n    expected_scores = [0.5, 0.3]\n    with self.test_session() as sess:\n      boxes_out, scores_out, extra_field_out = sess.run(\n          [refined_boxes.get(), refined_boxes.get_field(\'scores\'),\n           refined_boxes.get_field(\'ExtraField\')])\n\n      self.assertAllClose(expected_boxes, boxes_out)\n      self.assertAllClose(expected_scores, scores_out)\n      self.assertAllEqual(extra_field_out, [1, 3])\n\n  def test_refine_boxes_multi_class(self):\n    pool = box_list.BoxList(\n        tf.constant([[0.1, 0.1, 0.4, 0.4], [0.1, 0.1, 0.5, 0.5],\n                     [0.6, 0.6, 0.8, 0.8], [0.2, 0.2, 0.3, 0.3]], tf.float32))\n    pool.add_field(\'classes\', tf.constant([0, 0, 1, 1]))\n    pool.add_field(\'scores\', tf.constant([0.75, 0.25, 0.3, 0.2]))\n    refined_boxes = box_list_ops.refine_boxes_multi_class(pool, 3, 0.5, 10)\n\n    expected_boxes = [[0.1, 0.1, 0.425, 0.425], [0.6, 0.6, 0.8, 0.8],\n                      [0.2, 0.2, 0.3, 0.3]]\n    expected_scores = [0.5, 0.3, 0.2]\n    with self.test_session() as sess:\n      boxes_out, scores_out, extra_field_out = sess.run(\n          [refined_boxes.get(), refined_boxes.get_field(\'scores\'),\n           refined_boxes.get_field(\'classes\')])\n\n      self.assertAllClose(expected_boxes, boxes_out)\n      self.assertAllClose(expected_scores, scores_out)\n      self.assertAllEqual(extra_field_out, [0, 1, 1])\n\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
src/object_detection/core/box_list_test.py,22,"b'# Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n""""""Tests for object_detection.core.box_list.""""""\n\nimport tensorflow as tf\n\nfrom object_detection.core import box_list\n\n\nclass BoxListTest(tf.test.TestCase):\n  """"""Tests for BoxList class.""""""\n\n  def test_num_boxes(self):\n    data = tf.constant([[0, 0, 1, 1], [1, 1, 2, 3], [3, 4, 5, 5]], tf.float32)\n    expected_num_boxes = 3\n\n    boxes = box_list.BoxList(data)\n    with self.test_session() as sess:\n      num_boxes_output = sess.run(boxes.num_boxes())\n      self.assertEquals(num_boxes_output, expected_num_boxes)\n\n  def test_get_correct_center_coordinates_and_sizes(self):\n    boxes = [[10.0, 10.0, 20.0, 15.0], [0.2, 0.1, 0.5, 0.4]]\n    boxes = box_list.BoxList(tf.constant(boxes))\n    centers_sizes = boxes.get_center_coordinates_and_sizes()\n    expected_centers_sizes = [[15, 0.35], [12.5, 0.25], [10, 0.3], [5, 0.3]]\n    with self.test_session() as sess:\n      centers_sizes_out = sess.run(centers_sizes)\n      self.assertAllClose(centers_sizes_out, expected_centers_sizes)\n\n  def test_create_box_list_with_dynamic_shape(self):\n    data = tf.constant([[0, 0, 1, 1], [1, 1, 2, 3], [3, 4, 5, 5]], tf.float32)\n    indices = tf.reshape(tf.where(tf.greater([1, 0, 1], 0)), [-1])\n    data = tf.gather(data, indices)\n    assert data.get_shape().as_list() == [None, 4]\n    expected_num_boxes = 2\n\n    boxes = box_list.BoxList(data)\n    with self.test_session() as sess:\n      num_boxes_output = sess.run(boxes.num_boxes())\n      self.assertEquals(num_boxes_output, expected_num_boxes)\n\n  def test_transpose_coordinates(self):\n    boxes = [[10.0, 10.0, 20.0, 15.0], [0.2, 0.1, 0.5, 0.4]]\n    boxes = box_list.BoxList(tf.constant(boxes))\n    boxes.transpose_coordinates()\n    expected_corners = [[10.0, 10.0, 15.0, 20.0], [0.1, 0.2, 0.4, 0.5]]\n    with self.test_session() as sess:\n      corners_out = sess.run(boxes.get())\n      self.assertAllClose(corners_out, expected_corners)\n\n  def test_box_list_invalid_inputs(self):\n    data0 = tf.constant([[[0, 0, 1, 1], [3, 4, 5, 5]]], tf.float32)\n    data1 = tf.constant([[0, 0, 1], [1, 1, 2], [3, 4, 5]], tf.float32)\n    data2 = tf.constant([[0, 0, 1], [1, 1, 2], [3, 4, 5]], tf.int32)\n\n    with self.assertRaises(ValueError):\n      _ = box_list.BoxList(data0)\n    with self.assertRaises(ValueError):\n      _ = box_list.BoxList(data1)\n    with self.assertRaises(ValueError):\n      _ = box_list.BoxList(data2)\n\n  def test_num_boxes_static(self):\n    box_corners = [[10.0, 10.0, 20.0, 15.0], [0.2, 0.1, 0.5, 0.4]]\n    boxes = box_list.BoxList(tf.constant(box_corners))\n    self.assertEquals(boxes.num_boxes_static(), 2)\n    self.assertEquals(type(boxes.num_boxes_static()), int)\n\n  def test_num_boxes_static_for_uninferrable_shape(self):\n    placeholder = tf.placeholder(tf.float32, shape=[None, 4])\n    boxes = box_list.BoxList(placeholder)\n    self.assertEquals(boxes.num_boxes_static(), None)\n\n  def test_as_tensor_dict(self):\n    boxlist = box_list.BoxList(\n        tf.constant([[0.1, 0.1, 0.4, 0.4], [0.1, 0.1, 0.5, 0.5]], tf.float32))\n    boxlist.add_field(\'classes\', tf.constant([0, 1]))\n    boxlist.add_field(\'scores\', tf.constant([0.75, 0.2]))\n    tensor_dict = boxlist.as_tensor_dict()\n\n    expected_boxes = [[0.1, 0.1, 0.4, 0.4], [0.1, 0.1, 0.5, 0.5]]\n    expected_classes = [0, 1]\n    expected_scores = [0.75, 0.2]\n\n    with self.test_session() as sess:\n      tensor_dict_out = sess.run(tensor_dict)\n      self.assertAllEqual(3, len(tensor_dict_out))\n      self.assertAllClose(expected_boxes, tensor_dict_out[\'boxes\'])\n      self.assertAllEqual(expected_classes, tensor_dict_out[\'classes\'])\n      self.assertAllClose(expected_scores, tensor_dict_out[\'scores\'])\n\n  def test_as_tensor_dict_with_features(self):\n    boxlist = box_list.BoxList(\n        tf.constant([[0.1, 0.1, 0.4, 0.4], [0.1, 0.1, 0.5, 0.5]], tf.float32))\n    boxlist.add_field(\'classes\', tf.constant([0, 1]))\n    boxlist.add_field(\'scores\', tf.constant([0.75, 0.2]))\n    tensor_dict = boxlist.as_tensor_dict([\'boxes\', \'classes\', \'scores\'])\n\n    expected_boxes = [[0.1, 0.1, 0.4, 0.4], [0.1, 0.1, 0.5, 0.5]]\n    expected_classes = [0, 1]\n    expected_scores = [0.75, 0.2]\n\n    with self.test_session() as sess:\n      tensor_dict_out = sess.run(tensor_dict)\n      self.assertAllEqual(3, len(tensor_dict_out))\n      self.assertAllClose(expected_boxes, tensor_dict_out[\'boxes\'])\n      self.assertAllEqual(expected_classes, tensor_dict_out[\'classes\'])\n      self.assertAllClose(expected_scores, tensor_dict_out[\'scores\'])\n\n  def test_as_tensor_dict_missing_field(self):\n    boxlist = box_list.BoxList(\n        tf.constant([[0.1, 0.1, 0.4, 0.4], [0.1, 0.1, 0.5, 0.5]], tf.float32))\n    boxlist.add_field(\'classes\', tf.constant([0, 1]))\n    boxlist.add_field(\'scores\', tf.constant([0.75, 0.2]))\n    with self.assertRaises(ValueError):\n      boxlist.as_tensor_dict([\'foo\', \'bar\'])\n\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
src/object_detection/core/box_predictor.py,36,"b'# Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n""""""Box predictor for object detectors.\n\nBox predictors are classes that take a high level\nimage feature map as input and produce two predictions,\n(1) a tensor encoding box locations, and\n(2) a tensor encoding classes for each box.\n\nThese components are passed directly to loss functions\nin our detection models.\n\nThese modules are separated from the main model since the same\nfew box predictor architectures are shared across many models.\n""""""\nfrom abc import abstractmethod\nimport math\nimport tensorflow as tf\nfrom object_detection.utils import ops\nfrom object_detection.utils import shape_utils\nfrom object_detection.utils import static_shape\n\nslim = tf.contrib.slim\n\nBOX_ENCODINGS = \'box_encodings\'\nCLASS_PREDICTIONS_WITH_BACKGROUND = \'class_predictions_with_background\'\nMASK_PREDICTIONS = \'mask_predictions\'\n\n\nclass BoxPredictor(object):\n  """"""BoxPredictor.""""""\n\n  def __init__(self, is_training, num_classes):\n    """"""Constructor.\n\n    Args:\n      is_training: Indicates whether the BoxPredictor is in training mode.\n      num_classes: number of classes.  Note that num_classes *does not*\n        include the background category, so if groundtruth labels take values\n        in {0, 1, .., K-1}, num_classes=K (and not K+1, even though the\n        assigned classification targets can range from {0,... K}).\n    """"""\n    self._is_training = is_training\n    self._num_classes = num_classes\n\n  @property\n  def num_classes(self):\n    return self._num_classes\n\n  def predict(self, image_features, num_predictions_per_location,\n              scope=None, **params):\n    """"""Computes encoded object locations and corresponding confidences.\n\n    Takes a list of high level image feature maps as input and produces a list\n    of box encodings and a list of class scores where each element in the output\n    lists correspond to the feature maps in the input list.\n\n    Args:\n      image_features: A list of float tensors of shape [batch_size, height_i,\n      width_i, channels_i] containing features for a batch of images.\n      num_predictions_per_location: A list of integers representing the number\n        of box predictions to be made per spatial location for each feature map.\n      scope: Variable and Op scope name.\n      **params: Additional keyword arguments for specific implementations of\n              BoxPredictor.\n\n    Returns:\n      A dictionary containing at least the following tensors.\n        box_encodings: A list of float tensors of shape\n          [batch_size, num_anchors_i, q, code_size] representing the location of\n          the objects, where q is 1 or the number of classes. Each entry in the\n          list corresponds to a feature map in the input `image_features` list.\n        class_predictions_with_background: A list of float tensors of shape\n          [batch_size, num_anchors_i, num_classes + 1] representing the class\n          predictions for the proposals. Each entry in the list corresponds to a\n          feature map in the input `image_features` list.\n\n    Raises:\n      ValueError: If length of `image_features` is not equal to length of\n        `num_predictions_per_location`.\n    """"""\n    if len(image_features) != len(num_predictions_per_location):\n      raise ValueError(\'image_feature and num_predictions_per_location must \'\n                       \'be of same length, found: {} vs {}\'.\n                       format(len(image_features),\n                              len(num_predictions_per_location)))\n    if scope is not None:\n      with tf.variable_scope(scope):\n        return self._predict(image_features, num_predictions_per_location,\n                             **params)\n    return self._predict(image_features, num_predictions_per_location,\n                         **params)\n\n  # TODO(rathodv): num_predictions_per_location could be moved to constructor.\n  # This is currently only used by ConvolutionalBoxPredictor.\n  @abstractmethod\n  def _predict(self, image_features, num_predictions_per_location, **params):\n    """"""Implementations must override this method.\n\n    Args:\n      image_features: A list of float tensors of shape [batch_size, height_i,\n        width_i, channels_i] containing features for a batch of images.\n      num_predictions_per_location: A list of integers representing the number\n        of box predictions to be made per spatial location for each feature map.\n      **params: Additional keyword arguments for specific implementations of\n              BoxPredictor.\n\n    Returns:\n      A dictionary containing at least the following tensors.\n        box_encodings: A list of float tensors of shape\n          [batch_size, num_anchors_i, q, code_size] representing the location of\n          the objects, where q is 1 or the number of classes. Each entry in the\n          list corresponds to a feature map in the input `image_features` list.\n        class_predictions_with_background: A list of float tensors of shape\n          [batch_size, num_anchors_i, num_classes + 1] representing the class\n          predictions for the proposals. Each entry in the list corresponds to a\n          feature map in the input `image_features` list.\n    """"""\n    pass\n\n\nclass RfcnBoxPredictor(BoxPredictor):\n  """"""RFCN Box Predictor.\n\n  Applies a position sensitive ROI pooling on position sensitive feature maps to\n  predict classes and refined locations. See https://arxiv.org/abs/1605.06409\n  for details.\n\n  This is used for the second stage of the RFCN meta architecture. Notice that\n  locations are *not* shared across classes, thus for each anchor, a separate\n  prediction is made for each class.\n  """"""\n\n  def __init__(self,\n               is_training,\n               num_classes,\n               conv_hyperparams,\n               num_spatial_bins,\n               depth,\n               crop_size,\n               box_code_size):\n    """"""Constructor.\n\n    Args:\n      is_training: Indicates whether the BoxPredictor is in training mode.\n      num_classes: number of classes.  Note that num_classes *does not*\n        include the background category, so if groundtruth labels take values\n        in {0, 1, .., K-1}, num_classes=K (and not K+1, even though the\n        assigned classification targets can range from {0,... K}).\n      conv_hyperparams: Slim arg_scope with hyperparameters for conolutional\n        layers.\n      num_spatial_bins: A list of two integers `[spatial_bins_y,\n        spatial_bins_x]`.\n      depth: Target depth to reduce the input feature maps to.\n      crop_size: A list of two integers `[crop_height, crop_width]`.\n      box_code_size: Size of encoding for each box.\n    """"""\n    super(RfcnBoxPredictor, self).__init__(is_training, num_classes)\n    self._conv_hyperparams = conv_hyperparams\n    self._num_spatial_bins = num_spatial_bins\n    self._depth = depth\n    self._crop_size = crop_size\n    self._box_code_size = box_code_size\n\n  @property\n  def num_classes(self):\n    return self._num_classes\n\n  def _predict(self, image_features, num_predictions_per_location,\n               proposal_boxes):\n    """"""Computes encoded object locations and corresponding confidences.\n\n    Args:\n      image_features: A list of float tensors of shape [batch_size, height_i,\n      width_i, channels_i] containing features for a batch of images.\n      num_predictions_per_location: A list of integers representing the number\n        of box predictions to be made per spatial location for each feature map.\n        Currently, this must be set to [1], or an error will be raised.\n      proposal_boxes: A float tensor of shape [batch_size, num_proposals,\n        box_code_size].\n\n    Returns:\n      box_encodings: A list of float tensors of shape\n        [batch_size, num_anchors_i, q, code_size] representing the location of\n        the objects, where q is 1 or the number of classes. Each entry in the\n        list corresponds to a feature map in the input `image_features` list.\n      class_predictions_with_background: A list of float tensors of shape\n        [batch_size, num_anchors_i, num_classes + 1] representing the class\n        predictions for the proposals. Each entry in the list corresponds to a\n        feature map in the input `image_features` list.\n\n    Raises:\n      ValueError: if num_predictions_per_location is not 1 or if\n        len(image_features) is not 1.\n    """"""\n    if (len(num_predictions_per_location) != 1 or\n        num_predictions_per_location[0] != 1):\n      raise ValueError(\'Currently RfcnBoxPredictor only supports \'\n                       \'predicting a single box per class per location.\')\n    if len(image_features) != 1:\n      raise ValueError(\'length of `image_features` must be 1. Found {}\'.\n                       format(len(image_features)))\n    image_feature = image_features[0]\n    num_predictions_per_location = num_predictions_per_location[0]\n    batch_size = tf.shape(proposal_boxes)[0]\n    num_boxes = tf.shape(proposal_boxes)[1]\n    def get_box_indices(proposals):\n      proposals_shape = proposals.get_shape().as_list()\n      if any(dim is None for dim in proposals_shape):\n        proposals_shape = tf.shape(proposals)\n      ones_mat = tf.ones(proposals_shape[:2], dtype=tf.int32)\n      multiplier = tf.expand_dims(\n          tf.range(start=0, limit=proposals_shape[0]), 1)\n      return tf.reshape(ones_mat * multiplier, [-1])\n\n    net = image_feature\n    with slim.arg_scope(self._conv_hyperparams):\n      net = slim.conv2d(net, self._depth, [1, 1], scope=\'reduce_depth\')\n      # Location predictions.\n      location_feature_map_depth = (self._num_spatial_bins[0] *\n                                    self._num_spatial_bins[1] *\n                                    self.num_classes *\n                                    self._box_code_size)\n      location_feature_map = slim.conv2d(net, location_feature_map_depth,\n                                         [1, 1], activation_fn=None,\n                                         scope=\'refined_locations\')\n      box_encodings = ops.position_sensitive_crop_regions(\n          location_feature_map,\n          boxes=tf.reshape(proposal_boxes, [-1, self._box_code_size]),\n          box_ind=get_box_indices(proposal_boxes),\n          crop_size=self._crop_size,\n          num_spatial_bins=self._num_spatial_bins,\n          global_pool=True)\n      box_encodings = tf.squeeze(box_encodings, squeeze_dims=[1, 2])\n      box_encodings = tf.reshape(box_encodings,\n                                 [batch_size * num_boxes, 1, self.num_classes,\n                                  self._box_code_size])\n\n      # Class predictions.\n      total_classes = self.num_classes + 1  # Account for background class.\n      class_feature_map_depth = (self._num_spatial_bins[0] *\n                                 self._num_spatial_bins[1] *\n                                 total_classes)\n      class_feature_map = slim.conv2d(net, class_feature_map_depth, [1, 1],\n                                      activation_fn=None,\n                                      scope=\'class_predictions\')\n      class_predictions_with_background = ops.position_sensitive_crop_regions(\n          class_feature_map,\n          boxes=tf.reshape(proposal_boxes, [-1, self._box_code_size]),\n          box_ind=get_box_indices(proposal_boxes),\n          crop_size=self._crop_size,\n          num_spatial_bins=self._num_spatial_bins,\n          global_pool=True)\n      class_predictions_with_background = tf.squeeze(\n          class_predictions_with_background, squeeze_dims=[1, 2])\n      class_predictions_with_background = tf.reshape(\n          class_predictions_with_background,\n          [batch_size * num_boxes, 1, total_classes])\n\n    return {BOX_ENCODINGS: [box_encodings],\n            CLASS_PREDICTIONS_WITH_BACKGROUND:\n            [class_predictions_with_background]}\n\n\n# TODO(rathodv): Change the implementation to return lists of predictions.\nclass MaskRCNNBoxPredictor(BoxPredictor):\n  """"""Mask R-CNN Box Predictor.\n\n  See Mask R-CNN: He, K., Gkioxari, G., Dollar, P., & Girshick, R. (2017).\n  Mask R-CNN. arXiv preprint arXiv:1703.06870.\n\n  This is used for the second stage of the Mask R-CNN detector where proposals\n  cropped from an image are arranged along the batch dimension of the input\n  image_features tensor. Notice that locations are *not* shared across classes,\n  thus for each anchor, a separate prediction is made for each class.\n\n  In addition to predicting boxes and classes, optionally this class allows\n  predicting masks and/or keypoints inside detection boxes.\n\n  Currently this box predictor makes per-class predictions; that is, each\n  anchor makes a separate box prediction for each class.\n  """"""\n\n  def __init__(self,\n               is_training,\n               num_classes,\n               fc_hyperparams,\n               use_dropout,\n               dropout_keep_prob,\n               box_code_size,\n               conv_hyperparams=None,\n               predict_instance_masks=False,\n               mask_height=14,\n               mask_width=14,\n               mask_prediction_num_conv_layers=2,\n               mask_prediction_conv_depth=256,\n               predict_keypoints=False):\n    """"""Constructor.\n\n    Args:\n      is_training: Indicates whether the BoxPredictor is in training mode.\n      num_classes: number of classes.  Note that num_classes *does not*\n        include the background category, so if groundtruth labels take values\n        in {0, 1, .., K-1}, num_classes=K (and not K+1, even though the\n        assigned classification targets can range from {0,... K}).\n      fc_hyperparams: Slim arg_scope with hyperparameters for fully\n        connected ops.\n      use_dropout: Option to use dropout or not.  Note that a single dropout\n        op is applied here prior to both box and class predictions, which stands\n        in contrast to the ConvolutionalBoxPredictor below.\n      dropout_keep_prob: Keep probability for dropout.\n        This is only used if use_dropout is True.\n      box_code_size: Size of encoding for each box.\n      conv_hyperparams: Slim arg_scope with hyperparameters for convolution\n        ops.\n      predict_instance_masks: Whether to predict object masks inside detection\n        boxes.\n      mask_height: Desired output mask height. The default value is 14.\n      mask_width: Desired output mask width. The default value is 14.\n      mask_prediction_num_conv_layers: Number of convolution layers applied to\n        the image_features in mask prediction branch.\n      mask_prediction_conv_depth: The depth for the first conv2d_transpose op\n        applied to the image_features in the mask prediction branch. If set\n        to 0, the depth of the convolution layers will be automatically chosen\n        based on the number of object classes and the number of channels in the\n        image features.\n      predict_keypoints: Whether to predict keypoints insde detection boxes.\n\n\n    Raises:\n      ValueError: If predict_instance_masks is true but conv_hyperparams is not\n        set.\n      ValueError: If predict_keypoints is true since it is not implemented yet.\n      ValueError: If mask_prediction_num_conv_layers is smaller than two.\n    """"""\n    super(MaskRCNNBoxPredictor, self).__init__(is_training, num_classes)\n    self._fc_hyperparams = fc_hyperparams\n    self._use_dropout = use_dropout\n    self._box_code_size = box_code_size\n    self._dropout_keep_prob = dropout_keep_prob\n    self._conv_hyperparams = conv_hyperparams\n    self._predict_instance_masks = predict_instance_masks\n    self._mask_height = mask_height\n    self._mask_width = mask_width\n    self._mask_prediction_num_conv_layers = mask_prediction_num_conv_layers\n    self._mask_prediction_conv_depth = mask_prediction_conv_depth\n    self._predict_keypoints = predict_keypoints\n    if self._predict_keypoints:\n      raise ValueError(\'Keypoint prediction is unimplemented.\')\n    if ((self._predict_instance_masks or self._predict_keypoints) and\n        self._conv_hyperparams is None):\n      raise ValueError(\'`conv_hyperparams` must be provided when predicting \'\n                       \'masks.\')\n    if self._mask_prediction_num_conv_layers < 2:\n      raise ValueError(\n          \'Mask prediction should consist of at least 2 conv layers\')\n\n  @property\n  def num_classes(self):\n    return self._num_classes\n\n  @property\n  def predicts_instance_masks(self):\n    return self._predict_instance_masks\n\n  def _predict_boxes_and_classes(self, image_features):\n    """"""Predicts boxes and class scores.\n\n    Args:\n      image_features: A float tensor of shape [batch_size, height, width,\n        channels] containing features for a batch of images.\n\n    Returns:\n      box_encodings: A float tensor of shape\n        [batch_size, 1, num_classes, code_size] representing the location of the\n        objects.\n      class_predictions_with_background: A float tensor of shape\n        [batch_size, 1, num_classes + 1] representing the class predictions for\n        the proposals.\n    """"""\n    spatial_averaged_image_features = tf.reduce_mean(image_features, [1, 2],\n                                                     keep_dims=True,\n                                                     name=\'AvgPool\')\n    flattened_image_features = slim.flatten(spatial_averaged_image_features)\n    if self._use_dropout:\n      flattened_image_features = slim.dropout(flattened_image_features,\n                                              keep_prob=self._dropout_keep_prob,\n                                              is_training=self._is_training)\n    with slim.arg_scope(self._fc_hyperparams):\n      box_encodings = slim.fully_connected(\n          flattened_image_features,\n          self._num_classes * self._box_code_size,\n          activation_fn=None,\n          scope=\'BoxEncodingPredictor\')\n      class_predictions_with_background = slim.fully_connected(\n          flattened_image_features,\n          self._num_classes + 1,\n          activation_fn=None,\n          scope=\'ClassPredictor\')\n    box_encodings = tf.reshape(\n        box_encodings, [-1, 1, self._num_classes, self._box_code_size])\n    class_predictions_with_background = tf.reshape(\n        class_predictions_with_background, [-1, 1, self._num_classes + 1])\n    return box_encodings, class_predictions_with_background\n\n  def _get_mask_predictor_conv_depth(self, num_feature_channels, num_classes,\n                                     class_weight=3.0, feature_weight=2.0):\n    """"""Computes the depth of the mask predictor convolutions.\n\n    Computes the depth of the mask predictor convolutions given feature channels\n    and number of classes by performing a weighted average of the two in\n    log space to compute the number of convolution channels. The weights that\n    are used for computing the weighted average do not need to sum to 1.\n\n    Args:\n      num_feature_channels: An integer containing the number of feature\n        channels.\n      num_classes: An integer containing the number of classes.\n      class_weight: Class weight used in computing the weighted average.\n      feature_weight: Feature weight used in computing the weighted average.\n\n    Returns:\n      An integer containing the number of convolution channels used by mask\n        predictor.\n    """"""\n    num_feature_channels_log = math.log(float(num_feature_channels), 2.0)\n    num_classes_log = math.log(float(num_classes), 2.0)\n    weighted_num_feature_channels_log = (\n        num_feature_channels_log * feature_weight)\n    weighted_num_classes_log = num_classes_log * class_weight\n    total_weight = feature_weight + class_weight\n    num_conv_channels_log = round(\n        (weighted_num_feature_channels_log + weighted_num_classes_log) /\n        total_weight)\n    return int(math.pow(2.0, num_conv_channels_log))\n\n  def _predict_masks(self, image_features):\n    """"""Performs mask prediction.\n\n    Args:\n      image_features: A float tensor of shape [batch_size, height, width,\n        channels] containing features for a batch of images.\n\n    Returns:\n      instance_masks: A float tensor of shape\n          [batch_size, 1, num_classes, image_height, image_width].\n    """"""\n    num_conv_channels = self._mask_prediction_conv_depth\n    if num_conv_channels == 0:\n      num_feature_channels = image_features.get_shape().as_list()[3]\n      num_conv_channels = self._get_mask_predictor_conv_depth(\n          num_feature_channels, self.num_classes)\n    with slim.arg_scope(self._conv_hyperparams):\n      upsampled_features = tf.image.resize_bilinear(\n          image_features,\n          [self._mask_height, self._mask_width],\n          align_corners=True)\n      for _ in range(self._mask_prediction_num_conv_layers - 1):\n        upsampled_features = slim.conv2d(\n            upsampled_features,\n            num_outputs=num_conv_channels,\n            kernel_size=[3, 3])\n      mask_predictions = slim.conv2d(upsampled_features,\n                                     num_outputs=self.num_classes,\n                                     activation_fn=None,\n                                     kernel_size=[3, 3])\n      return tf.expand_dims(\n          tf.transpose(mask_predictions, perm=[0, 3, 1, 2]),\n          axis=1,\n          name=\'MaskPredictor\')\n\n  def _predict(self, image_features, num_predictions_per_location,\n               predict_boxes_and_classes=True, predict_auxiliary_outputs=False):\n    """"""Optionally computes encoded object locations, confidences, and masks.\n\n    Flattens image_features and applies fully connected ops (with no\n    non-linearity) to predict box encodings and class predictions.  In this\n    setting, anchors are not spatially arranged in any way and are assumed to\n    have been folded into the batch dimension.  Thus we output 1 for the\n    anchors dimension.\n\n    Also optionally predicts instance masks.\n    The mask prediction head is based on the Mask RCNN paper with the following\n    modifications: We replace the deconvolution layer with a bilinear resize\n    and a convolution.\n\n    Args:\n      image_features: A list of float tensors of shape [batch_size, height_i,\n        width_i, channels_i] containing features for a batch of images.\n      num_predictions_per_location: A list of integers representing the number\n        of box predictions to be made per spatial location for each feature map.\n        Currently, this must be set to [1], or an error will be raised.\n      predict_boxes_and_classes: If true, the function will perform box\n        refinement and classification.\n      predict_auxiliary_outputs: If true, the function will perform other\n        predictions such as mask, keypoint, boundaries, etc. if any.\n\n    Returns:\n      A dictionary containing the following tensors.\n        box_encodings: A float tensor of shape\n          [batch_size, 1, num_classes, code_size] representing the\n          location of the objects.\n        class_predictions_with_background: A float tensor of shape\n          [batch_size, 1, num_classes + 1] representing the class\n          predictions for the proposals.\n      If predict_masks is True the dictionary also contains:\n        instance_masks: A float tensor of shape\n          [batch_size, 1, num_classes, image_height, image_width]\n      If predict_keypoints is True the dictionary also contains:\n        keypoints: [batch_size, 1, num_keypoints, 2]\n\n    Raises:\n      ValueError: If num_predictions_per_location is not 1 or if both\n        predict_boxes_and_classes and predict_auxiliary_outputs are false or if\n        len(image_features) is not 1.\n    """"""\n    if (len(num_predictions_per_location) != 1 or\n        num_predictions_per_location[0] != 1):\n      raise ValueError(\'Currently FullyConnectedBoxPredictor only supports \'\n                       \'predicting a single box per class per location.\')\n    if not predict_boxes_and_classes and not predict_auxiliary_outputs:\n      raise ValueError(\'Should perform at least one prediction.\')\n    if len(image_features) != 1:\n      raise ValueError(\'length of `image_features` must be 1. Found {}\'.\n                       format(len(image_features)))\n    image_feature = image_features[0]\n    num_predictions_per_location = num_predictions_per_location[0]\n    predictions_dict = {}\n\n    if predict_boxes_and_classes:\n      (box_encodings, class_predictions_with_background\n      ) = self._predict_boxes_and_classes(image_feature)\n      predictions_dict[BOX_ENCODINGS] = box_encodings\n      predictions_dict[\n          CLASS_PREDICTIONS_WITH_BACKGROUND] = class_predictions_with_background\n\n    if self._predict_instance_masks and predict_auxiliary_outputs:\n      predictions_dict[MASK_PREDICTIONS] = self._predict_masks(image_feature)\n\n    return predictions_dict\n\n\nclass _NoopVariableScope(object):\n  """"""A dummy class that does not push any scope.""""""\n\n  def __enter__(self):\n    return None\n\n  def __exit__(self, exc_type, exc_value, traceback):\n    return False\n\n\nclass ConvolutionalBoxPredictor(BoxPredictor):\n  """"""Convolutional Box Predictor.\n\n  Optionally add an intermediate 1x1 convolutional layer after features and\n  predict in parallel branches box_encodings and\n  class_predictions_with_background.\n\n  Currently this box predictor assumes that predictions are ""shared"" across\n  classes --- that is each anchor makes box predictions which do not depend\n  on class.\n  """"""\n\n  def __init__(self,\n               is_training,\n               num_classes,\n               conv_hyperparams,\n               min_depth,\n               max_depth,\n               num_layers_before_predictor,\n               use_dropout,\n               dropout_keep_prob,\n               kernel_size,\n               box_code_size,\n               apply_sigmoid_to_scores=False,\n               class_prediction_bias_init=0.0,\n               use_depthwise=False):\n    """"""Constructor.\n\n    Args:\n      is_training: Indicates whether the BoxPredictor is in training mode.\n      num_classes: number of classes.  Note that num_classes *does not*\n        include the background category, so if groundtruth labels take values\n        in {0, 1, .., K-1}, num_classes=K (and not K+1, even though the\n        assigned classification targets can range from {0,... K}).\n      conv_hyperparams: Slim arg_scope with hyperparameters for convolution ops.\n      min_depth: Minumum feature depth prior to predicting box encodings\n        and class predictions.\n      max_depth: Maximum feature depth prior to predicting box encodings\n        and class predictions. If max_depth is set to 0, no additional\n        feature map will be inserted before location and class predictions.\n      num_layers_before_predictor: Number of the additional conv layers before\n        the predictor.\n      use_dropout: Option to use dropout for class prediction or not.\n      dropout_keep_prob: Keep probability for dropout.\n        This is only used if use_dropout is True.\n      kernel_size: Size of final convolution kernel.  If the\n        spatial resolution of the feature map is smaller than the kernel size,\n        then the kernel size is automatically set to be\n        min(feature_width, feature_height).\n      box_code_size: Size of encoding for each box.\n      apply_sigmoid_to_scores: if True, apply the sigmoid on the output\n        class_predictions.\n      class_prediction_bias_init: constant value to initialize bias of the last\n        conv2d layer before class prediction.\n      use_depthwise: Whether to use depthwise convolutions for prediction\n        steps. Default is False.\n\n    Raises:\n      ValueError: if min_depth > max_depth.\n    """"""\n    super(ConvolutionalBoxPredictor, self).__init__(is_training, num_classes)\n    if min_depth > max_depth:\n      raise ValueError(\'min_depth should be less than or equal to max_depth\')\n    self._conv_hyperparams = conv_hyperparams\n    self._min_depth = min_depth\n    self._max_depth = max_depth\n    self._num_layers_before_predictor = num_layers_before_predictor\n    self._use_dropout = use_dropout\n    self._kernel_size = kernel_size\n    self._box_code_size = box_code_size\n    self._dropout_keep_prob = dropout_keep_prob\n    self._apply_sigmoid_to_scores = apply_sigmoid_to_scores\n    self._class_prediction_bias_init = class_prediction_bias_init\n    self._use_depthwise = use_depthwise\n\n  def _predict(self, image_features, num_predictions_per_location_list):\n    """"""Computes encoded object locations and corresponding confidences.\n\n    Args:\n      image_features: A list of float tensors of shape [batch_size, height_i,\n        width_i, channels_i] containing features for a batch of images.\n      num_predictions_per_location_list: A list of integers representing the\n        number of box predictions to be made per spatial location for each\n        feature map.\n\n    Returns:\n      box_encodings: A list of float tensors of shape\n        [batch_size, num_anchors_i, q, code_size] representing the location of\n        the objects, where q is 1 or the number of classes. Each entry in the\n        list corresponds to a feature map in the input `image_features` list.\n      class_predictions_with_background: A list of float tensors of shape\n        [batch_size, num_anchors_i, num_classes + 1] representing the class\n        predictions for the proposals. Each entry in the list corresponds to a\n        feature map in the input `image_features` list.\n    """"""\n    box_encodings_list = []\n    class_predictions_list = []\n    # TODO(rathodv): Come up with a better way to generate scope names\n    # in box predictor once we have time to retrain all models in the zoo.\n    # The following lines create scope names to be backwards compatible with the\n    # existing checkpoints.\n    box_predictor_scopes = [_NoopVariableScope()]\n    if len(image_features) > 1:\n      box_predictor_scopes = [\n          tf.variable_scope(\'BoxPredictor_{}\'.format(i))\n          for i in range(len(image_features))\n      ]\n\n    for (image_feature,\n         num_predictions_per_location, box_predictor_scope) in zip(\n             image_features, num_predictions_per_location_list,\n             box_predictor_scopes):\n      with box_predictor_scope:\n        # Add a slot for the background class.\n        num_class_slots = self.num_classes + 1\n        net = image_feature\n        with slim.arg_scope(self._conv_hyperparams), \\\n             slim.arg_scope([slim.dropout], is_training=self._is_training):\n          # Add additional conv layers before the class predictor.\n          features_depth = static_shape.get_depth(image_feature.get_shape())\n          depth = max(min(features_depth, self._max_depth), self._min_depth)\n          tf.logging.info(\'depth of additional conv before box predictor: {}\'.\n                          format(depth))\n          if depth > 0 and self._num_layers_before_predictor > 0:\n            for i in range(self._num_layers_before_predictor):\n              net = slim.conv2d(\n                  net, depth, [1, 1], scope=\'Conv2d_%d_1x1_%d\' % (i, depth))\n          with slim.arg_scope([slim.conv2d], activation_fn=None,\n                              normalizer_fn=None, normalizer_params=None):\n            if self._use_depthwise:\n              box_encodings = slim.separable_conv2d(\n                  net, None, [self._kernel_size, self._kernel_size],\n                  padding=\'SAME\', depth_multiplier=1, stride=1,\n                  rate=1, scope=\'BoxEncodingPredictor_depthwise\')\n              box_encodings = slim.conv2d(\n                  box_encodings,\n                  num_predictions_per_location * self._box_code_size, [1, 1],\n                  scope=\'BoxEncodingPredictor\')\n            else:\n              box_encodings = slim.conv2d(\n                  net, num_predictions_per_location * self._box_code_size,\n                  [self._kernel_size, self._kernel_size],\n                  scope=\'BoxEncodingPredictor\')\n            if self._use_dropout:\n              net = slim.dropout(net, keep_prob=self._dropout_keep_prob)\n            if self._use_depthwise:\n              class_predictions_with_background = slim.separable_conv2d(\n                  net, None, [self._kernel_size, self._kernel_size],\n                  padding=\'SAME\', depth_multiplier=1, stride=1,\n                  rate=1, scope=\'ClassPredictor_depthwise\')\n              class_predictions_with_background = slim.conv2d(\n                  class_predictions_with_background,\n                  num_predictions_per_location * num_class_slots,\n                  [1, 1], scope=\'ClassPredictor\')\n            else:\n              class_predictions_with_background = slim.conv2d(\n                  net, num_predictions_per_location * num_class_slots,\n                  [self._kernel_size, self._kernel_size],\n                  scope=\'ClassPredictor\',\n                  biases_initializer=tf.constant_initializer(\n                      self._class_prediction_bias_init))\n            if self._apply_sigmoid_to_scores:\n              class_predictions_with_background = tf.sigmoid(\n                  class_predictions_with_background)\n\n        combined_feature_map_shape = (shape_utils.\n                                      combined_static_and_dynamic_shape(\n                                          image_feature))\n        box_encodings = tf.reshape(\n            box_encodings, tf.stack([combined_feature_map_shape[0],\n                                     combined_feature_map_shape[1] *\n                                     combined_feature_map_shape[2] *\n                                     num_predictions_per_location,\n                                     1, self._box_code_size]))\n        box_encodings_list.append(box_encodings)\n        class_predictions_with_background = tf.reshape(\n            class_predictions_with_background,\n            tf.stack([combined_feature_map_shape[0],\n                      combined_feature_map_shape[1] *\n                      combined_feature_map_shape[2] *\n                      num_predictions_per_location,\n                      num_class_slots]))\n        class_predictions_list.append(class_predictions_with_background)\n    return {\n        BOX_ENCODINGS: box_encodings_list,\n        CLASS_PREDICTIONS_WITH_BACKGROUND: class_predictions_list\n    }\n\n\n# TODO(rathodv): Merge the implementation with ConvolutionalBoxPredictor above\n# since they are very similar.\nclass WeightSharedConvolutionalBoxPredictor(BoxPredictor):\n  """"""Convolutional Box Predictor with weight sharing.\n\n  Defines the box predictor as defined in\n  https://arxiv.org/abs/1708.02002. This class differs from\n  ConvolutionalBoxPredictor in that it shares weights and biases while\n  predicting from different feature maps.  Separate multi-layer towers are\n  constructed for the box encoding and class predictors respectively.\n  """"""\n\n  def __init__(self,\n               is_training,\n               num_classes,\n               conv_hyperparams,\n               depth,\n               num_layers_before_predictor,\n               box_code_size,\n               kernel_size=3,\n               class_prediction_bias_init=0.0):\n    """"""Constructor.\n\n    Args:\n      is_training: Indicates whether the BoxPredictor is in training mode.\n      num_classes: number of classes.  Note that num_classes *does not*\n        include the background category, so if groundtruth labels take values\n        in {0, 1, .., K-1}, num_classes=K (and not K+1, even though the\n        assigned classification targets can range from {0,... K}).\n      conv_hyperparams: Slim arg_scope with hyperparameters for convolution ops.\n      depth: depth of conv layers.\n      num_layers_before_predictor: Number of the additional conv layers before\n        the predictor.\n      box_code_size: Size of encoding for each box.\n      kernel_size: Size of final convolution kernel.\n      class_prediction_bias_init: constant value to initialize bias of the last\n        conv2d layer before class prediction.\n    """"""\n    super(WeightSharedConvolutionalBoxPredictor, self).__init__(is_training,\n                                                                num_classes)\n    self._conv_hyperparams = conv_hyperparams\n    self._depth = depth\n    self._num_layers_before_predictor = num_layers_before_predictor\n    self._box_code_size = box_code_size\n    self._kernel_size = kernel_size\n    self._class_prediction_bias_init = class_prediction_bias_init\n\n  def _predict(self, image_features, num_predictions_per_location_list):\n    """"""Computes encoded object locations and corresponding confidences.\n\n    Args:\n      image_features: A list of float tensors of shape [batch_size, height_i,\n        width_i, channels] containing features for a batch of images. Note that\n        all tensors in the list must have the same number of channels.\n      num_predictions_per_location_list: A list of integers representing the\n        number of box predictions to be made per spatial location for each\n        feature map. Note that all values must be the same since the weights are\n        shared.\n\n    Returns:\n      box_encodings: A list of float tensors of shape\n        [batch_size, num_anchors_i, q, code_size] representing the location of\n        the objects, where q is 1 or the number of classes. Each entry in the\n        list corresponds to a feature map in the input `image_features` list.\n      class_predictions_with_background: A list of float tensors of shape\n        [batch_size, num_anchors_i, num_classes + 1] representing the class\n        predictions for the proposals. Each entry in the list corresponds to a\n        feature map in the input `image_features` list.\n\n    Raises:\n      ValueError: If the image feature maps do not have the same number of\n        channels or if the num predictions per locations is differs between the\n        feature maps.\n    """"""\n    if len(set(num_predictions_per_location_list)) > 1:\n      raise ValueError(\'num predictions per location must be same for all\'\n                       \'feature maps, found: {}\'.format(\n                           num_predictions_per_location_list))\n    feature_channels = [\n        image_feature.shape[3].value for image_feature in image_features\n    ]\n    if len(set(feature_channels)) > 1:\n      raise ValueError(\'all feature maps must have the same number of \'\n                       \'channels, found: {}\'.format(feature_channels))\n    box_encodings_list = []\n    class_predictions_list = []\n    for (image_feature, num_predictions_per_location) in zip(\n        image_features, num_predictions_per_location_list):\n      # Add a slot for the background class.\n      with tf.variable_scope(\'WeightSharedConvolutionalBoxPredictor\',\n                             reuse=tf.AUTO_REUSE):\n        num_class_slots = self.num_classes + 1\n        box_encodings_net = image_feature\n        class_predictions_net = image_feature\n        with slim.arg_scope(self._conv_hyperparams):\n          for i in range(self._num_layers_before_predictor):\n            box_encodings_net = slim.conv2d(\n                box_encodings_net,\n                self._depth,\n                [self._kernel_size, self._kernel_size],\n                stride=1,\n                padding=\'SAME\',\n                scope=\'BoxEncodingPredictionTower/conv2d_{}\'.format(i))\n          box_encodings = slim.conv2d(\n              box_encodings_net,\n              num_predictions_per_location * self._box_code_size,\n              [self._kernel_size, self._kernel_size],\n              activation_fn=None, stride=1, padding=\'SAME\',\n              scope=\'BoxEncodingPredictor\')\n\n          for i in range(self._num_layers_before_predictor):\n            class_predictions_net = slim.conv2d(\n                class_predictions_net,\n                self._depth,\n                [self._kernel_size, self._kernel_size],\n                stride=1,\n                padding=\'SAME\',\n                scope=\'ClassPredictionTower/conv2d_{}\'.format(i))\n          class_predictions_with_background = slim.conv2d(\n              class_predictions_net,\n              num_predictions_per_location * num_class_slots,\n              [self._kernel_size, self._kernel_size],\n              activation_fn=None, stride=1, padding=\'SAME\',\n              biases_initializer=tf.constant_initializer(\n                  self._class_prediction_bias_init),\n              scope=\'ClassPredictor\')\n\n          combined_feature_map_shape = (shape_utils.\n                                        combined_static_and_dynamic_shape(\n                                            image_feature))\n          box_encodings = tf.reshape(\n              box_encodings, tf.stack([combined_feature_map_shape[0],\n                                       combined_feature_map_shape[1] *\n                                       combined_feature_map_shape[2] *\n                                       num_predictions_per_location,\n                                       1, self._box_code_size]))\n          box_encodings_list.append(box_encodings)\n          class_predictions_with_background = tf.reshape(\n              class_predictions_with_background,\n              tf.stack([combined_feature_map_shape[0],\n                        combined_feature_map_shape[1] *\n                        combined_feature_map_shape[2] *\n                        num_predictions_per_location,\n                        num_class_slots]))\n          class_predictions_list.append(class_predictions_with_background)\n    return {\n        BOX_ENCODINGS: box_encodings_list,\n        CLASS_PREDICTIONS_WITH_BACKGROUND: class_predictions_list\n    }\n'"
src/object_detection/core/box_predictor_test.py,51,"b'# Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n""""""Tests for object_detection.core.box_predictor.""""""\nimport numpy as np\nimport tensorflow as tf\n\nfrom google.protobuf import text_format\nfrom object_detection.builders import hyperparams_builder\nfrom object_detection.core import box_predictor\nfrom object_detection.protos import hyperparams_pb2\nfrom object_detection.utils import test_case\n\n\nclass MaskRCNNBoxPredictorTest(tf.test.TestCase):\n\n  def _build_arg_scope_with_hyperparams(self,\n                                        op_type=hyperparams_pb2.Hyperparams.FC):\n    hyperparams = hyperparams_pb2.Hyperparams()\n    hyperparams_text_proto = """"""\n      activation: NONE\n      regularizer {\n        l2_regularizer {\n        }\n      }\n      initializer {\n        truncated_normal_initializer {\n        }\n      }\n    """"""\n    text_format.Merge(hyperparams_text_proto, hyperparams)\n    hyperparams.op = op_type\n    return hyperparams_builder.build(hyperparams, is_training=True)\n\n  def test_get_boxes_with_five_classes(self):\n    image_features = tf.random_uniform([2, 7, 7, 3], dtype=tf.float32)\n    mask_box_predictor = box_predictor.MaskRCNNBoxPredictor(\n        is_training=False,\n        num_classes=5,\n        fc_hyperparams=self._build_arg_scope_with_hyperparams(),\n        use_dropout=False,\n        dropout_keep_prob=0.5,\n        box_code_size=4,\n    )\n    box_predictions = mask_box_predictor.predict(\n        [image_features], num_predictions_per_location=[1],\n        scope=\'BoxPredictor\')\n    box_encodings = box_predictions[box_predictor.BOX_ENCODINGS]\n    class_predictions_with_background = box_predictions[\n        box_predictor.CLASS_PREDICTIONS_WITH_BACKGROUND]\n    init_op = tf.global_variables_initializer()\n    with self.test_session() as sess:\n      sess.run(init_op)\n      (box_encodings_shape,\n       class_predictions_with_background_shape) = sess.run(\n           [tf.shape(box_encodings),\n            tf.shape(class_predictions_with_background)])\n      self.assertAllEqual(box_encodings_shape, [2, 1, 5, 4])\n      self.assertAllEqual(class_predictions_with_background_shape, [2, 1, 6])\n\n  def test_value_error_on_predict_instance_masks_with_no_conv_hyperparms(self):\n    with self.assertRaises(ValueError):\n      box_predictor.MaskRCNNBoxPredictor(\n          is_training=False,\n          num_classes=5,\n          fc_hyperparams=self._build_arg_scope_with_hyperparams(),\n          use_dropout=False,\n          dropout_keep_prob=0.5,\n          box_code_size=4,\n          predict_instance_masks=True)\n\n  def test_get_instance_masks(self):\n    image_features = tf.random_uniform([2, 7, 7, 3], dtype=tf.float32)\n    mask_box_predictor = box_predictor.MaskRCNNBoxPredictor(\n        is_training=False,\n        num_classes=5,\n        fc_hyperparams=self._build_arg_scope_with_hyperparams(),\n        use_dropout=False,\n        dropout_keep_prob=0.5,\n        box_code_size=4,\n        conv_hyperparams=self._build_arg_scope_with_hyperparams(\n            op_type=hyperparams_pb2.Hyperparams.CONV),\n        predict_instance_masks=True)\n    box_predictions = mask_box_predictor.predict(\n        [image_features],\n        num_predictions_per_location=[1],\n        scope=\'BoxPredictor\',\n        predict_boxes_and_classes=True,\n        predict_auxiliary_outputs=True)\n    mask_predictions = box_predictions[box_predictor.MASK_PREDICTIONS]\n    self.assertListEqual([2, 1, 5, 14, 14],\n                         mask_predictions.get_shape().as_list())\n\n  def test_do_not_return_instance_masks_without_request(self):\n    image_features = tf.random_uniform([2, 7, 7, 3], dtype=tf.float32)\n    mask_box_predictor = box_predictor.MaskRCNNBoxPredictor(\n        is_training=False,\n        num_classes=5,\n        fc_hyperparams=self._build_arg_scope_with_hyperparams(),\n        use_dropout=False,\n        dropout_keep_prob=0.5,\n        box_code_size=4)\n    box_predictions = mask_box_predictor.predict(\n        [image_features], num_predictions_per_location=[1],\n        scope=\'BoxPredictor\')\n    self.assertEqual(len(box_predictions), 2)\n    self.assertTrue(box_predictor.BOX_ENCODINGS in box_predictions)\n    self.assertTrue(box_predictor.CLASS_PREDICTIONS_WITH_BACKGROUND\n                    in box_predictions)\n\n  def test_value_error_on_predict_keypoints(self):\n    with self.assertRaises(ValueError):\n      box_predictor.MaskRCNNBoxPredictor(\n          is_training=False,\n          num_classes=5,\n          fc_hyperparams=self._build_arg_scope_with_hyperparams(),\n          use_dropout=False,\n          dropout_keep_prob=0.5,\n          box_code_size=4,\n          predict_keypoints=True)\n\n\nclass RfcnBoxPredictorTest(tf.test.TestCase):\n\n  def _build_arg_scope_with_conv_hyperparams(self):\n    conv_hyperparams = hyperparams_pb2.Hyperparams()\n    conv_hyperparams_text_proto = """"""\n      regularizer {\n        l2_regularizer {\n        }\n      }\n      initializer {\n        truncated_normal_initializer {\n        }\n      }\n    """"""\n    text_format.Merge(conv_hyperparams_text_proto, conv_hyperparams)\n    return hyperparams_builder.build(conv_hyperparams, is_training=True)\n\n  def test_get_correct_box_encoding_and_class_prediction_shapes(self):\n    image_features = tf.random_uniform([4, 8, 8, 64], dtype=tf.float32)\n    proposal_boxes = tf.random_normal([4, 2, 4], dtype=tf.float32)\n    rfcn_box_predictor = box_predictor.RfcnBoxPredictor(\n        is_training=False,\n        num_classes=2,\n        conv_hyperparams=self._build_arg_scope_with_conv_hyperparams(),\n        num_spatial_bins=[3, 3],\n        depth=4,\n        crop_size=[12, 12],\n        box_code_size=4\n    )\n    box_predictions = rfcn_box_predictor.predict(\n        [image_features], num_predictions_per_location=[1],\n        scope=\'BoxPredictor\',\n        proposal_boxes=proposal_boxes)\n    box_encodings = tf.concat(\n        box_predictions[box_predictor.BOX_ENCODINGS], axis=1)\n    class_predictions_with_background = tf.concat(\n        box_predictions[box_predictor.CLASS_PREDICTIONS_WITH_BACKGROUND],\n        axis=1)\n\n    init_op = tf.global_variables_initializer()\n    with self.test_session() as sess:\n      sess.run(init_op)\n      (box_encodings_shape,\n       class_predictions_shape) = sess.run(\n           [tf.shape(box_encodings),\n            tf.shape(class_predictions_with_background)])\n      self.assertAllEqual(box_encodings_shape, [8, 1, 2, 4])\n      self.assertAllEqual(class_predictions_shape, [8, 1, 3])\n\n\nclass ConvolutionalBoxPredictorTest(test_case.TestCase):\n\n  def _build_arg_scope_with_conv_hyperparams(self):\n    conv_hyperparams = hyperparams_pb2.Hyperparams()\n    conv_hyperparams_text_proto = """"""\n      activation: RELU_6\n      regularizer {\n        l2_regularizer {\n        }\n      }\n      initializer {\n        truncated_normal_initializer {\n        }\n      }\n    """"""\n    text_format.Merge(conv_hyperparams_text_proto, conv_hyperparams)\n    return hyperparams_builder.build(conv_hyperparams, is_training=True)\n\n  def test_get_boxes_for_five_aspect_ratios_per_location(self):\n    def graph_fn(image_features):\n      conv_box_predictor = box_predictor.ConvolutionalBoxPredictor(\n          is_training=False,\n          num_classes=0,\n          conv_hyperparams=self._build_arg_scope_with_conv_hyperparams(),\n          min_depth=0,\n          max_depth=32,\n          num_layers_before_predictor=1,\n          use_dropout=True,\n          dropout_keep_prob=0.8,\n          kernel_size=1,\n          box_code_size=4\n      )\n      box_predictions = conv_box_predictor.predict(\n          [image_features], num_predictions_per_location=[5],\n          scope=\'BoxPredictor\')\n      box_encodings = tf.concat(\n          box_predictions[box_predictor.BOX_ENCODINGS], axis=1)\n      objectness_predictions = tf.concat(\n          box_predictions[box_predictor.CLASS_PREDICTIONS_WITH_BACKGROUND],\n          axis=1)\n      return (box_encodings, objectness_predictions)\n    image_features = np.random.rand(4, 8, 8, 64).astype(np.float32)\n    (box_encodings, objectness_predictions) = self.execute(graph_fn,\n                                                           [image_features])\n    self.assertAllEqual(box_encodings.shape, [4, 320, 1, 4])\n    self.assertAllEqual(objectness_predictions.shape, [4, 320, 1])\n\n  def test_get_boxes_for_one_aspect_ratio_per_location(self):\n    def graph_fn(image_features):\n      conv_box_predictor = box_predictor.ConvolutionalBoxPredictor(\n          is_training=False,\n          num_classes=0,\n          conv_hyperparams=self._build_arg_scope_with_conv_hyperparams(),\n          min_depth=0,\n          max_depth=32,\n          num_layers_before_predictor=1,\n          use_dropout=True,\n          dropout_keep_prob=0.8,\n          kernel_size=1,\n          box_code_size=4\n      )\n      box_predictions = conv_box_predictor.predict(\n          [image_features], num_predictions_per_location=[1],\n          scope=\'BoxPredictor\')\n      box_encodings = tf.concat(\n          box_predictions[box_predictor.BOX_ENCODINGS], axis=1)\n      objectness_predictions = tf.concat(box_predictions[\n          box_predictor.CLASS_PREDICTIONS_WITH_BACKGROUND], axis=1)\n      return (box_encodings, objectness_predictions)\n    image_features = np.random.rand(4, 8, 8, 64).astype(np.float32)\n    (box_encodings, objectness_predictions) = self.execute(graph_fn,\n                                                           [image_features])\n    self.assertAllEqual(box_encodings.shape, [4, 64, 1, 4])\n    self.assertAllEqual(objectness_predictions.shape, [4, 64, 1])\n\n  def test_get_multi_class_predictions_for_five_aspect_ratios_per_location(\n      self):\n    num_classes_without_background = 6\n    image_features = np.random.rand(4, 8, 8, 64).astype(np.float32)\n    def graph_fn(image_features):\n      conv_box_predictor = box_predictor.ConvolutionalBoxPredictor(\n          is_training=False,\n          num_classes=num_classes_without_background,\n          conv_hyperparams=self._build_arg_scope_with_conv_hyperparams(),\n          min_depth=0,\n          max_depth=32,\n          num_layers_before_predictor=1,\n          use_dropout=True,\n          dropout_keep_prob=0.8,\n          kernel_size=1,\n          box_code_size=4\n      )\n      box_predictions = conv_box_predictor.predict(\n          [image_features],\n          num_predictions_per_location=[5],\n          scope=\'BoxPredictor\')\n      box_encodings = tf.concat(\n          box_predictions[box_predictor.BOX_ENCODINGS], axis=1)\n      class_predictions_with_background = tf.concat(\n          box_predictions[box_predictor.CLASS_PREDICTIONS_WITH_BACKGROUND],\n          axis=1)\n      return (box_encodings, class_predictions_with_background)\n    (box_encodings,\n     class_predictions_with_background) = self.execute(graph_fn,\n                                                       [image_features])\n    self.assertAllEqual(box_encodings.shape, [4, 320, 1, 4])\n    self.assertAllEqual(class_predictions_with_background.shape,\n                        [4, 320, num_classes_without_background+1])\n\n  def test_get_predictions_with_feature_maps_of_dynamic_shape(\n      self):\n    image_features = tf.placeholder(dtype=tf.float32, shape=[4, None, None, 64])\n    conv_box_predictor = box_predictor.ConvolutionalBoxPredictor(\n        is_training=False,\n        num_classes=0,\n        conv_hyperparams=self._build_arg_scope_with_conv_hyperparams(),\n        min_depth=0,\n        max_depth=32,\n        num_layers_before_predictor=1,\n        use_dropout=True,\n        dropout_keep_prob=0.8,\n        kernel_size=1,\n        box_code_size=4\n    )\n    box_predictions = conv_box_predictor.predict(\n        [image_features], num_predictions_per_location=[5],\n        scope=\'BoxPredictor\')\n    box_encodings = tf.concat(\n        box_predictions[box_predictor.BOX_ENCODINGS], axis=1)\n    objectness_predictions = tf.concat(\n        box_predictions[box_predictor.CLASS_PREDICTIONS_WITH_BACKGROUND],\n        axis=1)\n    init_op = tf.global_variables_initializer()\n\n    resolution = 32\n    expected_num_anchors = resolution*resolution*5\n    with self.test_session() as sess:\n      sess.run(init_op)\n      (box_encodings_shape,\n       objectness_predictions_shape) = sess.run(\n           [tf.shape(box_encodings), tf.shape(objectness_predictions)],\n           feed_dict={image_features:\n                      np.random.rand(4, resolution, resolution, 64)})\n      actual_variable_set = set(\n          [var.op.name for var in tf.trainable_variables()])\n      self.assertAllEqual(box_encodings_shape, [4, expected_num_anchors, 1, 4])\n      self.assertAllEqual(objectness_predictions_shape,\n                          [4, expected_num_anchors, 1])\n    expected_variable_set = set([\n        \'BoxPredictor/Conv2d_0_1x1_32/biases\',\n        \'BoxPredictor/Conv2d_0_1x1_32/weights\',\n        \'BoxPredictor/BoxEncodingPredictor/biases\',\n        \'BoxPredictor/BoxEncodingPredictor/weights\',\n        \'BoxPredictor/ClassPredictor/biases\',\n        \'BoxPredictor/ClassPredictor/weights\'])\n    self.assertEqual(expected_variable_set, actual_variable_set)\n\n  def test_use_depthwise_convolution(self):\n    image_features = tf.placeholder(dtype=tf.float32, shape=[4, None, None, 64])\n    conv_box_predictor = box_predictor.ConvolutionalBoxPredictor(\n        is_training=False,\n        num_classes=0,\n        conv_hyperparams=self._build_arg_scope_with_conv_hyperparams(),\n        min_depth=0,\n        max_depth=32,\n        num_layers_before_predictor=1,\n        dropout_keep_prob=0.8,\n        kernel_size=1,\n        box_code_size=4,\n        use_dropout=True,\n        use_depthwise=True\n    )\n    box_predictions = conv_box_predictor.predict(\n        [image_features], num_predictions_per_location=[5],\n        scope=\'BoxPredictor\')\n    box_encodings = tf.concat(\n        box_predictions[box_predictor.BOX_ENCODINGS], axis=1)\n    objectness_predictions = tf.concat(\n        box_predictions[box_predictor.CLASS_PREDICTIONS_WITH_BACKGROUND],\n        axis=1)\n    init_op = tf.global_variables_initializer()\n\n    resolution = 32\n    expected_num_anchors = resolution*resolution*5\n    with self.test_session() as sess:\n      sess.run(init_op)\n      (box_encodings_shape,\n       objectness_predictions_shape) = sess.run(\n           [tf.shape(box_encodings), tf.shape(objectness_predictions)],\n           feed_dict={image_features:\n                      np.random.rand(4, resolution, resolution, 64)})\n      actual_variable_set = set(\n          [var.op.name for var in tf.trainable_variables()])\n    self.assertAllEqual(box_encodings_shape, [4, expected_num_anchors, 1, 4])\n    self.assertAllEqual(objectness_predictions_shape,\n                        [4, expected_num_anchors, 1])\n    expected_variable_set = set([\n        \'BoxPredictor/Conv2d_0_1x1_32/biases\',\n        \'BoxPredictor/Conv2d_0_1x1_32/weights\',\n        \'BoxPredictor/BoxEncodingPredictor_depthwise/biases\',\n        \'BoxPredictor/BoxEncodingPredictor_depthwise/depthwise_weights\',\n        \'BoxPredictor/BoxEncodingPredictor/biases\',\n        \'BoxPredictor/BoxEncodingPredictor/weights\',\n        \'BoxPredictor/ClassPredictor_depthwise/biases\',\n        \'BoxPredictor/ClassPredictor_depthwise/depthwise_weights\',\n        \'BoxPredictor/ClassPredictor/biases\',\n        \'BoxPredictor/ClassPredictor/weights\'])\n    self.assertEqual(expected_variable_set, actual_variable_set)\n\n\nclass WeightSharedConvolutionalBoxPredictorTest(test_case.TestCase):\n\n  def _build_arg_scope_with_conv_hyperparams(self):\n    conv_hyperparams = hyperparams_pb2.Hyperparams()\n    conv_hyperparams_text_proto = """"""\n      activation: RELU_6\n      regularizer {\n        l2_regularizer {\n        }\n      }\n      initializer {\n        truncated_normal_initializer {\n        }\n      }\n    """"""\n    text_format.Merge(conv_hyperparams_text_proto, conv_hyperparams)\n    return hyperparams_builder.build(conv_hyperparams, is_training=True)\n\n  def test_get_boxes_for_five_aspect_ratios_per_location(self):\n\n    def graph_fn(image_features):\n      conv_box_predictor = box_predictor.WeightSharedConvolutionalBoxPredictor(\n          is_training=False,\n          num_classes=0,\n          conv_hyperparams=self._build_arg_scope_with_conv_hyperparams(),\n          depth=32,\n          num_layers_before_predictor=1,\n          box_code_size=4)\n      box_predictions = conv_box_predictor.predict(\n          [image_features], num_predictions_per_location=[5],\n          scope=\'BoxPredictor\')\n      box_encodings = tf.concat(\n          box_predictions[box_predictor.BOX_ENCODINGS], axis=1)\n      objectness_predictions = tf.concat(box_predictions[\n          box_predictor.CLASS_PREDICTIONS_WITH_BACKGROUND], axis=1)\n      return (box_encodings, objectness_predictions)\n    image_features = np.random.rand(4, 8, 8, 64).astype(np.float32)\n    (box_encodings, objectness_predictions) = self.execute(\n        graph_fn, [image_features])\n    self.assertAllEqual(box_encodings.shape, [4, 320, 1, 4])\n    self.assertAllEqual(objectness_predictions.shape, [4, 320, 1])\n\n  def test_get_multi_class_predictions_for_five_aspect_ratios_per_location(\n      self):\n\n    num_classes_without_background = 6\n    def graph_fn(image_features):\n      conv_box_predictor = box_predictor.WeightSharedConvolutionalBoxPredictor(\n          is_training=False,\n          num_classes=num_classes_without_background,\n          conv_hyperparams=self._build_arg_scope_with_conv_hyperparams(),\n          depth=32,\n          num_layers_before_predictor=1,\n          box_code_size=4)\n      box_predictions = conv_box_predictor.predict(\n          [image_features],\n          num_predictions_per_location=[5],\n          scope=\'BoxPredictor\')\n      box_encodings = tf.concat(\n          box_predictions[box_predictor.BOX_ENCODINGS], axis=1)\n      class_predictions_with_background = tf.concat(box_predictions[\n          box_predictor.CLASS_PREDICTIONS_WITH_BACKGROUND], axis=1)\n      return (box_encodings, class_predictions_with_background)\n\n    image_features = np.random.rand(4, 8, 8, 64).astype(np.float32)\n    (box_encodings, class_predictions_with_background) = self.execute(\n        graph_fn, [image_features])\n    self.assertAllEqual(box_encodings.shape, [4, 320, 1, 4])\n    self.assertAllEqual(class_predictions_with_background.shape,\n                        [4, 320, num_classes_without_background+1])\n\n  def test_get_multi_class_predictions_from_two_feature_maps(\n      self):\n\n    num_classes_without_background = 6\n    def graph_fn(image_features1, image_features2):\n      conv_box_predictor = box_predictor.WeightSharedConvolutionalBoxPredictor(\n          is_training=False,\n          num_classes=num_classes_without_background,\n          conv_hyperparams=self._build_arg_scope_with_conv_hyperparams(),\n          depth=32,\n          num_layers_before_predictor=1,\n          box_code_size=4)\n      box_predictions = conv_box_predictor.predict(\n          [image_features1, image_features2],\n          num_predictions_per_location=[5, 5],\n          scope=\'BoxPredictor\')\n      box_encodings = tf.concat(\n          box_predictions[box_predictor.BOX_ENCODINGS], axis=1)\n      class_predictions_with_background = tf.concat(\n          box_predictions[box_predictor.CLASS_PREDICTIONS_WITH_BACKGROUND],\n          axis=1)\n      return (box_encodings, class_predictions_with_background)\n\n    image_features1 = np.random.rand(4, 8, 8, 64).astype(np.float32)\n    image_features2 = np.random.rand(4, 8, 8, 64).astype(np.float32)\n    (box_encodings, class_predictions_with_background) = self.execute(\n        graph_fn, [image_features1, image_features2])\n    self.assertAllEqual(box_encodings.shape, [4, 640, 1, 4])\n    self.assertAllEqual(class_predictions_with_background.shape,\n                        [4, 640, num_classes_without_background+1])\n\n  def test_predictions_from_multiple_feature_maps_share_weights(self):\n    num_classes_without_background = 6\n    def graph_fn(image_features1, image_features2):\n      conv_box_predictor = box_predictor.WeightSharedConvolutionalBoxPredictor(\n          is_training=False,\n          num_classes=num_classes_without_background,\n          conv_hyperparams=self._build_arg_scope_with_conv_hyperparams(),\n          depth=32,\n          num_layers_before_predictor=2,\n          box_code_size=4)\n      box_predictions = conv_box_predictor.predict(\n          [image_features1, image_features2],\n          num_predictions_per_location=[5, 5],\n          scope=\'BoxPredictor\')\n      box_encodings = tf.concat(\n          box_predictions[box_predictor.BOX_ENCODINGS], axis=1)\n      class_predictions_with_background = tf.concat(\n          box_predictions[box_predictor.CLASS_PREDICTIONS_WITH_BACKGROUND],\n          axis=1)\n      return (box_encodings, class_predictions_with_background)\n\n    with self.test_session(graph=tf.Graph()):\n      graph_fn(tf.random_uniform([4, 32, 32, 3], dtype=tf.float32),\n               tf.random_uniform([4, 16, 16, 3], dtype=tf.float32))\n      actual_variable_set = set(\n          [var.op.name for var in tf.trainable_variables()])\n    expected_variable_set = set([\n        (\'BoxPredictor/WeightSharedConvolutionalBoxPredictor/\'\n         \'BoxEncodingPredictionTower/conv2d_0/weights\'),\n        (\'BoxPredictor/WeightSharedConvolutionalBoxPredictor/\'\n         \'BoxEncodingPredictionTower/conv2d_0/biases\'),\n        (\'BoxPredictor/WeightSharedConvolutionalBoxPredictor/\'\n         \'BoxEncodingPredictionTower/conv2d_1/weights\'),\n        (\'BoxPredictor/WeightSharedConvolutionalBoxPredictor/\'\n         \'BoxEncodingPredictionTower/conv2d_1/biases\'),\n        (\'BoxPredictor/WeightSharedConvolutionalBoxPredictor/\'\n         \'ClassPredictionTower/conv2d_0/weights\'),\n        (\'BoxPredictor/WeightSharedConvolutionalBoxPredictor/\'\n         \'ClassPredictionTower/conv2d_0/biases\'),\n        (\'BoxPredictor/WeightSharedConvolutionalBoxPredictor/\'\n         \'ClassPredictionTower/conv2d_1/weights\'),\n        (\'BoxPredictor/WeightSharedConvolutionalBoxPredictor/\'\n         \'ClassPredictionTower/conv2d_1/biases\'),\n        (\'BoxPredictor/WeightSharedConvolutionalBoxPredictor/\'\n         \'BoxEncodingPredictor/weights\'),\n        (\'BoxPredictor/WeightSharedConvolutionalBoxPredictor/\'\n         \'BoxEncodingPredictor/biases\'),\n        (\'BoxPredictor/WeightSharedConvolutionalBoxPredictor/\'\n         \'ClassPredictor/weights\'),\n        (\'BoxPredictor/WeightSharedConvolutionalBoxPredictor/\'\n         \'ClassPredictor/biases\')])\n    self.assertEqual(expected_variable_set, actual_variable_set)\n\n  def test_get_predictions_with_feature_maps_of_dynamic_shape(\n      self):\n    image_features = tf.placeholder(dtype=tf.float32, shape=[4, None, None, 64])\n    conv_box_predictor = box_predictor.WeightSharedConvolutionalBoxPredictor(\n        is_training=False,\n        num_classes=0,\n        conv_hyperparams=self._build_arg_scope_with_conv_hyperparams(),\n        depth=32,\n        num_layers_before_predictor=1,\n        box_code_size=4)\n    box_predictions = conv_box_predictor.predict(\n        [image_features], num_predictions_per_location=[5],\n        scope=\'BoxPredictor\')\n    box_encodings = tf.concat(box_predictions[box_predictor.BOX_ENCODINGS],\n                              axis=1)\n    objectness_predictions = tf.concat(box_predictions[\n        box_predictor.CLASS_PREDICTIONS_WITH_BACKGROUND], axis=1)\n    init_op = tf.global_variables_initializer()\n\n    resolution = 32\n    expected_num_anchors = resolution*resolution*5\n    with self.test_session() as sess:\n      sess.run(init_op)\n      (box_encodings_shape,\n       objectness_predictions_shape) = sess.run(\n           [tf.shape(box_encodings), tf.shape(objectness_predictions)],\n           feed_dict={image_features:\n                      np.random.rand(4, resolution, resolution, 64)})\n      self.assertAllEqual(box_encodings_shape, [4, expected_num_anchors, 1, 4])\n      self.assertAllEqual(objectness_predictions_shape,\n                          [4, expected_num_anchors, 1])\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
src/object_detection/core/data_decoder.py,0,"b'# Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n""""""Interface for data decoders.\n\nData decoders decode the input data and return a dictionary of tensors keyed by\nthe entries in core.reader.Fields.\n""""""\nfrom abc import ABCMeta\nfrom abc import abstractmethod\n\n\nclass DataDecoder(object):\n  """"""Interface for data decoders.""""""\n  __metaclass__ = ABCMeta\n\n  @abstractmethod\n  def decode(self, data):\n    """"""Return a single image and associated labels.\n\n    Args:\n      data: a string tensor holding a serialized protocol buffer corresponding\n        to data for a single image.\n\n    Returns:\n      tensor_dict: a dictionary containing tensors. Possible keys are defined in\n          reader.Fields.\n    """"""\n    pass\n'"
src/object_detection/core/data_parser.py,0,"b'# Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Interface for data parsers.\n\nData parser parses input data and returns a dictionary of numpy arrays\nkeyed by the entries in standard_fields.py. Since the parser parses records\nto numpy arrays (materialized tensors) directly, it is used to read data for\nevaluation/visualization; to parse the data during training, DataDecoder should\nbe used.\n""""""\nfrom abc import ABCMeta\nfrom abc import abstractmethod\n\n\nclass DataToNumpyParser(object):\n  __metaclass__ = ABCMeta\n\n  @abstractmethod\n  def parse(self, input_data):\n    """"""Parses input and returns a numpy array or a dictionary of numpy arrays.\n\n    Args:\n      input_data: an input data\n\n    Returns:\n      A numpy array or a dictionary of numpy arrays or None, if input\n      cannot be parsed.\n    """"""\n    pass\n'"
src/object_detection/core/keypoint_ops.py,52,"b'# Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n""""""Keypoint operations.\n\nKeypoints are represented as tensors of shape [num_instances, num_keypoints, 2],\nwhere the last dimension holds rank 2 tensors of the form [y, x] representing\nthe coordinates of the keypoint.\n""""""\nimport numpy as np\nimport tensorflow as tf\n\n\ndef scale(keypoints, y_scale, x_scale, scope=None):\n  """"""Scales keypoint coordinates in x and y dimensions.\n\n  Args:\n    keypoints: a tensor of shape [num_instances, num_keypoints, 2]\n    y_scale: (float) scalar tensor\n    x_scale: (float) scalar tensor\n    scope: name scope.\n\n  Returns:\n    new_keypoints: a tensor of shape [num_instances, num_keypoints, 2]\n  """"""\n  with tf.name_scope(scope, \'Scale\'):\n    y_scale = tf.cast(y_scale, tf.float32)\n    x_scale = tf.cast(x_scale, tf.float32)\n    new_keypoints = keypoints * [[[y_scale, x_scale]]]\n    return new_keypoints\n\n\ndef clip_to_window(keypoints, window, scope=None):\n  """"""Clips keypoints to a window.\n\n  This op clips any input keypoints to a window.\n\n  Args:\n    keypoints: a tensor of shape [num_instances, num_keypoints, 2]\n    window: a tensor of shape [4] representing the [y_min, x_min, y_max, x_max]\n      window to which the op should clip the keypoints.\n    scope: name scope.\n\n  Returns:\n    new_keypoints: a tensor of shape [num_instances, num_keypoints, 2]\n  """"""\n  with tf.name_scope(scope, \'ClipToWindow\'):\n    y, x = tf.split(value=keypoints, num_or_size_splits=2, axis=2)\n    win_y_min, win_x_min, win_y_max, win_x_max = tf.unstack(window)\n    y = tf.maximum(tf.minimum(y, win_y_max), win_y_min)\n    x = tf.maximum(tf.minimum(x, win_x_max), win_x_min)\n    new_keypoints = tf.concat([y, x], 2)\n    return new_keypoints\n\n\ndef prune_outside_window(keypoints, window, scope=None):\n  """"""Prunes keypoints that fall outside a given window.\n\n  This function replaces keypoints that fall outside the given window with nan.\n  See also clip_to_window which clips any keypoints that fall outside the given\n  window.\n\n  Args:\n    keypoints: a tensor of shape [num_instances, num_keypoints, 2]\n    window: a tensor of shape [4] representing the [y_min, x_min, y_max, x_max]\n      window outside of which the op should prune the keypoints.\n    scope: name scope.\n\n  Returns:\n    new_keypoints: a tensor of shape [num_instances, num_keypoints, 2]\n  """"""\n  with tf.name_scope(scope, \'PruneOutsideWindow\'):\n    y, x = tf.split(value=keypoints, num_or_size_splits=2, axis=2)\n    win_y_min, win_x_min, win_y_max, win_x_max = tf.unstack(window)\n\n    valid_indices = tf.logical_and(\n        tf.logical_and(y >= win_y_min, y <= win_y_max),\n        tf.logical_and(x >= win_x_min, x <= win_x_max))\n\n    new_y = tf.where(valid_indices, y, np.nan * tf.ones_like(y))\n    new_x = tf.where(valid_indices, x, np.nan * tf.ones_like(x))\n    new_keypoints = tf.concat([new_y, new_x], 2)\n\n    return new_keypoints\n\n\ndef change_coordinate_frame(keypoints, window, scope=None):\n  """"""Changes coordinate frame of the keypoints to be relative to window\'s frame.\n\n  Given a window of the form [y_min, x_min, y_max, x_max], changes keypoint\n  coordinates from keypoints of shape [num_instances, num_keypoints, 2]\n  to be relative to this window.\n\n  An example use case is data augmentation: where we are given groundtruth\n  keypoints and would like to randomly crop the image to some window. In this\n  case we need to change the coordinate frame of each groundtruth keypoint to be\n  relative to this new window.\n\n  Args:\n    keypoints: a tensor of shape [num_instances, num_keypoints, 2]\n    window: a tensor of shape [4] representing the [y_min, x_min, y_max, x_max]\n      window we should change the coordinate frame to.\n    scope: name scope.\n\n  Returns:\n    new_keypoints: a tensor of shape [num_instances, num_keypoints, 2]\n  """"""\n  with tf.name_scope(scope, \'ChangeCoordinateFrame\'):\n    win_height = window[2] - window[0]\n    win_width = window[3] - window[1]\n    new_keypoints = scale(keypoints - [window[0], window[1]], 1.0 / win_height,\n                          1.0 / win_width)\n    return new_keypoints\n\n\ndef to_normalized_coordinates(keypoints, height, width,\n                              check_range=True, scope=None):\n  """"""Converts absolute keypoint coordinates to normalized coordinates in [0, 1].\n\n  Usually one uses the dynamic shape of the image or conv-layer tensor:\n    keypoints = keypoint_ops.to_normalized_coordinates(keypoints,\n                                                       tf.shape(images)[1],\n                                                       tf.shape(images)[2]),\n\n  This function raises an assertion failed error at graph execution time when\n  the maximum coordinate is smaller than 1.01 (which means that coordinates are\n  already normalized). The value 1.01 is to deal with small rounding errors.\n\n  Args:\n    keypoints: A tensor of shape [num_instances, num_keypoints, 2].\n    height: Maximum value for y coordinate of absolute keypoint coordinates.\n    width: Maximum value for x coordinate of absolute keypoint coordinates.\n    check_range: If True, checks if the coordinates are normalized.\n    scope: name scope.\n\n  Returns:\n    tensor of shape [num_instances, num_keypoints, 2] with normalized\n    coordinates in [0, 1].\n  """"""\n  with tf.name_scope(scope, \'ToNormalizedCoordinates\'):\n    height = tf.cast(height, tf.float32)\n    width = tf.cast(width, tf.float32)\n\n    if check_range:\n      max_val = tf.reduce_max(keypoints)\n      max_assert = tf.Assert(tf.greater(max_val, 1.01),\n                             [\'max value is lower than 1.01: \', max_val])\n      with tf.control_dependencies([max_assert]):\n        width = tf.identity(width)\n\n    return scale(keypoints, 1.0 / height, 1.0 / width)\n\n\ndef to_absolute_coordinates(keypoints, height, width,\n                            check_range=True, scope=None):\n  """"""Converts normalized keypoint coordinates to absolute pixel coordinates.\n\n  This function raises an assertion failed error when the maximum keypoint\n  coordinate value is larger than 1.01 (in which case coordinates are already\n  absolute).\n\n  Args:\n    keypoints: A tensor of shape [num_instances, num_keypoints, 2]\n    height: Maximum value for y coordinate of absolute keypoint coordinates.\n    width: Maximum value for x coordinate of absolute keypoint coordinates.\n    check_range: If True, checks if the coordinates are normalized or not.\n    scope: name scope.\n\n  Returns:\n    tensor of shape [num_instances, num_keypoints, 2] with absolute coordinates\n    in terms of the image size.\n\n  """"""\n  with tf.name_scope(scope, \'ToAbsoluteCoordinates\'):\n    height = tf.cast(height, tf.float32)\n    width = tf.cast(width, tf.float32)\n\n    # Ensure range of input keypoints is correct.\n    if check_range:\n      max_val = tf.reduce_max(keypoints)\n      max_assert = tf.Assert(tf.greater_equal(1.01, max_val),\n                             [\'maximum keypoint coordinate value is larger \'\n                              \'than 1.01: \', max_val])\n      with tf.control_dependencies([max_assert]):\n        width = tf.identity(width)\n\n    return scale(keypoints, height, width)\n\n\ndef flip_horizontal(keypoints, flip_point, flip_permutation, scope=None):\n  """"""Flips the keypoints horizontally around the flip_point.\n\n  This operation flips the x coordinate for each keypoint around the flip_point\n  and also permutes the keypoints in a manner specified by flip_permutation.\n\n  Args:\n    keypoints: a tensor of shape [num_instances, num_keypoints, 2]\n    flip_point:  (float) scalar tensor representing the x coordinate to flip the\n      keypoints around.\n    flip_permutation: rank 1 int32 tensor containing the keypoint flip\n      permutation. This specifies the mapping from original keypoint indices\n      to the flipped keypoint indices. This is used primarily for keypoints\n      that are not reflection invariant. E.g. Suppose there are 3 keypoints\n      representing [\'head\', \'right_eye\', \'left_eye\'], then a logical choice for\n      flip_permutation might be [0, 2, 1] since we want to swap the \'left_eye\'\n      and \'right_eye\' after a horizontal flip.\n    scope: name scope.\n\n  Returns:\n    new_keypoints: a tensor of shape [num_instances, num_keypoints, 2]\n  """"""\n  with tf.name_scope(scope, \'FlipHorizontal\'):\n    keypoints = tf.transpose(keypoints, [1, 0, 2])\n    keypoints = tf.gather(keypoints, flip_permutation)\n    v, u = tf.split(value=keypoints, num_or_size_splits=2, axis=2)\n    u = flip_point * 2.0 - u\n    new_keypoints = tf.concat([v, u], 2)\n    new_keypoints = tf.transpose(new_keypoints, [1, 0, 2])\n    return new_keypoints\n\n\ndef flip_vertical(keypoints, flip_point, flip_permutation, scope=None):\n  """"""Flips the keypoints vertically around the flip_point.\n\n  This operation flips the y coordinate for each keypoint around the flip_point\n  and also permutes the keypoints in a manner specified by flip_permutation.\n\n  Args:\n    keypoints: a tensor of shape [num_instances, num_keypoints, 2]\n    flip_point:  (float) scalar tensor representing the y coordinate to flip the\n      keypoints around.\n    flip_permutation: rank 1 int32 tensor containing the keypoint flip\n      permutation. This specifies the mapping from original keypoint indices\n      to the flipped keypoint indices. This is used primarily for keypoints\n      that are not reflection invariant. E.g. Suppose there are 3 keypoints\n      representing [\'head\', \'right_eye\', \'left_eye\'], then a logical choice for\n      flip_permutation might be [0, 2, 1] since we want to swap the \'left_eye\'\n      and \'right_eye\' after a horizontal flip.\n    scope: name scope.\n\n  Returns:\n    new_keypoints: a tensor of shape [num_instances, num_keypoints, 2]\n  """"""\n  with tf.name_scope(scope, \'FlipVertical\'):\n    keypoints = tf.transpose(keypoints, [1, 0, 2])\n    keypoints = tf.gather(keypoints, flip_permutation)\n    v, u = tf.split(value=keypoints, num_or_size_splits=2, axis=2)\n    v = flip_point * 2.0 - v\n    new_keypoints = tf.concat([v, u], 2)\n    new_keypoints = tf.transpose(new_keypoints, [1, 0, 2])\n    return new_keypoints\n\n\ndef rot90(keypoints, scope=None):\n  """"""Rotates the keypoints counter-clockwise by 90 degrees.\n\n  Args:\n    keypoints: a tensor of shape [num_instances, num_keypoints, 2]\n    scope: name scope.\n\n  Returns:\n    new_keypoints: a tensor of shape [num_instances, num_keypoints, 2]\n  """"""\n  with tf.name_scope(scope, \'Rot90\'):\n    keypoints = tf.transpose(keypoints, [1, 0, 2])\n    v, u = tf.split(value=keypoints[:, :, ::-1], num_or_size_splits=2, axis=2)\n    v = 1.0 - v\n    new_keypoints = tf.concat([v, u], 2)\n    new_keypoints = tf.transpose(new_keypoints, [1, 0, 2])\n    return new_keypoints\n'"
src/object_detection/core/keypoint_ops_test.py,27,"b'# Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n""""""Tests for object_detection.core.keypoint_ops.""""""\nimport numpy as np\nimport tensorflow as tf\n\nfrom object_detection.core import keypoint_ops\n\n\nclass KeypointOpsTest(tf.test.TestCase):\n  """"""Tests for common keypoint operations.""""""\n\n  def test_scale(self):\n    keypoints = tf.constant([\n        [[0.0, 0.0], [100.0, 200.0]],\n        [[50.0, 120.0], [100.0, 140.0]]\n    ])\n    y_scale = tf.constant(1.0 / 100)\n    x_scale = tf.constant(1.0 / 200)\n\n    expected_keypoints = tf.constant([\n        [[0., 0.], [1.0, 1.0]],\n        [[0.5, 0.6], [1.0, 0.7]]\n    ])\n    output = keypoint_ops.scale(keypoints, y_scale, x_scale)\n\n    with self.test_session() as sess:\n      output_, expected_keypoints_ = sess.run([output, expected_keypoints])\n      self.assertAllClose(output_, expected_keypoints_)\n\n  def test_clip_to_window(self):\n    keypoints = tf.constant([\n        [[0.25, 0.5], [0.75, 0.75]],\n        [[0.5, 0.0], [1.0, 1.0]]\n    ])\n    window = tf.constant([0.25, 0.25, 0.75, 0.75])\n\n    expected_keypoints = tf.constant([\n        [[0.25, 0.5], [0.75, 0.75]],\n        [[0.5, 0.25], [0.75, 0.75]]\n    ])\n    output = keypoint_ops.clip_to_window(keypoints, window)\n\n    with self.test_session() as sess:\n      output_, expected_keypoints_ = sess.run([output, expected_keypoints])\n      self.assertAllClose(output_, expected_keypoints_)\n\n  def test_prune_outside_window(self):\n    keypoints = tf.constant([\n        [[0.25, 0.5], [0.75, 0.75]],\n        [[0.5, 0.0], [1.0, 1.0]]\n    ])\n    window = tf.constant([0.25, 0.25, 0.75, 0.75])\n\n    expected_keypoints = tf.constant([[[0.25, 0.5], [0.75, 0.75]],\n                                      [[np.nan, np.nan], [np.nan, np.nan]]])\n    output = keypoint_ops.prune_outside_window(keypoints, window)\n\n    with self.test_session() as sess:\n      output_, expected_keypoints_ = sess.run([output, expected_keypoints])\n      self.assertAllClose(output_, expected_keypoints_)\n\n  def test_change_coordinate_frame(self):\n    keypoints = tf.constant([\n        [[0.25, 0.5], [0.75, 0.75]],\n        [[0.5, 0.0], [1.0, 1.0]]\n    ])\n    window = tf.constant([0.25, 0.25, 0.75, 0.75])\n\n    expected_keypoints = tf.constant([\n        [[0, 0.5], [1.0, 1.0]],\n        [[0.5, -0.5], [1.5, 1.5]]\n    ])\n    output = keypoint_ops.change_coordinate_frame(keypoints, window)\n\n    with self.test_session() as sess:\n      output_, expected_keypoints_ = sess.run([output, expected_keypoints])\n      self.assertAllClose(output_, expected_keypoints_)\n\n  def test_to_normalized_coordinates(self):\n    keypoints = tf.constant([\n        [[10., 30.], [30., 45.]],\n        [[20., 0.], [40., 60.]]\n    ])\n    output = keypoint_ops.to_normalized_coordinates(\n        keypoints, 40, 60)\n    expected_keypoints = tf.constant([\n        [[0.25, 0.5], [0.75, 0.75]],\n        [[0.5, 0.0], [1.0, 1.0]]\n    ])\n\n    with self.test_session() as sess:\n      output_, expected_keypoints_ = sess.run([output, expected_keypoints])\n      self.assertAllClose(output_, expected_keypoints_)\n\n  def test_to_normalized_coordinates_already_normalized(self):\n    keypoints = tf.constant([\n        [[0.25, 0.5], [0.75, 0.75]],\n        [[0.5, 0.0], [1.0, 1.0]]\n    ])\n    output = keypoint_ops.to_normalized_coordinates(\n        keypoints, 40, 60)\n\n    with self.test_session() as sess:\n      with self.assertRaisesOpError(\'assertion failed\'):\n        sess.run(output)\n\n  def test_to_absolute_coordinates(self):\n    keypoints = tf.constant([\n        [[0.25, 0.5], [0.75, 0.75]],\n        [[0.5, 0.0], [1.0, 1.0]]\n    ])\n    output = keypoint_ops.to_absolute_coordinates(\n        keypoints, 40, 60)\n    expected_keypoints = tf.constant([\n        [[10., 30.], [30., 45.]],\n        [[20., 0.], [40., 60.]]\n    ])\n\n    with self.test_session() as sess:\n      output_, expected_keypoints_ = sess.run([output, expected_keypoints])\n      self.assertAllClose(output_, expected_keypoints_)\n\n  def test_to_absolute_coordinates_already_absolute(self):\n    keypoints = tf.constant([\n        [[10., 30.], [30., 45.]],\n        [[20., 0.], [40., 60.]]\n    ])\n    output = keypoint_ops.to_absolute_coordinates(\n        keypoints, 40, 60)\n\n    with self.test_session() as sess:\n      with self.assertRaisesOpError(\'assertion failed\'):\n        sess.run(output)\n\n  def test_flip_horizontal(self):\n    keypoints = tf.constant([\n        [[0.1, 0.1], [0.2, 0.2], [0.3, 0.3]],\n        [[0.4, 0.4], [0.5, 0.5], [0.6, 0.6]]\n    ])\n    flip_permutation = [0, 2, 1]\n\n    expected_keypoints = tf.constant([\n        [[0.1, 0.9], [0.3, 0.7], [0.2, 0.8]],\n        [[0.4, 0.6], [0.6, 0.4], [0.5, 0.5]],\n    ])\n    output = keypoint_ops.flip_horizontal(keypoints, 0.5, flip_permutation)\n\n    with self.test_session() as sess:\n      output_, expected_keypoints_ = sess.run([output, expected_keypoints])\n      self.assertAllClose(output_, expected_keypoints_)\n\n  def test_flip_vertical(self):\n    keypoints = tf.constant([\n        [[0.1, 0.1], [0.2, 0.2], [0.3, 0.3]],\n        [[0.4, 0.4], [0.5, 0.5], [0.6, 0.6]]\n    ])\n    flip_permutation = [0, 2, 1]\n\n    expected_keypoints = tf.constant([\n        [[0.9, 0.1], [0.7, 0.3], [0.8, 0.2]],\n        [[0.6, 0.4], [0.4, 0.6], [0.5, 0.5]],\n    ])\n    output = keypoint_ops.flip_vertical(keypoints, 0.5, flip_permutation)\n\n    with self.test_session() as sess:\n      output_, expected_keypoints_ = sess.run([output, expected_keypoints])\n      self.assertAllClose(output_, expected_keypoints_)\n\n  def test_rot90(self):\n    keypoints = tf.constant([\n        [[0.1, 0.1], [0.2, 0.2], [0.3, 0.3]],\n        [[0.4, 0.6], [0.5, 0.6], [0.6, 0.7]]\n    ])\n    expected_keypoints = tf.constant([\n        [[0.9, 0.1], [0.8, 0.2], [0.7, 0.3]],\n        [[0.4, 0.4], [0.4, 0.5], [0.3, 0.6]],\n    ])\n    output = keypoint_ops.rot90(keypoints)\n\n    with self.test_session() as sess:\n      output_, expected_keypoints_ = sess.run([output, expected_keypoints])\n      self.assertAllClose(output_, expected_keypoints_)\n\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
src/object_detection/core/losses.py,55,"b'# Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n""""""Classification and regression loss functions for object detection.\n\nLocalization losses:\n * WeightedL2LocalizationLoss\n * WeightedSmoothL1LocalizationLoss\n * WeightedIOULocalizationLoss\n\nClassification losses:\n * WeightedSigmoidClassificationLoss\n * WeightedSoftmaxClassificationLoss\n * BootstrappedSigmoidClassificationLoss\n""""""\nfrom abc import ABCMeta\nfrom abc import abstractmethod\n\nimport tensorflow as tf\n\nfrom object_detection.core import box_list\nfrom object_detection.core import box_list_ops\nfrom object_detection.utils import ops\n\nslim = tf.contrib.slim\n\n\nclass Loss(object):\n  """"""Abstract base class for loss functions.""""""\n  __metaclass__ = ABCMeta\n\n  def __call__(self,\n               prediction_tensor,\n               target_tensor,\n               ignore_nan_targets=False,\n               scope=None,\n               **params):\n    """"""Call the loss function.\n\n    Args:\n      prediction_tensor: an N-d tensor of shape [batch, anchors, ...]\n        representing predicted quantities.\n      target_tensor: an N-d tensor of shape [batch, anchors, ...] representing\n        regression or classification targets.\n      ignore_nan_targets: whether to ignore nan targets in the loss computation.\n        E.g. can be used if the target tensor is missing groundtruth data that\n        shouldn\'t be factored into the loss.\n      scope: Op scope name. Defaults to \'Loss\' if None.\n      **params: Additional keyword arguments for specific implementations of\n              the Loss.\n\n    Returns:\n      loss: a tensor representing the value of the loss function.\n    """"""\n    with tf.name_scope(scope, \'Loss\',\n                       [prediction_tensor, target_tensor, params]) as scope:\n      if ignore_nan_targets:\n        target_tensor = tf.where(tf.is_nan(target_tensor),\n                                 prediction_tensor,\n                                 target_tensor)\n      return self._compute_loss(prediction_tensor, target_tensor, **params)\n\n  @abstractmethod\n  def _compute_loss(self, prediction_tensor, target_tensor, **params):\n    """"""Method to be overridden by implementations.\n\n    Args:\n      prediction_tensor: a tensor representing predicted quantities\n      target_tensor: a tensor representing regression or classification targets\n      **params: Additional keyword arguments for specific implementations of\n              the Loss.\n\n    Returns:\n      loss: an N-d tensor of shape [batch, anchors, ...] containing the loss per\n        anchor\n    """"""\n    pass\n\n\nclass WeightedL2LocalizationLoss(Loss):\n  """"""L2 localization loss function with anchorwise output support.\n\n  Loss[b,a] = .5 * ||weights[b,a] * (prediction[b,a,:] - target[b,a,:])||^2\n  """"""\n\n  def _compute_loss(self, prediction_tensor, target_tensor, weights):\n    """"""Compute loss function.\n\n    Args:\n      prediction_tensor: A float tensor of shape [batch_size, num_anchors,\n        code_size] representing the (encoded) predicted locations of objects.\n      target_tensor: A float tensor of shape [batch_size, num_anchors,\n        code_size] representing the regression targets\n      weights: a float tensor of shape [batch_size, num_anchors]\n\n    Returns:\n      loss: a float tensor of shape [batch_size, num_anchors] tensor\n        representing the value of the loss function.\n    """"""\n    weighted_diff = (prediction_tensor - target_tensor) * tf.expand_dims(\n        weights, 2)\n    square_diff = 0.5 * tf.square(weighted_diff)\n    return tf.reduce_sum(square_diff, 2)\n\n\nclass WeightedSmoothL1LocalizationLoss(Loss):\n  """"""Smooth L1 localization loss function aka Huber Loss..\n\n  The smooth L1_loss is defined elementwise as .5 x^2 if |x| <= delta and\n  0.5 x^2 + delta * (|x|-delta) otherwise, where x is the difference between\n  predictions and target.\n\n  See also Equation (3) in the Fast R-CNN paper by Ross Girshick (ICCV 2015)\n  """"""\n\n  def __init__(self, delta=1.0):\n    """"""Constructor.\n\n    Args:\n      delta: delta for smooth L1 loss.\n    """"""\n    self._delta = delta\n\n  def _compute_loss(self, prediction_tensor, target_tensor, weights):\n    """"""Compute loss function.\n\n    Args:\n      prediction_tensor: A float tensor of shape [batch_size, num_anchors,\n        code_size] representing the (encoded) predicted locations of objects.\n      target_tensor: A float tensor of shape [batch_size, num_anchors,\n        code_size] representing the regression targets\n      weights: a float tensor of shape [batch_size, num_anchors]\n\n    Returns:\n      loss: a float tensor of shape [batch_size, num_anchors] tensor\n        representing the value of the loss function.\n    """"""\n    return tf.reduce_sum(tf.losses.huber_loss(\n        target_tensor,\n        prediction_tensor,\n        delta=self._delta,\n        weights=tf.expand_dims(weights, axis=2),\n        loss_collection=None,\n        reduction=tf.losses.Reduction.NONE\n    ), axis=2)\n\n\nclass WeightedIOULocalizationLoss(Loss):\n  """"""IOU localization loss function.\n\n  Sums the IOU for corresponding pairs of predicted/groundtruth boxes\n  and for each pair assign a loss of 1 - IOU.  We then compute a weighted\n  sum over all pairs which is returned as the total loss.\n  """"""\n\n  def _compute_loss(self, prediction_tensor, target_tensor, weights):\n    """"""Compute loss function.\n\n    Args:\n      prediction_tensor: A float tensor of shape [batch_size, num_anchors, 4]\n        representing the decoded predicted boxes\n      target_tensor: A float tensor of shape [batch_size, num_anchors, 4]\n        representing the decoded target boxes\n      weights: a float tensor of shape [batch_size, num_anchors]\n\n    Returns:\n      loss: a float tensor of shape [batch_size, num_anchors] tensor\n        representing the value of the loss function.\n    """"""\n    predicted_boxes = box_list.BoxList(tf.reshape(prediction_tensor, [-1, 4]))\n    target_boxes = box_list.BoxList(tf.reshape(target_tensor, [-1, 4]))\n    per_anchor_iou_loss = 1.0 - box_list_ops.matched_iou(predicted_boxes,\n                                                         target_boxes)\n    return tf.reshape(weights, [-1]) * per_anchor_iou_loss\n\n\nclass WeightedSigmoidClassificationLoss(Loss):\n  """"""Sigmoid cross entropy classification loss function.""""""\n\n  def _compute_loss(self,\n                    prediction_tensor,\n                    target_tensor,\n                    weights,\n                    class_indices=None):\n    """"""Compute loss function.\n\n    Args:\n      prediction_tensor: A float tensor of shape [batch_size, num_anchors,\n        num_classes] representing the predicted logits for each class\n      target_tensor: A float tensor of shape [batch_size, num_anchors,\n        num_classes] representing one-hot encoded classification targets\n      weights: a float tensor of shape [batch_size, num_anchors]\n      class_indices: (Optional) A 1-D integer tensor of class indices.\n        If provided, computes loss only for the specified class indices.\n\n    Returns:\n      loss: a float tensor of shape [batch_size, num_anchors, num_classes]\n        representing the value of the loss function.\n    """"""\n    weights = tf.expand_dims(weights, 2)\n    if class_indices is not None:\n      weights *= tf.reshape(\n          ops.indices_to_dense_vector(class_indices,\n                                      tf.shape(prediction_tensor)[2]),\n          [1, 1, -1])\n    per_entry_cross_ent = (tf.nn.sigmoid_cross_entropy_with_logits(\n        labels=target_tensor, logits=prediction_tensor))\n    return per_entry_cross_ent * weights\n\n\nclass SigmoidFocalClassificationLoss(Loss):\n  """"""Sigmoid focal cross entropy loss.\n\n  Focal loss down-weights well classified examples and focusses on the hard\n  examples. See https://arxiv.org/pdf/1708.02002.pdf for the loss definition.\n  """"""\n\n  def __init__(self, gamma=2.0, alpha=0.25):\n    """"""Constructor.\n\n    Args:\n      gamma: exponent of the modulating factor (1 - p_t) ^ gamma.\n      alpha: optional alpha weighting factor to balance positives vs negatives.\n    """"""\n    self._alpha = alpha\n    self._gamma = gamma\n\n  def _compute_loss(self,\n                    prediction_tensor,\n                    target_tensor,\n                    weights,\n                    class_indices=None):\n    """"""Compute loss function.\n\n    Args:\n      prediction_tensor: A float tensor of shape [batch_size, num_anchors,\n        num_classes] representing the predicted logits for each class\n      target_tensor: A float tensor of shape [batch_size, num_anchors,\n        num_classes] representing one-hot encoded classification targets\n      weights: a float tensor of shape [batch_size, num_anchors]\n      class_indices: (Optional) A 1-D integer tensor of class indices.\n        If provided, computes loss only for the specified class indices.\n\n    Returns:\n      loss: a float tensor of shape [batch_size, num_anchors, num_classes]\n        representing the value of the loss function.\n    """"""\n    weights = tf.expand_dims(weights, 2)\n    if class_indices is not None:\n      weights *= tf.reshape(\n          ops.indices_to_dense_vector(class_indices,\n                                      tf.shape(prediction_tensor)[2]),\n          [1, 1, -1])\n    per_entry_cross_ent = (tf.nn.sigmoid_cross_entropy_with_logits(\n        labels=target_tensor, logits=prediction_tensor))\n    prediction_probabilities = tf.sigmoid(prediction_tensor)\n    p_t = ((target_tensor * prediction_probabilities) +\n           ((1 - target_tensor) * (1 - prediction_probabilities)))\n    modulating_factor = 1.0\n    if self._gamma:\n      modulating_factor = tf.pow(1.0 - p_t, self._gamma)\n    alpha_weight_factor = 1.0\n    if self._alpha is not None:\n      alpha_weight_factor = (target_tensor * self._alpha +\n                             (1 - target_tensor) * (1 - self._alpha))\n    focal_cross_entropy_loss = (modulating_factor * alpha_weight_factor *\n                                per_entry_cross_ent)\n    return focal_cross_entropy_loss * weights\n\n\nclass WeightedSoftmaxClassificationLoss(Loss):\n  """"""Softmax loss function.""""""\n\n  def __init__(self, logit_scale=1.0):\n    """"""Constructor.\n\n    Args:\n      logit_scale: When this value is high, the prediction is ""diffused"" and\n                   when this value is low, the prediction is made peakier.\n                   (default 1.0)\n\n    """"""\n    self._logit_scale = logit_scale\n\n  def _compute_loss(self, prediction_tensor, target_tensor, weights):\n    """"""Compute loss function.\n\n    Args:\n      prediction_tensor: A float tensor of shape [batch_size, num_anchors,\n        num_classes] representing the predicted logits for each class\n      target_tensor: A float tensor of shape [batch_size, num_anchors,\n        num_classes] representing one-hot encoded classification targets\n      weights: a float tensor of shape [batch_size, num_anchors]\n\n    Returns:\n      loss: a float tensor of shape [batch_size, num_anchors]\n        representing the value of the loss function.\n    """"""\n    num_classes = prediction_tensor.get_shape().as_list()[-1]\n    prediction_tensor = tf.divide(\n        prediction_tensor, self._logit_scale, name=\'scale_logit\')\n    per_row_cross_ent = (tf.nn.softmax_cross_entropy_with_logits(\n        labels=tf.reshape(target_tensor, [-1, num_classes]),\n        logits=tf.reshape(prediction_tensor, [-1, num_classes])))\n    return tf.reshape(per_row_cross_ent, tf.shape(weights)) * weights\n\n\nclass BootstrappedSigmoidClassificationLoss(Loss):\n  """"""Bootstrapped sigmoid cross entropy classification loss function.\n\n  This loss uses a convex combination of training labels and the current model\'s\n  predictions as training targets in the classification loss. The idea is that\n  as the model improves over time, its predictions can be trusted more and we\n  can use these predictions to mitigate the damage of noisy/incorrect labels,\n  because incorrect labels are likely to be eventually highly inconsistent with\n  other stimuli predicted to have the same label by the model.\n\n  In ""soft"" bootstrapping, we use all predicted class probabilities, whereas in\n  ""hard"" bootstrapping, we use the single class favored by the model.\n\n  See also Training Deep Neural Networks On Noisy Labels with Bootstrapping by\n  Reed et al. (ICLR 2015).\n  """"""\n\n  def __init__(self, alpha, bootstrap_type=\'soft\'):\n    """"""Constructor.\n\n    Args:\n      alpha: a float32 scalar tensor between 0 and 1 representing interpolation\n        weight\n      bootstrap_type: set to either \'hard\' or \'soft\' (default)\n\n    Raises:\n      ValueError: if bootstrap_type is not either \'hard\' or \'soft\'\n    """"""\n    if bootstrap_type != \'hard\' and bootstrap_type != \'soft\':\n      raise ValueError(\'Unrecognized bootstrap_type: must be one of \'\n                       \'\\\'hard\\\' or \\\'soft.\\\'\')\n    self._alpha = alpha\n    self._bootstrap_type = bootstrap_type\n\n  def _compute_loss(self, prediction_tensor, target_tensor, weights):\n    """"""Compute loss function.\n\n    Args:\n      prediction_tensor: A float tensor of shape [batch_size, num_anchors,\n        num_classes] representing the predicted logits for each class\n      target_tensor: A float tensor of shape [batch_size, num_anchors,\n        num_classes] representing one-hot encoded classification targets\n      weights: a float tensor of shape [batch_size, num_anchors]\n\n    Returns:\n      loss: a float tensor of shape [batch_size, num_anchors, num_classes]\n        representing the value of the loss function.\n    """"""\n    if self._bootstrap_type == \'soft\':\n      bootstrap_target_tensor = self._alpha * target_tensor + (\n          1.0 - self._alpha) * tf.sigmoid(prediction_tensor)\n    else:\n      bootstrap_target_tensor = self._alpha * target_tensor + (\n          1.0 - self._alpha) * tf.cast(\n              tf.sigmoid(prediction_tensor) > 0.5, tf.float32)\n    per_entry_cross_ent = (tf.nn.sigmoid_cross_entropy_with_logits(\n        labels=bootstrap_target_tensor, logits=prediction_tensor))\n    return per_entry_cross_ent * tf.expand_dims(weights, 2)\n\n\nclass HardExampleMiner(object):\n  """"""Hard example mining for regions in a list of images.\n\n  Implements hard example mining to select a subset of regions to be\n  back-propagated. For each image, selects the regions with highest losses,\n  subject to the condition that a newly selected region cannot have\n  an IOU > iou_threshold with any of the previously selected regions.\n  This can be achieved by re-using a greedy non-maximum suppression algorithm.\n  A constraint on the number of negatives mined per positive region can also be\n  enforced.\n\n  Reference papers: ""Training Region-based Object Detectors with Online\n  Hard Example Mining"" (CVPR 2016) by Srivastava et al., and\n  ""SSD: Single Shot MultiBox Detector"" (ECCV 2016) by Liu et al.\n  """"""\n\n  def __init__(self,\n               num_hard_examples=64,\n               iou_threshold=0.7,\n               loss_type=\'both\',\n               cls_loss_weight=0.05,\n               loc_loss_weight=0.06,\n               max_negatives_per_positive=None,\n               min_negatives_per_image=0):\n    """"""Constructor.\n\n    The hard example mining implemented by this class can replicate the behavior\n    in the two aforementioned papers (Srivastava et al., and Liu et al).\n    To replicate the A2 paper (Srivastava et al), num_hard_examples is set\n    to a fixed parameter (64 by default) and iou_threshold is set to .7 for\n    running non-max-suppression the predicted boxes prior to hard mining.\n    In order to replicate the SSD paper (Liu et al), num_hard_examples should\n    be set to None, max_negatives_per_positive should be 3 and iou_threshold\n    should be 1.0 (in order to effectively turn off NMS).\n\n    Args:\n      num_hard_examples: maximum number of hard examples to be\n        selected per image (prior to enforcing max negative to positive ratio\n        constraint).  If set to None, all examples obtained after NMS are\n        considered.\n      iou_threshold: minimum intersection over union for an example\n        to be discarded during NMS.\n      loss_type: use only classification losses (\'cls\', default),\n        localization losses (\'loc\') or both losses (\'both\').\n        In the last case, cls_loss_weight and loc_loss_weight are used to\n        compute weighted sum of the two losses.\n      cls_loss_weight: weight for classification loss.\n      loc_loss_weight: weight for location loss.\n      max_negatives_per_positive: maximum number of negatives to retain for\n        each positive anchor. By default, num_negatives_per_positive is None,\n        which means that we do not enforce a prespecified negative:positive\n        ratio.  Note also that num_negatives_per_positives can be a float\n        (and will be converted to be a float even if it is passed in otherwise).\n      min_negatives_per_image: minimum number of negative anchors to sample for\n        a given image. Setting this to a positive number allows sampling\n        negatives in an image without any positive anchors and thus not biased\n        towards at least one detection per image.\n    """"""\n    self._num_hard_examples = num_hard_examples\n    self._iou_threshold = iou_threshold\n    self._loss_type = loss_type\n    self._cls_loss_weight = cls_loss_weight\n    self._loc_loss_weight = loc_loss_weight\n    self._max_negatives_per_positive = max_negatives_per_positive\n    self._min_negatives_per_image = min_negatives_per_image\n    if self._max_negatives_per_positive is not None:\n      self._max_negatives_per_positive = float(self._max_negatives_per_positive)\n    self._num_positives_list = None\n    self._num_negatives_list = None\n\n  def __call__(self,\n               location_losses,\n               cls_losses,\n               decoded_boxlist_list,\n               match_list=None):\n    """"""Computes localization and classification losses after hard mining.\n\n    Args:\n      location_losses: a float tensor of shape [num_images, num_anchors]\n        representing anchorwise localization losses.\n      cls_losses: a float tensor of shape [num_images, num_anchors]\n        representing anchorwise classification losses.\n      decoded_boxlist_list: a list of decoded BoxList representing location\n        predictions for each image.\n      match_list: an optional list of matcher.Match objects encoding the match\n        between anchors and groundtruth boxes for each image of the batch,\n        with rows of the Match objects corresponding to groundtruth boxes\n        and columns corresponding to anchors.  Match objects in match_list are\n        used to reference which anchors are positive, negative or ignored.  If\n        self._max_negatives_per_positive exists, these are then used to enforce\n        a prespecified negative to positive ratio.\n\n    Returns:\n      mined_location_loss: a float scalar with sum of localization losses from\n        selected hard examples.\n      mined_cls_loss: a float scalar with sum of classification losses from\n        selected hard examples.\n    Raises:\n      ValueError: if location_losses, cls_losses and decoded_boxlist_list do\n        not have compatible shapes (i.e., they must correspond to the same\n        number of images).\n      ValueError: if match_list is specified but its length does not match\n        len(decoded_boxlist_list).\n    """"""\n    mined_location_losses = []\n    mined_cls_losses = []\n    location_losses = tf.unstack(location_losses)\n    cls_losses = tf.unstack(cls_losses)\n    num_images = len(decoded_boxlist_list)\n    if not match_list:\n      match_list = num_images * [None]\n    if not len(location_losses) == len(decoded_boxlist_list) == len(cls_losses):\n      raise ValueError(\'location_losses, cls_losses and decoded_boxlist_list \'\n                       \'do not have compatible shapes.\')\n    if not isinstance(match_list, list):\n      raise ValueError(\'match_list must be a list.\')\n    if len(match_list) != len(decoded_boxlist_list):\n      raise ValueError(\'match_list must either be None or have \'\n                       \'length=len(decoded_boxlist_list).\')\n    num_positives_list = []\n    num_negatives_list = []\n    for ind, detection_boxlist in enumerate(decoded_boxlist_list):\n      box_locations = detection_boxlist.get()\n      match = match_list[ind]\n      image_losses = cls_losses[ind]\n      if self._loss_type == \'loc\':\n        image_losses = location_losses[ind]\n      elif self._loss_type == \'both\':\n        image_losses *= self._cls_loss_weight\n        image_losses += location_losses[ind] * self._loc_loss_weight\n      if self._num_hard_examples is not None:\n        num_hard_examples = self._num_hard_examples\n      else:\n        num_hard_examples = detection_boxlist.num_boxes()\n      selected_indices = tf.image.non_max_suppression(\n          box_locations, image_losses, num_hard_examples, self._iou_threshold)\n      if self._max_negatives_per_positive is not None and match:\n        (selected_indices, num_positives,\n         num_negatives) = self._subsample_selection_to_desired_neg_pos_ratio(\n             selected_indices, match, self._max_negatives_per_positive,\n             self._min_negatives_per_image)\n        num_positives_list.append(num_positives)\n        num_negatives_list.append(num_negatives)\n      mined_location_losses.append(\n          tf.reduce_sum(tf.gather(location_losses[ind], selected_indices)))\n      mined_cls_losses.append(\n          tf.reduce_sum(tf.gather(cls_losses[ind], selected_indices)))\n    location_loss = tf.reduce_sum(tf.stack(mined_location_losses))\n    cls_loss = tf.reduce_sum(tf.stack(mined_cls_losses))\n    if match and self._max_negatives_per_positive:\n      self._num_positives_list = num_positives_list\n      self._num_negatives_list = num_negatives_list\n    return (location_loss, cls_loss)\n\n  def summarize(self):\n    """"""Summarize the number of positives and negatives after mining.""""""\n    if self._num_positives_list and self._num_negatives_list:\n      avg_num_positives = tf.reduce_mean(tf.to_float(self._num_positives_list))\n      avg_num_negatives = tf.reduce_mean(tf.to_float(self._num_negatives_list))\n      tf.summary.scalar(\'HardExampleMiner/NumPositives\', avg_num_positives)\n      tf.summary.scalar(\'HardExampleMiner/NumNegatives\', avg_num_negatives)\n\n  def _subsample_selection_to_desired_neg_pos_ratio(self,\n                                                    indices,\n                                                    match,\n                                                    max_negatives_per_positive,\n                                                    min_negatives_per_image=0):\n    """"""Subsample a collection of selected indices to a desired neg:pos ratio.\n\n    This function takes a subset of M indices (indexing into a large anchor\n    collection of N anchors where M<N) which are labeled as positive/negative\n    via a Match object (matched indices are positive, unmatched indices\n    are negative).  It returns a subset of the provided indices retaining all\n    positives as well as up to the first K negatives, where:\n      K=floor(num_negative_per_positive * num_positives).\n\n    For example, if indices=[2, 4, 5, 7, 9, 10] (indexing into 12 anchors),\n    with positives=[2, 5] and negatives=[4, 7, 9, 10] and\n    num_negatives_per_positive=1, then the returned subset of indices\n    is [2, 4, 5, 7].\n\n    Args:\n      indices: An integer tensor of shape [M] representing a collection\n        of selected anchor indices\n      match: A matcher.Match object encoding the match between anchors and\n        groundtruth boxes for a given image, with rows of the Match objects\n        corresponding to groundtruth boxes and columns corresponding to anchors.\n      max_negatives_per_positive: (float) maximum number of negatives for\n        each positive anchor.\n      min_negatives_per_image: minimum number of negative anchors for a given\n        image. Allow sampling negatives in image without any positive anchors.\n\n    Returns:\n      selected_indices: An integer tensor of shape [M\'] representing a\n        collection of selected anchor indices with M\' <= M.\n      num_positives: An integer tensor representing the number of positive\n        examples in selected set of indices.\n      num_negatives: An integer tensor representing the number of negative\n        examples in selected set of indices.\n    """"""\n    positives_indicator = tf.gather(match.matched_column_indicator(), indices)\n    negatives_indicator = tf.gather(match.unmatched_column_indicator(), indices)\n    num_positives = tf.reduce_sum(tf.to_int32(positives_indicator))\n    max_negatives = tf.maximum(min_negatives_per_image,\n                               tf.to_int32(max_negatives_per_positive *\n                                           tf.to_float(num_positives)))\n    topk_negatives_indicator = tf.less_equal(\n        tf.cumsum(tf.to_int32(negatives_indicator)), max_negatives)\n    subsampled_selection_indices = tf.where(\n        tf.logical_or(positives_indicator, topk_negatives_indicator))\n    num_negatives = tf.size(subsampled_selection_indices) - num_positives\n    return (tf.reshape(tf.gather(indices, subsampled_selection_indices), [-1]),\n            num_positives, num_negatives)\n'"
src/object_detection/core/losses_test.py,190,"b'# Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n""""""Tests for google3.research.vale.object_detection.losses.""""""\nimport math\n\nimport numpy as np\nimport tensorflow as tf\n\nfrom object_detection.core import box_list\nfrom object_detection.core import losses\nfrom object_detection.core import matcher\n\n\nclass WeightedL2LocalizationLossTest(tf.test.TestCase):\n\n  def testReturnsCorrectWeightedLoss(self):\n    batch_size = 3\n    num_anchors = 10\n    code_size = 4\n    prediction_tensor = tf.ones([batch_size, num_anchors, code_size])\n    target_tensor = tf.zeros([batch_size, num_anchors, code_size])\n    weights = tf.constant([[1, 1, 1, 1, 1, 0, 0, 0, 0, 0],\n                           [1, 1, 1, 1, 1, 0, 0, 0, 0, 0],\n                           [1, 1, 1, 1, 1, 0, 0, 0, 0, 0]], tf.float32)\n    loss_op = losses.WeightedL2LocalizationLoss()\n    loss = tf.reduce_sum(loss_op(prediction_tensor, target_tensor,\n                                 weights=weights))\n\n    expected_loss = (3 * 5 * 4) / 2.0\n    with self.test_session() as sess:\n      loss_output = sess.run(loss)\n      self.assertAllClose(loss_output, expected_loss)\n\n  def testReturnsCorrectAnchorwiseLoss(self):\n    batch_size = 3\n    num_anchors = 16\n    code_size = 4\n    prediction_tensor = tf.ones([batch_size, num_anchors, code_size])\n    target_tensor = tf.zeros([batch_size, num_anchors, code_size])\n    weights = tf.ones([batch_size, num_anchors])\n    loss_op = losses.WeightedL2LocalizationLoss()\n    loss = loss_op(prediction_tensor, target_tensor, weights=weights)\n\n    expected_loss = np.ones((batch_size, num_anchors)) * 2\n    with self.test_session() as sess:\n      loss_output = sess.run(loss)\n      self.assertAllClose(loss_output, expected_loss)\n\n  def testReturnsCorrectNanLoss(self):\n    batch_size = 3\n    num_anchors = 10\n    code_size = 4\n    prediction_tensor = tf.ones([batch_size, num_anchors, code_size])\n    target_tensor = tf.concat([\n        tf.zeros([batch_size, num_anchors, code_size / 2]),\n        tf.ones([batch_size, num_anchors, code_size / 2]) * np.nan\n    ], axis=2)\n    weights = tf.ones([batch_size, num_anchors])\n    loss_op = losses.WeightedL2LocalizationLoss()\n    loss = loss_op(prediction_tensor, target_tensor, weights=weights,\n                   ignore_nan_targets=True)\n    loss = tf.reduce_sum(loss)\n\n    expected_loss = (3 * 5 * 4) / 2.0\n    with self.test_session() as sess:\n      loss_output = sess.run(loss)\n      self.assertAllClose(loss_output, expected_loss)\n\n\nclass WeightedSmoothL1LocalizationLossTest(tf.test.TestCase):\n\n  def testReturnsCorrectLoss(self):\n    batch_size = 2\n    num_anchors = 3\n    code_size = 4\n    prediction_tensor = tf.constant([[[2.5, 0, .4, 0],\n                                      [0, 0, 0, 0],\n                                      [0, 2.5, 0, .4]],\n                                     [[3.5, 0, 0, 0],\n                                      [0, .4, 0, .9],\n                                      [0, 0, 1.5, 0]]], tf.float32)\n    target_tensor = tf.zeros([batch_size, num_anchors, code_size])\n    weights = tf.constant([[2, 1, 1],\n                           [0, 3, 0]], tf.float32)\n    loss_op = losses.WeightedSmoothL1LocalizationLoss()\n    loss = loss_op(prediction_tensor, target_tensor, weights=weights)\n    loss = tf.reduce_sum(loss)\n\n    exp_loss = 7.695\n    with self.test_session() as sess:\n      loss_output = sess.run(loss)\n      self.assertAllClose(loss_output, exp_loss)\n\n\nclass WeightedIOULocalizationLossTest(tf.test.TestCase):\n\n  def testReturnsCorrectLoss(self):\n    prediction_tensor = tf.constant([[[1.5, 0, 2.4, 1],\n                                      [0, 0, 1, 1],\n                                      [0, 0, .5, .25]]])\n    target_tensor = tf.constant([[[1.5, 0, 2.4, 1],\n                                  [0, 0, 1, 1],\n                                  [50, 50, 500.5, 100.25]]])\n    weights = [[1.0, .5, 2.0]]\n    loss_op = losses.WeightedIOULocalizationLoss()\n    loss = loss_op(prediction_tensor, target_tensor, weights=weights)\n    loss = tf.reduce_sum(loss)\n    exp_loss = 2.0\n    with self.test_session() as sess:\n      loss_output = sess.run(loss)\n      self.assertAllClose(loss_output, exp_loss)\n\n\nclass WeightedSigmoidClassificationLossTest(tf.test.TestCase):\n\n  def testReturnsCorrectLoss(self):\n    prediction_tensor = tf.constant([[[-100, 100, -100],\n                                      [100, -100, -100],\n                                      [100, 0, -100],\n                                      [-100, -100, 100]],\n                                     [[-100, 0, 100],\n                                      [-100, 100, -100],\n                                      [100, 100, 100],\n                                      [0, 0, -1]]], tf.float32)\n    target_tensor = tf.constant([[[0, 1, 0],\n                                  [1, 0, 0],\n                                  [1, 0, 0],\n                                  [0, 0, 1]],\n                                 [[0, 0, 1],\n                                  [0, 1, 0],\n                                  [1, 1, 1],\n                                  [1, 0, 0]]], tf.float32)\n    weights = tf.constant([[1, 1, 1, 1],\n                           [1, 1, 1, 0]], tf.float32)\n    loss_op = losses.WeightedSigmoidClassificationLoss()\n    loss = loss_op(prediction_tensor, target_tensor, weights=weights)\n    loss = tf.reduce_sum(loss)\n\n    exp_loss = -2 * math.log(.5)\n    with self.test_session() as sess:\n      loss_output = sess.run(loss)\n      self.assertAllClose(loss_output, exp_loss)\n\n  def testReturnsCorrectAnchorWiseLoss(self):\n    prediction_tensor = tf.constant([[[-100, 100, -100],\n                                      [100, -100, -100],\n                                      [100, 0, -100],\n                                      [-100, -100, 100]],\n                                     [[-100, 0, 100],\n                                      [-100, 100, -100],\n                                      [100, 100, 100],\n                                      [0, 0, -1]]], tf.float32)\n    target_tensor = tf.constant([[[0, 1, 0],\n                                  [1, 0, 0],\n                                  [1, 0, 0],\n                                  [0, 0, 1]],\n                                 [[0, 0, 1],\n                                  [0, 1, 0],\n                                  [1, 1, 1],\n                                  [1, 0, 0]]], tf.float32)\n    weights = tf.constant([[1, 1, 1, 1],\n                           [1, 1, 1, 0]], tf.float32)\n    loss_op = losses.WeightedSigmoidClassificationLoss()\n    loss = loss_op(prediction_tensor, target_tensor, weights=weights)\n    loss = tf.reduce_sum(loss, axis=2)\n\n    exp_loss = np.matrix([[0, 0, -math.log(.5), 0],\n                          [-math.log(.5), 0, 0, 0]])\n    with self.test_session() as sess:\n      loss_output = sess.run(loss)\n      self.assertAllClose(loss_output, exp_loss)\n\n  def testReturnsCorrectLossWithClassIndices(self):\n    prediction_tensor = tf.constant([[[-100, 100, -100, 100],\n                                      [100, -100, -100, -100],\n                                      [100, 0, -100, 100],\n                                      [-100, -100, 100, -100]],\n                                     [[-100, 0, 100, 100],\n                                      [-100, 100, -100, 100],\n                                      [100, 100, 100, 100],\n                                      [0, 0, -1, 100]]], tf.float32)\n    target_tensor = tf.constant([[[0, 1, 0, 0],\n                                  [1, 0, 0, 1],\n                                  [1, 0, 0, 0],\n                                  [0, 0, 1, 1]],\n                                 [[0, 0, 1, 0],\n                                  [0, 1, 0, 0],\n                                  [1, 1, 1, 0],\n                                  [1, 0, 0, 0]]], tf.float32)\n    weights = tf.constant([[1, 1, 1, 1],\n                           [1, 1, 1, 0]], tf.float32)\n    # Ignores the last class.\n    class_indices = tf.constant([0, 1, 2], tf.int32)\n    loss_op = losses.WeightedSigmoidClassificationLoss()\n    loss = loss_op(prediction_tensor, target_tensor, weights=weights,\n                   class_indices=class_indices)\n    loss = tf.reduce_sum(loss, axis=2)\n\n    exp_loss = np.matrix([[0, 0, -math.log(.5), 0],\n                          [-math.log(.5), 0, 0, 0]])\n    with self.test_session() as sess:\n      loss_output = sess.run(loss)\n      self.assertAllClose(loss_output, exp_loss)\n\n\ndef _logit(probability):\n  return math.log(probability / (1. - probability))\n\n\nclass SigmoidFocalClassificationLossTest(tf.test.TestCase):\n\n  def testEasyExamplesProduceSmallLossComparedToSigmoidXEntropy(self):\n    prediction_tensor = tf.constant([[[_logit(0.97)],\n                                      [_logit(0.90)],\n                                      [_logit(0.73)],\n                                      [_logit(0.27)],\n                                      [_logit(0.09)],\n                                      [_logit(0.03)]]], tf.float32)\n    target_tensor = tf.constant([[[1],\n                                  [1],\n                                  [1],\n                                  [0],\n                                  [0],\n                                  [0]]], tf.float32)\n    weights = tf.constant([[1, 1, 1, 1, 1, 1]], tf.float32)\n    focal_loss_op = losses.SigmoidFocalClassificationLoss(gamma=2.0, alpha=None)\n    sigmoid_loss_op = losses.WeightedSigmoidClassificationLoss()\n    focal_loss = tf.reduce_sum(focal_loss_op(prediction_tensor, target_tensor,\n                                             weights=weights), axis=2)\n    sigmoid_loss = tf.reduce_sum(sigmoid_loss_op(prediction_tensor,\n                                                 target_tensor,\n                                                 weights=weights), axis=2)\n\n    with self.test_session() as sess:\n      sigmoid_loss, focal_loss = sess.run([sigmoid_loss, focal_loss])\n      order_of_ratio = np.power(10,\n                                np.floor(np.log10(sigmoid_loss / focal_loss)))\n      self.assertAllClose(order_of_ratio, [[1000, 100, 10, 10, 100, 1000]])\n\n  def testHardExamplesProduceLossComparableToSigmoidXEntropy(self):\n    prediction_tensor = tf.constant([[[_logit(0.55)],\n                                      [_logit(0.52)],\n                                      [_logit(0.50)],\n                                      [_logit(0.48)],\n                                      [_logit(0.45)]]], tf.float32)\n    target_tensor = tf.constant([[[1],\n                                  [1],\n                                  [1],\n                                  [0],\n                                  [0]]], tf.float32)\n    weights = tf.constant([[1, 1, 1, 1, 1]], tf.float32)\n    focal_loss_op = losses.SigmoidFocalClassificationLoss(gamma=2.0, alpha=None)\n    sigmoid_loss_op = losses.WeightedSigmoidClassificationLoss()\n    focal_loss = tf.reduce_sum(focal_loss_op(prediction_tensor, target_tensor,\n                                             weights=weights), axis=2)\n    sigmoid_loss = tf.reduce_sum(sigmoid_loss_op(prediction_tensor,\n                                                 target_tensor,\n                                                 weights=weights), axis=2)\n\n    with self.test_session() as sess:\n      sigmoid_loss, focal_loss = sess.run([sigmoid_loss, focal_loss])\n      order_of_ratio = np.power(10,\n                                np.floor(np.log10(sigmoid_loss / focal_loss)))\n      self.assertAllClose(order_of_ratio, [[1., 1., 1., 1., 1.]])\n\n  def testNonAnchorWiseOutputComparableToSigmoidXEntropy(self):\n    prediction_tensor = tf.constant([[[_logit(0.55)],\n                                      [_logit(0.52)],\n                                      [_logit(0.50)],\n                                      [_logit(0.48)],\n                                      [_logit(0.45)]]], tf.float32)\n    target_tensor = tf.constant([[[1],\n                                  [1],\n                                  [1],\n                                  [0],\n                                  [0]]], tf.float32)\n    weights = tf.constant([[1, 1, 1, 1, 1]], tf.float32)\n    focal_loss_op = losses.SigmoidFocalClassificationLoss(gamma=2.0, alpha=None)\n    sigmoid_loss_op = losses.WeightedSigmoidClassificationLoss()\n    focal_loss = tf.reduce_sum(focal_loss_op(prediction_tensor, target_tensor,\n                                             weights=weights))\n    sigmoid_loss = tf.reduce_sum(sigmoid_loss_op(prediction_tensor,\n                                                 target_tensor,\n                                                 weights=weights))\n\n    with self.test_session() as sess:\n      sigmoid_loss, focal_loss = sess.run([sigmoid_loss, focal_loss])\n      order_of_ratio = np.power(10,\n                                np.floor(np.log10(sigmoid_loss / focal_loss)))\n      self.assertAlmostEqual(order_of_ratio, 1.)\n\n  def testIgnoreNegativeExampleLossViaAlphaMultiplier(self):\n    prediction_tensor = tf.constant([[[_logit(0.55)],\n                                      [_logit(0.52)],\n                                      [_logit(0.50)],\n                                      [_logit(0.48)],\n                                      [_logit(0.45)]]], tf.float32)\n    target_tensor = tf.constant([[[1],\n                                  [1],\n                                  [1],\n                                  [0],\n                                  [0]]], tf.float32)\n    weights = tf.constant([[1, 1, 1, 1, 1]], tf.float32)\n    focal_loss_op = losses.SigmoidFocalClassificationLoss(gamma=2.0, alpha=1.0)\n    sigmoid_loss_op = losses.WeightedSigmoidClassificationLoss()\n    focal_loss = tf.reduce_sum(focal_loss_op(prediction_tensor, target_tensor,\n                                             weights=weights), axis=2)\n    sigmoid_loss = tf.reduce_sum(sigmoid_loss_op(prediction_tensor,\n                                                 target_tensor,\n                                                 weights=weights), axis=2)\n\n    with self.test_session() as sess:\n      sigmoid_loss, focal_loss = sess.run([sigmoid_loss, focal_loss])\n      self.assertAllClose(focal_loss[0][3:], [0., 0.])\n      order_of_ratio = np.power(10,\n                                np.floor(np.log10(sigmoid_loss[0][:3] /\n                                                  focal_loss[0][:3])))\n      self.assertAllClose(order_of_ratio, [1., 1., 1.])\n\n  def testIgnorePositiveExampleLossViaAlphaMultiplier(self):\n    prediction_tensor = tf.constant([[[_logit(0.55)],\n                                      [_logit(0.52)],\n                                      [_logit(0.50)],\n                                      [_logit(0.48)],\n                                      [_logit(0.45)]]], tf.float32)\n    target_tensor = tf.constant([[[1],\n                                  [1],\n                                  [1],\n                                  [0],\n                                  [0]]], tf.float32)\n    weights = tf.constant([[1, 1, 1, 1, 1]], tf.float32)\n    focal_loss_op = losses.SigmoidFocalClassificationLoss(gamma=2.0, alpha=0.0)\n    sigmoid_loss_op = losses.WeightedSigmoidClassificationLoss()\n    focal_loss = tf.reduce_sum(focal_loss_op(prediction_tensor, target_tensor,\n                                             weights=weights), axis=2)\n    sigmoid_loss = tf.reduce_sum(sigmoid_loss_op(prediction_tensor,\n                                                 target_tensor,\n                                                 weights=weights), axis=2)\n\n    with self.test_session() as sess:\n      sigmoid_loss, focal_loss = sess.run([sigmoid_loss, focal_loss])\n      self.assertAllClose(focal_loss[0][:3], [0., 0., 0.])\n      order_of_ratio = np.power(10,\n                                np.floor(np.log10(sigmoid_loss[0][3:] /\n                                                  focal_loss[0][3:])))\n      self.assertAllClose(order_of_ratio, [1., 1.])\n\n  def testSimilarToSigmoidXEntropyWithHalfAlphaAndZeroGammaUpToAScale(self):\n    prediction_tensor = tf.constant([[[-100, 100, -100],\n                                      [100, -100, -100],\n                                      [100, 0, -100],\n                                      [-100, -100, 100]],\n                                     [[-100, 0, 100],\n                                      [-100, 100, -100],\n                                      [100, 100, 100],\n                                      [0, 0, -1]]], tf.float32)\n    target_tensor = tf.constant([[[0, 1, 0],\n                                  [1, 0, 0],\n                                  [1, 0, 0],\n                                  [0, 0, 1]],\n                                 [[0, 0, 1],\n                                  [0, 1, 0],\n                                  [1, 1, 1],\n                                  [1, 0, 0]]], tf.float32)\n    weights = tf.constant([[1, 1, 1, 1],\n                           [1, 1, 1, 0]], tf.float32)\n    focal_loss_op = losses.SigmoidFocalClassificationLoss(alpha=0.5, gamma=0.0)\n    sigmoid_loss_op = losses.WeightedSigmoidClassificationLoss()\n    focal_loss = focal_loss_op(prediction_tensor, target_tensor,\n                               weights=weights)\n    sigmoid_loss = sigmoid_loss_op(prediction_tensor, target_tensor,\n                                   weights=weights)\n\n    with self.test_session() as sess:\n      sigmoid_loss, focal_loss = sess.run([sigmoid_loss, focal_loss])\n      self.assertAllClose(sigmoid_loss, focal_loss * 2)\n\n  def testSameAsSigmoidXEntropyWithNoAlphaAndZeroGamma(self):\n    prediction_tensor = tf.constant([[[-100, 100, -100],\n                                      [100, -100, -100],\n                                      [100, 0, -100],\n                                      [-100, -100, 100]],\n                                     [[-100, 0, 100],\n                                      [-100, 100, -100],\n                                      [100, 100, 100],\n                                      [0, 0, -1]]], tf.float32)\n    target_tensor = tf.constant([[[0, 1, 0],\n                                  [1, 0, 0],\n                                  [1, 0, 0],\n                                  [0, 0, 1]],\n                                 [[0, 0, 1],\n                                  [0, 1, 0],\n                                  [1, 1, 1],\n                                  [1, 0, 0]]], tf.float32)\n    weights = tf.constant([[1, 1, 1, 1],\n                           [1, 1, 1, 0]], tf.float32)\n    focal_loss_op = losses.SigmoidFocalClassificationLoss(alpha=None, gamma=0.0)\n    sigmoid_loss_op = losses.WeightedSigmoidClassificationLoss()\n    focal_loss = focal_loss_op(prediction_tensor, target_tensor,\n                               weights=weights)\n    sigmoid_loss = sigmoid_loss_op(prediction_tensor, target_tensor,\n                                   weights=weights)\n\n    with self.test_session() as sess:\n      sigmoid_loss, focal_loss = sess.run([sigmoid_loss, focal_loss])\n      self.assertAllClose(sigmoid_loss, focal_loss)\n\n  def testExpectedLossWithAlphaOneAndZeroGamma(self):\n    # All zeros correspond to 0.5 probability.\n    prediction_tensor = tf.constant([[[0, 0, 0],\n                                      [0, 0, 0],\n                                      [0, 0, 0],\n                                      [0, 0, 0]],\n                                     [[0, 0, 0],\n                                      [0, 0, 0],\n                                      [0, 0, 0],\n                                      [0, 0, 0]]], tf.float32)\n    target_tensor = tf.constant([[[0, 1, 0],\n                                  [1, 0, 0],\n                                  [1, 0, 0],\n                                  [0, 0, 1]],\n                                 [[0, 0, 1],\n                                  [0, 1, 0],\n                                  [1, 0, 0],\n                                  [1, 0, 0]]], tf.float32)\n    weights = tf.constant([[1, 1, 1, 1],\n                           [1, 1, 1, 1]], tf.float32)\n    focal_loss_op = losses.SigmoidFocalClassificationLoss(alpha=1.0, gamma=0.0)\n\n    focal_loss = tf.reduce_sum(focal_loss_op(prediction_tensor, target_tensor,\n                                             weights=weights))\n    with self.test_session() as sess:\n      focal_loss = sess.run(focal_loss)\n      self.assertAllClose(\n          (-math.log(.5) *  # x-entropy per class per anchor\n           1.0 *            # alpha\n           8),              # positives from 8 anchors\n          focal_loss)\n\n  def testExpectedLossWithAlpha75AndZeroGamma(self):\n    # All zeros correspond to 0.5 probability.\n    prediction_tensor = tf.constant([[[0, 0, 0],\n                                      [0, 0, 0],\n                                      [0, 0, 0],\n                                      [0, 0, 0]],\n                                     [[0, 0, 0],\n                                      [0, 0, 0],\n                                      [0, 0, 0],\n                                      [0, 0, 0]]], tf.float32)\n    target_tensor = tf.constant([[[0, 1, 0],\n                                  [1, 0, 0],\n                                  [1, 0, 0],\n                                  [0, 0, 1]],\n                                 [[0, 0, 1],\n                                  [0, 1, 0],\n                                  [1, 0, 0],\n                                  [1, 0, 0]]], tf.float32)\n    weights = tf.constant([[1, 1, 1, 1],\n                           [1, 1, 1, 1]], tf.float32)\n    focal_loss_op = losses.SigmoidFocalClassificationLoss(alpha=0.75, gamma=0.0)\n\n    focal_loss = tf.reduce_sum(focal_loss_op(prediction_tensor, target_tensor,\n                                             weights=weights))\n    with self.test_session() as sess:\n      focal_loss = sess.run(focal_loss)\n      self.assertAllClose(\n          (-math.log(.5) *  # x-entropy per class per anchor.\n           ((0.75 *         # alpha for positives.\n             8) +           # positives from 8 anchors.\n            (0.25 *         # alpha for negatives.\n             8 * 2))),      # negatives from 8 anchors for two classes.\n          focal_loss)\n\n\nclass WeightedSoftmaxClassificationLossTest(tf.test.TestCase):\n\n  def testReturnsCorrectLoss(self):\n    prediction_tensor = tf.constant([[[-100, 100, -100],\n                                      [100, -100, -100],\n                                      [0, 0, -100],\n                                      [-100, -100, 100]],\n                                     [[-100, 0, 0],\n                                      [-100, 100, -100],\n                                      [-100, 100, -100],\n                                      [100, -100, -100]]], tf.float32)\n    target_tensor = tf.constant([[[0, 1, 0],\n                                  [1, 0, 0],\n                                  [1, 0, 0],\n                                  [0, 0, 1]],\n                                 [[0, 0, 1],\n                                  [0, 1, 0],\n                                  [0, 1, 0],\n                                  [1, 0, 0]]], tf.float32)\n    weights = tf.constant([[1, 1, .5, 1],\n                           [1, 1, 1, 0]], tf.float32)\n    loss_op = losses.WeightedSoftmaxClassificationLoss()\n    loss = loss_op(prediction_tensor, target_tensor, weights=weights)\n    loss = tf.reduce_sum(loss)\n\n    exp_loss = - 1.5 * math.log(.5)\n    with self.test_session() as sess:\n      loss_output = sess.run(loss)\n      self.assertAllClose(loss_output, exp_loss)\n\n  def testReturnsCorrectAnchorWiseLoss(self):\n    prediction_tensor = tf.constant([[[-100, 100, -100],\n                                      [100, -100, -100],\n                                      [0, 0, -100],\n                                      [-100, -100, 100]],\n                                     [[-100, 0, 0],\n                                      [-100, 100, -100],\n                                      [-100, 100, -100],\n                                      [100, -100, -100]]], tf.float32)\n    target_tensor = tf.constant([[[0, 1, 0],\n                                  [1, 0, 0],\n                                  [1, 0, 0],\n                                  [0, 0, 1]],\n                                 [[0, 0, 1],\n                                  [0, 1, 0],\n                                  [0, 1, 0],\n                                  [1, 0, 0]]], tf.float32)\n    weights = tf.constant([[1, 1, .5, 1],\n                           [1, 1, 1, 0]], tf.float32)\n    loss_op = losses.WeightedSoftmaxClassificationLoss()\n    loss = loss_op(prediction_tensor, target_tensor, weights=weights)\n\n    exp_loss = np.matrix([[0, 0, - 0.5 * math.log(.5), 0],\n                          [-math.log(.5), 0, 0, 0]])\n    with self.test_session() as sess:\n      loss_output = sess.run(loss)\n      self.assertAllClose(loss_output, exp_loss)\n\n  def testReturnsCorrectAnchorWiseLossWithHighLogitScaleSetting(self):\n    """"""At very high logit_scale, all predictions will be ~0.33.""""""\n    # TODO(yonib): Also test logit_scale with anchorwise=False.\n    logit_scale = 10e16\n    prediction_tensor = tf.constant([[[-100, 100, -100],\n                                      [100, -100, -100],\n                                      [0, 0, -100],\n                                      [-100, -100, 100]],\n                                     [[-100, 0, 0],\n                                      [-100, 100, -100],\n                                      [-100, 100, -100],\n                                      [100, -100, -100]]], tf.float32)\n    target_tensor = tf.constant([[[0, 1, 0],\n                                  [1, 0, 0],\n                                  [1, 0, 0],\n                                  [0, 0, 1]],\n                                 [[0, 0, 1],\n                                  [0, 1, 0],\n                                  [0, 1, 0],\n                                  [1, 0, 0]]], tf.float32)\n    weights = tf.constant([[1, 1, 1, 1],\n                           [1, 1, 1, 1]], tf.float32)\n    loss_op = losses.WeightedSoftmaxClassificationLoss(logit_scale=logit_scale)\n    loss = loss_op(prediction_tensor, target_tensor, weights=weights)\n\n    uniform_distribution_loss = - math.log(.33333333333)\n    exp_loss = np.matrix([[uniform_distribution_loss] * 4,\n                          [uniform_distribution_loss] * 4])\n    with self.test_session() as sess:\n      loss_output = sess.run(loss)\n      self.assertAllClose(loss_output, exp_loss)\n\n\nclass BootstrappedSigmoidClassificationLossTest(tf.test.TestCase):\n\n  def testReturnsCorrectLossSoftBootstrapping(self):\n    prediction_tensor = tf.constant([[[-100, 100, 0],\n                                      [100, -100, -100],\n                                      [100, -100, -100],\n                                      [-100, -100, 100]],\n                                     [[-100, -100, 100],\n                                      [-100, 100, -100],\n                                      [100, 100, 100],\n                                      [0, 0, -1]]], tf.float32)\n    target_tensor = tf.constant([[[0, 1, 0],\n                                  [1, 0, 0],\n                                  [1, 0, 0],\n                                  [0, 0, 1]],\n                                 [[0, 0, 1],\n                                  [0, 1, 0],\n                                  [1, 1, 1],\n                                  [1, 0, 0]]], tf.float32)\n    weights = tf.constant([[1, 1, 1, 1],\n                           [1, 1, 1, 0]], tf.float32)\n    alpha = tf.constant(.5, tf.float32)\n    loss_op = losses.BootstrappedSigmoidClassificationLoss(\n        alpha, bootstrap_type=\'soft\')\n    loss = loss_op(prediction_tensor, target_tensor, weights=weights)\n    loss = tf.reduce_sum(loss)\n    exp_loss = -math.log(.5)\n    with self.test_session() as sess:\n      loss_output = sess.run(loss)\n      self.assertAllClose(loss_output, exp_loss)\n\n  def testReturnsCorrectLossHardBootstrapping(self):\n    prediction_tensor = tf.constant([[[-100, 100, 0],\n                                      [100, -100, -100],\n                                      [100, -100, -100],\n                                      [-100, -100, 100]],\n                                     [[-100, -100, 100],\n                                      [-100, 100, -100],\n                                      [100, 100, 100],\n                                      [0, 0, -1]]], tf.float32)\n    target_tensor = tf.constant([[[0, 1, 0],\n                                  [1, 0, 0],\n                                  [1, 0, 0],\n                                  [0, 0, 1]],\n                                 [[0, 0, 1],\n                                  [0, 1, 0],\n                                  [1, 1, 1],\n                                  [1, 0, 0]]], tf.float32)\n    weights = tf.constant([[1, 1, 1, 1],\n                           [1, 1, 1, 0]], tf.float32)\n    alpha = tf.constant(.5, tf.float32)\n    loss_op = losses.BootstrappedSigmoidClassificationLoss(\n        alpha, bootstrap_type=\'hard\')\n    loss = loss_op(prediction_tensor, target_tensor, weights=weights)\n    loss = tf.reduce_sum(loss)\n    exp_loss = -math.log(.5)\n    with self.test_session() as sess:\n      loss_output = sess.run(loss)\n      self.assertAllClose(loss_output, exp_loss)\n\n  def testReturnsCorrectAnchorWiseLoss(self):\n    prediction_tensor = tf.constant([[[-100, 100, -100],\n                                      [100, -100, -100],\n                                      [100, 0, -100],\n                                      [-100, -100, 100]],\n                                     [[-100, 0, 100],\n                                      [-100, 100, -100],\n                                      [100, 100, 100],\n                                      [0, 0, -1]]], tf.float32)\n    target_tensor = tf.constant([[[0, 1, 0],\n                                  [1, 0, 0],\n                                  [1, 0, 0],\n                                  [0, 0, 1]],\n                                 [[0, 0, 1],\n                                  [0, 1, 0],\n                                  [1, 1, 1],\n                                  [1, 0, 0]]], tf.float32)\n    weights = tf.constant([[1, 1, 1, 1],\n                           [1, 1, 1, 0]], tf.float32)\n    alpha = tf.constant(.5, tf.float32)\n    loss_op = losses.BootstrappedSigmoidClassificationLoss(\n        alpha, bootstrap_type=\'hard\')\n    loss = loss_op(prediction_tensor, target_tensor, weights=weights)\n    loss = tf.reduce_sum(loss, axis=2)\n    exp_loss = np.matrix([[0, 0, -math.log(.5), 0],\n                          [-math.log(.5), 0, 0, 0]])\n    with self.test_session() as sess:\n      loss_output = sess.run(loss)\n      self.assertAllClose(loss_output, exp_loss)\n\n\nclass HardExampleMinerTest(tf.test.TestCase):\n\n  def testHardMiningWithSingleLossType(self):\n    location_losses = tf.constant([[100, 90, 80, 0],\n                                   [0, 1, 2, 3]], tf.float32)\n    cls_losses = tf.constant([[0, 10, 50, 110],\n                              [9, 6, 3, 0]], tf.float32)\n    box_corners = tf.constant([[0.1, 0.1, 0.9, 0.9],\n                               [0.1, 0.1, 0.9, 0.9],\n                               [0.1, 0.1, 0.9, 0.9],\n                               [0.1, 0.1, 0.9, 0.9]], tf.float32)\n    decoded_boxlist_list = []\n    decoded_boxlist_list.append(box_list.BoxList(box_corners))\n    decoded_boxlist_list.append(box_list.BoxList(box_corners))\n    # Uses only location loss to select hard examples\n    loss_op = losses.HardExampleMiner(num_hard_examples=1,\n                                      iou_threshold=0.0,\n                                      loss_type=\'loc\',\n                                      cls_loss_weight=1,\n                                      loc_loss_weight=1)\n    (loc_loss, cls_loss) = loss_op(location_losses, cls_losses,\n                                   decoded_boxlist_list)\n    exp_loc_loss = 100 + 3\n    exp_cls_loss = 0 + 0\n    with self.test_session() as sess:\n      loc_loss_output = sess.run(loc_loss)\n      self.assertAllClose(loc_loss_output, exp_loc_loss)\n      cls_loss_output = sess.run(cls_loss)\n      self.assertAllClose(cls_loss_output, exp_cls_loss)\n\n  def testHardMiningWithBothLossType(self):\n    location_losses = tf.constant([[100, 90, 80, 0],\n                                   [0, 1, 2, 3]], tf.float32)\n    cls_losses = tf.constant([[0, 10, 50, 110],\n                              [9, 6, 3, 0]], tf.float32)\n    box_corners = tf.constant([[0.1, 0.1, 0.9, 0.9],\n                               [0.1, 0.1, 0.9, 0.9],\n                               [0.1, 0.1, 0.9, 0.9],\n                               [0.1, 0.1, 0.9, 0.9]], tf.float32)\n    decoded_boxlist_list = []\n    decoded_boxlist_list.append(box_list.BoxList(box_corners))\n    decoded_boxlist_list.append(box_list.BoxList(box_corners))\n    loss_op = losses.HardExampleMiner(num_hard_examples=1,\n                                      iou_threshold=0.0,\n                                      loss_type=\'both\',\n                                      cls_loss_weight=1,\n                                      loc_loss_weight=1)\n    (loc_loss, cls_loss) = loss_op(location_losses, cls_losses,\n                                   decoded_boxlist_list)\n    exp_loc_loss = 80 + 0\n    exp_cls_loss = 50 + 9\n    with self.test_session() as sess:\n      loc_loss_output = sess.run(loc_loss)\n      self.assertAllClose(loc_loss_output, exp_loc_loss)\n      cls_loss_output = sess.run(cls_loss)\n      self.assertAllClose(cls_loss_output, exp_cls_loss)\n\n  def testHardMiningNMS(self):\n    location_losses = tf.constant([[100, 90, 80, 0],\n                                   [0, 1, 2, 3]], tf.float32)\n    cls_losses = tf.constant([[0, 10, 50, 110],\n                              [9, 6, 3, 0]], tf.float32)\n    box_corners = tf.constant([[0.1, 0.1, 0.9, 0.9],\n                               [0.9, 0.9, 0.99, 0.99],\n                               [0.1, 0.1, 0.9, 0.9],\n                               [0.1, 0.1, 0.9, 0.9]], tf.float32)\n    decoded_boxlist_list = []\n    decoded_boxlist_list.append(box_list.BoxList(box_corners))\n    decoded_boxlist_list.append(box_list.BoxList(box_corners))\n    loss_op = losses.HardExampleMiner(num_hard_examples=2,\n                                      iou_threshold=0.5,\n                                      loss_type=\'cls\',\n                                      cls_loss_weight=1,\n                                      loc_loss_weight=1)\n    (loc_loss, cls_loss) = loss_op(location_losses, cls_losses,\n                                   decoded_boxlist_list)\n    exp_loc_loss = 0 + 90 + 0 + 1\n    exp_cls_loss = 110 + 10 + 9 + 6\n    with self.test_session() as sess:\n      loc_loss_output = sess.run(loc_loss)\n      self.assertAllClose(loc_loss_output, exp_loc_loss)\n      cls_loss_output = sess.run(cls_loss)\n      self.assertAllClose(cls_loss_output, exp_cls_loss)\n\n  def testEnforceNegativesPerPositiveRatio(self):\n    location_losses = tf.constant([[100, 90, 80, 0, 1, 2,\n                                    3, 10, 20, 100, 20, 3]], tf.float32)\n    cls_losses = tf.constant([[0, 0, 100, 0, 90, 70,\n                               0, 60, 0, 17, 13, 0]], tf.float32)\n    box_corners = tf.constant([[0.0, 0.0, 0.2, 0.1],\n                               [0.0, 0.0, 0.2, 0.1],\n                               [0.0, 0.0, 0.2, 0.1],\n                               [0.0, 0.0, 0.2, 0.1],\n                               [0.0, 0.0, 0.5, 0.1],\n                               [0.0, 0.0, 0.6, 0.1],\n                               [0.0, 0.0, 0.2, 0.1],\n                               [0.0, 0.0, 0.8, 0.1],\n                               [0.0, 0.0, 0.2, 0.1],\n                               [0.0, 0.0, 1.0, 0.1],\n                               [0.0, 0.0, 1.1, 0.1],\n                               [0.0, 0.0, 0.2, 0.1]], tf.float32)\n    match_results = tf.constant([2, -1, 0, -1, -1, 1, -1, -1, -1, -1, -1, 3])\n    match_list = [matcher.Match(match_results)]\n    decoded_boxlist_list = []\n    decoded_boxlist_list.append(box_list.BoxList(box_corners))\n\n    max_negatives_per_positive_list = [0.0, 0.5, 1.0, 1.5, 10]\n    exp_loc_loss_list = [80 + 2,\n                         80 + 1 + 2,\n                         80 + 1 + 2 + 10,\n                         80 + 1 + 2 + 10 + 100,\n                         80 + 1 + 2 + 10 + 100 + 20]\n    exp_cls_loss_list = [100 + 70,\n                         100 + 90 + 70,\n                         100 + 90 + 70 + 60,\n                         100 + 90 + 70 + 60 + 17,\n                         100 + 90 + 70 + 60 + 17 + 13]\n\n    for max_negatives_per_positive, exp_loc_loss, exp_cls_loss in zip(\n        max_negatives_per_positive_list, exp_loc_loss_list, exp_cls_loss_list):\n      loss_op = losses.HardExampleMiner(\n          num_hard_examples=None, iou_threshold=0.9999, loss_type=\'cls\',\n          cls_loss_weight=1, loc_loss_weight=1,\n          max_negatives_per_positive=max_negatives_per_positive)\n      (loc_loss, cls_loss) = loss_op(location_losses, cls_losses,\n                                     decoded_boxlist_list, match_list)\n      loss_op.summarize()\n\n      with self.test_session() as sess:\n        loc_loss_output = sess.run(loc_loss)\n        self.assertAllClose(loc_loss_output, exp_loc_loss)\n        cls_loss_output = sess.run(cls_loss)\n        self.assertAllClose(cls_loss_output, exp_cls_loss)\n\n  def testEnforceNegativesPerPositiveRatioWithMinNegativesPerImage(self):\n    location_losses = tf.constant([[100, 90, 80, 0, 1, 2,\n                                    3, 10, 20, 100, 20, 3]], tf.float32)\n    cls_losses = tf.constant([[0, 0, 100, 0, 90, 70,\n                               0, 60, 0, 17, 13, 0]], tf.float32)\n    box_corners = tf.constant([[0.0, 0.0, 0.2, 0.1],\n                               [0.0, 0.0, 0.2, 0.1],\n                               [0.0, 0.0, 0.2, 0.1],\n                               [0.0, 0.0, 0.2, 0.1],\n                               [0.0, 0.0, 0.5, 0.1],\n                               [0.0, 0.0, 0.6, 0.1],\n                               [0.0, 0.0, 0.2, 0.1],\n                               [0.0, 0.0, 0.8, 0.1],\n                               [0.0, 0.0, 0.2, 0.1],\n                               [0.0, 0.0, 1.0, 0.1],\n                               [0.0, 0.0, 1.1, 0.1],\n                               [0.0, 0.0, 0.2, 0.1]], tf.float32)\n    match_results = tf.constant([-1] * 12)\n    match_list = [matcher.Match(match_results)]\n    decoded_boxlist_list = []\n    decoded_boxlist_list.append(box_list.BoxList(box_corners))\n\n    min_negatives_per_image_list = [0, 1, 2, 4, 5, 6]\n    exp_loc_loss_list = [0,\n                         80,\n                         80 + 1,\n                         80 + 1 + 2 + 10,\n                         80 + 1 + 2 + 10 + 100,\n                         80 + 1 + 2 + 10 + 100 + 20]\n    exp_cls_loss_list = [0,\n                         100,\n                         100 + 90,\n                         100 + 90 + 70 + 60,\n                         100 + 90 + 70 + 60 + 17,\n                         100 + 90 + 70 + 60 + 17 + 13]\n\n    for min_negatives_per_image, exp_loc_loss, exp_cls_loss in zip(\n        min_negatives_per_image_list, exp_loc_loss_list, exp_cls_loss_list):\n      loss_op = losses.HardExampleMiner(\n          num_hard_examples=None, iou_threshold=0.9999, loss_type=\'cls\',\n          cls_loss_weight=1, loc_loss_weight=1,\n          max_negatives_per_positive=3,\n          min_negatives_per_image=min_negatives_per_image)\n      (loc_loss, cls_loss) = loss_op(location_losses, cls_losses,\n                                     decoded_boxlist_list, match_list)\n      with self.test_session() as sess:\n        loc_loss_output = sess.run(loc_loss)\n        self.assertAllClose(loc_loss_output, exp_loc_loss)\n        cls_loss_output = sess.run(cls_loss)\n        self.assertAllClose(cls_loss_output, exp_cls_loss)\n\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
src/object_detection/core/matcher.py,18,"b'# Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n""""""Matcher interface and Match class.\n\nThis module defines the Matcher interface and the Match object. The job of the\nmatcher is to match row and column indices based on the similarity matrix and\nother optional parameters. Each column is matched to at most one row. There\nare three possibilities for the matching:\n\n1) match: A column matches a row.\n2) no_match: A column does not match any row.\n3) ignore: A column that is neither \'match\' nor no_match.\n\nThe ignore case is regularly encountered in object detection: when an anchor has\na relatively small overlap with a ground-truth box, one neither wants to\nconsider this box a positive example (match) nor a negative example (no match).\n\nThe Match class is used to store the match results and it provides simple apis\nto query the results.\n""""""\nfrom abc import ABCMeta\nfrom abc import abstractmethod\n\nimport tensorflow as tf\n\nfrom object_detection.utils import ops\n\n\nclass Match(object):\n  """"""Class to store results from the matcher.\n\n  This class is used to store the results from the matcher. It provides\n  convenient methods to query the matching results.\n  """"""\n\n  def __init__(self, match_results, use_matmul_gather=False):\n    """"""Constructs a Match object.\n\n    Args:\n      match_results: Integer tensor of shape [N] with (1) match_results[i]>=0,\n        meaning that column i is matched with row match_results[i].\n        (2) match_results[i]=-1, meaning that column i is not matched.\n        (3) match_results[i]=-2, meaning that column i is ignored.\n      use_matmul_gather: Use matrix multiplication based gather instead of\n        standard tf.gather. (Default: False).\n\n    Raises:\n      ValueError: if match_results does not have rank 1 or is not an\n        integer int32 scalar tensor\n    """"""\n    if match_results.shape.ndims != 1:\n      raise ValueError(\'match_results should have rank 1\')\n    if match_results.dtype != tf.int32:\n      raise ValueError(\'match_results should be an int32 or int64 scalar \'\n                       \'tensor\')\n    self._match_results = match_results\n    self._gather_op = tf.gather\n    if use_matmul_gather:\n      self._gather_op = ops.matmul_gather_on_zeroth_axis\n\n  @property\n  def match_results(self):\n    """"""The accessor for match results.\n\n    Returns:\n      the tensor which encodes the match results.\n    """"""\n    return self._match_results\n\n  def matched_column_indices(self):\n    """"""Returns column indices that match to some row.\n\n    The indices returned by this op are always sorted in increasing order.\n\n    Returns:\n      column_indices: int32 tensor of shape [K] with column indices.\n    """"""\n    return self._reshape_and_cast(tf.where(tf.greater(self._match_results, -1)))\n\n  def matched_column_indicator(self):\n    """"""Returns column indices that are matched.\n\n    Returns:\n      column_indices: int32 tensor of shape [K] with column indices.\n    """"""\n    return tf.greater_equal(self._match_results, 0)\n\n  def num_matched_columns(self):\n    """"""Returns number (int32 scalar tensor) of matched columns.""""""\n    return tf.size(self.matched_column_indices())\n\n  def unmatched_column_indices(self):\n    """"""Returns column indices that do not match any row.\n\n    The indices returned by this op are always sorted in increasing order.\n\n    Returns:\n      column_indices: int32 tensor of shape [K] with column indices.\n    """"""\n    return self._reshape_and_cast(tf.where(tf.equal(self._match_results, -1)))\n\n  def unmatched_column_indicator(self):\n    """"""Returns column indices that are unmatched.\n\n    Returns:\n      column_indices: int32 tensor of shape [K] with column indices.\n    """"""\n    return tf.equal(self._match_results, -1)\n\n  def num_unmatched_columns(self):\n    """"""Returns number (int32 scalar tensor) of unmatched columns.""""""\n    return tf.size(self.unmatched_column_indices())\n\n  def ignored_column_indices(self):\n    """"""Returns column indices that are ignored (neither Matched nor Unmatched).\n\n    The indices returned by this op are always sorted in increasing order.\n\n    Returns:\n      column_indices: int32 tensor of shape [K] with column indices.\n    """"""\n    return self._reshape_and_cast(tf.where(self.ignored_column_indicator()))\n\n  def ignored_column_indicator(self):\n    """"""Returns boolean column indicator where True means the colum is ignored.\n\n    Returns:\n      column_indicator: boolean vector which is True for all ignored column\n      indices.\n    """"""\n    return tf.equal(self._match_results, -2)\n\n  def num_ignored_columns(self):\n    """"""Returns number (int32 scalar tensor) of matched columns.""""""\n    return tf.size(self.ignored_column_indices())\n\n  def unmatched_or_ignored_column_indices(self):\n    """"""Returns column indices that are unmatched or ignored.\n\n    The indices returned by this op are always sorted in increasing order.\n\n    Returns:\n      column_indices: int32 tensor of shape [K] with column indices.\n    """"""\n    return self._reshape_and_cast(tf.where(tf.greater(0, self._match_results)))\n\n  def matched_row_indices(self):\n    """"""Returns row indices that match some column.\n\n    The indices returned by this op are ordered so as to be in correspondence\n    with the output of matched_column_indicator().  For example if\n    self.matched_column_indicator() is [0,2], and self.matched_row_indices() is\n    [7, 3], then we know that column 0 was matched to row 7 and column 2 was\n    matched to row 3.\n\n    Returns:\n      row_indices: int32 tensor of shape [K] with row indices.\n    """"""\n    return self._reshape_and_cast(\n        self._gather_op(self._match_results, self.matched_column_indices()))\n\n  def _reshape_and_cast(self, t):\n    return tf.cast(tf.reshape(t, [-1]), tf.int32)\n\n  def gather_based_on_match(self, input_tensor, unmatched_value,\n                            ignored_value):\n    """"""Gathers elements from `input_tensor` based on match results.\n\n    For columns that are matched to a row, gathered_tensor[col] is set to\n    input_tensor[match_results[col]]. For columns that are unmatched,\n    gathered_tensor[col] is set to unmatched_value. Finally, for columns that\n    are ignored gathered_tensor[col] is set to ignored_value.\n\n    Note that the input_tensor.shape[1:] must match with unmatched_value.shape\n    and ignored_value.shape\n\n    Args:\n      input_tensor: Tensor to gather values from.\n      unmatched_value: Constant tensor value for unmatched columns.\n      ignored_value: Constant tensor value for ignored columns.\n\n    Returns:\n      gathered_tensor: A tensor containing values gathered from input_tensor.\n        The shape of the gathered tensor is [match_results.shape[0]] +\n        input_tensor.shape[1:].\n    """"""\n    input_tensor = tf.concat([tf.stack([ignored_value, unmatched_value]),\n                              input_tensor], axis=0)\n    gather_indices = tf.maximum(self.match_results + 2, 0)\n    gathered_tensor = self._gather_op(input_tensor, gather_indices)\n    return gathered_tensor\n\n\nclass Matcher(object):\n  """"""Abstract base class for matcher.\n  """"""\n  __metaclass__ = ABCMeta\n\n  def __init__(self, use_matmul_gather=False):\n    """"""Constructs a Matcher.\n\n    Args:\n      use_matmul_gather: Force constructed match objects to use matrix\n        multiplication based gather instead of standard tf.gather.\n        (Default: False).\n    """"""\n    self._use_matmul_gather = use_matmul_gather\n\n  def match(self, similarity_matrix, scope=None, **params):\n    """"""Computes matches among row and column indices and returns the result.\n\n    Computes matches among the row and column indices based on the similarity\n    matrix and optional arguments.\n\n    Args:\n      similarity_matrix: Float tensor of shape [N, M] with pairwise similarity\n        where higher value means more similar.\n      scope: Op scope name. Defaults to \'Match\' if None.\n      **params: Additional keyword arguments for specific implementations of\n        the Matcher.\n\n    Returns:\n      A Match object with the results of matching.\n    """"""\n    with tf.name_scope(scope, \'Match\', [similarity_matrix, params]) as scope:\n      return Match(self._match(similarity_matrix, **params),\n                   self._use_matmul_gather)\n\n  @abstractmethod\n  def _match(self, similarity_matrix, **params):\n    """"""Method to be overridden by implementations.\n\n    Args:\n      similarity_matrix: Float tensor of shape [N, M] with pairwise similarity\n        where higher value means more similar.\n      **params: Additional keyword arguments for specific implementations of\n        the Matcher.\n\n    Returns:\n      match_results: Integer tensor of shape [M]: match_results[i]>=0 means\n        that column i is matched to row match_results[i], match_results[i]=-1\n        means that the column is not matched. match_results[i]=-2 means that\n        the column is ignored (usually this happens when there is a very weak\n        match which one neither wants as positive nor negative example).\n    """"""\n    pass\n'"
src/object_detection/core/matcher_test.py,39,"b'# Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n""""""Tests for object_detection.core.matcher.""""""\nimport numpy as np\nimport tensorflow as tf\n\nfrom object_detection.core import matcher\n\n\nclass MatchTest(tf.test.TestCase):\n\n  def test_get_correct_matched_columnIndices(self):\n    match_results = tf.constant([3, 1, -1, 0, -1, 5, -2])\n    match = matcher.Match(match_results)\n    expected_column_indices = [0, 1, 3, 5]\n    matched_column_indices = match.matched_column_indices()\n    self.assertEquals(matched_column_indices.dtype, tf.int32)\n    with self.test_session() as sess:\n      matched_column_indices = sess.run(matched_column_indices)\n      self.assertAllEqual(matched_column_indices, expected_column_indices)\n\n  def test_get_correct_counts(self):\n    match_results = tf.constant([3, 1, -1, 0, -1, 5, -2])\n    match = matcher.Match(match_results)\n    exp_num_matched_columns = 4\n    exp_num_unmatched_columns = 2\n    exp_num_ignored_columns = 1\n    num_matched_columns = match.num_matched_columns()\n    num_unmatched_columns = match.num_unmatched_columns()\n    num_ignored_columns = match.num_ignored_columns()\n    self.assertEquals(num_matched_columns.dtype, tf.int32)\n    self.assertEquals(num_unmatched_columns.dtype, tf.int32)\n    self.assertEquals(num_ignored_columns.dtype, tf.int32)\n    with self.test_session() as sess:\n      (num_matched_columns_out, num_unmatched_columns_out,\n       num_ignored_columns_out) = sess.run(\n           [num_matched_columns, num_unmatched_columns, num_ignored_columns])\n      self.assertAllEqual(num_matched_columns_out, exp_num_matched_columns)\n      self.assertAllEqual(num_unmatched_columns_out, exp_num_unmatched_columns)\n      self.assertAllEqual(num_ignored_columns_out, exp_num_ignored_columns)\n\n  def testGetCorrectUnmatchedColumnIndices(self):\n    match_results = tf.constant([3, 1, -1, 0, -1, 5, -2])\n    match = matcher.Match(match_results)\n    expected_column_indices = [2, 4]\n    unmatched_column_indices = match.unmatched_column_indices()\n    self.assertEquals(unmatched_column_indices.dtype, tf.int32)\n    with self.test_session() as sess:\n      unmatched_column_indices = sess.run(unmatched_column_indices)\n      self.assertAllEqual(unmatched_column_indices, expected_column_indices)\n\n  def testGetCorrectMatchedRowIndices(self):\n    match_results = tf.constant([3, 1, -1, 0, -1, 5, -2])\n    match = matcher.Match(match_results)\n    expected_row_indices = [3, 1, 0, 5]\n    matched_row_indices = match.matched_row_indices()\n    self.assertEquals(matched_row_indices.dtype, tf.int32)\n    with self.test_session() as sess:\n      matched_row_inds = sess.run(matched_row_indices)\n      self.assertAllEqual(matched_row_inds, expected_row_indices)\n\n  def test_get_correct_ignored_column_indices(self):\n    match_results = tf.constant([3, 1, -1, 0, -1, 5, -2])\n    match = matcher.Match(match_results)\n    expected_column_indices = [6]\n    ignored_column_indices = match.ignored_column_indices()\n    self.assertEquals(ignored_column_indices.dtype, tf.int32)\n    with self.test_session() as sess:\n      ignored_column_indices = sess.run(ignored_column_indices)\n      self.assertAllEqual(ignored_column_indices, expected_column_indices)\n\n  def test_get_correct_matched_column_indicator(self):\n    match_results = tf.constant([3, 1, -1, 0, -1, 5, -2])\n    match = matcher.Match(match_results)\n    expected_column_indicator = [True, True, False, True, False, True, False]\n    matched_column_indicator = match.matched_column_indicator()\n    self.assertEquals(matched_column_indicator.dtype, tf.bool)\n    with self.test_session() as sess:\n      matched_column_indicator = sess.run(matched_column_indicator)\n      self.assertAllEqual(matched_column_indicator, expected_column_indicator)\n\n  def test_get_correct_unmatched_column_indicator(self):\n    match_results = tf.constant([3, 1, -1, 0, -1, 5, -2])\n    match = matcher.Match(match_results)\n    expected_column_indicator = [False, False, True, False, True, False, False]\n    unmatched_column_indicator = match.unmatched_column_indicator()\n    self.assertEquals(unmatched_column_indicator.dtype, tf.bool)\n    with self.test_session() as sess:\n      unmatched_column_indicator = sess.run(unmatched_column_indicator)\n      self.assertAllEqual(unmatched_column_indicator, expected_column_indicator)\n\n  def test_get_correct_ignored_column_indicator(self):\n    match_results = tf.constant([3, 1, -1, 0, -1, 5, -2])\n    match = matcher.Match(match_results)\n    expected_column_indicator = [False, False, False, False, False, False, True]\n    ignored_column_indicator = match.ignored_column_indicator()\n    self.assertEquals(ignored_column_indicator.dtype, tf.bool)\n    with self.test_session() as sess:\n      ignored_column_indicator = sess.run(ignored_column_indicator)\n      self.assertAllEqual(ignored_column_indicator, expected_column_indicator)\n\n  def test_get_correct_unmatched_ignored_column_indices(self):\n    match_results = tf.constant([3, 1, -1, 0, -1, 5, -2])\n    match = matcher.Match(match_results)\n    expected_column_indices = [2, 4, 6]\n    unmatched_ignored_column_indices = (match.\n                                        unmatched_or_ignored_column_indices())\n    self.assertEquals(unmatched_ignored_column_indices.dtype, tf.int32)\n    with self.test_session() as sess:\n      unmatched_ignored_column_indices = sess.run(\n          unmatched_ignored_column_indices)\n      self.assertAllEqual(unmatched_ignored_column_indices,\n                          expected_column_indices)\n\n  def test_all_columns_accounted_for(self):\n    # Note: deliberately setting to small number so not always\n    # all possibilities appear (matched, unmatched, ignored)\n    num_matches = 10\n    match_results = tf.random_uniform(\n        [num_matches], minval=-2, maxval=5, dtype=tf.int32)\n    match = matcher.Match(match_results)\n    matched_column_indices = match.matched_column_indices()\n    unmatched_column_indices = match.unmatched_column_indices()\n    ignored_column_indices = match.ignored_column_indices()\n    with self.test_session() as sess:\n      matched, unmatched, ignored = sess.run([\n          matched_column_indices, unmatched_column_indices,\n          ignored_column_indices\n      ])\n      all_indices = np.hstack((matched, unmatched, ignored))\n      all_indices_sorted = np.sort(all_indices)\n      self.assertAllEqual(all_indices_sorted,\n                          np.arange(num_matches, dtype=np.int32))\n\n  def test_scalar_gather_based_on_match(self):\n    match_results = tf.constant([3, 1, -1, 0, -1, 5, -2])\n    input_tensor = tf.constant([0, 1, 2, 3, 4, 5, 6, 7], dtype=tf.float32)\n    expected_gathered_tensor = [3, 1, 100, 0, 100, 5, 200]\n    match = matcher.Match(match_results)\n    gathered_tensor = match.gather_based_on_match(input_tensor,\n                                                  unmatched_value=100.,\n                                                  ignored_value=200.)\n    self.assertEquals(gathered_tensor.dtype, tf.float32)\n    with self.test_session():\n      gathered_tensor_out = gathered_tensor.eval()\n    self.assertAllEqual(expected_gathered_tensor, gathered_tensor_out)\n\n  def test_multidimensional_gather_based_on_match(self):\n    match_results = tf.constant([1, -1, -2])\n    input_tensor = tf.constant([[0, 0.5, 0, 0.5], [0, 0, 0.5, 0.5]],\n                               dtype=tf.float32)\n    expected_gathered_tensor = [[0, 0, 0.5, 0.5], [0, 0, 0, 0], [0, 0, 0, 0]]\n    match = matcher.Match(match_results)\n    gathered_tensor = match.gather_based_on_match(input_tensor,\n                                                  unmatched_value=tf.zeros(4),\n                                                  ignored_value=tf.zeros(4))\n    self.assertEquals(gathered_tensor.dtype, tf.float32)\n    with self.test_session():\n      gathered_tensor_out = gathered_tensor.eval()\n    self.assertAllEqual(expected_gathered_tensor, gathered_tensor_out)\n\n  def test_multidimensional_gather_based_on_match_with_matmul_gather_op(self):\n    match_results = tf.constant([1, -1, -2])\n    input_tensor = tf.constant([[0, 0.5, 0, 0.5], [0, 0, 0.5, 0.5]],\n                               dtype=tf.float32)\n    expected_gathered_tensor = [[0, 0, 0.5, 0.5], [0, 0, 0, 0], [0, 0, 0, 0]]\n    match = matcher.Match(match_results, use_matmul_gather=True)\n    gathered_tensor = match.gather_based_on_match(input_tensor,\n                                                  unmatched_value=tf.zeros(4),\n                                                  ignored_value=tf.zeros(4))\n    self.assertEquals(gathered_tensor.dtype, tf.float32)\n    with self.test_session() as sess:\n      self.assertTrue(\n          all([op.name is not \'Gather\' for op in sess.graph.get_operations()]))\n      gathered_tensor_out = gathered_tensor.eval()\n    self.assertAllEqual(expected_gathered_tensor, gathered_tensor_out)\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
src/object_detection/core/minibatch_sampler.py,7,"b'# Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n""""""Base minibatch sampler module.\n\nThe job of the minibatch_sampler is to subsample a minibatch based on some\ncriterion.\n\nThe main function call is:\n    subsample(indicator, batch_size, **params).\nIndicator is a 1d boolean tensor where True denotes which examples can be\nsampled. It returns a boolean indicator where True denotes an example has been\nsampled..\n\nSubclasses should implement the Subsample function and can make use of the\n@staticmethod SubsampleIndicator.\n""""""\n\nfrom abc import ABCMeta\nfrom abc import abstractmethod\n\nimport tensorflow as tf\n\nfrom object_detection.utils import ops\n\n\nclass MinibatchSampler(object):\n  """"""Abstract base class for subsampling minibatches.""""""\n  __metaclass__ = ABCMeta\n\n  def __init__(self):\n    """"""Constructs a minibatch sampler.""""""\n    pass\n\n  @abstractmethod\n  def subsample(self, indicator, batch_size, **params):\n    """"""Returns subsample of entries in indicator.\n\n    Args:\n      indicator: boolean tensor of shape [N] whose True entries can be sampled.\n      batch_size: desired batch size.\n      **params: additional keyword arguments for specific implementations of\n          the MinibatchSampler.\n\n    Returns:\n      sample_indicator: boolean tensor of shape [N] whose True entries have been\n      sampled. If sum(indicator) >= batch_size, sum(is_sampled) = batch_size\n    """"""\n    pass\n\n  @staticmethod\n  def subsample_indicator(indicator, num_samples):\n    """"""Subsample indicator vector.\n\n    Given a boolean indicator vector with M elements set to `True`, the function\n    assigns all but `num_samples` of these previously `True` elements to\n    `False`. If `num_samples` is greater than M, the original indicator vector\n    is returned.\n\n    Args:\n      indicator: a 1-dimensional boolean tensor indicating which elements\n        are allowed to be sampled and which are not.\n      num_samples: int32 scalar tensor\n\n    Returns:\n      a boolean tensor with the same shape as input (indicator) tensor\n    """"""\n    indices = tf.where(indicator)\n    indices = tf.random_shuffle(indices)\n    indices = tf.reshape(indices, [-1])\n\n    num_samples = tf.minimum(tf.size(indices), num_samples)\n    selected_indices = tf.slice(indices, [0], tf.reshape(num_samples, [1]))\n\n    selected_indicator = ops.indices_to_dense_vector(selected_indices,\n                                                     tf.shape(indicator)[0])\n\n    return tf.equal(selected_indicator, 1)\n'"
src/object_detection/core/minibatch_sampler_test.py,7,"b'# Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n""""""Tests for google3.research.vale.object_detection.minibatch_sampler.""""""\n\nimport numpy as np\nimport tensorflow as tf\n\nfrom object_detection.core import minibatch_sampler\n\n\nclass MinibatchSamplerTest(tf.test.TestCase):\n\n  def test_subsample_indicator_when_more_true_elements_than_num_samples(self):\n    np_indicator = [True, False, True, False, True, True, False]\n    indicator = tf.constant(np_indicator)\n    samples = minibatch_sampler.MinibatchSampler.subsample_indicator(\n        indicator, 3)\n    with self.test_session() as sess:\n      samples_out = sess.run(samples)\n      self.assertTrue(np.sum(samples_out), 3)\n      self.assertAllEqual(samples_out,\n                          np.logical_and(samples_out, np_indicator))\n\n  def test_subsample_when_more_true_elements_than_num_samples_no_shape(self):\n    np_indicator = [True, False, True, False, True, True, False]\n    indicator = tf.placeholder(tf.bool)\n    feed_dict = {indicator: np_indicator}\n\n    samples = minibatch_sampler.MinibatchSampler.subsample_indicator(\n        indicator, 3)\n    with self.test_session() as sess:\n      samples_out = sess.run(samples, feed_dict=feed_dict)\n      self.assertTrue(np.sum(samples_out), 3)\n      self.assertAllEqual(samples_out,\n                          np.logical_and(samples_out, np_indicator))\n\n  def test_subsample_indicator_when_less_true_elements_than_num_samples(self):\n    np_indicator = [True, False, True, False, True, True, False]\n    indicator = tf.constant(np_indicator)\n    samples = minibatch_sampler.MinibatchSampler.subsample_indicator(\n        indicator, 5)\n    with self.test_session() as sess:\n      samples_out = sess.run(samples)\n      self.assertTrue(np.sum(samples_out), 4)\n      self.assertAllEqual(samples_out,\n                          np.logical_and(samples_out, np_indicator))\n\n  def test_subsample_indicator_when_num_samples_is_zero(self):\n    np_indicator = [True, False, True, False, True, True, False]\n    indicator = tf.constant(np_indicator)\n    samples_none = minibatch_sampler.MinibatchSampler.subsample_indicator(\n        indicator, 0)\n    with self.test_session() as sess:\n      samples_none_out = sess.run(samples_none)\n      self.assertAllEqual(\n          np.zeros_like(samples_none_out, dtype=bool),\n          samples_none_out)\n\n  def test_subsample_indicator_when_indicator_all_false(self):\n    indicator_empty = tf.zeros([0], dtype=tf.bool)\n    samples_empty = minibatch_sampler.MinibatchSampler.subsample_indicator(\n        indicator_empty, 4)\n    with self.test_session() as sess:\n      samples_empty_out = sess.run(samples_empty)\n      self.assertEqual(0, samples_empty_out.size)\n\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
src/object_detection/core/model.py,5,"b'# Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Abstract detection model.\n\nThis file defines a generic base class for detection models.  Programs that are\ndesigned to work with arbitrary detection models should only depend on this\nclass.  We intend for the functions in this class to follow tensor-in/tensor-out\ndesign, thus all functions have tensors or lists/dictionaries holding tensors as\ninputs and outputs.\n\nAbstractly, detection models predict output tensors given input images\nwhich can be passed to a loss function at training time or passed to a\npostprocessing function at eval time.  The computation graphs at a high level\nconsequently look as follows:\n\nTraining time:\ninputs (images tensor) -> preprocess -> predict -> loss -> outputs (loss tensor)\n\nEvaluation time:\ninputs (images tensor) -> preprocess -> predict -> postprocess\n -> outputs (boxes tensor, scores tensor, classes tensor, num_detections tensor)\n\nDetectionModels must thus implement four functions (1) preprocess, (2) predict,\n(3) postprocess and (4) loss.  DetectionModels should make no assumptions about\nthe input size or aspect ratio --- they are responsible for doing any\nresize/reshaping necessary (see docstring for the preprocess function).\nOutput classes are always integers in the range [0, num_classes).  Any mapping\nof these integers to semantic labels is to be handled outside of this class.\n\nImages are resized in the `preprocess` method. All of `preprocess`, `predict`,\nand `postprocess` should be reentrant.\n\nThe `preprocess` method runs `image_resizer_fn` that returns resized_images and\n`true_image_shapes`. Since `image_resizer_fn` can pad the images with zeros,\ntrue_image_shapes indicate the slices that contain the image without padding.\nThis is useful for padding images to be a fixed size for batching.\n\nThe `postprocess` method uses the true image shapes to clip predictions that lie\noutside of images.\n\nBy default, DetectionModels produce bounding box detections; However, we support\na handful of auxiliary annotations associated with each bounding box, namely,\ninstance masks and keypoints.\n""""""\nfrom abc import ABCMeta\nfrom abc import abstractmethod\n\nfrom object_detection.core import standard_fields as fields\n\n\nclass DetectionModel(object):\n  """"""Abstract base class for detection models.""""""\n  __metaclass__ = ABCMeta\n\n  def __init__(self, num_classes):\n    """"""Constructor.\n\n    Args:\n      num_classes: number of classes.  Note that num_classes *does not* include\n      background categories that might be implicitly be predicted in various\n      implementations.\n    """"""\n    self._num_classes = num_classes\n    self._groundtruth_lists = {}\n\n  @property\n  def num_classes(self):\n    return self._num_classes\n\n  def groundtruth_lists(self, field):\n    """"""Access list of groundtruth tensors.\n\n    Args:\n      field: a string key, options are\n        fields.BoxListFields.{boxes,classes,masks,keypoints}\n\n    Returns:\n      a list of tensors holding groundtruth information (see also\n      provide_groundtruth function below), with one entry for each image in the\n      batch.\n    Raises:\n      RuntimeError: if the field has not been provided via provide_groundtruth.\n    """"""\n    if field not in self._groundtruth_lists:\n      raise RuntimeError(\'Groundtruth tensor %s has not been provided\', field)\n    return self._groundtruth_lists[field]\n\n  def groundtruth_has_field(self, field):\n    """"""Determines whether the groundtruth includes the given field.\n\n    Args:\n      field: a string key, options are\n        fields.BoxListFields.{boxes,classes,masks,keypoints}\n\n    Returns:\n      True if the groundtruth includes the given field, False otherwise.\n    """"""\n    return field in self._groundtruth_lists\n\n  @abstractmethod\n  def preprocess(self, inputs):\n    """"""Input preprocessing.\n\n    To be overridden by implementations.\n\n    This function is responsible for any scaling/shifting of input values that\n    is necessary prior to running the detector on an input image.\n    It is also responsible for any resizing, padding that might be necessary\n    as images are assumed to arrive in arbitrary sizes.  While this function\n    could conceivably be part of the predict method (below), it is often\n    convenient to keep these separate --- for example, we may want to preprocess\n    on one device, place onto a queue, and let another device (e.g., the GPU)\n    handle prediction.\n\n    A few important notes about the preprocess function:\n    + We assume that this operation does not have any trainable variables nor\n    does it affect the groundtruth annotations in any way (thus data\n    augmentation operations such as random cropping should be performed\n    externally).\n    + There is no assumption that the batchsize in this function is the same as\n    the batch size in the predict function.  In fact, we recommend calling the\n    preprocess function prior to calling any batching operations (which should\n    happen outside of the model) and thus assuming that batch sizes are equal\n    to 1 in the preprocess function.\n    + There is also no explicit assumption that the output resolutions\n    must be fixed across inputs --- this is to support ""fully convolutional""\n    settings in which input images can have different shapes/resolutions.\n\n    Args:\n      inputs: a [batch, height_in, width_in, channels] float32 tensor\n        representing a batch of images with values between 0 and 255.0.\n\n    Returns:\n      preprocessed_inputs: a [batch, height_out, width_out, channels] float32\n        tensor representing a batch of images.\n      true_image_shapes: int32 tensor of shape [batch, 3] where each row is\n        of the form [height, width, channels] indicating the shapes\n        of true images in the resized images, as resized images can be padded\n        with zeros.\n    """"""\n    pass\n\n  @abstractmethod\n  def predict(self, preprocessed_inputs, true_image_shapes):\n    """"""Predict prediction tensors from inputs tensor.\n\n    Outputs of this function can be passed to loss or postprocess functions.\n\n    Args:\n      preprocessed_inputs: a [batch, height, width, channels] float32 tensor\n        representing a batch of images.\n      true_image_shapes: int32 tensor of shape [batch, 3] where each row is\n        of the form [height, width, channels] indicating the shapes\n        of true images in the resized images, as resized images can be padded\n        with zeros.\n\n    Returns:\n      prediction_dict: a dictionary holding prediction tensors to be\n        passed to the Loss or Postprocess functions.\n    """"""\n    pass\n\n  @abstractmethod\n  def postprocess(self, prediction_dict, true_image_shapes, **params):\n    """"""Convert predicted output tensors to final detections.\n\n    Outputs adhere to the following conventions:\n    * Classes are integers in [0, num_classes); background classes are removed\n      and the first non-background class is mapped to 0. If the model produces\n      class-agnostic detections, then no output is produced for classes.\n    * Boxes are to be interpreted as being in [y_min, x_min, y_max, x_max]\n      format and normalized relative to the image window.\n    * `num_detections` is provided for settings where detections are padded to a\n      fixed number of boxes.\n    * We do not specifically assume any kind of probabilistic interpretation\n      of the scores --- the only important thing is their relative ordering.\n      Thus implementations of the postprocess function are free to output\n      logits, probabilities, calibrated probabilities, or anything else.\n\n    Args:\n      prediction_dict: a dictionary holding prediction tensors.\n      true_image_shapes: int32 tensor of shape [batch, 3] where each row is\n        of the form [height, width, channels] indicating the shapes\n        of true images in the resized images, as resized images can be padded\n        with zeros.\n      **params: Additional keyword arguments for specific implementations of\n        DetectionModel.\n\n    Returns:\n      detections: a dictionary containing the following fields\n        detection_boxes: [batch, max_detections, 4]\n        detection_scores: [batch, max_detections]\n        detection_classes: [batch, max_detections]\n          (If a model is producing class-agnostic detections, this field may be\n          missing)\n        instance_masks: [batch, max_detections, image_height, image_width]\n          (optional)\n        keypoints: [batch, max_detections, num_keypoints, 2] (optional)\n        num_detections: [batch]\n    """"""\n    pass\n\n  @abstractmethod\n  def loss(self, prediction_dict, true_image_shapes):\n    """"""Compute scalar loss tensors with respect to provided groundtruth.\n\n    Calling this function requires that groundtruth tensors have been\n    provided via the provide_groundtruth function.\n\n    Args:\n      prediction_dict: a dictionary holding predicted tensors\n      true_image_shapes: int32 tensor of shape [batch, 3] where each row is\n        of the form [height, width, channels] indicating the shapes\n        of true images in the resized images, as resized images can be padded\n        with zeros.\n\n    Returns:\n      a dictionary mapping strings (loss names) to scalar tensors representing\n        loss values.\n    """"""\n    pass\n\n  def provide_groundtruth(self,\n                          groundtruth_boxes_list,\n                          groundtruth_classes_list,\n                          groundtruth_masks_list=None,\n                          groundtruth_keypoints_list=None,\n                          groundtruth_weights_list=None):\n    """"""Provide groundtruth tensors.\n\n    Args:\n      groundtruth_boxes_list: a list of 2-D tf.float32 tensors of shape\n        [num_boxes, 4] containing coordinates of the groundtruth boxes.\n          Groundtruth boxes are provided in [y_min, x_min, y_max, x_max]\n          format and assumed to be normalized and clipped\n          relative to the image window with y_min <= y_max and x_min <= x_max.\n      groundtruth_classes_list: a list of 2-D tf.float32 one-hot (or k-hot)\n        tensors of shape [num_boxes, num_classes] containing the class targets\n        with the 0th index assumed to map to the first non-background class.\n      groundtruth_masks_list: a list of 3-D tf.float32 tensors of\n        shape [num_boxes, height_in, width_in] containing instance\n        masks with values in {0, 1}.  If None, no masks are provided.\n        Mask resolution `height_in`x`width_in` must agree with the resolution\n        of the input image tensor provided to the `preprocess` function.\n      groundtruth_keypoints_list: a list of 3-D tf.float32 tensors of\n        shape [num_boxes, num_keypoints, 2] containing keypoints.\n        Keypoints are assumed to be provided in normalized coordinates and\n        missing keypoints should be encoded as NaN.\n      groundtruth_weights_list: A list of 1-D tf.float32 tensors of shape\n        [num_boxes] containing weights for groundtruth boxes.\n    """"""\n    self._groundtruth_lists[fields.BoxListFields.boxes] = groundtruth_boxes_list\n    self._groundtruth_lists[\n        fields.BoxListFields.classes] = groundtruth_classes_list\n    if groundtruth_weights_list:\n      self._groundtruth_lists[fields.BoxListFields.\n                              weights] = groundtruth_weights_list\n    if groundtruth_masks_list:\n      self._groundtruth_lists[\n          fields.BoxListFields.masks] = groundtruth_masks_list\n    if groundtruth_keypoints_list:\n      self._groundtruth_lists[\n          fields.BoxListFields.keypoints] = groundtruth_keypoints_list\n\n  @abstractmethod\n  def restore_map(self, fine_tune_checkpoint_type=\'detection\'):\n    """"""Returns a map of variables to load from a foreign checkpoint.\n\n    Returns a map of variable names to load from a checkpoint to variables in\n    the model graph. This enables the model to initialize based on weights from\n    another task. For example, the feature extractor variables from a\n    classification model can be used to bootstrap training of an object\n    detector. When loading from an object detection model, the checkpoint model\n    should have the same parameters as this detection model with exception of\n    the num_classes parameter.\n\n    Args:\n      fine_tune_checkpoint_type: whether to restore from a full detection\n        checkpoint (with compatible variable names) or to restore from a\n        classification checkpoint for initialization prior to training.\n        Valid values: `detection`, `classification`. Default \'detection\'.\n\n    Returns:\n      A dict mapping variable names (to load from a checkpoint) to variables in\n      the model graph.\n    """"""\n    pass\n'"
src/object_detection/core/post_processing.py,41,"b'# Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n""""""Post-processing operations on detected boxes.""""""\n\nimport tensorflow as tf\n\nfrom object_detection.core import box_list\nfrom object_detection.core import box_list_ops\nfrom object_detection.core import standard_fields as fields\nfrom object_detection.utils import shape_utils\n\n\ndef multiclass_non_max_suppression(boxes,\n                                   scores,\n                                   score_thresh,\n                                   iou_thresh,\n                                   max_size_per_class,\n                                   max_total_size=0,\n                                   clip_window=None,\n                                   change_coordinate_frame=False,\n                                   masks=None,\n                                   boundaries=None,\n                                   additional_fields=None,\n                                   scope=None):\n  """"""Multi-class version of non maximum suppression.\n\n  This op greedily selects a subset of detection bounding boxes, pruning\n  away boxes that have high IOU (intersection over union) overlap (> thresh)\n  with already selected boxes.  It operates independently for each class for\n  which scores are provided (via the scores field of the input box_list),\n  pruning boxes with score less than a provided threshold prior to\n  applying NMS.\n\n  Please note that this operation is performed on *all* classes, therefore any\n  background classes should be removed prior to calling this function.\n\n  Args:\n    boxes: A [k, q, 4] float32 tensor containing k detections. `q` can be either\n      number of classes or 1 depending on whether a separate box is predicted\n      per class.\n    scores: A [k, num_classes] float32 tensor containing the scores for each of\n      the k detections.\n    score_thresh: scalar threshold for score (low scoring boxes are removed).\n    iou_thresh: scalar threshold for IOU (new boxes that have high IOU overlap\n      with previously selected boxes are removed).\n    max_size_per_class: maximum number of retained boxes per class.\n    max_total_size: maximum number of boxes retained over all classes. By\n      default returns all boxes retained after capping boxes per class.\n    clip_window: A float32 tensor of the form [y_min, x_min, y_max, x_max]\n      representing the window to clip and normalize boxes to before performing\n      non-max suppression.\n    change_coordinate_frame: Whether to normalize coordinates after clipping\n      relative to clip_window (this can only be set to True if a clip_window\n      is provided)\n    masks: (optional) a [k, q, mask_height, mask_width] float32 tensor\n      containing box masks. `q` can be either number of classes or 1 depending\n      on whether a separate mask is predicted per class.\n    boundaries: (optional) a [k, q, boundary_height, boundary_width] float32\n      tensor containing box boundaries. `q` can be either number of classes or 1\n      depending on whether a separate boundary is predicted per class.\n    additional_fields: (optional) If not None, a dictionary that maps keys to\n      tensors whose first dimensions are all of size `k`. After non-maximum\n      suppression, all tensors corresponding to the selected boxes will be\n      added to resulting BoxList.\n    scope: name scope.\n\n  Returns:\n    a BoxList holding M boxes with a rank-1 scores field representing\n      corresponding scores for each box with scores sorted in decreasing order\n      and a rank-1 classes field representing a class label for each box.\n\n  Raises:\n    ValueError: if iou_thresh is not in [0, 1] or if input boxlist does not have\n      a valid scores field.\n  """"""\n  if not 0 <= iou_thresh <= 1.0:\n    raise ValueError(\'iou_thresh must be between 0 and 1\')\n  if scores.shape.ndims != 2:\n    raise ValueError(\'scores field must be of rank 2\')\n  if scores.shape[1].value is None:\n    raise ValueError(\'scores must have statically defined second \'\n                     \'dimension\')\n  if boxes.shape.ndims != 3:\n    raise ValueError(\'boxes must be of rank 3.\')\n  if not (boxes.shape[1].value == scores.shape[1].value or\n          boxes.shape[1].value == 1):\n    raise ValueError(\'second dimension of boxes must be either 1 or equal \'\n                     \'to the second dimension of scores\')\n  if boxes.shape[2].value != 4:\n    raise ValueError(\'last dimension of boxes must be of size 4.\')\n  if change_coordinate_frame and clip_window is None:\n    raise ValueError(\'if change_coordinate_frame is True, then a clip_window\'\n                     \'must be specified.\')\n\n  with tf.name_scope(scope, \'MultiClassNonMaxSuppression\'):\n    num_boxes = tf.shape(boxes)[0]\n    num_scores = tf.shape(scores)[0]\n    num_classes = scores.get_shape()[1]\n\n    length_assert = tf.Assert(\n        tf.equal(num_boxes, num_scores),\n        [\'Incorrect scores field length: actual vs expected.\',\n         num_scores, num_boxes])\n\n    selected_boxes_list = []\n    per_class_boxes_list = tf.unstack(boxes, axis=1)\n    if masks is not None:\n      per_class_masks_list = tf.unstack(masks, axis=1)\n    if boundaries is not None:\n      per_class_boundaries_list = tf.unstack(boundaries, axis=1)\n    boxes_ids = (range(num_classes) if len(per_class_boxes_list) > 1\n                 else [0] * num_classes.value)\n    for class_idx, boxes_idx in zip(range(num_classes), boxes_ids):\n      per_class_boxes = per_class_boxes_list[boxes_idx]\n      boxlist_and_class_scores = box_list.BoxList(per_class_boxes)\n      with tf.control_dependencies([length_assert]):\n        class_scores = tf.reshape(\n            tf.slice(scores, [0, class_idx], tf.stack([num_scores, 1])), [-1])\n      boxlist_and_class_scores.add_field(fields.BoxListFields.scores,\n                                         class_scores)\n      if masks is not None:\n        per_class_masks = per_class_masks_list[boxes_idx]\n        boxlist_and_class_scores.add_field(fields.BoxListFields.masks,\n                                           per_class_masks)\n      if boundaries is not None:\n        per_class_boundaries = per_class_boundaries_list[boxes_idx]\n        boxlist_and_class_scores.add_field(fields.BoxListFields.boundaries,\n                                           per_class_boundaries)\n      if additional_fields is not None:\n        for key, tensor in additional_fields.items():\n          boxlist_and_class_scores.add_field(key, tensor)\n      boxlist_filtered = box_list_ops.filter_greater_than(\n          boxlist_and_class_scores, score_thresh)\n      if clip_window is not None:\n        boxlist_filtered = box_list_ops.clip_to_window(\n            boxlist_filtered, clip_window)\n        if change_coordinate_frame:\n          boxlist_filtered = box_list_ops.change_coordinate_frame(\n              boxlist_filtered, clip_window)\n      max_selection_size = tf.minimum(max_size_per_class,\n                                      boxlist_filtered.num_boxes())\n      selected_indices = tf.image.non_max_suppression(\n          boxlist_filtered.get(),\n          boxlist_filtered.get_field(fields.BoxListFields.scores),\n          max_selection_size,\n          iou_threshold=iou_thresh)\n      nms_result = box_list_ops.gather(boxlist_filtered, selected_indices)\n      nms_result.add_field(\n          fields.BoxListFields.classes, (tf.zeros_like(\n              nms_result.get_field(fields.BoxListFields.scores)) + class_idx))\n      selected_boxes_list.append(nms_result)\n    selected_boxes = box_list_ops.concatenate(selected_boxes_list)\n    sorted_boxes = box_list_ops.sort_by_field(selected_boxes,\n                                              fields.BoxListFields.scores)\n    if max_total_size:\n      max_total_size = tf.minimum(max_total_size,\n                                  sorted_boxes.num_boxes())\n      sorted_boxes = box_list_ops.gather(sorted_boxes,\n                                         tf.range(max_total_size))\n    return sorted_boxes\n\n\ndef batch_multiclass_non_max_suppression(boxes,\n                                         scores,\n                                         score_thresh,\n                                         iou_thresh,\n                                         max_size_per_class,\n                                         max_total_size=0,\n                                         clip_window=None,\n                                         change_coordinate_frame=False,\n                                         num_valid_boxes=None,\n                                         masks=None,\n                                         additional_fields=None,\n                                         scope=None,\n                                         parallel_iterations=32):\n  """"""Multi-class version of non maximum suppression that operates on a batch.\n\n  This op is similar to `multiclass_non_max_suppression` but operates on a batch\n  of boxes and scores. See documentation for `multiclass_non_max_suppression`\n  for details.\n\n  Args:\n    boxes: A [batch_size, num_anchors, q, 4] float32 tensor containing\n      detections. If `q` is 1 then same boxes are used for all classes\n        otherwise, if `q` is equal to number of classes, class-specific boxes\n        are used.\n    scores: A [batch_size, num_anchors, num_classes] float32 tensor containing\n      the scores for each of the `num_anchors` detections.\n    score_thresh: scalar threshold for score (low scoring boxes are removed).\n    iou_thresh: scalar threshold for IOU (new boxes that have high IOU overlap\n      with previously selected boxes are removed).\n    max_size_per_class: maximum number of retained boxes per class.\n    max_total_size: maximum number of boxes retained over all classes. By\n      default returns all boxes retained after capping boxes per class.\n    clip_window: A float32 tensor of shape [batch_size, 4]  where each entry is\n      of the form [y_min, x_min, y_max, x_max] representing the window to clip\n      boxes to before performing non-max suppression. This argument can also be\n      a tensor of shape [4] in which case, the same clip window is applied to\n      all images in the batch. If clip_widow is None, all boxes are used to\n      perform non-max suppression.\n    change_coordinate_frame: Whether to normalize coordinates after clipping\n      relative to clip_window (this can only be set to True if a clip_window\n      is provided)\n    num_valid_boxes: (optional) a Tensor of type `int32`. A 1-D tensor of shape\n      [batch_size] representing the number of valid boxes to be considered\n      for each image in the batch.  This parameter allows for ignoring zero\n      paddings.\n    masks: (optional) a [batch_size, num_anchors, q, mask_height, mask_width]\n      float32 tensor containing box masks. `q` can be either number of classes\n      or 1 depending on whether a separate mask is predicted per class.\n    additional_fields: (optional) If not None, a dictionary that maps keys to\n      tensors whose dimensions are [batch_size, num_anchors, ...].\n    scope: tf scope name.\n    parallel_iterations: (optional) number of batch items to process in\n      parallel.\n\n  Returns:\n    \'nmsed_boxes\': A [batch_size, max_detections, 4] float32 tensor\n      containing the non-max suppressed boxes.\n    \'nmsed_scores\': A [batch_size, max_detections] float32 tensor containing\n      the scores for the boxes.\n    \'nmsed_classes\': A [batch_size, max_detections] float32 tensor\n      containing the class for boxes.\n    \'nmsed_masks\': (optional) a\n      [batch_size, max_detections, mask_height, mask_width] float32 tensor\n      containing masks for each selected box. This is set to None if input\n      `masks` is None.\n    \'nmsed_additional_fields\': (optional) a dictionary of\n      [batch_size, max_detections, ...] float32 tensors corresponding to the\n      tensors specified in the input `additional_fields`. This is not returned\n      if input `additional_fields` is None.\n    \'num_detections\': A [batch_size] int32 tensor indicating the number of\n      valid detections per batch item. Only the top num_detections[i] entries in\n      nms_boxes[i], nms_scores[i] and nms_class[i] are valid. The rest of the\n      entries are zero paddings.\n\n  Raises:\n    ValueError: if `q` in boxes.shape is not 1 or not equal to number of\n      classes as inferred from scores.shape.\n  """"""\n  q = boxes.shape[2].value\n  num_classes = scores.shape[2].value\n  if q != 1 and q != num_classes:\n    raise ValueError(\'third dimension of boxes must be either 1 or equal \'\n                     \'to the third dimension of scores\')\n  if change_coordinate_frame and clip_window is None:\n    raise ValueError(\'if change_coordinate_frame is True, then a clip_window\'\n                     \'must be specified.\')\n  original_masks = masks\n  original_additional_fields = additional_fields\n  with tf.name_scope(scope, \'BatchMultiClassNonMaxSuppression\'):\n    boxes_shape = boxes.shape\n    batch_size = boxes_shape[0].value\n    num_anchors = boxes_shape[1].value\n\n    if batch_size is None:\n      batch_size = tf.shape(boxes)[0]\n    if num_anchors is None:\n      num_anchors = tf.shape(boxes)[1]\n\n    # If num valid boxes aren\'t provided, create one and mark all boxes as\n    # valid.\n    if num_valid_boxes is None:\n      num_valid_boxes = tf.ones([batch_size], dtype=tf.int32) * num_anchors\n\n    # If masks aren\'t provided, create dummy masks so we can only have one copy\n    # of _single_image_nms_fn and discard the dummy masks after map_fn.\n    if masks is None:\n      masks_shape = tf.stack([batch_size, num_anchors, 1, 0, 0])\n      masks = tf.zeros(masks_shape)\n\n    if clip_window is None:\n      clip_window = tf.stack([\n          tf.reduce_min(boxes[:, :, :, 0]),\n          tf.reduce_min(boxes[:, :, :, 1]),\n          tf.reduce_max(boxes[:, :, :, 2]),\n          tf.reduce_max(boxes[:, :, :, 3])\n      ])\n    if clip_window.shape.ndims == 1:\n      clip_window = tf.tile(tf.expand_dims(clip_window, 0), [batch_size, 1])\n\n    if additional_fields is None:\n      additional_fields = {}\n\n    def _single_image_nms_fn(args):\n      """"""Runs NMS on a single image and returns padded output.\n\n      Args:\n        args: A list of tensors consisting of the following:\n          per_image_boxes - A [num_anchors, q, 4] float32 tensor containing\n            detections. If `q` is 1 then same boxes are used for all classes\n            otherwise, if `q` is equal to number of classes, class-specific\n            boxes are used.\n          per_image_scores - A [num_anchors, num_classes] float32 tensor\n            containing the scores for each of the `num_anchors` detections.\n          per_image_masks - A [num_anchors, q, mask_height, mask_width] float32\n            tensor containing box masks. `q` can be either number of classes\n            or 1 depending on whether a separate mask is predicted per class.\n          per_image_clip_window - A 1D float32 tensor of the form\n            [ymin, xmin, ymax, xmax] representing the window to clip the boxes\n            to.\n          per_image_additional_fields - (optional) A variable number of float32\n            tensors each with size [num_anchors, ...].\n          per_image_num_valid_boxes - A tensor of type `int32`. A 1-D tensor of\n            shape [batch_size] representing the number of valid boxes to be\n            considered for each image in the batch.  This parameter allows for\n            ignoring zero paddings.\n\n      Returns:\n        \'nmsed_boxes\': A [max_detections, 4] float32 tensor containing the\n          non-max suppressed boxes.\n        \'nmsed_scores\': A [max_detections] float32 tensor containing the scores\n          for the boxes.\n        \'nmsed_classes\': A [max_detections] float32 tensor containing the class\n          for boxes.\n        \'nmsed_masks\': (optional) a [max_detections, mask_height, mask_width]\n          float32 tensor containing masks for each selected box. This is set to\n          None if input `masks` is None.\n        \'nmsed_additional_fields\':  (optional) A variable number of float32\n          tensors each with size [max_detections, ...] corresponding to the\n          input `per_image_additional_fields`.\n        \'num_detections\': A [batch_size] int32 tensor indicating the number of\n          valid detections per batch item. Only the top num_detections[i]\n          entries in nms_boxes[i], nms_scores[i] and nms_class[i] are valid. The\n          rest of the entries are zero paddings.\n      """"""\n      per_image_boxes = args[0]\n      per_image_scores = args[1]\n      per_image_masks = args[2]\n      per_image_clip_window = args[3]\n      per_image_additional_fields = {\n          key: value\n          for key, value in zip(additional_fields, args[4:-1])\n      }\n      per_image_num_valid_boxes = args[-1]\n      per_image_boxes = tf.reshape(\n          tf.slice(per_image_boxes, 3 * [0],\n                   tf.stack([per_image_num_valid_boxes, -1, -1])), [-1, q, 4])\n      per_image_scores = tf.reshape(\n          tf.slice(per_image_scores, [0, 0],\n                   tf.stack([per_image_num_valid_boxes, -1])),\n          [-1, num_classes])\n      per_image_masks = tf.reshape(\n          tf.slice(per_image_masks, 4 * [0],\n                   tf.stack([per_image_num_valid_boxes, -1, -1, -1])),\n          [-1, q, per_image_masks.shape[2].value,\n           per_image_masks.shape[3].value])\n      if per_image_additional_fields is not None:\n        for key, tensor in per_image_additional_fields.items():\n          additional_field_shape = tensor.get_shape()\n          additional_field_dim = len(additional_field_shape)\n          per_image_additional_fields[key] = tf.reshape(\n              tf.slice(per_image_additional_fields[key],\n                       additional_field_dim * [0],\n                       tf.stack([per_image_num_valid_boxes] +\n                                (additional_field_dim - 1) * [-1])),\n              [-1] + [dim.value for dim in additional_field_shape[1:]])\n      nmsed_boxlist = multiclass_non_max_suppression(\n          per_image_boxes,\n          per_image_scores,\n          score_thresh,\n          iou_thresh,\n          max_size_per_class,\n          max_total_size,\n          clip_window=per_image_clip_window,\n          change_coordinate_frame=change_coordinate_frame,\n          masks=per_image_masks,\n          additional_fields=per_image_additional_fields)\n      padded_boxlist = box_list_ops.pad_or_clip_box_list(nmsed_boxlist,\n                                                         max_total_size)\n      num_detections = nmsed_boxlist.num_boxes()\n      nmsed_boxes = padded_boxlist.get()\n      nmsed_scores = padded_boxlist.get_field(fields.BoxListFields.scores)\n      nmsed_classes = padded_boxlist.get_field(fields.BoxListFields.classes)\n      nmsed_masks = padded_boxlist.get_field(fields.BoxListFields.masks)\n      nmsed_additional_fields = [\n          padded_boxlist.get_field(key) for key in per_image_additional_fields\n      ]\n      return ([nmsed_boxes, nmsed_scores, nmsed_classes, nmsed_masks] +\n              nmsed_additional_fields + [num_detections])\n\n    num_additional_fields = 0\n    if additional_fields is not None:\n      num_additional_fields = len(additional_fields)\n    num_nmsed_outputs = 4 + num_additional_fields\n\n    batch_outputs = shape_utils.static_or_dynamic_map_fn(\n        _single_image_nms_fn,\n        elems=([boxes, scores, masks, clip_window] +\n               list(additional_fields.values()) + [num_valid_boxes]),\n        dtype=(num_nmsed_outputs * [tf.float32] + [tf.int32]),\n        parallel_iterations=parallel_iterations)\n\n    batch_nmsed_boxes = batch_outputs[0]\n    batch_nmsed_scores = batch_outputs[1]\n    batch_nmsed_classes = batch_outputs[2]\n    batch_nmsed_masks = batch_outputs[3]\n    batch_nmsed_additional_fields = {\n        key: value\n        for key, value in zip(additional_fields, batch_outputs[4:-1])\n    }\n    batch_num_detections = batch_outputs[-1]\n\n    if original_masks is None:\n      batch_nmsed_masks = None\n\n    if original_additional_fields is None:\n      batch_nmsed_additional_fields = None\n\n    return (batch_nmsed_boxes, batch_nmsed_scores, batch_nmsed_classes,\n            batch_nmsed_masks, batch_nmsed_additional_fields,\n            batch_num_detections)\n'"
src/object_detection/core/post_processing_test.py,93,"b'# Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n""""""Tests for tensorflow_models.object_detection.core.post_processing.""""""\nimport numpy as np\nimport tensorflow as tf\nfrom object_detection.core import post_processing\nfrom object_detection.core import standard_fields as fields\n\n\nclass MulticlassNonMaxSuppressionTest(tf.test.TestCase):\n\n  def test_with_invalid_scores_size(self):\n    boxes = tf.constant([[[0, 0, 1, 1]],\n                         [[0, 0.1, 1, 1.1]],\n                         [[0, -0.1, 1, 0.9]],\n                         [[0, 10, 1, 11]],\n                         [[0, 10.1, 1, 11.1]],\n                         [[0, 100, 1, 101]]], tf.float32)\n    scores = tf.constant([[.9], [.75], [.6], [.95], [.5]])\n    iou_thresh = .5\n    score_thresh = 0.6\n    max_output_size = 3\n    nms = post_processing.multiclass_non_max_suppression(\n        boxes, scores, score_thresh, iou_thresh, max_output_size)\n    with self.test_session() as sess:\n      with self.assertRaisesWithPredicateMatch(\n          tf.errors.InvalidArgumentError, \'Incorrect scores field length\'):\n        sess.run(nms.get())\n\n  def test_multiclass_nms_select_with_shared_boxes(self):\n    boxes = tf.constant([[[0, 0, 1, 1]],\n                         [[0, 0.1, 1, 1.1]],\n                         [[0, -0.1, 1, 0.9]],\n                         [[0, 10, 1, 11]],\n                         [[0, 10.1, 1, 11.1]],\n                         [[0, 100, 1, 101]],\n                         [[0, 1000, 1, 1002]],\n                         [[0, 1000, 1, 1002.1]]], tf.float32)\n    scores = tf.constant([[.9, 0.01], [.75, 0.05],\n                          [.6, 0.01], [.95, 0],\n                          [.5, 0.01], [.3, 0.01],\n                          [.01, .85], [.01, .5]])\n    score_thresh = 0.1\n    iou_thresh = .5\n    max_output_size = 4\n\n    exp_nms_corners = [[0, 10, 1, 11],\n                       [0, 0, 1, 1],\n                       [0, 1000, 1, 1002],\n                       [0, 100, 1, 101]]\n    exp_nms_scores = [.95, .9, .85, .3]\n    exp_nms_classes = [0, 0, 1, 0]\n\n    nms = post_processing.multiclass_non_max_suppression(\n        boxes, scores, score_thresh, iou_thresh, max_output_size)\n    with self.test_session() as sess:\n      nms_corners_output, nms_scores_output, nms_classes_output = sess.run(\n          [nms.get(), nms.get_field(fields.BoxListFields.scores),\n           nms.get_field(fields.BoxListFields.classes)])\n      self.assertAllClose(nms_corners_output, exp_nms_corners)\n      self.assertAllClose(nms_scores_output, exp_nms_scores)\n      self.assertAllClose(nms_classes_output, exp_nms_classes)\n\n  def test_multiclass_nms_select_with_shared_boxes_given_keypoints(self):\n    boxes = tf.constant([[[0, 0, 1, 1]],\n                         [[0, 0.1, 1, 1.1]],\n                         [[0, -0.1, 1, 0.9]],\n                         [[0, 10, 1, 11]],\n                         [[0, 10.1, 1, 11.1]],\n                         [[0, 100, 1, 101]],\n                         [[0, 1000, 1, 1002]],\n                         [[0, 1000, 1, 1002.1]]], tf.float32)\n    scores = tf.constant([[.9, 0.01], [.75, 0.05],\n                          [.6, 0.01], [.95, 0],\n                          [.5, 0.01], [.3, 0.01],\n                          [.01, .85], [.01, .5]])\n    num_keypoints = 6\n    keypoints = tf.tile(\n        tf.reshape(tf.range(8), [8, 1, 1]),\n        [1, num_keypoints, 2])\n    score_thresh = 0.1\n    iou_thresh = .5\n    max_output_size = 4\n\n    exp_nms_corners = [[0, 10, 1, 11],\n                       [0, 0, 1, 1],\n                       [0, 1000, 1, 1002],\n                       [0, 100, 1, 101]]\n    exp_nms_scores = [.95, .9, .85, .3]\n    exp_nms_classes = [0, 0, 1, 0]\n    exp_nms_keypoints_tensor = tf.tile(\n        tf.reshape(tf.constant([3, 0, 6, 5], dtype=tf.float32), [4, 1, 1]),\n        [1, num_keypoints, 2])\n\n    nms = post_processing.multiclass_non_max_suppression(\n        boxes, scores, score_thresh, iou_thresh, max_output_size,\n        additional_fields={\n            fields.BoxListFields.keypoints: keypoints})\n\n    with self.test_session() as sess:\n      (nms_corners_output,\n       nms_scores_output,\n       nms_classes_output,\n       nms_keypoints,\n       exp_nms_keypoints) = sess.run([\n           nms.get(),\n           nms.get_field(fields.BoxListFields.scores),\n           nms.get_field(fields.BoxListFields.classes),\n           nms.get_field(fields.BoxListFields.keypoints),\n           exp_nms_keypoints_tensor\n       ])\n      self.assertAllClose(nms_corners_output, exp_nms_corners)\n      self.assertAllClose(nms_scores_output, exp_nms_scores)\n      self.assertAllClose(nms_classes_output, exp_nms_classes)\n      self.assertAllEqual(nms_keypoints, exp_nms_keypoints)\n\n  def test_multiclass_nms_with_shared_boxes_given_keypoint_heatmaps(self):\n    boxes = tf.constant([[[0, 0, 1, 1]],\n                         [[0, 0.1, 1, 1.1]],\n                         [[0, -0.1, 1, 0.9]],\n                         [[0, 10, 1, 11]],\n                         [[0, 10.1, 1, 11.1]],\n                         [[0, 100, 1, 101]],\n                         [[0, 1000, 1, 1002]],\n                         [[0, 1000, 1, 1002.1]]], tf.float32)\n\n    scores = tf.constant([[.9, 0.01], [.75, 0.05],\n                          [.6, 0.01], [.95, 0],\n                          [.5, 0.01], [.3, 0.01],\n                          [.01, .85], [.01, .5]])\n\n    num_boxes = tf.shape(boxes)[0]\n    heatmap_height = 5\n    heatmap_width = 5\n    num_keypoints = 17\n    keypoint_heatmaps = tf.ones(\n        [num_boxes, heatmap_height, heatmap_width, num_keypoints],\n        dtype=tf.float32)\n\n    score_thresh = 0.1\n    iou_thresh = .5\n    max_output_size = 4\n    exp_nms_corners = [[0, 10, 1, 11],\n                       [0, 0, 1, 1],\n                       [0, 1000, 1, 1002],\n                       [0, 100, 1, 101]]\n\n    exp_nms_scores = [.95, .9, .85, .3]\n    exp_nms_classes = [0, 0, 1, 0]\n    exp_nms_keypoint_heatmaps = np.ones(\n        (4, heatmap_height, heatmap_width, num_keypoints), dtype=np.float32)\n\n    nms = post_processing.multiclass_non_max_suppression(\n        boxes, scores, score_thresh, iou_thresh, max_output_size,\n        additional_fields={\n            fields.BoxListFields.keypoint_heatmaps: keypoint_heatmaps})\n\n    with self.test_session() as sess:\n      (nms_corners_output,\n       nms_scores_output,\n       nms_classes_output,\n       nms_keypoint_heatmaps) = sess.run(\n           [nms.get(),\n            nms.get_field(fields.BoxListFields.scores),\n            nms.get_field(fields.BoxListFields.classes),\n            nms.get_field(fields.BoxListFields.keypoint_heatmaps)])\n\n      self.assertAllClose(nms_corners_output, exp_nms_corners)\n      self.assertAllClose(nms_scores_output, exp_nms_scores)\n      self.assertAllClose(nms_classes_output, exp_nms_classes)\n      self.assertAllEqual(nms_keypoint_heatmaps, exp_nms_keypoint_heatmaps)\n\n  def test_multiclass_nms_with_additional_fields(self):\n    boxes = tf.constant([[[0, 0, 1, 1]],\n                         [[0, 0.1, 1, 1.1]],\n                         [[0, -0.1, 1, 0.9]],\n                         [[0, 10, 1, 11]],\n                         [[0, 10.1, 1, 11.1]],\n                         [[0, 100, 1, 101]],\n                         [[0, 1000, 1, 1002]],\n                         [[0, 1000, 1, 1002.1]]], tf.float32)\n\n    scores = tf.constant([[.9, 0.01], [.75, 0.05],\n                          [.6, 0.01], [.95, 0],\n                          [.5, 0.01], [.3, 0.01],\n                          [.01, .85], [.01, .5]])\n\n    coarse_boxes_key = \'coarse_boxes\'\n    coarse_boxes = tf.constant([[0.1, 0.1, 1.1, 1.1],\n                                [0.1, 0.2, 1.1, 1.2],\n                                [0.1, -0.2, 1.1, 1.0],\n                                [0.1, 10.1, 1.1, 11.1],\n                                [0.1, 10.2, 1.1, 11.2],\n                                [0.1, 100.1, 1.1, 101.1],\n                                [0.1, 1000.1, 1.1, 1002.1],\n                                [0.1, 1000.1, 1.1, 1002.2]], tf.float32)\n\n    score_thresh = 0.1\n    iou_thresh = .5\n    max_output_size = 4\n\n    exp_nms_corners = np.array([[0, 10, 1, 11],\n                                [0, 0, 1, 1],\n                                [0, 1000, 1, 1002],\n                                [0, 100, 1, 101]], dtype=np.float32)\n\n    exp_nms_coarse_corners = np.array([[0.1, 10.1, 1.1, 11.1],\n                                       [0.1, 0.1, 1.1, 1.1],\n                                       [0.1, 1000.1, 1.1, 1002.1],\n                                       [0.1, 100.1, 1.1, 101.1]],\n                                      dtype=np.float32)\n\n    exp_nms_scores = [.95, .9, .85, .3]\n    exp_nms_classes = [0, 0, 1, 0]\n\n    nms = post_processing.multiclass_non_max_suppression(\n        boxes, scores, score_thresh, iou_thresh, max_output_size,\n        additional_fields={coarse_boxes_key: coarse_boxes})\n\n    with self.test_session() as sess:\n      (nms_corners_output,\n       nms_scores_output,\n       nms_classes_output,\n       nms_coarse_corners) = sess.run(\n           [nms.get(),\n            nms.get_field(fields.BoxListFields.scores),\n            nms.get_field(fields.BoxListFields.classes),\n            nms.get_field(coarse_boxes_key)])\n\n      self.assertAllClose(nms_corners_output, exp_nms_corners)\n      self.assertAllClose(nms_scores_output, exp_nms_scores)\n      self.assertAllClose(nms_classes_output, exp_nms_classes)\n      self.assertAllEqual(nms_coarse_corners, exp_nms_coarse_corners)\n\n  def test_multiclass_nms_select_with_shared_boxes_given_masks(self):\n    boxes = tf.constant([[[0, 0, 1, 1]],\n                         [[0, 0.1, 1, 1.1]],\n                         [[0, -0.1, 1, 0.9]],\n                         [[0, 10, 1, 11]],\n                         [[0, 10.1, 1, 11.1]],\n                         [[0, 100, 1, 101]],\n                         [[0, 1000, 1, 1002]],\n                         [[0, 1000, 1, 1002.1]]], tf.float32)\n    scores = tf.constant([[.9, 0.01], [.75, 0.05],\n                          [.6, 0.01], [.95, 0],\n                          [.5, 0.01], [.3, 0.01],\n                          [.01, .85], [.01, .5]])\n    num_classes = 2\n    mask_height = 3\n    mask_width = 3\n    masks = tf.tile(\n        tf.reshape(tf.range(8), [8, 1, 1, 1]),\n        [1, num_classes, mask_height, mask_width])\n    score_thresh = 0.1\n    iou_thresh = .5\n    max_output_size = 4\n\n    exp_nms_corners = [[0, 10, 1, 11],\n                       [0, 0, 1, 1],\n                       [0, 1000, 1, 1002],\n                       [0, 100, 1, 101]]\n    exp_nms_scores = [.95, .9, .85, .3]\n    exp_nms_classes = [0, 0, 1, 0]\n    exp_nms_masks_tensor = tf.tile(\n        tf.reshape(tf.constant([3, 0, 6, 5], dtype=tf.float32), [4, 1, 1]),\n        [1, mask_height, mask_width])\n\n    nms = post_processing.multiclass_non_max_suppression(boxes, scores,\n                                                         score_thresh,\n                                                         iou_thresh,\n                                                         max_output_size,\n                                                         masks=masks)\n    with self.test_session() as sess:\n      (nms_corners_output,\n       nms_scores_output,\n       nms_classes_output,\n       nms_masks,\n       exp_nms_masks) = sess.run([nms.get(),\n                                  nms.get_field(fields.BoxListFields.scores),\n                                  nms.get_field(fields.BoxListFields.classes),\n                                  nms.get_field(fields.BoxListFields.masks),\n                                  exp_nms_masks_tensor])\n      self.assertAllClose(nms_corners_output, exp_nms_corners)\n      self.assertAllClose(nms_scores_output, exp_nms_scores)\n      self.assertAllClose(nms_classes_output, exp_nms_classes)\n      self.assertAllEqual(nms_masks, exp_nms_masks)\n\n  def test_multiclass_nms_select_with_clip_window(self):\n    boxes = tf.constant([[[0, 0, 10, 10]],\n                         [[1, 1, 11, 11]]], tf.float32)\n    scores = tf.constant([[.9], [.75]])\n    clip_window = tf.constant([5, 4, 8, 7], tf.float32)\n    score_thresh = 0.0\n    iou_thresh = 0.5\n    max_output_size = 100\n\n    exp_nms_corners = [[5, 4, 8, 7]]\n    exp_nms_scores = [.9]\n    exp_nms_classes = [0]\n\n    nms = post_processing.multiclass_non_max_suppression(\n        boxes, scores, score_thresh, iou_thresh, max_output_size,\n        clip_window=clip_window)\n    with self.test_session() as sess:\n      nms_corners_output, nms_scores_output, nms_classes_output = sess.run(\n          [nms.get(), nms.get_field(fields.BoxListFields.scores),\n           nms.get_field(fields.BoxListFields.classes)])\n      self.assertAllClose(nms_corners_output, exp_nms_corners)\n      self.assertAllClose(nms_scores_output, exp_nms_scores)\n      self.assertAllClose(nms_classes_output, exp_nms_classes)\n\n  def test_multiclass_nms_select_with_clip_window_change_coordinate_frame(self):\n    boxes = tf.constant([[[0, 0, 10, 10]],\n                         [[1, 1, 11, 11]]], tf.float32)\n    scores = tf.constant([[.9], [.75]])\n    clip_window = tf.constant([5, 4, 8, 7], tf.float32)\n    score_thresh = 0.0\n    iou_thresh = 0.5\n    max_output_size = 100\n\n    exp_nms_corners = [[0, 0, 1, 1]]\n    exp_nms_scores = [.9]\n    exp_nms_classes = [0]\n\n    nms = post_processing.multiclass_non_max_suppression(\n        boxes, scores, score_thresh, iou_thresh, max_output_size,\n        clip_window=clip_window, change_coordinate_frame=True)\n    with self.test_session() as sess:\n      nms_corners_output, nms_scores_output, nms_classes_output = sess.run(\n          [nms.get(), nms.get_field(fields.BoxListFields.scores),\n           nms.get_field(fields.BoxListFields.classes)])\n      self.assertAllClose(nms_corners_output, exp_nms_corners)\n      self.assertAllClose(nms_scores_output, exp_nms_scores)\n      self.assertAllClose(nms_classes_output, exp_nms_classes)\n\n  def test_multiclass_nms_select_with_per_class_cap(self):\n    boxes = tf.constant([[[0, 0, 1, 1]],\n                         [[0, 0.1, 1, 1.1]],\n                         [[0, -0.1, 1, 0.9]],\n                         [[0, 10, 1, 11]],\n                         [[0, 10.1, 1, 11.1]],\n                         [[0, 100, 1, 101]],\n                         [[0, 1000, 1, 1002]],\n                         [[0, 1000, 1, 1002.1]]], tf.float32)\n    scores = tf.constant([[.9, 0.01], [.75, 0.05],\n                          [.6, 0.01], [.95, 0],\n                          [.5, 0.01], [.3, 0.01],\n                          [.01, .85], [.01, .5]])\n    score_thresh = 0.1\n    iou_thresh = .5\n    max_size_per_class = 2\n\n    exp_nms_corners = [[0, 10, 1, 11],\n                       [0, 0, 1, 1],\n                       [0, 1000, 1, 1002]]\n    exp_nms_scores = [.95, .9, .85]\n    exp_nms_classes = [0, 0, 1]\n\n    nms = post_processing.multiclass_non_max_suppression(\n        boxes, scores, score_thresh, iou_thresh, max_size_per_class)\n    with self.test_session() as sess:\n      nms_corners_output, nms_scores_output, nms_classes_output = sess.run(\n          [nms.get(), nms.get_field(fields.BoxListFields.scores),\n           nms.get_field(fields.BoxListFields.classes)])\n      self.assertAllClose(nms_corners_output, exp_nms_corners)\n      self.assertAllClose(nms_scores_output, exp_nms_scores)\n      self.assertAllClose(nms_classes_output, exp_nms_classes)\n\n  def test_multiclass_nms_select_with_total_cap(self):\n    boxes = tf.constant([[[0, 0, 1, 1]],\n                         [[0, 0.1, 1, 1.1]],\n                         [[0, -0.1, 1, 0.9]],\n                         [[0, 10, 1, 11]],\n                         [[0, 10.1, 1, 11.1]],\n                         [[0, 100, 1, 101]],\n                         [[0, 1000, 1, 1002]],\n                         [[0, 1000, 1, 1002.1]]], tf.float32)\n    scores = tf.constant([[.9, 0.01], [.75, 0.05],\n                          [.6, 0.01], [.95, 0],\n                          [.5, 0.01], [.3, 0.01],\n                          [.01, .85], [.01, .5]])\n    score_thresh = 0.1\n    iou_thresh = .5\n    max_size_per_class = 4\n    max_total_size = 2\n\n    exp_nms_corners = [[0, 10, 1, 11],\n                       [0, 0, 1, 1]]\n    exp_nms_scores = [.95, .9]\n    exp_nms_classes = [0, 0]\n\n    nms = post_processing.multiclass_non_max_suppression(\n        boxes, scores, score_thresh, iou_thresh, max_size_per_class,\n        max_total_size)\n    with self.test_session() as sess:\n      nms_corners_output, nms_scores_output, nms_classes_output = sess.run(\n          [nms.get(), nms.get_field(fields.BoxListFields.scores),\n           nms.get_field(fields.BoxListFields.classes)])\n      self.assertAllClose(nms_corners_output, exp_nms_corners)\n      self.assertAllClose(nms_scores_output, exp_nms_scores)\n      self.assertAllClose(nms_classes_output, exp_nms_classes)\n\n  def test_multiclass_nms_threshold_then_select_with_shared_boxes(self):\n    boxes = tf.constant([[[0, 0, 1, 1]],\n                         [[0, 0.1, 1, 1.1]],\n                         [[0, -0.1, 1, 0.9]],\n                         [[0, 10, 1, 11]],\n                         [[0, 10.1, 1, 11.1]],\n                         [[0, 100, 1, 101]],\n                         [[0, 1000, 1, 1002]],\n                         [[0, 1000, 1, 1002.1]]], tf.float32)\n    scores = tf.constant([[.9], [.75], [.6], [.95], [.5], [.3], [.01], [.01]])\n    score_thresh = 0.1\n    iou_thresh = .5\n    max_output_size = 3\n\n    exp_nms = [[0, 10, 1, 11],\n               [0, 0, 1, 1],\n               [0, 100, 1, 101]]\n    nms = post_processing.multiclass_non_max_suppression(\n        boxes, scores, score_thresh, iou_thresh, max_output_size)\n    with self.test_session() as sess:\n      nms_output = sess.run(nms.get())\n      self.assertAllClose(nms_output, exp_nms)\n\n  def test_multiclass_nms_select_with_separate_boxes(self):\n    boxes = tf.constant([[[0, 0, 1, 1], [0, 0, 4, 5]],\n                         [[0, 0.1, 1, 1.1], [0, 0.1, 2, 1.1]],\n                         [[0, -0.1, 1, 0.9], [0, -0.1, 1, 0.9]],\n                         [[0, 10, 1, 11], [0, 10, 1, 11]],\n                         [[0, 10.1, 1, 11.1], [0, 10.1, 1, 11.1]],\n                         [[0, 100, 1, 101], [0, 100, 1, 101]],\n                         [[0, 1000, 1, 1002], [0, 999, 2, 1004]],\n                         [[0, 1000, 1, 1002.1], [0, 999, 2, 1002.7]]],\n                        tf.float32)\n    scores = tf.constant([[.9, 0.01], [.75, 0.05],\n                          [.6, 0.01], [.95, 0],\n                          [.5, 0.01], [.3, 0.01],\n                          [.01, .85], [.01, .5]])\n    score_thresh = 0.1\n    iou_thresh = .5\n    max_output_size = 4\n\n    exp_nms_corners = [[0, 10, 1, 11],\n                       [0, 0, 1, 1],\n                       [0, 999, 2, 1004],\n                       [0, 100, 1, 101]]\n    exp_nms_scores = [.95, .9, .85, .3]\n    exp_nms_classes = [0, 0, 1, 0]\n\n    nms = post_processing.multiclass_non_max_suppression(\n        boxes, scores, score_thresh, iou_thresh, max_output_size)\n    with self.test_session() as sess:\n      nms_corners_output, nms_scores_output, nms_classes_output = sess.run(\n          [nms.get(), nms.get_field(fields.BoxListFields.scores),\n           nms.get_field(fields.BoxListFields.classes)])\n      self.assertAllClose(nms_corners_output, exp_nms_corners)\n      self.assertAllClose(nms_scores_output, exp_nms_scores)\n      self.assertAllClose(nms_classes_output, exp_nms_classes)\n\n  def test_batch_multiclass_nms_with_batch_size_1(self):\n    boxes = tf.constant([[[[0, 0, 1, 1], [0, 0, 4, 5]],\n                          [[0, 0.1, 1, 1.1], [0, 0.1, 2, 1.1]],\n                          [[0, -0.1, 1, 0.9], [0, -0.1, 1, 0.9]],\n                          [[0, 10, 1, 11], [0, 10, 1, 11]],\n                          [[0, 10.1, 1, 11.1], [0, 10.1, 1, 11.1]],\n                          [[0, 100, 1, 101], [0, 100, 1, 101]],\n                          [[0, 1000, 1, 1002], [0, 999, 2, 1004]],\n                          [[0, 1000, 1, 1002.1], [0, 999, 2, 1002.7]]]],\n                        tf.float32)\n    scores = tf.constant([[[.9, 0.01], [.75, 0.05],\n                           [.6, 0.01], [.95, 0],\n                           [.5, 0.01], [.3, 0.01],\n                           [.01, .85], [.01, .5]]])\n    score_thresh = 0.1\n    iou_thresh = .5\n    max_output_size = 4\n\n    exp_nms_corners = [[[0, 10, 1, 11],\n                        [0, 0, 1, 1],\n                        [0, 999, 2, 1004],\n                        [0, 100, 1, 101]]]\n    exp_nms_scores = [[.95, .9, .85, .3]]\n    exp_nms_classes = [[0, 0, 1, 0]]\n\n    (nmsed_boxes, nmsed_scores, nmsed_classes, nmsed_masks,\n     nmsed_additional_fields, num_detections\n    ) = post_processing.batch_multiclass_non_max_suppression(\n        boxes, scores, score_thresh, iou_thresh,\n        max_size_per_class=max_output_size, max_total_size=max_output_size)\n\n    self.assertIsNone(nmsed_masks)\n    self.assertIsNone(nmsed_additional_fields)\n\n    with self.test_session() as sess:\n      (nmsed_boxes, nmsed_scores, nmsed_classes,\n       num_detections) = sess.run([nmsed_boxes, nmsed_scores, nmsed_classes,\n                                   num_detections])\n      self.assertAllClose(nmsed_boxes, exp_nms_corners)\n      self.assertAllClose(nmsed_scores, exp_nms_scores)\n      self.assertAllClose(nmsed_classes, exp_nms_classes)\n      self.assertEqual(num_detections, [4])\n\n  def test_batch_multiclass_nms_with_batch_size_2(self):\n    boxes = tf.constant([[[[0, 0, 1, 1], [0, 0, 4, 5]],\n                          [[0, 0.1, 1, 1.1], [0, 0.1, 2, 1.1]],\n                          [[0, -0.1, 1, 0.9], [0, -0.1, 1, 0.9]],\n                          [[0, 10, 1, 11], [0, 10, 1, 11]]],\n                         [[[0, 10.1, 1, 11.1], [0, 10.1, 1, 11.1]],\n                          [[0, 100, 1, 101], [0, 100, 1, 101]],\n                          [[0, 1000, 1, 1002], [0, 999, 2, 1004]],\n                          [[0, 1000, 1, 1002.1], [0, 999, 2, 1002.7]]]],\n                        tf.float32)\n    scores = tf.constant([[[.9, 0.01], [.75, 0.05],\n                           [.6, 0.01], [.95, 0]],\n                          [[.5, 0.01], [.3, 0.01],\n                           [.01, .85], [.01, .5]]])\n    score_thresh = 0.1\n    iou_thresh = .5\n    max_output_size = 4\n\n    exp_nms_corners = np.array([[[0, 10, 1, 11],\n                                 [0, 0, 1, 1],\n                                 [0, 0, 0, 0],\n                                 [0, 0, 0, 0]],\n                                [[0, 999, 2, 1004],\n                                 [0, 10.1, 1, 11.1],\n                                 [0, 100, 1, 101],\n                                 [0, 0, 0, 0]]])\n    exp_nms_scores = np.array([[.95, .9, 0, 0],\n                               [.85, .5, .3, 0]])\n    exp_nms_classes = np.array([[0, 0, 0, 0],\n                                [1, 0, 0, 0]])\n\n    (nmsed_boxes, nmsed_scores, nmsed_classes, nmsed_masks,\n     nmsed_additional_fields, num_detections\n    ) = post_processing.batch_multiclass_non_max_suppression(\n        boxes, scores, score_thresh, iou_thresh,\n        max_size_per_class=max_output_size, max_total_size=max_output_size)\n\n    self.assertIsNone(nmsed_masks)\n    self.assertIsNone(nmsed_additional_fields)\n    # Check static shapes\n    self.assertAllEqual(nmsed_boxes.shape.as_list(),\n                        exp_nms_corners.shape)\n    self.assertAllEqual(nmsed_scores.shape.as_list(),\n                        exp_nms_scores.shape)\n    self.assertAllEqual(nmsed_classes.shape.as_list(),\n                        exp_nms_classes.shape)\n    self.assertEqual(num_detections.shape.as_list(), [2])\n\n    with self.test_session() as sess:\n      (nmsed_boxes, nmsed_scores, nmsed_classes,\n       num_detections) = sess.run([nmsed_boxes, nmsed_scores, nmsed_classes,\n                                   num_detections])\n      self.assertAllClose(nmsed_boxes, exp_nms_corners)\n      self.assertAllClose(nmsed_scores, exp_nms_scores)\n      self.assertAllClose(nmsed_classes, exp_nms_classes)\n      self.assertAllClose(num_detections, [2, 3])\n\n  def test_batch_multiclass_nms_with_per_batch_clip_window(self):\n    boxes = tf.constant([[[[0, 0, 1, 1], [0, 0, 4, 5]],\n                          [[0, 0.1, 1, 1.1], [0, 0.1, 2, 1.1]],\n                          [[0, -0.1, 1, 0.9], [0, -0.1, 1, 0.9]],\n                          [[0, 10, 1, 11], [0, 10, 1, 11]]],\n                         [[[0, 10.1, 1, 11.1], [0, 10.1, 1, 11.1]],\n                          [[0, 100, 1, 101], [0, 100, 1, 101]],\n                          [[0, 1000, 1, 1002], [0, 999, 2, 1004]],\n                          [[0, 1000, 1, 1002.1], [0, 999, 2, 1002.7]]]],\n                        tf.float32)\n    scores = tf.constant([[[.9, 0.01], [.75, 0.05],\n                           [.6, 0.01], [.95, 0]],\n                          [[.5, 0.01], [.3, 0.01],\n                           [.01, .85], [.01, .5]]])\n    clip_window = tf.constant([0., 0., 200., 200.])\n    score_thresh = 0.1\n    iou_thresh = .5\n    max_output_size = 4\n\n    exp_nms_corners = np.array([[[0, 10, 1, 11],\n                                 [0, 0, 1, 1],\n                                 [0, 0, 0, 0],\n                                 [0, 0, 0, 0]],\n                                [[0, 10.1, 1, 11.1],\n                                 [0, 100, 1, 101],\n                                 [0, 0, 0, 0],\n                                 [0, 0, 0, 0]]])\n    exp_nms_scores = np.array([[.95, .9, 0, 0],\n                               [.5, .3, 0, 0]])\n    exp_nms_classes = np.array([[0, 0, 0, 0],\n                                [0, 0, 0, 0]])\n\n    (nmsed_boxes, nmsed_scores, nmsed_classes, nmsed_masks,\n     nmsed_additional_fields, num_detections\n    ) = post_processing.batch_multiclass_non_max_suppression(\n        boxes, scores, score_thresh, iou_thresh,\n        max_size_per_class=max_output_size, max_total_size=max_output_size,\n        clip_window=clip_window)\n\n    self.assertIsNone(nmsed_masks)\n    self.assertIsNone(nmsed_additional_fields)\n    # Check static shapes\n    self.assertAllEqual(nmsed_boxes.shape.as_list(),\n                        exp_nms_corners.shape)\n    self.assertAllEqual(nmsed_scores.shape.as_list(),\n                        exp_nms_scores.shape)\n    self.assertAllEqual(nmsed_classes.shape.as_list(),\n                        exp_nms_classes.shape)\n    self.assertEqual(num_detections.shape.as_list(), [2])\n\n    with self.test_session() as sess:\n      (nmsed_boxes, nmsed_scores, nmsed_classes,\n       num_detections) = sess.run([nmsed_boxes, nmsed_scores, nmsed_classes,\n                                   num_detections])\n      self.assertAllClose(nmsed_boxes, exp_nms_corners)\n      self.assertAllClose(nmsed_scores, exp_nms_scores)\n      self.assertAllClose(nmsed_classes, exp_nms_classes)\n      self.assertAllClose(num_detections, [2, 2])\n\n  def test_batch_multiclass_nms_with_per_image_clip_window(self):\n    boxes = tf.constant([[[[0, 0, 1, 1], [0, 0, 4, 5]],\n                          [[0, 0.1, 1, 1.1], [0, 0.1, 2, 1.1]],\n                          [[0, -0.1, 1, 0.9], [0, -0.1, 1, 0.9]],\n                          [[0, 10, 1, 11], [0, 10, 1, 11]]],\n                         [[[0, 10.1, 1, 11.1], [0, 10.1, 1, 11.1]],\n                          [[0, 100, 1, 101], [0, 100, 1, 101]],\n                          [[0, 1000, 1, 1002], [0, 999, 2, 1004]],\n                          [[0, 1000, 1, 1002.1], [0, 999, 2, 1002.7]]]],\n                        tf.float32)\n    scores = tf.constant([[[.9, 0.01], [.75, 0.05],\n                           [.6, 0.01], [.95, 0]],\n                          [[.5, 0.01], [.3, 0.01],\n                           [.01, .85], [.01, .5]]])\n    clip_window = tf.constant([[0., 0., 5., 5.],\n                               [0., 0., 200., 200.]])\n    score_thresh = 0.1\n    iou_thresh = .5\n    max_output_size = 4\n\n    exp_nms_corners = np.array([[[0, 0, 1, 1],\n                                 [0, 0, 0, 0],\n                                 [0, 0, 0, 0],\n                                 [0, 0, 0, 0]],\n                                [[0, 10.1, 1, 11.1],\n                                 [0, 100, 1, 101],\n                                 [0, 0, 0, 0],\n                                 [0, 0, 0, 0]]])\n    exp_nms_scores = np.array([[.9, 0., 0., 0.],\n                               [.5, .3, 0, 0]])\n    exp_nms_classes = np.array([[0, 0, 0, 0],\n                                [0, 0, 0, 0]])\n\n    (nmsed_boxes, nmsed_scores, nmsed_classes, nmsed_masks,\n     nmsed_additional_fields, num_detections\n    ) = post_processing.batch_multiclass_non_max_suppression(\n        boxes, scores, score_thresh, iou_thresh,\n        max_size_per_class=max_output_size, max_total_size=max_output_size,\n        clip_window=clip_window)\n\n    self.assertIsNone(nmsed_masks)\n    self.assertIsNone(nmsed_additional_fields)\n    # Check static shapes\n    self.assertAllEqual(nmsed_boxes.shape.as_list(),\n                        exp_nms_corners.shape)\n    self.assertAllEqual(nmsed_scores.shape.as_list(),\n                        exp_nms_scores.shape)\n    self.assertAllEqual(nmsed_classes.shape.as_list(),\n                        exp_nms_classes.shape)\n    self.assertEqual(num_detections.shape.as_list(), [2])\n\n    with self.test_session() as sess:\n      (nmsed_boxes, nmsed_scores, nmsed_classes,\n       num_detections) = sess.run([nmsed_boxes, nmsed_scores, nmsed_classes,\n                                   num_detections])\n      self.assertAllClose(nmsed_boxes, exp_nms_corners)\n      self.assertAllClose(nmsed_scores, exp_nms_scores)\n      self.assertAllClose(nmsed_classes, exp_nms_classes)\n      self.assertAllClose(num_detections, [1, 2])\n\n  def test_batch_multiclass_nms_with_masks(self):\n    boxes = tf.constant([[[[0, 0, 1, 1], [0, 0, 4, 5]],\n                          [[0, 0.1, 1, 1.1], [0, 0.1, 2, 1.1]],\n                          [[0, -0.1, 1, 0.9], [0, -0.1, 1, 0.9]],\n                          [[0, 10, 1, 11], [0, 10, 1, 11]]],\n                         [[[0, 10.1, 1, 11.1], [0, 10.1, 1, 11.1]],\n                          [[0, 100, 1, 101], [0, 100, 1, 101]],\n                          [[0, 1000, 1, 1002], [0, 999, 2, 1004]],\n                          [[0, 1000, 1, 1002.1], [0, 999, 2, 1002.7]]]],\n                        tf.float32)\n    scores = tf.constant([[[.9, 0.01], [.75, 0.05],\n                           [.6, 0.01], [.95, 0]],\n                          [[.5, 0.01], [.3, 0.01],\n                           [.01, .85], [.01, .5]]])\n    masks = tf.constant([[[[[0, 1], [2, 3]], [[1, 2], [3, 4]]],\n                          [[[2, 3], [4, 5]], [[3, 4], [5, 6]]],\n                          [[[4, 5], [6, 7]], [[5, 6], [7, 8]]],\n                          [[[6, 7], [8, 9]], [[7, 8], [9, 10]]]],\n                         [[[[8, 9], [10, 11]], [[9, 10], [11, 12]]],\n                          [[[10, 11], [12, 13]], [[11, 12], [13, 14]]],\n                          [[[12, 13], [14, 15]], [[13, 14], [15, 16]]],\n                          [[[14, 15], [16, 17]], [[15, 16], [17, 18]]]]],\n                        tf.float32)\n    score_thresh = 0.1\n    iou_thresh = .5\n    max_output_size = 4\n\n    exp_nms_corners = np.array([[[0, 10, 1, 11],\n                                 [0, 0, 1, 1],\n                                 [0, 0, 0, 0],\n                                 [0, 0, 0, 0]],\n                                [[0, 999, 2, 1004],\n                                 [0, 10.1, 1, 11.1],\n                                 [0, 100, 1, 101],\n                                 [0, 0, 0, 0]]])\n    exp_nms_scores = np.array([[.95, .9, 0, 0],\n                               [.85, .5, .3, 0]])\n    exp_nms_classes = np.array([[0, 0, 0, 0],\n                                [1, 0, 0, 0]])\n    exp_nms_masks = np.array([[[[6, 7], [8, 9]],\n                               [[0, 1], [2, 3]],\n                               [[0, 0], [0, 0]],\n                               [[0, 0], [0, 0]]],\n                              [[[13, 14], [15, 16]],\n                               [[8, 9], [10, 11]],\n                               [[10, 11], [12, 13]],\n                               [[0, 0], [0, 0]]]])\n\n    (nmsed_boxes, nmsed_scores, nmsed_classes, nmsed_masks,\n     nmsed_additional_fields, num_detections\n    ) = post_processing.batch_multiclass_non_max_suppression(\n        boxes, scores, score_thresh, iou_thresh,\n        max_size_per_class=max_output_size, max_total_size=max_output_size,\n        masks=masks)\n\n    self.assertIsNone(nmsed_additional_fields)\n    # Check static shapes\n    self.assertAllEqual(nmsed_boxes.shape.as_list(), exp_nms_corners.shape)\n    self.assertAllEqual(nmsed_scores.shape.as_list(), exp_nms_scores.shape)\n    self.assertAllEqual(nmsed_classes.shape.as_list(), exp_nms_classes.shape)\n    self.assertAllEqual(nmsed_masks.shape.as_list(), exp_nms_masks.shape)\n    self.assertEqual(num_detections.shape.as_list(), [2])\n\n    with self.test_session() as sess:\n      (nmsed_boxes, nmsed_scores, nmsed_classes, nmsed_masks,\n       num_detections) = sess.run([nmsed_boxes, nmsed_scores, nmsed_classes,\n                                   nmsed_masks, num_detections])\n\n      self.assertAllClose(nmsed_boxes, exp_nms_corners)\n      self.assertAllClose(nmsed_scores, exp_nms_scores)\n      self.assertAllClose(nmsed_classes, exp_nms_classes)\n      self.assertAllClose(num_detections, [2, 3])\n      self.assertAllClose(nmsed_masks, exp_nms_masks)\n\n  def test_batch_multiclass_nms_with_additional_fields(self):\n    boxes = tf.constant([[[[0, 0, 1, 1], [0, 0, 4, 5]],\n                          [[0, 0.1, 1, 1.1], [0, 0.1, 2, 1.1]],\n                          [[0, -0.1, 1, 0.9], [0, -0.1, 1, 0.9]],\n                          [[0, 10, 1, 11], [0, 10, 1, 11]]],\n                         [[[0, 10.1, 1, 11.1], [0, 10.1, 1, 11.1]],\n                          [[0, 100, 1, 101], [0, 100, 1, 101]],\n                          [[0, 1000, 1, 1002], [0, 999, 2, 1004]],\n                          [[0, 1000, 1, 1002.1], [0, 999, 2, 1002.7]]]],\n                        tf.float32)\n    scores = tf.constant([[[.9, 0.01], [.75, 0.05],\n                           [.6, 0.01], [.95, 0]],\n                          [[.5, 0.01], [.3, 0.01],\n                           [.01, .85], [.01, .5]]])\n    additional_fields = {\n        \'keypoints\': tf.constant(\n            [[[[6, 7], [8, 9]],\n              [[0, 1], [2, 3]],\n              [[0, 0], [0, 0]],\n              [[0, 0], [0, 0]]],\n             [[[13, 14], [15, 16]],\n              [[8, 9], [10, 11]],\n              [[10, 11], [12, 13]],\n              [[0, 0], [0, 0]]]],\n            tf.float32)\n    }\n    score_thresh = 0.1\n    iou_thresh = .5\n    max_output_size = 4\n\n    exp_nms_corners = np.array([[[0, 10, 1, 11],\n                                 [0, 0, 1, 1],\n                                 [0, 0, 0, 0],\n                                 [0, 0, 0, 0]],\n                                [[0, 999, 2, 1004],\n                                 [0, 10.1, 1, 11.1],\n                                 [0, 100, 1, 101],\n                                 [0, 0, 0, 0]]])\n    exp_nms_scores = np.array([[.95, .9, 0, 0],\n                               [.85, .5, .3, 0]])\n    exp_nms_classes = np.array([[0, 0, 0, 0],\n                                [1, 0, 0, 0]])\n    exp_nms_additional_fields = {\n        \'keypoints\': np.array([[[[0, 0], [0, 0]],\n                                [[6, 7], [8, 9]],\n                                [[0, 0], [0, 0]],\n                                [[0, 0], [0, 0]]],\n                               [[[10, 11], [12, 13]],\n                                [[13, 14], [15, 16]],\n                                [[8, 9], [10, 11]],\n                                [[0, 0], [0, 0]]]])\n    }\n\n    (nmsed_boxes, nmsed_scores, nmsed_classes, nmsed_masks,\n     nmsed_additional_fields, num_detections\n    ) = post_processing.batch_multiclass_non_max_suppression(\n        boxes, scores, score_thresh, iou_thresh,\n        max_size_per_class=max_output_size, max_total_size=max_output_size,\n        additional_fields=additional_fields)\n\n    self.assertIsNone(nmsed_masks)\n    # Check static shapes\n    self.assertAllEqual(nmsed_boxes.shape.as_list(), exp_nms_corners.shape)\n    self.assertAllEqual(nmsed_scores.shape.as_list(), exp_nms_scores.shape)\n    self.assertAllEqual(nmsed_classes.shape.as_list(), exp_nms_classes.shape)\n    self.assertEqual(len(nmsed_additional_fields),\n                     len(exp_nms_additional_fields))\n    for key in exp_nms_additional_fields:\n      self.assertAllEqual(nmsed_additional_fields[key].shape.as_list(),\n                          exp_nms_additional_fields[key].shape)\n    self.assertEqual(num_detections.shape.as_list(), [2])\n\n    with self.test_session() as sess:\n      (nmsed_boxes, nmsed_scores, nmsed_classes, nmsed_additional_fields,\n       num_detections) = sess.run([nmsed_boxes, nmsed_scores, nmsed_classes,\n                                   nmsed_additional_fields, num_detections])\n\n      self.assertAllClose(nmsed_boxes, exp_nms_corners)\n      self.assertAllClose(nmsed_scores, exp_nms_scores)\n      self.assertAllClose(nmsed_classes, exp_nms_classes)\n      for key in exp_nms_additional_fields:\n        self.assertAllClose(nmsed_additional_fields[key],\n                            exp_nms_additional_fields[key])\n      self.assertAllClose(num_detections, [2, 3])\n\n  def test_batch_multiclass_nms_with_dynamic_batch_size(self):\n    boxes_placeholder = tf.placeholder(tf.float32, shape=(None, None, 2, 4))\n    scores_placeholder = tf.placeholder(tf.float32, shape=(None, None, 2))\n    masks_placeholder = tf.placeholder(tf.float32, shape=(None, None, 2, 2, 2))\n\n    boxes = np.array([[[[0, 0, 1, 1], [0, 0, 4, 5]],\n                       [[0, 0.1, 1, 1.1], [0, 0.1, 2, 1.1]],\n                       [[0, -0.1, 1, 0.9], [0, -0.1, 1, 0.9]],\n                       [[0, 10, 1, 11], [0, 10, 1, 11]]],\n                      [[[0, 10.1, 1, 11.1], [0, 10.1, 1, 11.1]],\n                       [[0, 100, 1, 101], [0, 100, 1, 101]],\n                       [[0, 1000, 1, 1002], [0, 999, 2, 1004]],\n                       [[0, 1000, 1, 1002.1], [0, 999, 2, 1002.7]]]])\n    scores = np.array([[[.9, 0.01], [.75, 0.05],\n                        [.6, 0.01], [.95, 0]],\n                       [[.5, 0.01], [.3, 0.01],\n                        [.01, .85], [.01, .5]]])\n    masks = np.array([[[[[0, 1], [2, 3]], [[1, 2], [3, 4]]],\n                       [[[2, 3], [4, 5]], [[3, 4], [5, 6]]],\n                       [[[4, 5], [6, 7]], [[5, 6], [7, 8]]],\n                       [[[6, 7], [8, 9]], [[7, 8], [9, 10]]]],\n                      [[[[8, 9], [10, 11]], [[9, 10], [11, 12]]],\n                       [[[10, 11], [12, 13]], [[11, 12], [13, 14]]],\n                       [[[12, 13], [14, 15]], [[13, 14], [15, 16]]],\n                       [[[14, 15], [16, 17]], [[15, 16], [17, 18]]]]])\n    score_thresh = 0.1\n    iou_thresh = .5\n    max_output_size = 4\n\n    exp_nms_corners = np.array([[[0, 10, 1, 11],\n                                 [0, 0, 1, 1],\n                                 [0, 0, 0, 0],\n                                 [0, 0, 0, 0]],\n                                [[0, 999, 2, 1004],\n                                 [0, 10.1, 1, 11.1],\n                                 [0, 100, 1, 101],\n                                 [0, 0, 0, 0]]])\n    exp_nms_scores = np.array([[.95, .9, 0, 0],\n                               [.85, .5, .3, 0]])\n    exp_nms_classes = np.array([[0, 0, 0, 0],\n                                [1, 0, 0, 0]])\n    exp_nms_masks = np.array([[[[6, 7], [8, 9]],\n                               [[0, 1], [2, 3]],\n                               [[0, 0], [0, 0]],\n                               [[0, 0], [0, 0]]],\n                              [[[13, 14], [15, 16]],\n                               [[8, 9], [10, 11]],\n                               [[10, 11], [12, 13]],\n                               [[0, 0], [0, 0]]]])\n\n    (nmsed_boxes, nmsed_scores, nmsed_classes, nmsed_masks,\n     nmsed_additional_fields, num_detections\n    ) = post_processing.batch_multiclass_non_max_suppression(\n        boxes_placeholder, scores_placeholder, score_thresh, iou_thresh,\n        max_size_per_class=max_output_size, max_total_size=max_output_size,\n        masks=masks_placeholder)\n\n    self.assertIsNone(nmsed_additional_fields)\n    # Check static shapes\n    self.assertAllEqual(nmsed_boxes.shape.as_list(), [None, 4, 4])\n    self.assertAllEqual(nmsed_scores.shape.as_list(), [None, 4])\n    self.assertAllEqual(nmsed_classes.shape.as_list(), [None, 4])\n    self.assertAllEqual(nmsed_masks.shape.as_list(), [None, 4, 2, 2])\n    self.assertEqual(num_detections.shape.as_list(), [None])\n\n    with self.test_session() as sess:\n      (nmsed_boxes, nmsed_scores, nmsed_classes, nmsed_masks,\n       num_detections) = sess.run([nmsed_boxes, nmsed_scores, nmsed_classes,\n                                   nmsed_masks, num_detections],\n                                  feed_dict={boxes_placeholder: boxes,\n                                             scores_placeholder: scores,\n                                             masks_placeholder: masks})\n      self.assertAllClose(nmsed_boxes, exp_nms_corners)\n      self.assertAllClose(nmsed_scores, exp_nms_scores)\n      self.assertAllClose(nmsed_classes, exp_nms_classes)\n      self.assertAllClose(num_detections, [2, 3])\n      self.assertAllClose(nmsed_masks, exp_nms_masks)\n\n  def test_batch_multiclass_nms_with_masks_and_num_valid_boxes(self):\n    boxes = tf.constant([[[[0, 0, 1, 1], [0, 0, 4, 5]],\n                          [[0, 0.1, 1, 1.1], [0, 0.1, 2, 1.1]],\n                          [[0, -0.1, 1, 0.9], [0, -0.1, 1, 0.9]],\n                          [[0, 10, 1, 11], [0, 10, 1, 11]]],\n                         [[[0, 10.1, 1, 11.1], [0, 10.1, 1, 11.1]],\n                          [[0, 100, 1, 101], [0, 100, 1, 101]],\n                          [[0, 1000, 1, 1002], [0, 999, 2, 1004]],\n                          [[0, 1000, 1, 1002.1], [0, 999, 2, 1002.7]]]],\n                        tf.float32)\n    scores = tf.constant([[[.9, 0.01], [.75, 0.05],\n                           [.6, 0.01], [.95, 0]],\n                          [[.5, 0.01], [.3, 0.01],\n                           [.01, .85], [.01, .5]]])\n    masks = tf.constant([[[[[0, 1], [2, 3]], [[1, 2], [3, 4]]],\n                          [[[2, 3], [4, 5]], [[3, 4], [5, 6]]],\n                          [[[4, 5], [6, 7]], [[5, 6], [7, 8]]],\n                          [[[6, 7], [8, 9]], [[7, 8], [9, 10]]]],\n                         [[[[8, 9], [10, 11]], [[9, 10], [11, 12]]],\n                          [[[10, 11], [12, 13]], [[11, 12], [13, 14]]],\n                          [[[12, 13], [14, 15]], [[13, 14], [15, 16]]],\n                          [[[14, 15], [16, 17]], [[15, 16], [17, 18]]]]],\n                        tf.float32)\n    num_valid_boxes = tf.constant([1, 1], tf.int32)\n    score_thresh = 0.1\n    iou_thresh = .5\n    max_output_size = 4\n\n    exp_nms_corners = [[[0, 0, 1, 1],\n                        [0, 0, 0, 0],\n                        [0, 0, 0, 0],\n                        [0, 0, 0, 0]],\n                       [[0, 10.1, 1, 11.1],\n                        [0, 0, 0, 0],\n                        [0, 0, 0, 0],\n                        [0, 0, 0, 0]]]\n    exp_nms_scores = [[.9, 0, 0, 0],\n                      [.5, 0, 0, 0]]\n    exp_nms_classes = [[0, 0, 0, 0],\n                       [0, 0, 0, 0]]\n    exp_nms_masks = [[[[0, 1], [2, 3]],\n                      [[0, 0], [0, 0]],\n                      [[0, 0], [0, 0]],\n                      [[0, 0], [0, 0]]],\n                     [[[8, 9], [10, 11]],\n                      [[0, 0], [0, 0]],\n                      [[0, 0], [0, 0]],\n                      [[0, 0], [0, 0]]]]\n\n    (nmsed_boxes, nmsed_scores, nmsed_classes, nmsed_masks,\n     nmsed_additional_fields, num_detections\n    ) = post_processing.batch_multiclass_non_max_suppression(\n        boxes, scores, score_thresh, iou_thresh,\n        max_size_per_class=max_output_size, max_total_size=max_output_size,\n        num_valid_boxes=num_valid_boxes, masks=masks)\n\n    self.assertIsNone(nmsed_additional_fields)\n\n    with self.test_session() as sess:\n      (nmsed_boxes, nmsed_scores, nmsed_classes, nmsed_masks,\n       num_detections) = sess.run([nmsed_boxes, nmsed_scores, nmsed_classes,\n                                   nmsed_masks, num_detections])\n      self.assertAllClose(nmsed_boxes, exp_nms_corners)\n      self.assertAllClose(nmsed_scores, exp_nms_scores)\n      self.assertAllClose(nmsed_classes, exp_nms_classes)\n      self.assertAllClose(num_detections, [1, 1])\n      self.assertAllClose(nmsed_masks, exp_nms_masks)\n\n  def test_batch_multiclass_nms_with_additional_fields_and_num_valid_boxes(\n      self):\n    boxes = tf.constant([[[[0, 0, 1, 1], [0, 0, 4, 5]],\n                          [[0, 0.1, 1, 1.1], [0, 0.1, 2, 1.1]],\n                          [[0, -0.1, 1, 0.9], [0, -0.1, 1, 0.9]],\n                          [[0, 10, 1, 11], [0, 10, 1, 11]]],\n                         [[[0, 10.1, 1, 11.1], [0, 10.1, 1, 11.1]],\n                          [[0, 100, 1, 101], [0, 100, 1, 101]],\n                          [[0, 1000, 1, 1002], [0, 999, 2, 1004]],\n                          [[0, 1000, 1, 1002.1], [0, 999, 2, 1002.7]]]],\n                        tf.float32)\n    scores = tf.constant([[[.9, 0.01], [.75, 0.05],\n                           [.6, 0.01], [.95, 0]],\n                          [[.5, 0.01], [.3, 0.01],\n                           [.01, .85], [.01, .5]]])\n    additional_fields = {\n        \'keypoints\': tf.constant(\n            [[[[6, 7], [8, 9]],\n              [[0, 1], [2, 3]],\n              [[0, 0], [0, 0]],\n              [[0, 0], [0, 0]]],\n             [[[13, 14], [15, 16]],\n              [[8, 9], [10, 11]],\n              [[10, 11], [12, 13]],\n              [[0, 0], [0, 0]]]],\n            tf.float32)\n    }\n    num_valid_boxes = tf.constant([1, 1], tf.int32)\n    score_thresh = 0.1\n    iou_thresh = .5\n    max_output_size = 4\n\n    exp_nms_corners = [[[0, 0, 1, 1],\n                        [0, 0, 0, 0],\n                        [0, 0, 0, 0],\n                        [0, 0, 0, 0]],\n                       [[0, 10.1, 1, 11.1],\n                        [0, 0, 0, 0],\n                        [0, 0, 0, 0],\n                        [0, 0, 0, 0]]]\n    exp_nms_scores = [[.9, 0, 0, 0],\n                      [.5, 0, 0, 0]]\n    exp_nms_classes = [[0, 0, 0, 0],\n                       [0, 0, 0, 0]]\n    exp_nms_additional_fields = {\n        \'keypoints\': np.array([[[[6, 7], [8, 9]],\n                                [[0, 0], [0, 0]],\n                                [[0, 0], [0, 0]],\n                                [[0, 0], [0, 0]]],\n                               [[[13, 14], [15, 16]],\n                                [[0, 0], [0, 0]],\n                                [[0, 0], [0, 0]],\n                                [[0, 0], [0, 0]]]])\n    }\n\n    (nmsed_boxes, nmsed_scores, nmsed_classes, nmsed_masks,\n     nmsed_additional_fields, num_detections\n    ) = post_processing.batch_multiclass_non_max_suppression(\n        boxes, scores, score_thresh, iou_thresh,\n        max_size_per_class=max_output_size, max_total_size=max_output_size,\n        num_valid_boxes=num_valid_boxes,\n        additional_fields=additional_fields)\n\n    self.assertIsNone(nmsed_masks)\n\n    with self.test_session() as sess:\n      (nmsed_boxes, nmsed_scores, nmsed_classes, nmsed_additional_fields,\n       num_detections) = sess.run([nmsed_boxes, nmsed_scores, nmsed_classes,\n                                   nmsed_additional_fields, num_detections])\n\n      self.assertAllClose(nmsed_boxes, exp_nms_corners)\n      self.assertAllClose(nmsed_scores, exp_nms_scores)\n      self.assertAllClose(nmsed_classes, exp_nms_classes)\n      for key in exp_nms_additional_fields:\n        self.assertAllClose(nmsed_additional_fields[key],\n                            exp_nms_additional_fields[key])\n      self.assertAllClose(num_detections, [1, 1])\n\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
src/object_detection/core/prefetcher.py,4,"b'# Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n""""""Provides functions to prefetch tensors to feed into models.""""""\nimport tensorflow as tf\n\n\ndef prefetch(tensor_dict, capacity):\n  """"""Creates a prefetch queue for tensors.\n\n  Creates a FIFO queue to asynchronously enqueue tensor_dicts and returns a\n  dequeue op that evaluates to a tensor_dict. This function is useful in\n  prefetching preprocessed tensors so that the data is readily available for\n  consumers.\n\n  Example input pipeline when you don\'t need batching:\n  ----------------------------------------------------\n  key, string_tensor = slim.parallel_reader.parallel_read(...)\n  tensor_dict = decoder.decode(string_tensor)\n  tensor_dict = preprocessor.preprocess(tensor_dict, ...)\n  prefetch_queue = prefetcher.prefetch(tensor_dict, capacity=20)\n  tensor_dict = prefetch_queue.dequeue()\n  outputs = Model(tensor_dict)\n  ...\n  ----------------------------------------------------\n\n  For input pipelines with batching, refer to core/batcher.py\n\n  Args:\n    tensor_dict: a dictionary of tensors to prefetch.\n    capacity: the size of the prefetch queue.\n\n  Returns:\n    a FIFO prefetcher queue\n  """"""\n  names = list(tensor_dict.keys())\n  dtypes = [t.dtype for t in tensor_dict.values()]\n  shapes = [t.get_shape() for t in tensor_dict.values()]\n  prefetch_queue = tf.PaddingFIFOQueue(capacity, dtypes=dtypes,\n                                       shapes=shapes,\n                                       names=names,\n                                       name=\'prefetch_queue\')\n  enqueue_op = prefetch_queue.enqueue(tensor_dict)\n  tf.train.queue_runner.add_queue_runner(tf.train.queue_runner.QueueRunner(\n      prefetch_queue, [enqueue_op]))\n  tf.summary.scalar(\'queue/%s/fraction_of_%d_full\' % (prefetch_queue.name,\n                                                      capacity),\n                    tf.to_float(prefetch_queue.size()) * (1. / capacity))\n  return prefetch_queue\n'"
src/object_detection/core/prefetcher_test.py,19,"b'# Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n""""""Tests for object_detection.core.prefetcher.""""""\nimport tensorflow as tf\n\nfrom object_detection.core import prefetcher\n\nslim = tf.contrib.slim\n\n\nclass PrefetcherTest(tf.test.TestCase):\n\n  def test_prefetch_tensors_with_fully_defined_shapes(self):\n    with self.test_session() as sess:\n      batch_size = 10\n      image_size = 32\n      num_batches = 5\n      examples = tf.Variable(tf.constant(0, dtype=tf.int64))\n      counter = examples.count_up_to(num_batches)\n      image = tf.random_normal([batch_size, image_size,\n                                image_size, 3],\n                               dtype=tf.float32,\n                               name=\'images\')\n      label = tf.random_uniform([batch_size, 1], 0, 10,\n                                dtype=tf.int32, name=\'labels\')\n\n      prefetch_queue = prefetcher.prefetch(tensor_dict={\'counter\': counter,\n                                                        \'image\': image,\n                                                        \'label\': label},\n                                           capacity=100)\n      tensor_dict = prefetch_queue.dequeue()\n\n      self.assertAllEqual(tensor_dict[\'image\'].get_shape().as_list(),\n                          [batch_size, image_size, image_size, 3])\n      self.assertAllEqual(tensor_dict[\'label\'].get_shape().as_list(),\n                          [batch_size, 1])\n\n      tf.initialize_all_variables().run()\n      with slim.queues.QueueRunners(sess):\n        for _ in range(num_batches):\n          results = sess.run(tensor_dict)\n          self.assertEquals(results[\'image\'].shape,\n                            (batch_size, image_size, image_size, 3))\n          self.assertEquals(results[\'label\'].shape, (batch_size, 1))\n        with self.assertRaises(tf.errors.OutOfRangeError):\n          sess.run(tensor_dict)\n\n  def test_prefetch_tensors_with_partially_defined_shapes(self):\n    with self.test_session() as sess:\n      batch_size = 10\n      image_size = 32\n      num_batches = 5\n      examples = tf.Variable(tf.constant(0, dtype=tf.int64))\n      counter = examples.count_up_to(num_batches)\n      image = tf.random_normal([batch_size,\n                                tf.Variable(image_size),\n                                tf.Variable(image_size), 3],\n                               dtype=tf.float32,\n                               name=\'image\')\n      image.set_shape([batch_size, None, None, 3])\n      label = tf.random_uniform([batch_size, tf.Variable(1)], 0,\n                                10, dtype=tf.int32, name=\'label\')\n      label.set_shape([batch_size, None])\n\n      prefetch_queue = prefetcher.prefetch(tensor_dict={\'counter\': counter,\n                                                        \'image\': image,\n                                                        \'label\': label},\n                                           capacity=100)\n      tensor_dict = prefetch_queue.dequeue()\n\n      self.assertAllEqual(tensor_dict[\'image\'].get_shape().as_list(),\n                          [batch_size, None, None, 3])\n      self.assertAllEqual(tensor_dict[\'label\'].get_shape().as_list(),\n                          [batch_size, None])\n\n      tf.initialize_all_variables().run()\n      with slim.queues.QueueRunners(sess):\n        for _ in range(num_batches):\n          results = sess.run(tensor_dict)\n          self.assertEquals(results[\'image\'].shape,\n                            (batch_size, image_size, image_size, 3))\n          self.assertEquals(results[\'label\'].shape, (batch_size, 1))\n        with self.assertRaises(tf.errors.OutOfRangeError):\n          sess.run(tensor_dict)\n\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
src/object_detection/core/preprocessor.py,299,"b'# Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Preprocess images and bounding boxes for detection.\n\nWe perform two sets of operations in preprocessing stage:\n(a) operations that are applied to both training and testing data,\n(b) operations that are applied only to training data for the purpose of\n    data augmentation.\n\nA preprocessing function receives a set of inputs,\ne.g. an image and bounding boxes,\nperforms an operation on them, and returns them.\nSome examples are: randomly cropping the image, randomly mirroring the image,\n                   randomly changing the brightness, contrast, hue and\n                   randomly jittering the bounding boxes.\n\nThe preprocess function receives a tensor_dict which is a dictionary that maps\ndifferent field names to their tensors. For example,\ntensor_dict[fields.InputDataFields.image] holds the image tensor.\nThe image is a rank 4 tensor: [1, height, width, channels] with\ndtype=tf.float32. The groundtruth_boxes is a rank 2 tensor: [N, 4] where\nin each row there is a box with [ymin xmin ymax xmax].\nBoxes are in normalized coordinates meaning\ntheir coordinate values range in [0, 1]\n\nTo preprocess multiple images with the same operations in cases where\nnondeterministic operations are used, a preprocessor_cache.PreprocessorCache\nobject can be passed into the preprocess function or individual operations.\nAll nondeterministic operations except random_jitter_boxes support caching.\nE.g.\nLet tensor_dict{1,2,3,4,5} be copies of the same inputs.\nLet preprocess_options contain nondeterministic operation(s) excluding\nrandom_jitter_boxes.\n\ncache1 = preprocessor_cache.PreprocessorCache()\ncache2 = preprocessor_cache.PreprocessorCache()\na = preprocess(tensor_dict1, preprocess_options, preprocess_vars_cache=cache1)\nb = preprocess(tensor_dict2, preprocess_options, preprocess_vars_cache=cache1)\nc = preprocess(tensor_dict3, preprocess_options, preprocess_vars_cache=cache2)\nd = preprocess(tensor_dict4, preprocess_options, preprocess_vars_cache=cache2)\ne = preprocess(tensor_dict5, preprocess_options)\n\nThen correspondings tensors of object pairs (a,b) and (c,d)\nare guaranteed to be equal element-wise, but the equality of any other object\npair cannot be determined.\n\nImportant Note: In tensor_dict, images is a rank 4 tensor, but preprocessing\nfunctions receive a rank 3 tensor for processing the image. Thus, inside the\npreprocess function we squeeze the image to become a rank 3 tensor and then\nwe pass it to the functions. At the end of the preprocess we expand the image\nback to rank 4.\n""""""\n\nimport functools\nimport inspect\nimport sys\nimport tensorflow as tf\n\nfrom tensorflow.python.ops import control_flow_ops\n\nfrom object_detection.core import box_list\nfrom object_detection.core import box_list_ops\nfrom object_detection.core import keypoint_ops\nfrom object_detection.core import preprocessor_cache\nfrom object_detection.core import standard_fields as fields\nfrom object_detection.utils import shape_utils\n\n\ndef _apply_with_random_selector(x,\n                                func,\n                                num_cases,\n                                preprocess_vars_cache=None,\n                                key=\'\'):\n  """"""Computes func(x, sel), with sel sampled from [0...num_cases-1].\n\n  If both preprocess_vars_cache AND key are the same between two calls, sel will\n  be the same value in both calls.\n\n  Args:\n    x: input Tensor.\n    func: Python function to apply.\n    num_cases: Python int32, number of cases to sample sel from.\n    preprocess_vars_cache: PreprocessorCache object that records previously\n                           performed augmentations. Updated in-place. If this\n                           function is called multiple times with the same\n                           non-null cache, it will perform deterministically.\n    key: variable identifier for preprocess_vars_cache.\n\n  Returns:\n    The result of func(x, sel), where func receives the value of the\n    selector as a python integer, but sel is sampled dynamically.\n  """"""\n  generator_func = functools.partial(\n      tf.random_uniform, [], maxval=num_cases, dtype=tf.int32)\n  rand_sel = _get_or_create_preprocess_rand_vars(\n      generator_func, preprocessor_cache.PreprocessorCache.SELECTOR,\n      preprocess_vars_cache, key)\n\n  # Pass the real x only to one of the func calls.\n  return control_flow_ops.merge([func(\n      control_flow_ops.switch(x, tf.equal(rand_sel, case))[1], case)\n                                 for case in range(num_cases)])[0]\n\n\ndef _apply_with_random_selector_tuples(x,\n                                       func,\n                                       num_cases,\n                                       preprocess_vars_cache=None,\n                                       key=\'\'):\n  """"""Computes func(x, sel), with sel sampled from [0...num_cases-1].\n\n  If both preprocess_vars_cache AND key are the same between two calls, sel will\n  be the same value in both calls.\n\n  Args:\n    x: A tuple of input tensors.\n    func: Python function to apply.\n    num_cases: Python int32, number of cases to sample sel from.\n    preprocess_vars_cache: PreprocessorCache object that records previously\n                           performed augmentations. Updated in-place. If this\n                           function is called multiple times with the same\n                           non-null cache, it will perform deterministically.\n    key: variable identifier for preprocess_vars_cache.\n\n  Returns:\n    The result of func(x, sel), where func receives the value of the\n    selector as a python integer, but sel is sampled dynamically.\n  """"""\n  num_inputs = len(x)\n  generator_func = functools.partial(\n      tf.random_uniform, [], maxval=num_cases, dtype=tf.int32)\n  rand_sel = _get_or_create_preprocess_rand_vars(\n      generator_func, preprocessor_cache.PreprocessorCache.SELECTOR_TUPLES,\n      preprocess_vars_cache, key)\n\n  # Pass the real x only to one of the func calls.\n  tuples = [list() for t in x]\n  for case in range(num_cases):\n    new_x = [control_flow_ops.switch(t, tf.equal(rand_sel, case))[1] for t in x]\n    output = func(tuple(new_x), case)\n    for j in range(num_inputs):\n      tuples[j].append(output[j])\n\n  for i in range(num_inputs):\n    tuples[i] = control_flow_ops.merge(tuples[i])[0]\n  return tuple(tuples)\n\n\ndef _get_or_create_preprocess_rand_vars(generator_func,\n                                        function_id,\n                                        preprocess_vars_cache,\n                                        key=\'\'):\n  """"""Returns a tensor stored in preprocess_vars_cache or using generator_func.\n\n  If the tensor was previously generated and appears in the PreprocessorCache,\n  the previously generated tensor will be returned. Otherwise, a new tensor\n  is generated using generator_func and stored in the cache.\n\n  Args:\n    generator_func: A 0-argument function that generates a tensor.\n    function_id: identifier for the preprocessing function used.\n    preprocess_vars_cache: PreprocessorCache object that records previously\n                           performed augmentations. Updated in-place. If this\n                           function is called multiple times with the same\n                           non-null cache, it will perform deterministically.\n    key: identifier for the variable stored.\n  Returns:\n    The generated tensor.\n  """"""\n  if preprocess_vars_cache is not None:\n    var = preprocess_vars_cache.get(function_id, key)\n    if var is None:\n      var = generator_func()\n      preprocess_vars_cache.update(function_id, key, var)\n  else:\n    var = generator_func()\n  return var\n\n\ndef _random_integer(minval, maxval, seed):\n  """"""Returns a random 0-D tensor between minval and maxval.\n\n  Args:\n    minval: minimum value of the random tensor.\n    maxval: maximum value of the random tensor.\n    seed: random seed.\n\n  Returns:\n    A random 0-D tensor between minval and maxval.\n  """"""\n  return tf.random_uniform(\n      [], minval=minval, maxval=maxval, dtype=tf.int32, seed=seed)\n\n\n# TODO(mttang): This method is needed because the current\n# tf.image.rgb_to_grayscale method does not support quantization. Replace with\n# tf.image.rgb_to_grayscale after quantization support is added.\ndef _rgb_to_grayscale(images, name=None):\n  """"""Converts one or more images from RGB to Grayscale.\n\n  Outputs a tensor of the same `DType` and rank as `images`.  The size of the\n  last dimension of the output is 1, containing the Grayscale value of the\n  pixels.\n\n  Args:\n    images: The RGB tensor to convert. Last dimension must have size 3 and\n      should contain RGB values.\n    name: A name for the operation (optional).\n\n  Returns:\n    The converted grayscale image(s).\n  """"""\n  with tf.name_scope(name, \'rgb_to_grayscale\', [images]) as name:\n    images = tf.convert_to_tensor(images, name=\'images\')\n    # Remember original dtype to so we can convert back if needed\n    orig_dtype = images.dtype\n    flt_image = tf.image.convert_image_dtype(images, tf.float32)\n\n    # Reference for converting between RGB and grayscale.\n    # https://en.wikipedia.org/wiki/Luma_%28video%29\n    rgb_weights = [0.2989, 0.5870, 0.1140]\n    rank_1 = tf.expand_dims(tf.rank(images) - 1, 0)\n    gray_float = tf.reduce_sum(\n        flt_image * rgb_weights, rank_1, keep_dims=True)\n    gray_float.set_shape(images.get_shape()[:-1].concatenate([1]))\n    return tf.image.convert_image_dtype(gray_float, orig_dtype, name=name)\n\n\ndef normalize_image(image, original_minval, original_maxval, target_minval,\n                    target_maxval):\n  """"""Normalizes pixel values in the image.\n\n  Moves the pixel values from the current [original_minval, original_maxval]\n  range to a the [target_minval, target_maxval] range.\n\n  Args:\n    image: rank 3 float32 tensor containing 1\n           image -> [height, width, channels].\n    original_minval: current image minimum value.\n    original_maxval: current image maximum value.\n    target_minval: target image minimum value.\n    target_maxval: target image maximum value.\n\n  Returns:\n    image: image which is the same shape as input image.\n  """"""\n  with tf.name_scope(\'NormalizeImage\', values=[image]):\n    original_minval = float(original_minval)\n    original_maxval = float(original_maxval)\n    target_minval = float(target_minval)\n    target_maxval = float(target_maxval)\n    image = tf.to_float(image)\n    image = tf.subtract(image, original_minval)\n    image = tf.multiply(image, (target_maxval - target_minval) /\n                        (original_maxval - original_minval))\n    image = tf.add(image, target_minval)\n    return image\n\n\ndef retain_boxes_above_threshold(boxes,\n                                 labels,\n                                 label_scores,\n                                 masks=None,\n                                 keypoints=None,\n                                 threshold=0.0):\n  """"""Retains boxes whose label score is above a given threshold.\n\n  If the label score for a box is missing (represented by NaN), the box is\n  retained. The boxes that don\'t pass the threshold will not appear in the\n  returned tensor.\n\n  Args:\n    boxes: float32 tensor of shape [num_instance, 4] representing boxes\n      location in normalized coordinates.\n    labels: rank 1 int32 tensor of shape [num_instance] containing the object\n      classes.\n    label_scores: float32 tensor of shape [num_instance] representing the\n      score for each box.\n    masks: (optional) rank 3 float32 tensor with shape\n      [num_instances, height, width] containing instance masks. The masks are of\n      the same height, width as the input `image`.\n    keypoints: (optional) rank 3 float32 tensor with shape\n      [num_instances, num_keypoints, 2]. The keypoints are in y-x normalized\n      coordinates.\n    threshold: scalar python float.\n\n  Returns:\n    retained_boxes: [num_retained_instance, 4]\n    retianed_labels: [num_retained_instance]\n    retained_label_scores: [num_retained_instance]\n\n    If masks, or keypoints are not None, the function also returns:\n\n    retained_masks: [num_retained_instance, height, width]\n    retained_keypoints: [num_retained_instance, num_keypoints, 2]\n  """"""\n  with tf.name_scope(\'RetainBoxesAboveThreshold\',\n                     values=[boxes, labels, label_scores]):\n    indices = tf.where(\n        tf.logical_or(label_scores > threshold, tf.is_nan(label_scores)))\n    indices = tf.squeeze(indices, axis=1)\n    retained_boxes = tf.gather(boxes, indices)\n    retained_labels = tf.gather(labels, indices)\n    retained_label_scores = tf.gather(label_scores, indices)\n    result = [retained_boxes, retained_labels, retained_label_scores]\n\n    if masks is not None:\n      retained_masks = tf.gather(masks, indices)\n      result.append(retained_masks)\n\n    if keypoints is not None:\n      retained_keypoints = tf.gather(keypoints, indices)\n      result.append(retained_keypoints)\n\n    return result\n\n\ndef _flip_boxes_left_right(boxes):\n  """"""Left-right flip the boxes.\n\n  Args:\n    boxes: rank 2 float32 tensor containing the bounding boxes -> [N, 4].\n           Boxes are in normalized form meaning their coordinates vary\n           between [0, 1].\n           Each row is in the form of [ymin, xmin, ymax, xmax].\n\n  Returns:\n    Flipped boxes.\n  """"""\n  ymin, xmin, ymax, xmax = tf.split(value=boxes, num_or_size_splits=4, axis=1)\n  flipped_xmin = tf.subtract(1.0, xmax)\n  flipped_xmax = tf.subtract(1.0, xmin)\n  flipped_boxes = tf.concat([ymin, flipped_xmin, ymax, flipped_xmax], 1)\n  return flipped_boxes\n\n\ndef _flip_boxes_up_down(boxes):\n  """"""Up-down flip the boxes.\n\n  Args:\n    boxes: rank 2 float32 tensor containing the bounding boxes -> [N, 4].\n           Boxes are in normalized form meaning their coordinates vary\n           between [0, 1].\n           Each row is in the form of [ymin, xmin, ymax, xmax].\n\n  Returns:\n    Flipped boxes.\n  """"""\n  ymin, xmin, ymax, xmax = tf.split(value=boxes, num_or_size_splits=4, axis=1)\n  flipped_ymin = tf.subtract(1.0, ymax)\n  flipped_ymax = tf.subtract(1.0, ymin)\n  flipped_boxes = tf.concat([flipped_ymin, xmin, flipped_ymax, xmax], 1)\n  return flipped_boxes\n\n\ndef _rot90_boxes(boxes):\n  """"""Rotate boxes counter-clockwise by 90 degrees.\n\n  Args:\n    boxes: rank 2 float32 tensor containing the bounding boxes -> [N, 4].\n           Boxes are in normalized form meaning their coordinates vary\n           between [0, 1].\n           Each row is in the form of [ymin, xmin, ymax, xmax].\n\n  Returns:\n    Rotated boxes.\n  """"""\n  ymin, xmin, ymax, xmax = tf.split(value=boxes, num_or_size_splits=4, axis=1)\n  rotated_ymin = tf.subtract(1.0, xmax)\n  rotated_ymax = tf.subtract(1.0, xmin)\n  rotated_xmin = ymin\n  rotated_xmax = ymax\n  rotated_boxes = tf.concat(\n      [rotated_ymin, rotated_xmin, rotated_ymax, rotated_xmax], 1)\n  return rotated_boxes\n\n\ndef _flip_masks_left_right(masks):\n  """"""Left-right flip masks.\n\n  Args:\n    masks: rank 3 float32 tensor with shape\n      [num_instances, height, width] representing instance masks.\n\n  Returns:\n    flipped masks: rank 3 float32 tensor with shape\n      [num_instances, height, width] representing instance masks.\n  """"""\n  return masks[:, :, ::-1]\n\n\ndef _flip_masks_up_down(masks):\n  """"""Up-down flip masks.\n\n  Args:\n    masks: rank 3 float32 tensor with shape\n      [num_instances, height, width] representing instance masks.\n\n  Returns:\n    flipped masks: rank 3 float32 tensor with shape\n      [num_instances, height, width] representing instance masks.\n  """"""\n  return masks[:, ::-1, :]\n\n\ndef _rot90_masks(masks):\n  """"""Rotate masks counter-clockwise by 90 degrees.\n\n  Args:\n    masks: rank 3 float32 tensor with shape\n      [num_instances, height, width] representing instance masks.\n\n  Returns:\n    rotated masks: rank 3 float32 tensor with shape\n      [num_instances, height, width] representing instance masks.\n  """"""\n  masks = tf.transpose(masks, [0, 2, 1])\n  return masks[:, ::-1, :]\n\n\ndef random_horizontal_flip(image,\n                           boxes=None,\n                           masks=None,\n                           keypoints=None,\n                           keypoint_flip_permutation=None,\n                           seed=None,\n                           preprocess_vars_cache=None):\n  """"""Randomly flips the image and detections horizontally.\n\n  The probability of flipping the image is 50%.\n\n  Args:\n    image: rank 3 float32 tensor with shape [height, width, channels].\n    boxes: (optional) rank 2 float32 tensor with shape [N, 4]\n           containing the bounding boxes.\n           Boxes are in normalized form meaning their coordinates vary\n           between [0, 1].\n           Each row is in the form of [ymin, xmin, ymax, xmax].\n    masks: (optional) rank 3 float32 tensor with shape\n           [num_instances, height, width] containing instance masks. The masks\n           are of the same height, width as the input `image`.\n    keypoints: (optional) rank 3 float32 tensor with shape\n               [num_instances, num_keypoints, 2]. The keypoints are in y-x\n               normalized coordinates.\n    keypoint_flip_permutation: rank 1 int32 tensor containing the keypoint flip\n                               permutation.\n    seed: random seed\n    preprocess_vars_cache: PreprocessorCache object that records previously\n                           performed augmentations. Updated in-place. If this\n                           function is called multiple times with the same\n                           non-null cache, it will perform deterministically.\n\n  Returns:\n    image: image which is the same shape as input image.\n\n    If boxes, masks, keypoints, and keypoint_flip_permutation are not None,\n    the function also returns the following tensors.\n\n    boxes: rank 2 float32 tensor containing the bounding boxes -> [N, 4].\n           Boxes are in normalized form meaning their coordinates vary\n           between [0, 1].\n    masks: rank 3 float32 tensor with shape [num_instances, height, width]\n           containing instance masks.\n    keypoints: rank 3 float32 tensor with shape\n               [num_instances, num_keypoints, 2]\n\n  Raises:\n    ValueError: if keypoints are provided but keypoint_flip_permutation is not.\n  """"""\n\n  def _flip_image(image):\n    # flip image\n    image_flipped = tf.image.flip_left_right(image)\n    return image_flipped\n\n  if keypoints is not None and keypoint_flip_permutation is None:\n    raise ValueError(\n        \'keypoints are provided but keypoints_flip_permutation is not provided\')\n\n  with tf.name_scope(\'RandomHorizontalFlip\', values=[image, boxes]):\n    result = []\n    # random variable defining whether to do flip or not\n    generator_func = functools.partial(tf.random_uniform, [], seed=seed)\n    do_a_flip_random = _get_or_create_preprocess_rand_vars(\n        generator_func,\n        preprocessor_cache.PreprocessorCache.HORIZONTAL_FLIP,\n        preprocess_vars_cache)\n    do_a_flip_random = tf.greater(do_a_flip_random, 0.5)\n\n    # flip image\n    image = tf.cond(do_a_flip_random, lambda: _flip_image(image), lambda: image)\n    result.append(image)\n\n    # flip boxes\n    if boxes is not None:\n      boxes = tf.cond(do_a_flip_random, lambda: _flip_boxes_left_right(boxes),\n                      lambda: boxes)\n      result.append(boxes)\n\n    # flip masks\n    if masks is not None:\n      masks = tf.cond(do_a_flip_random, lambda: _flip_masks_left_right(masks),\n                      lambda: masks)\n      result.append(masks)\n\n    # flip keypoints\n    if keypoints is not None and keypoint_flip_permutation is not None:\n      permutation = keypoint_flip_permutation\n      keypoints = tf.cond(\n          do_a_flip_random,\n          lambda: keypoint_ops.flip_horizontal(keypoints, 0.5, permutation),\n          lambda: keypoints)\n      result.append(keypoints)\n\n    return tuple(result)\n\n\ndef random_vertical_flip(image,\n                         boxes=None,\n                         masks=None,\n                         keypoints=None,\n                         keypoint_flip_permutation=None,\n                         seed=None,\n                         preprocess_vars_cache=None):\n  """"""Randomly flips the image and detections vertically.\n\n  The probability of flipping the image is 50%.\n\n  Args:\n    image: rank 3 float32 tensor with shape [height, width, channels].\n    boxes: (optional) rank 2 float32 tensor with shape [N, 4]\n           containing the bounding boxes.\n           Boxes are in normalized form meaning their coordinates vary\n           between [0, 1].\n           Each row is in the form of [ymin, xmin, ymax, xmax].\n    masks: (optional) rank 3 float32 tensor with shape\n           [num_instances, height, width] containing instance masks. The masks\n           are of the same height, width as the input `image`.\n    keypoints: (optional) rank 3 float32 tensor with shape\n               [num_instances, num_keypoints, 2]. The keypoints are in y-x\n               normalized coordinates.\n    keypoint_flip_permutation: rank 1 int32 tensor containing the keypoint flip\n                               permutation.\n    seed: random seed\n    preprocess_vars_cache: PreprocessorCache object that records previously\n                           performed augmentations. Updated in-place. If this\n                           function is called multiple times with the same\n                           non-null cache, it will perform deterministically.\n\n  Returns:\n    image: image which is the same shape as input image.\n\n    If boxes, masks, keypoints, and keypoint_flip_permutation are not None,\n    the function also returns the following tensors.\n\n    boxes: rank 2 float32 tensor containing the bounding boxes -> [N, 4].\n           Boxes are in normalized form meaning their coordinates vary\n           between [0, 1].\n    masks: rank 3 float32 tensor with shape [num_instances, height, width]\n           containing instance masks.\n    keypoints: rank 3 float32 tensor with shape\n               [num_instances, num_keypoints, 2]\n\n  Raises:\n    ValueError: if keypoints are provided but keypoint_flip_permutation is not.\n  """"""\n\n  def _flip_image(image):\n    # flip image\n    image_flipped = tf.image.flip_up_down(image)\n    return image_flipped\n\n  if keypoints is not None and keypoint_flip_permutation is None:\n    raise ValueError(\n        \'keypoints are provided but keypoints_flip_permutation is not provided\')\n\n  with tf.name_scope(\'RandomVerticalFlip\', values=[image, boxes]):\n    result = []\n    # random variable defining whether to do flip or not\n    generator_func = functools.partial(tf.random_uniform, [], seed=seed)\n    do_a_flip_random = _get_or_create_preprocess_rand_vars(\n        generator_func, preprocessor_cache.PreprocessorCache.VERTICAL_FLIP,\n        preprocess_vars_cache)\n    do_a_flip_random = tf.greater(do_a_flip_random, 0.5)\n\n    # flip image\n    image = tf.cond(do_a_flip_random, lambda: _flip_image(image), lambda: image)\n    result.append(image)\n\n    # flip boxes\n    if boxes is not None:\n      boxes = tf.cond(do_a_flip_random, lambda: _flip_boxes_up_down(boxes),\n                      lambda: boxes)\n      result.append(boxes)\n\n    # flip masks\n    if masks is not None:\n      masks = tf.cond(do_a_flip_random, lambda: _flip_masks_up_down(masks),\n                      lambda: masks)\n      result.append(masks)\n\n    # flip keypoints\n    if keypoints is not None and keypoint_flip_permutation is not None:\n      permutation = keypoint_flip_permutation\n      keypoints = tf.cond(\n          do_a_flip_random,\n          lambda: keypoint_ops.flip_vertical(keypoints, 0.5, permutation),\n          lambda: keypoints)\n      result.append(keypoints)\n\n    return tuple(result)\n\n\ndef random_rotation90(image,\n                      boxes=None,\n                      masks=None,\n                      keypoints=None,\n                      seed=None,\n                      preprocess_vars_cache=None):\n  """"""Randomly rotates the image and detections 90 degrees counter-clockwise.\n\n  The probability of rotating the image is 50%. This can be combined with\n  random_horizontal_flip and random_vertical_flip to produce an output with a\n  uniform distribution of the eight possible 90 degree rotation / reflection\n  combinations.\n\n  Args:\n    image: rank 3 float32 tensor with shape [height, width, channels].\n    boxes: (optional) rank 2 float32 tensor with shape [N, 4]\n           containing the bounding boxes.\n           Boxes are in normalized form meaning their coordinates vary\n           between [0, 1].\n           Each row is in the form of [ymin, xmin, ymax, xmax].\n    masks: (optional) rank 3 float32 tensor with shape\n           [num_instances, height, width] containing instance masks. The masks\n           are of the same height, width as the input `image`.\n    keypoints: (optional) rank 3 float32 tensor with shape\n               [num_instances, num_keypoints, 2]. The keypoints are in y-x\n               normalized coordinates.\n    seed: random seed\n    preprocess_vars_cache: PreprocessorCache object that records previously\n                           performed augmentations. Updated in-place. If this\n                           function is called multiple times with the same\n                           non-null cache, it will perform deterministically.\n\n  Returns:\n    image: image which is the same shape as input image.\n\n    If boxes, masks, and keypoints, are not None,\n    the function also returns the following tensors.\n\n    boxes: rank 2 float32 tensor containing the bounding boxes -> [N, 4].\n           Boxes are in normalized form meaning their coordinates vary\n           between [0, 1].\n    masks: rank 3 float32 tensor with shape [num_instances, height, width]\n           containing instance masks.\n    keypoints: rank 3 float32 tensor with shape\n               [num_instances, num_keypoints, 2]\n  """"""\n\n  def _rot90_image(image):\n    # flip image\n    image_rotated = tf.image.rot90(image)\n    return image_rotated\n\n  with tf.name_scope(\'RandomRotation90\', values=[image, boxes]):\n    result = []\n\n    # random variable defining whether to rotate by 90 degrees or not\n    generator_func = functools.partial(tf.random_uniform, [], seed=seed)\n    do_a_rot90_random = _get_or_create_preprocess_rand_vars(\n        generator_func, preprocessor_cache.PreprocessorCache.ROTATION90,\n        preprocess_vars_cache)\n    do_a_rot90_random = tf.greater(do_a_rot90_random, 0.5)\n\n    # flip image\n    image = tf.cond(do_a_rot90_random, lambda: _rot90_image(image),\n                    lambda: image)\n    result.append(image)\n\n    # flip boxes\n    if boxes is not None:\n      boxes = tf.cond(do_a_rot90_random, lambda: _rot90_boxes(boxes),\n                      lambda: boxes)\n      result.append(boxes)\n\n    # flip masks\n    if masks is not None:\n      masks = tf.cond(do_a_rot90_random, lambda: _rot90_masks(masks),\n                      lambda: masks)\n      result.append(masks)\n\n    # flip keypoints\n    if keypoints is not None:\n      keypoints = tf.cond(\n          do_a_rot90_random,\n          lambda: keypoint_ops.rot90(keypoints),\n          lambda: keypoints)\n      result.append(keypoints)\n\n    return tuple(result)\n\n\ndef random_pixel_value_scale(image,\n                             minval=0.9,\n                             maxval=1.1,\n                             seed=None,\n                             preprocess_vars_cache=None):\n  """"""Scales each value in the pixels of the image.\n\n     This function scales each pixel independent of the other ones.\n     For each value in image tensor, draws a random number between\n     minval and maxval and multiples the values with them.\n\n  Args:\n    image: rank 3 float32 tensor contains 1 image -> [height, width, channels]\n           with pixel values varying between [0, 1].\n    minval: lower ratio of scaling pixel values.\n    maxval: upper ratio of scaling pixel values.\n    seed: random seed.\n    preprocess_vars_cache: PreprocessorCache object that records previously\n                           performed augmentations. Updated in-place. If this\n                           function is called multiple times with the same\n                           non-null cache, it will perform deterministically.\n\n  Returns:\n    image: image which is the same shape as input image.\n  """"""\n  with tf.name_scope(\'RandomPixelValueScale\', values=[image]):\n    generator_func = functools.partial(\n        tf.random_uniform, tf.shape(image),\n        minval=minval, maxval=maxval,\n        dtype=tf.float32, seed=seed)\n    color_coef = _get_or_create_preprocess_rand_vars(\n        generator_func,\n        preprocessor_cache.PreprocessorCache.PIXEL_VALUE_SCALE,\n        preprocess_vars_cache)\n\n    image = tf.multiply(image, color_coef)\n    image = tf.clip_by_value(image, 0.0, 1.0)\n\n  return image\n\n\ndef random_image_scale(image,\n                       masks=None,\n                       min_scale_ratio=0.5,\n                       max_scale_ratio=2.0,\n                       seed=None,\n                       preprocess_vars_cache=None):\n  """"""Scales the image size.\n\n  Args:\n    image: rank 3 float32 tensor contains 1 image -> [height, width, channels].\n    masks: (optional) rank 3 float32 tensor containing masks with\n      size [height, width, num_masks]. The value is set to None if there are no\n      masks.\n    min_scale_ratio: minimum scaling ratio.\n    max_scale_ratio: maximum scaling ratio.\n    seed: random seed.\n    preprocess_vars_cache: PreprocessorCache object that records previously\n                           performed augmentations. Updated in-place. If this\n                           function is called multiple times with the same\n                           non-null cache, it will perform deterministically.\n\n  Returns:\n    image: image which is the same rank as input image.\n    masks: If masks is not none, resized masks which are the same rank as input\n      masks will be returned.\n  """"""\n  with tf.name_scope(\'RandomImageScale\', values=[image]):\n    result = []\n    image_shape = tf.shape(image)\n    image_height = image_shape[0]\n    image_width = image_shape[1]\n    generator_func = functools.partial(\n        tf.random_uniform, [],\n        minval=min_scale_ratio, maxval=max_scale_ratio,\n        dtype=tf.float32, seed=seed)\n    size_coef = _get_or_create_preprocess_rand_vars(\n        generator_func, preprocessor_cache.PreprocessorCache.IMAGE_SCALE,\n        preprocess_vars_cache)\n\n    image_newysize = tf.to_int32(\n        tf.multiply(tf.to_float(image_height), size_coef))\n    image_newxsize = tf.to_int32(\n        tf.multiply(tf.to_float(image_width), size_coef))\n    image = tf.image.resize_images(\n        image, [image_newysize, image_newxsize], align_corners=True)\n    result.append(image)\n    if masks:\n      masks = tf.image.resize_nearest_neighbor(\n          masks, [image_newysize, image_newxsize], align_corners=True)\n      result.append(masks)\n    return tuple(result)\n\n\ndef random_rgb_to_gray(image,\n                       probability=0.1,\n                       seed=None,\n                       preprocess_vars_cache=None):\n  """"""Changes the image from RGB to Grayscale with the given probability.\n\n  Args:\n    image: rank 3 float32 tensor contains 1 image -> [height, width, channels]\n           with pixel values varying between [0, 1].\n    probability: the probability of returning a grayscale image.\n            The probability should be a number between [0, 1].\n    seed: random seed.\n    preprocess_vars_cache: PreprocessorCache object that records previously\n                           performed augmentations. Updated in-place. If this\n                           function is called multiple times with the same\n                           non-null cache, it will perform deterministically.\n\n  Returns:\n    image: image which is the same shape as input image.\n  """"""\n  def _image_to_gray(image):\n    image_gray1 = _rgb_to_grayscale(image)\n    image_gray3 = tf.image.grayscale_to_rgb(image_gray1)\n    return image_gray3\n\n  with tf.name_scope(\'RandomRGBtoGray\', values=[image]):\n    # random variable defining whether to change to grayscale or not\n    generator_func = functools.partial(tf.random_uniform, [], seed=seed)\n    do_gray_random = _get_or_create_preprocess_rand_vars(\n        generator_func, preprocessor_cache.PreprocessorCache.RGB_TO_GRAY,\n        preprocess_vars_cache)\n\n    image = tf.cond(\n        tf.greater(do_gray_random, probability), lambda: image,\n        lambda: _image_to_gray(image))\n\n  return image\n\n\ndef random_adjust_brightness(image,\n                             max_delta=0.2,\n                             seed=None,\n                             preprocess_vars_cache=None):\n  """"""Randomly adjusts brightness.\n\n  Makes sure the output image is still between 0 and 1.\n\n  Args:\n    image: rank 3 float32 tensor contains 1 image -> [height, width, channels]\n           with pixel values varying between [0, 1].\n    max_delta: how much to change the brightness. A value between [0, 1).\n    seed: random seed.\n    preprocess_vars_cache: PreprocessorCache object that records previously\n                           performed augmentations. Updated in-place. If this\n                           function is called multiple times with the same\n                           non-null cache, it will perform deterministically.\n\n  Returns:\n    image: image which is the same shape as input image.\n    boxes: boxes which is the same shape as input boxes.\n  """"""\n  with tf.name_scope(\'RandomAdjustBrightness\', values=[image]):\n    generator_func = functools.partial(tf.random_uniform, [],\n                                       -max_delta, max_delta, seed=seed)\n    delta = _get_or_create_preprocess_rand_vars(\n        generator_func,\n        preprocessor_cache.PreprocessorCache.ADJUST_BRIGHTNESS,\n        preprocess_vars_cache)\n\n    image = tf.image.adjust_brightness(image, delta)\n    image = tf.clip_by_value(image, clip_value_min=0.0, clip_value_max=1.0)\n    return image\n\n\ndef random_adjust_contrast(image,\n                           min_delta=0.8,\n                           max_delta=1.25,\n                           seed=None,\n                           preprocess_vars_cache=None):\n  """"""Randomly adjusts contrast.\n\n  Makes sure the output image is still between 0 and 1.\n\n  Args:\n    image: rank 3 float32 tensor contains 1 image -> [height, width, channels]\n           with pixel values varying between [0, 1].\n    min_delta: see max_delta.\n    max_delta: how much to change the contrast. Contrast will change with a\n               value between min_delta and max_delta. This value will be\n               multiplied to the current contrast of the image.\n    seed: random seed.\n    preprocess_vars_cache: PreprocessorCache object that records previously\n                           performed augmentations. Updated in-place. If this\n                           function is called multiple times with the same\n                           non-null cache, it will perform deterministically.\n\n  Returns:\n    image: image which is the same shape as input image.\n  """"""\n  with tf.name_scope(\'RandomAdjustContrast\', values=[image]):\n    generator_func = functools.partial(tf.random_uniform, [],\n                                       min_delta, max_delta, seed=seed)\n    contrast_factor = _get_or_create_preprocess_rand_vars(\n        generator_func,\n        preprocessor_cache.PreprocessorCache.ADJUST_CONTRAST,\n        preprocess_vars_cache)\n    image = tf.image.adjust_contrast(image, contrast_factor)\n    image = tf.clip_by_value(image, clip_value_min=0.0, clip_value_max=1.0)\n    return image\n\n\ndef random_adjust_hue(image,\n                      max_delta=0.02,\n                      seed=None,\n                      preprocess_vars_cache=None):\n  """"""Randomly adjusts hue.\n\n  Makes sure the output image is still between 0 and 1.\n\n  Args:\n    image: rank 3 float32 tensor contains 1 image -> [height, width, channels]\n           with pixel values varying between [0, 1].\n    max_delta: change hue randomly with a value between 0 and max_delta.\n    seed: random seed.\n    preprocess_vars_cache: PreprocessorCache object that records previously\n                           performed augmentations. Updated in-place. If this\n                           function is called multiple times with the same\n                           non-null cache, it will perform deterministically.\n\n  Returns:\n    image: image which is the same shape as input image.\n  """"""\n  with tf.name_scope(\'RandomAdjustHue\', values=[image]):\n    generator_func = functools.partial(tf.random_uniform, [],\n                                       -max_delta, max_delta, seed=seed)\n    delta = _get_or_create_preprocess_rand_vars(\n        generator_func, preprocessor_cache.PreprocessorCache.ADJUST_HUE,\n        preprocess_vars_cache)\n    image = tf.image.adjust_hue(image, delta)\n    image = tf.clip_by_value(image, clip_value_min=0.0, clip_value_max=1.0)\n    return image\n\n\ndef random_adjust_saturation(image,\n                             min_delta=0.8,\n                             max_delta=1.25,\n                             seed=None,\n                             preprocess_vars_cache=None):\n  """"""Randomly adjusts saturation.\n\n  Makes sure the output image is still between 0 and 1.\n\n  Args:\n    image: rank 3 float32 tensor contains 1 image -> [height, width, channels]\n           with pixel values varying between [0, 1].\n    min_delta: see max_delta.\n    max_delta: how much to change the saturation. Saturation will change with a\n               value between min_delta and max_delta. This value will be\n               multiplied to the current saturation of the image.\n    seed: random seed.\n    preprocess_vars_cache: PreprocessorCache object that records previously\n                           performed augmentations. Updated in-place. If this\n                           function is called multiple times with the same\n                           non-null cache, it will perform deterministically.\n\n  Returns:\n    image: image which is the same shape as input image.\n  """"""\n  with tf.name_scope(\'RandomAdjustSaturation\', values=[image]):\n    generator_func = functools.partial(tf.random_uniform, [],\n                                       min_delta, max_delta, seed=seed)\n    saturation_factor = _get_or_create_preprocess_rand_vars(\n        generator_func,\n        preprocessor_cache.PreprocessorCache.ADJUST_SATURATION,\n        preprocess_vars_cache)\n    image = tf.image.adjust_saturation(image, saturation_factor)\n    image = tf.clip_by_value(image, clip_value_min=0.0, clip_value_max=1.0)\n    return image\n\n\ndef random_distort_color(image, color_ordering=0, preprocess_vars_cache=None):\n  """"""Randomly distorts color.\n\n  Randomly distorts color using a combination of brightness, hue, contrast\n  and saturation changes. Makes sure the output image is still between 0 and 1.\n\n  Args:\n    image: rank 3 float32 tensor contains 1 image -> [height, width, channels]\n           with pixel values varying between [0, 1].\n    color_ordering: Python int, a type of distortion (valid values: 0, 1).\n    preprocess_vars_cache: PreprocessorCache object that records previously\n                           performed augmentations. Updated in-place. If this\n                           function is called multiple times with the same\n                           non-null cache, it will perform deterministically.\n\n  Returns:\n    image: image which is the same shape as input image.\n\n  Raises:\n    ValueError: if color_ordering is not in {0, 1}.\n  """"""\n  with tf.name_scope(\'RandomDistortColor\', values=[image]):\n    if color_ordering == 0:\n      image = random_adjust_brightness(\n          image, max_delta=32. / 255.,\n          preprocess_vars_cache=preprocess_vars_cache)\n      image = random_adjust_saturation(\n          image, min_delta=0.5, max_delta=1.5,\n          preprocess_vars_cache=preprocess_vars_cache)\n      image = random_adjust_hue(\n          image, max_delta=0.2,\n          preprocess_vars_cache=preprocess_vars_cache)\n      image = random_adjust_contrast(\n          image, min_delta=0.5, max_delta=1.5,\n          preprocess_vars_cache=preprocess_vars_cache)\n\n    elif color_ordering == 1:\n      image = random_adjust_brightness(\n          image, max_delta=32. / 255.,\n          preprocess_vars_cache=preprocess_vars_cache)\n      image = random_adjust_contrast(\n          image, min_delta=0.5, max_delta=1.5,\n          preprocess_vars_cache=preprocess_vars_cache)\n      image = random_adjust_saturation(\n          image, min_delta=0.5, max_delta=1.5,\n          preprocess_vars_cache=preprocess_vars_cache)\n      image = random_adjust_hue(\n          image, max_delta=0.2,\n          preprocess_vars_cache=preprocess_vars_cache)\n    else:\n      raise ValueError(\'color_ordering must be in {0, 1}\')\n    return image\n\n\ndef random_jitter_boxes(boxes, ratio=0.05, seed=None):\n  """"""Randomly jitter boxes in image.\n\n  Args:\n    boxes: rank 2 float32 tensor containing the bounding boxes -> [N, 4].\n           Boxes are in normalized form meaning their coordinates vary\n           between [0, 1].\n           Each row is in the form of [ymin, xmin, ymax, xmax].\n    ratio: The ratio of the box width and height that the corners can jitter.\n           For example if the width is 100 pixels and ratio is 0.05,\n           the corners can jitter up to 5 pixels in the x direction.\n    seed: random seed.\n\n  Returns:\n    boxes: boxes which is the same shape as input boxes.\n  """"""\n  def random_jitter_box(box, ratio, seed):\n    """"""Randomly jitter box.\n\n    Args:\n      box: bounding box [1, 1, 4].\n      ratio: max ratio between jittered box and original box,\n      a number between [0, 0.5].\n      seed: random seed.\n\n    Returns:\n      jittered_box: jittered box.\n    """"""\n    rand_numbers = tf.random_uniform(\n        [1, 1, 4], minval=-ratio, maxval=ratio, dtype=tf.float32, seed=seed)\n    box_width = tf.subtract(box[0, 0, 3], box[0, 0, 1])\n    box_height = tf.subtract(box[0, 0, 2], box[0, 0, 0])\n    hw_coefs = tf.stack([box_height, box_width, box_height, box_width])\n    hw_rand_coefs = tf.multiply(hw_coefs, rand_numbers)\n    jittered_box = tf.add(box, hw_rand_coefs)\n    jittered_box = tf.clip_by_value(jittered_box, 0.0, 1.0)\n    return jittered_box\n\n  with tf.name_scope(\'RandomJitterBoxes\', values=[boxes]):\n    # boxes are [N, 4]. Lets first make them [N, 1, 1, 4]\n    boxes_shape = tf.shape(boxes)\n    boxes = tf.expand_dims(boxes, 1)\n    boxes = tf.expand_dims(boxes, 2)\n\n    distorted_boxes = tf.map_fn(\n        lambda x: random_jitter_box(x, ratio, seed), boxes, dtype=tf.float32)\n\n    distorted_boxes = tf.reshape(distorted_boxes, boxes_shape)\n\n    return distorted_boxes\n\n\ndef _strict_random_crop_image(image,\n                              boxes,\n                              labels,\n                              label_scores=None,\n                              masks=None,\n                              keypoints=None,\n                              min_object_covered=1.0,\n                              aspect_ratio_range=(0.75, 1.33),\n                              area_range=(0.1, 1.0),\n                              overlap_thresh=0.3,\n                              preprocess_vars_cache=None):\n  """"""Performs random crop.\n\n  Note: boxes will be clipped to the crop. Keypoint coordinates that are\n  outside the crop will be set to NaN, which is consistent with the original\n  keypoint encoding for non-existing keypoints. This function always crops\n  the image and is supposed to be used by `random_crop_image` function which\n  sometimes returns image unchanged.\n\n  Args:\n    image: rank 3 float32 tensor containing 1 image -> [height, width, channels]\n           with pixel values varying between [0, 1].\n    boxes: rank 2 float32 tensor containing the bounding boxes with shape\n           [num_instances, 4].\n           Boxes are in normalized form meaning their coordinates vary\n           between [0, 1].\n           Each row is in the form of [ymin, xmin, ymax, xmax].\n    labels: rank 1 int32 tensor containing the object classes.\n    label_scores: (optional) float32 tensor of shape [num_instances]\n      representing the score for each box.\n    masks: (optional) rank 3 float32 tensor with shape\n           [num_instances, height, width] containing instance masks. The masks\n           are of the same height, width as the input `image`.\n    keypoints: (optional) rank 3 float32 tensor with shape\n               [num_instances, num_keypoints, 2]. The keypoints are in y-x\n               normalized coordinates.\n    min_object_covered: the cropped image must cover at least this fraction of\n                        at least one of the input bounding boxes.\n    aspect_ratio_range: allowed range for aspect ratio of cropped image.\n    area_range: allowed range for area ratio between cropped image and the\n                original image.\n    overlap_thresh: minimum overlap thresh with new cropped\n                    image to keep the box.\n    preprocess_vars_cache: PreprocessorCache object that records previously\n                           performed augmentations. Updated in-place. If this\n                           function is called multiple times with the same\n                           non-null cache, it will perform deterministically.\n\n  Returns:\n    image: image which is the same rank as input image.\n    boxes: boxes which is the same rank as input boxes.\n           Boxes are in normalized form.\n    labels: new labels.\n\n    If label_scores, masks, or keypoints is not None, the function also returns:\n    label_scores: rank 1 float32 tensor with shape [num_instances].\n    masks: rank 3 float32 tensor with shape [num_instances, height, width]\n           containing instance masks.\n    keypoints: rank 3 float32 tensor with shape\n               [num_instances, num_keypoints, 2]\n  """"""\n  with tf.name_scope(\'RandomCropImage\', values=[image, boxes]):\n    image_shape = tf.shape(image)\n\n    # boxes are [N, 4]. Lets first make them [N, 1, 4].\n    boxes_expanded = tf.expand_dims(\n        tf.clip_by_value(\n            boxes, clip_value_min=0.0, clip_value_max=1.0), 1)\n\n    generator_func = functools.partial(\n        tf.image.sample_distorted_bounding_box,\n        image_shape,\n        bounding_boxes=boxes_expanded,\n        min_object_covered=min_object_covered,\n        aspect_ratio_range=aspect_ratio_range,\n        area_range=area_range,\n        max_attempts=100,\n        use_image_if_no_bounding_boxes=True)\n\n    # for ssd cropping, each value of min_object_covered has its own\n    # cached random variable\n    sample_distorted_bounding_box = _get_or_create_preprocess_rand_vars(\n        generator_func,\n        preprocessor_cache.PreprocessorCache.STRICT_CROP_IMAGE,\n        preprocess_vars_cache, key=min_object_covered)\n\n    im_box_begin, im_box_size, im_box = sample_distorted_bounding_box\n\n    new_image = tf.slice(image, im_box_begin, im_box_size)\n    new_image.set_shape([None, None, image.get_shape()[2]])\n\n    # [1, 4]\n    im_box_rank2 = tf.squeeze(im_box, squeeze_dims=[0])\n    # [4]\n    im_box_rank1 = tf.squeeze(im_box)\n\n    boxlist = box_list.BoxList(boxes)\n    boxlist.add_field(\'labels\', labels)\n\n    if label_scores is not None:\n      boxlist.add_field(\'label_scores\', label_scores)\n\n    im_boxlist = box_list.BoxList(im_box_rank2)\n\n    # remove boxes that are outside cropped image\n    boxlist, inside_window_ids = box_list_ops.prune_completely_outside_window(\n        boxlist, im_box_rank1)\n\n    # remove boxes that are outside image\n    overlapping_boxlist, keep_ids = box_list_ops.prune_non_overlapping_boxes(\n        boxlist, im_boxlist, overlap_thresh)\n\n    # change the coordinate of the remaining boxes\n    new_labels = overlapping_boxlist.get_field(\'labels\')\n    new_boxlist = box_list_ops.change_coordinate_frame(overlapping_boxlist,\n                                                       im_box_rank1)\n    new_boxes = new_boxlist.get()\n    new_boxes = tf.clip_by_value(\n        new_boxes, clip_value_min=0.0, clip_value_max=1.0)\n\n    result = [new_image, new_boxes, new_labels]\n\n    if label_scores is not None:\n      new_label_scores = overlapping_boxlist.get_field(\'label_scores\')\n      result.append(new_label_scores)\n\n    if masks is not None:\n      masks_of_boxes_inside_window = tf.gather(masks, inside_window_ids)\n      masks_of_boxes_completely_inside_window = tf.gather(\n          masks_of_boxes_inside_window, keep_ids)\n      masks_box_begin = [0, im_box_begin[0], im_box_begin[1]]\n      masks_box_size = [-1, im_box_size[0], im_box_size[1]]\n      new_masks = tf.slice(\n          masks_of_boxes_completely_inside_window,\n          masks_box_begin, masks_box_size)\n      result.append(new_masks)\n\n    if keypoints is not None:\n      keypoints_of_boxes_inside_window = tf.gather(keypoints, inside_window_ids)\n      keypoints_of_boxes_completely_inside_window = tf.gather(\n          keypoints_of_boxes_inside_window, keep_ids)\n      new_keypoints = keypoint_ops.change_coordinate_frame(\n          keypoints_of_boxes_completely_inside_window, im_box_rank1)\n      new_keypoints = keypoint_ops.prune_outside_window(new_keypoints,\n                                                        [0.0, 0.0, 1.0, 1.0])\n      result.append(new_keypoints)\n\n    return tuple(result)\n\n\ndef random_crop_image(image,\n                      boxes,\n                      labels,\n                      label_scores=None,\n                      masks=None,\n                      keypoints=None,\n                      min_object_covered=1.0,\n                      aspect_ratio_range=(0.75, 1.33),\n                      area_range=(0.1, 1.0),\n                      overlap_thresh=0.3,\n                      random_coef=0.0,\n                      seed=None,\n                      preprocess_vars_cache=None):\n  """"""Randomly crops the image.\n\n  Given the input image and its bounding boxes, this op randomly\n  crops a subimage.  Given a user-provided set of input constraints,\n  the crop window is resampled until it satisfies these constraints.\n  If within 100 trials it is unable to find a valid crop, the original\n  image is returned. See the Args section for a description of the input\n  constraints. Both input boxes and returned Boxes are in normalized\n  form (e.g., lie in the unit square [0, 1]).\n  This function will return the original image with probability random_coef.\n\n  Note: boxes will be clipped to the crop. Keypoint coordinates that are\n  outside the crop will be set to NaN, which is consistent with the original\n  keypoint encoding for non-existing keypoints.\n\n  Args:\n    image: rank 3 float32 tensor contains 1 image -> [height, width, channels]\n           with pixel values varying between [0, 1].\n    boxes: rank 2 float32 tensor containing the bounding boxes with shape\n           [num_instances, 4].\n           Boxes are in normalized form meaning their coordinates vary\n           between [0, 1].\n           Each row is in the form of [ymin, xmin, ymax, xmax].\n    labels: rank 1 int32 tensor containing the object classes.\n    label_scores: (optional) float32 tensor of shape [num_instances].\n      representing the score for each box.\n    masks: (optional) rank 3 float32 tensor with shape\n           [num_instances, height, width] containing instance masks. The masks\n           are of the same height, width as the input `image`.\n    keypoints: (optional) rank 3 float32 tensor with shape\n               [num_instances, num_keypoints, 2]. The keypoints are in y-x\n               normalized coordinates.\n    min_object_covered: the cropped image must cover at least this fraction of\n                        at least one of the input bounding boxes.\n    aspect_ratio_range: allowed range for aspect ratio of cropped image.\n    area_range: allowed range for area ratio between cropped image and the\n                original image.\n    overlap_thresh: minimum overlap thresh with new cropped\n                    image to keep the box.\n    random_coef: a random coefficient that defines the chance of getting the\n                 original image. If random_coef is 0, we will always get the\n                 cropped image, and if it is 1.0, we will always get the\n                 original image.\n    seed: random seed.\n    preprocess_vars_cache: PreprocessorCache object that records previously\n                           performed augmentations. Updated in-place. If this\n                           function is called multiple times with the same\n                           non-null cache, it will perform deterministically.\n\n  Returns:\n    image: Image shape will be [new_height, new_width, channels].\n    boxes: boxes which is the same rank as input boxes. Boxes are in normalized\n           form.\n    labels: new labels.\n\n    If label_scores, masks, or keypoints are not None, the function also\n    returns:\n    label_scores: new scores.\n    masks: rank 3 float32 tensor with shape [num_instances, height, width]\n           containing instance masks.\n    keypoints: rank 3 float32 tensor with shape\n               [num_instances, num_keypoints, 2]\n  """"""\n\n  def strict_random_crop_image_fn():\n    return _strict_random_crop_image(\n        image,\n        boxes,\n        labels,\n        label_scores=label_scores,\n        masks=masks,\n        keypoints=keypoints,\n        min_object_covered=min_object_covered,\n        aspect_ratio_range=aspect_ratio_range,\n        area_range=area_range,\n        overlap_thresh=overlap_thresh,\n        preprocess_vars_cache=preprocess_vars_cache)\n\n  # avoids tf.cond to make faster RCNN training on borg. See b/140057645.\n  if random_coef < sys.float_info.min:\n    result = strict_random_crop_image_fn()\n  else:\n    generator_func = functools.partial(tf.random_uniform, [], seed=seed)\n    do_a_crop_random = _get_or_create_preprocess_rand_vars(\n        generator_func, preprocessor_cache.PreprocessorCache.CROP_IMAGE,\n        preprocess_vars_cache)\n    do_a_crop_random = tf.greater(do_a_crop_random, random_coef)\n\n    outputs = [image, boxes, labels]\n\n    if label_scores is not None:\n      outputs.append(label_scores)\n    if masks is not None:\n      outputs.append(masks)\n    if keypoints is not None:\n      outputs.append(keypoints)\n\n    result = tf.cond(do_a_crop_random, strict_random_crop_image_fn,\n                     lambda: tuple(outputs))\n  return result\n\n\ndef random_pad_image(image,\n                     boxes,\n                     min_image_size=None,\n                     max_image_size=None,\n                     pad_color=None,\n                     seed=None,\n                     preprocess_vars_cache=None):\n  """"""Randomly pads the image.\n\n  This function randomly pads the image with zeros. The final size of the\n  padded image will be between min_image_size and max_image_size.\n  if min_image_size is smaller than the input image size, min_image_size will\n  be set to the input image size. The same for max_image_size. The input image\n  will be located at a uniformly random location inside the padded image.\n  The relative location of the boxes to the original image will remain the same.\n\n  Args:\n    image: rank 3 float32 tensor containing 1 image -> [height, width, channels]\n           with pixel values varying between [0, 1].\n    boxes: rank 2 float32 tensor containing the bounding boxes -> [N, 4].\n           Boxes are in normalized form meaning their coordinates vary\n           between [0, 1].\n           Each row is in the form of [ymin, xmin, ymax, xmax].\n    min_image_size: a tensor of size [min_height, min_width], type tf.int32.\n                    If passed as None, will be set to image size\n                    [height, width].\n    max_image_size: a tensor of size [max_height, max_width], type tf.int32.\n                    If passed as None, will be set to twice the\n                    image [height * 2, width * 2].\n    pad_color: padding color. A rank 1 tensor of [3] with dtype=tf.float32.\n               if set as None, it will be set to average color of the input\n               image.\n    seed: random seed.\n    preprocess_vars_cache: PreprocessorCache object that records previously\n                           performed augmentations. Updated in-place. If this\n                           function is called multiple times with the same\n                           non-null cache, it will perform deterministically.\n\n  Returns:\n    image: Image shape will be [new_height, new_width, channels].\n    boxes: boxes which is the same rank as input boxes. Boxes are in normalized\n           form.\n  """"""\n  if pad_color is None:\n    pad_color = tf.reduce_mean(image, axis=[0, 1])\n\n  image_shape = tf.shape(image)\n  image_height = image_shape[0]\n  image_width = image_shape[1]\n\n  if max_image_size is None:\n    max_image_size = tf.stack([image_height * 2, image_width * 2])\n  max_image_size = tf.maximum(max_image_size,\n                              tf.stack([image_height, image_width]))\n\n  if min_image_size is None:\n    min_image_size = tf.stack([image_height, image_width])\n  min_image_size = tf.maximum(min_image_size,\n                              tf.stack([image_height, image_width]))\n\n  target_height = tf.cond(\n      max_image_size[0] > min_image_size[0],\n      lambda: _random_integer(min_image_size[0], max_image_size[0], seed),\n      lambda: max_image_size[0])\n\n  target_width = tf.cond(\n      max_image_size[1] > min_image_size[1],\n      lambda: _random_integer(min_image_size[1], max_image_size[1], seed),\n      lambda: max_image_size[1])\n\n  offset_height = tf.cond(\n      target_height > image_height,\n      lambda: _random_integer(0, target_height - image_height, seed),\n      lambda: tf.constant(0, dtype=tf.int32))\n\n  offset_width = tf.cond(\n      target_width > image_width,\n      lambda: _random_integer(0, target_width - image_width, seed),\n      lambda: tf.constant(0, dtype=tf.int32))\n\n  gen_func = lambda: (target_height, target_width, offset_height, offset_width)\n  params = _get_or_create_preprocess_rand_vars(\n      gen_func, preprocessor_cache.PreprocessorCache.PAD_IMAGE,\n      preprocess_vars_cache)\n  target_height, target_width, offset_height, offset_width = params\n\n  new_image = tf.image.pad_to_bounding_box(\n      image,\n      offset_height=offset_height,\n      offset_width=offset_width,\n      target_height=target_height,\n      target_width=target_width)\n\n  # Setting color of the padded pixels\n  image_ones = tf.ones_like(image)\n  image_ones_padded = tf.image.pad_to_bounding_box(\n      image_ones,\n      offset_height=offset_height,\n      offset_width=offset_width,\n      target_height=target_height,\n      target_width=target_width)\n  image_color_padded = (1.0 - image_ones_padded) * pad_color\n  new_image += image_color_padded\n\n  # setting boxes\n  new_window = tf.to_float(\n      tf.stack([\n          -offset_height, -offset_width, target_height - offset_height,\n          target_width - offset_width\n      ]))\n  new_window /= tf.to_float(\n      tf.stack([image_height, image_width, image_height, image_width]))\n  boxlist = box_list.BoxList(boxes)\n  new_boxlist = box_list_ops.change_coordinate_frame(boxlist, new_window)\n  new_boxes = new_boxlist.get()\n\n  return new_image, new_boxes\n\n\ndef random_crop_pad_image(image,\n                          boxes,\n                          labels,\n                          label_scores=None,\n                          min_object_covered=1.0,\n                          aspect_ratio_range=(0.75, 1.33),\n                          area_range=(0.1, 1.0),\n                          overlap_thresh=0.3,\n                          random_coef=0.0,\n                          min_padded_size_ratio=(1.0, 1.0),\n                          max_padded_size_ratio=(2.0, 2.0),\n                          pad_color=None,\n                          seed=None,\n                          preprocess_vars_cache=None):\n  """"""Randomly crops and pads the image.\n\n  Given an input image and its bounding boxes, this op first randomly crops\n  the image and then randomly pads the image with background values. Parameters\n  min_padded_size_ratio and max_padded_size_ratio, determine the range of the\n  final output image size.  Specifically, the final image size will have a size\n  in the range of min_padded_size_ratio * tf.shape(image) and\n  max_padded_size_ratio * tf.shape(image). Note that these ratios are with\n  respect to the size of the original image, so we can\'t capture the same\n  effect easily by independently applying RandomCropImage\n  followed by RandomPadImage.\n\n  Args:\n    image: rank 3 float32 tensor containing 1 image -> [height, width, channels]\n           with pixel values varying between [0, 1].\n    boxes: rank 2 float32 tensor containing the bounding boxes -> [N, 4].\n           Boxes are in normalized form meaning their coordinates vary\n           between [0, 1].\n           Each row is in the form of [ymin, xmin, ymax, xmax].\n    labels: rank 1 int32 tensor containing the object classes.\n    label_scores: rank 1 float32 containing the label scores.\n    min_object_covered: the cropped image must cover at least this fraction of\n                        at least one of the input bounding boxes.\n    aspect_ratio_range: allowed range for aspect ratio of cropped image.\n    area_range: allowed range for area ratio between cropped image and the\n                original image.\n    overlap_thresh: minimum overlap thresh with new cropped\n                    image to keep the box.\n    random_coef: a random coefficient that defines the chance of getting the\n                 original image. If random_coef is 0, we will always get the\n                 cropped image, and if it is 1.0, we will always get the\n                 original image.\n    min_padded_size_ratio: min ratio of padded image height and width to the\n                           input image\'s height and width.\n    max_padded_size_ratio: max ratio of padded image height and width to the\n                           input image\'s height and width.\n    pad_color: padding color. A rank 1 tensor of [3] with dtype=tf.float32.\n               if set as None, it will be set to average color of the randomly\n               cropped image.\n    seed: random seed.\n    preprocess_vars_cache: PreprocessorCache object that records previously\n                           performed augmentations. Updated in-place. If this\n                           function is called multiple times with the same\n                           non-null cache, it will perform deterministically.\n\n  Returns:\n    padded_image: padded image.\n    padded_boxes: boxes which is the same rank as input boxes. Boxes are in\n                  normalized form.\n    cropped_labels: cropped labels.\n    if label_scores is not None also returns:\n    cropped_label_scores: cropped label scores.\n  """"""\n  image_size = tf.shape(image)\n  image_height = image_size[0]\n  image_width = image_size[1]\n  result = random_crop_image(\n      image=image,\n      boxes=boxes,\n      labels=labels,\n      label_scores=label_scores,\n      min_object_covered=min_object_covered,\n      aspect_ratio_range=aspect_ratio_range,\n      area_range=area_range,\n      overlap_thresh=overlap_thresh,\n      random_coef=random_coef,\n      seed=seed,\n      preprocess_vars_cache=preprocess_vars_cache)\n\n  cropped_image, cropped_boxes, cropped_labels = result[:3]\n\n  min_image_size = tf.to_int32(\n      tf.to_float(tf.stack([image_height, image_width])) *\n      min_padded_size_ratio)\n  max_image_size = tf.to_int32(\n      tf.to_float(tf.stack([image_height, image_width])) *\n      max_padded_size_ratio)\n\n  padded_image, padded_boxes = random_pad_image(\n      cropped_image,\n      cropped_boxes,\n      min_image_size=min_image_size,\n      max_image_size=max_image_size,\n      pad_color=pad_color,\n      seed=seed,\n      preprocess_vars_cache=preprocess_vars_cache)\n\n  cropped_padded_output = (padded_image, padded_boxes, cropped_labels)\n\n  if label_scores is not None:\n    cropped_label_scores = result[3]\n    cropped_padded_output += (cropped_label_scores,)\n\n  return cropped_padded_output\n\n\ndef random_crop_to_aspect_ratio(image,\n                                boxes,\n                                labels,\n                                label_scores=None,\n                                masks=None,\n                                keypoints=None,\n                                aspect_ratio=1.0,\n                                overlap_thresh=0.3,\n                                seed=None,\n                                preprocess_vars_cache=None):\n  """"""Randomly crops an image to the specified aspect ratio.\n\n  Randomly crops the a portion of the image such that the crop is of the\n  specified aspect ratio, and the crop is as large as possible. If the specified\n  aspect ratio is larger than the aspect ratio of the image, this op will\n  randomly remove rows from the top and bottom of the image. If the specified\n  aspect ratio is less than the aspect ratio of the image, this op will randomly\n  remove cols from the left and right of the image. If the specified aspect\n  ratio is the same as the aspect ratio of the image, this op will return the\n  image.\n\n  Args:\n    image: rank 3 float32 tensor contains 1 image -> [height, width, channels]\n           with pixel values varying between [0, 1].\n    boxes: rank 2 float32 tensor containing the bounding boxes -> [N, 4].\n           Boxes are in normalized form meaning their coordinates vary\n           between [0, 1].\n           Each row is in the form of [ymin, xmin, ymax, xmax].\n    labels: rank 1 int32 tensor containing the object classes.\n    label_scores: (optional) float32 tensor of shape [num_instances]\n      representing the score for each box.\n    masks: (optional) rank 3 float32 tensor with shape\n           [num_instances, height, width] containing instance masks. The masks\n           are of the same height, width as the input `image`.\n    keypoints: (optional) rank 3 float32 tensor with shape\n               [num_instances, num_keypoints, 2]. The keypoints are in y-x\n               normalized coordinates.\n    aspect_ratio: the aspect ratio of cropped image.\n    overlap_thresh: minimum overlap thresh with new cropped\n                    image to keep the box.\n    seed: random seed.\n    preprocess_vars_cache: PreprocessorCache object that records previously\n                           performed augmentations. Updated in-place. If this\n                           function is called multiple times with the same\n                           non-null cache, it will perform deterministically.\n\n  Returns:\n    image: image which is the same rank as input image.\n    boxes: boxes which is the same rank as input boxes.\n           Boxes are in normalized form.\n    labels: new labels.\n\n    If label_scores, masks, or keypoints is not None, the function also returns:\n    label_scores: new label scores.\n    masks: rank 3 float32 tensor with shape [num_instances, height, width]\n           containing instance masks.\n    keypoints: rank 3 float32 tensor with shape\n               [num_instances, num_keypoints, 2]\n\n  Raises:\n    ValueError: If image is not a 3D tensor.\n  """"""\n  if len(image.get_shape()) != 3:\n    raise ValueError(\'Image should be 3D tensor\')\n\n  with tf.name_scope(\'RandomCropToAspectRatio\', values=[image]):\n    image_shape = tf.shape(image)\n    orig_height = image_shape[0]\n    orig_width = image_shape[1]\n    orig_aspect_ratio = tf.to_float(orig_width) / tf.to_float(orig_height)\n    new_aspect_ratio = tf.constant(aspect_ratio, dtype=tf.float32)\n    def target_height_fn():\n      return tf.to_int32(tf.round(tf.to_float(orig_width) / new_aspect_ratio))\n\n    target_height = tf.cond(orig_aspect_ratio >= new_aspect_ratio,\n                            lambda: orig_height, target_height_fn)\n\n    def target_width_fn():\n      return tf.to_int32(tf.round(tf.to_float(orig_height) * new_aspect_ratio))\n\n    target_width = tf.cond(orig_aspect_ratio <= new_aspect_ratio,\n                           lambda: orig_width, target_width_fn)\n\n    # either offset_height = 0 and offset_width is randomly chosen from\n    # [0, offset_width - target_width), or else offset_width = 0 and\n    # offset_height is randomly chosen from [0, offset_height - target_height)\n    offset_height = _random_integer(0, orig_height - target_height + 1, seed)\n    offset_width = _random_integer(0, orig_width - target_width + 1, seed)\n\n    generator_func = lambda: (offset_height, offset_width)\n    offset_height, offset_width = _get_or_create_preprocess_rand_vars(\n        generator_func,\n        preprocessor_cache.PreprocessorCache.CROP_TO_ASPECT_RATIO,\n        preprocess_vars_cache)\n\n    new_image = tf.image.crop_to_bounding_box(\n        image, offset_height, offset_width, target_height, target_width)\n\n    im_box = tf.stack([\n        tf.to_float(offset_height) / tf.to_float(orig_height),\n        tf.to_float(offset_width) / tf.to_float(orig_width),\n        tf.to_float(offset_height + target_height) / tf.to_float(orig_height),\n        tf.to_float(offset_width + target_width) / tf.to_float(orig_width)\n    ])\n\n    boxlist = box_list.BoxList(boxes)\n    boxlist.add_field(\'labels\', labels)\n\n    if label_scores is not None:\n      boxlist.add_field(\'label_scores\', label_scores)\n\n    im_boxlist = box_list.BoxList(tf.expand_dims(im_box, 0))\n\n    # remove boxes whose overlap with the image is less than overlap_thresh\n    overlapping_boxlist, keep_ids = box_list_ops.prune_non_overlapping_boxes(\n        boxlist, im_boxlist, overlap_thresh)\n\n    # change the coordinate of the remaining boxes\n    new_labels = overlapping_boxlist.get_field(\'labels\')\n    new_boxlist = box_list_ops.change_coordinate_frame(overlapping_boxlist,\n                                                       im_box)\n    new_boxlist = box_list_ops.clip_to_window(new_boxlist,\n                                              tf.constant([0.0, 0.0, 1.0, 1.0],\n                                                          tf.float32))\n    new_boxes = new_boxlist.get()\n\n    result = [new_image, new_boxes, new_labels]\n\n    if label_scores is not None:\n      new_label_scores = overlapping_boxlist.get_field(\'label_scores\')\n      result.append(new_label_scores)\n\n    if masks is not None:\n      masks_inside_window = tf.gather(masks, keep_ids)\n      masks_box_begin = tf.stack([0, offset_height, offset_width])\n      masks_box_size = tf.stack([-1, target_height, target_width])\n      new_masks = tf.slice(masks_inside_window, masks_box_begin, masks_box_size)\n      result.append(new_masks)\n\n    if keypoints is not None:\n      keypoints_inside_window = tf.gather(keypoints, keep_ids)\n      new_keypoints = keypoint_ops.change_coordinate_frame(\n          keypoints_inside_window, im_box)\n      new_keypoints = keypoint_ops.prune_outside_window(new_keypoints,\n                                                        [0.0, 0.0, 1.0, 1.0])\n      result.append(new_keypoints)\n\n    return tuple(result)\n\n\ndef random_pad_to_aspect_ratio(image,\n                               boxes,\n                               masks=None,\n                               keypoints=None,\n                               aspect_ratio=1.0,\n                               min_padded_size_ratio=(1.0, 1.0),\n                               max_padded_size_ratio=(2.0, 2.0),\n                               seed=None,\n                               preprocess_vars_cache=None):\n  """"""Randomly zero pads an image to the specified aspect ratio.\n\n  Pads the image so that the resulting image will have the specified aspect\n  ratio without scaling less than the min_padded_size_ratio or more than the\n  max_padded_size_ratio. If the min_padded_size_ratio or max_padded_size_ratio\n  is lower than what is possible to maintain the aspect ratio, then this method\n  will use the least padding to achieve the specified aspect ratio.\n\n  Args:\n    image: rank 3 float32 tensor contains 1 image -> [height, width, channels]\n           with pixel values varying between [0, 1].\n    boxes: rank 2 float32 tensor containing the bounding boxes -> [N, 4].\n           Boxes are in normalized form meaning their coordinates vary\n           between [0, 1].\n           Each row is in the form of [ymin, xmin, ymax, xmax].\n    masks: (optional) rank 3 float32 tensor with shape\n           [num_instances, height, width] containing instance masks. The masks\n           are of the same height, width as the input `image`.\n    keypoints: (optional) rank 3 float32 tensor with shape\n               [num_instances, num_keypoints, 2]. The keypoints are in y-x\n               normalized coordinates.\n    aspect_ratio: aspect ratio of the final image.\n    min_padded_size_ratio: min ratio of padded image height and width to the\n                           input image\'s height and width.\n    max_padded_size_ratio: max ratio of padded image height and width to the\n                           input image\'s height and width.\n    seed: random seed.\n    preprocess_vars_cache: PreprocessorCache object that records previously\n                           performed augmentations. Updated in-place. If this\n                           function is called multiple times with the same\n                           non-null cache, it will perform deterministically.\n\n  Returns:\n    image: image which is the same rank as input image.\n    boxes: boxes which is the same rank as input boxes.\n           Boxes are in normalized form.\n    labels: new labels.\n\n    If label_scores, masks, or keypoints is not None, the function also returns:\n    label_scores: new label scores.\n    masks: rank 3 float32 tensor with shape [num_instances, height, width]\n           containing instance masks.\n    keypoints: rank 3 float32 tensor with shape\n               [num_instances, num_keypoints, 2]\n\n  Raises:\n    ValueError: If image is not a 3D tensor.\n  """"""\n  if len(image.get_shape()) != 3:\n    raise ValueError(\'Image should be 3D tensor\')\n\n  with tf.name_scope(\'RandomPadToAspectRatio\', values=[image]):\n    image_shape = tf.shape(image)\n    image_height = tf.to_float(image_shape[0])\n    image_width = tf.to_float(image_shape[1])\n    image_aspect_ratio = image_width / image_height\n    new_aspect_ratio = tf.constant(aspect_ratio, dtype=tf.float32)\n    target_height = tf.cond(\n        image_aspect_ratio <= new_aspect_ratio,\n        lambda: image_height,\n        lambda: image_width / new_aspect_ratio)\n    target_width = tf.cond(\n        image_aspect_ratio >= new_aspect_ratio,\n        lambda: image_width,\n        lambda: image_height * new_aspect_ratio)\n\n    min_height = tf.maximum(\n        min_padded_size_ratio[0] * image_height, target_height)\n    min_width = tf.maximum(\n        min_padded_size_ratio[1] * image_width, target_width)\n    max_height = tf.maximum(\n        max_padded_size_ratio[0] * image_height, target_height)\n    max_width = tf.maximum(\n        max_padded_size_ratio[1] * image_width, target_width)\n\n    max_scale = tf.minimum(max_height / target_height, max_width / target_width)\n    min_scale = tf.minimum(\n        max_scale,\n        tf.maximum(min_height / target_height, min_width / target_width))\n\n    generator_func = functools.partial(tf.random_uniform, [],\n                                       min_scale, max_scale, seed=seed)\n    scale = _get_or_create_preprocess_rand_vars(\n        generator_func,\n        preprocessor_cache.PreprocessorCache.PAD_TO_ASPECT_RATIO,\n        preprocess_vars_cache)\n\n    target_height = tf.round(scale * target_height)\n    target_width = tf.round(scale * target_width)\n\n    new_image = tf.image.pad_to_bounding_box(\n        image, 0, 0, tf.to_int32(target_height), tf.to_int32(target_width))\n\n    im_box = tf.stack([\n        0.0,\n        0.0,\n        target_height / image_height,\n        target_width / image_width\n    ])\n    boxlist = box_list.BoxList(boxes)\n    new_boxlist = box_list_ops.change_coordinate_frame(boxlist, im_box)\n    new_boxes = new_boxlist.get()\n\n    result = [new_image, new_boxes]\n\n    if masks is not None:\n      new_masks = tf.expand_dims(masks, -1)\n      new_masks = tf.image.pad_to_bounding_box(new_masks, 0, 0,\n                                               tf.to_int32(target_height),\n                                               tf.to_int32(target_width))\n      new_masks = tf.squeeze(new_masks, [-1])\n      result.append(new_masks)\n\n    if keypoints is not None:\n      new_keypoints = keypoint_ops.change_coordinate_frame(keypoints, im_box)\n      result.append(new_keypoints)\n\n    return tuple(result)\n\n\ndef random_black_patches(image,\n                         max_black_patches=10,\n                         probability=0.5,\n                         size_to_image_ratio=0.1,\n                         random_seed=None,\n                         preprocess_vars_cache=None):\n  """"""Randomly adds some black patches to the image.\n\n  This op adds up to max_black_patches square black patches of a fixed size\n  to the image where size is specified via the size_to_image_ratio parameter.\n\n  Args:\n    image: rank 3 float32 tensor containing 1 image -> [height, width, channels]\n           with pixel values varying between [0, 1].\n    max_black_patches: number of times that the function tries to add a\n                       black box to the image.\n    probability: at each try, what is the chance of adding a box.\n    size_to_image_ratio: Determines the ratio of the size of the black patches\n                         to the size of the image.\n                         box_size = size_to_image_ratio *\n                                    min(image_width, image_height)\n    random_seed: random seed.\n    preprocess_vars_cache: PreprocessorCache object that records previously\n                           performed augmentations. Updated in-place. If this\n                           function is called multiple times with the same\n                           non-null cache, it will perform deterministically.\n\n  Returns:\n    image\n  """"""\n  def add_black_patch_to_image(image, idx):\n    """"""Function for adding one patch to the image.\n\n    Args:\n      image: image\n      idx: counter for number of patches that could have been added\n\n    Returns:\n      image with a randomly added black box\n    """"""\n    image_shape = tf.shape(image)\n    image_height = image_shape[0]\n    image_width = image_shape[1]\n    box_size = tf.to_int32(\n        tf.multiply(\n            tf.minimum(tf.to_float(image_height), tf.to_float(image_width)),\n            size_to_image_ratio))\n\n    generator_func = functools.partial(tf.random_uniform, [], minval=0.0,\n                                       maxval=(1.0 - size_to_image_ratio),\n                                       seed=random_seed)\n    normalized_y_min = _get_or_create_preprocess_rand_vars(\n        generator_func,\n        preprocessor_cache.PreprocessorCache.ADD_BLACK_PATCH,\n        preprocess_vars_cache, key=str(idx) + \'y\')\n    normalized_x_min = _get_or_create_preprocess_rand_vars(\n        generator_func,\n        preprocessor_cache.PreprocessorCache.ADD_BLACK_PATCH,\n        preprocess_vars_cache, key=str(idx) + \'x\')\n\n    y_min = tf.to_int32(normalized_y_min * tf.to_float(image_height))\n    x_min = tf.to_int32(normalized_x_min * tf.to_float(image_width))\n    black_box = tf.ones([box_size, box_size, 3], dtype=tf.float32)\n    mask = 1.0 - tf.image.pad_to_bounding_box(black_box, y_min, x_min,\n                                              image_height, image_width)\n    image = tf.multiply(image, mask)\n    return image\n\n  with tf.name_scope(\'RandomBlackPatchInImage\', values=[image]):\n    for idx in range(max_black_patches):\n      generator_func = functools.partial(tf.random_uniform, [],\n                                         minval=0.0, maxval=1.0,\n                                         dtype=tf.float32, seed=random_seed)\n      random_prob = _get_or_create_preprocess_rand_vars(\n          generator_func,\n          preprocessor_cache.PreprocessorCache.BLACK_PATCHES,\n          preprocess_vars_cache, key=idx)\n      image = tf.cond(\n          tf.greater(random_prob, probability), lambda: image,\n          functools.partial(add_black_patch_to_image, image=image, idx=idx))\n    return image\n\n\ndef image_to_float(image):\n  """"""Used in Faster R-CNN. Casts image pixel values to float.\n\n  Args:\n    image: input image which might be in tf.uint8 or sth else format\n\n  Returns:\n    image: image in tf.float32 format.\n  """"""\n  with tf.name_scope(\'ImageToFloat\', values=[image]):\n    image = tf.to_float(image)\n    return image\n\n\ndef random_resize_method(image, target_size, preprocess_vars_cache=None):\n  """"""Uses a random resize method to resize the image to target size.\n\n  Args:\n    image: a rank 3 tensor.\n    target_size: a list of [target_height, target_width]\n    preprocess_vars_cache: PreprocessorCache object that records previously\n                           performed augmentations. Updated in-place. If this\n                           function is called multiple times with the same\n                           non-null cache, it will perform deterministically.\n\n  Returns:\n    resized image.\n  """"""\n\n  resized_image = _apply_with_random_selector(\n      image,\n      lambda x, method: tf.image.resize_images(x, target_size, method),\n      num_cases=4,\n      preprocess_vars_cache=preprocess_vars_cache,\n      key=preprocessor_cache.PreprocessorCache.RESIZE_METHOD)\n\n  return resized_image\n\n\ndef _compute_new_static_size(image, min_dimension, max_dimension):\n  """"""Compute new static shape for resize_to_range method.""""""\n  image_shape = image.get_shape().as_list()\n  orig_height = image_shape[0]\n  orig_width = image_shape[1]\n  num_channels = image_shape[2]\n  orig_min_dim = min(orig_height, orig_width)\n  # Calculates the larger of the possible sizes\n  large_scale_factor = min_dimension / float(orig_min_dim)\n  # Scaling orig_(height|width) by large_scale_factor will make the smaller\n  # dimension equal to min_dimension, save for floating point rounding errors.\n  # For reasonably-sized images, taking the nearest integer will reliably\n  # eliminate this error.\n  large_height = int(round(orig_height * large_scale_factor))\n  large_width = int(round(orig_width * large_scale_factor))\n  large_size = [large_height, large_width]\n  if max_dimension:\n    # Calculates the smaller of the possible sizes, use that if the larger\n    # is too big.\n    orig_max_dim = max(orig_height, orig_width)\n    small_scale_factor = max_dimension / float(orig_max_dim)\n    # Scaling orig_(height|width) by small_scale_factor will make the larger\n    # dimension equal to max_dimension, save for floating point rounding\n    # errors. For reasonably-sized images, taking the nearest integer will\n    # reliably eliminate this error.\n    small_height = int(round(orig_height * small_scale_factor))\n    small_width = int(round(orig_width * small_scale_factor))\n    small_size = [small_height, small_width]\n    new_size = large_size\n    if max(large_size) > max_dimension:\n      new_size = small_size\n  else:\n    new_size = large_size\n  return tf.constant(new_size + [num_channels])\n\n\ndef _compute_new_dynamic_size(image, min_dimension, max_dimension):\n  """"""Compute new dynamic shape for resize_to_range method.""""""\n  image_shape = tf.shape(image)\n  orig_height = tf.to_float(image_shape[0])\n  orig_width = tf.to_float(image_shape[1])\n  num_channels = image_shape[2]\n  orig_min_dim = tf.minimum(orig_height, orig_width)\n  # Calculates the larger of the possible sizes\n  min_dimension = tf.constant(min_dimension, dtype=tf.float32)\n  large_scale_factor = min_dimension / orig_min_dim\n  # Scaling orig_(height|width) by large_scale_factor will make the smaller\n  # dimension equal to min_dimension, save for floating point rounding errors.\n  # For reasonably-sized images, taking the nearest integer will reliably\n  # eliminate this error.\n  large_height = tf.to_int32(tf.round(orig_height * large_scale_factor))\n  large_width = tf.to_int32(tf.round(orig_width * large_scale_factor))\n  large_size = tf.stack([large_height, large_width])\n  if max_dimension:\n    # Calculates the smaller of the possible sizes, use that if the larger\n    # is too big.\n    orig_max_dim = tf.maximum(orig_height, orig_width)\n    max_dimension = tf.constant(max_dimension, dtype=tf.float32)\n    small_scale_factor = max_dimension / orig_max_dim\n    # Scaling orig_(height|width) by small_scale_factor will make the larger\n    # dimension equal to max_dimension, save for floating point rounding\n    # errors. For reasonably-sized images, taking the nearest integer will\n    # reliably eliminate this error.\n    small_height = tf.to_int32(tf.round(orig_height * small_scale_factor))\n    small_width = tf.to_int32(tf.round(orig_width * small_scale_factor))\n    small_size = tf.stack([small_height, small_width])\n    new_size = tf.cond(\n        tf.to_float(tf.reduce_max(large_size)) > max_dimension,\n        lambda: small_size, lambda: large_size)\n  else:\n    new_size = large_size\n  return tf.stack(tf.unstack(new_size) + [num_channels])\n\n\ndef resize_to_range(image,\n                    masks=None,\n                    min_dimension=None,\n                    max_dimension=None,\n                    method=tf.image.ResizeMethod.BILINEAR,\n                    align_corners=False,\n                    pad_to_max_dimension=False):\n  """"""Resizes an image so its dimensions are within the provided value.\n\n  The output size can be described by two cases:\n  1. If the image can be rescaled so its minimum dimension is equal to the\n     provided value without the other dimension exceeding max_dimension,\n     then do so.\n  2. Otherwise, resize so the largest dimension is equal to max_dimension.\n\n  Args:\n    image: A 3D tensor of shape [height, width, channels]\n    masks: (optional) rank 3 float32 tensor with shape\n           [num_instances, height, width] containing instance masks.\n    min_dimension: (optional) (scalar) desired size of the smaller image\n                   dimension.\n    max_dimension: (optional) (scalar) maximum allowed size\n                   of the larger image dimension.\n    method: (optional) interpolation method used in resizing. Defaults to\n            BILINEAR.\n    align_corners: bool. If true, exactly align all 4 corners of the input\n                   and output. Defaults to False.\n    pad_to_max_dimension: Whether to resize the image and pad it with zeros\n      so the resulting image is of the spatial size\n      [max_dimension, max_dimension]. If masks are included they are padded\n      similarly.\n\n  Returns:\n    Note that the position of the resized_image_shape changes based on whether\n    masks are present.\n    resized_image: A 3D tensor of shape [new_height, new_width, channels],\n      where the image has been resized (with bilinear interpolation) so that\n      min(new_height, new_width) == min_dimension or\n      max(new_height, new_width) == max_dimension.\n    resized_masks: If masks is not None, also outputs masks. A 3D tensor of\n      shape [num_instances, new_height, new_width].\n    resized_image_shape: A 1D tensor of shape [3] containing shape of the\n      resized image.\n\n  Raises:\n    ValueError: if the image is not a 3D tensor.\n  """"""\n  if len(image.get_shape()) != 3:\n    raise ValueError(\'Image should be 3D tensor\')\n\n  with tf.name_scope(\'ResizeToRange\', values=[image, min_dimension]):\n    if image.get_shape().is_fully_defined():\n      new_size = _compute_new_static_size(image, min_dimension, max_dimension)\n    else:\n      new_size = _compute_new_dynamic_size(image, min_dimension, max_dimension)\n    new_image = tf.image.resize_images(\n        image, new_size[:-1], method=method, align_corners=align_corners)\n\n    if pad_to_max_dimension:\n      new_image = tf.image.pad_to_bounding_box(\n          new_image, 0, 0, max_dimension, max_dimension)\n\n    result = [new_image]\n    if masks is not None:\n      new_masks = tf.expand_dims(masks, 3)\n      new_masks = tf.image.resize_images(\n          new_masks,\n          new_size[:-1],\n          method=tf.image.ResizeMethod.NEAREST_NEIGHBOR,\n          align_corners=align_corners)\n      new_masks = tf.squeeze(new_masks, 3)\n      if pad_to_max_dimension:\n        new_masks = tf.image.pad_to_bounding_box(\n            new_masks, 0, 0, max_dimension, max_dimension)\n      result.append(new_masks)\n\n    result.append(new_size)\n    return result\n\n\n# TODO(alirezafathi): Make sure the static shapes are preserved.\ndef resize_to_min_dimension(image, masks=None, min_dimension=600):\n  """"""Resizes image and masks given the min size maintaining the aspect ratio.\n\n  If one of the image dimensions is smaller that min_dimension, it will scale\n  the image such that its smallest dimension is equal to min_dimension.\n  Otherwise, will keep the image size as is.\n\n  Args:\n    image: a tensor of size [height, width, channels].\n    masks: (optional) a tensors of size [num_instances, height, width].\n    min_dimension: minimum image dimension.\n\n  Returns:\n    Note that the position of the resized_image_shape changes based on whether\n    masks are present.\n    resized_image: A tensor of size [new_height, new_width, channels].\n    resized_masks: If masks is not None, also outputs masks. A 3D tensor of\n      shape [num_instances, new_height, new_width]\n    resized_image_shape: A 1D tensor of shape [3] containing the shape of the\n      resized image.\n\n  Raises:\n    ValueError: if the image is not a 3D tensor.\n  """"""\n  if len(image.get_shape()) != 3:\n    raise ValueError(\'Image should be 3D tensor\')\n\n  with tf.name_scope(\'ResizeGivenMinDimension\', values=[image, min_dimension]):\n    image_height = tf.shape(image)[0]\n    image_width = tf.shape(image)[1]\n    num_channels = tf.shape(image)[2]\n    min_image_dimension = tf.minimum(image_height, image_width)\n    min_target_dimension = tf.maximum(min_image_dimension, min_dimension)\n    target_ratio = tf.to_float(min_target_dimension) / tf.to_float(\n        min_image_dimension)\n    target_height = tf.to_int32(tf.to_float(image_height) * target_ratio)\n    target_width = tf.to_int32(tf.to_float(image_width) * target_ratio)\n    image = tf.image.resize_bilinear(\n        tf.expand_dims(image, axis=0),\n        size=[target_height, target_width],\n        align_corners=True)\n    result = [tf.squeeze(image, axis=0)]\n\n    if masks is not None:\n      masks = tf.image.resize_nearest_neighbor(\n          tf.expand_dims(masks, axis=3),\n          size=[target_height, target_width],\n          align_corners=True)\n      result.append(tf.squeeze(masks, axis=3))\n\n    result.append(tf.stack([target_height, target_width, num_channels]))\n    return result\n\n\ndef scale_boxes_to_pixel_coordinates(image, boxes, keypoints=None):\n  """"""Scales boxes from normalized to pixel coordinates.\n\n  Args:\n    image: A 3D float32 tensor of shape [height, width, channels].\n    boxes: A 2D float32 tensor of shape [num_boxes, 4] containing the bounding\n      boxes in normalized coordinates. Each row is of the form\n      [ymin, xmin, ymax, xmax].\n    keypoints: (optional) rank 3 float32 tensor with shape\n      [num_instances, num_keypoints, 2]. The keypoints are in y-x normalized\n      coordinates.\n\n  Returns:\n    image: unchanged input image.\n    scaled_boxes: a 2D float32 tensor of shape [num_boxes, 4] containing the\n      bounding boxes in pixel coordinates.\n    scaled_keypoints: a 3D float32 tensor with shape\n      [num_instances, num_keypoints, 2] containing the keypoints in pixel\n      coordinates.\n  """"""\n  boxlist = box_list.BoxList(boxes)\n  image_height = tf.shape(image)[0]\n  image_width = tf.shape(image)[1]\n  scaled_boxes = box_list_ops.scale(boxlist, image_height, image_width).get()\n  result = [image, scaled_boxes]\n  if keypoints is not None:\n    scaled_keypoints = keypoint_ops.scale(keypoints, image_height, image_width)\n    result.append(scaled_keypoints)\n  return tuple(result)\n\n\n# TODO(alirezafathi): Investigate if instead the function should return None if\n# masks is None.\n# pylint: disable=g-doc-return-or-yield\ndef resize_image(image,\n                 masks=None,\n                 new_height=600,\n                 new_width=1024,\n                 method=tf.image.ResizeMethod.BILINEAR,\n                 align_corners=False):\n  """"""Resizes images to the given height and width.\n\n  Args:\n    image: A 3D tensor of shape [height, width, channels]\n    masks: (optional) rank 3 float32 tensor with shape\n           [num_instances, height, width] containing instance masks.\n    new_height: (optional) (scalar) desired height of the image.\n    new_width: (optional) (scalar) desired width of the image.\n    method: (optional) interpolation method used in resizing. Defaults to\n            BILINEAR.\n    align_corners: bool. If true, exactly align all 4 corners of the input\n                   and output. Defaults to False.\n\n  Returns:\n    Note that the position of the resized_image_shape changes based on whether\n    masks are present.\n    resized_image: A tensor of size [new_height, new_width, channels].\n    resized_masks: If masks is not None, also outputs masks. A 3D tensor of\n      shape [num_instances, new_height, new_width]\n    resized_image_shape: A 1D tensor of shape [3] containing the shape of the\n      resized image.\n  """"""\n  with tf.name_scope(\n      \'ResizeImage\',\n      values=[image, new_height, new_width, method, align_corners]):\n    new_image = tf.image.resize_images(\n        image, tf.stack([new_height, new_width]),\n        method=method,\n        align_corners=align_corners)\n    image_shape = shape_utils.combined_static_and_dynamic_shape(image)\n    result = [new_image]\n    if masks is not None:\n      num_instances = tf.shape(masks)[0]\n      new_size = tf.stack([new_height, new_width])\n      def resize_masks_branch():\n        new_masks = tf.expand_dims(masks, 3)\n        new_masks = tf.image.resize_nearest_neighbor(\n            new_masks, new_size, align_corners=align_corners)\n        new_masks = tf.squeeze(new_masks, axis=3)\n        return new_masks\n\n      def reshape_masks_branch():\n        new_masks = tf.reshape(masks, [0, new_size[0], new_size[1]])\n        return new_masks\n\n      masks = tf.cond(num_instances > 0, resize_masks_branch,\n                      reshape_masks_branch)\n      result.append(masks)\n\n    result.append(tf.stack([new_height, new_width, image_shape[2]]))\n    return result\n\n\ndef subtract_channel_mean(image, means=None):\n  """"""Normalizes an image by subtracting a mean from each channel.\n\n  Args:\n    image: A 3D tensor of shape [height, width, channels]\n    means: float list containing a mean for each channel\n  Returns:\n    normalized_images: a tensor of shape [height, width, channels]\n  Raises:\n    ValueError: if images is not a 4D tensor or if the number of means is not\n      equal to the number of channels.\n  """"""\n  with tf.name_scope(\'SubtractChannelMean\', values=[image, means]):\n    if len(image.get_shape()) != 3:\n      raise ValueError(\'Input must be of size [height, width, channels]\')\n    if len(means) != image.get_shape()[-1]:\n      raise ValueError(\'len(means) must match the number of channels\')\n    return image - [[means]]\n\n\ndef one_hot_encoding(labels, num_classes=None):\n  """"""One-hot encodes the multiclass labels.\n\n  Example usage:\n    labels = tf.constant([1, 4], dtype=tf.int32)\n    one_hot = OneHotEncoding(labels, num_classes=5)\n    one_hot.eval()    # evaluates to [0, 1, 0, 0, 1]\n\n  Args:\n    labels: A tensor of shape [None] corresponding to the labels.\n    num_classes: Number of classes in the dataset.\n  Returns:\n    onehot_labels: a tensor of shape [num_classes] corresponding to the one hot\n      encoding of the labels.\n  Raises:\n    ValueError: if num_classes is not specified.\n  """"""\n  with tf.name_scope(\'OneHotEncoding\', values=[labels]):\n    if num_classes is None:\n      raise ValueError(\'num_classes must be specified\')\n\n    labels = tf.one_hot(labels, num_classes, 1, 0)\n    return tf.reduce_max(labels, 0)\n\n\ndef rgb_to_gray(image):\n  """"""Converts a 3 channel RGB image to a 1 channel grayscale image.\n\n  Args:\n    image: Rank 3 float32 tensor containing 1 image -> [height, width, 3]\n           with pixel values varying between [0, 1].\n\n  Returns:\n    image: A single channel grayscale image -> [image, height, 1].\n  """"""\n  return _rgb_to_grayscale(image)\n\n\ndef ssd_random_crop(image,\n                    boxes,\n                    labels,\n                    label_scores=None,\n                    masks=None,\n                    keypoints=None,\n                    min_object_covered=(0.0, 0.1, 0.3, 0.5, 0.7, 0.9, 1.0),\n                    aspect_ratio_range=((0.5, 2.0),) * 7,\n                    area_range=((0.1, 1.0),) * 7,\n                    overlap_thresh=(0.0, 0.1, 0.3, 0.5, 0.7, 0.9, 1.0),\n                    random_coef=(0.15,) * 7,\n                    seed=None,\n                    preprocess_vars_cache=None):\n  """"""Random crop preprocessing with default parameters as in SSD paper.\n\n  Liu et al., SSD: Single shot multibox detector.\n  For further information on random crop preprocessing refer to RandomCrop\n  function above.\n\n  Args:\n    image: rank 3 float32 tensor contains 1 image -> [height, width, channels]\n           with pixel values varying between [0, 1].\n    boxes: rank 2 float32 tensor containing the bounding boxes -> [N, 4].\n           Boxes are in normalized form meaning their coordinates vary\n           between [0, 1].\n           Each row is in the form of [ymin, xmin, ymax, xmax].\n    labels: rank 1 int32 tensor containing the object classes.\n    label_scores: rank 1 float32 tensor containing the scores.\n    masks: (optional) rank 3 float32 tensor with shape\n           [num_instances, height, width] containing instance masks. The masks\n           are of the same height, width as the input `image`.\n    keypoints: (optional) rank 3 float32 tensor with shape\n               [num_instances, num_keypoints, 2]. The keypoints are in y-x\n               normalized coordinates.\n    min_object_covered: the cropped image must cover at least this fraction of\n                        at least one of the input bounding boxes.\n    aspect_ratio_range: allowed range for aspect ratio of cropped image.\n    area_range: allowed range for area ratio between cropped image and the\n                original image.\n    overlap_thresh: minimum overlap thresh with new cropped\n                    image to keep the box.\n    random_coef: a random coefficient that defines the chance of getting the\n                 original image. If random_coef is 0, we will always get the\n                 cropped image, and if it is 1.0, we will always get the\n                 original image.\n    seed: random seed.\n    preprocess_vars_cache: PreprocessorCache object that records previously\n                           performed augmentations. Updated in-place. If this\n                           function is called multiple times with the same\n                           non-null cache, it will perform deterministically.\n\n  Returns:\n    image: image which is the same rank as input image.\n    boxes: boxes which is the same rank as input boxes.\n           Boxes are in normalized form.\n    labels: new labels.\n\n    If label_scores, masks, or keypoints is not None, the function also returns:\n    label_scores: new label scores.\n    masks: rank 3 float32 tensor with shape [num_instances, height, width]\n           containing instance masks.\n    keypoints: rank 3 float32 tensor with shape\n               [num_instances, num_keypoints, 2]\n  """"""\n\n  def random_crop_selector(selected_result, index):\n    """"""Applies random_crop_image to selected result.\n\n    Args:\n      selected_result: A tuple containing image, boxes, labels, keypoints (if\n                       not None), and masks (if not None).\n      index: The index that was randomly selected.\n\n    Returns: A tuple containing image, boxes, labels, keypoints (if not None),\n             and masks (if not None).\n    """"""\n    i = 3\n    image, boxes, labels = selected_result[:i]\n    selected_label_scores = None\n    selected_masks = None\n    selected_keypoints = None\n    if label_scores is not None:\n      selected_label_scores = selected_result[i]\n      i += 1\n    if masks is not None:\n      selected_masks = selected_result[i]\n      i += 1\n    if keypoints is not None:\n      selected_keypoints = selected_result[i]\n\n    return random_crop_image(\n        image=image,\n        boxes=boxes,\n        labels=labels,\n        label_scores=selected_label_scores,\n        masks=selected_masks,\n        keypoints=selected_keypoints,\n        min_object_covered=min_object_covered[index],\n        aspect_ratio_range=aspect_ratio_range[index],\n        area_range=area_range[index],\n        overlap_thresh=overlap_thresh[index],\n        random_coef=random_coef[index],\n        seed=seed,\n        preprocess_vars_cache=preprocess_vars_cache)\n\n  result = _apply_with_random_selector_tuples(\n      tuple(\n          t for t in (image, boxes, labels, label_scores, masks, keypoints)\n          if t is not None),\n      random_crop_selector,\n      num_cases=len(min_object_covered),\n      preprocess_vars_cache=preprocess_vars_cache,\n      key=preprocessor_cache.PreprocessorCache.SSD_CROP_SELECTOR_ID)\n  return result\n\n\ndef ssd_random_crop_pad(image,\n                        boxes,\n                        labels,\n                        label_scores=None,\n                        min_object_covered=(0.1, 0.3, 0.5, 0.7, 0.9, 1.0),\n                        aspect_ratio_range=((0.5, 2.0),) * 6,\n                        area_range=((0.1, 1.0),) * 6,\n                        overlap_thresh=(0.1, 0.3, 0.5, 0.7, 0.9, 1.0),\n                        random_coef=(0.15,) * 6,\n                        min_padded_size_ratio=((1.0, 1.0),) * 6,\n                        max_padded_size_ratio=((2.0, 2.0),) * 6,\n                        pad_color=(None,) * 6,\n                        seed=None,\n                        preprocess_vars_cache=None):\n  """"""Random crop preprocessing with default parameters as in SSD paper.\n\n  Liu et al., SSD: Single shot multibox detector.\n  For further information on random crop preprocessing refer to RandomCrop\n  function above.\n\n  Args:\n    image: rank 3 float32 tensor containing 1 image -> [height, width, channels]\n           with pixel values varying between [0, 1].\n    boxes: rank 2 float32 tensor containing the bounding boxes -> [N, 4].\n           Boxes are in normalized form meaning their coordinates vary\n           between [0, 1].\n           Each row is in the form of [ymin, xmin, ymax, xmax].\n    labels: rank 1 int32 tensor containing the object classes.\n    label_scores: float32 tensor of shape [num_instances] representing the\n      score for each box.\n    min_object_covered: the cropped image must cover at least this fraction of\n                        at least one of the input bounding boxes.\n    aspect_ratio_range: allowed range for aspect ratio of cropped image.\n    area_range: allowed range for area ratio between cropped image and the\n                original image.\n    overlap_thresh: minimum overlap thresh with new cropped\n                    image to keep the box.\n    random_coef: a random coefficient that defines the chance of getting the\n                 original image. If random_coef is 0, we will always get the\n                 cropped image, and if it is 1.0, we will always get the\n                 original image.\n    min_padded_size_ratio: min ratio of padded image height and width to the\n                           input image\'s height and width.\n    max_padded_size_ratio: max ratio of padded image height and width to the\n                           input image\'s height and width.\n    pad_color: padding color. A rank 1 tensor of [3] with dtype=tf.float32.\n               if set as None, it will be set to average color of the randomly\n               cropped image.\n    seed: random seed.\n    preprocess_vars_cache: PreprocessorCache object that records previously\n                           performed augmentations. Updated in-place. If this\n                           function is called multiple times with the same\n                           non-null cache, it will perform deterministically.\n\n  Returns:\n    image: Image shape will be [new_height, new_width, channels].\n    boxes: boxes which is the same rank as input boxes. Boxes are in normalized\n           form.\n    new_labels: new labels.\n    new_label_scores: new label scores.\n  """"""\n\n  def random_crop_pad_selector(image_boxes_labels, index):\n    i = 3\n    image, boxes, labels = image_boxes_labels[:i]\n    selected_label_scores = None\n    if label_scores is not None:\n      selected_label_scores = image_boxes_labels[i]\n\n    return random_crop_pad_image(\n        image,\n        boxes,\n        labels,\n        selected_label_scores,\n        min_object_covered=min_object_covered[index],\n        aspect_ratio_range=aspect_ratio_range[index],\n        area_range=area_range[index],\n        overlap_thresh=overlap_thresh[index],\n        random_coef=random_coef[index],\n        min_padded_size_ratio=min_padded_size_ratio[index],\n        max_padded_size_ratio=max_padded_size_ratio[index],\n        pad_color=pad_color[index],\n        seed=seed,\n        preprocess_vars_cache=preprocess_vars_cache)\n\n  return _apply_with_random_selector_tuples(\n      tuple(t for t in (image, boxes, labels, label_scores) if t is not None),\n      random_crop_pad_selector,\n      num_cases=len(min_object_covered),\n      preprocess_vars_cache=preprocess_vars_cache,\n      key=preprocessor_cache.PreprocessorCache.SSD_CROP_PAD_SELECTOR_ID)\n\n\ndef ssd_random_crop_fixed_aspect_ratio(\n    image,\n    boxes,\n    labels,\n    label_scores=None,\n    masks=None,\n    keypoints=None,\n    min_object_covered=(0.0, 0.1, 0.3, 0.5, 0.7, 0.9, 1.0),\n    aspect_ratio=1.0,\n    area_range=((0.1, 1.0),) * 7,\n    overlap_thresh=(0.0, 0.1, 0.3, 0.5, 0.7, 0.9, 1.0),\n    random_coef=(0.15,) * 7,\n    seed=None,\n    preprocess_vars_cache=None):\n  """"""Random crop preprocessing with default parameters as in SSD paper.\n\n  Liu et al., SSD: Single shot multibox detector.\n  For further information on random crop preprocessing refer to RandomCrop\n  function above.\n\n  The only difference is that the aspect ratio of the crops are fixed.\n\n  Args:\n    image: rank 3 float32 tensor contains 1 image -> [height, width, channels]\n           with pixel values varying between [0, 1].\n    boxes: rank 2 float32 tensor containing the bounding boxes -> [N, 4].\n           Boxes are in normalized form meaning their coordinates vary\n           between [0, 1].\n           Each row is in the form of [ymin, xmin, ymax, xmax].\n    labels: rank 1 int32 tensor containing the object classes.\n    label_scores: (optional) float32 tensor of shape [num_instances]\n      representing the score for each box.\n    masks: (optional) rank 3 float32 tensor with shape\n           [num_instances, height, width] containing instance masks. The masks\n           are of the same height, width as the input `image`.\n    keypoints: (optional) rank 3 float32 tensor with shape\n               [num_instances, num_keypoints, 2]. The keypoints are in y-x\n               normalized coordinates.\n    min_object_covered: the cropped image must cover at least this fraction of\n                        at least one of the input bounding boxes.\n    aspect_ratio: aspect ratio of the cropped image.\n    area_range: allowed range for area ratio between cropped image and the\n                original image.\n    overlap_thresh: minimum overlap thresh with new cropped\n                    image to keep the box.\n    random_coef: a random coefficient that defines the chance of getting the\n                 original image. If random_coef is 0, we will always get the\n                 cropped image, and if it is 1.0, we will always get the\n                 original image.\n    seed: random seed.\n    preprocess_vars_cache: PreprocessorCache object that records previously\n                           performed augmentations. Updated in-place. If this\n                           function is called multiple times with the same\n                           non-null cache, it will perform deterministically.\n\n  Returns:\n    image: image which is the same rank as input image.\n    boxes: boxes which is the same rank as input boxes.\n           Boxes are in normalized form.\n    labels: new labels.\n\n    If masks or keypoints is not None, the function also returns:\n\n    masks: rank 3 float32 tensor with shape [num_instances, height, width]\n           containing instance masks.\n    keypoints: rank 3 float32 tensor with shape\n               [num_instances, num_keypoints, 2]\n  """"""\n  aspect_ratio_range = ((aspect_ratio, aspect_ratio),) * len(area_range)\n\n  crop_result = ssd_random_crop(\n      image, boxes, labels, label_scores, masks, keypoints, min_object_covered,\n      aspect_ratio_range, area_range, overlap_thresh, random_coef, seed,\n      preprocess_vars_cache)\n  i = 3\n  new_image, new_boxes, new_labels = crop_result[:i]\n  new_label_scores = None\n  new_masks = None\n  new_keypoints = None\n  if label_scores is not None:\n    new_label_scores = crop_result[i]\n    i += 1\n  if masks is not None:\n    new_masks = crop_result[i]\n    i += 1\n  if keypoints is not None:\n    new_keypoints = crop_result[i]\n  result = random_crop_to_aspect_ratio(\n      new_image,\n      new_boxes,\n      new_labels,\n      new_label_scores,\n      new_masks,\n      new_keypoints,\n      aspect_ratio=aspect_ratio,\n      seed=seed,\n      preprocess_vars_cache=preprocess_vars_cache)\n\n  return result\n\n\ndef ssd_random_crop_pad_fixed_aspect_ratio(\n    image,\n    boxes,\n    labels,\n    label_scores=None,\n    masks=None,\n    keypoints=None,\n    min_object_covered=(0.0, 0.1, 0.3, 0.5, 0.7, 0.9, 1.0),\n    aspect_ratio=1.0,\n    aspect_ratio_range=((0.5, 2.0),) * 7,\n    area_range=((0.1, 1.0),) * 7,\n    overlap_thresh=(0.0, 0.1, 0.3, 0.5, 0.7, 0.9, 1.0),\n    random_coef=(0.15,) * 7,\n    min_padded_size_ratio=(1.0, 1.0),\n    max_padded_size_ratio=(2.0, 2.0),\n    seed=None,\n    preprocess_vars_cache=None):\n  """"""Random crop and pad preprocessing with default parameters as in SSD paper.\n\n  Liu et al., SSD: Single shot multibox detector.\n  For further information on random crop preprocessing refer to RandomCrop\n  function above.\n\n  The only difference is that after the initial crop, images are zero-padded\n  to a fixed aspect ratio instead of being resized to that aspect ratio.\n\n  Args:\n    image: rank 3 float32 tensor contains 1 image -> [height, width, channels]\n           with pixel values varying between [0, 1].\n    boxes: rank 2 float32 tensor containing the bounding boxes -> [N, 4].\n           Boxes are in normalized form meaning their coordinates vary\n           between [0, 1].\n           Each row is in the form of [ymin, xmin, ymax, xmax].\n    labels: rank 1 int32 tensor containing the object classes.\n    label_scores: (optional) float32 tensor of shape [num_instances]\n      representing the score for each box.\n    masks: (optional) rank 3 float32 tensor with shape\n           [num_instances, height, width] containing instance masks. The masks\n           are of the same height, width as the input `image`.\n    keypoints: (optional) rank 3 float32 tensor with shape\n               [num_instances, num_keypoints, 2]. The keypoints are in y-x\n               normalized coordinates.\n    min_object_covered: the cropped image must cover at least this fraction of\n                        at least one of the input bounding boxes.\n    aspect_ratio: the final aspect ratio to pad to.\n    aspect_ratio_range: allowed range for aspect ratio of cropped image.\n    area_range: allowed range for area ratio between cropped image and the\n                original image.\n    overlap_thresh: minimum overlap thresh with new cropped\n                    image to keep the box.\n    random_coef: a random coefficient that defines the chance of getting the\n                 original image. If random_coef is 0, we will always get the\n                 cropped image, and if it is 1.0, we will always get the\n                 original image.\n    min_padded_size_ratio: min ratio of padded image height and width to the\n                           input image\'s height and width.\n    max_padded_size_ratio: max ratio of padded image height and width to the\n                           input image\'s height and width.\n    seed: random seed.\n    preprocess_vars_cache: PreprocessorCache object that records previously\n                           performed augmentations. Updated in-place. If this\n                           function is called multiple times with the same\n                           non-null cache, it will perform deterministically.\n\n  Returns:\n    image: image which is the same rank as input image.\n    boxes: boxes which is the same rank as input boxes.\n           Boxes are in normalized form.\n    labels: new labels.\n\n    If masks or keypoints is not None, the function also returns:\n\n    masks: rank 3 float32 tensor with shape [num_instances, height, width]\n           containing instance masks.\n    keypoints: rank 3 float32 tensor with shape\n               [num_instances, num_keypoints, 2]\n  """"""\n  crop_result = ssd_random_crop(\n      image, boxes, labels, label_scores, masks, keypoints, min_object_covered,\n      aspect_ratio_range, area_range, overlap_thresh, random_coef, seed,\n      preprocess_vars_cache)\n  i = 3\n  new_image, new_boxes, new_labels = crop_result[:i]\n  new_label_scores = None\n  new_masks = None\n  new_keypoints = None\n  if label_scores is not None:\n    new_label_scores = crop_result[i]\n    i += 1\n  if masks is not None:\n    new_masks = crop_result[i]\n    i += 1\n  if keypoints is not None:\n    new_keypoints = crop_result[i]\n  result = random_pad_to_aspect_ratio(\n      new_image,\n      new_boxes,\n      new_masks,\n      new_keypoints,\n      aspect_ratio=aspect_ratio,\n      min_padded_size_ratio=min_padded_size_ratio,\n      max_padded_size_ratio=max_padded_size_ratio,\n      seed=seed,\n      preprocess_vars_cache=preprocess_vars_cache)\n\n  result = list(result)\n  if new_label_scores is not None:\n    result.insert(2, new_label_scores)\n  result.insert(2, new_labels)\n  result = tuple(result)\n\n  return result\n\n\ndef get_default_func_arg_map(include_label_scores=False,\n                             include_instance_masks=False,\n                             include_keypoints=False):\n  """"""Returns the default mapping from a preprocessor function to its args.\n\n  Args:\n    include_label_scores: If True, preprocessing functions will modify the\n      label scores, too.\n    include_instance_masks: If True, preprocessing functions will modify the\n      instance masks, too.\n    include_keypoints: If True, preprocessing functions will modify the\n      keypoints, too.\n\n  Returns:\n    A map from preprocessing functions to the arguments they receive.\n  """"""\n  groundtruth_label_scores = None\n  if include_label_scores:\n    groundtruth_label_scores = (fields.InputDataFields.groundtruth_label_scores)\n\n  groundtruth_instance_masks = None\n  if include_instance_masks:\n    groundtruth_instance_masks = (\n        fields.InputDataFields.groundtruth_instance_masks)\n\n  groundtruth_keypoints = None\n  if include_keypoints:\n    groundtruth_keypoints = fields.InputDataFields.groundtruth_keypoints\n\n  prep_func_arg_map = {\n      normalize_image: (fields.InputDataFields.image,),\n      random_horizontal_flip: (\n          fields.InputDataFields.image,\n          fields.InputDataFields.groundtruth_boxes,\n          groundtruth_instance_masks,\n          groundtruth_keypoints,),\n      random_vertical_flip: (\n          fields.InputDataFields.image,\n          fields.InputDataFields.groundtruth_boxes,\n          groundtruth_instance_masks,\n          groundtruth_keypoints,),\n      random_rotation90: (\n          fields.InputDataFields.image,\n          fields.InputDataFields.groundtruth_boxes,\n          groundtruth_instance_masks,\n          groundtruth_keypoints,),\n      random_pixel_value_scale: (fields.InputDataFields.image,),\n      random_image_scale: (\n          fields.InputDataFields.image,\n          groundtruth_instance_masks,),\n      random_rgb_to_gray: (fields.InputDataFields.image,),\n      random_adjust_brightness: (fields.InputDataFields.image,),\n      random_adjust_contrast: (fields.InputDataFields.image,),\n      random_adjust_hue: (fields.InputDataFields.image,),\n      random_adjust_saturation: (fields.InputDataFields.image,),\n      random_distort_color: (fields.InputDataFields.image,),\n      random_jitter_boxes: (fields.InputDataFields.groundtruth_boxes,),\n      random_crop_image: (\n          fields.InputDataFields.image,\n          fields.InputDataFields.groundtruth_boxes,\n          fields.InputDataFields.groundtruth_classes,\n          groundtruth_label_scores,\n          groundtruth_instance_masks,\n          groundtruth_keypoints,),\n      random_pad_image: (fields.InputDataFields.image,\n                         fields.InputDataFields.groundtruth_boxes),\n      random_crop_pad_image: (fields.InputDataFields.image,\n                              fields.InputDataFields.groundtruth_boxes,\n                              fields.InputDataFields.groundtruth_classes,\n                              groundtruth_label_scores),\n      random_crop_to_aspect_ratio: (\n          fields.InputDataFields.image,\n          fields.InputDataFields.groundtruth_boxes,\n          fields.InputDataFields.groundtruth_classes,\n          groundtruth_label_scores,\n          groundtruth_instance_masks,\n          groundtruth_keypoints,),\n      random_pad_to_aspect_ratio: (\n          fields.InputDataFields.image,\n          fields.InputDataFields.groundtruth_boxes,\n          groundtruth_instance_masks,\n          groundtruth_keypoints,),\n      random_black_patches: (fields.InputDataFields.image,),\n      retain_boxes_above_threshold: (\n          fields.InputDataFields.groundtruth_boxes,\n          fields.InputDataFields.groundtruth_classes,\n          groundtruth_label_scores,\n          groundtruth_instance_masks,\n          groundtruth_keypoints,),\n      image_to_float: (fields.InputDataFields.image,),\n      random_resize_method: (fields.InputDataFields.image,),\n      resize_to_range: (\n          fields.InputDataFields.image,\n          groundtruth_instance_masks,),\n      resize_to_min_dimension: (\n          fields.InputDataFields.image,\n          groundtruth_instance_masks,),\n      scale_boxes_to_pixel_coordinates: (\n          fields.InputDataFields.image,\n          fields.InputDataFields.groundtruth_boxes,\n          groundtruth_keypoints,),\n      resize_image: (\n          fields.InputDataFields.image,\n          groundtruth_instance_masks,),\n      subtract_channel_mean: (fields.InputDataFields.image,),\n      one_hot_encoding: (fields.InputDataFields.groundtruth_image_classes,),\n      rgb_to_gray: (fields.InputDataFields.image,),\n      ssd_random_crop: (\n          fields.InputDataFields.image,\n          fields.InputDataFields.groundtruth_boxes,\n          fields.InputDataFields.groundtruth_classes,\n          groundtruth_label_scores,\n          groundtruth_instance_masks,\n          groundtruth_keypoints,),\n      ssd_random_crop_pad: (fields.InputDataFields.image,\n                            fields.InputDataFields.groundtruth_boxes,\n                            fields.InputDataFields.groundtruth_classes,\n                            groundtruth_label_scores),\n      ssd_random_crop_fixed_aspect_ratio: (\n          fields.InputDataFields.image,\n          fields.InputDataFields.groundtruth_boxes,\n          fields.InputDataFields.groundtruth_classes,\n          groundtruth_label_scores,\n          groundtruth_instance_masks,\n          groundtruth_keypoints,),\n      ssd_random_crop_pad_fixed_aspect_ratio: (\n          fields.InputDataFields.image,\n          fields.InputDataFields.groundtruth_boxes,\n          fields.InputDataFields.groundtruth_classes,\n          groundtruth_label_scores,\n          groundtruth_instance_masks,\n          groundtruth_keypoints,),\n  }\n\n  return prep_func_arg_map\n\n\ndef preprocess(tensor_dict,\n               preprocess_options,\n               func_arg_map=None,\n               preprocess_vars_cache=None):\n  """"""Preprocess images and bounding boxes.\n\n  Various types of preprocessing (to be implemented) based on the\n  preprocess_options dictionary e.g. ""crop image"" (affects image and possibly\n  boxes), ""white balance image"" (affects only image), etc. If self._options\n  is None, no preprocessing is done.\n\n  Args:\n    tensor_dict: dictionary that contains images, boxes, and can contain other\n                 things as well.\n                 images-> rank 4 float32 tensor contains\n                          1 image -> [1, height, width, 3].\n                          with pixel values varying between [0, 1]\n                 boxes-> rank 2 float32 tensor containing\n                         the bounding boxes -> [N, 4].\n                         Boxes are in normalized form meaning\n                         their coordinates vary between [0, 1].\n                         Each row is in the form\n                         of [ymin, xmin, ymax, xmax].\n    preprocess_options: It is a list of tuples, where each tuple contains a\n                        function and a dictionary that contains arguments and\n                        their values.\n    func_arg_map: mapping from preprocessing functions to arguments that they\n                  expect to receive and return.\n    preprocess_vars_cache: PreprocessorCache object that records previously\n                           performed augmentations. Updated in-place. If this\n                           function is called multiple times with the same\n                           non-null cache, it will perform deterministically.\n\n  Returns:\n    tensor_dict: which contains the preprocessed images, bounding boxes, etc.\n\n  Raises:\n    ValueError: (a) If the functions passed to Preprocess\n                    are not in func_arg_map.\n                (b) If the arguments that a function needs\n                    do not exist in tensor_dict.\n                (c) If image in tensor_dict is not rank 4\n  """"""\n  if func_arg_map is None:\n    func_arg_map = get_default_func_arg_map()\n\n  # changes the images to image (rank 4 to rank 3) since the functions\n  # receive rank 3 tensor for image\n  if fields.InputDataFields.image in tensor_dict:\n    images = tensor_dict[fields.InputDataFields.image]\n    if len(images.get_shape()) != 4:\n      raise ValueError(\'images in tensor_dict should be rank 4\')\n    image = tf.squeeze(images, squeeze_dims=[0])\n    tensor_dict[fields.InputDataFields.image] = image\n\n  # Preprocess inputs based on preprocess_options\n  for option in preprocess_options:\n    func, params = option\n    if func not in func_arg_map:\n      raise ValueError(\'The function %s does not exist in func_arg_map\' %\n                       (func.__name__))\n    arg_names = func_arg_map[func]\n    for a in arg_names:\n      if a is not None and a not in tensor_dict:\n        raise ValueError(\'The function %s requires argument %s\' %\n                         (func.__name__, a))\n\n    def get_arg(key):\n      return tensor_dict[key] if key is not None else None\n\n    args = [get_arg(a) for a in arg_names]\n    if (preprocess_vars_cache is not None and\n        \'preprocess_vars_cache\' in inspect.getargspec(func).args):\n      params[\'preprocess_vars_cache\'] = preprocess_vars_cache\n    results = func(*args, **params)\n    if not isinstance(results, (list, tuple)):\n      results = (results,)\n    # Removes None args since the return values will not contain those.\n    arg_names = [arg_name for arg_name in arg_names if arg_name is not None]\n    for res, arg_name in zip(results, arg_names):\n      tensor_dict[arg_name] = res\n\n  # changes the image to images (rank 3 to rank 4) to be compatible to what\n  # we received in the first place\n  if fields.InputDataFields.image in tensor_dict:\n    image = tensor_dict[fields.InputDataFields.image]\n    images = tf.expand_dims(image, 0)\n    tensor_dict[fields.InputDataFields.image] = images\n\n  return tensor_dict\n'"
src/object_detection/core/preprocessor_cache.py,0,"b'# Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Records previous preprocessing operations and allows them to be repeated.\n\nUsed with object_detection.core.preprocessor. Passing a PreprocessorCache\ninto individual data augmentation functions or the general preprocess() function\nwill store all randomly generated variables in the PreprocessorCache. When\na preprocessor function is called multiple times with the same\nPreprocessorCache object, that function will perform the same augmentation\non all calls.\n""""""\n\nfrom collections import defaultdict\n\n\nclass PreprocessorCache(object):\n  """"""Dictionary wrapper storing random variables generated during preprocessing.\n  """"""\n\n  # Constant keys representing different preprocessing functions\n  ROTATION90 = \'rotation90\'\n  HORIZONTAL_FLIP = \'horizontal_flip\'\n  VERTICAL_FLIP = \'vertical_flip\'\n  PIXEL_VALUE_SCALE = \'pixel_value_scale\'\n  IMAGE_SCALE = \'image_scale\'\n  RGB_TO_GRAY = \'rgb_to_gray\'\n  ADJUST_BRIGHTNESS = \'adjust_brightness\'\n  ADJUST_CONTRAST = \'adjust_contrast\'\n  ADJUST_HUE = \'adjust_hue\'\n  ADJUST_SATURATION = \'adjust_saturation\'\n  DISTORT_COLOR = \'distort_color\'\n  STRICT_CROP_IMAGE = \'strict_crop_image\'\n  CROP_IMAGE = \'crop_image\'\n  PAD_IMAGE = \'pad_image\'\n  CROP_TO_ASPECT_RATIO = \'crop_to_aspect_ratio\'\n  RESIZE_METHOD = \'resize_method\'\n  PAD_TO_ASPECT_RATIO = \'pad_to_aspect_ratio\'\n  BLACK_PATCHES = \'black_patches\'\n  ADD_BLACK_PATCH = \'add_black_patch\'\n  SELECTOR = \'selector\'\n  SELECTOR_TUPLES = \'selector_tuples\'\n  SSD_CROP_SELECTOR_ID = \'ssd_crop_selector_id\'\n  SSD_CROP_PAD_SELECTOR_ID = \'ssd_crop_pad_selector_id\'\n\n  # 23 permitted function ids\n  _VALID_FNS = [ROTATION90, HORIZONTAL_FLIP, VERTICAL_FLIP, PIXEL_VALUE_SCALE,\n                IMAGE_SCALE, RGB_TO_GRAY, ADJUST_BRIGHTNESS, ADJUST_CONTRAST,\n                ADJUST_HUE, ADJUST_SATURATION, DISTORT_COLOR, STRICT_CROP_IMAGE,\n                CROP_IMAGE, PAD_IMAGE, CROP_TO_ASPECT_RATIO, RESIZE_METHOD,\n                PAD_TO_ASPECT_RATIO, BLACK_PATCHES, ADD_BLACK_PATCH, SELECTOR,\n                SELECTOR_TUPLES, SSD_CROP_SELECTOR_ID, SSD_CROP_PAD_SELECTOR_ID]\n\n  def __init__(self):\n    self._history = defaultdict(dict)\n\n  def clear(self):\n    """"""Resets cache.""""""\n    self._history = {}\n\n  def get(self, function_id, key):\n    """"""Gets stored value given a function id and key.\n\n    Args:\n      function_id: identifier for the preprocessing function used.\n      key: identifier for the variable stored.\n    Returns:\n      value: the corresponding value, expected to be a tensor or\n             nested structure of tensors.\n    Raises:\n      ValueError: if function_id is not one of the 23 valid function ids.\n    """"""\n    if function_id not in self._VALID_FNS:\n      raise ValueError(\'Function id not recognized: %s.\' % str(function_id))\n    return self._history[function_id].get(key)\n\n  def update(self, function_id, key, value):\n    """"""Adds a value to the dictionary.\n\n    Args:\n      function_id: identifier for the preprocessing function used.\n      key: identifier for the variable stored.\n      value: the value to store, expected to be a tensor or nested structure\n             of tensors.\n    Raises:\n      ValueError: if function_id is not one of the 23 valid function ids.\n    """"""\n    if function_id not in self._VALID_FNS:\n      raise ValueError(\'Function id not recognized: %s.\' % str(function_id))\n    self._history[function_id][key] = value\n\n'"
src/object_detection/core/preprocessor_test.py,321,"b'# Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n""""""Tests for object_detection.core.preprocessor.""""""\n\nimport numpy as np\nimport six\n\nimport tensorflow as tf\n\nfrom object_detection.core import preprocessor\nfrom object_detection.core import preprocessor_cache\nfrom object_detection.core import standard_fields as fields\n\nif six.PY2:\n  import mock  # pylint: disable=g-import-not-at-top\nelse:\n  from unittest import mock  # pylint: disable=g-import-not-at-top\n\n\nclass PreprocessorTest(tf.test.TestCase):\n\n  def createColorfulTestImage(self):\n    ch255 = tf.fill([1, 100, 200, 1], tf.constant(255, dtype=tf.uint8))\n    ch128 = tf.fill([1, 100, 200, 1], tf.constant(128, dtype=tf.uint8))\n    ch0 = tf.fill([1, 100, 200, 1], tf.constant(0, dtype=tf.uint8))\n    imr = tf.concat([ch255, ch0, ch0], 3)\n    img = tf.concat([ch255, ch255, ch0], 3)\n    imb = tf.concat([ch255, ch0, ch255], 3)\n    imw = tf.concat([ch128, ch128, ch128], 3)\n    imu = tf.concat([imr, img], 2)\n    imd = tf.concat([imb, imw], 2)\n    im = tf.concat([imu, imd], 1)\n    return im\n\n  def createTestImages(self):\n    images_r = tf.constant([[[128, 128, 128, 128], [0, 0, 128, 128],\n                             [0, 128, 128, 128], [192, 192, 128, 128]]],\n                           dtype=tf.uint8)\n    images_r = tf.expand_dims(images_r, 3)\n    images_g = tf.constant([[[0, 0, 128, 128], [0, 0, 128, 128],\n                             [0, 128, 192, 192], [192, 192, 128, 192]]],\n                           dtype=tf.uint8)\n    images_g = tf.expand_dims(images_g, 3)\n    images_b = tf.constant([[[128, 128, 192, 0], [0, 0, 128, 192],\n                             [0, 128, 128, 0], [192, 192, 192, 128]]],\n                           dtype=tf.uint8)\n    images_b = tf.expand_dims(images_b, 3)\n    images = tf.concat([images_r, images_g, images_b], 3)\n    return images\n\n  def createEmptyTestBoxes(self):\n    boxes = tf.constant([[]], dtype=tf.float32)\n    return boxes\n\n  def createTestBoxes(self):\n    boxes = tf.constant(\n        [[0.0, 0.25, 0.75, 1.0], [0.25, 0.5, 0.75, 1.0]], dtype=tf.float32)\n    return boxes\n\n  def createTestLabelScores(self):\n    return tf.constant([1.0, 0.5], dtype=tf.float32)\n\n  def createTestLabelScoresWithMissingScore(self):\n    return tf.constant([0.5, np.nan], dtype=tf.float32)\n\n  def createTestMasks(self):\n    mask = np.array([\n        [[255.0, 0.0, 0.0],\n         [255.0, 0.0, 0.0],\n         [255.0, 0.0, 0.0]],\n        [[255.0, 255.0, 0.0],\n         [255.0, 255.0, 0.0],\n         [255.0, 255.0, 0.0]]])\n    return tf.constant(mask, dtype=tf.float32)\n\n  def createTestKeypoints(self):\n    keypoints = np.array([\n        [[0.1, 0.1], [0.2, 0.2], [0.3, 0.3]],\n        [[0.4, 0.4], [0.5, 0.5], [0.6, 0.6]],\n    ])\n    return tf.constant(keypoints, dtype=tf.float32)\n\n  def createTestKeypointsInsideCrop(self):\n    keypoints = np.array([\n        [[0.4, 0.4], [0.5, 0.5], [0.6, 0.6]],\n        [[0.4, 0.4], [0.5, 0.5], [0.6, 0.6]],\n    ])\n    return tf.constant(keypoints, dtype=tf.float32)\n\n  def createTestKeypointsOutsideCrop(self):\n    keypoints = np.array([\n        [[0.1, 0.1], [0.2, 0.2], [0.3, 0.3]],\n        [[0.1, 0.1], [0.2, 0.2], [0.3, 0.3]],\n    ])\n    return tf.constant(keypoints, dtype=tf.float32)\n\n  def createKeypointFlipPermutation(self):\n    return np.array([0, 2, 1], dtype=np.int32)\n\n  def createTestLabels(self):\n    labels = tf.constant([1, 2], dtype=tf.int32)\n    return labels\n\n  def createTestBoxesOutOfImage(self):\n    boxes = tf.constant(\n        [[-0.1, 0.25, 0.75, 1], [0.25, 0.5, 0.75, 1.1]], dtype=tf.float32)\n    return boxes\n\n  def expectedImagesAfterNormalization(self):\n    images_r = tf.constant([[[0, 0, 0, 0], [-1, -1, 0, 0],\n                             [-1, 0, 0, 0], [0.5, 0.5, 0, 0]]],\n                           dtype=tf.float32)\n    images_r = tf.expand_dims(images_r, 3)\n    images_g = tf.constant([[[-1, -1, 0, 0], [-1, -1, 0, 0],\n                             [-1, 0, 0.5, 0.5], [0.5, 0.5, 0, 0.5]]],\n                           dtype=tf.float32)\n    images_g = tf.expand_dims(images_g, 3)\n    images_b = tf.constant([[[0, 0, 0.5, -1], [-1, -1, 0, 0.5],\n                             [-1, 0, 0, -1], [0.5, 0.5, 0.5, 0]]],\n                           dtype=tf.float32)\n    images_b = tf.expand_dims(images_b, 3)\n    images = tf.concat([images_r, images_g, images_b], 3)\n    return images\n\n  def expectedMaxImageAfterColorScale(self):\n    images_r = tf.constant([[[0.1, 0.1, 0.1, 0.1], [-0.9, -0.9, 0.1, 0.1],\n                             [-0.9, 0.1, 0.1, 0.1], [0.6, 0.6, 0.1, 0.1]]],\n                           dtype=tf.float32)\n    images_r = tf.expand_dims(images_r, 3)\n    images_g = tf.constant([[[-0.9, -0.9, 0.1, 0.1], [-0.9, -0.9, 0.1, 0.1],\n                             [-0.9, 0.1, 0.6, 0.6], [0.6, 0.6, 0.1, 0.6]]],\n                           dtype=tf.float32)\n    images_g = tf.expand_dims(images_g, 3)\n    images_b = tf.constant([[[0.1, 0.1, 0.6, -0.9], [-0.9, -0.9, 0.1, 0.6],\n                             [-0.9, 0.1, 0.1, -0.9], [0.6, 0.6, 0.6, 0.1]]],\n                           dtype=tf.float32)\n    images_b = tf.expand_dims(images_b, 3)\n    images = tf.concat([images_r, images_g, images_b], 3)\n    return images\n\n  def expectedMinImageAfterColorScale(self):\n    images_r = tf.constant([[[-0.1, -0.1, -0.1, -0.1], [-1, -1, -0.1, -0.1],\n                             [-1, -0.1, -0.1, -0.1], [0.4, 0.4, -0.1, -0.1]]],\n                           dtype=tf.float32)\n    images_r = tf.expand_dims(images_r, 3)\n    images_g = tf.constant([[[-1, -1, -0.1, -0.1], [-1, -1, -0.1, -0.1],\n                             [-1, -0.1, 0.4, 0.4], [0.4, 0.4, -0.1, 0.4]]],\n                           dtype=tf.float32)\n    images_g = tf.expand_dims(images_g, 3)\n    images_b = tf.constant([[[-0.1, -0.1, 0.4, -1], [-1, -1, -0.1, 0.4],\n                             [-1, -0.1, -0.1, -1], [0.4, 0.4, 0.4, -0.1]]],\n                           dtype=tf.float32)\n    images_b = tf.expand_dims(images_b, 3)\n    images = tf.concat([images_r, images_g, images_b], 3)\n    return images\n\n  def expectedImagesAfterLeftRightFlip(self):\n    images_r = tf.constant([[[0, 0, 0, 0], [0, 0, -1, -1],\n                             [0, 0, 0, -1], [0, 0, 0.5, 0.5]]],\n                           dtype=tf.float32)\n    images_r = tf.expand_dims(images_r, 3)\n    images_g = tf.constant([[[0, 0, -1, -1], [0, 0, -1, -1],\n                             [0.5, 0.5, 0, -1], [0.5, 0, 0.5, 0.5]]],\n                           dtype=tf.float32)\n    images_g = tf.expand_dims(images_g, 3)\n    images_b = tf.constant([[[-1, 0.5, 0, 0], [0.5, 0, -1, -1],\n                             [-1, 0, 0, -1], [0, 0.5, 0.5, 0.5]]],\n                           dtype=tf.float32)\n    images_b = tf.expand_dims(images_b, 3)\n    images = tf.concat([images_r, images_g, images_b], 3)\n    return images\n\n  def expectedImagesAfterUpDownFlip(self):\n    images_r = tf.constant([[[0.5, 0.5, 0, 0], [-1, 0, 0, 0],\n                             [-1, -1, 0, 0], [0, 0, 0, 0]]],\n                           dtype=tf.float32)\n    images_r = tf.expand_dims(images_r, 3)\n    images_g = tf.constant([[[0.5, 0.5, 0, 0.5], [-1, 0, 0.5, 0.5],\n                             [-1, -1, 0, 0], [-1, -1, 0, 0]]],\n                           dtype=tf.float32)\n    images_g = tf.expand_dims(images_g, 3)\n    images_b = tf.constant([[[0.5, 0.5, 0.5, 0], [-1, 0, 0, -1],\n                             [-1, -1, 0, 0.5], [0, 0, 0.5, -1]]],\n                           dtype=tf.float32)\n    images_b = tf.expand_dims(images_b, 3)\n    images = tf.concat([images_r, images_g, images_b], 3)\n    return images\n\n  def expectedImagesAfterRot90(self):\n    images_r = tf.constant([[[0, 0, 0, 0], [0, 0, 0, 0],\n                             [0, -1, 0, 0.5], [0, -1, -1, 0.5]]],\n                           dtype=tf.float32)\n    images_r = tf.expand_dims(images_r, 3)\n    images_g = tf.constant([[[0, 0, 0.5, 0.5], [0, 0, 0.5, 0],\n                             [-1, -1, 0, 0.5], [-1, -1, -1, 0.5]]],\n                           dtype=tf.float32)\n    images_g = tf.expand_dims(images_g, 3)\n    images_b = tf.constant([[[-1, 0.5, -1, 0], [0.5, 0, 0, 0.5],\n                             [0, -1, 0, 0.5], [0, -1, -1, 0.5]]],\n                           dtype=tf.float32)\n    images_b = tf.expand_dims(images_b, 3)\n    images = tf.concat([images_r, images_g, images_b], 3)\n    return images\n\n  def expectedBoxesAfterLeftRightFlip(self):\n    boxes = tf.constant([[0.0, 0.0, 0.75, 0.75], [0.25, 0.0, 0.75, 0.5]],\n                        dtype=tf.float32)\n    return boxes\n\n  def expectedBoxesAfterUpDownFlip(self):\n    boxes = tf.constant([[0.25, 0.25, 1.0, 1.0], [0.25, 0.5, 0.75, 1.0]],\n                        dtype=tf.float32)\n    return boxes\n\n  def expectedBoxesAfterRot90(self):\n    boxes = tf.constant(\n        [[0.0, 0.0, 0.75, 0.75], [0.0, 0.25, 0.5, 0.75]], dtype=tf.float32)\n    return boxes\n\n  def expectedMasksAfterLeftRightFlip(self):\n    mask = np.array([\n        [[0.0, 0.0, 255.0],\n         [0.0, 0.0, 255.0],\n         [0.0, 0.0, 255.0]],\n        [[0.0, 255.0, 255.0],\n         [0.0, 255.0, 255.0],\n         [0.0, 255.0, 255.0]]])\n    return tf.constant(mask, dtype=tf.float32)\n\n  def expectedMasksAfterUpDownFlip(self):\n    mask = np.array([\n        [[255.0, 0.0, 0.0],\n         [255.0, 0.0, 0.0],\n         [255.0, 0.0, 0.0]],\n        [[255.0, 255.0, 0.0],\n         [255.0, 255.0, 0.0],\n         [255.0, 255.0, 0.0]]])\n    return tf.constant(mask, dtype=tf.float32)\n\n  def expectedMasksAfterRot90(self):\n    mask = np.array([\n        [[0.0, 0.0, 0.0],\n         [0.0, 0.0, 0.0],\n         [255.0, 255.0, 255.0]],\n        [[0.0, 0.0, 0.0],\n         [255.0, 255.0, 255.0],\n         [255.0, 255.0, 255.0]]])\n    return tf.constant(mask, dtype=tf.float32)\n\n  def expectedLabelScoresAfterThresholding(self):\n    return tf.constant([1.0], dtype=tf.float32)\n\n  def expectedBoxesAfterThresholding(self):\n    return tf.constant([[0.0, 0.25, 0.75, 1.0]], dtype=tf.float32)\n\n  def expectedLabelsAfterThresholding(self):\n    return tf.constant([1], dtype=tf.float32)\n\n  def expectedMasksAfterThresholding(self):\n    mask = np.array([\n        [[255.0, 0.0, 0.0],\n         [255.0, 0.0, 0.0],\n         [255.0, 0.0, 0.0]]])\n    return tf.constant(mask, dtype=tf.float32)\n\n  def expectedKeypointsAfterThresholding(self):\n    keypoints = np.array([\n        [[0.1, 0.1], [0.2, 0.2], [0.3, 0.3]]\n    ])\n    return tf.constant(keypoints, dtype=tf.float32)\n\n  def expectedLabelScoresAfterThresholdingWithMissingScore(self):\n    return tf.constant([np.nan], dtype=tf.float32)\n\n  def expectedBoxesAfterThresholdingWithMissingScore(self):\n    return tf.constant([[0.25, 0.5, 0.75, 1]], dtype=tf.float32)\n\n  def expectedLabelsAfterThresholdingWithMissingScore(self):\n    return tf.constant([2], dtype=tf.float32)\n\n  def testRgbToGrayscale(self):\n    images = self.createTestImages()\n    grayscale_images = preprocessor._rgb_to_grayscale(images)\n    expected_images = tf.image.rgb_to_grayscale(images)\n    with self.test_session() as sess:\n      (grayscale_images, expected_images) = sess.run(\n          [grayscale_images, expected_images])\n      self.assertAllEqual(expected_images, grayscale_images)\n\n  def testNormalizeImage(self):\n    preprocess_options = [(preprocessor.normalize_image, {\n        \'original_minval\': 0,\n        \'original_maxval\': 256,\n        \'target_minval\': -1,\n        \'target_maxval\': 1\n    })]\n    images = self.createTestImages()\n    tensor_dict = {fields.InputDataFields.image: images}\n    tensor_dict = preprocessor.preprocess(tensor_dict, preprocess_options)\n    images = tensor_dict[fields.InputDataFields.image]\n    images_expected = self.expectedImagesAfterNormalization()\n\n    with self.test_session() as sess:\n      (images_, images_expected_) = sess.run(\n          [images, images_expected])\n      images_shape_ = images_.shape\n      images_expected_shape_ = images_expected_.shape\n      expected_shape = [1, 4, 4, 3]\n      self.assertAllEqual(images_expected_shape_, images_shape_)\n      self.assertAllEqual(images_shape_, expected_shape)\n      self.assertAllClose(images_, images_expected_)\n\n  def testRetainBoxesAboveThreshold(self):\n    boxes = self.createTestBoxes()\n    labels = self.createTestLabels()\n    label_scores = self.createTestLabelScores()\n    (retained_boxes, retained_labels,\n     retained_label_scores) = preprocessor.retain_boxes_above_threshold(\n         boxes, labels, label_scores, threshold=0.6)\n    with self.test_session() as sess:\n      (retained_boxes_, retained_labels_, retained_label_scores_,\n       expected_retained_boxes_, expected_retained_labels_,\n       expected_retained_label_scores_) = sess.run([\n           retained_boxes, retained_labels, retained_label_scores,\n           self.expectedBoxesAfterThresholding(),\n           self.expectedLabelsAfterThresholding(),\n           self.expectedLabelScoresAfterThresholding()])\n      self.assertAllClose(\n          retained_boxes_, expected_retained_boxes_)\n      self.assertAllClose(\n          retained_labels_, expected_retained_labels_)\n      self.assertAllClose(\n          retained_label_scores_, expected_retained_label_scores_)\n\n  def testRetainBoxesAboveThresholdWithMasks(self):\n    boxes = self.createTestBoxes()\n    labels = self.createTestLabels()\n    label_scores = self.createTestLabelScores()\n    masks = self.createTestMasks()\n    _, _, _, retained_masks = preprocessor.retain_boxes_above_threshold(\n        boxes, labels, label_scores, masks, threshold=0.6)\n    with self.test_session() as sess:\n      retained_masks_, expected_retained_masks_ = sess.run([\n          retained_masks, self.expectedMasksAfterThresholding()])\n\n      self.assertAllClose(\n          retained_masks_, expected_retained_masks_)\n\n  def testRetainBoxesAboveThresholdWithKeypoints(self):\n    boxes = self.createTestBoxes()\n    labels = self.createTestLabels()\n    label_scores = self.createTestLabelScores()\n    keypoints = self.createTestKeypoints()\n    (_, _, _, retained_keypoints) = preprocessor.retain_boxes_above_threshold(\n        boxes, labels, label_scores, keypoints=keypoints, threshold=0.6)\n    with self.test_session() as sess:\n      (retained_keypoints_,\n       expected_retained_keypoints_) = sess.run([\n           retained_keypoints,\n           self.expectedKeypointsAfterThresholding()])\n\n      self.assertAllClose(\n          retained_keypoints_, expected_retained_keypoints_)\n\n  def testRetainBoxesAboveThresholdWithMissingScore(self):\n    boxes = self.createTestBoxes()\n    labels = self.createTestLabels()\n    label_scores = self.createTestLabelScoresWithMissingScore()\n    (retained_boxes, retained_labels,\n     retained_label_scores) = preprocessor.retain_boxes_above_threshold(\n         boxes, labels, label_scores, threshold=0.6)\n    with self.test_session() as sess:\n      (retained_boxes_, retained_labels_, retained_label_scores_,\n       expected_retained_boxes_, expected_retained_labels_,\n       expected_retained_label_scores_) = sess.run([\n           retained_boxes, retained_labels, retained_label_scores,\n           self.expectedBoxesAfterThresholdingWithMissingScore(),\n           self.expectedLabelsAfterThresholdingWithMissingScore(),\n           self.expectedLabelScoresAfterThresholdingWithMissingScore()])\n      self.assertAllClose(\n          retained_boxes_, expected_retained_boxes_)\n      self.assertAllClose(\n          retained_labels_, expected_retained_labels_)\n      self.assertAllClose(\n          retained_label_scores_, expected_retained_label_scores_)\n\n  def testFlipBoxesLeftRight(self):\n    boxes = self.createTestBoxes()\n    flipped_boxes = preprocessor._flip_boxes_left_right(boxes)\n    expected_boxes = self.expectedBoxesAfterLeftRightFlip()\n    with self.test_session() as sess:\n      flipped_boxes, expected_boxes = sess.run([flipped_boxes, expected_boxes])\n      self.assertAllEqual(flipped_boxes.flatten(), expected_boxes.flatten())\n\n  def testFlipBoxesUpDown(self):\n    boxes = self.createTestBoxes()\n    flipped_boxes = preprocessor._flip_boxes_up_down(boxes)\n    expected_boxes = self.expectedBoxesAfterUpDownFlip()\n    with self.test_session() as sess:\n      flipped_boxes, expected_boxes = sess.run([flipped_boxes, expected_boxes])\n      self.assertAllEqual(flipped_boxes.flatten(), expected_boxes.flatten())\n\n  def testRot90Boxes(self):\n    boxes = self.createTestBoxes()\n    rotated_boxes = preprocessor._rot90_boxes(boxes)\n    expected_boxes = self.expectedBoxesAfterRot90()\n    with self.test_session() as sess:\n      rotated_boxes, expected_boxes = sess.run([rotated_boxes, expected_boxes])\n      self.assertAllEqual(rotated_boxes.flatten(), expected_boxes.flatten())\n\n  def testFlipMasksLeftRight(self):\n    test_mask = self.createTestMasks()\n    flipped_mask = preprocessor._flip_masks_left_right(test_mask)\n    expected_mask = self.expectedMasksAfterLeftRightFlip()\n    with self.test_session() as sess:\n      flipped_mask, expected_mask = sess.run([flipped_mask, expected_mask])\n      self.assertAllEqual(flipped_mask.flatten(), expected_mask.flatten())\n\n  def testFlipMasksUpDown(self):\n    test_mask = self.createTestMasks()\n    flipped_mask = preprocessor._flip_masks_up_down(test_mask)\n    expected_mask = self.expectedMasksAfterUpDownFlip()\n    with self.test_session() as sess:\n      flipped_mask, expected_mask = sess.run([flipped_mask, expected_mask])\n      self.assertAllEqual(flipped_mask.flatten(), expected_mask.flatten())\n\n  def testRot90Masks(self):\n    test_mask = self.createTestMasks()\n    rotated_mask = preprocessor._rot90_masks(test_mask)\n    expected_mask = self.expectedMasksAfterRot90()\n    with self.test_session() as sess:\n      rotated_mask, expected_mask = sess.run([rotated_mask, expected_mask])\n      self.assertAllEqual(rotated_mask.flatten(), expected_mask.flatten())\n\n  def _testPreprocessorCache(self,\n                             preprocess_options,\n                             test_boxes=False,\n                             test_masks=False,\n                             test_keypoints=False,\n                             num_runs=4):\n    cache = preprocessor_cache.PreprocessorCache()\n    images = self.createTestImages()\n    boxes = self.createTestBoxes()\n    classes = self.createTestLabels()\n    masks = self.createTestMasks()\n    keypoints = self.createTestKeypoints()\n    preprocessor_arg_map = preprocessor.get_default_func_arg_map(\n        include_instance_masks=test_masks, include_keypoints=test_keypoints)\n    out = []\n    for i in range(num_runs):\n      tensor_dict = {\n          fields.InputDataFields.image: images,\n      }\n      num_outputs = 1\n      if test_boxes:\n        tensor_dict[fields.InputDataFields.groundtruth_boxes] = boxes\n        tensor_dict[fields.InputDataFields.groundtruth_classes] = classes\n        num_outputs += 1\n      if test_masks:\n        tensor_dict[fields.InputDataFields.groundtruth_instance_masks] = masks\n        num_outputs += 1\n      if test_keypoints:\n        tensor_dict[fields.InputDataFields.groundtruth_keypoints] = keypoints\n        num_outputs += 1\n      out.append(preprocessor.preprocess(\n          tensor_dict, preprocess_options, preprocessor_arg_map, cache))\n\n    with self.test_session() as sess:\n      to_run = []\n      for i in range(num_runs):\n        to_run.append(out[i][fields.InputDataFields.image])\n        if test_boxes:\n          to_run.append(out[i][fields.InputDataFields.groundtruth_boxes])\n        if test_masks:\n          to_run.append(\n              out[i][fields.InputDataFields.groundtruth_instance_masks])\n        if test_keypoints:\n          to_run.append(out[i][fields.InputDataFields.groundtruth_keypoints])\n\n      out_array = sess.run(to_run)\n      for i in range(num_outputs, len(out_array)):\n        self.assertAllClose(out_array[i], out_array[i - num_outputs])\n\n  def testRandomHorizontalFlip(self):\n    preprocess_options = [(preprocessor.random_horizontal_flip, {})]\n    images = self.expectedImagesAfterNormalization()\n    boxes = self.createTestBoxes()\n    tensor_dict = {fields.InputDataFields.image: images,\n                   fields.InputDataFields.groundtruth_boxes: boxes}\n    images_expected1 = self.expectedImagesAfterLeftRightFlip()\n    boxes_expected1 = self.expectedBoxesAfterLeftRightFlip()\n    images_expected2 = images\n    boxes_expected2 = boxes\n    tensor_dict = preprocessor.preprocess(tensor_dict, preprocess_options)\n    images = tensor_dict[fields.InputDataFields.image]\n    boxes = tensor_dict[fields.InputDataFields.groundtruth_boxes]\n\n    boxes_diff1 = tf.squared_difference(boxes, boxes_expected1)\n    boxes_diff2 = tf.squared_difference(boxes, boxes_expected2)\n    boxes_diff = tf.multiply(boxes_diff1, boxes_diff2)\n    boxes_diff_expected = tf.zeros_like(boxes_diff)\n\n    images_diff1 = tf.squared_difference(images, images_expected1)\n    images_diff2 = tf.squared_difference(images, images_expected2)\n    images_diff = tf.multiply(images_diff1, images_diff2)\n    images_diff_expected = tf.zeros_like(images_diff)\n\n    with self.test_session() as sess:\n      (images_diff_, images_diff_expected_, boxes_diff_,\n       boxes_diff_expected_) = sess.run([images_diff, images_diff_expected,\n                                         boxes_diff, boxes_diff_expected])\n      self.assertAllClose(boxes_diff_, boxes_diff_expected_)\n      self.assertAllClose(images_diff_, images_diff_expected_)\n\n  def testRandomHorizontalFlipWithEmptyBoxes(self):\n    preprocess_options = [(preprocessor.random_horizontal_flip, {})]\n    images = self.expectedImagesAfterNormalization()\n    boxes = self.createEmptyTestBoxes()\n    tensor_dict = {fields.InputDataFields.image: images,\n                   fields.InputDataFields.groundtruth_boxes: boxes}\n    images_expected1 = self.expectedImagesAfterLeftRightFlip()\n    boxes_expected = self.createEmptyTestBoxes()\n    images_expected2 = images\n    tensor_dict = preprocessor.preprocess(tensor_dict, preprocess_options)\n    images = tensor_dict[fields.InputDataFields.image]\n    boxes = tensor_dict[fields.InputDataFields.groundtruth_boxes]\n\n    images_diff1 = tf.squared_difference(images, images_expected1)\n    images_diff2 = tf.squared_difference(images, images_expected2)\n    images_diff = tf.multiply(images_diff1, images_diff2)\n    images_diff_expected = tf.zeros_like(images_diff)\n\n    with self.test_session() as sess:\n      (images_diff_, images_diff_expected_, boxes_,\n       boxes_expected_) = sess.run([images_diff, images_diff_expected, boxes,\n                                    boxes_expected])\n      self.assertAllClose(boxes_, boxes_expected_)\n      self.assertAllClose(images_diff_, images_diff_expected_)\n\n  def testRandomHorizontalFlipWithCache(self):\n    keypoint_flip_permutation = self.createKeypointFlipPermutation()\n    preprocess_options = [\n        (preprocessor.random_horizontal_flip,\n         {\'keypoint_flip_permutation\': keypoint_flip_permutation})]\n    self._testPreprocessorCache(preprocess_options,\n                                test_boxes=True,\n                                test_masks=True,\n                                test_keypoints=True)\n\n  def testRunRandomHorizontalFlipWithMaskAndKeypoints(self):\n    preprocess_options = [(preprocessor.random_horizontal_flip, {})]\n    image_height = 3\n    image_width = 3\n    images = tf.random_uniform([1, image_height, image_width, 3])\n    boxes = self.createTestBoxes()\n    masks = self.createTestMasks()\n    keypoints = self.createTestKeypoints()\n    keypoint_flip_permutation = self.createKeypointFlipPermutation()\n    tensor_dict = {\n        fields.InputDataFields.image: images,\n        fields.InputDataFields.groundtruth_boxes: boxes,\n        fields.InputDataFields.groundtruth_instance_masks: masks,\n        fields.InputDataFields.groundtruth_keypoints: keypoints\n    }\n    preprocess_options = [\n        (preprocessor.random_horizontal_flip,\n         {\'keypoint_flip_permutation\': keypoint_flip_permutation})]\n    preprocessor_arg_map = preprocessor.get_default_func_arg_map(\n        include_instance_masks=True, include_keypoints=True)\n    tensor_dict = preprocessor.preprocess(\n        tensor_dict, preprocess_options, func_arg_map=preprocessor_arg_map)\n    boxes = tensor_dict[fields.InputDataFields.groundtruth_boxes]\n    masks = tensor_dict[fields.InputDataFields.groundtruth_instance_masks]\n    keypoints = tensor_dict[fields.InputDataFields.groundtruth_keypoints]\n    with self.test_session() as sess:\n      boxes, masks, keypoints = sess.run([boxes, masks, keypoints])\n      self.assertTrue(boxes is not None)\n      self.assertTrue(masks is not None)\n      self.assertTrue(keypoints is not None)\n\n  def testRandomVerticalFlip(self):\n    preprocess_options = [(preprocessor.random_vertical_flip, {})]\n    images = self.expectedImagesAfterNormalization()\n    boxes = self.createTestBoxes()\n    tensor_dict = {fields.InputDataFields.image: images,\n                   fields.InputDataFields.groundtruth_boxes: boxes}\n    images_expected1 = self.expectedImagesAfterUpDownFlip()\n    boxes_expected1 = self.expectedBoxesAfterUpDownFlip()\n    images_expected2 = images\n    boxes_expected2 = boxes\n    tensor_dict = preprocessor.preprocess(tensor_dict, preprocess_options)\n    images = tensor_dict[fields.InputDataFields.image]\n    boxes = tensor_dict[fields.InputDataFields.groundtruth_boxes]\n\n    boxes_diff1 = tf.squared_difference(boxes, boxes_expected1)\n    boxes_diff2 = tf.squared_difference(boxes, boxes_expected2)\n    boxes_diff = tf.multiply(boxes_diff1, boxes_diff2)\n    boxes_diff_expected = tf.zeros_like(boxes_diff)\n\n    images_diff1 = tf.squared_difference(images, images_expected1)\n    images_diff2 = tf.squared_difference(images, images_expected2)\n    images_diff = tf.multiply(images_diff1, images_diff2)\n    images_diff_expected = tf.zeros_like(images_diff)\n\n    with self.test_session() as sess:\n      (images_diff_, images_diff_expected_, boxes_diff_,\n       boxes_diff_expected_) = sess.run([images_diff, images_diff_expected,\n                                         boxes_diff, boxes_diff_expected])\n      self.assertAllClose(boxes_diff_, boxes_diff_expected_)\n      self.assertAllClose(images_diff_, images_diff_expected_)\n\n  def testRandomVerticalFlipWithEmptyBoxes(self):\n    preprocess_options = [(preprocessor.random_vertical_flip, {})]\n    images = self.expectedImagesAfterNormalization()\n    boxes = self.createEmptyTestBoxes()\n    tensor_dict = {fields.InputDataFields.image: images,\n                   fields.InputDataFields.groundtruth_boxes: boxes}\n    images_expected1 = self.expectedImagesAfterUpDownFlip()\n    boxes_expected = self.createEmptyTestBoxes()\n    images_expected2 = images\n    tensor_dict = preprocessor.preprocess(tensor_dict, preprocess_options)\n    images = tensor_dict[fields.InputDataFields.image]\n    boxes = tensor_dict[fields.InputDataFields.groundtruth_boxes]\n\n    images_diff1 = tf.squared_difference(images, images_expected1)\n    images_diff2 = tf.squared_difference(images, images_expected2)\n    images_diff = tf.multiply(images_diff1, images_diff2)\n    images_diff_expected = tf.zeros_like(images_diff)\n\n    with self.test_session() as sess:\n      (images_diff_, images_diff_expected_, boxes_,\n       boxes_expected_) = sess.run([images_diff, images_diff_expected, boxes,\n                                    boxes_expected])\n      self.assertAllClose(boxes_, boxes_expected_)\n      self.assertAllClose(images_diff_, images_diff_expected_)\n\n  def testRandomVerticalFlipWithCache(self):\n    keypoint_flip_permutation = self.createKeypointFlipPermutation()\n    preprocess_options = [\n        (preprocessor.random_vertical_flip,\n         {\'keypoint_flip_permutation\': keypoint_flip_permutation})]\n    self._testPreprocessorCache(preprocess_options,\n                                test_boxes=True,\n                                test_masks=True,\n                                test_keypoints=True)\n\n  def testRunRandomVerticalFlipWithMaskAndKeypoints(self):\n    preprocess_options = [(preprocessor.random_vertical_flip, {})]\n    image_height = 3\n    image_width = 3\n    images = tf.random_uniform([1, image_height, image_width, 3])\n    boxes = self.createTestBoxes()\n    masks = self.createTestMasks()\n    keypoints = self.createTestKeypoints()\n    keypoint_flip_permutation = self.createKeypointFlipPermutation()\n    tensor_dict = {\n        fields.InputDataFields.image: images,\n        fields.InputDataFields.groundtruth_boxes: boxes,\n        fields.InputDataFields.groundtruth_instance_masks: masks,\n        fields.InputDataFields.groundtruth_keypoints: keypoints\n    }\n    preprocess_options = [\n        (preprocessor.random_vertical_flip,\n         {\'keypoint_flip_permutation\': keypoint_flip_permutation})]\n    preprocessor_arg_map = preprocessor.get_default_func_arg_map(\n        include_instance_masks=True, include_keypoints=True)\n    tensor_dict = preprocessor.preprocess(\n        tensor_dict, preprocess_options, func_arg_map=preprocessor_arg_map)\n    boxes = tensor_dict[fields.InputDataFields.groundtruth_boxes]\n    masks = tensor_dict[fields.InputDataFields.groundtruth_instance_masks]\n    keypoints = tensor_dict[fields.InputDataFields.groundtruth_keypoints]\n    with self.test_session() as sess:\n      boxes, masks, keypoints = sess.run([boxes, masks, keypoints])\n      self.assertTrue(boxes is not None)\n      self.assertTrue(masks is not None)\n      self.assertTrue(keypoints is not None)\n\n  def testRandomRotation90(self):\n    preprocess_options = [(preprocessor.random_rotation90, {})]\n    images = self.expectedImagesAfterNormalization()\n    boxes = self.createTestBoxes()\n    tensor_dict = {fields.InputDataFields.image: images,\n                   fields.InputDataFields.groundtruth_boxes: boxes}\n    images_expected1 = self.expectedImagesAfterRot90()\n    boxes_expected1 = self.expectedBoxesAfterRot90()\n    images_expected2 = images\n    boxes_expected2 = boxes\n    tensor_dict = preprocessor.preprocess(tensor_dict, preprocess_options)\n    images = tensor_dict[fields.InputDataFields.image]\n    boxes = tensor_dict[fields.InputDataFields.groundtruth_boxes]\n\n    boxes_diff1 = tf.squared_difference(boxes, boxes_expected1)\n    boxes_diff2 = tf.squared_difference(boxes, boxes_expected2)\n    boxes_diff = tf.multiply(boxes_diff1, boxes_diff2)\n    boxes_diff_expected = tf.zeros_like(boxes_diff)\n\n    images_diff1 = tf.squared_difference(images, images_expected1)\n    images_diff2 = tf.squared_difference(images, images_expected2)\n    images_diff = tf.multiply(images_diff1, images_diff2)\n    images_diff_expected = tf.zeros_like(images_diff)\n\n    with self.test_session() as sess:\n      (images_diff_, images_diff_expected_, boxes_diff_,\n       boxes_diff_expected_) = sess.run([images_diff, images_diff_expected,\n                                         boxes_diff, boxes_diff_expected])\n      self.assertAllClose(boxes_diff_, boxes_diff_expected_)\n      self.assertAllClose(images_diff_, images_diff_expected_)\n\n  def testRandomRotation90WithEmptyBoxes(self):\n    preprocess_options = [(preprocessor.random_rotation90, {})]\n    images = self.expectedImagesAfterNormalization()\n    boxes = self.createEmptyTestBoxes()\n    tensor_dict = {fields.InputDataFields.image: images,\n                   fields.InputDataFields.groundtruth_boxes: boxes}\n    images_expected1 = self.expectedImagesAfterRot90()\n    boxes_expected = self.createEmptyTestBoxes()\n    images_expected2 = images\n    tensor_dict = preprocessor.preprocess(tensor_dict, preprocess_options)\n    images = tensor_dict[fields.InputDataFields.image]\n    boxes = tensor_dict[fields.InputDataFields.groundtruth_boxes]\n\n    images_diff1 = tf.squared_difference(images, images_expected1)\n    images_diff2 = tf.squared_difference(images, images_expected2)\n    images_diff = tf.multiply(images_diff1, images_diff2)\n    images_diff_expected = tf.zeros_like(images_diff)\n\n    with self.test_session() as sess:\n      (images_diff_, images_diff_expected_, boxes_,\n       boxes_expected_) = sess.run([images_diff, images_diff_expected, boxes,\n                                    boxes_expected])\n      self.assertAllClose(boxes_, boxes_expected_)\n      self.assertAllClose(images_diff_, images_diff_expected_)\n\n  def testRandomRotation90WithCache(self):\n    preprocess_options = [(preprocessor.random_rotation90, {})]\n    self._testPreprocessorCache(preprocess_options,\n                                test_boxes=True,\n                                test_masks=True,\n                                test_keypoints=True)\n\n  def testRunRandomRotation90WithMaskAndKeypoints(self):\n    preprocess_options = [(preprocessor.random_rotation90, {})]\n    image_height = 3\n    image_width = 3\n    images = tf.random_uniform([1, image_height, image_width, 3])\n    boxes = self.createTestBoxes()\n    masks = self.createTestMasks()\n    keypoints = self.createTestKeypoints()\n    tensor_dict = {\n        fields.InputDataFields.image: images,\n        fields.InputDataFields.groundtruth_boxes: boxes,\n        fields.InputDataFields.groundtruth_instance_masks: masks,\n        fields.InputDataFields.groundtruth_keypoints: keypoints\n    }\n    preprocessor_arg_map = preprocessor.get_default_func_arg_map(\n        include_instance_masks=True, include_keypoints=True)\n    tensor_dict = preprocessor.preprocess(\n        tensor_dict, preprocess_options, func_arg_map=preprocessor_arg_map)\n    boxes = tensor_dict[fields.InputDataFields.groundtruth_boxes]\n    masks = tensor_dict[fields.InputDataFields.groundtruth_instance_masks]\n    keypoints = tensor_dict[fields.InputDataFields.groundtruth_keypoints]\n    with self.test_session() as sess:\n      boxes, masks, keypoints = sess.run([boxes, masks, keypoints])\n      self.assertTrue(boxes is not None)\n      self.assertTrue(masks is not None)\n      self.assertTrue(keypoints is not None)\n\n  def testRandomPixelValueScale(self):\n    preprocessing_options = []\n    preprocessing_options.append((preprocessor.normalize_image, {\n        \'original_minval\': 0,\n        \'original_maxval\': 255,\n        \'target_minval\': 0,\n        \'target_maxval\': 1\n    }))\n    preprocessing_options.append((preprocessor.random_pixel_value_scale, {}))\n    images = self.createTestImages()\n    tensor_dict = {fields.InputDataFields.image: images}\n    tensor_dict = preprocessor.preprocess(tensor_dict, preprocessing_options)\n    images_min = tf.to_float(images) * 0.9 / 255.0\n    images_max = tf.to_float(images) * 1.1 / 255.0\n    images = tensor_dict[fields.InputDataFields.image]\n    values_greater = tf.greater_equal(images, images_min)\n    values_less = tf.less_equal(images, images_max)\n    values_true = tf.fill([1, 4, 4, 3], True)\n    with self.test_session() as sess:\n      (values_greater_, values_less_, values_true_) = sess.run(\n          [values_greater, values_less, values_true])\n      self.assertAllClose(values_greater_, values_true_)\n      self.assertAllClose(values_less_, values_true_)\n\n  def testRandomPixelValueScaleWithCache(self):\n    preprocess_options = []\n    preprocess_options.append((preprocessor.normalize_image, {\n        \'original_minval\': 0,\n        \'original_maxval\': 255,\n        \'target_minval\': 0,\n        \'target_maxval\': 1\n    }))\n    preprocess_options.append((preprocessor.random_pixel_value_scale, {}))\n    self._testPreprocessorCache(preprocess_options,\n                                test_boxes=True,\n                                test_masks=False,\n                                test_keypoints=False)\n\n  def testRandomImageScale(self):\n    preprocess_options = [(preprocessor.random_image_scale, {})]\n    images_original = self.createTestImages()\n    tensor_dict = {fields.InputDataFields.image: images_original}\n    tensor_dict = preprocessor.preprocess(tensor_dict, preprocess_options)\n    images_scaled = tensor_dict[fields.InputDataFields.image]\n    images_original_shape = tf.shape(images_original)\n    images_scaled_shape = tf.shape(images_scaled)\n    with self.test_session() as sess:\n      (images_original_shape_, images_scaled_shape_) = sess.run(\n          [images_original_shape, images_scaled_shape])\n      self.assertTrue(\n          images_original_shape_[1] * 0.5 <= images_scaled_shape_[1])\n      self.assertTrue(\n          images_original_shape_[1] * 2.0 >= images_scaled_shape_[1])\n      self.assertTrue(\n          images_original_shape_[2] * 0.5 <= images_scaled_shape_[2])\n      self.assertTrue(\n          images_original_shape_[2] * 2.0 >= images_scaled_shape_[2])\n\n  def testRandomImageScaleWithCache(self):\n    preprocess_options = [(preprocessor.random_image_scale, {})]\n    self._testPreprocessorCache(preprocess_options,\n                                test_boxes=False,\n                                test_masks=False,\n                                test_keypoints=False)\n\n  def testRandomRGBtoGray(self):\n    preprocess_options = [(preprocessor.random_rgb_to_gray, {})]\n    images_original = self.createTestImages()\n    tensor_dict = {fields.InputDataFields.image: images_original}\n    tensor_dict = preprocessor.preprocess(tensor_dict, preprocess_options)\n    images_gray = tensor_dict[fields.InputDataFields.image]\n    images_gray_r, images_gray_g, images_gray_b = tf.split(\n        value=images_gray, num_or_size_splits=3, axis=3)\n    images_r, images_g, images_b = tf.split(\n        value=images_original, num_or_size_splits=3, axis=3)\n    images_r_diff1 = tf.squared_difference(tf.to_float(images_r),\n                                           tf.to_float(images_gray_r))\n    images_r_diff2 = tf.squared_difference(tf.to_float(images_gray_r),\n                                           tf.to_float(images_gray_g))\n    images_r_diff = tf.multiply(images_r_diff1, images_r_diff2)\n    images_g_diff1 = tf.squared_difference(tf.to_float(images_g),\n                                           tf.to_float(images_gray_g))\n    images_g_diff2 = tf.squared_difference(tf.to_float(images_gray_g),\n                                           tf.to_float(images_gray_b))\n    images_g_diff = tf.multiply(images_g_diff1, images_g_diff2)\n    images_b_diff1 = tf.squared_difference(tf.to_float(images_b),\n                                           tf.to_float(images_gray_b))\n    images_b_diff2 = tf.squared_difference(tf.to_float(images_gray_b),\n                                           tf.to_float(images_gray_r))\n    images_b_diff = tf.multiply(images_b_diff1, images_b_diff2)\n    image_zero1 = tf.constant(0, dtype=tf.float32, shape=[1, 4, 4, 1])\n    with self.test_session() as sess:\n      (images_r_diff_, images_g_diff_, images_b_diff_, image_zero1_) = sess.run(\n          [images_r_diff, images_g_diff, images_b_diff, image_zero1])\n      self.assertAllClose(images_r_diff_, image_zero1_)\n      self.assertAllClose(images_g_diff_, image_zero1_)\n      self.assertAllClose(images_b_diff_, image_zero1_)\n\n  def testRandomRGBtoGrayWithCache(self):\n    preprocess_options = [(\n        preprocessor.random_rgb_to_gray, {\'probability\': 0.5})]\n    self._testPreprocessorCache(preprocess_options,\n                                test_boxes=False,\n                                test_masks=False,\n                                test_keypoints=False)\n\n  def testRandomAdjustBrightness(self):\n    preprocessing_options = []\n    preprocessing_options.append((preprocessor.normalize_image, {\n        \'original_minval\': 0,\n        \'original_maxval\': 255,\n        \'target_minval\': 0,\n        \'target_maxval\': 1\n    }))\n    preprocessing_options.append((preprocessor.random_adjust_brightness, {}))\n    images_original = self.createTestImages()\n    tensor_dict = {fields.InputDataFields.image: images_original}\n    tensor_dict = preprocessor.preprocess(tensor_dict, preprocessing_options)\n    images_bright = tensor_dict[fields.InputDataFields.image]\n    image_original_shape = tf.shape(images_original)\n    image_bright_shape = tf.shape(images_bright)\n    with self.test_session() as sess:\n      (image_original_shape_, image_bright_shape_) = sess.run(\n          [image_original_shape, image_bright_shape])\n      self.assertAllEqual(image_original_shape_, image_bright_shape_)\n\n  def testRandomAdjustBrightnessWithCache(self):\n    preprocess_options = []\n    preprocess_options.append((preprocessor.normalize_image, {\n        \'original_minval\': 0,\n        \'original_maxval\': 255,\n        \'target_minval\': 0,\n        \'target_maxval\': 1\n    }))\n    preprocess_options.append((preprocessor.random_adjust_brightness, {}))\n    self._testPreprocessorCache(preprocess_options,\n                                test_boxes=False,\n                                test_masks=False,\n                                test_keypoints=False)\n\n  def testRandomAdjustContrast(self):\n    preprocessing_options = []\n    preprocessing_options.append((preprocessor.normalize_image, {\n        \'original_minval\': 0,\n        \'original_maxval\': 255,\n        \'target_minval\': 0,\n        \'target_maxval\': 1\n    }))\n    preprocessing_options.append((preprocessor.random_adjust_contrast, {}))\n    images_original = self.createTestImages()\n    tensor_dict = {fields.InputDataFields.image: images_original}\n    tensor_dict = preprocessor.preprocess(tensor_dict, preprocessing_options)\n    images_contrast = tensor_dict[fields.InputDataFields.image]\n    image_original_shape = tf.shape(images_original)\n    image_contrast_shape = tf.shape(images_contrast)\n    with self.test_session() as sess:\n      (image_original_shape_, image_contrast_shape_) = sess.run(\n          [image_original_shape, image_contrast_shape])\n      self.assertAllEqual(image_original_shape_, image_contrast_shape_)\n\n  def testRandomAdjustContrastWithCache(self):\n    preprocess_options = []\n    preprocess_options.append((preprocessor.normalize_image, {\n        \'original_minval\': 0,\n        \'original_maxval\': 255,\n        \'target_minval\': 0,\n        \'target_maxval\': 1\n    }))\n    preprocess_options.append((preprocessor.random_adjust_contrast, {}))\n    self._testPreprocessorCache(preprocess_options,\n                                test_boxes=False,\n                                test_masks=False,\n                                test_keypoints=False)\n\n  def testRandomAdjustHue(self):\n    preprocessing_options = []\n    preprocessing_options.append((preprocessor.normalize_image, {\n        \'original_minval\': 0,\n        \'original_maxval\': 255,\n        \'target_minval\': 0,\n        \'target_maxval\': 1\n    }))\n    preprocessing_options.append((preprocessor.random_adjust_hue, {}))\n    images_original = self.createTestImages()\n    tensor_dict = {fields.InputDataFields.image: images_original}\n    tensor_dict = preprocessor.preprocess(tensor_dict, preprocessing_options)\n    images_hue = tensor_dict[fields.InputDataFields.image]\n    image_original_shape = tf.shape(images_original)\n    image_hue_shape = tf.shape(images_hue)\n    with self.test_session() as sess:\n      (image_original_shape_, image_hue_shape_) = sess.run(\n          [image_original_shape, image_hue_shape])\n      self.assertAllEqual(image_original_shape_, image_hue_shape_)\n\n  def testRandomAdjustHueWithCache(self):\n    preprocess_options = []\n    preprocess_options.append((preprocessor.normalize_image, {\n        \'original_minval\': 0,\n        \'original_maxval\': 255,\n        \'target_minval\': 0,\n        \'target_maxval\': 1\n    }))\n    preprocess_options.append((preprocessor.random_adjust_hue, {}))\n    self._testPreprocessorCache(preprocess_options,\n                                test_boxes=False,\n                                test_masks=False,\n                                test_keypoints=False)\n\n  def testRandomDistortColor(self):\n    preprocessing_options = []\n    preprocessing_options.append((preprocessor.normalize_image, {\n        \'original_minval\': 0,\n        \'original_maxval\': 255,\n        \'target_minval\': 0,\n        \'target_maxval\': 1\n    }))\n    preprocessing_options.append((preprocessor.random_distort_color, {}))\n    images_original = self.createTestImages()\n    images_original_shape = tf.shape(images_original)\n    tensor_dict = {fields.InputDataFields.image: images_original}\n    tensor_dict = preprocessor.preprocess(tensor_dict, preprocessing_options)\n    images_distorted_color = tensor_dict[fields.InputDataFields.image]\n    images_distorted_color_shape = tf.shape(images_distorted_color)\n    with self.test_session() as sess:\n      (images_original_shape_, images_distorted_color_shape_) = sess.run(\n          [images_original_shape, images_distorted_color_shape])\n      self.assertAllEqual(images_original_shape_, images_distorted_color_shape_)\n\n  def testRandomDistortColorWithCache(self):\n    preprocess_options = []\n    preprocess_options.append((preprocessor.normalize_image, {\n        \'original_minval\': 0,\n        \'original_maxval\': 255,\n        \'target_minval\': 0,\n        \'target_maxval\': 1\n    }))\n    preprocess_options.append((preprocessor.random_distort_color, {}))\n    self._testPreprocessorCache(preprocess_options,\n                                test_boxes=False,\n                                test_masks=False,\n                                test_keypoints=False)\n\n  def testRandomJitterBoxes(self):\n    preprocessing_options = []\n    preprocessing_options.append((preprocessor.random_jitter_boxes, {}))\n    boxes = self.createTestBoxes()\n    boxes_shape = tf.shape(boxes)\n    tensor_dict = {fields.InputDataFields.groundtruth_boxes: boxes}\n    tensor_dict = preprocessor.preprocess(tensor_dict, preprocessing_options)\n    distorted_boxes = tensor_dict[fields.InputDataFields.groundtruth_boxes]\n    distorted_boxes_shape = tf.shape(distorted_boxes)\n\n    with self.test_session() as sess:\n      (boxes_shape_, distorted_boxes_shape_) = sess.run(\n          [boxes_shape, distorted_boxes_shape])\n      self.assertAllEqual(boxes_shape_, distorted_boxes_shape_)\n\n  def testRandomCropImage(self):\n    preprocessing_options = []\n    preprocessing_options.append((preprocessor.normalize_image, {\n        \'original_minval\': 0,\n        \'original_maxval\': 255,\n        \'target_minval\': 0,\n        \'target_maxval\': 1\n    }))\n    preprocessing_options.append((preprocessor.random_crop_image, {}))\n    images = self.createTestImages()\n    boxes = self.createTestBoxes()\n    labels = self.createTestLabels()\n    tensor_dict = {\n        fields.InputDataFields.image: images,\n        fields.InputDataFields.groundtruth_boxes: boxes,\n        fields.InputDataFields.groundtruth_classes: labels,\n    }\n    distorted_tensor_dict = preprocessor.preprocess(tensor_dict,\n                                                    preprocessing_options)\n    distorted_images = distorted_tensor_dict[fields.InputDataFields.image]\n    distorted_boxes = distorted_tensor_dict[\n        fields.InputDataFields.groundtruth_boxes]\n    boxes_rank = tf.rank(boxes)\n    distorted_boxes_rank = tf.rank(distorted_boxes)\n    images_rank = tf.rank(images)\n    distorted_images_rank = tf.rank(distorted_images)\n    self.assertEqual(3, distorted_images.get_shape()[3])\n\n    with self.test_session() as sess:\n      (boxes_rank_, distorted_boxes_rank_, images_rank_,\n       distorted_images_rank_) = sess.run([\n           boxes_rank, distorted_boxes_rank, images_rank, distorted_images_rank\n       ])\n      self.assertAllEqual(boxes_rank_, distorted_boxes_rank_)\n      self.assertAllEqual(images_rank_, distorted_images_rank_)\n\n  def testRandomCropImageWithCache(self):\n    preprocess_options = [(preprocessor.random_rgb_to_gray,\n                           {\'probability\': 0.5}),\n                          (preprocessor.normalize_image, {\n                              \'original_minval\': 0,\n                              \'original_maxval\': 255,\n                              \'target_minval\': 0,\n                              \'target_maxval\': 1,\n                          }),\n                          (preprocessor.random_crop_image, {})]\n    self._testPreprocessorCache(preprocess_options,\n                                test_boxes=True,\n                                test_masks=False,\n                                test_keypoints=False)\n\n  def testRandomCropImageGrayscale(self):\n    preprocessing_options = [(preprocessor.rgb_to_gray, {}),\n                             (preprocessor.normalize_image, {\n                                 \'original_minval\': 0,\n                                 \'original_maxval\': 255,\n                                 \'target_minval\': 0,\n                                 \'target_maxval\': 1,\n                             }),\n                             (preprocessor.random_crop_image, {})]\n    images = self.createTestImages()\n    boxes = self.createTestBoxes()\n    labels = self.createTestLabels()\n    tensor_dict = {\n        fields.InputDataFields.image: images,\n        fields.InputDataFields.groundtruth_boxes: boxes,\n        fields.InputDataFields.groundtruth_classes: labels,\n    }\n    distorted_tensor_dict = preprocessor.preprocess(\n        tensor_dict, preprocessing_options)\n    distorted_images = distorted_tensor_dict[fields.InputDataFields.image]\n    distorted_boxes = distorted_tensor_dict[\n        fields.InputDataFields.groundtruth_boxes]\n    boxes_rank = tf.rank(boxes)\n    distorted_boxes_rank = tf.rank(distorted_boxes)\n    images_rank = tf.rank(images)\n    distorted_images_rank = tf.rank(distorted_images)\n    self.assertEqual(1, distorted_images.get_shape()[3])\n\n    with self.test_session() as sess:\n      session_results = sess.run([\n          boxes_rank, distorted_boxes_rank, images_rank, distorted_images_rank\n      ])\n      (boxes_rank_, distorted_boxes_rank_, images_rank_,\n       distorted_images_rank_) = session_results\n      self.assertAllEqual(boxes_rank_, distorted_boxes_rank_)\n      self.assertAllEqual(images_rank_, distorted_images_rank_)\n\n  def testRandomCropImageWithBoxOutOfImage(self):\n    preprocessing_options = []\n    preprocessing_options.append((preprocessor.normalize_image, {\n        \'original_minval\': 0,\n        \'original_maxval\': 255,\n        \'target_minval\': 0,\n        \'target_maxval\': 1\n    }))\n    preprocessing_options.append((preprocessor.random_crop_image, {}))\n    images = self.createTestImages()\n    boxes = self.createTestBoxesOutOfImage()\n    labels = self.createTestLabels()\n    tensor_dict = {\n        fields.InputDataFields.image: images,\n        fields.InputDataFields.groundtruth_boxes: boxes,\n        fields.InputDataFields.groundtruth_classes: labels,\n        }\n    distorted_tensor_dict = preprocessor.preprocess(tensor_dict,\n                                                    preprocessing_options)\n    distorted_images = distorted_tensor_dict[fields.InputDataFields.image]\n    distorted_boxes = distorted_tensor_dict[\n        fields.InputDataFields.groundtruth_boxes]\n    boxes_rank = tf.rank(boxes)\n    distorted_boxes_rank = tf.rank(distorted_boxes)\n    images_rank = tf.rank(images)\n    distorted_images_rank = tf.rank(distorted_images)\n\n    with self.test_session() as sess:\n      (boxes_rank_, distorted_boxes_rank_, images_rank_,\n       distorted_images_rank_) = sess.run(\n           [boxes_rank, distorted_boxes_rank, images_rank,\n            distorted_images_rank])\n      self.assertAllEqual(boxes_rank_, distorted_boxes_rank_)\n      self.assertAllEqual(images_rank_, distorted_images_rank_)\n\n  def testRandomCropImageWithRandomCoefOne(self):\n    preprocessing_options = [(preprocessor.normalize_image, {\n        \'original_minval\': 0,\n        \'original_maxval\': 255,\n        \'target_minval\': 0,\n        \'target_maxval\': 1\n    })]\n\n    images = self.createTestImages()\n    boxes = self.createTestBoxes()\n    labels = self.createTestLabels()\n    label_scores = self.createTestLabelScores()\n    tensor_dict = {\n        fields.InputDataFields.image: images,\n        fields.InputDataFields.groundtruth_boxes: boxes,\n        fields.InputDataFields.groundtruth_classes: labels,\n        fields.InputDataFields.groundtruth_label_scores: label_scores\n    }\n    tensor_dict = preprocessor.preprocess(tensor_dict, preprocessing_options)\n    images = tensor_dict[fields.InputDataFields.image]\n\n    preprocessing_options = [(preprocessor.random_crop_image, {\n        \'random_coef\': 1.0\n    })]\n    distorted_tensor_dict = preprocessor.preprocess(tensor_dict,\n                                                    preprocessing_options)\n\n    distorted_images = distorted_tensor_dict[fields.InputDataFields.image]\n    distorted_boxes = distorted_tensor_dict[\n        fields.InputDataFields.groundtruth_boxes]\n    distorted_labels = distorted_tensor_dict[\n        fields.InputDataFields.groundtruth_classes]\n    distorted_label_scores = distorted_tensor_dict[\n        fields.InputDataFields.groundtruth_label_scores]\n    boxes_shape = tf.shape(boxes)\n    distorted_boxes_shape = tf.shape(distorted_boxes)\n    images_shape = tf.shape(images)\n    distorted_images_shape = tf.shape(distorted_images)\n\n    with self.test_session() as sess:\n      (boxes_shape_, distorted_boxes_shape_, images_shape_,\n       distorted_images_shape_, images_, distorted_images_,\n       boxes_, distorted_boxes_, labels_, distorted_labels_,\n       label_scores_, distorted_label_scores_) = sess.run(\n           [boxes_shape, distorted_boxes_shape, images_shape,\n            distorted_images_shape, images, distorted_images,\n            boxes, distorted_boxes, labels, distorted_labels,\n            label_scores, distorted_label_scores])\n      self.assertAllEqual(boxes_shape_, distorted_boxes_shape_)\n      self.assertAllEqual(images_shape_, distorted_images_shape_)\n      self.assertAllClose(images_, distorted_images_)\n      self.assertAllClose(boxes_, distorted_boxes_)\n      self.assertAllEqual(labels_, distorted_labels_)\n      self.assertAllEqual(label_scores_, distorted_label_scores_)\n\n  def testRandomCropWithMockSampleDistortedBoundingBox(self):\n    preprocessing_options = [(preprocessor.normalize_image, {\n        \'original_minval\': 0,\n        \'original_maxval\': 255,\n        \'target_minval\': 0,\n        \'target_maxval\': 1\n    })]\n\n    images = self.createColorfulTestImage()\n    boxes = tf.constant([[0.1, 0.1, 0.8, 0.3],\n                         [0.2, 0.4, 0.75, 0.75],\n                         [0.3, 0.1, 0.4, 0.7]], dtype=tf.float32)\n    labels = tf.constant([1, 7, 11], dtype=tf.int32)\n\n    tensor_dict = {\n        fields.InputDataFields.image: images,\n        fields.InputDataFields.groundtruth_boxes: boxes,\n        fields.InputDataFields.groundtruth_classes: labels,\n    }\n    tensor_dict = preprocessor.preprocess(tensor_dict, preprocessing_options)\n    images = tensor_dict[fields.InputDataFields.image]\n\n    preprocessing_options = [(preprocessor.random_crop_image, {})]\n    with mock.patch.object(\n        tf.image,\n        \'sample_distorted_bounding_box\') as mock_sample_distorted_bounding_box:\n      mock_sample_distorted_bounding_box.return_value = (tf.constant(\n          [6, 143, 0], dtype=tf.int32), tf.constant(\n              [190, 237, -1], dtype=tf.int32), tf.constant(\n                  [[[0.03, 0.3575, 0.98, 0.95]]], dtype=tf.float32))\n\n      distorted_tensor_dict = preprocessor.preprocess(tensor_dict,\n                                                      preprocessing_options)\n\n      distorted_boxes = distorted_tensor_dict[\n          fields.InputDataFields.groundtruth_boxes]\n      distorted_labels = distorted_tensor_dict[\n          fields.InputDataFields.groundtruth_classes]\n      expected_boxes = tf.constant([[0.178947, 0.07173, 0.75789469, 0.66244733],\n                                    [0.28421, 0.0, 0.38947365, 0.57805908]],\n                                   dtype=tf.float32)\n      expected_labels = tf.constant([7, 11], dtype=tf.int32)\n\n      with self.test_session() as sess:\n        (distorted_boxes_, distorted_labels_,\n         expected_boxes_, expected_labels_) = sess.run(\n             [distorted_boxes, distorted_labels,\n              expected_boxes, expected_labels])\n        self.assertAllClose(distorted_boxes_, expected_boxes_)\n        self.assertAllEqual(distorted_labels_, expected_labels_)\n\n  def testStrictRandomCropImageWithLabelScores(self):\n    image = self.createColorfulTestImage()[0]\n    boxes = self.createTestBoxes()\n    labels = self.createTestLabels()\n    label_scores = self.createTestLabelScores()\n    with mock.patch.object(\n        tf.image,\n        \'sample_distorted_bounding_box\'\n    ) as mock_sample_distorted_bounding_box:\n      mock_sample_distorted_bounding_box.return_value = (\n          tf.constant([6, 143, 0], dtype=tf.int32),\n          tf.constant([190, 237, -1], dtype=tf.int32),\n          tf.constant([[[0.03, 0.3575, 0.98, 0.95]]], dtype=tf.float32))\n      new_image, new_boxes, new_labels, new_label_scores = (\n          preprocessor._strict_random_crop_image(\n              image, boxes, labels, label_scores))\n      with self.test_session() as sess:\n        new_image, new_boxes, new_labels, new_label_scores = (\n            sess.run(\n                [new_image, new_boxes, new_labels, new_label_scores])\n        )\n\n        expected_boxes = np.array(\n            [[0.0, 0.0, 0.75789469, 1.0],\n             [0.23157893, 0.24050637, 0.75789469, 1.0]], dtype=np.float32)\n        self.assertAllEqual(new_image.shape, [190, 237, 3])\n        self.assertAllEqual(new_label_scores, [1.0, 0.5])\n        self.assertAllClose(\n            new_boxes.flatten(), expected_boxes.flatten())\n\n  def testStrictRandomCropImageWithMasks(self):\n    image = self.createColorfulTestImage()[0]\n    boxes = self.createTestBoxes()\n    labels = self.createTestLabels()\n    masks = tf.random_uniform([2, 200, 400], dtype=tf.float32)\n    with mock.patch.object(\n        tf.image,\n        \'sample_distorted_bounding_box\'\n    ) as mock_sample_distorted_bounding_box:\n      mock_sample_distorted_bounding_box.return_value = (\n          tf.constant([6, 143, 0], dtype=tf.int32),\n          tf.constant([190, 237, -1], dtype=tf.int32),\n          tf.constant([[[0.03, 0.3575, 0.98, 0.95]]], dtype=tf.float32))\n      new_image, new_boxes, new_labels, new_masks = (\n          preprocessor._strict_random_crop_image(\n              image, boxes, labels, masks=masks))\n      with self.test_session() as sess:\n        new_image, new_boxes, new_labels, new_masks = sess.run(\n            [new_image, new_boxes, new_labels, new_masks])\n        expected_boxes = np.array(\n            [[0.0, 0.0, 0.75789469, 1.0],\n             [0.23157893, 0.24050637, 0.75789469, 1.0]], dtype=np.float32)\n        self.assertAllEqual(new_image.shape, [190, 237, 3])\n        self.assertAllEqual(new_masks.shape, [2, 190, 237])\n        self.assertAllClose(\n            new_boxes.flatten(), expected_boxes.flatten())\n\n  def testStrictRandomCropImageWithKeypoints(self):\n    image = self.createColorfulTestImage()[0]\n    boxes = self.createTestBoxes()\n    labels = self.createTestLabels()\n    keypoints = self.createTestKeypoints()\n    with mock.patch.object(\n        tf.image,\n        \'sample_distorted_bounding_box\'\n    ) as mock_sample_distorted_bounding_box:\n      mock_sample_distorted_bounding_box.return_value = (\n          tf.constant([6, 143, 0], dtype=tf.int32),\n          tf.constant([190, 237, -1], dtype=tf.int32),\n          tf.constant([[[0.03, 0.3575, 0.98, 0.95]]], dtype=tf.float32))\n      new_image, new_boxes, new_labels, new_keypoints = (\n          preprocessor._strict_random_crop_image(\n              image, boxes, labels, keypoints=keypoints))\n      with self.test_session() as sess:\n        new_image, new_boxes, new_labels, new_keypoints = sess.run(\n            [new_image, new_boxes, new_labels, new_keypoints])\n\n        expected_boxes = np.array([\n            [0.0, 0.0, 0.75789469, 1.0],\n            [0.23157893, 0.24050637, 0.75789469, 1.0],], dtype=np.float32)\n        expected_keypoints = np.array([\n            [[np.nan, np.nan],\n             [np.nan, np.nan],\n             [np.nan, np.nan]],\n            [[0.38947368, 0.07173],\n             [0.49473682, 0.24050637],\n             [0.60000002, 0.40928277]]\n        ], dtype=np.float32)\n        self.assertAllEqual(new_image.shape, [190, 237, 3])\n        self.assertAllClose(\n            new_boxes.flatten(), expected_boxes.flatten())\n        self.assertAllClose(\n            new_keypoints.flatten(), expected_keypoints.flatten())\n\n  def testRunRandomCropImageWithMasks(self):\n    image = self.createColorfulTestImage()\n    boxes = self.createTestBoxes()\n    labels = self.createTestLabels()\n    masks = tf.random_uniform([2, 200, 400], dtype=tf.float32)\n\n    tensor_dict = {\n        fields.InputDataFields.image: image,\n        fields.InputDataFields.groundtruth_boxes: boxes,\n        fields.InputDataFields.groundtruth_classes: labels,\n        fields.InputDataFields.groundtruth_instance_masks: masks,\n    }\n\n    preprocessor_arg_map = preprocessor.get_default_func_arg_map(\n        include_instance_masks=True)\n\n    preprocessing_options = [(preprocessor.random_crop_image, {})]\n\n    with mock.patch.object(\n        tf.image,\n        \'sample_distorted_bounding_box\'\n    ) as mock_sample_distorted_bounding_box:\n      mock_sample_distorted_bounding_box.return_value = (\n          tf.constant([6, 143, 0], dtype=tf.int32),\n          tf.constant([190, 237, -1], dtype=tf.int32),\n          tf.constant([[[0.03, 0.3575, 0.98, 0.95]]], dtype=tf.float32))\n      distorted_tensor_dict = preprocessor.preprocess(\n          tensor_dict, preprocessing_options, func_arg_map=preprocessor_arg_map)\n      distorted_image = distorted_tensor_dict[fields.InputDataFields.image]\n      distorted_boxes = distorted_tensor_dict[\n          fields.InputDataFields.groundtruth_boxes]\n      distorted_labels = distorted_tensor_dict[\n          fields.InputDataFields.groundtruth_classes]\n      distorted_masks = distorted_tensor_dict[\n          fields.InputDataFields.groundtruth_instance_masks]\n      with self.test_session() as sess:\n        (distorted_image_, distorted_boxes_, distorted_labels_,\n         distorted_masks_) = sess.run(\n             [distorted_image, distorted_boxes, distorted_labels,\n              distorted_masks])\n\n        expected_boxes = np.array([\n            [0.0, 0.0, 0.75789469, 1.0],\n            [0.23157893, 0.24050637, 0.75789469, 1.0],\n        ], dtype=np.float32)\n        self.assertAllEqual(distorted_image_.shape, [1, 190, 237, 3])\n        self.assertAllEqual(distorted_masks_.shape, [2, 190, 237])\n        self.assertAllEqual(distorted_labels_, [1, 2])\n        self.assertAllClose(\n            distorted_boxes_.flatten(), expected_boxes.flatten())\n\n  def testRunRandomCropImageWithKeypointsInsideCrop(self):\n    image = self.createColorfulTestImage()\n    boxes = self.createTestBoxes()\n    labels = self.createTestLabels()\n    keypoints = self.createTestKeypointsInsideCrop()\n\n    tensor_dict = {\n        fields.InputDataFields.image: image,\n        fields.InputDataFields.groundtruth_boxes: boxes,\n        fields.InputDataFields.groundtruth_classes: labels,\n        fields.InputDataFields.groundtruth_keypoints: keypoints\n    }\n\n    preprocessor_arg_map = preprocessor.get_default_func_arg_map(\n        include_keypoints=True)\n\n    preprocessing_options = [(preprocessor.random_crop_image, {})]\n\n    with mock.patch.object(\n        tf.image,\n        \'sample_distorted_bounding_box\'\n    ) as mock_sample_distorted_bounding_box:\n      mock_sample_distorted_bounding_box.return_value = (\n          tf.constant([6, 143, 0], dtype=tf.int32),\n          tf.constant([190, 237, -1], dtype=tf.int32),\n          tf.constant([[[0.03, 0.3575, 0.98, 0.95]]], dtype=tf.float32))\n      distorted_tensor_dict = preprocessor.preprocess(\n          tensor_dict, preprocessing_options, func_arg_map=preprocessor_arg_map)\n      distorted_image = distorted_tensor_dict[fields.InputDataFields.image]\n      distorted_boxes = distorted_tensor_dict[\n          fields.InputDataFields.groundtruth_boxes]\n      distorted_labels = distorted_tensor_dict[\n          fields.InputDataFields.groundtruth_classes]\n      distorted_keypoints = distorted_tensor_dict[\n          fields.InputDataFields.groundtruth_keypoints]\n      with self.test_session() as sess:\n        (distorted_image_, distorted_boxes_, distorted_labels_,\n         distorted_keypoints_) = sess.run(\n             [distorted_image, distorted_boxes, distorted_labels,\n              distorted_keypoints])\n\n        expected_boxes = np.array([\n            [0.0, 0.0, 0.75789469, 1.0],\n            [0.23157893, 0.24050637, 0.75789469, 1.0],\n        ], dtype=np.float32)\n        expected_keypoints = np.array([\n            [[0.38947368, 0.07173],\n             [0.49473682, 0.24050637],\n             [0.60000002, 0.40928277]],\n            [[0.38947368, 0.07173],\n             [0.49473682, 0.24050637],\n             [0.60000002, 0.40928277]]\n        ])\n        self.assertAllEqual(distorted_image_.shape, [1, 190, 237, 3])\n        self.assertAllEqual(distorted_labels_, [1, 2])\n        self.assertAllClose(\n            distorted_boxes_.flatten(), expected_boxes.flatten())\n        self.assertAllClose(\n            distorted_keypoints_.flatten(), expected_keypoints.flatten())\n\n  def testRunRandomCropImageWithKeypointsOutsideCrop(self):\n    image = self.createColorfulTestImage()\n    boxes = self.createTestBoxes()\n    labels = self.createTestLabels()\n    keypoints = self.createTestKeypointsOutsideCrop()\n\n    tensor_dict = {\n        fields.InputDataFields.image: image,\n        fields.InputDataFields.groundtruth_boxes: boxes,\n        fields.InputDataFields.groundtruth_classes: labels,\n        fields.InputDataFields.groundtruth_keypoints: keypoints\n    }\n\n    preprocessor_arg_map = preprocessor.get_default_func_arg_map(\n        include_keypoints=True)\n\n    preprocessing_options = [(preprocessor.random_crop_image, {})]\n\n    with mock.patch.object(\n        tf.image,\n        \'sample_distorted_bounding_box\'\n    ) as mock_sample_distorted_bounding_box:\n      mock_sample_distorted_bounding_box.return_value = (\n          tf.constant([6, 143, 0], dtype=tf.int32),\n          tf.constant([190, 237, -1], dtype=tf.int32),\n          tf.constant([[[0.03, 0.3575, 0.98, 0.95]]], dtype=tf.float32))\n      distorted_tensor_dict = preprocessor.preprocess(\n          tensor_dict, preprocessing_options, func_arg_map=preprocessor_arg_map)\n      distorted_image = distorted_tensor_dict[fields.InputDataFields.image]\n      distorted_boxes = distorted_tensor_dict[\n          fields.InputDataFields.groundtruth_boxes]\n      distorted_labels = distorted_tensor_dict[\n          fields.InputDataFields.groundtruth_classes]\n      distorted_keypoints = distorted_tensor_dict[\n          fields.InputDataFields.groundtruth_keypoints]\n      with self.test_session() as sess:\n        (distorted_image_, distorted_boxes_, distorted_labels_,\n         distorted_keypoints_) = sess.run(\n             [distorted_image, distorted_boxes, distorted_labels,\n              distorted_keypoints])\n\n        expected_boxes = np.array([\n            [0.0, 0.0, 0.75789469, 1.0],\n            [0.23157893, 0.24050637, 0.75789469, 1.0],\n        ], dtype=np.float32)\n        expected_keypoints = np.array([\n            [[np.nan, np.nan],\n             [np.nan, np.nan],\n             [np.nan, np.nan]],\n            [[np.nan, np.nan],\n             [np.nan, np.nan],\n             [np.nan, np.nan]],\n        ])\n        self.assertAllEqual(distorted_image_.shape, [1, 190, 237, 3])\n        self.assertAllEqual(distorted_labels_, [1, 2])\n        self.assertAllClose(\n            distorted_boxes_.flatten(), expected_boxes.flatten())\n        self.assertAllClose(\n            distorted_keypoints_.flatten(), expected_keypoints.flatten())\n\n  def testRunRetainBoxesAboveThreshold(self):\n    boxes = self.createTestBoxes()\n    labels = self.createTestLabels()\n    label_scores = self.createTestLabelScores()\n\n    tensor_dict = {\n        fields.InputDataFields.groundtruth_boxes: boxes,\n        fields.InputDataFields.groundtruth_classes: labels,\n        fields.InputDataFields.groundtruth_label_scores: label_scores\n    }\n\n    preprocessing_options = [\n        (preprocessor.retain_boxes_above_threshold, {\'threshold\': 0.6})\n    ]\n    preprocessor_arg_map = preprocessor.get_default_func_arg_map(\n        include_label_scores=True)\n    retained_tensor_dict = preprocessor.preprocess(\n        tensor_dict, preprocessing_options, func_arg_map=preprocessor_arg_map)\n    retained_boxes = retained_tensor_dict[\n        fields.InputDataFields.groundtruth_boxes]\n    retained_labels = retained_tensor_dict[\n        fields.InputDataFields.groundtruth_classes]\n    retained_label_scores = retained_tensor_dict[\n        fields.InputDataFields.groundtruth_label_scores]\n\n    with self.test_session() as sess:\n      (retained_boxes_, retained_labels_,\n       retained_label_scores_, expected_retained_boxes_,\n       expected_retained_labels_, expected_retained_label_scores_) = sess.run(\n           [retained_boxes, retained_labels, retained_label_scores,\n            self.expectedBoxesAfterThresholding(),\n            self.expectedLabelsAfterThresholding(),\n            self.expectedLabelScoresAfterThresholding()])\n\n      self.assertAllClose(retained_boxes_, expected_retained_boxes_)\n      self.assertAllClose(retained_labels_, expected_retained_labels_)\n      self.assertAllClose(\n          retained_label_scores_, expected_retained_label_scores_)\n\n  def testRunRetainBoxesAboveThresholdWithMasks(self):\n    boxes = self.createTestBoxes()\n    labels = self.createTestLabels()\n    label_scores = self.createTestLabelScores()\n    masks = self.createTestMasks()\n\n    tensor_dict = {\n        fields.InputDataFields.groundtruth_boxes: boxes,\n        fields.InputDataFields.groundtruth_classes: labels,\n        fields.InputDataFields.groundtruth_label_scores: label_scores,\n        fields.InputDataFields.groundtruth_instance_masks: masks\n    }\n\n    preprocessor_arg_map = preprocessor.get_default_func_arg_map(\n        include_label_scores=True,\n        include_instance_masks=True)\n\n    preprocessing_options = [\n        (preprocessor.retain_boxes_above_threshold, {\'threshold\': 0.6})\n    ]\n\n    retained_tensor_dict = preprocessor.preprocess(\n        tensor_dict, preprocessing_options, func_arg_map=preprocessor_arg_map)\n    retained_masks = retained_tensor_dict[\n        fields.InputDataFields.groundtruth_instance_masks]\n\n    with self.test_session() as sess:\n      (retained_masks_, expected_masks_) = sess.run(\n          [retained_masks,\n           self.expectedMasksAfterThresholding()])\n      self.assertAllClose(retained_masks_, expected_masks_)\n\n  def testRunRetainBoxesAboveThresholdWithKeypoints(self):\n    boxes = self.createTestBoxes()\n    labels = self.createTestLabels()\n    label_scores = self.createTestLabelScores()\n    keypoints = self.createTestKeypoints()\n\n    tensor_dict = {\n        fields.InputDataFields.groundtruth_boxes: boxes,\n        fields.InputDataFields.groundtruth_classes: labels,\n        fields.InputDataFields.groundtruth_label_scores: label_scores,\n        fields.InputDataFields.groundtruth_keypoints: keypoints\n    }\n\n    preprocessor_arg_map = preprocessor.get_default_func_arg_map(\n        include_label_scores=True,\n        include_keypoints=True)\n\n    preprocessing_options = [\n        (preprocessor.retain_boxes_above_threshold, {\'threshold\': 0.6})\n    ]\n\n    retained_tensor_dict = preprocessor.preprocess(\n        tensor_dict, preprocessing_options, func_arg_map=preprocessor_arg_map)\n    retained_keypoints = retained_tensor_dict[\n        fields.InputDataFields.groundtruth_keypoints]\n\n    with self.test_session() as sess:\n      (retained_keypoints_, expected_keypoints_) = sess.run(\n          [retained_keypoints,\n           self.expectedKeypointsAfterThresholding()])\n      self.assertAllClose(retained_keypoints_, expected_keypoints_)\n\n  def testRandomCropToAspectRatioWithCache(self):\n    preprocess_options = [(preprocessor.random_crop_to_aspect_ratio, {})]\n    self._testPreprocessorCache(preprocess_options,\n                                test_boxes=True,\n                                test_masks=False,\n                                test_keypoints=False)\n\n  def testRunRandomCropToAspectRatioWithMasks(self):\n    image = self.createColorfulTestImage()\n    boxes = self.createTestBoxes()\n    labels = self.createTestLabels()\n    masks = tf.random_uniform([2, 200, 400], dtype=tf.float32)\n\n    tensor_dict = {\n        fields.InputDataFields.image: image,\n        fields.InputDataFields.groundtruth_boxes: boxes,\n        fields.InputDataFields.groundtruth_classes: labels,\n        fields.InputDataFields.groundtruth_instance_masks: masks\n    }\n\n    preprocessor_arg_map = preprocessor.get_default_func_arg_map(\n        include_instance_masks=True)\n\n    preprocessing_options = [(preprocessor.random_crop_to_aspect_ratio, {})]\n\n    with mock.patch.object(preprocessor,\n                           \'_random_integer\') as mock_random_integer:\n      mock_random_integer.return_value = tf.constant(0, dtype=tf.int32)\n      distorted_tensor_dict = preprocessor.preprocess(\n          tensor_dict, preprocessing_options, func_arg_map=preprocessor_arg_map)\n      distorted_image = distorted_tensor_dict[fields.InputDataFields.image]\n      distorted_boxes = distorted_tensor_dict[\n          fields.InputDataFields.groundtruth_boxes]\n      distorted_labels = distorted_tensor_dict[\n          fields.InputDataFields.groundtruth_classes]\n      distorted_masks = distorted_tensor_dict[\n          fields.InputDataFields.groundtruth_instance_masks]\n      with self.test_session() as sess:\n        (distorted_image_, distorted_boxes_, distorted_labels_,\n         distorted_masks_) = sess.run([\n             distorted_image, distorted_boxes, distorted_labels, distorted_masks\n         ])\n\n        expected_boxes = np.array([0.0, 0.5, 0.75, 1.0], dtype=np.float32)\n        self.assertAllEqual(distorted_image_.shape, [1, 200, 200, 3])\n        self.assertAllEqual(distorted_labels_, [1])\n        self.assertAllClose(distorted_boxes_.flatten(),\n                            expected_boxes.flatten())\n        self.assertAllEqual(distorted_masks_.shape, [1, 200, 200])\n\n  def testRunRandomCropToAspectRatioWithKeypoints(self):\n    image = self.createColorfulTestImage()\n    boxes = self.createTestBoxes()\n    labels = self.createTestLabels()\n    keypoints = self.createTestKeypoints()\n\n    tensor_dict = {\n        fields.InputDataFields.image: image,\n        fields.InputDataFields.groundtruth_boxes: boxes,\n        fields.InputDataFields.groundtruth_classes: labels,\n        fields.InputDataFields.groundtruth_keypoints: keypoints\n    }\n\n    preprocessor_arg_map = preprocessor.get_default_func_arg_map(\n        include_keypoints=True)\n\n    preprocessing_options = [(preprocessor.random_crop_to_aspect_ratio, {})]\n\n    with mock.patch.object(preprocessor,\n                           \'_random_integer\') as mock_random_integer:\n      mock_random_integer.return_value = tf.constant(0, dtype=tf.int32)\n      distorted_tensor_dict = preprocessor.preprocess(\n          tensor_dict, preprocessing_options, func_arg_map=preprocessor_arg_map)\n      distorted_image = distorted_tensor_dict[fields.InputDataFields.image]\n      distorted_boxes = distorted_tensor_dict[\n          fields.InputDataFields.groundtruth_boxes]\n      distorted_labels = distorted_tensor_dict[\n          fields.InputDataFields.groundtruth_classes]\n      distorted_keypoints = distorted_tensor_dict[\n          fields.InputDataFields.groundtruth_keypoints]\n      with self.test_session() as sess:\n        (distorted_image_, distorted_boxes_, distorted_labels_,\n         distorted_keypoints_) = sess.run([\n             distorted_image, distorted_boxes, distorted_labels,\n             distorted_keypoints\n         ])\n\n        expected_boxes = np.array([0.0, 0.5, 0.75, 1.0], dtype=np.float32)\n        expected_keypoints = np.array(\n            [[0.1, 0.2], [0.2, 0.4], [0.3, 0.6]], dtype=np.float32)\n        self.assertAllEqual(distorted_image_.shape, [1, 200, 200, 3])\n        self.assertAllEqual(distorted_labels_, [1])\n        self.assertAllClose(distorted_boxes_.flatten(),\n                            expected_boxes.flatten())\n        self.assertAllClose(distorted_keypoints_.flatten(),\n                            expected_keypoints.flatten())\n\n  def testRandomPadToAspectRatioWithCache(self):\n    preprocess_options = [(preprocessor.random_pad_to_aspect_ratio, {})]\n    self._testPreprocessorCache(preprocess_options,\n                                test_boxes=True,\n                                test_masks=True,\n                                test_keypoints=True)\n\n  def testRunRandomPadToAspectRatioWithMinMaxPaddedSizeRatios(self):\n    image = self.createColorfulTestImage()\n    boxes = self.createTestBoxes()\n    labels = self.createTestLabels()\n\n    tensor_dict = {\n        fields.InputDataFields.image: image,\n        fields.InputDataFields.groundtruth_boxes: boxes,\n        fields.InputDataFields.groundtruth_classes: labels\n    }\n\n    preprocessor_arg_map = preprocessor.get_default_func_arg_map()\n    preprocessing_options = [(preprocessor.random_pad_to_aspect_ratio,\n                              {\'min_padded_size_ratio\': (4.0, 4.0),\n                               \'max_padded_size_ratio\': (4.0, 4.0)})]\n\n    distorted_tensor_dict = preprocessor.preprocess(\n        tensor_dict, preprocessing_options, func_arg_map=preprocessor_arg_map)\n    distorted_image = distorted_tensor_dict[fields.InputDataFields.image]\n    distorted_boxes = distorted_tensor_dict[\n        fields.InputDataFields.groundtruth_boxes]\n    distorted_labels = distorted_tensor_dict[\n        fields.InputDataFields.groundtruth_classes]\n    with self.test_session() as sess:\n      distorted_image_, distorted_boxes_, distorted_labels_ = sess.run([\n          distorted_image, distorted_boxes, distorted_labels])\n\n      expected_boxes = np.array(\n          [[0.0, 0.125, 0.1875, 0.5], [0.0625, 0.25, 0.1875, 0.5]],\n          dtype=np.float32)\n      self.assertAllEqual(distorted_image_.shape, [1, 800, 800, 3])\n      self.assertAllEqual(distorted_labels_, [1, 2])\n      self.assertAllClose(distorted_boxes_.flatten(),\n                          expected_boxes.flatten())\n\n  def testRunRandomPadToAspectRatioWithMasks(self):\n    image = self.createColorfulTestImage()\n    boxes = self.createTestBoxes()\n    labels = self.createTestLabels()\n    masks = tf.random_uniform([2, 200, 400], dtype=tf.float32)\n\n    tensor_dict = {\n        fields.InputDataFields.image: image,\n        fields.InputDataFields.groundtruth_boxes: boxes,\n        fields.InputDataFields.groundtruth_classes: labels,\n        fields.InputDataFields.groundtruth_instance_masks: masks\n    }\n\n    preprocessor_arg_map = preprocessor.get_default_func_arg_map(\n        include_instance_masks=True)\n\n    preprocessing_options = [(preprocessor.random_pad_to_aspect_ratio, {})]\n\n    distorted_tensor_dict = preprocessor.preprocess(\n        tensor_dict, preprocessing_options, func_arg_map=preprocessor_arg_map)\n    distorted_image = distorted_tensor_dict[fields.InputDataFields.image]\n    distorted_boxes = distorted_tensor_dict[\n        fields.InputDataFields.groundtruth_boxes]\n    distorted_labels = distorted_tensor_dict[\n        fields.InputDataFields.groundtruth_classes]\n    distorted_masks = distorted_tensor_dict[\n        fields.InputDataFields.groundtruth_instance_masks]\n    with self.test_session() as sess:\n      (distorted_image_, distorted_boxes_, distorted_labels_,\n       distorted_masks_) = sess.run([\n           distorted_image, distorted_boxes, distorted_labels, distorted_masks\n       ])\n\n      expected_boxes = np.array(\n          [[0.0, 0.25, 0.375, 1.0], [0.125, 0.5, 0.375, 1.0]], dtype=np.float32)\n      self.assertAllEqual(distorted_image_.shape, [1, 400, 400, 3])\n      self.assertAllEqual(distorted_labels_, [1, 2])\n      self.assertAllClose(distorted_boxes_.flatten(),\n                          expected_boxes.flatten())\n      self.assertAllEqual(distorted_masks_.shape, [2, 400, 400])\n\n  def testRunRandomPadToAspectRatioWithKeypoints(self):\n    image = self.createColorfulTestImage()\n    boxes = self.createTestBoxes()\n    labels = self.createTestLabels()\n    keypoints = self.createTestKeypoints()\n\n    tensor_dict = {\n        fields.InputDataFields.image: image,\n        fields.InputDataFields.groundtruth_boxes: boxes,\n        fields.InputDataFields.groundtruth_classes: labels,\n        fields.InputDataFields.groundtruth_keypoints: keypoints\n    }\n\n    preprocessor_arg_map = preprocessor.get_default_func_arg_map(\n        include_keypoints=True)\n\n    preprocessing_options = [(preprocessor.random_pad_to_aspect_ratio, {})]\n\n    distorted_tensor_dict = preprocessor.preprocess(\n        tensor_dict, preprocessing_options, func_arg_map=preprocessor_arg_map)\n    distorted_image = distorted_tensor_dict[fields.InputDataFields.image]\n    distorted_boxes = distorted_tensor_dict[\n        fields.InputDataFields.groundtruth_boxes]\n    distorted_labels = distorted_tensor_dict[\n        fields.InputDataFields.groundtruth_classes]\n    distorted_keypoints = distorted_tensor_dict[\n        fields.InputDataFields.groundtruth_keypoints]\n    with self.test_session() as sess:\n      (distorted_image_, distorted_boxes_, distorted_labels_,\n       distorted_keypoints_) = sess.run([\n           distorted_image, distorted_boxes, distorted_labels,\n           distorted_keypoints\n       ])\n\n      expected_boxes = np.array(\n          [[0.0, 0.25, 0.375, 1.0], [0.125, 0.5, 0.375, 1.0]], dtype=np.float32)\n      expected_keypoints = np.array([\n          [[0.05, 0.1], [0.1, 0.2], [0.15, 0.3]],\n          [[0.2, 0.4], [0.25, 0.5], [0.3, 0.6]],\n      ], dtype=np.float32)\n      self.assertAllEqual(distorted_image_.shape, [1, 400, 400, 3])\n      self.assertAllEqual(distorted_labels_, [1, 2])\n      self.assertAllClose(distorted_boxes_.flatten(),\n                          expected_boxes.flatten())\n      self.assertAllClose(distorted_keypoints_.flatten(),\n                          expected_keypoints.flatten())\n\n  def testRandomPadImageWithCache(self):\n    preprocess_options = [(preprocessor.normalize_image, {\n        \'original_minval\': 0,\n        \'original_maxval\': 255,\n        \'target_minval\': 0,\n        \'target_maxval\': 1,}), (preprocessor.random_pad_image, {})]\n    self._testPreprocessorCache(preprocess_options,\n                                test_boxes=True,\n                                test_masks=True,\n                                test_keypoints=True)\n\n  def testRandomPadImage(self):\n    preprocessing_options = [(preprocessor.normalize_image, {\n        \'original_minval\': 0,\n        \'original_maxval\': 255,\n        \'target_minval\': 0,\n        \'target_maxval\': 1\n    })]\n\n    images = self.createTestImages()\n    boxes = self.createTestBoxes()\n    labels = self.createTestLabels()\n    tensor_dict = {\n        fields.InputDataFields.image: images,\n        fields.InputDataFields.groundtruth_boxes: boxes,\n        fields.InputDataFields.groundtruth_classes: labels,\n    }\n    tensor_dict = preprocessor.preprocess(tensor_dict, preprocessing_options)\n    images = tensor_dict[fields.InputDataFields.image]\n\n    preprocessing_options = [(preprocessor.random_pad_image, {})]\n    padded_tensor_dict = preprocessor.preprocess(tensor_dict,\n                                                 preprocessing_options)\n\n    padded_images = padded_tensor_dict[fields.InputDataFields.image]\n    padded_boxes = padded_tensor_dict[\n        fields.InputDataFields.groundtruth_boxes]\n    boxes_shape = tf.shape(boxes)\n    padded_boxes_shape = tf.shape(padded_boxes)\n    images_shape = tf.shape(images)\n    padded_images_shape = tf.shape(padded_images)\n\n    with self.test_session() as sess:\n      (boxes_shape_, padded_boxes_shape_, images_shape_,\n       padded_images_shape_, boxes_, padded_boxes_) = sess.run(\n           [boxes_shape, padded_boxes_shape, images_shape,\n            padded_images_shape, boxes, padded_boxes])\n      self.assertAllEqual(boxes_shape_, padded_boxes_shape_)\n      self.assertTrue((images_shape_[1] >= padded_images_shape_[1] * 0.5).all)\n      self.assertTrue((images_shape_[2] >= padded_images_shape_[2] * 0.5).all)\n      self.assertTrue((images_shape_[1] <= padded_images_shape_[1]).all)\n      self.assertTrue((images_shape_[2] <= padded_images_shape_[2]).all)\n      self.assertTrue(np.all((boxes_[:, 2] - boxes_[:, 0]) >= (\n          padded_boxes_[:, 2] - padded_boxes_[:, 0])))\n      self.assertTrue(np.all((boxes_[:, 3] - boxes_[:, 1]) >= (\n          padded_boxes_[:, 3] - padded_boxes_[:, 1])))\n\n  def testRandomCropPadImageWithCache(self):\n    preprocess_options = [(preprocessor.normalize_image, {\n        \'original_minval\': 0,\n        \'original_maxval\': 255,\n        \'target_minval\': 0,\n        \'target_maxval\': 1,}), (preprocessor.random_crop_pad_image, {})]\n    self._testPreprocessorCache(preprocess_options,\n                                test_boxes=True,\n                                test_masks=True,\n                                test_keypoints=True)\n\n  def testRandomCropPadImageWithRandomCoefOne(self):\n    preprocessing_options = [(preprocessor.normalize_image, {\n        \'original_minval\': 0,\n        \'original_maxval\': 255,\n        \'target_minval\': 0,\n        \'target_maxval\': 1\n    })]\n\n    images = self.createTestImages()\n    boxes = self.createTestBoxes()\n    labels = self.createTestLabels()\n    tensor_dict = {\n        fields.InputDataFields.image: images,\n        fields.InputDataFields.groundtruth_boxes: boxes,\n        fields.InputDataFields.groundtruth_classes: labels,\n    }\n    tensor_dict = preprocessor.preprocess(tensor_dict, preprocessing_options)\n    images = tensor_dict[fields.InputDataFields.image]\n\n    preprocessing_options = [(preprocessor.random_crop_pad_image, {\n        \'random_coef\': 1.0\n    })]\n    padded_tensor_dict = preprocessor.preprocess(tensor_dict,\n                                                 preprocessing_options)\n\n    padded_images = padded_tensor_dict[fields.InputDataFields.image]\n    padded_boxes = padded_tensor_dict[\n        fields.InputDataFields.groundtruth_boxes]\n    boxes_shape = tf.shape(boxes)\n    padded_boxes_shape = tf.shape(padded_boxes)\n    images_shape = tf.shape(images)\n    padded_images_shape = tf.shape(padded_images)\n\n    with self.test_session() as sess:\n      (boxes_shape_, padded_boxes_shape_, images_shape_,\n       padded_images_shape_, boxes_, padded_boxes_) = sess.run(\n           [boxes_shape, padded_boxes_shape, images_shape,\n            padded_images_shape, boxes, padded_boxes])\n      self.assertAllEqual(boxes_shape_, padded_boxes_shape_)\n      self.assertTrue((images_shape_[1] >= padded_images_shape_[1] * 0.5).all)\n      self.assertTrue((images_shape_[2] >= padded_images_shape_[2] * 0.5).all)\n      self.assertTrue((images_shape_[1] <= padded_images_shape_[1]).all)\n      self.assertTrue((images_shape_[2] <= padded_images_shape_[2]).all)\n      self.assertTrue(np.all((boxes_[:, 2] - boxes_[:, 0]) >= (\n          padded_boxes_[:, 2] - padded_boxes_[:, 0])))\n      self.assertTrue(np.all((boxes_[:, 3] - boxes_[:, 1]) >= (\n          padded_boxes_[:, 3] - padded_boxes_[:, 1])))\n\n  def testRandomCropToAspectRatio(self):\n    images = self.createTestImages()\n    boxes = self.createTestBoxes()\n    labels = self.createTestLabels()\n    tensor_dict = {\n        fields.InputDataFields.image: images,\n        fields.InputDataFields.groundtruth_boxes: boxes,\n        fields.InputDataFields.groundtruth_classes: labels,\n    }\n    tensor_dict = preprocessor.preprocess(tensor_dict, [])\n    images = tensor_dict[fields.InputDataFields.image]\n\n    preprocessing_options = [(preprocessor.random_crop_to_aspect_ratio, {\n        \'aspect_ratio\': 2.0\n    })]\n    cropped_tensor_dict = preprocessor.preprocess(tensor_dict,\n                                                  preprocessing_options)\n\n    cropped_images = cropped_tensor_dict[fields.InputDataFields.image]\n    cropped_boxes = cropped_tensor_dict[\n        fields.InputDataFields.groundtruth_boxes]\n    boxes_shape = tf.shape(boxes)\n    cropped_boxes_shape = tf.shape(cropped_boxes)\n    images_shape = tf.shape(images)\n    cropped_images_shape = tf.shape(cropped_images)\n\n    with self.test_session() as sess:\n      (boxes_shape_, cropped_boxes_shape_, images_shape_,\n       cropped_images_shape_) = sess.run([\n           boxes_shape, cropped_boxes_shape, images_shape, cropped_images_shape\n       ])\n      self.assertAllEqual(boxes_shape_, cropped_boxes_shape_)\n      self.assertEqual(images_shape_[1], cropped_images_shape_[1] * 2)\n      self.assertEqual(images_shape_[2], cropped_images_shape_[2])\n\n  def testRandomPadToAspectRatio(self):\n    images = self.createTestImages()\n    boxes = self.createTestBoxes()\n    labels = self.createTestLabels()\n    tensor_dict = {\n        fields.InputDataFields.image: images,\n        fields.InputDataFields.groundtruth_boxes: boxes,\n        fields.InputDataFields.groundtruth_classes: labels,\n    }\n    tensor_dict = preprocessor.preprocess(tensor_dict, [])\n    images = tensor_dict[fields.InputDataFields.image]\n\n    preprocessing_options = [(preprocessor.random_pad_to_aspect_ratio, {\n        \'aspect_ratio\': 2.0\n    })]\n    padded_tensor_dict = preprocessor.preprocess(tensor_dict,\n                                                 preprocessing_options)\n\n    padded_images = padded_tensor_dict[fields.InputDataFields.image]\n    padded_boxes = padded_tensor_dict[\n        fields.InputDataFields.groundtruth_boxes]\n    boxes_shape = tf.shape(boxes)\n    padded_boxes_shape = tf.shape(padded_boxes)\n    images_shape = tf.shape(images)\n    padded_images_shape = tf.shape(padded_images)\n\n    with self.test_session() as sess:\n      (boxes_shape_, padded_boxes_shape_, images_shape_,\n       padded_images_shape_) = sess.run([\n           boxes_shape, padded_boxes_shape, images_shape, padded_images_shape\n       ])\n      self.assertAllEqual(boxes_shape_, padded_boxes_shape_)\n      self.assertEqual(images_shape_[1], padded_images_shape_[1])\n      self.assertEqual(2 * images_shape_[2], padded_images_shape_[2])\n\n  def testRandomBlackPatchesWithCache(self):\n    preprocess_options = []\n    preprocess_options.append((preprocessor.normalize_image, {\n        \'original_minval\': 0,\n        \'original_maxval\': 255,\n        \'target_minval\': 0,\n        \'target_maxval\': 1\n    }))\n    preprocess_options.append((preprocessor.random_black_patches, {\n        \'size_to_image_ratio\': 0.5\n    }))\n    self._testPreprocessorCache(preprocess_options,\n                                test_boxes=True,\n                                test_masks=True,\n                                test_keypoints=True)\n\n  def testRandomBlackPatches(self):\n    preprocessing_options = []\n    preprocessing_options.append((preprocessor.normalize_image, {\n        \'original_minval\': 0,\n        \'original_maxval\': 255,\n        \'target_minval\': 0,\n        \'target_maxval\': 1\n    }))\n    preprocessing_options.append((preprocessor.random_black_patches, {\n        \'size_to_image_ratio\': 0.5\n    }))\n    images = self.createTestImages()\n    tensor_dict = {fields.InputDataFields.image: images}\n    blacked_tensor_dict = preprocessor.preprocess(tensor_dict,\n                                                  preprocessing_options)\n    blacked_images = blacked_tensor_dict[fields.InputDataFields.image]\n    images_shape = tf.shape(images)\n    blacked_images_shape = tf.shape(blacked_images)\n\n    with self.test_session() as sess:\n      (images_shape_, blacked_images_shape_) = sess.run(\n          [images_shape, blacked_images_shape])\n      self.assertAllEqual(images_shape_, blacked_images_shape_)\n\n  def testRandomResizeMethodWithCache(self):\n    preprocess_options = []\n    preprocess_options.append((preprocessor.normalize_image, {\n        \'original_minval\': 0,\n        \'original_maxval\': 255,\n        \'target_minval\': 0,\n        \'target_maxval\': 1\n    }))\n    preprocess_options.append((preprocessor.random_resize_method, {\n        \'target_size\': (75, 150)\n    }))\n    self._testPreprocessorCache(preprocess_options,\n                                test_boxes=True,\n                                test_masks=True,\n                                test_keypoints=True)\n\n  def testRandomResizeMethod(self):\n    preprocessing_options = []\n    preprocessing_options.append((preprocessor.normalize_image, {\n        \'original_minval\': 0,\n        \'original_maxval\': 255,\n        \'target_minval\': 0,\n        \'target_maxval\': 1\n    }))\n    preprocessing_options.append((preprocessor.random_resize_method, {\n        \'target_size\': (75, 150)\n    }))\n    images = self.createTestImages()\n    tensor_dict = {fields.InputDataFields.image: images}\n    resized_tensor_dict = preprocessor.preprocess(tensor_dict,\n                                                  preprocessing_options)\n    resized_images = resized_tensor_dict[fields.InputDataFields.image]\n    resized_images_shape = tf.shape(resized_images)\n    expected_images_shape = tf.constant([1, 75, 150, 3], dtype=tf.int32)\n\n    with self.test_session() as sess:\n      (expected_images_shape_, resized_images_shape_) = sess.run(\n          [expected_images_shape, resized_images_shape])\n      self.assertAllEqual(expected_images_shape_,\n                          resized_images_shape_)\n\n  def testResizeImageWithMasks(self):\n    """"""Tests image resizing, checking output sizes.""""""\n    in_image_shape_list = [[60, 40, 3], [15, 30, 3]]\n    in_masks_shape_list = [[15, 60, 40], [10, 15, 30]]\n    height = 50\n    width = 100\n    expected_image_shape_list = [[50, 100, 3], [50, 100, 3]]\n    expected_masks_shape_list = [[15, 50, 100], [10, 50, 100]]\n\n    for (in_image_shape, expected_image_shape, in_masks_shape,\n         expected_mask_shape) in zip(in_image_shape_list,\n                                     expected_image_shape_list,\n                                     in_masks_shape_list,\n                                     expected_masks_shape_list):\n      in_image = tf.random_uniform(in_image_shape)\n      in_masks = tf.random_uniform(in_masks_shape)\n      out_image, out_masks, _ = preprocessor.resize_image(\n          in_image, in_masks, new_height=height, new_width=width)\n      out_image_shape = tf.shape(out_image)\n      out_masks_shape = tf.shape(out_masks)\n\n      with self.test_session() as sess:\n        out_image_shape, out_masks_shape = sess.run(\n            [out_image_shape, out_masks_shape])\n        self.assertAllEqual(out_image_shape, expected_image_shape)\n        self.assertAllEqual(out_masks_shape, expected_mask_shape)\n\n  def testResizeImageWithMasksTensorInputHeightAndWidth(self):\n    """"""Tests image resizing, checking output sizes.""""""\n    in_image_shape_list = [[60, 40, 3], [15, 30, 3]]\n    in_masks_shape_list = [[15, 60, 40], [10, 15, 30]]\n    height = tf.constant(50, dtype=tf.int32)\n    width = tf.constant(100, dtype=tf.int32)\n    expected_image_shape_list = [[50, 100, 3], [50, 100, 3]]\n    expected_masks_shape_list = [[15, 50, 100], [10, 50, 100]]\n\n    for (in_image_shape, expected_image_shape, in_masks_shape,\n         expected_mask_shape) in zip(in_image_shape_list,\n                                     expected_image_shape_list,\n                                     in_masks_shape_list,\n                                     expected_masks_shape_list):\n      in_image = tf.random_uniform(in_image_shape)\n      in_masks = tf.random_uniform(in_masks_shape)\n      out_image, out_masks, _ = preprocessor.resize_image(\n          in_image, in_masks, new_height=height, new_width=width)\n      out_image_shape = tf.shape(out_image)\n      out_masks_shape = tf.shape(out_masks)\n\n      with self.test_session() as sess:\n        out_image_shape, out_masks_shape = sess.run(\n            [out_image_shape, out_masks_shape])\n        self.assertAllEqual(out_image_shape, expected_image_shape)\n        self.assertAllEqual(out_masks_shape, expected_mask_shape)\n\n  def testResizeImageWithNoInstanceMask(self):\n    """"""Tests image resizing, checking output sizes.""""""\n    in_image_shape_list = [[60, 40, 3], [15, 30, 3]]\n    in_masks_shape_list = [[0, 60, 40], [0, 15, 30]]\n    height = 50\n    width = 100\n    expected_image_shape_list = [[50, 100, 3], [50, 100, 3]]\n    expected_masks_shape_list = [[0, 50, 100], [0, 50, 100]]\n\n    for (in_image_shape, expected_image_shape, in_masks_shape,\n         expected_mask_shape) in zip(in_image_shape_list,\n                                     expected_image_shape_list,\n                                     in_masks_shape_list,\n                                     expected_masks_shape_list):\n      in_image = tf.random_uniform(in_image_shape)\n      in_masks = tf.random_uniform(in_masks_shape)\n      out_image, out_masks, _ = preprocessor.resize_image(\n          in_image, in_masks, new_height=height, new_width=width)\n      out_image_shape = tf.shape(out_image)\n      out_masks_shape = tf.shape(out_masks)\n\n      with self.test_session() as sess:\n        out_image_shape, out_masks_shape = sess.run(\n            [out_image_shape, out_masks_shape])\n        self.assertAllEqual(out_image_shape, expected_image_shape)\n        self.assertAllEqual(out_masks_shape, expected_mask_shape)\n\n  def testResizeToRangePreservesStaticSpatialShape(self):\n    """"""Tests image resizing, checking output sizes.""""""\n    in_shape_list = [[60, 40, 3], [15, 30, 3], [15, 50, 3]]\n    min_dim = 50\n    max_dim = 100\n    expected_shape_list = [[75, 50, 3], [50, 100, 3], [30, 100, 3]]\n\n    for in_shape, expected_shape in zip(in_shape_list, expected_shape_list):\n      in_image = tf.random_uniform(in_shape)\n      out_image, _ = preprocessor.resize_to_range(\n          in_image, min_dimension=min_dim, max_dimension=max_dim)\n      self.assertAllEqual(out_image.get_shape().as_list(), expected_shape)\n\n  def testResizeToRangeWithDynamicSpatialShape(self):\n    """"""Tests image resizing, checking output sizes.""""""\n    in_shape_list = [[60, 40, 3], [15, 30, 3], [15, 50, 3]]\n    min_dim = 50\n    max_dim = 100\n    expected_shape_list = [[75, 50, 3], [50, 100, 3], [30, 100, 3]]\n\n    for in_shape, expected_shape in zip(in_shape_list, expected_shape_list):\n      in_image = tf.placeholder(tf.float32, shape=(None, None, 3))\n      out_image, _ = preprocessor.resize_to_range(\n          in_image, min_dimension=min_dim, max_dimension=max_dim)\n      out_image_shape = tf.shape(out_image)\n      with self.test_session() as sess:\n        out_image_shape = sess.run(out_image_shape,\n                                   feed_dict={in_image:\n                                              np.random.randn(*in_shape)})\n        self.assertAllEqual(out_image_shape, expected_shape)\n\n  def testResizeToRangeWithMasksPreservesStaticSpatialShape(self):\n    """"""Tests image resizing, checking output sizes.""""""\n    in_image_shape_list = [[60, 40, 3], [15, 30, 3]]\n    in_masks_shape_list = [[15, 60, 40], [10, 15, 30]]\n    min_dim = 50\n    max_dim = 100\n    expected_image_shape_list = [[75, 50, 3], [50, 100, 3]]\n    expected_masks_shape_list = [[15, 75, 50], [10, 50, 100]]\n\n    for (in_image_shape, expected_image_shape, in_masks_shape,\n         expected_mask_shape) in zip(in_image_shape_list,\n                                     expected_image_shape_list,\n                                     in_masks_shape_list,\n                                     expected_masks_shape_list):\n      in_image = tf.random_uniform(in_image_shape)\n      in_masks = tf.random_uniform(in_masks_shape)\n      out_image, out_masks, _ = preprocessor.resize_to_range(\n          in_image, in_masks, min_dimension=min_dim, max_dimension=max_dim)\n      self.assertAllEqual(out_masks.get_shape().as_list(), expected_mask_shape)\n      self.assertAllEqual(out_image.get_shape().as_list(), expected_image_shape)\n\n  def testResizeToRangeWithMasksAndDynamicSpatialShape(self):\n    """"""Tests image resizing, checking output sizes.""""""\n    in_image_shape_list = [[60, 40, 3], [15, 30, 3]]\n    in_masks_shape_list = [[15, 60, 40], [10, 15, 30]]\n    min_dim = 50\n    max_dim = 100\n    expected_image_shape_list = [[75, 50, 3], [50, 100, 3]]\n    expected_masks_shape_list = [[15, 75, 50], [10, 50, 100]]\n\n    for (in_image_shape, expected_image_shape, in_masks_shape,\n         expected_mask_shape) in zip(in_image_shape_list,\n                                     expected_image_shape_list,\n                                     in_masks_shape_list,\n                                     expected_masks_shape_list):\n      in_image = tf.placeholder(tf.float32, shape=(None, None, 3))\n      in_masks = tf.placeholder(tf.float32, shape=(None, None, None))\n      in_masks = tf.random_uniform(in_masks_shape)\n      out_image, out_masks, _ = preprocessor.resize_to_range(\n          in_image, in_masks, min_dimension=min_dim, max_dimension=max_dim)\n      out_image_shape = tf.shape(out_image)\n      out_masks_shape = tf.shape(out_masks)\n\n      with self.test_session() as sess:\n        out_image_shape, out_masks_shape = sess.run(\n            [out_image_shape, out_masks_shape],\n            feed_dict={\n                in_image: np.random.randn(*in_image_shape),\n                in_masks: np.random.randn(*in_masks_shape)\n            })\n        self.assertAllEqual(out_image_shape, expected_image_shape)\n        self.assertAllEqual(out_masks_shape, expected_mask_shape)\n\n  def testResizeToRangeWithInstanceMasksTensorOfSizeZero(self):\n    """"""Tests image resizing, checking output sizes.""""""\n    in_image_shape_list = [[60, 40, 3], [15, 30, 3]]\n    in_masks_shape_list = [[0, 60, 40], [0, 15, 30]]\n    min_dim = 50\n    max_dim = 100\n    expected_image_shape_list = [[75, 50, 3], [50, 100, 3]]\n    expected_masks_shape_list = [[0, 75, 50], [0, 50, 100]]\n\n    for (in_image_shape, expected_image_shape, in_masks_shape,\n         expected_mask_shape) in zip(in_image_shape_list,\n                                     expected_image_shape_list,\n                                     in_masks_shape_list,\n                                     expected_masks_shape_list):\n      in_image = tf.random_uniform(in_image_shape)\n      in_masks = tf.random_uniform(in_masks_shape)\n      out_image, out_masks, _ = preprocessor.resize_to_range(\n          in_image, in_masks, min_dimension=min_dim, max_dimension=max_dim)\n      out_image_shape = tf.shape(out_image)\n      out_masks_shape = tf.shape(out_masks)\n\n      with self.test_session() as sess:\n        out_image_shape, out_masks_shape = sess.run(\n            [out_image_shape, out_masks_shape])\n        self.assertAllEqual(out_image_shape, expected_image_shape)\n        self.assertAllEqual(out_masks_shape, expected_mask_shape)\n\n  def testResizeToRange4DImageTensor(self):\n    image = tf.random_uniform([1, 200, 300, 3])\n    with self.assertRaises(ValueError):\n      preprocessor.resize_to_range(image, 500, 600)\n\n  def testResizeToRangeSameMinMax(self):\n    """"""Tests image resizing, checking output sizes.""""""\n    in_shape_list = [[312, 312, 3], [299, 299, 3]]\n    min_dim = 320\n    max_dim = 320\n    expected_shape_list = [[320, 320, 3], [320, 320, 3]]\n\n    for in_shape, expected_shape in zip(in_shape_list, expected_shape_list):\n      in_image = tf.random_uniform(in_shape)\n      out_image, _ = preprocessor.resize_to_range(\n          in_image, min_dimension=min_dim, max_dimension=max_dim)\n      out_image_shape = tf.shape(out_image)\n\n      with self.test_session() as sess:\n        out_image_shape = sess.run(out_image_shape)\n        self.assertAllEqual(out_image_shape, expected_shape)\n\n  def testResizeToMinDimensionTensorShapes(self):\n    in_image_shape_list = [[60, 55, 3], [15, 30, 3]]\n    in_masks_shape_list = [[15, 60, 55], [10, 15, 30]]\n    min_dim = 50\n    expected_image_shape_list = [[60, 55, 3], [50, 100, 3]]\n    expected_masks_shape_list = [[15, 60, 55], [10, 50, 100]]\n\n    for (in_image_shape, expected_image_shape, in_masks_shape,\n         expected_mask_shape) in zip(in_image_shape_list,\n                                     expected_image_shape_list,\n                                     in_masks_shape_list,\n                                     expected_masks_shape_list):\n      in_image = tf.placeholder(tf.float32, shape=(None, None, 3))\n      in_masks = tf.placeholder(tf.float32, shape=(None, None, None))\n      in_masks = tf.random_uniform(in_masks_shape)\n      out_image, out_masks, _ = preprocessor.resize_to_min_dimension(\n          in_image, in_masks, min_dimension=min_dim)\n      out_image_shape = tf.shape(out_image)\n      out_masks_shape = tf.shape(out_masks)\n\n      with self.test_session() as sess:\n        out_image_shape, out_masks_shape = sess.run(\n            [out_image_shape, out_masks_shape],\n            feed_dict={\n                in_image: np.random.randn(*in_image_shape),\n                in_masks: np.random.randn(*in_masks_shape)\n            })\n        self.assertAllEqual(out_image_shape, expected_image_shape)\n        self.assertAllEqual(out_masks_shape, expected_mask_shape)\n\n  def testResizeToMinDimensionWithInstanceMasksTensorOfSizeZero(self):\n    """"""Tests image resizing, checking output sizes.""""""\n    in_image_shape_list = [[60, 40, 3], [15, 30, 3]]\n    in_masks_shape_list = [[0, 60, 40], [0, 15, 30]]\n    min_dim = 50\n    expected_image_shape_list = [[75, 50, 3], [50, 100, 3]]\n    expected_masks_shape_list = [[0, 75, 50], [0, 50, 100]]\n\n    for (in_image_shape, expected_image_shape, in_masks_shape,\n         expected_mask_shape) in zip(in_image_shape_list,\n                                     expected_image_shape_list,\n                                     in_masks_shape_list,\n                                     expected_masks_shape_list):\n      in_image = tf.random_uniform(in_image_shape)\n      in_masks = tf.random_uniform(in_masks_shape)\n      out_image, out_masks, _ = preprocessor.resize_to_min_dimension(\n          in_image, in_masks, min_dimension=min_dim)\n      out_image_shape = tf.shape(out_image)\n      out_masks_shape = tf.shape(out_masks)\n\n      with self.test_session() as sess:\n        out_image_shape, out_masks_shape = sess.run(\n            [out_image_shape, out_masks_shape])\n        self.assertAllEqual(out_image_shape, expected_image_shape)\n        self.assertAllEqual(out_masks_shape, expected_mask_shape)\n\n  def testResizeToMinDimensionRaisesErrorOn4DImage(self):\n    image = tf.random_uniform([1, 200, 300, 3])\n    with self.assertRaises(ValueError):\n      preprocessor.resize_to_min_dimension(image, 500)\n\n  def testScaleBoxesToPixelCoordinates(self):\n    """"""Tests box scaling, checking scaled values.""""""\n    in_shape = [60, 40, 3]\n    in_boxes = [[0.1, 0.2, 0.4, 0.6],\n                [0.5, 0.3, 0.9, 0.7]]\n\n    expected_boxes = [[6., 8., 24., 24.],\n                      [30., 12., 54., 28.]]\n\n    in_image = tf.random_uniform(in_shape)\n    in_boxes = tf.constant(in_boxes)\n    _, out_boxes = preprocessor.scale_boxes_to_pixel_coordinates(\n        in_image, boxes=in_boxes)\n    with self.test_session() as sess:\n      out_boxes = sess.run(out_boxes)\n      self.assertAllClose(out_boxes, expected_boxes)\n\n  def testScaleBoxesToPixelCoordinatesWithKeypoints(self):\n    """"""Tests box and keypoint scaling, checking scaled values.""""""\n    in_shape = [60, 40, 3]\n    in_boxes = self.createTestBoxes()\n    in_keypoints = self.createTestKeypoints()\n\n    expected_boxes = [[0., 10., 45., 40.],\n                      [15., 20., 45., 40.]]\n    expected_keypoints = [\n        [[6., 4.], [12., 8.], [18., 12.]],\n        [[24., 16.], [30., 20.], [36., 24.]],\n    ]\n\n    in_image = tf.random_uniform(in_shape)\n    _, out_boxes, out_keypoints = preprocessor.scale_boxes_to_pixel_coordinates(\n        in_image, boxes=in_boxes, keypoints=in_keypoints)\n    with self.test_session() as sess:\n      out_boxes_, out_keypoints_ = sess.run([out_boxes, out_keypoints])\n      self.assertAllClose(out_boxes_, expected_boxes)\n      self.assertAllClose(out_keypoints_, expected_keypoints)\n\n  def testSubtractChannelMean(self):\n    """"""Tests whether channel means have been subtracted.""""""\n    with self.test_session():\n      image = tf.zeros((240, 320, 3))\n      means = [1, 2, 3]\n      actual = preprocessor.subtract_channel_mean(image, means=means)\n      actual = actual.eval()\n\n      self.assertTrue((actual[:, :, 0] == -1).all())\n      self.assertTrue((actual[:, :, 1] == -2).all())\n      self.assertTrue((actual[:, :, 2] == -3).all())\n\n  def testOneHotEncoding(self):\n    """"""Tests one hot encoding of multiclass labels.""""""\n    with self.test_session():\n      labels = tf.constant([1, 4, 2], dtype=tf.int32)\n      one_hot = preprocessor.one_hot_encoding(labels, num_classes=5)\n      one_hot = one_hot.eval()\n\n      self.assertAllEqual([0, 1, 1, 0, 1], one_hot)\n\n  def testSSDRandomCropWithCache(self):\n    preprocess_options = [\n        (preprocessor.normalize_image, {\n            \'original_minval\': 0,\n            \'original_maxval\': 255,\n            \'target_minval\': 0,\n            \'target_maxval\': 1\n        }),\n        (preprocessor.ssd_random_crop, {})]\n    self._testPreprocessorCache(preprocess_options,\n                                test_boxes=True,\n                                test_masks=False,\n                                test_keypoints=False)\n\n  def testSSDRandomCrop(self):\n    preprocessing_options = [\n        (preprocessor.normalize_image, {\n            \'original_minval\': 0,\n            \'original_maxval\': 255,\n            \'target_minval\': 0,\n            \'target_maxval\': 1\n        }),\n        (preprocessor.ssd_random_crop, {})]\n    images = self.createTestImages()\n    boxes = self.createTestBoxes()\n    labels = self.createTestLabels()\n    tensor_dict = {\n        fields.InputDataFields.image: images,\n        fields.InputDataFields.groundtruth_boxes: boxes,\n        fields.InputDataFields.groundtruth_classes: labels,\n    }\n    distorted_tensor_dict = preprocessor.preprocess(tensor_dict,\n                                                    preprocessing_options)\n    distorted_images = distorted_tensor_dict[fields.InputDataFields.image]\n    distorted_boxes = distorted_tensor_dict[\n        fields.InputDataFields.groundtruth_boxes]\n\n    images_rank = tf.rank(images)\n    distorted_images_rank = tf.rank(distorted_images)\n    boxes_rank = tf.rank(boxes)\n    distorted_boxes_rank = tf.rank(distorted_boxes)\n\n    with self.test_session() as sess:\n      (boxes_rank_, distorted_boxes_rank_, images_rank_,\n       distorted_images_rank_) = sess.run(\n           [boxes_rank, distorted_boxes_rank, images_rank,\n            distorted_images_rank])\n      self.assertAllEqual(boxes_rank_, distorted_boxes_rank_)\n      self.assertAllEqual(images_rank_, distorted_images_rank_)\n\n  def testSSDRandomCropPad(self):\n    images = self.createTestImages()\n    boxes = self.createTestBoxes()\n    labels = self.createTestLabels()\n    preprocessing_options = [\n        (preprocessor.normalize_image, {\n            \'original_minval\': 0,\n            \'original_maxval\': 255,\n            \'target_minval\': 0,\n            \'target_maxval\': 1\n        }),\n        (preprocessor.ssd_random_crop_pad, {})]\n    tensor_dict = {\n        fields.InputDataFields.image: images,\n        fields.InputDataFields.groundtruth_boxes: boxes,\n        fields.InputDataFields.groundtruth_classes: labels,\n    }\n    distorted_tensor_dict = preprocessor.preprocess(tensor_dict,\n                                                    preprocessing_options)\n    distorted_images = distorted_tensor_dict[fields.InputDataFields.image]\n    distorted_boxes = distorted_tensor_dict[\n        fields.InputDataFields.groundtruth_boxes]\n\n    images_rank = tf.rank(images)\n    distorted_images_rank = tf.rank(distorted_images)\n    boxes_rank = tf.rank(boxes)\n    distorted_boxes_rank = tf.rank(distorted_boxes)\n\n    with self.test_session() as sess:\n      (boxes_rank_, distorted_boxes_rank_, images_rank_,\n       distorted_images_rank_) = sess.run([\n           boxes_rank, distorted_boxes_rank, images_rank, distorted_images_rank\n       ])\n      self.assertAllEqual(boxes_rank_, distorted_boxes_rank_)\n      self.assertAllEqual(images_rank_, distorted_images_rank_)\n\n  def testSSDRandomCropFixedAspectRatioWithCache(self):\n    preprocess_options = [\n        (preprocessor.normalize_image, {\n            \'original_minval\': 0,\n            \'original_maxval\': 255,\n            \'target_minval\': 0,\n            \'target_maxval\': 1\n        }),\n        (preprocessor.ssd_random_crop_fixed_aspect_ratio, {})]\n    self._testPreprocessorCache(preprocess_options,\n                                test_boxes=True,\n                                test_masks=False,\n                                test_keypoints=False)\n\n  def _testSSDRandomCropFixedAspectRatio(self,\n                                         include_label_scores,\n                                         include_instance_masks,\n                                         include_keypoints):\n    images = self.createTestImages()\n    boxes = self.createTestBoxes()\n    labels = self.createTestLabels()\n    preprocessing_options = [\n        (preprocessor.normalize_image, {\n            \'original_minval\': 0,\n            \'original_maxval\': 255,\n            \'target_minval\': 0,\n            \'target_maxval\': 1\n        }),\n        (preprocessor.ssd_random_crop_fixed_aspect_ratio, {})]\n    tensor_dict = {\n        fields.InputDataFields.image: images,\n        fields.InputDataFields.groundtruth_boxes: boxes,\n        fields.InputDataFields.groundtruth_classes: labels\n    }\n    if include_label_scores:\n      label_scores = self.createTestLabelScores()\n      tensor_dict[fields.InputDataFields.groundtruth_label_scores] = (\n          label_scores)\n    if include_instance_masks:\n      masks = self.createTestMasks()\n      tensor_dict[fields.InputDataFields.groundtruth_instance_masks] = masks\n    if include_keypoints:\n      keypoints = self.createTestKeypoints()\n      tensor_dict[fields.InputDataFields.groundtruth_keypoints] = keypoints\n\n    preprocessor_arg_map = preprocessor.get_default_func_arg_map(\n        include_label_scores=include_label_scores,\n        include_instance_masks=include_instance_masks,\n        include_keypoints=include_keypoints)\n    distorted_tensor_dict = preprocessor.preprocess(\n        tensor_dict, preprocessing_options, func_arg_map=preprocessor_arg_map)\n    distorted_images = distorted_tensor_dict[fields.InputDataFields.image]\n    distorted_boxes = distorted_tensor_dict[\n        fields.InputDataFields.groundtruth_boxes]\n    images_rank = tf.rank(images)\n    distorted_images_rank = tf.rank(distorted_images)\n    boxes_rank = tf.rank(boxes)\n    distorted_boxes_rank = tf.rank(distorted_boxes)\n\n    with self.test_session() as sess:\n      (boxes_rank_, distorted_boxes_rank_, images_rank_,\n       distorted_images_rank_) = sess.run(\n           [boxes_rank, distorted_boxes_rank, images_rank,\n            distorted_images_rank])\n      self.assertAllEqual(boxes_rank_, distorted_boxes_rank_)\n      self.assertAllEqual(images_rank_, distorted_images_rank_)\n\n  def testSSDRandomCropFixedAspectRatio(self):\n    self._testSSDRandomCropFixedAspectRatio(include_label_scores=False,\n                                            include_instance_masks=False,\n                                            include_keypoints=False)\n\n  def testSSDRandomCropFixedAspectRatioWithMasksAndKeypoints(self):\n    self._testSSDRandomCropFixedAspectRatio(include_label_scores=False,\n                                            include_instance_masks=True,\n                                            include_keypoints=True)\n\n  def testSSDRandomCropFixedAspectRatioWithLabelScoresMasksAndKeypoints(self):\n    self._testSSDRandomCropFixedAspectRatio(include_label_scores=True,\n                                            include_instance_masks=True,\n                                            include_keypoints=True)\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
src/object_detection/core/region_similarity_calculator.py,1,"b'# Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n""""""Region Similarity Calculators for BoxLists.\n\nRegion Similarity Calculators compare a pairwise measure of similarity\nbetween the boxes in two BoxLists.\n""""""\nfrom abc import ABCMeta\nfrom abc import abstractmethod\n\nimport tensorflow as tf\n\nfrom object_detection.core import box_list_ops\n\n\nclass RegionSimilarityCalculator(object):\n  """"""Abstract base class for region similarity calculator.""""""\n  __metaclass__ = ABCMeta\n\n  def compare(self, boxlist1, boxlist2, scope=None):\n    """"""Computes matrix of pairwise similarity between BoxLists.\n\n    This op (to be overriden) computes a measure of pairwise similarity between\n    the boxes in the given BoxLists. Higher values indicate more similarity.\n\n    Note that this method simply measures similarity and does not explicitly\n    perform a matching.\n\n    Args:\n      boxlist1: BoxList holding N boxes.\n      boxlist2: BoxList holding M boxes.\n      scope: Op scope name. Defaults to \'Compare\' if None.\n\n    Returns:\n      a (float32) tensor of shape [N, M] with pairwise similarity score.\n    """"""\n    with tf.name_scope(scope, \'Compare\', [boxlist1, boxlist2]) as scope:\n      return self._compare(boxlist1, boxlist2)\n\n  @abstractmethod\n  def _compare(self, boxlist1, boxlist2):\n    pass\n\n\nclass IouSimilarity(RegionSimilarityCalculator):\n  """"""Class to compute similarity based on Intersection over Union (IOU) metric.\n\n  This class computes pairwise similarity between two BoxLists based on IOU.\n  """"""\n\n  def _compare(self, boxlist1, boxlist2):\n    """"""Compute pairwise IOU similarity between the two BoxLists.\n\n    Args:\n      boxlist1: BoxList holding N boxes.\n      boxlist2: BoxList holding M boxes.\n\n    Returns:\n      A tensor with shape [N, M] representing pairwise iou scores.\n    """"""\n    return box_list_ops.iou(boxlist1, boxlist2)\n\n\nclass NegSqDistSimilarity(RegionSimilarityCalculator):\n  """"""Class to compute similarity based on the squared distance metric.\n\n  This class computes pairwise similarity between two BoxLists based on the\n  negative squared distance metric.\n  """"""\n\n  def _compare(self, boxlist1, boxlist2):\n    """"""Compute matrix of (negated) sq distances.\n\n    Args:\n      boxlist1: BoxList holding N boxes.\n      boxlist2: BoxList holding M boxes.\n\n    Returns:\n      A tensor with shape [N, M] representing negated pairwise squared distance.\n    """"""\n    return -1 * box_list_ops.sq_dist(boxlist1, boxlist2)\n\n\nclass IoaSimilarity(RegionSimilarityCalculator):\n  """"""Class to compute similarity based on Intersection over Area (IOA) metric.\n\n  This class computes pairwise similarity between two BoxLists based on their\n  pairwise intersections divided by the areas of second BoxLists.\n  """"""\n\n  def _compare(self, boxlist1, boxlist2):\n    """"""Compute pairwise IOA similarity between the two BoxLists.\n\n    Args:\n      boxlist1: BoxList holding N boxes.\n      boxlist2: BoxList holding M boxes.\n\n    Returns:\n      A tensor with shape [N, M] representing pairwise IOA scores.\n    """"""\n    return box_list_ops.ioa(boxlist1, boxlist2)\n'"
src/object_detection/core/region_similarity_calculator_test.py,8,"b'# Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n""""""Tests for region_similarity_calculator.""""""\nimport tensorflow as tf\n\nfrom object_detection.core import box_list\nfrom object_detection.core import region_similarity_calculator\n\n\nclass RegionSimilarityCalculatorTest(tf.test.TestCase):\n\n  def test_get_correct_pairwise_similarity_based_on_iou(self):\n    corners1 = tf.constant([[4.0, 3.0, 7.0, 5.0], [5.0, 6.0, 10.0, 7.0]])\n    corners2 = tf.constant([[3.0, 4.0, 6.0, 8.0], [14.0, 14.0, 15.0, 15.0],\n                            [0.0, 0.0, 20.0, 20.0]])\n    exp_output = [[2.0 / 16.0, 0, 6.0 / 400.0], [1.0 / 16.0, 0.0, 5.0 / 400.0]]\n    boxes1 = box_list.BoxList(corners1)\n    boxes2 = box_list.BoxList(corners2)\n    iou_similarity_calculator = region_similarity_calculator.IouSimilarity()\n    iou_similarity = iou_similarity_calculator.compare(boxes1, boxes2)\n    with self.test_session() as sess:\n      iou_output = sess.run(iou_similarity)\n      self.assertAllClose(iou_output, exp_output)\n\n  def test_get_correct_pairwise_similarity_based_on_squared_distances(self):\n    corners1 = tf.constant([[0.0, 0.0, 0.0, 0.0],\n                            [1.0, 1.0, 0.0, 2.0]])\n    corners2 = tf.constant([[3.0, 4.0, 1.0, 0.0],\n                            [-4.0, 0.0, 0.0, 3.0],\n                            [0.0, 0.0, 0.0, 0.0]])\n    exp_output = [[-26, -25, 0], [-18, -27, -6]]\n    boxes1 = box_list.BoxList(corners1)\n    boxes2 = box_list.BoxList(corners2)\n    dist_similarity_calc = region_similarity_calculator.NegSqDistSimilarity()\n    dist_similarity = dist_similarity_calc.compare(boxes1, boxes2)\n    with self.test_session() as sess:\n      dist_output = sess.run(dist_similarity)\n      self.assertAllClose(dist_output, exp_output)\n\n  def test_get_correct_pairwise_similarity_based_on_ioa(self):\n    corners1 = tf.constant([[4.0, 3.0, 7.0, 5.0], [5.0, 6.0, 10.0, 7.0]])\n    corners2 = tf.constant([[3.0, 4.0, 6.0, 8.0], [14.0, 14.0, 15.0, 15.0],\n                            [0.0, 0.0, 20.0, 20.0]])\n    exp_output_1 = [[2.0 / 12.0, 0, 6.0 / 400.0],\n                    [1.0 / 12.0, 0.0, 5.0 / 400.0]]\n    exp_output_2 = [[2.0 / 6.0, 1.0 / 5.0],\n                    [0, 0],\n                    [6.0 / 6.0, 5.0 / 5.0]]\n    boxes1 = box_list.BoxList(corners1)\n    boxes2 = box_list.BoxList(corners2)\n    ioa_similarity_calculator = region_similarity_calculator.IoaSimilarity()\n    ioa_similarity_1 = ioa_similarity_calculator.compare(boxes1, boxes2)\n    ioa_similarity_2 = ioa_similarity_calculator.compare(boxes2, boxes1)\n    with self.test_session() as sess:\n      iou_output_1, iou_output_2 = sess.run(\n          [ioa_similarity_1, ioa_similarity_2])\n      self.assertAllClose(iou_output_1, exp_output_1)\n      self.assertAllClose(iou_output_2, exp_output_2)\n\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
src/object_detection/core/standard_fields.py,0,"b'# Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n""""""Contains classes specifying naming conventions used for object detection.\n\n\nSpecifies:\n  InputDataFields: standard fields used by reader/preprocessor/batcher.\n  DetectionResultFields: standard fields returned by object detector.\n  BoxListFields: standard field used by BoxList\n  TfExampleFields: standard fields for tf-example data format (go/tf-example).\n""""""\n\n\nclass InputDataFields(object):\n  """"""Names for the input tensors.\n\n  Holds the standard data field names to use for identifying input tensors. This\n  should be used by the decoder to identify keys for the returned tensor_dict\n  containing input tensors. And it should be used by the model to identify the\n  tensors it needs.\n\n  Attributes:\n    image: image.\n    original_image: image in the original input size.\n    key: unique key corresponding to image.\n    source_id: source of the original image.\n    filename: original filename of the dataset (without common path).\n    groundtruth_image_classes: image-level class labels.\n    groundtruth_boxes: coordinates of the ground truth boxes in the image.\n    groundtruth_classes: box-level class labels.\n    groundtruth_label_types: box-level label types (e.g. explicit negative).\n    groundtruth_is_crowd: [DEPRECATED, use groundtruth_group_of instead]\n      is the groundtruth a single object or a crowd.\n    groundtruth_area: area of a groundtruth segment.\n    groundtruth_difficult: is a `difficult` object\n    groundtruth_group_of: is a `group_of` objects, e.g. multiple objects of the\n      same class, forming a connected group, where instances are heavily\n      occluding each other.\n    proposal_boxes: coordinates of object proposal boxes.\n    proposal_objectness: objectness score of each proposal.\n    groundtruth_instance_masks: ground truth instance masks.\n    groundtruth_instance_boundaries: ground truth instance boundaries.\n    groundtruth_instance_classes: instance mask-level class labels.\n    groundtruth_keypoints: ground truth keypoints.\n    groundtruth_keypoint_visibilities: ground truth keypoint visibilities.\n    groundtruth_label_scores: groundtruth label scores.\n    groundtruth_weights: groundtruth weight factor for bounding boxes.\n    num_groundtruth_boxes: number of groundtruth boxes.\n    true_image_shapes: true shapes of images in the resized images, as resized\n      images can be padded with zeros.\n  """"""\n  image = \'image\'\n  original_image = \'original_image\'\n  key = \'key\'\n  source_id = \'source_id\'\n  filename = \'filename\'\n  groundtruth_image_classes = \'groundtruth_image_classes\'\n  groundtruth_boxes = \'groundtruth_boxes\'\n  groundtruth_classes = \'groundtruth_classes\'\n  groundtruth_label_types = \'groundtruth_label_types\'\n  groundtruth_is_crowd = \'groundtruth_is_crowd\'\n  groundtruth_area = \'groundtruth_area\'\n  groundtruth_difficult = \'groundtruth_difficult\'\n  groundtruth_group_of = \'groundtruth_group_of\'\n  proposal_boxes = \'proposal_boxes\'\n  proposal_objectness = \'proposal_objectness\'\n  groundtruth_instance_masks = \'groundtruth_instance_masks\'\n  groundtruth_instance_boundaries = \'groundtruth_instance_boundaries\'\n  groundtruth_instance_classes = \'groundtruth_instance_classes\'\n  groundtruth_keypoints = \'groundtruth_keypoints\'\n  groundtruth_keypoint_visibilities = \'groundtruth_keypoint_visibilities\'\n  groundtruth_label_scores = \'groundtruth_label_scores\'\n  groundtruth_weights = \'groundtruth_weights\'\n  num_groundtruth_boxes = \'num_groundtruth_boxes\'\n  true_image_shape = \'true_image_shape\'\n\n\nclass DetectionResultFields(object):\n  """"""Naming conventions for storing the output of the detector.\n\n  Attributes:\n    source_id: source of the original image.\n    key: unique key corresponding to image.\n    detection_boxes: coordinates of the detection boxes in the image.\n    detection_scores: detection scores for the detection boxes in the image.\n    detection_classes: detection-level class labels.\n    detection_masks: contains a segmentation mask for each detection box.\n    detection_boundaries: contains an object boundary for each detection box.\n    detection_keypoints: contains detection keypoints for each detection box.\n    num_detections: number of detections in the batch.\n  """"""\n\n  source_id = \'source_id\'\n  key = \'key\'\n  detection_boxes = \'detection_boxes\'\n  detection_scores = \'detection_scores\'\n  detection_classes = \'detection_classes\'\n  detection_masks = \'detection_masks\'\n  detection_boundaries = \'detection_boundaries\'\n  detection_keypoints = \'detection_keypoints\'\n  num_detections = \'num_detections\'\n\n\nclass BoxListFields(object):\n  """"""Naming conventions for BoxLists.\n\n  Attributes:\n    boxes: bounding box coordinates.\n    classes: classes per bounding box.\n    scores: scores per bounding box.\n    weights: sample weights per bounding box.\n    objectness: objectness score per bounding box.\n    masks: masks per bounding box.\n    boundaries: boundaries per bounding box.\n    keypoints: keypoints per bounding box.\n    keypoint_heatmaps: keypoint heatmaps per bounding box.\n  """"""\n  boxes = \'boxes\'\n  classes = \'classes\'\n  scores = \'scores\'\n  weights = \'weights\'\n  objectness = \'objectness\'\n  masks = \'masks\'\n  boundaries = \'boundaries\'\n  keypoints = \'keypoints\'\n  keypoint_heatmaps = \'keypoint_heatmaps\'\n\n\nclass TfExampleFields(object):\n  """"""TF-example proto feature names for object detection.\n\n  Holds the standard feature names to load from an Example proto for object\n  detection.\n\n  Attributes:\n    image_encoded: JPEG encoded string\n    image_format: image format, e.g. ""JPEG""\n    filename: filename\n    channels: number of channels of image\n    colorspace: colorspace, e.g. ""RGB""\n    height: height of image in pixels, e.g. 462\n    width: width of image in pixels, e.g. 581\n    source_id: original source of the image\n    object_class_text: labels in text format, e.g. [""person"", ""cat""]\n    object_class_label: labels in numbers, e.g. [16, 8]\n    object_bbox_xmin: xmin coordinates of groundtruth box, e.g. 10, 30\n    object_bbox_xmax: xmax coordinates of groundtruth box, e.g. 50, 40\n    object_bbox_ymin: ymin coordinates of groundtruth box, e.g. 40, 50\n    object_bbox_ymax: ymax coordinates of groundtruth box, e.g. 80, 70\n    object_view: viewpoint of object, e.g. [""frontal"", ""left""]\n    object_truncated: is object truncated, e.g. [true, false]\n    object_occluded: is object occluded, e.g. [true, false]\n    object_difficult: is object difficult, e.g. [true, false]\n    object_group_of: is object a single object or a group of objects\n    object_depiction: is object a depiction\n    object_is_crowd: [DEPRECATED, use object_group_of instead]\n      is the object a single object or a crowd\n    object_segment_area: the area of the segment.\n    object_weight: a weight factor for the object\'s bounding box.\n    instance_masks: instance segmentation masks.\n    instance_boundaries: instance boundaries.\n    instance_classes: Classes for each instance segmentation mask.\n    detection_class_label: class label in numbers.\n    detection_bbox_ymin: ymin coordinates of a detection box.\n    detection_bbox_xmin: xmin coordinates of a detection box.\n    detection_bbox_ymax: ymax coordinates of a detection box.\n    detection_bbox_xmax: xmax coordinates of a detection box.\n    detection_score: detection score for the class label and box.\n  """"""\n  image_encoded = \'image/encoded\'\n  image_format = \'image/format\'  # format is reserved keyword\n  filename = \'image/filename\'\n  channels = \'image/channels\'\n  colorspace = \'image/colorspace\'\n  height = \'image/height\'\n  width = \'image/width\'\n  source_id = \'image/source_id\'\n  object_class_text = \'image/object/class/text\'\n  object_class_label = \'image/object/class/label\'\n  object_bbox_ymin = \'image/object/bbox/ymin\'\n  object_bbox_xmin = \'image/object/bbox/xmin\'\n  object_bbox_ymax = \'image/object/bbox/ymax\'\n  object_bbox_xmax = \'image/object/bbox/xmax\'\n  object_view = \'image/object/view\'\n  object_truncated = \'image/object/truncated\'\n  object_occluded = \'image/object/occluded\'\n  object_difficult = \'image/object/difficult\'\n  object_group_of = \'image/object/group_of\'\n  object_depiction = \'image/object/depiction\'\n  object_is_crowd = \'image/object/is_crowd\'\n  object_segment_area = \'image/object/segment/area\'\n  object_weight = \'image/object/weight\'\n  instance_masks = \'image/segmentation/object\'\n  instance_boundaries = \'image/boundaries/object\'\n  instance_classes = \'image/segmentation/object/class\'\n  detection_class_label = \'image/detection/label\'\n  detection_bbox_ymin = \'image/detection/bbox/ymin\'\n  detection_bbox_xmin = \'image/detection/bbox/xmin\'\n  detection_bbox_ymax = \'image/detection/bbox/ymax\'\n  detection_bbox_xmax = \'image/detection/bbox/xmax\'\n  detection_score = \'image/detection/score\'\n'"
src/object_detection/core/target_assigner.py,17,"b'# Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n""""""Base target assigner module.\n\nThe job of a TargetAssigner is, for a given set of anchors (bounding boxes) and\ngroundtruth detections (bounding boxes), to assign classification and regression\ntargets to each anchor as well as weights to each anchor (specifying, e.g.,\nwhich anchors should not contribute to training loss).\n\nIt assigns classification/regression targets by performing the following steps:\n1) Computing pairwise similarity between anchors and groundtruth boxes using a\n  provided RegionSimilarity Calculator\n2) Computing a matching based on the similarity matrix using a provided Matcher\n3) Assigning regression targets based on the matching and a provided BoxCoder\n4) Assigning classification targets based on the matching and groundtruth labels\n\nNote that TargetAssigners only operate on detections from a single\nimage at a time, so any logic for applying a TargetAssigner to multiple\nimages must be handled externally.\n""""""\nimport tensorflow as tf\n\nfrom object_detection.box_coders import faster_rcnn_box_coder\nfrom object_detection.box_coders import mean_stddev_box_coder\nfrom object_detection.core import box_coder as bcoder\nfrom object_detection.core import box_list\nfrom object_detection.core import matcher as mat\nfrom object_detection.core import region_similarity_calculator as sim_calc\nfrom object_detection.core import standard_fields as fields\nfrom object_detection.matchers import argmax_matcher\nfrom object_detection.matchers import bipartite_matcher\nfrom object_detection.utils import shape_utils\n\n\nclass TargetAssigner(object):\n  """"""Target assigner to compute classification and regression targets.""""""\n\n  def __init__(self, similarity_calc, matcher, box_coder,\n               negative_class_weight=1.0, unmatched_cls_target=None):\n    """"""Construct Object Detection Target Assigner.\n\n    Args:\n      similarity_calc: a RegionSimilarityCalculator\n      matcher: an object_detection.core.Matcher used to match groundtruth to\n        anchors.\n      box_coder: an object_detection.core.BoxCoder used to encode matching\n        groundtruth boxes with respect to anchors.\n      negative_class_weight: classification weight to be associated to negative\n        anchors (default: 1.0). The weight must be in [0., 1.].\n      unmatched_cls_target: a float32 tensor with shape [d_1, d_2, ..., d_k]\n        which is consistent with the classification target for each\n        anchor (and can be empty for scalar targets).  This shape must thus be\n        compatible with the groundtruth labels that are passed to the ""assign""\n        function (which have shape [num_gt_boxes, d_1, d_2, ..., d_k]).\n        If set to None, unmatched_cls_target is set to be [0] for each anchor.\n\n    Raises:\n      ValueError: if similarity_calc is not a RegionSimilarityCalculator or\n        if matcher is not a Matcher or if box_coder is not a BoxCoder\n    """"""\n    if not isinstance(similarity_calc, sim_calc.RegionSimilarityCalculator):\n      raise ValueError(\'similarity_calc must be a RegionSimilarityCalculator\')\n    if not isinstance(matcher, mat.Matcher):\n      raise ValueError(\'matcher must be a Matcher\')\n    if not isinstance(box_coder, bcoder.BoxCoder):\n      raise ValueError(\'box_coder must be a BoxCoder\')\n    self._similarity_calc = similarity_calc\n    self._matcher = matcher\n    self._box_coder = box_coder\n    self._negative_class_weight = negative_class_weight\n    if unmatched_cls_target is None:\n      self._unmatched_cls_target = tf.constant([0], tf.float32)\n    else:\n      self._unmatched_cls_target = unmatched_cls_target\n\n  @property\n  def box_coder(self):\n    return self._box_coder\n\n  def assign(self, anchors, groundtruth_boxes, groundtruth_labels=None,\n             groundtruth_weights=None, **params):\n    """"""Assign classification and regression targets to each anchor.\n\n    For a given set of anchors and groundtruth detections, match anchors\n    to groundtruth_boxes and assign classification and regression targets to\n    each anchor as well as weights based on the resulting match (specifying,\n    e.g., which anchors should not contribute to training loss).\n\n    Anchors that are not matched to anything are given a classification target\n    of self._unmatched_cls_target which can be specified via the constructor.\n\n    Args:\n      anchors: a BoxList representing N anchors\n      groundtruth_boxes: a BoxList representing M groundtruth boxes\n      groundtruth_labels:  a tensor of shape [M, d_1, ... d_k]\n        with labels for each of the ground_truth boxes. The subshape\n        [d_1, ... d_k] can be empty (corresponding to scalar inputs).  When set\n        to None, groundtruth_labels assumes a binary problem where all\n        ground_truth boxes get a positive label (of 1).\n      groundtruth_weights: a float tensor of shape [M] indicating the weight to\n        assign to all anchors match to a particular groundtruth box. The weights\n        must be in [0., 1.]. If None, all weights are set to 1.\n      **params: Additional keyword arguments for specific implementations of\n              the Matcher.\n\n    Returns:\n      cls_targets: a float32 tensor with shape [num_anchors, d_1, d_2 ... d_k],\n        where the subshape [d_1, ..., d_k] is compatible with groundtruth_labels\n        which has shape [num_gt_boxes, d_1, d_2, ... d_k].\n      cls_weights: a float32 tensor with shape [num_anchors]\n      reg_targets: a float32 tensor with shape [num_anchors, box_code_dimension]\n      reg_weights: a float32 tensor with shape [num_anchors]\n      match: a matcher.Match object encoding the match between anchors and\n        groundtruth boxes, with rows corresponding to groundtruth boxes\n        and columns corresponding to anchors.\n\n    Raises:\n      ValueError: if anchors or groundtruth_boxes are not of type\n        box_list.BoxList\n    """"""\n    if not isinstance(anchors, box_list.BoxList):\n      raise ValueError(\'anchors must be an BoxList\')\n    if not isinstance(groundtruth_boxes, box_list.BoxList):\n      raise ValueError(\'groundtruth_boxes must be an BoxList\')\n\n    if groundtruth_labels is None:\n      groundtruth_labels = tf.ones(tf.expand_dims(groundtruth_boxes.num_boxes(),\n                                                  0))\n      groundtruth_labels = tf.expand_dims(groundtruth_labels, -1)\n    unmatched_shape_assert = shape_utils.assert_shape_equal(\n        shape_utils.combined_static_and_dynamic_shape(groundtruth_labels)[1:],\n        shape_utils.combined_static_and_dynamic_shape(\n            self._unmatched_cls_target))\n    labels_and_box_shapes_assert = shape_utils.assert_shape_equal(\n        shape_utils.combined_static_and_dynamic_shape(\n            groundtruth_labels)[:1],\n        shape_utils.combined_static_and_dynamic_shape(\n            groundtruth_boxes.get())[:1])\n\n    if groundtruth_weights is None:\n      num_gt_boxes = groundtruth_boxes.num_boxes_static()\n      if not num_gt_boxes:\n        num_gt_boxes = groundtruth_boxes.num_boxes()\n      groundtruth_weights = tf.ones([num_gt_boxes], dtype=tf.float32)\n    with tf.control_dependencies(\n        [unmatched_shape_assert, labels_and_box_shapes_assert]):\n      match_quality_matrix = self._similarity_calc.compare(groundtruth_boxes,\n                                                           anchors)\n      match = self._matcher.match(match_quality_matrix, **params)\n      reg_targets = self._create_regression_targets(anchors,\n                                                    groundtruth_boxes,\n                                                    match)\n      cls_targets = self._create_classification_targets(groundtruth_labels,\n                                                        match)\n      reg_weights = self._create_regression_weights(match, groundtruth_weights)\n      cls_weights = self._create_classification_weights(match,\n                                                        groundtruth_weights)\n\n    num_anchors = anchors.num_boxes_static()\n    if num_anchors is not None:\n      reg_targets = self._reset_target_shape(reg_targets, num_anchors)\n      cls_targets = self._reset_target_shape(cls_targets, num_anchors)\n      reg_weights = self._reset_target_shape(reg_weights, num_anchors)\n      cls_weights = self._reset_target_shape(cls_weights, num_anchors)\n\n    return cls_targets, cls_weights, reg_targets, reg_weights, match\n\n  def _reset_target_shape(self, target, num_anchors):\n    """"""Sets the static shape of the target.\n\n    Args:\n      target: the target tensor. Its first dimension will be overwritten.\n      num_anchors: the number of anchors, which is used to override the target\'s\n        first dimension.\n\n    Returns:\n      A tensor with the shape info filled in.\n    """"""\n    target_shape = target.get_shape().as_list()\n    target_shape[0] = num_anchors\n    target.set_shape(target_shape)\n    return target\n\n  def _create_regression_targets(self, anchors, groundtruth_boxes, match):\n    """"""Returns a regression target for each anchor.\n\n    Args:\n      anchors: a BoxList representing N anchors\n      groundtruth_boxes: a BoxList representing M groundtruth_boxes\n      match: a matcher.Match object\n\n    Returns:\n      reg_targets: a float32 tensor with shape [N, box_code_dimension]\n    """"""\n    matched_gt_boxes = match.gather_based_on_match(\n        groundtruth_boxes.get(),\n        unmatched_value=tf.zeros(4),\n        ignored_value=tf.zeros(4))\n    matched_gt_boxlist = box_list.BoxList(matched_gt_boxes)\n    if groundtruth_boxes.has_field(fields.BoxListFields.keypoints):\n      groundtruth_keypoints = groundtruth_boxes.get_field(\n          fields.BoxListFields.keypoints)\n      matched_keypoints = match.gather_based_on_match(\n          groundtruth_keypoints,\n          unmatched_value=tf.zeros(groundtruth_keypoints.get_shape()[1:]),\n          ignored_value=tf.zeros(groundtruth_keypoints.get_shape()[1:]))\n      matched_gt_boxlist.add_field(fields.BoxListFields.keypoints,\n                                   matched_keypoints)\n    matched_reg_targets = self._box_coder.encode(matched_gt_boxlist, anchors)\n    match_results_shape = shape_utils.combined_static_and_dynamic_shape(\n        match.match_results)\n\n    # Zero out the unmatched and ignored regression targets.\n    unmatched_ignored_reg_targets = tf.tile(\n        self._default_regression_target(), [match_results_shape[0], 1])\n    matched_anchors_mask = match.matched_column_indicator()\n    reg_targets = tf.where(matched_anchors_mask,\n                           matched_reg_targets,\n                           unmatched_ignored_reg_targets)\n    return reg_targets\n\n  def _default_regression_target(self):\n    """"""Returns the default target for anchors to regress to.\n\n    Default regression targets are set to zero (though in\n    this implementation what these targets are set to should\n    not matter as the regression weight of any box set to\n    regress to the default target is zero).\n\n    Returns:\n      default_target: a float32 tensor with shape [1, box_code_dimension]\n    """"""\n    return tf.constant([self._box_coder.code_size*[0]], tf.float32)\n\n  def _create_classification_targets(self, groundtruth_labels, match):\n    """"""Create classification targets for each anchor.\n\n    Assign a classification target of for each anchor to the matching\n    groundtruth label that is provided by match.  Anchors that are not matched\n    to anything are given the target self._unmatched_cls_target\n\n    Args:\n      groundtruth_labels:  a tensor of shape [num_gt_boxes, d_1, ... d_k]\n        with labels for each of the ground_truth boxes. The subshape\n        [d_1, ... d_k] can be empty (corresponding to scalar labels).\n      match: a matcher.Match object that provides a matching between anchors\n        and groundtruth boxes.\n\n    Returns:\n      a float32 tensor with shape [num_anchors, d_1, d_2 ... d_k], where the\n      subshape [d_1, ..., d_k] is compatible with groundtruth_labels which has\n      shape [num_gt_boxes, d_1, d_2, ... d_k].\n    """"""\n    return match.gather_based_on_match(\n        groundtruth_labels,\n        unmatched_value=self._unmatched_cls_target,\n        ignored_value=self._unmatched_cls_target)\n\n  def _create_regression_weights(self, match, groundtruth_weights):\n    """"""Set regression weight for each anchor.\n\n    Only positive anchors are set to contribute to the regression loss, so this\n    method returns a weight of 1 for every positive anchor and 0 for every\n    negative anchor.\n\n    Args:\n      match: a matcher.Match object that provides a matching between anchors\n        and groundtruth boxes.\n      groundtruth_weights: a float tensor of shape [M] indicating the weight to\n        assign to all anchors match to a particular groundtruth box.\n\n    Returns:\n      a float32 tensor with shape [num_anchors] representing regression weights.\n    """"""\n    return match.gather_based_on_match(\n        groundtruth_weights, ignored_value=0., unmatched_value=0.)\n\n  def _create_classification_weights(self,\n                                     match,\n                                     groundtruth_weights):\n    """"""Create classification weights for each anchor.\n\n    Positive (matched) anchors are associated with a weight of\n    positive_class_weight and negative (unmatched) anchors are associated with\n    a weight of negative_class_weight. When anchors are ignored, weights are set\n    to zero. By default, both positive/negative weights are set to 1.0,\n    but they can be adjusted to handle class imbalance (which is almost always\n    the case in object detection).\n\n    Args:\n      match: a matcher.Match object that provides a matching between anchors\n        and groundtruth boxes.\n      groundtruth_weights: a float tensor of shape [M] indicating the weight to\n        assign to all anchors match to a particular groundtruth box.\n\n    Returns:\n      a float32 tensor with shape [num_anchors] representing classification\n      weights.\n    """"""\n    return match.gather_based_on_match(\n        groundtruth_weights,\n        ignored_value=0.,\n        unmatched_value=self._negative_class_weight)\n\n  def get_box_coder(self):\n    """"""Get BoxCoder of this TargetAssigner.\n\n    Returns:\n      BoxCoder object.\n    """"""\n    return self._box_coder\n\n\n# TODO(rathodv): This method pulls in all the implementation dependencies into\n# core. Therefore its best to have this factory method outside of core.\ndef create_target_assigner(reference, stage=None,\n                           negative_class_weight=1.0,\n                           unmatched_cls_target=None):\n  """"""Factory function for creating standard target assigners.\n\n  Args:\n    reference: string referencing the type of TargetAssigner.\n    stage: string denoting stage: {proposal, detection}.\n    negative_class_weight: classification weight to be associated to negative\n      anchors (default: 1.0)\n    unmatched_cls_target: a float32 tensor with shape [d_1, d_2, ..., d_k]\n      which is consistent with the classification target for each\n      anchor (and can be empty for scalar targets).  This shape must thus be\n      compatible with the groundtruth labels that are passed to the Assign\n      function (which have shape [num_gt_boxes, d_1, d_2, ..., d_k]).\n      If set to None, unmatched_cls_target is set to be 0 for each anchor.\n\n  Returns:\n    TargetAssigner: desired target assigner.\n\n  Raises:\n    ValueError: if combination reference+stage is invalid.\n  """"""\n  if reference == \'Multibox\' and stage == \'proposal\':\n    similarity_calc = sim_calc.NegSqDistSimilarity()\n    matcher = bipartite_matcher.GreedyBipartiteMatcher()\n    box_coder = mean_stddev_box_coder.MeanStddevBoxCoder()\n\n  elif reference == \'FasterRCNN\' and stage == \'proposal\':\n    similarity_calc = sim_calc.IouSimilarity()\n    matcher = argmax_matcher.ArgMaxMatcher(matched_threshold=0.7,\n                                           unmatched_threshold=0.3,\n                                           force_match_for_each_row=True)\n    box_coder = faster_rcnn_box_coder.FasterRcnnBoxCoder(\n        scale_factors=[10.0, 10.0, 5.0, 5.0])\n\n  elif reference == \'FasterRCNN\' and stage == \'detection\':\n    similarity_calc = sim_calc.IouSimilarity()\n    # Uses all proposals with IOU < 0.5 as candidate negatives.\n    matcher = argmax_matcher.ArgMaxMatcher(matched_threshold=0.5,\n                                           negatives_lower_than_unmatched=True)\n    box_coder = faster_rcnn_box_coder.FasterRcnnBoxCoder(\n        scale_factors=[10.0, 10.0, 5.0, 5.0])\n\n  elif reference == \'FastRCNN\':\n    similarity_calc = sim_calc.IouSimilarity()\n    matcher = argmax_matcher.ArgMaxMatcher(matched_threshold=0.5,\n                                           unmatched_threshold=0.1,\n                                           force_match_for_each_row=False,\n                                           negatives_lower_than_unmatched=False)\n    box_coder = faster_rcnn_box_coder.FasterRcnnBoxCoder()\n\n  else:\n    raise ValueError(\'No valid combination of reference and stage.\')\n\n  return TargetAssigner(similarity_calc, matcher, box_coder,\n                        negative_class_weight=negative_class_weight,\n                        unmatched_cls_target=unmatched_cls_target)\n\n\ndef batch_assign_targets(target_assigner,\n                         anchors_batch,\n                         gt_box_batch,\n                         gt_class_targets_batch,\n                         gt_weights_batch=None):\n  """"""Batched assignment of classification and regression targets.\n\n  Args:\n    target_assigner: a target assigner.\n    anchors_batch: BoxList representing N box anchors or list of BoxList objects\n      with length batch_size representing anchor sets.\n    gt_box_batch: a list of BoxList objects with length batch_size\n      representing groundtruth boxes for each image in the batch\n    gt_class_targets_batch: a list of tensors with length batch_size, where\n      each tensor has shape [num_gt_boxes_i, classification_target_size] and\n      num_gt_boxes_i is the number of boxes in the ith boxlist of\n      gt_box_batch.\n    gt_weights_batch: A list of 1-D tf.float32 tensors of shape\n      [num_boxes] containing weights for groundtruth boxes.\n\n  Returns:\n    batch_cls_targets: a tensor with shape [batch_size, num_anchors,\n      num_classes],\n    batch_cls_weights: a tensor with shape [batch_size, num_anchors],\n    batch_reg_targets: a tensor with shape [batch_size, num_anchors,\n      box_code_dimension]\n    batch_reg_weights: a tensor with shape [batch_size, num_anchors],\n    match_list: a list of matcher.Match objects encoding the match between\n      anchors and groundtruth boxes for each image of the batch,\n      with rows of the Match objects corresponding to groundtruth boxes\n      and columns corresponding to anchors.\n  Raises:\n    ValueError: if input list lengths are inconsistent, i.e.,\n      batch_size == len(gt_box_batch) == len(gt_class_targets_batch)\n        and batch_size == len(anchors_batch) unless anchors_batch is a single\n        BoxList.\n  """"""\n  if not isinstance(anchors_batch, list):\n    anchors_batch = len(gt_box_batch) * [anchors_batch]\n  if not all(\n      isinstance(anchors, box_list.BoxList) for anchors in anchors_batch):\n    raise ValueError(\'anchors_batch must be a BoxList or list of BoxLists.\')\n  if not (len(anchors_batch)\n          == len(gt_box_batch)\n          == len(gt_class_targets_batch)):\n    raise ValueError(\'batch size incompatible with lengths of anchors_batch, \'\n                     \'gt_box_batch and gt_class_targets_batch.\')\n  cls_targets_list = []\n  cls_weights_list = []\n  reg_targets_list = []\n  reg_weights_list = []\n  match_list = []\n  if gt_weights_batch is None:\n    gt_weights_batch = [None] * len(gt_class_targets_batch)\n  for anchors, gt_boxes, gt_class_targets, gt_weights in zip(\n      anchors_batch, gt_box_batch, gt_class_targets_batch, gt_weights_batch):\n    (cls_targets, cls_weights, reg_targets,\n     reg_weights, match) = target_assigner.assign(\n         anchors, gt_boxes, gt_class_targets, gt_weights)\n    cls_targets_list.append(cls_targets)\n    cls_weights_list.append(cls_weights)\n    reg_targets_list.append(reg_targets)\n    reg_weights_list.append(reg_weights)\n    match_list.append(match)\n  batch_cls_targets = tf.stack(cls_targets_list)\n  batch_cls_weights = tf.stack(cls_weights_list)\n  batch_reg_targets = tf.stack(reg_targets_list)\n  batch_reg_weights = tf.stack(reg_weights_list)\n  return (batch_cls_targets, batch_cls_weights, batch_reg_targets,\n          batch_reg_weights, match_list)\n'"
src/object_detection/core/target_assigner_test.py,24,"b'# Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n""""""Tests for object_detection.core.target_assigner.""""""\nimport numpy as np\nimport tensorflow as tf\n\nfrom object_detection.box_coders import keypoint_box_coder\nfrom object_detection.box_coders import mean_stddev_box_coder\nfrom object_detection.core import box_list\nfrom object_detection.core import region_similarity_calculator\nfrom object_detection.core import standard_fields as fields\nfrom object_detection.core import target_assigner as targetassigner\nfrom object_detection.matchers import argmax_matcher\nfrom object_detection.matchers import bipartite_matcher\nfrom object_detection.utils import test_case\n\n\nclass TargetAssignerTest(test_case.TestCase):\n\n  def test_assign_agnostic(self):\n    def graph_fn(anchor_means, anchor_stddevs, groundtruth_box_corners):\n      similarity_calc = region_similarity_calculator.IouSimilarity()\n      matcher = argmax_matcher.ArgMaxMatcher(matched_threshold=0.5,\n                                             unmatched_threshold=0.5)\n      box_coder = mean_stddev_box_coder.MeanStddevBoxCoder()\n      target_assigner = targetassigner.TargetAssigner(\n          similarity_calc, matcher, box_coder, unmatched_cls_target=None)\n      anchors_boxlist = box_list.BoxList(anchor_means)\n      anchors_boxlist.add_field(\'stddev\', anchor_stddevs)\n      groundtruth_boxlist = box_list.BoxList(groundtruth_box_corners)\n      result = target_assigner.assign(anchors_boxlist, groundtruth_boxlist)\n      (cls_targets, cls_weights, reg_targets, reg_weights, _) = result\n      return (cls_targets, cls_weights, reg_targets, reg_weights)\n\n    anchor_means = np.array([[0.0, 0.0, 0.5, 0.5],\n                             [0.5, 0.5, 1.0, 0.8],\n                             [0, 0.5, .5, 1.0]], dtype=np.float32)\n    anchor_stddevs = np.array(3 * [4 * [.1]], dtype=np.float32)\n    groundtruth_box_corners = np.array([[0.0, 0.0, 0.5, 0.5],\n                                        [0.5, 0.5, 0.9, 0.9]],\n                                       dtype=np.float32)\n    exp_cls_targets = [[1], [1], [0]]\n    exp_cls_weights = [1, 1, 1]\n    exp_reg_targets = [[0, 0, 0, 0],\n                       [0, 0, -1, 1],\n                       [0, 0, 0, 0]]\n    exp_reg_weights = [1, 1, 0]\n\n    (cls_targets_out, cls_weights_out, reg_targets_out,\n     reg_weights_out) = self.execute(graph_fn, [anchor_means, anchor_stddevs,\n                                                groundtruth_box_corners])\n    self.assertAllClose(cls_targets_out, exp_cls_targets)\n    self.assertAllClose(cls_weights_out, exp_cls_weights)\n    self.assertAllClose(reg_targets_out, exp_reg_targets)\n    self.assertAllClose(reg_weights_out, exp_reg_weights)\n    self.assertEquals(cls_targets_out.dtype, np.float32)\n    self.assertEquals(cls_weights_out.dtype, np.float32)\n    self.assertEquals(reg_targets_out.dtype, np.float32)\n    self.assertEquals(reg_weights_out.dtype, np.float32)\n\n  def test_assign_class_agnostic_with_ignored_matches(self):\n    # Note: test is very similar to above. The third box matched with an IOU\n    # of 0.35, which is between the matched and unmatched threshold. This means\n    # That like above the expected classification targets are [1, 1, 0].\n    # Unlike above, the third target is ignored and therefore expected\n    # classification weights are [1, 1, 0].\n    def graph_fn(anchor_means, anchor_stddevs, groundtruth_box_corners):\n      similarity_calc = region_similarity_calculator.IouSimilarity()\n      matcher = argmax_matcher.ArgMaxMatcher(matched_threshold=0.5,\n                                             unmatched_threshold=0.3)\n      box_coder = mean_stddev_box_coder.MeanStddevBoxCoder()\n      target_assigner = targetassigner.TargetAssigner(\n          similarity_calc, matcher, box_coder, unmatched_cls_target=None)\n      anchors_boxlist = box_list.BoxList(anchor_means)\n      anchors_boxlist.add_field(\'stddev\', anchor_stddevs)\n      groundtruth_boxlist = box_list.BoxList(groundtruth_box_corners)\n      result = target_assigner.assign(anchors_boxlist, groundtruth_boxlist)\n      (cls_targets, cls_weights, reg_targets, reg_weights, _) = result\n      return (cls_targets, cls_weights, reg_targets, reg_weights)\n\n    anchor_means = np.array([[0.0, 0.0, 0.5, 0.5],\n                             [0.5, 0.5, 1.0, 0.8],\n                             [0.0, 0.5, .9, 1.0]], dtype=np.float32)\n    anchor_stddevs = np.array(3 * [4 * [.1]], dtype=np.float32)\n    groundtruth_box_corners = np.array([[0.0, 0.0, 0.5, 0.5],\n                                        [0.5, 0.5, 0.9, 0.9]], dtype=np.float32)\n    exp_cls_targets = [[1], [1], [0]]\n    exp_cls_weights = [1, 1, 0]\n    exp_reg_targets = [[0, 0, 0, 0],\n                       [0, 0, -1, 1],\n                       [0, 0, 0, 0]]\n    exp_reg_weights = [1, 1, 0]\n    (cls_targets_out, cls_weights_out, reg_targets_out,\n     reg_weights_out) = self.execute(graph_fn, [anchor_means, anchor_stddevs,\n                                                groundtruth_box_corners])\n    self.assertAllClose(cls_targets_out, exp_cls_targets)\n    self.assertAllClose(cls_weights_out, exp_cls_weights)\n    self.assertAllClose(reg_targets_out, exp_reg_targets)\n    self.assertAllClose(reg_weights_out, exp_reg_weights)\n    self.assertEquals(cls_targets_out.dtype, np.float32)\n    self.assertEquals(cls_weights_out.dtype, np.float32)\n    self.assertEquals(reg_targets_out.dtype, np.float32)\n    self.assertEquals(reg_weights_out.dtype, np.float32)\n\n  def test_assign_agnostic_with_keypoints(self):\n    def graph_fn(anchor_means, groundtruth_box_corners,\n                 groundtruth_keypoints):\n      similarity_calc = region_similarity_calculator.IouSimilarity()\n      matcher = argmax_matcher.ArgMaxMatcher(matched_threshold=0.5,\n                                             unmatched_threshold=0.5)\n      box_coder = keypoint_box_coder.KeypointBoxCoder(\n          num_keypoints=6, scale_factors=[10.0, 10.0, 5.0, 5.0])\n      target_assigner = targetassigner.TargetAssigner(\n          similarity_calc, matcher, box_coder, unmatched_cls_target=None)\n      anchors_boxlist = box_list.BoxList(anchor_means)\n      groundtruth_boxlist = box_list.BoxList(groundtruth_box_corners)\n      groundtruth_boxlist.add_field(fields.BoxListFields.keypoints,\n                                    groundtruth_keypoints)\n      result = target_assigner.assign(anchors_boxlist, groundtruth_boxlist)\n      (cls_targets, cls_weights, reg_targets, reg_weights, _) = result\n      return (cls_targets, cls_weights, reg_targets, reg_weights)\n\n    anchor_means = np.array([[0.0, 0.0, 0.5, 0.5],\n                             [0.5, 0.5, 1.0, 1.0],\n                             [0.0, 0.5, .9, 1.0]], dtype=np.float32)\n    groundtruth_box_corners = np.array([[0.0, 0.0, 0.5, 0.5],\n                                        [0.45, 0.45, 0.95, 0.95]],\n                                       dtype=np.float32)\n    groundtruth_keypoints = np.array(\n        [[[0.1, 0.2], [0.1, 0.3], [0.2, 0.2], [0.2, 0.2], [0.1, 0.1], [0.9, 0]],\n         [[0, 0.3], [0.2, 0.4], [0.5, 0.6], [0, 0.6], [0.8, 0.2], [0.2, 0.4]]],\n        dtype=np.float32)\n    exp_cls_targets = [[1], [1], [0]]\n    exp_cls_weights = [1, 1, 1]\n    exp_reg_targets = [[0, 0, 0, 0, -3, -1, -3, 1, -1, -1, -1, -1, -3, -3, 13,\n                        -5],\n                       [-1, -1, 0, 0, -15, -9, -11, -7, -5, -3, -15, -3, 1, -11,\n                        -11, -7],\n                       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]\n    exp_reg_weights = [1, 1, 0]\n    (cls_targets_out, cls_weights_out, reg_targets_out,\n     reg_weights_out) = self.execute(graph_fn, [anchor_means,\n                                                groundtruth_box_corners,\n                                                groundtruth_keypoints])\n    self.assertAllClose(cls_targets_out, exp_cls_targets)\n    self.assertAllClose(cls_weights_out, exp_cls_weights)\n    self.assertAllClose(reg_targets_out, exp_reg_targets)\n    self.assertAllClose(reg_weights_out, exp_reg_weights)\n    self.assertEquals(cls_targets_out.dtype, np.float32)\n    self.assertEquals(cls_weights_out.dtype, np.float32)\n    self.assertEquals(reg_targets_out.dtype, np.float32)\n    self.assertEquals(reg_weights_out.dtype, np.float32)\n\n  def test_assign_class_agnostic_with_keypoints_and_ignored_matches(self):\n    # Note: test is very similar to above. The third box matched with an IOU\n    # of 0.35, which is between the matched and unmatched threshold. This means\n    # That like above the expected classification targets are [1, 1, 0].\n    # Unlike above, the third target is ignored and therefore expected\n    # classification weights are [1, 1, 0].\n    def graph_fn(anchor_means, groundtruth_box_corners,\n                 groundtruth_keypoints):\n      similarity_calc = region_similarity_calculator.IouSimilarity()\n      matcher = argmax_matcher.ArgMaxMatcher(matched_threshold=0.5,\n                                             unmatched_threshold=0.5)\n      box_coder = keypoint_box_coder.KeypointBoxCoder(\n          num_keypoints=6, scale_factors=[10.0, 10.0, 5.0, 5.0])\n      target_assigner = targetassigner.TargetAssigner(\n          similarity_calc, matcher, box_coder, unmatched_cls_target=None)\n      anchors_boxlist = box_list.BoxList(anchor_means)\n      groundtruth_boxlist = box_list.BoxList(groundtruth_box_corners)\n      groundtruth_boxlist.add_field(fields.BoxListFields.keypoints,\n                                    groundtruth_keypoints)\n      result = target_assigner.assign(anchors_boxlist, groundtruth_boxlist)\n      (cls_targets, cls_weights, reg_targets, reg_weights, _) = result\n      return (cls_targets, cls_weights, reg_targets, reg_weights)\n\n    anchor_means = np.array([[0.0, 0.0, 0.5, 0.5],\n                             [0.5, 0.5, 1.0, 1.0],\n                             [0.0, 0.5, .9, 1.0]], dtype=np.float32)\n    groundtruth_box_corners = np.array([[0.0, 0.0, 0.5, 0.5],\n                                        [0.45, 0.45, 0.95, 0.95]],\n                                       dtype=np.float32)\n    groundtruth_keypoints = np.array(\n        [[[0.1, 0.2], [0.1, 0.3], [0.2, 0.2], [0.2, 0.2], [0.1, 0.1], [0.9, 0]],\n         [[0, 0.3], [0.2, 0.4], [0.5, 0.6], [0, 0.6], [0.8, 0.2], [0.2, 0.4]]],\n        dtype=np.float32)\n    exp_cls_targets = [[1], [1], [0]]\n    exp_cls_weights = [1, 1, 1]\n    exp_reg_targets = [[0, 0, 0, 0, -3, -1, -3, 1, -1, -1, -1, -1, -3, -3, 13,\n                        -5],\n                       [-1, -1, 0, 0, -15, -9, -11, -7, -5, -3, -15, -3, 1, -11,\n                        -11, -7],\n                       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]\n    exp_reg_weights = [1, 1, 0]\n    (cls_targets_out, cls_weights_out, reg_targets_out,\n     reg_weights_out) = self.execute(graph_fn, [anchor_means,\n                                                groundtruth_box_corners,\n                                                groundtruth_keypoints])\n    self.assertAllClose(cls_targets_out, exp_cls_targets)\n    self.assertAllClose(cls_weights_out, exp_cls_weights)\n    self.assertAllClose(reg_targets_out, exp_reg_targets)\n    self.assertAllClose(reg_weights_out, exp_reg_weights)\n    self.assertEquals(cls_targets_out.dtype, np.float32)\n    self.assertEquals(cls_weights_out.dtype, np.float32)\n    self.assertEquals(reg_targets_out.dtype, np.float32)\n    self.assertEquals(reg_weights_out.dtype, np.float32)\n\n  def test_assign_multiclass(self):\n    def graph_fn(anchor_means, anchor_stddevs, groundtruth_box_corners,\n                 groundtruth_labels):\n      similarity_calc = region_similarity_calculator.IouSimilarity()\n      matcher = argmax_matcher.ArgMaxMatcher(matched_threshold=0.5,\n                                             unmatched_threshold=0.5)\n      box_coder = mean_stddev_box_coder.MeanStddevBoxCoder()\n      unmatched_cls_target = tf.constant([1, 0, 0, 0, 0, 0, 0], tf.float32)\n      target_assigner = targetassigner.TargetAssigner(\n          similarity_calc, matcher, box_coder,\n          unmatched_cls_target=unmatched_cls_target)\n\n      anchors_boxlist = box_list.BoxList(anchor_means)\n      anchors_boxlist.add_field(\'stddev\', anchor_stddevs)\n      groundtruth_boxlist = box_list.BoxList(groundtruth_box_corners)\n      result = target_assigner.assign(anchors_boxlist, groundtruth_boxlist,\n                                      groundtruth_labels)\n      (cls_targets, cls_weights, reg_targets, reg_weights, _) = result\n      return (cls_targets, cls_weights, reg_targets, reg_weights)\n\n    anchor_means = np.array([[0.0, 0.0, 0.5, 0.5],\n                             [0.5, 0.5, 1.0, 0.8],\n                             [0, 0.5, .5, 1.0],\n                             [.75, 0, 1.0, .25]], dtype=np.float32)\n    anchor_stddevs = np.array(4 * [4 * [.1]], dtype=np.float32)\n    groundtruth_box_corners = np.array([[0.0, 0.0, 0.5, 0.5],\n                                        [0.5, 0.5, 0.9, 0.9],\n                                        [.75, 0, .95, .27]], dtype=np.float32)\n    groundtruth_labels = np.array([[0, 1, 0, 0, 0, 0, 0],\n                                   [0, 0, 0, 0, 0, 1, 0],\n                                   [0, 0, 0, 1, 0, 0, 0]], dtype=np.float32)\n\n    exp_cls_targets = [[0, 1, 0, 0, 0, 0, 0],\n                       [0, 0, 0, 0, 0, 1, 0],\n                       [1, 0, 0, 0, 0, 0, 0],\n                       [0, 0, 0, 1, 0, 0, 0]]\n    exp_cls_weights = [1, 1, 1, 1]\n    exp_reg_targets = [[0, 0, 0, 0],\n                       [0, 0, -1, 1],\n                       [0, 0, 0, 0],\n                       [0, 0, -.5, .2]]\n    exp_reg_weights = [1, 1, 0, 1]\n\n    (cls_targets_out, cls_weights_out, reg_targets_out,\n     reg_weights_out) = self.execute(graph_fn, [anchor_means, anchor_stddevs,\n                                                groundtruth_box_corners,\n                                                groundtruth_labels])\n    self.assertAllClose(cls_targets_out, exp_cls_targets)\n    self.assertAllClose(cls_weights_out, exp_cls_weights)\n    self.assertAllClose(reg_targets_out, exp_reg_targets)\n    self.assertAllClose(reg_weights_out, exp_reg_weights)\n    self.assertEquals(cls_targets_out.dtype, np.float32)\n    self.assertEquals(cls_weights_out.dtype, np.float32)\n    self.assertEquals(reg_targets_out.dtype, np.float32)\n    self.assertEquals(reg_weights_out.dtype, np.float32)\n\n  def test_assign_multiclass_with_groundtruth_weights(self):\n    def graph_fn(anchor_means, anchor_stddevs, groundtruth_box_corners,\n                 groundtruth_labels, groundtruth_weights):\n      similarity_calc = region_similarity_calculator.IouSimilarity()\n      matcher = argmax_matcher.ArgMaxMatcher(matched_threshold=0.5,\n                                             unmatched_threshold=0.5)\n      box_coder = mean_stddev_box_coder.MeanStddevBoxCoder()\n      unmatched_cls_target = tf.constant([1, 0, 0, 0, 0, 0, 0], tf.float32)\n      target_assigner = targetassigner.TargetAssigner(\n          similarity_calc, matcher, box_coder,\n          unmatched_cls_target=unmatched_cls_target)\n\n      anchors_boxlist = box_list.BoxList(anchor_means)\n      anchors_boxlist.add_field(\'stddev\', anchor_stddevs)\n      groundtruth_boxlist = box_list.BoxList(groundtruth_box_corners)\n      result = target_assigner.assign(anchors_boxlist, groundtruth_boxlist,\n                                      groundtruth_labels,\n                                      groundtruth_weights)\n      (_, cls_weights, _, reg_weights, _) = result\n      return (cls_weights, reg_weights)\n\n    anchor_means = np.array([[0.0, 0.0, 0.5, 0.5],\n                             [0.5, 0.5, 1.0, 0.8],\n                             [0, 0.5, .5, 1.0],\n                             [.75, 0, 1.0, .25]], dtype=np.float32)\n    anchor_stddevs = np.array(4 * [4 * [.1]], dtype=np.float32)\n    groundtruth_box_corners = np.array([[0.0, 0.0, 0.5, 0.5],\n                                        [0.5, 0.5, 0.9, 0.9],\n                                        [.75, 0, .95, .27]], dtype=np.float32)\n    groundtruth_labels = np.array([[0, 1, 0, 0, 0, 0, 0],\n                                   [0, 0, 0, 0, 0, 1, 0],\n                                   [0, 0, 0, 1, 0, 0, 0]], dtype=np.float32)\n    groundtruth_weights = np.array([0.3, 0., 0.5], dtype=np.float32)\n\n    exp_cls_weights = [0.3, 0., 1, 0.5]   # background class gets weight of 1.\n    exp_reg_weights = [0.3, 0., 0., 0.5]  # background class gets weight of 0.\n\n    (cls_weights_out,\n     reg_weights_out) = self.execute(graph_fn, [anchor_means, anchor_stddevs,\n                                                groundtruth_box_corners,\n                                                groundtruth_labels,\n                                                groundtruth_weights])\n    self.assertAllClose(cls_weights_out, exp_cls_weights)\n    self.assertAllClose(reg_weights_out, exp_reg_weights)\n\n  def test_assign_multidimensional_class_targets(self):\n    def graph_fn(anchor_means, anchor_stddevs, groundtruth_box_corners,\n                 groundtruth_labels):\n      similarity_calc = region_similarity_calculator.IouSimilarity()\n      matcher = argmax_matcher.ArgMaxMatcher(matched_threshold=0.5,\n                                             unmatched_threshold=0.5)\n      box_coder = mean_stddev_box_coder.MeanStddevBoxCoder()\n\n      unmatched_cls_target = tf.constant([[0, 0], [0, 0]], tf.float32)\n      target_assigner = targetassigner.TargetAssigner(\n          similarity_calc, matcher, box_coder,\n          unmatched_cls_target=unmatched_cls_target)\n\n      anchors_boxlist = box_list.BoxList(anchor_means)\n      anchors_boxlist.add_field(\'stddev\', anchor_stddevs)\n      groundtruth_boxlist = box_list.BoxList(groundtruth_box_corners)\n      result = target_assigner.assign(anchors_boxlist, groundtruth_boxlist,\n                                      groundtruth_labels)\n      (cls_targets, cls_weights, reg_targets, reg_weights, _) = result\n      return (cls_targets, cls_weights, reg_targets, reg_weights)\n\n    anchor_means = np.array([[0.0, 0.0, 0.5, 0.5],\n                             [0.5, 0.5, 1.0, 0.8],\n                             [0, 0.5, .5, 1.0],\n                             [.75, 0, 1.0, .25]], dtype=np.float32)\n    anchor_stddevs = np.array(4 * [4 * [.1]], dtype=np.float32)\n    groundtruth_box_corners = np.array([[0.0, 0.0, 0.5, 0.5],\n                                        [0.5, 0.5, 0.9, 0.9],\n                                        [.75, 0, .95, .27]], dtype=np.float32)\n\n    groundtruth_labels = np.array([[[0, 1], [1, 0]],\n                                   [[1, 0], [0, 1]],\n                                   [[0, 1], [1, .5]]], np.float32)\n\n    exp_cls_targets = [[[0, 1], [1, 0]],\n                       [[1, 0], [0, 1]],\n                       [[0, 0], [0, 0]],\n                       [[0, 1], [1, .5]]]\n    exp_cls_weights = [1, 1, 1, 1]\n    exp_reg_targets = [[0, 0, 0, 0],\n                       [0, 0, -1, 1],\n                       [0, 0, 0, 0],\n                       [0, 0, -.5, .2]]\n    exp_reg_weights = [1, 1, 0, 1]\n    (cls_targets_out, cls_weights_out, reg_targets_out,\n     reg_weights_out) = self.execute(graph_fn, [anchor_means, anchor_stddevs,\n                                                groundtruth_box_corners,\n                                                groundtruth_labels])\n    self.assertAllClose(cls_targets_out, exp_cls_targets)\n    self.assertAllClose(cls_weights_out, exp_cls_weights)\n    self.assertAllClose(reg_targets_out, exp_reg_targets)\n    self.assertAllClose(reg_weights_out, exp_reg_weights)\n    self.assertEquals(cls_targets_out.dtype, np.float32)\n    self.assertEquals(cls_weights_out.dtype, np.float32)\n    self.assertEquals(reg_targets_out.dtype, np.float32)\n    self.assertEquals(reg_weights_out.dtype, np.float32)\n\n  def test_assign_empty_groundtruth(self):\n    def graph_fn(anchor_means, anchor_stddevs, groundtruth_box_corners,\n                 groundtruth_labels):\n      similarity_calc = region_similarity_calculator.IouSimilarity()\n      matcher = argmax_matcher.ArgMaxMatcher(matched_threshold=0.5,\n                                             unmatched_threshold=0.5)\n      box_coder = mean_stddev_box_coder.MeanStddevBoxCoder()\n      unmatched_cls_target = tf.constant([0, 0, 0], tf.float32)\n      anchors_boxlist = box_list.BoxList(anchor_means)\n      anchors_boxlist.add_field(\'stddev\', anchor_stddevs)\n      groundtruth_boxlist = box_list.BoxList(groundtruth_box_corners)\n      target_assigner = targetassigner.TargetAssigner(\n          similarity_calc, matcher, box_coder,\n          unmatched_cls_target=unmatched_cls_target)\n      result = target_assigner.assign(anchors_boxlist, groundtruth_boxlist,\n                                      groundtruth_labels)\n      (cls_targets, cls_weights, reg_targets, reg_weights, _) = result\n      return (cls_targets, cls_weights, reg_targets, reg_weights)\n\n    groundtruth_box_corners = np.zeros((0, 4), dtype=np.float32)\n    groundtruth_labels = np.zeros((0, 3), dtype=np.float32)\n    anchor_means = np.array([[0.0, 0.0, 0.5, 0.5],\n                             [0.5, 0.5, 1.0, 0.8],\n                             [0, 0.5, .5, 1.0],\n                             [.75, 0, 1.0, .25]],\n                            dtype=np.float32)\n    anchor_stddevs = np.array(4 * [4 * [.1]], dtype=np.float32)\n    exp_cls_targets = [[0, 0, 0],\n                       [0, 0, 0],\n                       [0, 0, 0],\n                       [0, 0, 0]]\n    exp_cls_weights = [1, 1, 1, 1]\n    exp_reg_targets = [[0, 0, 0, 0],\n                       [0, 0, 0, 0],\n                       [0, 0, 0, 0],\n                       [0, 0, 0, 0]]\n    exp_reg_weights = [0, 0, 0, 0]\n    (cls_targets_out, cls_weights_out, reg_targets_out,\n     reg_weights_out) = self.execute(graph_fn, [anchor_means, anchor_stddevs,\n                                                groundtruth_box_corners,\n                                                groundtruth_labels])\n    self.assertAllClose(cls_targets_out, exp_cls_targets)\n    self.assertAllClose(cls_weights_out, exp_cls_weights)\n    self.assertAllClose(reg_targets_out, exp_reg_targets)\n    self.assertAllClose(reg_weights_out, exp_reg_weights)\n    self.assertEquals(cls_targets_out.dtype, np.float32)\n    self.assertEquals(cls_weights_out.dtype, np.float32)\n    self.assertEquals(reg_targets_out.dtype, np.float32)\n    self.assertEquals(reg_weights_out.dtype, np.float32)\n\n  def test_raises_error_on_incompatible_groundtruth_boxes_and_labels(self):\n    similarity_calc = region_similarity_calculator.NegSqDistSimilarity()\n    matcher = bipartite_matcher.GreedyBipartiteMatcher()\n    box_coder = mean_stddev_box_coder.MeanStddevBoxCoder()\n    unmatched_cls_target = tf.constant([1, 0, 0, 0, 0, 0, 0], tf.float32)\n    target_assigner = targetassigner.TargetAssigner(\n        similarity_calc, matcher, box_coder,\n        unmatched_cls_target=unmatched_cls_target)\n\n    prior_means = tf.constant([[0.0, 0.0, 0.5, 0.5],\n                               [0.5, 0.5, 1.0, 0.8],\n                               [0, 0.5, .5, 1.0],\n                               [.75, 0, 1.0, .25]])\n    prior_stddevs = tf.constant(4 * [4 * [.1]])\n    priors = box_list.BoxList(prior_means)\n    priors.add_field(\'stddev\', prior_stddevs)\n\n    box_corners = [[0.0, 0.0, 0.5, 0.5],\n                   [0.0, 0.0, 0.5, 0.8],\n                   [0.5, 0.5, 0.9, 0.9],\n                   [.75, 0, .95, .27]]\n    boxes = box_list.BoxList(tf.constant(box_corners))\n\n    groundtruth_labels = tf.constant([[0, 1, 0, 0, 0, 0, 0],\n                                      [0, 0, 0, 0, 0, 1, 0],\n                                      [0, 0, 0, 1, 0, 0, 0]], tf.float32)\n    with self.assertRaisesRegexp(ValueError, \'Unequal shapes\'):\n      target_assigner.assign(priors, boxes, groundtruth_labels,\n                             num_valid_rows=3)\n\n  def test_raises_error_on_invalid_groundtruth_labels(self):\n    similarity_calc = region_similarity_calculator.NegSqDistSimilarity()\n    matcher = bipartite_matcher.GreedyBipartiteMatcher()\n    box_coder = mean_stddev_box_coder.MeanStddevBoxCoder()\n    unmatched_cls_target = tf.constant([[0, 0], [0, 0], [0, 0]], tf.float32)\n    target_assigner = targetassigner.TargetAssigner(\n        similarity_calc, matcher, box_coder,\n        unmatched_cls_target=unmatched_cls_target)\n\n    prior_means = tf.constant([[0.0, 0.0, 0.5, 0.5]])\n    prior_stddevs = tf.constant([[1.0, 1.0, 1.0, 1.0]])\n    priors = box_list.BoxList(prior_means)\n    priors.add_field(\'stddev\', prior_stddevs)\n\n    box_corners = [[0.0, 0.0, 0.5, 0.5],\n                   [0.5, 0.5, 0.9, 0.9],\n                   [.75, 0, .95, .27]]\n    boxes = box_list.BoxList(tf.constant(box_corners))\n    groundtruth_labels = tf.constant([[[0, 1], [1, 0]]], tf.float32)\n\n    with self.assertRaises(ValueError):\n      target_assigner.assign(priors, boxes, groundtruth_labels,\n                             num_valid_rows=3)\n\n\nclass BatchTargetAssignerTest(test_case.TestCase):\n\n  def _get_agnostic_target_assigner(self):\n    similarity_calc = region_similarity_calculator.IouSimilarity()\n    matcher = argmax_matcher.ArgMaxMatcher(matched_threshold=0.5,\n                                           unmatched_threshold=0.5)\n    box_coder = mean_stddev_box_coder.MeanStddevBoxCoder()\n    return targetassigner.TargetAssigner(\n        similarity_calc, matcher, box_coder,\n        unmatched_cls_target=None)\n\n  def _get_multi_class_target_assigner(self, num_classes):\n    similarity_calc = region_similarity_calculator.IouSimilarity()\n    matcher = argmax_matcher.ArgMaxMatcher(matched_threshold=0.5,\n                                           unmatched_threshold=0.5)\n    box_coder = mean_stddev_box_coder.MeanStddevBoxCoder()\n    unmatched_cls_target = tf.constant([1] + num_classes * [0], tf.float32)\n    return targetassigner.TargetAssigner(\n        similarity_calc, matcher, box_coder,\n        unmatched_cls_target=unmatched_cls_target)\n\n  def _get_multi_dimensional_target_assigner(self, target_dimensions):\n    similarity_calc = region_similarity_calculator.IouSimilarity()\n    matcher = argmax_matcher.ArgMaxMatcher(matched_threshold=0.5,\n                                           unmatched_threshold=0.5)\n    box_coder = mean_stddev_box_coder.MeanStddevBoxCoder()\n    unmatched_cls_target = tf.constant(np.zeros(target_dimensions),\n                                       tf.float32)\n    return targetassigner.TargetAssigner(\n        similarity_calc, matcher, box_coder,\n        unmatched_cls_target=unmatched_cls_target)\n\n  def test_batch_assign_targets(self):\n    def graph_fn(anchor_means, anchor_stddevs, groundtruth_boxlist1,\n                 groundtruth_boxlist2):\n      box_list1 = box_list.BoxList(groundtruth_boxlist1)\n      box_list2 = box_list.BoxList(groundtruth_boxlist2)\n      gt_box_batch = [box_list1, box_list2]\n      gt_class_targets = [None, None]\n      anchors_boxlist = box_list.BoxList(anchor_means)\n      anchors_boxlist.add_field(\'stddev\', anchor_stddevs)\n      agnostic_target_assigner = self._get_agnostic_target_assigner()\n      (cls_targets, cls_weights, reg_targets, reg_weights,\n       _) = targetassigner.batch_assign_targets(\n           agnostic_target_assigner, anchors_boxlist, gt_box_batch,\n           gt_class_targets)\n      return (cls_targets, cls_weights, reg_targets, reg_weights)\n\n    groundtruth_boxlist1 = np.array([[0., 0., 0.2, 0.2]], dtype=np.float32)\n    groundtruth_boxlist2 = np.array([[0, 0.25123152, 1, 1],\n                                     [0.015789, 0.0985, 0.55789, 0.3842]],\n                                    dtype=np.float32)\n    anchor_means = np.array([[0, 0, .25, .25],\n                             [0, .25, 1, 1],\n                             [0, .1, .5, .5],\n                             [.75, .75, 1, 1]], dtype=np.float32)\n    anchor_stddevs = np.array([[.1, .1, .1, .1],\n                               [.1, .1, .1, .1],\n                               [.1, .1, .1, .1],\n                               [.1, .1, .1, .1]], dtype=np.float32)\n\n    exp_reg_targets = [[[0, 0, -0.5, -0.5],\n                        [0, 0, 0, 0],\n                        [0, 0, 0, 0,],\n                        [0, 0, 0, 0,],],\n                       [[0, 0, 0, 0,],\n                        [0, 0.01231521, 0, 0],\n                        [0.15789001, -0.01500003, 0.57889998, -1.15799987],\n                        [0, 0, 0, 0]]]\n    exp_cls_weights = [[1, 1, 1, 1],\n                       [1, 1, 1, 1]]\n    exp_cls_targets = [[[1], [0], [0], [0]],\n                       [[0], [1], [1], [0]]]\n    exp_reg_weights = [[1, 0, 0, 0],\n                       [0, 1, 1, 0]]\n\n    (cls_targets_out, cls_weights_out, reg_targets_out,\n     reg_weights_out) = self.execute(graph_fn, [anchor_means, anchor_stddevs,\n                                                groundtruth_boxlist1,\n                                                groundtruth_boxlist2])\n    self.assertAllClose(cls_targets_out, exp_cls_targets)\n    self.assertAllClose(cls_weights_out, exp_cls_weights)\n    self.assertAllClose(reg_targets_out, exp_reg_targets)\n    self.assertAllClose(reg_weights_out, exp_reg_weights)\n\n  def test_batch_assign_multiclass_targets(self):\n    def graph_fn(anchor_means, anchor_stddevs, groundtruth_boxlist1,\n                 groundtruth_boxlist2, class_targets1, class_targets2):\n      box_list1 = box_list.BoxList(groundtruth_boxlist1)\n      box_list2 = box_list.BoxList(groundtruth_boxlist2)\n      gt_box_batch = [box_list1, box_list2]\n      gt_class_targets = [class_targets1, class_targets2]\n      anchors_boxlist = box_list.BoxList(anchor_means)\n      anchors_boxlist.add_field(\'stddev\', anchor_stddevs)\n      multiclass_target_assigner = self._get_multi_class_target_assigner(\n          num_classes=3)\n      (cls_targets, cls_weights, reg_targets, reg_weights,\n       _) = targetassigner.batch_assign_targets(\n           multiclass_target_assigner, anchors_boxlist, gt_box_batch,\n           gt_class_targets)\n      return (cls_targets, cls_weights, reg_targets, reg_weights)\n\n    groundtruth_boxlist1 = np.array([[0., 0., 0.2, 0.2]], dtype=np.float32)\n    groundtruth_boxlist2 = np.array([[0, 0.25123152, 1, 1],\n                                     [0.015789, 0.0985, 0.55789, 0.3842]],\n                                    dtype=np.float32)\n    class_targets1 = np.array([[0, 1, 0, 0]], dtype=np.float32)\n    class_targets2 = np.array([[0, 0, 0, 1],\n                               [0, 0, 1, 0]], dtype=np.float32)\n\n    anchor_means = np.array([[0, 0, .25, .25],\n                             [0, .25, 1, 1],\n                             [0, .1, .5, .5],\n                             [.75, .75, 1, 1]], dtype=np.float32)\n    anchor_stddevs = np.array([[.1, .1, .1, .1],\n                               [.1, .1, .1, .1],\n                               [.1, .1, .1, .1],\n                               [.1, .1, .1, .1]], dtype=np.float32)\n\n    exp_reg_targets = [[[0, 0, -0.5, -0.5],\n                        [0, 0, 0, 0],\n                        [0, 0, 0, 0,],\n                        [0, 0, 0, 0,],],\n                       [[0, 0, 0, 0,],\n                        [0, 0.01231521, 0, 0],\n                        [0.15789001, -0.01500003, 0.57889998, -1.15799987],\n                        [0, 0, 0, 0]]]\n    exp_cls_weights = [[1, 1, 1, 1],\n                       [1, 1, 1, 1]]\n    exp_cls_targets = [[[0, 1, 0, 0],\n                        [1, 0, 0, 0],\n                        [1, 0, 0, 0],\n                        [1, 0, 0, 0]],\n                       [[1, 0, 0, 0],\n                        [0, 0, 0, 1],\n                        [0, 0, 1, 0],\n                        [1, 0, 0, 0]]]\n    exp_reg_weights = [[1, 0, 0, 0],\n                       [0, 1, 1, 0]]\n\n    (cls_targets_out, cls_weights_out, reg_targets_out,\n     reg_weights_out) = self.execute(graph_fn, [anchor_means, anchor_stddevs,\n                                                groundtruth_boxlist1,\n                                                groundtruth_boxlist2,\n                                                class_targets1,\n                                                class_targets2])\n    self.assertAllClose(cls_targets_out, exp_cls_targets)\n    self.assertAllClose(cls_weights_out, exp_cls_weights)\n    self.assertAllClose(reg_targets_out, exp_reg_targets)\n    self.assertAllClose(reg_weights_out, exp_reg_weights)\n\n  def test_batch_assign_multiclass_targets_with_padded_groundtruth(self):\n    def graph_fn(anchor_means, anchor_stddevs, groundtruth_boxlist1,\n                 groundtruth_boxlist2, class_targets1, class_targets2,\n                 groundtruth_weights1, groundtruth_weights2):\n      box_list1 = box_list.BoxList(groundtruth_boxlist1)\n      box_list2 = box_list.BoxList(groundtruth_boxlist2)\n      gt_box_batch = [box_list1, box_list2]\n      gt_class_targets = [class_targets1, class_targets2]\n      gt_weights = [groundtruth_weights1, groundtruth_weights2]\n      anchors_boxlist = box_list.BoxList(anchor_means)\n      anchors_boxlist.add_field(\'stddev\', anchor_stddevs)\n      multiclass_target_assigner = self._get_multi_class_target_assigner(\n          num_classes=3)\n      (cls_targets, cls_weights, reg_targets, reg_weights,\n       _) = targetassigner.batch_assign_targets(\n           multiclass_target_assigner, anchors_boxlist, gt_box_batch,\n           gt_class_targets, gt_weights)\n      return (cls_targets, cls_weights, reg_targets, reg_weights)\n\n    groundtruth_boxlist1 = np.array([[0., 0., 0.2, 0.2],\n                                     [0., 0., 0., 0.]], dtype=np.float32)\n    groundtruth_weights1 = np.array([1, 0], dtype=np.float32)\n    groundtruth_boxlist2 = np.array([[0, 0.25123152, 1, 1],\n                                     [0.015789, 0.0985, 0.55789, 0.3842],\n                                     [0, 0, 0, 0]],\n                                    dtype=np.float32)\n    groundtruth_weights2 = np.array([1, 1, 0], dtype=np.float32)\n    class_targets1 = np.array([[0, 1, 0, 0], [0, 0, 0, 0]], dtype=np.float32)\n    class_targets2 = np.array([[0, 0, 0, 1],\n                               [0, 0, 1, 0],\n                               [0, 0, 0, 0]], dtype=np.float32)\n\n    anchor_means = np.array([[0, 0, .25, .25],\n                             [0, .25, 1, 1],\n                             [0, .1, .5, .5],\n                             [.75, .75, 1, 1]], dtype=np.float32)\n    anchor_stddevs = np.array([[.1, .1, .1, .1],\n                               [.1, .1, .1, .1],\n                               [.1, .1, .1, .1],\n                               [.1, .1, .1, .1]], dtype=np.float32)\n\n    exp_reg_targets = [[[0, 0, -0.5, -0.5],\n                        [0, 0, 0, 0],\n                        [0, 0, 0, 0,],\n                        [0, 0, 0, 0,],],\n                       [[0, 0, 0, 0,],\n                        [0, 0.01231521, 0, 0],\n                        [0.15789001, -0.01500003, 0.57889998, -1.15799987],\n                        [0, 0, 0, 0]]]\n    exp_cls_weights = [[1, 1, 1, 1],\n                       [1, 1, 1, 1]]\n    exp_cls_targets = [[[0, 1, 0, 0],\n                        [1, 0, 0, 0],\n                        [1, 0, 0, 0],\n                        [1, 0, 0, 0]],\n                       [[1, 0, 0, 0],\n                        [0, 0, 0, 1],\n                        [0, 0, 1, 0],\n                        [1, 0, 0, 0]]]\n    exp_reg_weights = [[1, 0, 0, 0],\n                       [0, 1, 1, 0]]\n\n    (cls_targets_out, cls_weights_out, reg_targets_out,\n     reg_weights_out) = self.execute(graph_fn, [anchor_means, anchor_stddevs,\n                                                groundtruth_boxlist1,\n                                                groundtruth_boxlist2,\n                                                class_targets1,\n                                                class_targets2,\n                                                groundtruth_weights1,\n                                                groundtruth_weights2])\n    self.assertAllClose(cls_targets_out, exp_cls_targets)\n    self.assertAllClose(cls_weights_out, exp_cls_weights)\n    self.assertAllClose(reg_targets_out, exp_reg_targets)\n    self.assertAllClose(reg_weights_out, exp_reg_weights)\n\n  def test_batch_assign_multidimensional_targets(self):\n    def graph_fn(anchor_means, anchor_stddevs, groundtruth_boxlist1,\n                 groundtruth_boxlist2, class_targets1, class_targets2):\n      box_list1 = box_list.BoxList(groundtruth_boxlist1)\n      box_list2 = box_list.BoxList(groundtruth_boxlist2)\n      gt_box_batch = [box_list1, box_list2]\n      gt_class_targets = [class_targets1, class_targets2]\n      anchors_boxlist = box_list.BoxList(anchor_means)\n      anchors_boxlist.add_field(\'stddev\', anchor_stddevs)\n      multiclass_target_assigner = self._get_multi_dimensional_target_assigner(\n          target_dimensions=(2, 3))\n      (cls_targets, cls_weights, reg_targets, reg_weights,\n       _) = targetassigner.batch_assign_targets(\n           multiclass_target_assigner, anchors_boxlist, gt_box_batch,\n           gt_class_targets)\n      return (cls_targets, cls_weights, reg_targets, reg_weights)\n\n    groundtruth_boxlist1 = np.array([[0., 0., 0.2, 0.2]], dtype=np.float32)\n    groundtruth_boxlist2 = np.array([[0, 0.25123152, 1, 1],\n                                     [0.015789, 0.0985, 0.55789, 0.3842]],\n                                    dtype=np.float32)\n    class_targets1 = np.array([[0, 1, 0, 0]], dtype=np.float32)\n    class_targets2 = np.array([[0, 0, 0, 1],\n                               [0, 0, 1, 0]], dtype=np.float32)\n    class_targets1 = np.array([[[0, 1, 1],\n                                [1, 1, 0]]], dtype=np.float32)\n    class_targets2 = np.array([[[0, 1, 1],\n                                [1, 1, 0]],\n                               [[0, 0, 1],\n                                [0, 0, 1]]], dtype=np.float32)\n\n    anchor_means = np.array([[0, 0, .25, .25],\n                             [0, .25, 1, 1],\n                             [0, .1, .5, .5],\n                             [.75, .75, 1, 1]], dtype=np.float32)\n    anchor_stddevs = np.array([[.1, .1, .1, .1],\n                               [.1, .1, .1, .1],\n                               [.1, .1, .1, .1],\n                               [.1, .1, .1, .1]], dtype=np.float32)\n\n    exp_reg_targets = [[[0, 0, -0.5, -0.5],\n                        [0, 0, 0, 0],\n                        [0, 0, 0, 0,],\n                        [0, 0, 0, 0,],],\n                       [[0, 0, 0, 0,],\n                        [0, 0.01231521, 0, 0],\n                        [0.15789001, -0.01500003, 0.57889998, -1.15799987],\n                        [0, 0, 0, 0]]]\n    exp_cls_weights = [[1, 1, 1, 1],\n                       [1, 1, 1, 1]]\n    exp_cls_targets = [[[[0., 1., 1.],\n                         [1., 1., 0.]],\n                        [[0., 0., 0.],\n                         [0., 0., 0.]],\n                        [[0., 0., 0.],\n                         [0., 0., 0.]],\n                        [[0., 0., 0.],\n                         [0., 0., 0.]]],\n                       [[[0., 0., 0.],\n                         [0., 0., 0.]],\n                        [[0., 1., 1.],\n                         [1., 1., 0.]],\n                        [[0., 0., 1.],\n                         [0., 0., 1.]],\n                        [[0., 0., 0.],\n                         [0., 0., 0.]]]]\n    exp_reg_weights = [[1, 0, 0, 0],\n                       [0, 1, 1, 0]]\n\n    (cls_targets_out, cls_weights_out, reg_targets_out,\n     reg_weights_out) = self.execute(graph_fn, [anchor_means, anchor_stddevs,\n                                                groundtruth_boxlist1,\n                                                groundtruth_boxlist2,\n                                                class_targets1,\n                                                class_targets2])\n    self.assertAllClose(cls_targets_out, exp_cls_targets)\n    self.assertAllClose(cls_weights_out, exp_cls_weights)\n    self.assertAllClose(reg_targets_out, exp_reg_targets)\n    self.assertAllClose(reg_weights_out, exp_reg_weights)\n\n  def test_batch_assign_empty_groundtruth(self):\n\n    def graph_fn(anchor_means, anchor_stddevs, groundtruth_box_corners,\n                 gt_class_targets):\n      groundtruth_boxlist = box_list.BoxList(groundtruth_box_corners)\n      gt_box_batch = [groundtruth_boxlist]\n      gt_class_targets_batch = [gt_class_targets]\n      anchors_boxlist = box_list.BoxList(anchor_means)\n      anchors_boxlist.add_field(\'stddev\', anchor_stddevs)\n\n      multiclass_target_assigner = self._get_multi_class_target_assigner(\n          num_classes=3)\n\n      (cls_targets, cls_weights, reg_targets, reg_weights,\n       _) = targetassigner.batch_assign_targets(\n           multiclass_target_assigner, anchors_boxlist,\n           gt_box_batch, gt_class_targets_batch)\n      return (cls_targets, cls_weights, reg_targets, reg_weights)\n\n    groundtruth_box_corners = np.zeros((0, 4), dtype=np.float32)\n    anchor_means = np.array([[0, 0, .25, .25],\n                             [0, .25, 1, 1]], dtype=np.float32)\n    anchor_stddevs = np.array([[.1, .1, .1, .1],\n                               [.1, .1, .1, .1]], dtype=np.float32)\n    exp_reg_targets = [[[0, 0, 0, 0],\n                        [0, 0, 0, 0]]]\n    exp_cls_weights = [[1, 1]]\n    exp_cls_targets = [[[1, 0, 0, 0],\n                        [1, 0, 0, 0]]]\n    exp_reg_weights = [[0, 0]]\n    num_classes = 3\n    pad = 1\n    gt_class_targets = np.zeros((0, num_classes + pad), dtype=np.float32)\n\n    (cls_targets_out, cls_weights_out, reg_targets_out,\n     reg_weights_out) = self.execute(\n         graph_fn, [anchor_means, anchor_stddevs, groundtruth_box_corners,\n                    gt_class_targets])\n    self.assertAllClose(cls_targets_out, exp_cls_targets)\n    self.assertAllClose(cls_weights_out, exp_cls_weights)\n    self.assertAllClose(reg_targets_out, exp_reg_targets)\n    self.assertAllClose(reg_weights_out, exp_reg_weights)\n\n\nclass CreateTargetAssignerTest(tf.test.TestCase):\n\n  def test_create_target_assigner(self):\n    """"""Tests that named constructor gives working target assigners.\n\n    TODO(rathodv): Make this test more general.\n    """"""\n    corners = [[0.0, 0.0, 1.0, 1.0]]\n    groundtruth = box_list.BoxList(tf.constant(corners))\n\n    priors = box_list.BoxList(tf.constant(corners))\n    prior_stddevs = tf.constant([[1.0, 1.0, 1.0, 1.0]])\n    priors.add_field(\'stddev\', prior_stddevs)\n    multibox_ta = (targetassigner\n                   .create_target_assigner(\'Multibox\', stage=\'proposal\'))\n    multibox_ta.assign(priors, groundtruth)\n    # No tests on output, as that may vary arbitrarily as new target assigners\n    # are added. As long as it is constructed correctly and runs without errors,\n    # tests on the individual assigners cover correctness of the assignments.\n\n    anchors = box_list.BoxList(tf.constant(corners))\n    faster_rcnn_proposals_ta = (targetassigner\n                                .create_target_assigner(\'FasterRCNN\',\n                                                        stage=\'proposal\'))\n    faster_rcnn_proposals_ta.assign(anchors, groundtruth)\n\n    fast_rcnn_ta = (targetassigner\n                    .create_target_assigner(\'FastRCNN\'))\n    fast_rcnn_ta.assign(anchors, groundtruth)\n\n    faster_rcnn_detection_ta = (targetassigner\n                                .create_target_assigner(\'FasterRCNN\',\n                                                        stage=\'detection\'))\n    faster_rcnn_detection_ta.assign(anchors, groundtruth)\n\n    with self.assertRaises(ValueError):\n      targetassigner.create_target_assigner(\'InvalidDetector\',\n                                            stage=\'invalid_stage\')\n\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
src/object_detection/data_decoders/__init__.py,0,b''
src/object_detection/data_decoders/tf_example_decoder.py,50,"b'# Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n""""""Tensorflow Example proto decoder for object detection.\n\nA decoder to decode string tensors containing serialized tensorflow.Example\nprotos for object detection.\n""""""\nimport tensorflow as tf\n\nfrom tensorflow.python.ops import array_ops\nfrom tensorflow.python.ops import control_flow_ops\nfrom tensorflow.python.ops import math_ops\nfrom object_detection.core import data_decoder\nfrom object_detection.core import standard_fields as fields\nfrom object_detection.protos import input_reader_pb2\nfrom object_detection.utils import label_map_util\n\nslim_example_decoder = tf.contrib.slim.tfexample_decoder\n\n\n# TODO(lzc): keep LookupTensor and BackupHandler in sync with\n# tf.contrib.slim.tfexample_decoder version.\nclass LookupTensor(slim_example_decoder.Tensor):\n  """"""An ItemHandler that returns a parsed Tensor, the result of a lookup.""""""\n\n  def __init__(self,\n               tensor_key,\n               table,\n               shape_keys=None,\n               shape=None,\n               default_value=\'\'):\n    """"""Initializes the LookupTensor handler.\n\n    Simply calls a vocabulary (most often, a label mapping) lookup.\n\n    Args:\n      tensor_key: the name of the `TFExample` feature to read the tensor from.\n      table: A tf.lookup table.\n      shape_keys: Optional name or list of names of the TF-Example feature in\n        which the tensor shape is stored. If a list, then each corresponds to\n        one dimension of the shape.\n      shape: Optional output shape of the `Tensor`. If provided, the `Tensor` is\n        reshaped accordingly.\n      default_value: The value used when the `tensor_key` is not found in a\n        particular `TFExample`.\n\n    Raises:\n      ValueError: if both `shape_keys` and `shape` are specified.\n    """"""\n    self._table = table\n    super(LookupTensor, self).__init__(tensor_key, shape_keys, shape,\n                                       default_value)\n\n  def tensors_to_item(self, keys_to_tensors):\n    unmapped_tensor = super(LookupTensor, self).tensors_to_item(keys_to_tensors)\n    return self._table.lookup(unmapped_tensor)\n\n\nclass BackupHandler(slim_example_decoder.ItemHandler):\n  """"""An ItemHandler that tries two ItemHandlers in order.""""""\n\n  def __init__(self, handler, backup):\n    """"""Initializes the BackupHandler handler.\n\n    If the first Handler\'s tensors_to_item returns a Tensor with no elements,\n    the second Handler is used.\n\n    Args:\n      handler: The primary ItemHandler.\n      backup: The backup ItemHandler.\n\n    Raises:\n      ValueError: if either is not an ItemHandler.\n    """"""\n    if not isinstance(handler, slim_example_decoder.ItemHandler):\n      raise ValueError(\'Primary handler is of type %s instead of ItemHandler\' %\n                       type(handler))\n    if not isinstance(backup, slim_example_decoder.ItemHandler):\n      raise ValueError(\n          \'Backup handler is of type %s instead of ItemHandler\' % type(backup))\n    self._handler = handler\n    self._backup = backup\n    super(BackupHandler, self).__init__(handler.keys + backup.keys)\n\n  def tensors_to_item(self, keys_to_tensors):\n    item = self._handler.tensors_to_item(keys_to_tensors)\n    return control_flow_ops.cond(\n        pred=math_ops.equal(math_ops.reduce_prod(array_ops.shape(item)), 0),\n        true_fn=lambda: self._backup.tensors_to_item(keys_to_tensors),\n        false_fn=lambda: item)\n\n\nclass TfExampleDecoder(data_decoder.DataDecoder):\n  """"""Tensorflow Example proto decoder.""""""\n\n  def __init__(self,\n               load_instance_masks=False,\n               instance_mask_type=input_reader_pb2.NUMERICAL_MASKS,\n               label_map_proto_file=None,\n               use_display_name=False,\n               dct_method=\'\'):\n    """"""Constructor sets keys_to_features and items_to_handlers.\n\n    Args:\n      load_instance_masks: whether or not to load and handle instance masks.\n      instance_mask_type: type of instance masks. Options are provided in\n        input_reader.proto. This is only used if `load_instance_masks` is True.\n      label_map_proto_file: a file path to a\n        object_detection.protos.StringIntLabelMap proto. If provided, then the\n        mapped IDs of \'image/object/class/text\' will take precedence over the\n        existing \'image/object/class/label\' ID.  Also, if provided, it is\n        assumed that \'image/object/class/text\' will be in the data.\n      use_display_name: whether or not to use the `display_name` for label\n        mapping (instead of `name`).  Only used if label_map_proto_file is\n        provided.\n      dct_method: An optional string. Defaults to None. It only takes\n        effect when image format is jpeg, used to specify a hint about the\n        algorithm used for jpeg decompression. Currently valid values\n        are [\'INTEGER_FAST\', \'INTEGER_ACCURATE\']. The hint may be ignored, for\n        example, the jpeg library does not have that specific option.\n\n    Raises:\n      ValueError: If `instance_mask_type` option is not one of\n        input_reader_pb2.DEFAULT, input_reader_pb2.NUMERICAL, or\n        input_reader_pb2.PNG_MASKS.\n    """"""\n    self.keys_to_features = {\n        \'image/encoded\':\n            tf.FixedLenFeature((), tf.string, default_value=\'\'),\n        \'image/format\':\n            tf.FixedLenFeature((), tf.string, default_value=\'jpeg\'),\n        \'image/filename\':\n            tf.FixedLenFeature((), tf.string, default_value=\'\'),\n        \'image/key/sha256\':\n            tf.FixedLenFeature((), tf.string, default_value=\'\'),\n        \'image/source_id\':\n            tf.FixedLenFeature((), tf.string, default_value=\'\'),\n        \'image/height\':\n            tf.FixedLenFeature((), tf.int64, 1),\n        \'image/width\':\n            tf.FixedLenFeature((), tf.int64, 1),\n        # Object boxes and classes.\n        \'image/object/bbox/xmin\':\n            tf.VarLenFeature(tf.float32),\n        \'image/object/bbox/xmax\':\n            tf.VarLenFeature(tf.float32),\n        \'image/object/bbox/ymin\':\n            tf.VarLenFeature(tf.float32),\n        \'image/object/bbox/ymax\':\n            tf.VarLenFeature(tf.float32),\n        \'image/object/class/label\':\n            tf.VarLenFeature(tf.int64),\n        \'image/object/class/text\':\n            tf.VarLenFeature(tf.string),\n        \'image/object/area\':\n            tf.VarLenFeature(tf.float32),\n        \'image/object/is_crowd\':\n            tf.VarLenFeature(tf.int64),\n        \'image/object/difficult\':\n            tf.VarLenFeature(tf.int64),\n        \'image/object/group_of\':\n            tf.VarLenFeature(tf.int64),\n        \'image/object/weight\':\n            tf.VarLenFeature(tf.float32),\n    }\n    if dct_method:\n      image = slim_example_decoder.Image(\n          image_key=\'image/encoded\',\n          format_key=\'image/format\',\n          channels=3,\n          dct_method=dct_method)\n    else:\n      image = slim_example_decoder.Image(\n          image_key=\'image/encoded\', format_key=\'image/format\', channels=3)\n    self.items_to_handlers = {\n        fields.InputDataFields.image:\n            image,\n        fields.InputDataFields.source_id: (\n            slim_example_decoder.Tensor(\'image/source_id\')),\n        fields.InputDataFields.key: (\n            slim_example_decoder.Tensor(\'image/key/sha256\')),\n        fields.InputDataFields.filename: (\n            slim_example_decoder.Tensor(\'image/filename\')),\n        # Object boxes and classes.\n        fields.InputDataFields.groundtruth_boxes: (\n            slim_example_decoder.BoundingBox([\'ymin\', \'xmin\', \'ymax\', \'xmax\'],\n                                             \'image/object/bbox/\')),\n        fields.InputDataFields.groundtruth_area:\n            slim_example_decoder.Tensor(\'image/object/area\'),\n        fields.InputDataFields.groundtruth_is_crowd: (\n            slim_example_decoder.Tensor(\'image/object/is_crowd\')),\n        fields.InputDataFields.groundtruth_difficult: (\n            slim_example_decoder.Tensor(\'image/object/difficult\')),\n        fields.InputDataFields.groundtruth_group_of: (\n            slim_example_decoder.Tensor(\'image/object/group_of\')),\n        fields.InputDataFields.groundtruth_weights: (\n            slim_example_decoder.Tensor(\'image/object/weight\')),\n    }\n    if load_instance_masks:\n      if instance_mask_type in (input_reader_pb2.DEFAULT,\n                                input_reader_pb2.NUMERICAL_MASKS):\n        self.keys_to_features[\'image/object/mask\'] = (\n            tf.VarLenFeature(tf.float32))\n        self.items_to_handlers[\n            fields.InputDataFields.groundtruth_instance_masks] = (\n                slim_example_decoder.ItemHandlerCallback(\n                    [\'image/object/mask\', \'image/height\', \'image/width\'],\n                    self._reshape_instance_masks))\n      elif instance_mask_type == input_reader_pb2.PNG_MASKS:\n        self.keys_to_features[\'image/object/mask\'] = tf.VarLenFeature(tf.string)\n        self.items_to_handlers[\n            fields.InputDataFields.groundtruth_instance_masks] = (\n                slim_example_decoder.ItemHandlerCallback(\n                    [\'image/object/mask\', \'image/height\', \'image/width\'],\n                    self._decode_png_instance_masks))\n      else:\n        raise ValueError(\'Did not recognize the `instance_mask_type` option.\')\n    if label_map_proto_file:\n      label_map = label_map_util.get_label_map_dict(label_map_proto_file,\n                                                    use_display_name)\n      # We use a default_value of -1, but we expect all labels to be contained\n      # in the label map.\n      table = tf.contrib.lookup.HashTable(\n          initializer=tf.contrib.lookup.KeyValueTensorInitializer(\n              keys=tf.constant(list(label_map.keys())),\n              values=tf.constant(list(label_map.values()), dtype=tf.int64)),\n          default_value=-1)\n      # If the label_map_proto is provided, try to use it in conjunction with\n      # the class text, and fall back to a materialized ID.\n      # TODO(lzc): note that here we are using BackupHandler defined in this\n      # file(which is branching slim_example_decoder.BackupHandler). Need to\n      # switch back to slim_example_decoder.BackupHandler once tf 1.5 becomes\n      # more popular.\n      label_handler = BackupHandler(\n          LookupTensor(\'image/object/class/text\', table, default_value=\'\'),\n          slim_example_decoder.Tensor(\'image/object/class/label\'))\n    else:\n      label_handler = slim_example_decoder.Tensor(\'image/object/class/label\')\n    self.items_to_handlers[\n        fields.InputDataFields.groundtruth_classes] = label_handler\n\n  def decode(self, tf_example_string_tensor):\n    """"""Decodes serialized tensorflow example and returns a tensor dictionary.\n\n    Args:\n      tf_example_string_tensor: a string tensor holding a serialized tensorflow\n        example proto.\n\n    Returns:\n      A dictionary of the following tensors.\n      fields.InputDataFields.image - 3D uint8 tensor of shape [None, None, 3]\n        containing image.\n      fields.InputDataFields.source_id - string tensor containing original\n        image id.\n      fields.InputDataFields.key - string tensor with unique sha256 hash key.\n      fields.InputDataFields.filename - string tensor with original dataset\n        filename.\n      fields.InputDataFields.groundtruth_boxes - 2D float32 tensor of shape\n        [None, 4] containing box corners.\n      fields.InputDataFields.groundtruth_classes - 1D int64 tensor of shape\n        [None] containing classes for the boxes.\n      fields.InputDataFields.groundtruth_weights - 1D float32 tensor of\n        shape [None] indicating the weights of groundtruth boxes.\n      fields.InputDataFields.num_groundtruth_boxes - int32 scalar indicating\n        the number of groundtruth_boxes.\n      fields.InputDataFields.groundtruth_area - 1D float32 tensor of shape\n        [None] containing containing object mask area in pixel squared.\n      fields.InputDataFields.groundtruth_is_crowd - 1D bool tensor of shape\n        [None] indicating if the boxes enclose a crowd.\n\n    Optional:\n      fields.InputDataFields.groundtruth_difficult - 1D bool tensor of shape\n        [None] indicating if the boxes represent `difficult` instances.\n      fields.InputDataFields.groundtruth_group_of - 1D bool tensor of shape\n        [None] indicating if the boxes represent `group_of` instances.\n      fields.InputDataFields.groundtruth_instance_masks - 3D float32 tensor of\n        shape [None, None, None] containing instance masks.\n    """"""\n    serialized_example = tf.reshape(tf_example_string_tensor, shape=[])\n    decoder = slim_example_decoder.TFExampleDecoder(self.keys_to_features,\n                                                    self.items_to_handlers)\n    keys = decoder.list_items()\n    tensors = decoder.decode(serialized_example, items=keys)\n    tensor_dict = dict(zip(keys, tensors))\n    is_crowd = fields.InputDataFields.groundtruth_is_crowd\n    tensor_dict[is_crowd] = tf.cast(tensor_dict[is_crowd], dtype=tf.bool)\n    tensor_dict[fields.InputDataFields.image].set_shape([None, None, 3])\n    tensor_dict[fields.InputDataFields.num_groundtruth_boxes] = tf.shape(\n        tensor_dict[fields.InputDataFields.groundtruth_boxes])[0]\n\n    def default_groundtruth_weights():\n      return tf.ones(\n          [tf.shape(tensor_dict[fields.InputDataFields.groundtruth_boxes])[0]],\n          dtype=tf.float32)\n\n    tensor_dict[fields.InputDataFields.groundtruth_weights] = tf.cond(\n        tf.greater(\n            tf.shape(\n                tensor_dict[fields.InputDataFields.groundtruth_weights])[0],\n            0), lambda: tensor_dict[fields.InputDataFields.groundtruth_weights],\n        default_groundtruth_weights)\n    return tensor_dict\n\n  def _reshape_instance_masks(self, keys_to_tensors):\n    """"""Reshape instance segmentation masks.\n\n    The instance segmentation masks are reshaped to [num_instances, height,\n    width].\n\n    Args:\n      keys_to_tensors: a dictionary from keys to tensors.\n\n    Returns:\n      A 3-D float tensor of shape [num_instances, height, width] with values\n        in {0, 1}.\n    """"""\n    height = keys_to_tensors[\'image/height\']\n    width = keys_to_tensors[\'image/width\']\n    to_shape = tf.cast(tf.stack([-1, height, width]), tf.int32)\n    masks = keys_to_tensors[\'image/object/mask\']\n    if isinstance(masks, tf.SparseTensor):\n      masks = tf.sparse_tensor_to_dense(masks)\n    masks = tf.reshape(tf.to_float(tf.greater(masks, 0.0)), to_shape)\n    return tf.cast(masks, tf.float32)\n\n  def _decode_png_instance_masks(self, keys_to_tensors):\n    """"""Decode PNG instance segmentation masks and stack into dense tensor.\n\n    The instance segmentation masks are reshaped to [num_instances, height,\n    width].\n\n    Args:\n      keys_to_tensors: a dictionary from keys to tensors.\n\n    Returns:\n      A 3-D float tensor of shape [num_instances, height, width] with values\n        in {0, 1}.\n    """"""\n\n    def decode_png_mask(image_buffer):\n      image = tf.squeeze(\n          tf.image.decode_image(image_buffer, channels=1), axis=2)\n      image.set_shape([None, None])\n      image = tf.to_float(tf.greater(image, 0))\n      return image\n\n    png_masks = keys_to_tensors[\'image/object/mask\']\n    height = keys_to_tensors[\'image/height\']\n    width = keys_to_tensors[\'image/width\']\n    if isinstance(png_masks, tf.SparseTensor):\n      png_masks = tf.sparse_tensor_to_dense(png_masks, default_value=\'\')\n    return tf.cond(\n        tf.greater(tf.size(png_masks), 0),\n        lambda: tf.map_fn(decode_png_mask, png_masks, dtype=tf.float32),\n        lambda: tf.zeros(tf.to_int32(tf.stack([0, height, width]))))\n'"
src/object_detection/data_decoders/tf_example_decoder_test.py,57,"b'# Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n""""""Tests for object_detection.data_decoders.tf_example_decoder.""""""\n\nimport os\nimport numpy as np\nimport tensorflow as tf\n\nfrom tensorflow.core.example import example_pb2\nfrom tensorflow.core.example import feature_pb2\nfrom tensorflow.python.framework import constant_op\nfrom tensorflow.python.framework import dtypes\nfrom tensorflow.python.ops import array_ops\nfrom tensorflow.python.ops import lookup_ops\nfrom tensorflow.python.ops import parsing_ops\nfrom object_detection.core import standard_fields as fields\nfrom object_detection.data_decoders import tf_example_decoder\nfrom object_detection.protos import input_reader_pb2\n\nslim_example_decoder = tf.contrib.slim.tfexample_decoder\n\n\nclass TfExampleDecoderTest(tf.test.TestCase):\n\n  def _EncodeImage(self, image_tensor, encoding_type=\'jpeg\'):\n    with self.test_session():\n      if encoding_type == \'jpeg\':\n        image_encoded = tf.image.encode_jpeg(tf.constant(image_tensor)).eval()\n      elif encoding_type == \'png\':\n        image_encoded = tf.image.encode_png(tf.constant(image_tensor)).eval()\n      else:\n        raise ValueError(\'Invalid encoding type.\')\n    return image_encoded\n\n  def _DecodeImage(self, image_encoded, encoding_type=\'jpeg\'):\n    with self.test_session():\n      if encoding_type == \'jpeg\':\n        image_decoded = tf.image.decode_jpeg(tf.constant(image_encoded)).eval()\n      elif encoding_type == \'png\':\n        image_decoded = tf.image.decode_png(tf.constant(image_encoded)).eval()\n      else:\n        raise ValueError(\'Invalid encoding type.\')\n    return image_decoded\n\n  def _Int64Feature(self, value):\n    return tf.train.Feature(int64_list=tf.train.Int64List(value=value))\n\n  def _FloatFeature(self, value):\n    return tf.train.Feature(float_list=tf.train.FloatList(value=value))\n\n  def _BytesFeature(self, value):\n    if isinstance(value, list):\n      return tf.train.Feature(bytes_list=tf.train.BytesList(value=value))\n    return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))\n\n  def _Int64FeatureFromList(self, ndarray):\n    return feature_pb2.Feature(\n        int64_list=feature_pb2.Int64List(value=ndarray.flatten().tolist()))\n\n  def _BytesFeatureFromList(self, ndarray):\n    values = ndarray.flatten().tolist()\n    for i in range(len(values)):\n      values[i] = values[i].encode(\'utf-8\')\n    return feature_pb2.Feature(bytes_list=feature_pb2.BytesList(value=values))\n\n  def testDecodeExampleWithBranchedBackupHandler(self):\n    example1 = example_pb2.Example(\n        features=feature_pb2.Features(\n            feature={\n                \'image/object/class/text\':\n                    self._BytesFeatureFromList(\n                        np.array([\'cat\', \'dog\', \'guinea pig\'])),\n                \'image/object/class/label\':\n                    self._Int64FeatureFromList(np.array([42, 10, 900]))\n            }))\n    example2 = example_pb2.Example(\n        features=feature_pb2.Features(\n            feature={\n                \'image/object/class/text\':\n                    self._BytesFeatureFromList(\n                        np.array([\'cat\', \'dog\', \'guinea pig\'])),\n            }))\n    example3 = example_pb2.Example(\n        features=feature_pb2.Features(\n            feature={\n                \'image/object/class/label\':\n                    self._Int64FeatureFromList(np.array([42, 10, 901]))\n            }))\n    # \'dog\' -> 0, \'guinea pig\' -> 1, \'cat\' -> 2\n    table = lookup_ops.index_table_from_tensor(\n        constant_op.constant([\'dog\', \'guinea pig\', \'cat\']))\n    keys_to_features = {\n        \'image/object/class/text\': parsing_ops.VarLenFeature(dtypes.string),\n        \'image/object/class/label\': parsing_ops.VarLenFeature(dtypes.int64),\n    }\n    backup_handler = tf_example_decoder.BackupHandler(\n        handler=slim_example_decoder.Tensor(\'image/object/class/label\'),\n        backup=tf_example_decoder.LookupTensor(\'image/object/class/text\',\n                                               table))\n    items_to_handlers = {\n        \'labels\': backup_handler,\n    }\n    decoder = slim_example_decoder.TFExampleDecoder(keys_to_features,\n                                                    items_to_handlers)\n    obtained_class_ids_each_example = []\n    with self.test_session() as sess:\n      sess.run(lookup_ops.tables_initializer())\n      for example in [example1, example2, example3]:\n        serialized_example = array_ops.reshape(\n            example.SerializeToString(), shape=[])\n        obtained_class_ids_each_example.append(\n            decoder.decode(serialized_example)[0].eval())\n\n    self.assertAllClose([42, 10, 900], obtained_class_ids_each_example[0])\n    self.assertAllClose([2, 0, 1], obtained_class_ids_each_example[1])\n    self.assertAllClose([42, 10, 901], obtained_class_ids_each_example[2])\n\n  def testDecodeExampleWithBranchedLookup(self):\n\n    example = example_pb2.Example(features=feature_pb2.Features(feature={\n        \'image/object/class/text\': self._BytesFeatureFromList(\n            np.array([\'cat\', \'dog\', \'guinea pig\'])),\n    }))\n    serialized_example = example.SerializeToString()\n    # \'dog\' -> 0, \'guinea pig\' -> 1, \'cat\' -> 2\n    table = lookup_ops.index_table_from_tensor(\n        constant_op.constant([\'dog\', \'guinea pig\', \'cat\']))\n\n    with self.test_session() as sess:\n      sess.run(lookup_ops.tables_initializer())\n\n      serialized_example = array_ops.reshape(serialized_example, shape=[])\n\n      keys_to_features = {\n          \'image/object/class/text\': parsing_ops.VarLenFeature(dtypes.string),\n      }\n\n      items_to_handlers = {\n          \'labels\':\n              tf_example_decoder.LookupTensor(\'image/object/class/text\', table),\n      }\n\n      decoder = slim_example_decoder.TFExampleDecoder(keys_to_features,\n                                                      items_to_handlers)\n      obtained_class_ids = decoder.decode(serialized_example)[0].eval()\n\n    self.assertAllClose([2, 0, 1], obtained_class_ids)\n\n  def testDecodeJpegImage(self):\n    image_tensor = np.random.randint(256, size=(4, 5, 3)).astype(np.uint8)\n    encoded_jpeg = self._EncodeImage(image_tensor)\n    decoded_jpeg = self._DecodeImage(encoded_jpeg)\n    example = tf.train.Example(features=tf.train.Features(feature={\n        \'image/encoded\': self._BytesFeature(encoded_jpeg),\n        \'image/format\': self._BytesFeature(\'jpeg\'),\n        \'image/source_id\': self._BytesFeature(\'image_id\'),\n    })).SerializeToString()\n\n    example_decoder = tf_example_decoder.TfExampleDecoder()\n    tensor_dict = example_decoder.decode(tf.convert_to_tensor(example))\n\n    self.assertAllEqual((tensor_dict[fields.InputDataFields.image].\n                         get_shape().as_list()), [None, None, 3])\n    with self.test_session() as sess:\n      tensor_dict = sess.run(tensor_dict)\n\n    self.assertAllEqual(decoded_jpeg, tensor_dict[fields.InputDataFields.image])\n    self.assertEqual(\'image_id\', tensor_dict[fields.InputDataFields.source_id])\n\n  def testDecodeImageKeyAndFilename(self):\n    image_tensor = np.random.randint(256, size=(4, 5, 3)).astype(np.uint8)\n    encoded_jpeg = self._EncodeImage(image_tensor)\n    example = tf.train.Example(features=tf.train.Features(feature={\n        \'image/encoded\': self._BytesFeature(encoded_jpeg),\n        \'image/key/sha256\': self._BytesFeature(\'abc\'),\n        \'image/filename\': self._BytesFeature(\'filename\')\n    })).SerializeToString()\n\n    example_decoder = tf_example_decoder.TfExampleDecoder()\n    tensor_dict = example_decoder.decode(tf.convert_to_tensor(example))\n\n    with self.test_session() as sess:\n      tensor_dict = sess.run(tensor_dict)\n\n    self.assertEqual(\'abc\', tensor_dict[fields.InputDataFields.key])\n    self.assertEqual(\'filename\', tensor_dict[fields.InputDataFields.filename])\n\n  def testDecodePngImage(self):\n    image_tensor = np.random.randint(256, size=(4, 5, 3)).astype(np.uint8)\n    encoded_png = self._EncodeImage(image_tensor, encoding_type=\'png\')\n    decoded_png = self._DecodeImage(encoded_png, encoding_type=\'png\')\n    example = tf.train.Example(features=tf.train.Features(feature={\n        \'image/encoded\': self._BytesFeature(encoded_png),\n        \'image/format\': self._BytesFeature(\'png\'),\n        \'image/source_id\': self._BytesFeature(\'image_id\')\n    })).SerializeToString()\n\n    example_decoder = tf_example_decoder.TfExampleDecoder()\n    tensor_dict = example_decoder.decode(tf.convert_to_tensor(example))\n\n    self.assertAllEqual((tensor_dict[fields.InputDataFields.image].\n                         get_shape().as_list()), [None, None, 3])\n    with self.test_session() as sess:\n      tensor_dict = sess.run(tensor_dict)\n\n    self.assertAllEqual(decoded_png, tensor_dict[fields.InputDataFields.image])\n    self.assertEqual(\'image_id\', tensor_dict[fields.InputDataFields.source_id])\n\n  def testDecodePngInstanceMasks(self):\n    image_tensor = np.random.randint(256, size=(10, 10, 3)).astype(np.uint8)\n    encoded_jpeg = self._EncodeImage(image_tensor)\n    mask_1 = np.random.randint(0, 2, size=(10, 10, 1)).astype(np.uint8)\n    mask_2 = np.random.randint(0, 2, size=(10, 10, 1)).astype(np.uint8)\n    encoded_png_1 = self._EncodeImage(mask_1, encoding_type=\'png\')\n    decoded_png_1 = np.squeeze(mask_1.astype(np.float32))\n    encoded_png_2 = self._EncodeImage(mask_2, encoding_type=\'png\')\n    decoded_png_2 = np.squeeze(mask_2.astype(np.float32))\n    encoded_masks = [encoded_png_1, encoded_png_2]\n    decoded_masks = np.stack([decoded_png_1, decoded_png_2])\n    example = tf.train.Example(\n        features=tf.train.Features(\n            feature={\n                \'image/encoded\': self._BytesFeature(encoded_jpeg),\n                \'image/format\': self._BytesFeature(\'jpeg\'),\n                \'image/object/mask\': self._BytesFeature(encoded_masks)\n            })).SerializeToString()\n\n    example_decoder = tf_example_decoder.TfExampleDecoder(\n        load_instance_masks=True, instance_mask_type=input_reader_pb2.PNG_MASKS)\n    tensor_dict = example_decoder.decode(tf.convert_to_tensor(example))\n\n    with self.test_session() as sess:\n      tensor_dict = sess.run(tensor_dict)\n\n    self.assertAllEqual(\n        decoded_masks,\n        tensor_dict[fields.InputDataFields.groundtruth_instance_masks])\n\n  def testDecodeEmptyPngInstanceMasks(self):\n    image_tensor = np.random.randint(256, size=(10, 10, 3)).astype(np.uint8)\n    encoded_jpeg = self._EncodeImage(image_tensor)\n    encoded_masks = []\n    example = tf.train.Example(\n        features=tf.train.Features(\n            feature={\n                \'image/encoded\': self._BytesFeature(encoded_jpeg),\n                \'image/format\': self._BytesFeature(\'jpeg\'),\n                \'image/object/mask\': self._BytesFeature(encoded_masks),\n                \'image/height\': self._Int64Feature([10]),\n                \'image/width\': self._Int64Feature([10]),\n            })).SerializeToString()\n\n    example_decoder = tf_example_decoder.TfExampleDecoder(\n        load_instance_masks=True, instance_mask_type=input_reader_pb2.PNG_MASKS)\n    tensor_dict = example_decoder.decode(tf.convert_to_tensor(example))\n\n    with self.test_session() as sess:\n      tensor_dict = sess.run(tensor_dict)\n      self.assertAllEqual(\n          tensor_dict[fields.InputDataFields.groundtruth_instance_masks].shape,\n          [0, 10, 10])\n\n  def testDecodeBoundingBox(self):\n    image_tensor = np.random.randint(256, size=(4, 5, 3)).astype(np.uint8)\n    encoded_jpeg = self._EncodeImage(image_tensor)\n    bbox_ymins = [0.0, 4.0]\n    bbox_xmins = [1.0, 5.0]\n    bbox_ymaxs = [2.0, 6.0]\n    bbox_xmaxs = [3.0, 7.0]\n    example = tf.train.Example(features=tf.train.Features(feature={\n        \'image/encoded\': self._BytesFeature(encoded_jpeg),\n        \'image/format\': self._BytesFeature(\'jpeg\'),\n        \'image/object/bbox/ymin\': self._FloatFeature(bbox_ymins),\n        \'image/object/bbox/xmin\': self._FloatFeature(bbox_xmins),\n        \'image/object/bbox/ymax\': self._FloatFeature(bbox_ymaxs),\n        \'image/object/bbox/xmax\': self._FloatFeature(bbox_xmaxs),\n    })).SerializeToString()\n\n    example_decoder = tf_example_decoder.TfExampleDecoder()\n    tensor_dict = example_decoder.decode(tf.convert_to_tensor(example))\n\n    self.assertAllEqual((tensor_dict[fields.InputDataFields.groundtruth_boxes].\n                         get_shape().as_list()), [None, 4])\n    with self.test_session() as sess:\n      tensor_dict = sess.run(tensor_dict)\n\n    expected_boxes = np.vstack([bbox_ymins, bbox_xmins,\n                                bbox_ymaxs, bbox_xmaxs]).transpose()\n    self.assertAllEqual(expected_boxes,\n                        tensor_dict[fields.InputDataFields.groundtruth_boxes])\n    self.assertAllEqual(\n        2, tensor_dict[fields.InputDataFields.num_groundtruth_boxes])\n\n  def testDecodeDefaultGroundtruthWeights(self):\n    image_tensor = np.random.randint(256, size=(4, 5, 3)).astype(np.uint8)\n    encoded_jpeg = self._EncodeImage(image_tensor)\n    bbox_ymins = [0.0, 4.0]\n    bbox_xmins = [1.0, 5.0]\n    bbox_ymaxs = [2.0, 6.0]\n    bbox_xmaxs = [3.0, 7.0]\n    example = tf.train.Example(features=tf.train.Features(feature={\n        \'image/encoded\': self._BytesFeature(encoded_jpeg),\n        \'image/format\': self._BytesFeature(\'jpeg\'),\n        \'image/object/bbox/ymin\': self._FloatFeature(bbox_ymins),\n        \'image/object/bbox/xmin\': self._FloatFeature(bbox_xmins),\n        \'image/object/bbox/ymax\': self._FloatFeature(bbox_ymaxs),\n        \'image/object/bbox/xmax\': self._FloatFeature(bbox_xmaxs),\n    })).SerializeToString()\n\n    example_decoder = tf_example_decoder.TfExampleDecoder()\n    tensor_dict = example_decoder.decode(tf.convert_to_tensor(example))\n\n    self.assertAllEqual((tensor_dict[fields.InputDataFields.groundtruth_boxes].\n                         get_shape().as_list()), [None, 4])\n\n    with self.test_session() as sess:\n      tensor_dict = sess.run(tensor_dict)\n\n    self.assertAllClose(tensor_dict[fields.InputDataFields.groundtruth_weights],\n                        np.ones(2, dtype=np.float32))\n\n  def testDecodeObjectLabel(self):\n    image_tensor = np.random.randint(256, size=(4, 5, 3)).astype(np.uint8)\n    encoded_jpeg = self._EncodeImage(image_tensor)\n    bbox_classes = [0, 1]\n    example = tf.train.Example(features=tf.train.Features(feature={\n        \'image/encoded\': self._BytesFeature(encoded_jpeg),\n        \'image/format\': self._BytesFeature(\'jpeg\'),\n        \'image/object/class/label\': self._Int64Feature(bbox_classes),\n    })).SerializeToString()\n\n    example_decoder = tf_example_decoder.TfExampleDecoder()\n    tensor_dict = example_decoder.decode(tf.convert_to_tensor(example))\n\n    self.assertAllEqual((tensor_dict[\n        fields.InputDataFields.groundtruth_classes].get_shape().as_list()),\n                        [None])\n\n    with self.test_session() as sess:\n      tensor_dict = sess.run(tensor_dict)\n\n    self.assertAllEqual(bbox_classes,\n                        tensor_dict[fields.InputDataFields.groundtruth_classes])\n\n  def testDecodeObjectLabelNoText(self):\n    image_tensor = np.random.randint(256, size=(4, 5, 3)).astype(np.uint8)\n    encoded_jpeg = self._EncodeImage(image_tensor)\n    bbox_classes = [1, 2]\n    example = tf.train.Example(features=tf.train.Features(feature={\n        \'image/encoded\': self._BytesFeature(encoded_jpeg),\n        \'image/format\': self._BytesFeature(\'jpeg\'),\n        \'image/object/class/label\': self._Int64Feature(bbox_classes),\n    })).SerializeToString()\n    label_map_string = """"""\n      item {\n        id:1\n        name:\'cat\'\n      }\n      item {\n        id:2\n        name:\'dog\'\n      }\n    """"""\n    label_map_path = os.path.join(self.get_temp_dir(), \'label_map.pbtxt\')\n    with tf.gfile.Open(label_map_path, \'wb\') as f:\n      f.write(label_map_string)\n\n    example_decoder = tf_example_decoder.TfExampleDecoder(\n        label_map_proto_file=label_map_path)\n    tensor_dict = example_decoder.decode(tf.convert_to_tensor(example))\n\n    self.assertAllEqual((tensor_dict[\n        fields.InputDataFields.groundtruth_classes].get_shape().as_list()),\n                        [None])\n\n    init = tf.tables_initializer()\n    with self.test_session() as sess:\n      sess.run(init)\n      tensor_dict = sess.run(tensor_dict)\n\n    self.assertAllEqual(bbox_classes,\n                        tensor_dict[fields.InputDataFields.groundtruth_classes])\n\n  def testDecodeObjectLabelUnrecognizedName(self):\n    image_tensor = np.random.randint(256, size=(4, 5, 3)).astype(np.uint8)\n    encoded_jpeg = self._EncodeImage(image_tensor)\n    bbox_classes_text = [\'cat\', \'cheetah\']\n    example = tf.train.Example(\n        features=tf.train.Features(\n            feature={\n                \'image/encoded\':\n                    self._BytesFeature(encoded_jpeg),\n                \'image/format\':\n                    self._BytesFeature(\'jpeg\'),\n                \'image/object/class/text\':\n                    self._BytesFeature(bbox_classes_text),\n            })).SerializeToString()\n\n    label_map_string = """"""\n      item {\n        id:2\n        name:\'cat\'\n      }\n      item {\n        id:1\n        name:\'dog\'\n      }\n    """"""\n    label_map_path = os.path.join(self.get_temp_dir(), \'label_map.pbtxt\')\n    with tf.gfile.Open(label_map_path, \'wb\') as f:\n      f.write(label_map_string)\n    example_decoder = tf_example_decoder.TfExampleDecoder(\n        label_map_proto_file=label_map_path)\n    tensor_dict = example_decoder.decode(tf.convert_to_tensor(example))\n\n    self.assertAllEqual((tensor_dict[fields.InputDataFields.groundtruth_classes]\n                         .get_shape().as_list()), [None])\n\n    with self.test_session() as sess:\n      sess.run(tf.tables_initializer())\n      tensor_dict = sess.run(tensor_dict)\n\n    self.assertAllEqual([2, -1],\n                        tensor_dict[fields.InputDataFields.groundtruth_classes])\n\n  def testDecodeObjectLabelWithMapping(self):\n    image_tensor = np.random.randint(256, size=(4, 5, 3)).astype(np.uint8)\n    encoded_jpeg = self._EncodeImage(image_tensor)\n    bbox_classes_text = [\'cat\', \'dog\']\n    example = tf.train.Example(\n        features=tf.train.Features(\n            feature={\n                \'image/encoded\':\n                    self._BytesFeature(encoded_jpeg),\n                \'image/format\':\n                    self._BytesFeature(\'jpeg\'),\n                \'image/object/class/text\':\n                    self._BytesFeature(bbox_classes_text),\n            })).SerializeToString()\n\n    label_map_string = """"""\n      item {\n        id:3\n        name:\'cat\'\n      }\n      item {\n        id:1\n        name:\'dog\'\n      }\n    """"""\n    label_map_path = os.path.join(self.get_temp_dir(), \'label_map.pbtxt\')\n    with tf.gfile.Open(label_map_path, \'wb\') as f:\n      f.write(label_map_string)\n    example_decoder = tf_example_decoder.TfExampleDecoder(\n        label_map_proto_file=label_map_path)\n    tensor_dict = example_decoder.decode(tf.convert_to_tensor(example))\n\n    self.assertAllEqual((tensor_dict[fields.InputDataFields.groundtruth_classes]\n                         .get_shape().as_list()), [None])\n\n    with self.test_session() as sess:\n      sess.run(tf.tables_initializer())\n      tensor_dict = sess.run(tensor_dict)\n\n    self.assertAllEqual([3, 1],\n                        tensor_dict[fields.InputDataFields.groundtruth_classes])\n\n  def testDecodeObjectArea(self):\n    image_tensor = np.random.randint(256, size=(4, 5, 3)).astype(np.uint8)\n    encoded_jpeg = self._EncodeImage(image_tensor)\n    object_area = [100., 174.]\n    example = tf.train.Example(features=tf.train.Features(feature={\n        \'image/encoded\': self._BytesFeature(encoded_jpeg),\n        \'image/format\': self._BytesFeature(\'jpeg\'),\n        \'image/object/area\': self._FloatFeature(object_area),\n    })).SerializeToString()\n\n    example_decoder = tf_example_decoder.TfExampleDecoder()\n    tensor_dict = example_decoder.decode(tf.convert_to_tensor(example))\n\n    self.assertAllEqual((tensor_dict[fields.InputDataFields.groundtruth_area].\n                         get_shape().as_list()), [None])\n    with self.test_session() as sess:\n      tensor_dict = sess.run(tensor_dict)\n\n    self.assertAllEqual(object_area,\n                        tensor_dict[fields.InputDataFields.groundtruth_area])\n\n  def testDecodeObjectIsCrowd(self):\n    image_tensor = np.random.randint(256, size=(4, 5, 3)).astype(np.uint8)\n    encoded_jpeg = self._EncodeImage(image_tensor)\n    object_is_crowd = [0, 1]\n    example = tf.train.Example(features=tf.train.Features(feature={\n        \'image/encoded\': self._BytesFeature(encoded_jpeg),\n        \'image/format\': self._BytesFeature(\'jpeg\'),\n        \'image/object/is_crowd\': self._Int64Feature(object_is_crowd),\n    })).SerializeToString()\n\n    example_decoder = tf_example_decoder.TfExampleDecoder()\n    tensor_dict = example_decoder.decode(tf.convert_to_tensor(example))\n\n    self.assertAllEqual((tensor_dict[\n        fields.InputDataFields.groundtruth_is_crowd].get_shape().as_list()),\n                        [None])\n    with self.test_session() as sess:\n      tensor_dict = sess.run(tensor_dict)\n\n    self.assertAllEqual([bool(item) for item in object_is_crowd],\n                        tensor_dict[\n                            fields.InputDataFields.groundtruth_is_crowd])\n\n  def testDecodeObjectDifficult(self):\n    image_tensor = np.random.randint(256, size=(4, 5, 3)).astype(np.uint8)\n    encoded_jpeg = self._EncodeImage(image_tensor)\n    object_difficult = [0, 1]\n    example = tf.train.Example(features=tf.train.Features(feature={\n        \'image/encoded\': self._BytesFeature(encoded_jpeg),\n        \'image/format\': self._BytesFeature(\'jpeg\'),\n        \'image/object/difficult\': self._Int64Feature(object_difficult),\n    })).SerializeToString()\n\n    example_decoder = tf_example_decoder.TfExampleDecoder()\n    tensor_dict = example_decoder.decode(tf.convert_to_tensor(example))\n\n    self.assertAllEqual((tensor_dict[\n        fields.InputDataFields.groundtruth_difficult].get_shape().as_list()),\n                        [None])\n    with self.test_session() as sess:\n      tensor_dict = sess.run(tensor_dict)\n\n    self.assertAllEqual([bool(item) for item in object_difficult],\n                        tensor_dict[\n                            fields.InputDataFields.groundtruth_difficult])\n\n  def testDecodeObjectGroupOf(self):\n    image_tensor = np.random.randint(256, size=(4, 5, 3)).astype(np.uint8)\n    encoded_jpeg = self._EncodeImage(image_tensor)\n    object_group_of = [0, 1]\n    example = tf.train.Example(features=tf.train.Features(\n        feature={\n            \'image/encoded\': self._BytesFeature(encoded_jpeg),\n            \'image/format\': self._BytesFeature(\'jpeg\'),\n            \'image/object/group_of\': self._Int64Feature(object_group_of),\n        })).SerializeToString()\n\n    example_decoder = tf_example_decoder.TfExampleDecoder()\n    tensor_dict = example_decoder.decode(tf.convert_to_tensor(example))\n\n    self.assertAllEqual((tensor_dict[\n        fields.InputDataFields.groundtruth_group_of].get_shape().as_list()),\n                        [None])\n    with self.test_session() as sess:\n      tensor_dict = sess.run(tensor_dict)\n\n    self.assertAllEqual(\n        [bool(item) for item in object_group_of],\n        tensor_dict[fields.InputDataFields.groundtruth_group_of])\n\n  def testDecodeObjectWeight(self):\n    image_tensor = np.random.randint(256, size=(4, 5, 3)).astype(np.uint8)\n    encoded_jpeg = self._EncodeImage(image_tensor)\n    object_weights = [0.75, 1.0]\n    example = tf.train.Example(features=tf.train.Features(\n        feature={\n            \'image/encoded\': self._BytesFeature(encoded_jpeg),\n            \'image/format\': self._BytesFeature(\'jpeg\'),\n            \'image/object/weight\': self._FloatFeature(object_weights),\n        })).SerializeToString()\n\n    example_decoder = tf_example_decoder.TfExampleDecoder()\n    tensor_dict = example_decoder.decode(tf.convert_to_tensor(example))\n\n    self.assertAllEqual((tensor_dict[\n        fields.InputDataFields.groundtruth_weights].get_shape().as_list()),\n                        [None])\n    with self.test_session() as sess:\n      tensor_dict = sess.run(tensor_dict)\n\n    self.assertAllEqual(\n        object_weights,\n        tensor_dict[fields.InputDataFields.groundtruth_weights])\n\n  def testDecodeInstanceSegmentation(self):\n    num_instances = 4\n    image_height = 5\n    image_width = 3\n\n    # Randomly generate image.\n    image_tensor = np.random.randint(256, size=(image_height,\n                                                image_width,\n                                                3)).astype(np.uint8)\n    encoded_jpeg = self._EncodeImage(image_tensor)\n\n    # Randomly generate instance segmentation masks.\n    instance_masks = (\n        np.random.randint(2, size=(num_instances,\n                                   image_height,\n                                   image_width)).astype(np.float32))\n    instance_masks_flattened = np.reshape(instance_masks, [-1])\n\n    # Randomly generate class labels for each instance.\n    object_classes = np.random.randint(\n        100, size=(num_instances)).astype(np.int64)\n\n    example = tf.train.Example(features=tf.train.Features(feature={\n        \'image/encoded\': self._BytesFeature(encoded_jpeg),\n        \'image/format\': self._BytesFeature(\'jpeg\'),\n        \'image/height\': self._Int64Feature([image_height]),\n        \'image/width\': self._Int64Feature([image_width]),\n        \'image/object/mask\': self._FloatFeature(instance_masks_flattened),\n        \'image/object/class/label\': self._Int64Feature(\n            object_classes)})).SerializeToString()\n    example_decoder = tf_example_decoder.TfExampleDecoder(\n        load_instance_masks=True)\n    tensor_dict = example_decoder.decode(tf.convert_to_tensor(example))\n\n    self.assertAllEqual((\n        tensor_dict[fields.InputDataFields.groundtruth_instance_masks].\n        get_shape().as_list()), [None, None, None])\n\n    self.assertAllEqual((\n        tensor_dict[fields.InputDataFields.groundtruth_classes].\n        get_shape().as_list()), [None])\n\n    with self.test_session() as sess:\n      tensor_dict = sess.run(tensor_dict)\n\n    self.assertAllEqual(\n        instance_masks.astype(np.float32),\n        tensor_dict[fields.InputDataFields.groundtruth_instance_masks])\n    self.assertAllEqual(\n        object_classes,\n        tensor_dict[fields.InputDataFields.groundtruth_classes])\n\n  def testInstancesNotAvailableByDefault(self):\n    num_instances = 4\n    image_height = 5\n    image_width = 3\n    # Randomly generate image.\n    image_tensor = np.random.randint(256, size=(image_height,\n                                                image_width,\n                                                3)).astype(np.uint8)\n    encoded_jpeg = self._EncodeImage(image_tensor)\n\n    # Randomly generate instance segmentation masks.\n    instance_masks = (\n        np.random.randint(2, size=(num_instances,\n                                   image_height,\n                                   image_width)).astype(np.float32))\n    instance_masks_flattened = np.reshape(instance_masks, [-1])\n\n    # Randomly generate class labels for each instance.\n    object_classes = np.random.randint(\n        100, size=(num_instances)).astype(np.int64)\n\n    example = tf.train.Example(features=tf.train.Features(feature={\n        \'image/encoded\': self._BytesFeature(encoded_jpeg),\n        \'image/format\': self._BytesFeature(\'jpeg\'),\n        \'image/height\': self._Int64Feature([image_height]),\n        \'image/width\': self._Int64Feature([image_width]),\n        \'image/object/mask\': self._FloatFeature(instance_masks_flattened),\n        \'image/object/class/label\': self._Int64Feature(\n            object_classes)})).SerializeToString()\n    example_decoder = tf_example_decoder.TfExampleDecoder()\n    tensor_dict = example_decoder.decode(tf.convert_to_tensor(example))\n    self.assertTrue(fields.InputDataFields.groundtruth_instance_masks\n                    not in tensor_dict)\n\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
src/object_detection/dataset_tools/__init__.py,0,b''
src/object_detection/dataset_tools/create_coco_tf_record.py,26,"b'# Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nr""""""Convert raw COCO dataset to TFRecord for object_detection.\n\nExample usage:\n    python create_coco_tf_record.py --logtostderr \\\n      --train_image_dir=""${TRAIN_IMAGE_DIR}"" \\\n      --val_image_dir=""${VAL_IMAGE_DIR}"" \\\n      --test_image_dir=""${TEST_IMAGE_DIR}"" \\\n      --train_annotations_file=""${TRAIN_ANNOTATIONS_FILE}"" \\\n      --val_annotations_file=""${VAL_ANNOTATIONS_FILE}"" \\\n      --testdev_annotations_file=""${TESTDEV_ANNOTATIONS_FILE}"" \\\n      --output_dir=""${OUTPUT_DIR}""\n""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport hashlib\nimport io\nimport json\nimport os\nimport numpy as np\nimport PIL.Image\n\nfrom pycocotools import mask\nimport tensorflow as tf\n\nfrom object_detection.utils import dataset_util\nfrom object_detection.utils import label_map_util\n\n\nflags = tf.app.flags\ntf.flags.DEFINE_boolean(\'include_masks\', False,\n                        \'Whether to include instance segmentations masks \'\n                        \'(PNG encoded) in the result. default: False.\')\ntf.flags.DEFINE_string(\'train_image_dir\', \'\',\n                       \'Training image directory.\')\ntf.flags.DEFINE_string(\'val_image_dir\', \'\',\n                       \'Validation image directory.\')\ntf.flags.DEFINE_string(\'test_image_dir\', \'\',\n                       \'Test image directory.\')\ntf.flags.DEFINE_string(\'train_annotations_file\', \'\',\n                       \'Training annotations JSON file.\')\ntf.flags.DEFINE_string(\'val_annotations_file\', \'\',\n                       \'Validation annotations JSON file.\')\ntf.flags.DEFINE_string(\'testdev_annotations_file\', \'\',\n                       \'Test-dev annotations JSON file.\')\ntf.flags.DEFINE_string(\'output_dir\', \'/tmp/\', \'Output data directory.\')\n\nFLAGS = flags.FLAGS\n\ntf.logging.set_verbosity(tf.logging.INFO)\n\n\ndef create_tf_example(image,\n                      annotations_list,\n                      image_dir,\n                      category_index,\n                      include_masks=False):\n  """"""Converts image and annotations to a tf.Example proto.\n\n  Args:\n    image: dict with keys:\n      [u\'license\', u\'file_name\', u\'coco_url\', u\'height\', u\'width\',\n      u\'date_captured\', u\'flickr_url\', u\'id\']\n    annotations_list:\n      list of dicts with keys:\n      [u\'segmentation\', u\'area\', u\'iscrowd\', u\'image_id\',\n      u\'bbox\', u\'category_id\', u\'id\']\n      Notice that bounding box coordinates in the official COCO dataset are\n      given as [x, y, width, height] tuples using absolute coordinates where\n      x, y represent the top-left (0-indexed) corner.  This function converts\n      to the format expected by the Tensorflow Object Detection API (which is\n      which is [ymin, xmin, ymax, xmax] with coordinates normalized relative\n      to image size).\n    image_dir: directory containing the image files.\n    category_index: a dict containing COCO category information keyed\n      by the \'id\' field of each category.  See the\n      label_map_util.create_category_index function.\n    include_masks: Whether to include instance segmentations masks\n      (PNG encoded) in the result. default: False.\n  Returns:\n    example: The converted tf.Example\n    num_annotations_skipped: Number of (invalid) annotations that were ignored.\n\n  Raises:\n    ValueError: if the image pointed to by data[\'filename\'] is not a valid JPEG\n  """"""\n  image_height = image[\'height\']\n  image_width = image[\'width\']\n  filename = image[\'file_name\']\n  image_id = image[\'id\']\n\n  full_path = os.path.join(image_dir, filename)\n  with tf.gfile.GFile(full_path, \'rb\') as fid:\n    encoded_jpg = fid.read()\n  encoded_jpg_io = io.BytesIO(encoded_jpg)\n  image = PIL.Image.open(encoded_jpg_io)\n  key = hashlib.sha256(encoded_jpg).hexdigest()\n\n  xmin = []\n  xmax = []\n  ymin = []\n  ymax = []\n  is_crowd = []\n  category_names = []\n  category_ids = []\n  area = []\n  encoded_mask_png = []\n  num_annotations_skipped = 0\n  for object_annotations in annotations_list:\n    (x, y, width, height) = tuple(object_annotations[\'bbox\'])\n    if width <= 0 or height <= 0:\n      num_annotations_skipped += 1\n      continue\n    if x + width > image_width or y + height > image_height:\n      num_annotations_skipped += 1\n      continue\n    xmin.append(float(x) / image_width)\n    xmax.append(float(x + width) / image_width)\n    ymin.append(float(y) / image_height)\n    ymax.append(float(y + height) / image_height)\n    is_crowd.append(object_annotations[\'iscrowd\'])\n    category_id = int(object_annotations[\'category_id\'])\n    category_ids.append(category_id)\n    category_names.append(category_index[category_id][\'name\'].encode(\'utf8\'))\n    area.append(object_annotations[\'area\'])\n\n    if include_masks:\n      run_len_encoding = mask.frPyObjects(object_annotations[\'segmentation\'],\n                                          image_height, image_width)\n      binary_mask = mask.decode(run_len_encoding)\n      if not object_annotations[\'iscrowd\']:\n        binary_mask = np.amax(binary_mask, axis=2)\n      pil_image = PIL.Image.fromarray(binary_mask)\n      output_io = io.BytesIO()\n      pil_image.save(output_io, format=\'PNG\')\n      encoded_mask_png.append(output_io.getvalue())\n  feature_dict = {\n      \'image/height\':\n          dataset_util.int64_feature(image_height),\n      \'image/width\':\n          dataset_util.int64_feature(image_width),\n      \'image/filename\':\n          dataset_util.bytes_feature(filename.encode(\'utf8\')),\n      \'image/source_id\':\n          dataset_util.bytes_feature(str(image_id).encode(\'utf8\')),\n      \'image/key/sha256\':\n          dataset_util.bytes_feature(key.encode(\'utf8\')),\n      \'image/encoded\':\n          dataset_util.bytes_feature(encoded_jpg),\n      \'image/format\':\n          dataset_util.bytes_feature(\'jpeg\'.encode(\'utf8\')),\n      \'image/object/bbox/xmin\':\n          dataset_util.float_list_feature(xmin),\n      \'image/object/bbox/xmax\':\n          dataset_util.float_list_feature(xmax),\n      \'image/object/bbox/ymin\':\n          dataset_util.float_list_feature(ymin),\n      \'image/object/bbox/ymax\':\n          dataset_util.float_list_feature(ymax),\n      \'image/object/class/label\':\n          dataset_util.int64_list_feature(category_ids),\n      \'image/object/is_crowd\':\n          dataset_util.int64_list_feature(is_crowd),\n      \'image/object/area\':\n          dataset_util.float_list_feature(area),\n  }\n  if include_masks:\n    feature_dict[\'image/object/mask\'] = (\n        dataset_util.bytes_list_feature(encoded_mask_png))\n  example = tf.train.Example(features=tf.train.Features(feature=feature_dict))\n  return key, example, num_annotations_skipped\n\n\ndef _create_tf_record_from_coco_annotations(\n    annotations_file, image_dir, output_path, include_masks):\n  """"""Loads COCO annotation json files and converts to tf.Record format.\n\n  Args:\n    annotations_file: JSON file containing bounding box annotations.\n    image_dir: Directory containing the image files.\n    output_path: Path to output tf.Record file.\n    include_masks: Whether to include instance segmentations masks\n      (PNG encoded) in the result. default: False.\n  """"""\n  with tf.gfile.GFile(annotations_file, \'r\') as fid:\n    groundtruth_data = json.load(fid)\n    images = groundtruth_data[\'images\']\n    category_index = label_map_util.create_category_index(\n        groundtruth_data[\'categories\'])\n\n    annotations_index = {}\n    if \'annotations\' in groundtruth_data:\n      tf.logging.info(\n          \'Found groundtruth annotations. Building annotations index.\')\n      for annotation in groundtruth_data[\'annotations\']:\n        image_id = annotation[\'image_id\']\n        if image_id not in annotations_index:\n          annotations_index[image_id] = []\n        annotations_index[image_id].append(annotation)\n    missing_annotation_count = 0\n    for image in images:\n      image_id = image[\'id\']\n      if image_id not in annotations_index:\n        missing_annotation_count += 1\n        annotations_index[image_id] = []\n    tf.logging.info(\'%d images are missing annotations.\',\n                    missing_annotation_count)\n\n    tf.logging.info(\'writing to output path: %s\', output_path)\n    writer = tf.python_io.TFRecordWriter(output_path)\n    total_num_annotations_skipped = 0\n    for idx, image in enumerate(images):\n      if idx % 100 == 0:\n        tf.logging.info(\'On image %d of %d\', idx, len(images))\n      annotations_list = annotations_index[image[\'id\']]\n      _, tf_example, num_annotations_skipped = create_tf_example(\n          image, annotations_list, image_dir, category_index, include_masks)\n      total_num_annotations_skipped += num_annotations_skipped\n      writer.write(tf_example.SerializeToString())\n    writer.close()\n    tf.logging.info(\'Finished writing, skipped %d annotations.\',\n                    total_num_annotations_skipped)\n\n\ndef main(_):\n  assert FLAGS.train_image_dir, \'`train_image_dir` missing.\'\n  assert FLAGS.val_image_dir, \'`val_image_dir` missing.\'\n  assert FLAGS.test_image_dir, \'`test_image_dir` missing.\'\n  assert FLAGS.train_annotations_file, \'`train_annotations_file` missing.\'\n  assert FLAGS.val_annotations_file, \'`val_annotations_file` missing.\'\n  assert FLAGS.testdev_annotations_file, \'`testdev_annotations_file` missing.\'\n\n  if not tf.gfile.IsDirectory(FLAGS.output_dir):\n    tf.gfile.MakeDirs(FLAGS.output_dir)\n  train_output_path = os.path.join(FLAGS.output_dir, \'coco_train.record\')\n  val_output_path = os.path.join(FLAGS.output_dir, \'coco_val.record\')\n  testdev_output_path = os.path.join(FLAGS.output_dir, \'coco_testdev.record\')\n\n  _create_tf_record_from_coco_annotations(\n      FLAGS.train_annotations_file,\n      FLAGS.train_image_dir,\n      train_output_path,\n      FLAGS.include_masks)\n  _create_tf_record_from_coco_annotations(\n      FLAGS.val_annotations_file,\n      FLAGS.val_image_dir,\n      val_output_path,\n      FLAGS.include_masks)\n  _create_tf_record_from_coco_annotations(\n      FLAGS.testdev_annotations_file,\n      FLAGS.test_image_dir,\n      testdev_output_path,\n      FLAGS.include_masks)\n\n\nif __name__ == \'__main__\':\n  tf.app.run()\n'"
src/object_detection/dataset_tools/create_coco_tf_record_test.py,2,"b'# Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Test for create_coco_tf_record.py.""""""\n\nimport io\nimport os\n\nimport numpy as np\nimport PIL.Image\nimport tensorflow as tf\n\nfrom object_detection.dataset_tools import create_coco_tf_record\n\n\nclass CreateCocoTFRecordTest(tf.test.TestCase):\n\n  def _assertProtoEqual(self, proto_field, expectation):\n    """"""Helper function to assert if a proto field equals some value.\n\n    Args:\n      proto_field: The protobuf field to compare.\n      expectation: The expected value of the protobuf field.\n    """"""\n    proto_list = [p for p in proto_field]\n    self.assertListEqual(proto_list, expectation)\n\n  def test_create_tf_example(self):\n    image_file_name = \'tmp_image.jpg\'\n    image_data = np.random.rand(256, 256, 3)\n    tmp_dir = self.get_temp_dir()\n    save_path = os.path.join(tmp_dir, image_file_name)\n    image = PIL.Image.fromarray(image_data, \'RGB\')\n    image.save(save_path)\n\n    image = {\n        \'file_name\': image_file_name,\n        \'height\': 256,\n        \'width\': 256,\n        \'id\': 11,\n    }\n\n    annotations_list = [{\n        \'area\': .5,\n        \'iscrowd\': False,\n        \'image_id\': 11,\n        \'bbox\': [64, 64, 128, 128],\n        \'category_id\': 2,\n        \'id\': 1000,\n    }]\n\n    image_dir = tmp_dir\n    category_index = {\n        1: {\n            \'name\': \'dog\',\n            \'id\': 1\n        },\n        2: {\n            \'name\': \'cat\',\n            \'id\': 2\n        },\n        3: {\n            \'name\': \'human\',\n            \'id\': 3\n        }\n    }\n\n    (_, example,\n     num_annotations_skipped) = create_coco_tf_record.create_tf_example(\n         image, annotations_list, image_dir, category_index)\n\n    self.assertEqual(num_annotations_skipped, 0)\n    self._assertProtoEqual(\n        example.features.feature[\'image/height\'].int64_list.value, [256])\n    self._assertProtoEqual(\n        example.features.feature[\'image/width\'].int64_list.value, [256])\n    self._assertProtoEqual(\n        example.features.feature[\'image/filename\'].bytes_list.value,\n        [image_file_name])\n    self._assertProtoEqual(\n        example.features.feature[\'image/source_id\'].bytes_list.value,\n        [str(image[\'id\'])])\n    self._assertProtoEqual(\n        example.features.feature[\'image/format\'].bytes_list.value, [\'jpeg\'])\n    self._assertProtoEqual(\n        example.features.feature[\'image/object/bbox/xmin\'].float_list.value,\n        [0.25])\n    self._assertProtoEqual(\n        example.features.feature[\'image/object/bbox/ymin\'].float_list.value,\n        [0.25])\n    self._assertProtoEqual(\n        example.features.feature[\'image/object/bbox/xmax\'].float_list.value,\n        [0.75])\n    self._assertProtoEqual(\n        example.features.feature[\'image/object/bbox/ymax\'].float_list.value,\n        [0.75])\n\n  def test_create_tf_example_with_instance_masks(self):\n    image_file_name = \'tmp_image.jpg\'\n    image_data = np.random.rand(8, 8, 3)\n    tmp_dir = self.get_temp_dir()\n    save_path = os.path.join(tmp_dir, image_file_name)\n    image = PIL.Image.fromarray(image_data, \'RGB\')\n    image.save(save_path)\n\n    image = {\n        \'file_name\': image_file_name,\n        \'height\': 8,\n        \'width\': 8,\n        \'id\': 11,\n    }\n\n    annotations_list = [{\n        \'area\': .5,\n        \'iscrowd\': False,\n        \'image_id\': 11,\n        \'bbox\': [0, 0, 8, 8],\n        \'segmentation\': [[4, 0, 0, 0, 0, 4], [8, 4, 4, 8, 8, 8]],\n        \'category_id\': 1,\n        \'id\': 1000,\n    }]\n\n    image_dir = tmp_dir\n    category_index = {\n        1: {\n            \'name\': \'dog\',\n            \'id\': 1\n        },\n    }\n\n    (_, example,\n     num_annotations_skipped) = create_coco_tf_record.create_tf_example(\n         image, annotations_list, image_dir, category_index, include_masks=True)\n\n    self.assertEqual(num_annotations_skipped, 0)\n    self._assertProtoEqual(\n        example.features.feature[\'image/height\'].int64_list.value, [8])\n    self._assertProtoEqual(\n        example.features.feature[\'image/width\'].int64_list.value, [8])\n    self._assertProtoEqual(\n        example.features.feature[\'image/filename\'].bytes_list.value,\n        [image_file_name])\n    self._assertProtoEqual(\n        example.features.feature[\'image/source_id\'].bytes_list.value,\n        [str(image[\'id\'])])\n    self._assertProtoEqual(\n        example.features.feature[\'image/format\'].bytes_list.value, [\'jpeg\'])\n    self._assertProtoEqual(\n        example.features.feature[\'image/object/bbox/xmin\'].float_list.value,\n        [0])\n    self._assertProtoEqual(\n        example.features.feature[\'image/object/bbox/ymin\'].float_list.value,\n        [0])\n    self._assertProtoEqual(\n        example.features.feature[\'image/object/bbox/xmax\'].float_list.value,\n        [1])\n    self._assertProtoEqual(\n        example.features.feature[\'image/object/bbox/ymax\'].float_list.value,\n        [1])\n    encoded_mask_pngs = [\n        io.BytesIO(encoded_masks) for encoded_masks in example.features.feature[\n            \'image/object/mask\'].bytes_list.value\n    ]\n    pil_masks = [\n        np.array(PIL.Image.open(encoded_mask_png))\n        for encoded_mask_png in encoded_mask_pngs\n    ]\n    self.assertTrue(len(pil_masks) == 1)\n    self.assertAllEqual(pil_masks[0],\n                        [[1, 1, 1, 0, 0, 0, 0, 0], [1, 1, 0, 0, 0, 0, 0, 0],\n                         [1, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0],\n                         [0, 0, 0, 0, 0, 0, 0, 1], [0, 0, 0, 0, 0, 0, 1, 1],\n                         [0, 0, 0, 0, 0, 1, 1, 1], [0, 0, 0, 0, 1, 1, 1, 1]])\n\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
src/object_detection/dataset_tools/create_kitti_tf_record.py,14,"b'# Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nr""""""Convert raw KITTI detection dataset to TFRecord for object_detection.\n\nConverts KITTI detection dataset to TFRecords with a standard format allowing\n  to use this dataset to train object detectors. The raw dataset can be\n  downloaded from:\n  http://kitti.is.tue.mpg.de/kitti/data_object_image_2.zip.\n  http://kitti.is.tue.mpg.de/kitti/data_object_label_2.zip\n  Permission can be requested at the main website.\n\n  KITTI detection dataset contains 7481 training images. Using this code with\n  the default settings will set aside the first 500 images as a validation set.\n  This can be altered using the flags, see details below.\n\nExample usage:\n    python object_detection/dataset_tools/create_kitti_tf_record.py \\\n        --data_dir=/home/user/kitti \\\n        --output_path=/home/user/kitti.record\n""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\n\nimport hashlib\nimport io\nimport os\n\nimport numpy as np\nimport PIL.Image as pil\nimport tensorflow as tf\n\nfrom object_detection.utils import dataset_util\nfrom object_detection.utils import label_map_util\nfrom object_detection.utils.np_box_ops import iou\n\ntf.app.flags.DEFINE_string(\'data_dir\', \'\', \'Location of root directory for the \'\n                           \'data. Folder structure is assumed to be:\'\n                           \'<data_dir>/training/label_2 (annotations) and\'\n                           \'<data_dir>/data_object_image_2/training/image_2\'\n                           \'(images).\')\ntf.app.flags.DEFINE_string(\'output_path\', \'\', \'Path to which TFRecord files\'\n                           \'will be written. The TFRecord with the training set\'\n                           \'will be located at: <output_path>_train.tfrecord.\'\n                           \'And the TFRecord with the validation set will be\'\n                           \'located at: <output_path>_val.tfrecord\')\ntf.app.flags.DEFINE_string(\'classes_to_use\', \'car,pedestrian,dontcare\',\n                           \'Comma separated list of class names that will be\'\n                           \'used. Adding the dontcare class will remove all\'\n                           \'bboxs in the dontcare regions.\')\ntf.app.flags.DEFINE_string(\'label_map_path\', \'data/kitti_label_map.pbtxt\',\n                           \'Path to label map proto.\')\ntf.app.flags.DEFINE_integer(\'validation_set_size\', \'500\', \'Number of images to\'\n                            \'be used as a validation set.\')\nFLAGS = tf.app.flags.FLAGS\n\n\ndef convert_kitti_to_tfrecords(data_dir, output_path, classes_to_use,\n                               label_map_path, validation_set_size):\n  """"""Convert the KITTI detection dataset to TFRecords.\n\n  Args:\n    data_dir: The full path to the unzipped folder containing the unzipped data\n      from data_object_image_2 and data_object_label_2.zip.\n      Folder structure is assumed to be: data_dir/training/label_2 (annotations)\n      and data_dir/data_object_image_2/training/image_2 (images).\n    output_path: The path to which TFRecord files will be written. The TFRecord\n      with the training set will be located at: <output_path>_train.tfrecord\n      And the TFRecord with the validation set will be located at:\n      <output_path>_val.tfrecord\n    classes_to_use: List of strings naming the classes for which data should be\n      converted. Use the same names as presented in the KIITI README file.\n      Adding dontcare class will remove all other bounding boxes that overlap\n      with areas marked as dontcare regions.\n    label_map_path: Path to label map proto\n    validation_set_size: How many images should be left as the validation set.\n      (Ffirst `validation_set_size` examples are selected to be in the\n      validation set).\n  """"""\n  label_map_dict = label_map_util.get_label_map_dict(label_map_path)\n  train_count = 0\n  val_count = 0\n\n  annotation_dir = os.path.join(data_dir,\n                                \'training\',\n                                \'label_2\')\n\n  image_dir = os.path.join(data_dir,\n                           \'data_object_image_2\',\n                           \'training\',\n                           \'image_2\')\n\n  train_writer = tf.python_io.TFRecordWriter(\'%s_train.tfrecord\'%\n                                             output_path)\n  val_writer = tf.python_io.TFRecordWriter(\'%s_val.tfrecord\'%\n                                           output_path)\n\n  images = sorted(tf.gfile.ListDirectory(image_dir))\n  for img_name in images:\n    img_num = int(img_name.split(\'.\')[0])\n    is_validation_img = img_num < validation_set_size\n    img_anno = read_annotation_file(os.path.join(annotation_dir,\n                                                 str(img_num).zfill(6)+\'.txt\'))\n\n    image_path = os.path.join(image_dir, img_name)\n\n    # Filter all bounding boxes of this frame that are of a legal class, and\n    # don\'t overlap with a dontcare region.\n    # TODO(talremez) filter out targets that are truncated or heavily occluded.\n    annotation_for_image = filter_annotations(img_anno, classes_to_use)\n\n    example = prepare_example(image_path, annotation_for_image, label_map_dict)\n    if is_validation_img:\n      val_writer.write(example.SerializeToString())\n      val_count += 1\n    else:\n      train_writer.write(example.SerializeToString())\n      train_count += 1\n\n  train_writer.close()\n  val_writer.close()\n\n\ndef prepare_example(image_path, annotations, label_map_dict):\n  """"""Converts a dictionary with annotations for an image to tf.Example proto.\n\n  Args:\n    image_path: The complete path to image.\n    annotations: A dictionary representing the annotation of a single object\n      that appears in the image.\n    label_map_dict: A map from string label names to integer ids.\n\n  Returns:\n    example: The converted tf.Example.\n  """"""\n  with tf.gfile.GFile(image_path, \'rb\') as fid:\n    encoded_png = fid.read()\n  encoded_png_io = io.BytesIO(encoded_png)\n  image = pil.open(encoded_png_io)\n  image = np.asarray(image)\n\n  key = hashlib.sha256(encoded_png).hexdigest()\n\n  width = int(image.shape[1])\n  height = int(image.shape[0])\n\n  xmin_norm = annotations[\'2d_bbox_left\'] / float(width)\n  ymin_norm = annotations[\'2d_bbox_top\'] / float(height)\n  xmax_norm = annotations[\'2d_bbox_right\'] / float(width)\n  ymax_norm = annotations[\'2d_bbox_bottom\'] / float(height)\n\n  difficult_obj = [0]*len(xmin_norm)\n\n  example = tf.train.Example(features=tf.train.Features(feature={\n      \'image/height\': dataset_util.int64_feature(height),\n      \'image/width\': dataset_util.int64_feature(width),\n      \'image/filename\': dataset_util.bytes_feature(image_path.encode(\'utf8\')),\n      \'image/source_id\': dataset_util.bytes_feature(image_path.encode(\'utf8\')),\n      \'image/key/sha256\': dataset_util.bytes_feature(key.encode(\'utf8\')),\n      \'image/encoded\': dataset_util.bytes_feature(encoded_png),\n      \'image/format\': dataset_util.bytes_feature(\'png\'.encode(\'utf8\')),\n      \'image/object/bbox/xmin\': dataset_util.float_list_feature(xmin_norm),\n      \'image/object/bbox/xmax\': dataset_util.float_list_feature(xmax_norm),\n      \'image/object/bbox/ymin\': dataset_util.float_list_feature(ymin_norm),\n      \'image/object/bbox/ymax\': dataset_util.float_list_feature(ymax_norm),\n      \'image/object/class/text\': dataset_util.bytes_list_feature(\n          [x.encode(\'utf8\') for x in annotations[\'type\']]),\n      \'image/object/class/label\': dataset_util.int64_list_feature(\n          [label_map_dict[x] for x in annotations[\'type\']]),\n      \'image/object/difficult\': dataset_util.int64_list_feature(difficult_obj),\n      \'image/object/truncated\': dataset_util.float_list_feature(\n          annotations[\'truncated\']),\n      \'image/object/alpha\': dataset_util.float_list_feature(\n          annotations[\'alpha\']),\n      \'image/object/3d_bbox/height\': dataset_util.float_list_feature(\n          annotations[\'3d_bbox_height\']),\n      \'image/object/3d_bbox/width\': dataset_util.float_list_feature(\n          annotations[\'3d_bbox_width\']),\n      \'image/object/3d_bbox/length\': dataset_util.float_list_feature(\n          annotations[\'3d_bbox_length\']),\n      \'image/object/3d_bbox/x\': dataset_util.float_list_feature(\n          annotations[\'3d_bbox_x\']),\n      \'image/object/3d_bbox/y\': dataset_util.float_list_feature(\n          annotations[\'3d_bbox_y\']),\n      \'image/object/3d_bbox/z\': dataset_util.float_list_feature(\n          annotations[\'3d_bbox_z\']),\n      \'image/object/3d_bbox/rot_y\': dataset_util.float_list_feature(\n          annotations[\'3d_bbox_rot_y\']),\n  }))\n\n  return example\n\n\ndef filter_annotations(img_all_annotations, used_classes):\n  """"""Filters out annotations from the unused classes and dontcare regions.\n\n  Filters out the annotations that belong to classes we do now wish to use and\n  (optionally) also removes all boxes that overlap with dontcare regions.\n\n  Args:\n    img_all_annotations: A list of annotation dictionaries. See documentation of\n      read_annotation_file for more details about the format of the annotations.\n    used_classes: A list of strings listing the classes we want to keep, if the\n    list contains ""dontcare"", all bounding boxes with overlapping with dont\n    care regions will also be filtered out.\n\n  Returns:\n    img_filtered_annotations: A list of annotation dictionaries that have passed\n      the filtering.\n  """"""\n\n  img_filtered_annotations = {}\n\n  # Filter the type of the objects.\n  relevant_annotation_indices = [\n      i for i, x in enumerate(img_all_annotations[\'type\']) if x in used_classes\n  ]\n\n  for key in img_all_annotations.keys():\n    img_filtered_annotations[key] = (\n        img_all_annotations[key][relevant_annotation_indices])\n\n  if \'dontcare\' in used_classes:\n    dont_care_indices = [i for i,\n                         x in enumerate(img_filtered_annotations[\'type\'])\n                         if x == \'dontcare\']\n\n    # bounding box format [y_min, x_min, y_max, x_max]\n    all_boxes = np.stack([img_filtered_annotations[\'2d_bbox_top\'],\n                          img_filtered_annotations[\'2d_bbox_left\'],\n                          img_filtered_annotations[\'2d_bbox_bottom\'],\n                          img_filtered_annotations[\'2d_bbox_right\']],\n                         axis=1)\n\n    ious = iou(boxes1=all_boxes,\n               boxes2=all_boxes[dont_care_indices])\n\n    # Remove all bounding boxes that overlap with a dontcare region.\n    if ious.size > 0:\n      boxes_to_remove = np.amax(ious, axis=1) > 0.0\n      for key in img_all_annotations.keys():\n        img_filtered_annotations[key] = (\n            img_filtered_annotations[key][np.logical_not(boxes_to_remove)])\n\n  return img_filtered_annotations\n\n\ndef read_annotation_file(filename):\n  """"""Reads a KITTI annotation file.\n\n  Converts a KITTI annotation file into a dictionary containing all the\n  relevant information.\n\n  Args:\n    filename: the path to the annotataion text file.\n\n  Returns:\n    anno: A dictionary with the converted annotation information. See annotation\n    README file for details on the different fields.\n  """"""\n  with open(filename) as f:\n    content = f.readlines()\n  content = [x.strip().split(\' \') for x in content]\n\n  anno = {}\n  anno[\'type\'] = np.array([x[0].lower() for x in content])\n  anno[\'truncated\'] = np.array([float(x[1]) for x in content])\n  anno[\'occluded\'] = np.array([int(x[2]) for x in content])\n  anno[\'alpha\'] = np.array([float(x[3]) for x in content])\n\n  anno[\'2d_bbox_left\'] = np.array([float(x[4]) for x in content])\n  anno[\'2d_bbox_top\'] = np.array([float(x[5]) for x in content])\n  anno[\'2d_bbox_right\'] = np.array([float(x[6]) for x in content])\n  anno[\'2d_bbox_bottom\'] = np.array([float(x[7]) for x in content])\n\n  anno[\'3d_bbox_height\'] = np.array([float(x[8]) for x in content])\n  anno[\'3d_bbox_width\'] = np.array([float(x[9]) for x in content])\n  anno[\'3d_bbox_length\'] = np.array([float(x[10]) for x in content])\n  anno[\'3d_bbox_x\'] = np.array([float(x[11]) for x in content])\n  anno[\'3d_bbox_y\'] = np.array([float(x[12]) for x in content])\n  anno[\'3d_bbox_z\'] = np.array([float(x[13]) for x in content])\n  anno[\'3d_bbox_rot_y\'] = np.array([float(x[14]) for x in content])\n\n  return anno\n\n\ndef main(_):\n  convert_kitti_to_tfrecords(\n      data_dir=FLAGS.data_dir,\n      output_path=FLAGS.output_path,\n      classes_to_use=FLAGS.classes_to_use.split(\',\'),\n      label_map_path=FLAGS.label_map_path,\n      validation_set_size=FLAGS.validation_set_size)\n\nif __name__ == \'__main__\':\n  tf.app.run()\n'"
src/object_detection/dataset_tools/create_kitti_tf_record_test.py,2,"b'# Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n""""""Test for create_kitti_tf_record.py.""""""\n\nimport os\n\nimport numpy as np\nimport PIL.Image\nimport tensorflow as tf\n\nfrom object_detection.dataset_tools import create_kitti_tf_record\n\n\nclass CreateKittiTFRecordTest(tf.test.TestCase):\n\n  def _assertProtoEqual(self, proto_field, expectation):\n    """"""Helper function to assert if a proto field equals some value.\n\n    Args:\n      proto_field: The protobuf field to compare.\n      expectation: The expected value of the protobuf field.\n    """"""\n    proto_list = [p for p in proto_field]\n    self.assertListEqual(proto_list, expectation)\n\n  def test_dict_to_tf_example(self):\n    image_file_name = \'tmp_image.jpg\'\n    image_data = np.random.rand(256, 256, 3)\n    save_path = os.path.join(self.get_temp_dir(), image_file_name)\n    image = PIL.Image.fromarray(image_data, \'RGB\')\n    image.save(save_path)\n\n    annotations = {}\n    annotations[\'2d_bbox_left\'] = np.array([64])\n    annotations[\'2d_bbox_top\'] = np.array([64])\n    annotations[\'2d_bbox_right\'] = np.array([192])\n    annotations[\'2d_bbox_bottom\'] = np.array([192])\n    annotations[\'type\'] = [\'car\']\n    annotations[\'truncated\'] = np.array([1])\n    annotations[\'alpha\'] = np.array([2])\n    annotations[\'3d_bbox_height\'] = np.array([10])\n    annotations[\'3d_bbox_width\'] = np.array([11])\n    annotations[\'3d_bbox_length\'] = np.array([12])\n    annotations[\'3d_bbox_x\'] = np.array([13])\n    annotations[\'3d_bbox_y\'] = np.array([14])\n    annotations[\'3d_bbox_z\'] = np.array([15])\n    annotations[\'3d_bbox_rot_y\'] = np.array([4])\n\n    label_map_dict = {\n        \'background\': 0,\n        \'car\': 1,\n    }\n\n    example = create_kitti_tf_record.prepare_example(\n        save_path,\n        annotations,\n        label_map_dict)\n\n    self._assertProtoEqual(\n        example.features.feature[\'image/height\'].int64_list.value, [256])\n    self._assertProtoEqual(\n        example.features.feature[\'image/width\'].int64_list.value, [256])\n    self._assertProtoEqual(\n        example.features.feature[\'image/filename\'].bytes_list.value,\n        [save_path])\n    self._assertProtoEqual(\n        example.features.feature[\'image/source_id\'].bytes_list.value,\n        [save_path])\n    self._assertProtoEqual(\n        example.features.feature[\'image/format\'].bytes_list.value, [\'png\'])\n    self._assertProtoEqual(\n        example.features.feature[\'image/object/bbox/xmin\'].float_list.value,\n        [0.25])\n    self._assertProtoEqual(\n        example.features.feature[\'image/object/bbox/ymin\'].float_list.value,\n        [0.25])\n    self._assertProtoEqual(\n        example.features.feature[\'image/object/bbox/xmax\'].float_list.value,\n        [0.75])\n    self._assertProtoEqual(\n        example.features.feature[\'image/object/bbox/ymax\'].float_list.value,\n        [0.75])\n    self._assertProtoEqual(\n        example.features.feature[\'image/object/class/text\'].bytes_list.value,\n        [\'car\'])\n    self._assertProtoEqual(\n        example.features.feature[\'image/object/class/label\'].int64_list.value,\n        [1])\n    self._assertProtoEqual(\n        example.features.feature[\'image/object/truncated\'].float_list.value,\n        [1])\n    self._assertProtoEqual(\n        example.features.feature[\'image/object/alpha\'].float_list.value,\n        [2])\n    self._assertProtoEqual(example.features.feature[\n        \'image/object/3d_bbox/height\'].float_list.value, [10])\n    self._assertProtoEqual(\n        example.features.feature[\'image/object/3d_bbox/width\'].float_list.value,\n        [11])\n    self._assertProtoEqual(example.features.feature[\n        \'image/object/3d_bbox/length\'].float_list.value, [12])\n    self._assertProtoEqual(\n        example.features.feature[\'image/object/3d_bbox/x\'].float_list.value,\n        [13])\n    self._assertProtoEqual(\n        example.features.feature[\'image/object/3d_bbox/y\'].float_list.value,\n        [14])\n    self._assertProtoEqual(\n        example.features.feature[\'image/object/3d_bbox/z\'].float_list.value,\n        [15])\n    self._assertProtoEqual(\n        example.features.feature[\'image/object/3d_bbox/rot_y\'].float_list.value,\n        [4])\n\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
src/object_detection/dataset_tools/create_oid_tf_record.py,12,"b'# Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\nr""""""Creates TFRecords of Open Images dataset for object detection.\n\nExample usage:\n  python object_detection/dataset_tools/create_oid_tf_record.py \\\n    --input_annotations_csv=/path/to/input/annotations-human-bbox.csv \\\n    --input_images_directory=/path/to/input/image_pixels_directory \\\n    --input_label_map=/path/to/input/labels_bbox_545.labelmap \\\n    --output_tf_record_path_prefix=/path/to/output/prefix.tfrecord\n\nCSVs with bounding box annotations and image metadata (including the image URLs)\ncan be downloaded from the Open Images GitHub repository:\nhttps://github.com/openimages/dataset\n\nThis script will include every image found in the input_images_directory in the\noutput TFRecord, even if the image has no corresponding bounding box annotations\nin the input_annotations_csv.\n""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport os\n\nimport contextlib2\nimport pandas as pd\nimport tensorflow as tf\n\nfrom object_detection.dataset_tools import oid_tfrecord_creation\nfrom object_detection.utils import label_map_util\n\ntf.flags.DEFINE_string(\'input_annotations_csv\', None,\n                       \'Path to CSV containing image bounding box annotations\')\ntf.flags.DEFINE_string(\'input_images_directory\', None,\n                       \'Directory containing the image pixels \'\n                       \'downloaded from the OpenImages GitHub repository.\')\ntf.flags.DEFINE_string(\'input_label_map\', None, \'Path to the label map proto\')\ntf.flags.DEFINE_string(\n    \'output_tf_record_path_prefix\', None,\n    \'Path to the output TFRecord. The shard index and the number of shards \'\n    \'will be appended for each output shard.\')\ntf.flags.DEFINE_integer(\'num_shards\', 100, \'Number of TFRecord shards\')\n\nFLAGS = tf.flags.FLAGS\n\n\ndef main(_):\n  tf.logging.set_verbosity(tf.logging.INFO)\n\n  required_flags = [\n      \'input_annotations_csv\', \'input_images_directory\', \'input_label_map\',\n      \'output_tf_record_path_prefix\'\n  ]\n  for flag_name in required_flags:\n    if not getattr(FLAGS, flag_name):\n      raise ValueError(\'Flag --{} is required\'.format(flag_name))\n\n  label_map = label_map_util.get_label_map_dict(FLAGS.input_label_map)\n  all_annotations = pd.read_csv(FLAGS.input_annotations_csv)\n  all_images = tf.gfile.Glob(\n      os.path.join(FLAGS.input_images_directory, \'*.jpg\'))\n  all_image_ids = [os.path.splitext(os.path.basename(v))[0] for v in all_images]\n  all_image_ids = pd.DataFrame({\'ImageID\': all_image_ids})\n  all_annotations = pd.concat([all_annotations, all_image_ids])\n\n  tf.logging.log(tf.logging.INFO, \'Found %d images...\', len(all_image_ids))\n\n  with contextlib2.ExitStack() as tf_record_close_stack:\n    output_tfrecords = oid_tfrecord_creation.open_sharded_output_tfrecords(\n        tf_record_close_stack, FLAGS.output_tf_record_path_prefix,\n        FLAGS.num_shards)\n\n    for counter, image_data in enumerate(all_annotations.groupby(\'ImageID\')):\n      tf.logging.log_every_n(tf.logging.INFO, \'Processed %d images...\', 1000,\n                             counter)\n\n      image_id, image_annotations = image_data\n      # In OID image file names are formed by appending "".jpg"" to the image ID.\n      image_path = os.path.join(FLAGS.input_images_directory, image_id + \'.jpg\')\n      with tf.gfile.Open(image_path) as image_file:\n        encoded_image = image_file.read()\n\n      tf_example = oid_tfrecord_creation.tf_example_from_annotations_data_frame(\n          image_annotations, label_map, encoded_image)\n      if tf_example:\n        shard_idx = int(image_id, 16) % FLAGS.num_shards\n        output_tfrecords[shard_idx].write(tf_example.SerializeToString())\n\n\nif __name__ == \'__main__\':\n  tf.app.run()\n'"
src/object_detection/dataset_tools/create_pascal_tf_record.py,8,"b'# Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nr""""""Convert raw PASCAL dataset to TFRecord for object_detection.\n\nExample usage:\n    python object_detection/dataset_tools/create_pascal_tf_record.py \\\n        --data_dir=/home/user/VOCdevkit \\\n        --year=VOC2012 \\\n        --output_path=/home/user/pascal.record\n""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport hashlib\nimport io\nimport logging\nimport os\n\nfrom lxml import etree\nimport PIL.Image\nimport tensorflow as tf\n\nfrom object_detection.utils import dataset_util\nfrom object_detection.utils import label_map_util\n\n\nflags = tf.app.flags\nflags.DEFINE_string(\'data_dir\', \'\', \'Root directory to raw PASCAL VOC dataset.\')\nflags.DEFINE_string(\'set\', \'train\', \'Convert training set, validation set or \'\n                    \'merged set.\')\nflags.DEFINE_string(\'annotations_dir\', \'Annotations\',\n                    \'(Relative) path to annotations directory.\')\nflags.DEFINE_string(\'year\', \'VOC2007\', \'Desired challenge year.\')\nflags.DEFINE_string(\'output_path\', \'\', \'Path to output TFRecord\')\nflags.DEFINE_string(\'label_map_path\', \'data/pascal_label_map.pbtxt\',\n                    \'Path to label map proto\')\nflags.DEFINE_boolean(\'ignore_difficult_instances\', False, \'Whether to ignore \'\n                     \'difficult instances\')\nFLAGS = flags.FLAGS\n\nSETS = [\'train\', \'val\', \'trainval\', \'test\']\nYEARS = [\'VOC2007\', \'VOC2012\', \'merged\']\n\n\ndef dict_to_tf_example(data,\n                       dataset_directory,\n                       label_map_dict,\n                       ignore_difficult_instances=False,\n                       image_subdirectory=\'JPEGImages\'):\n  """"""Convert XML derived dict to tf.Example proto.\n\n  Notice that this function normalizes the bounding box coordinates provided\n  by the raw data.\n\n  Args:\n    data: dict holding PASCAL XML fields for a single image (obtained by\n      running dataset_util.recursive_parse_xml_to_dict)\n    dataset_directory: Path to root directory holding PASCAL dataset\n    label_map_dict: A map from string label names to integers ids.\n    ignore_difficult_instances: Whether to skip difficult instances in the\n      dataset  (default: False).\n    image_subdirectory: String specifying subdirectory within the\n      PASCAL dataset directory holding the actual image data.\n\n  Returns:\n    example: The converted tf.Example.\n\n  Raises:\n    ValueError: if the image pointed to by data[\'filename\'] is not a valid JPEG\n  """"""\n  img_path = os.path.join(data[\'folder\'], image_subdirectory, data[\'filename\'])\n  full_path = os.path.join(dataset_directory, img_path)\n  with tf.gfile.GFile(full_path, \'rb\') as fid:\n    encoded_jpg = fid.read()\n  encoded_jpg_io = io.BytesIO(encoded_jpg)\n  image = PIL.Image.open(encoded_jpg_io)\n  if image.format != \'JPEG\':\n    raise ValueError(\'Image format not JPEG\')\n  key = hashlib.sha256(encoded_jpg).hexdigest()\n\n  width = int(data[\'size\'][\'width\'])\n  height = int(data[\'size\'][\'height\'])\n\n  xmin = []\n  ymin = []\n  xmax = []\n  ymax = []\n  classes = []\n  classes_text = []\n  truncated = []\n  poses = []\n  difficult_obj = []\n  for obj in data[\'object\']:\n    difficult = bool(int(obj[\'difficult\']))\n    if ignore_difficult_instances and difficult:\n      continue\n\n    difficult_obj.append(int(difficult))\n\n    xmin.append(float(obj[\'bndbox\'][\'xmin\']) / width)\n    ymin.append(float(obj[\'bndbox\'][\'ymin\']) / height)\n    xmax.append(float(obj[\'bndbox\'][\'xmax\']) / width)\n    ymax.append(float(obj[\'bndbox\'][\'ymax\']) / height)\n    classes_text.append(obj[\'name\'].encode(\'utf8\'))\n    classes.append(label_map_dict[obj[\'name\']])\n    truncated.append(int(obj[\'truncated\']))\n    poses.append(obj[\'pose\'].encode(\'utf8\'))\n\n  example = tf.train.Example(features=tf.train.Features(feature={\n      \'image/height\': dataset_util.int64_feature(height),\n      \'image/width\': dataset_util.int64_feature(width),\n      \'image/filename\': dataset_util.bytes_feature(\n          data[\'filename\'].encode(\'utf8\')),\n      \'image/source_id\': dataset_util.bytes_feature(\n          data[\'filename\'].encode(\'utf8\')),\n      \'image/key/sha256\': dataset_util.bytes_feature(key.encode(\'utf8\')),\n      \'image/encoded\': dataset_util.bytes_feature(encoded_jpg),\n      \'image/format\': dataset_util.bytes_feature(\'jpeg\'.encode(\'utf8\')),\n      \'image/object/bbox/xmin\': dataset_util.float_list_feature(xmin),\n      \'image/object/bbox/xmax\': dataset_util.float_list_feature(xmax),\n      \'image/object/bbox/ymin\': dataset_util.float_list_feature(ymin),\n      \'image/object/bbox/ymax\': dataset_util.float_list_feature(ymax),\n      \'image/object/class/text\': dataset_util.bytes_list_feature(classes_text),\n      \'image/object/class/label\': dataset_util.int64_list_feature(classes),\n      \'image/object/difficult\': dataset_util.int64_list_feature(difficult_obj),\n      \'image/object/truncated\': dataset_util.int64_list_feature(truncated),\n      \'image/object/view\': dataset_util.bytes_list_feature(poses),\n  }))\n  return example\n\n\ndef main(_):\n  if FLAGS.set not in SETS:\n    raise ValueError(\'set must be in : {}\'.format(SETS))\n  if FLAGS.year not in YEARS:\n    raise ValueError(\'year must be in : {}\'.format(YEARS))\n\n  data_dir = FLAGS.data_dir\n  years = [\'VOC2007\', \'VOC2012\']\n  if FLAGS.year != \'merged\':\n    years = [FLAGS.year]\n\n  writer = tf.python_io.TFRecordWriter(FLAGS.output_path)\n\n  label_map_dict = label_map_util.get_label_map_dict(FLAGS.label_map_path)\n\n  for year in years:\n    logging.info(\'Reading from PASCAL %s dataset.\', year)\n    examples_path = os.path.join(data_dir, year, \'ImageSets\', \'Main\',\n                                 \'aeroplane_\' + FLAGS.set + \'.txt\')\n    annotations_dir = os.path.join(data_dir, year, FLAGS.annotations_dir)\n    examples_list = dataset_util.read_examples_list(examples_path)\n    for idx, example in enumerate(examples_list):\n      if idx % 100 == 0:\n        logging.info(\'On image %d of %d\', idx, len(examples_list))\n      path = os.path.join(annotations_dir, example + \'.xml\')\n      with tf.gfile.GFile(path, \'r\') as fid:\n        xml_str = fid.read()\n      xml = etree.fromstring(xml_str)\n      data = dataset_util.recursive_parse_xml_to_dict(xml)[\'annotation\']\n\n      tf_example = dict_to_tf_example(data, FLAGS.data_dir, label_map_dict,\n                                      FLAGS.ignore_difficult_instances)\n      writer.write(tf_example.SerializeToString())\n\n  writer.close()\n\n\nif __name__ == \'__main__\':\n  tf.app.run()\n'"
src/object_detection/dataset_tools/create_pascal_tf_record_test.py,2,"b'# Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n""""""Test for create_pascal_tf_record.py.""""""\n\nimport os\n\nimport numpy as np\nimport PIL.Image\nimport tensorflow as tf\n\nfrom object_detection.dataset_tools import create_pascal_tf_record\n\n\nclass CreatePascalTFRecordTest(tf.test.TestCase):\n\n  def _assertProtoEqual(self, proto_field, expectation):\n    """"""Helper function to assert if a proto field equals some value.\n\n    Args:\n      proto_field: The protobuf field to compare.\n      expectation: The expected value of the protobuf field.\n    """"""\n    proto_list = [p for p in proto_field]\n    self.assertListEqual(proto_list, expectation)\n\n  def test_dict_to_tf_example(self):\n    image_file_name = \'tmp_image.jpg\'\n    image_data = np.random.rand(256, 256, 3)\n    save_path = os.path.join(self.get_temp_dir(), image_file_name)\n    image = PIL.Image.fromarray(image_data, \'RGB\')\n    image.save(save_path)\n\n    data = {\n        \'folder\': \'\',\n        \'filename\': image_file_name,\n        \'size\': {\n            \'height\': 256,\n            \'width\': 256,\n        },\n        \'object\': [\n            {\n                \'difficult\': 1,\n                \'bndbox\': {\n                    \'xmin\': 64,\n                    \'ymin\': 64,\n                    \'xmax\': 192,\n                    \'ymax\': 192,\n                },\n                \'name\': \'person\',\n                \'truncated\': 0,\n                \'pose\': \'\',\n            },\n        ],\n    }\n\n    label_map_dict = {\n        \'background\': 0,\n        \'person\': 1,\n        \'notperson\': 2,\n    }\n\n    example = create_pascal_tf_record.dict_to_tf_example(\n        data, self.get_temp_dir(), label_map_dict, image_subdirectory=\'\')\n    self._assertProtoEqual(\n        example.features.feature[\'image/height\'].int64_list.value, [256])\n    self._assertProtoEqual(\n        example.features.feature[\'image/width\'].int64_list.value, [256])\n    self._assertProtoEqual(\n        example.features.feature[\'image/filename\'].bytes_list.value,\n        [image_file_name])\n    self._assertProtoEqual(\n        example.features.feature[\'image/source_id\'].bytes_list.value,\n        [image_file_name])\n    self._assertProtoEqual(\n        example.features.feature[\'image/format\'].bytes_list.value, [\'jpeg\'])\n    self._assertProtoEqual(\n        example.features.feature[\'image/object/bbox/xmin\'].float_list.value,\n        [0.25])\n    self._assertProtoEqual(\n        example.features.feature[\'image/object/bbox/ymin\'].float_list.value,\n        [0.25])\n    self._assertProtoEqual(\n        example.features.feature[\'image/object/bbox/xmax\'].float_list.value,\n        [0.75])\n    self._assertProtoEqual(\n        example.features.feature[\'image/object/bbox/ymax\'].float_list.value,\n        [0.75])\n    self._assertProtoEqual(\n        example.features.feature[\'image/object/class/text\'].bytes_list.value,\n        [\'person\'])\n    self._assertProtoEqual(\n        example.features.feature[\'image/object/class/label\'].int64_list.value,\n        [1])\n    self._assertProtoEqual(\n        example.features.feature[\'image/object/difficult\'].int64_list.value,\n        [1])\n    self._assertProtoEqual(\n        example.features.feature[\'image/object/truncated\'].int64_list.value,\n        [0])\n    self._assertProtoEqual(\n        example.features.feature[\'image/object/view\'].bytes_list.value, [\'\'])\n\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
src/object_detection/dataset_tools/create_pet_tf_record.py,9,"b'# Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nr""""""Convert the Oxford pet dataset to TFRecord for object_detection.\n\nSee: O. M. Parkhi, A. Vedaldi, A. Zisserman, C. V. Jawahar\n     Cats and Dogs\n     IEEE Conference on Computer Vision and Pattern Recognition, 2012\n     http://www.robots.ox.ac.uk/~vgg/data/pets/\n\nExample usage:\n    python object_detection/dataset_tools/create_pet_tf_record.py \\\n        --data_dir=/home/user/pet \\\n        --output_dir=/home/user/pet/output\n""""""\n\nimport hashlib\nimport io\nimport logging\nimport os\nimport random\nimport re\n\nfrom lxml import etree\nimport numpy as np\nimport PIL.Image\nimport tensorflow as tf\n\nfrom object_detection.utils import dataset_util\nfrom object_detection.utils import label_map_util\n\nflags = tf.app.flags\nflags.DEFINE_string(\'data_dir\', \'\', \'Root directory to raw pet dataset.\')\nflags.DEFINE_string(\'output_dir\', \'\', \'Path to directory to output TFRecords.\')\nflags.DEFINE_string(\'label_map_path\', \'data/pet_label_map.pbtxt\',\n                    \'Path to label map proto\')\nflags.DEFINE_boolean(\'faces_only\', True, \'If True, generates bounding boxes \'\n                     \'for pet faces.  Otherwise generates bounding boxes (as \'\n                     \'well as segmentations for full pet bodies).  Note that \'\n                     \'in the latter case, the resulting files are much larger.\')\nflags.DEFINE_string(\'mask_type\', \'png\', \'How to represent instance \'\n                    \'segmentation masks. Options are ""png"" or ""numerical"".\')\nFLAGS = flags.FLAGS\n\n\ndef get_class_name_from_filename(file_name):\n  """"""Gets the class name from a file.\n\n  Args:\n    file_name: The file name to get the class name from.\n               ie. ""american_pit_bull_terrier_105.jpg""\n\n  Returns:\n    A string of the class name.\n  """"""\n  match = re.match(r\'([A-Za-z_]+)(_[0-9]+\\.jpg)\', file_name, re.I)\n  return match.groups()[0]\n\n\ndef dict_to_tf_example(data,\n                       mask_path,\n                       label_map_dict,\n                       image_subdirectory,\n                       ignore_difficult_instances=False,\n                       faces_only=True,\n                       mask_type=\'png\'):\n  """"""Convert XML derived dict to tf.Example proto.\n\n  Notice that this function normalizes the bounding box coordinates provided\n  by the raw data.\n\n  Args:\n    data: dict holding PASCAL XML fields for a single image (obtained by\n      running dataset_util.recursive_parse_xml_to_dict)\n    mask_path: String path to PNG encoded mask.\n    label_map_dict: A map from string label names to integers ids.\n    image_subdirectory: String specifying subdirectory within the\n      Pascal dataset directory holding the actual image data.\n    ignore_difficult_instances: Whether to skip difficult instances in the\n      dataset  (default: False).\n    faces_only: If True, generates bounding boxes for pet faces.  Otherwise\n      generates bounding boxes (as well as segmentations for full pet bodies).\n    mask_type: \'numerical\' or \'png\'. \'png\' is recommended because it leads to\n      smaller file sizes.\n\n  Returns:\n    example: The converted tf.Example.\n\n  Raises:\n    ValueError: if the image pointed to by data[\'filename\'] is not a valid JPEG\n  """"""\n  img_path = os.path.join(image_subdirectory, data[\'filename\'])\n  with tf.gfile.GFile(img_path, \'rb\') as fid:\n    encoded_jpg = fid.read()\n  encoded_jpg_io = io.BytesIO(encoded_jpg)\n  image = PIL.Image.open(encoded_jpg_io)\n  if image.format != \'JPEG\':\n    raise ValueError(\'Image format not JPEG\')\n  key = hashlib.sha256(encoded_jpg).hexdigest()\n\n  with tf.gfile.GFile(mask_path, \'rb\') as fid:\n    encoded_mask_png = fid.read()\n  encoded_png_io = io.BytesIO(encoded_mask_png)\n  mask = PIL.Image.open(encoded_png_io)\n  if mask.format != \'PNG\':\n    raise ValueError(\'Mask format not PNG\')\n\n  mask_np = np.asarray(mask)\n  nonbackground_indices_x = np.any(mask_np != 2, axis=0)\n  nonbackground_indices_y = np.any(mask_np != 2, axis=1)\n  nonzero_x_indices = np.where(nonbackground_indices_x)\n  nonzero_y_indices = np.where(nonbackground_indices_y)\n\n  width = int(data[\'size\'][\'width\'])\n  height = int(data[\'size\'][\'height\'])\n\n  xmins = []\n  ymins = []\n  xmaxs = []\n  ymaxs = []\n  classes = []\n  classes_text = []\n  truncated = []\n  poses = []\n  difficult_obj = []\n  masks = []\n  for obj in data[\'object\']:\n    difficult = bool(int(obj[\'difficult\']))\n    if ignore_difficult_instances and difficult:\n      continue\n    difficult_obj.append(int(difficult))\n\n    if faces_only:\n      xmin = float(obj[\'bndbox\'][\'xmin\'])\n      xmax = float(obj[\'bndbox\'][\'xmax\'])\n      ymin = float(obj[\'bndbox\'][\'ymin\'])\n      ymax = float(obj[\'bndbox\'][\'ymax\'])\n    else:\n      xmin = float(np.min(nonzero_x_indices))\n      xmax = float(np.max(nonzero_x_indices))\n      ymin = float(np.min(nonzero_y_indices))\n      ymax = float(np.max(nonzero_y_indices))\n\n    xmins.append(xmin / width)\n    ymins.append(ymin / height)\n    xmaxs.append(xmax / width)\n    ymaxs.append(ymax / height)\n    class_name = get_class_name_from_filename(data[\'filename\'])\n    classes_text.append(class_name.encode(\'utf8\'))\n    classes.append(label_map_dict[class_name])\n    truncated.append(int(obj[\'truncated\']))\n    poses.append(obj[\'pose\'].encode(\'utf8\'))\n    if not faces_only:\n      mask_remapped = (mask_np != 2).astype(np.uint8)\n      masks.append(mask_remapped)\n\n  feature_dict = {\n      \'image/height\': dataset_util.int64_feature(height),\n      \'image/width\': dataset_util.int64_feature(width),\n      \'image/filename\': dataset_util.bytes_feature(\n          data[\'filename\'].encode(\'utf8\')),\n      \'image/source_id\': dataset_util.bytes_feature(\n          data[\'filename\'].encode(\'utf8\')),\n      \'image/key/sha256\': dataset_util.bytes_feature(key.encode(\'utf8\')),\n      \'image/encoded\': dataset_util.bytes_feature(encoded_jpg),\n      \'image/format\': dataset_util.bytes_feature(\'jpeg\'.encode(\'utf8\')),\n      \'image/object/bbox/xmin\': dataset_util.float_list_feature(xmins),\n      \'image/object/bbox/xmax\': dataset_util.float_list_feature(xmaxs),\n      \'image/object/bbox/ymin\': dataset_util.float_list_feature(ymins),\n      \'image/object/bbox/ymax\': dataset_util.float_list_feature(ymaxs),\n      \'image/object/class/text\': dataset_util.bytes_list_feature(classes_text),\n      \'image/object/class/label\': dataset_util.int64_list_feature(classes),\n      \'image/object/difficult\': dataset_util.int64_list_feature(difficult_obj),\n      \'image/object/truncated\': dataset_util.int64_list_feature(truncated),\n      \'image/object/view\': dataset_util.bytes_list_feature(poses),\n  }\n  if not faces_only:\n    if mask_type == \'numerical\':\n      mask_stack = np.stack(masks).astype(np.float32)\n      masks_flattened = np.reshape(mask_stack, [-1])\n      feature_dict[\'image/object/mask\'] = (\n          dataset_util.float_list_feature(masks_flattened.tolist()))\n    elif mask_type == \'png\':\n      encoded_mask_png_list = []\n      for mask in masks:\n        img = PIL.Image.fromarray(mask)\n        output = io.BytesIO()\n        img.save(output, format=\'PNG\')\n        encoded_mask_png_list.append(output.getvalue())\n      feature_dict[\'image/object/mask\'] = (\n          dataset_util.bytes_list_feature(encoded_mask_png_list))\n\n  example = tf.train.Example(features=tf.train.Features(feature=feature_dict))\n  return example\n\n\ndef create_tf_record(output_filename,\n                     label_map_dict,\n                     annotations_dir,\n                     image_dir,\n                     examples,\n                     faces_only=True,\n                     mask_type=\'png\'):\n  """"""Creates a TFRecord file from examples.\n\n  Args:\n    output_filename: Path to where output file is saved.\n    label_map_dict: The label map dictionary.\n    annotations_dir: Directory where annotation files are stored.\n    image_dir: Directory where image files are stored.\n    examples: Examples to parse and save to tf record.\n    faces_only: If True, generates bounding boxes for pet faces.  Otherwise\n      generates bounding boxes (as well as segmentations for full pet bodies).\n    mask_type: \'numerical\' or \'png\'. \'png\' is recommended because it leads to\n      smaller file sizes.\n  """"""\n  writer = tf.python_io.TFRecordWriter(output_filename)\n  for idx, example in enumerate(examples):\n    if idx % 100 == 0:\n      logging.info(\'On image %d of %d\', idx, len(examples))\n    xml_path = os.path.join(annotations_dir, \'xmls\', example + \'.xml\')\n    mask_path = os.path.join(annotations_dir, \'trimaps\', example + \'.png\')\n\n    if not os.path.exists(xml_path):\n      logging.warning(\'Could not find %s, ignoring example.\', xml_path)\n      continue\n    with tf.gfile.GFile(xml_path, \'r\') as fid:\n      xml_str = fid.read()\n    xml = etree.fromstring(xml_str)\n    data = dataset_util.recursive_parse_xml_to_dict(xml)[\'annotation\']\n\n    try:\n      tf_example = dict_to_tf_example(\n          data,\n          mask_path,\n          label_map_dict,\n          image_dir,\n          faces_only=faces_only,\n          mask_type=mask_type)\n      writer.write(tf_example.SerializeToString())\n    except ValueError:\n      logging.warning(\'Invalid example: %s, ignoring.\', xml_path)\n\n  writer.close()\n\n\n# TODO(derekjchow): Add test for pet/PASCAL main files.\ndef main(_):\n  data_dir = FLAGS.data_dir\n  label_map_dict = label_map_util.get_label_map_dict(FLAGS.label_map_path)\n\n  logging.info(\'Reading from Pet dataset.\')\n  image_dir = os.path.join(data_dir, \'images\')\n  annotations_dir = os.path.join(data_dir, \'annotations\')\n  examples_path = os.path.join(annotations_dir, \'trainval.txt\')\n  examples_list = dataset_util.read_examples_list(examples_path)\n\n  # Test images are not included in the downloaded data set, so we shall perform\n  # our own split.\n  random.seed(42)\n  random.shuffle(examples_list)\n  num_examples = len(examples_list)\n  num_train = int(0.7 * num_examples)\n  train_examples = examples_list[:num_train]\n  val_examples = examples_list[num_train:]\n  logging.info(\'%d training and %d validation examples.\',\n               len(train_examples), len(val_examples))\n\n  train_output_path = os.path.join(FLAGS.output_dir, \'pet_train.record\')\n  val_output_path = os.path.join(FLAGS.output_dir, \'pet_val.record\')\n  if FLAGS.faces_only:\n    train_output_path = os.path.join(FLAGS.output_dir,\n                                     \'pet_train_with_masks.record\')\n    val_output_path = os.path.join(FLAGS.output_dir,\n                                   \'pet_val_with_masks.record\')\n  create_tf_record(\n      train_output_path,\n      label_map_dict,\n      annotations_dir,\n      image_dir,\n      train_examples,\n      faces_only=FLAGS.faces_only,\n      mask_type=FLAGS.mask_type)\n  create_tf_record(\n      val_output_path,\n      label_map_dict,\n      annotations_dir,\n      image_dir,\n      val_examples,\n      faces_only=FLAGS.faces_only,\n      mask_type=FLAGS.mask_type)\n\n\nif __name__ == \'__main__\':\n  tf.app.run()\n'"
src/object_detection/dataset_tools/oid_tfrecord_creation.py,2,"b'# Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\nr""""""Utilities for creating TFRecords of TF examples for the Open Images dataset.\n""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\n\nfrom object_detection.core import standard_fields\nfrom object_detection.utils import dataset_util\n\n\ndef tf_example_from_annotations_data_frame(annotations_data_frame, label_map,\n                                           encoded_image):\n  """"""Populates a TF Example message with image annotations from a data frame.\n\n  Args:\n    annotations_data_frame: Data frame containing the annotations for a single\n      image.\n    label_map: String to integer label map.\n    encoded_image: The encoded image string\n\n  Returns:\n    The populated TF Example, if the label of at least one object is present in\n    label_map. Otherwise, returns None.\n  """"""\n\n  filtered_data_frame = annotations_data_frame[\n      annotations_data_frame.LabelName.isin(label_map)]\n\n  image_id = annotations_data_frame.ImageID.iloc[0]\n\n  feature_map = {\n      standard_fields.TfExampleFields.object_bbox_ymin:\n          dataset_util.float_list_feature(filtered_data_frame.YMin.as_matrix()),\n      standard_fields.TfExampleFields.object_bbox_xmin:\n          dataset_util.float_list_feature(filtered_data_frame.XMin.as_matrix()),\n      standard_fields.TfExampleFields.object_bbox_ymax:\n          dataset_util.float_list_feature(filtered_data_frame.YMax.as_matrix()),\n      standard_fields.TfExampleFields.object_bbox_xmax:\n          dataset_util.float_list_feature(filtered_data_frame.XMax.as_matrix()),\n      standard_fields.TfExampleFields.object_class_text:\n          dataset_util.bytes_list_feature(\n              filtered_data_frame.LabelName.as_matrix()),\n      standard_fields.TfExampleFields.object_class_label:\n          dataset_util.int64_list_feature(\n              filtered_data_frame.LabelName.map(lambda x: label_map[x])\n              .as_matrix()),\n      standard_fields.TfExampleFields.filename:\n          dataset_util.bytes_feature(\'{}.jpg\'.format(image_id)),\n      standard_fields.TfExampleFields.source_id:\n          dataset_util.bytes_feature(image_id),\n      standard_fields.TfExampleFields.image_encoded:\n          dataset_util.bytes_feature(encoded_image),\n  }\n\n  if \'IsGroupOf\' in filtered_data_frame.columns:\n    feature_map[standard_fields.TfExampleFields.\n                object_group_of] = dataset_util.int64_list_feature(\n                    filtered_data_frame.IsGroupOf.as_matrix().astype(int))\n  if \'IsOccluded\' in filtered_data_frame.columns:\n    feature_map[standard_fields.TfExampleFields.\n                object_occluded] = dataset_util.int64_list_feature(\n                    filtered_data_frame.IsOccluded.as_matrix().astype(int))\n  if \'IsTruncated\' in filtered_data_frame.columns:\n    feature_map[standard_fields.TfExampleFields.\n                object_truncated] = dataset_util.int64_list_feature(\n                    filtered_data_frame.IsTruncated.as_matrix().astype(int))\n  if \'IsDepiction\' in filtered_data_frame.columns:\n    feature_map[standard_fields.TfExampleFields.\n                object_depiction] = dataset_util.int64_list_feature(\n                    filtered_data_frame.IsDepiction.as_matrix().astype(int))\n\n  return tf.train.Example(features=tf.train.Features(feature=feature_map))\n\n\ndef open_sharded_output_tfrecords(exit_stack, base_path, num_shards):\n  """"""Opens all TFRecord shards for writing and adds them to an exit stack.\n\n  Args:\n    exit_stack: A context2.ExitStack used to automatically closed the TFRecords\n      opened in this function.\n    base_path: The base path for all shards\n    num_shards: The number of shards\n\n  Returns:\n    The list of opened TFRecords. Position k in the list corresponds to shard k.\n  """"""\n  tf_record_output_filenames = [\n      \'{}-{:05d}-of-{:05d}\'.format(base_path, idx, num_shards)\n      for idx in range(num_shards)\n  ]\n\n  tfrecords = [\n      exit_stack.enter_context(tf.python_io.TFRecordWriter(file_name))\n      for file_name in tf_record_output_filenames\n  ]\n\n  return tfrecords\n'"
src/object_detection/dataset_tools/oid_tfrecord_creation_test.py,6,"b'# Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Tests for oid_tfrecord_creation.py.""""""\n\nimport os\nimport contextlib2\nimport pandas as pd\nimport tensorflow as tf\n\nfrom object_detection.dataset_tools import oid_tfrecord_creation\n\n\ndef create_test_data():\n  data = {\n      \'ImageID\': [\'i1\', \'i1\', \'i1\', \'i1\', \'i2\', \'i2\'],\n      \'LabelName\': [\'a\', \'a\', \'b\', \'b\', \'b\', \'c\'],\n      \'YMin\': [0.3, 0.6, 0.8, 0.1, 0.0, 0.0],\n      \'XMin\': [0.1, 0.3, 0.7, 0.0, 0.1, 0.1],\n      \'XMax\': [0.2, 0.3, 0.8, 0.5, 0.9, 0.9],\n      \'YMax\': [0.3, 0.6, 1, 0.8, 0.8, 0.8],\n      \'IsOccluded\': [0, 1, 1, 0, 0, 0],\n      \'IsTruncated\': [0, 0, 0, 1, 0, 0],\n      \'IsGroupOf\': [0, 0, 0, 0, 0, 1],\n      \'IsDepiction\': [1, 0, 0, 0, 0, 0],\n  }\n  df = pd.DataFrame(data=data)\n  label_map = {\'a\': 0, \'b\': 1, \'c\': 2}\n  return label_map, df\n\n\nclass TfExampleFromAnnotationsDataFrameTests(tf.test.TestCase):\n\n  def test_simple(self):\n    label_map, df = create_test_data()\n\n    tf_example = oid_tfrecord_creation.tf_example_from_annotations_data_frame(\n        df[df.ImageID == \'i1\'], label_map, \'encoded_image_test\')\n    self.assertProtoEquals(""""""\n        features {\n          feature {\n            key: ""image/encoded""\n            value { bytes_list { value: ""encoded_image_test"" } } }\n          feature {\n            key: ""image/filename""\n            value { bytes_list { value: ""i1.jpg"" } } }\n          feature {\n            key: ""image/object/bbox/ymin""\n            value { float_list { value: [0.3, 0.6, 0.8, 0.1] } } }\n          feature {\n            key: ""image/object/bbox/xmin""\n            value { float_list { value: [0.1, 0.3, 0.7, 0.0] } } }\n          feature {\n            key: ""image/object/bbox/ymax""\n            value { float_list { value: [0.3, 0.6, 1.0, 0.8] } } }\n          feature {\n            key: ""image/object/bbox/xmax""\n            value { float_list { value: [0.2, 0.3, 0.8, 0.5] } } }\n          feature {\n            key: ""image/object/class/label""\n            value { int64_list { value: [0, 0, 1, 1] } } }\n          feature {\n            key: ""image/object/class/text""\n            value { bytes_list { value: [""a"", ""a"", ""b"", ""b""] } } }\n          feature {\n            key: ""image/source_id""\n            value { bytes_list { value: ""i1"" } } }\n          feature {\n            key: ""image/object/depiction""\n            value { int64_list { value: [1, 0, 0, 0] } } }\n          feature {\n            key: ""image/object/group_of""\n            value { int64_list { value: [0, 0, 0, 0] } } }\n          feature {\n            key: ""image/object/occluded""\n            value { int64_list { value: [0, 1, 1, 0] } } }\n          feature {\n            key: ""image/object/truncated""\n            value { int64_list { value: [0, 0, 0, 1] } } } }\n    """""", tf_example)\n\n  def test_no_attributes(self):\n    label_map, df = create_test_data()\n\n    del df[\'IsDepiction\']\n    del df[\'IsGroupOf\']\n    del df[\'IsOccluded\']\n    del df[\'IsTruncated\']\n\n    tf_example = oid_tfrecord_creation.tf_example_from_annotations_data_frame(\n        df[df.ImageID == \'i2\'], label_map, \'encoded_image_test\')\n    self.assertProtoEquals(""""""\n        features {\n          feature {\n            key: ""image/encoded""\n            value { bytes_list { value: ""encoded_image_test"" } } }\n          feature {\n            key: ""image/filename""\n            value { bytes_list { value: ""i2.jpg"" } } }\n          feature {\n            key: ""image/object/bbox/ymin""\n            value { float_list { value: [0.0, 0.0] } } }\n          feature {\n            key: ""image/object/bbox/xmin""\n            value { float_list { value: [0.1, 0.1] } } }\n          feature {\n            key: ""image/object/bbox/ymax""\n            value { float_list { value: [0.8, 0.8] } } }\n          feature {\n            key: ""image/object/bbox/xmax""\n            value { float_list { value: [0.9, 0.9] } } }\n          feature {\n            key: ""image/object/class/label""\n            value { int64_list { value: [1, 2] } } }\n          feature {\n            key: ""image/object/class/text""\n            value { bytes_list { value: [""b"", ""c""] } } }\n          feature {\n            key: ""image/source_id""\n           value { bytes_list { value: ""i2"" } } } }\n    """""", tf_example)\n\n  def test_label_filtering(self):\n    label_map, df = create_test_data()\n\n    label_map = {\'a\': 0}\n\n    tf_example = oid_tfrecord_creation.tf_example_from_annotations_data_frame(\n        df[df.ImageID == \'i1\'], label_map, \'encoded_image_test\')\n    self.assertProtoEquals(""""""\n        features {\n          feature {\n            key: ""image/encoded""\n            value { bytes_list { value: ""encoded_image_test"" } } }\n          feature {\n            key: ""image/filename""\n            value { bytes_list { value: ""i1.jpg"" } } }\n          feature {\n            key: ""image/object/bbox/ymin""\n            value { float_list { value: [0.3, 0.6] } } }\n          feature {\n            key: ""image/object/bbox/xmin""\n            value { float_list { value: [0.1, 0.3] } } }\n          feature {\n            key: ""image/object/bbox/ymax""\n            value { float_list { value: [0.3, 0.6] } } }\n          feature {\n            key: ""image/object/bbox/xmax""\n            value { float_list { value: [0.2, 0.3] } } }\n          feature {\n            key: ""image/object/class/label""\n            value { int64_list { value: [0, 0] } } }\n          feature {\n            key: ""image/object/class/text""\n            value { bytes_list { value: [""a"", ""a""] } } }\n          feature {\n            key: ""image/source_id""\n            value { bytes_list { value: ""i1"" } } }\n          feature {\n            key: ""image/object/depiction""\n            value { int64_list { value: [1, 0] } } }\n          feature {\n            key: ""image/object/group_of""\n            value { int64_list { value: [0, 0] } } }\n          feature {\n            key: ""image/object/occluded""\n            value { int64_list { value: [0, 1] } } }\n          feature {\n            key: ""image/object/truncated""\n            value { int64_list { value: [0, 0] } } } }\n    """""", tf_example)\n\n\nclass OpenOutputTfrecordsTests(tf.test.TestCase):\n\n  def test_sharded_tfrecord_writes(self):\n    with contextlib2.ExitStack() as tf_record_close_stack:\n      output_tfrecords = oid_tfrecord_creation.open_sharded_output_tfrecords(\n          tf_record_close_stack,\n          os.path.join(tf.test.get_temp_dir(), \'test.tfrec\'), 10)\n      for idx in range(10):\n        output_tfrecords[idx].write(\'test_{}\'.format(idx))\n\n    for idx in range(10):\n      tf_record_path = \'{}-{:05d}-of-00010\'.format(\n          os.path.join(tf.test.get_temp_dir(), \'test.tfrec\'), idx)\n      records = list(tf.python_io.tf_record_iterator(tf_record_path))\n      self.assertAllEqual(records, [\'test_{}\'.format(idx)])\n\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
src/object_detection/inference/__init__.py,0,b''
src/object_detection/inference/detection_inference.py,18,"b'# Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Utility functions for detection inference.""""""\nfrom __future__ import division\n\nimport tensorflow as tf\n\nfrom object_detection.core import standard_fields\n\n\ndef build_input(tfrecord_paths):\n  """"""Builds the graph\'s input.\n\n  Args:\n    tfrecord_paths: List of paths to the input TFRecords\n\n  Returns:\n    serialized_example_tensor: The next serialized example. String scalar Tensor\n    image_tensor: The decoded image of the example. Uint8 tensor,\n        shape=[1, None, None,3]\n  """"""\n  filename_queue = tf.train.string_input_producer(\n      tfrecord_paths, shuffle=False, num_epochs=1)\n\n  tf_record_reader = tf.TFRecordReader()\n  _, serialized_example_tensor = tf_record_reader.read(filename_queue)\n  features = tf.parse_single_example(\n      serialized_example_tensor,\n      features={\n          standard_fields.TfExampleFields.image_encoded:\n              tf.FixedLenFeature([], tf.string),\n      })\n  encoded_image = features[standard_fields.TfExampleFields.image_encoded]\n  image_tensor = tf.image.decode_image(encoded_image, channels=3)\n  image_tensor.set_shape([None, None, 3])\n  image_tensor = tf.expand_dims(image_tensor, 0)\n\n  return serialized_example_tensor, image_tensor\n\n\ndef build_inference_graph(image_tensor, inference_graph_path):\n  """"""Loads the inference graph and connects it to the input image.\n\n  Args:\n    image_tensor: The input image. uint8 tensor, shape=[1, None, None, 3]\n    inference_graph_path: Path to the inference graph with embedded weights\n\n  Returns:\n    detected_boxes_tensor: Detected boxes. Float tensor,\n        shape=[num_detections, 4]\n    detected_scores_tensor: Detected scores. Float tensor,\n        shape=[num_detections]\n    detected_labels_tensor: Detected labels. Int64 tensor,\n        shape=[num_detections]\n  """"""\n  with tf.gfile.Open(inference_graph_path, \'r\') as graph_def_file:\n    graph_content = graph_def_file.read()\n  graph_def = tf.GraphDef()\n  graph_def.MergeFromString(graph_content)\n\n  tf.import_graph_def(\n      graph_def, name=\'\', input_map={\'image_tensor\': image_tensor})\n\n  g = tf.get_default_graph()\n\n  num_detections_tensor = tf.squeeze(\n      g.get_tensor_by_name(\'num_detections:0\'), 0)\n  num_detections_tensor = tf.cast(num_detections_tensor, tf.int32)\n\n  detected_boxes_tensor = tf.squeeze(\n      g.get_tensor_by_name(\'detection_boxes:0\'), 0)\n  detected_boxes_tensor = detected_boxes_tensor[:num_detections_tensor]\n\n  detected_scores_tensor = tf.squeeze(\n      g.get_tensor_by_name(\'detection_scores:0\'), 0)\n  detected_scores_tensor = detected_scores_tensor[:num_detections_tensor]\n\n  detected_labels_tensor = tf.squeeze(\n      g.get_tensor_by_name(\'detection_classes:0\'), 0)\n  detected_labels_tensor = tf.cast(detected_labels_tensor, tf.int64)\n  detected_labels_tensor = detected_labels_tensor[:num_detections_tensor]\n\n  return detected_boxes_tensor, detected_scores_tensor, detected_labels_tensor\n\n\ndef infer_detections_and_add_to_example(\n    serialized_example_tensor, detected_boxes_tensor, detected_scores_tensor,\n    detected_labels_tensor, discard_image_pixels):\n  """"""Runs the supplied tensors and adds the inferred detections to the example.\n\n  Args:\n    serialized_example_tensor: Serialized TF example. Scalar string tensor\n    detected_boxes_tensor: Detected boxes. Float tensor,\n        shape=[num_detections, 4]\n    detected_scores_tensor: Detected scores. Float tensor,\n        shape=[num_detections]\n    detected_labels_tensor: Detected labels. Int64 tensor,\n        shape=[num_detections]\n    discard_image_pixels: If true, discards the image from the result\n  Returns:\n    The de-serialized TF example augmented with the inferred detections.\n  """"""\n  tf_example = tf.train.Example()\n  (serialized_example, detected_boxes, detected_scores,\n   detected_classes) = tf.get_default_session().run([\n       serialized_example_tensor, detected_boxes_tensor, detected_scores_tensor,\n       detected_labels_tensor\n   ])\n  detected_boxes = detected_boxes.T\n\n  tf_example.ParseFromString(serialized_example)\n  feature = tf_example.features.feature\n  feature[standard_fields.TfExampleFields.\n          detection_score].float_list.value[:] = detected_scores\n  feature[standard_fields.TfExampleFields.\n          detection_bbox_ymin].float_list.value[:] = detected_boxes[0]\n  feature[standard_fields.TfExampleFields.\n          detection_bbox_xmin].float_list.value[:] = detected_boxes[1]\n  feature[standard_fields.TfExampleFields.\n          detection_bbox_ymax].float_list.value[:] = detected_boxes[2]\n  feature[standard_fields.TfExampleFields.\n          detection_bbox_xmax].float_list.value[:] = detected_boxes[3]\n  feature[standard_fields.TfExampleFields.\n          detection_class_label].int64_list.value[:] = detected_classes\n\n  if discard_image_pixels:\n    del feature[standard_fields.TfExampleFields.image_encoded]\n\n  return tf_example\n'"
src/object_detection/inference/detection_inference_test.py,22,"b'# Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\nr""""""Tests for detection_inference.py.""""""\n\nimport os\nimport StringIO\n\nimport numpy as np\nfrom PIL import Image\nimport tensorflow as tf\n\nfrom object_detection.core import standard_fields\nfrom object_detection.inference import detection_inference\nfrom object_detection.utils import dataset_util\n\n\ndef get_mock_tfrecord_path():\n  return os.path.join(tf.test.get_temp_dir(), \'mock.tfrec\')\n\n\ndef create_mock_tfrecord():\n  pil_image = Image.fromarray(np.array([[[123, 0, 0]]], dtype=np.uint8), \'RGB\')\n  image_output_stream = StringIO.StringIO()\n  pil_image.save(image_output_stream, format=\'png\')\n  encoded_image = image_output_stream.getvalue()\n\n  feature_map = {\n      \'test_field\':\n          dataset_util.float_list_feature([1, 2, 3, 4]),\n      standard_fields.TfExampleFields.image_encoded:\n          dataset_util.bytes_feature(encoded_image),\n  }\n\n  tf_example = tf.train.Example(features=tf.train.Features(feature=feature_map))\n  with tf.python_io.TFRecordWriter(get_mock_tfrecord_path()) as writer:\n    writer.write(tf_example.SerializeToString())\n\n\ndef get_mock_graph_path():\n  return os.path.join(tf.test.get_temp_dir(), \'mock_graph.pb\')\n\n\ndef create_mock_graph():\n  g = tf.Graph()\n  with g.as_default():\n    in_image_tensor = tf.placeholder(\n        tf.uint8, shape=[1, None, None, 3], name=\'image_tensor\')\n    tf.constant([2.0], name=\'num_detections\')\n    tf.constant(\n        [[[0, 0.8, 0.7, 1], [0.1, 0.2, 0.8, 0.9], [0.2, 0.3, 0.4, 0.5]]],\n        name=\'detection_boxes\')\n    tf.constant([[0.1, 0.2, 0.3]], name=\'detection_scores\')\n    tf.identity(\n        tf.constant([[1.0, 2.0, 3.0]]) *\n        tf.reduce_sum(tf.cast(in_image_tensor, dtype=tf.float32)),\n        name=\'detection_classes\')\n    graph_def = g.as_graph_def()\n\n  with tf.gfile.Open(get_mock_graph_path(), \'w\') as fl:\n    fl.write(graph_def.SerializeToString())\n\n\nclass InferDetectionsTests(tf.test.TestCase):\n\n  def test_simple(self):\n    create_mock_graph()\n    create_mock_tfrecord()\n\n    serialized_example_tensor, image_tensor = detection_inference.build_input(\n        [get_mock_tfrecord_path()])\n    self.assertAllEqual(image_tensor.get_shape().as_list(), [1, None, None, 3])\n\n    (detected_boxes_tensor, detected_scores_tensor,\n     detected_labels_tensor) = detection_inference.build_inference_graph(\n         image_tensor, get_mock_graph_path())\n\n    with self.test_session(use_gpu=False) as sess:\n      sess.run(tf.global_variables_initializer())\n      sess.run(tf.local_variables_initializer())\n      tf.train.start_queue_runners()\n\n      tf_example = detection_inference.infer_detections_and_add_to_example(\n          serialized_example_tensor, detected_boxes_tensor,\n          detected_scores_tensor, detected_labels_tensor, False)\n\n    self.assertProtoEquals(r""""""\n        features {\n          feature {\n            key: ""image/detection/bbox/ymin""\n            value { float_list { value: [0.0, 0.1] } } }\n          feature {\n            key: ""image/detection/bbox/xmin""\n            value { float_list { value: [0.8, 0.2] } } }\n          feature {\n            key: ""image/detection/bbox/ymax""\n            value { float_list { value: [0.7, 0.8] } } }\n          feature {\n            key: ""image/detection/bbox/xmax""\n            value { float_list { value: [1.0, 0.9] } } }\n          feature {\n            key: ""image/detection/label""\n            value { int64_list { value: [123, 246] } } }\n          feature {\n            key: ""image/detection/score""\n            value { float_list { value: [0.1, 0.2] } } }\n          feature {\n            key: ""image/encoded""\n            value { bytes_list { value:\n              ""\\211PNG\\r\\n\\032\\n\\000\\000\\000\\rIHDR\\000\\000\\000\\001\\000\\000""\n              ""\\000\\001\\010\\002\\000\\000\\000\\220wS\\336\\000\\000\\000\\022IDATx""\n              ""\\234b\\250f`\\000\\000\\000\\000\\377\\377\\003\\000\\001u\\000|gO\\242""\n              ""\\213\\000\\000\\000\\000IEND\\256B`\\202"" } } }\n          feature {\n            key: ""test_field""\n            value { float_list { value: [1.0, 2.0, 3.0, 4.0] } } } }\n    """""", tf_example)\n\n  def test_discard_image(self):\n    create_mock_graph()\n    create_mock_tfrecord()\n\n    serialized_example_tensor, image_tensor = detection_inference.build_input(\n        [get_mock_tfrecord_path()])\n    (detected_boxes_tensor, detected_scores_tensor,\n     detected_labels_tensor) = detection_inference.build_inference_graph(\n         image_tensor, get_mock_graph_path())\n\n    with self.test_session(use_gpu=False) as sess:\n      sess.run(tf.global_variables_initializer())\n      sess.run(tf.local_variables_initializer())\n      tf.train.start_queue_runners()\n\n      tf_example = detection_inference.infer_detections_and_add_to_example(\n          serialized_example_tensor, detected_boxes_tensor,\n          detected_scores_tensor, detected_labels_tensor, True)\n\n    self.assertProtoEquals(r""""""\n        features {\n          feature {\n            key: ""image/detection/bbox/ymin""\n            value { float_list { value: [0.0, 0.1] } } }\n          feature {\n            key: ""image/detection/bbox/xmin""\n            value { float_list { value: [0.8, 0.2] } } }\n          feature {\n            key: ""image/detection/bbox/ymax""\n            value { float_list { value: [0.7, 0.8] } } }\n          feature {\n            key: ""image/detection/bbox/xmax""\n            value { float_list { value: [1.0, 0.9] } } }\n          feature {\n            key: ""image/detection/label""\n            value { int64_list { value: [123, 246] } } }\n          feature {\n            key: ""image/detection/score""\n            value { float_list { value: [0.1, 0.2] } } }\n          feature {\n            key: ""test_field""\n            value { float_list { value: [1.0, 2.0, 3.0, 4.0] } } } }\n    """""", tf_example)\n\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
src/object_detection/inference/infer_detections.py,17,"b'# Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\nr""""""Infers detections on a TFRecord of TFExamples given an inference graph.\n\nExample usage:\n  ./infer_detections \\\n    --input_tfrecord_paths=/path/to/input/tfrecord1,/path/to/input/tfrecord2 \\\n    --output_tfrecord_path_prefix=/path/to/output/detections.tfrecord \\\n    --inference_graph=/path/to/frozen_weights_inference_graph.pb\n\nThe output is a TFRecord of TFExamples. Each TFExample from the input is first\naugmented with detections from the inference graph and then copied to the\noutput.\n\nThe input and output nodes of the inference graph are expected to have the same\ntypes, shapes, and semantics, as the input and output nodes of graphs produced\nby export_inference_graph.py, when run with --input_type=image_tensor.\n\nThe script can also discard the image pixels in the output. This greatly\nreduces the output size and can potentially accelerate reading data in\nsubsequent processing steps that don\'t require the images (e.g. computing\nmetrics).\n""""""\n\nimport itertools\nimport tensorflow as tf\nfrom object_detection.inference import detection_inference\n\ntf.flags.DEFINE_string(\'input_tfrecord_paths\', None,\n                       \'A comma separated list of paths to input TFRecords.\')\ntf.flags.DEFINE_string(\'output_tfrecord_path\', None,\n                       \'Path to the output TFRecord.\')\ntf.flags.DEFINE_string(\'inference_graph\', None,\n                       \'Path to the inference graph with embedded weights.\')\ntf.flags.DEFINE_boolean(\'discard_image_pixels\', False,\n                        \'Discards the images in the output TFExamples. This\'\n                        \' significantly reduces the output size and is useful\'\n                        \' if the subsequent tools don\\\'t need access to the\'\n                        \' images (e.g. when computing evaluation measures).\')\n\nFLAGS = tf.flags.FLAGS\n\n\ndef main(_):\n  tf.logging.set_verbosity(tf.logging.INFO)\n\n  required_flags = [\'input_tfrecord_paths\', \'output_tfrecord_path\',\n                    \'inference_graph\']\n  for flag_name in required_flags:\n    if not getattr(FLAGS, flag_name):\n      raise ValueError(\'Flag --{} is required\'.format(flag_name))\n\n  with tf.Session() as sess:\n    input_tfrecord_paths = [\n        v for v in FLAGS.input_tfrecord_paths.split(\',\') if v]\n    tf.logging.info(\'Reading input from %d files\', len(input_tfrecord_paths))\n    serialized_example_tensor, image_tensor = detection_inference.build_input(\n        input_tfrecord_paths)\n    tf.logging.info(\'Reading graph and building model...\')\n    (detected_boxes_tensor, detected_scores_tensor,\n     detected_labels_tensor) = detection_inference.build_inference_graph(\n         image_tensor, FLAGS.inference_graph)\n\n    tf.logging.info(\'Running inference and writing output to {}\'.format(\n        FLAGS.output_tfrecord_path))\n    sess.run(tf.local_variables_initializer())\n    tf.train.start_queue_runners()\n    with tf.python_io.TFRecordWriter(\n        FLAGS.output_tfrecord_path) as tf_record_writer:\n      try:\n        for counter in itertools.count():\n          tf.logging.log_every_n(tf.logging.INFO, \'Processed %d images...\', 10,\n                                 counter)\n          tf_example = detection_inference.infer_detections_and_add_to_example(\n              serialized_example_tensor, detected_boxes_tensor,\n              detected_scores_tensor, detected_labels_tensor,\n              FLAGS.discard_image_pixels)\n          tf_record_writer.write(tf_example.SerializeToString())\n      except tf.errors.OutOfRangeError:\n        tf.logging.info(\'Finished processing records\')\n\n\nif __name__ == \'__main__\':\n  tf.app.run()\n'"
src/object_detection/matchers/__init__.py,0,b''
src/object_detection/matchers/argmax_matcher.py,21,"b'# Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n""""""Argmax matcher implementation.\n\nThis class takes a similarity matrix and matches columns to rows based on the\nmaximum value per column. One can specify matched_thresholds and\nto prevent columns from matching to rows (generally resulting in a negative\ntraining example) and unmatched_theshold to ignore the match (generally\nresulting in neither a positive or negative training example).\n\nThis matcher is used in Fast(er)-RCNN.\n\nNote: matchers are used in TargetAssigners. There is a create_target_assigner\nfactory function for popular implementations.\n""""""\nimport tensorflow as tf\n\nfrom object_detection.core import matcher\nfrom object_detection.utils import shape_utils\n\n\nclass ArgMaxMatcher(matcher.Matcher):\n  """"""Matcher based on highest value.\n\n  This class computes matches from a similarity matrix. Each column is matched\n  to a single row.\n\n  To support object detection target assignment this class enables setting both\n  matched_threshold (upper threshold) and unmatched_threshold (lower thresholds)\n  defining three categories of similarity which define whether examples are\n  positive, negative, or ignored:\n  (1) similarity >= matched_threshold: Highest similarity. Matched/Positive!\n  (2) matched_threshold > similarity >= unmatched_threshold: Medium similarity.\n          Depending on negatives_lower_than_unmatched, this is either\n          Unmatched/Negative OR Ignore.\n  (3) unmatched_threshold > similarity: Lowest similarity. Depending on flag\n          negatives_lower_than_unmatched, either Unmatched/Negative OR Ignore.\n  For ignored matches this class sets the values in the Match object to -2.\n  """"""\n\n  def __init__(self,\n               matched_threshold,\n               unmatched_threshold=None,\n               negatives_lower_than_unmatched=True,\n               force_match_for_each_row=False,\n               use_matmul_gather=False):\n    """"""Construct ArgMaxMatcher.\n\n    Args:\n      matched_threshold: Threshold for positive matches. Positive if\n        sim >= matched_threshold, where sim is the maximum value of the\n        similarity matrix for a given column. Set to None for no threshold.\n      unmatched_threshold: Threshold for negative matches. Negative if\n        sim < unmatched_threshold. Defaults to matched_threshold\n        when set to None.\n      negatives_lower_than_unmatched: Boolean which defaults to True. If True\n        then negative matches are the ones below the unmatched_threshold,\n        whereas ignored matches are in between the matched and umatched\n        threshold. If False, then negative matches are in between the matched\n        and unmatched threshold, and everything lower than unmatched is ignored.\n      force_match_for_each_row: If True, ensures that each row is matched to\n        at least one column (which is not guaranteed otherwise if the\n        matched_threshold is high). Defaults to False. See\n        argmax_matcher_test.testMatcherForceMatch() for an example.\n      use_matmul_gather: Force constructed match objects to use matrix\n        multiplication based gather instead of standard tf.gather.\n        (Default: False).\n\n    Raises:\n      ValueError: if unmatched_threshold is set but matched_threshold is not set\n        or if unmatched_threshold > matched_threshold.\n    """"""\n    super(ArgMaxMatcher, self).__init__(use_matmul_gather=use_matmul_gather)\n    if (matched_threshold is None) and (unmatched_threshold is not None):\n      raise ValueError(\'Need to also define matched_threshold when\'\n                       \'unmatched_threshold is defined\')\n    self._matched_threshold = matched_threshold\n    if unmatched_threshold is None:\n      self._unmatched_threshold = matched_threshold\n    else:\n      if unmatched_threshold > matched_threshold:\n        raise ValueError(\'unmatched_threshold needs to be smaller or equal\'\n                         \'to matched_threshold\')\n      self._unmatched_threshold = unmatched_threshold\n    if not negatives_lower_than_unmatched:\n      if self._unmatched_threshold == self._matched_threshold:\n        raise ValueError(\'When negatives are in between matched and \'\n                         \'unmatched thresholds, these cannot be of equal \'\n                         \'value. matched: %s, unmatched: %s\',\n                         self._matched_threshold, self._unmatched_threshold)\n    self._force_match_for_each_row = force_match_for_each_row\n    self._negatives_lower_than_unmatched = negatives_lower_than_unmatched\n\n  def _match(self, similarity_matrix):\n    """"""Tries to match each column of the similarity matrix to a row.\n\n    Args:\n      similarity_matrix: tensor of shape [N, M] representing any similarity\n        metric.\n\n    Returns:\n      Match object with corresponding matches for each of M columns.\n    """"""\n\n    def _match_when_rows_are_empty():\n      """"""Performs matching when the rows of similarity matrix are empty.\n\n      When the rows are empty, all detections are false positives. So we return\n      a tensor of -1\'s to indicate that the columns do not match to any rows.\n\n      Returns:\n        matches:  int32 tensor indicating the row each column matches to.\n      """"""\n      similarity_matrix_shape = shape_utils.combined_static_and_dynamic_shape(\n          similarity_matrix)\n      return -1 * tf.ones([similarity_matrix_shape[1]], dtype=tf.int32)\n\n    def _match_when_rows_are_non_empty():\n      """"""Performs matching when the rows of similarity matrix are non empty.\n\n      Returns:\n        matches:  int32 tensor indicating the row each column matches to.\n      """"""\n      # Matches for each column\n      matches = tf.argmax(similarity_matrix, 0, output_type=tf.int32)\n\n      # Deal with matched and unmatched threshold\n      if self._matched_threshold is not None:\n        # Get logical indices of ignored and unmatched columns as tf.int64\n        matched_vals = tf.reduce_max(similarity_matrix, 0)\n        below_unmatched_threshold = tf.greater(self._unmatched_threshold,\n                                               matched_vals)\n        between_thresholds = tf.logical_and(\n            tf.greater_equal(matched_vals, self._unmatched_threshold),\n            tf.greater(self._matched_threshold, matched_vals))\n\n        if self._negatives_lower_than_unmatched:\n          matches = self._set_values_using_indicator(matches,\n                                                     below_unmatched_threshold,\n                                                     -1)\n          matches = self._set_values_using_indicator(matches,\n                                                     between_thresholds,\n                                                     -2)\n        else:\n          matches = self._set_values_using_indicator(matches,\n                                                     below_unmatched_threshold,\n                                                     -2)\n          matches = self._set_values_using_indicator(matches,\n                                                     between_thresholds,\n                                                     -1)\n\n      if self._force_match_for_each_row:\n        similarity_matrix_shape = shape_utils.combined_static_and_dynamic_shape(\n            similarity_matrix)\n        force_match_column_ids = tf.argmax(similarity_matrix, 1,\n                                           output_type=tf.int32)\n        force_match_column_indicators = tf.one_hot(\n            force_match_column_ids, depth=similarity_matrix_shape[1])\n        force_match_row_ids = tf.argmax(force_match_column_indicators, 0,\n                                        output_type=tf.int32)\n        force_match_column_mask = tf.cast(\n            tf.reduce_max(force_match_column_indicators, 0), tf.bool)\n        final_matches = tf.where(force_match_column_mask,\n                                 force_match_row_ids, matches)\n        return final_matches\n      else:\n        return matches\n\n    if similarity_matrix.shape.is_fully_defined():\n      if similarity_matrix.shape[0].value == 0:\n        return _match_when_rows_are_empty()\n      else:\n        return _match_when_rows_are_non_empty()\n    else:\n      return tf.cond(\n          tf.greater(tf.shape(similarity_matrix)[0], 0),\n          _match_when_rows_are_non_empty, _match_when_rows_are_empty)\n\n  def _set_values_using_indicator(self, x, indicator, val):\n    """"""Set the indicated fields of x to val.\n\n    Args:\n      x: tensor.\n      indicator: boolean with same shape as x.\n      val: scalar with value to set.\n\n    Returns:\n      modified tensor.\n    """"""\n    indicator = tf.cast(indicator, x.dtype)\n    return tf.add(tf.multiply(x, 1 - indicator), val * indicator)\n'"
src/object_detection/matchers/argmax_matcher_test.py,1,"b'# Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n""""""Tests for object_detection.matchers.argmax_matcher.""""""\n\nimport numpy as np\nimport tensorflow as tf\n\nfrom object_detection.matchers import argmax_matcher\nfrom object_detection.utils import test_case\n\n\nclass ArgMaxMatcherTest(test_case.TestCase):\n\n  def test_return_correct_matches_with_default_thresholds(self):\n\n    def graph_fn(similarity_matrix):\n      matcher = argmax_matcher.ArgMaxMatcher(matched_threshold=None)\n      match = matcher.match(similarity_matrix)\n      matched_cols = match.matched_column_indicator()\n      unmatched_cols = match.unmatched_column_indicator()\n      match_results = match.match_results\n      return (matched_cols, unmatched_cols, match_results)\n\n    similarity = np.array([[1., 1, 1, 3, 1],\n                           [2, -1, 2, 0, 4],\n                           [3, 0, -1, 0, 0]], dtype=np.float32)\n    expected_matched_rows = np.array([2, 0, 1, 0, 1])\n    (res_matched_cols, res_unmatched_cols,\n     res_match_results) = self.execute(graph_fn, [similarity])\n\n    self.assertAllEqual(res_match_results[res_matched_cols],\n                        expected_matched_rows)\n    self.assertAllEqual(np.nonzero(res_matched_cols)[0], [0, 1, 2, 3, 4])\n    self.assertFalse(np.all(res_unmatched_cols))\n\n  def test_return_correct_matches_with_empty_rows(self):\n\n    def graph_fn(similarity_matrix):\n      matcher = argmax_matcher.ArgMaxMatcher(matched_threshold=None)\n      match = matcher.match(similarity_matrix)\n      return match.unmatched_column_indicator()\n    similarity = 0.2 * np.ones([0, 5], dtype=np.float32)\n    res_unmatched_cols = self.execute(graph_fn, [similarity])\n    self.assertAllEqual(np.nonzero(res_unmatched_cols)[0], np.arange(5))\n\n  def test_return_correct_matches_with_matched_threshold(self):\n\n    def graph_fn(similarity):\n      matcher = argmax_matcher.ArgMaxMatcher(matched_threshold=3.)\n      match = matcher.match(similarity)\n      matched_cols = match.matched_column_indicator()\n      unmatched_cols = match.unmatched_column_indicator()\n      match_results = match.match_results\n      return (matched_cols, unmatched_cols, match_results)\n\n    similarity = np.array([[1, 1, 1, 3, 1],\n                           [2, -1, 2, 0, 4],\n                           [3, 0, -1, 0, 0]], dtype=np.float32)\n    expected_matched_cols = np.array([0, 3, 4])\n    expected_matched_rows = np.array([2, 0, 1])\n    expected_unmatched_cols = np.array([1, 2])\n\n    (res_matched_cols, res_unmatched_cols,\n     match_results) = self.execute(graph_fn, [similarity])\n    self.assertAllEqual(match_results[res_matched_cols], expected_matched_rows)\n    self.assertAllEqual(np.nonzero(res_matched_cols)[0], expected_matched_cols)\n    self.assertAllEqual(np.nonzero(res_unmatched_cols)[0],\n                        expected_unmatched_cols)\n\n  def test_return_correct_matches_with_matched_and_unmatched_threshold(self):\n\n    def graph_fn(similarity):\n      matcher = argmax_matcher.ArgMaxMatcher(matched_threshold=3.,\n                                             unmatched_threshold=2.)\n      match = matcher.match(similarity)\n      matched_cols = match.matched_column_indicator()\n      unmatched_cols = match.unmatched_column_indicator()\n      match_results = match.match_results\n      return (matched_cols, unmatched_cols, match_results)\n\n    similarity = np.array([[1, 1, 1, 3, 1],\n                           [2, -1, 2, 0, 4],\n                           [3, 0, -1, 0, 0]], dtype=np.float32)\n    expected_matched_cols = np.array([0, 3, 4])\n    expected_matched_rows = np.array([2, 0, 1])\n    expected_unmatched_cols = np.array([1])  # col 2 has too high maximum val\n\n    (res_matched_cols, res_unmatched_cols,\n     match_results) = self.execute(graph_fn, [similarity])\n    self.assertAllEqual(match_results[res_matched_cols], expected_matched_rows)\n    self.assertAllEqual(np.nonzero(res_matched_cols)[0], expected_matched_cols)\n    self.assertAllEqual(np.nonzero(res_unmatched_cols)[0],\n                        expected_unmatched_cols)\n\n  def test_return_correct_matches_negatives_lower_than_unmatched_false(self):\n\n    def graph_fn(similarity):\n      matcher = argmax_matcher.ArgMaxMatcher(\n          matched_threshold=3.,\n          unmatched_threshold=2.,\n          negatives_lower_than_unmatched=False)\n      match = matcher.match(similarity)\n      matched_cols = match.matched_column_indicator()\n      unmatched_cols = match.unmatched_column_indicator()\n      match_results = match.match_results\n      return (matched_cols, unmatched_cols, match_results)\n\n    similarity = np.array([[1, 1, 1, 3, 1],\n                           [2, -1, 2, 0, 4],\n                           [3, 0, -1, 0, 0]], dtype=np.float32)\n    expected_matched_cols = np.array([0, 3, 4])\n    expected_matched_rows = np.array([2, 0, 1])\n    expected_unmatched_cols = np.array([2])  # col 1 has too low maximum val\n\n    (res_matched_cols, res_unmatched_cols,\n     match_results) = self.execute(graph_fn, [similarity])\n    self.assertAllEqual(match_results[res_matched_cols], expected_matched_rows)\n    self.assertAllEqual(np.nonzero(res_matched_cols)[0], expected_matched_cols)\n    self.assertAllEqual(np.nonzero(res_unmatched_cols)[0],\n                        expected_unmatched_cols)\n\n  def test_return_correct_matches_unmatched_row_not_using_force_match(self):\n\n    def graph_fn(similarity):\n      matcher = argmax_matcher.ArgMaxMatcher(matched_threshold=3.,\n                                             unmatched_threshold=2.)\n      match = matcher.match(similarity)\n      matched_cols = match.matched_column_indicator()\n      unmatched_cols = match.unmatched_column_indicator()\n      match_results = match.match_results\n      return (matched_cols, unmatched_cols, match_results)\n\n    similarity = np.array([[1, 1, 1, 3, 1],\n                           [-1, 0, -2, -2, -1],\n                           [3, 0, -1, 2, 0]], dtype=np.float32)\n    expected_matched_cols = np.array([0, 3])\n    expected_matched_rows = np.array([2, 0])\n    expected_unmatched_cols = np.array([1, 2, 4])\n\n    (res_matched_cols, res_unmatched_cols,\n     match_results) = self.execute(graph_fn, [similarity])\n    self.assertAllEqual(match_results[res_matched_cols], expected_matched_rows)\n    self.assertAllEqual(np.nonzero(res_matched_cols)[0], expected_matched_cols)\n    self.assertAllEqual(np.nonzero(res_unmatched_cols)[0],\n                        expected_unmatched_cols)\n\n  def test_return_correct_matches_unmatched_row_while_using_force_match(self):\n    def graph_fn(similarity):\n      matcher = argmax_matcher.ArgMaxMatcher(matched_threshold=3.,\n                                             unmatched_threshold=2.,\n                                             force_match_for_each_row=True)\n      match = matcher.match(similarity)\n      matched_cols = match.matched_column_indicator()\n      unmatched_cols = match.unmatched_column_indicator()\n      match_results = match.match_results\n      return (matched_cols, unmatched_cols, match_results)\n\n    similarity = np.array([[1, 1, 1, 3, 1],\n                           [-1, 0, -2, -2, -1],\n                           [3, 0, -1, 2, 0]], dtype=np.float32)\n    expected_matched_cols = np.array([0, 1, 3])\n    expected_matched_rows = np.array([2, 1, 0])\n    expected_unmatched_cols = np.array([2, 4])  # col 2 has too high max val\n\n    (res_matched_cols, res_unmatched_cols,\n     match_results) = self.execute(graph_fn, [similarity])\n    self.assertAllEqual(match_results[res_matched_cols], expected_matched_rows)\n    self.assertAllEqual(np.nonzero(res_matched_cols)[0], expected_matched_cols)\n    self.assertAllEqual(np.nonzero(res_unmatched_cols)[0],\n                        expected_unmatched_cols)\n\n  def test_valid_arguments_corner_case(self):\n    argmax_matcher.ArgMaxMatcher(matched_threshold=1,\n                                 unmatched_threshold=1)\n\n  def test_invalid_arguments_corner_case_negatives_lower_than_thres_false(self):\n    with self.assertRaises(ValueError):\n      argmax_matcher.ArgMaxMatcher(matched_threshold=1,\n                                   unmatched_threshold=1,\n                                   negatives_lower_than_unmatched=False)\n\n  def test_invalid_arguments_no_matched_threshold(self):\n    with self.assertRaises(ValueError):\n      argmax_matcher.ArgMaxMatcher(matched_threshold=None,\n                                   unmatched_threshold=4)\n\n  def test_invalid_arguments_unmatched_thres_larger_than_matched_thres(self):\n    with self.assertRaises(ValueError):\n      argmax_matcher.ArgMaxMatcher(matched_threshold=1,\n                                   unmatched_threshold=2)\n\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
src/object_detection/matchers/bipartite_matcher.py,4,"b'# Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n""""""Bipartite matcher implementation.""""""\n\nimport tensorflow as tf\n\nfrom tensorflow.contrib.image.python.ops import image_ops\nfrom object_detection.core import matcher\n\n\nclass GreedyBipartiteMatcher(matcher.Matcher):\n  """"""Wraps a Tensorflow greedy bipartite matcher.""""""\n\n  def __init__(self, use_matmul_gather=False):\n    """"""Constructs a Matcher.\n\n    Args:\n      use_matmul_gather: Force constructed match objects to use matrix\n        multiplication based gather instead of standard tf.gather.\n        (Default: False).\n    """"""\n    super(GreedyBipartiteMatcher, self).__init__(\n        use_matmul_gather=use_matmul_gather)\n\n  def _match(self, similarity_matrix, num_valid_rows=-1):\n    """"""Bipartite matches a collection rows and columns. A greedy bi-partite.\n\n    TODO(rathodv): Add num_valid_columns options to match only that many columns\n    with all the rows.\n\n    Args:\n      similarity_matrix: Float tensor of shape [N, M] with pairwise similarity\n        where higher values mean more similar.\n      num_valid_rows: A scalar or a 1-D tensor with one element describing the\n        number of valid rows of similarity_matrix to consider for the bipartite\n        matching. If set to be negative, then all rows from similarity_matrix\n        are used.\n\n    Returns:\n      match_results: int32 tensor of shape [M] with match_results[i]=-1\n        meaning that column i is not matched and otherwise that it is matched to\n        row match_results[i].\n    """"""\n    # Convert similarity matrix to distance matrix as tf.image.bipartite tries\n    # to find minimum distance matches.\n    distance_matrix = -1 * similarity_matrix\n    _, match_results = image_ops.bipartite_match(\n        distance_matrix, num_valid_rows)\n    match_results = tf.reshape(match_results, [-1])\n    match_results = tf.cast(match_results, tf.int32)\n    return match_results\n'"
src/object_detection/matchers/bipartite_matcher_test.py,6,"b'# Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n""""""Tests for object_detection.core.bipartite_matcher.""""""\n\nimport tensorflow as tf\n\nfrom object_detection.matchers import bipartite_matcher\n\n\nclass GreedyBipartiteMatcherTest(tf.test.TestCase):\n\n  def test_get_expected_matches_when_all_rows_are_valid(self):\n    similarity_matrix = tf.constant([[0.50, 0.1, 0.8], [0.15, 0.2, 0.3]])\n    num_valid_rows = 2\n    expected_match_results = [-1, 1, 0]\n\n    matcher = bipartite_matcher.GreedyBipartiteMatcher()\n    match = matcher.match(similarity_matrix, num_valid_rows=num_valid_rows)\n    with self.test_session() as sess:\n      match_results_out = sess.run(match._match_results)\n      self.assertAllEqual(match_results_out, expected_match_results)\n\n  def test_get_expected_matches_with_valid_rows_set_to_minus_one(self):\n    similarity_matrix = tf.constant([[0.50, 0.1, 0.8], [0.15, 0.2, 0.3]])\n    num_valid_rows = -1\n    expected_match_results = [-1, 1, 0]\n\n    matcher = bipartite_matcher.GreedyBipartiteMatcher()\n    match = matcher.match(similarity_matrix, num_valid_rows=num_valid_rows)\n    with self.test_session() as sess:\n      match_results_out = sess.run(match._match_results)\n      self.assertAllEqual(match_results_out, expected_match_results)\n\n  def test_get_no_matches_with_zero_valid_rows(self):\n    similarity_matrix = tf.constant([[0.50, 0.1, 0.8], [0.15, 0.2, 0.3]])\n    num_valid_rows = 0\n    expected_match_results = [-1, -1, -1]\n\n    matcher = bipartite_matcher.GreedyBipartiteMatcher()\n    match = matcher.match(similarity_matrix, num_valid_rows=num_valid_rows)\n    with self.test_session() as sess:\n      match_results_out = sess.run(match._match_results)\n      self.assertAllEqual(match_results_out, expected_match_results)\n\n  def test_get_expected_matches_with_only_one_valid_row(self):\n    similarity_matrix = tf.constant([[0.50, 0.1, 0.8], [0.15, 0.2, 0.3]])\n    num_valid_rows = 1\n    expected_match_results = [-1, -1, 0]\n\n    matcher = bipartite_matcher.GreedyBipartiteMatcher()\n    match = matcher.match(similarity_matrix, num_valid_rows=num_valid_rows)\n    with self.test_session() as sess:\n      match_results_out = sess.run(match._match_results)\n      self.assertAllEqual(match_results_out, expected_match_results)\n\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
src/object_detection/meta_architectures/__init__.py,0,b''
src/object_detection/meta_architectures/faster_rcnn_meta_arch.py,166,"b'# Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n""""""Faster R-CNN meta-architecture definition.\n\nGeneral tensorflow implementation of Faster R-CNN detection models.\n\nSee Faster R-CNN: Ren, Shaoqing, et al.\n""Faster R-CNN: Towards real-time object detection with region proposal\nnetworks."" Advances in neural information processing systems. 2015.\n\nWe allow for three modes: number_of_stages={1, 2, 3}. In case of 1 stage,\nall of the user facing methods (e.g., predict, postprocess, loss) can be used as\nif the model consisted only of the RPN, returning class agnostic proposals\n(these can be thought of as approximate detections with no associated class\ninformation).  In case of 2 stages, proposals are computed, then passed\nthrough a second stage ""box classifier"" to yield (multi-class) detections.\nFinally, in case of 3 stages which is only used during eval, proposals are\ncomputed, then passed through a second stage ""box classifier"" that will compute\nrefined boxes and classes, and then features are pooled from the refined and\nnon-maximum suppressed boxes and are passed through the box classifier again. If\nnumber of stages is 3 during training it will be reduced to two automatically.\n\nImplementations of Faster R-CNN models must define a new\nFasterRCNNFeatureExtractor and override three methods: `preprocess`,\n`_extract_proposal_features` (the first stage of the model), and\n`_extract_box_classifier_features` (the second stage of the model). Optionally,\nthe `restore_fn` method can be overridden.  See tests for an example.\n\nA few important notes:\n+ Batching conventions:  We support batched inference and training where\nall images within a batch have the same resolution.  Batch sizes are determined\ndynamically via the shape of the input tensors (rather than being specified\ndirectly as, e.g., a model constructor).\n\nA complication is that due to non-max suppression, we are not guaranteed to get\nthe same number of proposals from the first stage RPN (region proposal network)\nfor each image (though in practice, we should often get the same number of\nproposals).  For this reason we pad to a max number of proposals per image\nwithin a batch. This `self.max_num_proposals` property is set to the\n`first_stage_max_proposals` parameter at inference time and the\n`second_stage_batch_size` at training time since we subsample the batch to\nbe sent through the box classifier during training.\n\nFor the second stage of the pipeline, we arrange the proposals for all images\nwithin the batch along a single batch dimension.  For example, the input to\n_extract_box_classifier_features is a tensor of shape\n`[total_num_proposals, crop_height, crop_width, depth]` where\ntotal_num_proposals is batch_size * self.max_num_proposals.  (And note that per\nthe above comment, a subset of these entries correspond to zero paddings.)\n\n+ Coordinate representations:\nFollowing the API (see model.DetectionModel definition), our outputs after\npostprocessing operations are always normalized boxes however, internally, we\nsometimes convert to absolute --- e.g. for loss computation.  In particular,\nanchors and proposal_boxes are both represented as absolute coordinates.\n\nImages are resized in the `preprocess` method.\n\nThe Faster R-CNN meta architecture has two post-processing methods\n`_postprocess_rpn` which is applied after first stage and\n`_postprocess_box_classifier` which is applied after second stage. There are\nthree different ways post-processing can happen depending on number_of_stages\nconfigured in the meta architecture:\n\n1. When number_of_stages is 1:\n  `_postprocess_rpn` is run as part of the `postprocess` method where\n  true_image_shapes is used to clip proposals, perform non-max suppression and\n  normalize them.\n2. When number of stages is 2:\n  `_postprocess_rpn` is run as part of the `_predict_second_stage` method where\n  `resized_image_shapes` is used to clip proposals, perform non-max suppression\n  and normalize them. In this case `postprocess` method skips `_postprocess_rpn`\n  and only runs `_postprocess_box_classifier` using `true_image_shapes` to clip\n  detections, perform non-max suppression and normalize them.\n3. When number of stages is 3:\n  `_postprocess_rpn` is run as part of the `_predict_second_stage` using\n  `resized_image_shapes` to clip proposals, perform non-max suppression and\n  normalize them. Subsequently, `_postprocess_box_classifier` is run as part of\n  `_predict_third_stage` using `true_image_shapes` to clip detections, peform\n  non-max suppression and normalize them. In this case, the `postprocess` method\n  skips both `_postprocess_rpn` and `_postprocess_box_classifier`.\n""""""\nfrom abc import abstractmethod\nfrom functools import partial\nimport tensorflow as tf\n\nfrom object_detection.anchor_generators import grid_anchor_generator\nfrom object_detection.core import balanced_positive_negative_sampler as sampler\nfrom object_detection.core import box_list\nfrom object_detection.core import box_list_ops\nfrom object_detection.core import box_predictor\nfrom object_detection.core import losses\nfrom object_detection.core import model\nfrom object_detection.core import post_processing\nfrom object_detection.core import standard_fields as fields\nfrom object_detection.core import target_assigner\nfrom object_detection.utils import ops\nfrom object_detection.utils import shape_utils\n\nslim = tf.contrib.slim\n\n\nclass FasterRCNNFeatureExtractor(object):\n  """"""Faster R-CNN Feature Extractor definition.""""""\n\n  def __init__(self,\n               is_training,\n               first_stage_features_stride,\n               batch_norm_trainable=False,\n               reuse_weights=None,\n               weight_decay=0.0):\n    """"""Constructor.\n\n    Args:\n      is_training: A boolean indicating whether the training version of the\n        computation graph should be constructed.\n      first_stage_features_stride: Output stride of extracted RPN feature map.\n      batch_norm_trainable: Whether to update batch norm parameters during\n        training or not. When training with a relative large batch size\n        (e.g. 8), it could be desirable to enable batch norm update.\n      reuse_weights: Whether to reuse variables. Default is None.\n      weight_decay: float weight decay for feature extractor (default: 0.0).\n    """"""\n    self._is_training = is_training\n    self._first_stage_features_stride = first_stage_features_stride\n    self._train_batch_norm = (batch_norm_trainable and is_training)\n    self._reuse_weights = reuse_weights\n    self._weight_decay = weight_decay\n\n  @abstractmethod\n  def preprocess(self, resized_inputs):\n    """"""Feature-extractor specific preprocessing (minus image resizing).""""""\n    pass\n\n  def extract_proposal_features(self, preprocessed_inputs, scope):\n    """"""Extracts first stage RPN features.\n\n    This function is responsible for extracting feature maps from preprocessed\n    images.  These features are used by the region proposal network (RPN) to\n    predict proposals.\n\n    Args:\n      preprocessed_inputs: A [batch, height, width, channels] float tensor\n        representing a batch of images.\n      scope: A scope name.\n\n    Returns:\n      rpn_feature_map: A tensor with shape [batch, height, width, depth]\n      activations: A dictionary mapping activation tensor names to tensors.\n    """"""\n    with tf.variable_scope(scope, values=[preprocessed_inputs]):\n      return self._extract_proposal_features(preprocessed_inputs, scope)\n\n  @abstractmethod\n  def _extract_proposal_features(self, preprocessed_inputs, scope):\n    """"""Extracts first stage RPN features, to be overridden.""""""\n    pass\n\n  def extract_box_classifier_features(self, proposal_feature_maps, scope):\n    """"""Extracts second stage box classifier features.\n\n    Args:\n      proposal_feature_maps: A 4-D float tensor with shape\n        [batch_size * self.max_num_proposals, crop_height, crop_width, depth]\n        representing the feature map cropped to each proposal.\n      scope: A scope name.\n\n    Returns:\n      proposal_classifier_features: A 4-D float tensor with shape\n        [batch_size * self.max_num_proposals, height, width, depth]\n        representing box classifier features for each proposal.\n    """"""\n    with tf.variable_scope(\n        scope, values=[proposal_feature_maps], reuse=tf.AUTO_REUSE):\n      return self._extract_box_classifier_features(proposal_feature_maps, scope)\n\n  @abstractmethod\n  def _extract_box_classifier_features(self, proposal_feature_maps, scope):\n    """"""Extracts second stage box classifier features, to be overridden.""""""\n    pass\n\n  def restore_from_classification_checkpoint_fn(\n      self,\n      first_stage_feature_extractor_scope,\n      second_stage_feature_extractor_scope):\n    """"""Returns a map of variables to load from a foreign checkpoint.\n\n    Args:\n      first_stage_feature_extractor_scope: A scope name for the first stage\n        feature extractor.\n      second_stage_feature_extractor_scope: A scope name for the second stage\n        feature extractor.\n\n    Returns:\n      A dict mapping variable names (to load from a checkpoint) to variables in\n      the model graph.\n    """"""\n    variables_to_restore = {}\n    for variable in tf.global_variables():\n      for scope_name in [first_stage_feature_extractor_scope,\n                         second_stage_feature_extractor_scope]:\n        if variable.op.name.startswith(scope_name):\n          var_name = variable.op.name.replace(scope_name + \'/\', \'\')\n          variables_to_restore[var_name] = variable\n    return variables_to_restore\n\n\nclass FasterRCNNMetaArch(model.DetectionModel):\n  """"""Faster R-CNN Meta-architecture definition.""""""\n\n  def __init__(self,\n               is_training,\n               num_classes,\n               image_resizer_fn,\n               feature_extractor,\n               number_of_stages,\n               first_stage_anchor_generator,\n               first_stage_atrous_rate,\n               first_stage_box_predictor_arg_scope,\n               first_stage_box_predictor_kernel_size,\n               first_stage_box_predictor_depth,\n               first_stage_minibatch_size,\n               first_stage_positive_balance_fraction,\n               first_stage_nms_score_threshold,\n               first_stage_nms_iou_threshold,\n               first_stage_max_proposals,\n               first_stage_localization_loss_weight,\n               first_stage_objectness_loss_weight,\n               initial_crop_size,\n               maxpool_kernel_size,\n               maxpool_stride,\n               second_stage_mask_rcnn_box_predictor,\n               second_stage_batch_size,\n               second_stage_balance_fraction,\n               second_stage_non_max_suppression_fn,\n               second_stage_score_conversion_fn,\n               second_stage_localization_loss_weight,\n               second_stage_classification_loss_weight,\n               second_stage_classification_loss,\n               second_stage_mask_prediction_loss_weight=1.0,\n               hard_example_miner=None,\n               parallel_iterations=16,\n               add_summaries=True):\n    """"""FasterRCNNMetaArch Constructor.\n\n    Args:\n      is_training: A boolean indicating whether the training version of the\n        computation graph should be constructed.\n      num_classes: Number of classes.  Note that num_classes *does not*\n        include the background category, so if groundtruth labels take values\n        in {0, 1, .., K-1}, num_classes=K (and not K+1, even though the\n        assigned classification targets can range from {0,... K}).\n      image_resizer_fn: A callable for image resizing.  This callable\n        takes a rank-3 image tensor of shape [height, width, channels]\n        (corresponding to a single image), an optional rank-3 instance mask\n        tensor of shape [num_masks, height, width] and returns a resized rank-3\n        image tensor, a resized mask tensor if one was provided in the input. In\n        addition this callable must also return a 1-D tensor of the form\n        [height, width, channels] containing the size of the true image, as the\n        image resizer can perform zero padding. See protos/image_resizer.proto.\n      feature_extractor: A FasterRCNNFeatureExtractor object.\n      number_of_stages:  An integer values taking values in {1, 2, 3}. If\n        1, the function will construct only the Region Proposal Network (RPN)\n        part of the model. If 2, the function will perform box refinement and\n        other auxiliary predictions all in the second stage. If 3, it will\n        extract features from refined boxes and perform the auxiliary\n        predictions on the non-maximum suppressed refined boxes.\n        If is_training is true and the value of number_of_stages is 3, it is\n        reduced to 2 since all the model heads are trained in parallel in second\n        stage during training.\n      first_stage_anchor_generator: An anchor_generator.AnchorGenerator object\n        (note that currently we only support\n        grid_anchor_generator.GridAnchorGenerator objects)\n      first_stage_atrous_rate: A single integer indicating the atrous rate for\n        the single convolution op which is applied to the `rpn_features_to_crop`\n        tensor to obtain a tensor to be used for box prediction. Some feature\n        extractors optionally allow for producing feature maps computed at\n        denser resolutions.  The atrous rate is used to compensate for the\n        denser feature maps by using an effectively larger receptive field.\n        (This should typically be set to 1).\n      first_stage_box_predictor_arg_scope: Slim arg_scope for conv2d,\n        separable_conv2d and fully_connected ops for the RPN box predictor.\n      first_stage_box_predictor_kernel_size: Kernel size to use for the\n        convolution op just prior to RPN box predictions.\n      first_stage_box_predictor_depth: Output depth for the convolution op\n        just prior to RPN box predictions.\n      first_stage_minibatch_size: The ""batch size"" to use for computing the\n        objectness and location loss of the region proposal network. This\n        ""batch size"" refers to the number of anchors selected as contributing\n        to the loss function for any given image within the image batch and is\n        only called ""batch_size"" due to terminology from the Faster R-CNN paper.\n      first_stage_positive_balance_fraction: Fraction of positive examples\n        per image for the RPN. The recommended value for Faster RCNN is 0.5.\n      first_stage_nms_score_threshold: Score threshold for non max suppression\n        for the Region Proposal Network (RPN).  This value is expected to be in\n        [0, 1] as it is applied directly after a softmax transformation.  The\n        recommended value for Faster R-CNN is 0.\n      first_stage_nms_iou_threshold: The Intersection Over Union (IOU) threshold\n        for performing Non-Max Suppression (NMS) on the boxes predicted by the\n        Region Proposal Network (RPN).\n      first_stage_max_proposals: Maximum number of boxes to retain after\n        performing Non-Max Suppression (NMS) on the boxes predicted by the\n        Region Proposal Network (RPN).\n      first_stage_localization_loss_weight: A float\n      first_stage_objectness_loss_weight: A float\n      initial_crop_size: A single integer indicating the output size\n        (width and height are set to be the same) of the initial bilinear\n        interpolation based cropping during ROI pooling.\n      maxpool_kernel_size: A single integer indicating the kernel size of the\n        max pool op on the cropped feature map during ROI pooling.\n      maxpool_stride: A single integer indicating the stride of the max pool\n        op on the cropped feature map during ROI pooling.\n      second_stage_mask_rcnn_box_predictor: Mask R-CNN box predictor to use for\n        the second stage.\n      second_stage_batch_size: The batch size used for computing the\n        classification and refined location loss of the box classifier.  This\n        ""batch size"" refers to the number of proposals selected as contributing\n        to the loss function for any given image within the image batch and is\n        only called ""batch_size"" due to terminology from the Faster R-CNN paper.\n      second_stage_balance_fraction: Fraction of positive examples to use\n        per image for the box classifier. The recommended value for Faster RCNN\n        is 0.25.\n      second_stage_non_max_suppression_fn: batch_multiclass_non_max_suppression\n        callable that takes `boxes`, `scores`, optional `clip_window` and\n        optional (kwarg) `mask` inputs (with all other inputs already set)\n        and returns a dictionary containing tensors with keys:\n        `detection_boxes`, `detection_scores`, `detection_classes`,\n        `num_detections`, and (optionally) `detection_masks`. See\n        `post_processing.batch_multiclass_non_max_suppression` for the type and\n        shape of these tensors.\n      second_stage_score_conversion_fn: Callable elementwise nonlinearity\n        (that takes tensors as inputs and returns tensors).  This is usually\n        used to convert logits to probabilities.\n      second_stage_localization_loss_weight: A float indicating the scale factor\n        for second stage localization loss.\n      second_stage_classification_loss_weight: A float indicating the scale\n        factor for second stage classification loss.\n      second_stage_classification_loss: Classification loss used by the second\n        stage classifier. Either losses.WeightedSigmoidClassificationLoss or\n        losses.WeightedSoftmaxClassificationLoss.\n      second_stage_mask_prediction_loss_weight: A float indicating the scale\n        factor for second stage mask prediction loss. This is applicable only if\n        second stage box predictor is configured to predict masks.\n      hard_example_miner:  A losses.HardExampleMiner object (can be None).\n      parallel_iterations: (Optional) The number of iterations allowed to run\n        in parallel for calls to tf.map_fn.\n      add_summaries: boolean (default: True) controlling whether summary ops\n        should be added to tensorflow graph.\n\n    Raises:\n      ValueError: If `second_stage_batch_size` > `first_stage_max_proposals` at\n        training time.\n      ValueError: If first_stage_anchor_generator is not of type\n        grid_anchor_generator.GridAnchorGenerator.\n    """"""\n    # TODO(rathodv): add_summaries is currently unused. Respect that directive\n    # in the future.\n    super(FasterRCNNMetaArch, self).__init__(num_classes=num_classes)\n\n    if is_training and second_stage_batch_size > first_stage_max_proposals:\n      raise ValueError(\'second_stage_batch_size should be no greater than \'\n                       \'first_stage_max_proposals.\')\n    if not isinstance(first_stage_anchor_generator,\n                      grid_anchor_generator.GridAnchorGenerator):\n      raise ValueError(\'first_stage_anchor_generator must be of type \'\n                       \'grid_anchor_generator.GridAnchorGenerator.\')\n\n    self._is_training = is_training\n    self._image_resizer_fn = image_resizer_fn\n    self._feature_extractor = feature_extractor\n    self._number_of_stages = number_of_stages\n\n    # The first class is reserved as background.\n    unmatched_cls_target = tf.constant(\n        [1] + self._num_classes * [0], dtype=tf.float32)\n    self._proposal_target_assigner = target_assigner.create_target_assigner(\n        \'FasterRCNN\', \'proposal\')\n    self._detector_target_assigner = target_assigner.create_target_assigner(\n        \'FasterRCNN\', \'detection\', unmatched_cls_target=unmatched_cls_target)\n    # Both proposal and detector target assigners use the same box coder\n    self._box_coder = self._proposal_target_assigner.box_coder\n\n    # (First stage) Region proposal network parameters\n    self._first_stage_anchor_generator = first_stage_anchor_generator\n    self._first_stage_atrous_rate = first_stage_atrous_rate\n    self._first_stage_box_predictor_arg_scope = (\n        first_stage_box_predictor_arg_scope)\n    self._first_stage_box_predictor_kernel_size = (\n        first_stage_box_predictor_kernel_size)\n    self._first_stage_box_predictor_depth = first_stage_box_predictor_depth\n    self._first_stage_minibatch_size = first_stage_minibatch_size\n    self._first_stage_sampler = sampler.BalancedPositiveNegativeSampler(\n        positive_fraction=first_stage_positive_balance_fraction)\n    self._first_stage_box_predictor = box_predictor.ConvolutionalBoxPredictor(\n        self._is_training, num_classes=1,\n        conv_hyperparams=self._first_stage_box_predictor_arg_scope,\n        min_depth=0, max_depth=0, num_layers_before_predictor=0,\n        use_dropout=False, dropout_keep_prob=1.0, kernel_size=1,\n        box_code_size=self._box_coder.code_size)\n\n    self._first_stage_nms_score_threshold = first_stage_nms_score_threshold\n    self._first_stage_nms_iou_threshold = first_stage_nms_iou_threshold\n    self._first_stage_max_proposals = first_stage_max_proposals\n\n    self._first_stage_localization_loss = (\n        losses.WeightedSmoothL1LocalizationLoss())\n    self._first_stage_objectness_loss = (\n        losses.WeightedSoftmaxClassificationLoss())\n    self._first_stage_loc_loss_weight = first_stage_localization_loss_weight\n    self._first_stage_obj_loss_weight = first_stage_objectness_loss_weight\n\n    # Per-region cropping parameters\n    self._initial_crop_size = initial_crop_size\n    self._maxpool_kernel_size = maxpool_kernel_size\n    self._maxpool_stride = maxpool_stride\n\n    self._mask_rcnn_box_predictor = second_stage_mask_rcnn_box_predictor\n\n    self._second_stage_batch_size = second_stage_batch_size\n    self._second_stage_sampler = sampler.BalancedPositiveNegativeSampler(\n        positive_fraction=second_stage_balance_fraction)\n\n    self._second_stage_nms_fn = second_stage_non_max_suppression_fn\n    self._second_stage_score_conversion_fn = second_stage_score_conversion_fn\n\n    self._second_stage_localization_loss = (\n        losses.WeightedSmoothL1LocalizationLoss())\n    self._second_stage_classification_loss = second_stage_classification_loss\n    self._second_stage_mask_loss = (\n        losses.WeightedSigmoidClassificationLoss())\n    self._second_stage_loc_loss_weight = second_stage_localization_loss_weight\n    self._second_stage_cls_loss_weight = second_stage_classification_loss_weight\n    self._second_stage_mask_loss_weight = (\n        second_stage_mask_prediction_loss_weight)\n    self._hard_example_miner = hard_example_miner\n    self._parallel_iterations = parallel_iterations\n\n    if self._number_of_stages <= 0 or self._number_of_stages > 3:\n      raise ValueError(\'Number of stages should be a value in {1, 2, 3}.\')\n    if self._is_training and self._number_of_stages == 3:\n      self._number_of_stages = 2\n\n  @property\n  def first_stage_feature_extractor_scope(self):\n    return \'FirstStageFeatureExtractor\'\n\n  @property\n  def second_stage_feature_extractor_scope(self):\n    return \'SecondStageFeatureExtractor\'\n\n  @property\n  def first_stage_box_predictor_scope(self):\n    return \'FirstStageBoxPredictor\'\n\n  @property\n  def second_stage_box_predictor_scope(self):\n    return \'SecondStageBoxPredictor\'\n\n  @property\n  def max_num_proposals(self):\n    """"""Max number of proposals (to pad to) for each image in the input batch.\n\n    At training time, this is set to be the `second_stage_batch_size` if hard\n    example miner is not configured, else it is set to\n    `first_stage_max_proposals`. At inference time, this is always set to\n    `first_stage_max_proposals`.\n\n    Returns:\n      A positive integer.\n    """"""\n    if self._is_training and not self._hard_example_miner:\n      return self._second_stage_batch_size\n    return self._first_stage_max_proposals\n\n  @property\n  def anchors(self):\n    if not self._anchors:\n      raise RuntimeError(\'anchors have not been constructed yet!\')\n    if not isinstance(self._anchors, box_list.BoxList):\n      raise RuntimeError(\'anchors should be a BoxList object, but is not.\')\n    return self._anchors\n\n  def preprocess(self, inputs):\n    """"""Feature-extractor specific preprocessing.\n\n    See base class.\n\n    For Faster R-CNN, we perform image resizing in the base class --- each\n    class subclassing FasterRCNNMetaArch is responsible for any additional\n    preprocessing (e.g., scaling pixel values to be in [-1, 1]).\n\n    Args:\n      inputs: a [batch, height_in, width_in, channels] float tensor representing\n        a batch of images with values between 0 and 255.0.\n\n    Returns:\n      preprocessed_inputs: a [batch, height_out, width_out, channels] float\n        tensor representing a batch of images.\n      true_image_shapes: int32 tensor of shape [batch, 3] where each row is\n        of the form [height, width, channels] indicating the shapes\n        of true images in the resized images, as resized images can be padded\n        with zeros.\n    Raises:\n      ValueError: if inputs tensor does not have type tf.float32\n    """"""\n    if inputs.dtype is not tf.float32:\n      raise ValueError(\'`preprocess` expects a tf.float32 tensor\')\n    with tf.name_scope(\'Preprocessor\'):\n      outputs = shape_utils.static_or_dynamic_map_fn(\n          self._image_resizer_fn,\n          elems=inputs,\n          dtype=[tf.float32, tf.int32],\n          parallel_iterations=self._parallel_iterations)\n      resized_inputs = outputs[0]\n      true_image_shapes = outputs[1]\n      return (self._feature_extractor.preprocess(resized_inputs),\n              true_image_shapes)\n\n  def _compute_clip_window(self, image_shapes):\n    """"""Computes clip window for non max suppression based on image shapes.\n\n    This function assumes that the clip window\'s left top corner is at (0, 0).\n\n    Args:\n      image_shapes: A 2-D int32 tensor of shape [batch_size, 3] containing\n      shapes of images in the batch. Each row represents [height, width,\n      channels] of an image.\n\n    Returns:\n      A 2-D float32 tensor of shape [batch_size, 4] containing the clip window\n      for each image in the form [ymin, xmin, ymax, xmax].\n    """"""\n    clip_heights = image_shapes[:, 0]\n    clip_widths = image_shapes[:, 1]\n    clip_window = tf.to_float(tf.stack([tf.zeros_like(clip_heights),\n                                        tf.zeros_like(clip_heights),\n                                        clip_heights, clip_widths], axis=1))\n    return clip_window\n\n  def predict(self, preprocessed_inputs, true_image_shapes):\n    """"""Predicts unpostprocessed tensors from input tensor.\n\n    This function takes an input batch of images and runs it through the\n    forward pass of the network to yield ""raw"" un-postprocessed predictions.\n    If `number_of_stages` is 1, this function only returns first stage\n    RPN predictions (un-postprocessed).  Otherwise it returns both\n    first stage RPN predictions as well as second stage box classifier\n    predictions.\n\n    Other remarks:\n    + Anchor pruning vs. clipping: following the recommendation of the Faster\n    R-CNN paper, we prune anchors that venture outside the image window at\n    training time and clip anchors to the image window at inference time.\n    + Proposal padding: as described at the top of the file, proposals are\n    padded to self._max_num_proposals and flattened so that proposals from all\n    images within the input batch are arranged along the same batch dimension.\n\n    Args:\n      preprocessed_inputs: a [batch, height, width, channels] float tensor\n        representing a batch of images.\n      true_image_shapes: int32 tensor of shape [batch, 3] where each row is\n        of the form [height, width, channels] indicating the shapes\n        of true images in the resized images, as resized images can be padded\n        with zeros.\n\n    Returns:\n      prediction_dict: a dictionary holding ""raw"" prediction tensors:\n        1) rpn_box_predictor_features: A 4-D float32 tensor with shape\n          [batch_size, height, width, depth] to be used for predicting proposal\n          boxes and corresponding objectness scores.\n        2) rpn_features_to_crop: A 4-D float32 tensor with shape\n          [batch_size, height, width, depth] representing image features to crop\n          using the proposal boxes predicted by the RPN.\n        3) image_shape: a 1-D tensor of shape [4] representing the input\n          image shape.\n        4) rpn_box_encodings:  3-D float tensor of shape\n          [batch_size, num_anchors, self._box_coder.code_size] containing\n          predicted boxes.\n        5) rpn_objectness_predictions_with_background: 3-D float tensor of shape\n          [batch_size, num_anchors, 2] containing class\n          predictions (logits) for each of the anchors.  Note that this\n          tensor *includes* background class predictions (at class index 0).\n        6) anchors: A 2-D tensor of shape [num_anchors, 4] representing anchors\n          for the first stage RPN (in absolute coordinates).  Note that\n          `num_anchors` can differ depending on whether the model is created in\n          training or inference mode.\n\n        (and if number_of_stages > 1):\n        7) refined_box_encodings: a 3-D tensor with shape\n          [total_num_proposals, num_classes, 4] representing predicted\n          (final) refined box encodings, where\n          total_num_proposals=batch_size*self._max_num_proposals\n        8) class_predictions_with_background: a 3-D tensor with shape\n          [total_num_proposals, num_classes + 1] containing class\n          predictions (logits) for each of the anchors, where\n          total_num_proposals=batch_size*self._max_num_proposals.\n          Note that this tensor *includes* background class predictions\n          (at class index 0).\n        9) num_proposals: An int32 tensor of shape [batch_size] representing the\n          number of proposals generated by the RPN.  `num_proposals` allows us\n          to keep track of which entries are to be treated as zero paddings and\n          which are not since we always pad the number of proposals to be\n          `self.max_num_proposals` for each image.\n        10) proposal_boxes: A float32 tensor of shape\n          [batch_size, self.max_num_proposals, 4] representing\n          decoded proposal bounding boxes in absolute coordinates.\n        11) mask_predictions: (optional) a 4-D tensor with shape\n          [total_num_padded_proposals, num_classes, mask_height, mask_width]\n          containing instance mask predictions.\n\n    Raises:\n      ValueError: If `predict` is called before `preprocess`.\n    """"""\n    (rpn_box_predictor_features, rpn_features_to_crop, anchors_boxlist,\n     image_shape) = self._extract_rpn_feature_maps(preprocessed_inputs)\n    (rpn_box_encodings, rpn_objectness_predictions_with_background\n    ) = self._predict_rpn_proposals(rpn_box_predictor_features)\n\n    # The Faster R-CNN paper recommends pruning anchors that venture outside\n    # the image window at training time and clipping at inference time.\n    clip_window = tf.to_float(tf.stack([0, 0, image_shape[1], image_shape[2]]))\n    if self._is_training:\n      (rpn_box_encodings, rpn_objectness_predictions_with_background,\n       anchors_boxlist) = self._remove_invalid_anchors_and_predictions(\n           rpn_box_encodings, rpn_objectness_predictions_with_background,\n           anchors_boxlist, clip_window)\n    else:\n      anchors_boxlist = box_list_ops.clip_to_window(\n          anchors_boxlist, clip_window)\n\n    self._anchors = anchors_boxlist\n    prediction_dict = {\n        \'rpn_box_predictor_features\': rpn_box_predictor_features,\n        \'rpn_features_to_crop\': rpn_features_to_crop,\n        \'image_shape\': image_shape,\n        \'rpn_box_encodings\': rpn_box_encodings,\n        \'rpn_objectness_predictions_with_background\':\n        rpn_objectness_predictions_with_background,\n        \'anchors\': self._anchors.get()\n    }\n\n    if self._number_of_stages >= 2:\n      prediction_dict.update(self._predict_second_stage(\n          rpn_box_encodings,\n          rpn_objectness_predictions_with_background,\n          rpn_features_to_crop,\n          self._anchors.get(), image_shape, true_image_shapes))\n\n    if self._number_of_stages == 3:\n      prediction_dict = self._predict_third_stage(\n          prediction_dict, true_image_shapes)\n\n    return prediction_dict\n\n  def _image_batch_shape_2d(self, image_batch_shape_1d):\n    """"""Takes a 1-D image batch shape tensor and converts it to a 2-D tensor.\n\n    Example:\n    If 1-D image batch shape tensor is [2, 300, 300, 3]. The corresponding 2-D\n    image batch tensor would be [[300, 300, 3], [300, 300, 3]]\n\n    Args:\n      image_batch_shape_1d: 1-D tensor of the form [batch_size, height,\n        width, channels].\n\n    Returns:\n      image_batch_shape_2d: 2-D tensor of shape [batch_size, 3] were each row is\n        of the form [height, width, channels].\n    """"""\n    return tf.tile(tf.expand_dims(image_batch_shape_1d[1:], 0),\n                   [image_batch_shape_1d[0], 1])\n\n  def _predict_second_stage(self, rpn_box_encodings,\n                            rpn_objectness_predictions_with_background,\n                            rpn_features_to_crop,\n                            anchors,\n                            image_shape,\n                            true_image_shapes):\n    """"""Predicts the output tensors from second stage of Faster R-CNN.\n\n    Args:\n      rpn_box_encodings: 4-D float tensor of shape\n        [batch_size, num_valid_anchors, self._box_coder.code_size] containing\n        predicted boxes.\n      rpn_objectness_predictions_with_background: 2-D float tensor of shape\n        [batch_size, num_valid_anchors, 2] containing class\n        predictions (logits) for each of the anchors.  Note that this\n        tensor *includes* background class predictions (at class index 0).\n      rpn_features_to_crop: A 4-D float32 tensor with shape\n        [batch_size, height, width, depth] representing image features to crop\n        using the proposal boxes predicted by the RPN.\n      anchors: 2-D float tensor of shape\n        [num_anchors, self._box_coder.code_size].\n      image_shape: A 1D int32 tensors of size [4] containing the image shape.\n      true_image_shapes: int32 tensor of shape [batch, 3] where each row is\n        of the form [height, width, channels] indicating the shapes\n        of true images in the resized images, as resized images can be padded\n        with zeros.\n\n    Returns:\n      prediction_dict: a dictionary holding ""raw"" prediction tensors:\n        1) refined_box_encodings: a 3-D tensor with shape\n          [total_num_proposals, num_classes, 4] representing predicted\n          (final) refined box encodings, where\n          total_num_proposals=batch_size*self._max_num_proposals\n        2) class_predictions_with_background: a 3-D tensor with shape\n          [total_num_proposals, num_classes + 1] containing class\n          predictions (logits) for each of the anchors, where\n          total_num_proposals=batch_size*self._max_num_proposals.\n          Note that this tensor *includes* background class predictions\n          (at class index 0).\n        3) num_proposals: An int32 tensor of shape [batch_size] representing the\n          number of proposals generated by the RPN.  `num_proposals` allows us\n          to keep track of which entries are to be treated as zero paddings and\n          which are not since we always pad the number of proposals to be\n          `self.max_num_proposals` for each image.\n        4) proposal_boxes: A float32 tensor of shape\n          [batch_size, self.max_num_proposals, 4] representing\n          decoded proposal bounding boxes in absolute coordinates.\n        5) proposal_boxes_normalized: A float32 tensor of shape\n          [batch_size, self.max_num_proposals, 4] representing decoded proposal\n          bounding boxes in normalized coordinates. Can be used to override the\n          boxes proposed by the RPN, thus enabling one to extract features and\n          get box classification and prediction for externally selected areas\n          of the image.\n        6) box_classifier_features: a 4-D float32 tensor representing the\n          features for each proposal.\n        7) mask_predictions: (optional) a 4-D tensor with shape\n          [total_num_padded_proposals, num_classes, mask_height, mask_width]\n          containing instance mask predictions.\n    """"""\n    image_shape_2d = self._image_batch_shape_2d(image_shape)\n    proposal_boxes_normalized, _, num_proposals = self._postprocess_rpn(\n        rpn_box_encodings, rpn_objectness_predictions_with_background,\n        anchors, image_shape_2d, true_image_shapes)\n\n    flattened_proposal_feature_maps = (\n        self._compute_second_stage_input_feature_maps(\n            rpn_features_to_crop, proposal_boxes_normalized))\n\n    box_classifier_features = (\n        self._feature_extractor.extract_box_classifier_features(\n            flattened_proposal_feature_maps,\n            scope=self.second_stage_feature_extractor_scope))\n\n    predict_auxiliary_outputs = False\n    if self._number_of_stages == 2:\n      predict_auxiliary_outputs = True\n    box_predictions = self._mask_rcnn_box_predictor.predict(\n        [box_classifier_features],\n        num_predictions_per_location=[1],\n        scope=self.second_stage_box_predictor_scope,\n        predict_boxes_and_classes=True,\n        predict_auxiliary_outputs=predict_auxiliary_outputs)\n\n    refined_box_encodings = tf.squeeze(\n        box_predictions[box_predictor.BOX_ENCODINGS], axis=1)\n    class_predictions_with_background = tf.squeeze(box_predictions[\n        box_predictor.CLASS_PREDICTIONS_WITH_BACKGROUND], axis=1)\n\n    absolute_proposal_boxes = ops.normalized_to_image_coordinates(\n        proposal_boxes_normalized, image_shape, self._parallel_iterations)\n\n    prediction_dict = {\n        \'refined_box_encodings\': refined_box_encodings,\n        \'class_predictions_with_background\':\n        class_predictions_with_background,\n        \'num_proposals\': num_proposals,\n        \'proposal_boxes\': absolute_proposal_boxes,\n        \'box_classifier_features\': box_classifier_features,\n        \'proposal_boxes_normalized\': proposal_boxes_normalized,\n    }\n    if box_predictor.MASK_PREDICTIONS in box_predictions:\n      mask_predictions = tf.squeeze(box_predictions[\n          box_predictor.MASK_PREDICTIONS], axis=1)\n      prediction_dict[\'mask_predictions\'] = mask_predictions\n\n    return prediction_dict\n\n  def _predict_third_stage(self, prediction_dict, image_shapes):\n    """"""Predicts non-box, non-class outputs using refined detections.\n\n    Args:\n     prediction_dict: a dictionary holding ""raw"" prediction tensors:\n        1) refined_box_encodings: a 3-D tensor with shape\n          [total_num_proposals, num_classes, 4] representing predicted\n          (final) refined box encodings, where\n          total_num_proposals=batch_size*self._max_num_proposals\n        2) class_predictions_with_background: a 3-D tensor with shape\n          [total_num_proposals, num_classes + 1] containing class\n          predictions (logits) for each of the anchors, where\n          total_num_proposals=batch_size*self._max_num_proposals.\n          Note that this tensor *includes* background class predictions\n          (at class index 0).\n        3) num_proposals: An int32 tensor of shape [batch_size] representing the\n          number of proposals generated by the RPN.  `num_proposals` allows us\n          to keep track of which entries are to be treated as zero paddings and\n          which are not since we always pad the number of proposals to be\n          `self.max_num_proposals` for each image.\n        4) proposal_boxes: A float32 tensor of shape\n          [batch_size, self.max_num_proposals, 4] representing\n          decoded proposal bounding boxes in absolute coordinates.\n      image_shapes: A 2-D int32 tensors of shape [batch_size, 3] containing\n        shapes of images in the batch.\n\n    Returns:\n      prediction_dict: a dictionary that in addition to the input predictions\n      does hold the following predictions as well:\n        1) mask_predictions: (optional) a 4-D tensor with shape\n          [batch_size, max_detection, mask_height, mask_width] containing\n          instance mask predictions.\n    """"""\n    detections_dict = self._postprocess_box_classifier(\n        prediction_dict[\'refined_box_encodings\'],\n        prediction_dict[\'class_predictions_with_background\'],\n        prediction_dict[\'proposal_boxes\'],\n        prediction_dict[\'num_proposals\'],\n        image_shapes)\n    prediction_dict.update(detections_dict)\n    detection_boxes = detections_dict[\n        fields.DetectionResultFields.detection_boxes]\n    detection_classes = detections_dict[\n        fields.DetectionResultFields.detection_classes]\n    rpn_features_to_crop = prediction_dict[\'rpn_features_to_crop\']\n    batch_size = tf.shape(detection_boxes)[0]\n    max_detection = tf.shape(detection_boxes)[1]\n    flattened_detected_feature_maps = (\n        self._compute_second_stage_input_feature_maps(\n            rpn_features_to_crop, detection_boxes))\n    detected_box_classifier_features = (\n        self._feature_extractor.extract_box_classifier_features(\n            flattened_detected_feature_maps,\n            scope=self.second_stage_feature_extractor_scope))\n    box_predictions = self._mask_rcnn_box_predictor.predict(\n        [detected_box_classifier_features],\n        num_predictions_per_location=[1],\n        scope=self.second_stage_box_predictor_scope,\n        predict_boxes_and_classes=False,\n        predict_auxiliary_outputs=True)\n    if box_predictor.MASK_PREDICTIONS in box_predictions:\n      detection_masks = tf.squeeze(box_predictions[\n          box_predictor.MASK_PREDICTIONS], axis=1)\n      detection_masks = self._gather_instance_masks(detection_masks,\n                                                    detection_classes)\n      mask_height = tf.shape(detection_masks)[1]\n      mask_width = tf.shape(detection_masks)[2]\n      prediction_dict[fields.DetectionResultFields.detection_masks] = (\n          tf.reshape(detection_masks,\n                     [batch_size, max_detection, mask_height, mask_width]))\n    return prediction_dict\n\n  def _gather_instance_masks(self, instance_masks, classes):\n    """"""Gathers the masks that correspond to classes.\n\n    Args:\n      instance_masks: A 4-D float32 tensor with shape\n        [K, num_classes, mask_height, mask_width].\n      classes: A 2-D int32 tensor with shape [batch_size, max_detection].\n\n    Returns:\n      masks: a 3-D float32 tensor with shape [K, mask_height, mask_width].\n    """"""\n    k = tf.shape(instance_masks)[0]\n    num_mask_classes = tf.shape(instance_masks)[1]\n    instance_mask_height = tf.shape(instance_masks)[2]\n    instance_mask_width = tf.shape(instance_masks)[3]\n    classes = tf.reshape(classes, [-1])\n    instance_masks = tf.reshape(instance_masks, [\n        -1, instance_mask_height, instance_mask_width\n    ])\n    return tf.gather(instance_masks,\n                     tf.range(k) * num_mask_classes + tf.to_int32(classes))\n\n  def _extract_rpn_feature_maps(self, preprocessed_inputs):\n    """"""Extracts RPN features.\n\n    This function extracts two feature maps: a feature map to be directly\n    fed to a box predictor (to predict location and objectness scores for\n    proposals) and a feature map from which to crop regions which will then\n    be sent to the second stage box classifier.\n\n    Args:\n      preprocessed_inputs: a [batch, height, width, channels] image tensor.\n\n    Returns:\n      rpn_box_predictor_features: A 4-D float32 tensor with shape\n        [batch, height, width, depth] to be used for predicting proposal boxes\n        and corresponding objectness scores.\n      rpn_features_to_crop: A 4-D float32 tensor with shape\n        [batch, height, width, depth] representing image features to crop using\n        the proposals boxes.\n      anchors: A BoxList representing anchors (for the RPN) in\n        absolute coordinates.\n      image_shape: A 1-D tensor representing the input image shape.\n    """"""\n    image_shape = tf.shape(preprocessed_inputs)\n    rpn_features_to_crop, _ = self._feature_extractor.extract_proposal_features(\n        preprocessed_inputs, scope=self.first_stage_feature_extractor_scope)\n\n    feature_map_shape = tf.shape(rpn_features_to_crop)\n    anchors = box_list_ops.concatenate(\n        self._first_stage_anchor_generator.generate([(feature_map_shape[1],\n                                                      feature_map_shape[2])]))\n    with slim.arg_scope(self._first_stage_box_predictor_arg_scope):\n      kernel_size = self._first_stage_box_predictor_kernel_size\n      rpn_box_predictor_features = slim.conv2d(\n          rpn_features_to_crop,\n          self._first_stage_box_predictor_depth,\n          kernel_size=[kernel_size, kernel_size],\n          rate=self._first_stage_atrous_rate,\n          activation_fn=tf.nn.relu6)\n    return (rpn_box_predictor_features, rpn_features_to_crop,\n            anchors, image_shape)\n\n  def _predict_rpn_proposals(self, rpn_box_predictor_features):\n    """"""Adds box predictors to RPN feature map to predict proposals.\n\n    Note resulting tensors will not have been postprocessed.\n\n    Args:\n      rpn_box_predictor_features: A 4-D float32 tensor with shape\n        [batch, height, width, depth] to be used for predicting proposal boxes\n        and corresponding objectness scores.\n\n    Returns:\n      box_encodings: 3-D float tensor of shape\n        [batch_size, num_anchors, self._box_coder.code_size] containing\n        predicted boxes.\n      objectness_predictions_with_background: 3-D float tensor of shape\n        [batch_size, num_anchors, 2] containing class\n        predictions (logits) for each of the anchors.  Note that this\n        tensor *includes* background class predictions (at class index 0).\n\n    Raises:\n      RuntimeError: if the anchor generator generates anchors corresponding to\n        multiple feature maps.  We currently assume that a single feature map\n        is generated for the RPN.\n    """"""\n    num_anchors_per_location = (\n        self._first_stage_anchor_generator.num_anchors_per_location())\n    if len(num_anchors_per_location) != 1:\n      raise RuntimeError(\'anchor_generator is expected to generate anchors \'\n                         \'corresponding to a single feature map.\')\n    box_predictions = self._first_stage_box_predictor.predict(\n        [rpn_box_predictor_features],\n        num_anchors_per_location,\n        scope=self.first_stage_box_predictor_scope)\n\n    box_encodings = tf.concat(\n        box_predictions[box_predictor.BOX_ENCODINGS], axis=1)\n    objectness_predictions_with_background = tf.concat(\n        box_predictions[box_predictor.CLASS_PREDICTIONS_WITH_BACKGROUND],\n        axis=1)\n    return (tf.squeeze(box_encodings, axis=2),\n            objectness_predictions_with_background)\n\n  def _remove_invalid_anchors_and_predictions(\n      self,\n      box_encodings,\n      objectness_predictions_with_background,\n      anchors_boxlist,\n      clip_window):\n    """"""Removes anchors that (partially) fall outside an image.\n\n    Also removes associated box encodings and objectness predictions.\n\n    Args:\n      box_encodings: 3-D float tensor of shape\n        [batch_size, num_anchors, self._box_coder.code_size] containing\n        predicted boxes.\n      objectness_predictions_with_background: 3-D float tensor of shape\n        [batch_size, num_anchors, 2] containing class\n        predictions (logits) for each of the anchors.  Note that this\n        tensor *includes* background class predictions (at class index 0).\n      anchors_boxlist: A BoxList representing num_anchors anchors (for the RPN)\n        in absolute coordinates.\n      clip_window: a 1-D tensor representing the [ymin, xmin, ymax, xmax]\n        extent of the window to clip/prune to.\n\n    Returns:\n      box_encodings: 4-D float tensor of shape\n        [batch_size, num_valid_anchors, self._box_coder.code_size] containing\n        predicted boxes, where num_valid_anchors <= num_anchors\n      objectness_predictions_with_background: 2-D float tensor of shape\n        [batch_size, num_valid_anchors, 2] containing class\n        predictions (logits) for each of the anchors, where\n        num_valid_anchors <= num_anchors.  Note that this\n        tensor *includes* background class predictions (at class index 0).\n      anchors: A BoxList representing num_valid_anchors anchors (for the RPN) in\n        absolute coordinates.\n    """"""\n    pruned_anchors_boxlist, keep_indices = box_list_ops.prune_outside_window(\n        anchors_boxlist, clip_window)\n    def _batch_gather_kept_indices(predictions_tensor):\n      return shape_utils.static_or_dynamic_map_fn(\n          partial(tf.gather, indices=keep_indices),\n          elems=predictions_tensor,\n          dtype=tf.float32,\n          parallel_iterations=self._parallel_iterations,\n          back_prop=True)\n    return (_batch_gather_kept_indices(box_encodings),\n            _batch_gather_kept_indices(objectness_predictions_with_background),\n            pruned_anchors_boxlist)\n\n  def _flatten_first_two_dimensions(self, inputs):\n    """"""Flattens `K-d` tensor along batch dimension to be a `(K-1)-d` tensor.\n\n    Converts `inputs` with shape [A, B, ..., depth] into a tensor of shape\n    [A * B, ..., depth].\n\n    Args:\n      inputs: A float tensor with shape [A, B, ..., depth].  Note that the first\n        two and last dimensions must be statically defined.\n    Returns:\n      A float tensor with shape [A * B, ..., depth] (where the first and last\n        dimension are statically defined.\n    """"""\n    combined_shape = shape_utils.combined_static_and_dynamic_shape(inputs)\n    flattened_shape = tf.stack([combined_shape[0] * combined_shape[1]] +\n                               combined_shape[2:])\n    return tf.reshape(inputs, flattened_shape)\n\n  def postprocess(self, prediction_dict, true_image_shapes):\n    """"""Convert prediction tensors to final detections.\n\n    This function converts raw predictions tensors to final detection results.\n    See base class for output format conventions.  Note also that by default,\n    scores are to be interpreted as logits, but if a score_converter is used,\n    then scores are remapped (and may thus have a different interpretation).\n\n    If number_of_stages=1, the returned results represent proposals from the\n    first stage RPN and are padded to have self.max_num_proposals for each\n    image; otherwise, the results can be interpreted as multiclass detections\n    from the full two-stage model and are padded to self._max_detections.\n\n    Args:\n      prediction_dict: a dictionary holding prediction tensors (see the\n        documentation for the predict method.  If number_of_stages=1, we\n        expect prediction_dict to contain `rpn_box_encodings`,\n        `rpn_objectness_predictions_with_background`, `rpn_features_to_crop`,\n        and `anchors` fields.  Otherwise we expect prediction_dict to\n        additionally contain `refined_box_encodings`,\n        `class_predictions_with_background`, `num_proposals`,\n        `proposal_boxes` and, optionally, `mask_predictions` fields.\n      true_image_shapes: int32 tensor of shape [batch, 3] where each row is\n        of the form [height, width, channels] indicating the shapes\n        of true images in the resized images, as resized images can be padded\n        with zeros.\n\n    Returns:\n      detections: a dictionary containing the following fields\n        detection_boxes: [batch, max_detection, 4]\n        detection_scores: [batch, max_detections]\n        detection_classes: [batch, max_detections]\n          (this entry is only created if rpn_mode=False)\n        num_detections: [batch]\n\n    Raises:\n      ValueError: If `predict` is called before `preprocess`.\n    """"""\n\n    with tf.name_scope(\'FirstStagePostprocessor\'):\n      if self._number_of_stages == 1:\n        proposal_boxes, proposal_scores, num_proposals = self._postprocess_rpn(\n            prediction_dict[\'rpn_box_encodings\'],\n            prediction_dict[\'rpn_objectness_predictions_with_background\'],\n            prediction_dict[\'anchors\'],\n            true_image_shapes,\n            true_image_shapes)\n        return {\n            fields.DetectionResultFields.detection_boxes: proposal_boxes,\n            fields.DetectionResultFields.detection_scores: proposal_scores,\n            fields.DetectionResultFields.num_detections:\n                tf.to_float(num_proposals),\n        }\n\n    with tf.name_scope(\'SecondStagePostprocessor\'):\n      if self._number_of_stages == 2:\n        mask_predictions = prediction_dict.get(box_predictor.MASK_PREDICTIONS)\n        detections_dict = self._postprocess_box_classifier(\n            prediction_dict[\'refined_box_encodings\'],\n            prediction_dict[\'class_predictions_with_background\'],\n            prediction_dict[\'proposal_boxes\'],\n            prediction_dict[\'num_proposals\'],\n            true_image_shapes,\n            mask_predictions=mask_predictions)\n        return detections_dict\n\n    if self._number_of_stages == 3:\n      # Post processing is already performed in 3rd stage. We need to transfer\n      # postprocessed tensors from `prediction_dict` to `detections_dict`.\n      detections_dict = {}\n      for key in prediction_dict:\n        if key == fields.DetectionResultFields.detection_masks:\n          detections_dict[key] = tf.sigmoid(prediction_dict[key])\n        elif \'detection\' in key:\n          detections_dict[key] = prediction_dict[key]\n      return detections_dict\n\n  def _postprocess_rpn(self,\n                       rpn_box_encodings_batch,\n                       rpn_objectness_predictions_with_background_batch,\n                       anchors,\n                       image_shapes,\n                       true_image_shapes):\n    """"""Converts first stage prediction tensors from the RPN to proposals.\n\n    This function decodes the raw RPN predictions, runs non-max suppression\n    on the result.\n\n    Note that the behavior of this function is slightly modified during\n    training --- specifically, we stop the gradient from passing through the\n    proposal boxes and we only return a balanced sampled subset of proposals\n    with size `second_stage_batch_size`.\n\n    Args:\n      rpn_box_encodings_batch: A 3-D float32 tensor of shape\n        [batch_size, num_anchors, self._box_coder.code_size] containing\n        predicted proposal box encodings.\n      rpn_objectness_predictions_with_background_batch: A 3-D float tensor of\n        shape [batch_size, num_anchors, 2] containing objectness predictions\n        (logits) for each of the anchors with 0 corresponding to background\n        and 1 corresponding to object.\n      anchors: A 2-D tensor of shape [num_anchors, 4] representing anchors\n        for the first stage RPN.  Note that `num_anchors` can differ depending\n        on whether the model is created in training or inference mode.\n      image_shapes: A 2-D tensor of shape [batch, 3] containing the shapes of\n        images in the batch.\n      true_image_shapes: int32 tensor of shape [batch, 3] where each row is\n        of the form [height, width, channels] indicating the shapes\n        of true images in the resized images, as resized images can be padded\n        with zeros.\n\n    Returns:\n      proposal_boxes: A float tensor with shape\n        [batch_size, max_num_proposals, 4] representing the (potentially zero\n        padded) proposal boxes for all images in the batch.  These boxes are\n        represented as normalized coordinates.\n      proposal_scores:  A float tensor with shape\n        [batch_size, max_num_proposals] representing the (potentially zero\n        padded) proposal objectness scores for all images in the batch.\n      num_proposals: A Tensor of type `int32`. A 1-D tensor of shape [batch]\n        representing the number of proposals predicted for each image in\n        the batch.\n    """"""\n    rpn_box_encodings_batch = tf.expand_dims(rpn_box_encodings_batch, axis=2)\n    rpn_encodings_shape = shape_utils.combined_static_and_dynamic_shape(\n        rpn_box_encodings_batch)\n    tiled_anchor_boxes = tf.tile(\n        tf.expand_dims(anchors, 0), [rpn_encodings_shape[0], 1, 1])\n    proposal_boxes = self._batch_decode_boxes(rpn_box_encodings_batch,\n                                              tiled_anchor_boxes)\n    proposal_boxes = tf.squeeze(proposal_boxes, axis=2)\n    rpn_objectness_softmax_without_background = tf.nn.softmax(\n        rpn_objectness_predictions_with_background_batch)[:, :, 1]\n    clip_window = self._compute_clip_window(image_shapes)\n    (proposal_boxes, proposal_scores, _, _, _,\n     num_proposals) = post_processing.batch_multiclass_non_max_suppression(\n         tf.expand_dims(proposal_boxes, axis=2),\n         tf.expand_dims(rpn_objectness_softmax_without_background,\n                        axis=2),\n         self._first_stage_nms_score_threshold,\n         self._first_stage_nms_iou_threshold,\n         self._first_stage_max_proposals,\n         self._first_stage_max_proposals,\n         clip_window=clip_window)\n    if self._is_training:\n      proposal_boxes = tf.stop_gradient(proposal_boxes)\n      if not self._hard_example_miner:\n        (groundtruth_boxlists, groundtruth_classes_with_background_list,\n         _) = self._format_groundtruth_data(true_image_shapes)\n        (proposal_boxes, proposal_scores,\n         num_proposals) = self._unpad_proposals_and_sample_box_classifier_batch(\n             proposal_boxes, proposal_scores, num_proposals,\n             groundtruth_boxlists, groundtruth_classes_with_background_list)\n    # normalize proposal boxes\n    def normalize_boxes(args):\n      proposal_boxes_per_image = args[0]\n      image_shape = args[1]\n      normalized_boxes_per_image = box_list_ops.to_normalized_coordinates(\n          box_list.BoxList(proposal_boxes_per_image), image_shape[0],\n          image_shape[1], check_range=False).get()\n      return normalized_boxes_per_image\n    normalized_proposal_boxes = shape_utils.static_or_dynamic_map_fn(\n        normalize_boxes, elems=[proposal_boxes, image_shapes], dtype=tf.float32)\n    return normalized_proposal_boxes, proposal_scores, num_proposals\n\n  def _unpad_proposals_and_sample_box_classifier_batch(\n      self,\n      proposal_boxes,\n      proposal_scores,\n      num_proposals,\n      groundtruth_boxlists,\n      groundtruth_classes_with_background_list):\n    """"""Unpads proposals and samples a minibatch for second stage.\n\n    Args:\n      proposal_boxes: A float tensor with shape\n        [batch_size, num_proposals, 4] representing the (potentially zero\n        padded) proposal boxes for all images in the batch.  These boxes are\n        represented in absolute coordinates.\n      proposal_scores:  A float tensor with shape\n        [batch_size, num_proposals] representing the (potentially zero\n        padded) proposal objectness scores for all images in the batch.\n      num_proposals: A Tensor of type `int32`. A 1-D tensor of shape [batch]\n        representing the number of proposals predicted for each image in\n        the batch.\n      groundtruth_boxlists: A list of BoxLists containing (absolute) coordinates\n        of the groundtruth boxes.\n      groundtruth_classes_with_background_list: A list of 2-D one-hot\n        (or k-hot) tensors of shape [num_boxes, num_classes+1] containing the\n        class targets with the 0th index assumed to map to the background class.\n\n    Returns:\n      proposal_boxes: A float tensor with shape\n        [batch_size, second_stage_batch_size, 4] representing the (potentially\n        zero padded) proposal boxes for all images in the batch.  These boxes\n        are represented in absolute coordinates.\n      proposal_scores:  A float tensor with shape\n        [batch_size, second_stage_batch_size] representing the (potentially zero\n        padded) proposal objectness scores for all images in the batch.\n      num_proposals: A Tensor of type `int32`. A 1-D tensor of shape [batch]\n        representing the number of proposals predicted for each image in\n        the batch.\n    """"""\n    single_image_proposal_box_sample = []\n    single_image_proposal_score_sample = []\n    single_image_num_proposals_sample = []\n    for (single_image_proposal_boxes,\n         single_image_proposal_scores,\n         single_image_num_proposals,\n         single_image_groundtruth_boxlist,\n         single_image_groundtruth_classes_with_background) in zip(\n             tf.unstack(proposal_boxes),\n             tf.unstack(proposal_scores),\n             tf.unstack(num_proposals),\n             groundtruth_boxlists,\n             groundtruth_classes_with_background_list):\n      static_shape = single_image_proposal_boxes.get_shape()\n      sliced_static_shape = tf.TensorShape([tf.Dimension(None),\n                                            static_shape.dims[-1]])\n      single_image_proposal_boxes = tf.slice(\n          single_image_proposal_boxes,\n          [0, 0],\n          [single_image_num_proposals, -1])\n      single_image_proposal_boxes.set_shape(sliced_static_shape)\n\n      single_image_proposal_scores = tf.slice(single_image_proposal_scores,\n                                              [0],\n                                              [single_image_num_proposals])\n      single_image_boxlist = box_list.BoxList(single_image_proposal_boxes)\n      single_image_boxlist.add_field(fields.BoxListFields.scores,\n                                     single_image_proposal_scores)\n      sampled_boxlist = self._sample_box_classifier_minibatch(\n          single_image_boxlist,\n          single_image_groundtruth_boxlist,\n          single_image_groundtruth_classes_with_background)\n      sampled_padded_boxlist = box_list_ops.pad_or_clip_box_list(\n          sampled_boxlist,\n          num_boxes=self._second_stage_batch_size)\n      single_image_num_proposals_sample.append(tf.minimum(\n          sampled_boxlist.num_boxes(),\n          self._second_stage_batch_size))\n      bb = sampled_padded_boxlist.get()\n      single_image_proposal_box_sample.append(bb)\n      single_image_proposal_score_sample.append(\n          sampled_padded_boxlist.get_field(fields.BoxListFields.scores))\n    return (tf.stack(single_image_proposal_box_sample),\n            tf.stack(single_image_proposal_score_sample),\n            tf.stack(single_image_num_proposals_sample))\n\n  def _format_groundtruth_data(self, true_image_shapes):\n    """"""Helper function for preparing groundtruth data for target assignment.\n\n    In order to be consistent with the model.DetectionModel interface,\n    groundtruth boxes are specified in normalized coordinates and classes are\n    specified as label indices with no assumed background category.  To prepare\n    for target assignment, we:\n    1) convert boxes to absolute coordinates,\n    2) add a background class at class index 0\n    3) groundtruth instance masks, if available, are resized to match\n       image_shape.\n\n    Args:\n      true_image_shapes: int32 tensor of shape [batch, 3] where each row is\n        of the form [height, width, channels] indicating the shapes\n        of true images in the resized images, as resized images can be padded\n        with zeros.\n\n    Returns:\n      groundtruth_boxlists: A list of BoxLists containing (absolute) coordinates\n        of the groundtruth boxes.\n      groundtruth_classes_with_background_list: A list of 2-D one-hot\n        (or k-hot) tensors of shape [num_boxes, num_classes+1] containing the\n        class targets with the 0th index assumed to map to the background class.\n      groundtruth_masks_list: If present, a list of 3-D tf.float32 tensors of\n        shape [num_boxes, image_height, image_width] containing instance masks.\n        This is set to None if no masks exist in the provided groundtruth.\n    """"""\n    groundtruth_boxlists = [\n        box_list_ops.to_absolute_coordinates(\n            box_list.BoxList(boxes), true_image_shapes[i, 0],\n            true_image_shapes[i, 1])\n        for i, boxes in enumerate(\n            self.groundtruth_lists(fields.BoxListFields.boxes))\n    ]\n    groundtruth_classes_with_background_list = [\n        tf.to_float(\n            tf.pad(one_hot_encoding, [[0, 0], [1, 0]], mode=\'CONSTANT\'))\n        for one_hot_encoding in self.groundtruth_lists(\n            fields.BoxListFields.classes)]\n\n    groundtruth_masks_list = self._groundtruth_lists.get(\n        fields.BoxListFields.masks)\n    if groundtruth_masks_list is not None:\n      resized_masks_list = []\n      for mask in groundtruth_masks_list:\n        _, resized_mask, _ = self._image_resizer_fn(\n            # Reuse the given `image_resizer_fn` to resize groundtruth masks.\n            # `mask` tensor for an image is of the shape [num_masks,\n            # image_height, image_width]. Below we create a dummy image of the\n            # the shape [image_height, image_width, 1] to use with\n            # `image_resizer_fn`.\n            image=tf.zeros(tf.stack([tf.shape(mask)[1], tf.shape(mask)[2], 1])),\n            masks=mask)\n        resized_masks_list.append(resized_mask)\n\n      groundtruth_masks_list = resized_masks_list\n\n    return (groundtruth_boxlists, groundtruth_classes_with_background_list,\n            groundtruth_masks_list)\n\n  def _sample_box_classifier_minibatch(self,\n                                       proposal_boxlist,\n                                       groundtruth_boxlist,\n                                       groundtruth_classes_with_background):\n    """"""Samples a mini-batch of proposals to be sent to the box classifier.\n\n    Helper function for self._postprocess_rpn.\n\n    Args:\n      proposal_boxlist: A BoxList containing K proposal boxes in absolute\n        coordinates.\n      groundtruth_boxlist: A Boxlist containing N groundtruth object boxes in\n        absolute coordinates.\n      groundtruth_classes_with_background: A tensor with shape\n        `[N, self.num_classes + 1]` representing groundtruth classes. The\n        classes are assumed to be k-hot encoded, and include background as the\n        zero-th class.\n\n    Returns:\n      a BoxList contained sampled proposals.\n    """"""\n    (cls_targets, cls_weights, _, _, _) = self._detector_target_assigner.assign(\n        proposal_boxlist, groundtruth_boxlist,\n        groundtruth_classes_with_background)\n    # Selects all boxes as candidates if none of them is selected according\n    # to cls_weights. This could happen as boxes within certain IOU ranges\n    # are ignored. If triggered, the selected boxes will still be ignored\n    # during loss computation.\n    cls_weights += tf.to_float(tf.equal(tf.reduce_sum(cls_weights), 0))\n    positive_indicator = tf.greater(tf.argmax(cls_targets, axis=1), 0)\n    sampled_indices = self._second_stage_sampler.subsample(\n        tf.cast(cls_weights, tf.bool),\n        self._second_stage_batch_size,\n        positive_indicator)\n    return box_list_ops.boolean_mask(proposal_boxlist, sampled_indices)\n\n  def _compute_second_stage_input_feature_maps(self, features_to_crop,\n                                               proposal_boxes_normalized):\n    """"""Crops to a set of proposals from the feature map for a batch of images.\n\n    Helper function for self._postprocess_rpn. This function calls\n    `tf.image.crop_and_resize` to create the feature map to be passed to the\n    second stage box classifier for each proposal.\n\n    Args:\n      features_to_crop: A float32 tensor with shape\n        [batch_size, height, width, depth]\n      proposal_boxes_normalized: A float32 tensor with shape [batch_size,\n        num_proposals, box_code_size] containing proposal boxes in\n        normalized coordinates.\n\n    Returns:\n      A float32 tensor with shape [K, new_height, new_width, depth].\n    """"""\n    def get_box_inds(proposals):\n      proposals_shape = proposals.get_shape().as_list()\n      if any(dim is None for dim in proposals_shape):\n        proposals_shape = tf.shape(proposals)\n      ones_mat = tf.ones(proposals_shape[:2], dtype=tf.int32)\n      multiplier = tf.expand_dims(\n          tf.range(start=0, limit=proposals_shape[0]), 1)\n      return tf.reshape(ones_mat * multiplier, [-1])\n\n    cropped_regions = tf.image.crop_and_resize(\n        features_to_crop,\n        self._flatten_first_two_dimensions(proposal_boxes_normalized),\n        get_box_inds(proposal_boxes_normalized),\n        (self._initial_crop_size, self._initial_crop_size))\n    return slim.max_pool2d(\n        cropped_regions,\n        [self._maxpool_kernel_size, self._maxpool_kernel_size],\n        stride=self._maxpool_stride)\n\n  def _postprocess_box_classifier(self,\n                                  refined_box_encodings,\n                                  class_predictions_with_background,\n                                  proposal_boxes,\n                                  num_proposals,\n                                  image_shapes,\n                                  mask_predictions=None):\n    """"""Converts predictions from the second stage box classifier to detections.\n\n    Args:\n      refined_box_encodings: a 3-D float tensor with shape\n        [total_num_padded_proposals, num_classes, 4] representing predicted\n        (final) refined box encodings.\n      class_predictions_with_background: a 3-D tensor float with shape\n        [total_num_padded_proposals, num_classes + 1] containing class\n        predictions (logits) for each of the proposals.  Note that this tensor\n        *includes* background class predictions (at class index 0).\n      proposal_boxes: a 3-D float tensor with shape\n        [batch_size, self.max_num_proposals, 4] representing decoded proposal\n        bounding boxes in absolute coordinates.\n      num_proposals: a 1-D int32 tensor of shape [batch] representing the number\n        of proposals predicted for each image in the batch.\n      image_shapes: a 2-D int32 tensor containing shapes of input image in the\n        batch.\n      mask_predictions: (optional) a 4-D float tensor with shape\n        [total_num_padded_proposals, num_classes, mask_height, mask_width]\n        containing instance mask prediction logits.\n\n    Returns:\n      A dictionary containing:\n        `detection_boxes`: [batch, max_detection, 4]\n        `detection_scores`: [batch, max_detections]\n        `detection_classes`: [batch, max_detections]\n        `num_detections`: [batch]\n        `detection_masks`:\n          (optional) [batch, max_detections, mask_height, mask_width]. Note\n          that a pixel-wise sigmoid score converter is applied to the detection\n          masks.\n    """"""\n    refined_box_encodings_batch = tf.reshape(refined_box_encodings,\n                                             [-1, self.max_num_proposals,\n                                              self.num_classes,\n                                              self._box_coder.code_size])\n    class_predictions_with_background_batch = tf.reshape(\n        class_predictions_with_background,\n        [-1, self.max_num_proposals, self.num_classes + 1]\n    )\n    refined_decoded_boxes_batch = self._batch_decode_boxes(\n        refined_box_encodings_batch, proposal_boxes)\n    class_predictions_with_background_batch = (\n        self._second_stage_score_conversion_fn(\n            class_predictions_with_background_batch))\n    class_predictions_batch = tf.reshape(\n        tf.slice(class_predictions_with_background_batch,\n                 [0, 0, 1], [-1, -1, -1]),\n        [-1, self.max_num_proposals, self.num_classes])\n    clip_window = self._compute_clip_window(image_shapes)\n    mask_predictions_batch = None\n    if mask_predictions is not None:\n      mask_height = mask_predictions.shape[2].value\n      mask_width = mask_predictions.shape[3].value\n      mask_predictions = tf.sigmoid(mask_predictions)\n      mask_predictions_batch = tf.reshape(\n          mask_predictions, [-1, self.max_num_proposals,\n                             self.num_classes, mask_height, mask_width])\n    (nmsed_boxes, nmsed_scores, nmsed_classes, nmsed_masks, _,\n     num_detections) = self._second_stage_nms_fn(\n         refined_decoded_boxes_batch,\n         class_predictions_batch,\n         clip_window=clip_window,\n         change_coordinate_frame=True,\n         num_valid_boxes=num_proposals,\n         masks=mask_predictions_batch)\n    detections = {\n        fields.DetectionResultFields.detection_boxes: nmsed_boxes,\n        fields.DetectionResultFields.detection_scores: nmsed_scores,\n        fields.DetectionResultFields.detection_classes: nmsed_classes,\n        fields.DetectionResultFields.num_detections: tf.to_float(num_detections)\n    }\n    if nmsed_masks is not None:\n      detections[fields.DetectionResultFields.detection_masks] = nmsed_masks\n    return detections\n\n  def _batch_decode_boxes(self, box_encodings, anchor_boxes):\n    """"""Decodes box encodings with respect to the anchor boxes.\n\n    Args:\n      box_encodings: a 4-D tensor with shape\n        [batch_size, num_anchors, num_classes, self._box_coder.code_size]\n        representing box encodings.\n      anchor_boxes: [batch_size, num_anchors, 4] representing\n        decoded bounding boxes.\n\n    Returns:\n      decoded_boxes: a [batch_size, num_anchors, num_classes, 4]\n        float tensor representing bounding box predictions\n        (for each image in batch, proposal and class).\n    """"""\n    combined_shape = shape_utils.combined_static_and_dynamic_shape(\n        box_encodings)\n    num_classes = combined_shape[2]\n    tiled_anchor_boxes = tf.tile(\n        tf.expand_dims(anchor_boxes, 2), [1, 1, num_classes, 1])\n    tiled_anchors_boxlist = box_list.BoxList(\n        tf.reshape(tiled_anchor_boxes, [-1, 4]))\n    decoded_boxes = self._box_coder.decode(\n        tf.reshape(box_encodings, [-1, self._box_coder.code_size]),\n        tiled_anchors_boxlist)\n    return tf.reshape(decoded_boxes.get(),\n                      tf.stack([combined_shape[0], combined_shape[1],\n                                num_classes, 4]))\n\n  def loss(self, prediction_dict, true_image_shapes, scope=None):\n    """"""Compute scalar loss tensors given prediction tensors.\n\n    If number_of_stages=1, only RPN related losses are computed (i.e.,\n    `rpn_localization_loss` and `rpn_objectness_loss`).  Otherwise all\n    losses are computed.\n\n    Args:\n      prediction_dict: a dictionary holding prediction tensors (see the\n        documentation for the predict method.  If number_of_stages=1, we\n        expect prediction_dict to contain `rpn_box_encodings`,\n        `rpn_objectness_predictions_with_background`, `rpn_features_to_crop`,\n        `image_shape`, and `anchors` fields.  Otherwise we expect\n        prediction_dict to additionally contain `refined_box_encodings`,\n        `class_predictions_with_background`, `num_proposals`, and\n        `proposal_boxes` fields.\n      true_image_shapes: int32 tensor of shape [batch, 3] where each row is\n        of the form [height, width, channels] indicating the shapes\n        of true images in the resized images, as resized images can be padded\n        with zeros.\n      scope: Optional scope name.\n\n    Returns:\n      a dictionary mapping loss keys (`first_stage_localization_loss`,\n        `first_stage_objectness_loss`, \'second_stage_localization_loss\',\n        \'second_stage_classification_loss\') to scalar tensors representing\n        corresponding loss values.\n    """"""\n    with tf.name_scope(scope, \'Loss\', prediction_dict.values()):\n      (groundtruth_boxlists, groundtruth_classes_with_background_list,\n       groundtruth_masks_list) = self._format_groundtruth_data(\n           true_image_shapes)\n      loss_dict = self._loss_rpn(\n          prediction_dict[\'rpn_box_encodings\'],\n          prediction_dict[\'rpn_objectness_predictions_with_background\'],\n          prediction_dict[\'anchors\'],\n          groundtruth_boxlists,\n          groundtruth_classes_with_background_list)\n      if self._number_of_stages > 1:\n        loss_dict.update(\n            self._loss_box_classifier(\n                prediction_dict[\'refined_box_encodings\'],\n                prediction_dict[\'class_predictions_with_background\'],\n                prediction_dict[\'proposal_boxes\'],\n                prediction_dict[\'num_proposals\'],\n                groundtruth_boxlists,\n                groundtruth_classes_with_background_list,\n                prediction_dict[\'image_shape\'],\n                prediction_dict.get(\'mask_predictions\'),\n                groundtruth_masks_list,\n            ))\n    return loss_dict\n\n  def _loss_rpn(self,\n                rpn_box_encodings,\n                rpn_objectness_predictions_with_background,\n                anchors,\n                groundtruth_boxlists,\n                groundtruth_classes_with_background_list):\n    """"""Computes scalar RPN loss tensors.\n\n    Uses self._proposal_target_assigner to obtain regression and classification\n    targets for the first stage RPN, samples a ""minibatch"" of anchors to\n    participate in the loss computation, and returns the RPN losses.\n\n    Args:\n      rpn_box_encodings: A 4-D float tensor of shape\n        [batch_size, num_anchors, self._box_coder.code_size] containing\n        predicted proposal box encodings.\n      rpn_objectness_predictions_with_background: A 2-D float tensor of shape\n        [batch_size, num_anchors, 2] containing objectness predictions\n        (logits) for each of the anchors with 0 corresponding to background\n        and 1 corresponding to object.\n      anchors: A 2-D tensor of shape [num_anchors, 4] representing anchors\n        for the first stage RPN.  Note that `num_anchors` can differ depending\n        on whether the model is created in training or inference mode.\n      groundtruth_boxlists: A list of BoxLists containing coordinates of the\n        groundtruth boxes.\n      groundtruth_classes_with_background_list: A list of 2-D one-hot\n        (or k-hot) tensors of shape [num_boxes, num_classes+1] containing the\n        class targets with the 0th index assumed to map to the background class.\n\n    Returns:\n      a dictionary mapping loss keys (`first_stage_localization_loss`,\n        `first_stage_objectness_loss`) to scalar tensors representing\n        corresponding loss values.\n    """"""\n    with tf.name_scope(\'RPNLoss\'):\n      (batch_cls_targets, batch_cls_weights, batch_reg_targets,\n       batch_reg_weights, _) = target_assigner.batch_assign_targets(\n           self._proposal_target_assigner, box_list.BoxList(anchors),\n           groundtruth_boxlists, len(groundtruth_boxlists)*[None])\n      batch_cls_targets = tf.squeeze(batch_cls_targets, axis=2)\n\n      def _minibatch_subsample_fn(inputs):\n        cls_targets, cls_weights = inputs\n        return self._first_stage_sampler.subsample(\n            tf.cast(cls_weights, tf.bool),\n            self._first_stage_minibatch_size, tf.cast(cls_targets, tf.bool))\n      batch_sampled_indices = tf.to_float(shape_utils.static_or_dynamic_map_fn(\n          _minibatch_subsample_fn,\n          [batch_cls_targets, batch_cls_weights],\n          dtype=tf.bool,\n          parallel_iterations=self._parallel_iterations,\n          back_prop=True))\n\n      # Normalize by number of examples in sampled minibatch\n      normalizer = tf.reduce_sum(batch_sampled_indices, axis=1)\n      batch_one_hot_targets = tf.one_hot(\n          tf.to_int32(batch_cls_targets), depth=2)\n      sampled_reg_indices = tf.multiply(batch_sampled_indices,\n                                        batch_reg_weights)\n\n      localization_losses = self._first_stage_localization_loss(\n          rpn_box_encodings, batch_reg_targets, weights=sampled_reg_indices)\n      objectness_losses = self._first_stage_objectness_loss(\n          rpn_objectness_predictions_with_background,\n          batch_one_hot_targets, weights=batch_sampled_indices)\n      localization_loss = tf.reduce_mean(\n          tf.reduce_sum(localization_losses, axis=1) / normalizer)\n      objectness_loss = tf.reduce_mean(\n          tf.reduce_sum(objectness_losses, axis=1) / normalizer)\n\n      localization_loss = tf.multiply(self._first_stage_loc_loss_weight,\n                                      localization_loss,\n                                      name=\'localization_loss\')\n      objectness_loss = tf.multiply(self._first_stage_obj_loss_weight,\n                                    objectness_loss, name=\'objectness_loss\')\n      loss_dict = {localization_loss.op.name: localization_loss,\n                   objectness_loss.op.name: objectness_loss}\n    return loss_dict\n\n  def _loss_box_classifier(self,\n                           refined_box_encodings,\n                           class_predictions_with_background,\n                           proposal_boxes,\n                           num_proposals,\n                           groundtruth_boxlists,\n                           groundtruth_classes_with_background_list,\n                           image_shape,\n                           prediction_masks=None,\n                           groundtruth_masks_list=None):\n    """"""Computes scalar box classifier loss tensors.\n\n    Uses self._detector_target_assigner to obtain regression and classification\n    targets for the second stage box classifier, optionally performs\n    hard mining, and returns losses.  All losses are computed independently\n    for each image and then averaged across the batch.\n    Please note that for boxes and masks with multiple labels, the box\n    regression and mask prediction losses are only computed for one label.\n\n    This function assumes that the proposal boxes in the ""padded"" regions are\n    actually zero (and thus should not be matched to).\n\n\n    Args:\n      refined_box_encodings: a 3-D tensor with shape\n        [total_num_proposals, num_classes, box_coder.code_size] representing\n        predicted (final) refined box encodings.\n      class_predictions_with_background: a 2-D tensor with shape\n        [total_num_proposals, num_classes + 1] containing class\n        predictions (logits) for each of the anchors.  Note that this tensor\n        *includes* background class predictions (at class index 0).\n      proposal_boxes: [batch_size, self.max_num_proposals, 4] representing\n        decoded proposal bounding boxes.\n      num_proposals: A Tensor of type `int32`. A 1-D tensor of shape [batch]\n        representing the number of proposals predicted for each image in\n        the batch.\n      groundtruth_boxlists: a list of BoxLists containing coordinates of the\n        groundtruth boxes.\n      groundtruth_classes_with_background_list: a list of 2-D one-hot\n        (or k-hot) tensors of shape [num_boxes, num_classes + 1] containing the\n        class targets with the 0th index assumed to map to the background class.\n      image_shape: a 1-D tensor of shape [4] representing the image shape.\n      prediction_masks: an optional 4-D tensor with shape [total_num_proposals,\n        num_classes, mask_height, mask_width] containing the instance masks for\n        each box.\n      groundtruth_masks_list: an optional list of 3-D tensors of shape\n        [num_boxes, image_height, image_width] containing the instance masks for\n        each of the boxes.\n\n    Returns:\n      a dictionary mapping loss keys (\'second_stage_localization_loss\',\n        \'second_stage_classification_loss\') to scalar tensors representing\n        corresponding loss values.\n\n    Raises:\n      ValueError: if `predict_instance_masks` in\n        second_stage_mask_rcnn_box_predictor is True and\n        `groundtruth_masks_list` is not provided.\n    """"""\n    with tf.name_scope(\'BoxClassifierLoss\'):\n      paddings_indicator = self._padded_batched_proposals_indicator(\n          num_proposals, self.max_num_proposals)\n      proposal_boxlists = [\n          box_list.BoxList(proposal_boxes_single_image)\n          for proposal_boxes_single_image in tf.unstack(proposal_boxes)]\n      batch_size = len(proposal_boxlists)\n\n      num_proposals_or_one = tf.to_float(tf.expand_dims(\n          tf.maximum(num_proposals, tf.ones_like(num_proposals)), 1))\n      normalizer = tf.tile(num_proposals_or_one,\n                           [1, self.max_num_proposals]) * batch_size\n\n      (batch_cls_targets_with_background, batch_cls_weights, batch_reg_targets,\n       batch_reg_weights, _) = target_assigner.batch_assign_targets(\n           self._detector_target_assigner, proposal_boxlists,\n           groundtruth_boxlists, groundtruth_classes_with_background_list)\n\n      # We only predict refined location encodings for the non background\n      # classes, but we now pad it to make it compatible with the class\n      # predictions\n      flat_cls_targets_with_background = tf.reshape(\n          batch_cls_targets_with_background,\n          [batch_size * self.max_num_proposals, -1])\n      refined_box_encodings_with_background = tf.pad(\n          refined_box_encodings, [[0, 0], [1, 0], [0, 0]])\n      # For anchors with multiple labels, picks refined_location_encodings\n      # for just one class to avoid over-counting for regression loss and\n      # (optionally) mask loss.\n      one_hot_flat_cls_targets_with_background = tf.argmax(\n          flat_cls_targets_with_background, axis=1)\n      one_hot_flat_cls_targets_with_background = tf.one_hot(\n          one_hot_flat_cls_targets_with_background,\n          flat_cls_targets_with_background.get_shape()[1])\n      refined_box_encodings_masked_by_class_targets = tf.boolean_mask(\n          refined_box_encodings_with_background,\n          tf.greater(one_hot_flat_cls_targets_with_background, 0))\n      class_predictions_with_background = tf.reshape(\n          class_predictions_with_background,\n          [batch_size, self.max_num_proposals, -1])\n      reshaped_refined_box_encodings = tf.reshape(\n          refined_box_encodings_masked_by_class_targets,\n          [batch_size, -1, 4])\n\n      second_stage_loc_losses = self._second_stage_localization_loss(\n          reshaped_refined_box_encodings,\n          batch_reg_targets, weights=batch_reg_weights) / normalizer\n      second_stage_cls_losses = ops.reduce_sum_trailing_dimensions(\n          self._second_stage_classification_loss(\n              class_predictions_with_background,\n              batch_cls_targets_with_background,\n              weights=batch_cls_weights),\n          ndims=2) / normalizer\n\n      second_stage_loc_loss = tf.reduce_sum(\n          tf.boolean_mask(second_stage_loc_losses, paddings_indicator))\n      second_stage_cls_loss = tf.reduce_sum(\n          tf.boolean_mask(second_stage_cls_losses, paddings_indicator))\n\n      if self._hard_example_miner:\n        (second_stage_loc_loss, second_stage_cls_loss\n        ) = self._unpad_proposals_and_apply_hard_mining(\n            proposal_boxlists, second_stage_loc_losses,\n            second_stage_cls_losses, num_proposals)\n      localization_loss = tf.multiply(self._second_stage_loc_loss_weight,\n                                      second_stage_loc_loss,\n                                      name=\'localization_loss\')\n\n      classification_loss = tf.multiply(self._second_stage_cls_loss_weight,\n                                        second_stage_cls_loss,\n                                        name=\'classification_loss\')\n\n      loss_dict = {localization_loss.op.name: localization_loss,\n                   classification_loss.op.name: classification_loss}\n      second_stage_mask_loss = None\n      if prediction_masks is not None:\n        if groundtruth_masks_list is None:\n          raise ValueError(\'Groundtruth instance masks not provided. \'\n                           \'Please configure input reader.\')\n\n        # Create a new target assigner that matches the proposals to groundtruth\n        # and returns the mask targets.\n        # TODO(rathodv): Move `unmatched_cls_target` from constructor to assign\n        # function. This will enable reuse of a single target assigner for both\n        # class targets and mask targets.\n        mask_target_assigner = target_assigner.create_target_assigner(\n            \'FasterRCNN\', \'detection\',\n            unmatched_cls_target=tf.zeros(image_shape[1:3], dtype=tf.float32))\n        (batch_mask_targets, _, _,\n         batch_mask_target_weights, _) = target_assigner.batch_assign_targets(\n             mask_target_assigner, proposal_boxlists,\n             groundtruth_boxlists, groundtruth_masks_list)\n\n        # Pad the prediction_masks with to add zeros for background class to be\n        # consistent with class predictions.\n        prediction_masks_with_background = tf.pad(\n            prediction_masks, [[0, 0], [1, 0], [0, 0], [0, 0]])\n        prediction_masks_masked_by_class_targets = tf.boolean_mask(\n            prediction_masks_with_background,\n            tf.greater(one_hot_flat_cls_targets_with_background, 0))\n        mask_height = prediction_masks.shape[2].value\n        mask_width = prediction_masks.shape[3].value\n        reshaped_prediction_masks = tf.reshape(\n            prediction_masks_masked_by_class_targets,\n            [batch_size, -1, mask_height * mask_width])\n\n        batch_mask_targets_shape = tf.shape(batch_mask_targets)\n        flat_gt_masks = tf.reshape(batch_mask_targets,\n                                   [-1, batch_mask_targets_shape[2],\n                                    batch_mask_targets_shape[3]])\n\n        # Use normalized proposals to crop mask targets from image masks.\n        flat_normalized_proposals = box_list_ops.to_normalized_coordinates(\n            box_list.BoxList(tf.reshape(proposal_boxes, [-1, 4])),\n            image_shape[1], image_shape[2]).get()\n\n        flat_cropped_gt_mask = tf.image.crop_and_resize(\n            tf.expand_dims(flat_gt_masks, -1),\n            flat_normalized_proposals,\n            tf.range(flat_normalized_proposals.shape[0].value),\n            [mask_height, mask_width])\n\n        batch_cropped_gt_mask = tf.reshape(\n            flat_cropped_gt_mask,\n            [batch_size, -1, mask_height * mask_width])\n\n        second_stage_mask_losses = ops.reduce_sum_trailing_dimensions(\n            self._second_stage_mask_loss(\n                reshaped_prediction_masks,\n                batch_cropped_gt_mask,\n                weights=batch_mask_target_weights),\n            ndims=2) / (\n                mask_height * mask_width * tf.maximum(\n                    tf.reduce_sum(\n                        batch_mask_target_weights, axis=1, keep_dims=True\n                    ), tf.ones((batch_size, 1))))\n        second_stage_mask_loss = tf.reduce_sum(\n            tf.boolean_mask(second_stage_mask_losses, paddings_indicator))\n\n      if second_stage_mask_loss is not None:\n        mask_loss = tf.multiply(self._second_stage_mask_loss_weight,\n                                second_stage_mask_loss, name=\'mask_loss\')\n        loss_dict[mask_loss.op.name] = mask_loss\n    return loss_dict\n\n  def _padded_batched_proposals_indicator(self,\n                                          num_proposals,\n                                          max_num_proposals):\n    """"""Creates indicator matrix of non-pad elements of padded batch proposals.\n\n    Args:\n      num_proposals: Tensor of type tf.int32 with shape [batch_size].\n      max_num_proposals: Maximum number of proposals per image (integer).\n\n    Returns:\n      A Tensor of type tf.bool with shape [batch_size, max_num_proposals].\n    """"""\n    batch_size = tf.size(num_proposals)\n    tiled_num_proposals = tf.tile(\n        tf.expand_dims(num_proposals, 1), [1, max_num_proposals])\n    tiled_proposal_index = tf.tile(\n        tf.expand_dims(tf.range(max_num_proposals), 0), [batch_size, 1])\n    return tf.greater(tiled_num_proposals, tiled_proposal_index)\n\n  def _unpad_proposals_and_apply_hard_mining(self,\n                                             proposal_boxlists,\n                                             second_stage_loc_losses,\n                                             second_stage_cls_losses,\n                                             num_proposals):\n    """"""Unpads proposals and applies hard mining.\n\n    Args:\n      proposal_boxlists: A list of `batch_size` BoxLists each representing\n        `self.max_num_proposals` representing decoded proposal bounding boxes\n        for each image.\n      second_stage_loc_losses: A Tensor of type `float32`. A tensor of shape\n        `[batch_size, self.max_num_proposals]` representing per-anchor\n        second stage localization loss values.\n      second_stage_cls_losses: A Tensor of type `float32`. A tensor of shape\n        `[batch_size, self.max_num_proposals]` representing per-anchor\n        second stage classification loss values.\n      num_proposals: A Tensor of type `int32`. A 1-D tensor of shape [batch]\n        representing the number of proposals predicted for each image in\n        the batch.\n\n    Returns:\n      second_stage_loc_loss: A scalar float32 tensor representing the second\n        stage localization loss.\n      second_stage_cls_loss: A scalar float32 tensor representing the second\n        stage classification loss.\n    """"""\n    for (proposal_boxlist, single_image_loc_loss, single_image_cls_loss,\n         single_image_num_proposals) in zip(\n             proposal_boxlists,\n             tf.unstack(second_stage_loc_losses),\n             tf.unstack(second_stage_cls_losses),\n             tf.unstack(num_proposals)):\n      proposal_boxlist = box_list.BoxList(\n          tf.slice(proposal_boxlist.get(),\n                   [0, 0], [single_image_num_proposals, -1]))\n      single_image_loc_loss = tf.slice(single_image_loc_loss,\n                                       [0], [single_image_num_proposals])\n      single_image_cls_loss = tf.slice(single_image_cls_loss,\n                                       [0], [single_image_num_proposals])\n      return self._hard_example_miner(\n          location_losses=tf.expand_dims(single_image_loc_loss, 0),\n          cls_losses=tf.expand_dims(single_image_cls_loss, 0),\n          decoded_boxlist_list=[proposal_boxlist])\n\n  def restore_map(self,\n                  fine_tune_checkpoint_type=\'detection\',\n                  load_all_detection_checkpoint_vars=False):\n    """"""Returns a map of variables to load from a foreign checkpoint.\n\n    See parent class for details.\n\n    Args:\n      fine_tune_checkpoint_type: whether to restore from a full detection\n        checkpoint (with compatible variable names) or to restore from a\n        classification checkpoint for initialization prior to training.\n        Valid values: `detection`, `classification`. Default \'detection\'.\n       load_all_detection_checkpoint_vars: whether to load all variables (when\n         `fine_tune_checkpoint_type` is `detection`). If False, only variables\n         within the feature extractor scopes are included. Default False.\n\n    Returns:\n      A dict mapping variable names (to load from a checkpoint) to variables in\n      the model graph.\n    Raises:\n      ValueError: if fine_tune_checkpoint_type is neither `classification`\n        nor `detection`.\n    """"""\n    if fine_tune_checkpoint_type not in [\'detection\', \'classification\']:\n      raise ValueError(\'Not supported fine_tune_checkpoint_type: {}\'.format(\n          fine_tune_checkpoint_type))\n    if fine_tune_checkpoint_type == \'classification\':\n      return self._feature_extractor.restore_from_classification_checkpoint_fn(\n          self.first_stage_feature_extractor_scope,\n          self.second_stage_feature_extractor_scope)\n\n    variables_to_restore = tf.global_variables()\n    variables_to_restore.append(slim.get_or_create_global_step())\n    # Only load feature extractor variables to be consistent with loading from\n    # a classification checkpoint.\n    include_patterns = None\n    if not load_all_detection_checkpoint_vars:\n      include_patterns = [\n          self.first_stage_feature_extractor_scope,\n          self.second_stage_feature_extractor_scope\n      ]\n    feature_extractor_variables = tf.contrib.framework.filter_variables(\n        variables_to_restore, include_patterns=include_patterns)\n    return {var.op.name: var for var in feature_extractor_variables}\n'"
src/object_detection/meta_architectures/faster_rcnn_meta_arch_test.py,38,"b'# Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n""""""Tests for object_detection.meta_architectures.faster_rcnn_meta_arch.""""""\n\nimport numpy as np\nimport tensorflow as tf\n\nfrom object_detection.meta_architectures import faster_rcnn_meta_arch_test_lib\n\n\nclass FasterRCNNMetaArchTest(\n    faster_rcnn_meta_arch_test_lib.FasterRCNNMetaArchTestBase):\n\n  def test_postprocess_second_stage_only_inference_mode_with_masks(self):\n    model = self._build_model(\n        is_training=False, number_of_stages=2, second_stage_batch_size=6)\n\n    batch_size = 2\n    total_num_padded_proposals = batch_size * model.max_num_proposals\n    proposal_boxes = tf.constant(\n        [[[1, 1, 2, 3],\n          [0, 0, 1, 1],\n          [.5, .5, .6, .6],\n          4*[0], 4*[0], 4*[0], 4*[0], 4*[0]],\n         [[2, 3, 6, 8],\n          [1, 2, 5, 3],\n          4*[0], 4*[0], 4*[0], 4*[0], 4*[0], 4*[0]]], dtype=tf.float32)\n    num_proposals = tf.constant([3, 2], dtype=tf.int32)\n    refined_box_encodings = tf.zeros(\n        [total_num_padded_proposals, model.num_classes, 4], dtype=tf.float32)\n    class_predictions_with_background = tf.ones(\n        [total_num_padded_proposals, model.num_classes+1], dtype=tf.float32)\n    image_shape = tf.constant([batch_size, 36, 48, 3], dtype=tf.int32)\n\n    mask_height = 2\n    mask_width = 2\n    mask_predictions = 30. * tf.ones(\n        [total_num_padded_proposals, model.num_classes,\n         mask_height, mask_width], dtype=tf.float32)\n    exp_detection_masks = np.array([[[[1, 1], [1, 1]],\n                                     [[1, 1], [1, 1]],\n                                     [[1, 1], [1, 1]],\n                                     [[1, 1], [1, 1]],\n                                     [[1, 1], [1, 1]]],\n                                    [[[1, 1], [1, 1]],\n                                     [[1, 1], [1, 1]],\n                                     [[1, 1], [1, 1]],\n                                     [[1, 1], [1, 1]],\n                                     [[0, 0], [0, 0]]]])\n\n    _, true_image_shapes = model.preprocess(tf.zeros(image_shape))\n    detections = model.postprocess({\n        \'refined_box_encodings\': refined_box_encodings,\n        \'class_predictions_with_background\': class_predictions_with_background,\n        \'num_proposals\': num_proposals,\n        \'proposal_boxes\': proposal_boxes,\n        \'image_shape\': image_shape,\n        \'mask_predictions\': mask_predictions\n    }, true_image_shapes)\n    with self.test_session() as sess:\n      detections_out = sess.run(detections)\n      self.assertAllEqual(detections_out[\'detection_boxes\'].shape, [2, 5, 4])\n      self.assertAllClose(detections_out[\'detection_scores\'],\n                          [[1, 1, 1, 1, 1], [1, 1, 1, 1, 0]])\n      self.assertAllClose(detections_out[\'detection_classes\'],\n                          [[0, 0, 0, 1, 1], [0, 0, 1, 1, 0]])\n      self.assertAllClose(detections_out[\'num_detections\'], [5, 4])\n      self.assertAllClose(detections_out[\'detection_masks\'],\n                          exp_detection_masks)\n      self.assertTrue(np.amax(detections_out[\'detection_masks\'] <= 1.0))\n      self.assertTrue(np.amin(detections_out[\'detection_masks\'] >= 0.0))\n\n  def test_predict_correct_shapes_in_inference_mode_three_stages_with_masks(\n      self):\n    batch_size = 2\n    image_size = 10\n    max_num_proposals = 8\n    initial_crop_size = 3\n    maxpool_stride = 1\n\n    input_shapes = [(batch_size, image_size, image_size, 3),\n                    (None, image_size, image_size, 3),\n                    (batch_size, None, None, 3),\n                    (None, None, None, 3)]\n    expected_num_anchors = image_size * image_size * 3 * 3\n    expected_shapes = {\n        \'rpn_box_predictor_features\':\n        (2, image_size, image_size, 512),\n        \'rpn_features_to_crop\': (2, image_size, image_size, 3),\n        \'image_shape\': (4,),\n        \'rpn_box_encodings\': (2, expected_num_anchors, 4),\n        \'rpn_objectness_predictions_with_background\':\n        (2, expected_num_anchors, 2),\n        \'anchors\': (expected_num_anchors, 4),\n        \'refined_box_encodings\': (2 * max_num_proposals, 2, 4),\n        \'class_predictions_with_background\': (2 * max_num_proposals, 2 + 1),\n        \'num_proposals\': (2,),\n        \'proposal_boxes\': (2, max_num_proposals, 4),\n        \'proposal_boxes_normalized\': (2, max_num_proposals, 4),\n        \'box_classifier_features\':\n        self._get_box_classifier_features_shape(image_size,\n                                                batch_size,\n                                                max_num_proposals,\n                                                initial_crop_size,\n                                                maxpool_stride,\n                                                3)\n    }\n\n    for input_shape in input_shapes:\n      test_graph = tf.Graph()\n      with test_graph.as_default():\n        model = self._build_model(\n            is_training=False,\n            number_of_stages=3,\n            second_stage_batch_size=2,\n            predict_masks=True)\n        preprocessed_inputs = tf.placeholder(tf.float32, shape=input_shape)\n        _, true_image_shapes = model.preprocess(preprocessed_inputs)\n        result_tensor_dict = model.predict(preprocessed_inputs,\n                                           true_image_shapes)\n        init_op = tf.global_variables_initializer()\n      with self.test_session(graph=test_graph) as sess:\n        sess.run(init_op)\n        tensor_dict_out = sess.run(result_tensor_dict, feed_dict={\n            preprocessed_inputs:\n            np.zeros((batch_size, image_size, image_size, 3))})\n      self.assertEqual(\n          set(tensor_dict_out.keys()),\n          set(expected_shapes.keys()).union(\n              set([\n                  \'detection_boxes\', \'detection_scores\', \'detection_classes\',\n                  \'detection_masks\', \'num_detections\'\n              ])))\n      for key in expected_shapes:\n        self.assertAllEqual(tensor_dict_out[key].shape, expected_shapes[key])\n      self.assertAllEqual(tensor_dict_out[\'detection_boxes\'].shape, [2, 5, 4])\n      self.assertAllEqual(tensor_dict_out[\'detection_masks\'].shape,\n                          [2, 5, 14, 14])\n      self.assertAllEqual(tensor_dict_out[\'detection_classes\'].shape, [2, 5])\n      self.assertAllEqual(tensor_dict_out[\'detection_scores\'].shape, [2, 5])\n      self.assertAllEqual(tensor_dict_out[\'num_detections\'].shape, [2])\n\n  def test_predict_gives_correct_shapes_in_train_mode_both_stages_with_masks(\n      self):\n    test_graph = tf.Graph()\n    with test_graph.as_default():\n      model = self._build_model(\n          is_training=True,\n          number_of_stages=2,\n          second_stage_batch_size=7,\n          predict_masks=True)\n\n      batch_size = 2\n      image_size = 10\n      max_num_proposals = 7\n      initial_crop_size = 3\n      maxpool_stride = 1\n\n      image_shape = (batch_size, image_size, image_size, 3)\n      preprocessed_inputs = tf.zeros(image_shape, dtype=tf.float32)\n      groundtruth_boxes_list = [\n          tf.constant([[0, 0, .5, .5], [.5, .5, 1, 1]], dtype=tf.float32),\n          tf.constant([[0, .5, .5, 1], [.5, 0, 1, .5]], dtype=tf.float32)\n      ]\n      groundtruth_classes_list = [\n          tf.constant([[1, 0], [0, 1]], dtype=tf.float32),\n          tf.constant([[1, 0], [1, 0]], dtype=tf.float32)\n      ]\n      _, true_image_shapes = model.preprocess(tf.zeros(image_shape))\n      model.provide_groundtruth(groundtruth_boxes_list,\n                                groundtruth_classes_list)\n\n      result_tensor_dict = model.predict(preprocessed_inputs, true_image_shapes)\n      expected_shapes = {\n          \'rpn_box_predictor_features\': (2, image_size, image_size, 512),\n          \'rpn_features_to_crop\': (2, image_size, image_size, 3),\n          \'image_shape\': (4,),\n          \'refined_box_encodings\': (2 * max_num_proposals, 2, 4),\n          \'class_predictions_with_background\': (2 * max_num_proposals, 2 + 1),\n          \'num_proposals\': (2,),\n          \'proposal_boxes\': (2, max_num_proposals, 4),\n          \'proposal_boxes_normalized\': (2, max_num_proposals, 4),\n          \'box_classifier_features\':\n              self._get_box_classifier_features_shape(\n                  image_size, batch_size, max_num_proposals, initial_crop_size,\n                  maxpool_stride, 3),\n          \'mask_predictions\': (2 * max_num_proposals, 2, 14, 14)\n      }\n\n      init_op = tf.global_variables_initializer()\n      with self.test_session(graph=test_graph) as sess:\n        sess.run(init_op)\n        tensor_dict_out = sess.run(result_tensor_dict)\n        self.assertEqual(\n            set(tensor_dict_out.keys()),\n            set(expected_shapes.keys()).union(\n                set([\n                    \'rpn_box_encodings\',\n                    \'rpn_objectness_predictions_with_background\',\n                    \'anchors\',\n                ])))\n        for key in expected_shapes:\n          self.assertAllEqual(tensor_dict_out[key].shape, expected_shapes[key])\n\n        anchors_shape_out = tensor_dict_out[\'anchors\'].shape\n        self.assertEqual(2, len(anchors_shape_out))\n        self.assertEqual(4, anchors_shape_out[1])\n        num_anchors_out = anchors_shape_out[0]\n        self.assertAllEqual(tensor_dict_out[\'rpn_box_encodings\'].shape,\n                            (2, num_anchors_out, 4))\n        self.assertAllEqual(\n            tensor_dict_out[\'rpn_objectness_predictions_with_background\'].shape,\n            (2, num_anchors_out, 2))\n\n  def test_postprocess_third_stage_only_inference_mode(self):\n    num_proposals_shapes = [(2), (None)]\n    refined_box_encodings_shapes = [(16, 2, 4), (None, 2, 4)]\n    class_predictions_with_background_shapes = [(16, 3), (None, 3)]\n    proposal_boxes_shapes = [(2, 8, 4), (None, 8, 4)]\n    batch_size = 2\n    image_shape = np.array((2, 36, 48, 3), dtype=np.int32)\n    for (num_proposals_shape, refined_box_encoding_shape,\n         class_predictions_with_background_shape,\n         proposal_boxes_shape) in zip(num_proposals_shapes,\n                                      refined_box_encodings_shapes,\n                                      class_predictions_with_background_shapes,\n                                      proposal_boxes_shapes):\n      tf_graph = tf.Graph()\n      with tf_graph.as_default():\n        model = self._build_model(\n            is_training=False, number_of_stages=3,\n            second_stage_batch_size=6, predict_masks=True)\n        total_num_padded_proposals = batch_size * model.max_num_proposals\n        proposal_boxes = np.array(\n            [[[1, 1, 2, 3],\n              [0, 0, 1, 1],\n              [.5, .5, .6, .6],\n              4*[0], 4*[0], 4*[0], 4*[0], 4*[0]],\n             [[2, 3, 6, 8],\n              [1, 2, 5, 3],\n              4*[0], 4*[0], 4*[0], 4*[0], 4*[0], 4*[0]]])\n        num_proposals = np.array([3, 2], dtype=np.int32)\n        refined_box_encodings = np.zeros(\n            [total_num_padded_proposals, model.num_classes, 4])\n        class_predictions_with_background = np.ones(\n            [total_num_padded_proposals, model.num_classes+1])\n\n        num_proposals_placeholder = tf.placeholder(tf.int32,\n                                                   shape=num_proposals_shape)\n        refined_box_encodings_placeholder = tf.placeholder(\n            tf.float32, shape=refined_box_encoding_shape)\n        class_predictions_with_background_placeholder = tf.placeholder(\n            tf.float32, shape=class_predictions_with_background_shape)\n        proposal_boxes_placeholder = tf.placeholder(\n            tf.float32, shape=proposal_boxes_shape)\n        image_shape_placeholder = tf.placeholder(tf.int32, shape=(4))\n        _, true_image_shapes = model.preprocess(\n            tf.zeros(image_shape_placeholder))\n        detections = model.postprocess({\n            \'refined_box_encodings\': refined_box_encodings_placeholder,\n            \'class_predictions_with_background\':\n            class_predictions_with_background_placeholder,\n            \'num_proposals\': num_proposals_placeholder,\n            \'proposal_boxes\': proposal_boxes_placeholder,\n            \'image_shape\': image_shape_placeholder,\n            \'detection_boxes\': tf.zeros([2, 5, 4]),\n            \'detection_masks\': tf.zeros([2, 5, 14, 14]),\n            \'detection_scores\': tf.zeros([2, 5]),\n            \'detection_classes\': tf.zeros([2, 5]),\n            \'num_detections\': tf.zeros([2]),\n        }, true_image_shapes)\n      with self.test_session(graph=tf_graph) as sess:\n        detections_out = sess.run(\n            detections,\n            feed_dict={\n                refined_box_encodings_placeholder: refined_box_encodings,\n                class_predictions_with_background_placeholder:\n                class_predictions_with_background,\n                num_proposals_placeholder: num_proposals,\n                proposal_boxes_placeholder: proposal_boxes,\n                image_shape_placeholder: image_shape\n            })\n      self.assertAllEqual(detections_out[\'detection_boxes\'].shape, [2, 5, 4])\n      self.assertAllEqual(detections_out[\'detection_masks\'].shape,\n                          [2, 5, 14, 14])\n      self.assertAllClose(detections_out[\'detection_scores\'].shape, [2, 5])\n      self.assertAllClose(detections_out[\'detection_classes\'].shape, [2, 5])\n      self.assertAllClose(detections_out[\'num_detections\'].shape, [2])\n      self.assertTrue(np.amax(detections_out[\'detection_masks\'] <= 1.0))\n      self.assertTrue(np.amin(detections_out[\'detection_masks\'] >= 0.0))\n\n  def _get_box_classifier_features_shape(self,\n                                         image_size,\n                                         batch_size,\n                                         max_num_proposals,\n                                         initial_crop_size,\n                                         maxpool_stride,\n                                         num_features):\n    return (batch_size * max_num_proposals,\n            initial_crop_size/maxpool_stride,\n            initial_crop_size/maxpool_stride,\n            num_features)\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
src/object_detection/meta_architectures/faster_rcnn_meta_arch_test_lib.py,209,"b'# Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n""""""Tests for object_detection.meta_architectures.faster_rcnn_meta_arch.""""""\nimport numpy as np\nimport tensorflow as tf\nfrom google.protobuf import text_format\nfrom object_detection.anchor_generators import grid_anchor_generator\nfrom object_detection.builders import box_predictor_builder\nfrom object_detection.builders import hyperparams_builder\nfrom object_detection.builders import post_processing_builder\nfrom object_detection.core import losses\nfrom object_detection.meta_architectures import faster_rcnn_meta_arch\nfrom object_detection.protos import box_predictor_pb2\nfrom object_detection.protos import hyperparams_pb2\nfrom object_detection.protos import post_processing_pb2\n\nslim = tf.contrib.slim\nBOX_CODE_SIZE = 4\n\n\nclass FakeFasterRCNNFeatureExtractor(\n    faster_rcnn_meta_arch.FasterRCNNFeatureExtractor):\n  """"""Fake feature extracture to use in tests.""""""\n\n  def __init__(self):\n    super(FakeFasterRCNNFeatureExtractor, self).__init__(\n        is_training=False,\n        first_stage_features_stride=32,\n        reuse_weights=None,\n        weight_decay=0.0)\n\n  def preprocess(self, resized_inputs):\n    return tf.identity(resized_inputs)\n\n  def _extract_proposal_features(self, preprocessed_inputs, scope):\n    with tf.variable_scope(\'mock_model\'):\n      proposal_features = 0 * slim.conv2d(\n          preprocessed_inputs, num_outputs=3, kernel_size=1, scope=\'layer1\')\n      return proposal_features, {}\n\n  def _extract_box_classifier_features(self, proposal_feature_maps, scope):\n    with tf.variable_scope(\'mock_model\'):\n      return 0 * slim.conv2d(proposal_feature_maps,\n                             num_outputs=3, kernel_size=1, scope=\'layer2\')\n\n\nclass FasterRCNNMetaArchTestBase(tf.test.TestCase):\n  """"""Base class to test Faster R-CNN and R-FCN meta architectures.""""""\n\n  def _build_arg_scope_with_hyperparams(self,\n                                        hyperparams_text_proto,\n                                        is_training):\n    hyperparams = hyperparams_pb2.Hyperparams()\n    text_format.Merge(hyperparams_text_proto, hyperparams)\n    return hyperparams_builder.build(hyperparams, is_training=is_training)\n\n  def _get_second_stage_box_predictor_text_proto(self):\n    box_predictor_text_proto = """"""\n      mask_rcnn_box_predictor {\n        fc_hyperparams {\n          op: FC\n          activation: NONE\n          regularizer {\n            l2_regularizer {\n              weight: 0.0005\n            }\n          }\n          initializer {\n            variance_scaling_initializer {\n              factor: 1.0\n              uniform: true\n              mode: FAN_AVG\n            }\n          }\n        }\n      }\n    """"""\n    return box_predictor_text_proto\n\n  def _add_mask_to_second_stage_box_predictor_text_proto(self):\n    box_predictor_text_proto = """"""\n      mask_rcnn_box_predictor {\n        predict_instance_masks: true\n        mask_height: 14\n        mask_width: 14\n        conv_hyperparams {\n          op: CONV\n          regularizer {\n            l2_regularizer {\n              weight: 0.0\n            }\n          }\n          initializer {\n            truncated_normal_initializer {\n              stddev: 0.01\n            }\n          }\n        }\n      }\n    """"""\n    return box_predictor_text_proto\n\n  def _get_second_stage_box_predictor(self, num_classes, is_training,\n                                      predict_masks):\n    box_predictor_proto = box_predictor_pb2.BoxPredictor()\n    text_format.Merge(self._get_second_stage_box_predictor_text_proto(),\n                      box_predictor_proto)\n    if predict_masks:\n      text_format.Merge(\n          self._add_mask_to_second_stage_box_predictor_text_proto(),\n          box_predictor_proto)\n\n    return box_predictor_builder.build(\n        hyperparams_builder.build,\n        box_predictor_proto,\n        num_classes=num_classes,\n        is_training=is_training)\n\n  def _get_model(self, box_predictor, **common_kwargs):\n    return faster_rcnn_meta_arch.FasterRCNNMetaArch(\n        initial_crop_size=3,\n        maxpool_kernel_size=1,\n        maxpool_stride=1,\n        second_stage_mask_rcnn_box_predictor=box_predictor,\n        **common_kwargs)\n\n  def _build_model(self,\n                   is_training,\n                   number_of_stages,\n                   second_stage_batch_size,\n                   first_stage_max_proposals=8,\n                   num_classes=2,\n                   hard_mining=False,\n                   softmax_second_stage_classification_loss=True,\n                   predict_masks=False,\n                   pad_to_max_dimension=None):\n\n    def image_resizer_fn(image, masks=None):\n      """"""Fake image resizer function.""""""\n      resized_inputs = []\n      resized_image = tf.identity(image)\n      if pad_to_max_dimension is not None:\n        resized_image = tf.image.pad_to_bounding_box(image, 0, 0,\n                                                     pad_to_max_dimension,\n                                                     pad_to_max_dimension)\n      resized_inputs.append(resized_image)\n      if masks is not None:\n        resized_masks = tf.identity(masks)\n        if pad_to_max_dimension is not None:\n          resized_masks = tf.image.pad_to_bounding_box(tf.transpose(masks,\n                                                                    [1, 2, 0]),\n                                                       0, 0,\n                                                       pad_to_max_dimension,\n                                                       pad_to_max_dimension)\n          resized_masks = tf.transpose(resized_masks, [2, 0, 1])\n        resized_inputs.append(resized_masks)\n      resized_inputs.append(tf.shape(image))\n      return resized_inputs\n\n    # anchors in this test are designed so that a subset of anchors are inside\n    # the image and a subset of anchors are outside.\n    first_stage_anchor_scales = (0.001, 0.005, 0.1)\n    first_stage_anchor_aspect_ratios = (0.5, 1.0, 2.0)\n    first_stage_anchor_strides = (1, 1)\n    first_stage_anchor_generator = grid_anchor_generator.GridAnchorGenerator(\n        first_stage_anchor_scales,\n        first_stage_anchor_aspect_ratios,\n        anchor_stride=first_stage_anchor_strides)\n\n    fake_feature_extractor = FakeFasterRCNNFeatureExtractor()\n\n    first_stage_box_predictor_hyperparams_text_proto = """"""\n      op: CONV\n      activation: RELU\n      regularizer {\n        l2_regularizer {\n          weight: 0.00004\n        }\n      }\n      initializer {\n        truncated_normal_initializer {\n          stddev: 0.03\n        }\n      }\n    """"""\n    first_stage_box_predictor_arg_scope = (\n        self._build_arg_scope_with_hyperparams(\n            first_stage_box_predictor_hyperparams_text_proto, is_training))\n\n    first_stage_box_predictor_kernel_size = 3\n    first_stage_atrous_rate = 1\n    first_stage_box_predictor_depth = 512\n    first_stage_minibatch_size = 3\n    first_stage_positive_balance_fraction = .5\n\n    first_stage_nms_score_threshold = -1.0\n    first_stage_nms_iou_threshold = 1.0\n    first_stage_max_proposals = first_stage_max_proposals\n\n    first_stage_localization_loss_weight = 1.0\n    first_stage_objectness_loss_weight = 1.0\n\n    post_processing_text_proto = """"""\n      batch_non_max_suppression {\n        score_threshold: -20.0\n        iou_threshold: 1.0\n        max_detections_per_class: 5\n        max_total_detections: 5\n      }\n    """"""\n    post_processing_config = post_processing_pb2.PostProcessing()\n    text_format.Merge(post_processing_text_proto, post_processing_config)\n    second_stage_non_max_suppression_fn, _ = post_processing_builder.build(\n        post_processing_config)\n    second_stage_balance_fraction = 1.0\n\n    second_stage_score_conversion_fn = tf.identity\n    second_stage_localization_loss_weight = 1.0\n    second_stage_classification_loss_weight = 1.0\n    if softmax_second_stage_classification_loss:\n      second_stage_classification_loss = (\n          losses.WeightedSoftmaxClassificationLoss())\n    else:\n      second_stage_classification_loss = (\n          losses.WeightedSigmoidClassificationLoss())\n\n    hard_example_miner = None\n    if hard_mining:\n      hard_example_miner = losses.HardExampleMiner(\n          num_hard_examples=1,\n          iou_threshold=0.99,\n          loss_type=\'both\',\n          cls_loss_weight=second_stage_classification_loss_weight,\n          loc_loss_weight=second_stage_localization_loss_weight,\n          max_negatives_per_positive=None)\n\n    common_kwargs = {\n        \'is_training\': is_training,\n        \'num_classes\': num_classes,\n        \'image_resizer_fn\': image_resizer_fn,\n        \'feature_extractor\': fake_feature_extractor,\n        \'number_of_stages\': number_of_stages,\n        \'first_stage_anchor_generator\': first_stage_anchor_generator,\n        \'first_stage_atrous_rate\': first_stage_atrous_rate,\n        \'first_stage_box_predictor_arg_scope\':\n        first_stage_box_predictor_arg_scope,\n        \'first_stage_box_predictor_kernel_size\':\n        first_stage_box_predictor_kernel_size,\n        \'first_stage_box_predictor_depth\': first_stage_box_predictor_depth,\n        \'first_stage_minibatch_size\': first_stage_minibatch_size,\n        \'first_stage_positive_balance_fraction\':\n        first_stage_positive_balance_fraction,\n        \'first_stage_nms_score_threshold\': first_stage_nms_score_threshold,\n        \'first_stage_nms_iou_threshold\': first_stage_nms_iou_threshold,\n        \'first_stage_max_proposals\': first_stage_max_proposals,\n        \'first_stage_localization_loss_weight\':\n        first_stage_localization_loss_weight,\n        \'first_stage_objectness_loss_weight\':\n        first_stage_objectness_loss_weight,\n        \'second_stage_batch_size\': second_stage_batch_size,\n        \'second_stage_balance_fraction\': second_stage_balance_fraction,\n        \'second_stage_non_max_suppression_fn\':\n        second_stage_non_max_suppression_fn,\n        \'second_stage_score_conversion_fn\': second_stage_score_conversion_fn,\n        \'second_stage_localization_loss_weight\':\n        second_stage_localization_loss_weight,\n        \'second_stage_classification_loss_weight\':\n        second_stage_classification_loss_weight,\n        \'second_stage_classification_loss\':\n        second_stage_classification_loss,\n        \'hard_example_miner\': hard_example_miner}\n\n    return self._get_model(\n        self._get_second_stage_box_predictor(\n            num_classes=num_classes,\n            is_training=is_training,\n            predict_masks=predict_masks), **common_kwargs)\n\n  def test_predict_gives_correct_shapes_in_inference_mode_first_stage_only(\n      self):\n    test_graph = tf.Graph()\n    with test_graph.as_default():\n      model = self._build_model(\n          is_training=False, number_of_stages=1, second_stage_batch_size=2)\n      batch_size = 2\n      height = 10\n      width = 12\n      input_image_shape = (batch_size, height, width, 3)\n\n      _, true_image_shapes = model.preprocess(tf.zeros(input_image_shape))\n      preprocessed_inputs = tf.placeholder(\n          dtype=tf.float32, shape=(batch_size, None, None, 3))\n      prediction_dict = model.predict(preprocessed_inputs, true_image_shapes)\n\n      # In inference mode, anchors are clipped to the image window, but not\n      # pruned.  Since MockFasterRCNN.extract_proposal_features returns a\n      # tensor with the same shape as its input, the expected number of anchors\n      # is height * width * the number of anchors per location (i.e. 3x3).\n      expected_num_anchors = height * width * 3 * 3\n      expected_output_keys = set([\n          \'rpn_box_predictor_features\', \'rpn_features_to_crop\', \'image_shape\',\n          \'rpn_box_encodings\', \'rpn_objectness_predictions_with_background\',\n          \'anchors\'])\n      expected_output_shapes = {\n          \'rpn_box_predictor_features\': (batch_size, height, width, 512),\n          \'rpn_features_to_crop\': (batch_size, height, width, 3),\n          \'rpn_box_encodings\': (batch_size, expected_num_anchors, 4),\n          \'rpn_objectness_predictions_with_background\':\n          (batch_size, expected_num_anchors, 2),\n          \'anchors\': (expected_num_anchors, 4)\n      }\n\n      init_op = tf.global_variables_initializer()\n      with self.test_session(graph=test_graph) as sess:\n        sess.run(init_op)\n        prediction_out = sess.run(prediction_dict,\n                                  feed_dict={\n                                      preprocessed_inputs:\n                                      np.zeros(input_image_shape)\n                                  })\n\n        self.assertEqual(set(prediction_out.keys()), expected_output_keys)\n\n        self.assertAllEqual(prediction_out[\'image_shape\'], input_image_shape)\n        for output_key, expected_shape in expected_output_shapes.items():\n          self.assertAllEqual(prediction_out[output_key].shape, expected_shape)\n\n        # Check that anchors are clipped to window.\n        anchors = prediction_out[\'anchors\']\n        self.assertTrue(np.all(np.greater_equal(anchors, 0)))\n        self.assertTrue(np.all(np.less_equal(anchors[:, 0], height)))\n        self.assertTrue(np.all(np.less_equal(anchors[:, 1], width)))\n        self.assertTrue(np.all(np.less_equal(anchors[:, 2], height)))\n        self.assertTrue(np.all(np.less_equal(anchors[:, 3], width)))\n\n  def test_predict_gives_valid_anchors_in_training_mode_first_stage_only(self):\n    test_graph = tf.Graph()\n    with test_graph.as_default():\n      model = self._build_model(\n          is_training=True, number_of_stages=1, second_stage_batch_size=2)\n      batch_size = 2\n      height = 10\n      width = 12\n      input_image_shape = (batch_size, height, width, 3)\n      _, true_image_shapes = model.preprocess(tf.zeros(input_image_shape))\n      preprocessed_inputs = tf.placeholder(\n          dtype=tf.float32, shape=(batch_size, None, None, 3))\n      prediction_dict = model.predict(preprocessed_inputs, true_image_shapes)\n\n      expected_output_keys = set([\n          \'rpn_box_predictor_features\', \'rpn_features_to_crop\', \'image_shape\',\n          \'rpn_box_encodings\', \'rpn_objectness_predictions_with_background\',\n          \'anchors\'])\n      # At training time, anchors that exceed image bounds are pruned.  Thus\n      # the `expected_num_anchors` in the above inference mode test is now\n      # a strict upper bound on the number of anchors.\n      num_anchors_strict_upper_bound = height * width * 3 * 3\n\n      init_op = tf.global_variables_initializer()\n      with self.test_session(graph=test_graph) as sess:\n        sess.run(init_op)\n        prediction_out = sess.run(prediction_dict,\n                                  feed_dict={\n                                      preprocessed_inputs:\n                                      np.zeros(input_image_shape)\n                                  })\n\n        self.assertEqual(set(prediction_out.keys()), expected_output_keys)\n        self.assertAllEqual(prediction_out[\'image_shape\'], input_image_shape)\n\n        # Check that anchors have less than the upper bound and\n        # are clipped to window.\n        anchors = prediction_out[\'anchors\']\n        self.assertTrue(len(anchors.shape) == 2 and anchors.shape[1] == 4)\n        num_anchors_out = anchors.shape[0]\n        self.assertTrue(num_anchors_out < num_anchors_strict_upper_bound)\n\n        self.assertTrue(np.all(np.greater_equal(anchors, 0)))\n        self.assertTrue(np.all(np.less_equal(anchors[:, 0], height)))\n        self.assertTrue(np.all(np.less_equal(anchors[:, 1], width)))\n        self.assertTrue(np.all(np.less_equal(anchors[:, 2], height)))\n        self.assertTrue(np.all(np.less_equal(anchors[:, 3], width)))\n\n        self.assertAllEqual(prediction_out[\'rpn_box_encodings\'].shape,\n                            (batch_size, num_anchors_out, 4))\n        self.assertAllEqual(\n            prediction_out[\'rpn_objectness_predictions_with_background\'].shape,\n            (batch_size, num_anchors_out, 2))\n\n  def test_predict_correct_shapes_in_inference_mode_two_stages(self):\n    batch_size = 2\n    image_size = 10\n    max_num_proposals = 8\n    initial_crop_size = 3\n    maxpool_stride = 1\n\n    input_shapes = [(batch_size, image_size, image_size, 3),\n                    (None, image_size, image_size, 3),\n                    (batch_size, None, None, 3),\n                    (None, None, None, 3)]\n    expected_num_anchors = image_size * image_size * 3 * 3\n    expected_shapes = {\n        \'rpn_box_predictor_features\':\n        (2, image_size, image_size, 512),\n        \'rpn_features_to_crop\': (2, image_size, image_size, 3),\n        \'image_shape\': (4,),\n        \'rpn_box_encodings\': (2, expected_num_anchors, 4),\n        \'rpn_objectness_predictions_with_background\':\n        (2, expected_num_anchors, 2),\n        \'anchors\': (expected_num_anchors, 4),\n        \'refined_box_encodings\': (2 * max_num_proposals, 2, 4),\n        \'class_predictions_with_background\': (2 * max_num_proposals, 2 + 1),\n        \'num_proposals\': (2,),\n        \'proposal_boxes\': (2, max_num_proposals, 4),\n        \'proposal_boxes_normalized\': (2, max_num_proposals, 4),\n        \'box_classifier_features\':\n        self._get_box_classifier_features_shape(image_size,\n                                                batch_size,\n                                                max_num_proposals,\n                                                initial_crop_size,\n                                                maxpool_stride,\n                                                3)\n    }\n\n    for input_shape in input_shapes:\n      test_graph = tf.Graph()\n      with test_graph.as_default():\n        model = self._build_model(\n            is_training=False,\n            number_of_stages=2,\n            second_stage_batch_size=2,\n            predict_masks=False)\n        preprocessed_inputs = tf.placeholder(tf.float32, shape=input_shape)\n        _, true_image_shapes = model.preprocess(preprocessed_inputs)\n        result_tensor_dict = model.predict(\n            preprocessed_inputs, true_image_shapes)\n        init_op = tf.global_variables_initializer()\n      with self.test_session(graph=test_graph) as sess:\n        sess.run(init_op)\n        tensor_dict_out = sess.run(result_tensor_dict, feed_dict={\n            preprocessed_inputs:\n            np.zeros((batch_size, image_size, image_size, 3))})\n      self.assertEqual(set(tensor_dict_out.keys()),\n                       set(expected_shapes.keys()))\n      for key in expected_shapes:\n        self.assertAllEqual(tensor_dict_out[key].shape, expected_shapes[key])\n\n  def test_predict_gives_correct_shapes_in_train_mode_both_stages(self):\n    test_graph = tf.Graph()\n    with test_graph.as_default():\n      model = self._build_model(\n          is_training=True,\n          number_of_stages=2,\n          second_stage_batch_size=7,\n          predict_masks=False)\n\n      batch_size = 2\n      image_size = 10\n      max_num_proposals = 7\n      initial_crop_size = 3\n      maxpool_stride = 1\n\n      image_shape = (batch_size, image_size, image_size, 3)\n      preprocessed_inputs = tf.zeros(image_shape, dtype=tf.float32)\n      groundtruth_boxes_list = [\n          tf.constant([[0, 0, .5, .5], [.5, .5, 1, 1]], dtype=tf.float32),\n          tf.constant([[0, .5, .5, 1], [.5, 0, 1, .5]], dtype=tf.float32)]\n      groundtruth_classes_list = [\n          tf.constant([[1, 0], [0, 1]], dtype=tf.float32),\n          tf.constant([[1, 0], [1, 0]], dtype=tf.float32)]\n\n      _, true_image_shapes = model.preprocess(tf.zeros(image_shape))\n      model.provide_groundtruth(groundtruth_boxes_list,\n                                groundtruth_classes_list)\n\n      result_tensor_dict = model.predict(preprocessed_inputs, true_image_shapes)\n      expected_shapes = {\n          \'rpn_box_predictor_features\':\n          (2, image_size, image_size, 512),\n          \'rpn_features_to_crop\': (2, image_size, image_size, 3),\n          \'image_shape\': (4,),\n          \'refined_box_encodings\': (2 * max_num_proposals, 2, 4),\n          \'class_predictions_with_background\': (2 * max_num_proposals, 2 + 1),\n          \'num_proposals\': (2,),\n          \'proposal_boxes\': (2, max_num_proposals, 4),\n          \'proposal_boxes_normalized\': (2, max_num_proposals, 4),\n          \'box_classifier_features\':\n          self._get_box_classifier_features_shape(image_size,\n                                                  batch_size,\n                                                  max_num_proposals,\n                                                  initial_crop_size,\n                                                  maxpool_stride,\n                                                  3)\n      }\n\n      init_op = tf.global_variables_initializer()\n      with self.test_session(graph=test_graph) as sess:\n        sess.run(init_op)\n        tensor_dict_out = sess.run(result_tensor_dict)\n        self.assertEqual(set(tensor_dict_out.keys()),\n                         set(expected_shapes.keys()).union(set([\n                             \'rpn_box_encodings\',\n                             \'rpn_objectness_predictions_with_background\',\n                             \'anchors\'])))\n        for key in expected_shapes:\n          self.assertAllEqual(tensor_dict_out[key].shape, expected_shapes[key])\n\n        anchors_shape_out = tensor_dict_out[\'anchors\'].shape\n        self.assertEqual(2, len(anchors_shape_out))\n        self.assertEqual(4, anchors_shape_out[1])\n        num_anchors_out = anchors_shape_out[0]\n        self.assertAllEqual(tensor_dict_out[\'rpn_box_encodings\'].shape,\n                            (2, num_anchors_out, 4))\n        self.assertAllEqual(\n            tensor_dict_out[\'rpn_objectness_predictions_with_background\'].shape,\n            (2, num_anchors_out, 2))\n\n  def _test_postprocess_first_stage_only_inference_mode(\n      self, pad_to_max_dimension=None):\n    model = self._build_model(\n        is_training=False, number_of_stages=1, second_stage_batch_size=6,\n        pad_to_max_dimension=pad_to_max_dimension)\n    batch_size = 2\n    anchors = tf.constant(\n        [[0, 0, 16, 16],\n         [0, 16, 16, 32],\n         [16, 0, 32, 16],\n         [16, 16, 32, 32]], dtype=tf.float32)\n    rpn_box_encodings = tf.zeros(\n        [batch_size, anchors.get_shape().as_list()[0],\n         BOX_CODE_SIZE], dtype=tf.float32)\n    # use different numbers for the objectness category to break ties in\n    # order of boxes returned by NMS\n    rpn_objectness_predictions_with_background = tf.constant([\n        [[-10, 13],\n         [10, -10],\n         [10, -11],\n         [-10, 12]],\n        [[10, -10],\n         [-10, 13],\n         [-10, 12],\n         [10, -11]]], dtype=tf.float32)\n    rpn_features_to_crop = tf.ones((batch_size, 8, 8, 10), dtype=tf.float32)\n    image_shape = tf.constant([batch_size, 32, 32, 3], dtype=tf.int32)\n    _, true_image_shapes = model.preprocess(tf.zeros(image_shape))\n    proposals = model.postprocess({\n        \'rpn_box_encodings\': rpn_box_encodings,\n        \'rpn_objectness_predictions_with_background\':\n        rpn_objectness_predictions_with_background,\n        \'rpn_features_to_crop\': rpn_features_to_crop,\n        \'anchors\': anchors}, true_image_shapes)\n    expected_proposal_boxes = [\n        [[0, 0, .5, .5], [.5, .5, 1, 1], [0, .5, .5, 1], [.5, 0, 1.0, .5]]\n        + 4 * [4 * [0]],\n        [[0, .5, .5, 1], [.5, 0, 1.0, .5], [0, 0, .5, .5], [.5, .5, 1, 1]]\n        + 4 * [4 * [0]]]\n    expected_proposal_scores = [[1, 1, 0, 0, 0, 0, 0, 0],\n                                [1, 1, 0, 0, 0, 0, 0, 0]]\n    expected_num_proposals = [4, 4]\n\n    expected_output_keys = set([\'detection_boxes\', \'detection_scores\',\n                                \'num_detections\'])\n    self.assertEqual(set(proposals.keys()), expected_output_keys)\n    with self.test_session() as sess:\n      proposals_out = sess.run(proposals)\n      self.assertAllClose(proposals_out[\'detection_boxes\'],\n                          expected_proposal_boxes)\n      self.assertAllClose(proposals_out[\'detection_scores\'],\n                          expected_proposal_scores)\n      self.assertAllEqual(proposals_out[\'num_detections\'],\n                          expected_num_proposals)\n\n  def test_postprocess_first_stage_only_inference_mode(self):\n    self._test_postprocess_first_stage_only_inference_mode()\n\n  def test_postprocess_first_stage_only_inference_mode_padded_image(self):\n    self._test_postprocess_first_stage_only_inference_mode(\n        pad_to_max_dimension=56)\n\n  def _test_postprocess_first_stage_only_train_mode(self,\n                                                    pad_to_max_dimension=None):\n    model = self._build_model(\n        is_training=True, number_of_stages=1, second_stage_batch_size=2,\n        pad_to_max_dimension=pad_to_max_dimension)\n    batch_size = 2\n    anchors = tf.constant(\n        [[0, 0, 16, 16],\n         [0, 16, 16, 32],\n         [16, 0, 32, 16],\n         [16, 16, 32, 32]], dtype=tf.float32)\n    rpn_box_encodings = tf.zeros(\n        [batch_size, anchors.get_shape().as_list()[0],\n         BOX_CODE_SIZE], dtype=tf.float32)\n    # use different numbers for the objectness category to break ties in\n    # order of boxes returned by NMS\n    rpn_objectness_predictions_with_background = tf.constant([\n        [[-10, 13],\n         [-10, 12],\n         [-10, 11],\n         [-10, 10]],\n        [[-10, 13],\n         [-10, 12],\n         [-10, 11],\n         [-10, 10]]], dtype=tf.float32)\n    rpn_features_to_crop = tf.ones((batch_size, 8, 8, 10), dtype=tf.float32)\n    image_shape = tf.constant([batch_size, 32, 32, 3], dtype=tf.int32)\n    groundtruth_boxes_list = [\n        tf.constant([[0, 0, .5, .5], [.5, .5, 1, 1]], dtype=tf.float32),\n        tf.constant([[0, .5, .5, 1], [.5, 0, 1, .5]], dtype=tf.float32)]\n    groundtruth_classes_list = [tf.constant([[1, 0], [0, 1]], dtype=tf.float32),\n                                tf.constant([[1, 0], [1, 0]], dtype=tf.float32)]\n\n    _, true_image_shapes = model.preprocess(tf.zeros(image_shape))\n    model.provide_groundtruth(groundtruth_boxes_list,\n                              groundtruth_classes_list)\n    proposals = model.postprocess({\n        \'rpn_box_encodings\': rpn_box_encodings,\n        \'rpn_objectness_predictions_with_background\':\n        rpn_objectness_predictions_with_background,\n        \'rpn_features_to_crop\': rpn_features_to_crop,\n        \'anchors\': anchors}, true_image_shapes)\n    expected_proposal_boxes = [\n        [[0, 0, .5, .5], [.5, .5, 1, 1]], [[0, .5, .5, 1], [.5, 0, 1, .5]]]\n    expected_proposal_scores = [[1, 1],\n                                [1, 1]]\n    expected_num_proposals = [2, 2]\n\n    expected_output_keys = set([\'detection_boxes\', \'detection_scores\',\n                                \'num_detections\'])\n    self.assertEqual(set(proposals.keys()), expected_output_keys)\n\n    with self.test_session() as sess:\n      proposals_out = sess.run(proposals)\n      self.assertAllClose(proposals_out[\'detection_boxes\'],\n                          expected_proposal_boxes)\n      self.assertAllClose(proposals_out[\'detection_scores\'],\n                          expected_proposal_scores)\n      self.assertAllEqual(proposals_out[\'num_detections\'],\n                          expected_num_proposals)\n\n  def test_postprocess_first_stage_only_train_mode(self):\n    self._test_postprocess_first_stage_only_train_mode()\n\n  def test_postprocess_first_stage_only_train_mode_padded_image(self):\n    self._test_postprocess_first_stage_only_train_mode(pad_to_max_dimension=56)\n\n  def _test_postprocess_second_stage_only_inference_mode(\n      self, pad_to_max_dimension=None):\n    num_proposals_shapes = [(2), (None,)]\n    refined_box_encodings_shapes = [(16, 2, 4), (None, 2, 4)]\n    class_predictions_with_background_shapes = [(16, 3), (None, 3)]\n    proposal_boxes_shapes = [(2, 8, 4), (None, 8, 4)]\n    batch_size = 2\n    image_shape = np.array((2, 36, 48, 3), dtype=np.int32)\n    for (num_proposals_shape, refined_box_encoding_shape,\n         class_predictions_with_background_shape,\n         proposal_boxes_shape) in zip(num_proposals_shapes,\n                                      refined_box_encodings_shapes,\n                                      class_predictions_with_background_shapes,\n                                      proposal_boxes_shapes):\n      tf_graph = tf.Graph()\n      with tf_graph.as_default():\n        model = self._build_model(\n            is_training=False, number_of_stages=2,\n            second_stage_batch_size=6,\n            pad_to_max_dimension=pad_to_max_dimension)\n        _, true_image_shapes = model.preprocess(tf.zeros(image_shape))\n        total_num_padded_proposals = batch_size * model.max_num_proposals\n        proposal_boxes = np.array(\n            [[[1, 1, 2, 3],\n              [0, 0, 1, 1],\n              [.5, .5, .6, .6],\n              4*[0], 4*[0], 4*[0], 4*[0], 4*[0]],\n             [[2, 3, 6, 8],\n              [1, 2, 5, 3],\n              4*[0], 4*[0], 4*[0], 4*[0], 4*[0], 4*[0]]])\n        num_proposals = np.array([3, 2], dtype=np.int32)\n        refined_box_encodings = np.zeros(\n            [total_num_padded_proposals, model.num_classes, 4])\n        class_predictions_with_background = np.ones(\n            [total_num_padded_proposals, model.num_classes+1])\n\n        num_proposals_placeholder = tf.placeholder(tf.int32,\n                                                   shape=num_proposals_shape)\n        refined_box_encodings_placeholder = tf.placeholder(\n            tf.float32, shape=refined_box_encoding_shape)\n        class_predictions_with_background_placeholder = tf.placeholder(\n            tf.float32, shape=class_predictions_with_background_shape)\n        proposal_boxes_placeholder = tf.placeholder(\n            tf.float32, shape=proposal_boxes_shape)\n        image_shape_placeholder = tf.placeholder(tf.int32, shape=(4))\n\n        detections = model.postprocess({\n            \'refined_box_encodings\': refined_box_encodings_placeholder,\n            \'class_predictions_with_background\':\n            class_predictions_with_background_placeholder,\n            \'num_proposals\': num_proposals_placeholder,\n            \'proposal_boxes\': proposal_boxes_placeholder,\n        }, true_image_shapes)\n      with self.test_session(graph=tf_graph) as sess:\n        detections_out = sess.run(\n            detections,\n            feed_dict={\n                refined_box_encodings_placeholder: refined_box_encodings,\n                class_predictions_with_background_placeholder:\n                class_predictions_with_background,\n                num_proposals_placeholder: num_proposals,\n                proposal_boxes_placeholder: proposal_boxes,\n                image_shape_placeholder: image_shape\n            })\n      self.assertAllEqual(detections_out[\'detection_boxes\'].shape, [2, 5, 4])\n      self.assertAllClose(detections_out[\'detection_scores\'],\n                          [[1, 1, 1, 1, 1], [1, 1, 1, 1, 0]])\n      self.assertAllClose(detections_out[\'detection_classes\'],\n                          [[0, 0, 0, 1, 1], [0, 0, 1, 1, 0]])\n      self.assertAllClose(detections_out[\'num_detections\'], [5, 4])\n\n  def test_postprocess_second_stage_only_inference_mode(self):\n    self._test_postprocess_second_stage_only_inference_mode()\n\n  def test_postprocess_second_stage_only_inference_mode_padded_image(self):\n    self._test_postprocess_second_stage_only_inference_mode(\n        pad_to_max_dimension=56)\n\n  def test_preprocess_preserves_input_shapes(self):\n    image_shapes = [(3, None, None, 3),\n                    (None, 10, 10, 3),\n                    (None, None, None, 3)]\n    for image_shape in image_shapes:\n      model = self._build_model(\n          is_training=False, number_of_stages=2, second_stage_batch_size=6)\n      image_placeholder = tf.placeholder(tf.float32, shape=image_shape)\n      preprocessed_inputs, _ = model.preprocess(image_placeholder)\n      self.assertAllEqual(preprocessed_inputs.shape.as_list(), image_shape)\n\n  # TODO(rathodv): Split test into two - with and without masks.\n  def test_loss_first_stage_only_mode(self):\n    model = self._build_model(\n        is_training=True, number_of_stages=1, second_stage_batch_size=6)\n    batch_size = 2\n    anchors = tf.constant(\n        [[0, 0, 16, 16],\n         [0, 16, 16, 32],\n         [16, 0, 32, 16],\n         [16, 16, 32, 32]], dtype=tf.float32)\n\n    rpn_box_encodings = tf.zeros(\n        [batch_size,\n         anchors.get_shape().as_list()[0],\n         BOX_CODE_SIZE], dtype=tf.float32)\n    # use different numbers for the objectness category to break ties in\n    # order of boxes returned by NMS\n    rpn_objectness_predictions_with_background = tf.constant([\n        [[-10, 13],\n         [10, -10],\n         [10, -11],\n         [-10, 12]],\n        [[10, -10],\n         [-10, 13],\n         [-10, 12],\n         [10, -11]]], dtype=tf.float32)\n    image_shape = tf.constant([batch_size, 32, 32, 3], dtype=tf.int32)\n\n    groundtruth_boxes_list = [\n        tf.constant([[0, 0, .5, .5], [.5, .5, 1, 1]], dtype=tf.float32),\n        tf.constant([[0, .5, .5, 1], [.5, 0, 1, .5]], dtype=tf.float32)]\n    groundtruth_classes_list = [tf.constant([[1, 0], [0, 1]], dtype=tf.float32),\n                                tf.constant([[1, 0], [1, 0]], dtype=tf.float32)]\n\n    prediction_dict = {\n        \'rpn_box_encodings\': rpn_box_encodings,\n        \'rpn_objectness_predictions_with_background\':\n        rpn_objectness_predictions_with_background,\n        \'image_shape\': image_shape,\n        \'anchors\': anchors\n    }\n    _, true_image_shapes = model.preprocess(tf.zeros(image_shape))\n    model.provide_groundtruth(groundtruth_boxes_list,\n                              groundtruth_classes_list)\n    loss_dict = model.loss(prediction_dict, true_image_shapes)\n    with self.test_session() as sess:\n      loss_dict_out = sess.run(loss_dict)\n      self.assertAllClose(loss_dict_out[\'Loss/RPNLoss/localization_loss\'], 0)\n      self.assertAllClose(loss_dict_out[\'Loss/RPNLoss/objectness_loss\'], 0)\n      self.assertTrue(\'Loss/BoxClassifierLoss/localization_loss\'\n                      not in loss_dict_out)\n      self.assertTrue(\'Loss/BoxClassifierLoss/classification_loss\'\n                      not in loss_dict_out)\n\n  # TODO(rathodv): Split test into two - with and without masks.\n  def test_loss_full(self):\n    model = self._build_model(\n        is_training=True, number_of_stages=2, second_stage_batch_size=6)\n    batch_size = 2\n    anchors = tf.constant(\n        [[0, 0, 16, 16],\n         [0, 16, 16, 32],\n         [16, 0, 32, 16],\n         [16, 16, 32, 32]], dtype=tf.float32)\n    rpn_box_encodings = tf.zeros(\n        [batch_size,\n         anchors.get_shape().as_list()[0],\n         BOX_CODE_SIZE], dtype=tf.float32)\n    # use different numbers for the objectness category to break ties in\n    # order of boxes returned by NMS\n    rpn_objectness_predictions_with_background = tf.constant([\n        [[-10, 13],\n         [10, -10],\n         [10, -11],\n         [-10, 12]],\n        [[10, -10],\n         [-10, 13],\n         [-10, 12],\n         [10, -11]]], dtype=tf.float32)\n    image_shape = tf.constant([batch_size, 32, 32, 3], dtype=tf.int32)\n\n    num_proposals = tf.constant([6, 6], dtype=tf.int32)\n    proposal_boxes = tf.constant(\n        2 * [[[0, 0, 16, 16],\n              [0, 16, 16, 32],\n              [16, 0, 32, 16],\n              [16, 16, 32, 32],\n              [0, 0, 16, 16],\n              [0, 16, 16, 32]]], dtype=tf.float32)\n    refined_box_encodings = tf.zeros(\n        (batch_size * model.max_num_proposals,\n         model.num_classes,\n         BOX_CODE_SIZE), dtype=tf.float32)\n    class_predictions_with_background = tf.constant(\n        [[-10, 10, -10],  # first image\n         [10, -10, -10],\n         [10, -10, -10],\n         [-10, -10, 10],\n         [-10, 10, -10],\n         [10, -10, -10],\n         [10, -10, -10],  # second image\n         [-10, 10, -10],\n         [-10, 10, -10],\n         [10, -10, -10],\n         [10, -10, -10],\n         [-10, 10, -10]], dtype=tf.float32)\n\n    mask_predictions_logits = 20 * tf.ones((batch_size *\n                                            model.max_num_proposals,\n                                            model.num_classes,\n                                            14, 14),\n                                           dtype=tf.float32)\n\n    groundtruth_boxes_list = [\n        tf.constant([[0, 0, .5, .5], [.5, .5, 1, 1]], dtype=tf.float32),\n        tf.constant([[0, .5, .5, 1], [.5, 0, 1, .5]], dtype=tf.float32)]\n    groundtruth_classes_list = [tf.constant([[1, 0], [0, 1]], dtype=tf.float32),\n                                tf.constant([[1, 0], [1, 0]], dtype=tf.float32)]\n\n    # Set all elements of groundtruth mask to 1.0. In this case all proposal\n    # crops of the groundtruth masks should return a mask that covers the entire\n    # proposal. Thus, if mask_predictions_logits element values are all greater\n    # than 20, the loss should be zero.\n    groundtruth_masks_list = [tf.convert_to_tensor(np.ones((2, 32, 32)),\n                                                   dtype=tf.float32),\n                              tf.convert_to_tensor(np.ones((2, 32, 32)),\n                                                   dtype=tf.float32)]\n    prediction_dict = {\n        \'rpn_box_encodings\': rpn_box_encodings,\n        \'rpn_objectness_predictions_with_background\':\n        rpn_objectness_predictions_with_background,\n        \'image_shape\': image_shape,\n        \'anchors\': anchors,\n        \'refined_box_encodings\': refined_box_encodings,\n        \'class_predictions_with_background\': class_predictions_with_background,\n        \'proposal_boxes\': proposal_boxes,\n        \'num_proposals\': num_proposals,\n        \'mask_predictions\': mask_predictions_logits\n    }\n    _, true_image_shapes = model.preprocess(tf.zeros(image_shape))\n    model.provide_groundtruth(groundtruth_boxes_list,\n                              groundtruth_classes_list,\n                              groundtruth_masks_list)\n    loss_dict = model.loss(prediction_dict, true_image_shapes)\n\n    with self.test_session() as sess:\n      loss_dict_out = sess.run(loss_dict)\n      self.assertAllClose(loss_dict_out[\'Loss/RPNLoss/localization_loss\'], 0)\n      self.assertAllClose(loss_dict_out[\'Loss/RPNLoss/objectness_loss\'], 0)\n      self.assertAllClose(loss_dict_out[\n          \'Loss/BoxClassifierLoss/localization_loss\'], 0)\n      self.assertAllClose(loss_dict_out[\n          \'Loss/BoxClassifierLoss/classification_loss\'], 0)\n      self.assertAllClose(loss_dict_out[\'Loss/BoxClassifierLoss/mask_loss\'], 0)\n\n  def test_loss_full_zero_padded_proposals(self):\n    model = self._build_model(\n        is_training=True, number_of_stages=2, second_stage_batch_size=6)\n    batch_size = 1\n    anchors = tf.constant(\n        [[0, 0, 16, 16],\n         [0, 16, 16, 32],\n         [16, 0, 32, 16],\n         [16, 16, 32, 32]], dtype=tf.float32)\n    rpn_box_encodings = tf.zeros(\n        [batch_size,\n         anchors.get_shape().as_list()[0],\n         BOX_CODE_SIZE], dtype=tf.float32)\n    # use different numbers for the objectness category to break ties in\n    # order of boxes returned by NMS\n    rpn_objectness_predictions_with_background = tf.constant([\n        [[-10, 13],\n         [10, -10],\n         [10, -11],\n         [10, -12]],], dtype=tf.float32)\n    image_shape = tf.constant([batch_size, 32, 32, 3], dtype=tf.int32)\n\n    # box_classifier_batch_size is 6, but here we assume that the number of\n    # actual proposals (not counting zero paddings) is fewer (3).\n    num_proposals = tf.constant([3], dtype=tf.int32)\n    proposal_boxes = tf.constant(\n        [[[0, 0, 16, 16],\n          [0, 16, 16, 32],\n          [16, 0, 32, 16],\n          [0, 0, 0, 0],  # begin paddings\n          [0, 0, 0, 0],\n          [0, 0, 0, 0]]], dtype=tf.float32)\n\n    refined_box_encodings = tf.zeros(\n        (batch_size * model.max_num_proposals,\n         model.num_classes,\n         BOX_CODE_SIZE), dtype=tf.float32)\n    class_predictions_with_background = tf.constant(\n        [[-10, 10, -10],\n         [10, -10, -10],\n         [10, -10, -10],\n         [0, 0, 0],  # begin paddings\n         [0, 0, 0],\n         [0, 0, 0]], dtype=tf.float32)\n\n    mask_predictions_logits = 20 * tf.ones((batch_size *\n                                            model.max_num_proposals,\n                                            model.num_classes,\n                                            14, 14),\n                                           dtype=tf.float32)\n\n    groundtruth_boxes_list = [\n        tf.constant([[0, 0, .5, .5]], dtype=tf.float32)]\n    groundtruth_classes_list = [tf.constant([[1, 0]], dtype=tf.float32)]\n\n    # Set all elements of groundtruth mask to 1.0. In this case all proposal\n    # crops of the groundtruth masks should return a mask that covers the entire\n    # proposal. Thus, if mask_predictions_logits element values are all greater\n    # than 20, the loss should be zero.\n    groundtruth_masks_list = [tf.convert_to_tensor(np.ones((1, 32, 32)),\n                                                   dtype=tf.float32)]\n\n    prediction_dict = {\n        \'rpn_box_encodings\': rpn_box_encodings,\n        \'rpn_objectness_predictions_with_background\':\n        rpn_objectness_predictions_with_background,\n        \'image_shape\': image_shape,\n        \'anchors\': anchors,\n        \'refined_box_encodings\': refined_box_encodings,\n        \'class_predictions_with_background\': class_predictions_with_background,\n        \'proposal_boxes\': proposal_boxes,\n        \'num_proposals\': num_proposals,\n        \'mask_predictions\': mask_predictions_logits\n    }\n    _, true_image_shapes = model.preprocess(tf.zeros(image_shape))\n    model.provide_groundtruth(groundtruth_boxes_list,\n                              groundtruth_classes_list,\n                              groundtruth_masks_list)\n    loss_dict = model.loss(prediction_dict, true_image_shapes)\n\n    with self.test_session() as sess:\n      loss_dict_out = sess.run(loss_dict)\n      self.assertAllClose(loss_dict_out[\'Loss/RPNLoss/localization_loss\'], 0)\n      self.assertAllClose(loss_dict_out[\'Loss/RPNLoss/objectness_loss\'], 0)\n      self.assertAllClose(loss_dict_out[\n          \'Loss/BoxClassifierLoss/localization_loss\'], 0)\n      self.assertAllClose(loss_dict_out[\n          \'Loss/BoxClassifierLoss/classification_loss\'], 0)\n      self.assertAllClose(loss_dict_out[\'Loss/BoxClassifierLoss/mask_loss\'], 0)\n\n  def test_loss_full_multiple_label_groundtruth(self):\n    model = self._build_model(\n        is_training=True, number_of_stages=2, second_stage_batch_size=6,\n        softmax_second_stage_classification_loss=False)\n    batch_size = 1\n    anchors = tf.constant(\n        [[0, 0, 16, 16],\n         [0, 16, 16, 32],\n         [16, 0, 32, 16],\n         [16, 16, 32, 32]], dtype=tf.float32)\n    rpn_box_encodings = tf.zeros(\n        [batch_size,\n         anchors.get_shape().as_list()[0],\n         BOX_CODE_SIZE], dtype=tf.float32)\n    # use different numbers for the objectness category to break ties in\n    # order of boxes returned by NMS\n    rpn_objectness_predictions_with_background = tf.constant([\n        [[-10, 13],\n         [10, -10],\n         [10, -11],\n         [10, -12]],], dtype=tf.float32)\n    image_shape = tf.constant([batch_size, 32, 32, 3], dtype=tf.int32)\n\n    # box_classifier_batch_size is 6, but here we assume that the number of\n    # actual proposals (not counting zero paddings) is fewer (3).\n    num_proposals = tf.constant([3], dtype=tf.int32)\n    proposal_boxes = tf.constant(\n        [[[0, 0, 16, 16],\n          [0, 16, 16, 32],\n          [16, 0, 32, 16],\n          [0, 0, 0, 0],  # begin paddings\n          [0, 0, 0, 0],\n          [0, 0, 0, 0]]], dtype=tf.float32)\n\n    # second_stage_localization_loss should only be computed for predictions\n    # that match groundtruth. For multiple label groundtruth boxes, the loss\n    # should only be computed once for the label with the smaller index.\n    refined_box_encodings = tf.constant(\n        [[[0, 0, 0, 0], [1, 1, -1, -1]],\n         [[1, 1, -1, -1], [1, 1, 1, 1]],\n         [[1, 1, -1, -1], [1, 1, 1, 1]],\n         [[1, 1, -1, -1], [1, 1, 1, 1]],\n         [[1, 1, -1, -1], [1, 1, 1, 1]],\n         [[1, 1, -1, -1], [1, 1, 1, 1]]], dtype=tf.float32)\n    class_predictions_with_background = tf.constant(\n        [[-100, 100, 100],\n         [100, -100, -100],\n         [100, -100, -100],\n         [0, 0, 0],  # begin paddings\n         [0, 0, 0],\n         [0, 0, 0]], dtype=tf.float32)\n\n    mask_predictions_logits = 20 * tf.ones((batch_size *\n                                            model.max_num_proposals,\n                                            model.num_classes,\n                                            14, 14),\n                                           dtype=tf.float32)\n\n    groundtruth_boxes_list = [\n        tf.constant([[0, 0, .5, .5]], dtype=tf.float32)]\n    # Box contains two ground truth labels.\n    groundtruth_classes_list = [tf.constant([[1, 1]], dtype=tf.float32)]\n\n    # Set all elements of groundtruth mask to 1.0. In this case all proposal\n    # crops of the groundtruth masks should return a mask that covers the entire\n    # proposal. Thus, if mask_predictions_logits element values are all greater\n    # than 20, the loss should be zero.\n    groundtruth_masks_list = [tf.convert_to_tensor(np.ones((1, 32, 32)),\n                                                   dtype=tf.float32)]\n\n    prediction_dict = {\n        \'rpn_box_encodings\': rpn_box_encodings,\n        \'rpn_objectness_predictions_with_background\':\n        rpn_objectness_predictions_with_background,\n        \'image_shape\': image_shape,\n        \'anchors\': anchors,\n        \'refined_box_encodings\': refined_box_encodings,\n        \'class_predictions_with_background\': class_predictions_with_background,\n        \'proposal_boxes\': proposal_boxes,\n        \'num_proposals\': num_proposals,\n        \'mask_predictions\': mask_predictions_logits\n    }\n    _, true_image_shapes = model.preprocess(tf.zeros(image_shape))\n    model.provide_groundtruth(groundtruth_boxes_list,\n                              groundtruth_classes_list,\n                              groundtruth_masks_list)\n    loss_dict = model.loss(prediction_dict, true_image_shapes)\n\n    with self.test_session() as sess:\n      loss_dict_out = sess.run(loss_dict)\n      self.assertAllClose(loss_dict_out[\'Loss/RPNLoss/localization_loss\'], 0)\n      self.assertAllClose(loss_dict_out[\'Loss/RPNLoss/objectness_loss\'], 0)\n      self.assertAllClose(loss_dict_out[\n          \'Loss/BoxClassifierLoss/localization_loss\'], 0)\n      self.assertAllClose(loss_dict_out[\n          \'Loss/BoxClassifierLoss/classification_loss\'], 0)\n      self.assertAllClose(loss_dict_out[\'Loss/BoxClassifierLoss/mask_loss\'], 0)\n\n  def test_loss_full_zero_padded_proposals_nonzero_loss_with_two_images(self):\n    model = self._build_model(\n        is_training=True, number_of_stages=2, second_stage_batch_size=6)\n    batch_size = 2\n    anchors = tf.constant(\n        [[0, 0, 16, 16],\n         [0, 16, 16, 32],\n         [16, 0, 32, 16],\n         [16, 16, 32, 32]], dtype=tf.float32)\n    rpn_box_encodings = tf.zeros(\n        [batch_size,\n         anchors.get_shape().as_list()[0],\n         BOX_CODE_SIZE], dtype=tf.float32)\n    # use different numbers for the objectness category to break ties in\n    # order of boxes returned by NMS\n    rpn_objectness_predictions_with_background = tf.constant(\n        [[[-10, 13],\n          [10, -10],\n          [10, -11],\n          [10, -12]],\n         [[-10, 13],\n          [10, -10],\n          [10, -11],\n          [10, -12]]], dtype=tf.float32)\n    image_shape = tf.constant([batch_size, 32, 32, 3], dtype=tf.int32)\n\n    # box_classifier_batch_size is 6, but here we assume that the number of\n    # actual proposals (not counting zero paddings) is fewer.\n    num_proposals = tf.constant([3, 2], dtype=tf.int32)\n    proposal_boxes = tf.constant(\n        [[[0, 0, 16, 16],\n          [0, 16, 16, 32],\n          [16, 0, 32, 16],\n          [0, 0, 0, 0],  # begin paddings\n          [0, 0, 0, 0],\n          [0, 0, 0, 0]],\n         [[0, 0, 16, 16],\n          [0, 16, 16, 32],\n          [0, 0, 0, 0],  # begin paddings\n          [0, 0, 0, 0],\n          [0, 0, 0, 0],\n          [0, 0, 0, 0]]], dtype=tf.float32)\n\n    refined_box_encodings = tf.zeros(\n        (batch_size * model.max_num_proposals,\n         model.num_classes,\n         BOX_CODE_SIZE), dtype=tf.float32)\n    class_predictions_with_background = tf.constant(\n        [[-10, 10, -10],  # first image\n         [10, -10, -10],\n         [10, -10, -10],\n         [0, 0, 0],  # begin paddings\n         [0, 0, 0],\n         [0, 0, 0],\n         [-10, -10, 10],  # second image\n         [10, -10, -10],\n         [0, 0, 0],  # begin paddings\n         [0, 0, 0],\n         [0, 0, 0],\n         [0, 0, 0],], dtype=tf.float32)\n\n    # The first groundtruth box is 4/5 of the anchor size in both directions\n    # experiencing a loss of:\n    # 2 * SmoothL1(5 * log(4/5)) / num_proposals\n    #   = 2 * (abs(5 * log(1/2)) - .5) / 3\n    # The second groundtruth box is identical to the prediction and thus\n    # experiences zero loss.\n    # Total average loss is (abs(5 * log(1/2)) - .5) / 3.\n    groundtruth_boxes_list = [\n        tf.constant([[0.05, 0.05, 0.45, 0.45]], dtype=tf.float32),\n        tf.constant([[0.0, 0.0, 0.5, 0.5]], dtype=tf.float32)]\n    groundtruth_classes_list = [tf.constant([[1, 0]], dtype=tf.float32),\n                                tf.constant([[0, 1]], dtype=tf.float32)]\n    exp_loc_loss = (-5 * np.log(.8) - 0.5) / 3.0\n\n    prediction_dict = {\n        \'rpn_box_encodings\': rpn_box_encodings,\n        \'rpn_objectness_predictions_with_background\':\n        rpn_objectness_predictions_with_background,\n        \'image_shape\': image_shape,\n        \'anchors\': anchors,\n        \'refined_box_encodings\': refined_box_encodings,\n        \'class_predictions_with_background\': class_predictions_with_background,\n        \'proposal_boxes\': proposal_boxes,\n        \'num_proposals\': num_proposals\n    }\n    _, true_image_shapes = model.preprocess(tf.zeros(image_shape))\n    model.provide_groundtruth(groundtruth_boxes_list,\n                              groundtruth_classes_list)\n    loss_dict = model.loss(prediction_dict, true_image_shapes)\n\n    with self.test_session() as sess:\n      loss_dict_out = sess.run(loss_dict)\n      self.assertAllClose(loss_dict_out[\'Loss/RPNLoss/localization_loss\'],\n                          exp_loc_loss)\n      self.assertAllClose(loss_dict_out[\'Loss/RPNLoss/objectness_loss\'], 0)\n      self.assertAllClose(loss_dict_out[\n          \'Loss/BoxClassifierLoss/localization_loss\'], exp_loc_loss)\n      self.assertAllClose(loss_dict_out[\n          \'Loss/BoxClassifierLoss/classification_loss\'], 0)\n\n  def test_loss_with_hard_mining(self):\n    model = self._build_model(is_training=True,\n                              number_of_stages=2,\n                              second_stage_batch_size=None,\n                              first_stage_max_proposals=6,\n                              hard_mining=True)\n    batch_size = 1\n    anchors = tf.constant(\n        [[0, 0, 16, 16],\n         [0, 16, 16, 32],\n         [16, 0, 32, 16],\n         [16, 16, 32, 32]], dtype=tf.float32)\n    rpn_box_encodings = tf.zeros(\n        [batch_size,\n         anchors.get_shape().as_list()[0],\n         BOX_CODE_SIZE], dtype=tf.float32)\n    # use different numbers for the objectness category to break ties in\n    # order of boxes returned by NMS\n    rpn_objectness_predictions_with_background = tf.constant(\n        [[[-10, 13],\n          [-10, 12],\n          [10, -11],\n          [10, -12]]], dtype=tf.float32)\n    image_shape = tf.constant([batch_size, 32, 32, 3], dtype=tf.int32)\n\n    # box_classifier_batch_size is 6, but here we assume that the number of\n    # actual proposals (not counting zero paddings) is fewer (3).\n    num_proposals = tf.constant([3], dtype=tf.int32)\n    proposal_boxes = tf.constant(\n        [[[0, 0, 16, 16],\n          [0, 16, 16, 32],\n          [16, 0, 32, 16],\n          [0, 0, 0, 0],  # begin paddings\n          [0, 0, 0, 0],\n          [0, 0, 0, 0]]], dtype=tf.float32)\n\n    refined_box_encodings = tf.zeros(\n        (batch_size * model.max_num_proposals,\n         model.num_classes,\n         BOX_CODE_SIZE), dtype=tf.float32)\n    class_predictions_with_background = tf.constant(\n        [[-10, 10, -10],  # first image\n         [-10, -10, 10],\n         [10, -10, -10],\n         [0, 0, 0],  # begin paddings\n         [0, 0, 0],\n         [0, 0, 0]], dtype=tf.float32)\n\n    # The first groundtruth box is 4/5 of the anchor size in both directions\n    # experiencing a loss of:\n    # 2 * SmoothL1(5 * log(4/5)) / num_proposals\n    #   = 2 * (abs(5 * log(1/2)) - .5) / 3\n    # The second groundtruth box is 46/50 of the anchor size in both directions\n    # experiencing a loss of:\n    # 2 * SmoothL1(5 * log(42/50)) / num_proposals\n    #   = 2 * (.5(5 * log(.92))^2 - .5) / 3.\n    # Since the first groundtruth box experiences greater loss, and we have\n    # set num_hard_examples=1 in the HardMiner, the final localization loss\n    # corresponds to that of the first groundtruth box.\n    groundtruth_boxes_list = [\n        tf.constant([[0.05, 0.05, 0.45, 0.45],\n                     [0.02, 0.52, 0.48, 0.98],], dtype=tf.float32)]\n    groundtruth_classes_list = [tf.constant([[1, 0], [0, 1]], dtype=tf.float32)]\n    exp_loc_loss = 2 * (-5 * np.log(.8) - 0.5) / 3.0\n\n    prediction_dict = {\n        \'rpn_box_encodings\': rpn_box_encodings,\n        \'rpn_objectness_predictions_with_background\':\n        rpn_objectness_predictions_with_background,\n        \'image_shape\': image_shape,\n        \'anchors\': anchors,\n        \'refined_box_encodings\': refined_box_encodings,\n        \'class_predictions_with_background\': class_predictions_with_background,\n        \'proposal_boxes\': proposal_boxes,\n        \'num_proposals\': num_proposals\n    }\n    _, true_image_shapes = model.preprocess(tf.zeros(image_shape))\n    model.provide_groundtruth(groundtruth_boxes_list,\n                              groundtruth_classes_list)\n    loss_dict = model.loss(prediction_dict, true_image_shapes)\n\n    with self.test_session() as sess:\n      loss_dict_out = sess.run(loss_dict)\n      self.assertAllClose(loss_dict_out[\n          \'Loss/BoxClassifierLoss/localization_loss\'], exp_loc_loss)\n      self.assertAllClose(loss_dict_out[\n          \'Loss/BoxClassifierLoss/classification_loss\'], 0)\n\n  def test_restore_map_for_classification_ckpt(self):\n    # Define mock tensorflow classification graph and save variables.\n    test_graph_classification = tf.Graph()\n    with test_graph_classification.as_default():\n      image = tf.placeholder(dtype=tf.float32, shape=[1, 20, 20, 3])\n      with tf.variable_scope(\'mock_model\'):\n        net = slim.conv2d(image, num_outputs=3, kernel_size=1, scope=\'layer1\')\n        slim.conv2d(net, num_outputs=3, kernel_size=1, scope=\'layer2\')\n\n      init_op = tf.global_variables_initializer()\n      saver = tf.train.Saver()\n      save_path = self.get_temp_dir()\n      with self.test_session(graph=test_graph_classification) as sess:\n        sess.run(init_op)\n        saved_model_path = saver.save(sess, save_path)\n\n    # Create tensorflow detection graph and load variables from\n    # classification checkpoint.\n    test_graph_detection = tf.Graph()\n    with test_graph_detection.as_default():\n      model = self._build_model(\n          is_training=False, number_of_stages=2, second_stage_batch_size=6)\n\n      inputs_shape = (2, 20, 20, 3)\n      inputs = tf.to_float(tf.random_uniform(\n          inputs_shape, minval=0, maxval=255, dtype=tf.int32))\n      preprocessed_inputs, true_image_shapes = model.preprocess(inputs)\n      prediction_dict = model.predict(preprocessed_inputs, true_image_shapes)\n      model.postprocess(prediction_dict, true_image_shapes)\n      var_map = model.restore_map(fine_tune_checkpoint_type=\'classification\')\n      self.assertIsInstance(var_map, dict)\n      saver = tf.train.Saver(var_map)\n      with self.test_session(graph=test_graph_classification) as sess:\n        saver.restore(sess, saved_model_path)\n        for var in sess.run(tf.report_uninitialized_variables()):\n          self.assertNotIn(model.first_stage_feature_extractor_scope, var)\n          self.assertNotIn(model.second_stage_feature_extractor_scope, var)\n\n  def test_restore_map_for_detection_ckpt(self):\n    # Define first detection graph and save variables.\n    test_graph_detection1 = tf.Graph()\n    with test_graph_detection1.as_default():\n      model = self._build_model(\n          is_training=False, number_of_stages=2, second_stage_batch_size=6)\n      inputs_shape = (2, 20, 20, 3)\n      inputs = tf.to_float(tf.random_uniform(\n          inputs_shape, minval=0, maxval=255, dtype=tf.int32))\n      preprocessed_inputs, true_image_shapes = model.preprocess(inputs)\n      prediction_dict = model.predict(preprocessed_inputs, true_image_shapes)\n      model.postprocess(prediction_dict, true_image_shapes)\n      another_variable = tf.Variable([17.0], name=\'another_variable\')  # pylint: disable=unused-variable\n      init_op = tf.global_variables_initializer()\n      saver = tf.train.Saver()\n      save_path = self.get_temp_dir()\n      with self.test_session(graph=test_graph_detection1) as sess:\n        sess.run(init_op)\n        saved_model_path = saver.save(sess, save_path)\n\n    # Define second detection graph and restore variables.\n    test_graph_detection2 = tf.Graph()\n    with test_graph_detection2.as_default():\n      model2 = self._build_model(is_training=False, number_of_stages=2,\n                                 second_stage_batch_size=6, num_classes=42)\n\n      inputs_shape2 = (2, 20, 20, 3)\n      inputs2 = tf.to_float(tf.random_uniform(\n          inputs_shape2, minval=0, maxval=255, dtype=tf.int32))\n      preprocessed_inputs2, true_image_shapes = model2.preprocess(inputs2)\n      prediction_dict2 = model2.predict(preprocessed_inputs2, true_image_shapes)\n      model2.postprocess(prediction_dict2, true_image_shapes)\n      another_variable = tf.Variable([17.0], name=\'another_variable\')  # pylint: disable=unused-variable\n      var_map = model2.restore_map(fine_tune_checkpoint_type=\'detection\')\n      self.assertIsInstance(var_map, dict)\n      saver = tf.train.Saver(var_map)\n      with self.test_session(graph=test_graph_detection2) as sess:\n        saver.restore(sess, saved_model_path)\n        uninitialized_vars_list = sess.run(tf.report_uninitialized_variables())\n        self.assertIn(\'another_variable\', uninitialized_vars_list)\n        for var in uninitialized_vars_list:\n          self.assertNotIn(model2.first_stage_feature_extractor_scope, var)\n          self.assertNotIn(model2.second_stage_feature_extractor_scope, var)\n\n  def test_load_all_det_checkpoint_vars(self):\n    test_graph_detection = tf.Graph()\n    with test_graph_detection.as_default():\n      model = self._build_model(\n          is_training=False,\n          number_of_stages=2,\n          second_stage_batch_size=6,\n          num_classes=42)\n\n      inputs_shape = (2, 20, 20, 3)\n      inputs = tf.to_float(\n          tf.random_uniform(inputs_shape, minval=0, maxval=255, dtype=tf.int32))\n      preprocessed_inputs, true_image_shapes = model.preprocess(inputs)\n      prediction_dict = model.predict(preprocessed_inputs, true_image_shapes)\n      model.postprocess(prediction_dict, true_image_shapes)\n      another_variable = tf.Variable([17.0], name=\'another_variable\')  # pylint: disable=unused-variable\n      var_map = model.restore_map(\n          fine_tune_checkpoint_type=\'detection\',\n          load_all_detection_checkpoint_vars=True)\n      self.assertIsInstance(var_map, dict)\n      self.assertIn(\'another_variable\', var_map)\n\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
src/object_detection/meta_architectures/rfcn_meta_arch.py,6,"b'# Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""R-FCN meta-architecture definition.\n\nR-FCN: Dai, Jifeng, et al. ""R-FCN: Object Detection via Region-based\nFully Convolutional Networks."" arXiv preprint arXiv:1605.06409 (2016).\n\nThe R-FCN meta architecture is similar to Faster R-CNN and only differs in the\nsecond stage. Hence this class inherits FasterRCNNMetaArch and overrides only\nthe `_predict_second_stage` method.\n\nSimilar to Faster R-CNN we allow for two modes: number_of_stages=1 and\nnumber_of_stages=2.  In the former setting, all of the user facing methods\n(e.g., predict, postprocess, loss) can be used as if the model consisted\nonly of the RPN, returning class agnostic proposals (these can be thought of as\napproximate detections with no associated class information).  In the latter\nsetting, proposals are computed, then passed through a second stage\n""box classifier"" to yield (multi-class) detections.\n\nImplementations of R-FCN models must define a new FasterRCNNFeatureExtractor and\noverride three methods: `preprocess`, `_extract_proposal_features` (the first\nstage of the model), and `_extract_box_classifier_features` (the second stage of\nthe model). Optionally, the `restore_fn` method can be overridden.  See tests\nfor an example.\n\nSee notes in the documentation of Faster R-CNN meta-architecture as they all\napply here.\n""""""\nimport tensorflow as tf\n\nfrom object_detection.core import box_predictor\nfrom object_detection.meta_architectures import faster_rcnn_meta_arch\nfrom object_detection.utils import ops\n\n\nclass RFCNMetaArch(faster_rcnn_meta_arch.FasterRCNNMetaArch):\n  """"""R-FCN Meta-architecture definition.""""""\n\n  def __init__(self,\n               is_training,\n               num_classes,\n               image_resizer_fn,\n               feature_extractor,\n               number_of_stages,\n               first_stage_anchor_generator,\n               first_stage_atrous_rate,\n               first_stage_box_predictor_arg_scope,\n               first_stage_box_predictor_kernel_size,\n               first_stage_box_predictor_depth,\n               first_stage_minibatch_size,\n               first_stage_positive_balance_fraction,\n               first_stage_nms_score_threshold,\n               first_stage_nms_iou_threshold,\n               first_stage_max_proposals,\n               first_stage_localization_loss_weight,\n               first_stage_objectness_loss_weight,\n               second_stage_rfcn_box_predictor,\n               second_stage_batch_size,\n               second_stage_balance_fraction,\n               second_stage_non_max_suppression_fn,\n               second_stage_score_conversion_fn,\n               second_stage_localization_loss_weight,\n               second_stage_classification_loss_weight,\n               second_stage_classification_loss,\n               hard_example_miner,\n               parallel_iterations=16,\n               add_summaries=True):\n    """"""RFCNMetaArch Constructor.\n\n    Args:\n      is_training: A boolean indicating whether the training version of the\n        computation graph should be constructed.\n      num_classes: Number of classes.  Note that num_classes *does not*\n        include the background category, so if groundtruth labels take values\n        in {0, 1, .., K-1}, num_classes=K (and not K+1, even though the\n        assigned classification targets can range from {0,... K}).\n      image_resizer_fn: A callable for image resizing.  This callable always\n        takes a rank-3 image tensor (corresponding to a single image) and\n        returns a rank-3 image tensor, possibly with new spatial dimensions.\n        See builders/image_resizer_builder.py.\n      feature_extractor: A FasterRCNNFeatureExtractor object.\n      number_of_stages:  Valid values are {1, 2}. If 1 will only construct the\n        Region Proposal Network (RPN) part of the model.\n      first_stage_anchor_generator: An anchor_generator.AnchorGenerator object\n        (note that currently we only support\n        grid_anchor_generator.GridAnchorGenerator objects)\n      first_stage_atrous_rate: A single integer indicating the atrous rate for\n        the single convolution op which is applied to the `rpn_features_to_crop`\n        tensor to obtain a tensor to be used for box prediction. Some feature\n        extractors optionally allow for producing feature maps computed at\n        denser resolutions.  The atrous rate is used to compensate for the\n        denser feature maps by using an effectively larger receptive field.\n        (This should typically be set to 1).\n      first_stage_box_predictor_arg_scope: Slim arg_scope for conv2d,\n        separable_conv2d and fully_connected ops for the RPN box predictor.\n      first_stage_box_predictor_kernel_size: Kernel size to use for the\n        convolution op just prior to RPN box predictions.\n      first_stage_box_predictor_depth: Output depth for the convolution op\n        just prior to RPN box predictions.\n      first_stage_minibatch_size: The ""batch size"" to use for computing the\n        objectness and location loss of the region proposal network. This\n        ""batch size"" refers to the number of anchors selected as contributing\n        to the loss function for any given image within the image batch and is\n        only called ""batch_size"" due to terminology from the Faster R-CNN paper.\n      first_stage_positive_balance_fraction: Fraction of positive examples\n        per image for the RPN. The recommended value for Faster RCNN is 0.5.\n      first_stage_nms_score_threshold: Score threshold for non max suppression\n        for the Region Proposal Network (RPN).  This value is expected to be in\n        [0, 1] as it is applied directly after a softmax transformation.  The\n        recommended value for Faster R-CNN is 0.\n      first_stage_nms_iou_threshold: The Intersection Over Union (IOU) threshold\n        for performing Non-Max Suppression (NMS) on the boxes predicted by the\n        Region Proposal Network (RPN).\n      first_stage_max_proposals: Maximum number of boxes to retain after\n        performing Non-Max Suppression (NMS) on the boxes predicted by the\n        Region Proposal Network (RPN).\n      first_stage_localization_loss_weight: A float\n      first_stage_objectness_loss_weight: A float\n      second_stage_rfcn_box_predictor: RFCN box predictor to use for\n        second stage.\n      second_stage_batch_size: The batch size used for computing the\n        classification and refined location loss of the box classifier.  This\n        ""batch size"" refers to the number of proposals selected as contributing\n        to the loss function for any given image within the image batch and is\n        only called ""batch_size"" due to terminology from the Faster R-CNN paper.\n      second_stage_balance_fraction: Fraction of positive examples to use\n        per image for the box classifier. The recommended value for Faster RCNN\n        is 0.25.\n      second_stage_non_max_suppression_fn: batch_multiclass_non_max_suppression\n        callable that takes `boxes`, `scores`, optional `clip_window` and\n        optional (kwarg) `mask` inputs (with all other inputs already set)\n        and returns a dictionary containing tensors with keys:\n        `detection_boxes`, `detection_scores`, `detection_classes`,\n        `num_detections`, and (optionally) `detection_masks`. See\n        `post_processing.batch_multiclass_non_max_suppression` for the type and\n        shape of these tensors.\n      second_stage_score_conversion_fn: Callable elementwise nonlinearity\n        (that takes tensors as inputs and returns tensors).  This is usually\n        used to convert logits to probabilities.\n      second_stage_localization_loss_weight: A float\n      second_stage_classification_loss_weight: A float\n      second_stage_classification_loss: A string indicating which loss function\n        to use, supports \'softmax\' and \'sigmoid\'.\n      hard_example_miner:  A losses.HardExampleMiner object (can be None).\n      parallel_iterations: (Optional) The number of iterations allowed to run\n        in parallel for calls to tf.map_fn.\n      add_summaries: boolean (default: True) controlling whether summary ops\n        should be added to tensorflow graph.\n\n    Raises:\n      ValueError: If `second_stage_batch_size` > `first_stage_max_proposals`\n      ValueError: If first_stage_anchor_generator is not of type\n        grid_anchor_generator.GridAnchorGenerator.\n    """"""\n    # TODO(rathodv): add_summaries is currently unused. Respect that directive\n    # in the future.\n    super(RFCNMetaArch, self).__init__(\n        is_training,\n        num_classes,\n        image_resizer_fn,\n        feature_extractor,\n        number_of_stages,\n        first_stage_anchor_generator,\n        first_stage_atrous_rate,\n        first_stage_box_predictor_arg_scope,\n        first_stage_box_predictor_kernel_size,\n        first_stage_box_predictor_depth,\n        first_stage_minibatch_size,\n        first_stage_positive_balance_fraction,\n        first_stage_nms_score_threshold,\n        first_stage_nms_iou_threshold,\n        first_stage_max_proposals,\n        first_stage_localization_loss_weight,\n        first_stage_objectness_loss_weight,\n        None,  # initial_crop_size is not used in R-FCN\n        None,  # maxpool_kernel_size is not use in R-FCN\n        None,  # maxpool_stride is not use in R-FCN\n        None,  # fully_connected_box_predictor is not used in R-FCN.\n        second_stage_batch_size,\n        second_stage_balance_fraction,\n        second_stage_non_max_suppression_fn,\n        second_stage_score_conversion_fn,\n        second_stage_localization_loss_weight,\n        second_stage_classification_loss_weight,\n        second_stage_classification_loss,\n        1.0,  # second stage mask prediction loss weight isn\'t used in R-FCN.\n        hard_example_miner,\n        parallel_iterations)\n\n    self._rfcn_box_predictor = second_stage_rfcn_box_predictor\n\n  def _predict_second_stage(self, rpn_box_encodings,\n                            rpn_objectness_predictions_with_background,\n                            rpn_features,\n                            anchors,\n                            image_shape,\n                            true_image_shapes):\n    """"""Predicts the output tensors from 2nd stage of R-FCN.\n\n    Args:\n      rpn_box_encodings: 3-D float tensor of shape\n        [batch_size, num_valid_anchors, self._box_coder.code_size] containing\n        predicted boxes.\n      rpn_objectness_predictions_with_background: 3-D float tensor of shape\n        [batch_size, num_valid_anchors, 2] containing class\n        predictions (logits) for each of the anchors.  Note that this\n        tensor *includes* background class predictions (at class index 0).\n      rpn_features: A 4-D float32 tensor with shape\n        [batch_size, height, width, depth] representing image features from the\n        RPN.\n      anchors: 2-D float tensor of shape\n        [num_anchors, self._box_coder.code_size].\n      image_shape: A 1D int32 tensors of size [4] containing the image shape.\n      true_image_shapes: int32 tensor of shape [batch, 3] where each row is\n        of the form [height, width, channels] indicating the shapes\n        of true images in the resized images, as resized images can be padded\n        with zeros.\n\n    Returns:\n      prediction_dict: a dictionary holding ""raw"" prediction tensors:\n        1) refined_box_encodings: a 3-D tensor with shape\n          [total_num_proposals, num_classes, 4] representing predicted\n          (final) refined box encodings, where\n          total_num_proposals=batch_size*self._max_num_proposals\n        2) class_predictions_with_background: a 2-D tensor with shape\n          [total_num_proposals, num_classes + 1] containing class\n          predictions (logits) for each of the anchors, where\n          total_num_proposals=batch_size*self._max_num_proposals.\n          Note that this tensor *includes* background class predictions\n          (at class index 0).\n        3) num_proposals: An int32 tensor of shape [batch_size] representing the\n          number of proposals generated by the RPN. `num_proposals` allows us\n          to keep track of which entries are to be treated as zero paddings and\n          which are not since we always pad the number of proposals to be\n          `self.max_num_proposals` for each image.\n        4) proposal_boxes: A float32 tensor of shape\n          [batch_size, self.max_num_proposals, 4] representing\n          decoded proposal bounding boxes (in absolute coordinates).\n        5) proposal_boxes_normalized: A float32 tensor of shape\n          [batch_size, self.max_num_proposals, 4] representing decoded proposal\n          bounding boxes (in normalized coordinates). Can be used to override\n          the boxes proposed by the RPN, thus enabling one to extract box\n          classification and prediction for externally selected areas of the\n          image.\n        6) box_classifier_features: a 4-D float32 tensor, of shape\n          [batch_size, feature_map_height, feature_map_width, depth],\n          representing the box classifier features.\n    """"""\n    image_shape_2d = tf.tile(tf.expand_dims(image_shape[1:], 0),\n                             [image_shape[0], 1])\n    proposal_boxes_normalized, _, num_proposals = self._postprocess_rpn(\n        rpn_box_encodings, rpn_objectness_predictions_with_background,\n        anchors, image_shape_2d, true_image_shapes)\n\n    box_classifier_features = (\n        self._feature_extractor.extract_box_classifier_features(\n            rpn_features,\n            scope=self.second_stage_feature_extractor_scope))\n\n    box_predictions = self._rfcn_box_predictor.predict(\n        [box_classifier_features],\n        num_predictions_per_location=[1],\n        scope=self.second_stage_box_predictor_scope,\n        proposal_boxes=proposal_boxes_normalized)\n    refined_box_encodings = tf.squeeze(\n        tf.concat(box_predictions[box_predictor.BOX_ENCODINGS], axis=1), axis=1)\n    class_predictions_with_background = tf.squeeze(\n        tf.concat(\n            box_predictions[box_predictor.CLASS_PREDICTIONS_WITH_BACKGROUND],\n            axis=1),\n        axis=1)\n\n    absolute_proposal_boxes = ops.normalized_to_image_coordinates(\n        proposal_boxes_normalized, image_shape,\n        parallel_iterations=self._parallel_iterations)\n\n    prediction_dict = {\n        \'refined_box_encodings\': refined_box_encodings,\n        \'class_predictions_with_background\':\n        class_predictions_with_background,\n        \'num_proposals\': num_proposals,\n        \'proposal_boxes\': absolute_proposal_boxes,\n        \'box_classifier_features\': box_classifier_features,\n        \'proposal_boxes_normalized\': proposal_boxes_normalized,\n    }\n    return prediction_dict\n'"
src/object_detection/meta_architectures/rfcn_meta_arch_test.py,1,"b'# Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n""""""Tests for object_detection.meta_architectures.rfcn_meta_arch.""""""\n\nimport tensorflow as tf\n\nfrom object_detection.meta_architectures import faster_rcnn_meta_arch_test_lib\nfrom object_detection.meta_architectures import rfcn_meta_arch\n\n\nclass RFCNMetaArchTest(\n    faster_rcnn_meta_arch_test_lib.FasterRCNNMetaArchTestBase):\n\n  def _get_second_stage_box_predictor_text_proto(self):\n    box_predictor_text_proto = """"""\n      rfcn_box_predictor {\n        conv_hyperparams {\n          op: CONV\n          activation: NONE\n          regularizer {\n            l2_regularizer {\n              weight: 0.0005\n            }\n          }\n          initializer {\n            variance_scaling_initializer {\n              factor: 1.0\n              uniform: true\n              mode: FAN_AVG\n            }\n          }\n        }\n      }\n    """"""\n    return box_predictor_text_proto\n\n  def _get_model(self, box_predictor, **common_kwargs):\n    return rfcn_meta_arch.RFCNMetaArch(\n        second_stage_rfcn_box_predictor=box_predictor, **common_kwargs)\n\n  def _get_box_classifier_features_shape(self,\n                                         image_size,\n                                         batch_size,\n                                         max_num_proposals,\n                                         initial_crop_size,\n                                         maxpool_stride,\n                                         num_features):\n    return (batch_size, image_size, image_size, num_features)\n\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
src/object_detection/meta_architectures/ssd_meta_arch.py,68,"b'# Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""SSD Meta-architecture definition.\n\nGeneral tensorflow implementation of convolutional Multibox/SSD detection\nmodels.\n""""""\nfrom abc import abstractmethod\n\nimport re\nimport tensorflow as tf\n\nfrom object_detection.core import box_list\nfrom object_detection.core import box_list_ops\nfrom object_detection.core import model\nfrom object_detection.core import standard_fields as fields\nfrom object_detection.core import target_assigner\nfrom object_detection.utils import ops\nfrom object_detection.utils import shape_utils\nfrom object_detection.utils import visualization_utils\n\nslim = tf.contrib.slim\n\n\nclass SSDFeatureExtractor(object):\n  """"""SSD Feature Extractor definition.""""""\n\n  def __init__(self,\n               is_training,\n               depth_multiplier,\n               min_depth,\n               pad_to_multiple,\n               conv_hyperparams,\n               batch_norm_trainable=True,\n               reuse_weights=None,\n               use_explicit_padding=False,\n               use_depthwise=False):\n    """"""Constructor.\n\n    Args:\n      is_training: whether the network is in training mode.\n      depth_multiplier: float depth multiplier for feature extractor.\n      min_depth: minimum feature extractor depth.\n      pad_to_multiple: the nearest multiple to zero pad the input height and\n        width dimensions to.\n      conv_hyperparams: tf slim arg_scope for conv2d and separable_conv2d ops.\n      batch_norm_trainable: Whether to update batch norm parameters during\n        training or not. When training with a small batch size\n        (e.g. 1), it is desirable to disable batch norm update and use\n        pretrained batch norm params.\n      reuse_weights: whether to reuse variables. Default is None.\n      use_explicit_padding: Whether to use explicit padding when extracting\n        features. Default is False.\n      use_depthwise: Whether to use depthwise convolutions. Default is False.\n    """"""\n    self._is_training = is_training\n    self._depth_multiplier = depth_multiplier\n    self._min_depth = min_depth\n    self._pad_to_multiple = pad_to_multiple\n    self._conv_hyperparams = conv_hyperparams\n    self._batch_norm_trainable = batch_norm_trainable\n    self._reuse_weights = reuse_weights\n    self._use_explicit_padding = use_explicit_padding\n    self._use_depthwise = use_depthwise\n\n  @abstractmethod\n  def preprocess(self, resized_inputs):\n    """"""Preprocesses images for feature extraction (minus image resizing).\n\n    Args:\n      resized_inputs: a [batch, height, width, channels] float tensor\n        representing a batch of images.\n\n    Returns:\n      preprocessed_inputs: a [batch, height, width, channels] float tensor\n        representing a batch of images.\n      true_image_shapes: int32 tensor of shape [batch, 3] where each row is\n        of the form [height, width, channels] indicating the shapes\n        of true images in the resized images, as resized images can be padded\n        with zeros.\n    """"""\n    pass\n\n  @abstractmethod\n  def extract_features(self, preprocessed_inputs):\n    """"""Extracts features from preprocessed inputs.\n\n    This function is responsible for extracting feature maps from preprocessed\n    images.\n\n    Args:\n      preprocessed_inputs: a [batch, height, width, channels] float tensor\n        representing a batch of images.\n\n    Returns:\n      feature_maps: a list of tensors where the ith tensor has shape\n        [batch, height_i, width_i, depth_i]\n    """"""\n    pass\n\n\nclass SSDMetaArch(model.DetectionModel):\n  """"""SSD Meta-architecture definition.""""""\n\n  def __init__(self,\n               is_training,\n               anchor_generator,\n               box_predictor,\n               box_coder,\n               feature_extractor,\n               matcher,\n               region_similarity_calculator,\n               encode_background_as_zeros,\n               negative_class_weight,\n               image_resizer_fn,\n               non_max_suppression_fn,\n               score_conversion_fn,\n               classification_loss,\n               localization_loss,\n               classification_loss_weight,\n               localization_loss_weight,\n               normalize_loss_by_num_matches,\n               hard_example_miner,\n               add_summaries=True,\n               normalize_loc_loss_by_codesize=False):\n    """"""SSDMetaArch Constructor.\n\n    TODO(rathodv,jonathanhuang): group NMS parameters + score converter into\n    a class and loss parameters into a class and write config protos for\n    postprocessing and losses.\n\n    Args:\n      is_training: A boolean indicating whether the training version of the\n        computation graph should be constructed.\n      anchor_generator: an anchor_generator.AnchorGenerator object.\n      box_predictor: a box_predictor.BoxPredictor object.\n      box_coder: a box_coder.BoxCoder object.\n      feature_extractor: a SSDFeatureExtractor object.\n      matcher: a matcher.Matcher object.\n      region_similarity_calculator: a\n        region_similarity_calculator.RegionSimilarityCalculator object.\n      encode_background_as_zeros: boolean determining whether background\n        targets are to be encoded as an all zeros vector or a one-hot\n        vector (where background is the 0th class).\n      negative_class_weight: Weight for confidence loss of negative anchors.\n      image_resizer_fn: a callable for image resizing.  This callable always\n        takes a rank-3 image tensor (corresponding to a single image) and\n        returns a rank-3 image tensor, possibly with new spatial dimensions and\n        a 1-D tensor of shape [3] indicating shape of true image within\n        the resized image tensor as the resized image tensor could be padded.\n        See builders/image_resizer_builder.py.\n      non_max_suppression_fn: batch_multiclass_non_max_suppression\n        callable that takes `boxes`, `scores` and optional `clip_window`\n        inputs (with all other inputs already set) and returns a dictionary\n        hold tensors with keys: `detection_boxes`, `detection_scores`,\n        `detection_classes` and `num_detections`. See `post_processing.\n        batch_multiclass_non_max_suppression` for the type and shape of these\n        tensors.\n      score_conversion_fn: callable elementwise nonlinearity (that takes tensors\n        as inputs and returns tensors).  This is usually used to convert logits\n        to probabilities.\n      classification_loss: an object_detection.core.losses.Loss object.\n      localization_loss: a object_detection.core.losses.Loss object.\n      classification_loss_weight: float\n      localization_loss_weight: float\n      normalize_loss_by_num_matches: boolean\n      hard_example_miner: a losses.HardExampleMiner object (can be None)\n      add_summaries: boolean (default: True) controlling whether summary ops\n        should be added to tensorflow graph.\n      normalize_loc_loss_by_codesize: whether to normalize localization loss\n        by code size of the box encoder.\n    """"""\n    super(SSDMetaArch, self).__init__(num_classes=box_predictor.num_classes)\n    self._is_training = is_training\n\n    # Needed for fine-tuning from classification checkpoints whose\n    # variables do not have the feature extractor scope.\n    self._extract_features_scope = \'FeatureExtractor\'\n\n    self._anchor_generator = anchor_generator\n    self._box_predictor = box_predictor\n\n    self._box_coder = box_coder\n    self._feature_extractor = feature_extractor\n    self._matcher = matcher\n    self._region_similarity_calculator = region_similarity_calculator\n\n    # TODO(jonathanhuang): handle agnostic mode\n    # weights\n    unmatched_cls_target = None\n    unmatched_cls_target = tf.constant([1] + self.num_classes * [0],\n                                       tf.float32)\n    if encode_background_as_zeros:\n      unmatched_cls_target = tf.constant((self.num_classes + 1) * [0],\n                                         tf.float32)\n\n    self._target_assigner = target_assigner.TargetAssigner(\n        self._region_similarity_calculator,\n        self._matcher,\n        self._box_coder,\n        negative_class_weight=negative_class_weight,\n        unmatched_cls_target=unmatched_cls_target)\n\n    self._classification_loss = classification_loss\n    self._localization_loss = localization_loss\n    self._classification_loss_weight = classification_loss_weight\n    self._localization_loss_weight = localization_loss_weight\n    self._normalize_loss_by_num_matches = normalize_loss_by_num_matches\n    self._normalize_loc_loss_by_codesize = normalize_loc_loss_by_codesize\n    self._hard_example_miner = hard_example_miner\n\n    self._image_resizer_fn = image_resizer_fn\n    self._non_max_suppression_fn = non_max_suppression_fn\n    self._score_conversion_fn = score_conversion_fn\n\n    self._anchors = None\n    self._add_summaries = add_summaries\n\n  @property\n  def anchors(self):\n    if not self._anchors:\n      raise RuntimeError(\'anchors have not been constructed yet!\')\n    if not isinstance(self._anchors, box_list.BoxList):\n      raise RuntimeError(\'anchors should be a BoxList object, but is not.\')\n    return self._anchors\n\n  def preprocess(self, inputs):\n    """"""Feature-extractor specific preprocessing.\n\n    SSD meta architecture uses a default clip_window of [0, 0, 1, 1] during\n    post-processing. On calling `preprocess` method, clip_window gets updated\n    based on `true_image_shapes` returned by `image_resizer_fn`.\n\n    Args:\n      inputs: a [batch, height_in, width_in, channels] float tensor representing\n        a batch of images with values between 0 and 255.0.\n\n    Returns:\n      preprocessed_inputs: a [batch, height_out, width_out, channels] float\n        tensor representing a batch of images.\n      true_image_shapes: int32 tensor of shape [batch, 3] where each row is\n        of the form [height, width, channels] indicating the shapes\n        of true images in the resized images, as resized images can be padded\n        with zeros.\n\n    Raises:\n      ValueError: if inputs tensor does not have type tf.float32\n    """"""\n    if inputs.dtype is not tf.float32:\n      raise ValueError(\'`preprocess` expects a tf.float32 tensor\')\n    with tf.name_scope(\'Preprocessor\'):\n      # TODO(jonathanhuang): revisit whether to always use batch size as\n      # the number of parallel iterations vs allow for dynamic batching.\n      outputs = shape_utils.static_or_dynamic_map_fn(\n          self._image_resizer_fn,\n          elems=inputs,\n          dtype=[tf.float32, tf.int32])\n      resized_inputs = outputs[0]\n      true_image_shapes = outputs[1]\n\n      return (self._feature_extractor.preprocess(resized_inputs),\n              true_image_shapes)\n\n  def _compute_clip_window(self, preprocessed_images, true_image_shapes):\n    """"""Computes clip window to use during post_processing.\n\n    Computes a new clip window to use during post-processing based on\n    `resized_image_shapes` and `true_image_shapes` only if `preprocess` method\n    has been called. Otherwise returns a default clip window of [0, 0, 1, 1].\n\n    Args:\n      preprocessed_images: the [batch, height, width, channels] image\n          tensor.\n      true_image_shapes: int32 tensor of shape [batch, 3] where each row is\n        of the form [height, width, channels] indicating the shapes\n        of true images in the resized images, as resized images can be padded\n        with zeros. Or None if the clip window should cover the full image.\n\n    Returns:\n      a 2-D float32 tensor of the form [batch_size, 4] containing the clip\n      window for each image in the batch in normalized coordinates (relative to\n      the resized dimensions) where each clip window is of the form [ymin, xmin,\n      ymax, xmax] or a default clip window of [0, 0, 1, 1].\n\n    """"""\n    if true_image_shapes is None:\n      return tf.constant([0, 0, 1, 1], dtype=tf.float32)\n\n    resized_inputs_shape = shape_utils.combined_static_and_dynamic_shape(\n        preprocessed_images)\n    true_heights, true_widths, _ = tf.unstack(\n        tf.to_float(true_image_shapes), axis=1)\n    padded_height = tf.to_float(resized_inputs_shape[1])\n    padded_width = tf.to_float(resized_inputs_shape[2])\n    return tf.stack(\n        [\n            tf.zeros_like(true_heights),\n            tf.zeros_like(true_widths), true_heights / padded_height,\n            true_widths / padded_width\n        ],\n        axis=1)\n\n  def predict(self, preprocessed_inputs, true_image_shapes):\n    """"""Predicts unpostprocessed tensors from input tensor.\n\n    This function takes an input batch of images and runs it through the forward\n    pass of the network to yield unpostprocessesed predictions.\n\n    A side effect of calling the predict method is that self._anchors is\n    populated with a box_list.BoxList of anchors.  These anchors must be\n    constructed before the postprocess or loss functions can be called.\n\n    Args:\n      preprocessed_inputs: a [batch, height, width, channels] image tensor.\n      true_image_shapes: int32 tensor of shape [batch, 3] where each row is\n        of the form [height, width, channels] indicating the shapes\n        of true images in the resized images, as resized images can be padded\n        with zeros.\n\n    Returns:\n      prediction_dict: a dictionary holding ""raw"" prediction tensors:\n        1) preprocessed_inputs: the [batch, height, width, channels] image\n          tensor.\n        2) box_encodings: 4-D float tensor of shape [batch_size, num_anchors,\n          box_code_dimension] containing predicted boxes.\n        3) class_predictions_with_background: 3-D float tensor of shape\n          [batch_size, num_anchors, num_classes+1] containing class predictions\n          (logits) for each of the anchors.  Note that this tensor *includes*\n          background class predictions (at class index 0).\n        4) feature_maps: a list of tensors where the ith tensor has shape\n          [batch, height_i, width_i, depth_i].\n        5) anchors: 2-D float tensor of shape [num_anchors, 4] containing\n          the generated anchors in normalized coordinates.\n    """"""\n    with tf.variable_scope(None, self._extract_features_scope,\n                           [preprocessed_inputs]):\n      feature_maps = self._feature_extractor.extract_features(\n          preprocessed_inputs)\n    feature_map_spatial_dims = self._get_feature_map_spatial_dims(feature_maps)\n    image_shape = shape_utils.combined_static_and_dynamic_shape(\n        preprocessed_inputs)\n    self._anchors = box_list_ops.concatenate(\n        self._anchor_generator.generate(\n            feature_map_spatial_dims,\n            im_height=image_shape[1],\n            im_width=image_shape[2]))\n    prediction_dict = self._box_predictor.predict(\n        feature_maps, self._anchor_generator.num_anchors_per_location())\n    box_encodings = tf.squeeze(\n        tf.concat(prediction_dict[\'box_encodings\'], axis=1), axis=2)\n    class_predictions_with_background = tf.concat(\n        prediction_dict[\'class_predictions_with_background\'], axis=1)\n    predictions_dict = {\n        \'preprocessed_inputs\': preprocessed_inputs,\n        \'box_encodings\': box_encodings,\n        \'class_predictions_with_background\': class_predictions_with_background,\n        \'feature_maps\': feature_maps,\n        \'anchors\': self._anchors.get()\n    }\n    return predictions_dict\n\n  def _get_feature_map_spatial_dims(self, feature_maps):\n    """"""Return list of spatial dimensions for each feature map in a list.\n\n    Args:\n      feature_maps: a list of tensors where the ith tensor has shape\n          [batch, height_i, width_i, depth_i].\n\n    Returns:\n      a list of pairs (height, width) for each feature map in feature_maps\n    """"""\n    feature_map_shapes = [\n        shape_utils.combined_static_and_dynamic_shape(\n            feature_map) for feature_map in feature_maps\n    ]\n    return [(shape[1], shape[2]) for shape in feature_map_shapes]\n\n  def postprocess(self, prediction_dict, true_image_shapes):\n    """"""Converts prediction tensors to final detections.\n\n    This function converts raw predictions tensors to final detection results by\n    slicing off the background class, decoding box predictions and applying\n    non max suppression and clipping to the image window.\n\n    See base class for output format conventions.  Note also that by default,\n    scores are to be interpreted as logits, but if a score_conversion_fn is\n    used, then scores are remapped (and may thus have a different\n    interpretation).\n\n    Args:\n      prediction_dict: a dictionary holding prediction tensors with\n        1) preprocessed_inputs: a [batch, height, width, channels] image\n          tensor.\n        2) box_encodings: 3-D float tensor of shape [batch_size, num_anchors,\n          box_code_dimension] containing predicted boxes.\n        3) class_predictions_with_background: 3-D float tensor of shape\n          [batch_size, num_anchors, num_classes+1] containing class predictions\n          (logits) for each of the anchors.  Note that this tensor *includes*\n          background class predictions.\n      true_image_shapes: int32 tensor of shape [batch, 3] where each row is\n        of the form [height, width, channels] indicating the shapes\n        of true images in the resized images, as resized images can be padded\n        with zeros. Or None, if the clip window should cover the full image.\n\n    Returns:\n      detections: a dictionary containing the following fields\n        detection_boxes: [batch, max_detections, 4]\n        detection_scores: [batch, max_detections]\n        detection_classes: [batch, max_detections]\n        detection_keypoints: [batch, max_detections, num_keypoints, 2] (if\n          encoded in the prediction_dict \'box_encodings\')\n        num_detections: [batch]\n    Raises:\n      ValueError: if prediction_dict does not contain `box_encodings` or\n        `class_predictions_with_background` fields.\n    """"""\n    if (\'box_encodings\' not in prediction_dict or\n        \'class_predictions_with_background\' not in prediction_dict):\n      raise ValueError(\'prediction_dict does not contain expected entries.\')\n    with tf.name_scope(\'Postprocessor\'):\n      preprocessed_images = prediction_dict[\'preprocessed_inputs\']\n      box_encodings = prediction_dict[\'box_encodings\']\n      class_predictions = prediction_dict[\'class_predictions_with_background\']\n      detection_boxes, detection_keypoints = self._batch_decode(box_encodings)\n      detection_boxes = tf.expand_dims(detection_boxes, axis=2)\n\n      detection_scores_with_background = self._score_conversion_fn(\n          class_predictions)\n      detection_scores = tf.slice(detection_scores_with_background, [0, 0, 1],\n                                  [-1, -1, -1])\n      additional_fields = None\n\n      if detection_keypoints is not None:\n        additional_fields = {\n            fields.BoxListFields.keypoints: detection_keypoints}\n      (nmsed_boxes, nmsed_scores, nmsed_classes, _, nmsed_additional_fields,\n       num_detections) = self._non_max_suppression_fn(\n           detection_boxes,\n           detection_scores,\n           clip_window=self._compute_clip_window(\n               preprocessed_images, true_image_shapes),\n           additional_fields=additional_fields)\n      detection_dict = {\n          fields.DetectionResultFields.detection_boxes: nmsed_boxes,\n          fields.DetectionResultFields.detection_scores: nmsed_scores,\n          fields.DetectionResultFields.detection_classes: nmsed_classes,\n          fields.DetectionResultFields.num_detections:\n              tf.to_float(num_detections)\n      }\n      if (nmsed_additional_fields is not None and\n          fields.BoxListFields.keypoints in nmsed_additional_fields):\n        detection_dict[fields.DetectionResultFields.detection_keypoints] = (\n            nmsed_additional_fields[fields.BoxListFields.keypoints])\n      return detection_dict\n\n  def loss(self, prediction_dict, true_image_shapes, scope=None):\n    """"""Compute scalar loss tensors with respect to provided groundtruth.\n\n    Calling this function requires that groundtruth tensors have been\n    provided via the provide_groundtruth function.\n\n    Args:\n      prediction_dict: a dictionary holding prediction tensors with\n        1) box_encodings: 3-D float tensor of shape [batch_size, num_anchors,\n          box_code_dimension] containing predicted boxes.\n        2) class_predictions_with_background: 3-D float tensor of shape\n          [batch_size, num_anchors, num_classes+1] containing class predictions\n          (logits) for each of the anchors. Note that this tensor *includes*\n          background class predictions.\n      true_image_shapes: int32 tensor of shape [batch, 3] where each row is\n        of the form [height, width, channels] indicating the shapes\n        of true images in the resized images, as resized images can be padded\n        with zeros.\n      scope: Optional scope name.\n\n    Returns:\n      a dictionary mapping loss keys (`localization_loss` and\n        `classification_loss`) to scalar tensors representing corresponding loss\n        values.\n    """"""\n    with tf.name_scope(scope, \'Loss\', prediction_dict.values()):\n      keypoints = None\n      if self.groundtruth_has_field(fields.BoxListFields.keypoints):\n        keypoints = self.groundtruth_lists(fields.BoxListFields.keypoints)\n      weights = None\n      if self.groundtruth_has_field(fields.BoxListFields.weights):\n        weights = self.groundtruth_lists(fields.BoxListFields.weights)\n      (batch_cls_targets, batch_cls_weights, batch_reg_targets,\n       batch_reg_weights, match_list) = self._assign_targets(\n           self.groundtruth_lists(fields.BoxListFields.boxes),\n           self.groundtruth_lists(fields.BoxListFields.classes),\n           keypoints, weights)\n      if self._add_summaries:\n        self._summarize_target_assignment(\n            self.groundtruth_lists(fields.BoxListFields.boxes), match_list)\n      location_losses = self._localization_loss(\n          prediction_dict[\'box_encodings\'],\n          batch_reg_targets,\n          ignore_nan_targets=True,\n          weights=batch_reg_weights)\n      cls_losses = ops.reduce_sum_trailing_dimensions(\n          self._classification_loss(\n              prediction_dict[\'class_predictions_with_background\'],\n              batch_cls_targets,\n              weights=batch_cls_weights),\n          ndims=2)\n\n      if self._hard_example_miner:\n        (localization_loss, classification_loss) = self._apply_hard_mining(\n            location_losses, cls_losses, prediction_dict, match_list)\n        if self._add_summaries:\n          self._hard_example_miner.summarize()\n      else:\n        if self._add_summaries:\n          class_ids = tf.argmax(batch_cls_targets, axis=2)\n          flattened_class_ids = tf.reshape(class_ids, [-1])\n          flattened_classification_losses = tf.reshape(cls_losses, [-1])\n          self._summarize_anchor_classification_loss(\n              flattened_class_ids, flattened_classification_losses)\n        localization_loss = tf.reduce_sum(location_losses)\n        classification_loss = tf.reduce_sum(cls_losses)\n\n      # Optionally normalize by number of positive matches\n      normalizer = tf.constant(1.0, dtype=tf.float32)\n      if self._normalize_loss_by_num_matches:\n        normalizer = tf.maximum(tf.to_float(tf.reduce_sum(batch_reg_weights)),\n                                1.0)\n\n      localization_loss_normalizer = normalizer\n      if self._normalize_loc_loss_by_codesize:\n        localization_loss_normalizer *= self._box_coder.code_size\n      localization_loss = tf.multiply((self._localization_loss_weight /\n                                       localization_loss_normalizer),\n                                      localization_loss,\n                                      name=\'localization_loss\')\n      classification_loss = tf.multiply((self._classification_loss_weight /\n                                         normalizer), classification_loss,\n                                        name=\'classification_loss\')\n\n      loss_dict = {\n          localization_loss.op.name: localization_loss,\n          classification_loss.op.name: classification_loss\n      }\n    return loss_dict\n\n  def _summarize_anchor_classification_loss(self, class_ids, cls_losses):\n    positive_indices = tf.where(tf.greater(class_ids, 0))\n    positive_anchor_cls_loss = tf.squeeze(\n        tf.gather(cls_losses, positive_indices), axis=1)\n    visualization_utils.add_cdf_image_summary(positive_anchor_cls_loss,\n                                              \'PositiveAnchorLossCDF\')\n    negative_indices = tf.where(tf.equal(class_ids, 0))\n    negative_anchor_cls_loss = tf.squeeze(\n        tf.gather(cls_losses, negative_indices), axis=1)\n    visualization_utils.add_cdf_image_summary(negative_anchor_cls_loss,\n                                              \'NegativeAnchorLossCDF\')\n\n  def _assign_targets(self, groundtruth_boxes_list, groundtruth_classes_list,\n                      groundtruth_keypoints_list=None,\n                      groundtruth_weights_list=None):\n    """"""Assign groundtruth targets.\n\n    Adds a background class to each one-hot encoding of groundtruth classes\n    and uses target assigner to obtain regression and classification targets.\n\n    Args:\n      groundtruth_boxes_list: a list of 2-D tensors of shape [num_boxes, 4]\n        containing coordinates of the groundtruth boxes.\n          Groundtruth boxes are provided in [y_min, x_min, y_max, x_max]\n          format and assumed to be normalized and clipped\n          relative to the image window with y_min <= y_max and x_min <= x_max.\n      groundtruth_classes_list: a list of 2-D one-hot (or k-hot) tensors of\n        shape [num_boxes, num_classes] containing the class targets with the 0th\n        index assumed to map to the first non-background class.\n      groundtruth_keypoints_list: (optional) a list of 3-D tensors of shape\n        [num_boxes, num_keypoints, 2]\n      groundtruth_weights_list: A list of 1-D tf.float32 tensors of shape\n        [num_boxes] containing weights for groundtruth boxes.\n\n    Returns:\n      batch_cls_targets: a tensor with shape [batch_size, num_anchors,\n        num_classes],\n      batch_cls_weights: a tensor with shape [batch_size, num_anchors],\n      batch_reg_targets: a tensor with shape [batch_size, num_anchors,\n        box_code_dimension]\n      batch_reg_weights: a tensor with shape [batch_size, num_anchors],\n      match_list: a list of matcher.Match objects encoding the match between\n        anchors and groundtruth boxes for each image of the batch,\n        with rows of the Match objects corresponding to groundtruth boxes\n        and columns corresponding to anchors.\n    """"""\n    groundtruth_boxlists = [\n        box_list.BoxList(boxes) for boxes in groundtruth_boxes_list\n    ]\n    groundtruth_classes_with_background_list = [\n        tf.pad(one_hot_encoding, [[0, 0], [1, 0]], mode=\'CONSTANT\')\n        for one_hot_encoding in groundtruth_classes_list\n    ]\n    if groundtruth_keypoints_list is not None:\n      for boxlist, keypoints in zip(\n          groundtruth_boxlists, groundtruth_keypoints_list):\n        boxlist.add_field(fields.BoxListFields.keypoints, keypoints)\n    return target_assigner.batch_assign_targets(\n        self._target_assigner, self.anchors, groundtruth_boxlists,\n        groundtruth_classes_with_background_list, groundtruth_weights_list)\n\n  def _summarize_target_assignment(self, groundtruth_boxes_list, match_list):\n    """"""Creates tensorflow summaries for the input boxes and anchors.\n\n    This function creates four summaries corresponding to the average\n    number (over images in a batch) of (1) groundtruth boxes, (2) anchors\n    marked as positive, (3) anchors marked as negative, and (4) anchors marked\n    as ignored.\n\n    Args:\n      groundtruth_boxes_list: a list of 2-D tensors of shape [num_boxes, 4]\n        containing corners of the groundtruth boxes.\n      match_list: a list of matcher.Match objects encoding the match between\n        anchors and groundtruth boxes for each image of the batch,\n        with rows of the Match objects corresponding to groundtruth boxes\n        and columns corresponding to anchors.\n    """"""\n    num_boxes_per_image = tf.stack(\n        [tf.shape(x)[0] for x in groundtruth_boxes_list])\n    pos_anchors_per_image = tf.stack(\n        [match.num_matched_columns() for match in match_list])\n    neg_anchors_per_image = tf.stack(\n        [match.num_unmatched_columns() for match in match_list])\n    ignored_anchors_per_image = tf.stack(\n        [match.num_ignored_columns() for match in match_list])\n    tf.summary.scalar(\'AvgNumGroundtruthBoxesPerImage\',\n                      tf.reduce_mean(tf.to_float(num_boxes_per_image)),\n                      family=\'TargetAssignment\')\n    tf.summary.scalar(\'AvgNumPositiveAnchorsPerImage\',\n                      tf.reduce_mean(tf.to_float(pos_anchors_per_image)),\n                      family=\'TargetAssignment\')\n    tf.summary.scalar(\'AvgNumNegativeAnchorsPerImage\',\n                      tf.reduce_mean(tf.to_float(neg_anchors_per_image)),\n                      family=\'TargetAssignment\')\n    tf.summary.scalar(\'AvgNumIgnoredAnchorsPerImage\',\n                      tf.reduce_mean(tf.to_float(ignored_anchors_per_image)),\n                      family=\'TargetAssignment\')\n\n  def _apply_hard_mining(self, location_losses, cls_losses, prediction_dict,\n                         match_list):\n    """"""Applies hard mining to anchorwise losses.\n\n    Args:\n      location_losses: Float tensor of shape [batch_size, num_anchors]\n        representing anchorwise location losses.\n      cls_losses: Float tensor of shape [batch_size, num_anchors]\n        representing anchorwise classification losses.\n      prediction_dict: p a dictionary holding prediction tensors with\n        1) box_encodings: 3-D float tensor of shape [batch_size, num_anchors,\n          box_code_dimension] containing predicted boxes.\n        2) class_predictions_with_background: 3-D float tensor of shape\n          [batch_size, num_anchors, num_classes+1] containing class predictions\n          (logits) for each of the anchors.  Note that this tensor *includes*\n          background class predictions.\n      match_list: a list of matcher.Match objects encoding the match between\n        anchors and groundtruth boxes for each image of the batch,\n        with rows of the Match objects corresponding to groundtruth boxes\n        and columns corresponding to anchors.\n\n    Returns:\n      mined_location_loss: a float scalar with sum of localization losses from\n        selected hard examples.\n      mined_cls_loss: a float scalar with sum of classification losses from\n        selected hard examples.\n    """"""\n    class_predictions = tf.slice(\n        prediction_dict[\'class_predictions_with_background\'], [0, 0,\n                                                               1], [-1, -1, -1])\n\n    decoded_boxes, _ = self._batch_decode(prediction_dict[\'box_encodings\'])\n    decoded_box_tensors_list = tf.unstack(decoded_boxes)\n    class_prediction_list = tf.unstack(class_predictions)\n    decoded_boxlist_list = []\n    for box_location, box_score in zip(decoded_box_tensors_list,\n                                       class_prediction_list):\n      decoded_boxlist = box_list.BoxList(box_location)\n      decoded_boxlist.add_field(\'scores\', box_score)\n      decoded_boxlist_list.append(decoded_boxlist)\n    return self._hard_example_miner(\n        location_losses=location_losses,\n        cls_losses=cls_losses,\n        decoded_boxlist_list=decoded_boxlist_list,\n        match_list=match_list)\n\n  def _batch_decode(self, box_encodings):\n    """"""Decodes a batch of box encodings with respect to the anchors.\n\n    Args:\n      box_encodings: A float32 tensor of shape\n        [batch_size, num_anchors, box_code_size] containing box encodings.\n\n    Returns:\n      decoded_boxes: A float32 tensor of shape\n        [batch_size, num_anchors, 4] containing the decoded boxes.\n      decoded_keypoints: A float32 tensor of shape\n        [batch_size, num_anchors, num_keypoints, 2] containing the decoded\n        keypoints if present in the input `box_encodings`, None otherwise.\n    """"""\n    combined_shape = shape_utils.combined_static_and_dynamic_shape(\n        box_encodings)\n    batch_size = combined_shape[0]\n    tiled_anchor_boxes = tf.tile(\n        tf.expand_dims(self.anchors.get(), 0), [batch_size, 1, 1])\n    tiled_anchors_boxlist = box_list.BoxList(\n        tf.reshape(tiled_anchor_boxes, [-1, 4]))\n    decoded_boxes = self._box_coder.decode(\n        tf.reshape(box_encodings, [-1, self._box_coder.code_size]),\n        tiled_anchors_boxlist)\n    decoded_keypoints = None\n    if decoded_boxes.has_field(fields.BoxListFields.keypoints):\n      decoded_keypoints = decoded_boxes.get_field(\n          fields.BoxListFields.keypoints)\n      num_keypoints = decoded_keypoints.get_shape()[1]\n      decoded_keypoints = tf.reshape(\n          decoded_keypoints,\n          tf.stack([combined_shape[0], combined_shape[1], num_keypoints, 2]))\n    decoded_boxes = tf.reshape(decoded_boxes.get(), tf.stack(\n        [combined_shape[0], combined_shape[1], 4]))\n    return decoded_boxes, decoded_keypoints\n\n  def restore_map(self,\n                  fine_tune_checkpoint_type=\'detection\',\n                  load_all_detection_checkpoint_vars=False):\n    """"""Returns a map of variables to load from a foreign checkpoint.\n\n    See parent class for details.\n\n    Args:\n      fine_tune_checkpoint_type: whether to restore from a full detection\n        checkpoint (with compatible variable names) or to restore from a\n        classification checkpoint for initialization prior to training.\n        Valid values: `detection`, `classification`. Default \'detection\'.\n      load_all_detection_checkpoint_vars: whether to load all variables (when\n         `from_detection_checkpoint` is True). If False, only variables within\n         the appropriate scopes are included. Default False.\n\n    Returns:\n      A dict mapping variable names (to load from a checkpoint) to variables in\n      the model graph.\n    Raises:\n      ValueError: if fine_tune_checkpoint_type is neither `classification`\n        nor `detection`.\n    """"""\n    if fine_tune_checkpoint_type not in [\'detection\', \'classification\']:\n      raise ValueError(\'Not supported fine_tune_checkpoint_type: {}\'.format(\n          fine_tune_checkpoint_type))\n    variables_to_restore = {}\n    for variable in tf.global_variables():\n      var_name = variable.op.name\n      if (fine_tune_checkpoint_type == \'detection\' and\n          load_all_detection_checkpoint_vars):\n        variables_to_restore[var_name] = variable\n      else:\n        if var_name.startswith(self._extract_features_scope):\n          if fine_tune_checkpoint_type == \'classification\':\n            var_name = (\n                re.split(\'^\' + self._extract_features_scope + \'/\',\n                         var_name)[-1])\n          variables_to_restore[var_name] = variable\n\n    return variables_to_restore\n'"
src/object_detection/meta_architectures/ssd_meta_arch_test.py,39,"b'# Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n""""""Tests for object_detection.meta_architectures.ssd_meta_arch.""""""\nimport functools\nimport numpy as np\nimport tensorflow as tf\n\nfrom object_detection.core import anchor_generator\nfrom object_detection.core import box_list\nfrom object_detection.core import losses\nfrom object_detection.core import post_processing\nfrom object_detection.core import region_similarity_calculator as sim_calc\nfrom object_detection.meta_architectures import ssd_meta_arch\nfrom object_detection.utils import test_case\nfrom object_detection.utils import test_utils\n\nslim = tf.contrib.slim\n\n\nclass FakeSSDFeatureExtractor(ssd_meta_arch.SSDFeatureExtractor):\n\n  def __init__(self):\n    super(FakeSSDFeatureExtractor, self).__init__(\n        is_training=True,\n        depth_multiplier=0,\n        min_depth=0,\n        pad_to_multiple=1,\n        batch_norm_trainable=True,\n        conv_hyperparams=None)\n\n  def preprocess(self, resized_inputs):\n    return tf.identity(resized_inputs)\n\n  def extract_features(self, preprocessed_inputs):\n    with tf.variable_scope(\'mock_model\'):\n      features = slim.conv2d(inputs=preprocessed_inputs, num_outputs=32,\n                             kernel_size=1, scope=\'layer1\')\n      return [features]\n\n\nclass MockAnchorGenerator2x2(anchor_generator.AnchorGenerator):\n  """"""Sets up a simple 2x2 anchor grid on the unit square.""""""\n\n  def name_scope(self):\n    return \'MockAnchorGenerator\'\n\n  def num_anchors_per_location(self):\n    return [1]\n\n  def _generate(self, feature_map_shape_list, im_height, im_width):\n    return [box_list.BoxList(\n        tf.constant([[0, 0, .5, .5],\n                     [0, .5, .5, 1],\n                     [.5, 0, 1, .5],\n                     [1., 1., 1.5, 1.5]  # Anchor that is outside clip_window.\n                    ], tf.float32))]\n\n  def num_anchors(self):\n    return 4\n\n\ndef _get_value_for_matching_key(dictionary, suffix):\n  for key in dictionary.keys():\n    if key.endswith(suffix):\n      return dictionary[key]\n  raise ValueError(\'key not found {}\'.format(suffix))\n\n\nclass SsdMetaArchTest(test_case.TestCase):\n\n  def _create_model(self, apply_hard_mining=True,\n                    normalize_loc_loss_by_codesize=False):\n    is_training = False\n    num_classes = 1\n    mock_anchor_generator = MockAnchorGenerator2x2()\n    mock_box_predictor = test_utils.MockBoxPredictor(\n        is_training, num_classes)\n    mock_box_coder = test_utils.MockBoxCoder()\n    fake_feature_extractor = FakeSSDFeatureExtractor()\n    mock_matcher = test_utils.MockMatcher()\n    region_similarity_calculator = sim_calc.IouSimilarity()\n    encode_background_as_zeros = False\n    def image_resizer_fn(image):\n      return [tf.identity(image), tf.shape(image)]\n\n    classification_loss = losses.WeightedSigmoidClassificationLoss()\n    localization_loss = losses.WeightedSmoothL1LocalizationLoss()\n    non_max_suppression_fn = functools.partial(\n        post_processing.batch_multiclass_non_max_suppression,\n        score_thresh=-20.0,\n        iou_thresh=1.0,\n        max_size_per_class=5,\n        max_total_size=5)\n    classification_loss_weight = 1.0\n    localization_loss_weight = 1.0\n    negative_class_weight = 1.0\n    normalize_loss_by_num_matches = False\n\n    hard_example_miner = None\n    if apply_hard_mining:\n      # This hard example miner is expected to be a no-op.\n      hard_example_miner = losses.HardExampleMiner(\n          num_hard_examples=None,\n          iou_threshold=1.0)\n\n    code_size = 4\n    model = ssd_meta_arch.SSDMetaArch(\n        is_training, mock_anchor_generator, mock_box_predictor, mock_box_coder,\n        fake_feature_extractor, mock_matcher, region_similarity_calculator,\n        encode_background_as_zeros, negative_class_weight, image_resizer_fn,\n        non_max_suppression_fn, tf.identity, classification_loss,\n        localization_loss, classification_loss_weight, localization_loss_weight,\n        normalize_loss_by_num_matches, hard_example_miner, add_summaries=False,\n        normalize_loc_loss_by_codesize=normalize_loc_loss_by_codesize)\n    return model, num_classes, mock_anchor_generator.num_anchors(), code_size\n\n  def test_preprocess_preserves_shapes_with_dynamic_input_image(self):\n    image_shapes = [(3, None, None, 3),\n                    (None, 10, 10, 3),\n                    (None, None, None, 3)]\n    model, _, _, _ = self._create_model()\n    for image_shape in image_shapes:\n      image_placeholder = tf.placeholder(tf.float32, shape=image_shape)\n      preprocessed_inputs, _ = model.preprocess(image_placeholder)\n      self.assertAllEqual(preprocessed_inputs.shape.as_list(), image_shape)\n\n  def test_preprocess_preserves_shape_with_static_input_image(self):\n    def graph_fn(input_image):\n      model, _, _, _ = self._create_model()\n      return model.preprocess(input_image)\n    input_image = np.random.rand(2, 3, 3, 3).astype(np.float32)\n    preprocessed_inputs, _ = self.execute(graph_fn, [input_image])\n    self.assertAllEqual(preprocessed_inputs.shape, [2, 3, 3, 3])\n\n  def test_predict_result_shapes_on_image_with_dynamic_shape(self):\n    batch_size = 3\n    image_size = 2\n    input_shapes = [(None, image_size, image_size, 3),\n                    (batch_size, None, None, 3),\n                    (None, None, None, 3)]\n\n    for input_shape in input_shapes:\n      tf_graph = tf.Graph()\n      with tf_graph.as_default():\n        model, num_classes, num_anchors, code_size = self._create_model()\n        preprocessed_input_placeholder = tf.placeholder(tf.float32,\n                                                        shape=input_shape)\n        prediction_dict = model.predict(\n            preprocessed_input_placeholder, true_image_shapes=None)\n\n        self.assertTrue(\'box_encodings\' in prediction_dict)\n        self.assertTrue(\'class_predictions_with_background\' in prediction_dict)\n        self.assertTrue(\'feature_maps\' in prediction_dict)\n        self.assertTrue(\'anchors\' in prediction_dict)\n\n        init_op = tf.global_variables_initializer()\n      with self.test_session(graph=tf_graph) as sess:\n        sess.run(init_op)\n        prediction_out = sess.run(prediction_dict,\n                                  feed_dict={\n                                      preprocessed_input_placeholder:\n                                      np.random.uniform(\n                                          size=(batch_size, 2, 2, 3))})\n      expected_box_encodings_shape_out = (batch_size, num_anchors, code_size)\n      expected_class_predictions_with_background_shape_out = (batch_size,\n                                                              num_anchors,\n                                                              num_classes + 1)\n\n      self.assertAllEqual(prediction_out[\'box_encodings\'].shape,\n                          expected_box_encodings_shape_out)\n      self.assertAllEqual(\n          prediction_out[\'class_predictions_with_background\'].shape,\n          expected_class_predictions_with_background_shape_out)\n\n  def test_predict_result_shapes_on_image_with_static_shape(self):\n\n    with tf.Graph().as_default():\n      _, num_classes, num_anchors, code_size = self._create_model()\n\n    def graph_fn(input_image):\n      model, _, _, _ = self._create_model()\n      predictions = model.predict(input_image, true_image_shapes=None)\n      return (predictions[\'box_encodings\'],\n              predictions[\'class_predictions_with_background\'],\n              predictions[\'feature_maps\'],\n              predictions[\'anchors\'])\n    batch_size = 3\n    image_size = 2\n    channels = 3\n    input_image = np.random.rand(batch_size, image_size, image_size,\n                                 channels).astype(np.float32)\n    expected_box_encodings_shape = (batch_size, num_anchors, code_size)\n    expected_class_predictions_shape = (batch_size, num_anchors, num_classes+1)\n    (box_encodings, class_predictions, _, _) = self.execute(graph_fn,\n                                                            [input_image])\n    self.assertAllEqual(box_encodings.shape, expected_box_encodings_shape)\n    self.assertAllEqual(class_predictions.shape,\n                        expected_class_predictions_shape)\n\n  def test_postprocess_results_are_correct(self):\n    batch_size = 2\n    image_size = 2\n    input_shapes = [(batch_size, image_size, image_size, 3),\n                    (None, image_size, image_size, 3),\n                    (batch_size, None, None, 3),\n                    (None, None, None, 3)]\n\n    expected_boxes = np.array([[[0, 0, .5, .5],\n                                [0, .5, .5, 1],\n                                [.5, 0, 1, .5],\n                                [0, 0, 0, 0],   # pruned prediction\n                                [0, 0, 0, 0]],  # padding\n                               [[0, 0, .5, .5],\n                                [0, .5, .5, 1],\n                                [.5, 0, 1, .5],\n                                [0, 0, 0, 0],  # pruned prediction\n                                [0, 0, 0, 0]]  # padding\n                              ])\n    expected_scores = np.array([[0, 0, 0, 0, 0],\n                                [0, 0, 0, 0, 0]])\n    expected_classes = np.array([[0, 0, 0, 0, 0],\n                                 [0, 0, 0, 0, 0]])\n    expected_num_detections = np.array([3, 3])\n\n    for input_shape in input_shapes:\n      tf_graph = tf.Graph()\n      with tf_graph.as_default():\n        model, _, _, _ = self._create_model()\n        input_placeholder = tf.placeholder(tf.float32, shape=input_shape)\n        preprocessed_inputs, true_image_shapes = model.preprocess(\n            input_placeholder)\n        prediction_dict = model.predict(preprocessed_inputs,\n                                        true_image_shapes)\n        detections = model.postprocess(prediction_dict, true_image_shapes)\n        self.assertTrue(\'detection_boxes\' in detections)\n        self.assertTrue(\'detection_scores\' in detections)\n        self.assertTrue(\'detection_classes\' in detections)\n        self.assertTrue(\'num_detections\' in detections)\n        init_op = tf.global_variables_initializer()\n      with self.test_session(graph=tf_graph) as sess:\n        sess.run(init_op)\n        detections_out = sess.run(detections,\n                                  feed_dict={\n                                      input_placeholder:\n                                      np.random.uniform(\n                                          size=(batch_size, 2, 2, 3))})\n      self.assertAllClose(detections_out[\'detection_boxes\'], expected_boxes)\n      self.assertAllClose(detections_out[\'detection_scores\'], expected_scores)\n      self.assertAllClose(detections_out[\'detection_classes\'], expected_classes)\n      self.assertAllClose(detections_out[\'num_detections\'],\n                          expected_num_detections)\n\n  def test_loss_results_are_correct(self):\n\n    with tf.Graph().as_default():\n      _, num_classes, num_anchors, _ = self._create_model()\n    def graph_fn(preprocessed_tensor, groundtruth_boxes1, groundtruth_boxes2,\n                 groundtruth_classes1, groundtruth_classes2):\n      groundtruth_boxes_list = [groundtruth_boxes1, groundtruth_boxes2]\n      groundtruth_classes_list = [groundtruth_classes1, groundtruth_classes2]\n      model, _, _, _ = self._create_model(apply_hard_mining=False)\n      model.provide_groundtruth(groundtruth_boxes_list,\n                                groundtruth_classes_list)\n      prediction_dict = model.predict(preprocessed_tensor,\n                                      true_image_shapes=None)\n      loss_dict = model.loss(prediction_dict, true_image_shapes=None)\n      return (\n          _get_value_for_matching_key(loss_dict, \'Loss/localization_loss\'),\n          _get_value_for_matching_key(loss_dict, \'Loss/classification_loss\'))\n\n    batch_size = 2\n    preprocessed_input = np.random.rand(batch_size, 2, 2, 3).astype(np.float32)\n    groundtruth_boxes1 = np.array([[0, 0, .5, .5]], dtype=np.float32)\n    groundtruth_boxes2 = np.array([[0, 0, .5, .5]], dtype=np.float32)\n    groundtruth_classes1 = np.array([[1]], dtype=np.float32)\n    groundtruth_classes2 = np.array([[1]], dtype=np.float32)\n    expected_localization_loss = 0.0\n    expected_classification_loss = (batch_size * num_anchors\n                                    * (num_classes+1) * np.log(2.0))\n    (localization_loss,\n     classification_loss) = self.execute(graph_fn, [preprocessed_input,\n                                                    groundtruth_boxes1,\n                                                    groundtruth_boxes2,\n                                                    groundtruth_classes1,\n                                                    groundtruth_classes2])\n    self.assertAllClose(localization_loss, expected_localization_loss)\n    self.assertAllClose(classification_loss, expected_classification_loss)\n\n  def test_loss_results_are_correct_with_normalize_by_codesize_true(self):\n\n    with tf.Graph().as_default():\n      _, _, _, _ = self._create_model()\n    def graph_fn(preprocessed_tensor, groundtruth_boxes1, groundtruth_boxes2,\n                 groundtruth_classes1, groundtruth_classes2):\n      groundtruth_boxes_list = [groundtruth_boxes1, groundtruth_boxes2]\n      groundtruth_classes_list = [groundtruth_classes1, groundtruth_classes2]\n      model, _, _, _ = self._create_model(apply_hard_mining=False,\n                                          normalize_loc_loss_by_codesize=True)\n      model.provide_groundtruth(groundtruth_boxes_list,\n                                groundtruth_classes_list)\n      prediction_dict = model.predict(preprocessed_tensor,\n                                      true_image_shapes=None)\n      loss_dict = model.loss(prediction_dict, true_image_shapes=None)\n      return (_get_value_for_matching_key(loss_dict, \'Loss/localization_loss\'),)\n\n    batch_size = 2\n    preprocessed_input = np.random.rand(batch_size, 2, 2, 3).astype(np.float32)\n    groundtruth_boxes1 = np.array([[0, 0, 1, 1]], dtype=np.float32)\n    groundtruth_boxes2 = np.array([[0, 0, 1, 1]], dtype=np.float32)\n    groundtruth_classes1 = np.array([[1]], dtype=np.float32)\n    groundtruth_classes2 = np.array([[1]], dtype=np.float32)\n    expected_localization_loss = 0.5 / 4\n    localization_loss = self.execute(graph_fn, [preprocessed_input,\n                                                groundtruth_boxes1,\n                                                groundtruth_boxes2,\n                                                groundtruth_classes1,\n                                                groundtruth_classes2])\n    self.assertAllClose(localization_loss, expected_localization_loss)\n\n  def test_loss_results_are_correct_with_hard_example_mining(self):\n\n    with tf.Graph().as_default():\n      _, num_classes, num_anchors, _ = self._create_model()\n    def graph_fn(preprocessed_tensor, groundtruth_boxes1, groundtruth_boxes2,\n                 groundtruth_classes1, groundtruth_classes2):\n      groundtruth_boxes_list = [groundtruth_boxes1, groundtruth_boxes2]\n      groundtruth_classes_list = [groundtruth_classes1, groundtruth_classes2]\n      model, _, _, _ = self._create_model()\n      model.provide_groundtruth(groundtruth_boxes_list,\n                                groundtruth_classes_list)\n      prediction_dict = model.predict(preprocessed_tensor,\n                                      true_image_shapes=None)\n      loss_dict = model.loss(prediction_dict, true_image_shapes=None)\n      return (\n          _get_value_for_matching_key(loss_dict, \'Loss/localization_loss\'),\n          _get_value_for_matching_key(loss_dict, \'Loss/classification_loss\'))\n\n    batch_size = 2\n    preprocessed_input = np.random.rand(batch_size, 2, 2, 3).astype(np.float32)\n    groundtruth_boxes1 = np.array([[0, 0, .5, .5]], dtype=np.float32)\n    groundtruth_boxes2 = np.array([[0, 0, .5, .5]], dtype=np.float32)\n    groundtruth_classes1 = np.array([[1]], dtype=np.float32)\n    groundtruth_classes2 = np.array([[1]], dtype=np.float32)\n    expected_localization_loss = 0.0\n    expected_classification_loss = (batch_size * num_anchors\n                                    * (num_classes+1) * np.log(2.0))\n    (localization_loss, classification_loss) = self.execute_cpu(\n        graph_fn, [\n            preprocessed_input, groundtruth_boxes1, groundtruth_boxes2,\n            groundtruth_classes1, groundtruth_classes2\n        ])\n    self.assertAllClose(localization_loss, expected_localization_loss)\n    self.assertAllClose(classification_loss, expected_classification_loss)\n\n  def test_restore_map_for_detection_ckpt(self):\n    model, _, _, _ = self._create_model()\n    model.predict(tf.constant(np.array([[[0, 0], [1, 1]], [[1, 0], [0, 1]]],\n                                       dtype=np.float32)),\n                  true_image_shapes=None)\n    init_op = tf.global_variables_initializer()\n    saver = tf.train.Saver()\n    save_path = self.get_temp_dir()\n    with self.test_session() as sess:\n      sess.run(init_op)\n      saved_model_path = saver.save(sess, save_path)\n      var_map = model.restore_map(\n          fine_tune_checkpoint_type=\'detection\',\n          load_all_detection_checkpoint_vars=False)\n      self.assertIsInstance(var_map, dict)\n      saver = tf.train.Saver(var_map)\n      saver.restore(sess, saved_model_path)\n      for var in sess.run(tf.report_uninitialized_variables()):\n        self.assertNotIn(\'FeatureExtractor\', var)\n\n  def test_restore_map_for_classification_ckpt(self):\n    # Define mock tensorflow classification graph and save variables.\n    test_graph_classification = tf.Graph()\n    with test_graph_classification.as_default():\n      image = tf.placeholder(dtype=tf.float32, shape=[1, 20, 20, 3])\n      with tf.variable_scope(\'mock_model\'):\n        net = slim.conv2d(image, num_outputs=32, kernel_size=1, scope=\'layer1\')\n        slim.conv2d(net, num_outputs=3, kernel_size=1, scope=\'layer2\')\n\n      init_op = tf.global_variables_initializer()\n      saver = tf.train.Saver()\n      save_path = self.get_temp_dir()\n      with self.test_session(graph=test_graph_classification) as sess:\n        sess.run(init_op)\n        saved_model_path = saver.save(sess, save_path)\n\n    # Create tensorflow detection graph and load variables from\n    # classification checkpoint.\n    test_graph_detection = tf.Graph()\n    with test_graph_detection.as_default():\n      model, _, _, _ = self._create_model()\n      inputs_shape = [2, 2, 2, 3]\n      inputs = tf.to_float(tf.random_uniform(\n          inputs_shape, minval=0, maxval=255, dtype=tf.int32))\n      preprocessed_inputs, true_image_shapes = model.preprocess(inputs)\n      prediction_dict = model.predict(preprocessed_inputs, true_image_shapes)\n      model.postprocess(prediction_dict, true_image_shapes)\n      another_variable = tf.Variable([17.0], name=\'another_variable\')  # pylint: disable=unused-variable\n      var_map = model.restore_map(fine_tune_checkpoint_type=\'classification\')\n      self.assertNotIn(\'another_variable\', var_map)\n      self.assertIsInstance(var_map, dict)\n      saver = tf.train.Saver(var_map)\n      with self.test_session(graph=test_graph_detection) as sess:\n        saver.restore(sess, saved_model_path)\n        for var in sess.run(tf.report_uninitialized_variables()):\n          self.assertNotIn(\'FeatureExtractor\', var)\n\n  def test_load_all_det_checkpoint_vars(self):\n    test_graph_detection = tf.Graph()\n    with test_graph_detection.as_default():\n      model, _, _, _ = self._create_model()\n      inputs_shape = [2, 2, 2, 3]\n      inputs = tf.to_float(\n          tf.random_uniform(inputs_shape, minval=0, maxval=255, dtype=tf.int32))\n      preprocessed_inputs, true_image_shapes = model.preprocess(inputs)\n      prediction_dict = model.predict(preprocessed_inputs, true_image_shapes)\n      model.postprocess(prediction_dict, true_image_shapes)\n      another_variable = tf.Variable([17.0], name=\'another_variable\')  # pylint: disable=unused-variable\n      var_map = model.restore_map(\n          fine_tune_checkpoint_type=\'detection\',\n          load_all_detection_checkpoint_vars=True)\n      self.assertIsInstance(var_map, dict)\n      self.assertIn(\'another_variable\', var_map)\n\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
src/object_detection/metrics/__init__.py,0,b''
src/object_detection/metrics/coco_evaluation.py,16,"b'# Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Class for evaluating object detections with COCO metrics.""""""\nimport numpy as np\nimport tensorflow as tf\n\nfrom object_detection.core import standard_fields\nfrom object_detection.metrics import coco_tools\nfrom object_detection.utils import object_detection_evaluation\n\n\nclass CocoDetectionEvaluator(object_detection_evaluation.DetectionEvaluator):\n  """"""Class to evaluate COCO detection metrics.""""""\n\n  def __init__(self,\n               categories,\n               include_metrics_per_category=False,\n               all_metrics_per_category=False):\n    """"""Constructor.\n\n    Args:\n      categories: A list of dicts, each of which has the following keys -\n        \'id\': (required) an integer id uniquely identifying this category.\n        \'name\': (required) string representing category name e.g., \'cat\', \'dog\'.\n      include_metrics_per_category: If True, include metrics for each category.\n      all_metrics_per_category: Whether to include all the summary metrics for\n        each category in per_category_ap. Be careful with setting it to true if\n        you have more than handful of categories, because it will pollute\n        your mldash.\n    """"""\n    super(CocoDetectionEvaluator, self).__init__(categories)\n    # _image_ids is a dictionary that maps unique image ids to Booleans which\n    # indicate whether a corresponding detection has been added.\n    self._image_ids = {}\n    self._groundtruth_list = []\n    self._detection_boxes_list = []\n    self._category_id_set = set([cat[\'id\'] for cat in self._categories])\n    self._annotation_id = 1\n    self._metrics = None\n    self._include_metrics_per_category = include_metrics_per_category\n    self._all_metrics_per_category = all_metrics_per_category\n\n  def clear(self):\n    """"""Clears the state to prepare for a fresh evaluation.""""""\n    self._image_ids.clear()\n    self._groundtruth_list = []\n    self._detection_boxes_list = []\n\n  def add_single_ground_truth_image_info(self,\n                                         image_id,\n                                         groundtruth_dict):\n    """"""Adds groundtruth for a single image to be used for evaluation.\n\n    If the image has already been added, a warning is logged, and groundtruth is\n    ignored.\n\n    Args:\n      image_id: A unique string/integer identifier for the image.\n      groundtruth_dict: A dictionary containing -\n        InputDataFields.groundtruth_boxes: float32 numpy array of shape\n          [num_boxes, 4] containing `num_boxes` groundtruth boxes of the format\n          [ymin, xmin, ymax, xmax] in absolute image coordinates.\n        InputDataFields.groundtruth_classes: integer numpy array of shape\n          [num_boxes] containing 1-indexed groundtruth classes for the boxes.\n        InputDataFields.groundtruth_is_crowd (optional): integer numpy array of\n          shape [num_boxes] containing iscrowd flag for groundtruth boxes.\n    """"""\n    if image_id in self._image_ids:\n      tf.logging.warning(\'Ignoring ground truth with image id %s since it was \'\n                         \'previously added\', image_id)\n      return\n\n    groundtruth_is_crowd = groundtruth_dict.get(\n        standard_fields.InputDataFields.groundtruth_is_crowd)\n    # Drop groundtruth_is_crowd if empty tensor.\n    if groundtruth_is_crowd is not None and not groundtruth_is_crowd.shape[0]:\n      groundtruth_is_crowd = None\n\n    self._groundtruth_list.extend(\n        coco_tools.ExportSingleImageGroundtruthToCoco(\n            image_id=image_id,\n            next_annotation_id=self._annotation_id,\n            category_id_set=self._category_id_set,\n            groundtruth_boxes=groundtruth_dict[\n                standard_fields.InputDataFields.groundtruth_boxes],\n            groundtruth_classes=groundtruth_dict[\n                standard_fields.InputDataFields.groundtruth_classes],\n            groundtruth_is_crowd=groundtruth_is_crowd))\n    self._annotation_id += groundtruth_dict[standard_fields.InputDataFields.\n                                            groundtruth_boxes].shape[0]\n    # Boolean to indicate whether a detection has been added for this image.\n    self._image_ids[image_id] = False\n\n  def add_single_detected_image_info(self,\n                                     image_id,\n                                     detections_dict):\n    """"""Adds detections for a single image to be used for evaluation.\n\n    If a detection has already been added for this image id, a warning is\n    logged, and the detection is skipped.\n\n    Args:\n      image_id: A unique string/integer identifier for the image.\n      detections_dict: A dictionary containing -\n        DetectionResultFields.detection_boxes: float32 numpy array of shape\n          [num_boxes, 4] containing `num_boxes` detection boxes of the format\n          [ymin, xmin, ymax, xmax] in absolute image coordinates.\n        DetectionResultFields.detection_scores: float32 numpy array of shape\n          [num_boxes] containing detection scores for the boxes.\n        DetectionResultFields.detection_classes: integer numpy array of shape\n          [num_boxes] containing 1-indexed detection classes for the boxes.\n\n    Raises:\n      ValueError: If groundtruth for the image_id is not available.\n    """"""\n    if image_id not in self._image_ids:\n      raise ValueError(\'Missing groundtruth for image id: {}\'.format(image_id))\n\n    if self._image_ids[image_id]:\n      tf.logging.warning(\'Ignoring detection with image id %s since it was \'\n                         \'previously added\', image_id)\n      return\n\n    self._detection_boxes_list.extend(\n        coco_tools.ExportSingleImageDetectionBoxesToCoco(\n            image_id=image_id,\n            category_id_set=self._category_id_set,\n            detection_boxes=detections_dict[standard_fields.\n                                            DetectionResultFields\n                                            .detection_boxes],\n            detection_scores=detections_dict[standard_fields.\n                                             DetectionResultFields.\n                                             detection_scores],\n            detection_classes=detections_dict[standard_fields.\n                                              DetectionResultFields.\n                                              detection_classes]))\n    self._image_ids[image_id] = True\n\n  def evaluate(self):\n    """"""Evaluates the detection boxes and returns a dictionary of coco metrics.\n\n    Returns:\n      A dictionary holding -\n\n      1. summary_metrics:\n      \'DetectionBoxes_Precision/mAP\': mean average precision over classes\n        averaged over IOU thresholds ranging from .5 to .95 with .05\n        increments.\n      \'DetectionBoxes_Precision/mAP@.50IOU\': mean average precision at 50% IOU\n      \'DetectionBoxes_Precision/mAP@.75IOU\': mean average precision at 75% IOU\n      \'DetectionBoxes_Precision/mAP (small)\': mean average precision for small\n        objects (area < 32^2 pixels).\n      \'DetectionBoxes_Precision/mAP (medium)\': mean average precision for\n        medium sized objects (32^2 pixels < area < 96^2 pixels).\n      \'DetectionBoxes_Precision/mAP (large)\': mean average precision for large\n        objects (96^2 pixels < area < 10000^2 pixels).\n      \'DetectionBoxes_Recall/AR@1\': average recall with 1 detection.\n      \'DetectionBoxes_Recall/AR@10\': average recall with 10 detections.\n      \'DetectionBoxes_Recall/AR@100\': average recall with 100 detections.\n      \'DetectionBoxes_Recall/AR@100 (small)\': average recall for small objects\n        with 100.\n      \'DetectionBoxes_Recall/AR@100 (medium)\': average recall for medium objects\n        with 100.\n      \'DetectionBoxes_Recall/AR@100 (large)\': average recall for large objects\n        with 100 detections.\n\n      2. per_category_ap: if include_metrics_per_category is True, category\n      specific results with keys of the form:\n      \'Precision mAP ByCategory/category\' (without the supercategory part if\n      no supercategories exist). For backward compatibility\n      \'PerformanceByCategory\' is included in the output regardless of\n      all_metrics_per_category.\n    """"""\n    groundtruth_dict = {\n        \'annotations\': self._groundtruth_list,\n        \'images\': [{\'id\': image_id} for image_id in self._image_ids],\n        \'categories\': self._categories\n    }\n    coco_wrapped_groundtruth = coco_tools.COCOWrapper(groundtruth_dict)\n    coco_wrapped_detections = coco_wrapped_groundtruth.LoadAnnotations(\n        self._detection_boxes_list)\n    box_evaluator = coco_tools.COCOEvalWrapper(\n        coco_wrapped_groundtruth, coco_wrapped_detections, agnostic_mode=False)\n    box_metrics, box_per_category_ap = box_evaluator.ComputeMetrics(\n        include_metrics_per_category=self._include_metrics_per_category,\n        all_metrics_per_category=self._all_metrics_per_category)\n    box_metrics.update(box_per_category_ap)\n    box_metrics = {\'DetectionBoxes_\'+ key: value\n                   for key, value in iter(box_metrics.items())}\n    return box_metrics\n\n  def get_estimator_eval_metric_ops(self, image_id, groundtruth_boxes,\n                                    groundtruth_classes, detection_boxes,\n                                    detection_scores, detection_classes):\n    """"""Returns a dictionary of eval metric ops to use with `tf.EstimatorSpec`.\n\n    Note that once value_op is called, the detections and groundtruth added via\n    update_op are cleared.\n\n    Args:\n      image_id: Unique string/integer identifier for the image.\n      groundtruth_boxes: float32 tensor of shape [num_boxes, 4] containing\n        `num_boxes` groundtruth boxes of the format\n        [ymin, xmin, ymax, xmax] in absolute image coordinates.\n      groundtruth_classes: int32 tensor of shape [num_boxes] containing\n        1-indexed groundtruth classes for the boxes.\n      detection_boxes: float32 tensor of shape [num_boxes, 4] containing\n        `num_boxes` detection boxes of the format [ymin, xmin, ymax, xmax]\n        in absolute image coordinates.\n      detection_scores: float32 tensor of shape [num_boxes] containing\n        detection scores for the boxes.\n      detection_classes: int32 tensor of shape [num_boxes] containing\n        1-indexed detection classes for the boxes.\n\n    Returns:\n      a dictionary of metric names to tuple of value_op and update_op that can\n      be used as eval metric ops in tf.EstimatorSpec. Note that all update ops\n      must be run together and similarly all value ops must be run together to\n      guarantee correct behaviour.\n    """"""\n    def update_op(\n        image_id,\n        groundtruth_boxes,\n        groundtruth_classes,\n        detection_boxes,\n        detection_scores,\n        detection_classes):\n      self.add_single_ground_truth_image_info(\n          image_id,\n          {\'groundtruth_boxes\': groundtruth_boxes,\n           \'groundtruth_classes\': groundtruth_classes})\n      self.add_single_detected_image_info(\n          image_id,\n          {\'detection_boxes\': detection_boxes,\n           \'detection_scores\': detection_scores,\n           \'detection_classes\': detection_classes})\n\n    update_op = tf.py_func(update_op, [image_id,\n                                       groundtruth_boxes,\n                                       groundtruth_classes,\n                                       detection_boxes,\n                                       detection_scores,\n                                       detection_classes], [])\n    metric_names = [\'DetectionBoxes_Precision/mAP\',\n                    \'DetectionBoxes_Precision/mAP@.50IOU\',\n                    \'DetectionBoxes_Precision/mAP@.75IOU\',\n                    \'DetectionBoxes_Precision/mAP (large)\',\n                    \'DetectionBoxes_Precision/mAP (medium)\',\n                    \'DetectionBoxes_Precision/mAP (small)\',\n                    \'DetectionBoxes_Recall/AR@1\',\n                    \'DetectionBoxes_Recall/AR@10\',\n                    \'DetectionBoxes_Recall/AR@100\',\n                    \'DetectionBoxes_Recall/AR@100 (large)\',\n                    \'DetectionBoxes_Recall/AR@100 (medium)\',\n                    \'DetectionBoxes_Recall/AR@100 (small)\']\n    if self._include_metrics_per_category:\n      for category_dict in self._categories:\n        metric_names.append(\'DetectionBoxes_PerformanceByCategory/mAP/\' +\n                            category_dict[\'name\'])\n\n    def first_value_func():\n      self._metrics = self.evaluate()\n      self.clear()\n      return np.float32(self._metrics[metric_names[0]])\n\n    def value_func_factory(metric_name):\n      def value_func():\n        return np.float32(self._metrics[metric_name])\n      return value_func\n\n    # Ensure that the metrics are only evaluated once.\n    first_value_op = tf.py_func(first_value_func, [], tf.float32)\n    eval_metric_ops = {metric_names[0]: (first_value_op, update_op)}\n    with tf.control_dependencies([first_value_op]):\n      for metric_name in metric_names[1:]:\n        eval_metric_ops[metric_name] = (tf.py_func(\n            value_func_factory(metric_name), [], np.float32), update_op)\n    return eval_metric_ops\n\n\ndef _check_mask_type_and_value(array_name, masks):\n  """"""Checks whether mask dtype is uint8 and the values are either 0 or 1.""""""\n  if masks.dtype != np.uint8:\n    raise ValueError(\'{} must be of type np.uint8. Found {}.\'.format(\n        array_name, masks.dtype))\n  if np.any(np.logical_and(masks != 0, masks != 1)):\n    raise ValueError(\'{} elements can only be either 0 or 1.\'.format(\n        array_name))\n\n\nclass CocoMaskEvaluator(object_detection_evaluation.DetectionEvaluator):\n  """"""Class to evaluate COCO detection metrics.""""""\n\n  def __init__(self, categories, include_metrics_per_category=False):\n    """"""Constructor.\n\n    Args:\n      categories: A list of dicts, each of which has the following keys -\n        \'id\': (required) an integer id uniquely identifying this category.\n        \'name\': (required) string representing category name e.g., \'cat\', \'dog\'.\n      include_metrics_per_category: If True, include metrics for each category.\n    """"""\n    super(CocoMaskEvaluator, self).__init__(categories)\n    self._image_id_to_mask_shape_map = {}\n    self._image_ids_with_detections = set([])\n    self._groundtruth_list = []\n    self._detection_masks_list = []\n    self._category_id_set = set([cat[\'id\'] for cat in self._categories])\n    self._annotation_id = 1\n    self._include_metrics_per_category = include_metrics_per_category\n\n  def clear(self):\n    """"""Clears the state to prepare for a fresh evaluation.""""""\n    self._image_id_to_mask_shape_map.clear()\n    self._image_ids_with_detections.clear()\n    self._groundtruth_list = []\n    self._detection_masks_list = []\n\n  def add_single_ground_truth_image_info(self,\n                                         image_id,\n                                         groundtruth_dict):\n    """"""Adds groundtruth for a single image to be used for evaluation.\n\n    If the image has already been added, a warning is logged, and groundtruth is\n    ignored.\n\n    Args:\n      image_id: A unique string/integer identifier for the image.\n      groundtruth_dict: A dictionary containing -\n        InputDataFields.groundtruth_boxes: float32 numpy array of shape\n          [num_boxes, 4] containing `num_boxes` groundtruth boxes of the format\n          [ymin, xmin, ymax, xmax] in absolute image coordinates.\n        InputDataFields.groundtruth_classes: integer numpy array of shape\n          [num_boxes] containing 1-indexed groundtruth classes for the boxes.\n        InputDataFields.groundtruth_instance_masks: uint8 numpy array of shape\n          [num_boxes, image_height, image_width] containing groundtruth masks\n          corresponding to the boxes. The elements of the array must be in\n          {0, 1}.\n    """"""\n    if image_id in self._image_id_to_mask_shape_map:\n      tf.logging.warning(\'Ignoring ground truth with image id %s since it was \'\n                         \'previously added\', image_id)\n      return\n\n    groundtruth_instance_masks = groundtruth_dict[\n        standard_fields.InputDataFields.groundtruth_instance_masks]\n    _check_mask_type_and_value(standard_fields.InputDataFields.\n                               groundtruth_instance_masks,\n                               groundtruth_instance_masks)\n    self._groundtruth_list.extend(\n        coco_tools.\n        ExportSingleImageGroundtruthToCoco(\n            image_id=image_id,\n            next_annotation_id=self._annotation_id,\n            category_id_set=self._category_id_set,\n            groundtruth_boxes=groundtruth_dict[standard_fields.InputDataFields.\n                                               groundtruth_boxes],\n            groundtruth_classes=groundtruth_dict[standard_fields.\n                                                 InputDataFields.\n                                                 groundtruth_classes],\n            groundtruth_masks=groundtruth_instance_masks))\n    self._annotation_id += groundtruth_dict[standard_fields.InputDataFields.\n                                            groundtruth_boxes].shape[0]\n    self._image_id_to_mask_shape_map[image_id] = groundtruth_dict[\n        standard_fields.InputDataFields.groundtruth_instance_masks].shape\n\n  def add_single_detected_image_info(self,\n                                     image_id,\n                                     detections_dict):\n    """"""Adds detections for a single image to be used for evaluation.\n\n    If a detection has already been added for this image id, a warning is\n    logged, and the detection is skipped.\n\n    Args:\n      image_id: A unique string/integer identifier for the image.\n      detections_dict: A dictionary containing -\n        DetectionResultFields.detection_scores: float32 numpy array of shape\n          [num_boxes] containing detection scores for the boxes.\n        DetectionResultFields.detection_classes: integer numpy array of shape\n          [num_boxes] containing 1-indexed detection classes for the boxes.\n        DetectionResultFields.detection_masks: optional uint8 numpy array of\n          shape [num_boxes, image_height, image_width] containing instance\n          masks corresponding to the boxes. The elements of the array must be\n          in {0, 1}.\n\n    Raises:\n      ValueError: If groundtruth for the image_id is not available or if\n        spatial shapes of groundtruth_instance_masks and detection_masks are\n        incompatible.\n    """"""\n    if image_id not in self._image_id_to_mask_shape_map:\n      raise ValueError(\'Missing groundtruth for image id: {}\'.format(image_id))\n\n    if image_id in self._image_ids_with_detections:\n      tf.logging.warning(\'Ignoring detection with image id %s since it was \'\n                         \'previously added\', image_id)\n      return\n\n    groundtruth_masks_shape = self._image_id_to_mask_shape_map[image_id]\n    detection_masks = detections_dict[standard_fields.DetectionResultFields.\n                                      detection_masks]\n    if groundtruth_masks_shape[1:] != detection_masks.shape[1:]:\n      raise ValueError(\'Spatial shape of groundtruth masks and detection masks \'\n                       \'are incompatible: {} vs {}\'.format(\n                           groundtruth_masks_shape,\n                           detection_masks.shape))\n    _check_mask_type_and_value(standard_fields.DetectionResultFields.\n                               detection_masks,\n                               detection_masks)\n    self._detection_masks_list.extend(\n        coco_tools.ExportSingleImageDetectionMasksToCoco(\n            image_id=image_id,\n            category_id_set=self._category_id_set,\n            detection_masks=detection_masks,\n            detection_scores=detections_dict[standard_fields.\n                                             DetectionResultFields.\n                                             detection_scores],\n            detection_classes=detections_dict[standard_fields.\n                                              DetectionResultFields.\n                                              detection_classes]))\n    self._image_ids_with_detections.update([image_id])\n\n  def evaluate(self):\n    """"""Evaluates the detection masks and returns a dictionary of coco metrics.\n\n    Returns:\n      A dictionary holding -\n\n      1. summary_metrics:\n      \'DetectionMasks_Precision/mAP\': mean average precision over classes\n        averaged over IOU thresholds ranging from .5 to .95 with .05 increments.\n      \'DetectionMasks_Precision/mAP@.50IOU\': mean average precision at 50% IOU.\n      \'DetectionMasks_Precision/mAP@.75IOU\': mean average precision at 75% IOU.\n      \'DetectionMasks_Precision/mAP (small)\': mean average precision for small\n        objects (area < 32^2 pixels).\n      \'DetectionMasks_Precision/mAP (medium)\': mean average precision for medium\n        sized objects (32^2 pixels < area < 96^2 pixels).\n      \'DetectionMasks_Precision/mAP (large)\': mean average precision for large\n        objects (96^2 pixels < area < 10000^2 pixels).\n      \'DetectionMasks_Recall/AR@1\': average recall with 1 detection.\n      \'DetectionMasks_Recall/AR@10\': average recall with 10 detections.\n      \'DetectionMasks_Recall/AR@100\': average recall with 100 detections.\n      \'DetectionMasks_Recall/AR@100 (small)\': average recall for small objects\n        with 100 detections.\n      \'DetectionMasks_Recall/AR@100 (medium)\': average recall for medium objects\n        with 100 detections.\n      \'DetectionMasks_Recall/AR@100 (large)\': average recall for large objects\n        with 100 detections.\n\n      2. per_category_ap: if include_metrics_per_category is True, category\n      specific results with keys of the form:\n      \'Precision mAP ByCategory/category\' (without the supercategory part if\n      no supercategories exist). For backward compatibility\n      \'PerformanceByCategory\' is included in the output regardless of\n      all_metrics_per_category.\n    """"""\n    groundtruth_dict = {\n        \'annotations\': self._groundtruth_list,\n        \'images\': [{\'id\': image_id, \'height\': shape[1], \'width\': shape[2]}\n                   for image_id, shape in self._image_id_to_mask_shape_map.\n                   iteritems()],\n        \'categories\': self._categories\n    }\n    coco_wrapped_groundtruth = coco_tools.COCOWrapper(\n        groundtruth_dict, detection_type=\'segmentation\')\n    coco_wrapped_detection_masks = coco_wrapped_groundtruth.LoadAnnotations(\n        self._detection_masks_list)\n    mask_evaluator = coco_tools.COCOEvalWrapper(\n        coco_wrapped_groundtruth, coco_wrapped_detection_masks,\n        agnostic_mode=False, iou_type=\'segm\')\n    mask_metrics, mask_per_category_ap = mask_evaluator.ComputeMetrics(\n        include_metrics_per_category=self._include_metrics_per_category)\n    mask_metrics.update(mask_per_category_ap)\n    mask_metrics = {\'DetectionMasks_\'+ key: value\n                    for key, value in mask_metrics.iteritems()}\n    return mask_metrics\n\n  def get_estimator_eval_metric_ops(self, image_id, groundtruth_boxes,\n                                    groundtruth_classes,\n                                    groundtruth_instance_masks,\n                                    detection_scores, detection_classes,\n                                    detection_masks):\n    """"""Returns a dictionary of eval metric ops to use with `tf.EstimatorSpec`.\n\n    Note that once value_op is called, the detections and groundtruth added via\n    update_op are cleared.\n\n    Args:\n      image_id: Unique string/integer identifier for the image.\n      groundtruth_boxes: float32 tensor of shape [num_boxes, 4] containing\n        `num_boxes` groundtruth boxes of the format\n        [ymin, xmin, ymax, xmax] in absolute image coordinates.\n      groundtruth_classes: int32 tensor of shape [num_boxes] containing\n        1-indexed groundtruth classes for the boxes.\n      groundtruth_instance_masks: uint8 tensor array of shape\n        [num_boxes, image_height, image_width] containing groundtruth masks\n        corresponding to the boxes. The elements of the array must be in {0, 1}.\n      detection_scores: float32 tensor of shape [num_boxes] containing\n        detection scores for the boxes.\n      detection_classes: int32 tensor of shape [num_boxes] containing\n        1-indexed detection classes for the boxes.\n      detection_masks: uint8 tensor array of shape\n        [num_boxes, image_height, image_width] containing instance masks\n        corresponding to the boxes. The elements of the array must be in {0, 1}.\n\n    Returns:\n      a dictionary of metric names to tuple of value_op and update_op that can\n      be used as eval metric ops in tf.EstimatorSpec. Note that all update ops\n      must be run together and similarly all value ops must be run together to\n      guarantee correct behaviour.\n    """"""\n    def update_op(\n        image_id,\n        groundtruth_boxes,\n        groundtruth_classes,\n        groundtruth_instance_masks,\n        detection_scores,\n        detection_classes,\n        detection_masks):\n      self.add_single_ground_truth_image_info(\n          image_id,\n          {\'groundtruth_boxes\': groundtruth_boxes,\n           \'groundtruth_classes\': groundtruth_classes,\n           \'groundtruth_instance_masks\': groundtruth_instance_masks})\n      self.add_single_detected_image_info(\n          image_id,\n          {\'detection_scores\': detection_scores,\n           \'detection_classes\': detection_classes,\n           \'detection_masks\': detection_masks})\n\n    update_op = tf.py_func(update_op, [image_id,\n                                       groundtruth_boxes,\n                                       groundtruth_classes,\n                                       groundtruth_instance_masks,\n                                       detection_scores,\n                                       detection_classes,\n                                       detection_masks], [])\n    metric_names = [\'DetectionMasks_Precision/mAP\',\n                    \'DetectionMasks_Precision/mAP@.50IOU\',\n                    \'DetectionMasks_Precision/mAP@.75IOU\',\n                    \'DetectionMasks_Precision/mAP (large)\',\n                    \'DetectionMasks_Precision/mAP (medium)\',\n                    \'DetectionMasks_Precision/mAP (small)\',\n                    \'DetectionMasks_Recall/AR@1\',\n                    \'DetectionMasks_Recall/AR@10\',\n                    \'DetectionMasks_Recall/AR@100\',\n                    \'DetectionMasks_Recall/AR@100 (large)\',\n                    \'DetectionMasks_Recall/AR@100 (medium)\',\n                    \'DetectionMasks_Recall/AR@100 (small)\']\n    if self._include_metrics_per_category:\n      for category_dict in self._categories:\n        metric_names.append(\'DetectionMasks_PerformanceByCategory/mAP/\' +\n                            category_dict[\'name\'])\n\n    def first_value_func():\n      self._metrics = self.evaluate()\n      self.clear()\n      return np.float32(self._metrics[metric_names[0]])\n\n    def value_func_factory(metric_name):\n      def value_func():\n        return np.float32(self._metrics[metric_name])\n      return value_func\n\n    # Ensure that the metrics are only evaluated once.\n    first_value_op = tf.py_func(first_value_func, [], tf.float32)\n    eval_metric_ops = {metric_names[0]: (first_value_op, update_op)}\n    with tf.control_dependencies([first_value_op]):\n      for metric_name in metric_names[1:]:\n        eval_metric_ops[metric_name] = (tf.py_func(\n            value_func_factory(metric_name), [], np.float32), update_op)\n    return eval_metric_ops\n\n\n'"
src/object_detection/metrics/coco_evaluation_test.py,18,"b'# Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Tests for tensorflow_models.object_detection.metrics.coco_evaluation.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport numpy as np\nimport tensorflow as tf\nfrom object_detection.core import standard_fields\nfrom object_detection.metrics import coco_evaluation\n\n\nclass CocoDetectionEvaluationTest(tf.test.TestCase):\n\n  def testGetOneMAPWithMatchingGroundtruthAndDetections(self):\n    """"""Tests that mAP is calculated correctly on GT and Detections.""""""\n    category_list = [{\'id\': 0, \'name\': \'person\'},\n                     {\'id\': 1, \'name\': \'cat\'},\n                     {\'id\': 2, \'name\': \'dog\'}]\n    coco_evaluator = coco_evaluation.CocoDetectionEvaluator(category_list)\n    coco_evaluator.add_single_ground_truth_image_info(\n        image_id=\'image1\',\n        groundtruth_dict={\n            standard_fields.InputDataFields.groundtruth_boxes:\n            np.array([[100., 100., 200., 200.]]),\n            standard_fields.InputDataFields.groundtruth_classes: np.array([1])\n        })\n    coco_evaluator.add_single_detected_image_info(\n        image_id=\'image1\',\n        detections_dict={\n            standard_fields.DetectionResultFields.detection_boxes:\n            np.array([[100., 100., 200., 200.]]),\n            standard_fields.DetectionResultFields.detection_scores:\n            np.array([.8]),\n            standard_fields.DetectionResultFields.detection_classes:\n            np.array([1])\n        })\n    coco_evaluator.add_single_ground_truth_image_info(\n        image_id=\'image2\',\n        groundtruth_dict={\n            standard_fields.InputDataFields.groundtruth_boxes:\n            np.array([[50., 50., 100., 100.]]),\n            standard_fields.InputDataFields.groundtruth_classes: np.array([1])\n        })\n    coco_evaluator.add_single_detected_image_info(\n        image_id=\'image2\',\n        detections_dict={\n            standard_fields.DetectionResultFields.detection_boxes:\n            np.array([[50., 50., 100., 100.]]),\n            standard_fields.DetectionResultFields.detection_scores:\n            np.array([.8]),\n            standard_fields.DetectionResultFields.detection_classes:\n            np.array([1])\n        })\n    coco_evaluator.add_single_ground_truth_image_info(\n        image_id=\'image3\',\n        groundtruth_dict={\n            standard_fields.InputDataFields.groundtruth_boxes:\n            np.array([[25., 25., 50., 50.]]),\n            standard_fields.InputDataFields.groundtruth_classes: np.array([1])\n        })\n    coco_evaluator.add_single_detected_image_info(\n        image_id=\'image3\',\n        detections_dict={\n            standard_fields.DetectionResultFields.detection_boxes:\n            np.array([[25., 25., 50., 50.]]),\n            standard_fields.DetectionResultFields.detection_scores:\n            np.array([.8]),\n            standard_fields.DetectionResultFields.detection_classes:\n            np.array([1])\n        })\n    metrics = coco_evaluator.evaluate()\n    self.assertAlmostEqual(metrics[\'DetectionBoxes_Precision/mAP\'], 1.0)\n\n  def testGetOneMAPWithMatchingGroundtruthAndDetectionsSkipCrowd(self):\n    """"""Tests computing mAP with is_crowd GT boxes skipped.""""""\n    category_list = [{\n        \'id\': 0,\n        \'name\': \'person\'\n    }, {\n        \'id\': 1,\n        \'name\': \'cat\'\n    }, {\n        \'id\': 2,\n        \'name\': \'dog\'\n    }]\n    coco_evaluator = coco_evaluation.CocoDetectionEvaluator(category_list)\n    coco_evaluator.add_single_ground_truth_image_info(\n        image_id=\'image1\',\n        groundtruth_dict={\n            standard_fields.InputDataFields.groundtruth_boxes:\n                np.array([[100., 100., 200., 200.], [99., 99., 200., 200.]]),\n            standard_fields.InputDataFields.groundtruth_classes:\n                np.array([1, 2]),\n            standard_fields.InputDataFields.groundtruth_is_crowd:\n                np.array([0, 1])\n        })\n    coco_evaluator.add_single_detected_image_info(\n        image_id=\'image1\',\n        detections_dict={\n            standard_fields.DetectionResultFields.detection_boxes:\n                np.array([[100., 100., 200., 200.]]),\n            standard_fields.DetectionResultFields.detection_scores:\n                np.array([.8]),\n            standard_fields.DetectionResultFields.detection_classes:\n                np.array([1])\n        })\n    metrics = coco_evaluator.evaluate()\n    self.assertAlmostEqual(metrics[\'DetectionBoxes_Precision/mAP\'], 1.0)\n\n  def testGetOneMAPWithMatchingGroundtruthAndDetectionsEmptyCrowd(self):\n    """"""Tests computing mAP with empty is_crowd array passed in.""""""\n    category_list = [{\n        \'id\': 0,\n        \'name\': \'person\'\n    }, {\n        \'id\': 1,\n        \'name\': \'cat\'\n    }, {\n        \'id\': 2,\n        \'name\': \'dog\'\n    }]\n    coco_evaluator = coco_evaluation.CocoDetectionEvaluator(category_list)\n    coco_evaluator.add_single_ground_truth_image_info(\n        image_id=\'image1\',\n        groundtruth_dict={\n            standard_fields.InputDataFields.groundtruth_boxes:\n                np.array([[100., 100., 200., 200.]]),\n            standard_fields.InputDataFields.groundtruth_classes:\n                np.array([1]),\n            standard_fields.InputDataFields.groundtruth_is_crowd:\n                np.array([])\n        })\n    coco_evaluator.add_single_detected_image_info(\n        image_id=\'image1\',\n        detections_dict={\n            standard_fields.DetectionResultFields.detection_boxes:\n                np.array([[100., 100., 200., 200.]]),\n            standard_fields.DetectionResultFields.detection_scores:\n                np.array([.8]),\n            standard_fields.DetectionResultFields.detection_classes:\n                np.array([1])\n        })\n    metrics = coco_evaluator.evaluate()\n    self.assertAlmostEqual(metrics[\'DetectionBoxes_Precision/mAP\'], 1.0)\n\n  def testRejectionOnDuplicateGroundtruth(self):\n    """"""Tests that groundtruth cannot be added more than once for an image.""""""\n    categories = [{\'id\': 1, \'name\': \'cat\'},\n                  {\'id\': 2, \'name\': \'dog\'},\n                  {\'id\': 3, \'name\': \'elephant\'}]\n    #  Add groundtruth\n    coco_evaluator = coco_evaluation.CocoDetectionEvaluator(categories)\n    image_key1 = \'img1\'\n    groundtruth_boxes1 = np.array([[0, 0, 1, 1], [0, 0, 2, 2], [0, 0, 3, 3]],\n                                  dtype=float)\n    groundtruth_class_labels1 = np.array([1, 3, 1], dtype=int)\n    coco_evaluator.add_single_ground_truth_image_info(image_key1, {\n        standard_fields.InputDataFields.groundtruth_boxes:\n            groundtruth_boxes1,\n        standard_fields.InputDataFields.groundtruth_classes:\n            groundtruth_class_labels1\n    })\n    groundtruth_lists_len = len(coco_evaluator._groundtruth_list)\n\n    # Add groundtruth with the same image id.\n    coco_evaluator.add_single_ground_truth_image_info(image_key1, {\n        standard_fields.InputDataFields.groundtruth_boxes:\n            groundtruth_boxes1,\n        standard_fields.InputDataFields.groundtruth_classes:\n            groundtruth_class_labels1\n    })\n    self.assertEqual(groundtruth_lists_len,\n                     len(coco_evaluator._groundtruth_list))\n\n  def testRejectionOnDuplicateDetections(self):\n    """"""Tests that detections cannot be added more than once for an image.""""""\n    categories = [{\'id\': 1, \'name\': \'cat\'},\n                  {\'id\': 2, \'name\': \'dog\'},\n                  {\'id\': 3, \'name\': \'elephant\'}]\n    #  Add groundtruth\n    coco_evaluator = coco_evaluation.CocoDetectionEvaluator(categories)\n    coco_evaluator.add_single_ground_truth_image_info(\n        image_id=\'image1\',\n        groundtruth_dict={\n            standard_fields.InputDataFields.groundtruth_boxes:\n            np.array([[99., 100., 200., 200.]]),\n            standard_fields.InputDataFields.groundtruth_classes: np.array([1])\n        })\n    coco_evaluator.add_single_detected_image_info(\n        image_id=\'image1\',\n        detections_dict={\n            standard_fields.DetectionResultFields.detection_boxes:\n            np.array([[100., 100., 200., 200.]]),\n            standard_fields.DetectionResultFields.detection_scores:\n            np.array([.8]),\n            standard_fields.DetectionResultFields.detection_classes:\n            np.array([1])\n        })\n    detections_lists_len = len(coco_evaluator._detection_boxes_list)\n    coco_evaluator.add_single_detected_image_info(\n        image_id=\'image1\',  # Note that this image id was previously added.\n        detections_dict={\n            standard_fields.DetectionResultFields.detection_boxes:\n            np.array([[100., 100., 200., 200.]]),\n            standard_fields.DetectionResultFields.detection_scores:\n            np.array([.8]),\n            standard_fields.DetectionResultFields.detection_classes:\n            np.array([1])\n        })\n    self.assertEqual(detections_lists_len,\n                     len(coco_evaluator._detection_boxes_list))\n\n  def testExceptionRaisedWithMissingGroundtruth(self):\n    """"""Tests that exception is raised for detection with missing groundtruth.""""""\n    categories = [{\'id\': 1, \'name\': \'cat\'},\n                  {\'id\': 2, \'name\': \'dog\'},\n                  {\'id\': 3, \'name\': \'elephant\'}]\n    coco_evaluator = coco_evaluation.CocoDetectionEvaluator(categories)\n    with self.assertRaises(ValueError):\n      coco_evaluator.add_single_detected_image_info(\n          image_id=\'image1\',\n          detections_dict={\n              standard_fields.DetectionResultFields.detection_boxes:\n                  np.array([[100., 100., 200., 200.]]),\n              standard_fields.DetectionResultFields.detection_scores:\n                  np.array([.8]),\n              standard_fields.DetectionResultFields.detection_classes:\n                  np.array([1])\n          })\n\n\nclass CocoEvaluationPyFuncTest(tf.test.TestCase):\n\n  def testGetOneMAPWithMatchingGroundtruthAndDetections(self):\n    category_list = [{\'id\': 0, \'name\': \'person\'},\n                     {\'id\': 1, \'name\': \'cat\'},\n                     {\'id\': 2, \'name\': \'dog\'}]\n    coco_evaluator = coco_evaluation.CocoDetectionEvaluator(category_list)\n    image_id = tf.placeholder(tf.string, shape=())\n    groundtruth_boxes = tf.placeholder(tf.float32, shape=(None, 4))\n    groundtruth_classes = tf.placeholder(tf.float32, shape=(None))\n    detection_boxes = tf.placeholder(tf.float32, shape=(None, 4))\n    detection_scores = tf.placeholder(tf.float32, shape=(None))\n    detection_classes = tf.placeholder(tf.float32, shape=(None))\n\n    eval_metric_ops = coco_evaluator.get_estimator_eval_metric_ops(\n        image_id, groundtruth_boxes,\n        groundtruth_classes,\n        detection_boxes,\n        detection_scores,\n        detection_classes)\n\n    _, update_op = eval_metric_ops[\'DetectionBoxes_Precision/mAP\']\n\n    with self.test_session() as sess:\n      sess.run(update_op,\n               feed_dict={\n                   image_id: \'image1\',\n                   groundtruth_boxes: np.array([[100., 100., 200., 200.]]),\n                   groundtruth_classes: np.array([1]),\n                   detection_boxes: np.array([[100., 100., 200., 200.]]),\n                   detection_scores: np.array([.8]),\n                   detection_classes: np.array([1])\n               })\n      sess.run(update_op,\n               feed_dict={\n                   image_id: \'image2\',\n                   groundtruth_boxes: np.array([[50., 50., 100., 100.]]),\n                   groundtruth_classes: np.array([3]),\n                   detection_boxes: np.array([[50., 50., 100., 100.]]),\n                   detection_scores: np.array([.7]),\n                   detection_classes: np.array([3])\n               })\n      sess.run(update_op,\n               feed_dict={\n                   image_id: \'image3\',\n                   groundtruth_boxes: np.array([[25., 25., 50., 50.]]),\n                   groundtruth_classes: np.array([2]),\n                   detection_boxes: np.array([[25., 25., 50., 50.]]),\n                   detection_scores: np.array([.9]),\n                   detection_classes: np.array([2])\n               })\n    metrics = {}\n    for key, (value_op, _) in eval_metric_ops.iteritems():\n      metrics[key] = value_op\n    metrics = sess.run(metrics)\n    self.assertAlmostEqual(metrics[\'DetectionBoxes_Precision/mAP\'], 1.0)\n    self.assertAlmostEqual(metrics[\'DetectionBoxes_Precision/mAP@.50IOU\'], 1.0)\n    self.assertAlmostEqual(metrics[\'DetectionBoxes_Precision/mAP@.75IOU\'], 1.0)\n    self.assertAlmostEqual(metrics[\'DetectionBoxes_Precision/mAP (large)\'], 1.0)\n    self.assertAlmostEqual(metrics[\'DetectionBoxes_Precision/mAP (medium)\'],\n                           -1.0)\n    self.assertAlmostEqual(metrics[\'DetectionBoxes_Precision/mAP (small)\'], 1.0)\n    self.assertAlmostEqual(metrics[\'DetectionBoxes_Recall/AR@1\'], 1.0)\n    self.assertAlmostEqual(metrics[\'DetectionBoxes_Recall/AR@10\'], 1.0)\n    self.assertAlmostEqual(metrics[\'DetectionBoxes_Recall/AR@100\'], 1.0)\n    self.assertAlmostEqual(metrics[\'DetectionBoxes_Recall/AR@100 (large)\'], 1.0)\n    self.assertAlmostEqual(metrics[\'DetectionBoxes_Recall/AR@100 (medium)\'],\n                           -1.0)\n    self.assertAlmostEqual(metrics[\'DetectionBoxes_Recall/AR@100 (small)\'], 1.0)\n    self.assertFalse(coco_evaluator._groundtruth_list)\n    self.assertFalse(coco_evaluator._detection_boxes_list)\n    self.assertFalse(coco_evaluator._image_ids)\n\n\nclass CocoMaskEvaluationTest(tf.test.TestCase):\n\n  def testGetOneMAPWithMatchingGroundtruthAndDetections(self):\n    category_list = [{\'id\': 0, \'name\': \'person\'},\n                     {\'id\': 1, \'name\': \'cat\'},\n                     {\'id\': 2, \'name\': \'dog\'}]\n    coco_evaluator = coco_evaluation.CocoMaskEvaluator(category_list)\n    coco_evaluator.add_single_ground_truth_image_info(\n        image_id=\'image1\',\n        groundtruth_dict={\n            standard_fields.InputDataFields.groundtruth_boxes:\n            np.array([[100., 100., 200., 200.]]),\n            standard_fields.InputDataFields.groundtruth_classes: np.array([1]),\n            standard_fields.InputDataFields.groundtruth_instance_masks:\n            np.pad(np.ones([1, 100, 100], dtype=np.uint8),\n                   ((0, 0), (10, 10), (10, 10)), mode=\'constant\')\n        })\n    coco_evaluator.add_single_detected_image_info(\n        image_id=\'image1\',\n        detections_dict={\n            standard_fields.DetectionResultFields.detection_boxes:\n            np.array([[100., 100., 200., 200.]]),\n            standard_fields.DetectionResultFields.detection_scores:\n            np.array([.8]),\n            standard_fields.DetectionResultFields.detection_classes:\n            np.array([1]),\n            standard_fields.DetectionResultFields.detection_masks:\n            np.pad(np.ones([1, 100, 100], dtype=np.uint8),\n                   ((0, 0), (10, 10), (10, 10)), mode=\'constant\')\n        })\n    coco_evaluator.add_single_ground_truth_image_info(\n        image_id=\'image2\',\n        groundtruth_dict={\n            standard_fields.InputDataFields.groundtruth_boxes:\n            np.array([[50., 50., 100., 100.]]),\n            standard_fields.InputDataFields.groundtruth_classes: np.array([1]),\n            standard_fields.InputDataFields.groundtruth_instance_masks:\n            np.pad(np.ones([1, 50, 50], dtype=np.uint8),\n                   ((0, 0), (10, 10), (10, 10)), mode=\'constant\')\n        })\n    coco_evaluator.add_single_detected_image_info(\n        image_id=\'image2\',\n        detections_dict={\n            standard_fields.DetectionResultFields.detection_boxes:\n            np.array([[50., 50., 100., 100.]]),\n            standard_fields.DetectionResultFields.detection_scores:\n            np.array([.8]),\n            standard_fields.DetectionResultFields.detection_classes:\n            np.array([1]),\n            standard_fields.DetectionResultFields.detection_masks:\n            np.pad(np.ones([1, 50, 50], dtype=np.uint8),\n                   ((0, 0), (10, 10), (10, 10)), mode=\'constant\')\n        })\n    coco_evaluator.add_single_ground_truth_image_info(\n        image_id=\'image3\',\n        groundtruth_dict={\n            standard_fields.InputDataFields.groundtruth_boxes:\n            np.array([[25., 25., 50., 50.]]),\n            standard_fields.InputDataFields.groundtruth_classes: np.array([1]),\n            standard_fields.InputDataFields.groundtruth_instance_masks:\n            np.pad(np.ones([1, 25, 25], dtype=np.uint8),\n                   ((0, 0), (10, 10), (10, 10)), mode=\'constant\')\n        })\n    coco_evaluator.add_single_detected_image_info(\n        image_id=\'image3\',\n        detections_dict={\n            standard_fields.DetectionResultFields.detection_boxes:\n            np.array([[25., 25., 50., 50.]]),\n            standard_fields.DetectionResultFields.detection_scores:\n            np.array([.8]),\n            standard_fields.DetectionResultFields.detection_classes:\n            np.array([1]),\n            standard_fields.DetectionResultFields.detection_masks:\n            np.pad(np.ones([1, 25, 25], dtype=np.uint8),\n                   ((0, 0), (10, 10), (10, 10)), mode=\'constant\')\n        })\n    metrics = coco_evaluator.evaluate()\n    self.assertAlmostEqual(metrics[\'DetectionMasks_Precision/mAP\'], 1.0)\n    coco_evaluator.clear()\n    self.assertFalse(coco_evaluator._image_id_to_mask_shape_map)\n    self.assertFalse(coco_evaluator._image_ids_with_detections)\n    self.assertFalse(coco_evaluator._groundtruth_list)\n    self.assertFalse(coco_evaluator._detection_masks_list)\n\n\nclass CocoMaskEvaluationPyFuncTest(tf.test.TestCase):\n\n  def testGetOneMAPWithMatchingGroundtruthAndDetections(self):\n    category_list = [{\'id\': 0, \'name\': \'person\'},\n                     {\'id\': 1, \'name\': \'cat\'},\n                     {\'id\': 2, \'name\': \'dog\'}]\n    coco_evaluator = coco_evaluation.CocoMaskEvaluator(category_list)\n    image_id = tf.placeholder(tf.string, shape=())\n    groundtruth_boxes = tf.placeholder(tf.float32, shape=(None, 4))\n    groundtruth_classes = tf.placeholder(tf.float32, shape=(None))\n    groundtruth_masks = tf.placeholder(tf.uint8, shape=(None, None, None))\n    detection_scores = tf.placeholder(tf.float32, shape=(None))\n    detection_classes = tf.placeholder(tf.float32, shape=(None))\n    detection_masks = tf.placeholder(tf.uint8, shape=(None, None, None))\n\n    eval_metric_ops = coco_evaluator.get_estimator_eval_metric_ops(\n        image_id, groundtruth_boxes,\n        groundtruth_classes,\n        groundtruth_masks,\n        detection_scores,\n        detection_classes,\n        detection_masks)\n\n    _, update_op = eval_metric_ops[\'DetectionMasks_Precision/mAP\']\n\n    with self.test_session() as sess:\n      sess.run(update_op,\n               feed_dict={\n                   image_id: \'image1\',\n                   groundtruth_boxes: np.array([[100., 100., 200., 200.]]),\n                   groundtruth_classes: np.array([1]),\n                   groundtruth_masks: np.pad(np.ones([1, 100, 100],\n                                                     dtype=np.uint8),\n                                             ((0, 0), (10, 10), (10, 10)),\n                                             mode=\'constant\'),\n                   detection_scores: np.array([.8]),\n                   detection_classes: np.array([1]),\n                   detection_masks: np.pad(np.ones([1, 100, 100],\n                                                   dtype=np.uint8),\n                                           ((0, 0), (10, 10), (10, 10)),\n                                           mode=\'constant\')\n               })\n      sess.run(update_op,\n               feed_dict={\n                   image_id: \'image2\',\n                   groundtruth_boxes: np.array([[50., 50., 100., 100.]]),\n                   groundtruth_classes: np.array([1]),\n                   groundtruth_masks: np.pad(np.ones([1, 50, 50],\n                                                     dtype=np.uint8),\n                                             ((0, 0), (10, 10), (10, 10)),\n                                             mode=\'constant\'),\n                   detection_scores: np.array([.8]),\n                   detection_classes: np.array([1]),\n                   detection_masks: np.pad(np.ones([1, 50, 50], dtype=np.uint8),\n                                           ((0, 0), (10, 10), (10, 10)),\n                                           mode=\'constant\')\n               })\n      sess.run(update_op,\n               feed_dict={\n                   image_id: \'image3\',\n                   groundtruth_boxes: np.array([[25., 25., 50., 50.]]),\n                   groundtruth_classes: np.array([1]),\n                   groundtruth_masks: np.pad(np.ones([1, 25, 25],\n                                                     dtype=np.uint8),\n                                             ((0, 0), (10, 10), (10, 10)),\n                                             mode=\'constant\'),\n                   detection_scores: np.array([.8]),\n                   detection_classes: np.array([1]),\n                   detection_masks: np.pad(np.ones([1, 25, 25],\n                                                   dtype=np.uint8),\n                                           ((0, 0), (10, 10), (10, 10)),\n                                           mode=\'constant\')\n               })\n    metrics = {}\n    for key, (value_op, _) in eval_metric_ops.iteritems():\n      metrics[key] = value_op\n    metrics = sess.run(metrics)\n    self.assertAlmostEqual(metrics[\'DetectionMasks_Precision/mAP\'], 1.0)\n    self.assertAlmostEqual(metrics[\'DetectionMasks_Precision/mAP@.50IOU\'], 1.0)\n    self.assertAlmostEqual(metrics[\'DetectionMasks_Precision/mAP@.75IOU\'], 1.0)\n    self.assertAlmostEqual(metrics[\'DetectionMasks_Precision/mAP (large)\'], 1.0)\n    self.assertAlmostEqual(metrics[\'DetectionMasks_Precision/mAP (medium)\'],\n                           1.0)\n    self.assertAlmostEqual(metrics[\'DetectionMasks_Precision/mAP (small)\'], 1.0)\n    self.assertAlmostEqual(metrics[\'DetectionMasks_Recall/AR@1\'], 1.0)\n    self.assertAlmostEqual(metrics[\'DetectionMasks_Recall/AR@10\'], 1.0)\n    self.assertAlmostEqual(metrics[\'DetectionMasks_Recall/AR@100\'], 1.0)\n    self.assertAlmostEqual(metrics[\'DetectionMasks_Recall/AR@100 (large)\'], 1.0)\n    self.assertAlmostEqual(metrics[\'DetectionMasks_Recall/AR@100 (medium)\'],\n                           1.0)\n    self.assertAlmostEqual(metrics[\'DetectionMasks_Recall/AR@100 (small)\'], 1.0)\n    self.assertFalse(coco_evaluator._groundtruth_list)\n    self.assertFalse(coco_evaluator._image_ids_with_detections)\n    self.assertFalse(coco_evaluator._image_id_to_mask_shape_map)\n    self.assertFalse(coco_evaluator._detection_masks_list)\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
src/object_detection/metrics/coco_tools.py,6,"b'# Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Wrappers for third party pycocotools to be used within object_detection.\n\nNote that nothing in this file is tensorflow related and thus cannot\nbe called directly as a slim metric, for example.\n\nTODO(jonathanhuang): wrap as a slim metric in metrics.py\n\n\nUsage example: given a set of images with ids in the list image_ids\nand corresponding lists of numpy arrays encoding groundtruth (boxes and classes)\nand detections (boxes, scores and classes), where elements of each list\ncorrespond to detections/annotations of a single image,\nthen evaluation (in multi-class mode) can be invoked as follows:\n\n  groundtruth_dict = coco_tools.ExportGroundtruthToCOCO(\n      image_ids, groundtruth_boxes_list, groundtruth_classes_list,\n      max_num_classes, output_path=None)\n  detections_list = coco_tools.ExportDetectionsToCOCO(\n      image_ids, detection_boxes_list, detection_scores_list,\n      detection_classes_list, output_path=None)\n  groundtruth = coco_tools.COCOWrapper(groundtruth_dict)\n  detections = groundtruth.LoadAnnotations(detections_list)\n  evaluator = coco_tools.COCOEvalWrapper(groundtruth, detections,\n                                         agnostic_mode=False)\n  metrics = evaluator.ComputeMetrics()\n\n""""""\nfrom collections import OrderedDict\nimport copy\nimport time\nimport numpy as np\n\nfrom pycocotools import coco\nfrom pycocotools import cocoeval\nfrom pycocotools import mask\n\nimport tensorflow as tf\n\nfrom object_detection.utils import json_utils\n\n\nclass COCOWrapper(coco.COCO):\n  """"""Wrapper for the pycocotools COCO class.""""""\n\n  def __init__(self, dataset, detection_type=\'bbox\'):\n    """"""COCOWrapper constructor.\n\n    See http://mscoco.org/dataset/#format for a description of the format.\n    By default, the coco.COCO class constructor reads from a JSON file.\n    This function duplicates the same behavior but loads from a dictionary,\n    allowing us to perform evaluation without writing to external storage.\n\n    Args:\n      dataset: a dictionary holding bounding box annotations in the COCO format.\n      detection_type: type of detections being wrapped. Can be one of [\'bbox\',\n        \'segmentation\']\n\n    Raises:\n      ValueError: if detection_type is unsupported.\n    """"""\n    supported_detection_types = [\'bbox\', \'segmentation\']\n    if detection_type not in supported_detection_types:\n      raise ValueError(\'Unsupported detection type: {}. \'\n                       \'Supported values are: {}\'.format(\n                           detection_type, supported_detection_types))\n    self._detection_type = detection_type\n    coco.COCO.__init__(self)\n    self.dataset = dataset\n    self.createIndex()\n\n  def LoadAnnotations(self, annotations):\n    """"""Load annotations dictionary into COCO datastructure.\n\n    See http://mscoco.org/dataset/#format for a description of the annotations\n    format.  As above, this function replicates the default behavior of the API\n    but does not require writing to external storage.\n\n    Args:\n      annotations: python list holding object detection results where each\n        detection is encoded as a dict with required keys [\'image_id\',\n        \'category_id\', \'score\'] and one of [\'bbox\', \'segmentation\'] based on\n        `detection_type`.\n\n    Returns:\n      a coco.COCO datastructure holding object detection annotations results\n\n    Raises:\n      ValueError: if annotations is not a list\n      ValueError: if annotations do not correspond to the images contained\n        in self.\n    """"""\n    results = coco.COCO()\n    results.dataset[\'images\'] = [img for img in self.dataset[\'images\']]\n\n    tf.logging.info(\'Loading and preparing annotation results...\')\n    tic = time.time()\n\n    if not isinstance(annotations, list):\n      raise ValueError(\'annotations is not a list of objects\')\n    annotation_img_ids = [ann[\'image_id\'] for ann in annotations]\n    if (set(annotation_img_ids) != (set(annotation_img_ids)\n                                    & set(self.getImgIds()))):\n      raise ValueError(\'Results do not correspond to current coco set\')\n    results.dataset[\'categories\'] = copy.deepcopy(self.dataset[\'categories\'])\n    if self._detection_type == \'bbox\':\n      for idx, ann in enumerate(annotations):\n        bb = ann[\'bbox\']\n        ann[\'area\'] = bb[2] * bb[3]\n        ann[\'id\'] = idx + 1\n        ann[\'iscrowd\'] = 0\n    elif self._detection_type == \'segmentation\':\n      for idx, ann in enumerate(annotations):\n        ann[\'area\'] = mask.area(ann[\'segmentation\'])\n        ann[\'bbox\'] = mask.toBbox(ann[\'segmentation\'])\n        ann[\'id\'] = idx + 1\n        ann[\'iscrowd\'] = 0\n    tf.logging.info(\'DONE (t=%0.2fs)\', (time.time() - tic))\n\n    results.dataset[\'annotations\'] = annotations\n    results.createIndex()\n    return results\n\n\nclass COCOEvalWrapper(cocoeval.COCOeval):\n  """"""Wrapper for the pycocotools COCOeval class.\n\n  To evaluate, create two objects (groundtruth_dict and detections_list)\n  using the conventions listed at http://mscoco.org/dataset/#format.\n  Then call evaluation as follows:\n\n    groundtruth = coco_tools.COCOWrapper(groundtruth_dict)\n    detections = groundtruth.LoadAnnotations(detections_list)\n    evaluator = coco_tools.COCOEvalWrapper(groundtruth, detections,\n                                           agnostic_mode=False)\n\n    metrics = evaluator.ComputeMetrics()\n  """"""\n\n  def __init__(self, groundtruth=None, detections=None, agnostic_mode=False,\n               iou_type=\'bbox\'):\n    """"""COCOEvalWrapper constructor.\n\n    Note that for the area-based metrics to be meaningful, detection and\n    groundtruth boxes must be in image coordinates measured in pixels.\n\n    Args:\n      groundtruth: a coco.COCO (or coco_tools.COCOWrapper) object holding\n        groundtruth annotations\n      detections: a coco.COCO (or coco_tools.COCOWrapper) object holding\n        detections\n      agnostic_mode: boolean (default: False).  If True, evaluation ignores\n        class labels, treating all detections as proposals.\n      iou_type: IOU type to use for evaluation. Supports `bbox` or `segm`.\n    """"""\n    cocoeval.COCOeval.__init__(self, groundtruth, detections,\n                               iouType=iou_type)\n    if agnostic_mode:\n      self.params.useCats = 0\n\n  def GetCategory(self, category_id):\n    """"""Fetches dictionary holding category information given category id.\n\n    Args:\n      category_id: integer id\n    Returns:\n      dictionary holding \'id\', \'name\'.\n    """"""\n    return self.cocoGt.cats[category_id]\n\n  def GetAgnosticMode(self):\n    """"""Returns true if COCO Eval is configured to evaluate in agnostic mode.""""""\n    return self.params.useCats == 0\n\n  def GetCategoryIdList(self):\n    """"""Returns list of valid category ids.""""""\n    return self.params.catIds\n\n  def ComputeMetrics(self,\n                     include_metrics_per_category=False,\n                     all_metrics_per_category=False):\n    """"""Computes detection metrics.\n\n    Args:\n      include_metrics_per_category: If True, will include metrics per category.\n      all_metrics_per_category: If true, include all the summery metrics for\n        each category in per_category_ap. Be careful with setting it to true if\n        you have more than handful of categories, because it will pollute\n        your mldash.\n\n    Returns:\n      1. summary_metrics: a dictionary holding:\n        \'Precision/mAP\': mean average precision over classes averaged over IOU\n          thresholds ranging from .5 to .95 with .05 increments\n        \'Precision/mAP@.50IOU\': mean average precision at 50% IOU\n        \'Precision/mAP@.75IOU\': mean average precision at 75% IOU\n        \'Precision/mAP (small)\': mean average precision for small objects\n                        (area < 32^2 pixels)\n        \'Precision/mAP (medium)\': mean average precision for medium sized\n                        objects (32^2 pixels < area < 96^2 pixels)\n        \'Precision/mAP (large)\': mean average precision for large objects\n                        (96^2 pixels < area < 10000^2 pixels)\n        \'Recall/AR@1\': average recall with 1 detection\n        \'Recall/AR@10\': average recall with 10 detections\n        \'Recall/AR@100\': average recall with 100 detections\n        \'Recall/AR@100 (small)\': average recall for small objects with 100\n          detections\n        \'Recall/AR@100 (medium)\': average recall for medium objects with 100\n          detections\n        \'Recall/AR@100 (large)\': average recall for large objects with 100\n          detections\n      2. per_category_ap: a dictionary holding category specific results with\n        keys of the form: \'Precision mAP ByCategory/category\'\n        (without the supercategory part if no supercategories exist).\n        For backward compatibility \'PerformanceByCategory\' is included in the\n        output regardless of all_metrics_per_category.\n        If evaluating class-agnostic mode, per_category_ap is an empty\n        dictionary.\n\n    Raises:\n      ValueError: If category_stats does not exist.\n    """"""\n    self.evaluate()\n    self.accumulate()\n    self.summarize()\n\n    summary_metrics = OrderedDict([\n        (\'Precision/mAP\', self.stats[0]),\n        (\'Precision/mAP@.50IOU\', self.stats[1]),\n        (\'Precision/mAP@.75IOU\', self.stats[2]),\n        (\'Precision/mAP (small)\', self.stats[3]),\n        (\'Precision/mAP (medium)\', self.stats[4]),\n        (\'Precision/mAP (large)\', self.stats[5]),\n        (\'Recall/AR@1\', self.stats[6]),\n        (\'Recall/AR@10\', self.stats[7]),\n        (\'Recall/AR@100\', self.stats[8]),\n        (\'Recall/AR@100 (small)\', self.stats[9]),\n        (\'Recall/AR@100 (medium)\', self.stats[10]),\n        (\'Recall/AR@100 (large)\', self.stats[11])\n    ])\n    if not include_metrics_per_category:\n      return summary_metrics, {}\n    if not hasattr(self, \'category_stats\'):\n      raise ValueError(\'Category stats do not exist\')\n    per_category_ap = OrderedDict([])\n    if self.GetAgnosticMode():\n      return summary_metrics, per_category_ap\n    for category_index, category_id in enumerate(self.GetCategoryIdList()):\n      category = self.GetCategory(category_id)[\'name\']\n      # Kept for backward compatilbility\n      per_category_ap[\'PerformanceByCategory/mAP/{}\'.format(\n          category)] = self.category_stats[0][category_index]\n      if all_metrics_per_category:\n        per_category_ap[\'Precision mAP ByCategory/{}\'.format(\n            category)] = self.category_stats[0][category_index]\n        per_category_ap[\'Precision mAP@.50IOU ByCategory/{}\'.format(\n            category)] = self.category_stats[1][category_index]\n        per_category_ap[\'Precision mAP@.75IOU ByCategory/{}\'.format(\n            category)] = self.category_stats[2][category_index]\n        per_category_ap[\'Precision mAP (small) ByCategory/{}\'.format(\n            category)] = self.category_stats[3][category_index]\n        per_category_ap[\'Precision mAP (medium) ByCategory/{}\'.format(\n            category)] = self.category_stats[4][category_index]\n        per_category_ap[\'Precision mAP (large) ByCategory/{}\'.format(\n            category)] = self.category_stats[5][category_index]\n        per_category_ap[\'Recall AR@1 ByCategory/{}\'.format(\n            category)] = self.category_stats[6][category_index]\n        per_category_ap[\'Recall AR@10 ByCategory/{}\'.format(\n            category)] = self.category_stats[7][category_index]\n        per_category_ap[\'Recall AR@100 ByCategory/{}\'.format(\n            category)] = self.category_stats[8][category_index]\n        per_category_ap[\'Recall AR@100 (small) ByCategory/{}\'.format(\n            category)] = self.category_stats[9][category_index]\n        per_category_ap[\'Recall AR@100 (medium) ByCategory/{}\'.format(\n            category)] = self.category_stats[10][category_index]\n        per_category_ap[\'Recall AR@100 (large) ByCategory/{}\'.format(\n            category)] = self.category_stats[11][category_index]\n\n    return summary_metrics, per_category_ap\n\n\ndef _ConvertBoxToCOCOFormat(box):\n  """"""Converts a box in [ymin, xmin, ymax, xmax] format to COCO format.\n\n  This is a utility function for converting from our internal\n  [ymin, xmin, ymax, xmax] convention to the convention used by the COCO API\n  i.e., [xmin, ymin, width, height].\n\n  Args:\n    box: a [ymin, xmin, ymax, xmax] numpy array\n\n  Returns:\n    a list of floats representing [xmin, ymin, width, height]\n  """"""\n  return [float(box[1]), float(box[0]), float(box[3] - box[1]),\n          float(box[2] - box[0])]\n\n\ndef _RleCompress(masks):\n  """"""Compresses mask using Run-length encoding provided by pycocotools.\n\n  Args:\n    masks: uint8 numpy array of shape [mask_height, mask_width] with values in\n    {0, 1}.\n\n  Returns:\n    A pycocotools Run-length encoding of the mask.\n  """"""\n  return mask.encode(np.asfortranarray(masks))\n\n\ndef ExportSingleImageGroundtruthToCoco(image_id,\n                                       next_annotation_id,\n                                       category_id_set,\n                                       groundtruth_boxes,\n                                       groundtruth_classes,\n                                       groundtruth_masks=None,\n                                       groundtruth_is_crowd=None):\n  """"""Export groundtruth of a single image to COCO format.\n\n  This function converts groundtruth detection annotations represented as numpy\n  arrays to dictionaries that can be ingested by the COCO evaluation API. Note\n  that the image_ids provided here must match the ones given to\n  ExportSingleImageDetectionsToCoco. We assume that boxes and classes are in\n  correspondence - that is: groundtruth_boxes[i, :], and\n  groundtruth_classes[i] are associated with the same groundtruth annotation.\n\n  In the exported result, ""area"" fields are always set to the area of the\n  groundtruth bounding box.\n\n  Args:\n    image_id: a unique image identifier either of type integer or string.\n    next_annotation_id: integer specifying the first id to use for the\n      groundtruth annotations. All annotations are assigned a continuous integer\n      id starting from this value.\n    category_id_set: A set of valid class ids. Groundtruth with classes not in\n      category_id_set are dropped.\n    groundtruth_boxes: numpy array (float32) with shape [num_gt_boxes, 4]\n    groundtruth_classes: numpy array (int) with shape [num_gt_boxes]\n    groundtruth_masks: optional uint8 numpy array of shape [num_detections,\n      image_height, image_width] containing detection_masks.\n    groundtruth_is_crowd: optional numpy array (int) with shape [num_gt_boxes]\n      indicating whether groundtruth boxes are crowd.\n\n  Returns:\n    a list of groundtruth annotations for a single image in the COCO format.\n\n  Raises:\n    ValueError: if (1) groundtruth_boxes and groundtruth_classes do not have the\n      right lengths or (2) if each of the elements inside these lists do not\n      have the correct shapes or (3) if image_ids are not integers\n  """"""\n\n  if len(groundtruth_classes.shape) != 1:\n    raise ValueError(\'groundtruth_classes is \'\n                     \'expected to be of rank 1.\')\n  if len(groundtruth_boxes.shape) != 2:\n    raise ValueError(\'groundtruth_boxes is expected to be of \'\n                     \'rank 2.\')\n  if groundtruth_boxes.shape[1] != 4:\n    raise ValueError(\'groundtruth_boxes should have \'\n                     \'shape[1] == 4.\')\n  num_boxes = groundtruth_classes.shape[0]\n  if num_boxes != groundtruth_boxes.shape[0]:\n    raise ValueError(\'Corresponding entries in groundtruth_classes, \'\n                     \'and groundtruth_boxes should have \'\n                     \'compatible shapes (i.e., agree on the 0th dimension).\'\n                     \'Classes shape: %d. Boxes shape: %d. Image ID: %s\' % (\n                         groundtruth_classes.shape[0],\n                         groundtruth_boxes.shape[0], image_id))\n  has_is_crowd = groundtruth_is_crowd is not None\n  if has_is_crowd and len(groundtruth_is_crowd.shape) != 1:\n    raise ValueError(\'groundtruth_is_crowd is expected to be of rank 1.\')\n  groundtruth_list = []\n  for i in range(num_boxes):\n    if groundtruth_classes[i] in category_id_set:\n      iscrowd = groundtruth_is_crowd[i] if has_is_crowd else 0\n      export_dict = {\n          \'id\':\n              next_annotation_id + i,\n          \'image_id\':\n              image_id,\n          \'category_id\':\n              int(groundtruth_classes[i]),\n          \'bbox\':\n              list(_ConvertBoxToCOCOFormat(groundtruth_boxes[i, :])),\n          \'area\':\n              float((groundtruth_boxes[i, 2] - groundtruth_boxes[i, 0]) *\n                    (groundtruth_boxes[i, 3] - groundtruth_boxes[i, 1])),\n          \'iscrowd\':\n              iscrowd\n      }\n      if groundtruth_masks is not None:\n        export_dict[\'segmentation\'] = _RleCompress(groundtruth_masks[i])\n      groundtruth_list.append(export_dict)\n  return groundtruth_list\n\n\ndef ExportGroundtruthToCOCO(image_ids,\n                            groundtruth_boxes,\n                            groundtruth_classes,\n                            categories,\n                            output_path=None):\n  """"""Export groundtruth detection annotations in numpy arrays to COCO API.\n\n  This function converts a set of groundtruth detection annotations represented\n  as numpy arrays to dictionaries that can be ingested by the COCO API.\n  Inputs to this function are three lists: image ids for each groundtruth image,\n  groundtruth boxes for each image and groundtruth classes respectively.\n  Note that the image_ids provided here must match the ones given to the\n  ExportDetectionsToCOCO function in order for evaluation to work properly.\n  We assume that for each image, boxes, scores and classes are in\n  correspondence --- that is: image_id[i], groundtruth_boxes[i, :] and\n  groundtruth_classes[i] are associated with the same groundtruth annotation.\n\n  In the exported result, ""area"" fields are always set to the area of the\n  groundtruth bounding box and ""iscrowd"" fields are always set to 0.\n  TODO(jonathanhuang): pass in ""iscrowd"" array for evaluating on COCO dataset.\n\n  Args:\n    image_ids: a list of unique image identifier either of type integer or\n      string.\n    groundtruth_boxes: list of numpy arrays with shape [num_gt_boxes, 4]\n      (note that num_gt_boxes can be different for each entry in the list)\n    groundtruth_classes: list of numpy arrays (int) with shape [num_gt_boxes]\n      (note that num_gt_boxes can be different for each entry in the list)\n    categories: a list of dictionaries representing all possible categories.\n        Each dict in this list has the following keys:\n          \'id\': (required) an integer id uniquely identifying this category\n          \'name\': (required) string representing category name\n            e.g., \'cat\', \'dog\', \'pizza\'\n          \'supercategory\': (optional) string representing the supercategory\n            e.g., \'animal\', \'vehicle\', \'food\', etc\n    output_path: (optional) path for exporting result to JSON\n  Returns:\n    dictionary that can be read by COCO API\n  Raises:\n    ValueError: if (1) groundtruth_boxes and groundtruth_classes do not have the\n      right lengths or (2) if each of the elements inside these lists do not\n      have the correct shapes or (3) if image_ids are not integers\n  """"""\n  category_id_set = set([cat[\'id\'] for cat in categories])\n  groundtruth_export_list = []\n  image_export_list = []\n  if not len(image_ids) == len(groundtruth_boxes) == len(groundtruth_classes):\n    raise ValueError(\'Input lists must have the same length\')\n\n  # For reasons internal to the COCO API, it is important that annotation ids\n  # are not equal to zero; we thus start counting from 1.\n  annotation_id = 1\n  for image_id, boxes, classes in zip(image_ids, groundtruth_boxes,\n                                      groundtruth_classes):\n    image_export_list.append({\'id\': image_id})\n    groundtruth_export_list.extend(ExportSingleImageGroundtruthToCoco(\n        image_id,\n        annotation_id,\n        category_id_set,\n        boxes,\n        classes))\n    num_boxes = classes.shape[0]\n    annotation_id += num_boxes\n\n  groundtruth_dict = {\n      \'annotations\': groundtruth_export_list,\n      \'images\': image_export_list,\n      \'categories\': categories\n  }\n  if output_path:\n    with tf.gfile.GFile(output_path, \'w\') as fid:\n      json_utils.Dump(groundtruth_dict, fid, float_digits=4, indent=2)\n  return groundtruth_dict\n\n\ndef ExportSingleImageDetectionBoxesToCoco(image_id,\n                                          category_id_set,\n                                          detection_boxes,\n                                          detection_scores,\n                                          detection_classes):\n  """"""Export detections of a single image to COCO format.\n\n  This function converts detections represented as numpy arrays to dictionaries\n  that can be ingested by the COCO evaluation API. Note that the image_ids\n  provided here must match the ones given to the\n  ExporSingleImageDetectionBoxesToCoco. We assume that boxes, and classes are in\n  correspondence - that is: boxes[i, :], and classes[i]\n  are associated with the same groundtruth annotation.\n\n  Args:\n    image_id: unique image identifier either of type integer or string.\n    category_id_set: A set of valid class ids. Detections with classes not in\n      category_id_set are dropped.\n    detection_boxes: float numpy array of shape [num_detections, 4] containing\n      detection boxes.\n    detection_scores: float numpy array of shape [num_detections] containing\n      scored for the detection boxes.\n    detection_classes: integer numpy array of shape [num_detections] containing\n      the classes for detection boxes.\n\n  Returns:\n    a list of detection annotations for a single image in the COCO format.\n\n  Raises:\n    ValueError: if (1) detection_boxes, detection_scores and detection_classes\n      do not have the right lengths or (2) if each of the elements inside these\n      lists do not have the correct shapes or (3) if image_ids are not integers.\n  """"""\n\n  if len(detection_classes.shape) != 1 or len(detection_scores.shape) != 1:\n    raise ValueError(\'All entries in detection_classes and detection_scores\'\n                     \'expected to be of rank 1.\')\n  if len(detection_boxes.shape) != 2:\n    raise ValueError(\'All entries in detection_boxes expected to be of \'\n                     \'rank 2.\')\n  if detection_boxes.shape[1] != 4:\n    raise ValueError(\'All entries in detection_boxes should have \'\n                     \'shape[1] == 4.\')\n  num_boxes = detection_classes.shape[0]\n  if not num_boxes == detection_boxes.shape[0] == detection_scores.shape[0]:\n    raise ValueError(\'Corresponding entries in detection_classes, \'\n                     \'detection_scores and detection_boxes should have \'\n                     \'compatible shapes (i.e., agree on the 0th dimension). \'\n                     \'Classes shape: %d. Boxes shape: %d. \'\n                     \'Scores shape: %d\' % (\n                         detection_classes.shape[0], detection_boxes.shape[0],\n                         detection_scores.shape[0]\n                     ))\n  detections_list = []\n  for i in range(num_boxes):\n    if detection_classes[i] in category_id_set:\n      detections_list.append({\n          \'image_id\': image_id,\n          \'category_id\': int(detection_classes[i]),\n          \'bbox\': list(_ConvertBoxToCOCOFormat(detection_boxes[i, :])),\n          \'score\': float(detection_scores[i])\n      })\n  return detections_list\n\n\ndef ExportSingleImageDetectionMasksToCoco(image_id,\n                                          category_id_set,\n                                          detection_masks,\n                                          detection_scores,\n                                          detection_classes):\n  """"""Export detection masks of a single image to COCO format.\n\n  This function converts detections represented as numpy arrays to dictionaries\n  that can be ingested by the COCO evaluation API. We assume that\n  detection_masks, detection_scores, and detection_classes are in correspondence\n  - that is: detection_masks[i, :], detection_classes[i] and detection_scores[i]\n    are associated with the same annotation.\n\n  Args:\n    image_id: unique image identifier either of type integer or string.\n    category_id_set: A set of valid class ids. Detections with classes not in\n      category_id_set are dropped.\n    detection_masks: uint8 numpy array of shape [num_detections, image_height,\n      image_width] containing detection_masks.\n    detection_scores: float numpy array of shape [num_detections] containing\n      scores for detection masks.\n    detection_classes: integer numpy array of shape [num_detections] containing\n      the classes for detection masks.\n\n  Returns:\n    a list of detection mask annotations for a single image in the COCO format.\n\n  Raises:\n    ValueError: if (1) detection_masks, detection_scores and detection_classes\n      do not have the right lengths or (2) if each of the elements inside these\n      lists do not have the correct shapes or (3) if image_ids are not integers.\n  """"""\n\n  if len(detection_classes.shape) != 1 or len(detection_scores.shape) != 1:\n    raise ValueError(\'All entries in detection_classes and detection_scores\'\n                     \'expected to be of rank 1.\')\n  num_boxes = detection_classes.shape[0]\n  if not num_boxes == len(detection_masks) == detection_scores.shape[0]:\n    raise ValueError(\'Corresponding entries in detection_classes, \'\n                     \'detection_scores and detection_masks should have \'\n                     \'compatible lengths and shapes \'\n                     \'Classes length: %d.  Masks length: %d. \'\n                     \'Scores length: %d\' % (\n                         detection_classes.shape[0], len(detection_masks),\n                         detection_scores.shape[0]\n                     ))\n  detections_list = []\n  for i in range(num_boxes):\n    if detection_classes[i] in category_id_set:\n      detections_list.append({\n          \'image_id\': image_id,\n          \'category_id\': int(detection_classes[i]),\n          \'segmentation\': _RleCompress(detection_masks[i]),\n          \'score\': float(detection_scores[i])\n      })\n  return detections_list\n\n\ndef ExportDetectionsToCOCO(image_ids,\n                           detection_boxes,\n                           detection_scores,\n                           detection_classes,\n                           categories,\n                           output_path=None):\n  """"""Export detection annotations in numpy arrays to COCO API.\n\n  This function converts a set of predicted detections represented\n  as numpy arrays to dictionaries that can be ingested by the COCO API.\n  Inputs to this function are lists, consisting of boxes, scores and\n  classes, respectively, corresponding to each image for which detections\n  have been produced.  Note that the image_ids provided here must\n  match the ones given to the ExportGroundtruthToCOCO function in order\n  for evaluation to work properly.\n\n  We assume that for each image, boxes, scores and classes are in\n  correspondence --- that is: detection_boxes[i, :], detection_scores[i] and\n  detection_classes[i] are associated with the same detection.\n\n  Args:\n    image_ids: a list of unique image identifier either of type integer or\n      string.\n    detection_boxes: list of numpy arrays with shape [num_detection_boxes, 4]\n    detection_scores: list of numpy arrays (float) with shape\n      [num_detection_boxes]. Note that num_detection_boxes can be different\n      for each entry in the list.\n    detection_classes: list of numpy arrays (int) with shape\n      [num_detection_boxes]. Note that num_detection_boxes can be different\n      for each entry in the list.\n    categories: a list of dictionaries representing all possible categories.\n      Each dict in this list must have an integer \'id\' key uniquely identifying\n      this category.\n    output_path: (optional) path for exporting result to JSON\n\n  Returns:\n    list of dictionaries that can be read by COCO API, where each entry\n    corresponds to a single detection and has keys from:\n    [\'image_id\', \'category_id\', \'bbox\', \'score\'].\n  Raises:\n    ValueError: if (1) detection_boxes and detection_classes do not have the\n      right lengths or (2) if each of the elements inside these lists do not\n      have the correct shapes or (3) if image_ids are not integers.\n  """"""\n  category_id_set = set([cat[\'id\'] for cat in categories])\n  detections_export_list = []\n  if not (len(image_ids) == len(detection_boxes) == len(detection_scores) ==\n          len(detection_classes)):\n    raise ValueError(\'Input lists must have the same length\')\n  for image_id, boxes, scores, classes in zip(image_ids, detection_boxes,\n                                              detection_scores,\n                                              detection_classes):\n    detections_export_list.extend(ExportSingleImageDetectionBoxesToCoco(\n        image_id,\n        category_id_set,\n        boxes,\n        scores,\n        classes))\n  if output_path:\n    with tf.gfile.GFile(output_path, \'w\') as fid:\n      json_utils.Dump(detections_export_list, fid, float_digits=4, indent=2)\n  return detections_export_list\n\n\ndef ExportSegmentsToCOCO(image_ids,\n                         detection_masks,\n                         detection_scores,\n                         detection_classes,\n                         categories,\n                         output_path=None):\n  """"""Export segmentation masks in numpy arrays to COCO API.\n\n  This function converts a set of predicted instance masks represented\n  as numpy arrays to dictionaries that can be ingested by the COCO API.\n  Inputs to this function are lists, consisting of segments, scores and\n  classes, respectively, corresponding to each image for which detections\n  have been produced.\n\n  Note this function is recommended to use for small dataset.\n  For large dataset, it should be used with a merge function\n  (e.g. in map reduce), otherwise the memory consumption is large.\n\n  We assume that for each image, masks, scores and classes are in\n  correspondence --- that is: detection_masks[i, :, :, :], detection_scores[i]\n  and detection_classes[i] are associated with the same detection.\n\n  Args:\n    image_ids: list of image ids (typically ints or strings)\n    detection_masks: list of numpy arrays with shape [num_detection, h, w, 1]\n      and type uint8. The height and width should match the shape of\n      corresponding image.\n    detection_scores: list of numpy arrays (float) with shape\n      [num_detection]. Note that num_detection can be different\n      for each entry in the list.\n    detection_classes: list of numpy arrays (int) with shape\n      [num_detection]. Note that num_detection can be different\n      for each entry in the list.\n    categories: a list of dictionaries representing all possible categories.\n      Each dict in this list must have an integer \'id\' key uniquely identifying\n      this category.\n    output_path: (optional) path for exporting result to JSON\n\n  Returns:\n    list of dictionaries that can be read by COCO API, where each entry\n    corresponds to a single detection and has keys from:\n    [\'image_id\', \'category_id\', \'segmentation\', \'score\'].\n\n  Raises:\n    ValueError: if detection_masks and detection_classes do not have the\n      right lengths or if each of the elements inside these lists do not\n      have the correct shapes.\n  """"""\n  if not (len(image_ids) == len(detection_masks) == len(detection_scores) ==\n          len(detection_classes)):\n    raise ValueError(\'Input lists must have the same length\')\n\n  segment_export_list = []\n  for image_id, masks, scores, classes in zip(image_ids, detection_masks,\n                                              detection_scores,\n                                              detection_classes):\n\n    if len(classes.shape) != 1 or len(scores.shape) != 1:\n      raise ValueError(\'All entries in detection_classes and detection_scores\'\n                       \'expected to be of rank 1.\')\n    if len(masks.shape) != 4:\n      raise ValueError(\'All entries in masks expected to be of \'\n                       \'rank 4. Given {}\'.format(masks.shape))\n\n    num_boxes = classes.shape[0]\n    if not num_boxes == masks.shape[0] == scores.shape[0]:\n      raise ValueError(\'Corresponding entries in segment_classes, \'\n                       \'detection_scores and detection_boxes should have \'\n                       \'compatible shapes (i.e., agree on the 0th dimension).\')\n\n    category_id_set = set([cat[\'id\'] for cat in categories])\n    segment_export_list.extend(ExportSingleImageDetectionMasksToCoco(\n        image_id, category_id_set, np.squeeze(masks, axis=3), scores, classes))\n\n  if output_path:\n    with tf.gfile.GFile(output_path, \'w\') as fid:\n      json_utils.Dump(segment_export_list, fid, float_digits=4, indent=2)\n  return segment_export_list\n\n\ndef ExportKeypointsToCOCO(image_ids,\n                          detection_keypoints,\n                          detection_scores,\n                          detection_classes,\n                          categories,\n                          output_path=None):\n  """"""Exports keypoints in numpy arrays to COCO API.\n\n  This function converts a set of predicted keypoints represented\n  as numpy arrays to dictionaries that can be ingested by the COCO API.\n  Inputs to this function are lists, consisting of keypoints, scores and\n  classes, respectively, corresponding to each image for which detections\n  have been produced.\n\n  We assume that for each image, keypoints, scores and classes are in\n  correspondence --- that is: detection_keypoints[i, :, :, :],\n  detection_scores[i] and detection_classes[i] are associated with the same\n  detection.\n\n  Args:\n    image_ids: list of image ids (typically ints or strings)\n    detection_keypoints: list of numpy arrays with shape\n      [num_detection, num_keypoints, 2] and type float32 in absolute\n      x-y coordinates.\n    detection_scores: list of numpy arrays (float) with shape\n      [num_detection]. Note that num_detection can be different\n      for each entry in the list.\n    detection_classes: list of numpy arrays (int) with shape\n      [num_detection]. Note that num_detection can be different\n      for each entry in the list.\n    categories: a list of dictionaries representing all possible categories.\n      Each dict in this list must have an integer \'id\' key uniquely identifying\n      this category and an integer \'num_keypoints\' key specifying the number of\n      keypoints the category has.\n    output_path: (optional) path for exporting result to JSON\n\n  Returns:\n    list of dictionaries that can be read by COCO API, where each entry\n    corresponds to a single detection and has keys from:\n    [\'image_id\', \'category_id\', \'keypoints\', \'score\'].\n\n  Raises:\n    ValueError: if detection_keypoints and detection_classes do not have the\n      right lengths or if each of the elements inside these lists do not\n      have the correct shapes.\n  """"""\n  if not (len(image_ids) == len(detection_keypoints) ==\n          len(detection_scores) == len(detection_classes)):\n    raise ValueError(\'Input lists must have the same length\')\n\n  keypoints_export_list = []\n  for image_id, keypoints, scores, classes in zip(\n      image_ids, detection_keypoints, detection_scores, detection_classes):\n\n    if len(classes.shape) != 1 or len(scores.shape) != 1:\n      raise ValueError(\'All entries in detection_classes and detection_scores\'\n                       \'expected to be of rank 1.\')\n    if len(keypoints.shape) != 3:\n      raise ValueError(\'All entries in keypoints expected to be of \'\n                       \'rank 3. Given {}\'.format(keypoints.shape))\n\n    num_boxes = classes.shape[0]\n    if not num_boxes == keypoints.shape[0] == scores.shape[0]:\n      raise ValueError(\'Corresponding entries in detection_classes, \'\n                       \'detection_keypoints, and detection_scores should have \'\n                       \'compatible shapes (i.e., agree on the 0th dimension).\')\n\n    category_id_set = set([cat[\'id\'] for cat in categories])\n    category_id_to_num_keypoints_map = {\n        cat[\'id\']: cat[\'num_keypoints\'] for cat in categories\n        if \'num_keypoints\' in cat}\n\n    for i in range(num_boxes):\n      if classes[i] not in category_id_set:\n        raise ValueError(\'class id should be in category_id_set\\n\')\n\n      if classes[i] in category_id_to_num_keypoints_map:\n        num_keypoints = category_id_to_num_keypoints_map[classes[i]]\n        # Adds extra ones to indicate the visibility for each keypoint as is\n        # recommended by MSCOCO.\n        instance_keypoints = np.concatenate(\n            [keypoints[i, 0:num_keypoints, :],\n             np.expand_dims(np.ones(num_keypoints), axis=1)],\n            axis=1).astype(int)\n\n        instance_keypoints = instance_keypoints.flatten().tolist()\n        keypoints_export_list.append({\n            \'image_id\': image_id,\n            \'category_id\': int(classes[i]),\n            \'keypoints\': instance_keypoints,\n            \'score\': float(scores[i])\n        })\n\n  if output_path:\n    with tf.gfile.GFile(output_path, \'w\') as fid:\n      json_utils.Dump(keypoints_export_list, fid, float_digits=4, indent=2)\n  return keypoints_export_list\n'"
src/object_detection/metrics/coco_tools_test.py,10,"b'# Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Tests for tensorflow_model.object_detection.metrics.coco_tools.""""""\nimport json\nimport os\nimport re\nimport numpy as np\n\nfrom pycocotools import mask\n\nimport tensorflow as tf\n\nfrom object_detection.metrics import coco_tools\n\n\nclass CocoToolsTest(tf.test.TestCase):\n\n  def setUp(self):\n    groundtruth_annotations_list = [\n        {\n            \'id\': 1,\n            \'image_id\': \'first\',\n            \'category_id\': 1,\n            \'bbox\': [100., 100., 100., 100.],\n            \'area\': 100.**2,\n            \'iscrowd\': 0\n        },\n        {\n            \'id\': 2,\n            \'image_id\': \'second\',\n            \'category_id\': 1,\n            \'bbox\': [50., 50., 50., 50.],\n            \'area\': 50.**2,\n            \'iscrowd\': 0\n        },\n    ]\n    image_list = [{\'id\': \'first\'}, {\'id\': \'second\'}]\n    category_list = [{\'id\': 0, \'name\': \'person\'},\n                     {\'id\': 1, \'name\': \'cat\'},\n                     {\'id\': 2, \'name\': \'dog\'}]\n    self._groundtruth_dict = {\n        \'annotations\': groundtruth_annotations_list,\n        \'images\': image_list,\n        \'categories\': category_list\n    }\n\n    self._detections_list = [\n        {\n            \'image_id\': \'first\',\n            \'category_id\': 1,\n            \'bbox\': [100., 100., 100., 100.],\n            \'score\': .8\n        },\n        {\n            \'image_id\': \'second\',\n            \'category_id\': 1,\n            \'bbox\': [50., 50., 50., 50.],\n            \'score\': .7\n        },\n    ]\n\n  def testCocoWrappers(self):\n    groundtruth = coco_tools.COCOWrapper(self._groundtruth_dict)\n    detections = groundtruth.LoadAnnotations(self._detections_list)\n    evaluator = coco_tools.COCOEvalWrapper(groundtruth, detections)\n    summary_metrics, _ = evaluator.ComputeMetrics()\n    self.assertAlmostEqual(1.0, summary_metrics[\'Precision/mAP\'])\n\n  def testExportGroundtruthToCOCO(self):\n    image_ids = [\'first\', \'second\']\n    groundtruth_boxes = [np.array([[100, 100, 200, 200]], np.float),\n                         np.array([[50, 50, 100, 100]], np.float)]\n    groundtruth_classes = [np.array([1], np.int32), np.array([1], np.int32)]\n    categories = [{\'id\': 0, \'name\': \'person\'},\n                  {\'id\': 1, \'name\': \'cat\'},\n                  {\'id\': 2, \'name\': \'dog\'}]\n    output_path = os.path.join(tf.test.get_temp_dir(), \'groundtruth.json\')\n    result = coco_tools.ExportGroundtruthToCOCO(\n        image_ids,\n        groundtruth_boxes,\n        groundtruth_classes,\n        categories,\n        output_path=output_path)\n    self.assertDictEqual(result, self._groundtruth_dict)\n    with tf.gfile.GFile(output_path, \'r\') as f:\n      written_result = f.read()\n      # The json output should have floats written to 4 digits of precision.\n      matcher = re.compile(r\'""bbox"":\\s+\\[\\n\\s+\\d+.\\d\\d\\d\\d,\', re.MULTILINE)\n      self.assertTrue(matcher.findall(written_result))\n      written_result = json.loads(written_result)\n      self.assertAlmostEqual(result, written_result)\n\n  def testExportDetectionsToCOCO(self):\n    image_ids = [\'first\', \'second\']\n    detections_boxes = [np.array([[100, 100, 200, 200]], np.float),\n                        np.array([[50, 50, 100, 100]], np.float)]\n    detections_scores = [np.array([.8], np.float), np.array([.7], np.float)]\n    detections_classes = [np.array([1], np.int32), np.array([1], np.int32)]\n    categories = [{\'id\': 0, \'name\': \'person\'},\n                  {\'id\': 1, \'name\': \'cat\'},\n                  {\'id\': 2, \'name\': \'dog\'}]\n    output_path = os.path.join(tf.test.get_temp_dir(), \'detections.json\')\n    result = coco_tools.ExportDetectionsToCOCO(\n        image_ids,\n        detections_boxes,\n        detections_scores,\n        detections_classes,\n        categories,\n        output_path=output_path)\n    self.assertListEqual(result, self._detections_list)\n    with tf.gfile.GFile(output_path, \'r\') as f:\n      written_result = f.read()\n      # The json output should have floats written to 4 digits of precision.\n      matcher = re.compile(r\'""bbox"":\\s+\\[\\n\\s+\\d+.\\d\\d\\d\\d,\', re.MULTILINE)\n      self.assertTrue(matcher.findall(written_result))\n      written_result = json.loads(written_result)\n      self.assertAlmostEqual(result, written_result)\n\n  def testExportSegmentsToCOCO(self):\n    image_ids = [\'first\', \'second\']\n    detection_masks = [np.array(\n        [[[0, 1, 0, 1], [0, 1, 1, 0], [0, 0, 0, 1], [0, 1, 0, 1]]],\n        dtype=np.uint8), np.array(\n            [[[0, 1, 0, 1], [0, 1, 1, 0], [0, 0, 0, 1], [0, 1, 0, 1]]],\n            dtype=np.uint8)]\n\n    for i, detection_mask in enumerate(detection_masks):\n      detection_masks[i] = detection_mask[:, :, :, None]\n\n    detection_scores = [np.array([.8], np.float), np.array([.7], np.float)]\n    detection_classes = [np.array([1], np.int32), np.array([1], np.int32)]\n\n    categories = [{\'id\': 0, \'name\': \'person\'},\n                  {\'id\': 1, \'name\': \'cat\'},\n                  {\'id\': 2, \'name\': \'dog\'}]\n    output_path = os.path.join(tf.test.get_temp_dir(), \'segments.json\')\n    result = coco_tools.ExportSegmentsToCOCO(\n        image_ids,\n        detection_masks,\n        detection_scores,\n        detection_classes,\n        categories,\n        output_path=output_path)\n    with tf.gfile.GFile(output_path, \'r\') as f:\n      written_result = f.read()\n      written_result = json.loads(written_result)\n      mask_load = mask.decode([written_result[0][\'segmentation\']])\n      self.assertTrue(np.allclose(mask_load, detection_masks[0]))\n      self.assertAlmostEqual(result, written_result)\n\n  def testExportKeypointsToCOCO(self):\n    image_ids = [\'first\', \'second\']\n    detection_keypoints = [\n        np.array(\n            [[[100, 200], [300, 400], [500, 600]],\n             [[50, 150], [250, 350], [450, 550]]], dtype=np.int32),\n        np.array(\n            [[[110, 210], [310, 410], [510, 610]],\n             [[60, 160], [260, 360], [460, 560]]], dtype=np.int32)]\n\n    detection_scores = [np.array([.8, 0.2], np.float),\n                        np.array([.7, 0.3], np.float)]\n    detection_classes = [np.array([1, 1], np.int32), np.array([1, 1], np.int32)]\n\n    categories = [{\'id\': 1, \'name\': \'person\', \'num_keypoints\': 3},\n                  {\'id\': 2, \'name\': \'cat\'},\n                  {\'id\': 3, \'name\': \'dog\'}]\n\n    output_path = os.path.join(tf.test.get_temp_dir(), \'keypoints.json\')\n    result = coco_tools.ExportKeypointsToCOCO(\n        image_ids,\n        detection_keypoints,\n        detection_scores,\n        detection_classes,\n        categories,\n        output_path=output_path)\n\n    with tf.gfile.GFile(output_path, \'r\') as f:\n      written_result = f.read()\n      written_result = json.loads(written_result)\n      self.assertAlmostEqual(result, written_result)\n\n  def testSingleImageDetectionBoxesExport(self):\n    boxes = np.array([[0, 0, 1, 1],\n                      [0, 0, .5, .5],\n                      [.5, .5, 1, 1]], dtype=np.float32)\n    classes = np.array([1, 2, 3], dtype=np.int32)\n    scores = np.array([0.8, 0.2, 0.7], dtype=np.float32)\n    coco_boxes = np.array([[0, 0, 1, 1],\n                           [0, 0, .5, .5],\n                           [.5, .5, .5, .5]], dtype=np.float32)\n    coco_annotations = coco_tools.ExportSingleImageDetectionBoxesToCoco(\n        image_id=\'first_image\',\n        category_id_set=set([1, 2, 3]),\n        detection_boxes=boxes,\n        detection_classes=classes,\n        detection_scores=scores)\n    for i, annotation in enumerate(coco_annotations):\n      self.assertEqual(annotation[\'image_id\'], \'first_image\')\n      self.assertEqual(annotation[\'category_id\'], classes[i])\n      self.assertAlmostEqual(annotation[\'score\'], scores[i])\n      self.assertTrue(np.all(np.isclose(annotation[\'bbox\'], coco_boxes[i])))\n\n  def testSingleImageDetectionMaskExport(self):\n    masks = np.array(\n        [[[1, 1,], [1, 1]],\n         [[0, 0], [0, 1]],\n         [[0, 0], [0, 0]]], dtype=np.uint8)\n    classes = np.array([1, 2, 3], dtype=np.int32)\n    scores = np.array([0.8, 0.2, 0.7], dtype=np.float32)\n    coco_annotations = coco_tools.ExportSingleImageDetectionMasksToCoco(\n        image_id=\'first_image\',\n        category_id_set=set([1, 2, 3]),\n        detection_classes=classes,\n        detection_scores=scores,\n        detection_masks=masks)\n    expected_counts = [\'04\', \'31\', \'4\']\n    for i, mask_annotation in enumerate(coco_annotations):\n      self.assertEqual(mask_annotation[\'segmentation\'][\'counts\'],\n                       expected_counts[i])\n      self.assertTrue(np.all(np.equal(mask.decode(\n          mask_annotation[\'segmentation\']), masks[i])))\n      self.assertEqual(mask_annotation[\'image_id\'], \'first_image\')\n      self.assertEqual(mask_annotation[\'category_id\'], classes[i])\n      self.assertAlmostEqual(mask_annotation[\'score\'], scores[i])\n\n  def testSingleImageGroundtruthExport(self):\n    masks = np.array(\n        [[[1, 1,], [1, 1]],\n         [[0, 0], [0, 1]],\n         [[0, 0], [0, 0]]], dtype=np.uint8)\n    boxes = np.array([[0, 0, 1, 1],\n                      [0, 0, .5, .5],\n                      [.5, .5, 1, 1]], dtype=np.float32)\n    coco_boxes = np.array([[0, 0, 1, 1],\n                           [0, 0, .5, .5],\n                           [.5, .5, .5, .5]], dtype=np.float32)\n    classes = np.array([1, 2, 3], dtype=np.int32)\n    is_crowd = np.array([0, 1, 0], dtype=np.int32)\n    next_annotation_id = 1\n    expected_counts = [\'04\', \'31\', \'4\']\n\n    # Tests exporting without passing in is_crowd (for backward compatibility).\n    coco_annotations = coco_tools.ExportSingleImageGroundtruthToCoco(\n        image_id=\'first_image\',\n        category_id_set=set([1, 2, 3]),\n        next_annotation_id=next_annotation_id,\n        groundtruth_boxes=boxes,\n        groundtruth_classes=classes,\n        groundtruth_masks=masks)\n    for i, annotation in enumerate(coco_annotations):\n      self.assertEqual(annotation[\'segmentation\'][\'counts\'],\n                       expected_counts[i])\n      self.assertTrue(np.all(np.equal(mask.decode(\n          annotation[\'segmentation\']), masks[i])))\n      self.assertTrue(np.all(np.isclose(annotation[\'bbox\'], coco_boxes[i])))\n      self.assertEqual(annotation[\'image_id\'], \'first_image\')\n      self.assertEqual(annotation[\'category_id\'], classes[i])\n      self.assertEqual(annotation[\'id\'], i + next_annotation_id)\n\n    # Tests exporting with is_crowd.\n    coco_annotations = coco_tools.ExportSingleImageGroundtruthToCoco(\n        image_id=\'first_image\',\n        category_id_set=set([1, 2, 3]),\n        next_annotation_id=next_annotation_id,\n        groundtruth_boxes=boxes,\n        groundtruth_classes=classes,\n        groundtruth_masks=masks,\n        groundtruth_is_crowd=is_crowd)\n    for i, annotation in enumerate(coco_annotations):\n      self.assertEqual(annotation[\'segmentation\'][\'counts\'],\n                       expected_counts[i])\n      self.assertTrue(np.all(np.equal(mask.decode(\n          annotation[\'segmentation\']), masks[i])))\n      self.assertTrue(np.all(np.isclose(annotation[\'bbox\'], coco_boxes[i])))\n      self.assertEqual(annotation[\'image_id\'], \'first_image\')\n      self.assertEqual(annotation[\'category_id\'], classes[i])\n      self.assertEqual(annotation[\'iscrowd\'], is_crowd[i])\n      self.assertEqual(annotation[\'id\'], i + next_annotation_id)\n\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
src/object_detection/metrics/offline_eval_map_corloc.py,9,"b'# Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\nr""""""Evaluation executable for detection data.\n\nThis executable evaluates precomputed detections produced by a detection\nmodel and writes the evaluation results into csv file metrics.csv, stored\nin the directory, specified by --eval_dir.\n\nThe evaluation metrics set is supplied in object_detection.protos.EvalConfig\nin metrics_set field.\nCurrently two set of metrics are supported:\n- pascal_voc_metrics: standard PASCAL VOC 2007 metric\n- open_images_detection_metrics: Open Image V2 metric\nAll other field of object_detection.protos.EvalConfig are ignored.\n\nExample usage:\n    ./compute_metrics \\\n        --eval_dir=path/to/eval_dir \\\n        --eval_config_path=path/to/evaluation/configuration/file \\\n        --input_config_path=path/to/input/configuration/file\n""""""\nimport csv\nimport os\nimport re\nimport tensorflow as tf\n\nfrom object_detection import evaluator\nfrom object_detection.core import standard_fields\nfrom object_detection.metrics import tf_example_parser\nfrom object_detection.utils import config_util\nfrom object_detection.utils import label_map_util\n\nflags = tf.app.flags\ntf.logging.set_verbosity(tf.logging.INFO)\n\nflags.DEFINE_string(\'eval_dir\', None, \'Directory to write eval summaries to.\')\nflags.DEFINE_string(\'eval_config_path\', None,\n                    \'Path to an eval_pb2.EvalConfig config file.\')\nflags.DEFINE_string(\'input_config_path\', None,\n                    \'Path to an eval_pb2.InputConfig config file.\')\n\nFLAGS = flags.FLAGS\n\n\ndef _generate_sharded_filenames(filename):\n  m = re.search(r\'@(\\d{1,})\', filename)\n  if m:\n    num_shards = int(m.group(1))\n    return [\n        re.sub(r\'@(\\d{1,})\', \'-%.5d-of-%.5d\' % (i, num_shards), filename)\n        for i in range(num_shards)\n    ]\n  else:\n    return [filename]\n\n\ndef _generate_filenames(filenames):\n  result = []\n  for filename in filenames:\n    result += _generate_sharded_filenames(filename)\n  return result\n\n\ndef read_data_and_evaluate(input_config, eval_config):\n  """"""Reads pre-computed object detections and groundtruth from tf_record.\n\n  Args:\n    input_config: input config proto of type\n      object_detection.protos.InputReader.\n    eval_config: evaluation config proto of type\n      object_detection.protos.EvalConfig.\n\n  Returns:\n    Evaluated detections metrics.\n\n  Raises:\n    ValueError: if input_reader type is not supported or metric type is unknown.\n  """"""\n  if input_config.WhichOneof(\'input_reader\') == \'tf_record_input_reader\':\n    input_paths = input_config.tf_record_input_reader.input_path\n\n    label_map = label_map_util.load_labelmap(input_config.label_map_path)\n    max_num_classes = max([item.id for item in label_map.item])\n    categories = label_map_util.convert_label_map_to_categories(\n        label_map, max_num_classes)\n\n    object_detection_evaluators = evaluator.get_evaluators(\n        eval_config, categories)\n    # Support a single evaluator\n    object_detection_evaluator = object_detection_evaluators[0]\n\n    skipped_images = 0\n    processed_images = 0\n    for input_path in _generate_filenames(input_paths):\n      tf.logging.info(\'Processing file: {0}\'.format(input_path))\n\n      record_iterator = tf.python_io.tf_record_iterator(path=input_path)\n      data_parser = tf_example_parser.TfExampleDetectionAndGTParser()\n\n      for string_record in record_iterator:\n        tf.logging.log_every_n(tf.logging.INFO, \'Processed %d images...\', 1000,\n                               processed_images)\n        processed_images += 1\n\n        example = tf.train.Example()\n        example.ParseFromString(string_record)\n        decoded_dict = data_parser.parse(example)\n\n        if decoded_dict:\n          object_detection_evaluator.add_single_ground_truth_image_info(\n              decoded_dict[standard_fields.DetectionResultFields.key],\n              decoded_dict)\n          object_detection_evaluator.add_single_detected_image_info(\n              decoded_dict[standard_fields.DetectionResultFields.key],\n              decoded_dict)\n        else:\n          skipped_images += 1\n          tf.logging.info(\'Skipped images: {0}\'.format(skipped_images))\n\n    return object_detection_evaluator.evaluate()\n\n  raise ValueError(\'Unsupported input_reader_config.\')\n\n\ndef write_metrics(metrics, output_dir):\n  """"""Write metrics to the output directory.\n\n  Args:\n    metrics: A dictionary containing metric names and values.\n    output_dir: Directory to write metrics to.\n  """"""\n  tf.logging.info(\'Writing metrics.\')\n\n  with open(os.path.join(output_dir, \'metrics.csv\'), \'w\') as csvfile:\n    metrics_writer = csv.writer(csvfile, delimiter=\',\')\n    for metric_name, metric_value in metrics.items():\n      metrics_writer.writerow([metric_name, str(metric_value)])\n\n\ndef main(argv):\n  del argv\n  required_flags = [\'input_config_path\', \'eval_config_path\', \'eval_dir\']\n  for flag_name in required_flags:\n    if not getattr(FLAGS, flag_name):\n      raise ValueError(\'Flag --{} is required\'.format(flag_name))\n\n  configs = config_util.get_configs_from_multiple_files(\n      eval_input_config_path=FLAGS.input_config_path,\n      eval_config_path=FLAGS.eval_config_path)\n\n  eval_config = configs[\'eval_config\']\n  input_config = configs[\'eval_input_config\']\n\n  metrics = read_data_and_evaluate(input_config, eval_config)\n\n  # Save metrics\n  write_metrics(metrics, FLAGS.eval_dir)\n\n\nif __name__ == \'__main__\':\n  tf.app.run(main)\n'"
src/object_detection/metrics/offline_eval_map_corloc_test.py,2,"b'# Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Tests for utilities in offline_eval_map_corloc binary.""""""\n\nimport tensorflow as tf\n\nfrom object_detection.metrics import offline_eval_map_corloc as offline_eval\n\n\nclass OfflineEvalMapCorlocTest(tf.test.TestCase):\n\n  def test_generateShardedFilenames(self):\n    test_filename = \'/path/to/file\'\n    result = offline_eval._generate_sharded_filenames(test_filename)\n    self.assertEqual(result, [test_filename])\n\n    test_filename = \'/path/to/file-00000-of-00050\'\n    result = offline_eval._generate_sharded_filenames(test_filename)\n    self.assertEqual(result, [test_filename])\n\n    result = offline_eval._generate_sharded_filenames(\'/path/to/@3.record\')\n    self.assertEqual(result, [\n        \'/path/to/-00000-of-00003.record\', \'/path/to/-00001-of-00003.record\',\n        \'/path/to/-00002-of-00003.record\'\n    ])\n\n    result = offline_eval._generate_sharded_filenames(\'/path/to/abc@3\')\n    self.assertEqual(result, [\n        \'/path/to/abc-00000-of-00003\', \'/path/to/abc-00001-of-00003\',\n        \'/path/to/abc-00002-of-00003\'\n    ])\n\n    result = offline_eval._generate_sharded_filenames(\'/path/to/@1\')\n    self.assertEqual(result, [\'/path/to/-00000-of-00001\'])\n\n  def test_generateFilenames(self):\n    test_filenames = [\'/path/to/file\', \'/path/to/@3.record\']\n    result = offline_eval._generate_filenames(test_filenames)\n    self.assertEqual(result, [\n        \'/path/to/file\', \'/path/to/-00000-of-00003.record\',\n        \'/path/to/-00001-of-00003.record\', \'/path/to/-00002-of-00003.record\'\n    ])\n\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
src/object_detection/metrics/tf_example_parser.py,2,"b'# Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Tensorflow Example proto parser for data loading.\n\nA parser to decode data containing serialized tensorflow.Example\nprotos into materialized tensors (numpy arrays).\n""""""\n\nimport numpy as np\n\nfrom object_detection.core import data_parser\nfrom object_detection.core import standard_fields as fields\n\n\nclass FloatParser(data_parser.DataToNumpyParser):\n  """"""Tensorflow Example float parser.""""""\n\n  def __init__(self, field_name):\n    self.field_name = field_name\n\n  def parse(self, tf_example):\n    return np.array(\n        tf_example.features.feature[self.field_name].float_list.value,\n        dtype=np.float).transpose() if tf_example.features.feature[\n            self.field_name].HasField(""float_list"") else None\n\n\nclass StringParser(data_parser.DataToNumpyParser):\n  """"""Tensorflow Example string parser.""""""\n\n  def __init__(self, field_name):\n    self.field_name = field_name\n\n  def parse(self, tf_example):\n    return """".join(tf_example.features.feature[self.field_name]\n                   .bytes_list.value) if tf_example.features.feature[\n                       self.field_name].HasField(""bytes_list"") else None\n\n\nclass Int64Parser(data_parser.DataToNumpyParser):\n  """"""Tensorflow Example int64 parser.""""""\n\n  def __init__(self, field_name):\n    self.field_name = field_name\n\n  def parse(self, tf_example):\n    return np.array(\n        tf_example.features.feature[self.field_name].int64_list.value,\n        dtype=np.int64).transpose() if tf_example.features.feature[\n            self.field_name].HasField(""int64_list"") else None\n\n\nclass BoundingBoxParser(data_parser.DataToNumpyParser):\n  """"""Tensorflow Example bounding box parser.""""""\n\n  def __init__(self, xmin_field_name, ymin_field_name, xmax_field_name,\n               ymax_field_name):\n    self.field_names = [\n        ymin_field_name, xmin_field_name, ymax_field_name, xmax_field_name\n    ]\n\n  def parse(self, tf_example):\n    result = []\n    parsed = True\n    for field_name in self.field_names:\n      result.append(tf_example.features.feature[field_name].float_list.value)\n      parsed &= (\n          tf_example.features.feature[field_name].HasField(""float_list""))\n\n    return np.array(result).transpose() if parsed else None\n\n\nclass TfExampleDetectionAndGTParser(data_parser.DataToNumpyParser):\n  """"""Tensorflow Example proto parser.""""""\n\n  def __init__(self):\n    self.items_to_handlers = {\n        fields.DetectionResultFields.key:\n            StringParser(fields.TfExampleFields.source_id),\n        # Object ground truth boxes and classes.\n        fields.InputDataFields.groundtruth_boxes: (BoundingBoxParser(\n            fields.TfExampleFields.object_bbox_xmin,\n            fields.TfExampleFields.object_bbox_ymin,\n            fields.TfExampleFields.object_bbox_xmax,\n            fields.TfExampleFields.object_bbox_ymax)),\n        fields.InputDataFields.groundtruth_classes: (\n            Int64Parser(fields.TfExampleFields.object_class_label)),\n        # Object detections.\n        fields.DetectionResultFields.detection_boxes: (BoundingBoxParser(\n            fields.TfExampleFields.detection_bbox_xmin,\n            fields.TfExampleFields.detection_bbox_ymin,\n            fields.TfExampleFields.detection_bbox_xmax,\n            fields.TfExampleFields.detection_bbox_ymax)),\n        fields.DetectionResultFields.detection_classes: (\n            Int64Parser(fields.TfExampleFields.detection_class_label)),\n        fields.DetectionResultFields.detection_scores: (\n            FloatParser(fields.TfExampleFields.detection_score)),\n    }\n\n    self.optional_items_to_handlers = {\n        fields.InputDataFields.groundtruth_difficult:\n            Int64Parser(fields.TfExampleFields.object_difficult),\n        fields.InputDataFields.groundtruth_group_of:\n            Int64Parser(fields.TfExampleFields.object_group_of)\n    }\n\n  def parse(self, tf_example):\n    """"""Parses tensorflow example and returns a tensor dictionary.\n\n    Args:\n      tf_example: a tf.Example object.\n\n    Returns:\n      A dictionary of the following numpy arrays:\n      fields.DetectionResultFields.source_id - string containing original image\n      id.\n      fields.InputDataFields.groundtruth_boxes - a numpy array containing\n      groundtruth boxes.\n      fields.InputDataFields.groundtruth_classes - a numpy array containing\n      groundtruth classes.\n      fields.InputDataFields.groundtruth_group_of - a numpy array containing\n      groundtruth group of flag (optional, None if not specified).\n      fields.InputDataFields.groundtruth_difficult - a numpy array containing\n      groundtruth difficult flag (optional, None if not specified).\n      fields.DetectionResultFields.detection_boxes - a numpy array containing\n      detection boxes.\n      fields.DetectionResultFields.detection_classes - a numpy array containing\n      detection class labels.\n      fields.DetectionResultFields.detection_scores - a numpy array containing\n      detection scores.\n      Returns None if tf.Example was not parsed or non-optional fields were not\n      found.\n    """"""\n    results_dict = {}\n    parsed = True\n    for key, parser in self.items_to_handlers.items():\n      results_dict[key] = parser.parse(tf_example)\n      parsed &= (results_dict[key] is not None)\n\n    for key, parser in self.optional_items_to_handlers.items():\n      results_dict[key] = parser.parse(tf_example)\n\n    return results_dict if parsed else None\n'"
src/object_detection/metrics/tf_example_parser_test.py,12,"b'# Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Tests for object_detection.data_decoders.tf_example_parser.""""""\n\nimport numpy as np\nimport numpy.testing as np_testing\nimport tensorflow as tf\n\nfrom object_detection.core import standard_fields as fields\nfrom object_detection.metrics import tf_example_parser\n\n\nclass TfExampleDecoderTest(tf.test.TestCase):\n\n  def _Int64Feature(self, value):\n    return tf.train.Feature(int64_list=tf.train.Int64List(value=value))\n\n  def _FloatFeature(self, value):\n    return tf.train.Feature(float_list=tf.train.FloatList(value=value))\n\n  def _BytesFeature(self, value):\n    return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))\n\n  def testParseDetectionsAndGT(self):\n    source_id = \'abc.jpg\'\n    # y_min, x_min, y_max, x_max\n    object_bb = np.array([[0.0, 0.5, 0.3], [0.0, 0.1, 0.6], [1.0, 0.6, 0.8],\n                          [1.0, 0.6, 0.7]]).transpose()\n    detection_bb = np.array([[0.1, 0.2], [0.0, 0.8], [1.0, 0.6],\n                             [1.0, 0.85]]).transpose()\n\n    object_class_label = [1, 1, 2]\n    object_difficult = [1, 0, 0]\n    object_group_of = [0, 0, 1]\n    detection_class_label = [2, 1]\n    detection_score = [0.5, 0.3]\n    features = {\n        fields.TfExampleFields.source_id:\n            self._BytesFeature(source_id),\n        fields.TfExampleFields.object_bbox_ymin:\n            self._FloatFeature(object_bb[:, 0].tolist()),\n        fields.TfExampleFields.object_bbox_xmin:\n            self._FloatFeature(object_bb[:, 1].tolist()),\n        fields.TfExampleFields.object_bbox_ymax:\n            self._FloatFeature(object_bb[:, 2].tolist()),\n        fields.TfExampleFields.object_bbox_xmax:\n            self._FloatFeature(object_bb[:, 3].tolist()),\n        fields.TfExampleFields.detection_bbox_ymin:\n            self._FloatFeature(detection_bb[:, 0].tolist()),\n        fields.TfExampleFields.detection_bbox_xmin:\n            self._FloatFeature(detection_bb[:, 1].tolist()),\n        fields.TfExampleFields.detection_bbox_ymax:\n            self._FloatFeature(detection_bb[:, 2].tolist()),\n        fields.TfExampleFields.detection_bbox_xmax:\n            self._FloatFeature(detection_bb[:, 3].tolist()),\n        fields.TfExampleFields.detection_class_label:\n            self._Int64Feature(detection_class_label),\n        fields.TfExampleFields.detection_score:\n            self._FloatFeature(detection_score),\n    }\n\n    example = tf.train.Example(features=tf.train.Features(feature=features))\n    parser = tf_example_parser.TfExampleDetectionAndGTParser()\n\n    results_dict = parser.parse(example)\n    self.assertIsNone(results_dict)\n\n    features[fields.TfExampleFields.object_class_label] = (\n        self._Int64Feature(object_class_label))\n    features[fields.TfExampleFields.object_difficult] = (\n        self._Int64Feature(object_difficult))\n\n    example = tf.train.Example(features=tf.train.Features(feature=features))\n    results_dict = parser.parse(example)\n\n    self.assertIsNotNone(results_dict)\n    self.assertEqual(source_id, results_dict[fields.DetectionResultFields.key])\n    np_testing.assert_almost_equal(\n        object_bb, results_dict[fields.InputDataFields.groundtruth_boxes])\n    np_testing.assert_almost_equal(\n        detection_bb,\n        results_dict[fields.DetectionResultFields.detection_boxes])\n    np_testing.assert_almost_equal(\n        detection_score,\n        results_dict[fields.DetectionResultFields.detection_scores])\n    np_testing.assert_almost_equal(\n        detection_class_label,\n        results_dict[fields.DetectionResultFields.detection_classes])\n    np_testing.assert_almost_equal(\n        object_difficult,\n        results_dict[fields.InputDataFields.groundtruth_difficult])\n    np_testing.assert_almost_equal(\n        object_class_label,\n        results_dict[fields.InputDataFields.groundtruth_classes])\n\n    parser = tf_example_parser.TfExampleDetectionAndGTParser()\n\n    features[fields.TfExampleFields.object_group_of] = (\n        self._Int64Feature(object_group_of))\n\n    example = tf.train.Example(features=tf.train.Features(feature=features))\n    results_dict = parser.parse(example)\n    self.assertIsNotNone(results_dict)\n    np_testing.assert_almost_equal(\n        object_group_of,\n        results_dict[fields.InputDataFields.groundtruth_group_of])\n\n  def testParseString(self):\n    string_val = \'abc\'\n    features = {\'string\': self._BytesFeature(string_val)}\n    example = tf.train.Example(features=tf.train.Features(feature=features))\n\n    parser = tf_example_parser.StringParser(\'string\')\n    result = parser.parse(example)\n    self.assertIsNotNone(result)\n    self.assertEqual(result, string_val)\n\n    parser = tf_example_parser.StringParser(\'another_string\')\n    result = parser.parse(example)\n    self.assertIsNone(result)\n\n  def testParseFloat(self):\n    float_array_val = [1.5, 1.4, 2.0]\n    features = {\'floats\': self._FloatFeature(float_array_val)}\n    example = tf.train.Example(features=tf.train.Features(feature=features))\n\n    parser = tf_example_parser.FloatParser(\'floats\')\n    result = parser.parse(example)\n    self.assertIsNotNone(result)\n    np_testing.assert_almost_equal(result, float_array_val)\n\n    parser = tf_example_parser.StringParser(\'another_floats\')\n    result = parser.parse(example)\n    self.assertIsNone(result)\n\n  def testInt64Parser(self):\n    int_val = [1, 2, 3]\n    features = {\'ints\': self._Int64Feature(int_val)}\n    example = tf.train.Example(features=tf.train.Features(feature=features))\n\n    parser = tf_example_parser.Int64Parser(\'ints\')\n    result = parser.parse(example)\n    self.assertIsNotNone(result)\n    np_testing.assert_almost_equal(result, int_val)\n\n    parser = tf_example_parser.Int64Parser(\'another_ints\')\n    result = parser.parse(example)\n    self.assertIsNone(result)\n\n  def testBoundingBoxParser(self):\n    bounding_boxes = np.array([[0.0, 0.5, 0.3], [0.0, 0.1, 0.6],\n                               [1.0, 0.6, 0.8], [1.0, 0.6, 0.7]]).transpose()\n    features = {\n        \'ymin\': self._FloatFeature(bounding_boxes[:, 0]),\n        \'xmin\': self._FloatFeature(bounding_boxes[:, 1]),\n        \'ymax\': self._FloatFeature(bounding_boxes[:, 2]),\n        \'xmax\': self._FloatFeature(bounding_boxes[:, 3])\n    }\n\n    example = tf.train.Example(features=tf.train.Features(feature=features))\n\n    parser = tf_example_parser.BoundingBoxParser(\'xmin\', \'ymin\', \'xmax\', \'ymax\')\n    result = parser.parse(example)\n    self.assertIsNotNone(result)\n    np_testing.assert_almost_equal(result, bounding_boxes)\n\n    parser = tf_example_parser.BoundingBoxParser(\'xmin\', \'ymin\', \'xmax\',\n                                                 \'another_ymax\')\n    result = parser.parse(example)\n    self.assertIsNone(result)\n\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
src/object_detection/models/__init__.py,0,b''
src/object_detection/models/embedded_ssd_mobilenet_v1_feature_extractor.py,7,"b'# Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n""""""Embedded-friendly SSDFeatureExtractor for MobilenetV1 features.""""""\n\nimport tensorflow as tf\n\nfrom object_detection.models import feature_map_generators\nfrom object_detection.models import ssd_mobilenet_v1_feature_extractor\nfrom object_detection.utils import ops\nfrom nets import mobilenet_v1\n\nslim = tf.contrib.slim\n\n\nclass EmbeddedSSDMobileNetV1FeatureExtractor(\n    ssd_mobilenet_v1_feature_extractor.SSDMobileNetV1FeatureExtractor):\n  """"""Embedded-friendly SSD Feature Extractor using MobilenetV1 features.\n\n  This feature extractor is similar to SSD MobileNetV1 feature extractor, and\n  it fixes input resolution to be 256x256, reduces the number of feature maps\n  used for box prediction and ensures convolution kernel to be no larger\n  than input tensor in spatial dimensions.\n\n  This feature extractor requires support of the following ops if used in\n  embedded devices:\n  - Conv\n  - DepthwiseConv\n  - Relu6\n\n  All conv/depthwiseconv use SAME padding, and no additional spatial padding is\n  needed.\n  """"""\n\n  def __init__(self,\n               is_training,\n               depth_multiplier,\n               min_depth,\n               pad_to_multiple,\n               conv_hyperparams,\n               batch_norm_trainable=True,\n               reuse_weights=None,\n               use_explicit_padding=False,\n               use_depthwise=False):\n    """"""MobileNetV1 Feature Extractor for Embedded-friendly SSD Models.\n\n    Args:\n      is_training: whether the network is in training mode.\n      depth_multiplier: float depth multiplier for feature extractor.\n      min_depth: minimum feature extractor depth.\n      pad_to_multiple: the nearest multiple to zero pad the input height and\n        width dimensions to. For EmbeddedSSD it must be set to 1.\n      conv_hyperparams: tf slim arg_scope for conv2d and separable_conv2d ops.\n      batch_norm_trainable:  Whether to update batch norm parameters during\n        training or not. When training with a small batch size\n        (e.g. 1), it is desirable to disable batch norm update and use\n        pretrained batch norm params.\n      reuse_weights: Whether to reuse variables. Default is None.\n      use_explicit_padding: Whether to use explicit padding when extracting\n        features. Default is False.\n      use_depthwise: Whether to use depthwise convolutions. Default is False.\n\n    Raises:\n      ValueError: upon invalid `pad_to_multiple` values.\n    """"""\n    if pad_to_multiple != 1:\n      raise ValueError(\'Embedded-specific SSD only supports `pad_to_multiple` \'\n                       \'of 1.\')\n\n    super(EmbeddedSSDMobileNetV1FeatureExtractor, self).__init__(\n        is_training, depth_multiplier, min_depth, pad_to_multiple,\n        conv_hyperparams, batch_norm_trainable, reuse_weights,\n        use_explicit_padding, use_depthwise)\n\n  def extract_features(self, preprocessed_inputs):\n    """"""Extract features from preprocessed inputs.\n\n    Args:\n      preprocessed_inputs: a [batch, height, width, channels] float tensor\n        representing a batch of images.\n\n    Returns:\n      feature_maps: a list of tensors where the ith tensor has shape\n        [batch, height_i, width_i, depth_i]\n\n    Raises:\n      ValueError: if image height or width are not 256 pixels.\n    """"""\n    image_shape = preprocessed_inputs.get_shape()\n    image_shape.assert_has_rank(4)\n    image_height = image_shape[1].value\n    image_width = image_shape[2].value\n\n    if image_height is None or image_width is None:\n      shape_assert = tf.Assert(\n          tf.logical_and(tf.equal(tf.shape(preprocessed_inputs)[1], 256),\n                         tf.equal(tf.shape(preprocessed_inputs)[2], 256)),\n          [\'image size must be 256 in both height and width.\'])\n      with tf.control_dependencies([shape_assert]):\n        preprocessed_inputs = tf.identity(preprocessed_inputs)\n    elif image_height != 256 or image_width != 256:\n      raise ValueError(\'image size must be = 256 in both height and width;\'\n                       \' image dim = %d,%d\' % (image_height, image_width))\n\n    feature_map_layout = {\n        \'from_layer\': [\n            \'Conv2d_11_pointwise\', \'Conv2d_13_pointwise\', \'\', \'\', \'\'\n        ],\n        \'layer_depth\': [-1, -1, 512, 256, 256],\n        \'conv_kernel_size\': [-1, -1, 3, 3, 2],\n        \'use_explicit_padding\': self._use_explicit_padding,\n        \'use_depthwise\': self._use_depthwise,\n    }\n\n    with slim.arg_scope(self._conv_hyperparams):\n      with slim.arg_scope([slim.batch_norm], fused=False):\n        with tf.variable_scope(\'MobilenetV1\',\n                               reuse=self._reuse_weights) as scope:\n          _, image_features = mobilenet_v1.mobilenet_v1_base(\n              ops.pad_to_multiple(preprocessed_inputs, self._pad_to_multiple),\n              final_endpoint=\'Conv2d_13_pointwise\',\n              min_depth=self._min_depth,\n              depth_multiplier=self._depth_multiplier,\n              scope=scope)\n          feature_maps = feature_map_generators.multi_resolution_feature_maps(\n              feature_map_layout=feature_map_layout,\n              depth_multiplier=self._depth_multiplier,\n              min_depth=self._min_depth,\n              insert_1x1_conv=True,\n              image_features=image_features)\n\n    return feature_maps.values()\n'"
src/object_detection/models/embedded_ssd_mobilenet_v1_feature_extractor_test.py,1,"b'# Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n""""""Tests for embedded_ssd_mobilenet_v1_feature_extractor.""""""\nimport numpy as np\nimport tensorflow as tf\n\nfrom object_detection.models import embedded_ssd_mobilenet_v1_feature_extractor\nfrom object_detection.models import ssd_feature_extractor_test\n\n\nclass EmbeddedSSDMobileNetV1FeatureExtractorTest(\n    ssd_feature_extractor_test.SsdFeatureExtractorTestBase):\n\n  def _create_feature_extractor(self, depth_multiplier, pad_to_multiple,\n                                is_training=True, batch_norm_trainable=True):\n    """"""Constructs a new feature extractor.\n\n    Args:\n      depth_multiplier: float depth multiplier for feature extractor\n      pad_to_multiple: the nearest multiple to zero pad the input height and\n        width dimensions to.\n      is_training: whether the network is in training mode.\n      batch_norm_trainable: whether to update batch norm parameters during\n        training.\n\n    Returns:\n      an ssd_meta_arch.SSDFeatureExtractor object.\n    """"""\n    min_depth = 32\n    conv_hyperparams = {}\n    return (embedded_ssd_mobilenet_v1_feature_extractor.\n            EmbeddedSSDMobileNetV1FeatureExtractor(\n                is_training, depth_multiplier, min_depth, pad_to_multiple,\n                conv_hyperparams, batch_norm_trainable))\n\n  def test_extract_features_returns_correct_shapes_256(self):\n    image_height = 256\n    image_width = 256\n    depth_multiplier = 1.0\n    pad_to_multiple = 1\n    expected_feature_map_shape = [(2, 16, 16, 512), (2, 8, 8, 1024),\n                                  (2, 4, 4, 512), (2, 2, 2, 256),\n                                  (2, 1, 1, 256)]\n    self.check_extract_features_returns_correct_shape(\n        2, image_height, image_width, depth_multiplier, pad_to_multiple,\n        expected_feature_map_shape)\n\n  def test_extract_features_returns_correct_shapes_with_dynamic_inputs(self):\n    image_height = 256\n    image_width = 256\n    depth_multiplier = 1.0\n    pad_to_multiple = 1\n    expected_feature_map_shape = [(2, 16, 16, 512), (2, 8, 8, 1024),\n                                  (2, 4, 4, 512), (2, 2, 2, 256),\n                                  (2, 1, 1, 256)]\n    self.check_extract_features_returns_correct_shapes_with_dynamic_inputs(\n        2, image_height, image_width, depth_multiplier, pad_to_multiple,\n        expected_feature_map_shape)\n\n  def test_extract_features_returns_correct_shapes_enforcing_min_depth(self):\n    image_height = 256\n    image_width = 256\n    depth_multiplier = 0.5**12\n    pad_to_multiple = 1\n    expected_feature_map_shape = [(2, 16, 16, 32), (2, 8, 8, 32), (2, 4, 4, 32),\n                                  (2, 2, 2, 32), (2, 1, 1, 32)]\n    self.check_extract_features_returns_correct_shape(\n        2, image_height, image_width, depth_multiplier, pad_to_multiple,\n        expected_feature_map_shape)\n\n  def test_extract_features_returns_correct_shapes_with_pad_to_multiple_of_1(\n      self):\n    image_height = 256\n    image_width = 256\n    depth_multiplier = 1.0\n    pad_to_multiple = 1\n    expected_feature_map_shape = [(2, 16, 16, 512), (2, 8, 8, 1024),\n                                  (2, 4, 4, 512), (2, 2, 2, 256),\n                                  (2, 1, 1, 256)]\n    self.check_extract_features_returns_correct_shape(\n        2, image_height, image_width, depth_multiplier, pad_to_multiple,\n        expected_feature_map_shape)\n\n  def test_extract_features_raises_error_with_pad_to_multiple_not_1(self):\n    depth_multiplier = 1.0\n    pad_to_multiple = 2\n    with self.assertRaises(ValueError):\n      _ = self._create_feature_extractor(depth_multiplier, pad_to_multiple)\n\n  def test_extract_features_raises_error_with_invalid_image_size(self):\n    image_height = 128\n    image_width = 128\n    depth_multiplier = 1.0\n    pad_to_multiple = 1\n    self.check_extract_features_raises_error_with_invalid_image_size(\n        image_height, image_width, depth_multiplier, pad_to_multiple)\n\n  def test_preprocess_returns_correct_value_range(self):\n    image_height = 256\n    image_width = 256\n    depth_multiplier = 1\n    pad_to_multiple = 1\n    test_image = np.random.rand(4, image_height, image_width, 3)\n    feature_extractor = self._create_feature_extractor(depth_multiplier,\n                                                       pad_to_multiple)\n    preprocessed_image = feature_extractor.preprocess(test_image)\n    self.assertTrue(np.all(np.less_equal(np.abs(preprocessed_image), 1.0)))\n\n  def test_variables_only_created_in_scope(self):\n    depth_multiplier = 1\n    pad_to_multiple = 1\n    scope_name = \'MobilenetV1\'\n    self.check_feature_extractor_variables_under_scope(\n        depth_multiplier, pad_to_multiple, scope_name)\n\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
src/object_detection/models/faster_rcnn_inception_resnet_v2_feature_extractor.py,10,"b'# Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n""""""Inception Resnet v2 Faster R-CNN implementation.\n\nSee ""Inception-v4, Inception-ResNet and the Impact of Residual Connections on\nLearning"" by Szegedy et al. (https://arxiv.org/abs/1602.07261)\nas well as\n""Speed/accuracy trade-offs for modern convolutional object detectors"" by\nHuang et al. (https://arxiv.org/abs/1611.10012)\n""""""\n\nimport tensorflow as tf\n\nfrom object_detection.meta_architectures import faster_rcnn_meta_arch\nfrom nets import inception_resnet_v2\n\nslim = tf.contrib.slim\n\n\nclass FasterRCNNInceptionResnetV2FeatureExtractor(\n    faster_rcnn_meta_arch.FasterRCNNFeatureExtractor):\n  """"""Faster R-CNN with Inception Resnet v2 feature extractor implementation.""""""\n\n  def __init__(self,\n               is_training,\n               first_stage_features_stride,\n               batch_norm_trainable=False,\n               reuse_weights=None,\n               weight_decay=0.0):\n    """"""Constructor.\n\n    Args:\n      is_training: See base class.\n      first_stage_features_stride: See base class.\n      batch_norm_trainable: See base class.\n      reuse_weights: See base class.\n      weight_decay: See base class.\n\n    Raises:\n      ValueError: If `first_stage_features_stride` is not 8 or 16.\n    """"""\n    if first_stage_features_stride != 8 and first_stage_features_stride != 16:\n      raise ValueError(\'`first_stage_features_stride` must be 8 or 16.\')\n    super(FasterRCNNInceptionResnetV2FeatureExtractor, self).__init__(\n        is_training, first_stage_features_stride, batch_norm_trainable,\n        reuse_weights, weight_decay)\n\n  def preprocess(self, resized_inputs):\n    """"""Faster R-CNN with Inception Resnet v2 preprocessing.\n\n    Maps pixel values to the range [-1, 1].\n\n    Args:\n      resized_inputs: A [batch, height_in, width_in, channels] float32 tensor\n        representing a batch of images with values between 0 and 255.0.\n\n    Returns:\n      preprocessed_inputs: A [batch, height_out, width_out, channels] float32\n        tensor representing a batch of images.\n\n    """"""\n    return (2.0 / 255.0) * resized_inputs - 1.0\n\n  def _extract_proposal_features(self, preprocessed_inputs, scope):\n    """"""Extracts first stage RPN features.\n\n    Extracts features using the first half of the Inception Resnet v2 network.\n    We construct the network in `align_feature_maps=True` mode, which means\n    that all VALID paddings in the network are changed to SAME padding so that\n    the feature maps are aligned.\n\n    Args:\n      preprocessed_inputs: A [batch, height, width, channels] float32 tensor\n        representing a batch of images.\n      scope: A scope name.\n\n    Returns:\n      rpn_feature_map: A tensor with shape [batch, height, width, depth]\n    Raises:\n      InvalidArgumentError: If the spatial size of `preprocessed_inputs`\n        (height or width) is less than 33.\n      ValueError: If the created network is missing the required activation.\n    """"""\n    if len(preprocessed_inputs.get_shape().as_list()) != 4:\n      raise ValueError(\'`preprocessed_inputs` must be 4 dimensional, got a \'\n                       \'tensor of shape %s\' % preprocessed_inputs.get_shape())\n\n    with slim.arg_scope(inception_resnet_v2.inception_resnet_v2_arg_scope(\n        weight_decay=self._weight_decay)):\n      # Forces is_training to False to disable batch norm update.\n      with slim.arg_scope([slim.batch_norm],\n                          is_training=self._train_batch_norm):\n        with tf.variable_scope(\'InceptionResnetV2\',\n                               reuse=self._reuse_weights) as scope:\n          return inception_resnet_v2.inception_resnet_v2_base(\n              preprocessed_inputs, final_endpoint=\'PreAuxLogits\',\n              scope=scope, output_stride=self._first_stage_features_stride,\n              align_feature_maps=True)\n\n  def _extract_box_classifier_features(self, proposal_feature_maps, scope):\n    """"""Extracts second stage box classifier features.\n\n    This function reconstructs the ""second half"" of the Inception ResNet v2\n    network after the part defined in `_extract_proposal_features`.\n\n    Args:\n      proposal_feature_maps: A 4-D float tensor with shape\n        [batch_size * self.max_num_proposals, crop_height, crop_width, depth]\n        representing the feature map cropped to each proposal.\n      scope: A scope name.\n\n    Returns:\n      proposal_classifier_features: A 4-D float tensor with shape\n        [batch_size * self.max_num_proposals, height, width, depth]\n        representing box classifier features for each proposal.\n    """"""\n    with tf.variable_scope(\'InceptionResnetV2\', reuse=self._reuse_weights):\n      with slim.arg_scope(inception_resnet_v2.inception_resnet_v2_arg_scope(\n          weight_decay=self._weight_decay)):\n        # Forces is_training to False to disable batch norm update.\n        with slim.arg_scope([slim.batch_norm],\n                            is_training=self._train_batch_norm):\n          with slim.arg_scope([slim.conv2d, slim.max_pool2d, slim.avg_pool2d],\n                              stride=1, padding=\'SAME\'):\n            with tf.variable_scope(\'Mixed_7a\'):\n              with tf.variable_scope(\'Branch_0\'):\n                tower_conv = slim.conv2d(proposal_feature_maps,\n                                         256, 1, scope=\'Conv2d_0a_1x1\')\n                tower_conv_1 = slim.conv2d(\n                    tower_conv, 384, 3, stride=2,\n                    padding=\'VALID\', scope=\'Conv2d_1a_3x3\')\n              with tf.variable_scope(\'Branch_1\'):\n                tower_conv1 = slim.conv2d(\n                    proposal_feature_maps, 256, 1, scope=\'Conv2d_0a_1x1\')\n                tower_conv1_1 = slim.conv2d(\n                    tower_conv1, 288, 3, stride=2,\n                    padding=\'VALID\', scope=\'Conv2d_1a_3x3\')\n              with tf.variable_scope(\'Branch_2\'):\n                tower_conv2 = slim.conv2d(\n                    proposal_feature_maps, 256, 1, scope=\'Conv2d_0a_1x1\')\n                tower_conv2_1 = slim.conv2d(tower_conv2, 288, 3,\n                                            scope=\'Conv2d_0b_3x3\')\n                tower_conv2_2 = slim.conv2d(\n                    tower_conv2_1, 320, 3, stride=2,\n                    padding=\'VALID\', scope=\'Conv2d_1a_3x3\')\n              with tf.variable_scope(\'Branch_3\'):\n                tower_pool = slim.max_pool2d(\n                    proposal_feature_maps, 3, stride=2, padding=\'VALID\',\n                    scope=\'MaxPool_1a_3x3\')\n              net = tf.concat(\n                  [tower_conv_1, tower_conv1_1, tower_conv2_2, tower_pool], 3)\n            net = slim.repeat(net, 9, inception_resnet_v2.block8, scale=0.20)\n            net = inception_resnet_v2.block8(net, activation_fn=None)\n            proposal_classifier_features = slim.conv2d(\n                net, 1536, 1, scope=\'Conv2d_7b_1x1\')\n        return proposal_classifier_features\n\n  def restore_from_classification_checkpoint_fn(\n      self,\n      first_stage_feature_extractor_scope,\n      second_stage_feature_extractor_scope):\n    """"""Returns a map of variables to load from a foreign checkpoint.\n\n    Note that this overrides the default implementation in\n    faster_rcnn_meta_arch.FasterRCNNFeatureExtractor which does not work for\n    InceptionResnetV2 checkpoints.\n\n    TODO(jonathanhuang,rathodv): revisit whether it\'s possible to force the\n    `Repeat` namescope as created in `_extract_box_classifier_features` to\n    start counting at 2 (e.g. `Repeat_2`) so that the default restore_fn can\n    be used.\n\n    Args:\n      first_stage_feature_extractor_scope: A scope name for the first stage\n        feature extractor.\n      second_stage_feature_extractor_scope: A scope name for the second stage\n        feature extractor.\n\n    Returns:\n      A dict mapping variable names (to load from a checkpoint) to variables in\n      the model graph.\n    """"""\n\n    variables_to_restore = {}\n    for variable in tf.global_variables():\n      if variable.op.name.startswith(\n          first_stage_feature_extractor_scope):\n        var_name = variable.op.name.replace(\n            first_stage_feature_extractor_scope + \'/\', \'\')\n        variables_to_restore[var_name] = variable\n      if variable.op.name.startswith(\n          second_stage_feature_extractor_scope):\n        var_name = variable.op.name.replace(\n            second_stage_feature_extractor_scope\n            + \'/InceptionResnetV2/Repeat\', \'InceptionResnetV2/Repeat_2\')\n        var_name = var_name.replace(\n            second_stage_feature_extractor_scope + \'/\', \'\')\n        variables_to_restore[var_name] = variable\n    return variables_to_restore\n\n'"
src/object_detection/models/faster_rcnn_inception_resnet_v2_feature_extractor_test.py,20,"b'# Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n""""""Tests for models.faster_rcnn_inception_resnet_v2_feature_extractor.""""""\n\nimport tensorflow as tf\n\nfrom object_detection.models import faster_rcnn_inception_resnet_v2_feature_extractor as frcnn_inc_res\n\n\nclass FasterRcnnInceptionResnetV2FeatureExtractorTest(tf.test.TestCase):\n\n  def _build_feature_extractor(self, first_stage_features_stride):\n    return frcnn_inc_res.FasterRCNNInceptionResnetV2FeatureExtractor(\n        is_training=False,\n        first_stage_features_stride=first_stage_features_stride,\n        batch_norm_trainable=False,\n        reuse_weights=None,\n        weight_decay=0.0)\n\n  def test_extract_proposal_features_returns_expected_size(self):\n    feature_extractor = self._build_feature_extractor(\n        first_stage_features_stride=16)\n    preprocessed_inputs = tf.random_uniform(\n        [1, 299, 299, 3], maxval=255, dtype=tf.float32)\n    rpn_feature_map, _ = feature_extractor.extract_proposal_features(\n        preprocessed_inputs, scope=\'TestScope\')\n    features_shape = tf.shape(rpn_feature_map)\n\n    init_op = tf.global_variables_initializer()\n    with self.test_session() as sess:\n      sess.run(init_op)\n      features_shape_out = sess.run(features_shape)\n      self.assertAllEqual(features_shape_out, [1, 19, 19, 1088])\n\n  def test_extract_proposal_features_stride_eight(self):\n    feature_extractor = self._build_feature_extractor(\n        first_stage_features_stride=8)\n    preprocessed_inputs = tf.random_uniform(\n        [1, 224, 224, 3], maxval=255, dtype=tf.float32)\n    rpn_feature_map, _ = feature_extractor.extract_proposal_features(\n        preprocessed_inputs, scope=\'TestScope\')\n    features_shape = tf.shape(rpn_feature_map)\n\n    init_op = tf.global_variables_initializer()\n    with self.test_session() as sess:\n      sess.run(init_op)\n      features_shape_out = sess.run(features_shape)\n      self.assertAllEqual(features_shape_out, [1, 28, 28, 1088])\n\n  def test_extract_proposal_features_half_size_input(self):\n    feature_extractor = self._build_feature_extractor(\n        first_stage_features_stride=16)\n    preprocessed_inputs = tf.random_uniform(\n        [1, 112, 112, 3], maxval=255, dtype=tf.float32)\n    rpn_feature_map, _ = feature_extractor.extract_proposal_features(\n        preprocessed_inputs, scope=\'TestScope\')\n    features_shape = tf.shape(rpn_feature_map)\n\n    init_op = tf.global_variables_initializer()\n    with self.test_session() as sess:\n      sess.run(init_op)\n      features_shape_out = sess.run(features_shape)\n      self.assertAllEqual(features_shape_out, [1, 7, 7, 1088])\n\n  def test_extract_proposal_features_dies_on_invalid_stride(self):\n    with self.assertRaises(ValueError):\n      self._build_feature_extractor(first_stage_features_stride=99)\n\n  def test_extract_proposal_features_dies_with_incorrect_rank_inputs(self):\n    feature_extractor = self._build_feature_extractor(\n        first_stage_features_stride=16)\n    preprocessed_inputs = tf.random_uniform(\n        [224, 224, 3], maxval=255, dtype=tf.float32)\n    with self.assertRaises(ValueError):\n      feature_extractor.extract_proposal_features(\n          preprocessed_inputs, scope=\'TestScope\')\n\n  def test_extract_box_classifier_features_returns_expected_size(self):\n    feature_extractor = self._build_feature_extractor(\n        first_stage_features_stride=16)\n    proposal_feature_maps = tf.random_uniform(\n        [2, 17, 17, 1088], maxval=255, dtype=tf.float32)\n    proposal_classifier_features = (\n        feature_extractor.extract_box_classifier_features(\n            proposal_feature_maps, scope=\'TestScope\'))\n    features_shape = tf.shape(proposal_classifier_features)\n\n    init_op = tf.global_variables_initializer()\n    with self.test_session() as sess:\n      sess.run(init_op)\n      features_shape_out = sess.run(features_shape)\n      self.assertAllEqual(features_shape_out, [2, 8, 8, 1536])\n\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
src/object_detection/models/faster_rcnn_inception_v2_feature_extractor.py,25,"b'# Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n""""""Inception V2 Faster R-CNN implementation.\n\nSee ""Rethinking the Inception Architecture for Computer Vision""\nhttps://arxiv.org/abs/1512.00567\n""""""\nimport tensorflow as tf\n\nfrom object_detection.meta_architectures import faster_rcnn_meta_arch\nfrom nets import inception_v2\n\nslim = tf.contrib.slim\n\n\ndef _batch_norm_arg_scope(list_ops,\n                          use_batch_norm=True,\n                          batch_norm_decay=0.9997,\n                          batch_norm_epsilon=0.001,\n                          batch_norm_scale=False,\n                          train_batch_norm=False):\n  """"""Slim arg scope for InceptionV2 batch norm.""""""\n  if use_batch_norm:\n    batch_norm_params = {\n        \'is_training\': train_batch_norm,\n        \'scale\': batch_norm_scale,\n        \'decay\': batch_norm_decay,\n        \'epsilon\': batch_norm_epsilon\n    }\n    normalizer_fn = slim.batch_norm\n  else:\n    normalizer_fn = None\n    batch_norm_params = None\n\n  return slim.arg_scope(list_ops,\n                        normalizer_fn=normalizer_fn,\n                        normalizer_params=batch_norm_params)\n\n\nclass FasterRCNNInceptionV2FeatureExtractor(\n    faster_rcnn_meta_arch.FasterRCNNFeatureExtractor):\n  """"""Faster R-CNN Inception V2 feature extractor implementation.""""""\n\n  def __init__(self,\n               is_training,\n               first_stage_features_stride,\n               batch_norm_trainable=False,\n               reuse_weights=None,\n               weight_decay=0.0,\n               depth_multiplier=1.0,\n               min_depth=16):\n    """"""Constructor.\n\n    Args:\n      is_training: See base class.\n      first_stage_features_stride: See base class.\n      batch_norm_trainable: See base class.\n      reuse_weights: See base class.\n      weight_decay: See base class.\n      depth_multiplier: float depth multiplier for feature extractor.\n      min_depth: minimum feature extractor depth.\n\n    Raises:\n      ValueError: If `first_stage_features_stride` is not 8 or 16.\n    """"""\n    if first_stage_features_stride != 8 and first_stage_features_stride != 16:\n      raise ValueError(\'`first_stage_features_stride` must be 8 or 16.\')\n    self._depth_multiplier = depth_multiplier\n    self._min_depth = min_depth\n    super(FasterRCNNInceptionV2FeatureExtractor, self).__init__(\n        is_training, first_stage_features_stride, batch_norm_trainable,\n        reuse_weights, weight_decay)\n\n  def preprocess(self, resized_inputs):\n    """"""Faster R-CNN Inception V2 preprocessing.\n\n    Maps pixel values to the range [-1, 1].\n\n    Args:\n      resized_inputs: a [batch, height, width, channels] float tensor\n        representing a batch of images.\n\n    Returns:\n      preprocessed_inputs: a [batch, height, width, channels] float tensor\n        representing a batch of images.\n    """"""\n    return (2.0 / 255.0) * resized_inputs - 1.0\n\n  def _extract_proposal_features(self, preprocessed_inputs, scope):\n    """"""Extracts first stage RPN features.\n\n    Args:\n      preprocessed_inputs: A [batch, height, width, channels] float32 tensor\n        representing a batch of images.\n      scope: A scope name.\n\n    Returns:\n      rpn_feature_map: A tensor with shape [batch, height, width, depth]\n      activations: A dictionary mapping feature extractor tensor names to\n        tensors\n\n    Raises:\n      InvalidArgumentError: If the spatial size of `preprocessed_inputs`\n        (height or width) is less than 33.\n      ValueError: If the created network is missing the required activation.\n    """"""\n\n    preprocessed_inputs.get_shape().assert_has_rank(4)\n    shape_assert = tf.Assert(\n        tf.logical_and(tf.greater_equal(tf.shape(preprocessed_inputs)[1], 33),\n                       tf.greater_equal(tf.shape(preprocessed_inputs)[2], 33)),\n        [\'image size must at least be 33 in both height and width.\'])\n\n    with tf.control_dependencies([shape_assert]):\n      with tf.variable_scope(\'InceptionV2\',\n                             reuse=self._reuse_weights) as scope:\n        with _batch_norm_arg_scope([slim.conv2d, slim.separable_conv2d],\n                                   batch_norm_scale=True,\n                                   train_batch_norm=self._train_batch_norm):\n          _, activations = inception_v2.inception_v2_base(\n              preprocessed_inputs,\n              final_endpoint=\'Mixed_4e\',\n              min_depth=self._min_depth,\n              depth_multiplier=self._depth_multiplier,\n              scope=scope)\n\n    return activations[\'Mixed_4e\'], activations\n\n  def _extract_box_classifier_features(self, proposal_feature_maps, scope):\n    """"""Extracts second stage box classifier features.\n\n    Args:\n      proposal_feature_maps: A 4-D float tensor with shape\n        [batch_size * self.max_num_proposals, crop_height, crop_width, depth]\n        representing the feature map cropped to each proposal.\n      scope: A scope name (unused).\n\n    Returns:\n      proposal_classifier_features: A 4-D float tensor with shape\n        [batch_size * self.max_num_proposals, height, width, depth]\n        representing box classifier features for each proposal.\n    """"""\n    net = proposal_feature_maps\n\n    depth = lambda d: max(int(d * self._depth_multiplier), self._min_depth)\n    trunc_normal = lambda stddev: tf.truncated_normal_initializer(0.0, stddev)\n\n    data_format = \'NHWC\'\n    concat_dim = 3 if data_format == \'NHWC\' else 1\n\n    with tf.variable_scope(\'InceptionV2\', reuse=self._reuse_weights):\n      with slim.arg_scope(\n          [slim.conv2d, slim.max_pool2d, slim.avg_pool2d],\n          stride=1,\n          padding=\'SAME\',\n          data_format=data_format):\n        with _batch_norm_arg_scope([slim.conv2d, slim.separable_conv2d],\n                                   batch_norm_scale=True,\n                                   train_batch_norm=self._train_batch_norm):\n\n          with tf.variable_scope(\'Mixed_5a\'):\n            with tf.variable_scope(\'Branch_0\'):\n              branch_0 = slim.conv2d(\n                  net, depth(128), [1, 1],\n                  weights_initializer=trunc_normal(0.09),\n                  scope=\'Conv2d_0a_1x1\')\n              branch_0 = slim.conv2d(branch_0, depth(192), [3, 3], stride=2,\n                                     scope=\'Conv2d_1a_3x3\')\n            with tf.variable_scope(\'Branch_1\'):\n              branch_1 = slim.conv2d(\n                  net, depth(192), [1, 1],\n                  weights_initializer=trunc_normal(0.09),\n                  scope=\'Conv2d_0a_1x1\')\n              branch_1 = slim.conv2d(branch_1, depth(256), [3, 3],\n                                     scope=\'Conv2d_0b_3x3\')\n              branch_1 = slim.conv2d(branch_1, depth(256), [3, 3], stride=2,\n                                     scope=\'Conv2d_1a_3x3\')\n            with tf.variable_scope(\'Branch_2\'):\n              branch_2 = slim.max_pool2d(net, [3, 3], stride=2,\n                                         scope=\'MaxPool_1a_3x3\')\n            net = tf.concat([branch_0, branch_1, branch_2], concat_dim)\n\n          with tf.variable_scope(\'Mixed_5b\'):\n            with tf.variable_scope(\'Branch_0\'):\n              branch_0 = slim.conv2d(net, depth(352), [1, 1],\n                                     scope=\'Conv2d_0a_1x1\')\n            with tf.variable_scope(\'Branch_1\'):\n              branch_1 = slim.conv2d(\n                  net, depth(192), [1, 1],\n                  weights_initializer=trunc_normal(0.09),\n                  scope=\'Conv2d_0a_1x1\')\n              branch_1 = slim.conv2d(branch_1, depth(320), [3, 3],\n                                     scope=\'Conv2d_0b_3x3\')\n            with tf.variable_scope(\'Branch_2\'):\n              branch_2 = slim.conv2d(\n                  net, depth(160), [1, 1],\n                  weights_initializer=trunc_normal(0.09),\n                  scope=\'Conv2d_0a_1x1\')\n              branch_2 = slim.conv2d(branch_2, depth(224), [3, 3],\n                                     scope=\'Conv2d_0b_3x3\')\n              branch_2 = slim.conv2d(branch_2, depth(224), [3, 3],\n                                     scope=\'Conv2d_0c_3x3\')\n            with tf.variable_scope(\'Branch_3\'):\n              branch_3 = slim.avg_pool2d(net, [3, 3], scope=\'AvgPool_0a_3x3\')\n              branch_3 = slim.conv2d(\n                  branch_3, depth(128), [1, 1],\n                  weights_initializer=trunc_normal(0.1),\n                  scope=\'Conv2d_0b_1x1\')\n            net = tf.concat([branch_0, branch_1, branch_2, branch_3],\n                            concat_dim)\n\n          with tf.variable_scope(\'Mixed_5c\'):\n            with tf.variable_scope(\'Branch_0\'):\n              branch_0 = slim.conv2d(net, depth(352), [1, 1],\n                                     scope=\'Conv2d_0a_1x1\')\n            with tf.variable_scope(\'Branch_1\'):\n              branch_1 = slim.conv2d(\n                  net, depth(192), [1, 1],\n                  weights_initializer=trunc_normal(0.09),\n                  scope=\'Conv2d_0a_1x1\')\n              branch_1 = slim.conv2d(branch_1, depth(320), [3, 3],\n                                     scope=\'Conv2d_0b_3x3\')\n            with tf.variable_scope(\'Branch_2\'):\n              branch_2 = slim.conv2d(\n                  net, depth(192), [1, 1],\n                  weights_initializer=trunc_normal(0.09),\n                  scope=\'Conv2d_0a_1x1\')\n              branch_2 = slim.conv2d(branch_2, depth(224), [3, 3],\n                                     scope=\'Conv2d_0b_3x3\')\n              branch_2 = slim.conv2d(branch_2, depth(224), [3, 3],\n                                     scope=\'Conv2d_0c_3x3\')\n            with tf.variable_scope(\'Branch_3\'):\n              branch_3 = slim.max_pool2d(net, [3, 3], scope=\'MaxPool_0a_3x3\')\n              branch_3 = slim.conv2d(\n                  branch_3, depth(128), [1, 1],\n                  weights_initializer=trunc_normal(0.1),\n                  scope=\'Conv2d_0b_1x1\')\n            proposal_classifier_features = tf.concat(\n                [branch_0, branch_1, branch_2, branch_3], concat_dim)\n\n    return proposal_classifier_features\n'"
src/object_detection/models/faster_rcnn_inception_v2_feature_extractor_test.py,24,"b'# Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n""""""Tests for faster_rcnn_inception_v2_feature_extractor.""""""\n\nimport numpy as np\nimport tensorflow as tf\n\nfrom object_detection.models import faster_rcnn_inception_v2_feature_extractor as faster_rcnn_inception_v2\n\n\nclass FasterRcnnInceptionV2FeatureExtractorTest(tf.test.TestCase):\n\n  def _build_feature_extractor(self, first_stage_features_stride):\n    return faster_rcnn_inception_v2.FasterRCNNInceptionV2FeatureExtractor(\n        is_training=False,\n        first_stage_features_stride=first_stage_features_stride,\n        batch_norm_trainable=False,\n        reuse_weights=None,\n        weight_decay=0.0)\n\n  def test_extract_proposal_features_returns_expected_size(self):\n    feature_extractor = self._build_feature_extractor(\n        first_stage_features_stride=16)\n    preprocessed_inputs = tf.random_uniform(\n        [4, 224, 224, 3], maxval=255, dtype=tf.float32)\n    rpn_feature_map, _ = feature_extractor.extract_proposal_features(\n        preprocessed_inputs, scope=\'TestScope\')\n    features_shape = tf.shape(rpn_feature_map)\n\n    init_op = tf.global_variables_initializer()\n    with self.test_session() as sess:\n      sess.run(init_op)\n      features_shape_out = sess.run(features_shape)\n      self.assertAllEqual(features_shape_out, [4, 14, 14, 576])\n\n  def test_extract_proposal_features_stride_eight(self):\n    feature_extractor = self._build_feature_extractor(\n        first_stage_features_stride=8)\n    preprocessed_inputs = tf.random_uniform(\n        [4, 224, 224, 3], maxval=255, dtype=tf.float32)\n    rpn_feature_map, _ = feature_extractor.extract_proposal_features(\n        preprocessed_inputs, scope=\'TestScope\')\n    features_shape = tf.shape(rpn_feature_map)\n\n    init_op = tf.global_variables_initializer()\n    with self.test_session() as sess:\n      sess.run(init_op)\n      features_shape_out = sess.run(features_shape)\n      self.assertAllEqual(features_shape_out, [4, 14, 14, 576])\n\n  def test_extract_proposal_features_half_size_input(self):\n    feature_extractor = self._build_feature_extractor(\n        first_stage_features_stride=16)\n    preprocessed_inputs = tf.random_uniform(\n        [1, 112, 112, 3], maxval=255, dtype=tf.float32)\n    rpn_feature_map, _ = feature_extractor.extract_proposal_features(\n        preprocessed_inputs, scope=\'TestScope\')\n    features_shape = tf.shape(rpn_feature_map)\n\n    init_op = tf.global_variables_initializer()\n    with self.test_session() as sess:\n      sess.run(init_op)\n      features_shape_out = sess.run(features_shape)\n      self.assertAllEqual(features_shape_out, [1, 7, 7, 576])\n\n  def test_extract_proposal_features_dies_on_invalid_stride(self):\n    with self.assertRaises(ValueError):\n      self._build_feature_extractor(first_stage_features_stride=99)\n\n  def test_extract_proposal_features_dies_on_very_small_images(self):\n    feature_extractor = self._build_feature_extractor(\n        first_stage_features_stride=16)\n    preprocessed_inputs = tf.placeholder(tf.float32, (4, None, None, 3))\n    rpn_feature_map, _ = feature_extractor.extract_proposal_features(\n        preprocessed_inputs, scope=\'TestScope\')\n    features_shape = tf.shape(rpn_feature_map)\n\n    init_op = tf.global_variables_initializer()\n    with self.test_session() as sess:\n      sess.run(init_op)\n      with self.assertRaises(tf.errors.InvalidArgumentError):\n        sess.run(\n            features_shape,\n            feed_dict={preprocessed_inputs: np.random.rand(4, 32, 32, 3)})\n\n  def test_extract_proposal_features_dies_with_incorrect_rank_inputs(self):\n    feature_extractor = self._build_feature_extractor(\n        first_stage_features_stride=16)\n    preprocessed_inputs = tf.random_uniform(\n        [224, 224, 3], maxval=255, dtype=tf.float32)\n    with self.assertRaises(ValueError):\n      feature_extractor.extract_proposal_features(\n          preprocessed_inputs, scope=\'TestScope\')\n\n  def test_extract_box_classifier_features_returns_expected_size(self):\n    feature_extractor = self._build_feature_extractor(\n        first_stage_features_stride=16)\n    proposal_feature_maps = tf.random_uniform(\n        [3, 14, 14, 576], maxval=255, dtype=tf.float32)\n    proposal_classifier_features = (\n        feature_extractor.extract_box_classifier_features(\n            proposal_feature_maps, scope=\'TestScope\'))\n    features_shape = tf.shape(proposal_classifier_features)\n\n    init_op = tf.global_variables_initializer()\n    with self.test_session() as sess:\n      sess.run(init_op)\n      features_shape_out = sess.run(features_shape)\n      self.assertAllEqual(features_shape_out, [3, 7, 7, 1024])\n\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
src/object_detection/models/faster_rcnn_mobilenet_v1_feature_extractor.py,7,"b'# Copyright 2018 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n""""""Mobilenet v1 Faster R-CNN implementation.""""""\nimport tensorflow as tf\n\nfrom object_detection.meta_architectures import faster_rcnn_meta_arch\nfrom nets import mobilenet_v1\n\nslim = tf.contrib.slim\n\n\ndef _batch_norm_arg_scope(list_ops,\n                          use_batch_norm=True,\n                          batch_norm_decay=0.9997,\n                          batch_norm_epsilon=0.001,\n                          batch_norm_scale=False,\n                          train_batch_norm=False):\n  """"""Slim arg scope for Mobilenet V1 batch norm.""""""\n  if use_batch_norm:\n    batch_norm_params = {\n        \'is_training\': train_batch_norm,\n        \'scale\': batch_norm_scale,\n        \'decay\': batch_norm_decay,\n        \'epsilon\': batch_norm_epsilon\n    }\n    normalizer_fn = slim.batch_norm\n  else:\n    normalizer_fn = None\n    batch_norm_params = None\n\n  return slim.arg_scope(list_ops,\n                        normalizer_fn=normalizer_fn,\n                        normalizer_params=batch_norm_params)\n\n\nclass FasterRCNNMobilenetV1FeatureExtractor(\n    faster_rcnn_meta_arch.FasterRCNNFeatureExtractor):\n  """"""Faster R-CNN Mobilenet V1 feature extractor implementation.""""""\n\n  def __init__(self,\n               is_training,\n               first_stage_features_stride,\n               batch_norm_trainable=False,\n               reuse_weights=None,\n               weight_decay=0.0,\n               depth_multiplier=1.0,\n               min_depth=16):\n    """"""Constructor.\n\n    Args:\n      is_training: See base class.\n      first_stage_features_stride: See base class.\n      batch_norm_trainable: See base class.\n      reuse_weights: See base class.\n      weight_decay: See base class.\n      depth_multiplier: float depth multiplier for feature extractor.\n      min_depth: minimum feature extractor depth.\n\n    Raises:\n      ValueError: If `first_stage_features_stride` is not 8 or 16.\n    """"""\n    if first_stage_features_stride != 8 and first_stage_features_stride != 16:\n      raise ValueError(\'`first_stage_features_stride` must be 8 or 16.\')\n    self._depth_multiplier = depth_multiplier\n    self._min_depth = min_depth\n    super(FasterRCNNMobilenetV1FeatureExtractor, self).__init__(\n        is_training, first_stage_features_stride, batch_norm_trainable,\n        reuse_weights, weight_decay)\n\n  def preprocess(self, resized_inputs):\n    """"""Faster R-CNN Mobilenet V1 preprocessing.\n\n    Maps pixel values to the range [-1, 1].\n\n    Args:\n      resized_inputs: a [batch, height, width, channels] float tensor\n        representing a batch of images.\n\n    Returns:\n      preprocessed_inputs: a [batch, height, width, channels] float tensor\n        representing a batch of images.\n    """"""\n    return (2.0 / 255.0) * resized_inputs - 1.0\n\n  def _extract_proposal_features(self, preprocessed_inputs, scope):\n    """"""Extracts first stage RPN features.\n\n    Args:\n      preprocessed_inputs: A [batch, height, width, channels] float32 tensor\n        representing a batch of images.\n      scope: A scope name.\n\n    Returns:\n      rpn_feature_map: A tensor with shape [batch, height, width, depth]\n      activations: A dictionary mapping feature extractor tensor names to\n        tensors\n\n    Raises:\n      InvalidArgumentError: If the spatial size of `preprocessed_inputs`\n        (height or width) is less than 33.\n      ValueError: If the created network is missing the required activation.\n    """"""\n\n    preprocessed_inputs.get_shape().assert_has_rank(4)\n    shape_assert = tf.Assert(\n        tf.logical_and(tf.greater_equal(tf.shape(preprocessed_inputs)[1], 33),\n                       tf.greater_equal(tf.shape(preprocessed_inputs)[2], 33)),\n        [\'image size must at least be 33 in both height and width.\'])\n\n    with tf.control_dependencies([shape_assert]):\n      with tf.variable_scope(\'MobilenetV1\',\n                             reuse=self._reuse_weights) as scope:\n        with _batch_norm_arg_scope([slim.conv2d, slim.separable_conv2d],\n                                   batch_norm_scale=True,\n                                   train_batch_norm=self._train_batch_norm):\n          _, activations = mobilenet_v1.mobilenet_v1_base(\n              preprocessed_inputs,\n              final_endpoint=\'Conv2d_13_pointwise\',\n              min_depth=self._min_depth,\n              depth_multiplier=self._depth_multiplier,\n              scope=scope)\n    return activations[\'Conv2d_13_pointwise\'], activations\n\n  def _extract_box_classifier_features(self, proposal_feature_maps, scope):\n    """"""Extracts second stage box classifier features.\n\n    Args:\n      proposal_feature_maps: A 4-D float tensor with shape\n        [batch_size * self.max_num_proposals, crop_height, crop_width, depth]\n        representing the feature map cropped to each proposal.\n      scope: A scope name (unused).\n\n    Returns:\n      proposal_classifier_features: A 4-D float tensor with shape\n        [batch_size * self.max_num_proposals, height, width, depth]\n        representing box classifier features for each proposal.\n    """"""\n    net = proposal_feature_maps\n\n    depth = lambda d: max(int(d * 1.0), 16)\n    with tf.variable_scope(\'MobilenetV1\', reuse=self._reuse_weights):\n      with _batch_norm_arg_scope([slim.conv2d, slim.separable_conv2d],\n                                 batch_norm_scale=True,\n                                 train_batch_norm=self._train_batch_norm):\n        with slim.arg_scope(\n            [slim.conv2d, slim.separable_conv2d], padding=\'SAME\'):\n          net = slim.separable_conv2d(\n              net,\n              depth(1024), [3, 3],\n              depth_multiplier=1,\n              stride=2,\n              scope=\'Conv2d_12_pointwise\')\n          return slim.separable_conv2d(\n              net,\n              depth(1024), [3, 3],\n              depth_multiplier=1,\n              stride=1,\n              scope=\'Conv2d_13_pointwise\')\n'"
src/object_detection/models/faster_rcnn_mobilenet_v1_feature_extractor_test.py,24,"b'# Copyright 2018 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n""""""Tests for faster_rcnn_mobilenet_v1_feature_extractor.""""""\n\nimport numpy as np\nimport tensorflow as tf\n\nfrom object_detection.models import faster_rcnn_mobilenet_v1_feature_extractor as faster_rcnn_mobilenet_v1\n\n\nclass FasterRcnnMobilenetV1FeatureExtractorTest(tf.test.TestCase):\n\n  def _build_feature_extractor(self, first_stage_features_stride):\n    return faster_rcnn_mobilenet_v1.FasterRCNNMobilenetV1FeatureExtractor(\n        is_training=False,\n        first_stage_features_stride=first_stage_features_stride,\n        batch_norm_trainable=False,\n        reuse_weights=None,\n        weight_decay=0.0)\n\n  def test_extract_proposal_features_returns_expected_size(self):\n    feature_extractor = self._build_feature_extractor(\n        first_stage_features_stride=16)\n    preprocessed_inputs = tf.random_uniform(\n        [4, 224, 224, 3], maxval=255, dtype=tf.float32)\n    rpn_feature_map, _ = feature_extractor.extract_proposal_features(\n        preprocessed_inputs, scope=\'TestScope\')\n    features_shape = tf.shape(rpn_feature_map)\n\n    init_op = tf.global_variables_initializer()\n    with self.test_session() as sess:\n      sess.run(init_op)\n      features_shape_out = sess.run(features_shape)\n      self.assertAllEqual(features_shape_out, [4, 7, 7, 1024])\n\n  def test_extract_proposal_features_stride_eight(self):\n    feature_extractor = self._build_feature_extractor(\n        first_stage_features_stride=8)\n    preprocessed_inputs = tf.random_uniform(\n        [4, 224, 224, 3], maxval=255, dtype=tf.float32)\n    rpn_feature_map, _ = feature_extractor.extract_proposal_features(\n        preprocessed_inputs, scope=\'TestScope\')\n    features_shape = tf.shape(rpn_feature_map)\n\n    init_op = tf.global_variables_initializer()\n    with self.test_session() as sess:\n      sess.run(init_op)\n      features_shape_out = sess.run(features_shape)\n      self.assertAllEqual(features_shape_out, [4, 7, 7, 1024])\n\n  def test_extract_proposal_features_half_size_input(self):\n    feature_extractor = self._build_feature_extractor(\n        first_stage_features_stride=16)\n    preprocessed_inputs = tf.random_uniform(\n        [1, 112, 112, 3], maxval=255, dtype=tf.float32)\n    rpn_feature_map, _ = feature_extractor.extract_proposal_features(\n        preprocessed_inputs, scope=\'TestScope\')\n    features_shape = tf.shape(rpn_feature_map)\n\n    init_op = tf.global_variables_initializer()\n    with self.test_session() as sess:\n      sess.run(init_op)\n      features_shape_out = sess.run(features_shape)\n      self.assertAllEqual(features_shape_out, [1, 4, 4, 1024])\n\n  def test_extract_proposal_features_dies_on_invalid_stride(self):\n    with self.assertRaises(ValueError):\n      self._build_feature_extractor(first_stage_features_stride=99)\n\n  def test_extract_proposal_features_dies_on_very_small_images(self):\n    feature_extractor = self._build_feature_extractor(\n        first_stage_features_stride=16)\n    preprocessed_inputs = tf.placeholder(tf.float32, (4, None, None, 3))\n    rpn_feature_map, _ = feature_extractor.extract_proposal_features(\n        preprocessed_inputs, scope=\'TestScope\')\n    features_shape = tf.shape(rpn_feature_map)\n\n    init_op = tf.global_variables_initializer()\n    with self.test_session() as sess:\n      sess.run(init_op)\n      with self.assertRaises(tf.errors.InvalidArgumentError):\n        sess.run(\n            features_shape,\n            feed_dict={preprocessed_inputs: np.random.rand(4, 32, 32, 3)})\n\n  def test_extract_proposal_features_dies_with_incorrect_rank_inputs(self):\n    feature_extractor = self._build_feature_extractor(\n        first_stage_features_stride=16)\n    preprocessed_inputs = tf.random_uniform(\n        [224, 224, 3], maxval=255, dtype=tf.float32)\n    with self.assertRaises(ValueError):\n      feature_extractor.extract_proposal_features(\n          preprocessed_inputs, scope=\'TestScope\')\n\n  def test_extract_box_classifier_features_returns_expected_size(self):\n    feature_extractor = self._build_feature_extractor(\n        first_stage_features_stride=16)\n    proposal_feature_maps = tf.random_uniform(\n        [3, 14, 14, 576], maxval=255, dtype=tf.float32)\n    proposal_classifier_features = (\n        feature_extractor.extract_box_classifier_features(\n            proposal_feature_maps, scope=\'TestScope\'))\n    features_shape = tf.shape(proposal_classifier_features)\n\n    init_op = tf.global_variables_initializer()\n    with self.test_session() as sess:\n      sess.run(init_op)\n      features_shape_out = sess.run(features_shape)\n      self.assertAllEqual(features_shape_out, [3, 7, 7, 1024])\n\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
src/object_detection/models/faster_rcnn_nas_feature_extractor.py,7,"b'# Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n""""""NASNet Faster R-CNN implementation.\n\nLearning Transferable Architectures for Scalable Image Recognition\nBarret Zoph, Vijay Vasudevan, Jonathon Shlens, Quoc V. Le\nhttps://arxiv.org/abs/1707.07012\n""""""\n\nimport tensorflow as tf\n\nfrom object_detection.meta_architectures import faster_rcnn_meta_arch\nfrom nets.nasnet import nasnet\nfrom nets.nasnet import nasnet_utils\n\narg_scope = tf.contrib.framework.arg_scope\nslim = tf.contrib.slim\n\n\ndef nasnet_large_arg_scope_for_detection(is_batch_norm_training=False):\n  """"""Defines the default arg scope for the NASNet-A Large for object detection.\n\n  This provides a small edit to switch batch norm training on and off.\n\n  Args:\n    is_batch_norm_training: Boolean indicating whether to train with batch norm.\n\n  Returns:\n    An `arg_scope` to use for the NASNet Large Model.\n  """"""\n  imagenet_scope = nasnet.nasnet_large_arg_scope()\n  with arg_scope(imagenet_scope):\n    with arg_scope([slim.batch_norm], is_training=is_batch_norm_training) as sc:\n      return sc\n\n\n# Note: This is largely a copy of _build_nasnet_base inside nasnet.py but\n# with special edits to remove instantiation of the stem and the special\n# ability to receive as input a pair of hidden states.\ndef _build_nasnet_base(hidden_previous,\n                       hidden,\n                       normal_cell,\n                       reduction_cell,\n                       hparams,\n                       true_cell_num,\n                       start_cell_num):\n  """"""Constructs a NASNet image model.""""""\n\n  # Find where to place the reduction cells or stride normal cells\n  reduction_indices = nasnet_utils.calc_reduction_layers(\n      hparams.num_cells, hparams.num_reduction_layers)\n\n  # Note: The None is prepended to match the behavior of _imagenet_stem()\n  cell_outputs = [None, hidden_previous, hidden]\n  net = hidden\n\n  # NOTE: In the nasnet.py code, filter_scaling starts at 1.0. We instead\n  # start at 2.0 because 1 reduction cell has been created which would\n  # update the filter_scaling to 2.0.\n  filter_scaling = 2.0\n\n  # Run the cells\n  for cell_num in range(start_cell_num, hparams.num_cells):\n    stride = 1\n    if hparams.skip_reduction_layer_input:\n      prev_layer = cell_outputs[-2]\n    if cell_num in reduction_indices:\n      filter_scaling *= hparams.filter_scaling_rate\n      net = reduction_cell(\n          net,\n          scope=\'reduction_cell_{}\'.format(reduction_indices.index(cell_num)),\n          filter_scaling=filter_scaling,\n          stride=2,\n          prev_layer=cell_outputs[-2],\n          cell_num=true_cell_num)\n      true_cell_num += 1\n      cell_outputs.append(net)\n    if not hparams.skip_reduction_layer_input:\n      prev_layer = cell_outputs[-2]\n    net = normal_cell(\n        net,\n        scope=\'cell_{}\'.format(cell_num),\n        filter_scaling=filter_scaling,\n        stride=stride,\n        prev_layer=prev_layer,\n        cell_num=true_cell_num)\n    true_cell_num += 1\n    cell_outputs.append(net)\n\n  # Final nonlinearity.\n  # Note that we have dropped the final pooling, dropout and softmax layers\n  # from the default nasnet version.\n  with tf.variable_scope(\'final_layer\'):\n    net = tf.nn.relu(net)\n  return net\n\n\n# TODO(shlens): Only fixed_shape_resizer is currently supported for NASNet\n# featurization. The reason for this is that nasnet.py only supports\n# inputs with fully known shapes. We need to update nasnet.py to handle\n# shapes not known at compile time.\nclass FasterRCNNNASFeatureExtractor(\n    faster_rcnn_meta_arch.FasterRCNNFeatureExtractor):\n  """"""Faster R-CNN with NASNet-A feature extractor implementation.""""""\n\n  def __init__(self,\n               is_training,\n               first_stage_features_stride,\n               batch_norm_trainable=False,\n               reuse_weights=None,\n               weight_decay=0.0):\n    """"""Constructor.\n\n    Args:\n      is_training: See base class.\n      first_stage_features_stride: See base class.\n      batch_norm_trainable: See base class.\n      reuse_weights: See base class.\n      weight_decay: See base class.\n\n    Raises:\n      ValueError: If `first_stage_features_stride` is not 16.\n    """"""\n    if first_stage_features_stride != 16:\n      raise ValueError(\'`first_stage_features_stride` must be 16.\')\n    super(FasterRCNNNASFeatureExtractor, self).__init__(\n        is_training, first_stage_features_stride, batch_norm_trainable,\n        reuse_weights, weight_decay)\n\n  def preprocess(self, resized_inputs):\n    """"""Faster R-CNN with NAS preprocessing.\n\n    Maps pixel values to the range [-1, 1].\n\n    Args:\n      resized_inputs: A [batch, height_in, width_in, channels] float32 tensor\n        representing a batch of images with values between 0 and 255.0.\n\n    Returns:\n      preprocessed_inputs: A [batch, height_out, width_out, channels] float32\n        tensor representing a batch of images.\n\n    """"""\n    return (2.0 / 255.0) * resized_inputs - 1.0\n\n  def _extract_proposal_features(self, preprocessed_inputs, scope):\n    """"""Extracts first stage RPN features.\n\n    Extracts features using the first half of the NASNet network.\n    We construct the network in `align_feature_maps=True` mode, which means\n    that all VALID paddings in the network are changed to SAME padding so that\n    the feature maps are aligned.\n\n    Args:\n      preprocessed_inputs: A [batch, height, width, channels] float32 tensor\n        representing a batch of images.\n      scope: A scope name.\n\n    Returns:\n      rpn_feature_map: A tensor with shape [batch, height, width, depth]\n      end_points: A dictionary mapping feature extractor tensor names to tensors\n\n    Raises:\n      ValueError: If the created network is missing the required activation.\n    """"""\n    del scope\n\n    if len(preprocessed_inputs.get_shape().as_list()) != 4:\n      raise ValueError(\'`preprocessed_inputs` must be 4 dimensional, got a \'\n                       \'tensor of shape %s\' % preprocessed_inputs.get_shape())\n\n    with slim.arg_scope(nasnet_large_arg_scope_for_detection(\n        is_batch_norm_training=self._train_batch_norm)):\n      with arg_scope([slim.conv2d,\n                      slim.batch_norm,\n                      slim.separable_conv2d],\n                     reuse=self._reuse_weights):\n        _, end_points = nasnet.build_nasnet_large(\n            preprocessed_inputs, num_classes=None,\n            is_training=self._is_training,\n            final_endpoint=\'Cell_11\')\n\n    # Note that both \'Cell_10\' and \'Cell_11\' have equal depth = 2016.\n    rpn_feature_map = tf.concat([end_points[\'Cell_10\'],\n                                 end_points[\'Cell_11\']], 3)\n\n    # nasnet.py does not maintain the batch size in the first dimension.\n    # This work around permits us retaining the batch for below.\n    batch = preprocessed_inputs.get_shape().as_list()[0]\n    shape_without_batch = rpn_feature_map.get_shape().as_list()[1:]\n    rpn_feature_map_shape = [batch] + shape_without_batch\n    rpn_feature_map.set_shape(rpn_feature_map_shape)\n\n    return rpn_feature_map, end_points\n\n  def _extract_box_classifier_features(self, proposal_feature_maps, scope):\n    """"""Extracts second stage box classifier features.\n\n    This function reconstructs the ""second half"" of the NASNet-A\n    network after the part defined in `_extract_proposal_features`.\n\n    Args:\n      proposal_feature_maps: A 4-D float tensor with shape\n        [batch_size * self.max_num_proposals, crop_height, crop_width, depth]\n        representing the feature map cropped to each proposal.\n      scope: A scope name.\n\n    Returns:\n      proposal_classifier_features: A 4-D float tensor with shape\n        [batch_size * self.max_num_proposals, height, width, depth]\n        representing box classifier features for each proposal.\n    """"""\n    del scope\n\n    # Note that we always feed into 2 layers of equal depth\n    # where the first N channels corresponds to previous hidden layer\n    # and the second N channels correspond to the final hidden layer.\n    hidden_previous, hidden = tf.split(proposal_feature_maps, 2, axis=3)\n\n    # Note that what follows is largely a copy of build_nasnet_large() within\n    # nasnet.py. We are copying to minimize code pollution in slim.\n\n    # TODO(shlens,skornblith): Determine the appropriate drop path schedule.\n    # For now the schedule is the default (1.0->0.7 over 250,000 train steps).\n    hparams = nasnet.large_imagenet_config()\n    if not self._is_training:\n      hparams.set_hparam(\'drop_path_keep_prob\', 1.0)\n\n    # Calculate the total number of cells in the network\n    # -- Add 2 for the reduction cells.\n    total_num_cells = hparams.num_cells + 2\n    # -- And add 2 for the stem cells for ImageNet training.\n    total_num_cells += 2\n\n    normal_cell = nasnet_utils.NasNetANormalCell(\n        hparams.num_conv_filters, hparams.drop_path_keep_prob,\n        total_num_cells, hparams.total_training_steps)\n    reduction_cell = nasnet_utils.NasNetAReductionCell(\n        hparams.num_conv_filters, hparams.drop_path_keep_prob,\n        total_num_cells, hparams.total_training_steps)\n    with arg_scope([slim.dropout, nasnet_utils.drop_path],\n                   is_training=self._is_training):\n      with arg_scope([slim.batch_norm], is_training=self._train_batch_norm):\n        with arg_scope([slim.avg_pool2d,\n                        slim.max_pool2d,\n                        slim.conv2d,\n                        slim.batch_norm,\n                        slim.separable_conv2d,\n                        nasnet_utils.factorized_reduction,\n                        nasnet_utils.global_avg_pool,\n                        nasnet_utils.get_channel_index,\n                        nasnet_utils.get_channel_dim],\n                       data_format=hparams.data_format):\n\n          # This corresponds to the cell number just past \'Cell_11\' used by\n          # by _extract_proposal_features().\n          start_cell_num = 12\n          # Note that this number equals:\n          #  start_cell_num + 2 stem cells + 1 reduction cell\n          true_cell_num = 15\n\n          with slim.arg_scope(nasnet.nasnet_large_arg_scope()):\n            net = _build_nasnet_base(hidden_previous,\n                                     hidden,\n                                     normal_cell=normal_cell,\n                                     reduction_cell=reduction_cell,\n                                     hparams=hparams,\n                                     true_cell_num=true_cell_num,\n                                     start_cell_num=start_cell_num)\n\n    proposal_classifier_features = net\n    return proposal_classifier_features\n\n  def restore_from_classification_checkpoint_fn(\n      self,\n      first_stage_feature_extractor_scope,\n      second_stage_feature_extractor_scope):\n    """"""Returns a map of variables to load from a foreign checkpoint.\n\n    Note that this overrides the default implementation in\n    faster_rcnn_meta_arch.FasterRCNNFeatureExtractor which does not work for\n    NASNet-A checkpoints.\n\n    Args:\n      first_stage_feature_extractor_scope: A scope name for the first stage\n        feature extractor.\n      second_stage_feature_extractor_scope: A scope name for the second stage\n        feature extractor.\n\n    Returns:\n      A dict mapping variable names (to load from a checkpoint) to variables in\n      the model graph.\n    """"""\n    # Note that the NAS checkpoint only contains the moving average version of\n    # the Variables so we need to generate an appropriate dictionary mapping.\n    variables_to_restore = {}\n    for variable in tf.global_variables():\n      if variable.op.name.startswith(\n          first_stage_feature_extractor_scope):\n        var_name = variable.op.name.replace(\n            first_stage_feature_extractor_scope + \'/\', \'\')\n        var_name += \'/ExponentialMovingAverage\'\n        variables_to_restore[var_name] = variable\n      if variable.op.name.startswith(\n          second_stage_feature_extractor_scope):\n        var_name = variable.op.name.replace(\n            second_stage_feature_extractor_scope + \'/\', \'\')\n        var_name += \'/ExponentialMovingAverage\'\n        variables_to_restore[var_name] = variable\n    return variables_to_restore\n\n'"
src/object_detection/models/faster_rcnn_nas_feature_extractor_test.py,20,"b'# Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n""""""Tests for models.faster_rcnn_nas_feature_extractor.""""""\n\nimport tensorflow as tf\n\nfrom object_detection.models import faster_rcnn_nas_feature_extractor as frcnn_nas\n\n\nclass FasterRcnnNASFeatureExtractorTest(tf.test.TestCase):\n\n  def _build_feature_extractor(self, first_stage_features_stride):\n    return frcnn_nas.FasterRCNNNASFeatureExtractor(\n        is_training=False,\n        first_stage_features_stride=first_stage_features_stride,\n        batch_norm_trainable=False,\n        reuse_weights=None,\n        weight_decay=0.0)\n\n  def test_extract_proposal_features_returns_expected_size(self):\n    feature_extractor = self._build_feature_extractor(\n        first_stage_features_stride=16)\n    preprocessed_inputs = tf.random_uniform(\n        [1, 299, 299, 3], maxval=255, dtype=tf.float32)\n    rpn_feature_map, _ = feature_extractor.extract_proposal_features(\n        preprocessed_inputs, scope=\'TestScope\')\n    features_shape = tf.shape(rpn_feature_map)\n\n    init_op = tf.global_variables_initializer()\n    with self.test_session() as sess:\n      sess.run(init_op)\n      features_shape_out = sess.run(features_shape)\n      self.assertAllEqual(features_shape_out, [1, 19, 19, 4032])\n\n  def test_extract_proposal_features_input_size_224(self):\n    feature_extractor = self._build_feature_extractor(\n        first_stage_features_stride=16)\n    preprocessed_inputs = tf.random_uniform(\n        [1, 224, 224, 3], maxval=255, dtype=tf.float32)\n    rpn_feature_map, _ = feature_extractor.extract_proposal_features(\n        preprocessed_inputs, scope=\'TestScope\')\n    features_shape = tf.shape(rpn_feature_map)\n\n    init_op = tf.global_variables_initializer()\n    with self.test_session() as sess:\n      sess.run(init_op)\n      features_shape_out = sess.run(features_shape)\n      self.assertAllEqual(features_shape_out, [1, 14, 14, 4032])\n\n  def test_extract_proposal_features_input_size_112(self):\n    feature_extractor = self._build_feature_extractor(\n        first_stage_features_stride=16)\n    preprocessed_inputs = tf.random_uniform(\n        [1, 112, 112, 3], maxval=255, dtype=tf.float32)\n    rpn_feature_map, _ = feature_extractor.extract_proposal_features(\n        preprocessed_inputs, scope=\'TestScope\')\n    features_shape = tf.shape(rpn_feature_map)\n\n    init_op = tf.global_variables_initializer()\n    with self.test_session() as sess:\n      sess.run(init_op)\n      features_shape_out = sess.run(features_shape)\n      self.assertAllEqual(features_shape_out, [1, 7, 7, 4032])\n\n  def test_extract_proposal_features_dies_on_invalid_stride(self):\n    with self.assertRaises(ValueError):\n      self._build_feature_extractor(first_stage_features_stride=99)\n\n  def test_extract_proposal_features_dies_with_incorrect_rank_inputs(self):\n    feature_extractor = self._build_feature_extractor(\n        first_stage_features_stride=16)\n    preprocessed_inputs = tf.random_uniform(\n        [224, 224, 3], maxval=255, dtype=tf.float32)\n    with self.assertRaises(ValueError):\n      feature_extractor.extract_proposal_features(\n          preprocessed_inputs, scope=\'TestScope\')\n\n  def test_extract_box_classifier_features_returns_expected_size(self):\n    feature_extractor = self._build_feature_extractor(\n        first_stage_features_stride=16)\n    proposal_feature_maps = tf.random_uniform(\n        [2, 17, 17, 1088], maxval=255, dtype=tf.float32)\n    proposal_classifier_features = (\n        feature_extractor.extract_box_classifier_features(\n            proposal_feature_maps, scope=\'TestScope\'))\n    features_shape = tf.shape(proposal_classifier_features)\n\n    init_op = tf.global_variables_initializer()\n    with self.test_session() as sess:\n      sess.run(init_op)\n      features_shape_out = sess.run(features_shape)\n      self.assertAllEqual(features_shape_out, [2, 9, 9, 4032])\n\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
src/object_detection/models/faster_rcnn_resnet_v1_feature_extractor.py,8,"b'# Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n""""""Resnet V1 Faster R-CNN implementation.\n\nSee ""Deep Residual Learning for Image Recognition"" by He et al., 2015.\nhttps://arxiv.org/abs/1512.03385\n\nNote: this implementation assumes that the classification checkpoint used\nto finetune this model is trained using the same configuration as that of\nthe MSRA provided checkpoints\n(see https://github.com/KaimingHe/deep-residual-networks), e.g., with\nsame preprocessing, batch norm scaling, etc.\n""""""\nimport tensorflow as tf\n\nfrom object_detection.meta_architectures import faster_rcnn_meta_arch\nfrom nets import resnet_utils\nfrom nets import resnet_v1\n\nslim = tf.contrib.slim\n\n\nclass FasterRCNNResnetV1FeatureExtractor(\n    faster_rcnn_meta_arch.FasterRCNNFeatureExtractor):\n  """"""Faster R-CNN Resnet V1 feature extractor implementation.""""""\n\n  def __init__(self,\n               architecture,\n               resnet_model,\n               is_training,\n               first_stage_features_stride,\n               batch_norm_trainable=False,\n               reuse_weights=None,\n               weight_decay=0.0):\n    """"""Constructor.\n\n    Args:\n      architecture: Architecture name of the Resnet V1 model.\n      resnet_model: Definition of the Resnet V1 model.\n      is_training: See base class.\n      first_stage_features_stride: See base class.\n      batch_norm_trainable: See base class.\n      reuse_weights: See base class.\n      weight_decay: See base class.\n\n    Raises:\n      ValueError: If `first_stage_features_stride` is not 8 or 16.\n    """"""\n    if first_stage_features_stride != 8 and first_stage_features_stride != 16:\n      raise ValueError(\'`first_stage_features_stride` must be 8 or 16.\')\n    self._architecture = architecture\n    self._resnet_model = resnet_model\n    super(FasterRCNNResnetV1FeatureExtractor, self).__init__(\n        is_training, first_stage_features_stride, batch_norm_trainable,\n        reuse_weights, weight_decay)\n\n  def preprocess(self, resized_inputs):\n    """"""Faster R-CNN Resnet V1 preprocessing.\n\n    VGG style channel mean subtraction as described here:\n    https://gist.github.com/ksimonyan/211839e770f7b538e2d8#file-readme-md\n\n    Args:\n      resized_inputs: A [batch, height_in, width_in, channels] float32 tensor\n        representing a batch of images with values between 0 and 255.0.\n\n    Returns:\n      preprocessed_inputs: A [batch, height_out, width_out, channels] float32\n        tensor representing a batch of images.\n\n    """"""\n    channel_means = [123.68, 116.779, 103.939]\n    return resized_inputs - [[channel_means]]\n\n  def _extract_proposal_features(self, preprocessed_inputs, scope):\n    """"""Extracts first stage RPN features.\n\n    Args:\n      preprocessed_inputs: A [batch, height, width, channels] float32 tensor\n        representing a batch of images.\n      scope: A scope name.\n\n    Returns:\n      rpn_feature_map: A tensor with shape [batch, height, width, depth]\n      activations: A dictionary mapping feature extractor tensor names to\n        tensors\n\n    Raises:\n      InvalidArgumentError: If the spatial size of `preprocessed_inputs`\n        (height or width) is less than 33.\n      ValueError: If the created network is missing the required activation.\n    """"""\n    if len(preprocessed_inputs.get_shape().as_list()) != 4:\n      raise ValueError(\'`preprocessed_inputs` must be 4 dimensional, got a \'\n                       \'tensor of shape %s\' % preprocessed_inputs.get_shape())\n    shape_assert = tf.Assert(\n        tf.logical_and(\n            tf.greater_equal(tf.shape(preprocessed_inputs)[1], 33),\n            tf.greater_equal(tf.shape(preprocessed_inputs)[2], 33)),\n        [\'image size must at least be 33 in both height and width.\'])\n\n    with tf.control_dependencies([shape_assert]):\n      # Disables batchnorm for fine-tuning with smaller batch sizes.\n      # TODO(chensun): Figure out if it is needed when image\n      # batch size is bigger.\n      with slim.arg_scope(\n          resnet_utils.resnet_arg_scope(\n              batch_norm_epsilon=1e-5,\n              batch_norm_scale=True,\n              weight_decay=self._weight_decay)):\n        with tf.variable_scope(\n            self._architecture, reuse=self._reuse_weights) as var_scope:\n          _, activations = self._resnet_model(\n              preprocessed_inputs,\n              num_classes=None,\n              is_training=self._train_batch_norm,\n              global_pool=False,\n              output_stride=self._first_stage_features_stride,\n              spatial_squeeze=False,\n              scope=var_scope)\n\n    handle = scope + \'/%s/block3\' % self._architecture\n    return activations[handle], activations\n\n  def _extract_box_classifier_features(self, proposal_feature_maps, scope):\n    """"""Extracts second stage box classifier features.\n\n    Args:\n      proposal_feature_maps: A 4-D float tensor with shape\n        [batch_size * self.max_num_proposals, crop_height, crop_width, depth]\n        representing the feature map cropped to each proposal.\n      scope: A scope name (unused).\n\n    Returns:\n      proposal_classifier_features: A 4-D float tensor with shape\n        [batch_size * self.max_num_proposals, height, width, depth]\n        representing box classifier features for each proposal.\n    """"""\n    with tf.variable_scope(self._architecture, reuse=self._reuse_weights):\n      with slim.arg_scope(\n          resnet_utils.resnet_arg_scope(\n              batch_norm_epsilon=1e-5,\n              batch_norm_scale=True,\n              weight_decay=self._weight_decay)):\n        with slim.arg_scope([slim.batch_norm],\n                            is_training=self._train_batch_norm):\n          blocks = [\n              resnet_utils.Block(\'block4\', resnet_v1.bottleneck, [{\n                  \'depth\': 2048,\n                  \'depth_bottleneck\': 512,\n                  \'stride\': 1\n              }] * 3)\n          ]\n          proposal_classifier_features = resnet_utils.stack_blocks_dense(\n              proposal_feature_maps, blocks)\n    return proposal_classifier_features\n\n\nclass FasterRCNNResnet50FeatureExtractor(FasterRCNNResnetV1FeatureExtractor):\n  """"""Faster R-CNN Resnet 50 feature extractor implementation.""""""\n\n  def __init__(self,\n               is_training,\n               first_stage_features_stride,\n               batch_norm_trainable=False,\n               reuse_weights=None,\n               weight_decay=0.0):\n    """"""Constructor.\n\n    Args:\n      is_training: See base class.\n      first_stage_features_stride: See base class.\n      batch_norm_trainable: See base class.\n      reuse_weights: See base class.\n      weight_decay: See base class.\n\n    Raises:\n      ValueError: If `first_stage_features_stride` is not 8 or 16,\n        or if `architecture` is not supported.\n    """"""\n    super(FasterRCNNResnet50FeatureExtractor, self).__init__(\n        \'resnet_v1_50\', resnet_v1.resnet_v1_50, is_training,\n        first_stage_features_stride, batch_norm_trainable,\n        reuse_weights, weight_decay)\n\n\nclass FasterRCNNResnet101FeatureExtractor(FasterRCNNResnetV1FeatureExtractor):\n  """"""Faster R-CNN Resnet 101 feature extractor implementation.""""""\n\n  def __init__(self,\n               is_training,\n               first_stage_features_stride,\n               batch_norm_trainable=False,\n               reuse_weights=None,\n               weight_decay=0.0):\n    """"""Constructor.\n\n    Args:\n      is_training: See base class.\n      first_stage_features_stride: See base class.\n      batch_norm_trainable: See base class.\n      reuse_weights: See base class.\n      weight_decay: See base class.\n\n    Raises:\n      ValueError: If `first_stage_features_stride` is not 8 or 16,\n        or if `architecture` is not supported.\n    """"""\n    super(FasterRCNNResnet101FeatureExtractor, self).__init__(\n        \'resnet_v1_101\', resnet_v1.resnet_v1_101, is_training,\n        first_stage_features_stride, batch_norm_trainable,\n        reuse_weights, weight_decay)\n\n\nclass FasterRCNNResnet152FeatureExtractor(FasterRCNNResnetV1FeatureExtractor):\n  """"""Faster R-CNN Resnet 152 feature extractor implementation.""""""\n\n  def __init__(self,\n               is_training,\n               first_stage_features_stride,\n               batch_norm_trainable=False,\n               reuse_weights=None,\n               weight_decay=0.0):\n    """"""Constructor.\n\n    Args:\n      is_training: See base class.\n      first_stage_features_stride: See base class.\n      batch_norm_trainable: See base class.\n      reuse_weights: See base class.\n      weight_decay: See base class.\n\n    Raises:\n      ValueError: If `first_stage_features_stride` is not 8 or 16,\n        or if `architecture` is not supported.\n    """"""\n    super(FasterRCNNResnet152FeatureExtractor, self).__init__(\n        \'resnet_v1_152\', resnet_v1.resnet_v1_152, is_training,\n        first_stage_features_stride, batch_norm_trainable,\n        reuse_weights, weight_decay)\n'"
src/object_detection/models/faster_rcnn_resnet_v1_feature_extractor_test.py,24,"b'# Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n""""""Tests for object_detection.models.faster_rcnn_resnet_v1_feature_extractor.""""""\n\nimport numpy as np\nimport tensorflow as tf\n\nfrom object_detection.models import faster_rcnn_resnet_v1_feature_extractor as faster_rcnn_resnet_v1\n\n\nclass FasterRcnnResnetV1FeatureExtractorTest(tf.test.TestCase):\n\n  def _build_feature_extractor(self,\n                               first_stage_features_stride,\n                               architecture=\'resnet_v1_101\'):\n    feature_extractor_map = {\n        \'resnet_v1_50\':\n            faster_rcnn_resnet_v1.FasterRCNNResnet50FeatureExtractor,\n        \'resnet_v1_101\':\n            faster_rcnn_resnet_v1.FasterRCNNResnet101FeatureExtractor,\n        \'resnet_v1_152\':\n            faster_rcnn_resnet_v1.FasterRCNNResnet152FeatureExtractor\n    }\n    return feature_extractor_map[architecture](\n        is_training=False,\n        first_stage_features_stride=first_stage_features_stride,\n        batch_norm_trainable=False,\n        reuse_weights=None,\n        weight_decay=0.0)\n\n  def test_extract_proposal_features_returns_expected_size(self):\n    for architecture in [\'resnet_v1_50\', \'resnet_v1_101\', \'resnet_v1_152\']:\n      feature_extractor = self._build_feature_extractor(\n          first_stage_features_stride=16, architecture=architecture)\n      preprocessed_inputs = tf.random_uniform(\n          [4, 224, 224, 3], maxval=255, dtype=tf.float32)\n      rpn_feature_map, _ = feature_extractor.extract_proposal_features(\n          preprocessed_inputs, scope=\'TestScope\')\n      features_shape = tf.shape(rpn_feature_map)\n\n      init_op = tf.global_variables_initializer()\n      with self.test_session() as sess:\n        sess.run(init_op)\n        features_shape_out = sess.run(features_shape)\n        self.assertAllEqual(features_shape_out, [4, 14, 14, 1024])\n\n  def test_extract_proposal_features_stride_eight(self):\n    feature_extractor = self._build_feature_extractor(\n        first_stage_features_stride=8)\n    preprocessed_inputs = tf.random_uniform(\n        [4, 224, 224, 3], maxval=255, dtype=tf.float32)\n    rpn_feature_map, _ = feature_extractor.extract_proposal_features(\n        preprocessed_inputs, scope=\'TestScope\')\n    features_shape = tf.shape(rpn_feature_map)\n\n    init_op = tf.global_variables_initializer()\n    with self.test_session() as sess:\n      sess.run(init_op)\n      features_shape_out = sess.run(features_shape)\n      self.assertAllEqual(features_shape_out, [4, 28, 28, 1024])\n\n  def test_extract_proposal_features_half_size_input(self):\n    feature_extractor = self._build_feature_extractor(\n        first_stage_features_stride=16)\n    preprocessed_inputs = tf.random_uniform(\n        [1, 112, 112, 3], maxval=255, dtype=tf.float32)\n    rpn_feature_map, _ = feature_extractor.extract_proposal_features(\n        preprocessed_inputs, scope=\'TestScope\')\n    features_shape = tf.shape(rpn_feature_map)\n\n    init_op = tf.global_variables_initializer()\n    with self.test_session() as sess:\n      sess.run(init_op)\n      features_shape_out = sess.run(features_shape)\n      self.assertAllEqual(features_shape_out, [1, 7, 7, 1024])\n\n  def test_extract_proposal_features_dies_on_invalid_stride(self):\n    with self.assertRaises(ValueError):\n      self._build_feature_extractor(first_stage_features_stride=99)\n\n  def test_extract_proposal_features_dies_on_very_small_images(self):\n    feature_extractor = self._build_feature_extractor(\n        first_stage_features_stride=16)\n    preprocessed_inputs = tf.placeholder(tf.float32, (4, None, None, 3))\n    rpn_feature_map, _ = feature_extractor.extract_proposal_features(\n        preprocessed_inputs, scope=\'TestScope\')\n    features_shape = tf.shape(rpn_feature_map)\n\n    init_op = tf.global_variables_initializer()\n    with self.test_session() as sess:\n      sess.run(init_op)\n      with self.assertRaises(tf.errors.InvalidArgumentError):\n        sess.run(\n            features_shape,\n            feed_dict={preprocessed_inputs: np.random.rand(4, 32, 32, 3)})\n\n  def test_extract_proposal_features_dies_with_incorrect_rank_inputs(self):\n    feature_extractor = self._build_feature_extractor(\n        first_stage_features_stride=16)\n    preprocessed_inputs = tf.random_uniform(\n        [224, 224, 3], maxval=255, dtype=tf.float32)\n    with self.assertRaises(ValueError):\n      feature_extractor.extract_proposal_features(\n          preprocessed_inputs, scope=\'TestScope\')\n\n  def test_extract_box_classifier_features_returns_expected_size(self):\n    feature_extractor = self._build_feature_extractor(\n        first_stage_features_stride=16)\n    proposal_feature_maps = tf.random_uniform(\n        [3, 7, 7, 1024], maxval=255, dtype=tf.float32)\n    proposal_classifier_features = (\n        feature_extractor.extract_box_classifier_features(\n            proposal_feature_maps, scope=\'TestScope\'))\n    features_shape = tf.shape(proposal_classifier_features)\n\n    init_op = tf.global_variables_initializer()\n    with self.test_session() as sess:\n      sess.run(init_op)\n      features_shape_out = sess.run(features_shape)\n      self.assertAllEqual(features_shape_out, [3, 7, 7, 2048])\n\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
src/object_detection/models/feature_map_generators.py,2,"b'# Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n""""""Functions to generate a list of feature maps based on image features.\n\nProvides several feature map generators that can be used to build object\ndetection feature extractors.\n\nObject detection feature extractors usually are built by stacking two components\n- A base feature extractor such as Inception V3 and a feature map generator.\nFeature map generators build on the base feature extractors and produce a list\nof final feature maps.\n""""""\nimport collections\nimport tensorflow as tf\nfrom object_detection.utils import ops\nslim = tf.contrib.slim\n\n\ndef get_depth_fn(depth_multiplier, min_depth):\n  """"""Builds a callable to compute depth (output channels) of conv filters.\n\n  Args:\n    depth_multiplier: a multiplier for the nominal depth.\n    min_depth: a lower bound on the depth of filters.\n\n  Returns:\n    A callable that takes in a nominal depth and returns the depth to use.\n  """"""\n  def multiply_depth(depth):\n    new_depth = int(depth * depth_multiplier)\n    return max(new_depth, min_depth)\n  return multiply_depth\n\n\ndef multi_resolution_feature_maps(feature_map_layout, depth_multiplier,\n                                  min_depth, insert_1x1_conv, image_features):\n  """"""Generates multi resolution feature maps from input image features.\n\n  Generates multi-scale feature maps for detection as in the SSD papers by\n  Liu et al: https://arxiv.org/pdf/1512.02325v2.pdf, See Sec 2.1.\n\n  More specifically, it performs the following two tasks:\n  1) If a layer name is provided in the configuration, returns that layer as a\n     feature map.\n  2) If a layer name is left as an empty string, constructs a new feature map\n     based on the spatial shape and depth configuration. Note that the current\n     implementation only supports generating new layers using convolution of\n     stride 2 resulting in a spatial resolution reduction by a factor of 2.\n     By default convolution kernel size is set to 3, and it can be customized\n     by caller.\n\n  An example of the configuration for Inception V3:\n  {\n    \'from_layer\': [\'Mixed_5d\', \'Mixed_6e\', \'Mixed_7c\', \'\', \'\', \'\'],\n    \'layer_depth\': [-1, -1, -1, 512, 256, 128]\n  }\n\n  Args:\n    feature_map_layout: Dictionary of specifications for the feature map\n      layouts in the following format (Inception V2/V3 respectively):\n      {\n        \'from_layer\': [\'Mixed_3c\', \'Mixed_4c\', \'Mixed_5c\', \'\', \'\', \'\'],\n        \'layer_depth\': [-1, -1, -1, 512, 256, 128]\n      }\n      or\n      {\n        \'from_layer\': [\'Mixed_5d\', \'Mixed_6e\', \'Mixed_7c\', \'\', \'\', \'\', \'\'],\n        \'layer_depth\': [-1, -1, -1, 512, 256, 128]\n      }\n      If \'from_layer\' is specified, the specified feature map is directly used\n      as a box predictor layer, and the layer_depth is directly infered from the\n      feature map (instead of using the provided \'layer_depth\' parameter). In\n      this case, our convention is to set \'layer_depth\' to -1 for clarity.\n      Otherwise, if \'from_layer\' is an empty string, then the box predictor\n      layer will be built from the previous layer using convolution operations.\n      Note that the current implementation only supports generating new layers\n      using convolutions of stride 2 (resulting in a spatial resolution\n      reduction by a factor of 2), and will be extended to a more flexible\n      design. Convolution kernel size is set to 3 by default, and can be\n      customized by \'conv_kernel_size\' parameter (similarily, \'conv_kernel_size\'\n      should be set to -1 if \'from_layer\' is specified). The created convolution\n      operation will be a normal 2D convolution by default, and a depthwise\n      convolution followed by 1x1 convolution if \'use_depthwise\' is set to True.\n    depth_multiplier: Depth multiplier for convolutional layers.\n    min_depth: Minimum depth for convolutional layers.\n    insert_1x1_conv: A boolean indicating whether an additional 1x1 convolution\n      should be inserted before shrinking the feature map.\n    image_features: A dictionary of handles to activation tensors from the\n      base feature extractor.\n\n  Returns:\n    feature_maps: an OrderedDict mapping keys (feature map names) to\n      tensors where each tensor has shape [batch, height_i, width_i, depth_i].\n\n  Raises:\n    ValueError: if the number entries in \'from_layer\' and\n      \'layer_depth\' do not match.\n    ValueError: if the generated layer does not have the same resolution\n      as specified.\n  """"""\n  depth_fn = get_depth_fn(depth_multiplier, min_depth)\n\n  feature_map_keys = []\n  feature_maps = []\n  base_from_layer = \'\'\n  use_explicit_padding = False\n  if \'use_explicit_padding\' in feature_map_layout:\n    use_explicit_padding = feature_map_layout[\'use_explicit_padding\']\n  use_depthwise = False\n  if \'use_depthwise\' in feature_map_layout:\n    use_depthwise = feature_map_layout[\'use_depthwise\']\n  for index, from_layer in enumerate(feature_map_layout[\'from_layer\']):\n    layer_depth = feature_map_layout[\'layer_depth\'][index]\n    conv_kernel_size = 3\n    if \'conv_kernel_size\' in feature_map_layout:\n      conv_kernel_size = feature_map_layout[\'conv_kernel_size\'][index]\n    if from_layer:\n      feature_map = image_features[from_layer]\n      base_from_layer = from_layer\n      feature_map_keys.append(from_layer)\n    else:\n      pre_layer = feature_maps[-1]\n      intermediate_layer = pre_layer\n      if insert_1x1_conv:\n        layer_name = \'{}_1_Conv2d_{}_1x1_{}\'.format(\n            base_from_layer, index, depth_fn(layer_depth / 2))\n        intermediate_layer = slim.conv2d(\n            pre_layer,\n            depth_fn(layer_depth / 2), [1, 1],\n            padding=\'SAME\',\n            stride=1,\n            scope=layer_name)\n      layer_name = \'{}_2_Conv2d_{}_{}x{}_s2_{}\'.format(\n          base_from_layer, index, conv_kernel_size, conv_kernel_size,\n          depth_fn(layer_depth))\n      stride = 2\n      padding = \'SAME\'\n      if use_explicit_padding:\n        padding = \'VALID\'\n        intermediate_layer = ops.fixed_padding(\n            intermediate_layer, conv_kernel_size)\n      if use_depthwise:\n        feature_map = slim.separable_conv2d(\n            intermediate_layer,\n            None, [conv_kernel_size, conv_kernel_size],\n            depth_multiplier=1,\n            padding=padding,\n            stride=stride,\n            scope=layer_name + \'_depthwise\')\n        feature_map = slim.conv2d(\n            feature_map,\n            depth_fn(layer_depth), [1, 1],\n            padding=\'SAME\',\n            stride=1,\n            scope=layer_name)\n      else:\n        feature_map = slim.conv2d(\n            intermediate_layer,\n            depth_fn(layer_depth), [conv_kernel_size, conv_kernel_size],\n            padding=padding,\n            stride=stride,\n            scope=layer_name)\n      feature_map_keys.append(layer_name)\n    feature_maps.append(feature_map)\n  return collections.OrderedDict(\n      [(x, y) for (x, y) in zip(feature_map_keys, feature_maps)])\n\n\ndef fpn_top_down_feature_maps(image_features, depth, scope=None):\n  """"""Generates `top-down` feature maps for Feature Pyramid Networks.\n\n  See https://arxiv.org/abs/1612.03144 for details.\n\n  Args:\n    image_features: list of image feature tensors. Spatial resolutions of\n      succesive tensors must reduce exactly by a factor of 2.\n    depth: depth of output feature maps.\n    scope: A scope name to wrap this op under.\n\n  Returns:\n    feature_maps: an OrderedDict mapping keys (feature map names) to\n      tensors where each tensor has shape [batch, height_i, width_i, depth_i].\n  """"""\n  with tf.variable_scope(\n      scope, \'top_down\', image_features):\n    num_levels = len(image_features)\n    output_feature_maps_list = []\n    output_feature_map_keys = []\n    with slim.arg_scope(\n        [slim.conv2d],\n        activation_fn=None, normalizer_fn=None, padding=\'SAME\', stride=1):\n      top_down = slim.conv2d(\n          image_features[-1],\n          depth, [1, 1], scope=\'projection_%d\' % num_levels)\n      output_feature_maps_list.append(top_down)\n      output_feature_map_keys.append(\n          \'top_down_feature_map_%d\' % (num_levels - 1))\n\n      for level in reversed(range(num_levels - 1)):\n        top_down = ops.nearest_neighbor_upsampling(top_down, 2)\n        residual = slim.conv2d(\n            image_features[level], depth, [1, 1],\n            scope=\'projection_%d\' % (level + 1))\n        top_down = 0.5 * top_down + 0.5 * residual\n        output_feature_maps_list.append(slim.conv2d(\n            top_down,\n            depth, [3, 3],\n            activation_fn=None,\n            scope=\'smoothing_%d\' % (level + 1)))\n        output_feature_map_keys.append(\'top_down_feature_map_%d\' % level)\n      return collections.OrderedDict(\n          reversed(zip(output_feature_map_keys, output_feature_maps_list)))\n'"
src/object_detection/models/feature_map_generators_test.py,22,"b'# Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n""""""Tests for feature map generators.""""""\n\nimport tensorflow as tf\n\nfrom object_detection.models import feature_map_generators\n\nINCEPTION_V2_LAYOUT = {\n    \'from_layer\': [\'Mixed_3c\', \'Mixed_4c\', \'Mixed_5c\', \'\', \'\', \'\'],\n    \'layer_depth\': [-1, -1, -1, 512, 256, 256],\n    \'anchor_strides\': [16, 32, 64, -1, -1, -1],\n    \'layer_target_norm\': [20.0, -1, -1, -1, -1, -1],\n}\n\nINCEPTION_V3_LAYOUT = {\n    \'from_layer\': [\'Mixed_5d\', \'Mixed_6e\', \'Mixed_7c\', \'\', \'\', \'\'],\n    \'layer_depth\': [-1, -1, -1, 512, 256, 128],\n    \'anchor_strides\': [16, 32, 64, -1, -1, -1],\n    \'aspect_ratios\': [1.0, 2.0, 1.0/2, 3.0, 1.0/3]\n}\n\nEMBEDDED_SSD_MOBILENET_V1_LAYOUT = {\n    \'from_layer\': [\'Conv2d_11_pointwise\', \'Conv2d_13_pointwise\', \'\', \'\', \'\'],\n    \'layer_depth\': [-1, -1, 512, 256, 256],\n    \'conv_kernel_size\': [-1, -1, 3, 3, 2],\n}\n\n\n# TODO(rathodv): add tests with different anchor strides.\nclass MultiResolutionFeatureMapGeneratorTest(tf.test.TestCase):\n\n  def test_get_expected_feature_map_shapes_with_inception_v2(self):\n    image_features = {\n        \'Mixed_3c\': tf.random_uniform([4, 28, 28, 256], dtype=tf.float32),\n        \'Mixed_4c\': tf.random_uniform([4, 14, 14, 576], dtype=tf.float32),\n        \'Mixed_5c\': tf.random_uniform([4, 7, 7, 1024], dtype=tf.float32)\n    }\n    feature_maps = feature_map_generators.multi_resolution_feature_maps(\n        feature_map_layout=INCEPTION_V2_LAYOUT,\n        depth_multiplier=1,\n        min_depth=32,\n        insert_1x1_conv=True,\n        image_features=image_features)\n\n    expected_feature_map_shapes = {\n        \'Mixed_3c\': (4, 28, 28, 256),\n        \'Mixed_4c\': (4, 14, 14, 576),\n        \'Mixed_5c\': (4, 7, 7, 1024),\n        \'Mixed_5c_2_Conv2d_3_3x3_s2_512\': (4, 4, 4, 512),\n        \'Mixed_5c_2_Conv2d_4_3x3_s2_256\': (4, 2, 2, 256),\n        \'Mixed_5c_2_Conv2d_5_3x3_s2_256\': (4, 1, 1, 256)}\n\n    init_op = tf.global_variables_initializer()\n    with self.test_session() as sess:\n      sess.run(init_op)\n      out_feature_maps = sess.run(feature_maps)\n      out_feature_map_shapes = dict(\n          (key, value.shape) for key, value in out_feature_maps.items())\n      self.assertDictEqual(out_feature_map_shapes, expected_feature_map_shapes)\n\n  def test_get_expected_feature_map_shapes_with_inception_v3(self):\n    image_features = {\n        \'Mixed_5d\': tf.random_uniform([4, 35, 35, 256], dtype=tf.float32),\n        \'Mixed_6e\': tf.random_uniform([4, 17, 17, 576], dtype=tf.float32),\n        \'Mixed_7c\': tf.random_uniform([4, 8, 8, 1024], dtype=tf.float32)\n    }\n\n    feature_maps = feature_map_generators.multi_resolution_feature_maps(\n        feature_map_layout=INCEPTION_V3_LAYOUT,\n        depth_multiplier=1,\n        min_depth=32,\n        insert_1x1_conv=True,\n        image_features=image_features)\n\n    expected_feature_map_shapes = {\n        \'Mixed_5d\': (4, 35, 35, 256),\n        \'Mixed_6e\': (4, 17, 17, 576),\n        \'Mixed_7c\': (4, 8, 8, 1024),\n        \'Mixed_7c_2_Conv2d_3_3x3_s2_512\': (4, 4, 4, 512),\n        \'Mixed_7c_2_Conv2d_4_3x3_s2_256\': (4, 2, 2, 256),\n        \'Mixed_7c_2_Conv2d_5_3x3_s2_128\': (4, 1, 1, 128)}\n\n    init_op = tf.global_variables_initializer()\n    with self.test_session() as sess:\n      sess.run(init_op)\n      out_feature_maps = sess.run(feature_maps)\n      out_feature_map_shapes = dict(\n          (key, value.shape) for key, value in out_feature_maps.items())\n      self.assertDictEqual(out_feature_map_shapes, expected_feature_map_shapes)\n\n  def test_get_expected_feature_map_shapes_with_embedded_ssd_mobilenet_v1(\n      self):\n    image_features = {\n        \'Conv2d_11_pointwise\': tf.random_uniform([4, 16, 16, 512],\n                                                 dtype=tf.float32),\n        \'Conv2d_13_pointwise\': tf.random_uniform([4, 8, 8, 1024],\n                                                 dtype=tf.float32),\n    }\n\n    feature_maps = feature_map_generators.multi_resolution_feature_maps(\n        feature_map_layout=EMBEDDED_SSD_MOBILENET_V1_LAYOUT,\n        depth_multiplier=1,\n        min_depth=32,\n        insert_1x1_conv=True,\n        image_features=image_features)\n\n    expected_feature_map_shapes = {\n        \'Conv2d_11_pointwise\': (4, 16, 16, 512),\n        \'Conv2d_13_pointwise\': (4, 8, 8, 1024),\n        \'Conv2d_13_pointwise_2_Conv2d_2_3x3_s2_512\': (4, 4, 4, 512),\n        \'Conv2d_13_pointwise_2_Conv2d_3_3x3_s2_256\': (4, 2, 2, 256),\n        \'Conv2d_13_pointwise_2_Conv2d_4_2x2_s2_256\': (4, 1, 1, 256)}\n\n    init_op = tf.global_variables_initializer()\n    with self.test_session() as sess:\n      sess.run(init_op)\n      out_feature_maps = sess.run(feature_maps)\n      out_feature_map_shapes = dict(\n          (key, value.shape) for key, value in out_feature_maps.items())\n      self.assertDictEqual(out_feature_map_shapes, expected_feature_map_shapes)\n\n\nclass FPNFeatureMapGeneratorTest(tf.test.TestCase):\n\n  def test_get_expected_feature_map_shapes(self):\n    image_features = [\n        tf.random_uniform([4, 8, 8, 256], dtype=tf.float32),\n        tf.random_uniform([4, 4, 4, 256], dtype=tf.float32),\n        tf.random_uniform([4, 2, 2, 256], dtype=tf.float32),\n        tf.random_uniform([4, 1, 1, 256], dtype=tf.float32),\n    ]\n    feature_maps = feature_map_generators.fpn_top_down_feature_maps(\n        image_features=image_features, depth=128)\n\n    expected_feature_map_shapes = {\n        \'top_down_feature_map_0\': (4, 8, 8, 128),\n        \'top_down_feature_map_1\': (4, 4, 4, 128),\n        \'top_down_feature_map_2\': (4, 2, 2, 128),\n        \'top_down_feature_map_3\': (4, 1, 1, 128)\n    }\n\n    init_op = tf.global_variables_initializer()\n    with self.test_session() as sess:\n      sess.run(init_op)\n      out_feature_maps = sess.run(feature_maps)\n      out_feature_map_shapes = {key: value.shape\n                                for key, value in out_feature_maps.items()}\n      self.assertDictEqual(out_feature_map_shapes, expected_feature_map_shapes)\n\n\nclass GetDepthFunctionTest(tf.test.TestCase):\n\n  def test_return_min_depth_when_multiplier_is_small(self):\n    depth_fn = feature_map_generators.get_depth_fn(depth_multiplier=0.5,\n                                                   min_depth=16)\n    self.assertEqual(depth_fn(16), 16)\n\n  def test_return_correct_depth_with_multiplier(self):\n    depth_fn = feature_map_generators.get_depth_fn(depth_multiplier=0.5,\n                                                   min_depth=16)\n    self.assertEqual(depth_fn(64), 32)\n\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
src/object_detection/models/ssd_feature_extractor_test.py,8,"b'# Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n""""""Base test class SSDFeatureExtractors.""""""\n\nfrom abc import abstractmethod\n\nimport itertools\nimport numpy as np\nimport tensorflow as tf\n\nfrom object_detection.utils import test_case\n\n\nclass SsdFeatureExtractorTestBase(test_case.TestCase):\n\n  @abstractmethod\n  def _create_feature_extractor(self, depth_multiplier, pad_to_multiple,\n                                use_explicit_padding=False):\n    """"""Constructs a new feature extractor.\n\n    Args:\n      depth_multiplier: float depth multiplier for feature extractor\n      pad_to_multiple: the nearest multiple to zero pad the input height and\n        width dimensions to.\n      use_explicit_padding: use \'VALID\' padding for convolutions, but prepad\n        inputs so that the output dimensions are the same as if \'SAME\' padding\n        were used.\n    Returns:\n      an ssd_meta_arch.SSDFeatureExtractor object.\n    """"""\n    pass\n\n  def check_extract_features_returns_correct_shape(\n      self, batch_size, image_height, image_width, depth_multiplier,\n      pad_to_multiple, expected_feature_map_shapes, use_explicit_padding=False):\n    def graph_fn(image_tensor):\n      feature_extractor = self._create_feature_extractor(depth_multiplier,\n                                                         pad_to_multiple,\n                                                         use_explicit_padding)\n      feature_maps = feature_extractor.extract_features(image_tensor)\n      return feature_maps\n\n    image_tensor = np.random.rand(batch_size, image_height, image_width,\n                                  3).astype(np.float32)\n    feature_maps = self.execute(graph_fn, [image_tensor])\n    for feature_map, expected_shape in itertools.izip(\n        feature_maps, expected_feature_map_shapes):\n      self.assertAllEqual(feature_map.shape, expected_shape)\n\n  def check_extract_features_returns_correct_shapes_with_dynamic_inputs(\n      self, batch_size, image_height, image_width, depth_multiplier,\n      pad_to_multiple, expected_feature_map_shapes, use_explicit_padding=False):\n    def graph_fn(image_height, image_width):\n      feature_extractor = self._create_feature_extractor(depth_multiplier,\n                                                         pad_to_multiple,\n                                                         use_explicit_padding)\n      image_tensor = tf.random_uniform([batch_size, image_height, image_width,\n                                        3], dtype=tf.float32)\n      feature_maps = feature_extractor.extract_features(image_tensor)\n      return feature_maps\n\n    feature_maps = self.execute_cpu(graph_fn, [\n        np.array(image_height, dtype=np.int32),\n        np.array(image_width, dtype=np.int32)\n    ])\n    for feature_map, expected_shape in itertools.izip(\n        feature_maps, expected_feature_map_shapes):\n      self.assertAllEqual(feature_map.shape, expected_shape)\n\n  def check_extract_features_raises_error_with_invalid_image_size(\n      self, image_height, image_width, depth_multiplier, pad_to_multiple):\n    feature_extractor = self._create_feature_extractor(depth_multiplier,\n                                                       pad_to_multiple)\n    preprocessed_inputs = tf.placeholder(tf.float32, (4, None, None, 3))\n    feature_maps = feature_extractor.extract_features(preprocessed_inputs)\n    test_preprocessed_image = np.random.rand(4, image_height, image_width, 3)\n    with self.test_session() as sess:\n      sess.run(tf.global_variables_initializer())\n      with self.assertRaises(tf.errors.InvalidArgumentError):\n        sess.run(feature_maps,\n                 feed_dict={preprocessed_inputs: test_preprocessed_image})\n\n  def check_feature_extractor_variables_under_scope(\n      self, depth_multiplier, pad_to_multiple, scope_name):\n    g = tf.Graph()\n    with g.as_default():\n      feature_extractor = self._create_feature_extractor(\n          depth_multiplier, pad_to_multiple)\n      preprocessed_inputs = tf.placeholder(tf.float32, (4, None, None, 3))\n      feature_extractor.extract_features(preprocessed_inputs)\n      variables = g.get_collection(tf.GraphKeys.GLOBAL_VARIABLES)\n      for variable in variables:\n        self.assertTrue(variable.name.startswith(scope_name))\n'"
src/object_detection/models/ssd_inception_v2_feature_extractor.py,2,"b'# Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n""""""SSDFeatureExtractor for InceptionV2 features.""""""\nimport tensorflow as tf\n\nfrom object_detection.meta_architectures import ssd_meta_arch\nfrom object_detection.models import feature_map_generators\nfrom object_detection.utils import ops\nfrom object_detection.utils import shape_utils\nfrom nets import inception_v2\n\nslim = tf.contrib.slim\n\n\nclass SSDInceptionV2FeatureExtractor(ssd_meta_arch.SSDFeatureExtractor):\n  """"""SSD Feature Extractor using InceptionV2 features.""""""\n\n  def __init__(self,\n               is_training,\n               depth_multiplier,\n               min_depth,\n               pad_to_multiple,\n               conv_hyperparams,\n               batch_norm_trainable=True,\n               reuse_weights=None,\n               use_explicit_padding=False,\n               use_depthwise=False):\n    """"""InceptionV2 Feature Extractor for SSD Models.\n\n    Args:\n      is_training: whether the network is in training mode.\n      depth_multiplier: float depth multiplier for feature extractor.\n      min_depth: minimum feature extractor depth.\n      pad_to_multiple: the nearest multiple to zero pad the input height and\n        width dimensions to.\n      conv_hyperparams: tf slim arg_scope for conv2d and separable_conv2d ops.\n      batch_norm_trainable: Whether to update batch norm parameters during\n        training or not. When training with a small batch size\n        (e.g. 1), it is desirable to disable batch norm update and use\n        pretrained batch norm params.\n      reuse_weights: Whether to reuse variables. Default is None.\n      use_explicit_padding: Whether to use explicit padding when extracting\n        features. Default is False.\n      use_depthwise: Whether to use depthwise convolutions. Default is False.\n    """"""\n    super(SSDInceptionV2FeatureExtractor, self).__init__(\n        is_training, depth_multiplier, min_depth, pad_to_multiple,\n        conv_hyperparams, batch_norm_trainable, reuse_weights,\n        use_explicit_padding, use_depthwise)\n\n  def preprocess(self, resized_inputs):\n    """"""SSD preprocessing.\n\n    Maps pixel values to the range [-1, 1].\n\n    Args:\n      resized_inputs: a [batch, height, width, channels] float tensor\n        representing a batch of images.\n\n    Returns:\n      preprocessed_inputs: a [batch, height, width, channels] float tensor\n        representing a batch of images.\n    """"""\n    return (2.0 / 255.0) * resized_inputs - 1.0\n\n  def extract_features(self, preprocessed_inputs):\n    """"""Extract features from preprocessed inputs.\n\n    Args:\n      preprocessed_inputs: a [batch, height, width, channels] float tensor\n        representing a batch of images.\n\n    Returns:\n      feature_maps: a list of tensors where the ith tensor has shape\n        [batch, height_i, width_i, depth_i]\n    """"""\n    preprocessed_inputs = shape_utils.check_min_image_dim(\n        33, preprocessed_inputs)\n\n    feature_map_layout = {\n        \'from_layer\': [\'Mixed_4c\', \'Mixed_5c\', \'\', \'\', \'\', \'\'],\n        \'layer_depth\': [-1, -1, 512, 256, 256, 128],\n        \'use_explicit_padding\': self._use_explicit_padding,\n        \'use_depthwise\': self._use_depthwise,\n    }\n\n    with slim.arg_scope(self._conv_hyperparams):\n      with tf.variable_scope(\'InceptionV2\',\n                             reuse=self._reuse_weights) as scope:\n        _, image_features = inception_v2.inception_v2_base(\n            ops.pad_to_multiple(preprocessed_inputs, self._pad_to_multiple),\n            final_endpoint=\'Mixed_5c\',\n            min_depth=self._min_depth,\n            depth_multiplier=self._depth_multiplier,\n            scope=scope)\n        feature_maps = feature_map_generators.multi_resolution_feature_maps(\n            feature_map_layout=feature_map_layout,\n            depth_multiplier=self._depth_multiplier,\n            min_depth=self._min_depth,\n            insert_1x1_conv=True,\n            image_features=image_features)\n\n    return feature_maps.values()\n'"
src/object_detection/models/ssd_inception_v2_feature_extractor_test.py,1,"b'# Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n""""""Tests for object_detection.models.ssd_inception_v2_feature_extractor.""""""\nimport numpy as np\nimport tensorflow as tf\n\nfrom object_detection.models import ssd_feature_extractor_test\nfrom object_detection.models import ssd_inception_v2_feature_extractor\n\n\nclass SsdInceptionV2FeatureExtractorTest(\n    ssd_feature_extractor_test.SsdFeatureExtractorTestBase):\n\n  def _create_feature_extractor(self, depth_multiplier, pad_to_multiple,\n                                is_training=True, batch_norm_trainable=True):\n    """"""Constructs a SsdInceptionV2FeatureExtractor.\n\n    Args:\n      depth_multiplier: float depth multiplier for feature extractor\n      pad_to_multiple: the nearest multiple to zero pad the input height and\n        width dimensions to.\n      is_training: whether the network is in training mode.\n      batch_norm_trainable: Whether to update batch norm parameters during\n        training or not\n    Returns:\n      an ssd_inception_v2_feature_extractor.SsdInceptionV2FeatureExtractor.\n    """"""\n    min_depth = 32\n    conv_hyperparams = {}\n    return ssd_inception_v2_feature_extractor.SSDInceptionV2FeatureExtractor(\n        is_training, depth_multiplier, min_depth, pad_to_multiple,\n        conv_hyperparams, batch_norm_trainable)\n\n  def test_extract_features_returns_correct_shapes_128(self):\n    image_height = 128\n    image_width = 128\n    depth_multiplier = 1.0\n    pad_to_multiple = 1\n    expected_feature_map_shape = [(2, 8, 8, 576), (2, 4, 4, 1024),\n                                  (2, 2, 2, 512), (2, 1, 1, 256),\n                                  (2, 1, 1, 256), (2, 1, 1, 128)]\n    self.check_extract_features_returns_correct_shape(\n        2, image_height, image_width, depth_multiplier, pad_to_multiple,\n        expected_feature_map_shape)\n\n  def test_extract_features_returns_correct_shapes_with_dynamic_inputs(self):\n    image_height = 128\n    image_width = 128\n    depth_multiplier = 1.0\n    pad_to_multiple = 1\n    expected_feature_map_shape = [(2, 8, 8, 576), (2, 4, 4, 1024),\n                                  (2, 2, 2, 512), (2, 1, 1, 256),\n                                  (2, 1, 1, 256), (2, 1, 1, 128)]\n    self.check_extract_features_returns_correct_shapes_with_dynamic_inputs(\n        2, image_height, image_width, depth_multiplier, pad_to_multiple,\n        expected_feature_map_shape)\n\n  def test_extract_features_returns_correct_shapes_299(self):\n    image_height = 299\n    image_width = 299\n    depth_multiplier = 1.0\n    pad_to_multiple = 1\n    expected_feature_map_shape = [(2, 19, 19, 576), (2, 10, 10, 1024),\n                                  (2, 5, 5, 512), (2, 3, 3, 256),\n                                  (2, 2, 2, 256), (2, 1, 1, 128)]\n    self.check_extract_features_returns_correct_shape(\n        2, image_height, image_width, depth_multiplier, pad_to_multiple,\n        expected_feature_map_shape)\n\n  def test_extract_features_returns_correct_shapes_enforcing_min_depth(self):\n    image_height = 299\n    image_width = 299\n    depth_multiplier = 0.5**12\n    pad_to_multiple = 1\n    expected_feature_map_shape = [(2, 19, 19, 128), (2, 10, 10, 128),\n                                  (2, 5, 5, 32), (2, 3, 3, 32),\n                                  (2, 2, 2, 32), (2, 1, 1, 32)]\n    self.check_extract_features_returns_correct_shape(\n        2, image_height, image_width, depth_multiplier, pad_to_multiple,\n        expected_feature_map_shape)\n\n  def test_extract_features_returns_correct_shapes_with_pad_to_multiple(self):\n    image_height = 299\n    image_width = 299\n    depth_multiplier = 1.0\n    pad_to_multiple = 32\n    expected_feature_map_shape = [(2, 20, 20, 576), (2, 10, 10, 1024),\n                                  (2, 5, 5, 512), (2, 3, 3, 256),\n                                  (2, 2, 2, 256), (2, 1, 1, 128)]\n    self.check_extract_features_returns_correct_shape(\n        2, image_height, image_width, depth_multiplier, pad_to_multiple,\n        expected_feature_map_shape)\n\n  def test_extract_features_raises_error_with_invalid_image_size(self):\n    image_height = 32\n    image_width = 32\n    depth_multiplier = 1.0\n    pad_to_multiple = 1\n    self.check_extract_features_raises_error_with_invalid_image_size(\n        image_height, image_width, depth_multiplier, pad_to_multiple)\n\n  def test_preprocess_returns_correct_value_range(self):\n    image_height = 128\n    image_width = 128\n    depth_multiplier = 1\n    pad_to_multiple = 1\n    test_image = np.random.rand(4, image_height, image_width, 3)\n    feature_extractor = self._create_feature_extractor(depth_multiplier,\n                                                       pad_to_multiple)\n    preprocessed_image = feature_extractor.preprocess(test_image)\n    self.assertTrue(np.all(np.less_equal(np.abs(preprocessed_image), 1.0)))\n\n  def test_variables_only_created_in_scope(self):\n    depth_multiplier = 1\n    pad_to_multiple = 1\n    scope_name = \'InceptionV2\'\n    self.check_feature_extractor_variables_under_scope(\n        depth_multiplier, pad_to_multiple, scope_name)\n\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
src/object_detection/models/ssd_inception_v3_feature_extractor.py,2,"b'# Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n""""""SSDFeatureExtractor for InceptionV3 features.""""""\nimport tensorflow as tf\n\nfrom object_detection.meta_architectures import ssd_meta_arch\nfrom object_detection.models import feature_map_generators\nfrom object_detection.utils import ops\nfrom object_detection.utils import shape_utils\nfrom nets import inception_v3\n\nslim = tf.contrib.slim\n\n\nclass SSDInceptionV3FeatureExtractor(ssd_meta_arch.SSDFeatureExtractor):\n  """"""SSD Feature Extractor using InceptionV3 features.""""""\n\n  def __init__(self,\n               is_training,\n               depth_multiplier,\n               min_depth,\n               pad_to_multiple,\n               conv_hyperparams,\n               batch_norm_trainable=True,\n               reuse_weights=None,\n               use_explicit_padding=False,\n               use_depthwise=False):\n    """"""InceptionV3 Feature Extractor for SSD Models.\n\n    Args:\n      is_training: whether the network is in training mode.\n      depth_multiplier: float depth multiplier for feature extractor.\n      min_depth: minimum feature extractor depth.\n      pad_to_multiple: the nearest multiple to zero pad the input height and\n        width dimensions to.\n      conv_hyperparams: tf slim arg_scope for conv2d and separable_conv2d ops.\n      batch_norm_trainable: Whether to update batch norm parameters during\n        training or not. When training with a small batch size\n        (e.g. 1), it is desirable to disable batch norm update and use\n        pretrained batch norm params.\n      reuse_weights: Whether to reuse variables. Default is None.\n      use_explicit_padding: Whether to use explicit padding when extracting\n        features. Default is False.\n      use_depthwise: Whether to use depthwise convolutions. Default is False.\n    """"""\n    super(SSDInceptionV3FeatureExtractor, self).__init__(\n        is_training, depth_multiplier, min_depth, pad_to_multiple,\n        conv_hyperparams, batch_norm_trainable, reuse_weights,\n        use_explicit_padding, use_depthwise)\n\n  def preprocess(self, resized_inputs):\n    """"""SSD preprocessing.\n\n    Maps pixel values to the range [-1, 1].\n\n    Args:\n      resized_inputs: a [batch, height, width, channels] float tensor\n        representing a batch of images.\n\n    Returns:\n      preprocessed_inputs: a [batch, height, width, channels] float tensor\n        representing a batch of images.\n    """"""\n    return (2.0 / 255.0) * resized_inputs - 1.0\n\n  def extract_features(self, preprocessed_inputs):\n    """"""Extract features from preprocessed inputs.\n\n    Args:\n      preprocessed_inputs: a [batch, height, width, channels] float tensor\n        representing a batch of images.\n\n    Returns:\n      feature_maps: a list of tensors where the ith tensor has shape\n        [batch, height_i, width_i, depth_i]\n    """"""\n    preprocessed_inputs = shape_utils.check_min_image_dim(\n        33, preprocessed_inputs)\n\n    feature_map_layout = {\n        \'from_layer\': [\'Mixed_5d\', \'Mixed_6e\', \'Mixed_7c\', \'\', \'\', \'\'],\n        \'layer_depth\': [-1, -1, -1, 512, 256, 128],\n        \'use_explicit_padding\': self._use_explicit_padding,\n        \'use_depthwise\': self._use_depthwise,\n    }\n\n    with slim.arg_scope(self._conv_hyperparams):\n      with tf.variable_scope(\'InceptionV3\', reuse=self._reuse_weights) as scope:\n        _, image_features = inception_v3.inception_v3_base(\n            ops.pad_to_multiple(preprocessed_inputs, self._pad_to_multiple),\n            final_endpoint=\'Mixed_7c\',\n            min_depth=self._min_depth,\n            depth_multiplier=self._depth_multiplier,\n            scope=scope)\n        feature_maps = feature_map_generators.multi_resolution_feature_maps(\n            feature_map_layout=feature_map_layout,\n            depth_multiplier=self._depth_multiplier,\n            min_depth=self._min_depth,\n            insert_1x1_conv=True,\n            image_features=image_features)\n\n    return feature_maps.values()\n'"
src/object_detection/models/ssd_inception_v3_feature_extractor_test.py,1,"b'# Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n""""""Tests for object_detection.models.ssd_inception_v3_feature_extractor.""""""\nimport numpy as np\nimport tensorflow as tf\n\nfrom object_detection.models import ssd_feature_extractor_test\nfrom object_detection.models import ssd_inception_v3_feature_extractor\n\n\nclass SsdInceptionV3FeatureExtractorTest(\n    ssd_feature_extractor_test.SsdFeatureExtractorTestBase):\n\n  def _create_feature_extractor(self, depth_multiplier, pad_to_multiple,\n                                is_training=True, batch_norm_trainable=True):\n    """"""Constructs a SsdInceptionV3FeatureExtractor.\n\n    Args:\n      depth_multiplier: float depth multiplier for feature extractor\n      pad_to_multiple: the nearest multiple to zero pad the input height and\n        width dimensions to.\n      is_training: whether the network is in training mode.\n      batch_norm_trainable: Whether to update batch norm parameters during\n        training or not\n    Returns:\n      an ssd_inception_v3_feature_extractor.SsdInceptionV3FeatureExtractor.\n    """"""\n    min_depth = 32\n    conv_hyperparams = {}\n    return ssd_inception_v3_feature_extractor.SSDInceptionV3FeatureExtractor(\n        is_training, depth_multiplier, min_depth, pad_to_multiple,\n        conv_hyperparams, batch_norm_trainable)\n\n  def test_extract_features_returns_correct_shapes_128(self):\n    image_height = 128\n    image_width = 128\n    depth_multiplier = 1.0\n    pad_to_multiple = 1\n    expected_feature_map_shape = [(2, 13, 13, 288), (2, 6, 6, 768),\n                                  (2, 2, 2, 2048), (2, 1, 1, 512),\n                                  (2, 1, 1, 256), (2, 1, 1, 128)]\n    self.check_extract_features_returns_correct_shape(\n        2, image_height, image_width, depth_multiplier, pad_to_multiple,\n        expected_feature_map_shape)\n\n  def test_extract_features_returns_correct_shapes_with_dynamic_inputs(self):\n    image_height = 128\n    image_width = 128\n    depth_multiplier = 1.0\n    pad_to_multiple = 1\n    expected_feature_map_shape = [(2, 13, 13, 288), (2, 6, 6, 768),\n                                  (2, 2, 2, 2048), (2, 1, 1, 512),\n                                  (2, 1, 1, 256), (2, 1, 1, 128)]\n    self.check_extract_features_returns_correct_shapes_with_dynamic_inputs(\n        2, image_height, image_width, depth_multiplier, pad_to_multiple,\n        expected_feature_map_shape)\n\n  def test_extract_features_returns_correct_shapes_299(self):\n    image_height = 299\n    image_width = 299\n    depth_multiplier = 1.0\n    pad_to_multiple = 1\n    expected_feature_map_shape = [(2, 35, 35, 288), (2, 17, 17, 768),\n                                  (2, 8, 8, 2048), (2, 4, 4, 512),\n                                  (2, 2, 2, 256), (2, 1, 1, 128)]\n    self.check_extract_features_returns_correct_shape(\n        2, image_height, image_width, depth_multiplier, pad_to_multiple,\n        expected_feature_map_shape)\n\n  def test_extract_features_returns_correct_shapes_enforcing_min_depth(self):\n    image_height = 299\n    image_width = 299\n    depth_multiplier = 0.5**12\n    pad_to_multiple = 1\n    expected_feature_map_shape = [(2, 35, 35, 128), (2, 17, 17, 128),\n                                  (2, 8, 8, 192), (2, 4, 4, 32),\n                                  (2, 2, 2, 32), (2, 1, 1, 32)]\n    self.check_extract_features_returns_correct_shape(\n        2, image_height, image_width, depth_multiplier, pad_to_multiple,\n        expected_feature_map_shape)\n\n  def test_extract_features_returns_correct_shapes_with_pad_to_multiple(self):\n    image_height = 299\n    image_width = 299\n    depth_multiplier = 1.0\n    pad_to_multiple = 32\n    expected_feature_map_shape = [(2, 37, 37, 288), (2, 18, 18, 768),\n                                  (2, 8, 8, 2048), (2, 4, 4, 512),\n                                  (2, 2, 2, 256), (2, 1, 1, 128)]\n    self.check_extract_features_returns_correct_shape(\n        2, image_height, image_width, depth_multiplier, pad_to_multiple,\n        expected_feature_map_shape)\n\n  def test_extract_features_raises_error_with_invalid_image_size(self):\n    image_height = 32\n    image_width = 32\n    depth_multiplier = 1.0\n    pad_to_multiple = 1\n    self.check_extract_features_raises_error_with_invalid_image_size(\n        image_height, image_width, depth_multiplier, pad_to_multiple)\n\n  def test_preprocess_returns_correct_value_range(self):\n    image_height = 128\n    image_width = 128\n    depth_multiplier = 1\n    pad_to_multiple = 1\n    test_image = np.random.rand(4, image_height, image_width, 3)\n    feature_extractor = self._create_feature_extractor(depth_multiplier,\n                                                       pad_to_multiple)\n    preprocessed_image = feature_extractor.preprocess(test_image)\n    self.assertTrue(np.all(np.less_equal(np.abs(preprocessed_image), 1.0)))\n\n  def test_variables_only_created_in_scope(self):\n    depth_multiplier = 1\n    pad_to_multiple = 1\n    scope_name = \'InceptionV3\'\n    self.check_feature_extractor_variables_under_scope(\n        depth_multiplier, pad_to_multiple, scope_name)\n\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
src/object_detection/models/ssd_mobilenet_v1_feature_extractor.py,2,"b'# Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n""""""SSDFeatureExtractor for MobilenetV1 features.""""""\n\nimport tensorflow as tf\n\nfrom object_detection.meta_architectures import ssd_meta_arch\nfrom object_detection.models import feature_map_generators\nfrom object_detection.utils import ops\nfrom object_detection.utils import shape_utils\nfrom nets import mobilenet_v1\n\nslim = tf.contrib.slim\n\n\nclass SSDMobileNetV1FeatureExtractor(ssd_meta_arch.SSDFeatureExtractor):\n  """"""SSD Feature Extractor using MobilenetV1 features.""""""\n\n  def __init__(self,\n               is_training,\n               depth_multiplier,\n               min_depth,\n               pad_to_multiple,\n               conv_hyperparams,\n               batch_norm_trainable=True,\n               reuse_weights=None,\n               use_explicit_padding=False,\n               use_depthwise=False):\n    """"""MobileNetV1 Feature Extractor for SSD Models.\n\n    Args:\n      is_training: whether the network is in training mode.\n      depth_multiplier: float depth multiplier for feature extractor.\n      min_depth: minimum feature extractor depth.\n      pad_to_multiple: the nearest multiple to zero pad the input height and\n        width dimensions to.\n      conv_hyperparams: tf slim arg_scope for conv2d and separable_conv2d ops.\n      batch_norm_trainable: Whether to update batch norm parameters during\n        training or not. When training with a small batch size\n        (e.g. 1), it is desirable to disable batch norm update and use\n        pretrained batch norm params.\n      reuse_weights: Whether to reuse variables. Default is None.\n      use_explicit_padding: Use \'VALID\' padding for convolutions, but prepad\n        inputs so that the output dimensions are the same as if \'SAME\' padding\n        were used.\n      use_depthwise: Whether to use depthwise convolutions. Default is False.\n    """"""\n    super(SSDMobileNetV1FeatureExtractor, self).__init__(\n        is_training, depth_multiplier, min_depth, pad_to_multiple,\n        conv_hyperparams, batch_norm_trainable, reuse_weights,\n        use_explicit_padding, use_depthwise)\n\n  def preprocess(self, resized_inputs):\n    """"""SSD preprocessing.\n\n    Maps pixel values to the range [-1, 1].\n\n    Args:\n      resized_inputs: a [batch, height, width, channels] float tensor\n        representing a batch of images.\n\n    Returns:\n      preprocessed_inputs: a [batch, height, width, channels] float tensor\n        representing a batch of images.\n    """"""\n    return (2.0 / 255.0) * resized_inputs - 1.0\n\n  def extract_features(self, preprocessed_inputs):\n    """"""Extract features from preprocessed inputs.\n\n    Args:\n      preprocessed_inputs: a [batch, height, width, channels] float tensor\n        representing a batch of images.\n\n    Returns:\n      feature_maps: a list of tensors where the ith tensor has shape\n        [batch, height_i, width_i, depth_i]\n    """"""\n    preprocessed_inputs = shape_utils.check_min_image_dim(\n        33, preprocessed_inputs)\n\n    feature_map_layout = {\n        \'from_layer\': [\'Conv2d_11_pointwise\', \'Conv2d_13_pointwise\', \'\', \'\',\n                       \'\', \'\'],\n        \'layer_depth\': [-1, -1, 512, 256, 256, 128],\n        \'use_explicit_padding\': self._use_explicit_padding,\n        \'use_depthwise\': self._use_depthwise,\n    }\n\n    with tf.variable_scope(\'MobilenetV1\',\n                           reuse=self._reuse_weights) as scope:\n      with slim.arg_scope(\n          mobilenet_v1.mobilenet_v1_arg_scope(\n              is_training=(self._batch_norm_trainable and self._is_training))):\n        # TODO(skligys): Enable fused batch norm once quantization supports it.\n        with slim.arg_scope([slim.batch_norm], fused=False):\n          _, image_features = mobilenet_v1.mobilenet_v1_base(\n              ops.pad_to_multiple(preprocessed_inputs, self._pad_to_multiple),\n              final_endpoint=\'Conv2d_13_pointwise\',\n              min_depth=self._min_depth,\n              depth_multiplier=self._depth_multiplier,\n              use_explicit_padding=self._use_explicit_padding,\n              scope=scope)\n      with slim.arg_scope(self._conv_hyperparams):\n        # TODO(skligys): Enable fused batch norm once quantization supports it.\n        with slim.arg_scope([slim.batch_norm], fused=False):\n          feature_maps = feature_map_generators.multi_resolution_feature_maps(\n              feature_map_layout=feature_map_layout,\n              depth_multiplier=self._depth_multiplier,\n              min_depth=self._min_depth,\n              insert_1x1_conv=True,\n              image_features=image_features)\n\n    return feature_maps.values()\n'"
src/object_detection/models/ssd_mobilenet_v1_feature_extractor_test.py,4,"b'# Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n""""""Tests for ssd_mobilenet_v1_feature_extractor.""""""\nimport numpy as np\nimport tensorflow as tf\n\nfrom object_detection.models import ssd_feature_extractor_test\nfrom object_detection.models import ssd_mobilenet_v1_feature_extractor\n\nslim = tf.contrib.slim\n\n\nclass SsdMobilenetV1FeatureExtractorTest(\n    ssd_feature_extractor_test.SsdFeatureExtractorTestBase):\n\n  def _create_feature_extractor(self, depth_multiplier, pad_to_multiple,\n                                is_training=True, batch_norm_trainable=True,\n                                use_explicit_padding=False):\n    """"""Constructs a new feature extractor.\n\n    Args:\n      depth_multiplier: float depth multiplier for feature extractor\n      pad_to_multiple: the nearest multiple to zero pad the input height and\n        width dimensions to.\n      is_training: whether the network is in training mode.\n      batch_norm_trainable: Whether to update batch norm parameters during\n        training or not.\n      use_explicit_padding: Use \'VALID\' padding for convolutions, but prepad\n        inputs so that the output dimensions are the same as if \'SAME\' padding\n        were used.\n    Returns:\n      an ssd_meta_arch.SSDFeatureExtractor object.\n    """"""\n    min_depth = 32\n    with slim.arg_scope([slim.conv2d], normalizer_fn=slim.batch_norm) as sc:\n      conv_hyperparams = sc\n    return ssd_mobilenet_v1_feature_extractor.SSDMobileNetV1FeatureExtractor(\n        is_training, depth_multiplier, min_depth, pad_to_multiple,\n        conv_hyperparams, batch_norm_trainable=batch_norm_trainable,\n        use_explicit_padding=use_explicit_padding)\n\n  def test_extract_features_returns_correct_shapes_128(self):\n    image_height = 128\n    image_width = 128\n    depth_multiplier = 1.0\n    pad_to_multiple = 1\n    expected_feature_map_shape = [(2, 8, 8, 512), (2, 4, 4, 1024),\n                                  (2, 2, 2, 512), (2, 1, 1, 256),\n                                  (2, 1, 1, 256), (2, 1, 1, 128)]\n    self.check_extract_features_returns_correct_shape(\n        2, image_height, image_width, depth_multiplier, pad_to_multiple,\n        expected_feature_map_shape, use_explicit_padding=False)\n    self.check_extract_features_returns_correct_shape(\n        2, image_height, image_width, depth_multiplier, pad_to_multiple,\n        expected_feature_map_shape, use_explicit_padding=True)\n\n  def test_extract_features_returns_correct_shapes_299(self):\n    image_height = 299\n    image_width = 299\n    depth_multiplier = 1.0\n    pad_to_multiple = 1\n    expected_feature_map_shape = [(2, 19, 19, 512), (2, 10, 10, 1024),\n                                  (2, 5, 5, 512), (2, 3, 3, 256),\n                                  (2, 2, 2, 256), (2, 1, 1, 128)]\n    self.check_extract_features_returns_correct_shape(\n        2, image_height, image_width, depth_multiplier, pad_to_multiple,\n        expected_feature_map_shape, use_explicit_padding=False)\n    self.check_extract_features_returns_correct_shape(\n        2, image_height, image_width, depth_multiplier, pad_to_multiple,\n        expected_feature_map_shape, use_explicit_padding=True)\n\n  def test_extract_features_with_dynamic_image_shape(self):\n    image_height = 128\n    image_width = 128\n    depth_multiplier = 1.0\n    pad_to_multiple = 1\n    expected_feature_map_shape = [(2, 8, 8, 512), (2, 4, 4, 1024),\n                                  (2, 2, 2, 512), (2, 1, 1, 256),\n                                  (2, 1, 1, 256), (2, 1, 1, 128)]\n    self.check_extract_features_returns_correct_shapes_with_dynamic_inputs(\n        2, image_height, image_width, depth_multiplier, pad_to_multiple,\n        expected_feature_map_shape, use_explicit_padding=False)\n    self.check_extract_features_returns_correct_shape(\n        2, image_height, image_width, depth_multiplier, pad_to_multiple,\n        expected_feature_map_shape, use_explicit_padding=True)\n\n  def test_extract_features_returns_correct_shapes_enforcing_min_depth(self):\n    image_height = 299\n    image_width = 299\n    depth_multiplier = 0.5**12\n    pad_to_multiple = 1\n    expected_feature_map_shape = [(2, 19, 19, 32), (2, 10, 10, 32),\n                                  (2, 5, 5, 32), (2, 3, 3, 32),\n                                  (2, 2, 2, 32), (2, 1, 1, 32)]\n    self.check_extract_features_returns_correct_shape(\n        2, image_height, image_width, depth_multiplier, pad_to_multiple,\n        expected_feature_map_shape, use_explicit_padding=False)\n    self.check_extract_features_returns_correct_shape(\n        2, image_height, image_width, depth_multiplier, pad_to_multiple,\n        expected_feature_map_shape, use_explicit_padding=True)\n\n  def test_extract_features_returns_correct_shapes_with_pad_to_multiple(self):\n    image_height = 299\n    image_width = 299\n    depth_multiplier = 1.0\n    pad_to_multiple = 32\n    expected_feature_map_shape = [(2, 20, 20, 512), (2, 10, 10, 1024),\n                                  (2, 5, 5, 512), (2, 3, 3, 256),\n                                  (2, 2, 2, 256), (2, 1, 1, 128)]\n    self.check_extract_features_returns_correct_shape(\n        2, image_height, image_width, depth_multiplier, pad_to_multiple,\n        expected_feature_map_shape, use_explicit_padding=False)\n    self.check_extract_features_returns_correct_shape(\n        2, image_height, image_width, depth_multiplier, pad_to_multiple,\n        expected_feature_map_shape, use_explicit_padding=True)\n\n  def test_extract_features_raises_error_with_invalid_image_size(self):\n    image_height = 32\n    image_width = 32\n    depth_multiplier = 1.0\n    pad_to_multiple = 1\n    self.check_extract_features_raises_error_with_invalid_image_size(\n        image_height, image_width, depth_multiplier, pad_to_multiple)\n\n  def test_preprocess_returns_correct_value_range(self):\n    image_height = 128\n    image_width = 128\n    depth_multiplier = 1\n    pad_to_multiple = 1\n    test_image = np.random.rand(2, image_height, image_width, 3)\n    feature_extractor = self._create_feature_extractor(depth_multiplier,\n                                                       pad_to_multiple)\n    preprocessed_image = feature_extractor.preprocess(test_image)\n    self.assertTrue(np.all(np.less_equal(np.abs(preprocessed_image), 1.0)))\n\n  def test_variables_only_created_in_scope(self):\n    depth_multiplier = 1\n    pad_to_multiple = 1\n    scope_name = \'MobilenetV1\'\n    self.check_feature_extractor_variables_under_scope(\n        depth_multiplier, pad_to_multiple, scope_name)\n\n  def test_nofused_batchnorm(self):\n    image_height = 40\n    image_width = 40\n    depth_multiplier = 1\n    pad_to_multiple = 1\n    image_placeholder = tf.placeholder(tf.float32,\n                                       [1, image_height, image_width, 3])\n    feature_extractor = self._create_feature_extractor(depth_multiplier,\n                                                       pad_to_multiple)\n    preprocessed_image = feature_extractor.preprocess(image_placeholder)\n    _ = feature_extractor.extract_features(preprocessed_image)\n    self.assertFalse(any(op.type == \'FusedBatchNorm\'\n                         for op in tf.get_default_graph().get_operations()))\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
src/object_detection/models/ssd_mobilenet_v2_feature_extractor.py,2,"b'# Copyright 2018 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n""""""SSDFeatureExtractor for MobilenetV2 features.""""""\n\nimport tensorflow as tf\n\nfrom object_detection.meta_architectures import ssd_meta_arch\nfrom object_detection.models import feature_map_generators\nfrom object_detection.utils import ops\nfrom object_detection.utils import shape_utils\nfrom nets.mobilenet import mobilenet\nfrom nets.mobilenet import mobilenet_v2\n\nslim = tf.contrib.slim\n\n\nclass SSDMobileNetV2FeatureExtractor(ssd_meta_arch.SSDFeatureExtractor):\n  """"""SSD Feature Extractor using MobilenetV2 features.""""""\n\n  def __init__(self,\n               is_training,\n               depth_multiplier,\n               min_depth,\n               pad_to_multiple,\n               conv_hyperparams,\n               batch_norm_trainable=True,\n               reuse_weights=None,\n               use_explicit_padding=False,\n               use_depthwise=False):\n    """"""MobileNetV2 Feature Extractor for SSD Models.\n\n    Mobilenet v2 (experimental), designed by sandler@. More details can be found\n    in //knowledge/cerebra/brain/compression/mobilenet/mobilenet_experimental.py\n\n    Args:\n      is_training: whether the network is in training mode.\n      depth_multiplier: float depth multiplier for feature extractor.\n      min_depth: minimum feature extractor depth.\n      pad_to_multiple: the nearest multiple to zero pad the input height and\n        width dimensions to.\n      conv_hyperparams: tf slim arg_scope for conv2d and separable_conv2d ops.\n      batch_norm_trainable:  Whether to update batch norm parameters during\n        training or not. When training with a small batch size\n        (e.g. 1), it is desirable to disable batch norm update and use\n        pretrained batch norm params.\n      reuse_weights: Whether to reuse variables. Default is None.\n      use_explicit_padding: Whether to use explicit padding when extracting\n        features. Default is False.\n      use_depthwise: Whether to use depthwise convolutions. Default is False.\n    """"""\n    super(SSDMobileNetV2FeatureExtractor, self).__init__(\n        is_training, depth_multiplier, min_depth, pad_to_multiple,\n        conv_hyperparams, batch_norm_trainable, reuse_weights,\n        use_explicit_padding, use_depthwise)\n\n  def preprocess(self, resized_inputs):\n    """"""SSD preprocessing.\n\n    Maps pixel values to the range [-1, 1].\n\n    Args:\n      resized_inputs: a [batch, height, width, channels] float tensor\n        representing a batch of images.\n\n    Returns:\n      preprocessed_inputs: a [batch, height, width, channels] float tensor\n        representing a batch of images.\n    """"""\n    return (2.0 / 255.0) * resized_inputs - 1.0\n\n  def extract_features(self, preprocessed_inputs):\n    """"""Extract features from preprocessed inputs.\n\n    Args:\n      preprocessed_inputs: a [batch, height, width, channels] float tensor\n        representing a batch of images.\n\n    Returns:\n      feature_maps: a list of tensors where the ith tensor has shape\n        [batch, height_i, width_i, depth_i]\n    """"""\n    preprocessed_inputs = shape_utils.check_min_image_dim(\n        33, preprocessed_inputs)\n\n    feature_map_layout = {\n        \'from_layer\': [\'layer_15/expansion_output\', \'layer_19\', \'\', \'\', \'\', \'\'],\n        \'layer_depth\': [-1, -1, 512, 256, 256, 128],\n        \'use_depthwise\': self._use_depthwise,\n        \'use_explicit_padding\': self._use_explicit_padding,\n    }\n\n    with tf.variable_scope(\'MobilenetV2\', reuse=self._reuse_weights) as scope:\n      with slim.arg_scope(\n          mobilenet_v2.training_scope(\n              is_training=(self._is_training and self._batch_norm_trainable),\n              bn_decay=0.9997)), \\\n          slim.arg_scope(\n              [mobilenet.depth_multiplier], min_depth=self._min_depth):\n        # TODO(b/68150321): Enable fused batch norm once quantization\n        # supports it.\n        with slim.arg_scope([slim.batch_norm], fused=False):\n          _, image_features = mobilenet_v2.mobilenet_base(\n              ops.pad_to_multiple(preprocessed_inputs, self._pad_to_multiple),\n              final_endpoint=\'layer_19\',\n              depth_multiplier=self._depth_multiplier,\n              use_explicit_padding=self._use_explicit_padding,\n              scope=scope)\n        with slim.arg_scope(self._conv_hyperparams):\n          # TODO(b/68150321): Enable fused batch norm once quantization\n          # supports it.\n          with slim.arg_scope([slim.batch_norm], fused=False):\n            feature_maps = feature_map_generators.multi_resolution_feature_maps(\n                feature_map_layout=feature_map_layout,\n                depth_multiplier=self._depth_multiplier,\n                min_depth=self._min_depth,\n                insert_1x1_conv=True,\n                image_features=image_features)\n\n    return feature_maps.values()\n'"
src/object_detection/models/ssd_mobilenet_v2_feature_extractor_test.py,4,"b'# Copyright 2018 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n""""""Tests for ssd_mobilenet_v2_feature_extractor.""""""\nimport numpy as np\nimport tensorflow as tf\n\nfrom object_detection.models import ssd_feature_extractor_test\nfrom object_detection.models import ssd_mobilenet_v2_feature_extractor\n\nslim = tf.contrib.slim\n\n\nclass SsdMobilenetV2FeatureExtractorTest(\n    ssd_feature_extractor_test.SsdFeatureExtractorTestBase):\n\n  def _create_feature_extractor(self, depth_multiplier, pad_to_multiple,\n                                use_explicit_padding=False):\n    """"""Constructs a new feature extractor.\n\n    Args:\n      depth_multiplier: float depth multiplier for feature extractor\n      pad_to_multiple: the nearest multiple to zero pad the input height and\n        width dimensions to.\n      use_explicit_padding: use \'VALID\' padding for convolutions, but prepad\n        inputs so that the output dimensions are the same as if \'SAME\' padding\n        were used.\n    Returns:\n      an ssd_meta_arch.SSDFeatureExtractor object.\n    """"""\n    min_depth = 32\n    with slim.arg_scope([slim.conv2d], normalizer_fn=slim.batch_norm) as sc:\n      conv_hyperparams = sc\n    return ssd_mobilenet_v2_feature_extractor.SSDMobileNetV2FeatureExtractor(\n        False,\n        depth_multiplier,\n        min_depth,\n        pad_to_multiple,\n        conv_hyperparams,\n        use_explicit_padding=use_explicit_padding)\n\n  def test_extract_features_returns_correct_shapes_128(self):\n    image_height = 128\n    image_width = 128\n    depth_multiplier = 1.0\n    pad_to_multiple = 1\n    expected_feature_map_shape = [(2, 8, 8, 576), (2, 4, 4, 1280),\n                                  (2, 2, 2, 512), (2, 1, 1, 256),\n                                  (2, 1, 1, 256), (2, 1, 1, 128)]\n    self.check_extract_features_returns_correct_shape(\n        2, image_height, image_width, depth_multiplier, pad_to_multiple,\n        expected_feature_map_shape)\n\n  def test_extract_features_returns_correct_shapes_with_dynamic_inputs(self):\n    image_height = 128\n    image_width = 128\n    depth_multiplier = 1.0\n    pad_to_multiple = 1\n    expected_feature_map_shape = [(2, 8, 8, 576), (2, 4, 4, 1280),\n                                  (2, 2, 2, 512), (2, 1, 1, 256),\n                                  (2, 1, 1, 256), (2, 1, 1, 128)]\n    self.check_extract_features_returns_correct_shapes_with_dynamic_inputs(\n        2, image_height, image_width, depth_multiplier, pad_to_multiple,\n        expected_feature_map_shape)\n\n  def test_extract_features_returns_correct_shapes_299(self):\n    image_height = 299\n    image_width = 299\n    depth_multiplier = 1.0\n    pad_to_multiple = 1\n    expected_feature_map_shape = [(2, 19, 19, 576), (2, 10, 10, 1280),\n                                  (2, 5, 5, 512), (2, 3, 3, 256),\n                                  (2, 2, 2, 256), (2, 1, 1, 128)]\n    self.check_extract_features_returns_correct_shape(\n        2, image_height, image_width, depth_multiplier, pad_to_multiple,\n        expected_feature_map_shape)\n\n  def test_extract_features_returns_correct_shapes_enforcing_min_depth(self):\n    image_height = 299\n    image_width = 299\n    depth_multiplier = 0.5**12\n    pad_to_multiple = 1\n    expected_feature_map_shape = [(2, 19, 19, 192), (2, 10, 10, 32),\n                                  (2, 5, 5, 32), (2, 3, 3, 32),\n                                  (2, 2, 2, 32), (2, 1, 1, 32)]\n    self.check_extract_features_returns_correct_shape(\n        2, image_height, image_width, depth_multiplier, pad_to_multiple,\n        expected_feature_map_shape)\n\n  def test_extract_features_returns_correct_shapes_with_pad_to_multiple(self):\n    image_height = 299\n    image_width = 299\n    depth_multiplier = 1.0\n    pad_to_multiple = 32\n    expected_feature_map_shape = [(2, 20, 20, 576), (2, 10, 10, 1280),\n                                  (2, 5, 5, 512), (2, 3, 3, 256),\n                                  (2, 2, 2, 256), (2, 1, 1, 128)]\n    self.check_extract_features_returns_correct_shape(\n        2, image_height, image_width, depth_multiplier, pad_to_multiple,\n        expected_feature_map_shape)\n\n  def test_extract_features_raises_error_with_invalid_image_size(self):\n    image_height = 32\n    image_width = 32\n    depth_multiplier = 1.0\n    pad_to_multiple = 1\n    self.check_extract_features_raises_error_with_invalid_image_size(\n        image_height, image_width, depth_multiplier, pad_to_multiple)\n\n  def test_preprocess_returns_correct_value_range(self):\n    image_height = 128\n    image_width = 128\n    depth_multiplier = 1\n    pad_to_multiple = 1\n    test_image = np.random.rand(4, image_height, image_width, 3)\n    feature_extractor = self._create_feature_extractor(depth_multiplier,\n                                                       pad_to_multiple)\n    preprocessed_image = feature_extractor.preprocess(test_image)\n    self.assertTrue(np.all(np.less_equal(np.abs(preprocessed_image), 1.0)))\n\n  def test_variables_only_created_in_scope(self):\n    depth_multiplier = 1\n    pad_to_multiple = 1\n    scope_name = \'MobilenetV2\'\n    self.check_feature_extractor_variables_under_scope(\n        depth_multiplier, pad_to_multiple, scope_name)\n\n  def test_nofused_batchnorm(self):\n    image_height = 40\n    image_width = 40\n    depth_multiplier = 1\n    pad_to_multiple = 1\n    image_placeholder = tf.placeholder(tf.float32,\n                                       [1, image_height, image_width, 3])\n    feature_extractor = self._create_feature_extractor(depth_multiplier,\n                                                       pad_to_multiple)\n    preprocessed_image = feature_extractor.preprocess(image_placeholder)\n    _ = feature_extractor.extract_features(preprocessed_image)\n    self.assertFalse(any(op.type == \'FusedBatchNorm\'\n                         for op in tf.get_default_graph().get_operations()))\n\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
src/object_detection/models/ssd_resnet_v1_fpn_feature_extractor.py,3,"b'# Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""SSD Feature Pyramid Network (FPN) feature extractors based on Resnet v1.\n\nSee https://arxiv.org/abs/1708.02002 for details.\n""""""\n\nimport tensorflow as tf\n\nfrom object_detection.meta_architectures import ssd_meta_arch\nfrom object_detection.models import feature_map_generators\nfrom object_detection.utils import ops\nfrom object_detection.utils import shape_utils\nfrom nets import resnet_v1\n\nslim = tf.contrib.slim\n\n\nclass _SSDResnetV1FpnFeatureExtractor(ssd_meta_arch.SSDFeatureExtractor):\n  """"""SSD FPN feature extractor based on Resnet v1 architecture.""""""\n\n  def __init__(self,\n               is_training,\n               depth_multiplier,\n               min_depth,\n               pad_to_multiple,\n               conv_hyperparams,\n               resnet_base_fn,\n               resnet_scope_name,\n               fpn_scope_name,\n               batch_norm_trainable=True,\n               reuse_weights=None,\n               use_explicit_padding=False,\n               use_depthwise=False):\n    """"""SSD FPN feature extractor based on Resnet v1 architecture.\n\n    Args:\n      is_training: whether the network is in training mode.\n      depth_multiplier: float depth multiplier for feature extractor.\n        UNUSED currently.\n      min_depth: minimum feature extractor depth. UNUSED Currently.\n      pad_to_multiple: the nearest multiple to zero pad the input height and\n        width dimensions to.\n      conv_hyperparams: tf slim arg_scope for conv2d and separable_conv2d ops.\n      resnet_base_fn: base resnet network to use.\n      resnet_scope_name: scope name under which to construct resnet\n      fpn_scope_name: scope name under which to construct the feature pyramid\n        network.\n      batch_norm_trainable: Whether to update batch norm parameters during\n        training or not. When training with a small batch size\n        (e.g. 1), it is desirable to disable batch norm update and use\n        pretrained batch norm params.\n      reuse_weights: Whether to reuse variables. Default is None.\n      use_explicit_padding: Whether to use explicit padding when extracting\n        features. Default is False. UNUSED currently.\n      use_depthwise: Whether to use depthwise convolutions. UNUSED currently.\n\n    Raises:\n      ValueError: On supplying invalid arguments for unused arguments.\n    """"""\n    super(_SSDResnetV1FpnFeatureExtractor, self).__init__(\n        is_training, depth_multiplier, min_depth, pad_to_multiple,\n        conv_hyperparams, batch_norm_trainable, reuse_weights,\n        use_explicit_padding)\n    if self._depth_multiplier != 1.0:\n      raise ValueError(\'Only depth 1.0 is supported, found: {}\'.\n                       format(self._depth_multiplier))\n    if self._use_explicit_padding is True:\n      raise ValueError(\'Explicit padding is not a valid option.\')\n    self._resnet_base_fn = resnet_base_fn\n    self._resnet_scope_name = resnet_scope_name\n    self._fpn_scope_name = fpn_scope_name\n\n  def preprocess(self, resized_inputs):\n    """"""SSD preprocessing.\n\n    VGG style channel mean subtraction as described here:\n    https://gist.github.com/ksimonyan/211839e770f7b538e2d8#file-readme-mdnge.\n\n    Args:\n      resized_inputs: a [batch, height, width, channels] float tensor\n        representing a batch of images.\n\n    Returns:\n      preprocessed_inputs: a [batch, height, width, channels] float tensor\n        representing a batch of images.\n    """"""\n    channel_means = [123.68, 116.779, 103.939]\n    return resized_inputs - [[channel_means]]\n\n  def _filter_features(self, image_features):\n    # TODO(rathodv): Change resnet endpoint to strip scope prefixes instead\n    # of munging the scope here.\n    filtered_image_features = dict({})\n    for key, feature in image_features.items():\n      feature_name = key.split(\'/\')[-1]\n      if feature_name in [\'block2\', \'block3\', \'block4\']:\n        filtered_image_features[feature_name] = feature\n    return filtered_image_features\n\n  def extract_features(self, preprocessed_inputs):\n    """"""Extract features from preprocessed inputs.\n\n    Args:\n      preprocessed_inputs: a [batch, height, width, channels] float tensor\n        representing a batch of images.\n\n    Returns:\n      feature_maps: a list of tensors where the ith tensor has shape\n        [batch, height_i, width_i, depth_i]\n\n    Raises:\n      ValueError: depth multiplier is not supported.\n    """"""\n    if self._depth_multiplier != 1.0:\n      raise ValueError(\'Depth multiplier not supported.\')\n\n    preprocessed_inputs = shape_utils.check_min_image_dim(\n        129, preprocessed_inputs)\n\n    with tf.variable_scope(\n        self._resnet_scope_name, reuse=self._reuse_weights) as scope:\n      with slim.arg_scope(resnet_v1.resnet_arg_scope()):\n        _, image_features = self._resnet_base_fn(\n            inputs=ops.pad_to_multiple(preprocessed_inputs,\n                                       self._pad_to_multiple),\n            num_classes=None,\n            is_training=self._is_training and self._batch_norm_trainable,\n            global_pool=False,\n            output_stride=None,\n            store_non_strided_activations=True,\n            scope=scope)\n      image_features = self._filter_features(image_features)\n      last_feature_map = image_features[\'block4\']\n    with tf.variable_scope(self._fpn_scope_name, reuse=self._reuse_weights):\n      with slim.arg_scope(self._conv_hyperparams):\n        for i in range(5, 7):\n          last_feature_map = slim.conv2d(\n              last_feature_map,\n              num_outputs=256,\n              kernel_size=[3, 3],\n              stride=2,\n              padding=\'SAME\',\n              scope=\'block{}\'.format(i))\n          image_features[\'bottomup_{}\'.format(i)] = last_feature_map\n        feature_maps = feature_map_generators.fpn_top_down_feature_maps(\n            [\n                image_features[key] for key in\n                [\'block2\', \'block3\', \'block4\', \'bottomup_5\', \'bottomup_6\']\n            ],\n            depth=256,\n            scope=\'top_down_features\')\n    return feature_maps.values()\n\n\nclass SSDResnet50V1FpnFeatureExtractor(_SSDResnetV1FpnFeatureExtractor):\n\n  def __init__(self,\n               is_training,\n               depth_multiplier,\n               min_depth,\n               pad_to_multiple,\n               conv_hyperparams,\n               batch_norm_trainable=True,\n               reuse_weights=None,\n               use_explicit_padding=False,\n               use_depthwise=False):\n    """"""Resnet50 v1 FPN Feature Extractor for SSD Models.\n\n    Args:\n      is_training: whether the network is in training mode.\n      depth_multiplier: float depth multiplier for feature extractor.\n      min_depth: minimum feature extractor depth.\n      pad_to_multiple: the nearest multiple to zero pad the input height and\n        width dimensions to.\n      conv_hyperparams: tf slim arg_scope for conv2d and separable_conv2d ops.\n      batch_norm_trainable: Whether to update batch norm parameters during\n        training or not. When training with a small batch size\n        (e.g. 1), it is desirable to disable batch norm update and use\n        pretrained batch norm params.\n      reuse_weights: Whether to reuse variables. Default is None.\n      use_explicit_padding: Whether to use explicit padding when extracting\n        features. Default is False. UNUSED currently.\n      use_depthwise: Whether to use depthwise convolutions. UNUSED currently.\n    """"""\n    super(SSDResnet50V1FpnFeatureExtractor, self).__init__(\n        is_training, depth_multiplier, min_depth, pad_to_multiple,\n        conv_hyperparams, resnet_v1.resnet_v1_50, \'resnet_v1_50\', \'fpn\',\n        batch_norm_trainable, reuse_weights, use_explicit_padding)\n\n\nclass SSDResnet101V1FpnFeatureExtractor(_SSDResnetV1FpnFeatureExtractor):\n\n  def __init__(self,\n               is_training,\n               depth_multiplier,\n               min_depth,\n               pad_to_multiple,\n               conv_hyperparams,\n               batch_norm_trainable=True,\n               reuse_weights=None,\n               use_explicit_padding=False,\n               use_depthwise=False):\n    """"""Resnet101 v1 FPN Feature Extractor for SSD Models.\n\n    Args:\n      is_training: whether the network is in training mode.\n      depth_multiplier: float depth multiplier for feature extractor.\n      min_depth: minimum feature extractor depth.\n      pad_to_multiple: the nearest multiple to zero pad the input height and\n        width dimensions to.\n      conv_hyperparams: tf slim arg_scope for conv2d and separable_conv2d ops.\n      batch_norm_trainable: Whether to update batch norm parameters during\n        training or not. When training with a small batch size\n        (e.g. 1), it is desirable to disable batch norm update and use\n        pretrained batch norm params.\n      reuse_weights: Whether to reuse variables. Default is None.\n      use_explicit_padding: Whether to use explicit padding when extracting\n        features. Default is False. UNUSED currently.\n      use_depthwise: Whether to use depthwise convolutions. UNUSED currently.\n    """"""\n    super(SSDResnet101V1FpnFeatureExtractor, self).__init__(\n        is_training, depth_multiplier, min_depth, pad_to_multiple,\n        conv_hyperparams, resnet_v1.resnet_v1_101, \'resnet_v1_101\', \'fpn\',\n        batch_norm_trainable, reuse_weights, use_explicit_padding)\n\n\nclass SSDResnet152V1FpnFeatureExtractor(_SSDResnetV1FpnFeatureExtractor):\n\n  def __init__(self,\n               is_training,\n               depth_multiplier,\n               min_depth,\n               pad_to_multiple,\n               conv_hyperparams,\n               batch_norm_trainable=True,\n               reuse_weights=None,\n               use_explicit_padding=False,\n               use_depthwise=False):\n    """"""Resnet152 v1 FPN Feature Extractor for SSD Models.\n\n    Args:\n      is_training: whether the network is in training mode.\n      depth_multiplier: float depth multiplier for feature extractor.\n      min_depth: minimum feature extractor depth.\n      pad_to_multiple: the nearest multiple to zero pad the input height and\n        width dimensions to.\n      conv_hyperparams: tf slim arg_scope for conv2d and separable_conv2d ops.\n      batch_norm_trainable: Whether to update batch norm parameters during\n        training or not. When training with a small batch size\n        (e.g. 1), it is desirable to disable batch norm update and use\n        pretrained batch norm params.\n      reuse_weights: Whether to reuse variables. Default is None.\n      use_explicit_padding: Whether to use explicit padding when extracting\n        features. Default is False. UNUSED currently.\n      use_depthwise: Whether to use depthwise convolutions. UNUSED currently.\n    """"""\n    super(SSDResnet152V1FpnFeatureExtractor, self).__init__(\n        is_training, depth_multiplier, min_depth, pad_to_multiple,\n        conv_hyperparams, resnet_v1.resnet_v1_152, \'resnet_v1_152\', \'fpn\',\n        batch_norm_trainable, reuse_weights, use_explicit_padding)\n'"
src/object_detection/models/ssd_resnet_v1_fpn_feature_extractor_test.py,1,"b'# Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Tests for ssd resnet v1 FPN feature extractors.""""""\nimport tensorflow as tf\n\nfrom object_detection.models import ssd_resnet_v1_fpn_feature_extractor\nfrom object_detection.models import ssd_resnet_v1_fpn_feature_extractor_testbase\n\n\nclass SSDResnet50V1FeatureExtractorTest(\n    ssd_resnet_v1_fpn_feature_extractor_testbase.\n    SSDResnetFPNFeatureExtractorTestBase):\n  """"""SSDResnet50v1Fpn feature extractor test.""""""\n\n  def _create_feature_extractor(self, depth_multiplier, pad_to_multiple,\n                                use_explicit_padding=False):\n    min_depth = 32\n    conv_hyperparams = {}\n    batch_norm_trainable = True\n    is_training = True\n    return ssd_resnet_v1_fpn_feature_extractor.SSDResnet50V1FpnFeatureExtractor(\n        is_training, depth_multiplier, min_depth, pad_to_multiple,\n        conv_hyperparams, batch_norm_trainable,\n        use_explicit_padding=use_explicit_padding)\n\n  def _resnet_scope_name(self):\n    return \'resnet_v1_50\'\n\n\nclass SSDResnet101V1FeatureExtractorTest(\n    ssd_resnet_v1_fpn_feature_extractor_testbase.\n    SSDResnetFPNFeatureExtractorTestBase):\n  """"""SSDResnet101v1Fpn feature extractor test.""""""\n\n  def _create_feature_extractor(self, depth_multiplier, pad_to_multiple,\n                                use_explicit_padding=False):\n    min_depth = 32\n    conv_hyperparams = {}\n    batch_norm_trainable = True\n    is_training = True\n    return (\n        ssd_resnet_v1_fpn_feature_extractor.SSDResnet101V1FpnFeatureExtractor(\n            is_training, depth_multiplier, min_depth, pad_to_multiple,\n            conv_hyperparams, batch_norm_trainable,\n            use_explicit_padding=use_explicit_padding))\n\n  def _resnet_scope_name(self):\n    return \'resnet_v1_101\'\n\n\nclass SSDResnet152V1FeatureExtractorTest(\n    ssd_resnet_v1_fpn_feature_extractor_testbase.\n    SSDResnetFPNFeatureExtractorTestBase):\n  """"""SSDResnet152v1Fpn feature extractor test.""""""\n\n  def _create_feature_extractor(self, depth_multiplier, pad_to_multiple,\n                                use_explicit_padding=False):\n    min_depth = 32\n    conv_hyperparams = {}\n    batch_norm_trainable = True\n    is_training = True\n    return (\n        ssd_resnet_v1_fpn_feature_extractor.SSDResnet152V1FpnFeatureExtractor(\n            is_training, depth_multiplier, min_depth, pad_to_multiple,\n            conv_hyperparams, batch_norm_trainable,\n            use_explicit_padding=use_explicit_padding))\n\n  def _resnet_scope_name(self):\n    return \'resnet_v1_152\'\n\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
src/object_detection/models/ssd_resnet_v1_fpn_feature_extractor_testbase.py,3,"b'# Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Tests for ssd resnet v1 FPN feature extractors.""""""\nimport abc\nimport numpy as np\nimport tensorflow as tf\n\nfrom object_detection.models import ssd_feature_extractor_test\n\n\nclass SSDResnetFPNFeatureExtractorTestBase(\n    ssd_feature_extractor_test.SsdFeatureExtractorTestBase):\n  """"""Helper test class for SSD Resnet v1 FPN feature extractors.""""""\n\n  @abc.abstractmethod\n  def _resnet_scope_name(self):\n    pass\n\n  @abc.abstractmethod\n  def _fpn_scope_name(self):\n    return \'fpn\'\n\n  def test_extract_features_returns_correct_shapes_256(self):\n    image_height = 256\n    image_width = 256\n    depth_multiplier = 1.0\n    pad_to_multiple = 1\n    expected_feature_map_shape = [(2, 32, 32, 256), (2, 16, 16, 256),\n                                  (2, 8, 8, 256), (2, 4, 4, 256),\n                                  (2, 2, 2, 256)]\n    self.check_extract_features_returns_correct_shape(\n        2, image_height, image_width, depth_multiplier, pad_to_multiple,\n        expected_feature_map_shape)\n\n  def test_extract_features_returns_correct_shapes_with_dynamic_inputs(self):\n    image_height = 256\n    image_width = 256\n    depth_multiplier = 1.0\n    pad_to_multiple = 1\n    expected_feature_map_shape = [(2, 32, 32, 256), (2, 16, 16, 256),\n                                  (2, 8, 8, 256), (2, 4, 4, 256),\n                                  (2, 2, 2, 256)]\n    self.check_extract_features_returns_correct_shapes_with_dynamic_inputs(\n        2, image_height, image_width, depth_multiplier, pad_to_multiple,\n        expected_feature_map_shape)\n\n  def test_extract_features_returns_correct_shapes_with_pad_to_multiple(self):\n    image_height = 254\n    image_width = 254\n    depth_multiplier = 1.0\n    pad_to_multiple = 32\n    expected_feature_map_shape = [(2, 32, 32, 256), (2, 16, 16, 256),\n                                  (2, 8, 8, 256), (2, 4, 4, 256),\n                                  (2, 2, 2, 256)]\n\n    self.check_extract_features_returns_correct_shape(\n        2, image_height, image_width, depth_multiplier, pad_to_multiple,\n        expected_feature_map_shape)\n\n  def test_extract_features_raises_error_with_invalid_image_size(self):\n    image_height = 32\n    image_width = 32\n    depth_multiplier = 1.0\n    pad_to_multiple = 1\n    self.check_extract_features_raises_error_with_invalid_image_size(\n        image_height, image_width, depth_multiplier, pad_to_multiple)\n\n  def test_preprocess_returns_correct_value_range(self):\n    image_height = 128\n    image_width = 128\n    depth_multiplier = 1\n    pad_to_multiple = 1\n    test_image = np.random.rand(4, image_height, image_width, 3)\n    feature_extractor = self._create_feature_extractor(depth_multiplier,\n                                                       pad_to_multiple)\n    preprocessed_image = feature_extractor.preprocess(test_image)\n    self.assertAllClose(preprocessed_image,\n                        test_image - [[123.68, 116.779, 103.939]])\n\n  def test_variables_only_created_in_scope(self):\n    depth_multiplier = 1\n    pad_to_multiple = 1\n    g = tf.Graph()\n    with g.as_default():\n      feature_extractor = self._create_feature_extractor(\n          depth_multiplier, pad_to_multiple)\n      preprocessed_inputs = tf.placeholder(tf.float32, (4, None, None, 3))\n      feature_extractor.extract_features(preprocessed_inputs)\n      variables = g.get_collection(tf.GraphKeys.GLOBAL_VARIABLES)\n      for variable in variables:\n        self.assertTrue(\n            variable.name.startswith(self._resnet_scope_name())\n            or variable.name.startswith(self._fpn_scope_name()))\n\n\n'"
src/object_detection/protos/__init__.py,0,b''
src/object_detection/protos/anchor_generator_pb2.py,0,"b'# Generated by the protocol buffer compiler.  DO NOT EDIT!\n# source: object_detection/protos/anchor_generator.proto\n\nimport sys\n_b=sys.version_info[0]<3 and (lambda x:x) or (lambda x:x.encode(\'latin1\'))\nfrom google.protobuf import descriptor as _descriptor\nfrom google.protobuf import message as _message\nfrom google.protobuf import reflection as _reflection\nfrom google.protobuf import symbol_database as _symbol_database\nfrom google.protobuf import descriptor_pb2\n# @@protoc_insertion_point(imports)\n\n_sym_db = _symbol_database.Default()\n\n\nfrom object_detection.protos import grid_anchor_generator_pb2 as object__detection_dot_protos_dot_grid__anchor__generator__pb2\nfrom object_detection.protos import ssd_anchor_generator_pb2 as object__detection_dot_protos_dot_ssd__anchor__generator__pb2\nfrom object_detection.protos import multiscale_anchor_generator_pb2 as object__detection_dot_protos_dot_multiscale__anchor__generator__pb2\n\n\nDESCRIPTOR = _descriptor.FileDescriptor(\n  name=\'object_detection/protos/anchor_generator.proto\',\n  package=\'object_detection.protos\',\n  syntax=\'proto2\',\n  serialized_pb=_b(\'\\n.object_detection/protos/anchor_generator.proto\\x12\\x17object_detection.protos\\x1a\\x33object_detection/protos/grid_anchor_generator.proto\\x1a\\x32object_detection/protos/ssd_anchor_generator.proto\\x1a\\x39object_detection/protos/multiscale_anchor_generator.proto\\""\\xa2\\x02\\n\\x0f\\x41nchorGenerator\\x12M\\n\\x15grid_anchor_generator\\x18\\x01 \\x01(\\x0b\\x32,.object_detection.protos.GridAnchorGeneratorH\\x00\\x12K\\n\\x14ssd_anchor_generator\\x18\\x02 \\x01(\\x0b\\x32+.object_detection.protos.SsdAnchorGeneratorH\\x00\\x12Y\\n\\x1bmultiscale_anchor_generator\\x18\\x03 \\x01(\\x0b\\x32\\x32.object_detection.protos.MultiscaleAnchorGeneratorH\\x00\\x42\\x18\\n\\x16\\x61nchor_generator_oneof\')\n  ,\n  dependencies=[object__detection_dot_protos_dot_grid__anchor__generator__pb2.DESCRIPTOR,object__detection_dot_protos_dot_ssd__anchor__generator__pb2.DESCRIPTOR,object__detection_dot_protos_dot_multiscale__anchor__generator__pb2.DESCRIPTOR,])\n\n\n\n\n_ANCHORGENERATOR = _descriptor.Descriptor(\n  name=\'AnchorGenerator\',\n  full_name=\'object_detection.protos.AnchorGenerator\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'grid_anchor_generator\', full_name=\'object_detection.protos.AnchorGenerator.grid_anchor_generator\', index=0,\n      number=1, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'ssd_anchor_generator\', full_name=\'object_detection.protos.AnchorGenerator.ssd_anchor_generator\', index=1,\n      number=2, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'multiscale_anchor_generator\', full_name=\'object_detection.protos.AnchorGenerator.multiscale_anchor_generator\', index=2,\n      number=3, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto2\',\n  extension_ranges=[],\n  oneofs=[\n    _descriptor.OneofDescriptor(\n      name=\'anchor_generator_oneof\', full_name=\'object_detection.protos.AnchorGenerator.anchor_generator_oneof\',\n      index=0, containing_type=None, fields=[]),\n  ],\n  serialized_start=240,\n  serialized_end=530,\n)\n\n_ANCHORGENERATOR.fields_by_name[\'grid_anchor_generator\'].message_type = object__detection_dot_protos_dot_grid__anchor__generator__pb2._GRIDANCHORGENERATOR\n_ANCHORGENERATOR.fields_by_name[\'ssd_anchor_generator\'].message_type = object__detection_dot_protos_dot_ssd__anchor__generator__pb2._SSDANCHORGENERATOR\n_ANCHORGENERATOR.fields_by_name[\'multiscale_anchor_generator\'].message_type = object__detection_dot_protos_dot_multiscale__anchor__generator__pb2._MULTISCALEANCHORGENERATOR\n_ANCHORGENERATOR.oneofs_by_name[\'anchor_generator_oneof\'].fields.append(\n  _ANCHORGENERATOR.fields_by_name[\'grid_anchor_generator\'])\n_ANCHORGENERATOR.fields_by_name[\'grid_anchor_generator\'].containing_oneof = _ANCHORGENERATOR.oneofs_by_name[\'anchor_generator_oneof\']\n_ANCHORGENERATOR.oneofs_by_name[\'anchor_generator_oneof\'].fields.append(\n  _ANCHORGENERATOR.fields_by_name[\'ssd_anchor_generator\'])\n_ANCHORGENERATOR.fields_by_name[\'ssd_anchor_generator\'].containing_oneof = _ANCHORGENERATOR.oneofs_by_name[\'anchor_generator_oneof\']\n_ANCHORGENERATOR.oneofs_by_name[\'anchor_generator_oneof\'].fields.append(\n  _ANCHORGENERATOR.fields_by_name[\'multiscale_anchor_generator\'])\n_ANCHORGENERATOR.fields_by_name[\'multiscale_anchor_generator\'].containing_oneof = _ANCHORGENERATOR.oneofs_by_name[\'anchor_generator_oneof\']\nDESCRIPTOR.message_types_by_name[\'AnchorGenerator\'] = _ANCHORGENERATOR\n_sym_db.RegisterFileDescriptor(DESCRIPTOR)\n\nAnchorGenerator = _reflection.GeneratedProtocolMessageType(\'AnchorGenerator\', (_message.Message,), dict(\n  DESCRIPTOR = _ANCHORGENERATOR,\n  __module__ = \'object_detection.protos.anchor_generator_pb2\'\n  # @@protoc_insertion_point(class_scope:object_detection.protos.AnchorGenerator)\n  ))\n_sym_db.RegisterMessage(AnchorGenerator)\n\n\n# @@protoc_insertion_point(module_scope)\n'"
src/object_detection/protos/argmax_matcher_pb2.py,0,"b'# Generated by the protocol buffer compiler.  DO NOT EDIT!\n# source: object_detection/protos/argmax_matcher.proto\n\nimport sys\n_b=sys.version_info[0]<3 and (lambda x:x) or (lambda x:x.encode(\'latin1\'))\nfrom google.protobuf import descriptor as _descriptor\nfrom google.protobuf import message as _message\nfrom google.protobuf import reflection as _reflection\nfrom google.protobuf import symbol_database as _symbol_database\nfrom google.protobuf import descriptor_pb2\n# @@protoc_insertion_point(imports)\n\n_sym_db = _symbol_database.Default()\n\n\n\n\nDESCRIPTOR = _descriptor.FileDescriptor(\n  name=\'object_detection/protos/argmax_matcher.proto\',\n  package=\'object_detection.protos\',\n  syntax=\'proto2\',\n  serialized_pb=_b(\'\\n,object_detection/protos/argmax_matcher.proto\\x12\\x17object_detection.protos\\""\\xec\\x01\\n\\rArgMaxMatcher\\x12\\x1e\\n\\x11matched_threshold\\x18\\x01 \\x01(\\x02:\\x03\\x30.5\\x12 \\n\\x13unmatched_threshold\\x18\\x02 \\x01(\\x02:\\x03\\x30.5\\x12 \\n\\x11ignore_thresholds\\x18\\x03 \\x01(\\x08:\\x05\\x66\\x61lse\\x12,\\n\\x1enegatives_lower_than_unmatched\\x18\\x04 \\x01(\\x08:\\x04true\\x12\\\'\\n\\x18\\x66orce_match_for_each_row\\x18\\x05 \\x01(\\x08:\\x05\\x66\\x61lse\\x12 \\n\\x11use_matmul_gather\\x18\\x06 \\x01(\\x08:\\x05\\x66\\x61lse\')\n)\n\n\n\n\n_ARGMAXMATCHER = _descriptor.Descriptor(\n  name=\'ArgMaxMatcher\',\n  full_name=\'object_detection.protos.ArgMaxMatcher\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'matched_threshold\', full_name=\'object_detection.protos.ArgMaxMatcher.matched_threshold\', index=0,\n      number=1, type=2, cpp_type=6, label=1,\n      has_default_value=True, default_value=float(0.5),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'unmatched_threshold\', full_name=\'object_detection.protos.ArgMaxMatcher.unmatched_threshold\', index=1,\n      number=2, type=2, cpp_type=6, label=1,\n      has_default_value=True, default_value=float(0.5),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'ignore_thresholds\', full_name=\'object_detection.protos.ArgMaxMatcher.ignore_thresholds\', index=2,\n      number=3, type=8, cpp_type=7, label=1,\n      has_default_value=True, default_value=False,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'negatives_lower_than_unmatched\', full_name=\'object_detection.protos.ArgMaxMatcher.negatives_lower_than_unmatched\', index=3,\n      number=4, type=8, cpp_type=7, label=1,\n      has_default_value=True, default_value=True,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'force_match_for_each_row\', full_name=\'object_detection.protos.ArgMaxMatcher.force_match_for_each_row\', index=4,\n      number=5, type=8, cpp_type=7, label=1,\n      has_default_value=True, default_value=False,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'use_matmul_gather\', full_name=\'object_detection.protos.ArgMaxMatcher.use_matmul_gather\', index=5,\n      number=6, type=8, cpp_type=7, label=1,\n      has_default_value=True, default_value=False,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto2\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=74,\n  serialized_end=310,\n)\n\nDESCRIPTOR.message_types_by_name[\'ArgMaxMatcher\'] = _ARGMAXMATCHER\n_sym_db.RegisterFileDescriptor(DESCRIPTOR)\n\nArgMaxMatcher = _reflection.GeneratedProtocolMessageType(\'ArgMaxMatcher\', (_message.Message,), dict(\n  DESCRIPTOR = _ARGMAXMATCHER,\n  __module__ = \'object_detection.protos.argmax_matcher_pb2\'\n  # @@protoc_insertion_point(class_scope:object_detection.protos.ArgMaxMatcher)\n  ))\n_sym_db.RegisterMessage(ArgMaxMatcher)\n\n\n# @@protoc_insertion_point(module_scope)\n'"
src/object_detection/protos/bipartite_matcher_pb2.py,0,"b'# Generated by the protocol buffer compiler.  DO NOT EDIT!\n# source: object_detection/protos/bipartite_matcher.proto\n\nimport sys\n_b=sys.version_info[0]<3 and (lambda x:x) or (lambda x:x.encode(\'latin1\'))\nfrom google.protobuf import descriptor as _descriptor\nfrom google.protobuf import message as _message\nfrom google.protobuf import reflection as _reflection\nfrom google.protobuf import symbol_database as _symbol_database\nfrom google.protobuf import descriptor_pb2\n# @@protoc_insertion_point(imports)\n\n_sym_db = _symbol_database.Default()\n\n\n\n\nDESCRIPTOR = _descriptor.FileDescriptor(\n  name=\'object_detection/protos/bipartite_matcher.proto\',\n  package=\'object_detection.protos\',\n  syntax=\'proto2\',\n  serialized_pb=_b(\'\\n/object_detection/protos/bipartite_matcher.proto\\x12\\x17object_detection.protos\\""4\\n\\x10\\x42ipartiteMatcher\\x12 \\n\\x11use_matmul_gather\\x18\\x06 \\x01(\\x08:\\x05\\x66\\x61lse\')\n)\n\n\n\n\n_BIPARTITEMATCHER = _descriptor.Descriptor(\n  name=\'BipartiteMatcher\',\n  full_name=\'object_detection.protos.BipartiteMatcher\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'use_matmul_gather\', full_name=\'object_detection.protos.BipartiteMatcher.use_matmul_gather\', index=0,\n      number=6, type=8, cpp_type=7, label=1,\n      has_default_value=True, default_value=False,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto2\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=76,\n  serialized_end=128,\n)\n\nDESCRIPTOR.message_types_by_name[\'BipartiteMatcher\'] = _BIPARTITEMATCHER\n_sym_db.RegisterFileDescriptor(DESCRIPTOR)\n\nBipartiteMatcher = _reflection.GeneratedProtocolMessageType(\'BipartiteMatcher\', (_message.Message,), dict(\n  DESCRIPTOR = _BIPARTITEMATCHER,\n  __module__ = \'object_detection.protos.bipartite_matcher_pb2\'\n  # @@protoc_insertion_point(class_scope:object_detection.protos.BipartiteMatcher)\n  ))\n_sym_db.RegisterMessage(BipartiteMatcher)\n\n\n# @@protoc_insertion_point(module_scope)\n'"
src/object_detection/protos/box_coder_pb2.py,0,"b'# Generated by the protocol buffer compiler.  DO NOT EDIT!\n# source: object_detection/protos/box_coder.proto\n\nimport sys\n_b=sys.version_info[0]<3 and (lambda x:x) or (lambda x:x.encode(\'latin1\'))\nfrom google.protobuf import descriptor as _descriptor\nfrom google.protobuf import message as _message\nfrom google.protobuf import reflection as _reflection\nfrom google.protobuf import symbol_database as _symbol_database\nfrom google.protobuf import descriptor_pb2\n# @@protoc_insertion_point(imports)\n\n_sym_db = _symbol_database.Default()\n\n\nfrom object_detection.protos import faster_rcnn_box_coder_pb2 as object__detection_dot_protos_dot_faster__rcnn__box__coder__pb2\nfrom object_detection.protos import keypoint_box_coder_pb2 as object__detection_dot_protos_dot_keypoint__box__coder__pb2\nfrom object_detection.protos import mean_stddev_box_coder_pb2 as object__detection_dot_protos_dot_mean__stddev__box__coder__pb2\nfrom object_detection.protos import square_box_coder_pb2 as object__detection_dot_protos_dot_square__box__coder__pb2\n\n\nDESCRIPTOR = _descriptor.FileDescriptor(\n  name=\'object_detection/protos/box_coder.proto\',\n  package=\'object_detection.protos\',\n  syntax=\'proto2\',\n  serialized_pb=_b(\'\\n\\\'object_detection/protos/box_coder.proto\\x12\\x17object_detection.protos\\x1a\\x33object_detection/protos/faster_rcnn_box_coder.proto\\x1a\\x30object_detection/protos/keypoint_box_coder.proto\\x1a\\x33object_detection/protos/mean_stddev_box_coder.proto\\x1a.object_detection/protos/square_box_coder.proto\\""\\xc7\\x02\\n\\x08\\x42oxCoder\\x12L\\n\\x15\\x66\\x61ster_rcnn_box_coder\\x18\\x01 \\x01(\\x0b\\x32+.object_detection.protos.FasterRcnnBoxCoderH\\x00\\x12L\\n\\x15mean_stddev_box_coder\\x18\\x02 \\x01(\\x0b\\x32+.object_detection.protos.MeanStddevBoxCoderH\\x00\\x12\\x43\\n\\x10square_box_coder\\x18\\x03 \\x01(\\x0b\\x32\\\'.object_detection.protos.SquareBoxCoderH\\x00\\x12G\\n\\x12keypoint_box_coder\\x18\\x04 \\x01(\\x0b\\x32).object_detection.protos.KeypointBoxCoderH\\x00\\x42\\x11\\n\\x0f\\x62ox_coder_oneof\')\n  ,\n  dependencies=[object__detection_dot_protos_dot_faster__rcnn__box__coder__pb2.DESCRIPTOR,object__detection_dot_protos_dot_keypoint__box__coder__pb2.DESCRIPTOR,object__detection_dot_protos_dot_mean__stddev__box__coder__pb2.DESCRIPTOR,object__detection_dot_protos_dot_square__box__coder__pb2.DESCRIPTOR,])\n\n\n\n\n_BOXCODER = _descriptor.Descriptor(\n  name=\'BoxCoder\',\n  full_name=\'object_detection.protos.BoxCoder\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'faster_rcnn_box_coder\', full_name=\'object_detection.protos.BoxCoder.faster_rcnn_box_coder\', index=0,\n      number=1, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'mean_stddev_box_coder\', full_name=\'object_detection.protos.BoxCoder.mean_stddev_box_coder\', index=1,\n      number=2, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'square_box_coder\', full_name=\'object_detection.protos.BoxCoder.square_box_coder\', index=2,\n      number=3, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'keypoint_box_coder\', full_name=\'object_detection.protos.BoxCoder.keypoint_box_coder\', index=3,\n      number=4, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto2\',\n  extension_ranges=[],\n  oneofs=[\n    _descriptor.OneofDescriptor(\n      name=\'box_coder_oneof\', full_name=\'object_detection.protos.BoxCoder.box_coder_oneof\',\n      index=0, containing_type=None, fields=[]),\n  ],\n  serialized_start=273,\n  serialized_end=600,\n)\n\n_BOXCODER.fields_by_name[\'faster_rcnn_box_coder\'].message_type = object__detection_dot_protos_dot_faster__rcnn__box__coder__pb2._FASTERRCNNBOXCODER\n_BOXCODER.fields_by_name[\'mean_stddev_box_coder\'].message_type = object__detection_dot_protos_dot_mean__stddev__box__coder__pb2._MEANSTDDEVBOXCODER\n_BOXCODER.fields_by_name[\'square_box_coder\'].message_type = object__detection_dot_protos_dot_square__box__coder__pb2._SQUAREBOXCODER\n_BOXCODER.fields_by_name[\'keypoint_box_coder\'].message_type = object__detection_dot_protos_dot_keypoint__box__coder__pb2._KEYPOINTBOXCODER\n_BOXCODER.oneofs_by_name[\'box_coder_oneof\'].fields.append(\n  _BOXCODER.fields_by_name[\'faster_rcnn_box_coder\'])\n_BOXCODER.fields_by_name[\'faster_rcnn_box_coder\'].containing_oneof = _BOXCODER.oneofs_by_name[\'box_coder_oneof\']\n_BOXCODER.oneofs_by_name[\'box_coder_oneof\'].fields.append(\n  _BOXCODER.fields_by_name[\'mean_stddev_box_coder\'])\n_BOXCODER.fields_by_name[\'mean_stddev_box_coder\'].containing_oneof = _BOXCODER.oneofs_by_name[\'box_coder_oneof\']\n_BOXCODER.oneofs_by_name[\'box_coder_oneof\'].fields.append(\n  _BOXCODER.fields_by_name[\'square_box_coder\'])\n_BOXCODER.fields_by_name[\'square_box_coder\'].containing_oneof = _BOXCODER.oneofs_by_name[\'box_coder_oneof\']\n_BOXCODER.oneofs_by_name[\'box_coder_oneof\'].fields.append(\n  _BOXCODER.fields_by_name[\'keypoint_box_coder\'])\n_BOXCODER.fields_by_name[\'keypoint_box_coder\'].containing_oneof = _BOXCODER.oneofs_by_name[\'box_coder_oneof\']\nDESCRIPTOR.message_types_by_name[\'BoxCoder\'] = _BOXCODER\n_sym_db.RegisterFileDescriptor(DESCRIPTOR)\n\nBoxCoder = _reflection.GeneratedProtocolMessageType(\'BoxCoder\', (_message.Message,), dict(\n  DESCRIPTOR = _BOXCODER,\n  __module__ = \'object_detection.protos.box_coder_pb2\'\n  # @@protoc_insertion_point(class_scope:object_detection.protos.BoxCoder)\n  ))\n_sym_db.RegisterMessage(BoxCoder)\n\n\n# @@protoc_insertion_point(module_scope)\n'"
src/object_detection/protos/box_predictor_pb2.py,0,"b'# Generated by the protocol buffer compiler.  DO NOT EDIT!\n# source: object_detection/protos/box_predictor.proto\n\nimport sys\n_b=sys.version_info[0]<3 and (lambda x:x) or (lambda x:x.encode(\'latin1\'))\nfrom google.protobuf import descriptor as _descriptor\nfrom google.protobuf import message as _message\nfrom google.protobuf import reflection as _reflection\nfrom google.protobuf import symbol_database as _symbol_database\nfrom google.protobuf import descriptor_pb2\n# @@protoc_insertion_point(imports)\n\n_sym_db = _symbol_database.Default()\n\n\nfrom object_detection.protos import hyperparams_pb2 as object__detection_dot_protos_dot_hyperparams__pb2\n\n\nDESCRIPTOR = _descriptor.FileDescriptor(\n  name=\'object_detection/protos/box_predictor.proto\',\n  package=\'object_detection.protos\',\n  syntax=\'proto2\',\n  serialized_pb=_b(\'\\n+object_detection/protos/box_predictor.proto\\x12\\x17object_detection.protos\\x1a)object_detection/protos/hyperparams.proto\\""\\x90\\x03\\n\\x0c\\x42oxPredictor\\x12Y\\n\\x1b\\x63onvolutional_box_predictor\\x18\\x01 \\x01(\\x0b\\x32\\x32.object_detection.protos.ConvolutionalBoxPredictorH\\x00\\x12P\\n\\x17mask_rcnn_box_predictor\\x18\\x02 \\x01(\\x0b\\x32-.object_detection.protos.MaskRCNNBoxPredictorH\\x00\\x12G\\n\\x12rfcn_box_predictor\\x18\\x03 \\x01(\\x0b\\x32).object_detection.protos.RfcnBoxPredictorH\\x00\\x12s\\n)weight_shared_convolutional_box_predictor\\x18\\x04 \\x01(\\x0b\\x32>.object_detection.protos.WeightSharedConvolutionalBoxPredictorH\\x00\\x42\\x15\\n\\x13\\x62ox_predictor_oneof\\""\\x90\\x03\\n\\x19\\x43onvolutionalBoxPredictor\\x12>\\n\\x10\\x63onv_hyperparams\\x18\\x01 \\x01(\\x0b\\x32$.object_detection.protos.Hyperparams\\x12\\x14\\n\\tmin_depth\\x18\\x02 \\x01(\\x05:\\x01\\x30\\x12\\x14\\n\\tmax_depth\\x18\\x03 \\x01(\\x05:\\x01\\x30\\x12&\\n\\x1bnum_layers_before_predictor\\x18\\x04 \\x01(\\x05:\\x01\\x30\\x12\\x19\\n\\x0buse_dropout\\x18\\x05 \\x01(\\x08:\\x04true\\x12%\\n\\x18\\x64ropout_keep_probability\\x18\\x06 \\x01(\\x02:\\x03\\x30.8\\x12\\x16\\n\\x0bkernel_size\\x18\\x07 \\x01(\\x05:\\x01\\x31\\x12\\x18\\n\\rbox_code_size\\x18\\x08 \\x01(\\x05:\\x01\\x34\\x12&\\n\\x17\\x61pply_sigmoid_to_scores\\x18\\t \\x01(\\x08:\\x05\\x66\\x61lse\\x12%\\n\\x1a\\x63lass_prediction_bias_init\\x18\\n \\x01(\\x02:\\x01\\x30\\x12\\x1c\\n\\ruse_depthwise\\x18\\x0b \\x01(\\x08:\\x05\\x66\\x61lse\\""\\xfa\\x01\\n%WeightSharedConvolutionalBoxPredictor\\x12>\\n\\x10\\x63onv_hyperparams\\x18\\x01 \\x01(\\x0b\\x32$.object_detection.protos.Hyperparams\\x12&\\n\\x1bnum_layers_before_predictor\\x18\\x04 \\x01(\\x05:\\x01\\x30\\x12\\x10\\n\\x05\\x64\\x65pth\\x18\\x02 \\x01(\\x05:\\x01\\x30\\x12\\x16\\n\\x0bkernel_size\\x18\\x07 \\x01(\\x05:\\x01\\x33\\x12\\x18\\n\\rbox_code_size\\x18\\x08 \\x01(\\x05:\\x01\\x34\\x12%\\n\\x1a\\x63lass_prediction_bias_init\\x18\\n \\x01(\\x02:\\x01\\x30\\""\\xc0\\x03\\n\\x14MaskRCNNBoxPredictor\\x12<\\n\\x0e\\x66\\x63_hyperparams\\x18\\x01 \\x01(\\x0b\\x32$.object_detection.protos.Hyperparams\\x12\\x1a\\n\\x0buse_dropout\\x18\\x02 \\x01(\\x08:\\x05\\x66\\x61lse\\x12%\\n\\x18\\x64ropout_keep_probability\\x18\\x03 \\x01(\\x02:\\x03\\x30.5\\x12\\x18\\n\\rbox_code_size\\x18\\x04 \\x01(\\x05:\\x01\\x34\\x12>\\n\\x10\\x63onv_hyperparams\\x18\\x05 \\x01(\\x0b\\x32$.object_detection.protos.Hyperparams\\x12%\\n\\x16predict_instance_masks\\x18\\x06 \\x01(\\x08:\\x05\\x66\\x61lse\\x12\\\'\\n\\x1amask_prediction_conv_depth\\x18\\x07 \\x01(\\x05:\\x03\\x32\\x35\\x36\\x12 \\n\\x11predict_keypoints\\x18\\x08 \\x01(\\x08:\\x05\\x66\\x61lse\\x12\\x17\\n\\x0bmask_height\\x18\\t \\x01(\\x05:\\x02\\x31\\x35\\x12\\x16\\n\\nmask_width\\x18\\n \\x01(\\x05:\\x02\\x31\\x35\\x12*\\n\\x1fmask_prediction_num_conv_layers\\x18\\x0b \\x01(\\x05:\\x01\\x32\\""\\xf9\\x01\\n\\x10RfcnBoxPredictor\\x12>\\n\\x10\\x63onv_hyperparams\\x18\\x01 \\x01(\\x0b\\x32$.object_detection.protos.Hyperparams\\x12\\""\\n\\x17num_spatial_bins_height\\x18\\x02 \\x01(\\x05:\\x01\\x33\\x12!\\n\\x16num_spatial_bins_width\\x18\\x03 \\x01(\\x05:\\x01\\x33\\x12\\x13\\n\\x05\\x64\\x65pth\\x18\\x04 \\x01(\\x05:\\x04\\x31\\x30\\x32\\x34\\x12\\x18\\n\\rbox_code_size\\x18\\x05 \\x01(\\x05:\\x01\\x34\\x12\\x17\\n\\x0b\\x63rop_height\\x18\\x06 \\x01(\\x05:\\x02\\x31\\x32\\x12\\x16\\n\\ncrop_width\\x18\\x07 \\x01(\\x05:\\x02\\x31\\x32\')\n  ,\n  dependencies=[object__detection_dot_protos_dot_hyperparams__pb2.DESCRIPTOR,])\n\n\n\n\n_BOXPREDICTOR = _descriptor.Descriptor(\n  name=\'BoxPredictor\',\n  full_name=\'object_detection.protos.BoxPredictor\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'convolutional_box_predictor\', full_name=\'object_detection.protos.BoxPredictor.convolutional_box_predictor\', index=0,\n      number=1, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'mask_rcnn_box_predictor\', full_name=\'object_detection.protos.BoxPredictor.mask_rcnn_box_predictor\', index=1,\n      number=2, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'rfcn_box_predictor\', full_name=\'object_detection.protos.BoxPredictor.rfcn_box_predictor\', index=2,\n      number=3, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'weight_shared_convolutional_box_predictor\', full_name=\'object_detection.protos.BoxPredictor.weight_shared_convolutional_box_predictor\', index=3,\n      number=4, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto2\',\n  extension_ranges=[],\n  oneofs=[\n    _descriptor.OneofDescriptor(\n      name=\'box_predictor_oneof\', full_name=\'object_detection.protos.BoxPredictor.box_predictor_oneof\',\n      index=0, containing_type=None, fields=[]),\n  ],\n  serialized_start=116,\n  serialized_end=516,\n)\n\n\n_CONVOLUTIONALBOXPREDICTOR = _descriptor.Descriptor(\n  name=\'ConvolutionalBoxPredictor\',\n  full_name=\'object_detection.protos.ConvolutionalBoxPredictor\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'conv_hyperparams\', full_name=\'object_detection.protos.ConvolutionalBoxPredictor.conv_hyperparams\', index=0,\n      number=1, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'min_depth\', full_name=\'object_detection.protos.ConvolutionalBoxPredictor.min_depth\', index=1,\n      number=2, type=5, cpp_type=1, label=1,\n      has_default_value=True, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'max_depth\', full_name=\'object_detection.protos.ConvolutionalBoxPredictor.max_depth\', index=2,\n      number=3, type=5, cpp_type=1, label=1,\n      has_default_value=True, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'num_layers_before_predictor\', full_name=\'object_detection.protos.ConvolutionalBoxPredictor.num_layers_before_predictor\', index=3,\n      number=4, type=5, cpp_type=1, label=1,\n      has_default_value=True, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'use_dropout\', full_name=\'object_detection.protos.ConvolutionalBoxPredictor.use_dropout\', index=4,\n      number=5, type=8, cpp_type=7, label=1,\n      has_default_value=True, default_value=True,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'dropout_keep_probability\', full_name=\'object_detection.protos.ConvolutionalBoxPredictor.dropout_keep_probability\', index=5,\n      number=6, type=2, cpp_type=6, label=1,\n      has_default_value=True, default_value=float(0.8),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'kernel_size\', full_name=\'object_detection.protos.ConvolutionalBoxPredictor.kernel_size\', index=6,\n      number=7, type=5, cpp_type=1, label=1,\n      has_default_value=True, default_value=1,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'box_code_size\', full_name=\'object_detection.protos.ConvolutionalBoxPredictor.box_code_size\', index=7,\n      number=8, type=5, cpp_type=1, label=1,\n      has_default_value=True, default_value=4,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'apply_sigmoid_to_scores\', full_name=\'object_detection.protos.ConvolutionalBoxPredictor.apply_sigmoid_to_scores\', index=8,\n      number=9, type=8, cpp_type=7, label=1,\n      has_default_value=True, default_value=False,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'class_prediction_bias_init\', full_name=\'object_detection.protos.ConvolutionalBoxPredictor.class_prediction_bias_init\', index=9,\n      number=10, type=2, cpp_type=6, label=1,\n      has_default_value=True, default_value=float(0),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'use_depthwise\', full_name=\'object_detection.protos.ConvolutionalBoxPredictor.use_depthwise\', index=10,\n      number=11, type=8, cpp_type=7, label=1,\n      has_default_value=True, default_value=False,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto2\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=519,\n  serialized_end=919,\n)\n\n\n_WEIGHTSHAREDCONVOLUTIONALBOXPREDICTOR = _descriptor.Descriptor(\n  name=\'WeightSharedConvolutionalBoxPredictor\',\n  full_name=\'object_detection.protos.WeightSharedConvolutionalBoxPredictor\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'conv_hyperparams\', full_name=\'object_detection.protos.WeightSharedConvolutionalBoxPredictor.conv_hyperparams\', index=0,\n      number=1, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'num_layers_before_predictor\', full_name=\'object_detection.protos.WeightSharedConvolutionalBoxPredictor.num_layers_before_predictor\', index=1,\n      number=4, type=5, cpp_type=1, label=1,\n      has_default_value=True, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'depth\', full_name=\'object_detection.protos.WeightSharedConvolutionalBoxPredictor.depth\', index=2,\n      number=2, type=5, cpp_type=1, label=1,\n      has_default_value=True, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'kernel_size\', full_name=\'object_detection.protos.WeightSharedConvolutionalBoxPredictor.kernel_size\', index=3,\n      number=7, type=5, cpp_type=1, label=1,\n      has_default_value=True, default_value=3,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'box_code_size\', full_name=\'object_detection.protos.WeightSharedConvolutionalBoxPredictor.box_code_size\', index=4,\n      number=8, type=5, cpp_type=1, label=1,\n      has_default_value=True, default_value=4,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'class_prediction_bias_init\', full_name=\'object_detection.protos.WeightSharedConvolutionalBoxPredictor.class_prediction_bias_init\', index=5,\n      number=10, type=2, cpp_type=6, label=1,\n      has_default_value=True, default_value=float(0),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto2\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=922,\n  serialized_end=1172,\n)\n\n\n_MASKRCNNBOXPREDICTOR = _descriptor.Descriptor(\n  name=\'MaskRCNNBoxPredictor\',\n  full_name=\'object_detection.protos.MaskRCNNBoxPredictor\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'fc_hyperparams\', full_name=\'object_detection.protos.MaskRCNNBoxPredictor.fc_hyperparams\', index=0,\n      number=1, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'use_dropout\', full_name=\'object_detection.protos.MaskRCNNBoxPredictor.use_dropout\', index=1,\n      number=2, type=8, cpp_type=7, label=1,\n      has_default_value=True, default_value=False,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'dropout_keep_probability\', full_name=\'object_detection.protos.MaskRCNNBoxPredictor.dropout_keep_probability\', index=2,\n      number=3, type=2, cpp_type=6, label=1,\n      has_default_value=True, default_value=float(0.5),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'box_code_size\', full_name=\'object_detection.protos.MaskRCNNBoxPredictor.box_code_size\', index=3,\n      number=4, type=5, cpp_type=1, label=1,\n      has_default_value=True, default_value=4,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'conv_hyperparams\', full_name=\'object_detection.protos.MaskRCNNBoxPredictor.conv_hyperparams\', index=4,\n      number=5, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'predict_instance_masks\', full_name=\'object_detection.protos.MaskRCNNBoxPredictor.predict_instance_masks\', index=5,\n      number=6, type=8, cpp_type=7, label=1,\n      has_default_value=True, default_value=False,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'mask_prediction_conv_depth\', full_name=\'object_detection.protos.MaskRCNNBoxPredictor.mask_prediction_conv_depth\', index=6,\n      number=7, type=5, cpp_type=1, label=1,\n      has_default_value=True, default_value=256,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'predict_keypoints\', full_name=\'object_detection.protos.MaskRCNNBoxPredictor.predict_keypoints\', index=7,\n      number=8, type=8, cpp_type=7, label=1,\n      has_default_value=True, default_value=False,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'mask_height\', full_name=\'object_detection.protos.MaskRCNNBoxPredictor.mask_height\', index=8,\n      number=9, type=5, cpp_type=1, label=1,\n      has_default_value=True, default_value=15,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'mask_width\', full_name=\'object_detection.protos.MaskRCNNBoxPredictor.mask_width\', index=9,\n      number=10, type=5, cpp_type=1, label=1,\n      has_default_value=True, default_value=15,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'mask_prediction_num_conv_layers\', full_name=\'object_detection.protos.MaskRCNNBoxPredictor.mask_prediction_num_conv_layers\', index=10,\n      number=11, type=5, cpp_type=1, label=1,\n      has_default_value=True, default_value=2,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto2\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=1175,\n  serialized_end=1623,\n)\n\n\n_RFCNBOXPREDICTOR = _descriptor.Descriptor(\n  name=\'RfcnBoxPredictor\',\n  full_name=\'object_detection.protos.RfcnBoxPredictor\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'conv_hyperparams\', full_name=\'object_detection.protos.RfcnBoxPredictor.conv_hyperparams\', index=0,\n      number=1, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'num_spatial_bins_height\', full_name=\'object_detection.protos.RfcnBoxPredictor.num_spatial_bins_height\', index=1,\n      number=2, type=5, cpp_type=1, label=1,\n      has_default_value=True, default_value=3,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'num_spatial_bins_width\', full_name=\'object_detection.protos.RfcnBoxPredictor.num_spatial_bins_width\', index=2,\n      number=3, type=5, cpp_type=1, label=1,\n      has_default_value=True, default_value=3,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'depth\', full_name=\'object_detection.protos.RfcnBoxPredictor.depth\', index=3,\n      number=4, type=5, cpp_type=1, label=1,\n      has_default_value=True, default_value=1024,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'box_code_size\', full_name=\'object_detection.protos.RfcnBoxPredictor.box_code_size\', index=4,\n      number=5, type=5, cpp_type=1, label=1,\n      has_default_value=True, default_value=4,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'crop_height\', full_name=\'object_detection.protos.RfcnBoxPredictor.crop_height\', index=5,\n      number=6, type=5, cpp_type=1, label=1,\n      has_default_value=True, default_value=12,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'crop_width\', full_name=\'object_detection.protos.RfcnBoxPredictor.crop_width\', index=6,\n      number=7, type=5, cpp_type=1, label=1,\n      has_default_value=True, default_value=12,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto2\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=1626,\n  serialized_end=1875,\n)\n\n_BOXPREDICTOR.fields_by_name[\'convolutional_box_predictor\'].message_type = _CONVOLUTIONALBOXPREDICTOR\n_BOXPREDICTOR.fields_by_name[\'mask_rcnn_box_predictor\'].message_type = _MASKRCNNBOXPREDICTOR\n_BOXPREDICTOR.fields_by_name[\'rfcn_box_predictor\'].message_type = _RFCNBOXPREDICTOR\n_BOXPREDICTOR.fields_by_name[\'weight_shared_convolutional_box_predictor\'].message_type = _WEIGHTSHAREDCONVOLUTIONALBOXPREDICTOR\n_BOXPREDICTOR.oneofs_by_name[\'box_predictor_oneof\'].fields.append(\n  _BOXPREDICTOR.fields_by_name[\'convolutional_box_predictor\'])\n_BOXPREDICTOR.fields_by_name[\'convolutional_box_predictor\'].containing_oneof = _BOXPREDICTOR.oneofs_by_name[\'box_predictor_oneof\']\n_BOXPREDICTOR.oneofs_by_name[\'box_predictor_oneof\'].fields.append(\n  _BOXPREDICTOR.fields_by_name[\'mask_rcnn_box_predictor\'])\n_BOXPREDICTOR.fields_by_name[\'mask_rcnn_box_predictor\'].containing_oneof = _BOXPREDICTOR.oneofs_by_name[\'box_predictor_oneof\']\n_BOXPREDICTOR.oneofs_by_name[\'box_predictor_oneof\'].fields.append(\n  _BOXPREDICTOR.fields_by_name[\'rfcn_box_predictor\'])\n_BOXPREDICTOR.fields_by_name[\'rfcn_box_predictor\'].containing_oneof = _BOXPREDICTOR.oneofs_by_name[\'box_predictor_oneof\']\n_BOXPREDICTOR.oneofs_by_name[\'box_predictor_oneof\'].fields.append(\n  _BOXPREDICTOR.fields_by_name[\'weight_shared_convolutional_box_predictor\'])\n_BOXPREDICTOR.fields_by_name[\'weight_shared_convolutional_box_predictor\'].containing_oneof = _BOXPREDICTOR.oneofs_by_name[\'box_predictor_oneof\']\n_CONVOLUTIONALBOXPREDICTOR.fields_by_name[\'conv_hyperparams\'].message_type = object__detection_dot_protos_dot_hyperparams__pb2._HYPERPARAMS\n_WEIGHTSHAREDCONVOLUTIONALBOXPREDICTOR.fields_by_name[\'conv_hyperparams\'].message_type = object__detection_dot_protos_dot_hyperparams__pb2._HYPERPARAMS\n_MASKRCNNBOXPREDICTOR.fields_by_name[\'fc_hyperparams\'].message_type = object__detection_dot_protos_dot_hyperparams__pb2._HYPERPARAMS\n_MASKRCNNBOXPREDICTOR.fields_by_name[\'conv_hyperparams\'].message_type = object__detection_dot_protos_dot_hyperparams__pb2._HYPERPARAMS\n_RFCNBOXPREDICTOR.fields_by_name[\'conv_hyperparams\'].message_type = object__detection_dot_protos_dot_hyperparams__pb2._HYPERPARAMS\nDESCRIPTOR.message_types_by_name[\'BoxPredictor\'] = _BOXPREDICTOR\nDESCRIPTOR.message_types_by_name[\'ConvolutionalBoxPredictor\'] = _CONVOLUTIONALBOXPREDICTOR\nDESCRIPTOR.message_types_by_name[\'WeightSharedConvolutionalBoxPredictor\'] = _WEIGHTSHAREDCONVOLUTIONALBOXPREDICTOR\nDESCRIPTOR.message_types_by_name[\'MaskRCNNBoxPredictor\'] = _MASKRCNNBOXPREDICTOR\nDESCRIPTOR.message_types_by_name[\'RfcnBoxPredictor\'] = _RFCNBOXPREDICTOR\n_sym_db.RegisterFileDescriptor(DESCRIPTOR)\n\nBoxPredictor = _reflection.GeneratedProtocolMessageType(\'BoxPredictor\', (_message.Message,), dict(\n  DESCRIPTOR = _BOXPREDICTOR,\n  __module__ = \'object_detection.protos.box_predictor_pb2\'\n  # @@protoc_insertion_point(class_scope:object_detection.protos.BoxPredictor)\n  ))\n_sym_db.RegisterMessage(BoxPredictor)\n\nConvolutionalBoxPredictor = _reflection.GeneratedProtocolMessageType(\'ConvolutionalBoxPredictor\', (_message.Message,), dict(\n  DESCRIPTOR = _CONVOLUTIONALBOXPREDICTOR,\n  __module__ = \'object_detection.protos.box_predictor_pb2\'\n  # @@protoc_insertion_point(class_scope:object_detection.protos.ConvolutionalBoxPredictor)\n  ))\n_sym_db.RegisterMessage(ConvolutionalBoxPredictor)\n\nWeightSharedConvolutionalBoxPredictor = _reflection.GeneratedProtocolMessageType(\'WeightSharedConvolutionalBoxPredictor\', (_message.Message,), dict(\n  DESCRIPTOR = _WEIGHTSHAREDCONVOLUTIONALBOXPREDICTOR,\n  __module__ = \'object_detection.protos.box_predictor_pb2\'\n  # @@protoc_insertion_point(class_scope:object_detection.protos.WeightSharedConvolutionalBoxPredictor)\n  ))\n_sym_db.RegisterMessage(WeightSharedConvolutionalBoxPredictor)\n\nMaskRCNNBoxPredictor = _reflection.GeneratedProtocolMessageType(\'MaskRCNNBoxPredictor\', (_message.Message,), dict(\n  DESCRIPTOR = _MASKRCNNBOXPREDICTOR,\n  __module__ = \'object_detection.protos.box_predictor_pb2\'\n  # @@protoc_insertion_point(class_scope:object_detection.protos.MaskRCNNBoxPredictor)\n  ))\n_sym_db.RegisterMessage(MaskRCNNBoxPredictor)\n\nRfcnBoxPredictor = _reflection.GeneratedProtocolMessageType(\'RfcnBoxPredictor\', (_message.Message,), dict(\n  DESCRIPTOR = _RFCNBOXPREDICTOR,\n  __module__ = \'object_detection.protos.box_predictor_pb2\'\n  # @@protoc_insertion_point(class_scope:object_detection.protos.RfcnBoxPredictor)\n  ))\n_sym_db.RegisterMessage(RfcnBoxPredictor)\n\n\n# @@protoc_insertion_point(module_scope)\n'"
src/object_detection/protos/eval_pb2.py,0,"b'# Generated by the protocol buffer compiler.  DO NOT EDIT!\n# source: object_detection/protos/eval.proto\n\nimport sys\n_b=sys.version_info[0]<3 and (lambda x:x) or (lambda x:x.encode(\'latin1\'))\nfrom google.protobuf import descriptor as _descriptor\nfrom google.protobuf import message as _message\nfrom google.protobuf import reflection as _reflection\nfrom google.protobuf import symbol_database as _symbol_database\nfrom google.protobuf import descriptor_pb2\n# @@protoc_insertion_point(imports)\n\n_sym_db = _symbol_database.Default()\n\n\n\n\nDESCRIPTOR = _descriptor.FileDescriptor(\n  name=\'object_detection/protos/eval.proto\',\n  package=\'object_detection.protos\',\n  syntax=\'proto2\',\n  serialized_pb=_b(\'\\n\\""object_detection/protos/eval.proto\\x12\\x17object_detection.protos\\""\\x85\\x05\\n\\nEvalConfig\\x12\\x1e\\n\\x12num_visualizations\\x18\\x01 \\x01(\\r:\\x02\\x31\\x30\\x12\\x1a\\n\\x0cnum_examples\\x18\\x02 \\x01(\\r:\\x04\\x35\\x30\\x30\\x30\\x12\\x1f\\n\\x12\\x65val_interval_secs\\x18\\x03 \\x01(\\r:\\x03\\x33\\x30\\x30\\x12\\x14\\n\\tmax_evals\\x18\\x04 \\x01(\\r:\\x01\\x30\\x12\\x19\\n\\nsave_graph\\x18\\x05 \\x01(\\x08:\\x05\\x66\\x61lse\\x12\\""\\n\\x18visualization_export_dir\\x18\\x06 \\x01(\\t:\\x00\\x12\\x15\\n\\x0b\\x65val_master\\x18\\x07 \\x01(\\t:\\x00\\x12\\x13\\n\\x0bmetrics_set\\x18\\x08 \\x03(\\t\\x12\\x15\\n\\x0b\\x65xport_path\\x18\\t \\x01(\\t:\\x00\\x12!\\n\\x12ignore_groundtruth\\x18\\n \\x01(\\x08:\\x05\\x66\\x61lse\\x12\\""\\n\\x13use_moving_averages\\x18\\x0b \\x01(\\x08:\\x05\\x66\\x61lse\\x12\\""\\n\\x13\\x65val_instance_masks\\x18\\x0c \\x01(\\x08:\\x05\\x66\\x61lse\\x12 \\n\\x13min_score_threshold\\x18\\r \\x01(\\x02:\\x03\\x30.5\\x12&\\n\\x1amax_num_boxes_to_visualize\\x18\\x0e \\x01(\\x05:\\x02\\x32\\x30\\x12\\x1a\\n\\x0bskip_scores\\x18\\x0f \\x01(\\x08:\\x05\\x66\\x61lse\\x12\\x1a\\n\\x0bskip_labels\\x18\\x10 \\x01(\\x08:\\x05\\x66\\x61lse\\x12*\\n\\x1bvisualize_groundtruth_boxes\\x18\\x11 \\x01(\\x08:\\x05\\x66\\x61lse\\x12\\x32\\n#groundtruth_box_visualization_color\\x18\\x12 \\x01(\\t:\\x05\\x62lack\\x12\\x35\\n&keep_image_id_for_visualization_export\\x18\\x13 \\x01(\\x08:\\x05\\x66\\x61lse\')\n)\n\n\n\n\n_EVALCONFIG = _descriptor.Descriptor(\n  name=\'EvalConfig\',\n  full_name=\'object_detection.protos.EvalConfig\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'num_visualizations\', full_name=\'object_detection.protos.EvalConfig.num_visualizations\', index=0,\n      number=1, type=13, cpp_type=3, label=1,\n      has_default_value=True, default_value=10,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'num_examples\', full_name=\'object_detection.protos.EvalConfig.num_examples\', index=1,\n      number=2, type=13, cpp_type=3, label=1,\n      has_default_value=True, default_value=5000,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'eval_interval_secs\', full_name=\'object_detection.protos.EvalConfig.eval_interval_secs\', index=2,\n      number=3, type=13, cpp_type=3, label=1,\n      has_default_value=True, default_value=300,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'max_evals\', full_name=\'object_detection.protos.EvalConfig.max_evals\', index=3,\n      number=4, type=13, cpp_type=3, label=1,\n      has_default_value=True, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'save_graph\', full_name=\'object_detection.protos.EvalConfig.save_graph\', index=4,\n      number=5, type=8, cpp_type=7, label=1,\n      has_default_value=True, default_value=False,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'visualization_export_dir\', full_name=\'object_detection.protos.EvalConfig.visualization_export_dir\', index=5,\n      number=6, type=9, cpp_type=9, label=1,\n      has_default_value=True, default_value=_b("""").decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'eval_master\', full_name=\'object_detection.protos.EvalConfig.eval_master\', index=6,\n      number=7, type=9, cpp_type=9, label=1,\n      has_default_value=True, default_value=_b("""").decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'metrics_set\', full_name=\'object_detection.protos.EvalConfig.metrics_set\', index=7,\n      number=8, type=9, cpp_type=9, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'export_path\', full_name=\'object_detection.protos.EvalConfig.export_path\', index=8,\n      number=9, type=9, cpp_type=9, label=1,\n      has_default_value=True, default_value=_b("""").decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'ignore_groundtruth\', full_name=\'object_detection.protos.EvalConfig.ignore_groundtruth\', index=9,\n      number=10, type=8, cpp_type=7, label=1,\n      has_default_value=True, default_value=False,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'use_moving_averages\', full_name=\'object_detection.protos.EvalConfig.use_moving_averages\', index=10,\n      number=11, type=8, cpp_type=7, label=1,\n      has_default_value=True, default_value=False,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'eval_instance_masks\', full_name=\'object_detection.protos.EvalConfig.eval_instance_masks\', index=11,\n      number=12, type=8, cpp_type=7, label=1,\n      has_default_value=True, default_value=False,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'min_score_threshold\', full_name=\'object_detection.protos.EvalConfig.min_score_threshold\', index=12,\n      number=13, type=2, cpp_type=6, label=1,\n      has_default_value=True, default_value=float(0.5),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'max_num_boxes_to_visualize\', full_name=\'object_detection.protos.EvalConfig.max_num_boxes_to_visualize\', index=13,\n      number=14, type=5, cpp_type=1, label=1,\n      has_default_value=True, default_value=20,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'skip_scores\', full_name=\'object_detection.protos.EvalConfig.skip_scores\', index=14,\n      number=15, type=8, cpp_type=7, label=1,\n      has_default_value=True, default_value=False,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'skip_labels\', full_name=\'object_detection.protos.EvalConfig.skip_labels\', index=15,\n      number=16, type=8, cpp_type=7, label=1,\n      has_default_value=True, default_value=False,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'visualize_groundtruth_boxes\', full_name=\'object_detection.protos.EvalConfig.visualize_groundtruth_boxes\', index=16,\n      number=17, type=8, cpp_type=7, label=1,\n      has_default_value=True, default_value=False,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'groundtruth_box_visualization_color\', full_name=\'object_detection.protos.EvalConfig.groundtruth_box_visualization_color\', index=17,\n      number=18, type=9, cpp_type=9, label=1,\n      has_default_value=True, default_value=_b(""black"").decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'keep_image_id_for_visualization_export\', full_name=\'object_detection.protos.EvalConfig.keep_image_id_for_visualization_export\', index=18,\n      number=19, type=8, cpp_type=7, label=1,\n      has_default_value=True, default_value=False,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto2\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=64,\n  serialized_end=709,\n)\n\nDESCRIPTOR.message_types_by_name[\'EvalConfig\'] = _EVALCONFIG\n_sym_db.RegisterFileDescriptor(DESCRIPTOR)\n\nEvalConfig = _reflection.GeneratedProtocolMessageType(\'EvalConfig\', (_message.Message,), dict(\n  DESCRIPTOR = _EVALCONFIG,\n  __module__ = \'object_detection.protos.eval_pb2\'\n  # @@protoc_insertion_point(class_scope:object_detection.protos.EvalConfig)\n  ))\n_sym_db.RegisterMessage(EvalConfig)\n\n\n# @@protoc_insertion_point(module_scope)\n'"
src/object_detection/protos/faster_rcnn_box_coder_pb2.py,0,"b'# Generated by the protocol buffer compiler.  DO NOT EDIT!\n# source: object_detection/protos/faster_rcnn_box_coder.proto\n\nimport sys\n_b=sys.version_info[0]<3 and (lambda x:x) or (lambda x:x.encode(\'latin1\'))\nfrom google.protobuf import descriptor as _descriptor\nfrom google.protobuf import message as _message\nfrom google.protobuf import reflection as _reflection\nfrom google.protobuf import symbol_database as _symbol_database\nfrom google.protobuf import descriptor_pb2\n# @@protoc_insertion_point(imports)\n\n_sym_db = _symbol_database.Default()\n\n\n\n\nDESCRIPTOR = _descriptor.FileDescriptor(\n  name=\'object_detection/protos/faster_rcnn_box_coder.proto\',\n  package=\'object_detection.protos\',\n  syntax=\'proto2\',\n  serialized_pb=_b(\'\\n3object_detection/protos/faster_rcnn_box_coder.proto\\x12\\x17object_detection.protos\\""o\\n\\x12\\x46\\x61sterRcnnBoxCoder\\x12\\x13\\n\\x07y_scale\\x18\\x01 \\x01(\\x02:\\x02\\x31\\x30\\x12\\x13\\n\\x07x_scale\\x18\\x02 \\x01(\\x02:\\x02\\x31\\x30\\x12\\x17\\n\\x0cheight_scale\\x18\\x03 \\x01(\\x02:\\x01\\x35\\x12\\x16\\n\\x0bwidth_scale\\x18\\x04 \\x01(\\x02:\\x01\\x35\')\n)\n\n\n\n\n_FASTERRCNNBOXCODER = _descriptor.Descriptor(\n  name=\'FasterRcnnBoxCoder\',\n  full_name=\'object_detection.protos.FasterRcnnBoxCoder\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'y_scale\', full_name=\'object_detection.protos.FasterRcnnBoxCoder.y_scale\', index=0,\n      number=1, type=2, cpp_type=6, label=1,\n      has_default_value=True, default_value=float(10),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'x_scale\', full_name=\'object_detection.protos.FasterRcnnBoxCoder.x_scale\', index=1,\n      number=2, type=2, cpp_type=6, label=1,\n      has_default_value=True, default_value=float(10),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'height_scale\', full_name=\'object_detection.protos.FasterRcnnBoxCoder.height_scale\', index=2,\n      number=3, type=2, cpp_type=6, label=1,\n      has_default_value=True, default_value=float(5),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'width_scale\', full_name=\'object_detection.protos.FasterRcnnBoxCoder.width_scale\', index=3,\n      number=4, type=2, cpp_type=6, label=1,\n      has_default_value=True, default_value=float(5),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto2\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=80,\n  serialized_end=191,\n)\n\nDESCRIPTOR.message_types_by_name[\'FasterRcnnBoxCoder\'] = _FASTERRCNNBOXCODER\n_sym_db.RegisterFileDescriptor(DESCRIPTOR)\n\nFasterRcnnBoxCoder = _reflection.GeneratedProtocolMessageType(\'FasterRcnnBoxCoder\', (_message.Message,), dict(\n  DESCRIPTOR = _FASTERRCNNBOXCODER,\n  __module__ = \'object_detection.protos.faster_rcnn_box_coder_pb2\'\n  # @@protoc_insertion_point(class_scope:object_detection.protos.FasterRcnnBoxCoder)\n  ))\n_sym_db.RegisterMessage(FasterRcnnBoxCoder)\n\n\n# @@protoc_insertion_point(module_scope)\n'"
src/object_detection/protos/faster_rcnn_pb2.py,0,"b'# Generated by the protocol buffer compiler.  DO NOT EDIT!\n# source: object_detection/protos/faster_rcnn.proto\n\nimport sys\n_b=sys.version_info[0]<3 and (lambda x:x) or (lambda x:x.encode(\'latin1\'))\nfrom google.protobuf import descriptor as _descriptor\nfrom google.protobuf import message as _message\nfrom google.protobuf import reflection as _reflection\nfrom google.protobuf import symbol_database as _symbol_database\nfrom google.protobuf import descriptor_pb2\n# @@protoc_insertion_point(imports)\n\n_sym_db = _symbol_database.Default()\n\n\nfrom object_detection.protos import anchor_generator_pb2 as object__detection_dot_protos_dot_anchor__generator__pb2\nfrom object_detection.protos import box_predictor_pb2 as object__detection_dot_protos_dot_box__predictor__pb2\nfrom object_detection.protos import hyperparams_pb2 as object__detection_dot_protos_dot_hyperparams__pb2\nfrom object_detection.protos import image_resizer_pb2 as object__detection_dot_protos_dot_image__resizer__pb2\nfrom object_detection.protos import losses_pb2 as object__detection_dot_protos_dot_losses__pb2\nfrom object_detection.protos import post_processing_pb2 as object__detection_dot_protos_dot_post__processing__pb2\n\n\nDESCRIPTOR = _descriptor.FileDescriptor(\n  name=\'object_detection/protos/faster_rcnn.proto\',\n  package=\'object_detection.protos\',\n  syntax=\'proto2\',\n  serialized_pb=_b(\'\\n)object_detection/protos/faster_rcnn.proto\\x12\\x17object_detection.protos\\x1a.object_detection/protos/anchor_generator.proto\\x1a+object_detection/protos/box_predictor.proto\\x1a)object_detection/protos/hyperparams.proto\\x1a+object_detection/protos/image_resizer.proto\\x1a$object_detection/protos/losses.proto\\x1a-object_detection/protos/post_processing.proto\\""\\xac\\x0b\\n\\nFasterRcnn\\x12\\x1b\\n\\x10number_of_stages\\x18\\x01 \\x01(\\x05:\\x01\\x32\\x12\\x13\\n\\x0bnum_classes\\x18\\x03 \\x01(\\x05\\x12<\\n\\rimage_resizer\\x18\\x04 \\x01(\\x0b\\x32%.object_detection.protos.ImageResizer\\x12N\\n\\x11\\x66\\x65\\x61ture_extractor\\x18\\x05 \\x01(\\x0b\\x32\\x33.object_detection.protos.FasterRcnnFeatureExtractor\\x12N\\n\\x1c\\x66irst_stage_anchor_generator\\x18\\x06 \\x01(\\x0b\\x32(.object_detection.protos.AnchorGenerator\\x12\\""\\n\\x17\\x66irst_stage_atrous_rate\\x18\\x07 \\x01(\\x05:\\x01\\x31\\x12X\\n*first_stage_box_predictor_conv_hyperparams\\x18\\x08 \\x01(\\x0b\\x32$.object_detection.protos.Hyperparams\\x12\\x30\\n%first_stage_box_predictor_kernel_size\\x18\\t \\x01(\\x05:\\x01\\x33\\x12,\\n\\x1f\\x66irst_stage_box_predictor_depth\\x18\\n \\x01(\\x05:\\x03\\x35\\x31\\x32\\x12\\\'\\n\\x1a\\x66irst_stage_minibatch_size\\x18\\x0b \\x01(\\x05:\\x03\\x32\\x35\\x36\\x12\\x32\\n%first_stage_positive_balance_fraction\\x18\\x0c \\x01(\\x02:\\x03\\x30.5\\x12*\\n\\x1f\\x66irst_stage_nms_score_threshold\\x18\\r \\x01(\\x02:\\x01\\x30\\x12*\\n\\x1d\\x66irst_stage_nms_iou_threshold\\x18\\x0e \\x01(\\x02:\\x03\\x30.7\\x12&\\n\\x19\\x66irst_stage_max_proposals\\x18\\x0f \\x01(\\x05:\\x03\\x33\\x30\\x30\\x12/\\n$first_stage_localization_loss_weight\\x18\\x10 \\x01(\\x02:\\x01\\x31\\x12-\\n\\""first_stage_objectness_loss_weight\\x18\\x11 \\x01(\\x02:\\x01\\x31\\x12\\x19\\n\\x11initial_crop_size\\x18\\x12 \\x01(\\x05\\x12\\x1b\\n\\x13maxpool_kernel_size\\x18\\x13 \\x01(\\x05\\x12\\x16\\n\\x0emaxpool_stride\\x18\\x14 \\x01(\\x05\\x12I\\n\\x1asecond_stage_box_predictor\\x18\\x15 \\x01(\\x0b\\x32%.object_detection.protos.BoxPredictor\\x12#\\n\\x17second_stage_batch_size\\x18\\x16 \\x01(\\x05:\\x02\\x36\\x34\\x12+\\n\\x1dsecond_stage_balance_fraction\\x18\\x17 \\x01(\\x02:\\x04\\x30.25\\x12M\\n\\x1csecond_stage_post_processing\\x18\\x18 \\x01(\\x0b\\x32\\\'.object_detection.protos.PostProcessing\\x12\\x30\\n%second_stage_localization_loss_weight\\x18\\x19 \\x01(\\x02:\\x01\\x31\\x12\\x32\\n\\\'second_stage_classification_loss_weight\\x18\\x1a \\x01(\\x02:\\x01\\x31\\x12\\x33\\n(second_stage_mask_prediction_loss_weight\\x18\\x1b \\x01(\\x02:\\x01\\x31\\x12\\x45\\n\\x12hard_example_miner\\x18\\x1c \\x01(\\x0b\\x32).object_detection.protos.HardExampleMiner\\x12U\\n second_stage_classification_loss\\x18\\x1d \\x01(\\x0b\\x32+.object_detection.protos.ClassificationLoss\\""x\\n\\x1a\\x46\\x61sterRcnnFeatureExtractor\\x12\\x0c\\n\\x04type\\x18\\x01 \\x01(\\t\\x12\\\'\\n\\x1b\\x66irst_stage_features_stride\\x18\\x02 \\x01(\\x05:\\x02\\x31\\x36\\x12#\\n\\x14\\x62\\x61tch_norm_trainable\\x18\\x03 \\x01(\\x08:\\x05\\x66\\x61lse\')\n  ,\n  dependencies=[object__detection_dot_protos_dot_anchor__generator__pb2.DESCRIPTOR,object__detection_dot_protos_dot_box__predictor__pb2.DESCRIPTOR,object__detection_dot_protos_dot_hyperparams__pb2.DESCRIPTOR,object__detection_dot_protos_dot_image__resizer__pb2.DESCRIPTOR,object__detection_dot_protos_dot_losses__pb2.DESCRIPTOR,object__detection_dot_protos_dot_post__processing__pb2.DESCRIPTOR,])\n\n\n\n\n_FASTERRCNN = _descriptor.Descriptor(\n  name=\'FasterRcnn\',\n  full_name=\'object_detection.protos.FasterRcnn\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'number_of_stages\', full_name=\'object_detection.protos.FasterRcnn.number_of_stages\', index=0,\n      number=1, type=5, cpp_type=1, label=1,\n      has_default_value=True, default_value=2,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'num_classes\', full_name=\'object_detection.protos.FasterRcnn.num_classes\', index=1,\n      number=3, type=5, cpp_type=1, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'image_resizer\', full_name=\'object_detection.protos.FasterRcnn.image_resizer\', index=2,\n      number=4, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'feature_extractor\', full_name=\'object_detection.protos.FasterRcnn.feature_extractor\', index=3,\n      number=5, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'first_stage_anchor_generator\', full_name=\'object_detection.protos.FasterRcnn.first_stage_anchor_generator\', index=4,\n      number=6, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'first_stage_atrous_rate\', full_name=\'object_detection.protos.FasterRcnn.first_stage_atrous_rate\', index=5,\n      number=7, type=5, cpp_type=1, label=1,\n      has_default_value=True, default_value=1,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'first_stage_box_predictor_conv_hyperparams\', full_name=\'object_detection.protos.FasterRcnn.first_stage_box_predictor_conv_hyperparams\', index=6,\n      number=8, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'first_stage_box_predictor_kernel_size\', full_name=\'object_detection.protos.FasterRcnn.first_stage_box_predictor_kernel_size\', index=7,\n      number=9, type=5, cpp_type=1, label=1,\n      has_default_value=True, default_value=3,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'first_stage_box_predictor_depth\', full_name=\'object_detection.protos.FasterRcnn.first_stage_box_predictor_depth\', index=8,\n      number=10, type=5, cpp_type=1, label=1,\n      has_default_value=True, default_value=512,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'first_stage_minibatch_size\', full_name=\'object_detection.protos.FasterRcnn.first_stage_minibatch_size\', index=9,\n      number=11, type=5, cpp_type=1, label=1,\n      has_default_value=True, default_value=256,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'first_stage_positive_balance_fraction\', full_name=\'object_detection.protos.FasterRcnn.first_stage_positive_balance_fraction\', index=10,\n      number=12, type=2, cpp_type=6, label=1,\n      has_default_value=True, default_value=float(0.5),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'first_stage_nms_score_threshold\', full_name=\'object_detection.protos.FasterRcnn.first_stage_nms_score_threshold\', index=11,\n      number=13, type=2, cpp_type=6, label=1,\n      has_default_value=True, default_value=float(0),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'first_stage_nms_iou_threshold\', full_name=\'object_detection.protos.FasterRcnn.first_stage_nms_iou_threshold\', index=12,\n      number=14, type=2, cpp_type=6, label=1,\n      has_default_value=True, default_value=float(0.7),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'first_stage_max_proposals\', full_name=\'object_detection.protos.FasterRcnn.first_stage_max_proposals\', index=13,\n      number=15, type=5, cpp_type=1, label=1,\n      has_default_value=True, default_value=300,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'first_stage_localization_loss_weight\', full_name=\'object_detection.protos.FasterRcnn.first_stage_localization_loss_weight\', index=14,\n      number=16, type=2, cpp_type=6, label=1,\n      has_default_value=True, default_value=float(1),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'first_stage_objectness_loss_weight\', full_name=\'object_detection.protos.FasterRcnn.first_stage_objectness_loss_weight\', index=15,\n      number=17, type=2, cpp_type=6, label=1,\n      has_default_value=True, default_value=float(1),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'initial_crop_size\', full_name=\'object_detection.protos.FasterRcnn.initial_crop_size\', index=16,\n      number=18, type=5, cpp_type=1, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'maxpool_kernel_size\', full_name=\'object_detection.protos.FasterRcnn.maxpool_kernel_size\', index=17,\n      number=19, type=5, cpp_type=1, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'maxpool_stride\', full_name=\'object_detection.protos.FasterRcnn.maxpool_stride\', index=18,\n      number=20, type=5, cpp_type=1, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'second_stage_box_predictor\', full_name=\'object_detection.protos.FasterRcnn.second_stage_box_predictor\', index=19,\n      number=21, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'second_stage_batch_size\', full_name=\'object_detection.protos.FasterRcnn.second_stage_batch_size\', index=20,\n      number=22, type=5, cpp_type=1, label=1,\n      has_default_value=True, default_value=64,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'second_stage_balance_fraction\', full_name=\'object_detection.protos.FasterRcnn.second_stage_balance_fraction\', index=21,\n      number=23, type=2, cpp_type=6, label=1,\n      has_default_value=True, default_value=float(0.25),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'second_stage_post_processing\', full_name=\'object_detection.protos.FasterRcnn.second_stage_post_processing\', index=22,\n      number=24, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'second_stage_localization_loss_weight\', full_name=\'object_detection.protos.FasterRcnn.second_stage_localization_loss_weight\', index=23,\n      number=25, type=2, cpp_type=6, label=1,\n      has_default_value=True, default_value=float(1),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'second_stage_classification_loss_weight\', full_name=\'object_detection.protos.FasterRcnn.second_stage_classification_loss_weight\', index=24,\n      number=26, type=2, cpp_type=6, label=1,\n      has_default_value=True, default_value=float(1),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'second_stage_mask_prediction_loss_weight\', full_name=\'object_detection.protos.FasterRcnn.second_stage_mask_prediction_loss_weight\', index=25,\n      number=27, type=2, cpp_type=6, label=1,\n      has_default_value=True, default_value=float(1),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'hard_example_miner\', full_name=\'object_detection.protos.FasterRcnn.hard_example_miner\', index=26,\n      number=28, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'second_stage_classification_loss\', full_name=\'object_detection.protos.FasterRcnn.second_stage_classification_loss\', index=27,\n      number=29, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto2\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=337,\n  serialized_end=1789,\n)\n\n\n_FASTERRCNNFEATUREEXTRACTOR = _descriptor.Descriptor(\n  name=\'FasterRcnnFeatureExtractor\',\n  full_name=\'object_detection.protos.FasterRcnnFeatureExtractor\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'type\', full_name=\'object_detection.protos.FasterRcnnFeatureExtractor.type\', index=0,\n      number=1, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=_b("""").decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'first_stage_features_stride\', full_name=\'object_detection.protos.FasterRcnnFeatureExtractor.first_stage_features_stride\', index=1,\n      number=2, type=5, cpp_type=1, label=1,\n      has_default_value=True, default_value=16,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'batch_norm_trainable\', full_name=\'object_detection.protos.FasterRcnnFeatureExtractor.batch_norm_trainable\', index=2,\n      number=3, type=8, cpp_type=7, label=1,\n      has_default_value=True, default_value=False,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto2\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=1791,\n  serialized_end=1911,\n)\n\n_FASTERRCNN.fields_by_name[\'image_resizer\'].message_type = object__detection_dot_protos_dot_image__resizer__pb2._IMAGERESIZER\n_FASTERRCNN.fields_by_name[\'feature_extractor\'].message_type = _FASTERRCNNFEATUREEXTRACTOR\n_FASTERRCNN.fields_by_name[\'first_stage_anchor_generator\'].message_type = object__detection_dot_protos_dot_anchor__generator__pb2._ANCHORGENERATOR\n_FASTERRCNN.fields_by_name[\'first_stage_box_predictor_conv_hyperparams\'].message_type = object__detection_dot_protos_dot_hyperparams__pb2._HYPERPARAMS\n_FASTERRCNN.fields_by_name[\'second_stage_box_predictor\'].message_type = object__detection_dot_protos_dot_box__predictor__pb2._BOXPREDICTOR\n_FASTERRCNN.fields_by_name[\'second_stage_post_processing\'].message_type = object__detection_dot_protos_dot_post__processing__pb2._POSTPROCESSING\n_FASTERRCNN.fields_by_name[\'hard_example_miner\'].message_type = object__detection_dot_protos_dot_losses__pb2._HARDEXAMPLEMINER\n_FASTERRCNN.fields_by_name[\'second_stage_classification_loss\'].message_type = object__detection_dot_protos_dot_losses__pb2._CLASSIFICATIONLOSS\nDESCRIPTOR.message_types_by_name[\'FasterRcnn\'] = _FASTERRCNN\nDESCRIPTOR.message_types_by_name[\'FasterRcnnFeatureExtractor\'] = _FASTERRCNNFEATUREEXTRACTOR\n_sym_db.RegisterFileDescriptor(DESCRIPTOR)\n\nFasterRcnn = _reflection.GeneratedProtocolMessageType(\'FasterRcnn\', (_message.Message,), dict(\n  DESCRIPTOR = _FASTERRCNN,\n  __module__ = \'object_detection.protos.faster_rcnn_pb2\'\n  # @@protoc_insertion_point(class_scope:object_detection.protos.FasterRcnn)\n  ))\n_sym_db.RegisterMessage(FasterRcnn)\n\nFasterRcnnFeatureExtractor = _reflection.GeneratedProtocolMessageType(\'FasterRcnnFeatureExtractor\', (_message.Message,), dict(\n  DESCRIPTOR = _FASTERRCNNFEATUREEXTRACTOR,\n  __module__ = \'object_detection.protos.faster_rcnn_pb2\'\n  # @@protoc_insertion_point(class_scope:object_detection.protos.FasterRcnnFeatureExtractor)\n  ))\n_sym_db.RegisterMessage(FasterRcnnFeatureExtractor)\n\n\n# @@protoc_insertion_point(module_scope)\n'"
src/object_detection/protos/grid_anchor_generator_pb2.py,0,"b'# Generated by the protocol buffer compiler.  DO NOT EDIT!\n# source: object_detection/protos/grid_anchor_generator.proto\n\nimport sys\n_b=sys.version_info[0]<3 and (lambda x:x) or (lambda x:x.encode(\'latin1\'))\nfrom google.protobuf import descriptor as _descriptor\nfrom google.protobuf import message as _message\nfrom google.protobuf import reflection as _reflection\nfrom google.protobuf import symbol_database as _symbol_database\nfrom google.protobuf import descriptor_pb2\n# @@protoc_insertion_point(imports)\n\n_sym_db = _symbol_database.Default()\n\n\n\n\nDESCRIPTOR = _descriptor.FileDescriptor(\n  name=\'object_detection/protos/grid_anchor_generator.proto\',\n  package=\'object_detection.protos\',\n  syntax=\'proto2\',\n  serialized_pb=_b(\'\\n3object_detection/protos/grid_anchor_generator.proto\\x12\\x17object_detection.protos\\""\\xcd\\x01\\n\\x13GridAnchorGenerator\\x12\\x13\\n\\x06height\\x18\\x01 \\x01(\\x05:\\x03\\x32\\x35\\x36\\x12\\x12\\n\\x05width\\x18\\x02 \\x01(\\x05:\\x03\\x32\\x35\\x36\\x12\\x19\\n\\rheight_stride\\x18\\x03 \\x01(\\x05:\\x02\\x31\\x36\\x12\\x18\\n\\x0cwidth_stride\\x18\\x04 \\x01(\\x05:\\x02\\x31\\x36\\x12\\x18\\n\\rheight_offset\\x18\\x05 \\x01(\\x05:\\x01\\x30\\x12\\x17\\n\\x0cwidth_offset\\x18\\x06 \\x01(\\x05:\\x01\\x30\\x12\\x0e\\n\\x06scales\\x18\\x07 \\x03(\\x02\\x12\\x15\\n\\raspect_ratios\\x18\\x08 \\x03(\\x02\')\n)\n\n\n\n\n_GRIDANCHORGENERATOR = _descriptor.Descriptor(\n  name=\'GridAnchorGenerator\',\n  full_name=\'object_detection.protos.GridAnchorGenerator\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'height\', full_name=\'object_detection.protos.GridAnchorGenerator.height\', index=0,\n      number=1, type=5, cpp_type=1, label=1,\n      has_default_value=True, default_value=256,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'width\', full_name=\'object_detection.protos.GridAnchorGenerator.width\', index=1,\n      number=2, type=5, cpp_type=1, label=1,\n      has_default_value=True, default_value=256,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'height_stride\', full_name=\'object_detection.protos.GridAnchorGenerator.height_stride\', index=2,\n      number=3, type=5, cpp_type=1, label=1,\n      has_default_value=True, default_value=16,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'width_stride\', full_name=\'object_detection.protos.GridAnchorGenerator.width_stride\', index=3,\n      number=4, type=5, cpp_type=1, label=1,\n      has_default_value=True, default_value=16,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'height_offset\', full_name=\'object_detection.protos.GridAnchorGenerator.height_offset\', index=4,\n      number=5, type=5, cpp_type=1, label=1,\n      has_default_value=True, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'width_offset\', full_name=\'object_detection.protos.GridAnchorGenerator.width_offset\', index=5,\n      number=6, type=5, cpp_type=1, label=1,\n      has_default_value=True, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'scales\', full_name=\'object_detection.protos.GridAnchorGenerator.scales\', index=6,\n      number=7, type=2, cpp_type=6, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'aspect_ratios\', full_name=\'object_detection.protos.GridAnchorGenerator.aspect_ratios\', index=7,\n      number=8, type=2, cpp_type=6, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto2\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=81,\n  serialized_end=286,\n)\n\nDESCRIPTOR.message_types_by_name[\'GridAnchorGenerator\'] = _GRIDANCHORGENERATOR\n_sym_db.RegisterFileDescriptor(DESCRIPTOR)\n\nGridAnchorGenerator = _reflection.GeneratedProtocolMessageType(\'GridAnchorGenerator\', (_message.Message,), dict(\n  DESCRIPTOR = _GRIDANCHORGENERATOR,\n  __module__ = \'object_detection.protos.grid_anchor_generator_pb2\'\n  # @@protoc_insertion_point(class_scope:object_detection.protos.GridAnchorGenerator)\n  ))\n_sym_db.RegisterMessage(GridAnchorGenerator)\n\n\n# @@protoc_insertion_point(module_scope)\n'"
src/object_detection/protos/hyperparams_pb2.py,0,"b'# Generated by the protocol buffer compiler.  DO NOT EDIT!\n# source: object_detection/protos/hyperparams.proto\n\nimport sys\n_b=sys.version_info[0]<3 and (lambda x:x) or (lambda x:x.encode(\'latin1\'))\nfrom google.protobuf import descriptor as _descriptor\nfrom google.protobuf import message as _message\nfrom google.protobuf import reflection as _reflection\nfrom google.protobuf import symbol_database as _symbol_database\nfrom google.protobuf import descriptor_pb2\n# @@protoc_insertion_point(imports)\n\n_sym_db = _symbol_database.Default()\n\n\n\n\nDESCRIPTOR = _descriptor.FileDescriptor(\n  name=\'object_detection/protos/hyperparams.proto\',\n  package=\'object_detection.protos\',\n  syntax=\'proto2\',\n  serialized_pb=_b(\'\\n)object_detection/protos/hyperparams.proto\\x12\\x17object_detection.protos\\""\\x87\\x03\\n\\x0bHyperparams\\x12\\x39\\n\\x02op\\x18\\x01 \\x01(\\x0e\\x32\\\'.object_detection.protos.Hyperparams.Op:\\x04\\x43ONV\\x12\\x39\\n\\x0bregularizer\\x18\\x02 \\x01(\\x0b\\x32$.object_detection.protos.Regularizer\\x12\\x39\\n\\x0binitializer\\x18\\x03 \\x01(\\x0b\\x32$.object_detection.protos.Initializer\\x12I\\n\\nactivation\\x18\\x04 \\x01(\\x0e\\x32/.object_detection.protos.Hyperparams.Activation:\\x04RELU\\x12\\x36\\n\\nbatch_norm\\x18\\x05 \\x01(\\x0b\\x32\\"".object_detection.protos.BatchNorm\\""\\x16\\n\\x02Op\\x12\\x08\\n\\x04\\x43ONV\\x10\\x01\\x12\\x06\\n\\x02\\x46\\x43\\x10\\x02\\"",\\n\\nActivation\\x12\\x08\\n\\x04NONE\\x10\\x00\\x12\\x08\\n\\x04RELU\\x10\\x01\\x12\\n\\n\\x06RELU_6\\x10\\x02\\""\\xa6\\x01\\n\\x0bRegularizer\\x12@\\n\\x0el1_regularizer\\x18\\x01 \\x01(\\x0b\\x32&.object_detection.protos.L1RegularizerH\\x00\\x12@\\n\\x0el2_regularizer\\x18\\x02 \\x01(\\x0b\\x32&.object_detection.protos.L2RegularizerH\\x00\\x42\\x13\\n\\x11regularizer_oneof\\""\\""\\n\\rL1Regularizer\\x12\\x11\\n\\x06weight\\x18\\x01 \\x01(\\x02:\\x01\\x31\\""\\""\\n\\rL2Regularizer\\x12\\x11\\n\\x06weight\\x18\\x01 \\x01(\\x02:\\x01\\x31\\""\\xb3\\x02\\n\\x0bInitializer\\x12[\\n\\x1ctruncated_normal_initializer\\x18\\x01 \\x01(\\x0b\\x32\\x33.object_detection.protos.TruncatedNormalInitializerH\\x00\\x12[\\n\\x1cvariance_scaling_initializer\\x18\\x02 \\x01(\\x0b\\x32\\x33.object_detection.protos.VarianceScalingInitializerH\\x00\\x12U\\n\\x19random_normal_initializer\\x18\\x03 \\x01(\\x0b\\x32\\x30.object_detection.protos.RandomNormalInitializerH\\x00\\x42\\x13\\n\\x11initializer_oneof\\""@\\n\\x1aTruncatedNormalInitializer\\x12\\x0f\\n\\x04mean\\x18\\x01 \\x01(\\x02:\\x01\\x30\\x12\\x11\\n\\x06stddev\\x18\\x02 \\x01(\\x02:\\x01\\x31\\""\\xc5\\x01\\n\\x1aVarianceScalingInitializer\\x12\\x11\\n\\x06\\x66\\x61\\x63tor\\x18\\x01 \\x01(\\x02:\\x01\\x32\\x12\\x16\\n\\x07uniform\\x18\\x02 \\x01(\\x08:\\x05\\x66\\x61lse\\x12N\\n\\x04mode\\x18\\x03 \\x01(\\x0e\\x32\\x38.object_detection.protos.VarianceScalingInitializer.Mode:\\x06\\x46\\x41N_IN\\"",\\n\\x04Mode\\x12\\n\\n\\x06\\x46\\x41N_IN\\x10\\x00\\x12\\x0b\\n\\x07\\x46\\x41N_OUT\\x10\\x01\\x12\\x0b\\n\\x07\\x46\\x41N_AVG\\x10\\x02\\""=\\n\\x17RandomNormalInitializer\\x12\\x0f\\n\\x04mean\\x18\\x01 \\x01(\\x02:\\x01\\x30\\x12\\x11\\n\\x06stddev\\x18\\x02 \\x01(\\x02:\\x01\\x31\\""z\\n\\tBatchNorm\\x12\\x14\\n\\x05\\x64\\x65\\x63\\x61y\\x18\\x01 \\x01(\\x02:\\x05\\x30.999\\x12\\x14\\n\\x06\\x63\\x65nter\\x18\\x02 \\x01(\\x08:\\x04true\\x12\\x14\\n\\x05scale\\x18\\x03 \\x01(\\x08:\\x05\\x66\\x61lse\\x12\\x16\\n\\x07\\x65psilon\\x18\\x04 \\x01(\\x02:\\x05\\x30.001\\x12\\x13\\n\\x05train\\x18\\x05 \\x01(\\x08:\\x04true\')\n)\n\n\n\n_HYPERPARAMS_OP = _descriptor.EnumDescriptor(\n  name=\'Op\',\n  full_name=\'object_detection.protos.Hyperparams.Op\',\n  filename=None,\n  file=DESCRIPTOR,\n  values=[\n    _descriptor.EnumValueDescriptor(\n      name=\'CONV\', index=0, number=1,\n      options=None,\n      type=None),\n    _descriptor.EnumValueDescriptor(\n      name=\'FC\', index=1, number=2,\n      options=None,\n      type=None),\n  ],\n  containing_type=None,\n  options=None,\n  serialized_start=394,\n  serialized_end=416,\n)\n_sym_db.RegisterEnumDescriptor(_HYPERPARAMS_OP)\n\n_HYPERPARAMS_ACTIVATION = _descriptor.EnumDescriptor(\n  name=\'Activation\',\n  full_name=\'object_detection.protos.Hyperparams.Activation\',\n  filename=None,\n  file=DESCRIPTOR,\n  values=[\n    _descriptor.EnumValueDescriptor(\n      name=\'NONE\', index=0, number=0,\n      options=None,\n      type=None),\n    _descriptor.EnumValueDescriptor(\n      name=\'RELU\', index=1, number=1,\n      options=None,\n      type=None),\n    _descriptor.EnumValueDescriptor(\n      name=\'RELU_6\', index=2, number=2,\n      options=None,\n      type=None),\n  ],\n  containing_type=None,\n  options=None,\n  serialized_start=418,\n  serialized_end=462,\n)\n_sym_db.RegisterEnumDescriptor(_HYPERPARAMS_ACTIVATION)\n\n_VARIANCESCALINGINITIALIZER_MODE = _descriptor.EnumDescriptor(\n  name=\'Mode\',\n  full_name=\'object_detection.protos.VarianceScalingInitializer.Mode\',\n  filename=None,\n  file=DESCRIPTOR,\n  values=[\n    _descriptor.EnumValueDescriptor(\n      name=\'FAN_IN\', index=0, number=0,\n      options=None,\n      type=None),\n    _descriptor.EnumValueDescriptor(\n      name=\'FAN_OUT\', index=1, number=1,\n      options=None,\n      type=None),\n    _descriptor.EnumValueDescriptor(\n      name=\'FAN_AVG\', index=2, number=2,\n      options=None,\n      type=None),\n  ],\n  containing_type=None,\n  options=None,\n  serialized_start=1235,\n  serialized_end=1279,\n)\n_sym_db.RegisterEnumDescriptor(_VARIANCESCALINGINITIALIZER_MODE)\n\n\n_HYPERPARAMS = _descriptor.Descriptor(\n  name=\'Hyperparams\',\n  full_name=\'object_detection.protos.Hyperparams\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'op\', full_name=\'object_detection.protos.Hyperparams.op\', index=0,\n      number=1, type=14, cpp_type=8, label=1,\n      has_default_value=True, default_value=1,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'regularizer\', full_name=\'object_detection.protos.Hyperparams.regularizer\', index=1,\n      number=2, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'initializer\', full_name=\'object_detection.protos.Hyperparams.initializer\', index=2,\n      number=3, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'activation\', full_name=\'object_detection.protos.Hyperparams.activation\', index=3,\n      number=4, type=14, cpp_type=8, label=1,\n      has_default_value=True, default_value=1,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'batch_norm\', full_name=\'object_detection.protos.Hyperparams.batch_norm\', index=4,\n      number=5, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n    _HYPERPARAMS_OP,\n    _HYPERPARAMS_ACTIVATION,\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto2\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=71,\n  serialized_end=462,\n)\n\n\n_REGULARIZER = _descriptor.Descriptor(\n  name=\'Regularizer\',\n  full_name=\'object_detection.protos.Regularizer\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'l1_regularizer\', full_name=\'object_detection.protos.Regularizer.l1_regularizer\', index=0,\n      number=1, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'l2_regularizer\', full_name=\'object_detection.protos.Regularizer.l2_regularizer\', index=1,\n      number=2, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto2\',\n  extension_ranges=[],\n  oneofs=[\n    _descriptor.OneofDescriptor(\n      name=\'regularizer_oneof\', full_name=\'object_detection.protos.Regularizer.regularizer_oneof\',\n      index=0, containing_type=None, fields=[]),\n  ],\n  serialized_start=465,\n  serialized_end=631,\n)\n\n\n_L1REGULARIZER = _descriptor.Descriptor(\n  name=\'L1Regularizer\',\n  full_name=\'object_detection.protos.L1Regularizer\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'weight\', full_name=\'object_detection.protos.L1Regularizer.weight\', index=0,\n      number=1, type=2, cpp_type=6, label=1,\n      has_default_value=True, default_value=float(1),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto2\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=633,\n  serialized_end=667,\n)\n\n\n_L2REGULARIZER = _descriptor.Descriptor(\n  name=\'L2Regularizer\',\n  full_name=\'object_detection.protos.L2Regularizer\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'weight\', full_name=\'object_detection.protos.L2Regularizer.weight\', index=0,\n      number=1, type=2, cpp_type=6, label=1,\n      has_default_value=True, default_value=float(1),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto2\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=669,\n  serialized_end=703,\n)\n\n\n_INITIALIZER = _descriptor.Descriptor(\n  name=\'Initializer\',\n  full_name=\'object_detection.protos.Initializer\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'truncated_normal_initializer\', full_name=\'object_detection.protos.Initializer.truncated_normal_initializer\', index=0,\n      number=1, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'variance_scaling_initializer\', full_name=\'object_detection.protos.Initializer.variance_scaling_initializer\', index=1,\n      number=2, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'random_normal_initializer\', full_name=\'object_detection.protos.Initializer.random_normal_initializer\', index=2,\n      number=3, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto2\',\n  extension_ranges=[],\n  oneofs=[\n    _descriptor.OneofDescriptor(\n      name=\'initializer_oneof\', full_name=\'object_detection.protos.Initializer.initializer_oneof\',\n      index=0, containing_type=None, fields=[]),\n  ],\n  serialized_start=706,\n  serialized_end=1013,\n)\n\n\n_TRUNCATEDNORMALINITIALIZER = _descriptor.Descriptor(\n  name=\'TruncatedNormalInitializer\',\n  full_name=\'object_detection.protos.TruncatedNormalInitializer\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'mean\', full_name=\'object_detection.protos.TruncatedNormalInitializer.mean\', index=0,\n      number=1, type=2, cpp_type=6, label=1,\n      has_default_value=True, default_value=float(0),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'stddev\', full_name=\'object_detection.protos.TruncatedNormalInitializer.stddev\', index=1,\n      number=2, type=2, cpp_type=6, label=1,\n      has_default_value=True, default_value=float(1),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto2\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=1015,\n  serialized_end=1079,\n)\n\n\n_VARIANCESCALINGINITIALIZER = _descriptor.Descriptor(\n  name=\'VarianceScalingInitializer\',\n  full_name=\'object_detection.protos.VarianceScalingInitializer\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'factor\', full_name=\'object_detection.protos.VarianceScalingInitializer.factor\', index=0,\n      number=1, type=2, cpp_type=6, label=1,\n      has_default_value=True, default_value=float(2),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'uniform\', full_name=\'object_detection.protos.VarianceScalingInitializer.uniform\', index=1,\n      number=2, type=8, cpp_type=7, label=1,\n      has_default_value=True, default_value=False,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'mode\', full_name=\'object_detection.protos.VarianceScalingInitializer.mode\', index=2,\n      number=3, type=14, cpp_type=8, label=1,\n      has_default_value=True, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n    _VARIANCESCALINGINITIALIZER_MODE,\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto2\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=1082,\n  serialized_end=1279,\n)\n\n\n_RANDOMNORMALINITIALIZER = _descriptor.Descriptor(\n  name=\'RandomNormalInitializer\',\n  full_name=\'object_detection.protos.RandomNormalInitializer\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'mean\', full_name=\'object_detection.protos.RandomNormalInitializer.mean\', index=0,\n      number=1, type=2, cpp_type=6, label=1,\n      has_default_value=True, default_value=float(0),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'stddev\', full_name=\'object_detection.protos.RandomNormalInitializer.stddev\', index=1,\n      number=2, type=2, cpp_type=6, label=1,\n      has_default_value=True, default_value=float(1),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto2\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=1281,\n  serialized_end=1342,\n)\n\n\n_BATCHNORM = _descriptor.Descriptor(\n  name=\'BatchNorm\',\n  full_name=\'object_detection.protos.BatchNorm\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'decay\', full_name=\'object_detection.protos.BatchNorm.decay\', index=0,\n      number=1, type=2, cpp_type=6, label=1,\n      has_default_value=True, default_value=float(0.999),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'center\', full_name=\'object_detection.protos.BatchNorm.center\', index=1,\n      number=2, type=8, cpp_type=7, label=1,\n      has_default_value=True, default_value=True,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'scale\', full_name=\'object_detection.protos.BatchNorm.scale\', index=2,\n      number=3, type=8, cpp_type=7, label=1,\n      has_default_value=True, default_value=False,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'epsilon\', full_name=\'object_detection.protos.BatchNorm.epsilon\', index=3,\n      number=4, type=2, cpp_type=6, label=1,\n      has_default_value=True, default_value=float(0.001),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'train\', full_name=\'object_detection.protos.BatchNorm.train\', index=4,\n      number=5, type=8, cpp_type=7, label=1,\n      has_default_value=True, default_value=True,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto2\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=1344,\n  serialized_end=1466,\n)\n\n_HYPERPARAMS.fields_by_name[\'op\'].enum_type = _HYPERPARAMS_OP\n_HYPERPARAMS.fields_by_name[\'regularizer\'].message_type = _REGULARIZER\n_HYPERPARAMS.fields_by_name[\'initializer\'].message_type = _INITIALIZER\n_HYPERPARAMS.fields_by_name[\'activation\'].enum_type = _HYPERPARAMS_ACTIVATION\n_HYPERPARAMS.fields_by_name[\'batch_norm\'].message_type = _BATCHNORM\n_HYPERPARAMS_OP.containing_type = _HYPERPARAMS\n_HYPERPARAMS_ACTIVATION.containing_type = _HYPERPARAMS\n_REGULARIZER.fields_by_name[\'l1_regularizer\'].message_type = _L1REGULARIZER\n_REGULARIZER.fields_by_name[\'l2_regularizer\'].message_type = _L2REGULARIZER\n_REGULARIZER.oneofs_by_name[\'regularizer_oneof\'].fields.append(\n  _REGULARIZER.fields_by_name[\'l1_regularizer\'])\n_REGULARIZER.fields_by_name[\'l1_regularizer\'].containing_oneof = _REGULARIZER.oneofs_by_name[\'regularizer_oneof\']\n_REGULARIZER.oneofs_by_name[\'regularizer_oneof\'].fields.append(\n  _REGULARIZER.fields_by_name[\'l2_regularizer\'])\n_REGULARIZER.fields_by_name[\'l2_regularizer\'].containing_oneof = _REGULARIZER.oneofs_by_name[\'regularizer_oneof\']\n_INITIALIZER.fields_by_name[\'truncated_normal_initializer\'].message_type = _TRUNCATEDNORMALINITIALIZER\n_INITIALIZER.fields_by_name[\'variance_scaling_initializer\'].message_type = _VARIANCESCALINGINITIALIZER\n_INITIALIZER.fields_by_name[\'random_normal_initializer\'].message_type = _RANDOMNORMALINITIALIZER\n_INITIALIZER.oneofs_by_name[\'initializer_oneof\'].fields.append(\n  _INITIALIZER.fields_by_name[\'truncated_normal_initializer\'])\n_INITIALIZER.fields_by_name[\'truncated_normal_initializer\'].containing_oneof = _INITIALIZER.oneofs_by_name[\'initializer_oneof\']\n_INITIALIZER.oneofs_by_name[\'initializer_oneof\'].fields.append(\n  _INITIALIZER.fields_by_name[\'variance_scaling_initializer\'])\n_INITIALIZER.fields_by_name[\'variance_scaling_initializer\'].containing_oneof = _INITIALIZER.oneofs_by_name[\'initializer_oneof\']\n_INITIALIZER.oneofs_by_name[\'initializer_oneof\'].fields.append(\n  _INITIALIZER.fields_by_name[\'random_normal_initializer\'])\n_INITIALIZER.fields_by_name[\'random_normal_initializer\'].containing_oneof = _INITIALIZER.oneofs_by_name[\'initializer_oneof\']\n_VARIANCESCALINGINITIALIZER.fields_by_name[\'mode\'].enum_type = _VARIANCESCALINGINITIALIZER_MODE\n_VARIANCESCALINGINITIALIZER_MODE.containing_type = _VARIANCESCALINGINITIALIZER\nDESCRIPTOR.message_types_by_name[\'Hyperparams\'] = _HYPERPARAMS\nDESCRIPTOR.message_types_by_name[\'Regularizer\'] = _REGULARIZER\nDESCRIPTOR.message_types_by_name[\'L1Regularizer\'] = _L1REGULARIZER\nDESCRIPTOR.message_types_by_name[\'L2Regularizer\'] = _L2REGULARIZER\nDESCRIPTOR.message_types_by_name[\'Initializer\'] = _INITIALIZER\nDESCRIPTOR.message_types_by_name[\'TruncatedNormalInitializer\'] = _TRUNCATEDNORMALINITIALIZER\nDESCRIPTOR.message_types_by_name[\'VarianceScalingInitializer\'] = _VARIANCESCALINGINITIALIZER\nDESCRIPTOR.message_types_by_name[\'RandomNormalInitializer\'] = _RANDOMNORMALINITIALIZER\nDESCRIPTOR.message_types_by_name[\'BatchNorm\'] = _BATCHNORM\n_sym_db.RegisterFileDescriptor(DESCRIPTOR)\n\nHyperparams = _reflection.GeneratedProtocolMessageType(\'Hyperparams\', (_message.Message,), dict(\n  DESCRIPTOR = _HYPERPARAMS,\n  __module__ = \'object_detection.protos.hyperparams_pb2\'\n  # @@protoc_insertion_point(class_scope:object_detection.protos.Hyperparams)\n  ))\n_sym_db.RegisterMessage(Hyperparams)\n\nRegularizer = _reflection.GeneratedProtocolMessageType(\'Regularizer\', (_message.Message,), dict(\n  DESCRIPTOR = _REGULARIZER,\n  __module__ = \'object_detection.protos.hyperparams_pb2\'\n  # @@protoc_insertion_point(class_scope:object_detection.protos.Regularizer)\n  ))\n_sym_db.RegisterMessage(Regularizer)\n\nL1Regularizer = _reflection.GeneratedProtocolMessageType(\'L1Regularizer\', (_message.Message,), dict(\n  DESCRIPTOR = _L1REGULARIZER,\n  __module__ = \'object_detection.protos.hyperparams_pb2\'\n  # @@protoc_insertion_point(class_scope:object_detection.protos.L1Regularizer)\n  ))\n_sym_db.RegisterMessage(L1Regularizer)\n\nL2Regularizer = _reflection.GeneratedProtocolMessageType(\'L2Regularizer\', (_message.Message,), dict(\n  DESCRIPTOR = _L2REGULARIZER,\n  __module__ = \'object_detection.protos.hyperparams_pb2\'\n  # @@protoc_insertion_point(class_scope:object_detection.protos.L2Regularizer)\n  ))\n_sym_db.RegisterMessage(L2Regularizer)\n\nInitializer = _reflection.GeneratedProtocolMessageType(\'Initializer\', (_message.Message,), dict(\n  DESCRIPTOR = _INITIALIZER,\n  __module__ = \'object_detection.protos.hyperparams_pb2\'\n  # @@protoc_insertion_point(class_scope:object_detection.protos.Initializer)\n  ))\n_sym_db.RegisterMessage(Initializer)\n\nTruncatedNormalInitializer = _reflection.GeneratedProtocolMessageType(\'TruncatedNormalInitializer\', (_message.Message,), dict(\n  DESCRIPTOR = _TRUNCATEDNORMALINITIALIZER,\n  __module__ = \'object_detection.protos.hyperparams_pb2\'\n  # @@protoc_insertion_point(class_scope:object_detection.protos.TruncatedNormalInitializer)\n  ))\n_sym_db.RegisterMessage(TruncatedNormalInitializer)\n\nVarianceScalingInitializer = _reflection.GeneratedProtocolMessageType(\'VarianceScalingInitializer\', (_message.Message,), dict(\n  DESCRIPTOR = _VARIANCESCALINGINITIALIZER,\n  __module__ = \'object_detection.protos.hyperparams_pb2\'\n  # @@protoc_insertion_point(class_scope:object_detection.protos.VarianceScalingInitializer)\n  ))\n_sym_db.RegisterMessage(VarianceScalingInitializer)\n\nRandomNormalInitializer = _reflection.GeneratedProtocolMessageType(\'RandomNormalInitializer\', (_message.Message,), dict(\n  DESCRIPTOR = _RANDOMNORMALINITIALIZER,\n  __module__ = \'object_detection.protos.hyperparams_pb2\'\n  # @@protoc_insertion_point(class_scope:object_detection.protos.RandomNormalInitializer)\n  ))\n_sym_db.RegisterMessage(RandomNormalInitializer)\n\nBatchNorm = _reflection.GeneratedProtocolMessageType(\'BatchNorm\', (_message.Message,), dict(\n  DESCRIPTOR = _BATCHNORM,\n  __module__ = \'object_detection.protos.hyperparams_pb2\'\n  # @@protoc_insertion_point(class_scope:object_detection.protos.BatchNorm)\n  ))\n_sym_db.RegisterMessage(BatchNorm)\n\n\n# @@protoc_insertion_point(module_scope)\n'"
src/object_detection/protos/image_resizer_pb2.py,0,"b'# Generated by the protocol buffer compiler.  DO NOT EDIT!\n# source: object_detection/protos/image_resizer.proto\n\nimport sys\n_b=sys.version_info[0]<3 and (lambda x:x) or (lambda x:x.encode(\'latin1\'))\nfrom google.protobuf.internal import enum_type_wrapper\nfrom google.protobuf import descriptor as _descriptor\nfrom google.protobuf import message as _message\nfrom google.protobuf import reflection as _reflection\nfrom google.protobuf import symbol_database as _symbol_database\nfrom google.protobuf import descriptor_pb2\n# @@protoc_insertion_point(imports)\n\n_sym_db = _symbol_database.Default()\n\n\n\n\nDESCRIPTOR = _descriptor.FileDescriptor(\n  name=\'object_detection/protos/image_resizer.proto\',\n  package=\'object_detection.protos\',\n  syntax=\'proto2\',\n  serialized_pb=_b(\'\\n+object_detection/protos/image_resizer.proto\\x12\\x17object_detection.protos\\""\\xc6\\x01\\n\\x0cImageResizer\\x12T\\n\\x19keep_aspect_ratio_resizer\\x18\\x01 \\x01(\\x0b\\x32/.object_detection.protos.KeepAspectRatioResizerH\\x00\\x12I\\n\\x13\\x66ixed_shape_resizer\\x18\\x02 \\x01(\\x0b\\x32*.object_detection.protos.FixedShapeResizerH\\x00\\x42\\x15\\n\\x13image_resizer_oneof\\""\\xe1\\x01\\n\\x16KeepAspectRatioResizer\\x12\\x1a\\n\\rmin_dimension\\x18\\x01 \\x01(\\x05:\\x03\\x36\\x30\\x30\\x12\\x1b\\n\\rmax_dimension\\x18\\x02 \\x01(\\x05:\\x04\\x31\\x30\\x32\\x34\\x12\\x44\\n\\rresize_method\\x18\\x03 \\x01(\\x0e\\x32#.object_detection.protos.ResizeType:\\x08\\x42ILINEAR\\x12#\\n\\x14pad_to_max_dimension\\x18\\x04 \\x01(\\x08:\\x05\\x66\\x61lse\\x12#\\n\\x14\\x63onvert_to_grayscale\\x18\\x05 \\x01(\\x08:\\x05\\x66\\x61lse\\""\\xa7\\x01\\n\\x11\\x46ixedShapeResizer\\x12\\x13\\n\\x06height\\x18\\x01 \\x01(\\x05:\\x03\\x33\\x30\\x30\\x12\\x12\\n\\x05width\\x18\\x02 \\x01(\\x05:\\x03\\x33\\x30\\x30\\x12\\x44\\n\\rresize_method\\x18\\x03 \\x01(\\x0e\\x32#.object_detection.protos.ResizeType:\\x08\\x42ILINEAR\\x12#\\n\\x14\\x63onvert_to_grayscale\\x18\\x04 \\x01(\\x08:\\x05\\x66\\x61lse*G\\n\\nResizeType\\x12\\x0c\\n\\x08\\x42ILINEAR\\x10\\x00\\x12\\x14\\n\\x10NEAREST_NEIGHBOR\\x10\\x01\\x12\\x0b\\n\\x07\\x42ICUBIC\\x10\\x02\\x12\\x08\\n\\x04\\x41REA\\x10\\x03\')\n)\n\n_RESIZETYPE = _descriptor.EnumDescriptor(\n  name=\'ResizeType\',\n  full_name=\'object_detection.protos.ResizeType\',\n  filename=None,\n  file=DESCRIPTOR,\n  values=[\n    _descriptor.EnumValueDescriptor(\n      name=\'BILINEAR\', index=0, number=0,\n      options=None,\n      type=None),\n    _descriptor.EnumValueDescriptor(\n      name=\'NEAREST_NEIGHBOR\', index=1, number=1,\n      options=None,\n      type=None),\n    _descriptor.EnumValueDescriptor(\n      name=\'BICUBIC\', index=2, number=2,\n      options=None,\n      type=None),\n    _descriptor.EnumValueDescriptor(\n      name=\'AREA\', index=3, number=3,\n      options=None,\n      type=None),\n  ],\n  containing_type=None,\n  options=None,\n  serialized_start=671,\n  serialized_end=742,\n)\n_sym_db.RegisterEnumDescriptor(_RESIZETYPE)\n\nResizeType = enum_type_wrapper.EnumTypeWrapper(_RESIZETYPE)\nBILINEAR = 0\nNEAREST_NEIGHBOR = 1\nBICUBIC = 2\nAREA = 3\n\n\n\n_IMAGERESIZER = _descriptor.Descriptor(\n  name=\'ImageResizer\',\n  full_name=\'object_detection.protos.ImageResizer\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'keep_aspect_ratio_resizer\', full_name=\'object_detection.protos.ImageResizer.keep_aspect_ratio_resizer\', index=0,\n      number=1, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'fixed_shape_resizer\', full_name=\'object_detection.protos.ImageResizer.fixed_shape_resizer\', index=1,\n      number=2, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto2\',\n  extension_ranges=[],\n  oneofs=[\n    _descriptor.OneofDescriptor(\n      name=\'image_resizer_oneof\', full_name=\'object_detection.protos.ImageResizer.image_resizer_oneof\',\n      index=0, containing_type=None, fields=[]),\n  ],\n  serialized_start=73,\n  serialized_end=271,\n)\n\n\n_KEEPASPECTRATIORESIZER = _descriptor.Descriptor(\n  name=\'KeepAspectRatioResizer\',\n  full_name=\'object_detection.protos.KeepAspectRatioResizer\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'min_dimension\', full_name=\'object_detection.protos.KeepAspectRatioResizer.min_dimension\', index=0,\n      number=1, type=5, cpp_type=1, label=1,\n      has_default_value=True, default_value=600,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'max_dimension\', full_name=\'object_detection.protos.KeepAspectRatioResizer.max_dimension\', index=1,\n      number=2, type=5, cpp_type=1, label=1,\n      has_default_value=True, default_value=1024,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'resize_method\', full_name=\'object_detection.protos.KeepAspectRatioResizer.resize_method\', index=2,\n      number=3, type=14, cpp_type=8, label=1,\n      has_default_value=True, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'pad_to_max_dimension\', full_name=\'object_detection.protos.KeepAspectRatioResizer.pad_to_max_dimension\', index=3,\n      number=4, type=8, cpp_type=7, label=1,\n      has_default_value=True, default_value=False,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'convert_to_grayscale\', full_name=\'object_detection.protos.KeepAspectRatioResizer.convert_to_grayscale\', index=4,\n      number=5, type=8, cpp_type=7, label=1,\n      has_default_value=True, default_value=False,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto2\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=274,\n  serialized_end=499,\n)\n\n\n_FIXEDSHAPERESIZER = _descriptor.Descriptor(\n  name=\'FixedShapeResizer\',\n  full_name=\'object_detection.protos.FixedShapeResizer\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'height\', full_name=\'object_detection.protos.FixedShapeResizer.height\', index=0,\n      number=1, type=5, cpp_type=1, label=1,\n      has_default_value=True, default_value=300,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'width\', full_name=\'object_detection.protos.FixedShapeResizer.width\', index=1,\n      number=2, type=5, cpp_type=1, label=1,\n      has_default_value=True, default_value=300,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'resize_method\', full_name=\'object_detection.protos.FixedShapeResizer.resize_method\', index=2,\n      number=3, type=14, cpp_type=8, label=1,\n      has_default_value=True, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'convert_to_grayscale\', full_name=\'object_detection.protos.FixedShapeResizer.convert_to_grayscale\', index=3,\n      number=4, type=8, cpp_type=7, label=1,\n      has_default_value=True, default_value=False,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto2\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=502,\n  serialized_end=669,\n)\n\n_IMAGERESIZER.fields_by_name[\'keep_aspect_ratio_resizer\'].message_type = _KEEPASPECTRATIORESIZER\n_IMAGERESIZER.fields_by_name[\'fixed_shape_resizer\'].message_type = _FIXEDSHAPERESIZER\n_IMAGERESIZER.oneofs_by_name[\'image_resizer_oneof\'].fields.append(\n  _IMAGERESIZER.fields_by_name[\'keep_aspect_ratio_resizer\'])\n_IMAGERESIZER.fields_by_name[\'keep_aspect_ratio_resizer\'].containing_oneof = _IMAGERESIZER.oneofs_by_name[\'image_resizer_oneof\']\n_IMAGERESIZER.oneofs_by_name[\'image_resizer_oneof\'].fields.append(\n  _IMAGERESIZER.fields_by_name[\'fixed_shape_resizer\'])\n_IMAGERESIZER.fields_by_name[\'fixed_shape_resizer\'].containing_oneof = _IMAGERESIZER.oneofs_by_name[\'image_resizer_oneof\']\n_KEEPASPECTRATIORESIZER.fields_by_name[\'resize_method\'].enum_type = _RESIZETYPE\n_FIXEDSHAPERESIZER.fields_by_name[\'resize_method\'].enum_type = _RESIZETYPE\nDESCRIPTOR.message_types_by_name[\'ImageResizer\'] = _IMAGERESIZER\nDESCRIPTOR.message_types_by_name[\'KeepAspectRatioResizer\'] = _KEEPASPECTRATIORESIZER\nDESCRIPTOR.message_types_by_name[\'FixedShapeResizer\'] = _FIXEDSHAPERESIZER\nDESCRIPTOR.enum_types_by_name[\'ResizeType\'] = _RESIZETYPE\n_sym_db.RegisterFileDescriptor(DESCRIPTOR)\n\nImageResizer = _reflection.GeneratedProtocolMessageType(\'ImageResizer\', (_message.Message,), dict(\n  DESCRIPTOR = _IMAGERESIZER,\n  __module__ = \'object_detection.protos.image_resizer_pb2\'\n  # @@protoc_insertion_point(class_scope:object_detection.protos.ImageResizer)\n  ))\n_sym_db.RegisterMessage(ImageResizer)\n\nKeepAspectRatioResizer = _reflection.GeneratedProtocolMessageType(\'KeepAspectRatioResizer\', (_message.Message,), dict(\n  DESCRIPTOR = _KEEPASPECTRATIORESIZER,\n  __module__ = \'object_detection.protos.image_resizer_pb2\'\n  # @@protoc_insertion_point(class_scope:object_detection.protos.KeepAspectRatioResizer)\n  ))\n_sym_db.RegisterMessage(KeepAspectRatioResizer)\n\nFixedShapeResizer = _reflection.GeneratedProtocolMessageType(\'FixedShapeResizer\', (_message.Message,), dict(\n  DESCRIPTOR = _FIXEDSHAPERESIZER,\n  __module__ = \'object_detection.protos.image_resizer_pb2\'\n  # @@protoc_insertion_point(class_scope:object_detection.protos.FixedShapeResizer)\n  ))\n_sym_db.RegisterMessage(FixedShapeResizer)\n\n\n# @@protoc_insertion_point(module_scope)\n'"
src/object_detection/protos/input_reader_pb2.py,0,"b'# Generated by the protocol buffer compiler.  DO NOT EDIT!\n# source: object_detection/protos/input_reader.proto\n\nimport sys\n_b=sys.version_info[0]<3 and (lambda x:x) or (lambda x:x.encode(\'latin1\'))\nfrom google.protobuf.internal import enum_type_wrapper\nfrom google.protobuf import descriptor as _descriptor\nfrom google.protobuf import message as _message\nfrom google.protobuf import reflection as _reflection\nfrom google.protobuf import symbol_database as _symbol_database\nfrom google.protobuf import descriptor_pb2\n# @@protoc_insertion_point(imports)\n\n_sym_db = _symbol_database.Default()\n\n\n\n\nDESCRIPTOR = _descriptor.FileDescriptor(\n  name=\'object_detection/protos/input_reader.proto\',\n  package=\'object_detection.protos\',\n  syntax=\'proto2\',\n  serialized_pb=_b(\'\\n*object_detection/protos/input_reader.proto\\x12\\x17object_detection.protos\\""\\xde\\x04\\n\\x0bInputReader\\x12\\x18\\n\\x0elabel_map_path\\x18\\x01 \\x01(\\t:\\x00\\x12\\x15\\n\\x07shuffle\\x18\\x02 \\x01(\\x08:\\x04true\\x12!\\n\\x13shuffle_buffer_size\\x18\\x0b \\x01(\\r:\\x04\\x32\\x30\\x34\\x38\\x12*\\n\\x1d\\x66ilenames_shuffle_buffer_size\\x18\\x0c \\x01(\\r:\\x03\\x31\\x30\\x30\\x12\\x1c\\n\\x0equeue_capacity\\x18\\x03 \\x01(\\r:\\x04\\x32\\x30\\x30\\x30\\x12\\x1f\\n\\x11min_after_dequeue\\x18\\x04 \\x01(\\r:\\x04\\x31\\x30\\x30\\x30\\x12\\x15\\n\\nnum_epochs\\x18\\x05 \\x01(\\r:\\x01\\x30\\x12\\x17\\n\\x0bnum_readers\\x18\\x06 \\x01(\\r:\\x02\\x33\\x32\\x12\\x1a\\n\\rprefetch_size\\x18\\r \\x01(\\r:\\x03\\x35\\x31\\x32\\x12\\""\\n\\x16num_parallel_map_calls\\x18\\x0e \\x01(\\r:\\x02\\x36\\x34\\x12\\""\\n\\x13load_instance_masks\\x18\\x07 \\x01(\\x08:\\x05\\x66\\x61lse\\x12M\\n\\tmask_type\\x18\\n \\x01(\\x0e\\x32).object_detection.protos.InstanceMaskType:\\x0fNUMERICAL_MASKS\\x12N\\n\\x16tf_record_input_reader\\x18\\x08 \\x01(\\x0b\\x32,.object_detection.protos.TFRecordInputReaderH\\x00\\x12M\\n\\x15\\x65xternal_input_reader\\x18\\t \\x01(\\x0b\\x32,.object_detection.protos.ExternalInputReaderH\\x00\\x42\\x0e\\n\\x0cinput_reader\\"")\\n\\x13TFRecordInputReader\\x12\\x12\\n\\ninput_path\\x18\\x01 \\x03(\\t\\""\\x1c\\n\\x13\\x45xternalInputReader*\\x05\\x08\\x01\\x10\\xe8\\x07*C\\n\\x10InstanceMaskType\\x12\\x0b\\n\\x07\\x44\\x45\\x46\\x41ULT\\x10\\x00\\x12\\x13\\n\\x0fNUMERICAL_MASKS\\x10\\x01\\x12\\r\\n\\tPNG_MASKS\\x10\\x02\')\n)\n\n_INSTANCEMASKTYPE = _descriptor.EnumDescriptor(\n  name=\'InstanceMaskType\',\n  full_name=\'object_detection.protos.InstanceMaskType\',\n  filename=None,\n  file=DESCRIPTOR,\n  values=[\n    _descriptor.EnumValueDescriptor(\n      name=\'DEFAULT\', index=0, number=0,\n      options=None,\n      type=None),\n    _descriptor.EnumValueDescriptor(\n      name=\'NUMERICAL_MASKS\', index=1, number=1,\n      options=None,\n      type=None),\n    _descriptor.EnumValueDescriptor(\n      name=\'PNG_MASKS\', index=2, number=2,\n      options=None,\n      type=None),\n  ],\n  containing_type=None,\n  options=None,\n  serialized_start=753,\n  serialized_end=820,\n)\n_sym_db.RegisterEnumDescriptor(_INSTANCEMASKTYPE)\n\nInstanceMaskType = enum_type_wrapper.EnumTypeWrapper(_INSTANCEMASKTYPE)\nDEFAULT = 0\nNUMERICAL_MASKS = 1\nPNG_MASKS = 2\n\n\n\n_INPUTREADER = _descriptor.Descriptor(\n  name=\'InputReader\',\n  full_name=\'object_detection.protos.InputReader\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'label_map_path\', full_name=\'object_detection.protos.InputReader.label_map_path\', index=0,\n      number=1, type=9, cpp_type=9, label=1,\n      has_default_value=True, default_value=_b("""").decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'shuffle\', full_name=\'object_detection.protos.InputReader.shuffle\', index=1,\n      number=2, type=8, cpp_type=7, label=1,\n      has_default_value=True, default_value=True,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'shuffle_buffer_size\', full_name=\'object_detection.protos.InputReader.shuffle_buffer_size\', index=2,\n      number=11, type=13, cpp_type=3, label=1,\n      has_default_value=True, default_value=2048,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'filenames_shuffle_buffer_size\', full_name=\'object_detection.protos.InputReader.filenames_shuffle_buffer_size\', index=3,\n      number=12, type=13, cpp_type=3, label=1,\n      has_default_value=True, default_value=100,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'queue_capacity\', full_name=\'object_detection.protos.InputReader.queue_capacity\', index=4,\n      number=3, type=13, cpp_type=3, label=1,\n      has_default_value=True, default_value=2000,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'min_after_dequeue\', full_name=\'object_detection.protos.InputReader.min_after_dequeue\', index=5,\n      number=4, type=13, cpp_type=3, label=1,\n      has_default_value=True, default_value=1000,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'num_epochs\', full_name=\'object_detection.protos.InputReader.num_epochs\', index=6,\n      number=5, type=13, cpp_type=3, label=1,\n      has_default_value=True, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'num_readers\', full_name=\'object_detection.protos.InputReader.num_readers\', index=7,\n      number=6, type=13, cpp_type=3, label=1,\n      has_default_value=True, default_value=32,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'prefetch_size\', full_name=\'object_detection.protos.InputReader.prefetch_size\', index=8,\n      number=13, type=13, cpp_type=3, label=1,\n      has_default_value=True, default_value=512,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'num_parallel_map_calls\', full_name=\'object_detection.protos.InputReader.num_parallel_map_calls\', index=9,\n      number=14, type=13, cpp_type=3, label=1,\n      has_default_value=True, default_value=64,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'load_instance_masks\', full_name=\'object_detection.protos.InputReader.load_instance_masks\', index=10,\n      number=7, type=8, cpp_type=7, label=1,\n      has_default_value=True, default_value=False,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'mask_type\', full_name=\'object_detection.protos.InputReader.mask_type\', index=11,\n      number=10, type=14, cpp_type=8, label=1,\n      has_default_value=True, default_value=1,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'tf_record_input_reader\', full_name=\'object_detection.protos.InputReader.tf_record_input_reader\', index=12,\n      number=8, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'external_input_reader\', full_name=\'object_detection.protos.InputReader.external_input_reader\', index=13,\n      number=9, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto2\',\n  extension_ranges=[],\n  oneofs=[\n    _descriptor.OneofDescriptor(\n      name=\'input_reader\', full_name=\'object_detection.protos.InputReader.input_reader\',\n      index=0, containing_type=None, fields=[]),\n  ],\n  serialized_start=72,\n  serialized_end=678,\n)\n\n\n_TFRECORDINPUTREADER = _descriptor.Descriptor(\n  name=\'TFRecordInputReader\',\n  full_name=\'object_detection.protos.TFRecordInputReader\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'input_path\', full_name=\'object_detection.protos.TFRecordInputReader.input_path\', index=0,\n      number=1, type=9, cpp_type=9, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto2\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=680,\n  serialized_end=721,\n)\n\n\n_EXTERNALINPUTREADER = _descriptor.Descriptor(\n  name=\'ExternalInputReader\',\n  full_name=\'object_detection.protos.ExternalInputReader\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=True,\n  syntax=\'proto2\',\n  extension_ranges=[(1, 1000), ],\n  oneofs=[\n  ],\n  serialized_start=723,\n  serialized_end=751,\n)\n\n_INPUTREADER.fields_by_name[\'mask_type\'].enum_type = _INSTANCEMASKTYPE\n_INPUTREADER.fields_by_name[\'tf_record_input_reader\'].message_type = _TFRECORDINPUTREADER\n_INPUTREADER.fields_by_name[\'external_input_reader\'].message_type = _EXTERNALINPUTREADER\n_INPUTREADER.oneofs_by_name[\'input_reader\'].fields.append(\n  _INPUTREADER.fields_by_name[\'tf_record_input_reader\'])\n_INPUTREADER.fields_by_name[\'tf_record_input_reader\'].containing_oneof = _INPUTREADER.oneofs_by_name[\'input_reader\']\n_INPUTREADER.oneofs_by_name[\'input_reader\'].fields.append(\n  _INPUTREADER.fields_by_name[\'external_input_reader\'])\n_INPUTREADER.fields_by_name[\'external_input_reader\'].containing_oneof = _INPUTREADER.oneofs_by_name[\'input_reader\']\nDESCRIPTOR.message_types_by_name[\'InputReader\'] = _INPUTREADER\nDESCRIPTOR.message_types_by_name[\'TFRecordInputReader\'] = _TFRECORDINPUTREADER\nDESCRIPTOR.message_types_by_name[\'ExternalInputReader\'] = _EXTERNALINPUTREADER\nDESCRIPTOR.enum_types_by_name[\'InstanceMaskType\'] = _INSTANCEMASKTYPE\n_sym_db.RegisterFileDescriptor(DESCRIPTOR)\n\nInputReader = _reflection.GeneratedProtocolMessageType(\'InputReader\', (_message.Message,), dict(\n  DESCRIPTOR = _INPUTREADER,\n  __module__ = \'object_detection.protos.input_reader_pb2\'\n  # @@protoc_insertion_point(class_scope:object_detection.protos.InputReader)\n  ))\n_sym_db.RegisterMessage(InputReader)\n\nTFRecordInputReader = _reflection.GeneratedProtocolMessageType(\'TFRecordInputReader\', (_message.Message,), dict(\n  DESCRIPTOR = _TFRECORDINPUTREADER,\n  __module__ = \'object_detection.protos.input_reader_pb2\'\n  # @@protoc_insertion_point(class_scope:object_detection.protos.TFRecordInputReader)\n  ))\n_sym_db.RegisterMessage(TFRecordInputReader)\n\nExternalInputReader = _reflection.GeneratedProtocolMessageType(\'ExternalInputReader\', (_message.Message,), dict(\n  DESCRIPTOR = _EXTERNALINPUTREADER,\n  __module__ = \'object_detection.protos.input_reader_pb2\'\n  # @@protoc_insertion_point(class_scope:object_detection.protos.ExternalInputReader)\n  ))\n_sym_db.RegisterMessage(ExternalInputReader)\n\n\n# @@protoc_insertion_point(module_scope)\n'"
src/object_detection/protos/keypoint_box_coder_pb2.py,0,"b'# Generated by the protocol buffer compiler.  DO NOT EDIT!\n# source: object_detection/protos/keypoint_box_coder.proto\n\nimport sys\n_b=sys.version_info[0]<3 and (lambda x:x) or (lambda x:x.encode(\'latin1\'))\nfrom google.protobuf import descriptor as _descriptor\nfrom google.protobuf import message as _message\nfrom google.protobuf import reflection as _reflection\nfrom google.protobuf import symbol_database as _symbol_database\nfrom google.protobuf import descriptor_pb2\n# @@protoc_insertion_point(imports)\n\n_sym_db = _symbol_database.Default()\n\n\n\n\nDESCRIPTOR = _descriptor.FileDescriptor(\n  name=\'object_detection/protos/keypoint_box_coder.proto\',\n  package=\'object_detection.protos\',\n  syntax=\'proto2\',\n  serialized_pb=_b(\'\\n0object_detection/protos/keypoint_box_coder.proto\\x12\\x17object_detection.protos\\""\\x84\\x01\\n\\x10KeypointBoxCoder\\x12\\x15\\n\\rnum_keypoints\\x18\\x01 \\x01(\\x05\\x12\\x13\\n\\x07y_scale\\x18\\x02 \\x01(\\x02:\\x02\\x31\\x30\\x12\\x13\\n\\x07x_scale\\x18\\x03 \\x01(\\x02:\\x02\\x31\\x30\\x12\\x17\\n\\x0cheight_scale\\x18\\x04 \\x01(\\x02:\\x01\\x35\\x12\\x16\\n\\x0bwidth_scale\\x18\\x05 \\x01(\\x02:\\x01\\x35\')\n)\n\n\n\n\n_KEYPOINTBOXCODER = _descriptor.Descriptor(\n  name=\'KeypointBoxCoder\',\n  full_name=\'object_detection.protos.KeypointBoxCoder\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'num_keypoints\', full_name=\'object_detection.protos.KeypointBoxCoder.num_keypoints\', index=0,\n      number=1, type=5, cpp_type=1, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'y_scale\', full_name=\'object_detection.protos.KeypointBoxCoder.y_scale\', index=1,\n      number=2, type=2, cpp_type=6, label=1,\n      has_default_value=True, default_value=float(10),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'x_scale\', full_name=\'object_detection.protos.KeypointBoxCoder.x_scale\', index=2,\n      number=3, type=2, cpp_type=6, label=1,\n      has_default_value=True, default_value=float(10),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'height_scale\', full_name=\'object_detection.protos.KeypointBoxCoder.height_scale\', index=3,\n      number=4, type=2, cpp_type=6, label=1,\n      has_default_value=True, default_value=float(5),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'width_scale\', full_name=\'object_detection.protos.KeypointBoxCoder.width_scale\', index=4,\n      number=5, type=2, cpp_type=6, label=1,\n      has_default_value=True, default_value=float(5),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto2\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=78,\n  serialized_end=210,\n)\n\nDESCRIPTOR.message_types_by_name[\'KeypointBoxCoder\'] = _KEYPOINTBOXCODER\n_sym_db.RegisterFileDescriptor(DESCRIPTOR)\n\nKeypointBoxCoder = _reflection.GeneratedProtocolMessageType(\'KeypointBoxCoder\', (_message.Message,), dict(\n  DESCRIPTOR = _KEYPOINTBOXCODER,\n  __module__ = \'object_detection.protos.keypoint_box_coder_pb2\'\n  # @@protoc_insertion_point(class_scope:object_detection.protos.KeypointBoxCoder)\n  ))\n_sym_db.RegisterMessage(KeypointBoxCoder)\n\n\n# @@protoc_insertion_point(module_scope)\n'"
src/object_detection/protos/losses_pb2.py,0,"b'# Generated by the protocol buffer compiler.  DO NOT EDIT!\n# source: object_detection/protos/losses.proto\n\nimport sys\n_b=sys.version_info[0]<3 and (lambda x:x) or (lambda x:x.encode(\'latin1\'))\nfrom google.protobuf import descriptor as _descriptor\nfrom google.protobuf import message as _message\nfrom google.protobuf import reflection as _reflection\nfrom google.protobuf import symbol_database as _symbol_database\nfrom google.protobuf import descriptor_pb2\n# @@protoc_insertion_point(imports)\n\n_sym_db = _symbol_database.Default()\n\n\n\n\nDESCRIPTOR = _descriptor.FileDescriptor(\n  name=\'object_detection/protos/losses.proto\',\n  package=\'object_detection.protos\',\n  syntax=\'proto2\',\n  serialized_pb=_b(\'\\n$object_detection/protos/losses.proto\\x12\\x17object_detection.protos\\""\\x9f\\x02\\n\\x04Loss\\x12\\x44\\n\\x11localization_loss\\x18\\x01 \\x01(\\x0b\\x32).object_detection.protos.LocalizationLoss\\x12H\\n\\x13\\x63lassification_loss\\x18\\x02 \\x01(\\x0b\\x32+.object_detection.protos.ClassificationLoss\\x12\\x45\\n\\x12hard_example_miner\\x18\\x03 \\x01(\\x0b\\x32).object_detection.protos.HardExampleMiner\\x12 \\n\\x15\\x63lassification_weight\\x18\\x04 \\x01(\\x02:\\x01\\x31\\x12\\x1e\\n\\x13localization_weight\\x18\\x05 \\x01(\\x02:\\x01\\x31\\""\\x9a\\x02\\n\\x10LocalizationLoss\\x12J\\n\\x0bweighted_l2\\x18\\x01 \\x01(\\x0b\\x32\\x33.object_detection.protos.WeightedL2LocalizationLossH\\x00\\x12W\\n\\x12weighted_smooth_l1\\x18\\x02 \\x01(\\x0b\\x32\\x39.object_detection.protos.WeightedSmoothL1LocalizationLossH\\x00\\x12L\\n\\x0cweighted_iou\\x18\\x03 \\x01(\\x0b\\x32\\x34.object_detection.protos.WeightedIOULocalizationLossH\\x00\\x42\\x13\\n\\x11localization_loss\\"">\\n\\x1aWeightedL2LocalizationLoss\\x12 \\n\\x11\\x61nchorwise_output\\x18\\x01 \\x01(\\x08:\\x05\\x66\\x61lse\\""V\\n WeightedSmoothL1LocalizationLoss\\x12 \\n\\x11\\x61nchorwise_output\\x18\\x01 \\x01(\\x08:\\x05\\x66\\x61lse\\x12\\x10\\n\\x05\\x64\\x65lta\\x18\\x02 \\x01(\\x02:\\x01\\x31\\""\\x1d\\n\\x1bWeightedIOULocalizationLoss\\""\\x96\\x03\\n\\x12\\x43lassificationLoss\\x12V\\n\\x10weighted_sigmoid\\x18\\x01 \\x01(\\x0b\\x32:.object_detection.protos.WeightedSigmoidClassificationLossH\\x00\\x12V\\n\\x10weighted_softmax\\x18\\x02 \\x01(\\x0b\\x32:.object_detection.protos.WeightedSoftmaxClassificationLossH\\x00\\x12^\\n\\x14\\x62ootstrapped_sigmoid\\x18\\x03 \\x01(\\x0b\\x32>.object_detection.protos.BootstrappedSigmoidClassificationLossH\\x00\\x12Y\\n\\x16weighted_sigmoid_focal\\x18\\x04 \\x01(\\x0b\\x32\\x37.object_detection.protos.SigmoidFocalClassificationLossH\\x00\\x42\\x15\\n\\x13\\x63lassification_loss\\""E\\n!WeightedSigmoidClassificationLoss\\x12 \\n\\x11\\x61nchorwise_output\\x18\\x01 \\x01(\\x08:\\x05\\x66\\x61lse\\""c\\n\\x1eSigmoidFocalClassificationLoss\\x12 \\n\\x11\\x61nchorwise_output\\x18\\x01 \\x01(\\x08:\\x05\\x66\\x61lse\\x12\\x10\\n\\x05gamma\\x18\\x02 \\x01(\\x02:\\x01\\x32\\x12\\r\\n\\x05\\x61lpha\\x18\\x03 \\x01(\\x02\\""]\\n!WeightedSoftmaxClassificationLoss\\x12 \\n\\x11\\x61nchorwise_output\\x18\\x01 \\x01(\\x08:\\x05\\x66\\x61lse\\x12\\x16\\n\\x0blogit_scale\\x18\\x02 \\x01(\\x02:\\x01\\x31\\""w\\n%BootstrappedSigmoidClassificationLoss\\x12\\r\\n\\x05\\x61lpha\\x18\\x01 \\x01(\\x02\\x12\\x1d\\n\\x0ehard_bootstrap\\x18\\x02 \\x01(\\x08:\\x05\\x66\\x61lse\\x12 \\n\\x11\\x61nchorwise_output\\x18\\x03 \\x01(\\x08:\\x05\\x66\\x61lse\\""\\xa1\\x02\\n\\x10HardExampleMiner\\x12\\x1d\\n\\x11num_hard_examples\\x18\\x01 \\x01(\\x05:\\x02\\x36\\x34\\x12\\x1a\\n\\riou_threshold\\x18\\x02 \\x01(\\x02:\\x03\\x30.7\\x12K\\n\\tloss_type\\x18\\x03 \\x01(\\x0e\\x32\\x32.object_detection.protos.HardExampleMiner.LossType:\\x04\\x42OTH\\x12%\\n\\x1amax_negatives_per_positive\\x18\\x04 \\x01(\\x05:\\x01\\x30\\x12\\""\\n\\x17min_negatives_per_image\\x18\\x05 \\x01(\\x05:\\x01\\x30\\"":\\n\\x08LossType\\x12\\x08\\n\\x04\\x42OTH\\x10\\x00\\x12\\x12\\n\\x0e\\x43LASSIFICATION\\x10\\x01\\x12\\x10\\n\\x0cLOCALIZATION\\x10\\x02\')\n)\n\n\n\n_HARDEXAMPLEMINER_LOSSTYPE = _descriptor.EnumDescriptor(\n  name=\'LossType\',\n  full_name=\'object_detection.protos.HardExampleMiner.LossType\',\n  filename=None,\n  file=DESCRIPTOR,\n  values=[\n    _descriptor.EnumValueDescriptor(\n      name=\'BOTH\', index=0, number=0,\n      options=None,\n      type=None),\n    _descriptor.EnumValueDescriptor(\n      name=\'CLASSIFICATION\', index=1, number=1,\n      options=None,\n      type=None),\n    _descriptor.EnumValueDescriptor(\n      name=\'LOCALIZATION\', index=2, number=2,\n      options=None,\n      type=None),\n  ],\n  containing_type=None,\n  options=None,\n  serialized_start=1852,\n  serialized_end=1910,\n)\n_sym_db.RegisterEnumDescriptor(_HARDEXAMPLEMINER_LOSSTYPE)\n\n\n_LOSS = _descriptor.Descriptor(\n  name=\'Loss\',\n  full_name=\'object_detection.protos.Loss\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'localization_loss\', full_name=\'object_detection.protos.Loss.localization_loss\', index=0,\n      number=1, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'classification_loss\', full_name=\'object_detection.protos.Loss.classification_loss\', index=1,\n      number=2, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'hard_example_miner\', full_name=\'object_detection.protos.Loss.hard_example_miner\', index=2,\n      number=3, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'classification_weight\', full_name=\'object_detection.protos.Loss.classification_weight\', index=3,\n      number=4, type=2, cpp_type=6, label=1,\n      has_default_value=True, default_value=float(1),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'localization_weight\', full_name=\'object_detection.protos.Loss.localization_weight\', index=4,\n      number=5, type=2, cpp_type=6, label=1,\n      has_default_value=True, default_value=float(1),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto2\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=66,\n  serialized_end=353,\n)\n\n\n_LOCALIZATIONLOSS = _descriptor.Descriptor(\n  name=\'LocalizationLoss\',\n  full_name=\'object_detection.protos.LocalizationLoss\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'weighted_l2\', full_name=\'object_detection.protos.LocalizationLoss.weighted_l2\', index=0,\n      number=1, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'weighted_smooth_l1\', full_name=\'object_detection.protos.LocalizationLoss.weighted_smooth_l1\', index=1,\n      number=2, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'weighted_iou\', full_name=\'object_detection.protos.LocalizationLoss.weighted_iou\', index=2,\n      number=3, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto2\',\n  extension_ranges=[],\n  oneofs=[\n    _descriptor.OneofDescriptor(\n      name=\'localization_loss\', full_name=\'object_detection.protos.LocalizationLoss.localization_loss\',\n      index=0, containing_type=None, fields=[]),\n  ],\n  serialized_start=356,\n  serialized_end=638,\n)\n\n\n_WEIGHTEDL2LOCALIZATIONLOSS = _descriptor.Descriptor(\n  name=\'WeightedL2LocalizationLoss\',\n  full_name=\'object_detection.protos.WeightedL2LocalizationLoss\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'anchorwise_output\', full_name=\'object_detection.protos.WeightedL2LocalizationLoss.anchorwise_output\', index=0,\n      number=1, type=8, cpp_type=7, label=1,\n      has_default_value=True, default_value=False,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto2\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=640,\n  serialized_end=702,\n)\n\n\n_WEIGHTEDSMOOTHL1LOCALIZATIONLOSS = _descriptor.Descriptor(\n  name=\'WeightedSmoothL1LocalizationLoss\',\n  full_name=\'object_detection.protos.WeightedSmoothL1LocalizationLoss\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'anchorwise_output\', full_name=\'object_detection.protos.WeightedSmoothL1LocalizationLoss.anchorwise_output\', index=0,\n      number=1, type=8, cpp_type=7, label=1,\n      has_default_value=True, default_value=False,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'delta\', full_name=\'object_detection.protos.WeightedSmoothL1LocalizationLoss.delta\', index=1,\n      number=2, type=2, cpp_type=6, label=1,\n      has_default_value=True, default_value=float(1),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto2\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=704,\n  serialized_end=790,\n)\n\n\n_WEIGHTEDIOULOCALIZATIONLOSS = _descriptor.Descriptor(\n  name=\'WeightedIOULocalizationLoss\',\n  full_name=\'object_detection.protos.WeightedIOULocalizationLoss\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto2\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=792,\n  serialized_end=821,\n)\n\n\n_CLASSIFICATIONLOSS = _descriptor.Descriptor(\n  name=\'ClassificationLoss\',\n  full_name=\'object_detection.protos.ClassificationLoss\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'weighted_sigmoid\', full_name=\'object_detection.protos.ClassificationLoss.weighted_sigmoid\', index=0,\n      number=1, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'weighted_softmax\', full_name=\'object_detection.protos.ClassificationLoss.weighted_softmax\', index=1,\n      number=2, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'bootstrapped_sigmoid\', full_name=\'object_detection.protos.ClassificationLoss.bootstrapped_sigmoid\', index=2,\n      number=3, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'weighted_sigmoid_focal\', full_name=\'object_detection.protos.ClassificationLoss.weighted_sigmoid_focal\', index=3,\n      number=4, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto2\',\n  extension_ranges=[],\n  oneofs=[\n    _descriptor.OneofDescriptor(\n      name=\'classification_loss\', full_name=\'object_detection.protos.ClassificationLoss.classification_loss\',\n      index=0, containing_type=None, fields=[]),\n  ],\n  serialized_start=824,\n  serialized_end=1230,\n)\n\n\n_WEIGHTEDSIGMOIDCLASSIFICATIONLOSS = _descriptor.Descriptor(\n  name=\'WeightedSigmoidClassificationLoss\',\n  full_name=\'object_detection.protos.WeightedSigmoidClassificationLoss\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'anchorwise_output\', full_name=\'object_detection.protos.WeightedSigmoidClassificationLoss.anchorwise_output\', index=0,\n      number=1, type=8, cpp_type=7, label=1,\n      has_default_value=True, default_value=False,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto2\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=1232,\n  serialized_end=1301,\n)\n\n\n_SIGMOIDFOCALCLASSIFICATIONLOSS = _descriptor.Descriptor(\n  name=\'SigmoidFocalClassificationLoss\',\n  full_name=\'object_detection.protos.SigmoidFocalClassificationLoss\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'anchorwise_output\', full_name=\'object_detection.protos.SigmoidFocalClassificationLoss.anchorwise_output\', index=0,\n      number=1, type=8, cpp_type=7, label=1,\n      has_default_value=True, default_value=False,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'gamma\', full_name=\'object_detection.protos.SigmoidFocalClassificationLoss.gamma\', index=1,\n      number=2, type=2, cpp_type=6, label=1,\n      has_default_value=True, default_value=float(2),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'alpha\', full_name=\'object_detection.protos.SigmoidFocalClassificationLoss.alpha\', index=2,\n      number=3, type=2, cpp_type=6, label=1,\n      has_default_value=False, default_value=float(0),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto2\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=1303,\n  serialized_end=1402,\n)\n\n\n_WEIGHTEDSOFTMAXCLASSIFICATIONLOSS = _descriptor.Descriptor(\n  name=\'WeightedSoftmaxClassificationLoss\',\n  full_name=\'object_detection.protos.WeightedSoftmaxClassificationLoss\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'anchorwise_output\', full_name=\'object_detection.protos.WeightedSoftmaxClassificationLoss.anchorwise_output\', index=0,\n      number=1, type=8, cpp_type=7, label=1,\n      has_default_value=True, default_value=False,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'logit_scale\', full_name=\'object_detection.protos.WeightedSoftmaxClassificationLoss.logit_scale\', index=1,\n      number=2, type=2, cpp_type=6, label=1,\n      has_default_value=True, default_value=float(1),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto2\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=1404,\n  serialized_end=1497,\n)\n\n\n_BOOTSTRAPPEDSIGMOIDCLASSIFICATIONLOSS = _descriptor.Descriptor(\n  name=\'BootstrappedSigmoidClassificationLoss\',\n  full_name=\'object_detection.protos.BootstrappedSigmoidClassificationLoss\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'alpha\', full_name=\'object_detection.protos.BootstrappedSigmoidClassificationLoss.alpha\', index=0,\n      number=1, type=2, cpp_type=6, label=1,\n      has_default_value=False, default_value=float(0),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'hard_bootstrap\', full_name=\'object_detection.protos.BootstrappedSigmoidClassificationLoss.hard_bootstrap\', index=1,\n      number=2, type=8, cpp_type=7, label=1,\n      has_default_value=True, default_value=False,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'anchorwise_output\', full_name=\'object_detection.protos.BootstrappedSigmoidClassificationLoss.anchorwise_output\', index=2,\n      number=3, type=8, cpp_type=7, label=1,\n      has_default_value=True, default_value=False,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto2\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=1499,\n  serialized_end=1618,\n)\n\n\n_HARDEXAMPLEMINER = _descriptor.Descriptor(\n  name=\'HardExampleMiner\',\n  full_name=\'object_detection.protos.HardExampleMiner\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'num_hard_examples\', full_name=\'object_detection.protos.HardExampleMiner.num_hard_examples\', index=0,\n      number=1, type=5, cpp_type=1, label=1,\n      has_default_value=True, default_value=64,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'iou_threshold\', full_name=\'object_detection.protos.HardExampleMiner.iou_threshold\', index=1,\n      number=2, type=2, cpp_type=6, label=1,\n      has_default_value=True, default_value=float(0.7),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'loss_type\', full_name=\'object_detection.protos.HardExampleMiner.loss_type\', index=2,\n      number=3, type=14, cpp_type=8, label=1,\n      has_default_value=True, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'max_negatives_per_positive\', full_name=\'object_detection.protos.HardExampleMiner.max_negatives_per_positive\', index=3,\n      number=4, type=5, cpp_type=1, label=1,\n      has_default_value=True, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'min_negatives_per_image\', full_name=\'object_detection.protos.HardExampleMiner.min_negatives_per_image\', index=4,\n      number=5, type=5, cpp_type=1, label=1,\n      has_default_value=True, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n    _HARDEXAMPLEMINER_LOSSTYPE,\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto2\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=1621,\n  serialized_end=1910,\n)\n\n_LOSS.fields_by_name[\'localization_loss\'].message_type = _LOCALIZATIONLOSS\n_LOSS.fields_by_name[\'classification_loss\'].message_type = _CLASSIFICATIONLOSS\n_LOSS.fields_by_name[\'hard_example_miner\'].message_type = _HARDEXAMPLEMINER\n_LOCALIZATIONLOSS.fields_by_name[\'weighted_l2\'].message_type = _WEIGHTEDL2LOCALIZATIONLOSS\n_LOCALIZATIONLOSS.fields_by_name[\'weighted_smooth_l1\'].message_type = _WEIGHTEDSMOOTHL1LOCALIZATIONLOSS\n_LOCALIZATIONLOSS.fields_by_name[\'weighted_iou\'].message_type = _WEIGHTEDIOULOCALIZATIONLOSS\n_LOCALIZATIONLOSS.oneofs_by_name[\'localization_loss\'].fields.append(\n  _LOCALIZATIONLOSS.fields_by_name[\'weighted_l2\'])\n_LOCALIZATIONLOSS.fields_by_name[\'weighted_l2\'].containing_oneof = _LOCALIZATIONLOSS.oneofs_by_name[\'localization_loss\']\n_LOCALIZATIONLOSS.oneofs_by_name[\'localization_loss\'].fields.append(\n  _LOCALIZATIONLOSS.fields_by_name[\'weighted_smooth_l1\'])\n_LOCALIZATIONLOSS.fields_by_name[\'weighted_smooth_l1\'].containing_oneof = _LOCALIZATIONLOSS.oneofs_by_name[\'localization_loss\']\n_LOCALIZATIONLOSS.oneofs_by_name[\'localization_loss\'].fields.append(\n  _LOCALIZATIONLOSS.fields_by_name[\'weighted_iou\'])\n_LOCALIZATIONLOSS.fields_by_name[\'weighted_iou\'].containing_oneof = _LOCALIZATIONLOSS.oneofs_by_name[\'localization_loss\']\n_CLASSIFICATIONLOSS.fields_by_name[\'weighted_sigmoid\'].message_type = _WEIGHTEDSIGMOIDCLASSIFICATIONLOSS\n_CLASSIFICATIONLOSS.fields_by_name[\'weighted_softmax\'].message_type = _WEIGHTEDSOFTMAXCLASSIFICATIONLOSS\n_CLASSIFICATIONLOSS.fields_by_name[\'bootstrapped_sigmoid\'].message_type = _BOOTSTRAPPEDSIGMOIDCLASSIFICATIONLOSS\n_CLASSIFICATIONLOSS.fields_by_name[\'weighted_sigmoid_focal\'].message_type = _SIGMOIDFOCALCLASSIFICATIONLOSS\n_CLASSIFICATIONLOSS.oneofs_by_name[\'classification_loss\'].fields.append(\n  _CLASSIFICATIONLOSS.fields_by_name[\'weighted_sigmoid\'])\n_CLASSIFICATIONLOSS.fields_by_name[\'weighted_sigmoid\'].containing_oneof = _CLASSIFICATIONLOSS.oneofs_by_name[\'classification_loss\']\n_CLASSIFICATIONLOSS.oneofs_by_name[\'classification_loss\'].fields.append(\n  _CLASSIFICATIONLOSS.fields_by_name[\'weighted_softmax\'])\n_CLASSIFICATIONLOSS.fields_by_name[\'weighted_softmax\'].containing_oneof = _CLASSIFICATIONLOSS.oneofs_by_name[\'classification_loss\']\n_CLASSIFICATIONLOSS.oneofs_by_name[\'classification_loss\'].fields.append(\n  _CLASSIFICATIONLOSS.fields_by_name[\'bootstrapped_sigmoid\'])\n_CLASSIFICATIONLOSS.fields_by_name[\'bootstrapped_sigmoid\'].containing_oneof = _CLASSIFICATIONLOSS.oneofs_by_name[\'classification_loss\']\n_CLASSIFICATIONLOSS.oneofs_by_name[\'classification_loss\'].fields.append(\n  _CLASSIFICATIONLOSS.fields_by_name[\'weighted_sigmoid_focal\'])\n_CLASSIFICATIONLOSS.fields_by_name[\'weighted_sigmoid_focal\'].containing_oneof = _CLASSIFICATIONLOSS.oneofs_by_name[\'classification_loss\']\n_HARDEXAMPLEMINER.fields_by_name[\'loss_type\'].enum_type = _HARDEXAMPLEMINER_LOSSTYPE\n_HARDEXAMPLEMINER_LOSSTYPE.containing_type = _HARDEXAMPLEMINER\nDESCRIPTOR.message_types_by_name[\'Loss\'] = _LOSS\nDESCRIPTOR.message_types_by_name[\'LocalizationLoss\'] = _LOCALIZATIONLOSS\nDESCRIPTOR.message_types_by_name[\'WeightedL2LocalizationLoss\'] = _WEIGHTEDL2LOCALIZATIONLOSS\nDESCRIPTOR.message_types_by_name[\'WeightedSmoothL1LocalizationLoss\'] = _WEIGHTEDSMOOTHL1LOCALIZATIONLOSS\nDESCRIPTOR.message_types_by_name[\'WeightedIOULocalizationLoss\'] = _WEIGHTEDIOULOCALIZATIONLOSS\nDESCRIPTOR.message_types_by_name[\'ClassificationLoss\'] = _CLASSIFICATIONLOSS\nDESCRIPTOR.message_types_by_name[\'WeightedSigmoidClassificationLoss\'] = _WEIGHTEDSIGMOIDCLASSIFICATIONLOSS\nDESCRIPTOR.message_types_by_name[\'SigmoidFocalClassificationLoss\'] = _SIGMOIDFOCALCLASSIFICATIONLOSS\nDESCRIPTOR.message_types_by_name[\'WeightedSoftmaxClassificationLoss\'] = _WEIGHTEDSOFTMAXCLASSIFICATIONLOSS\nDESCRIPTOR.message_types_by_name[\'BootstrappedSigmoidClassificationLoss\'] = _BOOTSTRAPPEDSIGMOIDCLASSIFICATIONLOSS\nDESCRIPTOR.message_types_by_name[\'HardExampleMiner\'] = _HARDEXAMPLEMINER\n_sym_db.RegisterFileDescriptor(DESCRIPTOR)\n\nLoss = _reflection.GeneratedProtocolMessageType(\'Loss\', (_message.Message,), dict(\n  DESCRIPTOR = _LOSS,\n  __module__ = \'object_detection.protos.losses_pb2\'\n  # @@protoc_insertion_point(class_scope:object_detection.protos.Loss)\n  ))\n_sym_db.RegisterMessage(Loss)\n\nLocalizationLoss = _reflection.GeneratedProtocolMessageType(\'LocalizationLoss\', (_message.Message,), dict(\n  DESCRIPTOR = _LOCALIZATIONLOSS,\n  __module__ = \'object_detection.protos.losses_pb2\'\n  # @@protoc_insertion_point(class_scope:object_detection.protos.LocalizationLoss)\n  ))\n_sym_db.RegisterMessage(LocalizationLoss)\n\nWeightedL2LocalizationLoss = _reflection.GeneratedProtocolMessageType(\'WeightedL2LocalizationLoss\', (_message.Message,), dict(\n  DESCRIPTOR = _WEIGHTEDL2LOCALIZATIONLOSS,\n  __module__ = \'object_detection.protos.losses_pb2\'\n  # @@protoc_insertion_point(class_scope:object_detection.protos.WeightedL2LocalizationLoss)\n  ))\n_sym_db.RegisterMessage(WeightedL2LocalizationLoss)\n\nWeightedSmoothL1LocalizationLoss = _reflection.GeneratedProtocolMessageType(\'WeightedSmoothL1LocalizationLoss\', (_message.Message,), dict(\n  DESCRIPTOR = _WEIGHTEDSMOOTHL1LOCALIZATIONLOSS,\n  __module__ = \'object_detection.protos.losses_pb2\'\n  # @@protoc_insertion_point(class_scope:object_detection.protos.WeightedSmoothL1LocalizationLoss)\n  ))\n_sym_db.RegisterMessage(WeightedSmoothL1LocalizationLoss)\n\nWeightedIOULocalizationLoss = _reflection.GeneratedProtocolMessageType(\'WeightedIOULocalizationLoss\', (_message.Message,), dict(\n  DESCRIPTOR = _WEIGHTEDIOULOCALIZATIONLOSS,\n  __module__ = \'object_detection.protos.losses_pb2\'\n  # @@protoc_insertion_point(class_scope:object_detection.protos.WeightedIOULocalizationLoss)\n  ))\n_sym_db.RegisterMessage(WeightedIOULocalizationLoss)\n\nClassificationLoss = _reflection.GeneratedProtocolMessageType(\'ClassificationLoss\', (_message.Message,), dict(\n  DESCRIPTOR = _CLASSIFICATIONLOSS,\n  __module__ = \'object_detection.protos.losses_pb2\'\n  # @@protoc_insertion_point(class_scope:object_detection.protos.ClassificationLoss)\n  ))\n_sym_db.RegisterMessage(ClassificationLoss)\n\nWeightedSigmoidClassificationLoss = _reflection.GeneratedProtocolMessageType(\'WeightedSigmoidClassificationLoss\', (_message.Message,), dict(\n  DESCRIPTOR = _WEIGHTEDSIGMOIDCLASSIFICATIONLOSS,\n  __module__ = \'object_detection.protos.losses_pb2\'\n  # @@protoc_insertion_point(class_scope:object_detection.protos.WeightedSigmoidClassificationLoss)\n  ))\n_sym_db.RegisterMessage(WeightedSigmoidClassificationLoss)\n\nSigmoidFocalClassificationLoss = _reflection.GeneratedProtocolMessageType(\'SigmoidFocalClassificationLoss\', (_message.Message,), dict(\n  DESCRIPTOR = _SIGMOIDFOCALCLASSIFICATIONLOSS,\n  __module__ = \'object_detection.protos.losses_pb2\'\n  # @@protoc_insertion_point(class_scope:object_detection.protos.SigmoidFocalClassificationLoss)\n  ))\n_sym_db.RegisterMessage(SigmoidFocalClassificationLoss)\n\nWeightedSoftmaxClassificationLoss = _reflection.GeneratedProtocolMessageType(\'WeightedSoftmaxClassificationLoss\', (_message.Message,), dict(\n  DESCRIPTOR = _WEIGHTEDSOFTMAXCLASSIFICATIONLOSS,\n  __module__ = \'object_detection.protos.losses_pb2\'\n  # @@protoc_insertion_point(class_scope:object_detection.protos.WeightedSoftmaxClassificationLoss)\n  ))\n_sym_db.RegisterMessage(WeightedSoftmaxClassificationLoss)\n\nBootstrappedSigmoidClassificationLoss = _reflection.GeneratedProtocolMessageType(\'BootstrappedSigmoidClassificationLoss\', (_message.Message,), dict(\n  DESCRIPTOR = _BOOTSTRAPPEDSIGMOIDCLASSIFICATIONLOSS,\n  __module__ = \'object_detection.protos.losses_pb2\'\n  # @@protoc_insertion_point(class_scope:object_detection.protos.BootstrappedSigmoidClassificationLoss)\n  ))\n_sym_db.RegisterMessage(BootstrappedSigmoidClassificationLoss)\n\nHardExampleMiner = _reflection.GeneratedProtocolMessageType(\'HardExampleMiner\', (_message.Message,), dict(\n  DESCRIPTOR = _HARDEXAMPLEMINER,\n  __module__ = \'object_detection.protos.losses_pb2\'\n  # @@protoc_insertion_point(class_scope:object_detection.protos.HardExampleMiner)\n  ))\n_sym_db.RegisterMessage(HardExampleMiner)\n\n\n# @@protoc_insertion_point(module_scope)\n'"
src/object_detection/protos/matcher_pb2.py,0,"b'# Generated by the protocol buffer compiler.  DO NOT EDIT!\n# source: object_detection/protos/matcher.proto\n\nimport sys\n_b=sys.version_info[0]<3 and (lambda x:x) or (lambda x:x.encode(\'latin1\'))\nfrom google.protobuf import descriptor as _descriptor\nfrom google.protobuf import message as _message\nfrom google.protobuf import reflection as _reflection\nfrom google.protobuf import symbol_database as _symbol_database\nfrom google.protobuf import descriptor_pb2\n# @@protoc_insertion_point(imports)\n\n_sym_db = _symbol_database.Default()\n\n\nfrom object_detection.protos import argmax_matcher_pb2 as object__detection_dot_protos_dot_argmax__matcher__pb2\nfrom object_detection.protos import bipartite_matcher_pb2 as object__detection_dot_protos_dot_bipartite__matcher__pb2\n\n\nDESCRIPTOR = _descriptor.FileDescriptor(\n  name=\'object_detection/protos/matcher.proto\',\n  package=\'object_detection.protos\',\n  syntax=\'proto2\',\n  serialized_pb=_b(\'\\n%object_detection/protos/matcher.proto\\x12\\x17object_detection.protos\\x1a,object_detection/protos/argmax_matcher.proto\\x1a/object_detection/protos/bipartite_matcher.proto\\""\\xa4\\x01\\n\\x07Matcher\\x12@\\n\\x0e\\x61rgmax_matcher\\x18\\x01 \\x01(\\x0b\\x32&.object_detection.protos.ArgMaxMatcherH\\x00\\x12\\x46\\n\\x11\\x62ipartite_matcher\\x18\\x02 \\x01(\\x0b\\x32).object_detection.protos.BipartiteMatcherH\\x00\\x42\\x0f\\n\\rmatcher_oneof\')\n  ,\n  dependencies=[object__detection_dot_protos_dot_argmax__matcher__pb2.DESCRIPTOR,object__detection_dot_protos_dot_bipartite__matcher__pb2.DESCRIPTOR,])\n\n\n\n\n_MATCHER = _descriptor.Descriptor(\n  name=\'Matcher\',\n  full_name=\'object_detection.protos.Matcher\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'argmax_matcher\', full_name=\'object_detection.protos.Matcher.argmax_matcher\', index=0,\n      number=1, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'bipartite_matcher\', full_name=\'object_detection.protos.Matcher.bipartite_matcher\', index=1,\n      number=2, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto2\',\n  extension_ranges=[],\n  oneofs=[\n    _descriptor.OneofDescriptor(\n      name=\'matcher_oneof\', full_name=\'object_detection.protos.Matcher.matcher_oneof\',\n      index=0, containing_type=None, fields=[]),\n  ],\n  serialized_start=162,\n  serialized_end=326,\n)\n\n_MATCHER.fields_by_name[\'argmax_matcher\'].message_type = object__detection_dot_protos_dot_argmax__matcher__pb2._ARGMAXMATCHER\n_MATCHER.fields_by_name[\'bipartite_matcher\'].message_type = object__detection_dot_protos_dot_bipartite__matcher__pb2._BIPARTITEMATCHER\n_MATCHER.oneofs_by_name[\'matcher_oneof\'].fields.append(\n  _MATCHER.fields_by_name[\'argmax_matcher\'])\n_MATCHER.fields_by_name[\'argmax_matcher\'].containing_oneof = _MATCHER.oneofs_by_name[\'matcher_oneof\']\n_MATCHER.oneofs_by_name[\'matcher_oneof\'].fields.append(\n  _MATCHER.fields_by_name[\'bipartite_matcher\'])\n_MATCHER.fields_by_name[\'bipartite_matcher\'].containing_oneof = _MATCHER.oneofs_by_name[\'matcher_oneof\']\nDESCRIPTOR.message_types_by_name[\'Matcher\'] = _MATCHER\n_sym_db.RegisterFileDescriptor(DESCRIPTOR)\n\nMatcher = _reflection.GeneratedProtocolMessageType(\'Matcher\', (_message.Message,), dict(\n  DESCRIPTOR = _MATCHER,\n  __module__ = \'object_detection.protos.matcher_pb2\'\n  # @@protoc_insertion_point(class_scope:object_detection.protos.Matcher)\n  ))\n_sym_db.RegisterMessage(Matcher)\n\n\n# @@protoc_insertion_point(module_scope)\n'"
src/object_detection/protos/mean_stddev_box_coder_pb2.py,0,"b'# Generated by the protocol buffer compiler.  DO NOT EDIT!\n# source: object_detection/protos/mean_stddev_box_coder.proto\n\nimport sys\n_b=sys.version_info[0]<3 and (lambda x:x) or (lambda x:x.encode(\'latin1\'))\nfrom google.protobuf import descriptor as _descriptor\nfrom google.protobuf import message as _message\nfrom google.protobuf import reflection as _reflection\nfrom google.protobuf import symbol_database as _symbol_database\nfrom google.protobuf import descriptor_pb2\n# @@protoc_insertion_point(imports)\n\n_sym_db = _symbol_database.Default()\n\n\n\n\nDESCRIPTOR = _descriptor.FileDescriptor(\n  name=\'object_detection/protos/mean_stddev_box_coder.proto\',\n  package=\'object_detection.protos\',\n  syntax=\'proto2\',\n  serialized_pb=_b(\'\\n3object_detection/protos/mean_stddev_box_coder.proto\\x12\\x17object_detection.protos\\""\\x14\\n\\x12MeanStddevBoxCoder\')\n)\n\n\n\n\n_MEANSTDDEVBOXCODER = _descriptor.Descriptor(\n  name=\'MeanStddevBoxCoder\',\n  full_name=\'object_detection.protos.MeanStddevBoxCoder\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto2\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=80,\n  serialized_end=100,\n)\n\nDESCRIPTOR.message_types_by_name[\'MeanStddevBoxCoder\'] = _MEANSTDDEVBOXCODER\n_sym_db.RegisterFileDescriptor(DESCRIPTOR)\n\nMeanStddevBoxCoder = _reflection.GeneratedProtocolMessageType(\'MeanStddevBoxCoder\', (_message.Message,), dict(\n  DESCRIPTOR = _MEANSTDDEVBOXCODER,\n  __module__ = \'object_detection.protos.mean_stddev_box_coder_pb2\'\n  # @@protoc_insertion_point(class_scope:object_detection.protos.MeanStddevBoxCoder)\n  ))\n_sym_db.RegisterMessage(MeanStddevBoxCoder)\n\n\n# @@protoc_insertion_point(module_scope)\n'"
src/object_detection/protos/model_pb2.py,0,"b'# Generated by the protocol buffer compiler.  DO NOT EDIT!\n# source: object_detection/protos/model.proto\n\nimport sys\n_b=sys.version_info[0]<3 and (lambda x:x) or (lambda x:x.encode(\'latin1\'))\nfrom google.protobuf import descriptor as _descriptor\nfrom google.protobuf import message as _message\nfrom google.protobuf import reflection as _reflection\nfrom google.protobuf import symbol_database as _symbol_database\nfrom google.protobuf import descriptor_pb2\n# @@protoc_insertion_point(imports)\n\n_sym_db = _symbol_database.Default()\n\n\nfrom object_detection.protos import faster_rcnn_pb2 as object__detection_dot_protos_dot_faster__rcnn__pb2\nfrom object_detection.protos import ssd_pb2 as object__detection_dot_protos_dot_ssd__pb2\n\n\nDESCRIPTOR = _descriptor.FileDescriptor(\n  name=\'object_detection/protos/model.proto\',\n  package=\'object_detection.protos\',\n  syntax=\'proto2\',\n  serialized_pb=_b(\'\\n#object_detection/protos/model.proto\\x12\\x17object_detection.protos\\x1a)object_detection/protos/faster_rcnn.proto\\x1a!object_detection/protos/ssd.proto\\""\\x82\\x01\\n\\x0e\\x44\\x65tectionModel\\x12:\\n\\x0b\\x66\\x61ster_rcnn\\x18\\x01 \\x01(\\x0b\\x32#.object_detection.protos.FasterRcnnH\\x00\\x12+\\n\\x03ssd\\x18\\x02 \\x01(\\x0b\\x32\\x1c.object_detection.protos.SsdH\\x00\\x42\\x07\\n\\x05model\')\n  ,\n  dependencies=[object__detection_dot_protos_dot_faster__rcnn__pb2.DESCRIPTOR,object__detection_dot_protos_dot_ssd__pb2.DESCRIPTOR,])\n\n\n\n\n_DETECTIONMODEL = _descriptor.Descriptor(\n  name=\'DetectionModel\',\n  full_name=\'object_detection.protos.DetectionModel\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'faster_rcnn\', full_name=\'object_detection.protos.DetectionModel.faster_rcnn\', index=0,\n      number=1, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'ssd\', full_name=\'object_detection.protos.DetectionModel.ssd\', index=1,\n      number=2, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto2\',\n  extension_ranges=[],\n  oneofs=[\n    _descriptor.OneofDescriptor(\n      name=\'model\', full_name=\'object_detection.protos.DetectionModel.model\',\n      index=0, containing_type=None, fields=[]),\n  ],\n  serialized_start=143,\n  serialized_end=273,\n)\n\n_DETECTIONMODEL.fields_by_name[\'faster_rcnn\'].message_type = object__detection_dot_protos_dot_faster__rcnn__pb2._FASTERRCNN\n_DETECTIONMODEL.fields_by_name[\'ssd\'].message_type = object__detection_dot_protos_dot_ssd__pb2._SSD\n_DETECTIONMODEL.oneofs_by_name[\'model\'].fields.append(\n  _DETECTIONMODEL.fields_by_name[\'faster_rcnn\'])\n_DETECTIONMODEL.fields_by_name[\'faster_rcnn\'].containing_oneof = _DETECTIONMODEL.oneofs_by_name[\'model\']\n_DETECTIONMODEL.oneofs_by_name[\'model\'].fields.append(\n  _DETECTIONMODEL.fields_by_name[\'ssd\'])\n_DETECTIONMODEL.fields_by_name[\'ssd\'].containing_oneof = _DETECTIONMODEL.oneofs_by_name[\'model\']\nDESCRIPTOR.message_types_by_name[\'DetectionModel\'] = _DETECTIONMODEL\n_sym_db.RegisterFileDescriptor(DESCRIPTOR)\n\nDetectionModel = _reflection.GeneratedProtocolMessageType(\'DetectionModel\', (_message.Message,), dict(\n  DESCRIPTOR = _DETECTIONMODEL,\n  __module__ = \'object_detection.protos.model_pb2\'\n  # @@protoc_insertion_point(class_scope:object_detection.protos.DetectionModel)\n  ))\n_sym_db.RegisterMessage(DetectionModel)\n\n\n# @@protoc_insertion_point(module_scope)\n'"
src/object_detection/protos/multiscale_anchor_generator_pb2.py,0,"b'# Generated by the protocol buffer compiler.  DO NOT EDIT!\n# source: object_detection/protos/multiscale_anchor_generator.proto\n\nimport sys\n_b=sys.version_info[0]<3 and (lambda x:x) or (lambda x:x.encode(\'latin1\'))\nfrom google.protobuf import descriptor as _descriptor\nfrom google.protobuf import message as _message\nfrom google.protobuf import reflection as _reflection\nfrom google.protobuf import symbol_database as _symbol_database\nfrom google.protobuf import descriptor_pb2\n# @@protoc_insertion_point(imports)\n\n_sym_db = _symbol_database.Default()\n\n\n\n\nDESCRIPTOR = _descriptor.FileDescriptor(\n  name=\'object_detection/protos/multiscale_anchor_generator.proto\',\n  package=\'object_detection.protos\',\n  syntax=\'proto2\',\n  serialized_pb=_b(\'\\n9object_detection/protos/multiscale_anchor_generator.proto\\x12\\x17object_detection.protos\\""\\xba\\x01\\n\\x19MultiscaleAnchorGenerator\\x12\\x14\\n\\tmin_level\\x18\\x01 \\x01(\\x05:\\x01\\x33\\x12\\x14\\n\\tmax_level\\x18\\x02 \\x01(\\x05:\\x01\\x37\\x12\\x17\\n\\x0c\\x61nchor_scale\\x18\\x03 \\x01(\\x02:\\x01\\x34\\x12\\x15\\n\\raspect_ratios\\x18\\x04 \\x03(\\x02\\x12\\x1c\\n\\x11scales_per_octave\\x18\\x05 \\x01(\\x05:\\x01\\x32\\x12#\\n\\x15normalize_coordinates\\x18\\x06 \\x01(\\x08:\\x04true\')\n)\n\n\n\n\n_MULTISCALEANCHORGENERATOR = _descriptor.Descriptor(\n  name=\'MultiscaleAnchorGenerator\',\n  full_name=\'object_detection.protos.MultiscaleAnchorGenerator\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'min_level\', full_name=\'object_detection.protos.MultiscaleAnchorGenerator.min_level\', index=0,\n      number=1, type=5, cpp_type=1, label=1,\n      has_default_value=True, default_value=3,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'max_level\', full_name=\'object_detection.protos.MultiscaleAnchorGenerator.max_level\', index=1,\n      number=2, type=5, cpp_type=1, label=1,\n      has_default_value=True, default_value=7,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'anchor_scale\', full_name=\'object_detection.protos.MultiscaleAnchorGenerator.anchor_scale\', index=2,\n      number=3, type=2, cpp_type=6, label=1,\n      has_default_value=True, default_value=float(4),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'aspect_ratios\', full_name=\'object_detection.protos.MultiscaleAnchorGenerator.aspect_ratios\', index=3,\n      number=4, type=2, cpp_type=6, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'scales_per_octave\', full_name=\'object_detection.protos.MultiscaleAnchorGenerator.scales_per_octave\', index=4,\n      number=5, type=5, cpp_type=1, label=1,\n      has_default_value=True, default_value=2,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'normalize_coordinates\', full_name=\'object_detection.protos.MultiscaleAnchorGenerator.normalize_coordinates\', index=5,\n      number=6, type=8, cpp_type=7, label=1,\n      has_default_value=True, default_value=True,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto2\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=87,\n  serialized_end=273,\n)\n\nDESCRIPTOR.message_types_by_name[\'MultiscaleAnchorGenerator\'] = _MULTISCALEANCHORGENERATOR\n_sym_db.RegisterFileDescriptor(DESCRIPTOR)\n\nMultiscaleAnchorGenerator = _reflection.GeneratedProtocolMessageType(\'MultiscaleAnchorGenerator\', (_message.Message,), dict(\n  DESCRIPTOR = _MULTISCALEANCHORGENERATOR,\n  __module__ = \'object_detection.protos.multiscale_anchor_generator_pb2\'\n  # @@protoc_insertion_point(class_scope:object_detection.protos.MultiscaleAnchorGenerator)\n  ))\n_sym_db.RegisterMessage(MultiscaleAnchorGenerator)\n\n\n# @@protoc_insertion_point(module_scope)\n'"
src/object_detection/protos/optimizer_pb2.py,0,"b'# Generated by the protocol buffer compiler.  DO NOT EDIT!\n# source: object_detection/protos/optimizer.proto\n\nimport sys\n_b=sys.version_info[0]<3 and (lambda x:x) or (lambda x:x.encode(\'latin1\'))\nfrom google.protobuf import descriptor as _descriptor\nfrom google.protobuf import message as _message\nfrom google.protobuf import reflection as _reflection\nfrom google.protobuf import symbol_database as _symbol_database\nfrom google.protobuf import descriptor_pb2\n# @@protoc_insertion_point(imports)\n\n_sym_db = _symbol_database.Default()\n\n\n\n\nDESCRIPTOR = _descriptor.FileDescriptor(\n  name=\'object_detection/protos/optimizer.proto\',\n  package=\'object_detection.protos\',\n  syntax=\'proto2\',\n  serialized_pb=_b(\'\\n\\\'object_detection/protos/optimizer.proto\\x12\\x17object_detection.protos\\""\\xb5\\x02\\n\\tOptimizer\\x12G\\n\\x12rms_prop_optimizer\\x18\\x01 \\x01(\\x0b\\x32).object_detection.protos.RMSPropOptimizerH\\x00\\x12H\\n\\x12momentum_optimizer\\x18\\x02 \\x01(\\x0b\\x32*.object_detection.protos.MomentumOptimizerH\\x00\\x12@\\n\\x0e\\x61\\x64\\x61m_optimizer\\x18\\x03 \\x01(\\x0b\\x32&.object_detection.protos.AdamOptimizerH\\x00\\x12 \\n\\x12use_moving_average\\x18\\x04 \\x01(\\x08:\\x04true\\x12$\\n\\x14moving_average_decay\\x18\\x05 \\x01(\\x02:\\x06\\x30.9999B\\x0b\\n\\toptimizer\\""\\x9f\\x01\\n\\x10RMSPropOptimizer\\x12<\\n\\rlearning_rate\\x18\\x01 \\x01(\\x0b\\x32%.object_detection.protos.LearningRate\\x12%\\n\\x18momentum_optimizer_value\\x18\\x02 \\x01(\\x02:\\x03\\x30.9\\x12\\x12\\n\\x05\\x64\\x65\\x63\\x61y\\x18\\x03 \\x01(\\x02:\\x03\\x30.9\\x12\\x12\\n\\x07\\x65psilon\\x18\\x04 \\x01(\\x02:\\x01\\x31\\""x\\n\\x11MomentumOptimizer\\x12<\\n\\rlearning_rate\\x18\\x01 \\x01(\\x0b\\x32%.object_detection.protos.LearningRate\\x12%\\n\\x18momentum_optimizer_value\\x18\\x02 \\x01(\\x02:\\x03\\x30.9\\""M\\n\\rAdamOptimizer\\x12<\\n\\rlearning_rate\\x18\\x01 \\x01(\\x0b\\x32%.object_detection.protos.LearningRate\\""\\x80\\x03\\n\\x0cLearningRate\\x12O\\n\\x16\\x63onstant_learning_rate\\x18\\x01 \\x01(\\x0b\\x32-.object_detection.protos.ConstantLearningRateH\\x00\\x12`\\n\\x1f\\x65xponential_decay_learning_rate\\x18\\x02 \\x01(\\x0b\\x32\\x35.object_detection.protos.ExponentialDecayLearningRateH\\x00\\x12T\\n\\x19manual_step_learning_rate\\x18\\x03 \\x01(\\x0b\\x32/.object_detection.protos.ManualStepLearningRateH\\x00\\x12V\\n\\x1a\\x63osine_decay_learning_rate\\x18\\x04 \\x01(\\x0b\\x32\\x30.object_detection.protos.CosineDecayLearningRateH\\x00\\x42\\x0f\\n\\rlearning_rate\\""4\\n\\x14\\x43onstantLearningRate\\x12\\x1c\\n\\rlearning_rate\\x18\\x01 \\x01(\\x02:\\x05\\x30.002\\""\\x97\\x01\\n\\x1c\\x45xponentialDecayLearningRate\\x12$\\n\\x15initial_learning_rate\\x18\\x01 \\x01(\\x02:\\x05\\x30.002\\x12\\x1c\\n\\x0b\\x64\\x65\\x63\\x61y_steps\\x18\\x02 \\x01(\\r:\\x07\\x34\\x30\\x30\\x30\\x30\\x30\\x30\\x12\\x1a\\n\\x0c\\x64\\x65\\x63\\x61y_factor\\x18\\x03 \\x01(\\x02:\\x04\\x30.95\\x12\\x17\\n\\tstaircase\\x18\\x04 \\x01(\\x08:\\x04true\\""\\xf1\\x01\\n\\x16ManualStepLearningRate\\x12$\\n\\x15initial_learning_rate\\x18\\x01 \\x01(\\x02:\\x05\\x30.002\\x12V\\n\\x08schedule\\x18\\x02 \\x03(\\x0b\\x32\\x44.object_detection.protos.ManualStepLearningRate.LearningRateSchedule\\x12\\x15\\n\\x06warmup\\x18\\x03 \\x01(\\x08:\\x05\\x66\\x61lse\\x1a\\x42\\n\\x14LearningRateSchedule\\x12\\x0c\\n\\x04step\\x18\\x01 \\x01(\\r\\x12\\x1c\\n\\rlearning_rate\\x18\\x02 \\x01(\\x02:\\x05\\x30.002\\""\\xbe\\x01\\n\\x17\\x43osineDecayLearningRate\\x12!\\n\\x12learning_rate_base\\x18\\x01 \\x01(\\x02:\\x05\\x30.002\\x12\\x1c\\n\\x0btotal_steps\\x18\\x02 \\x01(\\r:\\x07\\x34\\x30\\x30\\x30\\x30\\x30\\x30\\x12$\\n\\x14warmup_learning_rate\\x18\\x03 \\x01(\\x02:\\x06\\x30.0002\\x12\\x1b\\n\\x0cwarmup_steps\\x18\\x04 \\x01(\\r:\\x05\\x31\\x30\\x30\\x30\\x30\\x12\\x1f\\n\\x14hold_base_rate_steps\\x18\\x05 \\x01(\\r:\\x01\\x30\')\n)\n\n\n\n\n_OPTIMIZER = _descriptor.Descriptor(\n  name=\'Optimizer\',\n  full_name=\'object_detection.protos.Optimizer\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'rms_prop_optimizer\', full_name=\'object_detection.protos.Optimizer.rms_prop_optimizer\', index=0,\n      number=1, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'momentum_optimizer\', full_name=\'object_detection.protos.Optimizer.momentum_optimizer\', index=1,\n      number=2, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'adam_optimizer\', full_name=\'object_detection.protos.Optimizer.adam_optimizer\', index=2,\n      number=3, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'use_moving_average\', full_name=\'object_detection.protos.Optimizer.use_moving_average\', index=3,\n      number=4, type=8, cpp_type=7, label=1,\n      has_default_value=True, default_value=True,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'moving_average_decay\', full_name=\'object_detection.protos.Optimizer.moving_average_decay\', index=4,\n      number=5, type=2, cpp_type=6, label=1,\n      has_default_value=True, default_value=float(0.9999),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto2\',\n  extension_ranges=[],\n  oneofs=[\n    _descriptor.OneofDescriptor(\n      name=\'optimizer\', full_name=\'object_detection.protos.Optimizer.optimizer\',\n      index=0, containing_type=None, fields=[]),\n  ],\n  serialized_start=69,\n  serialized_end=378,\n)\n\n\n_RMSPROPOPTIMIZER = _descriptor.Descriptor(\n  name=\'RMSPropOptimizer\',\n  full_name=\'object_detection.protos.RMSPropOptimizer\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'learning_rate\', full_name=\'object_detection.protos.RMSPropOptimizer.learning_rate\', index=0,\n      number=1, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'momentum_optimizer_value\', full_name=\'object_detection.protos.RMSPropOptimizer.momentum_optimizer_value\', index=1,\n      number=2, type=2, cpp_type=6, label=1,\n      has_default_value=True, default_value=float(0.9),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'decay\', full_name=\'object_detection.protos.RMSPropOptimizer.decay\', index=2,\n      number=3, type=2, cpp_type=6, label=1,\n      has_default_value=True, default_value=float(0.9),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'epsilon\', full_name=\'object_detection.protos.RMSPropOptimizer.epsilon\', index=3,\n      number=4, type=2, cpp_type=6, label=1,\n      has_default_value=True, default_value=float(1),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto2\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=381,\n  serialized_end=540,\n)\n\n\n_MOMENTUMOPTIMIZER = _descriptor.Descriptor(\n  name=\'MomentumOptimizer\',\n  full_name=\'object_detection.protos.MomentumOptimizer\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'learning_rate\', full_name=\'object_detection.protos.MomentumOptimizer.learning_rate\', index=0,\n      number=1, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'momentum_optimizer_value\', full_name=\'object_detection.protos.MomentumOptimizer.momentum_optimizer_value\', index=1,\n      number=2, type=2, cpp_type=6, label=1,\n      has_default_value=True, default_value=float(0.9),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto2\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=542,\n  serialized_end=662,\n)\n\n\n_ADAMOPTIMIZER = _descriptor.Descriptor(\n  name=\'AdamOptimizer\',\n  full_name=\'object_detection.protos.AdamOptimizer\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'learning_rate\', full_name=\'object_detection.protos.AdamOptimizer.learning_rate\', index=0,\n      number=1, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto2\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=664,\n  serialized_end=741,\n)\n\n\n_LEARNINGRATE = _descriptor.Descriptor(\n  name=\'LearningRate\',\n  full_name=\'object_detection.protos.LearningRate\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'constant_learning_rate\', full_name=\'object_detection.protos.LearningRate.constant_learning_rate\', index=0,\n      number=1, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'exponential_decay_learning_rate\', full_name=\'object_detection.protos.LearningRate.exponential_decay_learning_rate\', index=1,\n      number=2, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'manual_step_learning_rate\', full_name=\'object_detection.protos.LearningRate.manual_step_learning_rate\', index=2,\n      number=3, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'cosine_decay_learning_rate\', full_name=\'object_detection.protos.LearningRate.cosine_decay_learning_rate\', index=3,\n      number=4, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto2\',\n  extension_ranges=[],\n  oneofs=[\n    _descriptor.OneofDescriptor(\n      name=\'learning_rate\', full_name=\'object_detection.protos.LearningRate.learning_rate\',\n      index=0, containing_type=None, fields=[]),\n  ],\n  serialized_start=744,\n  serialized_end=1128,\n)\n\n\n_CONSTANTLEARNINGRATE = _descriptor.Descriptor(\n  name=\'ConstantLearningRate\',\n  full_name=\'object_detection.protos.ConstantLearningRate\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'learning_rate\', full_name=\'object_detection.protos.ConstantLearningRate.learning_rate\', index=0,\n      number=1, type=2, cpp_type=6, label=1,\n      has_default_value=True, default_value=float(0.002),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto2\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=1130,\n  serialized_end=1182,\n)\n\n\n_EXPONENTIALDECAYLEARNINGRATE = _descriptor.Descriptor(\n  name=\'ExponentialDecayLearningRate\',\n  full_name=\'object_detection.protos.ExponentialDecayLearningRate\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'initial_learning_rate\', full_name=\'object_detection.protos.ExponentialDecayLearningRate.initial_learning_rate\', index=0,\n      number=1, type=2, cpp_type=6, label=1,\n      has_default_value=True, default_value=float(0.002),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'decay_steps\', full_name=\'object_detection.protos.ExponentialDecayLearningRate.decay_steps\', index=1,\n      number=2, type=13, cpp_type=3, label=1,\n      has_default_value=True, default_value=4000000,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'decay_factor\', full_name=\'object_detection.protos.ExponentialDecayLearningRate.decay_factor\', index=2,\n      number=3, type=2, cpp_type=6, label=1,\n      has_default_value=True, default_value=float(0.95),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'staircase\', full_name=\'object_detection.protos.ExponentialDecayLearningRate.staircase\', index=3,\n      number=4, type=8, cpp_type=7, label=1,\n      has_default_value=True, default_value=True,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto2\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=1185,\n  serialized_end=1336,\n)\n\n\n_MANUALSTEPLEARNINGRATE_LEARNINGRATESCHEDULE = _descriptor.Descriptor(\n  name=\'LearningRateSchedule\',\n  full_name=\'object_detection.protos.ManualStepLearningRate.LearningRateSchedule\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'step\', full_name=\'object_detection.protos.ManualStepLearningRate.LearningRateSchedule.step\', index=0,\n      number=1, type=13, cpp_type=3, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'learning_rate\', full_name=\'object_detection.protos.ManualStepLearningRate.LearningRateSchedule.learning_rate\', index=1,\n      number=2, type=2, cpp_type=6, label=1,\n      has_default_value=True, default_value=float(0.002),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto2\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=1514,\n  serialized_end=1580,\n)\n\n_MANUALSTEPLEARNINGRATE = _descriptor.Descriptor(\n  name=\'ManualStepLearningRate\',\n  full_name=\'object_detection.protos.ManualStepLearningRate\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'initial_learning_rate\', full_name=\'object_detection.protos.ManualStepLearningRate.initial_learning_rate\', index=0,\n      number=1, type=2, cpp_type=6, label=1,\n      has_default_value=True, default_value=float(0.002),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'schedule\', full_name=\'object_detection.protos.ManualStepLearningRate.schedule\', index=1,\n      number=2, type=11, cpp_type=10, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'warmup\', full_name=\'object_detection.protos.ManualStepLearningRate.warmup\', index=2,\n      number=3, type=8, cpp_type=7, label=1,\n      has_default_value=True, default_value=False,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[_MANUALSTEPLEARNINGRATE_LEARNINGRATESCHEDULE, ],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto2\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=1339,\n  serialized_end=1580,\n)\n\n\n_COSINEDECAYLEARNINGRATE = _descriptor.Descriptor(\n  name=\'CosineDecayLearningRate\',\n  full_name=\'object_detection.protos.CosineDecayLearningRate\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'learning_rate_base\', full_name=\'object_detection.protos.CosineDecayLearningRate.learning_rate_base\', index=0,\n      number=1, type=2, cpp_type=6, label=1,\n      has_default_value=True, default_value=float(0.002),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'total_steps\', full_name=\'object_detection.protos.CosineDecayLearningRate.total_steps\', index=1,\n      number=2, type=13, cpp_type=3, label=1,\n      has_default_value=True, default_value=4000000,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'warmup_learning_rate\', full_name=\'object_detection.protos.CosineDecayLearningRate.warmup_learning_rate\', index=2,\n      number=3, type=2, cpp_type=6, label=1,\n      has_default_value=True, default_value=float(0.0002),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'warmup_steps\', full_name=\'object_detection.protos.CosineDecayLearningRate.warmup_steps\', index=3,\n      number=4, type=13, cpp_type=3, label=1,\n      has_default_value=True, default_value=10000,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'hold_base_rate_steps\', full_name=\'object_detection.protos.CosineDecayLearningRate.hold_base_rate_steps\', index=4,\n      number=5, type=13, cpp_type=3, label=1,\n      has_default_value=True, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto2\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=1583,\n  serialized_end=1773,\n)\n\n_OPTIMIZER.fields_by_name[\'rms_prop_optimizer\'].message_type = _RMSPROPOPTIMIZER\n_OPTIMIZER.fields_by_name[\'momentum_optimizer\'].message_type = _MOMENTUMOPTIMIZER\n_OPTIMIZER.fields_by_name[\'adam_optimizer\'].message_type = _ADAMOPTIMIZER\n_OPTIMIZER.oneofs_by_name[\'optimizer\'].fields.append(\n  _OPTIMIZER.fields_by_name[\'rms_prop_optimizer\'])\n_OPTIMIZER.fields_by_name[\'rms_prop_optimizer\'].containing_oneof = _OPTIMIZER.oneofs_by_name[\'optimizer\']\n_OPTIMIZER.oneofs_by_name[\'optimizer\'].fields.append(\n  _OPTIMIZER.fields_by_name[\'momentum_optimizer\'])\n_OPTIMIZER.fields_by_name[\'momentum_optimizer\'].containing_oneof = _OPTIMIZER.oneofs_by_name[\'optimizer\']\n_OPTIMIZER.oneofs_by_name[\'optimizer\'].fields.append(\n  _OPTIMIZER.fields_by_name[\'adam_optimizer\'])\n_OPTIMIZER.fields_by_name[\'adam_optimizer\'].containing_oneof = _OPTIMIZER.oneofs_by_name[\'optimizer\']\n_RMSPROPOPTIMIZER.fields_by_name[\'learning_rate\'].message_type = _LEARNINGRATE\n_MOMENTUMOPTIMIZER.fields_by_name[\'learning_rate\'].message_type = _LEARNINGRATE\n_ADAMOPTIMIZER.fields_by_name[\'learning_rate\'].message_type = _LEARNINGRATE\n_LEARNINGRATE.fields_by_name[\'constant_learning_rate\'].message_type = _CONSTANTLEARNINGRATE\n_LEARNINGRATE.fields_by_name[\'exponential_decay_learning_rate\'].message_type = _EXPONENTIALDECAYLEARNINGRATE\n_LEARNINGRATE.fields_by_name[\'manual_step_learning_rate\'].message_type = _MANUALSTEPLEARNINGRATE\n_LEARNINGRATE.fields_by_name[\'cosine_decay_learning_rate\'].message_type = _COSINEDECAYLEARNINGRATE\n_LEARNINGRATE.oneofs_by_name[\'learning_rate\'].fields.append(\n  _LEARNINGRATE.fields_by_name[\'constant_learning_rate\'])\n_LEARNINGRATE.fields_by_name[\'constant_learning_rate\'].containing_oneof = _LEARNINGRATE.oneofs_by_name[\'learning_rate\']\n_LEARNINGRATE.oneofs_by_name[\'learning_rate\'].fields.append(\n  _LEARNINGRATE.fields_by_name[\'exponential_decay_learning_rate\'])\n_LEARNINGRATE.fields_by_name[\'exponential_decay_learning_rate\'].containing_oneof = _LEARNINGRATE.oneofs_by_name[\'learning_rate\']\n_LEARNINGRATE.oneofs_by_name[\'learning_rate\'].fields.append(\n  _LEARNINGRATE.fields_by_name[\'manual_step_learning_rate\'])\n_LEARNINGRATE.fields_by_name[\'manual_step_learning_rate\'].containing_oneof = _LEARNINGRATE.oneofs_by_name[\'learning_rate\']\n_LEARNINGRATE.oneofs_by_name[\'learning_rate\'].fields.append(\n  _LEARNINGRATE.fields_by_name[\'cosine_decay_learning_rate\'])\n_LEARNINGRATE.fields_by_name[\'cosine_decay_learning_rate\'].containing_oneof = _LEARNINGRATE.oneofs_by_name[\'learning_rate\']\n_MANUALSTEPLEARNINGRATE_LEARNINGRATESCHEDULE.containing_type = _MANUALSTEPLEARNINGRATE\n_MANUALSTEPLEARNINGRATE.fields_by_name[\'schedule\'].message_type = _MANUALSTEPLEARNINGRATE_LEARNINGRATESCHEDULE\nDESCRIPTOR.message_types_by_name[\'Optimizer\'] = _OPTIMIZER\nDESCRIPTOR.message_types_by_name[\'RMSPropOptimizer\'] = _RMSPROPOPTIMIZER\nDESCRIPTOR.message_types_by_name[\'MomentumOptimizer\'] = _MOMENTUMOPTIMIZER\nDESCRIPTOR.message_types_by_name[\'AdamOptimizer\'] = _ADAMOPTIMIZER\nDESCRIPTOR.message_types_by_name[\'LearningRate\'] = _LEARNINGRATE\nDESCRIPTOR.message_types_by_name[\'ConstantLearningRate\'] = _CONSTANTLEARNINGRATE\nDESCRIPTOR.message_types_by_name[\'ExponentialDecayLearningRate\'] = _EXPONENTIALDECAYLEARNINGRATE\nDESCRIPTOR.message_types_by_name[\'ManualStepLearningRate\'] = _MANUALSTEPLEARNINGRATE\nDESCRIPTOR.message_types_by_name[\'CosineDecayLearningRate\'] = _COSINEDECAYLEARNINGRATE\n_sym_db.RegisterFileDescriptor(DESCRIPTOR)\n\nOptimizer = _reflection.GeneratedProtocolMessageType(\'Optimizer\', (_message.Message,), dict(\n  DESCRIPTOR = _OPTIMIZER,\n  __module__ = \'object_detection.protos.optimizer_pb2\'\n  # @@protoc_insertion_point(class_scope:object_detection.protos.Optimizer)\n  ))\n_sym_db.RegisterMessage(Optimizer)\n\nRMSPropOptimizer = _reflection.GeneratedProtocolMessageType(\'RMSPropOptimizer\', (_message.Message,), dict(\n  DESCRIPTOR = _RMSPROPOPTIMIZER,\n  __module__ = \'object_detection.protos.optimizer_pb2\'\n  # @@protoc_insertion_point(class_scope:object_detection.protos.RMSPropOptimizer)\n  ))\n_sym_db.RegisterMessage(RMSPropOptimizer)\n\nMomentumOptimizer = _reflection.GeneratedProtocolMessageType(\'MomentumOptimizer\', (_message.Message,), dict(\n  DESCRIPTOR = _MOMENTUMOPTIMIZER,\n  __module__ = \'object_detection.protos.optimizer_pb2\'\n  # @@protoc_insertion_point(class_scope:object_detection.protos.MomentumOptimizer)\n  ))\n_sym_db.RegisterMessage(MomentumOptimizer)\n\nAdamOptimizer = _reflection.GeneratedProtocolMessageType(\'AdamOptimizer\', (_message.Message,), dict(\n  DESCRIPTOR = _ADAMOPTIMIZER,\n  __module__ = \'object_detection.protos.optimizer_pb2\'\n  # @@protoc_insertion_point(class_scope:object_detection.protos.AdamOptimizer)\n  ))\n_sym_db.RegisterMessage(AdamOptimizer)\n\nLearningRate = _reflection.GeneratedProtocolMessageType(\'LearningRate\', (_message.Message,), dict(\n  DESCRIPTOR = _LEARNINGRATE,\n  __module__ = \'object_detection.protos.optimizer_pb2\'\n  # @@protoc_insertion_point(class_scope:object_detection.protos.LearningRate)\n  ))\n_sym_db.RegisterMessage(LearningRate)\n\nConstantLearningRate = _reflection.GeneratedProtocolMessageType(\'ConstantLearningRate\', (_message.Message,), dict(\n  DESCRIPTOR = _CONSTANTLEARNINGRATE,\n  __module__ = \'object_detection.protos.optimizer_pb2\'\n  # @@protoc_insertion_point(class_scope:object_detection.protos.ConstantLearningRate)\n  ))\n_sym_db.RegisterMessage(ConstantLearningRate)\n\nExponentialDecayLearningRate = _reflection.GeneratedProtocolMessageType(\'ExponentialDecayLearningRate\', (_message.Message,), dict(\n  DESCRIPTOR = _EXPONENTIALDECAYLEARNINGRATE,\n  __module__ = \'object_detection.protos.optimizer_pb2\'\n  # @@protoc_insertion_point(class_scope:object_detection.protos.ExponentialDecayLearningRate)\n  ))\n_sym_db.RegisterMessage(ExponentialDecayLearningRate)\n\nManualStepLearningRate = _reflection.GeneratedProtocolMessageType(\'ManualStepLearningRate\', (_message.Message,), dict(\n\n  LearningRateSchedule = _reflection.GeneratedProtocolMessageType(\'LearningRateSchedule\', (_message.Message,), dict(\n    DESCRIPTOR = _MANUALSTEPLEARNINGRATE_LEARNINGRATESCHEDULE,\n    __module__ = \'object_detection.protos.optimizer_pb2\'\n    # @@protoc_insertion_point(class_scope:object_detection.protos.ManualStepLearningRate.LearningRateSchedule)\n    ))\n  ,\n  DESCRIPTOR = _MANUALSTEPLEARNINGRATE,\n  __module__ = \'object_detection.protos.optimizer_pb2\'\n  # @@protoc_insertion_point(class_scope:object_detection.protos.ManualStepLearningRate)\n  ))\n_sym_db.RegisterMessage(ManualStepLearningRate)\n_sym_db.RegisterMessage(ManualStepLearningRate.LearningRateSchedule)\n\nCosineDecayLearningRate = _reflection.GeneratedProtocolMessageType(\'CosineDecayLearningRate\', (_message.Message,), dict(\n  DESCRIPTOR = _COSINEDECAYLEARNINGRATE,\n  __module__ = \'object_detection.protos.optimizer_pb2\'\n  # @@protoc_insertion_point(class_scope:object_detection.protos.CosineDecayLearningRate)\n  ))\n_sym_db.RegisterMessage(CosineDecayLearningRate)\n\n\n# @@protoc_insertion_point(module_scope)\n'"
src/object_detection/protos/pipeline_pb2.py,0,"b'# Generated by the protocol buffer compiler.  DO NOT EDIT!\n# source: object_detection/protos/pipeline.proto\n\nimport sys\n_b=sys.version_info[0]<3 and (lambda x:x) or (lambda x:x.encode(\'latin1\'))\nfrom google.protobuf import descriptor as _descriptor\nfrom google.protobuf import message as _message\nfrom google.protobuf import reflection as _reflection\nfrom google.protobuf import symbol_database as _symbol_database\nfrom google.protobuf import descriptor_pb2\n# @@protoc_insertion_point(imports)\n\n_sym_db = _symbol_database.Default()\n\n\nfrom object_detection.protos import eval_pb2 as object__detection_dot_protos_dot_eval__pb2\nfrom object_detection.protos import input_reader_pb2 as object__detection_dot_protos_dot_input__reader__pb2\nfrom object_detection.protos import model_pb2 as object__detection_dot_protos_dot_model__pb2\nfrom object_detection.protos import train_pb2 as object__detection_dot_protos_dot_train__pb2\n\n\nDESCRIPTOR = _descriptor.FileDescriptor(\n  name=\'object_detection/protos/pipeline.proto\',\n  package=\'object_detection.protos\',\n  syntax=\'proto2\',\n  serialized_pb=_b(\'\\n&object_detection/protos/pipeline.proto\\x12\\x17object_detection.protos\\x1a\\""object_detection/protos/eval.proto\\x1a*object_detection/protos/input_reader.proto\\x1a#object_detection/protos/model.proto\\x1a#object_detection/protos/train.proto\\""\\xd5\\x02\\n\\x17TrainEvalPipelineConfig\\x12\\x36\\n\\x05model\\x18\\x01 \\x01(\\x0b\\x32\\\'.object_detection.protos.DetectionModel\\x12:\\n\\x0ctrain_config\\x18\\x02 \\x01(\\x0b\\x32$.object_detection.protos.TrainConfig\\x12@\\n\\x12train_input_reader\\x18\\x03 \\x01(\\x0b\\x32$.object_detection.protos.InputReader\\x12\\x38\\n\\x0b\\x65val_config\\x18\\x04 \\x01(\\x0b\\x32#.object_detection.protos.EvalConfig\\x12?\\n\\x11\\x65val_input_reader\\x18\\x05 \\x01(\\x0b\\x32$.object_detection.protos.InputReader*\\t\\x08\\xe8\\x07\\x10\\x80\\x80\\x80\\x80\\x02\')\n  ,\n  dependencies=[object__detection_dot_protos_dot_eval__pb2.DESCRIPTOR,object__detection_dot_protos_dot_input__reader__pb2.DESCRIPTOR,object__detection_dot_protos_dot_model__pb2.DESCRIPTOR,object__detection_dot_protos_dot_train__pb2.DESCRIPTOR,])\n\n\n\n\n_TRAINEVALPIPELINECONFIG = _descriptor.Descriptor(\n  name=\'TrainEvalPipelineConfig\',\n  full_name=\'object_detection.protos.TrainEvalPipelineConfig\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'model\', full_name=\'object_detection.protos.TrainEvalPipelineConfig.model\', index=0,\n      number=1, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'train_config\', full_name=\'object_detection.protos.TrainEvalPipelineConfig.train_config\', index=1,\n      number=2, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'train_input_reader\', full_name=\'object_detection.protos.TrainEvalPipelineConfig.train_input_reader\', index=2,\n      number=3, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'eval_config\', full_name=\'object_detection.protos.TrainEvalPipelineConfig.eval_config\', index=3,\n      number=4, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'eval_input_reader\', full_name=\'object_detection.protos.TrainEvalPipelineConfig.eval_input_reader\', index=4,\n      number=5, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=True,\n  syntax=\'proto2\',\n  extension_ranges=[(1000, 536870912), ],\n  oneofs=[\n  ],\n  serialized_start=222,\n  serialized_end=563,\n)\n\n_TRAINEVALPIPELINECONFIG.fields_by_name[\'model\'].message_type = object__detection_dot_protos_dot_model__pb2._DETECTIONMODEL\n_TRAINEVALPIPELINECONFIG.fields_by_name[\'train_config\'].message_type = object__detection_dot_protos_dot_train__pb2._TRAINCONFIG\n_TRAINEVALPIPELINECONFIG.fields_by_name[\'train_input_reader\'].message_type = object__detection_dot_protos_dot_input__reader__pb2._INPUTREADER\n_TRAINEVALPIPELINECONFIG.fields_by_name[\'eval_config\'].message_type = object__detection_dot_protos_dot_eval__pb2._EVALCONFIG\n_TRAINEVALPIPELINECONFIG.fields_by_name[\'eval_input_reader\'].message_type = object__detection_dot_protos_dot_input__reader__pb2._INPUTREADER\nDESCRIPTOR.message_types_by_name[\'TrainEvalPipelineConfig\'] = _TRAINEVALPIPELINECONFIG\n_sym_db.RegisterFileDescriptor(DESCRIPTOR)\n\nTrainEvalPipelineConfig = _reflection.GeneratedProtocolMessageType(\'TrainEvalPipelineConfig\', (_message.Message,), dict(\n  DESCRIPTOR = _TRAINEVALPIPELINECONFIG,\n  __module__ = \'object_detection.protos.pipeline_pb2\'\n  # @@protoc_insertion_point(class_scope:object_detection.protos.TrainEvalPipelineConfig)\n  ))\n_sym_db.RegisterMessage(TrainEvalPipelineConfig)\n\n\n# @@protoc_insertion_point(module_scope)\n'"
src/object_detection/protos/post_processing_pb2.py,0,"b'# Generated by the protocol buffer compiler.  DO NOT EDIT!\n# source: object_detection/protos/post_processing.proto\n\nimport sys\n_b=sys.version_info[0]<3 and (lambda x:x) or (lambda x:x.encode(\'latin1\'))\nfrom google.protobuf import descriptor as _descriptor\nfrom google.protobuf import message as _message\nfrom google.protobuf import reflection as _reflection\nfrom google.protobuf import symbol_database as _symbol_database\nfrom google.protobuf import descriptor_pb2\n# @@protoc_insertion_point(imports)\n\n_sym_db = _symbol_database.Default()\n\n\n\n\nDESCRIPTOR = _descriptor.FileDescriptor(\n  name=\'object_detection/protos/post_processing.proto\',\n  package=\'object_detection.protos\',\n  syntax=\'proto2\',\n  serialized_pb=_b(\'\\n-object_detection/protos/post_processing.proto\\x12\\x17object_detection.protos\\""\\x9a\\x01\\n\\x16\\x42\\x61tchNonMaxSuppression\\x12\\x1a\\n\\x0fscore_threshold\\x18\\x01 \\x01(\\x02:\\x01\\x30\\x12\\x1a\\n\\riou_threshold\\x18\\x02 \\x01(\\x02:\\x03\\x30.6\\x12%\\n\\x18max_detections_per_class\\x18\\x03 \\x01(\\x05:\\x03\\x31\\x30\\x30\\x12!\\n\\x14max_total_detections\\x18\\x05 \\x01(\\x05:\\x03\\x31\\x30\\x30\\""\\x91\\x02\\n\\x0ePostProcessing\\x12R\\n\\x19\\x62\\x61tch_non_max_suppression\\x18\\x01 \\x01(\\x0b\\x32/.object_detection.protos.BatchNonMaxSuppression\\x12Y\\n\\x0fscore_converter\\x18\\x02 \\x01(\\x0e\\x32\\x36.object_detection.protos.PostProcessing.ScoreConverter:\\x08IDENTITY\\x12\\x16\\n\\x0blogit_scale\\x18\\x03 \\x01(\\x02:\\x01\\x31\\""8\\n\\x0eScoreConverter\\x12\\x0c\\n\\x08IDENTITY\\x10\\x00\\x12\\x0b\\n\\x07SIGMOID\\x10\\x01\\x12\\x0b\\n\\x07SOFTMAX\\x10\\x02\')\n)\n\n\n\n_POSTPROCESSING_SCORECONVERTER = _descriptor.EnumDescriptor(\n  name=\'ScoreConverter\',\n  full_name=\'object_detection.protos.PostProcessing.ScoreConverter\',\n  filename=None,\n  file=DESCRIPTOR,\n  values=[\n    _descriptor.EnumValueDescriptor(\n      name=\'IDENTITY\', index=0, number=0,\n      options=None,\n      type=None),\n    _descriptor.EnumValueDescriptor(\n      name=\'SIGMOID\', index=1, number=1,\n      options=None,\n      type=None),\n    _descriptor.EnumValueDescriptor(\n      name=\'SOFTMAX\', index=2, number=2,\n      options=None,\n      type=None),\n  ],\n  containing_type=None,\n  options=None,\n  serialized_start=449,\n  serialized_end=505,\n)\n_sym_db.RegisterEnumDescriptor(_POSTPROCESSING_SCORECONVERTER)\n\n\n_BATCHNONMAXSUPPRESSION = _descriptor.Descriptor(\n  name=\'BatchNonMaxSuppression\',\n  full_name=\'object_detection.protos.BatchNonMaxSuppression\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'score_threshold\', full_name=\'object_detection.protos.BatchNonMaxSuppression.score_threshold\', index=0,\n      number=1, type=2, cpp_type=6, label=1,\n      has_default_value=True, default_value=float(0),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'iou_threshold\', full_name=\'object_detection.protos.BatchNonMaxSuppression.iou_threshold\', index=1,\n      number=2, type=2, cpp_type=6, label=1,\n      has_default_value=True, default_value=float(0.6),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'max_detections_per_class\', full_name=\'object_detection.protos.BatchNonMaxSuppression.max_detections_per_class\', index=2,\n      number=3, type=5, cpp_type=1, label=1,\n      has_default_value=True, default_value=100,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'max_total_detections\', full_name=\'object_detection.protos.BatchNonMaxSuppression.max_total_detections\', index=3,\n      number=5, type=5, cpp_type=1, label=1,\n      has_default_value=True, default_value=100,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto2\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=75,\n  serialized_end=229,\n)\n\n\n_POSTPROCESSING = _descriptor.Descriptor(\n  name=\'PostProcessing\',\n  full_name=\'object_detection.protos.PostProcessing\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'batch_non_max_suppression\', full_name=\'object_detection.protos.PostProcessing.batch_non_max_suppression\', index=0,\n      number=1, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'score_converter\', full_name=\'object_detection.protos.PostProcessing.score_converter\', index=1,\n      number=2, type=14, cpp_type=8, label=1,\n      has_default_value=True, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'logit_scale\', full_name=\'object_detection.protos.PostProcessing.logit_scale\', index=2,\n      number=3, type=2, cpp_type=6, label=1,\n      has_default_value=True, default_value=float(1),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n    _POSTPROCESSING_SCORECONVERTER,\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto2\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=232,\n  serialized_end=505,\n)\n\n_POSTPROCESSING.fields_by_name[\'batch_non_max_suppression\'].message_type = _BATCHNONMAXSUPPRESSION\n_POSTPROCESSING.fields_by_name[\'score_converter\'].enum_type = _POSTPROCESSING_SCORECONVERTER\n_POSTPROCESSING_SCORECONVERTER.containing_type = _POSTPROCESSING\nDESCRIPTOR.message_types_by_name[\'BatchNonMaxSuppression\'] = _BATCHNONMAXSUPPRESSION\nDESCRIPTOR.message_types_by_name[\'PostProcessing\'] = _POSTPROCESSING\n_sym_db.RegisterFileDescriptor(DESCRIPTOR)\n\nBatchNonMaxSuppression = _reflection.GeneratedProtocolMessageType(\'BatchNonMaxSuppression\', (_message.Message,), dict(\n  DESCRIPTOR = _BATCHNONMAXSUPPRESSION,\n  __module__ = \'object_detection.protos.post_processing_pb2\'\n  # @@protoc_insertion_point(class_scope:object_detection.protos.BatchNonMaxSuppression)\n  ))\n_sym_db.RegisterMessage(BatchNonMaxSuppression)\n\nPostProcessing = _reflection.GeneratedProtocolMessageType(\'PostProcessing\', (_message.Message,), dict(\n  DESCRIPTOR = _POSTPROCESSING,\n  __module__ = \'object_detection.protos.post_processing_pb2\'\n  # @@protoc_insertion_point(class_scope:object_detection.protos.PostProcessing)\n  ))\n_sym_db.RegisterMessage(PostProcessing)\n\n\n# @@protoc_insertion_point(module_scope)\n'"
src/object_detection/protos/preprocessor_pb2.py,0,"b'# Generated by the protocol buffer compiler.  DO NOT EDIT!\n# source: object_detection/protos/preprocessor.proto\n\nimport sys\n_b=sys.version_info[0]<3 and (lambda x:x) or (lambda x:x.encode(\'latin1\'))\nfrom google.protobuf import descriptor as _descriptor\nfrom google.protobuf import message as _message\nfrom google.protobuf import reflection as _reflection\nfrom google.protobuf import symbol_database as _symbol_database\nfrom google.protobuf import descriptor_pb2\n# @@protoc_insertion_point(imports)\n\n_sym_db = _symbol_database.Default()\n\n\n\n\nDESCRIPTOR = _descriptor.FileDescriptor(\n  name=\'object_detection/protos/preprocessor.proto\',\n  package=\'object_detection.protos\',\n  syntax=\'proto2\',\n  serialized_pb=_b(\'\\n*object_detection/protos/preprocessor.proto\\x12\\x17object_detection.protos\\""\\xea\\x10\\n\\x11PreprocessingStep\\x12\\x42\\n\\x0fnormalize_image\\x18\\x01 \\x01(\\x0b\\x32\\\'.object_detection.protos.NormalizeImageH\\x00\\x12O\\n\\x16random_horizontal_flip\\x18\\x02 \\x01(\\x0b\\x32-.object_detection.protos.RandomHorizontalFlipH\\x00\\x12R\\n\\x18random_pixel_value_scale\\x18\\x03 \\x01(\\x0b\\x32..object_detection.protos.RandomPixelValueScaleH\\x00\\x12G\\n\\x12random_image_scale\\x18\\x04 \\x01(\\x0b\\x32).object_detection.protos.RandomImageScaleH\\x00\\x12\\x46\\n\\x12random_rgb_to_gray\\x18\\x05 \\x01(\\x0b\\x32(.object_detection.protos.RandomRGBtoGrayH\\x00\\x12S\\n\\x18random_adjust_brightness\\x18\\x06 \\x01(\\x0b\\x32/.object_detection.protos.RandomAdjustBrightnessH\\x00\\x12O\\n\\x16random_adjust_contrast\\x18\\x07 \\x01(\\x0b\\x32-.object_detection.protos.RandomAdjustContrastH\\x00\\x12\\x45\\n\\x11random_adjust_hue\\x18\\x08 \\x01(\\x0b\\x32(.object_detection.protos.RandomAdjustHueH\\x00\\x12S\\n\\x18random_adjust_saturation\\x18\\t \\x01(\\x0b\\x32/.object_detection.protos.RandomAdjustSaturationH\\x00\\x12K\\n\\x14random_distort_color\\x18\\n \\x01(\\x0b\\x32+.object_detection.protos.RandomDistortColorH\\x00\\x12I\\n\\x13random_jitter_boxes\\x18\\x0b \\x01(\\x0b\\x32*.object_detection.protos.RandomJitterBoxesH\\x00\\x12\\x45\\n\\x11random_crop_image\\x18\\x0c \\x01(\\x0b\\x32(.object_detection.protos.RandomCropImageH\\x00\\x12\\x43\\n\\x10random_pad_image\\x18\\r \\x01(\\x0b\\x32\\\'.object_detection.protos.RandomPadImageH\\x00\\x12L\\n\\x15random_crop_pad_image\\x18\\x0e \\x01(\\x0b\\x32+.object_detection.protos.RandomCropPadImageH\\x00\\x12W\\n\\x1brandom_crop_to_aspect_ratio\\x18\\x0f \\x01(\\x0b\\x32\\x30.object_detection.protos.RandomCropToAspectRatioH\\x00\\x12K\\n\\x14random_black_patches\\x18\\x10 \\x01(\\x0b\\x32+.object_detection.protos.RandomBlackPatchesH\\x00\\x12K\\n\\x14random_resize_method\\x18\\x11 \\x01(\\x0b\\x32+.object_detection.protos.RandomResizeMethodH\\x00\\x12\\x61\\n scale_boxes_to_pixel_coordinates\\x18\\x12 \\x01(\\x0b\\x32\\x35.object_detection.protos.ScaleBoxesToPixelCoordinatesH\\x00\\x12<\\n\\x0cresize_image\\x18\\x13 \\x01(\\x0b\\x32$.object_detection.protos.ResizeImageH\\x00\\x12M\\n\\x15subtract_channel_mean\\x18\\x14 \\x01(\\x0b\\x32,.object_detection.protos.SubtractChannelMeanH\\x00\\x12\\x41\\n\\x0fssd_random_crop\\x18\\x15 \\x01(\\x0b\\x32&.object_detection.protos.SSDRandomCropH\\x00\\x12H\\n\\x13ssd_random_crop_pad\\x18\\x16 \\x01(\\x0b\\x32).object_detection.protos.SSDRandomCropPadH\\x00\\x12\\x64\\n\\""ssd_random_crop_fixed_aspect_ratio\\x18\\x17 \\x01(\\x0b\\x32\\x36.object_detection.protos.SSDRandomCropFixedAspectRatioH\\x00\\x12k\\n&ssd_random_crop_pad_fixed_aspect_ratio\\x18\\x18 \\x01(\\x0b\\x32\\x39.object_detection.protos.SSDRandomCropPadFixedAspectRatioH\\x00\\x12K\\n\\x14random_vertical_flip\\x18\\x19 \\x01(\\x0b\\x32+.object_detection.protos.RandomVerticalFlipH\\x00\\x12\\x46\\n\\x11random_rotation90\\x18\\x1a \\x01(\\x0b\\x32).object_detection.protos.RandomRotation90H\\x00\\x12\\x39\\n\\x0brgb_to_gray\\x18\\x1b \\x01(\\x0b\\x32\\"".object_detection.protos.RGBtoGrayH\\x00\\x42\\x14\\n\\x12preprocessing_step\\""v\\n\\x0eNormalizeImage\\x12\\x17\\n\\x0foriginal_minval\\x18\\x01 \\x01(\\x02\\x12\\x17\\n\\x0foriginal_maxval\\x18\\x02 \\x01(\\x02\\x12\\x18\\n\\rtarget_minval\\x18\\x03 \\x01(\\x02:\\x01\\x30\\x12\\x18\\n\\rtarget_maxval\\x18\\x04 \\x01(\\x02:\\x01\\x31\\""9\\n\\x14RandomHorizontalFlip\\x12!\\n\\x19keypoint_flip_permutation\\x18\\x01 \\x03(\\x05\\""7\\n\\x12RandomVerticalFlip\\x12!\\n\\x19keypoint_flip_permutation\\x18\\x01 \\x03(\\x05\\""\\x12\\n\\x10RandomRotation90\\""A\\n\\x15RandomPixelValueScale\\x12\\x13\\n\\x06minval\\x18\\x01 \\x01(\\x02:\\x03\\x30.9\\x12\\x13\\n\\x06maxval\\x18\\x02 \\x01(\\x02:\\x03\\x31.1\\""L\\n\\x10RandomImageScale\\x12\\x1c\\n\\x0fmin_scale_ratio\\x18\\x01 \\x01(\\x02:\\x03\\x30.5\\x12\\x1a\\n\\x0fmax_scale_ratio\\x18\\x02 \\x01(\\x02:\\x01\\x32\\""+\\n\\x0fRandomRGBtoGray\\x12\\x18\\n\\x0bprobability\\x18\\x01 \\x01(\\x02:\\x03\\x30.1\\""0\\n\\x16RandomAdjustBrightness\\x12\\x16\\n\\tmax_delta\\x18\\x01 \\x01(\\x02:\\x03\\x30.2\\""G\\n\\x14RandomAdjustContrast\\x12\\x16\\n\\tmin_delta\\x18\\x01 \\x01(\\x02:\\x03\\x30.8\\x12\\x17\\n\\tmax_delta\\x18\\x02 \\x01(\\x02:\\x04\\x31.25\\""*\\n\\x0fRandomAdjustHue\\x12\\x17\\n\\tmax_delta\\x18\\x01 \\x01(\\x02:\\x04\\x30.02\\""I\\n\\x16RandomAdjustSaturation\\x12\\x16\\n\\tmin_delta\\x18\\x01 \\x01(\\x02:\\x03\\x30.8\\x12\\x17\\n\\tmax_delta\\x18\\x02 \\x01(\\x02:\\x04\\x31.25\\"",\\n\\x12RandomDistortColor\\x12\\x16\\n\\x0e\\x63olor_ordering\\x18\\x01 \\x01(\\x05\\""(\\n\\x11RandomJitterBoxes\\x12\\x13\\n\\x05ratio\\x18\\x01 \\x01(\\x02:\\x04\\x30.05\\""\\xd1\\x01\\n\\x0fRandomCropImage\\x12\\x1d\\n\\x12min_object_covered\\x18\\x01 \\x01(\\x02:\\x01\\x31\\x12\\x1e\\n\\x10min_aspect_ratio\\x18\\x02 \\x01(\\x02:\\x04\\x30.75\\x12\\x1e\\n\\x10max_aspect_ratio\\x18\\x03 \\x01(\\x02:\\x04\\x31.33\\x12\\x15\\n\\x08min_area\\x18\\x04 \\x01(\\x02:\\x03\\x30.1\\x12\\x13\\n\\x08max_area\\x18\\x05 \\x01(\\x02:\\x01\\x31\\x12\\x1b\\n\\x0eoverlap_thresh\\x18\\x06 \\x01(\\x02:\\x03\\x30.3\\x12\\x16\\n\\x0brandom_coef\\x18\\x07 \\x01(\\x02:\\x01\\x30\\""\\x89\\x01\\n\\x0eRandomPadImage\\x12\\x18\\n\\x10min_image_height\\x18\\x01 \\x01(\\x02\\x12\\x17\\n\\x0fmin_image_width\\x18\\x02 \\x01(\\x02\\x12\\x18\\n\\x10max_image_height\\x18\\x03 \\x01(\\x02\\x12\\x17\\n\\x0fmax_image_width\\x18\\x04 \\x01(\\x02\\x12\\x11\\n\\tpad_color\\x18\\x05 \\x03(\\x02\\""\\xa5\\x02\\n\\x12RandomCropPadImage\\x12\\x1d\\n\\x12min_object_covered\\x18\\x01 \\x01(\\x02:\\x01\\x31\\x12\\x1e\\n\\x10min_aspect_ratio\\x18\\x02 \\x01(\\x02:\\x04\\x30.75\\x12\\x1e\\n\\x10max_aspect_ratio\\x18\\x03 \\x01(\\x02:\\x04\\x31.33\\x12\\x15\\n\\x08min_area\\x18\\x04 \\x01(\\x02:\\x03\\x30.1\\x12\\x13\\n\\x08max_area\\x18\\x05 \\x01(\\x02:\\x01\\x31\\x12\\x1b\\n\\x0eoverlap_thresh\\x18\\x06 \\x01(\\x02:\\x03\\x30.3\\x12\\x16\\n\\x0brandom_coef\\x18\\x07 \\x01(\\x02:\\x01\\x30\\x12\\x1d\\n\\x15min_padded_size_ratio\\x18\\x08 \\x03(\\x02\\x12\\x1d\\n\\x15max_padded_size_ratio\\x18\\t \\x03(\\x02\\x12\\x11\\n\\tpad_color\\x18\\n \\x03(\\x02\\""O\\n\\x17RandomCropToAspectRatio\\x12\\x17\\n\\x0c\\x61spect_ratio\\x18\\x01 \\x01(\\x02:\\x01\\x31\\x12\\x1b\\n\\x0eoverlap_thresh\\x18\\x02 \\x01(\\x02:\\x03\\x30.3\\""o\\n\\x12RandomBlackPatches\\x12\\x1d\\n\\x11max_black_patches\\x18\\x01 \\x01(\\x05:\\x02\\x31\\x30\\x12\\x18\\n\\x0bprobability\\x18\\x02 \\x01(\\x02:\\x03\\x30.5\\x12 \\n\\x13size_to_image_ratio\\x18\\x03 \\x01(\\x02:\\x03\\x30.1\\""A\\n\\x12RandomResizeMethod\\x12\\x15\\n\\rtarget_height\\x18\\x01 \\x01(\\x02\\x12\\x14\\n\\x0ctarget_width\\x18\\x02 \\x01(\\x02\\""\\x0b\\n\\tRGBtoGray\\""\\x1e\\n\\x1cScaleBoxesToPixelCoordinates\\""\\xc0\\x01\\n\\x0bResizeImage\\x12\\x12\\n\\nnew_height\\x18\\x01 \\x01(\\x05\\x12\\x11\\n\\tnew_width\\x18\\x02 \\x01(\\x05\\x12\\x45\\n\\x06method\\x18\\x03 \\x01(\\x0e\\x32+.object_detection.protos.ResizeImage.Method:\\x08\\x42ILINEAR\\""C\\n\\x06Method\\x12\\x08\\n\\x04\\x41REA\\x10\\x01\\x12\\x0b\\n\\x07\\x42ICUBIC\\x10\\x02\\x12\\x0c\\n\\x08\\x42ILINEAR\\x10\\x03\\x12\\x14\\n\\x10NEAREST_NEIGHBOR\\x10\\x04\\""$\\n\\x13SubtractChannelMean\\x12\\r\\n\\x05means\\x18\\x01 \\x03(\\x02\\""\\xb9\\x01\\n\\x16SSDRandomCropOperation\\x12\\x1a\\n\\x12min_object_covered\\x18\\x01 \\x01(\\x02\\x12\\x18\\n\\x10min_aspect_ratio\\x18\\x02 \\x01(\\x02\\x12\\x18\\n\\x10max_aspect_ratio\\x18\\x03 \\x01(\\x02\\x12\\x10\\n\\x08min_area\\x18\\x04 \\x01(\\x02\\x12\\x10\\n\\x08max_area\\x18\\x05 \\x01(\\x02\\x12\\x16\\n\\x0eoverlap_thresh\\x18\\x06 \\x01(\\x02\\x12\\x13\\n\\x0brandom_coef\\x18\\x07 \\x01(\\x02\\""T\\n\\rSSDRandomCrop\\x12\\x43\\n\\noperations\\x18\\x01 \\x03(\\x0b\\x32/.object_detection.protos.SSDRandomCropOperation\\""\\xb9\\x02\\n\\x19SSDRandomCropPadOperation\\x12\\x1a\\n\\x12min_object_covered\\x18\\x01 \\x01(\\x02\\x12\\x18\\n\\x10min_aspect_ratio\\x18\\x02 \\x01(\\x02\\x12\\x18\\n\\x10max_aspect_ratio\\x18\\x03 \\x01(\\x02\\x12\\x10\\n\\x08min_area\\x18\\x04 \\x01(\\x02\\x12\\x10\\n\\x08max_area\\x18\\x05 \\x01(\\x02\\x12\\x16\\n\\x0eoverlap_thresh\\x18\\x06 \\x01(\\x02\\x12\\x13\\n\\x0brandom_coef\\x18\\x07 \\x01(\\x02\\x12\\x1d\\n\\x15min_padded_size_ratio\\x18\\x08 \\x03(\\x02\\x12\\x1d\\n\\x15max_padded_size_ratio\\x18\\t \\x03(\\x02\\x12\\x13\\n\\x0bpad_color_r\\x18\\n \\x01(\\x02\\x12\\x13\\n\\x0bpad_color_g\\x18\\x0b \\x01(\\x02\\x12\\x13\\n\\x0bpad_color_b\\x18\\x0c \\x01(\\x02\\""Z\\n\\x10SSDRandomCropPad\\x12\\x46\\n\\noperations\\x18\\x01 \\x03(\\x0b\\x32\\x32.object_detection.protos.SSDRandomCropPadOperation\\""\\x95\\x01\\n&SSDRandomCropFixedAspectRatioOperation\\x12\\x1a\\n\\x12min_object_covered\\x18\\x01 \\x01(\\x02\\x12\\x10\\n\\x08min_area\\x18\\x04 \\x01(\\x02\\x12\\x10\\n\\x08max_area\\x18\\x05 \\x01(\\x02\\x12\\x16\\n\\x0eoverlap_thresh\\x18\\x06 \\x01(\\x02\\x12\\x13\\n\\x0brandom_coef\\x18\\x07 \\x01(\\x02\\""\\x8d\\x01\\n\\x1dSSDRandomCropFixedAspectRatio\\x12S\\n\\noperations\\x18\\x01 \\x03(\\x0b\\x32?.object_detection.protos.SSDRandomCropFixedAspectRatioOperation\\x12\\x17\\n\\x0c\\x61spect_ratio\\x18\\x02 \\x01(\\x02:\\x01\\x31\\""\\xcc\\x01\\n)SSDRandomCropPadFixedAspectRatioOperation\\x12\\x1a\\n\\x12min_object_covered\\x18\\x01 \\x01(\\x02\\x12\\x18\\n\\x10min_aspect_ratio\\x18\\x02 \\x01(\\x02\\x12\\x18\\n\\x10max_aspect_ratio\\x18\\x03 \\x01(\\x02\\x12\\x10\\n\\x08min_area\\x18\\x04 \\x01(\\x02\\x12\\x10\\n\\x08max_area\\x18\\x05 \\x01(\\x02\\x12\\x16\\n\\x0eoverlap_thresh\\x18\\x06 \\x01(\\x02\\x12\\x13\\n\\x0brandom_coef\\x18\\x07 \\x01(\\x02\\""\\xd1\\x01\\n SSDRandomCropPadFixedAspectRatio\\x12V\\n\\noperations\\x18\\x01 \\x03(\\x0b\\x32\\x42.object_detection.protos.SSDRandomCropPadFixedAspectRatioOperation\\x12\\x17\\n\\x0c\\x61spect_ratio\\x18\\x02 \\x01(\\x02:\\x01\\x31\\x12\\x1d\\n\\x15min_padded_size_ratio\\x18\\x03 \\x03(\\x02\\x12\\x1d\\n\\x15max_padded_size_ratio\\x18\\x04 \\x03(\\x02\')\n)\n\n\n\n_RESIZEIMAGE_METHOD = _descriptor.EnumDescriptor(\n  name=\'Method\',\n  full_name=\'object_detection.protos.ResizeImage.Method\',\n  filename=None,\n  file=DESCRIPTOR,\n  values=[\n    _descriptor.EnumValueDescriptor(\n      name=\'AREA\', index=0, number=1,\n      options=None,\n      type=None),\n    _descriptor.EnumValueDescriptor(\n      name=\'BICUBIC\', index=1, number=2,\n      options=None,\n      type=None),\n    _descriptor.EnumValueDescriptor(\n      name=\'BILINEAR\', index=2, number=3,\n      options=None,\n      type=None),\n    _descriptor.EnumValueDescriptor(\n      name=\'NEAREST_NEIGHBOR\', index=3, number=4,\n      options=None,\n      type=None),\n  ],\n  containing_type=None,\n  options=None,\n  serialized_start=4084,\n  serialized_end=4151,\n)\n_sym_db.RegisterEnumDescriptor(_RESIZEIMAGE_METHOD)\n\n\n_PREPROCESSINGSTEP = _descriptor.Descriptor(\n  name=\'PreprocessingStep\',\n  full_name=\'object_detection.protos.PreprocessingStep\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'normalize_image\', full_name=\'object_detection.protos.PreprocessingStep.normalize_image\', index=0,\n      number=1, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'random_horizontal_flip\', full_name=\'object_detection.protos.PreprocessingStep.random_horizontal_flip\', index=1,\n      number=2, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'random_pixel_value_scale\', full_name=\'object_detection.protos.PreprocessingStep.random_pixel_value_scale\', index=2,\n      number=3, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'random_image_scale\', full_name=\'object_detection.protos.PreprocessingStep.random_image_scale\', index=3,\n      number=4, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'random_rgb_to_gray\', full_name=\'object_detection.protos.PreprocessingStep.random_rgb_to_gray\', index=4,\n      number=5, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'random_adjust_brightness\', full_name=\'object_detection.protos.PreprocessingStep.random_adjust_brightness\', index=5,\n      number=6, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'random_adjust_contrast\', full_name=\'object_detection.protos.PreprocessingStep.random_adjust_contrast\', index=6,\n      number=7, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'random_adjust_hue\', full_name=\'object_detection.protos.PreprocessingStep.random_adjust_hue\', index=7,\n      number=8, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'random_adjust_saturation\', full_name=\'object_detection.protos.PreprocessingStep.random_adjust_saturation\', index=8,\n      number=9, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'random_distort_color\', full_name=\'object_detection.protos.PreprocessingStep.random_distort_color\', index=9,\n      number=10, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'random_jitter_boxes\', full_name=\'object_detection.protos.PreprocessingStep.random_jitter_boxes\', index=10,\n      number=11, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'random_crop_image\', full_name=\'object_detection.protos.PreprocessingStep.random_crop_image\', index=11,\n      number=12, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'random_pad_image\', full_name=\'object_detection.protos.PreprocessingStep.random_pad_image\', index=12,\n      number=13, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'random_crop_pad_image\', full_name=\'object_detection.protos.PreprocessingStep.random_crop_pad_image\', index=13,\n      number=14, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'random_crop_to_aspect_ratio\', full_name=\'object_detection.protos.PreprocessingStep.random_crop_to_aspect_ratio\', index=14,\n      number=15, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'random_black_patches\', full_name=\'object_detection.protos.PreprocessingStep.random_black_patches\', index=15,\n      number=16, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'random_resize_method\', full_name=\'object_detection.protos.PreprocessingStep.random_resize_method\', index=16,\n      number=17, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'scale_boxes_to_pixel_coordinates\', full_name=\'object_detection.protos.PreprocessingStep.scale_boxes_to_pixel_coordinates\', index=17,\n      number=18, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'resize_image\', full_name=\'object_detection.protos.PreprocessingStep.resize_image\', index=18,\n      number=19, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'subtract_channel_mean\', full_name=\'object_detection.protos.PreprocessingStep.subtract_channel_mean\', index=19,\n      number=20, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'ssd_random_crop\', full_name=\'object_detection.protos.PreprocessingStep.ssd_random_crop\', index=20,\n      number=21, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'ssd_random_crop_pad\', full_name=\'object_detection.protos.PreprocessingStep.ssd_random_crop_pad\', index=21,\n      number=22, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'ssd_random_crop_fixed_aspect_ratio\', full_name=\'object_detection.protos.PreprocessingStep.ssd_random_crop_fixed_aspect_ratio\', index=22,\n      number=23, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'ssd_random_crop_pad_fixed_aspect_ratio\', full_name=\'object_detection.protos.PreprocessingStep.ssd_random_crop_pad_fixed_aspect_ratio\', index=23,\n      number=24, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'random_vertical_flip\', full_name=\'object_detection.protos.PreprocessingStep.random_vertical_flip\', index=24,\n      number=25, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'random_rotation90\', full_name=\'object_detection.protos.PreprocessingStep.random_rotation90\', index=25,\n      number=26, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'rgb_to_gray\', full_name=\'object_detection.protos.PreprocessingStep.rgb_to_gray\', index=26,\n      number=27, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto2\',\n  extension_ranges=[],\n  oneofs=[\n    _descriptor.OneofDescriptor(\n      name=\'preprocessing_step\', full_name=\'object_detection.protos.PreprocessingStep.preprocessing_step\',\n      index=0, containing_type=None, fields=[]),\n  ],\n  serialized_start=72,\n  serialized_end=2226,\n)\n\n\n_NORMALIZEIMAGE = _descriptor.Descriptor(\n  name=\'NormalizeImage\',\n  full_name=\'object_detection.protos.NormalizeImage\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'original_minval\', full_name=\'object_detection.protos.NormalizeImage.original_minval\', index=0,\n      number=1, type=2, cpp_type=6, label=1,\n      has_default_value=False, default_value=float(0),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'original_maxval\', full_name=\'object_detection.protos.NormalizeImage.original_maxval\', index=1,\n      number=2, type=2, cpp_type=6, label=1,\n      has_default_value=False, default_value=float(0),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'target_minval\', full_name=\'object_detection.protos.NormalizeImage.target_minval\', index=2,\n      number=3, type=2, cpp_type=6, label=1,\n      has_default_value=True, default_value=float(0),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'target_maxval\', full_name=\'object_detection.protos.NormalizeImage.target_maxval\', index=3,\n      number=4, type=2, cpp_type=6, label=1,\n      has_default_value=True, default_value=float(1),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto2\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=2228,\n  serialized_end=2346,\n)\n\n\n_RANDOMHORIZONTALFLIP = _descriptor.Descriptor(\n  name=\'RandomHorizontalFlip\',\n  full_name=\'object_detection.protos.RandomHorizontalFlip\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'keypoint_flip_permutation\', full_name=\'object_detection.protos.RandomHorizontalFlip.keypoint_flip_permutation\', index=0,\n      number=1, type=5, cpp_type=1, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto2\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=2348,\n  serialized_end=2405,\n)\n\n\n_RANDOMVERTICALFLIP = _descriptor.Descriptor(\n  name=\'RandomVerticalFlip\',\n  full_name=\'object_detection.protos.RandomVerticalFlip\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'keypoint_flip_permutation\', full_name=\'object_detection.protos.RandomVerticalFlip.keypoint_flip_permutation\', index=0,\n      number=1, type=5, cpp_type=1, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto2\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=2407,\n  serialized_end=2462,\n)\n\n\n_RANDOMROTATION90 = _descriptor.Descriptor(\n  name=\'RandomRotation90\',\n  full_name=\'object_detection.protos.RandomRotation90\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto2\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=2464,\n  serialized_end=2482,\n)\n\n\n_RANDOMPIXELVALUESCALE = _descriptor.Descriptor(\n  name=\'RandomPixelValueScale\',\n  full_name=\'object_detection.protos.RandomPixelValueScale\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'minval\', full_name=\'object_detection.protos.RandomPixelValueScale.minval\', index=0,\n      number=1, type=2, cpp_type=6, label=1,\n      has_default_value=True, default_value=float(0.9),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'maxval\', full_name=\'object_detection.protos.RandomPixelValueScale.maxval\', index=1,\n      number=2, type=2, cpp_type=6, label=1,\n      has_default_value=True, default_value=float(1.1),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto2\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=2484,\n  serialized_end=2549,\n)\n\n\n_RANDOMIMAGESCALE = _descriptor.Descriptor(\n  name=\'RandomImageScale\',\n  full_name=\'object_detection.protos.RandomImageScale\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'min_scale_ratio\', full_name=\'object_detection.protos.RandomImageScale.min_scale_ratio\', index=0,\n      number=1, type=2, cpp_type=6, label=1,\n      has_default_value=True, default_value=float(0.5),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'max_scale_ratio\', full_name=\'object_detection.protos.RandomImageScale.max_scale_ratio\', index=1,\n      number=2, type=2, cpp_type=6, label=1,\n      has_default_value=True, default_value=float(2),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto2\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=2551,\n  serialized_end=2627,\n)\n\n\n_RANDOMRGBTOGRAY = _descriptor.Descriptor(\n  name=\'RandomRGBtoGray\',\n  full_name=\'object_detection.protos.RandomRGBtoGray\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'probability\', full_name=\'object_detection.protos.RandomRGBtoGray.probability\', index=0,\n      number=1, type=2, cpp_type=6, label=1,\n      has_default_value=True, default_value=float(0.1),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto2\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=2629,\n  serialized_end=2672,\n)\n\n\n_RANDOMADJUSTBRIGHTNESS = _descriptor.Descriptor(\n  name=\'RandomAdjustBrightness\',\n  full_name=\'object_detection.protos.RandomAdjustBrightness\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'max_delta\', full_name=\'object_detection.protos.RandomAdjustBrightness.max_delta\', index=0,\n      number=1, type=2, cpp_type=6, label=1,\n      has_default_value=True, default_value=float(0.2),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto2\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=2674,\n  serialized_end=2722,\n)\n\n\n_RANDOMADJUSTCONTRAST = _descriptor.Descriptor(\n  name=\'RandomAdjustContrast\',\n  full_name=\'object_detection.protos.RandomAdjustContrast\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'min_delta\', full_name=\'object_detection.protos.RandomAdjustContrast.min_delta\', index=0,\n      number=1, type=2, cpp_type=6, label=1,\n      has_default_value=True, default_value=float(0.8),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'max_delta\', full_name=\'object_detection.protos.RandomAdjustContrast.max_delta\', index=1,\n      number=2, type=2, cpp_type=6, label=1,\n      has_default_value=True, default_value=float(1.25),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto2\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=2724,\n  serialized_end=2795,\n)\n\n\n_RANDOMADJUSTHUE = _descriptor.Descriptor(\n  name=\'RandomAdjustHue\',\n  full_name=\'object_detection.protos.RandomAdjustHue\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'max_delta\', full_name=\'object_detection.protos.RandomAdjustHue.max_delta\', index=0,\n      number=1, type=2, cpp_type=6, label=1,\n      has_default_value=True, default_value=float(0.02),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto2\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=2797,\n  serialized_end=2839,\n)\n\n\n_RANDOMADJUSTSATURATION = _descriptor.Descriptor(\n  name=\'RandomAdjustSaturation\',\n  full_name=\'object_detection.protos.RandomAdjustSaturation\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'min_delta\', full_name=\'object_detection.protos.RandomAdjustSaturation.min_delta\', index=0,\n      number=1, type=2, cpp_type=6, label=1,\n      has_default_value=True, default_value=float(0.8),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'max_delta\', full_name=\'object_detection.protos.RandomAdjustSaturation.max_delta\', index=1,\n      number=2, type=2, cpp_type=6, label=1,\n      has_default_value=True, default_value=float(1.25),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto2\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=2841,\n  serialized_end=2914,\n)\n\n\n_RANDOMDISTORTCOLOR = _descriptor.Descriptor(\n  name=\'RandomDistortColor\',\n  full_name=\'object_detection.protos.RandomDistortColor\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'color_ordering\', full_name=\'object_detection.protos.RandomDistortColor.color_ordering\', index=0,\n      number=1, type=5, cpp_type=1, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto2\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=2916,\n  serialized_end=2960,\n)\n\n\n_RANDOMJITTERBOXES = _descriptor.Descriptor(\n  name=\'RandomJitterBoxes\',\n  full_name=\'object_detection.protos.RandomJitterBoxes\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'ratio\', full_name=\'object_detection.protos.RandomJitterBoxes.ratio\', index=0,\n      number=1, type=2, cpp_type=6, label=1,\n      has_default_value=True, default_value=float(0.05),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto2\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=2962,\n  serialized_end=3002,\n)\n\n\n_RANDOMCROPIMAGE = _descriptor.Descriptor(\n  name=\'RandomCropImage\',\n  full_name=\'object_detection.protos.RandomCropImage\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'min_object_covered\', full_name=\'object_detection.protos.RandomCropImage.min_object_covered\', index=0,\n      number=1, type=2, cpp_type=6, label=1,\n      has_default_value=True, default_value=float(1),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'min_aspect_ratio\', full_name=\'object_detection.protos.RandomCropImage.min_aspect_ratio\', index=1,\n      number=2, type=2, cpp_type=6, label=1,\n      has_default_value=True, default_value=float(0.75),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'max_aspect_ratio\', full_name=\'object_detection.protos.RandomCropImage.max_aspect_ratio\', index=2,\n      number=3, type=2, cpp_type=6, label=1,\n      has_default_value=True, default_value=float(1.33),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'min_area\', full_name=\'object_detection.protos.RandomCropImage.min_area\', index=3,\n      number=4, type=2, cpp_type=6, label=1,\n      has_default_value=True, default_value=float(0.1),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'max_area\', full_name=\'object_detection.protos.RandomCropImage.max_area\', index=4,\n      number=5, type=2, cpp_type=6, label=1,\n      has_default_value=True, default_value=float(1),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'overlap_thresh\', full_name=\'object_detection.protos.RandomCropImage.overlap_thresh\', index=5,\n      number=6, type=2, cpp_type=6, label=1,\n      has_default_value=True, default_value=float(0.3),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'random_coef\', full_name=\'object_detection.protos.RandomCropImage.random_coef\', index=6,\n      number=7, type=2, cpp_type=6, label=1,\n      has_default_value=True, default_value=float(0),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto2\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=3005,\n  serialized_end=3214,\n)\n\n\n_RANDOMPADIMAGE = _descriptor.Descriptor(\n  name=\'RandomPadImage\',\n  full_name=\'object_detection.protos.RandomPadImage\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'min_image_height\', full_name=\'object_detection.protos.RandomPadImage.min_image_height\', index=0,\n      number=1, type=2, cpp_type=6, label=1,\n      has_default_value=False, default_value=float(0),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'min_image_width\', full_name=\'object_detection.protos.RandomPadImage.min_image_width\', index=1,\n      number=2, type=2, cpp_type=6, label=1,\n      has_default_value=False, default_value=float(0),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'max_image_height\', full_name=\'object_detection.protos.RandomPadImage.max_image_height\', index=2,\n      number=3, type=2, cpp_type=6, label=1,\n      has_default_value=False, default_value=float(0),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'max_image_width\', full_name=\'object_detection.protos.RandomPadImage.max_image_width\', index=3,\n      number=4, type=2, cpp_type=6, label=1,\n      has_default_value=False, default_value=float(0),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'pad_color\', full_name=\'object_detection.protos.RandomPadImage.pad_color\', index=4,\n      number=5, type=2, cpp_type=6, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto2\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=3217,\n  serialized_end=3354,\n)\n\n\n_RANDOMCROPPADIMAGE = _descriptor.Descriptor(\n  name=\'RandomCropPadImage\',\n  full_name=\'object_detection.protos.RandomCropPadImage\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'min_object_covered\', full_name=\'object_detection.protos.RandomCropPadImage.min_object_covered\', index=0,\n      number=1, type=2, cpp_type=6, label=1,\n      has_default_value=True, default_value=float(1),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'min_aspect_ratio\', full_name=\'object_detection.protos.RandomCropPadImage.min_aspect_ratio\', index=1,\n      number=2, type=2, cpp_type=6, label=1,\n      has_default_value=True, default_value=float(0.75),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'max_aspect_ratio\', full_name=\'object_detection.protos.RandomCropPadImage.max_aspect_ratio\', index=2,\n      number=3, type=2, cpp_type=6, label=1,\n      has_default_value=True, default_value=float(1.33),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'min_area\', full_name=\'object_detection.protos.RandomCropPadImage.min_area\', index=3,\n      number=4, type=2, cpp_type=6, label=1,\n      has_default_value=True, default_value=float(0.1),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'max_area\', full_name=\'object_detection.protos.RandomCropPadImage.max_area\', index=4,\n      number=5, type=2, cpp_type=6, label=1,\n      has_default_value=True, default_value=float(1),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'overlap_thresh\', full_name=\'object_detection.protos.RandomCropPadImage.overlap_thresh\', index=5,\n      number=6, type=2, cpp_type=6, label=1,\n      has_default_value=True, default_value=float(0.3),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'random_coef\', full_name=\'object_detection.protos.RandomCropPadImage.random_coef\', index=6,\n      number=7, type=2, cpp_type=6, label=1,\n      has_default_value=True, default_value=float(0),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'min_padded_size_ratio\', full_name=\'object_detection.protos.RandomCropPadImage.min_padded_size_ratio\', index=7,\n      number=8, type=2, cpp_type=6, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'max_padded_size_ratio\', full_name=\'object_detection.protos.RandomCropPadImage.max_padded_size_ratio\', index=8,\n      number=9, type=2, cpp_type=6, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'pad_color\', full_name=\'object_detection.protos.RandomCropPadImage.pad_color\', index=9,\n      number=10, type=2, cpp_type=6, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto2\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=3357,\n  serialized_end=3650,\n)\n\n\n_RANDOMCROPTOASPECTRATIO = _descriptor.Descriptor(\n  name=\'RandomCropToAspectRatio\',\n  full_name=\'object_detection.protos.RandomCropToAspectRatio\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'aspect_ratio\', full_name=\'object_detection.protos.RandomCropToAspectRatio.aspect_ratio\', index=0,\n      number=1, type=2, cpp_type=6, label=1,\n      has_default_value=True, default_value=float(1),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'overlap_thresh\', full_name=\'object_detection.protos.RandomCropToAspectRatio.overlap_thresh\', index=1,\n      number=2, type=2, cpp_type=6, label=1,\n      has_default_value=True, default_value=float(0.3),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto2\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=3652,\n  serialized_end=3731,\n)\n\n\n_RANDOMBLACKPATCHES = _descriptor.Descriptor(\n  name=\'RandomBlackPatches\',\n  full_name=\'object_detection.protos.RandomBlackPatches\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'max_black_patches\', full_name=\'object_detection.protos.RandomBlackPatches.max_black_patches\', index=0,\n      number=1, type=5, cpp_type=1, label=1,\n      has_default_value=True, default_value=10,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'probability\', full_name=\'object_detection.protos.RandomBlackPatches.probability\', index=1,\n      number=2, type=2, cpp_type=6, label=1,\n      has_default_value=True, default_value=float(0.5),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'size_to_image_ratio\', full_name=\'object_detection.protos.RandomBlackPatches.size_to_image_ratio\', index=2,\n      number=3, type=2, cpp_type=6, label=1,\n      has_default_value=True, default_value=float(0.1),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto2\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=3733,\n  serialized_end=3844,\n)\n\n\n_RANDOMRESIZEMETHOD = _descriptor.Descriptor(\n  name=\'RandomResizeMethod\',\n  full_name=\'object_detection.protos.RandomResizeMethod\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'target_height\', full_name=\'object_detection.protos.RandomResizeMethod.target_height\', index=0,\n      number=1, type=2, cpp_type=6, label=1,\n      has_default_value=False, default_value=float(0),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'target_width\', full_name=\'object_detection.protos.RandomResizeMethod.target_width\', index=1,\n      number=2, type=2, cpp_type=6, label=1,\n      has_default_value=False, default_value=float(0),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto2\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=3846,\n  serialized_end=3911,\n)\n\n\n_RGBTOGRAY = _descriptor.Descriptor(\n  name=\'RGBtoGray\',\n  full_name=\'object_detection.protos.RGBtoGray\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto2\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=3913,\n  serialized_end=3924,\n)\n\n\n_SCALEBOXESTOPIXELCOORDINATES = _descriptor.Descriptor(\n  name=\'ScaleBoxesToPixelCoordinates\',\n  full_name=\'object_detection.protos.ScaleBoxesToPixelCoordinates\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto2\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=3926,\n  serialized_end=3956,\n)\n\n\n_RESIZEIMAGE = _descriptor.Descriptor(\n  name=\'ResizeImage\',\n  full_name=\'object_detection.protos.ResizeImage\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'new_height\', full_name=\'object_detection.protos.ResizeImage.new_height\', index=0,\n      number=1, type=5, cpp_type=1, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'new_width\', full_name=\'object_detection.protos.ResizeImage.new_width\', index=1,\n      number=2, type=5, cpp_type=1, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'method\', full_name=\'object_detection.protos.ResizeImage.method\', index=2,\n      number=3, type=14, cpp_type=8, label=1,\n      has_default_value=True, default_value=3,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n    _RESIZEIMAGE_METHOD,\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto2\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=3959,\n  serialized_end=4151,\n)\n\n\n_SUBTRACTCHANNELMEAN = _descriptor.Descriptor(\n  name=\'SubtractChannelMean\',\n  full_name=\'object_detection.protos.SubtractChannelMean\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'means\', full_name=\'object_detection.protos.SubtractChannelMean.means\', index=0,\n      number=1, type=2, cpp_type=6, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto2\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=4153,\n  serialized_end=4189,\n)\n\n\n_SSDRANDOMCROPOPERATION = _descriptor.Descriptor(\n  name=\'SSDRandomCropOperation\',\n  full_name=\'object_detection.protos.SSDRandomCropOperation\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'min_object_covered\', full_name=\'object_detection.protos.SSDRandomCropOperation.min_object_covered\', index=0,\n      number=1, type=2, cpp_type=6, label=1,\n      has_default_value=False, default_value=float(0),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'min_aspect_ratio\', full_name=\'object_detection.protos.SSDRandomCropOperation.min_aspect_ratio\', index=1,\n      number=2, type=2, cpp_type=6, label=1,\n      has_default_value=False, default_value=float(0),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'max_aspect_ratio\', full_name=\'object_detection.protos.SSDRandomCropOperation.max_aspect_ratio\', index=2,\n      number=3, type=2, cpp_type=6, label=1,\n      has_default_value=False, default_value=float(0),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'min_area\', full_name=\'object_detection.protos.SSDRandomCropOperation.min_area\', index=3,\n      number=4, type=2, cpp_type=6, label=1,\n      has_default_value=False, default_value=float(0),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'max_area\', full_name=\'object_detection.protos.SSDRandomCropOperation.max_area\', index=4,\n      number=5, type=2, cpp_type=6, label=1,\n      has_default_value=False, default_value=float(0),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'overlap_thresh\', full_name=\'object_detection.protos.SSDRandomCropOperation.overlap_thresh\', index=5,\n      number=6, type=2, cpp_type=6, label=1,\n      has_default_value=False, default_value=float(0),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'random_coef\', full_name=\'object_detection.protos.SSDRandomCropOperation.random_coef\', index=6,\n      number=7, type=2, cpp_type=6, label=1,\n      has_default_value=False, default_value=float(0),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto2\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=4192,\n  serialized_end=4377,\n)\n\n\n_SSDRANDOMCROP = _descriptor.Descriptor(\n  name=\'SSDRandomCrop\',\n  full_name=\'object_detection.protos.SSDRandomCrop\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'operations\', full_name=\'object_detection.protos.SSDRandomCrop.operations\', index=0,\n      number=1, type=11, cpp_type=10, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto2\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=4379,\n  serialized_end=4463,\n)\n\n\n_SSDRANDOMCROPPADOPERATION = _descriptor.Descriptor(\n  name=\'SSDRandomCropPadOperation\',\n  full_name=\'object_detection.protos.SSDRandomCropPadOperation\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'min_object_covered\', full_name=\'object_detection.protos.SSDRandomCropPadOperation.min_object_covered\', index=0,\n      number=1, type=2, cpp_type=6, label=1,\n      has_default_value=False, default_value=float(0),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'min_aspect_ratio\', full_name=\'object_detection.protos.SSDRandomCropPadOperation.min_aspect_ratio\', index=1,\n      number=2, type=2, cpp_type=6, label=1,\n      has_default_value=False, default_value=float(0),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'max_aspect_ratio\', full_name=\'object_detection.protos.SSDRandomCropPadOperation.max_aspect_ratio\', index=2,\n      number=3, type=2, cpp_type=6, label=1,\n      has_default_value=False, default_value=float(0),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'min_area\', full_name=\'object_detection.protos.SSDRandomCropPadOperation.min_area\', index=3,\n      number=4, type=2, cpp_type=6, label=1,\n      has_default_value=False, default_value=float(0),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'max_area\', full_name=\'object_detection.protos.SSDRandomCropPadOperation.max_area\', index=4,\n      number=5, type=2, cpp_type=6, label=1,\n      has_default_value=False, default_value=float(0),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'overlap_thresh\', full_name=\'object_detection.protos.SSDRandomCropPadOperation.overlap_thresh\', index=5,\n      number=6, type=2, cpp_type=6, label=1,\n      has_default_value=False, default_value=float(0),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'random_coef\', full_name=\'object_detection.protos.SSDRandomCropPadOperation.random_coef\', index=6,\n      number=7, type=2, cpp_type=6, label=1,\n      has_default_value=False, default_value=float(0),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'min_padded_size_ratio\', full_name=\'object_detection.protos.SSDRandomCropPadOperation.min_padded_size_ratio\', index=7,\n      number=8, type=2, cpp_type=6, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'max_padded_size_ratio\', full_name=\'object_detection.protos.SSDRandomCropPadOperation.max_padded_size_ratio\', index=8,\n      number=9, type=2, cpp_type=6, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'pad_color_r\', full_name=\'object_detection.protos.SSDRandomCropPadOperation.pad_color_r\', index=9,\n      number=10, type=2, cpp_type=6, label=1,\n      has_default_value=False, default_value=float(0),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'pad_color_g\', full_name=\'object_detection.protos.SSDRandomCropPadOperation.pad_color_g\', index=10,\n      number=11, type=2, cpp_type=6, label=1,\n      has_default_value=False, default_value=float(0),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'pad_color_b\', full_name=\'object_detection.protos.SSDRandomCropPadOperation.pad_color_b\', index=11,\n      number=12, type=2, cpp_type=6, label=1,\n      has_default_value=False, default_value=float(0),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto2\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=4466,\n  serialized_end=4779,\n)\n\n\n_SSDRANDOMCROPPAD = _descriptor.Descriptor(\n  name=\'SSDRandomCropPad\',\n  full_name=\'object_detection.protos.SSDRandomCropPad\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'operations\', full_name=\'object_detection.protos.SSDRandomCropPad.operations\', index=0,\n      number=1, type=11, cpp_type=10, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto2\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=4781,\n  serialized_end=4871,\n)\n\n\n_SSDRANDOMCROPFIXEDASPECTRATIOOPERATION = _descriptor.Descriptor(\n  name=\'SSDRandomCropFixedAspectRatioOperation\',\n  full_name=\'object_detection.protos.SSDRandomCropFixedAspectRatioOperation\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'min_object_covered\', full_name=\'object_detection.protos.SSDRandomCropFixedAspectRatioOperation.min_object_covered\', index=0,\n      number=1, type=2, cpp_type=6, label=1,\n      has_default_value=False, default_value=float(0),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'min_area\', full_name=\'object_detection.protos.SSDRandomCropFixedAspectRatioOperation.min_area\', index=1,\n      number=4, type=2, cpp_type=6, label=1,\n      has_default_value=False, default_value=float(0),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'max_area\', full_name=\'object_detection.protos.SSDRandomCropFixedAspectRatioOperation.max_area\', index=2,\n      number=5, type=2, cpp_type=6, label=1,\n      has_default_value=False, default_value=float(0),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'overlap_thresh\', full_name=\'object_detection.protos.SSDRandomCropFixedAspectRatioOperation.overlap_thresh\', index=3,\n      number=6, type=2, cpp_type=6, label=1,\n      has_default_value=False, default_value=float(0),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'random_coef\', full_name=\'object_detection.protos.SSDRandomCropFixedAspectRatioOperation.random_coef\', index=4,\n      number=7, type=2, cpp_type=6, label=1,\n      has_default_value=False, default_value=float(0),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto2\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=4874,\n  serialized_end=5023,\n)\n\n\n_SSDRANDOMCROPFIXEDASPECTRATIO = _descriptor.Descriptor(\n  name=\'SSDRandomCropFixedAspectRatio\',\n  full_name=\'object_detection.protos.SSDRandomCropFixedAspectRatio\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'operations\', full_name=\'object_detection.protos.SSDRandomCropFixedAspectRatio.operations\', index=0,\n      number=1, type=11, cpp_type=10, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'aspect_ratio\', full_name=\'object_detection.protos.SSDRandomCropFixedAspectRatio.aspect_ratio\', index=1,\n      number=2, type=2, cpp_type=6, label=1,\n      has_default_value=True, default_value=float(1),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto2\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=5026,\n  serialized_end=5167,\n)\n\n\n_SSDRANDOMCROPPADFIXEDASPECTRATIOOPERATION = _descriptor.Descriptor(\n  name=\'SSDRandomCropPadFixedAspectRatioOperation\',\n  full_name=\'object_detection.protos.SSDRandomCropPadFixedAspectRatioOperation\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'min_object_covered\', full_name=\'object_detection.protos.SSDRandomCropPadFixedAspectRatioOperation.min_object_covered\', index=0,\n      number=1, type=2, cpp_type=6, label=1,\n      has_default_value=False, default_value=float(0),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'min_aspect_ratio\', full_name=\'object_detection.protos.SSDRandomCropPadFixedAspectRatioOperation.min_aspect_ratio\', index=1,\n      number=2, type=2, cpp_type=6, label=1,\n      has_default_value=False, default_value=float(0),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'max_aspect_ratio\', full_name=\'object_detection.protos.SSDRandomCropPadFixedAspectRatioOperation.max_aspect_ratio\', index=2,\n      number=3, type=2, cpp_type=6, label=1,\n      has_default_value=False, default_value=float(0),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'min_area\', full_name=\'object_detection.protos.SSDRandomCropPadFixedAspectRatioOperation.min_area\', index=3,\n      number=4, type=2, cpp_type=6, label=1,\n      has_default_value=False, default_value=float(0),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'max_area\', full_name=\'object_detection.protos.SSDRandomCropPadFixedAspectRatioOperation.max_area\', index=4,\n      number=5, type=2, cpp_type=6, label=1,\n      has_default_value=False, default_value=float(0),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'overlap_thresh\', full_name=\'object_detection.protos.SSDRandomCropPadFixedAspectRatioOperation.overlap_thresh\', index=5,\n      number=6, type=2, cpp_type=6, label=1,\n      has_default_value=False, default_value=float(0),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'random_coef\', full_name=\'object_detection.protos.SSDRandomCropPadFixedAspectRatioOperation.random_coef\', index=6,\n      number=7, type=2, cpp_type=6, label=1,\n      has_default_value=False, default_value=float(0),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto2\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=5170,\n  serialized_end=5374,\n)\n\n\n_SSDRANDOMCROPPADFIXEDASPECTRATIO = _descriptor.Descriptor(\n  name=\'SSDRandomCropPadFixedAspectRatio\',\n  full_name=\'object_detection.protos.SSDRandomCropPadFixedAspectRatio\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'operations\', full_name=\'object_detection.protos.SSDRandomCropPadFixedAspectRatio.operations\', index=0,\n      number=1, type=11, cpp_type=10, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'aspect_ratio\', full_name=\'object_detection.protos.SSDRandomCropPadFixedAspectRatio.aspect_ratio\', index=1,\n      number=2, type=2, cpp_type=6, label=1,\n      has_default_value=True, default_value=float(1),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'min_padded_size_ratio\', full_name=\'object_detection.protos.SSDRandomCropPadFixedAspectRatio.min_padded_size_ratio\', index=2,\n      number=3, type=2, cpp_type=6, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'max_padded_size_ratio\', full_name=\'object_detection.protos.SSDRandomCropPadFixedAspectRatio.max_padded_size_ratio\', index=3,\n      number=4, type=2, cpp_type=6, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto2\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=5377,\n  serialized_end=5586,\n)\n\n_PREPROCESSINGSTEP.fields_by_name[\'normalize_image\'].message_type = _NORMALIZEIMAGE\n_PREPROCESSINGSTEP.fields_by_name[\'random_horizontal_flip\'].message_type = _RANDOMHORIZONTALFLIP\n_PREPROCESSINGSTEP.fields_by_name[\'random_pixel_value_scale\'].message_type = _RANDOMPIXELVALUESCALE\n_PREPROCESSINGSTEP.fields_by_name[\'random_image_scale\'].message_type = _RANDOMIMAGESCALE\n_PREPROCESSINGSTEP.fields_by_name[\'random_rgb_to_gray\'].message_type = _RANDOMRGBTOGRAY\n_PREPROCESSINGSTEP.fields_by_name[\'random_adjust_brightness\'].message_type = _RANDOMADJUSTBRIGHTNESS\n_PREPROCESSINGSTEP.fields_by_name[\'random_adjust_contrast\'].message_type = _RANDOMADJUSTCONTRAST\n_PREPROCESSINGSTEP.fields_by_name[\'random_adjust_hue\'].message_type = _RANDOMADJUSTHUE\n_PREPROCESSINGSTEP.fields_by_name[\'random_adjust_saturation\'].message_type = _RANDOMADJUSTSATURATION\n_PREPROCESSINGSTEP.fields_by_name[\'random_distort_color\'].message_type = _RANDOMDISTORTCOLOR\n_PREPROCESSINGSTEP.fields_by_name[\'random_jitter_boxes\'].message_type = _RANDOMJITTERBOXES\n_PREPROCESSINGSTEP.fields_by_name[\'random_crop_image\'].message_type = _RANDOMCROPIMAGE\n_PREPROCESSINGSTEP.fields_by_name[\'random_pad_image\'].message_type = _RANDOMPADIMAGE\n_PREPROCESSINGSTEP.fields_by_name[\'random_crop_pad_image\'].message_type = _RANDOMCROPPADIMAGE\n_PREPROCESSINGSTEP.fields_by_name[\'random_crop_to_aspect_ratio\'].message_type = _RANDOMCROPTOASPECTRATIO\n_PREPROCESSINGSTEP.fields_by_name[\'random_black_patches\'].message_type = _RANDOMBLACKPATCHES\n_PREPROCESSINGSTEP.fields_by_name[\'random_resize_method\'].message_type = _RANDOMRESIZEMETHOD\n_PREPROCESSINGSTEP.fields_by_name[\'scale_boxes_to_pixel_coordinates\'].message_type = _SCALEBOXESTOPIXELCOORDINATES\n_PREPROCESSINGSTEP.fields_by_name[\'resize_image\'].message_type = _RESIZEIMAGE\n_PREPROCESSINGSTEP.fields_by_name[\'subtract_channel_mean\'].message_type = _SUBTRACTCHANNELMEAN\n_PREPROCESSINGSTEP.fields_by_name[\'ssd_random_crop\'].message_type = _SSDRANDOMCROP\n_PREPROCESSINGSTEP.fields_by_name[\'ssd_random_crop_pad\'].message_type = _SSDRANDOMCROPPAD\n_PREPROCESSINGSTEP.fields_by_name[\'ssd_random_crop_fixed_aspect_ratio\'].message_type = _SSDRANDOMCROPFIXEDASPECTRATIO\n_PREPROCESSINGSTEP.fields_by_name[\'ssd_random_crop_pad_fixed_aspect_ratio\'].message_type = _SSDRANDOMCROPPADFIXEDASPECTRATIO\n_PREPROCESSINGSTEP.fields_by_name[\'random_vertical_flip\'].message_type = _RANDOMVERTICALFLIP\n_PREPROCESSINGSTEP.fields_by_name[\'random_rotation90\'].message_type = _RANDOMROTATION90\n_PREPROCESSINGSTEP.fields_by_name[\'rgb_to_gray\'].message_type = _RGBTOGRAY\n_PREPROCESSINGSTEP.oneofs_by_name[\'preprocessing_step\'].fields.append(\n  _PREPROCESSINGSTEP.fields_by_name[\'normalize_image\'])\n_PREPROCESSINGSTEP.fields_by_name[\'normalize_image\'].containing_oneof = _PREPROCESSINGSTEP.oneofs_by_name[\'preprocessing_step\']\n_PREPROCESSINGSTEP.oneofs_by_name[\'preprocessing_step\'].fields.append(\n  _PREPROCESSINGSTEP.fields_by_name[\'random_horizontal_flip\'])\n_PREPROCESSINGSTEP.fields_by_name[\'random_horizontal_flip\'].containing_oneof = _PREPROCESSINGSTEP.oneofs_by_name[\'preprocessing_step\']\n_PREPROCESSINGSTEP.oneofs_by_name[\'preprocessing_step\'].fields.append(\n  _PREPROCESSINGSTEP.fields_by_name[\'random_pixel_value_scale\'])\n_PREPROCESSINGSTEP.fields_by_name[\'random_pixel_value_scale\'].containing_oneof = _PREPROCESSINGSTEP.oneofs_by_name[\'preprocessing_step\']\n_PREPROCESSINGSTEP.oneofs_by_name[\'preprocessing_step\'].fields.append(\n  _PREPROCESSINGSTEP.fields_by_name[\'random_image_scale\'])\n_PREPROCESSINGSTEP.fields_by_name[\'random_image_scale\'].containing_oneof = _PREPROCESSINGSTEP.oneofs_by_name[\'preprocessing_step\']\n_PREPROCESSINGSTEP.oneofs_by_name[\'preprocessing_step\'].fields.append(\n  _PREPROCESSINGSTEP.fields_by_name[\'random_rgb_to_gray\'])\n_PREPROCESSINGSTEP.fields_by_name[\'random_rgb_to_gray\'].containing_oneof = _PREPROCESSINGSTEP.oneofs_by_name[\'preprocessing_step\']\n_PREPROCESSINGSTEP.oneofs_by_name[\'preprocessing_step\'].fields.append(\n  _PREPROCESSINGSTEP.fields_by_name[\'random_adjust_brightness\'])\n_PREPROCESSINGSTEP.fields_by_name[\'random_adjust_brightness\'].containing_oneof = _PREPROCESSINGSTEP.oneofs_by_name[\'preprocessing_step\']\n_PREPROCESSINGSTEP.oneofs_by_name[\'preprocessing_step\'].fields.append(\n  _PREPROCESSINGSTEP.fields_by_name[\'random_adjust_contrast\'])\n_PREPROCESSINGSTEP.fields_by_name[\'random_adjust_contrast\'].containing_oneof = _PREPROCESSINGSTEP.oneofs_by_name[\'preprocessing_step\']\n_PREPROCESSINGSTEP.oneofs_by_name[\'preprocessing_step\'].fields.append(\n  _PREPROCESSINGSTEP.fields_by_name[\'random_adjust_hue\'])\n_PREPROCESSINGSTEP.fields_by_name[\'random_adjust_hue\'].containing_oneof = _PREPROCESSINGSTEP.oneofs_by_name[\'preprocessing_step\']\n_PREPROCESSINGSTEP.oneofs_by_name[\'preprocessing_step\'].fields.append(\n  _PREPROCESSINGSTEP.fields_by_name[\'random_adjust_saturation\'])\n_PREPROCESSINGSTEP.fields_by_name[\'random_adjust_saturation\'].containing_oneof = _PREPROCESSINGSTEP.oneofs_by_name[\'preprocessing_step\']\n_PREPROCESSINGSTEP.oneofs_by_name[\'preprocessing_step\'].fields.append(\n  _PREPROCESSINGSTEP.fields_by_name[\'random_distort_color\'])\n_PREPROCESSINGSTEP.fields_by_name[\'random_distort_color\'].containing_oneof = _PREPROCESSINGSTEP.oneofs_by_name[\'preprocessing_step\']\n_PREPROCESSINGSTEP.oneofs_by_name[\'preprocessing_step\'].fields.append(\n  _PREPROCESSINGSTEP.fields_by_name[\'random_jitter_boxes\'])\n_PREPROCESSINGSTEP.fields_by_name[\'random_jitter_boxes\'].containing_oneof = _PREPROCESSINGSTEP.oneofs_by_name[\'preprocessing_step\']\n_PREPROCESSINGSTEP.oneofs_by_name[\'preprocessing_step\'].fields.append(\n  _PREPROCESSINGSTEP.fields_by_name[\'random_crop_image\'])\n_PREPROCESSINGSTEP.fields_by_name[\'random_crop_image\'].containing_oneof = _PREPROCESSINGSTEP.oneofs_by_name[\'preprocessing_step\']\n_PREPROCESSINGSTEP.oneofs_by_name[\'preprocessing_step\'].fields.append(\n  _PREPROCESSINGSTEP.fields_by_name[\'random_pad_image\'])\n_PREPROCESSINGSTEP.fields_by_name[\'random_pad_image\'].containing_oneof = _PREPROCESSINGSTEP.oneofs_by_name[\'preprocessing_step\']\n_PREPROCESSINGSTEP.oneofs_by_name[\'preprocessing_step\'].fields.append(\n  _PREPROCESSINGSTEP.fields_by_name[\'random_crop_pad_image\'])\n_PREPROCESSINGSTEP.fields_by_name[\'random_crop_pad_image\'].containing_oneof = _PREPROCESSINGSTEP.oneofs_by_name[\'preprocessing_step\']\n_PREPROCESSINGSTEP.oneofs_by_name[\'preprocessing_step\'].fields.append(\n  _PREPROCESSINGSTEP.fields_by_name[\'random_crop_to_aspect_ratio\'])\n_PREPROCESSINGSTEP.fields_by_name[\'random_crop_to_aspect_ratio\'].containing_oneof = _PREPROCESSINGSTEP.oneofs_by_name[\'preprocessing_step\']\n_PREPROCESSINGSTEP.oneofs_by_name[\'preprocessing_step\'].fields.append(\n  _PREPROCESSINGSTEP.fields_by_name[\'random_black_patches\'])\n_PREPROCESSINGSTEP.fields_by_name[\'random_black_patches\'].containing_oneof = _PREPROCESSINGSTEP.oneofs_by_name[\'preprocessing_step\']\n_PREPROCESSINGSTEP.oneofs_by_name[\'preprocessing_step\'].fields.append(\n  _PREPROCESSINGSTEP.fields_by_name[\'random_resize_method\'])\n_PREPROCESSINGSTEP.fields_by_name[\'random_resize_method\'].containing_oneof = _PREPROCESSINGSTEP.oneofs_by_name[\'preprocessing_step\']\n_PREPROCESSINGSTEP.oneofs_by_name[\'preprocessing_step\'].fields.append(\n  _PREPROCESSINGSTEP.fields_by_name[\'scale_boxes_to_pixel_coordinates\'])\n_PREPROCESSINGSTEP.fields_by_name[\'scale_boxes_to_pixel_coordinates\'].containing_oneof = _PREPROCESSINGSTEP.oneofs_by_name[\'preprocessing_step\']\n_PREPROCESSINGSTEP.oneofs_by_name[\'preprocessing_step\'].fields.append(\n  _PREPROCESSINGSTEP.fields_by_name[\'resize_image\'])\n_PREPROCESSINGSTEP.fields_by_name[\'resize_image\'].containing_oneof = _PREPROCESSINGSTEP.oneofs_by_name[\'preprocessing_step\']\n_PREPROCESSINGSTEP.oneofs_by_name[\'preprocessing_step\'].fields.append(\n  _PREPROCESSINGSTEP.fields_by_name[\'subtract_channel_mean\'])\n_PREPROCESSINGSTEP.fields_by_name[\'subtract_channel_mean\'].containing_oneof = _PREPROCESSINGSTEP.oneofs_by_name[\'preprocessing_step\']\n_PREPROCESSINGSTEP.oneofs_by_name[\'preprocessing_step\'].fields.append(\n  _PREPROCESSINGSTEP.fields_by_name[\'ssd_random_crop\'])\n_PREPROCESSINGSTEP.fields_by_name[\'ssd_random_crop\'].containing_oneof = _PREPROCESSINGSTEP.oneofs_by_name[\'preprocessing_step\']\n_PREPROCESSINGSTEP.oneofs_by_name[\'preprocessing_step\'].fields.append(\n  _PREPROCESSINGSTEP.fields_by_name[\'ssd_random_crop_pad\'])\n_PREPROCESSINGSTEP.fields_by_name[\'ssd_random_crop_pad\'].containing_oneof = _PREPROCESSINGSTEP.oneofs_by_name[\'preprocessing_step\']\n_PREPROCESSINGSTEP.oneofs_by_name[\'preprocessing_step\'].fields.append(\n  _PREPROCESSINGSTEP.fields_by_name[\'ssd_random_crop_fixed_aspect_ratio\'])\n_PREPROCESSINGSTEP.fields_by_name[\'ssd_random_crop_fixed_aspect_ratio\'].containing_oneof = _PREPROCESSINGSTEP.oneofs_by_name[\'preprocessing_step\']\n_PREPROCESSINGSTEP.oneofs_by_name[\'preprocessing_step\'].fields.append(\n  _PREPROCESSINGSTEP.fields_by_name[\'ssd_random_crop_pad_fixed_aspect_ratio\'])\n_PREPROCESSINGSTEP.fields_by_name[\'ssd_random_crop_pad_fixed_aspect_ratio\'].containing_oneof = _PREPROCESSINGSTEP.oneofs_by_name[\'preprocessing_step\']\n_PREPROCESSINGSTEP.oneofs_by_name[\'preprocessing_step\'].fields.append(\n  _PREPROCESSINGSTEP.fields_by_name[\'random_vertical_flip\'])\n_PREPROCESSINGSTEP.fields_by_name[\'random_vertical_flip\'].containing_oneof = _PREPROCESSINGSTEP.oneofs_by_name[\'preprocessing_step\']\n_PREPROCESSINGSTEP.oneofs_by_name[\'preprocessing_step\'].fields.append(\n  _PREPROCESSINGSTEP.fields_by_name[\'random_rotation90\'])\n_PREPROCESSINGSTEP.fields_by_name[\'random_rotation90\'].containing_oneof = _PREPROCESSINGSTEP.oneofs_by_name[\'preprocessing_step\']\n_PREPROCESSINGSTEP.oneofs_by_name[\'preprocessing_step\'].fields.append(\n  _PREPROCESSINGSTEP.fields_by_name[\'rgb_to_gray\'])\n_PREPROCESSINGSTEP.fields_by_name[\'rgb_to_gray\'].containing_oneof = _PREPROCESSINGSTEP.oneofs_by_name[\'preprocessing_step\']\n_RESIZEIMAGE.fields_by_name[\'method\'].enum_type = _RESIZEIMAGE_METHOD\n_RESIZEIMAGE_METHOD.containing_type = _RESIZEIMAGE\n_SSDRANDOMCROP.fields_by_name[\'operations\'].message_type = _SSDRANDOMCROPOPERATION\n_SSDRANDOMCROPPAD.fields_by_name[\'operations\'].message_type = _SSDRANDOMCROPPADOPERATION\n_SSDRANDOMCROPFIXEDASPECTRATIO.fields_by_name[\'operations\'].message_type = _SSDRANDOMCROPFIXEDASPECTRATIOOPERATION\n_SSDRANDOMCROPPADFIXEDASPECTRATIO.fields_by_name[\'operations\'].message_type = _SSDRANDOMCROPPADFIXEDASPECTRATIOOPERATION\nDESCRIPTOR.message_types_by_name[\'PreprocessingStep\'] = _PREPROCESSINGSTEP\nDESCRIPTOR.message_types_by_name[\'NormalizeImage\'] = _NORMALIZEIMAGE\nDESCRIPTOR.message_types_by_name[\'RandomHorizontalFlip\'] = _RANDOMHORIZONTALFLIP\nDESCRIPTOR.message_types_by_name[\'RandomVerticalFlip\'] = _RANDOMVERTICALFLIP\nDESCRIPTOR.message_types_by_name[\'RandomRotation90\'] = _RANDOMROTATION90\nDESCRIPTOR.message_types_by_name[\'RandomPixelValueScale\'] = _RANDOMPIXELVALUESCALE\nDESCRIPTOR.message_types_by_name[\'RandomImageScale\'] = _RANDOMIMAGESCALE\nDESCRIPTOR.message_types_by_name[\'RandomRGBtoGray\'] = _RANDOMRGBTOGRAY\nDESCRIPTOR.message_types_by_name[\'RandomAdjustBrightness\'] = _RANDOMADJUSTBRIGHTNESS\nDESCRIPTOR.message_types_by_name[\'RandomAdjustContrast\'] = _RANDOMADJUSTCONTRAST\nDESCRIPTOR.message_types_by_name[\'RandomAdjustHue\'] = _RANDOMADJUSTHUE\nDESCRIPTOR.message_types_by_name[\'RandomAdjustSaturation\'] = _RANDOMADJUSTSATURATION\nDESCRIPTOR.message_types_by_name[\'RandomDistortColor\'] = _RANDOMDISTORTCOLOR\nDESCRIPTOR.message_types_by_name[\'RandomJitterBoxes\'] = _RANDOMJITTERBOXES\nDESCRIPTOR.message_types_by_name[\'RandomCropImage\'] = _RANDOMCROPIMAGE\nDESCRIPTOR.message_types_by_name[\'RandomPadImage\'] = _RANDOMPADIMAGE\nDESCRIPTOR.message_types_by_name[\'RandomCropPadImage\'] = _RANDOMCROPPADIMAGE\nDESCRIPTOR.message_types_by_name[\'RandomCropToAspectRatio\'] = _RANDOMCROPTOASPECTRATIO\nDESCRIPTOR.message_types_by_name[\'RandomBlackPatches\'] = _RANDOMBLACKPATCHES\nDESCRIPTOR.message_types_by_name[\'RandomResizeMethod\'] = _RANDOMRESIZEMETHOD\nDESCRIPTOR.message_types_by_name[\'RGBtoGray\'] = _RGBTOGRAY\nDESCRIPTOR.message_types_by_name[\'ScaleBoxesToPixelCoordinates\'] = _SCALEBOXESTOPIXELCOORDINATES\nDESCRIPTOR.message_types_by_name[\'ResizeImage\'] = _RESIZEIMAGE\nDESCRIPTOR.message_types_by_name[\'SubtractChannelMean\'] = _SUBTRACTCHANNELMEAN\nDESCRIPTOR.message_types_by_name[\'SSDRandomCropOperation\'] = _SSDRANDOMCROPOPERATION\nDESCRIPTOR.message_types_by_name[\'SSDRandomCrop\'] = _SSDRANDOMCROP\nDESCRIPTOR.message_types_by_name[\'SSDRandomCropPadOperation\'] = _SSDRANDOMCROPPADOPERATION\nDESCRIPTOR.message_types_by_name[\'SSDRandomCropPad\'] = _SSDRANDOMCROPPAD\nDESCRIPTOR.message_types_by_name[\'SSDRandomCropFixedAspectRatioOperation\'] = _SSDRANDOMCROPFIXEDASPECTRATIOOPERATION\nDESCRIPTOR.message_types_by_name[\'SSDRandomCropFixedAspectRatio\'] = _SSDRANDOMCROPFIXEDASPECTRATIO\nDESCRIPTOR.message_types_by_name[\'SSDRandomCropPadFixedAspectRatioOperation\'] = _SSDRANDOMCROPPADFIXEDASPECTRATIOOPERATION\nDESCRIPTOR.message_types_by_name[\'SSDRandomCropPadFixedAspectRatio\'] = _SSDRANDOMCROPPADFIXEDASPECTRATIO\n_sym_db.RegisterFileDescriptor(DESCRIPTOR)\n\nPreprocessingStep = _reflection.GeneratedProtocolMessageType(\'PreprocessingStep\', (_message.Message,), dict(\n  DESCRIPTOR = _PREPROCESSINGSTEP,\n  __module__ = \'object_detection.protos.preprocessor_pb2\'\n  # @@protoc_insertion_point(class_scope:object_detection.protos.PreprocessingStep)\n  ))\n_sym_db.RegisterMessage(PreprocessingStep)\n\nNormalizeImage = _reflection.GeneratedProtocolMessageType(\'NormalizeImage\', (_message.Message,), dict(\n  DESCRIPTOR = _NORMALIZEIMAGE,\n  __module__ = \'object_detection.protos.preprocessor_pb2\'\n  # @@protoc_insertion_point(class_scope:object_detection.protos.NormalizeImage)\n  ))\n_sym_db.RegisterMessage(NormalizeImage)\n\nRandomHorizontalFlip = _reflection.GeneratedProtocolMessageType(\'RandomHorizontalFlip\', (_message.Message,), dict(\n  DESCRIPTOR = _RANDOMHORIZONTALFLIP,\n  __module__ = \'object_detection.protos.preprocessor_pb2\'\n  # @@protoc_insertion_point(class_scope:object_detection.protos.RandomHorizontalFlip)\n  ))\n_sym_db.RegisterMessage(RandomHorizontalFlip)\n\nRandomVerticalFlip = _reflection.GeneratedProtocolMessageType(\'RandomVerticalFlip\', (_message.Message,), dict(\n  DESCRIPTOR = _RANDOMVERTICALFLIP,\n  __module__ = \'object_detection.protos.preprocessor_pb2\'\n  # @@protoc_insertion_point(class_scope:object_detection.protos.RandomVerticalFlip)\n  ))\n_sym_db.RegisterMessage(RandomVerticalFlip)\n\nRandomRotation90 = _reflection.GeneratedProtocolMessageType(\'RandomRotation90\', (_message.Message,), dict(\n  DESCRIPTOR = _RANDOMROTATION90,\n  __module__ = \'object_detection.protos.preprocessor_pb2\'\n  # @@protoc_insertion_point(class_scope:object_detection.protos.RandomRotation90)\n  ))\n_sym_db.RegisterMessage(RandomRotation90)\n\nRandomPixelValueScale = _reflection.GeneratedProtocolMessageType(\'RandomPixelValueScale\', (_message.Message,), dict(\n  DESCRIPTOR = _RANDOMPIXELVALUESCALE,\n  __module__ = \'object_detection.protos.preprocessor_pb2\'\n  # @@protoc_insertion_point(class_scope:object_detection.protos.RandomPixelValueScale)\n  ))\n_sym_db.RegisterMessage(RandomPixelValueScale)\n\nRandomImageScale = _reflection.GeneratedProtocolMessageType(\'RandomImageScale\', (_message.Message,), dict(\n  DESCRIPTOR = _RANDOMIMAGESCALE,\n  __module__ = \'object_detection.protos.preprocessor_pb2\'\n  # @@protoc_insertion_point(class_scope:object_detection.protos.RandomImageScale)\n  ))\n_sym_db.RegisterMessage(RandomImageScale)\n\nRandomRGBtoGray = _reflection.GeneratedProtocolMessageType(\'RandomRGBtoGray\', (_message.Message,), dict(\n  DESCRIPTOR = _RANDOMRGBTOGRAY,\n  __module__ = \'object_detection.protos.preprocessor_pb2\'\n  # @@protoc_insertion_point(class_scope:object_detection.protos.RandomRGBtoGray)\n  ))\n_sym_db.RegisterMessage(RandomRGBtoGray)\n\nRandomAdjustBrightness = _reflection.GeneratedProtocolMessageType(\'RandomAdjustBrightness\', (_message.Message,), dict(\n  DESCRIPTOR = _RANDOMADJUSTBRIGHTNESS,\n  __module__ = \'object_detection.protos.preprocessor_pb2\'\n  # @@protoc_insertion_point(class_scope:object_detection.protos.RandomAdjustBrightness)\n  ))\n_sym_db.RegisterMessage(RandomAdjustBrightness)\n\nRandomAdjustContrast = _reflection.GeneratedProtocolMessageType(\'RandomAdjustContrast\', (_message.Message,), dict(\n  DESCRIPTOR = _RANDOMADJUSTCONTRAST,\n  __module__ = \'object_detection.protos.preprocessor_pb2\'\n  # @@protoc_insertion_point(class_scope:object_detection.protos.RandomAdjustContrast)\n  ))\n_sym_db.RegisterMessage(RandomAdjustContrast)\n\nRandomAdjustHue = _reflection.GeneratedProtocolMessageType(\'RandomAdjustHue\', (_message.Message,), dict(\n  DESCRIPTOR = _RANDOMADJUSTHUE,\n  __module__ = \'object_detection.protos.preprocessor_pb2\'\n  # @@protoc_insertion_point(class_scope:object_detection.protos.RandomAdjustHue)\n  ))\n_sym_db.RegisterMessage(RandomAdjustHue)\n\nRandomAdjustSaturation = _reflection.GeneratedProtocolMessageType(\'RandomAdjustSaturation\', (_message.Message,), dict(\n  DESCRIPTOR = _RANDOMADJUSTSATURATION,\n  __module__ = \'object_detection.protos.preprocessor_pb2\'\n  # @@protoc_insertion_point(class_scope:object_detection.protos.RandomAdjustSaturation)\n  ))\n_sym_db.RegisterMessage(RandomAdjustSaturation)\n\nRandomDistortColor = _reflection.GeneratedProtocolMessageType(\'RandomDistortColor\', (_message.Message,), dict(\n  DESCRIPTOR = _RANDOMDISTORTCOLOR,\n  __module__ = \'object_detection.protos.preprocessor_pb2\'\n  # @@protoc_insertion_point(class_scope:object_detection.protos.RandomDistortColor)\n  ))\n_sym_db.RegisterMessage(RandomDistortColor)\n\nRandomJitterBoxes = _reflection.GeneratedProtocolMessageType(\'RandomJitterBoxes\', (_message.Message,), dict(\n  DESCRIPTOR = _RANDOMJITTERBOXES,\n  __module__ = \'object_detection.protos.preprocessor_pb2\'\n  # @@protoc_insertion_point(class_scope:object_detection.protos.RandomJitterBoxes)\n  ))\n_sym_db.RegisterMessage(RandomJitterBoxes)\n\nRandomCropImage = _reflection.GeneratedProtocolMessageType(\'RandomCropImage\', (_message.Message,), dict(\n  DESCRIPTOR = _RANDOMCROPIMAGE,\n  __module__ = \'object_detection.protos.preprocessor_pb2\'\n  # @@protoc_insertion_point(class_scope:object_detection.protos.RandomCropImage)\n  ))\n_sym_db.RegisterMessage(RandomCropImage)\n\nRandomPadImage = _reflection.GeneratedProtocolMessageType(\'RandomPadImage\', (_message.Message,), dict(\n  DESCRIPTOR = _RANDOMPADIMAGE,\n  __module__ = \'object_detection.protos.preprocessor_pb2\'\n  # @@protoc_insertion_point(class_scope:object_detection.protos.RandomPadImage)\n  ))\n_sym_db.RegisterMessage(RandomPadImage)\n\nRandomCropPadImage = _reflection.GeneratedProtocolMessageType(\'RandomCropPadImage\', (_message.Message,), dict(\n  DESCRIPTOR = _RANDOMCROPPADIMAGE,\n  __module__ = \'object_detection.protos.preprocessor_pb2\'\n  # @@protoc_insertion_point(class_scope:object_detection.protos.RandomCropPadImage)\n  ))\n_sym_db.RegisterMessage(RandomCropPadImage)\n\nRandomCropToAspectRatio = _reflection.GeneratedProtocolMessageType(\'RandomCropToAspectRatio\', (_message.Message,), dict(\n  DESCRIPTOR = _RANDOMCROPTOASPECTRATIO,\n  __module__ = \'object_detection.protos.preprocessor_pb2\'\n  # @@protoc_insertion_point(class_scope:object_detection.protos.RandomCropToAspectRatio)\n  ))\n_sym_db.RegisterMessage(RandomCropToAspectRatio)\n\nRandomBlackPatches = _reflection.GeneratedProtocolMessageType(\'RandomBlackPatches\', (_message.Message,), dict(\n  DESCRIPTOR = _RANDOMBLACKPATCHES,\n  __module__ = \'object_detection.protos.preprocessor_pb2\'\n  # @@protoc_insertion_point(class_scope:object_detection.protos.RandomBlackPatches)\n  ))\n_sym_db.RegisterMessage(RandomBlackPatches)\n\nRandomResizeMethod = _reflection.GeneratedProtocolMessageType(\'RandomResizeMethod\', (_message.Message,), dict(\n  DESCRIPTOR = _RANDOMRESIZEMETHOD,\n  __module__ = \'object_detection.protos.preprocessor_pb2\'\n  # @@protoc_insertion_point(class_scope:object_detection.protos.RandomResizeMethod)\n  ))\n_sym_db.RegisterMessage(RandomResizeMethod)\n\nRGBtoGray = _reflection.GeneratedProtocolMessageType(\'RGBtoGray\', (_message.Message,), dict(\n  DESCRIPTOR = _RGBTOGRAY,\n  __module__ = \'object_detection.protos.preprocessor_pb2\'\n  # @@protoc_insertion_point(class_scope:object_detection.protos.RGBtoGray)\n  ))\n_sym_db.RegisterMessage(RGBtoGray)\n\nScaleBoxesToPixelCoordinates = _reflection.GeneratedProtocolMessageType(\'ScaleBoxesToPixelCoordinates\', (_message.Message,), dict(\n  DESCRIPTOR = _SCALEBOXESTOPIXELCOORDINATES,\n  __module__ = \'object_detection.protos.preprocessor_pb2\'\n  # @@protoc_insertion_point(class_scope:object_detection.protos.ScaleBoxesToPixelCoordinates)\n  ))\n_sym_db.RegisterMessage(ScaleBoxesToPixelCoordinates)\n\nResizeImage = _reflection.GeneratedProtocolMessageType(\'ResizeImage\', (_message.Message,), dict(\n  DESCRIPTOR = _RESIZEIMAGE,\n  __module__ = \'object_detection.protos.preprocessor_pb2\'\n  # @@protoc_insertion_point(class_scope:object_detection.protos.ResizeImage)\n  ))\n_sym_db.RegisterMessage(ResizeImage)\n\nSubtractChannelMean = _reflection.GeneratedProtocolMessageType(\'SubtractChannelMean\', (_message.Message,), dict(\n  DESCRIPTOR = _SUBTRACTCHANNELMEAN,\n  __module__ = \'object_detection.protos.preprocessor_pb2\'\n  # @@protoc_insertion_point(class_scope:object_detection.protos.SubtractChannelMean)\n  ))\n_sym_db.RegisterMessage(SubtractChannelMean)\n\nSSDRandomCropOperation = _reflection.GeneratedProtocolMessageType(\'SSDRandomCropOperation\', (_message.Message,), dict(\n  DESCRIPTOR = _SSDRANDOMCROPOPERATION,\n  __module__ = \'object_detection.protos.preprocessor_pb2\'\n  # @@protoc_insertion_point(class_scope:object_detection.protos.SSDRandomCropOperation)\n  ))\n_sym_db.RegisterMessage(SSDRandomCropOperation)\n\nSSDRandomCrop = _reflection.GeneratedProtocolMessageType(\'SSDRandomCrop\', (_message.Message,), dict(\n  DESCRIPTOR = _SSDRANDOMCROP,\n  __module__ = \'object_detection.protos.preprocessor_pb2\'\n  # @@protoc_insertion_point(class_scope:object_detection.protos.SSDRandomCrop)\n  ))\n_sym_db.RegisterMessage(SSDRandomCrop)\n\nSSDRandomCropPadOperation = _reflection.GeneratedProtocolMessageType(\'SSDRandomCropPadOperation\', (_message.Message,), dict(\n  DESCRIPTOR = _SSDRANDOMCROPPADOPERATION,\n  __module__ = \'object_detection.protos.preprocessor_pb2\'\n  # @@protoc_insertion_point(class_scope:object_detection.protos.SSDRandomCropPadOperation)\n  ))\n_sym_db.RegisterMessage(SSDRandomCropPadOperation)\n\nSSDRandomCropPad = _reflection.GeneratedProtocolMessageType(\'SSDRandomCropPad\', (_message.Message,), dict(\n  DESCRIPTOR = _SSDRANDOMCROPPAD,\n  __module__ = \'object_detection.protos.preprocessor_pb2\'\n  # @@protoc_insertion_point(class_scope:object_detection.protos.SSDRandomCropPad)\n  ))\n_sym_db.RegisterMessage(SSDRandomCropPad)\n\nSSDRandomCropFixedAspectRatioOperation = _reflection.GeneratedProtocolMessageType(\'SSDRandomCropFixedAspectRatioOperation\', (_message.Message,), dict(\n  DESCRIPTOR = _SSDRANDOMCROPFIXEDASPECTRATIOOPERATION,\n  __module__ = \'object_detection.protos.preprocessor_pb2\'\n  # @@protoc_insertion_point(class_scope:object_detection.protos.SSDRandomCropFixedAspectRatioOperation)\n  ))\n_sym_db.RegisterMessage(SSDRandomCropFixedAspectRatioOperation)\n\nSSDRandomCropFixedAspectRatio = _reflection.GeneratedProtocolMessageType(\'SSDRandomCropFixedAspectRatio\', (_message.Message,), dict(\n  DESCRIPTOR = _SSDRANDOMCROPFIXEDASPECTRATIO,\n  __module__ = \'object_detection.protos.preprocessor_pb2\'\n  # @@protoc_insertion_point(class_scope:object_detection.protos.SSDRandomCropFixedAspectRatio)\n  ))\n_sym_db.RegisterMessage(SSDRandomCropFixedAspectRatio)\n\nSSDRandomCropPadFixedAspectRatioOperation = _reflection.GeneratedProtocolMessageType(\'SSDRandomCropPadFixedAspectRatioOperation\', (_message.Message,), dict(\n  DESCRIPTOR = _SSDRANDOMCROPPADFIXEDASPECTRATIOOPERATION,\n  __module__ = \'object_detection.protos.preprocessor_pb2\'\n  # @@protoc_insertion_point(class_scope:object_detection.protos.SSDRandomCropPadFixedAspectRatioOperation)\n  ))\n_sym_db.RegisterMessage(SSDRandomCropPadFixedAspectRatioOperation)\n\nSSDRandomCropPadFixedAspectRatio = _reflection.GeneratedProtocolMessageType(\'SSDRandomCropPadFixedAspectRatio\', (_message.Message,), dict(\n  DESCRIPTOR = _SSDRANDOMCROPPADFIXEDASPECTRATIO,\n  __module__ = \'object_detection.protos.preprocessor_pb2\'\n  # @@protoc_insertion_point(class_scope:object_detection.protos.SSDRandomCropPadFixedAspectRatio)\n  ))\n_sym_db.RegisterMessage(SSDRandomCropPadFixedAspectRatio)\n\n\n# @@protoc_insertion_point(module_scope)\n'"
src/object_detection/protos/region_similarity_calculator_pb2.py,0,"b'# Generated by the protocol buffer compiler.  DO NOT EDIT!\n# source: object_detection/protos/region_similarity_calculator.proto\n\nimport sys\n_b=sys.version_info[0]<3 and (lambda x:x) or (lambda x:x.encode(\'latin1\'))\nfrom google.protobuf import descriptor as _descriptor\nfrom google.protobuf import message as _message\nfrom google.protobuf import reflection as _reflection\nfrom google.protobuf import symbol_database as _symbol_database\nfrom google.protobuf import descriptor_pb2\n# @@protoc_insertion_point(imports)\n\n_sym_db = _symbol_database.Default()\n\n\n\n\nDESCRIPTOR = _descriptor.FileDescriptor(\n  name=\'object_detection/protos/region_similarity_calculator.proto\',\n  package=\'object_detection.protos\',\n  syntax=\'proto2\',\n  serialized_pb=_b(\'\\n:object_detection/protos/region_similarity_calculator.proto\\x12\\x17object_detection.protos\\""\\x85\\x02\\n\\x1aRegionSimilarityCalculator\\x12N\\n\\x16neg_sq_dist_similarity\\x18\\x01 \\x01(\\x0b\\x32,.object_detection.protos.NegSqDistSimilarityH\\x00\\x12@\\n\\x0eiou_similarity\\x18\\x02 \\x01(\\x0b\\x32&.object_detection.protos.IouSimilarityH\\x00\\x12@\\n\\x0eioa_similarity\\x18\\x03 \\x01(\\x0b\\x32&.object_detection.protos.IoaSimilarityH\\x00\\x42\\x13\\n\\x11region_similarity\\""\\x15\\n\\x13NegSqDistSimilarity\\""\\x0f\\n\\rIouSimilarity\\""\\x0f\\n\\rIoaSimilarity\')\n)\n\n\n\n\n_REGIONSIMILARITYCALCULATOR = _descriptor.Descriptor(\n  name=\'RegionSimilarityCalculator\',\n  full_name=\'object_detection.protos.RegionSimilarityCalculator\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'neg_sq_dist_similarity\', full_name=\'object_detection.protos.RegionSimilarityCalculator.neg_sq_dist_similarity\', index=0,\n      number=1, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'iou_similarity\', full_name=\'object_detection.protos.RegionSimilarityCalculator.iou_similarity\', index=1,\n      number=2, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'ioa_similarity\', full_name=\'object_detection.protos.RegionSimilarityCalculator.ioa_similarity\', index=2,\n      number=3, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto2\',\n  extension_ranges=[],\n  oneofs=[\n    _descriptor.OneofDescriptor(\n      name=\'region_similarity\', full_name=\'object_detection.protos.RegionSimilarityCalculator.region_similarity\',\n      index=0, containing_type=None, fields=[]),\n  ],\n  serialized_start=88,\n  serialized_end=349,\n)\n\n\n_NEGSQDISTSIMILARITY = _descriptor.Descriptor(\n  name=\'NegSqDistSimilarity\',\n  full_name=\'object_detection.protos.NegSqDistSimilarity\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto2\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=351,\n  serialized_end=372,\n)\n\n\n_IOUSIMILARITY = _descriptor.Descriptor(\n  name=\'IouSimilarity\',\n  full_name=\'object_detection.protos.IouSimilarity\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto2\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=374,\n  serialized_end=389,\n)\n\n\n_IOASIMILARITY = _descriptor.Descriptor(\n  name=\'IoaSimilarity\',\n  full_name=\'object_detection.protos.IoaSimilarity\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto2\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=391,\n  serialized_end=406,\n)\n\n_REGIONSIMILARITYCALCULATOR.fields_by_name[\'neg_sq_dist_similarity\'].message_type = _NEGSQDISTSIMILARITY\n_REGIONSIMILARITYCALCULATOR.fields_by_name[\'iou_similarity\'].message_type = _IOUSIMILARITY\n_REGIONSIMILARITYCALCULATOR.fields_by_name[\'ioa_similarity\'].message_type = _IOASIMILARITY\n_REGIONSIMILARITYCALCULATOR.oneofs_by_name[\'region_similarity\'].fields.append(\n  _REGIONSIMILARITYCALCULATOR.fields_by_name[\'neg_sq_dist_similarity\'])\n_REGIONSIMILARITYCALCULATOR.fields_by_name[\'neg_sq_dist_similarity\'].containing_oneof = _REGIONSIMILARITYCALCULATOR.oneofs_by_name[\'region_similarity\']\n_REGIONSIMILARITYCALCULATOR.oneofs_by_name[\'region_similarity\'].fields.append(\n  _REGIONSIMILARITYCALCULATOR.fields_by_name[\'iou_similarity\'])\n_REGIONSIMILARITYCALCULATOR.fields_by_name[\'iou_similarity\'].containing_oneof = _REGIONSIMILARITYCALCULATOR.oneofs_by_name[\'region_similarity\']\n_REGIONSIMILARITYCALCULATOR.oneofs_by_name[\'region_similarity\'].fields.append(\n  _REGIONSIMILARITYCALCULATOR.fields_by_name[\'ioa_similarity\'])\n_REGIONSIMILARITYCALCULATOR.fields_by_name[\'ioa_similarity\'].containing_oneof = _REGIONSIMILARITYCALCULATOR.oneofs_by_name[\'region_similarity\']\nDESCRIPTOR.message_types_by_name[\'RegionSimilarityCalculator\'] = _REGIONSIMILARITYCALCULATOR\nDESCRIPTOR.message_types_by_name[\'NegSqDistSimilarity\'] = _NEGSQDISTSIMILARITY\nDESCRIPTOR.message_types_by_name[\'IouSimilarity\'] = _IOUSIMILARITY\nDESCRIPTOR.message_types_by_name[\'IoaSimilarity\'] = _IOASIMILARITY\n_sym_db.RegisterFileDescriptor(DESCRIPTOR)\n\nRegionSimilarityCalculator = _reflection.GeneratedProtocolMessageType(\'RegionSimilarityCalculator\', (_message.Message,), dict(\n  DESCRIPTOR = _REGIONSIMILARITYCALCULATOR,\n  __module__ = \'object_detection.protos.region_similarity_calculator_pb2\'\n  # @@protoc_insertion_point(class_scope:object_detection.protos.RegionSimilarityCalculator)\n  ))\n_sym_db.RegisterMessage(RegionSimilarityCalculator)\n\nNegSqDistSimilarity = _reflection.GeneratedProtocolMessageType(\'NegSqDistSimilarity\', (_message.Message,), dict(\n  DESCRIPTOR = _NEGSQDISTSIMILARITY,\n  __module__ = \'object_detection.protos.region_similarity_calculator_pb2\'\n  # @@protoc_insertion_point(class_scope:object_detection.protos.NegSqDistSimilarity)\n  ))\n_sym_db.RegisterMessage(NegSqDistSimilarity)\n\nIouSimilarity = _reflection.GeneratedProtocolMessageType(\'IouSimilarity\', (_message.Message,), dict(\n  DESCRIPTOR = _IOUSIMILARITY,\n  __module__ = \'object_detection.protos.region_similarity_calculator_pb2\'\n  # @@protoc_insertion_point(class_scope:object_detection.protos.IouSimilarity)\n  ))\n_sym_db.RegisterMessage(IouSimilarity)\n\nIoaSimilarity = _reflection.GeneratedProtocolMessageType(\'IoaSimilarity\', (_message.Message,), dict(\n  DESCRIPTOR = _IOASIMILARITY,\n  __module__ = \'object_detection.protos.region_similarity_calculator_pb2\'\n  # @@protoc_insertion_point(class_scope:object_detection.protos.IoaSimilarity)\n  ))\n_sym_db.RegisterMessage(IoaSimilarity)\n\n\n# @@protoc_insertion_point(module_scope)\n'"
src/object_detection/protos/square_box_coder_pb2.py,0,"b'# Generated by the protocol buffer compiler.  DO NOT EDIT!\n# source: object_detection/protos/square_box_coder.proto\n\nimport sys\n_b=sys.version_info[0]<3 and (lambda x:x) or (lambda x:x.encode(\'latin1\'))\nfrom google.protobuf import descriptor as _descriptor\nfrom google.protobuf import message as _message\nfrom google.protobuf import reflection as _reflection\nfrom google.protobuf import symbol_database as _symbol_database\nfrom google.protobuf import descriptor_pb2\n# @@protoc_insertion_point(imports)\n\n_sym_db = _symbol_database.Default()\n\n\n\n\nDESCRIPTOR = _descriptor.FileDescriptor(\n  name=\'object_detection/protos/square_box_coder.proto\',\n  package=\'object_detection.protos\',\n  syntax=\'proto2\',\n  serialized_pb=_b(\'\\n.object_detection/protos/square_box_coder.proto\\x12\\x17object_detection.protos\\""S\\n\\x0eSquareBoxCoder\\x12\\x13\\n\\x07y_scale\\x18\\x01 \\x01(\\x02:\\x02\\x31\\x30\\x12\\x13\\n\\x07x_scale\\x18\\x02 \\x01(\\x02:\\x02\\x31\\x30\\x12\\x17\\n\\x0clength_scale\\x18\\x03 \\x01(\\x02:\\x01\\x35\')\n)\n\n\n\n\n_SQUAREBOXCODER = _descriptor.Descriptor(\n  name=\'SquareBoxCoder\',\n  full_name=\'object_detection.protos.SquareBoxCoder\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'y_scale\', full_name=\'object_detection.protos.SquareBoxCoder.y_scale\', index=0,\n      number=1, type=2, cpp_type=6, label=1,\n      has_default_value=True, default_value=float(10),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'x_scale\', full_name=\'object_detection.protos.SquareBoxCoder.x_scale\', index=1,\n      number=2, type=2, cpp_type=6, label=1,\n      has_default_value=True, default_value=float(10),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'length_scale\', full_name=\'object_detection.protos.SquareBoxCoder.length_scale\', index=2,\n      number=3, type=2, cpp_type=6, label=1,\n      has_default_value=True, default_value=float(5),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto2\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=75,\n  serialized_end=158,\n)\n\nDESCRIPTOR.message_types_by_name[\'SquareBoxCoder\'] = _SQUAREBOXCODER\n_sym_db.RegisterFileDescriptor(DESCRIPTOR)\n\nSquareBoxCoder = _reflection.GeneratedProtocolMessageType(\'SquareBoxCoder\', (_message.Message,), dict(\n  DESCRIPTOR = _SQUAREBOXCODER,\n  __module__ = \'object_detection.protos.square_box_coder_pb2\'\n  # @@protoc_insertion_point(class_scope:object_detection.protos.SquareBoxCoder)\n  ))\n_sym_db.RegisterMessage(SquareBoxCoder)\n\n\n# @@protoc_insertion_point(module_scope)\n'"
src/object_detection/protos/ssd_anchor_generator_pb2.py,0,"b'# Generated by the protocol buffer compiler.  DO NOT EDIT!\n# source: object_detection/protos/ssd_anchor_generator.proto\n\nimport sys\n_b=sys.version_info[0]<3 and (lambda x:x) or (lambda x:x.encode(\'latin1\'))\nfrom google.protobuf import descriptor as _descriptor\nfrom google.protobuf import message as _message\nfrom google.protobuf import reflection as _reflection\nfrom google.protobuf import symbol_database as _symbol_database\nfrom google.protobuf import descriptor_pb2\n# @@protoc_insertion_point(imports)\n\n_sym_db = _symbol_database.Default()\n\n\n\n\nDESCRIPTOR = _descriptor.FileDescriptor(\n  name=\'object_detection/protos/ssd_anchor_generator.proto\',\n  package=\'object_detection.protos\',\n  syntax=\'proto2\',\n  serialized_pb=_b(\'\\n2object_detection/protos/ssd_anchor_generator.proto\\x12\\x17object_detection.protos\\""\\xf2\\x02\\n\\x12SsdAnchorGenerator\\x12\\x15\\n\\nnum_layers\\x18\\x01 \\x01(\\x05:\\x01\\x36\\x12\\x16\\n\\tmin_scale\\x18\\x02 \\x01(\\x02:\\x03\\x30.2\\x12\\x17\\n\\tmax_scale\\x18\\x03 \\x01(\\x02:\\x04\\x30.95\\x12\\x0e\\n\\x06scales\\x18\\x0c \\x03(\\x02\\x12\\x15\\n\\raspect_ratios\\x18\\x04 \\x03(\\x02\\x12*\\n\\x1finterpolated_scale_aspect_ratio\\x18\\r \\x01(\\x02:\\x01\\x31\\x12*\\n\\x1creduce_boxes_in_lowest_layer\\x18\\x05 \\x01(\\x08:\\x04true\\x12\\x1d\\n\\x12\\x62\\x61se_anchor_height\\x18\\x06 \\x01(\\x02:\\x01\\x31\\x12\\x1c\\n\\x11\\x62\\x61se_anchor_width\\x18\\x07 \\x01(\\x02:\\x01\\x31\\x12\\x15\\n\\rheight_stride\\x18\\x08 \\x03(\\x05\\x12\\x14\\n\\x0cwidth_stride\\x18\\t \\x03(\\x05\\x12\\x15\\n\\rheight_offset\\x18\\n \\x03(\\x05\\x12\\x14\\n\\x0cwidth_offset\\x18\\x0b \\x03(\\x05\')\n)\n\n\n\n\n_SSDANCHORGENERATOR = _descriptor.Descriptor(\n  name=\'SsdAnchorGenerator\',\n  full_name=\'object_detection.protos.SsdAnchorGenerator\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'num_layers\', full_name=\'object_detection.protos.SsdAnchorGenerator.num_layers\', index=0,\n      number=1, type=5, cpp_type=1, label=1,\n      has_default_value=True, default_value=6,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'min_scale\', full_name=\'object_detection.protos.SsdAnchorGenerator.min_scale\', index=1,\n      number=2, type=2, cpp_type=6, label=1,\n      has_default_value=True, default_value=float(0.2),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'max_scale\', full_name=\'object_detection.protos.SsdAnchorGenerator.max_scale\', index=2,\n      number=3, type=2, cpp_type=6, label=1,\n      has_default_value=True, default_value=float(0.95),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'scales\', full_name=\'object_detection.protos.SsdAnchorGenerator.scales\', index=3,\n      number=12, type=2, cpp_type=6, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'aspect_ratios\', full_name=\'object_detection.protos.SsdAnchorGenerator.aspect_ratios\', index=4,\n      number=4, type=2, cpp_type=6, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'interpolated_scale_aspect_ratio\', full_name=\'object_detection.protos.SsdAnchorGenerator.interpolated_scale_aspect_ratio\', index=5,\n      number=13, type=2, cpp_type=6, label=1,\n      has_default_value=True, default_value=float(1),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'reduce_boxes_in_lowest_layer\', full_name=\'object_detection.protos.SsdAnchorGenerator.reduce_boxes_in_lowest_layer\', index=6,\n      number=5, type=8, cpp_type=7, label=1,\n      has_default_value=True, default_value=True,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'base_anchor_height\', full_name=\'object_detection.protos.SsdAnchorGenerator.base_anchor_height\', index=7,\n      number=6, type=2, cpp_type=6, label=1,\n      has_default_value=True, default_value=float(1),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'base_anchor_width\', full_name=\'object_detection.protos.SsdAnchorGenerator.base_anchor_width\', index=8,\n      number=7, type=2, cpp_type=6, label=1,\n      has_default_value=True, default_value=float(1),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'height_stride\', full_name=\'object_detection.protos.SsdAnchorGenerator.height_stride\', index=9,\n      number=8, type=5, cpp_type=1, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'width_stride\', full_name=\'object_detection.protos.SsdAnchorGenerator.width_stride\', index=10,\n      number=9, type=5, cpp_type=1, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'height_offset\', full_name=\'object_detection.protos.SsdAnchorGenerator.height_offset\', index=11,\n      number=10, type=5, cpp_type=1, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'width_offset\', full_name=\'object_detection.protos.SsdAnchorGenerator.width_offset\', index=12,\n      number=11, type=5, cpp_type=1, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto2\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=80,\n  serialized_end=450,\n)\n\nDESCRIPTOR.message_types_by_name[\'SsdAnchorGenerator\'] = _SSDANCHORGENERATOR\n_sym_db.RegisterFileDescriptor(DESCRIPTOR)\n\nSsdAnchorGenerator = _reflection.GeneratedProtocolMessageType(\'SsdAnchorGenerator\', (_message.Message,), dict(\n  DESCRIPTOR = _SSDANCHORGENERATOR,\n  __module__ = \'object_detection.protos.ssd_anchor_generator_pb2\'\n  # @@protoc_insertion_point(class_scope:object_detection.protos.SsdAnchorGenerator)\n  ))\n_sym_db.RegisterMessage(SsdAnchorGenerator)\n\n\n# @@protoc_insertion_point(module_scope)\n'"
src/object_detection/protos/ssd_pb2.py,0,"b'# Generated by the protocol buffer compiler.  DO NOT EDIT!\n# source: object_detection/protos/ssd.proto\n\nimport sys\n_b=sys.version_info[0]<3 and (lambda x:x) or (lambda x:x.encode(\'latin1\'))\nfrom google.protobuf import descriptor as _descriptor\nfrom google.protobuf import message as _message\nfrom google.protobuf import reflection as _reflection\nfrom google.protobuf import symbol_database as _symbol_database\nfrom google.protobuf import descriptor_pb2\n# @@protoc_insertion_point(imports)\n\n_sym_db = _symbol_database.Default()\n\n\nfrom object_detection.protos import anchor_generator_pb2 as object__detection_dot_protos_dot_anchor__generator__pb2\nfrom object_detection.protos import box_coder_pb2 as object__detection_dot_protos_dot_box__coder__pb2\nfrom object_detection.protos import box_predictor_pb2 as object__detection_dot_protos_dot_box__predictor__pb2\nfrom object_detection.protos import hyperparams_pb2 as object__detection_dot_protos_dot_hyperparams__pb2\nfrom object_detection.protos import image_resizer_pb2 as object__detection_dot_protos_dot_image__resizer__pb2\nfrom object_detection.protos import matcher_pb2 as object__detection_dot_protos_dot_matcher__pb2\nfrom object_detection.protos import losses_pb2 as object__detection_dot_protos_dot_losses__pb2\nfrom object_detection.protos import post_processing_pb2 as object__detection_dot_protos_dot_post__processing__pb2\nfrom object_detection.protos import region_similarity_calculator_pb2 as object__detection_dot_protos_dot_region__similarity__calculator__pb2\n\n\nDESCRIPTOR = _descriptor.FileDescriptor(\n  name=\'object_detection/protos/ssd.proto\',\n  package=\'object_detection.protos\',\n  syntax=\'proto2\',\n  serialized_pb=_b(\'\\n!object_detection/protos/ssd.proto\\x12\\x17object_detection.protos\\x1a.object_detection/protos/anchor_generator.proto\\x1a\\\'object_detection/protos/box_coder.proto\\x1a+object_detection/protos/box_predictor.proto\\x1a)object_detection/protos/hyperparams.proto\\x1a+object_detection/protos/image_resizer.proto\\x1a%object_detection/protos/matcher.proto\\x1a$object_detection/protos/losses.proto\\x1a-object_detection/protos/post_processing.proto\\x1a:object_detection/protos/region_similarity_calculator.proto\\""\\xf8\\x05\\n\\x03Ssd\\x12\\x13\\n\\x0bnum_classes\\x18\\x01 \\x01(\\x05\\x12<\\n\\rimage_resizer\\x18\\x02 \\x01(\\x0b\\x32%.object_detection.protos.ImageResizer\\x12G\\n\\x11\\x66\\x65\\x61ture_extractor\\x18\\x03 \\x01(\\x0b\\x32,.object_detection.protos.SsdFeatureExtractor\\x12\\x34\\n\\tbox_coder\\x18\\x04 \\x01(\\x0b\\x32!.object_detection.protos.BoxCoder\\x12\\x31\\n\\x07matcher\\x18\\x05 \\x01(\\x0b\\x32 .object_detection.protos.Matcher\\x12R\\n\\x15similarity_calculator\\x18\\x06 \\x01(\\x0b\\x32\\x33.object_detection.protos.RegionSimilarityCalculator\\x12)\\n\\x1a\\x65ncode_background_as_zeros\\x18\\x0c \\x01(\\x08:\\x05\\x66\\x61lse\\x12 \\n\\x15negative_class_weight\\x18\\r \\x01(\\x02:\\x01\\x31\\x12<\\n\\rbox_predictor\\x18\\x07 \\x01(\\x0b\\x32%.object_detection.protos.BoxPredictor\\x12\\x42\\n\\x10\\x61nchor_generator\\x18\\x08 \\x01(\\x0b\\x32(.object_detection.protos.AnchorGenerator\\x12@\\n\\x0fpost_processing\\x18\\t \\x01(\\x0b\\x32\\\'.object_detection.protos.PostProcessing\\x12+\\n\\x1dnormalize_loss_by_num_matches\\x18\\n \\x01(\\x08:\\x04true\\x12-\\n\\x1enormalize_loc_loss_by_codesize\\x18\\x0e \\x01(\\x08:\\x05\\x66\\x61lse\\x12+\\n\\x04loss\\x18\\x0b \\x01(\\x0b\\x32\\x1d.object_detection.protos.Loss\\""\\x9a\\x02\\n\\x13SsdFeatureExtractor\\x12\\x0c\\n\\x04type\\x18\\x01 \\x01(\\t\\x12\\x1b\\n\\x10\\x64\\x65pth_multiplier\\x18\\x02 \\x01(\\x02:\\x01\\x31\\x12\\x15\\n\\tmin_depth\\x18\\x03 \\x01(\\x05:\\x02\\x31\\x36\\x12>\\n\\x10\\x63onv_hyperparams\\x18\\x04 \\x01(\\x0b\\x32$.object_detection.protos.Hyperparams\\x12\\x1a\\n\\x0fpad_to_multiple\\x18\\x05 \\x01(\\x05:\\x01\\x31\\x12\\""\\n\\x14\\x62\\x61tch_norm_trainable\\x18\\x06 \\x01(\\x08:\\x04true\\x12#\\n\\x14use_explicit_padding\\x18\\x07 \\x01(\\x08:\\x05\\x66\\x61lse\\x12\\x1c\\n\\ruse_depthwise\\x18\\x08 \\x01(\\x08:\\x05\\x66\\x61lse\')\n  ,\n  dependencies=[object__detection_dot_protos_dot_anchor__generator__pb2.DESCRIPTOR,object__detection_dot_protos_dot_box__coder__pb2.DESCRIPTOR,object__detection_dot_protos_dot_box__predictor__pb2.DESCRIPTOR,object__detection_dot_protos_dot_hyperparams__pb2.DESCRIPTOR,object__detection_dot_protos_dot_image__resizer__pb2.DESCRIPTOR,object__detection_dot_protos_dot_matcher__pb2.DESCRIPTOR,object__detection_dot_protos_dot_losses__pb2.DESCRIPTOR,object__detection_dot_protos_dot_post__processing__pb2.DESCRIPTOR,object__detection_dot_protos_dot_region__similarity__calculator__pb2.DESCRIPTOR,])\n\n\n\n\n_SSD = _descriptor.Descriptor(\n  name=\'Ssd\',\n  full_name=\'object_detection.protos.Ssd\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'num_classes\', full_name=\'object_detection.protos.Ssd.num_classes\', index=0,\n      number=1, type=5, cpp_type=1, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'image_resizer\', full_name=\'object_detection.protos.Ssd.image_resizer\', index=1,\n      number=2, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'feature_extractor\', full_name=\'object_detection.protos.Ssd.feature_extractor\', index=2,\n      number=3, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'box_coder\', full_name=\'object_detection.protos.Ssd.box_coder\', index=3,\n      number=4, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'matcher\', full_name=\'object_detection.protos.Ssd.matcher\', index=4,\n      number=5, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'similarity_calculator\', full_name=\'object_detection.protos.Ssd.similarity_calculator\', index=5,\n      number=6, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'encode_background_as_zeros\', full_name=\'object_detection.protos.Ssd.encode_background_as_zeros\', index=6,\n      number=12, type=8, cpp_type=7, label=1,\n      has_default_value=True, default_value=False,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'negative_class_weight\', full_name=\'object_detection.protos.Ssd.negative_class_weight\', index=7,\n      number=13, type=2, cpp_type=6, label=1,\n      has_default_value=True, default_value=float(1),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'box_predictor\', full_name=\'object_detection.protos.Ssd.box_predictor\', index=8,\n      number=7, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'anchor_generator\', full_name=\'object_detection.protos.Ssd.anchor_generator\', index=9,\n      number=8, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'post_processing\', full_name=\'object_detection.protos.Ssd.post_processing\', index=10,\n      number=9, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'normalize_loss_by_num_matches\', full_name=\'object_detection.protos.Ssd.normalize_loss_by_num_matches\', index=11,\n      number=10, type=8, cpp_type=7, label=1,\n      has_default_value=True, default_value=True,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'normalize_loc_loss_by_codesize\', full_name=\'object_detection.protos.Ssd.normalize_loc_loss_by_codesize\', index=12,\n      number=14, type=8, cpp_type=7, label=1,\n      has_default_value=True, default_value=False,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'loss\', full_name=\'object_detection.protos.Ssd.loss\', index=13,\n      number=11, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto2\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=469,\n  serialized_end=1229,\n)\n\n\n_SSDFEATUREEXTRACTOR = _descriptor.Descriptor(\n  name=\'SsdFeatureExtractor\',\n  full_name=\'object_detection.protos.SsdFeatureExtractor\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'type\', full_name=\'object_detection.protos.SsdFeatureExtractor.type\', index=0,\n      number=1, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=_b("""").decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'depth_multiplier\', full_name=\'object_detection.protos.SsdFeatureExtractor.depth_multiplier\', index=1,\n      number=2, type=2, cpp_type=6, label=1,\n      has_default_value=True, default_value=float(1),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'min_depth\', full_name=\'object_detection.protos.SsdFeatureExtractor.min_depth\', index=2,\n      number=3, type=5, cpp_type=1, label=1,\n      has_default_value=True, default_value=16,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'conv_hyperparams\', full_name=\'object_detection.protos.SsdFeatureExtractor.conv_hyperparams\', index=3,\n      number=4, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'pad_to_multiple\', full_name=\'object_detection.protos.SsdFeatureExtractor.pad_to_multiple\', index=4,\n      number=5, type=5, cpp_type=1, label=1,\n      has_default_value=True, default_value=1,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'batch_norm_trainable\', full_name=\'object_detection.protos.SsdFeatureExtractor.batch_norm_trainable\', index=5,\n      number=6, type=8, cpp_type=7, label=1,\n      has_default_value=True, default_value=True,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'use_explicit_padding\', full_name=\'object_detection.protos.SsdFeatureExtractor.use_explicit_padding\', index=6,\n      number=7, type=8, cpp_type=7, label=1,\n      has_default_value=True, default_value=False,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'use_depthwise\', full_name=\'object_detection.protos.SsdFeatureExtractor.use_depthwise\', index=7,\n      number=8, type=8, cpp_type=7, label=1,\n      has_default_value=True, default_value=False,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto2\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=1232,\n  serialized_end=1514,\n)\n\n_SSD.fields_by_name[\'image_resizer\'].message_type = object__detection_dot_protos_dot_image__resizer__pb2._IMAGERESIZER\n_SSD.fields_by_name[\'feature_extractor\'].message_type = _SSDFEATUREEXTRACTOR\n_SSD.fields_by_name[\'box_coder\'].message_type = object__detection_dot_protos_dot_box__coder__pb2._BOXCODER\n_SSD.fields_by_name[\'matcher\'].message_type = object__detection_dot_protos_dot_matcher__pb2._MATCHER\n_SSD.fields_by_name[\'similarity_calculator\'].message_type = object__detection_dot_protos_dot_region__similarity__calculator__pb2._REGIONSIMILARITYCALCULATOR\n_SSD.fields_by_name[\'box_predictor\'].message_type = object__detection_dot_protos_dot_box__predictor__pb2._BOXPREDICTOR\n_SSD.fields_by_name[\'anchor_generator\'].message_type = object__detection_dot_protos_dot_anchor__generator__pb2._ANCHORGENERATOR\n_SSD.fields_by_name[\'post_processing\'].message_type = object__detection_dot_protos_dot_post__processing__pb2._POSTPROCESSING\n_SSD.fields_by_name[\'loss\'].message_type = object__detection_dot_protos_dot_losses__pb2._LOSS\n_SSDFEATUREEXTRACTOR.fields_by_name[\'conv_hyperparams\'].message_type = object__detection_dot_protos_dot_hyperparams__pb2._HYPERPARAMS\nDESCRIPTOR.message_types_by_name[\'Ssd\'] = _SSD\nDESCRIPTOR.message_types_by_name[\'SsdFeatureExtractor\'] = _SSDFEATUREEXTRACTOR\n_sym_db.RegisterFileDescriptor(DESCRIPTOR)\n\nSsd = _reflection.GeneratedProtocolMessageType(\'Ssd\', (_message.Message,), dict(\n  DESCRIPTOR = _SSD,\n  __module__ = \'object_detection.protos.ssd_pb2\'\n  # @@protoc_insertion_point(class_scope:object_detection.protos.Ssd)\n  ))\n_sym_db.RegisterMessage(Ssd)\n\nSsdFeatureExtractor = _reflection.GeneratedProtocolMessageType(\'SsdFeatureExtractor\', (_message.Message,), dict(\n  DESCRIPTOR = _SSDFEATUREEXTRACTOR,\n  __module__ = \'object_detection.protos.ssd_pb2\'\n  # @@protoc_insertion_point(class_scope:object_detection.protos.SsdFeatureExtractor)\n  ))\n_sym_db.RegisterMessage(SsdFeatureExtractor)\n\n\n# @@protoc_insertion_point(module_scope)\n'"
src/object_detection/protos/string_int_label_map_pb2.py,0,"b'# Generated by the protocol buffer compiler.  DO NOT EDIT!\n# source: object_detection/protos/string_int_label_map.proto\n\nimport sys\n_b=sys.version_info[0]<3 and (lambda x:x) or (lambda x:x.encode(\'latin1\'))\nfrom google.protobuf import descriptor as _descriptor\nfrom google.protobuf import message as _message\nfrom google.protobuf import reflection as _reflection\nfrom google.protobuf import symbol_database as _symbol_database\nfrom google.protobuf import descriptor_pb2\n# @@protoc_insertion_point(imports)\n\n_sym_db = _symbol_database.Default()\n\n\n\n\nDESCRIPTOR = _descriptor.FileDescriptor(\n  name=\'object_detection/protos/string_int_label_map.proto\',\n  package=\'object_detection.protos\',\n  syntax=\'proto2\',\n  serialized_pb=_b(\'\\n2object_detection/protos/string_int_label_map.proto\\x12\\x17object_detection.protos\\""G\\n\\x15StringIntLabelMapItem\\x12\\x0c\\n\\x04name\\x18\\x01 \\x01(\\t\\x12\\n\\n\\x02id\\x18\\x02 \\x01(\\x05\\x12\\x14\\n\\x0c\\x64isplay_name\\x18\\x03 \\x01(\\t\\""Q\\n\\x11StringIntLabelMap\\x12<\\n\\x04item\\x18\\x01 \\x03(\\x0b\\x32..object_detection.protos.StringIntLabelMapItem\')\n)\n\n\n\n\n_STRINGINTLABELMAPITEM = _descriptor.Descriptor(\n  name=\'StringIntLabelMapItem\',\n  full_name=\'object_detection.protos.StringIntLabelMapItem\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'name\', full_name=\'object_detection.protos.StringIntLabelMapItem.name\', index=0,\n      number=1, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=_b("""").decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'id\', full_name=\'object_detection.protos.StringIntLabelMapItem.id\', index=1,\n      number=2, type=5, cpp_type=1, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'display_name\', full_name=\'object_detection.protos.StringIntLabelMapItem.display_name\', index=2,\n      number=3, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=_b("""").decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto2\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=79,\n  serialized_end=150,\n)\n\n\n_STRINGINTLABELMAP = _descriptor.Descriptor(\n  name=\'StringIntLabelMap\',\n  full_name=\'object_detection.protos.StringIntLabelMap\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'item\', full_name=\'object_detection.protos.StringIntLabelMap.item\', index=0,\n      number=1, type=11, cpp_type=10, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto2\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=152,\n  serialized_end=233,\n)\n\n_STRINGINTLABELMAP.fields_by_name[\'item\'].message_type = _STRINGINTLABELMAPITEM\nDESCRIPTOR.message_types_by_name[\'StringIntLabelMapItem\'] = _STRINGINTLABELMAPITEM\nDESCRIPTOR.message_types_by_name[\'StringIntLabelMap\'] = _STRINGINTLABELMAP\n_sym_db.RegisterFileDescriptor(DESCRIPTOR)\n\nStringIntLabelMapItem = _reflection.GeneratedProtocolMessageType(\'StringIntLabelMapItem\', (_message.Message,), dict(\n  DESCRIPTOR = _STRINGINTLABELMAPITEM,\n  __module__ = \'object_detection.protos.string_int_label_map_pb2\'\n  # @@protoc_insertion_point(class_scope:object_detection.protos.StringIntLabelMapItem)\n  ))\n_sym_db.RegisterMessage(StringIntLabelMapItem)\n\nStringIntLabelMap = _reflection.GeneratedProtocolMessageType(\'StringIntLabelMap\', (_message.Message,), dict(\n  DESCRIPTOR = _STRINGINTLABELMAP,\n  __module__ = \'object_detection.protos.string_int_label_map_pb2\'\n  # @@protoc_insertion_point(class_scope:object_detection.protos.StringIntLabelMap)\n  ))\n_sym_db.RegisterMessage(StringIntLabelMap)\n\n\n# @@protoc_insertion_point(module_scope)\n'"
src/object_detection/protos/train_pb2.py,0,"b'# Generated by the protocol buffer compiler.  DO NOT EDIT!\n# source: object_detection/protos/train.proto\n\nimport sys\n_b=sys.version_info[0]<3 and (lambda x:x) or (lambda x:x.encode(\'latin1\'))\nfrom google.protobuf import descriptor as _descriptor\nfrom google.protobuf import message as _message\nfrom google.protobuf import reflection as _reflection\nfrom google.protobuf import symbol_database as _symbol_database\nfrom google.protobuf import descriptor_pb2\n# @@protoc_insertion_point(imports)\n\n_sym_db = _symbol_database.Default()\n\n\nfrom object_detection.protos import optimizer_pb2 as object__detection_dot_protos_dot_optimizer__pb2\nfrom object_detection.protos import preprocessor_pb2 as object__detection_dot_protos_dot_preprocessor__pb2\n\n\nDESCRIPTOR = _descriptor.FileDescriptor(\n  name=\'object_detection/protos/train.proto\',\n  package=\'object_detection.protos\',\n  syntax=\'proto2\',\n  serialized_pb=_b(\'\\n#object_detection/protos/train.proto\\x12\\x17object_detection.protos\\x1a\\\'object_detection/protos/optimizer.proto\\x1a*object_detection/protos/preprocessor.proto\\""\\xde\\x06\\n\\x0bTrainConfig\\x12\\x16\\n\\nbatch_size\\x18\\x01 \\x01(\\r:\\x02\\x33\\x32\\x12M\\n\\x19\\x64\\x61ta_augmentation_options\\x18\\x02 \\x03(\\x0b\\x32*.object_detection.protos.PreprocessingStep\\x12\\x1c\\n\\rsync_replicas\\x18\\x03 \\x01(\\x08:\\x05\\x66\\x61lse\\x12+\\n\\x1dkeep_checkpoint_every_n_hours\\x18\\x04 \\x01(\\r:\\x04\\x31\\x30\\x30\\x30\\x12\\x35\\n\\toptimizer\\x18\\x05 \\x01(\\x0b\\x32\\"".object_detection.protos.Optimizer\\x12$\\n\\x19gradient_clipping_by_norm\\x18\\x06 \\x01(\\x02:\\x01\\x30\\x12\\x1e\\n\\x14\\x66ine_tune_checkpoint\\x18\\x07 \\x01(\\t:\\x00\\x12#\\n\\x19\\x66ine_tune_checkpoint_type\\x18\\x16 \\x01(\\t:\\x00\\x12,\\n\\x19\\x66rom_detection_checkpoint\\x18\\x08 \\x01(\\x08:\\x05\\x66\\x61lseB\\x02\\x18\\x01\\x12\\x31\\n\\""load_all_detection_checkpoint_vars\\x18\\x13 \\x01(\\x08:\\x05\\x66\\x61lse\\x12\\x14\\n\\tnum_steps\\x18\\t \\x01(\\r:\\x01\\x30\\x12\\x1f\\n\\x13startup_delay_steps\\x18\\n \\x01(\\x02:\\x02\\x31\\x35\\x12\\x1f\\n\\x14\\x62ias_grad_multiplier\\x18\\x0b \\x01(\\x02:\\x01\\x30\\x12\\x18\\n\\x10\\x66reeze_variables\\x18\\x0c \\x03(\\t\\x12 \\n\\x15replicas_to_aggregate\\x18\\r \\x01(\\x05:\\x01\\x31\\x12!\\n\\x14\\x62\\x61tch_queue_capacity\\x18\\x0e \\x01(\\x05:\\x03\\x31\\x35\\x30\\x12\\""\\n\\x17num_batch_queue_threads\\x18\\x0f \\x01(\\x05:\\x01\\x38\\x12\\""\\n\\x17prefetch_queue_capacity\\x18\\x10 \\x01(\\x05:\\x01\\x35\\x12)\\n\\x1amerge_multiple_label_boxes\\x18\\x11 \\x01(\\x08:\\x05\\x66\\x61lse\\x12%\\n\\x17\\x61\\x64\\x64_regularization_loss\\x18\\x12 \\x01(\\x08:\\x04true\\x12 \\n\\x13max_number_of_boxes\\x18\\x14 \\x01(\\x05:\\x03\\x31\\x30\\x30\\x12\\\'\\n\\x19unpad_groundtruth_tensors\\x18\\x15 \\x01(\\x08:\\x04true\')\n  ,\n  dependencies=[object__detection_dot_protos_dot_optimizer__pb2.DESCRIPTOR,object__detection_dot_protos_dot_preprocessor__pb2.DESCRIPTOR,])\n\n\n\n\n_TRAINCONFIG = _descriptor.Descriptor(\n  name=\'TrainConfig\',\n  full_name=\'object_detection.protos.TrainConfig\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'batch_size\', full_name=\'object_detection.protos.TrainConfig.batch_size\', index=0,\n      number=1, type=13, cpp_type=3, label=1,\n      has_default_value=True, default_value=32,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'data_augmentation_options\', full_name=\'object_detection.protos.TrainConfig.data_augmentation_options\', index=1,\n      number=2, type=11, cpp_type=10, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'sync_replicas\', full_name=\'object_detection.protos.TrainConfig.sync_replicas\', index=2,\n      number=3, type=8, cpp_type=7, label=1,\n      has_default_value=True, default_value=False,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'keep_checkpoint_every_n_hours\', full_name=\'object_detection.protos.TrainConfig.keep_checkpoint_every_n_hours\', index=3,\n      number=4, type=13, cpp_type=3, label=1,\n      has_default_value=True, default_value=1000,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'optimizer\', full_name=\'object_detection.protos.TrainConfig.optimizer\', index=4,\n      number=5, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'gradient_clipping_by_norm\', full_name=\'object_detection.protos.TrainConfig.gradient_clipping_by_norm\', index=5,\n      number=6, type=2, cpp_type=6, label=1,\n      has_default_value=True, default_value=float(0),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'fine_tune_checkpoint\', full_name=\'object_detection.protos.TrainConfig.fine_tune_checkpoint\', index=6,\n      number=7, type=9, cpp_type=9, label=1,\n      has_default_value=True, default_value=_b("""").decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'fine_tune_checkpoint_type\', full_name=\'object_detection.protos.TrainConfig.fine_tune_checkpoint_type\', index=7,\n      number=22, type=9, cpp_type=9, label=1,\n      has_default_value=True, default_value=_b("""").decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'from_detection_checkpoint\', full_name=\'object_detection.protos.TrainConfig.from_detection_checkpoint\', index=8,\n      number=8, type=8, cpp_type=7, label=1,\n      has_default_value=True, default_value=False,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=_descriptor._ParseOptions(descriptor_pb2.FieldOptions(), _b(\'\\030\\001\'))),\n    _descriptor.FieldDescriptor(\n      name=\'load_all_detection_checkpoint_vars\', full_name=\'object_detection.protos.TrainConfig.load_all_detection_checkpoint_vars\', index=9,\n      number=19, type=8, cpp_type=7, label=1,\n      has_default_value=True, default_value=False,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'num_steps\', full_name=\'object_detection.protos.TrainConfig.num_steps\', index=10,\n      number=9, type=13, cpp_type=3, label=1,\n      has_default_value=True, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'startup_delay_steps\', full_name=\'object_detection.protos.TrainConfig.startup_delay_steps\', index=11,\n      number=10, type=2, cpp_type=6, label=1,\n      has_default_value=True, default_value=float(15),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'bias_grad_multiplier\', full_name=\'object_detection.protos.TrainConfig.bias_grad_multiplier\', index=12,\n      number=11, type=2, cpp_type=6, label=1,\n      has_default_value=True, default_value=float(0),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'freeze_variables\', full_name=\'object_detection.protos.TrainConfig.freeze_variables\', index=13,\n      number=12, type=9, cpp_type=9, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'replicas_to_aggregate\', full_name=\'object_detection.protos.TrainConfig.replicas_to_aggregate\', index=14,\n      number=13, type=5, cpp_type=1, label=1,\n      has_default_value=True, default_value=1,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'batch_queue_capacity\', full_name=\'object_detection.protos.TrainConfig.batch_queue_capacity\', index=15,\n      number=14, type=5, cpp_type=1, label=1,\n      has_default_value=True, default_value=150,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'num_batch_queue_threads\', full_name=\'object_detection.protos.TrainConfig.num_batch_queue_threads\', index=16,\n      number=15, type=5, cpp_type=1, label=1,\n      has_default_value=True, default_value=8,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'prefetch_queue_capacity\', full_name=\'object_detection.protos.TrainConfig.prefetch_queue_capacity\', index=17,\n      number=16, type=5, cpp_type=1, label=1,\n      has_default_value=True, default_value=5,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'merge_multiple_label_boxes\', full_name=\'object_detection.protos.TrainConfig.merge_multiple_label_boxes\', index=18,\n      number=17, type=8, cpp_type=7, label=1,\n      has_default_value=True, default_value=False,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'add_regularization_loss\', full_name=\'object_detection.protos.TrainConfig.add_regularization_loss\', index=19,\n      number=18, type=8, cpp_type=7, label=1,\n      has_default_value=True, default_value=True,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'max_number_of_boxes\', full_name=\'object_detection.protos.TrainConfig.max_number_of_boxes\', index=20,\n      number=20, type=5, cpp_type=1, label=1,\n      has_default_value=True, default_value=100,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'unpad_groundtruth_tensors\', full_name=\'object_detection.protos.TrainConfig.unpad_groundtruth_tensors\', index=21,\n      number=21, type=8, cpp_type=7, label=1,\n      has_default_value=True, default_value=True,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto2\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=150,\n  serialized_end=1012,\n)\n\n_TRAINCONFIG.fields_by_name[\'data_augmentation_options\'].message_type = object__detection_dot_protos_dot_preprocessor__pb2._PREPROCESSINGSTEP\n_TRAINCONFIG.fields_by_name[\'optimizer\'].message_type = object__detection_dot_protos_dot_optimizer__pb2._OPTIMIZER\nDESCRIPTOR.message_types_by_name[\'TrainConfig\'] = _TRAINCONFIG\n_sym_db.RegisterFileDescriptor(DESCRIPTOR)\n\nTrainConfig = _reflection.GeneratedProtocolMessageType(\'TrainConfig\', (_message.Message,), dict(\n  DESCRIPTOR = _TRAINCONFIG,\n  __module__ = \'object_detection.protos.train_pb2\'\n  # @@protoc_insertion_point(class_scope:object_detection.protos.TrainConfig)\n  ))\n_sym_db.RegisterMessage(TrainConfig)\n\n\n_TRAINCONFIG.fields_by_name[\'from_detection_checkpoint\'].has_options = True\n_TRAINCONFIG.fields_by_name[\'from_detection_checkpoint\']._options = _descriptor._ParseOptions(descriptor_pb2.FieldOptions(), _b(\'\\030\\001\'))\n# @@protoc_insertion_point(module_scope)\n'"
src/object_detection/utils/__init__.py,0,b''
src/object_detection/utils/category_util.py,2,"b'# Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n""""""Functions for importing/exporting Object Detection categories.""""""\nimport csv\n\nimport tensorflow as tf\n\n\ndef load_categories_from_csv_file(csv_path):\n  """"""Loads categories from a csv file.\n\n  The CSV file should have one comma delimited numeric category id and string\n  category name pair per line. For example:\n\n  0,""cat""\n  1,""dog""\n  2,""bird""\n  ...\n\n  Args:\n    csv_path: Path to the csv file to be parsed into categories.\n  Returns:\n    categories: A list of dictionaries representing all possible categories.\n                The categories will contain an integer \'id\' field and a string\n                \'name\' field.\n  Raises:\n    ValueError: If the csv file is incorrectly formatted.\n  """"""\n  categories = []\n\n  with tf.gfile.Open(csv_path, \'r\') as csvfile:\n    reader = csv.reader(csvfile, delimiter=\',\', quotechar=\'""\')\n    for row in reader:\n      if not row:\n        continue\n\n      if len(row) != 2:\n        raise ValueError(\'Expected 2 fields per row in csv: %s\' % \',\'.join(row))\n\n      category_id = int(row[0])\n      category_name = row[1]\n      categories.append({\'id\': category_id, \'name\': category_name})\n\n  return categories\n\n\ndef save_categories_to_csv_file(categories, csv_path):\n  """"""Saves categories to a csv file.\n\n  Args:\n    categories: A list of dictionaries representing categories to save to file.\n                Each category must contain an \'id\' and \'name\' field.\n    csv_path: Path to the csv file to be parsed into categories.\n  """"""\n  categories.sort(key=lambda x: x[\'id\'])\n  with tf.gfile.Open(csv_path, \'w\') as csvfile:\n    writer = csv.writer(csvfile, delimiter=\',\', quotechar=\'""\')\n    for category in categories:\n      writer.writerow([category[\'id\'], category[\'name\']])\n'"
src/object_detection/utils/category_util_test.py,3,"b'# Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n""""""Tests for object_detection.utils.category_util.""""""\nimport os\n\nimport tensorflow as tf\n\nfrom object_detection.utils import category_util\n\n\nclass EvalUtilTest(tf.test.TestCase):\n\n  def test_load_categories_from_csv_file(self):\n    csv_data = """"""\n        0,""cat""\n        1,""dog""\n        2,""bird""\n    """""".strip(\' \')\n    csv_path = os.path.join(self.get_temp_dir(), \'test.csv\')\n    with tf.gfile.Open(csv_path, \'wb\') as f:\n      f.write(csv_data)\n\n    categories = category_util.load_categories_from_csv_file(csv_path)\n    self.assertTrue({\'id\': 0, \'name\': \'cat\'} in categories)\n    self.assertTrue({\'id\': 1, \'name\': \'dog\'} in categories)\n    self.assertTrue({\'id\': 2, \'name\': \'bird\'} in categories)\n\n  def test_save_categories_to_csv_file(self):\n    categories = [\n        {\'id\': 0, \'name\': \'cat\'},\n        {\'id\': 1, \'name\': \'dog\'},\n        {\'id\': 2, \'name\': \'bird\'},\n    ]\n    csv_path = os.path.join(self.get_temp_dir(), \'test.csv\')\n    category_util.save_categories_to_csv_file(categories, csv_path)\n    saved_categories = category_util.load_categories_from_csv_file(csv_path)\n    self.assertEqual(saved_categories, categories)\n\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
src/object_detection/utils/config_util.py,15,"b'# Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Functions for reading and updating configuration files.""""""\n\nimport tensorflow as tf\n\nfrom google.protobuf import text_format\n\nfrom object_detection.protos import eval_pb2\nfrom object_detection.protos import input_reader_pb2\nfrom object_detection.protos import model_pb2\nfrom object_detection.protos import pipeline_pb2\nfrom object_detection.protos import train_pb2\n\n\ndef get_image_resizer_config(model_config):\n  """"""Returns the image resizer config from a model config.\n\n  Args:\n    model_config: A model_pb2.DetectionModel.\n\n  Returns:\n    An image_resizer_pb2.ImageResizer.\n\n  Raises:\n    ValueError: If the model type is not recognized.\n  """"""\n  meta_architecture = model_config.WhichOneof(""model"")\n  if meta_architecture == ""faster_rcnn"":\n    return model_config.faster_rcnn.image_resizer\n  if meta_architecture == ""ssd"":\n    return model_config.ssd.image_resizer\n\n  raise ValueError(""Unknown model type: {}"".format(meta_architecture))\n\n\ndef get_spatial_image_size(image_resizer_config):\n  """"""Returns expected spatial size of the output image from a given config.\n\n  Args:\n    image_resizer_config: An image_resizer_pb2.ImageResizer.\n\n  Returns:\n    A list of two integers of the form [height, width]. `height` and `width` are\n    set  -1 if they cannot be determined during graph construction.\n\n  Raises:\n    ValueError: If the model type is not recognized.\n  """"""\n  if image_resizer_config.HasField(""fixed_shape_resizer""):\n    return [image_resizer_config.fixed_shape_resizer.height,\n            image_resizer_config.fixed_shape_resizer.width]\n  if image_resizer_config.HasField(""keep_aspect_ratio_resizer""):\n    if image_resizer_config.keep_aspect_ratio_resizer.pad_to_max_dimension:\n      return [image_resizer_config.keep_aspect_ratio_resizer.max_dimension] * 2\n    else:\n      return [-1, -1]\n  raise ValueError(""Unknown image resizer type."")\n\n\ndef get_configs_from_pipeline_file(pipeline_config_path):\n  """"""Reads configuration from a pipeline_pb2.TrainEvalPipelineConfig.\n\n  Args:\n    pipeline_config_path: Path to pipeline_pb2.TrainEvalPipelineConfig text\n      proto.\n\n  Returns:\n    Dictionary of configuration objects. Keys are `model`, `train_config`,\n      `train_input_config`, `eval_config`, `eval_input_config`. Value are the\n      corresponding config objects.\n  """"""\n  pipeline_config = pipeline_pb2.TrainEvalPipelineConfig()\n  with tf.gfile.GFile(pipeline_config_path, ""r"") as f:\n    proto_str = f.read()\n    text_format.Merge(proto_str, pipeline_config)\n\n  configs = {}\n  configs[""model""] = pipeline_config.model\n  configs[""train_config""] = pipeline_config.train_config\n  configs[""train_input_config""] = pipeline_config.train_input_reader\n  configs[""eval_config""] = pipeline_config.eval_config\n  configs[""eval_input_config""] = pipeline_config.eval_input_reader\n\n  return configs\n\n\ndef create_pipeline_proto_from_configs(configs):\n  """"""Creates a pipeline_pb2.TrainEvalPipelineConfig from configs dictionary.\n\n  This function nearly performs the inverse operation of\n  get_configs_from_pipeline_file(). Instead of returning a file path, it returns\n  a `TrainEvalPipelineConfig` object.\n\n  Args:\n    configs: Dictionary of configs. See get_configs_from_pipeline_file().\n\n  Returns:\n    A fully populated pipeline_pb2.TrainEvalPipelineConfig.\n  """"""\n  pipeline_config = pipeline_pb2.TrainEvalPipelineConfig()\n  pipeline_config.model.CopyFrom(configs[""model""])\n  pipeline_config.train_config.CopyFrom(configs[""train_config""])\n  pipeline_config.train_input_reader.CopyFrom(configs[""train_input_config""])\n  pipeline_config.eval_config.CopyFrom(configs[""eval_config""])\n  pipeline_config.eval_input_reader.CopyFrom(configs[""eval_input_config""])\n  return pipeline_config\n\n\ndef get_configs_from_multiple_files(model_config_path="""",\n                                    train_config_path="""",\n                                    train_input_config_path="""",\n                                    eval_config_path="""",\n                                    eval_input_config_path=""""):\n  """"""Reads training configuration from multiple config files.\n\n  Args:\n    model_config_path: Path to model_pb2.DetectionModel.\n    train_config_path: Path to train_pb2.TrainConfig.\n    train_input_config_path: Path to input_reader_pb2.InputReader.\n    eval_config_path: Path to eval_pb2.EvalConfig.\n    eval_input_config_path: Path to input_reader_pb2.InputReader.\n\n  Returns:\n    Dictionary of configuration objects. Keys are `model`, `train_config`,\n      `train_input_config`, `eval_config`, `eval_input_config`. Key/Values are\n        returned only for valid (non-empty) strings.\n  """"""\n  configs = {}\n  if model_config_path:\n    model_config = model_pb2.DetectionModel()\n    with tf.gfile.GFile(model_config_path, ""r"") as f:\n      text_format.Merge(f.read(), model_config)\n      configs[""model""] = model_config\n\n  if train_config_path:\n    train_config = train_pb2.TrainConfig()\n    with tf.gfile.GFile(train_config_path, ""r"") as f:\n      text_format.Merge(f.read(), train_config)\n      configs[""train_config""] = train_config\n\n  if train_input_config_path:\n    train_input_config = input_reader_pb2.InputReader()\n    with tf.gfile.GFile(train_input_config_path, ""r"") as f:\n      text_format.Merge(f.read(), train_input_config)\n      configs[""train_input_config""] = train_input_config\n\n  if eval_config_path:\n    eval_config = eval_pb2.EvalConfig()\n    with tf.gfile.GFile(eval_config_path, ""r"") as f:\n      text_format.Merge(f.read(), eval_config)\n      configs[""eval_config""] = eval_config\n\n  if eval_input_config_path:\n    eval_input_config = input_reader_pb2.InputReader()\n    with tf.gfile.GFile(eval_input_config_path, ""r"") as f:\n      text_format.Merge(f.read(), eval_input_config)\n      configs[""eval_input_config""] = eval_input_config\n\n  return configs\n\n\ndef get_number_of_classes(model_config):\n  """"""Returns the number of classes for a detection model.\n\n  Args:\n    model_config: A model_pb2.DetectionModel.\n\n  Returns:\n    Number of classes.\n\n  Raises:\n    ValueError: If the model type is not recognized.\n  """"""\n  meta_architecture = model_config.WhichOneof(""model"")\n  if meta_architecture == ""faster_rcnn"":\n    return model_config.faster_rcnn.num_classes\n  if meta_architecture == ""ssd"":\n    return model_config.ssd.num_classes\n\n  raise ValueError(""Expected the model to be one of \'faster_rcnn\' or \'ssd\'."")\n\n\ndef get_optimizer_type(train_config):\n  """"""Returns the optimizer type for training.\n\n  Args:\n    train_config: A train_pb2.TrainConfig.\n\n  Returns:\n    The type of the optimizer\n  """"""\n  return train_config.optimizer.WhichOneof(""optimizer"")\n\n\ndef get_learning_rate_type(optimizer_config):\n  """"""Returns the learning rate type for training.\n\n  Args:\n    optimizer_config: An optimizer_pb2.Optimizer.\n\n  Returns:\n    The type of the learning rate.\n  """"""\n  return optimizer_config.learning_rate.WhichOneof(""learning_rate"")\n\n\ndef merge_external_params_with_configs(configs, hparams=None, **kwargs):\n  """"""Updates `configs` dictionary based on supplied parameters.\n\n  This utility is for modifying specific fields in the object detection configs.\n  Say that one would like to experiment with different learning rates, momentum\n  values, or batch sizes. Rather than creating a new config text file for each\n  experiment, one can use a single base config file, and update particular\n  values.\n\n  Args:\n    configs: Dictionary of configuration objects. See outputs from\n      get_configs_from_pipeline_file() or get_configs_from_multiple_files().\n    hparams: A `HParams`.\n    **kwargs: Extra keyword arguments that are treated the same way as\n      attribute/value pairs in `hparams`. Note that hyperparameters with the\n      same names will override keyword arguments.\n\n  Returns:\n    `configs` dictionary.\n  """"""\n\n  if hparams:\n    kwargs.update(hparams.values())\n  for key, value in kwargs.items():\n    # pylint: disable=g-explicit-bool-comparison\n    if value == """" or value is None:\n      continue\n    # pylint: enable=g-explicit-bool-comparison\n    if key == ""learning_rate"":\n      _update_initial_learning_rate(configs, value)\n      tf.logging.info(""Overwriting learning rate: %f"", value)\n    if key == ""batch_size"":\n      _update_batch_size(configs, value)\n      tf.logging.info(""Overwriting batch size: %d"", value)\n    if key == ""momentum_optimizer_value"":\n      _update_momentum_optimizer_value(configs, value)\n      tf.logging.info(""Overwriting momentum optimizer value: %f"", value)\n    if key == ""classification_localization_weight_ratio"":\n      # Localization weight is fixed to 1.0.\n      _update_classification_localization_weight_ratio(configs, value)\n    if key == ""focal_loss_gamma"":\n      _update_focal_loss_gamma(configs, value)\n    if key == ""focal_loss_alpha"":\n      _update_focal_loss_alpha(configs, value)\n    if key == ""train_steps"":\n      _update_train_steps(configs, value)\n      tf.logging.info(""Overwriting train steps: %d"", value)\n    if key == ""eval_steps"":\n      _update_eval_steps(configs, value)\n      tf.logging.info(""Overwriting eval steps: %d"", value)\n    if key == ""train_input_path"":\n      _update_input_path(configs[""train_input_config""], value)\n      tf.logging.info(""Overwriting train input path: %s"", value)\n    if key == ""eval_input_path"":\n      _update_input_path(configs[""eval_input_config""], value)\n      tf.logging.info(""Overwriting eval input path: %s"", value)\n    if key == ""label_map_path"":\n      _update_label_map_path(configs, value)\n      tf.logging.info(""Overwriting label map path: %s"", value)\n    if key == ""mask_type"":\n      _update_mask_type(configs, value)\n      tf.logging.info(""Overwritten mask type: %s"", value)\n  return configs\n\n\ndef _update_initial_learning_rate(configs, learning_rate):\n  """"""Updates `configs` to reflect the new initial learning rate.\n\n  This function updates the initial learning rate. For learning rate schedules,\n  all other defined learning rates in the pipeline config are scaled to maintain\n  their same ratio with the initial learning rate.\n  The configs dictionary is updated in place, and hence not returned.\n\n  Args:\n    configs: Dictionary of configuration objects. See outputs from\n      get_configs_from_pipeline_file() or get_configs_from_multiple_files().\n    learning_rate: Initial learning rate for optimizer.\n\n  Raises:\n    TypeError: if optimizer type is not supported, or if learning rate type is\n      not supported.\n  """"""\n\n  optimizer_type = get_optimizer_type(configs[""train_config""])\n  if optimizer_type == ""rms_prop_optimizer"":\n    optimizer_config = configs[""train_config""].optimizer.rms_prop_optimizer\n  elif optimizer_type == ""momentum_optimizer"":\n    optimizer_config = configs[""train_config""].optimizer.momentum_optimizer\n  elif optimizer_type == ""adam_optimizer"":\n    optimizer_config = configs[""train_config""].optimizer.adam_optimizer\n  else:\n    raise TypeError(""Optimizer %s is not supported."" % optimizer_type)\n\n  learning_rate_type = get_learning_rate_type(optimizer_config)\n  if learning_rate_type == ""constant_learning_rate"":\n    constant_lr = optimizer_config.learning_rate.constant_learning_rate\n    constant_lr.learning_rate = learning_rate\n  elif learning_rate_type == ""exponential_decay_learning_rate"":\n    exponential_lr = (\n        optimizer_config.learning_rate.exponential_decay_learning_rate)\n    exponential_lr.initial_learning_rate = learning_rate\n  elif learning_rate_type == ""manual_step_learning_rate"":\n    manual_lr = optimizer_config.learning_rate.manual_step_learning_rate\n    original_learning_rate = manual_lr.initial_learning_rate\n    learning_rate_scaling = float(learning_rate) / original_learning_rate\n    manual_lr.initial_learning_rate = learning_rate\n    for schedule in manual_lr.schedule:\n      schedule.learning_rate *= learning_rate_scaling\n  elif learning_rate_type == ""cosine_decay_learning_rate"":\n    cosine_lr = optimizer_config.learning_rate.cosine_decay_learning_rate\n    learning_rate_base = cosine_lr.learning_rate_base\n    warmup_learning_rate = cosine_lr.warmup_learning_rate\n    warmup_scale_factor = warmup_learning_rate / learning_rate_base\n    cosine_lr.learning_rate_base = learning_rate\n    cosine_lr.warmup_learning_rate = warmup_scale_factor * learning_rate\n  else:\n    raise TypeError(""Learning rate %s is not supported."" % learning_rate_type)\n\n\ndef _update_batch_size(configs, batch_size):\n  """"""Updates `configs` to reflect the new training batch size.\n\n  The configs dictionary is updated in place, and hence not returned.\n\n  Args:\n    configs: Dictionary of configuration objects. See outputs from\n      get_configs_from_pipeline_file() or get_configs_from_multiple_files().\n    batch_size: Batch size to use for training (Ideally a power of 2). Inputs\n      are rounded, and capped to be 1 or greater.\n  """"""\n  configs[""train_config""].batch_size = max(1, int(round(batch_size)))\n\n\ndef _update_momentum_optimizer_value(configs, momentum):\n  """"""Updates `configs` to reflect the new momentum value.\n\n  Momentum is only supported for RMSPropOptimizer and MomentumOptimizer. For any\n  other optimizer, no changes take place. The configs dictionary is updated in\n  place, and hence not returned.\n\n  Args:\n    configs: Dictionary of configuration objects. See outputs from\n      get_configs_from_pipeline_file() or get_configs_from_multiple_files().\n    momentum: New momentum value. Values are clipped at 0.0 and 1.0.\n\n  Raises:\n    TypeError: If the optimizer type is not `rms_prop_optimizer` or\n    `momentum_optimizer`.\n  """"""\n  optimizer_type = get_optimizer_type(configs[""train_config""])\n  if optimizer_type == ""rms_prop_optimizer"":\n    optimizer_config = configs[""train_config""].optimizer.rms_prop_optimizer\n  elif optimizer_type == ""momentum_optimizer"":\n    optimizer_config = configs[""train_config""].optimizer.momentum_optimizer\n  else:\n    raise TypeError(""Optimizer type must be one of `rms_prop_optimizer` or ""\n                    ""`momentum_optimizer`."")\n\n  optimizer_config.momentum_optimizer_value = min(max(0.0, momentum), 1.0)\n\n\ndef _update_classification_localization_weight_ratio(configs, ratio):\n  """"""Updates the classification/localization weight loss ratio.\n\n  Detection models usually define a loss weight for both classification and\n  objectness. This function updates the weights such that the ratio between\n  classification weight to localization weight is the ratio provided.\n  Arbitrarily, localization weight is set to 1.0.\n\n  Note that in the case of Faster R-CNN, this same ratio is applied to the first\n  stage objectness loss weight relative to localization loss weight.\n\n  The configs dictionary is updated in place, and hence not returned.\n\n  Args:\n    configs: Dictionary of configuration objects. See outputs from\n      get_configs_from_pipeline_file() or get_configs_from_multiple_files().\n    ratio: Desired ratio of classification (and/or objectness) loss weight to\n      localization loss weight.\n  """"""\n  meta_architecture = configs[""model""].WhichOneof(""model"")\n  if meta_architecture == ""faster_rcnn"":\n    model = configs[""model""].faster_rcnn\n    model.first_stage_localization_loss_weight = 1.0\n    model.first_stage_objectness_loss_weight = ratio\n    model.second_stage_localization_loss_weight = 1.0\n    model.second_stage_classification_loss_weight = ratio\n  if meta_architecture == ""ssd"":\n    model = configs[""model""].ssd\n    model.loss.localization_weight = 1.0\n    model.loss.classification_weight = ratio\n\n\ndef _get_classification_loss(model_config):\n  """"""Returns the classification loss for a model.""""""\n  meta_architecture = model_config.WhichOneof(""model"")\n  if meta_architecture == ""faster_rcnn"":\n    model = model_config.faster_rcnn\n    classification_loss = model.second_stage_classification_loss\n  if meta_architecture == ""ssd"":\n    model = model_config.ssd\n    classification_loss = model.loss.classification_loss\n  else:\n    raise TypeError(""Did not recognize the model architecture."")\n  return classification_loss\n\n\ndef _update_focal_loss_gamma(configs, gamma):\n  """"""Updates the gamma value for a sigmoid focal loss.\n\n  The configs dictionary is updated in place, and hence not returned.\n\n  Args:\n    configs: Dictionary of configuration objects. See outputs from\n      get_configs_from_pipeline_file() or get_configs_from_multiple_files().\n    gamma: Exponent term in focal loss.\n\n  Raises:\n    TypeError: If the classification loss is not `weighted_sigmoid_focal`.\n  """"""\n  classification_loss = _get_classification_loss(configs[""model""])\n  classification_loss_type = classification_loss.WhichOneof(\n      ""classification_loss"")\n  if classification_loss_type != ""weighted_sigmoid_focal"":\n    raise TypeError(""Classification loss must be `weighted_sigmoid_focal`."")\n  classification_loss.weighted_sigmoid_focal.gamma = gamma\n\n\ndef _update_focal_loss_alpha(configs, alpha):\n  """"""Updates the alpha value for a sigmoid focal loss.\n\n  The configs dictionary is updated in place, and hence not returned.\n\n  Args:\n    configs: Dictionary of configuration objects. See outputs from\n      get_configs_from_pipeline_file() or get_configs_from_multiple_files().\n    alpha: Class weight multiplier for sigmoid loss.\n\n  Raises:\n    TypeError: If the classification loss is not `weighted_sigmoid_focal`.\n  """"""\n  classification_loss = _get_classification_loss(configs[""model""])\n  classification_loss_type = classification_loss.WhichOneof(\n      ""classification_loss"")\n  if classification_loss_type != ""weighted_sigmoid_focal"":\n    raise TypeError(""Classification loss must be `weighted_sigmoid_focal`."")\n  classification_loss.weighted_sigmoid_focal.alpha = alpha\n\n\ndef _update_train_steps(configs, train_steps):\n  """"""Updates `configs` to reflect new number of training steps.""""""\n  configs[""train_config""].num_steps = int(train_steps)\n\n\ndef _update_eval_steps(configs, eval_steps):\n  """"""Updates `configs` to reflect new number of eval steps per evaluation.""""""\n  configs[""eval_config""].num_examples = int(eval_steps)\n\n\ndef _update_input_path(input_config, input_path):\n  """"""Updates input configuration to reflect a new input path.\n\n  The input_config object is updated in place, and hence not returned.\n\n  Args:\n    input_config: A input_reader_pb2.InputReader.\n    input_path: A path to data or list of paths.\n\n  Raises:\n    TypeError: if input reader type is not `tf_record_input_reader`.\n  """"""\n  input_reader_type = input_config.WhichOneof(""input_reader"")\n  if input_reader_type == ""tf_record_input_reader"":\n    input_config.tf_record_input_reader.ClearField(""input_path"")\n    if isinstance(input_path, list):\n      input_config.tf_record_input_reader.input_path.extend(input_path)\n    else:\n      input_config.tf_record_input_reader.input_path.append(input_path)\n  else:\n    raise TypeError(""Input reader type must be `tf_record_input_reader`."")\n\n\ndef _update_label_map_path(configs, label_map_path):\n  """"""Updates the label map path for both train and eval input readers.\n\n  The configs dictionary is updated in place, and hence not returned.\n\n  Args:\n    configs: Dictionary of configuration objects. See outputs from\n      get_configs_from_pipeline_file() or get_configs_from_multiple_files().\n    label_map_path: New path to `StringIntLabelMap` pbtxt file.\n  """"""\n  configs[""train_input_config""].label_map_path = label_map_path\n  configs[""eval_input_config""].label_map_path = label_map_path\n\n\ndef _update_mask_type(configs, mask_type):\n  """"""Updates the mask type for both train and eval input readers.\n\n  The configs dictionary is updated in place, and hence not returned.\n\n  Args:\n    configs: Dictionary of configuration objects. See outputs from\n      get_configs_from_pipeline_file() or get_configs_from_multiple_files().\n    mask_type: A string name representing a value of\n      input_reader_pb2.InstanceMaskType\n  """"""\n  configs[""train_input_config""].mask_type = mask_type\n  configs[""eval_input_config""].mask_type = mask_type\n'"
src/object_detection/utils/config_util_test.py,9,"b'# Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Tests for object_detection.utils.config_util.""""""\n\nimport os\n\nimport tensorflow as tf\n\nfrom google.protobuf import text_format\n\nfrom object_detection.protos import eval_pb2\nfrom object_detection.protos import image_resizer_pb2\nfrom object_detection.protos import input_reader_pb2\nfrom object_detection.protos import model_pb2\nfrom object_detection.protos import pipeline_pb2\nfrom object_detection.protos import train_pb2\nfrom object_detection.utils import config_util\n\n\ndef _write_config(config, config_path):\n  """"""Writes a config object to disk.""""""\n  config_text = text_format.MessageToString(config)\n  with tf.gfile.Open(config_path, ""wb"") as f:\n    f.write(config_text)\n\n\ndef _update_optimizer_with_constant_learning_rate(optimizer, learning_rate):\n  """"""Adds a new constant learning rate.""""""\n  constant_lr = optimizer.learning_rate.constant_learning_rate\n  constant_lr.learning_rate = learning_rate\n\n\ndef _update_optimizer_with_exponential_decay_learning_rate(\n    optimizer, learning_rate):\n  """"""Adds a new exponential decay learning rate.""""""\n  exponential_lr = optimizer.learning_rate.exponential_decay_learning_rate\n  exponential_lr.initial_learning_rate = learning_rate\n\n\ndef _update_optimizer_with_manual_step_learning_rate(\n    optimizer, initial_learning_rate, learning_rate_scaling):\n  """"""Adds a learning rate schedule.""""""\n  manual_lr = optimizer.learning_rate.manual_step_learning_rate\n  manual_lr.initial_learning_rate = initial_learning_rate\n  for i in range(3):\n    schedule = manual_lr.schedule.add()\n    schedule.learning_rate = initial_learning_rate * learning_rate_scaling**i\n\n\ndef _update_optimizer_with_cosine_decay_learning_rate(\n    optimizer, learning_rate, warmup_learning_rate):\n  """"""Adds a new cosine decay learning rate.""""""\n  cosine_lr = optimizer.learning_rate.cosine_decay_learning_rate\n  cosine_lr.learning_rate_base = learning_rate\n  cosine_lr.warmup_learning_rate = warmup_learning_rate\n\n\nclass ConfigUtilTest(tf.test.TestCase):\n\n  def test_get_configs_from_pipeline_file(self):\n    """"""Test that proto configs can be read from pipeline config file.""""""\n    pipeline_config_path = os.path.join(self.get_temp_dir(), ""pipeline.config"")\n\n    pipeline_config = pipeline_pb2.TrainEvalPipelineConfig()\n    pipeline_config.model.faster_rcnn.num_classes = 10\n    pipeline_config.train_config.batch_size = 32\n    pipeline_config.train_input_reader.label_map_path = ""path/to/label_map""\n    pipeline_config.eval_config.num_examples = 20\n    pipeline_config.eval_input_reader.queue_capacity = 100\n\n    _write_config(pipeline_config, pipeline_config_path)\n\n    configs = config_util.get_configs_from_pipeline_file(pipeline_config_path)\n    self.assertProtoEquals(pipeline_config.model, configs[""model""])\n    self.assertProtoEquals(pipeline_config.train_config,\n                           configs[""train_config""])\n    self.assertProtoEquals(pipeline_config.train_input_reader,\n                           configs[""train_input_config""])\n    self.assertProtoEquals(pipeline_config.eval_config,\n                           configs[""eval_config""])\n    self.assertProtoEquals(pipeline_config.eval_input_reader,\n                           configs[""eval_input_config""])\n\n  def test_create_pipeline_proto_from_configs(self):\n    """"""Tests that proto can be reconstructed from configs dictionary.""""""\n    pipeline_config_path = os.path.join(self.get_temp_dir(), ""pipeline.config"")\n\n    pipeline_config = pipeline_pb2.TrainEvalPipelineConfig()\n    pipeline_config.model.faster_rcnn.num_classes = 10\n    pipeline_config.train_config.batch_size = 32\n    pipeline_config.train_input_reader.label_map_path = ""path/to/label_map""\n    pipeline_config.eval_config.num_examples = 20\n    pipeline_config.eval_input_reader.queue_capacity = 100\n    _write_config(pipeline_config, pipeline_config_path)\n\n    configs = config_util.get_configs_from_pipeline_file(pipeline_config_path)\n    pipeline_config_reconstructed = (\n        config_util.create_pipeline_proto_from_configs(configs))\n    self.assertEqual(pipeline_config, pipeline_config_reconstructed)\n\n  def test_get_configs_from_multiple_files(self):\n    """"""Tests that proto configs can be read from multiple files.""""""\n    temp_dir = self.get_temp_dir()\n\n    # Write model config file.\n    model_config_path = os.path.join(temp_dir, ""model.config"")\n    model = model_pb2.DetectionModel()\n    model.faster_rcnn.num_classes = 10\n    _write_config(model, model_config_path)\n\n    # Write train config file.\n    train_config_path = os.path.join(temp_dir, ""train.config"")\n    train_config = train_config = train_pb2.TrainConfig()\n    train_config.batch_size = 32\n    _write_config(train_config, train_config_path)\n\n    # Write train input config file.\n    train_input_config_path = os.path.join(temp_dir, ""train_input.config"")\n    train_input_config = input_reader_pb2.InputReader()\n    train_input_config.label_map_path = ""path/to/label_map""\n    _write_config(train_input_config, train_input_config_path)\n\n    # Write eval config file.\n    eval_config_path = os.path.join(temp_dir, ""eval.config"")\n    eval_config = eval_pb2.EvalConfig()\n    eval_config.num_examples = 20\n    _write_config(eval_config, eval_config_path)\n\n    # Write eval input config file.\n    eval_input_config_path = os.path.join(temp_dir, ""eval_input.config"")\n    eval_input_config = input_reader_pb2.InputReader()\n    eval_input_config.label_map_path = ""path/to/another/label_map""\n    _write_config(eval_input_config, eval_input_config_path)\n\n    configs = config_util.get_configs_from_multiple_files(\n        model_config_path=model_config_path,\n        train_config_path=train_config_path,\n        train_input_config_path=train_input_config_path,\n        eval_config_path=eval_config_path,\n        eval_input_config_path=eval_input_config_path)\n    self.assertProtoEquals(model, configs[""model""])\n    self.assertProtoEquals(train_config, configs[""train_config""])\n    self.assertProtoEquals(train_input_config,\n                           configs[""train_input_config""])\n    self.assertProtoEquals(eval_config, configs[""eval_config""])\n    self.assertProtoEquals(eval_input_config,\n                           configs[""eval_input_config""])\n\n  def _assertOptimizerWithNewLearningRate(self, optimizer_name):\n    """"""Asserts successful updating of all learning rate schemes.""""""\n    original_learning_rate = 0.7\n    learning_rate_scaling = 0.1\n    warmup_learning_rate = 0.07\n    hparams = tf.contrib.training.HParams(learning_rate=0.15)\n    pipeline_config_path = os.path.join(self.get_temp_dir(), ""pipeline.config"")\n\n    # Constant learning rate.\n    pipeline_config = pipeline_pb2.TrainEvalPipelineConfig()\n    optimizer = getattr(pipeline_config.train_config.optimizer, optimizer_name)\n    _update_optimizer_with_constant_learning_rate(optimizer,\n                                                  original_learning_rate)\n    _write_config(pipeline_config, pipeline_config_path)\n\n    configs = config_util.get_configs_from_pipeline_file(pipeline_config_path)\n    configs = config_util.merge_external_params_with_configs(configs, hparams)\n    optimizer = getattr(configs[""train_config""].optimizer, optimizer_name)\n    constant_lr = optimizer.learning_rate.constant_learning_rate\n    self.assertAlmostEqual(hparams.learning_rate, constant_lr.learning_rate)\n\n    # Exponential decay learning rate.\n    pipeline_config = pipeline_pb2.TrainEvalPipelineConfig()\n    optimizer = getattr(pipeline_config.train_config.optimizer, optimizer_name)\n    _update_optimizer_with_exponential_decay_learning_rate(\n        optimizer, original_learning_rate)\n    _write_config(pipeline_config, pipeline_config_path)\n\n    configs = config_util.get_configs_from_pipeline_file(pipeline_config_path)\n    configs = config_util.merge_external_params_with_configs(configs, hparams)\n    optimizer = getattr(configs[""train_config""].optimizer, optimizer_name)\n    exponential_lr = optimizer.learning_rate.exponential_decay_learning_rate\n    self.assertAlmostEqual(hparams.learning_rate,\n                           exponential_lr.initial_learning_rate)\n\n    # Manual step learning rate.\n    pipeline_config = pipeline_pb2.TrainEvalPipelineConfig()\n    optimizer = getattr(pipeline_config.train_config.optimizer, optimizer_name)\n    _update_optimizer_with_manual_step_learning_rate(\n        optimizer, original_learning_rate, learning_rate_scaling)\n    _write_config(pipeline_config, pipeline_config_path)\n\n    configs = config_util.get_configs_from_pipeline_file(pipeline_config_path)\n    configs = config_util.merge_external_params_with_configs(configs, hparams)\n    optimizer = getattr(configs[""train_config""].optimizer, optimizer_name)\n    manual_lr = optimizer.learning_rate.manual_step_learning_rate\n    self.assertAlmostEqual(hparams.learning_rate,\n                           manual_lr.initial_learning_rate)\n    for i, schedule in enumerate(manual_lr.schedule):\n      self.assertAlmostEqual(hparams.learning_rate * learning_rate_scaling**i,\n                             schedule.learning_rate)\n\n    # Cosine decay learning rate.\n    pipeline_config = pipeline_pb2.TrainEvalPipelineConfig()\n    optimizer = getattr(pipeline_config.train_config.optimizer, optimizer_name)\n    _update_optimizer_with_cosine_decay_learning_rate(optimizer,\n                                                      original_learning_rate,\n                                                      warmup_learning_rate)\n    _write_config(pipeline_config, pipeline_config_path)\n\n    configs = config_util.get_configs_from_pipeline_file(pipeline_config_path)\n    configs = config_util.merge_external_params_with_configs(configs, hparams)\n    optimizer = getattr(configs[""train_config""].optimizer, optimizer_name)\n    cosine_lr = optimizer.learning_rate.cosine_decay_learning_rate\n\n    self.assertAlmostEqual(hparams.learning_rate, cosine_lr.learning_rate_base)\n    warmup_scale_factor = warmup_learning_rate / original_learning_rate\n    self.assertAlmostEqual(hparams.learning_rate * warmup_scale_factor,\n                           cosine_lr.warmup_learning_rate)\n\n  def testRMSPropWithNewLearingRate(self):\n    """"""Tests new learning rates for RMSProp Optimizer.""""""\n    self._assertOptimizerWithNewLearningRate(""rms_prop_optimizer"")\n\n  def testMomentumOptimizerWithNewLearningRate(self):\n    """"""Tests new learning rates for Momentum Optimizer.""""""\n    self._assertOptimizerWithNewLearningRate(""momentum_optimizer"")\n\n  def testAdamOptimizerWithNewLearningRate(self):\n    """"""Tests new learning rates for Adam Optimizer.""""""\n    self._assertOptimizerWithNewLearningRate(""adam_optimizer"")\n\n  def testNewBatchSize(self):\n    """"""Tests that batch size is updated appropriately.""""""\n    original_batch_size = 2\n    hparams = tf.contrib.training.HParams(batch_size=16)\n    pipeline_config_path = os.path.join(self.get_temp_dir(), ""pipeline.config"")\n\n    pipeline_config = pipeline_pb2.TrainEvalPipelineConfig()\n    pipeline_config.train_config.batch_size = original_batch_size\n    _write_config(pipeline_config, pipeline_config_path)\n\n    configs = config_util.get_configs_from_pipeline_file(pipeline_config_path)\n    configs = config_util.merge_external_params_with_configs(configs, hparams)\n    new_batch_size = configs[""train_config""].batch_size\n    self.assertEqual(16, new_batch_size)\n\n  def testNewBatchSizeWithClipping(self):\n    """"""Tests that batch size is clipped to 1 from below.""""""\n    original_batch_size = 2\n    hparams = tf.contrib.training.HParams(batch_size=0.5)\n    pipeline_config_path = os.path.join(self.get_temp_dir(), ""pipeline.config"")\n\n    pipeline_config = pipeline_pb2.TrainEvalPipelineConfig()\n    pipeline_config.train_config.batch_size = original_batch_size\n    _write_config(pipeline_config, pipeline_config_path)\n\n    configs = config_util.get_configs_from_pipeline_file(pipeline_config_path)\n    configs = config_util.merge_external_params_with_configs(configs, hparams)\n    new_batch_size = configs[""train_config""].batch_size\n    self.assertEqual(1, new_batch_size)  # Clipped to 1.0.\n\n  def testNewMomentumOptimizerValue(self):\n    """"""Tests that new momentum value is updated appropriately.""""""\n    original_momentum_value = 0.4\n    hparams = tf.contrib.training.HParams(momentum_optimizer_value=1.1)\n    pipeline_config_path = os.path.join(self.get_temp_dir(), ""pipeline.config"")\n\n    pipeline_config = pipeline_pb2.TrainEvalPipelineConfig()\n    optimizer_config = pipeline_config.train_config.optimizer.rms_prop_optimizer\n    optimizer_config.momentum_optimizer_value = original_momentum_value\n    _write_config(pipeline_config, pipeline_config_path)\n\n    configs = config_util.get_configs_from_pipeline_file(pipeline_config_path)\n    configs = config_util.merge_external_params_with_configs(configs, hparams)\n    optimizer_config = configs[""train_config""].optimizer.rms_prop_optimizer\n    new_momentum_value = optimizer_config.momentum_optimizer_value\n    self.assertAlmostEqual(1.0, new_momentum_value)  # Clipped to 1.0.\n\n  def testNewClassificationLocalizationWeightRatio(self):\n    """"""Tests that the loss weight ratio is updated appropriately.""""""\n    original_localization_weight = 0.1\n    original_classification_weight = 0.2\n    new_weight_ratio = 5.0\n    hparams = tf.contrib.training.HParams(\n        classification_localization_weight_ratio=new_weight_ratio)\n    pipeline_config_path = os.path.join(self.get_temp_dir(), ""pipeline.config"")\n\n    pipeline_config = pipeline_pb2.TrainEvalPipelineConfig()\n    pipeline_config.model.ssd.loss.localization_weight = (\n        original_localization_weight)\n    pipeline_config.model.ssd.loss.classification_weight = (\n        original_classification_weight)\n    _write_config(pipeline_config, pipeline_config_path)\n\n    configs = config_util.get_configs_from_pipeline_file(pipeline_config_path)\n    configs = config_util.merge_external_params_with_configs(configs, hparams)\n    loss = configs[""model""].ssd.loss\n    self.assertAlmostEqual(1.0, loss.localization_weight)\n    self.assertAlmostEqual(new_weight_ratio, loss.classification_weight)\n\n  def testNewFocalLossParameters(self):\n    """"""Tests that the loss weight ratio is updated appropriately.""""""\n    original_alpha = 1.0\n    original_gamma = 1.0\n    new_alpha = 0.3\n    new_gamma = 2.0\n    hparams = tf.contrib.training.HParams(\n        focal_loss_alpha=new_alpha, focal_loss_gamma=new_gamma)\n    pipeline_config_path = os.path.join(self.get_temp_dir(), ""pipeline.config"")\n\n    pipeline_config = pipeline_pb2.TrainEvalPipelineConfig()\n    classification_loss = pipeline_config.model.ssd.loss.classification_loss\n    classification_loss.weighted_sigmoid_focal.alpha = original_alpha\n    classification_loss.weighted_sigmoid_focal.gamma = original_gamma\n    _write_config(pipeline_config, pipeline_config_path)\n\n    configs = config_util.get_configs_from_pipeline_file(pipeline_config_path)\n    configs = config_util.merge_external_params_with_configs(configs, hparams)\n    classification_loss = configs[""model""].ssd.loss.classification_loss\n    self.assertAlmostEqual(new_alpha,\n                           classification_loss.weighted_sigmoid_focal.alpha)\n    self.assertAlmostEqual(new_gamma,\n                           classification_loss.weighted_sigmoid_focal.gamma)\n\n  def testMergingKeywordArguments(self):\n    """"""Tests that keyword arguments get merged as do hyperparameters.""""""\n    original_num_train_steps = 100\n    original_num_eval_steps = 5\n    desired_num_train_steps = 10\n    desired_num_eval_steps = 1\n    pipeline_config_path = os.path.join(self.get_temp_dir(), ""pipeline.config"")\n\n    pipeline_config = pipeline_pb2.TrainEvalPipelineConfig()\n    pipeline_config.train_config.num_steps = original_num_train_steps\n    pipeline_config.eval_config.num_examples = original_num_eval_steps\n    _write_config(pipeline_config, pipeline_config_path)\n\n    configs = config_util.get_configs_from_pipeline_file(pipeline_config_path)\n    configs = config_util.merge_external_params_with_configs(\n        configs,\n        train_steps=desired_num_train_steps,\n        eval_steps=desired_num_eval_steps)\n    train_steps = configs[""train_config""].num_steps\n    eval_steps = configs[""eval_config""].num_examples\n    self.assertEqual(desired_num_train_steps, train_steps)\n    self.assertEqual(desired_num_eval_steps, eval_steps)\n\n  def testGetNumberOfClasses(self):\n    """"""Tests that number of classes can be retrieved.""""""\n    pipeline_config_path = os.path.join(self.get_temp_dir(), ""pipeline.config"")\n    pipeline_config = pipeline_pb2.TrainEvalPipelineConfig()\n    pipeline_config.model.faster_rcnn.num_classes = 20\n    _write_config(pipeline_config, pipeline_config_path)\n\n    configs = config_util.get_configs_from_pipeline_file(pipeline_config_path)\n    number_of_classes = config_util.get_number_of_classes(configs[""model""])\n    self.assertEqual(20, number_of_classes)\n\n  def testNewTrainInputPath(self):\n    """"""Tests that train input path can be overwritten with single file.""""""\n    original_train_path = [""path/to/data""]\n    new_train_path = ""another/path/to/data""\n    pipeline_config_path = os.path.join(self.get_temp_dir(), ""pipeline.config"")\n\n    pipeline_config = pipeline_pb2.TrainEvalPipelineConfig()\n    reader_config = pipeline_config.train_input_reader.tf_record_input_reader\n    reader_config.input_path.extend(original_train_path)\n    _write_config(pipeline_config, pipeline_config_path)\n\n    configs = config_util.get_configs_from_pipeline_file(pipeline_config_path)\n    configs = config_util.merge_external_params_with_configs(\n        configs, train_input_path=new_train_path)\n    reader_config = configs[""train_input_config""].tf_record_input_reader\n    final_path = reader_config.input_path\n    self.assertEqual([new_train_path], final_path)\n\n  def testNewTrainInputPathList(self):\n    """"""Tests that train input path can be overwritten with multiple files.""""""\n    original_train_path = [""path/to/data""]\n    new_train_path = [""another/path/to/data"", ""yet/another/path/to/data""]\n    pipeline_config_path = os.path.join(self.get_temp_dir(), ""pipeline.config"")\n\n    pipeline_config = pipeline_pb2.TrainEvalPipelineConfig()\n    reader_config = pipeline_config.train_input_reader.tf_record_input_reader\n    reader_config.input_path.extend(original_train_path)\n    _write_config(pipeline_config, pipeline_config_path)\n\n    configs = config_util.get_configs_from_pipeline_file(pipeline_config_path)\n    configs = config_util.merge_external_params_with_configs(\n        configs, train_input_path=new_train_path)\n    reader_config = configs[""train_input_config""].tf_record_input_reader\n    final_path = reader_config.input_path\n    self.assertEqual(new_train_path, final_path)\n\n  def testNewLabelMapPath(self):\n    """"""Tests that label map path can be overwritten in input readers.""""""\n    original_label_map_path = ""path/to/original/label_map""\n    new_label_map_path = ""path//to/new/label_map""\n    pipeline_config_path = os.path.join(self.get_temp_dir(), ""pipeline.config"")\n\n    pipeline_config = pipeline_pb2.TrainEvalPipelineConfig()\n    train_input_reader = pipeline_config.train_input_reader\n    train_input_reader.label_map_path = original_label_map_path\n    eval_input_reader = pipeline_config.eval_input_reader\n    eval_input_reader.label_map_path = original_label_map_path\n    _write_config(pipeline_config, pipeline_config_path)\n\n    configs = config_util.get_configs_from_pipeline_file(pipeline_config_path)\n    configs = config_util.merge_external_params_with_configs(\n        configs, label_map_path=new_label_map_path)\n    self.assertEqual(new_label_map_path,\n                     configs[""train_input_config""].label_map_path)\n    self.assertEqual(new_label_map_path,\n                     configs[""eval_input_config""].label_map_path)\n\n  def testDontOverwriteEmptyLabelMapPath(self):\n    """"""Tests that label map path will not by overwritten with empty string.""""""\n    original_label_map_path = ""path/to/original/label_map""\n    new_label_map_path = """"\n    pipeline_config_path = os.path.join(self.get_temp_dir(), ""pipeline.config"")\n\n    pipeline_config = pipeline_pb2.TrainEvalPipelineConfig()\n    train_input_reader = pipeline_config.train_input_reader\n    train_input_reader.label_map_path = original_label_map_path\n    eval_input_reader = pipeline_config.eval_input_reader\n    eval_input_reader.label_map_path = original_label_map_path\n    _write_config(pipeline_config, pipeline_config_path)\n\n    configs = config_util.get_configs_from_pipeline_file(pipeline_config_path)\n    configs = config_util.merge_external_params_with_configs(\n        configs, label_map_path=new_label_map_path)\n    self.assertEqual(original_label_map_path,\n                     configs[""train_input_config""].label_map_path)\n    self.assertEqual(original_label_map_path,\n                     configs[""eval_input_config""].label_map_path)\n\n  def testNewMaskType(self):\n    """"""Tests that mask type can be overwritten in input readers.""""""\n    original_mask_type = input_reader_pb2.NUMERICAL_MASKS\n    new_mask_type = input_reader_pb2.PNG_MASKS\n    pipeline_config_path = os.path.join(self.get_temp_dir(), ""pipeline.config"")\n\n    pipeline_config = pipeline_pb2.TrainEvalPipelineConfig()\n    train_input_reader = pipeline_config.train_input_reader\n    train_input_reader.mask_type = original_mask_type\n    eval_input_reader = pipeline_config.eval_input_reader\n    eval_input_reader.mask_type = original_mask_type\n    _write_config(pipeline_config, pipeline_config_path)\n\n    configs = config_util.get_configs_from_pipeline_file(pipeline_config_path)\n    configs = config_util.merge_external_params_with_configs(\n        configs, mask_type=new_mask_type)\n    self.assertEqual(new_mask_type, configs[""train_input_config""].mask_type)\n    self.assertEqual(new_mask_type, configs[""eval_input_config""].mask_type)\n\n  def  test_get_image_resizer_config(self):\n    """"""Tests that number of classes can be retrieved.""""""\n    model_config = model_pb2.DetectionModel()\n    model_config.faster_rcnn.image_resizer.fixed_shape_resizer.height = 100\n    model_config.faster_rcnn.image_resizer.fixed_shape_resizer.width = 300\n    image_resizer_config = config_util.get_image_resizer_config(model_config)\n    self.assertEqual(image_resizer_config.fixed_shape_resizer.height, 100)\n    self.assertEqual(image_resizer_config.fixed_shape_resizer.width, 300)\n\n  def test_get_spatial_image_size_from_fixed_shape_resizer_config(self):\n    image_resizer_config = image_resizer_pb2.ImageResizer()\n    image_resizer_config.fixed_shape_resizer.height = 100\n    image_resizer_config.fixed_shape_resizer.width = 200\n    image_shape = config_util.get_spatial_image_size(image_resizer_config)\n    self.assertAllEqual(image_shape, [100, 200])\n\n  def test_get_spatial_image_size_from_aspect_preserving_resizer_config(self):\n    image_resizer_config = image_resizer_pb2.ImageResizer()\n    image_resizer_config.keep_aspect_ratio_resizer.min_dimension = 100\n    image_resizer_config.keep_aspect_ratio_resizer.max_dimension = 600\n    image_resizer_config.keep_aspect_ratio_resizer.pad_to_max_dimension = True\n    image_shape = config_util.get_spatial_image_size(image_resizer_config)\n    self.assertAllEqual(image_shape, [600, 600])\n\n  def test_get_spatial_image_size_from_aspect_preserving_resizer_dynamic(self):\n    image_resizer_config = image_resizer_pb2.ImageResizer()\n    image_resizer_config.keep_aspect_ratio_resizer.min_dimension = 100\n    image_resizer_config.keep_aspect_ratio_resizer.max_dimension = 600\n    image_shape = config_util.get_spatial_image_size(image_resizer_config)\n    self.assertAllEqual(image_shape, [-1, -1])\n\n\nif __name__ == ""__main__"":\n  tf.test.main()\n'"
src/object_detection/utils/dataset_util.py,15,"b'# Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n""""""Utility functions for creating TFRecord data sets.""""""\n\nimport tensorflow as tf\n\n\ndef int64_feature(value):\n  return tf.train.Feature(int64_list=tf.train.Int64List(value=[value]))\n\n\ndef int64_list_feature(value):\n  return tf.train.Feature(int64_list=tf.train.Int64List(value=value))\n\n\ndef bytes_feature(value):\n  return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))\n\n\ndef bytes_list_feature(value):\n  return tf.train.Feature(bytes_list=tf.train.BytesList(value=value))\n\n\ndef float_list_feature(value):\n  return tf.train.Feature(float_list=tf.train.FloatList(value=value))\n\n\ndef read_examples_list(path):\n  """"""Read list of training or validation examples.\n\n  The file is assumed to contain a single example per line where the first\n  token in the line is an identifier that allows us to find the image and\n  annotation xml for that example.\n\n  For example, the line:\n  xyz 3\n  would allow us to find files xyz.jpg and xyz.xml (the 3 would be ignored).\n\n  Args:\n    path: absolute path to examples list file.\n\n  Returns:\n    list of example identifiers (strings).\n  """"""\n  with tf.gfile.GFile(path) as fid:\n    lines = fid.readlines()\n  return [line.strip().split(\' \')[0] for line in lines]\n\n\ndef recursive_parse_xml_to_dict(xml):\n  """"""Recursively parses XML contents to python dict.\n\n  We assume that `object` tags are the only ones that can appear\n  multiple times at the same level of a tree.\n\n  Args:\n    xml: xml tree obtained by parsing XML file contents using lxml.etree\n\n  Returns:\n    Python dictionary holding XML contents.\n  """"""\n  if not xml:\n    return {xml.tag: xml.text}\n  result = {}\n  for child in xml:\n    child_result = recursive_parse_xml_to_dict(child)\n    if child.tag != \'object\':\n      result[child.tag] = child_result[child.tag]\n    else:\n      if child.tag not in result:\n        result[child.tag] = []\n      result[child.tag].append(child_result[child.tag])\n  return {xml.tag: result}\n\n\ndef make_initializable_iterator(dataset):\n  """"""Creates an iterator, and initializes tables.\n\n  This is useful in cases where make_one_shot_iterator wouldn\'t work because\n  the graph contains a hash table that needs to be initialized.\n\n  Args:\n    dataset: A `tf.data.Dataset` object.\n\n  Returns:\n    A `tf.data.Iterator`.\n  """"""\n  iterator = dataset.make_initializable_iterator()\n  tf.add_to_collection(tf.GraphKeys.TABLE_INITIALIZERS, iterator.initializer)\n  return iterator\n\n\ndef read_dataset(file_read_func, decode_func, input_files, config):\n  """"""Reads a dataset, and handles repetition and shuffling.\n\n  Args:\n    file_read_func: Function to use in tf.data.Dataset.interleave, to read\n      every individual file into a tf.data.Dataset.\n    decode_func: Function to apply to all records.\n    input_files: A list of file paths to read.\n    config: A input_reader_builder.InputReader object.\n\n  Returns:\n    A tf.data.Dataset based on config.\n  """"""\n  # Shard, shuffle, and read files.\n  filenames = tf.concat([tf.matching_files(pattern) for pattern in input_files],\n                        0)\n  filename_dataset = tf.data.Dataset.from_tensor_slices(filenames)\n  if config.shuffle:\n    filename_dataset = filename_dataset.shuffle(\n        config.filenames_shuffle_buffer_size)\n  filename_dataset = filename_dataset.repeat(config.num_epochs or None)\n\n  records_dataset = filename_dataset.apply(\n      tf.contrib.data.parallel_interleave(\n          file_read_func, cycle_length=config.num_readers, sloppy=True))\n  if config.shuffle:\n    records_dataset.shuffle(config.shuffle_buffer_size)\n  tensor_dataset = records_dataset.map(\n      decode_func, num_parallel_calls=config.num_parallel_map_calls)\n  return tensor_dataset.prefetch(config.prefetch_size)\n'"
src/object_detection/utils/dataset_util_test.py,11,"b'# Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n""""""Tests for object_detection.utils.dataset_util.""""""\n\nimport os\nimport tensorflow as tf\n\nfrom object_detection.protos import input_reader_pb2\nfrom object_detection.utils import dataset_util\n\n\nclass DatasetUtilTest(tf.test.TestCase):\n\n  def setUp(self):\n    self._path_template = os.path.join(self.get_temp_dir(), \'examples_%s.txt\')\n\n    for i in range(5):\n      path = self._path_template % i\n      with tf.gfile.Open(path, \'wb\') as f:\n        f.write(\'\\n\'.join([str(i + 1), str((i + 1) * 10)]))\n\n  def _get_dataset_next(self, files, config, batch_size):\n    def decode_func(value):\n      return [tf.string_to_number(value, out_type=tf.int32)]\n\n    dataset = dataset_util.read_dataset(\n        tf.data.TextLineDataset, decode_func, files, config)\n    dataset = dataset.batch(batch_size)\n    return dataset.make_one_shot_iterator().get_next()\n\n  def test_read_examples_list(self):\n    example_list_data = """"""example1 1\\nexample2 2""""""\n    example_list_path = os.path.join(self.get_temp_dir(), \'examples.txt\')\n    with tf.gfile.Open(example_list_path, \'wb\') as f:\n      f.write(example_list_data)\n\n    examples = dataset_util.read_examples_list(example_list_path)\n    self.assertListEqual([\'example1\', \'example2\'], examples)\n\n  def test_make_initializable_iterator_with_hashTable(self):\n    keys = [1, 0, -1]\n    dataset = tf.data.Dataset.from_tensor_slices([[1, 2, -1, 5]])\n    table = tf.contrib.lookup.HashTable(\n        initializer=tf.contrib.lookup.KeyValueTensorInitializer(\n            keys=keys,\n            values=list(reversed(keys))),\n        default_value=100)\n    dataset = dataset.map(table.lookup)\n    data = dataset_util.make_initializable_iterator(dataset).get_next()\n    init = tf.tables_initializer()\n\n    with self.test_session() as sess:\n      sess.run(init)\n      self.assertAllEqual(sess.run(data), [-1, 100, 1, 100])\n\n  def test_read_dataset(self):\n    config = input_reader_pb2.InputReader()\n    config.num_readers = 1\n    config.shuffle = False\n\n    data = self._get_dataset_next([self._path_template % \'*\'], config,\n                                  batch_size=20)\n    with self.test_session() as sess:\n      self.assertAllEqual(sess.run(data),\n                          [[1, 10, 2, 20, 3, 30, 4, 40, 5, 50, 1, 10, 2, 20, 3,\n                            30, 4, 40, 5, 50]])\n\n  def test_read_dataset_single_epoch(self):\n    config = input_reader_pb2.InputReader()\n    config.num_epochs = 1\n    config.num_readers = 1\n    config.shuffle = False\n\n    data = self._get_dataset_next([self._path_template % \'0\'], config,\n                                  batch_size=30)\n    with self.test_session() as sess:\n      # First batch will retrieve as much as it can, second batch will fail.\n      self.assertAllEqual(sess.run(data), [[1, 10]])\n      self.assertRaises(tf.errors.OutOfRangeError, sess.run, data)\n\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
src/object_detection/utils/json_utils.py,0,"b'# Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Utilities for dealing with writing json strings.\n\njson_utils wraps json.dump and json.dumps so that they can be used to safely\ncontrol the precision of floats when writing to json strings or files.\n""""""\nimport json\nfrom json import encoder\n\n\ndef Dump(obj, fid, float_digits=-1, **params):\n  """"""Wrapper of json.dump that allows specifying the float precision used.\n\n  Args:\n    obj: The object to dump.\n    fid: The file id to write to.\n    float_digits: The number of digits of precision when writing floats out.\n    **params: Additional parameters to pass to json.dumps.\n  """"""\n  original_encoder = encoder.FLOAT_REPR\n  if float_digits >= 0:\n    encoder.FLOAT_REPR = lambda o: format(o, \'.%df\' % float_digits)\n  try:\n    json.dump(obj, fid, **params)\n  finally:\n    encoder.FLOAT_REPR = original_encoder\n\n\ndef Dumps(obj, float_digits=-1, **params):\n  """"""Wrapper of json.dumps that allows specifying the float precision used.\n\n  Args:\n    obj: The object to dump.\n    float_digits: The number of digits of precision when writing floats out.\n    **params: Additional parameters to pass to json.dumps.\n\n  Returns:\n    output: JSON string representation of obj.\n  """"""\n  original_encoder = encoder.FLOAT_REPR\n  original_c_make_encoder = encoder.c_make_encoder\n  if float_digits >= 0:\n    encoder.FLOAT_REPR = lambda o: format(o, \'.%df\' % float_digits)\n    encoder.c_make_encoder = None\n  try:\n    output = json.dumps(obj, **params)\n  finally:\n    encoder.FLOAT_REPR = original_encoder\n    encoder.c_make_encoder = original_c_make_encoder\n\n  return output\n\n\ndef PrettyParams(**params):\n  """"""Returns parameters for use with Dump and Dumps to output pretty json.\n\n  Example usage:\n    ```json_str = json_utils.Dumps(obj, **json_utils.PrettyParams())```\n    ```json_str = json_utils.Dumps(\n                      obj, **json_utils.PrettyParams(allow_nans=False))```\n\n  Args:\n    **params: Additional params to pass to json.dump or json.dumps.\n\n  Returns:\n    params: Parameters that are compatible with json_utils.Dump and\n      json_utils.Dumps.\n  """"""\n  params[\'float_digits\'] = 4\n  params[\'sort_keys\'] = True\n  params[\'indent\'] = 2\n  params[\'separators\'] = (\',\', \': \')\n  return params\n\n'"
src/object_detection/utils/json_utils_test.py,14,"b'# Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Tests for google3.image.understanding.object_detection.utils.json_utils.""""""\nimport os\n\nimport tensorflow as tf\n\nfrom object_detection.utils import json_utils\n\n\nclass JsonUtilsTest(tf.test.TestCase):\n\n  def testDumpReasonablePrecision(self):\n    output_path = os.path.join(tf.test.get_temp_dir(), \'test.json\')\n    with tf.gfile.GFile(output_path, \'w\') as f:\n      json_utils.Dump(1.0, f, float_digits=2)\n    with tf.gfile.GFile(output_path, \'r\') as f:\n      self.assertEqual(f.read(), \'1.00\')\n\n  def testDumpPassExtraParams(self):\n    output_path = os.path.join(tf.test.get_temp_dir(), \'test.json\')\n    with tf.gfile.GFile(output_path, \'w\') as f:\n      json_utils.Dump([1.0], f, float_digits=2, indent=3)\n    with tf.gfile.GFile(output_path, \'r\') as f:\n      self.assertEqual(f.read(), \'[\\n   1.00\\n]\')\n\n  def testDumpZeroPrecision(self):\n    output_path = os.path.join(tf.test.get_temp_dir(), \'test.json\')\n    with tf.gfile.GFile(output_path, \'w\') as f:\n      json_utils.Dump(1.0, f, float_digits=0, indent=3)\n    with tf.gfile.GFile(output_path, \'r\') as f:\n      self.assertEqual(f.read(), \'1\')\n\n  def testDumpUnspecifiedPrecision(self):\n    output_path = os.path.join(tf.test.get_temp_dir(), \'test.json\')\n    with tf.gfile.GFile(output_path, \'w\') as f:\n      json_utils.Dump(1.012345, f)\n    with tf.gfile.GFile(output_path, \'r\') as f:\n      self.assertEqual(f.read(), \'1.012345\')\n\n  def testDumpsReasonablePrecision(self):\n    s = json_utils.Dumps(1.0, float_digits=2)\n    self.assertEqual(s, \'1.00\')\n\n  def testDumpsPassExtraParams(self):\n    s = json_utils.Dumps([1.0], float_digits=2, indent=3)\n    self.assertEqual(s, \'[\\n   1.00\\n]\')\n\n  def testDumpsZeroPrecision(self):\n    s = json_utils.Dumps(1.0, float_digits=0)\n    self.assertEqual(s, \'1\')\n\n  def testDumpsUnspecifiedPrecision(self):\n    s = json_utils.Dumps(1.012345)\n    self.assertEqual(s, \'1.012345\')\n\n  def testPrettyParams(self):\n    s = json_utils.Dumps({\'v\': 1.012345, \'n\': 2}, **json_utils.PrettyParams())\n    self.assertEqual(s, \'{\\n  ""n"": 2,\\n  ""v"": 1.0123\\n}\')\n\n  def testPrettyParamsExtraParamsInside(self):\n    s = json_utils.Dumps(\n        {\'v\': 1.012345,\n         \'n\': float(\'nan\')}, **json_utils.PrettyParams(allow_nan=True))\n    self.assertEqual(s, \'{\\n  ""n"": NaN,\\n  ""v"": 1.0123\\n}\')\n\n    with self.assertRaises(ValueError):\n      s = json_utils.Dumps(\n          {\'v\': 1.012345,\n           \'n\': float(\'nan\')}, **json_utils.PrettyParams(allow_nan=False))\n\n  def testPrettyParamsExtraParamsOutside(self):\n    s = json_utils.Dumps(\n        {\'v\': 1.012345,\n         \'n\': float(\'nan\')}, allow_nan=True, **json_utils.PrettyParams())\n    self.assertEqual(s, \'{\\n  ""n"": NaN,\\n  ""v"": 1.0123\\n}\')\n\n    with self.assertRaises(ValueError):\n      s = json_utils.Dumps(\n          {\'v\': 1.012345,\n           \'n\': float(\'nan\')}, allow_nan=False, **json_utils.PrettyParams())\n\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
src/object_detection/utils/label_map_util.py,1,"b'# Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n""""""Label map utility functions.""""""\n\nimport logging\n\nimport tensorflow as tf\nfrom google.protobuf import text_format\nfrom object_detection.protos import string_int_label_map_pb2\n\n\ndef _validate_label_map(label_map):\n  """"""Checks if a label map is valid.\n\n  Args:\n    label_map: StringIntLabelMap to validate.\n\n  Raises:\n    ValueError: if label map is invalid.\n  """"""\n  for item in label_map.item:\n    if item.id < 0:\n      raise ValueError(\'Label map ids should be >= 0.\')\n    if item.id == 0 and item.name != \'background\':\n      raise ValueError(\'Label map id 0 is reserved for the background label\')\n\n\ndef create_category_index(categories):\n  """"""Creates dictionary of COCO compatible categories keyed by category id.\n\n  Args:\n    categories: a list of dicts, each of which has the following keys:\n      \'id\': (required) an integer id uniquely identifying this category.\n      \'name\': (required) string representing category name\n        e.g., \'cat\', \'dog\', \'pizza\'.\n\n  Returns:\n    category_index: a dict containing the same entries as categories, but keyed\n      by the \'id\' field of each category.\n  """"""\n  category_index = {}\n  for cat in categories:\n    category_index[cat[\'id\']] = cat\n  return category_index\n\n\ndef get_max_label_map_index(label_map):\n  """"""Get maximum index in label map.\n\n  Args:\n    label_map: a StringIntLabelMapProto\n\n  Returns:\n    an integer\n  """"""\n  return max([item.id for item in label_map.item])\n\n\ndef convert_label_map_to_categories(label_map,\n                                    max_num_classes,\n                                    use_display_name=True):\n  """"""Loads label map proto and returns categories list compatible with eval.\n\n  This function loads a label map and returns a list of dicts, each of which\n  has the following keys:\n    \'id\': (required) an integer id uniquely identifying this category.\n    \'name\': (required) string representing category name\n      e.g., \'cat\', \'dog\', \'pizza\'.\n  We only allow class into the list if its id-label_id_offset is\n  between 0 (inclusive) and max_num_classes (exclusive).\n  If there are several items mapping to the same id in the label map,\n  we will only keep the first one in the categories list.\n\n  Args:\n    label_map: a StringIntLabelMapProto or None.  If None, a default categories\n      list is created with max_num_classes categories.\n    max_num_classes: maximum number of (consecutive) label indices to include.\n    use_display_name: (boolean) choose whether to load \'display_name\' field\n      as category name.  If False or if the display_name field does not exist,\n      uses \'name\' field as category names instead.\n  Returns:\n    categories: a list of dictionaries representing all possible categories.\n  """"""\n  categories = []\n  list_of_ids_already_added = []\n  if not label_map:\n    label_id_offset = 1\n    for class_id in range(max_num_classes):\n      categories.append({\n          \'id\': class_id + label_id_offset,\n          \'name\': \'category_{}\'.format(class_id + label_id_offset)\n      })\n    return categories\n  for item in label_map.item:\n    if not 0 < item.id <= max_num_classes:\n      logging.info(\'Ignore item %d since it falls outside of requested \'\n                   \'label range.\', item.id)\n      continue\n    if use_display_name and item.HasField(\'display_name\'):\n      name = item.display_name\n    else:\n      name = item.name\n    if item.id not in list_of_ids_already_added:\n      list_of_ids_already_added.append(item.id)\n      categories.append({\'id\': item.id, \'name\': name})\n  return categories\n\n\ndef load_labelmap(path):\n  """"""Loads label map proto.\n\n  Args:\n    path: path to StringIntLabelMap proto text file.\n  Returns:\n    a StringIntLabelMapProto\n  """"""\n  with tf.gfile.GFile(path, \'r\') as fid:\n    label_map_string = fid.read()\n    label_map = string_int_label_map_pb2.StringIntLabelMap()\n    try:\n      text_format.Merge(label_map_string, label_map)\n    except text_format.ParseError:\n      label_map.ParseFromString(label_map_string)\n  _validate_label_map(label_map)\n  return label_map\n\n\ndef get_label_map_dict(label_map_path, use_display_name=False):\n  """"""Reads a label map and returns a dictionary of label names to id.\n\n  Args:\n    label_map_path: path to label_map.\n    use_display_name: whether to use the label map items\' display names as keys.\n\n  Returns:\n    A dictionary mapping label names to id.\n  """"""\n  label_map = load_labelmap(label_map_path)\n  label_map_dict = {}\n  for item in label_map.item:\n    if use_display_name:\n      label_map_dict[item.display_name] = item.id\n    else:\n      label_map_dict[item.name] = item.id\n  return label_map_dict\n\n\ndef create_category_index_from_labelmap(label_map_path):\n  """"""Reads a label map and returns a category index.\n\n  Args:\n    label_map_path: Path to `StringIntLabelMap` proto text file.\n\n  Returns:\n    A category index, which is a dictionary that maps integer ids to dicts\n    containing categories, e.g.\n    {1: {\'id\': 1, \'name\': \'dog\'}, 2: {\'id\': 2, \'name\': \'cat\'}, ...}\n  """"""\n  label_map = load_labelmap(label_map_path)\n  max_num_classes = max(item.id for item in label_map.item)\n  categories = convert_label_map_to_categories(label_map, max_num_classes)\n  return create_category_index(categories)\n\n\ndef create_class_agnostic_category_index():\n  """"""Creates a category index with a single `object` class.""""""\n  return {1: {\'id\': 1, \'name\': \'object\'}}\n'"
src/object_detection/utils/label_map_util_test.py,7,"b'# Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n""""""Tests for object_detection.utils.label_map_util.""""""\n\nimport os\nimport tensorflow as tf\n\nfrom google.protobuf import text_format\nfrom object_detection.protos import string_int_label_map_pb2\nfrom object_detection.utils import label_map_util\n\n\nclass LabelMapUtilTest(tf.test.TestCase):\n\n  def _generate_label_map(self, num_classes):\n    label_map_proto = string_int_label_map_pb2.StringIntLabelMap()\n    for i in range(1, num_classes + 1):\n      item = label_map_proto.item.add()\n      item.id = i\n      item.name = \'label_\' + str(i)\n      item.display_name = str(i)\n    return label_map_proto\n\n  def test_get_label_map_dict(self):\n    label_map_string = """"""\n      item {\n        id:2\n        name:\'cat\'\n      }\n      item {\n        id:1\n        name:\'dog\'\n      }\n    """"""\n    label_map_path = os.path.join(self.get_temp_dir(), \'label_map.pbtxt\')\n    with tf.gfile.Open(label_map_path, \'wb\') as f:\n      f.write(label_map_string)\n\n    label_map_dict = label_map_util.get_label_map_dict(label_map_path)\n    self.assertEqual(label_map_dict[\'dog\'], 1)\n    self.assertEqual(label_map_dict[\'cat\'], 2)\n\n  def test_get_label_map_dict_display(self):\n    label_map_string = """"""\n      item {\n        id:2\n        display_name:\'cat\'\n      }\n      item {\n        id:1\n        display_name:\'dog\'\n      }\n    """"""\n    label_map_path = os.path.join(self.get_temp_dir(), \'label_map.pbtxt\')\n    with tf.gfile.Open(label_map_path, \'wb\') as f:\n      f.write(label_map_string)\n\n    label_map_dict = label_map_util.get_label_map_dict(\n        label_map_path, use_display_name=True)\n    self.assertEqual(label_map_dict[\'dog\'], 1)\n    self.assertEqual(label_map_dict[\'cat\'], 2)\n\n  def test_load_bad_label_map(self):\n    label_map_string = """"""\n      item {\n        id:0\n        name:\'class that should not be indexed at zero\'\n      }\n      item {\n        id:2\n        name:\'cat\'\n      }\n      item {\n        id:1\n        name:\'dog\'\n      }\n    """"""\n    label_map_path = os.path.join(self.get_temp_dir(), \'label_map.pbtxt\')\n    with tf.gfile.Open(label_map_path, \'wb\') as f:\n      f.write(label_map_string)\n\n    with self.assertRaises(ValueError):\n      label_map_util.load_labelmap(label_map_path)\n\n  def test_load_label_map_with_background(self):\n    label_map_string = """"""\n      item {\n        id:0\n        name:\'background\'\n      }\n      item {\n        id:2\n        name:\'cat\'\n      }\n      item {\n        id:1\n        name:\'dog\'\n      }\n    """"""\n    label_map_path = os.path.join(self.get_temp_dir(), \'label_map.pbtxt\')\n    with tf.gfile.Open(label_map_path, \'wb\') as f:\n      f.write(label_map_string)\n\n    label_map_dict = label_map_util.get_label_map_dict(label_map_path)\n    self.assertEqual(label_map_dict[\'background\'], 0)\n    self.assertEqual(label_map_dict[\'dog\'], 1)\n    self.assertEqual(label_map_dict[\'cat\'], 2)\n\n  def test_keep_categories_with_unique_id(self):\n    label_map_proto = string_int_label_map_pb2.StringIntLabelMap()\n    label_map_string = """"""\n      item {\n        id:2\n        name:\'cat\'\n      }\n      item {\n        id:1\n        name:\'child\'\n      }\n      item {\n        id:1\n        name:\'person\'\n      }\n      item {\n        id:1\n        name:\'n00007846\'\n      }\n    """"""\n    text_format.Merge(label_map_string, label_map_proto)\n    categories = label_map_util.convert_label_map_to_categories(\n        label_map_proto, max_num_classes=3)\n    self.assertListEqual([{\n        \'id\': 2,\n        \'name\': u\'cat\'\n    }, {\n        \'id\': 1,\n        \'name\': u\'child\'\n    }], categories)\n\n  def test_convert_label_map_to_categories_no_label_map(self):\n    categories = label_map_util.convert_label_map_to_categories(\n        None, max_num_classes=3)\n    expected_categories_list = [{\n        \'name\': u\'category_1\',\n        \'id\': 1\n    }, {\n        \'name\': u\'category_2\',\n        \'id\': 2\n    }, {\n        \'name\': u\'category_3\',\n        \'id\': 3\n    }]\n    self.assertListEqual(expected_categories_list, categories)\n\n  def test_convert_label_map_to_coco_categories(self):\n    label_map_proto = self._generate_label_map(num_classes=4)\n    categories = label_map_util.convert_label_map_to_categories(\n        label_map_proto, max_num_classes=3)\n    expected_categories_list = [{\n        \'name\': u\'1\',\n        \'id\': 1\n    }, {\n        \'name\': u\'2\',\n        \'id\': 2\n    }, {\n        \'name\': u\'3\',\n        \'id\': 3\n    }]\n    self.assertListEqual(expected_categories_list, categories)\n\n  def test_convert_label_map_to_coco_categories_with_few_classes(self):\n    label_map_proto = self._generate_label_map(num_classes=4)\n    cat_no_offset = label_map_util.convert_label_map_to_categories(\n        label_map_proto, max_num_classes=2)\n    expected_categories_list = [{\n        \'name\': u\'1\',\n        \'id\': 1\n    }, {\n        \'name\': u\'2\',\n        \'id\': 2\n    }]\n    self.assertListEqual(expected_categories_list, cat_no_offset)\n\n  def test_get_max_label_map_index(self):\n    num_classes = 4\n    label_map_proto = self._generate_label_map(num_classes=num_classes)\n    max_index = label_map_util.get_max_label_map_index(label_map_proto)\n    self.assertEqual(num_classes, max_index)\n\n  def test_create_category_index(self):\n    categories = [{\'name\': u\'1\', \'id\': 1}, {\'name\': u\'2\', \'id\': 2}]\n    category_index = label_map_util.create_category_index(categories)\n    self.assertDictEqual({\n        1: {\n            \'name\': u\'1\',\n            \'id\': 1\n        },\n        2: {\n            \'name\': u\'2\',\n            \'id\': 2\n        }\n    }, category_index)\n\n  def test_create_category_index_from_labelmap(self):\n    label_map_string = """"""\n      item {\n        id:2\n        name:\'cat\'\n      }\n      item {\n        id:1\n        name:\'dog\'\n      }\n    """"""\n    label_map_path = os.path.join(self.get_temp_dir(), \'label_map.pbtxt\')\n    with tf.gfile.Open(label_map_path, \'wb\') as f:\n      f.write(label_map_string)\n\n    category_index = label_map_util.create_category_index_from_labelmap(\n        label_map_path)\n    self.assertDictEqual({\n        1: {\n            \'name\': u\'dog\',\n            \'id\': 1\n        },\n        2: {\n            \'name\': u\'cat\',\n            \'id\': 2\n        }\n    }, category_index)\n\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
src/object_detection/utils/learning_schedules.py,13,"b'# Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Library of common learning rate schedules.""""""\n\nimport numpy as np\nimport tensorflow as tf\n\n\ndef exponential_decay_with_burnin(global_step,\n                                  learning_rate_base,\n                                  learning_rate_decay_steps,\n                                  learning_rate_decay_factor,\n                                  burnin_learning_rate=0.0,\n                                  burnin_steps=0):\n  """"""Exponential decay schedule with burn-in period.\n\n  In this schedule, learning rate is fixed at burnin_learning_rate\n  for a fixed period, before transitioning to a regular exponential\n  decay schedule.\n\n  Args:\n    global_step: int tensor representing global step.\n    learning_rate_base: base learning rate.\n    learning_rate_decay_steps: steps to take between decaying the learning rate.\n      Note that this includes the number of burn-in steps.\n    learning_rate_decay_factor: multiplicative factor by which to decay\n      learning rate.\n    burnin_learning_rate: initial learning rate during burn-in period.  If\n      0.0 (which is the default), then the burn-in learning rate is simply\n      set to learning_rate_base.\n    burnin_steps: number of steps to use burnin learning rate.\n\n  Returns:\n    a (scalar) float tensor representing learning rate\n  """"""\n  if burnin_learning_rate == 0:\n    burnin_learning_rate = learning_rate_base\n  post_burnin_learning_rate = tf.train.exponential_decay(\n      learning_rate_base,\n      global_step,\n      learning_rate_decay_steps,\n      learning_rate_decay_factor,\n      staircase=True)\n  return tf.where(\n      tf.less(tf.cast(global_step, tf.int32), tf.constant(burnin_steps)),\n      tf.constant(burnin_learning_rate),\n      post_burnin_learning_rate, name=\'learning_rate\')\n\n\ndef cosine_decay_with_warmup(global_step,\n                             learning_rate_base,\n                             total_steps,\n                             warmup_learning_rate=0.0,\n                             warmup_steps=0,\n                             hold_base_rate_steps=0):\n  """"""Cosine decay schedule with warm up period.\n\n  Cosine annealing learning rate as described in:\n    Loshchilov and Hutter, SGDR: Stochastic Gradient Descent with Warm Restarts.\n    ICLR 2017. https://arxiv.org/abs/1608.03983\n  In this schedule, the learning rate grows linearly from warmup_learning_rate\n  to learning_rate_base for warmup_steps, then transitions to a cosine decay\n  schedule.\n\n  Args:\n    global_step: int64 (scalar) tensor representing global step.\n    learning_rate_base: base learning rate.\n    total_steps: total number of training steps.\n    warmup_learning_rate: initial learning rate for warm up.\n    warmup_steps: number of warmup steps.\n    hold_base_rate_steps: Optional number of steps to hold base learning rate\n      before decaying.\n\n  Returns:\n    a (scalar) float tensor representing learning rate.\n\n  Raises:\n    ValueError: if warmup_learning_rate is larger than learning_rate_base,\n      or if warmup_steps is larger than total_steps.\n  """"""\n  if learning_rate_base < warmup_learning_rate:\n    raise ValueError(\'learning_rate_base must be larger \'\n                     \'or equal to warmup_learning_rate.\')\n  if total_steps < warmup_steps:\n    raise ValueError(\'total_steps must be larger or equal to \'\n                     \'warmup_steps.\')\n  learning_rate = 0.5 * learning_rate_base * (1 + tf.cos(\n      np.pi *\n      (tf.cast(global_step, tf.float32) - warmup_steps - hold_base_rate_steps\n      ) / float(total_steps - warmup_steps - hold_base_rate_steps)))\n  if hold_base_rate_steps > 0:\n    learning_rate = tf.where(global_step > warmup_steps + hold_base_rate_steps,\n                             learning_rate, learning_rate_base)\n  if warmup_steps > 0:\n    slope = (learning_rate_base - warmup_learning_rate) / warmup_steps\n    warmup_rate = slope * tf.cast(global_step,\n                                  tf.float32) + warmup_learning_rate\n    learning_rate = tf.where(global_step < warmup_steps, warmup_rate,\n                             learning_rate)\n  return tf.where(global_step > total_steps, 0.0, learning_rate,\n                  name=\'learning_rate\')\n\n\ndef manual_stepping(global_step, boundaries, rates, warmup=False):\n  """"""Manually stepped learning rate schedule.\n\n  This function provides fine grained control over learning rates.  One must\n  specify a sequence of learning rates as well as a set of integer steps\n  at which the current learning rate must transition to the next.  For example,\n  if boundaries = [5, 10] and rates = [.1, .01, .001], then the learning\n  rate returned by this function is .1 for global_step=0,...,4, .01 for\n  global_step=5...9, and .001 for global_step=10 and onward.\n\n  Args:\n    global_step: int64 (scalar) tensor representing global step.\n    boundaries: a list of global steps at which to switch learning\n      rates.  This list is assumed to consist of increasing positive integers.\n    rates: a list of (float) learning rates corresponding to intervals between\n      the boundaries.  The length of this list must be exactly\n      len(boundaries) + 1.\n    warmup: Whether to linearly interpolate learning rate for steps in\n      [0, boundaries[0]].\n\n  Returns:\n    a (scalar) float tensor representing learning rate\n  Raises:\n    ValueError: if one of the following checks fails:\n      1. boundaries is a strictly increasing list of positive integers\n      2. len(rates) == len(boundaries) + 1\n      3. boundaries[0] != 0\n  """"""\n  if any([b < 0 for b in boundaries]) or any(\n      [not isinstance(b, int) for b in boundaries]):\n    raise ValueError(\'boundaries must be a list of positive integers\')\n  if any([bnext <= b for bnext, b in zip(boundaries[1:], boundaries[:-1])]):\n    raise ValueError(\'Entries in boundaries must be strictly increasing.\')\n  if any([not isinstance(r, float) for r in rates]):\n    raise ValueError(\'Learning rates must be floats\')\n  if len(rates) != len(boundaries) + 1:\n    raise ValueError(\'Number of provided learning rates must exceed \'\n                     \'number of boundary points by exactly 1.\')\n\n  if boundaries and boundaries[0] == 0:\n    raise ValueError(\'First step cannot be zero.\')\n\n  if warmup and boundaries:\n    slope = (rates[1] - rates[0]) * 1.0 / boundaries[0]\n    warmup_steps = range(boundaries[0])\n    warmup_rates = [rates[0] + slope * step for step in warmup_steps]\n    boundaries = warmup_steps + boundaries\n    rates = warmup_rates + rates[1:]\n  else:\n    boundaries = [0] + boundaries\n  num_boundaries = len(boundaries)\n  rate_index = tf.reduce_max(tf.where(tf.greater_equal(global_step, boundaries),\n                                      range(num_boundaries),\n                                      [0] * num_boundaries))\n  return tf.reduce_sum(rates * tf.one_hot(rate_index, depth=num_boundaries),\n                       name=\'learning_rate\')\n'"
src/object_detection/utils/learning_schedules_test.py,1,"b'# Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n""""""Tests for object_detection.utils.learning_schedules.""""""\nimport numpy as np\nimport tensorflow as tf\n\nfrom object_detection.utils import learning_schedules\nfrom object_detection.utils import test_case\n\n\nclass LearningSchedulesTest(test_case.TestCase):\n\n  def testExponentialDecayWithBurnin(self):\n    def graph_fn(global_step):\n      learning_rate_base = 1.0\n      learning_rate_decay_steps = 3\n      learning_rate_decay_factor = .1\n      burnin_learning_rate = .5\n      burnin_steps = 2\n      learning_rate = learning_schedules.exponential_decay_with_burnin(\n          global_step, learning_rate_base, learning_rate_decay_steps,\n          learning_rate_decay_factor, burnin_learning_rate, burnin_steps)\n      assert learning_rate.op.name.endswith(\'learning_rate\')\n      return (learning_rate,)\n\n    output_rates = [\n        self.execute(graph_fn, [np.array(i).astype(np.int64)]) for i in range(8)\n    ]\n\n    exp_rates = [.5, .5, 1, .1, .1, .1, .01, .01]\n    self.assertAllClose(output_rates, exp_rates, rtol=1e-4)\n\n  def testCosineDecayWithWarmup(self):\n    def graph_fn(global_step):\n      learning_rate_base = 1.0\n      total_steps = 100\n      warmup_learning_rate = 0.1\n      warmup_steps = 9\n      learning_rate = learning_schedules.cosine_decay_with_warmup(\n          global_step, learning_rate_base, total_steps,\n          warmup_learning_rate, warmup_steps)\n      assert learning_rate.op.name.endswith(\'learning_rate\')\n      return (learning_rate,)\n    exp_rates = [0.1, 0.5, 0.9, 1.0, 0]\n    input_global_steps = [0, 4, 8, 9, 100]\n    output_rates = [\n        self.execute(graph_fn, [np.array(step).astype(np.int64)])\n        for step in input_global_steps\n    ]\n    self.assertAllClose(output_rates, exp_rates)\n\n  def testCosineDecayAfterTotalSteps(self):\n    def graph_fn(global_step):\n      learning_rate_base = 1.0\n      total_steps = 100\n      warmup_learning_rate = 0.1\n      warmup_steps = 9\n      learning_rate = learning_schedules.cosine_decay_with_warmup(\n          global_step, learning_rate_base, total_steps,\n          warmup_learning_rate, warmup_steps)\n      assert learning_rate.op.name.endswith(\'learning_rate\')\n      return (learning_rate,)\n    exp_rates = [0]\n    input_global_steps = [101]\n    output_rates = [\n        self.execute(graph_fn, [np.array(step).astype(np.int64)])\n        for step in input_global_steps\n    ]\n    self.assertAllClose(output_rates, exp_rates)\n\n  def testCosineDecayWithHoldBaseLearningRateSteps(self):\n    def graph_fn(global_step):\n      learning_rate_base = 1.0\n      total_steps = 120\n      warmup_learning_rate = 0.1\n      warmup_steps = 9\n      hold_base_rate_steps = 20\n      learning_rate = learning_schedules.cosine_decay_with_warmup(\n          global_step, learning_rate_base, total_steps,\n          warmup_learning_rate, warmup_steps, hold_base_rate_steps)\n      assert learning_rate.op.name.endswith(\'learning_rate\')\n      return (learning_rate,)\n    exp_rates = [0.1, 0.5, 0.9, 1.0, 1.0, 1.0, 0.999702, 0.874255, 0.577365,\n                 0.0]\n    input_global_steps = [0, 4, 8, 9, 10, 29, 30, 50, 70, 120]\n    output_rates = [\n        self.execute(graph_fn, [np.array(step).astype(np.int64)])\n        for step in input_global_steps\n    ]\n    self.assertAllClose(output_rates, exp_rates)\n\n  def testManualStepping(self):\n    def graph_fn(global_step):\n      boundaries = [2, 3, 7]\n      rates = [1.0, 2.0, 3.0, 4.0]\n      learning_rate = learning_schedules.manual_stepping(\n          global_step, boundaries, rates)\n      assert learning_rate.op.name.endswith(\'learning_rate\')\n      return (learning_rate,)\n\n    output_rates = [\n        self.execute(graph_fn, [np.array(i).astype(np.int64)])\n        for i in range(10)\n    ]\n    exp_rates = [1.0, 1.0, 2.0, 3.0, 3.0, 3.0, 3.0, 4.0, 4.0, 4.0]\n    self.assertAllClose(output_rates, exp_rates)\n\n  def testManualSteppingWithWarmup(self):\n    def graph_fn(global_step):\n      boundaries = [4, 6, 8]\n      rates = [0.02, 0.10, 0.01, 0.001]\n      learning_rate = learning_schedules.manual_stepping(\n          global_step, boundaries, rates, warmup=True)\n      assert learning_rate.op.name.endswith(\'learning_rate\')\n      return (learning_rate,)\n\n    output_rates = [\n        self.execute(graph_fn, [np.array(i).astype(np.int64)])\n        for i in range(9)\n    ]\n    exp_rates = [0.02, 0.04, 0.06, 0.08, 0.10, 0.10, 0.01, 0.01, 0.001]\n    self.assertAllClose(output_rates, exp_rates)\n\n  def testManualSteppingWithZeroBoundaries(self):\n    def graph_fn(global_step):\n      boundaries = []\n      rates = [0.01]\n      learning_rate = learning_schedules.manual_stepping(\n          global_step, boundaries, rates)\n      return (learning_rate,)\n\n    output_rates = [\n        self.execute(graph_fn, [np.array(i).astype(np.int64)])\n        for i in range(4)\n    ]\n    exp_rates = [0.01] * 4\n    self.assertAllClose(output_rates, exp_rates)\n\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
src/object_detection/utils/metrics.py,0,"b'# Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n""""""Functions for computing metrics like precision, recall, CorLoc and etc.""""""\nfrom __future__ import division\n\nimport numpy as np\n\n\ndef compute_precision_recall(scores, labels, num_gt):\n  """"""Compute precision and recall.\n\n  Args:\n    scores: A float numpy array representing detection score\n    labels: A boolean numpy array representing true/false positive labels\n    num_gt: Number of ground truth instances\n\n  Raises:\n    ValueError: if the input is not of the correct format\n\n  Returns:\n    precision: Fraction of positive instances over detected ones. This value is\n      None if no ground truth labels are present.\n    recall: Fraction of detected positive instance over all positive instances.\n      This value is None if no ground truth labels are present.\n\n  """"""\n  if not isinstance(\n      labels, np.ndarray) or labels.dtype != np.bool or len(labels.shape) != 1:\n    raise ValueError(""labels must be single dimension bool numpy array"")\n\n  if not isinstance(\n      scores, np.ndarray) or len(scores.shape) != 1:\n    raise ValueError(""scores must be single dimension numpy array"")\n\n  if num_gt < np.sum(labels):\n    raise ValueError(""Number of true positives must be smaller than num_gt."")\n\n  if len(scores) != len(labels):\n    raise ValueError(""scores and labels must be of the same size."")\n\n  if num_gt == 0:\n    return None, None\n\n  sorted_indices = np.argsort(scores)\n  sorted_indices = sorted_indices[::-1]\n  labels = labels.astype(int)\n  true_positive_labels = labels[sorted_indices]\n  false_positive_labels = 1 - true_positive_labels\n  cum_true_positives = np.cumsum(true_positive_labels)\n  cum_false_positives = np.cumsum(false_positive_labels)\n  precision = cum_true_positives.astype(float) / (\n      cum_true_positives + cum_false_positives)\n  recall = cum_true_positives.astype(float) / num_gt\n  return precision, recall\n\n\ndef compute_average_precision(precision, recall):\n  """"""Compute Average Precision according to the definition in VOCdevkit.\n\n  Precision is modified to ensure that it does not decrease as recall\n  decrease.\n\n  Args:\n    precision: A float [N, 1] numpy array of precisions\n    recall: A float [N, 1] numpy array of recalls\n\n  Raises:\n    ValueError: if the input is not of the correct format\n\n  Returns:\n    average_precison: The area under the precision recall curve. NaN if\n      precision and recall are None.\n\n  """"""\n  if precision is None:\n    if recall is not None:\n      raise ValueError(""If precision is None, recall must also be None"")\n    return np.NAN\n\n  if not isinstance(precision, np.ndarray) or not isinstance(recall,\n                                                             np.ndarray):\n    raise ValueError(""precision and recall must be numpy array"")\n  if precision.dtype != np.float or recall.dtype != np.float:\n    raise ValueError(""input must be float numpy array."")\n  if len(precision) != len(recall):\n    raise ValueError(""precision and recall must be of the same size."")\n  if not precision.size:\n    return 0.0\n  if np.amin(precision) < 0 or np.amax(precision) > 1:\n    raise ValueError(""Precision must be in the range of [0, 1]."")\n  if np.amin(recall) < 0 or np.amax(recall) > 1:\n    raise ValueError(""recall must be in the range of [0, 1]."")\n  if not all(recall[i] <= recall[i + 1] for i in range(len(recall) - 1)):\n    raise ValueError(""recall must be a non-decreasing array"")\n\n  recall = np.concatenate([[0], recall, [1]])\n  precision = np.concatenate([[0], precision, [0]])\n\n  # Preprocess precision to be a non-decreasing array\n  for i in range(len(precision) - 2, -1, -1):\n    precision[i] = np.maximum(precision[i], precision[i + 1])\n\n  indices = np.where(recall[1:] != recall[:-1])[0] + 1\n  average_precision = np.sum(\n      (recall[indices] - recall[indices - 1]) * precision[indices])\n  return average_precision\n\n\ndef compute_cor_loc(num_gt_imgs_per_class,\n                    num_images_correctly_detected_per_class):\n  """"""Compute CorLoc according to the definition in the following paper.\n\n  https://www.robots.ox.ac.uk/~vgg/rg/papers/deselaers-eccv10.pdf\n\n  Returns nans if there are no ground truth images for a class.\n\n  Args:\n    num_gt_imgs_per_class: 1D array, representing number of images containing\n        at least one object instance of a particular class\n    num_images_correctly_detected_per_class: 1D array, representing number of\n        images that are correctly detected at least one object instance of a\n        particular class\n\n  Returns:\n    corloc_per_class: A float numpy array represents the corloc score of each\n      class\n  """"""\n  return np.where(\n      num_gt_imgs_per_class == 0,\n      np.nan,\n      num_images_correctly_detected_per_class / num_gt_imgs_per_class)\n'"
src/object_detection/utils/metrics_test.py,2,"b'# Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n""""""Tests for object_detection.metrics.""""""\n\nimport numpy as np\nimport tensorflow as tf\n\nfrom object_detection.utils import metrics\n\n\nclass MetricsTest(tf.test.TestCase):\n\n  def test_compute_cor_loc(self):\n    num_gt_imgs_per_class = np.array([100, 1, 5, 1, 1], dtype=int)\n    num_images_correctly_detected_per_class = np.array([10, 0, 1, 0, 0],\n                                                       dtype=int)\n    corloc = metrics.compute_cor_loc(num_gt_imgs_per_class,\n                                     num_images_correctly_detected_per_class)\n    expected_corloc = np.array([0.1, 0, 0.2, 0, 0], dtype=float)\n    self.assertTrue(np.allclose(corloc, expected_corloc))\n\n  def test_compute_cor_loc_nans(self):\n    num_gt_imgs_per_class = np.array([100, 0, 0, 1, 1], dtype=int)\n    num_images_correctly_detected_per_class = np.array([10, 0, 1, 0, 0],\n                                                       dtype=int)\n    corloc = metrics.compute_cor_loc(num_gt_imgs_per_class,\n                                     num_images_correctly_detected_per_class)\n    expected_corloc = np.array([0.1, np.nan, np.nan, 0, 0], dtype=float)\n    self.assertAllClose(corloc, expected_corloc)\n\n  def test_compute_precision_recall(self):\n    num_gt = 10\n    scores = np.array([0.4, 0.3, 0.6, 0.2, 0.7, 0.1], dtype=float)\n    labels = np.array([0, 1, 1, 0, 0, 1], dtype=bool)\n    accumulated_tp_count = np.array([0, 1, 1, 2, 2, 3], dtype=float)\n    expected_precision = accumulated_tp_count / np.array([1, 2, 3, 4, 5, 6])\n    expected_recall = accumulated_tp_count / num_gt\n    precision, recall = metrics.compute_precision_recall(scores, labels, num_gt)\n    self.assertAllClose(precision, expected_precision)\n    self.assertAllClose(recall, expected_recall)\n\n  def test_compute_average_precision(self):\n    precision = np.array([0.8, 0.76, 0.9, 0.65, 0.7, 0.5, 0.55, 0], dtype=float)\n    recall = np.array([0.3, 0.3, 0.4, 0.4, 0.45, 0.45, 0.5, 0.5], dtype=float)\n    processed_precision = np.array([0.9, 0.9, 0.9, 0.7, 0.7, 0.55, 0.55, 0],\n                                   dtype=float)\n    recall_interval = np.array([0.3, 0, 0.1, 0, 0.05, 0, 0.05, 0], dtype=float)\n    expected_mean_ap = np.sum(recall_interval * processed_precision)\n    mean_ap = metrics.compute_average_precision(precision, recall)\n    self.assertAlmostEqual(expected_mean_ap, mean_ap)\n\n  def test_compute_precision_recall_and_ap_no_groundtruth(self):\n    num_gt = 0\n    scores = np.array([0.4, 0.3, 0.6, 0.2, 0.7, 0.1], dtype=float)\n    labels = np.array([0, 0, 0, 0, 0, 0], dtype=bool)\n    expected_precision = None\n    expected_recall = None\n    precision, recall = metrics.compute_precision_recall(scores, labels, num_gt)\n    self.assertEquals(precision, expected_precision)\n    self.assertEquals(recall, expected_recall)\n    ap = metrics.compute_average_precision(precision, recall)\n    self.assertTrue(np.isnan(ap))\n\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
src/object_detection/utils/np_box_list.py,0,"b'# Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n""""""Numpy BoxList classes and functions.""""""\n\nimport numpy as np\n\n\nclass BoxList(object):\n  """"""Box collection.\n\n  BoxList represents a list of bounding boxes as numpy array, where each\n  bounding box is represented as a row of 4 numbers,\n  [y_min, x_min, y_max, x_max].  It is assumed that all bounding boxes within a\n  given list correspond to a single image.\n\n  Optionally, users can add additional related fields (such as\n  objectness/classification scores).\n  """"""\n\n  def __init__(self, data):\n    """"""Constructs box collection.\n\n    Args:\n      data: a numpy array of shape [N, 4] representing box coordinates\n\n    Raises:\n      ValueError: if bbox data is not a numpy array\n      ValueError: if invalid dimensions for bbox data\n    """"""\n    if not isinstance(data, np.ndarray):\n      raise ValueError(\'data must be a numpy array.\')\n    if len(data.shape) != 2 or data.shape[1] != 4:\n      raise ValueError(\'Invalid dimensions for box data.\')\n    if data.dtype != np.float32 and data.dtype != np.float64:\n      raise ValueError(\'Invalid data type for box data: float is required.\')\n    if not self._is_valid_boxes(data):\n      raise ValueError(\'Invalid box data. data must be a numpy array of \'\n                       \'N*[y_min, x_min, y_max, x_max]\')\n    self.data = {\'boxes\': data}\n\n  def num_boxes(self):\n    """"""Return number of boxes held in collections.""""""\n    return self.data[\'boxes\'].shape[0]\n\n  def get_extra_fields(self):\n    """"""Return all non-box fields.""""""\n    return [k for k in self.data.keys() if k != \'boxes\']\n\n  def has_field(self, field):\n    return field in self.data\n\n  def add_field(self, field, field_data):\n    """"""Add data to a specified field.\n\n    Args:\n      field: a string parameter used to speficy a related field to be accessed.\n      field_data: a numpy array of [N, ...] representing the data associated\n          with the field.\n    Raises:\n      ValueError: if the field is already exist or the dimension of the field\n          data does not matches the number of boxes.\n    """"""\n    if self.has_field(field):\n      raise ValueError(\'Field \' + field + \'already exists\')\n    if len(field_data.shape) < 1 or field_data.shape[0] != self.num_boxes():\n      raise ValueError(\'Invalid dimensions for field data\')\n    self.data[field] = field_data\n\n  def get(self):\n    """"""Convenience function for accesssing box coordinates.\n\n    Returns:\n      a numpy array of shape [N, 4] representing box corners\n    """"""\n    return self.get_field(\'boxes\')\n\n  def get_field(self, field):\n    """"""Accesses data associated with the specified field in the box collection.\n\n    Args:\n      field: a string parameter used to speficy a related field to be accessed.\n\n    Returns:\n      a numpy 1-d array representing data of an associated field\n\n    Raises:\n      ValueError: if invalid field\n    """"""\n    if not self.has_field(field):\n      raise ValueError(\'field {} does not exist\'.format(field))\n    return self.data[field]\n\n  def get_coordinates(self):\n    """"""Get corner coordinates of boxes.\n\n    Returns:\n     a list of 4 1-d numpy arrays [y_min, x_min, y_max, x_max]\n    """"""\n    box_coordinates = self.get()\n    y_min = box_coordinates[:, 0]\n    x_min = box_coordinates[:, 1]\n    y_max = box_coordinates[:, 2]\n    x_max = box_coordinates[:, 3]\n    return [y_min, x_min, y_max, x_max]\n\n  def _is_valid_boxes(self, data):\n    """"""Check whether data fullfills the format of N*[ymin, xmin, ymax, xmin].\n\n    Args:\n      data: a numpy array of shape [N, 4] representing box coordinates\n\n    Returns:\n      a boolean indicating whether all ymax of boxes are equal or greater than\n          ymin, and all xmax of boxes are equal or greater than xmin.\n    """"""\n    if data.shape[0] > 0:\n      for i in range(data.shape[0]):\n        if data[i, 0] > data[i, 2] or data[i, 1] > data[i, 3]:\n          return False\n    return True\n'"
src/object_detection/utils/np_box_list_ops.py,0,"b'# Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n""""""Bounding Box List operations for Numpy BoxLists.\n\nExample box operations that are supported:\n  * Areas: compute bounding box areas\n  * IOU: pairwise intersection-over-union scores\n""""""\nimport numpy as np\n\nfrom object_detection.utils import np_box_list\nfrom object_detection.utils import np_box_ops\n\n\nclass SortOrder(object):\n  """"""Enum class for sort order.\n\n  Attributes:\n    ascend: ascend order.\n    descend: descend order.\n  """"""\n  ASCEND = 1\n  DESCEND = 2\n\n\ndef area(boxlist):\n  """"""Computes area of boxes.\n\n  Args:\n    boxlist: BoxList holding N boxes\n\n  Returns:\n    a numpy array with shape [N*1] representing box areas\n  """"""\n  y_min, x_min, y_max, x_max = boxlist.get_coordinates()\n  return (y_max - y_min) * (x_max - x_min)\n\n\ndef intersection(boxlist1, boxlist2):\n  """"""Compute pairwise intersection areas between boxes.\n\n  Args:\n    boxlist1: BoxList holding N boxes\n    boxlist2: BoxList holding M boxes\n\n  Returns:\n    a numpy array with shape [N*M] representing pairwise intersection area\n  """"""\n  return np_box_ops.intersection(boxlist1.get(), boxlist2.get())\n\n\ndef iou(boxlist1, boxlist2):\n  """"""Computes pairwise intersection-over-union between box collections.\n\n  Args:\n    boxlist1: BoxList holding N boxes\n    boxlist2: BoxList holding M boxes\n\n  Returns:\n    a numpy array with shape [N, M] representing pairwise iou scores.\n  """"""\n  return np_box_ops.iou(boxlist1.get(), boxlist2.get())\n\n\ndef ioa(boxlist1, boxlist2):\n  """"""Computes pairwise intersection-over-area between box collections.\n\n  Intersection-over-area (ioa) between two boxes box1 and box2 is defined as\n  their intersection area over box2\'s area. Note that ioa is not symmetric,\n  that is, IOA(box1, box2) != IOA(box2, box1).\n\n  Args:\n    boxlist1: BoxList holding N boxes\n    boxlist2: BoxList holding M boxes\n\n  Returns:\n    a numpy array with shape [N, M] representing pairwise ioa scores.\n  """"""\n  return np_box_ops.ioa(boxlist1.get(), boxlist2.get())\n\n\ndef gather(boxlist, indices, fields=None):\n  """"""Gather boxes from BoxList according to indices and return new BoxList.\n\n  By default, gather returns boxes corresponding to the input index list, as\n  well as all additional fields stored in the boxlist (indexing into the\n  first dimension).  However one can optionally only gather from a\n  subset of fields.\n\n  Args:\n    boxlist: BoxList holding N boxes\n    indices: a 1-d numpy array of type int_\n    fields: (optional) list of fields to also gather from.  If None (default),\n        all fields are gathered from.  Pass an empty fields list to only gather\n        the box coordinates.\n\n  Returns:\n    subboxlist: a BoxList corresponding to the subset of the input BoxList\n        specified by indices\n\n  Raises:\n    ValueError: if specified field is not contained in boxlist or if the\n        indices are not of type int_\n  """"""\n  if indices.size:\n    if np.amax(indices) >= boxlist.num_boxes() or np.amin(indices) < 0:\n      raise ValueError(\'indices are out of valid range.\')\n  subboxlist = np_box_list.BoxList(boxlist.get()[indices, :])\n  if fields is None:\n    fields = boxlist.get_extra_fields()\n  for field in fields:\n    extra_field_data = boxlist.get_field(field)\n    subboxlist.add_field(field, extra_field_data[indices, ...])\n  return subboxlist\n\n\ndef sort_by_field(boxlist, field, order=SortOrder.DESCEND):\n  """"""Sort boxes and associated fields according to a scalar field.\n\n  A common use case is reordering the boxes according to descending scores.\n\n  Args:\n    boxlist: BoxList holding N boxes.\n    field: A BoxList field for sorting and reordering the BoxList.\n    order: (Optional) \'descend\' or \'ascend\'. Default is descend.\n\n  Returns:\n    sorted_boxlist: A sorted BoxList with the field in the specified order.\n\n  Raises:\n    ValueError: if specified field does not exist or is not of single dimension.\n    ValueError: if the order is not either descend or ascend.\n  """"""\n  if not boxlist.has_field(field):\n    raise ValueError(\'Field \' + field + \' does not exist\')\n  if len(boxlist.get_field(field).shape) != 1:\n    raise ValueError(\'Field \' + field + \'should be single dimension.\')\n  if order != SortOrder.DESCEND and order != SortOrder.ASCEND:\n    raise ValueError(\'Invalid sort order\')\n\n  field_to_sort = boxlist.get_field(field)\n  sorted_indices = np.argsort(field_to_sort)\n  if order == SortOrder.DESCEND:\n    sorted_indices = sorted_indices[::-1]\n  return gather(boxlist, sorted_indices)\n\n\ndef non_max_suppression(boxlist,\n                        max_output_size=10000,\n                        iou_threshold=1.0,\n                        score_threshold=-10.0):\n  """"""Non maximum suppression.\n\n  This op greedily selects a subset of detection bounding boxes, pruning\n  away boxes that have high IOU (intersection over union) overlap (> thresh)\n  with already selected boxes. In each iteration, the detected bounding box with\n  highest score in the available pool is selected.\n\n  Args:\n    boxlist: BoxList holding N boxes.  Must contain a \'scores\' field\n      representing detection scores. All scores belong to the same class.\n    max_output_size: maximum number of retained boxes\n    iou_threshold: intersection over union threshold.\n    score_threshold: minimum score threshold. Remove the boxes with scores\n                     less than this value. Default value is set to -10. A very\n                     low threshold to pass pretty much all the boxes, unless\n                     the user sets a different score threshold.\n\n  Returns:\n    a BoxList holding M boxes where M <= max_output_size\n  Raises:\n    ValueError: if \'scores\' field does not exist\n    ValueError: if threshold is not in [0, 1]\n    ValueError: if max_output_size < 0\n  """"""\n  if not boxlist.has_field(\'scores\'):\n    raise ValueError(\'Field scores does not exist\')\n  if iou_threshold < 0. or iou_threshold > 1.0:\n    raise ValueError(\'IOU threshold must be in [0, 1]\')\n  if max_output_size < 0:\n    raise ValueError(\'max_output_size must be bigger than 0.\')\n\n  boxlist = filter_scores_greater_than(boxlist, score_threshold)\n  if boxlist.num_boxes() == 0:\n    return boxlist\n\n  boxlist = sort_by_field(boxlist, \'scores\')\n\n  # Prevent further computation if NMS is disabled.\n  if iou_threshold == 1.0:\n    if boxlist.num_boxes() > max_output_size:\n      selected_indices = np.arange(max_output_size)\n      return gather(boxlist, selected_indices)\n    else:\n      return boxlist\n\n  boxes = boxlist.get()\n  num_boxes = boxlist.num_boxes()\n  # is_index_valid is True only for all remaining valid boxes,\n  is_index_valid = np.full(num_boxes, 1, dtype=bool)\n  selected_indices = []\n  num_output = 0\n  for i in range(num_boxes):\n    if num_output < max_output_size:\n      if is_index_valid[i]:\n        num_output += 1\n        selected_indices.append(i)\n        is_index_valid[i] = False\n        valid_indices = np.where(is_index_valid)[0]\n        if valid_indices.size == 0:\n          break\n\n        intersect_over_union = np_box_ops.iou(\n            np.expand_dims(boxes[i, :], axis=0), boxes[valid_indices, :])\n        intersect_over_union = np.squeeze(intersect_over_union, axis=0)\n        is_index_valid[valid_indices] = np.logical_and(\n            is_index_valid[valid_indices],\n            intersect_over_union <= iou_threshold)\n  return gather(boxlist, np.array(selected_indices))\n\n\ndef multi_class_non_max_suppression(boxlist, score_thresh, iou_thresh,\n                                    max_output_size):\n  """"""Multi-class version of non maximum suppression.\n\n  This op greedily selects a subset of detection bounding boxes, pruning\n  away boxes that have high IOU (intersection over union) overlap (> thresh)\n  with already selected boxes.  It operates independently for each class for\n  which scores are provided (via the scores field of the input box_list),\n  pruning boxes with score less than a provided threshold prior to\n  applying NMS.\n\n  Args:\n    boxlist: BoxList holding N boxes.  Must contain a \'scores\' field\n      representing detection scores.  This scores field is a tensor that can\n      be 1 dimensional (in the case of a single class) or 2-dimensional, which\n      which case we assume that it takes the shape [num_boxes, num_classes].\n      We further assume that this rank is known statically and that\n      scores.shape[1] is also known (i.e., the number of classes is fixed\n      and known at graph construction time).\n    score_thresh: scalar threshold for score (low scoring boxes are removed).\n    iou_thresh: scalar threshold for IOU (boxes that that high IOU overlap\n      with previously selected boxes are removed).\n    max_output_size: maximum number of retained boxes per class.\n\n  Returns:\n    a BoxList holding M boxes with a rank-1 scores field representing\n      corresponding scores for each box with scores sorted in decreasing order\n      and a rank-1 classes field representing a class label for each box.\n  Raises:\n    ValueError: if iou_thresh is not in [0, 1] or if input boxlist does not have\n      a valid scores field.\n  """"""\n  if not 0 <= iou_thresh <= 1.0:\n    raise ValueError(\'thresh must be between 0 and 1\')\n  if not isinstance(boxlist, np_box_list.BoxList):\n    raise ValueError(\'boxlist must be a BoxList\')\n  if not boxlist.has_field(\'scores\'):\n    raise ValueError(\'input boxlist must have \\\'scores\\\' field\')\n  scores = boxlist.get_field(\'scores\')\n  if len(scores.shape) == 1:\n    scores = np.reshape(scores, [-1, 1])\n  elif len(scores.shape) == 2:\n    if scores.shape[1] is None:\n      raise ValueError(\'scores field must have statically defined second \'\n                       \'dimension\')\n  else:\n    raise ValueError(\'scores field must be of rank 1 or 2\')\n  num_boxes = boxlist.num_boxes()\n  num_scores = scores.shape[0]\n  num_classes = scores.shape[1]\n\n  if num_boxes != num_scores:\n    raise ValueError(\'Incorrect scores field length: actual vs expected.\')\n\n  selected_boxes_list = []\n  for class_idx in range(num_classes):\n    boxlist_and_class_scores = np_box_list.BoxList(boxlist.get())\n    class_scores = np.reshape(scores[0:num_scores, class_idx], [-1])\n    boxlist_and_class_scores.add_field(\'scores\', class_scores)\n    boxlist_filt = filter_scores_greater_than(boxlist_and_class_scores,\n                                              score_thresh)\n    nms_result = non_max_suppression(boxlist_filt,\n                                     max_output_size=max_output_size,\n                                     iou_threshold=iou_thresh,\n                                     score_threshold=score_thresh)\n    nms_result.add_field(\n        \'classes\', np.zeros_like(nms_result.get_field(\'scores\')) + class_idx)\n    selected_boxes_list.append(nms_result)\n  selected_boxes = concatenate(selected_boxes_list)\n  sorted_boxes = sort_by_field(selected_boxes, \'scores\')\n  return sorted_boxes\n\n\ndef scale(boxlist, y_scale, x_scale):\n  """"""Scale box coordinates in x and y dimensions.\n\n  Args:\n    boxlist: BoxList holding N boxes\n    y_scale: float\n    x_scale: float\n\n  Returns:\n    boxlist: BoxList holding N boxes\n  """"""\n  y_min, x_min, y_max, x_max = np.array_split(boxlist.get(), 4, axis=1)\n  y_min = y_scale * y_min\n  y_max = y_scale * y_max\n  x_min = x_scale * x_min\n  x_max = x_scale * x_max\n  scaled_boxlist = np_box_list.BoxList(np.hstack([y_min, x_min, y_max, x_max]))\n\n  fields = boxlist.get_extra_fields()\n  for field in fields:\n    extra_field_data = boxlist.get_field(field)\n    scaled_boxlist.add_field(field, extra_field_data)\n\n  return scaled_boxlist\n\n\ndef clip_to_window(boxlist, window):\n  """"""Clip bounding boxes to a window.\n\n  This op clips input bounding boxes (represented by bounding box\n  corners) to a window, optionally filtering out boxes that do not\n  overlap at all with the window.\n\n  Args:\n    boxlist: BoxList holding M_in boxes\n    window: a numpy array of shape [4] representing the\n            [y_min, x_min, y_max, x_max] window to which the op\n            should clip boxes.\n\n  Returns:\n    a BoxList holding M_out boxes where M_out <= M_in\n  """"""\n  y_min, x_min, y_max, x_max = np.array_split(boxlist.get(), 4, axis=1)\n  win_y_min = window[0]\n  win_x_min = window[1]\n  win_y_max = window[2]\n  win_x_max = window[3]\n  y_min_clipped = np.fmax(np.fmin(y_min, win_y_max), win_y_min)\n  y_max_clipped = np.fmax(np.fmin(y_max, win_y_max), win_y_min)\n  x_min_clipped = np.fmax(np.fmin(x_min, win_x_max), win_x_min)\n  x_max_clipped = np.fmax(np.fmin(x_max, win_x_max), win_x_min)\n  clipped = np_box_list.BoxList(\n      np.hstack([y_min_clipped, x_min_clipped, y_max_clipped, x_max_clipped]))\n  clipped = _copy_extra_fields(clipped, boxlist)\n  areas = area(clipped)\n  nonzero_area_indices = np.reshape(np.nonzero(np.greater(areas, 0.0)),\n                                    [-1]).astype(np.int32)\n  return gather(clipped, nonzero_area_indices)\n\n\ndef prune_non_overlapping_boxes(boxlist1, boxlist2, minoverlap=0.0):\n  """"""Prunes the boxes in boxlist1 that overlap less than thresh with boxlist2.\n\n  For each box in boxlist1, we want its IOA to be more than minoverlap with\n  at least one of the boxes in boxlist2. If it does not, we remove it.\n\n  Args:\n    boxlist1: BoxList holding N boxes.\n    boxlist2: BoxList holding M boxes.\n    minoverlap: Minimum required overlap between boxes, to count them as\n                overlapping.\n\n  Returns:\n    A pruned boxlist with size [N\', 4].\n  """"""\n  intersection_over_area = ioa(boxlist2, boxlist1)  # [M, N] tensor\n  intersection_over_area = np.amax(intersection_over_area, axis=0)  # [N] tensor\n  keep_bool = np.greater_equal(intersection_over_area, np.array(minoverlap))\n  keep_inds = np.nonzero(keep_bool)[0]\n  new_boxlist1 = gather(boxlist1, keep_inds)\n  return new_boxlist1\n\n\ndef prune_outside_window(boxlist, window):\n  """"""Prunes bounding boxes that fall outside a given window.\n\n  This function prunes bounding boxes that even partially fall outside the given\n  window. See also ClipToWindow which only prunes bounding boxes that fall\n  completely outside the window, and clips any bounding boxes that partially\n  overflow.\n\n  Args:\n    boxlist: a BoxList holding M_in boxes.\n    window: a numpy array of size 4, representing [ymin, xmin, ymax, xmax]\n            of the window.\n\n  Returns:\n    pruned_corners: a tensor with shape [M_out, 4] where M_out <= M_in.\n    valid_indices: a tensor with shape [M_out] indexing the valid bounding boxes\n     in the input tensor.\n  """"""\n\n  y_min, x_min, y_max, x_max = np.array_split(boxlist.get(), 4, axis=1)\n  win_y_min = window[0]\n  win_x_min = window[1]\n  win_y_max = window[2]\n  win_x_max = window[3]\n  coordinate_violations = np.hstack([np.less(y_min, win_y_min),\n                                     np.less(x_min, win_x_min),\n                                     np.greater(y_max, win_y_max),\n                                     np.greater(x_max, win_x_max)])\n  valid_indices = np.reshape(\n      np.where(np.logical_not(np.max(coordinate_violations, axis=1))), [-1])\n  return gather(boxlist, valid_indices), valid_indices\n\n\ndef concatenate(boxlists, fields=None):\n  """"""Concatenate list of BoxLists.\n\n  This op concatenates a list of input BoxLists into a larger BoxList.  It also\n  handles concatenation of BoxList fields as long as the field tensor shapes\n  are equal except for the first dimension.\n\n  Args:\n    boxlists: list of BoxList objects\n    fields: optional list of fields to also concatenate.  By default, all\n      fields from the first BoxList in the list are included in the\n      concatenation.\n\n  Returns:\n    a BoxList with number of boxes equal to\n      sum([boxlist.num_boxes() for boxlist in BoxList])\n  Raises:\n    ValueError: if boxlists is invalid (i.e., is not a list, is empty, or\n      contains non BoxList objects), or if requested fields are not contained in\n      all boxlists\n  """"""\n  if not isinstance(boxlists, list):\n    raise ValueError(\'boxlists should be a list\')\n  if not boxlists:\n    raise ValueError(\'boxlists should have nonzero length\')\n  for boxlist in boxlists:\n    if not isinstance(boxlist, np_box_list.BoxList):\n      raise ValueError(\'all elements of boxlists should be BoxList objects\')\n  concatenated = np_box_list.BoxList(\n      np.vstack([boxlist.get() for boxlist in boxlists]))\n  if fields is None:\n    fields = boxlists[0].get_extra_fields()\n  for field in fields:\n    first_field_shape = boxlists[0].get_field(field).shape\n    first_field_shape = first_field_shape[1:]\n    for boxlist in boxlists:\n      if not boxlist.has_field(field):\n        raise ValueError(\'boxlist must contain all requested fields\')\n      field_shape = boxlist.get_field(field).shape\n      field_shape = field_shape[1:]\n      if field_shape != first_field_shape:\n        raise ValueError(\'field %s must have same shape for all boxlists \'\n                         \'except for the 0th dimension.\' % field)\n    concatenated_field = np.concatenate(\n        [boxlist.get_field(field) for boxlist in boxlists], axis=0)\n    concatenated.add_field(field, concatenated_field)\n  return concatenated\n\n\ndef filter_scores_greater_than(boxlist, thresh):\n  """"""Filter to keep only boxes with score exceeding a given threshold.\n\n  This op keeps the collection of boxes whose corresponding scores are\n  greater than the input threshold.\n\n  Args:\n    boxlist: BoxList holding N boxes.  Must contain a \'scores\' field\n      representing detection scores.\n    thresh: scalar threshold\n\n  Returns:\n    a BoxList holding M boxes where M <= N\n\n  Raises:\n    ValueError: if boxlist not a BoxList object or if it does not\n      have a scores field\n  """"""\n  if not isinstance(boxlist, np_box_list.BoxList):\n    raise ValueError(\'boxlist must be a BoxList\')\n  if not boxlist.has_field(\'scores\'):\n    raise ValueError(\'input boxlist must have \\\'scores\\\' field\')\n  scores = boxlist.get_field(\'scores\')\n  if len(scores.shape) > 2:\n    raise ValueError(\'Scores should have rank 1 or 2\')\n  if len(scores.shape) == 2 and scores.shape[1] != 1:\n    raise ValueError(\'Scores should have rank 1 or have shape \'\n                     \'consistent with [None, 1]\')\n  high_score_indices = np.reshape(np.where(np.greater(scores, thresh)),\n                                  [-1]).astype(np.int32)\n  return gather(boxlist, high_score_indices)\n\n\ndef change_coordinate_frame(boxlist, window):\n  """"""Change coordinate frame of the boxlist to be relative to window\'s frame.\n\n  Given a window of the form [ymin, xmin, ymax, xmax],\n  changes bounding box coordinates from boxlist to be relative to this window\n  (e.g., the min corner maps to (0,0) and the max corner maps to (1,1)).\n\n  An example use case is data augmentation: where we are given groundtruth\n  boxes (boxlist) and would like to randomly crop the image to some\n  window (window). In this case we need to change the coordinate frame of\n  each groundtruth box to be relative to this new window.\n\n  Args:\n    boxlist: A BoxList object holding N boxes.\n    window: a size 4 1-D numpy array.\n\n  Returns:\n    Returns a BoxList object with N boxes.\n  """"""\n  win_height = window[2] - window[0]\n  win_width = window[3] - window[1]\n  boxlist_new = scale(\n      np_box_list.BoxList(boxlist.get() -\n                          [window[0], window[1], window[0], window[1]]),\n      1.0 / win_height, 1.0 / win_width)\n  _copy_extra_fields(boxlist_new, boxlist)\n\n  return boxlist_new\n\n\ndef _copy_extra_fields(boxlist_to_copy_to, boxlist_to_copy_from):\n  """"""Copies the extra fields of boxlist_to_copy_from to boxlist_to_copy_to.\n\n  Args:\n    boxlist_to_copy_to: BoxList to which extra fields are copied.\n    boxlist_to_copy_from: BoxList from which fields are copied.\n\n  Returns:\n    boxlist_to_copy_to with extra fields.\n  """"""\n  for field in boxlist_to_copy_from.get_extra_fields():\n    boxlist_to_copy_to.add_field(field, boxlist_to_copy_from.get_field(field))\n  return boxlist_to_copy_to\n\n\ndef _update_valid_indices_by_removing_high_iou_boxes(\n    selected_indices, is_index_valid, intersect_over_union, threshold):\n  max_iou = np.max(intersect_over_union[:, selected_indices], axis=1)\n  return np.logical_and(is_index_valid, max_iou <= threshold)\n'"
src/object_detection/utils/np_box_list_ops_test.py,5,"b'# Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n""""""Tests for object_detection.utils.np_box_list_ops.""""""\n\nimport numpy as np\nimport tensorflow as tf\n\nfrom object_detection.utils import np_box_list\nfrom object_detection.utils import np_box_list_ops\n\n\nclass AreaRelatedTest(tf.test.TestCase):\n\n  def setUp(self):\n    boxes1 = np.array([[4.0, 3.0, 7.0, 5.0], [5.0, 6.0, 10.0, 7.0]],\n                      dtype=float)\n    boxes2 = np.array([[3.0, 4.0, 6.0, 8.0], [14.0, 14.0, 15.0, 15.0],\n                       [0.0, 0.0, 20.0, 20.0]],\n                      dtype=float)\n    self.boxlist1 = np_box_list.BoxList(boxes1)\n    self.boxlist2 = np_box_list.BoxList(boxes2)\n\n  def test_area(self):\n    areas = np_box_list_ops.area(self.boxlist1)\n    expected_areas = np.array([6.0, 5.0], dtype=float)\n    self.assertAllClose(expected_areas, areas)\n\n  def test_intersection(self):\n    intersection = np_box_list_ops.intersection(self.boxlist1, self.boxlist2)\n    expected_intersection = np.array([[2.0, 0.0, 6.0], [1.0, 0.0, 5.0]],\n                                     dtype=float)\n    self.assertAllClose(intersection, expected_intersection)\n\n  def test_iou(self):\n    iou = np_box_list_ops.iou(self.boxlist1, self.boxlist2)\n    expected_iou = np.array([[2.0 / 16.0, 0.0, 6.0 / 400.0],\n                             [1.0 / 16.0, 0.0, 5.0 / 400.0]],\n                            dtype=float)\n    self.assertAllClose(iou, expected_iou)\n\n  def test_ioa(self):\n    boxlist1 = np_box_list.BoxList(\n        np.array(\n            [[0.25, 0.25, 0.75, 0.75], [0.0, 0.0, 0.5, 0.75]], dtype=\n            np.float32))\n    boxlist2 = np_box_list.BoxList(\n        np.array(\n            [[0.5, 0.25, 1.0, 1.0], [0.0, 0.0, 1.0, 1.0]], dtype=np.float32))\n    ioa21 = np_box_list_ops.ioa(boxlist2, boxlist1)\n    expected_ioa21 = np.array([[0.5, 0.0],\n                               [1.0, 1.0]],\n                              dtype=np.float32)\n    self.assertAllClose(ioa21, expected_ioa21)\n\n  def test_scale(self):\n    boxlist = np_box_list.BoxList(\n        np.array(\n            [[0.25, 0.25, 0.75, 0.75], [0.0, 0.0, 0.5, 0.75]], dtype=\n            np.float32))\n    boxlist_scaled = np_box_list_ops.scale(boxlist, 2.0, 3.0)\n    expected_boxlist_scaled = np_box_list.BoxList(\n        np.array(\n            [[0.5, 0.75, 1.5, 2.25], [0.0, 0.0, 1.0, 2.25]], dtype=np.float32))\n    self.assertAllClose(expected_boxlist_scaled.get(), boxlist_scaled.get())\n\n  def test_clip_to_window(self):\n    boxlist = np_box_list.BoxList(\n        np.array(\n            [[0.25, 0.25, 0.75, 0.75], [0.0, 0.0, 0.5, 0.75],\n             [-0.2, -0.3, 0.7, 1.5]],\n            dtype=np.float32))\n    boxlist_clipped = np_box_list_ops.clip_to_window(boxlist,\n                                                     [0.0, 0.0, 1.0, 1.0])\n    expected_boxlist_clipped = np_box_list.BoxList(\n        np.array(\n            [[0.25, 0.25, 0.75, 0.75], [0.0, 0.0, 0.5, 0.75],\n             [0.0, 0.0, 0.7, 1.0]],\n            dtype=np.float32))\n    self.assertAllClose(expected_boxlist_clipped.get(), boxlist_clipped.get())\n\n  def test_prune_outside_window(self):\n    boxlist = np_box_list.BoxList(\n        np.array(\n            [[0.25, 0.25, 0.75, 0.75], [0.0, 0.0, 0.5, 0.75],\n             [-0.2, -0.3, 0.7, 1.5]],\n            dtype=np.float32))\n    boxlist_pruned, _ = np_box_list_ops.prune_outside_window(\n        boxlist, [0.0, 0.0, 1.0, 1.0])\n    expected_boxlist_pruned = np_box_list.BoxList(\n        np.array(\n            [[0.25, 0.25, 0.75, 0.75], [0.0, 0.0, 0.5, 0.75]], dtype=\n            np.float32))\n    self.assertAllClose(expected_boxlist_pruned.get(), boxlist_pruned.get())\n\n  def test_concatenate(self):\n    boxlist1 = np_box_list.BoxList(\n        np.array(\n            [[0.25, 0.25, 0.75, 0.75], [0.0, 0.0, 0.5, 0.75]], dtype=\n            np.float32))\n    boxlist2 = np_box_list.BoxList(\n        np.array(\n            [[0.5, 0.25, 1.0, 1.0], [0.0, 0.0, 1.0, 1.0]], dtype=np.float32))\n    boxlists = [boxlist1, boxlist2]\n    boxlist_concatenated = np_box_list_ops.concatenate(boxlists)\n    boxlist_concatenated_expected = np_box_list.BoxList(\n        np.array(\n            [[0.25, 0.25, 0.75, 0.75], [0.0, 0.0, 0.5, 0.75],\n             [0.5, 0.25, 1.0, 1.0], [0.0, 0.0, 1.0, 1.0]],\n            dtype=np.float32))\n    self.assertAllClose(boxlist_concatenated_expected.get(),\n                        boxlist_concatenated.get())\n\n  def test_change_coordinate_frame(self):\n    boxlist = np_box_list.BoxList(\n        np.array(\n            [[0.25, 0.25, 0.75, 0.75], [0.0, 0.0, 0.5, 0.75]], dtype=\n            np.float32))\n    boxlist_coord = np_box_list_ops.change_coordinate_frame(\n        boxlist, np.array([0, 0, 0.5, 0.5], dtype=np.float32))\n    expected_boxlist_coord = np_box_list.BoxList(\n        np.array([[0.5, 0.5, 1.5, 1.5], [0, 0, 1.0, 1.5]], dtype=np.float32))\n    self.assertAllClose(boxlist_coord.get(), expected_boxlist_coord.get())\n\n  def test_filter_scores_greater_than(self):\n    boxlist = np_box_list.BoxList(\n        np.array(\n            [[0.25, 0.25, 0.75, 0.75], [0.0, 0.0, 0.5, 0.75]], dtype=\n            np.float32))\n    boxlist.add_field(\'scores\', np.array([0.8, 0.2], np.float32))\n    boxlist_greater = np_box_list_ops.filter_scores_greater_than(boxlist, 0.5)\n\n    expected_boxlist_greater = np_box_list.BoxList(\n        np.array([[0.25, 0.25, 0.75, 0.75]], dtype=np.float32))\n\n    self.assertAllClose(boxlist_greater.get(), expected_boxlist_greater.get())\n\n\nclass GatherOpsTest(tf.test.TestCase):\n\n  def setUp(self):\n    boxes = np.array([[3.0, 4.0, 6.0, 8.0], [14.0, 14.0, 15.0, 15.0],\n                      [0.0, 0.0, 20.0, 20.0]],\n                     dtype=float)\n    self.boxlist = np_box_list.BoxList(boxes)\n    self.boxlist.add_field(\'scores\', np.array([0.5, 0.7, 0.9], dtype=float))\n    self.boxlist.add_field(\'labels\',\n                           np.array([[0, 0, 0, 1, 0], [0, 1, 0, 0, 0],\n                                     [0, 0, 0, 0, 1]],\n                                    dtype=int))\n\n  def test_gather_with_out_of_range_indices(self):\n    indices = np.array([3, 1], dtype=int)\n    boxlist = self.boxlist\n    with self.assertRaises(ValueError):\n      np_box_list_ops.gather(boxlist, indices)\n\n  def test_gather_with_invalid_multidimensional_indices(self):\n    indices = np.array([[0, 1], [1, 2]], dtype=int)\n    boxlist = self.boxlist\n    with self.assertRaises(ValueError):\n      np_box_list_ops.gather(boxlist, indices)\n\n  def test_gather_without_fields_specified(self):\n    indices = np.array([2, 0, 1], dtype=int)\n    boxlist = self.boxlist\n    subboxlist = np_box_list_ops.gather(boxlist, indices)\n\n    expected_scores = np.array([0.9, 0.5, 0.7], dtype=float)\n    self.assertAllClose(expected_scores, subboxlist.get_field(\'scores\'))\n\n    expected_boxes = np.array([[0.0, 0.0, 20.0, 20.0], [3.0, 4.0, 6.0, 8.0],\n                               [14.0, 14.0, 15.0, 15.0]],\n                              dtype=float)\n    self.assertAllClose(expected_boxes, subboxlist.get())\n\n    expected_labels = np.array([[0, 0, 0, 0, 1], [0, 0, 0, 1, 0],\n                                [0, 1, 0, 0, 0]],\n                               dtype=int)\n    self.assertAllClose(expected_labels, subboxlist.get_field(\'labels\'))\n\n  def test_gather_with_invalid_field_specified(self):\n    indices = np.array([2, 0, 1], dtype=int)\n    boxlist = self.boxlist\n\n    with self.assertRaises(ValueError):\n      np_box_list_ops.gather(boxlist, indices, \'labels\')\n\n    with self.assertRaises(ValueError):\n      np_box_list_ops.gather(boxlist, indices, [\'objectness\'])\n\n  def test_gather_with_fields_specified(self):\n    indices = np.array([2, 0, 1], dtype=int)\n    boxlist = self.boxlist\n    subboxlist = np_box_list_ops.gather(boxlist, indices, [\'labels\'])\n\n    self.assertFalse(subboxlist.has_field(\'scores\'))\n\n    expected_boxes = np.array([[0.0, 0.0, 20.0, 20.0], [3.0, 4.0, 6.0, 8.0],\n                               [14.0, 14.0, 15.0, 15.0]],\n                              dtype=float)\n    self.assertAllClose(expected_boxes, subboxlist.get())\n\n    expected_labels = np.array([[0, 0, 0, 0, 1], [0, 0, 0, 1, 0],\n                                [0, 1, 0, 0, 0]],\n                               dtype=int)\n    self.assertAllClose(expected_labels, subboxlist.get_field(\'labels\'))\n\n\nclass SortByFieldTest(tf.test.TestCase):\n\n  def setUp(self):\n    boxes = np.array([[3.0, 4.0, 6.0, 8.0], [14.0, 14.0, 15.0, 15.0],\n                      [0.0, 0.0, 20.0, 20.0]],\n                     dtype=float)\n    self.boxlist = np_box_list.BoxList(boxes)\n    self.boxlist.add_field(\'scores\', np.array([0.5, 0.9, 0.4], dtype=float))\n    self.boxlist.add_field(\'labels\',\n                           np.array([[0, 0, 0, 1, 0], [0, 1, 0, 0, 0],\n                                     [0, 0, 0, 0, 1]],\n                                    dtype=int))\n\n  def test_with_invalid_field(self):\n    with self.assertRaises(ValueError):\n      np_box_list_ops.sort_by_field(self.boxlist, \'objectness\')\n    with self.assertRaises(ValueError):\n      np_box_list_ops.sort_by_field(self.boxlist, \'labels\')\n\n  def test_with_invalid_sorting_order(self):\n    with self.assertRaises(ValueError):\n      np_box_list_ops.sort_by_field(self.boxlist, \'scores\', \'Descending\')\n\n  def test_with_descending_sorting(self):\n    sorted_boxlist = np_box_list_ops.sort_by_field(self.boxlist, \'scores\')\n\n    expected_boxes = np.array([[14.0, 14.0, 15.0, 15.0], [3.0, 4.0, 6.0, 8.0],\n                               [0.0, 0.0, 20.0, 20.0]],\n                              dtype=float)\n    self.assertAllClose(expected_boxes, sorted_boxlist.get())\n\n    expected_scores = np.array([0.9, 0.5, 0.4], dtype=float)\n    self.assertAllClose(expected_scores, sorted_boxlist.get_field(\'scores\'))\n\n  def test_with_ascending_sorting(self):\n    sorted_boxlist = np_box_list_ops.sort_by_field(\n        self.boxlist, \'scores\', np_box_list_ops.SortOrder.ASCEND)\n\n    expected_boxes = np.array([[0.0, 0.0, 20.0, 20.0],\n                               [3.0, 4.0, 6.0, 8.0],\n                               [14.0, 14.0, 15.0, 15.0],],\n                              dtype=float)\n    self.assertAllClose(expected_boxes, sorted_boxlist.get())\n\n    expected_scores = np.array([0.4, 0.5, 0.9], dtype=float)\n    self.assertAllClose(expected_scores, sorted_boxlist.get_field(\'scores\'))\n\n\nclass NonMaximumSuppressionTest(tf.test.TestCase):\n\n  def setUp(self):\n    self._boxes = np.array([[0, 0, 1, 1],\n                            [0, 0.1, 1, 1.1],\n                            [0, -0.1, 1, 0.9],\n                            [0, 10, 1, 11],\n                            [0, 10.1, 1, 11.1],\n                            [0, 100, 1, 101]],\n                           dtype=float)\n    self._boxlist = np_box_list.BoxList(self._boxes)\n\n  def test_with_no_scores_field(self):\n    boxlist = np_box_list.BoxList(self._boxes)\n    max_output_size = 3\n    iou_threshold = 0.5\n\n    with self.assertRaises(ValueError):\n      np_box_list_ops.non_max_suppression(\n          boxlist, max_output_size, iou_threshold)\n\n  def test_nms_disabled_max_output_size_equals_three(self):\n    boxlist = np_box_list.BoxList(self._boxes)\n    boxlist.add_field(\'scores\',\n                      np.array([.9, .75, .6, .95, .2, .3], dtype=float))\n    max_output_size = 3\n    iou_threshold = 1.  # No NMS\n\n    expected_boxes = np.array([[0, 10, 1, 11], [0, 0, 1, 1], [0, 0.1, 1, 1.1]],\n                              dtype=float)\n    nms_boxlist = np_box_list_ops.non_max_suppression(\n        boxlist, max_output_size, iou_threshold)\n    self.assertAllClose(nms_boxlist.get(), expected_boxes)\n\n  def test_select_from_three_clusters(self):\n    boxlist = np_box_list.BoxList(self._boxes)\n    boxlist.add_field(\'scores\',\n                      np.array([.9, .75, .6, .95, .2, .3], dtype=float))\n    max_output_size = 3\n    iou_threshold = 0.5\n\n    expected_boxes = np.array([[0, 10, 1, 11], [0, 0, 1, 1], [0, 100, 1, 101]],\n                              dtype=float)\n    nms_boxlist = np_box_list_ops.non_max_suppression(\n        boxlist, max_output_size, iou_threshold)\n    self.assertAllClose(nms_boxlist.get(), expected_boxes)\n\n  def test_select_at_most_two_from_three_clusters(self):\n    boxlist = np_box_list.BoxList(self._boxes)\n    boxlist.add_field(\'scores\',\n                      np.array([.9, .75, .6, .95, .5, .3], dtype=float))\n    max_output_size = 2\n    iou_threshold = 0.5\n\n    expected_boxes = np.array([[0, 10, 1, 11], [0, 0, 1, 1]], dtype=float)\n    nms_boxlist = np_box_list_ops.non_max_suppression(\n        boxlist, max_output_size, iou_threshold)\n    self.assertAllClose(nms_boxlist.get(), expected_boxes)\n\n  def test_select_at_most_thirty_from_three_clusters(self):\n    boxlist = np_box_list.BoxList(self._boxes)\n    boxlist.add_field(\'scores\',\n                      np.array([.9, .75, .6, .95, .5, .3], dtype=float))\n    max_output_size = 30\n    iou_threshold = 0.5\n\n    expected_boxes = np.array([[0, 10, 1, 11], [0, 0, 1, 1], [0, 100, 1, 101]],\n                              dtype=float)\n    nms_boxlist = np_box_list_ops.non_max_suppression(\n        boxlist, max_output_size, iou_threshold)\n    self.assertAllClose(nms_boxlist.get(), expected_boxes)\n\n  def test_select_from_ten_indentical_boxes(self):\n    boxes = np.array(10 * [[0, 0, 1, 1]], dtype=float)\n    boxlist = np_box_list.BoxList(boxes)\n    boxlist.add_field(\'scores\', np.array(10 * [0.8]))\n    iou_threshold = .5\n    max_output_size = 3\n    expected_boxes = np.array([[0, 0, 1, 1]], dtype=float)\n    nms_boxlist = np_box_list_ops.non_max_suppression(\n        boxlist, max_output_size, iou_threshold)\n    self.assertAllClose(nms_boxlist.get(), expected_boxes)\n\n  def test_different_iou_threshold(self):\n    boxes = np.array([[0, 0, 20, 100], [0, 0, 20, 80], [200, 200, 210, 300],\n                      [200, 200, 210, 250]],\n                     dtype=float)\n    boxlist = np_box_list.BoxList(boxes)\n    boxlist.add_field(\'scores\', np.array([0.9, 0.8, 0.7, 0.6]))\n    max_output_size = 4\n\n    iou_threshold = .4\n    expected_boxes = np.array([[0, 0, 20, 100],\n                               [200, 200, 210, 300],],\n                              dtype=float)\n    nms_boxlist = np_box_list_ops.non_max_suppression(\n        boxlist, max_output_size, iou_threshold)\n    self.assertAllClose(nms_boxlist.get(), expected_boxes)\n\n    iou_threshold = .5\n    expected_boxes = np.array([[0, 0, 20, 100], [200, 200, 210, 300],\n                               [200, 200, 210, 250]],\n                              dtype=float)\n    nms_boxlist = np_box_list_ops.non_max_suppression(\n        boxlist, max_output_size, iou_threshold)\n    self.assertAllClose(nms_boxlist.get(), expected_boxes)\n\n    iou_threshold = .8\n    expected_boxes = np.array([[0, 0, 20, 100], [0, 0, 20, 80],\n                               [200, 200, 210, 300], [200, 200, 210, 250]],\n                              dtype=float)\n    nms_boxlist = np_box_list_ops.non_max_suppression(\n        boxlist, max_output_size, iou_threshold)\n    self.assertAllClose(nms_boxlist.get(), expected_boxes)\n\n  def test_multiclass_nms(self):\n    boxlist = np_box_list.BoxList(\n        np.array(\n            [[0.2, 0.4, 0.8, 0.8], [0.4, 0.2, 0.8, 0.8], [0.6, 0.0, 1.0, 1.0]],\n            dtype=np.float32))\n    scores = np.array([[-0.2, 0.1, 0.5, -0.4, 0.3],\n                       [0.7, -0.7, 0.6, 0.2, -0.9],\n                       [0.4, 0.34, -0.9, 0.2, 0.31]],\n                      dtype=np.float32)\n    boxlist.add_field(\'scores\', scores)\n    boxlist_clean = np_box_list_ops.multi_class_non_max_suppression(\n        boxlist, score_thresh=0.25, iou_thresh=0.1, max_output_size=3)\n\n    scores_clean = boxlist_clean.get_field(\'scores\')\n    classes_clean = boxlist_clean.get_field(\'classes\')\n    boxes = boxlist_clean.get()\n    expected_scores = np.array([0.7, 0.6, 0.34, 0.31])\n    expected_classes = np.array([0, 2, 1, 4])\n    expected_boxes = np.array([[0.4, 0.2, 0.8, 0.8],\n                               [0.4, 0.2, 0.8, 0.8],\n                               [0.6, 0.0, 1.0, 1.0],\n                               [0.6, 0.0, 1.0, 1.0]],\n                              dtype=np.float32)\n    self.assertAllClose(scores_clean, expected_scores)\n    self.assertAllClose(classes_clean, expected_classes)\n    self.assertAllClose(boxes, expected_boxes)\n\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
src/object_detection/utils/np_box_list_test.py,3,"b'# Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n""""""Tests for object_detection.utils.np_box_list_test.""""""\n\nimport numpy as np\nimport tensorflow as tf\n\nfrom object_detection.utils import np_box_list\n\n\nclass BoxListTest(tf.test.TestCase):\n\n  def test_invalid_box_data(self):\n    with self.assertRaises(ValueError):\n      np_box_list.BoxList([0, 0, 1, 1])\n\n    with self.assertRaises(ValueError):\n      np_box_list.BoxList(np.array([[0, 0, 1, 1]], dtype=int))\n\n    with self.assertRaises(ValueError):\n      np_box_list.BoxList(np.array([0, 1, 1, 3, 4], dtype=float))\n\n    with self.assertRaises(ValueError):\n      np_box_list.BoxList(np.array([[0, 1, 1, 3], [3, 1, 1, 5]], dtype=float))\n\n  def test_has_field_with_existed_field(self):\n    boxes = np.array([[3.0, 4.0, 6.0, 8.0], [14.0, 14.0, 15.0, 15.0],\n                      [0.0, 0.0, 20.0, 20.0]],\n                     dtype=float)\n    boxlist = np_box_list.BoxList(boxes)\n    self.assertTrue(boxlist.has_field(\'boxes\'))\n\n  def test_has_field_with_nonexisted_field(self):\n    boxes = np.array([[3.0, 4.0, 6.0, 8.0], [14.0, 14.0, 15.0, 15.0],\n                      [0.0, 0.0, 20.0, 20.0]],\n                     dtype=float)\n    boxlist = np_box_list.BoxList(boxes)\n    self.assertFalse(boxlist.has_field(\'scores\'))\n\n  def test_get_field_with_existed_field(self):\n    boxes = np.array([[3.0, 4.0, 6.0, 8.0], [14.0, 14.0, 15.0, 15.0],\n                      [0.0, 0.0, 20.0, 20.0]],\n                     dtype=float)\n    boxlist = np_box_list.BoxList(boxes)\n    self.assertTrue(np.allclose(boxlist.get_field(\'boxes\'), boxes))\n\n  def test_get_field_with_nonexited_field(self):\n    boxes = np.array([[3.0, 4.0, 6.0, 8.0], [14.0, 14.0, 15.0, 15.0],\n                      [0.0, 0.0, 20.0, 20.0]],\n                     dtype=float)\n    boxlist = np_box_list.BoxList(boxes)\n    with self.assertRaises(ValueError):\n      boxlist.get_field(\'scores\')\n\n\nclass AddExtraFieldTest(tf.test.TestCase):\n\n  def setUp(self):\n    boxes = np.array([[3.0, 4.0, 6.0, 8.0], [14.0, 14.0, 15.0, 15.0],\n                      [0.0, 0.0, 20.0, 20.0]],\n                     dtype=float)\n    self.boxlist = np_box_list.BoxList(boxes)\n\n  def test_add_already_existed_field(self):\n    with self.assertRaises(ValueError):\n      self.boxlist.add_field(\'boxes\', np.array([[0, 0, 0, 1, 0]], dtype=float))\n\n  def test_add_invalid_field_data(self):\n    with self.assertRaises(ValueError):\n      self.boxlist.add_field(\'scores\', np.array([0.5, 0.7], dtype=float))\n    with self.assertRaises(ValueError):\n      self.boxlist.add_field(\'scores\',\n                             np.array([0.5, 0.7, 0.9, 0.1], dtype=float))\n\n  def test_add_single_dimensional_field_data(self):\n    boxlist = self.boxlist\n    scores = np.array([0.5, 0.7, 0.9], dtype=float)\n    boxlist.add_field(\'scores\', scores)\n    self.assertTrue(np.allclose(scores, self.boxlist.get_field(\'scores\')))\n\n  def test_add_multi_dimensional_field_data(self):\n    boxlist = self.boxlist\n    labels = np.array([[0, 0, 0, 1, 0], [0, 1, 0, 0, 0], [0, 0, 0, 0, 1]],\n                      dtype=int)\n    boxlist.add_field(\'labels\', labels)\n    self.assertTrue(np.allclose(labels, self.boxlist.get_field(\'labels\')))\n\n  def test_get_extra_fields(self):\n    boxlist = self.boxlist\n    self.assertItemsEqual(boxlist.get_extra_fields(), [])\n\n    scores = np.array([0.5, 0.7, 0.9], dtype=float)\n    boxlist.add_field(\'scores\', scores)\n    self.assertItemsEqual(boxlist.get_extra_fields(), [\'scores\'])\n\n    labels = np.array([[0, 0, 0, 1, 0], [0, 1, 0, 0, 0], [0, 0, 0, 0, 1]],\n                      dtype=int)\n    boxlist.add_field(\'labels\', labels)\n    self.assertItemsEqual(boxlist.get_extra_fields(), [\'scores\', \'labels\'])\n\n  def test_get_coordinates(self):\n    y_min, x_min, y_max, x_max = self.boxlist.get_coordinates()\n\n    expected_y_min = np.array([3.0, 14.0, 0.0], dtype=float)\n    expected_x_min = np.array([4.0, 14.0, 0.0], dtype=float)\n    expected_y_max = np.array([6.0, 15.0, 20.0], dtype=float)\n    expected_x_max = np.array([8.0, 15.0, 20.0], dtype=float)\n\n    self.assertTrue(np.allclose(y_min, expected_y_min))\n    self.assertTrue(np.allclose(x_min, expected_x_min))\n    self.assertTrue(np.allclose(y_max, expected_y_max))\n    self.assertTrue(np.allclose(x_max, expected_x_max))\n\n  def test_num_boxes(self):\n    boxes = np.array([[0., 0., 100., 100.], [10., 30., 50., 70.]], dtype=float)\n    boxlist = np_box_list.BoxList(boxes)\n    expected_num_boxes = 2\n    self.assertEquals(boxlist.num_boxes(), expected_num_boxes)\n\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
src/object_detection/utils/np_box_mask_list.py,0,"b'# Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n""""""Numpy BoxMaskList classes and functions.""""""\n\nimport numpy as np\nfrom object_detection.utils import np_box_list\n\n\nclass BoxMaskList(np_box_list.BoxList):\n  """"""Convenience wrapper for BoxList with masks.\n\n  BoxMaskList extends the np_box_list.BoxList to contain masks as well.\n  In particular, its constructor receives both boxes and masks. Note that the\n  masks correspond to the full image.\n  """"""\n\n  def __init__(self, box_data, mask_data):\n    """"""Constructs box collection.\n\n    Args:\n      box_data: a numpy array of shape [N, 4] representing box coordinates\n      mask_data: a numpy array of shape [N, height, width] representing masks\n        with values are in {0,1}. The masks correspond to the full\n        image. The height and the width will be equal to image height and width.\n\n    Raises:\n      ValueError: if bbox data is not a numpy array\n      ValueError: if invalid dimensions for bbox data\n      ValueError: if mask data is not a numpy array\n      ValueError: if invalid dimension for mask data\n    """"""\n    super(BoxMaskList, self).__init__(box_data)\n    if not isinstance(mask_data, np.ndarray):\n      raise ValueError(\'Mask data must be a numpy array.\')\n    if len(mask_data.shape) != 3:\n      raise ValueError(\'Invalid dimensions for mask data.\')\n    if mask_data.dtype != np.uint8:\n      raise ValueError(\'Invalid data type for mask data: uint8 is required.\')\n    if mask_data.shape[0] != box_data.shape[0]:\n      raise ValueError(\'There should be the same number of boxes and masks.\')\n    self.data[\'masks\'] = mask_data\n\n  def get_masks(self):\n    """"""Convenience function for accessing masks.\n\n    Returns:\n      a numpy array of shape [N, height, width] representing masks\n    """"""\n    return self.get_field(\'masks\')\n\n'"
src/object_detection/utils/np_box_mask_list_ops.py,0,"b'# Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n""""""Operations for np_box_mask_list.BoxMaskList.\n\nExample box operations that are supported:\n  * Areas: compute bounding box areas\n  * IOU: pairwise intersection-over-union scores\n""""""\nimport numpy as np\n\nfrom object_detection.utils import np_box_list_ops\nfrom object_detection.utils import np_box_mask_list\nfrom object_detection.utils import np_mask_ops\n\n\ndef box_list_to_box_mask_list(boxlist):\n  """"""Converts a BoxList containing \'masks\' into a BoxMaskList.\n\n  Args:\n    boxlist: An np_box_list.BoxList object.\n\n  Returns:\n    An np_box_mask_list.BoxMaskList object.\n\n  Raises:\n    ValueError: If boxlist does not contain `masks` as a field.\n  """"""\n  if not boxlist.has_field(\'masks\'):\n    raise ValueError(\'boxlist does not contain mask field.\')\n  box_mask_list = np_box_mask_list.BoxMaskList(\n      box_data=boxlist.get(),\n      mask_data=boxlist.get_field(\'masks\'))\n  extra_fields = boxlist.get_extra_fields()\n  for key in extra_fields:\n    if key != \'masks\':\n      box_mask_list.data[key] = boxlist.get_field(key)\n  return box_mask_list\n\n\ndef area(box_mask_list):\n  """"""Computes area of masks.\n\n  Args:\n    box_mask_list: np_box_mask_list.BoxMaskList holding N boxes and masks\n\n  Returns:\n    a numpy array with shape [N*1] representing mask areas\n  """"""\n  return np_mask_ops.area(box_mask_list.get_masks())\n\n\ndef intersection(box_mask_list1, box_mask_list2):\n  """"""Compute pairwise intersection areas between masks.\n\n  Args:\n    box_mask_list1: BoxMaskList holding N boxes and masks\n    box_mask_list2: BoxMaskList holding M boxes and masks\n\n  Returns:\n    a numpy array with shape [N*M] representing pairwise intersection area\n  """"""\n  return np_mask_ops.intersection(box_mask_list1.get_masks(),\n                                  box_mask_list2.get_masks())\n\n\ndef iou(box_mask_list1, box_mask_list2):\n  """"""Computes pairwise intersection-over-union between box and mask collections.\n\n  Args:\n    box_mask_list1: BoxMaskList holding N boxes and masks\n    box_mask_list2: BoxMaskList holding M boxes and masks\n\n  Returns:\n    a numpy array with shape [N, M] representing pairwise iou scores.\n  """"""\n  return np_mask_ops.iou(box_mask_list1.get_masks(),\n                         box_mask_list2.get_masks())\n\n\ndef ioa(box_mask_list1, box_mask_list2):\n  """"""Computes pairwise intersection-over-area between box and mask collections.\n\n  Intersection-over-area (ioa) between two masks mask1 and mask2 is defined as\n  their intersection area over mask2\'s area. Note that ioa is not symmetric,\n  that is, IOA(mask1, mask2) != IOA(mask2, mask1).\n\n  Args:\n    box_mask_list1: np_box_mask_list.BoxMaskList holding N boxes and masks\n    box_mask_list2: np_box_mask_list.BoxMaskList holding M boxes and masks\n\n  Returns:\n    a numpy array with shape [N, M] representing pairwise ioa scores.\n  """"""\n  return np_mask_ops.ioa(box_mask_list1.get_masks(), box_mask_list2.get_masks())\n\n\ndef gather(box_mask_list, indices, fields=None):\n  """"""Gather boxes from np_box_mask_list.BoxMaskList according to indices.\n\n  By default, gather returns boxes corresponding to the input index list, as\n  well as all additional fields stored in the box_mask_list (indexing into the\n  first dimension).  However one can optionally only gather from a\n  subset of fields.\n\n  Args:\n    box_mask_list: np_box_mask_list.BoxMaskList holding N boxes\n    indices: a 1-d numpy array of type int_\n    fields: (optional) list of fields to also gather from.  If None (default),\n        all fields are gathered from.  Pass an empty fields list to only gather\n        the box coordinates.\n\n  Returns:\n    subbox_mask_list: a np_box_mask_list.BoxMaskList corresponding to the subset\n        of the input box_mask_list specified by indices\n\n  Raises:\n    ValueError: if specified field is not contained in box_mask_list or if the\n        indices are not of type int_\n  """"""\n  if fields is not None:\n    if \'masks\' not in fields:\n      fields.append(\'masks\')\n  return box_list_to_box_mask_list(\n      np_box_list_ops.gather(\n          boxlist=box_mask_list, indices=indices, fields=fields))\n\n\ndef sort_by_field(box_mask_list, field,\n                  order=np_box_list_ops.SortOrder.DESCEND):\n  """"""Sort boxes and associated fields according to a scalar field.\n\n  A common use case is reordering the boxes according to descending scores.\n\n  Args:\n    box_mask_list: BoxMaskList holding N boxes.\n    field: A BoxMaskList field for sorting and reordering the BoxMaskList.\n    order: (Optional) \'descend\' or \'ascend\'. Default is descend.\n\n  Returns:\n    sorted_box_mask_list: A sorted BoxMaskList with the field in the specified\n      order.\n  """"""\n  return box_list_to_box_mask_list(\n      np_box_list_ops.sort_by_field(\n          boxlist=box_mask_list, field=field, order=order))\n\n\ndef non_max_suppression(box_mask_list,\n                        max_output_size=10000,\n                        iou_threshold=1.0,\n                        score_threshold=-10.0):\n  """"""Non maximum suppression.\n\n  This op greedily selects a subset of detection bounding boxes, pruning\n  away boxes that have high IOU (intersection over union) overlap (> thresh)\n  with already selected boxes. In each iteration, the detected bounding box with\n  highest score in the available pool is selected.\n\n  Args:\n    box_mask_list: np_box_mask_list.BoxMaskList holding N boxes.  Must contain\n      a \'scores\' field representing detection scores. All scores belong to the\n      same class.\n    max_output_size: maximum number of retained boxes\n    iou_threshold: intersection over union threshold.\n    score_threshold: minimum score threshold. Remove the boxes with scores\n                     less than this value. Default value is set to -10. A very\n                     low threshold to pass pretty much all the boxes, unless\n                     the user sets a different score threshold.\n\n  Returns:\n    an np_box_mask_list.BoxMaskList holding M boxes where M <= max_output_size\n\n  Raises:\n    ValueError: if \'scores\' field does not exist\n    ValueError: if threshold is not in [0, 1]\n    ValueError: if max_output_size < 0\n  """"""\n  if not box_mask_list.has_field(\'scores\'):\n    raise ValueError(\'Field scores does not exist\')\n  if iou_threshold < 0. or iou_threshold > 1.0:\n    raise ValueError(\'IOU threshold must be in [0, 1]\')\n  if max_output_size < 0:\n    raise ValueError(\'max_output_size must be bigger than 0.\')\n\n  box_mask_list = filter_scores_greater_than(box_mask_list, score_threshold)\n  if box_mask_list.num_boxes() == 0:\n    return box_mask_list\n\n  box_mask_list = sort_by_field(box_mask_list, \'scores\')\n\n  # Prevent further computation if NMS is disabled.\n  if iou_threshold == 1.0:\n    if box_mask_list.num_boxes() > max_output_size:\n      selected_indices = np.arange(max_output_size)\n      return gather(box_mask_list, selected_indices)\n    else:\n      return box_mask_list\n\n  masks = box_mask_list.get_masks()\n  num_masks = box_mask_list.num_boxes()\n\n  # is_index_valid is True only for all remaining valid boxes,\n  is_index_valid = np.full(num_masks, 1, dtype=bool)\n  selected_indices = []\n  num_output = 0\n  for i in range(num_masks):\n    if num_output < max_output_size:\n      if is_index_valid[i]:\n        num_output += 1\n        selected_indices.append(i)\n        is_index_valid[i] = False\n        valid_indices = np.where(is_index_valid)[0]\n        if valid_indices.size == 0:\n          break\n\n        intersect_over_union = np_mask_ops.iou(\n            np.expand_dims(masks[i], axis=0), masks[valid_indices])\n        intersect_over_union = np.squeeze(intersect_over_union, axis=0)\n        is_index_valid[valid_indices] = np.logical_and(\n            is_index_valid[valid_indices],\n            intersect_over_union <= iou_threshold)\n  return gather(box_mask_list, np.array(selected_indices))\n\n\ndef multi_class_non_max_suppression(box_mask_list, score_thresh, iou_thresh,\n                                    max_output_size):\n  """"""Multi-class version of non maximum suppression.\n\n  This op greedily selects a subset of detection bounding boxes, pruning\n  away boxes that have high IOU (intersection over union) overlap (> thresh)\n  with already selected boxes.  It operates independently for each class for\n  which scores are provided (via the scores field of the input box_list),\n  pruning boxes with score less than a provided threshold prior to\n  applying NMS.\n\n  Args:\n    box_mask_list: np_box_mask_list.BoxMaskList holding N boxes.  Must contain a\n      \'scores\' field representing detection scores.  This scores field is a\n      tensor that can be 1 dimensional (in the case of a single class) or\n      2-dimensional, in which case we assume that it takes the\n      shape [num_boxes, num_classes]. We further assume that this rank is known\n      statically and that scores.shape[1] is also known (i.e., the number of\n      classes is fixed and known at graph construction time).\n    score_thresh: scalar threshold for score (low scoring boxes are removed).\n    iou_thresh: scalar threshold for IOU (boxes that that high IOU overlap\n      with previously selected boxes are removed).\n    max_output_size: maximum number of retained boxes per class.\n\n  Returns:\n    a box_mask_list holding M boxes with a rank-1 scores field representing\n      corresponding scores for each box with scores sorted in decreasing order\n      and a rank-1 classes field representing a class label for each box.\n  Raises:\n    ValueError: if iou_thresh is not in [0, 1] or if input box_mask_list does\n      not have a valid scores field.\n  """"""\n  if not 0 <= iou_thresh <= 1.0:\n    raise ValueError(\'thresh must be between 0 and 1\')\n  if not isinstance(box_mask_list, np_box_mask_list.BoxMaskList):\n    raise ValueError(\'box_mask_list must be a box_mask_list\')\n  if not box_mask_list.has_field(\'scores\'):\n    raise ValueError(\'input box_mask_list must have \\\'scores\\\' field\')\n  scores = box_mask_list.get_field(\'scores\')\n  if len(scores.shape) == 1:\n    scores = np.reshape(scores, [-1, 1])\n  elif len(scores.shape) == 2:\n    if scores.shape[1] is None:\n      raise ValueError(\'scores field must have statically defined second \'\n                       \'dimension\')\n  else:\n    raise ValueError(\'scores field must be of rank 1 or 2\')\n\n  num_boxes = box_mask_list.num_boxes()\n  num_scores = scores.shape[0]\n  num_classes = scores.shape[1]\n\n  if num_boxes != num_scores:\n    raise ValueError(\'Incorrect scores field length: actual vs expected.\')\n\n  selected_boxes_list = []\n  for class_idx in range(num_classes):\n    box_mask_list_and_class_scores = np_box_mask_list.BoxMaskList(\n        box_data=box_mask_list.get(),\n        mask_data=box_mask_list.get_masks())\n    class_scores = np.reshape(scores[0:num_scores, class_idx], [-1])\n    box_mask_list_and_class_scores.add_field(\'scores\', class_scores)\n    box_mask_list_filt = filter_scores_greater_than(\n        box_mask_list_and_class_scores, score_thresh)\n    nms_result = non_max_suppression(\n        box_mask_list_filt,\n        max_output_size=max_output_size,\n        iou_threshold=iou_thresh,\n        score_threshold=score_thresh)\n    nms_result.add_field(\n        \'classes\',\n        np.zeros_like(nms_result.get_field(\'scores\')) + class_idx)\n    selected_boxes_list.append(nms_result)\n  selected_boxes = np_box_list_ops.concatenate(selected_boxes_list)\n  sorted_boxes = np_box_list_ops.sort_by_field(selected_boxes, \'scores\')\n  return box_list_to_box_mask_list(boxlist=sorted_boxes)\n\n\ndef prune_non_overlapping_masks(box_mask_list1, box_mask_list2, minoverlap=0.0):\n  """"""Prunes the boxes in list1 that overlap less than thresh with list2.\n\n  For each mask in box_mask_list1, we want its IOA to be more than minoverlap\n  with at least one of the masks in box_mask_list2. If it does not, we remove\n  it. If the masks are not full size image, we do the pruning based on boxes.\n\n  Args:\n    box_mask_list1: np_box_mask_list.BoxMaskList holding N boxes and masks.\n    box_mask_list2: np_box_mask_list.BoxMaskList holding M boxes and masks.\n    minoverlap: Minimum required overlap between boxes, to count them as\n                overlapping.\n\n  Returns:\n    A pruned box_mask_list with size [N\', 4].\n  """"""\n  intersection_over_area = ioa(box_mask_list2, box_mask_list1)  # [M, N] tensor\n  intersection_over_area = np.amax(intersection_over_area, axis=0)  # [N] tensor\n  keep_bool = np.greater_equal(intersection_over_area, np.array(minoverlap))\n  keep_inds = np.nonzero(keep_bool)[0]\n  new_box_mask_list1 = gather(box_mask_list1, keep_inds)\n  return new_box_mask_list1\n\n\ndef concatenate(box_mask_lists, fields=None):\n  """"""Concatenate list of box_mask_lists.\n\n  This op concatenates a list of input box_mask_lists into a larger\n  box_mask_list.  It also\n  handles concatenation of box_mask_list fields as long as the field tensor\n  shapes are equal except for the first dimension.\n\n  Args:\n    box_mask_lists: list of np_box_mask_list.BoxMaskList objects\n    fields: optional list of fields to also concatenate.  By default, all\n      fields from the first BoxMaskList in the list are included in the\n      concatenation.\n\n  Returns:\n    a box_mask_list with number of boxes equal to\n      sum([box_mask_list.num_boxes() for box_mask_list in box_mask_list])\n  Raises:\n    ValueError: if box_mask_lists is invalid (i.e., is not a list, is empty, or\n      contains non box_mask_list objects), or if requested fields are not\n      contained in all box_mask_lists\n  """"""\n  if fields is not None:\n    if \'masks\' not in fields:\n      fields.append(\'masks\')\n  return box_list_to_box_mask_list(\n      np_box_list_ops.concatenate(boxlists=box_mask_lists, fields=fields))\n\n\ndef filter_scores_greater_than(box_mask_list, thresh):\n  """"""Filter to keep only boxes and masks with score exceeding a given threshold.\n\n  This op keeps the collection of boxes and masks whose corresponding scores are\n  greater than the input threshold.\n\n  Args:\n    box_mask_list: BoxMaskList holding N boxes and masks.  Must contain a\n      \'scores\' field representing detection scores.\n    thresh: scalar threshold\n\n  Returns:\n    a BoxMaskList holding M boxes and masks where M <= N\n\n  Raises:\n    ValueError: if box_mask_list not a np_box_mask_list.BoxMaskList object or\n      if it does not have a scores field\n  """"""\n  if not isinstance(box_mask_list, np_box_mask_list.BoxMaskList):\n    raise ValueError(\'box_mask_list must be a BoxMaskList\')\n  if not box_mask_list.has_field(\'scores\'):\n    raise ValueError(\'input box_mask_list must have \\\'scores\\\' field\')\n  scores = box_mask_list.get_field(\'scores\')\n  if len(scores.shape) > 2:\n    raise ValueError(\'Scores should have rank 1 or 2\')\n  if len(scores.shape) == 2 and scores.shape[1] != 1:\n    raise ValueError(\'Scores should have rank 1 or have shape \'\n                     \'consistent with [None, 1]\')\n  high_score_indices = np.reshape(np.where(np.greater(scores, thresh)),\n                                  [-1]).astype(np.int32)\n  return gather(box_mask_list, high_score_indices)\n'"
src/object_detection/utils/np_box_mask_list_ops_test.py,3,"b'# Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n""""""Tests for object_detection.utils.np_box_mask_list_ops.""""""\n\nimport numpy as np\nimport tensorflow as tf\n\nfrom object_detection.utils import np_box_mask_list\nfrom object_detection.utils import np_box_mask_list_ops\n\n\nclass AreaRelatedTest(tf.test.TestCase):\n\n  def setUp(self):\n    boxes1 = np.array([[4.0, 3.0, 7.0, 5.0], [5.0, 6.0, 10.0, 7.0]],\n                      dtype=float)\n    masks1_0 = np.array([[0, 0, 0, 0, 0, 0, 0, 0],\n                         [0, 0, 0, 0, 0, 0, 0, 0],\n                         [0, 0, 0, 0, 0, 0, 0, 0],\n                         [1, 1, 1, 1, 0, 0, 0, 0],\n                         [1, 1, 1, 1, 0, 0, 0, 0]],\n                        dtype=np.uint8)\n    masks1_1 = np.array([[1, 1, 1, 1, 1, 1, 1, 1],\n                         [1, 1, 0, 0, 0, 0, 0, 0],\n                         [0, 0, 0, 0, 0, 0, 0, 0],\n                         [0, 0, 0, 0, 0, 0, 0, 0],\n                         [0, 0, 0, 0, 0, 0, 0, 0]],\n                        dtype=np.uint8)\n    masks1 = np.stack([masks1_0, masks1_1])\n    boxes2 = np.array([[3.0, 4.0, 6.0, 8.0], [14.0, 14.0, 15.0, 15.0],\n                       [0.0, 0.0, 20.0, 20.0]],\n                      dtype=float)\n    masks2_0 = np.array([[0, 0, 0, 0, 0, 0, 0, 0],\n                         [0, 0, 0, 0, 0, 0, 0, 0],\n                         [0, 0, 0, 0, 0, 0, 0, 0],\n                         [1, 1, 1, 1, 0, 0, 0, 0],\n                         [1, 1, 1, 1, 0, 0, 0, 0]],\n                        dtype=np.uint8)\n    masks2_1 = np.array([[1, 1, 1, 1, 1, 1, 1, 0],\n                         [1, 1, 1, 1, 1, 0, 0, 0],\n                         [1, 1, 1, 0, 0, 0, 0, 0],\n                         [0, 0, 0, 0, 0, 0, 0, 0],\n                         [0, 0, 0, 0, 0, 0, 0, 0]],\n                        dtype=np.uint8)\n    masks2_2 = np.array([[1, 1, 1, 1, 1, 0, 0, 0],\n                         [1, 1, 1, 1, 1, 0, 0, 0],\n                         [1, 1, 1, 1, 1, 0, 0, 0],\n                         [1, 1, 1, 1, 1, 0, 0, 0],\n                         [1, 1, 1, 1, 1, 0, 0, 0]],\n                        dtype=np.uint8)\n    masks2 = np.stack([masks2_0, masks2_1, masks2_2])\n    self.box_mask_list1 = np_box_mask_list.BoxMaskList(\n        box_data=boxes1, mask_data=masks1)\n    self.box_mask_list2 = np_box_mask_list.BoxMaskList(\n        box_data=boxes2, mask_data=masks2)\n\n  def test_area(self):\n    areas = np_box_mask_list_ops.area(self.box_mask_list1)\n    expected_areas = np.array([8.0, 10.0], dtype=float)\n    self.assertAllClose(expected_areas, areas)\n\n  def test_intersection(self):\n    intersection = np_box_mask_list_ops.intersection(self.box_mask_list1,\n                                                     self.box_mask_list2)\n    expected_intersection = np.array([[8.0, 0.0, 8.0], [0.0, 9.0, 7.0]],\n                                     dtype=float)\n    self.assertAllClose(intersection, expected_intersection)\n\n  def test_iou(self):\n    iou = np_box_mask_list_ops.iou(self.box_mask_list1, self.box_mask_list2)\n    expected_iou = np.array(\n        [[1.0, 0.0, 8.0 / 25.0], [0.0, 9.0 / 16.0, 7.0 / 28.0]], dtype=float)\n    self.assertAllClose(iou, expected_iou)\n\n  def test_ioa(self):\n    ioa21 = np_box_mask_list_ops.ioa(self.box_mask_list1, self.box_mask_list2)\n    expected_ioa21 = np.array([[1.0, 0.0, 8.0/25.0],\n                               [0.0, 9.0/15.0, 7.0/25.0]],\n                              dtype=np.float32)\n    self.assertAllClose(ioa21, expected_ioa21)\n\n\nclass NonMaximumSuppressionTest(tf.test.TestCase):\n\n  def setUp(self):\n    boxes1 = np.array(\n        [[4.0, 3.0, 7.0, 6.0], [5.0, 6.0, 10.0, 10.0]], dtype=float)\n    boxes2 = np.array(\n        [[3.0, 4.0, 6.0, 8.0], [5.0, 6.0, 10.0, 10.0], [1.0, 1.0, 10.0, 10.0]],\n        dtype=float)\n    masks1 = np.array(\n        [[[0, 1, 0], [1, 1, 0], [0, 0, 0]], [[0, 1, 1], [0, 1, 1], [0, 1, 1]]],\n        dtype=np.uint8)\n    masks2 = np.array(\n        [[[0, 1, 0], [1, 1, 1], [0, 0, 0]], [[0, 1, 0], [0, 0, 1], [0, 1, 1]],\n         [[0, 1, 1], [0, 1, 1], [0, 1, 1]]],\n        dtype=np.uint8)\n    self.boxes1 = boxes1\n    self.boxes2 = boxes2\n    self.masks1 = masks1\n    self.masks2 = masks2\n\n  def test_with_no_scores_field(self):\n    box_mask_list = np_box_mask_list.BoxMaskList(\n        box_data=self.boxes1, mask_data=self.masks1)\n    max_output_size = 3\n    iou_threshold = 0.5\n\n    with self.assertRaises(ValueError):\n      np_box_mask_list_ops.non_max_suppression(\n          box_mask_list, max_output_size, iou_threshold)\n\n  def test_nms_disabled_max_output_size_equals_one(self):\n    box_mask_list = np_box_mask_list.BoxMaskList(\n        box_data=self.boxes2, mask_data=self.masks2)\n    box_mask_list.add_field(\'scores\',\n                            np.array([.9, .75, .6], dtype=float))\n    max_output_size = 1\n    iou_threshold = 1.  # No NMS\n    expected_boxes = np.array([[3.0, 4.0, 6.0, 8.0]], dtype=float)\n    expected_masks = np.array(\n        [[[0, 1, 0], [1, 1, 1], [0, 0, 0]]], dtype=np.uint8)\n    nms_box_mask_list = np_box_mask_list_ops.non_max_suppression(\n        box_mask_list, max_output_size, iou_threshold)\n    self.assertAllClose(nms_box_mask_list.get(), expected_boxes)\n    self.assertAllClose(nms_box_mask_list.get_masks(), expected_masks)\n\n  def test_multiclass_nms(self):\n    boxes = np.array(\n        [[0.2, 0.4, 0.8, 0.8], [0.4, 0.2, 0.8, 0.8], [0.6, 0.0, 1.0, 1.0]],\n        dtype=np.float32)\n    mask0 = np.array([[0, 0, 0, 0, 0],\n                      [0, 0, 1, 1, 0],\n                      [0, 0, 1, 1, 0],\n                      [0, 0, 1, 1, 0],\n                      [0, 0, 0, 0, 0]],\n                     dtype=np.uint8)\n    mask1 = np.array([[0, 0, 0, 0, 0],\n                      [0, 0, 0, 0, 0],\n                      [0, 1, 1, 1, 0],\n                      [0, 1, 1, 1, 0],\n                      [0, 0, 0, 0, 0]],\n                     dtype=np.uint8)\n    mask2 = np.array([[0, 0, 0, 0, 0],\n                      [0, 0, 0, 0, 0],\n                      [0, 0, 0, 0, 0],\n                      [1, 1, 1, 1, 1],\n                      [1, 1, 1, 1, 1]],\n                     dtype=np.uint8)\n    masks = np.stack([mask0, mask1, mask2])\n    box_mask_list = np_box_mask_list.BoxMaskList(\n        box_data=boxes, mask_data=masks)\n    scores = np.array([[-0.2, 0.1, 0.5, -0.4, 0.3],\n                       [0.7, -0.7, 0.6, 0.2, -0.9],\n                       [0.4, 0.34, -0.9, 0.2, 0.31]],\n                      dtype=np.float32)\n    box_mask_list.add_field(\'scores\', scores)\n    box_mask_list_clean = np_box_mask_list_ops.multi_class_non_max_suppression(\n        box_mask_list, score_thresh=0.25, iou_thresh=0.1, max_output_size=3)\n\n    scores_clean = box_mask_list_clean.get_field(\'scores\')\n    classes_clean = box_mask_list_clean.get_field(\'classes\')\n    boxes = box_mask_list_clean.get()\n    masks = box_mask_list_clean.get_masks()\n    expected_scores = np.array([0.7, 0.6, 0.34, 0.31])\n    expected_classes = np.array([0, 2, 1, 4])\n    expected_boxes = np.array([[0.4, 0.2, 0.8, 0.8],\n                               [0.4, 0.2, 0.8, 0.8],\n                               [0.6, 0.0, 1.0, 1.0],\n                               [0.6, 0.0, 1.0, 1.0]],\n                              dtype=np.float32)\n    self.assertAllClose(scores_clean, expected_scores)\n    self.assertAllClose(classes_clean, expected_classes)\n    self.assertAllClose(boxes, expected_boxes)\n\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
src/object_detection/utils/np_box_mask_list_test.py,3,"b'# Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n""""""Tests for object_detection.utils.np_box_mask_list_test.""""""\n\nimport numpy as np\nimport tensorflow as tf\n\nfrom object_detection.utils import np_box_mask_list\n\n\nclass BoxMaskListTest(tf.test.TestCase):\n\n  def test_invalid_box_mask_data(self):\n    with self.assertRaises(ValueError):\n      np_box_mask_list.BoxMaskList(\n          box_data=[0, 0, 1, 1],\n          mask_data=np.zeros([1, 3, 3], dtype=np.uint8))\n\n    with self.assertRaises(ValueError):\n      np_box_mask_list.BoxMaskList(\n          box_data=np.array([[0, 0, 1, 1]], dtype=int),\n          mask_data=np.zeros([1, 3, 3], dtype=np.uint8))\n\n    with self.assertRaises(ValueError):\n      np_box_mask_list.BoxMaskList(\n          box_data=np.array([0, 1, 1, 3, 4], dtype=float),\n          mask_data=np.zeros([1, 3, 3], dtype=np.uint8))\n\n    with self.assertRaises(ValueError):\n      np_box_mask_list.BoxMaskList(\n          box_data=np.array([[0, 1, 1, 3], [3, 1, 1, 5]], dtype=float),\n          mask_data=np.zeros([2, 3, 3], dtype=np.uint8))\n\n    with self.assertRaises(ValueError):\n      np_box_mask_list.BoxMaskList(\n          box_data=np.array([[0, 1, 1, 3], [1, 1, 1, 5]], dtype=float),\n          mask_data=np.zeros([3, 5, 5], dtype=np.uint8))\n\n    with self.assertRaises(ValueError):\n      np_box_mask_list.BoxMaskList(\n          box_data=np.array([[0, 1, 1, 3], [1, 1, 1, 5]], dtype=float),\n          mask_data=np.zeros([2, 5], dtype=np.uint8))\n\n    with self.assertRaises(ValueError):\n      np_box_mask_list.BoxMaskList(\n          box_data=np.array([[0, 1, 1, 3], [1, 1, 1, 5]], dtype=float),\n          mask_data=np.zeros([2, 5, 5, 5], dtype=np.uint8))\n\n    with self.assertRaises(ValueError):\n      np_box_mask_list.BoxMaskList(\n          box_data=np.array([[0, 1, 1, 3], [1, 1, 1, 5]], dtype=float),\n          mask_data=np.zeros([2, 5, 5], dtype=np.int32))\n\n  def test_has_field_with_existed_field(self):\n    boxes = np.array([[3.0, 4.0, 6.0, 8.0], [14.0, 14.0, 15.0, 15.0],\n                      [0.0, 0.0, 20.0, 20.0]],\n                     dtype=float)\n    box_mask_list = np_box_mask_list.BoxMaskList(\n        box_data=boxes, mask_data=np.zeros([3, 5, 5], dtype=np.uint8))\n    self.assertTrue(box_mask_list.has_field(\'boxes\'))\n    self.assertTrue(box_mask_list.has_field(\'masks\'))\n\n  def test_has_field_with_nonexisted_field(self):\n    boxes = np.array([[3.0, 4.0, 6.0, 8.0], [14.0, 14.0, 15.0, 15.0],\n                      [0.0, 0.0, 20.0, 20.0]],\n                     dtype=float)\n    box_mask_list = np_box_mask_list.BoxMaskList(\n        box_data=boxes, mask_data=np.zeros([3, 3, 3], dtype=np.uint8))\n    self.assertFalse(box_mask_list.has_field(\'scores\'))\n\n  def test_get_field_with_existed_field(self):\n    boxes = np.array([[3.0, 4.0, 6.0, 8.0], [14.0, 14.0, 15.0, 15.0],\n                      [0.0, 0.0, 20.0, 20.0]],\n                     dtype=float)\n    masks = np.zeros([3, 3, 3], dtype=np.uint8)\n    box_mask_list = np_box_mask_list.BoxMaskList(\n        box_data=boxes, mask_data=masks)\n    self.assertTrue(np.allclose(box_mask_list.get_field(\'boxes\'), boxes))\n    self.assertTrue(np.allclose(box_mask_list.get_field(\'masks\'), masks))\n\n  def test_get_field_with_nonexited_field(self):\n    boxes = np.array([[3.0, 4.0, 6.0, 8.0], [14.0, 14.0, 15.0, 15.0],\n                      [0.0, 0.0, 20.0, 20.0]],\n                     dtype=float)\n    masks = np.zeros([3, 3, 3], dtype=np.uint8)\n    box_mask_list = np_box_mask_list.BoxMaskList(\n        box_data=boxes, mask_data=masks)\n    with self.assertRaises(ValueError):\n      box_mask_list.get_field(\'scores\')\n\n\nclass AddExtraFieldTest(tf.test.TestCase):\n\n  def setUp(self):\n    boxes = np.array([[3.0, 4.0, 6.0, 8.0], [14.0, 14.0, 15.0, 15.0],\n                      [0.0, 0.0, 20.0, 20.0]],\n                     dtype=float)\n    masks = np.zeros([3, 3, 3], dtype=np.uint8)\n    self.box_mask_list = np_box_mask_list.BoxMaskList(\n        box_data=boxes, mask_data=masks)\n\n  def test_add_already_existed_field_bbox(self):\n    with self.assertRaises(ValueError):\n      self.box_mask_list.add_field(\'boxes\',\n                                   np.array([[0, 0, 0, 1, 0]], dtype=float))\n\n  def test_add_already_existed_field_mask(self):\n    with self.assertRaises(ValueError):\n      self.box_mask_list.add_field(\'masks\',\n                                   np.zeros([3, 3, 3], dtype=np.uint8))\n\n  def test_add_invalid_field_data(self):\n    with self.assertRaises(ValueError):\n      self.box_mask_list.add_field(\'scores\', np.array([0.5, 0.7], dtype=float))\n    with self.assertRaises(ValueError):\n      self.box_mask_list.add_field(\'scores\',\n                                   np.array([0.5, 0.7, 0.9, 0.1], dtype=float))\n\n  def test_add_single_dimensional_field_data(self):\n    box_mask_list = self.box_mask_list\n    scores = np.array([0.5, 0.7, 0.9], dtype=float)\n    box_mask_list.add_field(\'scores\', scores)\n    self.assertTrue(np.allclose(scores, self.box_mask_list.get_field(\'scores\')))\n\n  def test_add_multi_dimensional_field_data(self):\n    box_mask_list = self.box_mask_list\n    labels = np.array([[0, 0, 0, 1, 0], [0, 1, 0, 0, 0], [0, 0, 0, 0, 1]],\n                      dtype=int)\n    box_mask_list.add_field(\'labels\', labels)\n    self.assertTrue(np.allclose(labels, self.box_mask_list.get_field(\'labels\')))\n\n  def test_get_extra_fields(self):\n    box_mask_list = self.box_mask_list\n    self.assertItemsEqual(box_mask_list.get_extra_fields(), [\'masks\'])\n\n    scores = np.array([0.5, 0.7, 0.9], dtype=float)\n    box_mask_list.add_field(\'scores\', scores)\n    self.assertItemsEqual(box_mask_list.get_extra_fields(), [\'masks\', \'scores\'])\n\n    labels = np.array([[0, 0, 0, 1, 0], [0, 1, 0, 0, 0], [0, 0, 0, 0, 1]],\n                      dtype=int)\n    box_mask_list.add_field(\'labels\', labels)\n    self.assertItemsEqual(box_mask_list.get_extra_fields(),\n                          [\'masks\', \'scores\', \'labels\'])\n\n  def test_get_coordinates(self):\n    y_min, x_min, y_max, x_max = self.box_mask_list.get_coordinates()\n\n    expected_y_min = np.array([3.0, 14.0, 0.0], dtype=float)\n    expected_x_min = np.array([4.0, 14.0, 0.0], dtype=float)\n    expected_y_max = np.array([6.0, 15.0, 20.0], dtype=float)\n    expected_x_max = np.array([8.0, 15.0, 20.0], dtype=float)\n\n    self.assertTrue(np.allclose(y_min, expected_y_min))\n    self.assertTrue(np.allclose(x_min, expected_x_min))\n    self.assertTrue(np.allclose(y_max, expected_y_max))\n    self.assertTrue(np.allclose(x_max, expected_x_max))\n\n  def test_num_boxes(self):\n    boxes = np.array([[0., 0., 100., 100.], [10., 30., 50., 70.]], dtype=float)\n    masks = np.zeros([2, 5, 5], dtype=np.uint8)\n    box_mask_list = np_box_mask_list.BoxMaskList(\n        box_data=boxes, mask_data=masks)\n    expected_num_boxes = 2\n    self.assertEquals(box_mask_list.num_boxes(), expected_num_boxes)\n\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
src/object_detection/utils/np_box_ops.py,0,"b'# Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n""""""Operations for [N, 4] numpy arrays representing bounding boxes.\n\nExample box operations that are supported:\n  * Areas: compute bounding box areas\n  * IOU: pairwise intersection-over-union scores\n""""""\nimport numpy as np\n\n\ndef area(boxes):\n  """"""Computes area of boxes.\n\n  Args:\n    boxes: Numpy array with shape [N, 4] holding N boxes\n\n  Returns:\n    a numpy array with shape [N*1] representing box areas\n  """"""\n  return (boxes[:, 2] - boxes[:, 0]) * (boxes[:, 3] - boxes[:, 1])\n\n\ndef intersection(boxes1, boxes2):\n  """"""Compute pairwise intersection areas between boxes.\n\n  Args:\n    boxes1: a numpy array with shape [N, 4] holding N boxes\n    boxes2: a numpy array with shape [M, 4] holding M boxes\n\n  Returns:\n    a numpy array with shape [N*M] representing pairwise intersection area\n  """"""\n  [y_min1, x_min1, y_max1, x_max1] = np.split(boxes1, 4, axis=1)\n  [y_min2, x_min2, y_max2, x_max2] = np.split(boxes2, 4, axis=1)\n\n  all_pairs_min_ymax = np.minimum(y_max1, np.transpose(y_max2))\n  all_pairs_max_ymin = np.maximum(y_min1, np.transpose(y_min2))\n  intersect_heights = np.maximum(\n      np.zeros(all_pairs_max_ymin.shape),\n      all_pairs_min_ymax - all_pairs_max_ymin)\n  all_pairs_min_xmax = np.minimum(x_max1, np.transpose(x_max2))\n  all_pairs_max_xmin = np.maximum(x_min1, np.transpose(x_min2))\n  intersect_widths = np.maximum(\n      np.zeros(all_pairs_max_xmin.shape),\n      all_pairs_min_xmax - all_pairs_max_xmin)\n  return intersect_heights * intersect_widths\n\n\ndef iou(boxes1, boxes2):\n  """"""Computes pairwise intersection-over-union between box collections.\n\n  Args:\n    boxes1: a numpy array with shape [N, 4] holding N boxes.\n    boxes2: a numpy array with shape [M, 4] holding N boxes.\n\n  Returns:\n    a numpy array with shape [N, M] representing pairwise iou scores.\n  """"""\n  intersect = intersection(boxes1, boxes2)\n  area1 = area(boxes1)\n  area2 = area(boxes2)\n  union = np.expand_dims(area1, axis=1) + np.expand_dims(\n      area2, axis=0) - intersect\n  return intersect / union\n\n\ndef ioa(boxes1, boxes2):\n  """"""Computes pairwise intersection-over-area between box collections.\n\n  Intersection-over-area (ioa) between two boxes box1 and box2 is defined as\n  their intersection area over box2\'s area. Note that ioa is not symmetric,\n  that is, IOA(box1, box2) != IOA(box2, box1).\n\n  Args:\n    boxes1: a numpy array with shape [N, 4] holding N boxes.\n    boxes2: a numpy array with shape [M, 4] holding N boxes.\n\n  Returns:\n    a numpy array with shape [N, M] representing pairwise ioa scores.\n  """"""\n  intersect = intersection(boxes1, boxes2)\n  areas = np.expand_dims(area(boxes2), axis=0)\n  return intersect / areas\n'"
src/object_detection/utils/np_box_ops_test.py,2,"b'# Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n""""""Tests for object_detection.np_box_ops.""""""\n\nimport numpy as np\nimport tensorflow as tf\n\nfrom object_detection.utils import np_box_ops\n\n\nclass BoxOpsTests(tf.test.TestCase):\n\n  def setUp(self):\n    boxes1 = np.array([[4.0, 3.0, 7.0, 5.0], [5.0, 6.0, 10.0, 7.0]],\n                      dtype=float)\n    boxes2 = np.array([[3.0, 4.0, 6.0, 8.0], [14.0, 14.0, 15.0, 15.0],\n                       [0.0, 0.0, 20.0, 20.0]],\n                      dtype=float)\n    self.boxes1 = boxes1\n    self.boxes2 = boxes2\n\n  def testArea(self):\n    areas = np_box_ops.area(self.boxes1)\n    expected_areas = np.array([6.0, 5.0], dtype=float)\n    self.assertAllClose(expected_areas, areas)\n\n  def testIntersection(self):\n    intersection = np_box_ops.intersection(self.boxes1, self.boxes2)\n    expected_intersection = np.array([[2.0, 0.0, 6.0], [1.0, 0.0, 5.0]],\n                                     dtype=float)\n    self.assertAllClose(intersection, expected_intersection)\n\n  def testIOU(self):\n    iou = np_box_ops.iou(self.boxes1, self.boxes2)\n    expected_iou = np.array([[2.0 / 16.0, 0.0, 6.0 / 400.0],\n                             [1.0 / 16.0, 0.0, 5.0 / 400.0]],\n                            dtype=float)\n    self.assertAllClose(iou, expected_iou)\n\n  def testIOA(self):\n    boxes1 = np.array([[0.25, 0.25, 0.75, 0.75],\n                       [0.0, 0.0, 0.5, 0.75]],\n                      dtype=np.float32)\n    boxes2 = np.array([[0.5, 0.25, 1.0, 1.0],\n                       [0.0, 0.0, 1.0, 1.0]],\n                      dtype=np.float32)\n    ioa21 = np_box_ops.ioa(boxes2, boxes1)\n    expected_ioa21 = np.array([[0.5, 0.0],\n                               [1.0, 1.0]],\n                              dtype=np.float32)\n    self.assertAllClose(ioa21, expected_ioa21)\n\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
src/object_detection/utils/np_mask_ops.py,0,"b'# Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n""""""Operations for [N, height, width] numpy arrays representing masks.\n\nExample mask operations that are supported:\n  * Areas: compute mask areas\n  * IOU: pairwise intersection-over-union scores\n""""""\nimport numpy as np\n\nEPSILON = 1e-7\n\n\ndef area(masks):\n  """"""Computes area of masks.\n\n  Args:\n    masks: Numpy array with shape [N, height, width] holding N masks. Masks\n      values are of type np.uint8 and values are in {0,1}.\n\n  Returns:\n    a numpy array with shape [N*1] representing mask areas.\n\n  Raises:\n    ValueError: If masks.dtype is not np.uint8\n  """"""\n  if masks.dtype != np.uint8:\n    raise ValueError(\'Masks type should be np.uint8\')\n  return np.sum(masks, axis=(1, 2), dtype=np.float32)\n\n\ndef intersection(masks1, masks2):\n  """"""Compute pairwise intersection areas between masks.\n\n  Args:\n    masks1: a numpy array with shape [N, height, width] holding N masks. Masks\n      values are of type np.uint8 and values are in {0,1}.\n    masks2: a numpy array with shape [M, height, width] holding M masks. Masks\n      values are of type np.uint8 and values are in {0,1}.\n\n  Returns:\n    a numpy array with shape [N*M] representing pairwise intersection area.\n\n  Raises:\n    ValueError: If masks1 and masks2 are not of type np.uint8.\n  """"""\n  if masks1.dtype != np.uint8 or masks2.dtype != np.uint8:\n    raise ValueError(\'masks1 and masks2 should be of type np.uint8\')\n  n = masks1.shape[0]\n  m = masks2.shape[0]\n  answer = np.zeros([n, m], dtype=np.float32)\n  for i in np.arange(n):\n    for j in np.arange(m):\n      answer[i, j] = np.sum(np.minimum(masks1[i], masks2[j]), dtype=np.float32)\n  return answer\n\n\ndef iou(masks1, masks2):\n  """"""Computes pairwise intersection-over-union between mask collections.\n\n  Args:\n    masks1: a numpy array with shape [N, height, width] holding N masks. Masks\n      values are of type np.uint8 and values are in {0,1}.\n    masks2: a numpy array with shape [M, height, width] holding N masks. Masks\n      values are of type np.uint8 and values are in {0,1}.\n\n  Returns:\n    a numpy array with shape [N, M] representing pairwise iou scores.\n\n  Raises:\n    ValueError: If masks1 and masks2 are not of type np.uint8.\n  """"""\n  if masks1.dtype != np.uint8 or masks2.dtype != np.uint8:\n    raise ValueError(\'masks1 and masks2 should be of type np.uint8\')\n  intersect = intersection(masks1, masks2)\n  area1 = area(masks1)\n  area2 = area(masks2)\n  union = np.expand_dims(area1, axis=1) + np.expand_dims(\n      area2, axis=0) - intersect\n  return intersect / np.maximum(union, EPSILON)\n\n\ndef ioa(masks1, masks2):\n  """"""Computes pairwise intersection-over-area between box collections.\n\n  Intersection-over-area (ioa) between two masks, mask1 and mask2 is defined as\n  their intersection area over mask2\'s area. Note that ioa is not symmetric,\n  that is, IOA(mask1, mask2) != IOA(mask2, mask1).\n\n  Args:\n    masks1: a numpy array with shape [N, height, width] holding N masks. Masks\n      values are of type np.uint8 and values are in {0,1}.\n    masks2: a numpy array with shape [M, height, width] holding N masks. Masks\n      values are of type np.uint8 and values are in {0,1}.\n\n  Returns:\n    a numpy array with shape [N, M] representing pairwise ioa scores.\n\n  Raises:\n    ValueError: If masks1 and masks2 are not of type np.uint8.\n  """"""\n  if masks1.dtype != np.uint8 or masks2.dtype != np.uint8:\n    raise ValueError(\'masks1 and masks2 should be of type np.uint8\')\n  intersect = intersection(masks1, masks2)\n  areas = np.expand_dims(area(masks2), axis=0)\n  return intersect / (areas + EPSILON)\n'"
src/object_detection/utils/np_mask_ops_test.py,2,"b'# Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n""""""Tests for object_detection.np_mask_ops.""""""\n\nimport numpy as np\nimport tensorflow as tf\n\nfrom object_detection.utils import np_mask_ops\n\n\nclass MaskOpsTests(tf.test.TestCase):\n\n  def setUp(self):\n    masks1_0 = np.array([[0, 0, 0, 0, 0, 0, 0, 0],\n                         [0, 0, 0, 0, 0, 0, 0, 0],\n                         [0, 0, 0, 0, 0, 0, 0, 0],\n                         [1, 1, 1, 1, 0, 0, 0, 0],\n                         [1, 1, 1, 1, 0, 0, 0, 0]],\n                        dtype=np.uint8)\n    masks1_1 = np.array([[1, 1, 1, 1, 1, 1, 1, 1],\n                         [1, 1, 0, 0, 0, 0, 0, 0],\n                         [0, 0, 0, 0, 0, 0, 0, 0],\n                         [0, 0, 0, 0, 0, 0, 0, 0],\n                         [0, 0, 0, 0, 0, 0, 0, 0]],\n                        dtype=np.uint8)\n    masks1 = np.stack([masks1_0, masks1_1])\n    masks2_0 = np.array([[0, 0, 0, 0, 0, 0, 0, 0],\n                         [0, 0, 0, 0, 0, 0, 0, 0],\n                         [0, 0, 0, 0, 0, 0, 0, 0],\n                         [1, 1, 1, 1, 0, 0, 0, 0],\n                         [1, 1, 1, 1, 0, 0, 0, 0]],\n                        dtype=np.uint8)\n    masks2_1 = np.array([[1, 1, 1, 1, 1, 1, 1, 0],\n                         [1, 1, 1, 1, 1, 0, 0, 0],\n                         [1, 1, 1, 0, 0, 0, 0, 0],\n                         [0, 0, 0, 0, 0, 0, 0, 0],\n                         [0, 0, 0, 0, 0, 0, 0, 0]],\n                        dtype=np.uint8)\n    masks2_2 = np.array([[1, 1, 1, 1, 1, 0, 0, 0],\n                         [1, 1, 1, 1, 1, 0, 0, 0],\n                         [1, 1, 1, 1, 1, 0, 0, 0],\n                         [1, 1, 1, 1, 1, 0, 0, 0],\n                         [1, 1, 1, 1, 1, 0, 0, 0]],\n                        dtype=np.uint8)\n    masks2 = np.stack([masks2_0, masks2_1, masks2_2])\n    self.masks1 = masks1\n    self.masks2 = masks2\n\n  def testArea(self):\n    areas = np_mask_ops.area(self.masks1)\n    expected_areas = np.array([8.0, 10.0], dtype=np.float32)\n    self.assertAllClose(expected_areas, areas)\n\n  def testIntersection(self):\n    intersection = np_mask_ops.intersection(self.masks1, self.masks2)\n    expected_intersection = np.array(\n        [[8.0, 0.0, 8.0], [0.0, 9.0, 7.0]], dtype=np.float32)\n    self.assertAllClose(intersection, expected_intersection)\n\n  def testIOU(self):\n    iou = np_mask_ops.iou(self.masks1, self.masks2)\n    expected_iou = np.array(\n        [[1.0, 0.0, 8.0/25.0], [0.0, 9.0 / 16.0, 7.0 / 28.0]], dtype=np.float32)\n    self.assertAllClose(iou, expected_iou)\n\n  def testIOA(self):\n    ioa21 = np_mask_ops.ioa(self.masks1, self.masks2)\n    expected_ioa21 = np.array([[1.0, 0.0, 8.0/25.0],\n                               [0.0, 9.0/15.0, 7.0/25.0]],\n                              dtype=np.float32)\n    self.assertAllClose(ioa21, expected_ioa21)\n\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
src/object_detection/utils/object_detection_evaluation.py,0,"b'# Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""object_detection_evaluation module.\n\nObjectDetectionEvaluation is a class which manages ground truth information of a\nobject detection dataset, and computes frequently used detection metrics such as\nPrecision, Recall, CorLoc of the provided detection results.\nIt supports the following operations:\n1) Add ground truth information of images sequentially.\n2) Add detection result of images sequentially.\n3) Evaluate detection metrics on already inserted detection results.\n4) Write evaluation result into a pickle file for future processing or\n   visualization.\n\nNote: This module operates on numpy boxes and box lists.\n""""""\n\nfrom abc import ABCMeta\nfrom abc import abstractmethod\nimport collections\nimport logging\nimport numpy as np\n\nfrom object_detection.core import standard_fields\nfrom object_detection.utils import label_map_util\nfrom object_detection.utils import metrics\nfrom object_detection.utils import per_image_evaluation\n\n\nclass DetectionEvaluator(object):\n  """"""Interface for object detection evalution classes.\n\n  Example usage of the Evaluator:\n  ------------------------------\n  evaluator = DetectionEvaluator(categories)\n\n  # Detections and groundtruth for image 1.\n  evaluator.add_single_groundtruth_image_info(...)\n  evaluator.add_single_detected_image_info(...)\n\n  # Detections and groundtruth for image 2.\n  evaluator.add_single_groundtruth_image_info(...)\n  evaluator.add_single_detected_image_info(...)\n\n  metrics_dict = evaluator.evaluate()\n  """"""\n  __metaclass__ = ABCMeta\n\n  def __init__(self, categories):\n    """"""Constructor.\n\n    Args:\n      categories: A list of dicts, each of which has the following keys -\n        \'id\': (required) an integer id uniquely identifying this category.\n        \'name\': (required) string representing category name e.g., \'cat\', \'dog\'.\n    """"""\n    self._categories = categories\n\n  @abstractmethod\n  def add_single_ground_truth_image_info(self, image_id, groundtruth_dict):\n    """"""Adds groundtruth for a single image to be used for evaluation.\n\n    Args:\n      image_id: A unique string/integer identifier for the image.\n      groundtruth_dict: A dictionary of groundtruth numpy arrays required\n        for evaluations.\n    """"""\n    pass\n\n  @abstractmethod\n  def add_single_detected_image_info(self, image_id, detections_dict):\n    """"""Adds detections for a single image to be used for evaluation.\n\n    Args:\n      image_id: A unique string/integer identifier for the image.\n      detections_dict: A dictionary of detection numpy arrays required\n        for evaluation.\n    """"""\n    pass\n\n  @abstractmethod\n  def evaluate(self):\n    """"""Evaluates detections and returns a dictionary of metrics.""""""\n    pass\n\n  @abstractmethod\n  def clear(self):\n    """"""Clears the state to prepare for a fresh evaluation.""""""\n    pass\n\n\nclass ObjectDetectionEvaluator(DetectionEvaluator):\n  """"""A class to evaluate detections.""""""\n\n  def __init__(self,\n               categories,\n               matching_iou_threshold=0.5,\n               evaluate_corlocs=False,\n               metric_prefix=None,\n               use_weighted_mean_ap=False,\n               evaluate_masks=False):\n    """"""Constructor.\n\n    Args:\n      categories: A list of dicts, each of which has the following keys -\n        \'id\': (required) an integer id uniquely identifying this category.\n        \'name\': (required) string representing category name e.g., \'cat\', \'dog\'.\n      matching_iou_threshold: IOU threshold to use for matching groundtruth\n        boxes to detection boxes.\n      evaluate_corlocs: (optional) boolean which determines if corloc scores\n        are to be returned or not.\n      metric_prefix: (optional) string prefix for metric name; if None, no\n        prefix is used.\n      use_weighted_mean_ap: (optional) boolean which determines if the mean\n        average precision is computed directly from the scores and tp_fp_labels\n        of all classes.\n      evaluate_masks: If False, evaluation will be performed based on boxes.\n        If True, mask evaluation will be performed instead.\n\n    Raises:\n      ValueError: If the category ids are not 1-indexed.\n    """"""\n    super(ObjectDetectionEvaluator, self).__init__(categories)\n    self._num_classes = max([cat[\'id\'] for cat in categories])\n    if min(cat[\'id\'] for cat in categories) < 1:\n      raise ValueError(\'Classes should be 1-indexed.\')\n    self._matching_iou_threshold = matching_iou_threshold\n    self._use_weighted_mean_ap = use_weighted_mean_ap\n    self._label_id_offset = 1\n    self._evaluate_masks = evaluate_masks\n    self._evaluation = ObjectDetectionEvaluation(\n        num_groundtruth_classes=self._num_classes,\n        matching_iou_threshold=self._matching_iou_threshold,\n        use_weighted_mean_ap=self._use_weighted_mean_ap,\n        label_id_offset=self._label_id_offset)\n    self._image_ids = set([])\n    self._evaluate_corlocs = evaluate_corlocs\n    self._metric_prefix = (metric_prefix + \'_\') if metric_prefix else \'\'\n\n  def add_single_ground_truth_image_info(self, image_id, groundtruth_dict):\n    """"""Adds groundtruth for a single image to be used for evaluation.\n\n    Args:\n      image_id: A unique string/integer identifier for the image.\n      groundtruth_dict: A dictionary containing -\n        standard_fields.InputDataFields.groundtruth_boxes: float32 numpy array\n          of shape [num_boxes, 4] containing `num_boxes` groundtruth boxes of\n          the format [ymin, xmin, ymax, xmax] in absolute image coordinates.\n        standard_fields.InputDataFields.groundtruth_classes: integer numpy array\n          of shape [num_boxes] containing 1-indexed groundtruth classes for the\n          boxes.\n        standard_fields.InputDataFields.groundtruth_difficult: Optional length\n          M numpy boolean array denoting whether a ground truth box is a\n          difficult instance or not. This field is optional to support the case\n          that no boxes are difficult.\n        standard_fields.InputDataFields.groundtruth_instance_masks: Optional\n          numpy array of shape [num_boxes, height, width] with values in {0, 1}.\n\n    Raises:\n      ValueError: On adding groundtruth for an image more than once. Will also\n        raise error if instance masks are not in groundtruth dictionary.\n    """"""\n    if image_id in self._image_ids:\n      raise ValueError(\'Image with id {} already added.\'.format(image_id))\n\n    groundtruth_classes = (\n        groundtruth_dict[standard_fields.InputDataFields.groundtruth_classes] -\n        self._label_id_offset)\n    # If the key is not present in the groundtruth_dict or the array is empty\n    # (unless there are no annotations for the groundtruth on this image)\n    # use values from the dictionary or insert None otherwise.\n    if (standard_fields.InputDataFields.groundtruth_difficult in\n        groundtruth_dict.keys() and\n        (groundtruth_dict[standard_fields.InputDataFields.groundtruth_difficult]\n         .size or not groundtruth_classes.size)):\n      groundtruth_difficult = groundtruth_dict[\n          standard_fields.InputDataFields.groundtruth_difficult]\n    else:\n      groundtruth_difficult = None\n      if not len(self._image_ids) % 1000:\n        logging.warn(\n            \'image %s does not have groundtruth difficult flag specified\',\n            image_id)\n    groundtruth_masks = None\n    if self._evaluate_masks:\n      if (standard_fields.InputDataFields.groundtruth_instance_masks not in\n          groundtruth_dict):\n        raise ValueError(\'Instance masks not in groundtruth dictionary.\')\n      groundtruth_masks = groundtruth_dict[\n          standard_fields.InputDataFields.groundtruth_instance_masks]\n    self._evaluation.add_single_ground_truth_image_info(\n        image_key=image_id,\n        groundtruth_boxes=groundtruth_dict[\n            standard_fields.InputDataFields.groundtruth_boxes],\n        groundtruth_class_labels=groundtruth_classes,\n        groundtruth_is_difficult_list=groundtruth_difficult,\n        groundtruth_masks=groundtruth_masks)\n    self._image_ids.update([image_id])\n\n  def add_single_detected_image_info(self, image_id, detections_dict):\n    """"""Adds detections for a single image to be used for evaluation.\n\n    Args:\n      image_id: A unique string/integer identifier for the image.\n      detections_dict: A dictionary containing -\n        standard_fields.DetectionResultFields.detection_boxes: float32 numpy\n          array of shape [num_boxes, 4] containing `num_boxes` detection boxes\n          of the format [ymin, xmin, ymax, xmax] in absolute image coordinates.\n        standard_fields.DetectionResultFields.detection_scores: float32 numpy\n          array of shape [num_boxes] containing detection scores for the boxes.\n        standard_fields.DetectionResultFields.detection_classes: integer numpy\n          array of shape [num_boxes] containing 1-indexed detection classes for\n          the boxes.\n        standard_fields.DetectionResultFields.detection_masks: uint8 numpy\n          array of shape [num_boxes, height, width] containing `num_boxes` masks\n          of values ranging between 0 and 1.\n\n    Raises:\n      ValueError: If detection masks are not in detections dictionary.\n    """"""\n    detection_classes = (\n        detections_dict[standard_fields.DetectionResultFields.detection_classes]\n        - self._label_id_offset)\n    detection_masks = None\n    if self._evaluate_masks:\n      if (standard_fields.DetectionResultFields.detection_masks not in\n          detections_dict):\n        raise ValueError(\'Detection masks not in detections dictionary.\')\n      detection_masks = detections_dict[\n          standard_fields.DetectionResultFields.detection_masks]\n    self._evaluation.add_single_detected_image_info(\n        image_key=image_id,\n        detected_boxes=detections_dict[\n            standard_fields.DetectionResultFields.detection_boxes],\n        detected_scores=detections_dict[\n            standard_fields.DetectionResultFields.detection_scores],\n        detected_class_labels=detection_classes,\n        detected_masks=detection_masks)\n\n  def evaluate(self):\n    """"""Compute evaluation result.\n\n    Returns:\n      A dictionary of metrics with the following fields -\n\n      1. summary_metrics:\n        \'Precision/mAP@<matching_iou_threshold>IOU\': mean average precision at\n        the specified IOU threshold.\n\n      2. per_category_ap: category specific results with keys of the form\n        \'PerformanceByCategory/mAP@<matching_iou_threshold>IOU/category\'.\n    """"""\n    (per_class_ap, mean_ap, _, _, per_class_corloc, mean_corloc) = (\n        self._evaluation.evaluate())\n    pascal_metrics = {\n        self._metric_prefix +\n        \'Precision/mAP@{}IOU\'.format(self._matching_iou_threshold):\n            mean_ap\n    }\n    if self._evaluate_corlocs:\n      pascal_metrics[self._metric_prefix + \'Precision/meanCorLoc@{}IOU\'.format(\n          self._matching_iou_threshold)] = mean_corloc\n    category_index = label_map_util.create_category_index(self._categories)\n    for idx in range(per_class_ap.size):\n      if idx + self._label_id_offset in category_index:\n        display_name = (\n            self._metric_prefix + \'PerformanceByCategory/AP@{}IOU/{}\'.format(\n                self._matching_iou_threshold,\n                category_index[idx + self._label_id_offset][\'name\']))\n        pascal_metrics[display_name] = per_class_ap[idx]\n\n        # Optionally add CorLoc metrics.classes\n        if self._evaluate_corlocs:\n          display_name = (\n              self._metric_prefix + \'PerformanceByCategory/CorLoc@{}IOU/{}\'\n              .format(self._matching_iou_threshold,\n                      category_index[idx + self._label_id_offset][\'name\']))\n          pascal_metrics[display_name] = per_class_corloc[idx]\n\n    return pascal_metrics\n\n  def clear(self):\n    """"""Clears the state to prepare for a fresh evaluation.""""""\n    self._evaluation = ObjectDetectionEvaluation(\n        num_groundtruth_classes=self._num_classes,\n        matching_iou_threshold=self._matching_iou_threshold,\n        use_weighted_mean_ap=self._use_weighted_mean_ap,\n        label_id_offset=self._label_id_offset)\n    self._image_ids.clear()\n\n\nclass PascalDetectionEvaluator(ObjectDetectionEvaluator):\n  """"""A class to evaluate detections using PASCAL metrics.""""""\n\n  def __init__(self, categories, matching_iou_threshold=0.5):\n    super(PascalDetectionEvaluator, self).__init__(\n        categories,\n        matching_iou_threshold=matching_iou_threshold,\n        evaluate_corlocs=False,\n        metric_prefix=\'PascalBoxes\',\n        use_weighted_mean_ap=False)\n\n\nclass WeightedPascalDetectionEvaluator(ObjectDetectionEvaluator):\n  """"""A class to evaluate detections using weighted PASCAL metrics.\n\n  Weighted PASCAL metrics computes the mean average precision as the average\n  precision given the scores and tp_fp_labels of all classes. In comparison,\n  PASCAL metrics computes the mean average precision as the mean of the\n  per-class average precisions.\n\n  This definition is very similar to the mean of the per-class average\n  precisions weighted by class frequency. However, they are typically not the\n  same as the average precision is not a linear function of the scores and\n  tp_fp_labels.\n  """"""\n\n  def __init__(self, categories, matching_iou_threshold=0.5):\n    super(WeightedPascalDetectionEvaluator, self).__init__(\n        categories,\n        matching_iou_threshold=matching_iou_threshold,\n        evaluate_corlocs=False,\n        metric_prefix=\'WeightedPascalBoxes\',\n        use_weighted_mean_ap=True)\n\n\nclass PascalInstanceSegmentationEvaluator(ObjectDetectionEvaluator):\n  """"""A class to evaluate instance masks using PASCAL metrics.""""""\n\n  def __init__(self, categories, matching_iou_threshold=0.5):\n    super(PascalInstanceSegmentationEvaluator, self).__init__(\n        categories,\n        matching_iou_threshold=matching_iou_threshold,\n        evaluate_corlocs=False,\n        metric_prefix=\'PascalMasks\',\n        use_weighted_mean_ap=False,\n        evaluate_masks=True)\n\n\nclass WeightedPascalInstanceSegmentationEvaluator(ObjectDetectionEvaluator):\n  """"""A class to evaluate instance masks using weighted PASCAL metrics.\n\n  Weighted PASCAL metrics computes the mean average precision as the average\n  precision given the scores and tp_fp_labels of all classes. In comparison,\n  PASCAL metrics computes the mean average precision as the mean of the\n  per-class average precisions.\n\n  This definition is very similar to the mean of the per-class average\n  precisions weighted by class frequency. However, they are typically not the\n  same as the average precision is not a linear function of the scores and\n  tp_fp_labels.\n  """"""\n\n  def __init__(self, categories, matching_iou_threshold=0.5):\n    super(WeightedPascalInstanceSegmentationEvaluator, self).__init__(\n        categories,\n        matching_iou_threshold=matching_iou_threshold,\n        evaluate_corlocs=False,\n        metric_prefix=\'WeightedPascalMasks\',\n        use_weighted_mean_ap=True,\n        evaluate_masks=True)\n\n\nclass OpenImagesDetectionEvaluator(ObjectDetectionEvaluator):\n  """"""A class to evaluate detections using Open Images V2 metrics.\n\n    Open Images V2 introduce group_of type of bounding boxes and this metric\n    handles those boxes appropriately.\n  """"""\n\n  def __init__(self,\n               categories,\n               matching_iou_threshold=0.5,\n               evaluate_corlocs=False):\n    """"""Constructor.\n\n    Args:\n      categories: A list of dicts, each of which has the following keys -\n        \'id\': (required) an integer id uniquely identifying this category.\n        \'name\': (required) string representing category name e.g., \'cat\', \'dog\'.\n      matching_iou_threshold: IOU threshold to use for matching groundtruth\n        boxes to detection boxes.\n      evaluate_corlocs: if True, additionally evaluates and returns CorLoc.\n    """"""\n    super(OpenImagesDetectionEvaluator, self).__init__(\n        categories,\n        matching_iou_threshold,\n        evaluate_corlocs,\n        metric_prefix=\'OpenImagesV2\')\n\n  def add_single_ground_truth_image_info(self, image_id, groundtruth_dict):\n    """"""Adds groundtruth for a single image to be used for evaluation.\n\n    Args:\n      image_id: A unique string/integer identifier for the image.\n      groundtruth_dict: A dictionary containing -\n        standard_fields.InputDataFields.groundtruth_boxes: float32 numpy array\n          of shape [num_boxes, 4] containing `num_boxes` groundtruth boxes of\n          the format [ymin, xmin, ymax, xmax] in absolute image coordinates.\n        standard_fields.InputDataFields.groundtruth_classes: integer numpy array\n          of shape [num_boxes] containing 1-indexed groundtruth classes for the\n          boxes.\n        standard_fields.InputDataFields.groundtruth_group_of: Optional length\n          M numpy boolean array denoting whether a groundtruth box contains a\n          group of instances.\n\n    Raises:\n      ValueError: On adding groundtruth for an image more than once.\n    """"""\n    if image_id in self._image_ids:\n      raise ValueError(\'Image with id {} already added.\'.format(image_id))\n\n    groundtruth_classes = (\n        groundtruth_dict[standard_fields.InputDataFields.groundtruth_classes] -\n        self._label_id_offset)\n    # If the key is not present in the groundtruth_dict or the array is empty\n    # (unless there are no annotations for the groundtruth on this image)\n    # use values from the dictionary or insert None otherwise.\n    if (standard_fields.InputDataFields.groundtruth_group_of in\n        groundtruth_dict.keys() and\n        (groundtruth_dict[standard_fields.InputDataFields.groundtruth_group_of]\n         .size or not groundtruth_classes.size)):\n      groundtruth_group_of = groundtruth_dict[\n          standard_fields.InputDataFields.groundtruth_group_of]\n    else:\n      groundtruth_group_of = None\n      if not len(self._image_ids) % 1000:\n        logging.warn(\n            \'image %s does not have groundtruth group_of flag specified\',\n            image_id)\n    self._evaluation.add_single_ground_truth_image_info(\n        image_id,\n        groundtruth_dict[standard_fields.InputDataFields.groundtruth_boxes],\n        groundtruth_classes,\n        groundtruth_is_difficult_list=None,\n        groundtruth_is_group_of_list=groundtruth_group_of)\n    self._image_ids.update([image_id])\n\n\nObjectDetectionEvalMetrics = collections.namedtuple(\n    \'ObjectDetectionEvalMetrics\', [\n        \'average_precisions\', \'mean_ap\', \'precisions\', \'recalls\', \'corlocs\',\n        \'mean_corloc\'\n    ])\n\n\nclass ObjectDetectionEvaluation(object):\n  """"""Internal implementation of Pascal object detection metrics.""""""\n\n  def __init__(self,\n               num_groundtruth_classes,\n               matching_iou_threshold=0.5,\n               nms_iou_threshold=1.0,\n               nms_max_output_boxes=10000,\n               use_weighted_mean_ap=False,\n               label_id_offset=0):\n    if num_groundtruth_classes < 1:\n      raise ValueError(\'Need at least 1 groundtruth class for evaluation.\')\n\n    self.per_image_eval = per_image_evaluation.PerImageEvaluation(\n        num_groundtruth_classes=num_groundtruth_classes,\n        matching_iou_threshold=matching_iou_threshold,\n        nms_iou_threshold=nms_iou_threshold,\n        nms_max_output_boxes=nms_max_output_boxes)\n    self.num_class = num_groundtruth_classes\n    self.use_weighted_mean_ap = use_weighted_mean_ap\n    self.label_id_offset = label_id_offset\n\n    self.groundtruth_boxes = {}\n    self.groundtruth_class_labels = {}\n    self.groundtruth_masks = {}\n    self.groundtruth_is_difficult_list = {}\n    self.groundtruth_is_group_of_list = {}\n    self.num_gt_instances_per_class = np.zeros(self.num_class, dtype=int)\n    self.num_gt_imgs_per_class = np.zeros(self.num_class, dtype=int)\n\n    self._initialize_detections()\n\n  def _initialize_detections(self):\n    self.detection_keys = set()\n    self.scores_per_class = [[] for _ in range(self.num_class)]\n    self.tp_fp_labels_per_class = [[] for _ in range(self.num_class)]\n    self.num_images_correctly_detected_per_class = np.zeros(self.num_class)\n    self.average_precision_per_class = np.empty(self.num_class, dtype=float)\n    self.average_precision_per_class.fill(np.nan)\n    self.precisions_per_class = []\n    self.recalls_per_class = []\n    self.corloc_per_class = np.ones(self.num_class, dtype=float)\n\n  def clear_detections(self):\n    self._initialize_detections()\n\n  def add_single_ground_truth_image_info(self,\n                                         image_key,\n                                         groundtruth_boxes,\n                                         groundtruth_class_labels,\n                                         groundtruth_is_difficult_list=None,\n                                         groundtruth_is_group_of_list=None,\n                                         groundtruth_masks=None):\n    """"""Adds groundtruth for a single image to be used for evaluation.\n\n    Args:\n      image_key: A unique string/integer identifier for the image.\n      groundtruth_boxes: float32 numpy array of shape [num_boxes, 4]\n        containing `num_boxes` groundtruth boxes of the format\n        [ymin, xmin, ymax, xmax] in absolute image coordinates.\n      groundtruth_class_labels: integer numpy array of shape [num_boxes]\n        containing 0-indexed groundtruth classes for the boxes.\n      groundtruth_is_difficult_list: A length M numpy boolean array denoting\n        whether a ground truth box is a difficult instance or not. To support\n        the case that no boxes are difficult, it is by default set as None.\n      groundtruth_is_group_of_list: A length M numpy boolean array denoting\n          whether a ground truth box is a group-of box or not. To support\n          the case that no boxes are groups-of, it is by default set as None.\n      groundtruth_masks: uint8 numpy array of shape\n        [num_boxes, height, width] containing `num_boxes` groundtruth masks.\n        The mask values range from 0 to 1.\n    """"""\n    if image_key in self.groundtruth_boxes:\n      logging.warn(\n          \'image %s has already been added to the ground truth database.\',\n          image_key)\n      return\n\n    self.groundtruth_boxes[image_key] = groundtruth_boxes\n    self.groundtruth_class_labels[image_key] = groundtruth_class_labels\n    self.groundtruth_masks[image_key] = groundtruth_masks\n    if groundtruth_is_difficult_list is None:\n      num_boxes = groundtruth_boxes.shape[0]\n      groundtruth_is_difficult_list = np.zeros(num_boxes, dtype=bool)\n    self.groundtruth_is_difficult_list[\n        image_key] = groundtruth_is_difficult_list.astype(dtype=bool)\n    if groundtruth_is_group_of_list is None:\n      num_boxes = groundtruth_boxes.shape[0]\n      groundtruth_is_group_of_list = np.zeros(num_boxes, dtype=bool)\n    self.groundtruth_is_group_of_list[\n        image_key] = groundtruth_is_group_of_list.astype(dtype=bool)\n\n    self._update_ground_truth_statistics(\n        groundtruth_class_labels,\n        groundtruth_is_difficult_list.astype(dtype=bool),\n        groundtruth_is_group_of_list.astype(dtype=bool))\n\n  def add_single_detected_image_info(self, image_key, detected_boxes,\n                                     detected_scores, detected_class_labels,\n                                     detected_masks=None):\n    """"""Adds detections for a single image to be used for evaluation.\n\n    Args:\n      image_key: A unique string/integer identifier for the image.\n      detected_boxes: float32 numpy array of shape [num_boxes, 4]\n        containing `num_boxes` detection boxes of the format\n        [ymin, xmin, ymax, xmax] in absolute image coordinates.\n      detected_scores: float32 numpy array of shape [num_boxes] containing\n        detection scores for the boxes.\n      detected_class_labels: integer numpy array of shape [num_boxes] containing\n        0-indexed detection classes for the boxes.\n      detected_masks: np.uint8 numpy array of shape [num_boxes, height, width]\n        containing `num_boxes` detection masks with values ranging\n        between 0 and 1.\n\n    Raises:\n      ValueError: if the number of boxes, scores and class labels differ in\n        length.\n    """"""\n    if (len(detected_boxes) != len(detected_scores) or\n        len(detected_boxes) != len(detected_class_labels)):\n      raise ValueError(\'detected_boxes, detected_scores and \'\n                       \'detected_class_labels should all have same lengths. Got\'\n                       \'[%d, %d, %d]\' % len(detected_boxes),\n                       len(detected_scores), len(detected_class_labels))\n\n    if image_key in self.detection_keys:\n      logging.warn(\n          \'image %s has already been added to the detection result database\',\n          image_key)\n      return\n\n    self.detection_keys.add(image_key)\n    if image_key in self.groundtruth_boxes:\n      groundtruth_boxes = self.groundtruth_boxes[image_key]\n      groundtruth_class_labels = self.groundtruth_class_labels[image_key]\n      # Masks are popped instead of look up. The reason is that we do not want\n      # to keep all masks in memory which can cause memory overflow.\n      groundtruth_masks = self.groundtruth_masks.pop(\n          image_key)\n      groundtruth_is_difficult_list = self.groundtruth_is_difficult_list[\n          image_key]\n      groundtruth_is_group_of_list = self.groundtruth_is_group_of_list[\n          image_key]\n    else:\n      groundtruth_boxes = np.empty(shape=[0, 4], dtype=float)\n      groundtruth_class_labels = np.array([], dtype=int)\n      if detected_masks is None:\n        groundtruth_masks = None\n      else:\n        groundtruth_masks = np.empty(shape=[0, 1, 1], dtype=float)\n      groundtruth_is_difficult_list = np.array([], dtype=bool)\n      groundtruth_is_group_of_list = np.array([], dtype=bool)\n    scores, tp_fp_labels, is_class_correctly_detected_in_image = (\n        self.per_image_eval.compute_object_detection_metrics(\n            detected_boxes=detected_boxes,\n            detected_scores=detected_scores,\n            detected_class_labels=detected_class_labels,\n            groundtruth_boxes=groundtruth_boxes,\n            groundtruth_class_labels=groundtruth_class_labels,\n            groundtruth_is_difficult_list=groundtruth_is_difficult_list,\n            groundtruth_is_group_of_list=groundtruth_is_group_of_list,\n            detected_masks=detected_masks,\n            groundtruth_masks=groundtruth_masks))\n\n    for i in range(self.num_class):\n      if scores[i].shape[0] > 0:\n        self.scores_per_class[i].append(scores[i])\n        self.tp_fp_labels_per_class[i].append(tp_fp_labels[i])\n    (self.num_images_correctly_detected_per_class\n    ) += is_class_correctly_detected_in_image\n\n  def _update_ground_truth_statistics(self, groundtruth_class_labels,\n                                      groundtruth_is_difficult_list,\n                                      groundtruth_is_group_of_list):\n    """"""Update grouth truth statitistics.\n\n    1. Difficult boxes are ignored when counting the number of ground truth\n    instances as done in Pascal VOC devkit.\n    2. Difficult boxes are treated as normal boxes when computing CorLoc related\n    statitistics.\n\n    Args:\n      groundtruth_class_labels: An integer numpy array of length M,\n          representing M class labels of object instances in ground truth\n      groundtruth_is_difficult_list: A boolean numpy array of length M denoting\n          whether a ground truth box is a difficult instance or not\n      groundtruth_is_group_of_list: A boolean numpy array of length M denoting\n          whether a ground truth box is a group-of box or not\n    """"""\n    for class_index in range(self.num_class):\n      num_gt_instances = np.sum(groundtruth_class_labels[\n          ~groundtruth_is_difficult_list\n          & ~groundtruth_is_group_of_list] == class_index)\n      self.num_gt_instances_per_class[class_index] += num_gt_instances\n      if np.any(groundtruth_class_labels == class_index):\n        self.num_gt_imgs_per_class[class_index] += 1\n\n  def evaluate(self):\n    """"""Compute evaluation result.\n\n    Returns:\n      A named tuple with the following fields -\n        average_precision: float numpy array of average precision for\n            each class.\n        mean_ap: mean average precision of all classes, float scalar\n        precisions: List of precisions, each precision is a float numpy\n            array\n        recalls: List of recalls, each recall is a float numpy array\n        corloc: numpy float array\n        mean_corloc: Mean CorLoc score for each class, float scalar\n    """"""\n    if (self.num_gt_instances_per_class == 0).any():\n      logging.warn(\n          \'The following classes have no ground truth examples: %s\',\n          np.squeeze(np.argwhere(self.num_gt_instances_per_class == 0)) +\n          self.label_id_offset)\n\n    if self.use_weighted_mean_ap:\n      all_scores = np.array([], dtype=float)\n      all_tp_fp_labels = np.array([], dtype=bool)\n\n    for class_index in range(self.num_class):\n      if self.num_gt_instances_per_class[class_index] == 0:\n        continue\n      if not self.scores_per_class[class_index]:\n        scores = np.array([], dtype=float)\n        tp_fp_labels = np.array([], dtype=bool)\n      else:\n        scores = np.concatenate(self.scores_per_class[class_index])\n        tp_fp_labels = np.concatenate(self.tp_fp_labels_per_class[class_index])\n      if self.use_weighted_mean_ap:\n        all_scores = np.append(all_scores, scores)\n        all_tp_fp_labels = np.append(all_tp_fp_labels, tp_fp_labels)\n      precision, recall = metrics.compute_precision_recall(\n          scores, tp_fp_labels, self.num_gt_instances_per_class[class_index])\n      self.precisions_per_class.append(precision)\n      self.recalls_per_class.append(recall)\n      average_precision = metrics.compute_average_precision(precision, recall)\n      self.average_precision_per_class[class_index] = average_precision\n\n    self.corloc_per_class = metrics.compute_cor_loc(\n        self.num_gt_imgs_per_class,\n        self.num_images_correctly_detected_per_class)\n\n    if self.use_weighted_mean_ap:\n      num_gt_instances = np.sum(self.num_gt_instances_per_class)\n      precision, recall = metrics.compute_precision_recall(\n          all_scores, all_tp_fp_labels, num_gt_instances)\n      mean_ap = metrics.compute_average_precision(precision, recall)\n    else:\n      mean_ap = np.nanmean(self.average_precision_per_class)\n    mean_corloc = np.nanmean(self.corloc_per_class)\n    return ObjectDetectionEvalMetrics(\n        self.average_precision_per_class, mean_ap, self.precisions_per_class,\n        self.recalls_per_class, self.corloc_per_class, mean_corloc)\n'"
src/object_detection/utils/object_detection_evaluation_test.py,5,"b'# Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n""""""Tests for object_detection.utils.object_detection_evaluation.""""""\n\nimport numpy as np\nimport tensorflow as tf\n\nfrom object_detection.core import standard_fields\nfrom object_detection.utils import object_detection_evaluation\n\n\nclass OpenImagesV2EvaluationTest(tf.test.TestCase):\n\n  def test_returns_correct_metric_values(self):\n    categories = [{\n        \'id\': 1,\n        \'name\': \'cat\'\n    }, {\n        \'id\': 2,\n        \'name\': \'dog\'\n    }, {\n        \'id\': 3,\n        \'name\': \'elephant\'\n    }]\n\n    oiv2_evaluator = object_detection_evaluation.OpenImagesDetectionEvaluator(\n        categories)\n    image_key1 = \'img1\'\n    groundtruth_boxes1 = np.array(\n        [[0, 0, 1, 1], [0, 0, 2, 2], [0, 0, 3, 3]], dtype=float)\n    groundtruth_class_labels1 = np.array([1, 3, 1], dtype=int)\n    oiv2_evaluator.add_single_ground_truth_image_info(image_key1, {\n        standard_fields.InputDataFields.groundtruth_boxes:\n            groundtruth_boxes1,\n        standard_fields.InputDataFields.groundtruth_classes:\n            groundtruth_class_labels1,\n        standard_fields.InputDataFields.groundtruth_group_of:\n            np.array([], dtype=bool)\n    })\n    image_key2 = \'img2\'\n    groundtruth_boxes2 = np.array(\n        [[10, 10, 11, 11], [500, 500, 510, 510], [10, 10, 12, 12]], dtype=float)\n    groundtruth_class_labels2 = np.array([1, 1, 3], dtype=int)\n    groundtruth_is_group_of_list2 = np.array([False, True, False], dtype=bool)\n    oiv2_evaluator.add_single_ground_truth_image_info(image_key2, {\n        standard_fields.InputDataFields.groundtruth_boxes:\n            groundtruth_boxes2,\n        standard_fields.InputDataFields.groundtruth_classes:\n            groundtruth_class_labels2,\n        standard_fields.InputDataFields.groundtruth_group_of:\n            groundtruth_is_group_of_list2\n    })\n    image_key3 = \'img3\'\n    groundtruth_boxes3 = np.array([[0, 0, 1, 1]], dtype=float)\n    groundtruth_class_labels3 = np.array([2], dtype=int)\n    oiv2_evaluator.add_single_ground_truth_image_info(image_key3, {\n        standard_fields.InputDataFields.groundtruth_boxes:\n            groundtruth_boxes3,\n        standard_fields.InputDataFields.groundtruth_classes:\n            groundtruth_class_labels3\n    })\n    # Add detections\n    image_key = \'img2\'\n    detected_boxes = np.array(\n        [[10, 10, 11, 11], [100, 100, 120, 120], [100, 100, 220, 220]],\n        dtype=float)\n    detected_class_labels = np.array([1, 1, 3], dtype=int)\n    detected_scores = np.array([0.7, 0.8, 0.9], dtype=float)\n    oiv2_evaluator.add_single_detected_image_info(image_key, {\n        standard_fields.DetectionResultFields.detection_boxes:\n            detected_boxes,\n        standard_fields.DetectionResultFields.detection_scores:\n            detected_scores,\n        standard_fields.DetectionResultFields.detection_classes:\n            detected_class_labels\n    })\n    metrics = oiv2_evaluator.evaluate()\n    self.assertAlmostEqual(\n        metrics[\'OpenImagesV2_PerformanceByCategory/AP@0.5IOU/dog\'], 0.0)\n    self.assertAlmostEqual(\n        metrics[\'OpenImagesV2_PerformanceByCategory/AP@0.5IOU/elephant\'], 0.0)\n    self.assertAlmostEqual(\n        metrics[\'OpenImagesV2_PerformanceByCategory/AP@0.5IOU/cat\'], 0.16666666)\n    self.assertAlmostEqual(metrics[\'OpenImagesV2_Precision/mAP@0.5IOU\'],\n                           0.05555555)\n    oiv2_evaluator.clear()\n    self.assertFalse(oiv2_evaluator._image_ids)\n\n\nclass PascalEvaluationTest(tf.test.TestCase):\n\n  def test_returns_correct_metric_values_on_boxes(self):\n    categories = [{\'id\': 1, \'name\': \'cat\'},\n                  {\'id\': 2, \'name\': \'dog\'},\n                  {\'id\': 3, \'name\': \'elephant\'}]\n    #  Add groundtruth\n    pascal_evaluator = object_detection_evaluation.PascalDetectionEvaluator(\n        categories)\n    image_key1 = \'img1\'\n    groundtruth_boxes1 = np.array([[0, 0, 1, 1], [0, 0, 2, 2], [0, 0, 3, 3]],\n                                  dtype=float)\n    groundtruth_class_labels1 = np.array([1, 3, 1], dtype=int)\n    pascal_evaluator.add_single_ground_truth_image_info(\n        image_key1,\n        {standard_fields.InputDataFields.groundtruth_boxes: groundtruth_boxes1,\n         standard_fields.InputDataFields.groundtruth_classes:\n         groundtruth_class_labels1,\n         standard_fields.InputDataFields.groundtruth_difficult:\n         np.array([], dtype=bool)})\n    image_key2 = \'img2\'\n    groundtruth_boxes2 = np.array([[10, 10, 11, 11], [500, 500, 510, 510],\n                                   [10, 10, 12, 12]], dtype=float)\n    groundtruth_class_labels2 = np.array([1, 1, 3], dtype=int)\n    groundtruth_is_difficult_list2 = np.array([False, True, False], dtype=bool)\n    pascal_evaluator.add_single_ground_truth_image_info(\n        image_key2,\n        {standard_fields.InputDataFields.groundtruth_boxes: groundtruth_boxes2,\n         standard_fields.InputDataFields.groundtruth_classes:\n         groundtruth_class_labels2,\n         standard_fields.InputDataFields.groundtruth_difficult:\n         groundtruth_is_difficult_list2})\n    image_key3 = \'img3\'\n    groundtruth_boxes3 = np.array([[0, 0, 1, 1]], dtype=float)\n    groundtruth_class_labels3 = np.array([2], dtype=int)\n    pascal_evaluator.add_single_ground_truth_image_info(\n        image_key3,\n        {standard_fields.InputDataFields.groundtruth_boxes: groundtruth_boxes3,\n         standard_fields.InputDataFields.groundtruth_classes:\n         groundtruth_class_labels3})\n\n    # Add detections\n    image_key = \'img2\'\n    detected_boxes = np.array(\n        [[10, 10, 11, 11], [100, 100, 120, 120], [100, 100, 220, 220]],\n        dtype=float)\n    detected_class_labels = np.array([1, 1, 3], dtype=int)\n    detected_scores = np.array([0.7, 0.8, 0.9], dtype=float)\n    pascal_evaluator.add_single_detected_image_info(\n        image_key,\n        {standard_fields.DetectionResultFields.detection_boxes: detected_boxes,\n         standard_fields.DetectionResultFields.detection_scores:\n         detected_scores,\n         standard_fields.DetectionResultFields.detection_classes:\n         detected_class_labels})\n\n    metrics = pascal_evaluator.evaluate()\n    self.assertAlmostEqual(\n        metrics[\'PascalBoxes_PerformanceByCategory/AP@0.5IOU/dog\'], 0.0)\n    self.assertAlmostEqual(\n        metrics[\'PascalBoxes_PerformanceByCategory/AP@0.5IOU/elephant\'], 0.0)\n    self.assertAlmostEqual(\n        metrics[\'PascalBoxes_PerformanceByCategory/AP@0.5IOU/cat\'], 0.16666666)\n    self.assertAlmostEqual(metrics[\'PascalBoxes_Precision/mAP@0.5IOU\'],\n                           0.05555555)\n    pascal_evaluator.clear()\n    self.assertFalse(pascal_evaluator._image_ids)\n\n  def test_returns_correct_metric_values_on_masks(self):\n    categories = [{\'id\': 1, \'name\': \'cat\'},\n                  {\'id\': 2, \'name\': \'dog\'},\n                  {\'id\': 3, \'name\': \'elephant\'}]\n    #  Add groundtruth\n    pascal_evaluator = (\n        object_detection_evaluation.PascalInstanceSegmentationEvaluator(\n            categories))\n    image_key1 = \'img1\'\n    groundtruth_boxes1 = np.array([[0, 0, 1, 1], [0, 0, 2, 2], [0, 0, 3, 3]],\n                                  dtype=float)\n    groundtruth_class_labels1 = np.array([1, 3, 1], dtype=int)\n    groundtruth_masks_1_0 = np.array([[1, 0, 0, 0],\n                                      [1, 0, 0, 0],\n                                      [1, 0, 0, 0]], dtype=np.uint8)\n    groundtruth_masks_1_1 = np.array([[0, 0, 1, 0],\n                                      [0, 0, 1, 0],\n                                      [0, 0, 1, 0]], dtype=np.uint8)\n    groundtruth_masks_1_2 = np.array([[0, 1, 0, 0],\n                                      [0, 1, 0, 0],\n                                      [0, 1, 0, 0]], dtype=np.uint8)\n    groundtruth_masks1 = np.stack(\n        [groundtruth_masks_1_0, groundtruth_masks_1_1, groundtruth_masks_1_2],\n        axis=0)\n\n    pascal_evaluator.add_single_ground_truth_image_info(\n        image_key1, {\n            standard_fields.InputDataFields.groundtruth_boxes:\n                groundtruth_boxes1,\n            standard_fields.InputDataFields.groundtruth_instance_masks:\n                groundtruth_masks1,\n            standard_fields.InputDataFields.groundtruth_classes:\n                groundtruth_class_labels1,\n            standard_fields.InputDataFields.groundtruth_difficult:\n                np.array([], dtype=bool)\n        })\n    image_key2 = \'img2\'\n    groundtruth_boxes2 = np.array([[10, 10, 11, 11], [500, 500, 510, 510],\n                                   [10, 10, 12, 12]], dtype=float)\n    groundtruth_class_labels2 = np.array([1, 1, 3], dtype=int)\n    groundtruth_is_difficult_list2 = np.array([False, True, False], dtype=bool)\n    groundtruth_masks_2_0 = np.array([[1, 1, 1, 1],\n                                      [0, 0, 0, 0],\n                                      [0, 0, 0, 0]], dtype=np.uint8)\n    groundtruth_masks_2_1 = np.array([[0, 0, 0, 0],\n                                      [1, 1, 1, 1],\n                                      [0, 0, 0, 0]], dtype=np.uint8)\n    groundtruth_masks_2_2 = np.array([[0, 0, 0, 0],\n                                      [0, 0, 0, 0],\n                                      [1, 1, 1, 1]], dtype=np.uint8)\n    groundtruth_masks2 = np.stack(\n        [groundtruth_masks_2_0, groundtruth_masks_2_1, groundtruth_masks_2_2],\n        axis=0)\n    pascal_evaluator.add_single_ground_truth_image_info(\n        image_key2, {\n            standard_fields.InputDataFields.groundtruth_boxes:\n                groundtruth_boxes2,\n            standard_fields.InputDataFields.groundtruth_instance_masks:\n                groundtruth_masks2,\n            standard_fields.InputDataFields.groundtruth_classes:\n                groundtruth_class_labels2,\n            standard_fields.InputDataFields.groundtruth_difficult:\n                groundtruth_is_difficult_list2\n        })\n    image_key3 = \'img3\'\n    groundtruth_boxes3 = np.array([[0, 0, 1, 1]], dtype=float)\n    groundtruth_class_labels3 = np.array([2], dtype=int)\n    groundtruth_masks_3_0 = np.array([[1, 1, 1, 1],\n                                      [1, 1, 1, 1],\n                                      [1, 1, 1, 1]], dtype=np.uint8)\n    groundtruth_masks3 = np.stack([groundtruth_masks_3_0], axis=0)\n    pascal_evaluator.add_single_ground_truth_image_info(\n        image_key3, {\n            standard_fields.InputDataFields.groundtruth_boxes:\n                groundtruth_boxes3,\n            standard_fields.InputDataFields.groundtruth_instance_masks:\n                groundtruth_masks3,\n            standard_fields.InputDataFields.groundtruth_classes:\n                groundtruth_class_labels3\n        })\n\n    # Add detections\n    image_key = \'img2\'\n    detected_boxes = np.array(\n        [[10, 10, 11, 11], [100, 100, 120, 120], [100, 100, 220, 220]],\n        dtype=float)\n    detected_class_labels = np.array([1, 1, 3], dtype=int)\n    detected_scores = np.array([0.7, 0.8, 0.9], dtype=float)\n    detected_masks_0 = np.array([[1, 1, 1, 1],\n                                 [0, 0, 1, 0],\n                                 [0, 0, 0, 0]], dtype=np.uint8)\n    detected_masks_1 = np.array([[1, 0, 0, 0],\n                                 [1, 1, 0, 0],\n                                 [0, 0, 0, 0]], dtype=np.uint8)\n    detected_masks_2 = np.array([[0, 1, 0, 0],\n                                 [0, 1, 1, 0],\n                                 [0, 1, 0, 0]], dtype=np.uint8)\n    detected_masks = np.stack(\n        [detected_masks_0, detected_masks_1, detected_masks_2], axis=0)\n\n    pascal_evaluator.add_single_detected_image_info(\n        image_key, {\n            standard_fields.DetectionResultFields.detection_boxes:\n                detected_boxes,\n            standard_fields.DetectionResultFields.detection_masks:\n                detected_masks,\n            standard_fields.DetectionResultFields.detection_scores:\n                detected_scores,\n            standard_fields.DetectionResultFields.detection_classes:\n                detected_class_labels\n        })\n\n    metrics = pascal_evaluator.evaluate()\n\n    self.assertAlmostEqual(\n        metrics[\'PascalMasks_PerformanceByCategory/AP@0.5IOU/dog\'], 0.0)\n    self.assertAlmostEqual(\n        metrics[\'PascalMasks_PerformanceByCategory/AP@0.5IOU/elephant\'], 0.0)\n    self.assertAlmostEqual(\n        metrics[\'PascalMasks_PerformanceByCategory/AP@0.5IOU/cat\'], 0.16666666)\n    self.assertAlmostEqual(metrics[\'PascalMasks_Precision/mAP@0.5IOU\'],\n                           0.05555555)\n    pascal_evaluator.clear()\n    self.assertFalse(pascal_evaluator._image_ids)\n\n  def test_value_error_on_duplicate_images(self):\n    categories = [{\'id\': 1, \'name\': \'cat\'},\n                  {\'id\': 2, \'name\': \'dog\'},\n                  {\'id\': 3, \'name\': \'elephant\'}]\n    #  Add groundtruth\n    pascal_evaluator = object_detection_evaluation.PascalDetectionEvaluator(\n        categories)\n    image_key1 = \'img1\'\n    groundtruth_boxes1 = np.array([[0, 0, 1, 1], [0, 0, 2, 2], [0, 0, 3, 3]],\n                                  dtype=float)\n    groundtruth_class_labels1 = np.array([1, 3, 1], dtype=int)\n    pascal_evaluator.add_single_ground_truth_image_info(\n        image_key1,\n        {standard_fields.InputDataFields.groundtruth_boxes: groundtruth_boxes1,\n         standard_fields.InputDataFields.groundtruth_classes:\n         groundtruth_class_labels1})\n    with self.assertRaises(ValueError):\n      pascal_evaluator.add_single_ground_truth_image_info(\n          image_key1,\n          {standard_fields.InputDataFields.groundtruth_boxes:\n           groundtruth_boxes1,\n           standard_fields.InputDataFields.groundtruth_classes:\n           groundtruth_class_labels1})\n\n\nclass WeightedPascalEvaluationTest(tf.test.TestCase):\n\n  def setUp(self):\n    self.categories = [{\'id\': 1, \'name\': \'cat\'},\n                       {\'id\': 2, \'name\': \'dog\'},\n                       {\'id\': 3, \'name\': \'elephant\'}]\n\n  def create_and_add_common_ground_truth(self):\n    #  Add groundtruth\n    self.wp_eval = (\n        object_detection_evaluation.WeightedPascalDetectionEvaluator(\n            self.categories))\n\n    image_key1 = \'img1\'\n    groundtruth_boxes1 = np.array([[0, 0, 1, 1], [0, 0, 2, 2], [0, 0, 3, 3]],\n                                  dtype=float)\n    groundtruth_class_labels1 = np.array([1, 3, 1], dtype=int)\n    self.wp_eval.add_single_ground_truth_image_info(\n        image_key1,\n        {standard_fields.InputDataFields.groundtruth_boxes: groundtruth_boxes1,\n         standard_fields.InputDataFields.groundtruth_classes:\n         groundtruth_class_labels1})\n    # add \'img2\' separately\n    image_key3 = \'img3\'\n    groundtruth_boxes3 = np.array([[0, 0, 1, 1]], dtype=float)\n    groundtruth_class_labels3 = np.array([2], dtype=int)\n    self.wp_eval.add_single_ground_truth_image_info(\n        image_key3,\n        {standard_fields.InputDataFields.groundtruth_boxes: groundtruth_boxes3,\n         standard_fields.InputDataFields.groundtruth_classes:\n         groundtruth_class_labels3})\n\n  def add_common_detected(self):\n    image_key = \'img2\'\n    detected_boxes = np.array(\n        [[10, 10, 11, 11], [100, 100, 120, 120], [100, 100, 220, 220]],\n        dtype=float)\n    detected_class_labels = np.array([1, 1, 3], dtype=int)\n    detected_scores = np.array([0.7, 0.8, 0.9], dtype=float)\n    self.wp_eval.add_single_detected_image_info(\n        image_key,\n        {standard_fields.DetectionResultFields.detection_boxes: detected_boxes,\n         standard_fields.DetectionResultFields.detection_scores:\n         detected_scores,\n         standard_fields.DetectionResultFields.detection_classes:\n         detected_class_labels})\n\n  def test_returns_correct_metric_values(self):\n    self.create_and_add_common_ground_truth()\n    image_key2 = \'img2\'\n    groundtruth_boxes2 = np.array([[10, 10, 11, 11], [500, 500, 510, 510],\n                                   [10, 10, 12, 12]], dtype=float)\n    groundtruth_class_labels2 = np.array([1, 1, 3], dtype=int)\n    self.wp_eval.add_single_ground_truth_image_info(\n        image_key2,\n        {standard_fields.InputDataFields.groundtruth_boxes: groundtruth_boxes2,\n         standard_fields.InputDataFields.groundtruth_classes:\n         groundtruth_class_labels2\n        })\n    self.add_common_detected()\n\n    metrics = self.wp_eval.evaluate()\n    self.assertAlmostEqual(\n        metrics[self.wp_eval._metric_prefix +\n                \'PerformanceByCategory/AP@0.5IOU/dog\'], 0.0)\n    self.assertAlmostEqual(\n        metrics[self.wp_eval._metric_prefix +\n                \'PerformanceByCategory/AP@0.5IOU/elephant\'], 0.0)\n    self.assertAlmostEqual(\n        metrics[self.wp_eval._metric_prefix +\n                \'PerformanceByCategory/AP@0.5IOU/cat\'], 0.5 / 4)\n    self.assertAlmostEqual(metrics[self.wp_eval._metric_prefix +\n                                   \'Precision/mAP@0.5IOU\'],\n                           1. / (4 + 1 + 2) / 3)\n    self.wp_eval.clear()\n    self.assertFalse(self.wp_eval._image_ids)\n\n  def test_returns_correct_metric_values_with_difficult_list(self):\n    self.create_and_add_common_ground_truth()\n    image_key2 = \'img2\'\n    groundtruth_boxes2 = np.array([[10, 10, 11, 11], [500, 500, 510, 510],\n                                   [10, 10, 12, 12]], dtype=float)\n    groundtruth_class_labels2 = np.array([1, 1, 3], dtype=int)\n    groundtruth_is_difficult_list2 = np.array([False, True, False], dtype=bool)\n    self.wp_eval.add_single_ground_truth_image_info(\n        image_key2,\n        {standard_fields.InputDataFields.groundtruth_boxes: groundtruth_boxes2,\n         standard_fields.InputDataFields.groundtruth_classes:\n         groundtruth_class_labels2,\n         standard_fields.InputDataFields.groundtruth_difficult:\n         groundtruth_is_difficult_list2\n        })\n    self.add_common_detected()\n\n    metrics = self.wp_eval.evaluate()\n    self.assertAlmostEqual(\n        metrics[self.wp_eval._metric_prefix +\n                \'PerformanceByCategory/AP@0.5IOU/dog\'], 0.0)\n    self.assertAlmostEqual(\n        metrics[self.wp_eval._metric_prefix +\n                \'PerformanceByCategory/AP@0.5IOU/elephant\'], 0.0)\n    self.assertAlmostEqual(\n        metrics[self.wp_eval._metric_prefix +\n                \'PerformanceByCategory/AP@0.5IOU/cat\'], 0.5 / 3)\n    self.assertAlmostEqual(metrics[self.wp_eval._metric_prefix +\n                                   \'Precision/mAP@0.5IOU\'],\n                           1. / (3 + 1 + 2) / 3)\n    self.wp_eval.clear()\n    self.assertFalse(self.wp_eval._image_ids)\n\n  def test_value_error_on_duplicate_images(self):\n    #  Add groundtruth\n    self.wp_eval = (\n        object_detection_evaluation.WeightedPascalDetectionEvaluator(\n            self.categories))\n    image_key1 = \'img1\'\n    groundtruth_boxes1 = np.array([[0, 0, 1, 1], [0, 0, 2, 2], [0, 0, 3, 3]],\n                                  dtype=float)\n    groundtruth_class_labels1 = np.array([1, 3, 1], dtype=int)\n    self.wp_eval.add_single_ground_truth_image_info(\n        image_key1,\n        {standard_fields.InputDataFields.groundtruth_boxes: groundtruth_boxes1,\n         standard_fields.InputDataFields.groundtruth_classes:\n         groundtruth_class_labels1})\n    with self.assertRaises(ValueError):\n      self.wp_eval.add_single_ground_truth_image_info(\n          image_key1,\n          {standard_fields.InputDataFields.groundtruth_boxes:\n           groundtruth_boxes1,\n           standard_fields.InputDataFields.groundtruth_classes:\n           groundtruth_class_labels1})\n\n\nclass ObjectDetectionEvaluationTest(tf.test.TestCase):\n\n  def setUp(self):\n    num_groundtruth_classes = 3\n    self.od_eval = object_detection_evaluation.ObjectDetectionEvaluation(\n        num_groundtruth_classes)\n\n    image_key1 = \'img1\'\n    groundtruth_boxes1 = np.array([[0, 0, 1, 1], [0, 0, 2, 2], [0, 0, 3, 3]],\n                                  dtype=float)\n    groundtruth_class_labels1 = np.array([0, 2, 0], dtype=int)\n    self.od_eval.add_single_ground_truth_image_info(\n        image_key1, groundtruth_boxes1, groundtruth_class_labels1)\n    image_key2 = \'img2\'\n    groundtruth_boxes2 = np.array([[10, 10, 11, 11], [500, 500, 510, 510],\n                                   [10, 10, 12, 12]], dtype=float)\n    groundtruth_class_labels2 = np.array([0, 0, 2], dtype=int)\n    groundtruth_is_difficult_list2 = np.array([False, True, False], dtype=bool)\n    groundtruth_is_group_of_list2 = np.array([False, False, True], dtype=bool)\n    self.od_eval.add_single_ground_truth_image_info(\n        image_key2, groundtruth_boxes2, groundtruth_class_labels2,\n        groundtruth_is_difficult_list2, groundtruth_is_group_of_list2)\n\n    image_key3 = \'img3\'\n    groundtruth_boxes3 = np.array([[0, 0, 1, 1]], dtype=float)\n    groundtruth_class_labels3 = np.array([1], dtype=int)\n    self.od_eval.add_single_ground_truth_image_info(\n        image_key3, groundtruth_boxes3, groundtruth_class_labels3)\n\n    image_key = \'img2\'\n    detected_boxes = np.array(\n        [[10, 10, 11, 11], [100, 100, 120, 120], [100, 100, 220, 220]],\n        dtype=float)\n    detected_class_labels = np.array([0, 0, 2], dtype=int)\n    detected_scores = np.array([0.7, 0.8, 0.9], dtype=float)\n    self.od_eval.add_single_detected_image_info(\n        image_key, detected_boxes, detected_scores, detected_class_labels)\n\n  def test_value_error_on_zero_classes(self):\n    with self.assertRaises(ValueError):\n      object_detection_evaluation.ObjectDetectionEvaluation(\n          num_groundtruth_classes=0)\n\n  def test_add_single_ground_truth_image_info(self):\n    expected_num_gt_instances_per_class = np.array([3, 1, 1], dtype=int)\n    expected_num_gt_imgs_per_class = np.array([2, 1, 2], dtype=int)\n    self.assertTrue(np.array_equal(expected_num_gt_instances_per_class,\n                                   self.od_eval.num_gt_instances_per_class))\n    self.assertTrue(np.array_equal(expected_num_gt_imgs_per_class,\n                                   self.od_eval.num_gt_imgs_per_class))\n    groundtruth_boxes2 = np.array([[10, 10, 11, 11], [500, 500, 510, 510],\n                                   [10, 10, 12, 12]], dtype=float)\n    self.assertTrue(np.allclose(self.od_eval.groundtruth_boxes[\'img2\'],\n                                groundtruth_boxes2))\n    groundtruth_is_difficult_list2 = np.array([False, True, False], dtype=bool)\n    self.assertTrue(np.allclose(\n        self.od_eval.groundtruth_is_difficult_list[\'img2\'],\n        groundtruth_is_difficult_list2))\n    groundtruth_is_group_of_list2 = np.array([False, False, True], dtype=bool)\n    self.assertTrue(\n        np.allclose(self.od_eval.groundtruth_is_group_of_list[\'img2\'],\n                    groundtruth_is_group_of_list2))\n\n    groundtruth_class_labels1 = np.array([0, 2, 0], dtype=int)\n    self.assertTrue(np.array_equal(self.od_eval.groundtruth_class_labels[\n        \'img1\'], groundtruth_class_labels1))\n\n  def test_add_single_detected_image_info(self):\n    expected_scores_per_class = [[np.array([0.8, 0.7], dtype=float)], [],\n                                 [np.array([0.9], dtype=float)]]\n    expected_tp_fp_labels_per_class = [[np.array([0, 1], dtype=bool)], [],\n                                       [np.array([0], dtype=bool)]]\n    expected_num_images_correctly_detected_per_class = np.array([0, 0, 0],\n                                                                dtype=int)\n    for i in range(self.od_eval.num_class):\n      for j in range(len(expected_scores_per_class[i])):\n        self.assertTrue(np.allclose(expected_scores_per_class[i][j],\n                                    self.od_eval.scores_per_class[i][j]))\n        self.assertTrue(np.array_equal(expected_tp_fp_labels_per_class[i][\n            j], self.od_eval.tp_fp_labels_per_class[i][j]))\n    self.assertTrue(np.array_equal(\n        expected_num_images_correctly_detected_per_class,\n        self.od_eval.num_images_correctly_detected_per_class))\n\n  def test_evaluate(self):\n    (average_precision_per_class, mean_ap, precisions_per_class,\n     recalls_per_class, corloc_per_class,\n     mean_corloc) = self.od_eval.evaluate()\n    expected_precisions_per_class = [np.array([0, 0.5], dtype=float),\n                                     np.array([], dtype=float),\n                                     np.array([0], dtype=float)]\n    expected_recalls_per_class = [\n        np.array([0, 1. / 3.], dtype=float), np.array([], dtype=float),\n        np.array([0], dtype=float)\n    ]\n    expected_average_precision_per_class = np.array([1. / 6., 0, 0],\n                                                    dtype=float)\n    expected_corloc_per_class = np.array([0, np.divide(0, 0), 0], dtype=float)\n    expected_mean_ap = 1. / 18\n    expected_mean_corloc = 0.0\n    for i in range(self.od_eval.num_class):\n      self.assertTrue(np.allclose(expected_precisions_per_class[i],\n                                  precisions_per_class[i]))\n      self.assertTrue(np.allclose(expected_recalls_per_class[i],\n                                  recalls_per_class[i]))\n    self.assertTrue(np.allclose(expected_average_precision_per_class,\n                                average_precision_per_class))\n    self.assertTrue(np.allclose(expected_corloc_per_class, corloc_per_class))\n    self.assertAlmostEqual(expected_mean_ap, mean_ap)\n    self.assertAlmostEqual(expected_mean_corloc, mean_corloc)\n\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
src/object_detection/utils/ops.py,137,"b'# Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n""""""A module for helper tensorflow ops.""""""\nimport math\nimport numpy as np\nimport six\n\nimport tensorflow as tf\n\nfrom object_detection.core import box_list\nfrom object_detection.core import box_list_ops\nfrom object_detection.core import standard_fields as fields\nfrom object_detection.utils import shape_utils\nfrom object_detection.utils import static_shape\n\n\ndef expanded_shape(orig_shape, start_dim, num_dims):\n  """"""Inserts multiple ones into a shape vector.\n\n  Inserts an all-1 vector of length num_dims at position start_dim into a shape.\n  Can be combined with tf.reshape to generalize tf.expand_dims.\n\n  Args:\n    orig_shape: the shape into which the all-1 vector is added (int32 vector)\n    start_dim: insertion position (int scalar)\n    num_dims: length of the inserted all-1 vector (int scalar)\n  Returns:\n    An int32 vector of length tf.size(orig_shape) + num_dims.\n  """"""\n  with tf.name_scope(\'ExpandedShape\'):\n    start_dim = tf.expand_dims(start_dim, 0)  # scalar to rank-1\n    before = tf.slice(orig_shape, [0], start_dim)\n    add_shape = tf.ones(tf.reshape(num_dims, [1]), dtype=tf.int32)\n    after = tf.slice(orig_shape, start_dim, [-1])\n    new_shape = tf.concat([before, add_shape, after], 0)\n    return new_shape\n\n\ndef normalized_to_image_coordinates(normalized_boxes, image_shape,\n                                    parallel_iterations=32):\n  """"""Converts a batch of boxes from normal to image coordinates.\n\n  Args:\n    normalized_boxes: a float32 tensor of shape [None, num_boxes, 4] in\n      normalized coordinates.\n    image_shape: a float32 tensor of shape [4] containing the image shape.\n    parallel_iterations: parallelism for the map_fn op.\n\n  Returns:\n    absolute_boxes: a float32 tensor of shape [None, num_boxes, 4] containg the\n      boxes in image coordinates.\n  """"""\n  def _to_absolute_coordinates(normalized_boxes):\n    return box_list_ops.to_absolute_coordinates(\n        box_list.BoxList(normalized_boxes),\n        image_shape[1], image_shape[2], check_range=False).get()\n\n  absolute_boxes = shape_utils.static_or_dynamic_map_fn(\n      _to_absolute_coordinates,\n      elems=(normalized_boxes),\n      dtype=tf.float32,\n      parallel_iterations=parallel_iterations,\n      back_prop=True)\n  return absolute_boxes\n\n\ndef meshgrid(x, y):\n  """"""Tiles the contents of x and y into a pair of grids.\n\n  Multidimensional analog of numpy.meshgrid, giving the same behavior if x and y\n  are vectors. Generally, this will give:\n\n  xgrid(i1, ..., i_m, j_1, ..., j_n) = x(j_1, ..., j_n)\n  ygrid(i1, ..., i_m, j_1, ..., j_n) = y(i_1, ..., i_m)\n\n  Keep in mind that the order of the arguments and outputs is reverse relative\n  to the order of the indices they go into, done for compatibility with numpy.\n  The output tensors have the same shapes.  Specifically:\n\n  xgrid.get_shape() = y.get_shape().concatenate(x.get_shape())\n  ygrid.get_shape() = y.get_shape().concatenate(x.get_shape())\n\n  Args:\n    x: A tensor of arbitrary shape and rank. xgrid will contain these values\n       varying in its last dimensions.\n    y: A tensor of arbitrary shape and rank. ygrid will contain these values\n       varying in its first dimensions.\n  Returns:\n    A tuple of tensors (xgrid, ygrid).\n  """"""\n  with tf.name_scope(\'Meshgrid\'):\n    x = tf.convert_to_tensor(x)\n    y = tf.convert_to_tensor(y)\n    x_exp_shape = expanded_shape(tf.shape(x), 0, tf.rank(y))\n    y_exp_shape = expanded_shape(tf.shape(y), tf.rank(y), tf.rank(x))\n\n    xgrid = tf.tile(tf.reshape(x, x_exp_shape), y_exp_shape)\n    ygrid = tf.tile(tf.reshape(y, y_exp_shape), x_exp_shape)\n    new_shape = y.get_shape().concatenate(x.get_shape())\n    xgrid.set_shape(new_shape)\n    ygrid.set_shape(new_shape)\n\n    return xgrid, ygrid\n\n\ndef fixed_padding(inputs, kernel_size, rate=1):\n  """"""Pads the input along the spatial dimensions independently of input size.\n\n  Args:\n    inputs: A tensor of size [batch, height_in, width_in, channels].\n    kernel_size: The kernel to be used in the conv2d or max_pool2d operation.\n                 Should be a positive integer.\n    rate: An integer, rate for atrous convolution.\n\n  Returns:\n    output: A tensor of size [batch, height_out, width_out, channels] with the\n      input, either intact (if kernel_size == 1) or padded (if kernel_size > 1).\n  """"""\n  kernel_size_effective = kernel_size + (kernel_size - 1) * (rate - 1)\n  pad_total = kernel_size_effective - 1\n  pad_beg = pad_total // 2\n  pad_end = pad_total - pad_beg\n  padded_inputs = tf.pad(inputs, [[0, 0], [pad_beg, pad_end],\n                                  [pad_beg, pad_end], [0, 0]])\n  return padded_inputs\n\n\ndef pad_to_multiple(tensor, multiple):\n  """"""Returns the tensor zero padded to the specified multiple.\n\n  Appends 0s to the end of the first and second dimension (height and width) of\n  the tensor until both dimensions are a multiple of the input argument\n  \'multiple\'. E.g. given an input tensor of shape [1, 3, 5, 1] and an input\n  multiple of 4, PadToMultiple will append 0s so that the resulting tensor will\n  be of shape [1, 4, 8, 1].\n\n  Args:\n    tensor: rank 4 float32 tensor, where\n            tensor -> [batch_size, height, width, channels].\n    multiple: the multiple to pad to.\n\n  Returns:\n    padded_tensor: the tensor zero padded to the specified multiple.\n  """"""\n  tensor_shape = tensor.get_shape()\n  batch_size = static_shape.get_batch_size(tensor_shape)\n  tensor_height = static_shape.get_height(tensor_shape)\n  tensor_width = static_shape.get_width(tensor_shape)\n  tensor_depth = static_shape.get_depth(tensor_shape)\n\n  if batch_size is None:\n    batch_size = tf.shape(tensor)[0]\n\n  if tensor_height is None:\n    tensor_height = tf.shape(tensor)[1]\n    padded_tensor_height = tf.to_int32(\n        tf.ceil(tf.to_float(tensor_height) / tf.to_float(multiple))) * multiple\n  else:\n    padded_tensor_height = int(\n        math.ceil(float(tensor_height) / multiple) * multiple)\n\n  if tensor_width is None:\n    tensor_width = tf.shape(tensor)[2]\n    padded_tensor_width = tf.to_int32(\n        tf.ceil(tf.to_float(tensor_width) / tf.to_float(multiple))) * multiple\n  else:\n    padded_tensor_width = int(\n        math.ceil(float(tensor_width) / multiple) * multiple)\n\n  if (padded_tensor_height == tensor_height and\n      padded_tensor_width == tensor_width):\n    return tensor\n\n  if tensor_depth is None:\n    tensor_depth = tf.shape(tensor)[3]\n\n  # Use tf.concat instead of tf.pad to preserve static shape\n  height_pad = tf.zeros([\n      batch_size, padded_tensor_height - tensor_height, tensor_width,\n      tensor_depth\n  ])\n  padded_tensor = tf.concat([tensor, height_pad], 1)\n  width_pad = tf.zeros([\n      batch_size, padded_tensor_height, padded_tensor_width - tensor_width,\n      tensor_depth\n  ])\n  padded_tensor = tf.concat([padded_tensor, width_pad], 2)\n\n  return padded_tensor\n\n\ndef padded_one_hot_encoding(indices, depth, left_pad):\n  """"""Returns a zero padded one-hot tensor.\n\n  This function converts a sparse representation of indices (e.g., [4]) to a\n  zero padded one-hot representation (e.g., [0, 0, 0, 0, 1] with depth = 4 and\n  left_pad = 1). If `indices` is empty, the result will simply be a tensor of\n  shape (0, depth + left_pad). If depth = 0, then this function just returns\n  `None`.\n\n  Args:\n    indices: an integer tensor of shape [num_indices].\n    depth: depth for the one-hot tensor (integer).\n    left_pad: number of zeros to left pad the one-hot tensor with (integer).\n\n  Returns:\n    padded_onehot: a tensor with shape (num_indices, depth + left_pad). Returns\n      `None` if the depth is zero.\n\n  Raises:\n    ValueError: if `indices` does not have rank 1 or if `left_pad` or `depth are\n      either negative or non-integers.\n\n  TODO(rathodv): add runtime checks for depth and indices.\n  """"""\n  if depth < 0 or not isinstance(depth, six.integer_types):\n    raise ValueError(\'`depth` must be a non-negative integer.\')\n  if left_pad < 0 or not isinstance(left_pad, six.integer_types):\n    raise ValueError(\'`left_pad` must be a non-negative integer.\')\n  if depth == 0:\n    return None\n\n  rank = len(indices.get_shape().as_list())\n  if rank != 1:\n    raise ValueError(\'`indices` must have rank 1, but has rank=%s\' % rank)\n\n  def one_hot_and_pad():\n    one_hot = tf.cast(tf.one_hot(tf.cast(indices, tf.int64), depth,\n                                 on_value=1, off_value=0), tf.float32)\n    return tf.pad(one_hot, [[0, 0], [left_pad, 0]], mode=\'CONSTANT\')\n  result = tf.cond(tf.greater(tf.size(indices), 0), one_hot_and_pad,\n                   lambda: tf.zeros((depth + left_pad, 0)))\n  return tf.reshape(result, [-1, depth + left_pad])\n\n\ndef dense_to_sparse_boxes(dense_locations, dense_num_boxes, num_classes):\n  """"""Converts bounding boxes from dense to sparse form.\n\n  Args:\n    dense_locations:  a [max_num_boxes, 4] tensor in which only the first k rows\n      are valid bounding box location coordinates, where k is the sum of\n      elements in dense_num_boxes.\n    dense_num_boxes: a [max_num_classes] tensor indicating the counts of\n       various bounding box classes e.g. [1, 0, 0, 2] means that the first\n       bounding box is of class 0 and the second and third bounding boxes are\n       of class 3. The sum of elements in this tensor is the number of valid\n       bounding boxes.\n    num_classes: number of classes\n\n  Returns:\n    box_locations: a [num_boxes, 4] tensor containing only valid bounding\n       boxes (i.e. the first num_boxes rows of dense_locations)\n    box_classes: a [num_boxes] tensor containing the classes of each bounding\n       box (e.g. dense_num_boxes = [1, 0, 0, 2] => box_classes = [0, 3, 3]\n  """"""\n\n  num_valid_boxes = tf.reduce_sum(dense_num_boxes)\n  box_locations = tf.slice(dense_locations,\n                           tf.constant([0, 0]), tf.stack([num_valid_boxes, 4]))\n  tiled_classes = [tf.tile([i], tf.expand_dims(dense_num_boxes[i], 0))\n                   for i in range(num_classes)]\n  box_classes = tf.concat(tiled_classes, 0)\n  box_locations.set_shape([None, 4])\n  return box_locations, box_classes\n\n\ndef indices_to_dense_vector(indices,\n                            size,\n                            indices_value=1.,\n                            default_value=0,\n                            dtype=tf.float32):\n  """"""Creates dense vector with indices set to specific value and rest to zeros.\n\n  This function exists because it is unclear if it is safe to use\n    tf.sparse_to_dense(indices, [size], 1, validate_indices=False)\n  with indices which are not ordered.\n  This function accepts a dynamic size (e.g. tf.shape(tensor)[0])\n\n  Args:\n    indices: 1d Tensor with integer indices which are to be set to\n        indices_values.\n    size: scalar with size (integer) of output Tensor.\n    indices_value: values of elements specified by indices in the output vector\n    default_value: values of other elements in the output vector.\n    dtype: data type.\n\n  Returns:\n    dense 1D Tensor of shape [size] with indices set to indices_values and the\n        rest set to default_value.\n  """"""\n  size = tf.to_int32(size)\n  zeros = tf.ones([size], dtype=dtype) * default_value\n  values = tf.ones_like(indices, dtype=dtype) * indices_value\n\n  return tf.dynamic_stitch([tf.range(size), tf.to_int32(indices)],\n                           [zeros, values])\n\n\ndef reduce_sum_trailing_dimensions(tensor, ndims):\n  """"""Computes sum across all dimensions following first `ndims` dimensions.""""""\n  return tf.reduce_sum(tensor, axis=tuple(range(ndims, tensor.shape.ndims)))\n\n\ndef retain_groundtruth(tensor_dict, valid_indices):\n  """"""Retains groundtruth by valid indices.\n\n  Args:\n    tensor_dict: a dictionary of following groundtruth tensors -\n      fields.InputDataFields.groundtruth_boxes\n      fields.InputDataFields.groundtruth_instance_masks\n      fields.InputDataFields.groundtruth_classes\n      fields.InputDataFields.groundtruth_is_crowd\n      fields.InputDataFields.groundtruth_area\n      fields.InputDataFields.groundtruth_label_types\n      fields.InputDataFields.groundtruth_difficult\n    valid_indices: a tensor with valid indices for the box-level groundtruth.\n\n  Returns:\n    a dictionary of tensors containing only the groundtruth for valid_indices.\n\n  Raises:\n    ValueError: If the shape of valid_indices is invalid.\n    ValueError: field fields.InputDataFields.groundtruth_boxes is\n      not present in tensor_dict.\n  """"""\n  input_shape = valid_indices.get_shape().as_list()\n  if not (len(input_shape) == 1 or\n          (len(input_shape) == 2 and input_shape[1] == 1)):\n    raise ValueError(\'The shape of valid_indices is invalid.\')\n  valid_indices = tf.reshape(valid_indices, [-1])\n  valid_dict = {}\n  if fields.InputDataFields.groundtruth_boxes in tensor_dict:\n    # Prevents reshape failure when num_boxes is 0.\n    num_boxes = tf.maximum(tf.shape(\n        tensor_dict[fields.InputDataFields.groundtruth_boxes])[0], 1)\n    for key in tensor_dict:\n      if key in [fields.InputDataFields.groundtruth_boxes,\n                 fields.InputDataFields.groundtruth_classes,\n                 fields.InputDataFields.groundtruth_instance_masks]:\n        valid_dict[key] = tf.gather(tensor_dict[key], valid_indices)\n      # Input decoder returns empty tensor when these fields are not provided.\n      # Needs to reshape into [num_boxes, -1] for tf.gather() to work.\n      elif key in [fields.InputDataFields.groundtruth_is_crowd,\n                   fields.InputDataFields.groundtruth_area,\n                   fields.InputDataFields.groundtruth_difficult,\n                   fields.InputDataFields.groundtruth_label_types]:\n        valid_dict[key] = tf.reshape(\n            tf.gather(tf.reshape(tensor_dict[key], [num_boxes, -1]),\n                      valid_indices), [-1])\n      # Fields that are not associated with boxes.\n      else:\n        valid_dict[key] = tensor_dict[key]\n  else:\n    raise ValueError(\'%s not present in input tensor dict.\' % (\n        fields.InputDataFields.groundtruth_boxes))\n  return valid_dict\n\n\ndef retain_groundtruth_with_positive_classes(tensor_dict):\n  """"""Retains only groundtruth with positive class ids.\n\n  Args:\n    tensor_dict: a dictionary of following groundtruth tensors -\n      fields.InputDataFields.groundtruth_boxes\n      fields.InputDataFields.groundtruth_classes\n      fields.InputDataFields.groundtruth_is_crowd\n      fields.InputDataFields.groundtruth_area\n      fields.InputDataFields.groundtruth_label_types\n      fields.InputDataFields.groundtruth_difficult\n\n  Returns:\n    a dictionary of tensors containing only the groundtruth with positive\n    classes.\n\n  Raises:\n    ValueError: If groundtruth_classes tensor is not in tensor_dict.\n  """"""\n  if fields.InputDataFields.groundtruth_classes not in tensor_dict:\n    raise ValueError(\'`groundtruth classes` not in tensor_dict.\')\n  keep_indices = tf.where(tf.greater(\n      tensor_dict[fields.InputDataFields.groundtruth_classes], 0))\n  return retain_groundtruth(tensor_dict, keep_indices)\n\n\ndef replace_nan_groundtruth_label_scores_with_ones(label_scores):\n  """"""Replaces nan label scores with 1.0.\n\n  Args:\n    label_scores: a tensor containing object annoation label scores.\n\n  Returns:\n    a tensor where NaN label scores have been replaced by ones.\n  """"""\n  return tf.where(\n      tf.is_nan(label_scores), tf.ones(tf.shape(label_scores)), label_scores)\n\n\ndef filter_groundtruth_with_crowd_boxes(tensor_dict):\n  """"""Filters out groundtruth with boxes corresponding to crowd.\n\n  Args:\n    tensor_dict: a dictionary of following groundtruth tensors -\n      fields.InputDataFields.groundtruth_boxes\n      fields.InputDataFields.groundtruth_classes\n      fields.InputDataFields.groundtruth_is_crowd\n      fields.InputDataFields.groundtruth_area\n      fields.InputDataFields.groundtruth_label_types\n\n  Returns:\n    a dictionary of tensors containing only the groundtruth that have bounding\n    boxes.\n  """"""\n  if fields.InputDataFields.groundtruth_is_crowd in tensor_dict:\n    is_crowd = tensor_dict[fields.InputDataFields.groundtruth_is_crowd]\n    is_not_crowd = tf.logical_not(is_crowd)\n    is_not_crowd_indices = tf.where(is_not_crowd)\n    tensor_dict = retain_groundtruth(tensor_dict, is_not_crowd_indices)\n  return tensor_dict\n\n\ndef filter_groundtruth_with_nan_box_coordinates(tensor_dict):\n  """"""Filters out groundtruth with no bounding boxes.\n\n  Args:\n    tensor_dict: a dictionary of following groundtruth tensors -\n      fields.InputDataFields.groundtruth_boxes\n      fields.InputDataFields.groundtruth_instance_masks\n      fields.InputDataFields.groundtruth_classes\n      fields.InputDataFields.groundtruth_is_crowd\n      fields.InputDataFields.groundtruth_area\n      fields.InputDataFields.groundtruth_label_types\n\n  Returns:\n    a dictionary of tensors containing only the groundtruth that have bounding\n    boxes.\n  """"""\n  groundtruth_boxes = tensor_dict[fields.InputDataFields.groundtruth_boxes]\n  nan_indicator_vector = tf.greater(tf.reduce_sum(tf.to_int32(\n      tf.is_nan(groundtruth_boxes)), reduction_indices=[1]), 0)\n  valid_indicator_vector = tf.logical_not(nan_indicator_vector)\n  valid_indices = tf.where(valid_indicator_vector)\n\n  return retain_groundtruth(tensor_dict, valid_indices)\n\n\ndef normalize_to_target(inputs,\n                        target_norm_value,\n                        dim,\n                        epsilon=1e-7,\n                        trainable=True,\n                        scope=\'NormalizeToTarget\',\n                        summarize=True):\n  """"""L2 normalizes the inputs across the specified dimension to a target norm.\n\n  This op implements the L2 Normalization layer introduced in\n  Liu, Wei, et al. ""SSD: Single Shot MultiBox Detector.""\n  and Liu, Wei, Andrew Rabinovich, and Alexander C. Berg.\n  ""Parsenet: Looking wider to see better."" and is useful for bringing\n  activations from multiple layers in a convnet to a standard scale.\n\n  Note that the rank of `inputs` must be known and the dimension to which\n  normalization is to be applied should be statically defined.\n\n  TODO(jonathanhuang): Add option to scale by L2 norm of the entire input.\n\n  Args:\n    inputs: A `Tensor` of arbitrary size.\n    target_norm_value: A float value that specifies an initial target norm or\n      a list of floats (whose length must be equal to the depth along the\n      dimension to be normalized) specifying a per-dimension multiplier\n      after normalization.\n    dim: The dimension along which the input is normalized.\n    epsilon: A small value to add to the inputs to avoid dividing by zero.\n    trainable: Whether the norm is trainable or not\n    scope: Optional scope for variable_scope.\n    summarize: Whether or not to add a tensorflow summary for the op.\n\n  Returns:\n    The input tensor normalized to the specified target norm.\n\n  Raises:\n    ValueError: If dim is smaller than the number of dimensions in \'inputs\'.\n    ValueError: If target_norm_value is not a float or a list of floats with\n      length equal to the depth along the dimension to be normalized.\n  """"""\n  with tf.variable_scope(scope, \'NormalizeToTarget\', [inputs]):\n    if not inputs.get_shape():\n      raise ValueError(\'The input rank must be known.\')\n    input_shape = inputs.get_shape().as_list()\n    input_rank = len(input_shape)\n    if dim < 0 or dim >= input_rank:\n      raise ValueError(\n          \'dim must be non-negative but smaller than the input rank.\')\n    if not input_shape[dim]:\n      raise ValueError(\'input shape should be statically defined along \'\n                       \'the specified dimension.\')\n    depth = input_shape[dim]\n    if not (isinstance(target_norm_value, float) or\n            (isinstance(target_norm_value, list) and\n             len(target_norm_value) == depth) and\n            all([isinstance(val, float) for val in target_norm_value])):\n      raise ValueError(\'target_norm_value must be a float or a list of floats \'\n                       \'with length equal to the depth along the dimension to \'\n                       \'be normalized.\')\n    if isinstance(target_norm_value, float):\n      initial_norm = depth * [target_norm_value]\n    else:\n      initial_norm = target_norm_value\n    target_norm = tf.contrib.framework.model_variable(\n        name=\'weights\', dtype=tf.float32,\n        initializer=tf.constant(initial_norm, dtype=tf.float32),\n        trainable=trainable)\n    if summarize:\n      mean = tf.reduce_mean(target_norm)\n      mean = tf.Print(mean, [\'NormalizeToTarget:\', mean])\n      tf.summary.scalar(tf.get_variable_scope().name, mean)\n    lengths = epsilon + tf.sqrt(tf.reduce_sum(tf.square(inputs), dim, True))\n    mult_shape = input_rank*[1]\n    mult_shape[dim] = depth\n    return tf.reshape(target_norm, mult_shape) * tf.truediv(inputs, lengths)\n\n\ndef position_sensitive_crop_regions(image,\n                                    boxes,\n                                    box_ind,\n                                    crop_size,\n                                    num_spatial_bins,\n                                    global_pool,\n                                    extrapolation_value=None):\n  """"""Position-sensitive crop and pool rectangular regions from a feature grid.\n\n  The output crops are split into `spatial_bins_y` vertical bins\n  and `spatial_bins_x` horizontal bins. For each intersection of a vertical\n  and a horizontal bin the output values are gathered by performing\n  `tf.image.crop_and_resize` (bilinear resampling) on a a separate subset of\n  channels of the image. This reduces `depth` by a factor of\n  `(spatial_bins_y * spatial_bins_x)`.\n\n  When global_pool is True, this function implements a differentiable version\n  of position-sensitive RoI pooling used in\n  [R-FCN detection system](https://arxiv.org/abs/1605.06409).\n\n  When global_pool is False, this function implements a differentiable version\n  of position-sensitive assembling operation used in\n  [instance FCN](https://arxiv.org/abs/1603.08678).\n\n  Args:\n    image: A `Tensor`. Must be one of the following types: `uint8`, `int8`,\n      `int16`, `int32`, `int64`, `half`, `float32`, `float64`.\n      A 4-D tensor of shape `[batch, image_height, image_width, depth]`.\n      Both `image_height` and `image_width` need to be positive.\n    boxes: A `Tensor` of type `float32`.\n      A 2-D tensor of shape `[num_boxes, 4]`. The `i`-th row of the tensor\n      specifies the coordinates of a box in the `box_ind[i]` image and is\n      specified in normalized coordinates `[y1, x1, y2, x2]`. A normalized\n      coordinate value of `y` is mapped to the image coordinate at\n      `y * (image_height - 1)`, so as the `[0, 1]` interval of normalized image\n      height is mapped to `[0, image_height - 1] in image height coordinates.\n      We do allow y1 > y2, in which case the sampled crop is an up-down flipped\n      version of the original image. The width dimension is treated similarly.\n      Normalized coordinates outside the `[0, 1]` range are allowed, in which\n      case we use `extrapolation_value` to extrapolate the input image values.\n    box_ind:  A `Tensor` of type `int32`.\n      A 1-D tensor of shape `[num_boxes]` with int32 values in `[0, batch)`.\n      The value of `box_ind[i]` specifies the image that the `i`-th box refers\n      to.\n    crop_size: A list of two integers `[crop_height, crop_width]`. All\n      cropped image patches are resized to this size. The aspect ratio of the\n      image content is not preserved. Both `crop_height` and `crop_width` need\n      to be positive.\n    num_spatial_bins: A list of two integers `[spatial_bins_y, spatial_bins_x]`.\n      Represents the number of position-sensitive bins in y and x directions.\n      Both values should be >= 1. `crop_height` should be divisible by\n      `spatial_bins_y`, and similarly for width.\n      The number of image channels should be divisible by\n      (spatial_bins_y * spatial_bins_x).\n      Suggested value from R-FCN paper: [3, 3].\n    global_pool: A boolean variable.\n      If True, we perform average global pooling on the features assembled from\n        the position-sensitive score maps.\n      If False, we keep the position-pooled features without global pooling\n        over the spatial coordinates.\n      Note that using global_pool=True is equivalent to but more efficient than\n        running the function with global_pool=False and then performing global\n        average pooling.\n    extrapolation_value: An optional `float`. Defaults to `0`.\n      Value used for extrapolation, when applicable.\n  Returns:\n    position_sensitive_features: A 4-D tensor of shape\n      `[num_boxes, K, K, crop_channels]`,\n      where `crop_channels = depth / (spatial_bins_y * spatial_bins_x)`,\n      where K = 1 when global_pool is True (Average-pooled cropped regions),\n      and K = crop_size when global_pool is False.\n  Raises:\n    ValueError: Raised in four situations:\n      `num_spatial_bins` is not >= 1;\n      `num_spatial_bins` does not divide `crop_size`;\n      `(spatial_bins_y*spatial_bins_x)` does not divide `depth`;\n      `bin_crop_size` is not square when global_pool=False due to the\n        constraint in function space_to_depth.\n  """"""\n  total_bins = 1\n  bin_crop_size = []\n\n  for (num_bins, crop_dim) in zip(num_spatial_bins, crop_size):\n    if num_bins < 1:\n      raise ValueError(\'num_spatial_bins should be >= 1\')\n\n    if crop_dim % num_bins != 0:\n      raise ValueError(\'crop_size should be divisible by num_spatial_bins\')\n\n    total_bins *= num_bins\n    bin_crop_size.append(crop_dim // num_bins)\n\n  if not global_pool and bin_crop_size[0] != bin_crop_size[1]:\n    raise ValueError(\'Only support square bin crop size for now.\')\n\n  ymin, xmin, ymax, xmax = tf.unstack(boxes, axis=1)\n  spatial_bins_y, spatial_bins_x = num_spatial_bins\n\n  # Split each box into spatial_bins_y * spatial_bins_x bins.\n  position_sensitive_boxes = []\n  for bin_y in range(spatial_bins_y):\n    step_y = (ymax - ymin) / spatial_bins_y\n    for bin_x in range(spatial_bins_x):\n      step_x = (xmax - xmin) / spatial_bins_x\n      box_coordinates = [ymin + bin_y * step_y,\n                         xmin + bin_x * step_x,\n                         ymin + (bin_y + 1) * step_y,\n                         xmin + (bin_x + 1) * step_x,\n                        ]\n      position_sensitive_boxes.append(tf.stack(box_coordinates, axis=1))\n\n  image_splits = tf.split(value=image, num_or_size_splits=total_bins, axis=3)\n\n  image_crops = []\n  for (split, box) in zip(image_splits, position_sensitive_boxes):\n    crop = tf.image.crop_and_resize(split, box, box_ind, bin_crop_size,\n                                    extrapolation_value=extrapolation_value)\n    image_crops.append(crop)\n\n  if global_pool:\n    # Average over all bins.\n    position_sensitive_features = tf.add_n(image_crops) / len(image_crops)\n    # Then average over spatial positions within the bins.\n    position_sensitive_features = tf.reduce_mean(\n        position_sensitive_features, [1, 2], keep_dims=True)\n  else:\n    # Reorder height/width to depth channel.\n    block_size = bin_crop_size[0]\n    if block_size >= 2:\n      image_crops = [tf.space_to_depth(\n          crop, block_size=block_size) for crop in image_crops]\n\n    # Pack image_crops so that first dimension is for position-senstive boxes.\n    position_sensitive_features = tf.stack(image_crops, axis=0)\n\n    # Unroll the position-sensitive boxes to spatial positions.\n    position_sensitive_features = tf.squeeze(\n        tf.batch_to_space_nd(position_sensitive_features,\n                             block_shape=[1] + num_spatial_bins,\n                             crops=tf.zeros((3, 2), dtype=tf.int32)),\n        squeeze_dims=[0])\n\n    # Reorder back the depth channel.\n    if block_size >= 2:\n      position_sensitive_features = tf.depth_to_space(\n          position_sensitive_features, block_size=block_size)\n\n  return position_sensitive_features\n\n\ndef reframe_box_masks_to_image_masks(box_masks, boxes, image_height,\n                                     image_width):\n  """"""Transforms the box masks back to full image masks.\n\n  Embeds masks in bounding boxes of larger masks whose shapes correspond to\n  image shape.\n\n  Args:\n    box_masks: A tf.float32 tensor of size [num_masks, mask_height, mask_width].\n    boxes: A tf.float32 tensor of size [num_masks, 4] containing the box\n           corners. Row i contains [ymin, xmin, ymax, xmax] of the box\n           corresponding to mask i. Note that the box corners are in\n           normalized coordinates.\n    image_height: Image height. The output mask will have the same height as\n                  the image height.\n    image_width: Image width. The output mask will have the same width as the\n                 image width.\n\n  Returns:\n    A tf.float32 tensor of size [num_masks, image_height, image_width].\n  """"""\n  # TODO(rathodv): Make this a public function.\n  def transform_boxes_relative_to_boxes(boxes, reference_boxes):\n    boxes = tf.reshape(boxes, [-1, 2, 2])\n    min_corner = tf.expand_dims(reference_boxes[:, 0:2], 1)\n    max_corner = tf.expand_dims(reference_boxes[:, 2:4], 1)\n    transformed_boxes = (boxes - min_corner) / (max_corner - min_corner)\n    return tf.reshape(transformed_boxes, [-1, 4])\n\n  box_masks = tf.expand_dims(box_masks, axis=3)\n  num_boxes = tf.shape(box_masks)[0]\n  unit_boxes = tf.concat(\n      [tf.zeros([num_boxes, 2]), tf.ones([num_boxes, 2])], axis=1)\n  reverse_boxes = transform_boxes_relative_to_boxes(unit_boxes, boxes)\n  image_masks = tf.image.crop_and_resize(image=box_masks,\n                                         boxes=reverse_boxes,\n                                         box_ind=tf.range(num_boxes),\n                                         crop_size=[image_height, image_width],\n                                         extrapolation_value=0.0)\n  return tf.squeeze(image_masks, axis=3)\n\n\ndef merge_boxes_with_multiple_labels(boxes, classes, num_classes):\n  """"""Merges boxes with same coordinates and returns K-hot encoded classes.\n\n  Args:\n    boxes: A tf.float32 tensor with shape [N, 4] holding N boxes.\n    classes: A tf.int32 tensor with shape [N] holding class indices.\n      The class index starts at 0.\n    num_classes: total number of classes to use for K-hot encoding.\n\n  Returns:\n    merged_boxes: A tf.float32 tensor with shape [N\', 4] holding boxes,\n      where N\' <= N.\n    class_encodings: A tf.int32 tensor with shape [N\', num_classes] holding\n      k-hot encodings for the merged boxes.\n    merged_box_indices: A tf.int32 tensor with shape [N\'] holding original\n      indices of the boxes.\n  """"""\n  def merge_numpy_boxes(boxes, classes, num_classes):\n    """"""Python function to merge numpy boxes.""""""\n    if boxes.size < 1:\n      return (np.zeros([0, 4], dtype=np.float32),\n              np.zeros([0, num_classes], dtype=np.int32),\n              np.zeros([0], dtype=np.int32))\n    box_to_class_indices = {}\n    for box_index in range(boxes.shape[0]):\n      box = tuple(boxes[box_index, :].tolist())\n      class_index = classes[box_index]\n      if box not in box_to_class_indices:\n        box_to_class_indices[box] = [box_index, np.zeros([num_classes])]\n      box_to_class_indices[box][1][class_index] = 1\n    merged_boxes = np.vstack(box_to_class_indices.keys()).astype(np.float32)\n    class_encodings = [item[1] for item in box_to_class_indices.values()]\n    class_encodings = np.vstack(class_encodings).astype(np.int32)\n    merged_box_indices = [item[0] for item in box_to_class_indices.values()]\n    merged_box_indices = np.array(merged_box_indices).astype(np.int32)\n    return merged_boxes, class_encodings, merged_box_indices\n\n  merged_boxes, class_encodings, merged_box_indices = tf.py_func(\n      merge_numpy_boxes, [boxes, classes, num_classes],\n      [tf.float32, tf.int32, tf.int32])\n  merged_boxes = tf.reshape(merged_boxes, [-1, 4])\n  class_encodings = tf.reshape(class_encodings, [-1, num_classes])\n  merged_box_indices = tf.reshape(merged_box_indices, [-1])\n  return merged_boxes, class_encodings, merged_box_indices\n\n\ndef nearest_neighbor_upsampling(input_tensor, scale):\n  """"""Nearest neighbor upsampling implementation.\n\n  Nearest neighbor upsampling function that maps input tensor with shape\n  [batch_size, height, width, channels] to [batch_size, height * scale\n  , width * scale, channels]. This implementation only uses reshape and tile to\n  make it compatible with certain hardware.\n\n  Args:\n    input_tensor: A float32 tensor of size [batch, height_in, width_in,\n      channels].\n    scale: An integer multiple to scale resolution of input data.\n  Returns:\n    data_up: A float32 tensor of size\n      [batch, height_in*scale, width_in*scale, channels].\n  """"""\n  shape = shape_utils.combined_static_and_dynamic_shape(input_tensor)\n  shape_before_tile = [shape[0], shape[1], 1, shape[2], 1, shape[3]]\n  shape_after_tile = [shape[0], shape[1] * scale, shape[2] * scale, shape[3]]\n  data_reshaped = tf.reshape(input_tensor, shape_before_tile)\n  resized_tensor = tf.tile(data_reshaped, [1, 1, scale, 1, scale, 1])\n  resized_tensor = tf.reshape(resized_tensor, shape_after_tile)\n  return resized_tensor\n\n\ndef matmul_gather_on_zeroth_axis(params, indices, scope=None):\n  """"""Matrix multiplication based implementation of tf.gather on zeroth axis.\n\n  TODO(rathodv, jonathanhuang): enable sparse matmul option.\n\n  Args:\n    params: A float32 Tensor. The tensor from which to gather values.\n      Must be at least rank 1.\n    indices: A Tensor. Must be one of the following types: int32, int64.\n      Must be in range [0, params.shape[0])\n    scope: A name for the operation (optional).\n\n  Returns:\n    A Tensor. Has the same type as params. Values from params gathered\n    from indices given by indices, with shape indices.shape + params.shape[1:].\n  """"""\n  with tf.name_scope(scope, \'MatMulGather\'):\n    params_shape = shape_utils.combined_static_and_dynamic_shape(params)\n    indices_shape = shape_utils.combined_static_and_dynamic_shape(indices)\n    params2d = tf.reshape(params, [params_shape[0], -1])\n    indicator_matrix = tf.one_hot(indices, params_shape[0])\n    gathered_result_flattened = tf.matmul(indicator_matrix, params2d)\n    return tf.reshape(gathered_result_flattened,\n                      tf.stack(indices_shape + params_shape[1:]))\n\n\ndef matmul_crop_and_resize(image, boxes, crop_size, scope=None):\n  """"""Matrix multiplication based implementation of the crop and resize op.\n\n  Extracts crops from the input image tensor and bilinearly resizes them\n  (possibly with aspect ratio change) to a common output size specified by\n  crop_size. This is more general than the crop_to_bounding_box op which\n  extracts a fixed size slice from the input image and does not allow\n  resizing or aspect ratio change.\n\n  Returns a tensor with crops from the input image at positions defined at\n  the bounding box locations in boxes. The cropped boxes are all resized\n  (with bilinear interpolation) to a fixed size = `[crop_height, crop_width]`.\n  The result is a 4-D tensor `[num_boxes, crop_height, crop_width, depth]`.\n\n  Running time complexity:\n    O((# channels) * (# boxes) * (crop_size)^2 * M), where M is the number\n  of pixels of the longer edge of the image.\n\n  Note that this operation is meant to replicate the behavior of the standard\n  tf.image.crop_and_resize operation but there are a few differences.\n  Specifically:\n    1) The extrapolation value (the values that are interpolated from outside\n      the bounds of the image window) is always zero\n    2) Only XLA supported operations are used (e.g., matrix multiplication).\n    3) There is no `box_indices` argument --- to run this op on multiple images,\n      one must currently call this op independently on each image.\n    4) All shapes and the `crop_size` parameter are assumed to be statically\n      defined.  Moreover, the number of boxes must be strictly nonzero.\n\n  Args:\n    image: A `Tensor`. Must be one of the following types: `uint8`, `int8`,\n      `int16`, `int32`, `int64`, `half`, `float32`, `float64`.\n      A 4-D tensor of shape `[batch, image_height, image_width, depth]`.\n      Both `image_height` and `image_width` need to be positive.\n    boxes: A `Tensor` of type `float32`.\n      A 2-D tensor of shape `[num_boxes, 4]`. The `i`-th row of the tensor\n      specifies the coordinates of a box in the `box_ind[i]` image and is\n      specified in normalized coordinates `[y1, x1, y2, x2]`. A normalized\n      coordinate value of `y` is mapped to the image coordinate at\n      `y * (image_height - 1)`, so as the `[0, 1]` interval of normalized image\n      height is mapped to `[0, image_height - 1] in image height coordinates.\n      We do allow y1 > y2, in which case the sampled crop is an up-down flipped\n      version of the original image. The width dimension is treated similarly.\n      Normalized coordinates outside the `[0, 1]` range are allowed, in which\n      case we use `extrapolation_value` to extrapolate the input image values.\n    crop_size: A list of two integers `[crop_height, crop_width]`. All\n      cropped image patches are resized to this size. The aspect ratio of the\n      image content is not preserved. Both `crop_height` and `crop_width` need\n      to be positive.\n    scope: A name for the operation (optional).\n\n  Returns:\n    A 4-D tensor of shape `[num_boxes, crop_height, crop_width, depth]`\n\n  Raises:\n    ValueError: if image tensor does not have shape\n      `[1, image_height, image_width, depth]` and all dimensions statically\n      defined.\n    ValueError: if boxes tensor does not have shape `[num_boxes, 4]` where\n      num_boxes > 0.\n    ValueError: if crop_size is not a list of two positive integers\n  """"""\n  img_shape = image.shape.as_list()\n  boxes_shape = boxes.shape.as_list()\n  _, img_height, img_width, _ = img_shape\n  if not isinstance(crop_size, list) or len(crop_size) != 2:\n    raise ValueError(\'`crop_size` must be a list of length 2\')\n  dimensions = img_shape + crop_size + boxes_shape\n  if not all([isinstance(dim, int) for dim in dimensions]):\n    raise ValueError(\'all input shapes must be statically defined\')\n  if len(crop_size) != 2:\n    raise ValueError(\'`crop_size` must be a list of length 2\')\n  if len(boxes_shape) != 2 or boxes_shape[1] != 4:\n    raise ValueError(\'`boxes` should have shape `[num_boxes, 4]`\')\n  if len(img_shape) != 4 and img_shape[0] != 1:\n    raise ValueError(\'image should have shape \'\n                     \'`[1, image_height, image_width, depth]`\')\n  num_crops = boxes_shape[0]\n  if not num_crops > 0:\n    raise ValueError(\'number of boxes must be > 0\')\n  if not (crop_size[0] > 0 and crop_size[1] > 0):\n    raise ValueError(\'`crop_size` must be a list of two positive integers.\')\n\n  def _lin_space_weights(num, img_size):\n    if num > 1:\n      alpha = (img_size - 1) / float(num - 1)\n      indices = np.reshape(np.arange(num), (1, num))\n      start_weights = alpha * (num - 1 - indices)\n      stop_weights = alpha * indices\n    else:\n      start_weights = num * [.5 * (img_size - 1)]\n      stop_weights = num * [.5 * (img_size - 1)]\n    return (tf.constant(start_weights, dtype=tf.float32),\n            tf.constant(stop_weights, dtype=tf.float32))\n\n  with tf.name_scope(scope, \'MatMulCropAndResize\'):\n    y1_weights, y2_weights = _lin_space_weights(crop_size[0], img_height)\n    x1_weights, x2_weights = _lin_space_weights(crop_size[1], img_width)\n    [y1, x1, y2, x2] = tf.split(value=boxes, num_or_size_splits=4, axis=1)\n\n    # Pixel centers of input image and grid points along height and width\n    image_idx_h = tf.constant(\n        np.reshape(np.arange(img_height), (1, 1, img_height)), dtype=tf.float32)\n    image_idx_w = tf.constant(\n        np.reshape(np.arange(img_width), (1, 1, img_width)), dtype=tf.float32)\n    grid_pos_h = tf.expand_dims(y1 * y1_weights + y2 * y2_weights, 2)\n    grid_pos_w = tf.expand_dims(x1 * x1_weights + x2 * x2_weights, 2)\n\n    # Create kernel matrices of pairwise kernel evaluations between pixel\n    # centers of image and grid points.\n    kernel_h = tf.nn.relu(1 - tf.abs(image_idx_h - grid_pos_h))\n    kernel_w = tf.nn.relu(1 - tf.abs(image_idx_w - grid_pos_w))\n\n    # TODO(jonathanhuang): investigate whether all channels can be processed\n    # without the explicit unstack --- possibly with a permute and map_fn call.\n    result_channels = []\n    for channel in tf.unstack(image, axis=3):\n      result_channels.append(\n          tf.matmul(\n              tf.matmul(kernel_h, tf.tile(channel, [num_crops, 1, 1])),\n              kernel_w, transpose_b=True))\n    return tf.stack(result_channels, axis=3)\n'"
src/object_detection/utils/ops_test.py,151,"b'# Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n""""""Tests for object_detection.utils.ops.""""""\nimport numpy as np\nimport tensorflow as tf\n\nfrom object_detection.core import standard_fields as fields\nfrom object_detection.utils import ops\nfrom object_detection.utils import test_case\n\n\nclass NormalizedToImageCoordinatesTest(tf.test.TestCase):\n\n  def test_normalized_to_image_coordinates(self):\n    normalized_boxes = tf.placeholder(tf.float32, shape=(None, 1, 4))\n    normalized_boxes_np = np.array([[[0.0, 0.0, 1.0, 1.0]],\n                                    [[0.5, 0.5, 1.0, 1.0]]])\n    image_shape = tf.convert_to_tensor([1, 4, 4, 3], dtype=tf.int32)\n    absolute_boxes = ops.normalized_to_image_coordinates(normalized_boxes,\n                                                         image_shape,\n                                                         parallel_iterations=2)\n\n    expected_boxes = np.array([[[0, 0, 4, 4]],\n                               [[2, 2, 4, 4]]])\n    with self.test_session() as sess:\n      absolute_boxes = sess.run(absolute_boxes,\n                                feed_dict={normalized_boxes:\n                                           normalized_boxes_np})\n\n    self.assertAllEqual(absolute_boxes, expected_boxes)\n\n\nclass ReduceSumTrailingDimensions(tf.test.TestCase):\n\n  def test_reduce_sum_trailing_dimensions(self):\n    input_tensor = tf.placeholder(tf.float32, shape=[None, None, None])\n    reduced_tensor = ops.reduce_sum_trailing_dimensions(input_tensor, ndims=2)\n    with self.test_session() as sess:\n      reduced_np = sess.run(reduced_tensor,\n                            feed_dict={input_tensor: np.ones((2, 2, 2),\n                                                             np.float32)})\n    self.assertAllClose(reduced_np, 2 * np.ones((2, 2), np.float32))\n\n\nclass MeshgridTest(tf.test.TestCase):\n\n  def test_meshgrid_numpy_comparison(self):\n    """"""Tests meshgrid op with vectors, for which it should match numpy.""""""\n    x = np.arange(4)\n    y = np.arange(6)\n    exp_xgrid, exp_ygrid = np.meshgrid(x, y)\n    xgrid, ygrid = ops.meshgrid(x, y)\n    with self.test_session() as sess:\n      xgrid_output, ygrid_output = sess.run([xgrid, ygrid])\n      self.assertAllEqual(xgrid_output, exp_xgrid)\n      self.assertAllEqual(ygrid_output, exp_ygrid)\n\n  def test_meshgrid_multidimensional(self):\n    np.random.seed(18)\n    x = np.random.rand(4, 1, 2).astype(np.float32)\n    y = np.random.rand(2, 3).astype(np.float32)\n\n    xgrid, ygrid = ops.meshgrid(x, y)\n\n    grid_shape = list(y.shape) + list(x.shape)\n    self.assertEqual(xgrid.get_shape().as_list(), grid_shape)\n    self.assertEqual(ygrid.get_shape().as_list(), grid_shape)\n    with self.test_session() as sess:\n      xgrid_output, ygrid_output = sess.run([xgrid, ygrid])\n\n    # Check the shape of the output grids\n    self.assertEqual(xgrid_output.shape, tuple(grid_shape))\n    self.assertEqual(ygrid_output.shape, tuple(grid_shape))\n\n    # Check a few elements\n    test_elements = [((3, 0, 0), (1, 2)),\n                     ((2, 0, 1), (0, 0)),\n                     ((0, 0, 0), (1, 1))]\n    for xind, yind in test_elements:\n      # These are float equality tests, but the meshgrid op should not introduce\n      # rounding.\n      self.assertEqual(xgrid_output[yind + xind], x[xind])\n      self.assertEqual(ygrid_output[yind + xind], y[yind])\n\n\nclass OpsTestFixedPadding(tf.test.TestCase):\n\n  def test_3x3_kernel(self):\n    tensor = tf.constant([[[[0.], [0.]], [[0.], [0.]]]])\n    padded_tensor = ops.fixed_padding(tensor, 3)\n    with self.test_session() as sess:\n      padded_tensor_out = sess.run(padded_tensor)\n    self.assertEqual((1, 4, 4, 1), padded_tensor_out.shape)\n\n  def test_5x5_kernel(self):\n    tensor = tf.constant([[[[0.], [0.]], [[0.], [0.]]]])\n    padded_tensor = ops.fixed_padding(tensor, 5)\n    with self.test_session() as sess:\n      padded_tensor_out = sess.run(padded_tensor)\n    self.assertEqual((1, 6, 6, 1), padded_tensor_out.shape)\n\n  def test_3x3_atrous_kernel(self):\n    tensor = tf.constant([[[[0.], [0.]], [[0.], [0.]]]])\n    padded_tensor = ops.fixed_padding(tensor, 3, 2)\n    with self.test_session() as sess:\n      padded_tensor_out = sess.run(padded_tensor)\n    self.assertEqual((1, 6, 6, 1), padded_tensor_out.shape)\n\n\nclass OpsTestPadToMultiple(tf.test.TestCase):\n\n  def test_zero_padding(self):\n    tensor = tf.constant([[[[0.], [0.]], [[0.], [0.]]]])\n    padded_tensor = ops.pad_to_multiple(tensor, 1)\n    with self.test_session() as sess:\n      padded_tensor_out = sess.run(padded_tensor)\n    self.assertEqual((1, 2, 2, 1), padded_tensor_out.shape)\n\n  def test_no_padding(self):\n    tensor = tf.constant([[[[0.], [0.]], [[0.], [0.]]]])\n    padded_tensor = ops.pad_to_multiple(tensor, 2)\n    with self.test_session() as sess:\n      padded_tensor_out = sess.run(padded_tensor)\n    self.assertEqual((1, 2, 2, 1), padded_tensor_out.shape)\n\n  def test_padding(self):\n    tensor = tf.constant([[[[0.], [0.]], [[0.], [0.]]]])\n    padded_tensor = ops.pad_to_multiple(tensor, 4)\n    with self.test_session() as sess:\n      padded_tensor_out = sess.run(padded_tensor)\n    self.assertEqual((1, 4, 4, 1), padded_tensor_out.shape)\n\n\nclass OpsTestPaddedOneHotEncoding(tf.test.TestCase):\n\n  def test_correct_one_hot_tensor_with_no_pad(self):\n    indices = tf.constant([1, 2, 3, 5])\n    one_hot_tensor = ops.padded_one_hot_encoding(indices, depth=6, left_pad=0)\n    expected_tensor = np.array([[0, 1, 0, 0, 0, 0],\n                                [0, 0, 1, 0, 0, 0],\n                                [0, 0, 0, 1, 0, 0],\n                                [0, 0, 0, 0, 0, 1]], np.float32)\n    with self.test_session() as sess:\n      out_one_hot_tensor = sess.run(one_hot_tensor)\n      self.assertAllClose(out_one_hot_tensor, expected_tensor, rtol=1e-10,\n                          atol=1e-10)\n\n  def test_correct_one_hot_tensor_with_pad_one(self):\n    indices = tf.constant([1, 2, 3, 5])\n    one_hot_tensor = ops.padded_one_hot_encoding(indices, depth=6, left_pad=1)\n    expected_tensor = np.array([[0, 0, 1, 0, 0, 0, 0],\n                                [0, 0, 0, 1, 0, 0, 0],\n                                [0, 0, 0, 0, 1, 0, 0],\n                                [0, 0, 0, 0, 0, 0, 1]], np.float32)\n    with self.test_session() as sess:\n      out_one_hot_tensor = sess.run(one_hot_tensor)\n      self.assertAllClose(out_one_hot_tensor, expected_tensor, rtol=1e-10,\n                          atol=1e-10)\n\n  def test_correct_one_hot_tensor_with_pad_three(self):\n    indices = tf.constant([1, 2, 3, 5])\n    one_hot_tensor = ops.padded_one_hot_encoding(indices, depth=6, left_pad=3)\n    expected_tensor = np.array([[0, 0, 0, 0, 1, 0, 0, 0, 0],\n                                [0, 0, 0, 0, 0, 1, 0, 0, 0],\n                                [0, 0, 0, 0, 0, 0, 1, 0, 0],\n                                [0, 0, 0, 0, 0, 0, 0, 0, 1]], np.float32)\n    with self.test_session() as sess:\n      out_one_hot_tensor = sess.run(one_hot_tensor)\n      self.assertAllClose(out_one_hot_tensor, expected_tensor, rtol=1e-10,\n                          atol=1e-10)\n\n  def test_correct_padded_one_hot_tensor_with_empty_indices(self):\n    depth = 6\n    pad = 2\n    indices = tf.constant([])\n    one_hot_tensor = ops.padded_one_hot_encoding(\n        indices, depth=depth, left_pad=pad)\n    expected_tensor = np.zeros((0, depth + pad))\n    with self.test_session() as sess:\n      out_one_hot_tensor = sess.run(one_hot_tensor)\n      self.assertAllClose(out_one_hot_tensor, expected_tensor, rtol=1e-10,\n                          atol=1e-10)\n\n  def test_return_none_on_zero_depth(self):\n    indices = tf.constant([1, 2, 3, 4, 5])\n    one_hot_tensor = ops.padded_one_hot_encoding(indices, depth=0, left_pad=2)\n    self.assertEqual(one_hot_tensor, None)\n\n  def test_raise_value_error_on_rank_two_input(self):\n    indices = tf.constant(1.0, shape=(2, 3))\n    with self.assertRaises(ValueError):\n      ops.padded_one_hot_encoding(indices, depth=6, left_pad=2)\n\n  def test_raise_value_error_on_negative_pad(self):\n    indices = tf.constant(1.0, shape=(2, 3))\n    with self.assertRaises(ValueError):\n      ops.padded_one_hot_encoding(indices, depth=6, left_pad=-1)\n\n  def test_raise_value_error_on_float_pad(self):\n    indices = tf.constant(1.0, shape=(2, 3))\n    with self.assertRaises(ValueError):\n      ops.padded_one_hot_encoding(indices, depth=6, left_pad=0.1)\n\n  def test_raise_value_error_on_float_depth(self):\n    indices = tf.constant(1.0, shape=(2, 3))\n    with self.assertRaises(ValueError):\n      ops.padded_one_hot_encoding(indices, depth=0.1, left_pad=2)\n\n\nclass OpsDenseToSparseBoxesTest(tf.test.TestCase):\n\n  def test_return_all_boxes_when_all_input_boxes_are_valid(self):\n    num_classes = 4\n    num_valid_boxes = 3\n    code_size = 4\n    dense_location_placeholder = tf.placeholder(tf.float32,\n                                                shape=(num_valid_boxes,\n                                                       code_size))\n    dense_num_boxes_placeholder = tf.placeholder(tf.int32, shape=(num_classes))\n    box_locations, box_classes = ops.dense_to_sparse_boxes(\n        dense_location_placeholder, dense_num_boxes_placeholder, num_classes)\n    feed_dict = {dense_location_placeholder: np.random.uniform(\n        size=[num_valid_boxes, code_size]),\n                 dense_num_boxes_placeholder: np.array([1, 0, 0, 2],\n                                                       dtype=np.int32)}\n\n    expected_box_locations = feed_dict[dense_location_placeholder]\n    expected_box_classses = np.array([0, 3, 3])\n    with self.test_session() as sess:\n      box_locations, box_classes = sess.run([box_locations, box_classes],\n                                            feed_dict=feed_dict)\n\n    self.assertAllClose(box_locations, expected_box_locations, rtol=1e-6,\n                        atol=1e-6)\n    self.assertAllEqual(box_classes, expected_box_classses)\n\n  def test_return_only_valid_boxes_when_input_contains_invalid_boxes(self):\n    num_classes = 4\n    num_valid_boxes = 3\n    num_boxes = 10\n    code_size = 4\n\n    dense_location_placeholder = tf.placeholder(tf.float32, shape=(num_boxes,\n                                                                   code_size))\n    dense_num_boxes_placeholder = tf.placeholder(tf.int32, shape=(num_classes))\n    box_locations, box_classes = ops.dense_to_sparse_boxes(\n        dense_location_placeholder, dense_num_boxes_placeholder, num_classes)\n    feed_dict = {dense_location_placeholder: np.random.uniform(\n        size=[num_boxes, code_size]),\n                 dense_num_boxes_placeholder: np.array([1, 0, 0, 2],\n                                                       dtype=np.int32)}\n\n    expected_box_locations = (feed_dict[dense_location_placeholder]\n                              [:num_valid_boxes])\n    expected_box_classses = np.array([0, 3, 3])\n    with self.test_session() as sess:\n      box_locations, box_classes = sess.run([box_locations, box_classes],\n                                            feed_dict=feed_dict)\n\n    self.assertAllClose(box_locations, expected_box_locations, rtol=1e-6,\n                        atol=1e-6)\n    self.assertAllEqual(box_classes, expected_box_classses)\n\n\nclass OpsTestIndicesToDenseVector(tf.test.TestCase):\n\n  def test_indices_to_dense_vector(self):\n    size = 10000\n    num_indices = np.random.randint(size)\n    rand_indices = np.random.permutation(np.arange(size))[0:num_indices]\n\n    expected_output = np.zeros(size, dtype=np.float32)\n    expected_output[rand_indices] = 1.\n\n    tf_rand_indices = tf.constant(rand_indices)\n    indicator = ops.indices_to_dense_vector(tf_rand_indices, size)\n\n    with self.test_session() as sess:\n      output = sess.run(indicator)\n      self.assertAllEqual(output, expected_output)\n      self.assertEqual(output.dtype, expected_output.dtype)\n\n  def test_indices_to_dense_vector_size_at_inference(self):\n    size = 5000\n    num_indices = 250\n    all_indices = np.arange(size)\n    rand_indices = np.random.permutation(all_indices)[0:num_indices]\n\n    expected_output = np.zeros(size, dtype=np.float32)\n    expected_output[rand_indices] = 1.\n\n    tf_all_indices = tf.placeholder(tf.int32)\n    tf_rand_indices = tf.constant(rand_indices)\n    indicator = ops.indices_to_dense_vector(tf_rand_indices,\n                                            tf.shape(tf_all_indices)[0])\n    feed_dict = {tf_all_indices: all_indices}\n\n    with self.test_session() as sess:\n      output = sess.run(indicator, feed_dict=feed_dict)\n      self.assertAllEqual(output, expected_output)\n      self.assertEqual(output.dtype, expected_output.dtype)\n\n  def test_indices_to_dense_vector_int(self):\n    size = 500\n    num_indices = 25\n    rand_indices = np.random.permutation(np.arange(size))[0:num_indices]\n\n    expected_output = np.zeros(size, dtype=np.int64)\n    expected_output[rand_indices] = 1\n\n    tf_rand_indices = tf.constant(rand_indices)\n    indicator = ops.indices_to_dense_vector(\n        tf_rand_indices, size, 1, dtype=tf.int64)\n\n    with self.test_session() as sess:\n      output = sess.run(indicator)\n      self.assertAllEqual(output, expected_output)\n      self.assertEqual(output.dtype, expected_output.dtype)\n\n  def test_indices_to_dense_vector_custom_values(self):\n    size = 100\n    num_indices = 10\n    rand_indices = np.random.permutation(np.arange(size))[0:num_indices]\n    indices_value = np.random.rand(1)\n    default_value = np.random.rand(1)\n\n    expected_output = np.float32(np.ones(size) * default_value)\n    expected_output[rand_indices] = indices_value\n\n    tf_rand_indices = tf.constant(rand_indices)\n    indicator = ops.indices_to_dense_vector(\n        tf_rand_indices,\n        size,\n        indices_value=indices_value,\n        default_value=default_value)\n\n    with self.test_session() as sess:\n      output = sess.run(indicator)\n      self.assertAllClose(output, expected_output)\n      self.assertEqual(output.dtype, expected_output.dtype)\n\n  def test_indices_to_dense_vector_all_indices_as_input(self):\n    size = 500\n    num_indices = 500\n    rand_indices = np.random.permutation(np.arange(size))[0:num_indices]\n\n    expected_output = np.ones(size, dtype=np.float32)\n\n    tf_rand_indices = tf.constant(rand_indices)\n    indicator = ops.indices_to_dense_vector(tf_rand_indices, size)\n\n    with self.test_session() as sess:\n      output = sess.run(indicator)\n      self.assertAllEqual(output, expected_output)\n      self.assertEqual(output.dtype, expected_output.dtype)\n\n  def test_indices_to_dense_vector_empty_indices_as_input(self):\n    size = 500\n    rand_indices = []\n\n    expected_output = np.zeros(size, dtype=np.float32)\n\n    tf_rand_indices = tf.constant(rand_indices)\n    indicator = ops.indices_to_dense_vector(tf_rand_indices, size)\n\n    with self.test_session() as sess:\n      output = sess.run(indicator)\n      self.assertAllEqual(output, expected_output)\n      self.assertEqual(output.dtype, expected_output.dtype)\n\n\nclass GroundtruthFilterTest(tf.test.TestCase):\n\n  def test_filter_groundtruth(self):\n    input_image = tf.placeholder(tf.float32, shape=(None, None, 3))\n    input_boxes = tf.placeholder(tf.float32, shape=(None, 4))\n    input_classes = tf.placeholder(tf.int32, shape=(None,))\n    input_is_crowd = tf.placeholder(tf.bool, shape=(None,))\n    input_area = tf.placeholder(tf.float32, shape=(None,))\n    input_difficult = tf.placeholder(tf.float32, shape=(None,))\n    input_label_types = tf.placeholder(tf.string, shape=(None,))\n    valid_indices = tf.placeholder(tf.int32, shape=(None,))\n    input_tensors = {\n        fields.InputDataFields.image: input_image,\n        fields.InputDataFields.groundtruth_boxes: input_boxes,\n        fields.InputDataFields.groundtruth_classes: input_classes,\n        fields.InputDataFields.groundtruth_is_crowd: input_is_crowd,\n        fields.InputDataFields.groundtruth_area: input_area,\n        fields.InputDataFields.groundtruth_difficult: input_difficult,\n        fields.InputDataFields.groundtruth_label_types: input_label_types\n    }\n    output_tensors = ops.retain_groundtruth(input_tensors, valid_indices)\n\n    image_tensor = np.random.rand(224, 224, 3)\n    feed_dict = {\n        input_image: image_tensor,\n        input_boxes:\n        np.array([[0.2, 0.4, 0.1, 0.8], [0.2, 0.4, 1.0, 0.8]], dtype=np.float),\n        input_classes:\n        np.array([1, 2], dtype=np.int32),\n        input_is_crowd:\n        np.array([False, True], dtype=np.bool),\n        input_area:\n        np.array([32, 48], dtype=np.float32),\n        input_difficult:\n        np.array([True, False], dtype=np.bool),\n        input_label_types:\n        np.array([\'APPROPRIATE\', \'INCORRECT\'], dtype=np.string_),\n        valid_indices:\n        np.array([0], dtype=np.int32)\n    }\n    expected_tensors = {\n        fields.InputDataFields.image:\n        image_tensor,\n        fields.InputDataFields.groundtruth_boxes:\n        [[0.2, 0.4, 0.1, 0.8]],\n        fields.InputDataFields.groundtruth_classes:\n        [1],\n        fields.InputDataFields.groundtruth_is_crowd:\n        [False],\n        fields.InputDataFields.groundtruth_area:\n        [32],\n        fields.InputDataFields.groundtruth_difficult:\n        [True],\n        fields.InputDataFields.groundtruth_label_types:\n        [\'APPROPRIATE\']\n    }\n    with self.test_session() as sess:\n      output_tensors = sess.run(output_tensors, feed_dict=feed_dict)\n      for key in [fields.InputDataFields.image,\n                  fields.InputDataFields.groundtruth_boxes,\n                  fields.InputDataFields.groundtruth_area]:\n        self.assertAllClose(expected_tensors[key], output_tensors[key])\n      for key in [fields.InputDataFields.groundtruth_classes,\n                  fields.InputDataFields.groundtruth_is_crowd,\n                  fields.InputDataFields.groundtruth_label_types]:\n        self.assertAllEqual(expected_tensors[key], output_tensors[key])\n\n  def test_filter_with_missing_fields(self):\n    input_boxes = tf.placeholder(tf.float32, shape=(None, 4))\n    input_classes = tf.placeholder(tf.int32, shape=(None,))\n    input_tensors = {\n        fields.InputDataFields.groundtruth_boxes: input_boxes,\n        fields.InputDataFields.groundtruth_classes: input_classes\n    }\n    valid_indices = tf.placeholder(tf.int32, shape=(None,))\n\n    feed_dict = {\n        input_boxes:\n        np.array([[0.2, 0.4, 0.1, 0.8], [0.2, 0.4, 1.0, 0.8]], dtype=np.float),\n        input_classes:\n        np.array([1, 2], dtype=np.int32),\n        valid_indices:\n        np.array([0], dtype=np.int32)\n    }\n    expected_tensors = {\n        fields.InputDataFields.groundtruth_boxes:\n        [[0.2, 0.4, 0.1, 0.8]],\n        fields.InputDataFields.groundtruth_classes:\n        [1]\n    }\n\n    output_tensors = ops.retain_groundtruth(input_tensors, valid_indices)\n    with self.test_session() as sess:\n      output_tensors = sess.run(output_tensors, feed_dict=feed_dict)\n      for key in [fields.InputDataFields.groundtruth_boxes]:\n        self.assertAllClose(expected_tensors[key], output_tensors[key])\n      for key in [fields.InputDataFields.groundtruth_classes]:\n        self.assertAllEqual(expected_tensors[key], output_tensors[key])\n\n  def test_filter_with_empty_fields(self):\n    input_boxes = tf.placeholder(tf.float32, shape=(None, 4))\n    input_classes = tf.placeholder(tf.int32, shape=(None,))\n    input_is_crowd = tf.placeholder(tf.bool, shape=(None,))\n    input_area = tf.placeholder(tf.float32, shape=(None,))\n    input_difficult = tf.placeholder(tf.float32, shape=(None,))\n    valid_indices = tf.placeholder(tf.int32, shape=(None,))\n    input_tensors = {\n        fields.InputDataFields.groundtruth_boxes: input_boxes,\n        fields.InputDataFields.groundtruth_classes: input_classes,\n        fields.InputDataFields.groundtruth_is_crowd: input_is_crowd,\n        fields.InputDataFields.groundtruth_area: input_area,\n        fields.InputDataFields.groundtruth_difficult: input_difficult\n    }\n    output_tensors = ops.retain_groundtruth(input_tensors, valid_indices)\n\n    feed_dict = {\n        input_boxes:\n        np.array([[0.2, 0.4, 0.1, 0.8], [0.2, 0.4, 1.0, 0.8]], dtype=np.float),\n        input_classes:\n        np.array([1, 2], dtype=np.int32),\n        input_is_crowd:\n        np.array([False, True], dtype=np.bool),\n        input_area:\n        np.array([], dtype=np.float32),\n        input_difficult:\n        np.array([], dtype=np.float32),\n        valid_indices:\n        np.array([0], dtype=np.int32)\n    }\n    expected_tensors = {\n        fields.InputDataFields.groundtruth_boxes:\n        [[0.2, 0.4, 0.1, 0.8]],\n        fields.InputDataFields.groundtruth_classes:\n        [1],\n        fields.InputDataFields.groundtruth_is_crowd:\n        [False],\n        fields.InputDataFields.groundtruth_area:\n        [],\n        fields.InputDataFields.groundtruth_difficult:\n        []\n    }\n    with self.test_session() as sess:\n      output_tensors = sess.run(output_tensors, feed_dict=feed_dict)\n      for key in [fields.InputDataFields.groundtruth_boxes,\n                  fields.InputDataFields.groundtruth_area]:\n        self.assertAllClose(expected_tensors[key], output_tensors[key])\n      for key in [fields.InputDataFields.groundtruth_classes,\n                  fields.InputDataFields.groundtruth_is_crowd]:\n        self.assertAllEqual(expected_tensors[key], output_tensors[key])\n\n  def test_filter_with_empty_groundtruth_boxes(self):\n    input_boxes = tf.placeholder(tf.float32, shape=(None, 4))\n    input_classes = tf.placeholder(tf.int32, shape=(None,))\n    input_is_crowd = tf.placeholder(tf.bool, shape=(None,))\n    input_area = tf.placeholder(tf.float32, shape=(None,))\n    input_difficult = tf.placeholder(tf.float32, shape=(None,))\n    valid_indices = tf.placeholder(tf.int32, shape=(None,))\n    input_tensors = {\n        fields.InputDataFields.groundtruth_boxes: input_boxes,\n        fields.InputDataFields.groundtruth_classes: input_classes,\n        fields.InputDataFields.groundtruth_is_crowd: input_is_crowd,\n        fields.InputDataFields.groundtruth_area: input_area,\n        fields.InputDataFields.groundtruth_difficult: input_difficult\n    }\n    output_tensors = ops.retain_groundtruth(input_tensors, valid_indices)\n\n    feed_dict = {\n        input_boxes:\n        np.array([], dtype=np.float).reshape(0, 4),\n        input_classes:\n        np.array([], dtype=np.int32),\n        input_is_crowd:\n        np.array([], dtype=np.bool),\n        input_area:\n        np.array([], dtype=np.float32),\n        input_difficult:\n        np.array([], dtype=np.float32),\n        valid_indices:\n        np.array([], dtype=np.int32)\n    }\n    with self.test_session() as sess:\n      output_tensors = sess.run(output_tensors, feed_dict=feed_dict)\n      for key in input_tensors:\n        if key == fields.InputDataFields.groundtruth_boxes:\n          self.assertAllEqual([0, 4], output_tensors[key].shape)\n        else:\n          self.assertAllEqual([0], output_tensors[key].shape)\n\n\nclass RetainGroundTruthWithPositiveClasses(tf.test.TestCase):\n\n  def test_filter_groundtruth_with_positive_classes(self):\n    input_image = tf.placeholder(tf.float32, shape=(None, None, 3))\n    input_boxes = tf.placeholder(tf.float32, shape=(None, 4))\n    input_classes = tf.placeholder(tf.int32, shape=(None,))\n    input_is_crowd = tf.placeholder(tf.bool, shape=(None,))\n    input_area = tf.placeholder(tf.float32, shape=(None,))\n    input_difficult = tf.placeholder(tf.float32, shape=(None,))\n    input_label_types = tf.placeholder(tf.string, shape=(None,))\n    valid_indices = tf.placeholder(tf.int32, shape=(None,))\n    input_tensors = {\n        fields.InputDataFields.image: input_image,\n        fields.InputDataFields.groundtruth_boxes: input_boxes,\n        fields.InputDataFields.groundtruth_classes: input_classes,\n        fields.InputDataFields.groundtruth_is_crowd: input_is_crowd,\n        fields.InputDataFields.groundtruth_area: input_area,\n        fields.InputDataFields.groundtruth_difficult: input_difficult,\n        fields.InputDataFields.groundtruth_label_types: input_label_types\n    }\n    output_tensors = ops.retain_groundtruth_with_positive_classes(input_tensors)\n\n    image_tensor = np.random.rand(224, 224, 3)\n    feed_dict = {\n        input_image: image_tensor,\n        input_boxes:\n        np.array([[0.2, 0.4, 0.1, 0.8], [0.2, 0.4, 1.0, 0.8]], dtype=np.float),\n        input_classes:\n        np.array([1, 0], dtype=np.int32),\n        input_is_crowd:\n        np.array([False, True], dtype=np.bool),\n        input_area:\n        np.array([32, 48], dtype=np.float32),\n        input_difficult:\n        np.array([True, False], dtype=np.bool),\n        input_label_types:\n        np.array([\'APPROPRIATE\', \'INCORRECT\'], dtype=np.string_),\n        valid_indices:\n        np.array([0], dtype=np.int32)\n    }\n    expected_tensors = {\n        fields.InputDataFields.image:\n        image_tensor,\n        fields.InputDataFields.groundtruth_boxes:\n        [[0.2, 0.4, 0.1, 0.8]],\n        fields.InputDataFields.groundtruth_classes:\n        [1],\n        fields.InputDataFields.groundtruth_is_crowd:\n        [False],\n        fields.InputDataFields.groundtruth_area:\n        [32],\n        fields.InputDataFields.groundtruth_difficult:\n        [True],\n        fields.InputDataFields.groundtruth_label_types:\n        [\'APPROPRIATE\']\n    }\n    with self.test_session() as sess:\n      output_tensors = sess.run(output_tensors, feed_dict=feed_dict)\n      for key in [fields.InputDataFields.image,\n                  fields.InputDataFields.groundtruth_boxes,\n                  fields.InputDataFields.groundtruth_area]:\n        self.assertAllClose(expected_tensors[key], output_tensors[key])\n      for key in [fields.InputDataFields.groundtruth_classes,\n                  fields.InputDataFields.groundtruth_is_crowd,\n                  fields.InputDataFields.groundtruth_label_types]:\n        self.assertAllEqual(expected_tensors[key], output_tensors[key])\n\n\nclass ReplaceNaNGroundtruthLabelScoresWithOnes(tf.test.TestCase):\n\n  def test_replace_nan_groundtruth_label_scores_with_ones(self):\n    label_scores = tf.constant([np.nan, 1.0, np.nan])\n    output_tensor = ops.replace_nan_groundtruth_label_scores_with_ones(\n        label_scores)\n    expected_tensor = [1.0, 1.0, 1.0]\n    with self.test_session():\n      output_tensor = output_tensor.eval()\n      self.assertAllClose(expected_tensor, output_tensor)\n\n  def test_input_equals_output_when_no_nans(self):\n    input_label_scores = [0.5, 1.0, 1.0]\n    label_scores_tensor = tf.constant(input_label_scores)\n    output_label_scores = ops.replace_nan_groundtruth_label_scores_with_ones(\n        label_scores_tensor)\n    with self.test_session():\n      output_label_scores = output_label_scores.eval()\n      self.assertAllClose(input_label_scores, output_label_scores)\n\n\nclass GroundtruthFilterWithCrowdBoxesTest(tf.test.TestCase):\n\n  def test_filter_groundtruth_with_crowd_boxes(self):\n    input_tensors = {\n        fields.InputDataFields.groundtruth_boxes:\n        [[0.1, 0.2, 0.6, 0.8], [0.2, 0.4, 0.1, 0.8]],\n        fields.InputDataFields.groundtruth_classes:\n        [1, 2],\n        fields.InputDataFields.groundtruth_is_crowd:\n        [True, False],\n        fields.InputDataFields.groundtruth_area:\n        [100.0, 238.7]\n    }\n\n    expected_tensors = {\n        fields.InputDataFields.groundtruth_boxes:\n        [[0.2, 0.4, 0.1, 0.8]],\n        fields.InputDataFields.groundtruth_classes:\n        [2],\n        fields.InputDataFields.groundtruth_is_crowd:\n        [False],\n        fields.InputDataFields.groundtruth_area:\n        [238.7]\n    }\n\n    output_tensors = ops.filter_groundtruth_with_crowd_boxes(\n        input_tensors)\n    with self.test_session() as sess:\n      output_tensors = sess.run(output_tensors)\n      for key in [fields.InputDataFields.groundtruth_boxes,\n                  fields.InputDataFields.groundtruth_area]:\n        self.assertAllClose(expected_tensors[key], output_tensors[key])\n      for key in [fields.InputDataFields.groundtruth_classes,\n                  fields.InputDataFields.groundtruth_is_crowd]:\n        self.assertAllEqual(expected_tensors[key], output_tensors[key])\n\n\nclass GroundtruthFilterWithNanBoxTest(tf.test.TestCase):\n\n  def test_filter_groundtruth_with_nan_box_coordinates(self):\n    input_tensors = {\n        fields.InputDataFields.groundtruth_boxes:\n        [[np.nan, np.nan, np.nan, np.nan], [0.2, 0.4, 0.1, 0.8]],\n        fields.InputDataFields.groundtruth_classes:\n        [1, 2],\n        fields.InputDataFields.groundtruth_is_crowd:\n        [False, True],\n        fields.InputDataFields.groundtruth_area:\n        [100.0, 238.7]\n    }\n\n    expected_tensors = {\n        fields.InputDataFields.groundtruth_boxes:\n        [[0.2, 0.4, 0.1, 0.8]],\n        fields.InputDataFields.groundtruth_classes:\n        [2],\n        fields.InputDataFields.groundtruth_is_crowd:\n        [True],\n        fields.InputDataFields.groundtruth_area:\n        [238.7]\n    }\n\n    output_tensors = ops.filter_groundtruth_with_nan_box_coordinates(\n        input_tensors)\n    with self.test_session() as sess:\n      output_tensors = sess.run(output_tensors)\n      for key in [fields.InputDataFields.groundtruth_boxes,\n                  fields.InputDataFields.groundtruth_area]:\n        self.assertAllClose(expected_tensors[key], output_tensors[key])\n      for key in [fields.InputDataFields.groundtruth_classes,\n                  fields.InputDataFields.groundtruth_is_crowd]:\n        self.assertAllEqual(expected_tensors[key], output_tensors[key])\n\n\nclass OpsTestNormalizeToTarget(tf.test.TestCase):\n\n  def test_create_normalize_to_target(self):\n    inputs = tf.random_uniform([5, 10, 12, 3])\n    target_norm_value = 4.0\n    dim = 3\n    with self.test_session():\n      output = ops.normalize_to_target(inputs, target_norm_value, dim)\n      self.assertEqual(output.op.name, \'NormalizeToTarget/mul\')\n      var_name = tf.contrib.framework.get_variables()[0].name\n      self.assertEqual(var_name, \'NormalizeToTarget/weights:0\')\n\n  def test_invalid_dim(self):\n    inputs = tf.random_uniform([5, 10, 12, 3])\n    target_norm_value = 4.0\n    dim = 10\n    with self.assertRaisesRegexp(\n        ValueError,\n        \'dim must be non-negative but smaller than the input rank.\'):\n      ops.normalize_to_target(inputs, target_norm_value, dim)\n\n  def test_invalid_target_norm_values(self):\n    inputs = tf.random_uniform([5, 10, 12, 3])\n    target_norm_value = [4.0, 4.0]\n    dim = 3\n    with self.assertRaisesRegexp(\n        ValueError, \'target_norm_value must be a float or a list of floats\'):\n      ops.normalize_to_target(inputs, target_norm_value, dim)\n\n  def test_correct_output_shape(self):\n    inputs = tf.random_uniform([5, 10, 12, 3])\n    target_norm_value = 4.0\n    dim = 3\n    with self.test_session():\n      output = ops.normalize_to_target(inputs, target_norm_value, dim)\n      self.assertEqual(output.get_shape().as_list(),\n                       inputs.get_shape().as_list())\n\n  def test_correct_initial_output_values(self):\n    inputs = tf.constant([[[[3, 4], [7, 24]],\n                           [[5, -12], [-1, 0]]]], tf.float32)\n    target_norm_value = 10.0\n    dim = 3\n    expected_output = [[[[30/5.0, 40/5.0], [70/25.0, 240/25.0]],\n                        [[50/13.0, -120/13.0], [-10, 0]]]]\n    with self.test_session() as sess:\n      normalized_inputs = ops.normalize_to_target(inputs, target_norm_value,\n                                                  dim)\n      sess.run(tf.global_variables_initializer())\n      output = normalized_inputs.eval()\n      self.assertAllClose(output, expected_output)\n\n  def test_multiple_target_norm_values(self):\n    inputs = tf.constant([[[[3, 4], [7, 24]],\n                           [[5, -12], [-1, 0]]]], tf.float32)\n    target_norm_value = [10.0, 20.0]\n    dim = 3\n    expected_output = [[[[30/5.0, 80/5.0], [70/25.0, 480/25.0]],\n                        [[50/13.0, -240/13.0], [-10, 0]]]]\n    with self.test_session() as sess:\n      normalized_inputs = ops.normalize_to_target(inputs, target_norm_value,\n                                                  dim)\n      sess.run(tf.global_variables_initializer())\n      output = normalized_inputs.eval()\n      self.assertAllClose(output, expected_output)\n\n\nclass OpsTestPositionSensitiveCropRegions(tf.test.TestCase):\n\n  def test_position_sensitive(self):\n    num_spatial_bins = [3, 2]\n    image_shape = [1, 3, 2, 6]\n\n    # First channel is 1\'s, second channel is 2\'s, etc.\n    image = tf.constant(range(1, 3 * 2 + 1) * 6, dtype=tf.float32,\n                        shape=image_shape)\n    boxes = tf.random_uniform((2, 4))\n    box_ind = tf.constant([0, 0], dtype=tf.int32)\n\n    # The result for both boxes should be [[1, 2], [3, 4], [5, 6]]\n    # before averaging.\n    expected_output = np.array([3.5, 3.5]).reshape([2, 1, 1, 1])\n\n    for crop_size_mult in range(1, 3):\n      crop_size = [3 * crop_size_mult, 2 * crop_size_mult]\n      ps_crop_and_pool = ops.position_sensitive_crop_regions(\n          image, boxes, box_ind, crop_size, num_spatial_bins, global_pool=True)\n\n      with self.test_session() as sess:\n        output = sess.run(ps_crop_and_pool)\n        self.assertAllClose(output, expected_output)\n\n  def test_position_sensitive_with_equal_channels(self):\n    num_spatial_bins = [2, 2]\n    image_shape = [1, 3, 3, 4]\n    crop_size = [2, 2]\n\n    image = tf.constant(range(1, 3 * 3 + 1), dtype=tf.float32,\n                        shape=[1, 3, 3, 1])\n    tiled_image = tf.tile(image, [1, 1, 1, image_shape[3]])\n    boxes = tf.random_uniform((3, 4))\n    box_ind = tf.constant([0, 0, 0], dtype=tf.int32)\n\n    # All channels are equal so position-sensitive crop and resize should\n    # work as the usual crop and resize for just one channel.\n    crop = tf.image.crop_and_resize(image, boxes, box_ind, crop_size)\n    crop_and_pool = tf.reduce_mean(crop, [1, 2], keep_dims=True)\n\n    ps_crop_and_pool = ops.position_sensitive_crop_regions(\n        tiled_image,\n        boxes,\n        box_ind,\n        crop_size,\n        num_spatial_bins,\n        global_pool=True)\n\n    with self.test_session() as sess:\n      expected_output, output = sess.run((crop_and_pool, ps_crop_and_pool))\n      self.assertAllClose(output, expected_output)\n\n  def test_position_sensitive_with_single_bin(self):\n    num_spatial_bins = [1, 1]\n    image_shape = [2, 3, 3, 4]\n    crop_size = [2, 2]\n\n    image = tf.random_uniform(image_shape)\n    boxes = tf.random_uniform((6, 4))\n    box_ind = tf.constant([0, 0, 0, 1, 1, 1], dtype=tf.int32)\n\n    # When a single bin is used, position-sensitive crop and pool should be\n    # the same as non-position sensitive crop and pool.\n    crop = tf.image.crop_and_resize(image, boxes, box_ind, crop_size)\n    crop_and_pool = tf.reduce_mean(crop, [1, 2], keep_dims=True)\n\n    ps_crop_and_pool = ops.position_sensitive_crop_regions(\n        image, boxes, box_ind, crop_size, num_spatial_bins, global_pool=True)\n\n    with self.test_session() as sess:\n      expected_output, output = sess.run((crop_and_pool, ps_crop_and_pool))\n      self.assertAllClose(output, expected_output)\n\n  def test_raise_value_error_on_num_bins_less_than_one(self):\n    num_spatial_bins = [1, -1]\n    image_shape = [1, 1, 1, 2]\n    crop_size = [2, 2]\n\n    image = tf.constant(1, dtype=tf.float32, shape=image_shape)\n    boxes = tf.constant([[0, 0, 1, 1]], dtype=tf.float32)\n    box_ind = tf.constant([0], dtype=tf.int32)\n\n    with self.assertRaisesRegexp(ValueError, \'num_spatial_bins should be >= 1\'):\n      ops.position_sensitive_crop_regions(\n          image, boxes, box_ind, crop_size, num_spatial_bins, global_pool=True)\n\n  def test_raise_value_error_on_non_divisible_crop_size(self):\n    num_spatial_bins = [2, 3]\n    image_shape = [1, 1, 1, 6]\n    crop_size = [3, 2]\n\n    image = tf.constant(1, dtype=tf.float32, shape=image_shape)\n    boxes = tf.constant([[0, 0, 1, 1]], dtype=tf.float32)\n    box_ind = tf.constant([0], dtype=tf.int32)\n\n    with self.assertRaisesRegexp(\n        ValueError, \'crop_size should be divisible by num_spatial_bins\'):\n      ops.position_sensitive_crop_regions(\n          image, boxes, box_ind, crop_size, num_spatial_bins, global_pool=True)\n\n  def test_raise_value_error_on_non_divisible_num_channels(self):\n    num_spatial_bins = [2, 2]\n    image_shape = [1, 1, 1, 5]\n    crop_size = [2, 2]\n\n    image = tf.constant(1, dtype=tf.float32, shape=image_shape)\n    boxes = tf.constant([[0, 0, 1, 1]], dtype=tf.float32)\n    box_ind = tf.constant([0], dtype=tf.int32)\n\n    with self.assertRaisesRegexp(\n        ValueError, \'Dimension size must be evenly divisible by 4 but is 5\'):\n      ops.position_sensitive_crop_regions(\n          image, boxes, box_ind, crop_size, num_spatial_bins, global_pool=True)\n\n  def test_position_sensitive_with_global_pool_false(self):\n    num_spatial_bins = [3, 2]\n    image_shape = [1, 3, 2, 6]\n    num_boxes = 2\n\n    # First channel is 1\'s, second channel is 2\'s, etc.\n    image = tf.constant(range(1, 3 * 2 + 1) * 6, dtype=tf.float32,\n                        shape=image_shape)\n    boxes = tf.random_uniform((num_boxes, 4))\n    box_ind = tf.constant([0, 0], dtype=tf.int32)\n\n    expected_output = []\n\n    # Expected output, when crop_size = [3, 2].\n    expected_output.append(np.expand_dims(\n        np.tile(np.array([[1, 2],\n                          [3, 4],\n                          [5, 6]]), (num_boxes, 1, 1)),\n        axis=-1))\n\n    # Expected output, when crop_size = [6, 4].\n    expected_output.append(np.expand_dims(\n        np.tile(np.array([[1, 1, 2, 2],\n                          [1, 1, 2, 2],\n                          [3, 3, 4, 4],\n                          [3, 3, 4, 4],\n                          [5, 5, 6, 6],\n                          [5, 5, 6, 6]]), (num_boxes, 1, 1)),\n        axis=-1))\n\n    for crop_size_mult in range(1, 3):\n      crop_size = [3 * crop_size_mult, 2 * crop_size_mult]\n      ps_crop = ops.position_sensitive_crop_regions(\n          image, boxes, box_ind, crop_size, num_spatial_bins, global_pool=False)\n      with self.test_session() as sess:\n        output = sess.run(ps_crop)\n\n      self.assertAllEqual(output, expected_output[crop_size_mult - 1])\n\n  def test_position_sensitive_with_global_pool_false_and_known_boxes(self):\n    num_spatial_bins = [2, 2]\n    image_shape = [2, 2, 2, 4]\n    crop_size = [2, 2]\n\n    image = tf.constant(range(1, 2 * 2 * 4  + 1) * 2, dtype=tf.float32,\n                        shape=image_shape)\n\n    # First box contains whole image, and second box contains only first row.\n    boxes = tf.constant(np.array([[0., 0., 1., 1.],\n                                  [0., 0., 0.5, 1.]]), dtype=tf.float32)\n    box_ind = tf.constant([0, 1], dtype=tf.int32)\n\n    expected_output = []\n\n    # Expected output, when the box containing whole image.\n    expected_output.append(\n        np.reshape(np.array([[4, 7],\n                             [10, 13]]),\n                   (1, 2, 2, 1))\n    )\n\n    # Expected output, when the box containing only first row.\n    expected_output.append(\n        np.reshape(np.array([[3, 6],\n                             [7, 10]]),\n                   (1, 2, 2, 1))\n    )\n    expected_output = np.concatenate(expected_output, axis=0)\n\n    ps_crop = ops.position_sensitive_crop_regions(\n        image, boxes, box_ind, crop_size, num_spatial_bins, global_pool=False)\n\n    with self.test_session() as sess:\n      output = sess.run(ps_crop)\n      self.assertAllEqual(output, expected_output)\n\n  def test_position_sensitive_with_global_pool_false_and_single_bin(self):\n    num_spatial_bins = [1, 1]\n    image_shape = [2, 3, 3, 4]\n    crop_size = [1, 1]\n\n    image = tf.random_uniform(image_shape)\n    boxes = tf.random_uniform((6, 4))\n    box_ind = tf.constant([0, 0, 0, 1, 1, 1], dtype=tf.int32)\n\n    # Since single_bin is used and crop_size = [1, 1] (i.e., no crop resize),\n    # the outputs are the same whatever the global_pool value is.\n    ps_crop_and_pool = ops.position_sensitive_crop_regions(\n        image, boxes, box_ind, crop_size, num_spatial_bins, global_pool=True)\n    ps_crop = ops.position_sensitive_crop_regions(\n        image, boxes, box_ind, crop_size, num_spatial_bins, global_pool=False)\n\n    with self.test_session() as sess:\n      pooled_output, unpooled_output = sess.run((ps_crop_and_pool, ps_crop))\n      self.assertAllClose(pooled_output, unpooled_output)\n\n  def test_position_sensitive_with_global_pool_false_and_do_global_pool(self):\n    num_spatial_bins = [3, 2]\n    image_shape = [1, 3, 2, 6]\n    num_boxes = 2\n\n    # First channel is 1\'s, second channel is 2\'s, etc.\n    image = tf.constant(range(1, 3 * 2 + 1) * 6, dtype=tf.float32,\n                        shape=image_shape)\n    boxes = tf.random_uniform((num_boxes, 4))\n    box_ind = tf.constant([0, 0], dtype=tf.int32)\n\n    expected_output = []\n\n    # Expected output, when crop_size = [3, 2].\n    expected_output.append(np.mean(\n        np.expand_dims(\n            np.tile(np.array([[1, 2],\n                              [3, 4],\n                              [5, 6]]), (num_boxes, 1, 1)),\n            axis=-1),\n        axis=(1, 2), keepdims=True))\n\n    # Expected output, when crop_size = [6, 4].\n    expected_output.append(np.mean(\n        np.expand_dims(\n            np.tile(np.array([[1, 1, 2, 2],\n                              [1, 1, 2, 2],\n                              [3, 3, 4, 4],\n                              [3, 3, 4, 4],\n                              [5, 5, 6, 6],\n                              [5, 5, 6, 6]]), (num_boxes, 1, 1)),\n            axis=-1),\n        axis=(1, 2), keepdims=True))\n\n    for crop_size_mult in range(1, 3):\n      crop_size = [3 * crop_size_mult, 2 * crop_size_mult]\n\n      # Perform global_pooling after running the function with\n      # global_pool=False.\n      ps_crop = ops.position_sensitive_crop_regions(\n          image, boxes, box_ind, crop_size, num_spatial_bins, global_pool=False)\n      ps_crop_and_pool = tf.reduce_mean(\n          ps_crop, reduction_indices=(1, 2), keep_dims=True)\n\n      with self.test_session() as sess:\n        output = sess.run(ps_crop_and_pool)\n\n      self.assertAllEqual(output, expected_output[crop_size_mult - 1])\n\n  def test_raise_value_error_on_non_square_block_size(self):\n    num_spatial_bins = [3, 2]\n    image_shape = [1, 3, 2, 6]\n    crop_size = [6, 2]\n\n    image = tf.constant(1, dtype=tf.float32, shape=image_shape)\n    boxes = tf.constant([[0, 0, 1, 1]], dtype=tf.float32)\n    box_ind = tf.constant([0], dtype=tf.int32)\n\n    with self.assertRaisesRegexp(\n        ValueError, \'Only support square bin crop size for now.\'):\n      ops.position_sensitive_crop_regions(\n          image, boxes, box_ind, crop_size, num_spatial_bins, global_pool=False)\n\n\nclass ReframeBoxMasksToImageMasksTest(tf.test.TestCase):\n\n  def testZeroImageOnEmptyMask(self):\n    box_masks = tf.constant([[[0, 0],\n                              [0, 0]]], dtype=tf.float32)\n    boxes = tf.constant([[0.0, 0.0, 1.0, 1.0]], dtype=tf.float32)\n    image_masks = ops.reframe_box_masks_to_image_masks(box_masks, boxes,\n                                                       image_height=4,\n                                                       image_width=4)\n    np_expected_image_masks = np.array([[[0, 0, 0, 0],\n                                         [0, 0, 0, 0],\n                                         [0, 0, 0, 0],\n                                         [0, 0, 0, 0]]], dtype=np.float32)\n    with self.test_session() as sess:\n      np_image_masks = sess.run(image_masks)\n      self.assertAllClose(np_image_masks, np_expected_image_masks)\n\n  def testMaskIsCenteredInImageWhenBoxIsCentered(self):\n    box_masks = tf.constant([[[1, 1],\n                              [1, 1]]], dtype=tf.float32)\n    boxes = tf.constant([[0.25, 0.25, 0.75, 0.75]], dtype=tf.float32)\n    image_masks = ops.reframe_box_masks_to_image_masks(box_masks, boxes,\n                                                       image_height=4,\n                                                       image_width=4)\n    np_expected_image_masks = np.array([[[0, 0, 0, 0],\n                                         [0, 1, 1, 0],\n                                         [0, 1, 1, 0],\n                                         [0, 0, 0, 0]]], dtype=np.float32)\n    with self.test_session() as sess:\n      np_image_masks = sess.run(image_masks)\n      self.assertAllClose(np_image_masks, np_expected_image_masks)\n\n  def testMaskOffCenterRemainsOffCenterInImage(self):\n    box_masks = tf.constant([[[1, 0],\n                              [0, 1]]], dtype=tf.float32)\n    boxes = tf.constant([[0.25, 0.5, 0.75, 1.0]], dtype=tf.float32)\n    image_masks = ops.reframe_box_masks_to_image_masks(box_masks, boxes,\n                                                       image_height=4,\n                                                       image_width=4)\n    np_expected_image_masks = np.array([[[0, 0, 0, 0],\n                                         [0, 0, 0.6111111, 0.16666669],\n                                         [0, 0, 0.3888889, 0.83333337],\n                                         [0, 0, 0, 0]]], dtype=np.float32)\n    with self.test_session() as sess:\n      np_image_masks = sess.run(image_masks)\n      self.assertAllClose(np_image_masks, np_expected_image_masks)\n\n\nclass MergeBoxesWithMultipleLabelsTest(tf.test.TestCase):\n\n  def testMergeBoxesWithMultipleLabels(self):\n    boxes = tf.constant(\n        [[0.25, 0.25, 0.75, 0.75], [0.0, 0.0, 0.5, 0.75],\n         [0.25, 0.25, 0.75, 0.75]],\n        dtype=tf.float32)\n    class_indices = tf.constant([0, 4, 2], dtype=tf.int32)\n    num_classes = 5\n    merged_boxes, merged_classes, merged_box_indices = (\n        ops.merge_boxes_with_multiple_labels(boxes, class_indices, num_classes))\n    expected_merged_boxes = np.array(\n        [[0.25, 0.25, 0.75, 0.75], [0.0, 0.0, 0.5, 0.75]], dtype=np.float32)\n    expected_merged_classes = np.array(\n        [[1, 0, 1, 0, 0], [0, 0, 0, 0, 1]], dtype=np.int32)\n    expected_merged_box_indices = np.array([0, 1], dtype=np.int32)\n    with self.test_session() as sess:\n      np_merged_boxes, np_merged_classes, np_merged_box_indices = sess.run(\n          [merged_boxes, merged_classes, merged_box_indices])\n      if np_merged_classes[0, 0] != 1:\n        expected_merged_boxes = expected_merged_boxes[::-1, :]\n        expected_merged_classes = expected_merged_classes[::-1, :]\n        expected_merged_box_indices = expected_merged_box_indices[::-1, :]\n      self.assertAllClose(np_merged_boxes, expected_merged_boxes)\n      self.assertAllClose(np_merged_classes, expected_merged_classes)\n      self.assertAllClose(np_merged_box_indices, expected_merged_box_indices)\n\n  def testMergeBoxesWithEmptyInputs(self):\n    boxes = tf.constant([[]])\n    class_indices = tf.constant([])\n    num_classes = 5\n    merged_boxes, merged_classes, merged_box_indices = (\n        ops.merge_boxes_with_multiple_labels(boxes, class_indices, num_classes))\n    with self.test_session() as sess:\n      np_merged_boxes, np_merged_classes, np_merged_box_indices = sess.run(\n          [merged_boxes, merged_classes, merged_box_indices])\n      self.assertAllEqual(np_merged_boxes.shape, [0, 4])\n      self.assertAllEqual(np_merged_classes.shape, [0, 5])\n      self.assertAllEqual(np_merged_box_indices.shape, [0])\n\n\nclass NearestNeighborUpsamplingTest(test_case.TestCase):\n\n  def test_upsampling(self):\n\n    def graph_fn(inputs):\n      custom_op_output = ops.nearest_neighbor_upsampling(inputs, scale=2)\n      return custom_op_output\n    inputs = np.reshape(np.arange(4).astype(np.float32), [1, 2, 2, 1])\n    custom_op_output = self.execute(graph_fn, [inputs])\n\n    expected_output = [[[[0], [0], [1], [1]],\n                        [[0], [0], [1], [1]],\n                        [[2], [2], [3], [3]],\n                        [[2], [2], [3], [3]]]]\n    self.assertAllClose(custom_op_output, expected_output)\n\n\nclass MatmulGatherOnZerothAxis(test_case.TestCase):\n\n  def test_gather_2d(self):\n\n    def graph_fn(params, indices):\n      return ops.matmul_gather_on_zeroth_axis(params, indices)\n\n    params = np.array([[1, 2, 3, 4],\n                       [5, 6, 7, 8],\n                       [9, 10, 11, 12],\n                       [0, 1, 0, 0]], dtype=np.float32)\n    indices = np.array([2, 2, 1], dtype=np.int32)\n    expected_output = np.array([[9, 10, 11, 12], [9, 10, 11, 12], [5, 6, 7, 8]])\n    gather_output = self.execute(graph_fn, [params, indices])\n    self.assertAllClose(gather_output, expected_output)\n\n  def test_gather_3d(self):\n\n    def graph_fn(params, indices):\n      return ops.matmul_gather_on_zeroth_axis(params, indices)\n\n    params = np.array([[[1, 2], [3, 4]],\n                       [[5, 6], [7, 8]],\n                       [[9, 10], [11, 12]],\n                       [[0, 1], [0, 0]]], dtype=np.float32)\n    indices = np.array([0, 3, 1], dtype=np.int32)\n    expected_output = np.array([[[1, 2], [3, 4]],\n                                [[0, 1], [0, 0]],\n                                [[5, 6], [7, 8]]])\n    gather_output = self.execute(graph_fn, [params, indices])\n    self.assertAllClose(gather_output, expected_output)\n\n  def test_gather_with_many_indices(self):\n\n    def graph_fn(params, indices):\n      return ops.matmul_gather_on_zeroth_axis(params, indices)\n\n    params = np.array([[1, 2, 3, 4],\n                       [5, 6, 7, 8],\n                       [9, 10, 11, 12],\n                       [0, 1, 0, 0]], dtype=np.float32)\n    indices = np.array([0, 0, 0, 0, 0, 0], dtype=np.int32)\n    expected_output = np.array(6*[[1, 2, 3, 4]])\n    gather_output = self.execute(graph_fn, [params, indices])\n    self.assertAllClose(gather_output, expected_output)\n\n  def test_gather_with_dynamic_shape_input(self):\n    params_placeholder = tf.placeholder(tf.float32, shape=[None, 4])\n    indices_placeholder = tf.placeholder(tf.int32, shape=[None])\n    gather_result = ops.matmul_gather_on_zeroth_axis(\n        params_placeholder, indices_placeholder)\n    params = np.array([[1, 2, 3, 4],\n                       [5, 6, 7, 8],\n                       [9, 10, 11, 12],\n                       [0, 1, 0, 0]], dtype=np.float32)\n    indices = np.array([0, 0, 0, 0, 0, 0])\n    expected_output = np.array(6*[[1, 2, 3, 4]])\n    with self.test_session() as sess:\n      gather_output = sess.run(gather_result, feed_dict={\n          params_placeholder: params, indices_placeholder: indices})\n      self.assertAllClose(gather_output, expected_output)\n\n\nclass OpsTestMatMulCropAndResize(test_case.TestCase):\n\n  def testMatMulCropAndResize2x2To1x1(self):\n\n    def graph_fn(image, boxes):\n      return ops.matmul_crop_and_resize(image, boxes, crop_size=[1, 1])\n\n    image = np.array([[[[1], [2]], [[3], [4]]]], dtype=np.float32)\n    boxes = np.array([[0, 0, 1, 1]], dtype=np.float32)\n    expected_output = [[[[2.5]]]]\n    crop_output = self.execute(graph_fn, [image, boxes])\n    self.assertAllClose(crop_output, expected_output)\n\n  def testMatMulCropAndResize2x2To1x1Flipped(self):\n\n    def graph_fn(image, boxes):\n      return ops.matmul_crop_and_resize(image, boxes, crop_size=[1, 1])\n\n    image = np.array([[[[1], [2]], [[3], [4]]]], dtype=np.float32)\n    boxes = np.array([[1, 1, 0, 0]], dtype=np.float32)\n    expected_output = [[[[2.5]]]]\n    crop_output = self.execute(graph_fn, [image, boxes])\n    self.assertAllClose(crop_output, expected_output)\n\n  def testMatMulCropAndResize2x2To3x3(self):\n\n    def graph_fn(image, boxes):\n      return ops.matmul_crop_and_resize(image, boxes, crop_size=[3, 3])\n\n    image = np.array([[[[1], [2]], [[3], [4]]]], dtype=np.float32)\n    boxes = np.array([[0, 0, 1, 1]], dtype=np.float32)\n    expected_output = [[[[1.0], [1.5], [2.0]],\n                        [[2.0], [2.5], [3.0]],\n                        [[3.0], [3.5], [4.0]]]]\n    crop_output = self.execute(graph_fn, [image, boxes])\n    self.assertAllClose(crop_output, expected_output)\n\n  def testMatMulCropAndResize2x2To3x3Flipped(self):\n\n    def graph_fn(image, boxes):\n      return ops.matmul_crop_and_resize(image, boxes, crop_size=[3, 3])\n\n    image = np.array([[[[1], [2]], [[3], [4]]]], dtype=np.float32)\n    boxes = np.array([[1, 1, 0, 0]], dtype=np.float32)\n    expected_output = [[[[4.0], [3.5], [3.0]],\n                        [[3.0], [2.5], [2.0]],\n                        [[2.0], [1.5], [1.0]]]]\n    crop_output = self.execute(graph_fn, [image, boxes])\n    self.assertAllClose(crop_output, expected_output)\n\n  def testMatMulCropAndResize3x3To2x2(self):\n\n    def graph_fn(image, boxes):\n      return ops.matmul_crop_and_resize(image, boxes, crop_size=[2, 2])\n\n    image = np.array([[[[1], [2], [3]],\n                       [[4], [5], [6]],\n                       [[7], [8], [9]]]], dtype=np.float32)\n    boxes = np.array([[0, 0, 1, 1],\n                      [0, 0, .5, .5]], dtype=np.float32)\n    expected_output = [[[[1], [3]], [[7], [9]]],\n                       [[[1], [2]], [[4], [5]]]]\n    crop_output = self.execute(graph_fn, [image, boxes])\n    self.assertAllClose(crop_output, expected_output)\n\n  def testMatMulCropAndResize3x3To2x2MultiChannel(self):\n\n    def graph_fn(image, boxes):\n      return ops.matmul_crop_and_resize(image, boxes, crop_size=[2, 2])\n\n    image = np.array([[[[1, 0], [2, 1], [3, 2]],\n                       [[4, 3], [5, 4], [6, 5]],\n                       [[7, 6], [8, 7], [9, 8]]]], dtype=np.float32)\n    boxes = np.array([[0, 0, 1, 1],\n                      [0, 0, .5, .5]], dtype=np.float32)\n    expected_output = [[[[1, 0], [3, 2]], [[7, 6], [9, 8]]],\n                       [[[1, 0], [2, 1]], [[4, 3], [5, 4]]]]\n    crop_output = self.execute(graph_fn, [image, boxes])\n    self.assertAllClose(crop_output, expected_output)\n\n  def testMatMulCropAndResize3x3To2x2Flipped(self):\n\n    def graph_fn(image, boxes):\n      return ops.matmul_crop_and_resize(image, boxes, crop_size=[2, 2])\n\n    image = np.array([[[[1], [2], [3]],\n                       [[4], [5], [6]],\n                       [[7], [8], [9]]]], dtype=np.float32)\n    boxes = np.array([[1, 1, 0, 0],\n                      [.5, .5, 0, 0]], dtype=np.float32)\n    expected_output = [[[[9], [7]], [[3], [1]]],\n                       [[[5], [4]], [[2], [1]]]]\n    crop_output = self.execute(graph_fn, [image, boxes])\n    self.assertAllClose(crop_output, expected_output)\n\n  def testInvalidInputShape(self):\n    image = tf.constant([[[1], [2]], [[3], [4]]], dtype=tf.float32)\n    boxes = tf.constant([[-1, -1, 1, 1]], dtype=tf.float32)\n    crop_size = [4, 4]\n    with self.assertRaises(ValueError):\n      _ = ops.matmul_crop_and_resize(image, boxes, crop_size)\n\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
src/object_detection/utils/per_image_evaluation.py,0,"b'# Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Evaluate Object Detection result on a single image.\n\nAnnotate each detected result as true positives or false positive according to\na predefined IOU ratio. Non Maximum Supression is used by default. Multi class\ndetection is supported by default.\nBased on the settings, per image evaluation is either performed on boxes or\non object masks.\n""""""\nimport numpy as np\n\nfrom object_detection.utils import np_box_list\nfrom object_detection.utils import np_box_list_ops\nfrom object_detection.utils import np_box_mask_list\nfrom object_detection.utils import np_box_mask_list_ops\n\n\nclass PerImageEvaluation(object):\n  """"""Evaluate detection result of a single image.""""""\n\n  def __init__(self,\n               num_groundtruth_classes,\n               matching_iou_threshold=0.5,\n               nms_iou_threshold=0.3,\n               nms_max_output_boxes=50):\n    """"""Initialized PerImageEvaluation by evaluation parameters.\n\n    Args:\n      num_groundtruth_classes: Number of ground truth object classes\n      matching_iou_threshold: A ratio of area intersection to union, which is\n          the threshold to consider whether a detection is true positive or not\n      nms_iou_threshold: IOU threshold used in Non Maximum Suppression.\n      nms_max_output_boxes: Number of maximum output boxes in NMS.\n    """"""\n    self.matching_iou_threshold = matching_iou_threshold\n    self.nms_iou_threshold = nms_iou_threshold\n    self.nms_max_output_boxes = nms_max_output_boxes\n    self.num_groundtruth_classes = num_groundtruth_classes\n\n  def compute_object_detection_metrics(\n      self, detected_boxes, detected_scores, detected_class_labels,\n      groundtruth_boxes, groundtruth_class_labels,\n      groundtruth_is_difficult_list, groundtruth_is_group_of_list,\n      detected_masks=None, groundtruth_masks=None):\n    """"""Evaluates detections as being tp, fp or ignored from a single image.\n\n    The evaluation is done in two stages:\n     1. All detections are matched to non group-of boxes; true positives are\n        determined and detections matched to difficult boxes are ignored.\n     2. Detections that are determined as false positives are matched against\n        group-of boxes and ignored if matched.\n\n    Args:\n      detected_boxes: A float numpy array of shape [N, 4], representing N\n          regions of detected object regions.\n          Each row is of the format [y_min, x_min, y_max, x_max]\n      detected_scores: A float numpy array of shape [N, 1], representing\n          the confidence scores of the detected N object instances.\n      detected_class_labels: A integer numpy array of shape [N, 1], repreneting\n          the class labels of the detected N object instances.\n      groundtruth_boxes: A float numpy array of shape [M, 4], representing M\n          regions of object instances in ground truth\n      groundtruth_class_labels: An integer numpy array of shape [M, 1],\n          representing M class labels of object instances in ground truth\n      groundtruth_is_difficult_list: A boolean numpy array of length M denoting\n          whether a ground truth box is a difficult instance or not\n      groundtruth_is_group_of_list: A boolean numpy array of length M denoting\n          whether a ground truth box has group-of tag\n      detected_masks: (optional) A uint8 numpy array of shape\n        [N, height, width]. If not None, the metrics will be computed based\n        on masks.\n      groundtruth_masks: (optional) A uint8 numpy array of shape\n        [M, height, width].\n\n    Returns:\n      scores: A list of C float numpy arrays. Each numpy array is of\n          shape [K, 1], representing K scores detected with object class\n          label c\n      tp_fp_labels: A list of C boolean numpy arrays. Each numpy array\n          is of shape [K, 1], representing K True/False positive label of\n          object instances detected with class label c\n      is_class_correctly_detected_in_image: a numpy integer array of\n          shape [C, 1], indicating whether the correponding class has a least\n          one instance being correctly detected in the image\n    """"""\n    detected_boxes, detected_scores, detected_class_labels, detected_masks = (\n        self._remove_invalid_boxes(detected_boxes, detected_scores,\n                                   detected_class_labels, detected_masks))\n    scores, tp_fp_labels = self._compute_tp_fp(\n        detected_boxes=detected_boxes,\n        detected_scores=detected_scores,\n        detected_class_labels=detected_class_labels,\n        groundtruth_boxes=groundtruth_boxes,\n        groundtruth_class_labels=groundtruth_class_labels,\n        groundtruth_is_difficult_list=groundtruth_is_difficult_list,\n        groundtruth_is_group_of_list=groundtruth_is_group_of_list,\n        detected_masks=detected_masks,\n        groundtruth_masks=groundtruth_masks)\n\n    is_class_correctly_detected_in_image = self._compute_cor_loc(\n        detected_boxes=detected_boxes,\n        detected_scores=detected_scores,\n        detected_class_labels=detected_class_labels,\n        groundtruth_boxes=groundtruth_boxes,\n        groundtruth_class_labels=groundtruth_class_labels,\n        detected_masks=detected_masks,\n        groundtruth_masks=groundtruth_masks)\n\n    return scores, tp_fp_labels, is_class_correctly_detected_in_image\n\n  def _compute_cor_loc(self, detected_boxes, detected_scores,\n                       detected_class_labels, groundtruth_boxes,\n                       groundtruth_class_labels, detected_masks=None,\n                       groundtruth_masks=None):\n    """"""Compute CorLoc score for object detection result.\n\n    Args:\n      detected_boxes: A float numpy array of shape [N, 4], representing N\n          regions of detected object regions.\n          Each row is of the format [y_min, x_min, y_max, x_max]\n      detected_scores: A float numpy array of shape [N, 1], representing\n          the confidence scores of the detected N object instances.\n      detected_class_labels: A integer numpy array of shape [N, 1], repreneting\n          the class labels of the detected N object instances.\n      groundtruth_boxes: A float numpy array of shape [M, 4], representing M\n          regions of object instances in ground truth\n      groundtruth_class_labels: An integer numpy array of shape [M, 1],\n          representing M class labels of object instances in ground truth\n      detected_masks: (optional) A uint8 numpy array of shape\n        [N, height, width]. If not None, the scores will be computed based\n        on masks.\n      groundtruth_masks: (optional) A uint8 numpy array of shape\n        [M, height, width].\n\n    Returns:\n      is_class_correctly_detected_in_image: a numpy integer array of\n          shape [C, 1], indicating whether the correponding class has a least\n          one instance being correctly detected in the image\n\n    Raises:\n      ValueError: If detected masks is not None but groundtruth masks are None,\n        or the other way around.\n    """"""\n    if (detected_masks is not None and\n        groundtruth_masks is None) or (detected_masks is None and\n                                       groundtruth_masks is not None):\n      raise ValueError(\n          \'If `detected_masks` is provided, then `groundtruth_masks` should \'\n          \'also be provided.\'\n      )\n\n    is_class_correctly_detected_in_image = np.zeros(\n        self.num_groundtruth_classes, dtype=int)\n    for i in range(self.num_groundtruth_classes):\n      (gt_boxes_at_ith_class, gt_masks_at_ith_class,\n       detected_boxes_at_ith_class, detected_scores_at_ith_class,\n       detected_masks_at_ith_class) = self._get_ith_class_arrays(\n           detected_boxes, detected_scores, detected_masks,\n           detected_class_labels, groundtruth_boxes, groundtruth_masks,\n           groundtruth_class_labels, i)\n      is_class_correctly_detected_in_image[i] = (\n          self._compute_is_class_correctly_detected_in_image(\n              detected_boxes=detected_boxes_at_ith_class,\n              detected_scores=detected_scores_at_ith_class,\n              groundtruth_boxes=gt_boxes_at_ith_class,\n              detected_masks=detected_masks_at_ith_class,\n              groundtruth_masks=gt_masks_at_ith_class))\n\n    return is_class_correctly_detected_in_image\n\n  def _compute_is_class_correctly_detected_in_image(\n      self, detected_boxes, detected_scores, groundtruth_boxes,\n      detected_masks=None, groundtruth_masks=None):\n    """"""Compute CorLoc score for a single class.\n\n    Args:\n      detected_boxes: A numpy array of shape [N, 4] representing detected box\n          coordinates\n      detected_scores: A 1-d numpy array of length N representing classification\n          score\n      groundtruth_boxes: A numpy array of shape [M, 4] representing ground truth\n          box coordinates\n      detected_masks: (optional) A np.uint8 numpy array of shape\n        [N, height, width]. If not None, the scores will be computed based\n        on masks.\n      groundtruth_masks: (optional) A np.uint8 numpy array of shape\n        [M, height, width].\n\n    Returns:\n      is_class_correctly_detected_in_image: An integer 1 or 0 denoting whether a\n          class is correctly detected in the image or not\n    """"""\n    if detected_boxes.size > 0:\n      if groundtruth_boxes.size > 0:\n        max_score_id = np.argmax(detected_scores)\n        mask_mode = False\n        if detected_masks is not None and groundtruth_masks is not None:\n          mask_mode = True\n        if mask_mode:\n          detected_boxlist = np_box_mask_list.BoxMaskList(\n              box_data=np.expand_dims(detected_boxes[max_score_id], axis=0),\n              mask_data=np.expand_dims(detected_masks[max_score_id], axis=0))\n          gt_boxlist = np_box_mask_list.BoxMaskList(\n              box_data=groundtruth_boxes, mask_data=groundtruth_masks)\n          iou = np_box_mask_list_ops.iou(detected_boxlist, gt_boxlist)\n        else:\n          detected_boxlist = np_box_list.BoxList(\n              np.expand_dims(detected_boxes[max_score_id, :], axis=0))\n          gt_boxlist = np_box_list.BoxList(groundtruth_boxes)\n          iou = np_box_list_ops.iou(detected_boxlist, gt_boxlist)\n        if np.max(iou) >= self.matching_iou_threshold:\n          return 1\n    return 0\n\n  def _compute_tp_fp(self, detected_boxes, detected_scores,\n                     detected_class_labels, groundtruth_boxes,\n                     groundtruth_class_labels, groundtruth_is_difficult_list,\n                     groundtruth_is_group_of_list,\n                     detected_masks=None, groundtruth_masks=None):\n    """"""Labels true/false positives of detections of an image across all classes.\n\n    Args:\n      detected_boxes: A float numpy array of shape [N, 4], representing N\n          regions of detected object regions.\n          Each row is of the format [y_min, x_min, y_max, x_max]\n      detected_scores: A float numpy array of shape [N, 1], representing\n          the confidence scores of the detected N object instances.\n      detected_class_labels: A integer numpy array of shape [N, 1], repreneting\n          the class labels of the detected N object instances.\n      groundtruth_boxes: A float numpy array of shape [M, 4], representing M\n          regions of object instances in ground truth\n      groundtruth_class_labels: An integer numpy array of shape [M, 1],\n          representing M class labels of object instances in ground truth\n      groundtruth_is_difficult_list: A boolean numpy array of length M denoting\n          whether a ground truth box is a difficult instance or not\n      groundtruth_is_group_of_list: A boolean numpy array of length M denoting\n          whether a ground truth box has group-of tag\n      detected_masks: (optional) A np.uint8 numpy array of shape\n        [N, height, width]. If not None, the scores will be computed based\n        on masks.\n      groundtruth_masks: (optional) A np.uint8 numpy array of shape\n        [M, height, width].\n\n    Returns:\n      result_scores: A list of float numpy arrays. Each numpy array is of\n          shape [K, 1], representing K scores detected with object class\n          label c\n      result_tp_fp_labels: A list of boolean numpy array. Each numpy array is of\n          shape [K, 1], representing K True/False positive label of object\n          instances detected with class label c\n\n    Raises:\n      ValueError: If detected masks is not None but groundtruth masks are None,\n        or the other way around.\n    """"""\n    if detected_masks is not None and groundtruth_masks is None:\n      raise ValueError(\n          \'Detected masks is available but groundtruth masks is not.\')\n    if detected_masks is None and groundtruth_masks is not None:\n      raise ValueError(\n          \'Groundtruth masks is available but detected masks is not.\')\n\n    result_scores = []\n    result_tp_fp_labels = []\n    for i in range(self.num_groundtruth_classes):\n      groundtruth_is_difficult_list_at_ith_class = (\n          groundtruth_is_difficult_list[groundtruth_class_labels == i])\n      groundtruth_is_group_of_list_at_ith_class = (\n          groundtruth_is_group_of_list[groundtruth_class_labels == i])\n      (gt_boxes_at_ith_class, gt_masks_at_ith_class,\n       detected_boxes_at_ith_class, detected_scores_at_ith_class,\n       detected_masks_at_ith_class) = self._get_ith_class_arrays(\n           detected_boxes, detected_scores, detected_masks,\n           detected_class_labels, groundtruth_boxes, groundtruth_masks,\n           groundtruth_class_labels, i)\n      scores, tp_fp_labels = self._compute_tp_fp_for_single_class(\n          detected_boxes=detected_boxes_at_ith_class,\n          detected_scores=detected_scores_at_ith_class,\n          groundtruth_boxes=gt_boxes_at_ith_class,\n          groundtruth_is_difficult_list=\n          groundtruth_is_difficult_list_at_ith_class,\n          groundtruth_is_group_of_list=\n          groundtruth_is_group_of_list_at_ith_class,\n          detected_masks=detected_masks_at_ith_class,\n          groundtruth_masks=gt_masks_at_ith_class)\n      result_scores.append(scores)\n      result_tp_fp_labels.append(tp_fp_labels)\n    return result_scores, result_tp_fp_labels\n\n  def _get_overlaps_and_scores_mask_mode(\n      self, detected_boxes, detected_scores, detected_masks, groundtruth_boxes,\n      groundtruth_masks, groundtruth_is_group_of_list):\n    """"""Computes overlaps and scores between detected and groudntruth masks.\n\n    Args:\n      detected_boxes: A numpy array of shape [N, 4] representing detected box\n          coordinates\n      detected_scores: A 1-d numpy array of length N representing classification\n          score\n      detected_masks: A uint8 numpy array of shape [N, height, width]. If not\n          None, the scores will be computed based on masks.\n      groundtruth_boxes: A numpy array of shape [M, 4] representing ground truth\n          box coordinates\n      groundtruth_masks: A uint8 numpy array of shape [M, height, width].\n      groundtruth_is_group_of_list: A boolean numpy array of length M denoting\n          whether a ground truth box has group-of tag. If a groundtruth box\n          is group-of box, every detection matching this box is ignored.\n\n    Returns:\n      iou: A float numpy array of size [num_detected_boxes, num_gt_boxes]. If\n          gt_non_group_of_boxlist.num_boxes() == 0 it will be None.\n      ioa: A float numpy array of size [num_detected_boxes, num_gt_boxes]. If\n          gt_group_of_boxlist.num_boxes() == 0 it will be None.\n      scores: The score of the detected boxlist.\n      num_boxes: Number of non-maximum suppressed detected boxes.\n    """"""\n    detected_boxlist = np_box_mask_list.BoxMaskList(\n        box_data=detected_boxes, mask_data=detected_masks)\n    detected_boxlist.add_field(\'scores\', detected_scores)\n    detected_boxlist = np_box_mask_list_ops.non_max_suppression(\n        detected_boxlist, self.nms_max_output_boxes, self.nms_iou_threshold)\n    gt_non_group_of_boxlist = np_box_mask_list.BoxMaskList(\n        box_data=groundtruth_boxes[~groundtruth_is_group_of_list],\n        mask_data=groundtruth_masks[~groundtruth_is_group_of_list])\n    gt_group_of_boxlist = np_box_mask_list.BoxMaskList(\n        box_data=groundtruth_boxes[groundtruth_is_group_of_list],\n        mask_data=groundtruth_masks[groundtruth_is_group_of_list])\n    iou = np_box_mask_list_ops.iou(detected_boxlist, gt_non_group_of_boxlist)\n    ioa = np_box_mask_list_ops.ioa(gt_group_of_boxlist, detected_boxlist)\n    scores = detected_boxlist.get_field(\'scores\')\n    num_boxes = detected_boxlist.num_boxes()\n    return iou, ioa, scores, num_boxes\n\n  def _get_overlaps_and_scores_box_mode(\n      self,\n      detected_boxes,\n      detected_scores,\n      groundtruth_boxes,\n      groundtruth_is_group_of_list):\n    """"""Computes overlaps and scores between detected and groudntruth boxes.\n\n    Args:\n      detected_boxes: A numpy array of shape [N, 4] representing detected box\n          coordinates\n      detected_scores: A 1-d numpy array of length N representing classification\n          score\n      groundtruth_boxes: A numpy array of shape [M, 4] representing ground truth\n          box coordinates\n      groundtruth_is_group_of_list: A boolean numpy array of length M denoting\n          whether a ground truth box has group-of tag. If a groundtruth box\n          is group-of box, every detection matching this box is ignored.\n\n    Returns:\n      iou: A float numpy array of size [num_detected_boxes, num_gt_boxes]. If\n          gt_non_group_of_boxlist.num_boxes() == 0 it will be None.\n      ioa: A float numpy array of size [num_detected_boxes, num_gt_boxes]. If\n          gt_group_of_boxlist.num_boxes() == 0 it will be None.\n      scores: The score of the detected boxlist.\n      num_boxes: Number of non-maximum suppressed detected boxes.\n    """"""\n    detected_boxlist = np_box_list.BoxList(detected_boxes)\n    detected_boxlist.add_field(\'scores\', detected_scores)\n    detected_boxlist = np_box_list_ops.non_max_suppression(\n        detected_boxlist, self.nms_max_output_boxes, self.nms_iou_threshold)\n    gt_non_group_of_boxlist = np_box_list.BoxList(\n        groundtruth_boxes[~groundtruth_is_group_of_list])\n    gt_group_of_boxlist = np_box_list.BoxList(\n        groundtruth_boxes[groundtruth_is_group_of_list])\n    iou = np_box_list_ops.iou(detected_boxlist, gt_non_group_of_boxlist)\n    ioa = np_box_list_ops.ioa(gt_group_of_boxlist, detected_boxlist)\n    scores = detected_boxlist.get_field(\'scores\')\n    num_boxes = detected_boxlist.num_boxes()\n    return iou, ioa, scores, num_boxes\n\n  def _compute_tp_fp_for_single_class(\n      self, detected_boxes, detected_scores, groundtruth_boxes,\n      groundtruth_is_difficult_list, groundtruth_is_group_of_list,\n      detected_masks=None, groundtruth_masks=None):\n    """"""Labels boxes detected with the same class from the same image as tp/fp.\n\n    Args:\n      detected_boxes: A numpy array of shape [N, 4] representing detected box\n          coordinates\n      detected_scores: A 1-d numpy array of length N representing classification\n          score\n      groundtruth_boxes: A numpy array of shape [M, 4] representing ground truth\n          box coordinates\n      groundtruth_is_difficult_list: A boolean numpy array of length M denoting\n          whether a ground truth box is a difficult instance or not. If a\n          groundtruth box is difficult, every detection matching this box\n          is ignored.\n      groundtruth_is_group_of_list: A boolean numpy array of length M denoting\n          whether a ground truth box has group-of tag. If a groundtruth box\n          is group-of box, every detection matching this box is ignored.\n      detected_masks: (optional) A uint8 numpy array of shape\n        [N, height, width]. If not None, the scores will be computed based\n        on masks.\n      groundtruth_masks: (optional) A uint8 numpy array of shape\n        [M, height, width].\n\n    Returns:\n      Two arrays of the same size, containing all boxes that were evaluated as\n      being true positives or false positives; if a box matched to a difficult\n      box or to a group-of box, it is ignored.\n\n      scores: A numpy array representing the detection scores.\n      tp_fp_labels: a boolean numpy array indicating whether a detection is a\n          true positive.\n    """"""\n    if detected_boxes.size == 0:\n      return np.array([], dtype=float), np.array([], dtype=bool)\n\n    mask_mode = False\n    if detected_masks is not None and groundtruth_masks is not None:\n      mask_mode = True\n\n    if mask_mode:\n      (iou, ioa, scores,\n       num_detected_boxes) = self._get_overlaps_and_scores_mask_mode(\n           detected_boxes=detected_boxes,\n           detected_scores=detected_scores,\n           detected_masks=detected_masks,\n           groundtruth_boxes=groundtruth_boxes,\n           groundtruth_masks=groundtruth_masks,\n           groundtruth_is_group_of_list=groundtruth_is_group_of_list)\n    else:\n      (iou, ioa, scores,\n       num_detected_boxes) = self._get_overlaps_and_scores_box_mode(\n           detected_boxes=detected_boxes,\n           detected_scores=detected_scores,\n           groundtruth_boxes=groundtruth_boxes,\n           groundtruth_is_group_of_list=groundtruth_is_group_of_list)\n\n    if groundtruth_boxes.size == 0:\n      return scores, np.zeros(num_detected_boxes, dtype=bool)\n\n    tp_fp_labels = np.zeros(num_detected_boxes, dtype=bool)\n    is_matched_to_difficult_box = np.zeros(num_detected_boxes, dtype=bool)\n    is_matched_to_group_of_box = np.zeros(num_detected_boxes, dtype=bool)\n\n    # The evaluation is done in two stages:\n    # 1. All detections are matched to non group-of boxes; true positives are\n    #    determined and detections matched to difficult boxes are ignored.\n    # 2. Detections that are determined as false positives are matched against\n    #    group-of boxes and ignored if matched.\n\n    # Tp-fp evaluation for non-group of boxes (if any).\n    if iou.shape[1] > 0:\n      groundtruth_nongroup_of_is_difficult_list = groundtruth_is_difficult_list[\n          ~groundtruth_is_group_of_list]\n      max_overlap_gt_ids = np.argmax(iou, axis=1)\n      is_gt_box_detected = np.zeros(iou.shape[1], dtype=bool)\n      for i in range(num_detected_boxes):\n        gt_id = max_overlap_gt_ids[i]\n        if iou[i, gt_id] >= self.matching_iou_threshold:\n          if not groundtruth_nongroup_of_is_difficult_list[gt_id]:\n            if not is_gt_box_detected[gt_id]:\n              tp_fp_labels[i] = True\n              is_gt_box_detected[gt_id] = True\n          else:\n            is_matched_to_difficult_box[i] = True\n\n    # Tp-fp evaluation for group of boxes.\n    if ioa.shape[0] > 0:\n      max_overlap_group_of_gt = np.max(ioa, axis=0)\n      for i in range(num_detected_boxes):\n        if (not tp_fp_labels[i] and not is_matched_to_difficult_box[i] and\n            max_overlap_group_of_gt[i] >= self.matching_iou_threshold):\n          is_matched_to_group_of_box[i] = True\n\n    return scores[~is_matched_to_difficult_box\n                  & ~is_matched_to_group_of_box], tp_fp_labels[\n                      ~is_matched_to_difficult_box\n                      & ~is_matched_to_group_of_box]\n\n  def _get_ith_class_arrays(self, detected_boxes, detected_scores,\n                            detected_masks, detected_class_labels,\n                            groundtruth_boxes, groundtruth_masks,\n                            groundtruth_class_labels, class_index):\n    """"""Returns numpy arrays belonging to class with index `class_index`.\n\n    Args:\n      detected_boxes: A numpy array containing detected boxes.\n      detected_scores: A numpy array containing detected scores.\n      detected_masks: A numpy array containing detected masks.\n      detected_class_labels: A numpy array containing detected class labels.\n      groundtruth_boxes: A numpy array containing groundtruth boxes.\n      groundtruth_masks: A numpy array containing groundtruth masks.\n      groundtruth_class_labels: A numpy array containing groundtruth class\n        labels.\n      class_index: An integer index.\n\n    Returns:\n      gt_boxes_at_ith_class: A numpy array containing groundtruth boxes labeled\n        as ith class.\n      gt_masks_at_ith_class: A numpy array containing groundtruth masks labeled\n        as ith class.\n      detected_boxes_at_ith_class: A numpy array containing detected boxes\n        corresponding to the ith class.\n      detected_scores_at_ith_class: A numpy array containing detected scores\n        corresponding to the ith class.\n      detected_masks_at_ith_class: A numpy array containing detected masks\n        corresponding to the ith class.\n    """"""\n    selected_groundtruth = (groundtruth_class_labels == class_index)\n    gt_boxes_at_ith_class = groundtruth_boxes[selected_groundtruth]\n    if groundtruth_masks is not None:\n      gt_masks_at_ith_class = groundtruth_masks[selected_groundtruth]\n    else:\n      gt_masks_at_ith_class = None\n    selected_detections = (detected_class_labels == class_index)\n    detected_boxes_at_ith_class = detected_boxes[selected_detections]\n    detected_scores_at_ith_class = detected_scores[selected_detections]\n    if detected_masks is not None:\n      detected_masks_at_ith_class = detected_masks[selected_detections]\n    else:\n      detected_masks_at_ith_class = None\n    return (gt_boxes_at_ith_class, gt_masks_at_ith_class,\n            detected_boxes_at_ith_class, detected_scores_at_ith_class,\n            detected_masks_at_ith_class)\n\n  def _remove_invalid_boxes(self, detected_boxes, detected_scores,\n                            detected_class_labels, detected_masks=None):\n    """"""Removes entries with invalid boxes.\n\n    A box is invalid if either its xmax is smaller than its xmin, or its ymax\n    is smaller than its ymin.\n\n    Args:\n      detected_boxes: A float numpy array of size [num_boxes, 4] containing box\n        coordinates in [ymin, xmin, ymax, xmax] format.\n      detected_scores: A float numpy array of size [num_boxes].\n      detected_class_labels: A int32 numpy array of size [num_boxes].\n      detected_masks: A uint8 numpy array of size [num_boxes, height, width].\n\n    Returns:\n      valid_detected_boxes: A float numpy array of size [num_valid_boxes, 4]\n        containing box coordinates in [ymin, xmin, ymax, xmax] format.\n      valid_detected_scores: A float numpy array of size [num_valid_boxes].\n      valid_detected_class_labels: A int32 numpy array of size\n        [num_valid_boxes].\n      valid_detected_masks: A uint8 numpy array of size\n        [num_valid_boxes, height, width].\n    """"""\n    valid_indices = np.logical_and(detected_boxes[:, 0] < detected_boxes[:, 2],\n                                   detected_boxes[:, 1] < detected_boxes[:, 3])\n    detected_boxes = detected_boxes[valid_indices]\n    detected_scores = detected_scores[valid_indices]\n    detected_class_labels = detected_class_labels[valid_indices]\n    if detected_masks is not None:\n      detected_masks = detected_masks[valid_indices]\n    return [\n        detected_boxes, detected_scores, detected_class_labels, detected_masks\n    ]\n'"
src/object_detection/utils/per_image_evaluation_test.py,6,"b'# Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n""""""Tests for object_detection.utils.per_image_evaluation.""""""\n\nimport numpy as np\nimport tensorflow as tf\n\nfrom object_detection.utils import per_image_evaluation\n\n\nclass SingleClassTpFpWithDifficultBoxesTest(tf.test.TestCase):\n\n  def setUp(self):\n    num_groundtruth_classes = 1\n    matching_iou_threshold = 0.5\n    nms_iou_threshold = 1.0\n    nms_max_output_boxes = 10000\n    self.eval = per_image_evaluation.PerImageEvaluation(\n        num_groundtruth_classes, matching_iou_threshold, nms_iou_threshold,\n        nms_max_output_boxes)\n\n    self.detected_boxes = np.array([[0, 0, 1, 1], [0, 0, 2, 2], [0, 0, 3, 3]],\n                                   dtype=float)\n    self.detected_scores = np.array([0.6, 0.8, 0.5], dtype=float)\n    detected_masks_0 = np.array([[0, 1, 1, 0],\n                                 [0, 0, 1, 0],\n                                 [0, 0, 0, 0]], dtype=np.uint8)\n    detected_masks_1 = np.array([[1, 0, 0, 0],\n                                 [1, 1, 0, 0],\n                                 [0, 0, 0, 0]], dtype=np.uint8)\n    detected_masks_2 = np.array([[0, 0, 0, 0],\n                                 [0, 1, 1, 0],\n                                 [0, 1, 0, 0]], dtype=np.uint8)\n    self.detected_masks = np.stack(\n        [detected_masks_0, detected_masks_1, detected_masks_2], axis=0)\n    self.groundtruth_boxes = np.array([[0, 0, 1, 1], [0, 0, 10, 10]],\n                                      dtype=float)\n    groundtruth_masks_0 = np.array([[1, 1, 0, 0],\n                                    [1, 1, 0, 0],\n                                    [0, 0, 0, 0]], dtype=np.uint8)\n    groundtruth_masks_1 = np.array([[0, 0, 0, 1],\n                                    [0, 0, 0, 1],\n                                    [0, 0, 0, 1]], dtype=np.uint8)\n    self.groundtruth_masks = np.stack(\n        [groundtruth_masks_0, groundtruth_masks_1], axis=0)\n\n  def test_match_to_gt_box_0(self):\n    groundtruth_groundtruth_is_difficult_list = np.array([False, True],\n                                                         dtype=bool)\n    groundtruth_groundtruth_is_group_of_list = np.array(\n        [False, False], dtype=bool)\n    scores, tp_fp_labels = self.eval._compute_tp_fp_for_single_class(\n        self.detected_boxes, self.detected_scores, self.groundtruth_boxes,\n        groundtruth_groundtruth_is_difficult_list,\n        groundtruth_groundtruth_is_group_of_list)\n    expected_scores = np.array([0.8, 0.6, 0.5], dtype=float)\n    expected_tp_fp_labels = np.array([False, True, False], dtype=bool)\n    self.assertTrue(np.allclose(expected_scores, scores))\n    self.assertTrue(np.allclose(expected_tp_fp_labels, tp_fp_labels))\n\n  def test_mask_match_to_gt_mask_0(self):\n    groundtruth_groundtruth_is_difficult_list = np.array([False, True],\n                                                         dtype=bool)\n    groundtruth_groundtruth_is_group_of_list = np.array(\n        [False, False], dtype=bool)\n    scores, tp_fp_labels = self.eval._compute_tp_fp_for_single_class(\n        self.detected_boxes,\n        self.detected_scores,\n        self.groundtruth_boxes,\n        groundtruth_groundtruth_is_difficult_list,\n        groundtruth_groundtruth_is_group_of_list,\n        detected_masks=self.detected_masks,\n        groundtruth_masks=self.groundtruth_masks)\n    expected_scores = np.array([0.8, 0.6, 0.5], dtype=float)\n    expected_tp_fp_labels = np.array([True, False, False], dtype=bool)\n    self.assertTrue(np.allclose(expected_scores, scores))\n    self.assertTrue(np.allclose(expected_tp_fp_labels, tp_fp_labels))\n\n  def test_match_to_gt_box_1(self):\n    groundtruth_groundtruth_is_difficult_list = np.array([True, False],\n                                                         dtype=bool)\n    groundtruth_groundtruth_is_group_of_list = np.array(\n        [False, False], dtype=bool)\n    scores, tp_fp_labels = self.eval._compute_tp_fp_for_single_class(\n        self.detected_boxes, self.detected_scores, self.groundtruth_boxes,\n        groundtruth_groundtruth_is_difficult_list,\n        groundtruth_groundtruth_is_group_of_list)\n    expected_scores = np.array([0.8, 0.5], dtype=float)\n    expected_tp_fp_labels = np.array([False, False], dtype=bool)\n    self.assertTrue(np.allclose(expected_scores, scores))\n    self.assertTrue(np.allclose(expected_tp_fp_labels, tp_fp_labels))\n\n  def test_mask_match_to_gt_mask_1(self):\n    groundtruth_groundtruth_is_difficult_list = np.array([True, False],\n                                                         dtype=bool)\n    groundtruth_groundtruth_is_group_of_list = np.array(\n        [False, False], dtype=bool)\n    scores, tp_fp_labels = self.eval._compute_tp_fp_for_single_class(\n        self.detected_boxes,\n        self.detected_scores,\n        self.groundtruth_boxes,\n        groundtruth_groundtruth_is_difficult_list,\n        groundtruth_groundtruth_is_group_of_list,\n        detected_masks=self.detected_masks,\n        groundtruth_masks=self.groundtruth_masks)\n    expected_scores = np.array([0.6, 0.5], dtype=float)\n    expected_tp_fp_labels = np.array([False, False], dtype=bool)\n    self.assertTrue(np.allclose(expected_scores, scores))\n    self.assertTrue(np.allclose(expected_tp_fp_labels, tp_fp_labels))\n\n\nclass SingleClassTpFpWithGroupOfBoxesTest(tf.test.TestCase):\n\n  def setUp(self):\n    num_groundtruth_classes = 1\n    matching_iou_threshold = 0.5\n    nms_iou_threshold = 1.0\n    nms_max_output_boxes = 10000\n    self.eval = per_image_evaluation.PerImageEvaluation(\n        num_groundtruth_classes, matching_iou_threshold, nms_iou_threshold,\n        nms_max_output_boxes)\n\n    self.detected_boxes = np.array(\n        [[0, 0, 1, 1], [0, 0, 2, 1], [0, 0, 3, 1]], dtype=float)\n    self.detected_scores = np.array([0.8, 0.6, 0.5], dtype=float)\n    detected_masks_0 = np.array([[0, 1, 1, 0],\n                                 [0, 0, 1, 0],\n                                 [0, 0, 0, 0]], dtype=np.uint8)\n    detected_masks_1 = np.array([[1, 0, 0, 0],\n                                 [1, 1, 0, 0],\n                                 [0, 0, 0, 0]], dtype=np.uint8)\n    detected_masks_2 = np.array([[0, 0, 0, 0],\n                                 [0, 1, 1, 0],\n                                 [0, 1, 0, 0]], dtype=np.uint8)\n    self.detected_masks = np.stack(\n        [detected_masks_0, detected_masks_1, detected_masks_2], axis=0)\n\n    self.groundtruth_boxes = np.array(\n        [[0, 0, 1, 1], [0, 0, 5, 5], [10, 10, 20, 20]], dtype=float)\n    groundtruth_masks_0 = np.array([[1, 0, 0, 0],\n                                    [1, 0, 0, 0],\n                                    [1, 0, 0, 0]], dtype=np.uint8)\n    groundtruth_masks_1 = np.array([[0, 0, 1, 0],\n                                    [0, 0, 1, 0],\n                                    [0, 0, 1, 0]], dtype=np.uint8)\n    groundtruth_masks_2 = np.array([[0, 1, 0, 0],\n                                    [0, 1, 0, 0],\n                                    [0, 1, 0, 0]], dtype=np.uint8)\n    self.groundtruth_masks = np.stack(\n        [groundtruth_masks_0, groundtruth_masks_1, groundtruth_masks_2], axis=0)\n\n  def test_match_to_non_group_of_and_group_of_box(self):\n    groundtruth_groundtruth_is_difficult_list = np.array(\n        [False, False, False], dtype=bool)\n    groundtruth_groundtruth_is_group_of_list = np.array(\n        [False, True, True], dtype=bool)\n    expected_scores = np.array([0.8], dtype=float)\n    expected_tp_fp_labels = np.array([True], dtype=bool)\n    scores, tp_fp_labels = self.eval._compute_tp_fp_for_single_class(\n        self.detected_boxes, self.detected_scores, self.groundtruth_boxes,\n        groundtruth_groundtruth_is_difficult_list,\n        groundtruth_groundtruth_is_group_of_list)\n    self.assertTrue(np.allclose(expected_scores, scores))\n    self.assertTrue(np.allclose(expected_tp_fp_labels, tp_fp_labels))\n\n  def test_mask_match_to_non_group_of_and_group_of_box(self):\n    groundtruth_groundtruth_is_difficult_list = np.array(\n        [False, False, False], dtype=bool)\n    groundtruth_groundtruth_is_group_of_list = np.array(\n        [False, True, True], dtype=bool)\n    expected_scores = np.array([0.6], dtype=float)\n    expected_tp_fp_labels = np.array([True], dtype=bool)\n    scores, tp_fp_labels = self.eval._compute_tp_fp_for_single_class(\n        self.detected_boxes,\n        self.detected_scores,\n        self.groundtruth_boxes,\n        groundtruth_groundtruth_is_difficult_list,\n        groundtruth_groundtruth_is_group_of_list,\n        detected_masks=self.detected_masks,\n        groundtruth_masks=self.groundtruth_masks)\n    self.assertTrue(np.allclose(expected_scores, scores))\n    self.assertTrue(np.allclose(expected_tp_fp_labels, tp_fp_labels))\n\n  def test_match_two_to_group_of_box(self):\n    groundtruth_groundtruth_is_difficult_list = np.array(\n        [False, False, False], dtype=bool)\n    groundtruth_groundtruth_is_group_of_list = np.array(\n        [True, False, True], dtype=bool)\n    expected_scores = np.array([0.5], dtype=float)\n    expected_tp_fp_labels = np.array([False], dtype=bool)\n    scores, tp_fp_labels = self.eval._compute_tp_fp_for_single_class(\n        self.detected_boxes, self.detected_scores, self.groundtruth_boxes,\n        groundtruth_groundtruth_is_difficult_list,\n        groundtruth_groundtruth_is_group_of_list)\n    self.assertTrue(np.allclose(expected_scores, scores))\n    self.assertTrue(np.allclose(expected_tp_fp_labels, tp_fp_labels))\n\n  def test_mask_match_two_to_group_of_box(self):\n    groundtruth_groundtruth_is_difficult_list = np.array(\n        [False, False, False], dtype=bool)\n    groundtruth_groundtruth_is_group_of_list = np.array(\n        [True, False, True], dtype=bool)\n    expected_scores = np.array([0.8], dtype=float)\n    expected_tp_fp_labels = np.array([True], dtype=bool)\n    scores, tp_fp_labels = self.eval._compute_tp_fp_for_single_class(\n        self.detected_boxes,\n        self.detected_scores,\n        self.groundtruth_boxes,\n        groundtruth_groundtruth_is_difficult_list,\n        groundtruth_groundtruth_is_group_of_list,\n        detected_masks=self.detected_masks,\n        groundtruth_masks=self.groundtruth_masks)\n    self.assertTrue(np.allclose(expected_scores, scores))\n    self.assertTrue(np.allclose(expected_tp_fp_labels, tp_fp_labels))\n\n\nclass SingleClassTpFpNoDifficultBoxesTest(tf.test.TestCase):\n\n  def setUp(self):\n    num_groundtruth_classes = 1\n    matching_iou_threshold_high_iou = 0.5\n    matching_iou_threshold_low_iou = 0.1\n    nms_iou_threshold = 1.0\n    nms_max_output_boxes = 10000\n    self.eval_high_iou = per_image_evaluation.PerImageEvaluation(\n        num_groundtruth_classes, matching_iou_threshold_high_iou,\n        nms_iou_threshold, nms_max_output_boxes)\n\n    self.eval_low_iou = per_image_evaluation.PerImageEvaluation(\n        num_groundtruth_classes, matching_iou_threshold_low_iou,\n        nms_iou_threshold, nms_max_output_boxes)\n\n    self.detected_boxes = np.array([[0, 0, 1, 1], [0, 0, 2, 2], [0, 0, 3, 3]],\n                                   dtype=float)\n    self.detected_scores = np.array([0.6, 0.8, 0.5], dtype=float)\n    detected_masks_0 = np.array([[0, 1, 1, 0],\n                                 [0, 0, 1, 0],\n                                 [0, 0, 0, 0]], dtype=np.uint8)\n    detected_masks_1 = np.array([[1, 0, 0, 0],\n                                 [1, 1, 0, 0],\n                                 [0, 0, 0, 0]], dtype=np.uint8)\n    detected_masks_2 = np.array([[0, 0, 0, 0],\n                                 [0, 1, 1, 0],\n                                 [0, 1, 0, 0]], dtype=np.uint8)\n    self.detected_masks = np.stack(\n        [detected_masks_0, detected_masks_1, detected_masks_2], axis=0)\n\n  def test_no_true_positives(self):\n    groundtruth_boxes = np.array([[100, 100, 105, 105]], dtype=float)\n    groundtruth_groundtruth_is_difficult_list = np.zeros(1, dtype=bool)\n    groundtruth_groundtruth_is_group_of_list = np.array([False], dtype=bool)\n    scores, tp_fp_labels = self.eval_high_iou._compute_tp_fp_for_single_class(\n        self.detected_boxes, self.detected_scores, groundtruth_boxes,\n        groundtruth_groundtruth_is_difficult_list,\n        groundtruth_groundtruth_is_group_of_list)\n    expected_scores = np.array([0.8, 0.6, 0.5], dtype=float)\n    expected_tp_fp_labels = np.array([False, False, False], dtype=bool)\n    self.assertTrue(np.allclose(expected_scores, scores))\n    self.assertTrue(np.allclose(expected_tp_fp_labels, tp_fp_labels))\n\n  def test_mask_no_true_positives(self):\n    groundtruth_boxes = np.array([[100, 100, 105, 105]], dtype=float)\n    groundtruth_masks_0 = np.array([[1, 1, 1, 1],\n                                    [1, 1, 1, 1],\n                                    [1, 1, 1, 1]], dtype=np.uint8)\n    groundtruth_masks = np.stack([groundtruth_masks_0], axis=0)\n    groundtruth_groundtruth_is_difficult_list = np.zeros(1, dtype=bool)\n    groundtruth_groundtruth_is_group_of_list = np.array([False], dtype=bool)\n    scores, tp_fp_labels = self.eval_high_iou._compute_tp_fp_for_single_class(\n        self.detected_boxes,\n        self.detected_scores,\n        groundtruth_boxes,\n        groundtruth_groundtruth_is_difficult_list,\n        groundtruth_groundtruth_is_group_of_list,\n        detected_masks=self.detected_masks,\n        groundtruth_masks=groundtruth_masks)\n    expected_scores = np.array([0.8, 0.6, 0.5], dtype=float)\n    expected_tp_fp_labels = np.array([False, False, False], dtype=bool)\n    self.assertTrue(np.allclose(expected_scores, scores))\n    self.assertTrue(np.allclose(expected_tp_fp_labels, tp_fp_labels))\n\n  def test_one_true_positives_with_large_iou_threshold(self):\n    groundtruth_boxes = np.array([[0, 0, 1, 1]], dtype=float)\n    groundtruth_groundtruth_is_difficult_list = np.zeros(1, dtype=bool)\n    groundtruth_groundtruth_is_group_of_list = np.array([False], dtype=bool)\n    scores, tp_fp_labels = self.eval_high_iou._compute_tp_fp_for_single_class(\n        self.detected_boxes, self.detected_scores, groundtruth_boxes,\n        groundtruth_groundtruth_is_difficult_list,\n        groundtruth_groundtruth_is_group_of_list)\n    expected_scores = np.array([0.8, 0.6, 0.5], dtype=float)\n    expected_tp_fp_labels = np.array([False, True, False], dtype=bool)\n    self.assertTrue(np.allclose(expected_scores, scores))\n    self.assertTrue(np.allclose(expected_tp_fp_labels, tp_fp_labels))\n\n  def test_mask_one_true_positives_with_large_iou_threshold(self):\n    groundtruth_boxes = np.array([[0, 0, 1, 1]], dtype=float)\n    groundtruth_masks_0 = np.array([[1, 0, 0, 0],\n                                    [1, 1, 0, 0],\n                                    [0, 0, 0, 0]], dtype=np.uint8)\n    groundtruth_masks = np.stack([groundtruth_masks_0], axis=0)\n    groundtruth_groundtruth_is_difficult_list = np.zeros(1, dtype=bool)\n    groundtruth_groundtruth_is_group_of_list = np.array([False], dtype=bool)\n    scores, tp_fp_labels = self.eval_high_iou._compute_tp_fp_for_single_class(\n        self.detected_boxes,\n        self.detected_scores,\n        groundtruth_boxes,\n        groundtruth_groundtruth_is_difficult_list,\n        groundtruth_groundtruth_is_group_of_list,\n        detected_masks=self.detected_masks,\n        groundtruth_masks=groundtruth_masks)\n    expected_scores = np.array([0.8, 0.6, 0.5], dtype=float)\n    expected_tp_fp_labels = np.array([True, False, False], dtype=bool)\n    self.assertTrue(np.allclose(expected_scores, scores))\n    self.assertTrue(np.allclose(expected_tp_fp_labels, tp_fp_labels))\n\n  def test_one_true_positives_with_very_small_iou_threshold(self):\n    groundtruth_boxes = np.array([[0, 0, 1, 1]], dtype=float)\n    groundtruth_groundtruth_is_difficult_list = np.zeros(1, dtype=bool)\n    groundtruth_groundtruth_is_group_of_list = np.array([False], dtype=bool)\n    scores, tp_fp_labels = self.eval_low_iou._compute_tp_fp_for_single_class(\n        self.detected_boxes, self.detected_scores, groundtruth_boxes,\n        groundtruth_groundtruth_is_difficult_list,\n        groundtruth_groundtruth_is_group_of_list)\n    expected_scores = np.array([0.8, 0.6, 0.5], dtype=float)\n    expected_tp_fp_labels = np.array([True, False, False], dtype=bool)\n    self.assertTrue(np.allclose(expected_scores, scores))\n    self.assertTrue(np.allclose(expected_tp_fp_labels, tp_fp_labels))\n\n  def test_two_true_positives_with_large_iou_threshold(self):\n    groundtruth_boxes = np.array([[0, 0, 1, 1], [0, 0, 3.5, 3.5]], dtype=float)\n    groundtruth_groundtruth_is_difficult_list = np.zeros(2, dtype=bool)\n    groundtruth_groundtruth_is_group_of_list = np.array(\n        [False, False], dtype=bool)\n    scores, tp_fp_labels = self.eval_high_iou._compute_tp_fp_for_single_class(\n        self.detected_boxes, self.detected_scores, groundtruth_boxes,\n        groundtruth_groundtruth_is_difficult_list,\n        groundtruth_groundtruth_is_group_of_list)\n    expected_scores = np.array([0.8, 0.6, 0.5], dtype=float)\n    expected_tp_fp_labels = np.array([False, True, True], dtype=bool)\n    self.assertTrue(np.allclose(expected_scores, scores))\n    self.assertTrue(np.allclose(expected_tp_fp_labels, tp_fp_labels))\n\n\nclass MultiClassesTpFpTest(tf.test.TestCase):\n\n  def test_tp_fp(self):\n    num_groundtruth_classes = 3\n    matching_iou_threshold = 0.5\n    nms_iou_threshold = 1.0\n    nms_max_output_boxes = 10000\n    eval1 = per_image_evaluation.PerImageEvaluation(num_groundtruth_classes,\n                                                    matching_iou_threshold,\n                                                    nms_iou_threshold,\n                                                    nms_max_output_boxes)\n    detected_boxes = np.array([[0, 0, 1, 1], [10, 10, 5, 5], [0, 0, 2, 2],\n                               [5, 10, 10, 5], [10, 5, 5, 10], [0, 0, 3, 3]],\n                              dtype=float)\n    detected_scores = np.array([0.8, 0.1, 0.8, 0.9, 0.7, 0.8], dtype=float)\n    detected_class_labels = np.array([0, 1, 1, 2, 0, 2], dtype=int)\n    groundtruth_boxes = np.array([[0, 0, 1, 1], [0, 0, 3.5, 3.5]], dtype=float)\n    groundtruth_class_labels = np.array([0, 2], dtype=int)\n    groundtruth_groundtruth_is_difficult_list = np.zeros(2, dtype=float)\n    groundtruth_groundtruth_is_group_of_list = np.array(\n        [False, False], dtype=bool)\n    scores, tp_fp_labels, _ = eval1.compute_object_detection_metrics(\n        detected_boxes, detected_scores, detected_class_labels,\n        groundtruth_boxes, groundtruth_class_labels,\n        groundtruth_groundtruth_is_difficult_list,\n        groundtruth_groundtruth_is_group_of_list)\n    expected_scores = [np.array([0.8], dtype=float)] * 3\n    expected_tp_fp_labels = [np.array([True]), np.array([False]), np.array([True\n                                                                           ])]\n    for i in range(len(expected_scores)):\n      self.assertTrue(np.allclose(expected_scores[i], scores[i]))\n      self.assertTrue(np.array_equal(expected_tp_fp_labels[i], tp_fp_labels[i]))\n\n\nclass CorLocTest(tf.test.TestCase):\n\n  def test_compute_corloc_with_normal_iou_threshold(self):\n    num_groundtruth_classes = 3\n    matching_iou_threshold = 0.5\n    nms_iou_threshold = 1.0\n    nms_max_output_boxes = 10000\n    eval1 = per_image_evaluation.PerImageEvaluation(num_groundtruth_classes,\n                                                    matching_iou_threshold,\n                                                    nms_iou_threshold,\n                                                    nms_max_output_boxes)\n    detected_boxes = np.array([[0, 0, 1, 1], [0, 0, 2, 2], [0, 0, 3, 3],\n                               [0, 0, 5, 5]], dtype=float)\n    detected_scores = np.array([0.9, 0.9, 0.1, 0.9], dtype=float)\n    detected_class_labels = np.array([0, 1, 0, 2], dtype=int)\n    groundtruth_boxes = np.array([[0, 0, 1, 1], [0, 0, 3, 3], [0, 0, 6, 6]],\n                                 dtype=float)\n    groundtruth_class_labels = np.array([0, 0, 2], dtype=int)\n\n    is_class_correctly_detected_in_image = eval1._compute_cor_loc(\n        detected_boxes, detected_scores, detected_class_labels,\n        groundtruth_boxes, groundtruth_class_labels)\n    expected_result = np.array([1, 0, 1], dtype=int)\n    self.assertTrue(np.array_equal(expected_result,\n                                   is_class_correctly_detected_in_image))\n\n  def test_compute_corloc_with_very_large_iou_threshold(self):\n    num_groundtruth_classes = 3\n    matching_iou_threshold = 0.9\n    nms_iou_threshold = 1.0\n    nms_max_output_boxes = 10000\n    eval1 = per_image_evaluation.PerImageEvaluation(num_groundtruth_classes,\n                                                    matching_iou_threshold,\n                                                    nms_iou_threshold,\n                                                    nms_max_output_boxes)\n    detected_boxes = np.array([[0, 0, 1, 1], [0, 0, 2, 2], [0, 0, 3, 3],\n                               [0, 0, 5, 5]], dtype=float)\n    detected_scores = np.array([0.9, 0.9, 0.1, 0.9], dtype=float)\n    detected_class_labels = np.array([0, 1, 0, 2], dtype=int)\n    groundtruth_boxes = np.array([[0, 0, 1, 1], [0, 0, 3, 3], [0, 0, 6, 6]],\n                                 dtype=float)\n    groundtruth_class_labels = np.array([0, 0, 2], dtype=int)\n\n    is_class_correctly_detected_in_image = eval1._compute_cor_loc(\n        detected_boxes, detected_scores, detected_class_labels,\n        groundtruth_boxes, groundtruth_class_labels)\n    expected_result = np.array([1, 0, 0], dtype=int)\n    self.assertTrue(np.array_equal(expected_result,\n                                   is_class_correctly_detected_in_image))\n\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
src/object_detection/utils/shape_utils.py,39,"b'# Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n""""""Utils used to manipulate tensor shapes.""""""\n\nimport tensorflow as tf\n\nfrom object_detection.utils import static_shape\n\n\ndef _is_tensor(t):\n  """"""Returns a boolean indicating whether the input is a tensor.\n\n  Args:\n    t: the input to be tested.\n\n  Returns:\n    a boolean that indicates whether t is a tensor.\n  """"""\n  return isinstance(t, (tf.Tensor, tf.SparseTensor, tf.Variable))\n\n\ndef _set_dim_0(t, d0):\n  """"""Sets the 0-th dimension of the input tensor.\n\n  Args:\n    t: the input tensor, assuming the rank is at least 1.\n    d0: an integer indicating the 0-th dimension of the input tensor.\n\n  Returns:\n    the tensor t with the 0-th dimension set.\n  """"""\n  t_shape = t.get_shape().as_list()\n  t_shape[0] = d0\n  t.set_shape(t_shape)\n  return t\n\n\ndef pad_tensor(t, length):\n  """"""Pads the input tensor with 0s along the first dimension up to the length.\n\n  Args:\n    t: the input tensor, assuming the rank is at least 1.\n    length: a tensor of shape [1]  or an integer, indicating the first dimension\n      of the input tensor t after padding, assuming length <= t.shape[0].\n\n  Returns:\n    padded_t: the padded tensor, whose first dimension is length. If the length\n      is an integer, the first dimension of padded_t is set to length\n      statically.\n  """"""\n  t_rank = tf.rank(t)\n  t_shape = tf.shape(t)\n  t_d0 = t_shape[0]\n  pad_d0 = tf.expand_dims(length - t_d0, 0)\n  pad_shape = tf.cond(\n      tf.greater(t_rank, 1), lambda: tf.concat([pad_d0, t_shape[1:]], 0),\n      lambda: tf.expand_dims(length - t_d0, 0))\n  padded_t = tf.concat([t, tf.zeros(pad_shape, dtype=t.dtype)], 0)\n  if not _is_tensor(length):\n    padded_t = _set_dim_0(padded_t, length)\n  return padded_t\n\n\ndef clip_tensor(t, length):\n  """"""Clips the input tensor along the first dimension up to the length.\n\n  Args:\n    t: the input tensor, assuming the rank is at least 1.\n    length: a tensor of shape [1]  or an integer, indicating the first dimension\n      of the input tensor t after clipping, assuming length <= t.shape[0].\n\n  Returns:\n    clipped_t: the clipped tensor, whose first dimension is length. If the\n      length is an integer, the first dimension of clipped_t is set to length\n      statically.\n  """"""\n  clipped_t = tf.gather(t, tf.range(length))\n  if not _is_tensor(length):\n    clipped_t = _set_dim_0(clipped_t, length)\n  return clipped_t\n\n\ndef pad_or_clip_tensor(t, length):\n  """"""Pad or clip the input tensor along the first dimension.\n\n  Args:\n    t: the input tensor, assuming the rank is at least 1.\n    length: a tensor of shape [1]  or an integer, indicating the first dimension\n      of the input tensor t after processing.\n\n  Returns:\n    processed_t: the processed tensor, whose first dimension is length. If the\n      length is an integer, the first dimension of the processed tensor is set\n      to length statically.\n  """"""\n  processed_t = tf.cond(\n      tf.greater(tf.shape(t)[0], length),\n      lambda: clip_tensor(t, length),\n      lambda: pad_tensor(t, length))\n  if not _is_tensor(length):\n    processed_t = _set_dim_0(processed_t, length)\n  return processed_t\n\n\ndef combined_static_and_dynamic_shape(tensor):\n  """"""Returns a list containing static and dynamic values for the dimensions.\n\n  Returns a list of static and dynamic values for shape dimensions. This is\n  useful to preserve static shapes when available in reshape operation.\n\n  Args:\n    tensor: A tensor of any type.\n\n  Returns:\n    A list of size tensor.shape.ndims containing integers or a scalar tensor.\n  """"""\n  static_tensor_shape = tensor.shape.as_list()\n  dynamic_tensor_shape = tf.shape(tensor)\n  combined_shape = []\n  for index, dim in enumerate(static_tensor_shape):\n    if dim is not None:\n      combined_shape.append(dim)\n    else:\n      combined_shape.append(dynamic_tensor_shape[index])\n  return combined_shape\n\n\ndef static_or_dynamic_map_fn(fn, elems, dtype=None,\n                             parallel_iterations=32, back_prop=True):\n  """"""Runs map_fn as a (static) for loop when possible.\n\n  This function rewrites the map_fn as an explicit unstack input -> for loop\n  over function calls -> stack result combination.  This allows our graphs to\n  be acyclic when the batch size is static.\n  For comparison, see https://www.tensorflow.org/api_docs/python/tf/map_fn.\n\n  Note that `static_or_dynamic_map_fn` currently is not *fully* interchangeable\n  with the default tf.map_fn function as it does not accept nested inputs (only\n  Tensors or lists of Tensors).  Likewise, the output of `fn` can only be a\n  Tensor or list of Tensors.\n\n  TODO(jonathanhuang): make this function fully interchangeable with tf.map_fn.\n\n  Args:\n    fn: The callable to be performed. It accepts one argument, which will have\n      the same structure as elems. Its output must have the\n      same structure as elems.\n    elems: A tensor or list of tensors, each of which will\n      be unpacked along their first dimension. The sequence of the\n      resulting slices will be applied to fn.\n    dtype:  (optional) The output type(s) of fn. If fn returns a structure of\n      Tensors differing from the structure of elems, then dtype is not optional\n      and must have the same structure as the output of fn.\n    parallel_iterations: (optional) number of batch items to process in\n      parallel.  This flag is only used if the native tf.map_fn is used\n      and defaults to 32 instead of 10 (unlike the standard tf.map_fn default).\n    back_prop: (optional) True enables support for back propagation.\n      This flag is only used if the native tf.map_fn is used.\n\n  Returns:\n    A tensor or sequence of tensors. Each tensor packs the\n    results of applying fn to tensors unpacked from elems along the first\n    dimension, from first to last.\n  Raises:\n    ValueError: if `elems` a Tensor or a list of Tensors.\n    ValueError: if `fn` does not return a Tensor or list of Tensors\n  """"""\n  if isinstance(elems, list):\n    for elem in elems:\n      if not isinstance(elem, tf.Tensor):\n        raise ValueError(\'`elems` must be a Tensor or list of Tensors.\')\n\n    elem_shapes = [elem.shape.as_list() for elem in elems]\n    # Fall back on tf.map_fn if shapes of each entry of `elems` are None or fail\n    # to all be the same size along the batch dimension.\n    for elem_shape in elem_shapes:\n      if (not elem_shape or not elem_shape[0]\n          or elem_shape[0] != elem_shapes[0][0]):\n        return tf.map_fn(fn, elems, dtype, parallel_iterations, back_prop)\n    arg_tuples = zip(*[tf.unstack(elem) for elem in elems])\n    outputs = [fn(arg_tuple) for arg_tuple in arg_tuples]\n  else:\n    if not isinstance(elems, tf.Tensor):\n      raise ValueError(\'`elems` must be a Tensor or list of Tensors.\')\n    elems_shape = elems.shape.as_list()\n    if not elems_shape or not elems_shape[0]:\n      return tf.map_fn(fn, elems, dtype, parallel_iterations, back_prop)\n    outputs = [fn(arg) for arg in tf.unstack(elems)]\n  # Stack `outputs`, which is a list of Tensors or list of lists of Tensors\n  if all([isinstance(output, tf.Tensor) for output in outputs]):\n    return tf.stack(outputs)\n  else:\n    if all([isinstance(output, list) for output in outputs]):\n      if all([all(\n          [isinstance(entry, tf.Tensor) for entry in output_list])\n              for output_list in outputs]):\n        return [tf.stack(output_tuple) for output_tuple in zip(*outputs)]\n  raise ValueError(\'`fn` should return a Tensor or a list of Tensors.\')\n\n\ndef check_min_image_dim(min_dim, image_tensor):\n  """"""Checks that the image width/height are greater than some number.\n\n  This function is used to check that the width and height of an image are above\n  a certain value. If the image shape is static, this function will perform the\n  check at graph construction time. Otherwise, if the image shape varies, an\n  Assertion control dependency will be added to the graph.\n\n  Args:\n    min_dim: The minimum number of pixels along the width and height of the\n             image.\n    image_tensor: The image tensor to check size for.\n\n  Returns:\n    If `image_tensor` has dynamic size, return `image_tensor` with a Assert\n    control dependency. Otherwise returns image_tensor.\n\n  Raises:\n    ValueError: if `image_tensor`\'s\' width or height is smaller than `min_dim`.\n  """"""\n  image_shape = image_tensor.get_shape()\n  image_height = static_shape.get_height(image_shape)\n  image_width = static_shape.get_width(image_shape)\n  if image_height is None or image_width is None:\n    shape_assert = tf.Assert(\n        tf.logical_and(tf.greater_equal(tf.shape(image_tensor)[1], min_dim),\n                       tf.greater_equal(tf.shape(image_tensor)[2], min_dim)),\n        [\'image size must be >= {} in both height and width.\'.format(min_dim)])\n    with tf.control_dependencies([shape_assert]):\n      return tf.identity(image_tensor)\n\n  if image_height < min_dim or image_width < min_dim:\n    raise ValueError(\n        \'image size must be >= %d in both height and width; image dim = %d,%d\' %\n        (min_dim, image_height, image_width))\n\n  return image_tensor\n\n\ndef assert_shape_equal(shape_a, shape_b):\n  """"""Asserts that shape_a and shape_b are equal.\n\n  If the shapes are static, raises a ValueError when the shapes\n  mismatch.\n\n  If the shapes are dynamic, raises a tf InvalidArgumentError when the shapes\n  mismatch.\n\n  Args:\n    shape_a: a list containing shape of the first tensor.\n    shape_b: a list containing shape of the second tensor.\n\n  Returns:\n    Either a tf.no_op() when shapes are all static and a tf.assert_equal() op\n    when the shapes are dynamic.\n\n  Raises:\n    ValueError: When shapes are both static and unequal.\n  """"""\n  if (all(isinstance(dim, int) for dim in shape_a) and\n      all(isinstance(dim, int) for dim in shape_b)):\n    if shape_a != shape_b:\n      raise ValueError(\'Unequal shapes {}, {}\'.format(shape_a, shape_b))\n    else: return tf.no_op()\n  else:\n    return tf.assert_equal(shape_a, shape_b)\n\n\ndef assert_shape_equal_along_first_dimension(shape_a, shape_b):\n  """"""Asserts that shape_a and shape_b are the same along the 0th-dimension.\n\n  If the shapes are static, raises a ValueError when the shapes\n  mismatch.\n\n  If the shapes are dynamic, raises a tf InvalidArgumentError when the shapes\n  mismatch.\n\n  Args:\n    shape_a: a list containing shape of the first tensor.\n    shape_b: a list containing shape of the second tensor.\n\n  Returns:\n    Either a tf.no_op() when shapes are all static and a tf.assert_equal() op\n    when the shapes are dynamic.\n\n  Raises:\n    ValueError: When shapes are both static and unequal.\n  """"""\n  if isinstance(shape_a[0], int) and isinstance(shape_b[0], int):\n    if shape_a[0] != shape_b[0]:\n      raise ValueError(\'Unequal first dimension {}, {}\'.format(\n          shape_a[0], shape_b[0]))\n    else: return tf.no_op()\n  else:\n    return tf.assert_equal(shape_a[0], shape_b[0])\n\n'"
src/object_detection/utils/shape_utils_test.py,72,"b'# Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n""""""Tests for object_detection.utils.shape_utils.""""""\n\nimport numpy as np\nimport tensorflow as tf\n\nfrom object_detection.utils import shape_utils\n\n\nclass UtilTest(tf.test.TestCase):\n\n  def test_pad_tensor_using_integer_input(self):\n    t1 = tf.constant([1], dtype=tf.int32)\n    pad_t1 = shape_utils.pad_tensor(t1, 2)\n    t2 = tf.constant([[0.1, 0.2]], dtype=tf.float32)\n    pad_t2 = shape_utils.pad_tensor(t2, 2)\n\n    self.assertEqual(2, pad_t1.get_shape()[0])\n    self.assertEqual(2, pad_t2.get_shape()[0])\n\n    with self.test_session() as sess:\n      pad_t1_result, pad_t2_result = sess.run([pad_t1, pad_t2])\n      self.assertAllEqual([1, 0], pad_t1_result)\n      self.assertAllClose([[0.1, 0.2], [0, 0]], pad_t2_result)\n\n  def test_pad_tensor_using_tensor_input(self):\n    t1 = tf.constant([1], dtype=tf.int32)\n    pad_t1 = shape_utils.pad_tensor(t1, tf.constant(2))\n    t2 = tf.constant([[0.1, 0.2]], dtype=tf.float32)\n    pad_t2 = shape_utils.pad_tensor(t2, tf.constant(2))\n\n    with self.test_session() as sess:\n      pad_t1_result, pad_t2_result = sess.run([pad_t1, pad_t2])\n      self.assertAllEqual([1, 0], pad_t1_result)\n      self.assertAllClose([[0.1, 0.2], [0, 0]], pad_t2_result)\n\n  def test_clip_tensor_using_integer_input(self):\n    t1 = tf.constant([1, 2, 3], dtype=tf.int32)\n    clip_t1 = shape_utils.clip_tensor(t1, 2)\n    t2 = tf.constant([[0.1, 0.2], [0.2, 0.4], [0.5, 0.8]], dtype=tf.float32)\n    clip_t2 = shape_utils.clip_tensor(t2, 2)\n\n    self.assertEqual(2, clip_t1.get_shape()[0])\n    self.assertEqual(2, clip_t2.get_shape()[0])\n\n    with self.test_session() as sess:\n      clip_t1_result, clip_t2_result = sess.run([clip_t1, clip_t2])\n      self.assertAllEqual([1, 2], clip_t1_result)\n      self.assertAllClose([[0.1, 0.2], [0.2, 0.4]], clip_t2_result)\n\n  def test_clip_tensor_using_tensor_input(self):\n    t1 = tf.constant([1, 2, 3], dtype=tf.int32)\n    clip_t1 = shape_utils.clip_tensor(t1, tf.constant(2))\n    t2 = tf.constant([[0.1, 0.2], [0.2, 0.4], [0.5, 0.8]], dtype=tf.float32)\n    clip_t2 = shape_utils.clip_tensor(t2, tf.constant(2))\n\n    with self.test_session() as sess:\n      clip_t1_result, clip_t2_result = sess.run([clip_t1, clip_t2])\n      self.assertAllEqual([1, 2], clip_t1_result)\n      self.assertAllClose([[0.1, 0.2], [0.2, 0.4]], clip_t2_result)\n\n  def test_pad_or_clip_tensor_using_integer_input(self):\n    t1 = tf.constant([1], dtype=tf.int32)\n    tt1 = shape_utils.pad_or_clip_tensor(t1, 2)\n    t2 = tf.constant([[0.1, 0.2]], dtype=tf.float32)\n    tt2 = shape_utils.pad_or_clip_tensor(t2, 2)\n\n    t3 = tf.constant([1, 2, 3], dtype=tf.int32)\n    tt3 = shape_utils.clip_tensor(t3, 2)\n    t4 = tf.constant([[0.1, 0.2], [0.2, 0.4], [0.5, 0.8]], dtype=tf.float32)\n    tt4 = shape_utils.clip_tensor(t4, 2)\n\n    self.assertEqual(2, tt1.get_shape()[0])\n    self.assertEqual(2, tt2.get_shape()[0])\n    self.assertEqual(2, tt3.get_shape()[0])\n    self.assertEqual(2, tt4.get_shape()[0])\n\n    with self.test_session() as sess:\n      tt1_result, tt2_result, tt3_result, tt4_result = sess.run(\n          [tt1, tt2, tt3, tt4])\n      self.assertAllEqual([1, 0], tt1_result)\n      self.assertAllClose([[0.1, 0.2], [0, 0]], tt2_result)\n      self.assertAllEqual([1, 2], tt3_result)\n      self.assertAllClose([[0.1, 0.2], [0.2, 0.4]], tt4_result)\n\n  def test_pad_or_clip_tensor_using_tensor_input(self):\n    t1 = tf.constant([1], dtype=tf.int32)\n    tt1 = shape_utils.pad_or_clip_tensor(t1, tf.constant(2))\n    t2 = tf.constant([[0.1, 0.2]], dtype=tf.float32)\n    tt2 = shape_utils.pad_or_clip_tensor(t2, tf.constant(2))\n\n    t3 = tf.constant([1, 2, 3], dtype=tf.int32)\n    tt3 = shape_utils.clip_tensor(t3, tf.constant(2))\n    t4 = tf.constant([[0.1, 0.2], [0.2, 0.4], [0.5, 0.8]], dtype=tf.float32)\n    tt4 = shape_utils.clip_tensor(t4, tf.constant(2))\n\n    with self.test_session() as sess:\n      tt1_result, tt2_result, tt3_result, tt4_result = sess.run(\n          [tt1, tt2, tt3, tt4])\n      self.assertAllEqual([1, 0], tt1_result)\n      self.assertAllClose([[0.1, 0.2], [0, 0]], tt2_result)\n      self.assertAllEqual([1, 2], tt3_result)\n      self.assertAllClose([[0.1, 0.2], [0.2, 0.4]], tt4_result)\n\n  def test_combines_static_dynamic_shape(self):\n    tensor = tf.placeholder(tf.float32, shape=(None, 2, 3))\n    combined_shape = shape_utils.combined_static_and_dynamic_shape(\n        tensor)\n    self.assertTrue(tf.contrib.framework.is_tensor(combined_shape[0]))\n    self.assertListEqual(combined_shape[1:], [2, 3])\n\n\nclass StaticOrDynamicMapFnTest(tf.test.TestCase):\n\n  def test_with_dynamic_shape(self):\n    def fn(input_tensor):\n      return tf.reduce_sum(input_tensor)\n    input_tensor = tf.placeholder(tf.float32, shape=(None, 2))\n    map_fn_output = shape_utils.static_or_dynamic_map_fn(fn, input_tensor)\n\n    op_names = [op.name for op in tf.get_default_graph().get_operations()]\n    self.assertTrue(any([\'map\' == op_name[:3] for op_name in op_names]))\n\n    with self.test_session() as sess:\n      result1 = sess.run(\n          map_fn_output, feed_dict={\n              input_tensor: [[1, 2], [3, 1], [0, 4]]})\n      result2 = sess.run(\n          map_fn_output, feed_dict={\n              input_tensor: [[-1, 1], [0, 9]]})\n      self.assertAllEqual(result1, [3, 4, 4])\n      self.assertAllEqual(result2, [0, 9])\n\n  def test_with_static_shape(self):\n    def fn(input_tensor):\n      return tf.reduce_sum(input_tensor)\n    input_tensor = tf.constant([[1, 2], [3, 1], [0, 4]], dtype=tf.float32)\n    map_fn_output = shape_utils.static_or_dynamic_map_fn(fn, input_tensor)\n\n    op_names = [op.name for op in tf.get_default_graph().get_operations()]\n    self.assertTrue(all([\'map\' != op_name[:3] for op_name in op_names]))\n\n    with self.test_session() as sess:\n      result = sess.run(map_fn_output)\n      self.assertAllEqual(result, [3, 4, 4])\n\n  def test_with_multiple_dynamic_shapes(self):\n    def fn(elems):\n      input_tensor, scalar_index_tensor = elems\n      return tf.reshape(tf.slice(input_tensor, scalar_index_tensor, [1]), [])\n\n    input_tensor = tf.placeholder(tf.float32, shape=(None, 3))\n    scalar_index_tensor = tf.placeholder(tf.int32, shape=(None, 1))\n    map_fn_output = shape_utils.static_or_dynamic_map_fn(\n        fn, [input_tensor, scalar_index_tensor], dtype=tf.float32)\n\n    op_names = [op.name for op in tf.get_default_graph().get_operations()]\n    self.assertTrue(any([\'map\' == op_name[:3] for op_name in op_names]))\n\n    with self.test_session() as sess:\n      result1 = sess.run(\n          map_fn_output, feed_dict={\n              input_tensor: [[1, 2, 3], [4, 5, -1], [0, 6, 9]],\n              scalar_index_tensor: [[0], [2], [1]],\n          })\n      result2 = sess.run(\n          map_fn_output, feed_dict={\n              input_tensor: [[-1, 1, 0], [3, 9, 30]],\n              scalar_index_tensor: [[1], [0]]\n          })\n      self.assertAllEqual(result1, [1, -1, 6])\n      self.assertAllEqual(result2, [1, 3])\n\n  def test_with_multiple_static_shapes(self):\n    def fn(elems):\n      input_tensor, scalar_index_tensor = elems\n      return tf.reshape(tf.slice(input_tensor, scalar_index_tensor, [1]), [])\n\n    input_tensor = tf.constant([[1, 2, 3], [4, 5, -1], [0, 6, 9]],\n                               dtype=tf.float32)\n    scalar_index_tensor = tf.constant([[0], [2], [1]], dtype=tf.int32)\n    map_fn_output = shape_utils.static_or_dynamic_map_fn(\n        fn, [input_tensor, scalar_index_tensor], dtype=tf.float32)\n\n    op_names = [op.name for op in tf.get_default_graph().get_operations()]\n    self.assertTrue(all([\'map\' != op_name[:3] for op_name in op_names]))\n\n    with self.test_session() as sess:\n      result = sess.run(map_fn_output)\n      self.assertAllEqual(result, [1, -1, 6])\n\n  def test_fails_with_nested_input(self):\n    def fn(input_tensor):\n      return input_tensor\n    input_tensor1 = tf.constant([1])\n    input_tensor2 = tf.constant([2])\n    with self.assertRaisesRegexp(\n        ValueError, \'`elems` must be a Tensor or list of Tensors.\'):\n      shape_utils.static_or_dynamic_map_fn(\n          fn, [input_tensor1, [input_tensor2]], dtype=tf.float32)\n\n\nclass CheckMinImageShapeTest(tf.test.TestCase):\n\n  def test_check_min_image_dim_static_shape(self):\n    input_tensor = tf.constant(np.zeros([1, 42, 42, 3]))\n    _ = shape_utils.check_min_image_dim(33, input_tensor)\n\n    with self.assertRaisesRegexp(\n        ValueError, \'image size must be >= 64 in both height and width.\'):\n      _ = shape_utils.check_min_image_dim(64, input_tensor)\n\n  def test_check_min_image_dim_dynamic_shape(self):\n    input_placeholder = tf.placeholder(tf.float32, shape=[1, None, None, 3])\n    image_tensor = shape_utils.check_min_image_dim(33, input_placeholder)\n\n    with self.test_session() as sess:\n      sess.run(image_tensor,\n               feed_dict={input_placeholder: np.zeros([1, 42, 42, 3])})\n      with self.assertRaises(tf.errors.InvalidArgumentError):\n        sess.run(image_tensor,\n                 feed_dict={input_placeholder: np.zeros([1, 32, 32, 3])})\n\n\nclass AssertShapeEqualTest(tf.test.TestCase):\n\n  def test_unequal_static_shape_raises_exception(self):\n    shape_a = tf.constant(np.zeros([4, 2, 2, 1]))\n    shape_b = tf.constant(np.zeros([4, 2, 3, 1]))\n    with self.assertRaisesRegexp(\n        ValueError, \'Unequal shapes\'):\n      shape_utils.assert_shape_equal(\n          shape_utils.combined_static_and_dynamic_shape(shape_a),\n          shape_utils.combined_static_and_dynamic_shape(shape_b))\n\n  def test_equal_static_shape_succeeds(self):\n    shape_a = tf.constant(np.zeros([4, 2, 2, 1]))\n    shape_b = tf.constant(np.zeros([4, 2, 2, 1]))\n    with self.test_session() as sess:\n      op = shape_utils.assert_shape_equal(\n          shape_utils.combined_static_and_dynamic_shape(shape_a),\n          shape_utils.combined_static_and_dynamic_shape(shape_b))\n      sess.run(op)\n\n  def test_unequal_dynamic_shape_raises_tf_assert(self):\n    tensor_a = tf.placeholder(tf.float32, shape=[1, None, None, 3])\n    tensor_b = tf.placeholder(tf.float32, shape=[1, None, None, 3])\n    op = shape_utils.assert_shape_equal(\n        shape_utils.combined_static_and_dynamic_shape(tensor_a),\n        shape_utils.combined_static_and_dynamic_shape(tensor_b))\n    with self.test_session() as sess:\n      with self.assertRaises(tf.errors.InvalidArgumentError):\n        sess.run(op, feed_dict={tensor_a: np.zeros([1, 2, 2, 3]),\n                                tensor_b: np.zeros([1, 4, 4, 3])})\n\n  def test_equal_dynamic_shape_succeeds(self):\n    tensor_a = tf.placeholder(tf.float32, shape=[1, None, None, 3])\n    tensor_b = tf.placeholder(tf.float32, shape=[1, None, None, 3])\n    op = shape_utils.assert_shape_equal(\n        shape_utils.combined_static_and_dynamic_shape(tensor_a),\n        shape_utils.combined_static_and_dynamic_shape(tensor_b))\n    with self.test_session() as sess:\n      sess.run(op, feed_dict={tensor_a: np.zeros([1, 2, 2, 3]),\n                              tensor_b: np.zeros([1, 2, 2, 3])})\n\n  def test_unequal_static_shape_along_first_dim_raises_exception(self):\n    shape_a = tf.constant(np.zeros([4, 2, 2, 1]))\n    shape_b = tf.constant(np.zeros([6, 2, 3, 1]))\n    with self.assertRaisesRegexp(\n        ValueError, \'Unequal first dimension\'):\n      shape_utils.assert_shape_equal_along_first_dimension(\n          shape_utils.combined_static_and_dynamic_shape(shape_a),\n          shape_utils.combined_static_and_dynamic_shape(shape_b))\n\n  def test_equal_static_shape_along_first_dim_succeeds(self):\n    shape_a = tf.constant(np.zeros([4, 2, 2, 1]))\n    shape_b = tf.constant(np.zeros([4, 7, 2]))\n    with self.test_session() as sess:\n      op = shape_utils.assert_shape_equal_along_first_dimension(\n          shape_utils.combined_static_and_dynamic_shape(shape_a),\n          shape_utils.combined_static_and_dynamic_shape(shape_b))\n      sess.run(op)\n\n  def test_unequal_dynamic_shape_along_first_dim_raises_tf_assert(self):\n    tensor_a = tf.placeholder(tf.float32, shape=[None, None, None, 3])\n    tensor_b = tf.placeholder(tf.float32, shape=[None, None, 3])\n    op = shape_utils.assert_shape_equal_along_first_dimension(\n        shape_utils.combined_static_and_dynamic_shape(tensor_a),\n        shape_utils.combined_static_and_dynamic_shape(tensor_b))\n    with self.test_session() as sess:\n      with self.assertRaises(tf.errors.InvalidArgumentError):\n        sess.run(op, feed_dict={tensor_a: np.zeros([1, 2, 2, 3]),\n                                tensor_b: np.zeros([2, 4, 3])})\n\n  def test_equal_dynamic_shape_along_first_dim_succeeds(self):\n    tensor_a = tf.placeholder(tf.float32, shape=[None, None, None, 3])\n    tensor_b = tf.placeholder(tf.float32, shape=[None])\n    op = shape_utils.assert_shape_equal_along_first_dimension(\n        shape_utils.combined_static_and_dynamic_shape(tensor_a),\n        shape_utils.combined_static_and_dynamic_shape(tensor_b))\n    with self.test_session() as sess:\n      sess.run(op, feed_dict={tensor_a: np.zeros([5, 2, 2, 3]),\n                              tensor_b: np.zeros([5])})\n\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
src/object_detection/utils/static_shape.py,0,"b'# Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n""""""Helper functions to access TensorShape values.\n\nThe rank 4 tensor_shape must be of the form [batch_size, height, width, depth].\n""""""\n\n\ndef get_batch_size(tensor_shape):\n  """"""Returns batch size from the tensor shape.\n\n  Args:\n    tensor_shape: A rank 4 TensorShape.\n\n  Returns:\n    An integer representing the batch size of the tensor.\n  """"""\n  tensor_shape.assert_has_rank(rank=4)\n  return tensor_shape[0].value\n\n\ndef get_height(tensor_shape):\n  """"""Returns height from the tensor shape.\n\n  Args:\n    tensor_shape: A rank 4 TensorShape.\n\n  Returns:\n    An integer representing the height of the tensor.\n  """"""\n  tensor_shape.assert_has_rank(rank=4)\n  return tensor_shape[1].value\n\n\ndef get_width(tensor_shape):\n  """"""Returns width from the tensor shape.\n\n  Args:\n    tensor_shape: A rank 4 TensorShape.\n\n  Returns:\n    An integer representing the width of the tensor.\n  """"""\n  tensor_shape.assert_has_rank(rank=4)\n  return tensor_shape[2].value\n\n\ndef get_depth(tensor_shape):\n  """"""Returns depth from the tensor shape.\n\n  Args:\n    tensor_shape: A rank 4 TensorShape.\n\n  Returns:\n    An integer representing the depth of the tensor.\n  """"""\n  tensor_shape.assert_has_rank(rank=4)\n  return tensor_shape[3].value\n'"
src/object_detection/utils/static_shape_test.py,7,"b'# Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n""""""Tests for object_detection.utils.static_shape.""""""\n\nimport tensorflow as tf\n\nfrom object_detection.utils import static_shape\n\n\nclass StaticShapeTest(tf.test.TestCase):\n\n  def test_return_correct_batchSize(self):\n    tensor_shape = tf.TensorShape(dims=[32, 299, 384, 3])\n    self.assertEqual(32, static_shape.get_batch_size(tensor_shape))\n\n  def test_return_correct_height(self):\n    tensor_shape = tf.TensorShape(dims=[32, 299, 384, 3])\n    self.assertEqual(299, static_shape.get_height(tensor_shape))\n\n  def test_return_correct_width(self):\n    tensor_shape = tf.TensorShape(dims=[32, 299, 384, 3])\n    self.assertEqual(384, static_shape.get_width(tensor_shape))\n\n  def test_return_correct_depth(self):\n    tensor_shape = tf.TensorShape(dims=[32, 299, 384, 3])\n    self.assertEqual(3, static_shape.get_depth(tensor_shape))\n\n  def test_die_on_tensor_shape_with_rank_three(self):\n    tensor_shape = tf.TensorShape(dims=[32, 299, 384])\n    with self.assertRaises(ValueError):\n      static_shape.get_batch_size(tensor_shape)\n      static_shape.get_height(tensor_shape)\n      static_shape.get_width(tensor_shape)\n      static_shape.get_depth(tensor_shape)\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
src/object_detection/utils/test_case.py,12,"b'# Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""A convenience wrapper around tf.test.TestCase to enable TPU tests.""""""\n\nimport tensorflow as tf\nfrom tensorflow.contrib import tpu\n\nflags = tf.app.flags\n\nflags.DEFINE_bool(\'tpu_test\', False, \'Whether to configure test for TPU.\')\nFLAGS = flags.FLAGS\n\n\nclass TestCase(tf.test.TestCase):\n  """"""Extends tf.test.TestCase to optionally allow running tests on TPU.""""""\n\n  def execute_tpu(self, graph_fn, inputs):\n    """"""Constructs the graph, executes it on TPU and returns the result.\n\n    Args:\n      graph_fn: a callable that constructs the tensorflow graph to test. The\n        arguments of this function should correspond to `inputs`.\n      inputs: a list of numpy arrays to feed input to the computation graph.\n\n    Returns:\n      A list of numpy arrays or a scalar returned from executing the tensorflow\n      graph.\n    """"""\n    with self.test_session(graph=tf.Graph()) as sess:\n      placeholders = [tf.placeholder_with_default(v, v.shape) for v in inputs]\n      tpu_computation = tpu.rewrite(graph_fn, placeholders)\n      sess.run(tpu.initialize_system())\n      sess.run([tf.global_variables_initializer(), tf.tables_initializer(),\n                tf.local_variables_initializer()])\n      materialized_results = sess.run(tpu_computation,\n                                      feed_dict=dict(zip(placeholders, inputs)))\n      sess.run(tpu.shutdown_system())\n      if (len(materialized_results) == 1\n          and (isinstance(materialized_results, list)\n               or isinstance(materialized_results, tuple))):\n        materialized_results = materialized_results[0]\n    return materialized_results\n\n  def execute_cpu(self, graph_fn, inputs):\n    """"""Constructs the graph, executes it on CPU and returns the result.\n\n    Args:\n      graph_fn: a callable that constructs the tensorflow graph to test. The\n        arguments of this function should correspond to `inputs`.\n      inputs: a list of numpy arrays to feed input to the computation graph.\n\n    Returns:\n      A list of numpy arrays or a scalar returned from executing the tensorflow\n      graph.\n    """"""\n    with self.test_session(graph=tf.Graph()) as sess:\n      placeholders = [tf.placeholder_with_default(v, v.shape) for v in inputs]\n      results = graph_fn(*placeholders)\n      sess.run([tf.global_variables_initializer(), tf.tables_initializer(),\n                tf.local_variables_initializer()])\n      materialized_results = sess.run(results, feed_dict=dict(zip(placeholders,\n                                                                  inputs)))\n      if (len(materialized_results) == 1\n          and (isinstance(materialized_results, list)\n               or isinstance(materialized_results, tuple))):\n        materialized_results = materialized_results[0]\n    return materialized_results\n\n  def execute(self, graph_fn, inputs):\n    """"""Constructs the graph, creates a test session and returns the results.\n\n    The graph is executed either on TPU or CPU based on the `tpu_test` flag.\n\n    Args:\n      graph_fn: a callable that constructs the tensorflow graph to test. The\n        arguments of this function should correspond to `inputs`.\n      inputs: a list of numpy arrays to feed input to the computation graph.\n\n    Returns:\n      A list of numpy arrays or a scalar returned from executing the tensorflow\n      graph.\n    """"""\n    if FLAGS.tpu_test:\n      return self.execute_tpu(graph_fn, inputs)\n    else:\n      return self.execute_cpu(graph_fn, inputs)\n'"
src/object_detection/utils/test_utils.py,7,"b'# Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n""""""Contains functions which are convenient for unit testing.""""""\nimport numpy as np\nimport tensorflow as tf\n\nfrom object_detection.core import anchor_generator\nfrom object_detection.core import box_coder\nfrom object_detection.core import box_list\nfrom object_detection.core import box_predictor\nfrom object_detection.core import matcher\nfrom object_detection.utils import shape_utils\n\n\nclass MockBoxCoder(box_coder.BoxCoder):\n  """"""Simple `difference` BoxCoder.""""""\n\n  @property\n  def code_size(self):\n    return 4\n\n  def _encode(self, boxes, anchors):\n    return boxes.get() - anchors.get()\n\n  def _decode(self, rel_codes, anchors):\n    return box_list.BoxList(rel_codes + anchors.get())\n\n\nclass MockBoxPredictor(box_predictor.BoxPredictor):\n  """"""Simple box predictor that ignores inputs and outputs all zeros.""""""\n\n  def __init__(self, is_training, num_classes):\n    super(MockBoxPredictor, self).__init__(is_training, num_classes)\n\n  def _predict(self, image_features, num_predictions_per_location):\n    image_feature = image_features[0]\n    combined_feature_shape = shape_utils.combined_static_and_dynamic_shape(\n        image_feature)\n    batch_size = combined_feature_shape[0]\n    num_anchors = (combined_feature_shape[1] * combined_feature_shape[2])\n    code_size = 4\n    zero = tf.reduce_sum(0 * image_feature)\n    box_encodings = zero + tf.zeros(\n        (batch_size, num_anchors, 1, code_size), dtype=tf.float32)\n    class_predictions_with_background = zero + tf.zeros(\n        (batch_size, num_anchors, self.num_classes + 1), dtype=tf.float32)\n    return {box_predictor.BOX_ENCODINGS: box_encodings,\n            box_predictor.CLASS_PREDICTIONS_WITH_BACKGROUND:\n            class_predictions_with_background}\n\n\nclass MockAnchorGenerator(anchor_generator.AnchorGenerator):\n  """"""Mock anchor generator.""""""\n\n  def name_scope(self):\n    return \'MockAnchorGenerator\'\n\n  def num_anchors_per_location(self):\n    return [1]\n\n  def _generate(self, feature_map_shape_list):\n    num_anchors = sum([shape[0] * shape[1] for shape in feature_map_shape_list])\n    return box_list.BoxList(tf.zeros((num_anchors, 4), dtype=tf.float32))\n\n\nclass MockMatcher(matcher.Matcher):\n  """"""Simple matcher that matches first anchor to first groundtruth box.""""""\n\n  def _match(self, similarity_matrix):\n    return tf.constant([0, -1, -1, -1], dtype=tf.int32)\n\n\ndef create_diagonal_gradient_image(height, width, depth):\n  """"""Creates pyramid image. Useful for testing.\n\n  For example, pyramid_image(5, 6, 1) looks like:\n  # [[[ 5.  4.  3.  2.  1.  0.]\n  #   [ 6.  5.  4.  3.  2.  1.]\n  #   [ 7.  6.  5.  4.  3.  2.]\n  #   [ 8.  7.  6.  5.  4.  3.]\n  #   [ 9.  8.  7.  6.  5.  4.]]]\n\n  Args:\n    height: height of image\n    width: width of image\n    depth: depth of image\n\n  Returns:\n    pyramid image\n  """"""\n  row = np.arange(height)\n  col = np.arange(width)[::-1]\n  image_layer = np.expand_dims(row, 1) + col\n  image_layer = np.expand_dims(image_layer, 2)\n\n  image = image_layer\n  for i in range(1, depth):\n    image = np.concatenate((image, image_layer * pow(10, i)), 2)\n\n  return image.astype(np.float32)\n\n\ndef create_random_boxes(num_boxes, max_height, max_width):\n  """"""Creates random bounding boxes of specific maximum height and width.\n\n  Args:\n    num_boxes: number of boxes.\n    max_height: maximum height of boxes.\n    max_width: maximum width of boxes.\n\n  Returns:\n    boxes: numpy array of shape [num_boxes, 4]. Each row is in form\n        [y_min, x_min, y_max, x_max].\n  """"""\n\n  y_1 = np.random.uniform(size=(1, num_boxes)) * max_height\n  y_2 = np.random.uniform(size=(1, num_boxes)) * max_height\n  x_1 = np.random.uniform(size=(1, num_boxes)) * max_width\n  x_2 = np.random.uniform(size=(1, num_boxes)) * max_width\n\n  boxes = np.zeros(shape=(num_boxes, 4))\n  boxes[:, 0] = np.minimum(y_1, y_2)\n  boxes[:, 1] = np.minimum(x_1, x_2)\n  boxes[:, 2] = np.maximum(y_1, y_2)\n  boxes[:, 3] = np.maximum(x_1, x_2)\n\n  return boxes.astype(np.float32)\n'"
src/object_detection/utils/test_utils_test.py,2,"b'# Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n""""""Tests for object_detection.utils.test_utils.""""""\n\nimport numpy as np\nimport tensorflow as tf\n\nfrom object_detection.utils import test_utils\n\n\nclass TestUtilsTest(tf.test.TestCase):\n\n  def test_diagonal_gradient_image(self):\n    """"""Tests if a good pyramid image is created.""""""\n    pyramid_image = test_utils.create_diagonal_gradient_image(3, 4, 2)\n\n    # Test which is easy to understand.\n    expected_first_channel = np.array([[3, 2, 1, 0],\n                                       [4, 3, 2, 1],\n                                       [5, 4, 3, 2]], dtype=np.float32)\n    self.assertAllEqual(np.squeeze(pyramid_image[:, :, 0]),\n                        expected_first_channel)\n\n    # Actual test.\n    expected_image = np.array([[[3, 30],\n                                [2, 20],\n                                [1, 10],\n                                [0, 0]],\n                               [[4, 40],\n                                [3, 30],\n                                [2, 20],\n                                [1, 10]],\n                               [[5, 50],\n                                [4, 40],\n                                [3, 30],\n                                [2, 20]]], dtype=np.float32)\n\n    self.assertAllEqual(pyramid_image, expected_image)\n\n  def test_random_boxes(self):\n    """"""Tests if valid random boxes are created.""""""\n    num_boxes = 1000\n    max_height = 3\n    max_width = 5\n    boxes = test_utils.create_random_boxes(num_boxes,\n                                           max_height,\n                                           max_width)\n\n    true_column = np.ones(shape=(num_boxes)) == 1\n    self.assertAllEqual(boxes[:, 0] < boxes[:, 2], true_column)\n    self.assertAllEqual(boxes[:, 1] < boxes[:, 3], true_column)\n\n    self.assertTrue(boxes[:, 0].min() >= 0)\n    self.assertTrue(boxes[:, 1].min() >= 0)\n    self.assertTrue(boxes[:, 2].max() <= max_height)\n    self.assertTrue(boxes[:, 3].max() <= max_width)\n\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
src/object_detection/utils/variables_helper.py,4,"b'# Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n""""""Helper functions for manipulating collections of variables during training.\n""""""\nimport logging\nimport re\n\nimport tensorflow as tf\n\nslim = tf.contrib.slim\n\n\n# TODO(derekjchow): Consider replacing with tf.contrib.filter_variables in\n# tensorflow/contrib/framework/python/ops/variables.py\ndef filter_variables(variables, filter_regex_list, invert=False):\n  """"""Filters out the variables matching the filter_regex.\n\n  Filter out the variables whose name matches the any of the regular\n  expressions in filter_regex_list and returns the remaining variables.\n  Optionally, if invert=True, the complement set is returned.\n\n  Args:\n    variables: a list of tensorflow variables.\n    filter_regex_list: a list of string regular expressions.\n    invert: (boolean).  If True, returns the complement of the filter set; that\n      is, all variables matching filter_regex are kept and all others discarded.\n\n  Returns:\n    a list of filtered variables.\n  """"""\n  kept_vars = []\n  variables_to_ignore_patterns = filter(None, filter_regex_list)\n  for var in variables:\n    add = True\n    for pattern in variables_to_ignore_patterns:\n      if re.match(pattern, var.op.name):\n        add = False\n        break\n    if add != invert:\n      kept_vars.append(var)\n  return kept_vars\n\n\ndef multiply_gradients_matching_regex(grads_and_vars, regex_list, multiplier):\n  """"""Multiply gradients whose variable names match a regular expression.\n\n  Args:\n    grads_and_vars: A list of gradient to variable pairs (tuples).\n    regex_list: A list of string regular expressions.\n    multiplier: A (float) multiplier to apply to each gradient matching the\n      regular expression.\n\n  Returns:\n    grads_and_vars: A list of gradient to variable pairs (tuples).\n  """"""\n  variables = [pair[1] for pair in grads_and_vars]\n  matching_vars = filter_variables(variables, regex_list, invert=True)\n  for var in matching_vars:\n    logging.info(\'Applying multiplier %f to variable [%s]\',\n                 multiplier, var.op.name)\n  grad_multipliers = {var: float(multiplier) for var in matching_vars}\n  return slim.learning.multiply_gradients(grads_and_vars,\n                                          grad_multipliers)\n\n\ndef freeze_gradients_matching_regex(grads_and_vars, regex_list):\n  """"""Freeze gradients whose variable names match a regular expression.\n\n  Args:\n    grads_and_vars: A list of gradient to variable pairs (tuples).\n    regex_list: A list of string regular expressions.\n\n  Returns:\n    grads_and_vars: A list of gradient to variable pairs (tuples) that do not\n      contain the variables and gradients matching the regex.\n  """"""\n  variables = [pair[1] for pair in grads_and_vars]\n  matching_vars = filter_variables(variables, regex_list, invert=True)\n  kept_grads_and_vars = [pair for pair in grads_and_vars\n                         if pair[1] not in matching_vars]\n  for var in matching_vars:\n    logging.info(\'Freezing variable [%s]\', var.op.name)\n  return kept_grads_and_vars\n\n\ndef get_variables_available_in_checkpoint(variables,\n                                          checkpoint_path,\n                                          include_global_step=True):\n  """"""Returns the subset of variables available in the checkpoint.\n\n  Inspects given checkpoint and returns the subset of variables that are\n  available in it.\n\n  TODO(rathodv): force input and output to be a dictionary.\n\n  Args:\n    variables: a list or dictionary of variables to find in checkpoint.\n    checkpoint_path: path to the checkpoint to restore variables from.\n    include_global_step: whether to include `global_step` variable, if it\n      exists. Default True.\n\n  Returns:\n    A list or dictionary of variables.\n  Raises:\n    ValueError: if `variables` is not a list or dict.\n  """"""\n  if isinstance(variables, list):\n    variable_names_map = {variable.op.name: variable for variable in variables}\n  elif isinstance(variables, dict):\n    variable_names_map = variables\n  else:\n    raise ValueError(\'`variables` is expected to be a list or dict.\')\n  ckpt_reader = tf.train.NewCheckpointReader(checkpoint_path)\n  ckpt_vars_to_shape_map = ckpt_reader.get_variable_to_shape_map()\n  if not include_global_step:\n    ckpt_vars_to_shape_map.pop(tf.GraphKeys.GLOBAL_STEP, None)\n  vars_in_ckpt = {}\n  for variable_name, variable in sorted(variable_names_map.items()):\n    if variable_name in ckpt_vars_to_shape_map:\n      if ckpt_vars_to_shape_map[variable_name] == variable.shape.as_list():\n        vars_in_ckpt[variable_name] = variable\n      else:\n        logging.warning(\'Variable [%s] is available in checkpoint, but has an \'\n                        \'incompatible shape with model variable.\',\n                        variable_name)\n    else:\n      logging.warning(\'Variable [%s] is not available in checkpoint\',\n                      variable_name)\n  if isinstance(variables, list):\n    return vars_in_ckpt.values()\n  return vars_in_ckpt\n'"
src/object_detection/utils/variables_helper_test.py,48,"b'# Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n""""""Tests for object_detection.utils.variables_helper.""""""\nimport os\n\nimport tensorflow as tf\n\nfrom object_detection.utils import variables_helper\n\n\nclass FilterVariablesTest(tf.test.TestCase):\n\n  def _create_variables(self):\n    return [tf.Variable(1.0, name=\'FeatureExtractor/InceptionV3/weights\'),\n            tf.Variable(1.0, name=\'FeatureExtractor/InceptionV3/biases\'),\n            tf.Variable(1.0, name=\'StackProposalGenerator/weights\'),\n            tf.Variable(1.0, name=\'StackProposalGenerator/biases\')]\n\n  def test_return_all_variables_when_empty_regex(self):\n    variables = self._create_variables()\n    out_variables = variables_helper.filter_variables(variables, [\'\'])\n    self.assertItemsEqual(out_variables, variables)\n\n  def test_return_variables_which_do_not_match_single_regex(self):\n    variables = self._create_variables()\n    out_variables = variables_helper.filter_variables(variables,\n                                                      [\'FeatureExtractor/.*\'])\n    self.assertItemsEqual(out_variables, variables[2:])\n\n  def test_return_variables_which_do_not_match_any_regex_in_list(self):\n    variables = self._create_variables()\n    out_variables = variables_helper.filter_variables(variables, [\n        \'FeatureExtractor.*biases\', \'StackProposalGenerator.*biases\'\n    ])\n    self.assertItemsEqual(out_variables, [variables[0], variables[2]])\n\n  def test_return_variables_matching_empty_regex_list(self):\n    variables = self._create_variables()\n    out_variables = variables_helper.filter_variables(\n        variables, [\'\'], invert=True)\n    self.assertItemsEqual(out_variables, [])\n\n  def test_return_variables_matching_some_regex_in_list(self):\n    variables = self._create_variables()\n    out_variables = variables_helper.filter_variables(\n        variables,\n        [\'FeatureExtractor.*biases\', \'StackProposalGenerator.*biases\'],\n        invert=True)\n    self.assertItemsEqual(out_variables, [variables[1], variables[3]])\n\n\nclass MultiplyGradientsMatchingRegexTest(tf.test.TestCase):\n\n  def _create_grads_and_vars(self):\n    return [(tf.constant(1.0),\n             tf.Variable(1.0, name=\'FeatureExtractor/InceptionV3/weights\')),\n            (tf.constant(2.0),\n             tf.Variable(2.0, name=\'FeatureExtractor/InceptionV3/biases\')),\n            (tf.constant(3.0),\n             tf.Variable(3.0, name=\'StackProposalGenerator/weights\')),\n            (tf.constant(4.0),\n             tf.Variable(4.0, name=\'StackProposalGenerator/biases\'))]\n\n  def test_multiply_all_feature_extractor_variables(self):\n    grads_and_vars = self._create_grads_and_vars()\n    regex_list = [\'FeatureExtractor/.*\']\n    multiplier = 0.0\n    grads_and_vars = variables_helper.multiply_gradients_matching_regex(\n        grads_and_vars, regex_list, multiplier)\n    exp_output = [(0.0, 1.0), (0.0, 2.0), (3.0, 3.0), (4.0, 4.0)]\n    init_op = tf.global_variables_initializer()\n    with self.test_session() as sess:\n      sess.run(init_op)\n      output = sess.run(grads_and_vars)\n      self.assertItemsEqual(output, exp_output)\n\n  def test_multiply_all_bias_variables(self):\n    grads_and_vars = self._create_grads_and_vars()\n    regex_list = [\'.*/biases\']\n    multiplier = 0.0\n    grads_and_vars = variables_helper.multiply_gradients_matching_regex(\n        grads_and_vars, regex_list, multiplier)\n    exp_output = [(1.0, 1.0), (0.0, 2.0), (3.0, 3.0), (0.0, 4.0)]\n    init_op = tf.global_variables_initializer()\n    with self.test_session() as sess:\n      sess.run(init_op)\n      output = sess.run(grads_and_vars)\n      self.assertItemsEqual(output, exp_output)\n\n\nclass FreezeGradientsMatchingRegexTest(tf.test.TestCase):\n\n  def _create_grads_and_vars(self):\n    return [(tf.constant(1.0),\n             tf.Variable(1.0, name=\'FeatureExtractor/InceptionV3/weights\')),\n            (tf.constant(2.0),\n             tf.Variable(2.0, name=\'FeatureExtractor/InceptionV3/biases\')),\n            (tf.constant(3.0),\n             tf.Variable(3.0, name=\'StackProposalGenerator/weights\')),\n            (tf.constant(4.0),\n             tf.Variable(4.0, name=\'StackProposalGenerator/biases\'))]\n\n  def test_freeze_all_feature_extractor_variables(self):\n    grads_and_vars = self._create_grads_and_vars()\n    regex_list = [\'FeatureExtractor/.*\']\n    grads_and_vars = variables_helper.freeze_gradients_matching_regex(\n        grads_and_vars, regex_list)\n    exp_output = [(3.0, 3.0), (4.0, 4.0)]\n    init_op = tf.global_variables_initializer()\n    with self.test_session() as sess:\n      sess.run(init_op)\n      output = sess.run(grads_and_vars)\n      self.assertItemsEqual(output, exp_output)\n\n\nclass GetVariablesAvailableInCheckpointTest(tf.test.TestCase):\n\n  def test_return_all_variables_from_checkpoint(self):\n    variables = [\n        tf.Variable(1.0, name=\'weights\'),\n        tf.Variable(1.0, name=\'biases\')\n    ]\n    checkpoint_path = os.path.join(self.get_temp_dir(), \'graph.pb\')\n    init_op = tf.global_variables_initializer()\n    saver = tf.train.Saver(variables)\n    with self.test_session() as sess:\n      sess.run(init_op)\n      saver.save(sess, checkpoint_path)\n    out_variables = variables_helper.get_variables_available_in_checkpoint(\n        variables, checkpoint_path)\n    self.assertItemsEqual(out_variables, variables)\n\n  def test_return_variables_available_in_checkpoint(self):\n    checkpoint_path = os.path.join(self.get_temp_dir(), \'graph.pb\')\n    weight_variable = tf.Variable(1.0, name=\'weights\')\n    global_step = tf.train.get_or_create_global_step()\n    graph1_variables = [\n        weight_variable,\n        global_step\n    ]\n    init_op = tf.global_variables_initializer()\n    saver = tf.train.Saver(graph1_variables)\n    with self.test_session() as sess:\n      sess.run(init_op)\n      saver.save(sess, checkpoint_path)\n\n    graph2_variables = graph1_variables + [tf.Variable(1.0, name=\'biases\')]\n    out_variables = variables_helper.get_variables_available_in_checkpoint(\n        graph2_variables, checkpoint_path, include_global_step=False)\n    self.assertItemsEqual(out_variables, [weight_variable])\n\n  def test_return_variables_available_an_checkpoint_with_dict_inputs(self):\n    checkpoint_path = os.path.join(self.get_temp_dir(), \'graph.pb\')\n    graph1_variables = [\n        tf.Variable(1.0, name=\'ckpt_weights\'),\n    ]\n    init_op = tf.global_variables_initializer()\n    saver = tf.train.Saver(graph1_variables)\n    with self.test_session() as sess:\n      sess.run(init_op)\n      saver.save(sess, checkpoint_path)\n\n    graph2_variables_dict = {\n        \'ckpt_weights\': tf.Variable(1.0, name=\'weights\'),\n        \'ckpt_biases\': tf.Variable(1.0, name=\'biases\')\n    }\n    out_variables = variables_helper.get_variables_available_in_checkpoint(\n        graph2_variables_dict, checkpoint_path)\n    self.assertTrue(isinstance(out_variables, dict))\n    self.assertItemsEqual(out_variables.keys(), [\'ckpt_weights\'])\n    self.assertTrue(out_variables[\'ckpt_weights\'].op.name == \'weights\')\n\n  def test_return_variables_with_correct_sizes(self):\n    checkpoint_path = os.path.join(self.get_temp_dir(), \'graph.pb\')\n    bias_variable = tf.Variable(3.0, name=\'biases\')\n    global_step = tf.train.get_or_create_global_step()\n    graph1_variables = [\n        tf.Variable([[1.0, 2.0], [3.0, 4.0]], name=\'weights\'),\n        bias_variable,\n        global_step\n    ]\n    init_op = tf.global_variables_initializer()\n    saver = tf.train.Saver(graph1_variables)\n    with self.test_session() as sess:\n      sess.run(init_op)\n      saver.save(sess, checkpoint_path)\n\n    graph2_variables = [\n        tf.Variable([1.0, 2.0], name=\'weights\'),  # Note the new variable shape.\n        bias_variable,\n        global_step\n    ]\n\n    out_variables = variables_helper.get_variables_available_in_checkpoint(\n        graph2_variables, checkpoint_path, include_global_step=True)\n    self.assertItemsEqual(out_variables, [bias_variable, global_step])\n\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
src/object_detection/utils/visualization_utils.py,23,"b'# Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n""""""A set of functions that are used for visualization.\n\nThese functions often receive an image, perform some visualization on the image.\nThe functions do not return a value, instead they modify the image itself.\n\n""""""\nimport collections\nimport functools\n# Set headless-friendly backend.\nimport matplotlib; matplotlib.use(\'Agg\')  # pylint: disable=multiple-statements\nimport matplotlib.pyplot as plt  # pylint: disable=g-import-not-at-top\nimport numpy as np\nimport PIL.Image as Image\nimport PIL.ImageColor as ImageColor\nimport PIL.ImageDraw as ImageDraw\nimport PIL.ImageFont as ImageFont\nimport six\nimport tensorflow as tf\n\nfrom object_detection.core import standard_fields as fields\n\n\n_TITLE_LEFT_MARGIN = 10\n_TITLE_TOP_MARGIN = 10\nSTANDARD_COLORS = [\n    \'AliceBlue\', \'Chartreuse\', \'Aqua\', \'Aquamarine\', \'Azure\', \'Beige\', \'Bisque\',\n    \'BlanchedAlmond\', \'BlueViolet\', \'BurlyWood\', \'CadetBlue\', \'AntiqueWhite\',\n    \'Chocolate\', \'Coral\', \'CornflowerBlue\', \'Cornsilk\', \'Crimson\', \'Cyan\',\n    \'DarkCyan\', \'DarkGoldenRod\', \'DarkGrey\', \'DarkKhaki\', \'DarkOrange\',\n    \'DarkOrchid\', \'DarkSalmon\', \'DarkSeaGreen\', \'DarkTurquoise\', \'DarkViolet\',\n    \'DeepPink\', \'DeepSkyBlue\', \'DodgerBlue\', \'FireBrick\', \'FloralWhite\',\n    \'ForestGreen\', \'Fuchsia\', \'Gainsboro\', \'GhostWhite\', \'Gold\', \'GoldenRod\',\n    \'Salmon\', \'Tan\', \'HoneyDew\', \'HotPink\', \'IndianRed\', \'Ivory\', \'Khaki\',\n    \'Lavender\', \'LavenderBlush\', \'LawnGreen\', \'LemonChiffon\', \'LightBlue\',\n    \'LightCoral\', \'LightCyan\', \'LightGoldenRodYellow\', \'LightGray\', \'LightGrey\',\n    \'LightGreen\', \'LightPink\', \'LightSalmon\', \'LightSeaGreen\', \'LightSkyBlue\',\n    \'LightSlateGray\', \'LightSlateGrey\', \'LightSteelBlue\', \'LightYellow\', \'Lime\',\n    \'LimeGreen\', \'Linen\', \'Magenta\', \'MediumAquaMarine\', \'MediumOrchid\',\n    \'MediumPurple\', \'MediumSeaGreen\', \'MediumSlateBlue\', \'MediumSpringGreen\',\n    \'MediumTurquoise\', \'MediumVioletRed\', \'MintCream\', \'MistyRose\', \'Moccasin\',\n    \'NavajoWhite\', \'OldLace\', \'Olive\', \'OliveDrab\', \'Orange\', \'OrangeRed\',\n    \'Orchid\', \'PaleGoldenRod\', \'PaleGreen\', \'PaleTurquoise\', \'PaleVioletRed\',\n    \'PapayaWhip\', \'PeachPuff\', \'Peru\', \'Pink\', \'Plum\', \'PowderBlue\', \'Purple\',\n    \'Red\', \'RosyBrown\', \'RoyalBlue\', \'SaddleBrown\', \'Green\', \'SandyBrown\',\n    \'SeaGreen\', \'SeaShell\', \'Sienna\', \'Silver\', \'SkyBlue\', \'SlateBlue\',\n    \'SlateGray\', \'SlateGrey\', \'Snow\', \'SpringGreen\', \'SteelBlue\', \'GreenYellow\',\n    \'Teal\', \'Thistle\', \'Tomato\', \'Turquoise\', \'Violet\', \'Wheat\', \'White\',\n    \'WhiteSmoke\', \'Yellow\', \'YellowGreen\'\n]\n\n\ndef save_image_array_as_png(image, output_path):\n  """"""Saves an image (represented as a numpy array) to PNG.\n\n  Args:\n    image: a numpy array with shape [height, width, 3].\n    output_path: path to which image should be written.\n  """"""\n  image_pil = Image.fromarray(np.uint8(image)).convert(\'RGB\')\n  with tf.gfile.Open(output_path, \'w\') as fid:\n    image_pil.save(fid, \'PNG\')\n\n\ndef encode_image_array_as_png_str(image):\n  """"""Encodes a numpy array into a PNG string.\n\n  Args:\n    image: a numpy array with shape [height, width, 3].\n\n  Returns:\n    PNG encoded image string.\n  """"""\n  image_pil = Image.fromarray(np.uint8(image))\n  output = six.BytesIO()\n  image_pil.save(output, format=\'PNG\')\n  png_string = output.getvalue()\n  output.close()\n  return png_string\n\n\ndef draw_bounding_box_on_image_array(image,\n                                     ymin,\n                                     xmin,\n                                     ymax,\n                                     xmax,\n                                     color=\'red\',\n                                     thickness=4,\n                                     display_str_list=(),\n                                     use_normalized_coordinates=True):\n  """"""Adds a bounding box to an image (numpy array).\n\n  Bounding box coordinates can be specified in either absolute (pixel) or\n  normalized coordinates by setting the use_normalized_coordinates argument.\n\n  Args:\n    image: a numpy array with shape [height, width, 3].\n    ymin: ymin of bounding box.\n    xmin: xmin of bounding box.\n    ymax: ymax of bounding box.\n    xmax: xmax of bounding box.\n    color: color to draw bounding box. Default is red.\n    thickness: line thickness. Default value is 4.\n    display_str_list: list of strings to display in box\n                      (each to be shown on its own line).\n    use_normalized_coordinates: If True (default), treat coordinates\n      ymin, xmin, ymax, xmax as relative to the image.  Otherwise treat\n      coordinates as absolute.\n  """"""\n  image_pil = Image.fromarray(np.uint8(image)).convert(\'RGB\')\n  draw_bounding_box_on_image(image_pil, ymin, xmin, ymax, xmax, color,\n                             thickness, display_str_list,\n                             use_normalized_coordinates)\n  np.copyto(image, np.array(image_pil))\n\n\ndef draw_bounding_box_on_image(image,\n                               ymin,\n                               xmin,\n                               ymax,\n                               xmax,\n                               color=\'red\',\n                               thickness=4,\n                               display_str_list=(),\n                               use_normalized_coordinates=True):\n  """"""Adds a bounding box to an image.\n\n  Bounding box coordinates can be specified in either absolute (pixel) or\n  normalized coordinates by setting the use_normalized_coordinates argument.\n\n  Each string in display_str_list is displayed on a separate line above the\n  bounding box in black text on a rectangle filled with the input \'color\'.\n  If the top of the bounding box extends to the edge of the image, the strings\n  are displayed below the bounding box.\n\n  Args:\n    image: a PIL.Image object.\n    ymin: ymin of bounding box.\n    xmin: xmin of bounding box.\n    ymax: ymax of bounding box.\n    xmax: xmax of bounding box.\n    color: color to draw bounding box. Default is red.\n    thickness: line thickness. Default value is 4.\n    display_str_list: list of strings to display in box\n                      (each to be shown on its own line).\n    use_normalized_coordinates: If True (default), treat coordinates\n      ymin, xmin, ymax, xmax as relative to the image.  Otherwise treat\n      coordinates as absolute.\n  """"""\n  draw = ImageDraw.Draw(image)\n  im_width, im_height = image.size\n  if use_normalized_coordinates:\n    (left, right, top, bottom) = (xmin * im_width, xmax * im_width,\n                                  ymin * im_height, ymax * im_height)\n  else:\n    (left, right, top, bottom) = (xmin, xmax, ymin, ymax)\n  draw.line([(left, top), (left, bottom), (right, bottom),\n             (right, top), (left, top)], width=thickness, fill=color)\n  try:\n    font = ImageFont.truetype(\'arial.ttf\', 24)\n  except IOError:\n    font = ImageFont.load_default()\n\n  # If the total height of the display strings added to the top of the bounding\n  # box exceeds the top of the image, stack the strings below the bounding box\n  # instead of above.\n  display_str_heights = [font.getsize(ds)[1] for ds in display_str_list]\n  # Each display_str has a top and bottom margin of 0.05x.\n  total_display_str_height = (1 + 2 * 0.05) * sum(display_str_heights)\n\n  if top > total_display_str_height:\n    text_bottom = top\n  else:\n    text_bottom = bottom + total_display_str_height\n  # Reverse list and print from bottom to top.\n  for display_str in display_str_list[::-1]:\n    text_width, text_height = font.getsize(display_str)\n    margin = np.ceil(0.05 * text_height)\n    draw.rectangle(\n        [(left, text_bottom - text_height - 2 * margin), (left + text_width,\n                                                          text_bottom)],\n        fill=color)\n    draw.text(\n        (left + margin, text_bottom - text_height - margin),\n        display_str,\n        fill=\'black\',\n        font=font)\n    text_bottom -= text_height - 2 * margin\n\n\ndef draw_bounding_boxes_on_image_array(image,\n                                       boxes,\n                                       color=\'red\',\n                                       thickness=4,\n                                       display_str_list_list=()):\n  """"""Draws bounding boxes on image (numpy array).\n\n  Args:\n    image: a numpy array object.\n    boxes: a 2 dimensional numpy array of [N, 4]: (ymin, xmin, ymax, xmax).\n           The coordinates are in normalized format between [0, 1].\n    color: color to draw bounding box. Default is red.\n    thickness: line thickness. Default value is 4.\n    display_str_list_list: list of list of strings.\n                           a list of strings for each bounding box.\n                           The reason to pass a list of strings for a\n                           bounding box is that it might contain\n                           multiple labels.\n\n  Raises:\n    ValueError: if boxes is not a [N, 4] array\n  """"""\n  image_pil = Image.fromarray(image)\n  draw_bounding_boxes_on_image(image_pil, boxes, color, thickness,\n                               display_str_list_list)\n  np.copyto(image, np.array(image_pil))\n\n\ndef draw_bounding_boxes_on_image(image,\n                                 boxes,\n                                 color=\'red\',\n                                 thickness=4,\n                                 display_str_list_list=()):\n  """"""Draws bounding boxes on image.\n\n  Args:\n    image: a PIL.Image object.\n    boxes: a 2 dimensional numpy array of [N, 4]: (ymin, xmin, ymax, xmax).\n           The coordinates are in normalized format between [0, 1].\n    color: color to draw bounding box. Default is red.\n    thickness: line thickness. Default value is 4.\n    display_str_list_list: list of list of strings.\n                           a list of strings for each bounding box.\n                           The reason to pass a list of strings for a\n                           bounding box is that it might contain\n                           multiple labels.\n\n  Raises:\n    ValueError: if boxes is not a [N, 4] array\n  """"""\n  boxes_shape = boxes.shape\n  if not boxes_shape:\n    return\n  if len(boxes_shape) != 2 or boxes_shape[1] != 4:\n    raise ValueError(\'Input must be of size [N, 4]\')\n  for i in range(boxes_shape[0]):\n    display_str_list = ()\n    if display_str_list_list:\n      display_str_list = display_str_list_list[i]\n    draw_bounding_box_on_image(image, boxes[i, 0], boxes[i, 1], boxes[i, 2],\n                               boxes[i, 3], color, thickness, display_str_list)\n\n\ndef _visualize_boxes(image, boxes, classes, scores, category_index, **kwargs):\n  return visualize_boxes_and_labels_on_image_array(\n      image, boxes, classes, scores, category_index=category_index, **kwargs)\n\n\ndef _visualize_boxes_and_masks(image, boxes, classes, scores, masks,\n                               category_index, **kwargs):\n  return visualize_boxes_and_labels_on_image_array(\n      image,\n      boxes,\n      classes,\n      scores,\n      category_index=category_index,\n      instance_masks=masks,\n      **kwargs)\n\n\ndef _visualize_boxes_and_keypoints(image, boxes, classes, scores, keypoints,\n                                   category_index, **kwargs):\n  return visualize_boxes_and_labels_on_image_array(\n      image,\n      boxes,\n      classes,\n      scores,\n      category_index=category_index,\n      keypoints=keypoints,\n      **kwargs)\n\n\ndef _visualize_boxes_and_masks_and_keypoints(\n    image, boxes, classes, scores, masks, keypoints, category_index, **kwargs):\n  return visualize_boxes_and_labels_on_image_array(\n      image,\n      boxes,\n      classes,\n      scores,\n      category_index=category_index,\n      instance_masks=masks,\n      keypoints=keypoints,\n      **kwargs)\n\n\ndef draw_bounding_boxes_on_image_tensors(images,\n                                         boxes,\n                                         classes,\n                                         scores,\n                                         category_index,\n                                         instance_masks=None,\n                                         keypoints=None,\n                                         max_boxes_to_draw=20,\n                                         min_score_thresh=0.2):\n  """"""Draws bounding boxes, masks, and keypoints on batch of image tensors.\n\n  Args:\n    images: A 4D uint8 image tensor of shape [N, H, W, C].\n    boxes: [N, max_detections, 4] float32 tensor of detection boxes.\n    classes: [N, max_detections] int tensor of detection classes. Note that\n      classes are 1-indexed.\n    scores: [N, max_detections] float32 tensor of detection scores.\n    category_index: a dict that maps integer ids to category dicts. e.g.\n      {1: {1: \'dog\'}, 2: {2: \'cat\'}, ...}\n    instance_masks: A 4D uint8 tensor of shape [N, max_detection, H, W] with\n      instance masks.\n    keypoints: A 4D float32 tensor of shape [N, max_detection, num_keypoints, 2]\n      with keypoints.\n    max_boxes_to_draw: Maximum number of boxes to draw on an image. Default 20.\n    min_score_thresh: Minimum score threshold for visualization. Default 0.2.\n\n  Returns:\n    4D image tensor of type uint8, with boxes drawn on top.\n  """"""\n  visualization_keyword_args = {\n      \'use_normalized_coordinates\': True,\n      \'max_boxes_to_draw\': max_boxes_to_draw,\n      \'min_score_thresh\': min_score_thresh,\n      \'agnostic_mode\': False,\n      \'line_thickness\': 4\n  }\n\n  if instance_masks is not None and keypoints is None:\n    visualize_boxes_fn = functools.partial(\n        _visualize_boxes_and_masks,\n        category_index=category_index,\n        **visualization_keyword_args)\n    elems = [images, boxes, classes, scores, instance_masks]\n  elif instance_masks is None and keypoints is not None:\n    visualize_boxes_fn = functools.partial(\n        _visualize_boxes_and_keypoints,\n        category_index=category_index,\n        **visualization_keyword_args)\n    elems = [images, boxes, classes, scores, keypoints]\n  elif instance_masks is not None and keypoints is not None:\n    visualize_boxes_fn = functools.partial(\n        _visualize_boxes_and_masks_and_keypoints,\n        category_index=category_index,\n        **visualization_keyword_args)\n    elems = [images, boxes, classes, scores, instance_masks, keypoints]\n  else:\n    visualize_boxes_fn = functools.partial(\n        _visualize_boxes,\n        category_index=category_index,\n        **visualization_keyword_args)\n    elems = [images, boxes, classes, scores]\n\n  def draw_boxes(image_and_detections):\n    """"""Draws boxes on image.""""""\n    image_with_boxes = tf.py_func(visualize_boxes_fn, image_and_detections,\n                                  tf.uint8)\n    return image_with_boxes\n\n  images = tf.map_fn(draw_boxes, elems, dtype=tf.uint8, back_prop=False)\n  return images\n\n\ndef draw_side_by_side_evaluation_image(eval_dict,\n                                       category_index,\n                                       max_boxes_to_draw=20,\n                                       min_score_thresh=0.2):\n  """"""Creates a side-by-side image with detections and groundtruth.\n\n  Bounding boxes (and instance masks, if available) are visualized on both\n  subimages.\n\n  Args:\n    eval_dict: The evaluation dictionary returned by\n      eval_util.result_dict_for_single_example().\n    category_index: A category index (dictionary) produced from a labelmap.\n    max_boxes_to_draw: The maximum number of boxes to draw for detections.\n    min_score_thresh: The minimum score threshold for showing detections.\n\n  Returns:\n    A [1, H, 2 * W, C] uint8 tensor. The subimage on the left corresponds to\n      detections, while the subimage on the right corresponds to groundtruth.\n  """"""\n  detection_fields = fields.DetectionResultFields()\n  input_data_fields = fields.InputDataFields()\n  instance_masks = None\n  if detection_fields.detection_masks in eval_dict:\n    instance_masks = tf.cast(\n        tf.expand_dims(eval_dict[detection_fields.detection_masks], axis=0),\n        tf.uint8)\n  keypoints = None\n  if detection_fields.detection_keypoints in eval_dict:\n    keypoints = tf.expand_dims(\n        eval_dict[detection_fields.detection_keypoints], axis=0)\n  groundtruth_instance_masks = None\n  if input_data_fields.groundtruth_instance_masks in eval_dict:\n    groundtruth_instance_masks = tf.cast(\n        tf.expand_dims(\n            eval_dict[input_data_fields.groundtruth_instance_masks], axis=0),\n        tf.uint8)\n  images_with_detections = draw_bounding_boxes_on_image_tensors(\n      eval_dict[input_data_fields.original_image],\n      tf.expand_dims(eval_dict[detection_fields.detection_boxes], axis=0),\n      tf.expand_dims(eval_dict[detection_fields.detection_classes], axis=0),\n      tf.expand_dims(eval_dict[detection_fields.detection_scores], axis=0),\n      category_index,\n      instance_masks=instance_masks,\n      keypoints=keypoints,\n      max_boxes_to_draw=max_boxes_to_draw,\n      min_score_thresh=min_score_thresh)\n  images_with_groundtruth = draw_bounding_boxes_on_image_tensors(\n      eval_dict[input_data_fields.original_image],\n      tf.expand_dims(eval_dict[input_data_fields.groundtruth_boxes], axis=0),\n      tf.expand_dims(eval_dict[input_data_fields.groundtruth_classes], axis=0),\n      tf.expand_dims(\n          tf.ones_like(\n              eval_dict[input_data_fields.groundtruth_classes],\n              dtype=tf.float32),\n          axis=0),\n      category_index,\n      instance_masks=groundtruth_instance_masks,\n      keypoints=None,\n      max_boxes_to_draw=None,\n      min_score_thresh=0.0)\n  return tf.concat([images_with_detections, images_with_groundtruth], axis=2)\n\n\ndef draw_keypoints_on_image_array(image,\n                                  keypoints,\n                                  color=\'red\',\n                                  radius=2,\n                                  use_normalized_coordinates=True):\n  """"""Draws keypoints on an image (numpy array).\n\n  Args:\n    image: a numpy array with shape [height, width, 3].\n    keypoints: a numpy array with shape [num_keypoints, 2].\n    color: color to draw the keypoints with. Default is red.\n    radius: keypoint radius. Default value is 2.\n    use_normalized_coordinates: if True (default), treat keypoint values as\n      relative to the image.  Otherwise treat them as absolute.\n  """"""\n  image_pil = Image.fromarray(np.uint8(image)).convert(\'RGB\')\n  draw_keypoints_on_image(image_pil, keypoints, color, radius,\n                          use_normalized_coordinates)\n  np.copyto(image, np.array(image_pil))\n\n\ndef draw_keypoints_on_image(image,\n                            keypoints,\n                            color=\'red\',\n                            radius=2,\n                            use_normalized_coordinates=True):\n  """"""Draws keypoints on an image.\n\n  Args:\n    image: a PIL.Image object.\n    keypoints: a numpy array with shape [num_keypoints, 2].\n    color: color to draw the keypoints with. Default is red.\n    radius: keypoint radius. Default value is 2.\n    use_normalized_coordinates: if True (default), treat keypoint values as\n      relative to the image.  Otherwise treat them as absolute.\n  """"""\n  draw = ImageDraw.Draw(image)\n  im_width, im_height = image.size\n  keypoints_x = [k[1] for k in keypoints]\n  keypoints_y = [k[0] for k in keypoints]\n  if use_normalized_coordinates:\n    keypoints_x = tuple([im_width * x for x in keypoints_x])\n    keypoints_y = tuple([im_height * y for y in keypoints_y])\n  for keypoint_x, keypoint_y in zip(keypoints_x, keypoints_y):\n    draw.ellipse([(keypoint_x - radius, keypoint_y - radius),\n                  (keypoint_x + radius, keypoint_y + radius)],\n                 outline=color, fill=color)\n\n\ndef draw_mask_on_image_array(image, mask, color=\'red\', alpha=0.4):\n  """"""Draws mask on an image.\n\n  Args:\n    image: uint8 numpy array with shape (img_height, img_height, 3)\n    mask: a uint8 numpy array of shape (img_height, img_height) with\n      values between either 0 or 1.\n    color: color to draw the keypoints with. Default is red.\n    alpha: transparency value between 0 and 1. (default: 0.4)\n\n  Raises:\n    ValueError: On incorrect data type for image or masks.\n  """"""\n  if image.dtype != np.uint8:\n    raise ValueError(\'`image` not of type np.uint8\')\n  if mask.dtype != np.uint8:\n    raise ValueError(\'`mask` not of type np.uint8\')\n  if np.any(np.logical_and(mask != 1, mask != 0)):\n    raise ValueError(\'`mask` elements should be in [0, 1]\')\n  if image.shape[:2] != mask.shape:\n    raise ValueError(\'The image has spatial dimensions %s but the mask has \'\n                     \'dimensions %s\' % (image.shape[:2], mask.shape))\n  rgb = ImageColor.getrgb(color)\n  pil_image = Image.fromarray(image)\n\n  solid_color = np.expand_dims(\n      np.ones_like(mask), axis=2) * np.reshape(list(rgb), [1, 1, 3])\n  pil_solid_color = Image.fromarray(np.uint8(solid_color)).convert(\'RGBA\')\n  pil_mask = Image.fromarray(np.uint8(255.0*alpha*mask)).convert(\'L\')\n  pil_image = Image.composite(pil_solid_color, pil_image, pil_mask)\n  np.copyto(image, np.array(pil_image.convert(\'RGB\')))\n\n\ndef visualize_boxes_and_labels_on_image_array(\n    image,\n    boxes,\n    classes,\n    scores,\n    category_index,\n    instance_masks=None,\n    instance_boundaries=None,\n    keypoints=None,\n    use_normalized_coordinates=False,\n    max_boxes_to_draw=20,\n    min_score_thresh=.5,\n    agnostic_mode=False,\n    line_thickness=4,\n    groundtruth_box_visualization_color=\'black\',\n    skip_scores=False,\n    skip_labels=False):\n  """"""Overlay labeled boxes on an image with formatted scores and label names.\n\n  This function groups boxes that correspond to the same location\n  and creates a display string for each detection and overlays these\n  on the image. Note that this function modifies the image in place, and returns\n  that same image.\n\n  Args:\n    image: uint8 numpy array with shape (img_height, img_width, 3)\n    boxes: a numpy array of shape [N, 4]\n    classes: a numpy array of shape [N]. Note that class indices are 1-based,\n      and match the keys in the label map.\n    scores: a numpy array of shape [N] or None.  If scores=None, then\n      this function assumes that the boxes to be plotted are groundtruth\n      boxes and plot all boxes as black with no classes or scores.\n    category_index: a dict containing category dictionaries (each holding\n      category index `id` and category name `name`) keyed by category indices.\n    instance_masks: a numpy array of shape [N, image_height, image_width] with\n      values ranging between 0 and 1, can be None.\n    instance_boundaries: a numpy array of shape [N, image_height, image_width]\n      with values ranging between 0 and 1, can be None.\n    keypoints: a numpy array of shape [N, num_keypoints, 2], can\n      be None\n    use_normalized_coordinates: whether boxes is to be interpreted as\n      normalized coordinates or not.\n    max_boxes_to_draw: maximum number of boxes to visualize.  If None, draw\n      all boxes.\n    min_score_thresh: minimum score threshold for a box to be visualized\n    agnostic_mode: boolean (default: False) controlling whether to evaluate in\n      class-agnostic mode or not.  This mode will display scores but ignore\n      classes.\n    line_thickness: integer (default: 4) controlling line width of the boxes.\n    groundtruth_box_visualization_color: box color for visualizing groundtruth\n      boxes\n    skip_scores: whether to skip score when drawing a single detection\n    skip_labels: whether to skip label when drawing a single detection\n\n  Returns:\n    uint8 numpy array with shape (img_height, img_width, 3) with overlaid boxes.\n  """"""\n  # Create a display string (and color) for every box location, group any boxes\n  # that correspond to the same location.\n  box_to_display_str_map = collections.defaultdict(list)\n  box_to_color_map = collections.defaultdict(str)\n  box_to_instance_masks_map = {}\n  box_to_instance_boundaries_map = {}\n  box_to_keypoints_map = collections.defaultdict(list)\n  if not max_boxes_to_draw:\n    max_boxes_to_draw = boxes.shape[0]\n  for i in range(min(max_boxes_to_draw, boxes.shape[0])):\n    if scores is None or scores[i] > min_score_thresh:\n      box = tuple(boxes[i].tolist())\n      if instance_masks is not None:\n        box_to_instance_masks_map[box] = instance_masks[i]\n      if instance_boundaries is not None:\n        box_to_instance_boundaries_map[box] = instance_boundaries[i]\n      if keypoints is not None:\n        box_to_keypoints_map[box].extend(keypoints[i])\n      if scores is None:\n        box_to_color_map[box] = groundtruth_box_visualization_color\n      else:\n        display_str = \'\'\n        if not skip_labels:\n          if not agnostic_mode:\n            if classes[i] in category_index.keys():\n              class_name = category_index[classes[i]][\'name\']\n            else:\n              class_name = \'N/A\'\n            display_str = str(class_name)\n        if not skip_scores:\n          if not display_str:\n            display_str = \'{}%\'.format(int(100*scores[i]))\n          else:\n            display_str = \'{}: {}%\'.format(display_str, int(100*scores[i]))\n        box_to_display_str_map[box].append(display_str)\n        if agnostic_mode:\n          box_to_color_map[box] = \'DarkOrange\'\n        else:\n          box_to_color_map[box] = STANDARD_COLORS[\n              classes[i] % len(STANDARD_COLORS)]\n\n  # Draw all boxes onto image.\n  for box, color in box_to_color_map.items():\n    ymin, xmin, ymax, xmax = box\n    if instance_masks is not None:\n      draw_mask_on_image_array(\n          image,\n          box_to_instance_masks_map[box],\n          color=color\n      )\n    if instance_boundaries is not None:\n      draw_mask_on_image_array(\n          image,\n          box_to_instance_boundaries_map[box],\n          color=\'red\',\n          alpha=1.0\n      )\n    draw_bounding_box_on_image_array(\n        image,\n        ymin,\n        xmin,\n        ymax,\n        xmax,\n        color=color,\n        thickness=line_thickness,\n        display_str_list=box_to_display_str_map[box],\n        use_normalized_coordinates=use_normalized_coordinates)\n    if keypoints is not None:\n      draw_keypoints_on_image_array(\n          image,\n          box_to_keypoints_map[box],\n          color=color,\n          radius=line_thickness / 2,\n          use_normalized_coordinates=use_normalized_coordinates)\n\n  return image\n\n\ndef add_cdf_image_summary(values, name):\n  """"""Adds a tf.summary.image for a CDF plot of the values.\n\n  Normalizes `values` such that they sum to 1, plots the cumulative distribution\n  function and creates a tf image summary.\n\n  Args:\n    values: a 1-D float32 tensor containing the values.\n    name: name for the image summary.\n  """"""\n  def cdf_plot(values):\n    """"""Numpy function to plot CDF.""""""\n    normalized_values = values / np.sum(values)\n    sorted_values = np.sort(normalized_values)\n    cumulative_values = np.cumsum(sorted_values)\n    fraction_of_examples = (np.arange(cumulative_values.size, dtype=np.float32)\n                            / cumulative_values.size)\n    fig = plt.figure(frameon=False)\n    ax = fig.add_subplot(\'111\')\n    ax.plot(fraction_of_examples, cumulative_values)\n    ax.set_ylabel(\'cumulative normalized values\')\n    ax.set_xlabel(\'fraction of examples\')\n    fig.canvas.draw()\n    width, height = fig.get_size_inches() * fig.get_dpi()\n    image = np.fromstring(fig.canvas.tostring_rgb(), dtype=\'uint8\').reshape(\n        1, int(height), int(width), 3)\n    return image\n  cdf_plot = tf.py_func(cdf_plot, [values], tf.uint8)\n  tf.summary.image(name, cdf_plot)\n'"
src/object_detection/utils/visualization_utils_test.py,9,"b'# Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n""""""Tests for object_detection.utils.visualization_utils.""""""\nimport logging\nimport os\n\nimport numpy as np\nimport PIL.Image as Image\nimport tensorflow as tf\n\nfrom object_detection.utils import visualization_utils\n\n_TESTDATA_PATH = \'object_detection/test_images\'\n\n\nclass VisualizationUtilsTest(tf.test.TestCase):\n\n  def create_colorful_test_image(self):\n    """"""This function creates an image that can be used to test vis functions.\n\n    It makes an image composed of four colored rectangles.\n\n    Returns:\n      colorful test numpy array image.\n    """"""\n    ch255 = np.full([100, 200, 1], 255, dtype=np.uint8)\n    ch128 = np.full([100, 200, 1], 128, dtype=np.uint8)\n    ch0 = np.full([100, 200, 1], 0, dtype=np.uint8)\n    imr = np.concatenate((ch255, ch128, ch128), axis=2)\n    img = np.concatenate((ch255, ch255, ch0), axis=2)\n    imb = np.concatenate((ch255, ch0, ch255), axis=2)\n    imw = np.concatenate((ch128, ch128, ch128), axis=2)\n    imu = np.concatenate((imr, img), axis=1)\n    imd = np.concatenate((imb, imw), axis=1)\n    image = np.concatenate((imu, imd), axis=0)\n    return image\n\n  def test_draw_bounding_box_on_image(self):\n    test_image = self.create_colorful_test_image()\n    test_image = Image.fromarray(test_image)\n    width_original, height_original = test_image.size\n    ymin = 0.25\n    ymax = 0.75\n    xmin = 0.4\n    xmax = 0.6\n\n    visualization_utils.draw_bounding_box_on_image(test_image, ymin, xmin, ymax,\n                                                   xmax)\n    width_final, height_final = test_image.size\n\n    self.assertEqual(width_original, width_final)\n    self.assertEqual(height_original, height_final)\n\n  def test_draw_bounding_box_on_image_array(self):\n    test_image = self.create_colorful_test_image()\n    width_original = test_image.shape[0]\n    height_original = test_image.shape[1]\n    ymin = 0.25\n    ymax = 0.75\n    xmin = 0.4\n    xmax = 0.6\n\n    visualization_utils.draw_bounding_box_on_image_array(\n        test_image, ymin, xmin, ymax, xmax)\n    width_final = test_image.shape[0]\n    height_final = test_image.shape[1]\n\n    self.assertEqual(width_original, width_final)\n    self.assertEqual(height_original, height_final)\n\n  def test_draw_bounding_boxes_on_image(self):\n    test_image = self.create_colorful_test_image()\n    test_image = Image.fromarray(test_image)\n    width_original, height_original = test_image.size\n    boxes = np.array([[0.25, 0.75, 0.4, 0.6],\n                      [0.1, 0.1, 0.9, 0.9]])\n\n    visualization_utils.draw_bounding_boxes_on_image(test_image, boxes)\n    width_final, height_final = test_image.size\n\n    self.assertEqual(width_original, width_final)\n    self.assertEqual(height_original, height_final)\n\n  def test_draw_bounding_boxes_on_image_array(self):\n    test_image = self.create_colorful_test_image()\n    width_original = test_image.shape[0]\n    height_original = test_image.shape[1]\n    boxes = np.array([[0.25, 0.75, 0.4, 0.6],\n                      [0.1, 0.1, 0.9, 0.9]])\n\n    visualization_utils.draw_bounding_boxes_on_image_array(test_image, boxes)\n    width_final = test_image.shape[0]\n    height_final = test_image.shape[1]\n\n    self.assertEqual(width_original, width_final)\n    self.assertEqual(height_original, height_final)\n\n  def test_draw_bounding_boxes_on_image_tensors(self):\n    """"""Tests that bounding box utility produces reasonable results.""""""\n    category_index = {1: {\'id\': 1, \'name\': \'dog\'}, 2: {\'id\': 2, \'name\': \'cat\'}}\n\n    fname = os.path.join(_TESTDATA_PATH, \'image1.jpg\')\n    image_np = np.array(Image.open(fname))\n    images_np = np.stack((image_np, image_np), axis=0)\n\n    with tf.Graph().as_default():\n      images_tensor = tf.constant(value=images_np, dtype=tf.uint8)\n      boxes = tf.constant([[[0.4, 0.25, 0.75, 0.75], [0.5, 0.3, 0.6, 0.9]],\n                           [[0.25, 0.25, 0.75, 0.75], [0.1, 0.3, 0.6, 1.0]]])\n      classes = tf.constant([[1, 1], [1, 2]], dtype=tf.int64)\n      scores = tf.constant([[0.8, 0.1], [0.6, 0.5]])\n      images_with_boxes = (\n          visualization_utils.draw_bounding_boxes_on_image_tensors(\n              images_tensor,\n              boxes,\n              classes,\n              scores,\n              category_index,\n              min_score_thresh=0.2))\n\n      with self.test_session() as sess:\n        sess.run(tf.global_variables_initializer())\n\n        # Write output images for visualization.\n        images_with_boxes_np = sess.run(images_with_boxes)\n        self.assertEqual(images_np.shape, images_with_boxes_np.shape)\n        for i in range(images_with_boxes_np.shape[0]):\n          img_name = \'image_\' + str(i) + \'.png\'\n          output_file = os.path.join(self.get_temp_dir(), img_name)\n          logging.info(\'Writing output image %d to %s\', i, output_file)\n          image_pil = Image.fromarray(images_with_boxes_np[i, ...])\n          image_pil.save(output_file)\n\n  def test_draw_keypoints_on_image(self):\n    test_image = self.create_colorful_test_image()\n    test_image = Image.fromarray(test_image)\n    width_original, height_original = test_image.size\n    keypoints = [[0.25, 0.75], [0.4, 0.6], [0.1, 0.1], [0.9, 0.9]]\n\n    visualization_utils.draw_keypoints_on_image(test_image, keypoints)\n    width_final, height_final = test_image.size\n\n    self.assertEqual(width_original, width_final)\n    self.assertEqual(height_original, height_final)\n\n  def test_draw_keypoints_on_image_array(self):\n    test_image = self.create_colorful_test_image()\n    width_original = test_image.shape[0]\n    height_original = test_image.shape[1]\n    keypoints = [[0.25, 0.75], [0.4, 0.6], [0.1, 0.1], [0.9, 0.9]]\n\n    visualization_utils.draw_keypoints_on_image_array(test_image, keypoints)\n    width_final = test_image.shape[0]\n    height_final = test_image.shape[1]\n\n    self.assertEqual(width_original, width_final)\n    self.assertEqual(height_original, height_final)\n\n  def test_draw_mask_on_image_array(self):\n    test_image = np.asarray([[[0, 0, 0], [0, 0, 0]],\n                             [[0, 0, 0], [0, 0, 0]]], dtype=np.uint8)\n    mask = np.asarray([[0, 1],\n                       [1, 1]], dtype=np.uint8)\n    expected_result = np.asarray([[[0, 0, 0], [0, 0, 127]],\n                                  [[0, 0, 127], [0, 0, 127]]], dtype=np.uint8)\n    visualization_utils.draw_mask_on_image_array(test_image, mask,\n                                                 color=\'Blue\', alpha=.5)\n    self.assertAllEqual(test_image, expected_result)\n\n  def test_add_cdf_image_summary(self):\n    values = [0.1, 0.2, 0.3, 0.4, 0.42, 0.44, 0.46, 0.48, 0.50]\n    visualization_utils.add_cdf_image_summary(values, \'PositiveAnchorLoss\')\n    cdf_image_summary = tf.get_collection(key=tf.GraphKeys.SUMMARIES)[0]\n    with self.test_session():\n      cdf_image_summary.eval()\n\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
