file_path,api_count,code
constants.py,0,"b""import os\n\n# Define the musical styles\ngenre = [\n    'baroque',\n    'classical',\n    'romantic'\n]\n\nstyles = [\n    [\n        'data/baroque/bach',\n        'data/baroque/handel',\n        'data/baroque/pachelbel'\n    ],\n    [\n        'data/classical/burgmueller',\n        'data/classical/clementi',\n        'data/classical/haydn',\n        'data/classical/beethoven',\n        'data/classical/brahms',\n        'data/classical/mozart'\n    ],\n    [\n        'data/romantic/balakirew',\n        'data/romantic/borodin',\n        'data/romantic/brahms',\n        'data/romantic/chopin',\n        'data/romantic/debussy',\n        'data/romantic/liszt',\n        'data/romantic/mendelssohn',\n        'data/romantic/moszkowski',\n        'data/romantic/mussorgsky',\n        'data/romantic/rachmaninov',\n        'data/romantic/schubert',\n        'data/romantic/schumann',\n        'data/romantic/tchaikovsky',\n        'data/romantic/tschai'\n    ]\n]\n\nNUM_STYLES = sum(len(s) for s in styles)\n\n# MIDI Resolution\nDEFAULT_RES = 96\nMIDI_MAX_NOTES = 128\nMAX_VELOCITY = 127\n\n# Number of octaves supported\nNUM_OCTAVES = 4\nOCTAVE = 12\n\n# Min and max note (in MIDI note number)\nMIN_NOTE = 36\nMAX_NOTE = MIN_NOTE + NUM_OCTAVES * OCTAVE\nNUM_NOTES = MAX_NOTE - MIN_NOTE\n\n# Number of beats in a bar\nBEATS_PER_BAR = 4\n# Notes per quarter note\nNOTES_PER_BEAT = 4\n# The quickest note is a half-note\nNOTES_PER_BAR = NOTES_PER_BEAT * BEATS_PER_BAR\n\n# Training parameters\nBATCH_SIZE = 16\nSEQ_LEN = 8 * NOTES_PER_BAR\n\n# Hyper Parameters\nOCTAVE_UNITS = 64\nSTYLE_UNITS = 64\nNOTE_UNITS = 3\nTIME_AXIS_UNITS = 256\nNOTE_AXIS_UNITS = 128\n\nTIME_AXIS_LAYERS = 2\nNOTE_AXIS_LAYERS = 2\n\n# Move file save location\nOUT_DIR = 'out'\nMODEL_DIR = os.path.join(OUT_DIR, 'models')\nMODEL_FILE = os.path.join(OUT_DIR, 'model.h5')\nSAMPLES_DIR = os.path.join(OUT_DIR, 'samples')\nCACHE_DIR = os.path.join(OUT_DIR, 'cache')\n"""
dataset.py,0,"b'""""""\nPreprocesses MIDI files\n""""""\nimport numpy as np\nimport math\nimport random\nfrom joblib import Parallel, delayed\nimport multiprocessing\n\nfrom constants import *\nfrom midi_util import load_midi\nfrom util import *\n\ndef compute_beat(beat, notes_in_bar):\n    return one_hot(beat % notes_in_bar, notes_in_bar)\n\ndef compute_completion(beat, len_melody):\n    return np.array([beat / len_melody])\n\ndef compute_genre(genre_id):\n    """""" Computes a vector that represents a particular genre """"""\n    genre_hot = np.zeros((NUM_STYLES,))\n    start_index = sum(len(s) for i, s in enumerate(styles) if i < genre_id)\n    styles_in_genre = len(styles[genre_id])\n    genre_hot[start_index:start_index + styles_in_genre] = 1 / styles_in_genre\n    return genre_hot\n\ndef stagger(data, time_steps):\n    dataX, dataY = [], []\n    # Buffer training for first event\n    data = ([np.zeros_like(data[0])] * time_steps) + list(data)\n\n    # Chop a sequence into measures\n    for i in range(0, len(data) - time_steps, NOTES_PER_BAR):\n        dataX.append(data[i:i + time_steps])\n        dataY.append(data[i + 1:(i + time_steps + 1)])\n    return dataX, dataY\n\ndef load_all(styles, batch_size, time_steps):\n    """"""\n    Loads all MIDI files as a piano roll.\n    (For Keras)\n    """"""\n    note_data = []\n    beat_data = []\n    style_data = []\n\n    note_target = []\n\n    # TODO: Can speed this up with better parallel loading. Order gaurentee.\n    styles = [y for x in styles for y in x]\n\n    for style_id, style in enumerate(styles):\n        style_hot = one_hot(style_id, NUM_STYLES)\n        # Parallel process all files into a list of music sequences\n        seqs = Parallel(n_jobs=multiprocessing.cpu_count(), backend=\'threading\')(delayed(load_midi)(f) for f in get_all_files([style]))\n\n        for seq in seqs:\n            if len(seq) >= time_steps:\n                # Clamp MIDI to note range\n                seq = clamp_midi(seq)\n                # Create training data and labels\n                train_data, label_data = stagger(seq, time_steps)\n                note_data += train_data\n                note_target += label_data\n\n                beats = [compute_beat(i, NOTES_PER_BAR) for i in range(len(seq))]\n                beat_data += stagger(beats, time_steps)[0]\n\n                style_data += stagger([style_hot for i in range(len(seq))], time_steps)[0]\n\n    note_data = np.array(note_data)\n    beat_data = np.array(beat_data)\n    style_data = np.array(style_data)\n    note_target = np.array(note_target)\n    return [note_data, note_target, beat_data, style_data], [note_target]\n\ndef clamp_midi(sequence):\n    """"""\n    Clamps the midi base on the MIN and MAX notes\n    """"""\n    return sequence[:, MIN_NOTE:MAX_NOTE, :]\n\ndef unclamp_midi(sequence):\n    """"""\n    Restore clamped MIDI sequence back to MIDI note values\n    """"""\n    return np.pad(sequence, ((0, 0), (MIN_NOTE, 0), (0, 0)), \'constant\')\n'"
distribution.py,0,"b'import matplotlib.pyplot as plt\nimport numpy as np\nimport sys\nimport dataset\nimport ntpath\n\nfrom music import autocorrelate, NUM_CLASSES, MIN_CLASS, NOTES_PER_BEAT, NOTE_OFF, NO_EVENT, MIN_NOTE\n\nMIDI_NOTE_RANGE = [\'C\', \'C#\', \'D\', \'D#\', \'E\', \'F\', \'F#\', \'G\', \'G#\', \'A\', \'A#\', \'B\'] * 4 + [\'C\']\nNOTE_LEN_RANGE = [\'0\', \'\'] + MIDI_NOTE_RANGE\n\ndef plot_note_distribution(melody_list):\n    for i, (name, melody) in enumerate(melody_list):\n        fig = plt.figure(figsize=(14, 5))\n        # Filter out 0\'s and 1\'s\n        # Subtract min class from each note to 0 index the whole list\n        notes = [x - MIN_CLASS for x in melody if x != 0 and x != 1]\n        # Plot\n        plt.hist(notes, bins=np.arange(len(MIDI_NOTE_RANGE) + 1))\n        plt.ylabel(\'Note frequency\')\n        plt.xticks(range(len(MIDI_NOTE_RANGE)), MIDI_NOTE_RANGE)\n        # plt.show()\n        plt.savefig(\'out/\' + ntpath.basename(name) + \' (note dist).png\')\n\ndef plot_note_length(melody_list):\n    for i, (name, melody) in enumerate(melody_list):\n        # Dict that stores notes and their lengths\n        note_len_dict = {}\n        # Initialize keys/values in dict\n        for i in range(len(NOTE_LEN_RANGE)):\n            note_len_dict[i] = 0\n\n        prev_note = 0\n        for m in melody:\n            # Note off\n            if m == 0:\n                note_len_dict[0] += 1\n            # No event\n            elif m == 1:\n                note_len_dict[prev_note] += 1\n            # Note\n            else:\n                note_len_dict[m] += 1\n                prev_note = m\n        # Convert dict into a list that can be put into histogram\n        note_lens = []\n        for k in note_len_dict.keys():\n            for i in range(note_len_dict[k]):\n                note_lens.append(k)\n\n        # Plot\n        fig = plt.figure(figsize=(14, 5))\n        plt.hist(note_lens, bins=np.arange(len(NOTE_LEN_RANGE) + 1))\n        plt.xlabel(\'0 represents a rest\')\n        plt.ylabel(\'Duration in eigth notes\')\n        plt.xticks(range(len(NOTE_LEN_RANGE)), NOTE_LEN_RANGE)\n        # plt.show()\n        plt.savefig(\'out/\' + ntpath.basename(name) + \' (note length).png\')\n\ndef calculate_correlation(melody_list):\n    correlations = []\n    for name, melody in melody_list:\n        correlation =  np.sum([autocorrelate(melody, i) ** 2 for i in range(1, 4)])\n        correlations.append(correlation)\n        print(\'Correlation Coefficient (r^2 for 1, 2, 3): \', name, correlation)\n\n    print(\'Mean: \', np.mean(correlations))\n    print(\'Std: \', np.std(correlations))\n\ndef distributions(paths):\n    melody_list = dataset.load_melodies(paths, shuffle=False, named=True)\n    plot_note_distribution(melody_list)\n    plot_note_length(melody_list)\n    calculate_correlation(melody_list)\n\ndistributions(sys.argv)\n\n""""""\nNOTES:\n2 maps to midi note 36 (MIN_NOTE)\n8 numbers in arr forms a bar\n2 elements in arr are quarter note\n1 element is a half a quarter note, or an eigth note\noutput a png of plot with plotpy save\n""""""\n'"
generate.py,0,"b'import numpy as np\nimport tensorflow as tf\nfrom collections import deque\nimport midi\nimport argparse\n\nfrom constants import *\nfrom util import *\nfrom dataset import *\nfrom tqdm import tqdm\nfrom midi_util import midi_encode\n\nclass MusicGeneration:\n    """"""\n    Represents a music generation\n    """"""\n    def __init__(self, style, default_temp=1):\n        self.notes_memory = deque([np.zeros((NUM_NOTES, NOTE_UNITS)) for _ in range(SEQ_LEN)], maxlen=SEQ_LEN)\n        self.beat_memory = deque([np.zeros(NOTES_PER_BAR) for _ in range(SEQ_LEN)], maxlen=SEQ_LEN)\n        self.style_memory = deque([style for _ in range(SEQ_LEN)], maxlen=SEQ_LEN)\n\n        # The next note being built\n        self.next_note = np.zeros((NUM_NOTES, NOTE_UNITS))\n        self.silent_time = NOTES_PER_BAR\n\n        # The outputs\n        self.results = []\n        # The temperature\n        self.default_temp = default_temp\n        self.temperature = default_temp\n\n    def build_time_inputs(self):\n        return (\n            np.array(self.notes_memory),\n            np.array(self.beat_memory),\n            np.array(self.style_memory)\n        )\n\n    def build_note_inputs(self, note_features):\n        # Timesteps = 1 (No temporal dimension)\n        return (\n            np.array(note_features),\n            np.array([self.next_note]),\n            np.array(list(self.style_memory)[-1:])\n        )\n\n    def choose(self, prob, n):\n        vol = prob[n, -1]\n        prob = apply_temperature(prob[n, :-1], self.temperature)\n\n        # Flip notes randomly\n        if np.random.random() <= prob[0]:\n            self.next_note[n, 0] = 1\n            # Apply volume\n            self.next_note[n, 2] = vol\n            # Flip articulation\n            if np.random.random() <= prob[1]:\n                self.next_note[n, 1] = 1\n\n    def end_time(self, t):\n        """"""\n        Finish generation for this time step.\n        """"""\n        # Increase temperature while silent.\n        if np.count_nonzero(self.next_note) == 0:\n            self.silent_time += 1\n            if self.silent_time >= NOTES_PER_BAR:\n                self.temperature += 0.1\n        else:\n            self.silent_time = 0\n            self.temperature = self.default_temp\n\n        self.notes_memory.append(self.next_note)\n        # Consistent with dataset representation\n        self.beat_memory.append(compute_beat(t, NOTES_PER_BAR))\n        self.results.append(self.next_note)\n        # Reset next note\n        self.next_note = np.zeros((NUM_NOTES, NOTE_UNITS))\n        return self.results[-1]\n\ndef apply_temperature(prob, temperature):\n    """"""\n    Applies temperature to a sigmoid vector.\n    """"""\n    # Apply temperature\n    if temperature != 1:\n        # Inverse sigmoid\n        x = -np.log(1 / prob - 1)\n        # Apply temperature to sigmoid function\n        prob = 1 / (1 + np.exp(-x / temperature))\n    return prob\n\ndef process_inputs(ins):\n    ins = list(zip(*ins))\n    ins = [np.array(i) for i in ins]\n    return ins\n\ndef generate(models, num_bars, styles):\n    print(\'Generating with styles:\', styles)\n\n    _, time_model, note_model = models\n    generations = [MusicGeneration(style) for style in styles]\n\n    for t in tqdm(range(NOTES_PER_BAR * num_bars)):\n        # Produce note-invariant features\n        ins = process_inputs([g.build_time_inputs() for g in generations])\n        # Pick only the last time step\n        note_features = time_model.predict(ins)\n        note_features = np.array(note_features)[:, -1:, :]\n\n        # Generate each note conditioned on previous\n        for n in range(NUM_NOTES):\n            ins = process_inputs([g.build_note_inputs(note_features[i, :, :, :]) for i, g in enumerate(generations)])\n            predictions = np.array(note_model.predict(ins))\n\n            for i, g in enumerate(generations):\n                # Remove the temporal dimension\n                g.choose(predictions[i][-1], n)\n\n        # Move one time step\n        yield [g.end_time(t) for g in generations]\n\ndef write_file(name, results):\n    """"""\n    Takes a list of all notes generated per track and writes it to file\n    """"""\n    results = zip(*list(results))\n\n    for i, result in enumerate(results):\n        fpath = os.path.join(SAMPLES_DIR, name + \'_\' + str(i) + \'.mid\')\n        print(\'Writing file\', fpath)\n        os.makedirs(os.path.dirname(fpath), exist_ok=True)\n        mf = midi_encode(unclamp_midi(result))\n        midi.write_midifile(fpath, mf)\n\ndef main():\n    parser = argparse.ArgumentParser(description=\'Generates music.\')\n    parser.add_argument(\'--bars\', default=32, type=int, help=\'Number of bars to generate\')\n    parser.add_argument(\'--styles\', default=None, type=int, nargs=\'+\', help=\'Styles to mix together\')\n    args = parser.parse_args()\n\n    models = build_or_load()\n\n    styles = [compute_genre(i) for i in range(len(genre))]\n\n    if args.styles:\n        # Custom style\n        styles = [np.mean([one_hot(i, NUM_STYLES) for i in args.styles], axis=0)]\n\n    write_file(\'output\', generate(models, args.bars, styles))\n\nif __name__ == \'__main__\':\n    main()\n'"
midi_util.py,0,"b'""""""\nHandles MIDI file loading\n""""""\nimport midi\nimport numpy as np\nimport os\nfrom constants import *\n\ndef midi_encode(note_seq, resolution=NOTES_PER_BEAT, step=1):\n    """"""\n    Takes a piano roll and encodes it into MIDI pattern\n    """"""\n    # Instantiate a MIDI Pattern (contains a list of tracks)\n    pattern = midi.Pattern()\n    pattern.resolution = resolution\n    # Instantiate a MIDI Track (contains a list of MIDI events)\n    track = midi.Track()\n    # Append the track to the pattern\n    pattern.append(track)\n\n    play = note_seq[:, :, 0]\n    replay = note_seq[:, :, 1]\n    volume = note_seq[:, :, 2]\n\n    # The current pattern being played\n    current = np.zeros_like(play[0])\n    # Absolute tick of last event\n    last_event_tick = 0\n    # Amount of NOOP ticks\n    noop_ticks = 0\n\n    for tick, data in enumerate(play):\n        data = np.array(data)\n\n        if not np.array_equal(current, data):# or np.any(replay[tick]):\n            noop_ticks = 0\n\n            for index, next_volume in np.ndenumerate(data):\n                if next_volume > 0 and current[index] == 0:\n                    # Was off, but now turned on\n                    evt = midi.NoteOnEvent(\n                        tick=(tick - last_event_tick) * step,\n                        velocity=int(volume[tick][index[0]] * MAX_VELOCITY),\n                        pitch=index[0]\n                    )\n                    track.append(evt)\n                    last_event_tick = tick\n                elif current[index] > 0 and next_volume == 0:\n                    # Was on, but now turned off\n                    evt = midi.NoteOffEvent(\n                        tick=(tick - last_event_tick) * step,\n                        pitch=index[0]\n                    )\n                    track.append(evt)\n                    last_event_tick = tick\n\n                elif current[index] > 0 and next_volume > 0 and replay[tick][index[0]] > 0:\n                    # Handle replay\n                    evt_off = midi.NoteOffEvent(\n                        tick=(tick- last_event_tick) * step,\n                        pitch=index[0]\n                    )\n                    track.append(evt_off)\n                    evt_on = midi.NoteOnEvent(\n                        tick=0,\n                        velocity=int(volume[tick][index[0]] * MAX_VELOCITY),\n                        pitch=index[0]\n                    )\n                    track.append(evt_on)\n                    last_event_tick = tick\n\n        else:\n            noop_ticks += 1\n\n        current = data\n\n    tick += 1\n\n    # Turn off all remaining on notes\n    for index, vol in np.ndenumerate(current):\n        if vol > 0:\n            # Was on, but now turned off\n            evt = midi.NoteOffEvent(\n                tick=(tick - last_event_tick) * step,\n                pitch=index[0]\n            )\n            track.append(evt)\n            last_event_tick = tick\n            noop_ticks = 0\n\n    # Add the end of track event, append it to the track\n    eot = midi.EndOfTrackEvent(tick=noop_ticks)\n    track.append(eot)\n\n    return pattern\n\ndef midi_decode(pattern,\n                classes=MIDI_MAX_NOTES,\n                step=None):\n    """"""\n    Takes a MIDI pattern and decodes it into a piano roll.\n    """"""\n    if step is None:\n        step = pattern.resolution // NOTES_PER_BEAT\n\n    # Extract all tracks at highest resolution\n    merged_replay = None\n    merged_volume = None\n\n    for track in pattern:\n        # The downsampled sequences\n        replay_sequence = []\n        volume_sequence = []\n\n        # Raw sequences\n        replay_buffer = [np.zeros((classes,))]\n        volume_buffer = [np.zeros((classes,))]\n\n        for i, event in enumerate(track):\n            # Duplicate the last note pattern to wait for next event\n            for _ in range(event.tick):\n                replay_buffer.append(np.zeros(classes))\n                volume_buffer.append(np.copy(volume_buffer[-1]))\n\n                # Buffer & downscale sequence\n                if len(volume_buffer) > step:\n                    # Take the min\n                    replay_any = np.minimum(np.sum(replay_buffer[:-1], axis=0), 1)\n                    replay_sequence.append(replay_any)\n\n                    # Determine volume by max\n                    volume_sum = np.amax(volume_buffer[:-1], axis=0)\n                    volume_sequence.append(volume_sum)\n\n                    # Keep the last one (discard things in the middle)\n                    replay_buffer = replay_buffer[-1:]\n                    volume_buffer = volume_buffer[-1:]\n\n            if isinstance(event, midi.EndOfTrackEvent):\n                break\n\n            # Modify the last note pattern\n            if isinstance(event, midi.NoteOnEvent):\n                pitch, velocity = event.data\n                volume_buffer[-1][pitch] = velocity / MAX_VELOCITY\n\n                # Check for replay_buffer, which is true if the current note was previously played and needs to be replayed\n                if len(volume_buffer) > 1 and volume_buffer[-2][pitch] > 0 and volume_buffer[-1][pitch] > 0:\n                    replay_buffer[-1][pitch] = 1\n                    # Override current volume with previous volume\n                    volume_buffer[-1][pitch] = volume_buffer[-2][pitch]\n\n            if isinstance(event, midi.NoteOffEvent):\n                pitch, velocity = event.data\n                volume_buffer[-1][pitch] = 0\n\n        # Add the remaining\n        replay_any = np.minimum(np.sum(replay_buffer, axis=0), 1)\n        replay_sequence.append(replay_any)\n        volume_sequence.append(volume_buffer[0])\n\n        replay_sequence = np.array(replay_sequence)\n        volume_sequence = np.array(volume_sequence)\n        assert len(volume_sequence) == len(replay_sequence)\n\n        if merged_volume is None:\n            merged_replay = replay_sequence\n            merged_volume = volume_sequence\n        else:\n            # Merge into a single track, padding with zeros of needed\n            if len(volume_sequence) > len(merged_volume):\n                # Swap variables such that merged_notes is always at least\n                # as large as play_sequence\n                tmp = replay_sequence\n                replay_sequence = merged_replay\n                merged_replay = tmp\n\n                tmp = volume_sequence\n                volume_sequence = merged_volume\n                merged_volume = tmp\n\n            assert len(merged_volume) >= len(volume_sequence)\n\n            diff = len(merged_volume) - len(volume_sequence)\n            merged_replay += np.pad(replay_sequence, ((0, diff), (0, 0)), \'constant\')\n            merged_volume += np.pad(volume_sequence, ((0, diff), (0, 0)), \'constant\')\n\n    merged = np.stack([np.ceil(merged_volume), merged_replay, merged_volume], axis=2)\n    # Prevent stacking duplicate notes to exceed one.\n    merged = np.minimum(merged, 1)\n    return merged\n\ndef load_midi(fname):\n    p = midi.read_midifile(fname)\n    cache_path = os.path.join(CACHE_DIR, fname + \'.npy\')\n    try:\n        note_seq = np.load(cache_path)\n    except Exception as e:\n        # Perform caching\n        os.makedirs(os.path.dirname(cache_path), exist_ok=True)\n\n        note_seq = midi_decode(p)\n        np.save(cache_path, note_seq)\n\n    assert len(note_seq.shape) == 3, note_seq.shape\n    assert note_seq.shape[1] == MIDI_MAX_NOTES, note_seq.shape\n    assert note_seq.shape[2] == 3, note_seq.shape\n    assert (note_seq >= 0).all()\n    assert (note_seq <= 1).all()\n    return note_seq\n\nif __name__ == \'__main__\':\n    # Test\n    # p = midi.read_midifile(""out/test_in.mid"")\n    p = midi.read_midifile(""out/test_in.mid"")\n    p = midi_encode(midi_decode(p))\n    midi.write_midifile(""out/test_out.mid"", p)\n'"
model.py,12,"b'import numpy as np\nimport tensorflow as tf\nfrom keras.layers import Input, LSTM, Dense, Dropout, Lambda, Reshape, Permute\nfrom keras.layers import TimeDistributed, RepeatVector, Conv1D, Activation\nfrom keras.layers import Embedding, Flatten\nfrom keras.layers.merge import Concatenate, Add\nfrom keras.models import Model\nimport keras.backend as K\nfrom keras import losses\n\nfrom util import *\nfrom constants import *\n\ndef primary_loss(y_true, y_pred):\n    # 3 separate loss calculations based on if note is played or not\n    played = y_true[:, :, :, 0]\n    bce_note = losses.binary_crossentropy(y_true[:, :, :, 0], y_pred[:, :, :, 0])\n    bce_replay = losses.binary_crossentropy(y_true[:, :, :, 1], tf.multiply(played, y_pred[:, :, :, 1]) + tf.multiply(1 - played, y_true[:, :, :, 1]))\n    mse = losses.mean_squared_error(y_true[:, :, :, 2], tf.multiply(played, y_pred[:, :, :, 2]) + tf.multiply(1 - played, y_true[:, :, :, 2]))\n    return bce_note + bce_replay + mse\n\ndef pitch_pos_in_f(time_steps):\n    """"""\n    Returns a constant containing pitch position of each note\n    """"""\n    def f(x):\n        note_ranges = tf.range(NUM_NOTES, dtype=\'float32\') / NUM_NOTES\n        repeated_ranges = tf.tile(note_ranges, [tf.shape(x)[0] * time_steps])\n        return tf.reshape(repeated_ranges, [tf.shape(x)[0], time_steps, NUM_NOTES, 1])\n    return f\n\ndef pitch_class_in_f(time_steps):\n    """"""\n    Returns a constant containing pitch class of each note\n    """"""\n    def f(x):\n        pitch_class_matrix = np.array([one_hot(n % OCTAVE, OCTAVE) for n in range(NUM_NOTES)])\n        pitch_class_matrix = tf.constant(pitch_class_matrix, dtype=\'float32\')\n        pitch_class_matrix = tf.reshape(pitch_class_matrix, [1, 1, NUM_NOTES, OCTAVE])\n        return tf.tile(pitch_class_matrix, [tf.shape(x)[0], time_steps, 1, 1])\n    return f\n\ndef pitch_bins_f(time_steps):\n    def f(x):\n        bins = tf.reduce_sum([x[:, :, i::OCTAVE, 0] for i in range(OCTAVE)], axis=3)\n        bins = tf.tile(bins, [NUM_OCTAVES, 1, 1])\n        bins = tf.reshape(bins, [tf.shape(x)[0], time_steps, NUM_NOTES, 1])\n        return bins\n    return f\n\ndef time_axis(dropout):\n    def f(notes, beat, style):\n        time_steps = int(notes.get_shape()[1])\n\n        # TODO: Experiment with when to apply conv\n        note_octave = TimeDistributed(Conv1D(OCTAVE_UNITS, 2 * OCTAVE, padding=\'same\'))(notes)\n        note_octave = Activation(\'tanh\')(note_octave)\n        note_octave = Dropout(dropout)(note_octave)\n\n        # Create features for every single note.\n        note_features = Concatenate()([\n            Lambda(pitch_pos_in_f(time_steps))(notes),\n            Lambda(pitch_class_in_f(time_steps))(notes),\n            Lambda(pitch_bins_f(time_steps))(notes),\n            note_octave,\n            TimeDistributed(RepeatVector(NUM_NOTES))(beat)\n        ])\n\n        x = note_features\n\n        # [batch, notes, time, features]\n        x = Permute((2, 1, 3))(x)\n\n        # Apply LSTMs\n        for l in range(TIME_AXIS_LAYERS):\n            # Integrate style\n            style_proj = Dense(int(x.get_shape()[3]))(style)\n            style_proj = TimeDistributed(RepeatVector(NUM_NOTES))(style_proj)\n            style_proj = Activation(\'tanh\')(style_proj)\n            style_proj = Dropout(dropout)(style_proj)\n            style_proj = Permute((2, 1, 3))(style_proj)\n            x = Add()([x, style_proj])\n\n            x = TimeDistributed(LSTM(TIME_AXIS_UNITS, return_sequences=True))(x)\n            x = Dropout(dropout)(x)\n\n        # [batch, time, notes, features]\n        return Permute((2, 1, 3))(x)\n    return f\n\ndef note_axis(dropout):\n    dense_layer_cache = {}\n    lstm_layer_cache = {}\n    note_dense = Dense(2, activation=\'sigmoid\', name=\'note_dense\')\n    volume_dense = Dense(1, name=\'volume_dense\')\n\n    def f(x, chosen, style):\n        time_steps = int(x.get_shape()[1])\n\n        # Shift target one note to the left.\n        shift_chosen = Lambda(lambda x: tf.pad(x[:, :, :-1, :], [[0, 0], [0, 0], [1, 0], [0, 0]]))(chosen)\n\n        # [batch, time, notes, 1]\n        shift_chosen = Reshape((time_steps, NUM_NOTES, -1))(shift_chosen)\n        # [batch, time, notes, features + 1]\n        x = Concatenate(axis=3)([x, shift_chosen])\n\n        for l in range(NOTE_AXIS_LAYERS):\n            # Integrate style\n            if l not in dense_layer_cache:\n                dense_layer_cache[l] = Dense(int(x.get_shape()[3]))\n\n            style_proj = dense_layer_cache[l](style)\n            style_proj = TimeDistributed(RepeatVector(NUM_NOTES))(style_proj)\n            style_proj = Activation(\'tanh\')(style_proj)\n            style_proj = Dropout(dropout)(style_proj)\n            x = Add()([x, style_proj])\n\n            if l not in lstm_layer_cache:\n                lstm_layer_cache[l] = LSTM(NOTE_AXIS_UNITS, return_sequences=True)\n\n            x = TimeDistributed(lstm_layer_cache[l])(x)\n            x = Dropout(dropout)(x)\n\n        return Concatenate()([note_dense(x), volume_dense(x)])\n    return f\n\ndef build_models(time_steps=SEQ_LEN, input_dropout=0.2, dropout=0.5):\n    notes_in = Input((time_steps, NUM_NOTES, NOTE_UNITS))\n    beat_in = Input((time_steps, NOTES_PER_BAR))\n    style_in = Input((time_steps, NUM_STYLES))\n    # Target input for conditioning\n    chosen_in = Input((time_steps, NUM_NOTES, NOTE_UNITS))\n\n    # Dropout inputs\n    notes = Dropout(input_dropout)(notes_in)\n    beat = Dropout(input_dropout)(beat_in)\n    chosen = Dropout(input_dropout)(chosen_in)\n\n    # Distributed representations\n    style_l = Dense(STYLE_UNITS, name=\'style\')\n    style = style_l(style_in)\n\n    """""" Time axis """"""\n    time_out = time_axis(dropout)(notes, beat, style)\n\n    """""" Note Axis & Prediction Layer """"""\n    naxis = note_axis(dropout)\n    notes_out = naxis(time_out, chosen, style)\n\n    model = Model([notes_in, chosen_in, beat_in, style_in], [notes_out])\n    model.compile(optimizer=\'nadam\', loss=[primary_loss])\n\n    """""" Generation Models """"""\n    time_model = Model([notes_in, beat_in, style_in], [time_out])\n\n    note_features = Input((1, NUM_NOTES, TIME_AXIS_UNITS), name=\'note_features\')\n    chosen_gen_in = Input((1, NUM_NOTES, NOTE_UNITS), name=\'chosen_gen_in\')\n    style_gen_in = Input((1, NUM_STYLES), name=\'style_in\')\n\n    # Dropout inputs\n    chosen_gen = Dropout(input_dropout)(chosen_gen_in)\n    style_gen = style_l(style_gen_in)\n\n    note_gen_out = naxis(note_features, chosen_gen, style_gen)\n\n    note_model = Model([note_features, chosen_gen_in, style_gen_in], note_gen_out)\n\n    return model, time_model, note_model\n'"
test.py,0,"b'from midi_util import *\nfrom util import *\nimport unittest\n\nclass TestMIDIUtil(unittest.TestCase):\n\n    def test_encode(self):\n        composition = [\n            [0, 1, 0, 0],\n            [0, 1, 0, 0],\n            [0, 1, 0, 1],\n            [0, 1, 0, 1],\n            [0, 0, 0, 1],\n            [0, 0, 0, 0]\n        ]\n\n        replay = [\n            [0, 0, 0, 0],\n            [0, 0, 0, 0],\n            [0, 0, 0, 0],\n            [0, 0, 0, 0],\n            [0, 0, 0, 0],\n            [0, 0, 0, 0]\n        ]\n\n        volume = [\n            [0, 0.5, 0, 0],\n            [0, 0.5, 0, 0],\n            [0, 0.5, 0, 0.5],\n            [0, 0.5, 0, 0.5],\n            [0, 0, 0, 0.5],\n            [0, 0, 0, 0]\n        ]\n\n        pattern = midi_encode(np.stack([composition, replay, volume], 2), step=1)\n        self.assertEqual(pattern.resolution, NOTES_PER_BEAT)\n        self.assertEqual(len(pattern), 1)\n        track = pattern[0]\n        self.assertEqual(len(track), 4 + 1)\n        on1, on2, off1, off2 = track[:-1]\n        self.assertIsInstance(on1, midi.NoteOnEvent)\n        self.assertIsInstance(on2, midi.NoteOnEvent)\n        self.assertIsInstance(off1, midi.NoteOffEvent)\n        self.assertIsInstance(off2, midi.NoteOffEvent)\n\n        self.assertEqual(on1.tick, 0)\n        self.assertEqual(on1.pitch, 1)\n        self.assertEqual(on2.tick, 2)\n        self.assertEqual(on2.pitch, 3)\n        self.assertEqual(off1.tick, 2)\n        self.assertEqual(off1.pitch, 1)\n        self.assertEqual(off2.tick, 1)\n        self.assertEqual(off2.pitch, 3)\n\n    def test_decode(self):\n        # Instantiate a MIDI Pattern (contains a list of tracks)\n        pattern = midi.Pattern(resolution=96)\n        # Instantiate a MIDI Track (contains a list of MIDI events)\n        track = midi.Track()\n        # Append the track to the pattern\n        pattern.append(track)\n\n        track.append(midi.NoteOnEvent(tick=0, velocity=127, pitch=0))\n        track.append(midi.NoteOnEvent(tick=96, velocity=127, pitch=1))\n        track.append(midi.NoteOffEvent(tick=0, velocity=127, pitch=0))\n        track.append(midi.NoteOffEvent(tick=48, velocity=127, pitch=1))\n        track.append(midi.EndOfTrackEvent(tick=1))\n\n        note_sequence = midi_decode(pattern, 4, step=DEFAULT_RES // 2)\n        composition = note_sequence[:, :, 0]\n\n        np.testing.assert_array_equal(composition, [\n            [1, 0, 0, 0],\n            [1, 0, 0, 0],\n            [0, 1, 0, 0],\n            [0, 0, 0, 0]\n        ])\n\n    def test_encode_decode(self):\n        composition = [\n            [0, 1, 0, 0],\n            [0, 1, 0, 0],\n            [0, 1, 0, 1],\n            [0, 1, 0, 1],\n            [0, 0, 0, 1],\n            [0, 0, 0, 0]\n        ]\n\n        replay = [\n            [0, 0, 0, 0],\n            [0, 0, 0, 0],\n            [0, 0, 0, 0],\n            [0, 0, 0, 0],\n            [0, 0, 0, 0],\n            [0, 0, 0, 0]\n        ]\n\n        volume = [\n            [0, 0.5, 0, 0],\n            [0, 0.5, 0, 0],\n            [0, 0.5, 0, 0.5],\n            [0, 0.5, 0, 0.5],\n            [0, 0, 0, 0.5],\n            [0, 0, 0, 0]\n        ]\n\n        note_seq = midi_decode(midi_encode(np.stack([composition, replay, volume], 2), step=1), 4, step=1)\n        np.testing.assert_array_equal(composition, note_seq[:, :, 0])\n\n    def test_replay_decode(self):\n        # Instantiate a MIDI Pattern (contains a list of tracks)\n        pattern = midi.Pattern(resolution=96)\n        # Instantiate a MIDI Track (contains a list of MIDI events)\n        track = midi.Track()\n        # Append the track to the pattern\n        pattern.append(track)\n\n        track.append(midi.NoteOnEvent(tick=0, velocity=127, pitch=1))\n        track.append(midi.NoteOnEvent(tick=0, velocity=127, pitch=3))\n        track.append(midi.NoteOffEvent(tick=1, velocity=127, pitch=1))\n        track.append(midi.NoteOnEvent(tick=2, velocity=127, pitch=1))\n        track.append(midi.NoteOnEvent(tick=2, velocity=127, pitch=3))\n        track.append(midi.EndOfTrackEvent(tick=1))\n\n        note_seq = midi_decode(pattern, 4, step=3)\n\n        np.testing.assert_array_equal(note_seq[:, :, 1], [\n            [0., 0., 0., 0.],\n            [0., 0., 0., 1.],\n            [0., 0., 0., 0.]\n        ])\n\n\n    def test_volume_decode(self):\n        # Instantiate a MIDI Pattern (contains a list of tracks)\n        pattern = midi.Pattern(resolution=96)\n        # Instantiate a MIDI Track (contains a list of MIDI events)\n        track = midi.Track()\n        # Append the track to the pattern\n        pattern.append(track)\n\n        track.append(midi.NoteOnEvent(tick=0, velocity=24, pitch=0))\n        track.append(midi.NoteOnEvent(tick=96, velocity=89, pitch=1))\n        track.append(midi.NoteOffEvent(tick=0, pitch=0))\n        track.append(midi.NoteOffEvent(tick=48, pitch=1))\n        track.append(midi.EndOfTrackEvent(tick=1))\n\n        note_seq = midi_decode(pattern, 4, step=DEFAULT_RES // 2)\n\n        np.testing.assert_array_almost_equal(note_seq[:, :, 2], [\n            [24/127, 0., 0., 0.],\n            [24/127, 0., 0., 0.],\n            [0., 89/127, 0., 0.],\n            [0., 0., 0., 0.]\n        ], decimal=5)\n\n\n    def test_replay_encode_decode(self):\n        # TODO: Fix this test\n        composition = [\n            [0, 1, 0, 1],\n            [0, 0, 0, 1],\n            [0, 0, 0, 1],\n            [0, 1, 0, 1],\n            [0, 1, 0, 1],\n            [0, 1, 0, 1],\n            [0, 0, 0, 0]\n        ]\n\n        replay = [\n            [0, 0, 0, 0],\n            [0, 0, 0, 0],\n            [0, 0, 0, 0],\n            [0, 0, 0, 0],\n            [0, 0, 0, 1],\n            [0, 1, 0, 1],\n            [0, 0, 0, 0]\n        ]\n\n        volume = [\n            [0, 0.5, 0, 0.5],\n            [0, 0, 0, 0.5],\n            [0, 0, 0, 0.5],\n            [0, 0.5, 0, 0.5],\n            [0, 0.5, 0, 0.5],\n            [0, 0.5, 0, 0.5],\n            [0, 0, 0, 0]\n        ]\n\n        note_seq = midi_decode(midi_encode(np.stack([composition, replay, volume], 2), step=2), 4, step=2)\n        np.testing.assert_array_equal(composition, note_seq[:, :, 0])\n        # TODO: Downsampling might have caused loss of information\n        # np.testing.assert_array_equal(replay, note_seq[:, :, 1])\n\nunittest.main()\n'"
train.py,0,"b""import tensorflow as tf\nfrom keras.callbacks import ModelCheckpoint, LambdaCallback\nfrom keras.callbacks import EarlyStopping, TensorBoard\nimport argparse\nimport midi\nimport os\n\nfrom constants import *\nfrom dataset import *\nfrom generate import *\nfrom midi_util import midi_encode\nfrom model import *\n\ndef main():\n    models = build_or_load()\n    train(models)\n\ndef train(models):\n    print('Loading data')\n    train_data, train_labels = load_all(styles, BATCH_SIZE, SEQ_LEN)\n\n    cbs = [\n        ModelCheckpoint(MODEL_FILE, monitor='loss', save_best_only=True, save_weights_only=True),\n        EarlyStopping(monitor='loss', patience=5),\n        TensorBoard(log_dir='out/logs', histogram_freq=1)\n    ]\n\n    print('Training')\n    models[0].fit(train_data, train_labels, epochs=1000, callbacks=cbs, batch_size=BATCH_SIZE)\n\nif __name__ == '__main__':\n    main()\n"""
util.py,0,"b""import numpy as np\nimport tensorflow as tf\nimport math\n\nfrom constants import *\nfrom midi_util import *\n\ndef one_hot(i, nb_classes):\n    arr = np.zeros((nb_classes,))\n    arr[i] = 1\n    return arr\n\ndef build_or_load(allow_load=True):\n    from model import build_models\n    models = build_models()\n    models[0].summary()\n    if allow_load:\n        try:\n            models[0].load_weights(MODEL_FILE)\n            print('Loaded model from file.')\n        except:\n            print('Unable to load model from file.')\n    return models\n\ndef get_all_files(paths):\n    potential_files = []\n    for path in paths:\n        for root, dirs, files in os.walk(path):\n            for f in files:\n                fname = os.path.join(root, f)\n                if os.path.isfile(fname) and fname.endswith('.mid'):\n                    potential_files.append(fname)\n    return potential_files\n"""
visualize.py,1,"b""import tensorflow as tf\nimport numpy as np\nimport os\nfrom keras import backend as K\n\nfrom util import *\nfrom constants import *\n\n# Visualize using:\n# http://projector.tensorflow.org/\ndef main():\n    models = build_or_load()\n    style_layer = models[0].get_layer('style')\n\n    print('Creating input')\n    style_in = tf.placeholder(tf.float32, shape=(NUM_STYLES, NUM_STYLES))\n    style_out = style_layer(style_in)\n\n    # All possible styles\n    all_styles = np.identity(NUM_STYLES)\n\n    with K.get_session() as sess:\n        embedding = sess.run(style_out, { style_in: all_styles })\n\n    print('Writing to out directory')\n    np.savetxt(os.path.join(OUT_DIR, 'style_embedding_vec.tsv'), embedding, delimiter='\\t')\n\n    labels = [[g] * len(styles[i]) for i, g in enumerate(genre)]\n    # Flatten\n    labels = [y for x in labels for y in x]\n\n    # Retreive specific artists\n    styles_labels = [y for x in styles for y in x]\n\n    styles_labels = np.reshape(styles_labels, [-1, 1])\n    labels = np.reshape(labels, [-1, 1])\n    labels = np.hstack([labels, styles_labels])\n\n    # Add metadata header\n    header = ['Genre', 'Artist']\n    labels = np.vstack([header, labels])\n\n    np.savetxt(os.path.join(OUT_DIR, 'style_embedding_labels.tsv'), labels, delimiter='\\t', fmt='%s')\n\nif __name__ == '__main__':\n    main()\n"""
