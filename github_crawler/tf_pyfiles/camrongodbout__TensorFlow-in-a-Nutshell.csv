file_path,api_count,code
part2/preprocess.py,0,"b'# coding: utf-8\nimport pandas as pd\ntrain = pd.read_csv(\'./train.csv\')\ntest = pd.read_csv(\'./test.csv\')\n\n# Impute the missing ages with median age\ntrain[""Age""] = train[""Age""].fillna(train[""Age""].median()).astype(int)\ntest[""Age""] = test[""Age""].fillna(test[""Age""].median()).astype(int)\n\n# Fill in missing embarked with S\ntrain[""Embarked""] = train[""Embarked""].fillna(""S"")\ntest[""Embarked""] = test[""Embarked""].fillna(""S"")\n\n# Fill in missing Cabin with None\ntrain[""Cabin""] = train[""Cabin""].fillna(""None"")\ntest[""Cabin""] = test[""Cabin""].fillna(""None"")\n\n# Write our changed dataframes to csv.\ntest.to_csv(""./test.csv"", index=False)\ntrain.to_csv(\'./train.csv\', index=False)\n'"
part2/wide.py,25,"b'import pandas as pd\nimport tensorflow as tf\n\nCATEGORICAL_COLUMNS = [""Name"", ""Sex"", ""Embarked"", ""Cabin""]\nCONTINUOUS_COLUMNS = [""Age"", ""SibSp"", ""Parch"", ""Fare"", ""PassengerId"", ""Pclass""]\n\nSURVIVED_COLUMN = ""Survived""\n\ndef build_estimator(model_dir):\n  """"""Build an estimator.""""""\n  # Categorical columns\n  sex = tf.contrib.layers.sparse_column_with_keys(column_name=""Sex"",\n                                                     keys=[""female"", ""male""])\n  embarked = tf.contrib.layers.sparse_column_with_keys(column_name=""Embarked"",\n                                                   keys=[""C"",\n                                                         ""S"",\n                                                         ""Q""])\n\n  cabin = tf.contrib.layers.sparse_column_with_hash_bucket(\n      ""Cabin"", hash_bucket_size=1000)\n  name = tf.contrib.layers.sparse_column_with_hash_bucket(\n      ""Name"", hash_bucket_size=1000)\n\n\n  # Continuous columns\n  age = tf.contrib.layers.real_valued_column(""Age"")\n  passenger_id = tf.contrib.layers.real_valued_column(""PassengerId"")\n  sib_sp = tf.contrib.layers.real_valued_column(""SibSp"")\n  parch = tf.contrib.layers.real_valued_column(""Parch"")\n  fare = tf.contrib.layers.real_valued_column(""Fare"")\n  p_class = tf.contrib.layers.real_valued_column(""Pclass"")\n\n  # Transformations.\n  age_buckets = tf.contrib.layers.bucketized_column(age,\n                                                    boundaries=[\n                                                        5, 18, 25, 30, 35, 40,\n                                                        45, 50, 55, 65\n                                                    ])\n   # Wide columns and deep columns.\n  wide_columns = [sex, embarked, cabin, name, age_buckets,\n                  tf.contrib.layers.crossed_column(\n                      [age_buckets, sex],\n                      hash_bucket_size=int(1e6)),\n                  tf.contrib.layers.crossed_column([embarked, name],\n                                                   hash_bucket_size=int(1e4))]\n  deep_columns = [\n      tf.contrib.layers.embedding_column(sex, dimension=8),\n      tf.contrib.layers.embedding_column(embarked, dimension=8),\n      tf.contrib.layers.embedding_column(cabin, dimension=8),\n      tf.contrib.layers.embedding_column(name, dimension=8),\n      age,\n      passenger_id,\n      sib_sp,\n      parch,\n      fare,\n      p_class\n  ]\n\n\n\n  return tf.contrib.learn.DNNLinearCombinedClassifier(\n        linear_feature_columns=wide_columns,\n        dnn_feature_columns=deep_columns,\n        dnn_hidden_units=[100, 50])\n\ndef input_fn(df, train=False):\n  """"""Input builder function.""""""\n  # Creates a dictionary mapping from each continuous feature column name (k) to\n  # the values of that column stored in a constant Tensor.\n  continuous_cols = {k: tf.constant(df[k].values) for k in CONTINUOUS_COLUMNS}\n  # Creates a dictionary mapping from each categorical feature column name (k)\n  # to the values of that column stored in a tf.SparseTensor.\n  categorical_cols = {k: tf.SparseTensor(\n    indices=[[i, 0] for i in range(df[k].size)],\n    values=df[k].values,\n    shape=[df[k].size, 1])\n                      for k in CATEGORICAL_COLUMNS}\n  # Merges the two dictionaries into one.\n  feature_cols = dict(continuous_cols)\n  feature_cols.update(categorical_cols)\n  # Converts the label column into a constant Tensor.\n  if train:\n    label = tf.constant(df[SURVIVED_COLUMN].values)\n      # Returns the feature columns and the label.\n    return feature_cols, label\n  else:\n    return feature_cols\n\n\ndef train_and_eval():\n  """"""Train and evaluate the model.""""""\n  df_train = pd.read_csv(\n      tf.gfile.Open(""./train.csv""),\n      skipinitialspace=True)\n  df_test = pd.read_csv(\n      tf.gfile.Open(""./test.csv""),\n      skipinitialspace=True)\n\n  model_dir = ""./models""\n  print(""model directory = %s"" % model_dir)\n\n  m = build_estimator(model_dir)\n  m.fit(input_fn=lambda: input_fn(df_train, True), steps=200)\n  print m.predict(input_fn=lambda: input_fn(df_test))\n  results = m.evaluate(input_fn=lambda: input_fn(df_train, True), steps=1)\n  for key in sorted(results):\n    print(""%s: %s"" % (key, results[key]))\n\ndef main(_):\n  train_and_eval()\n\n\nif __name__ == ""__main__"":\n  tf.app.run()'"
part3/cnn.py,8,"b""from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom sklearn import metrics\nimport tensorflow as tf\nfrom tensorflow.contrib import learn\n\n# from SKFLOW \n\n### Download and load MNIST data.\nmnist = learn.datasets.load_dataset('mnist')\n\n### Convolutional network\ndef max_pool_2x2(tensor_in):\n  return tf.nn.max_pool(\n      tensor_in, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n\n\ndef conv_model(X, y):\n  # pylint: disable=invalid-name,missing-docstring\n  # reshape X to 4d tensor with 2nd and 3rd dimensions being image width and\n  # height final dimension being the number of color channels.\n  X = tf.reshape(X, [-1, 28, 28, 1])\n  # first conv layer will compute 32 features for each 5x5 patch\n  with tf.variable_scope('conv_layer1'):\n    h_conv1 = learn.ops.conv2d(X, n_filters=32, filter_shape=[5, 5],\n                               bias=True, activation=tf.nn.relu)\n    h_pool1 = max_pool_2x2(h_conv1)\n  # second conv layer will compute 64 features for each 5x5 patch.\n  with tf.variable_scope('conv_layer2'):\n    h_conv2 = learn.ops.conv2d(h_pool1, n_filters=64, filter_shape=[5, 5],\n                               bias=True, activation=tf.nn.relu)\n    h_pool2 = max_pool_2x2(h_conv2)\n    # reshape tensor into a batch of vectors\n    h_pool2_flat = tf.reshape(h_pool2, [-1, 7 * 7 * 64])\n  # densely connected layer with 1024 neurons.\n  h_fc1 = learn.ops.dnn(\n      h_pool2_flat, [1024], activation=tf.nn.relu, dropout=0.5)\n  return learn.models.logistic_regression(h_fc1, y)\n\n# Training and predicting.\nclassifier = learn.TensorFlowEstimator(\n    model_fn=conv_model, n_classes=10, batch_size=100, steps=20000,\n    learning_rate=0.001)\nclassifier.fit(mnist.train.images, mnist.train.labels)\nscore = metrics.accuracy_score(\n    mnist.test.labels, classifier.predict(mnist.test.images))\nprint('Accuracy: {0:f}'.format(score))"""
part3/crf.py,14,"b'import numpy as np\nimport tensorflow as tf\n\n## FROM TENSORFLOW CONTRIB CRF\n\n\n# Data settings.\nnum_examples = 10\nnum_words = 20\nnum_features = 100\nnum_tags = 5\n\n# Random features.\nx = np.random.rand(num_examples, num_words, num_features).astype(np.float32)\n\n# Random tag indices representing the gold sequence.\ny = np.random.randint(num_tags, size=[num_examples, num_words]).astype(np.int32)\n\n# All sequences in this example have the same length, but they can be variable in a real model.\nsequence_lengths = np.full(num_examples, num_words - 1, dtype=np.int32)\n\n# Train and evaluate the model.\nwith tf.Graph().as_default():\n  with tf.Session() as session:\n    # Add the data to the TensorFlow graph.\n    x_t = tf.constant(x)\n    y_t = tf.constant(y)\n    sequence_lengths_t = tf.constant(sequence_lengths)\n\n    # Compute unary scores from a linear layer.\n    weights = tf.get_variable(""weights"", [num_features, num_tags])\n    matricized_x_t = tf.reshape(x_t, [-1, num_features])\n    matricized_unary_scores = tf.batch_matmul(matricized_x_t, weights)\n    unary_scores = tf.reshape(matricized_unary_scores,\n                              [num_examples, num_words, num_tags])\n\n    # Compute the log-likelihood of the gold sequences and keep the transition\n    # params for inference at test time.\n    log_likelihood, transition_params = tf.contrib.crf.crf_log_likelihood(\n        unary_scores, y_t, sequence_lengths_t)\n\n    # Add a training op to tune the parameters.\n    loss = tf.reduce_mean(-log_likelihood)\n    train_op = tf.train.GradientDescentOptimizer(0.01).minimize(loss)\n\n    # Train for a fixed number of iterations.\n    session.run(tf.initialize_all_variables())\n    for i in range(1000):\n      tf_unary_scores, tf_transition_params, _ = session.run(\n          [unary_scores, transition_params, train_op])\n      if i % 100 == 0:\n        correct_labels = 0\n        total_labels = 0\n        for tf_unary_scores_, y_, sequence_length_ in zip(tf_unary_scores, y,\n                                                          sequence_lengths):\n          # Remove padding from the scores and tag sequence.\n          tf_unary_scores_ = tf_unary_scores_[:sequence_length_]\n          y_ = y_[:sequence_length_]\n\n          # Compute the highest scoring sequence.\n          viterbi_sequence, _ = tf.contrib.crf.viterbi_decode(\n              tf_unary_scores_, tf_transition_params)\n\n          # Evaluate word-level accuracy.\n          correct_labels += np.sum(np.equal(viterbi_sequence, y_))\n          total_labels += sequence_length_\n        accuracy = 100.0 * correct_labels / float(total_labels)\n        print(""Accuracy: %.2f%%"" % accuracy)'"
part3/ffn.py,10,"b'# From https://github.com/nlintz/TensorFlow-Tutorials/blob/master/03_net.py\n\n\n#!/usr/bin/env python\n\nimport tensorflow as tf\nimport numpy as np\nimport input_data\n\n\ndef init_weights(shape):\n    return tf.Variable(tf.random_normal(shape, stddev=0.01))\n\n\ndef model(X, w_h, w_o):\n    h = tf.nn.sigmoid(tf.matmul(X, w_h)) # this is a basic mlp, think 2 stacked logistic regressions\n    return tf.matmul(h, w_o) # note that we dont take the softmax at the end because our cost fn does that for us\n\n\nmnist = input_data.read_data_sets(""MNIST_data/"", one_hot=True)\ntrX, trY, teX, teY = mnist.train.images, mnist.train.labels, mnist.test.images, mnist.test.labels\n\nX = tf.placeholder(""float"", [None, 784])\nY = tf.placeholder(""float"", [None, 10])\n\nw_h = init_weights([784, 625]) # create symbolic variables\nw_o = init_weights([625, 10])\n\npy_x = model(X, w_h, w_o)\n\ncost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(py_x, Y)) # compute costs\ntrain_op = tf.train.GradientDescentOptimizer(0.05).minimize(cost) # construct an optimizer\npredict_op = tf.argmax(py_x, 1)\n\n# Launch the graph in a session\nwith tf.Session() as sess:\n    # you need to initialize all variables\n    tf.initialize_all_variables().run()\n\n    for i in range(100):\n        for start, end in zip(range(0, len(trX), 128), range(128, len(trX)+1, 128)):\n            sess.run(train_op, feed_dict={X: trX[start:end], Y: trY[start:end]})\n        print(i, np.mean(np.argmax(teY, axis=1) ==\n                         sess.run(predict_op, feed_dict={X: teX, Y: teY})))'"
part3/linear.py,9,"b'import numpy as np\nimport tensorflow as tf\n\ndef weight_variable(shape):\n    initial = tf.truncated_normal(shape, stddev=1)\n    return tf.Variable(initial)\n\n# dataset\nxx = np.random.randint(0,1000,[1000,3])/1000.\nyy = xx[:,0] * 2 + xx[:,1] * 1.4 + xx[:,2] * 3\n\n# model\nx = tf.placeholder(tf.float32, shape=[None, 3])\ny_ = tf.placeholder(tf.float32, shape=[None])\nW1 = weight_variable([3, 1])\ny = tf.matmul(x, W1)\n\n# training and cost function\ncost_function = tf.reduce_mean(tf.square(tf.squeeze(y) - y_))\ntrain_function = tf.train.AdamOptimizer(1e-2).minimize(cost_function)\n\n# create a session\nsess = tf.Session()\n\n# train\nsess.run(tf.initialize_all_variables())\nfor i in range(10000):\n    sess.run(train_function, feed_dict={x:xx, y_:yy})\n    if i % 1000 == 0:\n        print(sess.run(cost_function, feed_dict={x:xx, y_:yy}))'"
part3/random_forest.py,8,"b'from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport numpy as np\nimport tensorflow as tf\n\n## Modified from tensorflow forest contrib\n\n## Random Forest Classifier\n\nhparams = tf.contrib.tensor_forest.python.tensor_forest.ForestHParams(\n        num_trees=3, max_nodes=1000, num_classes=3, num_features=4)\nclassifier = tf.contrib.learn.TensorForestEstimator(hparams)\n\niris = tf.contrib.learn.datasets.load_iris()\ndata = iris.data.astype(np.float32)\ntarget = iris.target.astype(np.float32)\n\nmonitors = [tf.contrib.learn.TensorForestLossMonitor(10, 10)]\nclassifier.fit(x=data, y=target, steps=100, monitors=monitors)\nclassifier.evaluate(x=data, y=target, steps=10)\n\n## Random Forest Regressor\n\nhparams = tf.contrib.tensor_forest.python.tensor_forest.ForestHParams(\n        num_trees=3, max_nodes=1000, num_classes=1, num_features=13,\n        regression=True)\n\nregressor = tf.contrib.learn.TensorForestEstimator(hparams)\n\nboston = tf.contrib.learn.datasets.load_boston()\ndata = boston.data.astype(np.float32)\ntarget = boston.target.astype(np.float32)\n\nmonitors = [tf.contrib.learn.TensorForestLossMonitor(10, 10)]\nregressor.fit(x=data, y=target, steps=100, monitors=monitors)\nregressor.evaluate(x=data, y=target, steps=10)\n'"
part3/reinforce.py,18,"b'""""""Simple examples of the REINFORCE algorithm.""""""\n""""""""This is modified from the bayesflow contrib sample folder """"""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\n\n\ndistributions = tf.contrib.distributions\nsg = tf.contrib.bayesflow.stochastic_graph\nst = tf.contrib.bayesflow.stochastic_tensor\n\n\ndef split_apply_merge(inp, partitions, fns):\n  """"""Split input according to partitions.  Pass results through fns and merge.\n  Args:\n    inp: the input vector\n    partitions: tensor of same length as input vector, having values 0, 1\n    fns: the two functions.\n  Returns:\n    the vector routed, where routed[i] = fns[partitions[i]](inp[i])\n  """"""\n  new_inputs = tf.dynamic_partition(inp, partitions, len(fns))\n  new_outputs = [fns[i](x) for i, x in enumerate(new_inputs)]\n  new_indices = tf.dynamic_partition(\n      tf.range(0, inp.get_shape()[0]), partitions, len(fns))\n  return tf.dynamic_stitch(new_indices, new_outputs)\n\n\ndef plus_1(inputs):\n  return inputs + 1.0\n\n\ndef minus_1(inputs):\n  return inputs - 1.0\n\n\ndef build_split_apply_merge_model():\n  """"""Build the Split-Apply-Merge Model.\n  Route each value of input [-1, -1, 1, 1] through one of the\n  functions, plus_1, minus_1.  The decision for routing is made by\n  4 Bernoulli R.V.s whose parameters are determined by a neural network\n  applied to the input.  REINFORCE is used to update the NN parameters.\n  Returns:\n    The 3-tuple (route_selection, routing_loss, final_loss), where:\n      - route_selection is an int 4-vector\n      - routing_loss is a float 4-vector\n      - final_loss is a float scalar.\n  """"""\n  inputs = tf.constant([[-1.0], [-1.0], [1.0], [1.0]])\n  targets = tf.constant([[0.0], [0.0], [0.0], [0.0]])\n  paths = [plus_1, minus_1]\n  weights = tf.get_variable(""w"", [1, 2])\n  bias = tf.get_variable(""b"", [1, 1])\n  logits = tf.matmul(inputs, weights) + bias\n\n  # REINFORCE forward step\n  route_selection = st.StochasticTensor(\n      distributions.Categorical, logits=logits)\n\n  # Accessing route_selection as a Tensor below forces a sample of\n  # the Categorical distribution based on its logits.\n  # This is equivalent to calling route_selection.value().\n  #\n  # route_selection.value() returns an int32 4-vector with random\n  # values in {0, 1}\n  # COPY+ROUTE+PASTE\n  outputs = split_apply_merge(inputs, route_selection, paths)\n\n  # flatten routing_loss to a row vector (from a column vector)\n  routing_loss = tf.reshape(tf.square(outputs - targets), shape=[-1])\n\n  # Total loss: score function loss + routing loss.\n  # The score function loss (through `route_selection.loss(routing_loss)`)\n  # returns:\n  #  [stop_gradient(routing_loss) *\n  #   route_selection.log_pmf(stop_gradient(route_selection.value()))],\n  # where log_pmf has gradients going all the way back to weights and bias.\n  # In this case, the routing_loss depends on the variables only through\n  # ""route_selection"", which has a stop_gradient on it.  So the\n  # gradient of the loss really come through the score function\n  surrogate_loss = sg.surrogate_loss([routing_loss])\n  final_loss = tf.reduce_sum(surrogate_loss)\n\n  return (route_selection, routing_loss, final_loss)\n\n\nclass REINFORCESimpleExample():\n\n  def SplitApplyMerge(self):\n    # Repeatability.  SGD has a tendency to jump around, even here.\n    tf.set_random_seed(1)\n\n    # Use sampling to train REINFORCE\n    with st.value_type(st.SampleAndReshapeValue(n=1)):\n      (route_selection,\n        routing_loss,\n        final_loss) = build_split_apply_merge_model()\n\n    sgd = tf.train.GradientDescentOptimizer(1.0).minimize(final_loss)\n\n    tf.initialize_all_variables().run()\n\n    for i in range(10):\n        # Run loss and inference step.  This toy problem converges VERY quickly.\n      (routing_loss_v, final_loss_v, route_selection_v, _) = sess.run(\n          [routing_loss, final_loss, tf.identity(route_selection), sgd])\n      print(\n          ""Iteration %d, routing loss: %s, final_loss: %s, ""\n          ""route selection: %s""\n          % (i, routing_loss_v, final_loss_v, route_selection_v))'"
part3/rnn.py,6,"b'import tensorflow as tf\nimport numpy as np\n# Create input data\nX = np.random.randn(2, 10, 8)\n\n# The second example is of length 6 \nX[1,6,:] = 0\nX_lengths = [10, 6]\n\ncell = tf.nn.rnn_cell.LSTMCell(num_units=64, state_is_tuple=True)\ncell = tf.nn.rnn_cell.DropoutWrapper(cell=cell, output_keep_prob=0.5)\ncell = tf.nn.rnn_cell.MultiRNNCell(cells=[cell] * 4, state_is_tuple=True)\n\noutputs, last_states = tf.nn.dynamic_rnn(\n    cell=cell,\n    dtype=tf.float64,\n    sequence_length=X_lengths,\n    inputs=X)\n\nresult = tf.contrib.learn.run_n(\n    {""outputs"": outputs, ""last_states"": last_states},\n    n=1,\n    feed_dict=None)'"
part3/svm.py,62,"b'from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\n\n"""""" Modified from tensorflow contrib svm  """"""\n\n\ndef RealValuedFeaturesPerfectlySeparable(self):\n  """""" SVM classifier with real valued features.""""""\n\n  def input_fn():\n    return {\n          \'example_id\': tf.constant([\'1\', \'2\', \'3\']),\n          \'feature1\': tf.constant([[0.0], [1.0], [3.0]]),\n          \'feature2\': tf.constant([[1.0], [-1.2], [1.0]]),\n    }, tf.constant([[1], [0], [1]])\n\n  feature1 = tf.contrib.layers.real_valued_column(\'feature1\')\n  feature2 = tf.contrib.layers.real_valued_column(\'feature2\')\n  svm_classifier = tf.contrib.learn.SVM(feature_columns=[feature1, feature2],\n                                          example_id_column=\'example_id\',\n                                          l1_regularization=0.0,\n                                          l2_regularization=0.0)\n  svm_classifier.fit(input_fn=input_fn, steps=30)\n  metrics = svm_classifier.evaluate(input_fn=input_fn, steps=1)\n  loss = metrics[\'loss\']\n  accuracy = metrics[\'accuracy\']\n  # The points are not only separable but there exist weights (for instance\n  # w1=0.0, w2=1.0) that satisfy the margin inequalities (y_i* w^T*x_i >=1).\n  # The unregularized loss should therefore be 0.0.\n\ndef RealValuedFeaturesWithL2Regularization(self):\n    """""" SVM classifier with real valued features and L2 regularization.""""""\n\n  def input_fn():\n    return {\n          \'example_id\': tf.constant([\'1\', \'2\', \'3\']),\n          \'feature1\': tf.constant([[0.5], [1.0], [1.0]]),\n          \'feature2\': tf.constant([[1.0], [-1.0], [0.5]]),\n      }, tf.constant([[1], [0], [1]])\n\n  feature1 = tf.contrib.layers.real_valued_column(\'feature1\')\n  feature2 = tf.contrib.layers.real_valued_column(\'feature2\')\n  svm_classifier = tf.contrib.learn.SVM(feature_columns=[feature1, feature2],\n                                          example_id_column=\'example_id\',\n                                          l1_regularization=0.0,\n                                          l2_regularization=1.0)\n  svm_classifier.fit(input_fn=input_fn, steps=30)\n  metrics = svm_classifier.evaluate(input_fn=input_fn, steps=1)\n  loss = metrics[\'loss\']\n  accuracy = metrics[\'accuracy\']\n  # The points are in general separable. Also, if there was no regularization,\n  # the margin inequalities would be satisfied too (for instance by w1=1.0,\n  # w2=5.0). Due to regularization, smaller weights are chosen. This results\n  # to a small but non-zero uneregularized loss. Still, all the predictions\n  # will be correct resulting to perfect accuracy.\n\ndef MultiDimensionalRealValuedFeaturesWithL2Regularization(self):\n   """""" SVM with multi-dimensional real features and L2 regularization.""""""\n\n  # This is identical to the one in RealValuedFeaturesWithL2Regularization\n  # where 2 tensors (dense features) of shape [3, 1] have been replaced by a\n  # single tensor (dense feature) of shape [3, 2].\n  def input_fn():\n    return {\n          \'example_id\': tf.constant([\'1\', \'2\', \'3\']),\n          \'multi_dim_feature\': tf.constant(\n              [[0.5, 1.0], [1.0, -1.0], [1.0, 0.5]]),\n    }, tf.constant([[1], [0], [1]])\n\n  multi_dim_feature = tf.contrib.layers.real_valued_column(\n        \'multi_dim_feature\', dimension=2)\n  svm_classifier = tf.contrib.learn.SVM(feature_columns=[multi_dim_feature],\n                                          example_id_column=\'example_id\',\n                                          l1_regularization=0.0,\n                                          l2_regularization=1.0)\n  svm_classifier.fit(input_fn=input_fn, steps=30)\n  metrics = svm_classifier.evaluate(input_fn=input_fn, steps=1)\n  loss = metrics[\'loss\']\n  accuracy = metrics[\'accuracy\']\n\n\ndef RealValuedFeaturesWithMildL1Regularization(self):\n    """""" SVM classifier with real valued features and L2 regularization.""""""\n\n  def input_fn():\n    return {\n          \'example_id\': tf.constant([\'1\', \'2\', \'3\']),\n          \'feature1\': tf.constant([[0.5], [1.0], [1.0]]),\n          \'feature2\': tf.constant([[1.0], [-1.0], [0.5]]),\n      }, tf.constant([[1], [0], [1]])\n\n  feature1 = tf.contrib.layers.real_valued_column(\'feature1\')\n  feature2 = tf.contrib.layers.real_valued_column(\'feature2\')\n  svm_classifier = tf.contrib.learn.SVM(feature_columns=[feature1, feature2],\n                                          example_id_column=\'example_id\',\n                                          l1_regularization=0.5,\n                                          l2_regularization=1.0)\n  svm_classifier.fit(input_fn=input_fn, steps=30)\n  metrics = svm_classifier.evaluate(input_fn=input_fn, steps=1)\n  loss = metrics[\'loss\']\n  accuracy = metrics[\'accuracy\']\n\n    # Adding small L1 regularization favors even smaller weights. This results\n    # to somewhat moderate unregularized loss (bigger than the one when there is\n    # no L1 regularization. Still, since L1 is small, all the predictions will\n    # be correct resulting to perfect accuracy.\n\ndef RealValuedFeaturesWithBigL1Regularization(self):\n    """""" SVM classifier with real valued features and L2 regularization.""""""\n\n  def input_fn():\n    return {\n          \'example_id\': tf.constant([\'1\', \'2\', \'3\']),\n          \'feature1\': tf.constant([[0.5], [1.0], [1.0]]),\n          \'feature2\': tf.constant([[1.0], [-1.0], [0.5]]),\n      }, tf.constant([[1], [0], [1]])\n\n  feature1 = tf.contrib.layers.real_valued_column(\'feature1\')\n  feature2 = tf.contrib.layers.real_valued_column(\'feature2\')\n  svm_classifier = tf.contrib.learn.SVM(feature_columns=[feature1, feature2],\n                                          example_id_column=\'example_id\',\n                                          l1_regularization=3.0,\n                                          l2_regularization=1.0)\n  svm_classifier.fit(input_fn=input_fn, steps=30)\n  metrics = svm_classifier.evaluate(input_fn=input_fn, steps=1)\n  loss = metrics[\'loss\']\n  accuracy = metrics[\'accuracy\']\n\n  # When L1 regularization parameter is large, the loss due to regularization\n  # outweights the unregularized loss. In this case, the classifier will favor\n  # very small weights (in current case 0) resulting both big unregularized\n  # loss and bad accuracy.\n\ndef parseFeatures(self):\n  """""" SVM classifier with (hashed) sparse features.""""""\n\n  def input_fn():\n    return {\n          \'example_id\': tf.constant([\'1\', \'2\', \'3\']),\n          \'price\': tf.constant([[0.8], [0.6], [0.3]]),\n          \'country\': tf.SparseTensor(\n              values=[\'IT\', \'US\', \'GB\'],\n              indices=[[0, 0], [1, 0], [2, 0]],\n              shape=[3, 1]),\n    }, tf.constant([[0], [1], [1]])\n\n  price = tf.contrib.layers.real_valued_column(\'price\')\n  country = tf.contrib.layers.sparse_column_with_hash_bucket(\n        \'country\', hash_bucket_size=5)\n  svm_classifier = tf.contrib.learn.SVM(feature_columns=[price, country],\n                                          example_id_column=\'example_id\',\n                                          l1_regularization=0.0,\n                                          l2_regularization=1.0)\n  svm_classifier.fit(input_fn=input_fn, steps=30)\n  accuracy = svm_classifier.evaluate(input_fn=input_fn, steps=1)[\'accuracy\']\n\n\ndef BucketizedFeatures(self):\n  """""" SVM classifier with bucketized features.""""""\n\n  def input_fn():\n    return {\n          \'example_id\': tf.constant([\'1\', \'2\', \'3\']),\n          \'price\': tf.constant([[600.0], [800.0], [400.0]]),\n          \'sq_footage\': tf.constant([[1000.0], [800.0], [500.0]]),\n          \'weights\': tf.constant([[1.0], [1.0], [1.0]])\n      }, tf.constant([[1], [0], [1]])\n\n  price_bucket = tf.contrib.layers.bucketized_column(\n        tf.contrib.layers.real_valued_column(\'price\'),\n        boundaries=[500.0, 700.0])\n  sq_footage_bucket = tf.contrib.layers.bucketized_column(\n        tf.contrib.layers.real_valued_column(\'sq_footage\'), boundaries=[650.0])\n\n  svm_classifier = tf.contrib.learn.SVM(\n        feature_columns=[price_bucket, sq_footage_bucket],\n        example_id_column=\'example_id\',\n        l1_regularization=0.1,\n        l2_regularization=1.0)\n  svm_classifier.fit(input_fn=input_fn, steps=30)\n  accuracy = svm_classifier.evaluate(input_fn=input_fn, steps=1)[\'accuracy\']\n\n\ndef MixedFeatures(self):\n  """""" SVM classifier with a mix of features.""""""\n\n  def input_fn():\n    return {\n          \'example_id\': tf.constant([\'1\', \'2\', \'3\']),\n          \'price\': tf.constant([[0.6], [0.8], [0.3]]),\n          \'sq_footage\': tf.constant([[900.0], [700.0], [600.0]]),\n          \'country\': tf.SparseTensor(\n              values=[\'IT\', \'US\', \'GB\'],\n              indices=[[0, 0], [1, 3], [2, 1]],\n              shape=[3, 5]),\n          \'weights\': tf.constant([[3.0], [1.0], [1.0]])\n      }, tf.constant([[1], [0], [1]])\n\n  price = tf.contrib.layers.real_valued_column(\'price\')\n  sq_footage_bucket = tf.contrib.layers.bucketized_column(\n        tf.contrib.layers.real_valued_column(\'sq_footage\'),\n        boundaries=[650.0, 800.0])\n  country = tf.contrib.layers.sparse_column_with_hash_bucket(\n        \'country\', hash_bucket_size=5)\n  sq_footage_country = tf.contrib.layers.crossed_column(\n        [sq_footage_bucket, country], hash_bucket_size=10)\n  svm_classifier = tf.contrib.learn.SVM(\n        feature_columns=[price, sq_footage_bucket, country, sq_footage_country],\n        example_id_column=\'example_id\',\n        weight_column_name=\'weights\',\n        l1_regularization=0.1,\n        l2_regularization=1.0)\n\n  svm_classifier.fit(input_fn=input_fn, steps=30)\n  accuracy = svm_classifier.evaluate(input_fn=input_fn, steps=1)[\'accuracy\']'"
