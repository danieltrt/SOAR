file_path,api_count,code
NN/Errors.py,0,b'class LayerError(Exception):\n    pass\n\n\nclass BuildLayerError(Exception):\n    pass\n\n\nclass BuildNetworkError(Exception):\n    pass\n'
NN/NN.py,0,"b'# TODO: Remove dummy Timings\n\ntry:\n    from NN.TF.Networks import *\n    print(""Using tensorflow backend"")\nexcept ImportError:\n    from NN.Basic.Networks import *\n    print(""Using numpy backend"")\n'"
Opt/Functions.py,0,"b'import numpy as np\n\n\nclass Function:\n    """""" The framework of target functions """"""\n    def __init__(self, n, **kwargs):\n        """"""\n        :param n      : Dimension of input vector \n        :param kwargs : May contain \'grad_eps\' which will be used to calculate numerical gradient\n        """"""\n        self.n = n\n        self._params = {\n            ""grad_eps"": kwargs.get(""grad_eps"", 1e-8)\n        }\n\n    def refresh_cache(self, x, dtype=""all""):\n        """"""\n        Refresh cache if needed \n        :param x     : input vector\n        :param dtype : control what kind of cache will be refreshed\n        :return      : None\n        """"""\n        pass\n\n    @property\n    def x0(self):\n        """""" Should return the initial point """"""\n        return np.random.random(self.n)\n\n    @property\n    def bounds(self):\n        """""" Should return the bounds. Format:\n            [[min, max],\n             [min, max],\n             ...,\n             [min, max]]\n        """"""\n        return\n\n    def grad(self, x):\n        """""" Should return the gradient, support automatic differentiation """"""\n        eps_base = np.random.random() * 1e-8\n        grad_eps = []\n        for i in range(len(x)):\n            eps = np.zeros(len(x))\n            eps[i] = eps_base\n            x1 = x - 2 * eps; self.refresh_cache(x1, dtype=""loss""); loss1 = self.loss(x1)  # type: float\n            x2 = x - eps; self.refresh_cache(x2, dtype=""loss""); loss2 = self.loss(x2)      # type: float\n            x3 = x + eps; self.refresh_cache(x3, dtype=""loss""); loss3 = self.loss(x3)      # type: float\n            x4 = x + 2 * eps; self.refresh_cache(x4, dtype=""loss""); loss4 = self.loss(x4)  # type: float\n            grad_eps.append(1 / (12 * eps_base) * (loss1 - 8 * loss2 + 8 * loss3 - loss4))\n        return np.array(grad_eps)\n\n    def hessian(self, x):\n        """""" Should return the hessian matrix, support automatic differentiation """"""\n        eps_base = np.random.random() * 1e-8\n        grad_eps = []\n        for i in range(len(x)):\n            eps = np.zeros(len(x))\n            eps[i] = eps_base\n            x1 = x - 2 * eps; self.refresh_cache(x1); grad1 = self.grad(x1)  # type: np.ndarray\n            x2 = x - eps; self.refresh_cache(x2); grad2 = self.grad(x2)      # type: np.ndarray\n            x3 = x + eps; self.refresh_cache(x3); grad3 = self.grad(x3)      # type: np.ndarray\n            x4 = x + 2 * eps; self.refresh_cache(x4); grad4 = self.grad(x4)  # type: np.ndarray\n            grad_eps.append(1 / (12 * eps_base) * (grad1 - 8 * grad2 + 8 * grad3 - grad4))\n        return np.array(grad_eps)\n\n    def loss(self, x):\n        """""" Should return the loss """"""\n        pass\n'"
Opt/Methods.py,0,"b'import random\r\nimport warnings\r\nimport numpy as np\r\ntry:\r\n    from scipy import linalg, optimize\r\n    from scipy.sparse import issparse, linalg as sparse_lin\r\n    scipy_flag = True\r\nexcept ImportError:\r\n    linalg = optimize = issparse = sparse_lin = None\r\n    scipy_flag = False\r\n\r\nfrom Opt.Functions import Function\r\nfrom Util.ProgressBar import ProgressBar\r\n\r\nnp.seterr(all=""warn"")\r\n\r\n\r\n# Config\r\n\r\nclass LineSearchConfig:\r\n    """""" Default values for LineSearch parameters """"""\r\n    method = ""0.618""\r\n    init = 0.5\r\n    floor = 0.001\r\n    epoch = 50\r\n    rho = 1e-4\r\n    sigma = 0.1\r\n    gamma = 0.1\r\n    eps = 1e-4\r\n    t = 1.1\r\n\r\n\r\nclass OptConfig:\r\n    """""" Default values for Optimizer parameters """"""\r\n    epoch = 1000\r\n    eps = 1e-8\r\n    nu = 0.1\r\n\r\n\r\n# Line Search\r\n\r\nclass LineSearch:\r\n    def __init__(self, func, **kwargs):\r\n        """"""\r\n        The framework for line search algorithms\r\n        :param func   : Should be a SubClass of class \'Function\' defined in Functions.py\r\n        :param kwargs : May contain these parameters:\r\n                init  : Average of random initial points   ; default: 0.5\r\n                floor : Lower bound of \xce\xb1 in Armijo         ; default: 0.001\r\n                epoch : Maximum iteration for line search  ; default: 50\r\n                rho   : \xcf\x81 in all line search method        ; default: 1e-4\r\n                sigma : \xcf\x83 in Wolfe, Strong Wolfe           ; default: 0.1\r\n                gamma : Initial increment of parameter \xce\xb1   ; default: 0.1\r\n                eps   : Lower bound of the interval length ; default: 1e-8\r\n                t     : Boosting rate of \xce\xb3                 ; default: 1.1\r\n        """"""\r\n        assert isinstance(func, Function)\r\n        self._org_loss_cache = self._org_grad_cache = self._cache = None\r\n        self._alpha = self._x = self._d = None\r\n        self._func = func\r\n        keys = [key for key in LineSearchConfig.__dict__ if ""__"" not in key]\r\n        self._params = {key: kwargs.get(key, getattr(LineSearchConfig, key)) for key in keys}\r\n        self.success_flag = [0, 0]\r\n        self.counter = 0\r\n\r\n    def min(self):\r\n        """""" Lower Bound Criterion """"""\r\n        pass\r\n\r\n    def max(self):\r\n        """""" Upper Bound Criterion """"""\r\n        pass\r\n\r\n    def func(self, alpha=None, diff=False):\r\n        """"""\r\n        Calculate the loss or the gradient\r\n        :param alpha : Step size \xce\xb1\r\n        :param diff  : If True, this function will return the gradient; If False, loss will be returned\r\n        :return      : Loss or gradient\r\n        """"""\r\n        alpha = self._alpha if alpha is None else alpha\r\n        (method, refresh, count) = (self._func.loss, ""loss"", 1) if not diff else (self._func.grad, ""all"", 0)\r\n        self.counter += count\r\n        if alpha == 0:\r\n            return method(self._x)\r\n        self._func.refresh_cache(self._x + alpha * self._d, dtype=refresh)\r\n        return method(self._x + alpha * self._d)\r\n\r\n    def step(self, x, d):\r\n        """"""\r\n        Perform a step of line search\r\n        :param x      : Current x\r\n        :param d      : Current direction\r\n        :return       : \xce\xb1, feva, success, loss, gradient\r\n                            where \'success\' is a list with two elements:\r\n                                success[0]: whether the initial search succeeded\r\n                                success[1]: whether the line search criteria are satisfied\r\n                            0: False; 1: True\r\n        """"""\r\n        self.counter = 0\r\n        self.success_flag = [0, 0]\r\n        self._alpha = 2 * self._params[""init""] * random.random()\r\n        self._x, self._d = x, d\r\n        self._func.refresh_cache(self._x)\r\n        self._org_loss_cache = self.func(0, False)\r\n        self._org_grad_cache = self.func(0, True)\r\n        a, b, fa, fb, self.success_flag[0] = self._get_init(self._params[""gamma""], self._params[""t""])\r\n        fl = fr = None\r\n        for _ in range(self._params[""epoch""]):\r\n            b_sub_a = b - a\r\n            if b_sub_a < self._params[""eps""]:\r\n                return self._alpha, self.counter, 0, self.func(diff=False), self.func(diff=True)\r\n            self._alpha = 0.5 * (a + b)\r\n            if self._params[""method""] == ""poly"":\r\n                self._cache = self.func()\r\n                c1 = (fb - fa) / b_sub_a\r\n                c2 = ((self._cache - fa) / (self._alpha - 1) - c1) / (self._alpha - b)\r\n                self._alpha = 0.5 * (a + b - c1 / c2)\r\n            if self.min() and self.max():\r\n                self.success_flag[1] = 1\r\n                break\r\n            self._func.refresh_cache(self._x)\r\n            if self._params[""method""] == ""0.618"":\r\n                al = a + 0.382 * b_sub_a\r\n                ar = a + 0.618 * b_sub_a\r\n                if fl is None:\r\n                    fl = self.func(al)\r\n                if fr is None:\r\n                    fr = self.func(ar)\r\n                if fl < fr:\r\n                    b, fl, fr = ar, None, fl\r\n                else:\r\n                    a, fl, fr = al, fr, None\r\n            else:\r\n                loss_cache = self.func()\r\n                if loss_cache < self._cache:\r\n                    a, fa = 0.5 * (a + b), self._cache\r\n                else:\r\n                    b, fb = self._alpha, loss_cache\r\n        self._alpha = (a + b) / 2\r\n        return self._alpha, self.counter, self.success_flag, self.func(diff=False), self.func(diff=True)\r\n\r\n    def _init_core(self, gamma, current_f=None, next_f=None):\r\n        """"""\r\n        Core function used in _get_init method\r\n        :param gamma     : Increment of \xce\xb1\r\n        :param current_f : Current loss\r\n        :param next_f    : \'Future\' loss\r\n        :return          : \'Future\' \xce\xb1, current loss, \'Future\' loss\r\n        """"""\r\n        next_alpha = self._alpha + gamma\r\n        if current_f is None:\r\n            current_f = self.func()\r\n        if next_f is None:\r\n            next_f = self.func(next_alpha)\r\n        return next_alpha, current_f, next_f\r\n\r\n    def _get_interval(self, next_alpha, current_f, next_f, success):\r\n        """""" Generate the target interval and corresponding loss and whether success """"""\r\n        if self._alpha <= next_alpha:\r\n            return self._alpha, next_alpha, current_f, next_f, int(success)\r\n        return next_alpha, self._alpha, next_f, current_f, int(success)\r\n\r\n    def _get_init(self, gamma, t):\r\n        """"""\r\n        Method to get the initial interval for line search\r\n        :param gamma : Initial increment of \xce\xb1\r\n        :param t     : Boosting rate of \xce\xb3\r\n        :return      : Target interval and corresponding loss and whether success\r\n        """"""\r\n        next_alpha, current_f, next_f = self._init_core(gamma)\r\n        if next_f >= current_f:\r\n            current_f, next_f = next_f, None\r\n            self._alpha = next_alpha\r\n            gamma *= -1\r\n        for _ in range(self._params[""epoch""]):\r\n            next_alpha, current_f, next_f = self._init_core(gamma, current_f, next_f)\r\n            if next_alpha <= 0:\r\n                return self._get_interval(0, current_f, self._org_loss_cache, next_f >= current_f)\r\n            if next_f >= current_f:\r\n                return self._get_interval(next_alpha, current_f, next_f, 1)\r\n            gamma *= t\r\n            self._alpha = next_alpha\r\n            current_f, next_f = next_f, None\r\n        if next_f is None:\r\n            next_f = self.func()\r\n        return self._get_interval(next_alpha, current_f, next_f, 0)\r\n\r\n\r\nclass Armijo(LineSearch):\r\n    def min(self):\r\n        return self._alpha >= self._params[""floor""]\r\n\r\n    def max(self):\r\n        return self.func() <= self._org_loss_cache + self._params[""rho""] * (\r\n            self._alpha * self._org_grad_cache.dot(self._d)\r\n        )\r\n\r\n\r\nclass Goldstein(LineSearch):\r\n    def min(self):\r\n        self._cache = (self.func(), self._alpha * self._org_grad_cache.dot(self._d))\r\n        return self._cache[0] <= self._org_loss_cache + self._cache[1] * (1 - self._params[""rho""])\r\n\r\n    def max(self):\r\n        return self._cache[0] <= self._org_loss_cache + self._cache[1] * self._params[""rho""]\r\n\r\n\r\nclass Wolfe(LineSearch):\r\n    def min(self):\r\n        self._cache = self._org_grad_cache.dot(self._d)\r\n        return self.func(diff=True).dot(self._d) >= self._cache * self._params[""sigma""]\r\n\r\n    def max(self):\r\n        return self.func() <= self._org_loss_cache + (\r\n            self._cache * self._params[""rho""] * self._alpha\r\n        )\r\n\r\n\r\nclass StrongWolfe(LineSearch):\r\n    def min(self):\r\n        self._cache = self._org_grad_cache.dot(self._d)\r\n        return np.abs(self.func(diff=True).dot(self._d)) <= -self._cache * self._params[""sigma""]\r\n\r\n    def max(self):\r\n        return self.func() <= self._org_loss_cache + (\r\n            self._cache * self._params[""rho""] * self._alpha\r\n        )\r\n\r\n\r\n# Optimizer Frameworks\r\n\r\nclass Optimizer:\r\n    def __init__(self, func, line_search=None, **kwargs):\r\n        """"""\r\n        The framework for general optimizers\r\n        :param func        : Should be a SubClass of class \'Function\' defined in Functions.py\r\n        :param line_search : Should be a SubClass of class \'LineSearch\' defined above\r\n        :param kwargs      : May contain two parameters:\r\n                     epoch : Maximum iteration for optimization  ; default: 1000\r\n                     eps   : Tolerance                           ; default: 1e-8\r\n        """"""\r\n        assert isinstance(func, Function)\r\n        assert line_search is None or isinstance(line_search, LineSearch)\r\n        self._func = func\r\n        self._search = line_search\r\n        self._x, self._d, self._loss_cache, self._grad_cache = func.x0, None, None, None\r\n        self.log = []\r\n        self.feva = self.iter = 0\r\n        self.success = np.zeros(2, dtype=np.int)\r\n        self._params = {\r\n            ""epoch"": kwargs.get(""epoch"", OptConfig.epoch),\r\n            ""eps"": kwargs.get(""eps"", OptConfig.eps),\r\n        }\r\n\r\n    @staticmethod\r\n    def solve(a, y, negative=True):\r\n        """"""\r\n        A wrapper for solving linear system \'ax = y\' using:\r\n            1) np.linalg.solve (if scipy is not available)\r\n            2) cholesky decompose (if scipy is available and a is not sparse)\r\n            3) spsolve (if scipy is available a is sparse)\r\n        If scipy is available and matrix a is not sparse and is not positive definite, LinAlgError will be thrown\r\n        :param a        : \'a\'\r\n        :param y        : \'y\'\r\n        :param negative : If True, we\'ll solve \'-ax = y\' instead\r\n        :return         : \'x\'\r\n        """"""\r\n        if scipy_flag:\r\n            if issparse(a):\r\n                if negative:\r\n                    return sparse_lin.spsolve(-a, y)\r\n                return sparse_lin.spsolve(a, y)\r\n            l = linalg.cholesky(a, lower=True)\r\n            if negative:\r\n                z = linalg.solve_triangular(-l, y, lower=True)\r\n            else:\r\n                z = linalg.solve_triangular(l, y, lower=True)\r\n            return linalg.solve_triangular(l.T, z)\r\n        if negative:\r\n            return np.linalg.solve(-a, y)\r\n        return np.linalg.solve(a, y)\r\n\r\n    def func(self, diff=0):\r\n        """"""\r\n        A wrapper of function calls\r\n        :param diff : Should be 0, 1 or 2\r\n                          0: return loss\r\n                          1: return gradient\r\n                          2: return hessian matrix\r\n        :return     : Loss or gradient or hessian matrix\r\n        """"""\r\n        if diff == 0:\r\n            self.feva += 1\r\n            return self._func.loss(self._x)\r\n        if diff == 1:\r\n            return self._func.grad(self._x)\r\n        return self._func.hessian(self._x)\r\n\r\n    def get_d(self):\r\n        """""" Method that returns the opt direction. Should be overwritten by SubClasses""""""\r\n        return self._d\r\n\r\n    def opt(self, epoch=None, eps=None):\r\n        """"""\r\n        Main procedure of opt\r\n        :param epoch : Maximum iteration ; default: 1000\r\n        :param eps   : Tolerance         ; default: 1e-8\r\n        :return      : x*, f*, n_iter, feva\r\n        """"""\r\n        if epoch is None:\r\n            epoch = self._params[""epoch""]\r\n        if eps is None:\r\n            eps = self._params[""eps""]\r\n        self._func.refresh_cache(self._x)\r\n        self._loss_cache, self._grad_cache = self.func(0), self.func(1)\r\n        bar = ProgressBar(max_value=epoch, name=""Opt"")\r\n        for _ in range(epoch):\r\n            self.iter += 1\r\n            with warnings.catch_warnings():\r\n                warnings.filterwarnings(""error"")\r\n                try:\r\n                    if self._core(eps):\r\n                        break\r\n                    self.log.append(self._loss_cache)\r\n                except RuntimeWarning as err:\r\n                    print(""\\n"", err, ""\\n"")\r\n                    break\r\n                except np.linalg.linalg.LinAlgError as err:\r\n                    print(""\\n"", err, ""\\n"")\r\n                    break\r\n            bar.update()\r\n        bar.update()\r\n        bar.terminate()\r\n        return self._x, self._loss_cache, self.iter, self.feva\r\n\r\n    def _core(self, eps):\r\n        """""" Core method for Optimizer, should return True if terminated """"""\r\n        loss_cache = self._loss_cache\r\n        self._x += self.get_d()\r\n        self._func.refresh_cache(self._x)\r\n        self._loss_cache = self.func(0)\r\n        self._d = None\r\n        if abs(loss_cache - self._loss_cache) < eps or np.linalg.norm(self._grad_cache) < eps:\r\n            return True\r\n\r\n    def _line_search_update(self):\r\n        """""" Proceed line search with x & d """"""\r\n        with warnings.catch_warnings():\r\n            warnings.filterwarnings(""error"")\r\n            try:\r\n                if self._search is not None:\r\n                    alpha, feva, success_flag, self._loss_cache, self._grad_cache = self._search.step(\r\n                        self._x, self.get_d())\r\n                    self.success += success_flag\r\n                elif not scipy_flag:\r\n                    feva, alpha = 0, 1\r\n                else:\r\n                    def f(x):\r\n                        self._func.refresh_cache(x, dtype=""loss"")\r\n                        return self._func.loss(x)\r\n\r\n                    def g(x):\r\n                        self._func.refresh_cache(x)\r\n                        return self._func.grad(x)\r\n\r\n                    alpha, feva, _, self._loss_cache, old_f, self._grad_cache = optimize.line_search(\r\n                        f, g, self._x, self.get_d()\r\n                    )\r\n            except RuntimeWarning:\r\n                feva = 0\r\n                if self._search is not None:\r\n                    alpha = self._search._params[""floor""]\r\n                else:\r\n                    alpha = 0.01\r\n        self._x += alpha * self._d\r\n        self.feva += feva\r\n        self._d = None\r\n\r\n\r\nclass LineSearchOptimizer(Optimizer):\r\n    """""" Optimizer with line search """"""\r\n    def _core(self, eps):\r\n        loss_cache = self._loss_cache\r\n        self._func.refresh_cache(self._x)\r\n        self._line_search_update()\r\n        if abs(loss_cache - self._loss_cache) < eps or np.linalg.norm(self._grad_cache) < eps:\r\n            return True\r\n\r\n\r\n# Gradient Descent Optimizers\r\n\r\nclass GradientDescent(LineSearchOptimizer):\r\n    def get_d(self):\r\n        if self._d is None:\r\n            self._grad_cache = self.func(1)\r\n            self._d = -self._grad_cache\r\n        return self._d\r\n\r\n\r\n# Newton Optimizers\r\n\r\nclass Newton(Optimizer):\r\n    def get_d(self):\r\n        if self._d is None:\r\n            self._grad_cache = self.func(1)\r\n            self._d = Optimizer.solve(self.func(2), self._grad_cache)\r\n        return self._d\r\n\r\n\r\n# Damped Newtons\r\n\r\nclass DampedNewton(Newton, LineSearchOptimizer):\r\n    pass\r\n\r\n\r\nclass MergedNewton(DampedNewton):\r\n    def __init__(self, func, line_search=None, **kwargs):\r\n        super(MergedNewton, self).__init__(func, line_search)\r\n        self._eps1 = kwargs.get(""eps1"", 0.75)\r\n        self._eps2 = kwargs.get(""eps2"", 0.1)\r\n\r\n    # noinspection PyTypeChecker\r\n    def get_d(self):\r\n        if self._d is not None:\r\n            return self._d\r\n        if self._grad_cache is None:\r\n            self._grad_cache = self.func(1)\r\n        try:\r\n            self._d = Optimizer.solve(self.func(2), self.func(1))\r\n            inner_prod, norm_prod = self._grad_cache.dot(self._d), np.linalg.norm(\r\n                self._grad_cache\r\n            ) * np.linalg.norm(self._d)\r\n            if inner_prod > self._eps1 * norm_prod:\r\n                self._d *= -1\r\n            elif abs(inner_prod) <= self._eps2 * norm_prod:\r\n                self._d = -self._grad_cache\r\n        except np.linalg.linalg.LinAlgError:\r\n            self._d = -self._grad_cache\r\n        return self._d\r\n\r\n\r\nclass LM(DampedNewton):\r\n    def __init__(self, func, line_search=None, **kwargs):\r\n        super(LM, self).__init__(func, line_search)\r\n        self._nu = kwargs.get(""nu"", OptConfig.nu)\r\n\r\n    def get_d(self):\r\n        if self._d is None:\r\n            nu = .0\r\n            while 1:\r\n                try:\r\n                    if nu == 0:\r\n                        hessian = self.func(2)\r\n                    else:\r\n                        hessian = self.func(2) + np.diag(np.full(self._func.n, nu))\r\n                    self._d = Optimizer.solve(hessian, self.func(1))\r\n                    break\r\n                except np.linalg.linalg.LinAlgError:\r\n                    if nu == 0:\r\n                        nu = self._nu\r\n                    else:\r\n                        nu *= 2\r\n        return self._d\r\n\r\n\r\n# Quasi Newtons\r\n\r\nclass QuasiNewton(DampedNewton):\r\n    def __init__(self, func, line_search=None, method=""H""):\r\n        """""" self._mat represents \'B\' or \'H\' depends on \'method\' parameter """"""\r\n        super(QuasiNewton, self).__init__(func, line_search)\r\n        self._method, self._mat = method.upper(), None\r\n        self._s = self._y = None\r\n\r\n    def get_d(self):\r\n        if self._d is None:\r\n            if self._method == ""B"":\r\n                self._d = Optimizer.solve(self._mat, self.func(1))\r\n            else:\r\n                self._d = -self._mat.dot(self.func(1))\r\n        return self._d\r\n\r\n    def _core(self, eps):\r\n        if self._mat is None:\r\n            self._mat = np.eye(self._func.n)\r\n        if self._s is None and self._y is None:\r\n            self._s, self._y = -self._x, -self._grad_cache\r\n        loss_cache = self._loss_cache\r\n        super(QuasiNewton, self)._core(eps)\r\n        self._s += self._x\r\n        self._y += self._grad_cache\r\n        self._update_mat()\r\n        self._s, self._y = -self._x, -self._grad_cache\r\n        if abs(loss_cache - self._loss_cache) < eps or np.linalg.norm(self._grad_cache) < eps:\r\n            return True\r\n\r\n    def _update_mat(self):\r\n        """""" Core method for Quasi Newtons, matrix update algorithm should be defined here """"""\r\n        pass\r\n\r\n\r\nclass SR1(QuasiNewton):\r\n    def _update_mat(self):\r\n        if self._method == ""B"":\r\n            cache = (\r\n                self._s - self._mat.dot(self._y),\r\n                self._y\r\n            )\r\n        else:\r\n            cache = (\r\n                self._y - self._mat.dot(self._s),\r\n                self._s\r\n            )\r\n        self._mat += cache[0][..., None].dot(cache[0][None, ...]) / cache[0].dot(cache[1])\r\n\r\n\r\nclass DFP(QuasiNewton):\r\n    def _update_mat(self):\r\n        if self._method == ""B"":\r\n            cache1 = self._s.dot(self._mat)\r\n            cache2 = self._s.dot(self._y)\r\n            y1, y2 = self._y[..., None], self._y[None, ...]\r\n            self._mat += (1 + cache1.dot(self._s) / cache2) * y1.dot(y2) / cache2 - (\r\n                (y1.dot(cache1[None, ...]) + cache1[..., None].dot(y2)) / cache2\r\n            )\r\n        else:\r\n            cache = self._y.dot(self._mat)\r\n            self._mat += self._s[..., None].dot(self._s[None, ...]) / self._s.dot(self._y) - (\r\n                cache[..., None].dot(cache[None, ...])\r\n            ) / cache.dot(self._y)\r\n\r\n\r\nclass BFGS(DFP):\r\n    def _update_mat(self):\r\n        self._method = ""H"" if self._method == ""B"" else ""B""\r\n        self._s, self._y = self._y, self._s\r\n        super(BFGS, self)._update_mat()\r\n        self._s, self._y = self._y, self._s\r\n        self._method = ""H"" if self._method == ""B"" else ""B""\r\n\r\n\r\n# Scipy Optimizer\r\n\r\nclass ScipyOpt:\r\n    def __init__(self, func):\r\n        self._func = func\r\n\r\n    def opt(self):\r\n        if not scipy_flag:\r\n            raise ValueError(""Scipy is not available"")\r\n\r\n        def f(_x):\r\n            self._func.refresh_cache(_x)\r\n            return self._func.loss(_x), self._func.grad(_x)\r\n        result = optimize.minimize(f, self._func.x0, jac=True, method=""L-BFGS-B"", bounds=self._func.bounds)\r\n        return result.x, result.fun, result.nit, result.nfev\r\n\r\n\r\n__all__ = [\r\n    ""Armijo"", ""Goldstein"", ""Wolfe"", ""StrongWolfe"",\r\n    ""GradientDescent"", ""Newton"", ""DampedNewton"", ""MergedNewton"", ""LM"",\r\n    ""SR1"", ""DFP"", ""BFGS"", ""ScipyOpt""\r\n]\r\n'"
Opt/Test.py,0,"b'import matplotlib.pyplot as plt\r\n\r\nfrom i_Clustering.KMeans import KMeans\r\nfrom Opt.Methods import *\r\nfrom Opt.Functions import *\r\nfrom Util.Util import DataUtil\r\nfrom Util.Bases import ClassifierBase, RegressorBase\r\n\r\n\r\n# Example1: Use GradientDescent & Wolfe to minimize l2_norm of (x^2 + 1)^2 using ""Automatic Differentiation""\r\nclass Test(Function):\r\n    def loss(self, x):\r\n        return np.linalg.norm((x ** 2 + 1) ** 2)\r\n\r\nfunc, method = Test(1024), GradientDescent\r\n_x, f, n_iter, n_feva = method(func, Wolfe(func)).opt()\r\nprint(f, n_iter, n_feva)\r\nprint(np.linalg.norm(_x))\r\n\r\n\r\n# Example2: Logistic Regression with LM & scipy\'s Strong Wolfe\r\n# However this naive version will sometimes encounter overflow / underflow in exp\r\nclass LogisticRegression(Function):\r\n    def __init__(self, n, x, y, **kwargs):\r\n        super(LogisticRegression, self).__init__(n, **kwargs)\r\n        self._x, self._y = np.atleast_2d(x), np.array(y)\r\n        self._beta = self._pi = None\r\n        self._dot_cache = self._exp_dot_cache = None\r\n\r\n    @property\r\n    def x0(self):\r\n        """""" Use some trick to initialize beta """"""\r\n        x, y = self._x, self._y\r\n        z = np.log((y + 0.5) / (1.5 - y))\r\n        d = np.diag(6 / ((y + 1) * (2 - y)))\r\n        self._beta = np.linalg.inv(x.T.dot(d).dot(x)).dot(x.T).dot(np.linalg.inv(d)).dot(z)\r\n        return self._beta\r\n\r\n    def refresh_cache(self, x, dtype=""all""):\r\n        self._beta = x\r\n        self._dot_cache = self._x.dot(x)\r\n        self._exp_dot_cache = np.exp(self._dot_cache)\r\n        self._pi = self._exp_dot_cache / (1 + self._exp_dot_cache)\r\n\r\n    def loss(self, x):\r\n        return np.log(1 + self._exp_dot_cache).sum() - self._dot_cache.dot(self._y)\r\n\r\n    def grad(self, x):\r\n        return self._x.T.dot(self._pi - self._y)\r\n\r\n    def hessian(self, x):\r\n        grad = self._pi * (1 - self._pi)  # type: np.ndarray\r\n        return self._x.T.dot(np.diag(grad)).dot(self._x)\r\n\r\n\r\nclass LR(ClassifierBase):\r\n    def __init__(self, opt, line_search=None, **kwargs):\r\n        super(LR, self).__init__(**kwargs)\r\n        self._func = LogisticRegression\r\n        self._beta = self._func_cache = None\r\n        self._line_search = line_search\r\n        self._opt_cache = None\r\n        self._opt = opt\r\n\r\n    def fit(self, x, y):\r\n        self._func_cache = self._func(x.shape[1], x, y)\r\n        line_search = None if self._line_search is None else self._line_search(self._func_cache)\r\n        self._opt_cache = self._opt(self._func_cache, line_search)\r\n        self._opt_cache.opt()\r\n        self._beta = self._func_cache._beta\r\n\r\n    def predict(self, x, get_raw_results=False, **kwargs):\r\n        pi = 1 / (1 + np.exp(-np.atleast_2d(x).dot(self._beta)))\r\n        if get_raw_results:\r\n            return pi\r\n        return (pi >= 0.5).astype(np.double)\r\n\r\ndata, labels = DataUtil.gen_two_clusters(one_hot=False)\r\nlr = LR(LM)\r\nlr.fit(data, labels)\r\nlr.evaluate(data, labels)\r\nlr[""func_cache""].refresh_cache(lr[""beta""])\r\nprint(""Loss:"", lr[""func_cache""].loss(lr[""beta""]))\r\nlr.visualize2d(data, labels, dense=400)\r\n# Draw Training Curve\r\nplt.figure()\r\nplt.plot(np.arange(len(lr[""opt_cache""].log))+1, lr[""opt_cache""].log)\r\nplt.show()\r\n\r\n\r\n# Example3: RBFN Regression with BFGS & Armijo using ""Automatic Differentiation""\r\n# ""RBFN"" represents ""Radial Basis Function Network"". Typically, we use Gaussian Function for this example\r\nclass RBFN(Function):\r\n    def __init__(self, mat, y, mat_cv, y_cv, centers, x0=10, n=1, **kwargs):\r\n        super(RBFN, self).__init__(n, **kwargs)\r\n        self._mat_high_dim = np.atleast_2d(mat)[:, None, ...]\r\n        self._mat_cv_high_dim = np.atleast_2d(mat_cv)[:, None, ...]\r\n        self._y, self._y_cv = np.array(y), np.array(y_cv)\r\n        self._centers = centers\r\n        self._cache = None\r\n        self._x0 = x0\r\n        self.n = 1\r\n\r\n    @property\r\n    def x0(self):\r\n        return np.array([self._x0], dtype=np.double)\r\n\r\n    def refresh_cache(self, x, dtype=""all""):\r\n        k_mat = np.sum(np.exp(-(self._mat_high_dim - self._centers) ** 2 / x), axis=2)\r\n        k_mat_cv = np.sum(np.exp(-(self._mat_cv_high_dim - self._centers) ** 2 / x), axis=2)\r\n        self._cache = k_mat, k_mat_cv, np.linalg.lstsq(k_mat, self._y)[0]\r\n\r\n    def loss(self, x):\r\n        k_mat, k_mat_cv, alpha = self._cache\r\n        return np.linalg.norm(k_mat_cv.dot(alpha) - self._y_cv)\r\n\r\n\r\nclass RBFNRegressor(RegressorBase):\r\n    def __init__(self, opt, line_search=None, **kwargs):\r\n        super(RBFNRegressor, self).__init__(**kwargs)\r\n        self._func = RBFN\r\n        self._sigma2 = self._centers = None\r\n        self._line_search = line_search\r\n        self._opt_cache = self._func_cache = None\r\n        self._opt = opt\r\n\r\n        self._params[""n_centers""] = kwargs.get(""n_centers"", None)\r\n        self._params[""n_centers_rate""] = kwargs.get(""n_centers_rate"", 0.75)\r\n\r\n    def rbf(self, x, s):\r\n        return np.sum(np.exp(-(x[:, None, ...] - self._centers) ** 2 / s), axis=2)\r\n\r\n    def fit(self, x, y, x_cv, y_cv, n_centers=None, n_centers_rate=None):\r\n        if n_centers is None:\r\n            if self._params[""n_centers""] is not None:\r\n                n_centers = int(self._params[""n_centers""])\r\n            else:\r\n                if n_centers_rate is None:\r\n                    n_centers_rate = self._params[""n_centers_rate""]\r\n                n_centers = int(n_centers_rate * len(x))\r\n        k_means = KMeans(n_clusters=n_centers)\r\n        k_means.fit(x)\r\n        self._centers = k_means[""centers""]\r\n        self._func_cache = self._func(x, y, x_cv, y_cv, self._centers)\r\n        line_search = None if self._line_search is None else self._line_search(self._func_cache)\r\n        self._opt_cache = self._opt(self._func_cache, line_search)\r\n        self._sigma2 = self._opt_cache.opt()[0]\r\n\r\n    def predict(self, x, get_raw_results=False):\r\n        alpha = self._func_cache._cache[-1]\r\n        k_mat = self.rbf(x, self._sigma2)\r\n        return k_mat.dot(alpha)\r\n\r\ndata = np.linspace(0, 20, 100)  # type: np.ndarray\r\nlabels = data ** 2 + np.random.rand(100) * 25\r\nindices = np.random.permutation(len(data))\r\ndata, labels = data[..., None][indices], labels[indices]\r\nrbfn = RBFNRegressor(BFGS, Armijo)\r\ncv_idx, test_idx = 70, 85\r\nrbfn.fit(data[:cv_idx], labels[:cv_idx], data[cv_idx:test_idx], labels[cv_idx:test_idx])\r\nrbfn.visualize2d(data, labels, padding=1)\r\n\r\n\r\n# Example4: Naive 3-Layer Neural Network with GradientDescent & Wolfe using ""Automatic Differentiation""\r\n# Structure: Input -> ReLU (12 units) -> Sigmoid + Cross Entropy (no bias)\r\nclass NNFunc(Function):\r\n    def __init__(self, x, y, n_hidden=12, **kwargs):\r\n        self._x, self._y = np.atleast_2d(x), np.array(y)\r\n        if len(self._y.shape) == 1:\r\n            self._y = np.array(self._y[..., None] == np.arange(np.max(self._y)), dtype=np.double)\r\n        n_input, n_output = self._x.shape[1], self._y.shape[1]\r\n        super(NNFunc, self).__init__(n_input * n_hidden + n_hidden * n_output, **kwargs)\r\n        self._dims, self._cut_idx = (n_input, n_hidden, n_output), n_input * n_hidden\r\n        self.mat1 = self.mat2 = None\r\n\r\n    def loss(self, x):\r\n        mat1 = x[:self._cut_idx].reshape(self._dims[:2])\r\n        mat2 = x[self._cut_idx:].reshape(self._dims[1:])\r\n        y_raw = np.maximum(0, self._x.dot(mat1)).dot(mat2)\r\n        y_pred = 1 / (1 + np.exp(-y_raw))\r\n        self.mat1, self.mat2 = mat1, mat2\r\n        return np.average(\r\n            -self._y * np.log(np.maximum(y_pred, 1e-12)) - (1 - self._y) * np.log(np.maximum(1 - y_pred, 1e-12)))\r\n\r\n\r\nclass NN(ClassifierBase):\r\n    def __init__(self, opt, line_search=None, **kwargs):\r\n        super(NN, self).__init__(**kwargs)\r\n        self._func = NNFunc\r\n        self._mat1 = self._mat2 = None\r\n        self._line_search = line_search\r\n        self._opt_cache = self._func_cache = None\r\n        self._opt = opt\r\n\r\n        self._params[""n_hidden""] = kwargs.get(""n_hidden"", 24)\r\n        self._params[""use_scipy""] = kwargs.get(""use_scipy"", False)\r\n\r\n    def fit(self, x, y, n_hidden=None, use_scipy=None):\r\n        if n_hidden is None:\r\n            n_hidden = self._params[""n_hidden""]\r\n        if use_scipy is None:\r\n            use_scipy = self._params[""use_scipy""]\r\n        self._func_cache = self._func(x, y, n_hidden)\r\n        if not use_scipy:\r\n            line_search = None if self._line_search is None else self._line_search(self._func_cache)\r\n            self._opt_cache = self._opt(self._func_cache, line_search)\r\n        else:\r\n            self._opt_cache = ScipyOpt(self._func_cache)\r\n        self._opt_cache.opt()\r\n        self._mat1, self._mat2 = self._func_cache.mat1, self._func_cache.mat2\r\n\r\n    def predict(self, x, get_raw_results=False, **kwargs):\r\n        y_raw = np.maximum(0, np.atleast_2d(x).dot(self._mat1)).dot(self._mat2)\r\n        y_pred = 1 / (1 + np.exp(-y_raw))  # type: np.ndarray\r\n        if get_raw_results:\r\n            return y_pred\r\n        return np.argmax(y_pred, axis=1)\r\n\r\ndata, labels = DataUtil.gen_xor()\r\nnn = NN(GradientDescent, Wolfe)\r\nnn.fit(data, labels)\r\nnn.evaluate(data, labels)\r\nnn.visualize2d(data, labels)\r\n# Draw Training Curve\r\nplt.figure()\r\nplt.plot(np.arange(len(nn[""opt_cache""].log))+1, nn[""opt_cache""].log)\r\nplt.show()\r\n'"
RNN/Cell.py,7,"b'import tensorflow as tf\nimport tensorflow.contrib.layers as layers\nfrom tensorflow.contrib.rnn import LSTMStateTuple\n\n\nclass LSTMCell(tf.contrib.rnn.BasicRNNCell):\n    def __call__(self, x, state, scope=""LSTM""):\n        with tf.variable_scope(scope):\n            s_old, h_old = state\n            gates = layers.fully_connected(\n                tf.concat([x, s_old], 1),\n                num_outputs=4 * self._num_units,\n                activation_fn=None)\n            r1, g1, g2, g3 = tf.split(gates, 4, 1)\n            r1, g1, g3 = tf.nn.sigmoid(r1), tf.nn.sigmoid(g1), tf.nn.sigmoid(g3)\n            g2 = tf.nn.tanh(g2)\n            h_new = h_old * r1 + g1 * g2\n            s_new = tf.nn.tanh(h_new) * g3\n            return s_new, LSTMStateTuple(s_new, h_new)\n\n    @property\n    def state_size(self):\n        return LSTMStateTuple(self._num_units, self._num_units)\n'"
RNN/Generator.py,0,"b'import numpy as np\n\n\n# Generator Framework\nclass Generator:\n    def __init__(self, im=None, om=None, **kwargs):\n        self._im, self._om = im, om\n        self._x_train = self._x_test = None\n        self._y_train = self._y_test = None\n\n    def gen(self, batch, test=False, **kwargs):\n        if batch == 0:\n            if test:\n                return self._x_test, self._y_test\n            return self._x_train, self._y_train\n        batch = np.random.choice(len(self._x_train), batch)\n        return self._x_train[batch], self._y_train[batch]\n'"
RNN/Wrapper.py,21,"b'import numpy as np\r\nimport matplotlib.pyplot as plt\r\n\r\nfrom RNN.Cell import *\r\nfrom g_CNN.Optimizers import OptFactory\r\nfrom Util.ProgressBar import ProgressBar\r\n\r\n\r\nclass RNNWrapper:\r\n    def __init__(self, **kwargs):\r\n        self._log = {}\r\n        self._optimizer = self._generator = None\r\n        self._tfx = self._tfy = self._input = self._output = None\r\n        self._cell = self._initial_state = self._sequence_lengths = None\r\n        self._im = self._om = self._activation = None\r\n        self._sess = tf.Session()\r\n\r\n        self._squeeze = kwargs.get(""squeeze"", False)\r\n        self._use_sparse_labels = kwargs.get(""use_sparse_labels"", False)\r\n        self._embedding_size = kwargs.get(""embedding_size"", None)\r\n        self._use_final_state = kwargs.get(""use_final_state"", False)\r\n        self._params = {\r\n            ""cell"": kwargs.get(""cell"", LSTMCell),\r\n            ""provide_sequence_length"": kwargs.get(""provide_sequence_length"", False),\r\n            ""n_hidden"": kwargs.get(""n_hidden"", 128),\r\n            ""n_history"": kwargs.get(""n_history"", 0),\r\n            ""activation"": kwargs.get(""activation"", tf.nn.sigmoid),\r\n            ""lr"": kwargs.get(""lr"", 0.01),\r\n            ""epoch"": kwargs.get(""epoch"", 25),\r\n            ""n_iter"": kwargs.get(""n_iter"", 128),\r\n            ""optimizer"": kwargs.get(""optimizer"", ""Adam""),\r\n            ""batch_size"": kwargs.get(""batch_size"", 64),\r\n            ""eps"": kwargs.get(""eps"", 1e-8),\r\n            ""verbose"": kwargs.get(""verbose"", 1)\r\n        }\r\n\r\n        if self._use_sparse_labels:\r\n            self._params[""activation""] = kwargs.get(""activation"", None)\r\n        if self._squeeze:\r\n            self._params[""n_history""] = kwargs.get(""n_history"", 1)\r\n\r\n    def _verbose(self):\r\n        if self._sequence_lengths is not None:\r\n            x_test, y_test, sequence_lengths = self._generator.gen(0, True)\r\n        else:\r\n            x_test, y_test = self._generator.gen(0, True)\r\n            sequence_lengths = None\r\n        axis = 1 if self._squeeze else 2\r\n        if self._use_sparse_labels:\r\n            y_true = y_test\r\n        else:\r\n            y_true = np.argmax(y_test, axis=axis).ravel()  # type: np.ndarray\r\n        y_pred = self.predict(x_test, sequence_lengths)\r\n        print(""Test acc: {:8.6} %"".format(np.mean(y_true == y_pred) * 100))\r\n\r\n    def _define_input(self, im, om):\r\n        if self._embedding_size:\r\n            self._tfx = tf.placeholder(tf.int32, shape=[None, None])\r\n            embeddings = tf.Variable(tf.random_uniform([im, self._embedding_size], -1.0, 1.0))\r\n            self._input = tf.nn.embedding_lookup(embeddings, self._tfx)\r\n        else:\r\n            self._input = self._tfx = tf.placeholder(tf.float32, shape=[None, None, im])\r\n        if self._use_sparse_labels:\r\n            if self._squeeze:\r\n                self._tfy = tf.placeholder(tf.int32, shape=[None])\r\n            else:\r\n                self._tfy = tf.placeholder(tf.int32, shape=[None, None])\r\n        elif self._squeeze:\r\n            self._tfy = tf.placeholder(tf.float32, shape=[None, om])\r\n        else:\r\n            self._tfy = tf.placeholder(tf.float32, shape=[None, None, om])\r\n\r\n    def _prepare_for_dynamic_rnn(self, provide_sequence_length):\r\n        self._initial_state = self._cell.zero_state(tf.shape(self._input)[0], tf.float32)\r\n        if provide_sequence_length:\r\n            self._sequence_lengths = tf.placeholder(tf.int32, [None])\r\n        else:\r\n            self._sequence_lengths = None\r\n\r\n    def _get_loss(self, eps):\r\n        if self._use_sparse_labels:\r\n            return tf.reduce_mean(\r\n                tf.nn.sparse_softmax_cross_entropy_with_logits(labels=self._tfy, logits=self._output)\r\n            )\r\n        return -tf.reduce_mean(\r\n            self._tfy * tf.log(self._output + eps) + (1 - self._tfy) * tf.log(1 - self._output + eps)\r\n        )\r\n\r\n    def _get_output(self, rnn_outputs, rnn_final_state, n_history):\r\n        if n_history == 0 and self._squeeze:\r\n            raise ValueError(""\'n_history\' should not be 0 when trying to squeeze the outputs"")\r\n        if n_history == 1 and self._squeeze:\r\n            outputs = rnn_outputs[..., -1, :]\r\n        else:\r\n            outputs = rnn_outputs[..., -n_history:, :]\r\n            if self._use_final_state:\r\n                outputs = tf.concat([outputs, rnn_final_state[..., -n_history:, :]], axis=2)\r\n            if self._squeeze:\r\n                outputs = tf.reshape(outputs, [-1, n_history * int(outputs.get_shape()[2])])\r\n        # if self._use_final_state and self._squeeze:\r\n        #     outputs = tf.concat([outputs, rnn_final_state[0]], axis=1)\r\n        self._output = layers.fully_connected(\r\n            outputs, num_outputs=self._om, activation_fn=self._activation)\r\n\r\n    def fit(self, im, om, generator, cell=None, provide_sequence_length=None,\r\n            squeeze=None, use_sparse_labels=None, embedding_size=None, use_final_state=None,\r\n            n_hidden=None, n_history=None, activation=None, lr=None, epoch=None, n_iter=None,\r\n            batch_size=None, optimizer=None, eps=None, verbose=None):\r\n        if cell is None:\r\n            cell = self._params[""cell""]\r\n        if provide_sequence_length is None:\r\n            provide_sequence_length = self._params[""provide_sequence_length""]\r\n        if n_hidden is None:\r\n            n_hidden = self._params[""n_hidden""]\r\n        if n_history is None:\r\n            n_history = self._params[""n_history""]\r\n        if squeeze:\r\n            self._squeeze = True\r\n        if use_sparse_labels:\r\n            self._use_sparse_labels = True\r\n        if self._squeeze and n_history == 0:\r\n            n_history = 1\r\n        if embedding_size:\r\n            self._embedding_size = embedding_size\r\n        if use_final_state:\r\n            self._use_final_state = True\r\n        if activation is None:\r\n            activation = self._params[""activation""]\r\n        if lr is None:\r\n            lr = self._params[""lr""]\r\n        if epoch is None:\r\n            epoch = self._params[""epoch""]\r\n        if n_iter is None:\r\n            n_iter = self._params[""n_iter""]\r\n        if optimizer is None:\r\n            optimizer = self._params[""optimizer""]\r\n        if batch_size is None:\r\n            batch_size = self._params[""batch_size""]\r\n        if eps is None:\r\n            eps = self._params[""eps""]\r\n        if verbose is None:\r\n            verbose = self._params[""verbose""]\r\n\r\n        self._generator = generator\r\n        self._im, self._om, self._activation = im, om, activation\r\n        self._optimizer = OptFactory().get_optimizer_by_name(optimizer, lr)\r\n        self._define_input(im, om)\r\n\r\n        self._cell = cell(n_hidden)\r\n        self._prepare_for_dynamic_rnn(provide_sequence_length)\r\n        rnn_outputs, rnn_final_state = tf.nn.dynamic_rnn(\r\n            self._cell, self._input, return_all_states=True,\r\n            sequence_length=self._sequence_lengths, initial_state=self._initial_state\r\n        )\r\n        self._get_output(rnn_outputs, rnn_final_state, n_history)\r\n        loss = self._get_loss(eps)\r\n        train_step = self._optimizer.minimize(loss)\r\n        self._log[""iter_err""] = []\r\n        self._log[""epoch_err""] = []\r\n        self._sess.run(tf.global_variables_initializer())\r\n        bar = ProgressBar(max_value=epoch, name=""Epoch"", start=False)\r\n        if verbose >= 2:\r\n            bar.start()\r\n        for _ in range(epoch):\r\n            epoch_err = 0\r\n            sub_bar = ProgressBar(max_value=n_iter, name=""Iter"", start=False)\r\n            if verbose >= 2:\r\n                sub_bar.start()\r\n            for __ in range(n_iter):\r\n                if provide_sequence_length:\r\n                    x_batch, y_batch, sequence_length = self._generator.gen(batch_size)\r\n                    feed_dict = {\r\n                        self._tfx: x_batch, self._tfy: y_batch,\r\n                        self._sequence_lengths: sequence_length\r\n                    }\r\n                else:\r\n                    x_batch, y_batch = self._generator.gen(batch_size)\r\n                    feed_dict = {self._tfx: x_batch, self._tfy: y_batch}\r\n                iter_err = self._sess.run([loss, train_step], feed_dict)[0]\r\n                self._log[""iter_err""].append(iter_err)\r\n                epoch_err += iter_err\r\n                if verbose >= 2:\r\n                    sub_bar.update()\r\n            self._log[""epoch_err""].append(epoch_err / n_iter)\r\n            if verbose >= 1:\r\n                self._verbose()\r\n                if verbose >= 2:\r\n                    bar.update()\r\n\r\n    def predict(self, x, sequence_lengths=None, get_raw=False):\r\n        x = np.atleast_3d(x)\r\n        if self._sequence_lengths is not None:\r\n            if sequence_lengths is None:\r\n                sequence_lengths = [x.shape[1]] * x.shape[0]\r\n            feed_dict = {self._tfx: x, self._sequence_lengths: sequence_lengths}\r\n        else:\r\n            feed_dict = {self._tfx: x}\r\n        output = self._sess.run(self._output, feed_dict)\r\n        if get_raw:\r\n            return output\r\n        axis = 1 if self._squeeze else 2\r\n        return np.argmax(output, axis=axis).ravel()\r\n\r\n    def draw_err_logs(self):\r\n        ee, ie = self._log[""epoch_err""], self._log[""iter_err""]\r\n        ee_base = np.arange(len(ee))\r\n        ie_base = np.linspace(0, len(ee) - 1, len(ie))\r\n        plt.figure()\r\n        plt.plot(ie_base, ie, label=""Iter error"")\r\n        plt.plot(ee_base, ee, linewidth=3, label=""Epoch error"")\r\n        plt.legend()\r\n        plt.show()\r\n'"
Util/Bases.py,1,"b'import io\r\nimport cv2\r\nimport time\r\nimport math\r\nimport ctypes\r\nimport multiprocessing\r\nimport numpy as np\r\nimport tensorflow as tf\r\nimport matplotlib.pyplot as plt\r\nfrom PIL import Image\r\nfrom multiprocessing import Pool\r\nfrom mpl_toolkits.mplot3d import Axes3D\r\n\r\nfrom NN.Basic.Optimizers import OptFactory\r\n\r\nfrom Util.Util import VisUtil\r\nfrom Util.Timing import Timing\r\nfrom Util.ProgressBar import ProgressBar\r\n\r\ntry:\r\n    import torch\r\n    from torch.autograd import Variable\r\nexcept ImportError:\r\n    torch = Variable = None\r\n\r\n\r\nclass TimingBase:\r\n    def show_timing_log(self):\r\n        pass\r\n\r\n\r\nclass ModelBase:\r\n    """"""\r\n        Base for all models\r\n        Magic methods:\r\n            1) __str__     : return self.name; __repr__ = __str__\r\n            2) __getitem__ : access to protected members\r\n        Properties:\r\n            1) name  : name of this model, self.__class__.__name__ or self._name\r\n            2) title : used in matplotlib (plt.title())\r\n        Static method:\r\n            1) disable_timing  : disable Timing()\r\n            2) show_timing_log : show Timing() records\r\n    """"""\r\n\r\n    clf_timing = Timing()\r\n\r\n    def __init__(self, **kwargs):\r\n        self._plot_label_dict = {}\r\n        self._title = self._name = None\r\n        self._metrics, self._available_metrics = [], {\r\n            ""acc"": ClassifierBase.acc\r\n        }\r\n        self._params = {\r\n            ""sample_weight"": kwargs.get(""sample_weight"", None)\r\n        }\r\n\r\n    def __str__(self):\r\n        return self.name\r\n\r\n    def __repr__(self):\r\n        return str(self)\r\n\r\n    def __getitem__(self, item):\r\n        if isinstance(item, str):\r\n            return getattr(self, ""_"" + item)\r\n\r\n    @property\r\n    def name(self):\r\n        return self.__class__.__name__ if self._name is None else self._name\r\n\r\n    @property\r\n    def title(self):\r\n        return str(self) if self._title is None else self._title\r\n\r\n    @staticmethod\r\n    def disable_timing():\r\n        ModelBase.clf_timing.disable()\r\n\r\n    @staticmethod\r\n    def show_timing_log(level=2):\r\n        ModelBase.clf_timing.show_timing_log(level)\r\n\r\n    # Handle animation\r\n\r\n    @staticmethod\r\n    def _refresh_animation_params(animation_params):\r\n        animation_params[""show""] = animation_params.get(""show"", False)\r\n        animation_params[""mp4""] = animation_params.get(""mp4"", False)\r\n        animation_params[""period""] = animation_params.get(""period"", 1)\r\n\r\n    def _get_animation_params(self, animation_params):\r\n        if animation_params is None:\r\n            animation_params = self._params[""animation_params""]\r\n        else:\r\n            ClassifierBase._refresh_animation_params(animation_params)\r\n        show, mp4, period = animation_params[""show""], animation_params[""mp4""], animation_params[""period""]\r\n        return show or mp4, show, mp4, period, animation_params\r\n\r\n    def _handle_animation(self, i, x, y, ims, animation_params, draw_ani, show_ani, make_mp4, ani_period,\r\n                          name=None, img=None):\r\n        if draw_ani and x.shape[1] == 2 and (i + 1) % ani_period == 0:\r\n            if img is None:\r\n                img = self.get_2d_plot(x, y, **animation_params)\r\n            if name is None:\r\n                name = str(self)\r\n            if show_ani:\r\n                cv2.imshow(name, img)\r\n                cv2.waitKey(1)\r\n            if make_mp4:\r\n                ims.append(img)\r\n\r\n    def _handle_mp4(self, ims, animation_properties, name=None):\r\n        if name is None:\r\n            name = str(self)\r\n        if animation_properties[2] and ims:\r\n            VisUtil.make_mp4(ims, name)\r\n\r\n    def get_2d_plot(self, x, y, padding=1, dense=200, draw_background=False, emphasize=None, extra=None, **kwargs):\r\n        pass\r\n\r\n    # Visualization\r\n\r\n    def scatter2d(self, x, y, padding=0.5, title=None):\r\n        axis, labels = np.asarray(x).T, np.asarray(y)\r\n\r\n        print(""="" * 30 + ""\\n"" + str(self))\r\n        x_min, x_max = np.min(axis[0]), np.max(axis[0])\r\n        y_min, y_max = np.min(axis[1]), np.max(axis[1])\r\n        x_padding = max(abs(x_min), abs(x_max)) * padding\r\n        y_padding = max(abs(y_min), abs(y_max)) * padding\r\n        x_min -= x_padding\r\n        x_max += x_padding\r\n        y_min -= y_padding\r\n        y_max += y_padding\r\n\r\n        if labels.ndim == 1:\r\n            if not self._plot_label_dict:\r\n                self._plot_label_dict = {c: i for i, c in enumerate(set(labels))}\r\n            dic = self._plot_label_dict\r\n            n_label = len(dic)\r\n            labels = np.array([dic[label] for label in labels])\r\n        else:\r\n            n_label = labels.shape[1]\r\n            labels = np.argmax(labels, axis=1)\r\n        colors = plt.cm.rainbow([i / n_label for i in range(n_label)])[labels]\r\n\r\n        if title is None:\r\n            title = self.title\r\n\r\n        indices = [labels == i for i in range(np.max(labels) + 1)]\r\n        scatters = []\r\n        plt.figure()\r\n        plt.title(title)\r\n        for idx in indices:\r\n            scatters.append(plt.scatter(axis[0][idx], axis[1][idx], c=colors[idx]))\r\n        plt.legend(scatters, [""$c_{}$"".format(""{"" + str(i) + ""}"") for i in range(len(scatters))],\r\n                   ncol=math.ceil(math.sqrt(len(scatters))), fontsize=8)\r\n        plt.xlim(x_min, x_max)\r\n        plt.ylim(y_min, y_max)\r\n        plt.show()\r\n\r\n        print(""Done."")\r\n\r\n    def scatter3d(self, x, y, padding=0.1, title=None):\r\n        axis, labels = np.asarray(x).T, np.asarray(y)\r\n\r\n        print(""="" * 30 + ""\\n"" + str(self))\r\n        x_min, x_max = np.min(axis[0]), np.max(axis[0])\r\n        y_min, y_max = np.min(axis[1]), np.max(axis[1])\r\n        z_min, z_max = np.min(axis[2]), np.max(axis[2])\r\n        x_padding = max(abs(x_min), abs(x_max)) * padding\r\n        y_padding = max(abs(y_min), abs(y_max)) * padding\r\n        z_padding = max(abs(z_min), abs(z_max)) * padding\r\n        x_min -= x_padding\r\n        x_max += x_padding\r\n        y_min -= y_padding\r\n        y_max += y_padding\r\n        z_min -= z_padding\r\n        z_max += z_padding\r\n\r\n        def transform_arr(arr):\r\n            if arr.ndim == 1:\r\n                dic = {c: i for i, c in enumerate(set(arr))}\r\n                n_dim = len(dic)\r\n                arr = np.array([dic[label] for label in arr])\r\n            else:\r\n                n_dim = arr.shape[1]\r\n                arr = np.argmax(arr, axis=1)\r\n            return arr, n_dim\r\n\r\n        if title is None:\r\n            try:\r\n                title = self.title\r\n            except AttributeError:\r\n                title = str(self)\r\n\r\n        labels, n_label = transform_arr(labels)\r\n        colors = plt.cm.rainbow([i / n_label for i in range(n_label)])[labels]\r\n        indices = [labels == i for i in range(n_label)]\r\n        scatters = []\r\n        fig = plt.figure()\r\n        plt.title(title)\r\n        ax = fig.add_subplot(111, projection=\'3d\')\r\n        for _index in indices:\r\n            scatters.append(ax.scatter(axis[0][_index], axis[1][_index], axis[2][_index], c=colors[_index]))\r\n        ax.legend(scatters, [""$c_{}$"".format(""{"" + str(i) + ""}"") for i in range(len(scatters))],\r\n                  ncol=math.ceil(math.sqrt(len(scatters))), fontsize=8)\r\n        plt.show()\r\n\r\n    # Util\r\n\r\n    def predict(self, x, get_raw_results=False, **kwargs):\r\n        pass\r\n\r\n\r\nclass ClassifierBase(ModelBase):\r\n    """"""\r\n        Base for classifiers\r\n        Static methods:\r\n            1) acc, f1_score           : Metrics\r\n            2) _multi_clf, _multi_data : Parallelization\r\n    """"""\r\n\r\n    clf_timing = Timing()\r\n\r\n    def __init__(self, **kwargs):\r\n        super(ClassifierBase, self).__init__(**kwargs)\r\n        self._params[""animation_params""] = kwargs.get(""animation_params"", {})\r\n        ClassifierBase._refresh_animation_params(self._params[""animation_params""])\r\n\r\n    # Metrics\r\n\r\n    @staticmethod\r\n    def acc(y, y_pred, weights=None):\r\n        y, y_pred = np.asarray(y), np.asarray(y_pred)\r\n        if weights is not None:\r\n            return np.average((y == y_pred) * weights)\r\n        return np.average(y == y_pred)\r\n\r\n    # noinspection PyTypeChecker\r\n    @staticmethod\r\n    def f1_score(y, y_pred):\r\n        tp = np.sum(y * y_pred)\r\n        if tp == 0:\r\n            return .0\r\n        fp = np.sum((1 - y) * y_pred)\r\n        fn = np.sum(y * (1 - y_pred))\r\n        return 2 * tp / (2 * tp + fn + fp)\r\n\r\n    # Parallelization\r\n\r\n    # noinspection PyUnusedLocal\r\n    @staticmethod\r\n    def _multi_clf(x, clfs, task, kwargs, stack=np.vstack, target=""single""):\r\n        if target != ""parallel"":\r\n            return np.array([clf.predict(x) for clf in clfs], dtype=np.float32).T\r\n        n_cores = kwargs.get(""n_cores"", 2)\r\n        n_cores = multiprocessing.cpu_count() if n_cores <= 0 else n_cores\r\n        if n_cores == 1:\r\n            matrix = np.array([clf.predict(x, n_cores=1) for clf in clfs], dtype=np.float32).T\r\n        else:\r\n            pool = Pool(processes=n_cores)\r\n            batch_size = int(len(clfs) / n_cores)\r\n            clfs = [clfs[i*batch_size:(i+1)*batch_size] for i in range(n_cores)]\r\n            x_size = np.prod(x.shape)  # type: int\r\n            shared_base = multiprocessing.Array(ctypes.c_float, int(x_size))\r\n            shared_matrix = np.ctypeslib.as_array(shared_base.get_obj()).reshape(x.shape)\r\n            shared_matrix[:] = x\r\n            matrix = stack(\r\n                pool.map(task, ((shared_matrix, clfs, n_cores) for clfs in clfs))\r\n            ).T.astype(np.float32)\r\n        return matrix\r\n\r\n    # noinspection PyUnusedLocal\r\n    def _multi_data(self, x, task, kwargs, stack=np.hstack, target=""single""):\r\n        if target != ""parallel"":\r\n            return task((x, self, 1))\r\n        n_cores = kwargs.get(""n_cores"", 2)\r\n        n_cores = multiprocessing.cpu_count() if n_cores <= 0 else n_cores\r\n        if n_cores == 1:\r\n            matrix = task((x, self, n_cores))\r\n        else:\r\n            pool = Pool(processes=n_cores)\r\n            batch_size = int(len(x) / n_cores)\r\n            batch_base, batch_data, cursor = [], [], 0\r\n            x_dim = x.shape[1]\r\n            for i in range(n_cores):\r\n                if i == n_cores - 1:\r\n                    batch_data.append(x[cursor:])\r\n                    batch_base.append(multiprocessing.Array(ctypes.c_float, (len(x) - cursor) * x_dim))\r\n                else:\r\n                    batch_data.append(x[cursor:cursor + batch_size])\r\n                    batch_base.append(multiprocessing.Array(ctypes.c_float, batch_size * x_dim))\r\n                cursor += batch_size\r\n            shared_arrays = [\r\n                np.ctypeslib.as_array(shared_base.get_obj()).reshape(-1, x_dim)\r\n                for shared_base in batch_base\r\n            ]\r\n            for i, data in enumerate(batch_data):\r\n                shared_arrays[i][:] = data\r\n            matrix = stack(\r\n                pool.map(task, ((x, self, n_cores) for x in shared_arrays))\r\n            )\r\n        return matrix.astype(np.float32)\r\n\r\n    # Training\r\n\r\n    @staticmethod\r\n    def _get_train_repeat(x, batch_size):\r\n        train_len = len(x)\r\n        batch_size = min(batch_size, train_len)\r\n        do_random_batch = train_len > batch_size\r\n        return 1 if not do_random_batch else int(train_len / batch_size) + 1\r\n\r\n    def _batch_work(self, *args):\r\n        pass\r\n\r\n    def _batch_training(self, x, y, batch_size, train_repeat, *args):\r\n        pass\r\n\r\n    # Visualization\r\n\r\n    def get_2d_plot(self, x, y, padding=1, dense=200, title=None,\r\n                    draw_background=False, emphasize=None, extra=None, **kwargs):\r\n        axis, labels = np.asarray(x).T, np.asarray(y)\r\n        nx, ny, padding = dense, dense, padding\r\n        x_min, x_max = np.min(axis[0]), np.max(axis[0])  # type: float\r\n        y_min, y_max = np.min(axis[1]), np.max(axis[1])  # type: float\r\n        x_padding = max(abs(x_min), abs(x_max)) * padding\r\n        y_padding = max(abs(y_min), abs(y_max)) * padding\r\n        x_min -= x_padding\r\n        x_max += x_padding\r\n        y_min -= y_padding\r\n        y_max += y_padding\r\n\r\n        def get_base(_nx, _ny):\r\n            _xf = np.linspace(x_min, x_max, _nx)\r\n            _yf = np.linspace(y_min, y_max, _ny)\r\n            n_xf, n_yf = np.meshgrid(_xf, _yf)\r\n            return _xf, _yf, np.c_[n_xf.ravel(), n_yf.ravel()]\r\n\r\n        xf, yf, base_matrix = get_base(nx, ny)\r\n        z = self.predict(base_matrix).reshape((nx, ny))\r\n\r\n        if labels.ndim == 1:\r\n            if not self._plot_label_dict:\r\n                self._plot_label_dict = {c: i for i, c in enumerate(set(labels))}\r\n            dic = self._plot_label_dict\r\n            n_label = len(dic)\r\n            labels = np.array([dic[label] for label in labels])\r\n        else:\r\n            n_label = labels.shape[1]\r\n            labels = np.argmax(labels, axis=1)\r\n        colors = plt.cm.rainbow([i / n_label for i in range(n_label)])[labels]\r\n\r\n        buffer_ = io.BytesIO()\r\n        plt.figure()\r\n        if title is None:\r\n            title = self.title\r\n        plt.title(title)\r\n        if draw_background:\r\n            xy_xf, xy_yf = np.meshgrid(xf, yf, sparse=True)\r\n            plt.pcolormesh(xy_xf, xy_yf, z, cmap=plt.cm.Pastel1)\r\n        else:\r\n            plt.contour(xf, yf, z, c=\'k-\', levels=[0])\r\n        plt.scatter(axis[0], axis[1], c=colors)\r\n        if emphasize is not None:\r\n            indices = np.array([False] * len(axis[0]))\r\n            indices[np.asarray(emphasize)] = True\r\n            plt.scatter(axis[0][indices], axis[1][indices], s=80,\r\n                        facecolors=""None"", zorder=10)\r\n        if extra is not None:\r\n            plt.scatter(*np.asarray(extra).T, s=80, zorder=25, facecolors=""red"")\r\n\r\n        plt.savefig(buffer_, format=""png"")\r\n        plt.close()\r\n        buffer_.seek(0)\r\n        image = Image.open(buffer_)\r\n        canvas = np.asarray(image)[..., :3]\r\n        buffer_.close()\r\n        return canvas\r\n\r\n    def visualize2d(self, x, y, padding=0.1, dense=200, title=None,\r\n                    show_org=False, draw_background=True, emphasize=None, extra=None, **kwargs):\r\n        axis, labels = np.asarray(x).T, np.asarray(y)\r\n\r\n        print(""="" * 30 + ""\\n"" + str(self))\r\n        nx, ny, padding = dense, dense, padding\r\n        x_min, x_max = np.min(axis[0]), np.max(axis[0])\r\n        y_min, y_max = np.min(axis[1]), np.max(axis[1])\r\n        x_padding = max(abs(x_min), abs(x_max)) * padding\r\n        y_padding = max(abs(y_min), abs(y_max)) * padding\r\n        x_min -= x_padding\r\n        x_max += x_padding\r\n        y_min -= y_padding\r\n        y_max += y_padding\r\n\r\n        def get_base(_nx, _ny):\r\n            _xf = np.linspace(x_min, x_max, _nx)\r\n            _yf = np.linspace(y_min, y_max, _ny)\r\n            n_xf, n_yf = np.meshgrid(_xf, _yf)\r\n            return _xf, _yf, np.c_[n_xf.ravel(), n_yf.ravel()]\r\n\r\n        xf, yf, base_matrix = get_base(nx, ny)\r\n\r\n        t = time.time()\r\n        z = self.predict(base_matrix, **kwargs).reshape((nx, ny))\r\n        print(""Decision Time: {:8.6} s"".format(time.time() - t))\r\n\r\n        print(""Drawing figures..."")\r\n        xy_xf, xy_yf = np.meshgrid(xf, yf, sparse=True)\r\n        if labels.ndim == 1:\r\n            if not self._plot_label_dict:\r\n                self._plot_label_dict = {c: i for i, c in enumerate(set(labels))}\r\n            dic = self._plot_label_dict\r\n            n_label = len(dic)\r\n            labels = np.array([dic[label] for label in labels])\r\n        else:\r\n            n_label = labels.shape[1]\r\n            labels = np.argmax(labels, axis=1)\r\n        colors = plt.cm.rainbow([i / n_label for i in range(n_label)])[labels]\r\n\r\n        if title is None:\r\n            title = self.title\r\n\r\n        if show_org:\r\n            plt.figure()\r\n            plt.scatter(axis[0], axis[1], c=colors)\r\n            plt.xlim(x_min, x_max)\r\n            plt.ylim(y_min, y_max)\r\n            plt.show()\r\n\r\n        plt.figure()\r\n        plt.title(title)\r\n        if draw_background:\r\n            plt.pcolormesh(xy_xf, xy_yf, z, cmap=plt.cm.Pastel1)\r\n        else:\r\n            plt.contour(xf, yf, z, c=\'k-\', levels=[0])\r\n        plt.scatter(axis[0], axis[1], c=colors)\r\n        if emphasize is not None:\r\n            indices = np.array([False] * len(axis[0]))\r\n            indices[np.asarray(emphasize)] = True\r\n            plt.scatter(axis[0][indices], axis[1][indices], s=80,\r\n                        facecolors=""None"", zorder=10)\r\n        if extra is not None:\r\n            plt.scatter(*np.asarray(extra).T, s=80, zorder=25, facecolors=""red"")\r\n        plt.xlim(x_min, x_max)\r\n        plt.ylim(y_min, y_max)\r\n        plt.show()\r\n\r\n        print(""Done."")\r\n\r\n    def visualize3d(self, x, y, padding=0.1, dense=100, title=None,\r\n                    show_org=False, draw_background=True, emphasize=None, extra=None, **kwargs):\r\n        if False:\r\n            print(Axes3D.add_artist)\r\n        axis, labels = np.asarray(x).T, np.asarray(y)\r\n\r\n        print(""="" * 30 + ""\\n"" + str(self))\r\n\r\n        def decision_function(xx):\r\n            return self.predict(xx, **kwargs)\r\n\r\n        nx, ny, nz, padding = dense, dense, dense, padding\r\n        x_min, x_max = np.min(axis[0]), np.max(axis[0])\r\n        y_min, y_max = np.min(axis[1]), np.max(axis[1])\r\n        z_min, z_max = np.min(axis[2]), np.max(axis[2])\r\n        x_padding = max(abs(x_min), abs(x_max)) * padding\r\n        y_padding = max(abs(y_min), abs(y_max)) * padding\r\n        z_padding = max(abs(z_min), abs(z_max)) * padding\r\n        x_min -= x_padding\r\n        x_max += x_padding\r\n        y_min -= y_padding\r\n        y_max += y_padding\r\n        z_min -= z_padding\r\n        z_max += z_padding\r\n\r\n        def get_base(_nx, _ny, _nz):\r\n            _xf = np.linspace(x_min, x_max, _nx)\r\n            _yf = np.linspace(y_min, y_max, _ny)\r\n            _zf = np.linspace(z_min, z_max, _nz)\r\n            n_xf, n_yf, n_zf = np.meshgrid(_xf, _yf, _zf)\r\n            return _xf, _yf, _zf, np.c_[n_xf.ravel(), n_yf.ravel(), n_zf.ravel()]\r\n\r\n        xf, yf, zf, base_matrix = get_base(nx, ny, nz)\r\n\r\n        t = time.time()\r\n        z_xyz = decision_function(base_matrix).reshape((nx, ny, nz))\r\n        p_classes = decision_function(x).astype(np.int8)\r\n        _, _, _, base_matrix = get_base(10, 10, 10)\r\n        z_classes = decision_function(base_matrix).astype(np.int8)\r\n        print(""Decision Time: {:8.6} s"".format(time.time() - t))\r\n\r\n        print(""Drawing figures..."")\r\n        z_xy = np.average(z_xyz, axis=2)\r\n        z_yz = np.average(z_xyz, axis=1)\r\n        z_xz = np.average(z_xyz, axis=0)\r\n\r\n        xy_xf, xy_yf = np.meshgrid(xf, yf, sparse=True)\r\n        yz_xf, yz_yf = np.meshgrid(yf, zf, sparse=True)\r\n        xz_xf, xz_yf = np.meshgrid(xf, zf, sparse=True)\r\n\r\n        def transform_arr(arr):\r\n            if arr.ndim == 1:\r\n                dic = {c: i for i, c in enumerate(set(arr))}\r\n                n_dim = len(dic)\r\n                arr = np.array([dic[label] for label in arr])\r\n            else:\r\n                n_dim = arr.shape[1]\r\n                arr = np.argmax(arr, axis=1)\r\n            return arr, n_dim\r\n\r\n        labels, n_label = transform_arr(labels)\r\n        p_classes, _ = transform_arr(p_classes)\r\n        z_classes, _ = transform_arr(z_classes)\r\n        colors = plt.cm.rainbow([i / n_label for i in range(n_label)])\r\n        if extra is not None:\r\n            ex0, ex1, ex2 = np.asarray(extra).T\r\n        else:\r\n            ex0 = ex1 = ex2 = None\r\n\r\n        if title is None:\r\n            try:\r\n                title = self.title\r\n            except AttributeError:\r\n                title = str(self)\r\n\r\n        if show_org:\r\n            fig = plt.figure()\r\n            ax = fig.add_subplot(111, projection=\'3d\')\r\n            ax.scatter(axis[0], axis[1], axis[2], c=colors[labels])\r\n            plt.show()\r\n\r\n        fig = plt.figure(figsize=(16, 4), dpi=100)\r\n        plt.title(title)\r\n        ax1 = fig.add_subplot(131, projection=\'3d\')\r\n        ax2 = fig.add_subplot(132, projection=\'3d\')\r\n        ax3 = fig.add_subplot(133, projection=\'3d\')\r\n\r\n        ax1.set_title(""Org"")\r\n        ax2.set_title(""Pred"")\r\n        ax3.set_title(""Boundary"")\r\n\r\n        ax1.scatter(axis[0], axis[1], axis[2], c=colors[labels])\r\n        ax2.scatter(axis[0], axis[1], axis[2], c=colors[p_classes], s=15)\r\n        if extra is not None:\r\n            ax2.scatter(ex0, ex1, ex2, s=80, zorder=25, facecolors=""red"")\r\n        xyz_xf, xyz_yf, xyz_zf = base_matrix[..., 0], base_matrix[..., 1], base_matrix[..., 2]\r\n        ax3.scatter(xyz_xf, xyz_yf, xyz_zf, c=colors[z_classes], s=15)\r\n\r\n        plt.show()\r\n        plt.close()\r\n\r\n        fig = plt.figure(figsize=(16, 4), dpi=100)\r\n        ax1 = fig.add_subplot(131)\r\n        ax2 = fig.add_subplot(132)\r\n        ax3 = fig.add_subplot(133)\r\n\r\n        def _draw(_ax, _x, _xf, _y, _yf, _z):\r\n            if draw_background:\r\n                _ax.pcolormesh(_x, _y, _z > 0, cmap=plt.cm.Pastel1)\r\n            else:\r\n                _ax.contour(_xf, _yf, _z, c=\'k-\', levels=[0])\r\n\r\n        def _emphasize(_ax, axis0, axis1, _c):\r\n            _ax.scatter(axis0, axis1, c=_c)\r\n            if emphasize is not None:\r\n                indices = np.array([False] * len(axis[0]))\r\n                indices[np.asarray(emphasize)] = True\r\n                _ax.scatter(axis0[indices], axis1[indices], s=80,\r\n                            facecolors=""None"", zorder=10)\r\n\r\n        def _extra(_ax, axis0, axis1, _c, _ex0, _ex1):\r\n            _emphasize(_ax, axis0, axis1, _c)\r\n            if extra is not None:\r\n                _ax.scatter(_ex0, _ex1, s=80, zorder=25, facecolors=""red"")\r\n\r\n        colors = colors[labels]\r\n\r\n        ax1.set_title(""xy figure"")\r\n        _draw(ax1, xy_xf, xf, xy_yf, yf, z_xy)\r\n        _extra(ax1, axis[0], axis[1], colors, ex0, ex1)\r\n\r\n        ax2.set_title(""yz figure"")\r\n        _draw(ax2, yz_xf, yf, yz_yf, zf, z_yz)\r\n        _extra(ax2, axis[1], axis[2], colors, ex1, ex2)\r\n\r\n        ax3.set_title(""xz figure"")\r\n        _draw(ax3, xz_xf, xf, xz_yf, zf, z_xz)\r\n        _extra(ax3, axis[0], axis[2], colors, ex0, ex2)\r\n\r\n        plt.show()\r\n\r\n        print(""Done."")\r\n\r\n    # Util\r\n\r\n    def get_metrics(self, metrics):\r\n        if len(metrics) == 0:\r\n            for metric in self._metrics:\r\n                metrics.append(metric)\r\n            return metrics\r\n        for i in range(len(metrics) - 1, -1, -1):\r\n            metric = metrics[i]\r\n            if isinstance(metric, str):\r\n                try:\r\n                    metrics[i] = self._available_metrics[metric]\r\n                except AttributeError:\r\n                    metrics.pop(i)\r\n        return metrics\r\n\r\n    @clf_timing.timeit(level=1, prefix=""[API] "")\r\n    def evaluate(self, x, y, metrics=None, tar=0, prefix=""Acc"", **kwargs):\r\n        if metrics is None:\r\n            metrics = [""acc""]\r\n        self.get_metrics(metrics)\r\n        logs, y_pred = [], self.predict(x, **kwargs)\r\n        y = np.asarray(y)\r\n        if y.ndim == 2:\r\n            y = np.argmax(y, axis=1)\r\n        for metric in metrics:\r\n            logs.append(metric(y, y_pred))\r\n        if isinstance(tar, int):\r\n            print(prefix + "": {:12.8}"".format(logs[tar]))\r\n        return logs\r\n\r\n\r\nclass GDBase(ClassifierBase):\r\n    """"""\r\n        Gradient descent classifier framework\r\n        Requirements:\r\n            1) Store all parameters in self._model_parameters\r\n            2) Calculate all gradients of the parameters and store them in self._grads\r\n              in self._get_grads() method\r\n        See self._update_model_params() method for more insights\r\n    """"""\r\n\r\n    GDBaseTiming = Timing()\r\n\r\n    def __init__(self, **kwargs):\r\n        super(GDBase, self).__init__(**kwargs)\r\n        self._optimizer = self._model_parameters = self._model_grads = None\r\n\r\n    def _get_grads(self, x_batch, y_batch, y_pred, sample_weight_batch, *args):\r\n        return 0\r\n\r\n    def _update_model_params(self):\r\n        for i, (param, grad) in enumerate(zip(self._model_parameters, self._model_grads)):\r\n            if grad is not None:\r\n                param -= self._optimizer.run(i, grad)\r\n\r\n    @GDBaseTiming.timeit(level=1, prefix=""[Core] "")\r\n    def _batch_training(self, x, y, batch_size, train_repeat, *args, **kwargs):\r\n        sample_weight, *args = args\r\n        epoch_loss = 0\r\n        for i in range(train_repeat):\r\n            if train_repeat != 1:\r\n                batch = np.random.permutation(len(x))[:batch_size]\r\n                x_batch, y_batch = x[batch], y[batch]\r\n                sample_weight_batch = sample_weight[batch]\r\n            else:\r\n                x_batch, y_batch, sample_weight_batch = x, y, sample_weight\r\n            y_pred = self.predict(x_batch, get_raw_results=True, **kwargs)\r\n            epoch_loss += self._get_grads(x_batch, y_batch, y_pred, sample_weight_batch, *args)\r\n            self._update_model_params()\r\n            self._batch_work(i, *args)\r\n        return epoch_loss / train_repeat\r\n\r\n\r\nclass TFClassifierBase(ClassifierBase):\r\n    """"""\r\n        Tensorfow classifier framework\r\n        Implemented tensorflow ver. metrics\r\n    """"""\r\n\r\n    clf_timing = Timing()\r\n\r\n    def __init__(self, **kwargs):\r\n        super(TFClassifierBase, self).__init__(**kwargs)\r\n        self._tfx = self._tfy = None\r\n        self._y_pred_raw = self._y_pred = None\r\n        self._sess = tf.Session()\r\n\r\n    @clf_timing.timeit(level=2, prefix=""[Core] "")\r\n    def _batch_training(self, x, y, batch_size, train_repeat, *args):\r\n        loss, train_step, *args = args\r\n        epoch_cost = 0\r\n        for i in range(train_repeat):\r\n            if train_repeat != 1:\r\n                batch = np.random.choice(len(x), batch_size)\r\n                x_batch, y_batch = x[batch], y[batch]\r\n            else:\r\n                x_batch, y_batch = x, y\r\n            epoch_cost += self._sess.run([loss, train_step], {\r\n                self._tfx: x_batch, self._tfy: y_batch\r\n            })[0]\r\n            self._batch_work(i, *args)\r\n        return epoch_cost / train_repeat\r\n\r\n\r\nif torch is not None:\r\n    class TorchBasicClassifierBase(ClassifierBase):\r\n        """""" Basic torch\'s classifier base """"""\r\n        def _handle_animation(self, i, x, y, ims, animation_params, draw_ani, show_ani, make_mp4, ani_period,\r\n                              name=None, img=None):\r\n            x, y = x.numpy(), y.numpy()\r\n            super(TorchBasicClassifierBase, self)._handle_animation(\r\n                i, x, y, ims, animation_params, draw_ani, show_ani, make_mp4, ani_period, name, img\r\n            )\r\n\r\n    class TorchAutoClassifierBase(TorchBasicClassifierBase):\r\n        """"""\r\n            Torch classifier framework\r\n            Utilizing torch\'s auto-grad, but not utilizing torch\'s optimizers\r\n            Static method:\r\n                _arr_to_variable : change iterable(s) to torch Variable(s)\r\n        """"""\r\n        def __init__(self, **kwargs):\r\n            super(TorchAutoClassifierBase, self).__init__(**kwargs)\r\n            self._optimizer = self._model_parameters = None\r\n\r\n        @staticmethod\r\n        def _arr_to_variable(requires_grad, *args):\r\n            return [\r\n                Variable(\r\n                    torch.from_numpy(np.asarray(arr, dtype=np.float32)),\r\n                    requires_grad=requires_grad\r\n                ) for arr in args\r\n            ]\r\n\r\n        # Training\r\n\r\n        def _update_model_params(self):\r\n            for i, param in enumerate(self._model_parameters):\r\n                if param.grad is not None:\r\n                    param.data -= self._optimizer.run(i, param.grad.data)\r\n\r\n        def _reset_grad(self):\r\n            for param in self._model_parameters:\r\n                param.grad.data.zero_()\r\n\r\n        def batch_training(self, x, y, batch_size, train_repeat, *args):\r\n            loss_function, *args = args\r\n            epoch_cost = 0\r\n            for i in range(train_repeat):\r\n                if train_repeat != 1:\r\n                    batch = torch.randperm(len(x))[:batch_size]\r\n                    x_batch, y_batch = x[batch], y[batch]\r\n                else:\r\n                    x_batch, y_batch = x, y\r\n                y_pred = self._predict(x_batch, get_raw_results=True)\r\n                local_loss = loss_function(y_batch, y_pred)\r\n                epoch_cost += local_loss\r\n                local_loss.backward()\r\n                self._update_model_params()\r\n                self._reset_grad()\r\n                self._batch_work(i, *args)\r\n            return float((epoch_cost / train_repeat).data.numpy()[0])\r\n\r\n        # Util\r\n\r\n        def _predict(self, x, get_raw_results=False, **kwargs):\r\n            pass\r\n\r\n        def predict(self, x, get_raw_results=False, **kwargs):\r\n            return self._predict(x, get_raw_results, **kwargs).data.numpy()\r\n\r\n        def _handle_animation(self, i, x, y, ims, animation_params, draw_ani, show_ani, make_mp4, ani_period,\r\n                              name=None, img=None):\r\n            x, y = x.data.numpy(), y.data.numpy()\r\n            super(TorchBasicClassifierBase, self)._handle_animation(\r\n                i, x, y, ims, animation_params, draw_ani, show_ani, make_mp4, ani_period, name, img\r\n            )\r\nelse:\r\n    TorchBasicClassifierBase = TorchAutoClassifierBase = None\r\n\r\n\r\nclass KernelBase(ClassifierBase):\r\n    """""" Kernel classifier with SMO-like algorithm """"""\r\n\r\n    KernelBaseTiming = Timing()\r\n\r\n    def __init__(self, **kwargs):\r\n        super(KernelBase, self).__init__(**kwargs)\r\n        self._do_log = True\r\n        self._is_torch = False\r\n        self._fit_args, self._fit_args_names = None, []\r\n        self._x = self._y = self._gram = None\r\n        self._w = self._b = self._alpha = None\r\n        self._kernel = self._kernel_name = self._kernel_param = None\r\n        self._prediction_cache = self._dw_cache = self._db_cache = None\r\n\r\n        self._params[""kernel""] = kwargs.get(""kernel"", ""rbf"")\r\n        self._params[""epoch""] = kwargs.get(""epoch"", 10 ** 4)\r\n        self._params[""x_test""] = kwargs.get(""x_test"", None)\r\n        self._params[""y_test""] = kwargs.get(""y_test"", None)\r\n        self._params[""metrics""] = kwargs.get(""metrics"", None)\r\n        self._params[""c""] = kwargs.get(""c"", 1)\r\n        self._params[""p""] = kwargs.get(""p"", 3)\r\n        self._params[""lr""] = kwargs.get(""lr"", 0.001)\r\n\r\n    @property\r\n    def title(self):\r\n        return ""{} {} ({})"".format(self._kernel_name, self, self._kernel_param)\r\n\r\n    # Kernel\r\n\r\n    @staticmethod\r\n    @KernelBaseTiming.timeit(level=1, prefix=""[Kernel] "")\r\n    def _poly(x, y, p):\r\n        return (x.dot(y.T) + 1) ** p\r\n\r\n    @staticmethod\r\n    @KernelBaseTiming.timeit(level=1, prefix=""[Kernel] "")\r\n    def _rbf(x, y, gamma):\r\n        return np.exp(-gamma * np.sum((x[..., None, :] - y) ** 2, axis=2))\r\n\r\n    # Training\r\n\r\n    def _update_dw_cache(self, *args):\r\n        pass\r\n\r\n    def _update_db_cache(self, *args):\r\n        pass\r\n\r\n    @KernelBaseTiming.timeit(level=1, prefix=""[Core] "")\r\n    def _update_pred_cache(self, *args):\r\n        self._prediction_cache += self._db_cache\r\n        if len(args) == 1:\r\n            self._prediction_cache += self._dw_cache * self._gram[args[0]]\r\n        elif len(args) == len(self._gram):\r\n            self._prediction_cache = self._dw_cache.dot(self._gram)\r\n        else:\r\n            self._prediction_cache += self._dw_cache.dot(self._gram[args, ...])\r\n\r\n    def _prepare(self, sample_weight, **kwargs):\r\n        pass\r\n\r\n    def _torch_transform(self, y_cv):\r\n        pass\r\n\r\n    def _fit(self, *args):\r\n        pass\r\n\r\n    @KernelBaseTiming.timeit(level=1, prefix=""[API] "")\r\n    def fit(self, x, y, sample_weight=None, kernel=None, epoch=None,\r\n            x_test=None, y_test=None, metrics=None, animation_params=None, **kwargs):\r\n        if sample_weight is None:\r\n            sample_weight = self._params[""sample_weight""]  # type: list\r\n        if kernel is None:\r\n            kernel = self._params[""kernel""]\r\n        if epoch is None:\r\n            epoch = self._params[""epoch""]\r\n        if x_test is None:\r\n            x_test = self._params[""x_test""]  # type: list\r\n        if y_test is None:\r\n            y_test = self._params[""y_test""]  # type: list\r\n        if metrics is None:\r\n            metrics = self._params[""metrics""]  # type: list\r\n        *animation_properties, animation_params = self._get_animation_params(animation_params)\r\n        self._x, self._y = np.atleast_2d(x), np.asarray(y)\r\n        if kernel == ""poly"":\r\n            _p = kwargs.get(""p"", self._params[""p""])\r\n            self._kernel_name = ""Polynomial""\r\n            self._kernel_param = ""degree = {}"".format(_p)\r\n            self._kernel = lambda _x, _y: KernelBase._poly(_x, _y, _p)\r\n        elif kernel == ""rbf"":\r\n            _gamma = kwargs.get(""gamma"", 1 / self._x.shape[1])\r\n            self._kernel_name = ""RBF""\r\n            self._kernel_param = r""$\\gamma = {:8.6}$"".format(_gamma)\r\n            self._kernel = lambda _x, _y: KernelBase._rbf(_x, _y, _gamma)\r\n        else:\r\n            raise NotImplementedError(""Kernel \'{}\' has not defined"".format(kernel))\r\n        if sample_weight is None:\r\n            sample_weight = np.ones(len(y))\r\n        else:\r\n            sample_weight = np.asarray(sample_weight) * len(y)\r\n\r\n        self._alpha, self._w, self._prediction_cache = (\r\n            np.zeros(len(x)), np.zeros(len(x)), np.zeros(len(x)))\r\n        self._gram = self._kernel(self._x, self._x)\r\n        self._b = 0\r\n        self._prepare(sample_weight, **kwargs)\r\n\r\n        fit_args, logs, ims = [], [], []\r\n        for name, arg in zip(self._fit_args_names, self._fit_args):\r\n            if name in kwargs:\r\n                arg = kwargs[name]\r\n            fit_args.append(arg)\r\n        if self._do_log:\r\n            if metrics is not None:\r\n                self.get_metrics(metrics)\r\n            test_gram = None\r\n            if x_test is not None and y_test is not None:\r\n                x_cv, y_cv = np.atleast_2d(x_test), np.asarray(y_test)\r\n                test_gram = self._kernel(self._x, x_cv)\r\n            else:\r\n                x_cv, y_cv = self._x, self._y\r\n        else:\r\n            y_cv = test_gram = None\r\n\r\n        if self._is_torch:\r\n            y_cv, self._x, self._y = self._torch_transform(y_cv)\r\n\r\n        bar = ProgressBar(max_value=epoch, name=str(self))\r\n        for i in range(epoch):\r\n            if self._fit(sample_weight, *fit_args):\r\n                bar.terminate()\r\n                break\r\n            if self._do_log and metrics is not None:\r\n                local_logs = []\r\n                for metric in metrics:\r\n                    if test_gram is None:\r\n                        if self._is_torch:\r\n                            local_y = self._y.data.numpy()\r\n                        else:\r\n                            local_y = self._y\r\n                        local_logs.append(metric(local_y, np.sign(self._prediction_cache)))\r\n                    else:\r\n                        if self._is_torch:\r\n                            local_y = y_cv.data.numpy()\r\n                        else:\r\n                            local_y = y_cv\r\n                        local_logs.append(metric(local_y, self.predict(test_gram, gram_provided=True)))\r\n                logs.append(local_logs)\r\n            self._handle_animation(i, self._x, self._y, ims, animation_params, *animation_properties)\r\n            bar.update()\r\n        self._handle_mp4(ims, animation_properties)\r\n        return logs\r\n\r\n    # Util\r\n\r\n    @KernelBaseTiming.timeit(level=1, prefix=""[API] "")\r\n    def predict(self, x, get_raw_results=False, gram_provided=False):\r\n        if not gram_provided:\r\n            x = self._kernel(self._x, np.atleast_2d(x))\r\n        y_pred = self._w.dot(x) + self._b\r\n        if not get_raw_results:\r\n            return np.sign(y_pred)\r\n        return y_pred\r\n\r\n\r\nclass GDKernelBase(KernelBase, GDBase):\r\n    """""" Kernel classifier with gradient descent algorithm """"""\r\n\r\n    GDKernelBaseTiming = Timing()\r\n\r\n    def __init__(self, **kwargs):\r\n        super(GDKernelBase, self).__init__(**kwargs)\r\n        self._fit_args, self._fit_args_names = [1e-3], [""tol""]\r\n        self._batch_size = kwargs.get(""batch_size"", 128)\r\n        self._optimizer = kwargs.get(""optimizer"", ""Adam"")\r\n        self._train_repeat = 0\r\n\r\n    def _prepare(self, sample_weight, **kwargs):\r\n        lr = kwargs.get(""lr"", self._params[""lr""])\r\n        self._alpha = np.random.random(len(self._x)).astype(np.float32)\r\n        self._b = np.random.random(1).astype(np.float32)\r\n        self._model_parameters = [self._alpha, self._b]\r\n        self._optimizer = OptFactory().get_optimizer_by_name(\r\n            self._optimizer, self._model_parameters, lr, self._params[""epoch""]\r\n        )\r\n\r\n    @GDKernelBaseTiming.timeit(level=1, prefix=""[Core] "")\r\n    def _fit(self, sample_weight, tol):\r\n        if self._train_repeat == 0:\r\n            self._train_repeat = self._get_train_repeat(self._x, self._batch_size)\r\n        l = self._batch_training(\r\n            self._gram, self._y, self._batch_size, self._train_repeat,\r\n            sample_weight, gram_provided=True\r\n        )\r\n        if l < tol:\r\n            return True\r\n\r\n    @GDKernelBaseTiming.timeit(level=1, prefix=""[API] "")\r\n    def predict(self, x, get_raw_results=False, gram_provided=False):\r\n        if not gram_provided:\r\n            x = self._kernel(self._x, np.atleast_2d(x))\r\n        elif self._alpha.shape[0] != x.shape[0]:\r\n            x = x.T\r\n        y_pred = (self._alpha.dot(x) + self._b).ravel()\r\n        if not get_raw_results:\r\n            return np.sign(y_pred)\r\n        return y_pred\r\n\r\n\r\nclass TFKernelBase(KernelBase, TFClassifierBase):\r\n    """""" Kernel classifier with tensorflow """"""\r\n\r\n    def __init__(self, **kwargs):\r\n        super(TFKernelBase, self).__init__(**kwargs)\r\n        self._loss = None\r\n\r\n\r\nif TorchAutoClassifierBase is not None:\r\n    class TorchKernelBase(KernelBase, TorchAutoClassifierBase):\r\n        """""" Kernel classifier with torch """"""\r\n\r\n        def __init__(self, **kwargs):\r\n            super(TorchKernelBase, self).__init__(**kwargs)\r\n            self._loss_function = None\r\n            self._is_torch = True\r\n\r\n        def _torch_transform(self, y_cv):\r\n            return self._arr_to_variable(False, y_cv, self._x, self._y)\r\nelse:\r\n    TorchKernelBase = None\r\n\r\n\r\nclass RegressorBase(ModelBase):\r\n    def predict(self, x, **kwargs):\r\n        return x\r\n\r\n    def visualize2d(self, x, y, padding=0.1, dense=400, title=None):\r\n        x, y = np.asarray(x).ravel(), np.asarray(y)\r\n\r\n        print(""="" * 30 + ""\\n"" + str(self))\r\n\r\n        x_min, x_max = np.min(x), np.max(x)\r\n        y_min, y_max = np.min(y), np.max(y)\r\n        x_padding = max(abs(x_min), abs(x_max)) * padding\r\n        x_min -= x_padding\r\n        x_max += x_padding\r\n\r\n        t = time.time()\r\n        x_base = np.linspace(x_min, x_max, dense)  # type: np.ndarray\r\n        y_pred = self.predict(x_base[..., None])\r\n        print(""Decision Time: {:8.6} s"".format(time.time() - t))\r\n\r\n        print(""Drawing figures..."")\r\n\r\n        if title is None:\r\n            title = self.title\r\n\r\n        plt.figure()\r\n        plt.title(title)\r\n        plt.scatter(x, y, c=""g"", s=20)\r\n        plt.plot(x_base, y_pred)\r\n        plt.xlim(x_min, x_max)\r\n        plt.ylim(y_min, y_max)\r\n        plt.show()\r\n\r\n        print(""Done."")\r\n'"
Util/CythonUtil.py,0,"b'import sys\r\nimport Cython.Compiler.Main\r\nfrom numpy.distutils.command import build_src\r\nfrom numpy.distutils.misc_util import appendpath\r\nfrom numpy.distutils import log\r\nfrom os.path import join as p_join, dirname\r\nfrom distutils.dep_util import newer_group\r\nfrom distutils.errors import DistutilsError\r\n\r\ntry:\r\n    import Cython.Compiler.Main\r\n    sys.modules[\'Pyrex\'] = Cython\r\n    sys.modules[\'Pyrex.Compiler\'] = Cython.Compiler\r\n    sys.modules[\'Pyrex.Compiler.Main\'] = Cython.Compiler.Main\r\n    have_pyrex = True\r\nexcept ImportError:\r\n    Cython = None\r\n    have_pyrex = False\r\n\r\nbuild_src.Pyrex = Cython\r\nbuild_src.have_pyrex = have_pyrex\r\n\r\n\r\ndef generate_a_pyrex_source(self, base, ext_name, source, extension):\r\n    if self.inplace:\r\n        target_dir = dirname(base)\r\n    else:\r\n        target_dir = appendpath(self.build_src, dirname(base))\r\n    target_file = p_join(target_dir, ext_name + \'.c\')\r\n    depends = [source] + extension.depends\r\n    if self.force or newer_group(depends, target_file, \'newer\'):\r\n        import Cython.Compiler.Main\r\n        log.info(""cythonc:> %s"" % target_file)\r\n        self.mkpath(target_dir)\r\n        options = Cython.Compiler.Main.CompilationOptions(\r\n            defaults=Cython.Compiler.Main.default_options,\r\n            include_path=extension.include_dirs,\r\n            output_file=target_file)\r\n        cython_result = Cython.Compiler.Main.compile(source, options=options)\r\n        if cython_result.num_errors != 0:\r\n            raise DistutilsError(""%d errors while compiling %r with Cython"" % (cython_result.num_errors, source))\r\n    return target_file\r\n\r\nbuild_src.build_src.generate_a_pyrex_source = generate_a_pyrex_source\r\n'"
Util/DataToolkit.py,0,"b'import numpy as np\r\nimport scipy.stats as stats\r\nimport matplotlib.pyplot as plt\r\n\r\n\r\n# noinspection PyTypeChecker\r\nclass DataToolkit:\r\n    def __init__(self, data):\r\n        self._data = np.asarray(data)\r\n        self._sorted_data = np.sort(self._data)\r\n        self._n = len(self._data)\r\n        self._mean = self._variance = self._std = None\r\n        self._moments = []\r\n        self._q1 = self._q3 = None\r\n\r\n    def get_moment(self, k):\r\n        if len(self._moments) < k:\r\n            self._moments += [None] * (k - len(self._moments))\r\n        if self._moments[k-1] is None:\r\n            self._moments[k-1] = np.sum((self._data - self.mean) ** k) / self._n\r\n        return self._moments[k-1]\r\n\r\n    def get_mp(self, p):\r\n        _np = self._n * p\r\n        int_np = int(_np)\r\n        if not int(_np % 1):\r\n            return self._sorted_data[int_np]\r\n        return 0.5 * (self._sorted_data[int_np-1] + self._sorted_data[int_np])\r\n\r\n    @property\r\n    def min(self):\r\n        return self._sorted_data[0]\r\n\r\n    @property\r\n    def max(self):\r\n        return self._sorted_data[-1]\r\n\r\n    @property\r\n    def mean(self):\r\n        if self._mean is None:\r\n            self._mean = self._data.mean()\r\n        return self._mean\r\n\r\n    @property\r\n    def variance(self):\r\n        if self._variance is None:\r\n            self._variance = np.sum((self._data - self.mean) ** 2) / (self._n - 1)\r\n        return self._variance\r\n\r\n    @property\r\n    def std(self):\r\n        if self._std is None:\r\n            self._std = (np.sum((self._data - self.mean) ** 2) / (self._n - 1)) ** 0.5\r\n        return self._std\r\n\r\n    @property\r\n    def g1(self):\r\n        n, moment3 = self._n, self.get_moment(3)\r\n        return n ** 2 * moment3 / ((n - 1) * (n - 2) * self.std ** 3)\r\n\r\n    @property\r\n    def g2(self):\r\n        n, moment4 = self._n, self.get_moment(4)\r\n        return n**2*(n+1)*moment4 / ((n-1)*(n-2)*(n-3)*self.std**4) - 3*(n-1)**2/((n-2)*(n-3))\r\n\r\n    @property\r\n    def med(self):\r\n        n, hn = self._n, int(self._n*0.5)\r\n        if n & 1:\r\n            return self._sorted_data[hn-1]\r\n        return 0.5 * (self._sorted_data[hn-1] + self._sorted_data[hn])\r\n\r\n    @property\r\n    def q1(self):\r\n        if self._q1 is None:\r\n            self._q1 = self.get_mp(0.25)\r\n        return self._q1\r\n\r\n    @property\r\n    def q3(self):\r\n        if self._q3 is None:\r\n            self._q3 = self.get_mp(0.75)\r\n        return self._q3\r\n\r\n    @property\r\n    def r(self):\r\n        return self._sorted_data[-1] - self._sorted_data[0]\r\n\r\n    @property\r\n    def r1(self):\r\n        return self.q3 - self.q1\r\n\r\n    @property\r\n    def trimean(self):\r\n        return 0.25 * (self.q1 + self.q3) + 0.5 * self.med\r\n\r\n    @property\r\n    def loval(self):\r\n        return self.q1 - 1.5 * self.r1\r\n\r\n    @property\r\n    def hival(self):\r\n        return self.q3 + 1.5 * self.r1\r\n\r\n    def draw_histogram(self, bin_size=10):\r\n        bins = np.arange(self._sorted_data[0]-self.r1, self._sorted_data[-1]+self.r1, bin_size)\r\n        plt.hist(self._data, bins=bins, alpha=0.5)\r\n        plt.title(""Histogram (bin_size: {})"".format(bin_size))\r\n        plt.show()\r\n\r\n    def qq_plot(self):\r\n        stats.probplot(self._data, dist=""norm"", plot=plt)\r\n        plt.show()\r\n\r\n    def box_plot(self):\r\n        plt.figure()\r\n        plt.boxplot(self._data, vert=False, showmeans=True)\r\n        plt.show()\r\n\r\nif __name__ == \'__main__\':\r\n    toolkit = DataToolkit([\r\n        53, 70.2, 84.3, 55.3, 78.5, 63.5, 71.4, 53.4, 82.5, 67.3, 69.5, 73, 55.7, 85.8, 95.4, 51.1, 74.4,\r\n        54.1, 77.8, 52.4, 69.1, 53.5, 64.3, 82.7, 55.7, 70.5, 87.5, 50.7, 72.3, 59.5\r\n    ])\r\n    print(""mean     : "", toolkit.mean)\r\n    print(""variance : "", toolkit.variance)\r\n    print(""g1       : "", toolkit.g1)\r\n    print(""g2       : "", toolkit.g2)\r\n    print(""med      : "", toolkit.med)\r\n    print(""r        : "", toolkit.r)\r\n    print(""q3       : "", toolkit.q3)\r\n    print(""q1       : "", toolkit.q1)\r\n    print(""r1       : "", toolkit.r1)\r\n    print(""trimean  : "", toolkit.trimean)\r\n    print(""hival    : "", toolkit.hival)\r\n    print(""loval    : "", toolkit.loval)\r\n    print(""min      : "", toolkit.min)\r\n    print(""max      : "", toolkit.max)\r\n    toolkit.draw_histogram()\r\n    toolkit.qq_plot()\r\n    toolkit.box_plot()\r\n'"
Util/Metas.py,0,"b'from abc import ABCMeta\n\nfrom Util.Util import Util\nfrom Util.Timing import Timing\n\n\nclass TimingMeta(type):\n    def __new__(mcs, *args, **kwargs):\n        name, bases, attr = args[:3]\n        timing = Timing()\n\n        for _name, _value in attr.items():\n            if ""__"" in _name or ""timing"" in _name or ""evaluate"" in _name:\n                continue\n            if Util.callable(_value):\n                attr[_name] = timing.timeit(level=2)(_value)\n\n        def show_timing_log(self, level=2):\n            getattr(self, name + ""Timing"").show_timing_log(level)\n\n        attr[""show_timing_log""] = show_timing_log\n        return type(name, bases, attr)\n\n\nclass SubClassTimingMeta(type):\n    def __new__(mcs, *args, **kwargs):\n        name, bases, attr = args[:3]\n        timing = Timing()\n        for _name, _value in attr.items():\n            if ""__"" in _name or ""timing"" in _name or ""evaluate"" in _name:\n                continue\n            if Util.callable(_value):\n                attr[_name] = timing.timeit(level=2)(_value)\n        return type(name, bases, attr)\n\n\nclass SKCompatibleMeta(ABCMeta):\n    def __new__(mcs, *args, **kwargs):\n        name, bases, attr = args[:3]\n\n        def __init__(self, *_args, **_kwargs):\n            for base in bases:\n                base.__init__(self, *_args, **_kwargs)\n        attr[""__init__""] = __init__\n        return type(name, bases, attr)\n'"
Util/ProgressBar.py,0,"b'import time\r\n\r\n\r\nclass ProgressBar:\r\n    def __init__(self, min_value=0, max_value=None, min_refresh_period=0.5, width=30, name="""", start=True):\r\n        self._min, self._max = min_value, max_value\r\n        self._task_length = int(max_value - min_value) if (\r\n            min_value is not None and max_value is not None\r\n        ) else None\r\n        self._counter = min_value\r\n        self._min_period = min_refresh_period\r\n        self._bar_width = int(width)\r\n        self._bar_name = "" "" if not name else "" # {:^12s} # "".format(name)\r\n        self._terminated = False\r\n        self._started = False\r\n        self._ended = False\r\n        self._current = 0\r\n        self._clock = 0\r\n        self._cost = 0\r\n        if start:\r\n            self.start()\r\n\r\n    def _flush(self):\r\n        if self._ended:\r\n            return False\r\n        if not self._started:\r\n            print(""Progress bar not started yet."")\r\n            return False\r\n        if self._terminated:\r\n            if self._counter == self._min:\r\n                self._counter = self._min + 1\r\n            self._cost = time.time() - self._clock\r\n            tmp_hour = int(self._cost / 3600)\r\n            tmp_min = int((self._cost - tmp_hour * 3600) / 60)\r\n            tmp_sec = self._cost % 60\r\n            tmp_avg = self._cost / (self._counter - self._min)\r\n            tmp_avg_hour = int(tmp_avg / 3600)\r\n            tmp_avg_min = int((tmp_avg - tmp_avg_hour * 3600) / 60)\r\n            tmp_avg_sec = tmp_avg % 60\r\n            print(\r\n                ""\\r"" +\r\n                ""##{}({:d} : {:d} -> {:d}) Task Finished. ""\r\n                ""Time Cost: {:3d} h {:3d} min {:6.4} s; Average: {:3d} h {:3d} min {:6.4} s "".format(\r\n                    self._bar_name, self._task_length, self._min, self._counter - self._min,\r\n                    tmp_hour, tmp_min, tmp_sec, tmp_avg_hour, tmp_avg_min, tmp_avg_sec\r\n                ) + "" ##\\n"", end=""""\r\n            )\r\n            self._ended = True\r\n            return True\r\n        if self._counter >= self._max:\r\n            self._terminated = True\r\n            return self._flush()\r\n        if self._counter != self._min and time.time() - self._current <= self._min_period:\r\n            return False\r\n        self._current = time.time()\r\n        self._cost = time.time() - self._clock\r\n        if self._counter > self._min:\r\n            tmp_hour = int(self._cost / 3600)\r\n            tmp_min = int((self._cost - tmp_hour * 3600) / 60)\r\n            tmp_sec = self._cost % 60\r\n            tmp_avg = self._cost / (self._counter - self._min)\r\n            tmp_avg_hour = int(tmp_avg / 3600)\r\n            tmp_avg_min = int((tmp_avg - tmp_avg_hour * 3600) / 60)\r\n            tmp_avg_sec = tmp_avg % 60\r\n        else:\r\n            print()\r\n            tmp_hour = 0\r\n            tmp_min = 0\r\n            tmp_sec = 0\r\n            tmp_avg_hour = 0\r\n            tmp_avg_min = 0\r\n            tmp_avg_sec = 0\r\n        passed = int(self._counter * self._bar_width / self._max)\r\n        print(\r\n            ""\\r"" + ""##{}["".format(\r\n                self._bar_name\r\n            ) + ""-"" * passed + "" "" * (self._bar_width - passed) + ""] : {} / {}"".format(\r\n                self._counter, self._max\r\n            ) + "" ##  Time Cost: {:3d} h {:3d} min {:6.4} s; Average: {:3d} h {:3d} min {:6.4} s "".format(\r\n                tmp_hour, tmp_min, tmp_sec, tmp_avg_hour, tmp_avg_min, tmp_avg_sec\r\n            ) if self._counter != self._min else ""##{}Progress bar initialized  ##"".format(\r\n                self._bar_name\r\n            ), end=""""\r\n        )\r\n        return True\r\n\r\n    def set_min(self, min_val):\r\n        if self._max is not None:\r\n            if self._max <= min_val:\r\n                print(""Target min_val: {} is larger than current max_val: {}"".format(min_val, self._max))\r\n                return\r\n            self._task_length = self._max - min_val\r\n        self._counter = self._min = min_val\r\n\r\n    def set_max(self, max_val):\r\n        if self._min is not None:\r\n            if self._min >= max_val:\r\n                print(""Target max_val: {} is smaller than current min_val: {}"".format(max_val, self._min))\r\n                return\r\n            self._task_length = max_val - self._min\r\n        self._max = max_val\r\n\r\n    def update(self, new_value=None):\r\n        if new_value is None:\r\n            new_value = self._counter + 1\r\n        if new_value != self._min:\r\n            self._counter = self._max if new_value >= self._max else int(new_value)\r\n            return self._flush()\r\n\r\n    def start(self):\r\n        if self._task_length is None:\r\n            print(""Error: Progress bar not initialized properly."")\r\n            return\r\n        self._current = self._clock = time.time()\r\n        self._started = True\r\n        self._flush()\r\n\r\n    def terminate(self):\r\n        self._terminated = True\r\n        self._flush()\r\n\r\nif __name__ == \'__main__\':\r\n\r\n    def task(cost=0.25, epoch=3, name="""", _sub_task=None):\r\n        def _sub():\r\n            bar = ProgressBar(max_value=epoch, name=name)\r\n            for _ in range(epoch):\r\n                time.sleep(cost)\r\n                if _sub_task is not None:\r\n                    _sub_task()\r\n                bar.update()\r\n\r\n        return _sub\r\n\r\n\r\n    task(name=""Task1"", _sub_task=task(\r\n        name=""Task2"", _sub_task=task(\r\n            name=""Task3"")))()\r\n'"
Util/Timing.py,0,"b'import time\nimport wrapt\n\n\nclass Timing:\n    timings = {}\n    enabled = False\n\n    def __init__(self, enabled=True):\n        Timing.enabled = enabled\n\n    def __str__(self):\n        return ""Timing""\n\n    __repr__ = __str__\n\n    @classmethod\n    def timeit(cls, level=0, func_name=None, cls_name=None, prefix=""[Method] ""):\n        @wrapt.decorator\n        def wrapper(func, instance, args, kwargs):\n            if not cls.enabled:\n                return func(*args, **kwargs)\n            if instance is not None:\n                instance_name = ""{:>18s}"".format(instance.__class__.__name__)\n            else:\n                instance_name = "" "" * 18 if cls_name is None else ""{:>18s}"".format(cls_name)\n            _prefix = ""{:>26s}"".format(prefix)\n            try:\n                _func_name = ""{:>28}"".format(func.__name__ if func_name is None else func_name)\n            except AttributeError:\n                str_func = str(func)\n                _at_idx = str_func.rfind(""at"")\n                _dot_idx = str_func.rfind(""."", None, _at_idx)\n                _func_name = ""{:>28}"".format(str_func[_dot_idx+1:_at_idx-1])\n            _name = instance_name + _prefix + _func_name\n            _t = time.time()\n            rs = func(*args, **kwargs)\n            _t = time.time() - _t\n            try:\n                cls.timings[_name][""timing""] += _t\n                cls.timings[_name][""call_time""] += 1\n            except KeyError:\n                cls.timings[_name] = {\n                    ""level"": level,\n                    ""timing"": _t,\n                    ""call_time"": 1\n                }\n            return rs\n        return wrapper\n\n    @classmethod\n    def show_timing_log(cls, level=2):\n        print()\n        print(""="" * 110 + ""\\n"" + ""Timing log\\n"" + ""-"" * 110)\n        if cls.timings:\n            for key in sorted(cls.timings.keys()):\n                timing_info = cls.timings[key]\n                if level >= timing_info[""level""]:\n                    print(""{:<42s} :  {:12.7} s (Call Time: {:6d})"".format(\n                        key, timing_info[""timing""], timing_info[""call_time""]))\n        print(""-"" * 110)\n\n    @classmethod\n    def disable(cls):\n        cls.enabled = False\n\nif __name__ == \'__main__\':\n    class Test:\n        timing = Timing()\n\n        def __init__(self, rate):\n            self.rate = rate\n\n        @timing.timeit()\n        def test(self, cost=0.1, epoch=3):\n            for _ in range(epoch):\n                self._test(cost * self.rate)\n\n        @timing.timeit(prefix=""[Core] "")\n        def _test(self, cost):\n            time.sleep(cost)\n\n    class Test1(Test):\n        def __init__(self):\n            Test.__init__(self, 1)\n\n    class Test2(Test):\n        def __init__(self):\n            Test.__init__(self, 2)\n\n    class Test3(Test):\n        def __init__(self):\n            Test.__init__(self, 3)\n\n    test1 = Test1()\n    test2 = Test2()\n    test3 = Test3()\n    test1.test()\n    test2.test()\n    test3.test()\n    test1.timing.show_timing_log()\n'"
Util/Util.py,5,"b'import os\r\nimport cv2\r\nimport math\r\nimport pickle\r\nimport imageio\r\nimport numpy as np\r\nimport tensorflow as tf\r\nimport matplotlib.pyplot as plt\r\nfrom math import pi, sqrt, ceil\r\nfrom tensorflow.python.platform import gfile\r\nfrom tensorflow.python.framework import graph_io\r\nfrom tensorflow.python.tools import freeze_graph\r\n\r\nfrom pylab import mpl\r\nmpl.rcParams[\'font.sans-serif\'] = [\'FangSong\']\r\nmpl.rcParams[\'axes.unicode_minus\'] = False\r\n\r\nplt.switch_backend(""Qt5Agg"")\r\nnp.random.seed(142857)\r\n\r\n\r\nclass Util:\r\n    @staticmethod\r\n    def callable(obj):\r\n        _str_obj = str(obj)\r\n        if callable(obj):\r\n            return True\r\n        if ""<"" not in _str_obj and "">"" not in _str_obj:\r\n            return False\r\n        if _str_obj.find(""function"") >= 0 or _str_obj.find(""staticmethod"") >= 0:\r\n            return True\r\n\r\n    @staticmethod\r\n    def freeze_graph(sess, ckpt, output):\r\n        print(""Loading checkpoint..."")\r\n        saver = tf.train.Saver()\r\n        saver.restore(sess, ckpt)\r\n        print(""Writing graph..."")\r\n        if not os.path.isdir(""_Cache""):\r\n            os.makedirs(""_Cache"")\r\n        _dir = os.path.join(""_Cache"", ""Model"")\r\n        saver.save(sess, _dir)\r\n        graph_io.write_graph(sess.graph, ""_Cache"", ""Model.pb"", False)\r\n        print(""Freezing graph..."")\r\n        freeze_graph.freeze_graph(\r\n            os.path.join(""_Cache"", ""Model.pb""),\r\n            """", True, os.path.join(""_Cache"", ""Model""),\r\n            output, ""save/restore_all"", ""save/Const:0"", ""Frozen.pb"", True, """"\r\n        )\r\n        print(""Done"")\r\n\r\n    @staticmethod\r\n    def load_frozen_graph(graph_dir, fix_nodes=True, entry=None, output=None):\r\n        with gfile.FastGFile(graph_dir, ""rb"") as file:\r\n            graph_def = tf.GraphDef()\r\n            graph_def.ParseFromString(file.read())\r\n            if fix_nodes:\r\n                for node in graph_def.node:\r\n                    if node.op == \'RefSwitch\':\r\n                        node.op = \'Switch\'\r\n                        for index in range(len(node.input)):\r\n                            if \'moving_\' in node.input[index]:\r\n                                node.input[index] = node.input[index] + \'/read\'\r\n                    elif node.op == \'AssignSub\':\r\n                        node.op = \'Sub\'\r\n                        if \'use_locking\' in node.attr:\r\n                            del node.attr[\'use_locking\']\r\n            tf.import_graph_def(graph_def, name="""")\r\n            if entry is not None:\r\n                entry = tf.get_default_graph().get_tensor_by_name(entry)\r\n            if output is not None:\r\n                output = tf.get_default_graph().get_tensor_by_name(output)\r\n            return entry, output\r\n\r\n\r\nclass DataUtil:\r\n    naive_sets = {\r\n        ""mushroom"", ""balloon"", ""mnist"", ""cifar"", ""test""\r\n    }\r\n\r\n    @staticmethod\r\n    def is_naive(name):\r\n        for naive_dataset in DataUtil.naive_sets:\r\n            if naive_dataset in name:\r\n                return True\r\n        return False\r\n\r\n    @staticmethod\r\n    def get_dataset(name, path, n_train=None, tar_idx=None, shuffle=True,\r\n                    quantize=False, quantized=False, one_hot=False, **kwargs):\r\n        x = []\r\n        with open(path, ""r"", encoding=""utf8"") as file:\r\n            if DataUtil.is_naive(name):\r\n                for sample in file:\r\n                    x.append(sample.strip().split("",""))\r\n            elif name == ""bank1.0"":\r\n                for sample in file:\r\n                    sample = sample.replace(\'""\', """")\r\n                    x.append(list(map(lambda c: c.strip(), sample.split("";""))))\r\n            else:\r\n                raise NotImplementedError\r\n        if shuffle:\r\n            np.random.shuffle(x)\r\n        tar_idx = -1 if tar_idx is None else tar_idx\r\n        y = np.array([xx.pop(tar_idx) for xx in x])\r\n        if quantized:\r\n            x = np.asarray(x, dtype=np.float32)\r\n            y = y.astype(np.int8)\r\n            if one_hot:\r\n                y = (y[..., None] == np.arange(np.max(y) + 1))\r\n        else:\r\n            x = np.asarray(x)\r\n        if quantized or not quantize:\r\n            if n_train is None:\r\n                return x, y\r\n            return (x[:n_train], y[:n_train]), (x[n_train:], y[n_train:])\r\n        x, y, wc, features, feat_dicts, label_dict = DataUtil.quantize_data(x, y, **kwargs)\r\n        if one_hot:\r\n            y = (y[..., None] == np.arange(np.max(y)+1)).astype(np.int8)\r\n        if n_train is None:\r\n            return x, y, wc, features, feat_dicts, label_dict\r\n        return (\r\n            (x[:n_train], y[:n_train]), (x[n_train:], y[n_train:]),\r\n            wc, features, feat_dicts, label_dict\r\n        )\r\n\r\n    @staticmethod\r\n    def get_one_hot(y, n_class):\r\n        one_hot = np.zeros([len(y), n_class])\r\n        one_hot[range(len(y)), y] = 1\r\n        return one_hot\r\n\r\n    @staticmethod\r\n    def gen_xor(size=100, scale=1, one_hot=True):\r\n        x = np.random.randn(size) * scale\r\n        y = np.random.randn(size) * scale\r\n        z = np.zeros((size, 2))\r\n        z[x * y >= 0, :] = [0, 1]\r\n        z[x * y < 0, :] = [1, 0]\r\n        if one_hot:\r\n            return np.c_[x, y].astype(np.float32), z\r\n        return np.c_[x, y].astype(np.float32), np.argmax(z, axis=1)\r\n\r\n    @staticmethod\r\n    def gen_spiral(size=50, n=7, n_class=7, scale=4, one_hot=True):\r\n        xs = np.zeros((size * n, 2), dtype=np.float32)\r\n        ys = np.zeros(size * n, dtype=np.int8)\r\n        for i in range(n):\r\n            ix = range(size * i, size * (i + 1))\r\n            r = np.linspace(0.0, 1, size+1)[1:]\r\n            t = np.linspace(2 * i * pi / n, 2 * (i + scale) * pi / n, size) + np.random.random(size=size) * 0.1\r\n            xs[ix] = np.c_[r * np.sin(t), r * np.cos(t)]\r\n            ys[ix] = i % n_class\r\n        if not one_hot:\r\n            return xs, ys\r\n        return xs, DataUtil.get_one_hot(ys, n_class)\r\n\r\n    @staticmethod\r\n    def gen_random(size=100, n_dim=2, n_class=2, scale=1, one_hot=True):\r\n        xs = np.random.randn(size, n_dim).astype(np.float32) * scale\r\n        ys = np.random.randint(n_class, size=size).astype(np.int8)\r\n        if not one_hot:\r\n            return xs, ys\r\n        return xs, DataUtil.get_one_hot(ys, n_class)\r\n\r\n    @staticmethod\r\n    def gen_two_clusters(size=100, n_dim=2, center=0, dis=2, scale=1, one_hot=True):\r\n        center1 = (np.random.random(n_dim) + center - 0.5) * scale + dis\r\n        center2 = (np.random.random(n_dim) + center - 0.5) * scale - dis\r\n        cluster1 = (np.random.randn(size, n_dim) + center1) * scale\r\n        cluster2 = (np.random.randn(size, n_dim) + center2) * scale\r\n        data = np.vstack((cluster1, cluster2)).astype(np.float32)\r\n        labels = np.array([1] * size + [0] * size)\r\n        indices = np.random.permutation(size * 2)\r\n        data, labels = data[indices], labels[indices]\r\n        if not one_hot:\r\n            return data, labels\r\n        return data, DataUtil.get_one_hot(labels, 2)\r\n\r\n    @staticmethod\r\n    def gen_simple_non_linear(size=120, one_hot=True):\r\n        xs = np.random.randn(size, 2).astype(np.float32) * 1.5\r\n        ys = np.zeros(size, dtype=np.int8)\r\n        mask = xs[..., 1] >= xs[..., 0] ** 2\r\n        xs[..., 1][mask] += 2\r\n        ys[mask] = 1\r\n        if not one_hot:\r\n            return xs, ys\r\n        return xs, DataUtil.get_one_hot(ys, 2)\r\n\r\n    @staticmethod\r\n    def gen_nine_grid(size=120, one_hot=True):\r\n        x, y = np.random.randn(2, size).astype(np.float32)\r\n        labels = np.zeros(size, np.int8)\r\n        xl, xr = x <= -1, x >= 1\r\n        yf, yc = y <= -1, y >= 1\r\n        x_mid_mask = ~xl & ~xr\r\n        y_mid_mask = ~yf & ~yc\r\n        mask2 = x_mid_mask & y_mid_mask\r\n        labels[mask2] = 2\r\n        labels[(x_mid_mask | y_mid_mask) & ~mask2] = 1\r\n        xs = np.vstack([x, y]).T\r\n        if not one_hot:\r\n            return xs, labels\r\n        return xs, DataUtil.get_one_hot(labels, 3)\r\n\r\n    @staticmethod\r\n    def gen_x_set(size=1000, centers=(1, 1), slopes=(1, -1), gaps=(0.1, 0.1), one_hot=True):\r\n        xc, yc = centers\r\n        x, y = (2 * np.random.random([size, 2]) + np.asarray(centers) - 1).T.astype(np.float32)\r\n        l1 = (-slopes[0] * (x - xc) + y - yc) > 0\r\n        l2 = (-slopes[1] * (x - xc) + y - yc) > 0\r\n        labels = np.zeros(size, dtype=np.int8)\r\n        mask = (l1 & ~l2) | (~l1 & l2)\r\n        labels[mask] = 1\r\n        x[mask] += gaps[0] * np.sign(x[mask] - centers[0])\r\n        y[~mask] += gaps[1] * np.sign(y[~mask] - centers[1])\r\n        xs = np.vstack([x, y]).T\r\n        if not one_hot:\r\n            return xs, labels\r\n        return xs, DataUtil.get_one_hot(labels, 2)\r\n\r\n    @staticmethod\r\n    def gen_noisy_linear(size=10000, n_dim=100, n_valid=5, noise_scale=0.5, test_ratio=0.15, one_hot=True):\r\n        x_train = np.random.randn(size, n_dim)\r\n        x_train_noise = x_train + np.random.randn(size, n_dim) * noise_scale\r\n        x_test = np.random.randn(int(size*test_ratio), n_dim)\r\n        idx = np.random.permutation(n_dim)[:n_valid]\r\n        w = np.random.randn(n_valid, 1)\r\n        y_train = (x_train[..., idx].dot(w) > 0).astype(np.int8).ravel()\r\n        y_test = (x_test[..., idx].dot(w) > 0).astype(np.int8).ravel()\r\n        if not one_hot:\r\n            return (x_train_noise, y_train), (x_test, y_test)\r\n        return (x_train_noise, DataUtil.get_one_hot(y_train, 2)), (x_test, DataUtil.get_one_hot(y_test, 2))\r\n\r\n    @staticmethod\r\n    def gen_noisy_poly(size=10000, p=3, n_dim=100, n_valid=5, noise_scale=0.5, test_ratio=0.15, one_hot=True):\r\n        p = int(p)\r\n        assert p > 1, ""p should be greater than 1""\r\n        x_train = np.random.randn(size, n_dim)\r\n        x_train_list = [x_train] + [x_train ** i for i in range(2, p+1)]\r\n        x_train_noise = x_train + np.random.randn(size, n_dim) * noise_scale\r\n        x_test = np.random.randn(int(size * test_ratio), n_dim)\r\n        x_test_list = [x_test] + [x_test ** i for i in range(2, p+1)]\r\n        idx_list = [np.random.permutation(n_dim)[:n_valid] for _ in range(p)]\r\n        w_list = [np.random.randn(n_valid, 1) for _ in range(p)]\r\n        o_train = [x[..., idx].dot(w) for x, idx, w in zip(x_train_list, idx_list, w_list)]\r\n        o_test = [x[..., idx].dot(w) for x, idx, w in zip(x_test_list, idx_list, w_list)]\r\n        y_train = (np.sum(o_train, axis=0) > 0).astype(np.int8).ravel()\r\n        y_test = (np.sum(o_test, axis=0) > 0).astype(np.int8).ravel()\r\n        if not one_hot:\r\n            return (x_train_noise, y_train), (x_test, y_test)\r\n        return (x_train_noise, DataUtil.get_one_hot(y_train, 2)), (x_test, DataUtil.get_one_hot(y_test, 2))\r\n\r\n    @staticmethod\r\n    def gen_special_linear(size=10000, n_dim=10, n_redundant=3, n_categorical=3,\r\n                           cv_ratio=0.15, test_ratio=0.15, one_hot=True):\r\n        x_train = np.random.randn(size, n_dim)\r\n        x_train_redundant = np.ones([size, n_redundant]) * np.random.randint(0, 3, n_redundant)\r\n        x_train_categorical = np.random.randint(3, 8, [size, n_categorical])\r\n        x_train_stacked = np.hstack([x_train, x_train_redundant, x_train_categorical])\r\n        n_test = int(size * test_ratio)\r\n        x_test = np.random.randn(n_test, n_dim)\r\n        x_test_redundant = np.ones([n_test, n_redundant]) * np.random.randint(3, 6, n_redundant)\r\n        x_test_categorical = np.random.randint(0, 5, [n_test, n_categorical])\r\n        x_test_stacked = np.hstack([x_test, x_test_redundant, x_test_categorical])\r\n        w = np.random.randn(n_dim, 1)\r\n        y_train = (x_train.dot(w) > 0).astype(np.int8).ravel()\r\n        y_test = (x_test.dot(w) > 0).astype(np.int8).ravel()\r\n        n_cv = int(size * cv_ratio)\r\n        x_train_stacked, x_cv_stacked = x_train_stacked[:-n_cv], x_train_stacked[-n_cv:]\r\n        y_train, y_cv = y_train[:-n_cv], y_train[-n_cv:]\r\n        if not one_hot:\r\n            return (x_train_stacked, y_train), (x_cv_stacked, y_cv), (x_test_stacked, y_test)\r\n        return (\r\n            (x_train_stacked, DataUtil.get_one_hot(y_train, 2)),\r\n            (x_cv_stacked, DataUtil.get_one_hot(y_cv, 2)),\r\n            (x_test_stacked, DataUtil.get_one_hot(y_test, 2))\r\n        )\r\n\r\n    @staticmethod\r\n    def quantize_data(x, y, wc=None, continuous_rate=0.1, separate=False):\r\n        if isinstance(x, list):\r\n            xt = map(list, zip(*x))\r\n        else:\r\n            xt = x.T\r\n        features = [set(feat) for feat in xt]\r\n        if wc is None:\r\n            wc = np.array([len(feat) >= int(continuous_rate * len(y)) for feat in features])\r\n        else:\r\n            wc = np.asarray(wc)\r\n        feat_dicts = [\r\n            {_l: i for i, _l in enumerate(feats)} if not wc[i] else None\r\n            for i, feats in enumerate(features)\r\n        ]\r\n        if not separate:\r\n            if np.all(~wc):\r\n                dtype = np.int\r\n            else:\r\n                dtype = np.float32\r\n            x = np.array([[feat_dicts[i][_l] if not wc[i] else _l for i, _l in enumerate(sample)]\r\n                          for sample in x], dtype=dtype)\r\n        else:\r\n            x = np.array([[feat_dicts[i][_l] if not wc[i] else _l for i, _l in enumerate(sample)]\r\n                          for sample in x], dtype=np.float32)\r\n            x = (x[:, ~wc].astype(np.int), x[:, wc])\r\n        label_dict = {l: i for i, l in enumerate(set(y))}\r\n        y = np.array([label_dict[yy] for yy in y], dtype=np.int8)\r\n        label_dict = {i: l for l, i in label_dict.items()}\r\n        return x, y, wc, features, feat_dicts, label_dict\r\n\r\n    @staticmethod\r\n    def transform_data(x, y, wc, feat_dicts, label_dict):\r\n        if np.all(~wc):\r\n            dtype = np.int\r\n        else:\r\n            dtype = np.float32\r\n        label_dict = {l: i for i, l in label_dict.items()}\r\n        x = np.array([[feat_dicts[i][_l] if not wc[i] else _l for i, _l in enumerate(sample)]\r\n                      for sample in x], dtype=dtype)\r\n        y = np.array([label_dict[yy] for yy in y], dtype=np.int8)\r\n        return x, y\r\n\r\n\r\nclass VisUtil:\r\n    @staticmethod\r\n    def get_colors(line, all_positive):\r\n        # c_base = 60\r\n        # colors = []\r\n        # for weight in line:\r\n        #     colors.append([int(255 * (1 - weight)), int(255 - c_base * abs(1 - 2 * weight)), int(255 * weight)])\r\n        # return colors\r\n        # noinspection PyTypeChecker\r\n        colors = np.full([len(line), 3], [0, 195, 255], dtype=np.uint8)\r\n        if all_positive:\r\n            return colors.tolist()\r\n        colors[line < 0] = [255, 195, 0]\r\n        return colors.tolist()\r\n\r\n    @staticmethod\r\n    def get_line_info(weight, max_thickness=4, threshold=0.2):\r\n        w_min, w_max = np.min(weight), np.max(weight)\r\n        if w_min >= 0:\r\n            weight -= w_min\r\n            all_pos = True\r\n        else:\r\n            all_pos = False\r\n        weight /= max(w_max, -w_min)\r\n        masks = np.abs(weight) >= threshold  # type: np.ndarray\r\n        colors = [VisUtil.get_colors(line, all_pos) for line in weight]\r\n        thicknesses = np.array(\r\n            [[int((max_thickness - 1) * abs(n)) + 1 for n in line] for line in weight]\r\n        )\r\n        return colors, thicknesses, masks\r\n\r\n    @staticmethod\r\n    def get_graphs_from_logs():\r\n        with open(""Results/logs.dat"", ""rb"") as file:\r\n            logs = pickle.load(file)\r\n        for (hus, ep, bt), log in logs.items():\r\n            hus = list(map(lambda _c: str(_c), hus))\r\n            title = ""hus: {} ep: {} bt: {}"".format(\r\n                ""- "" + "" -> "".join(hus) + "" -"", ep, bt\r\n            )\r\n            fb_log, acc_log = log[""fb_log""], log[""acc_log""]\r\n            xs = np.arange(len(fb_log)) + 1\r\n            plt.figure()\r\n            plt.title(title)\r\n            plt.plot(xs, fb_log)\r\n            plt.plot(xs, acc_log, c=""g"")\r\n            plt.savefig(""Results/img/"" + ""{}_{}_{}"".format(\r\n                ""-"".join(hus), ep, bt\r\n            ))\r\n            plt.close()\r\n\r\n    @staticmethod\r\n    def show_img(img, title, normalize=True):\r\n        if normalize:\r\n            img_max, img_min = np.max(img), np.min(img)\r\n            img = 255.0 * (img - img_min) / (img_max - img_min)\r\n        plt.figure()\r\n        plt.title(title)\r\n        plt.imshow(img.astype(\'uint8\'), cmap=plt.cm.gray)\r\n        plt.gca().axis(\'off\')\r\n        plt.show()\r\n\r\n    @staticmethod\r\n    def show_batch_img(batch_img, title, normalize=True):\r\n        _n, height, width = batch_img.shape\r\n        a = int(ceil(sqrt(_n)))\r\n        g = np.ones((a * height + a, a * width + a), batch_img.dtype)\r\n        g *= np.min(batch_img)\r\n        _i = 0\r\n        for y in range(a):\r\n            for x in range(a):\r\n                if _i < _n:\r\n                    g[y * height + y:(y + 1) * height + y, x * width + x:(x + 1) * width + x] = batch_img[_i, :, :]\r\n                    _i += 1\r\n        max_g = g.max()\r\n        min_g = g.min()\r\n        g = (g - min_g) / (max_g - min_g)\r\n        VisUtil.show_img(g, title, normalize)\r\n\r\n    @staticmethod\r\n    def trans_img(img, shape=None):\r\n        if shape is not None:\r\n            img = img.reshape(shape)\r\n        if img.shape[0] == 1:\r\n            return img.reshape(img.shape[1:])\r\n        return img.transpose(1, 2, 0)\r\n\r\n    @staticmethod\r\n    def make_mp4(ims, name="""", fps=20, scale=1, extend=30):\r\n        print(""Making mp4..."")\r\n        ims += [ims[-1]] * extend\r\n        with imageio.get_writer(""{}.mp4"".format(name), mode=\'I\', fps=fps) as writer:\r\n            for im in ims:\r\n                if scale != 1:\r\n                    new_shape = (int(im.shape[1] * scale), int(im.shape[0] * scale))\r\n                    interpolation = cv2.INTER_CUBIC if scale > 1 else cv2.INTER_AREA\r\n                    im = cv2.resize(im, new_shape, interpolation=interpolation)\r\n                writer.append_data(im[..., ::-1])\r\n        print(""Done"")\r\n\r\n\r\nclass Overview:\r\n    def __init__(self, label_dict, shape=(1440, 576)):\r\n        self.shape = shape\r\n        self.label_dict = label_dict\r\n        self.n_col = self.n_row = 0\r\n        self.ans = self.pred = self.results = self.prob = None\r\n\r\n    def _get_detail(self, event, x, y, *_):\r\n        if event == cv2.EVENT_LBUTTONDBLCLK:\r\n            w, h = self.shape\r\n            pw, ph = w / self.n_col, h / self.n_row\r\n            idx = int(x // pw + self.n_col * (y // ph))\r\n            prob = self.prob[idx]\r\n            if self.ans is None or self.ans[idx] == self.pred[idx]:\r\n                title = ""Detail (prob: {:6.4})"".format(prob)\r\n            else:\r\n                title = ""True label: {} (prob: {:6.4})"".format(\r\n                    self.label_dict[self.ans[idx]], prob)\r\n            while 1:\r\n                cv2.imshow(title, self.results[idx])\r\n                if cv2.waitKey(20) & 0xFF == 27:\r\n                    break\r\n            cv2.destroyWindow(title)\r\n\r\n    def _get_results(self, ans, y_pred, images):\r\n        y_pred = np.exp(y_pred)\r\n        y_pred /= np.sum(y_pred, axis=1, keepdims=True)\r\n        pred_classes = np.argmax(y_pred, axis=1)\r\n        if ans is not None:\r\n            true_classes = np.argmax(ans, axis=1)\r\n            true_prob = y_pred[range(len(y_pred)), true_classes]\r\n        else:\r\n            true_classes = None\r\n            true_prob = y_pred[range(len(y_pred)), pred_classes]\r\n        self.ans, self.pred, self.prob = true_classes, pred_classes, true_prob\r\n        c_base = 60\r\n        results = []\r\n        for i, img in enumerate(images):\r\n            pred = y_pred[i]\r\n            indices = np.argsort(pred)[-3:][::-1]\r\n            ps, labels = pred[indices], self.label_dict[indices]\r\n            if true_classes is None:\r\n                color = np.array([255, 255, 255], dtype=np.uint8)\r\n            else:\r\n                p = ps[0]\r\n                if p <= 1 / 2:\r\n                    _l, _r = 2 * c_base + (255 - 2 * c_base) * 2 * p, c_base + (255 - c_base) * 2 * p\r\n                else:\r\n                    _l, _r = 255, 510 * (1 - p)\r\n                if true_classes[i] == pred_classes[i]:\r\n                    color = np.array([0, _l, _r], dtype=np.uint8)\r\n                else:\r\n                    color = np.array([0, _r, _l], dtype=np.uint8)\r\n            rs = np.zeros((256, 640, 3), dtype=np.uint8)\r\n            img = cv2.resize(img, (256, 256))\r\n            rs[:, :256] = img\r\n            rs[:, 256:] = color\r\n            bar_len = 180\r\n            for j, (p, _label) in enumerate(zip(ps, labels)):\r\n                cv2.putText(rs, _label, (288, 64 + 64 * j), cv2.LINE_AA, 0.6, (0, 0, 0), 1)\r\n                cv2.rectangle(rs, (420, 49 + 64 * j), (420 + int(bar_len * p), 69 + 64 * j), (125, 0, 125), -1)\r\n            results.append(rs)\r\n        return results\r\n\r\n    def run(self, ans, y_pred, images):\r\n        print(""-"" * 30)\r\n        print(""Visualizing results..."")\r\n        results = self._get_results(ans, y_pred, images)\r\n        n_row = math.ceil(math.sqrt(len(results)))  # type: int\r\n        n_col = math.ceil(len(results) / n_row)\r\n        pictures = []\r\n        for i in range(n_row):\r\n            if i == n_row - 1:\r\n                pictures.append(np.hstack(\r\n                    [*results[i * n_col:], np.zeros((256, 640 * (n_row * n_col - len(results)), 3)) + 255]).astype(\r\n                    np.uint8))\r\n            else:\r\n                pictures.append(np.hstack(\r\n                    results[i * n_col:(i + 1) * n_col]).astype(np.uint8))\r\n        self.results = results\r\n        self.n_row, self.n_col = n_row, n_col\r\n        big_canvas = np.vstack(pictures).astype(np.uint8)\r\n        overview = cv2.resize(big_canvas, self.shape)\r\n\r\n        cv2.namedWindow(""Overview"")\r\n        cv2.setMouseCallback(""Overview"", self._get_detail)\r\n        cv2.imshow(""Overview"", overview)\r\n        cv2.waitKey(0)\r\n        cv2.destroyAllWindows()\r\n\r\n        print(""-"" * 30)\r\n        print(""Done"")\r\n'"
_SKlearn/Ensemble.py,0,"b'import os\nimport sys\nroot_path = os.path.abspath(""../"")\nif root_path not in sys.path:\n    sys.path.append(root_path)\n\nfrom Util.Bases import ClassifierBase\nfrom Util.Metas import SKCompatibleMeta\n\nfrom sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n\n\nclass SKAdaBoost(AdaBoostClassifier, ClassifierBase, metaclass=SKCompatibleMeta):\n    pass\n\n\nclass SKRandomForest(RandomForestClassifier, ClassifierBase, metaclass=SKCompatibleMeta):\n    pass\n'"
_SKlearn/NaiveBayes.py,0,"b'import os\nimport sys\nroot_path = os.path.abspath(""../"")\nif root_path not in sys.path:\n    sys.path.append(root_path)\n\nfrom Util.Bases import ClassifierBase\nfrom Util.Metas import SKCompatibleMeta\n\nimport sklearn.naive_bayes as nb\n\n\nclass SKMultinomialNB(nb.MultinomialNB, ClassifierBase, metaclass=SKCompatibleMeta):\n    pass\n\n\nclass SKGaussianNB(nb.GaussianNB, ClassifierBase, metaclass=SKCompatibleMeta):\n    pass\n'"
_SKlearn/Perceptron.py,0,"b'import os\nimport sys\nroot_path = os.path.abspath(""../"")\nif root_path not in sys.path:\n    sys.path.append(root_path)\n\nfrom Util.Bases import ClassifierBase\nfrom Util.Metas import SKCompatibleMeta\n\nfrom sklearn.linear_model.perceptron import Perceptron\n\n\nclass SKPerceptron(Perceptron, ClassifierBase, metaclass=SKCompatibleMeta):\n    pass\n'"
_SKlearn/SVM.py,0,"b'import os\nimport sys\nroot_path = os.path.abspath(""../"")\nif root_path not in sys.path:\n    sys.path.append(root_path)\n\nfrom Util.Bases import ClassifierBase\nfrom Util.Metas import SKCompatibleMeta\n\nfrom sklearn.svm import SVC, LinearSVC\n\n\nclass SKSVM(SVC, ClassifierBase, metaclass=SKCompatibleMeta):\n    pass\n\n\nclass SKLinearSVM(LinearSVC, ClassifierBase, metaclass=SKCompatibleMeta):\n    pass\n'"
_SKlearn/Tree.py,0,"b'import os\nimport sys\nroot_path = os.path.abspath(""../"")\nif root_path not in sys.path:\n    sys.path.append(root_path)\n\nfrom Util.Bases import ClassifierBase\nfrom Util.Metas import SKCompatibleMeta\n\nfrom sklearn.tree import _tree, DecisionTreeClassifier\n\n\nclass SKTree(DecisionTreeClassifier, ClassifierBase, metaclass=SKCompatibleMeta):\n    def export_structure(self):\n        tree = self.tree_\n\n        def recurse(node, depth):\n            if tree.feature[node] != _tree.TREE_UNDEFINED:\n                feat = tree.feature[node]\n                threshold = tree.threshold[node]\n                yield depth, feat, threshold\n                for pair in recurse(tree.children_left[node], depth + 1):\n                    yield pair\n                yield depth, feat, threshold\n                for pair in recurse(tree.children_right[node], depth + 1):\n                    yield pair\n            else:\n                yield depth, -1, tree.value[node]\n\n        return [pair for pair in recurse(0, 1)]\n\n    def print_structure(self):\n        tree = self.tree_\n        feature_names = [""x"", ""y""]\n\n        feature_name = [\n            feature_names[i] if i != _tree.TREE_UNDEFINED else ""undefined!""\n            for i in tree.feature\n        ]\n        print(""def tree({}):"".format("", "".join(feature_names)))\n\n        def recurse(node, depth):\n            indent = ""  "" * depth\n            if tree.feature[node] != _tree.TREE_UNDEFINED:\n                name = feature_name[node]\n                threshold = tree.threshold[node]\n                print(""{}if {} <= {}:"".format(indent, name, threshold))\n                recurse(tree.children_left[node], depth + 1)\n                print(""{}else:  # if {} > {}"".format(indent, name, threshold))\n                recurse(tree.children_right[node], depth + 1)\n            else:\n                print(""{}return {}"".format(indent, tree.value[node]))\n        recurse(0, 1)\n'"
a_FirstExample/Regression.py,0,"b'import numpy as np\r\nimport matplotlib.pyplot as plt\r\n\r\n\r\n# Read dataset\r\nx, y = [], []\r\nfor sample in open(""../_Data/prices.txt"", ""r""):\r\n    xx, yy = sample.split("","")\r\n    x.append(float(xx))\r\n    y.append(float(yy))\r\nx, y = np.array(x), np.array(y)\r\n# Perform normalization\r\nx = (x - x.mean()) / x.std()\r\n# Scatter dataset\r\nplt.figure()\r\nplt.scatter(x, y, c=""g"", s=20)\r\nplt.show()\r\n\r\nx0 = np.linspace(-2, 4, 100)\r\n\r\n\r\n# Get regression model under LSE criterion with degree \'deg\'\r\ndef get_model(deg):\r\n    return lambda input_x=x0: np.polyval(np.polyfit(x, y, deg), input_x)\r\n\r\n\r\n# Get the cost of regression model above under given x, y\r\ndef get_cost(deg, input_x, input_y):\r\n    return 0.5 * ((get_model(deg)(input_x) - input_y) ** 2).sum()\r\n\r\n# Set degrees\r\ntest_set = (1, 4, 10)\r\nfor d in test_set:\r\n    print(get_cost(d, x, y))\r\n\r\n# Visualize results\r\nplt.scatter(x, y, c=""g"", s=20)\r\nfor d in test_set:\r\n    plt.plot(x0, get_model(d)(), label=""degree = {}"".format(d))\r\nplt.xlim(-2, 4)\r\nplt.ylim(1e5, 8e5)\r\nplt.legend()\r\nplt.show()\r\n'"
c_CvDTree/Cluster.py,0,"b'import os\r\nimport sys\r\nroot_path = os.path.abspath(""../"")\r\nif root_path not in sys.path:\r\n    sys.path.append(root_path)\r\n\r\nimport math\r\nimport numpy as np\r\n\r\nfrom Util.Metas import TimingMeta\r\n\r\n\r\nclass Cluster(metaclass=TimingMeta):\r\n    def __init__(self, x, y, sample_weight=None, base=2):\r\n        self._x, self._y = x.T, y\r\n        if sample_weight is None:\r\n            self._counters = np.bincount(self._y)\r\n        else:\r\n            # noinspection PyTypeChecker\r\n            self._counters = np.bincount(self._y, weights=sample_weight * len(sample_weight))\r\n        self._sample_weight = sample_weight\r\n        self._con_chaos_cache = self._ent_cache = self._gini_cache = None\r\n        self._base = base\r\n\r\n    def __str__(self):\r\n        return ""Cluster""\r\n\r\n    __repr__ = __str__\r\n\r\n    def ent(self, ent=None, eps=1e-12):\r\n        if self._ent_cache is not None and ent is None:\r\n            return self._ent_cache\r\n        y_len = len(self._y)\r\n        if ent is None:\r\n            ent = self._counters\r\n        ent_cache = max(eps, -sum(\r\n            [c / y_len * math.log(c / y_len, self._base) if c != 0 else 0 for c in ent]))\r\n        if ent is None:\r\n            self._ent_cache = ent_cache\r\n        return ent_cache\r\n\r\n    def gini(self, p=None):\r\n        if self._gini_cache is not None and p is None:\r\n            return self._gini_cache\r\n        if p is None:\r\n            p = self._counters\r\n        gini_cache = 1 - np.sum((p / len(self._y)) ** 2)\r\n        if p is None:\r\n            self._gini_cache = gini_cache\r\n        return gini_cache\r\n\r\n    def con_chaos(self, idx, criterion=""ent"", features=None):\r\n        if criterion == ""ent"":\r\n            method = lambda cluster: cluster.ent()\r\n        elif criterion == ""gini"":\r\n            method = lambda cluster: cluster.gini()\r\n        else:\r\n            raise NotImplementedError(""Conditional info criterion \'{}\' not defined"".format(criterion))\r\n        data = self._x[idx]\r\n        if features is None:\r\n            features = np.unique(data)\r\n        tmp_labels = [data == feature for feature in features]\r\n        # noinspection PyTypeChecker\r\n        self._con_chaos_cache = [np.sum(label) for label in tmp_labels]\r\n        label_lst = [self._y[label] for label in tmp_labels]\r\n        rs, chaos_lst, xt = 0, [], self._x.T\r\n        append = chaos_lst.append\r\n        for data_label, tar_label in zip(tmp_labels, label_lst):\r\n            tmp_data = xt[data_label]\r\n            if self._sample_weight is None:\r\n                chaos = method(Cluster(tmp_data, tar_label, base=self._base))\r\n            else:\r\n                new_weights = self._sample_weight[data_label]\r\n                chaos = method(Cluster(tmp_data, tar_label, new_weights / np.sum(new_weights), base=self._base))\r\n            rs += len(tmp_data) / len(data) * chaos\r\n            append(chaos)\r\n        return rs, chaos_lst\r\n\r\n    def info_gain(self, idx, criterion=""ent"", get_chaos_lst=False, features=None):\r\n        if criterion in (""ent"", ""ratio""):\r\n            con_chaos, chaos_lst = self.con_chaos(idx, criterion=""ent"", features=features)\r\n            gain = self.ent() - con_chaos\r\n            if criterion == ""ratio"":\r\n                gain /= self.ent(self._con_chaos_cache)\r\n        elif criterion == ""gini"":\r\n            con_chaos, chaos_lst = self.con_chaos(idx, criterion=""gini"", features=features)\r\n            gain = self.gini() - con_chaos\r\n        else:\r\n            raise NotImplementedError(""Info_gain criterion \'{}\' not defined"".format(criterion))\r\n        return (gain, chaos_lst) if get_chaos_lst else gain\r\n\r\n    def bin_con_chaos(self, idx, tar, criterion=""gini"", continuous=False):\r\n        if criterion == ""ent"":\r\n            method = lambda cluster: cluster.ent()\r\n        elif criterion == ""gini"":\r\n            method = lambda cluster: cluster.gini()\r\n        else:\r\n            raise NotImplementedError(""Conditional info criterion \'{}\' not defined"".format(criterion))\r\n        data = self._x[idx]\r\n        tar = data == tar if not continuous else data < tar\r\n        tmp_labels = [tar, ~tar]\r\n        # noinspection PyTypeChecker\r\n        self._con_chaos_cache = [np.sum(label) for label in tmp_labels]\r\n        label_lst = [self._y[label] for label in tmp_labels]\r\n        rs, chaos_lst, xt = 0, [], self._x.T\r\n        append = chaos_lst.append\r\n        for data_label, tar_label in zip(tmp_labels, label_lst):\r\n            tmp_data = xt[data_label]\r\n            if self._sample_weight is None:\r\n                chaos = method(Cluster(tmp_data, tar_label, base=self._base))\r\n            else:\r\n                new_weights = self._sample_weight[data_label]\r\n                chaos = method(Cluster(tmp_data, tar_label, new_weights / np.sum(new_weights), base=self._base))\r\n            rs += len(tmp_data) / len(data) * chaos\r\n            append(chaos)\r\n        return rs, chaos_lst\r\n\r\n    def bin_info_gain(self, idx, tar, criterion=""gini"", get_chaos_lst=False, continuous=False):\r\n        if criterion in (""ent"", ""ratio""):\r\n            con_chaos, chaos_lst = self.bin_con_chaos(idx, tar, ""ent"", continuous)\r\n            gain = self.ent() - con_chaos\r\n            if criterion == ""ratio"":\r\n                # noinspection PyTypeChecker\r\n                gain = gain / self.ent(self._con_chaos_cache)\r\n        elif criterion == ""gini"":\r\n            con_chaos, chaos_lst = self.bin_con_chaos(idx, tar, ""gini"", continuous)\r\n            gain = self.gini() - con_chaos\r\n        else:\r\n            raise NotImplementedError(""Info_gain criterion \'{}\' not defined"".format(criterion))\r\n        return (gain, chaos_lst) if get_chaos_lst else gain\r\n'"
c_CvDTree/Node.py,0,"b'import os\r\nimport sys\r\nroot_path = os.path.abspath(""../"")\r\nif root_path not in sys.path:\r\n    sys.path.append(root_path)\r\n\r\nimport numpy as np\r\nfrom math import log2\r\n\r\nfrom c_CvDTree.Cluster import Cluster\r\n\r\nfrom Util.Metas import TimingMeta\r\n\r\n\r\nclass CvDNode(metaclass=TimingMeta):\r\n    def __init__(self, tree=None, base=2, chaos=None,\r\n                 depth=0, parent=None, is_root=True, prev_feat=""Root"", **kwargs):\r\n        self._x = self._y = None\r\n        self.base, self.chaos = base, chaos\r\n        self.criterion = self.category = None\r\n        self.left_child = self.right_child = None\r\n        self._children, self.leafs = {}, {}\r\n        self.sample_weight = None\r\n        self.wc = None\r\n\r\n        self.tree = tree\r\n        if tree is not None:\r\n            self.wc = tree.whether_continuous\r\n            tree.nodes.append(self)\r\n        self.feature_dim, self.tar, self.feats = None, None, []\r\n        self.parent, self.is_root = parent, is_root\r\n        self._depth, self.prev_feat = depth, prev_feat\r\n        self.is_cart = self.is_continuous = self.affected = self.pruned = False\r\n\r\n    def __getitem__(self, item):\r\n        if isinstance(item, str):\r\n            return getattr(self, ""_"" + item)\r\n\r\n    def __lt__(self, other):\r\n        return self.prev_feat < other.prev_feat\r\n\r\n    def __str__(self):\r\n        return self.__class__.__name__\r\n\r\n    __repr__ = __str__\r\n\r\n    @property\r\n    def info(self):\r\n        if self.category is None:\r\n            return ""CvDNode ({}) ({} -> {})"".format(\r\n                self._depth, self.prev_feat, self.feature_dim)\r\n        return ""CvDNode ({}) ({} -> class: {})"".format(\r\n            self._depth, self.prev_feat, self.tree.y_transformer[self.category])\r\n\r\n    @property\r\n    def children(self):\r\n        return {\r\n            ""left"": self.left_child, ""right"": self.right_child\r\n        } if (self.is_cart or self.is_continuous) else self._children\r\n\r\n    @property\r\n    def height(self):\r\n        if self.category is not None:\r\n            return 1\r\n        return 1 + max([child.height if child is not None else 0 for child in self.children.values()])\r\n\r\n    @property\r\n    def info_dict(self):\r\n        return {\r\n            ""chaos"": self.chaos,\r\n            ""y"": self._y\r\n        }\r\n\r\n    # Grow\r\n\r\n    def stop1(self, eps):\r\n        if (\r\n            self._x.shape[1] == 0 or (self.chaos is not None and self.chaos <= eps)\r\n            or (self.tree.max_depth is not None and self._depth >= self.tree.max_depth)\r\n        ):\r\n            self._handle_terminate()\r\n            return True\r\n        return False\r\n\r\n    def stop2(self, max_gain, eps):\r\n        if max_gain <= eps:\r\n            self._handle_terminate()\r\n            return True\r\n        return False\r\n\r\n    def get_category(self):\r\n        return np.argmax(np.bincount(self._y))\r\n\r\n    def _handle_terminate(self):\r\n        self.category = self.get_category()\r\n        parent = self.parent\r\n        while parent is not None:\r\n            parent.leafs[id(self)] = self.info_dict\r\n            parent = parent.parent\r\n\r\n    def prune(self):\r\n        self.category = self.get_category()\r\n        pop_lst = [key for key in self.leafs]\r\n        parent = self.parent\r\n        while parent is not None:\r\n            parent.affected = True\r\n            pop = parent.leafs.pop\r\n            for k in pop_lst:\r\n                pop(k)\r\n            parent.leafs[id(self)] = self.info_dict\r\n            parent = parent.parent\r\n        self.mark_pruned()\r\n        self.feature_dim = None\r\n        self.left_child = self.right_child = None\r\n        self._children = {}\r\n        self.leafs = {}\r\n\r\n    def mark_pruned(self):\r\n        self.pruned = True\r\n        for child in self.children.values():\r\n            if child is not None:\r\n                child.mark_pruned()\r\n\r\n    def fit(self, x, y, sample_weight, feature_bound=None, eps=1e-8):\r\n        self._x, self._y = np.atleast_2d(x), np.asarray(y)\r\n        self.sample_weight = sample_weight\r\n        if self.stop1(eps):\r\n            return\r\n        cluster = Cluster(self._x, self._y, sample_weight, self.base)\r\n        if self.is_root:\r\n            if self.criterion == ""gini"":\r\n                self.chaos = cluster.gini()\r\n            else:\r\n                self.chaos = cluster.ent()\r\n        max_gain, chaos_lst = 0, []\r\n        max_feature = max_tar = None\r\n        feat_len = len(self.feats)\r\n        if feature_bound is None:\r\n            indices = range(0, feat_len)\r\n        elif feature_bound == ""log"":\r\n            indices = np.random.permutation(feat_len)[:max(1, int(log2(feat_len)))]\r\n        else:\r\n            indices = np.random.permutation(feat_len)[:feature_bound]\r\n        tmp_feats = [self.feats[i] for i in indices]\r\n        xt, feat_sets = self._x.T, self.tree.feature_sets\r\n        bin_ig, ig = cluster.bin_info_gain, cluster.info_gain\r\n        for feat in tmp_feats:\r\n            if self.wc[feat]:\r\n                samples = np.sort(xt[feat])\r\n                feat_set = (samples[:-1] + samples[1:]) * 0.5\r\n            else:\r\n                if self.is_cart:\r\n                    feat_set = feat_sets[feat]\r\n                else:\r\n                    feat_set = None\r\n            if self.is_cart or self.wc[feat]:\r\n                for tar in feat_set:\r\n                    tmp_gain, tmp_chaos_lst = bin_ig(\r\n                        feat, tar, criterion=self.criterion, get_chaos_lst=True, continuous=self.wc[feat])\r\n                    if tmp_gain > max_gain:\r\n                        (max_gain, chaos_lst), max_feature, max_tar = (tmp_gain, tmp_chaos_lst), feat, tar\r\n            else:\r\n                tmp_gain, tmp_chaos_lst = ig(\r\n                    feat, self.criterion, True, self.tree.feature_sets[feat])\r\n                if tmp_gain > max_gain:\r\n                    (max_gain, chaos_lst), max_feature = (tmp_gain, tmp_chaos_lst), feat\r\n        if self.stop2(max_gain, eps):\r\n            return\r\n        self.feature_dim = max_feature\r\n        if self.is_cart or self.wc[max_feature]:\r\n            self.tar = max_tar\r\n            self._gen_children(chaos_lst, feature_bound)\r\n            if (self.left_child.category is not None and\r\n                    self.left_child.category == self.right_child.category):\r\n                self.prune()\r\n                self.tree.reduce_nodes()\r\n        else:\r\n            self._gen_children(chaos_lst, feature_bound)\r\n\r\n    def _gen_children(self, chaos_lst, feature_bound):\r\n        feat, tar = self.feature_dim, self.tar\r\n        self.is_continuous = continuous = self.wc[feat]\r\n        features = self._x[..., feat]\r\n        new_feats = self.feats.copy()\r\n        if continuous:\r\n            mask = features < tar\r\n            masks = [mask, ~mask]\r\n        else:\r\n            if self.is_cart:\r\n                mask = features == tar\r\n                masks = [mask, ~mask]\r\n                self.tree.feature_sets[feat].discard(tar)\r\n            else:\r\n                masks = None\r\n        if self.is_cart or continuous:\r\n            feats = [tar, ""+""] if not continuous else [""{:6.4}-"".format(tar), ""{:6.4}+"".format(tar)]\r\n            for feat, side, chaos in zip(feats, [""left_child"", ""right_child""], chaos_lst):\r\n                new_node = self.__class__(\r\n                    self.tree, self.base, chaos=chaos,\r\n                    depth=self._depth + 1, parent=self, is_root=False, prev_feat=feat)\r\n                new_node.criterion = self.criterion\r\n                setattr(self, side, new_node)\r\n            for node, feat_mask in zip([self.left_child, self.right_child], masks):\r\n                if self.sample_weight is None:\r\n                    local_weights = None\r\n                else:\r\n                    local_weights = self.sample_weight[feat_mask]\r\n                    local_weights /= np.sum(local_weights)\r\n                tmp_data, tmp_labels = self._x[feat_mask, ...], self._y[feat_mask]\r\n                if len(tmp_labels) == 0:\r\n                    continue\r\n                node.feats = new_feats\r\n                node.fit(tmp_data, tmp_labels, local_weights, feature_bound)\r\n        else:\r\n            new_feats.remove(self.feature_dim)\r\n            for feat, chaos in zip(self.tree.feature_sets[self.feature_dim], chaos_lst):\r\n                feat_mask = features == feat\r\n                tmp_x = self._x[feat_mask, ...]\r\n                if len(tmp_x) == 0:\r\n                    continue\r\n                new_node = self.__class__(\r\n                    tree=self.tree, base=self.base, chaos=chaos,\r\n                    depth=self._depth + 1, parent=self, is_root=False, prev_feat=feat)\r\n                new_node.feats = new_feats\r\n                self.children[feat] = new_node\r\n                if self.sample_weight is None:\r\n                    local_weights = None\r\n                else:\r\n                    local_weights = self.sample_weight[feat_mask]\r\n                    local_weights /= np.sum(local_weights)\r\n                new_node.fit(tmp_x, self._y[feat_mask], local_weights, feature_bound)\r\n\r\n    # Util\r\n\r\n    def update_layers(self):\r\n        self.tree.layers[self._depth].append(self)\r\n        for node in sorted(self.children):\r\n            node = self.children[node]\r\n            if node is not None:\r\n                node.update_layers()\r\n\r\n    def cost(self, pruned=False):\r\n        if not pruned:\r\n            return sum([leaf[""chaos""] * len(leaf[""y""]) for leaf in self.leafs.values()])\r\n        return self.chaos * len(self._y)\r\n\r\n    def get_threshold(self):\r\n        return (self.cost(pruned=True) - self.cost()) / (len(self.leafs) - 1)\r\n\r\n    def cut_tree(self):\r\n        self.tree = None\r\n        for child in self.children.values():\r\n            if child is not None:\r\n                child.cut_tree()\r\n\r\n    def feed_tree(self, tree):\r\n        self.tree = tree\r\n        self.tree.nodes.append(self)\r\n        self.wc = tree.whether_continuous\r\n        for child in self.children.values():\r\n            if child is not None:\r\n                child.feed_tree(tree)\r\n\r\n    def predict_one(self, x):\r\n        if self.category is not None:\r\n            return self.category\r\n        if self.is_continuous:\r\n            if x[self.feature_dim] < self.tar:\r\n                return self.left_child.predict_one(x)\r\n            return self.right_child.predict_one(x)\r\n        if self.is_cart:\r\n            if x[self.feature_dim] == self.tar:\r\n                return self.left_child.predict_one(x)\r\n            return self.right_child.predict_one(x)\r\n        else:\r\n            try:\r\n                return self.children[x[self.feature_dim]].predict_one(x)\r\n            except KeyError:\r\n                return self.get_category()\r\n\r\n    def predict(self, x):\r\n        return np.array([self.predict_one(xx) for xx in x])\r\n\r\n    def view(self, indent=4):\r\n        print("" "" * indent * self._depth, self.info)\r\n        for node in sorted(self.children):\r\n            node = self.children[node]\r\n            if node is not None:\r\n                node.view()\r\n\r\n\r\nclass ID3Node(CvDNode):\r\n    def __init__(self, *args, **kwargs):\r\n        CvDNode.__init__(self, *args, **kwargs)\r\n        self.criterion = ""ent""\r\n\r\n\r\nclass C45Node(CvDNode):\r\n    def __init__(self, *args, **kwargs):\r\n        CvDNode.__init__(self, *args, **kwargs)\r\n        self.criterion = ""ratio""\r\n\r\n\r\nclass CartNode(CvDNode):\r\n    def __init__(self, *args, **kwargs):\r\n        CvDNode.__init__(self, *args, **kwargs)\r\n        self.criterion = ""gini""\r\n        self.is_cart = True\r\n'"
c_CvDTree/TestTree.py,0,"b'import os\nimport sys\nroot_path = os.path.abspath(""../"")\nif root_path not in sys.path:\n    sys.path.append(root_path)\n\nimport time\n\nfrom c_CvDTree.Tree import *\n\nfrom Util.Util import DataUtil\n\n\ndef main(visualize=True):\n    # x, y = DataUtil.get_dataset(""balloon1.0(en)"", ""../_Data/balloon1.0(en).txt"")\n    x, y = DataUtil.get_dataset(""test"", ""../_Data/test.txt"")\n    fit_time = time.time()\n    tree = CartTree(whether_continuous=[False] * 4)\n    tree.fit(x, y, train_only=True)\n    fit_time = time.time() - fit_time\n    if visualize:\n        tree.view()\n    estimate_time = time.time()\n    tree.evaluate(x, y)\n    estimate_time = time.time() - estimate_time\n    print(\n        ""Model building  : {:12.6} s\\n""\n        ""Estimation      : {:12.6} s\\n""\n        ""Total           : {:12.6} s"".format(\n            fit_time, estimate_time,\n            fit_time + estimate_time\n        )\n    )\n    if visualize:\n        tree.visualize()\n\n    train_num = 6000\n    (x_train, y_train), (x_test, y_test), *_ = DataUtil.get_dataset(\n        ""mushroom"", ""../_Data/mushroom.txt"", tar_idx=0, n_train=train_num)\n    fit_time = time.time()\n    tree = C45Tree()\n    tree.fit(x_train, y_train)\n    fit_time = time.time() - fit_time\n    if visualize:\n        tree.view()\n    estimate_time = time.time()\n    tree.evaluate(x_train, y_train)\n    tree.evaluate(x_test, y_test)\n    estimate_time = time.time() - estimate_time\n    print(\n        ""Model building  : {:12.6} s\\n""\n        ""Estimation      : {:12.6} s\\n""\n        ""Total           : {:12.6} s"".format(\n            fit_time, estimate_time,\n            fit_time + estimate_time\n        )\n    )\n    if visualize:\n        tree.visualize()\n\n    x, y = DataUtil.gen_xor(one_hot=False)\n    fit_time = time.time()\n    tree = CartTree()\n    tree.fit(x, y, train_only=True)\n    fit_time = time.time() - fit_time\n    if visualize:\n        tree.view()\n    estimate_time = time.time()\n    tree.evaluate(x, y, n_cores=1)\n    estimate_time = time.time() - estimate_time\n    print(\n        ""Model building  : {:12.6} s\\n""\n        ""Estimation      : {:12.6} s\\n""\n        ""Total           : {:12.6} s"".format(\n            fit_time, estimate_time,\n            fit_time + estimate_time\n        )\n    )\n    if visualize:\n        tree.visualize2d(x, y, dense=1000)\n        tree.visualize()\n\n    wc = [False] * 16\n    continuous_lst = [0, 5, 9, 11, 12, 13, 14]\n    for _cl in continuous_lst:\n        wc[_cl] = True\n\n    train_num = 2000\n    (x_train, y_train), (x_test, y_test), *_ = DataUtil.get_dataset(\n        ""bank1.0"", ""../_Data/bank1.0.txt"", n_train=train_num, quantize=True)\n    fit_time = time.time()\n    tree = CartTree()\n    tree.fit(x_train, y_train)\n    fit_time = time.time() - fit_time\n    if visualize:\n        tree.view()\n    estimate_time = time.time()\n    tree.evaluate(x_test, y_test)\n    estimate_time = time.time() - estimate_time\n    print(\n        ""Model building  : {:12.6} s\\n""\n        ""Estimation      : {:12.6} s\\n""\n        ""Total           : {:12.6} s"".format(\n            fit_time, estimate_time,\n            fit_time + estimate_time\n        )\n    )\n    if visualize:\n        tree.visualize()\n\n    tree.show_timing_log()\n\n\nif __name__ == \'__main__\':\n    main(False)\n'"
c_CvDTree/Tree.py,0,"b'import os\r\nimport sys\r\nroot_path = os.path.abspath(""../"")\r\nif root_path not in sys.path:\r\n    sys.path.append(root_path)\r\n\r\nimport cv2\r\nfrom copy import deepcopy\r\n\r\nfrom c_CvDTree.Node import *\r\n\r\nfrom Util.Timing import Timing\r\nfrom Util.Bases import ClassifierBase\r\n\r\n\r\ndef cvd_task(args):\r\n    x, clf, n_cores = args\r\n    return np.array([clf.root.predict_one(xx) for xx in x])\r\n\r\n\r\nclass CvDBase(ClassifierBase):\r\n    CvDBaseTiming = Timing()\r\n\r\n    def __init__(self, whether_continuous=None, max_depth=None, node=None, **kwargs):\r\n        super(CvDBase, self).__init__(**kwargs)\r\n        self.nodes, self.layers, self.roots = [], [], []\r\n        self.max_depth = max_depth\r\n        self.root = node\r\n        self.feature_sets = []\r\n        self.prune_alpha = 1\r\n        self.y_transformer = None\r\n        self.whether_continuous = whether_continuous\r\n\r\n        self._params[""alpha""] = kwargs.get(""alpha"", None)\r\n        self._params[""eps""] = kwargs.get(""eps"", 1e-8)\r\n        self._params[""cv_rate""] = kwargs.get(""cv_rate"", 0.2)\r\n        self._params[""train_only""] = kwargs.get(""train_only"", False)\r\n        self._params[""feature_bound""] = kwargs.get(""feature_bound"", None)\r\n\r\n    def feed_data(self, x, continuous_rate=0.2):\r\n        xt = x.T\r\n        self.feature_sets = [set(dimension) for dimension in xt]\r\n        data_len, data_dim = x.shape\r\n        if self.whether_continuous is None:\r\n            self.whether_continuous = np.array(\r\n                [len(feat) >= int(continuous_rate * data_len) for feat in self.feature_sets])\r\n        else:\r\n            self.whether_continuous = np.asarray(self.whether_continuous)\r\n        self.root.feats = [i for i in range(x.shape[1])]\r\n        self.root.feed_tree(self)\r\n\r\n    # Grow\r\n\r\n    @CvDBaseTiming.timeit(level=1, prefix=""[API] "")\r\n    def fit(self, x, y, sample_weight=None, alpha=None, eps=None,\r\n            cv_rate=None, train_only=None, feature_bound=None):\r\n        if sample_weight is None:\r\n            sample_weight = self._params[""sample_weight""]\r\n        if alpha is None:\r\n            alpha = self._params[""alpha""]\r\n        if eps is None:\r\n            eps = self._params[""eps""]\r\n        if cv_rate is None:\r\n            cv_rate = self._params[""cv_rate""]\r\n        if train_only is None:\r\n            train_only = self._params[""train_only""]\r\n        if feature_bound is None:\r\n            feature_bound = self._params[""feature_bound""]\r\n        self.y_transformer, y = np.unique(y, return_inverse=True)\r\n        x = np.atleast_2d(x)\r\n        self.prune_alpha = alpha if alpha is not None else x.shape[1] / 2\r\n        if not train_only and self.root.is_cart:\r\n            train_num = int(len(x) * (1-cv_rate))\r\n            indices = np.random.permutation(np.arange(len(x)))\r\n            train_indices = indices[:train_num]\r\n            test_indices = indices[train_num:]\r\n            if sample_weight is not None:\r\n                train_weights = sample_weight[train_indices]\r\n                test_weights = sample_weight[test_indices]\r\n                train_weights /= np.sum(train_weights)\r\n                test_weights /= np.sum(test_weights)\r\n            else:\r\n                train_weights = test_weights = None\r\n            x_train, y_train = x[train_indices], y[train_indices]\r\n            x_cv, y_cv = x[test_indices], y[test_indices]\r\n        else:\r\n            x_train, y_train, train_weights = x, y, sample_weight\r\n            x_cv = y_cv = test_weights = None\r\n        self.feed_data(x_train)\r\n        self.root.fit(x_train, y_train, train_weights, feature_bound, eps)\r\n        self.prune(x_cv, y_cv, test_weights)\r\n\r\n    @CvDBaseTiming.timeit(level=3, prefix=""[Util] "")\r\n    def reduce_nodes(self):\r\n        pop = self.nodes.pop\r\n        for i in range(len(self.nodes)-1, -1, -1):\r\n            if self.nodes[i].pruned:\r\n                pop(i)\r\n\r\n    # Prune\r\n\r\n    @CvDBaseTiming.timeit(level=4)\r\n    def _update_layers(self):\r\n        self.layers = [[] for _ in range(self.root.height)]\r\n        self.root.update_layers()\r\n\r\n    @CvDBaseTiming.timeit(level=1)\r\n    def _prune(self):\r\n        self._update_layers()\r\n        tmp_nodes = []\r\n        append = tmp_nodes.append\r\n        for node_lst in self.layers[::-1]:\r\n            for node in node_lst[::-1]:\r\n                if node.category is None:\r\n                    append(node)\r\n        old = np.array([node.cost() + self.prune_alpha * len(node.leafs) for node in tmp_nodes])\r\n        new = np.array([node.cost(pruned=True) + self.prune_alpha for node in tmp_nodes])\r\n        mask = old >= new\r\n        while True:\r\n            if self.root.height == 1:\r\n                break\r\n            p = np.argmax(mask)  # type: int\r\n            if mask[p]:\r\n                tmp_nodes[p].prune()\r\n                for i, node in enumerate(tmp_nodes):\r\n                    if node.affected:\r\n                        old[i] = node.cost() + self.prune_alpha * len(node.leafs)\r\n                        mask[i] = old[i] >= new[i]\r\n                        node.affected = False\r\n                for i in range(len(tmp_nodes) - 1, -1, -1):\r\n                    if tmp_nodes[i].pruned:\r\n                        tmp_nodes.pop(i)\r\n                        old = np.delete(old, i)\r\n                        new = np.delete(new, i)\r\n                        mask = np.delete(mask, i)\r\n            else:\r\n                break\r\n        self.reduce_nodes()\r\n\r\n    @CvDBaseTiming.timeit(level=1)\r\n    def _cart_prune(self):\r\n        self.root.cut_tree()\r\n        tmp_nodes = [node for node in self.nodes if node.category is None]\r\n        thresholds = np.array([node.get_threshold() for node in tmp_nodes])\r\n        while True:\r\n            root_copy = deepcopy(self.root)\r\n            self.roots.append(root_copy)\r\n            if self.root.height == 1:\r\n                break\r\n            p = np.argmin(thresholds)  # type: int\r\n            tmp_nodes[p].prune()\r\n            for i, node in enumerate(tmp_nodes):\r\n                if node.affected:\r\n                    thresholds[i] = node.get_threshold()\r\n                    node.affected = False\r\n            pop = tmp_nodes.pop\r\n            for i in range(len(tmp_nodes) - 1, -1, -1):\r\n                if tmp_nodes[i].pruned:\r\n                    pop(i)\r\n                    thresholds = np.delete(thresholds, i)\r\n        self.reduce_nodes()\r\n\r\n    @CvDBaseTiming.timeit(level=3, prefix=""[Util] "")\r\n    def prune(self, x_cv, y_cv, weights):\r\n        if self.root.is_cart:\r\n            if x_cv is not None and y_cv is not None:\r\n                self._cart_prune()\r\n                arg = np.argmax([CvDBase.acc(y_cv, tree.predict(x_cv), weights) for tree in self.roots])  # type: int\r\n                tar_root = self.roots[arg]\r\n                self.nodes = []\r\n                tar_root.feed_tree(self)\r\n                self.root = tar_root\r\n        else:\r\n            self._prune()\r\n\r\n    # Util\r\n\r\n    @CvDBaseTiming.timeit(level=1, prefix=""[API] "")\r\n    def predict_one(self, x):\r\n        return self.y_transformer[self.root.predict_one(x)]\r\n\r\n    @CvDBaseTiming.timeit(level=3, prefix=""[API] "")\r\n    def predict(self, x, get_raw_results=False, **kwargs):\r\n        return self.y_transformer[self._multi_data(x, cvd_task, kwargs)]\r\n\r\n    @CvDBaseTiming.timeit(level=3, prefix=""[API] "")\r\n    def view(self):\r\n        self.root.view()\r\n\r\n    @CvDBaseTiming.timeit(level=2, prefix=""[API] "")\r\n    def visualize(self, radius=24, width=1200, height=800,\r\n                  height_padding_ratio=0.2, width_padding=30, title=""CvDTree""):\r\n        self._update_layers()\r\n        n_units = [len(layer) for layer in self.layers]\r\n\r\n        img = np.ones((height, width, 3), np.uint8) * 255\r\n        height_padding = int(\r\n            height / (len(self.layers) - 1 + 2 * height_padding_ratio)\r\n        ) * height_padding_ratio + width_padding\r\n        height_axis = np.linspace(\r\n            height_padding, height - height_padding, len(self.layers), dtype=np.int)\r\n        width_axis = [\r\n            np.linspace(width_padding, width - width_padding, unit + 2, dtype=np.int)\r\n            for unit in n_units\r\n        ]\r\n        width_axis = [axis[1:-1] for axis in width_axis]\r\n\r\n        for i, (y, xs) in enumerate(zip(height_axis, width_axis)):\r\n            for j, x in enumerate(xs):\r\n                if i == 0:\r\n                    cv2.circle(img, (x, y), radius, (225, 100, 125), 1)\r\n                else:\r\n                    cv2.circle(img, (x, y), radius, (125, 100, 225), 1)\r\n                node = self.layers[i][j]\r\n                if node.feature_dim is not None:\r\n                    text = str(node.feature_dim + 1)\r\n                    color = (0, 0, 255)\r\n                else:\r\n                    text = str(self.y_transformer[node.category])\r\n                    color = (0, 255, 0)\r\n                cv2.putText(img, text, (x-7*len(text)+2, y+3), cv2.LINE_AA, 0.6, color, 1)\r\n\r\n        for i, y in enumerate(height_axis):\r\n            if i == len(height_axis) - 1:\r\n                break\r\n            for j, x in enumerate(width_axis[i]):\r\n                new_y = height_axis[i + 1]\r\n                dy = new_y - y - 2 * radius\r\n                for k, new_x in enumerate(width_axis[i + 1]):\r\n                    dx = new_x - x\r\n                    length = np.sqrt(dx**2+dy**2)\r\n                    ratio = 0.5 - min(0.4, 1.2 * 24/length)\r\n                    if self.layers[i + 1][k] in self.layers[i][j].children.values():\r\n                        cv2.line(img, (x, y+radius), (x+int(dx*ratio), y+radius+int(dy*ratio)),\r\n                                 (125, 125, 125), 1)\r\n                        cv2.putText(img, str(self.layers[i+1][k].prev_feat),\r\n                                    (x+int(dx*0.5)-6, y+radius+int(dy*0.5)),\r\n                                    cv2.LINE_AA, 0.6, (0, 0, 0), 1)\r\n                        cv2.line(img, (new_x-int(dx*ratio), new_y-radius-int(dy*ratio)), (new_x, new_y-radius),\r\n                                 (125, 125, 125), 1)\r\n\r\n        cv2.imshow(title, img)\r\n        cv2.waitKey(0)\r\n        return img\r\n\r\n\r\nclass CvDMeta(type):\r\n    def __new__(mcs, *args, **kwargs):\r\n        name, bases, attr = args[:3]\r\n        _, _node = bases\r\n\r\n        def __init__(self, whether_continuous=None, max_depth=None, node=None, **_kwargs):\r\n            tmp_node = node if isinstance(node, CvDNode) else _node\r\n            CvDBase.__init__(self, whether_continuous, max_depth, tmp_node(**_kwargs))\r\n            self._name = name\r\n\r\n        attr[""__init__""] = __init__\r\n        return type(name, bases, attr)\r\n\r\n\r\nclass ID3Tree(CvDBase, ID3Node, metaclass=CvDMeta):\r\n    pass\r\n\r\n\r\nclass C45Tree(CvDBase, C45Node, metaclass=CvDMeta):\r\n    pass\r\n\r\n\r\nclass CartTree(CvDBase, CartNode, metaclass=CvDMeta):\r\n    pass\r\n'"
d_Ensemble/AdaBoost.py,0,"b'import os\r\nimport sys\r\nroot_path = os.path.abspath(""../"")\r\nif root_path not in sys.path:\r\n    sys.path.append(root_path)\r\n\r\nfrom math import log\r\n\r\nfrom b_NaiveBayes.Vectorized.MultinomialNB import MultinomialNB\r\nfrom b_NaiveBayes.Vectorized.GaussianNB import GaussianNB\r\nfrom c_CvDTree.Tree import *\r\nfrom d_Ensemble.RandomForest import RandomForest\r\nfrom e_SVM.Perceptron import Perceptron\r\nfrom e_SVM.KP import KP\r\nfrom e_SVM.SVM import SVM\r\n\r\nfrom Util.ProgressBar import ProgressBar\r\n\r\nfrom _SKlearn.NaiveBayes import *\r\nfrom _SKlearn.Tree import SKTree\r\nfrom _SKlearn.SVM import SKSVM\r\n\r\n\r\ndef boost_task(args):\r\n    x, clfs, n_cores = args\r\n    return [clf.predict(x, n_cores=n_cores) for clf in clfs]\r\n\r\n\r\nclass AdaBoost(ClassifierBase):\r\n    AdaBoostTiming = Timing()\r\n    _weak_clf = {\r\n        ""SKMNB"": SKMultinomialNB,\r\n        ""SKGNB"": SKGaussianNB,\r\n        ""SKTree"": SKTree,\r\n        ""SKSVM"": SKSVM,\r\n\r\n        ""MNB"": MultinomialNB,\r\n        ""GNB"": GaussianNB,\r\n        ""ID3"": ID3Tree,\r\n        ""C45"": C45Tree,\r\n        ""Cart"": CartTree,\r\n        ""RF"": RandomForest,\r\n        ""Perceptron"": Perceptron,\r\n        ""KP"": KP,\r\n        ""SVM"": SVM\r\n    }\r\n\r\n    def __init__(self, **kwargs):\r\n        super(AdaBoost, self).__init__(**kwargs)\r\n        self._clf, self._clfs, self._clfs_weights = """", [], []\r\n        self._kwarg_cache = {}\r\n\r\n        self._params[""clf""] = kwargs.get(""clf"", None)\r\n        self._params[""epoch""] = kwargs.get(""epoch"", 10)\r\n        self._params[""eps""] = kwargs.get(""eps"", 1e-12)\r\n\r\n    @property\r\n    def params(self):\r\n        rs = """"\r\n        if self._kwarg_cache:\r\n            tmp_rs = []\r\n            for key, value in self._kwarg_cache.items():\r\n                tmp_rs.append(""{}: {}"".format(key, value))\r\n            rs += ""( "" + ""; "".join(tmp_rs) + "" )""\r\n        return rs\r\n\r\n    @property\r\n    def title(self):\r\n        rs = ""Classifier: {}; Num: {}"".format(self._clf, len(self._clfs))\r\n        rs += "" "" + self.params\r\n        return rs\r\n\r\n    @AdaBoostTiming.timeit(level=1, prefix=""[API] "")\r\n    def fit(self, x, y, sample_weight=None, clf=None, epoch=None, eps=None, **kwargs):\r\n        if sample_weight is None:\r\n            sample_weight = self._params[""sample_weight""]\r\n        if clf is None:\r\n            clf = self._params[""clf""]\r\n        if epoch is None:\r\n            epoch = self._params[""epoch""]\r\n        if eps is None:\r\n            eps = self._params[""eps""]\r\n        x, y = np.atleast_2d(x), np.asarray(y)\r\n        if clf is None:\r\n            clf = ""Cart""\r\n            kwargs = {""max_depth"": 1}\r\n        self._kwarg_cache = kwargs\r\n        self._clf = clf\r\n        if sample_weight is None:\r\n            sample_weight = np.ones(len(y)) / len(y)\r\n        else:\r\n            sample_weight = np.asarray(sample_weight)\r\n        bar = ProgressBar(max_value=epoch, name=""AdaBoost"")\r\n        for _ in range(epoch):\r\n            tmp_clf = AdaBoost._weak_clf[clf](**kwargs)\r\n            tmp_clf.fit(x, y, sample_weight=sample_weight)\r\n            y_pred = tmp_clf.predict(x)\r\n            em = min(max((y_pred != y).astype(np.int8).dot(sample_weight[..., None])[0], eps), 1 - eps)\r\n            am = 0.5 * log(1 / em - 1)\r\n            sample_weight *= np.exp(-am * y * y_pred)\r\n            sample_weight /= np.sum(sample_weight)\r\n            self._clfs.append(deepcopy(tmp_clf))\r\n            self._clfs_weights.append(am)\r\n            bar.update()\r\n        self._clfs_weights = np.array(self._clfs_weights, dtype=np.float32)\r\n\r\n    @AdaBoostTiming.timeit(level=1, prefix=""[API] "")\r\n    def predict(self, x, get_raw_results=False, bound=None, **kwargs):\r\n        x = np.atleast_2d(x)\r\n        if bound is None:\r\n            clfs, clfs_weights = self._clfs, self._clfs_weights\r\n        else:\r\n            clfs, clfs_weights = self._clfs[:bound], self._clfs_weights[:bound]\r\n        matrix = self._multi_clf(x, clfs, boost_task, kwargs)\r\n        matrix *= clfs_weights\r\n        rs = np.sum(matrix, axis=1)\r\n        del matrix\r\n        if not get_raw_results:\r\n            return np.sign(rs)\r\n        return rs\r\n'"
d_Ensemble/RandomForest.py,0,"b'import os\r\nimport sys\r\nroot_path = os.path.abspath(""../"")\r\nif root_path not in sys.path:\r\n    sys.path.append(root_path)\r\n\r\nfrom c_CvDTree.Tree import *\r\n\r\nfrom Util.Util import DataUtil\r\nfrom Util.ProgressBar import ProgressBar\r\n\r\n\r\ndef rf_task(args):\r\n    x, trees, n_cores = args\r\n    return [tree.predict(x, n_cores=n_cores) for tree in trees]\r\n\r\n\r\nclass RandomForest(ClassifierBase):\r\n    RandomForestTiming = Timing()\r\n    cvd_trees = {\r\n        ""ID3"": ID3Tree,\r\n        ""C45"": C45Tree,\r\n        ""Cart"": CartTree\r\n    }\r\n\r\n    def __init__(self, **kwargs):\r\n        super(RandomForest, self).__init__(**kwargs)\r\n        self._tree, self._trees = """", []\r\n\r\n        self._params[""tree""] = kwargs.get(""tree"", ""Cart"")\r\n        self._params[""epoch""] = kwargs.get(""epoch"", 10)\r\n        self._params[""feature_bound""] = kwargs.get(""feature_bound"", ""log"")\r\n\r\n    @property\r\n    def title(self):\r\n        return ""Tree: {}; Num: {}"".format(self._tree, len(self._trees))\r\n\r\n    @staticmethod\r\n    @RandomForestTiming.timeit(level=2, prefix=""[Core] "")\r\n    def most_appearance(arr):\r\n        u, c = np.unique(arr, return_counts=True)\r\n        return u[np.argmax(c)]\r\n\r\n    @RandomForestTiming.timeit(level=1, prefix=""[API] "")\r\n    def fit(self, x, y, sample_weight=None, tree=None, epoch=None, feature_bound=None, **kwargs):\r\n        if sample_weight is None:\r\n            sample_weight = self._params[""sample_weight""]\r\n        if tree is None:\r\n            tree = self._params[""tree""]\r\n        if epoch is None:\r\n            epoch = self._params[""epoch""]\r\n        if feature_bound is None:\r\n            feature_bound = self._params[""feature_bound""]\r\n        x, y = np.atleast_2d(x), np.asarray(y)\r\n        n_sample = len(y)\r\n        self._tree = tree\r\n        bar = ProgressBar(max_value=epoch, name=""RF"")\r\n        for _ in range(epoch):\r\n            tmp_tree = RandomForest.cvd_trees[tree](**kwargs)\r\n            indices = np.random.randint(n_sample, size=n_sample)\r\n            if sample_weight is None:\r\n                local_weight = None\r\n            else:\r\n                local_weight = sample_weight[indices]\r\n                local_weight /= local_weight.sum()\r\n            tmp_tree.fit(x[indices], y[indices], sample_weight=local_weight, feature_bound=feature_bound)\r\n            self._trees.append(deepcopy(tmp_tree))\r\n            bar.update()\r\n\r\n    @RandomForestTiming.timeit(level=1, prefix=""[API] "")\r\n    def predict(self, x, get_raw_results=False, bound=None, **kwargs):\r\n        trees = self._trees if bound is None else self._trees[:bound]\r\n        matrix = self._multi_clf(x, trees, rf_task, kwargs, target=kwargs.get(""target"", ""parallel""))\r\n        return np.array([RandomForest.most_appearance(rs) for rs in matrix])\r\n\r\n    @RandomForestTiming.timeit(level=1, prefix=""[API] "")\r\n    def evaluate(self, x, y, metrics=None, tar=0, prefix=""Acc"", **kwargs):\r\n        kwargs[""target""] = ""single""\r\n        super(RandomForest, self).evaluate(x, y, metrics, tar, prefix, **kwargs)\r\n\r\nif __name__ == \'__main__\':\r\n    import time\r\n\r\n    train_num = 100\r\n    (x_train, y_train), (x_test, y_test) = DataUtil.get_dataset(\r\n        ""mushroom"", ""../_Data/mushroom.txt"", n_train=train_num, tar_idx=0)\r\n\r\n    learning_time = time.time()\r\n    forest = RandomForest()\r\n    forest.fit(x_train, y_train)\r\n    learning_time = time.time() - learning_time\r\n    estimation_time = time.time()\r\n    forest.evaluate(x_train, y_train)\r\n    forest.evaluate(x_test, y_test)\r\n    estimation_time = time.time() - estimation_time\r\n    print(\r\n        ""Model building  : {:12.6} s\\n""\r\n        ""Estimation      : {:12.6} s\\n""\r\n        ""Total           : {:12.6} s"".format(\r\n            learning_time, estimation_time,\r\n            learning_time + estimation_time\r\n        )\r\n    )\r\n    forest.show_timing_log()\r\n'"
d_Ensemble/TestEnsemble.py,0,"b'import os\r\nimport sys\r\nroot_path = os.path.abspath(""../"")\r\nif root_path not in sys.path:\r\n    sys.path.append(root_path)\r\n\r\nimport time\r\n\r\nfrom d_Ensemble.AdaBoost import AdaBoost\r\nfrom d_Ensemble.RandomForest import RandomForest\r\nfrom _SKlearn.Ensemble import SKAdaBoost, SKRandomForest\r\n\r\nfrom Util.Util import DataUtil\r\n\r\nclf_dict = {\r\n    ""AdaBoost"": AdaBoost, ""RF"": RandomForest,\r\n    ""SKAdaBoost"": SKAdaBoost, ""SKRandomForest"": SKRandomForest\r\n}\r\n\r\n\r\ndef test(x, y, algorithm=""AdaBoost"", clf=""Cart"", epoch=10, **kwargs):\r\n    ensemble = clf_dict[algorithm]()\r\n    if ""SK"" in algorithm:\r\n        ensemble.fit(x, y)\r\n    else:\r\n        ensemble.fit(x, y, None, clf, epoch, **kwargs)\r\n    ensemble.visualize2d(x, y, **kwargs)\r\n    ensemble.evaluate(x, y, **kwargs)\r\n\r\n\r\ndef cv_test(x, y, xt, yt, algorithm=""AdaBoost"", clf=""Cart"", epoch=10, **kwargs):\r\n    print(""="" * 30)\r\n    print(""Testing {} ({})..."".format(algorithm, clf))\r\n    print(""-"" * 30)\r\n    t = time.time()\r\n    ensemble = clf_dict[algorithm]()\r\n    ensemble.fit(x, y, None, clf, epoch, **kwargs)\r\n    ensemble.evaluate(xt, yt)\r\n    print(""Time cost: {:8.6} s"".format(time.time() - t))\r\n\r\nif __name__ == \'__main__\':\r\n    # _x, _y = gen_random()\r\n    # test(_x, _y, algorithm=""RF"", epoch=1)\r\n    # test(_x, _y, algorithm=""RF"", epoch=10)\r\n    # test(_x, _y, algorithm=""RF"", epoch=50)\r\n    # test(_x, _y, algorithm=""SKRandomForest"")\r\n    # test(_x, _y, epoch=1)\r\n    # test(_x, _y, epoch=1)\r\n    # test(_x, _y, epoch=10)\r\n    # _x, _y = gen_xor()\r\n    # test(_x, _y, algorithm=""RF"", epoch=1)\r\n    # test(_x, _y, algorithm=""RF"", epoch=10)\r\n    # test(_x, _y, algorithm=""RF"", epoch=1000)\r\n    # test(_x, _y, algorithm=""SKAdaBoost"")\r\n    _x, _y = DataUtil.gen_spiral(size=20, n=4, n_class=2, one_hot=False)\r\n    _y[_y == 0] = -1\r\n    # test(_x, _y, clf=""SKTree"", epoch=10)\r\n    # test(_x, _y, clf=""SKTree"", epoch=1000)\r\n    # test(_x, _y, algorithm=""RF"", epoch=10)\r\n    test(_x, _y, algorithm=""RF"", epoch=30, n_cores=4)\r\n    test(_x, _y, algorithm=""SKAdaBoost"")\r\n\r\n    train_num = 6000\r\n    (x_train, y_train), (x_test, y_test), *_ = DataUtil.get_dataset(\r\n        ""mushroom"", ""../_Data/mushroom.txt"", n_train=train_num, quantize=True, tar_idx=0)\r\n    y_train[y_train == 0] = -1\r\n    y_test[y_test == 0] = -1\r\n\r\n    cv_test(x_train, y_train, x_test, y_test, clf=""MNB"", epoch=1)\r\n    cv_test(x_train, y_train, x_test, y_test, clf=""MNB"", epoch=5)\r\n    cv_test(x_train, y_train, x_test, y_test, clf=""MNB"", epoch=10)\r\n    cv_test(x_train, y_train, x_test, y_test, clf=""MNB"", epoch=15)\r\n\r\n    AdaBoost().show_timing_log()\r\n'"
e_SVM/KP.py,0,"b'import os\r\nimport sys\r\nroot_path = os.path.abspath(""../"")\r\nif root_path not in sys.path:\r\n    sys.path.append(root_path)\r\n\r\nimport numpy as np\r\nimport matplotlib.pyplot as plt\r\n\r\nfrom Util.Util import DataUtil\r\nfrom Util.Timing import Timing\r\nfrom Util.Bases import KernelBase, GDKernelBase\r\n\r\n\r\nclass KP(KernelBase):\r\n    KernelPerceptronTiming = Timing()\r\n\r\n    def __init__(self, **kwargs):\r\n        super(KP, self).__init__(**kwargs)\r\n        self._fit_args, self._fit_args_names = [0.01], [""lr""]\r\n\r\n    @KernelPerceptronTiming.timeit(level=1, prefix=""[Core] "")\r\n    def _update_dw_cache(self, idx, lr, sample_weight):\r\n        self._dw_cache = lr * self._y[idx] * sample_weight[idx]\r\n        self._w[idx] += self._dw_cache\r\n\r\n    @KernelPerceptronTiming.timeit(level=1, prefix=""[Core] "")\r\n    def _update_db_cache(self, idx, lr, sample_weight):\r\n        self._db_cache = self._dw_cache\r\n        self._b += self._db_cache\r\n\r\n    @KernelPerceptronTiming.timeit(level=1, prefix=""[Core] "")\r\n    def _fit(self, sample_weight, lr):\r\n        err = (np.sign(self._prediction_cache) != self._y) * sample_weight\r\n        indices = np.random.permutation(len(self._y))\r\n        idx = indices[np.argmax(err[indices])]\r\n        if self._prediction_cache[idx] == self._y[idx]:\r\n            return True\r\n        self._update_dw_cache(idx, lr, sample_weight)\r\n        self._update_db_cache(idx, lr, sample_weight)\r\n        self._update_pred_cache(idx)\r\n\r\n\r\nclass GDKP(GDKernelBase):\r\n    GDKPTiming = Timing()\r\n\r\n    @GDKPTiming.timeit(level=1, prefix=""[Core] "")\r\n    def _get_grads(self, x_batch, y_batch, y_pred, sample_weight_batch, *args):\r\n        err = -y_batch * (x_batch.dot(self._alpha) + self._b) * sample_weight_batch\r\n        mask = err >= 0  # type: np.ndarray\r\n        if not np.any(mask):\r\n            self._model_grads = [None, None]\r\n        else:\r\n            delta = -y_batch[mask] * sample_weight_batch[mask]\r\n            self._model_grads = [\r\n                np.sum(delta[..., None] * x_batch[mask], axis=0),\r\n                np.sum(delta)\r\n            ]\r\n        return np.sum(err[mask])\r\n\r\n\r\nif __name__ == \'__main__\':\r\n    # xs, ys = DataUtil.gen_two_clusters(center=5, dis=1, scale=2, one_hot=False)\r\n    xs, ys = DataUtil.gen_spiral(20, 4, 2, 2, one_hot=False)\r\n    # xs, ys = DataUtil.gen_xor(one_hot=False)\r\n    ys[ys == 0] = -1\r\n\r\n    animation_params = {\r\n        ""show"": False, ""mp4"": False, ""period"": 50,\r\n        ""dense"": 400, ""draw_background"": True\r\n    }\r\n\r\n    kp = KP(animation_params=animation_params)\r\n    kp.fit(xs, ys, kernel=""poly"", p=12, epoch=200)\r\n    kp.evaluate(xs, ys)\r\n    kp.visualize2d(xs, ys, dense=400)\r\n\r\n    kp = GDKP(animation_params=animation_params)\r\n    kp.fit(xs, ys, kernel=""poly"", p=12, epoch=10000)\r\n    kp.evaluate(xs, ys)\r\n    kp.visualize2d(xs, ys, dense=400)\r\n\r\n    (x_train, y_train), (x_test, y_test), *_ = DataUtil.get_dataset(\r\n        ""mushroom"", ""../_Data/mushroom.txt"", n_train=100, quantize=True, tar_idx=0)\r\n    y_train[y_train == 0] = -1\r\n    y_test[y_test == 0] = -1\r\n\r\n    kp = KP()\r\n    logs = [log[0] for log in kp.fit(\r\n        x_train, y_train, metrics=[""acc""], x_test=x_test, y_test=y_test\r\n    )]\r\n    kp.evaluate(x_train, y_train)\r\n    kp.evaluate(x_test, y_test)\r\n\r\n    plt.figure()\r\n    plt.title(kp.title)\r\n    plt.plot(range(len(logs)), logs)\r\n    plt.show()\r\n\r\n    kp = GDKP()\r\n    logs = [log[0] for log in kp.fit(\r\n        x_train, y_train, metrics=[""acc""], x_test=x_test, y_test=y_test\r\n    )]\r\n    kp.evaluate(x_train, y_train)\r\n    kp.evaluate(x_test, y_test)\r\n\r\n    plt.figure()\r\n    plt.title(kp.title)\r\n    plt.plot(range(len(logs)), logs)\r\n    plt.show()\r\n\r\n    kp.show_timing_log()\r\n'"
e_SVM/LinearSVM.py,10,"b'import os\r\nimport sys\r\nroot_path = os.path.abspath(""../"")\r\nif root_path not in sys.path:\r\n    sys.path.append(root_path)\r\n\r\nimport numpy as np\r\nimport tensorflow as tf\r\n\r\nfrom NN.Basic.Optimizers import OptFactory\r\nfrom NN.TF.Optimizers import OptFactory as TFOptFac\r\n\r\nfrom Util.Timing import Timing\r\nfrom Util.ProgressBar import ProgressBar\r\nfrom Util.Bases import GDBase, TFClassifierBase, TorchAutoClassifierBase\r\n\r\ntry:\r\n    import torch\r\n    from torch.autograd import Variable\r\n    from NN.PyTorch.Optimizers import OptFactory as PyTorchOptFac\r\nexcept ImportError:\r\n    torch = Variable = PyTorchOptFac = None\r\n\r\n\r\nclass LinearSVM(GDBase):\r\n    LinearSVMTiming = Timing()\r\n\r\n    def __init__(self, **kwargs):\r\n        super(LinearSVM, self).__init__(**kwargs)\r\n        self._w = self._b = None\r\n\r\n        self._params[""c""] = kwargs.get(""c"", 1)\r\n        self._params[""lr""] = kwargs.get(""lr"", 0.001)\r\n        self._params[""batch_size""] = kwargs.get(""batch_size"", 128)\r\n        self._params[""epoch""] = kwargs.get(""epoch"", 10 ** 4)\r\n        self._params[""tol""] = kwargs.get(""tol"", 1e-3)\r\n        self._params[""optimizer""] = kwargs.get(""optimizer"", ""Adam"")\r\n\r\n    @LinearSVMTiming.timeit(level=1, prefix=""[Core] "")\r\n    def _get_grads(self, x_batch, y_batch, y_pred, sample_weight_batch, *args):\r\n        c = args[0]\r\n        err = (1 - y_pred * y_batch) * sample_weight_batch\r\n        mask = err > 0  # type: np.ndarray\r\n        if not np.any(mask):\r\n            self._model_grads = [None, None]\r\n        else:\r\n            delta = -c * y_batch[mask] * sample_weight_batch[mask]\r\n            self._model_grads = [\r\n                np.sum(delta[..., None] * x_batch[mask], axis=0),\r\n                np.sum(delta)\r\n            ]\r\n        return np.sum(err[mask]) + c * np.linalg.norm(self._w)\r\n\r\n    @LinearSVMTiming.timeit(level=1, prefix=""[API] "")\r\n    def fit(self, x, y, sample_weight=None, c=None, lr=None, optimizer=None,\r\n            batch_size=None, epoch=None, tol=None, animation_params=None):\r\n        if sample_weight is None:\r\n            sample_weight = self._params[""sample_weight""]\r\n        if c is None:\r\n            c = self._params[""c""]\r\n        if lr is None:\r\n            lr = self._params[""lr""]\r\n        if batch_size is None:\r\n            batch_size = self._params[""batch_size""]\r\n        if epoch is None:\r\n            epoch = self._params[""epoch""]\r\n        if tol is None:\r\n            tol = self._params[""tol""]\r\n        if optimizer is None:\r\n            optimizer = self._params[""optimizer""]\r\n        *animation_properties, animation_params = self._get_animation_params(animation_params)\r\n        x, y = np.atleast_2d(x), np.asarray(y, dtype=np.float32)\r\n        if sample_weight is None:\r\n            sample_weight = np.ones(len(y))\r\n        else:\r\n            sample_weight = np.asarray(sample_weight) * len(y)\r\n\r\n        self._w = np.zeros(x.shape[1], dtype=np.float32)\r\n        self._b = np.zeros(1, dtype=np.float32)\r\n        self._model_parameters = [self._w, self._b]\r\n        self._optimizer = OptFactory().get_optimizer_by_name(\r\n            optimizer, self._model_parameters, lr, epoch\r\n        )\r\n\r\n        bar = ProgressBar(max_value=epoch, name=""LinearSVM"")\r\n        ims = []\r\n        train_repeat = self._get_train_repeat(x, batch_size)\r\n        for i in range(epoch):\r\n            self._optimizer.update()\r\n            l = self._batch_training(\r\n                x, y, batch_size, train_repeat, sample_weight, c\r\n            )\r\n            if l < tol:\r\n                bar.terminate()\r\n                break\r\n            self._handle_animation(i, x, y, ims, animation_params, *animation_properties)\r\n            bar.update()\r\n        self._handle_mp4(ims, animation_properties)\r\n\r\n    @LinearSVMTiming.timeit(level=1, prefix=""[API] "")\r\n    def predict(self, x, get_raw_results=False, **kwargs):\r\n        rs = np.sum(self._w * x, axis=1) + self._b\r\n        if get_raw_results:\r\n            return rs\r\n        return np.sign(rs)\r\n\r\n\r\nclass TFLinearSVM(TFClassifierBase):\r\n    TFLinearSVMTiming = Timing()\r\n\r\n    def __init__(self, **kwargs):\r\n        super(TFLinearSVM, self).__init__(**kwargs)\r\n        self._w = self._b = None\r\n\r\n        self._params[""c""] = kwargs.get(""c"", 1)\r\n        self._params[""lr""] = kwargs.get(""lr"", 0.01)\r\n        self._params[""batch_size""] = kwargs.get(""batch_size"", 128)\r\n        self._params[""epoch""] = kwargs.get(""epoch"", 10 ** 4)\r\n        self._params[""tol""] = kwargs.get(""tol"", 1e-3)\r\n        self._params[""optimizer""] = kwargs.get(""optimizer"", ""Adam"")\r\n\r\n    @TFLinearSVMTiming.timeit(level=1, prefix=""[API] "")\r\n    def fit(self, x, y, c=None, lr=None, batch_size=None, epoch=None, tol=None,\r\n            optimizer=None, animation_params=None):\r\n        if c is None:\r\n            c = self._params[""c""]\r\n        if lr is None:\r\n            lr = self._params[""lr""]\r\n        if batch_size is None:\r\n            batch_size = self._params[""batch_size""]\r\n        if epoch is None:\r\n            epoch = self._params[""epoch""]\r\n        if tol is None:\r\n            tol = self._params[""tol""]\r\n        if optimizer is None:\r\n            optimizer = self._params[""optimizer""]\r\n        *animation_properties, animation_params = self._get_animation_params(animation_params)\r\n        x, y = np.atleast_2d(x), np.asarray(y)\r\n        y_2d = y[..., None]\r\n\r\n        self._w = tf.Variable(np.zeros([x.shape[1], 1]), dtype=tf.float32, name=""w"")\r\n        self._b = tf.Variable(0., dtype=tf.float32, name=""b"")\r\n        self._tfx = tf.placeholder(tf.float32, [None, x.shape[1]])\r\n        self._tfy = tf.placeholder(tf.float32, [None, 1])\r\n        self._y_pred_raw = tf.matmul(self._tfx, self._w) + self._b\r\n        self._y_pred = tf.sign(self._y_pred_raw)\r\n        loss = tf.reduce_sum(\r\n            tf.nn.relu(1 - self._tfy * self._y_pred_raw)\r\n        ) + c * tf.nn.l2_loss(self._w)\r\n        train_step = TFOptFac().get_optimizer_by_name(optimizer, lr).minimize(loss)\r\n        self._sess.run(tf.global_variables_initializer())\r\n        bar = ProgressBar(max_value=epoch, name=""TFLinearSVM"")\r\n        ims = []\r\n        train_repeat = self._get_train_repeat(x, batch_size)\r\n        for i in range(epoch):\r\n            l = self._batch_training(x, y_2d, batch_size, train_repeat, loss, train_step)\r\n            if l < tol:\r\n                bar.terminate()\r\n                break\r\n            self._handle_animation(i, x, y, ims, animation_params, *animation_properties)\r\n            bar.update()\r\n        self._handle_mp4(ims, animation_properties)\r\n\r\n    @TFLinearSVMTiming.timeit(level=1, prefix=""[API] "")\r\n    def predict(self, x, get_raw_results=False, **kwargs):\r\n        rs = self._y_pred_raw if get_raw_results else self._y_pred\r\n        return self._sess.run(rs, {self._tfx: x}).ravel()\r\n\r\n\r\nif TorchAutoClassifierBase is not None:\r\n    class TorchLinearSVM(TorchAutoClassifierBase):\r\n        TorchLinearSVMTiming = Timing()\r\n\r\n        def __init__(self, **kwargs):\r\n            super(TorchLinearSVM, self).__init__(**kwargs)\r\n            self._w = self._b = None\r\n\r\n            self._params[""c""] = kwargs.get(""c"", 1)\r\n            self._params[""lr""] = kwargs.get(""lr"", 0.001)\r\n            self._params[""batch_size""] = kwargs.get(""batch_size"", 128)\r\n            self._params[""epoch""] = kwargs.get(""epoch"", 10 ** 4)\r\n            self._params[""tol""] = kwargs.get(""tol"", 1e-3)\r\n            self._params[""optimizer""] = kwargs.get(""optimizer"", ""Adam"")\r\n\r\n        @TorchLinearSVMTiming.timeit(level=1, prefix=""[Core] "")\r\n        def _loss(self, y, y_pred, c):\r\n            return torch.sum(\r\n                torch.clamp(1 - y * y_pred, min=0)\r\n            ) + c * torch.sqrt(torch.sum(self._w * self._w))\r\n\r\n        @TorchLinearSVMTiming.timeit(level=1, prefix=""[API] "")\r\n        def fit(self, x, y, c=None, lr=None, batch_size=None, epoch=None, tol=None,\r\n                optimizer=None, animation_params=None):\r\n            if c is None:\r\n                c = self._params[""c""]\r\n            if lr is None:\r\n                lr = self._params[""lr""]\r\n            if batch_size is None:\r\n                batch_size = self._params[""batch_size""]\r\n            if epoch is None:\r\n                epoch = self._params[""epoch""]\r\n            if tol is None:\r\n                tol = self._params[""tol""]\r\n            if optimizer is None:\r\n                optimizer = self._params[""optimizer""]\r\n            *animation_properties, animation_params = self._get_animation_params(animation_params)\r\n            x, y = np.atleast_2d(x), np.asarray(y, dtype=np.float32)\r\n            y_2d = y[..., None]\r\n\r\n            self._w = Variable(torch.rand([x.shape[1], 1]), requires_grad=True)\r\n            self._b = Variable(torch.Tensor([0.]), requires_grad=True)\r\n            self._model_parameters = [self._w, self._b]\r\n            self._optimizer = PyTorchOptFac().get_optimizer_by_name(\r\n                optimizer, self._model_parameters, lr, epoch\r\n            )\r\n\r\n            x, y, y_2d = self._arr_to_variable(False, x, y, y_2d)\r\n            loss_function = lambda _y, _y_pred: self._loss(_y, _y_pred, c)\r\n\r\n            bar = ProgressBar(max_value=epoch, name=""TorchLinearSVM"")\r\n            ims = []\r\n            train_repeat = self._get_train_repeat(x, batch_size)\r\n            for i in range(epoch):\r\n                self._optimizer.update()\r\n                l = self.batch_training(\r\n                    x, y_2d, batch_size, train_repeat, loss_function\r\n                )\r\n                if l < tol:\r\n                    bar.terminate()\r\n                    break\r\n                self._handle_animation(i, x, y, ims, animation_params, *animation_properties)\r\n                bar.update()\r\n            self._handle_mp4(ims, animation_properties)\r\n\r\n        @TorchLinearSVMTiming.timeit(level=1, prefix=""[API] "")\r\n        def _predict(self, x, get_raw_results=False, **kwargs):\r\n            if not isinstance(x, Variable):\r\n                x = Variable(torch.from_numpy(np.asarray(x).astype(np.float32)))\r\n            rs = x.mm(self._w)\r\n            rs = rs.add_(self._b.expand_as(rs)).squeeze(1)\r\n            if get_raw_results:\r\n                return rs\r\n            return torch.sign(rs)\r\nelse:\r\n    TorchLinearSVM = None\r\n'"
e_SVM/Perceptron.py,0,"b'import os\r\nimport sys\r\nroot_path = os.path.abspath(""../"")\r\nif root_path not in sys.path:\r\n    sys.path.append(root_path)\r\n\r\nimport numpy as np\r\n\r\nfrom Util.Timing import Timing\r\nfrom Util.Bases import ClassifierBase\r\nfrom Util.ProgressBar import ProgressBar\r\n\r\n\r\nclass Perceptron(ClassifierBase):\r\n    PerceptronTiming = Timing()\r\n\r\n    def __init__(self, **kwargs):\r\n        super(Perceptron, self).__init__(**kwargs)\r\n        self._w = self._b = None\r\n\r\n        self._params[""lr""] = kwargs.get(""lr"", 0.01)\r\n        self._params[""epoch""] = kwargs.get(""epoch"", 10 ** 4)\r\n\r\n    @PerceptronTiming.timeit(level=1, prefix=""[API] "")\r\n    def fit(self, x, y, sample_weight=None, lr=None, epoch=None, animation_params=None):\r\n        if sample_weight is None:\r\n            sample_weight = self._params[""sample_weight""]\r\n        if lr is None:\r\n            lr = self._params[""lr""]\r\n        if epoch is None:\r\n            epoch = self._params[""epoch""]\r\n        *animation_properties, animation_params = self._get_animation_params(animation_params)\r\n\r\n        x, y = np.atleast_2d(x), np.asarray(y)\r\n        if sample_weight is None:\r\n            sample_weight = np.ones(len(y))\r\n        else:\r\n            sample_weight = np.asarray(sample_weight) * len(y)\r\n\r\n        self._w = np.zeros(x.shape[1])\r\n        self._b = 0.\r\n        ims = []\r\n        bar = ProgressBar(max_value=epoch, name=""Perceptron"")\r\n        for i in range(epoch):\r\n            err = -y * self.predict(x, True) * sample_weight\r\n            idx = np.argmax(err)\r\n            if err[idx] < 0:\r\n                bar.terminate()\r\n                break\r\n            delta = lr * y[idx] * sample_weight[idx]\r\n            self._w += delta * x[idx]\r\n            self._b += delta\r\n            self._handle_animation(i, x, y, ims, animation_params, *animation_properties)\r\n            bar.update()\r\n        self._handle_mp4(ims, animation_properties)\r\n\r\n    @PerceptronTiming.timeit(level=1, prefix=""[API] "")\r\n    def predict(self, x, get_raw_results=False, **kwargs):\r\n        rs = np.asarray(x, dtype=np.float32).dot(self._w) + self._b\r\n        if get_raw_results:\r\n            return rs\r\n        return np.sign(rs).astype(np.float32)\r\n\r\n\r\nclass Perceptron2(Perceptron):\r\n    def fit(self, x, y, sample_weight=None, lr=None, epoch=None, animation_params=None):\r\n        if sample_weight is None:\r\n            sample_weight = self._params[""sample_weight""]\r\n        if lr is None:\r\n            lr = self._params[""lr""]\r\n        if epoch is None:\r\n            epoch = self._params[""epoch""]\r\n        *animation_properties, animation_params = self._get_animation_params(animation_params)\r\n\r\n        x, y = np.atleast_2d(x), np.asarray(y)\r\n        if sample_weight is None:\r\n            sample_weight = np.ones(len(y))\r\n        else:\r\n            sample_weight = np.asarray(sample_weight) * len(y)\r\n\r\n        self._w = np.random.random(x.shape[1])\r\n        self._b = 0.\r\n        ims = []\r\n        bar = ProgressBar(max_value=epoch, name=""Perceptron"")\r\n        for i in range(epoch):\r\n            y_pred = self.predict(x, True)\r\n            err = -y * y_pred * sample_weight\r\n            idx = np.argmax(err)\r\n            if err[idx] < 0:\r\n                bar.terminate()\r\n                break\r\n            w_norm = np.linalg.norm(self._w)\r\n            delta = lr * y[idx] * sample_weight[idx] / w_norm\r\n            self._w += delta * (x[idx] - y_pred[idx] * self._w / w_norm ** 2)\r\n            self._b += delta\r\n            self._handle_animation(i, x, y, ims, animation_params, *animation_properties)\r\n            bar.update()\r\n        self._handle_mp4(ims, animation_properties)\r\n'"
e_SVM/SVM.py,11,"b'import os\nimport sys\nroot_path = os.path.abspath(""../"")\nif root_path not in sys.path:\n    sys.path.append(root_path)\n\nimport numpy as np\nimport tensorflow as tf\n\nfrom NN.TF.Optimizers import OptFactory as TFOptFac\n\nfrom Util.Timing import Timing\nfrom Util.Bases import KernelBase, GDKernelBase, TFKernelBase, TorchKernelBase\n\ntry:\n    import torch\n    from torch.autograd import Variable\n    from NN.PyTorch.Optimizers import OptFactory as PyTorchOptFac\nexcept ImportError:\n    torch = Variable = PyTorchOptFac = None\n\n\nclass SVM(KernelBase):\n    SVMTiming = Timing()\n\n    def __init__(self, **kwargs):\n        super(SVM, self).__init__(**kwargs)\n        self._fit_args, self._fit_args_names = [1e-3], [""tol""]\n        self._c = None\n\n    @SVMTiming.timeit(level=1, prefix=""[SMO] "")\n    def _pick_first(self, tol):\n        con1 = self._alpha > 0\n        con2 = self._alpha < self._c\n        err1 = self._y * self._prediction_cache - 1\n        err2 = err1.copy()\n        err3 = err1.copy()\n        err1[(con1 & (err1 <= 0)) | (~con1 & (err1 > 0))] = 0\n        err2[((~con1 | ~con2) & (err2 != 0)) | ((con1 & con2) & (err2 == 0))] = 0\n        err3[(con2 & (err3 >= 0)) | (~con2 & (err3 < 0))] = 0\n        err = err1 ** 2 + err2 ** 2 + err3 ** 2\n        idx = np.argmax(err)\n        if err[idx] < tol:\n            return\n        return idx\n\n    @SVMTiming.timeit(level=1, prefix=""[SMO] "")\n    def _pick_second(self, idx1):\n        idx = np.random.randint(len(self._y))\n        while idx == idx1:\n            idx = np.random.randint(len(self._y))\n        return idx\n\n    @SVMTiming.timeit(level=2, prefix=""[SMO] "")\n    def _get_lower_bound(self, idx1, idx2):\n        if self._y[idx1] != self._y[idx2]:\n            return max(0., self._alpha[idx2] - self._alpha[idx1])\n        return max(0., self._alpha[idx2] + self._alpha[idx1] - self._c)\n\n    @SVMTiming.timeit(level=2, prefix=""[SMO] "")\n    def _get_upper_bound(self, idx1, idx2):\n        if self._y[idx1] != self._y[idx2]:\n            return min(self._c, self._c + self._alpha[idx2] - self._alpha[idx1])\n        return min(self._c, self._alpha[idx2] + self._alpha[idx1])\n\n    @SVMTiming.timeit(level=1, prefix=""[SMO] "")\n    def _update_alpha(self, idx1, idx2):\n        l, h = self._get_lower_bound(idx1, idx2), self._get_upper_bound(idx1, idx2)\n        y1, y2 = self._y[idx1], self._y[idx2]\n        e1 = self._prediction_cache[idx1] - self._y[idx1]\n        e2 = self._prediction_cache[idx2] - self._y[idx2]\n        eta = self._gram[idx1][idx1] + self._gram[idx2][idx2] - 2 * self._gram[idx1][idx2]\n        a2_new = self._alpha[idx2] + (y2 * (e1 - e2)) / eta\n        if a2_new > h:\n            a2_new = h\n        elif a2_new < l:\n            a2_new = l\n        a1_old, a2_old = self._alpha[idx1], self._alpha[idx2]\n        da2 = a2_new - a2_old\n        da1 = -y1 * y2 * da2\n        self._alpha[idx1] += da1\n        self._alpha[idx2] = a2_new\n        self._update_dw_cache(idx1, idx2, da1, da2, y1, y2)\n        self._update_db_cache(idx1, idx2, da1, da2, y1, y2, e1, e2)\n        self._update_pred_cache(idx1, idx2)\n\n    @SVMTiming.timeit(level=1, prefix=""[Core] "")\n    def _update_dw_cache(self, idx1, idx2, da1, da2, y1, y2):\n        self._dw_cache = np.array([da1 * y1, da2 * y2])\n        self._w[idx1] += self._dw_cache[0]\n        self._w[idx2] += self._dw_cache[1]\n\n    @SVMTiming.timeit(level=1, prefix=""[Core] "")\n    def _update_db_cache(self, idx1, idx2, da1, da2, y1, y2, e1, e2):\n        gram_12 = self._gram[idx1][idx2]\n        b1 = -e1 - y1 * self._gram[idx1][idx1] * da1 - y2 * gram_12 * da2\n        b2 = -e2 - y1 * gram_12 * da1 - y2 * self._gram[idx2][idx2] * da2\n        self._db_cache = (b1 + b2) * 0.5\n        self._b += self._db_cache\n\n    @SVMTiming.timeit(level=4, prefix=""[Util] "")\n    def _prepare(self, sample_weight, **kwargs):\n        self._c = kwargs.get(""c"", self._params[""c""])\n\n    @SVMTiming.timeit(level=1, prefix=""[Core] "")\n    def _fit(self, sample_weight, tol):\n        idx1 = self._pick_first(tol)\n        if idx1 is None:\n            return True\n        idx2 = self._pick_second(idx1)\n        self._update_alpha(idx1, idx2)\n\n\nclass GDSVM(GDKernelBase):\n    GDSVMTiming = Timing()\n\n    @GDSVMTiming.timeit(level=1, prefix=""[Core] "")\n    def _get_grads(self, x_batch, y_batch, y_pred, sample_weight_batch, *args):\n        err = -y_batch * (x_batch.dot(self._alpha) + self._b)\n        mask = err >= 0\n        if np.max(err) < 0:\n            self._model_grads = [None, None]\n        else:\n            delta = -y_batch[mask] * sample_weight_batch[mask]\n            self._model_grads = [\n                np.sum(delta[..., None] * x_batch[mask], axis=0),\n                np.sum(delta)\n            ]\n        if len(y_pred) == len(self._alpha):\n            return np.sum(err[mask]) + 0.5 * (y_pred - self._b).dot(self._alpha)\n        return np.sum(err[mask]) + 0.5 * self._alpha.dot(self._gram).dot(self._alpha)\n\n\nclass TFSVM(TFKernelBase):\n    TFSVMTiming = Timing()\n\n    def __init__(self, **kwargs):\n        super(TFSVM, self).__init__(**kwargs)\n        self._fit_args, self._fit_args_names = [1e-3], [""tol""]\n        self._batch_size = kwargs.get(""batch_size"", 128)\n        self._optimizer = kwargs.get(""optimizer"", ""Adam"")\n        self._train_repeat = None\n\n    def _prepare(self, sample_weight, **kwargs):\n        lr = kwargs.get(""lr"", self._params[""lr""])\n        self._w = tf.Variable(np.zeros([len(self._x), 1]), dtype=tf.float32, name=""w"")\n        self._b = tf.Variable(.0, dtype=tf.float32, name=""b"")\n\n        self._tfx = tf.placeholder(tf.float32, [None, None])\n        self._tfy = tf.placeholder(tf.float32, [None])\n        self._y_pred_raw = tf.transpose(tf.matmul(self._tfx, self._w) + self._b)\n        self._y_pred = tf.sign(self._y_pred_raw)\n        self._loss = tf.reduce_sum(\n            tf.maximum(1 - self._tfy * self._y_pred_raw, 0) * sample_weight\n        ) + 0.5 * tf.matmul(\n            # self._w, tf.matmul(self._tfx, self._w, transpose_b=True)\n            (self._y_pred_raw - self._b), self._w\n        )[0][0]\n        self._train_step = TFOptFac().get_optimizer_by_name(\n            self._optimizer, lr\n        ).minimize(self._loss)\n        self._sess.run(tf.global_variables_initializer())\n\n    @TFSVMTiming.timeit(level=1, prefix=""[API] "")\n    def _fit(self, sample_weight, tol):\n        if self._train_repeat is None:\n            self._train_repeat = self._get_train_repeat(self._x, self._batch_size)\n        l = self._batch_training(\n            self._gram, self._y, self._batch_size, self._train_repeat,\n            self._loss, self._train_step\n        )\n        if l < tol:\n            return True\n\n    @TFSVMTiming.timeit(level=1, prefix=""[API] "")\n    def predict(self, x, get_raw_results=False, gram_provided=False):\n        rs = self._y_pred_raw if get_raw_results else self._y_pred\n        if gram_provided:\n            return self._sess.run(rs, {self._tfx: x}).ravel()\n        return self._sess.run(rs, {self._tfx: self._kernel(np.atleast_2d(x), self._x)}).ravel()\n\n\nif TorchKernelBase is not None:\n    class TorchSVM(TorchKernelBase):\n        TorchSVMTiming = Timing()\n\n        def __init__(self, **kwargs):\n            super(TorchSVM, self).__init__(**kwargs)\n            self._fit_args, self._fit_args_names = [1e-3], [""tol""]\n            self._batch_size = kwargs.get(""batch_size"", 128)\n            self._optimizer = kwargs.get(""optimizer"", ""Adam"")\n            self._train_repeat = None\n\n        @TorchSVMTiming.timeit(level=1, prefix=""[Core] "")\n        def _loss(self, y, y_pred, sample_weight):\n            return torch.sum(\n                torch.clamp(1 - y * y_pred, min=0) * sample_weight\n            ) + 0.5 * (y_pred - self._b.expand_as(y_pred)).unsqueeze(0).mm(self._w)\n\n        def _prepare(self, sample_weight, **kwargs):\n            lr = kwargs.get(""lr"", self._params[""lr""])\n            self._w = Variable(torch.zeros([len(self._x), 1]), requires_grad=True)\n            self._b = Variable(torch.Tensor([.0]), requires_grad=True)\n            self._model_parameters = [self._w, self._b]\n            self._optimizer = PyTorchOptFac().get_optimizer_by_name(\n                self._optimizer, self._model_parameters, lr, self._params[""epoch""]\n            )\n            sample_weight, = self._arr_to_variable(False, sample_weight)\n            self._loss_function = lambda y, y_pred: self._loss(y, y_pred, sample_weight)\n\n        @TorchSVMTiming.timeit(level=1, prefix=""[Core] "")\n        def _fit(self, sample_weight, tol):\n            if self._train_repeat is None:\n                self._train_repeat = self._get_train_repeat(self._x, self._batch_size)\n            l = self.batch_training(\n                self._gram, self._y, self._batch_size, self._train_repeat,\n                self._loss_function\n            )\n            if l < tol:\n                return True\n\n        @TorchSVMTiming.timeit(level=1, prefix=""[Core] "")\n        def _predict(self, x, get_raw_results=False, **kwargs):\n            if not isinstance(x, Variable):\n                x = Variable(torch.from_numpy(np.asarray(x).astype(np.float32)))\n            rs = x.mm(self._w)\n            rs = rs.add_(self._b.expand_as(rs)).squeeze(1)\n            if get_raw_results:\n                return rs\n            return torch.sign(rs)\n\n        @TorchSVMTiming.timeit(level=1, prefix=""[API] "")\n        def predict(self, x, get_raw_results=False, gram_provided=False):\n            if not gram_provided:\n                x = self._kernel(self._x.data.numpy(), np.atleast_2d(x))\n            y_pred = (self._w.data.numpy().ravel().dot(x) + self._b.data.numpy()).ravel()\n            if not get_raw_results:\n                return np.sign(y_pred)\n            return y_pred\nelse:\n    TorchSVM = None\n'"
e_SVM/TestLinear.py,0,"b'import os\r\nimport sys\r\nroot_path = os.path.abspath(""../"")\r\nif root_path not in sys.path:\r\n    sys.path.append(root_path)\r\n\r\nfrom e_SVM.Perceptron import Perceptron\r\nfrom e_SVM.LinearSVM import LinearSVM, TFLinearSVM, TorchLinearSVM\r\n\r\nfrom Util.Util import DataUtil\r\n\r\n\r\ndef main():\r\n\r\n    x, y = DataUtil.gen_two_clusters(n_dim=2, dis=2.5, center=5, one_hot=False)\r\n    y[y == 0] = -1\r\n\r\n    animation_params = {\r\n        ""show"": False, ""period"": 50, ""mp4"": False,\r\n        ""dense"": 400, ""draw_background"": True\r\n    }\r\n\r\n    svm = LinearSVM(animation_params=animation_params)\r\n    svm.fit(x, y)\r\n    svm.evaluate(x, y)\r\n    svm.visualize2d(x, y, padding=0.1, dense=400)\r\n\r\n    svm = TFLinearSVM(animation_params=animation_params)\r\n    svm.fit(x, y)\r\n    svm.evaluate(x, y)\r\n    svm.visualize2d(x, y, padding=0.1, dense=400)\r\n\r\n    if TorchLinearSVM is not None:\r\n        svm = TorchLinearSVM(animation_params=animation_params)\r\n        svm.fit(x, y)\r\n        svm.evaluate(x, y)\r\n        svm.visualize2d(x, y, padding=0.1, dense=400)\r\n\r\n    perceptron = Perceptron()\r\n    perceptron.fit(x, y)\r\n    perceptron.evaluate(x, y)\r\n    perceptron.visualize2d(x, y, padding=0.1, dense=400)\r\n\r\n    perceptron.show_timing_log()\r\n\r\n\r\nif __name__ == \'__main__\':\r\n    main()\r\n'"
e_SVM/TestSVM.py,0,"b'import os\r\nimport sys\r\nroot_path = os.path.abspath(""../"")\r\nif root_path not in sys.path:\r\n    sys.path.append(root_path)\r\n\r\nimport matplotlib.pyplot as plt\r\n\r\nfrom e_SVM.SVM import SVM, GDSVM, TFSVM, TorchSVM\r\nfrom _SKlearn.SVM import SKSVM\r\n\r\nfrom Util.Util import DataUtil\r\n\r\n\r\ndef main():\r\n\r\n    # x, y = DataUtil.gen_xor(100, one_hot=False)\r\n    x, y = DataUtil.gen_spiral(20, 4, 2, 2, one_hot=False)\r\n    # x, y = DataUtil.gen_two_clusters(n_dim=2, one_hot=False)\r\n    y[y == 0] = -1\r\n\r\n    animation_params = {\r\n        ""show"": False, ""mp4"": False, ""period"": 50,\r\n        ""dense"": 400, ""draw_background"": True\r\n    }\r\n\r\n    svm = SVM(animation_params=animation_params)\r\n    svm.fit(x, y, kernel=""poly"", p=12, epoch=600)\r\n    svm.evaluate(x, y)\r\n    svm.visualize2d(x, y, padding=0.1, dense=400, emphasize=svm[""alpha""] > 0)\r\n\r\n    svm = GDSVM(animation_params=animation_params)\r\n    svm.fit(x, y, kernel=""poly"", p=12, epoch=10000)\r\n    svm.evaluate(x, y)\r\n    svm.visualize2d(x, y, padding=0.1, dense=400, emphasize=svm[""alpha""] > 0)\r\n\r\n    if TorchSVM is not None:\r\n        svm = TorchSVM(animation_params=animation_params)\r\n        svm.fit(x, y, kernel=""poly"", p=12)\r\n        svm.evaluate(x, y)\r\n        svm.visualize2d(x, y, padding=0.1, dense=400, emphasize=svm[""alpha""] > 0)\r\n\r\n    svm = TFSVM()\r\n    svm.fit(x, y)\r\n    svm.evaluate(x, y)\r\n    svm.visualize2d(x, y, padding=0.1, dense=400)\r\n\r\n    svm = SKSVM()\r\n    # svm = SKSVM(kernel=""poly"", degree=12)\r\n    svm.fit(x, y)\r\n    svm.evaluate(x, y)\r\n    svm.visualize2d(x, y, padding=0.1, dense=400, emphasize=svm.support_)\r\n\r\n    (x_train, y_train), (x_test, y_test), *_ = DataUtil.get_dataset(\r\n        ""mushroom"", ""../_Data/mushroom.txt"", n_train=100, quantize=True, tar_idx=0)\r\n    y_train[y_train == 0] = -1\r\n    y_test[y_test == 0] = -1\r\n\r\n    svm = SKSVM()\r\n    svm.fit(x_train, y_train)\r\n    svm.evaluate(x_train, y_train)\r\n    svm.evaluate(x_test, y_test)\r\n\r\n    svm = TFSVM()\r\n    svm.fit(x_train, y_train)\r\n    svm.evaluate(x_train, y_train)\r\n    svm.evaluate(x_test, y_test)\r\n\r\n    if TorchSVM is not None:\r\n        svm = TorchSVM()\r\n        svm.fit(x_train, y_train)\r\n        svm.evaluate(x_train, y_train)\r\n        svm.evaluate(x_test, y_test)\r\n\r\n        svm = TorchSVM()\r\n        logs = [log[0] for log in svm.fit(\r\n            x_train, y_train, metrics=[""acc""], x_test=x_test, y_test=y_test\r\n        )]\r\n        svm.evaluate(x_train, y_train)\r\n        svm.evaluate(x_test, y_test)\r\n\r\n        plt.figure()\r\n        plt.title(svm.title)\r\n        plt.plot(range(len(logs)), logs)\r\n        plt.show()\r\n\r\n    svm = SVM()\r\n    logs = [log[0] for log in svm.fit(\r\n        x_train, y_train, metrics=[""acc""], x_test=x_test, y_test=y_test\r\n    )]\r\n    svm.evaluate(x_train, y_train)\r\n    svm.evaluate(x_test, y_test)\r\n\r\n    plt.figure()\r\n    plt.title(svm.title)\r\n    plt.plot(range(len(logs)), logs)\r\n    plt.show()\r\n\r\n    svm = GDSVM()\r\n    logs = [log[0] for log in svm.fit(\r\n        x_train, y_train, metrics=[""acc""], x_test=x_test, y_test=y_test\r\n    )]\r\n    svm.evaluate(x_train, y_train)\r\n    svm.evaluate(x_test, y_test)\r\n\r\n    plt.figure()\r\n    plt.title(svm.title)\r\n    plt.plot(range(len(logs)), logs)\r\n    plt.show()\r\n\r\n    svm.show_timing_log()\r\n\r\n\r\nif __name__ == \'__main__\':\r\n    main()\r\n'"
f_NN/Layers.py,0,"b'import os\r\nimport sys\r\nroot_path = os.path.abspath(""../"")\r\nif root_path not in sys.path:\r\n    sys.path.append(root_path)\r\n\r\nimport numpy as np\r\n\r\nfrom Util.Timing import Timing\r\n\r\n\r\n# Abstract Layer\r\n\r\nclass Layer:\r\n    LayerTiming = Timing()\r\n\r\n    def __init__(self, shape):\r\n        """"""\r\n        :param shape: shape[0] = units of previous layer\r\n                      shape[1] = units of current layer (self)\r\n        """"""\r\n        self.shape = shape\r\n\r\n    def __str__(self):\r\n        return self.__class__.__name__\r\n\r\n    def __repr__(self):\r\n        return str(self)\r\n\r\n    @property\r\n    def name(self):\r\n        return str(self)\r\n\r\n    # Core\r\n\r\n    def _activate(self, x, predict):\r\n        pass\r\n\r\n    def derivative(self, y):\r\n        pass\r\n\r\n    @LayerTiming.timeit(level=1, prefix=""[Core] "")\r\n    def activate(self, x, w, bias, predict=False):\r\n        return self._activate(x.dot(w) + bias, predict)\r\n\r\n    @LayerTiming.timeit(level=1, prefix=""[Core] "")\r\n    def bp(self, y, w, prev_delta):\r\n        return prev_delta.dot(w.T) * self.derivative(y)\r\n\r\n\r\n# Activation Layers\r\n\r\nclass Sigmoid(Layer):\r\n    def _activate(self, x, predict):\r\n        return 1 / (1 + np.exp(-x))\r\n\r\n    def derivative(self, y):\r\n        return y * (1 - y)\r\n\r\n\r\nclass Tanh(Layer):\r\n    def _activate(self, x, predict):\r\n        return np.tanh(x)\r\n\r\n    def derivative(self, y):\r\n        return 1 - y ** 2\r\n\r\n\r\nclass ReLU(Layer):\r\n    def _activate(self, x, predict):\r\n        return np.maximum(0, x)\r\n\r\n    def derivative(self, y):\r\n        return y != 0\r\n\r\n\r\nclass ELU(Layer):\r\n    def _activate(self, x, predict):\r\n        rs, rs0 = x.copy(), x < 0\r\n        rs[rs0] = np.exp(rs[rs0]) - 1\r\n        return rs\r\n\r\n    def derivative(self, y):\r\n        rs, indices = np.ones(y.shape), y < 0\r\n        rs[indices] = y[indices] + 1\r\n        return rs\r\n\r\n\r\nclass Softplus(Layer):\r\n    def _activate(self, x, predict):\r\n        return np.log(1 + np.exp(x))\r\n\r\n    def derivative(self, y):\r\n        return 1 - 1 / np.exp(y)\r\n\r\n\r\nclass Identical(Layer):\r\n    def _activate(self, x, predict):\r\n        return x\r\n\r\n    def derivative(self, y):\r\n        return 1\r\n\r\n\r\n# Cost Layer\r\n\r\nclass CostLayer(Layer):\r\n    # Optimization\r\n    batch_range = None\r\n\r\n    def __init__(self, shape, cost_function=""MSE"", transform=None):\r\n        super(CostLayer, self).__init__(shape)\r\n        self._available_cost_functions = {\r\n            ""MSE"": CostLayer._mse,\r\n            ""SVM"": CostLayer._svm,\r\n            ""CrossEntropy"": CostLayer._cross_entropy\r\n        }\r\n        self._available_transform_functions = {\r\n            ""Softmax"": CostLayer._softmax,\r\n            ""Sigmoid"": CostLayer._sigmoid\r\n        }\r\n        self._cost_function_name = cost_function\r\n        self._cost_function = self._available_cost_functions[cost_function]\r\n        if transform is None and cost_function == ""CrossEntropy"":\r\n            self._transform = ""Softmax""\r\n            self._transform_function = CostLayer._softmax\r\n        else:\r\n            self._transform = transform\r\n            self._transform_function = self._available_transform_functions.get(transform, None)\r\n\r\n    def __str__(self):\r\n        return self._cost_function_name\r\n\r\n    def _activate(self, x, predict):\r\n        if self._transform_function is None:\r\n            return x\r\n        return self._transform_function(x)\r\n\r\n    def _derivative(self, y, delta=None):\r\n        pass\r\n\r\n    def bp_first(self, y, y_pred):\r\n        if self._cost_function_name == ""CrossEntropy"" and (\r\n                self._transform == ""Softmax"" or self._transform == ""Sigmoid""):\r\n            return y - y_pred\r\n        dy = -self._cost_function(y, y_pred)\r\n        if self._transform_function is None:\r\n            return dy\r\n        return dy * self._transform_function(y_pred, diff=True)\r\n\r\n    @property\r\n    def calculate(self):\r\n        return lambda y, y_pred: self._cost_function(y, y_pred, False)\r\n\r\n    @property\r\n    def cost_function(self):\r\n        return self._cost_function_name\r\n\r\n    @cost_function.setter\r\n    def cost_function(self, value):\r\n        self._cost_function_name = value\r\n        self._cost_function = self._available_cost_functions[value]\r\n\r\n    def set_cost_function_derivative(self, func, name=None):\r\n        name = ""Custom Cost Function"" if name is None else name\r\n        self._cost_function_name = name\r\n        self._cost_function = func\r\n\r\n    # Transform Functions\r\n\r\n    @staticmethod\r\n    def safe_exp(y):\r\n        return np.exp(y - np.max(y, axis=1, keepdims=True))\r\n\r\n    @staticmethod\r\n    def _softmax(y, diff=False):\r\n        if diff:\r\n            return y * (1 - y)\r\n        exp_y = CostLayer.safe_exp(y)\r\n        return exp_y / np.sum(exp_y, axis=1, keepdims=True)\r\n\r\n    @staticmethod\r\n    def _sigmoid(y, diff=False):\r\n        if diff:\r\n            return y * (1 - y)\r\n        return 1 / (1 + np.exp(-y))\r\n\r\n    # Cost Functions\r\n\r\n    @staticmethod\r\n    def _mse(y, y_pred, diff=True):\r\n        if diff:\r\n            return -y + y_pred\r\n        assert_string = ""y or y_pred should be np.ndarray in cost function""\r\n        assert isinstance(y, np.ndarray) or isinstance(y_pred, np.ndarray), assert_string\r\n        return 0.5 * np.average((y - y_pred) ** 2)\r\n\r\n    @staticmethod\r\n    def _svm(y, y_pred, diff=True):\r\n        n, y = y_pred.shape[0], np.argmax(y, axis=1)\r\n        correct_class_scores = y_pred[np.arange(n), y]\r\n        margins = np.maximum(0, y_pred - correct_class_scores[:, None] + 1.0)\r\n        margins[np.arange(n), y] = 0\r\n        loss = np.sum(margins) / n\r\n        num_pos = np.sum(margins > 0, axis=1)\r\n        if not diff:\r\n            return loss\r\n        dx = np.zeros_like(y_pred)\r\n        dx[margins > 0] = 1\r\n        dx[np.arange(n), y] -= num_pos\r\n        dx /= n\r\n        return dx\r\n\r\n    @staticmethod\r\n    def _cross_entropy(y, y_pred, diff=True):\r\n        if diff:\r\n            return -y / y_pred + (1 - y) / (1 - y_pred)\r\n        # noinspection PyTypeChecker\r\n        return np.average(-y * np.log(np.maximum(y_pred, 1e-12)) - (1 - y) * np.log(np.maximum(1 - y_pred, 1e-12)))\r\n'"
f_NN/Networks.py,0,"b'import os\nimport sys\nroot_path = os.path.abspath(""../"")\nif root_path not in sys.path:\n    sys.path.append(root_path)\n\nimport matplotlib.pyplot as plt\n\nfrom f_NN.Layers import *\nfrom f_NN.Optimizers import *\n\nfrom Util.Bases import ClassifierBase\nfrom Util.ProgressBar import ProgressBar\n\n\nclass NNVerbose:\n    NONE = 0\n    EPOCH = 1\n    METRICS = 2\n    METRICS_DETAIL = 3\n    DETAIL = 4\n    DEBUG = 5\n\n\nclass NaiveNN(ClassifierBase):\n    NaiveNNTiming = Timing()\n\n    def __init__(self, **kwargs):\n        super(NaiveNN, self).__init__(**kwargs)\n        self._layers, self._weights, self._bias = [], [], []\n        self._w_optimizer = self._b_optimizer = None\n        self._current_dimension = 0\n\n        self._params[""lr""] = kwargs.get(""lr"", 0.001)\n        self._params[""epoch""] = kwargs.get(""epoch"", 10)\n        self._params[""optimizer""] = kwargs.get(""optimizer"", ""Adam"")\n\n    # Utils\n\n    @NaiveNNTiming.timeit(level=4)\n    def _add_params(self, shape):\n        self._weights.append(np.random.randn(*shape))\n        self._bias.append(np.zeros((1, shape[1])))\n\n    @NaiveNNTiming.timeit(level=4)\n    def _add_layer(self, layer, *args):\n        current, nxt = args\n        self._add_params((current, nxt))\n        self._current_dimension = nxt\n        self._layers.append(layer)\n\n    @NaiveNNTiming.timeit(level=1)\n    def _get_activations(self, x):\n        activations = [self._layers[0].activate(x, self._weights[0], self._bias[0])]\n        for i, layer in enumerate(self._layers[1:]):\n            activations.append(layer.activate(\n                activations[-1], self._weights[i + 1], self._bias[i + 1]))\n        return activations\n\n    @NaiveNNTiming.timeit(level=1)\n    def _get_prediction(self, x):\n        return self._get_activations(x)[-1]\n\n    # Optimizing Process\n\n    @NaiveNNTiming.timeit(level=4)\n    def _init_optimizers(self, optimizer, lr, epoch):\n        opt_fac = OptFactory()\n        self._w_optimizer = opt_fac.get_optimizer_by_name(\n            optimizer, self._weights, lr, epoch)\n        self._b_optimizer = opt_fac.get_optimizer_by_name(\n            optimizer, self._bias, lr, epoch)\n\n    @NaiveNNTiming.timeit(level=1)\n    def _opt(self, i, _activation, _delta):\n        self._weights[i] += self._w_optimizer.run(\n            i, _activation.T.dot(_delta)\n        )\n        self._bias[i] += self._b_optimizer.run(\n            i, np.sum(_delta, axis=0, keepdims=True)\n        )\n\n    # API\n\n    @NaiveNNTiming.timeit(level=4, prefix=""[API] "")\n    def add(self, layer):\n        if not self._layers:\n            self._layers, self._current_dimension = [layer], layer.shape[1]\n            self._add_params(layer.shape)\n        else:\n            nxt = layer.shape[0]\n            layer.shape = (self._current_dimension, nxt)\n            self._add_layer(layer, self._current_dimension, nxt)\n\n    @NaiveNNTiming.timeit(level=1, prefix=""[API] "")\n    def fit(self, x, y, lr=None, epoch=None, optimizer=None):\n        if lr is None:\n            lr = self._params[""lr""]\n        if epoch is None:\n            epoch = self._params[""epoch""]\n        if optimizer is None:\n            optimizer = self._params[""optimizer""]\n        self._init_optimizers(optimizer, lr, epoch)\n        layer_width = len(self._layers)\n        for counter in range(epoch):\n            self._w_optimizer.update()\n            self._b_optimizer.update()\n            activations = self._get_activations(x)\n            deltas = [self._layers[-1].bp_first(y, activations[-1])]\n            for i in range(-1, -len(activations), -1):\n                deltas.append(self._layers[i - 1].bp(\n                    activations[i - 1], self._weights[i], deltas[-1]\n                ))\n            for i in range(layer_width - 1, 0, -1):\n                self._opt(i, activations[i - 1], deltas[layer_width - i - 1])\n            self._opt(0, x, deltas[-1])\n\n    @NaiveNNTiming.timeit(level=4, prefix=""[API] "")\n    def predict(self, x, get_raw_results=False, **kwargs):\n        y_pred = self._get_prediction(np.atleast_2d(x))\n        if get_raw_results:\n            return y_pred\n        return np.argmax(y_pred, axis=1)\n\n\nclass NN(NaiveNN):\n    NNTiming = Timing()\n\n    def __init__(self, **kwargs):\n        super(NN, self).__init__(**kwargs)\n        self._available_metrics = {\n            key: value for key, value in zip([""acc"", ""f1-score""], [NN.acc, NN.f1_score])\n        }\n        self._metrics, self._metric_names, self._logs = [], [], {}\n        self.verbose = None\n\n        self._params[""batch_size""] = kwargs.get(""batch_size"", 256)\n        self._params[""train_rate""] = kwargs.get(""train_rate"", None)\n        self._params[""metrics""] = kwargs.get(""metrics"", None)\n        self._params[""record_period""] = kwargs.get(""record_period"", 100)\n        self._params[""verbose""] = kwargs.get(""verbose"", 1)\n\n    # Utils\n\n    @NNTiming.timeit(level=1)\n    def _get_prediction(self, x, name=None, batch_size=1e6, verbose=None):\n        if verbose is None:\n            verbose = self.verbose\n        single_batch = batch_size / np.prod(x.shape[1:])  # type: float\n        single_batch = int(single_batch)\n        if not single_batch:\n            single_batch = 1\n        if single_batch >= len(x):\n            return self._get_activations(x).pop()\n        epoch = int(len(x) / single_batch)\n        if not len(x) % single_batch:\n            epoch += 1\n        name = ""Prediction"" if name is None else ""Prediction ({})"".format(name)\n        sub_bar = ProgressBar(max_value=epoch, name=name, start=False)\n        if verbose >= NNVerbose.METRICS:\n            sub_bar.start()\n        rs, count = [self._get_activations(x[:single_batch]).pop()], single_batch\n        if verbose >= NNVerbose.METRICS:\n            sub_bar.update()\n        while count < len(x):\n            count += single_batch\n            if count >= len(x):\n                rs.append(self._get_activations(x[count - single_batch:]).pop())\n            else:\n                rs.append(self._get_activations(x[count - single_batch:count]).pop())\n            if verbose >= NNVerbose.METRICS:\n                sub_bar.update()\n        return np.vstack(rs)\n\n    @NNTiming.timeit(level=4, prefix=""[API] "")\n    def _preview(self):\n        if not self._layers:\n            rs = ""None""\n        else:\n            rs = (\n                ""Input  :  {:<10s} - {}\\n"".format(""Dimension"", self._layers[0].shape[0]) +\n                ""\\n"".join(\n                    [""Layer  :  {:<10s} - {}"".format(\n                        _layer.name, _layer.shape[1]\n                    ) for _layer in self._layers[:-1]]\n                ) + ""\\nCost   :  {:<10s}"".format(self._layers[-1].name)\n            )\n        print(""="" * 30 + ""\\n"" + ""Structure\\n"" + ""-"" * 30 + ""\\n"" + rs + ""\\n"" + ""="" * 30)\n        print(""Optimizer"")\n        print(""-"" * 30)\n        print(self._w_optimizer)\n        print(""="" * 30)\n\n    @NNTiming.timeit(level=2)\n    def _append_log(self, x, y, y_classes, name):\n        y_pred = self._get_prediction(x, name)\n        y_pred_classes = np.argmax(y_pred, axis=1)\n        for i, metric in enumerate(self._metrics):\n            self._logs[name][i].append(metric(y_classes, y_pred_classes))\n        self._logs[name][-1].append(self._layers[-1].calculate(y, y_pred) / len(y))\n\n    @NNTiming.timeit(level=3)\n    def _print_metric_logs(self, data_type):\n        print()\n        print(""="" * 47)\n        for i, name in enumerate(self._metric_names):\n            print(""{:<16s} {:<16s}: {:12.8}"".format(\n                data_type, name, self._logs[data_type][i][-1]))\n        print(""{:<16s} {:<16s}: {:12.8}"".format(\n            data_type, ""loss"", self._logs[data_type][-1][-1]))\n        print(""="" * 47)\n\n    @NNTiming.timeit(level=1, prefix=""[API] "")\n    def fit(self, x, y, lr=None, epoch=None, batch_size=None, train_rate=None,\n            optimizer=None, metrics=None, record_period=None, verbose=None):\n        if lr is None:\n            lr = self._params[""lr""]\n        if epoch is None:\n            epoch = self._params[""epoch""]\n        if optimizer is None:\n            optimizer = self._params[""optimizer""]\n        if batch_size is None:\n            batch_size = self._params[""batch_size""]\n        if train_rate is None:\n            train_rate = self._params[""train_rate""]\n        if metrics is None:\n            metrics = self._params[""metrics""]\n        if record_period is None:\n            record_period = self._params[""record_period""]\n        if verbose is None:\n            verbose = self._params[""verbose""]\n        self.verbose = verbose\n        self._init_optimizers(optimizer, lr, epoch)\n        layer_width = len(self._layers)\n        self._preview()\n\n        if train_rate is not None:\n            train_rate = float(train_rate)\n            train_len = int(len(x) * train_rate)\n            shuffle_suffix = np.random.permutation(len(x))\n            x, y = x[shuffle_suffix], y[shuffle_suffix]\n            x_train, y_train = x[:train_len], y[:train_len]\n            x_test, y_test = x[train_len:], y[train_len:]\n        else:\n            x_train = x_test = x\n            y_train = y_test = y\n        y_train_classes = np.argmax(y_train, axis=1)\n        y_test_classes = np.argmax(y_test, axis=1)\n\n        train_len = len(x_train)\n        batch_size = min(batch_size, train_len)\n        do_random_batch = train_len > batch_size\n        train_repeat = 1 if not do_random_batch else int(train_len / batch_size) + 1\n\n        if metrics is None:\n            metrics = []\n        self._metrics = self.get_metrics(metrics)\n        self._metric_names = [_m.__name__ for _m in metrics]\n        self._logs = {\n            name: [[] for _ in range(len(metrics) + 1)] for name in (""train"", ""test"")\n        }\n\n        bar = ProgressBar(max_value=max(1, epoch // record_period), name=""Epoch"", start=False)\n        if self.verbose >= NNVerbose.EPOCH:\n            bar.start()\n\n        sub_bar = ProgressBar(max_value=train_repeat * record_period - 1, name=""Iteration"", start=False)\n        for counter in range(epoch):\n            if self.verbose >= NNVerbose.EPOCH and counter % record_period == 0:\n                sub_bar.start()\n            for _ in range(train_repeat):\n                if do_random_batch:\n                    batch = np.random.choice(train_len, batch_size)\n                    x_batch, y_batch = x_train[batch], y_train[batch]\n                else:\n                    x_batch, y_batch = x_train, y_train\n                self._w_optimizer.update()\n                self._b_optimizer.update()\n                activations = self._get_activations(x_batch)\n                deltas = [self._layers[-1].bp_first(y_batch, activations[-1])]\n                for i in range(-1, -len(activations), -1):\n                    deltas.append(\n                        self._layers[i - 1].bp(activations[i - 1], self._weights[i], deltas[-1])\n                    )\n                for i in range(layer_width - 1, 0, -1):\n                    self._opt(i, activations[i - 1], deltas[layer_width - i - 1])\n                self._opt(0, x_batch, deltas[-1])\n                if self.verbose >= NNVerbose.EPOCH:\n                    if sub_bar.update() and self.verbose >= NNVerbose.METRICS_DETAIL:\n                        self._append_log(x_train, y_train, y_train_classes, ""train"")\n                        self._append_log(x_test, y_test, y_test_classes, ""test"")\n                        self._print_metric_logs(""train"")\n                        self._print_metric_logs(""test"")\n            if self.verbose >= NNVerbose.EPOCH:\n                sub_bar.update()\n            if (counter + 1) % record_period == 0:\n                self._append_log(x_train, y_train, y_train_classes, ""train"")\n                self._append_log(x_test, y_test, y_test_classes, ""test"")\n                if self.verbose >= NNVerbose.METRICS:\n                    self._print_metric_logs(""train"")\n                    self._print_metric_logs(""test"")\n                if self.verbose >= NNVerbose.EPOCH:\n                    bar.update(counter // record_period + 1)\n                    sub_bar = ProgressBar(max_value=train_repeat * record_period - 1, name=""Iteration"", start=False)\n\n    def draw_logs(self):\n        metrics_log, loss_log = {}, {}\n        for key, value in sorted(self._logs.items()):\n            metrics_log[key], loss_log[key] = value[:-1], value[-1]\n        for i, name in enumerate(sorted(self._metric_names)):\n            plt.figure()\n            plt.title(""Metric Type: {}"".format(name))\n            for key, log in sorted(metrics_log.items()):\n                xs = np.arange(len(log[i])) + 1\n                plt.plot(xs, log[i], label=""Data Type: {}"".format(key))\n            plt.legend(loc=4)\n            plt.show()\n            plt.close()\n        plt.figure()\n        plt.title(""Cost"")\n        for key, loss in sorted(loss_log.items()):\n            xs = np.arange(len(loss)) + 1\n            plt.plot(xs, loss, label=""Data Type: {}"".format(key))\n        plt.legend()\n        plt.show()\n'"
f_NN/Optimizers.py,0,"b'import os\r\nimport sys\r\nroot_path = os.path.abspath(""../"")\r\nif root_path not in sys.path:\r\n    sys.path.append(root_path)\r\n\r\nimport numpy as np\r\n\r\nfrom Util.Metas import TimingMeta\r\n\r\n\r\nclass Optimizer:\r\n    def __init__(self, lr=0.01, cache=None):\r\n        self.lr = lr\r\n        self._cache = cache\r\n\r\n    def __str__(self):\r\n        return self.__class__.__name__\r\n\r\n    def __repr__(self):\r\n        return str(self)\r\n\r\n    def feed_variables(self, variables):\r\n        self._cache = [\r\n            np.zeros(var.shape) for var in variables\r\n        ]\r\n\r\n    def run(self, i, dw):\r\n        pass\r\n\r\n    def update(self):\r\n        pass\r\n\r\n\r\nclass MBGD(Optimizer, metaclass=TimingMeta):\r\n    def run(self, i, dw):\r\n        return self.lr * dw\r\n\r\n\r\nclass Momentum(Optimizer, metaclass=TimingMeta):\r\n    def __init__(self, lr=0.01, cache=None, epoch=100, floor=0.5, ceiling=0.999):\r\n        Optimizer.__init__(self, lr, cache)\r\n        self._momentum = floor\r\n        self._step = (ceiling - floor) / epoch\r\n        self._floor, self._ceiling = floor, ceiling\r\n        self._is_nesterov = False\r\n\r\n    def run(self, i, dw):\r\n        dw *= self.lr\r\n        velocity = self._cache\r\n        velocity[i] *= self._momentum\r\n        velocity[i] += dw\r\n        if not self._is_nesterov:\r\n            return velocity[i]\r\n        return self._momentum * velocity[i] + dw\r\n\r\n    def update(self):\r\n        if self._momentum < self._ceiling:\r\n            self._momentum += self._step\r\n\r\n\r\nclass NAG(Momentum, metaclass=TimingMeta):\r\n    def __init__(self, lr=0.01, cache=None, epoch=100, floor=0.5, ceiling=0.999):\r\n        Momentum.__init__(self, lr, cache, epoch, floor, ceiling)\r\n        self._is_nesterov = True\r\n\r\n\r\nclass RMSProp(Optimizer, metaclass=TimingMeta):\r\n    def __init__(self, lr=0.01, cache=None, decay_rate=0.9, eps=1e-8):\r\n        Optimizer.__init__(self, lr, cache)\r\n        self.decay_rate, self.eps = decay_rate, eps\r\n\r\n    def run(self, i, dw):\r\n        self._cache[i] = self._cache[i] * self.decay_rate + (1 - self.decay_rate) * dw ** 2\r\n        return self.lr * dw / (np.sqrt(self._cache[i] + self.eps))\r\n\r\n\r\nclass Adam(Optimizer, metaclass=TimingMeta):\r\n    def __init__(self, lr=0.01, cache=None, beta1=0.9, beta2=0.999, eps=1e-8):\r\n        Optimizer.__init__(self, lr, cache)\r\n        self.beta1, self.beta2, self.eps = beta1, beta2, eps\r\n\r\n    def feed_variables(self, variables):\r\n        self._cache = [\r\n            [np.zeros(var.shape) for var in variables],\r\n            [np.zeros(var.shape) for var in variables],\r\n        ]\r\n\r\n    def run(self, i, dw):\r\n        self._cache[0][i] = self._cache[0][i] * self.beta1 + (1 - self.beta1) * dw\r\n        self._cache[1][i] = self._cache[1][i] * self.beta2 + (1 - self.beta2) * (dw ** 2)\r\n        return self.lr * self._cache[0][i] / (np.sqrt(self._cache[1][i] + self.eps))\r\n\r\n\r\n# Factory\r\n\r\nclass OptFactory:\r\n    available_optimizers = {\r\n        ""MBGD"": MBGD, ""Momentum"": Momentum, ""NAG"": NAG, ""RMSProp"": RMSProp, ""Adam"": Adam,\r\n    }\r\n\r\n    def get_optimizer_by_name(self, name, variables, lr, epoch=100):\r\n        try:\r\n            optimizer = self.available_optimizers[name](lr)\r\n            if variables is not None:\r\n                optimizer.feed_variables(variables)\r\n            if epoch is not None and isinstance(optimizer, Momentum):\r\n                optimizer.epoch = epoch\r\n            return optimizer\r\n        except KeyError:\r\n            raise NotImplementedError(""Undefined Optimizer \'{}\' found"".format(name))\r\n'"
f_NN/Test.py,0,"b'import os\r\nimport sys\r\nroot_path = os.path.abspath(""../"")\r\nif root_path not in sys.path:\r\n    sys.path.append(root_path)\r\n\r\nfrom f_NN.Networks import *\r\nfrom f_NN.Layers import *\r\n\r\nfrom Util.Util import DataUtil\r\n\r\n\r\ndef main():\r\n\r\n    nn = NN()\r\n    epoch = 1000\r\n\r\n    x, y = DataUtil.gen_spiral(120, 7, 7, 4)\r\n\r\n    nn.add(ReLU((x.shape[1], 24)))\r\n    nn.add(ReLU((24, )))\r\n    nn.add(CostLayer((y.shape[1],), ""CrossEntropy""))\r\n\r\n    # nn.disable_timing()\r\n    nn.fit(x, y, epoch=epoch, train_rate=0.8, metrics=[""acc""])\r\n    nn.evaluate(x, y)\r\n    nn.visualize2d(x, y)\r\n    nn.show_timing_log()\r\n    nn.draw_logs()\r\n\r\nif __name__ == \'__main__\':\r\n    main()\r\n'"
g_CNN/CIFAR10.py,0,"b'import os\r\nimport sys\r\nroot_path = os.path.abspath(""../"")\r\nif root_path not in sys.path:\r\n    sys.path.append(root_path)\r\n\r\nfrom g_CNN.Networks import *\r\n\r\nfrom Util.Util import DataUtil\r\n\r\n\r\ndef main():\r\n\r\n    nn = NN()\r\n    epoch = 50\r\n\r\n    x, y = DataUtil.get_dataset(""cifar10"", ""../_Data/cifar10.txt"", quantized=True, one_hot=True)\r\n\r\n    x = x.reshape(len(x), 3, 32, 32)\r\n    nn.add(""ConvReLU"", (x.shape[1:], (32, 3, 3)))\r\n    nn.add(""ConvReLU"", ((32, 3, 3),))\r\n    nn.add(""MaxPool"", ((3, 3),), 2)\r\n    nn.add(""ConvNorm"")\r\n    nn.add(""ConvDrop"")\r\n    nn.add(""ConvReLU"", ((64, 3, 3),))\r\n    nn.add(""ConvReLU"", ((64, 3, 3),))\r\n    nn.add(""AvgPool"", ((3, 3),), 2)\r\n    nn.add(""ConvNorm"")\r\n    nn.add(""ConvDrop"")\r\n    nn.add(""ConvReLU"", ((32, 3, 3),))\r\n    nn.add(""ConvReLU"", ((32, 3, 3),))\r\n    nn.add(""AvgPool"", ((3, 3),), 2)\r\n    nn.add(""ReLU"", (512,))\r\n    nn.add(""Identical"", (64,), apply_bias=False)\r\n    nn.add(""Normalize"", activation=""ReLU"")\r\n    nn.add(""Dropout"")\r\n    nn.add(""CrossEntropy"", (y.shape[1],))\r\n\r\n    # nn.disable_timing()\r\n    nn.fit(x, y, lr=0.001, epoch=epoch, train_rate=0.8,\r\n           metrics=[""acc""], record_period=1, verbose=4)\r\n    nn.evaluate(x, y)\r\n    nn.show_timing_log()\r\n    nn.draw_logs()\r\n\r\nif __name__ == \'__main__\':\r\n    main()\r\n'"
g_CNN/Layers.py,32,"b'import os\r\nimport sys\r\nroot_path = os.path.abspath(""../"")\r\nif root_path not in sys.path:\r\n    sys.path.append(root_path)\r\n\r\nimport numpy as np\r\nimport tensorflow as tf\r\nfrom math import ceil\r\n\r\nfrom Util.Timing import Timing\r\n\r\n\r\n# Abstract Layers\r\n\r\nclass Layer:\r\n    LayerTiming = Timing\r\n\r\n    def __init__(self, shape, **kwargs):\r\n        self.shape = shape\r\n        self.is_fc = self.is_sub_layer = False\r\n        self.apply_bias = kwargs.get(""apply_bias"", True)\r\n\r\n    def __str__(self):\r\n        return self.__class__.__name__\r\n\r\n    def __repr__(self):\r\n        return str(self)\r\n\r\n    @property\r\n    def name(self):\r\n        return str(self)\r\n\r\n    @property\r\n    def root(self):\r\n        return self\r\n\r\n    @LayerTiming.timeit(level=1, prefix=""[Core] "")\r\n    def activate(self, x, w, bias=None, predict=False):\r\n        if self.is_fc:\r\n            fc_shape = np.prod(x.get_shape()[1:])  # type: int\r\n            x = tf.reshape(x, [-1, int(fc_shape)])\r\n        if self.is_sub_layer:\r\n            return self._activate(x, predict)\r\n        if not self.apply_bias:\r\n            return self._activate(tf.matmul(x, w), predict)\r\n        return self._activate(tf.matmul(x, w) + bias, predict)\r\n\r\n    def _activate(self, x, predict):\r\n        pass\r\n\r\n\r\nclass SubLayer(Layer):\r\n    def __init__(self, parent, shape):\r\n        Layer.__init__(self, shape)\r\n        self.parent = parent\r\n        self.description = """"\r\n        self.is_sub_layer = True\r\n\r\n    @property\r\n    def root(self):\r\n        root = self.parent\r\n        while root.parent:\r\n            root = root.parent\r\n        return root\r\n\r\n    @property\r\n    def info(self):\r\n        return ""Layer  :  {:<16s} - {} {}"".format(self.name, self.shape[1], self.description)\r\n\r\n\r\nclass ConvLayer(Layer):\r\n    LayerTiming = Timing()\r\n\r\n    def __init__(self, shape, stride=1, padding=None, parent=None):\r\n        """"""\r\n        :param shape:    shape[0] = shape of previous layer           c x h x w\r\n                         shape[1] = shape of current layer\'s weight   f x h x w\r\n        :param stride:   stride\r\n        :param padding:  zero-padding\r\n        :param parent:   parent\r\n        """"""\r\n        if parent is not None:\r\n            _parent = parent.root if parent.is_sub_layer else parent\r\n            shape = _parent.shape\r\n        Layer.__init__(self, shape)\r\n        self.stride = stride\r\n        if padding is None:\r\n            padding = ""SAME""\r\n        if isinstance(padding, str):\r\n            if padding.upper() == ""VALID"":\r\n                self.padding = 0\r\n                self.pad_flag = ""VALID""\r\n            else:\r\n                self.padding = self.pad_flag = ""SAME""\r\n        elif isinstance(padding, int):\r\n            self.padding = padding\r\n            self.pad_flag = ""VALID""\r\n        else:\r\n            raise ValueError(""Padding should be \'SAME\' or \'VALID\' or integer"")\r\n        self.parent = parent\r\n        if len(shape) == 1:\r\n            self.n_channels = self.n_filters = self.out_h = self.out_w = None\r\n        else:\r\n            self.feed_shape(shape)\r\n\r\n    def feed_shape(self, shape):\r\n        self.shape = shape\r\n        self.n_channels, height, width = shape[0]\r\n        self.n_filters, filter_height, filter_width = shape[1]\r\n        if self.pad_flag == ""VALID"":\r\n            self.out_h = ceil((height - filter_height + 1) / self.stride)\r\n            self.out_w = ceil((width - filter_width + 1) / self.stride)\r\n        else:\r\n            self.out_h = ceil(height / self.stride)\r\n            self.out_w = ceil(width / self.stride)\r\n\r\n\r\nclass ConvPoolLayer(ConvLayer):\r\n    LayerTiming = Timing()\r\n\r\n    def feed_shape(self, shape):\r\n        shape = (shape[0], (shape[0][0], *shape[1]))\r\n        ConvLayer.feed_shape(self, shape)\r\n\r\n    @LayerTiming.timeit(level=1, prefix=""[Core] "")\r\n    def activate(self, x, w, bias=None, predict=False):\r\n        pool_height, pool_width = self.shape[1][1:]\r\n        if self.pad_flag == ""VALID"" and self.padding > 0:\r\n            _pad = [self.padding] * 2\r\n            x = tf.pad(x, [[0, 0], _pad, _pad, [0, 0]], ""CONSTANT"")\r\n        return self._activate(None)(\r\n            x, ksize=[1, pool_height, pool_width, 1],\r\n            strides=[1, self.stride, self.stride, 1], padding=self.pad_flag)\r\n\r\n    def _activate(self, x, *args):\r\n        raise NotImplementedError(""Please implement activation function for {}"".format(str(self)))\r\n\r\n\r\nclass ConvLayerMeta(type):\r\n    def __new__(mcs, *args, **kwargs):\r\n        name, bases, attr = args[:3]\r\n        conv_layer, layer = bases\r\n\r\n        def __init__(self, shape, stride=1, padding=""SAME""):\r\n            conv_layer.__init__(self, shape, stride, padding)\r\n\r\n        def _conv(self, x, w):\r\n            return tf.nn.conv2d(x, w, strides=[1, self.stride, self.stride, 1], padding=self.pad_flag)\r\n\r\n        def _activate(self, x, w, bias, predict):\r\n            res = self._conv(x, w) + bias\r\n            return layer._activate(self, res, predict)\r\n\r\n        def activate(self, x, w, bias=None, predict=False):\r\n            if self.pad_flag == ""VALID"" and self.padding > 0:\r\n                _pad = [self.padding] * 2\r\n                x = tf.pad(x, [[0, 0], _pad, _pad, [0, 0]], ""CONSTANT"")\r\n            return _activate(self, x, w, bias, predict)\r\n\r\n        for key, value in locals().items():\r\n            if str(value).find(""function"") >= 0:\r\n                attr[key] = value\r\n\r\n        return type(name, bases, attr)\r\n\r\n\r\nclass ConvSubLayerMeta(type):\r\n    def __new__(mcs, *args, **kwargs):\r\n        name, bases, attr = args[:3]\r\n        conv_layer, sub_layer = bases\r\n\r\n        def __init__(self, parent, shape, *_args, **_kwargs):\r\n            conv_layer.__init__(self, None, parent=parent)\r\n            self.out_h, self.out_w = parent.out_h, parent.out_w\r\n            sub_layer.__init__(self, parent, shape, *_args, **_kwargs)\r\n            self.shape = ((shape[0][0], self.out_h, self.out_w), shape[0])\r\n            if name == ""ConvNorm"":\r\n                self.tf_gamma = tf.Variable(tf.ones(self.n_filters), name=""norm_scale"")\r\n                self.tf_beta = tf.Variable(tf.zeros(self.n_filters), name=""norm_beta"")\r\n\r\n        def _activate(self, x, predict):\r\n            return sub_layer._activate(self, x, predict)\r\n\r\n        # noinspection PyUnusedLocal\r\n        def activate(self, x, w, bias=None, predict=False):\r\n            return self.LayerTiming.timeit(level=1, func_name=""activate"", cls_name=name, prefix=""[Core] "")(\r\n                _activate)(self, x, predict)\r\n\r\n        for key, value in locals().items():\r\n            if str(value).find(""function"") >= 0 or str(value).find(""property""):\r\n                attr[key] = value\r\n\r\n        return type(name, bases, attr)\r\n\r\n\r\n# Activation Layers\r\n\r\nclass Tanh(Layer):\r\n    def _activate(self, x, predict):\r\n        return tf.tanh(x)\r\n\r\n\r\nclass Sigmoid(Layer):\r\n    def _activate(self, x, predict):\r\n        return tf.nn.sigmoid(x)\r\n\r\n\r\nclass ELU(Layer):\r\n    def _activate(self, x, predict):\r\n        return tf.nn.elu(x)\r\n\r\n\r\nclass ReLU(Layer):\r\n    def _activate(self, x, predict):\r\n        return tf.nn.relu(x)\r\n\r\n\r\nclass Softplus(Layer):\r\n    def _activate(self, x, predict):\r\n        return tf.nn.softplus(x)\r\n\r\n\r\nclass Identical(Layer):\r\n    def _activate(self, x, predict):\r\n        return x\r\n\r\n\r\n# Convolution Layers\r\n\r\nclass ConvTanh(ConvLayer, Tanh, metaclass=ConvLayerMeta):\r\n    pass\r\n\r\n\r\nclass ConvSigmoid(ConvLayer, Sigmoid, metaclass=ConvLayerMeta):\r\n    pass\r\n\r\n\r\nclass ConvELU(ConvLayer, ELU, metaclass=ConvLayerMeta):\r\n    pass\r\n\r\n\r\nclass ConvReLU(ConvLayer, ReLU, metaclass=ConvLayerMeta):\r\n    pass\r\n\r\n\r\nclass ConvSoftplus(ConvLayer, Softplus, metaclass=ConvLayerMeta):\r\n    pass\r\n\r\n\r\nclass ConvIdentical(ConvLayer, Identical, metaclass=ConvLayerMeta):\r\n    pass\r\n\r\n\r\n# Pooling Layers\r\n\r\nclass MaxPool(ConvPoolLayer):\r\n    def _activate(self, x, *args):\r\n        return tf.nn.max_pool\r\n\r\n\r\nclass AvgPool(ConvPoolLayer):\r\n    def _activate(self, x, *args):\r\n        return tf.nn.avg_pool\r\n\r\n\r\n# Special Layers\r\n\r\nclass Dropout(SubLayer):\r\n    def __init__(self, parent, shape, drop_prob=0.5):\r\n        if drop_prob < 0 or drop_prob >= 1:\r\n            raise ValueError(""(Dropout) Probability of Dropout should be a positive float smaller than 1"")\r\n        SubLayer.__init__(self, parent, shape)\r\n        self._prob = tf.constant(1 - drop_prob, dtype=tf.float32)\r\n        self.description = ""(Drop prob: {})"".format(drop_prob)\r\n\r\n    def _activate(self, x, predict):\r\n        if not predict:\r\n            return tf.nn.dropout(x, self._prob)\r\n        return x\r\n\r\n\r\nclass Normalize(SubLayer):\r\n    def __init__(self, parent, shape, activation=""ReLU"", eps=1e-8, momentum=0.9):\r\n        SubLayer.__init__(self, parent, shape)\r\n        self._eps, self._activation = eps, activation\r\n        self.tf_rm = self.tf_rv = None\r\n        self.tf_gamma = tf.Variable(tf.ones(self.shape[1]), name=""norm_scale"")\r\n        self.tf_beta = tf.Variable(tf.zeros(self.shape[1]), name=""norm_beta"")\r\n        self._momentum = momentum\r\n        self.description = ""(eps: {}, momentum: {})"".format(eps, momentum)\r\n\r\n    # noinspection PyTypeChecker\r\n    def _activate(self, x, predict):\r\n        if self.tf_rm is None or self.tf_rv is None:\r\n            shape = x.get_shape()[-1]\r\n            self.tf_rm = tf.Variable(tf.zeros(shape), trainable=False, name=""norm_mean"")\r\n            self.tf_rv = tf.Variable(tf.ones(shape), trainable=False, name=""norm_var"")\r\n        if not predict:\r\n            sm, sv = tf.nn.moments(x, list(range(len(x.get_shape()) - 1)))\r\n            rm = tf.assign(self.tf_rm, self._momentum * self.tf_rm + (1 - self._momentum) * sm)\r\n            rv = tf.assign(self.tf_rv, self._momentum * self.tf_rv + (1 - self._momentum) * sv)\r\n            with tf.control_dependencies([rm, rv]):\r\n                norm = tf.nn.batch_normalization(x, sm, sv, self.tf_beta, self.tf_gamma, self._eps)\r\n        else:\r\n            norm = tf.nn.batch_normalization(x, self.tf_rm, self.tf_rv, self.tf_beta, self.tf_gamma, self._eps)\r\n        if self._activation == ""ReLU"":\r\n            return tf.nn.relu(norm)\r\n        if self._activation == ""Sigmoid"":\r\n            return tf.nn.sigmoid(norm)\r\n        return norm\r\n\r\n\r\nclass ConvDrop(ConvLayer, Dropout, metaclass=ConvSubLayerMeta):\r\n    pass\r\n\r\n\r\nclass ConvNorm(ConvLayer, Normalize, metaclass=ConvSubLayerMeta):\r\n    pass\r\n\r\n\r\n# Cost Layers\r\n\r\nclass CostLayer(Layer):\r\n    def calculate(self, y, y_pred):\r\n        return self._activate(y_pred, y)\r\n\r\n\r\nclass CrossEntropy(CostLayer):\r\n    def _activate(self, x, y):\r\n        return tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=x, labels=y))\r\n\r\n\r\nclass MSE(CostLayer):\r\n    def _activate(self, x, y):\r\n        return tf.reduce_mean(tf.square(x - y))\r\n\r\n\r\n# Factory\r\n\r\nclass LayerFactory:\r\n    available_root_layers = {\r\n        # Normal Layers\r\n        ""Tanh"": Tanh, ""Sigmoid"": Sigmoid,\r\n        ""ELU"": ELU, ""ReLU"": ReLU, ""Softplus"": Softplus,\r\n        ""Identical"": Identical,\r\n\r\n        # Cost Layers\r\n        ""CrossEntropy"": CrossEntropy, ""MSE"": MSE,\r\n\r\n        # Conv Layers\r\n        ""ConvTanh"": ConvTanh, ""ConvSigmoid"": ConvSigmoid,\r\n        ""ConvELU"": ConvELU, ""ConvReLU"": ConvReLU, ""ConvSoftplus"": ConvSoftplus,\r\n        ""ConvIdentical"": ConvIdentical,\r\n        ""MaxPool"": MaxPool, ""AvgPool"": AvgPool\r\n    }\r\n    available_special_layers = {\r\n        ""Dropout"": Dropout,\r\n        ""Normalize"": Normalize,\r\n        ""ConvDrop"": ConvDrop,\r\n        ""ConvNorm"": ConvNorm\r\n    }\r\n    special_layer_default_params = {\r\n        ""Dropout"": (0.5,),\r\n        ""Normalize"": (""Identical"", 1e-8, 0.9),\r\n        ""ConvDrop"": (0.5,),\r\n        ""ConvNorm"": (""Identical"", 1e-8, 0.9)\r\n    }\r\n\r\n    def get_root_layer_by_name(self, name, *args, **kwargs):\r\n        if name not in self.available_special_layers:\r\n            if name in self.available_root_layers:\r\n                layer = self.available_root_layers[name]\r\n            else:\r\n                raise ValueError(""Undefined layer \'{}\' found"".format(name))\r\n            return layer(*args, **kwargs)\r\n        return None\r\n\r\n    def get_layer_by_name(self, name, parent, current_dimension, *args, **kwargs):\r\n        layer = self.get_root_layer_by_name(name, *args, **kwargs)\r\n        if layer:\r\n            return layer, None\r\n        _current, _next = parent.shape[1], current_dimension\r\n        layer_param = self.special_layer_default_params[name]\r\n        layer = self.available_special_layers[name]\r\n        if args or kwargs:\r\n            layer = layer(parent, (_current, _next), *args, **kwargs)\r\n        else:\r\n            layer = layer(parent, (_current, _next), *layer_param)\r\n        return layer, (_current, _next)\r\n\r\nif __name__ == \'__main__\':\r\n    with tf.Session().as_default() as sess:\r\n        # NN Process\r\n        nn_x = np.array([\r\n            [ 0,  1,  2,  1,  0],\r\n            [-1, -2,  0,  2,  1],\r\n            [ 0,  1, -2, -1,  2],\r\n            [ 1,  2, -1,  0, -2]\r\n        ], dtype=np.float32)\r\n        nn_w = np.array([\r\n            [-2, -1, 0,  1,  2],\r\n            [ 2,  1, 0, -1, -2]\r\n        ], dtype=np.float32).T\r\n        nn_b = 1.\r\n        nn_id = Identical([nn_x.shape[1], 2])\r\n        print(nn_id.activate(nn_x, nn_w, nn_b).eval())\r\n        # CNN Process\r\n        conv_x = np.array([\r\n            [\r\n                [ 0, 2,  1, 2],\r\n                [-1, 0,  0, 1],\r\n                [ 1, 1,  0, 1],\r\n                [-2, 1, -1, 0]\r\n            ]\r\n        ], dtype=np.float32).reshape(1, 4, 4, 1)\r\n        conv_w = np.array([\r\n            [[ 1, 0,  1],\r\n             [-1, 0,  1],\r\n             [ 1, 0, -1]],\r\n            [[0,  1,  0],\r\n             [1,  0, -1],\r\n             [0, -1,  1]]\r\n        ], dtype=np.float32).transpose([1, 2, 0])[..., None, :]\r\n        conv_b = np.array([1, -1], dtype=np.float32)\r\n        # Using ""VALID"" Padding -> out_h = out_w = 2\r\n        conv_id = ConvIdentical([(conv_x.shape[1:], [2, 3, 3])], padding=""VALID"")\r\n        print(conv_id.activate(conv_x, conv_w, conv_b).eval())\r\n        conv_x = np.array([\r\n            [\r\n                [ 1,  2,  1],\r\n                [-1,  0, -2],\r\n                [ 1, -1,  2]\r\n            ]\r\n        ], dtype=np.float32).reshape(1, 3, 3, 1)\r\n        """"""\r\n        Using ""SAME"" Padding -> out_h = out_w = 3 & input_x = \r\n            [ [ 0  0  0  0  0 ]\r\n              [ 0  1  2  1  0 ]\r\n              [ 0 -1  0 -2  0 ]\r\n              [ 0  1 -1  2  0 ]\r\n              [ 0  0  0  0  0 ] ]\r\n        """"""\r\n        conv_id = ConvIdentical([(conv_x.shape[1:], [2, 3, 3])], padding=1, stride=2)\r\n        print(conv_id.activate(conv_x, conv_w, conv_b).eval())\r\n'"
g_CNN/Mnist.py,0,"b'import os\r\nimport sys\r\nroot_path = os.path.abspath(""../"")\r\nif root_path not in sys.path:\r\n    sys.path.append(root_path)\r\n\r\nfrom g_CNN.Networks import *\r\n\r\nfrom Util.Util import DataUtil\r\n\r\n\r\ndef main():\r\n\r\n    nn = NN()\r\n    epoch = 10\r\n\r\n    x, y = DataUtil.get_dataset(""mnist"", ""../_Data/mnist.txt"", quantized=True, one_hot=True)\r\n\r\n    # nn.add(""ReLU"", (x.shape[1], 24))\r\n    # nn.add(""ReLU"", (24, ))\r\n    # nn.add(""CrossEntropy"", (y.shape[1], ))\r\n\r\n    x = x.reshape(len(x), 1, 28, 28)\r\n    nn.add(""ConvReLU"", (x.shape[1:], (32, 3, 3)))\r\n    nn.add(""ConvReLU"", ((32, 3, 3),))\r\n    nn.add(""MaxPool"", ((3, 3),), 2)\r\n    nn.add(""ConvNorm"")\r\n    nn.add(""ConvDrop"")\r\n    nn.add(""ConvReLU"", ((64, 3, 3),))\r\n    nn.add(""ConvReLU"", ((64, 3, 3),))\r\n    nn.add(""AvgPool"", ((3, 3),), 2)\r\n    nn.add(""ConvNorm"")\r\n    nn.add(""ConvDrop"")\r\n    nn.add(""ConvReLU"", ((64, 3, 3),))\r\n    nn.add(""ConvReLU"", ((64, 3, 3),))\r\n    nn.add(""AvgPool"", ((3, 3),), 2)\r\n    nn.add(""ReLU"", (512,))\r\n    nn.add(""Identical"", (64,))\r\n    nn.add(""Normalize"", activation=""ReLU"")\r\n    nn.add(""Dropout"")\r\n    nn.add(""CrossEntropy"", (y.shape[1],))\r\n\r\n    # nn.disable_timing()\r\n    nn.fit(x, y, lr=0.001, epoch=epoch, train_rate=0.8,\r\n           metrics=[""acc""], record_period=1, verbose=2)\r\n    nn.evaluate(x, y)\r\n    nn.show_timing_log()\r\n    nn.draw_logs()\r\n\r\nif __name__ == \'__main__\':\r\n    main()\r\n'"
g_CNN/Networks.py,11,"b'import os\r\nimport sys\r\nroot_path = os.path.abspath(""../"")\r\nif root_path not in sys.path:\r\n    sys.path.append(root_path)\r\n\r\nimport matplotlib.pyplot as plt\r\n\r\nfrom g_CNN.Layers import *\r\nfrom g_CNN.Optimizers import *\r\n\r\nfrom Util.Timing import Timing\r\nfrom Util.Bases import TFClassifierBase\r\nfrom Util.ProgressBar import ProgressBar\r\n\r\n\r\nclass NNVerbose:\r\n    NONE = 0\r\n    EPOCH = 1\r\n    ITER = 1.5\r\n    METRICS = 2\r\n    METRICS_DETAIL = 3\r\n    DETAIL = 4\r\n    DEBUG = 5\r\n\r\n\r\nclass NN(TFClassifierBase):\r\n    NNTiming = Timing()\r\n\r\n    def __init__(self, **kwargs):\r\n        super(NN, self).__init__(**kwargs)\r\n        self._layers = []\r\n        self._optimizer = None\r\n        self._current_dimension = 0\r\n        self._available_metrics = {\r\n            key: value for key, value in zip([""acc"", ""f1-score""], [NN.acc, NN.f1_score])\r\n        }\r\n        self._metrics, self._metric_names, self._logs = [], [], {}\r\n\r\n        self.verbose = 0\r\n        self._layer_factory = LayerFactory()\r\n        self._tf_weights, self._tf_bias = [], []\r\n        self._loss = self._train_step = self._inner_y = None\r\n\r\n        self._params[""lr""] = kwargs.get(""lr"", 0.001)\r\n        self._params[""epoch""] = kwargs.get(""epoch"", 10)\r\n        self._params[""optimizer""] = kwargs.get(""optimizer"", ""Adam"")\r\n        self._params[""batch_size""] = kwargs.get(""batch_size"", 256)\r\n        self._params[""train_rate""] = kwargs.get(""train_rate"", None)\r\n        self._params[""metrics""] = kwargs.get(""metrics"", None)\r\n        self._params[""record_period""] = kwargs.get(""record_period"", 100)\r\n        self._params[""verbose""] = kwargs.get(""verbose"", 1)\r\n        self._params[""preview""] = kwargs.get(""preview"", True)\r\n\r\n    @NNTiming.timeit(level=1)\r\n    def _get_prediction(self, x, name=None, batch_size=1e6, verbose=None):\r\n        if verbose is None:\r\n            verbose = self.verbose\r\n        single_batch = batch_size / np.prod(x.shape[1:])  # type: float\r\n        single_batch = int(single_batch)\r\n        if not single_batch:\r\n            single_batch = 1\r\n        if single_batch >= len(x):\r\n            return self._sess.run(self._y_pred, {self._tfx: x})\r\n        epoch = int(len(x) / single_batch)\r\n        if not len(x) % single_batch:\r\n            epoch += 1\r\n        name = ""Prediction"" if name is None else ""Prediction ({})"".format(name)\r\n        sub_bar = ProgressBar(max_value=epoch, name=name, start=False)\r\n        if verbose >= NNVerbose.METRICS:\r\n            sub_bar.start()\r\n        rs = [self._sess.run(self._y_pred, {self._tfx: x[:single_batch]})]\r\n        count = single_batch\r\n        if verbose >= NNVerbose.METRICS:\r\n            sub_bar.update()\r\n        while count < len(x):\r\n            count += single_batch\r\n            if count >= len(x):\r\n                rs.append(self._sess.run(self._y_pred, {self._tfx: x[count - single_batch:]}))\r\n            else:\r\n                rs.append(self._sess.run(self._y_pred, {self._tfx: x[count - single_batch:count]}))\r\n            if verbose >= NNVerbose.METRICS:\r\n                sub_bar.update()\r\n        return np.vstack(rs)\r\n\r\n    @staticmethod\r\n    @NNTiming.timeit(level=4)\r\n    def _get_w(shape):\r\n        initial = tf.truncated_normal(shape, stddev=0.1)\r\n        return tf.Variable(initial, name=""w"")\r\n\r\n    @staticmethod\r\n    @NNTiming.timeit(level=4)\r\n    def _get_b(shape):\r\n        return tf.Variable(np.zeros(shape, dtype=np.float32) + 0.1, name=""b"")\r\n\r\n    @NNTiming.timeit(level=4)\r\n    def _add_params(self, shape, conv_channel=None, fc_shape=None, apply_bias=True):\r\n        if fc_shape is not None:\r\n            w_shape = (fc_shape, shape[1])\r\n            b_shape = shape[1],\r\n        elif conv_channel is not None:\r\n            if len(shape[1]) <= 2:\r\n                w_shape = shape[1][0], shape[1][1], conv_channel, conv_channel\r\n            else:\r\n                w_shape = (shape[1][1], shape[1][2], conv_channel, shape[1][0])\r\n            b_shape = shape[1][0],\r\n        else:\r\n            w_shape = shape\r\n            b_shape = shape[1],\r\n        self._tf_weights.append(self._get_w(w_shape))\r\n        if apply_bias:\r\n            self._tf_bias.append(self._get_b(b_shape))\r\n        else:\r\n            self._tf_bias.append(None)\r\n\r\n    @NNTiming.timeit(level=4)\r\n    def _add_param_placeholder(self):\r\n        self._tf_weights.append(tf.constant([.0]))\r\n        self._tf_bias.append(tf.constant([.0]))\r\n\r\n    @NNTiming.timeit(level=4)\r\n    def _add_layer(self, layer, *args, **kwargs):\r\n        if not self._layers and isinstance(layer, str):\r\n            layer = self._layer_factory.get_root_layer_by_name(layer, *args, **kwargs)\r\n            if layer:\r\n                self.add(layer)\r\n                return\r\n        parent = self._layers[-1]\r\n        if isinstance(layer, str):\r\n            layer, shape = self._layer_factory.get_layer_by_name(\r\n                layer, parent, self._current_dimension, *args, **kwargs\r\n            )\r\n            if shape is None:\r\n                self.add(layer)\r\n                return\r\n            current, nxt = shape\r\n        else:\r\n            current, nxt = args\r\n        if isinstance(layer, SubLayer):\r\n            self.parent = parent\r\n            self._layers.append(layer)\r\n            self._add_param_placeholder()\r\n            self._current_dimension = nxt\r\n        else:\r\n            fc_shape, conv_channel, last_layer = None, None, self._layers[-1]\r\n            if isinstance(last_layer, ConvLayer):\r\n                if isinstance(layer, ConvLayer):\r\n                    conv_channel = last_layer.n_filters\r\n                    current = (conv_channel, last_layer.out_h, last_layer.out_w)\r\n                    layer.feed_shape((current, nxt))\r\n                else:\r\n                    layer.is_fc = True\r\n                    last_layer.is_fc_base = True\r\n                    fc_shape = last_layer.out_h * last_layer.out_w * last_layer.n_filters\r\n            self._layers.append(layer)\r\n            self._add_params((current, nxt), conv_channel, fc_shape, layer.apply_bias)\r\n            self._current_dimension = nxt\r\n\r\n    @NNTiming.timeit(level=1)\r\n    def _get_rs(self, x, predict=True, idx=-1):\r\n        cache = self._layers[0].activate(x, self._tf_weights[0], self._tf_bias[0], predict)\r\n        idx = idx + 1 if idx >= 0 else len(self._layers) + idx + 1\r\n        for i, layer in enumerate(self._layers[1:idx]):\r\n            if i == len(self._layers) - 2:\r\n                if isinstance(self._layers[-2], ConvLayer):\r\n                    fc_shape = np.prod(cache.get_shape()[1:])  # type: int\r\n                    cache = tf.reshape(cache, [-1, fc_shape])\r\n                if self._tf_bias[-1] is not None:\r\n                    return tf.matmul(cache, self._tf_weights[-1]) + self._tf_bias[-1]\r\n                return tf.matmul(cache, self._tf_weights[-1])\r\n            cache = layer.activate(cache, self._tf_weights[i + 1], self._tf_bias[i + 1], predict)\r\n        return cache\r\n\r\n    @NNTiming.timeit(level=2)\r\n    def _append_log(self, x, y, y_classes, name):\r\n        y_pred = self._get_prediction(x, name)\r\n        y_pred_class = np.argmax(y_pred, axis=1)\r\n        for i, metric in enumerate(self._metrics):\r\n            self._logs[name][i].append(metric(y_classes, y_pred_class))\r\n        self._logs[name][-1].append(self._sess.run(\r\n            self._layers[-1].calculate(y, y_pred)\r\n        ))\r\n\r\n    @NNTiming.timeit(level=2)\r\n    def _print_metric_logs(self, name):\r\n        print()\r\n        print(""="" * 47)\r\n        for i, metric in enumerate(self._metric_names):\r\n            print(""{:<16s} {:<16s}: {:12.8}"".format(\r\n                name, metric, self._logs[name][i][-1]))\r\n        print(""{:<16s} {:<16s}: {:12.8}"".format(\r\n            name, ""loss"", self._logs[name][-1][-1]))\r\n        print(""="" * 47)\r\n\r\n    @staticmethod\r\n    @NNTiming.timeit(level=4, prefix=""[Private StaticMethod] "")\r\n    def _transfer_x(x):\r\n        if len(x.shape) == 1:\r\n            x = x.reshape(1, -1)\r\n        if len(x.shape) == 4:\r\n            x = x.transpose(0, 2, 3, 1)\r\n        return x.astype(np.float32)\r\n\r\n    @NNTiming.timeit(level=4)\r\n    def _preview(self):\r\n        if not self._layers:\r\n            rs = ""None""\r\n        else:\r\n            rs = (\r\n                ""Input  :  {:<10s} - {}\\n"".format(""Dimension"", self._layers[0].shape[0]) +\r\n                ""\\n"".join(\r\n                    [""Layer  :  {:<10s} - {}"".format(\r\n                        _layer.name, _layer.shape[1]\r\n                    ) for _layer in self._layers[:-1]]\r\n                ) + ""\\nCost   :  {:<10s}"".format(self._layers[-1].name)\r\n            )\r\n        print(""\\n"" + ""="" * 30 + ""\\n"" + ""Structure\\n"" + ""-"" * 30 + ""\\n"" + rs + ""\\n"" + ""="" * 30)\r\n        print(""Optimizer"")\r\n        print(""-"" * 30)\r\n        print(self._optimizer)\r\n        print(""="" * 30)\r\n\r\n    @NNTiming.timeit(level=2)\r\n    def _batch_work(self, i, bar, x_train, y_train, y_train_classes, x_test, y_test, y_test_classes, condition):\r\n        if bar is not None:\r\n            condition = bar.update() and condition\r\n        if condition:\r\n            self._append_log(x_train, y_train, y_train_classes, ""Train"")\r\n            self._append_log(x_test, y_test, y_test_classes, ""Test"")\r\n            self._print_metric_logs(""Train"")\r\n            self._print_metric_logs(""Test"")\r\n\r\n    @NNTiming.timeit(level=4, prefix=""[API] "")\r\n    def add(self, layer, *args, **kwargs):\r\n        if isinstance(layer, str):\r\n            self._add_layer(layer, *args, **kwargs)\r\n        else:\r\n            if not self._layers:\r\n                self._layers, self._current_dimension = [layer], layer.shape[1]\r\n                if isinstance(layer, ConvLayer):\r\n                    self._add_params(layer.shape, layer.n_channels, apply_bias=layer.apply_bias)\r\n                else:\r\n                    self._add_params(layer.shape, apply_bias=layer.apply_bias)\r\n            else:\r\n                if len(layer.shape) == 2:\r\n                    _current, _next = layer.shape\r\n                else:\r\n                    _current, _next = self._current_dimension, layer.shape[0]\r\n                    layer.shape = (_current, _next)\r\n                self._add_layer(layer, _current, _next)\r\n\r\n    @NNTiming.timeit(level=1, prefix=""[API] "")\r\n    def fit(self, x, y, lr=None, epoch=None, batch_size=None, train_rate=None,\r\n            optimizer=None, metrics=None, record_period=None, verbose=None, preview=None):\r\n        if lr is None:\r\n            lr = self._params[""lr""]\r\n        if epoch is None:\r\n            epoch = self._params[""epoch""]\r\n        if optimizer is None:\r\n            optimizer = self._params[""optimizer""]\r\n        if batch_size is None:\r\n            batch_size = self._params[""batch_size""]\r\n        if train_rate is None:\r\n            train_rate = self._params[""train_rate""]\r\n        if metrics is None:\r\n            metrics = self._params[""metrics""]\r\n        if record_period is None:\r\n            record_period = self._params[""record_period""]\r\n        if verbose is None:\r\n            verbose = self._params[""verbose""]\r\n        if preview is None:\r\n            preview = self._params[""preview""]\r\n\r\n        x = NN._transfer_x(x)\r\n        self.verbose = verbose\r\n        self._optimizer = OptFactory().get_optimizer_by_name(optimizer, lr)\r\n        self._tfx = tf.placeholder(tf.float32, shape=[None, *x.shape[1:]])\r\n        self._tfy = tf.placeholder(tf.float32, shape=[None, y.shape[1]])\r\n\r\n        if train_rate is not None:\r\n            train_rate = float(train_rate)\r\n            train_len = int(len(x) * train_rate)\r\n            shuffle_suffix = np.random.permutation(int(len(x)))\r\n            x, y = x[shuffle_suffix], y[shuffle_suffix]\r\n            x_train, y_train = x[:train_len], y[:train_len]\r\n            x_test, y_test = x[train_len:], y[train_len:]\r\n        else:\r\n            x_train = x_test = x\r\n            y_train = y_test = y\r\n        y_train_classes = np.argmax(y_train, axis=1)\r\n        y_test_classes = np.argmax(y_test, axis=1)\r\n\r\n        if metrics is None:\r\n            metrics = []\r\n        self._metrics = self.get_metrics(metrics)\r\n        self._metric_names = [_m.__name__ for _m in metrics]\r\n        self._logs = {\r\n            name: [[] for _ in range(len(metrics) + 1)] for name in (""Train"", ""Test"")\r\n        }\r\n\r\n        bar = ProgressBar(max_value=max(1, epoch // record_period), name=""Epoch"", start=False)\r\n        if self.verbose >= NNVerbose.EPOCH:\r\n            bar.start()\r\n\r\n        if preview:\r\n            self._preview()\r\n\r\n        args = (\r\n            (x_train, y_train, y_train_classes,\r\n             x_test, y_test, y_test_classes,\r\n             self.verbose >= NNVerbose.METRICS_DETAIL),\r\n            (None, None, x_train, y_train, y_train_classes, x_test, y_test, y_test_classes,\r\n             self.verbose >= NNVerbose.METRICS)\r\n        )\r\n        train_repeat = self._get_train_repeat(x, batch_size)\r\n        with self._sess.as_default() as sess:\r\n            self._y_pred = self._get_rs(self._tfx)\r\n            self._inner_y = self._get_rs(self._tfx, predict=False)\r\n            self._loss = self._layers[-1].calculate(self._tfy, self._inner_y)\r\n            self._train_step = self._optimizer.minimize(self._loss)\r\n            sess.run(tf.global_variables_initializer())\r\n            for counter in range(epoch):\r\n                if self.verbose >= NNVerbose.ITER and counter % record_period == 0:\r\n                    sub_bar = ProgressBar(max_value=train_repeat * record_period - 1, name=""Iteration"")\r\n                else:\r\n                    sub_bar = None\r\n                self._batch_training(x_train, y_train, batch_size, train_repeat,\r\n                                     self._loss, self._train_step, sub_bar, *args[0])\r\n                if (counter + 1) % record_period == 0:\r\n                    self._batch_work(*args[1])\r\n                    if self.verbose >= NNVerbose.EPOCH:\r\n                        bar.update(counter // record_period + 1)\r\n\r\n    @NNTiming.timeit(level=1, prefix=""[API] "")\r\n    def predict(self, x, get_raw_results=False, **kwargs):\r\n        y_pred = self._get_prediction(NN._transfer_x(x))\r\n        if get_raw_results:\r\n            return y_pred\r\n        return np.argmax(y_pred, axis=1)\r\n\r\n    def draw_logs(self):\r\n        metrics_log, cost_log = {}, {}\r\n        for key, value in sorted(self._logs.items()):\r\n            metrics_log[key], cost_log[key] = value[:-1], value[-1]\r\n        for i, name in enumerate(sorted(self._metric_names)):\r\n            plt.figure()\r\n            plt.title(""Metric Type: {}"".format(name))\r\n            for key, log in sorted(metrics_log.items()):\r\n                xs = np.arange(len(log[i])) + 1\r\n                plt.plot(xs, log[i], label=""Data Type: {}"".format(key))\r\n            plt.legend(loc=4)\r\n            plt.show()\r\n            plt.close()\r\n        plt.figure()\r\n        plt.title(""Cost"")\r\n        for key, loss in sorted(cost_log.items()):\r\n            xs = np.arange(len(loss)) + 1\r\n            plt.plot(xs, loss, label=""Data Type: {}"".format(key))\r\n        plt.legend()\r\n        plt.show()\r\n'"
g_CNN/Optimizers.py,7,"b'import os\r\nimport sys\r\nroot_path = os.path.abspath(""../"")\r\nif root_path not in sys.path:\r\n    sys.path.append(root_path)\r\n\r\nimport tensorflow as tf\r\n\r\n\r\nclass Optimizer:\r\n    def __init__(self, lr=1e-3):\r\n        self._lr = lr\r\n        self._opt = None\r\n\r\n    @property\r\n    def name(self):\r\n        return str(self)\r\n\r\n    def minimize(self, x, *args, **kwargs):\r\n        return self._opt.minimize(x, *args, **kwargs)\r\n\r\n    def __str__(self):\r\n        return self.__class__.__name__\r\n\r\n    def __repr__(self):\r\n        return str(self)\r\n\r\n\r\nclass MBGD(Optimizer):\r\n    def __init__(self, lr=1e-3):\r\n        Optimizer.__init__(self, lr)\r\n        self._opt = tf.train.GradientDescentOptimizer(self._lr)\r\n\r\n\r\nclass Momentum(Optimizer):\r\n    def __init__(self, lr=1e-3, momentum=0.8):\r\n        Optimizer.__init__(self, lr)\r\n        self._opt = tf.train.MomentumOptimizer(self._lr, momentum)\r\n\r\n\r\nclass NAG(Optimizer):\r\n    def __init__(self, lr=1e-3, momentum=0.8):\r\n        Optimizer.__init__(self, lr)\r\n        self._opt = tf.train.MomentumOptimizer(self._lr, momentum, use_nesterov=True)\r\n\r\n\r\nclass AdaDelta(Optimizer):\r\n    def __init__(self, lr=1e-3, rho=0.95, eps=1e-8):\r\n        Optimizer.__init__(self, lr)\r\n        self._opt = tf.train.AdadeltaOptimizer(self._lr, rho, eps)\r\n\r\n\r\nclass AdaGrad(Optimizer):\r\n    def __init__(self, lr=1e-3, init=0.1):\r\n        Optimizer.__init__(self, lr)\r\n        self._opt = tf.train.AdagradOptimizer(self._lr, init)\r\n\r\n\r\nclass Adam(Optimizer):\r\n    def __init__(self, lr=1e-3, beta1=0.9, beta2=0.999, eps=1e-8):\r\n        Optimizer.__init__(self, lr)\r\n        self._opt = tf.train.AdamOptimizer(self._lr, beta1, beta2, eps)\r\n\r\n\r\nclass RMSProp(Optimizer):\r\n    def __init__(self, lr=1e-3, decay=0.9, momentum=0.0, eps=1e-10):\r\n        Optimizer.__init__(self, lr)\r\n        self._opt = tf.train.RMSPropOptimizer(self._lr, decay, momentum, eps)\r\n\r\n\r\n# Factory\r\n\r\nclass OptFactory:\r\n\r\n    available_optimizers = {\r\n        ""MBGD"": MBGD, ""Momentum"": Momentum, ""NAG"": NAG,\r\n        ""AdaDelta"": AdaDelta, ""AdaGrad"": AdaGrad,\r\n        ""Adam"": Adam, ""RMSProp"": RMSProp\r\n    }\r\n\r\n    def get_optimizer_by_name(self, name, lr, *args, **kwargs):\r\n        try:\r\n            optimizer = self.available_optimizers[name](lr, *args, **kwargs)\r\n            return optimizer\r\n        except KeyError:\r\n            raise NotImplementedError(""Undefined Optimizer \'{}\' found"".format(name))\r\n'"
h_RNN/EmbedRNN.py,8,"b'import os\r\nimport sys\r\nroot_path = os.path.abspath(""../"")\r\nif root_path not in sys.path:\r\n    sys.path.append(root_path)\r\n\r\nimport numpy as np\r\nimport tensorflow as tf\r\n\r\nfrom h_RNN.RNN import Generator\r\nfrom h_RNN.SpRNN import SparseRNN\r\n\r\n\r\nclass EmbedRNN(SparseRNN):\r\n    def __init__(self, embedding_size=200):\r\n        super(EmbedRNN, self).__init__()\r\n        self._embedding_size = embedding_size\r\n\r\n    def _define_input(self, im, om):\r\n        self._tfx = tf.placeholder(tf.int32, shape=[None, None])\r\n        embeddings = tf.Variable(tf.random_uniform([im, self._embedding_size], -1.0, 1.0))\r\n        self._input = tf.nn.embedding_lookup(embeddings, self._tfx)\r\n        self._tfy = tf.placeholder(tf.int32, shape=[None])\r\n\r\n\r\nclass EmbedRNNForOp(EmbedRNN):\r\n    def __init__(self, embedding_size=200):\r\n        super(EmbedRNNForOp, self).__init__(embedding_size)\r\n        self._op = """"\r\n\r\n    def _verbose(self):\r\n        x_test, y_test = self._generator.gen(1)\r\n        ans = self.predict(x_test)\r\n        x_test = x_test.astype(np.int)\r\n        print(""I think {} = {}, answer: {}..."".format(\r\n            "" {} "".format(self._op).join(map(lambda n: str(n), x_test[0])),\r\n            """".join(map(lambda n: str(n), ans)),\r\n            """".join(map(lambda n: str(n), y_test))))\r\n\r\n\r\nclass EmbedRNNForAddition(EmbedRNNForOp):\r\n    def __init__(self, embedding_size=200):\r\n        super(EmbedRNNForAddition, self).__init__(embedding_size)\r\n        self._op = ""+""\r\n\r\n\r\nclass EmbedRNNForMultiple(EmbedRNNForOp):\r\n    def __init__(self, embedding_size=200):\r\n        super(EmbedRNNForMultiple, self).__init__(embedding_size)\r\n        self._op = ""*""\r\n\r\n\r\nclass EmbedOpGenerator(Generator):\r\n    def __init__(self, im, om, n_digit):\r\n        super(EmbedOpGenerator, self).__init__(im, om)\r\n        self._n_digit = n_digit\r\n\r\n    def _op(self, x):\r\n        return 0\r\n\r\n    def _get_x(self):\r\n        return 0\r\n\r\n    def gen(self, batch_size, test=False, boost=0):\r\n        x = np.empty([batch_size, self._n_digit], dtype=np.int32)\r\n        y = np.zeros(batch_size, dtype=np.int32)\r\n        for i in range(batch_size):\r\n            x[i] = self._get_x()\r\n            y[i] = self._op(x[i])\r\n        return x, y\r\n\r\n\r\nclass EmbedAdditionGenerator(EmbedOpGenerator):\r\n    def _op(self, seq):\r\n        return sum(seq)\r\n\r\n    def _get_x(self):\r\n        return np.random.randint(0, int(min(self._im, self._om / self._n_digit)), self._n_digit)\r\n\r\n\r\nclass EmbedMultipleGenerator(EmbedOpGenerator):\r\n    def _op(self, seq):\r\n        return np.prod(seq)\r\n\r\n    def _get_x(self):\r\n        return np.random.randint(0, int(min(self._im, self._om ** (1 / self._n_digit))), self._n_digit)\r\n\r\nif __name__ == \'__main__\':\r\n    _n_digit = 2\r\n    _im, _om = 100, 10000\r\n    _generator = EmbedMultipleGenerator(_im, _om, n_digit=_n_digit)\r\n    lstm = EmbedRNNForMultiple(embedding_size=50)\r\n    lstm.fit(\r\n        _im, _om, _generator,\r\n        # cell=tf.contrib.rnn.GRUCell,\r\n        # cell=tf.contrib.rnn.LSTMCell,\r\n        # cell=tf.contrib.rnn.BasicRNNCell,\r\n        # cell=tf.contrib.rnn.BasicLSTMCell,\r\n        epoch=20, n_history=_n_digit\r\n    )\r\n    lstm.draw_err_logs()\r\n'"
h_RNN/Mnist.py,2,"b'import os\r\nimport sys\r\nroot_path = os.path.abspath(""../"")\r\nif root_path not in sys.path:\r\n    sys.path.append(root_path)\r\n\r\nimport time\r\nimport numpy as np\r\nimport tensorflow as tf\r\n\r\nfrom h_RNN.RNN import RNNWrapper, Generator\r\nfrom h_RNN.SpRNN import SparseRNN\r\nfrom Util.Util import DataUtil\r\n\r\n\r\nclass MnistGenerator(Generator):\r\n    def __init__(self, im=None, om=None, one_hot=True):\r\n        super(MnistGenerator, self).__init__(im, om)\r\n        self._x, self._y = DataUtil.get_dataset(""mnist"", ""../_Data/mnist.txt"", quantized=True, one_hot=one_hot)\r\n        self._x = self._x.reshape(-1, 28, 28)\r\n        self._x_train, self._x_test = self._x[:1800], self._x[1800:]\r\n        self._y_train, self._y_test = self._y[:1800], self._y[1800:]\r\n\r\n    def gen(self, batch, test=False, **kwargs):\r\n        if batch == 0:\r\n            if test:\r\n                return self._x_test, self._y_test\r\n            return self._x_train, self._y_train\r\n        batch = np.random.choice(len(self._x_train), batch)\r\n        return self._x_train[batch], self._y_train[batch]\r\n\r\nif __name__ == \'__main__\':\r\n    n_history = 3\r\n    print(""="" * 60, ""\\n"" + ""Normal LSTM"", ""\\n"" + ""-"" * 60)\r\n    generator = MnistGenerator()\r\n    t = time.time()\r\n    tf.reset_default_graph()\r\n    rnn = RNNWrapper()\r\n    rnn.fit(28, 10, generator, n_history=n_history, epoch=10, squeeze=True)\r\n    print(""Time Cost: {}"".format(time.time() - t))\r\n    rnn.draw_err_logs()\r\n\r\n    print(""="" * 60, ""\\n"" + ""Sparse LSTM"" + ""\\n"" + ""-"" * 60)\r\n    generator = MnistGenerator(one_hot=False)\r\n    t = time.time()\r\n    tf.reset_default_graph()\r\n    rnn = SparseRNN()\r\n    rnn.fit(28, 10, generator, n_history=n_history, epoch=10)\r\n    print(""Time Cost: {}"".format(time.time() - t))\r\n    rnn.draw_err_logs()\r\n'"
h_RNN/Playground.py,0,"b""import numpy as np\r\n\r\n\r\nclass RNN1:\r\n    def __init__(self, u, v, w):\r\n        self._u, self._v, self._w = np.asarray(u), np.asarray(v), np.asarray(w)\r\n        self._states = None\r\n\r\n    def activate(self, x):\r\n        return x\r\n\r\n    def transform(self, x):\r\n        return x\r\n\r\n    def run(self, x):\r\n        output = []\r\n        x = np.atleast_2d(x)\r\n        self._states = np.zeros([len(x)+1, self._u.shape[0]])\r\n        for t, xt in enumerate(x):\r\n            self._states[t] = self.activate(\r\n                self._u.dot(xt) + self._w.dot(self._states[t-1])\r\n            )\r\n            output.append(self.transform(\r\n                self._v.dot(self._states[t]))\r\n            )\r\n        return np.array(output)\r\n\r\n\r\nclass RNN2(RNN1):\r\n    def activate(self, x):\r\n        return 1 / (1 + np.exp(-x))\r\n\r\n    def transform(self, x):\r\n        safe_exp = np.exp(x - np.max(x))\r\n        return safe_exp / np.sum(safe_exp)\r\n\r\n    def bptt(self, x, y):\r\n        x, y, T = np.asarray(x), np.asarray(y), len(y)\r\n        o = self.run(x)\r\n        dis = o - y\r\n        dv = dis.T.dot(self._states[:-1])\r\n        du = np.zeros_like(self._u)\r\n        dw = np.zeros_like(self._w)\r\n        for t in range(T-1, -1, -1):\r\n            ds = self._v.T.dot(dis[t])\r\n            for bptt_step in range(t, max(-1, t-10), -1):\r\n                du += np.outer(ds, x[bptt_step])\r\n                dw += np.outer(ds, self._states[bptt_step-1])\r\n                st = self._states[bptt_step-1]\r\n                ds = self._w.T.dot(ds) * st * (1 - st)\r\n        return du, dv, dw\r\n\r\nif __name__ == '__main__':\r\n    _T = 5\r\n    rnn = RNN1(np.eye(_T), np.eye(_T), np.eye(_T) * 2)\r\n    print(rnn.run(np.eye(_T)))\r\n"""
h_RNN/RNN.py,25,"b'import os\r\nimport sys\r\nroot_path = os.path.abspath(""../"")\r\nif root_path not in sys.path:\r\n    sys.path.append(root_path)\r\n\r\nimport random\r\nimport numpy as np\r\nimport tensorflow as tf\r\nimport matplotlib.pyplot as plt\r\nimport tensorflow.contrib.layers as layers\r\n\r\nfrom g_CNN.Optimizers import OptFactory\r\nfrom Util.ProgressBar import ProgressBar\r\n\r\n\r\nclass LSTMCell(tf.contrib.rnn.BasicRNNCell):\r\n    def __call__(self, x, state, scope=""LSTM""):\r\n        with tf.variable_scope(scope):\r\n            s_old, h_old = tf.split(state, 2, 1)\r\n            gates = layers.fully_connected(\r\n                tf.concat([x, s_old], 1),\r\n                num_outputs=4 * self._num_units,\r\n                activation_fn=None)\r\n            r1, g1, g2, g3 = tf.split(gates, 4, 1)\r\n            r1, g1, g3 = tf.nn.sigmoid(r1), tf.nn.sigmoid(g1), tf.nn.sigmoid(g3)\r\n            g2 = tf.nn.tanh(g2)\r\n            h_new = h_old * r1 + g1 * g2\r\n            s_new = tf.nn.tanh(h_new) * g3\r\n            return s_new, tf.concat([s_new, h_new], 1)\r\n\r\n    @property\r\n    def state_size(self):\r\n        return 2 * self._num_units\r\n\r\n\r\nclass RNNWrapper:\r\n    def __init__(self):\r\n        self._log = {}\r\n        self._im = self._om = None\r\n        self._optimizer = self._generator = None\r\n        self._tfx = self._tfy = self._input = self._output = None\r\n        self._sess = tf.Session()\r\n\r\n        self._squeeze = self._use_final_state = False\r\n        self._activation = tf.nn.sigmoid\r\n\r\n    def _define_input(self, im, om):\r\n        self._input = self._tfx = tf.placeholder(tf.float32, shape=[None, None, im])\r\n        if self._squeeze:\r\n            self._tfy = tf.placeholder(tf.float32, shape=[None, om])\r\n        else:\r\n            self._tfy = tf.placeholder(tf.float32, shape=[None, None, om])\r\n\r\n    def _get_loss(self, eps):\r\n        return -tf.reduce_mean(\r\n            self._tfy * tf.log(self._output + eps) + (1 - self._tfy) * tf.log(1 - self._output + eps)\r\n        )\r\n\r\n    def _get_output(self, rnn_outputs, rnn_final_state, n_history):\r\n        if n_history == 0 and self._squeeze:\r\n            raise ValueError(""\'n_history\' should not be 0 when trying to squeeze the outputs"")\r\n        if n_history == 1 and self._squeeze:\r\n            outputs = rnn_outputs[..., -1, :]\r\n        else:\r\n            outputs = rnn_outputs[..., -n_history:, :]\r\n            if self._squeeze:\r\n                outputs = tf.reshape(outputs, [-1, n_history * int(outputs.get_shape()[2])])\r\n        if self._use_final_state and self._squeeze:\r\n            outputs = tf.concat([outputs, rnn_final_state], axis=1)\r\n        self._output = layers.fully_connected(\r\n            outputs, num_outputs=self._om, activation_fn=self._activation)\r\n\r\n    def _verbose(self):\r\n        x_test, y_test = self._generator.gen(0, True)\r\n        axis = 1 if self._squeeze else 2\r\n        if len(y_test.shape) == 1:\r\n            y_true = y_test\r\n        else:\r\n            y_true = np.argmax(y_test, axis=axis).ravel()  # type: np.ndarray\r\n        y_pred = self.predict(x_test).ravel()  # type: np.ndarray\r\n        print(""Test acc: {:8.6} %"".format(np.mean(y_true == y_pred) * 100))\r\n\r\n    def fit(self, im, om, generator, cell=LSTMCell,\r\n            n_hidden=128, n_history=0, squeeze=None, use_final_state=None, activation=None,\r\n            lr=0.01, epoch=10, n_iter=128, batch_size=64, optimizer=""Adam"", eps=1e-8, verbose=1):\r\n        if squeeze:\r\n            self._squeeze = True\r\n        if use_final_state:\r\n            self._use_final_state = True\r\n        if callable(activation):\r\n            self._activation = activation\r\n        self._generator = generator\r\n        self._im, self._om = im, om\r\n        self._optimizer = OptFactory().get_optimizer_by_name(optimizer, lr)\r\n        self._define_input(im, om)\r\n\r\n        cell = cell(n_hidden)\r\n        initial_state = cell.zero_state(tf.shape(self._input)[0], tf.float32)\r\n        rnn_outputs, rnn_final_state = tf.nn.dynamic_rnn(\r\n            cell, self._input, initial_state=initial_state)\r\n        self._get_output(rnn_outputs, rnn_final_state, n_history)\r\n        loss = self._get_loss(eps)\r\n        train_step = self._optimizer.minimize(loss)\r\n        self._log[""iter_err""] = []\r\n        self._log[""epoch_err""] = []\r\n        self._sess.run(tf.global_variables_initializer())\r\n        bar = ProgressBar(max_value=epoch, name=""Epoch"", start=False)\r\n        if verbose >= 2:\r\n            bar.start()\r\n        for _ in range(epoch):\r\n            epoch_err = 0\r\n            sub_bar = ProgressBar(max_value=n_iter, name=""Iter"", start=False)\r\n            if verbose >= 2:\r\n                sub_bar.start()\r\n            for __ in range(n_iter):\r\n                x_batch, y_batch = self._generator.gen(batch_size)\r\n                iter_err = self._sess.run([loss, train_step], {\r\n                    self._tfx: x_batch, self._tfy: y_batch,\r\n                })[0]\r\n                self._log[""iter_err""].append(iter_err)\r\n                epoch_err += iter_err\r\n                if verbose >= 2:\r\n                    sub_bar.update()\r\n            self._log[""epoch_err""].append(epoch_err / n_iter)\r\n            if verbose >= 1:\r\n                self._verbose()\r\n                if verbose >= 2:\r\n                    bar.update()\r\n\r\n    def predict(self, x, get_raw_results=False):\r\n        y_pred = self._sess.run(self._output, {self._tfx: x})\r\n        axis = 1 if self._squeeze else 2\r\n        if get_raw_results:\r\n            return y_pred\r\n        return np.argmax(y_pred, axis=axis)\r\n\r\n    def draw_err_logs(self):\r\n        ee, ie = self._log[""epoch_err""], self._log[""iter_err""]\r\n        ee_base = np.arange(len(ee))\r\n        ie_base = np.linspace(0, len(ee) - 1, len(ie))\r\n        plt.figure()\r\n        plt.plot(ie_base, ie, label=""Iter error"")\r\n        plt.plot(ee_base, ee, linewidth=3, label=""Epoch error"")\r\n        plt.legend()\r\n        plt.show()\r\n\r\n\r\nclass RNNForOp(RNNWrapper):\r\n    def __init__(self, boost=2):\r\n        super(RNNForOp, self).__init__()\r\n        self._boost = boost\r\n        self._op = """"\r\n\r\n    def _verbose(self):\r\n        x_test, y_test = self._generator.gen(1, boost=self._boost)\r\n        ans = self.predict(x_test).ravel()\r\n        x_test = x_test.astype(np.int)\r\n        print(""I think {} = {}, answer: {}..."".format(\r\n            "" {} "".format(self._op).join(\r\n                ["""".join(map(lambda n: str(n), x_test[0, ..., i][::-1])) for i in range(x_test.shape[2])]\r\n            ),\r\n            """".join(map(lambda n: str(n), ans[::-1])),\r\n            """".join(map(lambda n: str(n), np.argmax(y_test, axis=2).ravel()[::-1]))))\r\n\r\n\r\nclass RNNForAddition(RNNForOp):\r\n    def __init__(self, boost=2):\r\n        super(RNNForAddition, self).__init__(boost)\r\n        self._op = ""+""\r\n\r\n\r\nclass RNNForMultiple(RNNForOp):\r\n    def __init__(self, boost=2):\r\n        super(RNNForMultiple, self).__init__(boost)\r\n        self._op = ""*""\r\n\r\n\r\nclass Generator:\r\n    def __init__(self, im=None, om=None, **kwargs):\r\n        self._im, self._om = im, om\r\n\r\n    def gen(self, batch, test=False, **kwargs):\r\n        pass\r\n\r\n\r\nclass OpGenerator(Generator):\r\n    def __init__(self, im, om, n_time_step, random_scale):\r\n        super(OpGenerator, self).__init__(im, om)\r\n        self._base = self._om\r\n        self._n_time_step = n_time_step\r\n        self._random_scale = random_scale\r\n\r\n    def _op(self, seq):\r\n        return 0\r\n\r\n    def _gen_seq(self, n_time_step, tar):\r\n        seq = []\r\n        for _ in range(n_time_step):\r\n            seq.append(tar % self._base)\r\n            tar //= self._base\r\n        return seq\r\n\r\n    def _gen_targets(self, n_time_step):\r\n        return []\r\n\r\n    def gen(self, batch_size, test=False, boost=0):\r\n        if boost:\r\n            n_time_step = self._n_time_step + self._random_scale + random.randint(1, boost)\r\n        else:\r\n            n_time_step = self._n_time_step + random.randint(0, self._random_scale)\r\n        x = np.empty([batch_size, n_time_step, self._im])\r\n        y = np.zeros([batch_size, n_time_step, self._om])\r\n        for i in range(batch_size):\r\n            targets = self._gen_targets(n_time_step)\r\n            sequences = [self._gen_seq(n_time_step, tar) for tar in targets]\r\n            for j in range(self._im):\r\n                x[i, ..., j] = sequences[j]\r\n            y[i, range(n_time_step), self._gen_seq(n_time_step, self._op(targets))] = 1\r\n        return x, y\r\n\r\n\r\nclass AdditionGenerator(OpGenerator):\r\n    def _op(self, seq):\r\n        return sum(seq)\r\n\r\n    def _gen_targets(self, n_time_step):\r\n        return [int(random.randint(0, self._om ** n_time_step - 1) / self._im) for _ in range(self._im)]\r\n\r\n\r\nclass MultipleGenerator(OpGenerator):\r\n    def _op(self, seq):\r\n        return np.prod(seq)\r\n\r\n    def _gen_targets(self, n_time_step):\r\n        return [int(random.randint(0, self._om ** n_time_step - 1) ** (1 / self._im)) for _ in range(self._im)]\r\n\r\nif __name__ == \'__main__\':\r\n    _random_scale = 0\r\n    _digit_len, _digit_base, _n_digit = 3, 10, 2\r\n    _generator = MultipleGenerator(\r\n        _n_digit, _digit_base,\r\n        n_time_step=_digit_len, random_scale=_random_scale\r\n    )\r\n    lstm = RNNForMultiple(boost=1)\r\n    lstm.fit(\r\n        _n_digit, _digit_base, _generator,\r\n        # cell=tf.contrib.rnn.GRUCell,\r\n        # cell=tf.contrib.rnn.LSTMCell,\r\n        # cell=tf.contrib.rnn.BasicRNNCell,\r\n        # cell=tf.contrib.rnn.BasicLSTMCell,\r\n        epoch=100\r\n    )\r\n    lstm.draw_err_logs()\r\n'"
h_RNN/SpRNN.py,8,"b'import os\r\nimport sys\r\nroot_path = os.path.abspath(""../"")\r\nif root_path not in sys.path:\r\n    sys.path.append(root_path)\r\n\r\nimport random\r\nimport numpy as np\r\nimport tensorflow as tf\r\n\r\nfrom h_RNN.RNN import RNNWrapper, OpGenerator\r\n\r\n\r\nclass SparseRNN(RNNWrapper):\r\n    def __init__(self):\r\n        super(SparseRNN, self).__init__()\r\n        self._squeeze = True\r\n        self._activation = None\r\n\r\n    def _define_input(self, im, om):\r\n        self._input = self._tfx = tf.placeholder(tf.float32, shape=[None, None, im])\r\n        self._tfy = tf.placeholder(tf.int32, shape=[None])\r\n\r\n    def _get_loss(self, eps):\r\n        return tf.reduce_mean(\r\n            tf.nn.sparse_softmax_cross_entropy_with_logits(labels=self._tfy, logits=self._output)\r\n        )\r\n\r\n\r\nclass SpRNNForOp(SparseRNN):\r\n    def __init__(self):\r\n        super(SpRNNForOp, self).__init__()\r\n        self._op = """"\r\n\r\n    def _verbose(self):\r\n        x_test, y_test = self._generator.gen(1)\r\n        ans = self.predict(x_test)\r\n        x_test = x_test.astype(np.int)\r\n        print(""I think {} = {}, answer: {}..."".format(\r\n            "" {} "".format(self._op).join(\r\n                ["""".join(map(lambda n: str(n), x_test[0, ..., i][::-1])) for i in range(x_test.shape[2])]\r\n            ),\r\n            """".join(map(lambda n: str(n), ans)),\r\n            """".join(map(lambda n: str(n), y_test))))\r\n\r\n\r\nclass SpRNNForAddition(SpRNNForOp):\r\n    def __init__(self):\r\n        super(SpRNNForAddition, self).__init__()\r\n        self._op = ""+""\r\n\r\n\r\nclass SpRNNForMultiple(SpRNNForOp):\r\n    def __init__(self):\r\n        super(SpRNNForMultiple, self).__init__()\r\n        self._op = ""*""\r\n\r\n\r\nclass SpOpGenerator(OpGenerator):\r\n    def __init__(self, im, om, n_time_step, random_scale):\r\n        super(SpOpGenerator, self).__init__(im, om, n_time_step, random_scale)\r\n        self._base = round(self._om ** (1 / (n_time_step + random_scale)))\r\n\r\n    def gen(self, batch_size, test=False, boost=0):\r\n        if boost:\r\n            n_time_step = self._n_time_step + self._random_scale + random.randint(1, boost)\r\n        else:\r\n            n_time_step = self._n_time_step + random.randint(0, self._random_scale)\r\n        x = np.empty([batch_size, n_time_step, self._im])\r\n        y = np.zeros(batch_size, dtype=np.int32)\r\n        for i in range(batch_size):\r\n            targets = self._gen_targets(n_time_step)\r\n            sequences = [self._gen_seq(n_time_step, tar) for tar in targets]\r\n            for j in range(self._im):\r\n                x[i, ..., j] = sequences[j]\r\n            y[i] = self._op(targets)\r\n        return x, y\r\n\r\n\r\nclass SpAdditionGenerator(SpOpGenerator):\r\n    def _op(self, seq):\r\n        return sum(seq)\r\n\r\n    def _gen_targets(self, n_time_step):\r\n        return [int(random.randint(0, self._base ** n_time_step - 1) / self._im) for _ in range(self._im)]\r\n\r\n\r\nclass SpMultipleGenerator(SpOpGenerator):\r\n    def _op(self, seq):\r\n        return np.prod(seq)\r\n\r\n    def _gen_targets(self, n_time_step):\r\n        return [int(random.randint(0, self._base ** n_time_step - 1) ** (1 / self._im)) for _ in range(self._im)]\r\n\r\nif __name__ == \'__main__\':\r\n    _random_scale = 0\r\n    _digit_len, _digit_base, _n_digit = 4, 10, 2\r\n    _generator = SpAdditionGenerator(\r\n        _n_digit, _digit_base ** (_digit_len + _random_scale),\r\n        n_time_step=_digit_len, random_scale=_random_scale\r\n    )\r\n    lstm = SpRNNForAddition()\r\n    lstm.fit(\r\n        _n_digit, _digit_base ** (_digit_len + _random_scale), _generator,\r\n        # cell=tf.contrib.rnn.GRUCell,\r\n        # cell=tf.contrib.rnn.LSTMCell,\r\n        # cell=tf.contrib.rnn.BasicRNNCell,\r\n        # cell=tf.contrib.rnn.BasicLSTMCell,\r\n        epoch=100, n_history=2\r\n    )\r\n    lstm.draw_err_logs()\r\n'"
i_Clustering/KMeans.py,0,"b'import os\r\nimport sys\r\nroot_path = os.path.abspath(""../"")\r\nif root_path not in sys.path:\r\n    sys.path.append(root_path)\r\n\r\nimport numpy as np\r\n\r\nfrom Util.Util import DataUtil\r\nfrom Util.Bases import ClassifierBase\r\nfrom Util.ProgressBar import ProgressBar\r\n\r\n\r\nclass KMeans(ClassifierBase):\r\n    def __init__(self, **kwargs):\r\n        super(KMeans, self).__init__(**kwargs)\r\n        self._centers = self._counter = None\r\n\r\n        self._params[""n_clusters""] = kwargs.get(""n_clusters"", 2)\r\n        self._params[""epoch""] = kwargs.get(""epoch"", 1000)\r\n        self._params[""norm""] = kwargs.get(""norm"", ""l2"")\r\n\r\n    def fit(self, x, n_clusters=None, epoch=None, norm=None, animation_params=None):\r\n        if n_clusters is None:\r\n            n_clusters = self._params[""n_clusters""]\r\n        if epoch is None:\r\n            epoch = self._params[""epoch""]\r\n        if norm is not None:\r\n            self._params[""norm""] = norm\r\n        *animation_properties, animation_params = self._get_animation_params(animation_params)\r\n        x = np.atleast_2d(x)\r\n        arange = np.arange(n_clusters)[..., None]\r\n        x_high_dim, labels_cache, counter = x[:, None, ...], None, 0\r\n        self._centers = x[np.random.permutation(len(x))[:n_clusters]]\r\n        bar = ProgressBar(max_value=epoch, name=""KMeans"")\r\n        ims = []\r\n        for i in range(epoch):\r\n            labels = self.predict(x_high_dim, high_dim=True)\r\n            if labels_cache is None:\r\n                labels_cache = labels\r\n            else:\r\n                if np.all(labels_cache == labels):\r\n                    bar.update(epoch)\r\n                    break\r\n                else:\r\n                    labels_cache = labels\r\n            for j, indices in enumerate(labels == arange):\r\n                self._centers[j] = np.average(x[indices], axis=0)\r\n            counter += 1\r\n            animation_params[""extra""] = self._centers\r\n            self._handle_animation(i, x, labels, ims, animation_params, *animation_properties)\r\n            bar.update()\r\n        self._counter = counter\r\n        self._handle_mp4(ims, animation_properties)\r\n\r\n    def predict(self, x, get_raw_results=False, high_dim=False):\r\n        if not high_dim:\r\n            x = x[:, None, ...]\r\n        dis = np.abs(x - self._centers) if self._params[""norm""] == ""l1"" else (x - self._centers) ** 2\r\n        return np.argmin(np.sum(dis, axis=2), axis=1)\r\n\r\nif __name__ == \'__main__\':\r\n    _x, _y = DataUtil.gen_random(size=2000, scale=6)\r\n    k_means = KMeans(n_clusters=8, animation_params={\r\n        ""show"": False, ""mp4"": True, ""period"": 1, ""draw_background"": True\r\n    })\r\n    k_means.fit(_x)\r\n    k_means.visualize2d(_x, _y, dense=400, extra=k_means[""centers""])\r\n    _x, _y = DataUtil.gen_two_clusters()\r\n    k_means = KMeans()\r\n    k_means.fit(_x)\r\n    k_means.visualize2d(_x, _y, dense=400, extra=k_means[""centers""])\r\n    _x, _y = DataUtil.gen_two_clusters(n_dim=3)\r\n    k_means = KMeans()\r\n    k_means.fit(_x)\r\n    k_means.visualize3d(_x, _y, dense=100, extra=k_means[""centers""])\r\n'"
NN/Basic/Layers.py,0,"b'import numba\r\n\r\nfrom NN.Errors import *\r\nfrom NN.Basic.Optimizers import *\r\n\r\n# TODO: Support \'SAME\' padding\r\n\r\n\r\n@numba.jit([\r\n    ""void(int64, int64, int64, int64, float32[:,:,:,:],""\r\n    ""int64, int64, int64, float32[:,:,:,:], float32[:,:,:,:])""\r\n], nopython=True)\r\ndef conv_bp(n, n_filters, out_h, out_w, dx_padded,\r\n            filter_height, filter_width, sd, inner_weight, delta):\r\n    for i in range(n):\r\n        for f in range(n_filters):\r\n            for j in range(out_h):\r\n                for k in range(out_w):\r\n                    for h in range(dx_padded.shape[1]):\r\n                        jsd, ksd = j * sd, k * sd\r\n                        for p in range(filter_height):\r\n                            for q in range(filter_width):\r\n                                dx_padded[i, h, jsd+p, ksd+q] += (\r\n                                    inner_weight[f][h][p][q] * delta[i, f, j, k]\r\n                                )\r\n\r\n\r\n@numba.jit([\r\n    ""void(int64, int64, int64, int64, float32[:,:,:,:], float32[:,:,:,:],""\r\n    ""int64, int64, int64, int32[:,:,:,:,:])""\r\n], nopython=True)\r\ndef max_pool(n, n_channels, out_h, out_w, x, out,\r\n             pool_height, pool_width, sd, pos_cache):\r\n    for i in range(n):\r\n        for j in range(n_channels):\r\n            for k in range(out_h):\r\n                for l in range(out_w):\r\n                    ksd, lsd = k * sd, l * sd\r\n                    _max = x[i, j, ksd, lsd]\r\n                    pos = (0, 0)\r\n                    for p in range(pool_height):\r\n                        for q in range(pool_width):\r\n                            if x[i, j, ksd+p, lsd+q] > _max:\r\n                                _max = x[i, j, ksd+p, lsd+q]\r\n                                pos = (p, q)\r\n                    pos_cache[i, j, k, l] = pos\r\n                    out[i, j, k, l] = _max\r\n\r\n\r\n@numba.jit([\r\n    ""void(int64, int64, int64, int64, int64, float32[:,:,:,:], float32[:,:,:,:], int32[:,:,:,:,:])""\r\n], nopython=True)\r\ndef max_pool_bp(n, n_channels, out_h, out_w, sd, dx, delta, pos_cache):\r\n    for i in range(n):\r\n        for j in range(n_channels):\r\n            for k in range(out_h):\r\n                for l in range(out_w):\r\n                    ksd, lsd = k * sd, l * sd\r\n                    pos = pos_cache[i, j, k, l]\r\n                    dx[i, j, ksd+pos[0], lsd+pos[1]] = delta[i, j, k, l]\r\n\r\n\r\n# Abstract Layers\r\n\r\nclass Layer:\r\n    LayerTiming = Timing()\r\n\r\n    def __init__(self, shape):\r\n        """"""\r\n        :param shape: shape[0] = units of previous layer\r\n                      shape[1] = units of current layer (self)\r\n        """"""\r\n        self._shape = shape\r\n        self.parent = None\r\n        self.child = None\r\n        self.is_fc = False\r\n        self.is_fc_base = False\r\n        self.is_sub_layer = False\r\n        self._last_sub_layer = None\r\n\r\n    def __str__(self):\r\n        return self.__class__.__name__\r\n\r\n    def __repr__(self):\r\n        return str(self)\r\n\r\n    @property\r\n    def name(self):\r\n        return str(self)\r\n\r\n    @property\r\n    def shape(self):\r\n        return self._shape\r\n\r\n    @shape.setter\r\n    def shape(self, value):\r\n        self._shape = value\r\n\r\n    @property\r\n    def params(self):\r\n        return self._shape,\r\n\r\n    @property\r\n    def special_params(self):\r\n        return\r\n\r\n    def set_special_params(self, dic):\r\n        for key, value in dic.items():\r\n            setattr(self, key, value)\r\n\r\n    @property\r\n    def root(self):\r\n        return self\r\n\r\n    @root.setter\r\n    def root(self, value):\r\n        raise BuildLayerError(""Setting Layer\'s root is not permitted"")\r\n\r\n    @property\r\n    def last_sub_layer(self):\r\n        child = self.child\r\n        if not child:\r\n            return None\r\n        while child.child:\r\n            child = child.child\r\n        return child\r\n\r\n    @last_sub_layer.setter\r\n    def last_sub_layer(self, value):\r\n            self._last_sub_layer = value\r\n\r\n    # Core\r\n\r\n    def derivative(self, y, delta=None):\r\n        return self._derivative(y, delta)\r\n\r\n    @LayerTiming.timeit(level=1, prefix=""[Core] "")\r\n    def activate(self, x, w, bias=None, predict=False):\r\n        if self.is_fc:\r\n            x = x.reshape(x.shape[0], -1)\r\n        if self.is_sub_layer:\r\n            if bias is None:\r\n                return self._activate(x, predict)\r\n            return self._activate(x + bias, predict)\r\n        if bias is None:\r\n            return self._activate(x.dot(w), predict)\r\n        return self._activate(x.dot(w) + bias, predict)\r\n\r\n    @LayerTiming.timeit(level=1, prefix=""[Core] "")\r\n    def bp(self, y, w, prev_delta):\r\n        if self.child is not None and isinstance(self.child, SubLayer):\r\n            if not isinstance(self, SubLayer):\r\n                return prev_delta\r\n            return self._derivative(y, prev_delta)\r\n        if isinstance(self, SubLayer):\r\n            return self._derivative(y, prev_delta.dot(w.T) * self._root.derivative(y))\r\n        return prev_delta.dot(w.T) * self._derivative(y)\r\n\r\n    def _activate(self, x, predict):\r\n        pass\r\n\r\n    def _derivative(self, y, delta=None):\r\n        pass\r\n\r\n    # Util\r\n\r\n    @staticmethod\r\n    @LayerTiming.timeit(level=2, prefix=""[Core Util] "")\r\n    def safe_exp(y):\r\n        return np.exp(y - np.max(y, axis=1, keepdims=True))\r\n\r\n\r\nclass SubLayer(Layer):\r\n    def __init__(self, parent, shape):\r\n        super(SubLayer, self).__init__(shape)\r\n        self.parent = parent\r\n        parent.child = self\r\n        self._root = None\r\n        self.description = """"\r\n\r\n    @property\r\n    def root(self):\r\n        parent = self.parent\r\n        while parent.parent:\r\n            parent = parent.parent\r\n        return parent\r\n\r\n    @root.setter\r\n    def root(self, value):\r\n        self._root = value\r\n\r\n    def get_params(self):\r\n        pass\r\n\r\n    @property\r\n    def params(self):\r\n        return self.get_params()\r\n\r\n    def _activate(self, x, predict):\r\n        raise NotImplementedError(""Please implement activation function for "" + self.name)\r\n\r\n    def _derivative(self, y, delta=None):\r\n        raise NotImplementedError(""Please implement derivative function for "" + self.name)\r\n\r\n\r\nclass ConvLayer(Layer):\r\n    LayerTiming = Timing()\r\n\r\n    def __init__(self, shape, stride=1, padding=0, parent=None):\r\n        """"""\r\n        :param shape:    shape[0] = shape of previous layer           c x h x w\r\n                         shape[1] = shape of current layer\'s weight   f x c x h x w\r\n        :param stride:   stride\r\n        :param padding:  zero-padding\r\n        """"""\r\n        if parent is not None:\r\n            parent = parent.root if parent.is_sub_layer else parent\r\n            shape = parent.shape\r\n        Layer.__init__(self, shape)\r\n        self._stride, self._padding = stride, padding\r\n        if len(shape) == 1:\r\n            self.n_channels = self.n_filters = self.out_h = self.out_w = None\r\n        else:\r\n            self.feed_shape(shape)\r\n        self.x_cache = self.x_col_cache = None\r\n        self.inner_weight = None\r\n\r\n    def feed_shape(self, shape):\r\n        self._shape = shape\r\n        self.n_channels, height, width = shape[0]\r\n        self.n_filters, filter_height, filter_width = shape[1]\r\n        full_height, full_width = width + 2 * self._padding, height + 2 * self._padding\r\n        if (\r\n            (full_height - filter_height) % self._stride != 0 or\r\n            (full_width - filter_width) % self._stride != 0\r\n        ):\r\n            raise BuildLayerError(\r\n                ""Weight shape does not work, ""\r\n                ""shape: {} - stride: {} - padding: {} not compatible with {}"".format(\r\n                    self._shape[1][1:], self._stride, self._padding, (height, width)\r\n                ))\r\n        self.out_h = int((height + 2 * self._padding - filter_height) / self._stride) + 1\r\n        self.out_w = int((width + 2 * self._padding - filter_width) / self._stride) + 1\r\n\r\n    @property\r\n    def params(self):\r\n        return self._shape, self._stride, self._padding\r\n\r\n    @property\r\n    def stride(self):\r\n        return self._stride\r\n\r\n    @property\r\n    def padding(self):\r\n        return self._padding\r\n\r\n    def _activate(self, x, predict):\r\n        raise NotImplementedError(""Please implement activation function for "" + self.name)\r\n\r\n    def _derivative(self, y, delta=None):\r\n        raise NotImplementedError(""Please implement derivative function for "" + self.name)\r\n\r\n\r\nclass ConvPoolLayer(ConvLayer):\r\n    LayerTiming = Timing()\r\n\r\n    def __init__(self, shape, stride=1, padding=0):\r\n        """"""\r\n        :param shape:    shape[0] = shape of previous layer           c x h x w\r\n                         shape[1] = shape of pool window              c x ph x pw\r\n        :param stride:   stride\r\n        :param padding:  zero-padding\r\n        """"""\r\n        ConvLayer.__init__(self, shape, stride, padding)\r\n        self._pool_cache = {}\r\n\r\n    @property\r\n    def params(self):\r\n        return (self._shape[0], self._shape[1][1:]), self._stride, self._padding\r\n\r\n    def feed_shape(self, shape):\r\n        if len(shape[1]) == 2:\r\n            shape = (shape[0], (shape[0][0], *shape[1]))\r\n        ConvLayer.feed_shape(self, shape)\r\n\r\n    @LayerTiming.timeit(level=1, prefix=""[Core] "")\r\n    def activate(self, x, w, bias=None, predict=False):\r\n        return self._activate(x, w, bias, predict)\r\n\r\n    def _activate(self, x, *args):\r\n        raise NotImplementedError(""Please implement activation function for "" + self.name)\r\n\r\n    def _derivative(self, y, *args):\r\n        raise NotImplementedError(""Please implement derivative function for "" + self.name)\r\n\r\n    @LayerTiming.timeit(level=1, prefix=""[Core] "")\r\n    def bp(self, y, w, prev_delta):\r\n        return self._derivative(y, w, prev_delta)\r\n\r\n\r\n# noinspection PyProtectedMember\r\nclass ConvMeta(type):\r\n    def __new__(mcs, *args, **kwargs):\r\n        name, bases, attr = args[:3]\r\n        conv_layer, layer = bases\r\n\r\n        def __init__(self, shape, stride=1, padding=0):\r\n            conv_layer.__init__(self, shape, stride, padding)\r\n\r\n        def _activate(self, x, w, bias, predict):\r\n            self.x_cache, self.inner_weight = x, w\r\n            n, n_channels, height, width = x.shape\r\n            n_filters, _, filter_height, filter_width = w.shape\r\n\r\n            p, sd = self._padding, self._stride\r\n            x_padded = np.pad(x, ((0, 0), (0, 0), (p, p), (p, p)), mode=\'constant\')\r\n\r\n            height += 2 * p\r\n            width += 2 * p\r\n\r\n            shape = (n_channels, filter_height, filter_width, n, self.out_h, self.out_w)\r\n            strides = (height * width, width, 1, n_channels * height * width, sd * width, sd)\r\n            strides = x.itemsize * np.asarray(strides)\r\n            x_cols = np.lib.stride_tricks.as_strided(x_padded, shape=shape, strides=strides).reshape(\r\n                n_channels * filter_height * filter_width, n * self.out_h * self.out_w)\r\n            self.x_col_cache = x_cols\r\n\r\n            if bias is None:\r\n                res = w.reshape(n_filters, -1).dot(x_cols)\r\n            else:\r\n                res = w.reshape(n_filters, -1).dot(x_cols) + bias.reshape(-1, 1)\r\n            res.shape = (n_filters, n, self.out_h, self.out_w)\r\n            return layer._activate(self, res.transpose(1, 0, 2, 3), predict)\r\n\r\n        def _derivative(self, y, w, prev_delta):\r\n            n = len(y)\r\n            n_channels, height, width = self._shape[0]\r\n            n_filters, filter_height, filter_width = self._shape[1]\r\n\r\n            p, sd = self._padding, self._stride\r\n            if isinstance(prev_delta, tuple):\r\n                prev_delta = prev_delta[0]\r\n\r\n            __derivative = self.LayerTiming.timeit(level=1, func_name=""bp"", cls_name=name, prefix=""[Core] "")(\r\n                layer._derivative)\r\n            if self.is_fc_base:\r\n                delta = __derivative(self, y) * prev_delta.dot(w.T).reshape(y.shape)\r\n            else:\r\n                delta = __derivative(self, y) * prev_delta\r\n\r\n            dw = delta.transpose(1, 0, 2, 3).reshape(n_filters, -1).dot(\r\n                self.x_col_cache.T).reshape(self.inner_weight.shape)\r\n            db = np.sum(delta, axis=(0, 2, 3))\r\n\r\n            n_filters, _, filter_height, filter_width = self.inner_weight.shape\r\n            *_, out_h, out_w = delta.shape\r\n\r\n            dx_padded = np.zeros((n, n_channels, height + 2 * p, width + 2 * p), dtype=np.float32)\r\n            conv_bp(\r\n                n, n_filters, out_h, out_w, dx_padded,\r\n                filter_height, filter_width, sd, self.inner_weight, delta\r\n            )\r\n            dx = dx_padded[..., p:-p, p:-p] if p > 0 else dx_padded\r\n            return dx, dw, db\r\n\r\n        def activate(self, x, w, bias=None, predict=False):\r\n            return self.LayerTiming.timeit(level=1, func_name=""activate"", cls_name=name, prefix=""[Core] "")(\r\n                _activate)(self, x, w, bias, predict)\r\n\r\n        def bp(self, y, w, prev_delta):\r\n            return self.LayerTiming.timeit(level=1, func_name=""bp"", cls_name=name, prefix=""[Core] "")(\r\n                _derivative)(self, y, w, prev_delta)\r\n\r\n        for key, value in locals().items():\r\n            if str(value).find(""function"") >= 0 or str(value).find(""property""):\r\n                attr[key] = value\r\n\r\n        return type(name, bases, attr)\r\n\r\n\r\n# noinspection PyProtectedMember\r\nclass ConvSubMeta(type):\r\n    def __new__(mcs, *args, **kwargs):\r\n        name, bases, attr = args[:3]\r\n        conv_layer, sub_layer = bases\r\n\r\n        def __init__(self, parent, shape, *_args, **_kwargs):\r\n            conv_layer.__init__(self, None, parent=parent, **_kwargs)\r\n            self.out_h, self.out_w = parent.out_h, parent.out_w\r\n            sub_layer.__init__(self, parent, shape, *_args, **_kwargs)\r\n            self._shape = ((shape[0][0], self.out_h, self.out_w), shape[0])\r\n            if name == ""ConvNorm"":\r\n                self.gamma = np.ones(self.n_filters, dtype=np.float32)\r\n                self.beta = np.ones(self.n_filters, dtype=np.float32)\r\n                self.init_optimizers()\r\n\r\n        def _activate(self, x, predict):\r\n            n, n_channels, height, width = x.shape\r\n            out = sub_layer._activate(self, x.transpose(0, 2, 3, 1).reshape(-1, n_channels), predict)\r\n            return out.reshape(n, height, width, n_channels).transpose(0, 3, 1, 2)\r\n\r\n        def _derivative(self, y, w, delta=None):\r\n            if self.is_fc_base:\r\n                delta = delta.dot(w.T).reshape(y.shape)\r\n            n, n_channels, height, width = delta.shape\r\n            dx = sub_layer._derivative(self, y, delta.transpose(0, 2, 3, 1).reshape(-1, n_channels))\r\n            return dx.reshape(n, height, width, n_channels).transpose(0, 3, 1, 2)\r\n\r\n        # noinspection PyUnusedLocal\r\n        def activate(self, x, w, bias=None, predict=False):\r\n            return self.LayerTiming.timeit(level=1, func_name=""activate"", cls_name=name, prefix=""[Core] "")(\r\n                _activate)(self, x, predict)\r\n\r\n        def bp(self, y, w, prev_delta):\r\n            if isinstance(prev_delta, tuple):\r\n                prev_delta = prev_delta[0]\r\n            return self.LayerTiming.timeit(level=1, func_name=""bp"", cls_name=name, prefix=""[Core] "")(\r\n                _derivative)(self, y, w, prev_delta)\r\n\r\n        @property\r\n        def params(self):\r\n            return sub_layer.get_params(self)\r\n\r\n        for key, value in locals().items():\r\n            if str(value).find(""function"") >= 0 or str(value).find(""property""):\r\n                attr[key] = value\r\n\r\n        return type(name, bases, attr)\r\n\r\n\r\n# Activation Layers\r\n\r\nclass Tanh(Layer):\r\n    def _activate(self, x, predict):\r\n        return np.tanh(x)\r\n\r\n    def _derivative(self, y, delta=None):\r\n        return 1 - y ** 2\r\n\r\n\r\nclass Sigmoid(Layer):\r\n    def _activate(self, x, predict):\r\n        return 1 / (1 + np.exp(-x))\r\n\r\n    def _derivative(self, y, delta=None):\r\n        return y * (1 - y)\r\n\r\n\r\nclass ELU(Layer):\r\n    def _activate(self, x, predict):\r\n        rs, mask = x.copy(), x < 0\r\n        rs[mask] = np.exp(rs[mask]) - 1\r\n        return rs\r\n\r\n    def _derivative(self, y, delta=None):\r\n        _rs, _indices = np.ones(y.shape), y < 0\r\n        _rs[_indices] = y[_indices] + 1\r\n        return _rs\r\n\r\n\r\nclass ReLU(Layer):\r\n    def _activate(self, x, predict):\r\n        return np.maximum(0, x)\r\n\r\n    def _derivative(self, y, delta=None):\r\n        return y > 0\r\n\r\n\r\nclass Softplus(Layer):\r\n    def _activate(self, x, predict):\r\n        return np.log(1 + np.exp(x))\r\n\r\n    def _derivative(self, y, delta=None):\r\n        return 1 - 1 / np.exp(y)\r\n\r\n\r\nclass Identical(Layer):\r\n    def _activate(self, x, predict):\r\n        return x\r\n\r\n    def _derivative(self, y, delta=None):\r\n        return 1\r\n\r\n\r\n# Convolution Layers\r\n\r\nclass ConvTanh(ConvLayer, Tanh, metaclass=ConvMeta):\r\n    pass\r\n\r\n\r\nclass ConvSigmoid(ConvLayer, Sigmoid, metaclass=ConvMeta):\r\n    pass\r\n\r\n\r\nclass ConvELU(ConvLayer, ELU, metaclass=ConvMeta):\r\n    pass\r\n\r\n\r\nclass ConvReLU(ConvLayer, ReLU, metaclass=ConvMeta):\r\n    pass\r\n\r\n\r\nclass ConvSoftplus(ConvLayer, Softplus, metaclass=ConvMeta):\r\n    pass\r\n\r\n\r\nclass ConvIdentical(ConvLayer, Identical, metaclass=ConvMeta):\r\n    pass\r\n\r\n\r\n# Pooling Layers\r\n\r\nclass MaxPool(ConvPoolLayer):\r\n    def _activate(self, x, *args):\r\n        self.x_cache = x\r\n        sd = self._stride\r\n        n, n_channels, height, width = x.shape\r\n        # noinspection PyTupleAssignmentBalance\r\n        _, pool_height, pool_width = self._shape[1]\r\n        same_size = pool_height == pool_width == sd\r\n        tiles = height % pool_height == 0 and width % pool_width == 0\r\n        if same_size and tiles:\r\n            x_reshaped = x.reshape(n, n_channels, int(height / pool_height), pool_height,\r\n                                   int(width / pool_width), pool_width)\r\n            self._pool_cache[""x_reshaped""] = x_reshaped\r\n            out = x_reshaped.max(axis=3).max(axis=4)\r\n            self._pool_cache[""method""] = ""reshape""\r\n        else:\r\n            out = np.zeros((n, n_channels, self.out_h, self.out_w), dtype=np.float32)\r\n            pos_cache = np.zeros((n, n_channels, self.out_h, self.out_w, 2), dtype=np.int32)\r\n            max_pool(\r\n                n, n_channels, self.out_h, self.out_w, x, out,\r\n                pool_height, pool_width, sd, pos_cache\r\n            )\r\n            self._pool_cache[""method""] = ""original""\r\n            self._pool_cache[""pos_cache""] = pos_cache\r\n        return out\r\n\r\n    def _derivative(self, y, *args):\r\n        w, prev_delta = args\r\n        if isinstance(prev_delta, tuple):\r\n            prev_delta = prev_delta[0]\r\n        if self.is_fc_base:\r\n            delta = prev_delta.dot(w.T).reshape(y.shape)\r\n        else:\r\n            delta = prev_delta\r\n        method = self._pool_cache[""method""]\r\n        if method == ""reshape"":\r\n            x_reshaped_cache = self._pool_cache[""x_reshaped""]\r\n            dx_reshaped = np.zeros_like(x_reshaped_cache)\r\n            out_newaxis = y[..., None, :, None]\r\n            mask = (x_reshaped_cache == out_newaxis)  # type: np.ndarray\r\n            dout_newaxis = delta[..., None, :, None]\r\n            dout_broadcast, _ = np.broadcast_arrays(dout_newaxis, dx_reshaped)\r\n            dx_reshaped[mask] = dout_broadcast[mask]\r\n            dx_reshaped /= np.sum(mask, axis=(3, 5), keepdims=True)\r\n            dx = dx_reshaped.reshape(self.x_cache.shape)\r\n        elif method == ""original"":\r\n            sd = self._stride\r\n            dx = np.zeros_like(self.x_cache)\r\n            n, n_channels, *_ = self.x_cache.shape\r\n            max_pool_bp(\r\n                n, n_channels, self.out_h, self.out_w, sd, dx, delta, self._pool_cache[""pos_cache""]\r\n            )\r\n        else:\r\n            raise LayerError(""Undefined pooling method \'{}\' found"".format(method))\r\n        return dx, None, None\r\n\r\n\r\n# Special Layer\r\n\r\nclass Dropout(SubLayer):\r\n    def __init__(self, parent, shape, keep_prob=0.5):\r\n        if keep_prob < 0 or keep_prob >= 1:\r\n            raise BuildLayerError(""Probability of Dropout should be a positive float smaller than 1"")\r\n        SubLayer.__init__(self, parent, shape)\r\n        self._mask = None\r\n        self._prob = keep_prob\r\n        self._prob_inv = 1 / keep_prob\r\n        self.description = ""(Keep prob: {})"".format(keep_prob)\r\n\r\n    def get_params(self):\r\n        return self._prob,\r\n\r\n    def _activate(self, x, predict):\r\n        if not predict:\r\n            # noinspection PyTypeChecker\r\n            self._mask = np.random.binomial(\r\n                [np.ones(x.shape)], self._prob\r\n            )[0].astype(np.float32) * self._prob_inv\r\n            return x * self._mask\r\n        return x\r\n\r\n    def _derivative(self, y, delta=None):\r\n        return delta * self._mask\r\n\r\n\r\nclass Normalize(SubLayer):\r\n    def __init__(self, parent, shape, lr=0.001, eps=1e-8, momentum=0.9, optimizers=None):\r\n        SubLayer.__init__(self, parent, shape)\r\n        self.sample_mean, self.sample_var = None, None\r\n        self.running_mean, self.running_var = None, None\r\n        self.x_cache, self.x_normalized_cache = None, None\r\n        self._lr, self._eps = lr, eps\r\n        if optimizers is None:\r\n            self._g_optimizer, self._b_optimizer = Adam(self._lr), Adam(self._lr)\r\n        else:\r\n            self._g_optimizer, self._b_optimizer = optimizers\r\n        self.gamma = np.ones(self.shape[1], dtype=np.float32)\r\n        self.beta = np.ones(self.shape[1], dtype=np.float32)\r\n        self._momentum = momentum\r\n        self.init_optimizers()\r\n        self.description = ""(lr: {}, eps: {}, momentum: {}, optimizer: ({}, {}))"".format(\r\n            lr, eps, momentum, self._g_optimizer.name, self._b_optimizer.name\r\n        )\r\n\r\n    def get_params(self):\r\n        return self._lr, self._eps, self._momentum, (self._g_optimizer.name, self._b_optimizer.name)\r\n\r\n    @property\r\n    def special_params(self):\r\n        return {\r\n            ""gamma"": self.gamma, ""beta"": self.beta,\r\n            ""running_mean"": self.running_mean, ""running_var"": self.running_var,\r\n            ""_g_optimizer"": self._g_optimizer, ""_b_optimizer"": self._b_optimizer\r\n        }\r\n\r\n    def init_optimizers(self):\r\n        _opt_fac = OptFactory()\r\n        if not isinstance(self._g_optimizer, Optimizer):\r\n            self._g_optimizer = _opt_fac.get_optimizer_by_name(\r\n                self._g_optimizer, None, self._lr, None\r\n            )\r\n        if not isinstance(self._b_optimizer, Optimizer):\r\n            self._b_optimizer = _opt_fac.get_optimizer_by_name(\r\n                self._b_optimizer, None, self._lr, None\r\n            )\r\n        self._g_optimizer.feed_variables([self.gamma])\r\n        self._b_optimizer.feed_variables([self.beta])\r\n\r\n    # noinspection PyTypeChecker\r\n    def _activate(self, x, predict):\r\n        if self.running_mean is None or self.running_var is None:\r\n            self.running_mean = np.zeros(x.shape[1], dtype=np.float32)\r\n            self.running_var = np.zeros(x.shape[1], dtype=np.float32)\r\n        if not predict:\r\n            self.sample_mean = np.mean(x, axis=0, keepdims=True)\r\n            self.sample_var = np.var(x, axis=0, keepdims=True)\r\n            x_normalized = (x - self.sample_mean) / np.sqrt(self.sample_var + self._eps)\r\n            self.x_cache, self.x_normalized_cache = x, x_normalized\r\n            out = self.gamma * x_normalized + self.beta\r\n            self.running_mean = self._momentum * self.running_mean + (1 - self._momentum) * self.sample_mean\r\n            self.running_var = self._momentum * self.running_var + (1 - self._momentum) * self.sample_var\r\n        else:\r\n            x_normalized = (x - self.running_mean) / np.sqrt(self.running_var + self._eps)\r\n            out = self.gamma * x_normalized + self.beta\r\n        return out\r\n\r\n    def _derivative(self, y, delta=None):\r\n        n, d = self.x_cache.shape\r\n        dx_normalized = delta * self.gamma\r\n        x_mu = self.x_cache - self.sample_mean\r\n        sample_std_inv = 1.0 / np.sqrt(self.sample_var + self._eps)\r\n        ds_var = -0.5 * np.sum(dx_normalized * x_mu, axis=0, keepdims=True) * sample_std_inv ** 3\r\n        ds_mean = (-1.0 * np.sum(dx_normalized * sample_std_inv, axis=0, keepdims=True) - 2.0 *\r\n                   ds_var * np.mean(x_mu, axis=0, keepdims=True))\r\n        dx1 = dx_normalized * sample_std_inv\r\n        dx2 = 2.0 / n * ds_var * x_mu\r\n        dx = dx1 + dx2 + 1.0 / n * ds_mean\r\n        dg = np.sum(delta * self.x_normalized_cache, axis=0)\r\n        db = np.sum(delta, axis=0)\r\n        self.gamma += self._g_optimizer.run(0, dg)\r\n        self.beta += self._b_optimizer.run(0, db)\r\n        self._g_optimizer.update()\r\n        self._b_optimizer.update()\r\n        return dx\r\n\r\n\r\nclass ConvDrop(ConvLayer, Dropout, metaclass=ConvSubMeta):\r\n    pass\r\n\r\n\r\nclass ConvNorm(ConvLayer, Normalize, metaclass=ConvSubMeta):\r\n    pass\r\n\r\n\r\n# Cost Layer\r\n\r\nclass CostLayer(Layer):\r\n    def __init__(self, shape, cost_function=""MSE"", transform=None):\r\n        super(CostLayer, self).__init__(shape)\r\n        self._available_cost_functions = {\r\n            ""MSE"": CostLayer._mse,\r\n            ""SVM"": CostLayer._svm,\r\n            ""CrossEntropy"": CostLayer._cross_entropy\r\n        }\r\n        self._available_transform_functions = {\r\n            ""Softmax"": CostLayer._softmax,\r\n            ""Sigmoid"": CostLayer._sigmoid\r\n        }\r\n        if cost_function not in self._available_cost_functions:\r\n            raise LayerError(""Cost function \'{}\' not implemented"".format(cost_function))\r\n        self._cost_function_name = cost_function\r\n        self._cost_function = self._available_cost_functions[cost_function]\r\n        if transform is None and cost_function == ""CrossEntropy"":\r\n            self._transform = ""Softmax""\r\n            self._transform_function = CostLayer._softmax\r\n        else:\r\n            self._transform = transform\r\n            self._transform_function = self._available_transform_functions.get(transform, None)\r\n\r\n    def __str__(self):\r\n        return self._cost_function_name\r\n\r\n    def _activate(self, x, predict):\r\n        if self._transform_function is None:\r\n            return x\r\n        return self._transform_function(x)\r\n\r\n    def _derivative(self, y, delta=None):\r\n        raise LayerError(""derivative function should not be called in CostLayer"")\r\n\r\n    def bp_first(self, y, y_pred):\r\n        if self._cost_function_name == ""CrossEntropy"" and (\r\n                self._transform == ""Softmax"" or self._transform == ""Sigmoid""):\r\n            return y - y_pred\r\n        dy = -self._cost_function(y, y_pred)\r\n        if self._transform_function is None:\r\n            return dy\r\n        return dy * self._transform_function(y_pred, diff=True)\r\n\r\n    @property\r\n    def calculate(self):\r\n        return lambda y, y_pred: self._cost_function(y, y_pred, False)\r\n\r\n    @property\r\n    def cost_function(self):\r\n        return self._cost_function_name\r\n\r\n    @cost_function.setter\r\n    def cost_function(self, value):\r\n        if value not in self._available_cost_functions:\r\n            raise LayerError(""\'{}\' is not implemented"".format(value))\r\n        self._cost_function_name = value\r\n        self._cost_function = self._available_cost_functions[value]\r\n\r\n    def set_cost_function_derivative(self, func, name=None):\r\n        name = ""Custom Cost Function"" if name is None else name\r\n        self._cost_function_name = name\r\n        self._cost_function = func\r\n\r\n    # Transform Functions\r\n\r\n    @staticmethod\r\n    def _softmax(y, diff=False):\r\n        if diff:\r\n            return y * (1 - y)\r\n        exp_y = CostLayer.safe_exp(y)\r\n        return exp_y / np.sum(exp_y, axis=1, keepdims=True)\r\n\r\n    @staticmethod\r\n    def _sigmoid(y, diff=False):\r\n        if diff:\r\n            return y * (1 - y)\r\n        return 1 / (1 + np.exp(-y))\r\n\r\n    # Cost Functions\r\n\r\n    @staticmethod\r\n    def _mse(y, y_pred, diff=True):\r\n        if diff:\r\n            return -y + y_pred\r\n        return 0.5 * np.average((y - y_pred) ** 2)\r\n\r\n    @staticmethod\r\n    def _svm(y, y_pred, diff=True):\r\n        n, y = y_pred.shape[0], np.argmax(y, axis=1)\r\n        correct_class_scores = y_pred[np.arange(n), y]\r\n        margins = np.maximum(0, y_pred - correct_class_scores[..., None] + 1.0)\r\n        margins[np.arange(n), y] = 0\r\n        loss = np.sum(margins) / n\r\n        num_pos = np.sum(margins > 0, axis=1)\r\n        if not diff:\r\n            return loss\r\n        dx = np.zeros_like(y_pred)\r\n        dx[margins > 0] = 1\r\n        dx[np.arange(n), y] -= num_pos\r\n        dx /= n\r\n        return dx\r\n\r\n    @staticmethod\r\n    def _cross_entropy(y, y_pred, diff=True):\r\n        if diff:\r\n            return -y / y_pred + (1 - y) / (1 - y_pred)\r\n        # noinspection PyTypeChecker\r\n        return np.average(-y * np.log(np.maximum(y_pred, 1e-12)) - (1 - y) * np.log(np.maximum(1 - y_pred, 1e-12)))\r\n\r\n    \r\n# Factory\r\n\r\nclass LayerFactory:\r\n    available_root_layers = {\r\n        ""Tanh"": Tanh, ""Sigmoid"": Sigmoid,\r\n        ""ELU"": ELU, ""ReLU"": ReLU, ""Softplus"": Softplus,\r\n        ""Identical"": Identical,\r\n        ""ConvTanh"": ConvTanh, ""ConvSigmoid"": ConvSigmoid,\r\n        ""ConvELU"": ConvELU, ""ConvReLU"": ConvReLU, ""ConvSoftplus"": ConvSoftplus,\r\n        ""ConvIdentical"": ConvIdentical,\r\n        ""MaxPool"": MaxPool,\r\n        ""MSE"": CostLayer, ""SVM"": CostLayer, ""CrossEntropy"": CostLayer\r\n    }\r\n    available_sub_layers = {\r\n        ""Dropout"", ""Normalize"", ""ConvNorm"", ""ConvDrop""\r\n    }\r\n    available_cost_functions = {\r\n        ""MSE"", ""SVM"", ""CrossEntropy""\r\n    }\r\n    available_special_layers = {\r\n        ""Dropout"": Dropout,\r\n        ""Normalize"": Normalize,\r\n        ""ConvDrop"": ConvDrop,\r\n        ""ConvNorm"": ConvNorm\r\n    }\r\n    special_layer_default_params = {\r\n        ""Dropout"": (0.5, ),\r\n        ""Normalize"": (0.001, 1e-8, 0.9),\r\n        ""ConvDrop"": (0.5, ),\r\n        ""ConvNorm"": (0.001, 1e-8, 0.9)\r\n    }\r\n\r\n    def get_root_layer_by_name(self, name, *args, **kwargs):\r\n        if name not in self.available_sub_layers:\r\n            if name in self.available_root_layers:\r\n                if name in self.available_cost_functions:\r\n                    kwargs[""cost_function""] = name\r\n                name = self.available_root_layers[name]\r\n            else:\r\n                raise BuildNetworkError(""Undefined layer \'{}\' found"".format(name))\r\n            return name(*args, **kwargs)\r\n        return None\r\n    \r\n    def get_layer_by_name(self, name, parent, current_dimension, *args, **kwargs):\r\n        _layer = self.get_root_layer_by_name(name, *args, **kwargs)\r\n        if _layer:\r\n            return _layer, None\r\n        _current, _next = parent.shape[1], current_dimension\r\n        layer_param = self.special_layer_default_params[name]\r\n        _layer = self.available_special_layers[name]\r\n        if args or kwargs:\r\n            _layer = _layer(parent, (_current, _next), *args, **kwargs)\r\n        else:\r\n            _layer = _layer(parent, (_current, _next), *layer_param)\r\n        return _layer, (_current, _next)\r\n'"
NN/Basic/Networks.py,0,"b'import os\r\nimport cv2\r\nimport time\r\nimport pickle\r\nimport matplotlib.pyplot as plt\r\nfrom math import sqrt, ceil\r\n\r\nfrom NN.Basic.Layers import *\r\nfrom NN.Basic.Optimizers import OptFactory\r\n\r\nfrom Util.Util import VisUtil\r\nfrom Util.Bases import ClassifierBase\r\nfrom Util.ProgressBar import ProgressBar\r\n\r\n\r\n# Naive pure numpy version\r\n\r\nclass NNVerbose:\r\n    NONE = 0\r\n    EPOCH = 1\r\n    ITER = 1.5\r\n    METRICS = 2\r\n    METRICS_DETAIL = 3\r\n    DETAIL = 4\r\n    DEBUG = 5\r\n\r\n\r\nclass NNConfig:\r\n    BOOST_LESS_SAMPLES = False\r\n    TRAINING_SCALE = 5 / 6\r\n\r\n\r\n# Neural Network\r\n\r\nclass NNDist(ClassifierBase):\r\n    NNTiming = Timing()\r\n\r\n    def __init__(self, **kwargs):\r\n        super(NNDist, self).__init__(**kwargs)\r\n        self._layers, self._weights, self._bias = [], [], []\r\n        self._layer_names, self._layer_shapes, self._layer_params = [], [], []\r\n        self._lr, self._epoch, self._regularization_param = 0, 0, 0\r\n        self._w_optimizer, self._b_optimizer, self._optimizer_name = None, None, """"\r\n        self.verbose = 1\r\n\r\n        self._apply_bias = False\r\n        self._current_dimension = 0\r\n\r\n        self._logs = {}\r\n        self._metrics, self._metric_names = [], []\r\n\r\n        self._x_min, self._x_max = 0, 0\r\n        self._y_min, self._y_max = 0, 0\r\n\r\n        self._layer_factory = LayerFactory()\r\n        self._optimizer_factory = OptFactory()\r\n\r\n        self._available_metrics = {\r\n            ""acc"": self.acc, ""_acc"": self.acc,\r\n            ""f1"": self.f1_score, ""_f1_score"": self.f1_score\r\n        }\r\n\r\n    @NNTiming.timeit(level=4, prefix=""[Initialize] "")\r\n    def initialize(self):\r\n        self._layers, self._weights, self._bias = [], [], []\r\n        self._layer_names, self._layer_shapes, self._layer_params = [], [], []\r\n        self._lr, self._epoch, self._regularization_param = 0, 0, 0\r\n        self._w_optimizer, self._b_optimizer, self._optimizer_name = None, None, """"\r\n        self.verbose = 1\r\n\r\n        self._apply_bias = False\r\n        self._current_dimension = 0\r\n\r\n        self._logs = []\r\n        self._metrics, self._metric_names = [], []\r\n\r\n        self._x_min, self._x_max = 0, 0\r\n        self._y_min, self._y_max = 0, 0\r\n\r\n    # Property\r\n\r\n    @property\r\n    def name(self):\r\n        return (\r\n            ""-"".join([str(_layer.shape[1]) for _layer in self._layers]) +\r\n            "" at {}"".format(time.strftime(""%Y-%m-%d %H:%M:%S"", time.localtime()))\r\n        )\r\n\r\n    @property\r\n    def layer_names(self):\r\n        return [layer.name for layer in self._layers]\r\n\r\n    @layer_names.setter\r\n    def layer_names(self, value):\r\n        self._layer_names = value\r\n\r\n    @property\r\n    def layer_shapes(self):\r\n        return [layer.shape for layer in self._layers]\r\n\r\n    @layer_shapes.setter\r\n    def layer_shapes(self, value):\r\n        self._layer_shapes = value\r\n\r\n    @property\r\n    def layer_params(self):\r\n        return self._layer_params\r\n\r\n    @layer_params.setter\r\n    def layer_params(self, value):\r\n        self.layer_params = value\r\n\r\n    @property\r\n    def layer_special_params(self):\r\n        return [layer.special_params for layer in self._layers]\r\n\r\n    @layer_special_params.setter\r\n    def layer_special_params(self, value):\r\n        for layer, sp_param in zip(self._layers, value):\r\n            if sp_param is not None:\r\n                layer.set_special_params(sp_param)\r\n\r\n    @property\r\n    def optimizer(self):\r\n        return self._optimizer_name\r\n\r\n    @optimizer.setter\r\n    def optimizer(self, value):\r\n        try:\r\n            self._optimizer_name = value\r\n        except KeyError:\r\n            raise BuildNetworkError(""Invalid Optimizer \'{}\' provided"".format(value))\r\n\r\n    # Utils\r\n\r\n    @NNTiming.timeit(level=4)\r\n    def _get_min_max(self, x, y):\r\n        self._x_min, self._x_max = np.min(x), np.max(x)\r\n        self._y_min, self._y_max = np.min(y), np.max(y)\r\n\r\n    @NNTiming.timeit(level=4)\r\n    def _split_data(self, x, y, x_test, y_test,\r\n                    train_only, training_scale=NNConfig.TRAINING_SCALE):\r\n        x, y = np.asarray(x, dtype=np.float32), np.asarray(y, dtype=np.float32)\r\n        if x_test is not None and y_test is not None:\r\n            x_test, y_test = np.asarray(x_test, dtype=np.float32), np.asarray(y_test, dtype=np.float32)\r\n        if train_only:\r\n            if x_test is not None and y_test is not None:\r\n                x, y = np.vstack((x, x_test)), np.vstack((y, y_test))\r\n            x_train, y_train, x_test, y_test = x, y, x, y\r\n        else:\r\n            shuffle_suffix = np.random.permutation(len(x))\r\n            x, y = x[shuffle_suffix], y[shuffle_suffix]\r\n            if x_test is None or y_test is None:\r\n                train_len = int(len(x) * training_scale)\r\n                x_train, y_train = x[:train_len], y[:train_len]\r\n                x_test, y_test = x[train_len:], y[train_len:]\r\n            elif x_test is None or y_test is None:\r\n                raise BuildNetworkError(""Please provide test sets if you want to split data on your own"")\r\n            else:\r\n                x_train, y_train = x, y\r\n        if NNConfig.BOOST_LESS_SAMPLES:\r\n            if y_train.shape[1] != 2:\r\n                raise BuildNetworkError(""It is not permitted to boost less samples in multiple classification"")\r\n            y_train_arg = np.argmax(y_train, axis=1)\r\n            y0 = y_train_arg == 0\r\n            y1 = ~y0\r\n            y_len, y0_len = len(y_train), np.sum(y0)  # type: int\r\n            if y0_len > int(0.5 * y_len):\r\n                y0, y1 = y1, y0\r\n                y0_len = y_len - y0_len\r\n            boost_suffix = np.random.randint(y0_len, size=y_len - y0_len)\r\n            x_train = np.vstack((x_train[y1], x_train[y0][boost_suffix]))\r\n            y_train = np.vstack((y_train[y1], y_train[y0][boost_suffix]))\r\n            shuffle_suffix = np.random.permutation(len(x_train))\r\n            x_train, y_train = x_train[shuffle_suffix], y_train[shuffle_suffix]\r\n        return (x_train, x_test), (y_train, y_test)\r\n\r\n    @NNTiming.timeit(level=4)\r\n    def _add_params(self, shape, conv_channel=None, fc_shape=None):\r\n        if fc_shape is not None:\r\n            self._weights.append(np.random.randn(fc_shape, shape[1]).astype(np.float32))\r\n            self._bias.append(np.zeros((1, shape[1]), dtype=np.float32))\r\n        elif conv_channel is not None:\r\n            if len(shape[1]) <= 2:\r\n                self._weights.append(np.random.randn(\r\n                    conv_channel, conv_channel, shape[1][0], shape[1][1]).astype(np.float32)\r\n                )\r\n            else:\r\n                self._weights.append(np.random.randn(\r\n                    shape[1][0], conv_channel, shape[1][1], shape[1][2]).astype(np.float32)\r\n                )\r\n            self._bias.append(np.zeros((1, shape[1][0]), dtype=np.float32))\r\n        else:\r\n            self._weights.append(np.random.randn(*shape).astype(np.float32))\r\n            self._bias.append(np.zeros((1, shape[1]), dtype=np.float32))\r\n\r\n    @NNTiming.timeit(level=4)\r\n    def _add_layer(self, layer, *args, **kwargs):\r\n        if not self._layers and isinstance(layer, str):\r\n            layer = self._layer_factory.get_root_layer_by_name(layer, *args, **kwargs)\r\n            if layer:\r\n                self.add(layer)\r\n                return\r\n        parent = self._layers[-1]\r\n        if isinstance(parent, CostLayer):\r\n            raise BuildLayerError(""Adding layer after CostLayer is not permitted"")\r\n        if isinstance(layer, str):\r\n            layer, shape = self._layer_factory.get_layer_by_name(\r\n                layer, parent, self._current_dimension, *args, **kwargs\r\n            )\r\n            if shape is None:\r\n                self.add(layer)\r\n                return\r\n            _current, _next = shape\r\n        else:\r\n            _current, _next = args\r\n        if isinstance(layer, SubLayer):\r\n            parent.child = layer\r\n            layer.is_sub_layer = True\r\n            layer.root = layer.root\r\n            layer.root.last_sub_layer = layer\r\n            self.parent = parent\r\n            self._layers.append(layer)\r\n            self._weights.append(np.array([.0]))\r\n            self._bias.append(np.array([.0]))\r\n            self._current_dimension = _next\r\n        else:\r\n            fc_shape, conv_channel, last_layer = None, None, self._layers[-1]\r\n            if isinstance(last_layer, ConvLayer):\r\n                if isinstance(layer, ConvLayer):\r\n                    conv_channel = last_layer.n_filters\r\n                    _current = (conv_channel, last_layer.out_h, last_layer.out_w)\r\n                    layer.feed_shape((_current, _next))\r\n                else:\r\n                    layer.is_fc = True\r\n                    last_layer.is_fc_base = True\r\n                    fc_shape = last_layer.out_h * last_layer.out_w * last_layer.n_filters\r\n            self._layers.append(layer)\r\n            self._add_params((_current, _next), conv_channel, fc_shape)\r\n            self._current_dimension = _next\r\n        self._update_layer_information(layer)\r\n\r\n    @NNTiming.timeit(level=4)\r\n    def _update_layer_information(self, layer):\r\n        self._layer_params.append(layer.params)\r\n        if len(self._layer_params) > 1 and not layer.is_sub_layer:\r\n            self._layer_params[-1] = ((self._layer_params[-1][0][1],), *self._layer_params[-1][1:])\r\n\r\n    @NNTiming.timeit(level=1)\r\n    def _get_prediction(self, x, name=None, batch_size=1e6, verbose=None):\r\n        if verbose is None:\r\n            verbose = self.verbose\r\n        fc_shape = np.prod(x.shape[1:])  # type: int\r\n        single_batch = int(batch_size / fc_shape)\r\n        if not single_batch:\r\n            single_batch = 1\r\n        if single_batch >= len(x):\r\n            return self._get_activations(x, predict=True).pop()\r\n        epoch = int(len(x) / single_batch)\r\n        if not len(x) % single_batch:\r\n            epoch += 1\r\n        name = ""Prediction"" if name is None else ""Prediction ({})"".format(name)\r\n        sub_bar = ProgressBar(max_value=epoch, name=name, start=False)\r\n        if verbose >= NNVerbose.METRICS:\r\n            sub_bar.start()\r\n        rs, count = [self._get_activations(x[:single_batch], predict=True).pop()], single_batch\r\n        if verbose >= NNVerbose.METRICS:\r\n            sub_bar.update()\r\n        while count < len(x):\r\n            count += single_batch\r\n            if count >= len(x):\r\n                rs.append(self._get_activations(x[count-single_batch:], predict=True).pop())\r\n            else:\r\n                rs.append(self._get_activations(x[count-single_batch:count], predict=True).pop())\r\n            if verbose >= NNVerbose.METRICS:\r\n                sub_bar.update()\r\n        return np.vstack(rs)\r\n\r\n    @NNTiming.timeit(level=1)\r\n    def _get_activations(self, x, predict=False):\r\n        activations = [self._layers[0].activate(x, self._weights[0], self._bias[0], predict)]\r\n        for i, layer in enumerate(self._layers[1:]):\r\n            activations.append(layer.activate(\r\n                activations[-1], self._weights[i + 1], self._bias[i + 1], predict))\r\n        return activations\r\n\r\n    @NNTiming.timeit(level=3)\r\n    def _append_log(self, x, y, name, get_loss=True):\r\n        y_pred = self._get_prediction(x, name)\r\n        for i, metric in enumerate(self._metrics):\r\n            self._logs[name][i].append(metric(\r\n                np.argmax(y, axis=1), np.argmax(y_pred, axis=1)\r\n            ))\r\n        if get_loss:\r\n            self._logs[name][-1].append(self._layers[-1].calculate(y, y_pred) / len(y))\r\n\r\n    @NNTiming.timeit(level=3)\r\n    def _print_metric_logs(self, show_loss, data_type):\r\n        print()\r\n        print(""="" * 47)\r\n        for i, name in enumerate(self._metric_names):\r\n            print(""{:<16s} {:<16s}: {:12.8}"".format(\r\n                data_type, name, self._logs[data_type][i][-1]))\r\n        if show_loss:\r\n            print(""{:<16s} {:<16s}: {:12.8}"".format(\r\n                data_type, ""loss"", self._logs[data_type][-1][-1]))\r\n        print(""="" * 47)\r\n\r\n    @NNTiming.timeit(level=1)\r\n    def _draw_2d_network(self, radius=6, width=1200, height=800, padding=0.2,\r\n                         plot_scale=2, plot_precision=0.03,\r\n                         sub_layer_height_scale=0, **kwargs):\r\n        if not kwargs[""show""] and not kwargs[""mp4""]:\r\n            return\r\n        layers = len(self._layers) + 1\r\n        units = [layer.shape[0] for layer in self._layers] + [self._layers[-1].shape[1]]\r\n        whether_sub_layers = np.array([False] + [isinstance(layer, SubLayer) for layer in self._layers])\r\n        n_sub_layers = np.sum(whether_sub_layers)  # type: int\r\n\r\n        plot_num = int(1 / plot_precision)\r\n        if plot_num % 2 == 1:\r\n            plot_num += 1\r\n        half_plot_num = int(plot_num * 0.5)\r\n        xf = np.linspace(self._x_min * plot_scale, self._x_max * plot_scale, plot_num)\r\n        yf = np.linspace(self._x_min * plot_scale, self._x_max * plot_scale, plot_num) * -1\r\n        input_x, input_y = np.meshgrid(xf, yf)\r\n        input_xs = np.c_[input_x.ravel(), input_y.ravel()]\r\n\r\n        activations = [activation.T.reshape(units[i + 1], plot_num, plot_num)\r\n                       for i, activation in enumerate(self._get_activations(input_xs, predict=True))]\r\n        graphs = []\r\n        for j, activation in enumerate(activations):\r\n            graph_group = []\r\n            if j == len(activations) - 1:\r\n                classes = np.argmax(activation, axis=0)\r\n            else:\r\n                classes = None\r\n            for k, ac in enumerate(activation):\r\n                data = np.zeros((plot_num, plot_num, 3), np.uint8)\r\n                if j != len(activations) - 1:\r\n                    mask = ac >= np.average(ac)\r\n                else:\r\n                    mask = classes == k\r\n                data[mask], data[~mask] = [0, 165, 255], [255, 165, 0]\r\n                graph_group.append(data)\r\n            graphs.append(graph_group)\r\n\r\n        img = np.full([height, width, 3], 255, dtype=np.uint8)\r\n        axis0_padding = int(height / (layers - 1 + 2 * padding)) * padding + plot_num\r\n        axis0_step = (height - 2 * axis0_padding) / layers\r\n        sub_layer_decrease = int((1 - sub_layer_height_scale) * axis0_step)\r\n        axis0 = np.linspace(\r\n            axis0_padding,\r\n            height + n_sub_layers * sub_layer_decrease - axis0_padding,\r\n            layers, dtype=np.int)\r\n        axis0 -= sub_layer_decrease * np.cumsum(whether_sub_layers)\r\n        axis1_padding = plot_num\r\n        axis1 = [np.linspace(axis1_padding, width - axis1_padding, unit + 2, dtype=np.int)\r\n                 for unit in units]\r\n        axis1 = [axis[1:-1] for axis in axis1]\r\n\r\n        colors, thicknesses, masks = [], [], []\r\n        for weight in self._weights:\r\n            line_info = VisUtil.get_line_info(weight.copy())\r\n            colors.append(line_info[0])\r\n            thicknesses.append(line_info[1])\r\n            masks.append(line_info[2])\r\n\r\n        for i, (y, xs) in enumerate(zip(axis0, axis1)):\r\n            for j, x in enumerate(xs):\r\n                if i == 0:\r\n                    cv2.circle(img, (x, y), radius, (20, 215, 20), int(radius / 2))\r\n                else:\r\n                    graph = graphs[i - 1][j]\r\n                    img[y - half_plot_num:y + half_plot_num, x - half_plot_num:x + half_plot_num] = graph\r\n            if i > 0:\r\n                cv2.putText(img, self._layers[i - 1].name, (12, y - 36), cv2.LINE_AA, 0.6, (0, 0, 0), 1)\r\n        for i, y in enumerate(axis0):\r\n            if i == len(axis0) - 1:\r\n                break\r\n            for j, x in enumerate(axis1[i]):\r\n                new_y = axis0[i + 1]\r\n                whether_sub_layer = isinstance(self._layers[i], SubLayer)\r\n                for k, new_x in enumerate(axis1[i + 1]):\r\n                    if whether_sub_layer and j != k:\r\n                        continue\r\n                    if masks[i][j][k]:\r\n                        cv2.line(img, (x, y + half_plot_num), (new_x, new_y - half_plot_num),\r\n                                 colors[i][j][k], thicknesses[i][j][k])\r\n\r\n        return img\r\n\r\n    # Optimizing Process\r\n\r\n    @NNTiming.timeit(level=4)\r\n    def _init_optimizer(self):\r\n        if not isinstance(self._w_optimizer, Optimizer):\r\n            self._w_optimizer = self._optimizer_factory.get_optimizer_by_name(\r\n                self._w_optimizer, self._weights, self._lr, self._epoch)\r\n        if not isinstance(self._b_optimizer, Optimizer):\r\n            self._b_optimizer = self._optimizer_factory.get_optimizer_by_name(\r\n                self._b_optimizer, self._bias, self._lr, self._epoch)\r\n        if self._w_optimizer.name != self._b_optimizer.name:\r\n            self._optimizer_name = None\r\n        else:\r\n            self._optimizer_name = self._w_optimizer.name\r\n\r\n    @NNTiming.timeit(level=1)\r\n    def _opt(self, i, activation, delta):\r\n        if not isinstance(self._layers[i], ConvLayer):\r\n            self._weights[i] *= self._regularization_param\r\n            self._weights[i] += self._w_optimizer.run(\r\n                i, activation.reshape(activation.shape[0], -1).T.dot(delta)\r\n            )\r\n            if self._apply_bias:\r\n                self._bias[i] += self._b_optimizer.run(\r\n                    i, np.sum(delta, axis=0, keepdims=True)\r\n                )\r\n        else:\r\n            self._weights[i] *= self._regularization_param\r\n            if delta[1] is not None:\r\n                self._weights[i] += self._w_optimizer.run(i, delta[1])\r\n            if self._apply_bias and delta[2] is not None:\r\n                self._bias[i] += self._b_optimizer.run(i, delta[2])\r\n\r\n    # API\r\n\r\n    @NNTiming.timeit(level=4, prefix=""[API] "")\r\n    def add(self, layer, *args, **kwargs):\r\n        if isinstance(layer, str):\r\n            # noinspection PyTypeChecker\r\n            self._add_layer(layer, *args, **kwargs)\r\n        else:\r\n            if not isinstance(layer, Layer):\r\n                raise BuildLayerError(""Invalid Layer provided (should be subclass of Layer)"")\r\n            if not self._layers:\r\n                if isinstance(layer, SubLayer):\r\n                    raise BuildLayerError(""Invalid Layer provided (first layer should not be subclass of SubLayer)"")\r\n                if len(layer.shape) != 2:\r\n                    raise BuildLayerError(""Invalid input Layer provided (shape should be {}, {} found)"".format(\r\n                        2, len(layer.shape)\r\n                    ))\r\n                self._layers, self._current_dimension = [layer], layer.shape[1]\r\n                self._update_layer_information(layer)\r\n                if isinstance(layer, ConvLayer):\r\n                    self._add_params(layer.shape, layer.n_channels)\r\n                else:\r\n                    self._add_params(layer.shape)\r\n            else:\r\n                if len(layer.shape) > 2:\r\n                    raise BuildLayerError(""Invalid Layer provided (shape should be {}, {} found)"".format(\r\n                        2, len(layer.shape)\r\n                    ))\r\n                if len(layer.shape) == 2:\r\n                    _current, _next = layer.shape\r\n                    if isinstance(layer, SubLayer):\r\n                        if _next != self._current_dimension:\r\n                            raise BuildLayerError(""Invalid SubLayer provided (shape[1] should be {}, {} found)"".format(\r\n                                self._current_dimension, _next\r\n                            ))\r\n                    elif not isinstance(layer, ConvLayer) and _current != self._current_dimension:\r\n                        raise BuildLayerError(""Invalid Layer provided (shape[0] should be {}, {} found)"".format(\r\n                            self._current_dimension, _current\r\n                        ))\r\n                    self._add_layer(layer, _current, _next)\r\n\r\n                elif len(layer.shape) == 1:\r\n                    _next = layer.shape[0]\r\n                    layer.shape = (self._current_dimension, _next)\r\n                    self._add_layer(layer, self._current_dimension, _next)\r\n                else:\r\n                    raise LayerError(""Invalid Layer provided (invalid shape \'{}\' found)"".format(layer.shape))\r\n\r\n    @NNTiming.timeit(level=4, prefix=""[API] "")\r\n    def build(self, units=""build""):\r\n        if isinstance(units, str):\r\n            if units == ""build"":\r\n                for name, param in zip(self._layer_names, self._layer_params):\r\n                    self.add(name, *param)\r\n            else:\r\n                raise NotImplementedError(""Invalid param \'{}\' provided to \'build\' method"".format(units))\r\n        else:\r\n            try:\r\n                units = np.asarray(units).flatten().astype(np.int)\r\n            except ValueError as err:\r\n                raise BuildLayerError(err)\r\n            if len(units) < 2:\r\n                raise BuildLayerError(""At least 2 layers are needed"")\r\n            _input_shape = (units[0], units[1])\r\n            self.initialize()\r\n            self.add(ReLU(_input_shape))\r\n            for unit_num in units[2:-1]:\r\n                self.add(ReLU((unit_num,)))\r\n            self.add(""CrossEntropy"", (units[-1],))\r\n\r\n    @NNTiming.timeit(level=4, prefix=""[API] "")\r\n    def preview(self):\r\n        if not self._layers:\r\n            rs = ""None""\r\n        else:\r\n            rs = (\r\n                ""Input  :  {:<10s} - {}\\n"".format(""Dimension"", self._layers[0].shape[0]) +\r\n                ""\\n"".join([\r\n                    ""Layer  :  {:<10s} - {} {}"".format(\r\n                        layer.name, layer.shape[1], layer.description\r\n                    ) if isinstance(layer, SubLayer) else\r\n                    ""Layer  :  {:<10s} - {:<14s} - strides: {:2d} - padding: {:2d} - out: {}"".format(\r\n                        layer.name, str(layer.shape[1]), layer.stride, layer.padding,\r\n                        (layer.n_filters, layer.out_h, layer.out_w)\r\n                    ) if isinstance(layer, ConvLayer) else ""Layer  :  {:<10s} - {}"".format(\r\n                        layer.name, layer.shape[1]\r\n                    ) for layer in self._layers[:-1]\r\n                ]) + ""\\nCost   :  {:<16s}"".format(str(self._layers[-1]))\r\n            )\r\n        print(""="" * 30 + ""\\n"" + ""Structure\\n"" + ""-"" * 30 + ""\\n"" + rs + ""\\n"" + ""-"" * 30 + ""\\n"")\r\n\r\n    @NNTiming.timeit(level=1, prefix=""[API] "")\r\n    def fit(self,\r\n            x=None, y=None, x_test=None, y_test=None,\r\n            batch_size=128, record_period=1, train_only=False,\r\n            optimizer=None, w_optimizer=None, b_optimizer=None,\r\n            lr=0.001, lb=0.001, epoch=20, weight_scale=1, apply_bias=True,\r\n            show_loss=True, metrics=None, do_log=True, verbose=None,\r\n            visualize=False, visualize_setting=None,\r\n            draw_weights=False, animation_params=None):\r\n        self._lr, self._epoch = lr, epoch\r\n        for weight in self._weights:\r\n            weight *= weight_scale\r\n        if not self._w_optimizer or not self._b_optimizer:\r\n            if not self._optimizer_name:\r\n                if optimizer is None:\r\n                    optimizer = ""Adam""\r\n                self._w_optimizer = optimizer if w_optimizer is None else w_optimizer\r\n                self._b_optimizer = optimizer if b_optimizer is None else b_optimizer\r\n            else:\r\n                if not self._w_optimizer:\r\n                    self._w_optimizer = self._optimizer_name\r\n                if not self._b_optimizer:\r\n                    self._b_optimizer = self._optimizer_name\r\n        self._init_optimizer()\r\n        assert isinstance(self._w_optimizer, Optimizer) and isinstance(self._b_optimizer, Optimizer)\r\n        print()\r\n        print(""="" * 30)\r\n        print(""Optimizers"")\r\n        print(""-"" * 30)\r\n        print(""w: {}\\nb: {}"".format(self._w_optimizer, self._b_optimizer))\r\n        print(""-"" * 30)\r\n        if not self._layers:\r\n            raise BuildNetworkError(""Please provide layers before fitting data"")\r\n        if y.shape[1] != self._current_dimension:\r\n            raise BuildNetworkError(""Output layer\'s shape should be {}, {} found"".format(\r\n                self._current_dimension, y.shape[1]))\r\n\r\n        (x_train, x_test), (y_train, y_test) = self._split_data(\r\n            x, y, x_test, y_test, train_only)\r\n        train_len = len(x_train)\r\n        batch_size = min(batch_size, train_len)\r\n        do_random_batch = train_len > batch_size\r\n        train_repeat = 1 if not do_random_batch else int(train_len / batch_size) + 1\r\n        self._regularization_param = 1 - lb * lr / batch_size\r\n        self._get_min_max(x_train, y_train)\r\n\r\n        self._metrics = [""acc""] if metrics is None else metrics\r\n        for i, metric in enumerate(self._metrics):\r\n            if isinstance(metric, str):\r\n                if metric not in self._available_metrics:\r\n                    raise BuildNetworkError(""Metric \'{}\' is not implemented"".format(metric))\r\n                self._metrics[i] = self._available_metrics[metric]\r\n        self._metric_names = [_m.__name__ for _m in self._metrics]\r\n\r\n        self._logs = {\r\n            name: [[] for _ in range(len(self._metrics) + 1)] for name in (""train"", ""cv"", ""test"")\r\n        }\r\n        if verbose is not None:\r\n            self.verbose = verbose\r\n\r\n        layer_width = len(self._layers)\r\n        self._apply_bias = apply_bias\r\n\r\n        bar = ProgressBar(max_value=max(1, epoch // record_period), name=""Epoch"", start=False)\r\n        if self.verbose >= NNVerbose.EPOCH:\r\n            bar.start()\r\n        img, ims = None, []\r\n\r\n        if draw_weights:\r\n            weight_trace = [[[org] for org in weight] for weight in self._weights]\r\n        else:\r\n            weight_trace = []\r\n\r\n        *animation_properties, animation_params = self._get_animation_params(animation_params)\r\n        sub_bar = ProgressBar(max_value=train_repeat * record_period - 1, name=""Iteration"", start=False)\r\n        for counter in range(epoch):\r\n            self._w_optimizer.update()\r\n            self._b_optimizer.update()\r\n            if self.verbose >= NNVerbose.ITER and counter % record_period == 0:\r\n                sub_bar.start()\r\n            for _ in range(train_repeat):\r\n                if do_random_batch:\r\n                    batch = np.random.choice(train_len, batch_size)\r\n                    x_batch, y_batch = x_train[batch], y_train[batch]\r\n                else:\r\n                    x_batch, y_batch = x_train, y_train\r\n                activations = self._get_activations(x_batch)\r\n\r\n                deltas = [self._layers[-1].bp_first(y_batch, activations[-1])]\r\n                for i in range(-1, -len(activations), -1):\r\n                    deltas.append(self._layers[i - 1].bp(activations[i - 1], self._weights[i], deltas[-1]))\r\n\r\n                for i in range(layer_width - 1, 0, -1):\r\n                    if not isinstance(self._layers[i], SubLayer):\r\n                        self._opt(i, activations[i - 1], deltas[layer_width - i - 1])\r\n                self._opt(0, x_batch, deltas[-1])\r\n\r\n                if draw_weights:\r\n                    for i, weight in enumerate(self._weights):\r\n                        for j, new_weight in enumerate(weight.copy()):\r\n                            weight_trace[i][j].append(new_weight)\r\n                if self.verbose >= NNVerbose.DEBUG:\r\n                    pass\r\n                if self.verbose >= NNVerbose.ITER:\r\n                    if sub_bar.update() and self.verbose >= NNVerbose.METRICS_DETAIL:\r\n                        self._append_log(x, y, ""train"", get_loss=show_loss)\r\n                        self._append_log(x_test, y_test, ""cv"", get_loss=show_loss)\r\n                        self._print_metric_logs(show_loss, ""train"")\r\n                        self._print_metric_logs(show_loss, ""cv"")\r\n            if self.verbose >= NNVerbose.ITER:\r\n                sub_bar.update()\r\n            self._handle_animation(\r\n                counter, x, y, ims, animation_params, *animation_properties,\r\n                img=self._draw_2d_network(**animation_params), name=""Neural Network""\r\n            )\r\n            if do_log:\r\n                self._append_log(x, y, ""train"", get_loss=show_loss)\r\n                self._append_log(x_test, y_test, ""cv"", get_loss=show_loss)\r\n            if (counter + 1) % record_period == 0:\r\n                if do_log and self.verbose >= NNVerbose.METRICS:\r\n                    self._print_metric_logs(show_loss, ""train"")\r\n                    self._print_metric_logs(show_loss, ""cv"")\r\n                if visualize:\r\n                    if visualize_setting is None:\r\n                        self.visualize2d(x_test, y_test)\r\n                    else:\r\n                        self.visualize2d(x_test, y_test, *visualize_setting)\r\n                if self.verbose >= NNVerbose.EPOCH:\r\n                    bar.update(counter // record_period + 1)\r\n                    if self.verbose >= NNVerbose.ITER:\r\n                        sub_bar = ProgressBar(max_value=train_repeat * record_period - 1, name=""Iteration"", start=False)\r\n\r\n        if do_log:\r\n            self._append_log(x_test, y_test, ""test"", get_loss=show_loss)\r\n        if img is not None:\r\n            cv2.waitKey(0)\r\n            cv2.destroyAllWindows()\r\n        if draw_weights:\r\n            ts = np.arange(epoch * train_repeat + 1)\r\n            for i, weight in enumerate(self._weights):\r\n                plt.figure()\r\n                for j in range(len(weight)):\r\n                    plt.plot(ts, weight_trace[i][j])\r\n                plt.title(""Weights toward layer {} ({})"".format(i + 1, self._layers[i].name))\r\n                plt.show()\r\n        self._handle_mp4(ims, animation_properties, ""NN"")\r\n        return self._logs\r\n\r\n    @NNTiming.timeit(level=2, prefix=""[API] "")\r\n    def save(self, path=None, name=None, overwrite=True):\r\n        path = os.path.join(""Models"", ""Cache"") if path is None else os.path.join(""Models"", path)\r\n        name = ""Model.nn"" if name is None else name\r\n        if not os.path.exists(path):\r\n            os.makedirs(path)\r\n        _dir = os.path.join(path, name)\r\n        if not overwrite and os.path.isfile(_dir):\r\n            _count = 1\r\n            _new_dir = _dir + ""({})"".format(_count)\r\n            while os.path.isfile(_new_dir):\r\n                _count += 1\r\n                _new_dir = _dir + ""({})"".format(_count)\r\n            _dir = _new_dir\r\n        print()\r\n        print(""="" * 60)\r\n        print(""Saving Model to {}..."".format(_dir))\r\n        print(""-"" * 60)\r\n        with open(_dir, ""wb"") as file:\r\n            pickle.dump({\r\n                ""structures"": {\r\n                    ""_layer_names"": self.layer_names,\r\n                    ""_layer_params"": self._layer_params,\r\n                    ""_cost_layer"": self._layers[-1].name,\r\n                    ""_next_dimension"": self._current_dimension\r\n                },\r\n                ""params"": {\r\n                    ""_logs"": self._logs,\r\n                    ""_metric_names"": self._metric_names,\r\n                    ""_weights"": self._weights,\r\n                    ""_bias"": self._bias,\r\n                    ""_optimizer_name"": self._optimizer_name,\r\n                    ""_w_optimizer"": self._w_optimizer,\r\n                    ""_b_optimizer"": self._b_optimizer,\r\n                    ""layer_special_params"": self.layer_special_params,\r\n                }\r\n            }, file)\r\n        print(""Done"")\r\n        print(""="" * 60)\r\n\r\n    @NNTiming.timeit(level=2, prefix=""[API] "")\r\n    def load(self, path=os.path.join(""Models"", ""Cache"", ""Model.nn"")):\r\n        self.initialize()\r\n        try:\r\n            with open(path, ""rb"") as file:\r\n                dic = pickle.load(file)\r\n                for key, value in dic[""structures""].items():\r\n                    setattr(self, key, value)\r\n                self.build()\r\n                for key, value in dic[""params""].items():\r\n                    setattr(self, key, value)\r\n                self._init_optimizer()\r\n                for i in range(len(self._metric_names) - 1, -1, -1):\r\n                    name = self._metric_names[i]\r\n                    if name not in self._available_metrics:\r\n                        self._metric_names.pop(i)\r\n                    else:\r\n                        self._metrics.insert(0, self._available_metrics[name])\r\n                print()\r\n                print(""="" * 30)\r\n                print(""Model restored"")\r\n                print(""="" * 30)\r\n                return dic\r\n        except Exception as err:\r\n            raise BuildNetworkError(""Failed to load Network ({}), structure initialized"".format(err))\r\n\r\n    @NNTiming.timeit(level=4, prefix=""[API] "")\r\n    def predict(self, x, get_raw_results=False, **kwargs):\r\n        x = np.asarray(x)\r\n        if len(x.shape) == 1:\r\n            x = x.reshape(1, -1)\r\n        y_pred = self._get_prediction(x)\r\n        return y_pred if get_raw_results else np.argmax(y_pred, axis=1)\r\n\r\n    def draw_results(self):\r\n        metrics_log, loss_log = {}, {}\r\n        for key, value in sorted(self._logs.items()):\r\n            metrics_log[key], loss_log[key] = value[:-1], value[-1]\r\n\r\n        for i, name in enumerate(sorted(self._metric_names)):\r\n            plt.figure()\r\n            plt.title(""Metric Type: {}"".format(name))\r\n            for key, log in sorted(metrics_log.items()):\r\n                if key == ""test"":\r\n                    continue\r\n                xs = np.arange(len(log[i])) + 1\r\n                plt.plot(xs, log[i], label=""Data Type: {}"".format(key))\r\n            plt.legend(loc=4)\r\n            plt.show()\r\n            plt.close()\r\n\r\n        plt.figure()\r\n        plt.title(""Cost"")\r\n        for key, loss in sorted(loss_log.items()):\r\n            if key == ""test"":\r\n                continue\r\n            xs = np.arange(len(loss)) + 1\r\n            plt.plot(xs, loss, label=""Data Type: {}"".format(key))\r\n        plt.legend()\r\n        plt.show()\r\n\r\n    def draw_conv_weights(self):\r\n        for i, (name, weight) in enumerate(zip(self.layer_names, self._weights)):\r\n            if len(weight.shape) != 4:\r\n                return\r\n            for j, _w in enumerate(weight):\r\n                for k, _ww in enumerate(_w):\r\n                    VisUtil.show_img(_ww, ""{} {} filter {} channel {}"".format(name, i+1, j+1, k+1))\r\n\r\n    def draw_conv_series(self, x, shape=None):\r\n        for xx in x:\r\n            VisUtil.show_img(VisUtil.trans_img(xx, shape), ""Original"")\r\n            activations = self._get_activations(np.array([xx]), predict=True)\r\n            for i, (layer, ac) in enumerate(zip(self._layers, activations)):\r\n                if len(ac.shape) == 4:\r\n                    for n in ac:\r\n                        _n, height, width = n.shape\r\n                        a = int(ceil(sqrt(_n)))\r\n                        g = np.ones((a * height + a, a * width + a), n.dtype)\r\n                        g *= np.min(n)\r\n                        _i = 0\r\n                        for y in range(a):\r\n                            for x in range(a):\r\n                                if _i < _n:\r\n                                    g[y * height + y:(y + 1) * height + y, x * width + x:(x + 1) * width + x] = n[\r\n                                        _i, :, :]\r\n                                    _i += 1\r\n                        # normalize to [0,1]\r\n                        max_g = g.max()\r\n                        min_g = g.min()\r\n                        g = (g - min_g) / (max_g - min_g)\r\n                        VisUtil.show_img(g, ""Layer {} ({})"".format(i + 1, layer.name))\r\n                else:\r\n                    ac = ac[0]\r\n                    length = sqrt(np.prod(ac.shape))\r\n                    if length < 10:\r\n                        continue\r\n                    (height, width) = xx.shape[1:] if shape is None else shape[1:]\r\n                    sqrt_shape = sqrt(height * width)\r\n                    oh, ow = int(length * height / sqrt_shape), int(length * width / sqrt_shape)\r\n                    VisUtil.show_img(ac[:oh*ow].reshape(oh, ow), ""Layer {} ({})"".format(i + 1, layer.name))\r\n'"
NN/Basic/Optimizers.py,0,"b'import numpy as np\n\nfrom Util.Timing import Timing\n\n\nclass Optimizer:\n    OptTiming = Timing()\n\n    def __init__(self, lr=0.01, cache=None):\n        self.lr = lr\n        self._cache = cache\n\n    def __str__(self):\n        return self.__class__.__name__\n\n    def __repr__(self):\n        return str(self)\n\n    @property\n    def name(self):\n        return str(self)\n\n    def feed_variables(self, variables):\n        self._cache = [\n            np.zeros(var.shape) for var in variables\n        ]\n\n    @OptTiming.timeit(level=1, prefix=""[API] "")\n    def run(self, i, dw):\n        return self._run(i, dw)\n\n    def _run(self, i, dw):\n        raise NotImplementedError(""Please implement a \'feed\' method for your optimizer"")\n\n    @OptTiming.timeit(level=4, prefix=""[API] "")\n    def update(self):\n        return self._update()\n\n    def _update(self):\n        raise NotImplementedError(""Please implement an \'update\' method for your optimizer"")\n\n\nclass MBGD(Optimizer):\n    def _run(self, i, dw):\n        return self.lr * dw\n\n    def _update(self):\n        pass\n\n\nclass Momentum(Optimizer):\n    def __init__(self, lr=0.01, cache=None, epoch=100, floor=0.5, ceiling=0.999):\n        Optimizer.__init__(self, lr, cache)\n        self._epoch, self._floor, self._ceiling = epoch, floor, ceiling\n        self._step = (ceiling - floor) / epoch\n        self._momentum = floor\n        self._is_nesterov = False\n\n    @property\n    def epoch(self):\n        return self._epoch\n\n    @property\n    def floor(self):\n        return self._floor\n\n    @property\n    def ceiling(self):\n        return self._ceiling\n\n    def update_step(self):\n        self._step = (self._ceiling - self._floor) / self._epoch\n\n    @epoch.setter\n    def epoch(self, value):\n        self._epoch = value\n        self.update_step()\n\n    @floor.setter\n    def floor(self, value):\n        self._floor = value\n        self.update_step()\n\n    @ceiling.setter\n    def ceiling(self, value):\n        self._ceiling = value\n        self.update_step()\n\n    def _run(self, i, dw):\n        dw *= self.lr\n        velocity = self._cache\n        velocity[i] *= self._momentum\n        velocity[i] += dw\n        if not self._is_nesterov:\n            return velocity[i]\n        return self._momentum * velocity[i] + dw\n\n    def _update(self):\n        if self._momentum < self._ceiling:\n            self._momentum += self._step\n\n\nclass NAG(Momentum):\n    def __init__(self, lr=0.01, cache=None, epoch=100, floor=0.5, ceiling=0.999):\n        Momentum.__init__(self, lr, cache, epoch, floor, ceiling)\n        self._is_nesterov = True\n\n\nclass RMSProp(Optimizer):\n    def __init__(self, lr=0.01, cache=None, decay_rate=0.9, eps=1e-8):\n        Optimizer.__init__(self, lr, cache)\n        self.decay_rate, self.eps = decay_rate, eps\n\n    def _run(self, i, dw):\n        self._cache[i] = self._cache[i] * self.decay_rate + (1 - self.decay_rate) * dw ** 2\n        return self.lr * dw / (np.sqrt(self._cache[i] + self.eps))\n\n    def _update(self):\n        pass\n\n\nclass Adam(Optimizer):\n    def __init__(self, lr=0.01, cache=None, beta1=0.9, beta2=0.999, eps=1e-8):\n        Optimizer.__init__(self, lr, cache)\n        self.beta1, self.beta2, self.eps = beta1, beta2, eps\n\n    def feed_variables(self, variables):\n        self._cache = [\n            [np.zeros(var.shape) for var in variables],\n            [np.zeros(var.shape) for var in variables],\n        ]\n\n    def _run(self, i, dw):\n        self._cache[0][i] = self._cache[0][i] * self.beta1 + (1 - self.beta1) * dw\n        self._cache[1][i] = self._cache[1][i] * self.beta2 + (1 - self.beta2) * (dw ** 2)\n        return self.lr * self._cache[0][i] / (np.sqrt(self._cache[1][i] + self.eps))\n\n    def _update(self):\n        pass\n\n\n# Factory\n\nclass OptFactory:\n    available_optimizers = {\n        ""MBGD"": MBGD, ""Momentum"": Momentum, ""NAG"": NAG, ""Adam"": Adam, ""RMSProp"": RMSProp\n    }\n\n    def get_optimizer_by_name(self, name, variables, lr, epoch):\n        try:\n            optimizer = self.available_optimizers[name](lr)\n            if variables is not None:\n                optimizer.feed_variables(variables)\n            if epoch is not None and isinstance(optimizer, Momentum):\n                optimizer.epoch = epoch\n            return optimizer\n        except KeyError:\n            raise NotImplementedError(""Undefined Optimizer \'{}\' found"".format(name))\n'"
NN/PyTorch/Optimizers.py,0,"b'import torch\n\nfrom Util.Timing import Timing\n\n\nclass Optimizer:\n    OptTiming = Timing()\n\n    def __init__(self, lr=0.01, cache=None):\n        self.lr = lr\n        self._cache = cache\n\n    def __str__(self):\n        return self.__class__.__name__\n\n    def __repr__(self):\n        return str(self)\n\n    @property\n    def name(self):\n        return str(self)\n\n    def feed_variables(self, variables):\n        self._cache = [\n            torch.zeros(var.size()) for var in variables\n        ]\n\n    @OptTiming.timeit(level=1, prefix=""[API] "")\n    def run(self, i, dw):\n        return self._run(i, dw)\n\n    def _run(self, i, dw):\n        raise NotImplementedError(""Please implement a \'feed\' method for your optimizer"")\n\n    @OptTiming.timeit(level=4, prefix=""[API] "")\n    def update(self):\n        return self._update()\n\n    def _update(self):\n        raise NotImplementedError(""Please implement an \'update\' method for your optimizer"")\n\n\nclass MBGD(Optimizer):\n    def _run(self, i, dw):\n        return self.lr * dw\n\n    def _update(self):\n        pass\n\n\nclass Momentum(Optimizer):\n    def __init__(self, lr=0.01, cache=None, epoch=100, floor=0.5, ceiling=0.999):\n        Optimizer.__init__(self, lr, cache)\n        self._epoch, self._floor, self._ceiling = epoch, floor, ceiling\n        self._step = (ceiling - floor) / epoch\n        self._momentum = floor\n        self._is_nesterov = False\n\n    @property\n    def epoch(self):\n        return self._epoch\n\n    @property\n    def floor(self):\n        return self._floor\n\n    @property\n    def ceiling(self):\n        return self._ceiling\n\n    def update_step(self):\n        self._step = (self._ceiling - self._floor) / self._epoch\n\n    @epoch.setter\n    def epoch(self, value):\n        self._epoch = value\n        self.update_step()\n\n    @floor.setter\n    def floor(self, value):\n        self._floor = value\n        self.update_step()\n\n    @ceiling.setter\n    def ceiling(self, value):\n        self._ceiling = value\n        self.update_step()\n\n    def _run(self, i, dw):\n        dw *= self.lr\n        velocity = self._cache\n        velocity[i] *= self._momentum\n        velocity[i] += dw\n        if not self._is_nesterov:\n            return velocity[i]\n        return self._momentum * velocity[i] + dw\n\n    def _update(self):\n        if self._momentum < self._ceiling:\n            self._momentum += self._step\n\n\nclass NAG(Momentum):\n    def __init__(self, lr=0.01, cache=None, epoch=100, floor=0.5, ceiling=0.999):\n        Momentum.__init__(self, lr, cache, epoch, floor, ceiling)\n        self._is_nesterov = True\n\n\nclass RMSProp(Optimizer):\n    def __init__(self, lr=0.01, cache=None, decay_rate=0.9, eps=1e-8):\n        Optimizer.__init__(self, lr, cache)\n        self.decay_rate, self.eps = decay_rate, eps\n\n    def _run(self, i, dw):\n        self._cache[i] = self._cache[i] * self.decay_rate + (1 - self.decay_rate) * dw * dw\n        return self.lr * dw / (torch.sqrt(self._cache[i] + self.eps))\n\n    def _update(self):\n        pass\n\n\nclass Adam(Optimizer):\n    def __init__(self, lr=0.01, cache=None, beta1=0.9, beta2=0.999, eps=1e-8):\n        Optimizer.__init__(self, lr, cache)\n        self.beta1, self.beta2, self.eps = beta1, beta2, eps\n\n    def feed_variables(self, variables):\n        self._cache = [\n            [torch.zeros(var.size()) for var in variables],\n            [torch.zeros(var.size()) for var in variables],\n        ]\n\n    def _run(self, i, dw):\n        self._cache[0][i] = self._cache[0][i] * self.beta1 + (1 - self.beta1) * dw\n        self._cache[1][i] = self._cache[1][i] * self.beta2 + (1 - self.beta2) * (dw ** 2)\n        return self.lr * self._cache[0][i] / (torch.sqrt(self._cache[1][i] + self.eps))\n\n    def _update(self):\n        pass\n\n\n# Factory\n\nclass OptFactory:\n    available_optimizers = {\n        ""MBGD"": MBGD, ""Momentum"": Momentum, ""NAG"": NAG, ""Adam"": Adam, ""RMSProp"": RMSProp\n    }\n\n    def get_optimizer_by_name(self, name, variables, lr, epoch):\n        try:\n            optimizer = self.available_optimizers[name](lr)\n            if variables is not None:\n                optimizer.feed_variables(variables)\n            if epoch is not None and isinstance(optimizer, Momentum):\n                optimizer.epoch = epoch\n            return optimizer\n        except KeyError:\n            raise NotImplementedError(""Undefined Optimizer \'{}\' found"".format(name))\n'"
NN/TF/Layers.py,34,"b'import numpy as np\r\nfrom math import ceil\r\n\r\nfrom NN.Errors import *\r\nfrom NN.TF.Optimizers import *\r\n\r\n\r\nclass Layer:\r\n    LayerTiming = Timing()\r\n\r\n    def __init__(self, shape, **kwargs):\r\n        """"""\r\n        :param shape: shape[0] = units of previous layer\r\n                      shape[1] = units of current layer (self)\r\n        """"""\r\n        self._shape = shape\r\n        self.parent = None\r\n        self.is_fc = False\r\n        self.is_sub_layer = False\r\n        self.apply_bias = kwargs[""apply_bias""]\r\n        self.position = kwargs[""position""]\r\n\r\n    def __str__(self):\r\n        return self.__class__.__name__\r\n\r\n    def __repr__(self):\r\n        return str(self)\r\n\r\n    def init(self, sess):\r\n        pass\r\n\r\n    @property\r\n    def name(self):\r\n        return str(self)\r\n\r\n    @property\r\n    def root(self):\r\n        return self\r\n\r\n    @property\r\n    def shape(self):\r\n        return self._shape\r\n\r\n    @shape.setter\r\n    def shape(self, value):\r\n        self._shape = value\r\n\r\n    @property\r\n    def params(self):\r\n        return self._shape,\r\n\r\n    @property\r\n    def info(self):\r\n        return ""Layer  :  {:<16s} - {}{}"".format(\r\n            self.name, self.shape[1], "" (apply_bias: False)"" if not self.apply_bias else """")\r\n\r\n    def get_special_params(self, sess):\r\n        pass\r\n\r\n    def set_special_params(self, dic):\r\n        for key, value in dic.items():\r\n            setattr(self, key, value)\r\n\r\n    # Core\r\n\r\n    @LayerTiming.timeit(level=1, prefix=""[Core] "")\r\n    def activate(self, x, w, bias=None, predict=False):\r\n        if self.is_fc:\r\n            fc_shape = np.prod(x.get_shape()[1:])  # type: int\r\n            x = tf.reshape(x, [-1, int(fc_shape)])\r\n        if self.is_sub_layer:\r\n            if not self.apply_bias:\r\n                return self._activate(x, predict)\r\n            return self._activate(x + bias, predict)\r\n        if not self.apply_bias:\r\n            return self._activate(tf.matmul(x, w), predict)\r\n        return self._activate(tf.matmul(x, w) + bias, predict)\r\n\r\n    def _activate(self, x, predict):\r\n        pass\r\n\r\n\r\nclass SubLayer(Layer):\r\n    def __init__(self, parent, shape, **kwargs):\r\n        Layer.__init__(self, shape, **kwargs)\r\n        self.parent = parent\r\n        self.description = """"\r\n        self.is_sub_layer = True\r\n\r\n    @property\r\n    def root(self):\r\n        _root = self.parent\r\n        while _root.parent:\r\n            _root = _root.parent\r\n        return _root\r\n\r\n    def get_params(self):\r\n        pass\r\n\r\n    @property\r\n    def params(self):\r\n        return self.get_params()\r\n\r\n    @property\r\n    def info(self):\r\n        return ""Layer  :  {:<16s} - {} {}"".format(self.name, self.shape[1], self.description)\r\n\r\n    def _activate(self, x, predict):\r\n        raise NotImplementedError(""Please implement activation function for "" + self.name)\r\n\r\n\r\nclass ConvLayer(Layer):\r\n    LayerTiming = Timing()\r\n\r\n    def __init__(self, shape, stride=1, padding=None, parent=None, **kwargs):\r\n        """"""\r\n        :param shape:    shape[0] = shape of previous layer           c x h x w\r\n                         shape[1] = shape of current layer\'s weight   f x c x h x w\r\n        :param stride:   stride\r\n        :param padding:  zero-padding\r\n        """"""\r\n        if parent is not None:\r\n            _parent = parent.root if parent.is_sub_layer else parent\r\n            shape = _parent.shape\r\n        Layer.__init__(self, shape, **kwargs)\r\n        self._stride = stride\r\n        if padding is None:\r\n            padding = ""SAME""\r\n        if isinstance(padding, str):\r\n            if padding.upper() == ""VALID"":\r\n                self._padding = 0\r\n                self._pad_flag = ""VALID""\r\n            else:\r\n                self._padding = self._pad_flag = ""SAME""\r\n        elif isinstance(padding, int):\r\n            self._padding = padding\r\n            self._pad_flag = ""VALID""\r\n        else:\r\n            raise BuildLayerError(""Padding should be \'SAME\' or \'VALID\' or integer"")\r\n        self.parent = parent\r\n        if len(shape) == 1:\r\n            self.n_channels, self.n_filters, self.out_h, self.out_w = None, None, None, None\r\n        else:\r\n            self.feed_shape(shape)\r\n\r\n    def feed_shape(self, shape):\r\n        self._shape = shape\r\n        self.n_channels, height, width = shape[0]\r\n        self.n_filters, filter_height, filter_width = shape[1]\r\n        if self._pad_flag == ""VALID"":\r\n            self.out_h = ceil((height - filter_height + 1) / self._stride)\r\n            self.out_w = ceil((width - filter_width + 1) / self._stride)\r\n        else:\r\n            self.out_h = ceil(height / self._stride)\r\n            self.out_w = ceil(width / self._stride)\r\n\r\n    @property\r\n    def params(self):\r\n        return self._shape, self._stride, self._padding\r\n\r\n    @property\r\n    def stride(self):\r\n        return self._stride\r\n\r\n    @property\r\n    def padding(self):\r\n        return self._padding\r\n\r\n    @property\r\n    def pad_flag(self):\r\n        return self._pad_flag\r\n\r\n    @property\r\n    def info(self):\r\n        return ""Layer  :  {:<16s} - {:<14s} - strides: {:2d} - padding: {:6s}{} - out: {}"".format(\r\n            self.name, str(self.shape[1]), self.stride, self.pad_flag,\r\n            "" "" * 5 if self.pad_flag == ""SAME"" else "" ({:2d})"".format(self.padding),\r\n            (self.n_filters, self.out_h, self.out_w)\r\n        )\r\n\r\n\r\nclass ConvPoolLayer(ConvLayer):\r\n    LayerTiming = Timing()\r\n\r\n    @property\r\n    def params(self):\r\n        return (self._shape[0], self._shape[1][1:]), self._stride, self._padding\r\n\r\n    def feed_shape(self, shape):\r\n        shape = (shape[0], (shape[0][0], *shape[1]))\r\n        ConvLayer.feed_shape(self, shape)\r\n\r\n    @LayerTiming.timeit(level=1, prefix=""[Core] "")\r\n    def activate(self, x, w, bias=None, predict=False):\r\n        pool_height, pool_width = self._shape[1][1:]\r\n        if self._pad_flag == ""VALID"" and self._padding > 0:\r\n            _pad = [self._padding] * 2\r\n            x = tf.pad(x, [[0, 0], _pad, _pad, [0, 0]], ""CONSTANT"")\r\n        return self._activate(None)(\r\n            x, ksize=[1, pool_height, pool_width, 1],\r\n            strides=[1, self._stride, self._stride, 1], padding=self._pad_flag)\r\n\r\n    def _activate(self, x, *args):\r\n        raise NotImplementedError(""Please implement activation function for "" + self.name)\r\n\r\n\r\n# noinspection PyProtectedMember\r\nclass ConvMeta(type):\r\n    def __new__(mcs, *args, **kwargs):\r\n        name, bases, attr = args[:3]\r\n        conv_layer, layer = bases\r\n\r\n        def __init__(self, shape, stride=1, padding=""SAME"", **_kwargs):\r\n            conv_layer.__init__(self, shape, stride, padding, **_kwargs)\r\n\r\n        def _conv(self, x, w):\r\n            return tf.nn.conv2d(x, w, strides=[1, self.stride, self.stride, 1], padding=self._pad_flag)\r\n\r\n        def _activate(self, x, w, bias, predict):\r\n            res = self._conv(x, w) + bias if self.apply_bias else self._conv(x, w)\r\n            return layer._activate(self, res, predict)\r\n\r\n        def activate(self, x, w, bias=None, predict=False):\r\n            if self._pad_flag == ""VALID"" and self._padding > 0:\r\n                _pad = [self._padding] * 2\r\n                x = tf.pad(x, [[0, 0], _pad, _pad, [0, 0]], ""CONSTANT"")\r\n            return self.LayerTiming.timeit(level=1, func_name=""activate"", cls_name=name, prefix=""[Core] "")(\r\n                _activate)(self, x, w, bias, predict)\r\n\r\n        for key, value in locals().items():\r\n            if str(value).find(""function"") >= 0 or str(value).find(""property""):\r\n                attr[key] = value\r\n\r\n        return type(name, bases, attr)\r\n\r\n\r\n# noinspection PyProtectedMember\r\nclass ConvSubMeta(type):\r\n    def __new__(mcs, *args, **kwargs):\r\n        name, bases, attr = args[:3]\r\n        conv_layer, sub_layer = bases\r\n\r\n        def __init__(self, parent, shape, *_args, **_kwargs):\r\n            conv_layer.__init__(self, None, parent=parent, **_kwargs)\r\n            self.out_h, self.out_w = parent.out_h, parent.out_w\r\n            sub_layer.__init__(self, parent, shape, *_args, **_kwargs)\r\n            self._shape = ((shape[0][0], self.out_h, self.out_w), shape[0])\r\n            if name == ""ConvNorm"":\r\n                self.tf_gamma = tf.Variable(tf.ones(self.n_filters), name=""norm_scale"")\r\n                self.tf_beta = tf.Variable(tf.zeros(self.n_filters), name=""norm_beta"")\r\n\r\n        def _activate(self, x, predict):\r\n            return sub_layer._activate(self, x, predict)\r\n\r\n        # noinspection PyUnusedLocal\r\n        def activate(self, x, w, bias=None, predict=False):\r\n            return self.LayerTiming.timeit(level=1, func_name=""activate"", cls_name=name, prefix=""[Core] "")(\r\n                _activate)(self, x, predict)\r\n\r\n        @property\r\n        def params(self):\r\n            return sub_layer.get_params(self)\r\n\r\n        for key, value in locals().items():\r\n            if str(value).find(""function"") >= 0 or str(value).find(""property""):\r\n                attr[key] = value\r\n\r\n        return type(name, bases, attr)\r\n\r\n\r\n# Activation Layers\r\n\r\nclass Tanh(Layer):\r\n    def _activate(self, x, predict):\r\n        return tf.tanh(x)\r\n\r\n\r\nclass Sigmoid(Layer):\r\n    def _activate(self, x, predict):\r\n        return tf.nn.sigmoid(x)\r\n\r\n\r\nclass ELU(Layer):\r\n    def _activate(self, x, predict):\r\n        return tf.nn.elu(x)\r\n\r\n\r\nclass ReLU(Layer):\r\n    def _activate(self, x, predict):\r\n        return tf.nn.relu(x)\r\n\r\n\r\nclass Softplus(Layer):\r\n    def _activate(self, x, predict):\r\n        return tf.nn.softplus(x)\r\n\r\n\r\nclass Identical(Layer):\r\n    def _activate(self, x, predict):\r\n        return x\r\n\r\n\r\nclass CF0910(Layer):\r\n    def _activate(self, x, predict):\r\n        return tf.minimum(tf.maximum(x, 0), 6)\r\n\r\n\r\n# Convolution Layers\r\n\r\nclass ConvTanh(ConvLayer, Tanh, metaclass=ConvMeta):\r\n    pass\r\n\r\n\r\nclass ConvSigmoid(ConvLayer, Sigmoid, metaclass=ConvMeta):\r\n    pass\r\n\r\n\r\nclass ConvELU(ConvLayer, ELU, metaclass=ConvMeta):\r\n    pass\r\n\r\n\r\nclass ConvReLU(ConvLayer, ReLU, metaclass=ConvMeta):\r\n    pass\r\n\r\n\r\nclass ConvSoftplus(ConvLayer, Softplus, metaclass=ConvMeta):\r\n    pass\r\n\r\n\r\nclass ConvIdentical(ConvLayer, Identical, metaclass=ConvMeta):\r\n    pass\r\n\r\n\r\nclass ConvCF0910(ConvLayer, CF0910, metaclass=ConvMeta):\r\n    pass\r\n\r\n\r\n# Pooling Layers\r\n\r\nclass MaxPool(ConvPoolLayer):\r\n    def _activate(self, x, *args):\r\n        return tf.nn.max_pool\r\n\r\n\r\nclass AvgPool(ConvPoolLayer):\r\n    def _activate(self, x, *args):\r\n        return tf.nn.avg_pool\r\n\r\n\r\n# Special Layers\r\n\r\nclass Dropout(SubLayer):\r\n    def __init__(self, parent, shape, keep_prob=0.5, **kwargs):\r\n        if keep_prob < 0 or keep_prob >= 1:\r\n            raise BuildLayerError(""(Dropout) Keep probability of Dropout should be a positive float smaller than 1"")\r\n        SubLayer.__init__(self, parent, shape, **kwargs)\r\n        self._keep_prob = keep_prob\r\n        self.description = ""(Keep prob: {})"".format(keep_prob)\r\n\r\n    def get_params(self):\r\n        return self._keep_prob,\r\n\r\n    def _activate(self, x, predict):\r\n        if not predict:\r\n            return tf.nn.dropout(x, self._keep_prob)\r\n        return x\r\n\r\n\r\nclass Normalize(SubLayer):\r\n    def __init__(self, parent, shape, activation=""Identical"", eps=1e-8, momentum=0.9, **kwargs):\r\n        SubLayer.__init__(self, parent, shape, **kwargs)\r\n        self._eps, self._activation = eps, activation\r\n        self.rm = self.rv = None\r\n        self.tf_rm = self.tf_rv = None\r\n        self.tf_gamma = tf.Variable(tf.ones(self.shape[1]), name=""norm_scale"")\r\n        self.tf_beta = tf.Variable(tf.zeros(self.shape[1]), name=""norm_beta"")\r\n        self._momentum = momentum\r\n        self.description = ""(eps: {}, momentum: {}, activation: {})"".format(eps, momentum, activation)\r\n\r\n    def init(self, sess):\r\n        if self.rm is not None:\r\n            self.tf_rm = tf.Variable(self.rm, trainable=False, name=""norm_mean"")\r\n        if self.rv is not None:\r\n            self.tf_rv = tf.Variable(self.rv, trainable=False, name=""norm_var"")\r\n        sess.run(tf.variables_initializer([self.tf_rm, self.tf_rv]))\r\n\r\n    def get_special_params(self, sess):\r\n        with sess.as_default():\r\n            return {\r\n                ""rm"": self.tf_rm.eval(), ""rv"": self.tf_rv.eval(),\r\n            }\r\n\r\n    def get_params(self):\r\n        return self._activation, self._eps, self._momentum\r\n\r\n    # noinspection PyTypeChecker\r\n    def _activate(self, x, predict):\r\n        if self.tf_rm is None or self.tf_rv is None:\r\n            shape = x.get_shape()[-1]\r\n            self.tf_rm = tf.Variable(tf.zeros(shape), trainable=False, name=""norm_mean"")\r\n            self.tf_rv = tf.Variable(tf.ones(shape), trainable=False, name=""norm_var"")\r\n        if not predict:\r\n            _sm, _sv = tf.nn.moments(x, list(range(len(x.get_shape()) - 1)))\r\n            _rm = tf.assign(self.tf_rm, self._momentum * self.tf_rm + (1 - self._momentum) * _sm)\r\n            _rv = tf.assign(self.tf_rv, self._momentum * self.tf_rv + (1 - self._momentum) * _sv)\r\n            with tf.control_dependencies([_rm, _rv]):\r\n                _norm = tf.nn.batch_normalization(x, _sm, _sv, self.tf_beta, self.tf_gamma, self._eps)\r\n        else:\r\n            _norm = tf.nn.batch_normalization(x, self.tf_rm, self.tf_rv, self.tf_beta, self.tf_gamma, self._eps)\r\n        if self._activation == ""ReLU"":\r\n            return tf.nn.relu(_norm)\r\n        if self._activation == ""Sigmoid"":\r\n            return tf.nn.sigmoid(_norm)\r\n        return _norm\r\n\r\n\r\nclass ConvDrop(ConvLayer, Dropout, metaclass=ConvSubMeta):\r\n    pass\r\n\r\n\r\nclass ConvNorm(ConvLayer, Normalize, metaclass=ConvSubMeta):\r\n    pass\r\n\r\n\r\n# Cost Layers\r\n\r\nclass CostLayer(Layer):\r\n    @property\r\n    def info(self):\r\n        return ""Cost   :  {:<16s}"".format(self.name)\r\n\r\n    def calculate(self, y, y_pred):\r\n        return self._activate(y_pred, y)\r\n\r\n\r\nclass CrossEntropy(CostLayer):\r\n    def _activate(self, x, y):\r\n        return tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=x, labels=y))\r\n\r\n\r\nclass MSE(CostLayer):\r\n    def _activate(self, x, y):\r\n        return tf.reduce_mean(tf.square(x - y))\r\n\r\n\r\n# Factory\r\n\r\nclass LayerFactory:\r\n    available_root_layers = {\r\n\r\n        # Normal Layers\r\n        ""Tanh"": Tanh, ""Sigmoid"": Sigmoid,\r\n        ""ELU"": ELU, ""ReLU"": ReLU, ""Softplus"": Softplus,\r\n        ""Identical"": Identical,\r\n        ""CF0910"": CF0910,\r\n\r\n        # Cost Layers\r\n        ""CrossEntropy"": CrossEntropy, ""MSE"": MSE,\r\n\r\n        # Conv Layers\r\n        ""ConvTanh"": ConvTanh, ""ConvSigmoid"": ConvSigmoid,\r\n        ""ConvELU"": ConvELU, ""ConvReLU"": ConvReLU, ""ConvSoftplus"": ConvSoftplus,\r\n        ""ConvIdentical"": ConvIdentical,\r\n        ""ConvCF0910"": ConvCF0910,\r\n        ""MaxPool"": MaxPool, ""AvgPool"": AvgPool\r\n    }\r\n    available_special_layers = {\r\n        ""Dropout"": Dropout,\r\n        ""Normalize"": Normalize,\r\n        ""ConvDrop"": ConvDrop,\r\n        ""ConvNorm"": ConvNorm\r\n    }\r\n\r\n    def get_root_layer_by_name(self, name, *args, **kwargs):\r\n        if name not in self.available_special_layers:\r\n            if name in self.available_root_layers:\r\n                name = self.available_root_layers[name]\r\n            else:\r\n                raise BuildNetworkError(""Undefined layer \'{}\' found"".format(name))\r\n            return name(*args, **kwargs)\r\n        return None\r\n\r\n    def get_layer_by_name(self, name, parent, current_dimension, *args, **kwargs):\r\n        _layer = self.get_root_layer_by_name(name, *args, **kwargs)\r\n        if _layer:\r\n            return _layer, None\r\n        _current, _next = parent.shape[1], current_dimension\r\n        _layer = self.available_special_layers[name]\r\n        if args:\r\n            _layer = _layer(parent, (_current, _next), *args, **kwargs)\r\n        else:\r\n            _layer = _layer(parent, (_current, _next), **kwargs)\r\n        return _layer, (_current, _next)\r\n'"
NN/TF/Networks.py,45,"b'import os\r\nimport cv2\r\nimport time\r\nimport pickle\r\nimport datetime\r\nimport matplotlib.pyplot as plt\r\nfrom math import floor, sqrt\r\nfrom mpl_toolkits.mplot3d import Axes3D\r\nfrom tensorflow.python.framework import graph_io\r\nfrom tensorflow.python.tools import freeze_graph\r\n\r\nfrom NN.TF.Layers import *\r\n\r\nfrom Util.Util import Util, VisUtil\r\nfrom Util.Bases import TFClassifierBase\r\nfrom Util.ProgressBar import ProgressBar\r\n\r\n\r\n# TODO: Saving NNPipe; Add \'idx\' param to \'get_rs\' method\r\n\r\n\r\nclass NNVerbose:\r\n    NONE = 0\r\n    EPOCH = 1\r\n    ITER = 1.5\r\n    METRICS = 2\r\n    METRICS_DETAIL = 3\r\n    DETAIL = 4\r\n    DEBUG = 5\r\n\r\n\r\nclass NNConfig:\r\n    BOOST_LESS_SAMPLES = False\r\n    TRAINING_SCALE = 5 / 6\r\n    BATCH_SIZE = 1e6\r\n\r\n\r\n# Neural Network\r\n\r\nclass NNBase(TFClassifierBase):\r\n    NNTiming = Timing()\r\n\r\n    def __init__(self, **kwargs):\r\n        super(NNBase, self).__init__(**kwargs)\r\n        self._layers = []\r\n        self._optimizer = None\r\n        self._w_stds, self._b_inits = [], []\r\n        self._layer_names, self._layer_params = [], []\r\n        self._lr = 0\r\n        self.verbose = 1\r\n        self._current_dimension = 0\r\n\r\n        self._logs = {}\r\n        self._metrics, self._metric_names, self._metric_rs = [], [], []\r\n\r\n        self._loaded = False\r\n        self._x_min = self._x_max = self._y_min = self._y_max = 0\r\n        self._transferred_flags = {""train"": False, ""test"": False}\r\n\r\n        self._activations = None\r\n        self._loss = self._train_step = None\r\n        self._layer_factory = LayerFactory()\r\n        self._tf_weights, self._tf_bias = [], []\r\n\r\n    @property\r\n    def name(self):\r\n        return (\r\n            ""-"".join([str(_layer.shape[1]) for _layer in self._layers]) +\r\n            "" at {}"".format(time.strftime(""%Y-%m-%d %H:%M:%S"", time.localtime()))\r\n        )\r\n\r\n    @NNTiming.timeit(level=4)\r\n    def _get_w(self, shape):\r\n        if self._w_stds[-1] is None:\r\n            self._w_stds[-1] = sqrt(2 / sum(shape))\r\n        initial = tf.truncated_normal(shape, stddev=self._w_stds[-1])\r\n        return tf.Variable(initial, name=""w"")\r\n\r\n    @NNTiming.timeit(level=4)\r\n    def _get_b(self, shape):\r\n        return tf.Variable(np.zeros(shape, dtype=np.float32) + self._b_inits[-1], name=""b"")\r\n\r\n    @NNTiming.timeit(level=4)\r\n    def _get_tb_name(self, layer):\r\n        return ""{}_{}"".format(layer.position, layer.name)\r\n\r\n    @staticmethod\r\n    @NNTiming.timeit(level=4)\r\n    def _summary_var(var):\r\n        with tf.name_scope(""summaries""):\r\n            mean = tf.reduce_mean(var)\r\n            tf.summary.scalar(""mean"", mean)\r\n            with tf.name_scope(""std""):\r\n                stddev = tf.sqrt(tf.reduce_mean(tf.square(var - mean)))\r\n            tf.summary.scalar(""std"", stddev)\r\n\r\n    # noinspection PyTypeChecker\r\n    @NNTiming.timeit(level=4)\r\n    def _add_params(self, layer, shape, conv_channel=None, fc_shape=None, apply_bias=True):\r\n        if fc_shape is not None:\r\n            w_shape = (fc_shape, shape[1])\r\n            b_shape = shape[1],\r\n        elif conv_channel is not None:\r\n            if len(shape[1]) <= 2:\r\n                w_shape = shape[1][0], shape[1][1], conv_channel, conv_channel\r\n            else:\r\n                w_shape = (shape[1][1], shape[1][2], conv_channel, shape[1][0])\r\n            b_shape = shape[1][0],\r\n        else:\r\n            w_shape = shape\r\n            b_shape = shape[1],\r\n        _new_w = self._get_w(w_shape)\r\n        _new_b = self._get_b(b_shape) if apply_bias else None\r\n        self._tf_weights.append(_new_w)\r\n        if apply_bias:\r\n            self._tf_bias.append(_new_b)\r\n        else:\r\n            self._tf_bias.append(None)\r\n        with tf.name_scope(self._get_tb_name(layer)):\r\n            with tf.name_scope(""weight""):\r\n                NNBase._summary_var(_new_w)\r\n            if layer.apply_bias:\r\n                with tf.name_scope(""bias""):\r\n                    NNBase._summary_var(_new_b)\r\n\r\n    @NNTiming.timeit(level=4)\r\n    def _add_param_placeholder(self):\r\n        self._tf_weights.append(tf.constant([.0]))\r\n        self._tf_bias.append(tf.constant([.0]))\r\n\r\n    @NNTiming.timeit(level=4)\r\n    def _add_layer(self, layer, *args, **kwargs):\r\n        if not self._layers and isinstance(layer, str):\r\n            if layer.lower() == ""pipe"":\r\n                self._layers.append(NNPipe(args[0]))\r\n                self._add_param_placeholder()\r\n                return\r\n            _layer = self._layer_factory.get_root_layer_by_name(layer, *args, **kwargs)\r\n            if _layer:\r\n                self.add(_layer, pop_last_init=True)\r\n                return\r\n        _parent = self._layers[-1]\r\n        if isinstance(_parent, CostLayer):\r\n            raise BuildLayerError(""Adding layer after CostLayer is not permitted"")\r\n        if isinstance(_parent, NNPipe):\r\n            self._current_dimension = _parent.shape[1]\r\n        if isinstance(layer, str):\r\n            if layer.lower() == ""pipe"":\r\n                self._layers.append(NNPipe(args[0]))\r\n                self._add_param_placeholder()\r\n                return\r\n            layer, shape = self._layer_factory.get_layer_by_name(\r\n                layer, _parent, self._current_dimension, *args, **kwargs\r\n            )\r\n            if shape is None:\r\n                self.add(layer, pop_last_init=True)\r\n                return\r\n            _current, _next = shape\r\n        else:\r\n            _current, _next = args\r\n        if isinstance(layer, SubLayer):\r\n            if _current != _parent.shape[1]:\r\n                raise BuildLayerError(""Output shape should be identical with input shape ""\r\n                                      ""if chosen SubLayer is not a CostLayer"")\r\n            self.parent = _parent\r\n            self._layers.append(layer)\r\n            self._add_param_placeholder()\r\n            self._current_dimension = _next\r\n        else:\r\n            fc_shape, conv_channel, last_layer = None, None, self._layers[-1]\r\n            if NNBase._is_conv(last_layer):\r\n                if NNBase._is_conv(layer):\r\n                    conv_channel = last_layer.n_filters\r\n                    _current = (conv_channel, last_layer.out_h, last_layer.out_w)\r\n                    layer.feed_shape((_current, _next))\r\n                else:\r\n                    layer.is_fc = True\r\n                    fc_shape = last_layer.out_h * last_layer.out_w * last_layer.n_filters\r\n            self._layers.append(layer)\r\n            if isinstance(layer, ConvPoolLayer):\r\n                self._add_param_placeholder()\r\n            else:\r\n                self._add_params(layer, (_current, _next), conv_channel, fc_shape)\r\n            self._current_dimension = _next\r\n        self._update_layer_information(layer)\r\n\r\n    @NNTiming.timeit(level=4)\r\n    def _update_layer_information(self, layer):\r\n        self._layer_params.append(layer.params)\r\n        if len(self._layer_params) > 1 and not layer.is_sub_layer:\r\n            self._layer_params[-1] = ((self._layer_params[-1][0][1],), *self._layer_params[-1][1:])\r\n\r\n    @staticmethod\r\n    @NNTiming.timeit(level=4)\r\n    def _is_conv(layer):\r\n        return isinstance(layer, ConvLayer) or isinstance(layer, NNPipe)\r\n\r\n    @NNTiming.timeit(level=1, prefix=""[API] "")\r\n    def get_rs(self, x, predict=True, pipe=False):\r\n        if not isinstance(self._layers[0], NNPipe):\r\n            cache = self._layers[0].activate(x, self._tf_weights[0], self._tf_bias[0], predict)\r\n        else:\r\n            cache = self._layers[0].get_rs(x, predict)\r\n        for i, layer in enumerate(self._layers[1:]):\r\n            if i == len(self._layers) - 2:\r\n                if not pipe:\r\n                    if NNDist._is_conv(self._layers[i]):\r\n                        fc_shape = np.prod(cache.get_shape()[1:])  # type: int\r\n                        cache = tf.reshape(cache, [-1, int(fc_shape)])\r\n                    if self._tf_bias[-1] is not None:\r\n                        return tf.matmul(cache, self._tf_weights[-1]) + self._tf_bias[-1]\r\n                    return tf.matmul(cache, self._tf_weights[-1])\r\n                else:\r\n                    if not isinstance(layer, NNPipe):\r\n                        return layer.activate(cache, self._tf_weights[i + 1], self._tf_bias[i + 1], predict)\r\n                    return layer.get_rs(cache, predict)\r\n            if not isinstance(layer, NNPipe):\r\n                cache = layer.activate(cache, self._tf_weights[i + 1], self._tf_bias[i + 1], predict)\r\n            else:\r\n                cache = layer.get_rs(cache, predict)\r\n\r\n    @NNTiming.timeit(level=4, prefix=""[API] "")\r\n    def add(self, layer, *args, **kwargs):\r\n        # Init kwargs\r\n        kwargs[""apply_bias""] = kwargs.get(""apply_bias"", True)\r\n        kwargs[""position""] = kwargs.get(""position"", len(self._layers) + 1)\r\n\r\n        self._w_stds.append(kwargs.pop(""w_std"", None))\r\n        self._b_inits.append(kwargs.pop(""b_init"", 0.1))\r\n        if kwargs.pop(""pop_last_init"", False):\r\n            self._w_stds.pop()\r\n            self._b_inits.pop()\r\n        if isinstance(layer, str):\r\n            # noinspection PyTypeChecker\r\n            self._add_layer(layer, *args, **kwargs)\r\n        else:\r\n            if not isinstance(layer, Layer):\r\n                raise BuildLayerError(""Invalid Layer provided (should be subclass of Layer)"")\r\n            if not self._layers:\r\n                if isinstance(layer, SubLayer):\r\n                    raise BuildLayerError(""Invalid Layer provided (first layer should not be subclass of SubLayer)"")\r\n                if len(layer.shape) != 2:\r\n                    raise BuildLayerError(""Invalid input Layer provided (shape should be {}, {} found)"".format(\r\n                        2, len(layer.shape)\r\n                    ))\r\n                self._layers, self._current_dimension = [layer], layer.shape[1]\r\n                self._update_layer_information(layer)\r\n                if isinstance(layer, ConvLayer):\r\n                    self._add_params(layer, layer.shape, layer.n_channels)\r\n                else:\r\n                    self._add_params(layer, layer.shape)\r\n            else:\r\n                if len(layer.shape) > 2:\r\n                    raise BuildLayerError(""Invalid Layer provided (shape should be {}, {} found)"".format(\r\n                        2, len(layer.shape)\r\n                    ))\r\n                if len(layer.shape) == 2:\r\n                    _current, _next = layer.shape\r\n                    if isinstance(layer, SubLayer):\r\n                        if _next != self._current_dimension:\r\n                            raise BuildLayerError(""Invalid SubLayer provided (shape[1] should be {}, {} found)"".format(\r\n                                self._current_dimension, _next\r\n                            ))\r\n                    elif not NNDist._is_conv(layer) and _current != self._current_dimension:\r\n                        raise BuildLayerError(""Invalid Layer provided (shape[0] should be {}, {} found)"".format(\r\n                            self._current_dimension, _current\r\n                        ))\r\n                    self._add_layer(layer, _current, _next)\r\n                elif len(layer.shape) == 1:\r\n                    _next = layer.shape[0]\r\n                    layer.shape = (self._current_dimension, _next)\r\n                    self._add_layer(layer, self._current_dimension, _next)\r\n                else:\r\n                    raise LayerError(""Invalid Layer provided (invalid shape \'{}\' found)"".format(layer.shape))\r\n\r\n    @NNTiming.timeit(level=4, prefix=""[API] "")\r\n    def add_pipe_layer(self, idx, layer, shape=None, *args, **kwargs):\r\n        _last_layer = self._layers[-1]\r\n        if len(self._layers) == 1:\r\n            _last_parent = None\r\n        else:\r\n            _last_parent = self._layers[-2]\r\n        if not isinstance(_last_layer, NNPipe):\r\n            raise BuildLayerError(""Adding pipe layers to a non-NNPipe object is not allowed"")\r\n        if not _last_layer.initialized[idx] and len(shape) == 1:\r\n            if _last_parent is None:\r\n                raise BuildLayerError(""Adding pipe layers at first without input shape is not allowed"")\r\n            _dim = (_last_parent.n_filters, _last_parent.out_h, _last_parent.out_w)\r\n            shape = (_dim, shape[0])\r\n        _last_layer.add(idx, layer, shape, *args, **kwargs)\r\n\r\n    @NNTiming.timeit(level=4, prefix=""[API] "")\r\n    def preview(self, verbose=0):\r\n        if not self._layers:\r\n            rs = ""None""\r\n        else:\r\n            rs = (\r\n                ""Input  :  {:<16s} - {}\\n"".format(""Dimension"", self._layers[0].shape[0]) +\r\n                ""\\n"".join([_layer.info for _layer in self._layers]))\r\n        print(""="" * 30 + ""\\n"" + ""Structure\\n"" + ""-"" * 30 + ""\\n"" + rs + ""\\n"" + ""-"" * 30)\r\n        if verbose >= 1:\r\n            print(""Initial Values\\n"" + ""-"" * 30)\r\n            print(""\\n"".join([""({:^16s}) w_std: {:8.6} ; b_init: {:8.6}"".format(\r\n                batch[0].name, float(batch[1]), float(batch[2])) if not isinstance(\r\n                batch[0], NNPipe) else ""({:^16s}) ({:^3d})"".format(\r\n                ""Pipe"", len(batch[0][""nn_lst""])\r\n            ) for batch in zip(self._layers, self._w_stds, self._b_inits) if not isinstance(\r\n                batch[0], SubLayer) and not isinstance(\r\n                batch[0], CostLayer) and not isinstance(\r\n                batch[0], ConvPoolLayer)])\r\n                  )\r\n        if verbose >= 2:\r\n            for layer in self._layers:\r\n                if isinstance(layer, NNPipe):\r\n                    layer.preview()\r\n        print(""-"" * 30)\r\n\r\n\r\nclass NNDist(NNBase):\r\n    NNTiming = Timing()\r\n\r\n    def __init__(self, **kwargs):\r\n        super(NNDist, self).__init__(**kwargs)\r\n\r\n        self._sess = tf.Session()\r\n        self._optimizer_factory = OptFactory()\r\n\r\n        self._available_metrics = {\r\n            ""acc"": self.acc, ""_acc"": self.acc,\r\n            ""f1"": self.f1_score, ""_f1_score"": self.f1_score\r\n        }\r\n\r\n    @NNTiming.timeit(level=4, prefix=""[Initialize] "")\r\n    def initialize(self):\r\n        self._layers = []\r\n        self._optimizer = None\r\n        self._w_stds, self._b_inits = [], []\r\n        self._layer_names, self._layer_params = [], []\r\n        self._lr = 0\r\n        self.verbose = 1\r\n        self._current_dimension = 0\r\n\r\n        self._logs = {}\r\n        self._metrics, self._metric_names, self._metric_rs = [], [], []\r\n\r\n        self._loaded = False\r\n        self._x_min = self._x_max = self._y_min = self._y_max = 0\r\n        self._transferred_flags = {""train"": False, ""test"": False}\r\n\r\n        self._activations = None\r\n        self._loss = self._train_step = None\r\n        self._layer_factory = LayerFactory()\r\n        self._tf_weights, self._tf_bias = [], []\r\n\r\n        self._sess = tf.Session()\r\n\r\n    # Property\r\n\r\n    @property\r\n    def layer_names(self):\r\n        return [layer.name for layer in self._layers]\r\n\r\n    @layer_names.setter\r\n    def layer_names(self, value):\r\n        self._layer_names = value\r\n\r\n    @property\r\n    def layer_special_params(self):\r\n        return [layer.get_special_params(self._sess) for layer in self._layers]\r\n\r\n    @layer_special_params.setter\r\n    def layer_special_params(self, value):\r\n        for layer, sp_param in zip(self._layers, value):\r\n            if sp_param is not None:\r\n                layer.set_special_params(sp_param)\r\n\r\n    @property\r\n    def optimizer(self):\r\n        return self._optimizer.name\r\n\r\n    @optimizer.setter\r\n    def optimizer(self, value):\r\n        self._optimizer = value\r\n\r\n    # Utils\r\n\r\n    @staticmethod\r\n    @NNTiming.timeit(level=4, prefix=""[Private StaticMethod] "")\r\n    def _transfer_x(x):\r\n        if len(x.shape) == 1:\r\n            x = x.reshape(1, -1)\r\n        if len(x.shape) == 4:\r\n            x = x.transpose(0, 2, 3, 1)\r\n        return x.astype(np.float32)\r\n\r\n    @NNTiming.timeit(level=4)\r\n    def _feed_data(self, x, y):\r\n        if not self._transferred_flags[""train""]:\r\n            x = NNDist._transfer_x(x)\r\n            self._transferred_flags[""train""] = True\r\n        y = np.asarray(y, dtype=np.float32)\r\n        if len(x) != len(y):\r\n            raise BuildNetworkError(""Data fed to network should be identical in length, x: {} and y: {} found"".format(\r\n                len(x), len(y)\r\n            ))\r\n        self._x_min, self._x_max = np.min(x), np.max(x)\r\n        self._y_min, self._y_max = np.min(y), np.max(y)\r\n        return x, y\r\n\r\n    @NNTiming.timeit(level=2)\r\n    def _get_prediction(self, x, name=None, verbose=None):\r\n        if verbose is None:\r\n            verbose = self.verbose\r\n        fc_shape = np.prod(x.shape[1:])  # type: int\r\n        single_batch = int(NNConfig.BATCH_SIZE / fc_shape)\r\n        if not single_batch:\r\n            single_batch = 1\r\n        if not single_batch:\r\n            single_batch = 1\r\n        if single_batch >= len(x):\r\n            return self._sess.run(self._y_pred, {self._tfx: x})\r\n        epoch = int(len(x) / single_batch)\r\n        if not len(x) % single_batch:\r\n            epoch += 1\r\n        name = ""Prediction"" if name is None else ""Prediction ({})"".format(name)\r\n        sub_bar = ProgressBar(max_value=epoch, name=name, start=False)\r\n        if verbose >= NNVerbose.METRICS:\r\n            sub_bar.start()\r\n        rs, count = [], 0\r\n        while count < len(x):\r\n            count += single_batch\r\n            if count >= len(x):\r\n                rs.append(self._sess.run(self._y_pred, {self._tfx: x[count - single_batch:]}))\r\n            else:\r\n                rs.append(self._sess.run(self._y_pred, {self._tfx: x[count - single_batch:count]}))\r\n            if verbose >= NNVerbose.METRICS:\r\n                sub_bar.update()\r\n        return np.vstack(rs)\r\n\r\n    @NNTiming.timeit(level=4)\r\n    def _get_activations(self, x, predict=False):\r\n        if not isinstance(self._layers[0], NNPipe):\r\n            activations = [self._layers[0].activate(x, self._tf_weights[0], self._tf_bias[0], predict)]\r\n        else:\r\n            activations = [self._layers[0].get_rs(x, predict)]\r\n        for i, layer in enumerate(self._layers[1:]):\r\n            if i == len(self._layers) - 2:\r\n                if NNDist._is_conv(self._layers[i]):\r\n                    fc_shape = np.prod(activations[-1].get_shape()[1:])  # type: int\r\n                    activations[-1] = tf.reshape(activations[-1], [-1, int(fc_shape)])\r\n                if self._tf_bias[-1] is not None:\r\n                    activations.append(tf.matmul(activations[-1], self._tf_weights[-1]) + self._tf_bias[-1])\r\n                else:\r\n                    activations.append(tf.matmul(activations[-1], self._tf_weights[-1]))\r\n            else:\r\n                if not isinstance(layer, NNPipe):\r\n                    activations.append(layer.activate(\r\n                        activations[-1], self._tf_weights[i + 1], self._tf_bias[i + 1], predict))\r\n                else:\r\n                    activations.append(layer.get_rs(activations[-1], predict))\r\n        return activations\r\n\r\n    @NNTiming.timeit(level=1)\r\n    def _get_l2_losses(self, lb):\r\n        if lb <= 0:\r\n            return 0.\r\n        return [lb * tf.nn.l2_loss(w) for l, w in zip(self._layers, self._tf_weights)\r\n                if not isinstance(l, SubLayer) and not isinstance(l, ConvPoolLayer)]\r\n\r\n    @NNTiming.timeit(level=1)\r\n    def _get_acts(self, x):\r\n        with self._sess.as_default():\r\n            activations = [_ac.eval() for _ac in self._get_activations(x, True)]\r\n        return activations\r\n\r\n    @NNTiming.timeit(level=3)\r\n    def _append_log(self, x, y, y_pred, name, get_loss):\r\n        if y_pred is None:\r\n            y_pred = self._get_prediction(x, name)\r\n        for i, metric_rs in enumerate(self._metric_rs):\r\n            self._logs[name][i].append(metric_rs.eval({\r\n                self._tfy: y, self._y_pred: y_pred\r\n            }))\r\n        if get_loss:\r\n            self._logs[name][-1].append(\r\n                self._loss.eval({self._tfy: y, self._y_pred: y_pred})\r\n            )\r\n\r\n    @NNTiming.timeit(level=3)\r\n    def _print_metric_logs(self, name, show_loss):\r\n        print()\r\n        print(""="" * 47)\r\n        for i, metric in enumerate(self._metric_names):\r\n            print(""{:<16s} {:<16s}: {:12.8}"".format(\r\n                name, metric, self._logs[name][i][-1]))\r\n        if show_loss:\r\n            print(""{:<16s} {:<16s}: {:12.8}"".format(\r\n                name, ""loss"", self._logs[name][-1][-1]))\r\n        print(""="" * 47)\r\n\r\n    @NNTiming.timeit(level=1)\r\n    def _draw_2d_network(self, radius=6, width=1200, height=800, padding=0.2,\r\n                         plot_scale=2, plot_precision=0.03,\r\n                         sub_layer_height_scale=0, **kwargs):\r\n        if not kwargs[""show""] and not kwargs[""mp4""]:\r\n            return\r\n        layers = len(self._layers) + 1\r\n        units = [layer.shape[0] for layer in self._layers] + [self._layers[-1].shape[1]]\r\n        whether_sub_layers = np.array([False] + [isinstance(layer, SubLayer) for layer in self._layers])\r\n        n_sub_layers = np.sum(whether_sub_layers)  # type: int\r\n\r\n        plot_num = int(1 / plot_precision)\r\n        if plot_num % 2 == 1:\r\n            plot_num += 1\r\n        half_plot_num = int(plot_num * 0.5)\r\n        xf = np.linspace(self._x_min * plot_scale, self._x_max * plot_scale, plot_num)\r\n        yf = np.linspace(self._x_min * plot_scale, self._x_max * plot_scale, plot_num) * -1\r\n        input_x, input_y = np.meshgrid(xf, yf)\r\n        input_xs = np.c_[input_x.ravel().astype(np.float32), input_y.ravel().astype(np.float32)]\r\n\r\n        activations = self._sess.run(self._activations, {self._tfx: input_xs})\r\n        activations = [activation.T.reshape(units[i + 1], plot_num, plot_num)\r\n                       for i, activation in enumerate(activations)]\r\n        graphs = []\r\n        for j, activation in enumerate(activations):\r\n            graph_group = []\r\n            if j == len(activations) - 1:\r\n                classes = np.argmax(activation, axis=0)\r\n            else:\r\n                classes = None\r\n            for k, ac in enumerate(activation):\r\n                data = np.zeros((plot_num, plot_num, 3), np.uint8)\r\n                if j != len(activations) - 1:\r\n                    mask = ac >= np.average(ac)\r\n                else:\r\n                    mask = classes == k\r\n                data[mask], data[~mask] = [0, 165, 255], [255, 165, 0]\r\n                graph_group.append(data)\r\n            graphs.append(graph_group)\r\n\r\n        img = np.full([height, width, 3], 255, dtype=np.uint8)\r\n        axis0_padding = int(height / (layers - 1 + 2 * padding)) * padding + plot_num\r\n        axis0_step = (height - 2 * axis0_padding) / layers\r\n        sub_layer_decrease = int((1 - sub_layer_height_scale) * axis0_step)\r\n        axis0 = np.linspace(\r\n            axis0_padding,\r\n            height + n_sub_layers * sub_layer_decrease - axis0_padding,\r\n            layers, dtype=np.int)\r\n        axis0 -= sub_layer_decrease * np.cumsum(whether_sub_layers)\r\n        axis1_padding = plot_num\r\n        axis1 = [np.linspace(axis1_padding, width - axis1_padding, unit + 2, dtype=np.int)\r\n                 for unit in units]\r\n        axis1 = [axis[1:-1] for axis in axis1]\r\n\r\n        colors, thicknesses, masks = [], [], []\r\n        for weight in self._tf_weights:\r\n            line_info = VisUtil.get_line_info(weight.eval())\r\n            colors.append(line_info[0])\r\n            thicknesses.append(line_info[1])\r\n            masks.append(line_info[2])\r\n\r\n        for i, (y, xs) in enumerate(zip(axis0, axis1)):\r\n            for j, x in enumerate(xs):\r\n                if i == 0:\r\n                    cv2.circle(img, (x, y), radius, (20, 215, 20), int(radius / 2))\r\n                else:\r\n                    graph = graphs[i - 1][j]\r\n                    img[y - half_plot_num:y + half_plot_num, x - half_plot_num:x + half_plot_num] = graph\r\n            if i > 0:\r\n                cv2.putText(img, self._layers[i - 1].name, (12, y - 36), cv2.LINE_AA, 0.6, (0, 0, 0), 1)\r\n        for i, y in enumerate(axis0):\r\n            if i == len(axis0) - 1:\r\n                break\r\n            for j, x in enumerate(axis1[i]):\r\n                new_y = axis0[i + 1]\r\n                whether_sub_layer = isinstance(self._layers[i], SubLayer)\r\n                for k, new_x in enumerate(axis1[i + 1]):\r\n                    if whether_sub_layer and j != k:\r\n                        continue\r\n                    if masks[i][j][k]:\r\n                        cv2.line(img, (x, y + half_plot_num), (new_x, new_y - half_plot_num),\r\n                                 colors[i][j][k], thicknesses[i][j][k])\r\n\r\n        return img\r\n\r\n    # Init\r\n\r\n    @NNTiming.timeit(level=4)\r\n    def _init_optimizer(self, optimizer=None):\r\n        if optimizer is None:\r\n            if isinstance(self._optimizer, str):\r\n                optimizer = self._optimizer\r\n            else:\r\n                if self._optimizer is None:\r\n                    self._optimizer = Adam(self._lr)\r\n                if isinstance(self._optimizer, Optimizer):\r\n                    return\r\n                raise BuildNetworkError(""Invalid optimizer \'{}\' provided"".format(self._optimizer))\r\n        if isinstance(optimizer, str):\r\n            self._optimizer = self._optimizer_factory.get_optimizer_by_name(optimizer, self._lr)\r\n        elif isinstance(optimizer, Optimizer):\r\n            self._optimizer = optimizer\r\n        else:\r\n            raise BuildNetworkError(""Invalid optimizer \'{}\' provided"".format(optimizer))\r\n\r\n    @NNTiming.timeit(level=4)\r\n    def _init_layers(self):\r\n        for _layer in self._layers:\r\n            _layer.init(self._sess)\r\n\r\n    @NNTiming.timeit(level=4)\r\n    def _init_structure(self, verbose):\r\n        x_shape = self._layers[0].shape[0]\r\n        if isinstance(x_shape, int):\r\n            x_shape = x_shape,\r\n        y_shape = self._layers[-1].shape[1]\r\n        x_placeholder, y_placeholder = np.zeros((1, *x_shape)), np.zeros((1, y_shape))\r\n        self.fit(x_placeholder, y_placeholder, epoch=0, train_only=True, verbose=verbose)\r\n        self._transferred_flags[""train""] = False\r\n\r\n    @NNTiming.timeit(level=4)\r\n    def _init_train_step(self, sess):\r\n        if not self._loaded:\r\n            self._train_step = self._optimizer.minimize(self._loss)\r\n            sess.run(tf.global_variables_initializer())\r\n        else:\r\n            _var_cache = set(tf.global_variables())\r\n            self._train_step = self._optimizer.minimize(self._loss)\r\n            sess.run(tf.variables_initializer(set(tf.global_variables()) - _var_cache))\r\n\r\n    # Batch Work\r\n\r\n    @NNTiming.timeit(level=2)\r\n    def _batch_work(self, i, bar, counter, x_train, y_train, x_test, y_test, show_loss, condition,\r\n                    tensorboard_verbose, train_repeat, sess, train_merge_op, test_merge_op,\r\n                    train_writer, test_writer):\r\n        if tensorboard_verbose > 0:\r\n            count = counter * train_repeat + i\r\n            y_train_pred = self.predict(x_train, get_raw_results=True, transfer_x=False)\r\n            y_test_pred = self.predict(x_test, get_raw_results=True, transfer_x=False)\r\n            train_summary = sess.run(train_merge_op, feed_dict={\r\n                self._tfy: y_train, self._y_pred: y_train_pred\r\n            })\r\n            test_summary = sess.run(test_merge_op, feed_dict={\r\n                self._tfy: y_test, self._y_pred: y_test_pred\r\n            })\r\n            train_writer.add_summary(train_summary, count)\r\n            test_writer.add_summary(test_summary, count)\r\n        else:\r\n            y_train_pred = y_test_pred = None\r\n        if bar is not None:\r\n            condition = bar.update() and condition\r\n        if condition:\r\n            self._append_log(x_train, y_train, y_train_pred, ""Train"", show_loss)\r\n            self._append_log(x_test, y_test, y_test_pred, ""Test"", show_loss)\r\n            self._print_metric_logs(""Train"", show_loss)\r\n            self._print_metric_logs(""Test"", show_loss)\r\n\r\n    # API\r\n\r\n    @NNTiming.timeit(level=4, prefix=""[API] "")\r\n    def get_current_pipe(self, idx):\r\n        _last_layer = self._layers[-1]\r\n        if not isinstance(_last_layer, NNPipe):\r\n            return\r\n        return _last_layer[""nn_lst""][idx]\r\n\r\n    @NNTiming.timeit(level=4, prefix=""[API] "")\r\n    def build(self, units=""load""):\r\n        if isinstance(units, str):\r\n            if units == ""load"":\r\n                for name, param in zip(self._layer_names, self._layer_params):\r\n                    self.add(name, *param)\r\n            else:\r\n                raise NotImplementedError(""Invalid param \'{}\' provided to \'build\' method"".format(units))\r\n        else:\r\n            try:\r\n                units = np.asarray(units).flatten().astype(np.int)\r\n            except ValueError as err:\r\n                raise BuildLayerError(err)\r\n            if len(units) < 2:\r\n                raise BuildLayerError(""At least 2 layers are needed"")\r\n            _input_shape = (units[0], units[1])\r\n            self.initialize()\r\n            self.add(""ReLU"", _input_shape)\r\n            for unit_num in units[2:-1]:\r\n                self.add(""ReLU"", (unit_num,))\r\n            self.add(""CrossEntropy"", (units[-1],))\r\n\r\n    @NNTiming.timeit(level=4, prefix=""[API] "")\r\n    def split_data(self, x, y, x_test, y_test,\r\n                   train_only, training_scale=NNConfig.TRAINING_SCALE):\r\n        if train_only:\r\n            if x_test is not None and y_test is not None:\r\n                if not self._transferred_flags[""test""]:\r\n                    x, y = np.vstack((x, NNDist._transfer_x(np.asarray(x_test)))), np.vstack((y, y_test))\r\n                    self._transferred_flags[""test""] = True\r\n            x_train = x_test = x.astype(np.float32)\r\n            y_train = y_test = y.astype(np.float32)\r\n        else:\r\n            shuffle_suffix = np.random.permutation(len(x))\r\n            x, y = x[shuffle_suffix], y[shuffle_suffix]\r\n            if x_test is None or y_test is None:\r\n                train_len = int(len(x) * training_scale)\r\n                x_train, y_train = x[:train_len], y[:train_len]\r\n                x_test, y_test = x[train_len:], y[train_len:]\r\n            else:\r\n                x_train, y_train = x, y\r\n                if not self._transferred_flags[""test""]:\r\n                    x_test, y_test = NNDist._transfer_x(np.asarray(x_test)), np.asarray(y_test, dtype=np.float32)\r\n                    self._transferred_flags[""test""] = True\r\n        if NNConfig.BOOST_LESS_SAMPLES:\r\n            if y_train.shape[1] != 2:\r\n                raise BuildNetworkError(""It is not permitted to boost less samples in multiple classification"")\r\n            y_train_arg = np.argmax(y_train, axis=1)\r\n            y0 = y_train_arg == 0\r\n            y1 = ~y0\r\n            y_len, y0_len = len(y_train), np.sum(y0)  # type: int\r\n            if y0_len > int(0.5 * y_len):\r\n                y0, y1 = y1, y0\r\n                y0_len = y_len - y0_len\r\n            boost_suffix = np.random.randint(y0_len, size=y_len - y0_len)\r\n            x_train = np.vstack((x_train[y1], x_train[y0][boost_suffix]))\r\n            y_train = np.vstack((y_train[y1], y_train[y0][boost_suffix]))\r\n            shuffle_suffix = np.random.permutation(len(x_train))\r\n            x_train, y_train = x_train[shuffle_suffix], y_train[shuffle_suffix]\r\n        return (x_train, x_test), (y_train, y_test)\r\n\r\n    @NNTiming.timeit(level=1, prefix=""[API] "")\r\n    def fit(self,\r\n            x, y, x_test=None, y_test=None,\r\n            lr=0.001, lb=0.001, epoch=10, weight_scale=1,\r\n            batch_size=128, record_period=1, train_only=False, optimizer=None,\r\n            show_loss=True, metrics=None, do_log=False, verbose=None,\r\n            tensorboard_verbose=0, animation_params=None):\r\n        x, y = self._feed_data(x, y)\r\n        self._lr = lr\r\n        self._init_optimizer(optimizer)\r\n        print(""Optimizer: "", self._optimizer.name)\r\n        print(""-"" * 30)\r\n\r\n        if not self._layers:\r\n            raise BuildNetworkError(""Please provide layers before fitting data"")\r\n\r\n        if y.shape[1] != self._current_dimension:\r\n            raise BuildNetworkError(""Output layer\'s shape should be {}, {} found"".format(\r\n                self._current_dimension, y.shape[1]))\r\n\r\n        (x_train, x_test), (y_train, y_test) = self.split_data(x, y, x_test, y_test, train_only)\r\n        train_len, test_len = len(x_train), len(x_test)\r\n        batch_size = min(batch_size, train_len)\r\n        do_random_batch = train_len > batch_size\r\n        train_repeat = 1 if not do_random_batch else int(train_len / batch_size) + 1\r\n\r\n        with tf.name_scope(""Entry""):\r\n            self._tfx = tf.placeholder(tf.float32, shape=[None, *x.shape[1:]])\r\n        self._tfy = tf.placeholder(tf.float32, shape=[None, y.shape[1]])\r\n        if epoch <= 0:\r\n            return\r\n\r\n        self._metrics = [""acc""] if metrics is None else metrics\r\n        for i, metric in enumerate(self._metrics):\r\n            if isinstance(metric, str):\r\n                if metric not in self._available_metrics:\r\n                    raise BuildNetworkError(""Metric \'{}\' is not implemented"".format(metric))\r\n                self._metrics[i] = self._available_metrics[metric]\r\n        self._metric_names = [_m.__name__ for _m in self._metrics]\r\n\r\n        self._logs = {\r\n            name: [[] for _ in range(len(self._metrics) + 1)] for name in (""Train"", ""Test"")\r\n        }\r\n        if verbose is not None:\r\n            self.verbose = verbose\r\n\r\n        bar = ProgressBar(max_value=max(1, epoch // record_period), name=""Epoch"", start=False)\r\n        if self.verbose >= NNVerbose.EPOCH:\r\n            bar.start()\r\n        img = None\r\n\r\n        *animation_properties, animation_params = self._get_animation_params(animation_params)\r\n        with self._sess.as_default() as sess:\r\n            with tf.name_scope(""ActivationFlow""):\r\n                self._activations = self._get_activations(self._tfx)\r\n            self._y_pred = self._activations[-1]\r\n            l2_losses = self._get_l2_losses(lb)  # type: list\r\n            self._loss = self._layers[-1].calculate(self._tfy, self._y_pred) + tf.reduce_sum(l2_losses)\r\n            self._metric_rs = [metric(self._tfy, self._y_pred) for metric in self._metrics]\r\n            self._init_train_step(sess)\r\n            for weight in self._tf_weights:\r\n                weight *= weight_scale\r\n\r\n            if tensorboard_verbose > 0:\r\n                log_dir = os.path.join(""tbLogs"", str(datetime.datetime.now())[:19].replace("":"", ""-""))\r\n                train_dir = os.path.join(log_dir, ""train"")\r\n                test_dir = os.path.join(log_dir, ""test"")\r\n                for _dir in (log_dir, train_dir, test_dir):\r\n                    if not os.path.isdir(_dir):\r\n                        os.makedirs(_dir)\r\n                test_summary_ops = []\r\n                with tf.name_scope(""l2_loss""):\r\n                    layer_names = [\r\n                        self._get_tb_name(layer) for layer in self._layers\r\n                        if not isinstance(layer, SubLayer) and not isinstance(layer, ConvPoolLayer)\r\n                    ]\r\n                    for name, l2_loss in zip(layer_names, l2_losses):\r\n                        tf.summary.scalar(name, l2_loss)\r\n                with tf.name_scope(""GlobalSummaries""):\r\n                    test_summary_ops.append(tf.summary.scalar(""loss"", self._loss))\r\n                    for name, metric_rs in zip(self._metric_names, self._metric_rs):\r\n                        test_summary_ops.append(tf.summary.scalar(name, metric_rs))\r\n                train_merge_op = tf.summary.merge_all()\r\n                train_writer = tf.summary.FileWriter(train_dir, sess.graph)\r\n                train_writer.add_graph(sess.graph)\r\n                test_writer = tf.summary.FileWriter(test_dir)\r\n                test_merge_op = tf.summary.merge(test_summary_ops)\r\n            else:\r\n                train_writer = test_writer = train_merge_op = test_merge_op = None\r\n\r\n            args = (\r\n                x_train, y_train, x_test, y_test, show_loss,\r\n                self.verbose >= NNVerbose.METRICS_DETAIL,\r\n                tensorboard_verbose, train_repeat, sess, train_merge_op, test_merge_op,\r\n                train_writer, test_writer\r\n            )\r\n            ims = []\r\n            for counter in range(epoch):\r\n                if self.verbose >= NNVerbose.ITER and counter % record_period == 0:\r\n                    sub_bar = ProgressBar(max_value=train_repeat * record_period - 1, name=""Iteration"")\r\n                else:\r\n                    sub_bar = None\r\n                self._batch_training(\r\n                    x_train, y_train, batch_size, train_repeat,\r\n                    self._loss, self._train_step, sub_bar, counter, *args)\r\n                self._handle_animation(\r\n                    counter, x, y, ims, animation_params, *animation_properties,\r\n                    img=self._draw_2d_network(**animation_params), name=""Neural Network""\r\n                )\r\n                if (counter + 1) % record_period == 0:\r\n                    if do_log:\r\n                        self._append_log(x_train, y_train, None, ""Train"", show_loss)\r\n                        self._append_log(x_test, y_test, None,  ""Test"", show_loss)\r\n                        if self.verbose >= NNVerbose.METRICS:\r\n                            self._print_metric_logs(""Train"", show_loss)\r\n                            self._print_metric_logs(""Test"", show_loss)\r\n                    if self.verbose >= NNVerbose.EPOCH:\r\n                        bar.update(counter // record_period + 1)\r\n        if img is not None:\r\n            cv2.waitKey(0)\r\n            cv2.destroyAllWindows()\r\n        self._handle_mp4(ims, animation_properties, ""NN"")\r\n\r\n    @NNTiming.timeit(level=2, prefix=""[API] "")\r\n    def save(self, path=None, name=None, overwrite=True):\r\n        path = ""Models"" if path is None else path\r\n        name = ""Cache"" if name is None else name\r\n        folder = os.path.join(path, name)\r\n        if not os.path.exists(folder):\r\n            os.makedirs(folder)\r\n        _dir = os.path.join(folder, ""Model"")\r\n        if os.path.isfile(_dir):\r\n            if not overwrite:\r\n                _count = 1\r\n                _new_dir = _dir + ""({})"".format(_count)\r\n                while os.path.isfile(_new_dir):\r\n                    _count += 1\r\n                    _new_dir = _dir + ""({})"".format(_count)\r\n                _dir = _new_dir\r\n            else:\r\n                os.remove(_dir)\r\n\r\n        print()\r\n        print(""="" * 60)\r\n        print(""Saving Model to {}..."".format(folder))\r\n        print(""-"" * 60)\r\n\r\n        with open(_dir + "".nn"", ""wb"") as file:\r\n            # We don\'t need w_stds & b_inits when we load a model\r\n            _dic = {\r\n                ""structures"": {\r\n                    ""_lr"": self._lr,\r\n                    ""_layer_names"": self.layer_names,\r\n                    ""_layer_params"": self._layer_params,\r\n                    ""_next_dimension"": self._current_dimension\r\n                },\r\n                ""params"": {\r\n                    ""_logs"": self._logs,\r\n                    ""_metric_names"": self._metric_names,\r\n                    ""_optimizer"": self._optimizer.name,\r\n                    ""layer_special_params"": self.layer_special_params\r\n                }\r\n            }\r\n            pickle.dump(_dic, file)\r\n        saver = tf.train.Saver()\r\n        saver.save(self._sess, _dir)\r\n        graph_io.write_graph(self._sess.graph, os.path.join(path, name), ""Model.pb"", False)\r\n        with tf.name_scope(""OutputFlow""):\r\n            self.get_rs(self._tfx)\r\n        _output = """"\r\n        for op in self._sess.graph.get_operations()[::-1]:\r\n            if ""OutputFlow"" in op.name:\r\n                _output = op.name\r\n                break\r\n        with open(os.path.join(path, name, ""IO.txt""), ""w"") as file:\r\n            file.write(""\\n"".join([\r\n                ""Input  : Entry/Placeholder:0"",\r\n                ""Output : {}:0"".format(_output)\r\n            ]))\r\n        graph_io.write_graph(self._sess.graph, os.path.join(path, name), ""Cache.pb"", False)\r\n        freeze_graph.freeze_graph(\r\n            os.path.join(path, name, ""Cache.pb""),\r\n            """", True, os.path.join(path, name, ""Model""),\r\n            _output, ""save/restore_all"", ""save/Const:0"",\r\n            os.path.join(path, name, ""Frozen.pb""), True, """"\r\n        )\r\n        os.remove(os.path.join(path, name, ""Cache.pb""))\r\n\r\n        print(""Done"")\r\n        print(""="" * 60)\r\n\r\n    @NNTiming.timeit(level=2, prefix=""[API] "")\r\n    def load(self, path=None, verbose=2):\r\n        if path is None:\r\n            path = os.path.join(""Models"", ""Cache"", ""Model"")\r\n        else:\r\n            path = os.path.join(path, ""Model"")\r\n        self.initialize()\r\n        try:\r\n            with open(path + "".nn"", ""rb"") as file:\r\n                _dic = pickle.load(file)\r\n                for key, value in _dic[""structures""].items():\r\n                    setattr(self, key, value)\r\n                self.build()\r\n                for key, value in _dic[""params""].items():\r\n                    setattr(self, key, value)\r\n                self._init_optimizer()\r\n                for i in range(len(self._metric_names) - 1, -1, -1):\r\n                    name = self._metric_names[i]\r\n                    if name not in self._available_metrics:\r\n                        self._metric_names.pop(i)\r\n                    else:\r\n                        self._metrics.insert(0, self._available_metrics[name])\r\n        except Exception as err:\r\n            raise BuildNetworkError(""Failed to load Network ({}), structure initialized"".format(err))\r\n        self._loaded = True\r\n\r\n        saver = tf.train.Saver()\r\n        saver.restore(self._sess, path)\r\n        self._init_layers()\r\n        self._init_structure(verbose)\r\n\r\n        print()\r\n        print(""="" * 30)\r\n        print(""Model restored"")\r\n        print(""="" * 30)\r\n\r\n    @NNTiming.timeit(level=4, prefix=""[API] "")\r\n    def predict(self, x, get_raw_results=False, transfer_x=True):\r\n        x = np.asarray(x, dtype=np.float32)\r\n        if transfer_x:\r\n            x = NNDist._transfer_x(x)\r\n        y_pred = self._get_prediction(x)\r\n        return y_pred if get_raw_results else np.argmax(y_pred, axis=1)\r\n\r\n    @NNTiming.timeit()\r\n    def evaluate(self, x, y, metrics=None, tar=0, prefix=""Acc"", **kwargs):\r\n        logs, y_pred = [], self._get_prediction(NNDist._transfer_x(x))\r\n        for i, metric_rs in enumerate(self._metric_rs):\r\n            logs.append(self._sess.run(metric_rs, {\r\n                self._tfy: y, self._y_pred: y_pred\r\n            }))\r\n        if isinstance(tar, int):\r\n            print(prefix + "": {:12.8}"".format(logs[tar]))\r\n        return logs\r\n\r\n    def draw_results(self):\r\n        metrics_log, loss_log = {}, {}\r\n        for key, value in sorted(self._logs.items()):\r\n            metrics_log[key], loss_log[key] = value[:-1], value[-1]\r\n\r\n        for i, name in enumerate(sorted(self._metric_names)):\r\n            plt.figure()\r\n            plt.title(""Metric Type: {}"".format(name))\r\n            for key, log in sorted(metrics_log.items()):\r\n                xs = np.arange(len(log[i])) + 1\r\n                plt.plot(xs, log[i], label=""Data Type: {}"".format(key))\r\n            plt.legend(loc=4)\r\n            plt.show()\r\n            plt.close()\r\n\r\n        plt.figure()\r\n        plt.title(""Cost"")\r\n        for key, loss in sorted(loss_log.items()):\r\n            xs = np.arange(len(loss)) + 1\r\n            plt.plot(xs, loss, label=""Data Type: {}"".format(key))\r\n        plt.legend()\r\n        plt.show()\r\n\r\n    def draw_conv_weights(self):\r\n        with self._sess.as_default():\r\n            for i, (name, weight) in enumerate(zip(self.layer_names, self._tf_weights)):\r\n                weight = weight.eval()\r\n                if len(weight.shape) != 4:\r\n                    continue\r\n                for j, _w in enumerate(weight.transpose(2, 3, 0, 1)):\r\n                    VisUtil.show_batch_img(_w, ""{} {} filter {}"".format(name, i + 1, j + 1))\r\n\r\n    def draw_conv_series(self, x, shape=None):\r\n        x = np.asarray(x)\r\n        for xx in x:\r\n            VisUtil.show_img(VisUtil.trans_img(xx, shape), ""Original"")\r\n            for i, (layer, ac) in enumerate(zip(\r\n                    self._layers, self._get_acts(np.array([xx.transpose(1, 2, 0)], dtype=np.float32)))):\r\n                if len(ac.shape) == 4:\r\n                    VisUtil.show_batch_img(ac[0].transpose(2, 0, 1), ""Layer {} ({})"".format(i + 1, layer.name))\r\n                else:\r\n                    ac = ac[0]\r\n                    length = sqrt(np.prod(ac.shape))\r\n                    if length < 10:\r\n                        continue\r\n                    (height, width) = xx.shape[1:] if shape is None else shape[1:]\r\n                    sqrt_shape = sqrt(height * width)\r\n                    oh, ow = int(length * height / sqrt_shape), int(length * width / sqrt_shape)\r\n                    VisUtil.show_img(ac[:oh * ow].reshape(oh, ow), ""Layer {} ({})"".format(i + 1, layer.name))\r\n\r\n    @staticmethod\r\n    def fuck_pycharm_warning():\r\n        print(Axes3D.acorr)\r\n\r\n\r\nclass NNFrozen(NNBase):\r\n    NNTiming = Timing()\r\n\r\n    def __init__(self):\r\n        super(NNFrozen, self).__init__()\r\n        self._sess = tf.Session()\r\n        self._entry = self._output = None\r\n\r\n    @NNTiming.timeit(level=4, prefix=""[API] "")\r\n    def load(self, path=None, pb=""Frozen.pb""):\r\n        if path is None:\r\n            path = os.path.join(""Models"", ""Cache"")\r\n        try:\r\n            with open(os.path.join(path, ""Model.nn""), ""rb"") as file:\r\n                _dic = pickle.load(file)\r\n                for key, value in _dic[""structures""].items():\r\n                    setattr(self, key, value)\r\n                for name, param in zip(self._layer_names, self._layer_params):\r\n                    self.add(name, *param)\r\n                for key, value in _dic[""params""].items():\r\n                    setattr(self, key, value)\r\n        except Exception as err:\r\n            raise BuildNetworkError(""Failed to load Network ({}), structure initialized"".format(err))\r\n\r\n        with open(os.path.join(path, ""IO.txt""), ""r"") as file:\r\n            self._entry = file.readline().strip()[9:]\r\n            self._output = file.readline().strip()[9:]\r\n        Util.load_frozen_graph(os.path.join(path, pb), True, self._entry, self._output)\r\n\r\n        print()\r\n        print(""="" * 30)\r\n        print(""Model restored"")\r\n        print(""="" * 30)\r\n\r\n    @NNTiming.timeit(level=2, prefix=""[API] "")\r\n    def predict(self, x, get_raw_results=False, **kwargs):\r\n        x = NNDist._transfer_x(np.asarray(x))\r\n        rs = []\r\n        batch_size = floor(1e6 / np.prod(x.shape[1:]))\r\n        epoch = int(ceil(len(x) / batch_size))\r\n        output = self._sess.graph.get_tensor_by_name(self._output)\r\n        bar = ProgressBar(max_value=epoch, name=""Predict"")\r\n        for i in range(epoch):\r\n            if i == epoch - 1:\r\n                rs.append(self._sess.run(output, {\r\n                    self._entry: x[i * batch_size:]\r\n                }))\r\n            else:\r\n                rs.append(self._sess.run(output, {\r\n                    self._entry: x[i * batch_size:(i + 1) * batch_size]\r\n                }))\r\n            bar.update()\r\n        y_pred = np.vstack(rs).astype(np.float32)\r\n        return y_pred if get_raw_results else np.argmax(y_pred, axis=1)\r\n\r\n    @NNTiming.timeit(level=1, prefix=""[API] "")\r\n    def evaluate(self, x, y, metrics=None, tar=None, prefix=""Acc"", **kwargs):\r\n        y_pred = self.predict(x)\r\n        print(""Acc: {:8.6} %"".format(100 * np.sum(np.argmax(y, axis=1) == np.argmax(y_pred, axis=1)) / len(y)))\r\n\r\n\r\nclass NNPipe:\r\n    NNTiming = Timing()\r\n\r\n    def __init__(self, num):\r\n        self._nn_lst = [NNBase() for _ in range(num)]\r\n        for _nn in self._nn_lst:\r\n            _nn.verbose = 0\r\n        self._initialized = [False] * num\r\n        self.is_sub_layer = False\r\n        self.parent = None\r\n\r\n    def __getitem__(self, item):\r\n        if isinstance(item, str):\r\n            return getattr(self, ""_"" + item)\r\n        return\r\n\r\n    def __str__(self):\r\n        return ""NNPipe""\r\n\r\n    __repr__ = __str__\r\n\r\n    @property\r\n    def name(self):\r\n        return ""NNPipe""\r\n\r\n    @property\r\n    def n_filters(self):\r\n        return sum([_nn[""layers""][-1].n_filters for _nn in self._nn_lst])\r\n\r\n    @property\r\n    def out_h(self):\r\n        return self._nn_lst[0][""layers""][-1].out_h\r\n\r\n    @property\r\n    def out_w(self):\r\n        return self._nn_lst[0][""layers""][-1].out_w\r\n\r\n    @property\r\n    def shape(self):\r\n        # TODO: modify shape[0] to be the correct one\r\n        return (self.n_filters, self.out_h, self.out_w), (self.n_filters, self.out_h, self.out_w)\r\n\r\n    @property\r\n    def info(self):\r\n        return ""Pipe ({:^3d})"".format(len(self._nn_lst)) + "" "" * 65 + ""- out: {}"".format(\r\n            self.shape[1])\r\n\r\n    @property\r\n    def initialized(self):\r\n        return self._initialized\r\n\r\n    @NNTiming.timeit(level=4, prefix=""[API] "")\r\n    def preview(self):\r\n        print(""="" * 90)\r\n        print(""Pipe Structure"")\r\n        for i, nn in enumerate(self._nn_lst):\r\n            print(""-"" * 60 + ""\\n"" + str(i) + ""\\n"" + ""-"" * 60)\r\n            nn.preview()\r\n\r\n    @NNTiming.timeit(level=4, prefix=""[API] "")\r\n    def add(self, idx, layer, shape, *args, **kwargs):\r\n        if shape is None:\r\n            self._nn_lst[idx].add(layer, *args, **kwargs)\r\n        else:\r\n            self._nn_lst[idx].add(layer, shape, *args, **kwargs)\r\n        self._initialized[idx] = True\r\n\r\n    @NNTiming.timeit(level=1, prefix=""[API] "")\r\n    def get_rs(self, x, predict):\r\n        return tf.concat([nn.get_rs(x, predict=predict, pipe=True) for nn in self._nn_lst], 3)\r\n'"
NN/TF/Optimizers.py,7,"b'import tensorflow as tf\n\nfrom Util.Timing import Timing\n\n# TODO: Customize Optimizer\n\n\nclass Optimizer:\n    OptTiming = Timing()\n\n    def __init__(self, lr=1e-3):\n        self._lr = lr\n        self._opt = None\n\n    def __str__(self):\n        return self.__class__.__name__\n\n    def __repr__(self):\n        return str(self)\n\n    @property\n    def name(self):\n        return str(self)\n\n    @OptTiming.timeit(level=1, prefix=""[API] "")\n    def minimize(self, x, *args, **kwargs):\n        return self._opt.minimize(x, *args, **kwargs)\n\n\nclass MBGD(Optimizer):\n    def __init__(self, lr=1e-3):\n        Optimizer.__init__(self, lr)\n        self._opt = tf.train.GradientDescentOptimizer(self._lr)\n\n\nclass Momentum(Optimizer):\n    def __init__(self, lr=1e-3, momentum=0.8):\n        Optimizer.__init__(self, lr)\n        self._opt = tf.train.MomentumOptimizer(self._lr, momentum)\n\n\nclass NAG(Optimizer):\n    def __init__(self, lr=1e-3, momentum=0.8):\n        Optimizer.__init__(self, lr)\n        self._opt = tf.train.MomentumOptimizer(self._lr, momentum, use_nesterov=True)\n\n\nclass AdaDelta(Optimizer):\n    def __init__(self, lr=1e-3, rho=0.95, eps=1e-8):\n        Optimizer.__init__(self, lr)\n        self._opt = tf.train.AdadeltaOptimizer(self._lr, rho, eps)\n\n\nclass AdaGrad(Optimizer):\n    def __init__(self, lr=1e-3, init=0.1):\n        Optimizer.__init__(self, lr)\n        self._opt = tf.train.AdagradOptimizer(self._lr, init)\n\n\nclass Adam(Optimizer):\n    def __init__(self, lr=1e-3, beta1=0.9, beta2=0.999, eps=1e-8):\n        Optimizer.__init__(self, lr)\n        self._opt = tf.train.AdamOptimizer(self._lr, beta1, beta2, eps)\n\n\nclass RMSProp(Optimizer):\n    def __init__(self, lr=1e-3, decay=0.9, momentum=0.0, eps=1e-10):\n        Optimizer.__init__(self, lr)\n        self._opt = tf.train.RMSPropOptimizer(self._lr, decay, momentum, eps)\n\n\n# Factory\n\nclass OptFactory:\n    available_optimizers = {\n        ""MBGD"": MBGD, ""Momentum"": Momentum, ""NAG"": NAG,\n        ""AdaDelta"": AdaDelta, ""AdaGrad"": AdaGrad,\n        ""Adam"": Adam, ""RMSProp"": RMSProp\n    }\n\n    def get_optimizer_by_name(self, name, lr, *args, **kwargs):\n        try:\n            optimizer = self.available_optimizers[name](lr, *args, **kwargs)\n            return optimizer\n        except KeyError:\n            raise NotImplementedError(""Undefined Optimizer \'{}\' found"".format(name))\n'"
RNN/Test/Mnist.py,4,"b'import time\r\nimport tflearn\r\nimport tensorflow as tf\r\n\r\nfrom RNN.Wrapper import RNNWrapper\r\nfrom RNN.Generator import Generator\r\nfrom Util.Util import DataUtil\r\n\r\n\r\n# Generator\r\nclass MnistGenerator(Generator):\r\n    def __init__(self, im=None, om=None, one_hot=True):\r\n        super(MnistGenerator, self).__init__(im, om)\r\n        self._x, self._y = DataUtil.get_dataset(""mnist"", ""../../_Data/mnist.txt"", quantized=True, one_hot=one_hot)\r\n        self._x = self._x.reshape(-1, 28, 28)\r\n        self._x_train, self._x_test = self._x[:1800], self._x[1800:]\r\n        self._y_train, self._y_test = self._y[:1800], self._y[1800:]\r\n\r\n\r\n# Test Case\r\ndef test_mnist(n_history=3, draw=False):\r\n    print(""="" * 60, ""\\n"" + ""Normal LSTM"", ""\\n"" + ""-"" * 60)\r\n    generator = MnistGenerator()\r\n    t = time.time()\r\n    tf.reset_default_graph()\r\n    rnn = RNNWrapper(n_history=n_history, epoch=10, squeeze=True)\r\n    rnn.fit(28, 10, generator, n_iter=28)\r\n    print(""Time Cost: {}"".format(time.time() - t))\r\n    if draw:\r\n        rnn.draw_err_logs()\r\n\r\n    print(""="" * 60, ""\\n"" + ""Sparse LSTM"" + ""\\n"" + ""-"" * 60)\r\n    generator = MnistGenerator(one_hot=False)\r\n    t = time.time()\r\n    tf.reset_default_graph()\r\n    rnn = RNNWrapper(n_history=n_history, epoch=10, squeeze=True, use_sparse_labels=True)\r\n    rnn.fit(28, 10, generator, n_iter=28)\r\n    print(""Time Cost: {}"".format(time.time() - t))\r\n    if draw:\r\n        rnn.draw_err_logs()\r\n\r\n    print(""="" * 60, ""\\n"" + ""Tflearn"", ""\\n"" + ""-"" * 60)\r\n    generator = MnistGenerator()\r\n    t = time.time()\r\n    tf.reset_default_graph()\r\n    net = tflearn.input_data(shape=[None, 28, 28])\r\n    net = tf.concat(tflearn.lstm(net, 128, return_seq=True)[-n_history:], axis=1)\r\n    net = tflearn.fully_connected(net, 10, activation=\'softmax\')\r\n    net = tflearn.regression(net, optimizer=\'adam\', batch_size=64,\r\n                             loss=\'categorical_crossentropy\', name=""output1"")\r\n    model = tflearn.DNN(net, tensorboard_verbose=0)\r\n    model.fit(*generator.gen(0), n_epoch=10, validation_set=generator.gen(0, True), show_metric=True)\r\n    print(""Time Cost: {}"".format(time.time() - t))\r\n\r\nif __name__ == \'__main__\':\r\n    test_mnist(draw=True)\r\n'"
RNN/Test/Operations.py,15,"b'import random\nimport tensorflow as tf\n\nfrom RNN.Generator import *\nfrom RNN.Wrapper import RNNWrapper\n\n\n# RNN For Op\nclass RNNForOp(RNNWrapper):\n    def __init__(self, **kwargs):\n        super(RNNForOp, self).__init__(**kwargs)\n        self._params[""boost""] = kwargs.get(""boost"", 2)\n        self._op = """"\n\n    def _verbose(self):\n        x_test, y_test = self._generator.gen(1, boost=self._params[""boost""])\n        ans = np.argmax(self._sess.run(self._output, {\n            self._tfx: x_test\n        }), axis=2).ravel()\n        x_test = x_test.astype(np.int)\n        if self._use_sparse_labels:\n            y_test = y_test.ravel()\n        else:\n            y_test = np.argmax(y_test, axis=2).ravel()\n        print(""I think {} = {}, answer: {}..."".format(\n            "" {} "".format(self._op).join(\n                ["""".join(map(lambda n: str(n), x_test[0, ..., i][::-1])) for i in range(x_test.shape[2])]\n            ),\n            """".join(map(lambda n: str(n), ans[::-1])),\n            """".join(map(lambda n: str(n), y_test[::-1]))))\n\n\nclass RNNForAddition(RNNForOp):\n    def __init__(self, **kwargs):\n        super(RNNForAddition, self).__init__(**kwargs)\n        self._op = ""+""\n\n\nclass RNNForMultiple(RNNForOp):\n    def __init__(self, **kwargs):\n        super(RNNForMultiple, self).__init__(**kwargs)\n        self._op = ""*""\n\n\n# Sparse RNN For Op\nclass SpRNNForOp(RNNWrapper):\n    def __init__(self, **kwargs):\n        kwargs[""use_sparse_labels""] = kwargs[""squeeze""] = True\n        super(SpRNNForOp, self).__init__(**kwargs)\n        self._params[""boost""] = 0\n        self._op = """"\n\n    def _verbose(self):\n        x_test, y_test = self._generator.gen(1, boost=self._params[""boost""])\n        if self._squeeze:\n            y_test = [y_test]\n        if self._squeeze:\n            ans = [np.argmax(self._sess.run(self._output, {self._tfx: x_test}), axis=1)]\n        else:\n            ans = np.argmax(self._sess.run(self._output, {self._tfx: x_test}), axis=2)\n        x_test = x_test.astype(np.int)\n        print(""I think {} = {}, answer: {}..."".format(\n            "" {} "".format(self._op).join(\n                ["""".join(map(lambda n: str(n), x_test[0, ..., i][::-1])) for i in range(x_test.shape[2])]\n            ),\n            """".join(map(lambda n: str(n), ans[0][::-1])),\n            """".join(map(lambda n: str(n), y_test[0][::-1]))))\n\n\nclass SpRNNForAddition(SpRNNForOp):\n    def __init__(self, **kwargs):\n        super(SpRNNForAddition, self).__init__(**kwargs)\n        self._op = ""+""\n\n\nclass SpRNNForMultiple(SpRNNForOp):\n    def __init__(self, **kwargs):\n        super(SpRNNForMultiple, self).__init__(**kwargs)\n        self._op = ""*""\n\n\n# Embedding RNN For Op\nclass EmbedRNNForOp(RNNWrapper):\n    def __init__(self, **kwargs):\n        kwargs[""use_sparse_labels""] = kwargs[""squeeze""] = True\n        super(EmbedRNNForOp, self).__init__(**kwargs)\n        self._params[""boost""] = 0\n        self._op = """"\n\n    def _verbose(self):\n        x_test, y_test = self._generator.gen(1, boost=self._params[""boost""])\n        ans = np.argmax(self._sess.run(self._output, {self._tfx: x_test}), axis=1)\n        x_test = x_test.astype(np.int)\n        print(""I think {} = {}, answer: {}..."".format(\n            "" {} "".format(self._op).join(map(lambda n: str(n), x_test[0])),\n            ans[0], y_test[0]))\n\n\nclass EmbedRNNForAddition(EmbedRNNForOp):\n    def __init__(self, **kwargs):\n        super(EmbedRNNForAddition, self).__init__(**kwargs)\n        self._op = ""+""\n\n\nclass EmbedRNNForMultiple(EmbedRNNForOp):\n    def __init__(self, **kwargs):\n        super(EmbedRNNForMultiple, self).__init__(**kwargs)\n        self._op = ""*""\n\n\n# Generators\n\n# Op Generator\nclass OpGenerator(Generator):\n    def __init__(self, im, om, n_time_step, random_scale, use_sparse_labels=False):\n        super(OpGenerator, self).__init__(im, om)\n        self._base = self._om\n        self._n_time_step = n_time_step\n        self._random_scale = random_scale\n        self._use_sparse_labels = use_sparse_labels\n\n    def _op(self, seq):\n        return 0\n\n    def _gen_seq(self, n_time_step, tar):\n        seq = []\n        for _ in range(n_time_step):\n            seq.append(tar % self._base)\n            tar //= self._base\n        return seq\n\n    def _gen_targets(self, n_time_step):\n        return []\n\n    def gen(self, batch_size, test=False, boost=0):\n        if boost:\n            n_time_step = self._n_time_step + self._random_scale + random.randint(1, boost)\n        else:\n            n_time_step = self._n_time_step + random.randint(0, self._random_scale)\n        x = np.empty([batch_size, n_time_step, self._im])\n        if not self._use_sparse_labels:\n            y = np.zeros([batch_size, n_time_step, self._om])\n        else:\n            y = np.zeros([batch_size, n_time_step], dtype=np.int32)\n        for i in range(batch_size):\n            targets = self._gen_targets(n_time_step)\n            sequences = [self._gen_seq(n_time_step, tar) for tar in targets]\n            for j in range(self._im):\n                x[i, ..., j] = sequences[j]\n            ans_seq = self._gen_seq(n_time_step, self._op(targets))\n            if not self._use_sparse_labels:\n                y[i, range(n_time_step), ans_seq] = 1\n            else:\n                y[i] = ans_seq\n        return x, y\n\n\nclass AdditionGenerator(OpGenerator):\n    def _op(self, seq):\n        return sum(seq)\n\n    def _gen_targets(self, n_time_step):\n        return [int(random.randint(0, self._om ** n_time_step - 1) / self._im) for _ in range(self._im)]\n\n\nclass MultipleGenerator(OpGenerator):\n    def _op(self, seq):\n        return np.prod(seq)\n\n    def _gen_targets(self, n_time_step):\n        return [int(random.randint(0, self._om ** n_time_step - 1) ** (1 / self._im)) for _ in range(self._im)]\n\n\n# Sparse Op Generator\nclass SpOpGenerator(OpGenerator):\n    def __init__(self, im, om, n_time_step, random_scale):\n        super(SpOpGenerator, self).__init__(im, om, n_time_step, random_scale)\n        self._base = round(self._om ** (1 / (n_time_step + random_scale)))\n\n    def gen(self, batch_size, test=False, boost=0):\n        if boost:\n            n_time_step = self._n_time_step + self._random_scale + random.randint(1, boost)\n        else:\n            n_time_step = self._n_time_step + random.randint(0, self._random_scale)\n        x = np.empty([batch_size, n_time_step, self._im])\n        y = np.zeros(batch_size, dtype=np.int32)\n        for i in range(batch_size):\n            targets = self._gen_targets(n_time_step)\n            sequences = [self._gen_seq(n_time_step, tar) for tar in targets]\n            for j in range(self._im):\n                x[i, ..., j] = sequences[j]\n            y[i] = self._op(targets)\n        return x, y\n\n\nclass SpAdditionGenerator(SpOpGenerator):\n    def _op(self, seq):\n        return sum(seq)\n\n    def _gen_targets(self, n_time_step):\n        return [int(random.randint(0, self._base ** n_time_step - 1) / self._im) for _ in range(self._im)]\n\n\nclass SpMultipleGenerator(SpOpGenerator):\n    def _op(self, seq):\n        return np.prod(seq)\n\n    def _gen_targets(self, n_time_step):\n        return [int(random.randint(0, self._base ** n_time_step - 1) ** (1 / self._im)) for _ in range(self._im)]\n\n\n# Embedding Sparse Op Generator\nclass EmbedOpGenerator(Generator):\n    def __init__(self, im, om, n_digit, random_scale=0):\n        super(EmbedOpGenerator, self).__init__(im, om)\n        self._n_digit = n_digit\n        self._random_scale = random_scale\n\n    def _op(self, x):\n        return 0\n\n    def _get_x(self, n_digit):\n        return 0\n\n    def gen(self, batch_size, test=False, boost=0):\n        n_digit = self._n_digit + random.randint(0, self._random_scale)\n        x = np.empty([batch_size, n_digit], dtype=np.int32)\n        y = np.zeros(batch_size, dtype=np.int32)\n        for i in range(batch_size):\n            x[i] = self._get_x(n_digit)\n            y[i] = self._op(x[i])\n        return x, y\n\n\nclass EmbedAdditionGenerator(EmbedOpGenerator):\n    def _op(self, seq):\n        return sum(seq)\n\n    def _get_x(self, n_digit):\n        return np.random.randint(0, int(min(self._im, self._om / n_digit)), n_digit)\n\n\nclass EmbedMultipleGenerator(EmbedOpGenerator):\n    def _op(self, seq):\n        return np.prod(seq)\n\n    def _get_x(self, n_digit):\n        return np.random.randint(0, int(min(self._im, self._om ** (1 / n_digit))), n_digit)\n\n\n# Test Cases\ndef test_rnn(random_scale=2, digit_len=2, digit_base=10, n_digit=3, use_sparse_labels=False):\n    tf.reset_default_graph()\n    generator = AdditionGenerator(\n        n_digit, digit_base, n_time_step=digit_len, random_scale=random_scale,\n        use_sparse_labels=use_sparse_labels\n    )\n    lstm = RNNForAddition(\n        # cell=tf.contrib.rnn.GRUCell,\n        # cell=tf.contrib.rnn.LSTMCell,\n        # cell=tf.contrib.rnn.BasicRNNCell,\n        # cell=tf.contrib.rnn.BasicLSTMCell,\n        epoch=100, boost=2, use_sparse_labels=use_sparse_labels\n    )\n    lstm.fit(n_digit, digit_base, generator)\n    lstm.draw_err_logs()\n\n\ndef test_sp_rnn(random_scale=0, digit_len=4, digit_base=10, n_digit=2):\n    tf.reset_default_graph()\n    generator = SpMultipleGenerator(\n        n_digit, digit_base ** (digit_len + random_scale),\n        n_time_step=digit_len, random_scale=random_scale\n    )\n    lstm = SpRNNForMultiple(\n        # cell=tf.contrib.rnn.GRUCell,\n        # cell=tf.contrib.rnn.LSTMCell,\n        # cell=tf.contrib.rnn.BasicRNNCell,\n        # cell=tf.contrib.rnn.BasicLSTMCell,\n        epoch=20, n_history=2\n    )\n    lstm.fit(n_digit, digit_base ** (digit_len + random_scale), generator)\n    lstm.draw_err_logs()\n\n\ndef test_embed_rnn(random_scale=0, n_digit=2, im=100, om=10000):\n    tf.reset_default_graph()\n    generator = EmbedMultipleGenerator(im, om, n_digit=n_digit, random_scale=random_scale)\n    lstm = EmbedRNNForMultiple(\n        # cell=tf.contrib.rnn.GRUCell,\n        # cell=tf.contrib.rnn.LSTMCell,\n        # cell=tf.contrib.rnn.BasicRNNCell,\n        # cell=tf.contrib.rnn.BasicLSTMCell,\n        epoch=20, use_sparse_labels=True, embedding_size=50,\n        use_final_state=False, n_history=n_digit\n    )\n    lstm.fit(im, om, generator)\n    lstm.draw_err_logs()\n\nif __name__ == \'__main__\':\n    test_rnn()\n    test_rnn(use_sparse_labels=True)\n    test_sp_rnn()\n    test_embed_rnn()\n    test_embed_rnn(1)\n'"
RNN/Test/UnitTest.py,16,"b'import random\r\nimport numpy as np\r\nimport tensorflow as tf\r\nimport matplotlib.pyplot as plt\r\nimport tensorflow.contrib.layers as layers\r\nimport tensorflow.contrib.rnn as rnn\r\n\r\n\r\nclass AdditionGenerator:\r\n    def __init__(self, im, om, n_time_step):\r\n        self._im, self._om = im, om\r\n        self._base = self._om\r\n        self._n_time_step = n_time_step\r\n\r\n    def _op(self, seq):\r\n        return sum(seq)\r\n\r\n    def _gen_seq(self, n_time_step, tar):\r\n        seq = []\r\n        for _ in range(n_time_step):\r\n            seq.append(tar % self._base)\r\n            tar //= self._base\r\n        return seq\r\n\r\n    def _gen_targets(self, n_time_step):\r\n        return [int(random.randint(0, self._om ** n_time_step - 1) / self._im) for _ in range(self._im)]\r\n\r\n    def gen(self, batch_size):\r\n        x = np.empty([batch_size, self._n_time_step, self._im])\r\n        y = np.zeros([batch_size, self._n_time_step, self._om])\r\n        for i in range(batch_size):\r\n            targets = self._gen_targets(self._n_time_step)\r\n            sequences = [self._gen_seq(self._n_time_step, tar) for tar in targets]\r\n            for j in range(self._im):\r\n                x[i, ..., j] = sequences[j]\r\n            y[i, range(self._n_time_step), self._gen_seq(self._n_time_step, self._op(targets))] = 1\r\n        return x, y\r\n\r\n\r\nclass RNNWrapper:\r\n    def __init__(self):\r\n        self._log = {}\r\n        self._return_all_states = None\r\n        self._optimizer = self._generator = None\r\n        self._tfx = self._tfy = self._input = self._output = None\r\n        self._cell = self._im = self._om = None\r\n        self._sess = tf.Session()\r\n\r\n    def _verbose(self):\r\n        x_test, y_test = self._generator.gen(1)\r\n        ans = np.argmax(self._sess.run(self._output, {\r\n            self._tfx: x_test\r\n        }), axis=2).ravel()\r\n        x_test = x_test.astype(np.int)\r\n        print(""I think {} = {}, answer: {}..."".format(\r\n            "" + "".join(\r\n                ["""".join(map(lambda n: str(n), x_test[0, ..., i][::-1])) for i in range(x_test.shape[2])]\r\n            ),\r\n            """".join(map(lambda n: str(n), ans[::-1])),\r\n            """".join(map(lambda n: str(n), np.argmax(y_test, axis=2).ravel()[::-1]))))\r\n\r\n    def _get_output(self, rnn_outputs, rnn_states):\r\n        print(""Outputs :"", rnn_outputs)\r\n        print(""States  :"", rnn_states)\r\n        if self._return_all_states:\r\n            outputs = tf.concat([rnn_outputs, rnn_states], axis=2)\r\n        else:\r\n            outputs = rnn_outputs\r\n        self._output = layers.fully_connected(\r\n            outputs, num_outputs=self._om, activation_fn=tf.nn.sigmoid)\r\n\r\n    def fit(self, im, om, generator, cell=rnn.BasicLSTMCell, return_all_states=False):\r\n        self._generator = generator\r\n        self._im, self._om = im, om\r\n        self._return_all_states = return_all_states\r\n        self._optimizer = tf.train.AdamOptimizer(0.01)\r\n        self._input = self._tfx = tf.placeholder(tf.float32, shape=[None, None, im])\r\n        self._tfy = tf.placeholder(tf.float32, shape=[None, None, om])\r\n\r\n        self._cell = cell(128)\r\n        rnn_outputs, rnn_states = tf.nn.dynamic_rnn(\r\n            self._cell, self._input, return_all_states=self._return_all_states,\r\n            initial_state=self._cell.zero_state(tf.shape(self._input)[0], tf.float32)\r\n        )\r\n        self._get_output(rnn_outputs, rnn_states)\r\n        loss = -tf.reduce_mean(\r\n            self._tfy * tf.log(self._output + 1e-8) + (1 - self._tfy) * tf.log(1 - self._output + 1e-8)\r\n        )\r\n        train_step = self._optimizer.minimize(loss)\r\n        self._log[""iter_err""] = []\r\n        self._log[""epoch_err""] = []\r\n        self._sess.run(tf.global_variables_initializer())\r\n        for _ in range(20):\r\n            epoch_err = 0\r\n            for __ in range(128):\r\n                x_batch, y_batch = self._generator.gen(64)\r\n                feed_dict = {self._tfx: x_batch, self._tfy: y_batch}\r\n                iter_err = self._sess.run([loss, train_step], feed_dict)[0]\r\n                self._log[""iter_err""].append(iter_err)\r\n                epoch_err += iter_err\r\n            self._log[""epoch_err""].append(epoch_err / 128)\r\n            self._verbose()\r\n\r\n    def predict(self, x):\r\n        x = np.atleast_3d(x)\r\n        output = self._sess.run(self._output, {self._tfx: x})\r\n        return np.argmax(output, axis=2).ravel()\r\n\r\n    def draw_err_logs(self):\r\n        ee, ie = self._log[""epoch_err""], self._log[""iter_err""]\r\n        ee_base = np.arange(len(ee))\r\n        ie_base = np.linspace(0, len(ee) - 1, len(ie))\r\n        plt.figure()\r\n        plt.plot(ie_base, ie, label=""Iter error"")\r\n        plt.plot(ee_base, ee, linewidth=3, label=""Epoch error"")\r\n        plt.legend()\r\n        plt.show()\r\n\r\nif __name__ == \'__main__\':\r\n    random_scale = 2\r\n    digit_len = 4\r\n    digit_base = 10\r\n    n_digit = 2\r\n    _generator = AdditionGenerator(n_digit, digit_base, n_time_step=digit_len)\r\n\r\n    # Return final state only\r\n    # BasicRNNCell\r\n    print(""="" * 60 + ""\\n"" + ""Return final state only (BasicRNNCell)\\n"" + ""-"" * 60)\r\n    lstm = RNNWrapper()\r\n    lstm.fit(n_digit, digit_base, _generator, rnn.BasicRNNCell)\r\n    lstm.draw_err_logs()\r\n\r\n    # BasicLSTMCell\r\n    print(""="" * 60 + ""\\n"" + ""Return final state only (BasicLSTMCell)\\n"" + ""-"" * 60)\r\n    tf.reset_default_graph()\r\n    lstm = RNNWrapper()\r\n    lstm.fit(n_digit, digit_base, _generator)\r\n    lstm.draw_err_logs()\r\n\r\n    # LSTMCell\r\n    print(""="" * 60 + ""\\n"" + ""Return final state only (LSTMCell)\\n"" + ""-"" * 60)\r\n    tf.reset_default_graph()\r\n    lstm = RNNWrapper()\r\n    lstm.fit(n_digit, digit_base, _generator, rnn.LSTMCell)\r\n    lstm.draw_err_logs()\r\n\r\n    # GRUCell\r\n    print(""="" * 60 + ""\\n"" + ""Return final state only (GRUCell)\\n"" + ""-"" * 60)\r\n    tf.reset_default_graph()\r\n    lstm = RNNWrapper()\r\n    lstm.fit(n_digit, digit_base, _generator, rnn.GRUCell)\r\n    lstm.draw_err_logs()\r\n\r\n    # Return all states\r\n    # LSTMCell\r\n    print(""="" * 60 + ""\\n"" + ""Return all states generated (LSTMCell)\\n"" + ""-"" * 60)\r\n    tf.reset_default_graph()\r\n    lstm = RNNWrapper()\r\n    lstm.fit(n_digit, digit_base, _generator, rnn.LSTMCell, return_all_states=True)\r\n    lstm.draw_err_logs()\r\n\r\n    # GRUCell\r\n    print(""="" * 60 + ""\\n"" + ""Return all states generated (GRUCell)\\n"" + ""-"" * 60)\r\n    tf.reset_default_graph()\r\n    lstm = RNNWrapper()\r\n    lstm.fit(n_digit, digit_base, _generator, rnn.GRUCell, return_all_states=True)\r\n    lstm.draw_err_logs()\r\n'"
Zhihu/NN/Layers.py,10,"b'import numpy as np\r\nimport tensorflow as tf\r\nfrom abc import ABCMeta, abstractmethod\r\n\r\n\r\nclass Layer(metaclass=ABCMeta):\r\n    def __init__(self, shape):\r\n        self.shape = shape\r\n\r\n    def activate(self, x, w, bias=None, predict=False):\r\n        if bias is None:\r\n            return self._activate(tf.matmul(x, w), predict)\r\n        return self._activate(tf.matmul(x, w) + bias, predict)\r\n\r\n    @abstractmethod\r\n    def _activate(self, x, predict):\r\n        pass\r\n\r\n\r\n# Activation Layers\r\n\r\nclass Tanh(Layer):\r\n    def _activate(self, x, predict):\r\n        return tf.tanh(x)\r\n\r\n\r\nclass Sigmoid(Layer):\r\n    def _activate(self, x, predict):\r\n        return tf.nn.sigmoid(x)\r\n\r\n\r\nclass ELU(Layer):\r\n    def _activate(self, x, predict):\r\n        return tf.nn.elu(x)\r\n\r\n\r\nclass ReLU(Layer):\r\n    def _activate(self, x, predict):\r\n        return tf.nn.relu(x)\r\n\r\n\r\nclass Softplus(Layer):\r\n    def _activate(self, x, predict):\r\n        return tf.nn.softplus(x)\r\n\r\n\r\nclass Identical(Layer):\r\n    def _activate(self, x, predict):\r\n        return x\r\n\r\n\r\nclass CF0910(Layer):\r\n    def _activate(self, x, predict):\r\n        return tf.minimum(tf.maximum(x, 0), 6)\r\n\r\n\r\n# Cost Layers\r\n\r\nclass CostLayer(Layer):\r\n    def _activate(self, x, y):\r\n        pass\r\n\r\n    def calculate(self, y, y_pred):\r\n        return self._activate(y.astype(np.float32), y_pred)\r\n\r\n\r\nclass CrossEntropy(CostLayer):\r\n    def _activate(self, x, y):\r\n        return tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=x, labels=y))\r\n\r\n\r\nclass MSE(CostLayer):\r\n    def _activate(self, x, y):\r\n        return tf.reduce_mean(tf.square(x - y))\r\n'"
Zhihu/NN/Optimizers.py,7,"b'import tensorflow as tf\n\nfrom Util.Timing import Timing\n\n\nclass Optimizers:\n\n    OptTiming = Timing()\n\n    def __init__(self, lr=1e-3):\n        self._lr = lr\n        self._opt = None\n\n    @property\n    def name(self):\n        return str(self)\n\n    def feed_timing(self, timing):\n        if isinstance(timing, Timing):\n            self.OptTiming = timing\n\n    @OptTiming.timeit(level=1, prefix=""[API] "")\n    def minimize(self, x, *args, **kwargs):\n        return self._opt.minimize(x, *args, **kwargs)\n\n    def __str__(self):\n        return self.__class__.__name__\n\n    def __repr__(self):\n        return str(self)\n\n\nclass SGD(Optimizers):\n\n    def __init__(self, lr=1e-3):\n        Optimizers.__init__(self, lr)\n        self._opt = tf.train.GradientDescentOptimizer(self._lr)\n\n\nclass Momentum(Optimizers):\n\n    def __init__(self, lr=1e-3, momentum=0.8):\n        Optimizers.__init__(self, lr)\n        self._opt = tf.train.MomentumOptimizer(self._lr, momentum)\n\n\nclass NAG(Optimizers):\n\n    def __init__(self, lr=1e-3, momentum=0.8):\n        Optimizers.__init__(self, lr)\n        self._opt = tf.train.MomentumOptimizer(self._lr, momentum, use_nesterov=True)\n\n\nclass AdaDelta(Optimizers):\n\n    def __init__(self, lr=1e-3, rho=0.95, eps=1e-8):\n        Optimizers.__init__(self, lr)\n        self._opt = tf.train.AdadeltaOptimizer(self._lr, rho, eps)\n\n\nclass AdaGrad(Optimizers):\n\n    def __init__(self, lr=1e-3, init=0.1):\n        Optimizers.__init__(self, lr)\n        self._opt = tf.train.AdagradOptimizer(self._lr, init)\n\n\nclass Adam(Optimizers):\n\n    def __init__(self, lr=1e-3, beta1=0.9, beta2=0.999, eps=1e-8):\n        Optimizers.__init__(self, lr)\n        self._opt = tf.train.AdamOptimizer(self._lr, beta1, beta2, eps)\n\n\nclass RMSProp(Optimizers):\n\n    def __init__(self, lr=1e-3, decay=0.9, momentum=0.0, eps=1e-10):\n        Optimizers.__init__(self, lr)\n        self._opt = tf.train.RMSPropOptimizer(self._lr, decay, momentum, eps)\n\n\n# Factory\n\nclass OptFactory:\n\n    available_optimizers = {\n        ""MBGD"": SGD, ""Momentum"": Momentum, ""NAG"": NAG,\n        ""AdaDelta"": AdaDelta, ""AdaGrad"": AdaGrad,\n        ""Adam"": Adam, ""RMSProp"": RMSProp\n    }\n\n    def get_optimizer_by_name(self, name, timing, lr, *args, **kwargs):\n        try:\n            _optimizer = self.available_optimizers[name](lr, *args, **kwargs)\n            _optimizer.feed_timing(timing)\n            return _optimizer\n        except KeyError:\n            raise NotImplementedError(""Undefined Optimizer \'{}\' found"".format(name))\n'"
Zhihu/Python/cv2_example.py,0,"b'from NN.Basic.Networks import *\nfrom c_CvDTree.Tree import *\n\nfrom Util.Util import DataUtil\n\n\ndef cv2_example():\n    pass\n\n\ndef visualize_nn():\n    x, y = DataUtil.gen_xor()\n    nn = NNDist()\n    nn.add(""ReLU"", (x.shape[1], 6))\n    nn.add(""ReLU"", (6,))\n    nn.add(""Softmax"", (y.shape[1],))\n    nn.fit(x, y, epoch=1000, draw_detailed_network=True)\n\n\ndef visualize_tree():\n    data, x, y = [], [], []\n    with open(""../CvDTree/data.txt"", ""r"") as _file:\n        for _line in _file:\n            data.append(_line.split("",""))\n    np.random.shuffle(data)\n    for _line in data:\n        y.append(_line.pop(0))\n        x.append(list(map(lambda c: c.strip(), _line)))\n    x, y = np.array(x), np.array(y)\n\n    _tree = C45Tree()\n    _tree.fit(x, y)\n    _tree.visualize()\n\nif __name__ == \'__main__\':\n    cv2_example()\n    visualize_nn()\n    visualize_tree()\n'"
Zhihu/Python/metaclass_example.py,0,"b'class Person:\n    def __init__(self):\n        self.ability = 1\n\n    def eat(self):\n        print(""Eat: "", self.ability)\n\n    def sleep(self):\n        print(""Sleep: "", self.ability)\n\n    def save_life(self):\n        print(""+ "", self.ability, "" s"")\n\n\nclass Wang(Person):\n    def eat(self):\n        print(""Eat: "", self.ability * 2)\n\n\nclass Zhang(Person):\n    def sleep(self):\n        print(""Sleep: "", self.ability * 2)\n\n\nclass Jiang(Person):\n    def save_life(self):\n        print(""+ inf s"")\n\n\nclass Mixture(type):\n    def __new__(mcs, *args, **kwargs):\n        name, bases, attr = args[:3]\n        person1, person2, person3 = bases\n\n        def eat(self):\n            person1.eat(self)\n\n        def sleep(self):\n            person2.sleep(self)\n\n        def save_life(self):\n            person3.save_life(self)\n\n        for key, value in locals().items():\n            if str(value).find(""function"") >= 0:\n                attr[key] = value\n\n        return type(name, bases, attr)\n\n\nclass Compare(Zhang, Jiang, Wang):\n    pass\n\n\nclass Hong(Wang, Zhang, Jiang, metaclass=Mixture):\n    pass\n\n\ndef test(person):\n    person.eat()\n    person.sleep()\n    person.save_life()\n\nif __name__ == \'__main__\':\n    test(Hong())\n    test(Compare())\n'"
Zhihu/Python/metaclass_example2.py,0,"b'import numpy as np\r\n\r\nfrom Util.Bases import ClassifierBase, TimingBase\r\nfrom Util.Metas import SubClassTimingMeta, TimingMeta\r\nfrom Util.Timing import Timing\r\n\r\n\r\nclass T1234(ClassifierBase):\r\n    T1234Timing = Timing()\r\n\r\n    @staticmethod\r\n    @T1234Timing.timeit(level=1, prefix=""[API] "")\r\n    def predict(self, x, get_raw_results=False, **kwargs):\r\n        y_pred = np.zeros(len(x))\r\n        x_axis, y_axis = x.T\r\n        x_lt_0, y_lt_0 = x_axis < 0, y_axis < 0\r\n        x_gt_0, y_gt_0 = ~x_lt_0, ~y_lt_0\r\n        y_pred[x_lt_0 & y_gt_0] = 1\r\n        y_pred[x_lt_0 & y_lt_0] = 2\r\n        y_pred[x_gt_0 & y_gt_0] = 3\r\n        return y_pred\r\n\r\n\r\nclass Child(T1234, metaclass=SubClassTimingMeta):\r\n    @staticmethod\r\n    def test():\r\n        for _ in range(10 ** 6):\r\n            pass\r\n\r\n\r\nclass Test(TimingBase, metaclass=TimingMeta):\r\n    @staticmethod\r\n    def test1():\r\n        for _ in range(10 ** 6):\r\n            pass\r\n\r\n    def test2(self):\r\n        for _ in range(10 ** 6):\r\n            _ = 1\r\n\r\nif __name__ == \'__main__\':\r\n    test = Test()\r\n    n_call = 100\r\n    for _ in range(n_call):\r\n        test.test1()\r\n        test.test2()\r\n    test.show_timing_log()\r\n'"
Zhihu/RNN/Mnist.py,4,"b'import time\r\nimport tflearn\r\nimport numpy as np\r\nimport tensorflow as tf\r\n\r\nfrom Zhihu.RNN.RNN import RNNWrapper, FastLSTMCell\r\nfrom Util.Util import DataUtil\r\n\r\n\r\nclass MnistGenerator:\r\n    def __init__(self, im=None, om=None):\r\n        self._im, self._om = im, om\r\n        self._cursor = self._indices = None\r\n        self._x, self._y = DataUtil.get_dataset(""mnist"", ""../../_Data/mnist.txt"", quantized=True, one_hot=True)\r\n        self._x = self._x.reshape(-1, 28, 28)\r\n        self._x_train, self._x_test = self._x[:1800], self._x[1800:]\r\n        self._y_train, self._y_test = self._y[:1800], self._y[1800:]\r\n\r\n    def refresh(self):\r\n        self._cursor = 0\r\n        self._indices = np.random.permutation(len(self._x_train))\r\n\r\n    def gen(self, batch, test=False):\r\n        if batch == 0:\r\n            if test:\r\n                return self._x_test, self._y_test\r\n            return self._x_train, self._y_train\r\n        end = min(self._cursor + batch, len(self._x_train))\r\n        start, self._cursor = self._cursor, end\r\n        if start == end:\r\n            self.refresh()\r\n            end = batch\r\n            start = self._cursor = 0\r\n        indices = self._indices[start:end]\r\n        return self._x_train[indices], self._y_train[indices]\r\n\r\nif __name__ == \'__main__\':\r\n    generator = MnistGenerator()\r\n\r\n    print(""="" * 60, ""\\n"" + ""My LSTM"", ""\\n"" + ""-"" * 60)\r\n    tf.reset_default_graph()\r\n    t = time.time()\r\n    rnn = RNNWrapper()\r\n    rnn.fit(28, 10, generator)\r\n    print(""Time Cost: {}"".format(time.time() - t))\r\n\r\n    print(""="" * 60, ""\\n"" + ""My Fast LSTM"", ""\\n"" + ""-"" * 60)\r\n    tf.reset_default_graph()\r\n    t = time.time()\r\n    rnn = RNNWrapper()\r\n    rnn.fit(28, 10, generator, cell=FastLSTMCell)\r\n    print(""Time Cost: {}"".format(time.time() - t))\r\n\r\n    print(""="" * 60, ""\\n"" + ""Tflearn"", ""\\n"" + ""-"" * 60)\r\n    tf.reset_default_graph()\r\n    t = time.time()\r\n    net = tflearn.input_data(shape=[None, 28, 28])\r\n    net = tf.concat(tflearn.lstm(net, 128, return_seq=True)[-3:], axis=1)\r\n    net = tflearn.fully_connected(net, 10, activation=\'softmax\')\r\n    net = tflearn.regression(net, optimizer=\'adam\', batch_size=64, learning_rate=0.001,\r\n                             loss=\'categorical_crossentropy\')\r\n    model = tflearn.DNN(net, tensorboard_verbose=0)\r\n    model.fit(*generator.gen(0), validation_set=generator.gen(0, True), show_metric=True)\r\n    print(""Time Cost: {}"".format(time.time() - t))\r\n'"
Zhihu/RNN/RNN.py,28,"b'import numpy as np\r\nimport tensorflow as tf\r\nimport tensorflow.contrib.layers as layers\r\nfrom tensorflow.contrib.rnn import BasicRNNCell, LSTMStateTuple\r\n\r\n\r\nclass LSTMCell(BasicRNNCell):\r\n    def __call__(self, x, state, scope=""LSTM""):\r\n        with tf.variable_scope(scope):\r\n            s_old, h_old = tf.split(state, 2, 1)\r\n            gates = layers.fully_connected(\r\n                tf.concat([x, s_old], 1),\r\n                num_outputs=4 * self._num_units,\r\n                activation_fn=None)\r\n            r1, g1, g2, g3 = tf.split(gates, 4, 1)\r\n            r1 = tf.nn.sigmoid(r1)\r\n            g1 = tf.nn.sigmoid(g1)\r\n            g2 = tf.nn.tanh(g2)\r\n            g3 = tf.nn.sigmoid(g3)\r\n            h_new = h_old * r1 + g1 * g2\r\n            s_new = tf.nn.tanh(h_new) * g3\r\n            return s_new, tf.concat([s_new, h_new], 1)\r\n\r\n    @property\r\n    def state_size(self):\r\n        return 2 * self._num_units\r\n\r\n\r\nclass FastLSTMCell(BasicRNNCell):\r\n    def __call__(self, x, state, scope=""LSTM""):\r\n        with tf.variable_scope(scope):\r\n            s_old, h_old = state\r\n            gates = layers.fully_connected(\r\n                tf.concat([x, s_old], 1),\r\n                num_outputs=4 * self._num_units,\r\n                activation_fn=None)\r\n            r1, g1, g2, g3 = tf.split(gates, 4, 1)\r\n            r1 = tf.nn.sigmoid(r1)\r\n            g1 = tf.nn.sigmoid(g1)\r\n            g2 = tf.nn.tanh(g2)\r\n            g3 = tf.nn.sigmoid(g3)\r\n            h_new = h_old * r1 + g1 * g2\r\n            s_new = tf.nn.tanh(h_new) * g3\r\n            return s_new, LSTMStateTuple(s_new, h_new)\r\n\r\n    @property\r\n    def state_size(self):\r\n        return LSTMStateTuple(self._num_units, self._num_units)\r\n\r\n\r\nclass RNNWrapper:\r\n    def __init__(self):\r\n        self._generator = None\r\n        self._tfx = self._tfy = self._output = None\r\n        self._cell = self._im = self._om = self._hidden_units = None\r\n        self._sess = tf.Session()\r\n\r\n    def _verbose(self):\r\n        x_test, y_test = self._generator.gen(0, True)\r\n        y_pred = self.predict(x_test)  # type: np.ndarray\r\n        print(""Test acc: {:8.6} %"".format(np.mean(np.argmax(y_test, axis=1) == y_pred) * 100))\r\n\r\n    def _get_output(self, rnn_outputs):\r\n        outputs = tf.reshape(rnn_outputs[..., -3:, :], [-1, self._hidden_units * 3])\r\n        self._output = layers.fully_connected(\r\n            outputs, num_outputs=self._om, activation_fn=tf.nn.sigmoid)\r\n\r\n    def fit(self, im, om, generator, hidden_units=128, cell=LSTMCell):\r\n        self._generator = generator\r\n        self._im, self._om, self._hidden_units = im, om, hidden_units\r\n        self._tfx = tf.placeholder(tf.float32, shape=[None, None, im])\r\n        self._tfy = tf.placeholder(tf.float32, shape=[None, om])\r\n\r\n        self._cell = cell(self._hidden_units)\r\n        rnn_outputs, _ = tf.nn.dynamic_rnn(\r\n            self._cell, self._tfx,\r\n            initial_state=self._cell.zero_state(tf.shape(self._tfx)[0], tf.float32)\r\n        )\r\n        self._get_output(rnn_outputs)\r\n        loss = tf.nn.softmax_cross_entropy_with_logits(logits=self._output, labels=self._tfy)\r\n        train_step = tf.train.AdamOptimizer(0.001).minimize(loss)\r\n        self._sess.run(tf.global_variables_initializer())\r\n        for _ in range(10):\r\n            self._generator.refresh()\r\n            for __ in range(29):\r\n                x_batch, y_batch = self._generator.gen(64)\r\n                self._sess.run(train_step, {self._tfx: x_batch, self._tfy: y_batch})\r\n            self._verbose()\r\n\r\n    def predict(self, x):\r\n        x = np.atleast_3d(x)\r\n        output = self._sess.run(self._output, {self._tfx: x})\r\n        return np.argmax(output, axis=1).ravel()\r\n'"
_Dist/ImageRecognition/Main.py,1,"b'import os\r\nimport sys\r\nimport time\r\nimport shutil\r\nimport imghdr\r\nimport argparse\r\nimport numpy as np\r\nimport tensorflow as tf\r\n\r\nfrom _Dist.ImageRecognition.ToolBox import Pipeline, Extractor\r\nfrom NN.NN import NNDist\r\n\r\n\r\ndef fetch_img_data(name=""_Data""):\r\n    img_paths, labels = [], []\r\n    data_folder_lst = os.listdir(name)\r\n    for i in range(len(data_folder_lst) - 1, -1, -1):\r\n        if not os.path.isdir(os.path.join(name, data_folder_lst[i])):\r\n            data_folder_lst.pop(i)\r\n        elif data_folder_lst[i] == ""_Cache"":\r\n            data_folder_lst.pop(i)\r\n    label_dic_dir = os.path.join(name, ""_Cache"", ""LABEL_DIC"")\r\n    if not os.path.isfile(label_dic_dir):\r\n        if not os.path.isdir(os.path.join(name, ""_Cache"")):\r\n            os.makedirs(os.path.join(name, ""_Cache""))\r\n        np.save(label_dic_dir, np.array(data_folder_lst))\r\n    for i, folder in enumerate(data_folder_lst):\r\n        for img in os.listdir(os.path.join(name, folder)):\r\n            img_dir = os.path.join(name, folder, img)\r\n            if not os.path.isfile(img_dir):\r\n                continue\r\n            if imghdr.what(img_dir) is None:\r\n                continue\r\n            img_paths.append(img_dir)\r\n            labels.append(i)\r\n    max_label = max(labels)  # type: int\r\n    labels = np.array(\r\n        [[0 if i != yy else 1 for i in range(max_label + 1)] for yy in labels],\r\n        dtype=np.float32\r\n    )\r\n    return img_paths, labels\r\n\r\n\r\ndef main(_):\r\n    if FLAGS.gen_test:\r\n        test_list = [\r\n            file for file in os.listdir(""Test"") if os.path.isfile(os.path.join(""Test"", file)) and imghdr.what(\r\n                os.path.join(""Test"", file)) is not None\r\n        ]\r\n        if test_list:\r\n            print(""Test set already exists"")\r\n        else:\r\n            print(""Generating Test set..."")\r\n            img_paths, labels = fetch_img_data()\r\n            n_test = min(196, int(0.2 * len(labels)))\r\n            _indices = np.random.choice(len(labels), n_test, replace=False)\r\n            img_paths = np.array(img_paths)[_indices]\r\n            labels = labels[_indices]\r\n            for i, _path in enumerate(img_paths):\r\n                base_name = os.path.basename(_path)\r\n                shutil.move(_path, os.path.join(""Test"", ""{:04d}{}"".format(i, base_name[base_name.rfind("".""):])))\r\n            np.save(""Test/_answer"", labels)\r\n            print(""Done"")\r\n    predictor_dir = os.path.join(""Models"", ""Predictors"", FLAGS.model, ""Model.pb"")\r\n    if not os.path.isfile(predictor_dir):\r\n        _t = time.time()\r\n        print(""Predictor not found, training with images in \'_Data\' folder..."")\r\n        if not os.path.isfile(""_Data/_Cache/features.npy"") or not os.path.isfile(""_Data/_Cache/labels.npy""):\r\n            img_paths, labels = fetch_img_data()\r\n            extractor = Extractor(FLAGS.model, img_paths, labels)\r\n            features, labels = extractor.run()\r\n            if not os.path.isdir(""_Data/_Cache""):\r\n                os.makedirs(""_Data/_Cache"")\r\n            _indices = np.random.permutation(len(labels))\r\n            features = features[_indices]\r\n            labels = labels[_indices]\r\n            np.save(""_Data/_Cache/features"", features)\r\n            np.save(""_Data/_Cache/labels"", labels)\r\n        else:\r\n            features, labels = np.load(""_Data/_Cache/features.npy""), np.load(""_Data/_Cache/labels.npy"")\r\n        print(""="" * 30)\r\n        print(""Training Neural Network..."")\r\n        print(""="" * 30)\r\n        nn = NNDist()\r\n        nn.add(""ReLU"", (features.shape[1], 1024), std=0.001, init=0)\r\n        nn.add(""Normalize"")\r\n        nn.add(""Dropout"")\r\n        nn.add(""ReLU"", (1024,), std=0.001, init=0)\r\n        nn.add(""Normalize"")\r\n        nn.add(""Dropout"")\r\n        nn.add(""ReLU"", (512,), std=0.001, init=0)\r\n        nn.add(""Normalize"")\r\n        nn.add(""Dropout"")\r\n        nn.add(""CrossEntropy"", (labels.shape[1],))\r\n        nn.fit(features, labels, lr=0.0001, epoch=25, verbose=1)\r\n        nn.save()\r\n        if not os.path.isdir(os.path.join(""Models"", ""Predictors"", FLAGS.model)):\r\n            os.makedirs(os.path.join(""Models"", ""Predictors"", FLAGS.model))\r\n        print(""Moving \'Frozen.pb\' to {}..."".format(predictor_dir))\r\n        shutil.move(os.path.join(""Models"", ""Cache"", ""Frozen.pb""), predictor_dir)\r\n        print(""Removing \'Cache\' folder..."")\r\n        shutil.rmtree(os.path.join(""Models"", ""Cache""))\r\n        print(""Done"")\r\n        print(""-"" * 30)\r\n        print(""(Train) Time cost: {:8.6} s"".format(time.time() - _t))\r\n    pipeline = Pipeline()\r\n    pipeline.run(FLAGS.images_dir, FLAGS.image_shape, FLAGS.model,\r\n                 FLAGS.delete_cache, FLAGS.extract_only, FLAGS.visualize_only, FLAGS.overview, FLAGS.verbose)\r\n\r\nif __name__ == \'__main__\':\r\n    parser = argparse.ArgumentParser()\r\n    parser.add_argument(\r\n        ""--gen_test"",\r\n        type=bool,\r\n        default=True,\r\n        help=""Whether generate test images""\r\n    )\r\n    parser.add_argument(\r\n        ""--images_dir"",\r\n        type=str,\r\n        default=""Test"",\r\n        help=""Path to test set""\r\n    )\r\n    parser.add_argument(\r\n        ""--image_shape"",\r\n        type=tuple,\r\n        default=(64, 64),\r\n        help=""Image shape""\r\n    )\r\n    parser.add_argument(\r\n        ""--model"",\r\n        type=str,\r\n        default=""v3"",\r\n        help=""Model used to extract & predict""\r\n    )\r\n    parser.add_argument(\r\n        ""--delete_cache"",\r\n        type=bool,\r\n        default=True,\r\n        help=""Whether delete cache""\r\n    )\r\n    parser.add_argument(\r\n        ""--extract_only"",\r\n        type=bool,\r\n        default=False,\r\n        help=""Whether extract only""\r\n    )\r\n    parser.add_argument(\r\n        ""--visualize_only"",\r\n        type=bool,\r\n        default=False,\r\n        help=""Whether visualize only""\r\n    )\r\n    parser.add_argument(\r\n        ""--overview"",\r\n        type=bool,\r\n        default=True,\r\n        help=""Whether overview""\r\n    )\r\n    parser.add_argument(\r\n        ""--verbose"",\r\n        type=bool,\r\n        default=True,\r\n        help=""Whether verbose""\r\n    )\r\n    FLAGS, unparsed = parser.parse_known_args()\r\n    tf.app.run(main=main, argv=[sys.argv[0]] + unparsed)\r\n'"
_Dist/ImageRecognition/ToolBox.py,8,"b'import os\r\nimport cv2\r\nimport math\r\nimport time\r\nimport shutil\r\nimport imghdr\r\nimport numpy as np\r\nimport tensorflow as tf\r\nfrom tensorflow.python.platform import gfile\r\n\r\nfrom Util.ProgressBar import ProgressBar\r\n\r\n\r\nclass Config:\r\n    n_class = 19\r\n    extractors_path = ""Models/Extractors/""\r\n    predictors_path = ""Models/Predictors/""\r\n    image_paths = [""sources/{:04d}"".format(i) for i in range(n_class)]\r\n\r\n    @staticmethod\r\n    def get_image_paths():\r\n        new_paths = []\r\n        for i, path in enumerate(Config.image_paths):\r\n            files = [path + ""/"" + file for file in os.listdir(path)]\r\n            new_paths += files\r\n        return new_paths\r\n\r\n    @staticmethod\r\n    def get_labels():\r\n        labels = []\r\n        for i in range(Config.n_class):\r\n            labels += [[0 if j != i else 1 for j in range(\r\n                Config.n_class)] for _ in range(len(os.listdir(""sources/{:04d}"".format(i))))]\r\n        np.save(""sources/labels"", labels)\r\n\r\n    @staticmethod\r\n    def shuffle(folder):\r\n        _features = np.load(folder + ""/features.npy"")\r\n        _labels = np.load(folder + ""/labels.npy"")\r\n        _indices = np.random.permutation(len(_features))\r\n        _features, _labels = _features[_indices], _labels[_indices]\r\n        np.save(folder + ""/features"", _features)\r\n        np.save(folder + ""/labels"", _labels)\r\n\r\n    @staticmethod\r\n    def split(folder):\r\n        _features = np.load(folder + ""/features.npy"")\r\n        _labels = np.load(folder + ""/labels.npy"")\r\n        _indices = np.random.permutation(len(_features))\r\n        _features, _labels = _features[_indices], _labels[_indices]\r\n        train_len = int(0.9 * len(_features))\r\n        x_train, x_test = _features[:train_len], _features[train_len:]\r\n        y_train, y_test = _labels[:train_len], _labels[train_len:]\r\n        np.save(folder + ""/x_train"", x_train)\r\n        np.save(folder + ""/x_test"", x_test)\r\n        np.save(folder + ""/y_train"", y_train)\r\n        np.save(folder + ""/y_test"", y_test)\r\n\r\n\r\nclass Extractor:\r\n    def __init__(self, extractor=""v3"", image_paths=None, labels=None, mat_dir=None):\r\n        self._extractor = extractor if ""v3"" not in extractor else ""v3""\r\n        self._image_paths = Config.get_image_paths() if image_paths is None else image_paths\r\n        self._labels, self._mat_dir = labels, mat_dir\r\n\r\n    def _create_graph(self):\r\n        tf.reset_default_graph()\r\n        with gfile.FastGFile(Config.extractors_path+self._extractor+""/Model.pb"", ""rb"") as file:\r\n            graph_def = tf.GraphDef()\r\n            graph_def.ParseFromString(file.read())\r\n            if ""cnn"" in self._extractor:\r\n                # Fix nodes\r\n                for node in graph_def.node:\r\n                    if node.op == \'RefSwitch\':\r\n                        node.op = \'Switch\'\r\n                        for index in range(len(node.input)):\r\n                            if \'moving_\' in node.input[index]:\r\n                                node.input[index] += \'/read\'\r\n                    elif node.op == \'Assign\':\r\n                        node.op = \'Sub\'\r\n                        if \'use_locking\' in node.attr:\r\n                            del node.attr[\'use_locking\']\r\n            tf.import_graph_def(graph_def, name="""")\r\n\r\n    def _extract(self, verbose):\r\n        features = []\r\n        with tf.Session() as sess:\r\n            if self._extractor == ""v3"":\r\n                _output = ""pool_3:0""\r\n            elif self._extractor == ""ResNet-v2"":\r\n                _output = ""InceptionResnetV2/Logits/Flatten/Reshape:0""\r\n            elif self._extractor == ""cnn"":\r\n                _output = ""final_result/Reshape:0""\r\n            else:\r\n                _output = ""OutputFlow/Reshape:0""\r\n            flattened_tensor = sess.graph.get_tensor_by_name(_output)\r\n            if self._extractor == ""v3"":\r\n                _entry = ""DecodeJpeg/contents:0""\r\n            elif self._extractor == ""ResNet-v2"":\r\n                _entry = ""Placeholder:0""\r\n            else:\r\n                _entry = ""Entry/Placeholder:0""\r\n            pop_lst = []\r\n            if ""cnn"" in self._extractor or ""ResNet"" in self._extractor and self._mat_dir is not None:\r\n                features = np.load(self._mat_dir)\r\n            else:\r\n                def process(img_path):\r\n                    img_data = gfile.FastGFile(img_path, ""rb"").read()\r\n                    feature = sess.run(flattened_tensor, {\r\n                        _entry: img_data\r\n                    })\r\n                    features.append(np.squeeze(feature))\r\n                for i, image_path in enumerate(self._image_paths):\r\n                    if not os.path.isfile(image_path):\r\n                        continue\r\n                    if ""v3"" in self._extractor:\r\n                        if verbose:\r\n                            print(""Processing {}..."".format(image_path))\r\n                        try:\r\n                            process(image_path)\r\n                        except Exception as err:\r\n                            if verbose:\r\n                                print(err)\r\n                            name, extension = os.path.splitext(image_path)\r\n                            base = os.path.basename(image_path)\r\n                            if extension.lower() in ("".jpg"", "".jpeg""):\r\n                                new_name = name[:image_path.rfind(base)] + ""{:06d}{}"".format(i, extension)\r\n                                print(""Renaming {} to {}..."".format(image_path, new_name))\r\n                                os.rename(image_path, new_name)\r\n                                process(new_name)\r\n                            else:\r\n                                new_name_base = name[:image_path.rfind(base)] + ""{:06d}"".format(i)\r\n                                new_name = new_name_base + "".jpg""\r\n                                print(""Transforming {} to {}..."".format(image_path, new_name))\r\n                                try:\r\n                                    if imghdr.what(image_path) is None:\r\n                                        raise ValueError(""{} is not an image"".format(image_path))\r\n                                    os.rename(image_path, new_name_base + extension)\r\n                                    cv2.imwrite(new_name, cv2.imread(new_name_base + extension))\r\n                                    os.remove(new_name_base + extension)\r\n                                    process(new_name)\r\n                                except Exception as err:\r\n                                    print(err)\r\n                                    print(""Moving {} to \'_err\' folder..."".format(image_path))\r\n                                    if not os.path.isdir(""_err""):\r\n                                        os.makedirs(""_err"")\r\n                                    shutil.move(image_path, os.path.join(""_err"", os.path.basename(image_path)))\r\n                                    pop_lst.append(i)\r\n                    else:\r\n                        if verbose:\r\n                            print(""Reading {}..."".format(image_path))\r\n                        image_data = cv2.imread(image_path)\r\n                        if self._extractor == ""ResNet-v2"":\r\n                            features.append(cv2.resize(image_data, (299, 299)))\r\n                        else:\r\n                            features.append(cv2.resize(image_data, (64, 64)))\r\n            if ""v3"" not in self._extractor:\r\n                features = np.array(features)\r\n                print(""Extracting features..."")\r\n                rs = []\r\n                batch_size = math.floor(1e6 / np.prod(features.shape[1:]))\r\n                epoch = int(math.ceil(len(features) / batch_size))\r\n                bar = ProgressBar(max_value=epoch, name=""Extract"")\r\n                for i in range(epoch):\r\n                    if i == epoch - 1:\r\n                        rs.append(sess.run(flattened_tensor, {\r\n                            _entry: features[i*batch_size:]\r\n                        }))\r\n                    else:\r\n                        rs.append(sess.run(flattened_tensor, {\r\n                            _entry: features[i*batch_size:(i+1)*batch_size]\r\n                        }))\r\n                    bar.update()\r\n                return np.vstack(rs).astype(np.float32)\r\n            if pop_lst:\r\n                labels = []\r\n                pop_cursor, pop_idx = 0, pop_lst[0]\r\n                for i, label in enumerate(self._labels):\r\n                    if i == pop_idx:\r\n                        pop_cursor += 1\r\n                        if pop_cursor < len(pop_lst):\r\n                            pop_idx = pop_lst[pop_cursor]\r\n                        else:\r\n                            pop_idx = -1\r\n                        continue\r\n                    labels.append(label)\r\n                labels = np.array(labels, dtype=np.float32)\r\n            elif self._labels is None:\r\n                labels = None\r\n            else:\r\n                labels = np.array(self._labels, dtype=np.float32)\r\n            return np.array(features, dtype=np.float32), labels\r\n\r\n    def run(self, verbose=True):\r\n        self._create_graph()\r\n        return self._extract(verbose)\r\n\r\n\r\nclass Predictor:\r\n    def __init__(self, predictor=""v3""):\r\n        self._predictor = predictor\r\n        self._entry, self._output = ""Entry/Placeholder:0"", ""OutputFlow/add_9:0""\r\n\r\n    def _create_graph(self):\r\n        tf.reset_default_graph()\r\n        with gfile.FastGFile(Config.predictors_path+self._predictor+""/Model.pb"", ""rb"") as file:\r\n            graph_def = tf.GraphDef()\r\n            graph_def.ParseFromString(file.read())\r\n            if ""cnn"" in self._predictor:\r\n                # Fix nodes\r\n                for node in graph_def.node:\r\n                    if node.op == \'RefSwitch\':\r\n                        node.op = \'Switch\'\r\n                        for index in range(len(node.input)):\r\n                            if \'moving_\' in node.input[index]:\r\n                                node.input[index] += \'/read\'\r\n                    elif node.op == \'Assign\':\r\n                        node.op = \'Sub\'\r\n                        if \'use_locking\' in node.attr:\r\n                            del node.attr[\'use_locking\']\r\n            tf.import_graph_def(graph_def, name="""")\r\n\r\n    def predict(self, x):\r\n        self._create_graph()\r\n        x, rs = np.atleast_2d(x).astype(np.float32), []\r\n        with tf.Session() as sess:\r\n            flattened_tensor = sess.graph.get_tensor_by_name(self._output)\r\n            print(""Predicting..."")\r\n            batch_size = math.floor(1e6 / np.prod(x.shape[1:]))\r\n            epoch = math.ceil(len(x) / batch_size)  # type: int\r\n            bar = ProgressBar(max_value=epoch, name=""Predict"")\r\n            for i in range(epoch):\r\n                if i == epoch - 1:\r\n                    rs.append(sess.run(flattened_tensor, {\r\n                        self._entry: x[i*batch_size:]\r\n                    }))\r\n                else:\r\n                    rs.append(sess.run(flattened_tensor, {\r\n                        self._entry: x[i*batch_size:(i+1)*batch_size]\r\n                    }))\r\n                bar.update()\r\n            return np.vstack(rs).astype(np.float32)\r\n\r\n\r\nclass Pipeline:\r\n    shape = (1440, 576)\r\n\r\n    def __init__(self):\r\n        self._image_paths = []\r\n        self._img_dir = self._rs_dir = None\r\n\r\n        self._n_row = self._n_col = None\r\n        self._results = []\r\n        self._ans = self._pred = self._prob = None\r\n\r\n    @staticmethod\r\n    def get_image_paths(img_dir, pre_process=False):\r\n        image_paths = [img_dir + ""/"" + file for file in os.listdir(img_dir)]\r\n        if not pre_process:\r\n            return [img for img in image_paths if os.path.isfile(img)]\r\n        return [img for img in image_paths if os.path.isfile(img) and imghdr.what(img) is not None]\r\n\r\n    def _get_results(self, ans, img_dir=None, rs_dir=None):\r\n        if img_dir is None:\r\n            img_dir = self._img_dir\r\n        if rs_dir is None:\r\n            rs_dir = self._rs_dir\r\n        y_pred = np.exp(np.load(rs_dir + ""/prediction.npy""))\r\n        y_pred /= np.sum(y_pred, axis=1, keepdims=True)\r\n        pred_classes = np.argmax(y_pred, axis=1)\r\n        if ans is not None:\r\n            true_classes = np.argmax(ans, axis=1)\r\n            true_prob = y_pred[range(len(y_pred)), true_classes]\r\n        else:\r\n            true_classes = None\r\n            true_prob = y_pred[range(len(y_pred)), pred_classes]\r\n        self._ans, self._pred, self._prob = true_classes, pred_classes, true_prob\r\n        images = []\r\n        c_base = 60\r\n        for i, img in enumerate(Pipeline.get_image_paths(img_dir, True)):\r\n            _pred = y_pred[i]\r\n            _indices = np.argsort(_pred)[-3:][::-1]\r\n            label_dic = np.load(os.path.join(""_Data"", ""_Cache"", ""LABEL_DIC.npy""))\r\n            _ps, _labels = _pred[_indices], label_dic[_indices]\r\n            _img = cv2.imread(img)\r\n            if true_classes is None:\r\n                color = np.array([255, 255, 255], dtype=np.uint8)\r\n            else:\r\n                _p = _ps[0]\r\n                if _p <= 1 / 2:\r\n                    _l, _r = 2 * c_base + (255 - 2 * c_base) * 2 * _p, c_base + (255 - c_base) * 2 * _p\r\n                else:\r\n                    _l, _r = 255, 510 * (1 - _p)\r\n                if true_classes[i] == pred_classes[i]:\r\n                    color = np.array([0, _l, _r], dtype=np.uint8)\r\n                else:\r\n                    color = np.array([0, _r, _l], dtype=np.uint8)\r\n            canvas = np.zeros((256, 640, 3), dtype=np.uint8)\r\n            _img = cv2.resize(_img, (256, 256))\r\n            canvas[:, :256] = _img\r\n            canvas[:, 256:] = color\r\n            bar_len = 180\r\n            for j, (_p, _label) in enumerate(zip(_ps, _labels)):\r\n                cv2.putText(canvas, _label, (288, 64 + 64 * j), cv2.LINE_AA, 0.6, (0, 0, 0), 1)\r\n                cv2.rectangle(canvas, (420, 49 + 64 * j), (420 + int(bar_len * _p), 69 + 64 * j), (125, 0, 125), -1)\r\n            images.append(canvas)\r\n        return images\r\n\r\n    def _get_detail(self, event, x, y, *_):\r\n        label_dic = np.load(os.path.join(""_Data"", ""_Cache"", ""LABEL_DIC.npy""))\r\n        if event == cv2.EVENT_LBUTTONDBLCLK:\r\n            _w, _h = Pipeline.shape\r\n            _pw, _ph = _w / self._n_col, _h / self._n_row\r\n            _idx = int(x // _pw + self._n_col * (y // _ph))\r\n            _prob = self._prob[_idx]\r\n            if self._ans is None or self._ans[_idx] == self._pred[_idx]:\r\n                title = ""Detail (prob: {:6.4})"".format(_prob)\r\n            else:\r\n                title = ""True label: {} (prob: {:6.4})"".format(\r\n                    label_dic[self._ans[_idx]], _prob)\r\n            while 1:\r\n                cv2.imshow(title, self._results[_idx])\r\n                if cv2.waitKey(20) & 0xFF == 27:\r\n                    break\r\n            cv2.destroyWindow(title)\r\n\r\n    def run(self, images_dir=""Test"", image_shape=(64, 64), model=""v3"",\r\n            delete_cache=True, extract_only=False, visualize_only=False, overview=True, verbose=True):\r\n        _t = time.time()\r\n        y_pred = None\r\n        if not visualize_only:\r\n            if model != ""v3"":\r\n                print(""="" * 30)\r\n                print(""Resizing images..."")\r\n                print(""="" * 30)\r\n            else:\r\n                print(""="" * 30)\r\n                print(""Reading images..."")\r\n                print(""="" * 30)\r\n            if extract_only:\r\n                self._image_paths = Pipeline.get_image_paths(images_dir)\r\n            else:\r\n                self._image_paths = Pipeline.get_image_paths(images_dir, True)\r\n            rs = []\r\n            _new_path, _mat_dir = images_dir + ""/_cache"", None\r\n            if not os.path.isdir(_new_path):\r\n                os.makedirs(_new_path)\r\n            if model != ""v3"":\r\n                for i, img in enumerate(self._image_paths):\r\n                    _img = cv2.imread(img)\r\n                    if model != ""ResNet-v2"":\r\n                        _img = cv2.resize(_img, image_shape)\r\n                    else:\r\n                        image_shape = (299, 299)\r\n                        _img = cv2.resize(_img, image_shape)/127.5-1\r\n                    if model == ""v3(64)"":\r\n                        _slash_idx = img.rfind(""/"")\r\n                        _new_img = _new_path + ""/"" + img[_slash_idx+1:]\r\n                        cv2.imwrite(_new_img, _img)\r\n                        print(""{} transformed to {} with shape {}"".format(img, _new_img, image_shape))\r\n                        self._image_paths[i] = _new_img\r\n                    else:\r\n                        print(""{} transformed to shape {}"".format(img, image_shape))\r\n                        rs.append(_img.astype(np.float32))\r\n                _mat_dir = _new_path + ""/_mat.npy""\r\n                np.save(_mat_dir, rs)\r\n            print(""Done"")\r\n            print(""="" * 30)\r\n            print(""Using {} to extract features..."".format(model))\r\n            print(""="" * 30)\r\n            features, _ = Extractor(model, self._image_paths, _mat_dir).run(verbose)\r\n            if extract_only:\r\n                np.save(""features"", features)\r\n                Pipeline._delete_cache(images_dir)\r\n                return\r\n            print(""-"" * 30)\r\n            print(""Loading predictor..."")\r\n            y_pred = Predictor(model).predict(features)\r\n            print(""-"" * 30)\r\n        self._img_dir = images_dir\r\n        self._rs_dir = images_dir + ""/_Result""\r\n        label_dic = np.load(os.path.join(""_Data"", ""_Cache"", ""LABEL_DIC.npy""))\r\n        if not visualize_only:\r\n            if not os.path.isdir(images_dir + ""/_Result""):\r\n                os.makedirs(self._rs_dir)\r\n            np.save(self._rs_dir + ""/prediction"", y_pred)\r\n            labels = label_dic[np.argmax(y_pred, axis=1)]\r\n            with open(self._rs_dir + ""/labels.txt"", ""w"") as file:\r\n                file.write(""\\n"".join(labels))\r\n            print(""Done; results saved to \'{}\' folder"".format(self._rs_dir))\r\n            if delete_cache:\r\n                Pipeline._delete_cache(images_dir)\r\n            print(""-"" * 30)\r\n            print(""Done"")\r\n        print(""(Test) Time cost: {:8.6} s"".format(time.time() - _t))\r\n        if overview:\r\n            print(""-"" * 30)\r\n            print(""Visualizing results..."")\r\n            if os.path.isfile(images_dir + ""/_answer.npy""):\r\n                _ans = np.load(images_dir + ""/_answer.npy"")\r\n            else:\r\n                _ans = None\r\n            images = self._get_results(_ans)\r\n            n_row = math.ceil(math.sqrt(len(images)))  # type: int\r\n            n_col = math.ceil(len(images) / n_row)\r\n            pictures = []\r\n            for i in range(n_row):\r\n                if i == n_row - 1:\r\n                    pictures.append(np.hstack(\r\n                        [*images[i*n_col:], np.zeros((256, 640*(n_row*n_col-len(images)), 3)) + 255]).astype(np.uint8))\r\n                else:\r\n                    pictures.append(np.hstack(\r\n                        images[i*n_col:(i+1)*n_col]).astype(np.uint8))\r\n            self._results = images\r\n            self._n_row, self._n_col = n_row, n_col\r\n            big_canvas = np.vstack(pictures).astype(np.uint8)\r\n            overview = cv2.resize(big_canvas, Pipeline.shape)\r\n\r\n            cv2.namedWindow(""Overview"")\r\n            cv2.setMouseCallback(""Overview"", self._get_detail)\r\n            cv2.imshow(""Overview"", overview)\r\n            cv2.waitKey(0)\r\n            cv2.destroyAllWindows()\r\n\r\n            print(""-"" * 30)\r\n            print(""Done"")\r\n\r\n    @staticmethod\r\n    def _delete_cache(images_dir):\r\n        print(""-"" * 30)\r\n        print(""Deleting \'_cache\' folder..."")\r\n        shutil.rmtree(images_dir + ""/_cache"")\r\n'"
_Dist/NeuralNetworks/Base.py,23,"b'import os\nimport sys\nroot_path = os.path.abspath(""../../"")\nif root_path not in sys.path:\n    sys.path.append(root_path)\n\nimport time\nimport math\nimport random\nimport pickle\nimport shutil\nimport logging\nimport numpy as np\nimport tensorflow as tf\nimport matplotlib.pyplot as plt\n\nfrom mpl_toolkits.mplot3d import Axes3D\n\nfrom _Dist.NeuralNetworks.NNUtil import *\n\n\nclass Generator:\n    def __init__(self, x, y, name=""Generator"", weights=None, n_class=None, shuffle=True):\n        self._cache = {}\n        self._x, self._y = np.asarray(x, np.float32), np.asarray(y, np.float32)\n        if weights is None:\n            self._sample_weights = None\n        else:\n            self._sample_weights = np.asarray(weights, np.float32)\n        if n_class is not None:\n            self.n_class = n_class\n        else:\n            y_int = self._y.astype(np.int32)\n            if np.allclose(self._y, y_int):\n                assert y_int.min() == 0, ""Labels should start from 0""\n                self.n_class = y_int.max() + 1\n            else:\n                self.n_class = 1\n        self._name = name\n        self._do_shuffle = shuffle\n        self._all_valid_data = self._generate_all_valid_data()\n        self._valid_indices = np.arange(len(self._all_valid_data))\n        self._random_indices = self._valid_indices.copy()\n        np.random.shuffle(self._random_indices)\n        self._batch_cursor = -1\n\n    def __enter__(self):\n        self._cache_current_status()\n\n    def __exit__(self, exc_type, exc_val, exc_tb):\n        self._restore_cache()\n\n    def __getitem__(self, item):\n        return getattr(self, ""_"" + item)\n\n    def __len__(self):\n        return self.n_valid\n\n    def __str__(self):\n        return ""{}_{}"".format(self._name, self.shape)\n\n    __repr__ = __str__\n\n    @property\n    def n_valid(self):\n        return len(self._valid_indices)\n\n    @property\n    def n_dim(self):\n        return self._x.shape[-1]\n\n    @property\n    def shape(self):\n        return self.n_valid, self.n_dim\n\n    def _generate_all_valid_data(self):\n        return np.hstack([self._x, self._y.reshape([-1, 1])])\n\n    def _cache_current_status(self):\n        self._cache[""_valid_indices""] = self._valid_indices\n        self._cache[""_random_indices""] = self._random_indices\n\n    def _restore_cache(self):\n        self._valid_indices = self._cache[""_valid_indices""]\n        self._random_indices = self._cache[""_random_indices""]\n        self._cache = {}\n\n    def set_indices(self, indices):\n        indices = np.asarray(indices, np.int)\n        self._valid_indices = self._valid_indices[indices]\n        self._random_indices = self._random_indices[indices]\n\n    def set_range(self, start, end=None):\n        if end is None:\n            self._valid_indices = self._valid_indices[start:]\n            self._random_indices = self._random_indices[start:]\n        else:\n            self._valid_indices = self._valid_indices[start:end]\n            self._random_indices = self._random_indices[start:end]\n\n    def get_indices(self, indices):\n        return self._get_data(np.asarray(indices, np.int))\n\n    def get_range(self, start, end=None):\n        if end is None:\n            return self._get_data(self._valid_indices[start:])\n        return self._get_data(self._valid_indices[start:end])\n\n    def _get_data(self, indices, return_weights=True):\n        data = self._all_valid_data[indices]\n        if not return_weights:\n            return data\n        weights = None if self._sample_weights is None else self._sample_weights[indices]\n        return data, weights\n\n    def gen_batch(self, n_batch, re_shuffle=True):\n        n_batch = min(n_batch, self.n_valid)\n        logger = logging.getLogger(""DataReader"")\n        if n_batch == -1:\n            n_batch = self.n_valid\n        if self._batch_cursor < 0:\n            self._batch_cursor = 0\n        if self._do_shuffle:\n            if self._batch_cursor == 0 and re_shuffle:\n                logger.debug(""Re-shuffling random indices"")\n                np.random.shuffle(self._random_indices)\n            indices = self._random_indices\n        else:\n            indices = self._valid_indices\n        logger.debug(""Generating batch with size={}"".format(n_batch))\n        end = False\n        next_cursor = self._batch_cursor + n_batch\n        if next_cursor >= self.n_valid:\n            next_cursor = self.n_valid\n            end = True\n        data, w = self._get_data(indices[self._batch_cursor:next_cursor])\n        self._batch_cursor = -1 if end else next_cursor\n        logger.debug(""Done"")\n        return data, w\n\n    def gen_random_subset(self, n):\n        n = min(n, self.n_valid)\n        logger = logging.getLogger(""DataReader"")\n        logger.debug(""Generating random subset with size={}"".format(n))\n        start = random.randint(0, self.n_valid - n)\n        subset, weights = self._get_data(self._random_indices[start:start + n])\n        logger.debug(""Done"")\n        return subset, weights\n\n    def get_all_data(self, return_weights=True):\n        if self._all_valid_data is not None:\n            if return_weights:\n                return self._all_valid_data, self._sample_weights\n            return self._all_valid_data\n        return self._get_data(self._valid_indices, return_weights)\n\n\nclass Generator3d(Generator):\n    @property\n    def n_time_step(self):\n        return self._x.shape[1]\n\n    @property\n    def shape(self):\n        return self.n_valid, self.n_time_step, self.n_dim\n\n    def _generate_all_valid_data(self):\n        return np.array([(x, y) for x, y in zip(self._x, self._y)])\n\n\nclass Generator4d(Generator3d):\n    @property\n    def height(self):\n        return self._x.shape[1]\n\n    @property\n    def width(self):\n        return self._x.shape[2]\n\n    @property\n    def shape(self):\n        return self.n_valid, self.height, self.width, self.n_dim\n\n\nclass Base:\n    signature = ""Base""\n\n    def __init__(self, name=None, model_param_settings=None, model_structure_settings=None):\n        self.log = {}\n        self._name = name\n        self._name_appendix = """"\n        self._settings_initialized = False\n\n        self._generator_base = Generator\n        self._train_generator = self._test_generator = None\n        self._sample_weights = self._tf_sample_weights = None\n        self.n_dim = self.n_class = None\n        self.n_random_train_subset = self.n_random_test_subset = None\n\n        if model_param_settings is None:\n            self.model_param_settings = {}\n        else:\n            assert_msg = ""model_param_settings should be a dictionary""\n            assert isinstance(model_param_settings, dict), assert_msg\n            self.model_param_settings = model_param_settings\n        self.lr = None\n        self._loss = self._loss_name = self._metric_name = None\n        self._optimizer_name = self._optimizer = None\n        self.n_epoch = self.max_epoch = self.n_iter = self.batch_size = None\n\n        if model_structure_settings is None:\n            self.model_structure_settings = {}\n        else:\n            assert_msg = ""model_structure_settings should be a dictionary""\n            assert isinstance(model_structure_settings, dict), assert_msg\n            self.model_structure_settings = model_structure_settings\n\n        self._model_built = False\n        self.py_collections = self.tf_collections = None\n        self._define_py_collections()\n        self._define_tf_collections()\n\n        self._ws, self._bs = [], []\n        self._is_training = None\n        self._loss = self._train_step = None\n        self._tfx = self._tfy = self._output = self._prob_output = None\n\n        self._sess = None\n        self._graph = tf.Graph()\n        self._sess_config = self.model_param_settings.pop(""sess_config"", None)\n\n    def __str__(self):\n        return self.model_saving_name\n\n    __repr__ = __str__\n\n    @property\n    def name(self):\n        return ""Base"" if self._name is None else self._name\n\n    @property\n    def metric(self):\n        return getattr(Metrics, self._metric_name)\n\n    @property\n    def model_saving_name(self):\n        return ""{}_{}"".format(self.name, self._name_appendix)\n\n    @property\n    def model_saving_path(self):\n        return os.path.join(os.getcwd(), ""_Models"", self.model_saving_name)\n\n    # Settings\n\n    def init_from_data(self, x, y, x_test, y_test, sample_weights, names):\n        self._sample_weights = sample_weights\n        if self._sample_weights is None:\n            self._tf_sample_weights = None\n        else:\n            self._tf_sample_weights = tf.placeholder(tf.float32, name=""sample_weights"")\n\n        self._train_generator = self._generator_base(x, y, ""TrainGenerator"", self._sample_weights, self.n_class)\n        if x_test is not None and y_test is not None:\n            self._test_generator = self._generator_base(x_test, y_test, ""TestGenerator"", n_class=self.n_class)\n        else:\n            self._test_generator = None\n        self.n_random_train_subset = int(len(self._train_generator) * 0.1)\n        if self._test_generator is None:\n            self.n_random_test_subset = -1\n        else:\n            self.n_random_test_subset = len(self._test_generator)\n\n        self.n_dim = self._train_generator.shape[-1]\n        self.n_class = self._train_generator.n_class\n\n        batch_size = self.model_param_settings.setdefault(""batch_size"", 128)\n        self.model_param_settings[""batch_size""] = min(batch_size, len(self._train_generator))\n        n_iter = self.model_param_settings.setdefault(""n_iter"", -1)\n        if n_iter < 0:\n            self.model_param_settings[""n_iter""] = int(len(self._train_generator) / batch_size)\n\n    def init_all_settings(self):\n        self.init_model_param_settings()\n        self.init_model_structure_settings()\n\n    def init_model_param_settings(self):\n        loss = self.model_param_settings.get(""loss"", None)\n        if loss is None:\n            self._loss_name = ""correlation"" if self.n_class == 1 else ""cross_entropy""\n        else:\n            self._loss_name = loss\n        metric = self.model_param_settings.get(""metric"", None)\n        if metric is None:\n            if self.n_class == 1:\n                self._metric_name = ""correlation""\n            elif self.n_class == 2:\n                self._metric_name = ""auc""\n            else:\n                self._metric_name = ""multi_auc""\n        else:\n            self._metric_name = metric\n        self.n_epoch = self.model_param_settings.get(""n_epoch"", 32)\n        self.max_epoch = self.model_param_settings.get(""max_epoch"", 256)\n        self.max_epoch = max(self.max_epoch, self.n_epoch)\n\n        self.batch_size = self.model_param_settings[""batch_size""]\n        self.n_iter = self.model_param_settings[""n_iter""]\n\n        self._optimizer_name = self.model_param_settings.get(""optimizer"", ""Adam"")\n        self.lr = self.model_param_settings.get(""lr"", 1e-3)\n        self._optimizer = getattr(tf.train, ""{}Optimizer"".format(self._optimizer_name))(self.lr)\n\n    def init_model_structure_settings(self):\n        pass\n\n    # Core\n\n    def _fully_connected_linear(self, net, shape, appendix):\n        with tf.name_scope(""Linear{}"".format(appendix)):\n            w = init_w(shape, ""W{}"".format(appendix))\n            b = init_b([shape[1]], ""b{}"".format(appendix))\n            self._ws.append(w)\n            self._bs.append(b)\n            return tf.add(tf.matmul(net, w), b, name=""Linear{}_Output"".format(appendix))\n\n    def _build_model(self, net=None):\n        pass\n\n    def _gen_batch(self, generator, n_batch, gen_random_subset=False, one_hot=False):\n        if gen_random_subset:\n            data, weights = generator.gen_random_subset(n_batch)\n        else:\n            data, weights = generator.gen_batch(n_batch)\n        x, y = data[..., :-1], data[..., -1]\n        if not one_hot:\n            return x, y, weights\n        if self.n_class == 1:\n            y = y.reshape([-1, 1])\n        else:\n            y = Toolbox.get_one_hot(y, self.n_class)\n        return x, y, weights\n\n    def _get_feed_dict(self, x, y=None, weights=None, is_training=False):\n        feed_dict = {self._tfx: x, self._is_training: is_training}\n        if y is not None:\n            feed_dict[self._tfy] = y\n        if self._tf_sample_weights is not None:\n            if weights is None:\n                weights = np.ones(len(x))\n            feed_dict[self._tf_sample_weights] = weights\n        return feed_dict\n\n    def _define_loss_and_train_step(self):\n        self._loss = getattr(Losses, self._loss_name)(self._tfy, self._output, False, self._tf_sample_weights)\n        with tf.control_dependencies(tf.get_collection(tf.GraphKeys.UPDATE_OPS)):\n            self._train_step = self._optimizer.minimize(self._loss)\n\n    def _initialize_session(self):\n        self._sess = tf.Session(graph=self._graph, config=self._sess_config)\n\n    def _initialize_variables(self):\n        self._sess.run(tf.global_variables_initializer())\n\n    def _snapshot(self, i_epoch, i_iter, snapshot_cursor):\n        x_train, y_train, sw_train = self._gen_batch(\n            self._train_generator, self.n_random_train_subset,\n            gen_random_subset=True\n        )\n        if self._test_generator is not None:\n            x_test, y_test, sw_test = self._gen_batch(\n                self._test_generator, self.n_random_test_subset,\n                gen_random_subset=True\n            )\n            if self.n_class == 1:\n                y_test = y_test.reshape([-1, 1])\n            else:\n                y_test = Toolbox.get_one_hot(y_test, self.n_class)\n        else:\n            x_test = y_test = sw_test = None\n        y_train_pred = self._predict(x_train)\n        if x_test is not None:\n            tensor = self._output if self.n_class == 1 else self._prob_output\n            y_test_pred, test_snapshot_loss = self._calculate(\n                x_test, y_test, sw_test,\n                [tensor, self._loss], is_training=False\n            )\n            y_test_pred, test_snapshot_loss = y_test_pred[0], test_snapshot_loss[0]\n        else:\n            y_test_pred = test_snapshot_loss = None\n        train_metric = self.metric(y_train, y_train_pred)\n        if y_test is not None and y_test_pred is not None:\n            test_metric = self.metric(y_test, y_test_pred)\n            if i_epoch >= 0 and i_iter >= 0 and snapshot_cursor >= 0:\n                self.log[""test_snapshot_loss""].append(test_snapshot_loss)\n                self.log[""test_{}"".format(self._metric_name)].append(test_metric)\n                self.log[""train_{}"".format(self._metric_name)].append(train_metric)\n        else:\n            test_metric = None\n        print(""\\rEpoch {:6}   Iter {:8}   Snapshot {:6} ({})  -  Train : {:8.6f}   Test : {}"".format(\n            i_epoch, i_iter, snapshot_cursor, self._metric_name, train_metric,\n            ""None"" if test_metric is None else ""{:8.6f}"".format(test_metric)\n        ), end="""")\n        if i_epoch == i_iter == snapshot_cursor == 0:\n            print()\n        return train_metric, test_metric\n\n    def _calculate(self, x, y=None, weights=None, tensor=None, n_elem=1e7, is_training=False):\n        n_batch = int(n_elem / x.shape[1])\n        n_repeat = int(len(x) / n_batch)\n        if n_repeat * n_batch < len(x):\n            n_repeat += 1\n        cursors = [0]\n        if tensor is None:\n            target = self._prob_output\n        elif isinstance(tensor, list):\n            target = []\n            for t in tensor:\n                if isinstance(t, str):\n                    t = getattr(self, t)\n                if isinstance(t, list):\n                    target += t\n                    cursors.append(len(t))\n                else:\n                    target.append(t)\n                    cursors.append(cursors[-1] + 1)\n        else:\n            target = getattr(self, tensor) if isinstance(tensor, str) else tensor\n        results = [self._sess.run(\n            target, self._get_feed_dict(\n                x[i * n_batch:(i + 1) * n_batch],\n                None if y is None else y[i * n_batch:(i + 1) * n_batch],\n                None if weights is None else weights[i * n_batch:(i + 1) * n_batch],\n                is_training=is_training\n            )\n        ) for i in range(n_repeat)]\n        if not isinstance(target, list):\n            if len(results) == 1:\n                return results[0]\n            return np.vstack(results)\n        if n_repeat > 1:\n            results = [\n                np.vstack([result[i] for result in results]) if target[i].shape.ndims else\n                np.mean([result[i] for result in results]) for i in range(len(target))\n            ]\n        else:\n            results = results[0]\n        if len(cursors) == 1:\n            return results\n        return [results[cursor:cursors[i + 1]] for i, cursor in enumerate(cursors[:-1])]\n\n    def _predict(self, x):\n        tensor = self._output if self.n_class == 1 else self._prob_output\n        output = self._calculate(x, tensor=tensor, is_training=False)\n        if self.n_class == 1:\n            return output.ravel()\n        return output\n\n    def _evaluate(self, x=None, y=None, x_cv=None, y_cv=None, x_test=None, y_test=None, metric=None):\n        if isinstance(metric, str):\n            metric_name, metric = metric, getattr(Metrics, metric)\n        else:\n            metric_name, metric = self._metric_name, self.metric\n        pred = self._predict(x) if x is not None else None\n        cv_pred = self._predict(x_cv) if x_cv is not None else None\n        test_pred = self._predict(x_test) if x_test is not None else None\n        train_metric = None if y is None else metric(y, pred)\n        cv_metric = None if y_cv is None else metric(y_cv, cv_pred)\n        test_metric = None if y_test is None else metric(y_test, test_pred)\n        self._print_metrics(metric_name, train_metric, cv_metric, test_metric)\n        return train_metric, cv_metric, test_metric\n\n    @staticmethod\n    def _print_metrics(metric_name, train_metric=None, cv_metric=None, test_metric=None, only_return=False):\n        msg = ""{}  -  Train : {}   CV : {}   Test : {}"".format(\n            metric_name,\n            ""None"" if train_metric is None else ""{:10.8f}"".format(train_metric),\n            ""None"" if cv_metric is None else ""{:10.8f}"".format(cv_metric),\n            ""None"" if test_metric is None else ""{:10.8f}"".format(test_metric)\n        )\n        return msg if only_return else print(msg)\n\n    def _define_input_and_placeholder(self):\n        self._is_training = tf.placeholder(tf.bool, name=""is_training"")\n        self._tfx = tf.placeholder(tf.float32, [None, self.n_dim], name=""X"")\n        self._tfy = tf.placeholder(tf.float32, [None, self.n_class], name=""Y"")\n\n    def _define_py_collections(self):\n        self.py_collections = [\n            ""_name"", ""n_class"",\n            ""model_param_settings"", ""model_structure_settings""\n        ]\n\n    def _define_tf_collections(self):\n        self.tf_collections = [\n            ""_tfx"", ""_tfy"", ""_output"", ""_prob_output"",\n            ""_loss"", ""_train_step"", ""_is_training""\n        ]\n\n    # Save & Load\n\n    def add_tf_collections(self):\n        for tensor in self.tf_collections:\n            target = getattr(self, tensor)\n            if target is not None:\n                tf.add_to_collection(tensor, target)\n\n    def clear_tf_collections(self):\n        for key in self.tf_collections:\n            tf.get_collection_ref(key).clear()\n\n    def save_collections(self, folder):\n        with open(os.path.join(folder, ""py.core""), ""wb"") as file:\n            param_dict = {name: getattr(self, name) for name in self.py_collections}\n            pickle.dump(param_dict, file)\n        self.add_tf_collections()\n\n    def restore_collections(self, folder):\n        with open(os.path.join(folder, ""py.core""), ""rb"") as file:\n            param_dict = pickle.load(file)\n            for name, value in param_dict.items():\n                setattr(self, name, value)\n        for tensor in self.tf_collections:\n            target = tf.get_collection(tensor)\n            if target is None:\n                continue\n            assert len(target) == 1, ""{} available \'{}\' found"".format(len(target), tensor)\n            setattr(self, tensor, target[0])\n        self.clear_tf_collections()\n\n    @staticmethod\n    def get_model_name(path, idx):\n        targets = os.listdir(path)\n        if idx is None:\n            idx = max([int(target) for target in targets if target.isnumeric()])\n        return os.path.join(path, ""{:06}"".format(idx))\n\n    def save(self, run_id=0, path=None):\n        if path is None:\n            path = self.model_saving_path\n        folder = os.path.join(path, ""{:06}"".format(run_id))\n        if not os.path.isdir(folder):\n            os.makedirs(folder)\n        print(""Saving model"")\n        with self._graph.as_default():\n            saver = tf.train.Saver()\n            self.save_collections(folder)\n            saver.save(self._sess, os.path.join(folder, ""Model""))\n            print(""Model saved to "" + folder)\n            return self\n\n    def load(self, run_id=None, clear_devices=False, path=None):\n        self._model_built = True\n        if path is None:\n            path = self.model_saving_path\n        folder = self.get_model_name(path, run_id)\n        path = os.path.join(folder, ""Model"")\n        print(""Restoring model"")\n        with self._graph.as_default():\n            if self._sess is None:\n                self._initialize_session()\n            saver = tf.train.import_meta_graph(""{}.meta"".format(path), clear_devices)\n            saver.restore(self._sess, os.path.join(folder, ""Model""))\n            self.restore_collections(folder)\n            self.init_all_settings()\n            print(""Model restored from "" + folder)\n            return self\n\n    def save_checkpoint(self, folder):\n        if not os.path.isdir(folder):\n            os.makedirs(folder)\n        with self._graph.as_default():\n            tf.train.Saver().save(self._sess, os.path.join(folder, ""Model""))\n\n    def restore_checkpoint(self, folder):\n        with self._graph.as_default():\n            tf.train.Saver().restore(self._sess, os.path.join(folder, ""Model""))\n\n    # API\n\n    def print_settings(self):\n        pass\n\n    def fit(self, x, y, x_test=None, y_test=None, sample_weights=None, names=(""train"", ""test""),\n            timeit=True, time_limit=-1, snapshot_ratio=3, print_settings=True, verbose=1):\n        t = None\n        if timeit:\n            t = time.time()\n\n        self.init_from_data(x, y, x_test, y_test, sample_weights, names)\n        if not self._settings_initialized:\n            self.init_all_settings()\n            self._settings_initialized = True\n\n        if not self._model_built:\n            with self._graph.as_default():\n                self._initialize_session()\n                with tf.name_scope(""Input""):\n                    self._define_input_and_placeholder()\n                with tf.name_scope(""Model""):\n                    self._build_model()\n                    self._prob_output = tf.nn.softmax(self._output, name=""Prob_Output"")\n                with tf.name_scope(""LossAndTrainStep""):\n                    self._define_loss_and_train_step()\n                with tf.name_scope(""InitializeVariables""):\n                    self._initialize_variables()\n\n        i_epoch = i_iter = j = snapshot_cursor = 0\n        if snapshot_ratio == 0 or x_test is None or y_test is None:\n            use_monitor = False\n            snapshot_step = self.n_iter\n        else:\n            use_monitor = True\n            snapshot_ratio = min(snapshot_ratio, self.n_iter)\n            snapshot_step = int(self.n_iter / snapshot_ratio)\n\n        terminate = False\n        over_fitting_flag = 0\n        n_epoch = self.n_epoch\n        tmp_checkpoint_folder = os.path.join(self.model_saving_path, ""tmp"")\n        if time_limit > 0:\n            time_limit -= time.time() - t\n            if time_limit <= 0:\n                print(""Time limit exceeded before training process started"")\n                return self\n        monitor = TrainMonitor(Metrics.sign_dict[self._metric_name], snapshot_ratio)\n\n        if verbose >= 2:\n            prepare_tensorboard_verbose(self._sess)\n\n        if print_settings:\n            self.print_settings()\n\n        self.log[""iter_loss""] = []\n        self.log[""epoch_loss""] = []\n        self.log[""test_snapshot_loss""] = []\n        self.log[""train_{}"".format(self._metric_name)] = []\n        self.log[""test_{}"".format(self._metric_name)] = []\n        self._snapshot(0, 0, 0)\n\n        while i_epoch < n_epoch:\n            i_epoch += 1\n            epoch_loss = 0\n            for j in range(self.n_iter):\n                i_iter += 1\n                x_batch, y_batch, sw_batch = self._gen_batch(self._train_generator, self.batch_size, one_hot=True)\n                iter_loss = self._sess.run(\n                    [self._loss, self._train_step],\n                    self._get_feed_dict(x_batch, y_batch, sw_batch, is_training=True)\n                )[0]\n                self.log[""iter_loss""].append(iter_loss)\n                epoch_loss += iter_loss\n                if i_iter % snapshot_step == 0 and verbose >= 1:\n                    snapshot_cursor += 1\n                    train_metric, test_metric = self._snapshot(i_epoch, i_iter, snapshot_cursor)\n                    if use_monitor:\n                        check_rs = monitor.check(test_metric)\n                        over_fitting_flag = monitor.over_fitting_flag\n                        if check_rs[""terminate""]:\n                            n_epoch = i_epoch\n                            print(""  -  Early stopped at n_epoch={} due to \'{}\'"".format(\n                                n_epoch, check_rs[""info""]\n                            ))\n                            terminate = True\n                            break\n                        if check_rs[""save_checkpoint""]:\n                            print(""  -  {}"".format(check_rs[""info""]))\n                            self.save_checkpoint(tmp_checkpoint_folder)\n                if 0 < time_limit <= time.time() - t:\n                    print(""  -  Early stopped at n_epoch={} ""\n                          ""due to \'Time limit exceeded\'"".format(i_epoch))\n                    terminate = True\n                    break\n            self.log[""epoch_loss""].append(epoch_loss / (j + 1))\n            if use_monitor:\n                if i_epoch == n_epoch and i_epoch < self.max_epoch and not monitor.info[""terminate""]:\n                    monitor.flat_flag = True\n                    monitor.punish_extension()\n                    n_epoch = min(n_epoch + monitor.extension, self.max_epoch)\n                    print(""  -  Extending n_epoch to {}"".format(n_epoch))\n                if i_epoch == self.max_epoch:\n                    terminate = True\n                    if not monitor.info[""terminate""]:\n                        if not over_fitting_flag:\n                            print(\n                                ""  -  Model seems to be under-fitting but max_epoch reached. ""\n                                ""Increasing max_epoch may improve performance""\n                            )\n                        else:\n                            print(""  -  max_epoch reached"")\n            elif i_epoch == n_epoch:\n                terminate = True\n            if terminate:\n                if os.path.isdir(tmp_checkpoint_folder):\n                    print(""  -  Rolling back to the best checkpoint"")\n                    self.restore_checkpoint(tmp_checkpoint_folder)\n                    shutil.rmtree(tmp_checkpoint_folder)\n                break\n        self._snapshot(-1, -1, -1)\n\n        if timeit:\n            print(""  -  Time Cost: {}"".format(time.time() - t))\n\n        return self\n\n    def predict(self, x):\n        return self._predict(x)\n\n    def predict_classes(self, x):\n        if self.n_class == 1:\n            raise ValueError(""Predicting classes is not permitted in regression problem"")\n        return self._predict(x).argmax(1).astype(np.int32)\n\n    def evaluate(self, x, y, x_cv=None, y_cv=None, x_test=None, y_test=None, metric=None):\n        return self._evaluate(x, y, x_cv, y_cv, x_test, y_test, metric)\n\n    # Visualization\n\n    def draw_losses(self):\n        el, il = self.log[""epoch_loss""], self.log[""iter_loss""]\n        ee_base = np.arange(len(el))\n        ie_base = np.linspace(0, len(el) - 1, len(il))\n        plt.figure()\n        plt.plot(ie_base, il, label=""Iter loss"")\n        plt.plot(ee_base, el, linewidth=3, label=""Epoch loss"")\n        plt.legend()\n        plt.show()\n        return self\n\n    def scatter2d(self, x, y, padding=0.5, title=None):\n        axis, labels = np.asarray(x).T, np.asarray(y)\n\n        print(""="" * 30 + ""\\n"" + str(self))\n        x_min, x_max = np.min(axis[0]), np.max(axis[0])\n        y_min, y_max = np.min(axis[1]), np.max(axis[1])\n        x_padding = max(abs(x_min), abs(x_max)) * padding\n        y_padding = max(abs(y_min), abs(y_max)) * padding\n        x_min -= x_padding\n        x_max += x_padding\n        y_min -= y_padding\n        y_max += y_padding\n\n        if labels.ndim == 1:\n            plot_label_dict = {c: i for i, c in enumerate(set(labels))}\n            n_label = len(plot_label_dict)\n            labels = np.array([plot_label_dict[label] for label in labels])\n        else:\n            n_label = labels.shape[1]\n            labels = np.argmax(labels, axis=1)\n        colors = plt.cm.rainbow([i / n_label for i in range(n_label)])[labels]\n\n        if title is None:\n            title = self.model_saving_name\n\n        indices = [labels == i for i in range(np.max(labels) + 1)]\n        scatters = []\n        plt.figure()\n        plt.title(title)\n        for idx in indices:\n            scatters.append(plt.scatter(axis[0][idx], axis[1][idx], c=colors[idx]))\n        plt.legend(scatters, [""$c_{}$"".format(""{"" + str(i) + ""}"") for i in range(len(scatters))],\n                   ncol=math.ceil(math.sqrt(len(scatters))), fontsize=8)\n        plt.xlim(x_min, x_max)\n        plt.ylim(y_min, y_max)\n        plt.show()\n        return self\n\n    def scatter3d(self, x, y, padding=0.1, title=None):\n        axis, labels = np.asarray(x).T, np.asarray(y)\n\n        print(""="" * 30 + ""\\n"" + str(self))\n        x_min, x_max = np.min(axis[0]), np.max(axis[0])\n        y_min, y_max = np.min(axis[1]), np.max(axis[1])\n        z_min, z_max = np.min(axis[2]), np.max(axis[2])\n        x_padding = max(abs(x_min), abs(x_max)) * padding\n        y_padding = max(abs(y_min), abs(y_max)) * padding\n        z_padding = max(abs(z_min), abs(z_max)) * padding\n        x_min -= x_padding\n        x_max += x_padding\n        y_min -= y_padding\n        y_max += y_padding\n        z_min -= z_padding\n        z_max += z_padding\n\n        def transform_arr(arr):\n            if arr.ndim == 1:\n                dic = {c: i for i, c in enumerate(set(arr))}\n                n_dim = len(dic)\n                arr = np.array([dic[label] for label in arr])\n            else:\n                n_dim = arr.shape[1]\n                arr = np.argmax(arr, axis=1)\n            return arr, n_dim\n\n        if title is None:\n            title = self.model_saving_name\n\n        labels, n_label = transform_arr(labels)\n        colors = plt.cm.rainbow([i / n_label for i in range(n_label)])[labels]\n        indices = [labels == i for i in range(n_label)]\n        scatters = []\n        fig = plt.figure()\n        plt.title(title)\n        ax = fig.add_subplot(111, projection=\'3d\')\n        for _index in indices:\n            scatters.append(ax.scatter(axis[0][_index], axis[1][_index], axis[2][_index], c=colors[_index]))\n        ax.legend(scatters, [""$c_{}$"".format(""{"" + str(i) + ""}"") for i in range(len(scatters))],\n                  ncol=math.ceil(math.sqrt(len(scatters))), fontsize=8)\n        plt.show()\n        return self\n\n    def visualize2d(self, x, y, padding=0.1, dense=200, title=None,\n                    scatter=True, show_org=False, draw_background=True, emphasize=None, extra=None):\n        axis, labels = np.asarray(x).T, np.asarray(y)\n\n        print(""="" * 30 + ""\\n"" + str(self))\n        nx, ny, padding = dense, dense, padding\n        x_min, x_max = np.min(axis[0]), np.max(axis[0])\n        y_min, y_max = np.min(axis[1]), np.max(axis[1])\n        x_padding = max(abs(x_min), abs(x_max)) * padding\n        y_padding = max(abs(y_min), abs(y_max)) * padding\n        x_min -= x_padding\n        x_max += x_padding\n        y_min -= y_padding\n        y_max += y_padding\n\n        def get_base(_nx, _ny):\n            _xf = np.linspace(x_min, x_max, _nx)\n            _yf = np.linspace(y_min, y_max, _ny)\n            n_xf, n_yf = np.meshgrid(_xf, _yf)\n            return _xf, _yf, np.c_[n_xf.ravel(), n_yf.ravel()]\n\n        xf, yf, base_matrix = get_base(nx, ny)\n\n        t = time.time()\n        z = self.predict_classes(base_matrix).reshape((nx, ny))\n        print(""Decision Time: {:8.6f} s"".format(time.time() - t))\n\n        print(""Drawing figures..."")\n        xy_xf, xy_yf = np.meshgrid(xf, yf, sparse=True)\n        if labels.ndim == 1:\n            plot_label_dict = {c: i for i, c in enumerate(set(labels))}\n            n_label = len(plot_label_dict)\n            labels = np.array([plot_label_dict[label] for label in labels])\n        else:\n            n_label = labels.shape[1]\n            labels = np.argmax(labels, axis=1)\n        colors = plt.cm.rainbow([i / n_label for i in range(n_label)])[labels]\n\n        if title is None:\n            title = self.model_saving_name\n\n        if show_org:\n            plt.figure()\n            plt.scatter(axis[0], axis[1], c=colors)\n            plt.xlim(x_min, x_max)\n            plt.ylim(y_min, y_max)\n            plt.show()\n\n        plt.figure()\n        plt.title(title)\n        if draw_background:\n            plt.pcolormesh(xy_xf, xy_yf, z, cmap=plt.cm.Pastel1)\n        else:\n            plt.contour(xf, yf, z, c=\'k-\', levels=[0])\n        if scatter:\n            plt.scatter(axis[0], axis[1], c=colors)\n        if emphasize is not None:\n            indices = np.array([False] * len(axis[0]))\n            indices[np.asarray(emphasize)] = True\n            plt.scatter(axis[0][indices], axis[1][indices], s=80,\n                        facecolors=""None"", zorder=10)\n        if extra is not None:\n            plt.scatter(*np.asarray(extra).T, s=80, zorder=25, facecolors=""red"")\n        plt.xlim(x_min, x_max)\n        plt.ylim(y_min, y_max)\n        plt.show()\n        print(""Done."")\n        return self\n\n    def visualize3d(self, x, y, padding=0.1, dense=100, title=None,\n                    show_org=False, draw_background=True, emphasize=None, extra=None):\n        if False:\n            print(Axes3D.add_artist)\n        axis, labels = np.asarray(x).T, np.asarray(y)\n\n        print(""="" * 30 + ""\\n"" + str(self))\n\n        nx, ny, nz, padding = dense, dense, dense, padding\n        x_min, x_max = np.min(axis[0]), np.max(axis[0])\n        y_min, y_max = np.min(axis[1]), np.max(axis[1])\n        z_min, z_max = np.min(axis[2]), np.max(axis[2])\n        x_padding = max(abs(x_min), abs(x_max)) * padding\n        y_padding = max(abs(y_min), abs(y_max)) * padding\n        z_padding = max(abs(z_min), abs(z_max)) * padding\n        x_min -= x_padding\n        x_max += x_padding\n        y_min -= y_padding\n        y_max += y_padding\n        z_min -= z_padding\n        z_max += z_padding\n\n        def get_base(_nx, _ny, _nz):\n            _xf = np.linspace(x_min, x_max, _nx)\n            _yf = np.linspace(y_min, y_max, _ny)\n            _zf = np.linspace(z_min, z_max, _nz)\n            n_xf, n_yf, n_zf = np.meshgrid(_xf, _yf, _zf)\n            return _xf, _yf, _zf, np.c_[n_xf.ravel(), n_yf.ravel(), n_zf.ravel()]\n\n        xf, yf, zf, base_matrix = get_base(nx, ny, nz)\n\n        t = time.time()\n        z_xyz = self.predict_classes(base_matrix).reshape((nx, ny, nz))\n        p_classes = self.predict_classes(x).astype(np.int8)\n        _, _, _, base_matrix = get_base(10, 10, 10)\n        z_classes = self.predict_classes(base_matrix).astype(np.int8)\n        print(""Decision Time: {:8.6f} s"".format(time.time() - t))\n\n        print(""Drawing figures..."")\n        z_xy = np.average(z_xyz, axis=2)\n        z_yz = np.average(z_xyz, axis=1)\n        z_xz = np.average(z_xyz, axis=0)\n\n        xy_xf, xy_yf = np.meshgrid(xf, yf, sparse=True)\n        yz_xf, yz_yf = np.meshgrid(yf, zf, sparse=True)\n        xz_xf, xz_yf = np.meshgrid(xf, zf, sparse=True)\n\n        def transform_arr(arr):\n            if arr.ndim == 1:\n                dic = {c: i for i, c in enumerate(set(arr))}\n                n_dim = len(dic)\n                arr = np.array([dic[label] for label in arr])\n            else:\n                n_dim = arr.shape[1]\n                arr = np.argmax(arr, axis=1)\n            return arr, n_dim\n\n        labels, n_label = transform_arr(labels)\n        p_classes, _ = transform_arr(p_classes)\n        z_classes, _ = transform_arr(z_classes)\n        colors = plt.cm.rainbow([i / n_label for i in range(n_label)])\n        if extra is not None:\n            ex0, ex1, ex2 = np.asarray(extra).T\n        else:\n            ex0 = ex1 = ex2 = None\n\n        if title is None:\n            title = self.model_saving_name\n\n        if show_org:\n            fig = plt.figure()\n            ax = fig.add_subplot(111, projection=\'3d\')\n            ax.scatter(axis[0], axis[1], axis[2], c=colors[labels])\n            plt.show()\n\n        fig = plt.figure(figsize=(16, 4), dpi=100)\n        plt.title(title)\n        ax1 = fig.add_subplot(131, projection=\'3d\')\n        ax2 = fig.add_subplot(132, projection=\'3d\')\n        ax3 = fig.add_subplot(133, projection=\'3d\')\n\n        ax1.set_title(""Org"")\n        ax2.set_title(""Pred"")\n        ax3.set_title(""Boundary"")\n\n        ax1.scatter(axis[0], axis[1], axis[2], c=colors[labels])\n        ax2.scatter(axis[0], axis[1], axis[2], c=colors[p_classes], s=15)\n        if extra is not None:\n            ax2.scatter(ex0, ex1, ex2, s=80, zorder=25, facecolors=""red"")\n        xyz_xf, xyz_yf, xyz_zf = base_matrix[..., 0], base_matrix[..., 1], base_matrix[..., 2]\n        ax3.scatter(xyz_xf, xyz_yf, xyz_zf, c=colors[z_classes], s=15)\n\n        plt.show()\n        plt.close()\n\n        fig = plt.figure(figsize=(16, 4), dpi=100)\n        ax1 = fig.add_subplot(131)\n        ax2 = fig.add_subplot(132)\n        ax3 = fig.add_subplot(133)\n\n        def _draw(_ax, _x, _xf, _y, _yf, _z):\n            if draw_background:\n                _ax.pcolormesh(_x, _y, _z > 0, cmap=plt.cm.Pastel1)\n            else:\n                _ax.contour(_xf, _yf, _z, c=\'k-\', levels=[0])\n\n        def _emphasize(_ax, axis0, axis1, _c):\n            _ax.scatter(axis0, axis1, c=_c)\n            if emphasize is not None:\n                indices = np.array([False] * len(axis[0]))\n                indices[np.asarray(emphasize)] = True\n                _ax.scatter(axis0[indices], axis1[indices], s=80,\n                            facecolors=""None"", zorder=10)\n\n        def _extra(_ax, axis0, axis1, _c, _ex0, _ex1):\n            _emphasize(_ax, axis0, axis1, _c)\n            if extra is not None:\n                _ax.scatter(_ex0, _ex1, s=80, zorder=25, facecolors=""red"")\n\n        colors = colors[labels]\n\n        ax1.set_title(""xy figure"")\n        _draw(ax1, xy_xf, xf, xy_yf, yf, z_xy)\n        _extra(ax1, axis[0], axis[1], colors, ex0, ex1)\n\n        ax2.set_title(""yz figure"")\n        _draw(ax2, yz_xf, yf, yz_yf, zf, z_yz)\n        _extra(ax2, axis[1], axis[2], colors, ex1, ex2)\n\n        ax3.set_title(""xz figure"")\n        _draw(ax3, xz_xf, xf, xz_yf, zf, z_xz)\n        _extra(ax3, axis[0], axis[2], colors, ex0, ex2)\n\n        plt.show()\n        print(""Done."")\n        return self\n\n\nclass AutoBase:\n    # noinspection PyUnusedLocal\n    def __init__(self, name=None, data_info=None, pre_process_settings=None, nan_handler_settings=None,\n                 *args, **kwargs):\n        if name is None:\n            raise ValueError(""name should be provided when using AutoBase"")\n        self._name = name\n\n        self._data_folder = None\n        self.whether_redundant = None\n        self.feature_sets = self.sparsity = self.class_prior = None\n        self.n_features = self.all_num_idx = self.transform_dicts = None\n\n        self.py_collections = []\n\n        if data_info is None:\n            data_info = {}\n        else:\n            assert_msg = ""data_info should be a dictionary""\n            assert isinstance(data_info, dict), assert_msg\n        self.data_info = data_info\n        self._data_info_initialized = False\n        self.numerical_idx = self.categorical_columns = None\n\n        if pre_process_settings is None:\n            pre_process_settings = {}\n        else:\n            assert_msg = ""pre_process_settings should be a dictionary""\n            assert isinstance(pre_process_settings, dict), assert_msg\n        self.pre_process_settings = pre_process_settings\n        self._pre_processors = None\n        self.pre_process_method = self.scale_method = self.reuse_mean_and_std = None\n\n        if nan_handler_settings is None:\n            nan_handler_settings = {}\n        else:\n            assert_msg = ""nan_handler_settings should be a dictionary""\n            assert isinstance(nan_handler_settings, dict), assert_msg\n        self.nan_handler_settings = nan_handler_settings\n        self._nan_handler = None\n        self.nan_handler_method = self.reuse_nan_handler_values = None\n\n        self.init_pre_process_settings()\n        self.init_nan_handler_settings()\n\n    @property\n    def label2num_dict(self):\n        return None if not self.transform_dicts[-1] else self.transform_dicts[-1]\n\n    @property\n    def num2label_dict(self):\n        label2num_dict = self.label2num_dict\n        if label2num_dict is None:\n            return\n        num_label_list = sorted([(i, c) for c, i in label2num_dict.items()])\n        return np.array([label for _, label in num_label_list])\n\n    @property\n    def valid_numerical_idx(self):\n        return np.array([\n            is_numerical for is_numerical in self.numerical_idx\n            if is_numerical is not None\n        ])\n\n    @property\n    def valid_n_features(self):\n        return np.array([\n            n_feature for i, n_feature in enumerate(self.n_features)\n            if self.numerical_idx[i] is not None\n        ])\n\n    def init_data_info(self):\n        if self._data_info_initialized:\n            return\n        self._data_info_initialized = True\n        self.numerical_idx = self.data_info.get(""numerical_idx"", None)\n        self.categorical_columns = self.data_info.get(""categorical_columns"", None)\n        self.feature_sets = self.data_info.get(""feature_sets"", None)\n        self.sparsity = self.data_info.get(""sparsity"", None)\n        self.class_prior = self.data_info.get(""class_prior"", None)\n        if self.feature_sets is not None and self.numerical_idx is not None:\n            self.n_features = [len(feature_set) for feature_set in self.feature_sets]\n            self._gen_categorical_columns()\n        self._data_folder = self.data_info.get(""data_folder"", ""_Data"")\n        self.data_info.setdefault(""file_type"", ""txt"")\n        self.data_info.setdefault(""shuffle"", True)\n        self.data_info.setdefault(""test_rate"", 0.1)\n        self.data_info.setdefault(""stage"", 3)\n\n    def init_pre_process_settings(self):\n        self.pre_process_method = self.pre_process_settings.setdefault(""pre_process_method"", ""normalize"")\n        self.scale_method = self.pre_process_settings.setdefault(""scale_method"", ""truncate"")\n        self.reuse_mean_and_std = self.pre_process_settings.setdefault(""reuse_mean_and_std"", False)\n        if self.pre_process_method is not None and self._pre_processors is None:\n            self._pre_processors = {}\n\n    def init_nan_handler_settings(self):\n        self.nan_handler_method = self.nan_handler_settings.setdefault(""nan_handler_method"", ""median"")\n        self.reuse_nan_handler_values = self.nan_handler_settings.setdefault(""reuse_nan_handler_values"", True)\n\n    def _auto_init_from_data(self, x, y, x_test, y_test, names):\n        stage = self.data_info[""stage""]\n        shuffle = self.data_info[""shuffle""]\n        file_type = self.data_info[""file_type""]\n        test_rate = self.data_info[""test_rate""]\n        args = (self.numerical_idx, file_type, names, shuffle, test_rate, stage)\n        if x is None or y is None:\n            x, y, x_test, y_test = self._load_data(None, *args)\n        else:\n            data = np.hstack([x, y.reshape([-1, 1])])\n            if x_test is not None and y_test is not None:\n                data = (data, np.hstack([x_test, y_test.reshape([-1, 1])]))\n            x, y, x_test, y_test = self._load_data(data, *args)\n        self._handle_unbalance(y)\n        self._handle_sparsity()\n        return x, y, x_test, y_test\n\n    def _handle_unbalance(self, y):\n        if self.n_class == 1:\n            return\n        class_ratio = self.class_prior.min() / self.class_prior.max()\n        if class_ratio < 0.1:\n            warn_msg = ""Sample weights will be used since class_ratio < 0.1 ({:8.6f})"".format(class_ratio)\n            print(warn_msg)\n            if self._sample_weights is None:\n                print(""Sample weights are not provided, they\'ll be generated automatically"")\n                self._sample_weights = np.ones(len(y)) / self.class_prior[y.astype(np.int)]\n                self._sample_weights /= self._sample_weights.sum()\n                self._sample_weights *= len(y)\n\n    def _handle_sparsity(self):\n        if self.sparsity >= 0.75:\n            warn_msg = ""Dropout will be disabled since data sparsity >= 0.75 ({:8.6f})"".format(self.sparsity)\n            print(warn_msg)\n            self.dropout_keep_prob = 1.\n\n    def _gen_categorical_columns(self):\n        self.categorical_columns = [\n            (i, value) for i, value in enumerate(self.valid_n_features)\n            if not self.valid_numerical_idx[i] and self.valid_numerical_idx[i] is not None\n        ]\n        if not self.valid_numerical_idx[-1]:\n            self.categorical_columns.pop()\n\n    def _transform_data(self, data, name, train_name=""train"",\n                        include_label=False, refresh_redundant_info=False, stage=3):\n        print(""Transforming {0}data{2} at stage {1}"".format(\n            ""{} "".format(name), stage,\n            """" if name == train_name or not self.reuse_mean_and_std else\n            "" with {} data"".format(train_name),\n        ))\n        is_ndarray = isinstance(data, np.ndarray)\n        if refresh_redundant_info or self.whether_redundant is None:\n            self.whether_redundant = np.array([\n                True if local_dict is None else False\n                for local_dict in self.transform_dicts\n            ])\n        targets = [\n            (i, local_dict) for i, (idx, local_dict) in enumerate(\n                zip(self.numerical_idx, self.transform_dicts)\n            ) if not idx and local_dict and not self.whether_redundant[i]\n        ]\n        if targets and targets[-1][0] == len(self.numerical_idx) - 1 and not include_label:\n            targets = targets[:-1]\n        if stage == 1 or stage == 3:\n            # Transform data & Handle redundant\n            n_redundant = np.sum(self.whether_redundant)\n            if n_redundant == 0:\n                whether_redundant = None\n            else:\n                whether_redundant = self.whether_redundant\n                if not include_label:\n                    whether_redundant = whether_redundant[:-1]\n                if refresh_redundant_info:\n                    warn_msg = ""{} redundant: {}{}"".format(\n                        ""These {} columns are"".format(n_redundant) if n_redundant > 1 else ""One column is"",\n                        [i for i, redundant in enumerate(whether_redundant) if redundant],\n                        "", {} will be removed"".format(""it"" if n_redundant == 1 else ""they"")\n                    )\n                    print(warn_msg)\n            valid_indices = [\n                i for i, redundant in enumerate(self.whether_redundant)\n                if not redundant\n            ]\n            if not include_label:\n                valid_indices = valid_indices[:-1]\n            for i, line in enumerate(data):\n                for j, local_dict in targets:\n                    elem = line[j]\n                    if isinstance(elem, str):\n                        line[j] = local_dict.get(elem, local_dict.get(""nan"", len(local_dict)))\n                    elif math.isnan(elem):\n                        line[j] = local_dict[""nan""]\n                    else:\n                        line[j] = local_dict.get(elem, local_dict.get(""nan"", len(local_dict)))\n                if not is_ndarray and whether_redundant is not None:\n                    data[i] = [line[j] for j in valid_indices]\n            if is_ndarray and whether_redundant is not None:\n                data = data[..., valid_indices].astype(np.float32)\n            else:\n                data = np.array(data, dtype=np.float32)\n        if stage == 2 or stage == 3:\n            data = np.asarray(data, dtype=np.float32)\n            # Handle nan\n            if self._nan_handler is None:\n                self._nan_handler = NanHandler(\n                    method=self.nan_handler_method,\n                    reuse_values=self.reuse_nan_handler_values\n                )\n            data = self._nan_handler.transform(data, self.valid_numerical_idx[:-1])\n            # Pre-process data\n            if self._pre_processors is not None:\n                pre_processor_name = train_name if self.reuse_mean_and_std else name\n                pre_processor = self._pre_processors.setdefault(\n                    pre_processor_name, PreProcessor(\n                        self.pre_process_method, self.scale_method\n                    )\n                )\n                if not include_label:\n                    data = pre_processor.transform(data, self.valid_numerical_idx[:-1])\n                else:\n                    data[..., :-1] = pre_processor.transform(data[..., :-1], self.valid_numerical_idx[:-1])\n        return data\n\n    def _get_label_dict(self):\n        labels = self.feature_sets[-1]\n        sorted_labels = sorted(labels)\n        if not all(Toolbox.is_number(str(label)) for label in labels):\n            return {key: i for i, key in enumerate(sorted_labels)}\n        if not sorted_labels:\n            return {}\n        numerical_labels = np.array(sorted_labels, np.float32)\n        if numerical_labels.max() - numerical_labels.min() != self.n_class - 1:\n            return {key: i for i, key in enumerate(sorted_labels)}\n        return {}\n\n    def _get_transform_dicts(self):\n        self.transform_dicts = [\n            None if is_numerical is None else\n            {key: i for i, key in enumerate(sorted(feature_set))}\n            if not is_numerical and (not all_num or not np.allclose(\n                np.sort(np.array(list(feature_set), np.float32).astype(np.int32)),\n                np.arange(0, len(feature_set))\n            )) else {} for is_numerical, feature_set, all_num in zip(\n                self.numerical_idx[:-1], self.feature_sets[:-1], self.all_num_idx[:-1]\n            )\n        ]\n        if self.n_class == 1:\n            self.transform_dicts.append({})\n        else:\n            self.transform_dicts.append(self._get_label_dict())\n\n    def _get_data_from_file(self, file_type, test_rate, target=None):\n        if file_type == ""txt"":\n            sep, include_header = "" "", False\n        elif file_type == ""csv"":\n            sep, include_header = "","", True\n        else:\n            raise NotImplementedError(""File type \'{}\' not recognized"".format(file_type))\n        if target is None:\n            target = os.path.join(self._data_folder, self._name)\n        if not os.path.isdir(target):\n            with open(target + "".{}"".format(file_type), ""r"") as file:\n                data = Toolbox.get_data(file, sep, include_header)\n        else:\n            with open(os.path.join(target, ""train.{}"".format(file_type)), ""r"") as file:\n                train_data = Toolbox.get_data(file, sep, include_header)\n            test_rate = 0\n            test_file = os.path.join(target, ""test.{}"".format(file_type))\n            if not os.path.isfile(test_file):\n                data = train_data\n            else:\n                with open(test_file, ""r"") as file:\n                    test_data = Toolbox.get_data(file, sep, include_header)\n                data = (train_data, test_data)\n        return data, test_rate\n\n    def _load_data(self, data=None, numerical_idx=None, file_type=""txt"", names=(""train"", ""test""),\n                   shuffle=True, test_rate=0.1, stage=3):\n        use_cached_data = False\n        train_data = test_data = None\n        data_cache_folder = os.path.join(self._data_folder, ""_Cache"", self._name)\n        data_info_folder = os.path.join(self._data_folder, ""_DataInfo"")\n        data_info_file = os.path.join(data_info_folder, ""{}.info"".format(self._name))\n        train_data_file = os.path.join(data_cache_folder, ""train.npy"")\n        test_data_file = os.path.join(data_cache_folder, ""test.npy"")\n\n        if data is None and stage >= 2 and os.path.isfile(train_data_file):\n            print(""Restoring data"")\n            use_cached_data = True\n            train_data = np.load(train_data_file)\n            if not os.path.isfile(test_data_file):\n                test_data = None\n                data = train_data\n            else:\n                test_data = np.load(test_data_file)\n                data = (train_data, test_data)\n        if use_cached_data:\n            n_train = None\n        else:\n            if data is None:\n                is_ndarray = False\n                data, test_rate = self._get_data_from_file(file_type, test_rate)\n            else:\n                is_ndarray = True\n                if not isinstance(data, tuple):\n                    test_rate = 0\n                    data = np.asarray(data, dtype=np.float32)\n                else:\n                    data = tuple(\n                        arr if isinstance(arr, list) else\n                        np.asarray(arr, np.float32) for arr in data\n                    )\n            if isinstance(data, tuple):\n                if shuffle:\n                    np.random.shuffle(data[0]) if is_ndarray else random.shuffle(data[0])\n                n_train = len(data[0])\n                data = np.vstack(data) if is_ndarray else data[0] + data[1]\n            else:\n                if shuffle:\n                    np.random.shuffle(data) if is_ndarray else random.shuffle(data)\n                n_train = int(len(data) * (1 - test_rate)) if test_rate > 0 else -1\n\n        if not os.path.isdir(data_info_folder):\n            os.makedirs(data_info_folder)\n        if not os.path.isfile(data_info_file) or stage == 1:\n            print(""Generating data info"")\n            if numerical_idx is not None:\n                self.numerical_idx = numerical_idx\n            elif self.numerical_idx is not None:\n                numerical_idx = self.numerical_idx\n            if not self.feature_sets or not self.n_features or not self.all_num_idx:\n                is_regression = self.data_info.pop(\n                    ""is_regression"",\n                    numerical_idx is not None and numerical_idx[-1]\n                )\n                self.feature_sets, self.n_features, self.all_num_idx, self.numerical_idx = (\n                    Toolbox.get_feature_info(data, numerical_idx, is_regression)\n                )\n            self.n_class = 1 if self.numerical_idx[-1] else self.n_features[-1]\n            self._get_transform_dicts()\n            with open(data_info_file, ""wb"") as file:\n                pickle.dump([\n                    self.n_features, self.numerical_idx, self.transform_dicts\n                ], file)\n        elif stage == 3:\n            print(""Restoring data info"")\n            with open(data_info_file, ""rb"") as file:\n                info = pickle.load(file)\n                self.n_features, self.numerical_idx, self.transform_dicts = info\n            self.n_class = 1 if self.numerical_idx[-1] else self.n_features[-1]\n\n        if not use_cached_data:\n            if n_train > 0:\n                train_data, test_data = data[:n_train], data[n_train:]\n            else:\n                train_data, test_data = data, None\n            train_name, test_name = names\n            train_data = self._transform_data(train_data, train_name, train_name, True, True, stage)\n            if test_data is not None:\n                test_data = self._transform_data(test_data, test_name, train_name, True, stage=stage)\n        self._gen_categorical_columns()\n        if not use_cached_data and stage == 3:\n            print(""Caching data..."")\n            if not os.path.isdir(data_cache_folder):\n                os.makedirs(data_cache_folder)\n            np.save(train_data_file, train_data)\n            if test_data is not None:\n                np.save(test_data_file, test_data)\n\n        x, y = train_data[..., :-1], train_data[..., -1]\n        if test_data is not None:\n            x_test, y_test = test_data[..., :-1], test_data[..., -1]\n        else:\n            x_test = y_test = None\n        self.sparsity = ((x == 0).sum() + np.isnan(x).sum()) / np.prod(x.shape)\n        _, class_counts = np.unique(y, return_counts=True)\n        self.class_prior = class_counts / class_counts.sum()\n\n        self.data_info[""numerical_idx""] = self.numerical_idx\n        self.data_info[""categorical_columns""] = self.categorical_columns\n\n        return x, y, x_test, y_test\n\n    def _pop_preprocessor(self, name):\n        if isinstance(self._pre_processors, dict) and name in self._pre_processors:\n            self._pre_processors.pop(name)\n\n    def get_transformed_data_from_file(self, file, file_type=""txt"", include_label=False):\n        x, _ = self._get_data_from_file(file_type, 0, file)\n        x = self._transform_data(x, ""new"", include_label=include_label)\n        self._pop_preprocessor(""new"")\n        return x\n\n    def get_labels_from_classes(self, classes):\n        num2label_dict = self.num2label_dict\n        if num2label_dict is None:\n            return classes\n        return num2label_dict[classes]\n\n    def predict_labels(self, x):\n        return self.get_labels_from_classes(self.predict_classes(x))\n\n    # Signatures\n\n    def fit(self, x=None, y=None, x_test=None, y_test=None, sample_weights=None, names=(""train"", ""test""),\n            timeit=True, time_limit=-1, snapshot_ratio=3, print_settings=True, verbose=1):\n        raise ValueError\n\n    def predict_classes(self, x):\n        raise ValueError\n\n    def predict_from_file(self, file, file_type=""txt"", include_label=False):\n        raise ValueError\n\n    def predict_classes_from_file(self, file, file_type=""txt"", include_label=False):\n        raise ValueError\n\n    def predict_labels_from_file(self, file, file_type=""txt"", include_label=False):\n        raise ValueError\n\n    def evaluate_from_file(self, file, file_type=""txt""):\n        raise ValueError\n\n\nclass AutoMeta(type):\n    def __new__(mcs, *args, **kwargs):\n        name_, bases, attr = args[:3]\n        auto_base, model = bases\n\n        def __init__(self, name=None, data_info=None, model_param_settings=None, model_structure_settings=None,\n                     pre_process_settings=None, nan_handler_settings=None):\n            auto_base.__init__(self, name, data_info, pre_process_settings, nan_handler_settings)\n            if model.signature != ""Advanced"":\n                model.__init__(self, name, model_param_settings, model_structure_settings)\n            else:\n                model.__init__(self, name, data_info, model_param_settings, model_structure_settings)\n\n        def _define_py_collections(self):\n            model._define_py_collections(self)\n            self.py_collections += [\n                ""pre_process_settings"", ""nan_handler_settings"",\n                ""_pre_processors"", ""_nan_handler"", ""transform_dicts"",\n                ""numerical_idx"", ""categorical_columns"", ""transform_dicts""\n            ]\n\n        def init_data_info(self):\n            auto_base.init_data_info(self)\n\n        def init_from_data(self, x, y, x_test, y_test, sample_weights, names):\n            self.init_data_info()\n            x, y, x_test, y_test = self._auto_init_from_data(x, y, x_test, y_test, names)\n            model.init_from_data(self, x, y, x_test, y_test, sample_weights, names)\n\n        def fit(self, x=None, y=None, x_test=None, y_test=None, sample_weights=None, names=(""train"", ""test""),\n                timeit=True, time_limit=-1, snapshot_ratio=3, print_settings=True, verbose=1):\n            return model.fit(\n                self, x, y, x_test, y_test, sample_weights, names,\n                timeit, time_limit, snapshot_ratio, print_settings, verbose\n            )\n\n        def predict(self, x):\n            rs = self._predict(self._transform_data(x, ""new"", include_label=False))\n            self._pop_preprocessor(""new"")\n            return rs\n\n        def predict_classes(self, x):\n            if self.n_class == 1:\n                raise ValueError(""Predicting classes is not permitted in regression problem"")\n            return self.predict(x).argmax(1).astype(np.int32)\n\n        def predict_target_prob(self, x, target):\n            prob = self.predict(x)\n            label2num_dict = self.label2num_dict\n            if label2num_dict is not None:\n                target = label2num_dict[target]\n            return prob[..., target]\n\n        def predict_from_file(self, file, file_type=""txt"", include_label=False):\n            x = self.get_transformed_data_from_file(file, file_type, include_label)\n            if include_label:\n                x = x[..., :-1]\n            return self._predict(x)\n\n        def predict_classes_from_file(self, file, file_type=""txt"", include_label=False):\n            if self.numerical_idx[-1]:\n                raise ValueError(""Predicting classes is not permitted in regression problem"")\n            x = self.get_transformed_data_from_file(file, file_type, include_label)\n            if include_label:\n                x = x[..., :-1]\n            return self._predict(x).argmax(1).astype(np.int32)\n\n        def predict_labels_from_file(self, file, file_type=""txt"", include_label=False):\n            classes = self.predict_classes_from_file(file, file_type, include_label)\n            return self.get_labels_from_classes(classes)\n\n        def evaluate(self, x, y, x_cv=None, y_cv=None, x_test=None, y_test=None, metric=None):\n            x = self._transform_data(x, ""train"")\n            cv_name = ""cv"" if ""cv"" in self._pre_processors else ""tmp_cv""\n            test_name = ""test"" if ""test"" in self._pre_processors else ""tmp_test""\n            if x_cv is not None:\n                x_cv = self._transform_data(x_cv, cv_name)\n            if x_test is not None:\n                x_test = self._transform_data(x_test, test_name)\n            if cv_name == ""tmp_cv"":\n                self._pop_preprocessor(""tmp_cv"")\n            if test_name == ""tmp_test"":\n                self._pop_preprocessor(""tmp_test"")\n            return self._evaluate(x, y, x_cv, y_cv, x_test, y_test, metric)\n\n        def print_settings(self):\n            model.print_settings(self)\n            msg = ""NanHandler       : {}"".format(""None"" if not self._nan_handler else """") + ""\\n""\n            if self._nan_handler:\n                msg += ""\\n"".join(""-> {:14}: {}"".format(k, v) for k, v in sorted(\n                    self.nan_handler_settings.items()\n                )) + ""\\n""\n            msg += ""-"" * 100 + ""\\n\\n""\n            msg += ""PreProcessor     : {}"".format(""None"" if not self._pre_processors else """") + ""\\n""\n            if self._pre_processors:\n                msg += ""\\n"".join(""-> {:14}: {}"".format(k, v) for k, v in sorted(\n                    self.pre_process_settings.items()\n                )) + ""\\n""\n            msg += ""-"" * 100 + ""\\n""\n            print(msg)\n\n        for key, value in locals().items():\n            if str(value).find(""function"") >= 0:\n                attr[key] = value\n\n        return type(name_, bases, attr)\n'"
_Dist/NeuralNetworks/DistBase.py,27,"b'import os\nimport sys\nroot_path = os.path.abspath(""../../"")\nif root_path not in sys.path:\n    sys.path.append(root_path)\n\nimport time\nimport math\nimport random\nimport pickle\nimport shutil\nimport logging\nimport itertools\nimport collections\nimport numpy as np\nimport tensorflow as tf\nimport matplotlib.pyplot as plt\n\ntf.logging.set_verbosity(tf.logging.FATAL)\n\nfrom copy import deepcopy\nfrom mpl_toolkits.mplot3d import Axes3D\n\nfrom Util.ProgressBar import ProgressBar\nfrom _Dist.NeuralNetworks.NNUtil import *\nfrom _Dist.NeuralNetworks.Base import Generator\n\n\nclass DataCacheMixin:\n    @property\n    def data_folder(self):\n        return self.data_info.get(""data_folder"", ""_Data"")\n\n    @property\n    def data_cache_folder(self):\n        folder = os.path.join(self.data_folder, ""_Cache"", self._name)\n        if not os.path.isdir(folder):\n            os.makedirs(folder)\n        return folder\n\n    @property\n    def data_info_folder(self):\n        folder = os.path.join(self.data_folder, ""_DataInfo"")\n        if not os.path.isdir(folder):\n            os.makedirs(folder)\n        return folder\n\n    @property\n    def data_info_file(self):\n        return os.path.join(self.data_info_folder, ""{}.info"".format(self._name))\n\n    @property\n    def train_data_file(self):\n        return os.path.join(self.data_cache_folder, ""train.npy"")\n\n    @property\n    def test_data_file(self):\n        return os.path.join(self.data_cache_folder, ""test.npy"")\n\n\nclass LoggingMixin:\n    logger = logging.getLogger("""")\n    initialized_log_file = set()\n\n    @property\n    def logging_folder_name(self):\n        folder = os.path.join(os.getcwd(), ""_Tmp"", ""_Logging"", self.name)\n        if not os.path.isdir(folder):\n            os.makedirs(folder)\n        return folder\n\n    def _init_logging(self):\n        if self.loggers is not None:\n            return\n        console = logging.StreamHandler()\n        console.setLevel(logging.INFO)\n        formatter = logging.Formatter(""%(name)20s - %(levelname)8s  -  %(message)s"")\n        console.setFormatter(formatter)\n        root_logger = logging.getLogger("""")\n        root_logger.handlers.clear()\n        root_logger.setLevel(logging.DEBUG)\n        root_logger.addHandler(console)\n        self.loggers = {}\n\n    def get_logger(self, name, file):\n        if name in self.loggers:\n            return self.loggers[name]\n        folder = self.logging_folder_name\n        log_file = os.path.join(folder, file)\n        if log_file not in self.initialized_log_file:\n            with open(log_file, ""w""):\n                pass\n            self.initialized_log_file.add(log_file)\n        log_file = logging.FileHandler(log_file, ""a"")\n        log_file.setLevel(logging.DEBUG)\n        formatter = logging.Formatter(\n            ""%(asctime)s - %(name)20s - %(levelname)8s  -  %(message)s"",\n            ""%Y-%m-%d %H:%M:%S""\n        )\n        log_file.setFormatter(formatter)\n        logger = logging.getLogger(name)\n        logger.addHandler(log_file)\n        self.loggers[name] = logger\n        return logger\n\n    def log_msg(self, msg, level=logging.DEBUG, logger=None):\n        logger = self.logger if logger is None else logger\n        print(msg) if logger is print else logger.log(level, msg)\n\n    def log_block_msg(self, title=""Done"", header=""Result"", body="""", level=logging.DEBUG, logger=None):\n        msg = title + ""\\n"" + ""\\n"".join([""="" * 100, header, ""-"" * 100])\n        if body:\n            msg += ""\\n{}\\n"".format(body) + ""-"" * 100\n        self.log_msg(msg, level, logger)\n\n\nclass Base(LoggingMixin):\n    signature = ""Base""\n\n    def __init__(self, name=None, model_param_settings=None, model_structure_settings=None):\n        self.log = {}\n        self._name = name\n        self._name_appendix = """"\n        self._settings_initialized = False\n\n        self._generator_base = Generator\n        self._train_generator = self._test_generator = None\n        self._sample_weights = self._tf_sample_weights = None\n        self.n_dim = self.n_class = None\n        self.n_random_train_subset = self.n_random_test_subset = None\n\n        if model_param_settings is None:\n            self.model_param_settings = {}\n        else:\n            assert_msg = ""model_param_settings should be a dictionary""\n            assert isinstance(model_param_settings, dict), assert_msg\n            self.model_param_settings = model_param_settings\n        self.lr = None\n        self._loss = self._loss_name = self._metric_name = None\n        self._optimizer_name = self._optimizer = None\n        self.n_epoch = self.max_epoch = self.n_iter = self.batch_size = None\n\n        if model_structure_settings is None:\n            self.model_structure_settings = {}\n        else:\n            assert_msg = ""model_structure_settings should be a dictionary""\n            assert isinstance(model_structure_settings, dict), assert_msg\n            self.model_structure_settings = model_structure_settings\n\n        self._model_built = False\n        self.py_collections = self.tf_collections = None\n        self._define_py_collections()\n        self._define_tf_collections()\n\n        self._ws, self._bs = [], []\n        self._is_training = None\n        self._loss = self._train_step = None\n        self._tfx = self._tfy = self._output = self._prob_output = None\n\n        self._sess = None\n        self._graph = tf.Graph()\n        self._sess_config = self.model_param_settings.pop(""sess_config"", None)\n        self.loggers = None\n        self._init_logging()\n\n    def __str__(self):\n        return self.model_saving_name\n\n    __repr__ = __str__\n\n    @property\n    def name(self):\n        return ""Base"" if self._name is None else self._name\n\n    @property\n    def metric(self):\n        return getattr(Metrics, self._metric_name)\n\n    @property\n    def model_saving_name(self):\n        return ""{}_{}"".format(self.name, self._name_appendix)\n\n    @property\n    def model_saving_path(self):\n        return os.path.join(os.getcwd(), ""_Models"", self.model_saving_name)\n\n    # Settings\n\n    def init_from_data(self, x, y, x_test, y_test, sample_weights, names):\n        self._sample_weights = sample_weights\n        if self._sample_weights is None:\n            self._tf_sample_weights = None\n        else:\n            self._tf_sample_weights = tf.placeholder(tf.float32, name=""sample_weights"")\n\n        self._train_generator = self._generator_base(x, y, ""TrainGenerator"", self._sample_weights, self.n_class)\n        if x_test is not None and y_test is not None:\n            self._test_generator = self._generator_base(x_test, y_test, ""TestGenerator"", n_class=self.n_class)\n        else:\n            self._test_generator = None\n        self.n_random_train_subset = int(len(self._train_generator) * 0.1)\n        if self._test_generator is None:\n            self.n_random_test_subset = -1\n        else:\n            self.n_random_test_subset = len(self._test_generator)\n\n        self.n_dim = self._train_generator.shape[-1]\n        self.n_class = self._train_generator.n_class\n\n        batch_size = self.model_param_settings.setdefault(""batch_size"", 128)\n        self.model_param_settings[""batch_size""] = min(batch_size, len(self._train_generator))\n        n_iter = self.model_param_settings.setdefault(""n_iter"", -1)\n        if n_iter < 0:\n            self.model_param_settings[""n_iter""] = int(len(self._train_generator) / batch_size)\n\n    def init_all_settings(self):\n        self.init_model_param_settings()\n        self.init_model_structure_settings()\n\n    def init_model_param_settings(self):\n        loss = self.model_param_settings.get(""loss"", None)\n        if loss is None:\n            self._loss_name = ""correlation"" if self.n_class == 1 else ""cross_entropy""\n        else:\n            self._loss_name = loss\n        metric = self.model_param_settings.get(""metric"", None)\n        if metric is None:\n            if self.n_class == 1:\n                self._metric_name = ""correlation""\n            elif self.n_class == 2:\n                self._metric_name = ""auc""\n            else:\n                self._metric_name = ""multi_auc""\n        else:\n            self._metric_name = metric\n        self.n_epoch = self.model_param_settings.get(""n_epoch"", 32)\n        self.max_epoch = self.model_param_settings.get(""max_epoch"", 256)\n        self.max_epoch = max(self.max_epoch, self.n_epoch)\n\n        self.batch_size = self.model_param_settings[""batch_size""]\n        self.n_iter = self.model_param_settings[""n_iter""]\n\n        self._optimizer_name = self.model_param_settings.get(""optimizer"", ""Adam"")\n        self.lr = self.model_param_settings.get(""lr"", 1e-3)\n        self._optimizer = getattr(tf.train, ""{}Optimizer"".format(self._optimizer_name))(self.lr)\n\n    def init_model_structure_settings(self):\n        pass\n\n    # Core\n\n    def _fully_connected_linear(self, net, shape, appendix):\n        with tf.name_scope(""Linear{}"".format(appendix)):\n            w = init_w(shape, ""W{}"".format(appendix))\n            b = init_b([shape[1]], ""b{}"".format(appendix))\n            self._ws.append(w)\n            self._bs.append(b)\n            return tf.add(tf.matmul(net, w), b, name=""Linear{}_Output"".format(appendix))\n\n    def _build_model(self, net=None):\n        pass\n\n    def _gen_batch(self, generator, n_batch, gen_random_subset=False, one_hot=False):\n        if gen_random_subset:\n            data, weights = generator.gen_random_subset(n_batch)\n        else:\n            data, weights = generator.gen_batch(n_batch)\n        x, y = data[..., :-1], data[..., -1]\n        if not one_hot:\n            return x, y, weights\n        if self.n_class == 1:\n            y = y.reshape([-1, 1])\n        else:\n            y = Toolbox.get_one_hot(y, self.n_class)\n        return x, y, weights\n\n    def _get_feed_dict(self, x, y=None, weights=None, is_training=False):\n        feed_dict = {self._tfx: x, self._is_training: is_training}\n        if y is not None:\n            feed_dict[self._tfy] = y\n        if self._tf_sample_weights is not None:\n            if weights is None:\n                weights = np.ones(len(x))\n            feed_dict[self._tf_sample_weights] = weights\n        return feed_dict\n\n    def _define_loss_and_train_step(self):\n        self._loss = getattr(Losses, self._loss_name)(self._tfy, self._output, False, self._tf_sample_weights)\n        with tf.control_dependencies(tf.get_collection(tf.GraphKeys.UPDATE_OPS)):\n            self._train_step = self._optimizer.minimize(self._loss)\n\n    def _initialize_session(self):\n        self._sess = tf.Session(graph=self._graph, config=self._sess_config)\n\n    def _initialize_variables(self):\n        self._sess.run(tf.global_variables_initializer())\n\n    def _snapshot(self, i_epoch, i_iter, snapshot_cursor):\n        x_train, y_train, sw_train = self._gen_batch(\n            self._train_generator, self.n_random_train_subset,\n            gen_random_subset=True\n        )\n        if self._test_generator is not None:\n            x_test, y_test, sw_test = self._gen_batch(\n                self._test_generator, self.n_random_test_subset,\n                gen_random_subset=True\n            )\n            if self.n_class == 1:\n                y_test = y_test.reshape([-1, 1])\n            else:\n                y_test = Toolbox.get_one_hot(y_test, self.n_class)\n        else:\n            x_test = y_test = sw_test = None\n        y_train_pred = self._predict(x_train)\n        if x_test is not None:\n            tensor = self._output if self.n_class == 1 else self._prob_output\n            y_test_pred, test_snapshot_loss = self._calculate(\n                x_test, y_test, sw_test,\n                [tensor, self._loss], is_training=False\n            )\n            y_test_pred, test_snapshot_loss = y_test_pred[0], test_snapshot_loss[0]\n        else:\n            y_test_pred = test_snapshot_loss = None\n        train_metric = self.metric(y_train, y_train_pred)\n        if y_test is not None and y_test_pred is not None:\n            test_metric = self.metric(y_test, y_test_pred)\n            if i_epoch >= 0 and i_iter >= 0 and snapshot_cursor >= 0:\n                self.log[""test_snapshot_loss""].append(test_snapshot_loss)\n                self.log[""test_{}"".format(self._metric_name)].append(test_metric)\n                self.log[""train_{}"".format(self._metric_name)].append(train_metric)\n        else:\n            test_metric = None\n        msg = (\n            ""Epoch {:6}   Iter {:8}   Snapshot {:6} ({})  -  ""\n            ""Train : {:8.6f}   Test : {}"".format(\n                i_epoch, i_iter, snapshot_cursor, self._metric_name, train_metric,\n                ""None"" if test_metric is None else ""{:8.6f}"".format(test_metric)\n            )\n        )\n        logger = self.get_logger(""_snapshot"", ""general.log"")\n        self.log_msg(msg, logger=logger)\n        return train_metric, test_metric\n\n    def _calculate(self, x, y=None, weights=None, tensor=None, n_elem=1e7, is_training=False):\n        n_batch = int(n_elem / x.shape[1])\n        n_repeat = int(len(x) / n_batch)\n        if n_repeat * n_batch < len(x):\n            n_repeat += 1\n        cursors = [0]\n        if tensor is None:\n            target = self._prob_output\n        elif isinstance(tensor, list):\n            target = []\n            for t in tensor:\n                if isinstance(t, str):\n                    t = getattr(self, t)\n                if isinstance(t, list):\n                    target += t\n                    cursors.append(len(t))\n                else:\n                    target.append(t)\n                    cursors.append(cursors[-1] + 1)\n        else:\n            target = getattr(self, tensor) if isinstance(tensor, str) else tensor\n        results = [self._sess.run(\n            target, self._get_feed_dict(\n                x[i * n_batch:(i + 1) * n_batch],\n                None if y is None else y[i * n_batch:(i + 1) * n_batch],\n                None if weights is None else weights[i * n_batch:(i + 1) * n_batch],\n                is_training=is_training\n            )\n        ) for i in range(n_repeat)]\n        if not isinstance(target, list):\n            if len(results) == 1:\n                return results[0]\n            return np.vstack(results)\n        if n_repeat > 1:\n            results = [\n                np.vstack([result[i] for result in results]) if target[i].shape.ndims else\n                np.mean([result[i] for result in results]) for i in range(len(target))\n            ]\n        else:\n            results = results[0]\n        if len(cursors) == 1:\n            return results\n        return [results[cursor:cursors[i + 1]] for i, cursor in enumerate(cursors[:-1])]\n\n    def _predict(self, x):\n        tensor = self._output if self.n_class == 1 else self._prob_output\n        output = self._calculate(x, tensor=tensor, is_training=False)\n        if self.n_class == 1:\n            return output.ravel()\n        return output\n\n    def _evaluate(self, x=None, y=None, x_cv=None, y_cv=None, x_test=None, y_test=None, metric=None):\n        if isinstance(metric, str):\n            metric_name, metric = metric, getattr(Metrics, metric)\n        else:\n            metric_name, metric = self._metric_name, self.metric\n        pred = self._predict(x) if x is not None else None\n        cv_pred = self._predict(x_cv) if x_cv is not None else None\n        test_pred = self._predict(x_test) if x_test is not None else None\n        train_metric = None if y is None else metric(y, pred)\n        cv_metric = None if y_cv is None else metric(y_cv, cv_pred)\n        test_metric = None if y_test is None else metric(y_test, test_pred)\n        self._print_metrics(metric_name, train_metric, cv_metric, test_metric)\n        return train_metric, cv_metric, test_metric\n\n    @staticmethod\n    def _print_metrics(metric_name, train_metric=None, cv_metric=None, test_metric=None, only_return=False):\n        msg = ""{}  -  Train : {}   CV : {}   Test : {}"".format(\n            metric_name,\n            ""None"" if train_metric is None else ""{:10.8f}"".format(train_metric),\n            ""None"" if cv_metric is None else ""{:10.8f}"".format(cv_metric),\n            ""None"" if test_metric is None else ""{:10.8f}"".format(test_metric)\n        )\n        return msg if only_return else print(msg)\n\n    def _define_input_and_placeholder(self):\n        self._is_training = tf.placeholder(tf.bool, name=""is_training"")\n        self._tfx = tf.placeholder(tf.float32, [None, self.n_dim], name=""X"")\n        self._tfy = tf.placeholder(tf.float32, [None, self.n_class], name=""Y"")\n\n    def _define_py_collections(self):\n        self.py_collections = [\n            ""_name"", ""n_class"",\n            ""model_param_settings"", ""model_structure_settings""\n        ]\n\n    def _define_tf_collections(self):\n        self.tf_collections = [\n            ""_tfx"", ""_tfy"", ""_output"", ""_prob_output"",\n            ""_loss"", ""_train_step"", ""_is_training""\n        ]\n\n    # Save & Load\n\n    def add_tf_collections(self):\n        for tensor in self.tf_collections:\n            target = getattr(self, tensor)\n            if target is not None:\n                tf.add_to_collection(tensor, target)\n\n    def clear_tf_collections(self):\n        for key in self.tf_collections:\n            tf.get_collection_ref(key).clear()\n\n    def save_collections(self, folder):\n        with open(os.path.join(folder, ""py.core""), ""wb"") as file:\n            param_dict = {name: getattr(self, name) for name in self.py_collections}\n            pickle.dump(param_dict, file)\n        self.add_tf_collections()\n\n    def restore_collections(self, folder):\n        with open(os.path.join(folder, ""py.core""), ""rb"") as file:\n            param_dict = pickle.load(file)\n            for name, value in param_dict.items():\n                setattr(self, name, value)\n        for tensor in self.tf_collections:\n            target = tf.get_collection(tensor)\n            if target is None:\n                continue\n            assert len(target) == 1, ""{} available \'{}\' found"".format(len(target), tensor)\n            setattr(self, tensor, target[0])\n        self.clear_tf_collections()\n\n    @staticmethod\n    def get_model_name(path, idx):\n        targets = os.listdir(path)\n        if idx is None:\n            idx = max([int(target) for target in targets if target.isnumeric()])\n        return os.path.join(path, ""{:06}"".format(idx))\n\n    def save(self, run_id=0, path=None):\n        if path is None:\n            path = self.model_saving_path\n        folder = os.path.join(path, ""{:06}"".format(run_id))\n        while os.path.isdir(folder):\n            run_id += 1\n            folder = os.path.join(path, ""{:06}"".format(run_id))\n        if not os.path.isdir(folder):\n            os.makedirs(folder)\n        logger = self.get_logger(""save"", ""general.log"")\n        self.log_msg(""Saving model"", logger=logger)\n        with self._graph.as_default():\n            saver = tf.train.Saver()\n            self.save_collections(folder)\n            saver.save(self._sess, os.path.join(folder, ""Model""))\n            self.log_msg(""Model saved to "" + folder, logger=logger)\n            return self\n\n    def load(self, run_id=None, clear_devices=False, path=None):\n        self._model_built = True\n        if path is None:\n            path = self.model_saving_path\n        folder = self.get_model_name(path, run_id)\n        path = os.path.join(folder, ""Model"")\n        logger = self.get_logger(""save"", ""general.log"")\n        self.log_msg(""Restoring model"", logger=logger)\n        with self._graph.as_default():\n            if self._sess is None:\n                self._initialize_session()\n            saver = tf.train.import_meta_graph(""{}.meta"".format(path), clear_devices)\n            saver.restore(self._sess, tf.train.latest_checkpoint(folder))\n            self.restore_collections(folder)\n            self.init_all_settings()\n            self.log_msg(""Model restored from "" + folder, logger=logger)\n            return self\n\n    def save_checkpoint(self, folder):\n        if not os.path.exists(folder):\n            os.makedirs(folder)\n        with self._graph.as_default():\n            tf.train.Saver().save(self._sess, os.path.join(folder, ""Model""))\n\n    def restore_checkpoint(self, folder):\n        with self._graph.as_default():\n            tf.train.Saver().restore(self._sess, os.path.join(folder, ""Model""))\n\n    # API\n\n    def print_settings(self, only_return=False):\n        pass\n\n    def fit(self, x, y, x_test=None, y_test=None, sample_weights=None, names=(""train"", ""test""),\n            timeit=True, time_limit=-1, snapshot_ratio=3, print_settings=True, verbose=1):\n        t = time.time()\n        self.init_from_data(x, y, x_test, y_test, sample_weights, names)\n        if not self._settings_initialized:\n            self.init_all_settings()\n            self._settings_initialized = True\n\n        if not self._model_built:\n            with self._graph.as_default():\n                self._initialize_session()\n                with tf.name_scope(""Input""):\n                    self._define_input_and_placeholder()\n                with tf.name_scope(""Model""):\n                    self._build_model()\n                    self._prob_output = tf.nn.softmax(self._output, name=""Prob_Output"")\n                with tf.name_scope(""LossAndTrainStep""):\n                    self._define_loss_and_train_step()\n                with tf.name_scope(""InitializeVariables""):\n                    self._initialize_variables()\n\n        i_epoch = i_iter = j = snapshot_cursor = 0\n        if snapshot_ratio == 0 or x_test is None or y_test is None:\n            use_monitor = False\n            snapshot_step = self.n_iter\n        else:\n            use_monitor = True\n            snapshot_ratio = min(snapshot_ratio, self.n_iter)\n            snapshot_step = int(self.n_iter / snapshot_ratio)\n\n        logger = self.get_logger(""fit"", ""general.log"")\n        terminate = False\n        over_fitting_flag = 0\n        n_epoch = self.n_epoch\n        tmp_checkpoint_folder = os.path.join(self.model_saving_path, ""tmp"")\n        if time_limit > 0:\n            time_limit -= time.time() - t\n            if time_limit <= 0:\n                self.log_msg(\n                    ""Time limit exceeded before training process started"",\n                    level=logging.INFO, logger=logger\n                )\n                return self\n        monitor = TrainMonitor(Metrics.sign_dict[self._metric_name], snapshot_ratio)\n\n        if verbose >= 2:\n            prepare_tensorboard_verbose(self._sess)\n\n        if print_settings:\n            self.print_settings()\n\n        self.log[""iter_loss""] = []\n        self.log[""epoch_loss""] = []\n        self.log[""test_snapshot_loss""] = []\n        self.log[""train_{}"".format(self._metric_name)] = []\n        self.log[""test_{}"".format(self._metric_name)] = []\n        self._snapshot(0, 0, 0)\n\n        bar = ProgressBar(max_value=n_epoch, name=""Epoch"")\n        while i_epoch < n_epoch:\n            i_epoch += 1\n            epoch_loss = 0\n            for j in range(self.n_iter):\n                i_iter += 1\n                x_batch, y_batch, sw_batch = self._gen_batch(self._train_generator, self.batch_size, one_hot=True)\n                iter_loss = self._sess.run(\n                    [self._loss, self._train_step],\n                    self._get_feed_dict(x_batch, y_batch, sw_batch, is_training=True)\n                )[0]\n                self.log[""iter_loss""].append(iter_loss)\n                epoch_loss += iter_loss\n                if i_iter % snapshot_step == 0 and verbose >= 1:\n                    snapshot_cursor += 1\n                    train_metric, test_metric = self._snapshot(i_epoch, i_iter, snapshot_cursor)\n                    if use_monitor:\n                        check_rs = monitor.check(test_metric)\n                        over_fitting_flag = monitor.over_fitting_flag\n                        if check_rs[""terminate""]:\n                            n_epoch = i_epoch\n                            self.log_msg(""Early stopped at n_epoch={} due to \'{}\'"".format(\n                                n_epoch, check_rs[""info""]\n                            ), level=logging.INFO, logger=logger)\n                            terminate = True\n                            break\n                        if check_rs[""save_checkpoint""]:\n                            self.log_msg(check_rs[""info""], logger=logger)\n                            self.save_checkpoint(tmp_checkpoint_folder)\n                if 0 < time_limit <= time.time() - t:\n                    self.log_msg(\n                        ""Early stopped at n_epoch={} due to \'Time limit exceeded\'"".format(i_epoch),\n                        level=logging.INFO, logger=logger\n                    )\n                    terminate = True\n                    break\n            self.log[""epoch_loss""].append(epoch_loss / (j + 1))\n            if use_monitor:\n                if i_epoch == n_epoch and i_epoch < self.max_epoch and not monitor.info[""terminate""]:\n                    monitor.flat_flag = True\n                    monitor.punish_extension()\n                    n_epoch = min(n_epoch + monitor.extension, self.max_epoch)\n                    self.log_msg(""Extending n_epoch to {}"".format(n_epoch), logger=logger)\n                    bar.set_max(n_epoch)\n                if i_epoch == self.max_epoch:\n                    terminate = True\n                    if not monitor.info[""terminate""]:\n                        if not over_fitting_flag:\n                            self.log_msg(\n                                ""Model seems to be under-fitting but max_epoch reached. ""\n                                ""Increasing max_epoch may improve performance"",\n                                level=logging.INFO, logger=logger\n                            )\n                        else:\n                            self.log_msg(""max_epoch reached"", level=logging.INFO, logger=logger)\n            elif i_epoch == n_epoch:\n                terminate = True\n            if terminate:\n                bar.terminate()\n                if os.path.exists(tmp_checkpoint_folder):\n                    self.log_msg(""Rolling back to the best checkpoint"", logger=logger)\n                    self.restore_checkpoint(tmp_checkpoint_folder)\n                    shutil.rmtree(tmp_checkpoint_folder)\n                break\n            bar.update()\n        self._snapshot(-1, -1, -1)\n\n        if timeit:\n            self.log_msg(""Time Cost: {}"".format(time.time() - t), level=logging.INFO, logger=logger)\n\n        return self\n\n    def predict(self, x):\n        return self._predict(x)\n\n    def predict_classes(self, x):\n        if self.n_class == 1:\n            raise ValueError(""Predicting classes is not permitted in regression problem"")\n        return self._predict(x).argmax(1).astype(np.int32)\n\n    def evaluate(self, x, y, x_cv=None, y_cv=None, x_test=None, y_test=None, metric=None):\n        return self._evaluate(x, y, x_cv, y_cv, x_test, y_test, metric)\n\n    # Visualization\n\n    def draw_losses(self):\n        el, il = self.log[""epoch_loss""], self.log[""iter_loss""]\n        ee_base = np.arange(len(el))\n        ie_base = np.linspace(0, len(el) - 1, len(il))\n        plt.figure()\n        plt.plot(ie_base, il, label=""Iter loss"")\n        plt.plot(ee_base, el, linewidth=3, label=""Epoch loss"")\n        plt.legend()\n        plt.show()\n        return self\n\n    def scatter2d(self, x, y, padding=0.5, title=None):\n        axis, labels = np.asarray(x).T, np.asarray(y)\n\n        print(""="" * 30 + ""\\n"" + str(self))\n        x_min, x_max = np.min(axis[0]), np.max(axis[0])\n        y_min, y_max = np.min(axis[1]), np.max(axis[1])\n        x_padding = max(abs(x_min), abs(x_max)) * padding\n        y_padding = max(abs(y_min), abs(y_max)) * padding\n        x_min -= x_padding\n        x_max += x_padding\n        y_min -= y_padding\n        y_max += y_padding\n\n        if labels.ndim == 1:\n            plot_label_dict = {c: i for i, c in enumerate(set(labels))}\n            n_label = len(plot_label_dict)\n            labels = np.array([plot_label_dict[label] for label in labels])\n        else:\n            n_label = labels.shape[1]\n            labels = np.argmax(labels, axis=1)\n        colors = plt.cm.rainbow([i / n_label for i in range(n_label)])[labels]\n\n        if title is None:\n            title = self.model_saving_name\n\n        indices = [labels == i for i in range(np.max(labels) + 1)]\n        scatters = []\n        plt.figure()\n        plt.title(title)\n        for idx in indices:\n            scatters.append(plt.scatter(axis[0][idx], axis[1][idx], c=colors[idx]))\n        plt.legend(scatters, [""$c_{}$"".format(""{"" + str(i) + ""}"") for i in range(len(scatters))],\n                   ncol=math.ceil(math.sqrt(len(scatters))), fontsize=8)\n        plt.xlim(x_min, x_max)\n        plt.ylim(y_min, y_max)\n        plt.show()\n        return self\n\n    def scatter3d(self, x, y, padding=0.1, title=None):\n        axis, labels = np.asarray(x).T, np.asarray(y)\n\n        print(""="" * 30 + ""\\n"" + str(self))\n        x_min, x_max = np.min(axis[0]), np.max(axis[0])\n        y_min, y_max = np.min(axis[1]), np.max(axis[1])\n        z_min, z_max = np.min(axis[2]), np.max(axis[2])\n        x_padding = max(abs(x_min), abs(x_max)) * padding\n        y_padding = max(abs(y_min), abs(y_max)) * padding\n        z_padding = max(abs(z_min), abs(z_max)) * padding\n        x_min -= x_padding\n        x_max += x_padding\n        y_min -= y_padding\n        y_max += y_padding\n        z_min -= z_padding\n        z_max += z_padding\n\n        def transform_arr(arr):\n            if arr.ndim == 1:\n                dic = {c: i for i, c in enumerate(set(arr))}\n                n_dim = len(dic)\n                arr = np.array([dic[label] for label in arr])\n            else:\n                n_dim = arr.shape[1]\n                arr = np.argmax(arr, axis=1)\n            return arr, n_dim\n\n        if title is None:\n            title = self.model_saving_name\n\n        labels, n_label = transform_arr(labels)\n        colors = plt.cm.rainbow([i / n_label for i in range(n_label)])[labels]\n        indices = [labels == i for i in range(n_label)]\n        scatters = []\n        fig = plt.figure()\n        plt.title(title)\n        ax = fig.add_subplot(111, projection=\'3d\')\n        for _index in indices:\n            scatters.append(ax.scatter(axis[0][_index], axis[1][_index], axis[2][_index], c=colors[_index]))\n        ax.legend(scatters, [""$c_{}$"".format(""{"" + str(i) + ""}"") for i in range(len(scatters))],\n                  ncol=math.ceil(math.sqrt(len(scatters))), fontsize=8)\n        plt.show()\n        return self\n\n    def visualize2d(self, x, y, padding=0.1, dense=200, title=None,\n                    scatter=True, show_org=False, draw_background=True, emphasize=None, extra=None):\n        axis, labels = np.asarray(x).T, np.asarray(y)\n\n        print(""="" * 30 + ""\\n"" + str(self))\n        nx, ny, padding = dense, dense, padding\n        x_min, x_max = np.min(axis[0]), np.max(axis[0])\n        y_min, y_max = np.min(axis[1]), np.max(axis[1])\n        x_padding = max(abs(x_min), abs(x_max)) * padding\n        y_padding = max(abs(y_min), abs(y_max)) * padding\n        x_min -= x_padding\n        x_max += x_padding\n        y_min -= y_padding\n        y_max += y_padding\n\n        def get_base(_nx, _ny):\n            _xf = np.linspace(x_min, x_max, _nx)\n            _yf = np.linspace(y_min, y_max, _ny)\n            n_xf, n_yf = np.meshgrid(_xf, _yf)\n            return _xf, _yf, np.c_[n_xf.ravel(), n_yf.ravel()]\n\n        xf, yf, base_matrix = get_base(nx, ny)\n\n        t = time.time()\n        z = self.predict_classes(base_matrix).reshape((nx, ny))\n        print(""Decision Time: {:8.6f} s"".format(time.time() - t))\n\n        print(""Drawing figures..."")\n        xy_xf, xy_yf = np.meshgrid(xf, yf, sparse=True)\n        if labels.ndim == 1:\n            plot_label_dict = {c: i for i, c in enumerate(set(labels))}\n            n_label = len(plot_label_dict)\n            labels = np.array([plot_label_dict[label] for label in labels])\n        else:\n            n_label = labels.shape[1]\n            labels = np.argmax(labels, axis=1)\n        colors = plt.cm.rainbow([i / n_label for i in range(n_label)])[labels]\n\n        if title is None:\n            title = self.model_saving_name\n\n        if show_org:\n            plt.figure()\n            plt.scatter(axis[0], axis[1], c=colors)\n            plt.xlim(x_min, x_max)\n            plt.ylim(y_min, y_max)\n            plt.show()\n\n        plt.figure()\n        plt.title(title)\n        if draw_background:\n            plt.pcolormesh(xy_xf, xy_yf, z, cmap=plt.cm.Pastel1)\n        else:\n            plt.contour(xf, yf, z, c=\'k-\', levels=[0])\n        if scatter:\n            plt.scatter(axis[0], axis[1], c=colors)\n        if emphasize is not None:\n            indices = np.array([False] * len(axis[0]))\n            indices[np.asarray(emphasize)] = True\n            plt.scatter(axis[0][indices], axis[1][indices], s=80,\n                        facecolors=""None"", zorder=10)\n        if extra is not None:\n            plt.scatter(*np.asarray(extra).T, s=80, zorder=25, facecolors=""red"")\n        plt.xlim(x_min, x_max)\n        plt.ylim(y_min, y_max)\n        plt.show()\n        print(""Done."")\n        return self\n\n    def visualize3d(self, x, y, padding=0.1, dense=100, title=None,\n                    show_org=False, draw_background=True, emphasize=None, extra=None):\n        if False:\n            print(Axes3D.add_artist)\n        axis, labels = np.asarray(x).T, np.asarray(y)\n\n        print(""="" * 30 + ""\\n"" + str(self))\n\n        nx, ny, nz, padding = dense, dense, dense, padding\n        x_min, x_max = np.min(axis[0]), np.max(axis[0])\n        y_min, y_max = np.min(axis[1]), np.max(axis[1])\n        z_min, z_max = np.min(axis[2]), np.max(axis[2])\n        x_padding = max(abs(x_min), abs(x_max)) * padding\n        y_padding = max(abs(y_min), abs(y_max)) * padding\n        z_padding = max(abs(z_min), abs(z_max)) * padding\n        x_min -= x_padding\n        x_max += x_padding\n        y_min -= y_padding\n        y_max += y_padding\n        z_min -= z_padding\n        z_max += z_padding\n\n        def get_base(_nx, _ny, _nz):\n            _xf = np.linspace(x_min, x_max, _nx)\n            _yf = np.linspace(y_min, y_max, _ny)\n            _zf = np.linspace(z_min, z_max, _nz)\n            n_xf, n_yf, n_zf = np.meshgrid(_xf, _yf, _zf)\n            return _xf, _yf, _zf, np.c_[n_xf.ravel(), n_yf.ravel(), n_zf.ravel()]\n\n        xf, yf, zf, base_matrix = get_base(nx, ny, nz)\n\n        t = time.time()\n        z_xyz = self.predict_classes(base_matrix).reshape((nx, ny, nz))\n        p_classes = self.predict_classes(x).astype(np.int8)\n        _, _, _, base_matrix = get_base(10, 10, 10)\n        z_classes = self.predict_classes(base_matrix).astype(np.int8)\n        print(""Decision Time: {:8.6f} s"".format(time.time() - t))\n\n        print(""Drawing figures..."")\n        z_xy = np.average(z_xyz, axis=2)\n        z_yz = np.average(z_xyz, axis=1)\n        z_xz = np.average(z_xyz, axis=0)\n\n        xy_xf, xy_yf = np.meshgrid(xf, yf, sparse=True)\n        yz_xf, yz_yf = np.meshgrid(yf, zf, sparse=True)\n        xz_xf, xz_yf = np.meshgrid(xf, zf, sparse=True)\n\n        def transform_arr(arr):\n            if arr.ndim == 1:\n                dic = {c: i for i, c in enumerate(set(arr))}\n                n_dim = len(dic)\n                arr = np.array([dic[label] for label in arr])\n            else:\n                n_dim = arr.shape[1]\n                arr = np.argmax(arr, axis=1)\n            return arr, n_dim\n\n        labels, n_label = transform_arr(labels)\n        p_classes, _ = transform_arr(p_classes)\n        z_classes, _ = transform_arr(z_classes)\n        colors = plt.cm.rainbow([i / n_label for i in range(n_label)])\n        if extra is not None:\n            ex0, ex1, ex2 = np.asarray(extra).T\n        else:\n            ex0 = ex1 = ex2 = None\n\n        if title is None:\n            title = self.model_saving_name\n\n        if show_org:\n            fig = plt.figure()\n            ax = fig.add_subplot(111, projection=\'3d\')\n            ax.scatter(axis[0], axis[1], axis[2], c=colors[labels])\n            plt.show()\n\n        fig = plt.figure(figsize=(16, 4), dpi=100)\n        plt.title(title)\n        ax1 = fig.add_subplot(131, projection=\'3d\')\n        ax2 = fig.add_subplot(132, projection=\'3d\')\n        ax3 = fig.add_subplot(133, projection=\'3d\')\n\n        ax1.set_title(""Org"")\n        ax2.set_title(""Pred"")\n        ax3.set_title(""Boundary"")\n\n        ax1.scatter(axis[0], axis[1], axis[2], c=colors[labels])\n        ax2.scatter(axis[0], axis[1], axis[2], c=colors[p_classes], s=15)\n        if extra is not None:\n            ax2.scatter(ex0, ex1, ex2, s=80, zorder=25, facecolors=""red"")\n        xyz_xf, xyz_yf, xyz_zf = base_matrix[..., 0], base_matrix[..., 1], base_matrix[..., 2]\n        ax3.scatter(xyz_xf, xyz_yf, xyz_zf, c=colors[z_classes], s=15)\n\n        plt.show()\n        plt.close()\n\n        fig = plt.figure(figsize=(16, 4), dpi=100)\n        ax1 = fig.add_subplot(131)\n        ax2 = fig.add_subplot(132)\n        ax3 = fig.add_subplot(133)\n\n        def _draw(_ax, _x, _xf, _y, _yf, _z):\n            if draw_background:\n                _ax.pcolormesh(_x, _y, _z > 0, cmap=plt.cm.Pastel1)\n            else:\n                _ax.contour(_xf, _yf, _z, c=\'k-\', levels=[0])\n\n        def _emphasize(_ax, axis0, axis1, _c):\n            _ax.scatter(axis0, axis1, c=_c)\n            if emphasize is not None:\n                indices = np.array([False] * len(axis[0]))\n                indices[np.asarray(emphasize)] = True\n                _ax.scatter(axis0[indices], axis1[indices], s=80,\n                            facecolors=""None"", zorder=10)\n\n        def _extra(_ax, axis0, axis1, _c, _ex0, _ex1):\n            _emphasize(_ax, axis0, axis1, _c)\n            if extra is not None:\n                _ax.scatter(_ex0, _ex1, s=80, zorder=25, facecolors=""red"")\n\n        colors = colors[labels]\n\n        ax1.set_title(""xy figure"")\n        _draw(ax1, xy_xf, xf, xy_yf, yf, z_xy)\n        _extra(ax1, axis[0], axis[1], colors, ex0, ex1)\n\n        ax2.set_title(""yz figure"")\n        _draw(ax2, yz_xf, yf, yz_yf, zf, z_yz)\n        _extra(ax2, axis[1], axis[2], colors, ex1, ex2)\n\n        ax3.set_title(""xz figure"")\n        _draw(ax3, xz_xf, xf, xz_yf, zf, z_xz)\n        _extra(ax3, axis[0], axis[2], colors, ex0, ex2)\n\n        plt.show()\n        print(""Done."")\n        return self\n\n\nclass AutoBase(LoggingMixin, DataCacheMixin):\n    # noinspection PyUnusedLocal\n    def __init__(self, name=None, data_info=None, pre_process_settings=None, nan_handler_settings=None,\n                 *args, **kwargs):\n        if name is None:\n            raise ValueError(""name should be provided when using AutoBase"")\n        self._name = name\n\n        self.whether_redundant = None\n        self.feature_sets = self.sparsity = self.class_prior = None\n        self.n_features = self.all_num_idx = self.transform_dicts = None\n\n        self.py_collections = []\n\n        if data_info is None:\n            data_info = {}\n        else:\n            assert_msg = ""data_info should be a dictionary""\n            assert isinstance(data_info, dict), assert_msg\n        self.data_info = data_info\n        self._data_info_initialized = False\n        self.numerical_idx = self.categorical_columns = None\n\n        if pre_process_settings is None:\n            pre_process_settings = {}\n        else:\n            assert_msg = ""pre_process_settings should be a dictionary""\n            assert isinstance(pre_process_settings, dict), assert_msg\n        self.pre_process_settings = pre_process_settings\n        self._pre_processors = None\n        self.pre_process_method = self.scale_method = self.reuse_mean_and_std = None\n\n        if nan_handler_settings is None:\n            nan_handler_settings = {}\n        else:\n            assert_msg = ""nan_handler_settings should be a dictionary""\n            assert isinstance(nan_handler_settings, dict), assert_msg\n        self.nan_handler_settings = nan_handler_settings\n        self._nan_handler = None\n        self.nan_handler_method = self.reuse_nan_handler_values = None\n\n        self.init_pre_process_settings()\n        self.init_nan_handler_settings()\n\n    @property\n    def label2num_dict(self):\n        return None if not self.transform_dicts[-1] else self.transform_dicts[-1]\n\n    @property\n    def num2label_dict(self):\n        label2num_dict = self.label2num_dict\n        if label2num_dict is None:\n            return\n        num_label_list = sorted([(i, c) for c, i in label2num_dict.items()])\n        return np.array([label for _, label in num_label_list])\n\n    @property\n    def valid_numerical_idx(self):\n        return np.array([\n            is_numerical for is_numerical in self.numerical_idx\n            if is_numerical is not None\n        ])\n\n    @property\n    def valid_n_features(self):\n        return np.array([\n            n_feature for i, n_feature in enumerate(self.n_features)\n            if self.numerical_idx[i] is not None\n        ])\n\n    def init_data_info(self):\n        if self._data_info_initialized:\n            return\n        self._data_info_initialized = True\n        self.numerical_idx = self.data_info.get(""numerical_idx"", None)\n        self.categorical_columns = self.data_info.get(""categorical_columns"", None)\n        self.feature_sets = self.data_info.get(""feature_sets"", None)\n        self.sparsity = self.data_info.get(""sparsity"", None)\n        self.class_prior = self.data_info.get(""class_prior"", None)\n        if self.feature_sets is not None and self.numerical_idx is not None:\n            self.n_features = [len(feature_set) for feature_set in self.feature_sets]\n            self._gen_categorical_columns()\n        self.data_info.setdefault(""file_type"", ""txt"")\n        self.data_info.setdefault(""shuffle"", True)\n        self.data_info.setdefault(""test_rate"", 0.1)\n        self.data_info.setdefault(""stage"", 3)\n\n    def init_pre_process_settings(self):\n        self.pre_process_method = self.pre_process_settings.setdefault(""pre_process_method"", ""normalize"")\n        self.scale_method = self.pre_process_settings.setdefault(""scale_method"", ""truncate"")\n        self.reuse_mean_and_std = self.pre_process_settings.setdefault(""reuse_mean_and_std"", False)\n        if self.pre_process_method is not None and self._pre_processors is None:\n            self._pre_processors = {}\n\n    def init_nan_handler_settings(self):\n        self.nan_handler_method = self.nan_handler_settings.setdefault(""nan_handler_method"", ""median"")\n        self.reuse_nan_handler_values = self.nan_handler_settings.setdefault(""reuse_nan_handler_values"", True)\n\n    def _auto_init_from_data(self, x, y, x_test, y_test, names):\n        stage = self.data_info[""stage""]\n        shuffle = self.data_info[""shuffle""]\n        file_type = self.data_info[""file_type""]\n        test_rate = self.data_info[""test_rate""]\n        args = (self.numerical_idx, file_type, names, shuffle, test_rate, stage)\n        if x is None or y is None:\n            x, y, x_test, y_test = self._load_data(None, *args)\n        else:\n            data = np.hstack([x, y.reshape([-1, 1])])\n            if x_test is not None and y_test is not None:\n                data = (data, np.hstack([x_test, y_test.reshape([-1, 1])]))\n            x, y, x_test, y_test = self._load_data(data, *args)\n        self._handle_unbalance(y)\n        self._handle_sparsity()\n        return x, y, x_test, y_test\n\n    def _handle_unbalance(self, y):\n        if self.n_class == 1:\n            return\n        class_ratio = self.class_prior.min() / self.class_prior.max()\n        logger = self.get_logger(""_handle_unbalance"", ""general.log"")\n        if class_ratio < 0.1:\n            warn_msg = ""Sample weights will be used since class_ratio < 0.1 ({:8.6f})"".format(class_ratio)\n            self.log_msg(warn_msg, logger=logger)\n            if self._sample_weights is None:\n                self.log_msg(\n                    ""Sample weights are not provided, they\'ll be generated automatically"",\n                    logger=logger\n                )\n                self._sample_weights = np.ones(len(y)) / self.class_prior[y.astype(np.int)]\n                self._sample_weights /= self._sample_weights.sum()\n                self._sample_weights *= len(y)\n\n    def _handle_sparsity(self):\n        if self.sparsity >= 0.75:\n            warn_msg = ""Dropout will be disabled since data sparsity >= 0.75 ({:8.6f})"".format(self.sparsity)\n            self.log_msg(warn_msg, logger=self.get_logger(""_handle_sparsity"", ""general.log""))\n            self.dropout_keep_prob = 1.\n\n    def _gen_categorical_columns(self):\n        self.categorical_columns = [\n            (i, value) for i, value in enumerate(self.valid_n_features)\n            if not self.valid_numerical_idx[i] and self.valid_numerical_idx[i] is not None\n        ]\n        if not self.valid_numerical_idx[-1]:\n            self.categorical_columns.pop()\n\n    def _transform_data(self, data, name, train_name=""train"",\n                        include_label=False, refresh_redundant_info=False, stage=3):\n        logger = self.get_logger(""_transform_data"", ""general.log"")\n        self.log_msg(""Transforming {0}data{2} at stage {1}"".format(\n            ""{} "".format(name), stage,\n            """" if name == train_name or not self.reuse_mean_and_std else\n            "" with {} data"".format(train_name),\n        ), logger=logger)\n        is_ndarray = isinstance(data, np.ndarray)\n        if refresh_redundant_info or self.whether_redundant is None:\n            self.whether_redundant = np.array([\n                True if local_dict is None else False\n                for local_dict in self.transform_dicts\n            ])\n        targets = [\n            (i, local_dict) for i, (idx, local_dict) in enumerate(\n                zip(self.numerical_idx, self.transform_dicts)\n            ) if not idx and local_dict and not self.whether_redundant[i]\n        ]\n        if targets and targets[-1][0] == len(self.numerical_idx) - 1 and not include_label:\n            targets = targets[:-1]\n        if stage == 1 or stage == 3:\n            # Transform data & Handle redundant\n            n_redundant = np.sum(self.whether_redundant)\n            if n_redundant == 0:\n                whether_redundant = None\n            else:\n                whether_redundant = self.whether_redundant\n                if not include_label:\n                    whether_redundant = whether_redundant[:-1]\n                if refresh_redundant_info:\n                    warn_msg = ""{} redundant: {}{}"".format(\n                        ""These {} columns are"".format(n_redundant) if n_redundant > 1 else ""One column is"",\n                        [i for i, redundant in enumerate(whether_redundant) if redundant],\n                        "", {} will be removed"".format(""it"" if n_redundant == 1 else ""they"")\n                    )\n                    self.log_msg(warn_msg, logger=logger)\n            valid_indices = [\n                i for i, redundant in enumerate(self.whether_redundant)\n                if not redundant\n            ]\n            if not include_label:\n                valid_indices = valid_indices[:-1]\n            for i, line in enumerate(data):\n                for j, local_dict in targets:\n                    elem = line[j]\n                    if isinstance(elem, str):\n                        line[j] = local_dict.get(elem, local_dict.get(""nan"", len(local_dict)))\n                    elif math.isnan(elem):\n                        line[j] = local_dict[""nan""]\n                    else:\n                        line[j] = local_dict.get(elem, local_dict.get(""nan"", len(local_dict)))\n                if not is_ndarray and whether_redundant is not None:\n                    data[i] = [line[j] for j in valid_indices]\n            if is_ndarray and whether_redundant is not None:\n                data = data[..., valid_indices].astype(np.float32)\n            else:\n                data = np.array(data, dtype=np.float32)\n        if stage == 2 or stage == 3:\n            data = np.asarray(data, dtype=np.float32)\n            # Handle nan\n            if self._nan_handler is None:\n                self._nan_handler = NanHandler(\n                    method=self.nan_handler_method,\n                    reuse_values=self.reuse_nan_handler_values\n                )\n            data = self._nan_handler.transform(data, self.valid_numerical_idx[:-1])\n            # Pre-process data\n            if self._pre_processors is not None:\n                pre_processor_name = train_name if self.reuse_mean_and_std else name\n                pre_processor = self._pre_processors.setdefault(\n                    pre_processor_name, PreProcessor(\n                        self.pre_process_method, self.scale_method\n                    )\n                )\n                if not include_label:\n                    data = pre_processor.transform(data, self.valid_numerical_idx[:-1])\n                else:\n                    data[..., :-1] = pre_processor.transform(data[..., :-1], self.valid_numerical_idx[:-1])\n        return data\n\n    def _get_label_dict(self):\n        labels = self.feature_sets[-1]\n        sorted_labels = sorted(labels)\n        if not all(Toolbox.is_number(str(label)) for label in labels):\n            return {key: i for i, key in enumerate(sorted_labels)}\n        if not sorted_labels:\n            return {}\n        numerical_labels = np.array(sorted_labels, np.float32)\n        if numerical_labels.max() - numerical_labels.min() != self.n_class - 1:\n            return {key: i for i, key in enumerate(sorted_labels)}\n        return {}\n\n    def _get_transform_dicts(self):\n        self.transform_dicts = [\n            None if is_numerical is None else\n            {key: i for i, key in enumerate(sorted(feature_set))}\n            if not is_numerical and (not all_num or not np.allclose(\n                np.sort(np.array(list(feature_set), np.float32).astype(np.int32)),\n                np.arange(0, len(feature_set))\n            )) else {} for is_numerical, feature_set, all_num in zip(\n                self.numerical_idx[:-1], self.feature_sets[:-1], self.all_num_idx[:-1]\n            )\n        ]\n        if self.n_class == 1:\n            self.transform_dicts.append({})\n        else:\n            self.transform_dicts.append(self._get_label_dict())\n\n    def _get_data_from_file(self, file_type, test_rate, target=None):\n        if file_type == ""txt"":\n            sep, include_header = "" "", False\n        elif file_type == ""csv"":\n            sep, include_header = "","", True\n        else:\n            raise NotImplementedError(""File type \'{}\' not recognized"".format(file_type))\n        logger = self.get_logger(""_get_data_from_file"", ""general.log"")\n        if target is None:\n            target = os.path.join(self.data_folder, self._name)\n        if not os.path.isdir(target):\n            with open(target + "".{}"".format(file_type), ""r"") as file:\n                data = Toolbox.get_data(file, sep, include_header, logger)\n        else:\n            with open(os.path.join(target, ""train.{}"".format(file_type)), ""r"") as file:\n                train_data = Toolbox.get_data(file, sep, include_header, logger)\n            test_rate = 0\n            test_file = os.path.join(target, ""test.{}"".format(file_type))\n            if not os.path.isfile(test_file):\n                data = train_data\n            else:\n                with open(test_file, ""r"") as file:\n                    test_data = Toolbox.get_data(file, sep, include_header, logger)\n                data = (train_data, test_data)\n        return data, test_rate\n\n    def _load_data(self, data=None, numerical_idx=None, file_type=""txt"", names=(""train"", ""test""),\n                   shuffle=True, test_rate=0.1, stage=3):\n        use_cached_data = False\n        train_data = test_data = None\n        logger = self.get_logger(""_load_data"", ""general.log"")\n        if data is None and stage >= 2 and os.path.isfile(self.train_data_file):\n            self.log_msg(""Restoring data"", logger=logger)\n            use_cached_data = True\n            train_data = np.load(self.train_data_file)\n            if not os.path.isfile(self.test_data_file):\n                test_data = None\n                data = train_data\n            else:\n                test_data = np.load(self.test_data_file)\n                data = (train_data, test_data)\n        if use_cached_data:\n            n_train = None\n        else:\n            if data is None:\n                is_ndarray = False\n                data, test_rate = self._get_data_from_file(file_type, test_rate)\n            else:\n                is_ndarray = True\n                if not isinstance(data, tuple):\n                    test_rate = 0\n                    data = np.asarray(data, dtype=np.float32)\n                else:\n                    data = tuple(\n                        arr if isinstance(arr, list) else\n                        np.asarray(arr, np.float32) for arr in data\n                    )\n            if isinstance(data, tuple):\n                if shuffle:\n                    np.random.shuffle(data[0]) if is_ndarray else random.shuffle(data[0])\n                n_train = len(data[0])\n                data = np.vstack(data) if is_ndarray else data[0] + data[1]\n            else:\n                if shuffle:\n                    np.random.shuffle(data) if is_ndarray else random.shuffle(data)\n                n_train = int(len(data) * (1 - test_rate)) if test_rate > 0 else -1\n\n        if not os.path.isdir(self.data_info_folder):\n            os.makedirs(self.data_info_folder)\n        if not os.path.isfile(self.data_info_file) or stage == 1:\n            self.log_msg(""Generating data info"", logger=logger)\n            if numerical_idx is not None:\n                self.numerical_idx = numerical_idx\n            elif self.numerical_idx is not None:\n                numerical_idx = self.numerical_idx\n            if not self.feature_sets or not self.n_features or not self.all_num_idx:\n                is_regression = self.data_info.pop(\n                    ""is_regression"",\n                    numerical_idx is not None and numerical_idx[-1]\n                )\n                self.feature_sets, self.n_features, self.all_num_idx, self.numerical_idx = (\n                    Toolbox.get_feature_info(data, numerical_idx, is_regression, logger)\n                )\n            self.n_class = 1 if self.numerical_idx[-1] else self.n_features[-1]\n            self._get_transform_dicts()\n            with open(self.data_info_file, ""wb"") as file:\n                pickle.dump([\n                    self.n_features, self.numerical_idx, self.transform_dicts\n                ], file)\n        elif stage == 3:\n            self.log_msg(""Restoring data info"", logger=logger)\n            with open(self.data_info_file, ""rb"") as file:\n                info = pickle.load(file)\n                self.n_features, self.numerical_idx, self.transform_dicts = info\n            self.n_class = 1 if self.numerical_idx[-1] else self.n_features[-1]\n\n        if not use_cached_data:\n            if n_train > 0:\n                train_data, test_data = data[:n_train], data[n_train:]\n            else:\n                train_data, test_data = data, None\n            train_name, test_name = names\n            train_data = self._transform_data(train_data, train_name, train_name, True, True, stage)\n            if test_data is not None:\n                test_data = self._transform_data(test_data, test_name, train_name, True, stage=stage)\n        self._gen_categorical_columns()\n        if not use_cached_data and stage == 3:\n            self.log_msg(""Caching data..."", logger=logger)\n            if not os.path.exists(self.data_cache_folder):\n                os.makedirs(self.data_cache_folder)\n            np.save(self.train_data_file, train_data)\n            if test_data is not None:\n                np.save(self.test_data_file, test_data)\n\n        x, y = train_data[..., :-1], train_data[..., -1]\n        if test_data is not None:\n            x_test, y_test = test_data[..., :-1], test_data[..., -1]\n        else:\n            x_test = y_test = None\n        self.sparsity = ((x == 0).sum() + np.isnan(x).sum()) / np.prod(x.shape)\n        _, class_counts = np.unique(y, return_counts=True)\n        self.class_prior = class_counts / class_counts.sum()\n\n        self.data_info[""numerical_idx""] = self.numerical_idx\n        self.data_info[""categorical_columns""] = self.categorical_columns\n\n        return x, y, x_test, y_test\n\n    def _pop_preprocessor(self, name):\n        if isinstance(self._pre_processors, dict) and name in self._pre_processors:\n            self._pre_processors.pop(name)\n\n    def get_transformed_data_from_file(self, file, file_type=""txt"", include_label=False):\n        x, _ = self._get_data_from_file(file_type, 0, file)\n        x = self._transform_data(x, ""new"", include_label=include_label)\n        self._pop_preprocessor(""new"")\n        return x\n\n    def get_labels_from_classes(self, classes):\n        num2label_dict = self.num2label_dict\n        if num2label_dict is None:\n            return classes\n        return num2label_dict[classes]\n\n    def predict_labels(self, x):\n        return self.get_labels_from_classes(self.predict_classes(x))\n\n    # Signatures\n\n    def fit(self, x=None, y=None, x_test=None, y_test=None, sample_weights=None, names=(""train"", ""test""),\n            timeit=True, time_limit=-1, snapshot_ratio=3, print_settings=True, verbose=1):\n        raise ValueError\n\n    def predict_classes(self, x):\n        raise ValueError\n\n    def predict_from_file(self, file, file_type=""txt"", include_label=False):\n        raise ValueError\n\n    def predict_classes_from_file(self, file, file_type=""txt"", include_label=False):\n        raise ValueError\n\n    def predict_labels_from_file(self, file, file_type=""txt"", include_label=False):\n        raise ValueError\n\n    def evaluate_from_file(self, file, file_type=""txt""):\n        raise ValueError\n\n\nclass AutoMeta(type):\n    def __new__(mcs, *args, **kwargs):\n        name_, bases, attr = args[:3]\n        auto_base, model = bases\n\n        def __init__(self, name=None, data_info=None, model_param_settings=None, model_structure_settings=None,\n                     pre_process_settings=None, nan_handler_settings=None):\n            auto_base.__init__(self, name, data_info, pre_process_settings, nan_handler_settings)\n            if model.signature != ""Advanced"":\n                model.__init__(self, name, model_param_settings, model_structure_settings)\n            else:\n                model.__init__(self, name, data_info, model_param_settings, model_structure_settings)\n\n        def _define_py_collections(self):\n            model._define_py_collections(self)\n            self.py_collections += [\n                ""pre_process_settings"", ""nan_handler_settings"",\n                ""_pre_processors"", ""_nan_handler"", ""transform_dicts"",\n                ""numerical_idx"", ""categorical_columns"", ""transform_dicts""\n            ]\n\n        def init_data_info(self):\n            auto_base.init_data_info(self)\n\n        def init_from_data(self, x, y, x_test, y_test, sample_weights, names):\n            self.init_data_info()\n            x, y, x_test, y_test = self._auto_init_from_data(x, y, x_test, y_test, names)\n            model.init_from_data(self, x, y, x_test, y_test, sample_weights, names)\n\n        def fit(self, x=None, y=None, x_test=None, y_test=None, sample_weights=None, names=(""train"", ""test""),\n                timeit=True, time_limit=-1, snapshot_ratio=3, print_settings=True, verbose=1):\n            return model.fit(\n                self, x, y, x_test, y_test, sample_weights, names,\n                timeit, time_limit, snapshot_ratio, print_settings, verbose\n            )\n\n        def predict(self, x):\n            rs = self._predict(self._transform_data(x, ""new"", include_label=False))\n            self._pop_preprocessor(""new"")\n            return rs\n\n        def predict_classes(self, x):\n            if self.n_class == 1:\n                raise ValueError(""Predicting classes is not permitted in regression problem"")\n            return self.predict(x).argmax(1).astype(np.int32)\n\n        def predict_target_prob(self, x, target):\n            prob = self.predict(x)\n            label2num_dict = self.label2num_dict\n            if label2num_dict is not None:\n                target = label2num_dict[target]\n            return prob[..., target]\n\n        def predict_from_file(self, file, file_type=""txt"", include_label=False):\n            x = self.get_transformed_data_from_file(file, file_type, include_label)\n            if include_label:\n                x = x[..., :-1]\n            return self._predict(x)\n\n        def predict_classes_from_file(self, file, file_type=""txt"", include_label=False):\n            if self.numerical_idx[-1]:\n                raise ValueError(""Predicting classes is not permitted in regression problem"")\n            x = self.get_transformed_data_from_file(file, file_type, include_label)\n            if include_label:\n                x = x[..., :-1]\n            return self._predict(x).argmax(1).astype(np.int32)\n\n        def predict_labels_from_file(self, file, file_type=""txt"", include_label=False):\n            classes = self.predict_classes_from_file(file, file_type, include_label)\n            return self.get_labels_from_classes(classes)\n\n        def evaluate(self, x, y, x_cv=None, y_cv=None, x_test=None, y_test=None, metric=None):\n            x = self._transform_data(x, ""train"")\n            cv_name = ""cv"" if ""cv"" in self._pre_processors else ""tmp_cv""\n            test_name = ""test"" if ""test"" in self._pre_processors else ""tmp_test""\n            if x_cv is not None:\n                x_cv = self._transform_data(x_cv, cv_name)\n            if x_test is not None:\n                x_test = self._transform_data(x_test, test_name)\n            if cv_name == ""tmp_cv"":\n                self._pop_preprocessor(""tmp_cv"")\n            if test_name == ""tmp_test"":\n                self._pop_preprocessor(""tmp_test"")\n            return self._evaluate(x, y, x_cv, y_cv, x_test, y_test, metric)\n\n        def print_settings(self):\n            msg = model.print_settings(self, only_return=True)\n            if msg is None:\n                msg = """"\n            msg += ""\\nNanHandler       : {}"".format(""None"" if not self._nan_handler else """") + ""\\n""\n            if self._nan_handler:\n                msg += ""\\n"".join(""-> {:14}: {}"".format(k, v) for k, v in sorted(\n                    self.nan_handler_settings.items()\n                )) + ""\\n""\n            msg += ""-"" * 100 + ""\\n\\n""\n            msg += ""PreProcessor     : {}"".format(""None"" if not self._pre_processors else """") + ""\\n""\n            if self._pre_processors:\n                msg += ""\\n"".join(""-> {:14}: {}"".format(k, v) for k, v in sorted(\n                    self.pre_process_settings.items()\n                )) + ""\\n""\n            msg += ""-"" * 100\n            self.log_msg(""\\n"" + msg, logger=self.get_logger(""print_settings"", ""general.log""))\n\n        for key, value in locals().items():\n            if str(value).find(""function"") >= 0:\n                attr[key] = value\n\n        return type(name_, bases, attr)\n\n\nclass DistMixin(LoggingMixin, DataCacheMixin):\n    @property\n    def k_series_time_delta(self):\n        return time.time() - self._k_series_t\n\n    @property\n    def param_search_time_delta(self):\n        return time.time() - self._param_search_t\n\n    @property\n    def data_cache_folder_name(self):\n        folder = os.path.join(os.getcwd(), ""_Tmp"", ""_Cache"")\n        if not os.path.isdir(folder):\n            os.makedirs(folder)\n        return folder\n\n    @property\n    def k_series_logger(self):\n        name = ""{}_k_series"".format(self.name)\n        if name not in self.loggers:\n            self.get_logger(name, ""{}.log"".format(name))\n        return self.loggers[name]\n\n    @property\n    def param_search_logger(self):\n        name = ""{}_param_search"".format(self.name)\n        if name not in self.loggers:\n            self.get_logger(name, ""{}.log"".format(name))\n        return self.loggers[name]\n\n    # noinspection PyAttributeOutsideInit\n    def reset_graph(self, i):\n        del self._graph\n        self._sess = None\n        self._graph = tf.Graph()\n        self._search_cursor = i\n\n    def reset_all_variables(self):\n        with self._graph.as_default():\n            self._sess.run(tf.global_variables_initializer())\n\n    def _handle_param_search_time_limit(self, time_limit):\n        if self.param_search_time_limit is None:\n            time_limit -= self.k_series_time_delta\n        else:\n            time_limit = min(\n                time_limit,\n                self.param_search_time_limit - self.k_series_time_delta\n            )\n        self._k_series_t = time.time()\n        return time_limit\n\n    def _k_series_initialization(self, k, data, test_rate):\n        self._k_series_t = time.time()\n        self.data_info.setdefault(""test_rate"", test_rate)\n        self.init_data_info()\n        self._k_performances = []\n        self._k_performances_mean = self._k_performances_std = None\n        kwargs = {\n            ""numerical_idx"": self.numerical_idx,\n            ""shuffle"": self.data_info[""shuffle""],\n            ""file_type"": self.data_info[""file_type""]\n        }\n        x_1, y_1, x_test_1, y_test_1 = self._load_data(\n            data, test_rate=self.data_info[""test_rate""], stage=1, **kwargs)\n        if not self._searching_params:\n            train_1 = np.hstack([x_1, y_1.reshape([-1, 1])])\n            test_1 = np.hstack([x_test_1, y_test_1.reshape([-1, 1])])\n            np.save(self.train_data_file, train_1)\n            np.save(self.test_data_file, test_1)\n        self._load_data(\n            np.hstack([x_1, y_1.reshape([-1, 1])]),\n            names=(""train"", None), test_rate=0, stage=2, **kwargs\n        )\n        if x_test_1 is None or y_test_1 is None:\n            x_test_2 = y_test_2 = None\n        else:\n            x_test_2, y_test_2, *_ = self._load_data(\n                np.hstack([x_test_1, y_test_1.reshape([-1, 1])]),\n                names=(""test"", None), test_rate=0, stage=2, **kwargs\n            )\n        names = [(""train{}"".format(i), ""cv{}"".format(i)) for i in range(k)]\n        return x_1, y_1, x_test_2, y_test_2, names\n\n    def _k_series_evaluation(self, i, x_test, y_test, time_limit):\n        if i == -1:\n            if x_test is None or y_test is None:\n                valid_performances = [performance[:2] for performance in self._k_performances]\n            else:\n                valid_performances = self._k_performances\n            performances_mean = np.mean(valid_performances, axis=0)\n            performances_std = np.std(valid_performances, axis=0)\n            msg = ""  -  Mean   | {}\\n"".format(\n                self._print_metrics(self._metric_name, *performances_mean, only_return=True))\n            msg += ""  -   Std   | {}"".format(\n                self._print_metrics(self._metric_name, *performances_std, only_return=True))\n            if self._searching_params:\n                level = logging.DEBUG\n                logger = self.param_search_logger\n            else:\n                level = logging.INFO\n                logger = self.k_series_logger\n            self.log_block_msg(\n                ""Generating performance summary"", body=msg,\n                level=level, logger=logger\n            )\n            return performances_mean, performances_std\n        train_data = self._train_generator.get_all_data(return_weights=False)\n        cv_data = self._test_generator.get_all_data(return_weights=False)\n        x, y = train_data[..., :-1], train_data[..., -1]\n        x_cv, y_cv = cv_data[..., :-1], cv_data[..., -1]\n        msg = ""Performance of run {:2} | "".format(i + 1)\n        print(""  -  "" + msg, end="""")\n        self._k_performances.append(self._evaluate(x, y, x_cv, y_cv, x_test, y_test))\n        msg += self._print_metrics(self._metric_name, *self._k_performances[-1], only_return=True)\n        self.log_msg(\n            msg, logging.DEBUG,\n            self.param_search_logger if self._searching_params else self.k_series_logger\n        )\n        return self.k_series_time_delta >= time_limit > 0\n\n    def _k_series_completion(self, x_test, y_test, names, sample_weights_store):\n        performance_info = self._k_series_evaluation(-1, x_test, y_test, None)\n        self._k_performances_mean, self._k_performances_std = performance_info\n        self.data_info[""stage""] = 3\n        for name in names:\n            self._pop_preprocessor(name)\n        self._sample_weights = sample_weights_store\n\n    def _k_series_process(self, k, data, cv_rate, test_rate, sample_weights,\n                          msg, cv_method, kwargs):\n        x_1, y_1, x_test_2, y_test_2, names = self._k_series_initialization(k, data, test_rate)\n        time_limit = kwargs.pop(""time_limit"", -1)\n        logger = self.get_logger(""_k_series_process"", ""general.log"")\n        if 0 < time_limit <= self.k_series_time_delta:\n            self.log_msg(""Time limit exceeded before k_series started"", logger=logger)\n            return\n        time_limit = self._handle_param_search_time_limit(time_limit)\n        n_cv = int(cv_rate * len(x_1))\n        print_settings = True\n        if sample_weights is not None:\n            self._sample_weights = np.asarray(sample_weights, np.float32)\n        sample_weights_store = self._sample_weights\n        self.log_msg(msg, logger=logger)\n        all_idx = np.random.permutation(len(x_1))\n        for i in range(k):\n            if self._sess is not None:\n                self.reset_all_variables()\n            skip = False\n            while True:\n                rs = cv_method(x_1, y_1, n_cv, i, k, all_idx)\n                if rs[""success""]:\n                    x_train, y_train, x_cv, y_cv, train_idx = rs[""info""]\n                    break\n                if rs[""info""] == ""retry"":\n                    continue\n                x_train = y_train = x_cv = y_cv = train_idx = None\n                skip = True\n                break\n            if skip:\n                self.log_msg(\n                    ""{}th fold was skipped since labels in train set and cv set are not identical"".format(i + 1),\n                    level=logging.INFO, logger=logger\n                )\n                continue\n            if sample_weights is not None:\n                self._sample_weights = sample_weights_store[train_idx]\n            else:\n                self._sample_weights = None\n            kwargs[""print_settings""] = print_settings\n            kwargs[""names""] = names[i]\n            self.data_info[""stage""] = 2\n            self.fit(x_train, y_train, x_cv, y_cv, timeit=False, time_limit=time_limit, **kwargs)\n            if self._k_series_evaluation(i, x_test_2, y_test_2, time_limit):\n                break\n            print_settings = False\n        self._k_series_completion(x_test_2, y_test_2, names, sample_weights_store)\n        return self\n\n    def _cv_sanity_check(self, rs, handler, train_idx, x_train, y_train, x_cv, y_cv):\n        if self.n_class == 1:\n            rs[""info""] = (x_train, y_train, x_cv, y_cv, train_idx)\n        else:\n            y_train_unique, y_cv_unique = np.unique(y_train), np.unique(y_cv)\n            if len(y_train_unique) == len(y_cv_unique) and np.allclose(y_train_unique, y_cv_unique):\n                rs[""info""] = (x_train, y_train, x_cv, y_cv, train_idx)\n            else:\n                rs[""success""] = False\n                rs[""info""] = handler\n\n    def _k_fold_method(self, x_1, y_1, *args):\n        _, i, k, all_idx = args\n        rs = {""success"": True}\n        n_batch = int(len(x_1) / k)\n        cv_idx = all_idx[np.arange(i * n_batch, (i + 1) * n_batch)]\n        train_idx = all_idx[[\n            j for j in range(len(all_idx))\n            if j < i * n_batch or j >= (i + 1) * n_batch\n        ]]\n        x_cv, y_cv = x_1[cv_idx], y_1[cv_idx]\n        x_train, y_train = x_1[train_idx], y_1[train_idx]\n        self._cv_sanity_check(rs, ""skip"", train_idx, x_train, y_train, x_cv, y_cv)\n        return rs\n\n    def _k_random_method(self, x_1, y_1, *args):\n        n_cv, *_ = args\n        rs = {""success"": True}\n        all_idx = np.random.permutation(len(x_1))\n        cv_idx, train_idx = all_idx[:n_cv], all_idx[n_cv:]\n        x_cv, y_cv = x_1[cv_idx], y_1[cv_idx]\n        x_train, y_train = x_1[train_idx], y_1[train_idx]\n        self._cv_sanity_check(rs, ""retry"", train_idx, x_train, y_train, x_cv, y_cv)\n        return rs\n\n    def k_fold(self, k=10, data=None, test_rate=0., sample_weights=None, **kwargs):\n        return self._k_series_process(\n            k, data, -1, test_rate, sample_weights, cv_method=self._k_fold_method, kwargs=kwargs,\n            msg=""Training k-fold with k={} and test_rate={}"".format(k, test_rate)\n        )\n\n    def k_random(self, k=3, data=None, cv_rate=0.1, test_rate=0., sample_weights=None, **kwargs):\n        return self._k_series_process(\n            k, data, cv_rate, test_rate, sample_weights, cv_method=self._k_random_method, kwargs=kwargs,\n            msg=""Training k-random with k={}, cv_rate={} and test_rate={}"".format(k, cv_rate, test_rate)\n        )\n\n    def _log_param_msg(self, i, param):\n        msg = """"\n        for j, (key, setting) in enumerate(param.items()):\n            msg += ""\\n"".join([key, ""-"" * 100]) + ""\\n""\n            msg += ""\\n"".join([\n                ""  ->  {:32} : {}"".format(\n                    name, value if not isinstance(value, dict) else ""\\n{}"".format(\n                        ""\\n"".join([""      ->  {:28} : {}"".format(\n                            local_name, local_value\n                        ) for local_name, local_value in value.items()])\n                    )\n                ) for name, value in sorted(setting.items())\n            ])\n            if j != len(param) - 1:\n                msg += ""\\n"" + ""-"" * 100 + ""\\n""\n        if i >= 0:\n            title = ""Generating parameter setting {:3}"".format(i + 1)\n        else:\n            title = ""Generating best parameter setting""\n        self.log_block_msg(\n            title=title, body=msg,\n            level=logging.DEBUG, logger=self.param_search_logger\n        )\n\n    @staticmethod\n    def _get_score(mean, std, sign):\n        if sign > 0:\n            return mean - std\n        return mean + std\n\n    @staticmethod\n    def _extract_param_from_info(dtype, info):\n        if dtype == ""choice"":\n            return info[0][random.randint(0, len(info[0]) - 1)]\n        if len(info) == 2:\n            floor, ceiling = info\n            distribution = ""linear""\n        else:\n            floor, ceiling, distribution = info\n        if ceiling <= floor:\n            raise ValueError(""ceiling should be greater than floor"")\n        if dtype == ""int"":\n            return random.randint(floor, ceiling)\n        if dtype == ""float"":\n            linear_target = floor + random.random() * (ceiling - floor)\n            distribution_error_msg = ""distribution \'{}\' not supported in range_search"".format(distribution)\n            if distribution == ""linear"":\n                return linear_target\n            if distribution[:3] == ""log"":\n                sign, log = int(linear_target > 0), math.log(math.fabs(linear_target))\n                if distribution == ""log"":\n                    return sign * math.exp(log)\n                if distribution == ""log2"":\n                    return sign * 2 ** log\n                if distribution == ""log10"":\n                    return sign * 10 ** log\n                raise NotImplementedError(distribution_error_msg)\n            raise NotImplementedError(distribution_error_msg)\n        raise NotImplementedError(""dtype \'{}\' not supported in range_search"".format(dtype))\n\n    def _update_param(self, param):\n        self._model_built = False\n        self._settings_initialized = False\n        self.model_param_settings = deepcopy(self._settings_base[""model_param_settings""])\n        self.model_structure_settings = deepcopy(self._settings_base[""model_structure_settings""])\n        new_model_param_settings = param.get(""model_param_settings"", {})\n        new_model_structure_settings = param.get(""model_structure_settings"", {})\n        self.model_param_settings.update(new_model_param_settings)\n        self.model_structure_settings.update(new_model_structure_settings)\n        if not self.model_structure_settings.get(""use_pruner"", True):\n            self._pruner = None\n        if not self.model_structure_settings.get(""use_dndf"", True):\n            self._dndf = None\n        if not self.model_structure_settings.get(""use_dndf_pruner"", False):\n            self._dndf_pruner = None\n        if self._nan_handler is not None:\n            self._nan_handler.reset()\n        if self._pre_processors:\n            self._pre_processors = {}\n\n    def _select_param(self, params, search_with_test_set):\n        scores = []\n        sign = Metrics.sign_dict[self._metric_name]\n        assert len(self.mean_record) == len(self.std_record)\n        for mean, std in zip(self.mean_record, self.std_record):\n            if len(mean) == 2 or not search_with_test_set:\n                train_mean, cv_mean = mean\n                train_std, cv_std = std\n                weighted_mean = 0.05 * train_mean + 0.95 * cv_mean\n                weighted_std = 0.05 * train_std + 0.95 * cv_std\n            else:\n                train_mean, cv_mean, test_mean = mean\n                train_std, cv_std, test_std = std\n                weighted_mean = 0.05 * train_mean + 0.1 * cv_mean + 0.85 * test_mean\n                weighted_std = 0.05 * train_std + 0.1 * cv_std + 0.85 * test_std\n            scores.append(self._get_score(weighted_mean, weighted_std, sign))\n        scores = np.array(scores, np.float32)\n        scores[np.isnan(scores)] = -np.inf\n        best_idx = np.argmax(scores)\n        return best_idx, params[best_idx]\n\n    def _prepare_param_search_data(self, data, test_rate):\n        file_type = self.data_info.setdefault(""file_type"", ""txt"")\n        data_folder = self.data_info.setdefault(""data_folder"", ""_Data"")\n        self._file_type_store = file_type\n        self._data_folder_store = data_folder\n        if data is not None:\n            return data\n        cache_folder = self.data_cache_folder_name\n        target = os.path.join(data_folder, self._name)\n        data, test_rate = self._get_data_from_file(file_type, test_rate, target)\n        if isinstance(data, tuple):\n            train_data, test_data = data\n        else:\n            if test_rate > 0:\n                random.shuffle(data)\n                n_train = int(len(data) * (1 - test_rate))\n                train_data, test_data = data[:n_train], data[n_train:]\n            else:\n                train_data, test_data = data, None\n        cache_target = os.path.join(cache_folder, self._name)\n        if not os.path.isdir(cache_target):\n            os.makedirs(cache_target)\n        self.log_msg(""Writing tmp data for param searching"", level=logging.INFO, logger=self.param_search_logger)\n        with open(os.path.join(cache_target, ""train.txt""), ""w"") as file:\n            file.write(""\\n"".join(["" "".join(line) for line in train_data]))\n        if test_data is not None:\n            with open(os.path.join(cache_target, ""test.txt""), ""w"") as file:\n                file.write(""\\n"".join(["" "".join(line) for line in test_data]))\n        self.data_info[""file_type""] = ""txt""\n        self.data_info[""data_folder""] = cache_folder\n\n    def _param_search_completion(self):\n        self._searching_params = False\n        self.param_search_time_limit = None\n        self._data_info_initialized = False\n        self.data_info[""file_type""] = self._file_type_store\n        self.data_info[""data_folder""] = self._data_folder_store\n\n    def get_param_by_range(self, param):\n        if isinstance(param, dict):\n            return {key: self.get_param_by_range(value) for key, value in param.items()}\n        dtype, *info = param\n        if not isinstance(dtype, str) and isinstance(dtype, collections.Iterable):\n            local_param_list = []\n            for local_dtype, local_info in zip(dtype, info):\n                local_param_list.append(self._extract_param_from_info(local_dtype, local_info))\n            return local_param_list\n        return self._extract_param_from_info(dtype, info)\n\n    # noinspection PyAttributeOutsideInit\n    def param_search(self, params,\n                     search_with_test_set=True, switch_to_best_param=True,\n                     single_search_time_limit=None, param_search_time_limit=3600,\n                     k=3, data=None, cv_rate=0.1, test_rate=0.1, sample_weights=None, **kwargs):\n        self._param_search_t = time.time()\n        self.param_search_time_limit = param_search_time_limit\n        logger = self.param_search_logger\n        self._searching_params = True\n        self._settings_base = {\n            ""model_param_settings"": deepcopy(self.model_param_settings),\n            ""model_structure_settings"": deepcopy(self.model_structure_settings)\n        }\n        self.mean_record, self.std_record = [], []\n        self.log_msg(\n            ""Searching best parameter setting (time_limit: {}s per run, {}s in total)"".format(\n                ""default"" if single_search_time_limit is None else single_search_time_limit,\n                param_search_time_limit\n            ), logging.DEBUG, logger\n        )\n        self._prepare_param_search_data(data, test_rate)\n        n_param = len(params)\n        for i, param in enumerate(params):\n            self.reset_graph(i)\n            self._log_param_msg(i, param)\n            self._update_param(param)\n            time_left = param_search_time_limit - self.param_search_time_delta\n            if single_search_time_limit is None:\n                local_time_limit = time_left / (n_param - i)\n            else:\n                local_time_limit = single_search_time_limit\n            kwargs[""time_limit""] = min(local_time_limit, time_left)\n            if self.k_random(k, data, cv_rate, test_rate, sample_weights, **kwargs) is not None:\n                self.save()\n                self.mean_record.append(self._k_performances_mean)\n                self.std_record.append(self._k_performances_std)\n            if self.param_search_time_delta >= param_search_time_limit:\n                self.log_msg(""Search interrupted due to \'Time limit exceeded\'"", level=logging.INFO, logger=logger)\n                break\n        self.log_msg(""Search complete"", level=logging.DEBUG, logger=logger)\n        best_idx, best_param = self._select_param(params, search_with_test_set)\n        self._log_param_msg(-1, best_param)\n        msg = """"\n        for i, (mean, std) in enumerate(zip(self.mean_record, self.std_record)):\n            msg += ""  -{} Mean   | "".format("">"" if i == best_idx else "" "")\n            msg += self._print_metrics(self._metric_name, *mean, only_return=True) + ""\\n""\n            msg += ""  -{}  Std   | "".format("">"" if i == best_idx else "" "")\n            msg += self._print_metrics(self._metric_name, *std, only_return=True)\n            if i != len(self.mean_record) - 1:\n                msg += ""\\n"" + ""-"" * 100 + ""\\n""\n        self.log_block_msg(""Generating performances"", body=msg, level=logging.DEBUG, logger=logger)\n        if switch_to_best_param:\n            self.reset_graph(-1)\n            self._update_param(best_param)\n        self._param_search_completion()\n        return self\n\n    def random_search(self, n, grid_params, grid_order=""list_first"",\n                      search_with_test_set=True, switch_to_best_params=True,\n                      single_search_time_limit=None, param_search_time_limit=3600,\n                      k=3, data=None, cv_rate=0.1, test_rate=0.1, sample_weights=None, **kwargs):\n        if grid_order == ""list_first"":\n            param_types = sorted(grid_params)\n            n_param_base = [\n                np.arange(len(grid_params[param_type]))\n                for param_type in param_types\n            ]\n            params = [\n                {\n                    param_type: grid_params[param_type][indices[i]]\n                    for i, param_type in enumerate(param_types)\n                } for indices in itertools.product(*n_param_base)\n            ]\n        elif grid_order == ""dict_first"":\n            param_types = sorted(grid_params)\n            params_names = [sorted(grid_params[param_type]) for param_type in param_types]\n            params_names_cumsum = np.cumsum([0] + [len(params_name) for params_name in params_names])\n            n_param_base = sum([\n                [np.arange(len(grid_params[param_type][param_name])) for param_name in params_name]\n                for param_type, params_name in zip(param_types, params_names)\n            ], [])\n            params = [\n                {\n                    param_type: {\n                        local_params: grid_params[param_type][local_params][indices[cumsum + j]]\n                        for j, local_params in enumerate(params_names[i])\n                    } for i, (param_type, cumsum) in enumerate(zip(param_types, params_names_cumsum))\n                } for indices in itertools.product(*n_param_base)\n            ]\n        else:\n            raise NotImplementedError(""grid_sort_type \'{}\' not implemented"".format(grid_order))\n        if n > 0:\n            params = [params[i] for i in np.random.permutation(len(params))[:n]]\n        return self.param_search(\n            params,\n            search_with_test_set, switch_to_best_params,\n            single_search_time_limit, param_search_time_limit,\n            k, data, cv_rate, test_rate, sample_weights, **kwargs\n        )\n\n    def grid_search(self, grid_params, grid_order=""list_first"",\n                    search_with_test_set=True, switch_to_best_params=True,\n                    single_search_time_limit=None, param_search_time_limit=3600,\n                    k=3, data=None, cv_rate=0.1, test_rate=0.1, sample_weights=None, **kwargs):\n        return self.random_search(\n            -1, grid_params, grid_order,\n            search_with_test_set, switch_to_best_params,\n            single_search_time_limit, param_search_time_limit,\n            k, data, cv_rate, test_rate, sample_weights, **kwargs\n        )\n\n    def range_search(self, n, grid_params,\n                     search_with_test_set=True, switch_to_best_params=True,\n                     single_search_time_limit=None, param_search_time_limit=3600,\n                     k=3, data=None, cv_rate=0.1, test_rate=0.1, sample_weights=None, **kwargs):\n        params = []\n        for _ in range(n):\n            local_params = {\n                param_type: {\n                    param_name: self.get_param_by_range(param_value)\n                    for param_name, param_value in param_values.items()\n                } for param_type, param_values in grid_params.items()\n            }\n            params.append(local_params)\n        return self.param_search(\n            params,\n            search_with_test_set, switch_to_best_params,\n            single_search_time_limit, param_search_time_limit,\n            k, data, cv_rate, test_rate, sample_weights, **kwargs\n        )\n\n    def empirical_search(self, search_with_test_set=True, switch_to_best_params=True,\n                         level=3, single_search_time_limit=None, param_search_time_limit=3600,\n                         k=3, data=None, cv_rate=0.1, test_rate=0.1, sample_weights=None, **kwargs):\n        grid_params = {\n            ""model_structure_settings"": [\n                {""use_wide_network"": False, ""use_pruner"": False, ""use_dndf_pruner"": False},\n                {""use_wide_network"": False, ""use_pruner"": True, ""use_dndf_pruner"": False},\n                {""use_wide_network"": True, ""use_pruner"": True, ""use_dndf_pruner"": False},\n            ]\n        }\n        if level >= 2:\n            grid_params[""model_structure_settings""] += [\n                {""use_wide_network"": True, ""use_pruner"": True, ""use_dndf_pruner"": True},\n                {""use_wide_network"": True, ""use_pruner"": False, ""use_dndf_pruner"": True},\n                {""use_wide_network"": True, ""use_pruner"": False, ""use_dndf_pruner"": False}\n            ]\n        if level >= 3:\n            grid_params[""pre_process_settings""] = [\n                {""reuse_mean_and_std"": False}, {""reuse_mean_and_std"": True}\n            ]\n        if level >= 4:\n            grid_params[""model_param_settings""] = [\n                {""use_batch_norm"": False}, {""use_batch_norm"": True}\n            ]\n        if level >= 5:\n            grid_params[""model_param_settings""] = [\n                {""use_batch_norm"": False, ""batch_size"": 64},\n                {""use_batch_norm"": False, ""batch_size"": 128},\n                {""use_batch_norm"": False, ""batch_size"": 256},\n                {""use_batch_norm"": True, ""batch_size"": 64},\n                {""use_batch_norm"": True, ""batch_size"": 128},\n                {""use_batch_norm"": True, ""batch_size"": 256}\n            ]\n        return self.grid_search(\n            grid_params, ""list_first"",\n            search_with_test_set, switch_to_best_params,\n            single_search_time_limit, param_search_time_limit,\n            k, data, cv_rate, test_rate, sample_weights, **kwargs\n        )\n\n    # Signatures\n\n    @staticmethod\n    def _print_metrics(metric_name, train_metric=None, cv_metric=None, test_metric=None, only_return=False):\n        raise ValueError\n\n    def _gen_batch(self, generator, n_batch, gen_random_subset=False, one_hot=False):\n        raise ValueError\n\n    def _load_data(self, data=None, numerical_idx=None, file_type=""txt"", names=(""train"", ""test""),\n                   shuffle=True, test_rate=0.1, stage=3):\n        raise ValueError\n\n    def _handle_unbalance(self, y):\n        raise ValueError\n\n    def _handle_sparsity(self):\n        raise ValueError\n\n    def _get_data_from_file(self, file_type, test_rate, target=None):\n        raise ValueError\n\n    def _evaluate(self, x=None, y=None, x_cv=None, y_cv=None, x_test=None, y_test=None, metric=None):\n        raise ValueError\n\n    def _pop_preprocessor(self, name):\n        raise ValueError\n\n    def init_data_info(self):\n        raise ValueError\n\n    def save(self, run_id=0, path=None):\n        raise ValueError\n\n    def fit(self, x=None, y=None, x_test=None, y_test=None, sample_weights=None, names=(""train"", ""test""),\n            timeit=True, time_limit=-1, snapshot_ratio=3, print_settings=True, verbose=1):\n        raise ValueError\n\n    def evaluate(self, x=None, y=None, x_cv=None, y_cv=None, x_test=None, y_test=None, metric=None):\n        raise ValueError\n\n\nclass DistMeta(type):\n    def __new__(mcs, *args, **kwargs):\n        name_, bases, attr = args[:3]\n        model, dist_mixin = bases\n\n        def __init__(self, name=None, data_info=None, model_param_settings=None, model_structure_settings=None,\n                     pre_process_settings=None, nan_handler_settings=None):\n            self._search_cursor = None\n            self._param_search_t = None\n            self.param_search_time_limit = None\n            self.mean_record = self.std_record = None\n            self._searching_params = self._settings_base = None\n\n            dist_mixin.__init__(self)\n            model.__init__(\n                self, name, data_info, model_param_settings, model_structure_settings,\n                pre_process_settings, nan_handler_settings\n            )\n\n        attr[""__init__""] = __init__\n        return type(name_, bases, attr)\n'"
_Dist/NeuralNetworks/NNUtil.py,68,"b'import os\nimport math\nimport datetime\nimport unicodedata\nimport numpy as np\nimport tensorflow as tf\nimport scipy.stats as ss\n\nfrom scipy import interp\nfrom sklearn import metrics\n\n\ndef init_w(shape, name):\n    return tf.get_variable(name, shape, initializer=tf.contrib.layers.xavier_initializer())\n\n\ndef init_b(shape, name):\n    return tf.get_variable(name, shape, initializer=tf.zeros_initializer())\n\n\ndef fully_connected_linear(net, shape, appendix, pruner=None):\n    with tf.name_scope(""Linear{}"".format(appendix)):\n        w_name = ""W{}"".format(appendix)\n        w = init_w(shape, w_name)\n        if pruner is not None:\n            w = pruner.prune_w(*pruner.get_w_info(w))\n        b = init_b(shape[1], ""b{}"".format(appendix))\n        return tf.add(tf.matmul(net, w), b, name=""Linear{}"".format(appendix))\n\n\ndef prepare_tensorboard_verbose(sess):\n    tb_log_folder = os.path.join(\n        os.path.sep, ""tmp"", ""tbLogs"",\n        str(datetime.datetime.now())[:19].replace("":"", ""-"")\n    )\n    train_dir = os.path.join(tb_log_folder, ""train"")\n    test_dir = os.path.join(tb_log_folder, ""test"")\n    for tmp_dir in (train_dir, test_dir):\n        if not os.path.isdir(tmp_dir):\n            os.makedirs(tmp_dir)\n    tf.summary.merge_all()\n    tf.summary.FileWriter(train_dir, sess.graph)\n\n\nclass Metrics:\n    sign_dict = {\n        ""f1_score"": 1,\n        ""r2_score"": 1,\n        ""auc"": 1, ""multi_auc"": 1, ""acc"": 1, ""binary_acc"": 1,\n        ""mse"": -1, ""ber"": -1, ""log_loss"": -1,\n        ""correlation"": 1, ""top_10_return"": 1\n    }\n    require_prob = {key: False for key in sign_dict}\n    require_prob[""auc""] = True\n    require_prob[""multi_auc""] = True\n\n    @staticmethod\n    def check_shape(y, binary=False):\n        y = np.asarray(y, np.float32)\n        if len(y.shape) == 2:\n            if binary:\n                if y.shape[1] == 2:\n                    return y[..., 1]\n                assert y.shape[1] == 1\n                return y.ravel()\n            return np.argmax(y, axis=1)\n        return y\n\n    @staticmethod\n    def f1_score(y, pred):\n        return metrics.f1_score(Metrics.check_shape(y), Metrics.check_shape(pred))\n\n    @staticmethod\n    def r2_score(y, pred):\n        return metrics.r2_score(y, pred)\n\n    @staticmethod\n    def auc(y, pred):\n        return metrics.roc_auc_score(\n            Metrics.check_shape(y, True),\n            Metrics.check_shape(pred, True)\n        )\n\n    @staticmethod\n    def multi_auc(y, pred):\n        n_classes = pred.shape[1]\n        if len(y.shape) == 1:\n            y = Toolbox.get_one_hot(y, n_classes)\n        fpr, tpr = [None] * n_classes, [None] * n_classes\n        for i in range(n_classes):\n            fpr[i], tpr[i], _ = metrics.roc_curve(y[:, i], pred[:, i])\n        new_fpr = np.unique(np.concatenate([fpr[i] for i in range(n_classes)]))\n        new_tpr = np.zeros_like(new_fpr)\n        for i in range(n_classes):\n            new_tpr += interp(new_fpr, fpr[i], tpr[i])\n        new_tpr /= n_classes\n        return metrics.auc(new_fpr, new_tpr)\n\n    @staticmethod\n    def acc(y, pred):\n        return np.mean(Metrics.check_shape(y) == Metrics.check_shape(pred))\n\n    @staticmethod\n    def binary_acc(y, pred):\n        return np.mean((y > 0) == (pred > 0))\n\n    @staticmethod\n    def mse(y, pred):\n        return np.mean(np.square(y.ravel() - pred.ravel()))\n\n    @staticmethod\n    def ber(y, pred):\n        mat = metrics.confusion_matrix(Metrics.check_shape(y), Metrics.check_shape(pred))\n        tp = np.diag(mat)\n        fp = mat.sum(axis=0) - tp\n        fn = mat.sum(axis=1) - tp\n        tn = mat.sum() - (tp + fp + fn)\n        return 0.5 * np.mean((fn / (tp + fn) + fp / (tn + fp)))\n\n    @staticmethod\n    def log_loss(y, pred):\n        return metrics.log_loss(y, pred)\n\n    @staticmethod\n    def correlation(y, pred):\n        return float(ss.pearsonr(y, pred)[0])\n\n    @staticmethod\n    def top_10_return(y, pred):\n        return np.mean(y[pred >= np.percentile(pred, 90)])\n\n    @staticmethod\n    def from_fpr_tpr(pos, fpr, tpr, metric):\n        if metric == ""ber"":\n            return 0.5 * (1 - tpr + fpr)\n        return tpr * pos + (1 - fpr) * (1 - pos)\n\n\nclass Losses:\n    @staticmethod\n    def mse(y, pred, _, weights=None):\n        if weights is None:\n            return tf.losses.mean_squared_error(y, pred)\n        return tf.losses.mean_squared_error(y, pred, tf.reshape(weights, [-1, 1]))\n\n    @staticmethod\n    def cross_entropy(y, pred, already_prob, weights=None):\n        if already_prob:\n            eps = 1e-12\n            pred = tf.log(tf.clip_by_value(pred, eps, 1 - eps))\n        if weights is None:\n            return tf.losses.softmax_cross_entropy(y, pred)\n        return tf.losses.softmax_cross_entropy(y, pred, weights)\n\n    @staticmethod\n    def correlation(y, pred, _, weights=None):\n        y_mean, y_var = tf.nn.moments(y, 0)\n        pred_mean, pred_var = tf.nn.moments(pred, 0)\n        if weights is None:\n            e = tf.reduce_mean((y - y_mean) * (pred - pred_mean))\n        else:\n            e = tf.reduce_mean((y - y_mean) * (pred - pred_mean) * weights)\n        return -e / tf.sqrt(y_var * pred_var)\n\n    @staticmethod\n    def perceptron(y, pred, _, weights=None):\n        if weights is None:\n            return -tf.reduce_mean(y * pred)\n        return -tf.reduce_mean(y * pred * weights)\n\n    @staticmethod\n    def clipped_perceptron(y, pred, _, weights=None):\n        if weights is None:\n            return -tf.reduce_mean(tf.maximum(0., y * pred))\n        return -tf.reduce_mean(tf.maximum(0., y * pred * weights))\n\n    @staticmethod\n    def regression(y, pred, *_):\n        return Losses.correlation(y, pred, *_)\n\n\nclass Activations:\n    @staticmethod\n    def elu(x, name):\n        return tf.nn.elu(x, name)\n\n    @staticmethod\n    def relu(x, name):\n        return tf.nn.relu(x, name)\n\n    @staticmethod\n    def selu(x, name):\n        alpha = 1.6732632423543772848170429916717\n        scale = 1.0507009873554804934193349852946\n        return tf.multiply(scale, tf.where(x >= 0., x, alpha * tf.nn.elu(x)), name)\n\n    @staticmethod\n    def sigmoid(x, name):\n        return tf.nn.sigmoid(x, name)\n\n    @staticmethod\n    def tanh(x, name):\n        return tf.nn.tanh(x, name)\n\n    @staticmethod\n    def softplus(x, name):\n        return tf.nn.softplus(x, name)\n\n    @staticmethod\n    def softmax(x, name):\n        return tf.nn.softmax(x, name=name)\n\n    @staticmethod\n    def sign(x, name):\n        return tf.sign(x, name)\n\n    @staticmethod\n    def one_hot(x, name):\n        return tf.multiply(\n            x,\n            tf.cast(tf.equal(x, tf.expand_dims(tf.reduce_max(x, 1), 1)), tf.float32),\n            name=name\n        )\n\n\nclass Toolbox:\n    @staticmethod\n    def is_number(s):\n        try:\n            s = float(s)\n            if math.isnan(s):\n                return False\n            return True\n        except ValueError:\n            try:\n                unicodedata.numeric(s)\n                return True\n            except (TypeError, ValueError):\n                return False\n\n    @staticmethod\n    def all_same(target):\n        x = target[0]\n        for new in target[1:]:\n            if new != x:\n                return False\n        return True\n\n    @staticmethod\n    def all_unique(target):\n        seen = set()\n        return not any(x in seen or seen.add(x) for x in target)\n\n    @staticmethod\n    def warn_all_same(i, logger=None):\n        warn_msg = ""All values in column {} are the same, it\'ll be treated as redundant"".format(i)\n        print(warn_msg) if logger is None else logger.debug(warn_msg)\n\n    @staticmethod\n    def warn_all_unique(i, logger=None):\n        warn_msg = ""All values in column {} are unique, it\'ll be treated as redundant"".format(i)\n        print(warn_msg) if logger is None else logger.debug(warn_msg)\n\n    @staticmethod\n    def pop_nan(feat):\n        no_nan_feat = []\n        for f in feat:\n            try:\n                f = float(f)\n                if math.isnan(f):\n                    continue\n                no_nan_feat.append(f)\n            except ValueError:\n                no_nan_feat.append(f)\n        return no_nan_feat\n\n    @staticmethod\n    def shrink_nan(feat):\n        new = np.asarray(feat, np.float32)\n        new = new[~np.isnan(new)].tolist()\n        if len(new) < len(feat):\n            new.append(float(""nan""))\n        return new\n\n    @staticmethod\n    def get_data(file, sep="" "", include_header=False, logger=None):\n        msg = ""Fetching data""\n        print(msg) if logger is None else logger.debug(msg)\n        data = [[elem if elem else ""nan"" for elem in line.strip().split(sep)] for line in file]\n        if include_header:\n            return data[1:]\n        return data\n\n    @staticmethod\n    def get_one_hot(y, n_class):\n        if y is None:\n            return\n        one_hot = np.zeros([len(y), n_class])\n        one_hot[range(len(one_hot)), np.asarray(y, np.int)] = 1\n        return one_hot\n\n    @staticmethod\n    def get_feature_info(data, numerical_idx, is_regression, logger=None):\n        generate_numerical_idx = False\n        if numerical_idx is None:\n            generate_numerical_idx = True\n            numerical_idx = [False] * len(data[0])\n        else:\n            numerical_idx = list(numerical_idx)\n        data_t = data.T if isinstance(data, np.ndarray) else list(zip(*data))\n        if type(data[0][0]) is not str:\n            shrink_features = [Toolbox.shrink_nan(feat) for feat in data_t]\n        else:\n            shrink_features = data_t\n        feature_sets = [\n            set() if idx is None or idx else set(shrink_feature)\n            for idx, shrink_feature in zip(numerical_idx, shrink_features)\n        ]\n        n_features = [len(feature_set) for feature_set in feature_sets]\n        all_num_idx = [\n            True if not feature_set else all(Toolbox.is_number(str(feat)) for feat in feature_set)\n            for feature_set in feature_sets\n        ]\n        if generate_numerical_idx:\n            np_shrink_features = [\n                shrink_feature if not all_num else np.asarray(shrink_feature, np.float32)\n                for all_num, shrink_feature in zip(all_num_idx, shrink_features)\n            ]\n            all_unique_idx = [\n                len(feature_set) == len(np_shrink_feature)\n                and (not all_num or np.allclose(np_shrink_feature, np_shrink_feature.astype(np.int32)))\n                for all_num, feature_set, np_shrink_feature in zip(all_num_idx, feature_sets, np_shrink_features)\n            ]\n            numerical_idx = Toolbox.get_numerical_idx(feature_sets, all_num_idx, all_unique_idx, logger)\n            for i, numerical in enumerate(numerical_idx):\n                if numerical is None:\n                    all_num_idx[i] = None\n        else:\n            for i, (feature_set, shrink_feature) in enumerate(zip(feature_sets, shrink_features)):\n                if i == len(numerical_idx) - 1 or numerical_idx[i] is None:\n                    continue\n                if feature_set:\n                    if len(feature_set) == 1:\n                        Toolbox.warn_all_same(i, logger)\n                        all_num_idx[i] = numerical_idx[i] = None\n                    continue\n                if Toolbox.all_same(shrink_feature):\n                    Toolbox.warn_all_same(i, logger)\n                    all_num_idx[i] = numerical_idx[i] = None\n                elif numerical_idx[i]:\n                    shrink_feature = np.asarray(shrink_feature, np.float32)\n                    if np.max(shrink_feature[~np.isnan(shrink_feature)]) < 2 ** 30:\n                        if np.allclose(shrink_feature, np.array(shrink_feature, np.int32)):\n                            if Toolbox.all_unique(shrink_feature):\n                                Toolbox.warn_all_unique(i, logger)\n                                all_num_idx[i] = numerical_idx[i] = None\n        if is_regression:\n            all_num_idx[-1] = numerical_idx[-1] = True\n            feature_sets[-1] = set()\n            n_features.pop()\n        return feature_sets, n_features, all_num_idx, numerical_idx\n\n    @staticmethod\n    def get_numerical_idx(feature_sets, all_num_idx, all_unique_idx, logger=None):\n        rs = []\n        print(""Generating numerical_idx"")\n        for i, (feat_set, all_num, all_unique) in enumerate(\n            zip(feature_sets, all_num_idx, all_unique_idx)\n        ):\n            if len(feat_set) == 1:\n                Toolbox.warn_all_same(i, logger)\n                rs.append(None)\n                continue\n            no_nan_feat = Toolbox.pop_nan(feat_set)\n            if not all_num:\n                if len(feat_set) == len(no_nan_feat):\n                    rs.append(False)\n                    continue\n                if not all(Toolbox.is_number(str(feat)) for feat in no_nan_feat):\n                    rs.append(False)\n                    continue\n            no_nan_feat = np.array(list(no_nan_feat), np.float32)\n            int_no_nan_feat = no_nan_feat.astype(np.int32)\n            n_feat, feat_min, feat_max = len(no_nan_feat), no_nan_feat.min(), no_nan_feat.max()\n            if not np.allclose(no_nan_feat, int_no_nan_feat):\n                rs.append(True)\n                continue\n            if all_unique:\n                Toolbox.warn_all_unique(i, logger)\n                rs.append(None)\n                continue\n            feat_min, feat_max = int(feat_min), int(feat_max)\n            if np.allclose(np.sort(no_nan_feat), np.linspace(feat_min, feat_max, n_feat)):\n                rs.append(False)\n                continue\n            if feat_min >= 20 and n_feat >= 20:\n                rs.append(True)\n            elif 1.5 * n_feat >= feat_max - feat_min:\n                rs.append(False)\n            else:\n                rs.append(True)\n        return np.array(rs)\n\n\nclass TrainMonitor:\n    def __init__(self, sign, snapshot_ratio, history_ratio=3, tolerance_ratio=2,\n                 extension=5, std_floor=0.001, std_ceiling=0.01):\n        self.sign = sign\n        self.snapshot_ratio = snapshot_ratio\n        self.n_history = int(snapshot_ratio * history_ratio)\n        self.n_tolerance = int(snapshot_ratio * tolerance_ratio)\n        self.extension = extension\n        self.std_floor, self.std_ceiling = std_floor, std_ceiling\n        self._scores = []\n        self.flat_flag = False\n        self._is_best = self._running_best = None\n        self._running_sum = self._running_square_sum = None\n        self._descend_increment = self.n_history * extension / 30\n\n        self._over_fit_performance = math.inf\n        self._best_checkpoint_performance = -math.inf\n        self._descend_counter = self._flat_counter = self.over_fitting_flag = 0\n        self.info = {""terminate"": False, ""save_checkpoint"": False, ""save_best"": False, ""info"": None}\n\n    def punish_extension(self):\n        self._descend_counter += self._descend_increment\n\n    def _update_running_info(self, last_score, n_history):\n        if n_history < self.n_history or n_history == len(self._scores):\n            if self._running_sum is None or self._running_square_sum is None:\n                self._running_sum = self._scores[0] + self._scores[1]\n                self._running_square_sum = self._scores[0] ** 2 + self._scores[1] ** 2\n            else:\n                self._running_sum += last_score\n                self._running_square_sum += last_score ** 2\n        else:\n            previous = self._scores[-n_history - 1]\n            self._running_sum += last_score - previous\n            self._running_square_sum += last_score ** 2 - previous ** 2\n        if self._running_best is None:\n            if self._scores[0] > self._scores[1]:\n                improvement = 0\n                self._running_best, self._is_best = self._scores[0], False\n            else:\n                improvement = self._scores[1] - self._scores[0]\n                self._running_best, self._is_best = self._scores[1], True\n        elif self._running_best > last_score:\n            improvement = 0\n            self._is_best = False\n        else:\n            improvement = last_score - self._running_best\n            self._running_best = last_score\n            self._is_best = True\n        return improvement\n\n    def _handle_overfitting(self, last_score, res, std):\n        if self._descend_counter == 0:\n            self.info[""save_best""] = True\n            self._over_fit_performance = last_score\n        self._descend_counter += min(self.n_tolerance / 3, -res / std)\n        self.over_fitting_flag = 1\n\n    def _handle_recovering(self, improvement, last_score, res, std):\n        if res > 3 * std and self._is_best and improvement > std:\n            self.info[""save_best""] = True\n        new_counter = self._descend_counter - res / std\n        if self._descend_counter > 0 >= new_counter:\n            self._over_fit_performance = math.inf\n            if last_score > self._best_checkpoint_performance:\n                self._best_checkpoint_performance = last_score\n                if last_score > self._running_best - std:\n                    self.info[""save_checkpoint""] = True\n                    self.info[""info""] = (\n                        ""Current snapshot ({}) seems to be working well, ""\n                        ""saving checkpoint in case we need to restore"".format(len(self._scores))\n                    )\n            self.over_fitting_flag = 0\n        self._descend_counter = max(new_counter, 0)\n\n    def _handle_is_best(self):\n        if self._is_best:\n            self.info[""terminate""] = False\n            if self.info[""save_best""]:\n                self.info[""save_checkpoint""] = True\n                self.info[""save_best""] = False\n                self.info[""info""] = (\n                    ""Current snapshot ({}) leads to best result we\'ve ever had, ""\n                    ""saving checkpoint since "".format(len(self._scores))\n                )\n                if self.over_fitting_flag:\n                    self.info[""info""] += ""we\'ve suffered from over-fitting""\n                else:\n                    self.info[""info""] += ""performance has improved significantly""\n\n    def _handle_period(self, last_score):\n        if len(self._scores) % self.snapshot_ratio == 0 and last_score > self._best_checkpoint_performance:\n            self._best_checkpoint_performance = last_score\n            self.info[""terminate""] = False\n            self.info[""save_checkpoint""] = True\n            self.info[""info""] = (\n                ""Current snapshot ({}) leads to best checkpoint we\'ve ever had, ""\n                ""saving checkpoint in case we need to restore"".format(len(self._scores))\n            )\n\n    def check(self, new_metric):\n        last_score = new_metric * self.sign\n        self._scores.append(last_score)\n        n_history = min(self.n_history, len(self._scores))\n        if n_history == 1:\n            return self.info\n        improvement = self._update_running_info(last_score, n_history)\n        self.info[""save_checkpoint""] = False\n        mean = self._running_sum / n_history\n        std = math.sqrt(max(self._running_square_sum / n_history - mean ** 2, 1e-12))\n        std = min(std, self.std_ceiling)\n        if std < self.std_floor:\n            if self.flat_flag:\n                self._flat_counter += 1\n        else:\n            self._flat_counter = max(self._flat_counter - 1, 0)\n            res = last_score - mean\n            if res < -std and last_score < self._over_fit_performance - std:\n                self._handle_overfitting(last_score, res, std)\n            elif res > std:\n                self._handle_recovering(improvement, last_score, res, std)\n        if self._flat_counter >= self.n_tolerance * self.n_history:\n            self.info[""info""] = ""Performance not improving""\n            self.info[""terminate""] = True\n            return self.info\n        if self._descend_counter >= self.n_tolerance:\n            self.info[""info""] = ""Over-fitting""\n            self.info[""terminate""] = True\n            return self.info\n        self._handle_is_best()\n        self._handle_period(last_score)\n        return self.info\n\n\nclass DNDF:\n    def __init__(self, n_class, n_tree=10, tree_depth=4):\n        self.n_class = n_class\n        self.n_tree, self.tree_depth = n_tree, tree_depth\n        self.n_leaf = 2 ** (tree_depth + 1)\n        self.n_internals = self.n_leaf - 1\n\n    def __call__(self, net, n_batch_placeholder, dtype=""output"", pruner=None):\n        name = ""DNDF_{}"".format(dtype)\n        with tf.variable_scope(name, reuse=tf.AUTO_REUSE):\n            flat_probabilities = self.build_tree_projection(dtype, net, pruner)\n            routes = self.build_routes(flat_probabilities, n_batch_placeholder)\n            features = tf.concat(routes, 1, name=""Feature_Concat"")\n            if dtype == ""feature"":\n                return features\n            leafs = self.build_leafs()\n            leafs_matrix = tf.concat(leafs, 0, name=""Prob_Concat"")\n            return tf.divide(\n                tf.matmul(features, leafs_matrix),\n                float(self.n_tree), name=name\n            )\n\n    def build_tree_projection(self, dtype, net, pruner):\n        with tf.name_scope(""Tree_Projection""):\n            flat_probabilities = []\n            fc_shape = net.shape[1].value\n            for i in range(self.n_tree):\n                with tf.name_scope(""Decisions""):\n                    p_left = tf.nn.sigmoid(fully_connected_linear(\n                        net=net,\n                        shape=[fc_shape, self.n_internals],\n                        appendix=""_tree_mapping{}_{}"".format(i, dtype), pruner=pruner\n                    ))\n                    p_right = 1 - p_left\n                    p_all = tf.concat([p_left, p_right], 1)\n                    flat_probabilities.append(tf.reshape(p_all, [-1]))\n        return flat_probabilities\n\n    def build_routes(self, flat_probabilities, n_batch_placeholder):\n        with tf.name_scope(""Routes""):\n            n_flat_prob = 2 * self.n_internals\n            batch_indices = tf.reshape(\n                tf.range(0, n_flat_prob * n_batch_placeholder, n_flat_prob),\n                [-1, 1]\n            )\n            n_repeat, n_local_internals = int(self.n_leaf * 0.5), 1\n            increment_mask = np.repeat([0, self.n_internals], n_repeat)\n            routes = [\n                tf.gather(p_flat, batch_indices + increment_mask)\n                for p_flat in flat_probabilities\n            ]\n            for depth in range(1, self.tree_depth + 1):\n                n_repeat = int(n_repeat * 0.5)\n                n_local_internals *= 2\n                increment_mask = np.repeat(np.arange(\n                    n_local_internals - 1, 2 * n_local_internals - 1\n                ), 2)\n                increment_mask += np.tile([0, self.n_internals], n_local_internals)\n                increment_mask = np.repeat(increment_mask, n_repeat)\n                for i, p_flat in enumerate(flat_probabilities):\n                    routes[i] *= tf.gather(p_flat, batch_indices + increment_mask)\n        return routes\n\n    def build_leafs(self):\n        with tf.name_scope(""Leafs""):\n            if self.n_class == 1:\n                local_leafs = [\n                    init_w([self.n_leaf, 1], ""RegLeaf{}"".format(i))\n                    for i in range(self.n_tree)\n                ]\n            else:\n                local_leafs = [\n                    tf.nn.softmax(w, name=""ClfLeafs{}"".format(i))\n                    for i, w in enumerate([\n                        init_w([self.n_leaf, self.n_class], ""RawClfLeafs"")\n                        for _ in range(self.n_tree)\n                    ])\n                ]\n        return local_leafs\n\n\nclass Pruner:\n    def __init__(self, alpha=None, beta=None, gamma=None, r=1., eps=1e-12, prune_method=""soft_prune""):\n        self.alpha, self.beta, self.gamma, self.r, self.eps = alpha, beta, gamma, r, eps\n        self.org_ws, self.masks, self.cursor = [], [], -1\n        self.method = prune_method\n        if prune_method == ""soft_prune"" or prune_method == ""hard_prune"":\n            if alpha is None:\n                self.alpha = 1e-2\n            if beta is None:\n                self.beta = 1\n            if gamma is None:\n                self.gamma = 1\n            if prune_method == ""hard_prune"":\n                self.alpha *= 0.01\n            self.cond_placeholder = None\n        elif prune_method == ""surgery"":\n            if alpha is None:\n                self.alpha = 1\n            if beta is None:\n                self.beta = 1\n            if gamma is None:\n                self.gamma = 0.0001\n            self.r = None\n            self.cond_placeholder = tf.placeholder(tf.bool, (), name=""Prune_flag"")\n        else:\n            raise NotImplementedError(""prune_method \'{}\' is not defined"".format(prune_method))\n\n    @property\n    def params(self):\n        return {\n            ""eps"": self.eps, ""alpha"": self.alpha, ""beta"": self.beta, ""gamma"": self.gamma,\n            ""max_ratio"": self.r, ""method"": self.method\n        }\n\n    @staticmethod\n    def get_w_info(w):\n        with tf.name_scope(""get_w_info""):\n            w_abs = tf.abs(w)\n            w_abs_mean, w_abs_var = tf.nn.moments(w_abs, None)\n            return w, w_abs, w_abs_mean, tf.sqrt(w_abs_var)\n\n    def prune_w(self, w, w_abs, w_abs_mean, w_abs_std):\n        self.cursor += 1\n        self.org_ws.append(w)\n        with tf.name_scope(""Prune""):\n            if self.cond_placeholder is None:\n                log_w = tf.log(tf.maximum(self.eps, w_abs / (w_abs_mean * self.gamma)))\n                if self.r > 0:\n                    log_w = tf.minimum(self.r, self.beta * log_w)\n                self.masks.append(tf.maximum(self.alpha / self.beta * log_w, log_w))\n                return w * self.masks[self.cursor]\n\n            self.masks.append(tf.Variable(tf.ones_like(w), trainable=False))\n\n            def prune(i, do_prune):\n                def sub():\n                    if do_prune:\n                        mask = self.masks[i]\n                        self.masks[i] = tf.assign(mask, tf.where(\n                            tf.logical_and(\n                                tf.equal(mask, 1),\n                                tf.less_equal(w_abs, 0.9 * tf.maximum(w_abs_mean + self.beta * w_abs_std, self.eps))\n                            ),\n                            tf.zeros_like(mask), mask\n                        ))\n                        mask = self.masks[i]\n                        self.masks[i] = tf.assign(mask, tf.where(\n                            tf.logical_and(\n                                tf.equal(mask, 0),\n                                tf.greater(w_abs, 1.1 * tf.maximum(w_abs_mean + self.beta * w_abs_std, self.eps))\n                            ),\n                            tf.ones_like(mask), mask\n                        ))\n                    return w * self.masks[i]\n                return sub\n\n            return tf.cond(self.cond_placeholder, prune(self.cursor, True), prune(self.cursor, False))\n\n\nclass NanHandler:\n    def __init__(self, method, reuse_values=True):\n        self._values = None\n        self.method = method\n        self.reuse_values = reuse_values\n\n    def transform(self, x, numerical_idx, refresh_values=False):\n        if self.method is None:\n            pass\n        elif self.method == ""delete"":\n            x = x[~np.any(np.isnan(x[..., numerical_idx]), axis=1)]\n        else:\n            if self._values is None:\n                self._values = [None] * len(numerical_idx)\n            for i, (v, numerical) in enumerate(zip(self._values, numerical_idx)):\n                if not numerical:\n                    continue\n                feat = x[..., i]\n                mask = np.isnan(feat)\n                if not np.any(mask):\n                    continue\n                if self.reuse_values and not refresh_values and v is not None:\n                    new_value = v\n                else:\n                    new_value = getattr(np, self.method)(feat[~mask])\n                    if self.reuse_values and (v is None or refresh_values):\n                        self._values[i] = new_value\n                feat[mask] = new_value\n        return x\n\n    def reset(self):\n        self._values = None\n\n\nclass PreProcessor:\n    def __init__(self, method, scale_method, eps_floor=1e-4, eps_ceiling=1e12):\n        self.method, self.scale_method = method, scale_method\n        self.eps_floor, self.eps_ceiling = eps_floor, eps_ceiling\n        self.redundant_idx = None\n        self.min = self.max = self.mean = self.std = None\n\n    def _scale(self, x, numerical_idx):\n        targets = x[..., numerical_idx]\n        self.redundant_idx = [False] * len(numerical_idx)\n        mean = std = None\n        if self.mean is not None:\n            mean = self.mean\n        if self.std is not None:\n            std = self.std\n        if self.min is None:\n            self.min = targets.min(axis=0)\n        if self.max is None:\n            self.max = targets.max(axis=0)\n        if mean is None:\n            mean = targets.mean(axis=0)\n        abs_targets = np.abs(targets)\n        max_features = abs_targets.max(axis=0)\n        if self.scale_method is not None:\n            max_features_res = max_features - mean\n            mask = max_features_res > self.eps_ceiling\n            n_large = np.sum(mask)\n            if n_large > 0:\n                idx_lst, val_lst = [], []\n                mask_cursor = -1\n                for i, numerical in enumerate(numerical_idx):\n                    if not numerical:\n                        continue\n                    mask_cursor += 1\n                    if not mask[mask_cursor]:\n                        continue\n                    idx_lst.append(i)\n                    val_lst.append(max_features_res[mask_cursor])\n                    local_target = targets[..., mask_cursor]\n                    local_abs_target = abs_targets[..., mask_cursor]\n                    sign_mask = np.ones(len(targets))\n                    sign_mask[local_target < 0] *= -1\n                    scaled_value = self._scale_abs_features(local_abs_target) * sign_mask\n                    targets[..., mask_cursor] = scaled_value\n                    if self.mean is None:\n                        mean[mask_cursor] = np.mean(scaled_value)\n                    max_features[mask_cursor] = np.max(scaled_value)\n                warn_msg = ""{} value which is too large: [{}]{}"".format(\n                    ""These {} columns contain"".format(n_large) if n_large > 1 else ""One column contains"",\n                    "", "".join(\n                        ""{}: {:8.6f}"".format(idx, val)\n                        for idx, val in zip(idx_lst, val_lst)\n                    ),\n                    "", {} will be scaled by \'{}\' method"".format(\n                        ""it"" if n_large == 1 else ""they"", self.scale_method\n                    )\n                )\n                print(warn_msg)\n                x[..., numerical_idx] = targets\n        if std is None:\n            if np.any(max_features > self.eps_ceiling):\n                targets = targets - mean\n            std = np.maximum(self.eps_floor, targets.std(axis=0))\n        if self.mean is None and self.std is None:\n            self.mean, self.std = mean, std\n        return x\n\n    def _scale_abs_features(self, abs_features):\n        if self.scale_method == ""truncate"":\n            return np.minimum(abs_features, self.eps_ceiling)\n        if self.scale_method == ""divide"":\n            return abs_features / self.max\n        if self.scale_method == ""log"":\n            return np.log(abs_features + 1)\n        return getattr(np, self.scale_method)(abs_features)\n\n    def _normalize(self, x, numerical_idx):\n        x[..., numerical_idx] -= self.mean\n        x[..., numerical_idx] /= np.maximum(self.eps_floor, self.std)\n        return x\n\n    def _min_max(self, x, numerical_idx):\n        x[..., numerical_idx] -= self.min\n        x[..., numerical_idx] /= np.maximum(self.eps_floor, self.max - self.min)\n        return x\n\n    def transform(self, x, numerical_idx):\n        x = self._scale(np.array(x, dtype=np.float32), numerical_idx)\n        x = getattr(self, ""_"" + self.method)(x, numerical_idx)\n        return x\n\n\n__all__ = [\n    ""init_w"", ""init_b"", ""fully_connected_linear"", ""prepare_tensorboard_verbose"",\n    ""Toolbox"", ""Metrics"", ""Losses"", ""Activations"", ""TrainMonitor"",\n    ""DNDF"", ""Pruner"", ""NanHandler"", ""PreProcessor""\n]\n'"
_Dist/TextClassification/GenDataset.py,0,"b'import os\nimport pickle\nimport numpy as np\n\n\ndef gen_dataset(dat_path):\n    if not os.path.isfile(dat_path):\n        print(""\\nGenerating Dataset..."")\n        folders = os.listdir(""_Data"")\n        label_dic = [folder for folder in folders if os.path.isdir(os.path.join(""_Data"", folder))]\n        folders_path = [os.path.join(""_Data"", folder) for folder in label_dic]\n        x, y = [], []\n        for i, folder in enumerate(folders_path):\n            for txt in os.listdir(folder):\n                with open(os.path.join(folder, txt), ""r"", encoding=""utf-8"") as file:\n                    try:\n                        x.append(file.read().strip().split())\n                        y.append(i)\n                    except Exception as err:\n                        print(err)\n        np.save(os.path.join(""_Data"", ""LABEL_DIC.npy""), label_dic)\n        with open(dat_path, ""wb"") as file:\n            pickle.dump((x, y), file)\n        print(""Done"")\n'"
_Dist/TextClassification/Main.py,0,"b'import os\nimport pickle\n\nfrom SkRun import run\n\nif not os.path.isfile(""dataset.dat""):\n    print(""Processing data..."")\n    rs, labels = [], []\n    data_folder = ""_Data""\n    for i, folder in enumerate(os.listdir(data_folder)):\n        for txt_file in os.listdir(os.path.join(data_folder, folder)):\n            with open(os.path.join(data_folder, folder, txt_file), ""r"", encoding=""utf-8"") as file:\n                try:\n                    rs.append(file.readline().split())\n                    labels.append(i)\n                except UnicodeDecodeError as err:\n                    print(err)\n    with open(""dataset.dat"", ""wb"") as file:\n        pickle.dump((rs, labels), file)\n    print(""Done"")\n\nprint(""Running Naive Bayes written by myself..."")\nos.system(""python _NB.py"")\n\nprint(""Running Naive Bayes in sklearn..."")\nrun(""Naive Bayes"")\n\nprint(""Running LinearSVM in sklearn"")\nrun(""SVM"")\n'"
_Dist/TextClassification/SkRun.py,0,"b'import os\r\nimport math\r\nimport pickle\r\nimport numpy as np\r\nimport matplotlib.pyplot as plt\r\n\r\nfrom sklearn.feature_extraction.text import CountVectorizer\r\nfrom sklearn.feature_extraction.text import TfidfTransformer\r\nfrom sklearn import metrics\r\n\r\nfrom _SKlearn.NaiveBayes import SKMultinomialNB\r\nfrom _SKlearn.SVM import SKSVM, SKLinearSVM\r\nfrom _Dist.TextClassification.GenDataset import gen_dataset\r\nfrom Util.ProgressBar import ProgressBar\r\n\r\n\r\ndef main(clf):\r\n    dat_path = os.path.join(""_Data"", ""dataset.dat"")\r\n    gen_dataset(dat_path)\r\n    with open(dat_path, ""rb"") as _file:\r\n        x, y = pickle.load(_file)\r\n    x = ["" "".join(sentence) for sentence in x]\r\n    _indices = np.random.permutation(len(x))\r\n    x = list(np.array(x)[_indices])\r\n    y = list(np.array(y)[_indices])\r\n    data_len = len(x)\r\n    batch_size = math.ceil(data_len * 0.1)\r\n    acc_lst, y_results = [], []\r\n    bar = ProgressBar(max_value=10, name=str(clf))\r\n    for i in range(10):\r\n        _next = (i + 1) * batch_size if i != 9 else data_len\r\n        x_train = x[:i * batch_size] + x[(i + 1) * batch_size:]\r\n        y_train = y[:i * batch_size] + y[(i + 1) * batch_size:]\r\n        x_test, y_test = x[i * batch_size:_next], y[i * batch_size:_next]\r\n        count_vec = CountVectorizer()\r\n        counts_train = count_vec.fit_transform(x_train)\r\n        x_test = count_vec.transform(x_test)\r\n        tfidf_transformer = TfidfTransformer()\r\n        x_train = tfidf_transformer.fit_transform(counts_train)\r\n        clf.fit(x_train, y_train)\r\n        y_pred = clf.predict(x_test)\r\n        acc_lst.append(clf.acc(y_test, y_pred))\r\n        y_results.append([y_test, y_pred])\r\n        del x_train, y_train, x_test, y_test, y_pred\r\n        bar.update()\r\n    return acc_lst, y_results\r\n\r\n\r\ndef run(clf):\r\n    acc_records, y_records = [], []\r\n    bar = ProgressBar(max_value=10, name=""Main"")\r\n    for _ in range(10):\r\n        if clf == ""Naive Bayes"":\r\n            _clf = SKMultinomialNB(alpha=0.1)\r\n        elif clf == ""Non-linear SVM"":\r\n            _clf = SKSVM()\r\n        else:\r\n            _clf = SKLinearSVM()\r\n        rs = main(_clf)\r\n        acc_records.append(rs[0])\r\n        y_records += rs[1]\r\n        bar.update()\r\n    acc_records = np.array(acc_records) * 100\r\n\r\n    plt.figure()\r\n    plt.boxplot(acc_records, vert=False, showmeans=True)\r\n    plt.show()\r\n\r\n    from Util.DataToolkit import DataToolkit\r\n    idx = np.argmax(acc_records)  # type: int\r\n    print(metrics.classification_report(y_records[idx][0], y_records[idx][1], target_names=np.load(os.path.join(\r\n        ""_Data"", ""LABEL_DIC.npy""\r\n    ))))\r\n    toolkit = DataToolkit(acc_records[np.argmax(np.average(acc_records, axis=1))])\r\n    print(""Acc Mean     : {:8.6}"".format(toolkit.mean))\r\n    print(""Acc Variance : {:8.6}"".format(toolkit.variance))\r\n    print(""Done"")\r\n\r\nif __name__ == \'__main__\':\r\n    run(""SVM"")\r\n'"
_Dist/TextClassification/_NB.py,0,"b'import os\r\nimport math\r\nimport pickle\r\nimport numpy as np\r\nfrom collections import Counter\r\nimport matplotlib.pyplot as plt\r\n\r\nfrom sklearn import metrics\r\n\r\nfrom _Dist.TextClassification.GenDataset import gen_dataset\r\nfrom Util.ProgressBar import ProgressBar\r\n\r\n\r\ndef pick_best(sentence, prob_lst):\r\n    rs = [prob[""prior""] for prob in prob_lst]\r\n    for j, _prob_dic in enumerate(prob_lst):\r\n        for word in sentence:\r\n            if word in _prob_dic:\r\n                rs[j] *= _prob_dic[word]\r\n            else:\r\n                rs[j] /= _prob_dic[""null""]\r\n    return np.argmax(rs)\r\n\r\n\r\ndef train(power=6.46):\r\n    dat_path = os.path.join(""_Data"", ""dataset.dat"")\r\n    gen_dataset(dat_path)\r\n    with open(dat_path, ""rb"") as _file:\r\n        x, y = pickle.load(_file)\r\n    _indices = np.random.permutation(len(x))\r\n    x = [x[i] for i in _indices]\r\n    y = [y[i] for i in _indices]\r\n    data_len = len(x)\r\n    batch_size = math.ceil(data_len*0.1)\r\n    _test_sets, _prob_lists = [], []\r\n    _total = sum([len(sentence) for sentence in x])\r\n    for i in range(10):\r\n        rs = [[] for _ in range(9)]\r\n        _next = (i+1)*batch_size if i != 9 else data_len\r\n        x_train = x[:i * batch_size] + x[(i + 1) * batch_size:]\r\n        y_train = y[:i * batch_size] + y[(i + 1) * batch_size:]\r\n        x_test, y_test = x[i*batch_size:_next], y[i*batch_size:_next]\r\n        for xx, yy in zip(x_train, y_train):\r\n            rs[yy] += xx\r\n        _counters = [Counter(group) for group in rs]\r\n        _test_sets.append((x_test, y_test))\r\n        _prob_lst = []\r\n        for counter in _counters:\r\n            _sum = sum(counter.values())\r\n            _prob_lst.append({\r\n                key: value / _sum for key, value in counter.items()\r\n            })\r\n            _prob_lst[-1][""null""] = _sum * 2 ** power\r\n            _prob_lst[-1][""prior""] = _sum / _total\r\n        _prob_lists.append(_prob_lst)\r\n    return _test_sets, _prob_lists\r\n\r\n\r\ndef test(test_sets, prob_lists):\r\n    acc_lst = []\r\n    for i in range(10):\r\n        _prob_lst = prob_lists[i]\r\n        x_test, y_test = test_sets[i]\r\n        y_pred = np.array([pick_best(sentence, _prob_lst) for sentence in x_test])\r\n        y_test = np.array(y_test)\r\n        acc_lst.append(100 * np.sum(y_pred == y_test) / len(y_pred))\r\n    return acc_lst\r\n\r\nif __name__ == \'__main__\':\r\n    _rs, epoch = [], 10\r\n    bar = ProgressBar(max_value=epoch, name=""_NB"")\r\n    for _ in range(epoch):\r\n        _rs.append(test(*train()))\r\n        bar.update()\r\n    _rs = np.array(_rs).T\r\n    # x_base = np.arange(len(_rs[0])) + 1\r\n    # plt.figure()\r\n    # for _acc_lst in _rs:\r\n    #     plt.plot(x_base, _acc_lst)\r\n    # plt.plot(x_base, np.average(_rs, axis=0), linewidth=4, label=""Average"")\r\n    # plt.xlim(1, epoch)\r\n    # plt.ylim(np.min(_rs), np.max(_rs)+2)\r\n    # plt.legend(loc=""lower right"")\r\n    # plt.show()\r\n    plt.figure()\r\n    plt.boxplot(_rs.T, vert=False, showmeans=True)\r\n    plt.show()\r\n    _rs = np.array(_rs).ravel()\r\n    print(""Acc Mean     : {:8.6}"".format(np.average(_rs)))\r\n    print(""Acc Variance : {:8.6}"".format(np.average((_rs - np.average(_rs)) ** 2)))\r\n\r\n    sets, lists = train()\r\n    acc_list = test(sets, lists)\r\n    idx = np.argmax(acc_list)  # type: int\r\n    lst_, (x_, y_) = lists[idx], sets[idx]\r\n    print(metrics.classification_report(y_, [\r\n        pick_best(sentence, lst_) for sentence in x_\r\n    ], target_names=np.load(os.path.join(""_Data"", ""LABEL_DIC.npy""))))\r\n\r\n    print(""Done"")\r\n'"
b_NaiveBayes/Original/Basic.py,0,"b'import os\r\nimport sys\r\nroot_path = os.path.abspath(""../../"")\r\nif root_path not in sys.path:\r\n    sys.path.append(root_path)\r\n\r\nimport numpy as np\r\nfrom math import pi, exp\r\n\r\nfrom Util.Timing import Timing\r\nfrom Util.Bases import ClassifierBase\r\n\r\nsqrt_pi = (2 * pi) ** 0.5\r\n\r\n\r\nclass NBFunctions:\r\n    @staticmethod\r\n    def gaussian(x, mu, sigma):\r\n        return exp(-(x - mu) ** 2 / (2 * sigma ** 2)) / (sqrt_pi * sigma)\r\n\r\n    @staticmethod\r\n    def gaussian_maximum_likelihood(labelled_x, n_category, dim):\r\n\r\n        mu = [np.sum(\r\n            labelled_x[c][dim]) / len(labelled_x[c][dim]) for c in range(n_category)]\r\n        sigma = [np.sum(\r\n            (labelled_x[c][dim] - mu[c]) ** 2) / len(labelled_x[c][dim]) for c in range(n_category)]\r\n\r\n        def func(_c):\r\n            def sub(x):\r\n                return NBFunctions.gaussian(x, mu[_c], sigma[_c])\r\n            return sub\r\n\r\n        return [func(_c=c) for c in range(n_category)]\r\n\r\n\r\nclass NaiveBayes(ClassifierBase):\r\n    NaiveBayesTiming = Timing()\r\n\r\n    def __init__(self, **kwargs):\r\n        super(NaiveBayes, self).__init__(**kwargs)\r\n        self._x = self._y = None\r\n        self._data = self._func = None\r\n        self._n_possibilities = None\r\n        self._labelled_x = self._label_zip = None\r\n        self._cat_counter = self._con_counter = None\r\n        self.label_dict = self._feat_dicts = None\r\n\r\n        self._params[""lb""] = kwargs.get(""lb"", 1)\r\n\r\n    def feed_data(self, x, y, sample_weight=None):\r\n        pass\r\n\r\n    def feed_sample_weight(self, sample_weight=None):\r\n        pass\r\n\r\n    @NaiveBayesTiming.timeit(level=2, prefix=""[API] "")\r\n    def get_prior_probability(self, lb=1):\r\n        return [(c_num + lb) / (len(self._y) + lb * len(self._cat_counter))\r\n                for c_num in self._cat_counter]\r\n\r\n    @NaiveBayesTiming.timeit(level=2, prefix=""[API] "")\r\n    def fit(self, x=None, y=None, sample_weight=None, lb=None):\r\n        if sample_weight is None:\r\n            sample_weight = self._params[""sample_weight""]\r\n        if lb is None:\r\n            lb = self._params[""lb""]\r\n        if x is not None and y is not None:\r\n            self.feed_data(x, y, sample_weight)\r\n        self._func = self._fit(lb)\r\n\r\n    def _fit(self, lb):\r\n        pass\r\n\r\n    @NaiveBayesTiming.timeit(level=1, prefix=""[API] "")\r\n    def predict_one(self, x, get_raw_result=False):\r\n        if type(x) is np.ndarray:\r\n            x = x.tolist()\r\n        else:\r\n            x = x[:]\r\n        x = self._transfer_x(x)\r\n        m_arg, m_probability = 0, 0\r\n        for i in range(len(self._cat_counter)):\r\n            p = self._func(x, i)\r\n            if p > m_probability:\r\n                m_arg, m_probability = i, p\r\n        if not get_raw_result:\r\n            return self.label_dict[m_arg]\r\n        return m_probability\r\n\r\n    @NaiveBayesTiming.timeit(level=3, prefix=""[API] "")\r\n    def predict(self, x, get_raw_result=False, **kwargs):\r\n        return np.array([self.predict_one(xx, get_raw_result) for xx in x])\r\n\r\n    def _transfer_x(self, x):\r\n        return x\r\n'"
b_NaiveBayes/Original/GaussianNB.py,0,"b'import os\r\nimport sys\r\nroot_path = os.path.abspath(""../../"")\r\nif root_path not in sys.path:\r\n    sys.path.append(root_path)\r\n\r\nimport matplotlib.pyplot as plt\r\n\r\nfrom b_NaiveBayes.Original.Basic import *\r\nfrom b_NaiveBayes.Original.MultinomialNB import MultinomialNB\r\n\r\nfrom Util.Util import DataUtil\r\n\r\n\r\nclass GaussianNB(NaiveBayes):\r\n    GaussianNBTiming = Timing()\r\n\r\n    @GaussianNBTiming.timeit(level=1, prefix=""[API] "")\r\n    def feed_data(self, x, y, sample_weight=None):\r\n        if sample_weight is not None:\r\n            sample_weight = np.asarray(sample_weight)\r\n        x = np.array([list(map(lambda c: float(c), sample)) for sample in x])\r\n        labels = list(set(y))\r\n        label_dict = {label: i for i, label in enumerate(labels)}\r\n        y = np.array([label_dict[yy] for yy in y])\r\n        cat_counter = np.bincount(y)\r\n        labels = [y == value for value in range(len(cat_counter))]\r\n        labelled_x = [x[label].T for label in labels]\r\n\r\n        self._x, self._y = x.T, y\r\n        self._labelled_x, self._label_zip = labelled_x, labels\r\n        self._cat_counter, self.label_dict = cat_counter, {i: l for l, i in label_dict.items()}\r\n        self.feed_sample_weight(sample_weight)\r\n\r\n    @GaussianNBTiming.timeit(level=1, prefix=""[Core] "")\r\n    def feed_sample_weight(self, sample_weight=None):\r\n        if sample_weight is not None:\r\n            local_weights = sample_weight * len(sample_weight)\r\n            for i, label in enumerate(self._label_zip):\r\n                self._labelled_x[i] *= local_weights[label]\r\n\r\n    @GaussianNBTiming.timeit(level=1, prefix=""[Core] "")\r\n    def _fit(self, lb):\r\n        n_category = len(self._cat_counter)\r\n        p_category = self.get_prior_probability(lb)\r\n        data = [\r\n            NBFunctions.gaussian_maximum_likelihood(\r\n                self._labelled_x, n_category, dim) for dim in range(len(self._x))]\r\n        self._data = data\r\n\r\n        def func(input_x, tar_category):\r\n            rs = 1\r\n            for d, xx in enumerate(input_x):\r\n                rs *= data[d][tar_category](xx)\r\n            return rs * p_category[tar_category]\r\n\r\n        return func\r\n\r\n    def visualize(self, save=False):\r\n        colors = plt.cm.Paired([i / len(self.label_dict) for i in range(len(self.label_dict))])\r\n        colors = {cat: color for cat, color in zip(self.label_dict.values(), colors)}\r\n        for j in range(len(self._x)):\r\n            tmp_data = self._x[j]\r\n            x_min, x_max = np.min(tmp_data), np.max(tmp_data)\r\n            gap = x_max - x_min\r\n            tmp_x = np.linspace(x_min-0.1*gap, x_max+0.1*gap, 200)\r\n            title = ""$j = {}$"".format(j + 1)\r\n            plt.figure()\r\n            plt.title(title)\r\n            for c in range(len(self.label_dict)):\r\n                plt.plot(tmp_x, [self._data[j][c](xx) for xx in tmp_x],\r\n                         c=colors[self.label_dict[c]], label=""class: {}"".format(self.label_dict[c]))\r\n            plt.xlim(x_min-0.2*gap, x_max+0.2*gap)\r\n            plt.legend()\r\n            if not save:\r\n                plt.show()\r\n            else:\r\n                plt.savefig(""d{}"".format(j + 1))\r\n\r\nif __name__ == \'__main__\':\r\n    import time\r\n\r\n    xs, ys = DataUtil.get_dataset(""mushroom"", ""../../_Data/mushroom.txt"", tar_idx=0)\r\n    nb = MultinomialNB()\r\n    nb.feed_data(xs, ys)\r\n    xs, ys = nb[""x""].tolist(), nb[""y""].tolist()\r\n\r\n    train_num = 6000\r\n    x_train, x_test = xs[:train_num], xs[train_num:]\r\n    y_train, y_test = ys[:train_num], ys[train_num:]\r\n\r\n    learning_time = time.time()\r\n    nb = GaussianNB()\r\n    nb.fit(x_train, y_train)\r\n    learning_time = time.time() - learning_time\r\n\r\n    estimation_time = time.time()\r\n    nb.evaluate(x_train, y_train)\r\n    nb.evaluate(x_test, y_test)\r\n    estimation_time = time.time() - estimation_time\r\n\r\n    print(\r\n        ""Model building  : {:12.6} s\\n""\r\n        ""Estimation      : {:12.6} s\\n""\r\n        ""Total           : {:12.6} s"".format(\r\n            learning_time, estimation_time,\r\n            learning_time + estimation_time\r\n        )\r\n    )\r\n    nb.show_timing_log()\r\n    nb.visualize()\r\n'"
b_NaiveBayes/Original/MergedNB.py,0,"b'import os\r\nimport sys\r\nroot_path = os.path.abspath(""../../"")\r\nif root_path not in sys.path:\r\n    sys.path.append(root_path)\r\n\r\nfrom b_NaiveBayes.Original.Basic import *\r\nfrom b_NaiveBayes.Original.MultinomialNB import MultinomialNB\r\nfrom b_NaiveBayes.Original.GaussianNB import GaussianNB\r\n\r\nfrom Util.Util import DataUtil\r\nfrom Util.Timing import Timing\r\n\r\n\r\nclass MergedNB(NaiveBayes):\r\n    MergedNBTiming = Timing()\r\n\r\n    def __init__(self, **kwargs):\r\n        super(MergedNB, self).__init__(**kwargs)\r\n        self._multinomial, self._gaussian = MultinomialNB(), GaussianNB()\r\n\r\n        wc = kwargs.get(""whether_continuous"")\r\n        if wc is None:\r\n            self._whether_discrete = self._whether_continuous = None\r\n        else:\r\n            self._whether_continuous = np.asarray(wc)\r\n            self._whether_discrete = ~self._whether_continuous\r\n\r\n    @MergedNBTiming.timeit(level=1, prefix=""[API] "")\r\n    def feed_data(self, x, y, sample_weight=None):\r\n        if sample_weight is not None:\r\n            sample_weight = np.asarray(sample_weight)\r\n        x, y, wc, features, feat_dicts, label_dict = DataUtil.quantize_data(\r\n            x, y, wc=self._whether_continuous, separate=True)\r\n        if self._whether_continuous is None:\r\n            self._whether_continuous = wc\r\n            self._whether_discrete = ~self._whether_continuous\r\n        self.label_dict = label_dict\r\n        discrete_x, continuous_x = x\r\n        cat_counter = np.bincount(y)\r\n        self._cat_counter = cat_counter\r\n        labels = [y == value for value in range(len(cat_counter))]\r\n\r\n        labelled_x = [discrete_x[ci].T for ci in labels]\r\n        self._multinomial._x, self._multinomial._y = x, y\r\n        self._multinomial._labelled_x, self._multinomial._label_zip = labelled_x, list(zip(labels, labelled_x))\r\n        self._multinomial._cat_counter = cat_counter\r\n        self._multinomial._feat_dicts = [dic for i, dic in enumerate(feat_dicts) if self._whether_discrete[i]]\r\n        self._multinomial._n_possibilities = [len(feats) for i, feats in enumerate(features)\r\n                                              if self._whether_discrete[i]]\r\n        self._multinomial.label_dict = label_dict\r\n\r\n        labelled_x = [continuous_x[label].T for label in labels]\r\n        self._gaussian._x, self._gaussian._y = continuous_x.T, y\r\n        self._gaussian._labelled_x, self._gaussian._label_zip = labelled_x, labels\r\n        self._gaussian._cat_counter, self._gaussian.label_dict = cat_counter, label_dict\r\n\r\n        self.feed_sample_weight(sample_weight)\r\n\r\n    @MergedNBTiming.timeit(level=1, prefix=""[Core] "")\r\n    def feed_sample_weight(self, sample_weight=None):\r\n        self._multinomial.feed_sample_weight(sample_weight)\r\n        self._gaussian.feed_sample_weight(sample_weight)\r\n\r\n    @MergedNBTiming.timeit(level=1, prefix=""[Core] "")\r\n    def _fit(self, lb):\r\n        self._multinomial.fit()\r\n        self._gaussian.fit()\r\n        p_category = self._multinomial.get_prior_probability(lb)\r\n        discrete_func, continuous_func = self._multinomial[""func""], self._gaussian[""func""]\r\n\r\n        def func(input_x, tar_category):\r\n            input_x = np.asarray(input_x)\r\n            return discrete_func(\r\n                input_x[self._whether_discrete].astype(np.int), tar_category) * continuous_func(\r\n                input_x[self._whether_continuous], tar_category) / p_category[tar_category]\r\n\r\n        return func\r\n\r\n    @MergedNBTiming.timeit(level=1, prefix=""[Core] "")\r\n    def _transfer_x(self, x):\r\n        feat_dicts = self._multinomial[""feat_dicts""]\r\n        idx = 0\r\n        for d, discrete in enumerate(self._whether_discrete):\r\n            if not discrete:\r\n                x[d] = float(x[d])\r\n            else:\r\n                x[d] = feat_dicts[idx][x[d]]\r\n            if discrete:\r\n                idx += 1\r\n        return x\r\n\r\nif __name__ == \'__main__\':\r\n    import time\r\n\r\n    # whether_discrete = [True, False, True, True]\r\n    # x = DataUtil.get_dataset(""balloon2.0"", ""../../_Data/{}.txt"".format(""balloon2.0""))\r\n    # y = [xx.pop() for xx in x]\r\n    # learning_time = time.time()\r\n    # nb = MergedNB(whether_discrete)\r\n    # nb.fit(x, y)\r\n    # learning_time = time.time() - learning_time\r\n    # estimation_time = time.time()\r\n    # nb.evaluate(x, y)\r\n    # estimation_time = time.time() - estimation_time\r\n    # print(\r\n    #     ""Model building  : {:12.6} s\\n""\r\n    #     ""Estimation      : {:12.6} s\\n""\r\n    #     ""Total           : {:12.6} s"".format(\r\n    #         learning_time, estimation_time,\r\n    #         learning_time + estimation_time\r\n    #     )\r\n    # )\r\n\r\n    whether_continuous = [False] * 16\r\n    continuous_lst = [0, 5, 9, 11, 12, 13, 14]\r\n    for cl in continuous_lst:\r\n        whether_continuous[cl] = True\r\n\r\n    train_num = 40000\r\n    data_time = time.time()\r\n    (x_train, y_train), (x_test, y_test) = DataUtil.get_dataset(\r\n        ""bank1.0"", ""../../_Data/bank1.0.txt"", n_train=train_num)\r\n    data_time = time.time() - data_time\r\n    learning_time = time.time()\r\n    nb = MergedNB(whether_continuous=whether_continuous)\r\n    nb.fit(x_train, y_train)\r\n    learning_time = time.time() - learning_time\r\n    estimation_time = time.time()\r\n    nb.evaluate(x_train, y_train)\r\n    nb.evaluate(x_test, y_test)\r\n    estimation_time = time.time() - estimation_time\r\n    print(\r\n        ""Data cleaning   : {:12.6} s\\n""\r\n        ""Model building  : {:12.6} s\\n""\r\n        ""Estimation      : {:12.6} s\\n""\r\n        ""Total           : {:12.6} s"".format(\r\n            data_time, learning_time, estimation_time,\r\n            data_time + learning_time + estimation_time\r\n        )\r\n    )\r\n'"
b_NaiveBayes/Original/MultinomialNB.py,0,"b'import os\r\nimport sys\r\nroot_path = os.path.abspath(""../../"")\r\nif root_path not in sys.path:\r\n    sys.path.append(root_path)\r\n\r\nimport matplotlib.pyplot as plt\r\n\r\nfrom b_NaiveBayes.Original.Basic import *\r\n\r\nfrom Util.Util import DataUtil\r\n\r\n\r\nclass MultinomialNB(NaiveBayes):\r\n    MultinomialNBTiming = Timing()\r\n\r\n    @MultinomialNBTiming.timeit(level=1, prefix=""[API] "")\r\n    def feed_data(self, x, y, sample_weight=None):\r\n        if sample_weight is not None:\r\n            sample_weight = np.asarray(sample_weight)\r\n        x, y, _, features, feat_dicts, label_dict = DataUtil.quantize_data(x, y, wc=np.array([False] * len(x[0])))\r\n        cat_counter = np.bincount(y)\r\n        n_possibilities = [len(feats) for feats in features]\r\n        labels = [y == value for value in range(len(cat_counter))]\r\n        labelled_x = [x[ci].T for ci in labels]\r\n\r\n        self._x, self._y = x, y\r\n        self._labelled_x, self._label_zip = labelled_x, list(zip(labels, labelled_x))\r\n        self._cat_counter, self._feat_dicts, self._n_possibilities = cat_counter, feat_dicts, n_possibilities\r\n        self.label_dict = label_dict\r\n        self.feed_sample_weight(sample_weight)\r\n\r\n    @MultinomialNBTiming.timeit(level=1, prefix=""[Core] "")\r\n    def feed_sample_weight(self, sample_weight=None):\r\n        self._con_counter = []\r\n        for dim, p in enumerate(self._n_possibilities):\r\n            if sample_weight is None:\r\n                self._con_counter.append([\r\n                    np.bincount(xx[dim], minlength=p) for xx in self._labelled_x])\r\n            else:\r\n                local_weights = sample_weight * len(sample_weight)\r\n                self._con_counter.append([\r\n                    np.bincount(xx[dim], weights=local_weights[label], minlength=p)\r\n                    for label, xx in self._label_zip])\r\n\r\n    @MultinomialNBTiming.timeit(level=1, prefix=""[Core] "")\r\n    def _fit(self, lb):\r\n        n_dim = len(self._n_possibilities)\r\n        n_category = len(self._cat_counter)\r\n        p_category = self.get_prior_probability(lb)\r\n\r\n        data = [[] for _ in range(n_dim)]\r\n        for dim, n_possibilities in enumerate(self._n_possibilities):\r\n            data[dim] = [\r\n                [(self._con_counter[dim][c][p] + lb) / (self._cat_counter[c] + lb * n_possibilities)\r\n                 for p in range(n_possibilities)] for c in range(n_category)]\r\n        self._data = [np.asarray(dim_info) for dim_info in data]\r\n\r\n        def func(input_x, tar_category):\r\n            rs = 1\r\n            for d, xx in enumerate(input_x):\r\n                rs *= data[d][tar_category][xx]\r\n            return rs * p_category[tar_category]\r\n\r\n        return func\r\n\r\n    @MultinomialNBTiming.timeit(level=1, prefix=""[Core] "")\r\n    def _transfer_x(self, x):\r\n        for j, char in enumerate(x):\r\n            x[j] = self._feat_dicts[j][char]\r\n        return x\r\n\r\n    def visualize(self, save=False):\r\n        colors = plt.cm.Paired([i / len(self.label_dict) for i in range(len(self.label_dict))])\r\n        colors = {cat: color for cat, color in zip(self.label_dict.values(), colors)}\r\n        rev_feat_dicts = [{val: key for key, val in feat_dict.items()} for feat_dict in self._feat_dicts]\r\n        for j in range(len(self._n_possibilities)):\r\n            rev_dict = rev_feat_dicts[j]\r\n            sj = self._n_possibilities[j]\r\n            tmp_x = np.arange(1, sj + 1)\r\n            title = ""$j = {}; S_j = {}$"".format(j + 1, sj)\r\n            plt.figure()\r\n            plt.title(title)\r\n            for c in range(len(self.label_dict)):\r\n                plt.bar(tmp_x - 0.35 * c, self._data[j][c, :], width=0.35,\r\n                        facecolor=colors[self.label_dict[c]], edgecolor=""white"",\r\n                        label=u""class: {}"".format(self.label_dict[c]))\r\n            plt.xticks([i for i in range(sj + 2)], [""""] + [rev_dict[i] for i in range(sj)] + [""""])\r\n            plt.ylim(0, 1.0)\r\n            plt.legend()\r\n            if not save:\r\n                plt.show()\r\n            else:\r\n                plt.savefig(""d{}"".format(j + 1))\r\n\r\nif __name__ == \'__main__\':\r\n    import time\r\n\r\n    for dataset in (""balloon1.0"", ""balloon1.5""):\r\n        _x, _y = DataUtil.get_dataset(dataset, ""../../_Data/{}.txt"".format(dataset))\r\n        learning_time = time.time()\r\n        nb = MultinomialNB()\r\n        nb.fit(_x, _y)\r\n        learning_time = time.time() - learning_time\r\n        print(""="" * 30)\r\n        print(dataset)\r\n        print(""-"" * 30)\r\n        estimation_time = time.time()\r\n        nb.evaluate(_x, _y)\r\n        estimation_time = time.time() - estimation_time\r\n        print(\r\n            ""Model building  : {:12.6} s\\n""\r\n            ""Estimation      : {:12.6} s\\n""\r\n            ""Total           : {:12.6} s"".format(\r\n                learning_time, estimation_time,\r\n                learning_time + estimation_time\r\n            )\r\n        )\r\n\r\n    print(""="" * 30)\r\n    print(""mushroom"")\r\n    print(""-"" * 30)\r\n\r\n    train_num = 6000\r\n    (x_train, y_train), (x_test, y_test) = DataUtil.get_dataset(\r\n        ""mushroom"", ""../../_Data/mushroom.txt"", n_train=train_num, tar_idx=0)\r\n\r\n    learning_time = time.time()\r\n    nb = MultinomialNB()\r\n    nb.fit(x_train, y_train)\r\n    learning_time = time.time() - learning_time\r\n\r\n    estimation_time = time.time()\r\n    nb.evaluate(x_train, y_train)\r\n    nb.evaluate(x_test, y_test)\r\n    estimation_time = time.time() - estimation_time\r\n\r\n    print(\r\n        ""Model building  : {:12.6} s\\n""\r\n        ""Estimation      : {:12.6} s\\n""\r\n        ""Total           : {:12.6} s"".format(\r\n            learning_time, estimation_time,\r\n            learning_time + estimation_time\r\n        )\r\n    )\r\n    nb.show_timing_log()\r\n    nb.visualize()\r\n'"
b_NaiveBayes/Vectorized/Basic.py,0,"b'import os\r\nimport sys\r\nroot_path = os.path.abspath(""../../"")\r\nif root_path not in sys.path:\r\n    sys.path.append(root_path)\r\n\r\nimport numpy as np\r\nfrom math import pi\r\n\r\nfrom Util.Timing import Timing\r\nfrom Util.Bases import ClassifierBase\r\n\r\nsqrt_pi = (2 * pi) ** 0.5\r\n\r\n\r\nclass NBFunctions:\r\n    @staticmethod\r\n    def gaussian(x, mu, sigma):\r\n        return np.exp(-(x - mu) ** 2 / (2 * sigma ** 2)) / (sqrt_pi * sigma)\r\n\r\n    @staticmethod\r\n    def gaussian_maximum_likelihood(labelled_x, n_category, dim):\r\n\r\n        mu = [np.sum(\r\n            labelled_x[c][dim]) / len(labelled_x[c][dim]) for c in range(n_category)]\r\n        sigma = [np.sum(\r\n            (labelled_x[c][dim] - mu[c]) ** 2) / len(labelled_x[c][dim]) for c in range(n_category)]\r\n\r\n        def func(_c):\r\n            def sub(x):\r\n                return NBFunctions.gaussian(x, mu[_c], sigma[_c])\r\n            return sub\r\n\r\n        return [func(_c=c) for c in range(n_category)]\r\n\r\n\r\nclass NaiveBayes(ClassifierBase):\r\n    NaiveBayesTiming = Timing()\r\n\r\n    def __init__(self, **kwargs):\r\n        super(NaiveBayes, self).__init__(**kwargs)\r\n        self._x = self._y = self._data = None\r\n        self._n_possibilities = self._p_category = None\r\n        self._labelled_x = self._label_zip = None\r\n        self._cat_counter = self._con_counter = None\r\n        self.label_dict = self._feat_dicts = None\r\n\r\n        self._params[""lb""] = kwargs.get(""lb"", 1)\r\n\r\n    def feed_data(self, x, y, sample_weight=None):\r\n        pass\r\n\r\n    def feed_sample_weight(self, sample_weight=None):\r\n        pass\r\n\r\n    @NaiveBayesTiming.timeit(level=2, prefix=""[API] "")\r\n    def get_prior_probability(self, lb=1):\r\n        return [(c_num + lb) / (len(self._y) + lb * len(self._cat_counter))\r\n                for c_num in self._cat_counter]\r\n\r\n    @NaiveBayesTiming.timeit(level=2, prefix=""[API] "")\r\n    def fit(self, x=None, y=None, sample_weight=None, lb=None):\r\n        if sample_weight is None:\r\n            sample_weight = self._params[""sample_weight""]\r\n        if lb is None:\r\n            lb = self._params[""lb""]\r\n        if x is not None and y is not None:\r\n            self.feed_data(x, y, sample_weight)\r\n        self._fit(lb)\r\n\r\n    def _fit(self, lb):\r\n        pass\r\n\r\n    def _func(self, x, i):\r\n        pass\r\n\r\n    @NaiveBayesTiming.timeit(level=1, prefix=""[API] "")\r\n    def predict(self, x, get_raw_result=False, **kwargs):\r\n        if isinstance(x, np.ndarray):\r\n            x = x.tolist()\r\n        else:\r\n            x = [xx[:] for xx in x]\r\n        x = self._transfer_x(x)\r\n        m_arg, m_probability = np.zeros(len(x), dtype=np.int8), np.zeros(len(x))\r\n        for i in range(len(self._cat_counter)):\r\n            p = self._func(x, i)\r\n            mask = p > m_probability\r\n            m_arg[mask], m_probability[mask] = i, p[mask]\r\n        if not get_raw_result:\r\n            return np.array([self.label_dict[arg] for arg in m_arg])\r\n        return m_probability\r\n\r\n    def _transfer_x(self, x):\r\n        return x\r\n'"
b_NaiveBayes/Vectorized/GaussianNB.py,0,"b'import os\r\nimport sys\r\nroot_path = os.path.abspath(""../../"")\r\nif root_path not in sys.path:\r\n    sys.path.append(root_path)\r\n\r\nimport matplotlib.pyplot as plt\r\n\r\nfrom b_NaiveBayes.Vectorized.Basic import *\r\n\r\nfrom Util.Timing import Timing\r\n\r\n\r\nclass GaussianNB(NaiveBayes):\r\n    GaussianNBTiming = Timing()\r\n\r\n    @GaussianNBTiming.timeit(level=1, prefix=""[API] "")\r\n    def feed_data(self, x, y, sample_weight=None):\r\n        if sample_weight is not None:\r\n            sample_weight = np.asarray(sample_weight)\r\n        x = np.array([list(map(lambda c: float(c), sample)) for sample in x])\r\n        labels = list(set(y))\r\n        label_dict = {label: i for i, label in enumerate(labels)}\r\n        y = np.array([label_dict[yy] for yy in y])\r\n        cat_counter = np.bincount(y)\r\n        labels = [y == value for value in range(len(cat_counter))]\r\n        labelled_x = [x[label].T for label in labels]\r\n\r\n        self._x, self._y = x.T, y\r\n        self._labelled_x, self._label_zip = labelled_x, labels\r\n        self._cat_counter, self.label_dict = cat_counter, {i: l for l, i in label_dict.items()}\r\n        self.feed_sample_weight(sample_weight)\r\n\r\n    @GaussianNBTiming.timeit(level=1, prefix=""[Core] "")\r\n    def feed_sample_weight(self, sample_weight=None):\r\n        if sample_weight is not None:\r\n            local_weights = sample_weight * len(sample_weight)\r\n            for i, label in enumerate(self._label_zip):\r\n                self._labelled_x[i] *= local_weights[label]\r\n\r\n    @GaussianNBTiming.timeit(level=1, prefix=""[Core] "")\r\n    def _fit(self, lb):\r\n        lb = 0\r\n        n_category = len(self._cat_counter)\r\n        self._p_category = self.get_prior_probability(lb)\r\n        data = [\r\n            NBFunctions.gaussian_maximum_likelihood(\r\n                self._labelled_x, n_category, dim) for dim in range(len(self._x))]\r\n        self._data = data\r\n\r\n    @GaussianNBTiming.timeit(level=1, prefix=""[Core] "")\r\n    def _func(self, x, i):\r\n        x = np.atleast_2d(x).T\r\n        rs = np.ones(x.shape[1])\r\n        for d, xx in enumerate(x):\r\n            rs *= self._data[d][i](xx)\r\n        return rs * self._p_category[i]\r\n\r\n    def visualize(self, save=False):\r\n        colors = plt.cm.Paired([i / len(self.label_dict) for i in range(len(self.label_dict))])\r\n        colors = {cat: color for cat, color in zip(self.label_dict.values(), colors)}\r\n        for j in range(len(self._x)):\r\n            tmp_data = self._x[j]\r\n            x_min, x_max = np.min(tmp_data), np.max(tmp_data)\r\n            gap = x_max - x_min\r\n            tmp_x = np.linspace(x_min-0.1*gap, x_max+0.1*gap, 200)\r\n            title = ""$j = {}$"".format(j + 1)\r\n            plt.figure()\r\n            plt.title(title)\r\n            for c in range(len(self.label_dict)):\r\n                plt.plot(tmp_x, self._data[j][c](tmp_x),\r\n                         c=colors[self.label_dict[c]], label=""class: {}"".format(self.label_dict[c]))\r\n            plt.xlim(x_min-0.2*gap, x_max+0.2*gap)\r\n            plt.legend()\r\n            if not save:\r\n                plt.show()\r\n            else:\r\n                plt.savefig(""d{}"".format(j + 1))\r\n'"
b_NaiveBayes/Vectorized/MergedNB.py,0,"b'import os\r\nimport sys\r\nroot_path = os.path.abspath(""../../"")\r\nif root_path not in sys.path:\r\n    sys.path.append(root_path)\r\n\r\nfrom b_NaiveBayes.Vectorized.Basic import *\r\nfrom b_NaiveBayes.Vectorized.MultinomialNB import MultinomialNB\r\nfrom b_NaiveBayes.Vectorized.GaussianNB import GaussianNB\r\n\r\nfrom Util.Util import DataUtil\r\nfrom Util.Timing import Timing\r\n\r\n\r\nclass MergedNB(NaiveBayes):\r\n    MergedNBTiming = Timing()\r\n\r\n    def __init__(self, **kwargs):\r\n        super(MergedNB, self).__init__(**kwargs)\r\n        self._multinomial, self._gaussian = MultinomialNB(), GaussianNB()\r\n\r\n        wc = kwargs.get(""whether_continuous"")\r\n        if wc is None:\r\n            self._whether_discrete = self._whether_continuous = None\r\n        else:\r\n            self._whether_continuous = np.asarray(wc)\r\n            self._whether_discrete = ~self._whether_continuous\r\n\r\n    @MergedNBTiming.timeit(level=1, prefix=""[API] "")\r\n    def feed_data(self, x, y, sample_weight=None):\r\n        if sample_weight is not None:\r\n            sample_weight = np.asarray(sample_weight)\r\n        x, y, wc, features, feat_dicts, label_dict = DataUtil.quantize_data(\r\n            x, y, wc=self._whether_continuous, separate=True)\r\n        if self._whether_continuous is None:\r\n            self._whether_continuous = wc\r\n            self._whether_discrete = ~self._whether_continuous\r\n        self.label_dict = label_dict\r\n\r\n        discrete_x, continuous_x = x\r\n\r\n        cat_counter = np.bincount(y)\r\n        self._cat_counter = cat_counter\r\n\r\n        labels = [y == value for value in range(len(cat_counter))]\r\n        labelled_x = [discrete_x[ci].T for ci in labels]\r\n\r\n        self._multinomial._x, self._multinomial._y = x, y\r\n        self._multinomial._labelled_x, self._multinomial._label_zip = labelled_x, list(zip(labels, labelled_x))\r\n        self._multinomial._cat_counter = cat_counter\r\n        self._multinomial._feat_dicts = [dic for i, dic in enumerate(feat_dicts) if self._whether_discrete[i]]\r\n        self._multinomial._n_possibilities = [len(feats) for i, feats in enumerate(features)\r\n                                              if self._whether_discrete[i]]\r\n        self._multinomial.label_dict = label_dict\r\n\r\n        labelled_x = [continuous_x[label].T for label in labels]\r\n\r\n        self._gaussian._x, self._gaussian._y = continuous_x.T, y\r\n        self._gaussian._labelled_x, self._gaussian._label_zip = labelled_x, labels\r\n        self._gaussian._cat_counter, self._gaussian.label_dict = cat_counter, label_dict\r\n\r\n        self.feed_sample_weight(sample_weight)\r\n\r\n    @MergedNBTiming.timeit(level=1, prefix=""[Core] "")\r\n    def feed_sample_weight(self, sample_weight=None):\r\n        self._multinomial.feed_sample_weight(sample_weight)\r\n        self._gaussian.feed_sample_weight(sample_weight)\r\n\r\n    @MergedNBTiming.timeit(level=1, prefix=""[Core] "")\r\n    def _fit(self, lb):\r\n        self._multinomial.fit()\r\n        self._gaussian.fit()\r\n        self._p_category = self._multinomial[""p_category""]\r\n\r\n    @MergedNBTiming.timeit(level=1, prefix=""[Core] "")\r\n    def _func(self, x, i):\r\n        x = np.atleast_2d(x)\r\n        return self._multinomial[""func""](\r\n            x[:, self._whether_discrete].astype(np.int), i) * self._gaussian[""func""](\r\n            x[:, self._whether_continuous], i) / self._p_category[i]\r\n\r\n    @MergedNBTiming.timeit(level=1, prefix=""[Core] "")\r\n    def _transfer_x(self, x):\r\n        feat_dicts = self._multinomial[""feat_dicts""]\r\n        idx = 0\r\n        for d, discrete in enumerate(self._whether_discrete):\r\n            for i, sample in enumerate(x):\r\n                if not discrete:\r\n                    x[i][d] = float(x[i][d])\r\n                else:\r\n                    x[i][d] = feat_dicts[idx][sample[d]]\r\n            if discrete:\r\n                idx += 1\r\n        return x\r\n\r\nif __name__ == \'__main__\':\r\n    import time\r\n\r\n    whether_continuous = [False] * 16\r\n    continuous_lst = [0, 5, 9, 11, 12, 13, 14]\r\n    for cl in continuous_lst:\r\n        whether_continuous[cl] = True\r\n\r\n    train_num = 40000\r\n\r\n    data_time = time.time()\r\n    (x_train, y_train), (x_test, y_test) = DataUtil.get_dataset(\r\n        ""bank1.0"", ""../../_Data/bank1.0.txt"", n_train=train_num)\r\n    data_time = time.time() - data_time\r\n\r\n    learning_time = time.time()\r\n    nb = MergedNB(whether_continuous=whether_continuous)\r\n    nb.fit(x_train, y_train)\r\n    learning_time = time.time() - learning_time\r\n\r\n    estimation_time = time.time()\r\n    nb.evaluate(x_train, y_train)\r\n    nb.evaluate(x_test, y_test)\r\n    estimation_time = time.time() - estimation_time\r\n\r\n    print(\r\n        ""Data cleaning   : {:12.6} s\\n""\r\n        ""Model building  : {:12.6} s\\n""\r\n        ""Estimation      : {:12.6} s\\n""\r\n        ""Total           : {:12.6} s"".format(\r\n            data_time, learning_time, estimation_time,\r\n            data_time + learning_time + estimation_time\r\n        )\r\n    )\r\n    nb.show_timing_log()\r\n    nb[""multinomial""].visualize()\r\n    nb[""gaussian""].visualize()\r\n'"
b_NaiveBayes/Vectorized/MultinomialNB.py,0,"b'import os\r\nimport sys\r\nroot_path = os.path.abspath(""../../"")\r\nif root_path not in sys.path:\r\n    sys.path.append(root_path)\r\n\r\nimport matplotlib.pyplot as plt\r\n\r\nfrom b_NaiveBayes.Vectorized.Basic import *\r\n\r\nfrom Util.Util import DataUtil\r\nfrom Util.Timing import Timing\r\n\r\n\r\nclass MultinomialNB(NaiveBayes):\r\n    MultinomialNBTiming = Timing()\r\n\r\n    @MultinomialNBTiming.timeit(level=1, prefix=""[API] "")\r\n    def feed_data(self, x, y, sample_weight=None):\r\n        if sample_weight is not None:\r\n            sample_weight = np.asarray(sample_weight)\r\n        x, y, _, features, feat_dicts, label_dict = DataUtil.quantize_data(x, y, wc=np.array([False] * len(x[0])))\r\n        cat_counter = np.bincount(y)\r\n        n_possibilities = [len(feats) for feats in features]\r\n\r\n        labels = [y == value for value in range(len(cat_counter))]\r\n        labelled_x = [x[ci].T for ci in labels]\r\n\r\n        self._x, self._y = x, y\r\n        self._labelled_x, self._label_zip = labelled_x, list(zip(labels, labelled_x))\r\n        self._cat_counter, self._feat_dicts, self._n_possibilities = cat_counter, feat_dicts, n_possibilities\r\n        self.label_dict = label_dict\r\n        self.feed_sample_weight(sample_weight)\r\n\r\n    @MultinomialNBTiming.timeit(level=1, prefix=""[Core] "")\r\n    def feed_sample_weight(self, sample_weight=None):\r\n        self._con_counter = []\r\n        for dim, p in enumerate(self._n_possibilities):\r\n            if sample_weight is None:\r\n                self._con_counter.append([\r\n                    np.bincount(xx[dim], minlength=p) for xx in self._labelled_x])\r\n            else:\r\n                self._con_counter.append([\r\n                    np.bincount(xx[dim], weights=sample_weight[label] / sample_weight[label].mean(), minlength=p)\r\n                    for label, xx in self._label_zip])\r\n\r\n    @MultinomialNBTiming.timeit(level=1, prefix=""[Core] "")\r\n    def _fit(self, lb):\r\n        n_dim = len(self._n_possibilities)\r\n        n_category = len(self._cat_counter)\r\n        self._p_category = self.get_prior_probability(lb)\r\n\r\n        data = [[] for _ in range(n_dim)]\r\n        for dim, n_possibilities in enumerate(self._n_possibilities):\r\n            data[dim] = [\r\n                [(self._con_counter[dim][c][p] + lb) / (self._cat_counter[c] + lb * n_possibilities)\r\n                 for p in range(n_possibilities)] for c in range(n_category)]\r\n        self._data = [np.asarray(dim_info) for dim_info in data]\r\n\r\n    @MultinomialNBTiming.timeit(level=1, prefix=""[Core] "")\r\n    def _func(self, x, i):\r\n        x = np.atleast_2d(x).T\r\n        rs = np.ones(x.shape[1])\r\n        for d, xx in enumerate(x):\r\n            rs *= self._data[d][i][xx]\r\n        return rs * self._p_category[i]\r\n\r\n    @MultinomialNBTiming.timeit(level=1, prefix=""[Core] "")\r\n    def _transfer_x(self, x):\r\n        for i, sample in enumerate(x):\r\n            for j, char in enumerate(sample):\r\n                x[i][j] = self._feat_dicts[j][char]\r\n        return x\r\n\r\n    def visualize(self, save=False):\r\n        colors = plt.cm.Paired([i / len(self.label_dict) for i in range(len(self.label_dict))])\r\n        colors = {cat: color for cat, color in zip(self.label_dict.values(), colors)}\r\n        rev_feat_dicts = [{val: key for key, val in feat_dict.items()} for feat_dict in self._feat_dicts]\r\n        for j in range(len(self._n_possibilities)):\r\n            rev_dict = rev_feat_dicts[j]\r\n            sj = self._n_possibilities[j]\r\n            tmp_x = np.arange(1, sj + 1)\r\n            title = ""$j = {}; S_j = {}$"".format(j + 1, sj)\r\n            plt.figure()\r\n            plt.title(title)\r\n            for c in range(len(self.label_dict)):\r\n                plt.bar(tmp_x - 0.35 * c, self._data[j][c, :], width=0.35,\r\n                        facecolor=colors[self.label_dict[c]], edgecolor=""white"",\r\n                        label=u""class: {}"".format(self.label_dict[c]))\r\n            plt.xticks([i for i in range(sj + 2)], [""""] + [rev_dict[i] for i in range(sj)] + [""""])\r\n            plt.ylim(0, 1.0)\r\n            plt.legend()\r\n            if not save:\r\n                plt.show()\r\n            else:\r\n                plt.savefig(""d{}"".format(j + 1))\r\n\r\nif __name__ == \'__main__\':\r\n    import time\r\n\r\n    train_num = 6000\r\n    (x_train, y_train), (x_test, y_test) = DataUtil.get_dataset(\r\n        ""mushroom"", ""../../_Data/mushroom.txt"", n_train=train_num, tar_idx=0)\r\n\r\n    learning_time = time.time()\r\n    nb = MultinomialNB()\r\n    nb.fit(x_train, y_train)\r\n    learning_time = time.time() - learning_time\r\n    estimation_time = time.time()\r\n    nb.evaluate(x_train, y_train)\r\n    nb.evaluate(x_test, y_test)\r\n    estimation_time = time.time() - estimation_time\r\n    print(\r\n        ""Model building  : {:12.6} s\\n""\r\n        ""Estimation      : {:12.6} s\\n""\r\n        ""Total           : {:12.6} s"".format(\r\n            learning_time, estimation_time,\r\n            learning_time + estimation_time\r\n        )\r\n    )\r\n    nb.show_timing_log()\r\n    nb.visualize()\r\n'"
NN/PyTorch/Auto/Layers.py,0,"b'import torch.nn.functional as F\r\nfrom torch.autograd import Variable\r\n\r\nfrom NN.Errors import *\r\nfrom NN.PyTorch.Optimizers import *\r\n\r\n\r\n# Abstract Layers\r\n\r\nclass Layer:\r\n    LayerTiming = Timing()\r\n\r\n    def __init__(self, shape):\r\n        """"""\r\n        :param shape: shape[0] = units of previous layer\r\n                      shape[1] = units of current layer (self)\r\n        """"""\r\n        self._shape = shape\r\n        self.parent = None\r\n        self.child = None\r\n        self.is_sub_layer = False\r\n        self._last_sub_layer = None\r\n\r\n    def __str__(self):\r\n        return self.__class__.__name__\r\n\r\n    def __repr__(self):\r\n        return str(self)\r\n\r\n    @property\r\n    def name(self):\r\n        return str(self)\r\n\r\n    @property\r\n    def shape(self):\r\n        return self._shape\r\n\r\n    @shape.setter\r\n    def shape(self, value):\r\n        self._shape = value\r\n\r\n    @property\r\n    def params(self):\r\n        return self._shape,\r\n\r\n    @property\r\n    def special_params(self):\r\n        return\r\n\r\n    def set_special_params(self, dic):\r\n        for key, value in dic.items():\r\n            setattr(self, key, value)\r\n\r\n    @property\r\n    def root(self):\r\n        return self\r\n\r\n    @root.setter\r\n    def root(self, value):\r\n        raise BuildLayerError(""Setting Layer\'s root is not permitted"")\r\n\r\n    @property\r\n    def last_sub_layer(self):\r\n        child = self.child\r\n        if not child:\r\n            return None\r\n        while child.child:\r\n            child = child.child\r\n        return child\r\n\r\n    @last_sub_layer.setter\r\n    def last_sub_layer(self, value):\r\n            self._last_sub_layer = value\r\n\r\n    # Core\r\n\r\n    def derivative(self, y, delta=None):\r\n        return self._derivative(y, delta)\r\n\r\n    @LayerTiming.timeit(level=1, prefix=""[Core] "")\r\n    def activate(self, x, w, bias=None, predict=False):\r\n        if self.is_sub_layer:\r\n            if bias is None:\r\n                return self._activate(x, predict)\r\n            return self._activate(x + bias.expand_as(x), predict)\r\n        mul = x.mm(w)\r\n        if bias is None:\r\n            return self._activate(mul, predict)\r\n        return self._activate(x.mm(w) + bias.expand_as(mul), predict)\r\n\r\n    @LayerTiming.timeit(level=1, prefix=""[Core] "")\r\n    def bp(self, y, w, prev_delta):\r\n        if self.child is not None and isinstance(self.child, SubLayer):\r\n            if not isinstance(self, SubLayer):\r\n                return prev_delta\r\n            return self._derivative(y, prev_delta)\r\n        if isinstance(self, SubLayer):\r\n            return self._derivative(y, prev_delta.mm(w.t()) * self._root.derivative(y))\r\n        return prev_delta.mm(w.t()) * self._derivative(y)\r\n\r\n    def _activate(self, x, predict):\r\n        pass\r\n\r\n    def _derivative(self, y, delta=None):\r\n        pass\r\n\r\n    # Util\r\n\r\n    @staticmethod\r\n    @LayerTiming.timeit(level=2, prefix=""[Core Util] "")\r\n    def safe_exp(y):\r\n        return torch.exp(y - torch.max(y, dim=1)[0].expand_as(y))\r\n\r\n\r\nclass SubLayer(Layer):\r\n    def __init__(self, parent, shape):\r\n        super(SubLayer, self).__init__(shape)\r\n        self.parent = parent\r\n        parent.child = self\r\n        self._root = None\r\n        self.description = """"\r\n\r\n    @property\r\n    def root(self):\r\n        parent = self.parent\r\n        while parent.parent:\r\n            parent = parent.parent\r\n        return parent\r\n\r\n    @root.setter\r\n    def root(self, value):\r\n        self._root = value\r\n\r\n    def get_params(self):\r\n        pass\r\n\r\n    @property\r\n    def params(self):\r\n        return self.get_params()\r\n\r\n    def _activate(self, x, predict):\r\n        raise NotImplementedError(""Please implement activation function for "" + self.name)\r\n\r\n    def _derivative(self, y, delta=None):\r\n        raise NotImplementedError(""Please implement derivative function for "" + self.name)\r\n\r\n\r\n# Activation Layers\r\n\r\nclass Tanh(Layer):\r\n    def _activate(self, x, predict):\r\n        return F.tanh(x)\r\n\r\n\r\nclass Sigmoid(Layer):\r\n    def _activate(self, x, predict):\r\n        return F.sigmoid(x)\r\n\r\n\r\nclass ELU(Layer):\r\n    def _activate(self, x, predict):\r\n        return F.elu(x)\r\n\r\n\r\nclass ReLU(Layer):\r\n    def _activate(self, x, predict):\r\n        return F.relu(x)\r\n\r\n\r\nclass Softplus(Layer):\r\n    def _activate(self, x, predict):\r\n        return F.softplus(x)\r\n\r\n\r\nclass Identical(Layer):\r\n    def _activate(self, x, predict):\r\n        return x\r\n\r\n\r\n# Special Layer\r\n\r\nclass Dropout(SubLayer):\r\n    def __init__(self, parent, shape, prob=0.5):\r\n        if prob < 0 or prob >= 1:\r\n            raise BuildLayerError(""Keep probability of Dropout should be a positive float smaller than 1"")\r\n        SubLayer.__init__(self, parent, shape)\r\n        self._prob = prob\r\n        self.description = ""(Keep prob: {})"".format(prob)\r\n\r\n    def get_params(self):\r\n        return self._prob,\r\n\r\n    def _activate(self, x, predict):\r\n        return F.dropout(x, 1 - self._prob, not predict)\r\n\r\n\r\nclass Normalize(SubLayer):\r\n    def __init__(self, parent, shape, lr=0.001, eps=1e-8, momentum=0.9, optimizers=None):\r\n        SubLayer.__init__(self, parent, shape)\r\n        self.sample_mean, self.sample_var = None, None\r\n        self.running_mean, self.running_var = None, None\r\n        self.x_cache, self.x_normalized_cache = None, None\r\n        self._lr, self._eps = lr, eps\r\n        if optimizers is None:\r\n            self._g_optimizer, self._b_optimizer = Adam(self._lr), Adam(self._lr)\r\n        else:\r\n            self._g_optimizer, self._b_optimizer = optimizers\r\n        self.gamma = Variable(torch.ones(self.shape[1]), requires_grad=True)\r\n        self.beta = Variable(torch.ones(self.shape[1]), requires_grad=True)\r\n        self._momentum = momentum\r\n        self.init_optimizers()\r\n        self.description = ""(lr: {}, eps: {}, momentum: {}, optimizer: ({}, {}))"".format(\r\n            lr, eps, momentum, self._g_optimizer.name, self._b_optimizer.name\r\n        )\r\n\r\n    def get_params(self):\r\n        return self._lr, self._eps, self._momentum, (self._g_optimizer.name, self._b_optimizer.name)\r\n\r\n    @property\r\n    def special_params(self):\r\n        return {\r\n            ""gamma"": self.gamma, ""beta"": self.beta,\r\n            ""running_mean"": self.running_mean, ""running_var"": self.running_var,\r\n            ""_g_optimizer"": self._g_optimizer, ""_b_optimizer"": self._b_optimizer\r\n        }\r\n\r\n    def init_optimizers(self):\r\n        _opt_fac = OptFactory()\r\n        if not isinstance(self._g_optimizer, Optimizer):\r\n            self._g_optimizer = _opt_fac.get_optimizer_by_name(\r\n                self._g_optimizer, None, self._lr, None\r\n            )\r\n        if not isinstance(self._b_optimizer, Optimizer):\r\n            self._b_optimizer = _opt_fac.get_optimizer_by_name(\r\n                self._b_optimizer, None, self._lr, None\r\n            )\r\n        self._g_optimizer.feed_variables([self.gamma])\r\n        self._b_optimizer.feed_variables([self.beta])\r\n\r\n    # noinspection PyTypeChecker\r\n    def _activate(self, x, predict):\r\n        if self.running_mean is None or self.running_var is None:\r\n            self.running_mean = Variable(torch.zeros(x.size()[1]))\r\n            self.running_var = Variable(torch.zeros(x.size()[1]))\r\n        if not predict:\r\n            self.sample_mean = torch.mean(x, dim=0)\r\n            self.sample_var = torch.var(x, dim=0)\r\n            x_normalized = (x - self.sample_mean.expand_as(x)) / torch.sqrt(self.sample_var + self._eps).expand_as(x)\r\n            self.x_cache, self.x_normalized_cache = x, x_normalized\r\n            out = self.gamma.expand_as(x_normalized) * x_normalized + self.beta.expand_as(x_normalized)\r\n            self.running_mean = self._momentum * self.running_mean + (1 - self._momentum) * self.sample_mean\r\n            self.running_var = self._momentum * self.running_var + (1 - self._momentum) * self.sample_var\r\n            if self.gamma.grad is not None and self.beta.grad is not None:\r\n                self._g_optimizer.update()\r\n                self._b_optimizer.update()\r\n                self.gamma.data -= self._g_optimizer.run(0, self.gamma.grad.data)\r\n                self.beta.data -= self._b_optimizer.run(0, self.beta.grad.data)\r\n                self.gamma.grad.data.zero_()\r\n                self.beta.grad.data.zero_()\r\n        else:\r\n            x_normalized = (x - self.running_mean.expand_as(x)) / torch.sqrt(self.running_var + self._eps).expand_as(x)\r\n            out = self.gamma.expand_as(x) * x_normalized + self.beta.expand_as(x)\r\n        return out\r\n\r\n\r\n# Cost Layer\r\n\r\nclass CostLayer(Layer):\r\n    def __init__(self, shape, cost_function=""MSE"", transform=None):\r\n        super(CostLayer, self).__init__(shape)\r\n        self._available_cost_functions = {\r\n            ""MSE"": CostLayer._mse,\r\n            ""CrossEntropy"": CostLayer._cross_entropy\r\n        }\r\n        self._available_transform_functions = {\r\n            ""Softmax"": CostLayer._softmax,\r\n            ""Sigmoid"": CostLayer._sigmoid\r\n        }\r\n        if cost_function not in self._available_cost_functions:\r\n            raise LayerError(""Cost function \'{}\' not implemented"".format(cost_function))\r\n        self._cost_function_name = cost_function\r\n        self._cost_function = self._available_cost_functions[cost_function]\r\n        if transform is None and cost_function == ""CrossEntropy"":\r\n            self._transform = ""Softmax""\r\n            self._transform_function = CostLayer._softmax\r\n        else:\r\n            self._transform = transform\r\n            self._transform_function = self._available_transform_functions.get(transform, None)\r\n\r\n    def __str__(self):\r\n        return self._cost_function_name\r\n\r\n    def _activate(self, x, predict):\r\n        if self._transform_function is None:\r\n            return x\r\n        return self._transform_function(x)\r\n\r\n    @property\r\n    def calculate(self):\r\n        return lambda y, y_pred: self._cost_function(y, y_pred)\r\n\r\n    @property\r\n    def cost_function(self):\r\n        return self._cost_function_name\r\n\r\n    @cost_function.setter\r\n    def cost_function(self, value):\r\n        if value not in self._available_cost_functions:\r\n            raise LayerError(""\'{}\' is not implemented"".format(value))\r\n        self._cost_function_name = value\r\n        self._cost_function = self._available_cost_functions[value]\r\n\r\n    def set_cost_function_derivative(self, func, name=None):\r\n        name = ""Custom Cost Function"" if name is None else name\r\n        self._cost_function_name = name\r\n        self._cost_function = func\r\n\r\n    # Transform Functions\r\n\r\n    @staticmethod\r\n    def _softmax(y):\r\n        return F.log_softmax(y)\r\n\r\n    @staticmethod\r\n    def _sigmoid(y):\r\n        return F.sigmoid(y)\r\n\r\n    # Cost Functions\r\n\r\n    @staticmethod\r\n    def _mse(y, y_pred):\r\n        return torch.nn.MSELoss()(y_pred, y)\r\n\r\n    @staticmethod\r\n    def _cross_entropy(y, y_pred):\r\n        return F.cross_entropy(\r\n            y_pred, torch.squeeze(torch.max(y, dim=1)[1], 1)\r\n        )\r\n\r\n    \r\n# Factory\r\n\r\nclass LayerFactory:\r\n    available_root_layers = {\r\n        ""Tanh"": Tanh, ""Sigmoid"": Sigmoid,\r\n        ""ELU"": ELU, ""ReLU"": ReLU, ""Softplus"": Softplus,\r\n        ""Identical"": Identical,\r\n        ""MSE"": CostLayer, ""CrossEntropy"": CostLayer\r\n    }\r\n    available_sub_layers = {\r\n        ""Dropout"", ""Normalize""\r\n    }\r\n    available_cost_functions = {\r\n        ""MSE"", ""CrossEntropy""\r\n    }\r\n    available_special_layers = {\r\n        ""Dropout"": Dropout,\r\n        ""Normalize"": Normalize\r\n    }\r\n    special_layer_default_params = {\r\n        ""Dropout"": (0.5, ),\r\n        ""Normalize"": (0.001, 1e-8, 0.9)\r\n    }\r\n\r\n    def get_root_layer_by_name(self, name, *args, **kwargs):\r\n        if name not in self.available_sub_layers:\r\n            if name in self.available_root_layers:\r\n                if name in self.available_cost_functions:\r\n                    kwargs[""cost_function""] = name\r\n                name = self.available_root_layers[name]\r\n            else:\r\n                raise BuildNetworkError(""Undefined layer \'{}\' found"".format(name))\r\n            return name(*args, **kwargs)\r\n        return None\r\n    \r\n    def get_layer_by_name(self, name, parent, current_dimension, *args, **kwargs):\r\n        _layer = self.get_root_layer_by_name(name, *args, **kwargs)\r\n        if _layer:\r\n            return _layer, None\r\n        _current, _next = parent.shape[1], current_dimension\r\n        layer_param = self.special_layer_default_params[name]\r\n        _layer = self.available_special_layers[name]\r\n        if args or kwargs:\r\n            _layer = _layer(parent, (_current, _next), *args, **kwargs)\r\n        else:\r\n            _layer = _layer(parent, (_current, _next), *layer_param)\r\n        return _layer, (_current, _next)\r\n'"
NN/PyTorch/Auto/Networks.py,0,"b'import os\r\nimport cv2\r\nimport time\r\nimport pickle\r\nimport numpy as np\r\n\r\nfrom NN.PyTorch.Auto.Layers import *\r\nfrom NN.Basic.Networks import NNConfig, NNVerbose\r\nfrom NN.PyTorch.Optimizers import OptFactory\r\n\r\nfrom Util.Util import VisUtil\r\nfrom Util.ProgressBar import ProgressBar\r\nfrom Util.Bases import TorchAutoClassifierBase\r\n\r\n\r\n# PyTorch Implementation with auto-grad & custom Optimizers\r\n\r\nclass NNDist(TorchAutoClassifierBase):\r\n    NNTiming = Timing()\r\n\r\n    def __init__(self, **kwargs):\r\n        super(NNDist, self).__init__(**kwargs)\r\n        self._layers, self._weights, self._bias = [], [], []\r\n        self._layer_names, self._layer_shapes, self._layer_params = [], [], []\r\n        self._lr, self._epoch, self._regularization_param = 0, 0, 0\r\n        self.verbose = 1\r\n\r\n        self._apply_bias = False\r\n        self._current_dimension = 0\r\n\r\n        self._logs = {}\r\n        self._metrics, self._metric_names = [], []\r\n\r\n        self._x_min, self._x_max = 0, 0\r\n        self._y_min, self._y_max = 0, 0\r\n\r\n        self._layer_factory = LayerFactory()\r\n        self._optimizer_factory = OptFactory()\r\n\r\n        self._available_metrics = {\r\n            ""acc"": NNDist.acc, ""_acc"": NNDist.acc,\r\n            ""f1"": NNDist.f1_score, ""_f1_score"": NNDist.f1_score\r\n        }\r\n\r\n    @NNTiming.timeit(level=4, prefix=""[Initialize] "")\r\n    def initialize(self):\r\n        self._layers, self._weights, self._bias = [], [], []\r\n        self._layer_names, self._layer_shapes, self._layer_params = [], [], []\r\n        self._lr, self._epoch, self._regularization_param = 0, 0, 0\r\n        self.verbose = 1\r\n\r\n        self._apply_bias = False\r\n        self._current_dimension = 0\r\n\r\n        self._logs = []\r\n        self._metrics, self._metric_names = [], []\r\n\r\n        self._x_min, self._x_max = 0, 0\r\n        self._y_min, self._y_max = 0, 0\r\n\r\n    # Property\r\n\r\n    @property\r\n    def name(self):\r\n        return (\r\n            ""-"".join([str(_layer.shape[1]) for _layer in self._layers]) +\r\n            "" at {}"".format(time.strftime(""%Y-%m-%d %H:%M:%S"", time.localtime()))\r\n        )\r\n\r\n    @property\r\n    def layer_names(self):\r\n        return [layer.name for layer in self._layers]\r\n\r\n    @layer_names.setter\r\n    def layer_names(self, value):\r\n        self._layer_names = value\r\n\r\n    @property\r\n    def layer_shapes(self):\r\n        return [layer.size() for layer in self._layers]\r\n\r\n    @layer_shapes.setter\r\n    def layer_shapes(self, value):\r\n        self._layer_shapes = value\r\n\r\n    @property\r\n    def layer_params(self):\r\n        return self._layer_params\r\n\r\n    @layer_params.setter\r\n    def layer_params(self, value):\r\n        self.layer_params = value\r\n\r\n    @property\r\n    def layer_special_params(self):\r\n        return [layer.special_params for layer in self._layers]\r\n\r\n    @layer_special_params.setter\r\n    def layer_special_params(self, value):\r\n        for layer, sp_param in zip(self._layers, value):\r\n            if sp_param is not None:\r\n                layer.set_special_params(sp_param)\r\n\r\n    # Utils\r\n\r\n    @NNTiming.timeit(level=4)\r\n    def _get_min_max(self, x, y):\r\n        x, y = x.data.numpy(), y.data.numpy()\r\n        self._x_min, self._x_max = np.min(x), np.max(x)\r\n        self._y_min, self._y_max = np.min(y), np.max(y)\r\n\r\n    @NNTiming.timeit(level=4, prefix=""[API] "")\r\n    def _split_data(self, x, y, x_test, y_test,\r\n                    train_only, training_scale=NNConfig.TRAINING_SCALE):\r\n        if train_only:\r\n            if x_test is not None and y_test is not None:\r\n                x, y = torch.cat((x, x_test)), torch.cat((y, y_test))\r\n            x_train, y_train, x_test, y_test = x, y, x, y\r\n        else:\r\n            shuffle_suffix = torch.randperm(len(x))\r\n            x, y = x[shuffle_suffix], y[shuffle_suffix]\r\n            if x_test is None or y_test is None:\r\n                train_len = int(len(x) * training_scale)\r\n                x_train, y_train = x[:train_len], y[:train_len]\r\n                x_test, y_test = x[train_len:], y[train_len:]\r\n            elif x_test is None or y_test is None:\r\n                raise BuildNetworkError(""Please provide test sets if you want to split data on your own"")\r\n            else:\r\n                x_train, y_train = x, y\r\n        if NNConfig.BOOST_LESS_SAMPLES:\r\n            if y_train.shape[1] != 2:\r\n                raise BuildNetworkError(""It is not permitted to boost less samples in multiple classification"")\r\n            y_train_arg = torch.max(y_train, dim=1)[1]\r\n            y0 = y_train_arg == 0\r\n            y1 = ~y0\r\n            y_len, y0_len = len(y_train), torch.sum(y0)  # type: float\r\n            if y0_len > int(0.5 * y_len):\r\n                y0, y1 = y1, y0\r\n                y0_len = y_len - y0_len\r\n            boost_suffix = torch.IntTensor(y_len - y0_len).random_(y0_len)\r\n            x_train = torch.cat((x_train[y1], x_train[y0][boost_suffix]))\r\n            y_train = torch.cat((y_train[y1], y_train[y0][boost_suffix]))\r\n            shuffle_suffix = torch.randperm(len(x_train))\r\n            x_train, y_train = x_train[shuffle_suffix], y_train[shuffle_suffix]\r\n        return (x_train, x_test), (y_train, y_test)\r\n\r\n    @NNTiming.timeit(level=4)\r\n    def _add_params(self, shape):\r\n        self._weights.append(Variable(torch.randn(*shape), requires_grad=True))\r\n        self._bias.append(Variable(torch.zeros((1, shape[1])), requires_grad=True))\r\n\r\n    @NNTiming.timeit(level=4)\r\n    def _add_layer(self, layer, *args, **kwargs):\r\n        if not self._layers and isinstance(layer, str):\r\n            layer = self._layer_factory.get_root_layer_by_name(layer, *args, **kwargs)\r\n            if layer:\r\n                self.add(layer)\r\n                return\r\n        parent = self._layers[-1]\r\n        if isinstance(parent, CostLayer):\r\n            raise BuildLayerError(""Adding layer after CostLayer is not permitted"")\r\n        if isinstance(layer, str):\r\n            layer, shape = self._layer_factory.get_layer_by_name(\r\n                layer, parent, self._current_dimension, *args, **kwargs\r\n            )\r\n            if shape is None:\r\n                self.add(layer)\r\n                return\r\n            _current, _next = shape\r\n        else:\r\n            _current, _next = args\r\n        if isinstance(layer, SubLayer):\r\n            parent.child = layer\r\n            layer.is_sub_layer = True\r\n            layer.root = layer.root\r\n            layer.root.last_sub_layer = layer\r\n            self.parent = parent\r\n            self._layers.append(layer)\r\n            self._weights.append(None)\r\n            self._bias.append(Variable(torch.zeros((1, _next)), requires_grad=True))\r\n            self._current_dimension = _next\r\n        else:\r\n            self._layers.append(layer)\r\n            self._add_params((_current, _next))\r\n            self._current_dimension = _next\r\n        self._update_layer_information(layer)\r\n\r\n    @NNTiming.timeit(level=4)\r\n    def _update_layer_information(self, layer):\r\n        self._layer_params.append(layer.params)\r\n        if len(self._layer_params) > 1 and not layer.is_sub_layer:\r\n            self._layer_params[-1] = ((self._layer_params[-1][0][1],), *self._layer_params[-1][1:])\r\n\r\n    @NNTiming.timeit(level=1)\r\n    def _get_prediction(self, x, name=None, batch_size=1e6, verbose=None):\r\n        if verbose is None:\r\n            verbose = self.verbose\r\n        fc_shape = np.prod(x.size()[1:])  # type: int\r\n        single_batch = int(batch_size / fc_shape)\r\n        if not single_batch:\r\n            single_batch = 1\r\n        if single_batch >= len(x):\r\n            return self._get_activations(x, predict=True).pop()\r\n        epoch = int(len(x) / single_batch)\r\n        if not len(x) % single_batch:\r\n            epoch += 1\r\n        name = ""Prediction"" if name is None else ""Prediction ({})"".format(name)\r\n        sub_bar = ProgressBar(max_value=epoch, name=name, start=False)\r\n        if verbose >= NNVerbose.METRICS:\r\n            sub_bar.start()\r\n        rs, count = [self._get_activations(x[:single_batch], predict=True).pop()], single_batch\r\n        if verbose >= NNVerbose.METRICS:\r\n            sub_bar.update()\r\n        while count < len(x):\r\n            count += single_batch\r\n            if count >= len(x):\r\n                rs.append(self._get_activations(x[count-single_batch:], predict=True).pop())\r\n            else:\r\n                rs.append(self._get_activations(x[count-single_batch:count], predict=True).pop())\r\n            if verbose >= NNVerbose.METRICS:\r\n                sub_bar.update()\r\n        return torch.cat(rs)\r\n\r\n    @NNTiming.timeit(level=1)\r\n    def _get_activations(self, x, predict=False):\r\n        activations = [self._layers[0].activate(x, self._weights[0], self._bias[0], predict)]\r\n        for i, layer in enumerate(self._layers[1:]):\r\n            activations.append(layer.activate(\r\n                activations[-1], self._weights[i + 1], self._bias[i + 1], predict))\r\n        return activations\r\n\r\n    @NNTiming.timeit(level=1)\r\n    def _get_final_activation(self, x, predict=False):\r\n        activation = self._layers[0].activate(x, self._weights[0], self._bias[0], predict)\r\n        for i, layer in enumerate(self._layers[1:]):\r\n            activation = layer.activate(activation, self._weights[i + 1], self._bias[i + 1], predict)\r\n        return activation\r\n\r\n    @NNTiming.timeit(level=3)\r\n    def _append_log(self, x, y, name, get_loss=True):\r\n        y_pred = self._get_prediction(x, name)\r\n        for i, metric in enumerate(self._metrics):\r\n            self._logs[name][i].append(metric(\r\n                torch.max(y, dim=1)[1].data.numpy(),\r\n                torch.max(y_pred, dim=1)[1].data.numpy()\r\n            ))\r\n        if get_loss:\r\n            self._logs[name][-1].append(\r\n                (self._layers[-1].calculate(y, y_pred) / len(y)).data.numpy()[0]\r\n            )\r\n\r\n    @NNTiming.timeit(level=3)\r\n    def _print_metric_logs(self, show_loss, data_type):\r\n        print()\r\n        print(""="" * 47)\r\n        for i, name in enumerate(self._metric_names):\r\n            print(""{:<16s} {:<16s}: {:12.8}"".format(\r\n                data_type, name, self._logs[data_type][i][-1]))\r\n        if show_loss:\r\n            print(""{:<16s} {:<16s}: {:12.8}"".format(\r\n                data_type, ""loss"", self._logs[data_type][-1][-1]))\r\n        print(""="" * 47)\r\n\r\n    # TODO\r\n    @NNTiming.timeit(level=1)\r\n    def _draw_2d_network(self, radius=6, width=1200, height=800, padding=0.2,\r\n                         plot_scale=2, plot_precision=0.03,\r\n                         sub_layer_height_scale=0, **kwargs):\r\n        if not kwargs[""show""] and not kwargs[""mp4""]:\r\n            return\r\n        layers = len(self._layers) + 1\r\n        units = [layer.shape[0] for layer in self._layers] + [self._layers[-1].shape[1]]\r\n        whether_sub_layers = np.array([False] + [isinstance(layer, SubLayer) for layer in self._layers])\r\n        n_sub_layers = np.sum(whether_sub_layers)  # type: int\r\n\r\n        plot_num = int(1 / plot_precision)\r\n        if plot_num % 2 == 1:\r\n            plot_num += 1\r\n        half_plot_num = int(plot_num * 0.5)\r\n        xf = torch.linspace(self._x_min * plot_scale, self._x_max * plot_scale, plot_num)\r\n        yf = torch.linspace(self._x_min * plot_scale, self._x_max * plot_scale, plot_num) * -1\r\n        input_xs = Variable(torch.stack([\r\n            xf.repeat(plot_num), yf.repeat(plot_num, 1).t().contiguous().view(-1)\r\n        ], 1))\r\n\r\n        activations = [\r\n            activation.data.numpy().T.reshape(units[i + 1], plot_num, plot_num)\r\n            for i, activation in enumerate(\r\n                self._get_activations(input_xs, predict=True)\r\n            )]\r\n        graphs = []\r\n        for j, activation in enumerate(activations):\r\n            graph_group = []\r\n            if j == len(activations) - 1:\r\n                classes = np.argmax(activation, axis=0)\r\n            else:\r\n                classes = None\r\n            for k, ac in enumerate(activation):\r\n                data = np.zeros((plot_num, plot_num, 3), np.uint8)\r\n                if j != len(activations) - 1:\r\n                    mask = ac >= np.average(ac)\r\n                else:\r\n                    mask = classes == k\r\n                data[mask], data[~mask] = [0, 165, 255], [255, 165, 0]\r\n                graph_group.append(data)\r\n            graphs.append(graph_group)\r\n\r\n        img = np.full([height, width, 3], 255, dtype=np.uint8)\r\n        axis0_padding = int(height / (layers - 1 + 2 * padding)) * padding + plot_num\r\n        axis0_step = (height - 2 * axis0_padding) / layers\r\n        sub_layer_decrease = int((1 - sub_layer_height_scale) * axis0_step)\r\n        axis0 = np.linspace(\r\n            axis0_padding,\r\n            height + n_sub_layers * sub_layer_decrease - axis0_padding,\r\n            layers, dtype=np.int)\r\n        axis0 -= sub_layer_decrease * np.cumsum(whether_sub_layers)\r\n        axis1_padding = plot_num\r\n        axis1 = [np.linspace(axis1_padding, width - axis1_padding, unit + 2, dtype=np.int)\r\n                 for unit in units]\r\n        axis1 = [axis[1:-1] for axis in axis1]\r\n\r\n        colors, thicknesses, masks = [], [], []\r\n        for weight in self._weights:\r\n            line_info = VisUtil.get_line_info(weight.data.numpy().copy())\r\n            colors.append(line_info[0])\r\n            thicknesses.append(line_info[1])\r\n            masks.append(line_info[2])\r\n\r\n        for i, (y, xs) in enumerate(zip(axis0, axis1)):\r\n            for j, x in enumerate(xs):\r\n                if i == 0:\r\n                    cv2.circle(img, (x, y), radius, (20, 215, 20), int(radius / 2))\r\n                else:\r\n                    graph = graphs[i - 1][j]\r\n                    img[y - half_plot_num:y + half_plot_num, x - half_plot_num:x + half_plot_num] = graph\r\n            if i > 0:\r\n                cv2.putText(img, self._layers[i - 1].name, (12, y - 36), cv2.LINE_AA, 0.6, (0, 0, 0), 1)\r\n        for i, y in enumerate(axis0):\r\n            if i == len(axis0) - 1:\r\n                break\r\n            for j, x in enumerate(axis1[i]):\r\n                new_y = axis0[i + 1]\r\n                whether_sub_layer = isinstance(self._layers[i], SubLayer)\r\n                for k, new_x in enumerate(axis1[i + 1]):\r\n                    if whether_sub_layer and j != k:\r\n                        continue\r\n                    if masks[i][j][k]:\r\n                        cv2.line(img, (x, y + half_plot_num), (new_x, new_y - half_plot_num),\r\n                                 colors[i][j][k], thicknesses[i][j][k])\r\n\r\n        return img\r\n\r\n    # Optimizing Process\r\n\r\n    @NNTiming.timeit(level=4)\r\n    def _init_optimizer(self):\r\n        if not isinstance(self._optimizer, Optimizer):\r\n            self._optimizer = self._optimizer_factory.get_optimizer_by_name(\r\n                self._optimizer, self._model_parameters, self._lr, self._epoch)\r\n\r\n    # Batch Work\r\n\r\n    @NNTiming.timeit(level=2)\r\n    def _batch_work(self, i, sub_bar, x, y, x_test, y_test,\r\n                    draw_weights, weight_trace, show_loss):\r\n        if draw_weights:\r\n            for i, weight in enumerate(self._weights):\r\n                for j, new_weight in enumerate(weight.copy()):\r\n                    weight_trace[i][j].append(new_weight)\r\n        if self.verbose >= NNVerbose.DEBUG:\r\n            pass\r\n        if self.verbose >= NNVerbose.ITER:\r\n            if sub_bar.update() and self.verbose >= NNVerbose.METRICS_DETAIL:\r\n                self._append_log(x, y, ""train"", get_loss=show_loss)\r\n                self._append_log(x_test, y_test, ""cv"", get_loss=show_loss)\r\n                self._print_metric_logs(show_loss, ""train"")\r\n                self._print_metric_logs(show_loss, ""cv"")\r\n\r\n    @NNTiming.timeit(level=2)\r\n    def _predict(self, x, get_raw_results=False, **kwargs):\r\n        rs = self._get_final_activation(x)\r\n        if get_raw_results:\r\n            return rs\r\n        return torch.sign(rs)\r\n\r\n    # API\r\n\r\n    @NNTiming.timeit(level=4, prefix=""[API] "")\r\n    def add(self, layer, *args, **kwargs):\r\n        if isinstance(layer, str):\r\n            # noinspection PyTypeChecker\r\n            self._add_layer(layer, *args, **kwargs)\r\n        else:\r\n            if not isinstance(layer, Layer):\r\n                raise BuildLayerError(""Invalid Layer provided (should be subclass of Layer)"")\r\n            if not self._layers:\r\n                if isinstance(layer, SubLayer):\r\n                    raise BuildLayerError(""Invalid Layer provided (first layer should not be subclass of SubLayer)"")\r\n                if len(layer.shape) != 2:\r\n                    raise BuildLayerError(""Invalid input Layer provided (shape should be {}, {} found)"".format(\r\n                        2, len(layer.shape)\r\n                    ))\r\n                self._layers, self._current_dimension = [layer], layer.shape[1]\r\n                self._update_layer_information(layer)\r\n                self._add_params(layer.shape)\r\n            else:\r\n                if len(layer.shape) > 2:\r\n                    raise BuildLayerError(""Invalid Layer provided (shape should be {}, {} found)"".format(\r\n                        2, len(layer.shape)\r\n                    ))\r\n                if len(layer.shape) == 2:\r\n                    _current, _next = layer.shape\r\n                    if isinstance(layer, SubLayer):\r\n                        if _next != self._current_dimension:\r\n                            raise BuildLayerError(""Invalid SubLayer provided (shape[1] should be {}, {} found)"".format(\r\n                                self._current_dimension, _next\r\n                            ))\r\n                    elif _current != self._current_dimension:\r\n                        raise BuildLayerError(""Invalid Layer provided (shape[0] should be {}, {} found)"".format(\r\n                            self._current_dimension, _current\r\n                        ))\r\n                    self._add_layer(layer, _current, _next)\r\n\r\n                elif len(layer.shape) == 1:\r\n                    _next = layer.shape[0]\r\n                    layer.shape = (self._current_dimension, _next)\r\n                    self._add_layer(layer, self._current_dimension, _next)\r\n                else:\r\n                    raise LayerError(""Invalid Layer provided (invalid shape \'{}\' found)"".format(layer.shape))\r\n\r\n    # TODO\r\n    @NNTiming.timeit(level=4, prefix=""[API] "")\r\n    def build(self, units=""build""):\r\n        if isinstance(units, str):\r\n            if units == ""build"":\r\n                for name, param in zip(self._layer_names, self._layer_params):\r\n                    self.add(name, *param)\r\n            else:\r\n                raise NotImplementedError(""Invalid param \'{}\' provided to \'build\' method"".format(units))\r\n        else:\r\n            try:\r\n                units = list(units)\r\n            except ValueError as err:\r\n                raise BuildLayerError(err)\r\n            if len(units) < 2:\r\n                raise BuildLayerError(""At least 2 layers are needed"")\r\n            _input_shape = (units[0], units[1])\r\n            self.initialize()\r\n            self.add(ReLU(_input_shape))\r\n            for unit_num in units[2:-1]:\r\n                self.add(ReLU((unit_num,)))\r\n            self.add(""CrossEntropy"", (units[-1],))\r\n\r\n    @NNTiming.timeit(level=4, prefix=""[API] "")\r\n    def preview(self):\r\n        if not self._layers:\r\n            rs = ""None""\r\n        else:\r\n            rs = (\r\n                ""Input  :  {:<10s} - {}\\n"".format(""Dimension"", self._layers[0].shape[0]) +\r\n                ""\\n"".join([\r\n                    ""Layer  :  {:<10s} - {} {}"".format(\r\n                        layer.name, layer.shape[1], layer.description\r\n                    ) if isinstance(layer, SubLayer) else\r\n                    ""Layer  :  {:<10s} - {}"".format(\r\n                        layer.name, layer.shape[1]\r\n                    ) for layer in self._layers[:-1]\r\n                ]) + ""\\nCost   :  {:<16s}"".format(str(self._layers[-1]))\r\n            )\r\n        print(""="" * 30 + ""\\n"" + ""Structure\\n"" + ""-"" * 30 + ""\\n"" + rs + ""\\n"" + ""-"" * 30 + ""\\n"")\r\n\r\n    @NNTiming.timeit(level=1, prefix=""[API] "")\r\n    def fit(self,\r\n            x, y, x_test=None, y_test=None,\r\n            batch_size=128, record_period=1, train_only=False,\r\n            optimizer=""Adam"", lr=0.001, lb=0.001, epoch=20, weight_scale=1, apply_bias=True,\r\n            show_loss=True, metrics=None, do_log=True, verbose=None,\r\n            visualize=False, visualize_setting=None,\r\n            draw_weights=False, animation_params=None):\r\n\r\n        self._lr, self._epoch = lr, epoch\r\n        for weight in self._weights:\r\n            if weight is not None:\r\n                weight.data *= weight_scale\r\n        self._model_parameters = [w for w in self._weights if w is not None]\r\n        if apply_bias:\r\n            self._model_parameters += self._bias\r\n        self._optimizer = optimizer\r\n        self._init_optimizer()\r\n        assert isinstance(self._optimizer, Optimizer)\r\n        print()\r\n        print(""="" * 30)\r\n        print(""Optimizers"")\r\n        print(""-"" * 30)\r\n        print(self._optimizer)\r\n        print(""-"" * 30)\r\n\r\n        if not self._layers:\r\n            raise BuildNetworkError(""Please provide layers before fitting data"")\r\n        if y.shape[1] != self._current_dimension:\r\n            raise BuildNetworkError(""Output layer\'s shape should be {}, {} found"".format(\r\n                self._current_dimension, y.shape[1]))\r\n\r\n        x, y = self._arr_to_variable(False, x, y)\r\n        if x_test is not None and y_test is not None:\r\n            x_test, y_test = self._arr_to_variable(False, x_test, y_test)\r\n        (x_train, x_test), (y_train, y_test) = self._split_data(\r\n            x, y, x_test, y_test, train_only)\r\n        train_len = len(x_train)\r\n        batch_size = min(batch_size, train_len)\r\n        do_random_batch = train_len > batch_size\r\n        train_repeat = 1 if not do_random_batch else int(train_len / batch_size) + 1\r\n        self._regularization_param = 1 - lb * lr / batch_size\r\n        self._get_min_max(x_train, y_train)\r\n\r\n        self._metrics = [""acc""] if metrics is None else metrics\r\n        for i, metric in enumerate(self._metrics):\r\n            if isinstance(metric, str):\r\n                if metric not in self._available_metrics:\r\n                    raise BuildNetworkError(""Metric \'{}\' is not implemented"".format(metric))\r\n                self._metrics[i] = self._available_metrics[metric]\r\n        self._metric_names = [_m.__name__ for _m in self._metrics]\r\n\r\n        self._logs = {\r\n            name: [[] for _ in range(len(self._metrics) + 1)] for name in (""train"", ""cv"", ""test"")\r\n        }\r\n        if verbose is not None:\r\n            self.verbose = verbose\r\n\r\n        self._apply_bias = apply_bias\r\n\r\n        bar = ProgressBar(max_value=max(1, epoch // record_period), name=""Epoch"", start=False)\r\n        if self.verbose >= NNVerbose.EPOCH:\r\n            bar.start()\r\n        img, ims = None, []\r\n\r\n        if draw_weights:\r\n            weight_trace = [[[org] for org in weight] for weight in self._weights]\r\n        else:\r\n            weight_trace = []\r\n\r\n        loss_function = self._layers[-1].calculate\r\n        args = (\r\n            x_train, y_train, x_test, y_test,\r\n            draw_weights, weight_trace, show_loss\r\n        )\r\n\r\n        *animation_properties, animation_params = self._get_animation_params(animation_params)\r\n        sub_bar = ProgressBar(max_value=train_repeat * record_period - 1, name=""Iteration"", start=False)\r\n        for counter in range(epoch):\r\n            self._optimizer.update()\r\n            if self.verbose >= NNVerbose.ITER and counter % record_period == 0:\r\n                sub_bar.start()\r\n            self.batch_training(\r\n                x_train, y_train, batch_size, train_repeat, loss_function, sub_bar, *args\r\n            )\r\n            if self.verbose >= NNVerbose.ITER:\r\n                sub_bar.update()\r\n            self._handle_animation(\r\n                counter, x, y, ims, animation_params, *animation_properties,\r\n                img=self._draw_2d_network(**animation_params), name=""Neural Network""\r\n            )\r\n            if do_log:\r\n                self._append_log(x, y, ""train"", get_loss=show_loss)\r\n                self._append_log(x_test, y_test, ""cv"", get_loss=show_loss)\r\n            if (counter + 1) % record_period == 0:\r\n                if do_log and self.verbose >= NNVerbose.METRICS:\r\n                    self._print_metric_logs(show_loss, ""train"")\r\n                    self._print_metric_logs(show_loss, ""cv"")\r\n                if visualize:\r\n                    if visualize_setting is None:\r\n                        self.visualize2d(x_test, y_test)\r\n                    else:\r\n                        self.visualize2d(x_test, y_test, *visualize_setting)\r\n                if self.verbose >= NNVerbose.EPOCH:\r\n                    bar.update(counter // record_period + 1)\r\n                    if self.verbose >= NNVerbose.ITER:\r\n                        sub_bar = ProgressBar(max_value=train_repeat * record_period - 1, name=""Iteration"", start=False)\r\n\r\n        if do_log:\r\n            self._append_log(x_test, y_test, ""test"", get_loss=show_loss)\r\n        if img is not None:\r\n            cv2.waitKey(0)\r\n            cv2.destroyAllWindows()\r\n        if draw_weights:\r\n            ts = np.arange(epoch * train_repeat + 1)\r\n            for i, weight in enumerate(self._weights):\r\n                plt.figure()\r\n                for j in range(len(weight)):\r\n                    plt.plot(ts, weight_trace[i][j])\r\n                plt.title(""Weights toward layer {} ({})"".format(i + 1, self._layers[i].name))\r\n                plt.show()\r\n        self._handle_mp4(ims, animation_properties, ""NN"")\r\n        return self._logs\r\n\r\n    # TODO\r\n    @NNTiming.timeit(level=2, prefix=""[API] "")\r\n    def save(self, path=None, name=None, overwrite=True):\r\n        path = os.path.join(""Models"", ""Cache"") if path is None else os.path.join(""Models"", path)\r\n        name = ""Model.nn"" if name is None else name\r\n        if not os.path.exists(path):\r\n            os.makedirs(path)\r\n        _dir = os.path.join(path, name)\r\n        if not overwrite and os.path.isfile(_dir):\r\n            _count = 1\r\n            _new_dir = _dir + ""({})"".format(_count)\r\n            while os.path.isfile(_new_dir):\r\n                _count += 1\r\n                _new_dir = _dir + ""({})"".format(_count)\r\n            _dir = _new_dir\r\n        print()\r\n        print(""="" * 60)\r\n        print(""Saving Model to {}..."".format(_dir))\r\n        print(""-"" * 60)\r\n        with open(_dir, ""wb"") as file:\r\n            pickle.dump({\r\n                ""structures"": {\r\n                    ""_layer_names"": self.layer_names,\r\n                    ""_layer_params"": self._layer_params,\r\n                    ""_cost_layer"": self._layers[-1].name,\r\n                    ""_next_dimension"": self._current_dimension\r\n                },\r\n                ""params"": {\r\n                    ""_logs"": self._logs,\r\n                    ""_metric_names"": self._metric_names,\r\n                    ""_weights"": self._weights,\r\n                    ""_bias"": self._bias,\r\n                    ""_optimizer"": self._optimizer,\r\n                    ""layer_special_params"": self.layer_special_params,\r\n                }\r\n            }, file)\r\n        print(""Done"")\r\n        print(""="" * 60)\r\n\r\n    # TODO\r\n    @NNTiming.timeit(level=2, prefix=""[API] "")\r\n    def load(self, path=os.path.join(""Models"", ""Cache"", ""Model.nn"")):\r\n        self.initialize()\r\n        try:\r\n            with open(path, ""rb"") as file:\r\n                dic = pickle.load(file)\r\n                for key, value in dic[""structures""].items():\r\n                    setattr(self, key, value)\r\n                self.build()\r\n                for key, value in dic[""params""].items():\r\n                    setattr(self, key, value)\r\n                self._init_optimizer()\r\n                for i in range(len(self._metric_names) - 1, -1, -1):\r\n                    name = self._metric_names[i]\r\n                    if name not in self._available_metrics:\r\n                        self._metric_names.pop(i)\r\n                    else:\r\n                        self._metrics.insert(0, self._available_metrics[name])\r\n                print()\r\n                print(""="" * 30)\r\n                print(""Model restored"")\r\n                print(""="" * 30)\r\n                return dic\r\n        except Exception as err:\r\n            raise BuildNetworkError(""Failed to load Network ({}), structure initialized"".format(err))\r\n\r\n    @NNTiming.timeit(level=4, prefix=""[API] "")\r\n    def predict(self, x, get_raw_results=False, **kwargs):\r\n        if not isinstance(x, Variable):\r\n            x = Variable(torch.from_numpy(np.asarray(x, dtype=np.float32)))\r\n        if len(x.size()) == 1:\r\n            x = x.view(1, -1)\r\n        y_pred = self._get_prediction(x).data.numpy()\r\n        return y_pred if get_raw_results else np.argmax(y_pred, axis=1)\r\n\r\n    def draw_results(self):\r\n        metrics_log, loss_log = {}, {}\r\n        for key, value in sorted(self._logs.items()):\r\n            metrics_log[key], loss_log[key] = value[:-1], value[-1]\r\n\r\n        for i, name in enumerate(sorted(self._metric_names)):\r\n            plt.figure()\r\n            plt.title(""Metric Type: {}"".format(name))\r\n            for key, log in sorted(metrics_log.items()):\r\n                if key == ""test"":\r\n                    continue\r\n                xs = np.arange(len(log[i])) + 1\r\n                plt.plot(xs, log[i], label=""Data Type: {}"".format(key))\r\n            plt.legend(loc=4)\r\n            plt.show()\r\n            plt.close()\r\n\r\n        plt.figure()\r\n        plt.title(""Cost"")\r\n        for key, loss in sorted(loss_log.items()):\r\n            if key == ""test"":\r\n                continue\r\n            xs = np.arange(len(loss)) + 1\r\n            plt.plot(xs, loss, label=""Data Type: {}"".format(key))\r\n        plt.legend()\r\n        plt.show()\r\n'"
NN/PyTorch/Basic/Layers.py,0,"b'from NN.Errors import *\r\nfrom NN.PyTorch.Optimizers import *\r\n\r\n\r\n# Abstract Layers\r\n\r\nclass Layer:\r\n    LayerTiming = Timing()\r\n\r\n    def __init__(self, shape):\r\n        """"""\r\n        :param shape: shape[0] = units of previous layer\r\n                      shape[1] = units of current layer (self)\r\n        """"""\r\n        self._shape = shape\r\n        self.parent = None\r\n        self.child = None\r\n        self.is_sub_layer = False\r\n        self._last_sub_layer = None\r\n\r\n    def __str__(self):\r\n        return self.__class__.__name__\r\n\r\n    def __repr__(self):\r\n        return str(self)\r\n\r\n    @property\r\n    def name(self):\r\n        return str(self)\r\n\r\n    @property\r\n    def shape(self):\r\n        return self._shape\r\n\r\n    @shape.setter\r\n    def shape(self, value):\r\n        self._shape = value\r\n\r\n    @property\r\n    def params(self):\r\n        return self._shape,\r\n\r\n    @property\r\n    def special_params(self):\r\n        return\r\n\r\n    def set_special_params(self, dic):\r\n        for key, value in dic.items():\r\n            setattr(self, key, value)\r\n\r\n    @property\r\n    def root(self):\r\n        return self\r\n\r\n    @root.setter\r\n    def root(self, value):\r\n        raise BuildLayerError(""Setting Layer\'s root is not permitted"")\r\n\r\n    @property\r\n    def last_sub_layer(self):\r\n        child = self.child\r\n        if not child:\r\n            return None\r\n        while child.child:\r\n            child = child.child\r\n        return child\r\n\r\n    @last_sub_layer.setter\r\n    def last_sub_layer(self, value):\r\n            self._last_sub_layer = value\r\n\r\n    # Core\r\n\r\n    def derivative(self, y, delta=None):\r\n        return self._derivative(y, delta)\r\n\r\n    @LayerTiming.timeit(level=1, prefix=""[Core] "")\r\n    def activate(self, x, w, bias=None, predict=False):\r\n        if self.is_sub_layer:\r\n            if bias is None:\r\n                return self._activate(x, predict)\r\n            return self._activate(x + bias.expand_as(x), predict)\r\n        mul = x.mm(w)\r\n        if bias is None:\r\n            return self._activate(mul, predict)\r\n        return self._activate(x.mm(w) + bias.expand_as(mul), predict)\r\n\r\n    @LayerTiming.timeit(level=1, prefix=""[Core] "")\r\n    def bp(self, y, w, prev_delta):\r\n        if self.child is not None and isinstance(self.child, SubLayer):\r\n            if not isinstance(self, SubLayer):\r\n                return prev_delta\r\n            return self._derivative(y, prev_delta)\r\n        if isinstance(self, SubLayer):\r\n            return self._derivative(y, prev_delta.mm(w.t()) * self._root.derivative(y))\r\n        return prev_delta.mm(w.t()) * self._derivative(y)\r\n\r\n    def _activate(self, x, predict):\r\n        pass\r\n\r\n    def _derivative(self, y, delta=None):\r\n        pass\r\n\r\n    # Util\r\n\r\n    @staticmethod\r\n    @LayerTiming.timeit(level=2, prefix=""[Core Util] "")\r\n    def safe_exp(y):\r\n        return torch.exp(y - torch.max(y, dim=1)[0].expand_as(y))\r\n\r\n\r\nclass SubLayer(Layer):\r\n    def __init__(self, parent, shape):\r\n        super(SubLayer, self).__init__(shape)\r\n        self.parent = parent\r\n        parent.child = self\r\n        self._root = None\r\n        self.description = """"\r\n\r\n    @property\r\n    def root(self):\r\n        parent = self.parent\r\n        while parent.parent:\r\n            parent = parent.parent\r\n        return parent\r\n\r\n    @root.setter\r\n    def root(self, value):\r\n        self._root = value\r\n\r\n    def get_params(self):\r\n        pass\r\n\r\n    @property\r\n    def params(self):\r\n        return self.get_params()\r\n\r\n    def _activate(self, x, predict):\r\n        raise NotImplementedError(""Please implement activation function for "" + self.name)\r\n\r\n    def _derivative(self, y, delta=None):\r\n        raise NotImplementedError(""Please implement derivative function for "" + self.name)\r\n\r\n\r\n# Activation Layers\r\n\r\nclass Tanh(Layer):\r\n    def _activate(self, x, predict):\r\n        return torch.tanh(x)\r\n\r\n    def _derivative(self, y, delta=None):\r\n        return 1 - y * y\r\n\r\n\r\nclass Sigmoid(Layer):\r\n    def _activate(self, x, predict):\r\n        return 1 / (1 + torch.exp(-x))\r\n\r\n    def _derivative(self, y, delta=None):\r\n        return y * (1 - y)\r\n\r\n\r\nclass ELU(Layer):\r\n    def _activate(self, x, predict):\r\n        rs, mask = torch.Tensor(x.size()).copy_(x), x < 0\r\n        rs[mask] = torch.exp(rs[mask]) - 1\r\n        return rs\r\n\r\n    def _derivative(self, y, delta=None):\r\n        rs, mask = torch.ones(y.size()), y < 0\r\n        rs[mask] = y[mask] + 1\r\n        return rs\r\n\r\n\r\nclass ReLU(Layer):\r\n    def _activate(self, x, predict):\r\n        return x.clamp(min=0)\r\n\r\n    def _derivative(self, y, delta=None):\r\n        return (y > 0).float()\r\n\r\n\r\nclass Softplus(Layer):\r\n    def _activate(self, x, predict):\r\n        return torch.log(1 + torch.exp(x))\r\n\r\n    def _derivative(self, y, delta=None):\r\n        return 1 - 1 / torch.exp(y)\r\n\r\n\r\nclass Identical(Layer):\r\n    def _activate(self, x, predict):\r\n        return x\r\n\r\n    def _derivative(self, y, delta=None):\r\n        return 1\r\n\r\n\r\n# Special Layer\r\n\r\nclass Dropout(SubLayer):\r\n    def __init__(self, parent, shape, keep_prob=0.5):\r\n        if keep_prob < 0 or keep_prob >= 1:\r\n            raise BuildLayerError(""Keep probability of Dropout should be a positive float smaller than 1"")\r\n        SubLayer.__init__(self, parent, shape)\r\n        self._mask = None\r\n        self._prob = keep_prob\r\n        self._prob_inv = 1 / keep_prob\r\n        self.description = ""(Keep prob: {})"".format(keep_prob)\r\n\r\n    def get_params(self):\r\n        return self._prob,\r\n\r\n    def _activate(self, x, predict):\r\n        if not predict:\r\n            self._mask = (torch.rand(x.size()) < self._prob).float() * self._prob_inv\r\n            return x * self._mask\r\n        return x\r\n\r\n    def _derivative(self, y, delta=None):\r\n        return delta * self._mask\r\n\r\n\r\nclass Normalize(SubLayer):\r\n    def __init__(self, parent, shape, lr=0.001, eps=1e-8, momentum=0.9, optimizers=None):\r\n        SubLayer.__init__(self, parent, shape)\r\n        self.sample_mean, self.sample_var = None, None\r\n        self.running_mean, self.running_var = None, None\r\n        self.x_cache, self.x_normalized_cache = None, None\r\n        self._lr, self._eps = lr, eps\r\n        if optimizers is None:\r\n            self._g_optimizer, self._b_optimizer = Adam(self._lr), Adam(self._lr)\r\n        else:\r\n            self._g_optimizer, self._b_optimizer = optimizers\r\n        self.gamma, self.beta = torch.ones(self.shape[1]), torch.zeros(self.shape[1])\r\n        self._momentum = momentum\r\n        self.init_optimizers()\r\n        self.description = ""(lr: {}, eps: {}, momentum: {}, optimizer: ({}, {}))"".format(\r\n            lr, eps, momentum, self._g_optimizer.name, self._b_optimizer.name\r\n        )\r\n\r\n    def get_params(self):\r\n        return self._lr, self._eps, self._momentum, (self._g_optimizer.name, self._b_optimizer.name)\r\n\r\n    @property\r\n    def special_params(self):\r\n        return {\r\n            ""gamma"": self.gamma, ""beta"": self.beta,\r\n            ""running_mean"": self.running_mean, ""running_var"": self.running_var,\r\n            ""_g_optimizer"": self._g_optimizer, ""_b_optimizer"": self._b_optimizer\r\n        }\r\n\r\n    def init_optimizers(self):\r\n        _opt_fac = OptFactory()\r\n        if not isinstance(self._g_optimizer, Optimizer):\r\n            self._g_optimizer = _opt_fac.get_optimizer_by_name(\r\n                self._g_optimizer, None, self._lr, None\r\n            )\r\n        if not isinstance(self._b_optimizer, Optimizer):\r\n            self._b_optimizer = _opt_fac.get_optimizer_by_name(\r\n                self._b_optimizer, None, self._lr, None\r\n            )\r\n        self._g_optimizer.feed_variables([self.gamma])\r\n        self._b_optimizer.feed_variables([self.beta])\r\n\r\n    # noinspection PyTypeChecker\r\n    def _activate(self, x, predict):\r\n        if self.running_mean is None or self.running_var is None:\r\n            self.running_mean = torch.zeros(x.size()[1])\r\n            self.running_var = torch.zeros(x.size()[1])\r\n        if not predict:\r\n            self.sample_mean = torch.mean(x, dim=0)\r\n            self.sample_var = torch.var(x, dim=0)\r\n            x_normalized = (x - self.sample_mean.expand_as(x)) / torch.sqrt(self.sample_var + self._eps).expand_as(x)\r\n            self.x_cache, self.x_normalized_cache = x, x_normalized\r\n            out = self.gamma.expand_as(x_normalized) * x_normalized + self.beta.expand_as(x_normalized)\r\n            self.running_mean = self._momentum * self.running_mean + (1 - self._momentum) * self.sample_mean\r\n            self.running_var = self._momentum * self.running_var + (1 - self._momentum) * self.sample_var\r\n        else:\r\n            x_normalized = (x - self.running_mean.expand_as(x)) / torch.sqrt(self.running_var + self._eps).expand_as(x)\r\n            out = self.gamma.expand_as(x) * x_normalized + self.beta.expand_as(x)\r\n        return out\r\n\r\n    def _derivative(self, y, delta=None):\r\n        n, d = self.x_cache.size()\r\n        dx_normalized = delta * self.gamma.expand_as(delta)\r\n        x_mu = self.x_cache - self.sample_mean.expand_as(self.x_cache)\r\n        sample_std_inv = 1.0 / torch.sqrt(self.sample_var + self._eps)\r\n        ds_var = -0.5 * torch.sum(dx_normalized * x_mu, dim=0).expand_as(sample_std_inv) * (\r\n            sample_std_inv * sample_std_inv * sample_std_inv\r\n        )\r\n        ds_mean = (\r\n            -1.0 * torch.sum(dx_normalized * sample_std_inv.expand_as(x_mu), dim=0) - 2.0 *\r\n            ds_var * torch.mean(x_mu, dim=0)\r\n        )\r\n        dx1 = dx_normalized * sample_std_inv.expand_as(x_mu)\r\n        dx2 = 2.0 / n * ds_var.expand_as(x_mu) * x_mu\r\n        dx = dx1 + dx2 + 1.0 / n * ds_mean.expand_as(x_mu)\r\n        dg = torch.sum(delta * self.x_normalized_cache, dim=0)\r\n        db = torch.sum(delta, dim=0)\r\n        self._g_optimizer.update()\r\n        self._b_optimizer.update()\r\n        self.gamma += self._g_optimizer.run(0, dg)\r\n        self.beta += self._b_optimizer.run(0, db)\r\n        return dx\r\n\r\n\r\n# Cost Layer\r\n\r\nclass CostLayer(Layer):\r\n    def __init__(self, shape, cost_function=""MSE"", transform=None):\r\n        super(CostLayer, self).__init__(shape)\r\n        self._available_cost_functions = {\r\n            ""MSE"": CostLayer._mse,\r\n            ""CrossEntropy"": CostLayer._cross_entropy\r\n        }\r\n        self._available_transform_functions = {\r\n            ""Softmax"": CostLayer._softmax,\r\n            ""Sigmoid"": CostLayer._sigmoid\r\n        }\r\n        if cost_function not in self._available_cost_functions:\r\n            raise LayerError(""Cost function \'{}\' not implemented"".format(cost_function))\r\n        self._cost_function_name = cost_function\r\n        self._cost_function = self._available_cost_functions[cost_function]\r\n        if transform is None and cost_function == ""CrossEntropy"":\r\n            self._transform = ""Softmax""\r\n            self._transform_function = CostLayer._softmax\r\n        else:\r\n            self._transform = transform\r\n            self._transform_function = self._available_transform_functions.get(transform, None)\r\n\r\n    def __str__(self):\r\n        return self._cost_function_name\r\n\r\n    def _activate(self, x, predict):\r\n        if self._transform_function is None:\r\n            return x\r\n        return self._transform_function(x)\r\n\r\n    def _derivative(self, y, delta=None):\r\n        raise LayerError(""derivative function should not be called in CostLayer"")\r\n\r\n    def bp_first(self, y, y_pred):\r\n        if self._cost_function_name == ""CrossEntropy"" and (\r\n                self._transform == ""Softmax"" or self._transform == ""Sigmoid""):\r\n            return y - y_pred\r\n        dy = -self._cost_function(y, y_pred)\r\n        if self._transform_function is None:\r\n            return dy\r\n        return dy * self._transform_function(y_pred, diff=True)\r\n\r\n    @property\r\n    def calculate(self):\r\n        return lambda y, y_pred: self._cost_function(y, y_pred, False)\r\n\r\n    @property\r\n    def cost_function(self):\r\n        return self._cost_function_name\r\n\r\n    @cost_function.setter\r\n    def cost_function(self, value):\r\n        if value not in self._available_cost_functions:\r\n            raise LayerError(""\'{}\' is not implemented"".format(value))\r\n        self._cost_function_name = value\r\n        self._cost_function = self._available_cost_functions[value]\r\n\r\n    def set_cost_function_derivative(self, func, name=None):\r\n        name = ""Custom Cost Function"" if name is None else name\r\n        self._cost_function_name = name\r\n        self._cost_function = func\r\n\r\n    # Transform Functions\r\n\r\n    @staticmethod\r\n    def _softmax(y, diff=False):\r\n        if diff:\r\n            return y * (1 - y)\r\n        exp_y = CostLayer.safe_exp(y)\r\n        return exp_y / torch.sum(exp_y, dim=1).expand_as(exp_y)\r\n\r\n    @staticmethod\r\n    def _sigmoid(y, diff=False):\r\n        if diff:\r\n            return y * (1 - y)\r\n        return 1 / (1 + torch.exp(-y))\r\n\r\n    # Cost Functions\r\n\r\n    @staticmethod\r\n    def _mse(y, y_pred, diff=True):\r\n        if diff:\r\n            return -y + y_pred\r\n        dis = y - y_pred\r\n        return 0.5 * torch.mean(dis * dis)\r\n\r\n    @staticmethod\r\n    def _cross_entropy(y, y_pred, diff=True):\r\n        if diff:\r\n            return -y / y_pred + (1 - y) / (1 - y_pred)\r\n        # noinspection PyTypeChecker\r\n        return torch.mean(\r\n            -y * torch.log(torch.clamp(y_pred, min=1e-12)) -\r\n            (1 - y) * torch.log(torch.clamp(1 - y_pred, min=1e-12))\r\n        )\r\n\r\n    \r\n# Factory\r\n\r\nclass LayerFactory:\r\n    available_root_layers = {\r\n        ""Tanh"": Tanh, ""Sigmoid"": Sigmoid,\r\n        ""ELU"": ELU, ""ReLU"": ReLU, ""Softplus"": Softplus,\r\n        ""Identical"": Identical,\r\n        ""MSE"": CostLayer, ""CrossEntropy"": CostLayer\r\n    }\r\n    available_sub_layers = {\r\n        ""Dropout"", ""Normalize""\r\n    }\r\n    available_cost_functions = {\r\n        ""MSE"", ""CrossEntropy""\r\n    }\r\n    available_special_layers = {\r\n        ""Dropout"": Dropout,\r\n        ""Normalize"": Normalize\r\n    }\r\n    special_layer_default_params = {\r\n        ""Dropout"": (0.5, ),\r\n        ""Normalize"": (0.001, 1e-8, 0.9)\r\n    }\r\n\r\n    def get_root_layer_by_name(self, name, *args, **kwargs):\r\n        if name not in self.available_sub_layers:\r\n            if name in self.available_root_layers:\r\n                if name in self.available_cost_functions:\r\n                    kwargs[""cost_function""] = name\r\n                name = self.available_root_layers[name]\r\n            else:\r\n                raise BuildNetworkError(""Undefined layer \'{}\' found"".format(name))\r\n            return name(*args, **kwargs)\r\n        return None\r\n    \r\n    def get_layer_by_name(self, name, parent, current_dimension, *args, **kwargs):\r\n        _layer = self.get_root_layer_by_name(name, *args, **kwargs)\r\n        if _layer:\r\n            return _layer, None\r\n        _current, _next = parent.shape[1], current_dimension\r\n        layer_param = self.special_layer_default_params[name]\r\n        _layer = self.available_special_layers[name]\r\n        if args or kwargs:\r\n            _layer = _layer(parent, (_current, _next), *args, **kwargs)\r\n        else:\r\n            _layer = _layer(parent, (_current, _next), *layer_param)\r\n        return _layer, (_current, _next)\r\n'"
NN/PyTorch/Basic/Networks.py,0,"b'import os\r\nimport cv2\r\nimport time\r\nimport pickle\r\nimport numpy as np\r\nimport matplotlib.pyplot as plt\r\nfrom mpl_toolkits.mplot3d import Axes3D\r\n\r\nfrom NN.PyTorch.Basic.Layers import *\r\nfrom NN.Basic.Networks import NNConfig, NNVerbose\r\nfrom NN.PyTorch.Optimizers import OptFactory\r\n\r\nfrom Util.Util import VisUtil\r\nfrom Util.ProgressBar import ProgressBar\r\nfrom Util.Bases import TorchBasicClassifierBase\r\n\r\n\r\n# PyTorch Implementation without using auto-grad\r\n\r\nclass NNDist(TorchBasicClassifierBase):\r\n    NNTiming = Timing()\r\n\r\n    def __init__(self, **kwargs):\r\n        super(NNDist, self).__init__(**kwargs)\r\n        self._layers, self._weights, self._bias = [], [], []\r\n        self._layer_names, self._layer_shapes, self._layer_params = [], [], []\r\n        self._lr, self._epoch, self._regularization_param = 0, 0, 0\r\n        self._w_optimizer, self._b_optimizer, self._optimizer_name = None, None, """"\r\n        self.verbose = 1\r\n\r\n        self._whether_apply_bias = False\r\n        self._current_dimension = 0\r\n\r\n        self._logs = {}\r\n        self._metrics, self._metric_names = [], []\r\n\r\n        self._x_min, self._x_max = 0, 0\r\n        self._y_min, self._y_max = 0, 0\r\n\r\n        self._layer_factory = LayerFactory()\r\n        self._optimizer_factory = OptFactory()\r\n\r\n        self._available_metrics = {\r\n            ""acc"": NNDist.acc, ""_acc"": NNDist.acc,\r\n            ""f1"": NNDist.f1_score, ""_f1_score"": NNDist.f1_score\r\n        }\r\n\r\n    @NNTiming.timeit(level=4, prefix=""[Initialize] "")\r\n    def initialize(self):\r\n        self._layers, self._weights, self._bias = [], [], []\r\n        self._layer_names, self._layer_shapes, self._layer_params = [], [], []\r\n        self._lr, self._epoch, self._regularization_param = 0, 0, 0\r\n        self._w_optimizer, self._b_optimizer, self._optimizer_name = None, None, """"\r\n        self.verbose = 1\r\n\r\n        self._whether_apply_bias = False\r\n        self._current_dimension = 0\r\n\r\n        self._logs = []\r\n        self._metrics, self._metric_names = [], []\r\n\r\n        self._x_min, self._x_max = 0, 0\r\n        self._y_min, self._y_max = 0, 0\r\n\r\n    # Property\r\n\r\n    @property\r\n    def name(self):\r\n        return (\r\n            ""-"".join([str(_layer.shape[1]) for _layer in self._layers]) +\r\n            "" at {}"".format(time.strftime(""%Y-%m-%d %H:%M:%S"", time.localtime()))\r\n        )\r\n\r\n    @property\r\n    def layer_names(self):\r\n        return [layer.name for layer in self._layers]\r\n\r\n    @layer_names.setter\r\n    def layer_names(self, value):\r\n        self._layer_names = value\r\n\r\n    @property\r\n    def layer_shapes(self):\r\n        return [layer.size() for layer in self._layers]\r\n\r\n    @layer_shapes.setter\r\n    def layer_shapes(self, value):\r\n        self._layer_shapes = value\r\n\r\n    @property\r\n    def layer_params(self):\r\n        return self._layer_params\r\n\r\n    @layer_params.setter\r\n    def layer_params(self, value):\r\n        self.layer_params = value\r\n\r\n    @property\r\n    def layer_special_params(self):\r\n        return [layer.special_params for layer in self._layers]\r\n\r\n    @layer_special_params.setter\r\n    def layer_special_params(self, value):\r\n        for layer, sp_param in zip(self._layers, value):\r\n            if sp_param is not None:\r\n                layer.set_special_params(sp_param)\r\n\r\n    @property\r\n    def optimizer(self):\r\n        return self._optimizer_name\r\n\r\n    @optimizer.setter\r\n    def optimizer(self, value):\r\n        try:\r\n            self._optimizer_name = value\r\n        except KeyError:\r\n            raise BuildNetworkError(""Invalid Optimizer \'{}\' provided"".format(value))\r\n\r\n    # Utils\r\n\r\n    @NNTiming.timeit(level=4)\r\n    def _get_min_max(self, x, y):\r\n        x, y = x.numpy(), y.numpy()\r\n        self._x_min, self._x_max = np.min(x), np.max(x)\r\n        self._y_min, self._y_max = np.min(y), np.max(y)\r\n\r\n    @NNTiming.timeit(level=4, prefix=""[API] "")\r\n    def _split_data(self, x, y, x_test, y_test,\r\n                    train_only, training_scale=NNConfig.TRAINING_SCALE):\r\n        if train_only:\r\n            if x_test is not None and y_test is not None:\r\n                x, y = torch.cat((x, x_test)), torch.cat((y, y_test))\r\n            x_train, y_train, x_test, y_test = x, y, x, y\r\n        else:\r\n            shuffle_suffix = torch.randperm(len(x))\r\n            x, y = x[shuffle_suffix], y[shuffle_suffix]\r\n            if x_test is None or y_test is None:\r\n                train_len = int(len(x) * training_scale)\r\n                x_train, y_train = x[:train_len], y[:train_len]\r\n                x_test, y_test = x[train_len:], y[train_len:]\r\n            elif x_test is None or y_test is None:\r\n                raise BuildNetworkError(""Please provide test sets if you want to split data on your own"")\r\n            else:\r\n                x_train, y_train = x, y\r\n        if NNConfig.BOOST_LESS_SAMPLES:\r\n            if y_train.shape[1] != 2:\r\n                raise BuildNetworkError(""It is not permitted to boost less samples in multiple classification"")\r\n            y_train_arg = torch.max(y_train, dim=1)[1]\r\n            y0 = y_train_arg == 0\r\n            y1 = ~y0\r\n            y_len, y0_len = len(y_train), torch.sum(y0)  # type: float\r\n            if y0_len > int(0.5 * y_len):\r\n                y0, y1 = y1, y0\r\n                y0_len = y_len - y0_len\r\n            boost_suffix = torch.IntTensor(y_len - y0_len).random_(y0_len)\r\n            x_train = torch.cat((x_train[y1], x_train[y0][boost_suffix]))\r\n            y_train = torch.cat((y_train[y1], y_train[y0][boost_suffix]))\r\n            shuffle_suffix = torch.randperm(len(x_train))\r\n            x_train, y_train = x_train[shuffle_suffix], y_train[shuffle_suffix]\r\n        return (x_train, x_test), (y_train, y_test)\r\n\r\n    @NNTiming.timeit(level=4)\r\n    def _add_weight(self, shape):\r\n        self._weights.append(torch.randn(*shape))\r\n        self._bias.append(torch.zeros((1, shape[1])))\r\n\r\n    @NNTiming.timeit(level=4)\r\n    def _add_layer(self, layer, *args, **kwargs):\r\n        if not self._layers and isinstance(layer, str):\r\n            layer = self._layer_factory.get_root_layer_by_name(layer, *args, **kwargs)\r\n            if layer:\r\n                self.add(layer)\r\n                return\r\n        parent = self._layers[-1]\r\n        if isinstance(parent, CostLayer):\r\n            raise BuildLayerError(""Adding layer after CostLayer is not permitted"")\r\n        if isinstance(layer, str):\r\n            layer, shape = self._layer_factory.get_layer_by_name(\r\n                layer, parent, self._current_dimension, *args, **kwargs\r\n            )\r\n            if shape is None:\r\n                self.add(layer)\r\n                return\r\n            _current, _next = shape\r\n        else:\r\n            _current, _next = args\r\n        if isinstance(layer, SubLayer):\r\n            parent.child = layer\r\n            layer.is_sub_layer = True\r\n            layer.root = layer.root\r\n            layer.root.last_sub_layer = layer\r\n            self.parent = parent\r\n            self._layers.append(layer)\r\n            self._weights.append(torch.Tensor(0))\r\n            self._bias.append(torch.Tensor([0.]))\r\n            self._current_dimension = _next\r\n        else:\r\n            self._layers.append(layer)\r\n            self._add_weight((_current, _next))\r\n            self._current_dimension = _next\r\n        self._update_layer_information(layer)\r\n\r\n    @NNTiming.timeit(level=4)\r\n    def _update_layer_information(self, layer):\r\n        self._layer_params.append(layer.params)\r\n        if len(self._layer_params) > 1 and not layer.is_sub_layer:\r\n            self._layer_params[-1] = ((self._layer_params[-1][0][1],), *self._layer_params[-1][1:])\r\n\r\n    @NNTiming.timeit(level=1)\r\n    def _get_prediction(self, x, name=None, batch_size=1e6, verbose=None):\r\n        if verbose is None:\r\n            verbose = self.verbose\r\n        fc_shape = np.prod(x.size()[1:])  # type: int\r\n        single_batch = int(batch_size / fc_shape)\r\n        if not single_batch:\r\n            single_batch = 1\r\n        if single_batch >= len(x):\r\n            return self._get_activations(x, predict=True).pop()\r\n        epoch = int(len(x) / single_batch)\r\n        if not len(x) % single_batch:\r\n            epoch += 1\r\n        name = ""Prediction"" if name is None else ""Prediction ({})"".format(name)\r\n        sub_bar = ProgressBar(max_value=epoch, name=name, start=False)\r\n        if verbose >= NNVerbose.METRICS:\r\n            sub_bar.start()\r\n        rs, count = [self._get_activations(x[:single_batch], predict=True).pop()], single_batch\r\n        if verbose >= NNVerbose.METRICS:\r\n            sub_bar.update()\r\n        while count < len(x):\r\n            count += single_batch\r\n            if count >= len(x):\r\n                rs.append(self._get_activations(x[count-single_batch:], predict=True).pop())\r\n            else:\r\n                rs.append(self._get_activations(x[count-single_batch:count], predict=True).pop())\r\n            if verbose >= NNVerbose.METRICS:\r\n                sub_bar.update()\r\n        return torch.cat(rs)\r\n\r\n    @NNTiming.timeit(level=1)\r\n    def _get_activations(self, x, predict=False):\r\n        activations = [self._layers[0].activate(x, self._weights[0], self._bias[0], predict)]\r\n        for i, layer in enumerate(self._layers[1:]):\r\n            activations.append(layer.activate(\r\n                activations[-1], self._weights[i + 1], self._bias[i + 1], predict))\r\n        return activations\r\n\r\n    @NNTiming.timeit(level=3)\r\n    def _append_log(self, x, y, name, get_loss=True):\r\n        y_pred = self._get_prediction(x, name)\r\n        for i, metric in enumerate(self._metrics):\r\n            self._logs[name][i].append(metric(\r\n                torch.max(y, dim=1)[1].numpy(), torch.max(y_pred, dim=1)[1].numpy()\r\n            ))\r\n        if get_loss:\r\n            self._logs[name][-1].append(\r\n                (self._layers[-1].calculate(y, y_pred) / len(y))\r\n            )\r\n\r\n    @NNTiming.timeit(level=3)\r\n    def _print_metric_logs(self, show_loss, data_type):\r\n        print()\r\n        print(""="" * 47)\r\n        for i, name in enumerate(self._metric_names):\r\n            print(""{:<16s} {:<16s}: {:12.8}"".format(\r\n                data_type, name, self._logs[data_type][i][-1]))\r\n        if show_loss:\r\n            print(""{:<16s} {:<16s}: {:12.8}"".format(\r\n                data_type, ""loss"", self._logs[data_type][-1][-1]))\r\n        print(""="" * 47)\r\n\r\n    # TODO\r\n    @NNTiming.timeit(level=1)\r\n    def _draw_2d_network(self, radius=6, width=1200, height=800, padding=0.2,\r\n                         plot_scale=2, plot_precision=0.03,\r\n                         sub_layer_height_scale=0, **kwargs):\r\n        if not kwargs[""show""] and not kwargs[""mp4""]:\r\n            return\r\n        layers = len(self._layers) + 1\r\n        units = [layer.shape[0] for layer in self._layers] + [self._layers[-1].shape[1]]\r\n        whether_sub_layers = np.array([False] + [isinstance(layer, SubLayer) for layer in self._layers])\r\n        n_sub_layers = np.sum(whether_sub_layers)  # type: int\r\n\r\n        plot_num = int(1 / plot_precision)\r\n        if plot_num % 2 == 1:\r\n            plot_num += 1\r\n        half_plot_num = int(plot_num * 0.5)\r\n        xf = torch.linspace(self._x_min * plot_scale, self._x_max * plot_scale, plot_num)\r\n        yf = torch.linspace(self._x_min * plot_scale, self._x_max * plot_scale, plot_num) * -1\r\n        input_xs = torch.stack([\r\n            xf.repeat(plot_num), yf.repeat(plot_num, 1).t().contiguous().view(-1)\r\n        ], 1)\r\n\r\n        activations = [\r\n            activation.numpy().T.reshape(units[i + 1], plot_num, plot_num)\r\n            for i, activation in enumerate(\r\n                self._get_activations(input_xs, predict=True)\r\n            )]\r\n        graphs = []\r\n        for j, activation in enumerate(activations):\r\n            graph_group = []\r\n            if j == len(activations) - 1:\r\n                classes = np.argmax(activation, axis=0)\r\n            else:\r\n                classes = None\r\n            for k, ac in enumerate(activation):\r\n                data = np.zeros((plot_num, plot_num, 3), np.uint8)\r\n                if j != len(activations) - 1:\r\n                    mask = ac >= np.average(ac)\r\n                else:\r\n                    mask = classes == k\r\n                data[mask], data[~mask] = [0, 165, 255], [255, 165, 0]\r\n                graph_group.append(data)\r\n            graphs.append(graph_group)\r\n\r\n        img = np.full([height, width, 3], 255, dtype=np.uint8)\r\n        axis0_padding = int(height / (layers - 1 + 2 * padding)) * padding + plot_num\r\n        axis0_step = (height - 2 * axis0_padding) / layers\r\n        sub_layer_decrease = int((1 - sub_layer_height_scale) * axis0_step)\r\n        axis0 = np.linspace(\r\n            axis0_padding,\r\n            height + n_sub_layers * sub_layer_decrease - axis0_padding,\r\n            layers, dtype=np.int)\r\n        axis0 -= sub_layer_decrease * np.cumsum(whether_sub_layers)\r\n        axis1_padding = plot_num\r\n        axis1 = [np.linspace(axis1_padding, width - axis1_padding, unit + 2, dtype=np.int)\r\n                 for unit in units]\r\n        axis1 = [axis[1:-1] for axis in axis1]\r\n\r\n        colors, thicknesses, masks = [], [], []\r\n        for weight in self._weights:\r\n            line_info = VisUtil.get_line_info(weight.numpy().copy())\r\n            colors.append(line_info[0])\r\n            thicknesses.append(line_info[1])\r\n            masks.append(line_info[2])\r\n\r\n        for i, (y, xs) in enumerate(zip(axis0, axis1)):\r\n            for j, x in enumerate(xs):\r\n                if i == 0:\r\n                    cv2.circle(img, (x, y), radius, (20, 215, 20), int(radius / 2))\r\n                else:\r\n                    graph = graphs[i - 1][j]\r\n                    img[y - half_plot_num:y + half_plot_num, x - half_plot_num:x + half_plot_num] = graph\r\n            if i > 0:\r\n                cv2.putText(img, self._layers[i - 1].name, (12, y - 36), cv2.LINE_AA, 0.6, (0, 0, 0), 1)\r\n        for i, y in enumerate(axis0):\r\n            if i == len(axis0) - 1:\r\n                break\r\n            for j, x in enumerate(axis1[i]):\r\n                new_y = axis0[i + 1]\r\n                whether_sub_layer = isinstance(self._layers[i], SubLayer)\r\n                for k, new_x in enumerate(axis1[i + 1]):\r\n                    if whether_sub_layer and j != k:\r\n                        continue\r\n                    if masks[i][j][k]:\r\n                        cv2.line(img, (x, y + half_plot_num), (new_x, new_y - half_plot_num),\r\n                                 colors[i][j][k], thicknesses[i][j][k])\r\n\r\n        return img\r\n\r\n    # Optimizing Process\r\n\r\n    @NNTiming.timeit(level=4)\r\n    def _init_optimizer(self):\r\n        if not isinstance(self._w_optimizer, Optimizer):\r\n            self._w_optimizer = self._optimizer_factory.get_optimizer_by_name(\r\n                self._w_optimizer, self._weights, self._lr, self._epoch)\r\n        if not isinstance(self._b_optimizer, Optimizer):\r\n            self._b_optimizer = self._optimizer_factory.get_optimizer_by_name(\r\n                self._b_optimizer, self._bias, self._lr, self._epoch)\r\n        if self._w_optimizer.name != self._b_optimizer.name:\r\n            self._optimizer_name = None\r\n        else:\r\n            self._optimizer_name = self._w_optimizer.name\r\n\r\n    @NNTiming.timeit(level=1)\r\n    def _opt(self, i, activation, delta):\r\n        self._weights[i] *= self._regularization_param\r\n        self._weights[i] += self._w_optimizer.run(\r\n            i, activation.view(activation.size()[0], -1).t().mm(delta)\r\n        )\r\n        if self._whether_apply_bias:\r\n            self._bias[i] += self._b_optimizer.run(\r\n                i, torch.sum(delta, dim=0)\r\n            )\r\n\r\n    # API\r\n\r\n    @NNTiming.timeit(level=4, prefix=""[API] "")\r\n    def add(self, layer, *args, **kwargs):\r\n        if isinstance(layer, str):\r\n            # noinspection PyTypeChecker\r\n            self._add_layer(layer, *args, **kwargs)\r\n        else:\r\n            if not isinstance(layer, Layer):\r\n                raise BuildLayerError(""Invalid Layer provided (should be subclass of Layer)"")\r\n            if not self._layers:\r\n                if isinstance(layer, SubLayer):\r\n                    raise BuildLayerError(""Invalid Layer provided (first layer should not be subclass of SubLayer)"")\r\n                if len(layer.shape) != 2:\r\n                    raise BuildLayerError(""Invalid input Layer provided (shape should be {}, {} found)"".format(\r\n                        2, len(layer.shape)\r\n                    ))\r\n                self._layers, self._current_dimension = [layer], layer.shape[1]\r\n                self._update_layer_information(layer)\r\n                self._add_weight(layer.shape)\r\n            else:\r\n                if len(layer.shape) > 2:\r\n                    raise BuildLayerError(""Invalid Layer provided (shape should be {}, {} found)"".format(\r\n                        2, len(layer.shape)\r\n                    ))\r\n                if len(layer.shape) == 2:\r\n                    _current, _next = layer.shape\r\n                    if isinstance(layer, SubLayer):\r\n                        if _next != self._current_dimension:\r\n                            raise BuildLayerError(""Invalid SubLayer provided (shape[1] should be {}, {} found)"".format(\r\n                                self._current_dimension, _next\r\n                            ))\r\n                    elif _current != self._current_dimension:\r\n                        raise BuildLayerError(""Invalid Layer provided (shape[0] should be {}, {} found)"".format(\r\n                            self._current_dimension, _current\r\n                        ))\r\n                    self._add_layer(layer, _current, _next)\r\n\r\n                elif len(layer.shape) == 1:\r\n                    _next = layer.shape[0]\r\n                    layer.shape = (self._current_dimension, _next)\r\n                    self._add_layer(layer, self._current_dimension, _next)\r\n                else:\r\n                    raise LayerError(""Invalid Layer provided (invalid shape \'{}\' found)"".format(layer.shape))\r\n\r\n    # TODO\r\n    @NNTiming.timeit(level=4, prefix=""[API] "")\r\n    def build(self, units=""build""):\r\n        if isinstance(units, str):\r\n            if units == ""build"":\r\n                for name, param in zip(self._layer_names, self._layer_params):\r\n                    self.add(name, *param)\r\n            else:\r\n                raise NotImplementedError(""Invalid param \'{}\' provided to \'build\' method"".format(units))\r\n        else:\r\n            try:\r\n                units = list(units)\r\n            except ValueError as err:\r\n                raise BuildLayerError(err)\r\n            if len(units) < 2:\r\n                raise BuildLayerError(""At least 2 layers are needed"")\r\n            _input_shape = (units[0], units[1])\r\n            self.initialize()\r\n            self.add(ReLU(_input_shape))\r\n            for unit_num in units[2:-1]:\r\n                self.add(ReLU((unit_num,)))\r\n            self.add(""CrossEntropy"", (units[-1],))\r\n\r\n    @NNTiming.timeit(level=4, prefix=""[API] "")\r\n    def preview(self):\r\n        if not self._layers:\r\n            rs = ""None""\r\n        else:\r\n            rs = (\r\n                ""Input  :  {:<10s} - {}\\n"".format(""Dimension"", self._layers[0].shape[0]) +\r\n                ""\\n"".join([\r\n                    ""Layer  :  {:<10s} - {} {}"".format(\r\n                        layer.name, layer.shape[1], layer.description\r\n                    ) if isinstance(layer, SubLayer) else\r\n                    ""Layer  :  {:<10s} - {}"".format(\r\n                        layer.name, layer.shape[1]\r\n                    ) for layer in self._layers[:-1]\r\n                ]) + ""\\nCost   :  {:<16s}"".format(str(self._layers[-1]))\r\n            )\r\n        print(""="" * 30 + ""\\n"" + ""Structure\\n"" + ""-"" * 30 + ""\\n"" + rs + ""\\n"" + ""-"" * 30 + ""\\n"")\r\n\r\n    @NNTiming.timeit(level=1, prefix=""[API] "")\r\n    def fit(self,\r\n            x, y, x_test=None, y_test=None,\r\n            batch_size=128, record_period=1, train_only=False,\r\n            optimizer=None, w_optimizer=None, b_optimizer=None,\r\n            lr=0.001, lb=0.001, epoch=20, weight_scale=1, apply_bias=True,\r\n            show_loss=True, metrics=None, do_log=True, verbose=None,\r\n            visualize=False, visualize_setting=None,\r\n            draw_weights=False, animation_params=None):\r\n        self._lr, self._epoch = lr, epoch\r\n        for weight in self._weights:\r\n            weight *= weight_scale\r\n        if not self._w_optimizer or not self._b_optimizer:\r\n            if not self._optimizer_name:\r\n                if optimizer is None:\r\n                    optimizer = ""Adam""\r\n                self._w_optimizer = optimizer if w_optimizer is None else w_optimizer\r\n                self._b_optimizer = optimizer if b_optimizer is None else b_optimizer\r\n            else:\r\n                if not self._w_optimizer:\r\n                    self._w_optimizer = self._optimizer_name\r\n                if not self._b_optimizer:\r\n                    self._b_optimizer = self._optimizer_name\r\n        self._init_optimizer()\r\n        assert isinstance(self._w_optimizer, Optimizer) and isinstance(self._b_optimizer, Optimizer)\r\n        print()\r\n        print(""="" * 30)\r\n        print(""Optimizers"")\r\n        print(""-"" * 30)\r\n        print(""w: {}\\nb: {}"".format(self._w_optimizer, self._b_optimizer))\r\n        print(""-"" * 30)\r\n        if not self._layers:\r\n            raise BuildNetworkError(""Please provide layers before fitting data"")\r\n        if y.shape[1] != self._current_dimension:\r\n            raise BuildNetworkError(""Output layer\'s shape should be {}, {} found"".format(\r\n                self._current_dimension, y.shape[1]))\r\n\r\n        x = torch.from_numpy(np.asarray(x, dtype=np.float32))\r\n        y = torch.from_numpy(np.asarray(y, dtype=np.float32))\r\n        if x_test is not None and y_test is not None:\r\n            x_test = torch.from_numpy(np.asarray(x_test, dtype=np.float32))\r\n            y_test = torch.from_numpy(np.asarray(y_test, dtype=np.float32))\r\n        (x_train, x_test), (y_train, y_test) = self._split_data(\r\n            x, y, x_test, y_test, train_only)\r\n        train_len = len(x_train)\r\n        batch_size = min(batch_size, train_len)\r\n        do_random_batch = train_len > batch_size\r\n        train_repeat = 1 if not do_random_batch else int(train_len / batch_size) + 1\r\n        self._regularization_param = 1 - lb * lr / batch_size\r\n        self._get_min_max(x_train, y_train)\r\n\r\n        self._metrics = [""acc""] if metrics is None else metrics\r\n        for i, metric in enumerate(self._metrics):\r\n            if isinstance(metric, str):\r\n                if metric not in self._available_metrics:\r\n                    raise BuildNetworkError(""Metric \'{}\' is not implemented"".format(metric))\r\n                self._metrics[i] = self._available_metrics[metric]\r\n        self._metric_names = [_m.__name__ for _m in self._metrics]\r\n\r\n        self._logs = {\r\n            name: [[] for _ in range(len(self._metrics) + 1)] for name in (""train"", ""cv"", ""test"")\r\n        }\r\n        if verbose is not None:\r\n            self.verbose = verbose\r\n\r\n        layer_width = len(self._layers)\r\n        self._whether_apply_bias = apply_bias\r\n\r\n        bar = ProgressBar(max_value=max(1, epoch // record_period), name=""Epoch"", start=False)\r\n        if self.verbose >= NNVerbose.EPOCH:\r\n            bar.start()\r\n        img, ims = None, []\r\n\r\n        if draw_weights:\r\n            weight_trace = [[[org] for org in weight] for weight in self._weights]\r\n        else:\r\n            weight_trace = []\r\n\r\n        *animation_properties, animation_params = self._get_animation_params(animation_params)\r\n        sub_bar = ProgressBar(max_value=train_repeat * record_period - 1, name=""Iteration"", start=False)\r\n        for counter in range(epoch):\r\n            self._w_optimizer.update()\r\n            self._b_optimizer.update()\r\n            if self.verbose >= NNVerbose.ITER and counter % record_period == 0:\r\n                sub_bar.start()\r\n            for _ in range(train_repeat):\r\n                if do_random_batch:\r\n                    batch = torch.randperm(train_len)[:batch_size]\r\n                    x_batch, y_batch = x_train[batch], y_train[batch]\r\n                else:\r\n                    x_batch, y_batch = x_train, y_train\r\n                activations = self._get_activations(x_batch)\r\n\r\n                deltas = [self._layers[-1].bp_first(y_batch, activations[-1])]\r\n                for i in range(-1, -len(activations), -1):\r\n                    deltas.append(self._layers[i - 1].bp(activations[i - 1], self._weights[i], deltas[-1]))\r\n\r\n                for i in range(layer_width - 1, 0, -1):\r\n                    if not isinstance(self._layers[i], SubLayer):\r\n                        self._opt(i, activations[i - 1], deltas[layer_width - i - 1])\r\n                self._opt(0, x_batch, deltas[-1])\r\n\r\n                if draw_weights:\r\n                    for i, weight in enumerate(self._weights):\r\n                        for j, new_weight in enumerate(weight.copy()):\r\n                            weight_trace[i][j].append(new_weight)\r\n                if self.verbose >= NNVerbose.DEBUG:\r\n                    pass\r\n                if self.verbose >= NNVerbose.ITER:\r\n                    if sub_bar.update() and self.verbose >= NNVerbose.METRICS_DETAIL:\r\n                        self._append_log(x, y, ""train"", get_loss=show_loss)\r\n                        self._append_log(x_test, y_test, ""cv"", get_loss=show_loss)\r\n                        self._print_metric_logs(show_loss, ""train"")\r\n                        self._print_metric_logs(show_loss, ""cv"")\r\n            if self.verbose >= NNVerbose.ITER:\r\n                sub_bar.update()\r\n            self._handle_animation(\r\n                counter, x, y, ims, animation_params, *animation_properties,\r\n                img=self._draw_2d_network(**animation_params), name=""Neural Network""\r\n            )\r\n            if do_log:\r\n                self._append_log(x, y, ""train"", get_loss=show_loss)\r\n                self._append_log(x_test, y_test, ""cv"", get_loss=show_loss)\r\n            if (counter + 1) % record_period == 0:\r\n                if do_log and self.verbose >= NNVerbose.METRICS:\r\n                    self._print_metric_logs(show_loss, ""train"")\r\n                    self._print_metric_logs(show_loss, ""cv"")\r\n                if visualize:\r\n                    if visualize_setting is None:\r\n                        self.visualize2d(x_test, y_test)\r\n                    else:\r\n                        self.visualize2d(x_test, y_test, *visualize_setting)\r\n                if self.verbose >= NNVerbose.EPOCH:\r\n                    bar.update(counter // record_period + 1)\r\n                    if self.verbose >= NNVerbose.ITER:\r\n                        sub_bar = ProgressBar(max_value=train_repeat * record_period - 1, name=""Iteration"", start=False)\r\n\r\n        if do_log:\r\n            self._append_log(x_test, y_test, ""test"", get_loss=show_loss)\r\n        if img is not None:\r\n            cv2.waitKey(0)\r\n            cv2.destroyAllWindows()\r\n        if draw_weights:\r\n            ts = np.arange(epoch * train_repeat + 1)\r\n            for i, weight in enumerate(self._weights):\r\n                plt.figure()\r\n                for j in range(len(weight)):\r\n                    plt.plot(ts, weight_trace[i][j])\r\n                plt.title(""Weights toward layer {} ({})"".format(i + 1, self._layers[i].name))\r\n                plt.show()\r\n        self._handle_mp4(ims, animation_properties, ""NN"")\r\n        return self._logs\r\n\r\n    # TODO\r\n    @NNTiming.timeit(level=2, prefix=""[API] "")\r\n    def save(self, path=None, name=None, overwrite=True):\r\n        path = os.path.join(""Models"", ""Cache"") if path is None else os.path.join(""Models"", path)\r\n        name = ""Model.nn"" if name is None else name\r\n        if not os.path.exists(path):\r\n            os.makedirs(path)\r\n        _dir = os.path.join(path, name)\r\n        if not overwrite and os.path.isfile(_dir):\r\n            _count = 1\r\n            _new_dir = _dir + ""({})"".format(_count)\r\n            while os.path.isfile(_new_dir):\r\n                _count += 1\r\n                _new_dir = _dir + ""({})"".format(_count)\r\n            _dir = _new_dir\r\n        print()\r\n        print(""="" * 60)\r\n        print(""Saving Model to {}..."".format(_dir))\r\n        print(""-"" * 60)\r\n        with open(_dir, ""wb"") as file:\r\n            pickle.dump({\r\n                ""structures"": {\r\n                    ""_layer_names"": self.layer_names,\r\n                    ""_layer_params"": self._layer_params,\r\n                    ""_cost_layer"": self._layers[-1].name,\r\n                    ""_next_dimension"": self._current_dimension\r\n                },\r\n                ""params"": {\r\n                    ""_logs"": self._logs,\r\n                    ""_metric_names"": self._metric_names,\r\n                    ""_weights"": self._weights,\r\n                    ""_bias"": self._bias,\r\n                    ""_optimizer_name"": self._optimizer_name,\r\n                    ""_w_optimizer"": self._w_optimizer,\r\n                    ""_b_optimizer"": self._b_optimizer,\r\n                    ""layer_special_params"": self.layer_special_params,\r\n                }\r\n            }, file)\r\n        print(""Done"")\r\n        print(""="" * 60)\r\n\r\n    # TODO\r\n    @NNTiming.timeit(level=2, prefix=""[API] "")\r\n    def load(self, path=os.path.join(""Models"", ""Cache"", ""Model.nn"")):\r\n        self.initialize()\r\n        try:\r\n            with open(path, ""rb"") as file:\r\n                dic = pickle.load(file)\r\n                for key, value in dic[""structures""].items():\r\n                    setattr(self, key, value)\r\n                self.build()\r\n                for key, value in dic[""params""].items():\r\n                    setattr(self, key, value)\r\n                self._init_optimizer()\r\n                for i in range(len(self._metric_names) - 1, -1, -1):\r\n                    name = self._metric_names[i]\r\n                    if name not in self._available_metrics:\r\n                        self._metric_names.pop(i)\r\n                    else:\r\n                        self._metrics.insert(0, self._available_metrics[name])\r\n                print()\r\n                print(""="" * 30)\r\n                print(""Model restored"")\r\n                print(""="" * 30)\r\n                return dic\r\n        except Exception as err:\r\n            raise BuildNetworkError(""Failed to load Network ({}), structure initialized"".format(err))\r\n\r\n    @NNTiming.timeit(level=4, prefix=""[API] "")\r\n    def predict(self, x, get_raw_results=False, **kwargs):\r\n        if not isinstance(x, torch.Tensor):\r\n            x = torch.Tensor(np.asarray(x, dtype=np.float32))\r\n        if len(x.size()) == 1:\r\n            x = x.view(1, -1)\r\n        y_pred = self._get_prediction(x).numpy()\r\n        return y_pred if get_raw_results else np.argmax(y_pred, axis=1)\r\n\r\n    def draw_results(self):\r\n        metrics_log, loss_log = {}, {}\r\n        for key, value in sorted(self._logs.items()):\r\n            metrics_log[key], loss_log[key] = value[:-1], value[-1]\r\n\r\n        for i, name in enumerate(sorted(self._metric_names)):\r\n            plt.figure()\r\n            plt.title(""Metric Type: {}"".format(name))\r\n            for key, log in sorted(metrics_log.items()):\r\n                if key == ""test"":\r\n                    continue\r\n                xs = np.arange(len(log[i])) + 1\r\n                plt.plot(xs, log[i], label=""Data Type: {}"".format(key))\r\n            plt.legend(loc=4)\r\n            plt.show()\r\n            plt.close()\r\n\r\n        plt.figure()\r\n        plt.title(""Cost"")\r\n        for key, loss in sorted(loss_log.items()):\r\n            if key == ""test"":\r\n                continue\r\n            xs = np.arange(len(loss)) + 1\r\n            plt.plot(xs, loss, label=""Data Type: {}"".format(key))\r\n        plt.legend()\r\n        plt.show()\r\n\r\n    @staticmethod\r\n    def fuck_pycharm_warning():\r\n        print(Axes3D.acorr)\r\n'"
NN/PyTorch/__Dev/Layers.py,0,"b'import torch.nn.functional as F\r\n\r\nfrom NN.Errors import *\r\nfrom NN.PyTorch.Auto.Optimizers import *\r\n\r\n# TODO: Support \'SAME\' padding\r\n\r\n\r\n# Abstract Layers\r\n\r\nclass Layer:\r\n    LayerTiming = Timing()\r\n\r\n    def __init__(self, shape):\r\n        """"""\r\n        :param shape: shape[0] = units of previous layer\r\n                      shape[1] = units of current layer (self)\r\n        """"""\r\n        self._shape = shape\r\n        self.parent = None\r\n        self.child = None\r\n        self.is_fc = False\r\n        self.is_fc_base = False\r\n        self.is_sub_layer = False\r\n        self._last_sub_layer = None\r\n\r\n    def __str__(self):\r\n        return self.__class__.__name__\r\n\r\n    def __repr__(self):\r\n        return str(self)\r\n\r\n    @property\r\n    def name(self):\r\n        return str(self)\r\n\r\n    @property\r\n    def shape(self):\r\n        return self._shape\r\n\r\n    @shape.setter\r\n    def shape(self, value):\r\n        self._shape = value\r\n\r\n    @property\r\n    def params(self):\r\n        return self._shape,\r\n\r\n    @property\r\n    def special_params(self):\r\n        return\r\n\r\n    def set_special_params(self, dic):\r\n        for key, value in dic.items():\r\n            setattr(self, key, value)\r\n\r\n    @property\r\n    def root(self):\r\n        return self\r\n\r\n    @root.setter\r\n    def root(self, value):\r\n        raise BuildLayerError(""Setting Layer\'s root is not permitted"")\r\n\r\n    @property\r\n    def last_sub_layer(self):\r\n        child = self.child\r\n        if not child:\r\n            return None\r\n        while child.child:\r\n            child = child.child\r\n        return child\r\n\r\n    @last_sub_layer.setter\r\n    def last_sub_layer(self, value):\r\n            self._last_sub_layer = value\r\n\r\n    # Core\r\n\r\n    def derivative(self, y, delta=None):\r\n        return self._derivative(y, delta)\r\n\r\n    @LayerTiming.timeit(level=1, prefix=""[Core] "")\r\n    def activate(self, x, w, bias=None, predict=False):\r\n        if self.is_fc:\r\n            x = x.view(x.size()[0], -1)\r\n        if self.is_sub_layer:\r\n            if bias is None:\r\n                return self._activate(x, predict)\r\n            return self._activate(x + bias, predict)\r\n        mul = x.mm(w)\r\n        if bias is None:\r\n            return self._activate(mul, predict)\r\n        return self._activate(x.mm(w) + bias.expand_as(mul), predict)\r\n\r\n    @LayerTiming.timeit(level=1, prefix=""[Core] "")\r\n    def bp(self, y, w, prev_delta):\r\n        if self.child is not None and isinstance(self.child, SubLayer):\r\n            if not isinstance(self, SubLayer):\r\n                return prev_delta\r\n            return self._derivative(y, prev_delta)\r\n        if isinstance(self, SubLayer):\r\n            return self._derivative(y, prev_delta.mm(w.t()) * self._root.derivative(y))\r\n        return prev_delta.mm(w.t()) * self._derivative(y)\r\n\r\n    def _activate(self, x, predict):\r\n        pass\r\n\r\n    def _derivative(self, y, delta=None):\r\n        pass\r\n\r\n    # Util\r\n\r\n    @staticmethod\r\n    @LayerTiming.timeit(level=2, prefix=""[Core Util] "")\r\n    def safe_exp(y):\r\n        return torch.exp(y - torch.max(y, dim=1)[0].expand_as(y))\r\n\r\n\r\nclass SubLayer(Layer):\r\n    def __init__(self, parent, shape):\r\n        super(SubLayer, self).__init__(shape)\r\n        self.parent = parent\r\n        parent.child = self\r\n        self._root = None\r\n        self.description = """"\r\n\r\n    @property\r\n    def root(self):\r\n        parent = self.parent\r\n        while parent.parent:\r\n            parent = parent.parent\r\n        return parent\r\n\r\n    @root.setter\r\n    def root(self, value):\r\n        self._root = value\r\n\r\n    def get_params(self):\r\n        pass\r\n\r\n    @property\r\n    def params(self):\r\n        return self.get_params()\r\n\r\n    def _activate(self, x, predict):\r\n        raise NotImplementedError(""Please implement activation function for "" + self.name)\r\n\r\n    def _derivative(self, y, delta=None):\r\n        raise NotImplementedError(""Please implement derivative function for "" + self.name)\r\n\r\n\r\nclass ConvLayer(Layer):\r\n    LayerTiming = Timing()\r\n\r\n    def __init__(self, shape, stride=1, padding=0, parent=None):\r\n        """"""\r\n        :param shape:    shape[0] = shape of previous layer           c x h x w\r\n                         shape[1] = shape of current layer\'s weight   f x c x h x w\r\n        :param stride:   stride\r\n        :param padding:  zero-padding\r\n        """"""\r\n        if parent is not None:\r\n            parent = parent.root if parent.is_sub_layer else parent\r\n            shape = parent.shape\r\n        Layer.__init__(self, shape)\r\n        self._stride, self._padding = stride, padding\r\n        if len(shape) == 1:\r\n            self.n_channels = self.n_filters = self.out_h = self.out_w = None\r\n        else:\r\n            self.feed_shape(shape)\r\n        self.x_cache = self.x_col_cache = None\r\n        self.inner_weight = None\r\n\r\n    def feed_shape(self, shape):\r\n        self._shape = shape\r\n        self.n_channels, height, width = shape[0]\r\n        self.n_filters, filter_height, filter_width = shape[1]\r\n        full_height, full_width = width + 2 * self._padding, height + 2 * self._padding\r\n        if (\r\n            (full_height - filter_height) % self._stride != 0 or\r\n            (full_width - filter_width) % self._stride != 0\r\n        ):\r\n            raise BuildLayerError(\r\n                ""Weight shape does not work, ""\r\n                ""shape: {} - stride: {} - padding: {} not compatible with {}"".format(\r\n                    self._shape[1][1:], self._stride, self._padding, (height, width)\r\n                ))\r\n        self.out_h = int((height + 2 * self._padding - filter_height) / self._stride) + 1\r\n        self.out_w = int((width + 2 * self._padding - filter_width) / self._stride) + 1\r\n\r\n    @property\r\n    def params(self):\r\n        return self._shape, self._stride, self._padding\r\n\r\n    @property\r\n    def stride(self):\r\n        return self._stride\r\n\r\n    @property\r\n    def padding(self):\r\n        return self._padding\r\n\r\n    def _activate(self, x, predict):\r\n        raise NotImplementedError(""Please implement activation function for "" + self.name)\r\n\r\n    def _derivative(self, y, delta=None):\r\n        raise NotImplementedError(""Please implement derivative function for "" + self.name)\r\n\r\n\r\nclass ConvPoolLayer(ConvLayer):\r\n    LayerTiming = Timing()\r\n\r\n    def __init__(self, shape, stride=1, padding=0):\r\n        """"""\r\n        :param shape:    shape[0] = shape of previous layer           c x h x w\r\n                         shape[1] = shape of pool window              c x ph x pw\r\n        :param stride:   stride\r\n        :param padding:  zero-padding\r\n        """"""\r\n        ConvLayer.__init__(self, shape, stride, padding)\r\n        self._pool_cache = {}\r\n\r\n    @property\r\n    def params(self):\r\n        return (self._shape[0], self._shape[1][1:]), self._stride, self._padding\r\n\r\n    def feed_shape(self, shape):\r\n        if len(shape[1]) == 2:\r\n            shape = (shape[0], (shape[0][0], *shape[1]))\r\n        ConvLayer.feed_shape(self, shape)\r\n\r\n    @LayerTiming.timeit(level=1, prefix=""[Core] "")\r\n    def activate(self, x, w, bias=None, predict=False):\r\n        return self._activate(x, w, bias, predict)\r\n\r\n    def _activate(self, x, *args):\r\n        raise NotImplementedError(""Please implement activation function for "" + self.name)\r\n\r\n    def _derivative(self, y, *args):\r\n        raise NotImplementedError(""Please implement derivative function for "" + self.name)\r\n\r\n    @LayerTiming.timeit(level=1, prefix=""[Core] "")\r\n    def bp(self, y, w, prev_delta):\r\n        return self._derivative(y, w, prev_delta)\r\n\r\n\r\n# noinspection PyProtectedMember\r\nclass ConvMeta(type):\r\n    def __new__(mcs, *args, **kwargs):\r\n        name, bases, attr = args[:3]\r\n        conv_layer, layer = bases\r\n\r\n        def __init__(self, shape, stride=1, padding=0):\r\n            conv_layer.__init__(self, shape, stride, padding)\r\n\r\n        def _activate(self, x, w, bias, predict):\r\n            self.x_cache, self.inner_weight = x, w\r\n            n, n_channels, height, width = x.shape\r\n            n_filters, _, filter_height, filter_width = w.shape\r\n\r\n            p, sd = self._padding, self._stride\r\n            x_padded = F.pad(x, (p, p, p, p))\r\n\r\n            height += 2 * p\r\n            width += 2 * p\r\n\r\n\r\n            return layer._activate(self, res.transpose(1, 0, 2, 3), predict)\r\n\r\n        def activate(self, x, w, bias=None, predict=False):\r\n            return self.LayerTiming.timeit(level=1, func_name=""activate"", cls_name=name, prefix=""[Core] "")(\r\n                _activate)(self, x, w, bias, predict)\r\n\r\n        for key, value in locals().items():\r\n            if str(value).find(""function"") >= 0 or str(value).find(""property""):\r\n                attr[key] = value\r\n\r\n        return type(name, bases, attr)\r\n\r\n\r\n# noinspection PyProtectedMember\r\nclass ConvSubMeta(type):\r\n    def __new__(mcs, *args, **kwargs):\r\n        name, bases, attr = args[:3]\r\n        conv_layer, sub_layer = bases\r\n\r\n        def __init__(self, parent, shape, *_args, **_kwargs):\r\n            conv_layer.__init__(self, None, parent=parent, **_kwargs)\r\n            self.out_h, self.out_w = parent.out_h, parent.out_w\r\n            sub_layer.__init__(self, parent, shape, *_args, **_kwargs)\r\n            self._shape = ((shape[0][0], self.out_h, self.out_w), shape[0])\r\n            if name == ""ConvNorm"":\r\n                self.gamma, self.beta = np.ones(self.n_filters), np.zeros(self.n_filters)\r\n                self.init_optimizers()\r\n\r\n        def _activate(self, x, predict):\r\n            n, n_channels, height, width = x.shape\r\n            out = sub_layer._activate(self, x.transpose(0, 2, 3, 1).reshape(-1, n_channels), predict)\r\n            return out.reshape(n, height, width, n_channels).transpose(0, 3, 1, 2)\r\n\r\n        def _derivative(self, y, w, delta=None):\r\n            if self.is_fc_base:\r\n                delta = delta.dot(w.T).reshape(y.shape)\r\n            n, n_channels, height, width = delta.shape\r\n            # delta_new = delta.transpose(0, 2, 3, 1).reshape(-1, n_channels)\r\n            dx = sub_layer._derivative(self, y, delta.transpose(0, 2, 3, 1).reshape(-1, n_channels))\r\n            return dx.reshape(n, height, width, n_channels).transpose(0, 3, 1, 2)\r\n\r\n        # noinspection PyUnusedLocal\r\n        def activate(self, x, w, bias=None, predict=False):\r\n            return self.LayerTiming.timeit(level=1, func_name=""activate"", cls_name=name, prefix=""[Core] "")(\r\n                _activate)(self, x, predict)\r\n\r\n        def bp(self, y, w, prev_delta):\r\n            if isinstance(prev_delta, tuple):\r\n                prev_delta = prev_delta[0]\r\n            return self.LayerTiming.timeit(level=1, func_name=""bp"", cls_name=name, prefix=""[Core] "")(\r\n                _derivative)(self, y, w, prev_delta)\r\n\r\n        @property\r\n        def params(self):\r\n            return sub_layer.get_params(self)\r\n\r\n        for key, value in locals().items():\r\n            if str(value).find(""function"") >= 0 or str(value).find(""property""):\r\n                attr[key] = value\r\n\r\n        return type(name, bases, attr)\r\n\r\n\r\n# Activation Layers\r\n\r\nclass Tanh(Layer):\r\n    def _activate(self, x, predict):\r\n        return F.tanh(x)\r\n\r\n\r\nclass Sigmoid(Layer):\r\n    def _activate(self, x, predict):\r\n        return F.sigmoid(x)\r\n\r\n\r\nclass ELU(Layer):\r\n    def _activate(self, x, predict):\r\n        return F.elu(x)\r\n\r\n\r\nclass ReLU(Layer):\r\n    def _activate(self, x, predict):\r\n        return F.relu(x)\r\n\r\n\r\nclass Softplus(Layer):\r\n    def _activate(self, x, predict):\r\n        return F.softplus(x)\r\n\r\n\r\nclass Identical(Layer):\r\n    def _activate(self, x, predict):\r\n        return x\r\n\r\n\r\n# Convolution Layers\r\n\r\nclass ConvTanh(ConvLayer, Tanh, metaclass=ConvMeta):\r\n    pass\r\n\r\n\r\nclass ConvSigmoid(ConvLayer, Sigmoid, metaclass=ConvMeta):\r\n    pass\r\n\r\n\r\nclass ConvELU(ConvLayer, ELU, metaclass=ConvMeta):\r\n    pass\r\n\r\n\r\nclass ConvReLU(ConvLayer, ReLU, metaclass=ConvMeta):\r\n    pass\r\n\r\n\r\nclass ConvSoftplus(ConvLayer, Softplus, metaclass=ConvMeta):\r\n    pass\r\n\r\n\r\nclass ConvIdentical(ConvLayer, Identical, metaclass=ConvMeta):\r\n    pass\r\n\r\n\r\n# Pooling Layers\r\n\r\nclass MaxPool(ConvPoolLayer):\r\n    def _activate(self, x, *args):\r\n        self.x_cache = x\r\n        sd = self._stride\r\n        n, n_channels, height, width = x.shape\r\n        # noinspection PyTupleAssignmentBalance\r\n        _, pool_height, pool_width = self._shape[1]\r\n        same_size = pool_height == pool_width == sd\r\n        tiles = height % pool_height == 0 and width % pool_width == 0\r\n        if same_size and tiles:\r\n            x_reshaped = x.reshape(n, n_channels, int(height / pool_height), pool_height,\r\n                                   int(width / pool_width), pool_width)\r\n            self._pool_cache[""x_reshaped""] = x_reshaped\r\n            out = x_reshaped.max(axis=3).max(axis=4)\r\n            self._pool_cache[""method""] = ""reshape""\r\n        else:\r\n            out = np.zeros((n, n_channels, self.out_h, self.out_w))\r\n            for i in range(n):\r\n                for j in range(n_channels):\r\n                    for k in range(self.out_h):\r\n                        for l in range(self.out_w):\r\n                            window = x[i, j, k * sd:pool_height + k * sd, l * sd:pool_width + l * sd]\r\n                            out[i, j, k, l] = np.max(window)\r\n            self._pool_cache[""method""] = ""original""\r\n        return out\r\n\r\n    def _derivative(self, y, *args):\r\n        w, prev_delta = args\r\n        if isinstance(prev_delta, tuple):\r\n            prev_delta = prev_delta[0]\r\n        if self.is_fc_base:\r\n            delta = prev_delta.dot(w.T).reshape(y.shape)\r\n        else:\r\n            delta = prev_delta\r\n        method = self._pool_cache[""method""]\r\n        if method == ""reshape"":\r\n            x_reshaped_cache = self._pool_cache[""x_reshaped""]\r\n            dx_reshaped = np.zeros_like(x_reshaped_cache)\r\n            out_newaxis = y[..., None, :, None]\r\n            mask = (x_reshaped_cache == out_newaxis)\r\n            dout_newaxis = delta[..., None, :, None]\r\n            dout_broadcast, _ = np.broadcast_arrays(dout_newaxis, dx_reshaped)\r\n            dx_reshaped[mask] = dout_broadcast[mask]\r\n            # noinspection PyTypeChecker\r\n            dx_reshaped /= np.sum(mask, axis=(3, 5), keepdims=True)\r\n            dx = dx_reshaped.reshape(self.x_cache.shape)\r\n        elif method == ""original"":\r\n            sd = self._stride\r\n            dx = np.zeros_like(self.x_cache)\r\n            n, n_channels, *_ = self.x_cache.shape\r\n            # noinspection PyTupleAssignmentBalance\r\n            _, pool_height, pool_width = self._shape[1]\r\n            for i in range(n):\r\n                for j in range(n_channels):\r\n                    for k in range(self.out_h):\r\n                        for l in range(self.out_w):\r\n                            window = self.x_cache[i, j, k*sd:pool_height+k*sd, l*sd:pool_width+l*sd]\r\n                            # noinspection PyTypeChecker\r\n                            dx[i, j, k*sd:pool_height+k*sd, l*sd:pool_width+l*sd] = (\r\n                                window == np.max(window)) * delta[i, j, k, l]\r\n        else:\r\n            raise LayerError(""Undefined pooling method \'{}\' found"".format(method))\r\n        return dx, None, None\r\n\r\n\r\n# Special Layer\r\n\r\nclass Dropout(SubLayer):\r\n    def __init__(self, parent, shape, prob=0.5):\r\n        if prob < 0 or prob >= 1:\r\n            raise BuildLayerError(""Probability of Dropout should be a positive float smaller than 1"")\r\n        SubLayer.__init__(self, parent, shape)\r\n        self._prob = prob\r\n        self.description = ""(Drop prob: {})"".format(prob)\r\n\r\n    def get_params(self):\r\n        return self._prob,\r\n\r\n    def _activate(self, x, predict):\r\n        F.dropout(x, self._prob, not predict)\r\n\r\n\r\nclass Normalize(SubLayer):\r\n    def __init__(self, parent, shape, lr=0.001, eps=1e-8, momentum=0.9, optimizers=None):\r\n        SubLayer.__init__(self, parent, shape)\r\n        self.sample_mean, self.sample_var = None, None\r\n        self.running_mean, self.running_var = None, None\r\n        self.x_cache, self.x_normalized_cache = None, None\r\n        self._lr, self._eps = lr, eps\r\n        if optimizers is None:\r\n            self._g_optimizer, self._b_optimizer = Adam(self._lr), Adam(self._lr)\r\n        else:\r\n            self._g_optimizer, self._b_optimizer = optimizers\r\n        self.gamma, self.beta = torch.ones(self.shape[1]), torch.zeros(self.shape[1])\r\n        self._momentum = momentum\r\n        self.init_optimizers()\r\n        self.description = ""(lr: {}, eps: {}, momentum: {}, optimizer: ({}, {}))"".format(\r\n            lr, eps, momentum, self._g_optimizer.name, self._b_optimizer.name\r\n        )\r\n\r\n    def get_params(self):\r\n        return self._lr, self._eps, self._momentum, (self._g_optimizer.name, self._b_optimizer.name)\r\n\r\n    @property\r\n    def special_params(self):\r\n        return {\r\n            ""gamma"": self.gamma, ""beta"": self.beta,\r\n            ""running_mean"": self.running_mean, ""running_var"": self.running_var,\r\n            ""_g_optimizer"": self._g_optimizer, ""_b_optimizer"": self._b_optimizer\r\n        }\r\n\r\n    def init_optimizers(self):\r\n        _opt_fac = OptFactory()\r\n        if not isinstance(self._g_optimizer, Optimizer):\r\n            self._g_optimizer = _opt_fac.get_optimizer_by_name(\r\n                self._g_optimizer, None, self._lr, None\r\n            )\r\n        if not isinstance(self._b_optimizer, Optimizer):\r\n            self._b_optimizer = _opt_fac.get_optimizer_by_name(\r\n                self._b_optimizer, None, self._lr, None\r\n            )\r\n        self._g_optimizer.feed_variables([self.gamma])\r\n        self._b_optimizer.feed_variables([self.beta])\r\n\r\n    # noinspection PyTypeChecker\r\n    def _activate(self, x, predict):\r\n        if self.running_mean is None or self.running_var is None:\r\n            self.running_mean, self.running_var = torch.zeros(x.size()[1]), torch.zeros(x.size()[1])\r\n        if not predict:\r\n            self.sample_mean = torch.mean(x, dim=0)\r\n            self.sample_var = torch.var(x, dim=0)\r\n            x_normalized = (x - self.sample_mean) / torch.sqrt(self.sample_var + self._eps)\r\n            self.x_cache, self.x_normalized_cache = x, x_normalized\r\n            out = self.gamma * x_normalized + self.beta\r\n            self.running_mean = self._momentum * self.running_mean + (1 - self._momentum) * self.sample_mean\r\n            self.running_var = self._momentum * self.running_var + (1 - self._momentum) * self.sample_var\r\n        else:\r\n            x_normalized = (x - self.running_mean) / torch.sqrt(self.running_var + self._eps)\r\n            out = self.gamma * x_normalized + self.beta\r\n        return out\r\n\r\n\r\nclass ConvDrop(ConvLayer, Dropout, metaclass=ConvSubMeta):\r\n    pass\r\n\r\n\r\nclass ConvNorm(ConvLayer, Normalize, metaclass=ConvSubMeta):\r\n    pass\r\n\r\n\r\n# Cost Layer\r\n\r\nclass CostLayer(Layer):\r\n    def __init__(self, shape, cost_function=""MSE"", transform=None):\r\n        super(CostLayer, self).__init__(shape)\r\n        self._available_cost_functions = {\r\n            ""MSE"": CostLayer._mse,\r\n            ""CrossEntropy"": CostLayer._cross_entropy\r\n        }\r\n        self._available_transform_functions = {\r\n            ""Softmax"": CostLayer._softmax,\r\n            ""Sigmoid"": CostLayer._sigmoid\r\n        }\r\n        if cost_function not in self._available_cost_functions:\r\n            raise LayerError(""Cost function \'{}\' not implemented"".format(cost_function))\r\n        self._cost_function_name = cost_function\r\n        self._cost_function = self._available_cost_functions[cost_function]\r\n        if transform is None and cost_function == ""CrossEntropy"":\r\n            self._transform = ""Softmax""\r\n            self._transform_function = CostLayer._softmax\r\n        else:\r\n            self._transform = transform\r\n            self._transform_function = self._available_transform_functions.get(transform, None)\r\n\r\n    def __str__(self):\r\n        return self._cost_function_name\r\n\r\n    def _activate(self, x, predict):\r\n        if self._transform_function is None:\r\n            return x\r\n        return self._transform_function(x)\r\n\r\n    @property\r\n    def calculate(self):\r\n        return lambda y, y_pred: self._cost_function(y, y_pred)\r\n\r\n    @property\r\n    def cost_function(self):\r\n        return self._cost_function_name\r\n\r\n    @cost_function.setter\r\n    def cost_function(self, value):\r\n        if value not in self._available_cost_functions:\r\n            raise LayerError(""\'{}\' is not implemented"".format(value))\r\n        self._cost_function_name = value\r\n        self._cost_function = self._available_cost_functions[value]\r\n\r\n    def set_cost_function_derivative(self, func, name=None):\r\n        name = ""Custom Cost Function"" if name is None else name\r\n        self._cost_function_name = name\r\n        self._cost_function = func\r\n\r\n    # Transform Functions\r\n\r\n    @staticmethod\r\n    def _softmax(y):\r\n        return F.log_softmax(y)\r\n\r\n    @staticmethod\r\n    def _sigmoid(y):\r\n        return F.sigmoid(y)\r\n\r\n    # Cost Functions\r\n\r\n    @staticmethod\r\n    def _mse(y, y_pred):\r\n        return torch.nn.MSELoss()(y_pred, y)\r\n\r\n    @staticmethod\r\n    def _cross_entropy(y, y_pred):\r\n        return F.cross_entropy(\r\n            y_pred, torch.squeeze(torch.max(y, dim=1)[1], 1)\r\n        )\r\n\r\n    \r\n# Factory\r\n\r\nclass LayerFactory:\r\n    available_root_layers = {\r\n        ""Tanh"": Tanh, ""Sigmoid"": Sigmoid,\r\n        ""ELU"": ELU, ""ReLU"": ReLU, ""Softplus"": Softplus,\r\n        ""Identical"": Identical,\r\n        ""ConvTanh"": ConvTanh, ""ConvSigmoid"": ConvSigmoid,\r\n        ""ConvELU"": ConvELU, ""ConvReLU"": ConvReLU, ""ConvSoftplus"": ConvSoftplus,\r\n        ""ConvIdentical"": ConvIdentical,\r\n        ""MaxPool"": MaxPool,\r\n        ""MSE"": CostLayer, ""CrossEntropy"": CostLayer\r\n    }\r\n    available_sub_layers = {\r\n        ""Dropout"", ""Normalize"", ""ConvNorm"", ""ConvDrop""\r\n    }\r\n    available_cost_functions = {\r\n        ""MSE"", ""CrossEntropy""\r\n    }\r\n    available_special_layers = {\r\n        ""Dropout"": Dropout,\r\n        ""Normalize"": Normalize,\r\n        ""ConvDrop"": ConvDrop,\r\n        ""ConvNorm"": ConvNorm\r\n    }\r\n    special_layer_default_params = {\r\n        ""Dropout"": (0.5, ),\r\n        ""Normalize"": (0.001, 1e-8, 0.9),\r\n        ""ConvDrop"": (0.5, ),\r\n        ""ConvNorm"": (0.001, 1e-8, 0.9)\r\n    }\r\n\r\n    def get_root_layer_by_name(self, name, *args, **kwargs):\r\n        if name not in self.available_sub_layers:\r\n            if name in self.available_root_layers:\r\n                if name in self.available_cost_functions:\r\n                    kwargs[""cost_function""] = name\r\n                name = self.available_root_layers[name]\r\n            else:\r\n                raise BuildNetworkError(""Undefined layer \'{}\' found"".format(name))\r\n            return name(*args, **kwargs)\r\n        return None\r\n    \r\n    def get_layer_by_name(self, name, parent, current_dimension, *args, **kwargs):\r\n        _layer = self.get_root_layer_by_name(name, *args, **kwargs)\r\n        if _layer:\r\n            return _layer, None\r\n        _current, _next = parent.shape[1], current_dimension\r\n        layer_param = self.special_layer_default_params[name]\r\n        _layer = self.available_special_layers[name]\r\n        if args or kwargs:\r\n            _layer = _layer(parent, (_current, _next), *args, **kwargs)\r\n        else:\r\n            _layer = _layer(parent, (_current, _next), *layer_param)\r\n        return _layer, (_current, _next)\r\n'"
NN/PyTorch/__Dev/Networks.py,0,"b'import os\r\nimport cv2\r\nimport time\r\nimport pickle\r\nimport numpy as np\r\nimport matplotlib.pyplot as plt\r\nfrom math import sqrt, ceil\r\nfrom torch.autograd import Variable\r\n\r\nfrom NN.PyTorch.Auto.Layers import *\r\nfrom NN.Basic.Networks import NNConfig, NNVerbose\r\nfrom NN.PyTorch.Auto.Optimizers import OptFactory\r\n\r\nfrom Util.Util import VisUtil\r\nfrom Util.ProgressBar import ProgressBar\r\nfrom Util.Bases import TorchAutoClassifierBase\r\n\r\n\r\n# PyTorch Implementation with auto-grad & custom Optimizers\r\n\r\nclass NNDist(TorchAutoClassifierBase):\r\n    NNTiming = Timing()\r\n\r\n    def __init__(self, **kwargs):\r\n        super(NNDist, self).__init__(**kwargs)\r\n        self._layers, self._weights, self._bias = [], [], []\r\n        self._layer_names, self._layer_shapes, self._layer_params = [], [], []\r\n        self._lr, self._epoch, self._regularization_param = 0, 0, 0\r\n        self.verbose = 1\r\n\r\n        self._apply_bias = False\r\n        self._current_dimension = 0\r\n\r\n        self._logs = {}\r\n        self._metrics, self._metric_names = [], []\r\n\r\n        self._x_min, self._x_max = 0, 0\r\n        self._y_min, self._y_max = 0, 0\r\n\r\n        self._layer_factory = LayerFactory()\r\n        self._optimizer_factory = OptFactory()\r\n\r\n        self._available_metrics = {\r\n            ""acc"": NNDist.acc, ""_acc"": NNDist.acc,\r\n            ""f1"": NNDist.f1_score, ""_f1_score"": NNDist.f1_score\r\n        }\r\n\r\n    @NNTiming.timeit(level=4, prefix=""[Initialize] "")\r\n    def initialize(self):\r\n        self._layers, self._weights, self._bias = [], [], []\r\n        self._layer_names, self._layer_shapes, self._layer_params = [], [], []\r\n        self._lr, self._epoch, self._regularization_param = 0, 0, 0\r\n        self.verbose = 1\r\n\r\n        self._apply_bias = False\r\n        self._current_dimension = 0\r\n\r\n        self._logs = []\r\n        self._metrics, self._metric_names = [], []\r\n\r\n        self._x_min, self._x_max = 0, 0\r\n        self._y_min, self._y_max = 0, 0\r\n\r\n    # Property\r\n\r\n    @property\r\n    def name(self):\r\n        return (\r\n            ""-"".join([str(_layer.shape[1]) for _layer in self._layers]) +\r\n            "" at {}"".format(time.strftime(""%Y-%m-%d %H:%M:%S"", time.localtime()))\r\n        )\r\n\r\n    @property\r\n    def layer_names(self):\r\n        return [layer.name for layer in self._layers]\r\n\r\n    @layer_names.setter\r\n    def layer_names(self, value):\r\n        self._layer_names = value\r\n\r\n    @property\r\n    def layer_shapes(self):\r\n        return [layer.size() for layer in self._layers]\r\n\r\n    @layer_shapes.setter\r\n    def layer_shapes(self, value):\r\n        self._layer_shapes = value\r\n\r\n    @property\r\n    def layer_params(self):\r\n        return self._layer_params\r\n\r\n    @layer_params.setter\r\n    def layer_params(self, value):\r\n        self.layer_params = value\r\n\r\n    @property\r\n    def layer_special_params(self):\r\n        return [layer.special_params for layer in self._layers]\r\n\r\n    @layer_special_params.setter\r\n    def layer_special_params(self, value):\r\n        for layer, sp_param in zip(self._layers, value):\r\n            if sp_param is not None:\r\n                layer.set_special_params(sp_param)\r\n\r\n    # Utils\r\n\r\n    @NNTiming.timeit(level=4)\r\n    def _get_min_max(self, x, y):\r\n        x, y = x.data.numpy(), y.data.numpy()\r\n        self._x_min, self._x_max = np.min(x), np.max(x)\r\n        self._y_min, self._y_max = np.min(y), np.max(y)\r\n\r\n    @NNTiming.timeit(level=4, prefix=""[API] "")\r\n    def _split_data(self, x, y, x_test, y_test,\r\n                    train_only, training_scale=NNConfig.TRAINING_SCALE):\r\n        if train_only:\r\n            if x_test is not None and y_test is not None:\r\n                x, y = torch.cat((x, x_test)), torch.cat((y, y_test))\r\n            x_train, y_train, x_test, y_test = x, y, x, y\r\n        else:\r\n            shuffle_suffix = torch.randperm(len(x))\r\n            x, y = x[shuffle_suffix], y[shuffle_suffix]\r\n            if x_test is None or y_test is None:\r\n                train_len = int(len(x) * training_scale)\r\n                x_train, y_train = x[:train_len], y[:train_len]\r\n                x_test, y_test = x[train_len:], y[train_len:]\r\n            elif x_test is None or y_test is None:\r\n                raise BuildNetworkError(""Please provide test sets if you want to split data on your own"")\r\n            else:\r\n                x_train, y_train = x, y\r\n        if NNConfig.BOOST_LESS_SAMPLES:\r\n            if y_train.shape[1] != 2:\r\n                raise BuildNetworkError(""It is not permitted to boost less samples in multiple classification"")\r\n            y_train_arg = torch.max(y_train, dim=1)[1]\r\n            y0 = y_train_arg == 0\r\n            y1 = ~y0\r\n            y_len, y0_len = len(y_train), torch.sum(y0)  # type: float\r\n            if y0_len > int(0.5 * y_len):\r\n                y0, y1 = y1, y0\r\n                y0_len = y_len - y0_len\r\n            boost_suffix = torch.IntTensor(y_len - y0_len).random_(y0_len)\r\n            x_train = torch.cat((x_train[y1], x_train[y0][boost_suffix]))\r\n            y_train = torch.cat((y_train[y1], y_train[y0][boost_suffix]))\r\n            shuffle_suffix = torch.randperm(len(x_train))\r\n            x_train, y_train = x_train[shuffle_suffix], y_train[shuffle_suffix]\r\n        return (x_train, x_test), (y_train, y_test)\r\n\r\n    @NNTiming.timeit(level=4)\r\n    def _add_weight(self, shape, conv_channel=None, fc_shape=None):\r\n        if fc_shape is not None:\r\n            self._weights.append(Variable(torch.randn(fc_shape, shape[1]), requires_grad=True))\r\n            self._bias.append(Variable(torch.zeros((1, shape[1])), requires_grad=True))\r\n        elif conv_channel is not None:\r\n            if len(shape[1]) <= 2:\r\n                self._weights.append(Variable(\r\n                    torch.randn(conv_channel, conv_channel, shape[1][0], shape[1][1]),\r\n                    requires_grad=True\r\n                ))\r\n            else:\r\n                self._weights.append(Variable(\r\n                    torch.randn(shape[1][0], conv_channel, shape[1][1], shape[1][2]),\r\n                    requires_grad=True\r\n                ))\r\n            self._bias.append(Variable(torch.zeros((1, shape[1][0])), requires_grad=True))\r\n        else:\r\n            self._weights.append(Variable(torch.randn(*shape), requires_grad=True))\r\n            self._bias.append(Variable(torch.zeros((1, shape[1])), requires_grad=True))\r\n\r\n    @NNTiming.timeit(level=4)\r\n    def _add_layer(self, layer, *args, **kwargs):\r\n        if not self._layers and isinstance(layer, str):\r\n            _layer = self._layer_factory.get_root_layer_by_name(layer, *args, **kwargs)\r\n            if _layer:\r\n                self.add(_layer)\r\n                return\r\n        _parent = self._layers[-1]\r\n        if isinstance(_parent, CostLayer):\r\n            raise BuildLayerError(""Adding layer after CostLayer is not permitted"")\r\n        if isinstance(layer, str):\r\n            layer, shape = self._layer_factory.get_layer_by_name(\r\n                layer, _parent, self._current_dimension, *args, **kwargs\r\n            )\r\n            if shape is None:\r\n                self.add(layer)\r\n                return\r\n            _current, _next = shape\r\n        else:\r\n            _current, _next = args\r\n        if isinstance(layer, SubLayer):\r\n            _parent.child = layer\r\n            layer.is_sub_layer = True\r\n            layer.root = layer.root\r\n            layer.root.last_sub_layer = layer\r\n            self.parent = _parent\r\n            self._layers.append(layer)\r\n            self._weights.append(torch.Tensor(0))\r\n            self._bias.append(torch.Tensor(0))\r\n            self._current_dimension = _next\r\n        else:\r\n            fc_shape, conv_channel, last_layer = None, None, self._layers[-1]\r\n            if isinstance(last_layer, ConvLayer):\r\n                if isinstance(layer, ConvLayer):\r\n                    conv_channel = last_layer.n_filters\r\n                    _current = (conv_channel, last_layer.out_h, last_layer.out_w)\r\n                    layer.feed_shape((_current, _next))\r\n                else:\r\n                    layer.is_fc = True\r\n                    last_layer.is_fc_base = True\r\n                    fc_shape = last_layer.out_h * last_layer.out_w * last_layer.n_filters\r\n            self._layers.append(layer)\r\n            self._add_weight((_current, _next), conv_channel, fc_shape)\r\n            self._current_dimension = _next\r\n        self._update_layer_information(layer)\r\n\r\n    @NNTiming.timeit(level=4)\r\n    def _update_layer_information(self, layer):\r\n        self._layer_params.append(layer.params)\r\n        if len(self._layer_params) > 1 and not layer.is_sub_layer:\r\n            self._layer_params[-1] = ((self._layer_params[-1][0][1],), *self._layer_params[-1][1:])\r\n\r\n    @NNTiming.timeit(level=1)\r\n    def _get_prediction(self, x, name=None, batch_size=1e6, verbose=None):\r\n        if verbose is None:\r\n            verbose = self.verbose\r\n        fc_shape = np.prod(x.size()[1:])  # type: int\r\n        single_batch = int(batch_size / fc_shape)\r\n        if not single_batch:\r\n            single_batch = 1\r\n        if single_batch >= len(x):\r\n            return self._get_activations(x, predict=True).pop()\r\n        epoch = int(len(x) / single_batch)\r\n        if not len(x) % single_batch:\r\n            epoch += 1\r\n        name = ""Prediction"" if name is None else ""Prediction ({})"".format(name)\r\n        sub_bar = ProgressBar(max_value=epoch, name=name, start=False)\r\n        if verbose >= NNVerbose.METRICS:\r\n            sub_bar.start()\r\n        rs, count = [self._get_activations(x[:single_batch], predict=True).pop()], single_batch\r\n        if verbose >= NNVerbose.METRICS:\r\n            sub_bar.update()\r\n        while count < len(x):\r\n            count += single_batch\r\n            if count >= len(x):\r\n                rs.append(self._get_activations(x[count-single_batch:], predict=True).pop())\r\n            else:\r\n                rs.append(self._get_activations(x[count-single_batch:count], predict=True).pop())\r\n            if verbose >= NNVerbose.METRICS:\r\n                sub_bar.update()\r\n        return torch.cat(rs)\r\n\r\n    @NNTiming.timeit(level=1)\r\n    def _get_activations(self, x, predict=False):\r\n        activations = [self._layers[0].activate(x, self._weights[0], self._bias[0], predict)]\r\n        for i, layer in enumerate(self._layers[1:]):\r\n            activations.append(layer.activate(\r\n                activations[-1], self._weights[i + 1], self._bias[i + 1], predict))\r\n        return activations\r\n\r\n    @NNTiming.timeit(level=1)\r\n    def _get_final_activation(self, x, predict=False):\r\n        activation = self._layers[0].activate(x, self._weights[0], self._bias[0], predict)\r\n        for i, layer in enumerate(self._layers[1:]):\r\n            activation = layer.activate(activation, self._weights[i + 1], self._bias[i + 1], predict)\r\n        return activation\r\n\r\n    @NNTiming.timeit(level=3)\r\n    def _append_log(self, x, y, name, get_loss=True):\r\n        y_pred = self._get_prediction(x, name)\r\n        for i, metric in enumerate(self._metrics):\r\n            self._logs[name][i].append(metric(\r\n                torch.max(y, dim=1)[1].data.numpy(),\r\n                torch.max(y_pred, dim=1)[1].data.numpy()\r\n            ))\r\n        if get_loss:\r\n            self._logs[name][-1].append(\r\n                (self._layers[-1].calculate(y, y_pred) / len(y)).data.numpy()[0]\r\n            )\r\n\r\n    @NNTiming.timeit(level=3)\r\n    def _print_metric_logs(self, show_loss, data_type):\r\n        print()\r\n        print(""="" * 47)\r\n        for i, name in enumerate(self._metric_names):\r\n            print(""{:<16s} {:<16s}: {:12.8}"".format(\r\n                data_type, name, self._logs[data_type][i][-1]))\r\n        if show_loss:\r\n            print(""{:<16s} {:<16s}: {:12.8}"".format(\r\n                data_type, ""loss"", self._logs[data_type][-1][-1]))\r\n        print(""="" * 47)\r\n\r\n    # TODO\r\n    @NNTiming.timeit(level=1)\r\n    def _draw_2d_network(self, radius=6, width=1200, height=800, padding=0.2,\r\n                         plot_scale=2, plot_precision=0.03,\r\n                         sub_layer_height_scale=0, **kwargs):\r\n        if not kwargs[""show""] and not kwargs[""mp4""]:\r\n            return\r\n        layers = len(self._layers) + 1\r\n        units = [layer.shape[0] for layer in self._layers] + [self._layers[-1].shape[1]]\r\n        whether_sub_layers = np.array([False] + [isinstance(layer, SubLayer) for layer in self._layers])\r\n        n_sub_layers = np.sum(whether_sub_layers)  # type: int\r\n\r\n        plot_num = int(1 / plot_precision)\r\n        if plot_num % 2 == 1:\r\n            plot_num += 1\r\n        half_plot_num = int(plot_num * 0.5)\r\n        xf = torch.linspace(self._x_min * plot_scale, self._x_max * plot_scale, plot_num)\r\n        yf = torch.linspace(self._x_min * plot_scale, self._x_max * plot_scale, plot_num) * -1\r\n        input_xs = Variable(torch.stack([\r\n            xf.repeat(plot_num), yf.repeat(plot_num, 1).t().contiguous().view(-1)\r\n        ], 1))\r\n\r\n        activations = [\r\n            activation.data.numpy().T.reshape(units[i + 1], plot_num, plot_num)\r\n            for i, activation in enumerate(\r\n                self._get_activations(input_xs, predict=True)\r\n            )]\r\n        graphs = []\r\n        for j, activation in enumerate(activations):\r\n            graph_group = []\r\n            if j == len(activations) - 1:\r\n                classes = np.argmax(activation, axis=0)\r\n            else:\r\n                classes = None\r\n            for k, ac in enumerate(activation):\r\n                data = np.zeros((plot_num, plot_num, 3), np.uint8)\r\n                if j != len(activations) - 1:\r\n                    mask = ac >= np.average(ac)\r\n                else:\r\n                    mask = classes == k\r\n                data[mask], data[~mask] = [0, 165, 255], [255, 165, 0]\r\n                graph_group.append(data)\r\n            graphs.append(graph_group)\r\n\r\n        img = np.full([height, width, 3], 255, dtype=np.uint8)\r\n        axis0_padding = int(height / (layers - 1 + 2 * padding)) * padding + plot_num\r\n        axis0_step = (height - 2 * axis0_padding) / layers\r\n        sub_layer_decrease = int((1 - sub_layer_height_scale) * axis0_step)\r\n        axis0 = np.linspace(\r\n            axis0_padding,\r\n            height + n_sub_layers * sub_layer_decrease - axis0_padding,\r\n            layers, dtype=np.int)\r\n        axis0 -= sub_layer_decrease * np.cumsum(whether_sub_layers)\r\n        axis1_padding = plot_num\r\n        axis1 = [np.linspace(axis1_padding, width - axis1_padding, unit + 2, dtype=np.int)\r\n                 for unit in units]\r\n        axis1 = [axis[1:-1] for axis in axis1]\r\n\r\n        colors, thicknesses, masks = [], [], []\r\n        for weight in self._weights:\r\n            line_info = VisUtil.get_line_info(weight.data.numpy().copy())\r\n            colors.append(line_info[0])\r\n            thicknesses.append(line_info[1])\r\n            masks.append(line_info[2])\r\n\r\n        for i, (y, xs) in enumerate(zip(axis0, axis1)):\r\n            for j, x in enumerate(xs):\r\n                if i == 0:\r\n                    cv2.circle(img, (x, y), radius, (20, 215, 20), int(radius / 2))\r\n                else:\r\n                    graph = graphs[i - 1][j]\r\n                    img[y - half_plot_num:y + half_plot_num, x - half_plot_num:x + half_plot_num] = graph\r\n            if i > 0:\r\n                cv2.putText(img, self._layers[i - 1].name, (12, y - 36), cv2.LINE_AA, 0.6, (0, 0, 0), 1)\r\n        for i, y in enumerate(axis0):\r\n            if i == len(axis0) - 1:\r\n                break\r\n            for j, x in enumerate(axis1[i]):\r\n                new_y = axis0[i + 1]\r\n                whether_sub_layer = isinstance(self._layers[i], SubLayer)\r\n                for k, new_x in enumerate(axis1[i + 1]):\r\n                    if whether_sub_layer and j != k:\r\n                        continue\r\n                    if masks[i][j][k]:\r\n                        cv2.line(img, (x, y + half_plot_num), (new_x, new_y - half_plot_num),\r\n                                 colors[i][j][k], thicknesses[i][j][k])\r\n\r\n        return img\r\n\r\n    # Optimizing Process\r\n\r\n    @NNTiming.timeit(level=4)\r\n    def _init_optimizer(self):\r\n        if not isinstance(self._optimizer, Optimizer):\r\n            self._optimizer = self._optimizer_factory.get_optimizer_by_name(\r\n                self._optimizer, self._model_parameters, self._lr, self._epoch)\r\n\r\n    # Batch Work\r\n\r\n    @NNTiming.timeit(level=2)\r\n    def _batch_work(self, i, sub_bar, x, y, x_test, y_test,\r\n                    draw_weights, weight_trace, show_loss):\r\n        if draw_weights:\r\n            for i, weight in enumerate(self._weights):\r\n                for j, new_weight in enumerate(weight.copy()):\r\n                    weight_trace[i][j].append(new_weight)\r\n        if self.verbose >= NNVerbose.DEBUG:\r\n            pass\r\n        if self.verbose >= NNVerbose.ITER:\r\n            if sub_bar.update() and self.verbose >= NNVerbose.METRICS_DETAIL:\r\n                self._append_log(x, y, ""train"", get_loss=show_loss)\r\n                self._append_log(x_test, y_test, ""cv"", get_loss=show_loss)\r\n                self._print_metric_logs(show_loss, ""train"")\r\n                self._print_metric_logs(show_loss, ""cv"")\r\n\r\n    @NNTiming.timeit(level=2)\r\n    def _predict(self, x, get_raw_results=False, **kwargs):\r\n        rs = self._get_final_activation(x)\r\n        if get_raw_results:\r\n            return rs\r\n        return torch.sign(rs)\r\n\r\n    # API\r\n\r\n    @NNTiming.timeit(level=4, prefix=""[API] "")\r\n    def add(self, layer, *args, **kwargs):\r\n        if isinstance(layer, str):\r\n            # noinspection PyTypeChecker\r\n            self._add_layer(layer, *args, **kwargs)\r\n        else:\r\n            if not isinstance(layer, Layer):\r\n                raise BuildLayerError(""Invalid Layer provided (should be subclass of Layer)"")\r\n            if not self._layers:\r\n                if isinstance(layer, SubLayer):\r\n                    raise BuildLayerError(""Invalid Layer provided (first layer should not be subclass of SubLayer)"")\r\n                if len(layer.shape) != 2:\r\n                    raise BuildLayerError(""Invalid input Layer provided (shape should be {}, {} found)"".format(\r\n                        2, len(layer.shape)\r\n                    ))\r\n                self._layers, self._current_dimension = [layer], layer.shape[1]\r\n                self._update_layer_information(layer)\r\n                if isinstance(layer, ConvLayer):\r\n                    self._add_weight(layer.shape, layer.n_channels)\r\n                else:\r\n                    self._add_weight(layer.shape)\r\n            else:\r\n                if len(layer.shape) > 2:\r\n                    raise BuildLayerError(""Invalid Layer provided (shape should be {}, {} found)"".format(\r\n                        2, len(layer.shape)\r\n                    ))\r\n                if len(layer.shape) == 2:\r\n                    _current, _next = layer.shape\r\n                    if isinstance(layer, SubLayer):\r\n                        if _next != self._current_dimension:\r\n                            raise BuildLayerError(""Invalid SubLayer provided (shape[1] should be {}, {} found)"".format(\r\n                                self._current_dimension, _next\r\n                            ))\r\n                    elif not isinstance(layer, ConvLayer) and _current != self._current_dimension:\r\n                        raise BuildLayerError(""Invalid Layer provided (shape[0] should be {}, {} found)"".format(\r\n                            self._current_dimension, _current\r\n                        ))\r\n                    self._add_layer(layer, _current, _next)\r\n\r\n                elif len(layer.shape) == 1:\r\n                    _next = layer.shape[0]\r\n                    layer.shape = (self._current_dimension, _next)\r\n                    self._add_layer(layer, self._current_dimension, _next)\r\n                else:\r\n                    raise LayerError(""Invalid Layer provided (invalid shape \'{}\' found)"".format(layer.shape))\r\n\r\n    # TODO\r\n    @NNTiming.timeit(level=4, prefix=""[API] "")\r\n    def build(self, units=""build""):\r\n        if isinstance(units, str):\r\n            if units == ""build"":\r\n                for name, param in zip(self._layer_names, self._layer_params):\r\n                    self.add(name, *param)\r\n            else:\r\n                raise NotImplementedError(""Invalid param \'{}\' provided to \'build\' method"".format(units))\r\n        else:\r\n            try:\r\n                units = list(units)\r\n            except ValueError as err:\r\n                raise BuildLayerError(err)\r\n            if len(units) < 2:\r\n                raise BuildLayerError(""At least 2 layers are needed"")\r\n            _input_shape = (units[0], units[1])\r\n            self.initialize()\r\n            self.add(ReLU(_input_shape))\r\n            for unit_num in units[2:-1]:\r\n                self.add(ReLU((unit_num,)))\r\n            self.add(""CrossEntropy"", (units[-1],))\r\n\r\n    @NNTiming.timeit(level=4, prefix=""[API] "")\r\n    def preview(self):\r\n        if not self._layers:\r\n            rs = ""None""\r\n        else:\r\n            rs = (\r\n                ""Input  :  {:<10s} - {}\\n"".format(""Dimension"", self._layers[0].shape[0]) +\r\n                ""\\n"".join([\r\n                    ""Layer  :  {:<16s} - {} {}"".format(\r\n                        _layer.name, _layer.shape[1], _layer.description\r\n                    ) if isinstance(_layer, SubLayer) else\r\n                    ""Layer  :  {:<16s} - {:<14s} - strides: {:2d} - padding: {:2d} - out: {}"".format(\r\n                        _layer.name, str(_layer.shape[1]), _layer.stride, _layer.padding,\r\n                        (_layer.n_filters, _layer.out_h, _layer.out_w)\r\n                    ) if isinstance(_layer, ConvLayer) else ""Layer  :  {:<10s} - {}"".format(\r\n                        _layer.name, _layer.shape[1]\r\n                    ) for _layer in self._layers[:-1]\r\n                ]) + ""\\nCost   :  {:<10s}"".format(str(self._layers[-1]))\r\n            )\r\n        print(""="" * 30 + ""\\n"" + ""Structure\\n"" + ""-"" * 30 + ""\\n"" + rs + ""\\n"" + ""-"" * 30 + ""\\n"")\r\n\r\n    @NNTiming.timeit(level=1, prefix=""[API] "")\r\n    def fit(self,\r\n            x, y, x_test=None, y_test=None,\r\n            batch_size=128, record_period=1, train_only=False,\r\n            optimizer=""Adam"", lr=0.001, lb=0.001, epoch=20, weight_scale=1, apply_bias=True,\r\n            show_loss=True, metrics=None, do_log=True, verbose=None,\r\n            visualize=False, visualize_setting=None,\r\n            draw_weights=False, animation_params=None):\r\n\r\n        self._lr, self._epoch = lr, epoch\r\n        for weight in self._weights:\r\n            weight.data *= weight_scale\r\n        self._model_parameters = self._weights\r\n        if apply_bias:\r\n            self._model_parameters += self._bias\r\n        self._optimizer = optimizer\r\n        self._init_optimizer()\r\n        assert isinstance(self._optimizer, Optimizer)\r\n        print()\r\n        print(""="" * 30)\r\n        print(""Optimizers"")\r\n        print(""-"" * 30)\r\n        print(self._optimizer)\r\n        print(""-"" * 30)\r\n\r\n        if not self._layers:\r\n            raise BuildNetworkError(""Please provide layers before fitting data"")\r\n        if y.shape[1] != self._current_dimension:\r\n            raise BuildNetworkError(""Output layer\'s shape should be {}, {} found"".format(\r\n                self._current_dimension, y.shape[1]))\r\n\r\n        x, y = self._arr_to_variable(False, x, y)\r\n        if x_test is not None and y_test is not None:\r\n            x_test, y_test = self._arr_to_variable(False, x_test, y_test)\r\n        (x_train, x_test), (y_train, y_test) = self._split_data(\r\n            x, y, x_test, y_test, train_only)\r\n        train_len = len(x_train)\r\n        batch_size = min(batch_size, train_len)\r\n        do_random_batch = train_len > batch_size\r\n        train_repeat = 1 if not do_random_batch else int(train_len / batch_size) + 1\r\n        self._regularization_param = 1 - lb * lr / batch_size\r\n        self._get_min_max(x_train, y_train)\r\n\r\n        self._metrics = [""acc""] if metrics is None else metrics\r\n        for i, metric in enumerate(self._metrics):\r\n            if isinstance(metric, str):\r\n                if metric not in self._available_metrics:\r\n                    raise BuildNetworkError(""Metric \'{}\' is not implemented"".format(metric))\r\n                self._metrics[i] = self._available_metrics[metric]\r\n        self._metric_names = [_m.__name__ for _m in self._metrics]\r\n\r\n        self._logs = {\r\n            name: [[] for _ in range(len(self._metrics) + 1)] for name in (""train"", ""cv"", ""test"")\r\n        }\r\n        if verbose is not None:\r\n            self.verbose = verbose\r\n\r\n        self._apply_bias = apply_bias\r\n\r\n        bar = ProgressBar(max_value=max(1, epoch // record_period), name=""Epoch"", start=False)\r\n        if self.verbose >= NNVerbose.EPOCH:\r\n            bar.start()\r\n        img, ims = None, []\r\n\r\n        if draw_weights:\r\n            weight_trace = [[[org] for org in weight] for weight in self._weights]\r\n        else:\r\n            weight_trace = []\r\n\r\n        loss_function = self._layers[-1].calculate\r\n        args = (\r\n            x_train, y_train, x_test, y_test,\r\n            draw_weights, weight_trace, show_loss\r\n        )\r\n\r\n        *animation_properties, animation_params = self._get_animation_params(animation_params)\r\n        sub_bar = ProgressBar(max_value=train_repeat * record_period - 1, name=""Iteration"", start=False)\r\n        for counter in range(epoch):\r\n            self._optimizer.update()\r\n            if self.verbose >= NNVerbose.ITER and counter % record_period == 0:\r\n                sub_bar.start()\r\n            self.batch_training(\r\n                x_train, y_train, batch_size, train_repeat, loss_function, sub_bar, *args\r\n            )\r\n            if self.verbose >= NNVerbose.ITER:\r\n                sub_bar.update()\r\n            self._handle_animation(\r\n                counter, x, y, ims, animation_params, *animation_properties,\r\n                img=self._draw_2d_network(**animation_params), name=""Neural Network""\r\n            )\r\n            if do_log:\r\n                self._append_log(x, y, ""train"", get_loss=show_loss)\r\n                self._append_log(x_test, y_test, ""cv"", get_loss=show_loss)\r\n            if (counter + 1) % record_period == 0:\r\n                if do_log and self.verbose >= NNVerbose.METRICS:\r\n                    self._print_metric_logs(show_loss, ""train"")\r\n                    self._print_metric_logs(show_loss, ""cv"")\r\n                if visualize:\r\n                    if visualize_setting is None:\r\n                        self.visualize2d(x_test, y_test)\r\n                    else:\r\n                        self.visualize2d(x_test, y_test, *visualize_setting)\r\n                if self.verbose >= NNVerbose.EPOCH:\r\n                    bar.update(counter // record_period + 1)\r\n                    if self.verbose >= NNVerbose.ITER:\r\n                        sub_bar = ProgressBar(max_value=train_repeat * record_period - 1, name=""Iteration"", start=False)\r\n\r\n        if do_log:\r\n            self._append_log(x_test, y_test, ""test"", get_loss=show_loss)\r\n        if img is not None:\r\n            cv2.waitKey(0)\r\n            cv2.destroyAllWindows()\r\n        if draw_weights:\r\n            ts = np.arange(epoch * train_repeat + 1)\r\n            for i, weight in enumerate(self._weights):\r\n                plt.figure()\r\n                for j in range(len(weight)):\r\n                    plt.plot(ts, weight_trace[i][j])\r\n                plt.title(""Weights toward layer {} ({})"".format(i + 1, self._layers[i].name))\r\n                plt.show()\r\n        self._handle_mp4(ims, animation_properties, ""NN"")\r\n        return self._logs\r\n\r\n    # TODO\r\n    @NNTiming.timeit(level=2, prefix=""[API] "")\r\n    def save(self, path=None, name=None, overwrite=True):\r\n        path = os.path.join(""Models"", ""Cache"") if path is None else os.path.join(""Models"", path)\r\n        name = ""Model.nn"" if name is None else name\r\n        if not os.path.exists(path):\r\n            os.makedirs(path)\r\n        _dir = os.path.join(path, name)\r\n        if not overwrite and os.path.isfile(_dir):\r\n            _count = 1\r\n            _new_dir = _dir + ""({})"".format(_count)\r\n            while os.path.isfile(_new_dir):\r\n                _count += 1\r\n                _new_dir = _dir + ""({})"".format(_count)\r\n            _dir = _new_dir\r\n        print()\r\n        print(""="" * 60)\r\n        print(""Saving Model to {}..."".format(_dir))\r\n        print(""-"" * 60)\r\n        with open(_dir, ""wb"") as file:\r\n            pickle.dump({\r\n                ""structures"": {\r\n                    ""_layer_names"": self.layer_names,\r\n                    ""_layer_params"": self._layer_params,\r\n                    ""_cost_layer"": self._layers[-1].name,\r\n                    ""_next_dimension"": self._current_dimension\r\n                },\r\n                ""params"": {\r\n                    ""_logs"": self._logs,\r\n                    ""_metric_names"": self._metric_names,\r\n                    ""_weights"": self._weights,\r\n                    ""_bias"": self._bias,\r\n                    ""_optimizer"": self._optimizer,\r\n                    ""layer_special_params"": self.layer_special_params,\r\n                }\r\n            }, file)\r\n        print(""Done"")\r\n        print(""="" * 60)\r\n\r\n    # TODO\r\n    @NNTiming.timeit(level=2, prefix=""[API] "")\r\n    def load(self, path=os.path.join(""Models"", ""Cache"", ""Model.nn"")):\r\n        self.initialize()\r\n        try:\r\n            with open(path, ""rb"") as file:\r\n                dic = pickle.load(file)\r\n                for key, value in dic[""structures""].items():\r\n                    setattr(self, key, value)\r\n                self.build()\r\n                for key, value in dic[""params""].items():\r\n                    setattr(self, key, value)\r\n                self._init_optimizer()\r\n                for i in range(len(self._metric_names) - 1, -1, -1):\r\n                    name = self._metric_names[i]\r\n                    if name not in self._available_metrics:\r\n                        self._metric_names.pop(i)\r\n                    else:\r\n                        self._metrics.insert(0, self._available_metrics[name])\r\n                print()\r\n                print(""="" * 30)\r\n                print(""Model restored"")\r\n                print(""="" * 30)\r\n                return dic\r\n        except Exception as err:\r\n            raise BuildNetworkError(""Failed to load Network ({}), structure initialized"".format(err))\r\n\r\n    @NNTiming.timeit(level=4, prefix=""[API] "")\r\n    def predict(self, x, get_raw_results=False, **kwargs):\r\n        if not isinstance(x, Variable):\r\n            x = Variable(torch.from_numpy(np.asarray(x, dtype=np.float32)))\r\n        if len(x.size()) == 1:\r\n            x = x.view(1, -1)\r\n        y_pred = self._get_prediction(x).data.numpy()\r\n        return y_pred if get_raw_results else np.argmax(y_pred, axis=1)\r\n\r\n    def draw_results(self):\r\n        metrics_log, loss_log = {}, {}\r\n        for key, value in sorted(self._logs.items()):\r\n            metrics_log[key], loss_log[key] = value[:-1], value[-1]\r\n\r\n        for i, name in enumerate(sorted(self._metric_names)):\r\n            plt.figure()\r\n            plt.title(""Metric Type: {}"".format(name))\r\n            for key, log in sorted(metrics_log.items()):\r\n                if key == ""test"":\r\n                    continue\r\n                xs = np.arange(len(log[i])) + 1\r\n                plt.plot(xs, log[i], label=""Data Type: {}"".format(key))\r\n            plt.legend(loc=4)\r\n            plt.show()\r\n            plt.close()\r\n\r\n        plt.figure()\r\n        plt.title(""Cost"")\r\n        for key, loss in sorted(loss_log.items()):\r\n            if key == ""test"":\r\n                continue\r\n            xs = np.arange(len(loss)) + 1\r\n            plt.plot(xs, loss, label=""Data Type: {}"".format(key))\r\n        plt.legend()\r\n        plt.show()\r\n\r\n    def draw_conv_weights(self):\r\n        for i, (name, weight) in enumerate(zip(self.layer_names, self._weights)):\r\n            if len(weight.size()) != 4:\r\n                return\r\n            for j, _w in enumerate(weight):\r\n                for k, _ww in enumerate(_w):\r\n                    VisUtil.show_img(_ww, ""{} {} filter {} channel {}"".format(name, i+1, j+1, k+1))\r\n\r\n    def draw_conv_series(self, x, shape=None):\r\n        for xx in x:\r\n            VisUtil.show_img(VisUtil.trans_img(xx, shape), ""Original"")\r\n            activations = self._get_activations(np.array([xx]), predict=True)\r\n            for i, (layer, ac) in enumerate(zip(self._layers, activations)):\r\n                if len(ac.shape) == 4:\r\n                    for n in ac:\r\n                        _n, height, width = n.shape\r\n                        a = int(ceil(sqrt(_n)))\r\n                        g = np.ones((a * height + a, a * width + a), n.dtype)\r\n                        g *= np.min(n)\r\n                        _i = 0\r\n                        for y in range(a):\r\n                            for x in range(a):\r\n                                if _i < _n:\r\n                                    g[y * height + y:(y + 1) * height + y, x * width + x:(x + 1) * width + x] = n[\r\n                                        _i, :, :]\r\n                                    _i += 1\r\n                        # normalize to [0,1]\r\n                        max_g = g.max()\r\n                        min_g = g.min()\r\n                        g = (g - min_g) / (max_g - min_g)\r\n                        VisUtil.show_img(g, ""Layer {} ({})"".format(i + 1, layer.name))\r\n                else:\r\n                    ac = ac[0]\r\n                    length = sqrt(np.prod(ac.shape))\r\n                    if length < 10:\r\n                        continue\r\n                    (height, width) = xx.shape[1:] if shape is None else shape[1:]\r\n                    sqrt_shape = sqrt(height * width)\r\n                    oh, ow = int(length * height / sqrt_shape), int(length * width / sqrt_shape)\r\n                    VisUtil.show_img(ac[:oh*ow].reshape(oh, ow), ""Layer {} ({})"".format(i + 1, layer.name))\r\n'"
NN/PyTorch/__Dev/Optimizers.py,0,"b'import torch\n\nfrom Util.Timing import Timing\n\n\nclass Optimizer:\n    OptTiming = Timing()\n\n    def __init__(self, lr=0.01, cache=None):\n        self.lr = lr\n        self._cache = cache\n\n    def __str__(self):\n        return self.__class__.__name__\n\n    def __repr__(self):\n        return str(self)\n\n    @property\n    def name(self):\n        return str(self)\n\n    def feed_variables(self, variables):\n        self._cache = [\n            torch.zeros(var.size()) for var in variables\n        ]\n\n    @OptTiming.timeit(level=1, prefix=""[API] "")\n    def run(self, i, dw):\n        return self._run(i, dw)\n\n    def _run(self, i, dw):\n        raise NotImplementedError(""Please implement a \'feed\' method for your optimizer"")\n\n    @OptTiming.timeit(level=4, prefix=""[API] "")\n    def update(self):\n        return self._update()\n\n    def _update(self):\n        raise NotImplementedError(""Please implement an \'update\' method for your optimizer"")\n\n\nclass MBGD(Optimizer):\n    def _run(self, i, dw):\n        return self.lr * dw\n\n    def _update(self):\n        pass\n\n\nclass Momentum(Optimizer):\n    def __init__(self, lr=0.01, cache=None, epoch=100, floor=0.5, ceiling=0.999):\n        Optimizer.__init__(self, lr, cache)\n        self._epoch, self._floor, self._ceiling = epoch, floor, ceiling\n        self._step = (ceiling - floor) / epoch\n        self._momentum = floor\n        self._is_nesterov = False\n\n    @property\n    def epoch(self):\n        return self._epoch\n\n    @property\n    def floor(self):\n        return self._floor\n\n    @property\n    def ceiling(self):\n        return self._ceiling\n\n    def update_step(self):\n        self._step = (self._ceiling - self._floor) / self._epoch\n\n    @epoch.setter\n    def epoch(self, value):\n        self._epoch = value\n        self.update_step()\n\n    @floor.setter\n    def floor(self, value):\n        self._floor = value\n        self.update_step()\n\n    @ceiling.setter\n    def ceiling(self, value):\n        self._ceiling = value\n        self.update_step()\n\n    def _run(self, i, dw):\n        dw *= self.lr\n        velocity = self._cache\n        velocity[i] *= self._momentum\n        velocity[i] += dw\n        if not self._is_nesterov:\n            return velocity[i]\n        return self._momentum * velocity[i] + dw\n\n    def _update(self):\n        if self._momentum < self._ceiling:\n            self._momentum += self._step\n\n\nclass NAG(Momentum):\n    def __init__(self, lr=0.01, cache=None, epoch=100, floor=0.5, ceiling=0.999):\n        Momentum.__init__(self, lr, cache, epoch, floor, ceiling)\n        self._is_nesterov = True\n\n\nclass RMSProp(Optimizer):\n\n    def __init__(self, lr=0.01, cache=None, decay_rate=0.9, eps=1e-8):\n        Optimizer.__init__(self, lr, cache)\n        self.decay_rate, self.eps = decay_rate, eps\n\n    def _run(self, i, dw):\n        self._cache[i] = self._cache[i] * self.decay_rate + (1 - self.decay_rate) * dw * dw\n        return self.lr * dw / (torch.sqrt(self._cache[i] + self.eps))\n\n    def _update(self):\n        pass\n\n\nclass Adam(Optimizer):\n    def __init__(self, lr=0.01, cache=None, beta1=0.9, beta2=0.999, eps=1e-8):\n        Optimizer.__init__(self, lr, cache)\n        self.beta1, self.beta2, self.eps = beta1, beta2, eps\n\n    def feed_variables(self, variables):\n        self._cache = [\n            [torch.zeros(var.size()) for var in variables],\n            [torch.zeros(var.size()) for var in variables],\n        ]\n\n    def _run(self, i, dw):\n        self._cache[0][i] = self._cache[0][i] * self.beta1 + (1 - self.beta1) * dw\n        self._cache[1][i] = self._cache[1][i] * self.beta2 + (1 - self.beta2) * (dw ** 2)\n        return self.lr * self._cache[0][i] / (torch.sqrt(self._cache[1][i] + self.eps))\n\n    def _update(self):\n        pass\n\n\n# Factory\n\nclass OptFactory:\n\n    available_optimizers = {\n        ""MBGD"": MBGD, ""Momentum"": Momentum, ""NAG"": NAG, ""Adam"": Adam, ""RMSProp"": RMSProp\n    }\n\n    def get_optimizer_by_name(self, name, variables, lr, epoch):\n        try:\n            optimizer = self.available_optimizers[name](lr)\n            if variables is not None:\n                optimizer.feed_variables(variables)\n            if epoch is not None and isinstance(optimizer, Momentum):\n                optimizer.epoch = epoch\n            return optimizer\n        except KeyError:\n            raise NotImplementedError(""Undefined Optimizer \'{}\' found"".format(name))\n'"
NN/Test/Basic/Test.py,0,"b'from NN.Basic.Networks import *\r\n\r\nfrom Util.Util import DataUtil\r\n\r\n\r\ndef main():\r\n    nn = NNDist()\r\n    save = False\r\n    load = False\r\n    show_loss = True\r\n    train_only = False\r\n    verbose = 2\r\n\r\n    lr = 0.001\r\n    lb = 0.001\r\n    epoch = 5\r\n    record_period = 1\r\n    use_cnn = True\r\n\r\n    x, y = DataUtil.get_dataset(""mnist"", ""../../../_Data/mnist.txt"", quantized=True, one_hot=True)\r\n    if use_cnn:\r\n        x = x.reshape(len(x), 1, 28, 28)\r\n        batch_size = 32\r\n    else:\r\n        batch_size = 128\r\n\r\n    if not load:\r\n        if use_cnn:\r\n            nn.add(""ConvReLU"", (x.shape[1:], (16, 3, 3)))\r\n            nn.add(""MaxPool"", ((3, 3),), 1)\r\n            nn.add(""ConvNorm"")\r\n            nn.add(""ConvDrop"")\r\n        else:\r\n            nn.add(""ReLU"", (x.shape[1], 1024))\r\n            nn.add(""ReLU"", (1024,))\r\n        nn.add(""CrossEntropy"", (y.shape[1],))\r\n        nn.optimizer = ""Adam""\r\n        nn.preview()\r\n        nn.fit(x, y, lr=lr, lb=lb,\r\n               epoch=epoch, batch_size=batch_size, record_period=record_period,\r\n               show_loss=show_loss, train_only=train_only, do_log=True, verbose=verbose)\r\n        if save:\r\n            nn.save()\r\n        nn.draw_results()\r\n    else:\r\n        nn.load()\r\n        nn.preview()\r\n        print(nn.evaluate(x, y)[0])\r\n\r\n    nn.show_timing_log()\r\n\r\nif __name__ == \'__main__\':\r\n    main()\r\n'"
NN/Test/Basic/Vis.py,0,"b'from NN.Basic.Networks import *\r\n\r\nfrom Util.Util import DataUtil\r\n\r\n\r\ndef main():\r\n    nn = NNDist()\r\n    save = False\r\n    load = False\r\n\r\n    lr = 0.001\r\n    lb = 0.001\r\n    epoch = 1000\r\n    record_period = 4\r\n\r\n    x, y = DataUtil.gen_spiral(50, 3, 3, 2.5)\r\n\r\n    if not load:\r\n        nn.build([x.shape[1], 6, 6, 6, y.shape[1]])\r\n        nn.optimizer = ""Adam""\r\n        nn.preview()\r\n        nn.fit(x, y, lr=lr, lb=lb, verbose=1, record_period=record_period,\r\n               epoch=epoch, batch_size=128, train_only=True,\r\n               animation_params={""show"": True, ""mp4"": False, ""period"": record_period})\r\n        if save:\r\n            nn.save()\r\n        nn.visualize2d(x, y)\r\n        nn.draw_results()\r\n    else:\r\n        nn.load()\r\n        nn.preview()\r\n        nn.evaluate(x, y)\r\n\r\n    nn.show_timing_log()\r\n\r\nif __name__ == \'__main__\':\r\n    main()\r\n'"
NN/Test/TF/CIFAR10.py,0,"b'from NN.TF.Networks import *\n\nfrom Util.Util import DataUtil\n\n\ndef main():\n    log = """"\n\n    nn = NNDist()\n    save = False\n    load = False\n    show_loss = True\n    train_only = False\n    do_log = True\n    verbose = 4\n\n    lr = 0.001\n    lb = 0.001\n    epoch = 10\n    record_period = 1\n    weight_scale = 0.001\n    optimizer = ""Adam""\n    nn.optimizer = optimizer\n\n    x, y = DataUtil.get_dataset(""cifar10"", ""../../../_Data/cifar10.txt"", quantized=True, one_hot=True)\n\n    draw = True\n    img_shape = (3, 32, 32)\n    x = x.reshape(len(x), *img_shape)\n\n    if not load:\n        nn.add(""ConvReLU"", (x.shape[1:], (32, 3, 3)))\n        nn.add(""ConvReLU"", ((32, 3, 3),))\n        nn.add(""MaxPool"", ((3, 3),), 2)\n        nn.add(""ConvNorm"")\n        nn.add(""ConvDrop"")\n        nn.add(""ConvReLU"", ((64, 3, 3),), std=0.01)\n        nn.add(""ConvReLU"", ((64, 3, 3),), std=0.01)\n        nn.add(""AvgPool"", ((3, 3),), 2)\n        nn.add(""ConvNorm"")\n        nn.add(""ConvDrop"")\n        nn.add(""ConvReLU"", ((32, 3, 3),))\n        nn.add(""ConvReLU"", ((32, 3, 3),))\n        nn.add(""AvgPool"", ((3, 3),), 2)\n        nn.add(""ReLU"", (512, ))\n        nn.add(""Identical"", (64, ), apply_bias=False)\n        nn.add(""Normalize"", activation=""ReLU"")\n        nn.add(""Dropout"")\n        nn.add(""CrossEntropy"", (y.shape[1], ))\n\n        nn.preview()\n        nn.fit(x, y,\n               lr=lr, lb=0, epoch=epoch, weight_scale=weight_scale,\n               record_period=record_period, show_loss=show_loss, train_only=train_only,\n               do_log=do_log, verbose=verbose)\n        nn.draw_results()\n\n        if save:\n            nn.save()\n        if draw:\n            # nn.draw_conv_weights()\n            nn.draw_conv_series(x[:3], img_shape)\n    else:\n        nn.load()\n        print(""Optimizer: "" + nn.optimizer)\n        nn.preview()\n        nn.fit(x, y, epoch=1, lr=lr, lb=lb, verbose=verbose)\n        # nn.fit(x, y, x_cv, y_cv, x_test, y_test, epoch=1, lr=lr, lb=lb, verbose=verbose)\n        if draw:\n            # nn.draw_conv_weights()\n            nn.draw_conv_series(x[:3], img_shape)\n        nn.draw_results()\n\n        acc = nn.evaluate(x, y)[0]\n        log += ""Test set Accuracy  : {:12.6} %"".format(100 * acc) + ""\\n""\n        print(""="" * 30 + ""\\n"" + ""Results\\n"" + ""-"" * 30)\n        print(log)\n\n    nn.show_timing_log()\n\nif __name__ == \'__main__\':\n    main()\n'"
NN/Test/TF/Mnist.py,0,"b'from NN.TF.Networks import *\r\n\r\nfrom Util.Util import DataUtil\r\n\r\n\r\ndef main():\r\n    x, y = DataUtil.get_dataset(""mnist"", ""../../../_Data/mnist.txt"", quantized=True, one_hot=True)\r\n    x = x.reshape(len(x), 1, 28, 28)\r\n\r\n    nn = NNDist()\r\n\r\n    # nn.add(""ReLU"", (x.shape[1], 24))\r\n    # nn.add(""ReLU"", (24, ))\r\n    # nn.add(""CrossEntropy"", (y.shape[1], ))\r\n\r\n    nn.add(""ConvReLU"", (x.shape[1:], (32, 3, 3)))\r\n    nn.add(""ConvReLU"", ((32, 3, 3),))\r\n    nn.add(""MaxPool"", ((3, 3),), 2)\r\n    nn.add(""ConvNorm"")\r\n    nn.add(""ConvDrop"")\r\n    nn.add(""ConvReLU"", ((64, 3, 3),), std=0.01)\r\n    nn.add(""ConvReLU"", ((64, 3, 3),), std=0.01)\r\n    nn.add(""AvgPool"", ((3, 3),), 2)\r\n    nn.add(""ConvNorm"")\r\n    nn.add(""ConvDrop"")\r\n    nn.add(""ConvReLU"", ((32, 3, 3),))\r\n    nn.add(""ConvReLU"", ((32, 3, 3),))\r\n    nn.add(""AvgPool"", ((3, 3),), 2)\r\n    nn.add(""ReLU"", (512,))\r\n    nn.add(""Identical"", (64,))\r\n    nn.add(""Normalize"", activation=""ReLU"")\r\n    nn.add(""Dropout"")\r\n    nn.add(""CrossEntropy"", (y.shape[1],))\r\n\r\n    nn.optimizer = ""Adam""\r\n    nn.preview()\r\n    nn.fit(x, y, verbose=2, do_log=True)\r\n    nn.evaluate(x, y)\r\n    nn.draw_results()\r\n    nn.show_timing_log()\r\n\r\nif __name__ == \'__main__\':\r\n    main()\r\n'"
NN/Test/TF/Tensorboard.py,0,"b'from NN.TF.Networks import *\r\n\r\nfrom Util.Util import DataUtil\r\n\r\n\r\ndef main():\r\n    save = False\r\n    load = False\r\n    show_loss = True\r\n    train_only = False\r\n    verbose = 2\r\n\r\n    lr = 0.001\r\n    lb = 0.001\r\n    epoch = 10\r\n    record_period = 1\r\n\r\n    x, y = DataUtil.get_dataset(""mnist"", ""../../../_Data/mnist.txt"", quantized=True, one_hot=True)\r\n    x = x.reshape(len(x), 1, 28, 28)\r\n\r\n    if not load:\r\n        nn = NNDist()\r\n\r\n        # nn.add(""ReLU"", (x.shape[1], 24))\r\n        # nn.add(""ReLU"", (24, ))\r\n        # nn.add(""CrossEntropy"", (y.shape[1], ))\r\n\r\n        nn.add(""ConvReLU"", (x.shape[1:], (32, 3, 3)))\r\n        nn.add(""ConvReLU"", ((32, 3, 3),))\r\n        nn.add(""MaxPool"", ((3, 3),), 2)\r\n        nn.add(""ConvNorm"")\r\n        nn.add(""ConvDrop"")\r\n        nn.add(""ConvReLU"", ((64, 3, 3),), std=0.01)\r\n        nn.add(""ConvReLU"", ((64, 3, 3),), std=0.01)\r\n        nn.add(""AvgPool"", ((3, 3),), 2)\r\n        nn.add(""ConvNorm"")\r\n        nn.add(""ConvDrop"")\r\n        nn.add(""ConvReLU"", ((32, 3, 3),))\r\n        nn.add(""ConvReLU"", ((32, 3, 3),))\r\n        nn.add(""AvgPool"", ((3, 3),), 2)\r\n        nn.add(""ReLU"", (512,))\r\n        nn.add(""Identical"", (64,))\r\n        nn.add(""Normalize"", activation=""ReLU"")\r\n        nn.add(""Dropout"")\r\n        nn.add(""CrossEntropy"", (y.shape[1],))\r\n        nn.optimizer = ""Adam""\r\n        nn.preview(verbose=verbose)\r\n        nn.fit(x, y, lr=lr, lb=lb,\r\n               epoch=epoch, batch_size=256, record_period=record_period,\r\n               show_loss=show_loss, train_only=train_only, do_log=True, tensorboard_verbose=1, verbose=verbose)\r\n        if save:\r\n            nn.save()\r\n    else:\r\n        nn = NNFrozen()\r\n        nn.load()\r\n        nn.preview()\r\n        nn.evaluate(x, y)\r\n\r\n    nn.show_timing_log()\r\n\r\nif __name__ == \'__main__\':\r\n    main()\r\n'"
Notebooks/NN/zh-cn/Util.py,0,"b""import numpy as np\nimport matplotlib.pyplot as plt\n\nfrom math import pi\n\ndef gen_five_clusters(size=200):\n    x = np.random.randn(size) * 2\n    y = np.random.randn(size) * 2\n    z = np.full(size, -1)\n    mask1, mask2 = x + y >= 1, x + y >= -1\n    mask3, mask4 = x - y >= 1, x - y >= -1\n    z[mask1 & ~mask4] = 0\n    z[mask1 & mask3] = 1\n    z[~mask2 & mask3] = 2\n    z[~mask2 & ~mask4] = 3\n    z[z == -1] = 4\n    one_hot = np.zeros([size, 5])\n    one_hot[range(size), z] = 1\n    return np.c_[x, y].astype(np.float32), one_hot\n\ndef visualize2d(clf, x, y, padding=0.2, draw_background=False):\n    axis, labels = np.array(x).T, np.array(y)\n\n    nx, ny, padding = 400, 400, padding\n    x_min, x_max = np.min(axis[0]), np.max(axis[0])\n    y_min, y_max = np.min(axis[1]), np.max(axis[1])\n    x_padding = max(abs(x_min), abs(x_max)) * padding\n    y_padding = max(abs(y_min), abs(y_max)) * padding\n    x_min -= x_padding\n    x_max += x_padding\n    y_min -= y_padding\n    y_max += y_padding\n\n    def get_base(nx, ny):\n        xf = np.linspace(x_min, x_max, nx)\n        yf = np.linspace(y_min, y_max, ny)\n        n_xf, n_yf = np.meshgrid(xf, yf)\n        return xf, yf, np.c_[n_xf.ravel(), n_yf.ravel()]\n\n    xf, yf, base_matrix = get_base(nx, ny)\n    z = clf.predict(base_matrix).reshape([nx, ny])\n    \n    n_label = len(np.unique(labels))\n    xy_xf, xy_yf = np.meshgrid(xf, yf, sparse=True)\n    colors = plt.cm.rainbow([i / n_label for i in range(n_label)])[labels.astype(np.int)]\n\n    plt.figure()\n    if draw_background:\n        plt.pcolormesh(xy_xf, xy_yf, z, cmap=plt.cm.Paired)\n    else:\n        plt.contour(xf, yf, z, c='k-', levels=[0])\n    plt.scatter(axis[0], axis[1], c=colors)\n    plt.xlim(x_min, x_max)\n    plt.ylim(y_min, y_max)\n    plt.show()\n"""
Notebooks/SVM/zh-cn/Util.py,0,"b""import numpy as np\nimport matplotlib.pyplot as plt\n\nfrom math import pi\n\nnp.random.seed(142857)\n\n# \xe7\x94\x9f\xe6\x88\x90\xe7\xae\x80\xe5\x8d\x95\xe7\x9a\x84\xe6\xb5\x8b\xe8\xaf\x95\xe6\x95\xb0\xe6\x8d\xae\xe9\x9b\x86\ndef gen_two_clusters(size=100, center=0, scale=1, dis=2):\n    center1 = (np.random.random(2) + center - 0.5) * scale + dis\n    center2 = (np.random.random(2) + center - 0.5) * scale - dis\n    cluster1 = (np.random.randn(size, 2) + center1) * scale\n    cluster2 = (np.random.randn(size, 2) + center2) * scale\n    data = np.vstack((cluster1, cluster2)).astype(np.float32)\n    labels = np.array([1] * size + [-1] * size)\n    indices = np.random.permutation(size * 2)\n    data, labels = data[indices], labels[indices]\n    return data, labels\n\n# \xe7\x94\x9f\xe6\x88\x90\xe8\x9e\xba\xe6\x97\x8b\xe7\xba\xbf\xe6\x95\xb0\xe6\x8d\xae\xe9\x9b\x86\ndef gen_spiral(size=50, n=4, scale=2):\n    xs = np.zeros((size * n, 2), dtype=np.float32)\n    ys = np.zeros(size * n, dtype=np.int8)\n    for i in range(n):\n        ix = range(size * i, size * (i + 1))\n        r = np.linspace(0.0, 1, size+1)[1:]\n        t = np.linspace(2 * i * pi / n, 2 * (i + scale) * pi / n, size) + np.random.random(size=size) * 0.1\n        xs[ix] = np.c_[r * np.sin(t), r * np.cos(t)]\n        ys[ix] = 2 * (i % 2) - 1\n    return xs, ys\n\n# \xe7\x94\xbb\xe5\x87\xba\xe5\x86\xb3\xe7\xad\x96\xe8\xbe\xb9\xe7\x95\x8c\xef\xbc\x9b\xe5\xa6\x82\xe6\x9e\x9c\xe5\x8f\xaa\xe5\x85\xb3\xe5\xbf\x83\xe7\xae\x97\xe6\xb3\x95\xe6\x9c\xac\xe8\xba\xab\xef\xbc\x8c\xe5\x8f\xaf\xe4\xbb\xa5\xe7\x95\xa5\xe5\x8e\xbb\xe8\xbf\x99\xe4\xb8\x80\xe6\xae\xb5\xe4\xbb\xa3\xe7\xa0\x81\xe4\xb8\x8d\xe7\x9c\x8b\ndef visualize2d(clf, x, y, draw_background=False):\n    axis, labels = np.array(x).T, np.array(y)\n    decision_function = lambda xx: clf.predict(xx)\n\n    nx, ny, padding = 400, 400, 0.2\n    x_min, x_max = np.min(axis[0]), np.max(axis[0])\n    y_min, y_max = np.min(axis[1]), np.max(axis[1])\n    x_padding = max(abs(x_min), abs(x_max)) * padding\n    y_padding = max(abs(y_min), abs(y_max)) * padding\n    x_min -= x_padding\n    x_max += x_padding\n    y_min -= y_padding\n    y_max += y_padding\n\n    def get_base(nx, ny):\n        xf = np.linspace(x_min, x_max, nx)\n        yf = np.linspace(y_min, y_max, ny)\n        n_xf, n_yf = np.meshgrid(xf, yf)\n        return xf, yf, np.c_[n_xf.ravel(), n_yf.ravel()]\n\n    xf, yf, base_matrix = get_base(nx, ny)\n    z = decision_function(base_matrix).reshape((nx, ny))\n    \n    labels[labels == -1] = 0\n    n_label = len(np.unique(labels))\n    xy_xf, xy_yf = np.meshgrid(xf, yf, sparse=True)\n    colors = plt.cm.rainbow([i / n_label for i in range(n_label)])[labels]\n\n    plt.figure()\n    if draw_background:\n        plt.pcolormesh(xy_xf, xy_yf, z, cmap=plt.cm.Paired)\n    else:\n        plt.contour(xf, yf, z, c='k-', levels=[0])\n    plt.scatter(axis[0], axis[1], c=colors)\n    plt.xlim(x_min, x_max)\n    plt.ylim(y_min, y_max)\n    plt.show()\n\n"""
Zhihu/CvDTree/one/CvDTree.py,0,"b'import time\nimport math\nimport numpy as np\nfrom collections import Counter\n\n\n# Util\n\nclass Cluster:\n    def __init__(self, data, labels, base=2):\n        self._data = np.array(data).T\n        self._counters = Counter(labels)\n        self._labels = np.array(labels)\n        self._base = base\n\n    def ent(self, ent=None, eps=1e-12):\n        _len = len(self._labels)\n        if ent is None:\n            ent = [_val for _val in self._counters.values()]\n        return max(eps, -sum([_c / _len * math.log(_c / _len, self._base) for _c in ent]))\n\n    def con_ent(self, idx):\n        data = self._data[idx]\n        features = set(data)\n        tmp_labels = [data == feature for feature in features]\n        label_lst = [self._labels[label] for label in tmp_labels]\n        rs = 0\n        for data_label, tar_label in zip(tmp_labels, label_lst):\n            tmp_data = self._data.T[data_label]\n            _ent = Cluster(tmp_data, tar_label, base=self._base).ent()\n            rs += len(tmp_data) / len(data) * _ent\n        return rs\n\n    def info_gain(self, idx):\n        _con_ent = self.con_ent(idx)\n        _gain = self.ent() - _con_ent\n        return _gain, _con_ent\n\n\n# Node\n\nclass CvDNode:\n    def __init__(self, tree=None, max_depth=None, base=2, ent=None,\n                 depth=0, parent=None, is_root=True, prev_feat=""Root""):\n        self._data = self.labels = None\n        self._max_depth = max_depth\n        self._base = base\n        self.ent = ent\n        self.children = {}\n        self.category = None\n\n        self.tree = tree\n        if tree is not None:\n            tree.nodes.append(self)\n        self.feature_dim = None\n        self.feats = []\n        self._depth = depth\n        self.parent = parent\n        self.is_root = is_root\n        self.prev_feat = prev_feat\n        self.leafs = {}\n        self.pruned = False\n\n    @property\n    def key(self):\n        return self._depth, self.prev_feat, id(self)\n\n    @property\n    def height(self):\n        if self.category is not None:\n            return 1\n        return 1 + max([_child.height for _child in self.children.values()])\n\n    def feed_data(self, data, labels):\n        self._data = data\n        self.labels = labels\n\n    def stop(self, eps):\n        if (\n            self._data.shape[1] == 1 or (self.ent is not None and self.ent <= eps)\n            or (self._max_depth is not None and self._depth >= self._max_depth)\n        ):\n            self._handle_terminate()\n            return True\n        return False\n\n    def early_stop(self, max_gain, eps):\n        if max_gain <= eps:\n            self._handle_terminate()\n            return True\n        return False\n\n    def get_class(self):\n        _counter = Counter(self.labels)\n        return max(_counter.keys(), key=(lambda key: _counter[key]))\n\n    def _gen_children(self, feat, con_ent):\n        features = self._data[:, feat]\n        _new_feats = self.feats[:]\n        _new_feats.remove(feat)\n        for feat in set(features):\n            _feat_mask = features == feat\n            _new_node = self.__class__(\n                self.tree, self._max_depth, self._base, ent=con_ent,\n                depth=self._depth + 1, parent=self, is_root=False, prev_feat=feat)\n            _new_node.feats = _new_feats\n            self.children[feat] = _new_node\n            _new_node.fit(self._data[_feat_mask, :], self.labels[_feat_mask])\n\n    def _handle_terminate(self):\n        self.category = self.get_class()\n        _parent = self\n        while _parent is not None:\n            _parent.leafs[self.key] = self\n            _parent = _parent.parent\n\n    def fit(self, data, labels, eps=1e-8):\n        if data is not None and labels is not None:\n            self.feed_data(data, labels)\n        if self.stop(eps):\n            return\n        _cluster = Cluster(self._data, self.labels, self._base)\n        _max_gain, _con_ent = _cluster.info_gain(self.feats[0])\n        _max_feature = self.feats[0]\n        for feat in self.feats[1:]:\n            _tmp_gain, _tmp_con_ent = _cluster.info_gain(feat)\n            if _tmp_gain > _max_gain:\n                (_max_gain, _con_ent), _max_feature = (_tmp_gain, _tmp_con_ent), feat\n        if self.early_stop(_max_gain, eps):\n            return\n        self.feature_dim = _max_feature\n        self._gen_children(_max_feature, _con_ent)\n        if self.is_root:\n            self.tree.prune()\n\n    def mark_pruned(self):\n        self.pruned = True\n        if self.children:\n            for child in self.children.values():\n                child.mark_pruned()\n\n    def prune(self):\n        if self.category is None:\n            self.category = self.get_class()\n            self.feature_dim = None\n        _pop_lst = [key for key in self.leafs]\n        _parent = self\n        while _parent is not None:\n            for _k in _pop_lst:\n                _parent.leafs.pop(_k)\n            _parent.leafs[self.key] = self\n            _parent = _parent.parent\n        self.mark_pruned()\n        self.children = {}\n\n    def predict_one(self, x):\n        if self.category is not None:\n            return self.category\n        try:\n            return self.children[x[self.feature_dim]].predict_one(x)\n        except KeyError:\n            return self.get_class()\n\n    def predict(self, x):\n        if self.category is not None:\n            if self.is_root:\n                return [self.category] * len(x)\n            return self.category\n        x = np.atleast_2d(x)\n        return [self.predict_one(xx) for xx in x]\n\n    def view(self, indent=4):\n        print("" "" * indent * self._depth, self)\n        for _node in sorted(self.children.values()):\n            _node.view()\n\n    def __lt__(self, other):\n        return self.prev_feat < other.prev_feat\n\n    def __str__(self):\n        if self.category is None:\n            return ""CvDNode ({}) ({} -> {})"".format(\n                self._depth, self.prev_feat, self.feature_dim)\n        return ""CvDNode ({}) ({} -> class: {})"".format(\n            self._depth, self.prev_feat, self.category)\n\n    __repr__ = __str__\n\n\n# Tree\n\nclass CvDBase:\n    def __init__(self, max_depth=None):\n        self.nodes = []\n        self._max_depth = max_depth\n        self.root = CvDNode(self, max_depth)\n\n    @property\n    def depth(self):\n        return self.root.height\n\n    @staticmethod\n    def acc(y, y_pred):\n        return np.sum(np.array(y) == np.array(y_pred)) / len(y)\n\n    def fit(self, data=None, labels=None, eps=1e-8):\n        data, labels = np.array(data), np.array(labels)\n        self.root.feats = [i for i in range(data.shape[1])]\n        self.root.fit(data, labels, eps)\n\n    def prune(self, alpha=1):\n        if self.depth <= 2:\n            return\n        _tmp_nodes = [node for node in self.nodes if not node.is_root and not node.category]\n        _old = np.array([sum(\n            [leaf.ent * len(leaf.labels) for leaf in node.leafs.values()]\n        ) + alpha * len(node.leafs) for node in _tmp_nodes])\n        _new = np.array([node.ent * len(node.labels) + alpha for node in _tmp_nodes])\n        _mask = (_old - _new) > 0\n        arg = np.argmax(_mask)\n        if _mask[arg]:\n            _tmp_nodes[arg].prune()\n            for i in range(len(self.nodes) - 1, -1, -1):\n                if self.nodes[i].pruned:\n                    self.nodes.pop(i)\n            self.prune(alpha)\n\n    def predict_one(self, x):\n        return self.root.predict_one(x)\n\n    def predict(self, x):\n        return self.root.predict(x)\n\n    def estimate(self, x, y):\n        y = np.array(y)\n        print(""Acc: {:8.6} %"".format(100 * np.sum(self.predict(x) == y) / len(y)))\n\n    def view(self):\n        self.root.view()\n\n    def __str__(self):\n        return ""CvDTree ({})"".format(self.depth)\n\n    __repr__ = __str__\n\nif __name__ == \'__main__\':\n    _data, _x, _y = [], [], []\n    with open(""../data.txt"", ""r"") as file:\n        for line in file:\n            _data.append(line.split("",""))\n    np.random.shuffle(_data)\n    for line in _data:\n        _y.append(line.pop(0))\n        _x.append(line)\n    _x, _y = np.array(_x), np.array(_y)\n    train_num = 5000\n    x_train = _x[:train_num]\n    y_train = _y[:train_num]\n    x_test = _x[train_num:]\n    y_test = _y[train_num:]\n\n    _t = time.time()\n    _tree = CvDBase()\n    _tree.fit(x_train, y_train)\n    _tree.view()\n    _tree.estimate(x_test, y_test)\n    print(""Time cost: {:8.6}"".format(time.time() - _t))\n'"
Zhihu/CvDTree/three/CvDTree.py,0,"b'import cv2\r\nimport time\r\nimport math\r\nimport numpy as np\r\n\r\n\r\n# Util\r\n\r\nclass Cluster:\r\n    def __init__(self, data, labels, sample_weights=None, base=2):\r\n        self._data = np.array(data).T\r\n        if sample_weights is None:\r\n            self._counters = np.bincount(labels)\r\n        else:\r\n            # noinspection PyTypeChecker\r\n            self._counters = np.bincount(labels, weights=sample_weights * len(sample_weights))\r\n        self._sample_weights = sample_weights\r\n        self._labels = np.array(labels)\r\n        self._cache = None\r\n        self._base = base\r\n\r\n    def ent(self, ent=None, eps=1e-12):\r\n        _len = len(self._labels)\r\n        if ent is None:\r\n            ent = [_val for _val in self._counters]\r\n        return max(eps, -sum([_c / _len * math.log(_c / _len, self._base) if _c != 0 else 0 for _c in ent]))\r\n\r\n    def gini(self, p=None):\r\n        if p is None:\r\n            p = [_val for _val in self._counters]\r\n        return 1 - sum([(_p / len(self._labels)) ** 2 for _p in p])\r\n\r\n    def con_chaos(self, idx, criterion=""ent""):\r\n        if criterion == ""ent"":\r\n            _method = lambda cluster: cluster.ent()\r\n        elif criterion == ""gini"":\r\n            _method = lambda cluster: cluster.gini()\r\n        else:\r\n            raise NotImplementedError(""Conditional info criterion \'{}\' not defined"".format(criterion))\r\n        data = self._data[idx]\r\n        features = list(sorted(set(data)))\r\n        self._cache = tmp_labels = [data == feature for feature in features]\r\n        label_lst = [self._labels[label] for label in tmp_labels]\r\n        rs = 0\r\n        for data_label, tar_label in zip(tmp_labels, label_lst):\r\n            tmp_data = self._data.T[data_label]\r\n            if self._sample_weights is None:\r\n                _ent = _method(Cluster(tmp_data, tar_label, base=self._base))\r\n            else:\r\n                _new_weights = self._sample_weights[data_label]\r\n                _ent = _method(Cluster(tmp_data, tar_label, _new_weights / np.sum(_new_weights), base=self._base))\r\n            rs += len(tmp_data) / len(data) * _ent\r\n        return rs\r\n\r\n    def info_gain(self, idx, criterion=""ent"", get_con_chaos=False):\r\n        if criterion in (""ent"", ""ratio""):\r\n            _con_chaos = self.con_chaos(idx)\r\n            _gain = self.ent() - _con_chaos\r\n            if criterion == ""ratio"":\r\n                _gain = _gain / self.ent([np.sum(_cache) for _cache in self._cache])\r\n        elif criterion == ""gini"":\r\n            _con_chaos = self.con_chaos(idx, criterion=""gini"")\r\n            _gain = self.gini() - _con_chaos\r\n        else:\r\n            raise NotImplementedError(""Info_gain criterion \'{}\' not defined"".format(criterion))\r\n        return (_gain, _con_chaos) if get_con_chaos else _gain\r\n\r\n\r\n# Node\r\n\r\nclass CvDNode:\r\n    def __init__(self, tree=None, base=2, ent=None,\r\n                 depth=0, parent=None, is_root=True, prev_feat=""Root""):\r\n        self._data = self.labels = None\r\n        self._base = base\r\n        self.ent = ent\r\n        self.criterion = None\r\n        self.children = {}\r\n        self.category = None\r\n        self.sample_weights = None\r\n\r\n        self.tree = tree\r\n        if tree is not None:\r\n            tree.nodes.append(self)\r\n        self.feature_dim = None\r\n        self.feats = []\r\n        self._depth = depth\r\n        self.parent = parent\r\n        self.is_root = is_root\r\n        self._prev_feat = prev_feat\r\n        self.leafs = {}\r\n        self.pruned = False\r\n        self.label_dict = {}\r\n\r\n    def __getitem__(self, item):\r\n        if isinstance(item, str):\r\n            return getattr(self, ""_"" + item)\r\n        return\r\n\r\n    def __lt__(self, other):\r\n        return self.prev_feat < other.prev_feat\r\n\r\n    def __str__(self):\r\n        if self.children:\r\n            return ""CvDNode ({}) ({} -> {})"".format(\r\n                self._depth, self._prev_feat, self.feature_dim)\r\n        return ""CvDNode ({}) ({} -> class: {})"".format(\r\n            self._depth, self._prev_feat, self.label_dict[self.category])\r\n\r\n    __repr__ = __str__\r\n\r\n    @property\r\n    def key(self):\r\n        return self._depth, self.prev_feat, id(self)\r\n\r\n    @property\r\n    def height(self):\r\n        if self.category is not None:\r\n            return 1\r\n        return 1 + max([_child.height for _child in self.children.values()])\r\n\r\n    @property\r\n    def max_depth(self):\r\n        return self.tree.max_depth\r\n\r\n    @property\r\n    def prev_feat(self):\r\n        return self._prev_feat\r\n\r\n    @property\r\n    def info_dict(self):\r\n        return {\r\n            ""ent"": self.ent,\r\n            ""labels"": self.labels\r\n        }\r\n\r\n    def feed_tree(self, tree):\r\n        self.tree = tree\r\n        self.tree.nodes.append(self)\r\n\r\n    def feed_data(self, data, labels):\r\n        self._data = np.array(data)\r\n        self.labels = np.array(labels)\r\n\r\n    def stop(self, eps):\r\n        if (\r\n            self._data.shape[1] == 1 or (self.ent is not None and self.ent <= eps)\r\n            or (self.max_depth is not None and self._depth >= self.max_depth)\r\n        ):\r\n            self._handle_terminate()\r\n            return True\r\n        return False\r\n\r\n    def early_stop(self, max_gain, eps):\r\n        if max_gain <= eps:\r\n            self._handle_terminate()\r\n            return True\r\n        return False\r\n\r\n    def get_class(self):\r\n        _counter = np.bincount(self.labels)\r\n        return np.argmax(_counter)\r\n\r\n    def _gen_children(self, feat, con_chaos):\r\n        features = self._data[:, feat]\r\n        new_feats = self.feats[:]\r\n        new_feats.remove(feat)\r\n        for feat in set(features):\r\n            feat_mask = features == feat\r\n            new_node = self.__class__(\r\n                self.tree, self._base, ent=con_chaos,\r\n                depth=self._depth + 1, parent=self, is_root=False, prev_feat=feat)\r\n            new_node.feats = new_feats\r\n            new_node.label_dict = self.label_dict\r\n            self.children[feat] = new_node\r\n            _local_weights = None if self.sample_weights is None else self.sample_weights[feat_mask]\r\n            new_node.fit(self._data[feat_mask, :], self.labels[feat_mask], _local_weights)\r\n\r\n    def _handle_terminate(self):\r\n        self.category = self.get_class()\r\n        parent = self\r\n        while parent is not None:\r\n            parent.leafs[self.key] = self.info_dict\r\n            parent = parent.parent\r\n\r\n    def fit(self, data, labels, sample_weights, eps=1e-8):\r\n        if data is not None and labels is not None:\r\n            self.feed_data(data, labels)\r\n        self.sample_weights = sample_weights\r\n        if self.stop(eps):\r\n            return\r\n        _cluster = Cluster(self._data, self.labels, sample_weights, self._base)\r\n        _max_gain, _con_chaos = _cluster.info_gain(self.feats[0], criterion=self.criterion, get_con_chaos=True)\r\n        _max_feature = self.feats[0]\r\n        for feat in self.feats[1:]:\r\n            _tmp_gain, _tmp_con_chaos = _cluster.info_gain(feat, criterion=self.criterion, get_con_chaos=True)\r\n            if _tmp_gain > _max_gain:\r\n                (_max_gain, _con_chaos), _max_feature = (_tmp_gain, _tmp_con_chaos), feat\r\n        if self.early_stop(_max_gain, eps):\r\n            return\r\n        self.feature_dim = _max_feature\r\n        self._gen_children(_max_feature, _con_chaos)\r\n        if self.is_root:\r\n            self.tree.prune()\r\n\r\n    def prune(self):\r\n        if self.category is None:\r\n            self.category = self.get_class()\r\n            self.feature_dim = None\r\n        _pop_lst = [key for key in self.leafs]\r\n        _parent = self\r\n        while _parent is not None:\r\n            for _k in _pop_lst:\r\n                _parent.leafs.pop(_k)\r\n            _parent.leafs[self.key] = self.info_dict\r\n            _parent = _parent.parent\r\n        self.mark_pruned()\r\n        self.children = {}\r\n\r\n    def mark_pruned(self):\r\n        self.pruned = True\r\n        if self.children is not None:\r\n            for _child in self.children.values():\r\n                _child.mark_pruned()\r\n\r\n    def predict_one(self, x):\r\n        if self.category is not None:\r\n            return self.category\r\n        try:\r\n            return self.children[x[self.feature_dim]].predict_one(x)\r\n        except KeyError:\r\n            return self.get_class()\r\n\r\n    def view(self, indent=4):\r\n        print("" "" * indent * self._depth, self)\r\n        for _node in sorted(self.children.values()):\r\n            _node.view()\r\n\r\n    def update_layers(self):\r\n        self.tree.layers[self._depth].append(self)\r\n        for _node in sorted(self.children.values()):\r\n            _node.update_layers()\r\n\r\n\r\nclass ID3Node(CvDNode):\r\n    def __init__(self, *args, **kwargs):\r\n        CvDNode.__init__(self, *args, **kwargs)\r\n        self.criterion = ""ent""\r\n\r\n\r\nclass C45Node(CvDNode):\r\n    def __init__(self, *args, **kwargs):\r\n        CvDNode.__init__(self, *args, **kwargs)\r\n        self.criterion = ""ratio""\r\n\r\n\r\n# Tree\r\n\r\nclass CvDBase:\r\n    def __init__(self, max_depth=None, node=None):\r\n        self.nodes = []\r\n        self.trees = []\r\n        self.layers = []\r\n        self._threshold_cache = None\r\n        self._max_depth = max_depth\r\n        self.root = node\r\n        self.root.feed_tree(self)\r\n        self.label_dict = {}\r\n        self.prune_alpha = 1\r\n\r\n    def __str__(self):\r\n        return ""CvDTree ({})"".format(self.depth)\r\n\r\n    __repr__ = __str__\r\n\r\n    @property\r\n    def depth(self):\r\n        return self.root.height\r\n\r\n    @staticmethod\r\n    def acc(y, y_pred):\r\n        return np.sum(np.array(y) == np.array(y_pred)) / len(y)\r\n\r\n    def fit(self, data=None, labels=None, sample_weights=None, eps=1e-8, **kwargs):\r\n        dic = {c: i for i, c in enumerate(set(labels))}\r\n        labels = np.array([dic[yy] for yy in labels])\r\n        self.label_dict = {value: key for key, value in dic.items()}\r\n        data = np.array(data)\r\n        self.root.label_dict = self.label_dict\r\n        self.root.feats = [i for i in range(data.shape[1])]\r\n        self.root.fit(data, labels, sample_weights, eps)\r\n        self.prune_alpha = kwargs.get(""alpha"", self.prune_alpha)\r\n\r\n    def _reduce_nodes(self):\r\n        for i in range(len(self.nodes) - 1, -1, -1):\r\n            if self.nodes[i].pruned:\r\n                self.nodes.pop(i)\r\n\r\n    def prune(self):\r\n        _continue = False\r\n        if self.depth <= 2:\r\n            return\r\n        _tmp_nodes = [node for node in self.nodes if not node.is_root and not node.category]\r\n        if not _tmp_nodes:\r\n            return\r\n        _old = np.array([sum(\r\n            [leaf[""ent""] * len(leaf[""labels""]) for leaf in node.leafs.values()]\r\n        ) + self.prune_alpha * len(node.leafs) for node in _tmp_nodes])\r\n        _new = np.array([node.ent * len(node.labels) + self.prune_alpha for node in _tmp_nodes])\r\n        _mask = (_old - _new) > 0\r\n        arg = np.argmax(_mask)  # type: int\r\n        if _mask[arg]:\r\n            _tmp_nodes[arg].prune()\r\n            _continue = True\r\n        if _continue:\r\n            self._reduce_nodes()\r\n            self.prune()\r\n\r\n    def predict_one(self, x, transform=True):\r\n        if transform:\r\n            return self.label_dict[self.root.predict_one(x)]\r\n        return self.root.predict_one(x)\r\n\r\n    def predict(self, x, transform=True):\r\n        x = np.atleast_2d(x)\r\n        return np.array([self.predict_one(xx, transform) for xx in x])\r\n\r\n    def estimate(self, x, y):\r\n        y = np.array(y)\r\n        print(""Acc: {:8.6} %"".format(100 * np.sum(self.predict(x) == y) / len(y)))\r\n\r\n    def view(self):\r\n        self.root.view()\r\n\r\n    def draw(self, radius=24, width=1200, height=800, padding=0.2, plot_num=30, title=""CvDTree""):\r\n        self.layers = [[] for _ in range(self.depth)]\r\n        self.root.update_layers()\r\n        units = [len(layer) for layer in self.layers]\r\n\r\n        img = np.ones((height, width, 3), np.uint8) * 255\r\n        axis0_padding = int(height / (len(self.layers) - 1 + 2 * padding)) * padding + plot_num\r\n        axis0 = np.linspace(\r\n            axis0_padding, height - axis0_padding, len(self.layers), dtype=np.int)\r\n        axis1_padding = plot_num\r\n        axis1 = [np.linspace(axis1_padding, width - axis1_padding, unit + 2, dtype=np.int)\r\n                 for unit in units]\r\n        axis1 = [axis[1:-1] for axis in axis1]\r\n\r\n        for i, (y, xs) in enumerate(zip(axis0, axis1)):\r\n            for j, x in enumerate(xs):\r\n                if i == 0:\r\n                    cv2.circle(img, (x, y), radius, (225, 100, 125), 1)\r\n                else:\r\n                    cv2.circle(img, (x, y), radius, (125, 100, 225), 1)\r\n                node = self.layers[i][j]\r\n                if node.feature_dim is not None:\r\n                    text = str(node.feature_dim)\r\n                    color = (0, 0, 255)\r\n                else:\r\n                    text = str(self.label_dict[node.category])\r\n                    color = (0, 255, 0)\r\n                cv2.putText(img, text, (x-7*len(text)+2, y+3), cv2.LINE_AA, 0.6, color, 1)\r\n\r\n        for i, y in enumerate(axis0):\r\n            if i == len(axis0) - 1:\r\n                break\r\n            for j, x in enumerate(axis1[i]):\r\n                new_y = axis0[i + 1]\r\n                dy = new_y - y - 2 * radius\r\n                for k, new_x in enumerate(axis1[i + 1]):\r\n                    dx = new_x - x\r\n                    length = np.sqrt(dx**2+dy**2)\r\n                    ratio = 0.5 - min(0.4, 1.2 * 24/length)\r\n                    if self.layers[i + 1][k] in self.layers[i][j].children.values():\r\n                        cv2.line(img, (x, y+radius), (x+int(dx*ratio), y+radius+int(dy*ratio)),\r\n                                 (125, 125, 125), 1)\r\n                        cv2.putText(img, self.layers[i+1][k].prev_feat,\r\n                                    (x+int(dx*0.5)-6, y+radius+int(dy*0.5)),\r\n                                    cv2.LINE_AA, 0.6, (0, 0, 0), 1)\r\n                        cv2.line(img, (new_x-int(dx*ratio), new_y-radius-int(dy*ratio)), (new_x, new_y-radius),\r\n                                 (125, 125, 125), 1)\r\n\r\n        cv2.imshow(title, img)\r\n        cv2.waitKey(0)\r\n        return img\r\n\r\n\r\nclass CvDMeta(type):\r\n    def __new__(mcs, *args, **kwargs):\r\n        name, bases, attr = args[:3]\r\n        _, node = bases\r\n\r\n        def __init__(self, **_kwargs):\r\n            _max_depth = None if ""max_depth"" not in _kwargs else _kwargs.pop(""max_depth"")\r\n            CvDBase.__init__(self, _max_depth, node(**_kwargs))\r\n\r\n        @property\r\n        def max_depth(self):\r\n            return self._max_depth\r\n\r\n        attr[""__init__""] = __init__\r\n        attr[""max_depth""] = max_depth\r\n        return type(name, bases, attr)\r\n\r\n\r\nclass ID3Tree(CvDBase, ID3Node, metaclass=CvDMeta):\r\n    pass\r\n\r\n\r\nclass C45Tree(CvDBase, C45Node, metaclass=CvDMeta):\r\n    pass\r\n\r\nif __name__ == \'__main__\':\r\n    _data, _x, _y = [], [], []\r\n    with open(""Data/data.txt"", ""r"") as file:\r\n        for line in file:\r\n            _data.append(line.split("",""))\r\n    np.random.shuffle(_data)\r\n    for line in _data:\r\n        _y.append(line.pop(0))\r\n        _x.append(line)\r\n    _x, _y = np.array(_x), np.array(_y)\r\n    train_num = 5000\r\n    x_train = _x[:train_num]\r\n    y_train = _y[:train_num]\r\n    x_test = _x[train_num:]\r\n    y_test = _y[train_num:]\r\n\r\n    _t = time.time()\r\n    _tree = C45Tree()\r\n    _tree.fit(x_train, y_train)\r\n    _tree.view()\r\n    _tree.estimate(x_test, y_test)\r\n    print(""Time cost: {:8.6}"".format(time.time() - _t))\r\n    _tree.draw()\r\n'"
Zhihu/CvDTree/two/CvDTree.py,0,"b'import time\nimport math\nimport numpy as np\n\n\n# Util\n\nclass Cluster:\n    def __init__(self, data, labels, sample_weights=None, base=2):\n        self._data = np.array(data).T\n        if sample_weights is None:\n            self._counters = np.bincount(labels)\n        else:\n            self._counters = np.bincount(labels, weights=sample_weights)\n        self._sample_weights = sample_weights\n        self._labels = np.array(labels)\n        self._base = base\n\n    def ent(self, ent=None, eps=1e-12):\n        _len = len(self._labels)\n        if ent is None:\n            ent = [max(eps, _val) for _val in self._counters]\n        return max(eps, -sum([_c / _len * math.log(_c / _len, self._base) for _c in ent]))\n\n    def con_ent(self, idx):\n        data = self._data[idx]\n        features = set(data)\n        tmp_labels = [data == feature for feature in features]\n        label_lst = [self._labels[label] for label in tmp_labels]\n        rs = 0\n        for data_label, tar_label in zip(tmp_labels, label_lst):\n            tmp_data = self._data.T[data_label]\n            if self._sample_weights is None:\n                _ent = Cluster(tmp_data, tar_label, base=self._base).ent()\n            else:\n                _ent = Cluster(tmp_data, tar_label, self._sample_weights[data_label], base=self._base).ent()\n            rs += len(tmp_data) / len(data) * _ent\n        return rs\n\n    def info_gain(self, idx):\n        _con_ent = self.con_ent(idx)\n        _gain = self.ent() - _con_ent\n        return _gain, _con_ent\n\n\n# Node\n\nclass CvDNode:\n    def __init__(self, tree=None, max_depth=None, base=2, ent=None,\n                 depth=0, parent=None, is_root=True, prev_feat=""Root""):\n        self._data = self.labels = None\n        self._max_depth = max_depth\n        self._base = base\n        self.ent = ent\n        self.children = {}\n        self.category = None\n        self.label_dict = {}\n\n        self.tree = tree\n        if tree is not None:\n            tree.nodes.append(self)\n        self.feature_dim = None\n        self.feats = []\n        self._depth = depth\n        self.parent = parent\n        self.is_root = is_root\n        self.prev_feat = prev_feat\n        self.leafs = {}\n        self.pruned = False\n\n    @property\n    def key(self):\n        return self._depth, self.prev_feat, id(self)\n\n    @property\n    def height(self):\n        if self.category is not None:\n            return 1\n        return 1 + max([_child.height for _child in self.children.values()])\n\n    def feed_data(self, data, labels):\n        self._data = data\n        self.labels = labels\n\n    def stop(self, eps):\n        if (\n            self._data.shape[1] == 1 or (self.ent is not None and self.ent <= eps)\n            or (self._max_depth is not None and self._depth >= self._max_depth)\n        ):\n            self._handle_terminate()\n            return True\n        return False\n\n    def early_stop(self, max_gain, eps):\n        if max_gain <= eps:\n            self._handle_terminate()\n            return True\n        return False\n\n    def get_class(self):\n        _counter = np.bincount(self.labels)\n        return np.argmax(_counter)\n\n    def _gen_children(self, feat, con_ent):\n        features = self._data[:, feat]\n        new_feats = self.feats[:]\n        new_feats.remove(feat)\n        for feat in set(features):\n            feat_mask = features == feat\n            new_node = self.__class__(\n                self.tree, self._max_depth, self._base, ent=con_ent,\n                depth=self._depth + 1, parent=self, is_root=False, prev_feat=feat)\n            new_node.feats = new_feats\n            new_node.label_dict = self.label_dict\n            self.children[feat] = new_node\n            new_node.fit(self._data[feat_mask, :], self.labels[feat_mask])\n\n    def _handle_terminate(self):\n        self.category = self.get_class()\n        _parent = self\n        while _parent is not None:\n            _parent.leafs[self.key] = self\n            _parent = _parent.parent\n\n    def fit(self, data, labels, sample_weights=None, eps=1e-8):\n        if data is not None and labels is not None:\n            self.feed_data(data, labels)\n        if self.stop(eps):\n            return\n        _cluster = Cluster(self._data, self.labels, sample_weights, self._base)\n        _max_gain, _con_ent = _cluster.info_gain(self.feats[0])\n        _max_feature = self.feats[0]\n        for feat in self.feats[1:]:\n            _tmp_gain, _tmp_con_ent = _cluster.info_gain(feat)\n            if _tmp_gain > _max_gain:\n                (_max_gain, _con_ent), _max_feature = (_tmp_gain, _tmp_con_ent), feat\n        if self.early_stop(_max_gain, eps):\n            return\n        self.feature_dim = _max_feature\n        self._gen_children(_max_feature, _con_ent)\n        if self.is_root:\n            self.tree.prune()\n\n    def mark_pruned(self):\n        self.pruned = True\n        if self.children:\n            for child in self.children.values():\n                child.mark_pruned()\n\n    def prune(self):\n        if self.category is None:\n            self.category = self.get_class()\n            self.feature_dim = None\n        _pop_lst = [key for key in self.leafs]\n        _parent = self\n        while _parent is not None:\n            for _k in _pop_lst:\n                _parent.leafs.pop(_k)\n            _parent.leafs[self.key] = self\n            _parent = _parent.parent\n        self.mark_pruned()\n        self.children = {}\n\n    def predict_one(self, x):\n        if self.category is not None:\n            return self.category\n        try:\n            return self.children[x[self.feature_dim]].predict_one(x)\n        except KeyError:\n            return self.get_class()\n\n    def view(self, indent=4):\n        print("" "" * indent * self._depth, self)\n        for _node in sorted(self.children.values()):\n            _node.view()\n\n    def __lt__(self, other):\n        return self.prev_feat < other.prev_feat\n\n    def __str__(self):\n        if self.category is None:\n            return ""CvDNode ({}) ({} -> {})"".format(\n                self._depth, self.prev_feat, self.feature_dim)\n        return ""CvDNode ({}) ({} -> class: {})"".format(\n            self._depth, self.prev_feat, self.label_dict[self.category])\n\n    __repr__ = __str__\n\n\n# Tree\n\nclass CvDBase:\n    def __init__(self, max_depth=None):\n        self.nodes = []\n        self._max_depth = max_depth\n        self.root = CvDNode(self, max_depth)\n        self.label_dict = {}\n\n    @property\n    def depth(self):\n        return self.root.height\n\n    @staticmethod\n    def acc(y, y_pred):\n        return np.sum(np.array(y) == np.array(y_pred)) / len(y)\n\n    def fit(self, data=None, labels=None, sample_weights=None, eps=1e-8):\n        dic = {c: i for i, c in enumerate(set(labels))}\n        labels = np.array([dic[yy] for yy in labels])\n        self.label_dict = {value: key for key, value in dic.items()}\n        data = np.array(data)\n        self.root.label_dict = self.label_dict\n        self.root.feats = [i for i in range(data.shape[1])]\n        self.root.fit(data, labels, sample_weights, eps)\n\n    def prune(self, alpha=1):\n        if self.depth <= 2:\n            return\n        _tmp_nodes = [node for node in self.nodes if not node.is_root and not node.category]\n        if not _tmp_nodes:\n            return\n        _old = np.array([sum(\n            [leaf.ent * len(leaf.labels) for leaf in node.leafs.values()]\n        ) + alpha * len(node.leafs) for node in _tmp_nodes])\n        _new = np.array([node.ent * len(node.labels) + alpha for node in _tmp_nodes])\n        _mask = (_old - _new) > 0\n        arg = np.argmax(_mask)\n        if _mask[arg]:\n            _tmp_nodes[arg].prune()\n            for i in range(len(self.nodes) - 1, -1, -1):\n                if self.nodes[i].pruned:\n                    self.nodes.pop(i)\n            self.prune(alpha)\n\n    def predict_one(self, x):\n        return self.label_dict[self.root.predict_one(x)]\n\n    def predict(self, x):\n        x = np.atleast_2d(x)\n        return [self.predict_one(xx) for xx in x]\n\n    def estimate(self, x, y):\n        y = np.array(y)\n        print(""Acc: {:8.6} %"".format(100 * np.sum(self.predict(x) == y) / len(y)))\n\n    def view(self):\n        self.root.view()\n\n    def __str__(self):\n        return ""CvDTree ({})"".format(self.depth)\n\n    __repr__ = __str__\n\nif __name__ == \'__main__\':\n    _data, _x, _y = [], [], []\n    with open(""../data.txt"", ""r"") as file:\n        for line in file:\n            _data.append(line.split("",""))\n    np.random.shuffle(_data)\n    for line in _data:\n        _y.append(line.pop(0))\n        _x.append(list(map(lambda c: c.strip(), line)))\n    _x, _y = np.array(_x), np.array(_y)\n    train_num = 5000\n    x_train = _x[:train_num]\n    y_train = _y[:train_num]\n    x_test = _x[train_num:]\n    y_test = _y[train_num:]\n\n    _t = time.time()\n    _tree = CvDBase()\n    _tree.fit(x_train, y_train)\n    _tree.view()\n    _tree.estimate(x_test, y_test)\n    print(""Time cost: {:8.6}"".format(time.time() - _t))\n'"
Zhihu/NN/_extra/Layers.py,0,"b'from Zhihu.NN._extra.Optimizers import *\n\n\n# Abstract Layers\n\nclass Layer(metaclass=ABCMeta):\n    LayerTiming = Timing()\n\n    def __init__(self, shape):\n        """"""\n        :param shape: shape[0] = units of previous layer\n                      shape[1] = units of current layer (self)\n        """"""\n        self._shape = shape\n        self.parent = None\n        self.child = None\n\n    def feed_timing(self, timing):\n        if isinstance(timing, Timing):\n            self.LayerTiming = timing\n\n    @property\n    def name(self):\n        return str(self)\n\n    @property\n    def shape(self):\n        return self._shape\n\n    @shape.setter\n    def shape(self, value):\n        self._shape = value\n\n    # Core\n\n    def derivative(self, y, delta=None):\n        return self._derivative(y, delta)\n\n    @LayerTiming.timeit(level=1, prefix=""[Core] "")\n    def activate(self, x, w, bias=None, predict=False):\n        if isinstance(self, CostLayer):\n            return self._activate(x + bias, predict)\n        return self._activate(x.dot(w) + bias, predict)\n\n    @LayerTiming.timeit(level=1, prefix=""[Core] "")\n    def bp(self, y, w, prev_delta):\n        if self.child is not None:\n            return prev_delta\n        return prev_delta.dot(w.T) * self._derivative(y)\n\n    @abstractmethod\n    def _activate(self, x, predict):\n        pass\n\n    @abstractmethod\n    def _derivative(self, y, delta=None):\n        pass\n\n    # Util\n\n    @staticmethod\n    @LayerTiming.timeit(level=2, prefix=""[Core Util] "")\n    def safe_exp(y):\n        return np.exp(y - np.max(y, axis=1, keepdims=True))\n\n    def __str__(self):\n        return self.__class__.__name__\n\n    def __repr__(self):\n        return str(self)\n\n\n# Activation Layers\n\nclass Tanh(Layer):\n    def _activate(self, x, predict):\n        return np.tanh(x)\n\n    def _derivative(self, y, delta=None):\n        return 1 - y ** 2\n\n\nclass Sigmoid(Layer):\n    def _activate(self, x, predict):\n        return 1 / (1 + np.exp(-x))\n\n    def _derivative(self, y, delta=None):\n        return y * (1 - y)\n\n\nclass ELU(Layer):\n    def _activate(self, x, predict):\n        _rs, _rs0 = x.copy(), x < 0\n        _rs[_rs0] = np.exp(_rs[_rs0]) - 1\n        return _rs\n\n    def _derivative(self, y, delta=None):\n        _rs, _arg0 = np.zeros(y.shape), y < 0\n        _rs[_arg0], _rs[~_arg0] = y[_arg0] + 1, 1\n        return _rs\n\n\nclass ReLU(Layer):\n    def _activate(self, x, predict):\n        return np.maximum(0, x)\n\n    def _derivative(self, y, delta=None):\n        return y > 0\n\n\nclass Softplus(Layer):\n    def _activate(self, x, predict):\n        return np.log(1 + np.exp(x))\n\n    def _derivative(self, y, delta=None):\n        return 1 / (1 + 1 / (np.exp(y) - 1))\n\n\nclass Identical(Layer):\n    def _activate(self, x, predict):\n        return x\n\n    def _derivative(self, y, delta=None):\n        return 1\n\n\nclass Softmax(Layer):\n    def _activate(self, x, predict):\n        exp_y = Layer.safe_exp(x)\n        return exp_y / np.sum(exp_y, axis=1, keepdims=True)\n\n    def _derivative(self, y, delta=None):\n        return y * (1 - y)\n\n\n# Cost Layer\n\nclass CostLayer(Layer):\n    # Optimization\n    _batch_range = None\n\n    def __init__(self, parent, shape, cost_function=""Log Likelihood""):\n\n        Layer.__init__(self, shape)\n        self._parent = parent\n        self._available_cost_functions = {\n            ""MSE"": CostLayer._mse,\n            ""Cross Entropy"": CostLayer._cross_entropy,\n            ""Log Likelihood"": CostLayer._log_likelihood\n        }\n        self._cost_function_name = cost_function\n        self._cost_function = self._available_cost_functions[cost_function]\n\n    def _activate(self, x, predict):\n        return x\n\n    def _derivative(self, y, delta=None):\n        raise ValueError(""derivative function should not be called in CostLayer"")\n\n    def bp_first(self, y, y_pred):\n        if self._parent.name == ""Sigmoid"" and self.cost_function == ""Cross Entropy"":\n            return y * (1 - y_pred) - (1 - y) * y_pred\n        if self.cost_function == ""Log Likelihood"":\n            return -self._cost_function(y, y_pred) / 4\n        return -self._cost_function(y, y_pred) * self._parent.derivative(y_pred)\n\n    @property\n    def calculate(self):\n        return lambda y, y_pred: self._cost_function(y, y_pred, False)\n\n    @property\n    def cost_function(self):\n        return self._cost_function_name\n\n    # Cost Functions\n\n    @staticmethod\n    def _mse(y, y_pred, diff=True):\n        if diff:\n            return -y + y_pred\n        assert_string = ""y or y_pred should be np.ndarray in cost function""\n        assert isinstance(y, np.ndarray) or isinstance(y_pred, np.ndarray), assert_string\n        return 0.5 * np.average((y - y_pred) ** 2)\n\n    @staticmethod\n    def _cross_entropy(y, y_pred, diff=True):\n        if diff:\n            return -y / y_pred + (1 - y) / (1 - y_pred)\n        assert_string = ""y or y_pred should be np.ndarray in cost function""\n        assert isinstance(y, np.ndarray) or isinstance(y_pred, np.ndarray), assert_string\n        # noinspection PyTypeChecker\n        return np.average(-y * np.log(y_pred) - (1 - y) * np.log(1 - y_pred))\n\n    @classmethod\n    def _log_likelihood(cls, y, y_pred, diff=True, eps=1e-8):\n        if cls._batch_range is None:\n            cls._batch_range = np.arange(len(y_pred))\n        y_arg_max = np.argmax(y, axis=1)\n        if diff:\n            y_pred = y_pred.copy()\n            y_pred[cls._batch_range, y_arg_max] -= 1\n            return y_pred\n        return np.sum(-np.log(y_pred[range(len(y_pred)), y_arg_max] + eps)) / len(y)\n\n    def __str__(self):\n        return self._cost_function_name\n'"
Zhihu/NN/_extra/Optimizers.py,0,"b'import numpy as np\nfrom abc import ABCMeta, abstractmethod\n\nfrom Util.Timing import Timing\n\n\nclass Optimizers(metaclass=ABCMeta):\n\n    OptTiming = Timing()\n\n    def __init__(self, lr=0.01, cache=None):\n        self.lr = lr\n        self._cache = cache\n\n    @property\n    def name(self):\n        return str(self)\n\n    def feed_variables(self, variables):\n        self._cache = [\n            np.zeros(var.shape) for var in variables\n        ]\n\n    def feed_timing(self, timing):\n        if isinstance(timing, Timing):\n            self.OptTiming = timing\n\n    @abstractmethod\n    @OptTiming.timeit(level=1, prefix=""[API] "")\n    def run(self, i, dw):\n        raise NotImplementedError(""Please implement a \'run\' method for your optimizer"")\n\n    @OptTiming.timeit(level=4, prefix=""[API] "")\n    def update(self):\n        return self._update()\n\n    @abstractmethod\n    def _update(self):\n        raise NotImplementedError(""Please implement an \'update\' method for your optimizer"")\n\n    def __str__(self):\n        return self.__class__.__name__\n\n    def __repr__(self):\n        return str(self)\n\n\nclass SGD(Optimizers):\n\n    def run(self, i, dw):\n        return self.lr * dw\n\n    def _update(self):\n        pass\n\n\nclass Momentum(Optimizers):\n\n    def __init__(self, lr=0.01, cache=None, epoch=100, floor=0.5, ceiling=0.999):\n        Optimizers.__init__(self, lr, cache)\n        self._epoch, self._floor, self._ceiling = epoch, floor, ceiling\n        self._step = (ceiling - floor) / epoch\n        self._momentum = 0.5\n\n    @property\n    def epoch(self):\n        return self._epoch\n\n    @property\n    def floor(self):\n        return self._floor\n\n    @property\n    def ceiling(self):\n        return self._ceiling\n\n    def update_step(self):\n        self._step = (self._ceiling - self._floor) / self._epoch\n\n    @epoch.setter\n    def epoch(self, value):\n        self._epoch = value\n        self.update_step()\n\n    @floor.setter\n    def floor(self, value):\n        self._floor = value\n        self.update_step()\n\n    @ceiling.setter\n    def ceiling(self, value):\n        self._ceiling = value\n        self.update_step()\n\n    def run(self, i, dw):\n        velocity = self._cache\n        velocity[i] = velocity[i] * self._momentum + self.lr * dw\n        return velocity[i]\n\n    def _update(self):\n        if self._momentum < self._ceiling:\n            self._momentum += self._step\n\n\nclass NAG(Momentum):\n\n    def run(self, i, dw):\n        dw *= self.lr\n        velocity = self._cache\n        velocity[i] = self._momentum * velocity[i] + dw\n        return self._momentum * velocity[i] + dw\n\n\nclass Adam(Optimizers):\n\n    def __init__(self, lr=0.01, cache=None, beta1=0.9, beta2=0.999, eps=1e-8):\n        Optimizers.__init__(self, lr, cache)\n        self.beta1, self.beta2, self.eps = beta1, beta2, eps\n\n    def feed_variables(self, variables):\n        self._cache = [\n            [np.zeros(var.shape) for var in variables],\n            [np.zeros(var.shape) for var in variables],\n        ]\n\n    def run(self, i, dw):\n        self._cache[0][i] = self._cache[0][i] * self.beta1 + (1 - self.beta1) * dw\n        self._cache[1][i] = self._cache[1][i] * self.beta2 + (1 - self.beta2) * (dw ** 2)\n        return self.lr * self._cache[0][i] / (np.sqrt(self._cache[1][i] + self.eps))\n\n    def _update(self):\n        pass\n\n\nclass RMSProp(Optimizers):\n\n    def __init__(self, lr=0.01, cache=None, decay_rate=0.9, eps=1e-8):\n        Optimizers.__init__(self, lr, cache)\n        self.decay_rate, self.eps = decay_rate, eps\n\n    def run(self, i, dw):\n        self._cache[i] = self._cache[i] * self.decay_rate + (1 - self.decay_rate) * dw ** 2\n        return self.lr * dw / (np.sqrt(self._cache[i] + self.eps))\n\n    def _update(self):\n        pass\n\n\n# Factory\n\nclass OptFactory:\n    available_optimizers = {\n        ""MBGD"": SGD, ""Momentum"": Momentum, ""NAG"": NAG, ""Adam"": Adam, ""RMSProp"": RMSProp\n    }\n\n    def get_optimizer_by_name(self, name, variables, timing, lr, epoch):\n        try:\n            _optimizer = self.available_optimizers[name](lr)\n            if variables is not None:\n                _optimizer.feed_variables(variables)\n            _optimizer.feed_timing(timing)\n            if epoch is not None and isinstance(_optimizer, Momentum):\n                _optimizer.epoch = epoch\n            return _optimizer\n        except KeyError:\n            raise NotImplementedError(""Undefined Optimizer \'{}\' found"".format(name))\n'"
Zhihu/NN/one/Network.py,9,"b'from Zhihu.NN.Layers import *\nfrom Zhihu.NN.Optimizers import *\n\nnp.random.seed(142857)  # for reproducibility\n\n\n# Neural Network\n\nclass NNBase:\n    NNTiming = Timing()\n\n    def __init__(self):\n        self._layers = []\n        self._optimizer = None\n        self._current_dimension = 0\n\n        self._tfx = self._tfy = None\n        self._tf_weights, self._tf_bias = [], []\n        self._cost = self._y_pred = None\n\n        self._train_step = None\n\n    def __str__(self):\n        return ""Neural Network""\n\n    __repr__ = __str__\n\n    def feed_timing(self, timing):\n        if isinstance(timing, Timing):\n            self.NNTiming = timing\n            for layer in self._layers:\n                layer.feed_timing(timing)\n\n    @staticmethod\n    def _get_w(shape):\n        initial = tf.truncated_normal(shape, stddev=0.1)\n        return tf.Variable(initial, name=""w"")\n\n    @staticmethod\n    def _get_b(shape):\n        initial = tf.constant(0.1, shape=shape)\n        return tf.Variable(initial, name=""b"")\n\n    def _add_weight(self, shape):\n        w_shape = shape\n        b_shape = shape[1],\n        self._tf_weights.append(self._get_w(w_shape))\n        self._tf_bias.append(self._get_b(b_shape))\n\n    def _get_rs(self, x, y=None):\n        _cache = self._layers[0].activate(x, self._tf_weights[0], self._tf_bias[0])\n        for i, layer in enumerate(self._layers[1:]):\n            if i == len(self._layers) - 2:\n                if y is None:\n                    return tf.matmul(_cache, self._tf_weights[-1]) + self._tf_bias[-1]\n                return layer.activate(_cache, self._tf_weights[i + 1], self._tf_bias[i + 1], y)\n            _cache = layer.activate(_cache, self._tf_weights[i + 1], self._tf_bias[i + 1])\n        return _cache\n\n    def add(self, layer):\n        if not self._layers:\n            self._layers, self._current_dimension = [layer], layer.shape[1]\n            self._add_weight(layer.shape)\n        else:\n            _next = layer.shape[0]\n            self._layers.append(layer)\n            self._add_weight((self._current_dimension, _next))\n            self._current_dimension = _next\n\n\nclass NNDist(NNBase):\n\n    def __init__(self):\n        NNBase.__init__(self)\n        self._sess = tf.Session()\n\n    # Utils\n\n    def _get_prediction(self, x):\n        with self._sess.as_default():\n            return self._get_rs(x).eval(feed_dict={self._tfx: x})\n\n    # API\n\n    def fit(self, x=None, y=None, lr=0.001, epoch=10):\n        self._optimizer = Adam(lr)\n        self._tfx = tf.placeholder(tf.float32, shape=[None, x.shape[1]])\n        self._tfy = tf.placeholder(tf.float32, shape=[None, y.shape[1]])\n        with self._sess.as_default() as sess:\n            # Define session\n            self._cost = self._get_rs(self._tfx, self._tfy)\n            self._y_pred = self._get_rs(self._tfx)\n            self._train_step = self._optimizer.minimize(self._cost)\n            sess.run(tf.global_variables_initializer())\n            # Train\n            for counter in range(epoch):\n                self._train_step.run(feed_dict={self._tfx: x, self._tfy: y})\n\n    def predict_classes(self, x):\n        x = np.array(x)\n        return np.argmax(self._get_prediction(x), axis=1)\n\n    def evaluate(self, x, y):\n        y_pred = self.predict_classes(x)\n        y_arg = np.argmax(y, axis=1)\n        print(""Acc: {:8.6}"".format(np.sum(y_arg == y_pred) / len(y_arg)))\n'"
Zhihu/NN/one/Test.py,0,"b""from Zhihu.NN.one.Network import *\nfrom Zhihu.NN.Layers import *\n\nfrom Util.Util import DataUtil\n\nnp.random.seed(142857)  # for reproducibility\n\n\ndef main():\n\n    nn = NNDist()\n    epoch = 1000\n\n    x, y = DataUtil.gen_xor(100)\n\n    nn.add(ReLU((x.shape[1], 24)))\n    nn.add(CrossEntropy((y.shape[1],)))\n\n    nn.fit(x, y, epoch=epoch)\n    nn.evaluate(x, y)\n\nif __name__ == '__main__':\n    main()\n"""
Zhihu/NN/two/Network.py,9,"b'import matplotlib.pyplot as plt\r\n\r\nfrom Zhihu.NN.Layers import *\r\nfrom Zhihu.NN.Optimizers import *\r\n\r\nfrom Util.ProgressBar import ProgressBar\r\n\r\n\r\nclass NNVerbose:\r\n    NONE = 0\r\n    EPOCH = 1\r\n    METRICS = 2\r\n    METRICS_DETAIL = 3\r\n    DETAIL = 4\r\n    DEBUG = 5\r\n\r\n\r\n# Neural Network\r\n\r\nclass NNBase:\r\n\r\n    def __init__(self):\r\n        self._layers = []\r\n        self._optimizer = None\r\n        self._current_dimension = 0\r\n\r\n        self._tfx = self._tfy = None\r\n        self._tf_weights, self._tf_bias = [], []\r\n        self._cost = self._y_pred = None\r\n\r\n        self._train_step = None\r\n        self.verbose = 0\r\n\r\n    def __str__(self):\r\n        return ""Neural Network""\r\n\r\n    __repr__ = __str__\r\n\r\n    @staticmethod\r\n    def _get_w(shape):\r\n        initial = tf.truncated_normal(shape, stddev=0.1)\r\n        return tf.Variable(initial, name=""w"")\r\n\r\n    @staticmethod\r\n    def _get_b(shape):\r\n        initial = tf.constant(0.1, shape=shape)\r\n        return tf.Variable(initial, name=""b"")\r\n\r\n    def _add_weight(self, shape):\r\n        w_shape = shape\r\n        b_shape = shape[1],\r\n        self._tf_weights.append(self._get_w(w_shape))\r\n        self._tf_bias.append(self._get_b(b_shape))\r\n\r\n    def get_rs(self, x, y=None):\r\n        _cache = self._layers[0].activate(x, self._tf_weights[0], self._tf_bias[0])\r\n        for i, layer in enumerate(self._layers[1:]):\r\n            if i == len(self._layers) - 2:\r\n                if y is None:\r\n                    return tf.matmul(_cache, self._tf_weights[-1]) + self._tf_bias[-1]\r\n                return layer.activate(_cache, self._tf_weights[i + 1], self._tf_bias[i + 1], y)\r\n            _cache = layer.activate(_cache, self._tf_weights[i + 1], self._tf_bias[i + 1])\r\n        return _cache\r\n\r\n    def add(self, layer):\r\n        if not self._layers:\r\n            self._layers, self._current_dimension = [layer], layer.shape[1]\r\n            self._add_weight(layer.shape)\r\n        else:\r\n            _next = layer.shape[0]\r\n            self._layers.append(layer)\r\n            self._add_weight((self._current_dimension, _next))\r\n            self._current_dimension = _next\r\n\r\n\r\nclass NNDist(NNBase):\r\n\r\n    def __init__(self):\r\n        NNBase.__init__(self)\r\n        self._logs = {}\r\n        self._sess = tf.Session()\r\n        self._metrics, self._metric_names = [], []\r\n        self._available_metrics = {\r\n            ""acc"": NNDist._acc, ""_acc"": NNDist._acc,\r\n            ""f1_score"": NNDist._f1_score, ""_f1_score"": NNDist._f1_score\r\n        }\r\n\r\n    # Utils\r\n\r\n    def _get_prediction(self, x, name=None, batch_size=1e6, verbose=None, out_of_sess=False):\r\n        if verbose is None:\r\n            verbose = self.verbose\r\n        fc_shape = np.prod(x.shape[1:])  # type: float\r\n        single_batch = int(batch_size / fc_shape)\r\n        if not single_batch:\r\n            single_batch = 1\r\n        if single_batch >= len(x):\r\n            if not out_of_sess:\r\n                return self._y_pred.eval(feed_dict={self._tfx: x})\r\n            with self._sess.as_default():\r\n                x = x.astype(np.float32)\r\n                return self.get_rs(x).eval(feed_dict={self._tfx: x})\r\n        epoch = int(len(x) / single_batch)\r\n        if not len(x) % single_batch:\r\n            epoch += 1\r\n        name = ""Prediction"" if name is None else ""Prediction ({})"".format(name)\r\n        sub_bar = ProgressBar(max_value=epoch, name=name, start=False)\r\n        if verbose >= NNVerbose.METRICS:\r\n            sub_bar.start()\r\n        if not out_of_sess:\r\n            rs = [self._y_pred.eval(feed_dict={self._tfx: x[:single_batch]})]\r\n        else:\r\n            rs = [self.get_rs(x[:single_batch])]\r\n        count = single_batch\r\n        if verbose >= NNVerbose.METRICS:\r\n            sub_bar.update()\r\n        while count < len(x):\r\n            count += single_batch\r\n            if count >= len(x):\r\n                if not out_of_sess:\r\n                    rs.append(self._y_pred.eval(feed_dict={self._tfx: x[count - single_batch:]}))\r\n                else:\r\n                    rs.append(self.get_rs(x[count - single_batch:]))\r\n            else:\r\n                if not out_of_sess:\r\n                    rs.append(self._y_pred.eval(feed_dict={self._tfx: x[count - single_batch:count]}))\r\n                else:\r\n                    rs.append(self.get_rs(x[count - single_batch:count]))\r\n            if verbose >= NNVerbose.METRICS:\r\n                sub_bar.update()\r\n        if out_of_sess:\r\n            with self._sess.as_default():\r\n                rs = [_rs.eval() for _rs in rs]\r\n        return np.vstack(rs)\r\n\r\n    def _append_log(self, x, y, name, out_of_sess=False):\r\n        y_pred = self._get_prediction(x, name, out_of_sess=out_of_sess)\r\n        for i, metric in enumerate(self._metrics):\r\n            self._logs[name][i].append(metric(y, y_pred))\r\n        if not out_of_sess:\r\n            self._logs[name][-1].append(self._layers[-1].calculate(y, y_pred).eval())\r\n        else:\r\n            with self._sess.as_default():\r\n                self._logs[name][-1].append(self._layers[-1].calculate(y, y_pred).eval())\r\n\r\n    def _print_metric_logs(self, data_type):\r\n        print()\r\n        print(""="" * 47)\r\n        for i, name in enumerate(self._metric_names):\r\n            print(""{:<16s} {:<16s}: {:12.8}"".format(\r\n                data_type, name, self._logs[data_type][i][-1]))\r\n        print(""{:<16s} {:<16s}: {:12.8}"".format(\r\n            data_type, ""loss"", self._logs[data_type][-1][-1]))\r\n        print(""="" * 47)\r\n\r\n    # Metrics\r\n\r\n    @staticmethod\r\n    def _acc(y, y_pred):\r\n        y_arg, y_pred_arg = np.argmax(y, axis=1), np.argmax(y_pred, axis=1)\r\n        return np.sum(y_arg == y_pred_arg) / len(y_arg)\r\n\r\n    @staticmethod\r\n    def _f1_score(y, y_pred):\r\n        y_true, y_pred = np.argmax(y, axis=1), np.argmax(y_pred, axis=1)\r\n        tp = np.sum(y_true * y_pred)\r\n        if tp == 0:\r\n            return .0\r\n        fp = np.sum((1 - y_true) * y_pred)\r\n        fn = np.sum(y_true * (1 - y_pred))\r\n        return 2 * tp / (2 * tp + fn + fp)\r\n\r\n    # API\r\n\r\n    def fit(self, x=None, y=None, lr=0.01, epoch=10, batch_size=128, train_rate=None,\r\n            verbose=0, metrics=None, record_period=100):\r\n\r\n        self.verbose = verbose\r\n        self._optimizer = Adam(lr)\r\n        self._tfx = tf.placeholder(tf.float32, shape=[None, x.shape[1]])\r\n        self._tfy = tf.placeholder(tf.float32, shape=[None, y.shape[1]])\r\n\r\n        if train_rate is not None:\r\n            train_rate = float(train_rate)\r\n            train_len = int(len(x) * train_rate)\r\n            shuffle_suffix = np.random.permutation(int(len(x)))\r\n            x, y = x[shuffle_suffix], y[shuffle_suffix]\r\n            x_train, y_train = x[:train_len], y[:train_len]\r\n            x_test, y_test = x[train_len:], y[train_len:]\r\n        else:\r\n            x_train = x_test = x\r\n            y_train = y_test = y\r\n\r\n        train_len = len(x_train)\r\n        batch_size = min(batch_size, train_len)\r\n        do_random_batch = train_len >= batch_size\r\n        train_repeat = int(train_len / batch_size) + 1\r\n\r\n        self._metrics = [""acc""] if metrics is None else metrics\r\n        for i, metric in enumerate(self._metrics):\r\n            if isinstance(metric, str):\r\n                self._metrics[i] = self._available_metrics[metric]\r\n        self._metric_names = [_m.__name__ for _m in self._metrics]\r\n        self._logs = {\r\n            name: [[] for _ in range(len(self._metrics) + 1)] for name in (""train"", ""test"")\r\n        }\r\n\r\n        bar = ProgressBar(max_value=max(1, epoch // record_period), name=""Epoch"", start=False)\r\n        if self.verbose >= NNVerbose.EPOCH:\r\n            bar.start()\r\n\r\n        with self._sess.as_default() as sess:\r\n\r\n            # Define session\r\n            self._cost = self.get_rs(self._tfx, self._tfy)\r\n            self._y_pred = self.get_rs(self._tfx)\r\n            self._train_step = self._optimizer.minimize(self._cost)\r\n            sess.run(tf.global_variables_initializer())\r\n\r\n            # Train\r\n            sub_bar = ProgressBar(max_value=train_repeat * record_period - 1, name=""Iteration"", start=False)\r\n            for counter in range(epoch):\r\n                if self.verbose >= NNVerbose.EPOCH and counter % record_period == 0:\r\n                    sub_bar.start()\r\n                for _i in range(train_repeat):\r\n                    if do_random_batch:\r\n                        batch = np.random.choice(train_len, batch_size)\r\n                        x_batch, y_batch = x_train[batch], y_train[batch]\r\n                    else:\r\n                        x_batch, y_batch = x_train, y_train\r\n                    self._train_step.run(feed_dict={self._tfx: x_batch, self._tfy: y_batch})\r\n                    if self.verbose >= NNVerbose.EPOCH:\r\n                        if sub_bar.update() and self.verbose >= NNVerbose.METRICS_DETAIL:\r\n                            self._append_log(x_train, y_train, ""train"")\r\n                            self._append_log(x_test, y_test, ""test"")\r\n                            self._print_metric_logs(""train"")\r\n                            self._print_metric_logs(""test"")\r\n                if self.verbose >= NNVerbose.EPOCH:\r\n                    sub_bar.update()\r\n                if (counter + 1) % record_period == 0:\r\n                    self._append_log(x_train, y_train, ""train"")\r\n                    self._append_log(x_test, y_test, ""test"")\r\n                    if self.verbose >= NNVerbose.METRICS:\r\n                        self._print_metric_logs(""train"")\r\n                        self._print_metric_logs(""test"")\r\n                    if self.verbose >= NNVerbose.EPOCH:\r\n                        bar.update(counter // record_period + 1)\r\n                        sub_bar = ProgressBar(max_value=train_repeat * record_period - 1, name=""Iteration"", start=False)\r\n\r\n    def draw_logs(self):\r\n        metrics_log, loss_log = {}, {}\r\n        for key, value in sorted(self._logs.items()):\r\n            metrics_log[key], loss_log[key] = value[:-1], value[-1]\r\n        for i, name in enumerate(sorted(self._metric_names)):\r\n            plt.figure()\r\n            plt.title(""Metric Type: {}"".format(name))\r\n            for key, log in sorted(metrics_log.items()):\r\n                xs = np.arange(len(log[i])) + 1\r\n                plt.plot(xs, log[i], label=""Data Type: {}"".format(key))\r\n            plt.legend(loc=4)\r\n            plt.show()\r\n            plt.close()\r\n        plt.figure()\r\n        plt.title(""Loss"")\r\n        for key, loss in sorted(loss_log.items()):\r\n            xs = np.arange(len(loss)) + 1\r\n            plt.plot(xs, loss, label=""Data Type: {}"".format(key))\r\n        plt.legend()\r\n        plt.show()\r\n\r\n    def predict(self, x):\r\n        return self._get_prediction(x, out_of_sess=True)\r\n\r\n    def predict_classes(self, x):\r\n        x = np.array(x)\r\n        return np.argmax(self._get_prediction(x, out_of_sess=True), axis=1)\r\n\r\n    def evaluate(self, x, y):\r\n        y_pred = self.predict_classes(x)\r\n        y_arg = np.argmax(y, axis=1)\r\n        print(""Acc: {:8.6}"".format(np.sum(y_arg == y_pred) / len(y_arg)))\r\n\r\n    def visualize_2d(self, x, y, plot_scale=2, plot_precision=0.01):\r\n        plot_num = int(1 / plot_precision)\r\n        xf = np.linspace(np.min(x) * plot_scale, np.max(x) * plot_scale, plot_num)\r\n        yf = np.linspace(np.min(x) * plot_scale, np.max(x) * plot_scale, plot_num)\r\n        input_x, input_y = np.meshgrid(xf, yf)\r\n        input_xs = np.c_[input_x.ravel(), input_y.ravel()]\r\n        output_ys_2d = np.argmax(self.predict(input_xs), axis=1).reshape(len(xf), len(yf))\r\n        plt.contourf(input_x, input_y, output_ys_2d, cmap=plt.cm.Spectral)\r\n        plt.scatter(x[:, 0], x[:, 1], c=np.argmax(y, axis=1), s=40, cmap=plt.cm.Spectral)\r\n        plt.axis(""off"")\r\n        plt.show()\r\n'"
Zhihu/NN/two/Test.py,0,"b'from Zhihu.NN.two.Network import *\r\nfrom Zhihu.NN.Layers import *\r\n\r\nfrom Util.Util import DataUtil\r\n\r\nnp.random.seed(142857)  # for reproducibility\r\n\r\n\r\ndef main():\r\n\r\n    nn = NNDist()\r\n    epoch = 1000\r\n\r\n    x, y = DataUtil.gen_spiral(100)\r\n\r\n    nn.add(ReLU((x.shape[1], 24)))\r\n    nn.add(ReLU((24,)))\r\n    nn.add(CrossEntropy((y.shape[1],)))\r\n\r\n    nn.fit(x, y, epoch=epoch, verbose=2, metrics=[""acc"", ""f1_score""], train_rate=0.8)\r\n    nn.draw_logs()\r\n    nn.visualize_2d(x, y)\r\n\r\nif __name__ == \'__main__\':\r\n    main()\r\n'"
Zhihu/Python/Utils/pycmd.py,0,"b'import os\nimport shutil\nimport platform\n\n# Handle packages\n\ntry:\n    from pynput.keyboard import Key, Listener, Controller\nexcept ImportError:\n    print(""-- Warning: pynput is not installed on this machine, auto completion will not be supported --"")\n    print(""-- You can type \'pip install pynput\' to get pynput and then type \'refresh\' to play with auto completion --"")\n    Key = Listener = Controller = None\n\n# Handle cross-platform figures\n\n_bad_slash, _tar_slash = ""/"", ""\\\\""\nif platform.system() != ""Windows"":\n    _bad_slash, _tar_slash = ""\\\\"", ""/""\n\n\nclass Util:\n\n    @staticmethod\n    def msg(dt, *args):\n        if dt == ""undefined_error"":\n            return Util.msg(\n                ""block_msg"", ""Error\\n"",\n                ""Undefined command \'{}\', type \'help\' for more information\\n"".format(args[0]))\n        if dt == ""root_path_error"":\n            return Util.msg(\n                ""block_msg"", ""Error\\n"",\n                ""Current path \'{}\' is the root path\\n"".format(args[0]))\n        if dt == ""valid_path_error"":\n            return Util.msg(\n                ""block_msg"", ""Error\\n"",\n                ""\'{}\' is not a valid {}\\n"".format(args[0], args[1]))\n\n        if dt == ""block_msg"":\n            return ""="" * 30 + ""\\n"" + args[0] + ""-"" * 30 + ""\\n"" + args[1] + ""-"" * 30\n        if dt == ""show_ls_message"":\n            title, folder_lst, file_lst = args\n            body = (\n                ""\\n"".join(["" folder - {}"".format(_f) for _f in folder_lst]) + ""\\n"" +\n                ""\\n"".join(["" file   - {}"".format(_f) for _f in file_lst]) + ""\\n""\n            ) if file_lst or folder_lst else ""-- Empty --\\n""\n            return Util.msg(""block_msg"", title, body)\n        \n    @staticmethod\n    def get_cmd(msg):\n        if Listener is not None:\n            with Listener(on_press=CmdTool.on_press, on_release=CmdTool.on_release) as listener:\n                cmd = input(msg).strip()\n                listener.join()\n                return cmd.replace(_bad_slash, _tar_slash)\n        else:\n            cmd = input(msg).strip()\n            return cmd.replace(_bad_slash, _tar_slash)\n\n    @staticmethod\n    def get_formatted_error(err):\n        return ""-- {} --\\n"".format(err)\n\n    @staticmethod\n    def get_clean_path(path, user_platform):\n        if user_platform == ""Windows"":\n            while path.find(""\\\\\\\\"") >= 0:\n                path = path.replace(""\\\\\\\\"", ""\\\\"")\n        else:\n            while path.find(""//"") >= 0:\n                path = path.replace(""//"", ""/"")\n        while len(path) > 3 and path[-1] == _tar_slash:\n            path = path[:-1]\n        return path\n\n    @staticmethod\n    def get_short_path(path, length):\n        if len(path) >= length:\n            return ""..{}"".format(path[len(path) - length:len(path)])\n        return path\n\n    @staticmethod\n    def get_two_paths(cmd_tool, msgs, args):\n        cmd = args[0].split()\n        first_flag, second_flag = False, False\n        first_arg, second_arg = """", """"\n        if len(cmd) == 1:\n            first_flag = second_flag = True\n        elif len(cmd) == 2:\n            first_arg = cmd[1]\n            second_flag = True\n        elif len(cmd) == 3:\n            first_arg, second_arg = cmd[1:]\n        if first_flag:\n            first_arg = Util.get_cmd(msgs[0])\n        if second_flag:\n            second_arg = Util.get_cmd(msgs[1])\n        first_path = cmd_tool.file_path if first_arg == ""."" else CmdTool.get_path(first_arg)\n        second_path = cmd_tool.file_path if first_arg == ""."" else CmdTool.get_path(second_arg)\n        return first_path, second_path\n\n    @staticmethod\n    def show_help_msg(dt):\n\n        # Basic\n        if dt == ""help"":\n            print(""Help (help)    -> type \'help\' to see available commands,\\n""\n                  ""                  type \'help **\' to see the function of **"")\n        elif dt == ""cd"":\n            print(""Help (cd)      -> used to get into a folder"")\n        elif dt == ""ls"":\n            print(""Help (ls)      -> used to view files & folders"")\n        elif dt == ""rm"":\n            print(""Help (rm)      -> used to remove files or folders"")\n        elif dt == ""mk"":\n            print(""Help (mk)      -> used to make a file or a folder\\n""\n                  ""                  it is recommended to type \'mk (dir) (type) (name)\'\\n""\n                  ""                  dir == \'.\' -> direct to current folder\\n""\n                  ""                  assert(type in (\'folder\', \'file\'))"")\n        elif dt == ""mv"":\n            print(""Help (mv)      -> used to move a file or a folder\\n""\n                  ""                  it is recommended to type \'mv (old_dir) (new_dir)\'\\n"")\n\n        elif dt == ""refresh"":\n            print(""Help (refresh) -> used to refresh this command line tool.\\n""\n                  ""                  you\'ll be sent back to \'Root\' status"")\n        elif dt == ""exit"":\n            print(""Help (exit)    -> used to exit current status.\\n""\n                  ""                  used to exit this command line tool when you\'re under \'Root\' status"")\n\n        elif dt == ""config"":\n            print(""Help (config)  -> type \'config\' to change configurations of all available tools,\\n""\n                  ""                  type \'config rename\' to change configurations of \'Rename\' tool"")\n\n        # Root\n        elif dt == ""rename"":\n            print(""Help (rename)  -> get use of \'Rename\' tool"")\n        elif dt == ""python"":\n            print(""Help (python)  -> get use of Python"")\n\n        # Rename\n        elif dt == ""folder"":\n            print(""Help (folder)  -> used to rename a folder in current folder\\n""\n                  ""                  it is recommended to type \'folder (old_name) (new_name)\'\\n"")\n        elif dt == ""file"":\n            print(""Help (file)    -> used to rename a file in current folder\\n""\n                  ""                  it is recommended to type \'file (old_name) (new_name)\'\\n"")\n\n        else:\n            raise NotImplementedError\n\n    @staticmethod\n    def do_system_default(cmd):\n        print(Util.msg(""block_msg"", ""Caution\\n"", ""Running default command in Windows command line\\n""))\n        os.system(cmd)\n\n\nclass CmdTool:\n\n    _platform = platform.system()\n    _file_path = os.getcwd()\n    _self_path = ""\\""{}{}pycmd\\"".py"".format(_file_path, _tar_slash)\n\n    _current_command = []\n    _do_auto_complete = True\n    _auto_complete = """"\n    _auto_complete_lst = []\n    _auto_complete_track = []\n    _auto_complete_flag = False\n    _auto_complete_cursor = 0\n    _auto_complete_finish = True\n\n    if Controller is not None:\n        _keyboard = Controller()\n    else:\n        _keyboard = None\n\n    @classmethod\n    def clear_cache(cls):\n        cls._current_command = []\n        cls._do_auto_complete = True\n        cls._auto_complete = """"\n        cls._auto_complete_lst = []\n        cls._auto_complete_track = []\n\n    @classmethod\n    def click(cls, key):\n        cls._keyboard.press(key)\n        cls._keyboard.release(key)\n\n    @classmethod\n    def on_press(cls, key):\n        if key == Key.tab and cls._current_command and cls._do_auto_complete:\n            cls._auto_complete_flag = True\n            cls._auto_complete_finish = False\n\n            full_cmd = """".join(cls._current_command).split()\n            cmd = full_cmd[0]\n            if not cls._auto_complete_track:\n                pth_head = full_cmd[-1]\n                pth_head_len = len(pth_head)\n            else:\n                pth_head = """".join(cls._current_command).split(_tar_slash)[-1]\n                pth_head_len = len(pth_head)\n\n            track = _tar_slash.join(cls._auto_complete_track)\n            cls._auto_complete_lst = [_f for _f in os.listdir(cls.get_path(track)) if _f[:pth_head_len] == pth_head]\n\n            add_back_slash = False\n            if cmd in (""cd"", ""ls""):\n                cls._auto_complete_lst = [\n                    _f for _f in cls._auto_complete_lst if os.path.isdir(cls.get_path(track + _tar_slash + _f))]\n                add_back_slash = True\n            if len(cls._auto_complete_lst) == 1:\n                cls._auto_complete = cls._auto_complete_lst[0][pth_head_len:]\n            else:\n                cls._auto_complete = """"\n\n            cls._current_command.append(""\\t"")\n            cls.click(Key.backspace)\n            if cls._auto_complete:\n                cls._auto_complete_track.append(pth_head + cls._auto_complete)\n                if add_back_slash:\n                    cls._auto_complete += _tar_slash\n                cls._keyboard.type(cls._auto_complete)\n\n            cls._auto_complete_flag = False\n            if not cls._auto_complete:\n                cls._auto_complete_finish = True\n\n    @classmethod\n    def on_release(cls, key):\n        if not cls._auto_complete_flag:\n            if key == Key.enter:\n                cls.clear_cache()\n                return False\n            if key == Key.left or key == Key.right or key == Key.up:\n                cls._do_auto_complete = False\n            elif key == Key.backspace:\n                if cls._auto_complete_finish:\n                    cls._do_auto_complete = False\n                if cls._current_command:\n                    cls._current_command.pop()\n            elif key == Key.space and cls._auto_complete_finish:\n                cls._auto_complete_track = []\n                cls._current_command.append("" "")\n            elif key == Key.tab:\n                pass\n            else:\n                if cls._auto_complete_finish:\n                    char = str(key)[1:-1].strip()\n                    if len(char) == 1:\n                        cls._current_command.append(char)\n                    elif char == ""\\\\\\\\"":\n                        cls._current_command.append(""\\\\"")\n                    elif char == ""/"":\n                        cls._current_command.append(""/"")\n                else:\n                    if cls._auto_complete_cursor < len(cls._auto_complete):\n                        cls._current_command.append(cls._auto_complete[cls._auto_complete_cursor])\n                        cls._auto_complete_cursor += 1\n                        if cls._auto_complete_cursor == len(cls._auto_complete):\n                            cls._auto_complete_cursor = 0\n                            cls._auto_complete_finish = True\n\n    @classmethod\n    def get_path(cls, ad):\n        if not ad or ad == ""."":\n            return cls._file_path\n        return cls._file_path + _tar_slash + ad\n    \n    @classmethod\n    def get_path_from_cmd(cls, raw_cmd, cmd_type, tar_type, base_path=None, allow_no_param=True, custom_msg=""""):\n        \n        if base_path is None:\n            tmp_path = Util.get_clean_path(cls._file_path, CmdTool._platform)\n        else:\n            tmp_path = Util.get_clean_path(base_path, CmdTool._platform)\n\n        if cmd_type and not allow_no_param:\n            if raw_cmd == cmd_type:\n                if not custom_msg:\n                    raw_cmd = cmd_type + "" "" + Util.get_cmd(""{} -> "".format(cmd_type))\n                else:\n                    raw_cmd = cmd_type + "" "" + Util.get_cmd(custom_msg)\n\n            if raw_cmd[2] != "" "":\n                print(Util.msg(""undefined_error"", raw_cmd))\n                return tmp_path\n\n            if not raw_cmd or raw_cmd == ""."":\n                return tmp_path\n\n        elif (not raw_cmd and not cmd_type) or (allow_no_param and raw_cmd == cmd_type):\n            raw_cmd = ""** .""\n\n        if len(raw_cmd) > 3:\n            raw_cmd = raw_cmd[3:]\n\n        if raw_cmd[0] == ""."":\n\n            dot_counter = 1\n            while dot_counter < len(raw_cmd) and raw_cmd[dot_counter] == ""."":\n                dot_counter += 1\n            pth_length = tmp_path.count(_tar_slash) + 1\n            if dot_counter > pth_length:\n                print(Util.msg(""block_msg"", ""Path Error\\n"",\n                               ""-- Too many \'.\': {} which exceed {} --"".format(dot_counter, pth_length)))\n                return tmp_path\n            if dot_counter == len(raw_cmd):\n                while dot_counter > 1:\n                    tmp_path = tmp_path[:tmp_path.rfind(_tar_slash)]\n                    dot_counter -= 1\n            else:\n                addition_path = raw_cmd[dot_counter:]\n                while dot_counter > 1:\n                    tmp_path = tmp_path[:tmp_path.rfind(_tar_slash)]\n                    dot_counter -= 1\n                tmp_path += _tar_slash + addition_path\n                tmp_path = Util.get_clean_path(tmp_path, CmdTool._platform)\n\n        else:\n            tmp_path = Util.get_clean_path(CmdTool.get_path(raw_cmd), CmdTool._platform)\n\n        if tar_type != ""all"" and (\n            (tar_type.find(""folder"") >= 0 and not os.path.isdir(tmp_path)) or\n            (tar_type.find(""file"") >= 0and not os.path.isfile(tmp_path))\n        ):\n            print(Util.msg(""valid_path_error"", tmp_path, tar_type))\n            return tmp_path\n\n        return tmp_path\n\n    @property\n    def file_path(self):\n        return self._file_path\n\n    def __init__(self, file_path=None):\n\n        self._status = ""root""\n        if file_path is None:\n            self._file_path = CmdTool._file_path\n        else:\n            self._file_path = CmdTool._file_path = file_path\n\n        self._common_command = (""help"", ""config"", ""refresh"", ""exit"")\n        self._advance_command = (""cd"", ""ls"", ""rm"", ""mk"", ""mv"", ""cp"")\n        self._break_command = (""refresh"", ""exit"")\n\n    def _get_cmd(self, msg, format_path=False):\n\n        def _cmd():\n            if format_path:\n                return Util.get_cmd(msg.format(self._file_path))\n            return Util.get_cmd(msg)\n\n        cmd = _cmd()\n        while self._do_common_work(cmd):\n            if self._status == ""exit"":\n                return ""exit""\n            cmd = _cmd()\n        return cmd\n\n    def _cd(self, args):\n        self._file_path = CmdTool.get_path_from_cmd(args[0], ""cd"", ""folder"", allow_no_param=False)\n        self._update_path()\n        return\n\n    @classmethod\n    def _ls(cls, args):\n        pth = cls.get_path_from_cmd(args[0], ""ls"", ""folder"")\n\n        folder_lst, file_lst = [], []\n        for _f in os.listdir(pth):\n            _p = pth + _tar_slash + _f\n            if os.path.isdir(_p):\n                folder_lst.append(_f)\n            elif os.path.isfile(_p):\n                file_lst.append(_f)\n\n        try:\n            print(Util.msg(""show_ls_message"", ""Files & Folders in \'{}\':\\n"".format(pth), folder_lst, file_lst))\n        except UnicodeEncodeError as err:\n            print(Util.msg(""block_msg"", ""Encoding Error\\n"", Util.get_formatted_error(err)))\n\n    def _rm(self, args):\n        pth = CmdTool.get_path_from_cmd(args[0], ""rm"", ""all"")\n\n        if not os.path.isdir(pth):\n            if not os.path.isfile(pth):\n                print(Util.msg(""block_msg"", ""Path Error\\n"", ""-- \'{}\' not exists\\n --"".format(pth)))\n            else:\n                print(Util.msg(""block_msg"", ""Removing\\n"", pth + ""\\n""))\n                if self._get_cmd(""(Root) Sure to proceed ? (y/n) -> "").lower() == ""y"":\n                    try:\n                        os.remove(pth)\n                    except PermissionError as err:\n                        print(Util.msg(""block_msg"", ""Permission Error\\n"", Util.get_formatted_error(err)))\n            return\n\n        folder_lst, file_lst = [], []\n        for _f in os.listdir(pth):\n            _p = pth + _tar_slash + _f\n            if os.path.isdir(_p):\n                folder_lst.append(_f)\n            elif os.path.isfile(_p):\n                file_lst.append(_f)\n\n        try:\n            print(Util.msg(""show_ls_message"", ""Removing \'{}\'\\nWhich contains\\n"".format(pth), folder_lst, file_lst))\n        except UnicodeEncodeError as err:\n            print(Util.msg(""block_msg"", ""Encoding Error\\n"", Util.get_formatted_error(err)))\n\n        if self._get_cmd(""(Root) Sure to proceed ? (y/n) -> "").lower() == ""y"":\n            try:\n                if os.path.isfile(pth):\n                    os.remove(pth)\n                else:\n                    if pth == self._file_path:\n                        self._do_common(""cd"", ""cd .."")\n                    shutil.rmtree(pth)\n            except PermissionError as err:\n                print(Util.msg(""block_msg"", ""Permission Error\\n"", Util.get_formatted_error(err)))\n        else:\n            print(""(Root) Caution -> Nothing happened"")\n\n    def _mk(self, args):\n\n        mk = args[0].split()\n\n        path_flag, type_flag, name_flag = False, False, False\n        mk_path, mk_type, mk_name = """", """", """"\n\n        if len(mk) == 1:\n            path_flag = type_flag = name_flag = True\n        elif len(mk) == 2:\n            mk_path = mk[1]\n            type_flag = name_flag = True\n        elif len(mk) == 3:\n            mk_path, mk_type = mk[1:]\n            name_flag = True\n        else:\n            mk_path, mk_type = mk[1:3]\n            mk_name = "" "".join(mk[3:])\n\n        if path_flag:\n            mk_path = CmdTool.get_path(Util.get_cmd(""(mk) dir -> ""))\n        else:\n            mk_path = self._file_path if mk_path == ""."" else CmdTool.get_path(mk_path)\n        if type_flag:\n            mk_type = Util.get_cmd(""(mk) type (folder or file) -> "")\n        if name_flag:\n            mk_name = Util.get_cmd(""(mk) name -> "")\n\n        if not os.path.isdir(mk_path):\n            print(Util.msg(""valid_path_error"", mk_path, ""folder""))\n            return\n\n        if not mk_type or mk_type not in (""folder"", ""file""):\n            print(""(Root) Caution -> Nothing happened"")\n            return\n\n        mk_dir = mk_path + _tar_slash + mk_name\n\n        if mk_type == ""folder"":\n            try:\n                os.mkdir(mk_dir)\n            except FileExistsError as err:\n                print(Util.msg(""block_msg"", ""File Exists Error\\n"", Util.get_formatted_error(err)))\n        elif mk_type == ""file"":\n            if os.path.isfile(mk_dir):\n                print(Util.msg(""block_msg"", ""Error\\n"", ""\'{}\' already exists\\n"".format(mk_dir)))\n            else:\n                try:\n                    with open(mk_dir, ""w""):\n                        pass\n                except PermissionError as err:\n                    print(Util.msg(""block_msg"", ""Permission Error\\n"", Util.get_formatted_error(err)))\n\n    def _mv(self, args):\n        old_path, new_path = Util.get_two_paths(\n            self, (""(mv) Old dir -> "", ""(mv) New dir -> ""), args\n        )\n\n        if not os.path.isfile(old_path) and not os.path.isdir(old_path):\n            print(Util.msg(""valid_path_error"", old_path, ""dir""))\n\n        try:\n            shutil.move(old_path, new_path)\n        except FileExistsError as err:\n            print(Util.msg(""block_msg"", ""File Exists Error\\n"", Util.get_formatted_error(err)))\n\n    def _cp(self, args):\n        old_path, new_path = Util.get_two_paths(\n            self, (""(cp) Old dir -> "", ""(cp) New dir -> ""), args\n        )\n\n        if not os.path.isfile(old_path) and not os.path.isdir(old_path):\n            print(Util.msg(""valid_path_error"", old_path, ""dir""))\n\n        try:\n            if os.path.isfile(old_path):\n                shutil.copyfile(old_path, new_path)\n            else:\n                shutil.copytree(old_path, new_path)\n        except FileExistsError as err:\n            print(Util.msg(""block_msg"", ""File Exists Error\\n"", Util.get_formatted_error(err)))\n\n    def _help(self, args):\n        pass\n\n    def _config(self, args):\n        pass\n\n    def _check_path(self, cmd):\n        if len(cmd) >= 2 and cmd[1] == "":"":\n            if len(cmd) == 2:\n                cmd += _tar_slash\n            if os.path.isdir(cmd):\n                self._file_path = cmd\n                self._update_path()\n                return True\n            return False\n\n    def _do_common(self, dt, *args):\n        try:\n            if dt in self._break_command:\n                self._status = ""exit""\n                if dt == ""refresh"":\n                    os.system(""python {}"".format(CmdTool._self_path))\n                return False\n            if dt == ""help"":\n                self._help(args)\n            elif dt == ""config"":\n                self._config(args)\n            elif dt == ""cd"":\n                self._cd(args)\n            elif dt == ""ls"":\n                self._ls(args)\n            elif dt == ""rm"":\n                self._rm(args)\n            elif dt == ""mk"":\n                self._mk(args)\n            elif dt == ""mv"":\n                self._mv(args)\n            elif dt == ""cp"":\n                self._cp(args)\n        except Exception as err:\n            print(Util.msg(""block_msg"", ""-- Special Error --\\n"", Util.get_formatted_error(err)))\n        return True\n\n    def _do_common_work(self, cmd):\n        if self._status == ""exit"":\n            return False\n        if self._check_path(cmd):\n            return True\n        if cmd in self._common_command:\n            return self._do_common(cmd)\n        if cmd[:2] in self._advance_command:\n            self._do_common(cmd[:2], cmd)\n        elif cmd[:4] == ""help"":\n            self._do_common(""help"", cmd[4:].strip())\n        elif cmd[:6] == ""config"":\n            self._do_common(""config"", cmd[6:].strip())\n        else:\n            return False\n        return True\n\n    def _renew_path(self, parent):\n        parent._file_path = self._file_path\n\n    def _update_path(self):\n        os.chdir(self._file_path)\n        CmdTool._file_path = self._file_path\n\n\nclass Rename(CmdTool):\n\n    def __init__(self, file_path, pl, fl, whether_preview_detail, parent):\n\n        CmdTool.__init__(self, file_path)\n        self._parent = parent\n\n        self._basic_batch_command = (""folders"", ""files"")\n        self._basic_command = (""folder"", ""file"")\n        self._special_command = (""batch"", ""seq"")\n\n        self._root_commands = [\n            self._common_command, self._advance_command,\n            self._basic_batch_command, self._basic_command,\n            self._special_command\n        ]\n        self._batch_commands = [\n            self._common_command, self._advance_command,\n            self._basic_batch_command, self._basic_command\n        ]\n\n        self._batch_lst = []\n        self._log_lst = []\n        self._err_lst = []\n\n        self._default_pl = pl\n        self._default_fl = fl\n        self._prev_detail = whether_preview_detail\n\n        self._path_length = pl\n        self._fn_length = fl\n        self._fn_max = 10 ** self._fn_length - 1\n\n    def _rename(self, args):\n        old_path, new_path = Util.get_two_paths(\n            self, (\n                ""{} (Rename {}) Old name -> "".format(args[1], args[0]),\n                ""{} (Rename {}) New name -> "".format(args[1], args[0])\n            ), args\n        )\n\n        if (\n            (args[0] == ""file"" and not os.path.isfile(old_path)) or\n            (args[0] == ""folder"" and not os.path.isdir(old_path))\n        ):\n            print(Util.msg(""valid_path_error"", old_path, args[0]))\n            return\n\n        try:\n            os.rename(old_path, new_path)\n        except FileExistsError as err:\n            print(Util.msg(""block_msg"", ""File Exists Error\\n"", Util.get_formatted_error(err)))\n\n    def _rename_batch(self, tar_type, tar_path=None, show_preview=True):\n\n        tar_path = self._file_path if tar_path is None else tar_path\n\n        if tar_type not in self._basic_batch_command:\n            first_space = tar_type.find("" "")\n            tar_type, addition_dir = tar_type[:first_space], tar_type[first_space + 1:]\n\n        try:\n            file_lst = os.listdir(tar_path)\n        except FileNotFoundError as err:\n            self._err_lst.append(Util.get_formatted_error(err))\n            return\n\n        counter = 0\n        pipeline = []\n    \n        for i, file in enumerate(file_lst):\n    \n            old_dir = os.path.join(tar_path, file)\n\n            if tar_type == ""files"" and os.path.isdir(old_dir):\n                continue\n            if tar_type == ""folders"" and os.path.isfile(old_dir):\n                continue\n    \n            new_name = str(counter).zfill(self._fn_length)\n            extension = os.path.splitext(file)[1]\n            new_dir = os.path.join(tar_path, new_name + extension)\n\n            pipeline.append((old_dir, new_dir))\n            counter += 1\n\n        if not pipeline:\n            return\n\n        flag = True\n\n        if self._prev_detail and show_preview:\n            self._preview_batch_detail(pipeline)\n            _proceed = self._get_cmd(""(Rename Batch) Sure to proceed ? (y/n) (Default: y) -> "")\n            if _proceed and _proceed.lower() != ""y"":\n                flag = False\n\n        if not flag:\n            return\n\n        for old_dir, new_dir in pipeline:\n            try:\n                os.rename(old_dir, new_dir)\n                self._log_lst.append(""{} -> {}"".format(\n                    Util.get_short_path(old_dir, self._path_length),\n                    Util.get_short_path(new_dir, self._path_length)\n                ))\n            except FileExistsError as err:\n                self._err_lst.append(Util.get_formatted_error(err))\n            \n    def _do(self, dt, *args):\n        if dt == ""batch"":\n            self._batch()\n            self._status = ""root""\n        elif dt == ""finish_batch"":\n            self._finish_batch(args)\n        elif dt == ""handle_batch_result"":\n            self._handle_batch_result()\n        elif dt == ""seq"":\n            self._seq()\n            self._status = ""root""\n        elif dt == ""rename"":\n            self._rename(args)\n        else:\n            raise NotImplementedError\n\n    def _help(self, args):\n\n        if not args or not args[0]:\n            if self._status == ""root"":\n                rs = ""\\n"".join(["", "".join(_cmd) for _cmd in self._root_commands]) + ""\\n""\n            elif self._status in self._special_command:\n                rs = ""\\n"".join(["", "".join(_cmd) for _cmd in self._batch_commands]) + ""\\n""\n            else:\n                raise NotImplementedError\n            print(Util.msg(""block_msg"", ""Available commands:\\n"", rs))\n\n        else:\n            dt = args[0]\n\n            if dt in self._common_command or dt in self._advance_command:\n                Util.show_help_msg(dt)\n\n            elif dt == ""folders"":\n                print(""Help (folders) -> used to rename folders in current folder.\\n""\n                      ""                  By default, they will be like \'{}\', \'{}\' and so on"".format(\n                          ""0"".zfill(self._fn_length), ""1"".zfill(self._fn_length)))\n            elif dt == ""files"":\n                print(""Help (files)   -> used to rename files in current folder.\\n""\n                      ""                  They will be like \'{}.***\', \'{}.***\' and so on.\\n""\n                      ""                  If you want to rename the files on your own, ""\n                      ""please enter \'Batch\' status"".format(\n                          ""0"".zfill(self._fn_length), ""1"".zfill(self._fn_length)))\n\n            elif dt in self._basic_command:\n                Util.show_help_msg(dt)\n\n            elif dt == ""batch"":\n                print(""Help (batch)   -> only available under \'Rename Root\' status, used to enter \'Batch\' status.\\n""\n                      ""                  You can build a \'pipeline\' under \'Batch\' status"")\n            elif dt == ""seq"":\n                print(""Help (seq)     -> only available under \'Rename Root\' status, ""\n                      ""used to enter \'Sequence\' (\'Seq\') status.\\n""\n                      ""                  You can use it to do specific sequential work. Useful only if:\\n""\n                      ""                  1) you want to rename files or folders in folders naming \'0000\', \'0001\', ...\\n""\n                      ""                  2) you want to rename them to ""\n                      ""\'{}.***\', \'{}.***\', ... or ""\n                      ""\'{}\', \'{}\', ..."".format(\n                          ""0"".zfill(self._fn_length), ""1"".zfill(self._fn_length),\n                          ""0"".zfill(self._fn_length), ""1"".zfill(self._fn_length)\n                      ))\n\n            else:\n                print(""Help (error)   -> \'{}\' is not a valid command"".format(dt))\n\n    def _config(self, args):\n\n        if not args or not args[0]:\n            try:\n                self._fn_length = int(Util.get_cmd(""(Rename Config) (file_name_length)  -> ""))\n                self._path_length = int(Util.get_cmd(""(Rename Config) (path_shown_length) -> ""))\n            except ValueError as err:\n                print(Util.msg(""block_msg"", ""Value Error\\n"", Util.get_formatted_error(err)))\n                self._fn_length, self._path_length = self._default_fl, self._default_pl\n            self._fn_max = 10 ** self._fn_length - 1\n\n        else:\n            dt = args[0]\n\n            if dt == ""fl"":\n                try:\n                    self._fn_length = int(Util.get_cmd(""(Rename Config) (file_name_length)  -> ""))\n                except ValueError as err:\n                    print(Util.msg(""block_msg"", ""Value Error\\n"", Util.get_formatted_error(err)))\n                    self._fn_length = self._default_fl\n                self._fn_max = 10 ** self._fn_length - 1\n\n            elif dt == ""pl"":\n                try:\n                    self._path_length = int(Util.get_cmd(""(Rename Config) (path_shown_length) -> ""))\n                except ValueError as err:\n                    print(Util.msg(""block_msg"", ""Value Error\\n"", Util.get_formatted_error(err)))\n                    self._path_length = self._default_pl\n\n    def _batch(self):\n\n        self._status = ""batch""\n\n        while True:\n\n            new_cmd = self._get_cmd(""(Rename Batch) {} -> "", True)\n\n            if self._status == ""exit"":\n                break\n\n            if new_cmd == ""end"":\n                if self._batch_lst:\n                    self._preview_batch()\n                    self._do(""finish_batch"")\n                break\n\n            elif new_cmd in self._basic_batch_command:\n                addition_dir = self._get_cmd(""(Rename Batch) (Rename {}) dir -> "".format(new_cmd))\n                tmp_path = CmdTool.get_path_from_cmd(addition_dir, """", ""folder"")\n                if not os.path.isdir(tmp_path):\n                    print(Util.msg(""valid_path_error"", tmp_path, ""folder""))\n                    continue\n                self._batch_lst.append((new_cmd, tmp_path))\n\n            elif new_cmd in self._basic_command:\n                while True:\n                    self._do(""rename"", new_cmd, ""(Rename Batch)"")\n                    _continue = self._get_cmd(""(Rename Batch) Continue ? (y/n) (default: y) -> "")\n                    if _continue and _continue.lower() != ""y"":\n                        break\n\n            else:\n                print(Util.msg(""undefined_error"", new_cmd))\n\n    def _preview_batch_detail(self, pipeline):\n        rs = ""\\n"".join([\n            Util.get_short_path(_old, self._path_length) + "" -> "" + Util.get_short_path(_new, self._path_length)\n            for _old, _new in pipeline\n        ]) + ""\\n""\n        print(Util.msg(""block_msg"", ""Batch Detail\\n"", rs))\n\n    def _preview_batch(self):\n        rs = ""\\n"".join([\n            Util.get_short_path(_p, self._path_length) + "" -> {:>8s}"".format(_c)\n            for _c, _p in self._batch_lst\n        ]) + ""\\n""\n        print(Util.msg(""block_msg"", ""Batch Preview\\n"", rs))\n\n    def _finish_batch(self, args):\n        args = True if not args else args[0]\n        if len(self._batch_lst) != 0:\n            for _c, _p in self._batch_lst:\n                self._rename_batch(_c, _p, args)\n        self._do(""handle_batch_result"")\n        self._batch_lst = []\n\n    def _handle_batch_result(self):\n        if self._log_lst:\n            print(Util.msg(""block_msg"", ""Batch Results\\n"", ""\\n"".join(self._log_lst) + ""\\n""))\n            self._log_lst = []\n        else:\n            print(Util.msg(""block_msg"", ""Batch Results\\n"", ""None\\n""))\n        if self._err_lst:\n            print(Util.msg(""block_msg"", ""Batch Errors\\n"", ""\\n"".join(self._err_lst) + ""\\n""))\n            self._err_lst = []\n\n    def _seq(self):\n\n        self._status = ""seq""\n\n        while True:\n\n            seq_cmd = self._get_cmd(""(Rename Seq) {} -> "", True)\n\n            if self._status == ""exit"":\n                break\n\n            if seq_cmd in self._basic_command:\n                seq_cmd += ""s""\n            if seq_cmd not in self._basic_batch_command:\n                print(Util.msg(""undefined_error"", seq_cmd))\n                continue\n\n            try:\n                seq_start = int(self._get_cmd(""(Rename Seq)   start point -> ""))\n                seq_end = int(self._get_cmd(""(Rename Seq)   end point   -> ""))\n            except ValueError as err:\n                print(Util.msg(""block_msg"", ""Value Error\\n"", Util.get_formatted_error(err)))\n                continue\n\n            if seq_end < seq_start:\n                print(Util.msg(""block_msg"", ""Error\\n"", ""start point \'{}\' exceeded end point \'{}\'\\n"".format(\n                    seq_start, seq_end)))\n                continue\n\n            if seq_end >= self._fn_max:\n                print(Util.msg(""block_msg"", ""Error\\n"", ""end point \'{}\' exceeded ceiling \'{}\'\\n"".format(\n                    seq_end, self._fn_max)))\n                continue\n\n            name_lst = [\n                Util.get_short_path(CmdTool.get_path(str(i).zfill(self._fn_length)), self._path_length)\n                for i in range(seq_start, seq_end + 1)\n            ]\n            self._batch_lst = [\n                (seq_cmd, CmdTool.get_path(str(i).zfill(self._fn_length)))\n                for i in range(seq_start, seq_end + 1)\n            ]\n            print(Util.msg(""block_msg"", ""Sequential Task ({})\\n"".format(seq_cmd), ""\\n"".join(name_lst) + ""\\n""))\n\n            _proceed = self._get_cmd(""(Rename Seq) Sure to proceed ? (y/n) (Default: y) -> "")\n            if _proceed and _proceed.lower() == ""y"":\n                self._do(""finish_batch"", False)\n\n    def _cmd_tool(self):\n\n        while True:\n\n            new_cmd = self._get_cmd(""(Rename Root) {} -> "", True)\n\n            if self._do_common_work(new_cmd):\n                continue\n\n            if self._status == ""exit"":\n                self._renew_path(self._parent)\n                break\n\n            if new_cmd in self._special_command:\n                self._do(new_cmd)\n\n            elif new_cmd:\n                cmd_lst = new_cmd.split()\n\n                if cmd_lst:\n\n                    if cmd_lst[0] in self._basic_batch_command:\n                        self._rename_batch(new_cmd)\n                        self._do(""handle_batch_result"")\n\n                    elif cmd_lst[0] in self._basic_command:\n                        self._do(""rename"", new_cmd, ""(Rename Root)"")\n\n                else:\n                    Util.do_system_default(new_cmd)\n\n    def run(self, dt=""cmd""):\n        if dt == ""cmd"":\n            self._cmd_tool()\n        else:\n            raise NotImplementedError\n\n\nclass PyCmd(CmdTool):\n\n    def __init__(self):\n\n        CmdTool.__init__(self)\n        print(""-- Welcome to pycmd, a light & extensible command line tool --"")\n        print(""-- Your platform: {} --"".format(CmdTool._platform))\n\n        self._special_command = (""rename"", )\n\n        self._commands = [\n            self._common_command, self._advance_command,\n            self._special_command\n        ]\n\n        self._config = {\n            ""rename"": (16, 4, True)\n        }\n\n    def _help(self, args):\n        if not args or not args[0]:\n            rs = ""\\n"".join(["", "".join(_cmd) for _cmd in self._commands]) + ""\\n""\n            print(Util.msg(""block_msg"", ""Available commands:\\n"", rs))\n\n        else:\n            dt = args[0]\n\n            if dt in self._common_command or dt in self._advance_command or dt in self._special_command:\n                Util.show_help_msg(dt)\n\n            else:\n                print(""Help (error)   -> \'{}\' is not a valid command"".format(dt))\n\n    def _config(self, args):\n\n        if not args or not args[0]:\n            self._config_rename()\n\n        else:\n            dt = args[0]\n\n            if dt == ""rename"":\n                self._config_rename()\n\n    def _config_rename(self):\n        try:\n            tmp_fl = int(Util.get_cmd(""Config \'Rename\' (file_name_length)  -> ""))\n            tmp_pl = int(Util.get_cmd(""Config \'Rename\' (path_shown_length) -> ""))\n            tmp_pd = bool(Util.get_cmd(""Config \'Rename\' (preview_batch_detail) -> ""))\n            self._config[""rename""] = (tmp_fl, tmp_pl, tmp_pd)\n        except ValueError as err:\n            print(Util.msg(""block_msg"", ""Value Error\\n"", Util.get_formatted_error(err)))\n\n    def _do_special(self, cmd):\n\n        self._status = cmd\n\n        if cmd == ""rename"":\n            pl, fl, pd = self._config[""rename""]\n            rename = Rename(self._file_path, pl, fl, pd, self)\n            rename.run()\n\n    def run(self):\n\n        while True:\n\n            new_cmd = self._get_cmd(""(Root) {} -> "", True)\n\n            if self._do_common_work(new_cmd):\n                continue\n\n            if self._status == ""exit"":\n                break\n\n            if new_cmd in self._special_command:\n                self._do_special(new_cmd)\n\n            elif new_cmd:\n                Util.do_system_default(new_cmd)\n\n\nif __name__ == ""__main__"":\n    tool = PyCmd()\n    tool.run()\n'"
_Dist/NeuralNetworks/_Tests/TestUtil.py,0,"b'import os\nimport sys\nroot_path = os.path.abspath(""../../../../"")\nif root_path not in sys.path:\n    sys.path.append(root_path)\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n\ndef draw_acc(*models, ylim=(0.6, 1.05), draw_train=True):\n    plt.figure()\n    for model in models:\n        name = str(model)\n        metric = ""acc"" if ""train_acc"" in model.log else ""binary_acc""\n        el, tl = model.log[""train_{}"".format(metric)], model.log[""test_{}"".format(metric)]\n        ee_base = np.arange(len(el))\n        cse_base = np.linspace(0, len(el) - 1, len(tl))\n        if draw_train:\n            plt.plot(ee_base, el, label=""Train acc ({})"".format(name))\n        plt.plot(cse_base, tl, label=""Test acc ({})"".format(name))\n    plt.ylim(*ylim)\n    plt.legend(prop={\'size\': 14})\n    plt.show()\n'"
_Dist/NeuralNetworks/b_TraditionalML/MultinomialNB.py,0,"b'import numpy as np\nfrom sklearn.preprocessing import OneHotEncoder\n\n\nclass MultinomialNB:\n    """""" Naive Bayes algorithm with discrete inputs\n\n    Parameters\n    ----------\n    alpha : float, optional (default=1.)\n        Smooth parameter used in Naive Bayes, default is 1 (which indicates a laplace smoothing)\n\n    Attributes\n    ----------\n    enc : OneHotEncoder\n        One-Hot encoder used to transform (discrete) inputs\n\n    class_log_prior : np.ndarray of float\n        Log class prior used to calculate (linear) prediction\n\n    feature_log_prob : np.ndarray of float\n        Feature log probability used to calculate (linear) prediction\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> x = np.random.randint(0, 10, [1000, 10])  #  Generate feature vectors\n    >>> y = np.random.randint(0, 5, 1000)         #  Generate labels\n    >>> nb = MultinomialNB().fit(x, y)            #  fit the model\n    >>> nb.predict(x)                             #  (linear) prediction\n    >>> nb.predict_class(x)                       #  predict labels\n\n    """"""\n    def __init__(self, alpha=1.):\n        self.alpha = alpha\n        self.enc = self.class_log_prior = self.feature_log_prob = None\n\n    def fit(self, x, y, do_one_hot=True):\n        """""" Fit the model with x & y\n\n        Parameters\n        ----------\n        x : {list of float, np.ndarray of float}\n            Feature vectors used for training\n\n            Note: features are assumed to be discrete\n\n        y : {list of float, np.ndarray of float}\n            Labels used for training\n\n        do_one_hot : bool, optional (default=True)\n            Whether do one-hot encoding on x\n\n        Returns\n        -------\n        self : MultinomialNB\n            Returns self.\n\n        """"""\n        if do_one_hot:\n            self.enc = OneHotEncoder(dtype=np.float32)\n            x = self.enc.fit_transform(x)\n        else:\n            self.enc = None\n            x = np.array(x, np.float32)\n        n = x.shape[0]\n        y = np.array(y, np.int8)\n        self.class_log_prior = np.log(np.bincount(y) / n)\n        masks = [y == i for i in range(len(self.class_log_prior))]\n        masked_xs = [x[mask] for mask in masks]\n        feature_counts = np.array([np.asarray(masked_x.sum(0))[0] for masked_x in masked_xs])\n        smoothed_fc = feature_counts + self.alpha\n        self.feature_log_prob = np.log(smoothed_fc / smoothed_fc.sum(1, keepdims=True))\n        return self\n\n    def _predict(self, x):\n        """""" Internal method for calculating (linear) predictions\n\n        Parameters\n        ----------\n        x : {np.ndarray of float, scipy.sparse.csr.csr_matrix of float}\n            One-Hot encoded feature vectors\n\n        Returns\n        -------\n        predictions : np.ndarray of float\n            Returns (linear) predictions.\n\n        """"""\n        return x.dot(self.feature_log_prob.T) + self.class_log_prior\n\n    def predict(self, x):\n        """""" API for calculating (linear) predictions\n\n        Parameters\n        ----------\n        x : {list of float, np.ndarray of float}\n            Target feature vectors\n\n        Returns\n        -------\n        predictions : np.ndarray of float\n            Returns (linear) predictions.\n\n        """"""\n        if self.enc is not None:\n            x = self.enc.transform(x)\n        return self._predict(x)\n\n    def predict_class(self, x):\n        """""" API for predicting labels\n\n        Parameters\n        ----------\n        x : {list of float, np.ndarray of float}\n            Target feature vectors\n\n        Returns\n        -------\n        labels : np.ndarray of int\n            Returns labels.\n\n        """"""\n        return np.argmax(self.predict(x), 1)\n'"
_Dist/NeuralNetworks/b_TraditionalML/SVM.py,7,"b'import os\nimport sys\nroot_path = os.path.abspath(""../../../"")\nif root_path not in sys.path:\n    sys.path.append(root_path)\n\nimport numpy as np\nimport tensorflow as tf\n\nfrom _Dist.NeuralNetworks.DistBase import Base,  AutoBase, AutoMeta, DistMixin, DistMeta\n\n\nclass LinearSVM(Base):\n    def __init__(self, *args, **kwargs):\n        super(LinearSVM, self).__init__(*args, **kwargs)\n        self._name_appendix = ""LinearSVM""\n        self.c = None\n\n    def init_from_data(self, x, y, x_test, y_test, sample_weights, names):\n        super(LinearSVM, self).init_from_data(x, y, x_test, y_test, sample_weights, names)\n        metric = self.model_param_settings.setdefault(""metric"", ""binary_acc"")\n        if metric == ""acc"":\n            self.model_param_settings[""metric""] = ""binary_acc""\n        self.n_class = 1\n\n    def init_model_param_settings(self):\n        self.model_param_settings.setdefault(""lr"", 0.01)\n        self.model_param_settings.setdefault(""n_epoch"", 10 ** 3)\n        self.model_param_settings.setdefault(""max_epoch"", 10 ** 6)\n        super(LinearSVM, self).init_model_param_settings()\n        self.c = self.model_param_settings.get(""C"", 1.)\n\n    def _build_model(self, net=None):\n        self._model_built = True\n        if net is None:\n            net = self._tfx\n        current_dimension = net.shape[1].value\n        self._output = self._fully_connected_linear(\n            net, [current_dimension, 1], ""_final_projection""\n        )\n\n    def _define_loss_and_train_step(self):\n        self._loss = self.c * tf.reduce_sum(\n            tf.maximum(0., 1 - self._tfy * self._output)\n        ) + tf.nn.l2_loss(self._ws[0])\n        with tf.control_dependencies(tf.get_collection(tf.GraphKeys.UPDATE_OPS)):\n            self._train_step = self._optimizer.minimize(self._loss)\n\n    def _get_feed_dict(self, x, y=None, weights=None, is_training=False):\n        if y is not None:\n            y[y == 0] = -1\n        return super(LinearSVM, self)._get_feed_dict(x, y, weights, is_training)\n\n    def predict_classes(self, x):\n        return (self._calculate(x, tensor=self._output, is_training=False) >= 0).astype(np.int32)\n\n\nclass SVM(LinearSVM):\n    def __init__(self, *args, **kwargs):\n        super(SVM, self).__init__(*args, **kwargs)\n        self._name_appendix = ""SVM""\n        self._p = self._gamma = None\n        self._x = self._gram = self._kernel_name = None\n\n    @property\n    def kernel(self):\n        if self._kernel_name == ""linear"":\n            return self.linear\n        if self._kernel_name == ""poly"":\n            return lambda x, y: self.poly(x, y, self._p)\n        if self._kernel_name == ""rbf"":\n            return lambda x, y: self.rbf(x, y, self._gamma)\n        raise NotImplementedError(""Kernel \'{}\' is not implemented"".format(self._kernel_name))\n\n    @staticmethod\n    def linear(x, y):\n        return x.dot(y.T)\n\n    @staticmethod\n    def poly(x, y, p):\n        return (x.dot(y.T) + 1) ** p\n\n    @staticmethod\n    def rbf(x, y, gamma):\n        return np.exp(-gamma * np.sum((x[..., None, :] - y) ** 2, axis=2))\n\n    def init_from_data(self, x, y, x_test, y_test, sample_weights, names):\n        self._x, y = np.atleast_2d(x).astype(np.float32), np.asarray(y, np.float32)\n        self._p = self.model_param_settings.setdefault(""p"", 3)\n        self._gamma = self.model_param_settings.setdefault(""gamma"", 1 / self._x.shape[1])\n        self._kernel_name = self.model_param_settings.setdefault(""kernel_name"", ""rbf"")\n        self._gram, x_test = self.kernel(self._x, self._x), self.kernel(x_test, self._x)\n        super(SVM, self).init_from_data(self._gram, y, x_test, y_test, sample_weights, names)\n\n    def init_model_param_settings(self):\n        super(SVM, self).init_model_param_settings()\n        self._p = self.model_param_settings[""p""]\n        self._gamma = self.model_param_settings[""gamma""]\n        self._kernel_name = self.model_param_settings[""kernel_name""]\n\n    def _define_py_collections(self):\n        super(SVM, self)._define_py_collections()\n        self.py_collections += [""_x"", ""_gram""]\n\n    def _define_loss_and_train_step(self):\n        self._loss = self.c * tf.reduce_sum(tf.maximum(0., 1 - self._tfy * self._output)) + 0.5 * tf.matmul(\n            self._ws[0], tf.matmul(self._gram, self._ws[0]), transpose_a=True\n        )[0]\n        with tf.control_dependencies(tf.get_collection(tf.GraphKeys.UPDATE_OPS)):\n            self._train_step = self._optimizer.minimize(self._loss)\n\n    def _evaluate(self, x=None, y=None, x_cv=None, y_cv=None, x_test=None, y_test=None, metric=None):\n        n_sample = self._x.shape[0]\n        cv_feat_dim = None if x_cv is None else x_cv.shape[1]\n        test_feat_dim = None if x_test is None else x_test.shape[1]\n        x_cv = None if x_cv is None else self.kernel(x_cv, self._x) if cv_feat_dim != n_sample else x_cv\n        x_test = None if x_test is None else self.kernel(x_test, self._x) if test_feat_dim != n_sample else x_test\n        return super(SVM, self)._evaluate(x, y, x_cv, y_cv, x_test, y_test)\n\n    def predict(self, x):\n        # noinspection PyTypeChecker\n        return self._predict(self.kernel(x, self._x))\n\n    def predict_classes(self, x):\n        return (self.predict(x) >= 0).astype(np.int32)\n\n    def evaluate(self, x, y, x_cv=None, y_cv=None, x_test=None, y_test=None, metric=None):\n        return self._evaluate(self.kernel(x, self._x), y, x_cv, y_cv, x_test, y_test, metric)\n\n\nclass AutoLinearSVM(AutoBase, LinearSVM, metaclass=AutoMeta):\n    pass\n\n\nclass DistLinearSVM(AutoLinearSVM, DistMixin, metaclass=DistMeta):\n    pass\n'"
_Dist/NeuralNetworks/c_BasicNN/DistNN.py,0,"b'import os\nimport sys\nroot_path = os.path.abspath(""../../../"")\nif root_path not in sys.path:\n    sys.path.append(root_path)\n\nfrom _Dist.NeuralNetworks.NNUtil import *\nfrom _Dist.NeuralNetworks.DistBase import Base\n\n\nclass Basic(Base):\n    signature = ""Basic""\n\n    def __init__(self, *args, **kwargs):\n        super(Basic, self).__init__(*args, **kwargs)\n        self._name_appendix = ""BasicNN""\n        self.activations = self.hidden_units = None\n\n    @property\n    def name(self):\n        return ""NN"" if self._name is None else self._name\n\n    def init_model_param_settings(self):\n        super(Basic, self).init_model_param_settings()\n        self.activations = self.model_param_settings.get(""activations"", ""relu"")\n\n    def init_model_structure_settings(self):\n        super(Basic, self).init_model_structure_settings()\n        self.hidden_units = self.model_structure_settings.get(""hidden_units"", [256, 256])\n\n    def _build_layer(self, i, net):\n        activation = self.activations[i]\n        if activation is not None:\n            net = getattr(Activations, activation)(net, ""{}{}"".format(activation, i))\n        return net\n\n    def _build_model(self, net=None):\n        self._model_built = True\n        if net is None:\n            net = self._tfx\n        current_dimension = net.shape[1].value\n        if self.activations is None:\n            self.activations = [None] * len(self.hidden_units)\n        elif isinstance(self.activations, str):\n            self.activations = [self.activations] * len(self.hidden_units)\n        else:\n            self.activations = self.activations\n        for i, n_unit in enumerate(self.hidden_units):\n            net = self._fully_connected_linear(net, [current_dimension, n_unit], i)\n            net = self._build_layer(i, net)\n            current_dimension = n_unit\n        appendix = ""_final_projection""\n        fc_shape = self.hidden_units[-1] if self.hidden_units else current_dimension\n        self._output = self._fully_connected_linear(net, [fc_shape, self.n_class], appendix)\n'"
_Dist/NeuralNetworks/c_BasicNN/NN.py,0,"b'import os\nimport sys\nroot_path = os.path.abspath(""../../../"")\nif root_path not in sys.path:\n    sys.path.append(root_path)\n\nfrom _Dist.NeuralNetworks.NNUtil import *\nfrom _Dist.NeuralNetworks.Base import Base\n\n\nclass Basic(Base):\n    signature = ""Basic""\n\n    def __init__(self, *args, **kwargs):\n        super(Basic, self).__init__(*args, **kwargs)\n        self._name_appendix = ""BasicNN""\n        self.activations = self.hidden_units = None\n\n    @property\n    def name(self):\n        return ""NN"" if self._name is None else self._name\n\n    def init_model_param_settings(self):\n        super(Basic, self).init_model_param_settings()\n        self.activations = self.model_param_settings.get(""activations"", ""relu"")\n\n    def init_model_structure_settings(self):\n        super(Basic, self).init_model_structure_settings()\n        self.hidden_units = self.model_structure_settings.get(""hidden_units"", [256, 256])\n\n    def _build_layer(self, i, net):\n        activation = self.activations[i]\n        if activation is not None:\n            net = getattr(Activations, activation)(net, ""{}{}"".format(activation, i))\n        return net\n\n    def _build_model(self, net=None):\n        self._model_built = True\n        if net is None:\n            net = self._tfx\n        current_dimension = net.shape[1].value\n        if self.activations is None:\n            self.activations = [None] * len(self.hidden_units)\n        elif isinstance(self.activations, str):\n            self.activations = [self.activations] * len(self.hidden_units)\n        else:\n            self.activations = self.activations\n        for i, n_unit in enumerate(self.hidden_units):\n            net = self._fully_connected_linear(net, [current_dimension, n_unit], i)\n            net = self._build_layer(i, net)\n            current_dimension = n_unit\n        appendix = ""_final_projection""\n        fc_shape = self.hidden_units[-1] if self.hidden_units else current_dimension\n        self._output = self._fully_connected_linear(net, [fc_shape, self.n_class], appendix)\n\n\nif __name__ == \'__main__\':\n    from Util.Util import DataUtil\n\n    for generator in (DataUtil.gen_xor, DataUtil.gen_spiral, DataUtil.gen_nine_grid):\n        x_train, y_train = generator(size=1000, one_hot=False)\n        x_test, y_test = generator(size=100, one_hot=False)\n        nn = Basic(model_param_settings={""n_epoch"": 200}).scatter2d(x_train, y_train).fit(\n            x_train, y_train, x_test, y_test, snapshot_ratio=0\n        ).draw_losses().visualize2d(\n            x_train, y_train, title=""Train""\n        ).visualize2d(\n            x_test, y_test, padding=2, title=""Test""\n        )\n\n    for size in (256, 1000, 10000):\n        (x_train, y_train), (x_test, y_test) = DataUtil.gen_noisy_linear(\n            size=size, n_dim=2, n_valid=2, test_ratio=100 / size, one_hot=False\n        )\n        nn = Basic(model_param_settings={""n_epoch"": 200}).scatter2d(x_train, y_train).fit(\n            x_train, y_train, x_test, y_test, snapshot_ratio=0\n        ).draw_losses().visualize2d(x_train, y_train, title=""Train"").visualize2d(x_test, y_test, title=""Test"")\n'"
_Dist/NeuralNetworks/d_Traditional2NN/Toolbox.py,0,"b'import os\nimport sys\nroot_path = os.path.abspath(""../../../"")\nif root_path not in sys.path:\n    sys.path.append(root_path)\n\nimport numpy as np\n\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.tree import _tree, DecisionTreeClassifier\n\nfrom _Dist.NeuralNetworks.c_BasicNN.NN import Basic\n\n\n# Transformation base\nclass TransformationBase(Basic):\n    def __init__(self, *args, **kwargs):\n        super(TransformationBase, self).__init__(*args, **kwargs)\n        self._transform_ws = self._transform_bs = None\n\n    def _get_all_data(self, shuffle=True):\n        train, train_weights = self._train_generator.get_all_data()\n        if shuffle:\n            np.random.shuffle(train)\n        x, y = train[..., :-1], train[..., -1]\n        if self._test_generator is not None:\n            test, test_weights = self._test_generator.get_all_data()\n            if shuffle:\n                np.random.shuffle(test)\n            x_test, y_test = test[..., :-1], test[..., -1]\n        else:\n            x_test = y_test = None\n        return x, y, x_test, y_test\n\n    def _transform(self):\n        pass\n\n    def _print_model_performance(self, clf, name, x, y, x_test, y_test):\n        print(""\\n"".join([""="" * 60, ""{} performance"".format(name), ""-"" * 60]))\n        y_train_pred = clf.predict(x)\n        y_test_pred = clf.predict(x_test)\n        train_metric = self.metric(y, y_train_pred)\n        test_metric = self.metric(y_test, y_test_pred)\n        print(""{}  -  Train : {:8.6}   CV : {:8.6}"".format(\n            self._metric_name, train_metric, test_metric\n        ))\n        print(""-"" * 60)\n\n    def _build_model(self, net=None):\n        self._transform()\n        super(TransformationBase, self)._build_model(net)\n\n    def _initialize_variables(self):\n        super(TransformationBase, self)._initialize_variables()\n        self.feed_weights()\n        self.feed_biases()\n        x, y, x_test, y_test = self._get_all_data()\n        print(""\\n"".join([""="" * 60, ""Initial performance"", ""-"" * 60]))\n        self._evaluate(x, y, x_test, y_test)\n        print(""-"" * 60)\n\n    def feed_weights(self):\n        for i, w in enumerate(self._transform_ws):\n            if w is not None:\n                self._sess.run(self._ws[i].assign(w))\n\n    def feed_biases(self):\n        for i, b in enumerate(self._transform_bs):\n            if b is not None:\n                self._sess.run(self._bs[i].assign(b))\n\n\n# NaiveBayes -> NN\nclass NB2NN(TransformationBase):\n    def __init__(self, *args, **kwargs):\n        super(NB2NN, self).__init__(*args, **kwargs)\n        self._name_appendix = ""NaiveBayes""\n        self.model_param_settings.setdefault(""activations"", None)\n\n    def _transform(self):\n        self.hidden_units = []\n        x, y, x_test, y_test = self._get_all_data()\n        nb = MultinomialNB()\n        nb.fit(x, y)\n        self._print_model_performance(nb, ""Naive Bayes"", x, y, x_test, y_test)\n        self._transform_ws = [nb.feature_log_prob_.T]\n        self._transform_bs = [nb.class_log_prior_]\n\n\n# DTree -> NN\ndef export_structure(tree):\n    tree = tree.tree_\n\n    def recurse(node, depth):\n        feature_dim = tree.feature[node]\n        if feature_dim == _tree.TREE_UNDEFINED:\n            yield depth, -1, tree.value[node]\n        else:\n            threshold = tree.threshold[node]\n            yield depth, feature_dim, threshold\n            yield from recurse(tree.children_left[node], depth + 1)\n            yield depth, feature_dim, threshold\n            yield from recurse(tree.children_right[node], depth + 1)\n\n    return list(recurse(0, 0))\n\n\nclass DT2NN(TransformationBase):\n    def __init__(self, *args, **kwargs):\n        super(DT2NN, self).__init__(*args, **kwargs)\n        self._name_appendix = ""DTree""\n        self.model_param_settings.setdefault(""activations"", [""sign"", ""one_hot""])\n\n    def _transform(self):\n        x, y, x_test, y_test = self._get_all_data()\n        tree = DecisionTreeClassifier()\n        tree.fit(x, y)\n        self._print_model_performance(tree, ""Decision Tree"", x, y, x_test, y_test)\n\n        tree_structure = export_structure(tree)\n        n_leafs = sum([1 if pair[1] == -1 else 0 for pair in tree_structure])\n        n_internals = n_leafs - 1\n\n        print(""Internals : {} ; Leafs : {}"".format(n_internals, n_leafs))\n\n        b = np.zeros(n_internals, dtype=np.float32)\n        w1 = np.zeros([x.shape[1], n_internals], dtype=np.float32)\n        w2 = np.zeros([n_internals, n_leafs], dtype=np.float32)\n        w3 = np.zeros([n_leafs, self.n_class], dtype=np.float32)\n        node_list = []\n        node_sign_list = []\n        node_id_cursor = leaf_id_cursor = 0\n        max_route_length = 0\n        self.hidden_units = [n_internals, n_leafs]\n\n        for depth, feat_dim, rs in tree_structure:\n            if feat_dim != -1:\n                if depth == len(node_list):\n                    node_sign_list.append(-1)\n                    node_list.append([node_id_cursor, feat_dim, rs])\n                    w1[feat_dim, node_id_cursor] = 1\n                    b[node_id_cursor] = -rs\n                    node_id_cursor += 1\n                else:\n                    node_list = node_list[:depth + 1]\n                    node_sign_list = node_sign_list[:depth] + [1]\n            else:\n                valid_nodes = set()\n                local_sign_list = node_sign_list[:]\n                for i, ((node_id, node_dim, node_threshold), node_sign) in enumerate(\n                    zip(node_list, node_sign_list)\n                ):\n                    valid_nodes.add((node_id, node_sign))\n                    if i >= 1:\n                        for j, ((local_id, local_dim, local_threshold), local_sign) in enumerate(zip(\n                            node_list[:i], local_sign_list[:i]\n                        )):\n                            if node_sign == local_sign and node_dim == local_dim:\n                                if (\n                                    (node_sign == -1 and node_threshold < local_threshold) or\n                                    (node_sign == 1 and node_threshold > local_threshold)\n                                ):\n                                    local_sign_list[j] = 0\n                                    valid_nodes.remove((local_id, local_sign))\n                                    break\n                for node_id, node_sign in valid_nodes:\n                    w2[node_id, leaf_id_cursor] = node_sign / len(valid_nodes)\n                max_route_length = max(max_route_length, len(valid_nodes))\n                w3[leaf_id_cursor] = rs / np.sum(rs)\n                leaf_id_cursor += 1\n\n        w2 *= max_route_length\n        self._transform_ws = [w1, w2, w3]\n        self._transform_bs = [b]\n'"
_Dist/NeuralNetworks/e_AdvancedNN/DistNN.py,23,"b'import os\nimport sys\nroot_path = os.path.abspath(""../../../"")\nif root_path not in sys.path:\n    sys.path.append(root_path)\n\nimport math\nimport numpy as np\nimport tensorflow as tf\n\nfrom _Dist.NeuralNetworks.NNUtil import *\nfrom _Dist.NeuralNetworks.c_BasicNN.DistNN import Basic\n\n\nclass Advanced(Basic):\n    signature = ""Advanced""\n\n    def __init__(self, name=None, data_info=None, model_param_settings=None, model_structure_settings=None):\n        self.tf_list_collections = None\n        super(Advanced, self).__init__(name, model_param_settings, model_structure_settings)\n        self._name_appendix = ""Advanced""\n\n        if data_info is None:\n            self.data_info = {}\n        else:\n            assert_msg = ""data_info should be a dictionary""\n            assert isinstance(data_info, dict), assert_msg\n            self.data_info = data_info\n        self._data_info_initialized = False\n        self.numerical_idx = self.categorical_columns = None\n\n        self._deep_input = self._wide_input = None\n        self._categorical_xs = None\n        self.embedding_size = None\n        self._embedding = self._one_hot = self._embedding_concat = self._one_hot_concat = None\n        self._embedding_with_one_hot = self._embedding_with_one_hot_concat = None\n\n        self.dropout_keep_prob = self.use_batch_norm = None\n        self._use_wide_network = self._dndf = self._pruner = self._dndf_pruner = None\n\n        self._tf_p_keep = None\n        self._n_batch_placeholder = None\n\n    @property\n    def valid_numerical_idx(self):\n        return np.array([\n            is_numerical for is_numerical in self.numerical_idx\n            if is_numerical is not None\n        ])\n\n    def init_data_info(self):\n        if self._data_info_initialized:\n            return\n        self._data_info_initialized = True\n        self.numerical_idx = self.data_info.get(""numerical_idx"", None)\n        self.categorical_columns = self.data_info.get(""categorical_columns"", None)\n        if self.numerical_idx is None:\n            raise ValueError(""numerical_idx should be provided"")\n        if self.categorical_columns is None:\n            raise ValueError(""categorical_columns should be provided"")\n\n    def init_from_data(self, x, y, x_test, y_test, sample_weights, names):\n        self.init_data_info()\n        super(Advanced, self).init_from_data(x, y, x_test, y_test, sample_weights, names)\n        if len(self.valid_numerical_idx) != self.n_dim + 1:\n            raise ValueError(""Length of valid_numerical_idx should be {}, {} found"".format(\n                self.n_dim + 1, len(self.valid_numerical_idx)\n            ))\n        self.n_dim -= len(self.categorical_columns)\n        self.model_structure_settings.setdefault(""use_wide_network"", self.n_dim > 0)\n\n    def init_model_param_settings(self):\n        super(Advanced, self).init_model_param_settings()\n        self.dropout_keep_prob = float(self.model_param_settings.get(""keep_prob"", 0.5))\n        self.use_batch_norm = self.model_param_settings.get(""use_batch_norm"", False)\n\n    def init_model_structure_settings(self):\n        self.hidden_units = self.model_structure_settings.get(""hidden_units"", None)\n        self._deep_input = self.model_structure_settings.get(""deep_input"", ""embedding_concat"")\n        self._wide_input = self.model_structure_settings.get(""wide_input"", ""continuous"")\n        self.embedding_size = self.model_structure_settings.get(""embedding_size"", 8)\n\n        self._use_wide_network = self.model_structure_settings[""use_wide_network""]\n        if not self._use_wide_network:\n            self._dndf = None\n        else:\n            dndf_params = self.model_structure_settings.get(""dndf_params"", {})\n            if self.model_structure_settings.get(""use_dndf"", True):\n                self._dndf = DNDF(self.n_class, **dndf_params)\n        if self.model_structure_settings.get(""use_pruner"", True):\n            pruner_params = self.model_structure_settings.get(""pruner_params"", {})\n            self._pruner = Pruner(**pruner_params)\n        if self.model_structure_settings.get(""use_dndf_pruner"", False):\n            dndf_pruner_params = self.model_structure_settings.get(""dndf_pruner_params"", {})\n            self._dndf_pruner = Pruner(**dndf_pruner_params)\n\n    def _get_embedding(self, i, n):\n        embedding_size = math.ceil(math.log2(n)) + 1 if self.embedding_size == ""log"" else self.embedding_size\n        embedding = tf.Variable(tf.truncated_normal(\n            [n, embedding_size], mean=0, stddev=0.02\n        ), name=""Embedding{}"".format(i))\n        return tf.nn.embedding_lookup(embedding, self._categorical_xs[i], name=""Embedded_X{}"".format(i))\n\n    def _define_hidden_units(self):\n        n_data = len(self._train_generator)\n        current_units = self._deep_input.shape[1].value\n        if current_units > 512:\n            self.hidden_units = [1024, 1024]\n        elif current_units > 256:\n            if n_data >= 10000:\n                self.hidden_units = [1024, 1024]\n            else:\n                self.hidden_units = [2 * current_units, 2 * current_units]\n        else:\n            if n_data >= 100000:\n                self.hidden_units = [768, 768]\n            elif n_data >= 10000:\n                self.hidden_units = [512, 512]\n            else:\n                self.hidden_units = [2 * current_units, 2 * current_units]\n\n    def _fully_connected_linear(self, net, shape, appendix):\n        with tf.name_scope(""Linear{}"".format(appendix)):\n            w = init_w(shape, ""W{}"".format(appendix))\n            if self._pruner is not None:\n                w = self._pruner.prune_w(*self._pruner.get_w_info(w))\n            b = init_b([shape[1]], ""b{}"".format(appendix))\n            self._ws.append(w)\n            self._bs.append(b)\n            return tf.add(tf.matmul(net, w), b, name=""Linear{}_Output"".format(appendix))\n\n    def _build_layer(self, i, net):\n        if self.use_batch_norm:\n            net = tf.layers.batch_normalization(net, training=self._is_training, name=""BN{}"".format(i))\n        activation = self.activations[i]\n        if activation is not None:\n            net = getattr(Activations, activation)(net, ""{}{}"".format(activation, i))\n        if self.dropout_keep_prob < 1:\n            net = tf.nn.dropout(net, keep_prob=self._tf_p_keep)\n        return net\n\n    def _build_model(self, net=None):\n        super(Advanced, self)._build_model(self._deep_input)\n        if self._use_wide_network:\n            if self._dndf is None:\n                wide_output = self._fully_connected_linear(\n                    self._wide_input, appendix=""_wide_output"",\n                    shape=[self._wide_input.shape[1].value, self.n_class]\n                )\n            else:\n                wide_output = self._dndf(\n                    self._wide_input, self._n_batch_placeholder,\n                    pruner=self._dndf_pruner\n                )\n            self._output += wide_output\n\n    def _get_feed_dict(self, x, y=None, weights=None, is_training=True):\n        continuous_x = x[..., self.valid_numerical_idx[:-1]] if self._categorical_xs else x\n        feed_dict = super(Advanced, self)._get_feed_dict(continuous_x, y, weights, is_training)\n        if self._dndf is not None:\n            feed_dict[self._n_batch_placeholder] = len(x)\n        if self._pruner is not None:\n            cond_placeholder = self._pruner.cond_placeholder\n            if cond_placeholder is not None:\n                feed_dict[cond_placeholder] = True\n        if self._dndf is not None and self._dndf_pruner is not None:\n            cond_placeholder = self._dndf_pruner.cond_placeholder\n            if cond_placeholder is not None:\n                feed_dict[cond_placeholder] = True\n        for (idx, _), categorical_x in zip(self.categorical_columns, self._categorical_xs):\n            feed_dict.update({categorical_x: x[..., idx].astype(np.int32)})\n        return feed_dict\n\n    def _define_input_and_placeholder(self):\n        super(Advanced, self)._define_input_and_placeholder()\n        if not self.categorical_columns:\n            self._categorical_xs = []\n            self._one_hot = self._one_hot_concat = self._tfx\n            self._embedding = self._embedding_concat = self._tfx\n            self._embedding_with_one_hot = self._embedding_with_one_hot_concat = self._tfx\n        else:\n            all_categorical = self.n_dim == 0\n            with tf.name_scope(""Categorical_Xs""):\n                self._categorical_xs = [\n                    tf.placeholder(tf.int32, shape=[None], name=""Categorical_X{}"".format(i))\n                    for i in range(len(self.categorical_columns))\n                ]\n            with tf.name_scope(""One_hot""):\n                one_hot_vars = [\n                    tf.one_hot(self._categorical_xs[i], n)\n                    for i, (_, n) in enumerate(self.categorical_columns)\n                ]\n                self._one_hot = self._one_hot_concat = tf.concat(one_hot_vars, 1, name=""Raw"")\n                if not all_categorical:\n                    self._one_hot_concat = tf.concat([self._tfx, self._one_hot], 1, name=""Concat"")\n            with tf.name_scope(""Embedding""):\n                embeddings = [\n                    self._get_embedding(i, n)\n                    for i, (_, n) in enumerate(self.categorical_columns)\n                ]\n                self._embedding = self._embedding_concat = tf.concat(embeddings, 1, name=""Raw"")\n                if not all_categorical:\n                    self._embedding_concat = tf.concat([self._tfx, self._embedding], 1, name=""Concat"")\n            with tf.name_scope(""Embedding_with_one_hot""):\n                self._embedding_with_one_hot = self._embedding_with_one_hot_concat = tf.concat(\n                    embeddings + one_hot_vars, 1, name=""Raw""\n                )\n                if not all_categorical:\n                    self._embedding_with_one_hot_concat = tf.concat(\n                        [self._tfx, self._embedding_with_one_hot], 1, name=""Concat""\n                    )\n        if self._wide_input == ""continuous"":\n            self._wide_input = self._tfx\n        else:\n            self._wide_input = getattr(self, ""_"" + self._wide_input)\n        if self._deep_input == ""continuous"":\n            self._deep_input = self._tfx\n        else:\n            self._deep_input = getattr(self, ""_"" + self._deep_input)\n        if self.hidden_units is None:\n            self._define_hidden_units()\n        self._tf_p_keep = tf.cond(\n            self._is_training, lambda: self.dropout_keep_prob, lambda: 1.,\n            name=""keep_prob""\n        )\n        self._n_batch_placeholder = tf.placeholder(tf.int32, name=""n_batch"")\n\n    def _define_py_collections(self):\n        super(Advanced, self)._define_py_collections()\n        self.py_collections += [""data_info"", ""numerical_idx"", ""categorical_columns""]\n\n    def _define_tf_collections(self):\n        super(Advanced, self)._define_tf_collections()\n        self.tf_collections += [\n            ""_deep_input"", ""_wide_input"", ""_n_batch_placeholder"",\n            ""_embedding"", ""_one_hot"", ""_embedding_with_one_hot"",\n            ""_embedding_concat"", ""_one_hot_concat"", ""_embedding_with_one_hot_concat""\n        ]\n        self.tf_list_collections = [""_categorical_xs""]\n\n    def add_tf_collections(self):\n        super(Advanced, self).add_tf_collections()\n        for tf_list in self.tf_list_collections:\n            target_list = getattr(self, tf_list)\n            if target_list is None:\n                continue\n            for tensor in target_list:\n                tf.add_to_collection(tf_list, tensor)\n\n    def restore_collections(self, folder):\n        for tf_list in self.tf_list_collections:\n            if tf_list is not None:\n                setattr(self, tf_list, tf.get_collection(tf_list))\n        super(Advanced, self).restore_collections(folder)\n\n    def clear_tf_collections(self):\n        super(Advanced, self).clear_tf_collections()\n        for key in self.tf_list_collections:\n            tf.get_collection_ref(key).clear()\n\n    def print_settings(self, only_return=False):\n        msg = ""\\n"".join([\n            ""="" * 100, ""This is a {}"".format(\n                ""{}-classes problem"".format(self.n_class) if not self.n_class == 1\n                else ""regression problem""\n            ), ""-"" * 100,\n            ""Data     : {} training samples, {} test samples"".format(\n                len(self._train_generator), len(self._test_generator) if self._test_generator is not None else 0\n            ),\n            ""Features : {} categorical, {} numerical"".format(\n                len(self.categorical_columns), np.sum(self.valid_numerical_idx)\n            )\n        ]) + ""\\n""\n\n        msg += ""="" * 100 + ""\\n""\n        msg += ""Deep model: DNN\\n""\n        msg += ""Deep model input: {}\\n"".format(\n            ""Continuous features only"" if not self.categorical_columns else\n            ""Continuous features with embeddings"" if np.any(self.numerical_idx) else\n            ""Embeddings only""\n        )\n        msg += ""-"" * 100 + ""\\n""\n        if self.categorical_columns:\n            msg += ""Embedding size: {}\\n"".format(self.embedding_size)\n            msg += ""Actual feature dimension: {}\\n"".format(self._embedding_concat.shape[1].value)\n        msg += ""-"" * 100 + ""\\n""\n        if self.dropout_keep_prob < 1:\n            msg += ""Using dropout with keep_prob = {}\\n"".format(self.dropout_keep_prob)\n        else:\n            msg += ""Training without dropout\\n""\n        msg += ""Training {} batch norm\\n"".format(""with"" if self.use_batch_norm else ""without"")\n        msg += ""Hidden units: {}\\n"".format(self.hidden_units)\n\n        msg += ""="" * 100 + ""\\n""\n        if not self._use_wide_network:\n            msg += ""Wide model: None\\n""\n        else:\n            msg += ""Wide model: {}\\n"".format(""logistic regression"" if self._dndf is None else ""DNDF"")\n            msg += ""Wide model input: Continuous features only\\n""\n            msg += ""-"" * 100 + \'\\n\'\n            if self._dndf is not None:\n                msg += ""Using DNDF with n_tree = {}, tree_depth = {}\\n"".format(\n                    self._dndf.n_tree, self._dndf.tree_depth\n                )\n\n        msg += ""\\n"".join([""="" * 100, ""Hyper parameters"", ""-"" * 100, ""{}"".format(\n            ""This is a DNN model"" if self._dndf is None and not self._use_wide_network else\n            ""This is a Wide & Deep model"" if self._dndf is None else\n            ""This is a hybrid model""\n        ), ""-"" * 100]) + ""\\n""\n        msg += ""Activation       : "" + str(self.activations) + ""\\n""\n        msg += ""Batch size       : "" + str(self.batch_size) + ""\\n""\n        msg += ""Epoch num        : "" + str(self.n_epoch) + ""\\n""\n        msg += ""Optimizer        : "" + self._optimizer_name + ""\\n""\n        msg += ""Metric           : "" + self._metric_name + ""\\n""\n        msg += ""Loss             : "" + self._loss_name + ""\\n""\n        msg += ""lr               : "" + str(self.lr) + ""\\n""\n        msg += ""-"" * 100 + ""\\n""\n        msg += ""Pruner           : {}"".format(""None"" if self._pruner is None else """") + ""\\n""\n        if self._pruner is not None:\n            msg += ""\\n"".join(""-> {:14}: {}"".format(key, value) for key, value in sorted(\n                self._pruner.params.items()\n            )) + ""\\n""\n        msg += ""-"" * 100\n        return msg if only_return else self.log_msg(\n            ""\\n"" + msg, logger=self.get_logger(""print_settings"", ""general.log""))\n'"
_Dist/NeuralNetworks/e_AdvancedNN/NN.py,23,"b'import os\nimport sys\nroot_path = os.path.abspath(""../../../"")\nif root_path not in sys.path:\n    sys.path.append(root_path)\n\nimport math\nimport numpy as np\nimport tensorflow as tf\n\nfrom _Dist.NeuralNetworks.NNUtil import *\nfrom _Dist.NeuralNetworks.c_BasicNN.NN import Basic\n\n\nclass Advanced(Basic):\n    signature = ""Advanced""\n\n    def __init__(self, name=None, data_info=None, model_param_settings=None, model_structure_settings=None):\n        self.tf_list_collections = None\n        super(Advanced, self).__init__(name, model_param_settings, model_structure_settings)\n        self._name_appendix = ""Advanced""\n\n        if data_info is None:\n            self.data_info = {}\n        else:\n            assert_msg = ""data_info should be a dictionary""\n            assert isinstance(data_info, dict), assert_msg\n            self.data_info = data_info\n        self._data_info_initialized = False\n        self.numerical_idx = self.categorical_columns = None\n\n        self._deep_input = self._wide_input = None\n        self._categorical_xs = None\n        self.embedding_size = None\n        self._embedding = self._one_hot = self._embedding_concat = self._one_hot_concat = None\n        self._embedding_with_one_hot = self._embedding_with_one_hot_concat = None\n\n        self.dropout_keep_prob = self.use_batch_norm = None\n        self._use_wide_network = self._dndf = self._pruner = self._dndf_pruner = None\n\n        self._tf_p_keep = None\n        self._n_batch_placeholder = None\n\n    @property\n    def valid_numerical_idx(self):\n        return np.array([\n            is_numerical for is_numerical in self.numerical_idx\n            if is_numerical is not None\n        ])\n\n    def init_data_info(self):\n        if self._data_info_initialized:\n            return\n        self._data_info_initialized = True\n        self.numerical_idx = self.data_info.get(""numerical_idx"", None)\n        self.categorical_columns = self.data_info.get(""categorical_columns"", None)\n        if self.numerical_idx is None:\n            raise ValueError(""numerical_idx should be provided"")\n        if self.categorical_columns is None:\n            raise ValueError(""categorical_columns should be provided"")\n\n    def init_from_data(self, x, y, x_test, y_test, sample_weights, names):\n        self.init_data_info()\n        super(Advanced, self).init_from_data(x, y, x_test, y_test, sample_weights, names)\n        if len(self.valid_numerical_idx) != self.n_dim + 1:\n            raise ValueError(""Length of valid_numerical_idx should be {}, {} found"".format(\n                self.n_dim + 1, len(self.valid_numerical_idx)\n            ))\n        self.n_dim -= len(self.categorical_columns)\n        self.model_structure_settings.setdefault(""use_wide_network"", self.n_dim > 0)\n\n    def init_model_param_settings(self):\n        super(Advanced, self).init_model_param_settings()\n        self.dropout_keep_prob = float(self.model_param_settings.get(""keep_prob"", 0.5))\n        self.use_batch_norm = self.model_param_settings.get(""use_batch_norm"", False)\n\n    def init_model_structure_settings(self):\n        self.hidden_units = self.model_structure_settings.get(""hidden_units"", None)\n        self._deep_input = self.model_structure_settings.get(""deep_input"", ""embedding_concat"")\n        self._wide_input = self.model_structure_settings.get(""wide_input"", ""continuous"")\n        self.embedding_size = self.model_structure_settings.get(""embedding_size"", 8)\n\n        self._use_wide_network = self.model_structure_settings[""use_wide_network""]\n        if not self._use_wide_network:\n            self._dndf = None\n        else:\n            dndf_params = self.model_structure_settings.get(""dndf_params"", {})\n            if self.model_structure_settings.get(""use_dndf"", True):\n                self._dndf = DNDF(self.n_class, **dndf_params)\n        if self.model_structure_settings.get(""use_pruner"", True):\n            pruner_params = self.model_structure_settings.get(""pruner_params"", {})\n            self._pruner = Pruner(**pruner_params)\n        if self.model_structure_settings.get(""use_dndf_pruner"", False):\n            dndf_pruner_params = self.model_structure_settings.get(""dndf_pruner_params"", {})\n            self._dndf_pruner = Pruner(**dndf_pruner_params)\n\n    def _get_embedding(self, i, n):\n        embedding_size = math.ceil(math.log2(n)) + 1 if self.embedding_size == ""log"" else self.embedding_size\n        embedding = tf.Variable(tf.truncated_normal(\n            [n, embedding_size], mean=0, stddev=0.02\n        ), name=""Embedding{}"".format(i))\n        return tf.nn.embedding_lookup(embedding, self._categorical_xs[i], name=""Embedded_X{}"".format(i))\n\n    def _define_hidden_units(self):\n        n_data = len(self._train_generator)\n        current_units = self._deep_input.shape[1].value\n        if current_units > 512:\n            self.hidden_units = [1024, 1024]\n        elif current_units > 256:\n            if n_data >= 10000:\n                self.hidden_units = [1024, 1024]\n            else:\n                self.hidden_units = [2 * current_units, 2 * current_units]\n        else:\n            if n_data >= 100000:\n                self.hidden_units = [768, 768]\n            elif n_data >= 10000:\n                self.hidden_units = [512, 512]\n            else:\n                self.hidden_units = [2 * current_units, 2 * current_units]\n\n    def _fully_connected_linear(self, net, shape, appendix):\n        with tf.name_scope(""Linear{}"".format(appendix)):\n            w = init_w(shape, ""W{}"".format(appendix))\n            if self._pruner is not None:\n                w = self._pruner.prune_w(*self._pruner.get_w_info(w))\n            b = init_b([shape[1]], ""b{}"".format(appendix))\n            self._ws.append(w)\n            self._bs.append(b)\n            return tf.add(tf.matmul(net, w), b, name=""Linear{}_Output"".format(appendix))\n\n    def _build_layer(self, i, net):\n        if self.use_batch_norm:\n            net = tf.layers.batch_normalization(net, training=self._is_training, name=""BN{}"".format(i))\n        activation = self.activations[i]\n        if activation is not None:\n            net = getattr(Activations, activation)(net, ""{}{}"".format(activation, i))\n        if self.dropout_keep_prob < 1:\n            net = tf.nn.dropout(net, keep_prob=self._tf_p_keep)\n        return net\n\n    def _build_model(self, net=None):\n        super(Advanced, self)._build_model(self._deep_input)\n        if self._use_wide_network:\n            if self._dndf is None:\n                wide_output = self._fully_connected_linear(\n                    self._wide_input, appendix=""_wide_output"",\n                    shape=[self._wide_input.shape[1].value, self.n_class]\n                )\n            else:\n                wide_output = self._dndf(\n                    self._wide_input, self._n_batch_placeholder,\n                    pruner=self._dndf_pruner\n                )\n            self._output += wide_output\n\n    def _get_feed_dict(self, x, y=None, weights=None, is_training=True):\n        continuous_x = x[..., self.valid_numerical_idx[:-1]] if self._categorical_xs else x\n        feed_dict = super(Advanced, self)._get_feed_dict(continuous_x, y, weights, is_training)\n        if self._dndf is not None:\n            feed_dict[self._n_batch_placeholder] = len(x)\n        if self._pruner is not None:\n            cond_placeholder = self._pruner.cond_placeholder\n            if cond_placeholder is not None:\n                feed_dict[cond_placeholder] = True\n        if self._dndf is not None and self._dndf_pruner is not None:\n            cond_placeholder = self._dndf_pruner.cond_placeholder\n            if cond_placeholder is not None:\n                feed_dict[cond_placeholder] = True\n        for (idx, _), categorical_x in zip(self.categorical_columns, self._categorical_xs):\n            feed_dict.update({categorical_x: x[..., idx].astype(np.int32)})\n        return feed_dict\n\n    def _define_input_and_placeholder(self):\n        super(Advanced, self)._define_input_and_placeholder()\n        if not self.categorical_columns:\n            self._categorical_xs = []\n            self._one_hot = self._one_hot_concat = self._tfx\n            self._embedding = self._embedding_concat = self._tfx\n            self._embedding_with_one_hot = self._embedding_with_one_hot_concat = self._tfx\n        else:\n            all_categorical = self.n_dim == 0\n            with tf.name_scope(""Categorical_Xs""):\n                self._categorical_xs = [\n                    tf.placeholder(tf.int32, shape=[None], name=""Categorical_X{}"".format(i))\n                    for i in range(len(self.categorical_columns))\n                ]\n            with tf.name_scope(""One_hot""):\n                one_hot_vars = [\n                    tf.one_hot(self._categorical_xs[i], n)\n                    for i, (_, n) in enumerate(self.categorical_columns)\n                ]\n                self._one_hot = self._one_hot_concat = tf.concat(one_hot_vars, 1, name=""Raw"")\n                if not all_categorical:\n                    self._one_hot_concat = tf.concat([self._tfx, self._one_hot], 1, name=""Concat"")\n            with tf.name_scope(""Embedding""):\n                embeddings = [\n                    self._get_embedding(i, n)\n                    for i, (_, n) in enumerate(self.categorical_columns)\n                ]\n                self._embedding = self._embedding_concat = tf.concat(embeddings, 1, name=""Raw"")\n                if not all_categorical:\n                    self._embedding_concat = tf.concat([self._tfx, self._embedding], 1, name=""Concat"")\n            with tf.name_scope(""Embedding_with_one_hot""):\n                self._embedding_with_one_hot = self._embedding_with_one_hot_concat = tf.concat(\n                    embeddings + one_hot_vars, 1, name=""Raw""\n                )\n                if not all_categorical:\n                    self._embedding_with_one_hot_concat = tf.concat(\n                        [self._tfx, self._embedding_with_one_hot], 1, name=""Concat""\n                    )\n        if self._wide_input == ""continuous"":\n            self._wide_input = self._tfx\n        else:\n            self._wide_input = getattr(self, ""_"" + self._wide_input)\n        if self._deep_input == ""continuous"":\n            self._deep_input = self._tfx\n        else:\n            self._deep_input = getattr(self, ""_"" + self._deep_input)\n        if self.hidden_units is None:\n            self._define_hidden_units()\n        self._tf_p_keep = tf.cond(\n            self._is_training, lambda: self.dropout_keep_prob, lambda: 1.,\n            name=""keep_prob""\n        )\n        self._n_batch_placeholder = tf.placeholder(tf.int32, name=""n_batch"")\n\n    def _define_py_collections(self):\n        super(Advanced, self)._define_py_collections()\n        self.py_collections += [""data_info"", ""numerical_idx"", ""categorical_columns""]\n\n    def _define_tf_collections(self):\n        super(Advanced, self)._define_tf_collections()\n        self.tf_collections += [\n            ""_deep_input"", ""_wide_input"", ""_n_batch_placeholder"",\n            ""_embedding"", ""_one_hot"", ""_embedding_with_one_hot"",\n            ""_embedding_concat"", ""_one_hot_concat"", ""_embedding_with_one_hot_concat""\n        ]\n        self.tf_list_collections = [""_categorical_xs""]\n\n    def add_tf_collections(self):\n        super(Advanced, self).add_tf_collections()\n        for tf_list in self.tf_list_collections:\n            target_list = getattr(self, tf_list)\n            if target_list is None:\n                continue\n            for tensor in target_list:\n                tf.add_to_collection(tf_list, tensor)\n\n    def restore_collections(self, folder):\n        for tf_list in self.tf_list_collections:\n            if tf_list is not None:\n                setattr(self, tf_list, tf.get_collection(tf_list))\n        super(Advanced, self).restore_collections(folder)\n\n    def clear_tf_collections(self):\n        super(Advanced, self).clear_tf_collections()\n        for key in self.tf_list_collections:\n            tf.get_collection_ref(key).clear()\n\n    def print_settings(self):\n        msg = ""\\n"".join([\n            ""="" * 100, ""This is a {}"".format(\n                ""{}-classes problem"".format(self.n_class) if not self.n_class == 1\n                else ""regression problem""\n            ), ""-"" * 100,\n            ""Data     : {} training samples, {} test samples"".format(\n                len(self._train_generator), len(self._test_generator) if self._test_generator is not None else 0\n            ),\n            ""Features : {} categorical, {} numerical"".format(\n                len(self.categorical_columns), np.sum(self.valid_numerical_idx)\n            )\n        ]) + ""\\n""\n\n        msg += ""="" * 100 + ""\\n""\n        msg += ""Deep model: DNN\\n""\n        msg += ""Deep model input: {}\\n"".format(\n            ""Continuous features only"" if not self.categorical_columns else\n            ""Continuous features with embeddings"" if np.any(self.numerical_idx) else\n            ""Embeddings only""\n        )\n        msg += ""-"" * 100 + ""\\n""\n        if self.categorical_columns:\n            msg += ""Embedding size: {}\\n"".format(self.embedding_size)\n            msg += ""Actual feature dimension: {}\\n"".format(self._embedding_concat.shape[1].value)\n        msg += ""-"" * 100 + ""\\n""\n        if self.dropout_keep_prob < 1:\n            msg += ""Using dropout with keep_prob = {}\\n"".format(self.dropout_keep_prob)\n        else:\n            msg += ""Training without dropout\\n""\n        msg += ""Training {} batch norm\\n"".format(""with"" if self.use_batch_norm else ""without"")\n        msg += ""Hidden units: {}\\n"".format(self.hidden_units)\n\n        msg += ""="" * 100 + ""\\n""\n        if not self._use_wide_network:\n            msg += ""Wide model: None\\n""\n        else:\n            msg += ""Wide model: {}\\n"".format(""logistic regression"" if self._dndf is None else ""DNDF"")\n            msg += ""Wide model input: Continuous features only\\n""\n            msg += ""-"" * 100 + \'\\n\'\n            if self._dndf is not None:\n                msg += ""Using DNDF with n_tree = {}, tree_depth = {}\\n"".format(\n                    self._dndf.n_tree, self._dndf.tree_depth\n                )\n\n        msg += ""\\n"".join([""="" * 100, ""Hyper parameters"", ""-"" * 100, ""{}"".format(\n            ""This is a DNN model"" if self._dndf is None and not self._use_wide_network else\n            ""This is a Wide & Deep model"" if self._dndf is None else\n            ""This is a hybrid model""\n        ), ""-"" * 100]) + ""\\n""\n        msg += ""Activation       : "" + str(self.activations) + ""\\n""\n        msg += ""Batch size       : "" + str(self.batch_size) + ""\\n""\n        msg += ""Epoch num        : "" + str(self.n_epoch) + ""\\n""\n        msg += ""Optimizer        : "" + self._optimizer_name + ""\\n""\n        msg += ""Metric           : "" + self._metric_name + ""\\n""\n        msg += ""Loss             : "" + self._loss_name + ""\\n""\n        msg += ""lr               : "" + str(self.lr) + ""\\n""\n        msg += ""-"" * 100 + ""\\n""\n        msg += ""Pruner           : {}"".format(""None"" if self._pruner is None else """") + ""\\n""\n        if self._pruner is not None:\n            msg += ""\\n"".join(""-> {:14}: {}"".format(key, value) for key, value in sorted(\n                self._pruner.params.items()\n            )) + ""\\n""\n        msg += ""-"" * 100 + ""\\n""\n        print(msg)\n'"
_Dist/NeuralNetworks/f_AutoNN/DistNN.py,0,"b'import os\nimport sys\nroot_path = os.path.abspath(""../../../"")\nif root_path not in sys.path:\n    sys.path.append(root_path)\n\nfrom _Dist.NeuralNetworks.DistBase import AutoBase, AutoMeta\nfrom _Dist.NeuralNetworks.e_AdvancedNN.DistNN import Basic, Advanced\n\n\nclass AutoBasic(AutoBase, Basic, metaclass=AutoMeta):\n    pass\n\n\nclass AutoAdvanced(AutoBase, Advanced, metaclass=AutoMeta):\n    pass\n'"
_Dist/NeuralNetworks/f_AutoNN/NN.py,0,"b'import os\nimport sys\nroot_path = os.path.abspath(""../../../"")\nif root_path not in sys.path:\n    sys.path.append(root_path)\n\nfrom _Dist.NeuralNetworks.c_BasicNN.NN import Basic\nfrom _Dist.NeuralNetworks.e_AdvancedNN.NN import Advanced\nfrom _Dist.NeuralNetworks.Base import AutoBase, AutoMeta\n\n\nclass AutoBasic(AutoBase, Basic, metaclass=AutoMeta):\n    pass\n\n\nclass AutoAdvanced(AutoBase, Advanced, metaclass=AutoMeta):\n    pass\n'"
_Dist/NeuralNetworks/g_DistNN/NN.py,0,"b'import os\nimport sys\nroot_path = os.path.abspath(""../../../"")\nif root_path not in sys.path:\n    sys.path.append(root_path)\n\nfrom _Dist.NeuralNetworks.DistBase import DistMixin, DistMeta\nfrom _Dist.NeuralNetworks.f_AutoNN.DistNN import AutoBasic, AutoAdvanced\n\n\nclass DistBasic(AutoBasic, DistMixin, metaclass=DistMeta):\n    pass\n\n\nclass DistAdvanced(AutoAdvanced, DistMixin, metaclass=DistMeta):\n    pass\n'"
_Dist/NeuralNetworks/h_RNN/Cell.py,32,"b'import os\nimport sys\nroot_path = os.path.abspath(""../../../"")\nif root_path not in sys.path:\n    sys.path.append(root_path)\n\nimport tensorflow as tf\nfrom tensorflow.contrib.rnn import LSTMStateTuple\n\nfrom _Dist.NeuralNetworks.NNUtil import DNDF\n\n\nclass LSTMCell(tf.contrib.rnn.LSTMCell):\n    def __str__(self):\n        return ""LSTMCell""\n\n\nclass BasicLSTMCell(tf.contrib.rnn.BasicLSTMCell):\n    def __str__(self):\n        return ""BasicLSTMCell""\n\n\nclass CustomLSTMCell(tf.contrib.rnn.BasicRNNCell):\n    def __init__(self, *args, **kwargs):\n        super(CustomLSTMCell, self).__init__(*args, **kwargs)\n        self._n_batch_placeholder = tf.placeholder(tf.int32, [], ""n_batch_placeholder"")\n\n    def __str__(self):\n        return ""CustomLSTMCell""\n\n    def __call__(self, x, state, scope=""LSTM""):\n        with tf.variable_scope(scope, reuse=tf.AUTO_REUSE):\n            s_old, h_old = state\n            net = tf.concat([x, s_old], 1)\n\n            w = tf.get_variable(\n                ""W"", [net.shape[1].value, 4 * self._num_units], tf.float32,\n                tf.contrib.layers.xavier_initializer()\n            )\n            b = tf.get_variable(\n                ""b"", [4 * self._num_units], tf.float32,\n                tf.zeros_initializer()\n            )\n            gates = tf.nn.xw_plus_b(net, w, b)\n\n            r1, g1, g2, g3 = tf.split(gates, 4, 1)\n            r1, g1, g3 = tf.nn.sigmoid(r1), tf.nn.sigmoid(g1), tf.nn.sigmoid(g3)\n            g2 = tf.nn.tanh(g2)\n            h_new = h_old * r1 + g1 * g2\n            s_new = tf.nn.tanh(h_new) * g3\n            return s_new, LSTMStateTuple(s_new, h_new)\n\n    @property\n    def state_size(self):\n        return LSTMStateTuple(self._num_units, self._num_units)\n\n\nclass DNDFCell(tf.contrib.rnn.BasicRNNCell):\n    def __init__(self, *args, **kwargs):\n        self._dndf = DNDF(reuse=True)\n        self.n_batch_placeholder = kwargs.pop(""n_batch_placeholder"", None)\n        if self.n_batch_placeholder is None:\n            self.n_batch_placeholder = tf.placeholder(tf.int32, name=""n_batch_placeholder"")\n        super(DNDFCell, self).__init__(*args, **kwargs)\n\n    def __str__(self):\n        return ""DNDFCell""\n\n    def __call__(self, x, state, scope=""DNDFCell""):\n        with tf.variable_scope(scope, reuse=tf.AUTO_REUSE):\n            s_old, h_old = state\n            net = tf.concat([\n                x,\n                s_old,\n                self._dndf(s_old, self.n_batch_placeholder, ""feature""),\n            ], 1)\n\n            w = tf.get_variable(\n                ""W"", [net.shape[1].value, 4 * self._num_units], tf.float32,\n                tf.contrib.layers.xavier_initializer()\n            )\n            b = tf.get_variable(\n                ""b"", [4 * self._num_units], tf.float32,\n                tf.zeros_initializer()\n            )\n            gates = tf.nn.xw_plus_b(net, w, b)\n\n            r1, g1, g2, g3 = tf.split(gates, 4, 1)\n            r1, g1, g3 = tf.nn.sigmoid(r1), tf.nn.sigmoid(g1), tf.nn.sigmoid(g3)\n            g2 = tf.nn.tanh(g2)\n            h_new = h_old * r1 + g1 * g2\n            s_new = tf.nn.tanh(h_new) * g3\n\n            return s_new, LSTMStateTuple(s_new, h_new)\n\n    @property\n    def state_size(self):\n        return LSTMStateTuple(self._num_units, self._num_units)\n\n\nclass CellFactory:\n    @staticmethod\n    def get_cell(name, n_hidden, **kwargs):\n        if name == ""LSTM"" or name == ""LSTMCell"":\n            cell = LSTMCell\n        elif name == ""BasicLSTM"" or name == ""BasicLSTMCell"":\n            cell = BasicLSTMCell\n        elif name == ""CustomLSTM"" or name == ""CustomLSTMCell"":\n            cell = CustomLSTMCell\n        elif name == ""DNDF"" or name == ""DNDFCell"":\n            cell = DNDFCell\n        else:\n            raise NotImplementedError(""Cell \'{}\' not implemented"".format(name))\n        return cell(n_hidden, **kwargs)\n'"
_Dist/NeuralNetworks/h_RNN/RNN.py,7,"b'import os\nimport sys\nroot_path = os.path.abspath(""../../../"")\nif root_path not in sys.path:\n    sys.path.append(root_path)\n\nimport numpy as np\nimport tensorflow as tf\n\nfrom _Dist.NeuralNetworks.NNUtil import Toolbox\nfrom _Dist.NeuralNetworks.Base import Generator3d\nfrom _Dist.NeuralNetworks.c_BasicNN.NN import Basic\nfrom _Dist.NeuralNetworks.h_RNN.Cell import CellFactory\n\n\nclass Basic3d(Basic):\n    def _gen_batch(self, generator, n_batch, gen_random_subset=False, one_hot=False):\n        if gen_random_subset:\n            data, weights = generator.gen_random_subset(n_batch)\n        else:\n            data, weights = generator.gen_batch(n_batch)\n        x = np.array([d[0] for d in data], np.float32)\n        y = np.array([d[1] for d in data], np.float32)\n        if not one_hot:\n            return x, y, weights\n        if self.n_class == 1:\n            y = y.reshape([-1, 1])\n        else:\n            y = Toolbox.get_one_hot(y, self.n_class)\n        return x, y, weights\n\n\nclass RNN(Basic3d):\n    def __init__(self, *args, **kwargs):\n        self.n_time_step = kwargs.pop(""n_time_step"", None)\n\n        super(RNN, self).__init__(*args, **kwargs)\n        self._name_appendix = ""RNN""\n        self._generator_base = Generator3d\n\n        self._using_dndf_cell = False\n        self._n_batch_placeholder = None\n        self._cell = self._cell_name = None\n        self.n_hidden = self.n_history = self.use_final_state = None\n\n    def init_model_param_settings(self):\n        super(RNN, self).init_model_param_settings()\n        self._cell_name = self.model_param_settings.get(""cell"", ""CustomLSTM"")\n\n    def init_model_structure_settings(self):\n        super(RNN, self).init_model_structure_settings()\n        self.n_hidden = self.model_structure_settings.get(""n_hidden"", 128)\n        self.n_history = self.model_structure_settings.get(""n_history"", 0)\n        self.use_final_state = self.model_structure_settings.get(""use_final_state"", True)\n\n    def init_from_data(self, x, y, x_test, y_test, sample_weights, names):\n        if self.n_time_step is None:\n            assert len(x.shape) == 3, ""n_time_step is not provided, hence len(x.shape) should be 3""\n            self.n_time_step = x.shape[1]\n        if len(x.shape) == 2:\n            x = x.reshape(len(x), self.n_time_step, -1)\n        else:\n            assert self.n_time_step == x.shape[1], ""n_time_step is set to be {}, but {} found"".format(\n                self.n_time_step, x.shape[1]\n            )\n        if len(x_test.shape) == 2:\n            x_test = x_test.reshape(len(x_test), self.n_time_step, -1)\n        super(RNN, self).init_from_data(x, y, x_test, y_test, sample_weights, names)\n\n    def _define_input_and_placeholder(self):\n        self._is_training = tf.placeholder(tf.bool, name=""is_training"")\n        self._tfx = tf.placeholder(tf.float32, [None, self.n_time_step, self.n_dim], name=""X"")\n        self._tfy = tf.placeholder(tf.float32, [None, self.n_class], name=""Y"")\n\n    def _build_model(self, net=None):\n        self._model_built = True\n        if net is None:\n            net = self._tfx\n\n        self._cell = CellFactory.get_cell(self._cell_name, self.n_hidden)\n        if ""DNDF"" in self._cell_name:\n            self._using_dndf_cell = True\n            self._n_batch_placeholder = self._cell.n_batch_placeholder\n\n        initial_state = self._cell.zero_state(tf.shape(net)[0], tf.float32)\n        rnn_outputs, rnn_final_state = tf.nn.dynamic_rnn(self._cell, net, initial_state=initial_state)\n\n        if self.n_history == 0:\n            net = None\n        elif self.n_history == 1:\n            net = rnn_outputs[..., -1, :]\n        else:\n            net = rnn_outputs[..., -self.n_history:, :]\n            net = tf.reshape(net, [-1, self.n_history * int(net.shape[2].value)])\n        if self.use_final_state:\n            if net is None:\n                net = rnn_final_state[1]\n            else:\n                net = tf.concat([net, rnn_final_state[1]], axis=1)\n        return super(RNN, self)._build_model(net)\n\n    def _get_feed_dict(self, x, y=None, weights=None, is_training=False):\n        feed_dict = super(RNN, self)._get_feed_dict(x, y, weights, is_training)\n        if self._using_dndf_cell:\n            feed_dict[self._n_batch_placeholder] = len(x)\n        return feed_dict\n\n    def _define_py_collections(self):\n        super(RNN, self)._define_py_collections()\n        self.py_collections.append(""_using_dndf_cell"")\n\n    def _define_tf_collections(self):\n        super(RNN, self)._define_tf_collections()\n        self.tf_collections.append(""_n_batch_placeholder"")\n'"
_Dist/NeuralNetworks/i_CNN/CNN.py,8,"b'import os\nimport sys\nroot_path = os.path.abspath(""../../../"")\nif root_path not in sys.path:\n    sys.path.append(root_path)\n\nimport numpy as np\nimport tensorflow as tf\n\nfrom _Dist.NeuralNetworks.Base import Generator4d\nfrom _Dist.NeuralNetworks.h_RNN.RNN import Basic3d\nfrom _Dist.NeuralNetworks.NNUtil import Activations\n\n\nclass Basic4d(Basic3d):\n    def _calculate(self, x, y=None, weights=None, tensor=None, n_elem=1e7, is_training=False):\n        return super(Basic4d, self)._calculate(x, y, weights, tensor, n_elem / 10, is_training)\n\n\nclass CNN(Basic4d):\n    def __init__(self, *args, **kwargs):\n        self.height, self.width = kwargs.pop(""height"", None), kwargs.pop(""width"", None)\n\n        super(CNN, self).__init__(*args, **kwargs)\n        self._name_appendix = ""CNN""\n        self._generator_base = Generator4d\n\n        self.conv_activations = None\n        self.n_filters = self.filter_sizes = self.poolings = None\n\n    def init_model_param_settings(self):\n        super(CNN, self).init_model_param_settings()\n        self.conv_activations = self.model_param_settings.get(""conv_activations"", ""relu"")\n\n    def init_model_structure_settings(self):\n        super(CNN, self).init_model_structure_settings()\n        self.n_filters = self.model_structure_settings.get(""n_filters"", [32, 32])\n        self.filter_sizes = self.model_structure_settings.get(""filter_sizes"", [(3, 3), (3, 3)])\n        self.poolings = self.model_structure_settings.get(""poolings"", [None, ""max_pool""])\n        if not len(self.filter_sizes) == len(self.poolings) == len(self.n_filters):\n            raise ValueError(""Length of filter_sizes, n_filters & pooling should be the same"")\n        if isinstance(self.conv_activations, str):\n            self.conv_activations = [self.conv_activations] * len(self.filter_sizes)\n\n    def init_from_data(self, x, y, x_test, y_test, sample_weights, names):\n        if self.height is None or self.width is None:\n            assert len(x.shape) == 4, ""height and width are not provided, hence len(x.shape) should be 4""\n            self.height, self.width = x.shape[1:3]\n        if len(x.shape) == 2:\n            x = x.reshape(len(x), self.height, self.width, -1)\n        else:\n            assert self.height == x.shape[1], ""height is set to be {}, but {} found"".format(self.height, x.shape[1])\n            assert self.width == x.shape[2], ""width is set to be {}, but {} found"".format(self.height, x.shape[2])\n        if x_test is not None and len(x_test.shape) == 2:\n            x_test = x_test.reshape(len(x_test), self.height, self.width, -1)\n        super(CNN, self).init_from_data(x, y, x_test, y_test, sample_weights, names)\n\n    def _define_input_and_placeholder(self):\n        self._is_training = tf.placeholder(tf.bool, name=""is_training"")\n        self._tfx = tf.placeholder(tf.float32, [None, self.height, self.width, self.n_dim], name=""X"")\n        self._tfy = tf.placeholder(tf.float32, [None, self.n_class], name=""Y"")\n\n    def _build_model(self, net=None):\n        self._model_built = True\n        if net is None:\n            net = self._tfx\n        for i, (filter_size, n_filter, pooling) in enumerate(zip(\n            self.filter_sizes, self.n_filters, self.poolings\n        )):\n            net = tf.layers.conv2d(net, n_filter, filter_size, padding=""same"")\n            net = tf.layers.batch_normalization(net, training=self._is_training)\n            activation = self.conv_activations[i]\n            if activation is not None:\n                net = getattr(Activations, activation)(net, activation)\n            net = tf.layers.dropout(net, training=self._is_training)\n            if pooling is not None:\n                net = tf.layers.max_pooling2d(net, 2, 2, name=""pool"")\n\n        fc_shape = np.prod([net.shape[i].value for i in range(1, 4)])\n        net = tf.reshape(net, [-1, fc_shape])\n        super(CNN, self)._build_model(net)\n'"
NN/Test/PyTorch/Auto/Test.py,0,"b'from NN.PyTorch.Auto.Networks import *\r\n\r\nfrom Util.Util import DataUtil\r\n\r\n\r\ndef main():\r\n    nn = NNDist()\r\n    save = False\r\n    load = False\r\n    show_loss = True\r\n    train_only = False\r\n    verbose = 2\r\n\r\n    lr = 0.001\r\n    lb = 0.001\r\n    epoch = 5\r\n    record_period = 1\r\n\r\n    x, y = DataUtil.get_dataset(""mnist"", ""../../../../_Data/mnist.txt"", quantized=True, one_hot=True)\r\n    batch_size = 128\r\n\r\n    if not load:\r\n        nn.add(""ReLU"", (x.shape[1], 1024))\r\n        nn.add(""ReLU"", (1024,))\r\n        nn.add(""CrossEntropy"", (y.shape[1],))\r\n        nn.optimizer = ""Adam""\r\n        nn.preview()\r\n        nn.fit(x, y, lr=lr, lb=lb,\r\n               epoch=epoch, batch_size=batch_size, record_period=record_period,\r\n               show_loss=show_loss, train_only=train_only, do_log=True, verbose=verbose)\r\n        if save:\r\n            nn.save()\r\n        nn.draw_results()\r\n    else:\r\n        nn.load()\r\n        nn.preview()\r\n        print(nn.evaluate(x, y)[0])\r\n\r\n    nn.show_timing_log()\r\n\r\nif __name__ == \'__main__\':\r\n    main()\r\n'"
NN/Test/PyTorch/Auto/Vis.py,0,"b'from NN.PyTorch.Auto.Networks import *\r\n\r\nfrom Util.Util import DataUtil\r\n\r\n\r\ndef main():\r\n    nn = NNDist()\r\n    save = False\r\n    load = False\r\n\r\n    lr = 0.001\r\n    lb = 0.001\r\n    epoch = 1000\r\n    record_period = 4\r\n\r\n    x, y = DataUtil.gen_spiral(50, 3, 3, 2.5)\r\n\r\n    if not load:\r\n        nn.build([x.shape[1], 6, 6, 6, y.shape[1]])\r\n        nn.optimizer = ""Adam""\r\n        nn.preview()\r\n        nn.fit(x, y, lr=lr, lb=lb, verbose=1, record_period=record_period,\r\n               epoch=epoch, batch_size=128, train_only=True,\r\n               animation_params={""show"": True, ""mp4"": False, ""period"": record_period})\r\n        if save:\r\n            nn.save()\r\n        nn.visualize2d(x, y)\r\n        nn.draw_results()\r\n    else:\r\n        nn.load()\r\n        nn.preview()\r\n        nn.evaluate(x, y)\r\n\r\n    nn.show_timing_log()\r\n\r\nif __name__ == \'__main__\':\r\n    main()\r\n'"
NN/Test/PyTorch/Basic/Test.py,0,"b'from NN.PyTorch.Basic.Networks import *\n\nfrom Util.Util import DataUtil\n\n\ndef main():\n    nn = NNDist()\n    save = False\n    load = False\n    show_loss = True\n    train_only = False\n    verbose = 2\n\n    lr = 0.001\n    lb = 0.001\n    epoch = 5\n    record_period = 1\n\n    x, y = DataUtil.get_dataset(""mnist"", ""../../../../_Data/mnist.txt"", quantized=True, one_hot=True)\n    batch_size = 128\n\n    if not load:\n        nn.add(""ReLU"", (x.shape[1], 1024))\n        nn.add(""ReLU"", (1024,))\n        nn.add(""CrossEntropy"", (y.shape[1],))\n        nn.optimizer = ""Adam""\n        nn.preview()\n        nn.fit(x, y, lr=lr, lb=lb,\n               epoch=epoch, batch_size=batch_size, record_period=record_period,\n               show_loss=show_loss, train_only=train_only, do_log=True, verbose=verbose)\n        if save:\n            nn.save()\n        nn.draw_results()\n    else:\n        nn.load()\n        nn.preview()\n        print(nn.evaluate(x, y)[0])\n\n    nn.show_timing_log()\n\nif __name__ == \'__main__\':\n    main()\n'"
NN/Test/PyTorch/Basic/Vis.py,0,"b'from NN.PyTorch.Basic.Networks import *\n\nfrom Util.Util import DataUtil\n\n\ndef main():\n    nn = NNDist()\n    save = False\n    load = False\n\n    lr = 0.001\n    lb = 0.001\n    epoch = 1000\n    record_period = 4\n\n    x, y = DataUtil.gen_spiral(50, 3, 3, 2.5)\n\n    if not load:\n        nn.build([x.shape[1], 6, 6, 6, y.shape[1]])\n        nn.optimizer = ""Adam""\n        nn.preview()\n        nn.fit(x, y, lr=lr, lb=lb, verbose=1, record_period=record_period,\n               epoch=epoch, batch_size=128, train_only=True,\n               animation_params={""show"": True, ""mp4"": False, ""period"": record_period})\n        if save:\n            nn.save()\n        nn.visualize2d(x, y)\n        nn.draw_results()\n    else:\n        nn.load()\n        nn.preview()\n        nn.evaluate(x, y)\n\n    nn.show_timing_log()\n\nif __name__ == \'__main__\':\n    main()\n'"
NN/Test/PyTorch/__Dev/Test.py,0,"b'from NN.PyTorch.__Dev.Networks import *\n\nfrom Util.Util import DataUtil\n\n\ndef main():\n    nn = NNDist()\n    save = False\n    load = False\n    show_loss = True\n    train_only = False\n    verbose = 2\n\n    lr = 0.001\n    lb = 0.001\n    epoch = 5\n    record_period = 1\n    use_cnn = False\n\n    x, y = DataUtil.get_dataset(""mnist"", ""../../../../_Data/mnist.txt"", quantized=True, one_hot=True)\n    if use_cnn:\n        x = x.reshape(len(x), 1, 28, 28)\n        batch_size = 32\n    else:\n        batch_size = 128\n\n    if not load:\n        if use_cnn:\n            nn.add(""ConvReLU"", (x.shape[1:], (16, 3, 3)))\n            nn.add(""MaxPool"", ((3, 3),), 1)\n            nn.add(""ConvNorm"")\n            nn.add(""ConvDrop"")\n        else:\n            nn.add(""ReLU"", (x.shape[1], 1024))\n            nn.add(""ReLU"", (1024,))\n        nn.add(""CrossEntropy"", (y.shape[1],))\n        nn.optimizer = ""Adam""\n        nn.preview()\n        nn.fit(x, y, lr=lr, lb=lb,\n               epoch=epoch, batch_size=batch_size, record_period=record_period,\n               show_loss=show_loss, train_only=train_only, do_log=True, verbose=verbose)\n        if save:\n            nn.save()\n        nn.draw_results()\n    else:\n        nn.load()\n        nn.preview()\n        print(nn.evaluate(x, y)[0])\n\n    nn.show_timing_log()\n\nif __name__ == \'__main__\':\n    main()\n'"
NN/Test/PyTorch/__Dev/Vis.py,0,"b'from NN.PyTorch.__Dev.Networks import *\n\nfrom Util.Util import DataUtil\n\n\ndef main():\n    nn = NNDist()\n    save = False\n    load = False\n\n    lr = 0.001\n    lb = 0.001\n    epoch = 1000\n    record_period = 4\n\n    x, y = DataUtil.gen_spiral(50, 3, 3, 2.5)\n\n    if not load:\n        nn.build([x.shape[1], 6, 6, 6, y.shape[1]])\n        nn.optimizer = ""Adam""\n        nn.preview()\n        nn.fit(x, y, lr=lr, lb=lb, verbose=1, record_period=record_period,\n               epoch=epoch, batch_size=128, train_only=True,\n               animation_params={""show"": True, ""mp4"": False, ""period"": record_period})\n        if save:\n            nn.save()\n        nn.visualize2d(x, y)\n        nn.draw_results()\n    else:\n        nn.load()\n        nn.preview()\n        nn.evaluate(x, y)\n\n    nn.show_timing_log()\n\nif __name__ == \'__main__\':\n    main()\n'"
Zhihu/NN/_extra/one/Networks.py,0,"b'import matplotlib.pyplot as plt\n\nfrom Zhihu.NN._extra.Layers import *\nfrom Zhihu.NN._extra.Optimizers import *\n\n\nclass NNDist:\n    NNTiming = Timing()\n\n    def __init__(self):\n        self._layers, self._weights, self._bias = [], [], []\n        self._w_optimizer = self._b_optimizer = None\n        self._current_dimension = 0\n\n    @NNTiming.timeit(level=4, prefix=""[API] "")\n    def feed_timing(self, timing):\n        if isinstance(timing, Timing):\n            self.NNTiming = timing\n            for layer in self._layers:\n                layer.feed_timing(timing)\n\n    def __str__(self):\n        return ""Neural Network""\n\n    __repr__ = __str__\n\n    # Utils\n\n    @NNTiming.timeit(level=4)\n    def _add_weight(self, shape):\n        self._weights.append(np.random.randn(*shape))\n        self._bias.append(np.zeros((1, shape[1])))\n\n    @NNTiming.timeit(level=4)\n    def _add_layer(self, layer, *args):\n        _parent = self._layers[-1]\n        _current, _next = args\n        self._layers.append(layer)\n        if isinstance(layer, CostLayer):\n            _parent.child = layer\n            self.parent = _parent\n            self._add_weight((1, 1))\n            self._current_dimension = _next\n        else:\n            self._add_weight((_current, _next))\n            self._current_dimension = _next\n\n    @NNTiming.timeit(level=4)\n    def _add_cost_layer(self):\n        _last_layer = self._layers[-1]\n        if _last_layer.name == ""Sigmoid"":\n            _cost_func = ""Cross Entropy""\n        elif _last_layer.name == ""Softmax"":\n            _cost_func = ""Log Likelihood""\n        else:\n            _cost_func = ""MSE""\n        _cost_layer = CostLayer(_last_layer, (self._current_dimension,), _cost_func)\n        self.add(_cost_layer)\n\n    @NNTiming.timeit(level=1)\n    def _get_prediction(self, x):\n        return self._get_activations(x).pop()\n\n    @NNTiming.timeit(level=1)\n    def _get_activations(self, x):\n        _activations = [self._layers[0].activate(x, self._weights[0], self._bias[0])]\n        for i, layer in enumerate(self._layers[1:]):\n            _activations.append(layer.activate(\n                _activations[-1], self._weights[i + 1], self._bias[i + 1]))\n        return _activations\n\n    # Optimizing Process\n\n    def _init_optimizers(self, lr):\n        self._w_optimizer, self._b_optimizer = Adam(lr), Adam(lr)\n        self._w_optimizer.feed_variables(self._weights)\n        self._b_optimizer.feed_variables(self._bias)\n\n    @NNTiming.timeit(level=1)\n    def _opt(self, i, _activation, _delta):\n        self._weights[i] += self._w_optimizer.run(\n            i, _activation.reshape(_activation.shape[0], -1).T.dot(_delta)\n        )\n        self._bias[i] += self._b_optimizer.run(\n            i, np.sum(_delta, axis=0, keepdims=True)\n        )\n\n    # API\n\n    @NNTiming.timeit(level=4, prefix=""[API] "")\n    def add(self, layer):\n        if not self._layers:\n            self._layers, self._current_dimension = [layer], layer.shape[1]\n            self._add_weight(layer.shape)\n        else:\n            _next = layer.shape[0]\n            layer.shape = (self._current_dimension, _next)\n            self._add_layer(layer, self._current_dimension, _next)\n\n    @NNTiming.timeit(level=1, prefix=""[API] "")\n    def fit(self, x=None, y=None, lr=0.01, epoch=10):\n        # Initialize\n        self._add_cost_layer()\n        self._init_optimizers(lr)\n        layer_width = len(self._layers)\n        # Train\n        for counter in range(epoch):\n            self._w_optimizer.update(); self._b_optimizer.update()\n            _activations = self._get_activations(x)\n            _deltas = [self._layers[-1].bp_first(y, _activations[-1])]\n            for i in range(-1, -len(_activations), -1):\n                _deltas.append(self._layers[i - 1].bp(\n                    _activations[i - 1], self._weights[i], _deltas[-1]\n                ))\n            for i in range(layer_width - 2, 0, -1):\n                self._opt(i, _activations[i - 1], _deltas[layer_width-i-1])\n            self._opt(0, x, _deltas[-1])\n\n    @NNTiming.timeit(level=4, prefix=""[API] "")\n    def predict(self, x):\n        return self._get_prediction(x)\n\n    @NNTiming.timeit(level=4, prefix=""[API] "")\n    def evaluate(self, x, y):\n        y_pred = self.predict(x)\n        y_arg = np.argmax(y, axis=1)\n        y_pred_arg = np.argmax(y_pred, axis=1)\n        print(""Acc: {:8.6}"".format(np.sum(y_arg == y_pred_arg) / len(y_arg)))\n\n    def visualize_2d(self, x, y, plot_scale=2, plot_precision=0.01):\n\n        plot_num = int(1 / plot_precision)\n\n        xf = np.linspace(np.min(x) * plot_scale, np.max(x) * plot_scale, plot_num)\n        yf = np.linspace(np.min(x) * plot_scale, np.max(x) * plot_scale, plot_num)\n        input_x, input_y = np.meshgrid(xf, yf)\n        input_xs = np.c_[input_x.ravel(), input_y.ravel()]\n\n        output_ys_2d = np.argmax(self.predict(input_xs), axis=1).reshape(len(xf), len(yf))\n\n        plt.contourf(input_x, input_y, output_ys_2d, cmap=plt.cm.Spectral)\n        plt.scatter(x[:, 0], x[:, 1], c=np.argmax(y, axis=1), s=40, cmap=plt.cm.Spectral)\n        plt.axis(""off"")\n        plt.show()\n'"
Zhihu/NN/_extra/one/Test.py,0,"b""from Util.Util import DataUtil\r\nfrom Zhihu.NN._extra.one.Networks import *\r\n\r\nnp.random.seed(142857)  # for reproducibility\r\n\r\n\r\ndef main():\r\n\r\n    nn = NNDist()\r\n    epoch = 1000\r\n\r\n    timing = Timing(enabled=True)\r\n    timing_level = 1\r\n    nn.feed_timing(timing)\r\n\r\n    x, y = DataUtil.gen_spiral(100)\r\n\r\n    nn.add(ReLU((x.shape[1], 24)))\r\n    nn.add(ReLU((24,)))\r\n    nn.add(Softmax((y.shape[1],)))\r\n\r\n    nn.fit(x, y, epoch=epoch)\r\n    nn.visualize_2d(x, y)\r\n    nn.evaluate(x, y)\r\n\r\n    timing.show_timing_log(timing_level)\r\n\r\nif __name__ == '__main__':\r\n    main()\r\n"""
Zhihu/NN/_extra/two/Networks.py,0,"b'import matplotlib.pyplot as plt\r\n\r\nfrom Zhihu.NN._extra.Layers import *\r\nfrom Zhihu.NN._extra.Optimizers import *\r\n\r\nfrom Util.Timing import Timing\r\nfrom Util.ProgressBar import ProgressBar\r\n\r\n\r\nclass NNVerbose:\r\n    NONE = 0\r\n    EPOCH = 1\r\n    METRICS = 2\r\n    METRICS_DETAIL = 3\r\n    DETAIL = 4\r\n    DEBUG = 5\r\n\r\n\r\n# Neural Network\r\n\r\nclass NNDist:\r\n    NNTiming = Timing()\r\n\r\n    def __init__(self):\r\n        self._layers, self._weights, self._bias = [], [], []\r\n        self._w_optimizer = self._b_optimizer = None\r\n        self._current_dimension = 0\r\n\r\n        self.verbose = 0\r\n        self._logs = {}\r\n        self._metrics, self._metric_names = [], []\r\n        self._available_metrics = {\r\n            ""acc"": NNDist._acc, ""_acc"": NNDist._acc,\r\n            ""f1_score"": NNDist._f1_score, ""_f1_score"": NNDist._f1_score\r\n        }\r\n\r\n    @NNTiming.timeit(level=4, prefix=""[API] "")\r\n    def feed_timing(self, timing):\r\n        if isinstance(timing, Timing):\r\n            self.NNTiming = timing\r\n            for layer in self._layers:\r\n                layer.feed_timing(timing)\r\n\r\n    def __str__(self):\r\n        return ""Neural Network""\r\n\r\n    __repr__ = __str__\r\n\r\n    # Utils\r\n\r\n    @NNTiming.timeit(level=4)\r\n    def _add_weight(self, shape):\r\n        self._weights.append(np.random.randn(*shape))\r\n        self._bias.append(np.zeros((1, shape[1])))\r\n\r\n    @NNTiming.timeit(level=4)\r\n    def _add_layer(self, layer, *args):\r\n        _parent = self._layers[-1]\r\n        _current, _next = args\r\n        self._layers.append(layer)\r\n        if isinstance(layer, CostLayer):\r\n            _parent.child = layer\r\n            self.parent = _parent\r\n            self._weights.append(np.eye(_current))\r\n            self._bias.append(np.zeros((1, _current)))\r\n            self._current_dimension = _next\r\n        else:\r\n            self._add_weight((_current, _next))\r\n            self._current_dimension = _next\r\n\r\n    @NNTiming.timeit(level=4)\r\n    def _add_cost_layer(self):\r\n        _last_layer = self._layers[-1]\r\n        if _last_layer.name == ""Sigmoid"":\r\n            _cost_func = ""Cross Entropy""\r\n        elif _last_layer.name == ""Softmax"":\r\n            _cost_func = ""Log Likelihood""\r\n        else:\r\n            _cost_func = ""MSE""\r\n        _cost_layer = CostLayer(_last_layer, (self._current_dimension,), _cost_func)\r\n        self.add(_cost_layer)\r\n\r\n    @NNTiming.timeit(level=1)\r\n    def _get_prediction(self, x, name=None, batch_size=1e6, verbose=None):\r\n        if verbose is None:\r\n            verbose = self.verbose\r\n        fc_shape = np.prod(x.shape[1:])  # type: float\r\n        single_batch = int(batch_size / fc_shape)\r\n        if not single_batch:\r\n            single_batch = 1\r\n        if single_batch >= len(x):\r\n            return self._get_activations(x, predict=True).pop()\r\n        epoch = int(len(x) / single_batch)\r\n        if not len(x) % single_batch:\r\n            epoch += 1\r\n        name = ""Prediction"" if name is None else ""Prediction ({})"".format(name)\r\n        sub_bar = ProgressBar(max_value=epoch, name=name, start=False)\r\n        if verbose >= NNVerbose.METRICS:\r\n            sub_bar.start()\r\n        rs, count = [self._get_activations(x[:single_batch], predict=True).pop()], single_batch\r\n        if verbose >= NNVerbose.METRICS:\r\n            sub_bar.update()\r\n        while count < len(x):\r\n            count += single_batch\r\n            if count >= len(x):\r\n                rs.append(self._get_activations(x[count - single_batch:], predict=True).pop())\r\n            else:\r\n                rs.append(self._get_activations(x[count - single_batch:count], predict=True).pop())\r\n            if verbose >= NNVerbose.METRICS:\r\n                sub_bar.update()\r\n        return np.vstack(rs)\r\n\r\n    @NNTiming.timeit(level=1)\r\n    def _get_activations(self, x, predict=False):\r\n        _activations = [self._layers[0].activate(x, self._weights[0], self._bias[0], predict)]\r\n        for i, layer in enumerate(self._layers[1:]):\r\n            _activations.append(layer.activate(\r\n                _activations[-1], self._weights[i + 1], self._bias[i + 1], predict))\r\n        return _activations\r\n\r\n    @NNTiming.timeit(level=3)\r\n    def _append_log(self, x, y, name):\r\n        y_pred = self._get_prediction(x, name)\r\n        for i, metric in enumerate(self._metrics):\r\n            self._logs[name][i].append(metric(y, y_pred))\r\n        self._logs[name][-1].append(self._layers[-1].calculate(y, y_pred) / len(y))\r\n\r\n    @NNTiming.timeit(level=3)\r\n    def _print_metric_logs(self, data_type):\r\n        print()\r\n        print(""="" * 47)\r\n        for i, name in enumerate(self._metric_names):\r\n            print(""{:<16s} {:<16s}: {:12.8}"".format(\r\n                data_type, name, self._logs[data_type][i][-1]))\r\n        print(""{:<16s} {:<16s}: {:12.8}"".format(\r\n            data_type, ""loss"", self._logs[data_type][-1][-1]))\r\n        print(""="" * 47)\r\n\r\n    # Metrics\r\n\r\n    @staticmethod\r\n    @NNTiming.timeit(level=2, prefix=""[Private StaticMethod] "")\r\n    def _acc(y, y_pred):\r\n        y_arg, y_pred_arg = np.argmax(y, axis=1), np.argmax(y_pred, axis=1)\r\n        return np.sum(y_arg == y_pred_arg) / len(y_arg)\r\n\r\n    @staticmethod\r\n    @NNTiming.timeit(level=2, prefix=""[Private StaticMethod] "")\r\n    def _f1_score(y, y_pred):\r\n        y_true, y_pred = np.argmax(y, axis=1), np.argmax(y_pred, axis=1)\r\n        tp = np.sum(y_true * y_pred)\r\n        if tp == 0:\r\n            return .0\r\n        fp = np.sum((1 - y_true) * y_pred)\r\n        fn = np.sum(y_true * (1 - y_pred))\r\n        return 2 * tp / (2 * tp + fn + fp)\r\n\r\n    # Optimizing Process\r\n\r\n    def _init_optimizers(self, lr):\r\n        self._w_optimizer, self._b_optimizer = Adam(lr), Adam(lr)\r\n        self._w_optimizer.feed_variables(self._weights)\r\n        self._b_optimizer.feed_variables(self._bias)\r\n\r\n    @NNTiming.timeit(level=1)\r\n    def _opt(self, i, _activation, _delta):\r\n        self._weights[i] += self._w_optimizer.run(\r\n            i, _activation.reshape(_activation.shape[0], -1).T.dot(_delta)\r\n        )\r\n        self._bias[i] += self._b_optimizer.run(\r\n            i, np.sum(_delta, axis=0, keepdims=True)\r\n        )\r\n\r\n    # API\r\n\r\n    @NNTiming.timeit(level=4, prefix=""[API] "")\r\n    def add(self, layer):\r\n        if not self._layers:\r\n            self._layers, self._current_dimension = [layer], layer.shape[1]\r\n            self._add_weight(layer.shape)\r\n        else:\r\n            _next = layer.shape[0]\r\n            layer.shape = (self._current_dimension, _next)\r\n            self._add_layer(layer, self._current_dimension, _next)\r\n\r\n    @NNTiming.timeit(level=1, prefix=""[API] "")\r\n    def fit(self, x=None, y=None, lr=0.01, epoch=10, batch_size=128, train_rate=None,\r\n            verbose=0, metrics=None, record_period=100):\r\n\r\n        # Initialize\r\n        self.verbose = verbose\r\n        self._add_cost_layer()\r\n        self._init_optimizers(lr)\r\n        layer_width = len(self._layers)\r\n\r\n        if train_rate is not None:\r\n            train_rate = float(train_rate)\r\n            train_len = int(len(x) * train_rate)\r\n            shuffle_suffix = np.random.permutation(int(len(x)))\r\n            x, y = x[shuffle_suffix], y[shuffle_suffix]\r\n            x_train, y_train = x[:train_len], y[:train_len]\r\n            x_test, y_test = x[train_len:], y[train_len:]\r\n        else:\r\n            x_train = x_test = x\r\n            y_train = y_test = y\r\n\r\n        train_len = len(x_train)\r\n        batch_size = min(batch_size, train_len)\r\n        do_random_batch = train_len >= batch_size\r\n        train_repeat = int(train_len / batch_size) + 1\r\n\r\n        self._metrics = [""acc""] if metrics is None else metrics\r\n        for i, metric in enumerate(self._metrics):\r\n            if isinstance(metric, str):\r\n                self._metrics[i] = self._available_metrics[metric]\r\n        self._metric_names = [_m.__name__ for _m in self._metrics]\r\n        self._logs = {\r\n            name: [[] for _ in range(len(self._metrics) + 1)] for name in (""train"", ""test"")\r\n        }\r\n\r\n        bar = ProgressBar(max_value=max(1, epoch // record_period), name=""Epoch"", start=False)\r\n        if self.verbose >= NNVerbose.EPOCH:\r\n            bar.start()\r\n\r\n        # Train\r\n        sub_bar = ProgressBar(max_value=train_repeat * record_period - 1, name=""Iteration"", start=False)\r\n        for counter in range(epoch):\r\n            if self.verbose >= NNVerbose.EPOCH and counter % record_period == 0:\r\n                sub_bar.start()\r\n            for _ in range(train_repeat):\r\n                if do_random_batch:\r\n                    batch = np.random.choice(train_len, batch_size)\r\n                    x_batch, y_batch = x_train[batch], y_train[batch]\r\n                else:\r\n                    x_batch, y_batch = x_train, y_train\r\n                self._w_optimizer.update(); self._b_optimizer.update()\r\n                _activations = self._get_activations(x_batch)\r\n                _deltas = [self._layers[-1].bp_first(y_batch, _activations[-1])]\r\n                for i in range(-1, -len(_activations), -1):\r\n                    _deltas.append(\r\n                        self._layers[i - 1].bp(_activations[i - 1], self._weights[i], _deltas[-1])\r\n                    )\r\n                for i in range(layer_width - 2, 0, -1):\r\n                    self._opt(i, _activations[i - 1], _deltas[layer_width - i - 1])\r\n                self._opt(0, x_batch, _deltas[-1])\r\n                if self.verbose >= NNVerbose.EPOCH:\r\n                    if sub_bar.update() and self.verbose >= NNVerbose.METRICS_DETAIL:\r\n                        self._append_log(x_train, y_train, ""train"")\r\n                        self._append_log(x_test, y_test, ""test"")\r\n                        self._print_metric_logs(""train"")\r\n                        self._print_metric_logs(""test"")\r\n            if self.verbose >= NNVerbose.EPOCH:\r\n                sub_bar.update()\r\n            if (counter + 1) % record_period == 0:\r\n                self._append_log(x_train, y_train, ""train"")\r\n                self._append_log(x_test, y_test, ""test"")\r\n                if self.verbose >= NNVerbose.METRICS:\r\n                    self._print_metric_logs(""train"")\r\n                    self._print_metric_logs(""test"")\r\n                if self.verbose >= NNVerbose.EPOCH:\r\n                    bar.update(counter // record_period + 1)\r\n                    sub_bar = ProgressBar(max_value=train_repeat * record_period - 1, name=""Iteration"", start=False)\r\n\r\n    def draw_logs(self):\r\n        metrics_log, loss_log = {}, {}\r\n        for key, value in sorted(self._logs.items()):\r\n            metrics_log[key], loss_log[key] = value[:-1], value[-1]\r\n        for i, name in enumerate(sorted(self._metric_names)):\r\n            plt.figure()\r\n            plt.title(""Metric Type: {}"".format(name))\r\n            for key, log in sorted(metrics_log.items()):\r\n                xs = np.arange(len(log[i])) + 1\r\n                plt.plot(xs, log[i], label=""Data Type: {}"".format(key))\r\n            plt.legend(loc=4)\r\n            plt.show()\r\n            plt.close()\r\n        plt.figure()\r\n        plt.title(""Loss"")\r\n        for key, loss in sorted(loss_log.items()):\r\n            xs = np.arange(len(loss)) + 1\r\n            plt.plot(xs, loss, label=""Data Type: {}"".format(key))\r\n        plt.legend()\r\n        plt.show()\r\n\r\n    @NNTiming.timeit(level=4, prefix=""[API] "")\r\n    def predict(self, x):\r\n        return self._get_prediction(x)\r\n\r\n    @NNTiming.timeit(level=4, prefix=""[API] "")\r\n    def predict_classes(self, x):\r\n        x = np.array(x)\r\n        return np.argmax(self._get_prediction(x), axis=1)\r\n\r\n    @NNTiming.timeit(level=4, prefix=""[API] "")\r\n    def evaluate(self, x, y):\r\n        y_pred = self.predict_classes(x)\r\n        y_arg = np.argmax(y, axis=1)\r\n        print(""Acc: {:8.6}"".format(np.sum(y_arg == y_pred) / len(y_arg)))\r\n\r\n    def visualize_2d(self, x, y, plot_scale=2, plot_precision=0.01):\r\n\r\n        plot_num = int(1 / plot_precision)\r\n\r\n        xf = np.linspace(np.min(x) * plot_scale, np.max(x) * plot_scale, plot_num)\r\n        yf = np.linspace(np.min(x) * plot_scale, np.max(x) * plot_scale, plot_num)\r\n        input_x, input_y = np.meshgrid(xf, yf)\r\n        input_xs = np.c_[input_x.ravel(), input_y.ravel()]\r\n\r\n        output_ys_2d = np.argmax(self.predict(input_xs), axis=1).reshape(len(xf), len(yf))\r\n\r\n        plt.contourf(input_x, input_y, output_ys_2d, cmap=plt.cm.Spectral)\r\n        plt.scatter(x[:, 0], x[:, 1], c=np.argmax(y, axis=1), s=40, cmap=plt.cm.Spectral)\r\n        plt.axis(""off"")\r\n        plt.show()\r\n'"
Zhihu/NN/_extra/two/Test.py,0,"b'from Zhihu.NN._extra.two.Networks import *\r\n\r\nfrom Util.Util import DataUtil\r\n\r\nnp.random.seed(142857)  # for reproducibility\r\n\r\n\r\ndef main():\r\n\r\n    nn = NNDist()\r\n    epoch = 1000\r\n\r\n    timing = Timing(enabled=True)\r\n    timing_level = 1\r\n    nn.feed_timing(timing)\r\n\r\n    x, y = DataUtil.gen_spiral(100)\r\n\r\n    nn.add(ReLU((x.shape[1], 24)))\r\n    nn.add(ReLU((24,)))\r\n    nn.add(Softmax((y.shape[1],)))\r\n\r\n    nn.fit(x, y, epoch=epoch, verbose=2, metrics=[""acc"", ""f1_score""], train_rate=0.8)\r\n    nn.draw_logs()\r\n    nn.visualize_2d(x, y)\r\n\r\n    timing.show_timing_log(timing_level)\r\n\r\nif __name__ == \'__main__\':\r\n    main()\r\n'"
Zhihu/Python/Utils/Dictionary/Dictionary.py,0,"b'import pickle\nimport random\n\n\ndef dic():\n    new_data = yield\n    word, tp, accent, meaning = new_data[""word""], new_data[""type""], new_data[""accent""], new_data[""meaning""]\n    data_path = new_data[""path""]\n    if not word:\n        print(""Please don\'t send an empty word to the dictionary!"")\n    else:\n        with open(data_path + ""dic.dat"", ""rb"") as file:\n            pack = pickle.load(file)\n\n            pack[""dictionary""][(word, tp)] = {\n                ""accent"": accent,\n                ""meaning"": meaning\n            }\n\n            new_data = {\n                ""word"": word,\n                ""type"": tp,\n                ""accent"": accent,\n                ""meaning"": meaning\n            }\n\n            succeed = {\n                ""suc"": True\n            }\n            failed = {\n                ""suc"": False,\n                ""old"": None,\n                ""new"": new_data\n            }\n\n            for i, data in enumerate(pack[""list""]):\n                if data[""word""] == word and data[""type""] == tp:\n                    failed[""old""] = data\n                    respond = yield failed\n                    if respond[""modify""]:\n                        length = len(pack[""list""])\n                        pack[""list""][i] = respond[""data""]\n                        pack[""list""][length - 1], pack[""list""][i] = pack[""list""][i], pack[""list""][length - 1]\n                    break\n            else:\n                yield succeed\n                pack[""list""].append(new_data)\n\n        with open(data_path + ""dic.dat"", ""wb"") as file:\n            pickle.dump(pack, file)\n\n\ndef print_dic(data_path):\n    with open(data_path + ""dic.dat"", ""rb"") as file:\n        pack = pickle.load(file)\n        print(pack[""list""])\n        print(pack[""dictionary""])\n\n\ndef reset_dic(data_path):\n    with open(data_path + ""dic.dat"", ""wb"") as file:\n        pickle.dump({\n            ""dictionary"": {},\n            ""list"": []\n        }, file)\n\n\ndef shuffle_dic(data_path):\n    with open(data_path + ""dic.dat"", ""rb"") as file:\n        pack = pickle.load(file)\n        lst, dictionary = pack[""list""], pack[""dictionary""]\n        random.shuffle(lst)\n    with open(data_path + ""dic.dat"", ""wb"") as file:\n        pickle.dump({\n            ""dictionary"": dictionary,\n            ""list"": lst\n        }, file)\n\n\ndef rotate_list(idx, data_path):\n    with open(data_path + ""dic.dat"", ""rb"") as file:\n        pack = pickle.load(file)\n        lst, dictionary = pack[""list""], pack[""dictionary""]\n        selected = lst.pop(idx)\n        lst.append(selected)\n    with open(data_path + ""dic.dat"", ""wb"") as file:\n        pickle.dump({\n            ""dictionary"": dictionary,\n            ""list"": lst\n        }, file)\n\n\ndef find_word(word, data_path):\n    with open(data_path + ""dic.dat"", ""rb"") as file:\n        dictionary = pickle.load(file)[""dictionary""]\n        data_list = []\n        for key, data in dictionary.items():\n            if word in key[0]:\n                data_list.append({\n                    ""word"": key[0],\n                    ""type"": key[1],\n                    ""accent"": data[""accent""],\n                    ""meaning"": data[""meaning""]\n                })\n        return data_list\n\n\ndef delete_word(word, tp, data_path):\n    if not word:\n        print(""Please delete a valid word!"")\n    else:\n        with open(data_path + ""dic.dat"", ""rb"") as file:\n            pack = pickle.load(file)\n            pack[""dictionary""].pop((word, tp), None)\n            for i, data in enumerate(pack[""list""]):\n                if data[""word""] == word and data[""type""] == tp:\n                    pack[""list""].pop(i)\n                    break\n        with open(data_path + ""dic.dat"", ""wb"") as file:\n            pickle.dump(pack, file)\n'"
Zhihu/Python/Utils/Dictionary/GUI.py,0,"b'import tkinter as tk\nimport tkinter.messagebox as tkm\n\nfrom Zhihu.Python.Utils.Dictionary.Dictionary import *\n\nDISPLAY_RECITE_NUM = 3\nDATA_PATH = ""Data/Song/""\n\n\n# noinspection PyUnusedLocal\nclass GUI(tk.Frame):\n\n    @staticmethod\n    def show_help():\n        help_msg = ""Shortcuts: \\n\\n"" \\\n                   ""    Ctrl + D       : Show help  \\n"" \\\n                   ""    Ctrl + Alt + A : Switch to \'Add word\' \\n"" \\\n                   ""    Ctrl + Alt + S : Switch to \'Check word\' \\n"" \\\n                   ""    Ctrl + Alt + D : Switch to \'Check word box\' \\n\\n""\n        tkm.showinfo(""made by carefree0910"", help_msg)\n\n    @staticmethod\n    def gen_show_hide_button(frame, pack):\n        return tk.Button(frame, width=2, bd=0, command=GUI.show_hide_info(pack), text=""..."")\n\n    def gen_confirm_del_button(self, category, frame, pack):\n        if category == ""delete"":\n            return tk.Button(frame, image=self.del_icon, bd=0, command=self.do_confirm_del(""delete"", pack))\n        else:\n            return tk.Button(frame, image=self.confirm_icon, bd=0, command=self.do_confirm_del(category, pack))\n\n    def gen_recite_word(self):\n        return tk.Label(self.recite_frame, width=18)\n\n    def gen_recite_type(self):\n        return tk.Label(self.recite_frame, width=14)\n\n    def gen_recite_accent(self):\n        return tk.Label(self.recite_frame, width=2)\n\n    def gen_recite_meaning(self):\n        return tk.Label(self.recite_frame, width=24)\n\n    @staticmethod\n    def gen_recite_pack(target):\n        return {\n            ""target"": target,\n            ""message"": """"\n        }\n\n    def gen_recite_controller(self, target):\n        return GUI.gen_show_hide_button(self.recite_frame, target)\n\n    def gen_recite_confirm(self, idx, word, tp, accent, meaning):\n        return self.gen_confirm_del_button(""confirm"", self.recite_frame, {\n            ""idx"": idx,\n            ""word"": word,\n            ""type"": tp,\n            ""accent"": accent,\n            ""meaning"": meaning\n        })\n\n    def gen_recite_del(self, idx, word, tp, accent, meaning):\n        return self.gen_confirm_del_button(""delete"", self.recite_frame, {\n            ""idx"": idx,\n            ""word"": word,\n            ""type"": tp,\n            ""accent"": accent,\n            ""meaning"": meaning\n        })\n\n    def __init__(self):\n\n        # ==============================================================================================================\n        # Initialize Frame\n\n        tk.Frame.__init__(self)\n        self.master.title(""Recite!"")\n        self.master.protocol(""WM_DELETE_WINDOW"", self.close_gui)\n\n        self.wrapper_frame = tk.LabelFrame()\n        self.recite_frame = tk.LabelFrame(self.wrapper_frame, text=""Recite here!"")\n        self.add_frame = tk.LabelFrame(self.wrapper_frame, text=""Add new word here!"")\n\n        self.check_frame = tk.LabelFrame(self.wrapper_frame, text=""Check word here!"")\n        self.check_search_frame = tk.LabelFrame(self.check_frame)\n        self.check_all_frame = tk.LabelFrame(self.check_frame)\n\n        self.wrapper_frame.grid()\n        self.recite_frame.grid(row=0, columnspan=2)\n        self.add_frame.grid(row=1, columnspan=2)\n        self.check_frame.grid(row=2, column=0)\n        self.check_search_frame.grid(row=0, column=0)\n        self.check_all_frame.grid(row=0, column=1, rowspan=2)\n\n        self.recite_num = DISPLAY_RECITE_NUM\n        self.path = DATA_PATH\n\n        # ==============================================================================================================\n        # Initialize Icons\n\n        self.search_icon = tk.PhotoImage(file=""Icons/search.png"")\n        self.confirm_icon = tk.PhotoImage(file=""Icons/confirm.png"")\n        self.del_icon = tk.PhotoImage(file=""Icons/del.png"")\n        self.logo_pic = tk.PhotoImage(file=""Pictures/logo.png"")\n\n        # ==============================================================================================================\n        # Initialize Details\n\n        # Recite Frame\n        self.recite_words = []\n        self.recite_types = []\n\n        self.recite_accents = []\n        self.recite_accent_packs = []\n        self.recite_accent_controllers = []\n\n        self.recite_meanings = []\n        self.recite_meaning_packs = []\n        self.recite_meaning_controllers = []\n\n        self.recite_confirms = []\n        self.recite_deletes = []\n\n        for i in range(self.recite_num):\n            word = self.gen_recite_word()\n            self.recite_words.append(word)\n\n            tp = self.gen_recite_type()\n            self.recite_types.append(tp)\n\n            accent = self.gen_recite_accent()\n            accent_pack = GUI.gen_recite_pack(accent)\n            accent_controller = self.gen_recite_controller(accent_pack)\n\n            self.recite_accents.append(accent)\n            self.recite_accent_packs.append(accent_pack)\n            self.recite_accent_controllers.append(accent_controller)\n\n            meaning = self.gen_recite_meaning()\n            meaning_pack = GUI.gen_recite_pack(meaning)\n            meaning_controller = self.gen_recite_controller(meaning_pack)\n\n            self.recite_meanings.append(meaning)\n            self.recite_meaning_packs.append(meaning_pack)\n            self.recite_meaning_controllers.append(meaning_controller)\n\n            confirm = self.gen_recite_confirm(i, word, tp, accent, meaning)\n            delete = self.gen_recite_del(i, word, tp, accent, meaning)\n\n            self.recite_confirms.append(confirm)\n            self.recite_deletes.append(delete)\n\n            for idx, obj in enumerate([\n                word, tp, accent, accent_controller, meaning, meaning_controller, confirm, delete\n            ]):\n                obj.grid(row=i, column=idx)\n\n        self.refresh_recite()\n\n        # Add Frame\n        self.add_word_label = tk.Label(self.add_frame, text=""Word: "")\n        self.add_word = tk.Entry(self.add_frame, width=18)\n\n        self.add_type_label = tk.Label(self.add_frame, text=""Type: "")\n        self.add_type = tk.Entry(self.add_frame, width=8)\n\n        self.add_accent_label = tk.Label(self.add_frame, text=""Accent: "")\n        self.add_accent = tk.Entry(self.add_frame, width=4)\n\n        self.add_meaning_label = tk.Label(self.add_frame, text=""Meaning: "")\n        self.add_meaning = tk.Entry(self.add_frame, width=18)\n\n        self.add_confirm = self.gen_confirm_del_button(""modify"", self.add_frame, {\n            ""word"": self.add_word,\n            ""type"": self.add_type,\n            ""accent"": self.add_accent,\n            ""meaning"": self.add_meaning\n        })\n\n        for idx, obj in enumerate([\n            self.add_word_label, self.add_word, self.add_type_label, self.add_type,\n            self.add_accent_label, self.add_accent, self.add_meaning_label, self.add_meaning,\n            self.add_confirm\n        ]):\n            obj.grid(row=0, column=idx)\n\n        # Check Frame\n        self.check_word_label = tk.Label(self.check_search_frame, text=""Word: "")\n        self.check_type_label = tk.Label(self.check_search_frame, text=""Type: "")\n        self.check_accent_label = tk.Label(self.check_search_frame, text=""Accent: "")\n        self.check_meaning_label = tk.Label(self.check_search_frame, text=""Meaning: "")\n\n        self.check_word = tk.Entry(self.check_search_frame, width=21)\n        self.check_type = tk.Entry(self.check_search_frame, width=8)\n        self.check_accent = tk.Entry(self.check_search_frame, width=4)\n        self.check_meaning = tk.Entry(self.check_search_frame, width=21)\n\n        self.check_search = tk.Button(self.check_search_frame, image=self.search_icon, bd=0,\n                                      command=self.do_search)\n        self.check_confirm = self.gen_confirm_del_button(""modify"", self.check_search_frame, {\n            ""word"": self.check_word,\n            ""type"": self.check_type,\n            ""accent"": self.check_accent,\n            ""meaning"": self.check_meaning\n        })\n        self.check_del = self.gen_confirm_del_button(""delete"", self.check_search_frame, {\n            ""word"": self.check_word,\n            ""type"": self.check_type,\n            ""accent"": self.check_accent,\n            ""meaning"": self.check_meaning\n        })\n\n        self.check_word_label.grid(row=0, column=0)\n        self.check_word.grid(row=0, column=1, columnspan=3)\n\n        self.check_search.grid(row=0, column=4)\n        self.check_confirm.grid(row=0, column=5)\n        self.check_del.grid(row=0, column=6)\n\n        self.check_type_label.grid(row=1, column=0)\n        self.check_type.grid(row=1, column=1)\n        self.check_accent_label.grid(row=1, column=2)\n        self.check_accent.grid(row=1, column=3)\n\n        self.check_meaning_label.grid(row=2, column=0)\n        self.check_meaning.grid(row=2, column=1, columnspan=3)\n\n        self.check_scroll = tk.Scrollbar(self.check_all_frame, orient=tk.VERTICAL)\n        self.check_scroll.pack(side=""right"", fill=""y"")\n        self.check_box = tk.Listbox(self.check_all_frame, width=39, height=7, yscrollcommand=self.check_scroll.set)\n        self.check_box.pack(side=""left"", fill=""both"")\n        self.check_scroll.config(command=self.check_box.yview)\n\n        self.check_box_list = []\n\n        # Logo\n        self.logo = tk.Button(self.check_frame, image=self.logo_pic, width=160, bd=0, command=self.shuffle)\n        self.logo.grid(row=1, column=0)\n\n        # Binding\n        self.add_word.bind(""<Return>"", GUI.focus_and_select(self.add_type))\n        self.add_type.bind(""<Return>"", GUI.focus_and_select(self.add_accent))\n        self.add_accent.bind(""<Return>"", GUI.focus_and_select(self.add_meaning))\n        self.add_meaning.bind(""<Return>"", self.finish_add())\n\n        self.check_word.bind(""<Return>"", lambda event: self.do_search())\n        self.check_box.bind(""<ButtonRelease-1>"", lambda event: self.check_select())\n        self.check_box.bind(""<KeyRelease>"", lambda event: self.check_select())\n\n        self.master.bind(""<Control-d>"", lambda event: GUI.show_help())\n        self.master.bind(""<Control-Alt-a>"", GUI.focus_and_select(self.add_word))\n        self.master.bind(""<Control-Alt-s>"", GUI.focus_and_select(self.check_word))\n        self.master.bind(""<Control-Alt-d>"", self.select_box())\n\n        # Initialize\n        self.add_word.focus_set()\n\n    def shuffle(self):\n        shuffle_dic(self.path)\n        self.refresh_recite()\n\n    def refresh_recite(self):\n        try:\n            with open(self.path + ""dic.dat"", ""rb"") as file:\n                lst = pickle.load(file)[""list""]\n                for i in range(self.recite_num):\n                    if i < len(lst):\n                        data = lst[i]\n                        self.recite_words[i][""text""] = data[""word""]\n                        self.recite_types[i][""text""] = data[""type""]\n                        self.recite_accent_packs[i][""message""] = data[""accent""]\n                        self.recite_meaning_packs[i][""message""] = data[""meaning""]\n                    else:\n                        self.recite_words[i][""text""] = """"\n                        self.recite_types[i][""text""] = """"\n                        self.recite_accent_packs[i][""message""] = """"\n                        self.recite_meaning_packs[i][""message""] = """"\n\n                    self.recite_accents[i][""text""] = """"\n                    self.recite_meanings[i][""text""] = """"\n        except FileNotFoundError:\n            reset_dic(self.path)\n\n    def clear_check(self):\n        self.check_box_list = []\n        self.check_word.delete(0, tk.END)\n        self.check_type.delete(0, tk.END)\n        self.check_accent.delete(0, tk.END)\n        self.check_meaning.delete(0, tk.END)\n        self.check_box.delete(0, tk.END)\n\n    @staticmethod\n    def show_hide_info(pack):\n        def sub():\n            if pack[""message""]:\n                pack[""target""][""text""] = pack[""message""]\n                pack[""target""].grid()\n                pack[""message""] = """"\n            else:\n                pack[""message""] = pack[""target""][""text""]\n                pack[""target""][""text""] = """"\n        return sub\n\n    def do_confirm_del(self, category, pack):\n        def sub():\n            new_pack = {key: data[""text""] if isinstance(data, tk.Label) else data.get()\n                        for key, data in pack.items() if key != ""idx""}\n            new_pack[""path""] = self.path\n            if category == ""confirm"":\n                rotate_list(pack[""idx""], self.path)\n            elif category == ""modify"":\n                try:\n                    dictionary = dic()\n                    dictionary.__next__()\n                    respond = dictionary.send(new_pack)\n                    if respond[""suc""]:\n                        dictionary.send(None)\n                    else:\n                        old, new = respond[""old""], respond[""new""]\n                        msg = ""Word:\\n    \'{} [{}] {} {}\'\\nalready exist, replace it?"".format(\n                            old[""word""], old[""type""], old[""accent""], old[""meaning""]\n                        )\n                        modify = {\n                            ""modify"": True,\n                            ""data"": new\n                        }\n                        cancel = {\n                            ""modify"": False\n                        }\n                        result = tkm.askquestion(""made by carefree0910"", msg)\n                        if result == ""yes"":\n                            dictionary.send(modify)\n                        else:\n                            dictionary.send(cancel)\n                except StopIteration:\n                    pass\n                finally:\n                    pass\n            else:\n                delete_word(new_pack[""word""], new_pack[""type""], self.path)\n            self.refresh_recite()\n            self.clear_check()\n        return sub\n\n    def finish_add(self):\n        def sub(event=""""):\n            self.do_confirm_del(""modify"", {\n                ""word"": self.add_word,\n                ""type"": self.add_type,\n                ""accent"": self.add_accent,\n                ""meaning"": self.add_meaning\n            })()\n            GUI.focus_and_select(self.add_word)()\n        return sub\n\n    def do_search(self):\n        data_list = find_word(self.check_word.get(), self.path)\n        self.clear_check()\n        if not data_list:\n            self.check_meaning.insert(0, ""Not Found..."")\n        else:\n            self.check_word.insert(0, data_list[0][""word""])\n            self.check_type.insert(0, data_list[0][""type""])\n            self.check_accent.insert(0, data_list[0][""accent""])\n            self.check_meaning.insert(0, data_list[0][""meaning""])\n            for i, data in enumerate(data_list):\n                self.check_box_list.append(data)\n                self.check_box.insert(i, data[""word""] + "" [{}]"".format(data[""type""]))\n\n    def select_box(self):\n        def sub(event=""""):\n            if not self.check_box_list:\n                pass\n            else:\n                self.check_box.selection_set(0)\n                self.check_box.focus_set()\n        return sub\n\n    @staticmethod\n    def focus_and_select(entry):\n        def sub(event=""""):\n            if entry.get():\n                entry.select_from(0)\n                entry.select_to(tk.END)\n            entry.focus_set()\n        return sub\n\n    def check_select(self):\n        cursor = self.check_box.curselection()\n        self.check_word.delete(0, tk.END)\n        self.check_type.delete(0, tk.END)\n        self.check_accent.delete(0, tk.END)\n        self.check_meaning.delete(0, tk.END)\n        if not cursor or not self.check_box_list[cursor[0]]:\n            pass\n        else:\n            data = self.check_box_list[cursor[0]]\n            self.check_word.insert(0, data[""word""])\n            self.check_type.insert(0, data[""type""])\n            self.check_accent.insert(0, data[""accent""])\n            self.check_meaning.insert(0, data[""meaning""])\n\n    def close_gui(self):\n        result = tkm.askquestion(""made by carefree0910"", ""Backup?"")\n        if result == ""yes"":\n            with open(self.path + ""backup.dat"", ""wb"") as file:\n                with open(self.path + ""dic.dat"", ""rb"") as f:\n                    pickle.dump(pickle.load(f), file)\n        self.master.destroy()\n'"
Zhihu/Python/Utils/Dictionary/Main.py,0,b'from GUI import *\n\n\nGUI().mainloop()\n'
_Dist/NeuralNetworks/_Tests/AdvancedNN/Test.py,0,"b'import os\nimport sys\nroot_path = os.path.abspath(""../../../../"")\nif root_path not in sys.path:\n    sys.path.append(root_path)\n\nfrom Util.Util import DataUtil\nfrom _Dist.NeuralNetworks.c_BasicNN.NN import Basic\nfrom _Dist.NeuralNetworks.e_AdvancedNN.NN import Advanced\nfrom _Dist.NeuralNetworks._Tests.TestUtil import draw_acc\n\nx_cv = y_cv = None\n(x, y), (x_test, y_test) = DataUtil.gen_noisy_linear(one_hot=False)\n\n\ndef block_test(generator, **kwargs):\n    (x_, y_), (x_test_, y_test_) = generator(**kwargs, one_hot=False)\n\n    basic_ = Basic(**base_params).fit(x_, y_, x_test_, y_test_, snapshot_ratio=0)\n    advanced_params[""model_structure_settings""][""use_pruner""] = False\n    wnd_dndf_ = Advanced(**advanced_params).fit(x_, y_, x_test_, y_test_, snapshot_ratio=0)\n    advanced_params[""model_structure_settings""][""use_pruner""] = True\n    wnd_dndf_pruned_ = Advanced(**advanced_params).fit(x_, y_, x_test_, y_test_, snapshot_ratio=0)\n\n    print(""BasicNN              "", end="""")\n    basic_.evaluate(x_, y_, x_cv, y_cv, x_test_, y_test_)\n    print(""WnD & DNDF           "", end="""")\n    wnd_dndf_.evaluate(x_, y_, x_cv, y_cv, x_test_, y_test_)\n    print(""WnD & DNDF & Pruner  "", end="""")\n    wnd_dndf_pruned_.evaluate(x_, y_, x_cv, y_cv, x_test_, y_test_)\n\n    return basic_, wnd_dndf_, wnd_dndf_pruned_\n\n\ndef block_test_pruner(generator, **kwargs):\n    (x_, y_), (x_test_, y_test_) = generator(**kwargs, one_hot=False)\n\n    advanced_params[""model_structure_settings""][""use_pruner""] = True\n    advanced_params[""model_structure_settings""][""pruner_params""] = {""prune_method"": ""soft_prune""}\n    soft_ = Advanced(**advanced_params).fit(x_, y_, x_test_, y_test_, snapshot_ratio=0)\n\n    advanced_params[""model_structure_settings""][""pruner_params""] = {\n        ""prune_method"": ""hard_prune"",\n        ""alpha"": 1e-8,\n        ""beta"": 1e12\n    }\n    hard_ = Advanced(**advanced_params).fit(x_, y_, x_test_, y_test_, snapshot_ratio=0)\n\n    advanced_params[""model_structure_settings""][""pruner_params""] = {""prune_method"": ""surgery""}\n    surgery_ = Advanced(**advanced_params).fit(x_, y_, x_test_, y_test_, snapshot_ratio=0)\n\n    print(""Soft     "", end="""")\n    soft_.evaluate(x_, y_, x_cv, y_cv, x_test_, y_test_)\n    print(""Hard     "", end="""")\n    hard_.evaluate(x_, y_, x_cv, y_cv, x_test_, y_test_)\n    print(""Surgery  "", end="""")\n    surgery_.evaluate(x_, y_, x_cv, y_cv, x_test_, y_test_)\n\n    return soft_, hard_, surgery_\n\n\nbase_params = {\n    ""model_param_settings"": {""n_epoch"": 40, ""metric"": ""acc""},\n    ""model_structure_settings"": {""hidden_units"": [500, 500]}\n}\nbasic = Basic(**base_params).fit(x, y, x_test, y_test, snapshot_ratio=0)\n\nnumerical_idx = [True] * 100 + [False]\ncategorical_columns = []\nadvanced_params = {""data_info"": {\n    ""numerical_idx"": numerical_idx, ""categorical_columns"": categorical_columns\n}}\nadvanced_params.update(base_params)\nadvanced_params[""model_structure_settings""][""use_dndf""] = False\nadvanced_params[""model_structure_settings""][""use_pruner""] = False\n\nwnd = Advanced(**advanced_params).fit(x, y, x_test, y_test, snapshot_ratio=0)\n\nadvanced_params[""model_structure_settings""][""use_dndf""] = True\nwnd_dndf = Advanced(**advanced_params).fit(x, y, x_test, y_test, snapshot_ratio=0)\n\nadvanced_params[""model_structure_settings""][""use_pruner""] = True\nwnd_dndf_pruned = Advanced(**advanced_params).fit(x, y, x_test, y_test, snapshot_ratio=0)\n\nprint(""BasicNN              "", end="""")\nbasic.evaluate(x, y, x_cv, y_cv, x_test, y_test)\nprint(""WnD                  "", end="""")\nwnd.evaluate(x, y, x_cv, y_cv, x_test, y_test)\nprint(""WnD & DNDF           "", end="""")\nwnd_dndf.evaluate(x, y, x_cv, y_cv, x_test, y_test)\nprint(""WnD & DNDF & Pruner  "", end="""")\nwnd_dndf_pruned.evaluate(x, y, x_cv, y_cv, x_test, y_test)\ndraw_acc(basic, wnd_dndf, wnd_dndf_pruned)\n\nadvanced_params[""data_info""][""numerical_idx""] = [True] * 500 + [False]\nadvanced_params[""model_structure_settings""][""pruner_params""] = {""prune_method"": ""hard_prune""}\nbasic, wnd_dndf, wnd_dndf_pruned = block_test(DataUtil.gen_noisy_linear, n_dim=500)\ndraw_acc(basic, wnd_dndf, wnd_dndf_pruned, ylim=(0.8, 0.95), draw_train=False)\n\nbasic, wnd_dndf, wnd_dndf_pruned = block_test(DataUtil.gen_noisy_linear, n_dim=500, noise_scale=1.)\ndraw_acc(basic, wnd_dndf, wnd_dndf_pruned, ylim=(0.7, 0.9), draw_train=False)\n\nadvanced_params[""data_info""][""numerical_idx""] = [True] * 100 + [False]\nbasic, wnd_dndf, wnd_dndf_pruned = block_test(DataUtil.gen_noisy_linear, n_valid=100, noise_scale=0.)\ndraw_acc(basic, wnd_dndf, wnd_dndf_pruned, ylim=(0.95, 1.), draw_train=False)\n\nadvanced_params[""model_structure_settings""][""pruner_params""] = {\n    ""prune_method"": ""soft_prune"",\n    ""alpha"": 0.01\n}\nbasic, wnd_dndf, wnd_dndf_pruned = block_test(DataUtil.gen_noisy_linear, n_valid=100, noise_scale=0.)\ndraw_acc(basic, wnd_dndf, wnd_dndf_pruned, ylim=(0.95, 1.), draw_train=False)\n\nfor i, p in enumerate([3, 5, 8, 12]):\n    basic, wnd_dndf, wnd_dndf_pruned = block_test(DataUtil.gen_noisy_poly, p=p)\n    draw_acc(basic, wnd_dndf, wnd_dndf_pruned, ylim=(0.7-i*0.05, 0.9-i*0.05), draw_train=False)\n'"
_Dist/NeuralNetworks/_Tests/AutoNN/Test.py,0,"b'import os\nimport sys\nroot_path = os.path.abspath(""../../../../"")\nif root_path not in sys.path:\n    sys.path.append(root_path)\n\nfrom _Dist.NeuralNetworks.f_AutoNN.NN import AutoAdvanced\n\n\nAutoAdvanced(\n    ""Adult"",\n    model_param_settings={""n_epoch"": 3},\n    data_info={""file_type"": ""csv"", ""data_folder"": ""../_Data""}\n).fit(snapshot_ratio=0).fit(snapshot_ratio=0).save()\nAutoAdvanced(""Adult"").load().fit(snapshot_ratio=0)\n'"
_Dist/NeuralNetworks/_Tests/DistNN/Test.py,0,"b'import os\nimport sys\nroot_path = os.path.abspath(""../../../../"")\nif root_path not in sys.path:\n    sys.path.append(root_path)\n\nfrom _Dist.NeuralNetworks.g_DistNN.NN import DistAdvanced\n\n\nbase_params = {\n    ""data_info"": {\n        ""data_folder"": ""../_Data"",\n        ""file_type"": ""csv""\n    },\n    ""model_param_settings"": {\n        ""metric"": ""acc"",\n        ""n_epoch"": 1,\n        ""max_epoch"": 2\n    }\n}\n\ngrid_params = {\n    ""model_param_settings"": {\n        ""lr"": [1e-2, 1e-3],\n        ""loss"": [""mse"", ""cross_entropy""]\n    },\n    ""model_structure_settings"": {\n        ""use_pruner"": [False, True],\n        ""use_wide_network"": [False, True],\n        ""hidden_units"": [[128, 128], [256, 256]]\n    },\n}\ndist = DistAdvanced(""Adult"", **base_params).grid_search(grid_params, ""dict_first"")\n'"
_Dist/NeuralNetworks/_Tests/DistNN/TestOpenML.py,1,"b'import os\nimport sys\nroot_path = os.path.abspath(""../../../../"")\nif root_path not in sys.path:\n    sys.path.append(root_path)\n\nimport copy\nimport numpy as np\nimport tensorflow as tf\n\nfrom openml import datasets\n\nfrom _Dist.NeuralNetworks.g_DistNN.NN import DistAdvanced\n\nGPU_ID = ""0""\nK_RANDOM = 9\nIDS = [\n    38, 46, 179,\n    184, 389, 554,\n    772, 917, 1049,\n    1111, 1120, 1128,\n    293,\n]\n\n\ndef download_data():\n    data_folder = ""_Data""\n    idx_folder = os.path.join(data_folder, ""idx"")\n    if not os.path.isdir(data_folder):\n        os.makedirs(data_folder)\n    if not os.path.isdir(idx_folder):\n        os.makedirs(idx_folder)\n    for idx in IDS:\n        print(""Downloading {}"".format(idx))\n        data_file = os.path.join(data_folder, ""{}.txt"".format(idx))\n        idx_file = os.path.join(idx_folder, ""{}.npy"".format(idx))\n        if os.path.isfile(data_file) and os.path.isfile(idx_file):\n            continue\n        dataset = datasets.get_dataset(idx)\n        x, y, categorical_idx, names = dataset.get_data(\n            target=dataset.default_target_attribute, dataset_format=""array"")\n        categorical_idx.append(True)\n        to_array = lambda arr: arr.toarray() if not isinstance(arr, np.ndarray) else arr\n        data = np.hstack(list(map(to_array, [x, y.reshape([-1, 1])])))\n        numerical_idx = ~np.array(categorical_idx)\n        with open(data_file, ""w"") as file:\n            file.write(""\\n"".join(["" "".join(map(lambda n: str(n), line)) for line in data]))\n        np.save(idx_file, numerical_idx)\n\n\ndef main():\n    base_params = {\n        ""data_info"": {},\n        ""model_param_settings"": {}\n    }\n    config = tf.ConfigProto()\n    config.gpu_options.allow_growth = True\n    if GPU_ID is not None:\n        os.environ[""CUDA_VISIBLE_DEVICES""] = GPU_ID\n    base_params[""model_param_settings""][""sess_config""] = config\n    for idx in IDS:\n        numerical_idx = np.load(""_Data/idx/{}.npy"".format(idx))\n        local_params = copy.deepcopy(base_params)\n        local_params[""name""] = str(idx)\n        local_params[""data_info""][""numerical_idx""] = numerical_idx\n        DistAdvanced(**local_params).empirical_search(cv_rate=0.1, test_rate=0.1).k_random(\n            K_RANDOM, cv_rate=0.1, test_rate=0.1)\n\n\nif __name__ == \'__main__\':\n    download_data()\n    main()\n'"
_Dist/NeuralNetworks/_Tests/Madelon/MadelonUtil.py,0,"b'import os\nimport sys\nroot_path = os.path.abspath(""../../../../"")\nif root_path not in sys.path:\n    sys.path.append(root_path)\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nplt.switch_backend(""Qt5Agg"")\n\nfrom _Dist.NeuralNetworks.NNUtil import Toolbox\n\n\ndef get_madelon():\n    with open(os.path.join(root_path, ""_Data"", ""madelon.txt""), ""r"") as file:\n        data = np.array(Toolbox.get_data(file), np.float32)\n    np.random.shuffle(data)\n    train_set, test_set = data[:2000], data[2000:]\n    x, y = train_set[..., :-1], train_set[..., -1]\n    x_test, y_test = test_set[..., :-1], test_set[..., -1]\n    return x, y, x_test, y_test\n'"
_Dist/NeuralNetworks/_Tests/Madelon/TestAdvancedNN.py,0,"b'import os\nimport sys\nroot_path = os.path.abspath(""../../../../"")\nif root_path not in sys.path:\n    sys.path.append(root_path)\n\nfrom _Dist.NeuralNetworks.c_BasicNN.NN import Basic\nfrom _Dist.NeuralNetworks.e_AdvancedNN.NN import Advanced\nfrom _Dist.NeuralNetworks._Tests.TestUtil import draw_acc\nfrom _Dist.NeuralNetworks._Tests.Madelon.MadelonUtil import get_madelon\n\nx, y, x_test, y_test = get_madelon()\n\n\ndef normalize(arr):\n    arr = arr.copy()\n    arr -= arr.mean(0)\n    arr /= arr.std(0)\n    return arr\n\n\ndef block_evaluate():\n    advanced_nn = Advanced(**advanced_params).fit(x, y, x_test, y_test, snapshot_ratio=0)\n    print(""BasicNN              "", end="""")\n    basic.evaluate(x, y, None, None, x_test, y_test)\n    print(""AdvancedNN           "", end="""")\n    advanced_nn.evaluate(x, y, None, None, x_test, y_test)\n    return advanced_nn\n\n\nx, x_test = normalize(x), normalize(x_test)\n\nylim = (0.5, 1.05)\nbase_params = {\n    ""model_param_settings"": {""n_epoch"": 200, ""metric"": ""acc""},\n    ""model_structure_settings"": {""hidden_units"": [152, 153]}\n}\nbasic = Basic(**base_params).fit(x, y, x_test, y_test, snapshot_ratio=0)\n\nadvanced_params = {""data_info"": {\n    ""numerical_idx"": [True] * 500 + [False], ""categorical_columns"": []\n}}\nadvanced_params.update(base_params)\nadvanced_params[""model_param_settings""][""keep_prob""] = 0.5\nadvanced_params[""model_param_settings""][""use_batch_norm""] = False\nadvanced_params[""model_structure_settings""][""use_pruner""] = False\nadvanced_params[""model_structure_settings""][""use_wide_network""] = False\nadvanced = block_evaluate()\ndraw_acc(basic, advanced, ylim=ylim)\n\nadvanced_params[""model_param_settings""][""keep_prob""] = 0.25\nadvanced = block_evaluate()\ndraw_acc(basic, advanced, ylim=ylim)\n\nadvanced_params[""model_param_settings""][""keep_prob""] = 0.1\nadvanced_params[""model_param_settings""][""n_epoch""] = 600\nadvanced = block_evaluate()\ndraw_acc(basic, advanced, ylim=ylim)\n\nadvanced_params[""model_param_settings""][""n_epoch""] = 200\nadvanced_params[""model_param_settings""][""keep_prob""] = 0.25\nadvanced_params[""model_param_settings""][""use_batch_norm""] = True\nadvanced = block_evaluate()\ndraw_acc(basic, advanced, ylim=ylim)\n\nadvanced_params[""model_structure_settings""][""use_pruner""] = True\nadvanced_params[""model_structure_settings""][""use_wide_network""] = True\nadvanced = block_evaluate()\ndraw_acc(basic, advanced, ylim=ylim)\n'"
_Dist/NeuralNetworks/_Tests/Madelon/TestAutoNN.py,0,"b'import os\nimport sys\nroot_path = os.path.abspath(""../../../../"")\nif root_path not in sys.path:\n    sys.path.append(root_path)\n\nfrom _Dist.NeuralNetworks._Tests.TestUtil import draw_acc\nfrom _Dist.NeuralNetworks.f_AutoNN.NN import AutoBasic, AutoAdvanced\nfrom _Dist.NeuralNetworks._Tests.Madelon.MadelonUtil import get_madelon\n\nx, y, x_test, y_test = get_madelon()\n\nbase_params = {\n    ""name"": ""Madelon"",\n    ""model_param_settings"": {""n_epoch"": 200, ""metric"": ""acc""},\n    ""model_structure_settings"": {""hidden_units"": [152, 153]}\n}\nbasic = AutoBasic(**base_params).fit(x, y, x_test, y_test, snapshot_ratio=0)\nadvanced = AutoAdvanced(**base_params).fit(x, y, x_test, y_test, snapshot_ratio=0)\ndraw_acc(basic, advanced, ylim=(0.5, 1.05))\n'"
_Dist/NeuralNetworks/_Tests/Madelon/TestDT2NN.py,0,"b'import os\nimport sys\nroot_path = os.path.abspath(""../../../../"")\nif root_path not in sys.path:\n    sys.path.append(root_path)\n\nfrom _Dist.NeuralNetworks.c_BasicNN.NN import Basic\nfrom _Dist.NeuralNetworks.d_Traditional2NN.Toolbox import DT2NN\nfrom _Dist.NeuralNetworks._Tests.Madelon.MadelonUtil import get_madelon\n\nx, y, x_test, y_test = get_madelon()\n\nbasic = Basic(model_structure_settings={""hidden_units"": [152, 153]}).fit(x, y, x_test, y_test)\ndt2dnn = DT2NN(model_param_settings={""activations"": [""sign"", ""softmax""]}).fit(x, y, x_test, y_test)\n'"
_Dist/NeuralNetworks/_Tests/Pruner/Advanced.py,23,"b'import os\nimport sys\nroot_path = os.path.abspath(""../../../../"")\nif root_path not in sys.path:\n    sys.path.append(root_path)\n\nimport math\nimport numpy as np\nimport tensorflow as tf\n\nfrom _Dist.NeuralNetworks.NNUtil import *\nfrom _Dist.NeuralNetworks._Tests.Pruner.Basic import Basic\n\n\nclass Advanced(Basic):\n    def __init__(self, name=None, data_info=None, model_param_settings=None, model_structure_settings=None):\n        super(Advanced, self).__init__(name, model_param_settings, model_structure_settings)\n        self._name_appendix = ""Advanced""\n        self.tf_list_collections = None\n\n        if data_info is None:\n            self.data_info = {}\n        else:\n            assert_msg = ""data_info should be a dictionary""\n            assert isinstance(data_info, dict), assert_msg\n            self.data_info = data_info\n        self._data_info_initialized = False\n        self.numerical_idx = self.categorical_columns = None\n\n        self._deep_input = self._wide_input = None\n        self._categorical_xs = None\n        self.embedding_size = None\n        self._embedding = self._one_hot = self._embedding_concat = self._one_hot_concat = None\n        self._embedding_with_one_hot = self._embedding_with_one_hot_concat = None\n\n        self.dropout_keep_prob = self.use_batch_norm = None\n        self._use_wide_network = self._dndf = self._pruner = self._dndf_pruner = None\n\n        self._tf_p_keep = None\n        self._n_batch_placeholder = None\n\n    def init_data_info(self):\n        if self._data_info_initialized:\n            return\n        self._data_info_initialized = True\n        self.numerical_idx = self.data_info.get(""numerical_idx"", None)\n        self.categorical_columns = self.data_info.get(""categorical_columns"", None)\n        if self.numerical_idx is None:\n            raise ValueError(""numerical_idx should be provided"")\n        if self.categorical_columns is None:\n            raise ValueError(""categorical_columns should be provided"")\n\n    def init_from_data(self, x, y, x_test, y_test, sample_weights, names):\n        self.init_data_info()\n        super(Advanced, self).init_from_data(x, y, x_test, y_test, sample_weights, names)\n        if len(self.numerical_idx) != self.n_dim + 1:\n            raise ValueError(""Length of numerical_idx should be {}, {} found"".format(\n                self.n_dim + 1, len(self.numerical_idx)\n            ))\n        self.n_dim -= len(self.categorical_columns)\n\n    def init_model_param_settings(self):\n        super(Advanced, self).init_model_param_settings()\n        self.dropout_keep_prob = float(self.model_param_settings.get(""keep_prob"", 0.5))\n        self.use_batch_norm = self.model_param_settings.get(""use_batch_norm"", True)\n\n    def init_model_structure_settings(self):\n        self.hidden_units = self.model_structure_settings.get(""hidden_units"", None)\n        self._deep_input = self.model_structure_settings.get(""deep_input"", ""embedding_concat"")\n        self._wide_input = self.model_structure_settings.get(""wide_input"", ""continuous"")\n        self.embedding_size = self.model_structure_settings.get(""embedding_size"", 8)\n\n        self._use_wide_network = self.model_structure_settings.get(""use_wide_network"", self.n_dim > 0)\n        if not self._use_wide_network:\n            self._dndf = None\n        else:\n            dndf_params = self.model_structure_settings.get(""dndf_params"", {})\n            if self.model_structure_settings.get(""use_dndf"", True):\n                self._dndf = DNDF(self.n_class, **dndf_params)\n        if self.model_structure_settings.get(""use_pruner"", True):\n            pruner_params = self.model_structure_settings.get(""pruner_params"", {})\n            self._pruner = Pruner(**pruner_params)\n        if self.model_structure_settings.get(""use_dndf_pruner"", True):\n            dndf_pruner_params = self.model_structure_settings.get(""dndf_pruner_params"", {})\n            self._dndf_pruner = Pruner(**dndf_pruner_params)\n\n    def _get_embedding(self, i, n):\n        embedding_size = math.ceil(math.log2(n)) + 1 if self.embedding_size == ""log"" else self.embedding_size\n        embedding = tf.Variable(tf.truncated_normal(\n            [n, embedding_size], mean=0, stddev=0.02\n        ), name=""Embedding{}"".format(i))\n        return tf.nn.embedding_lookup(embedding, self._categorical_xs[i], name=""Embedded_X{}"".format(i))\n\n    def _define_hidden_units(self):\n        n_data = len(self._train_generator)\n        current_units = self._deep_input.shape[1].value\n        if current_units > 512:\n            if n_data >= 100000:\n                self.hidden_units = [1024, 1024, 512]\n            elif n_data >= 10000:\n                self.hidden_units = [1024, 1024]\n            elif n_data >= 5000:\n                self.hidden_units = [512, 512]\n            else:\n                self.hidden_units = [256, 256]\n        elif current_units > 256:\n            if n_data >= 100000:\n                self.hidden_units = [2 * current_units, 2 * current_units, 512]\n            elif n_data >= 10000:\n                self.hidden_units = [2 * current_units, 2 * current_units]\n            elif n_data >= 5000:\n                self.hidden_units = [512, 512]\n            else:\n                self.hidden_units = [256, 256]\n        else:\n            if n_data >= 100000:\n                self.hidden_units = [512, 512, 512]\n            elif n_data >= 10000:\n                self.hidden_units = [512, 512]\n            else:\n                if current_units > 128:\n                    if n_data >= 5000:\n                        self.hidden_units = [2 * current_units, 2 * current_units]\n                    else:\n                        self.hidden_units = [current_units, current_units]\n                else:\n                    if n_data >= 5000:\n                        self.hidden_units = [256, 256]\n                    else:\n                        self.hidden_units = [128, 128]\n\n    def _fully_connected_linear(self, net, shape, appendix):\n        with tf.name_scope(""Linear{}"".format(appendix)):\n            w = init_w(shape, ""W{}"".format(appendix))\n            if self._pruner is not None:\n                w = self._pruner.prune_w(*self._pruner.get_w_info(w))\n            b = init_b([shape[1]], ""b{}"".format(appendix))\n            self._ws.append(w)\n            self._bs.append(b)\n            return tf.add(tf.matmul(net, w), b, name=""Linear{}_Output"".format(appendix))\n\n    def _build_layer(self, i, net):\n        if self.use_batch_norm:\n            net = tf.layers.batch_normalization(net, training=self._is_training, name=""BN{}"".format(i))\n        activation = self.activations[i]\n        if activation is not None:\n            net = getattr(Activations, activation)(net, ""{}{}"".format(activation, i))\n        if self.dropout_keep_prob < 1:\n            net = tf.nn.dropout(net, keep_prob=self._tf_p_keep)\n        return net\n\n    def _build_model(self, net=None):\n        super(Advanced, self)._build_model(self._deep_input)\n        if self._use_wide_network:\n            if self._dndf is None:\n                wide_output = self._fully_connected_linear(\n                    self._wide_input, appendix=""_wide_output"",\n                    shape=[self._wide_input.shape[1].value, self.n_class]\n                )\n            else:\n                wide_output = self._dndf(\n                    self._wide_input, self._n_batch_placeholder,\n                    pruner=self._dndf_pruner\n                )\n            self._output += wide_output\n\n    def _get_feed_dict(self, x, y=None, weights=None, is_training=True):\n        continuous_x = x[..., self.numerical_idx[:-1]] if self._categorical_xs else x\n        feed_dict = super(Advanced, self)._get_feed_dict(continuous_x, y, weights, is_training)\n        if self._dndf is not None:\n            feed_dict[self._n_batch_placeholder] = len(x)\n        if self._pruner is not None:\n            cond_placeholder = self._pruner.cond_placeholder\n            if cond_placeholder is not None:\n                feed_dict[cond_placeholder] = True\n        if self._dndf is not None and self._dndf_pruner is not None:\n            cond_placeholder = self._dndf_pruner.cond_placeholder\n            if cond_placeholder is not None:\n                feed_dict[cond_placeholder] = True\n        for (idx, _), categorical_x in zip(self.categorical_columns, self._categorical_xs):\n            feed_dict.update({categorical_x: x[..., idx].astype(np.int32)})\n        return feed_dict\n\n    def _define_input_and_placeholder(self):\n        super(Advanced, self)._define_input_and_placeholder()\n        if not self.categorical_columns:\n            self._categorical_xs = []\n            self._one_hot = self._one_hot_concat = self._tfx\n            self._embedding = self._embedding_concat = self._tfx\n            self._embedding_with_one_hot = self._embedding_with_one_hot_concat = self._tfx\n        else:\n            all_categorical = self.n_dim == 0\n            with tf.name_scope(""Categorical_Xs""):\n                self._categorical_xs = [\n                    tf.placeholder(tf.int32, shape=[None], name=""Categorical_X{}"".format(i))\n                    for i in range(len(self.categorical_columns))\n                ]\n            with tf.name_scope(""One_hot""):\n                one_hot_vars = [\n                    tf.one_hot(self._categorical_xs[i], n)\n                    for i, (_, n) in enumerate(self.categorical_columns)\n                ]\n                self._one_hot = self._one_hot_concat = tf.concat(one_hot_vars, 1, name=""Raw"")\n                if not all_categorical:\n                    self._one_hot_concat = tf.concat([self._tfx, self._one_hot], 1, name=""Concat"")\n            with tf.name_scope(""Embedding""):\n                embeddings = [\n                    self._get_embedding(i, n)\n                    for i, (_, n) in enumerate(self.categorical_columns)\n                ]\n                self._embedding = self._embedding_concat = tf.concat(embeddings, 1, name=""Raw"")\n                if not all_categorical:\n                    self._embedding_concat = tf.concat([self._tfx, self._embedding], 1, name=""Concat"")\n            with tf.name_scope(""Embedding_with_one_hot""):\n                self._embedding_with_one_hot = self._embedding_with_one_hot_concat = tf.concat(\n                    embeddings + one_hot_vars, 1, name=""Raw""\n                )\n                if not all_categorical:\n                    self._embedding_with_one_hot_concat = tf.concat(\n                        [self._tfx, self._embedding_with_one_hot], 1, name=""Concat""\n                    )\n        if self._wide_input == ""continuous"":\n            self._wide_input = self._tfx\n        else:\n            self._wide_input = getattr(self, ""_"" + self._wide_input)\n        if self._deep_input == ""continuous"":\n            self._deep_input = self._tfx\n        else:\n            self._deep_input = getattr(self, ""_"" + self._deep_input)\n        if self.hidden_units is None:\n            self._define_hidden_units()\n        self._tf_p_keep = tf.cond(\n            self._is_training, lambda: self.dropout_keep_prob, lambda: 1.,\n            name=""keep_prob""\n        )\n        self._n_batch_placeholder = tf.placeholder(tf.int32, name=""n_batch"")\n\n    def _define_py_collections(self):\n        super(Advanced, self)._define_py_collections()\n        self.py_collections.append(""data_info"")\n\n    def _define_tf_collections(self):\n        super(Advanced, self)._define_tf_collections()\n        self.tf_collections += [\n            ""_deep_input"", ""_wide_input"", ""_n_batch_placeholder"",\n            ""_embedding"", ""_one_hot"", ""_embedding_with_one_hot"",\n            ""_embedding_concat"", ""_one_hot_concat"", ""_embedding_with_one_hot_concat""\n        ]\n        self.tf_list_collections = [""_categorical_xs""]\n\n    def add_tf_collections(self):\n        super(Advanced, self).add_tf_collections()\n        for tf_list in self.tf_list_collections:\n            target_list = getattr(self, tf_list)\n            if target_list is None:\n                continue\n            for tensor in target_list:\n                tf.add_to_collection(tf_list, tensor)\n\n    def restore_collections(self, folder):\n        for tf_list in self.tf_list_collections:\n            if tf_list is not None:\n                setattr(self, tf_list, tf.get_collection(tf_list))\n        super(Advanced, self).restore_collections(folder)\n\n    def clear_tf_collections(self):\n        super(Advanced, self).clear_tf_collections()\n        for key in self.tf_list_collections:\n            tf.get_collection_ref(key).clear()\n\n    def print_settings(self):\n        msg = ""\\n"".join([\n            ""="" * 60, ""This is a {}"".format(\n                ""{}-classes problem"".format(self.n_class) if not self.n_class == 1\n                else ""regression problem""\n            ), ""-"" * 60,\n            ""Data     : {} training samples, {} test samples"".format(\n                len(self._train_generator), len(self._test_generator) if self._test_generator is not None else 0\n            ),\n            ""Features : {} categorical, {} numerical"".format(\n                len(self.categorical_columns), np.sum(self.numerical_idx)\n            )\n        ]) + ""\\n""\n\n        msg += ""="" * 60 + ""\\n""\n        msg += ""Deep model: DNN\\n""\n        msg += ""Deep model input: {}\\n"".format(\n            ""Continuous features only"" if not self.categorical_columns else\n            ""Continuous features with embeddings"" if np.any(self.numerical_idx) else\n            ""Embeddings only""\n        )\n        msg += ""-"" * 60 + ""\\n""\n        if self.categorical_columns:\n            msg += ""Embedding size: {}\\n"".format(self.embedding_size)\n            msg += ""Actual feature dimension: {}\\n"".format(self._embedding_concat.shape[1].value)\n        msg += ""-"" * 60 + ""\\n""\n        if self.dropout_keep_prob < 1:\n            msg += ""Using dropout with keep_prob = {}\\n"".format(self.dropout_keep_prob)\n        else:\n            msg += ""Training without dropout\\n""\n        msg += ""Training {} batch norm\\n"".format(""with"" if self.use_batch_norm else ""without"")\n        msg += ""Hidden units: {}\\n"".format(self.hidden_units)\n\n        msg += ""="" * 60 + ""\\n""\n        if not self._use_wide_network:\n            msg += ""Wide model: None\\n""\n        else:\n            msg += ""Wide model: {}\\n"".format(""logistic regression"" if self._dndf is None else ""DNDF"")\n            msg += ""Wide model input: Continuous features only\\n""\n            msg += ""-"" * 60 + \'\\n\'\n            if self._dndf is not None:\n                msg += ""Using DNDF with n_tree = {}, tree_depth = {}\\n"".format(\n                    self._dndf.n_tree, self._dndf.tree_depth\n                )\n\n        msg += ""\\n"".join([""="" * 60, ""Hyper parameters"", ""-"" * 60, ""{}"".format(\n            ""This is a DNN model"" if self._dndf is None and not self._use_wide_network else\n            ""This is a Wide & Deep model"" if self._dndf is None else\n            ""This is a hybrid model""\n        ), ""-"" * 60]) + ""\\n""\n        msg += ""Activation       : "" + str(self.activations) + ""\\n""\n        msg += ""Batch size       : "" + str(self.batch_size) + ""\\n""\n        msg += ""Epoch num        : "" + str(self.n_epoch) + ""\\n""\n        msg += ""Optimizer        : "" + self._optimizer_name + ""\\n""\n        msg += ""Metric           : "" + self._metric_name + ""\\n""\n        msg += ""Loss             : "" + self._loss_name + ""\\n""\n        msg += ""lr               : "" + str(self.lr) + ""\\n""\n        msg += ""-"" * 60 + ""\\n""\n        msg += ""Pruner           : {}"".format(""None"" if self._pruner is None else """") + ""\\n""\n        if self._pruner is not None:\n            msg += ""\\n"".join(""-> {:14}: {}"".format(key, value) for key, value in sorted(\n                self._pruner.params.items()\n            )) + ""\\n""\n        msg += ""-"" * 60 + ""\\n""\n        print(msg)\n'"
_Dist/NeuralNetworks/_Tests/Pruner/Base.py,24,"b'import os\nimport sys\n\nroot_path = os.path.abspath(""../../../../"")\nif root_path not in sys.path:\n    sys.path.append(root_path)\n\nimport os\nimport time\nimport math\nimport random\nimport pickle\nimport shutil\nimport logging\nimport numpy as np\nimport tensorflow as tf\nimport matplotlib.pyplot as plt\n\nfrom mpl_toolkits.mplot3d import Axes3D\n\nfrom _Dist.NeuralNetworks.NNUtil import *\n\n\nclass Generator:\n    def __init__(self, x, y, weights=None, name=""Generator"", shuffle=True):\n        self._cache = {}\n        self._x, self._y = np.asarray(x, np.float32), np.asarray(y, np.float32)\n        if weights is None:\n            self._sample_weights = None\n        else:\n            self._sample_weights = np.asarray(weights, np.float32)\n        if len(self._y.shape) == 1:\n            y_int = self._y.astype(np.int32)\n            if np.allclose(self._y, y_int):\n                assert y_int.min() == 0, ""Labels should start from 0""\n                self.n_class = y_int.max() + 1\n            else:\n                self.n_class = 1\n        self._name = name\n        self._do_shuffle = shuffle\n        self._all_valid_data = self._generate_all_valid_data()\n        self._valid_indices = np.arange(len(self._all_valid_data))\n        self._random_indices = self._valid_indices.copy()\n        np.random.shuffle(self._random_indices)\n        self._batch_cursor = -1\n\n    def __enter__(self):\n        self._cache_current_status()\n\n    def __exit__(self, exc_type, exc_val, exc_tb):\n        self._restore_cache()\n\n    def __getitem__(self, item):\n        return getattr(self, ""_"" + item)\n\n    def __len__(self):\n        return self.n_valid\n\n    def __str__(self):\n        return ""{}_{}"".format(self._name, self.shape)\n\n    __repr__ = __str__\n\n    @property\n    def n_valid(self):\n        return len(self._valid_indices)\n\n    @property\n    def n_dim(self):\n        return self._x.shape[-1]\n\n    @property\n    def shape(self):\n        return self.n_valid, self.n_dim\n\n    def _generate_all_valid_data(self):\n        return np.hstack([self._x, self._y.reshape([-1, 1])])\n\n    def _cache_current_status(self):\n        self._cache[""_valid_indices""] = self._valid_indices\n        self._cache[""_random_indices""] = self._random_indices\n\n    def _restore_cache(self):\n        self._valid_indices = self._cache[""_valid_indices""]\n        self._random_indices = self._cache[""_random_indices""]\n        self._cache = {}\n\n    def set_indices(self, indices):\n        indices = np.asarray(indices, np.int)\n        self._valid_indices = self._valid_indices[indices]\n        self._random_indices = self._random_indices[indices]\n\n    def set_range(self, start, end=None):\n        if end is None:\n            self._valid_indices = self._valid_indices[start:]\n            self._random_indices = self._random_indices[start:]\n        else:\n            self._valid_indices = self._valid_indices[start:end]\n            self._random_indices = self._random_indices[start:end]\n\n    def get_indices(self, indices):\n        return self._get_data(np.asarray(indices, np.int))\n\n    def get_range(self, start, end=None):\n        if end is None:\n            return self._get_data(self._valid_indices[start:])\n        return self._get_data(self._valid_indices[start:end])\n\n    def _get_data(self, indices):\n        weights = None if self._sample_weights is None else self._sample_weights[indices]\n        return self._all_valid_data[indices], weights\n\n    def gen_batch(self, n_batch, re_shuffle=True):\n        n_batch = min(n_batch, self.n_valid)\n        logger = logging.getLogger(""DataReader"")\n        if n_batch == -1:\n            n_batch = self.n_valid\n        if self._batch_cursor < 0:\n            self._batch_cursor = 0\n        if self._do_shuffle:\n            if self._batch_cursor == 0 and re_shuffle:\n                logger.debug(""Re-shuffling random indices"")\n                np.random.shuffle(self._random_indices)\n            indices = self._random_indices\n        else:\n            indices = self._valid_indices\n        logger.debug(""Generating batch with size={}"".format(n_batch))\n        end = False\n        next_cursor = self._batch_cursor + n_batch\n        if next_cursor >= self.n_valid:\n            next_cursor = self.n_valid\n            end = True\n        rs, w = self._get_data(indices[self._batch_cursor:next_cursor])\n        if end:\n            self._batch_cursor = -1\n        else:\n            self._batch_cursor = next_cursor\n        logger.debug(""Done"")\n        return rs, w\n\n    def gen_random_subset(self, n):\n        n = min(n, self.n_valid)\n        logger = logging.getLogger(""DataReader"")\n        logger.debug(""Generating random subset with size={}"".format(n))\n        start = random.randint(0, self.n_valid - n)\n        subset, weights = self._get_data(self._random_indices[start:start + n])\n        logger.debug(""Done"")\n        return subset, weights\n\n    def get_all_data(self):\n        if self._all_valid_data is not None:\n            return self._all_valid_data, self._sample_weights\n        return self._get_data(self._valid_indices)\n\n\nclass Generator3d(Generator):\n    @property\n    def n_time_step(self):\n        return self._x.shape[1]\n\n    @property\n    def shape(self):\n        return self.n_valid, self.n_time_step, self.n_dim\n\n    def _generate_all_valid_data(self):\n        return np.array([(x, y) for x, y in zip(self._x, self._y)])\n\n\nclass Generator4d(Generator3d):\n    @property\n    def height(self):\n        return self._x.shape[1]\n\n    @property\n    def width(self):\n        return self._x.shape[2]\n\n    @property\n    def shape(self):\n        return self.n_valid, self.height, self.width, self.n_dim\n\n\nclass Base:\n    def __init__(self, name=None, model_param_settings=None, model_structure_settings=None):\n        tf.reset_default_graph()\n\n        self.log = {}\n        self._name = name\n        self._name_appendix = """"\n        self._settings_initialized = False\n\n        self._generator_base = Generator\n        self._train_generator = self._test_generator = None\n        self._sample_weights = self._tf_sample_weights = None\n        self.n_dim = self.n_class = None\n        self.n_random_train_subset = self.n_random_test_subset = None\n\n        if model_param_settings is None:\n            self.model_param_settings = {}\n        else:\n            assert_msg = ""model_param_settings should be a dictionary""\n            assert isinstance(model_param_settings, dict), assert_msg\n            self.model_param_settings = model_param_settings\n        self.lr = None\n        self._loss = self._metric = None\n        self._loss_name = self._metric_name = None\n        self._optimizer_name = self._optimizer = None\n        self.n_epoch = self.max_epoch = self.n_iter = self.batch_size = None\n\n        if model_structure_settings is None:\n            self.model_structure_settings = {}\n        else:\n            assert_msg = ""model_structure_settings should be a dictionary""\n            assert isinstance(model_structure_settings, dict), assert_msg\n            self.model_structure_settings = model_structure_settings\n\n        self._model_built = False\n        self.py_collections = self.tf_collections = None\n        self._define_py_collections()\n        self._define_tf_collections()\n\n        self._ws, self._bs = [], []\n        self._pruner = None\n        self._is_training = None\n        self._loss = self._train_step = None\n        self._tfx = self._tfy = self._output = self._prob_output = None\n\n        self._sess = tf.Session()\n\n    def __str__(self):\n        return self.model_saving_name\n\n    __repr__ = __str__\n\n    @property\n    def name(self):\n        return ""Base"" if self._name is None else self._name\n\n    @property\n    def model_saving_name(self):\n        return ""{}_{}"".format(self.name, self._name_appendix)\n\n    @property\n    def model_saving_path(self):\n        return os.path.join(""_Models"", self.model_saving_name)\n\n    # Settings\n\n    def init_from_data(self, x, y, x_test, y_test, sample_weights, names):\n        self._sample_weights = sample_weights\n        if self._sample_weights is None:\n            self._tf_sample_weights = None\n        else:\n            self._tf_sample_weights = tf.placeholder(tf.float32, name=""sample_weights"")\n\n        self._train_generator = self._generator_base(x, y, self._sample_weights, name=""TrainGenerator"")\n        if x_test is not None and y_test is not None:\n            self._test_generator = self._generator_base(x_test, y_test, name=""TestGenerator"")\n        else:\n            self._test_generator = None\n        self.n_random_train_subset = int(len(self._train_generator) * 0.1)\n        if self._test_generator is None:\n            self.n_random_test_subset = -1\n        else:\n            self.n_random_test_subset = len(self._test_generator)\n\n        self.n_dim = self._train_generator.shape[-1]\n        self.n_class = self._train_generator.n_class\n\n    def init_all_settings(self):\n        self.init_model_param_settings()\n        self.init_model_structure_settings()\n\n    def init_model_param_settings(self):\n        loss = self.model_param_settings.get(""loss"", None)\n        if loss is None:\n            self._loss_name = ""correlation"" if self.n_class == 1 else ""cross_entropy""\n        else:\n            self._loss_name = loss\n        metric = self.model_param_settings.get(""metric"", None)\n        if metric is None:\n            if self.n_class == 1:\n                self._metric, self._metric_name = Metrics.correlation, ""correlation""\n            else:\n                self._metric, self._metric_name = Metrics.acc, ""acc""\n        else:\n            self._metric, self._metric_name = getattr(Metrics, metric), metric\n        self.n_epoch = self.model_param_settings.get(""n_epoch"", 32)\n        self.max_epoch = self.model_param_settings.get(""max_epoch"", 256)\n        self.max_epoch = max(self.max_epoch, self.n_epoch)\n        self.batch_size = self.model_param_settings.get(""batch_size"", 128)\n        self.batch_size = min(self.batch_size, len(self._train_generator))\n        self.n_iter = self.model_param_settings.get(""n_iter"", -1)\n        if self.n_iter < 0:\n            self.n_iter = int(len(self._train_generator) / self.batch_size)\n        self._optimizer_name = self.model_param_settings.get(""optimizer"", ""Adam"")\n        self.lr = self.model_param_settings.get(""lr"", 1e-3)\n        self._optimizer = getattr(tf.train, ""{}Optimizer"".format(self._optimizer_name))(self.lr)\n\n    def init_model_structure_settings(self):\n        pass\n\n    # Core\n\n    def _fully_connected_linear(self, net, shape, appendix):\n        with tf.name_scope(""Linear{}"".format(appendix)):\n            w = init_w(shape, ""W{}"".format(appendix))\n            b = init_b([shape[1]], ""b{}"".format(appendix))\n            self._ws.append(w)\n            self._bs.append(b)\n            return tf.add(tf.matmul(net, w), b, name=""Linear{}_Output"".format(appendix))\n\n    def _build_model(self, net=None):\n        pass\n\n    def _gen_batch(self, generator, n_batch, gen_random_subset=False, one_hot=False):\n        if gen_random_subset:\n            data, weights = generator.gen_random_subset(n_batch)\n        else:\n            data, weights = generator.gen_batch(n_batch)\n        x, y = data[..., :-1], data[..., -1]\n        if not one_hot:\n            return x, y, weights\n        if self.n_class == 1:\n            y = y.reshape([-1, 1])\n        else:\n            y = Toolbox.get_one_hot(y, self.n_class)\n        return x, y, weights\n\n    def _get_feed_dict(self, x, y=None, weights=None, is_training=False):\n        feed_dict = {self._tfx: x, self._is_training: is_training}\n        if y is not None:\n            feed_dict[self._tfy] = y\n        if self._tf_sample_weights is not None:\n            if weights is None:\n                weights = np.ones(len(x))\n            feed_dict[self._tf_sample_weights] = weights\n        return feed_dict\n\n    def _define_loss_and_train_step(self):\n        self._loss = getattr(Losses, self._loss_name)(self._tfy, self._output, False, self._tf_sample_weights)\n        with tf.control_dependencies(tf.get_collection(tf.GraphKeys.UPDATE_OPS)):\n            self._train_step = self._optimizer.minimize(self._loss)\n\n    def _initialize(self):\n        self._sess.run(tf.global_variables_initializer())\n\n    def _snapshot(self, i_epoch, i_iter, snapshot_cursor):\n        x_train, y_train, sw_train = self._gen_batch(\n            self._train_generator, self.n_random_train_subset,\n            gen_random_subset=True\n        )\n        if self._test_generator is not None:\n            x_test, y_test, sw_test = self._gen_batch(\n                self._test_generator, self.n_random_test_subset,\n                gen_random_subset=True\n            )\n            if self.n_class == 1:\n                y_test = y_test.reshape([-1, 1])\n            else:\n                y_test = Toolbox.get_one_hot(y_test, self.n_class)\n        else:\n            x_test = y_test = sw_test = None\n        y_train_pred = self._predict(x_train)\n        if x_test is not None:\n            y_test_pred, test_snapshot_loss = self._calculate(\n                x_test, y_test, sw_test,\n                [self._output, self._loss], is_training=False\n            )\n            y_test_pred, test_snapshot_loss = y_test_pred[0], test_snapshot_loss[0]\n            self.log[""test_snapshot_loss""].append(test_snapshot_loss)\n        else:\n            y_test_pred = None\n        train_metric = self._metric(y_train, y_train_pred)\n        if y_test is not None and y_test_pred is not None:\n            test_metric = self._metric(y_test, y_test_pred)\n        else:\n            test_metric = None\n        print(""\\rEpoch {:6}   Iter {:8}   Snapshot {:6} ({})  -  Train : {:8.6}   Test : {}"".format(\n            i_epoch, i_iter, snapshot_cursor, self._metric_name, train_metric,\n            ""None"" if test_metric is None else ""{:8.6}"".format(test_metric)\n        ), end="""")\n        if i_epoch == i_iter == snapshot_cursor == 0:\n            print()\n        return train_metric, test_metric\n\n    def _calculate(self, x, y=None, weights=None, tensor=None, n_elem=1e7, is_training=False):\n        n_batch = int(n_elem / x.shape[1])\n        n_repeat = int(len(x) / n_batch)\n        if n_repeat * n_batch < len(x):\n            n_repeat += 1\n        cursors = [0]\n        if tensor is None:\n            target = self._prob_output\n        elif isinstance(tensor, list):\n            target = []\n            for t in tensor:\n                if isinstance(t, str):\n                    t = getattr(self, t)\n                if isinstance(t, list):\n                    target += t\n                    cursors.append(len(t))\n                else:\n                    target.append(t)\n                    cursors.append(cursors[-1] + 1)\n        else:\n            target = getattr(self, tensor) if isinstance(tensor, str) else tensor\n        results = [self._sess.run(\n            target, self._get_feed_dict(\n                x[i * n_batch:(i + 1) * n_batch],\n                None if y is None else y[i * n_batch:(i + 1) * n_batch],\n                None if weights is None else weights[i * n_batch:(i + 1) * n_batch],\n                is_training=is_training\n            )\n        ) for i in range(n_repeat)]\n        if not isinstance(target, list):\n            if len(results) == 1:\n                return results[0]\n            return np.vstack(results)\n        if n_repeat > 1:\n            results = [\n                np.vstack([result[i] for result in results]) if target[i].shape.ndims else\n                np.mean([result[i] for result in results]) for i in range(len(target))\n            ]\n        else:\n            results = results[0]\n        if len(cursors) == 1:\n            return results\n        return [results[cursor:cursors[i + 1]] for i, cursor in enumerate(cursors[:-1])]\n\n    def _predict(self, x):\n        tensor = self._output if self.n_class == 1 else self._prob_output\n        output = self._calculate(x, tensor=tensor, is_training=False)\n        if self.n_class == 1:\n            return output.ravel()\n        return output\n\n    def _evaluate(self, x=None, y=None, x_cv=None, y_cv=None, x_test=None, y_test=None):\n        pred = self._predict(x) if x is not None else None\n        cv_pred = self._predict(x_cv) if x_cv is not None else None\n        test_pred = self._predict(x_test) if x_test is not None else None\n        print(""{}  -  Train : {}   CV : {}   Test : {}"".format(\n            self._metric_name,\n            ""None"" if y is None else ""{:8.6}"".format(self._metric(y, pred)),\n            ""None"" if y_cv is None else ""{:8.6}"".format(self._metric(y_cv, cv_pred)),\n            ""None"" if y_test is None else ""{:8.6}"".format(self._metric(y_test, test_pred))\n        ))\n        return self\n\n    def _define_input_and_placeholder(self):\n        self._is_training = tf.placeholder(tf.bool, name=""is_training"")\n        self._tfx = tf.placeholder(tf.float32, [None, self.n_dim], name=""X"")\n        self._tfy = tf.placeholder(tf.float32, [None, self.n_class], name=""Y"")\n\n    def _define_py_collections(self):\n        self.py_collections = [""_name"", ""model_param_settings"", ""model_structure_settings""]\n\n    def _define_tf_collections(self):\n        self.tf_collections = [\n            ""_tfx"", ""_tfy"", ""_output"", ""_prob_output"",\n            ""_loss"", ""_train_step"", ""_is_training""\n        ]\n\n    # Save & Load\n\n    def add_tf_collections(self):\n        for tensor in self.tf_collections:\n            target = getattr(self, tensor)\n            if target is not None:\n                tf.add_to_collection(tensor, target)\n\n    def clear_tf_collections(self):\n        for key in self.tf_collections:\n            tf.get_collection_ref(key).clear()\n\n    def save_collections(self, folder):\n        with open(os.path.join(folder, ""py.core""), ""wb"") as file:\n            param_dict = {name: getattr(self, name) for name in self.py_collections}\n            pickle.dump(param_dict, file)\n        self.add_tf_collections()\n\n    def restore_collections(self, folder):\n        with open(os.path.join(folder, ""py.core""), ""rb"") as file:\n            param_dict = pickle.load(file)\n            for name, value in param_dict.items():\n                setattr(self, name, value)\n        for tensor in self.tf_collections:\n            target = tf.get_collection(tensor)\n            if target is None:\n                continue\n            assert len(target) == 1, ""{} available \'{}\' found"".format(len(target), tensor)\n            setattr(self, tensor, target[0])\n        self.clear_tf_collections()\n\n    @staticmethod\n    def get_model_name(path, idx):\n        targets = os.listdir(path)\n        if idx is None:\n            idx = max([int(target) for target in targets if target.isnumeric()])\n        return os.path.join(path, ""{:06}"".format(idx))\n\n    def save(self, run_id=0, path=None):\n        if path is None:\n            path = self.model_saving_path\n        folder = os.path.join(path, ""{:06}"".format(run_id))\n        if not os.path.exists(folder):\n            os.makedirs(folder)\n        print(""Saving model"")\n        saver = tf.train.Saver()\n        self.save_collections(folder)\n        saver.save(self._sess, os.path.join(folder, ""Model""))\n        print(""Model saved to "" + folder)\n        return self\n\n    def load(self, run_id=None, clear_devices=False, path=None):\n        self._model_built = True\n        if path is None:\n            path = self.model_saving_path\n        folder = self.get_model_name(path, run_id)\n        path = os.path.join(folder, ""Model"")\n        print(""Restoring model"")\n        saver = tf.train.import_meta_graph(""{}.meta"".format(path), clear_devices)\n        saver.restore(self._sess, tf.train.latest_checkpoint(folder))\n        self.restore_collections(folder)\n        print(""Model restored from "" + folder)\n        return self\n\n    def save_checkpoint(self, folder):\n        if not os.path.exists(folder):\n            os.makedirs(folder)\n        tf.train.Saver().save(self._sess, os.path.join(folder, ""Model""))\n\n    def restore_checkpoint(self, folder):\n        tf.train.Saver().restore(self._sess, tf.train.latest_checkpoint(folder))\n\n    # API\n\n    def print_settings(self):\n        pass\n\n    def fit(self, x, y, x_test=None, y_test=None, sample_weights=None, names=(""train"", ""test""),\n            timeit=True, snapshot_ratio=3, print_settings=True, verbose=1):\n        t = None\n        if timeit:\n            t = time.time()\n\n        self.init_from_data(x, y, x_test, y_test, sample_weights, names)\n        if not self._settings_initialized:\n            self.init_all_settings()\n        self._settings_initialized = True\n\n        if not self._model_built:\n            with tf.name_scope(""Input""):\n                self._define_input_and_placeholder()\n            with tf.name_scope(""Model""):\n                self._build_model()\n                self._prob_output = tf.nn.softmax(self._output, name=""Prob_Output"")\n            with tf.name_scope(""LossAndTrainStep""):\n                self._define_loss_and_train_step()\n            with tf.name_scope(""Initialize""):\n                self._initialize()\n\n        i_epoch = i_iter = j = snapshot_cursor = 0\n        if snapshot_ratio == 0 or x_test is None or y_test is None:\n            use_monitor = False\n            snapshot_step = self.n_iter\n        else:\n            use_monitor = True\n            snapshot_ratio = min(snapshot_ratio, self.n_iter)\n            snapshot_step = int(self.n_iter / snapshot_ratio)\n\n        terminate = False\n        over_fitting_flag = 0\n        n_epoch = self.n_epoch\n        tmp_checkpoint_folder = os.path.join(self.model_saving_path, ""tmp"")\n        monitor = TrainMonitor(Metrics.sign_dict[self._metric_name], snapshot_ratio)\n\n        if verbose >= 2:\n            prepare_tensorboard_verbose(self._sess)\n\n        if print_settings:\n            self.print_settings()\n\n        self.log[""iter_loss""] = []\n        self.log[""epoch_loss""] = []\n        self.log[""test_snapshot_loss""] = []\n        self._snapshot(0, 0, 0)\n\n        # ==========================================================\n        # Test\n        # ----------------------------------------------------------\n        self.log[""w_abs_mean""] = w_abs_means = []\n        self.log[""w_abs_std""] = w_abs_stds = []\n        self.log[""target_ratio""] = target_ratio = []\n        self.log[""pruned_target_ratio""] = pruned_target_ratio = []\n        self.log[""org_w_abs_mean""] = org_w_abs_mean = []\n        self.log[""org_pruned_w_abs_mean""] = org_pruned_w_abs_mean = []\n        self.log[""pruned_w_abs_residual""] = pruned_w_abs_residual = []\n        self.log[""pruned_w_abs_mean""] = pruned_w_abs_means = []\n        self.log[""pruned_w_abs_std""] = pruned_w_abs_stds = []\n        self.log[""pruned_ratio""] = pruned_ratio = []\n        self.log[""recovered_ratio""] = recovered = []\n        self.log[""running_recovered""] = running_recovered = []\n        self.log[""rwowam""] = rwowam = []\n        self.log[""rwowamr""] = rwowamr = []\n        self.log[""rwnwam""] = rwnwam = []\n        self.log[""rwnwamr""] = rwnwamr = []\n        self.log[""rwwamr""] = rwwamr = []\n        self.log[""acc""] = accs = []\n        w = self._sess.run(self._pruner.org_ws[0])\n        pruned_w = None\n        if self._pruner.cond_placeholder is not None:\n            feed_dict = {self._pruner.cond_placeholder: True}\n            pruned_w = self._sess.run(self._ws[0], feed_dict=feed_dict)\n        nw = int(np.prod(w.shape))\n        w_abs = np.abs(w)\n        w_abs_mean = w_abs.mean()\n        w_abs_std = w_abs.std()\n        if pruned_w is None:\n            org_pruned_mask = w_abs < w_abs_mean\n        else:\n            org_pruned_mask = pruned_w == 0\n        pruned_w_abs = w_abs[org_pruned_mask]\n        pruned_w_abs_mean = pruned_w_abs.mean()\n        org_w_abs_mean.append(w_abs[~org_pruned_mask].mean())\n        org_pruned_w_abs_mean.append(pruned_w_abs_mean)\n        pruned_w_abs_std = pruned_w_abs.std()\n        n_org_pruned = org_pruned_mask.sum()\n        w_abs_means.append(w_abs_mean)\n        w_abs_stds.append(w_abs_std)\n        pruned_w_abs_means.append(pruned_w_abs_mean)\n        pruned_w_abs_stds.append(pruned_w_abs_std)\n        target_ratio.append(((w_abs <= (w_abs_mean + w_abs_std)) & (w_abs >= (w_abs_mean - w_abs_std))).mean())\n        pruned_target_ratio.append((w_abs[org_pruned_mask] >= (w_abs_mean - w_abs_std)).mean())\n        pruned_ratio.append(n_org_pruned / nw)\n        previous_pruned_mask = org_pruned_mask\n        previous_w_abs = w_abs\n\n        while i_epoch < n_epoch:\n            i_epoch += 1\n            epoch_loss = 0\n            for j in range(self.n_iter):\n                i_iter += 1\n                x_batch, y_batch, sw_batch = self._gen_batch(self._train_generator, self.batch_size, one_hot=True)\n                iter_loss = self._sess.run(\n                    [self._loss, self._train_step],\n                    self._get_feed_dict(x_batch, y_batch, sw_batch, is_training=True)\n                )[0]\n                self.log[""iter_loss""].append(iter_loss)\n                epoch_loss += iter_loss\n                if i_iter % snapshot_step == 0 and verbose >= 1:\n                    snapshot_cursor += 1\n                    train_metric, test_metric = self._snapshot(i_epoch, i_iter, snapshot_cursor)\n                    if use_monitor:\n                        check_rs = monitor.check(test_metric)\n                        over_fitting_flag = monitor.over_fitting_flag\n                        if check_rs[""terminate""]:\n                            n_epoch = i_epoch\n                            print(""  -  Early stopped at n_epoch={} due to \'{}\'"".format(\n                                n_epoch, check_rs[""info""]\n                            ))\n                            terminate = True\n                            break\n                        if check_rs[""save_checkpoint""]:\n                            print(""  -  {}"".format(check_rs[""info""]))\n                            self.save_checkpoint(tmp_checkpoint_folder)\n\n                # ==========================================================\n                # Test\n                # ----------------------------------------------------------\n                accs.append(self._metric(y_test, self._predict(x_test)))\n                w = self._sess.run(self._pruner.org_ws[0])\n                if self._pruner.cond_placeholder is not None:\n                    feed_dict = {self._pruner.cond_placeholder: True}\n                    pruned_w = self._sess.run(self._ws[0], feed_dict=feed_dict)\n                nw = int(np.prod(w.shape))\n                w_abs = np.abs(w)\n                w_abs_mean = w_abs.mean()\n                w_abs_std = w_abs.std()\n                target_ratio.append(((w_abs <= (w_abs_mean + w_abs_std)) & (w_abs >= (w_abs_mean - w_abs_std))).mean())\n                pruned_target_ratio.append((w_abs[org_pruned_mask] >= (w_abs_mean - w_abs_std)).mean())\n                if pruned_w is None:\n                    new_pruned_mask = w_abs < w_abs_mean\n                else:\n                    new_pruned_mask = pruned_w == 0\n                new_pruned_w_abs = w_abs[new_pruned_mask]\n                w_abs_means.append(w_abs_mean)\n                w_abs_stds.append(w_abs_std)\n                org_w_abs_mean.append(w_abs[~org_pruned_mask].mean())\n                org_pruned_w_abs_mean.append(w_abs[org_pruned_mask].mean())\n                pruned_w_abs_mean = new_pruned_w_abs.mean()\n                pruned_w_abs_std = new_pruned_w_abs.std()\n                pruned_w_abs_means.append(pruned_w_abs_mean)\n                pruned_w_abs_stds.append(pruned_w_abs_std)\n                pruned_ratio.append(new_pruned_mask.sum() / nw)\n                pruned_w_abs_residual.append(\n                    np.abs(w_abs[previous_pruned_mask] - previous_w_abs[previous_pruned_mask]).mean())\n                recovered_mask = ~new_pruned_mask & org_pruned_mask\n                recovered.append(recovered_mask.sum() / n_org_pruned)\n                running_recovered_mask = ~new_pruned_mask & previous_pruned_mask\n                running_recovered.append(running_recovered_mask.sum())\n                rwowam.append(previous_w_abs[running_recovered_mask].mean())\n                rwowamr.append((previous_w_abs[running_recovered_mask] / previous_w_abs.mean()).mean())\n                rwnwam.append(w_abs[running_recovered_mask].mean())\n                rwnwamr.append((w_abs[running_recovered_mask] / w_abs_mean).mean())\n                rwwamr.append((w_abs[running_recovered_mask] / previous_w_abs[running_recovered_mask]).mean())\n                previous_pruned_mask = new_pruned_mask\n                previous_w_abs = w_abs\n\n            self.log[""epoch_loss""].append(epoch_loss / (j + 1))\n            if use_monitor:\n                if i_epoch == n_epoch and i_epoch < self.max_epoch and not monitor.info[""terminate""]:\n                    monitor.flat_flag = True\n                    monitor.punish_extension()\n                    n_epoch = min(n_epoch + monitor.extension, self.max_epoch)\n                    print(""  -  Extending n_epoch to {}"".format(n_epoch))\n                if i_epoch == self.max_epoch:\n                    terminate = True\n                    if not monitor.info[""terminate""]:\n                        if not over_fitting_flag:\n                            print(\n                                ""  -  Model seems to be under-fitting but max_epoch reached. ""\n                                ""Increasing max_epoch may improve performance""\n                            )\n                        else:\n                            print(""  -  max_epoch reached"")\n            elif i_epoch == n_epoch:\n                terminate = True\n            if terminate:\n                if os.path.exists(tmp_checkpoint_folder):\n                    print(""  -  Rolling back to the best checkpoint"")\n                    self.restore_checkpoint(tmp_checkpoint_folder)\n                    shutil.rmtree(tmp_checkpoint_folder)\n                break\n        self._snapshot(-1, -1, -1)\n\n        if timeit:\n            print(""  -  Time Cost: {}"".format(time.time() - t))\n\n        return self\n\n    def predict(self, x):\n        return self._predict(x)\n\n    def predict_classes(self, x):\n        if self.n_class == 1:\n            raise ValueError(""Predicting classes is not permitted in regression problem"")\n        return self._predict(x).argmax(1)\n\n    def evaluate(self, x, y, x_cv=None, y_cv=None, x_test=None, y_test=None):\n        return self._evaluate(x, y, x_cv, y_cv, x_test, y_test)\n\n    # Visualization\n\n    def draw_losses(self):\n        el, il = self.log[""epoch_loss""], self.log[""iter_loss""]\n        ee_base = np.arange(len(el))\n        ie_base = np.linspace(0, len(el) - 1, len(il))\n        plt.figure()\n        plt.plot(ie_base, il, label=""Iter loss"")\n        plt.plot(ee_base, el, linewidth=3, label=""Epoch loss"")\n        plt.legend()\n        plt.show()\n        return self\n\n    def scatter2d(self, x, y, padding=0.5, title=None):\n        axis, labels = np.asarray(x).T, np.asarray(y)\n\n        print(""="" * 30 + ""\\n"" + str(self))\n        x_min, x_max = np.min(axis[0]), np.max(axis[0])\n        y_min, y_max = np.min(axis[1]), np.max(axis[1])\n        x_padding = max(abs(x_min), abs(x_max)) * padding\n        y_padding = max(abs(y_min), abs(y_max)) * padding\n        x_min -= x_padding\n        x_max += x_padding\n        y_min -= y_padding\n        y_max += y_padding\n\n        if labels.ndim == 1:\n            plot_label_dict = {c: i for i, c in enumerate(set(labels))}\n            n_label = len(plot_label_dict)\n            labels = np.array([plot_label_dict[label] for label in labels])\n        else:\n            n_label = labels.shape[1]\n            labels = np.argmax(labels, axis=1)\n        colors = plt.cm.rainbow([i / n_label for i in range(n_label)])[labels]\n\n        if title is None:\n            title = self.model_saving_name\n\n        indices = [labels == i for i in range(np.max(labels) + 1)]\n        scatters = []\n        plt.figure()\n        plt.title(title)\n        for idx in indices:\n            scatters.append(plt.scatter(axis[0][idx], axis[1][idx], c=colors[idx]))\n        plt.legend(scatters, [""$c_{}$"".format(""{"" + str(i) + ""}"") for i in range(len(scatters))],\n                   ncol=math.ceil(math.sqrt(len(scatters))), fontsize=8)\n        plt.xlim(x_min, x_max)\n        plt.ylim(y_min, y_max)\n        plt.show()\n        return self\n\n    def scatter3d(self, x, y, padding=0.1, title=None):\n        axis, labels = np.asarray(x).T, np.asarray(y)\n\n        print(""="" * 30 + ""\\n"" + str(self))\n        x_min, x_max = np.min(axis[0]), np.max(axis[0])\n        y_min, y_max = np.min(axis[1]), np.max(axis[1])\n        z_min, z_max = np.min(axis[2]), np.max(axis[2])\n        x_padding = max(abs(x_min), abs(x_max)) * padding\n        y_padding = max(abs(y_min), abs(y_max)) * padding\n        z_padding = max(abs(z_min), abs(z_max)) * padding\n        x_min -= x_padding\n        x_max += x_padding\n        y_min -= y_padding\n        y_max += y_padding\n        z_min -= z_padding\n        z_max += z_padding\n\n        def transform_arr(arr):\n            if arr.ndim == 1:\n                dic = {c: i for i, c in enumerate(set(arr))}\n                n_dim = len(dic)\n                arr = np.array([dic[label] for label in arr])\n            else:\n                n_dim = arr.shape[1]\n                arr = np.argmax(arr, axis=1)\n            return arr, n_dim\n\n        if title is None:\n            title = self.model_saving_name\n\n        labels, n_label = transform_arr(labels)\n        colors = plt.cm.rainbow([i / n_label for i in range(n_label)])[labels]\n        indices = [labels == i for i in range(n_label)]\n        scatters = []\n        fig = plt.figure()\n        plt.title(title)\n        ax = fig.add_subplot(111, projection=\'3d\')\n        for _index in indices:\n            scatters.append(ax.scatter(axis[0][_index], axis[1][_index], axis[2][_index], c=colors[_index]))\n        ax.legend(scatters, [""$c_{}$"".format(""{"" + str(i) + ""}"") for i in range(len(scatters))],\n                  ncol=math.ceil(math.sqrt(len(scatters))), fontsize=8)\n        plt.show()\n        return self\n\n    def visualize2d(self, x, y, padding=0.1, dense=200, title=None,\n                    scatter=True, show_org=False, draw_background=True, emphasize=None, extra=None):\n        axis, labels = np.asarray(x).T, np.asarray(y)\n\n        print(""="" * 30 + ""\\n"" + str(self))\n        nx, ny, padding = dense, dense, padding\n        x_min, x_max = np.min(axis[0]), np.max(axis[0])\n        y_min, y_max = np.min(axis[1]), np.max(axis[1])\n        x_padding = max(abs(x_min), abs(x_max)) * padding\n        y_padding = max(abs(y_min), abs(y_max)) * padding\n        x_min -= x_padding\n        x_max += x_padding\n        y_min -= y_padding\n        y_max += y_padding\n\n        def get_base(_nx, _ny):\n            _xf = np.linspace(x_min, x_max, _nx)\n            _yf = np.linspace(y_min, y_max, _ny)\n            n_xf, n_yf = np.meshgrid(_xf, _yf)\n            return _xf, _yf, np.c_[n_xf.ravel(), n_yf.ravel()]\n\n        xf, yf, base_matrix = get_base(nx, ny)\n\n        t = time.time()\n        z = self.predict_classes(base_matrix).reshape((nx, ny))\n        print(""Decision Time: {:8.6} s"".format(time.time() - t))\n\n        print(""Drawing figures..."")\n        xy_xf, xy_yf = np.meshgrid(xf, yf, sparse=True)\n        if labels.ndim == 1:\n            plot_label_dict = {c: i for i, c in enumerate(set(labels))}\n            n_label = len(plot_label_dict)\n            labels = np.array([plot_label_dict[label] for label in labels])\n        else:\n            n_label = labels.shape[1]\n            labels = np.argmax(labels, axis=1)\n        colors = plt.cm.rainbow([i / n_label for i in range(n_label)])[labels]\n\n        if title is None:\n            title = self.model_saving_name\n\n        if show_org:\n            plt.figure()\n            plt.scatter(axis[0], axis[1], c=colors)\n            plt.xlim(x_min, x_max)\n            plt.ylim(y_min, y_max)\n            plt.show()\n\n        plt.figure()\n        plt.title(title)\n        if draw_background:\n            plt.pcolormesh(xy_xf, xy_yf, z, cmap=plt.cm.Pastel1)\n        else:\n            plt.contour(xf, yf, z, c=\'k-\', levels=[0])\n        if scatter:\n            plt.scatter(axis[0], axis[1], c=colors)\n        if emphasize is not None:\n            indices = np.array([False] * len(axis[0]))\n            indices[np.asarray(emphasize)] = True\n            plt.scatter(axis[0][indices], axis[1][indices], s=80,\n                        facecolors=""None"", zorder=10)\n        if extra is not None:\n            plt.scatter(*np.asarray(extra).T, s=80, zorder=25, facecolors=""red"")\n        plt.xlim(x_min, x_max)\n        plt.ylim(y_min, y_max)\n        plt.show()\n        print(""Done."")\n        return self\n\n    def visualize3d(self, x, y, padding=0.1, dense=100, title=None,\n                    show_org=False, draw_background=True, emphasize=None, extra=None):\n        if False:\n            print(Axes3D.add_artist)\n        axis, labels = np.asarray(x).T, np.asarray(y)\n\n        print(""="" * 30 + ""\\n"" + str(self))\n\n        nx, ny, nz, padding = dense, dense, dense, padding\n        x_min, x_max = np.min(axis[0]), np.max(axis[0])\n        y_min, y_max = np.min(axis[1]), np.max(axis[1])\n        z_min, z_max = np.min(axis[2]), np.max(axis[2])\n        x_padding = max(abs(x_min), abs(x_max)) * padding\n        y_padding = max(abs(y_min), abs(y_max)) * padding\n        z_padding = max(abs(z_min), abs(z_max)) * padding\n        x_min -= x_padding\n        x_max += x_padding\n        y_min -= y_padding\n        y_max += y_padding\n        z_min -= z_padding\n        z_max += z_padding\n\n        def get_base(_nx, _ny, _nz):\n            _xf = np.linspace(x_min, x_max, _nx)\n            _yf = np.linspace(y_min, y_max, _ny)\n            _zf = np.linspace(z_min, z_max, _nz)\n            n_xf, n_yf, n_zf = np.meshgrid(_xf, _yf, _zf)\n            return _xf, _yf, _zf, np.c_[n_xf.ravel(), n_yf.ravel(), n_zf.ravel()]\n\n        xf, yf, zf, base_matrix = get_base(nx, ny, nz)\n\n        t = time.time()\n        z_xyz = self.predict_classes(base_matrix).reshape((nx, ny, nz))\n        p_classes = self.predict_classes(x).astype(np.int8)\n        _, _, _, base_matrix = get_base(10, 10, 10)\n        z_classes = self.predict_classes(base_matrix).astype(np.int8)\n        print(""Decision Time: {:8.6} s"".format(time.time() - t))\n\n        print(""Drawing figures..."")\n        z_xy = np.average(z_xyz, axis=2)\n        z_yz = np.average(z_xyz, axis=1)\n        z_xz = np.average(z_xyz, axis=0)\n\n        xy_xf, xy_yf = np.meshgrid(xf, yf, sparse=True)\n        yz_xf, yz_yf = np.meshgrid(yf, zf, sparse=True)\n        xz_xf, xz_yf = np.meshgrid(xf, zf, sparse=True)\n\n        def transform_arr(arr):\n            if arr.ndim == 1:\n                dic = {c: i for i, c in enumerate(set(arr))}\n                n_dim = len(dic)\n                arr = np.array([dic[label] for label in arr])\n            else:\n                n_dim = arr.shape[1]\n                arr = np.argmax(arr, axis=1)\n            return arr, n_dim\n\n        labels, n_label = transform_arr(labels)\n        p_classes, _ = transform_arr(p_classes)\n        z_classes, _ = transform_arr(z_classes)\n        colors = plt.cm.rainbow([i / n_label for i in range(n_label)])\n        if extra is not None:\n            ex0, ex1, ex2 = np.asarray(extra).T\n        else:\n            ex0 = ex1 = ex2 = None\n\n        if title is None:\n            title = self.model_saving_name\n\n        if show_org:\n            fig = plt.figure()\n            ax = fig.add_subplot(111, projection=\'3d\')\n            ax.scatter(axis[0], axis[1], axis[2], c=colors[labels])\n            plt.show()\n\n        fig = plt.figure(figsize=(16, 4), dpi=100)\n        plt.title(title)\n        ax1 = fig.add_subplot(131, projection=\'3d\')\n        ax2 = fig.add_subplot(132, projection=\'3d\')\n        ax3 = fig.add_subplot(133, projection=\'3d\')\n\n        ax1.set_title(""Org"")\n        ax2.set_title(""Pred"")\n        ax3.set_title(""Boundary"")\n\n        ax1.scatter(axis[0], axis[1], axis[2], c=colors[labels])\n        ax2.scatter(axis[0], axis[1], axis[2], c=colors[p_classes], s=15)\n        if extra is not None:\n            ax2.scatter(ex0, ex1, ex2, s=80, zorder=25, facecolors=""red"")\n        xyz_xf, xyz_yf, xyz_zf = base_matrix[..., 0], base_matrix[..., 1], base_matrix[..., 2]\n        ax3.scatter(xyz_xf, xyz_yf, xyz_zf, c=colors[z_classes], s=15)\n\n        plt.show()\n        plt.close()\n\n        fig = plt.figure(figsize=(16, 4), dpi=100)\n        ax1 = fig.add_subplot(131)\n        ax2 = fig.add_subplot(132)\n        ax3 = fig.add_subplot(133)\n\n        def _draw(_ax, _x, _xf, _y, _yf, _z):\n            if draw_background:\n                _ax.pcolormesh(_x, _y, _z > 0, cmap=plt.cm.Pastel1)\n            else:\n                _ax.contour(_xf, _yf, _z, c=\'k-\', levels=[0])\n\n        def _emphasize(_ax, axis0, axis1, _c):\n            _ax.scatter(axis0, axis1, c=_c)\n            if emphasize is not None:\n                indices = np.array([False] * len(axis[0]))\n                indices[np.asarray(emphasize)] = True\n                _ax.scatter(axis0[indices], axis1[indices], s=80,\n                            facecolors=""None"", zorder=10)\n\n        def _extra(_ax, axis0, axis1, _c, _ex0, _ex1):\n            _emphasize(_ax, axis0, axis1, _c)\n            if extra is not None:\n                _ax.scatter(_ex0, _ex1, s=80, zorder=25, facecolors=""red"")\n\n        colors = colors[labels]\n\n        ax1.set_title(""xy figure"")\n        _draw(ax1, xy_xf, xf, xy_yf, yf, z_xy)\n        _extra(ax1, axis[0], axis[1], colors, ex0, ex1)\n\n        ax2.set_title(""yz figure"")\n        _draw(ax2, yz_xf, yf, yz_yf, zf, z_yz)\n        _extra(ax2, axis[1], axis[2], colors, ex1, ex2)\n\n        ax3.set_title(""xz figure"")\n        _draw(ax3, xz_xf, xf, xz_yf, zf, z_xz)\n        _extra(ax3, axis[0], axis[2], colors, ex0, ex2)\n\n        plt.show()\n        print(""Done."")\n        return self\n'"
_Dist/NeuralNetworks/_Tests/Pruner/Basic.py,0,"b'import os\nimport sys\nroot_path = os.path.abspath(""../../../../"")\nif root_path not in sys.path:\n    sys.path.append(root_path)\n\nfrom _Dist.NeuralNetworks.NNUtil import *\nfrom _Dist.NeuralNetworks._Tests.Pruner.Base import Base\n\n\nclass Basic(Base):\n    def __init__(self, *args, **kwargs):\n        super(Basic, self).__init__(*args, **kwargs)\n        self._name_appendix = ""Basic""\n        self.activations = self.hidden_units = None\n\n    @property\n    def name(self):\n        return ""NN"" if self._name is None else self._name\n\n    def init_model_param_settings(self):\n        super(Basic, self).init_model_param_settings()\n        self.activations = self.model_param_settings.get(""activations"", ""relu"")\n\n    def init_model_structure_settings(self):\n        super(Basic, self).init_model_structure_settings()\n        self.hidden_units = self.model_structure_settings.get(""hidden_units"", [256, 256])\n\n    def _build_layer(self, i, net):\n        activation = self.activations[i]\n        if activation is not None:\n            net = getattr(Activations, activation)(net, ""{}{}"".format(activation, i))\n        return net\n\n    def _build_model(self, net=None):\n        self._model_built = True\n        if net is None:\n            net = self._tfx\n        current_dimension = net.shape[1].value\n        if self.activations is None:\n            self.activations = [None] * len(self.hidden_units)\n        elif isinstance(self.activations, str):\n            self.activations = [self.activations] * len(self.hidden_units)\n        else:\n            self.activations = self.activations\n        for i, n_unit in enumerate(self.hidden_units):\n            net = self._fully_connected_linear(net, [current_dimension, n_unit], i)\n            net = self._build_layer(i, net)\n            current_dimension = n_unit\n        appendix = ""_final_projection""\n        fc_shape = self.hidden_units[-1] if self.hidden_units else current_dimension\n        self._output = self._fully_connected_linear(net, [fc_shape, self.n_class], appendix)\n\n\nif __name__ == \'__main__\':\n    from Util.Util import DataUtil\n\n    for generator in (DataUtil.gen_xor, DataUtil.gen_spiral, DataUtil.gen_nine_grid):\n        x_train, y_train = generator(size=1000, one_hot=False)\n        x_test, y_test = generator(size=100, one_hot=False)\n        nn = Basic(model_param_settings={""n_epoch"": 200}).scatter2d(x_train, y_train).fit(\n            x_train, y_train, x_test, y_test, snapshot_ratio=0\n        ).draw_losses().visualize2d(\n            x_train, y_train, title=""Train""\n        ).visualize2d(\n            x_test, y_test, padding=2, title=""Test""\n        )\n\n    for size in (256, 1000, 10000):\n        (x_train, y_train), (x_test, y_test) = DataUtil.gen_noisy_linear(\n            size=size, n_dim=2, n_valid=2, test_ratio=100 / size, one_hot=False\n        )\n        nn = Basic(model_param_settings={""n_epoch"": 200}).scatter2d(x_train, y_train).fit(\n            x_train, y_train, x_test, y_test, snapshot_ratio=0\n        ).draw_losses().visualize2d(x_train, y_train, title=""Train"").visualize2d(x_test, y_test, title=""Test"")\n'"
_Dist/NeuralNetworks/_Tests/Pruner/Test.py,0,"b'import os\nimport sys\nroot_path = os.path.abspath(""../../../../"")\nif root_path not in sys.path:\n    sys.path.append(root_path)\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfrom Util.Util import DataUtil\nfrom _Dist.NeuralNetworks._Tests.Pruner.Advanced import Advanced\n\n# (x, y), (x_test, y_test), *_ = DataUtil.get_dataset(""mnist"", ""_Data/mnist.txt"", n_train=1600, quantized=True)\n(x, y), (x_test, y_test) = DataUtil.gen_noisy_linear(n_dim=100, n_valid=5, one_hot=False)\n\ndata_info = {\n    ""numerical_idx"": [True] * 100 + [False],\n    ""categorical_columns"": []\n}\n\n# nn = Advanced(\n#     ""NoisyLinear"",\n#     data_info=data_info,\n#     model_param_settings={\n#         ""n_epoch"": 40\n#     },\n#     model_structure_settings={\n#         ""use_wide_network"": False,\n#         ""use_pruner"": True,\n#         ""pruner_params"": {\n#             ""prune_method"": ""surgery"",\n#             # ""alpha"": 1e-8,\n#             # ""beta"": 1e8\n#         },\n#         ""hidden_units"": [512]\n#     }\n# ).fit(x, y, x_test, y_test, snapshot_ratio=0)\nnn = Advanced(""NoisyLinear"", data_info=data_info).fit(x, y, x_test, y_test, snapshot_ratio=0)\n\n\ndef normalize(arr):\n    arr = np.array(arr, np.float32)\n    arr -= arr.mean()\n    arr /= arr.std()\n    return arr\n\n\nplt.figure()\npruned_ratio = normalize(nn.log[""pruned_ratio""])\nrecovered_ratio = normalize(nn.log[""recovered_ratio""])\nrunning_recovered = nn.log[""running_recovered""]\nw_abs_mean = nn.log[""w_abs_mean""]\nplt.plot(np.arange(len(pruned_ratio)), pruned_ratio, label=""pruned_ratio"")\nplt.plot(np.arange(len(recovered_ratio)), recovered_ratio, label=""recovered_ratio"")\nplt.plot(np.arange(len(w_abs_mean)), normalize(w_abs_mean), label=""w_abs_mean"")\nplt.legend(loc=4, prop={\'size\': 14})\nplt.show()\nplt.figure()\nplt.plot(np.arange(len(running_recovered)), running_recovered, label=""running_recovered"")\nplt.legend(prop={\'size\': 14})\nplt.show()\nplt.figure()\nw_abs_mean = nn.log[""w_abs_mean""]\npruned_w_abs_mean = nn.log[""pruned_w_abs_mean""]\norg_survived_w_abs_mean = nn.log[""org_w_abs_mean""]\norg_pruned_w_abs_mean = nn.log[""org_pruned_w_abs_mean""]\nplt.plot(np.arange(len(w_abs_mean)), w_abs_mean, label=""w_abs_mean"")\nplt.plot(np.arange(len(pruned_w_abs_mean)), pruned_w_abs_mean, label=""pruned_w_abs_mean"")\nplt.plot(np.arange(len(org_pruned_w_abs_mean)), org_pruned_w_abs_mean, label=""org_pruned_w_abs_mean"")\nplt.plot(np.arange(len(org_survived_w_abs_mean)), org_survived_w_abs_mean, label=""org_survived_w_abs_mean"")\nplt.legend(prop={\'size\': 14})\nplt.show()\nplt.figure()\npruned_w_abs_residual = nn.log[""pruned_w_abs_residual""]\nplt.plot(np.arange(len(pruned_w_abs_residual)), pruned_w_abs_residual, label=""pruned_w_abs_residual"")\nplt.legend(prop={\'size\': 14})\nplt.show()\nplt.figure()\ntarget_ratio = nn.log[""target_ratio""]\nplt.plot(np.arange(len(target_ratio)), target_ratio, label=""target_ratio"")\nplt.legend(prop={\'size\': 14})\nplt.show()\nplt.figure()\nptr = nn.log[""pruned_target_ratio""]\nplt.plot(np.arange(len(ptr)), ptr, label=""pruned_target_ratio"")\nplt.legend(prop={\'size\': 14})\nplt.show()\nplt.figure()\nrwwamr = np.array(nn.log[""rwwamr""])\nrwowamr = np.array(nn.log[""rwowamr""])\nrwnwamr = np.array(nn.log[""rwnwamr""])\nplt.plot(np.arange(len(rwwamr)), rwwamr, label=""rwwamr"")\nplt.plot(np.arange(len(rwowamr)), rwowamr, label=""rwowamr"")\nplt.plot(np.arange(len(rwnwamr)), rwnwamr, label=""rwnwamr"")\nplt.legend(prop={\'size\': 14})\nplt.show()\nplt.figure()\naccs = nn.log[""acc""]\npwamr = np.array(nn.log[""pruned_w_abs_mean""]) / np.array(nn.log[""w_abs_mean""])\nplt.plot(np.arange(len(pwamr)), pwamr, label=""pruned_w_abs_mean_ratio"")\nplt.plot(np.arange(len(accs)), accs, label=""acc"")\nplt.legend()\nplt.show()\nplt.figure()\npwamr = np.array(nn.log[""pruned_w_abs_mean""]) / np.array(nn.log[""w_abs_mean""])\nplt.plot(np.arange(len(pwamr)), pwamr, label=""pruned_w_abs_mean_ratio"")\nplt.legend(prop={\'size\': 14})\nplt.show()\n'"
_Dist/NeuralNetworks/_Tests/SVM/TestLinearSVM.py,0,"b'import os\nimport sys\nroot_path = os.path.abspath(""../../../../"")\nif root_path not in sys.path:\n    sys.path.append(root_path)\n\nfrom copy import deepcopy\n\nfrom Util.Util import DataUtil\nfrom _Dist.NeuralNetworks._Tests.TestUtil import draw_acc\nfrom _Dist.NeuralNetworks.f_AutoNN.DistNN import AutoAdvanced\nfrom _Dist.NeuralNetworks.b_TraditionalML.SVM import LinearSVM, AutoLinearSVM, DistLinearSVM\n\nbase_params = {""model_param_settings"": {""n_epoch"": 30, ""metric"": ""acc""}}\n(x, y), (x_test, y_test) = DataUtil.gen_noisy_linear(n_dim=2, n_valid=2, test_ratio=0.01, one_hot=False)\nsvm = LinearSVM(**deepcopy(base_params)).fit(\n    x, y, x_test, y_test, snapshot_ratio=1).visualize2d(x_test, y_test)\nnn = AutoAdvanced(""NoisyLinear"", **deepcopy(base_params), pre_process_settings={""reuse_mean_and_std"": True}).fit(\n    x, y, x_test, y_test, snapshot_ratio=1).visualize2d(x_test, y_test)\ndraw_acc(svm, nn)\n\nbase_params[""data_info""] = {""data_folder"": ""../_Data""}\nsvm = AutoLinearSVM(""mushroom"", **deepcopy(base_params)).fit(snapshot_ratio=0)\nnn = AutoAdvanced(""mushroom"", **deepcopy(base_params)).fit(snapshot_ratio=0)\ndraw_acc(svm, nn)\n\nbase_params[""data_info""][""file_type""] = ""csv""\nsvm = AutoLinearSVM(""Adult"", **deepcopy(base_params)).fit(snapshot_ratio=0)\nnn = AutoAdvanced(""Adult"", **deepcopy(base_params)).fit(snapshot_ratio=0)\ndraw_acc(svm, nn)\n\nDistLinearSVM(""Adult"", **deepcopy(base_params)).k_random()\n'"
_Dist/NeuralNetworks/_Tests/SVM/TestOpenML.py,1,"b'import os\nimport sys\nroot_path = os.path.abspath(""./"")\nif root_path not in sys.path:\n    sys.path.append(root_path)\n\nimport copy\nimport numpy as np\nimport tensorflow as tf\n\nfrom openml import datasets\n\nfrom _Dist.NeuralNetworks.b_TraditionalML.SVM import DistLinearSVM\n\nGPU_ID = None\nK_RANDOM = 9\nIDS = [\n    38, 46, 179,\n    184, 389, 554,\n    772, 917, 1049,\n    1111, 1120, 1128,\n    293,\n]\n\n\ndef swap(arr, i1, i2):\n    arr[..., i1], arr[..., i2] = arr[..., i2], arr[..., i1].copy()\n\n\ndef download_data():\n    data_folder = ""_Data""\n    idx_folder = os.path.join(data_folder, ""idx"")\n    if not os.path.isdir(data_folder):\n        os.makedirs(data_folder)\n    if not os.path.isdir(idx_folder):\n        os.makedirs(idx_folder)\n    for idx in IDS:\n        print(""Downloading {}"".format(idx))\n        data_file = os.path.join(data_folder, ""{}.txt"".format(idx))\n        idx_file = os.path.join(idx_folder, ""{}.npy"".format(idx))\n        if os.path.isfile(data_file) and os.path.isfile(idx_file):\n            continue\n        dataset = datasets.get_dataset(idx)\n        data, categorical_idx, names = dataset.get_data(\n            return_categorical_indicator=True,\n            return_attribute_names=True\n        )\n        data = data.toarray() if not isinstance(data, np.ndarray) else data\n        target_idx = names.index(dataset.default_target_attribute)\n        numerical_idx = ~np.array(categorical_idx)\n        swap(numerical_idx, target_idx, -1)\n        swap(data, target_idx, -1)\n        with open(data_file, ""w"") as file:\n            file.write(""\\n"".join(["" "".join(map(lambda n: str(n), line)) for line in data]))\n        np.save(idx_file, numerical_idx)\n\n\ndef main():\n    base_params = {\n        ""data_info"": {},\n        ""model_param_settings"": {},\n        # ""model_param_settings"": {""n_epoch"": 1, ""max_epoch"": 1},\n        # ""model_structure_settings"": {""use_wide_network"": False, ""use_pruner"": False}\n    }\n    config = tf.ConfigProto()\n    config.gpu_options.allow_growth = True\n    if GPU_ID is not None:\n        os.environ[""CUDA_VISIBLE_DEVICES""] = GPU_ID\n    base_params[""model_param_settings""][""sess_config""] = config\n    for idx in IDS:\n        numerical_idx = np.load(""_Data/idx/{}.npy"".format(idx))\n        local_params = copy.deepcopy(base_params)\n        local_params[""name""] = str(idx)\n        local_params[""data_info""][""numerical_idx""] = numerical_idx\n        DistLinearSVM(**copy.deepcopy(local_params)).k_random(K_RANDOM, cv_rate=0.1, test_rate=0.1)\n\n\nif __name__ == \'__main__\':\n    download_data()\n    main()\n'"
_Dist/NeuralNetworks/_Tests/SVM/TestSVM.py,0,"b'import os\nimport sys\nroot_path = os.path.abspath(""../../../../"")\nif root_path not in sys.path:\n    sys.path.append(root_path)\n\nfrom copy import deepcopy\n\nfrom Util.Util import DataUtil\nfrom _Dist.NeuralNetworks._Tests.TestUtil import draw_acc\nfrom _Dist.NeuralNetworks.b_TraditionalML.SVM import SVM\nfrom _Dist.NeuralNetworks.f_AutoNN.DistNN import AutoAdvanced\n\nbase_params = {""model_param_settings"": {""n_epoch"": 30, ""metric"": ""acc""}}\n(x, y), (x_test, y_test) = DataUtil.gen_noisy_linear(n_dim=2, n_valid=2, test_ratio=0.01, one_hot=False)\nsvm = SVM(**deepcopy(base_params)).fit(\n    x, y, x_test, y_test, snapshot_ratio=1).visualize2d(x_test, y_test)\nnn = AutoAdvanced(""NoisyLinear"", **deepcopy(base_params), pre_process_settings={""reuse_mean_and_std"": True}).fit(\n    x, y, x_test, y_test, snapshot_ratio=1).visualize2d(x_test, y_test)\ndraw_acc(svm, nn)\n'"
_Dist/NeuralNetworks/_Tests/_UnitTests/UnitTestUtil.py,0,"b'import os\nimport shutil\n\nroot_cwd = os.path.abspath(""../"")\n\n\ndef clear_cache():\n    cwd = os.getcwd()\n    local_data_folder = os.path.join(cwd, ""_Data"")\n    if os.path.isdir(local_data_folder):\n        shutil.rmtree(local_data_folder)\n    shutil.rmtree(os.path.join(cwd, ""_Models""))\n    remote_cache_folder = os.path.join(root_cwd, ""_Data"", ""_Cache"")\n    remote_info_folder = os.path.join(root_cwd, ""_Data"", ""_DataInfo"")\n    if os.path.isdir(remote_cache_folder):\n        shutil.rmtree(remote_cache_folder)\n    if os.path.isdir(remote_info_folder):\n        shutil.rmtree(remote_info_folder)\n'"
_Dist/NeuralNetworks/_Tests/_UnitTests/a_Basic.py,0,"b'import os\nimport sys\nroot_path = os.path.abspath(""../../../../"")\nif root_path not in sys.path:\n    sys.path.append(root_path)\n\nimport copy\nimport unittest\nimport numpy as np\n\nfrom Util.Util import DataUtil\nfrom _Dist.NeuralNetworks.c_BasicNN.NN import Basic\nfrom _Dist.NeuralNetworks.b_TraditionalML.SVM import LinearSVM, SVM\nfrom _Dist.NeuralNetworks._Tests._UnitTests.UnitTestUtil import clear_cache\n\n\nbase_params = {\n    ""name"": ""UnitTest"",\n    ""model_param_settings"": {""n_epoch"": 3, ""max_epoch"": 5}\n}\nsvm = SVM(**copy.deepcopy(base_params))\nnn = Basic(**copy.deepcopy(base_params))\nlinear_svm = LinearSVM(**copy.deepcopy(base_params))\ntrain_set, cv_set, test_set = DataUtil.gen_special_linear(1000, 2, 2, 2, one_hot=False)\n\n\nclass TestSVM(unittest.TestCase):\n    def test_00_train(self):\n        self.assertIsInstance(\n            svm.fit(*train_set, *cv_set, verbose=0), SVM,\n            msg=""Train failed""\n        )\n\n    def test_01_predict(self):\n        self.assertIs(svm.predict(train_set[0]).dtype, np.dtype(""float32""), ""Predict failed"")\n        self.assertIs(svm.predict_classes(cv_set[0]).dtype, np.dtype(""int32""), ""Predict classes failed"")\n\n    def test_02_evaluate(self):\n        self.assertEqual(len(svm.evaluate(*train_set, *cv_set)), 3, ""Evaluation failed"")\n\n    def test_03_save(self):\n        self.assertIsInstance(svm.save(), SVM, msg=""Save failed"")\n\n    def test_04_load(self):\n        global svm\n        svm = SVM(base_params[""name""]).load()\n        self.assertIsInstance(svm, SVM, ""Load failed"")\n\n    def test_05_re_predict(self):\n        self.assertIs(svm.predict(train_set[0]).dtype, np.dtype(""float32""), ""Re-Predict failed"")\n        self.assertIs(svm.predict_classes(cv_set[0]).dtype, np.dtype(""int32""), ""Re-Predict classes failed"")\n\n    def test_06_re_evaluate(self):\n        self.assertEqual(len(svm.evaluate(*train_set, *cv_set)), 3, ""Re-Evaluation failed"")\n\n    def test_07_re_train(self):\n        self.assertIsInstance(\n            svm.fit(*train_set, *cv_set, verbose=0), SVM,\n            msg=""Re-Train failed""\n        )\n\n    def test_99_clear_cache(self):\n        clear_cache()\n\n\nclass TestBasicNN(unittest.TestCase):\n    def test_00_train(self):\n        self.assertIsInstance(\n            nn.fit(*train_set, *cv_set, verbose=0), Basic,\n            msg=""Train failed""\n        )\n\n    def test_01_predict(self):\n        self.assertIs(nn.predict(train_set[0]).dtype, np.dtype(""float32""), ""Predict failed"")\n        self.assertIs(nn.predict_classes(cv_set[0]).dtype, np.dtype(""int32""), ""Predict classes failed"")\n\n    def test_02_evaluate(self):\n        self.assertEqual(len(nn.evaluate(*train_set, *cv_set)), 3, ""Evaluation failed"")\n\n    def test_03_save(self):\n        self.assertIsInstance(nn.save(), Basic, msg=""Save failed"")\n\n    def test_04_load(self):\n        global nn\n        nn = Basic(base_params[""name""]).load()\n        self.assertIsInstance(nn, Basic, ""Load failed"")\n\n    def test_05_re_predict(self):\n        self.assertIs(nn.predict(train_set[0]).dtype, np.dtype(""float32""), ""Re-Predict failed"")\n        self.assertIs(nn.predict_classes(cv_set[0]).dtype, np.dtype(""int32""), ""Re-Predict classes failed"")\n\n    def test_06_re_evaluate(self):\n        self.assertEqual(len(nn.evaluate(*train_set, *cv_set)), 3, ""Re-Evaluation failed"")\n\n    def test_07_re_train(self):\n        self.assertIsInstance(\n            nn.fit(*train_set, *cv_set, verbose=0), Basic,\n            msg=""Re-Train failed""\n        )\n\n    def test_99_clear_cache(self):\n        clear_cache()\n\n\nclass TestLinearSVM(unittest.TestCase):\n    def test_00_train(self):\n        self.assertIsInstance(\n            linear_svm.fit(*train_set, *cv_set, verbose=0), LinearSVM,\n            msg=""Train failed""\n        )\n\n    def test_01_predict(self):\n        self.assertIs(linear_svm.predict(train_set[0]).dtype, np.dtype(""float32""), ""Predict failed"")\n        self.assertIs(linear_svm.predict_classes(cv_set[0]).dtype, np.dtype(""int32""), ""Predict classes failed"")\n        self.assertIs(linear_svm.predict_classes(test_set[0]).dtype, np.dtype(""int32""), ""Predict classes failed"")\n\n    def test_02_evaluate(self):\n        self.assertEqual(len(linear_svm.evaluate(*train_set, *cv_set, *test_set)), 3, ""Evaluation failed"")\n\n    def test_03_save(self):\n        self.assertIsInstance(linear_svm.save(), LinearSVM, msg=""Save failed"")\n\n    def test_04_load(self):\n        global linear_svm\n        linear_svm = LinearSVM(base_params[""name""]).load()\n        self.assertIsInstance(linear_svm, LinearSVM, ""Load failed"")\n\n    def test_05_re_predict(self):\n        self.assertIs(linear_svm.predict(train_set[0]).dtype, np.dtype(""float32""), ""Re-Predict failed"")\n        self.assertIs(linear_svm.predict_classes(cv_set[0]).dtype, np.dtype(""int32""), ""Re-Predict classes failed"")\n        self.assertIs(linear_svm.predict_classes(test_set[0]).dtype, np.dtype(""int32""), ""Re-Predict classes failed"")\n\n    def test_06_re_evaluate(self):\n        self.assertEqual(len(linear_svm.evaluate(*train_set, *cv_set, *test_set)), 3, ""Re-Evaluation failed"")\n\n    def test_07_re_train(self):\n        self.assertIsInstance(\n            linear_svm.fit(*train_set, *cv_set, verbose=0), LinearSVM,\n            msg=""Re-Train failed""\n        )\n\n    def test_99_clear_cache(self):\n        clear_cache()\n\n\nif __name__ == \'__main__\':\n    unittest.main()\n'"
_Dist/NeuralNetworks/_Tests/_UnitTests/b_Advanced.py,0,"b'import os\nimport sys\nroot_path = os.path.abspath(""../../../../"")\nif root_path not in sys.path:\n    sys.path.append(root_path)\n\nimport unittest\nimport numpy as np\n\nfrom Util.Util import DataUtil\nfrom _Dist.NeuralNetworks.e_AdvancedNN.NN import Advanced\nfrom _Dist.NeuralNetworks._Tests._UnitTests.UnitTestUtil import clear_cache\n\n\nbase_params = {\n    ""name"": ""UnitTest"",\n    ""data_info"": {\n        ""numerical_idx"": [True] * 6 + [False],\n        ""categorical_columns"": []\n    },\n    ""model_param_settings"": {""n_epoch"": 3, ""max_epoch"": 5}\n}\nnn = Advanced(**base_params)\ntrain_set, cv_set, test_set = DataUtil.gen_special_linear(1000, 2, 2, 2, one_hot=False)\n\n\nclass TestAdvancedNN(unittest.TestCase):\n    def test_00_train(self):\n        self.assertIsInstance(\n            nn.fit(*train_set, *cv_set, verbose=0), Advanced,\n            msg=""Train failed""\n        )\n\n    def test_01_predict(self):\n        self.assertIs(nn.predict(train_set[0]).dtype, np.dtype(""float32""), ""Predict failed"")\n        self.assertIs(nn.predict_classes(cv_set[0]).dtype, np.dtype(""int32""), ""Predict classes failed"")\n        self.assertIs(nn.predict_classes(test_set[0]).dtype, np.dtype(""int32""), ""Predict classes failed"")\n\n    def test_02_evaluate(self):\n        self.assertEqual(len(nn.evaluate(*train_set, *cv_set, *test_set)), 3, ""Evaluation failed"")\n\n    def test_03_save(self):\n        self.assertIsInstance(nn.save(), Advanced, msg=""Save failed"")\n\n    def test_04_load(self):\n        global nn\n        nn = Advanced(**base_params).load()\n        self.assertIsInstance(nn, Advanced, ""Load failed"")\n\n    def test_05_re_predict(self):\n        self.assertIs(nn.predict(train_set[0]).dtype, np.dtype(""float32""), ""Re-Predict failed"")\n        self.assertIs(nn.predict_classes(cv_set[0]).dtype, np.dtype(""int32""), ""Re-Predict classes failed"")\n        self.assertIs(nn.predict_classes(test_set[0]).dtype, np.dtype(""int32""), ""Re-Predict classes failed"")\n\n    def test_06_re_evaluate(self):\n        self.assertEqual(len(nn.evaluate(*train_set, *cv_set, *test_set)), 3, ""Re-Evaluation failed"")\n\n    def test_07_re_train(self):\n        self.assertIsInstance(\n            nn.fit(*train_set, *cv_set, verbose=0), Advanced,\n            msg=""Re-Train failed""\n        )\n\n    def test_99_clear_cache(self):\n        clear_cache()\n\n\nif __name__ == \'__main__\':\n    unittest.main()\n'"
_Dist/NeuralNetworks/_Tests/_UnitTests/c_Auto.py,0,"b'import os\nimport sys\nroot_path = os.path.abspath(""../../../../"")\nif root_path not in sys.path:\n    sys.path.append(root_path)\n\nimport copy\nimport shutil\nimport unittest\nimport numpy as np\n\nfrom Util.Util import DataUtil\nfrom _Dist.NeuralNetworks.b_TraditionalML.SVM import AutoLinearSVM\nfrom _Dist.NeuralNetworks.f_AutoNN.NN import AutoBasic, AutoAdvanced\nfrom _Dist.NeuralNetworks._Tests._UnitTests.UnitTestUtil import clear_cache\n\n\nbase_params = {\n    ""name"": ""UnitTest"", ""data_info"": {},\n    ""model_param_settings"": {""n_epoch"": 1, ""max_epoch"": 2}\n}\nnn = AutoAdvanced(**copy.deepcopy(base_params))\nbasic_nn = AutoBasic(**copy.deepcopy(base_params))\nlinear_svm = AutoLinearSVM(**copy.deepcopy(base_params))\ntrain_set, cv_set, test_set = DataUtil.gen_special_linear(1000, 2, 2, 2, one_hot=False)\n\nauto_mushroom_params = copy.deepcopy(base_params)\nauto_mushroom_params[""name""] = ""mushroom""\nauto_mushroom_params[""data_info""][""file_type""] = ""txt""\nauto_mushroom_params[""data_info""][""data_folder""] = ""../_Data""\nmushroom_labels = {""p"", ""e""}\nmushroom_file = ""../_Data/mushroom""\nauto_mushroom_nn = AutoAdvanced(**copy.deepcopy(auto_mushroom_params))\nauto_mushroom_basic_nn = AutoBasic(**copy.deepcopy(auto_mushroom_params))\nauto_mushroom_linear_svm = AutoLinearSVM(**copy.deepcopy(auto_mushroom_params))\n\nauto_adult_params = copy.deepcopy(base_params)\nauto_adult_params[""name""] = ""Adult""\nauto_adult_params[""data_info""][""file_type""] = ""csv""\nauto_adult_params[""data_info""][""data_folder""] = ""../_Data""\nadult_file = ""../_Data/Adult/test""\nauto_adult_nn = AutoAdvanced(**copy.deepcopy(auto_adult_params))\nauto_adult_basic_nn = AutoBasic(**copy.deepcopy(auto_adult_params))\nauto_adult_linear_svm = AutoLinearSVM(**copy.deepcopy(auto_adult_params))\n\nauto_lmgpip_params = copy.deepcopy(base_params)\nauto_lmgpip_params[""name""] = ""lmgpip""\nauto_lmgpip_params[""data_info""][""file_type""] = ""txt""\nauto_lmgpip_params[""data_info""][""data_folder""] = ""../_Data""\nlmgpip_test_file = ""../_Data/lmgpip_test""\nauto_lmgpip_nn = AutoAdvanced(**auto_lmgpip_params)\nauto_lmgpip_basic_nn = AutoBasic(**auto_lmgpip_params)\n\nauto_lmgpip_regressor_params = copy.deepcopy(auto_lmgpip_params)\nauto_lmgpip_regressor_params[""data_info""][""is_regression""] = True\nauto_lmgpip_regressor = AutoAdvanced(**auto_lmgpip_regressor_params)\nauto_lmgpip_basic_regressor = AutoBasic(**auto_lmgpip_regressor_params)\n\n\nclass TestAutoNN(unittest.TestCase):\n    def test_00_train_from_numpy(self):\n        self.assertIsInstance(\n            nn.fit(*train_set, *cv_set, verbose=0), AutoAdvanced,\n            msg=""Train failed""\n        )\n        self.assertIsInstance(\n            basic_nn.fit(*train_set, *cv_set, verbose=0), AutoBasic,\n            msg=""Train failed""\n        )\n\n    def test_01_predict(self):\n        self.assertIs(nn.predict(train_set[0]).dtype, np.dtype(""float32""), ""Predict failed"")\n        self.assertIs(basic_nn.predict(train_set[0]).dtype, np.dtype(""float32""), ""Predict failed"")\n        self.assertIs(nn.predict_classes(cv_set[0]).dtype, np.dtype(""int32""), ""Predict classes failed"")\n        self.assertIs(basic_nn.predict_classes(cv_set[0]).dtype, np.dtype(""int32""), ""Predict classes failed"")\n        self.assertIs(nn.predict_classes(test_set[0]).dtype, np.dtype(""int32""), ""Predict classes failed"")\n        self.assertIs(basic_nn.predict_classes(test_set[0]).dtype, np.dtype(""int32""), ""Predict classes failed"")\n\n    def test_02_evaluate(self):\n        self.assertEqual(len(nn.evaluate(*train_set, *cv_set, *test_set)), 3, ""Evaluation failed"")\n        self.assertEqual(len(basic_nn.evaluate(*train_set, *cv_set, *test_set)), 3, ""Evaluation failed"")\n\n    def test_03_save(self):\n        self.assertIsInstance(nn.save(), AutoAdvanced, msg=""Save failed"")\n        self.assertIsInstance(basic_nn.save(), AutoBasic, msg=""Save failed"")\n\n    def test_04_load(self):\n        global nn, basic_nn\n        nn = AutoAdvanced(**base_params).load()\n        basic_nn = AutoBasic(**base_params).load()\n        self.assertIsInstance(nn, AutoAdvanced, ""Load failed"")\n        self.assertIsInstance(basic_nn, AutoBasic, ""Load failed"")\n\n    def test_05_re_predict(self):\n        self.assertIs(nn.predict(train_set[0]).dtype, np.dtype(""float32""), ""Re-Predict failed"")\n        self.assertIs(basic_nn.predict(train_set[0]).dtype, np.dtype(""float32""), ""Re-Predict failed"")\n        self.assertIs(nn.predict_classes(cv_set[0]).dtype, np.dtype(""int32""), ""Re-Predict classes failed"")\n        self.assertIs(basic_nn.predict_classes(cv_set[0]).dtype, np.dtype(""int32""), ""Re-Predict classes failed"")\n        self.assertIs(nn.predict_classes(test_set[0]).dtype, np.dtype(""int32""), ""Re-Predict classes failed"")\n        self.assertIs(basic_nn.predict_classes(test_set[0]).dtype, np.dtype(""int32""), ""Re-Predict classes failed"")\n\n    def test_06_re_evaluate(self):\n        self.assertEqual(len(nn.evaluate(*train_set, *cv_set, *test_set)), 3, ""Re-Evaluation failed"")\n        self.assertEqual(len(basic_nn.evaluate(*train_set, *cv_set, *test_set)), 3, ""Re-Evaluation failed"")\n\n    def test_07_re_train(self):\n        self.assertIsInstance(\n            nn.fit(*train_set, *cv_set, verbose=0), AutoAdvanced,\n            msg=""Re-Train failed""\n        )\n        self.assertIsInstance(\n            basic_nn.fit(*train_set, *cv_set, verbose=0), AutoBasic,\n            msg=""Re-Train failed""\n        )\n\n    def test_08_train_from_txt(self):\n        self.assertIsInstance(\n            auto_mushroom_nn.fit(verbose=0), AutoAdvanced,\n            msg=""Train failed""\n        )\n        self.assertIsInstance(\n            auto_mushroom_basic_nn.fit(verbose=0), AutoBasic,\n            msg=""Train failed""\n        )\n\n    def test_09_predict_from_txt(self):\n        for local_nn in (auto_mushroom_nn, auto_mushroom_basic_nn):\n            self.assertIs(\n                local_nn.predict_from_file(\n                    mushroom_file, ""txt"", include_label=True\n                ).dtype, np.dtype(""float32""), msg=""Predict failed""\n            )\n            self.assertIs(\n                local_nn.predict_classes_from_file(\n                    mushroom_file, ""txt"", include_label=True\n                ).dtype, np.dtype(""int32""), msg=""Predict classes failed""\n            )\n            predict_labels = np.unique(local_nn.predict_labels_from_file(\n                mushroom_file, ""txt"", include_label=True\n            ))\n            for pred_label in predict_labels:\n                self.assertIn(pred_label, mushroom_labels, msg=""Predict labels failed"")\n\n    def test_10_save(self):\n        self.assertIsInstance(auto_mushroom_nn.save(), AutoAdvanced, msg=""Save failed"")\n        self.assertIsInstance(auto_mushroom_basic_nn.save(), AutoBasic, msg=""Save failed"")\n\n    def test_11_load(self):\n        global auto_mushroom_nn, auto_mushroom_basic_nn\n        auto_mushroom_nn = AutoAdvanced(**auto_mushroom_params).load()\n        auto_mushroom_basic_nn = AutoBasic(**auto_mushroom_params).load()\n        self.assertIsInstance(auto_mushroom_nn, AutoAdvanced, ""Load failed"")\n        self.assertIsInstance(auto_mushroom_basic_nn, AutoBasic, ""Load failed"")\n\n    def test_12_re_predict_from_txt(self):\n        for local_nn in (auto_mushroom_nn, auto_mushroom_basic_nn):\n            self.assertIs(\n                local_nn.predict_from_file(\n                    mushroom_file, ""txt"", include_label=True\n                ).dtype, np.dtype(""float32""), msg=""Re-Predict failed""\n            )\n            self.assertIs(\n                local_nn.predict_classes_from_file(\n                    mushroom_file, ""txt"", include_label=True\n                ).dtype, np.dtype(""int32""), msg=""Re-Predict classes failed""\n            )\n            predict_labels = np.unique(local_nn.predict_labels_from_file(\n                mushroom_file, ""txt"", include_label=True\n            ))\n            for pred_label in predict_labels:\n                self.assertIn(pred_label, mushroom_labels, msg=""Re-Predict labels failed"")\n\n    def test_13_re_train_from_txt(self):\n        self.assertIsInstance(\n            auto_mushroom_nn.fit(verbose=0), AutoAdvanced,\n            msg=""Re-Train failed""\n        )\n        self.assertIsInstance(\n            auto_mushroom_basic_nn.fit(verbose=0), AutoBasic,\n            msg=""Re-Train failed""\n        )\n\n    def test_14_train_from_csv(self):\n        self.assertIsInstance(\n            auto_adult_nn.fit(verbose=0), AutoAdvanced,\n            msg=""Train failed""\n        )\n        self.assertIsInstance(\n            auto_adult_basic_nn.fit(verbose=0), AutoBasic,\n            msg=""Train failed""\n        )\n\n    def test_15_predict_from_csv(self):\n        for local_nn in (auto_adult_nn, auto_adult_basic_nn):\n            self.assertIs(\n                local_nn.predict_from_file(\n                    adult_file, ""csv"", include_label=True\n                ).dtype, np.dtype(""float32""), msg=""Predict failed""\n            )\n            self.assertIs(\n                local_nn.predict_classes_from_file(\n                    adult_file, ""csv"", include_label=True\n                ).dtype, np.dtype(""int32""), msg=""Predict classes failed""\n            )\n\n    def test_16_save(self):\n        self.assertIsInstance(auto_adult_nn.save(), AutoAdvanced, msg=""Save failed"")\n        self.assertIsInstance(auto_adult_basic_nn.save(), AutoBasic, msg=""Save failed"")\n\n    def test_17_load(self):\n        global auto_adult_nn, auto_adult_basic_nn\n        auto_adult_nn = AutoAdvanced(**auto_adult_params).load()\n        auto_adult_basic_nn = AutoBasic(**auto_adult_params).load()\n        self.assertIsInstance(auto_adult_nn, AutoAdvanced, ""Load failed"")\n        self.assertIsInstance(auto_adult_basic_nn, AutoBasic, ""Load failed"")\n\n    def test_18_re_predict_from_csv(self):\n        for local_nn in (auto_adult_nn, auto_adult_basic_nn):\n            self.assertIs(\n                local_nn.predict_from_file(\n                    adult_file, ""csv"", include_label=True\n                ).dtype, np.dtype(""float32""), msg=""Re-Predict failed""\n            )\n            self.assertIs(\n                local_nn.predict_classes_from_file(\n                    adult_file, ""csv"", include_label=True\n                ).dtype, np.dtype(""int32""), msg=""Re-Predict classes failed""\n            )\n\n    def test_19_re_train_from_csv(self):\n        self.assertIsInstance(\n            auto_adult_nn.fit(verbose=0), AutoAdvanced,\n            msg=""Re-Train failed""\n        )\n        self.assertIsInstance(\n            auto_adult_basic_nn.fit(verbose=0), AutoBasic,\n            msg=""Re-Train failed""\n        )\n\n    def test_20_train_from_mixed_features(self):\n        self.assertIsInstance(\n            auto_lmgpip_nn.fit(verbose=0), AutoAdvanced,\n            msg=""Train failed""\n        )\n        self.assertIsInstance(\n            auto_lmgpip_basic_nn.fit(verbose=0), AutoBasic,\n            msg=""Train failed""\n        )\n\n    def test_21_predict_from_mixed_features(self):\n        for local_nn in (auto_lmgpip_nn, auto_lmgpip_basic_nn):\n            self.assertIs(\n                local_nn.predict_from_file(\n                    lmgpip_test_file, ""txt"", include_label=False\n                ).dtype, np.dtype(""float32""), msg=""Predict failed""\n            )\n            self.assertIs(\n                local_nn.predict_classes_from_file(\n                    lmgpip_test_file, ""txt"", include_label=False\n                ).dtype, np.dtype(""int32""), msg=""Predict classes failed""\n            )\n\n    def test_22_save(self):\n        self.assertIsInstance(auto_lmgpip_nn.save(), AutoAdvanced, msg=""Save failed"")\n        self.assertIsInstance(auto_lmgpip_basic_nn.save(), AutoBasic, msg=""Save failed"")\n\n    def test_23_load(self):\n        global auto_lmgpip_nn, auto_lmgpip_basic_nn\n        auto_lmgpip_nn = AutoAdvanced(**auto_lmgpip_params).load()\n        auto_lmgpip_basic_nn = AutoBasic(**auto_lmgpip_params).load()\n        self.assertIsInstance(auto_lmgpip_nn, AutoAdvanced, ""Load failed"")\n        self.assertIsInstance(auto_lmgpip_basic_nn, AutoBasic, ""Load failed"")\n\n    def test_24_re_predict_from_mixed_features(self):\n        for local_nn in (auto_lmgpip_nn, auto_lmgpip_basic_nn):\n            self.assertIs(\n                local_nn.predict_from_file(\n                    lmgpip_test_file, ""txt"", include_label=False\n                ).dtype, np.dtype(""float32""), msg=""Re-Predict failed""\n            )\n            self.assertIs(\n                local_nn.predict_classes_from_file(\n                    lmgpip_test_file, ""txt"", include_label=False\n                ).dtype, np.dtype(""int32""), msg=""Re-Predict classes failed""\n            )\n\n    def test_25_re_train_from_mixed_features(self):\n        self.assertIsInstance(\n            auto_lmgpip_nn.fit(verbose=0), AutoAdvanced,\n            msg=""Re-Train failed""\n        )\n        self.assertIsInstance(\n            auto_lmgpip_basic_nn.fit(verbose=0), AutoBasic,\n            msg=""Re-Train failed""\n        )\n\n    def test_26_train_regressor_from_mixed_features(self):\n        shutil.rmtree(""../_Data/_Cache"")\n        shutil.rmtree(""../_Data/_DataInfo"")\n        self.assertIsInstance(\n            auto_lmgpip_regressor.fit(verbose=0), AutoAdvanced,\n            msg=""Train failed""\n        )\n        self.assertIsInstance(\n            auto_lmgpip_basic_regressor.fit(verbose=0), AutoBasic,\n            msg=""Train failed""\n        )\n\n    def test_27_predict_regressor_from_mixed_features(self):\n        for local_nn in (auto_lmgpip_regressor, auto_lmgpip_basic_regressor):\n            self.assertIs(\n                local_nn.predict_from_file(\n                    lmgpip_test_file, ""txt"", include_label=False\n                ).dtype, np.dtype(""float32""), msg=""Predict failed""\n            )\n            with self.assertRaises(ValueError):\n                local_nn.predict_classes_from_file(\n                    lmgpip_test_file, ""txt"", include_label=False\n                )\n\n    def test_28_save(self):\n        self.assertIsInstance(auto_lmgpip_regressor.save(), AutoAdvanced, msg=""Save failed"")\n        self.assertIsInstance(auto_lmgpip_basic_regressor.save(), AutoBasic, msg=""Save failed"")\n\n    def test_29_load(self):\n        global auto_lmgpip_regressor, auto_lmgpip_basic_regressor\n        auto_lmgpip_regressor = AutoAdvanced(**auto_lmgpip_regressor_params).load()\n        auto_lmgpip_basic_regressor = AutoBasic(**auto_lmgpip_regressor_params).load()\n        self.assertIsInstance(auto_lmgpip_regressor, AutoAdvanced, ""Load failed"")\n        self.assertIsInstance(auto_lmgpip_basic_regressor, AutoBasic, ""Load failed"")\n\n    def test_30_re_predict_regressor_from_mixed_features(self):\n        for local_nn in (auto_lmgpip_regressor, auto_lmgpip_basic_regressor):\n            self.assertIs(\n                local_nn.predict_from_file(\n                    lmgpip_test_file, ""txt"", include_label=False\n                ).dtype, np.dtype(""float32""), msg=""Re-Predict failed""\n            )\n            with self.assertRaises(ValueError):\n                local_nn.predict_classes_from_file(\n                    lmgpip_test_file, ""txt"", include_label=False\n                )\n\n    def test_31_re_train_regressor_from_mixed_features(self):\n        self.assertIsInstance(\n            auto_lmgpip_regressor.fit(verbose=0), AutoAdvanced,\n            msg=""Re-Train failed""\n        )\n        self.assertIsInstance(\n            auto_lmgpip_basic_regressor.fit(verbose=0), AutoBasic,\n            msg=""Re-Train failed""\n        )\n\n    def test_99_clear_cache(self):\n        clear_cache()\n\n\nclass TestAutoLinearSVM(unittest.TestCase):\n    def test_00_train_from_numpy(self):\n        self.assertIsInstance(\n            linear_svm.fit(*train_set, *cv_set, verbose=0), AutoLinearSVM,\n            msg=""Train failed""\n        )\n\n    def test_01_predict(self):\n        self.assertIs(linear_svm.predict(train_set[0]).dtype, np.dtype(""float32""), ""Predict failed"")\n        self.assertIs(linear_svm.predict(cv_set[0]).dtype, np.dtype(""float32""), ""Predict failed"")\n        self.assertIs(linear_svm.predict(test_set[0]).dtype, np.dtype(""float32""), ""Predict failed"")\n\n    def test_02_evaluate(self):\n        self.assertEqual(len(linear_svm.evaluate(*train_set, *cv_set, *test_set)), 3, ""Evaluation failed"")\n\n    def test_03_save(self):\n        self.assertIsInstance(linear_svm.save(), AutoLinearSVM, msg=""Save failed"")\n\n    def test_04_load(self):\n        global linear_svm\n        linear_svm = AutoLinearSVM(**base_params).load()\n        self.assertIsInstance(linear_svm, AutoLinearSVM, ""Load failed"")\n\n    def test_05_re_predict(self):\n        self.assertIs(linear_svm.predict(train_set[0]).dtype, np.dtype(""float32""), ""Re-Predict failed"")\n        self.assertIs(linear_svm.predict(cv_set[0]).dtype, np.dtype(""float32""), ""Re-Predict failed"")\n        self.assertIs(linear_svm.predict(test_set[0]).dtype, np.dtype(""float32""), ""Re-Predict failed"")\n\n    def test_06_re_evaluate(self):\n        self.assertEqual(len(linear_svm.evaluate(*train_set, *cv_set, *test_set)), 3, ""Re-Evaluation failed"")\n\n    def test_07_re_train(self):\n        self.assertIsInstance(\n            linear_svm.fit(*train_set, *cv_set, verbose=0), AutoLinearSVM,\n            msg=""Re-Train failed""\n        )\n\n    def test_08_train_from_txt(self):\n        self.assertIsInstance(\n            auto_mushroom_linear_svm.fit(verbose=0), AutoLinearSVM,\n            msg=""Train failed""\n        )\n\n    def test_09_predict_from_txt(self):\n        self.assertIs(\n            auto_mushroom_linear_svm.predict_from_file(\n                mushroom_file, ""txt"", include_label=True\n            ).dtype, np.dtype(""float32""), msg=""Predict failed""\n        )\n\n    def test_10_save(self):\n        self.assertIsInstance(auto_mushroom_linear_svm.save(), AutoLinearSVM, msg=""Save failed"")\n\n    def test_11_load(self):\n        global auto_mushroom_linear_svm\n        auto_mushroom_linear_svm = AutoLinearSVM(**auto_mushroom_params).load()\n        self.assertIsInstance(auto_mushroom_linear_svm, AutoLinearSVM, ""Load failed"")\n\n    def test_12_re_predict_from_txt(self):\n        self.assertIs(\n            auto_mushroom_linear_svm.predict_from_file(\n                mushroom_file, ""txt"", include_label=True\n            ).dtype, np.dtype(""float32""), msg=""Re-Predict failed""\n        )\n\n    def test_13_re_train_from_txt(self):\n        self.assertIsInstance(\n            auto_mushroom_linear_svm.fit(verbose=0), AutoLinearSVM,\n            msg=""Re-Train failed""\n        )\n\n    def test_14_train_from_csv(self):\n        self.assertIsInstance(\n            auto_adult_linear_svm.fit(verbose=0), AutoLinearSVM,\n            msg=""Train failed""\n        )\n\n    def test_15_predict_from_csv(self):\n        self.assertIs(\n            auto_adult_linear_svm.predict_from_file(\n                adult_file, ""csv"", include_label=True\n            ).dtype, np.dtype(""float32""), msg=""Predict failed""\n        )\n\n    def test_16_save(self):\n        self.assertIsInstance(auto_adult_linear_svm.save(), AutoLinearSVM, msg=""Save failed"")\n\n    def test_17_load(self):\n        global auto_adult_linear_svm\n        auto_adult_linear_svm = AutoLinearSVM(**auto_adult_params).load()\n        self.assertIsInstance(auto_adult_linear_svm, AutoLinearSVM, ""Load failed"")\n\n    def test_18_re_predict_from_csv(self):\n        self.assertIs(\n            auto_adult_linear_svm.predict_from_file(\n                adult_file, ""csv"", include_label=True\n            ).dtype, np.dtype(""float32""), msg=""Re-Predict failed""\n        )\n\n    def test_19_re_train_from_csv(self):\n        self.assertIsInstance(\n            auto_adult_linear_svm.fit(verbose=0), AutoLinearSVM,\n            msg=""Re-Train failed""\n        )\n\n    def test_99_clear_cache(self):\n        clear_cache()\n\n\nif __name__ == \'__main__\':\n    unittest.main()\n'"
_Dist/NeuralNetworks/_Tests/_UnitTests/d_Dist.py,0,"b'import os\nimport sys\nroot_path = os.path.abspath(""../../../../"")\nif root_path not in sys.path:\n    sys.path.append(root_path)\n\nimport copy\nimport unittest\nimport numpy as np\n\nfrom Util.Util import DataUtil\nfrom _Dist.NeuralNetworks.b_TraditionalML.SVM import DistLinearSVM\nfrom _Dist.NeuralNetworks.g_DistNN.NN import DistBasic, DistAdvanced\nfrom _Dist.NeuralNetworks._Tests._UnitTests.UnitTestUtil import clear_cache\n\n\nbase_params = {\n    ""name"": ""UnitTest"", ""data_info"": {},\n    ""model_param_settings"": {""n_epoch"": 1, ""max_epoch"": 2}\n}\nnn = DistAdvanced(**copy.deepcopy(base_params))\nbasic_nn = DistBasic(**copy.deepcopy(base_params))\nlinear_svm = DistLinearSVM(**copy.deepcopy(base_params))\ntrain_set, cv_set, test_set = DataUtil.gen_special_linear(1000, 2, 2, 2, one_hot=False)\n(x, y), (x_cv, y_cv), (x_test, y_test) = train_set, cv_set, test_set\ntrain_data = np.hstack([x, y.reshape([-1, 1])])\ncv_data = np.hstack([x_cv, y_cv.reshape([-1, 1])])\ntest_data = np.hstack([x_test, y_test.reshape([-1, 1])])\ntrain_and_cv_data = np.vstack([train_data, cv_data])\n\n\nclass TestDistNN(unittest.TestCase):\n    def test_00_k_series_from_numpy(self):\n        self.assertIsInstance(\n            nn.k_random(3, (train_and_cv_data, test_data), verbose=0), DistAdvanced,\n            msg=""k-random failed""\n        )\n        self.assertIsInstance(\n            nn.k_fold(3, (train_and_cv_data, test_data), verbose=0), DistAdvanced,\n            msg=""k-fold failed""\n        )\n        self.assertIsInstance(\n            basic_nn.k_random(3, (train_and_cv_data, test_data), verbose=0), DistBasic,\n            msg=""k-random failed""\n        )\n        self.assertIsInstance(\n            basic_nn.k_fold(3, (train_and_cv_data, test_data), verbose=0), DistBasic,\n            msg=""k-fold failed""\n        )\n\n    def test_01_predict(self):\n        self.assertIs(nn.predict(train_set[0]).dtype, np.dtype(""float32""), ""Predict failed"")\n        self.assertIs(nn.predict_classes(cv_set[0]).dtype, np.dtype(""int32""), ""Predict classes failed"")\n        self.assertIs(nn.predict_classes(test_set[0]).dtype, np.dtype(""int32""), ""Predict classes failed"")\n        self.assertIs(basic_nn.predict(train_set[0]).dtype, np.dtype(""float32""), ""Predict failed"")\n        self.assertIs(basic_nn.predict_classes(cv_set[0]).dtype, np.dtype(""int32""), ""Predict classes failed"")\n        self.assertIs(basic_nn.predict_classes(test_set[0]).dtype, np.dtype(""int32""), ""Predict classes failed"")\n\n    def test_02_evaluate(self):\n        self.assertEqual(len(nn.evaluate(*train_set, *cv_set, *test_set)), 3, ""Evaluation failed"")\n        self.assertEqual(len(basic_nn.evaluate(*train_set, *cv_set, *test_set)), 3, ""Evaluation failed"")\n\n    def test_03_save(self):\n        self.assertIsInstance(nn.save(), DistAdvanced, msg=""Save failed"")\n        self.assertIsInstance(basic_nn.save(), DistBasic, msg=""Save failed"")\n\n    def test_04_load(self):\n        global nn, basic_nn\n        nn = DistAdvanced(**base_params).load()\n        basic_nn = DistBasic(**base_params).load()\n        self.assertIsInstance(nn, DistAdvanced, ""Load failed"")\n        self.assertIsInstance(basic_nn, DistBasic, ""Load failed"")\n\n    def test_05_re_predict(self):\n        self.assertIs(nn.predict(train_set[0]).dtype, np.dtype(""float32""), ""Re-Predict failed"")\n        self.assertIs(nn.predict_classes(cv_set[0]).dtype, np.dtype(""int32""), ""Re-Predict classes failed"")\n        self.assertIs(nn.predict_classes(test_set[0]).dtype, np.dtype(""int32""), ""Re-Predict classes failed"")\n        self.assertIs(basic_nn.predict(train_set[0]).dtype, np.dtype(""float32""), ""Re-Predict failed"")\n        self.assertIs(basic_nn.predict_classes(cv_set[0]).dtype, np.dtype(""int32""), ""Re-Predict classes failed"")\n        self.assertIs(basic_nn.predict_classes(test_set[0]).dtype, np.dtype(""int32""), ""Re-Predict classes failed"")\n\n    def test_06_re_evaluate(self):\n        self.assertEqual(len(nn.evaluate(*train_set, *cv_set, *test_set)), 3, ""Re-Evaluation failed"")\n        self.assertEqual(len(basic_nn.evaluate(*train_set, *cv_set, *test_set)), 3, ""Re-Evaluation failed"")\n\n    def test_07_param_search(self):\n        params = [\n            {""model_param_settings"": {""lr"": 1e-2}},\n            {""model_param_settings"": {""lr"": 1e-3}, ""model_structure_settings"": {""use_pruner"": False}}\n        ]\n        self.assertIsInstance(\n            nn.param_search(params, data=(train_and_cv_data, test_data), verbose=0), DistAdvanced,\n            msg=""param_search failed""\n        )\n        self.assertIsInstance(\n            basic_nn.param_search(params, data=(train_and_cv_data, test_data), verbose=0), DistBasic,\n            msg=""param_search failed""\n        )\n\n    def test_08_random_search(self):\n        list_first_grid_params = {\n            ""model_param_settings"": [\n                {""lr"": 1e-2},\n                {""lr"": 1e-2, ""loss"": ""mse""},\n                {""lr"": 1e-3, ""loss"": ""mse""}\n            ],\n            ""model_structure_settings"": [\n                {""hidden_units"": [256, 256]},\n                {""hidden_units"": [128, 128], ""use_pruner"": False},\n                {""hidden_units"": [128, 128], ""use_pruner"": False, ""use_wide_network"": False}\n            ]\n        }\n        dict_first_grid_params = {\n            ""model_param_settings"": {\n                ""lr"": [1e-2, 1e-3],\n                ""loss"": [""mse"", ""cross_entropy""]\n            },\n            ""model_structure_settings"": {\n                ""use_pruner"": [False, True],\n                ""use_wide_network"": [False, True],\n                ""hidden_units"": [[128, 128], [256, 256]]\n            },\n        }\n        self.assertIsInstance(\n            nn.random_search(\n                4, list_first_grid_params, grid_order=""list_first"",\n                data=(train_and_cv_data, test_data), verbose=0\n            ), DistAdvanced, msg=""list_first_grid_search failed""\n        )\n        self.assertIsInstance(\n            nn.random_search(\n                8, dict_first_grid_params, grid_order=""dict_first"",\n                data=(train_and_cv_data, test_data), verbose=0\n            ), DistAdvanced, msg=""dict_first_grid_search failed""\n        )\n        self.assertIsInstance(\n            basic_nn.random_search(\n                4, list_first_grid_params, grid_order=""list_first"",\n                data=(train_and_cv_data, test_data), verbose=0\n            ), DistBasic, msg=""list_first_grid_search failed""\n        )\n        self.assertIsInstance(\n            basic_nn.random_search(\n                8, dict_first_grid_params, grid_order=""dict_first"",\n                data=(train_and_cv_data, test_data), verbose=0\n            ), DistBasic, msg=""dict_first_grid_search failed""\n        )\n\n    def test_09_grid_search(self):\n        list_first_grid_params = {\n            ""model_param_settings"": [\n                {""lr"": 1e-2},\n                {""lr"": 1e-3, ""loss"": ""mse""}\n            ],\n            ""model_structure_settings"": [\n                {""hidden_units"": [256, 256]},\n                {""hidden_units"": [128, 128], ""use_pruner"": False}\n            ]\n        }\n        dict_first_grid_params = {\n            ""model_param_settings"": {\n                ""lr"": [1e-3, 1e-2],\n                ""loss"": [""mse"", ""cross_entropy""]\n            },\n            ""model_structure_settings"": {\n                ""hidden_units"": [[128, 256], [128, 256]]\n            }\n        }\n        self.assertIsInstance(\n            nn.grid_search(\n                list_first_grid_params, grid_order=""list_first"",\n                data=(train_and_cv_data, test_data), verbose=0\n            ), DistAdvanced, msg=""list_first_grid_search failed""\n        )\n        self.assertIsInstance(\n            nn.grid_search(\n                dict_first_grid_params, grid_order=""dict_first"",\n                data=(train_and_cv_data, test_data), verbose=0\n            ), DistAdvanced, msg=""dict_first_grid_search failed""\n        )\n        self.assertIsInstance(\n            basic_nn.grid_search(\n                list_first_grid_params, grid_order=""list_first"",\n                data=(train_and_cv_data, test_data), verbose=0\n            ), DistBasic, msg=""list_first_grid_search failed""\n        )\n        self.assertIsInstance(\n            basic_nn.grid_search(\n                dict_first_grid_params, grid_order=""dict_first"",\n                data=(train_and_cv_data, test_data), verbose=0\n            ), DistBasic, msg=""dict_first_grid_search failed""\n        )\n\n    def test_10_range_search(self):\n        range_grid_params = {\n            ""model_param_settings"": {\n                ""lr"": [""float"", 1e-3, 1e-1, ""log""],\n                ""loss"": [""choice"", [""mse"", ""cross_entropy""]]\n            },\n            ""model_structure_settings"": {\n                ""hidden_units"": [\n                    [""int"", ""int""],\n                    [128, 256], [128, 256]\n                ],\n                ""pruner_params"": {\n                    ""alpha"": [""float"", 1e-4, 1e-2, ""log""],\n                    ""beta"": [""float"", 0.3, 3, ""log""],\n                    ""gamma"": [""float"", 0.5, 2, ""log""]\n                }\n            },\n            ""pre_process_settings"": {\n                ""pre_process_method"": [""choice"", [""normalize"", None]],\n                ""reuse_mean_and_std"": [""choice"", [True, False]]\n            },\n            ""nan_handler_settings"": {\n                ""nan_handler_method"": [""choice"", [""median"", ""mean""]],\n                ""reuse_nan_handler_values"": [""choice"", [True, False]]\n            }\n        }\n        self.assertIsInstance(\n            nn.range_search(\n                8, range_grid_params,\n                data=(train_and_cv_data, test_data), verbose=0\n            ), DistAdvanced, msg=""range_search failed""\n        )\n        self.assertIsInstance(\n            basic_nn.range_search(\n                8, range_grid_params,\n                data=(train_and_cv_data, test_data), verbose=0\n            ), DistBasic, msg=""range_search failed""\n        )\n\n    def test_99_clear_cache(self):\n        clear_cache()\n\n\nclass TestDistLinearSVM(unittest.TestCase):\n    def test_00_k_series_from_numpy(self):\n        self.assertIsInstance(\n            linear_svm.k_random(3, (train_and_cv_data, test_data), verbose=0), DistLinearSVM,\n            msg=""k-random failed""\n        )\n        self.assertIsInstance(\n            linear_svm.k_fold(3, (train_and_cv_data, test_data), verbose=0), DistLinearSVM,\n            msg=""k-fold failed""\n        )\n\n    def test_01_predict(self):\n        self.assertIs(linear_svm.predict(train_set[0]).dtype, np.dtype(""float32""), ""Predict failed"")\n        self.assertIs(linear_svm.predict(cv_set[0]).dtype, np.dtype(""float32""), ""Predict failed"")\n        self.assertIs(linear_svm.predict(test_set[0]).dtype, np.dtype(""float32""), ""Predict failed"")\n\n    def test_02_evaluate(self):\n        self.assertEqual(len(linear_svm.evaluate(*train_set, *cv_set, *test_set)), 3, ""Evaluation failed"")\n\n    def test_03_save(self):\n        self.assertIsInstance(linear_svm.save(), DistLinearSVM, msg=""Save failed"")\n\n    def test_04_load(self):\n        global linear_svm\n        model = DistLinearSVM(**base_params).load()\n        self.assertIsInstance(model, DistLinearSVM, ""Load failed"")\n\n    def test_05_re_predict(self):\n        self.assertIs(linear_svm.predict(train_set[0]).dtype, np.dtype(""float32""), ""Re-Predict failed"")\n        self.assertIs(linear_svm.predict(cv_set[0]).dtype, np.dtype(""float32""), ""Re-Predict failed"")\n        self.assertIs(linear_svm.predict(test_set[0]).dtype, np.dtype(""float32""), ""Re-Predict failed"")\n\n    def test_06_re_evaluate(self):\n        self.assertEqual(len(linear_svm.evaluate(*train_set, *cv_set, *test_set)), 3, ""Re-Evaluation failed"")\n\n    def test_99_clear_cache(self):\n        clear_cache()\n\n\nif __name__ == \'__main__\':\n    unittest.main()\n'"
