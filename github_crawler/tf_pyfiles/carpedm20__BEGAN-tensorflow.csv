file_path,api_count,code
config.py,0,"b""#-*- coding: utf-8 -*-\nimport argparse\n\ndef str2bool(v):\n    return v.lower() in ('true', '1')\n\narg_lists = []\nparser = argparse.ArgumentParser()\n\ndef add_argument_group(name):\n    arg = parser.add_argument_group(name)\n    arg_lists.append(arg)\n    return arg\n\n# Network\nnet_arg = add_argument_group('Network')\nnet_arg.add_argument('--input_scale_size', type=int, default=64,\n                     help='input image will be resized with the given value as width and height')\nnet_arg.add_argument('--conv_hidden_num', type=int, default=128,\n                     choices=[64, 128],help='n in the paper')\nnet_arg.add_argument('--z_num', type=int, default=64, choices=[64, 128])\n\n# Data\ndata_arg = add_argument_group('Data')\ndata_arg.add_argument('--dataset', type=str, default='CelebA')\ndata_arg.add_argument('--split', type=str, default='train')\ndata_arg.add_argument('--batch_size', type=int, default=16)\ndata_arg.add_argument('--grayscale', type=str2bool, default=False)\ndata_arg.add_argument('--num_worker', type=int, default=4)\n\n# Training / test parameters\ntrain_arg = add_argument_group('Training')\ntrain_arg.add_argument('--is_train', type=str2bool, default=True)\ntrain_arg.add_argument('--optimizer', type=str, default='adam')\ntrain_arg.add_argument('--max_step', type=int, default=500000)\ntrain_arg.add_argument('--lr_update_step', type=int, default=100000, choices=[100000, 75000])\ntrain_arg.add_argument('--d_lr', type=float, default=0.00008)\ntrain_arg.add_argument('--g_lr', type=float, default=0.00008)\ntrain_arg.add_argument('--lr_lower_boundary', type=float, default=0.00002)\ntrain_arg.add_argument('--beta1', type=float, default=0.5)\ntrain_arg.add_argument('--beta2', type=float, default=0.999)\ntrain_arg.add_argument('--gamma', type=float, default=0.5)\ntrain_arg.add_argument('--lambda_k', type=float, default=0.001)\ntrain_arg.add_argument('--use_gpu', type=str2bool, default=True)\n\n# Misc\nmisc_arg = add_argument_group('Misc')\nmisc_arg.add_argument('--load_path', type=str, default='')\nmisc_arg.add_argument('--log_step', type=int, default=50)\nmisc_arg.add_argument('--save_step', type=int, default=5000)\nmisc_arg.add_argument('--num_log_samples', type=int, default=3)\nmisc_arg.add_argument('--log_level', type=str, default='INFO', choices=['INFO', 'DEBUG', 'WARN'])\nmisc_arg.add_argument('--log_dir', type=str, default='logs')\nmisc_arg.add_argument('--data_dir', type=str, default='data')\nmisc_arg.add_argument('--test_data_path', type=str, default=None,\n                      help='directory with images which will be used in test sample generation')\nmisc_arg.add_argument('--sample_per_image', type=int, default=64,\n                      help='# of sample per image during test sample generation')\nmisc_arg.add_argument('--random_seed', type=int, default=123)\n\ndef get_config():\n    config, unparsed = parser.parse_known_args()\n    if config.use_gpu:\n        data_format = 'NCHW'\n    else:\n        data_format = 'NHWC'\n    setattr(config, 'data_format', data_format)\n    return config, unparsed\n"""
data_loader.py,11,"b'import os\nfrom PIL import Image\nfrom glob import glob\nimport tensorflow as tf\n\ndef get_loader(root, batch_size, scale_size, data_format, split=None, is_grayscale=False, seed=None):\n    dataset_name = os.path.basename(root)\n    if dataset_name in [\'CelebA\'] and split:\n        root = os.path.join(root, \'splits\', split)\n\n    for ext in [""jpg"", ""png""]:\n        paths = glob(""{}/*.{}"".format(root, ext))\n\n        if ext == ""jpg"":\n            tf_decode = tf.image.decode_jpeg\n        elif ext == ""png"":\n            tf_decode = tf.image.decode_png\n        \n        if len(paths) != 0:\n            break\n\n    with Image.open(paths[0]) as img:\n        w, h = img.size\n        shape = [h, w, 3]\n\n    filename_queue = tf.train.string_input_producer(list(paths), shuffle=False, seed=seed)\n    reader = tf.WholeFileReader()\n    filename, data = reader.read(filename_queue)\n    image = tf_decode(data, channels=3)\n\n    if is_grayscale:\n        image = tf.image.rgb_to_grayscale(image)\n    image.set_shape(shape)\n\n    min_after_dequeue = 5000\n    capacity = min_after_dequeue + 3 * batch_size\n\n    queue = tf.train.shuffle_batch(\n        [image], batch_size=batch_size,\n        num_threads=4, capacity=capacity,\n        min_after_dequeue=min_after_dequeue, name=\'synthetic_inputs\')\n\n    if dataset_name in [\'CelebA\']:\n        queue = tf.image.crop_to_bounding_box(queue, 50, 25, 128, 128)\n        queue = tf.image.resize_nearest_neighbor(queue, [scale_size, scale_size])\n    else:\n        queue = tf.image.resize_nearest_neighbor(queue, [scale_size, scale_size])\n\n    if data_format == \'NCHW\':\n        queue = tf.transpose(queue, [0, 3, 1, 2])\n    elif data_format == \'NHWC\':\n        pass\n    else:\n        raise Exception(""[!] Unkown data_format: {}"".format(data_format))\n\n    return tf.to_float(queue)\n'"
download.py,0,"b'""""""\nModification of\n- https://github.com/carpedm20/DCGAN-tensorflow/blob/master/download.py\n- http://stackoverflow.com/a/39225039\n""""""\nfrom __future__ import print_function\nimport os\nimport zipfile\nimport requests\nimport subprocess\nfrom tqdm import tqdm\nfrom collections import OrderedDict\n\ndef download_file_from_google_drive(id, destination):\n    URL = ""https://docs.google.com/uc?export=download""\n    session = requests.Session()\n\n    response = session.get(URL, params={ \'id\': id }, stream=True)\n    token = get_confirm_token(response)\n\n    if token:\n        params = { \'id\' : id, \'confirm\' : token }\n        response = session.get(URL, params=params, stream=True)\n\n    save_response_content(response, destination)    \n\ndef get_confirm_token(response):\n    for key, value in response.cookies.items():\n        if key.startswith(\'download_warning\'):\n            return value\n    return None\n\ndef save_response_content(response, destination, chunk_size=32*1024):\n    total_size = int(response.headers.get(\'content-length\', 0))\n    with open(destination, ""wb"") as f:\n        for chunk in tqdm(response.iter_content(chunk_size), total=total_size, \n                          unit=\'B\', unit_scale=True, desc=destination):\n            if chunk: # filter out keep-alive new chunks\n                f.write(chunk)\n\ndef unzip(filepath):\n    print(""Extracting: "" + filepath)\n    base_path = os.path.dirname(filepath)\n    with zipfile.ZipFile(filepath) as zf:\n        zf.extractall(base_path)\n    os.remove(filepath)\n\ndef download_celeb_a(base_path):\n    data_path = os.path.join(base_path, \'CelebA\')\n    images_path = os.path.join(data_path, \'images\')\n    if os.path.exists(data_path):\n        print(\'[!] Found Celeb-A - skip\')\n        return\n\n    filename, drive_id  = ""img_align_celeba.zip"", ""0B7EVK8r0v71pZjFTYXZWM3FlRnM""\n    save_path = os.path.join(base_path, filename)\n\n    if os.path.exists(save_path):\n        print(\'[*] {} already exists\'.format(save_path))\n    else:\n        download_file_from_google_drive(drive_id, save_path)\n\n    zip_dir = \'\'\n    with zipfile.ZipFile(save_path) as zf:\n        zip_dir = zf.namelist()[0]\n        zf.extractall(base_path)\n    if not os.path.exists(data_path):\n        os.mkdir(data_path)\n    os.rename(os.path.join(base_path, ""img_align_celeba""), images_path)\n    os.remove(save_path)\n\ndef prepare_data_dir(path = \'./data\'):\n    if not os.path.exists(path):\n        os.mkdir(path)\n\n# check, if file exists, make link\ndef check_link(in_dir, basename, out_dir):\n    in_file = os.path.join(in_dir, basename)\n    if os.path.exists(in_file):\n        link_file = os.path.join(out_dir, basename)\n        rel_link = os.path.relpath(in_file, out_dir)\n        os.symlink(rel_link, link_file)\n\ndef add_splits(base_path):\n    data_path = os.path.join(base_path, \'CelebA\')\n    images_path = os.path.join(data_path, \'images\')\n    train_dir = os.path.join(data_path, \'splits\', \'train\')\n    valid_dir = os.path.join(data_path, \'splits\', \'valid\')\n    test_dir = os.path.join(data_path, \'splits\', \'test\')\n    if not os.path.exists(train_dir):\n        os.makedirs(train_dir)\n    if not os.path.exists(valid_dir):\n        os.makedirs(valid_dir)\n    if not os.path.exists(test_dir):\n        os.makedirs(test_dir)\n\n    # these constants based on the standard CelebA splits\n    NUM_EXAMPLES = 202599\n    TRAIN_STOP = 162770\n    VALID_STOP = 182637\n\n    for i in range(0, TRAIN_STOP):\n        basename = ""{:06d}.jpg"".format(i+1)\n        check_link(images_path, basename, train_dir)\n    for i in range(TRAIN_STOP, VALID_STOP):\n        basename = ""{:06d}.jpg"".format(i+1)\n        check_link(images_path, basename, valid_dir)\n    for i in range(VALID_STOP, NUM_EXAMPLES):\n        basename = ""{:06d}.jpg"".format(i+1)\n        check_link(images_path, basename, test_dir)\n\nif __name__ == \'__main__\':\n    base_path = \'./data\'\n    prepare_data_dir()\n    download_celeb_a(base_path)\n    add_splits(base_path)\n'"
folder.py,0,"b'import torch.utils.data as data\n\nfrom PIL import Image\nimport os\nimport os.path\n\nIMG_EXTENSIONS = [\n    \'.jpg\', \'.JPG\', \'.jpeg\', \'.JPEG\',\n    \'.png\', \'.PNG\', \'.ppm\', \'.PPM\', \'.bmp\', \'.BMP\',\n]\n\ndef is_image_file(filename):\n    return any(filename.endswith(extension) for extension in IMG_EXTENSIONS)\n\ndef make_dataset(dir):\n    images = []\n    for root, _, fnames in sorted(os.walk(dir)):\n        for fname in sorted(fnames):\n            if is_image_file(fname):\n                path = os.path.join(root, fname)\n                item = (path, 0)\n                images.append(item)\n\n    return images\n\ndef default_loader(path):\n    return Image.open(path).convert(\'RGB\')\n\nclass ImageFolder(data.Dataset):\n\n    def __init__(self, root, transform=None, target_transform=None,\n                 loader=default_loader):\n        imgs = make_dataset(root)\n        if len(imgs) == 0:\n            raise(RuntimeError(""Found 0 images in subfolders of: "" + root + ""\\n""\n                               ""Supported image extensions are: "" + "","".join(IMG_EXTENSIONS)))\n\n        print(""Found {} images in subfolders of: {}"".format(len(imgs), root))\n\n        self.root = root\n        self.imgs = imgs\n        self.transform = transform\n        self.target_transform = target_transform\n        self.loader = loader\n\n    def __getitem__(self, index):\n        path, target = self.imgs[index]\n        img = self.loader(path)\n        if self.transform is not None:\n            img = self.transform(img)\n        if self.target_transform is not None:\n            target = self.target_transform(target)\n\n        return img, target\n\n    def __len__(self):\n        return len(self.imgs)\n'"
layers.py,18,"b'# Code from https://github.com/david-berthelot/tf_img_tech/blob/master/tfswag/layers.py\nimport numpy as N\nimport numpy.linalg as LA\nimport tensorflow as tf\n\n__author__ = \'David Berthelot\'\n\n\ndef unboxn(vin, n):\n    """"""vin = (batch, h, w, depth), returns vout = (batch, n*h, n*w, depth), each pixel is duplicated.""""""\n    s = tf.shape(vin)\n    vout = tf.concat([vin] * (n ** 2), 0)  # Poor man\'s replacement for tf.tile (required for Adversarial Training support).\n    vout = tf.reshape(vout, [s[0] * (n ** 2), s[1], s[2], s[3]])\n    vout = tf.batch_to_space(vout, [[0, 0], [0, 0]], n)\n    return vout\n\n\ndef boxn(vin, n):\n    """"""vin = (batch, h, w, depth), returns vout = (batch, h//n, w//n, depth), each pixel is averaged.""""""\n    if n == 1:\n        return vin\n    s = tf.shape(vin)\n    vout = tf.reshape(vin, [s[0], s[1] // n, n, s[2] // n, n, s[3]])\n    vout = tf.reduce_mean(vout, [2, 4])\n    return vout\n\n\nclass LayerBase:\n    pass\n\n\nclass LayerConv(LayerBase):\n    def __init__(self, name, w, n, nl=lambda x, y: x + y, strides=(1, 1, 1, 1),\n                 padding=\'SAME\', conv=None, use_bias=True, data_format=""NCHW""):\n        """"""w = (wy, wx), n = (n_in, n_out)""""""\n        self.nl = nl\n        self.strides = list(strides)\n        self.padding = padding\n        self.data_format = data_format\n        with tf.name_scope(name):\n            if conv is None:\n                conv = tf.Variable(tf.truncated_normal([w[0], w[1], n[0], n[1]], stddev=0.01), name=\'conv\')\n            self.conv = conv\n            self.bias = tf.Variable(tf.zeros([n[1]]), name=\'bias\') if use_bias else 0\n\n    def __call__(self, vin):\n        return self.nl(tf.nn.conv2d(vin, self.conv, strides=self.strides,\n                                    padding=self.padding, data_format=self.data_format), self.bias)\n\nclass LayerEncodeConvGrowLinear(LayerBase):\n    def __init__(self, name, n, width, colors, depth, scales, nl=lambda x, y: x + y, data_format=""NCHW""):\n        with tf.variable_scope(name) as vs:\n            encode = []\n            nn = n\n            for x in range(scales):\n                cl = []\n                for y in range(depth - 1):\n                    cl.append(LayerConv(\'conv_%d_%d\' % (x, y), [width, width],\n                                        [nn, nn], nl, data_format=data_format))\n                cl.append(LayerConv(\'conv_%d_%d\' % (x, depth - 1), [width, width],\n                                    [nn, nn + n], nl, strides=[1, 2, 2, 1], data_format=data_format))\n                encode.append(cl)\n                nn += n\n            self.encode = [LayerConv(\'conv_pre\', [width, width], [colors, n], nl, data_format=data_format), encode]\n        self.variables = tf.contrib.framework.get_variables(vs)\n\n    def __call__(self, vin, carry=0, train=True):\n        vout = self.encode[0](vin)\n        for convs in self.encode[1]:\n            for conv in convs[:-1]:\n                vtmp = tf.nn.elu(conv(vout))\n                vout = carry * vout + (1 - carry) * vtmp\n            vout = convs[-1](vout)\n        return vout, self.variables\n\n\nclass LayerDecodeConvBlend(LayerBase):\n    def __init__(self, name, n, width, colors, depth, scales, nl=lambda x, y: x + y, data_format=""NCHW""):\n        with tf.variable_scope(name) as vs:\n            decode = []\n            for x in range(scales):\n                cl = []\n                n2 = 2 * n if x else n\n                cl.append(LayerConv(\'conv_%d_%d\' % (x, 0), [width, width],\n                                    [n2, n], nl, data_format=data_format))\n                for y in range(1, depth):\n                    cl.append(LayerConv(\'conv_%d_%d\' % (x, y), [width, width], [n, n], nl, data_format=data_format))\n                decode.append(cl)\n            self.decode = [decode, LayerConv(\'conv_post\', [width, width], [n, colors], data_format=data_format)]\n        self.variables = tf.contrib.framework.get_variables(vs)\n\n    def __call__(self, data, carry, train=True):\n        vout = data\n        layers = []\n        for x, convs in enumerate(self.decode[0]):\n            vout = tf.concat([vout, data], 3) if x else vout\n            vout = unboxn(convs[0](vout), 2)\n            data = unboxn(data, 2)\n            for conv in convs[1:]:\n                vtmp = tf.nn.elu(conv(vout))\n                vout = carry * vout + (1 - carry) * vtmp\n            layers.append(vout)\n        return self.decode[1](vout), self.variables\n'"
main.py,1,"b'import numpy as np\nimport tensorflow as tf\n\nfrom trainer import Trainer\nfrom config import get_config\nfrom data_loader import get_loader\nfrom utils import prepare_dirs_and_logger, save_config\n\ndef main(config):\n    prepare_dirs_and_logger(config)\n\n    rng = np.random.RandomState(config.random_seed)\n    tf.set_random_seed(config.random_seed)\n\n    if config.is_train:\n        data_path = config.data_path\n        batch_size = config.batch_size\n        do_shuffle = True\n    else:\n        setattr(config, \'batch_size\', 64)\n        if config.test_data_path is None:\n            data_path = config.data_path\n        else:\n            data_path = config.test_data_path\n        batch_size = config.sample_per_image\n        do_shuffle = False\n\n    data_loader = get_loader(\n            data_path, config.batch_size, config.input_scale_size,\n            config.data_format, config.split)\n    trainer = Trainer(config, data_loader)\n\n    if config.is_train:\n        save_config(config)\n        trainer.train()\n    else:\n        if not config.load_path:\n            raise Exception(""[!] You should specify `load_path` to load a pretrained model"")\n        trainer.test()\n\nif __name__ == ""__main__"":\n    config, unparsed = get_config()\n    main(config)\n'"
models.py,21,"b'import numpy as np\nimport tensorflow as tf\nslim = tf.contrib.slim\n\ndef GeneratorCNN(z, hidden_num, output_num, repeat_num, data_format, reuse):\n    with tf.variable_scope(""G"", reuse=reuse) as vs:\n        num_output = int(np.prod([8, 8, hidden_num]))\n        x = slim.fully_connected(z, num_output, activation_fn=None)\n        x = reshape(x, 8, 8, hidden_num, data_format)\n        \n        for idx in range(repeat_num):\n            x = slim.conv2d(x, hidden_num, 3, 1, activation_fn=tf.nn.elu, data_format=data_format)\n            x = slim.conv2d(x, hidden_num, 3, 1, activation_fn=tf.nn.elu, data_format=data_format)\n            if idx < repeat_num - 1:\n                x = upscale(x, 2, data_format)\n\n        out = slim.conv2d(x, 3, 3, 1, activation_fn=None, data_format=data_format)\n\n    variables = tf.contrib.framework.get_variables(vs)\n    return out, variables\n\ndef DiscriminatorCNN(x, input_channel, z_num, repeat_num, hidden_num, data_format):\n    with tf.variable_scope(""D"") as vs:\n        # Encoder\n        x = slim.conv2d(x, hidden_num, 3, 1, activation_fn=tf.nn.elu, data_format=data_format)\n\n        prev_channel_num = hidden_num\n        for idx in range(repeat_num):\n            channel_num = hidden_num * (idx + 1)\n            x = slim.conv2d(x, channel_num, 3, 1, activation_fn=tf.nn.elu, data_format=data_format)\n            x = slim.conv2d(x, channel_num, 3, 1, activation_fn=tf.nn.elu, data_format=data_format)\n            if idx < repeat_num - 1:\n                x = slim.conv2d(x, channel_num, 3, 2, activation_fn=tf.nn.elu, data_format=data_format)\n                #x = tf.contrib.layers.max_pool2d(x, [2, 2], [2, 2], padding=\'VALID\')\n\n        x = tf.reshape(x, [-1, np.prod([8, 8, channel_num])])\n        z = x = slim.fully_connected(x, z_num, activation_fn=None)\n\n        # Decoder\n        num_output = int(np.prod([8, 8, hidden_num]))\n        x = slim.fully_connected(x, num_output, activation_fn=None)\n        x = reshape(x, 8, 8, hidden_num, data_format)\n        \n        for idx in range(repeat_num):\n            x = slim.conv2d(x, hidden_num, 3, 1, activation_fn=tf.nn.elu, data_format=data_format)\n            x = slim.conv2d(x, hidden_num, 3, 1, activation_fn=tf.nn.elu, data_format=data_format)\n            if idx < repeat_num - 1:\n                x = upscale(x, 2, data_format)\n\n        out = slim.conv2d(x, input_channel, 3, 1, activation_fn=None, data_format=data_format)\n\n    variables = tf.contrib.framework.get_variables(vs)\n    return out, z, variables\n\ndef int_shape(tensor):\n    shape = tensor.get_shape().as_list()\n    return [num if num is not None else -1 for num in shape]\n\ndef get_conv_shape(tensor, data_format):\n    shape = int_shape(tensor)\n    # always return [N, H, W, C]\n    if data_format == \'NCHW\':\n        return [shape[0], shape[2], shape[3], shape[1]]\n    elif data_format == \'NHWC\':\n        return shape\n\ndef nchw_to_nhwc(x):\n    return tf.transpose(x, [0, 2, 3, 1])\n\ndef nhwc_to_nchw(x):\n    return tf.transpose(x, [0, 3, 1, 2])\n\ndef reshape(x, h, w, c, data_format):\n    if data_format == \'NCHW\':\n        x = tf.reshape(x, [-1, c, h, w])\n    else:\n        x = tf.reshape(x, [-1, h, w, c])\n    return x\n\ndef resize_nearest_neighbor(x, new_size, data_format):\n    if data_format == \'NCHW\':\n        x = nchw_to_nhwc(x)\n        x = tf.image.resize_nearest_neighbor(x, new_size)\n        x = nhwc_to_nchw(x)\n    else:\n        x = tf.image.resize_nearest_neighbor(x, new_size)\n    return x\n\ndef upscale(x, scale, data_format):\n    _, h, w, _ = get_conv_shape(x, data_format)\n    return resize_nearest_neighbor(x, (h*scale, w*scale), data_format)\n'"
trainer.py,46,"b'from __future__ import print_function\n\nimport os\ntry:\n    from StringIO import StringIO\nexcept ImportError:\n    from io import StringIO\nimport scipy.misc\nimport numpy as np\nfrom glob import glob\nfrom tqdm import trange\nfrom itertools import chain\nfrom collections import deque\n\nfrom models import *\nfrom utils import save_image\n\ndef next(loader):\n    return loader.next()[0].data.numpy()\n\ndef to_nhwc(image, data_format):\n    if data_format == \'NCHW\':\n        new_image = nchw_to_nhwc(image)\n    else:\n        new_image = image\n    return new_image\n\ndef to_nchw_numpy(image):\n    if image.shape[3] in [1, 3]:\n        new_image = image.transpose([0, 3, 1, 2])\n    else:\n        new_image = image\n    return new_image\n\ndef norm_img(image, data_format=None):\n    image = image/127.5 - 1.\n    if data_format:\n        image = to_nhwc(image, data_format)\n    return image\n\ndef denorm_img(norm, data_format):\n    return tf.clip_by_value(to_nhwc((norm + 1)*127.5, data_format), 0, 255)\n\ndef slerp(val, low, high):\n    """"""Code from https://github.com/soumith/dcgan.torch/issues/14""""""\n    omega = np.arccos(np.clip(np.dot(low/np.linalg.norm(low), high/np.linalg.norm(high)), -1, 1))\n    so = np.sin(omega)\n    if so == 0:\n        return (1.0-val) * low + val * high # L\'Hopital\'s rule/LERP\n    return np.sin((1.0-val)*omega) / so * low + np.sin(val*omega) / so * high\n\nclass Trainer(object):\n    def __init__(self, config, data_loader):\n        self.config = config\n        self.data_loader = data_loader\n        self.dataset = config.dataset\n\n        self.beta1 = config.beta1\n        self.beta2 = config.beta2\n        self.optimizer = config.optimizer\n        self.batch_size = config.batch_size\n\n        self.step = tf.Variable(0, name=\'step\', trainable=False)\n\n        self.g_lr = tf.Variable(config.g_lr, name=\'g_lr\')\n        self.d_lr = tf.Variable(config.d_lr, name=\'d_lr\')\n\n        self.g_lr_update = tf.assign(self.g_lr, tf.maximum(self.g_lr * 0.5, config.lr_lower_boundary), name=\'g_lr_update\')\n        self.d_lr_update = tf.assign(self.d_lr, tf.maximum(self.d_lr * 0.5, config.lr_lower_boundary), name=\'d_lr_update\')\n\n        self.gamma = config.gamma\n        self.lambda_k = config.lambda_k\n\n        self.z_num = config.z_num\n        self.conv_hidden_num = config.conv_hidden_num\n        self.input_scale_size = config.input_scale_size\n\n        self.model_dir = config.model_dir\n        self.load_path = config.load_path\n\n        self.use_gpu = config.use_gpu\n        self.data_format = config.data_format\n\n        _, height, width, self.channel = \\\n                get_conv_shape(self.data_loader, self.data_format)\n        self.repeat_num = int(np.log2(height)) - 2\n\n        self.start_step = 0\n        self.log_step = config.log_step\n        self.max_step = config.max_step\n        self.save_step = config.save_step\n        self.lr_update_step = config.lr_update_step\n\n        self.is_train = config.is_train\n        self.build_model()\n\n        self.saver = tf.train.Saver()\n        self.summary_writer = tf.summary.FileWriter(self.model_dir)\n\n        sv = tf.train.Supervisor(logdir=self.model_dir,\n                                is_chief=True,\n                                saver=self.saver,\n                                summary_op=None,\n                                summary_writer=self.summary_writer,\n                                save_model_secs=300,\n                                global_step=self.step,\n                                ready_for_local_init_op=None)\n\n        gpu_options = tf.GPUOptions(allow_growth=True)\n        sess_config = tf.ConfigProto(allow_soft_placement=True,\n                                    gpu_options=gpu_options)\n\n        self.sess = sv.prepare_or_wait_for_session(config=sess_config)\n\n        if not self.is_train:\n            # dirty way to bypass graph finilization error\n            g = tf.get_default_graph()\n            g._finalized = False\n\n            self.build_test_model()\n\n    def train(self):\n        z_fixed = np.random.uniform(-1, 1, size=(self.batch_size, self.z_num))\n\n        x_fixed = self.get_image_from_loader()\n        save_image(x_fixed, \'{}/x_fixed.png\'.format(self.model_dir))\n\n        prev_measure = 1\n        measure_history = deque([0]*self.lr_update_step, self.lr_update_step)\n\n        for step in trange(self.start_step, self.max_step):\n            fetch_dict = {\n                ""k_update"": self.k_update,\n                ""measure"": self.measure,\n            }\n            if step % self.log_step == 0:\n                fetch_dict.update({\n                    ""summary"": self.summary_op,\n                    ""g_loss"": self.g_loss,\n                    ""d_loss"": self.d_loss,\n                    ""k_t"": self.k_t,\n                })\n            result = self.sess.run(fetch_dict)\n\n            measure = result[\'measure\']\n            measure_history.append(measure)\n\n            if step % self.log_step == 0:\n                self.summary_writer.add_summary(result[\'summary\'], step)\n                self.summary_writer.flush()\n\n                g_loss = result[\'g_loss\']\n                d_loss = result[\'d_loss\']\n                k_t = result[\'k_t\']\n\n                print(""[{}/{}] Loss_D: {:.6f} Loss_G: {:.6f} measure: {:.4f}, k_t: {:.4f}"". \\\n                      format(step, self.max_step, d_loss, g_loss, measure, k_t))\n\n            if step % (self.log_step * 10) == 0:\n                x_fake = self.generate(z_fixed, self.model_dir, idx=step)\n                self.autoencode(x_fixed, self.model_dir, idx=step, x_fake=x_fake)\n\n            if step % self.lr_update_step == self.lr_update_step - 1:\n                self.sess.run([self.g_lr_update, self.d_lr_update])\n                #cur_measure = np.mean(measure_history)\n                #if cur_measure > prev_measure * 0.99:\n                #prev_measure = cur_measure\n\n    def build_model(self):\n        self.x = self.data_loader\n        x = norm_img(self.x)\n\n        self.z = tf.random_uniform(\n                (tf.shape(x)[0], self.z_num), minval=-1.0, maxval=1.0)\n        self.k_t = tf.Variable(0., trainable=False, name=\'k_t\')\n\n        G, self.G_var = GeneratorCNN(\n                self.z, self.conv_hidden_num, self.channel,\n                self.repeat_num, self.data_format, reuse=False)\n\n        d_out, self.D_z, self.D_var = DiscriminatorCNN(\n                tf.concat([G, x], 0), self.channel, self.z_num, self.repeat_num,\n                self.conv_hidden_num, self.data_format)\n        AE_G, AE_x = tf.split(d_out, 2)\n\n        self.G = denorm_img(G, self.data_format)\n        self.AE_G, self.AE_x = denorm_img(AE_G, self.data_format), denorm_img(AE_x, self.data_format)\n\n        if self.optimizer == \'adam\':\n            optimizer = tf.train.AdamOptimizer\n        else:\n            raise Exception(""[!] Caution! Paper didn\'t use {} opimizer other than Adam"".format(config.optimizer))\n\n        g_optimizer, d_optimizer = optimizer(self.g_lr), optimizer(self.d_lr)\n\n        self.d_loss_real = tf.reduce_mean(tf.abs(AE_x - x))\n        self.d_loss_fake = tf.reduce_mean(tf.abs(AE_G - G))\n\n        self.d_loss = self.d_loss_real - self.k_t * self.d_loss_fake\n        self.g_loss = tf.reduce_mean(tf.abs(AE_G - G))\n\n        d_optim = d_optimizer.minimize(self.d_loss, var_list=self.D_var)\n        g_optim = g_optimizer.minimize(self.g_loss, global_step=self.step, var_list=self.G_var)\n\n        self.balance = self.gamma * self.d_loss_real - self.g_loss\n        self.measure = self.d_loss_real + tf.abs(self.balance)\n\n        with tf.control_dependencies([d_optim, g_optim]):\n            self.k_update = tf.assign(\n                self.k_t, tf.clip_by_value(self.k_t + self.lambda_k * self.balance, 0, 1))\n\n        self.summary_op = tf.summary.merge([\n            tf.summary.image(""G"", self.G),\n            tf.summary.image(""AE_G"", self.AE_G),\n            tf.summary.image(""AE_x"", self.AE_x),\n\n            tf.summary.scalar(""loss/d_loss"", self.d_loss),\n            tf.summary.scalar(""loss/d_loss_real"", self.d_loss_real),\n            tf.summary.scalar(""loss/d_loss_fake"", self.d_loss_fake),\n            tf.summary.scalar(""loss/g_loss"", self.g_loss),\n            tf.summary.scalar(""misc/measure"", self.measure),\n            tf.summary.scalar(""misc/k_t"", self.k_t),\n            tf.summary.scalar(""misc/d_lr"", self.d_lr),\n            tf.summary.scalar(""misc/g_lr"", self.g_lr),\n            tf.summary.scalar(""misc/balance"", self.balance),\n        ])\n\n    def build_test_model(self):\n        with tf.variable_scope(""test"") as vs:\n            # Extra ops for interpolation\n            z_optimizer = tf.train.AdamOptimizer(0.0001)\n\n            self.z_r = tf.get_variable(""z_r"", [self.batch_size, self.z_num], tf.float32)\n            self.z_r_update = tf.assign(self.z_r, self.z)\n\n        G_z_r, _ = GeneratorCNN(\n                self.z_r, self.conv_hidden_num, self.channel, self.repeat_num, self.data_format, reuse=True)\n\n        with tf.variable_scope(""test"") as vs:\n            self.z_r_loss = tf.reduce_mean(tf.abs(self.x - G_z_r))\n            self.z_r_optim = z_optimizer.minimize(self.z_r_loss, var_list=[self.z_r])\n\n        test_variables = tf.contrib.framework.get_variables(vs)\n        self.sess.run(tf.variables_initializer(test_variables))\n\n    def generate(self, inputs, root_path=None, path=None, idx=None, save=True):\n        x = self.sess.run(self.G, {self.z: inputs})\n        if path is None and save:\n            path = os.path.join(root_path, \'{}_G.png\'.format(idx))\n            save_image(x, path)\n            print(""[*] Samples saved: {}"".format(path))\n        return x\n\n    def autoencode(self, inputs, path, idx=None, x_fake=None):\n        items = {\n            \'real\': inputs,\n            \'fake\': x_fake,\n        }\n        for key, img in items.items():\n            if img is None:\n                continue\n            if img.shape[3] in [1, 3]:\n                img = img.transpose([0, 3, 1, 2])\n\n            x_path = os.path.join(path, \'{}_D_{}.png\'.format(idx, key))\n            x = self.sess.run(self.AE_x, {self.x: img})\n            save_image(x, x_path)\n            print(""[*] Samples saved: {}"".format(x_path))\n\n    def encode(self, inputs):\n        if inputs.shape[3] in [1, 3]:\n            inputs = inputs.transpose([0, 3, 1, 2])\n        return self.sess.run(self.D_z, {self.x: inputs})\n\n    def decode(self, z):\n        return self.sess.run(self.AE_x, {self.D_z: z})\n\n    def interpolate_G(self, real_batch, step=0, root_path=\'.\', train_epoch=0):\n        batch_size = len(real_batch)\n        half_batch_size = int(batch_size/2)\n\n        self.sess.run(self.z_r_update)\n        tf_real_batch = to_nchw_numpy(real_batch)\n        for i in trange(train_epoch):\n            z_r_loss, _ = self.sess.run([self.z_r_loss, self.z_r_optim], {self.x: tf_real_batch})\n        z = self.sess.run(self.z_r)\n\n        z1, z2 = z[:half_batch_size], z[half_batch_size:]\n        real1_batch, real2_batch = real_batch[:half_batch_size], real_batch[half_batch_size:]\n\n        generated = []\n        for idx, ratio in enumerate(np.linspace(0, 1, 10)):\n            z = np.stack([slerp(ratio, r1, r2) for r1, r2 in zip(z1, z2)])\n            z_decode = self.generate(z, save=False)\n            generated.append(z_decode)\n\n        generated = np.stack(generated).transpose([1, 0, 2, 3, 4])\n        for idx, img in enumerate(generated):\n            save_image(img, os.path.join(root_path, \'test{}_interp_G_{}.png\'.format(step, idx)), nrow=10)\n\n        all_img_num = np.prod(generated.shape[:2])\n        batch_generated = np.reshape(generated, [all_img_num] + list(generated.shape[2:]))\n        save_image(batch_generated, os.path.join(root_path, \'test{}_interp_G.png\'.format(step)), nrow=10)\n\n    def interpolate_D(self, real1_batch, real2_batch, step=0, root_path="".""):\n        real1_encode = self.encode(real1_batch)\n        real2_encode = self.encode(real2_batch)\n\n        decodes = []\n        for idx, ratio in enumerate(np.linspace(0, 1, 10)):\n            z = np.stack([slerp(ratio, r1, r2) for r1, r2 in zip(real1_encode, real2_encode)])\n            z_decode = self.decode(z)\n            decodes.append(z_decode)\n\n        decodes = np.stack(decodes).transpose([1, 0, 2, 3, 4])\n        for idx, img in enumerate(decodes):\n            img = np.concatenate([[real1_batch[idx]], img, [real2_batch[idx]]], 0)\n            save_image(img, os.path.join(root_path, \'test{}_interp_D_{}.png\'.format(step, idx)), nrow=10 + 2)\n\n    def test(self):\n        root_path = ""./""#self.model_dir\n\n        all_G_z = None\n        for step in range(3):\n            real1_batch = self.get_image_from_loader()\n            real2_batch = self.get_image_from_loader()\n\n            save_image(real1_batch, os.path.join(root_path, \'test{}_real1.png\'.format(step)))\n            save_image(real2_batch, os.path.join(root_path, \'test{}_real2.png\'.format(step)))\n\n            self.autoencode(\n                    real1_batch, self.model_dir, idx=os.path.join(root_path, ""test{}_real1"".format(step)))\n            self.autoencode(\n                    real2_batch, self.model_dir, idx=os.path.join(root_path, ""test{}_real2"".format(step)))\n\n            self.interpolate_G(real1_batch, step, root_path)\n            #self.interpolate_D(real1_batch, real2_batch, step, root_path)\n\n            z_fixed = np.random.uniform(-1, 1, size=(self.batch_size, self.z_num))\n            G_z = self.generate(z_fixed, path=os.path.join(root_path, ""test{}_G_z.png"".format(step)))\n\n            if all_G_z is None:\n                all_G_z = G_z\n            else:\n                all_G_z = np.concatenate([all_G_z, G_z])\n            save_image(all_G_z, \'{}/G_z{}.png\'.format(root_path, step))\n\n        save_image(all_G_z, \'{}/all_G_z.png\'.format(root_path), nrow=16)\n\n    def get_image_from_loader(self):\n        x = self.data_loader.eval(session=self.sess)\n        if self.data_format == \'NCHW\':\n            x = x.transpose([0, 2, 3, 1])\n        return x\n'"
utils.py,0,"b'from __future__ import print_function\n\nimport os\nimport math\nimport json\nimport logging\nimport numpy as np\nfrom PIL import Image\nfrom datetime import datetime\n\ndef prepare_dirs_and_logger(config):\n    formatter = logging.Formatter(""%(asctime)s:%(levelname)s::%(message)s"")\n    logger = logging.getLogger()\n\n    for hdlr in logger.handlers:\n        logger.removeHandler(hdlr)\n\n    handler = logging.StreamHandler()\n    handler.setFormatter(formatter)\n\n    logger.addHandler(handler)\n\n    if config.load_path:\n        if config.load_path.startswith(config.log_dir):\n            config.model_dir = config.load_path\n        else:\n            if config.load_path.startswith(config.dataset):\n                config.model_name = config.load_path\n            else:\n                config.model_name = ""{}_{}"".format(config.dataset, config.load_path)\n    else:\n        config.model_name = ""{}_{}"".format(config.dataset, get_time())\n\n    if not hasattr(config, \'model_dir\'):\n        config.model_dir = os.path.join(config.log_dir, config.model_name)\n    config.data_path = os.path.join(config.data_dir, config.dataset)\n\n    for path in [config.log_dir, config.data_dir, config.model_dir]:\n        if not os.path.exists(path):\n            os.makedirs(path)\n\ndef get_time():\n    return datetime.now().strftime(""%m%d_%H%M%S"")\n\ndef save_config(config):\n    param_path = os.path.join(config.model_dir, ""params.json"")\n\n    print(""[*] MODEL dir: %s"" % config.model_dir)\n    print(""[*] PARAM path: %s"" % param_path)\n\n    with open(param_path, \'w\') as fp:\n        json.dump(config.__dict__, fp, indent=4, sort_keys=True)\n\ndef rank(array):\n    return len(array.shape)\n\ndef make_grid(tensor, nrow=8, padding=2,\n              normalize=False, scale_each=False):\n    """"""Code based on https://github.com/pytorch/vision/blob/master/torchvision/utils.py""""""\n    nmaps = tensor.shape[0]\n    xmaps = min(nrow, nmaps)\n    ymaps = int(math.ceil(float(nmaps) / xmaps))\n    height, width = int(tensor.shape[1] + padding), int(tensor.shape[2] + padding)\n    grid = np.zeros([height * ymaps + 1 + padding // 2, width * xmaps + 1 + padding // 2, 3], dtype=np.uint8)\n    k = 0\n    for y in range(ymaps):\n        for x in range(xmaps):\n            if k >= nmaps:\n                break\n            h, h_width = y * height + 1 + padding // 2, height - padding\n            w, w_width = x * width + 1 + padding // 2, width - padding\n\n            grid[h:h+h_width, w:w+w_width] = tensor[k]\n            k = k + 1\n    return grid\n\ndef save_image(tensor, filename, nrow=8, padding=2,\n               normalize=False, scale_each=False):\n    ndarr = make_grid(tensor, nrow=nrow, padding=padding,\n                            normalize=normalize, scale_each=scale_each)\n    im = Image.fromarray(ndarr)\n    im.save(filename)\n'"
