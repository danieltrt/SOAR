file_path,api_count,code
data.py,0,"b'import os\nfrom collections import Counter\n\ndef read_data(fname, count, word2idx):\n    if os.path.isfile(fname):\n        with open(fname) as f:\n            lines = f.readlines()\n    else:\n        raise(""[!] Data %s not found"" % fname)\n\n    words = []\n    for line in lines:\n        words.extend(line.split())\n\n    if len(count) == 0:\n        count.append([\'<eos>\', 0])\n\n    count[0][1] += len(lines)\n    count.extend(Counter(words).most_common())\n\n    if len(word2idx) == 0:\n        word2idx[\'<eos>\'] = 0\n\n    for word, _ in count:\n        if word not in word2idx:\n            word2idx[word] = len(word2idx)\n\n    data = list()\n    for line in lines:\n        for word in line.split():\n            index = word2idx[word]\n            data.append(index)\n        data.append(word2idx[\'<eos>\'])\n\n    print(""Read %s words from %s"" % (len(data), fname))\n    return data\n'"
main.py,3,"b'import os\nimport pprint\nimport tensorflow as tf\n\nfrom data import read_data\nfrom model import MemN2N\n\npp = pprint.PrettyPrinter()\n\nflags = tf.app.flags\n\nflags.DEFINE_integer(""edim"", 150, ""internal state dimension [150]"")\nflags.DEFINE_integer(""lindim"", 75, ""linear part of the state [75]"")\nflags.DEFINE_integer(""nhop"", 6, ""number of hops [6]"")\nflags.DEFINE_integer(""mem_size"", 100, ""memory size [100]"")\nflags.DEFINE_integer(""batch_size"", 128, ""batch size to use during training [128]"")\nflags.DEFINE_integer(""nepoch"", 100, ""number of epoch to use during training [100]"")\nflags.DEFINE_float(""init_lr"", 0.01, ""initial learning rate [0.01]"")\nflags.DEFINE_float(""init_hid"", 0.1, ""initial internal state value [0.1]"")\nflags.DEFINE_float(""init_std"", 0.05, ""weight initialization std [0.05]"")\nflags.DEFINE_float(""max_grad_norm"", 50, ""clip gradients to this norm [50]"")\nflags.DEFINE_string(""data_dir"", ""data"", ""data directory [data]"")\nflags.DEFINE_string(""checkpoint_dir"", ""checkpoints"", ""checkpoint directory [checkpoints]"")\nflags.DEFINE_string(""data_name"", ""ptb"", ""data set name [ptb]"")\nflags.DEFINE_boolean(""is_test"", False, ""True for testing, False for Training [False]"")\nflags.DEFINE_boolean(""show"", False, ""print progress [False]"")\n\nFLAGS = flags.FLAGS\n\ndef main(_):\n    count = []\n    word2idx = {}\n\n    if not os.path.exists(FLAGS.checkpoint_dir):\n      os.makedirs(FLAGS.checkpoint_dir)\n\n    train_data = read_data(\'%s/%s.train.txt\' % (FLAGS.data_dir, FLAGS.data_name), count, word2idx)\n    valid_data = read_data(\'%s/%s.valid.txt\' % (FLAGS.data_dir, FLAGS.data_name), count, word2idx)\n    test_data = read_data(\'%s/%s.test.txt\' % (FLAGS.data_dir, FLAGS.data_name), count, word2idx)\n\n    idx2word = dict(zip(word2idx.values(), word2idx.keys()))\n    FLAGS.nwords = len(word2idx)\n\n    pp.pprint(flags.FLAGS.__flags)\n\n    with tf.Session() as sess:\n        model = MemN2N(FLAGS, sess)\n        model.build_model()\n\n        if FLAGS.is_test:\n            model.run(valid_data, test_data)\n        else:\n            model.run(train_data, valid_data)\n\nif __name__ == \'__main__\':\n    tf.app.run()\n'"
model.py,40,"b'import os\nimport math\nimport random\nimport numpy as np\nimport tensorflow as tf\nfrom past.builtins import xrange\n\nclass MemN2N(object):\n    def __init__(self, config, sess):\n        self.nwords = config.nwords\n        self.init_hid = config.init_hid\n        self.init_std = config.init_std\n        self.batch_size = config.batch_size\n        self.nepoch = config.nepoch\n        self.nhop = config.nhop\n        self.edim = config.edim\n        self.mem_size = config.mem_size\n        self.lindim = config.lindim\n        self.max_grad_norm = config.max_grad_norm\n\n        self.show = config.show\n        self.is_test = config.is_test\n        self.checkpoint_dir = config.checkpoint_dir\n\n        if not os.path.isdir(self.checkpoint_dir):\n            raise Exception("" [!] Directory %s not found"" % self.checkpoint_dir)\n\n        self.input = tf.placeholder(tf.float32, [None, self.edim], name=""input"")\n        self.time = tf.placeholder(tf.int32, [None, self.mem_size], name=""time"")\n        self.target = tf.placeholder(tf.float32, [self.batch_size, self.nwords], name=""target"")\n        self.context = tf.placeholder(tf.int32, [self.batch_size, self.mem_size], name=""context"")\n\n        self.hid = []\n        self.hid.append(self.input)\n        self.share_list = []\n        self.share_list.append([])\n\n        self.lr = None\n        self.current_lr = config.init_lr\n        self.loss = None\n        self.step = None\n        self.optim = None\n\n        self.sess = sess\n        self.log_loss = []\n        self.log_perp = []\n\n    def build_memory(self):\n        self.global_step = tf.Variable(0, name=""global_step"")\n\n        self.A = tf.Variable(tf.random_normal([self.nwords, self.edim], stddev=self.init_std))\n        self.B = tf.Variable(tf.random_normal([self.nwords, self.edim], stddev=self.init_std))\n        self.C = tf.Variable(tf.random_normal([self.edim, self.edim], stddev=self.init_std))\n\n        # Temporal Encoding\n        self.T_A = tf.Variable(tf.random_normal([self.mem_size, self.edim], stddev=self.init_std))\n        self.T_B = tf.Variable(tf.random_normal([self.mem_size, self.edim], stddev=self.init_std))\n\n        # m_i = sum A_ij * x_ij + T_A_i\n        Ain_c = tf.nn.embedding_lookup(self.A, self.context)\n        Ain_t = tf.nn.embedding_lookup(self.T_A, self.time)\n        Ain = tf.add(Ain_c, Ain_t)\n\n        # c_i = sum B_ij * u + T_B_i\n        Bin_c = tf.nn.embedding_lookup(self.B, self.context)\n        Bin_t = tf.nn.embedding_lookup(self.T_B, self.time)\n        Bin = tf.add(Bin_c, Bin_t)\n\n        for h in xrange(self.nhop):\n            self.hid3dim = tf.reshape(self.hid[-1], [-1, 1, self.edim])\n            Aout = tf.matmul(self.hid3dim, Ain, adjoint_b=True)\n            Aout2dim = tf.reshape(Aout, [-1, self.mem_size])\n            P = tf.nn.softmax(Aout2dim)\n\n            probs3dim = tf.reshape(P, [-1, 1, self.mem_size])\n            Bout = tf.matmul(probs3dim, Bin)\n            Bout2dim = tf.reshape(Bout, [-1, self.edim])\n\n            Cout = tf.matmul(self.hid[-1], self.C)\n            Dout = tf.add(Cout, Bout2dim)\n\n            self.share_list[0].append(Cout)\n\n            if self.lindim == self.edim:\n                self.hid.append(Dout)\n            elif self.lindim == 0:\n                self.hid.append(tf.nn.relu(Dout))\n            else:\n                F = tf.slice(Dout, [0, 0], [self.batch_size, self.lindim])\n                G = tf.slice(Dout, [0, self.lindim], [self.batch_size, self.edim-self.lindim])\n                K = tf.nn.relu(G)\n                self.hid.append(tf.concat(axis=1, values=[F, K]))\n\n    def build_model(self):\n        self.build_memory()\n\n        self.W = tf.Variable(tf.random_normal([self.edim, self.nwords], stddev=self.init_std))\n        z = tf.matmul(self.hid[-1], self.W)\n\n        self.loss = tf.nn.softmax_cross_entropy_with_logits(logits=z, labels=self.target)\n\n        self.lr = tf.Variable(self.current_lr)\n        self.opt = tf.train.GradientDescentOptimizer(self.lr)\n\n        params = [self.A, self.B, self.C, self.T_A, self.T_B, self.W]\n        grads_and_vars = self.opt.compute_gradients(self.loss,params)\n        clipped_grads_and_vars = [(tf.clip_by_norm(gv[0], self.max_grad_norm), gv[1]) \\\n                                   for gv in grads_and_vars]\n\n        inc = self.global_step.assign_add(1)\n        with tf.control_dependencies([inc]):\n            self.optim = self.opt.apply_gradients(clipped_grads_and_vars)\n\n        tf.global_variables_initializer().run()\n        self.saver = tf.train.Saver()\n\n    def train(self, data):\n        N = int(math.ceil(len(data) / self.batch_size))\n        cost = 0\n\n        x = np.ndarray([self.batch_size, self.edim], dtype=np.float32)\n        time = np.ndarray([self.batch_size, self.mem_size], dtype=np.int32)\n        target = np.zeros([self.batch_size, self.nwords]) # one-hot-encoded\n        context = np.ndarray([self.batch_size, self.mem_size])\n\n        x.fill(self.init_hid)\n        for t in xrange(self.mem_size):\n            time[:,t].fill(t)\n\n        if self.show:\n            from utils import ProgressBar\n            bar = ProgressBar(\'Train\', max=N)\n\n        for idx in xrange(N):\n            if self.show: bar.next()\n            target.fill(0)\n            for b in xrange(self.batch_size):\n                m = random.randrange(self.mem_size, len(data))\n                target[b][data[m]] = 1\n                context[b] = data[m - self.mem_size:m]\n\n            _, loss, self.step = self.sess.run([self.optim,\n                                                self.loss,\n                                                self.global_step],\n                                                feed_dict={\n                                                    self.input: x,\n                                                    self.time: time,\n                                                    self.target: target,\n                                                    self.context: context})\n            cost += np.sum(loss)\n\n        if self.show: bar.finish()\n        return cost/N/self.batch_size\n\n    def test(self, data, label=\'Test\'):\n        N = int(math.ceil(len(data) / self.batch_size))\n        cost = 0\n\n        x = np.ndarray([self.batch_size, self.edim], dtype=np.float32)\n        time = np.ndarray([self.batch_size, self.mem_size], dtype=np.int32)\n        target = np.zeros([self.batch_size, self.nwords]) # one-hot-encoded\n        context = np.ndarray([self.batch_size, self.mem_size])\n\n        x.fill(self.init_hid)\n        for t in xrange(self.mem_size):\n            time[:,t].fill(t)\n\n        if self.show:\n            from utils import ProgressBar\n            bar = ProgressBar(label, max=N)\n\n        m = self.mem_size\n        for idx in xrange(N):\n            if self.show: bar.next()\n            target.fill(0)\n            for b in xrange(self.batch_size):\n                target[b][data[m]] = 1\n                context[b] = data[m - self.mem_size:m]\n                m += 1\n\n                if m >= len(data):\n                    m = self.mem_size\n\n            loss = self.sess.run([self.loss], feed_dict={self.input: x,\n                                                         self.time: time,\n                                                         self.target: target,\n                                                         self.context: context})\n            cost += np.sum(loss)\n\n        if self.show: bar.finish()\n        return cost/N/self.batch_size\n\n    def run(self, train_data, test_data):\n        if not self.is_test:\n            for idx in xrange(self.nepoch):\n                train_loss = np.sum(self.train(train_data))\n                test_loss = np.sum(self.test(test_data, label=\'Validation\'))\n\n                # Logging\n                self.log_loss.append([train_loss, test_loss])\n                self.log_perp.append([math.exp(train_loss), math.exp(test_loss)])\n\n                state = {\n                    \'perplexity\': math.exp(train_loss),\n                    \'epoch\': idx,\n                    \'learning_rate\': self.current_lr,\n                    \'valid_perplexity\': math.exp(test_loss)\n                }\n                print(state)\n\n                # Learning rate annealing\n                if len(self.log_loss) > 1 and self.log_loss[idx][1] > self.log_loss[idx-1][1] * 0.9999:\n                    self.current_lr = self.current_lr / 1.5\n                    self.lr.assign(self.current_lr).eval()\n                if self.current_lr < 1e-5: break\n\n                if idx % 10 == 0:\n                    self.saver.save(self.sess,\n                                    os.path.join(self.checkpoint_dir, ""MemN2N.model""),\n                                    global_step = self.step.astype(int))\n        else:\n            self.load()\n\n            valid_loss = np.sum(self.test(train_data, label=\'Validation\'))\n            test_loss = np.sum(self.test(test_data, label=\'Test\'))\n\n            state = {\n                \'valid_perplexity\': math.exp(valid_loss),\n                \'test_perplexity\': math.exp(test_loss)\n            }\n            print(state)\n\n    def load(self):\n        print("" [*] Reading checkpoints..."")\n        ckpt = tf.train.get_checkpoint_state(self.checkpoint_dir)\n        if ckpt and ckpt.model_checkpoint_path:\n            self.saver.restore(self.sess, ckpt.model_checkpoint_path)\n        else:\n            raise Exception("" [!] Trest mode but no checkpoint found"")\n'"
utils.py,0,"b""from progress.bar import Bar\n\nclass ProgressBar(Bar):\n    message = 'Loading'\n    fill = '#'\n    suffix = '%(percent).1f%% | ETA: %(eta)ds'\n"""
