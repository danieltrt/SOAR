file_path,api_count,code
main.py,4,"b'import gym\nimport logging\nimport numpy as np\nimport tensorflow as tf\n\nfrom src.naf import NAF\nfrom src.network import Network\nfrom src.statistic import Statistic\nfrom src.exploration import OUExploration, BrownianExploration, LinearDecayExploration\nfrom utils import get_model_dir, preprocess_conf\n\nflags = tf.app.flags\n\n# environment\nflags.DEFINE_string(\'env_name\', \'Pendulum-v0\', \'name of environment\')\n\n# network\nflags.DEFINE_string(\'hidden_dims\', \'[100, 100]\', \'dimension of hidden layers\')\nflags.DEFINE_boolean(\'use_batch_norm\', False, \'use batch normalization or not\')\nflags.DEFINE_boolean(\'clip_action\', False, \'whether to clip an action with given bound\')\nflags.DEFINE_boolean(\'use_seperate_networks\', False, \'use seperate networks for mu, V and A\')\nflags.DEFINE_string(\'hidden_w\', \'uniform_big\', \'weight initialization of hidden layers [uniform_small, uniform_big, he]\')\nflags.DEFINE_string(\'hidden_fn\', \'tanh\', \'activation function of hidden layer [none, tanh, relu]\')\nflags.DEFINE_string(\'action_w\', \'uniform_big\', \'weight initilization of action layer [uniform_small, uniform_big, he]\')\nflags.DEFINE_string(\'action_fn\', \'tanh\', \'activation function of action layer [none, tanh, relu]\')\nflags.DEFINE_string(\'w_reg\', \'none\', \'weight regularization [none, l1, l2]\')\nflags.DEFINE_float(\'w_reg_scale\', 0.001, \'scale of regularization\')\n\n# exploration\nflags.DEFINE_float(\'noise_scale\', 0.3, \'scale of noise\')\nflags.DEFINE_string(\'noise\', \'linear_decay\', \'type of noise exploration [ou, linear_decay, brownian]\')\n\n# training\nflags.DEFINE_float(\'tau\', 0.001, \'tau of soft target update\')\nflags.DEFINE_float(\'discount\', 0.99, \'discount factor of Q-learning\')\nflags.DEFINE_float(\'learning_rate\', 1e-3, \'value of learning rate\')\nflags.DEFINE_integer(\'batch_size\', 100, \'The size of batch for minibatch training\')\nflags.DEFINE_integer(\'max_steps\', 200, \'maximum # of steps for each episode\')\nflags.DEFINE_integer(\'update_repeat\', 10, \'maximum # of q-learning updates for each step\')\nflags.DEFINE_integer(\'max_episodes\', 10000, \'maximum # of episodes to train\')\n\n# Debug\nflags.DEFINE_boolean(\'is_train\', True, \'training or testing\')\nflags.DEFINE_integer(\'random_seed\', 123, \'random seed\')\nflags.DEFINE_boolean(\'monitor\', False, \'monitor the training or not\')\nflags.DEFINE_boolean(\'display\', False, \'display the game screen or not\')\nflags.DEFINE_string(\'log_level\', \'INFO\', \'log level [DEBUG, INFO, WARNING, ERROR, CRITICAL]\')\n\nconf = flags.FLAGS\n\nlogger = logging.getLogger()\nlogger.propagate = False\nlogger.setLevel(conf.log_level)\n\n# set random seed\ntf.set_random_seed(conf.random_seed)\nnp.random.seed(conf.random_seed)\n\ndef main(_):\n  model_dir = get_model_dir(conf,\n      [\'is_train\', \'random_seed\', \'monitor\', \'display\', \'log_level\'])\n\n  preprocess_conf(conf)\n\n  with tf.Session() as sess:\n    # environment\n    env = gym.make(conf.env_name)\n    env.seed(conf.random_seed)\n\n    assert isinstance(env.observation_space, gym.spaces.Box), \\\n      ""observation space must be continuous""\n    assert isinstance(env.action_space, gym.spaces.Box), \\\n      ""action space must be continuous""\n\n    # exploration strategy\n    if conf.noise == \'ou\':\n      strategy = OUExploration(env, sigma=conf.noise_scale)\n    elif conf.noise == \'brownian\':\n      strategy = BrownianExploration(env, conf.noise_scale)\n    elif conf.noise == \'linear_decay\':\n      strategy = LinearDecayExploration(env)\n    else:\n      raise ValueError(\'Unkown exploration strategy: %s\' % conf.noise)\n\n    # networks\n    shared_args = {\n      \'sess\': sess,\n      \'input_shape\': env.observation_space.shape,\n      \'action_size\': env.action_space.shape[0],\n      \'hidden_dims\': conf.hidden_dims,\n      \'use_batch_norm\': conf.use_batch_norm,\n      \'use_seperate_networks\': conf.use_seperate_networks,\n      \'hidden_w\': conf.hidden_w, \'action_w\': conf.action_w,\n      \'hidden_fn\': conf.hidden_fn, \'action_fn\': conf.action_fn,\n      \'w_reg\': conf.w_reg,\n    }\n\n    logger.info(""Creating prediction network..."")\n    pred_network = Network(\n      scope=\'pred_network\', **shared_args\n    )\n\n    logger.info(""Creating target network..."")\n    target_network = Network(\n      scope=\'target_network\', **shared_args\n    )\n    target_network.make_soft_update_from(pred_network, conf.tau)\n\n    # statistic\n    stat = Statistic(sess, conf.env_name, model_dir, pred_network.variables, conf.update_repeat)\n\n    agent = NAF(sess, env, strategy, pred_network, target_network, stat,\n                conf.discount, conf.batch_size, conf.learning_rate,\n                conf.max_steps, conf.update_repeat, conf.max_episodes)\n\n    agent.run(conf.monitor, conf.display, conf.is_train)\n\nif __name__ == \'__main__\':\n  tf.app.run()\n'"
utils.py,2,"b'import os\nimport pprint\nimport tensorflow as tf\n\nfrom src.network import *\n\npp = pprint.PrettyPrinter().pprint\n\ndef get_model_dir(config, exceptions=None):\n\n  attrs = config.__flags\n  pp(attrs)\n\n  keys = attrs.keys()\n  keys.sort()\n  keys.remove(\'env_name\')\n  keys = [\'env_name\'] + keys\n\n  names =[]\n  for key in keys:\n    # Only use useful flags\n    if key not in exceptions:\n      names.append(""%s=%s"" % (key, "","".join([str(i) for i in attrs[key]])\n          if type(attrs[key]) == list else attrs[key]))\n  return os.path.join(\'checkpoints\', *names) + \'/\'\n\ndef preprocess_conf(conf):\n  options = conf.__flags\n\n  for option, value in options.items():\n    option = option.lower()\n    value = value.value\n\n    if option == \'hidden_dims\':\n      conf.hidden_dims = eval(conf.hidden_dims)\n    elif option == \'w_reg\':\n      if value == \'l1\':\n        w_reg = l1_regularizer(conf.w_reg_scale)\n      elif value == \'l2\':\n        w_reg = l2_regularizer(conf.w_reg_scale)\n      elif value == \'none\':\n        w_reg = None\n      else:\n        raise ValueError(\'Wrong weight regularizer %s: %s\' % (option, value))\n      conf.w_reg = w_reg\n    elif option.endswith(\'_w\'):\n      if value == \'uniform_small\':\n        weights_initializer = random_uniform_small\n      elif value == \'uniform_big\':\n        weights_initializer = random_uniform_big\n      elif value == \'he\':\n        weights_initializer = he_uniform\n      else:\n        raise ValueError(\'Wrong %s: %s\' % (option, value))\n      setattr(conf, option, weights_initializer)\n    elif option.endswith(\'_fn\'):\n      if value == \'tanh\':\n        activation_fn = tf.nn.tanh\n      elif value == \'relu\':\n        activation_fn = tf.nn.relu\n      elif value == \'none\':\n        activation_fn = None\n      else:\n        raise ValueError(\'Wrong %s: %s\' % (option, value))\n      setattr(conf, option, activation_fn)\n'"
src/__init__.py,0,b''
src/exploration.py,0,"b""import numpy as np\nimport numpy.random as nr\n\nclass Exploration(object):\n  def __init__(self, env):\n    self.action_size = env.action_space.shape[0]\n\n  def add_noise(self, action, info={}):\n    pass\n\n  def reset(self):\n    pass\n\nclass OUExploration(Exploration):\n  # Reference: https://github.com/rllab/rllab/blob/master/rllab/exploration_strategies/ou_strategy.py\n\n  def __init__(self, env, sigma=0.3, mu=0, theta=0.15):\n    super(OUExploration, self).__init__(env)\n\n    self.mu = mu\n    self.theta = theta\n    self.sigma = sigma\n\n    self.state = np.ones(self.action_size) * self.mu\n    self.reset()\n\n  def add_noise(self, action, info={}):\n    x = self.state\n    dx = self.theta * (self.mu - x) + self.sigma * nr.randn(len(x))\n    self.state = x + dx\n\n    return action + self.state\n\n  def reset(self):\n    self.state = np.ones(self.action_size) * self.mu\n\nclass LinearDecayExploration(Exploration):\n  def __init__(self, env):\n    super(LinearDecayExploration, self).__init__(env)\n\n  def add_noise(self, action, info={}):\n    return action + np.random.randn(self.action_size) / (info['idx_episode'] + 1)\n\nclass BrownianExploration(Exploration):\n  def __init__(self, env, noise_scale):\n    super(BrownianExploration, self).__init__(env)\n\n    raise Exception('not implemented yet')\n"""
src/naf.py,7,"b'from logging import getLogger\nlogger = getLogger(__name__)\n\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow.contrib.framework import get_variables\n\nfrom .utils import get_timestamp\n\nclass NAF(object):\n  def __init__(self, sess,\n               env, strategy, pred_network, target_network, stat,\n               discount, batch_size, learning_rate,\n               max_steps, update_repeat, max_episodes):\n    self.sess = sess\n    self.env = env\n    self.strategy = strategy\n    self.pred_network = pred_network\n    self.target_network = target_network\n    self.stat = stat\n\n    self.discount = discount\n    self.batch_size = batch_size\n    self.learning_rate = learning_rate\n    self.action_size = env.action_space.shape[0]\n\n    self.max_steps = max_steps\n    self.update_repeat = update_repeat\n    self.max_episodes = max_episodes\n\n    self.prestates = []\n    self.actions = []\n    self.rewards = []\n    self.poststates = []\n    self.terminals = []\n\n    with tf.name_scope(\'optimizer\'):\n      self.target_y = tf.placeholder(tf.float32, [None], name=\'target_y\')\n      self.loss = tf.reduce_mean(tf.squared_difference(self.target_y, tf.squeeze(self.pred_network.Q)), name=\'loss\')\n\n      self.optim = tf.train.AdamOptimizer(self.learning_rate).minimize(self.loss)\n\n  def run(self, monitor=False, display=False, is_train=True):\n    self.stat.load_model()\n    self.target_network.hard_copy_from(self.pred_network)\n\n    if monitor:\n      self.env.monitor.start(\'/tmp/%s-%s\' % (self.stat.env_name, get_timestamp()))\n\n    for self.idx_episode in xrange(self.max_episodes):\n      state = self.env.reset()\n\n      for t in xrange(0, self.max_steps):\n        if display: self.env.render()\n\n        # 1. predict\n        action = self.predict(state)\n\n        # 2. step\n        self.prestates.append(state)\n        state, reward, terminal, _ = self.env.step(action)\n        self.poststates.append(state)\n\n        terminal = True if t == self.max_steps - 1 else terminal\n\n        # 3. perceive\n        if is_train:\n          q, v, a, l = self.perceive(state, reward, action, terminal)\n\n          if self.stat:\n            self.stat.on_step(action, reward, terminal, q, v, a, l)\n\n        if terminal:\n          self.strategy.reset()\n          break\n\n    if monitor:\n      self.env.monitor.close()\n\n  def run2(self, monitor=False, display=False, is_train=True):\n    target_y = tf.placeholder(tf.float32, [None], name=\'target_y\')\n    loss = tf.reduce_mean(tf.squared_difference(target_y, tf.squeeze(self.pred_network.Q)), name=\'loss\')\n\n    optim = tf.train.AdamOptimizer(self.learning_rate).minimize(loss)\n\n    self.stat.load_model()\n    self.target_network.hard_copy_from(self.pred_network)\n\n    # replay memory\n    prestates = []\n    actions = []\n    rewards = []\n    poststates = []\n    terminals = []\n\n    # the main learning loop\n    total_reward = 0\n    for i_episode in xrange(self.max_episodes):\n      observation = self.env.reset()\n      episode_reward = 0\n\n      for t in xrange(self.max_steps):\n        if display:\n          self.env.render()\n\n        # predict the mean action from current observation\n        x_ = np.array([observation])\n        u_ = self.pred_network.mu.eval({self.pred_network.x: x_})[0]\n\n        action = u_ + np.random.randn(1) / (i_episode + 1)\n\n        prestates.append(observation)\n        actions.append(action)\n\n        observation, reward, done, info = self.env.step(action)\n        episode_reward += reward\n\n        rewards.append(reward); poststates.append(observation); terminals.append(done)\n\n        if len(prestates) > 10:\n          loss_ = 0\n          for k in xrange(self.update_repeat):\n            if len(prestates) > self.batch_size:\n              indexes = np.random.choice(len(prestates), size=self.batch_size)\n            else:\n              indexes = range(len(prestates))\n\n            # Q-update\n            v_ = self.target_network.V.eval({self.target_network.x: np.array(poststates)[indexes]})\n            y_ = np.array(rewards)[indexes] + self.discount * np.squeeze(v_)\n\n            tmp1, tmp2 = np.array(prestates)[indexes], np.array(actions)[indexes]\n            loss_ += l_\n\n            self.target_network.soft_update_from(self.pred_network)\n\n        if done:\n          break\n\n      print ""average loss:"", loss_/k\n      print ""Episode {} finished after {} timesteps, reward {}"".format(i_episode + 1, t + 1, episode_reward)\n      total_reward += episode_reward\n\n    print ""Average reward per episode {}"".format(total_reward / self.episodes)\n\n  def predict(self, state):\n    u = self.pred_network.predict([state])[0]\n\n    return self.strategy.add_noise(u, {\'idx_episode\': self.idx_episode})\n\n  def perceive(self, state, reward, action, terminal):\n    self.rewards.append(reward)\n    self.actions.append(action)\n\n    return self.q_learning_minibatch()\n\n  def q_learning_minibatch(self):\n    q_list = []\n    v_list = []\n    a_list = []\n    l_list = []\n\n    for iteration in xrange(self.update_repeat):\n      if len(self.rewards) >= self.batch_size:\n        indexes = np.random.choice(len(self.rewards), size=self.batch_size)\n      else:\n        indexes = np.arange(len(self.rewards))\n\n      x_t = np.array(self.prestates)[indexes]\n      x_t_plus_1 = np.array(self.poststates)[indexes]\n      r_t = np.array(self.rewards)[indexes]\n      u_t = np.array(self.actions)[indexes]\n\n      v = self.target_network.predict_v(x_t_plus_1, u_t)\n      target_y = self.discount * np.squeeze(v) + r_t\n\n      _, l, q, v, a = self.sess.run([\n        self.optim, self.loss,\n        self.pred_network.Q, self.pred_network.V, self.pred_network.A,\n      ], {\n        self.target_y: target_y,\n        self.pred_network.x: x_t,\n        self.pred_network.u: u_t,\n        self.pred_network.is_train: True,\n      })\n\n      q_list.extend(q)\n      v_list.extend(v)\n      a_list.extend(a)\n      l_list.append(l)\n\n      self.target_network.soft_update_from(self.pred_network)\n\n      logger.debug(""q: %s, v: %s, a: %s, l: %s"" \\\n        % (np.mean(q), np.mean(v), np.mean(a), np.mean(l)))\n\n    return np.sum(q_list), np.sum(v_list), np.sum(a_list), np.sum(l_list)\n'"
src/network.py,20,"b'from logging import getLogger\nlogger = getLogger(__name__)\n\nimport tensorflow as tf\nfrom tensorflow.contrib.framework import get_variables\n\nfrom .ops import *\n\nclass Network:\n  def __init__(self, sess, input_shape, action_size, hidden_dims,\n               use_batch_norm, use_seperate_networks,\n               hidden_w, action_w, hidden_fn, action_fn, w_reg,\n               scope=\'NAF\'):\n    self.sess = sess\n    with tf.variable_scope(scope):\n      x = tf.placeholder(tf.float32, (None,) + tuple(input_shape), name=\'observations\')\n      u = tf.placeholder(tf.float32, (None, action_size), name=\'actions\')\n      is_train = tf.placeholder(tf.bool, name=\'is_train\')\n\n      hid_outs = {}\n      with tf.name_scope(\'hidden\'):\n        if use_seperate_networks:\n          logger.info(""Creating seperate networks for v, l, and mu"")\n\n          for scope in [\'v\', \'l\', \'mu\']:\n            with tf.variable_scope(scope):\n              if use_batch_norm:\n                h = batch_norm(x, is_training=is_train)\n              else:\n                h = x\n\n              for idx, hidden_dim in enumerate(hidden_dims):\n                h = fc(h, hidden_dim, is_train, hidden_w, weight_reg=w_reg,\n                       activation_fn=hidden_fn, use_batch_norm=use_batch_norm, scope=\'hid%d\' % idx)\n              hid_outs[scope] = h\n        else:\n          logger.info(""Creating shared networks for v, l, and mu"")\n\n          if use_batch_norm:\n            h = batch_norm(x, is_training=is_train)\n          else:\n            h = x\n\n          for idx, hidden_dim in enumerate(hidden_dims):\n            h = fc(h, hidden_dim, is_train, hidden_w, weight_reg=w_reg,\n                   activation_fn=hidden_fn, use_batch_norm=use_batch_norm, scope=\'hid%d\' % idx)\n          hid_outs[\'v\'], hid_outs[\'l\'], hid_outs[\'mu\'] = h, h, h\n\n      with tf.name_scope(\'value\'):\n        V = fc(hid_outs[\'v\'], 1, is_train,\n               hidden_w, use_batch_norm=use_batch_norm, scope=\'V\')\n\n      with tf.name_scope(\'advantage\'):\n        l = fc(hid_outs[\'l\'], (action_size * (action_size + 1))/2, is_train, hidden_w,\n               use_batch_norm=use_batch_norm, scope=\'l\')\n        mu = fc(hid_outs[\'mu\'], action_size, is_train, action_w,\n                activation_fn=action_fn, use_batch_norm=use_batch_norm, scope=\'mu\')\n\n        pivot = 0\n        rows = []\n        for idx in xrange(action_size):\n          count = action_size - idx\n\n          diag_elem = tf.exp(tf.slice(l, (0, pivot), (-1, 1)))\n          non_diag_elems = tf.slice(l, (0, pivot+1), (-1, count-1))\n          row = tf.pad(tf.concat((diag_elem, non_diag_elems), 1), ((0, 0), (idx, 0)))\n          rows.append(row)\n\n          pivot += count\n\n        L = tf.transpose(tf.stack(rows, axis=1), (0, 2, 1))\n        P = tf.matmul(L, tf.transpose(L, (0, 2, 1)))\n\n        tmp = tf.expand_dims(u - mu, -1)\n        A = -tf.matmul(tf.transpose(tmp, [0, 2, 1]), tf.matmul(P, tmp))/2\n        A = tf.reshape(A, [-1, 1])\n\n      with tf.name_scope(\'Q\'):\n        Q = A + V\n\n      with tf.name_scope(\'optimization\'):\n        self.target_y = tf.placeholder(tf.float32, [None], name=\'target_y\')\n        self.loss = tf.reduce_mean(tf.squared_difference(self.target_y, tf.squeeze(Q)), name=\'loss\')\n\n    self.is_train = is_train\n    self.variables = get_variables(scope)\n\n    self.x, self.u, self.mu, self.V, self.Q, self.P, self.A = x, u, mu, V, Q, P, A\n\n  def predict_v(self, x, u):\n    return self.sess.run(self.V, {\n      self.x: x, self.u: u, self.is_train: False,\n    })\n\n  def predict(self, state):\n    return self.sess.run(self.mu, {\n      self.x: state, self.is_train: False\n    })\n\n  def update(self, optim, target_v, x_t, u_t):\n    _, q, v, a, l = self.sess.run([\n        optim, self.Q, self.V, self.A, self.loss\n      ], {\n        self.target_y: target_v,\n        self.x: x_t,\n        self.u: u_t,\n        self.is_train: True,\n      })\n    return q, v, a, l\n\n  def make_soft_update_from(self, network, tau):\n    logger.info(""Creating ops for soft target update..."")\n    assert len(network.variables) == len(self.variables), \\\n      ""target and prediction network should have same # of variables""\n\n    self.assign_op = {}\n    for from_, to_ in zip(network.variables, self.variables):\n      if \'BatchNorm\' in to_.name:\n        self.assign_op[to_.name] = to_.assign(from_)\n      else:\n        self.assign_op[to_.name] = to_.assign(tau * from_ + (1-tau) * to_)\n\n  def hard_copy_from(self, network):\n    logger.info(""Creating ops for hard target update..."")\n    assert len(network.variables) == len(self.variables), \\\n      ""target and prediction network should have same # of variables""\n\n    for from_, to_ in zip(network.variables, self.variables):\n      self.sess.run(to_.assign(from_))\n\n  def soft_update_from(self, network):\n    for variable in self.variables:\n      self.sess.run(self.assign_op[variable.name])\n    return True\n'"
src/ops.py,6,"b'import tensorflow as tf\n\nfrom tensorflow.contrib.layers import fully_connected\n# from tensorflow.contrib.layers import initializers\nfrom tensorflow.contrib.layers import l1_regularizer\nfrom tensorflow.contrib.layers import l2_regularizer\nfrom tensorflow.contrib.layers import batch_norm\n\nrandom_uniform_big = tf.random_uniform_initializer(-0.05, 0.05)\nrandom_uniform_small = tf.random_uniform_initializer(-3e-4, 3e-4)\n# he_uniform = initializers.variance_scaling_initializer(factor=2.0, mode=\'FAN_IN\', uniform=False)\nhe_uniform = tf.contrib.layers.variance_scaling_initializer(factor=2.0, mode=\'FAN_IN\', uniform=False)\n\ndef fc(layer, output_size, is_training,\n       weight_init, weight_reg=None, activation_fn=None,\n       use_batch_norm=False, scope=\'fc\'):\n  if use_batch_norm:\n    batch_norm_args = {\n      \'normalizer_fn\': batch_norm,\n      \'normalizer_params\': {\n        \'is_training\': is_training,\n      }\n    }\n  else:\n    batch_norm_args = {}\n\n  with tf.variable_scope(scope):\n    return fully_connected(\n      layer,\n      num_outputs=output_size,\n      activation_fn=activation_fn,\n      weights_initializer=weight_init,\n      weights_regularizer=weight_reg,\n      biases_initializer=tf.constant_initializer(0.0),\n      scope=scope,\n      **batch_norm_args\n    )\n\nfrom tensorflow.contrib.framework.python.ops import add_arg_scope\nfrom tensorflow.contrib.framework.python.ops import variables\nfrom tensorflow.contrib.layers.python.layers import initializers\nfrom tensorflow.contrib.layers.python.layers import utils\n\nfrom tensorflow.python.framework import dtypes\nfrom tensorflow.python.framework import ops\nfrom tensorflow.python.ops import array_ops\nfrom tensorflow.python.ops import control_flow_ops\nfrom tensorflow.python.ops import init_ops\nfrom tensorflow.python.ops import nn\nfrom tensorflow.python.ops import standard_ops\nfrom tensorflow.python.ops import variable_scope\nfrom tensorflow.python.training import moving_averages\n\n@add_arg_scope\ndef batch_norm(inputs,\n               decay=0.999,\n               center=True,\n               scale=False,\n               epsilon=0.001,\n               updates_collections=ops.GraphKeys.UPDATE_OPS,\n               is_training=True,\n               reuse=None,\n               variables_collections=None,\n               outputs_collections=None,\n               trainable=True,\n               scope=None):\n  """"""Code modification of tensorflow/contrib/layers/python/layers/layers.py\n  """"""\n  with variable_scope.variable_op_scope([inputs],\n                                        scope, \'BatchNorm\', reuse=reuse) as sc:\n    inputs = ops.convert_to_tensor(inputs)\n    inputs_shape = inputs.get_shape()\n    inputs_rank = inputs_shape.ndims\n    if inputs_rank is None:\n      raise ValueError(\'Inputs %s has undefined rank.\' % inputs.name)\n    dtype = inputs.dtype.base_dtype\n    axis = list(range(inputs_rank - 1))\n    params_shape = inputs_shape[-1:]\n    if not params_shape.is_fully_defined():\n      raise ValueError(\'Inputs %s has undefined last dimension %s.\' % (\n          inputs.name, params_shape))\n    # Allocate parameters for the beta and gamma of the normalization.\n    beta, gamma = None, None\n    if center:\n      beta_collections = utils.get_variable_collections(variables_collections,\n                                                        \'beta\')\n      beta = variables.model_variable(\'beta\',\n                                      shape=params_shape,\n                                      dtype=dtype,\n                                      initializer=init_ops.zeros_initializer,\n                                      collections=beta_collections,\n                                      trainable=trainable)\n    if scale:\n      gamma_collections = utils.get_variable_collections(variables_collections,\n                                                         \'gamma\')\n      gamma = variables.model_variable(\'gamma\',\n                                       shape=params_shape,\n                                       dtype=dtype,\n                                       initializer=init_ops.ones_initializer,\n                                       collections=gamma_collections,\n                                       trainable=trainable)\n    # Create moving_mean and moving_variance variables and add them to the\n    # appropiate collections.\n    moving_mean_collections = utils.get_variable_collections(\n        variables_collections, \'moving_mean\')\n    moving_mean = variables.model_variable(\n        \'moving_mean\',\n        shape=params_shape,\n        dtype=dtype,\n        initializer=init_ops.zeros_initializer,\n        trainable=False,\n        collections=moving_mean_collections)\n    moving_variance_collections = utils.get_variable_collections(\n        variables_collections, \'moving_variance\')\n    moving_variance = variables.model_variable(\n        \'moving_variance\',\n        shape=params_shape,\n        dtype=dtype,\n        initializer=init_ops.ones_initializer,\n        trainable=False,\n        collections=moving_variance_collections)\n\n    # Calculate the moments based on the individual batch.\n    mean, variance = nn.moments(inputs, axis, shift=moving_mean)\n    # Update the moving_mean and moving_variance moments.\n    update_moving_mean = moving_averages.assign_moving_average(\n        moving_mean, mean, decay)\n    update_moving_variance = moving_averages.assign_moving_average(\n        moving_variance, variance, decay)\n    if updates_collections is None:\n      # Make sure the updates are computed here.\n      with ops.control_dependencies([update_moving_mean,\n                                      update_moving_variance]):\n        outputs = nn.batch_normalization(\n            inputs, mean, variance, beta, gamma, epsilon)\n    else:\n      # Collect the updates to be computed later.\n      ops.add_to_collections(updates_collections, update_moving_mean)\n      ops.add_to_collections(updates_collections, update_moving_variance)\n      outputs = nn.batch_normalization(\n          inputs, mean, variance, beta, gamma, epsilon)\n\n    test_outputs = nn.batch_normalization(\n        inputs, moving_mean, moving_variance, beta, gamma, epsilon)\n\n    outputs = tf.cond(is_training, lambda: outputs, lambda: test_outputs)\n    outputs.set_shape(inputs_shape)\n\n    return utils.collect_named_outputs(outputs_collections, sc.name, outputs)\n'"
src/statistic.py,9,"b'import os\nimport numpy as np\nimport tensorflow as tf\nfrom logging import getLogger\n\nlogger = getLogger(__name__)\n\nclass Statistic(object):\n  def __init__(self, sess, env_name, model_dir, variables, max_update_per_step, max_to_keep=20):\n    self.sess = sess\n    self.env_name = env_name\n    self.max_update_per_step = max_update_per_step\n\n    self.reset()\n    self.max_avg_r = None\n\n    with tf.variable_scope(\'t\'):\n      self.t_op = tf.Variable(0, trainable=False, name=\'t\')\n      self.t_add_op = self.t_op.assign_add(1)\n\n    self.model_dir = model_dir\n    self.saver = tf.train.Saver(variables + [self.t_op], max_to_keep=max_to_keep)\n    self.writer = tf.summary.FileWriter(\'./logs/%s\' % self.model_dir, self.sess.graph)\n\n    with tf.variable_scope(\'summary\'):\n      scalar_summary_tags = [\'total r\', \'avg r\', \'avg q\', \'avg v\', \'avg a\', \'avg l\']\n\n      self.summary_placeholders = {}\n      self.summary_ops = {}\n\n      for tag in scalar_summary_tags:\n        self.summary_placeholders[tag] = tf.placeholder(\'float32\', None, name=tag.replace(\' \', \'_\'))\n        self.summary_ops[tag]  = tf.summary.scalar(\'%s/%s\' % (self.env_name, tag), self.summary_placeholders[tag])\n\n  def reset(self):\n    self.total_q = 0.\n    self.total_v = 0.\n    self.total_a = 0.\n    self.total_l = 0.\n\n    self.ep_step = 0\n    self.ep_rewards = []\n\n  def on_step(self, action, reward, terminal, q, v, a, l):\n    self.t = self.t_add_op.eval(session=self.sess)\n\n    self.total_q += q\n    self.total_v += v\n    self.total_a += a\n    self.total_l += l\n\n    self.ep_step += 1\n    self.ep_rewards.append(reward)\n\n    if terminal:\n      avg_q = self.total_q / self.ep_step / self.max_update_per_step\n      avg_v = self.total_v / self.ep_step / self.max_update_per_step\n      avg_a = self.total_a / self.ep_step / self.max_update_per_step\n      avg_l = self.total_l / self.ep_step / self.max_update_per_step\n\n      avg_r = np.mean(self.ep_rewards)\n      total_r = np.sum(self.ep_rewards)\n\n      logger.info(\'t: %d, R: %.3f, r: %.3f, q: %.3f, v: %.3f, a: %.3f, l: %.3f\' \\\n          % (self.t, total_r, avg_r, avg_q, avg_q, avg_a, avg_l))\n\n      if self.max_avg_r == None:\n        self.max_avg_r = avg_r\n\n      if self.max_avg_r * 0.9 <= avg_r:\n        self.save_model(self.t)\n        self.max_avg_r = max(self.max_avg_r, avg_r)\n\n      self.inject_summary({\n        \'total r\': total_r, \'avg r\': avg_r,\n        \'avg q\': avg_q, \'avg v\': avg_v, \'avg a\': avg_a, \'avg l\': avg_l,\n      }, self.t)\n\n      self.reset()\n\n  def inject_summary(self, tag_dict, t):\n    summary_str_lists = self.sess.run([self.summary_ops[tag] for tag in tag_dict.keys()], {\n      self.summary_placeholders[tag]: value for tag, value in tag_dict.items()\n    })\n    for summary_str in summary_str_lists:\n      self.writer.add_summary(summary_str, t)\n\n  def save_model(self, t):\n    logger.info(""Saving checkpoints..."")\n    model_name = type(self).__name__\n\n    if not os.path.exists(self.model_dir):\n      os.makedirs(self.model_dir)\n    self.saver.save(self.sess, self.model_dir, global_step=t)\n\n  def load_model(self):\n    logger.info(""Loading checkpoints..."")\n    tf.initialize_all_variables().run()\n\n    ckpt = tf.train.get_checkpoint_state(self.model_dir)\n    if ckpt and ckpt.model_checkpoint_path:\n      ckpt_name = os.path.basename(ckpt.model_checkpoint_path)\n      fname = os.path.join(self.model_dir, ckpt_name)\n      self.saver.restore(self.sess, fname)\n      logger.info(""Load SUCCESS: %s"" % fname)\n    else:\n      logger.info(""Load FAILED: %s"" % self.model_dir)\n\n    self.t = self.t_add_op.eval(session=self.sess)\n'"
src/utils.py,0,"b""import datetime\nimport dateutil.tz\n\ndef get_timestamp():\n  now = datetime.datetime.now(dateutil.tz.tzlocal())\n  return now.strftime('%Y_%m_%d_%H_%M_%S')\n"""
