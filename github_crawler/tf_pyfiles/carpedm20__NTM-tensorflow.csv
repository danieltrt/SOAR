file_path,api_count,code
main.py,3,"b'from __future__ import absolute_import\n\nimport importlib\nimport tensorflow as tf\nfrom ntm_cell import NTMCell\nfrom ntm import NTM\n\nfrom utils import pp\n\nflags = tf.app.flags\nflags.DEFINE_string(""task"", ""copy"", ""Task to run [copy, recall]"")\nflags.DEFINE_integer(""epoch"", 100000, ""Epoch to train [100000]"")\nflags.DEFINE_integer(""input_dim"", 10, ""Dimension of input [10]"")\nflags.DEFINE_integer(""output_dim"", 10, ""Dimension of output [10]"")\nflags.DEFINE_integer(""min_length"", 1, ""Minimum length of input sequence [1]"")\nflags.DEFINE_integer(""max_length"", 10, ""Maximum length of output sequence [10]"")\nflags.DEFINE_integer(""controller_layer_size"", 1, ""The size of LSTM controller [1]"")\nflags.DEFINE_integer(""controller_dim"", 100, ""Dimension of LSTM controller [100]"")\nflags.DEFINE_integer(""write_head_size"", 1, ""The number of write head [1]"")\nflags.DEFINE_integer(""read_head_size"", 1, ""The number of read head [1]"")\nflags.DEFINE_integer(""test_max_length"", 120, ""Maximum length of output sequence [120]"")\nflags.DEFINE_string(""checkpoint_dir"", ""checkpoint"", ""Directory name to save the checkpoints [checkpoint]"")\nflags.DEFINE_boolean(""is_train"", False, ""True for training, False for testing [False]"")\nflags.DEFINE_boolean(""continue_train"", None, ""True to continue training from saved checkpoint. False for restarting. None for automatic [None]"")\nFLAGS = flags.FLAGS\n\n\ndef create_ntm(config, sess, **ntm_args):\n    cell = NTMCell(\n        input_dim=config.input_dim,\n        output_dim=config.output_dim,\n        controller_layer_size=config.controller_layer_size,\n        controller_dim=config.controller_dim,\n        write_head_size=config.write_head_size,\n        read_head_size=config.read_head_size)\n    scope = ntm_args.pop(\'scope\', \'NTM-%s\' % config.task)\n    ntm = NTM(\n        cell, sess, config.min_length, config.max_length,\n        test_max_length=config.test_max_length, scope=scope, **ntm_args)\n    return cell, ntm\n\n\ndef main(_):\n    pp.pprint(flags.FLAGS.__flags)\n\n    with tf.device(\'/cpu:0\'), tf.Session() as sess:\n        try:\n            task = importlib.import_module(\'tasks.%s\' % FLAGS.task)\n        except ImportError:\n            print(""task \'%s\' does not have implementation"" % FLAGS.task)\n            raise\n\n        if FLAGS.is_train:\n            cell, ntm = create_ntm(FLAGS, sess)\n            task.train(ntm, FLAGS, sess)\n        else:\n            cell, ntm = create_ntm(FLAGS, sess, forward_only=True)\n\n        ntm.load(FLAGS.checkpoint_dir, FLAGS.task)\n\n        if FLAGS.task == \'copy\':\n            task.run(ntm, int(FLAGS.test_max_length * 1 / 3), sess)\n            print\n            task.run(ntm, int(FLAGS.test_max_length * 2 / 3), sess)\n            print\n            task.run(ntm, int(FLAGS.test_max_length * 3 / 3), sess)\n        else:\n            task.run(ntm, int(FLAGS.test_max_length), sess)\n\n\nif __name__ == \'__main__\':\n    tf.app.run()\n'"
ntm.py,23,"b'from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport numpy as np\nimport tensorflow as tf\nfrom collections import defaultdict\nfrom tensorflow.contrib.legacy_seq2seq import sequence_loss\n\nimport ntm_cell\n\nimport os\nfrom utils import progress\n\ndef softmax_loss_function(labels, inputs):\n  return tf.nn.sigmoid_cross_entropy_with_logits(labels=labels, logits=inputs)\n\nclass NTM(object):\n    def __init__(self, cell, sess,\n                 min_length, max_length,\n                 test_max_length=120,\n                 min_grad=-10, max_grad=+10,\n                 lr=1e-4, momentum=0.9, decay=0.95,\n                 scope=""NTM"", forward_only=False):\n        """"""Create a neural turing machine specified by NTMCell ""cell"".\n\n        Args:\n            cell: An instantce of NTMCell.\n            sess: A TensorFlow session.\n            min_length: Minimum length of input sequence.\n            max_length: Maximum length of input sequence for training.\n            test_max_length: Maximum length of input sequence for testing.\n            min_grad: (optional) Minimum gradient for gradient clipping [-10].\n            max_grad: (optional) Maximum gradient for gradient clipping [+10].\n            lr: (optional) Learning rate [1e-4].\n            momentum: (optional) Momentum of RMSProp [0.9].\n            decay: (optional) Decay rate of RMSProp [0.95].\n        """"""\n        if not isinstance(cell, ntm_cell.NTMCell):\n            raise TypeError(""cell must be an instance of NTMCell"")\n\n        self.cell = cell\n        self.sess = sess\n        self.scope = scope\n\n        self.lr = lr\n        self.momentum = momentum\n        self.decay = decay\n\n        self.min_grad = min_grad\n        self.max_grad = max_grad\n        self.min_length = min_length\n        self.max_length = max_length\n        self._max_length = max_length\n\n        if forward_only:\n            self.max_length = test_max_length\n\n        self.inputs = []\n        self.outputs = {}\n        self.output_logits = {}\n        self.true_outputs = []\n\n        self.prev_states = {}\n        self.input_states = defaultdict(list)\n        self.output_states = defaultdict(list)\n\n        self.start_symbol = tf.placeholder(tf.float32, [self.cell.input_dim],\n                                           name=\'start_symbol\')\n        self.end_symbol = tf.placeholder(tf.float32, [self.cell.input_dim],\n                                         name=\'end_symbol\')\n\n        self.losses = {}\n        self.optims = {}\n        self.grads = {}\n\n        self.saver = None\n        self.params = None\n\n        with tf.variable_scope(self.scope):\n            self.global_step = tf.Variable(0, trainable=False)\n\n        self.build_model(forward_only)\n\n    def build_model(self, forward_only, is_copy=True):\n        print("" [*] Building a NTM model"")\n\n        with tf.variable_scope(self.scope):\n            # present start symbol\n            if is_copy:\n                _, _, prev_state = self.cell(self.start_symbol, state=None)\n                self.save_state(prev_state, 0, self.max_length)\n\n            zeros = np.zeros(self.cell.input_dim, dtype=np.float32)\n\n            tf.get_variable_scope().reuse_variables()\n            for seq_length in xrange(1, self.max_length + 1):\n                progress(seq_length / float(self.max_length))\n\n                input_ = tf.placeholder(tf.float32, [self.cell.input_dim],\n                                        name=\'input_%s\' % seq_length)\n                true_output = tf.placeholder(tf.float32, [self.cell.output_dim],\n                                             name=\'true_output_%s\' % seq_length)\n\n                self.inputs.append(input_)\n                self.true_outputs.append(true_output)\n\n                # present inputs\n                _, _, prev_state = self.cell(input_, prev_state)\n                self.save_state(prev_state, seq_length, self.max_length)\n\n                # present end symbol\n                if is_copy:\n                    _, _, state = self.cell(self.end_symbol, prev_state)\n                    self.save_state(state, seq_length)\n\n                self.prev_states[seq_length] = state\n\n                if not forward_only:\n                    # present targets\n                    outputs, output_logits = [], []\n                    for _ in xrange(seq_length):\n                        output, output_logit, state = self.cell(zeros, state)\n                        self.save_state(state, seq_length, is_output=True)\n                        outputs.append(output)\n                        output_logits.append(output_logit)\n\n                    self.outputs[seq_length] = outputs\n                    self.output_logits[seq_length] = output_logits\n\n            if not forward_only:\n                for seq_length in xrange(self.min_length, self.max_length + 1):\n                    print("" [*] Building a loss model for seq_length %s"" % seq_length)\n\n                    loss = sequence_loss(\n                        logits=self.output_logits[seq_length],\n                        targets=self.true_outputs[0:seq_length],\n                        weights=[1] * seq_length,\n                        average_across_timesteps=False,\n                        average_across_batch=False,\n                        softmax_loss_function=softmax_loss_function)\n\n                    self.losses[seq_length] = loss\n\n                    if not self.params:\n                        self.params = tf.trainable_variables()\n\n                    # grads, norm = tf.clip_by_global_norm(\n                    #                  tf.gradients(loss, self.params), 5)\n\n                    grads = []\n                    for grad in tf.gradients(loss, self.params):\n                        if grad is not None:\n                            grads.append(tf.clip_by_value(grad,\n                                                          self.min_grad,\n                                                          self.max_grad))\n                        else:\n                            grads.append(grad)\n\n                    self.grads[seq_length] = grads\n                    opt = tf.train.RMSPropOptimizer(self.lr,\n                                                    decay=self.decay,\n                                                    momentum=self.momentum)\n\n                    reuse = seq_length != 1\n                    with tf.variable_scope(tf.get_variable_scope(), reuse=reuse):\n                        self.optims[seq_length] = opt.apply_gradients(\n                            zip(grads, self.params),\n                            global_step=self.global_step)\n\n        model_vars = \\\n            [v for v in tf.global_variables() if v.name.startswith(self.scope)]\n        self.saver = tf.train.Saver(model_vars)\n        print("" [*] Build a NTM model finished"")\n\n    def get_outputs(self, seq_length):\n        if not self.outputs.has_key(seq_length):\n            with tf.variable_scope(self.scope):\n                tf.get_variable_scope().reuse_variables()\n\n                zeros = np.zeros(self.cell.input_dim, dtype=np.float32)\n                state = self.prev_states[seq_length]\n\n                outputs, output_logits = [], []\n                for _ in xrange(seq_length):\n                    output, output_logit, state = self.cell(zeros, state)\n                    self.save_state(state, seq_length, is_output=True)\n                    outputs.append(output)\n                    output_logits.append(output_logit)\n\n                self.outputs[seq_length] = outputs\n                self.output_logits[seq_length] = output_logits\n        return self.outputs[seq_length]\n\n    def get_loss(self, seq_length):\n        if not self.outputs.has_key(seq_length):\n            self.get_outputs(seq_length)\n\n        if not self.losses.has_key(seq_length):\n            loss = sequence_loss(\n                logits=self.output_logits[seq_length],\n                targets=self.true_outputs[0:seq_length],\n                weights=[1] * seq_length,\n                average_across_timesteps=False,\n                average_across_batch=False,\n                softmax_loss_function=softmax_loss_function)\n\n            self.losses[seq_length] = loss\n        return self.losses[seq_length]\n\n    def get_output_states(self, seq_length):\n        zeros = np.zeros(self.cell.input_dim, dtype=np.float32)\n\n        if not self.output_states.has_key(seq_length):\n            with tf.variable_scope(self.scope):\n                tf.get_variable_scope().reuse_variables()\n\n                outputs, output_logits = [], []\n                state = self.prev_states[seq_length]\n\n                for _ in xrange(seq_length):\n                    output, output_logit, state = self.cell(zeros, state)\n                    self.save_state(state, seq_length, is_output=True)\n                    outputs.append(output)\n                    output_logits.append(output_logit)\n                self.outputs[seq_length] = outputs\n                self.output_logits[seq_length] = output_logits\n        return self.output_states[seq_length]\n\n    @property\n    def loss(self):\n        return self.losses[self.cell.depth]\n\n    @property\n    def optim(self):\n        return self.optims[self.cell.depth]\n\n    def save_state(self, state, from_, to=None, is_output=False):\n        if is_output:\n            state_to_add = self.output_states\n        else:\n            state_to_add = self.input_states\n\n        if to:\n            for idx in xrange(from_, to + 1):\n                state_to_add[idx].append(state)\n        else:\n            state_to_add[from_].append(state)\n\n    def save(self, checkpoint_dir, task_name, step):\n        task_dir = os.path.join(checkpoint_dir, ""%s_%s"" % (task_name, self.max_length))\n        file_name = ""%s_%s.model"" % (self.scope, task_name)\n\n        if not os.path.exists(task_dir):\n            os.makedirs(task_dir)\n\n        self.saver.save(\n            self.sess,\n            os.path.join(task_dir, file_name),\n            global_step=step.astype(int))\n\n    def load(self, checkpoint_dir, task_name, strict=True):\n        print("" [*] Reading checkpoints..."")\n\n        task_dir = ""%s_%s"" % (task_name, self._max_length)\n        checkpoint_dir = os.path.join(checkpoint_dir, task_dir)\n\n        ckpt = tf.train.get_checkpoint_state(checkpoint_dir)\n        if ckpt and ckpt.model_checkpoint_path:\n            ckpt_name = os.path.basename(ckpt.model_checkpoint_path)\n            self.saver.restore(self.sess, os.path.join(checkpoint_dir, ckpt_name))\n        else:\n            if strict:\n                raise Exception("" [!] Testing, but %s not found"" % checkpoint_dir)\n            else:\n                print(\' [!] Training, but previous training data %s not found\' % checkpoint_dir)\n'"
ntm_cell.py,42,"b'from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nfrom functools import reduce\n\nimport numpy as np\nimport tensorflow as tf\n\nfrom tensorflow.python.ops import array_ops\n\nfrom utils import *\nfrom ops import *\n\nclass NTMCell(object):\n    def __init__(self, input_dim, output_dim,\n                 mem_size=128, mem_dim=20, controller_dim=100,\n                 controller_layer_size=1, shift_range=1,\n                 write_head_size=1, read_head_size=1):\n        """"""Initialize the parameters for an NTM cell.\n        Args:\n            input_dim: int, The number of units in the LSTM cell\n            output_dim: int, The dimensionality of the inputs into the LSTM cell\n            mem_size: (optional) int, The size of memory [128]\n            mem_dim: (optional) int, The dimensionality for memory [20]\n            controller_dim: (optional) int, The dimensionality for controller [100]\n            controller_layer_size: (optional) int, The size of controller layer [1]\n        """"""\n        # initialize configs\n        self.input_dim = input_dim\n        self.output_dim = output_dim\n        self.mem_size = mem_size\n        self.mem_dim = mem_dim\n        self.controller_dim = controller_dim\n        self.controller_layer_size = controller_layer_size\n        self.shift_range = shift_range\n        self.write_head_size = write_head_size\n        self.read_head_size = read_head_size\n\n        self.depth = 0\n        self.states = []\n\n    def __call__(self, input_, state=None, scope=None):\n        """"""Run one step of NTM.\n\n        Args:\n            inputs: input Tensor, 2D, 1 x input_size.\n            state: state Dictionary which contains M, read_w, write_w, read,\n                output, hidden.\n            scope: VariableScope for the created subgraph; defaults to class name.\n\n        Returns:\n            A tuple containing:\n            - A 2D, batch x output_dim, Tensor representing the output of the LSTM\n                after reading ""input_"" when previous state was ""state"".\n                Here output_dim is:\n                     num_proj if num_proj was set,\n                     num_units otherwise.\n            - A 2D, batch x state_size, Tensor representing the new state of LSTM\n                after reading ""input_"" when previous state was ""state"".\n        """"""\n        if state == None:\n            _, state = self.initial_state()\n\n        M_prev = state[\'M\']\n        read_w_list_prev = state[\'read_w\']\n        write_w_list_prev = state[\'write_w\']\n        read_list_prev = state[\'read\']\n        output_list_prev = state[\'output\']\n        hidden_list_prev = state[\'hidden\']\n\n        # build a controller\n        output_list, hidden_list = self.build_controller(input_, read_list_prev,\n                                                         output_list_prev,\n                                                         hidden_list_prev)\n\n        # last output layer from LSTM controller\n        last_output = output_list[-1]\n\n        # build a memory\n        M, read_w_list, write_w_list, read_list = self.build_memory(M_prev,\n                                                                    read_w_list_prev,\n                                                                    write_w_list_prev,\n                                                                    last_output)\n\n        # get a new output\n        new_output, new_output_logit = self.new_output(last_output)\n\n        state = {\n            \'M\': M,\n            \'read_w\': read_w_list,\n            \'write_w\': write_w_list,\n            \'read\': read_list,\n            \'output\': output_list,\n            \'hidden\': hidden_list,\n        }\n\n        self.depth += 1\n        self.states.append(state)\n\n        return new_output, new_output_logit, state\n\n    def new_output(self, output):\n        """"""Logistic sigmoid output layers.""""""\n\n        with tf.variable_scope(\'output\'):\n            logit = Linear(output, self.output_dim, name=\'output\')\n            return tf.sigmoid(logit), logit\n\n    def build_controller(self, input_,\n                         read_list_prev, output_list_prev, hidden_list_prev):\n        """"""Build LSTM controller.""""""\n\n        with tf.variable_scope(""controller""):\n            output_list = []\n            hidden_list = []\n            for layer_idx in xrange(self.controller_layer_size):\n                o_prev = output_list_prev[layer_idx]\n                h_prev = hidden_list_prev[layer_idx]\n\n                if layer_idx == 0:\n                    def new_gate(gate_name):\n                        return linear([input_, o_prev] + read_list_prev,\n                                      output_size = self.controller_dim,\n                                      bias = True,\n                                      scope = ""%s_gate_%s"" % (gate_name, layer_idx))\n                else:\n                    def new_gate(gate_name):\n                        return linear([output_list[-1], o_prev],\n                                      output_size = self.controller_dim,\n                                      bias = True,\n                                      scope=""%s_gate_%s"" % (gate_name, layer_idx))\n\n                # input, forget, and output gates for LSTM\n                i = tf.sigmoid(new_gate(\'input\'))\n                f = tf.sigmoid(new_gate(\'forget\'))\n                o = tf.sigmoid(new_gate(\'output\'))\n                update = tf.tanh(new_gate(\'update\'))\n\n                # update the sate of the LSTM cell\n                hid = tf.add_n([f * h_prev, i * update])\n                out = o * tf.tanh(hid)\n\n                hidden_list.append(hid)\n                output_list.append(out)\n\n            return output_list, hidden_list\n\n    def build_memory(self, M_prev, read_w_list_prev, write_w_list_prev, last_output):\n        """"""Build a memory to read & write.""""""\n\n        with tf.variable_scope(""memory""):\n            # 3.1 Reading\n            if self.read_head_size == 1:\n                read_w_prev = read_w_list_prev[0]\n\n                read_w, read = self.build_read_head(M_prev, tf.squeeze(read_w_prev),\n                                                    last_output, 0)\n                read_w_list = [read_w]\n                read_list = [read]\n            else:\n                read_w_list = []\n                read_list = []\n\n                for idx in xrange(self.read_head_size):\n                    read_w_prev_idx = read_w_list_prev[idx]\n                    read_w_idx, read_idx = self.build_read_head(M_prev, read_w_prev_idx,\n                                                                last_output, idx)\n\n                    read_w_list.append(read_w_idx)\n                    read_list.append(read_idx)\n\n            # 3.2 Writing\n            if self.write_head_size == 1:\n                write_w_prev = write_w_list_prev[0]\n\n                write_w, write, erase = self.build_write_head(M_prev,\n                                                              tf.squeeze(write_w_prev),\n                                                              last_output, 0)\n\n                M_erase = tf.ones([self.mem_size, self.mem_dim]) \\\n                                  - outer_product(write_w, erase)\n                M_write = outer_product(write_w, write)\n\n                write_w_list = [write_w]\n            else:\n                write_w_list = []\n                write_list = []\n                erase_list = []\n\n                M_erases = []\n                M_writes = []\n\n                for idx in xrange(self.write_head_size):\n                    write_w_prev_idx = write_w_list_prev[idx]\n\n                    write_w_idx, write_idx, erase_idx = \\\n                        self.build_write_head(M_prev, write_w_prev_idx,\n                                              last_output, idx)\n\n                    write_w_list.append(tf.transpose(write_w_idx))\n                    write_list.append(write_idx)\n                    erase_list.append(erase_idx)\n\n                    M_erases.append(tf.ones([self.mem_size, self.mem_dim]) \\\n                                    - outer_product(write_w_idx, erase_idx))\n                    M_writes.append(outer_product(write_w_idx, write_idx))\n\n                M_erase = reduce(lambda x, y: x*y, M_erases)\n                M_write = tf.add_n(M_writes)\n\n            M = M_prev * M_erase + M_write\n\n            return M, read_w_list, write_w_list, read_list\n\n    def build_read_head(self, M_prev, read_w_prev, last_output, idx):\n        return self.build_head(M_prev, read_w_prev, last_output, True, idx)\n\n    def build_write_head(self, M_prev, write_w_prev, last_output, idx):\n        return self.build_head(M_prev, write_w_prev, last_output, False, idx)\n\n    def build_head(self, M_prev, w_prev, last_output, is_read, idx):\n        scope = ""read"" if is_read else ""write""\n\n        with tf.variable_scope(scope):\n            # Figure 2.\n            # Amplify or attenuate the precision\n            with tf.variable_scope(""k""):\n                k = tf.tanh(Linear(last_output, self.mem_dim, name=\'k_%s\' % idx))\n            # Interpolation gate\n            with tf.variable_scope(""g""):\n                g = tf.sigmoid(Linear(last_output, 1, name=\'g_%s\' % idx))\n            # shift weighting\n            with tf.variable_scope(""s_w""):\n                w = Linear(last_output, 2 * self.shift_range + 1, name=\'s_w_%s\' % idx)\n                s_w = softmax(w)\n            with tf.variable_scope(""beta""):\n                beta  = tf.nn.softplus(Linear(last_output, 1, name=\'beta_%s\' % idx))\n            with tf.variable_scope(""gamma""):\n                gamma = tf.add(tf.nn.softplus(Linear(last_output, 1, name=\'gamma_%s\' % idx)),\n                               tf.constant(1.0))\n\n            # 3.3.1\n            # Cosine similarity\n            similarity = smooth_cosine_similarity(M_prev, k) # [mem_size x 1]\n            # Focusing by content\n            content_focused_w = softmax(scalar_mul(similarity, beta))\n\n            # 3.3.2\n            # Focusing by location\n            gated_w = tf.add_n([\n                scalar_mul(content_focused_w, g),\n                scalar_mul(w_prev, (tf.constant(1.0) - g))\n            ])\n\n            # Convolutional shifts\n            conv_w = circular_convolution(gated_w, s_w)\n\n            # Sharpening\n            powed_conv_w = tf.pow(conv_w, gamma)\n            w = powed_conv_w / tf.reduce_sum(powed_conv_w)\n\n            if is_read:\n                # 3.1 Reading\n                read = matmul(tf.transpose(M_prev), w)\n                return w, read\n            else:\n                # 3.2 Writing\n                erase = tf.sigmoid(Linear(last_output, self.mem_dim, name=\'erase_%s\' % idx))\n                add = tf.tanh(Linear(last_output, self.mem_dim, name=\'add_%s\' % idx))\n                return w, add, erase\n\n    def initial_state(self, dummy_value=0.0):\n        self.depth = 0\n        self.states = []\n        with tf.variable_scope(""init_cell""):\n            # always zero\n            dummy = tf.Variable(tf.constant([[dummy_value]], dtype=tf.float32))\n\n            # memory\n            M_init_linear = tf.tanh(Linear(dummy, self.mem_size * self.mem_dim,\n                                    name=\'M_init_linear\'))\n            M_init = tf.reshape(M_init_linear, [self.mem_size, self.mem_dim])\n\n            # read weights\n            read_w_list_init = []\n            read_list_init = []\n            for idx in xrange(self.read_head_size):\n                read_w_idx = Linear(dummy, self.mem_size, is_range=True, \n                                    squeeze=True, name=\'read_w_%d\' % idx)\n                read_w_list_init.append(softmax(read_w_idx))\n\n                read_init_idx = Linear(dummy, self.mem_dim,\n                                       squeeze=True, name=\'read_init_%d\' % idx)\n                read_list_init.append(tf.tanh(read_init_idx))\n\n            # write weights\n            write_w_list_init = []\n            for idx in xrange(self.write_head_size):\n                write_w_idx = Linear(dummy, self.mem_size, is_range=True,\n                                     squeeze=True, name=\'write_w_%s\' % idx)\n                write_w_list_init.append(softmax(write_w_idx))\n\n            # controller state\n            output_init_list = []                     \n            hidden_init_list = []                     \n            for idx in xrange(self.controller_layer_size):\n                output_init_idx = Linear(dummy, self.controller_dim,\n                                         squeeze=True, name=\'output_init_%s\' % idx)\n                output_init_list.append(tf.tanh(output_init_idx))\n                hidden_init_idx = Linear(dummy, self.controller_dim,\n                                         squeeze=True, name=\'hidden_init_%s\' % idx)\n                hidden_init_list.append(tf.tanh(hidden_init_idx))\n\n            output = tf.tanh(Linear(dummy, self.output_dim, name=\'new_output\'))\n\n            state = {\n                \'M\': M_init,\n                \'read_w\': read_w_list_init,\n                \'write_w\': write_w_list_init,\n                \'read\': read_list_init,\n                \'output\': output_init_list,\n                \'hidden\': hidden_init_list\n            }\n\n            self.depth += 1\n            self.states.append(state)\n\n            return output, state\n\n    def get_memory(self, depth=None):\n        depth = depth if depth else self.depth\n        return self.states[depth - 1][\'M\']\n\n    def get_read_weights(self, depth=None):\n        depth = depth if depth else self.depth\n        return self.states[depth - 1][\'read_w\']\n\n    def get_write_weights(self, depth=None):\n        depth = depth if depth else self.depth\n        return self.states[depth - 1][\'write_w\']\n\n    def get_read_vector(self, depth=None):\n        depth = depth if depth else self.depth\n        return self.states[depth - 1][\'read\']\n\n    def print_read_max(self, sess):\n        read_w_list = sess.run(self.get_read_weights())\n\n        fmt = ""%-4d %.4f""\n        if self.read_head_size == 1:\n            print(fmt % (argmax(read_w_list[0])))\n        else:\n            for idx in xrange(self.read_head_size):\n                print(fmt % np.argmax(read_w_list[idx]))\n\n    def print_write_max(self, sess):\n        write_w_list = sess.run(self.get_write_weights())\n\n        fmt = ""%-4d %.4f""\n        if self.write_head_size == 1:\n            print(fmt % (argmax(write_w_list[0])))\n        else:\n            for idx in xrange(self.write_head_size):\n                print(fmt % argmax(write_w_list[idx]))\n'"
ops.py,33,"b'import math\nimport numpy as np \nimport tensorflow as tf\n\nfrom tensorflow.python.ops import array_ops\nfrom tensorflow.python.ops import init_ops\nfrom tensorflow.python.framework import ops\nfrom tensorflow.python.ops import variable_scope as vs\n\nfrom utils import *\n\ndef linear(args, output_size, bias, bias_start=0.0, scope=None):\n    """"""Linear map: sum_i(args[i] * W[i]), where W[i] is a variable.\n\n    Args:\n        args: a 2D Tensor or a list of 2D, batch x n, Tensors.\n        output_size: int, second dimension of W[i].\n        bias: boolean, whether to add a bias term or not.\n        bias_start: starting value to initialize the bias; 0 by default.\n        scope: VariableScope for the created subgraph; defaults to ""Linear"".\n\n    Returns:\n        A 2D Tensor with shape [batch x output_size] equal to\n        sum_i(args[i] * W[i]), where W[i]s are newly created matrices.\n\n    Raises:\n        ValueError: if some of the arguments has unspecified or wrong shape.\n    """"""\n    if not isinstance(args, (list, tuple)):\n        args = [args]\n\n    # Calculate the total size of arguments on dimension 1.\n    total_arg_size = 0\n    shapes = []\n    for a in args:\n        try:\n            shapes.append(a.get_shape().as_list())\n        except Exception as e:\n            shapes.append(a.shape)\n\n    is_vector = False\n    for idx, shape in enumerate(shapes):\n        if len(shape) != 2:\n            is_vector = True\n            args[idx] = tf.reshape(args[idx], [1, -1])\n            total_arg_size += shape[0]\n        else:\n            total_arg_size += shape[1]\n\n    # Now the computation.\n    with vs.variable_scope(scope or ""Linear""):\n        matrix = vs.get_variable(""Matrix"", [total_arg_size, output_size])\n        if len(args) == 1:\n            res = tf.matmul(args[0], matrix)\n        else:\n            res = tf.matmul(tf.concat(args, 1), matrix)\n        if not bias:\n            return res\n        bias_term = vs.get_variable(\n            ""Bias"", [output_size],\n            initializer=init_ops.constant_initializer(bias_start))\n\n    if is_vector:\n        return tf.reshape(res + bias_term, [-1])\n    else:\n        return res + bias_term\n\ndef Linear(input_, output_size, stddev=0.5,\n           is_range=False, squeeze=False,\n           name=None, reuse=None):\n    """"""Applies a linear transformation to the incoming data.\n\n    Args:\n        input: a 2-D or 1-D data (`Tensor` or `ndarray`)\n        output_size: the size of output matrix or vector\n    """"""\n    with tf.variable_scope(""Linear"", reuse=reuse):\n        if type(input_) == np.ndarray:\n            shape = input_.shape\n        else:\n            shape = input_.get_shape().as_list()\n\n        is_vector = False\n        if len(shape) == 1:\n            is_vector = True\n            input_ = tf.reshape(input_, [1, -1])\n            input_size = shape[0]\n        elif len(shape) == 2:\n            input_size = shape[1]\n        else:\n            raise ValueError(""Linear expects shape[1] of inputuments: %s"" % str(shape))\n\n        w_name = ""%s_w"" % name if name else None\n        b_name = ""%s_b"" % name if name else None\n\n        w = tf.get_variable(w_name, [input_size, output_size], tf.float32,\n                            tf.random_normal_initializer(stddev=stddev))\n        mul = tf.matmul(input_, w)\n\n        if is_range:\n            def identity_initializer(tensor):\n                def _initializer(shape, dtype=tf.float32, partition_info=None):\n                    return tf.identity(tensor)\n                return _initializer\n\n            range_ = tf.range(output_size, 0, -1)\n            b = tf.get_variable(b_name, [output_size], tf.float32,\n                                identity_initializer(tf.cast(range_, tf.float32)))\n        else:\n            b = tf.get_variable(b_name, [output_size], tf.float32, \n                                tf.random_normal_initializer(stddev=stddev))\n\n        if squeeze:\n            output = tf.squeeze(tf.nn.bias_add(mul, b))\n        else:\n            output = tf.nn.bias_add(mul, b)\n\n        if is_vector:\n            return tf.reshape(output, [-1])\n        else:\n            return output\n\ndef smooth_cosine_similarity(m, v):\n    """"""Computes smooth cosine similarity.\n\n    Args:\n        m: a 2-D `Tensor` (matrix)\n        v: a 1-D `Tensor` (vector)\n    """"""\n    shape_x = m.get_shape().as_list()\n    shape_y = v.get_shape().as_list()\n    if shape_x[1] != shape_y[0]:\n        raise ValueError(""Smooth cosine similarity is expecting same dimemsnion"")\n\n    m_norm = tf.sqrt(tf.reduce_sum(tf.pow(m, 2),1))\n    v_norm = tf.sqrt(tf.reduce_sum(tf.pow(v, 2)))\n    m_dot_v = tf.matmul(m, tf.reshape(v, [-1, 1]))\n\n    similarity = tf.div(tf.reshape(m_dot_v, [-1]), m_norm * v_norm + 1e-3)\n    return similarity\n\ndef circular_convolution(v, k):\n    """"""Computes circular convolution.\n\n    Args:\n        v: a 1-D `Tensor` (vector)\n        k: a 1-D `Tensor` (kernel)\n    """"""\n    size = int(v.get_shape()[0])\n    kernel_size = int(k.get_shape()[0])\n    kernel_shift = int(math.floor(kernel_size/2.0))\n\n    def loop(idx):\n        if idx < 0: return size + idx\n        if idx >= size : return idx - size\n        else: return idx\n\n    kernels = []\n    for i in xrange(size):\n        indices = [loop(i+j) for j in xrange(kernel_shift, -kernel_shift-1, -1)]\n        v_ = tf.gather(v, indices)\n        kernels.append(tf.reduce_sum(v_ * k, 0))\n\n    # # code with double loop\n    # for i in xrange(size):\n    #     for j in xrange(kernel_size):\n    #         idx = i + kernel_shift - j + 1\n    #         if idx < 0: idx = idx + size\n    #         if idx >= size: idx = idx - size\n    #         w = tf.gather(v, int(idx)) * tf.gather(kernel, j)\n    #         output = tf.scatter_add(output, [i], tf.reshape(w, [1, -1]))\n\n    return tf.dynamic_stitch([i for i in xrange(size)], kernels)\n\ndef outer_product(*inputs):\n    """"""Computes outer product.\n\n    Args:\n        inputs: a list of 1-D `Tensor` (vector)\n    """"""\n    inputs = list(inputs)\n    order = len(inputs)\n\n    for idx, input_ in enumerate(inputs):\n        if len(input_.get_shape()) == 1:\n            inputs[idx] = tf.reshape(input_, [-1, 1] if idx % 2 == 0 else [1, -1])\n\n    if order == 2:\n        output = tf.multiply(inputs[0], inputs[1])\n    elif order == 3:\n        size = []\n        idx = 1\n        for i in xrange(order):\n            size.append(inputs[i].get_shape()[0])\n        output = tf.zeros(size)\n\n        u, v, w = inputs[0], inputs[1], inputs[2]\n        uv = tf.multiply(inputs[0], inputs[1])\n        for i in xrange(self.size[-1]):\n            output = tf.scatter_add(output, [0,0,i], uv)\n\n    return output\n\ndef scalar_mul(x, beta, name=None):\n    return x * beta\n\ndef scalar_div(x, beta, name=None):\n    return x / beta\n'"
ops_test.py,2,"b'""""""Tests for ops.""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport numpy as np\n\nfrom tensorflow.python.ops import constant_op\nfrom tensorflow.python.framework import test_util\nfrom tensorflow.python.platform import googletest\n\nfrom ops import *\n\nclass SmoothCosineSimilarityTest(test_util.TensorFlowTestCase):\n\n    def testSmoothCosineSimilarity(self):\n        """"""Test code for torch:\n\n            th> x=torch.Tensor{{1,2,3},{2,2,2},{3,2,1},{0,2,4}}\n            th> y=torch.Tensor{2,2,2}\n            th> c=nn.SmoothCosineSimilarity()\n            th> c:forward{x,y}\n             0.9257\n             0.9999\n             0.9257\n             0.7745\n            [torch.DoubleTensor of size 4]\n        """"""\n        m = constant_op.constant(\n            [[1,2,3],\n             [2,2,2],\n             [3,2,1],\n             [0,2,4]], dtype=np.float32)\n        v = constant_op.constant([2,2,2], dtype=np.float32)\n        for use_gpu in [True, False]:\n            with self.test_session(use_gpu=use_gpu):\n                loss = smooth_cosine_similarity(m, v).eval()\n                self.assertAllClose(loss, [0.92574867671153,\n                                           0.99991667361053,\n                                           0.92574867671153,\n                                           0.77454667246876])\n\nclass CircularConvolutionTest(test_util.TensorFlowTestCase):\n\n    def testCircularConvolution(self):\n        v = constant_op.constant([1,2,3,4,5,6,7], dtype=tf.float32)\n        k = constant_op.constant([0,0,1], dtype=tf.float32)\n        for use_gpu in [True, False]:\n            with self.test_session(use_gpu=use_gpu):\n                loss = circular_convolution(v, k).eval()\n                self.assertAllEqual(loss, [7,1,2,3,4,5,6])\n\nif __name__ == ""__main__"":\n    googletest.main()\n'"
utils.py,5,"b'import sys\nimport pprint\nimport numpy as np\nimport tensorflow as tf\n\neps = 1e-12\npp = pprint.PrettyPrinter()\n\ntry:\n    xrange\nexcept NameError:\n    xrange = range\n\ndef progress(progress):\n    barLength = 20 # Modify this to change the length of the progress bar\n    status = """"\n    if isinstance(progress, int):\n        progress = float(progress)\n    if not isinstance(progress, float):\n        progress = 0\n        status = ""error: progress var must be float\\r\\n""\n    if progress < 0:\n        progress = 0\n        status = ""Halt...\\r\\n""\n    if progress >= 1:\n        progress = 1\n        status = ""Finished.\\r\\n""\n    block = int(round(barLength*progress))\n    text = ""\\rPercent: [%s] %.2f%% %s"" % (""#""*block + "" ""*(barLength-block), progress*100, status)\n    sys.stdout.write(text)\n    sys.stdout.flush()\n\ndef pprint(seq):\n    seq = np.array(seq)\n    seq = np.char.mod(\'%d\', np.around(seq))\n    seq[seq == \'1\'] = \'#\'\n    seq[seq == \'0\'] = \' \'\n    print(""\\n"".join(["""".join(x) for x in seq.tolist()]))\n\ndef gather(m_or_v, idx):\n    if len(m_or_v.get_shape()) > 1:\n        return tf.gather(m_or_v, idx)\n    else:\n        assert idx == 0, ""Error: idx should be 0 but %d"" % idx\n        return m_or_v\n\ndef argmax(x):\n    index = 0\n    max_num = x[index]\n    for idx in xrange(1, len(x)-1):\n        if x[idx] > max_num:\n            index = idx\n            max_num = x[idx]\n    return index, max_num\n\ndef softmax(x):\n    """"""Compute softmax.\n\n    Args:\n        x: a 2-D `Tensor` (matrix) or 1-D `Tensor` (vector)\n    """"""\n    try:\n        return tf.nn.softmax(x + eps)\n    except:\n        return tf.reshape(tf.nn.softmax(tf.reshape(x + eps, [1, -1])), [-1])\n\ndef matmul(x, y):\n    """"""Compute matrix multiplication.\n\n    Args:\n        x: a 2-D `Tensor` (matrix)\n        y: a 2-D `Tensor` (matrix) or 1-D `Tensor` (vector)\n    """"""\n    try:\n        return tf.matmul(x, y)\n    except:\n        return tf.reshape(tf.matmul(x, tf.reshape(y, [-1, 1])), [-1])\n'"
tasks/__init__.py,0,b''
tasks/copy.py,1,"b'import os\nimport time\nimport numpy as np\nimport tensorflow as tf\nfrom random import randint\n\nfrom utils import pprint\n\nprint_interval = 5\n\n\ndef run(ntm, seq_length, sess, print_=True):\n    start_symbol = np.zeros([ntm.cell.input_dim], dtype=np.float32)\n    start_symbol[0] = 1\n    end_symbol = np.zeros([ntm.cell.input_dim], dtype=np.float32)\n    end_symbol[1] = 1\n\n    seq = generate_copy_sequence(seq_length, ntm.cell.input_dim - 2)\n\n    feed_dict = {input_: vec for vec, input_ in zip(seq, ntm.inputs)}\n    feed_dict.update(\n        {true_output: vec for vec, true_output in zip(seq, ntm.true_outputs)}\n    )\n    feed_dict.update({\n        ntm.start_symbol: start_symbol,\n        ntm.end_symbol: end_symbol\n    })\n\n    input_states = [state[\'write_w\'][0] for state in ntm.input_states[seq_length]]\n    output_states = [state[\'read_w\'][0] for state in ntm.get_output_states(seq_length)]\n\n    result = sess.run(\n        ntm.get_outputs(seq_length) +\n        input_states + output_states +\n        [ntm.get_loss(seq_length)],\n        feed_dict=feed_dict)\n\n    is_sz = len(input_states)\n    os_sz = len(output_states)\n\n    outputs = result[:seq_length]\n    read_ws = result[seq_length:seq_length + is_sz]\n    write_ws = result[seq_length + is_sz:seq_length + is_sz + os_sz]\n    loss = result[-1]\n\n    if print_:\n        np.set_printoptions(suppress=True)\n        print("" true output : "")\n        pprint(seq)\n        print("" predicted output :"")\n        pprint(np.round(outputs))\n        print("" Loss : %f"" % loss)\n        np.set_printoptions(suppress=False)\n    else:\n        return seq, outputs, read_ws, write_ws, loss\n\n\ndef train(ntm, config, sess):\n    if not os.path.isdir(config.checkpoint_dir):\n        raise Exception("" [!] Directory %s not found"" % config.checkpoint_dir)\n\n    # delimiter flag for start and end\n    start_symbol = np.zeros([config.input_dim], dtype=np.float32)\n    start_symbol[0] = 1\n    end_symbol = np.zeros([config.input_dim], dtype=np.float32)\n    end_symbol[1] = 1\n\n    print("" [*] Initialize all variables"")\n    tf.global_variables_initializer().run()\n    print("" [*] Initialization finished"")\n\n    if config.continue_train is not False:\n        ntm.load(config.checkpoint_dir, config.task, strict=config.continue_train is True)\n\n    start_time = time.time()\n    for idx in xrange(config.epoch):\n        seq_length = randint(config.min_length, config.max_length)\n        seq = generate_copy_sequence(seq_length, config.input_dim - 2)\n\n        feed_dict = {input_: vec for vec, input_ in zip(seq, ntm.inputs)}\n        feed_dict.update(\n            {true_output: vec for vec, true_output in zip(seq, ntm.true_outputs)}\n        )\n        feed_dict.update({\n            ntm.start_symbol: start_symbol,\n            ntm.end_symbol: end_symbol\n        })\n\n        _, cost, step = sess.run([ntm.optims[seq_length],\n                                  ntm.get_loss(seq_length),\n                                  ntm.global_step], feed_dict=feed_dict)\n\n        if idx % 100 == 0:\n            ntm.save(config.checkpoint_dir, config.task, step)\n\n        if idx % print_interval == 0:\n            print(\n                ""[%5d] %2d: %.2f (%.1fs)""\n                % (idx, seq_length, cost, time.time() - start_time))\n\n    ntm.save(config.checkpoint_dir, config.task, step)\n\n    print(""Training %s task finished"" % config.task)\n\n\ndef generate_copy_sequence(length, bits):\n    seq = np.zeros([length, bits + 2], dtype=np.float32)\n    for idx in xrange(length):\n        seq[idx, 2:bits+2] = np.random.rand(bits).round()\n    return list(seq)\n'"
tasks/recall.py,1,"b'import os\nimport time\nimport numpy as np\nimport tensorflow as tf\nfrom random import randint\n\nfrom ntm import NTM\nfrom utils import pprint\nfrom ntm_cell import NTMCell\n\nprint_interval = 5\n\n\ndef run(ntm, seq_length, sess, print_=True):\n    start_symbol = np.zeros([ntm.cell.input_dim], dtype=np.float32)\n    start_symbol[0] = 1\n    end_symbol = np.zeros([ntm.cell.input_dim], dtype=np.float32)\n    end_symbol[1] = 1\n\n    seq = generate_recall_sequence(seq_length, ntm.cell.input_dim - 2)\n\n    feed_dict = {input_:vec for vec, input_ in zip(seq, ntm.inputs)}\n    feed_dict.update(\n        {true_output:vec for vec, true_output in zip(seq, ntm.true_outputs)}\n    )\n    feed_dict.update({\n        ntm.start_symbol: start_symbol,\n        ntm.end_symbol: end_symbol\n    })\n\n    input_states = [state[\'write_w\'] for state in ntm.input_states[seq_length]]\n    output_states = [state[\'read_w\'] for state in ntm.get_output_states(seq_length)]\n\n    result = sess.run(ntm.get_outputs(seq_length) + \\\n                      input_states + output_states + \\\n                      [ntm.get_loss(seq_length)],\n                      feed_dict=feed_dict)\n\n    is_sz = len(input_states)\n    os_sz = len(output_states)\n\n    outputs = result[:seq_length]\n    read_ws = result[seq_length:seq_length + is_sz]\n    write_ws = result[seq_length + is_sz:seq_length + is_sz + os_sz]\n    loss = result[-1]\n\n    if print_:\n        np.set_printoptions(suppress=True)\n        print("" true output : "")\n        pprint(seq)\n        print("" predicted output :"")\n        pprint(np.round(outputs))\n        print("" Loss : %f"" % loss)\n        np.set_printoptions(suppress=False)\n    else:\n        return seq, outputs, read_ws, write_ws, loss\n\n\ndef train(ntm, config, sess):\n    if not os.path.isdir(config.checkpoint_dir):\n        raise Exception("" [!] Directory %s not found"" % config.checkpoint_dir)\n\n    delim_symbol = np.zeros([config.input_dim], dtype=np.float32)\n    start_symbol[0] = 1\n    query_symbol = np.zeros([config.input_dim], dtype=np.float32)\n    end_symbol[1] = 1\n\n    print("" [*] Initialize all variables"")\n    tf.initialize_all_variables().run()\n    print("" [*] Initialization finished"")\n\n    start_time = time.time()\n    for idx in xrange(config.epoch):\n        seq_length = randint(config.min_length, config.max_length)\n        seq = generate_recall_sequence(seq_length, config.input_dim - 2)\n\n        feed_dict = {input_:vec for vec, input_ in zip(seq, ntm.inputs)}\n        feed_dict.update(\n            {true_output:vec for vec, true_output in zip(seq, ntm.true_outputs)}\n        )\n        feed_dict.update({\n            ntm.start_symbol: start_symbol,\n            ntm.end_symbol: end_symbol\n        })\n\n        _, cost, step = sess.run([ntm.optims[seq_length],\n                                  ntm.get_loss(seq_length),\n                                  ntm.global_step], feed_dict=feed_dict)\n\n        if idx % 100 == 0:\n            ntm.save(config.checkpoint_dir, \'recall\', step)\n\n        if idx % print_interval == 0:\n            print(""[%5d] %2d: %.2f (%.1fs)"" \\\n                % (idx, seq_length, cost, time.time() - start_time))\n\n    print(""Training Copy task finished"")\n\n\ndef generate_recall_sequence(num_items, item_length, input_dim):\n    items = []\n    for idx in xrange(num_items):\n        item = np.random.rand(item_length, input_dim).round()\n        item[0:item_length+1, 0:2] = 0\n        items.append(item)\n    return items\n'"
