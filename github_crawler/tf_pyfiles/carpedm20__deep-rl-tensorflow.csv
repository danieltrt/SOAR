file_path,api_count,code
main.py,8,"b'import gym\nimport random\nimport logging\nimport tensorflow as tf\n\nfrom utils import get_model_dir\nfrom networks.cnn import CNN\nfrom networks.mlp import MLPSmall\nfrom agents.statistic import Statistic\nfrom environments.environment import ToyEnvironment, AtariEnvironment\n\nflags = tf.app.flags\n\n# Deep q Network\nflags.DEFINE_boolean(\'use_gpu\', True, \'Whether to use gpu or not. gpu use NHWC and gpu use NCHW for data_format\')\nflags.DEFINE_string(\'agent_type\', \'DQN\', \'The type of agent [DQN]\')\nflags.DEFINE_boolean(\'double_q\', False, \'Whether to use double Q-learning\')\nflags.DEFINE_string(\'network_header_type\', \'nips\', \'The type of network header [mlp, nature, nips]\')\nflags.DEFINE_string(\'network_output_type\', \'normal\', \'The type of network output [normal, dueling]\')\n\n# Environment\nflags.DEFINE_string(\'env_name\', \'Breakout-v0\', \'The name of gym environment to use\')\nflags.DEFINE_integer(\'n_action_repeat\', 1, \'The number of actions to repeat\')\nflags.DEFINE_integer(\'max_random_start\', 30, \'The maximum number of NOOP actions at the beginning of an episode\')\nflags.DEFINE_integer(\'history_length\', 4, \'The length of history of observation to use as an input to DQN\')\nflags.DEFINE_integer(\'max_r\', +1, \'The maximum value of clipped reward\')\nflags.DEFINE_integer(\'min_r\', -1, \'The minimum value of clipped reward\')\nflags.DEFINE_string(\'observation_dims\', \'[80, 80]\', \'The dimension of gym observation\')\nflags.DEFINE_boolean(\'random_start\', True, \'Whether to start with random state\')\nflags.DEFINE_boolean(\'use_cumulated_reward\', False, \'Whether to use cumulated reward or not\')\n\n# Training\nflags.DEFINE_boolean(\'is_train\', True, \'Whether to do training or testing\')\nflags.DEFINE_integer(\'max_delta\', None, \'The maximum value of delta\')\nflags.DEFINE_integer(\'min_delta\', None, \'The minimum value of delta\')\nflags.DEFINE_float(\'ep_start\', 1., \'The value of epsilon at start in e-greedy\')\nflags.DEFINE_float(\'ep_end\', 0.01, \'The value of epsilnon at the end in e-greedy\')\nflags.DEFINE_integer(\'batch_size\', 32, \'The size of batch for minibatch training\')\nflags.DEFINE_integer(\'max_grad_norm\', None, \'The maximum norm of gradient while updating\')\nflags.DEFINE_float(\'discount_r\', 0.99, \'The discount factor for reward\')\n\n# Timer\nflags.DEFINE_integer(\'t_train_freq\', 4, \'\')\n\n# Below numbers will be multiplied by scale\nflags.DEFINE_integer(\'scale\', 10000, \'The scale for big numbers\')\nflags.DEFINE_integer(\'memory_size\', 100, \'The size of experience memory (*= scale)\')\nflags.DEFINE_integer(\'t_target_q_update_freq\', 1, \'The frequency of target network to be updated (*= scale)\')\nflags.DEFINE_integer(\'t_test\', 1, \'The maximum number of t while training (*= scale)\')\nflags.DEFINE_integer(\'t_ep_end\', 100, \'The time when epsilon reach ep_end (*= scale)\')\nflags.DEFINE_integer(\'t_train_max\', 5000, \'The maximum number of t while training (*= scale)\')\nflags.DEFINE_float(\'t_learn_start\', 5, \'The time when to begin training (*= scale)\')\nflags.DEFINE_float(\'learning_rate_decay_step\', 5, \'The learning rate of training (*= scale)\')\n\n# Optimizer\nflags.DEFINE_float(\'learning_rate\', 0.00025, \'The learning rate of training\')\nflags.DEFINE_float(\'learning_rate_minimum\', 0.00025, \'The minimum learning rate of training\')\nflags.DEFINE_float(\'learning_rate_decay\', 0.96, \'The decay of learning rate of training\')\nflags.DEFINE_float(\'decay\', 0.99, \'Decay of RMSProp optimizer\')\nflags.DEFINE_float(\'momentum\', 0.0, \'Momentum of RMSProp optimizer\')\nflags.DEFINE_float(\'gamma\', 0.99, \'Discount factor of return\')\nflags.DEFINE_float(\'beta\', 0.01, \'Beta of RMSProp optimizer\')\n\n# Debug\nflags.DEFINE_boolean(\'display\', False, \'Whether to do display the game screen or not\')\nflags.DEFINE_string(\'log_level\', \'INFO\', \'Log level [DEBUG, INFO, WARNING, ERROR, CRITICAL]\')\nflags.DEFINE_integer(\'random_seed\', 123, \'Value of random seed\')\nflags.DEFINE_string(\'tag\', \'\', \'The name of tag for a model, only for debugging\')\nflags.DEFINE_boolean(\'allow_soft_placement\', True, \'Whether to use part or all of a GPU\')\n#flags.DEFINE_string(\'gpu_fraction\', \'1/1\', \'idx / # of gpu fraction e.g. 1/3, 2/3, 3/3\')\n\n# Internal\n# It is forbidden to set a flag that is not defined\nflags.DEFINE_string(\'data_format\', \'NCHW\', \'INTERNAL USED ONLY\')\n\ndef calc_gpu_fraction(fraction_string):\n  idx, num = fraction_string.split(\'/\')\n  idx, num = float(idx), float(num)\n\n  fraction = 1 / (num - idx + 1)\n  print ("" [*] GPU : %.4f"" % fraction)\n  return fraction\n\nconf = flags.FLAGS\n\nif conf.agent_type == \'DQN\':\n  from agents.deep_q import DeepQ\n  TrainAgent = DeepQ\nelse:\n  raise ValueError(\'Unknown agent_type: %s\' % conf.agent_type)\n\nlogger = logging.getLogger()\nlogger.propagate = False\nlogger.setLevel(conf.log_level)\n\n# set random seed\ntf.set_random_seed(conf.random_seed)\nrandom.seed(conf.random_seed)\n\ndef main(_):\n  # preprocess\n  conf.observation_dims = eval(conf.observation_dims)\n\n  for flag in [\'memory_size\', \'t_target_q_update_freq\', \'t_test\',\n               \'t_ep_end\', \'t_train_max\', \'t_learn_start\', \'learning_rate_decay_step\']:\n    setattr(conf, flag, getattr(conf, flag) * conf.scale)\n\n  if conf.use_gpu:\n    conf.data_format = \'NCHW\'\n  else:\n    conf.data_format = \'NHWC\'\n\n  model_dir = get_model_dir(conf,\n      [\'use_gpu\', \'max_random_start\', \'n_worker\', \'is_train\', \'memory_size\', \'gpu_fraction\',\n       \'t_save\', \'t_train\', \'display\', \'log_level\', \'random_seed\', \'tag\', \'scale\'])\n\n  # start\n  #gpu_options = tf.GPUOptions(\n  #    per_process_gpu_memory_fraction=calc_gpu_fraction(conf.gpu_fraction))\n\n  sess_config = tf.ConfigProto(\n      log_device_placement=False, allow_soft_placement=conf.allow_soft_placement)\n  sess_config.gpu_options.allow_growth = conf.allow_soft_placement\n\n  with tf.Session(config=sess_config) as sess:\n    if any(name in conf.env_name for name in [\'Corridor\', \'FrozenLake\']) :\n      env = ToyEnvironment(conf.env_name, conf.n_action_repeat,\n                           conf.max_random_start, conf.observation_dims,\n                           conf.data_format, conf.display, conf.use_cumulated_reward)\n    else:\n      env = AtariEnvironment(conf.env_name, conf.n_action_repeat,\n                             conf.max_random_start, conf.observation_dims,\n                             conf.data_format, conf.display, conf.use_cumulated_reward)\n\n    if conf.network_header_type in [\'nature\', \'nips\']:\n      pred_network = CNN(sess=sess,\n                         data_format=conf.data_format,\n                         history_length=conf.history_length,\n                         observation_dims=conf.observation_dims,\n                         output_size=env.env.action_space.n,\n                         network_header_type=conf.network_header_type,\n                         name=\'pred_network\', trainable=True)\n      target_network = CNN(sess=sess,\n                           data_format=conf.data_format,\n                           history_length=conf.history_length,\n                           observation_dims=conf.observation_dims,\n                           output_size=env.env.action_space.n,\n                           network_header_type=conf.network_header_type,\n                           name=\'target_network\', trainable=False)\n    elif conf.network_header_type == \'mlp\':\n      pred_network = MLPSmall(sess=sess,\n                              data_format=conf.data_format,\n                              observation_dims=conf.observation_dims,\n                              history_length=conf.history_length,\n                              output_size=env.env.action_space.n,\n                              hidden_activation_fn=tf.sigmoid,\n                              network_output_type=conf.network_output_type,\n                              name=\'pred_network\', trainable=True)\n      target_network = MLPSmall(sess=sess,\n                                data_format=conf.data_format,\n                                observation_dims=conf.observation_dims,\n                                history_length=conf.history_length,\n                                output_size=env.env.action_space.n,\n                                hidden_activation_fn=tf.sigmoid,\n                                network_output_type=conf.network_output_type,\n                                name=\'target_network\', trainable=False)\n    else:\n      raise ValueError(\'Unkown network_header_type: %s\' % (conf.network_header_type))\n\n    stat = Statistic(sess, conf.t_test, conf.t_learn_start, model_dir, pred_network.var.values())\n    agent = TrainAgent(sess, pred_network, env, stat, conf, target_network=target_network)\n\n    if conf.is_train:\n      agent.train(conf.t_train_max)\n    else:\n      agent.play(conf.ep_end)\n\nif __name__ == \'__main__\':\n  tf.app.run()\n'"
utils.py,0,"b'import os\nimport time\nimport tensorflow as tf\nfrom six.moves import range\nfrom logging import getLogger\n\nlogger = getLogger(__name__)\n\ndef get_model_dir(config, exceptions=None):\n  keys = dir(config)\n  keys.sort()\n  keys.remove(\'env_name\')\n  keys = [\'env_name\'] + keys\n\n  names = [config.env_name]\n  for key in keys:\n    # Only use useful flags\n    if key not in exceptions:\n      value = getattr(config, key)\n      names.append(\n        ""%s=%s"" % (key, "","".join([str(i) for i in value])\n                   if type(value) == list else value))\n\n  return os.path.join(\'checkpoints\', *names) + \'/\'\n\ndef timeit(f):\n  def timed(*args, **kwargs):\n    start_time = time.time()\n    result = f(*args, **kwargs)\n    end_time = time.time()\n\n    logger.info(""%s : %2.2f sec"" % (f.__name__, end_time - start_time))\n    return result\n  return timed\n'"
agents/__init__.py,0,b''
agents/_async.py,11,"b'import os\nimport time\nimport numpy as np\nimport tensorflow as tf\nfrom threading import Thread\nfrom logging import getLogger\n\nfrom .agent import Agent\nfrom .history import History\nfrom .experience import Experience\n\nlogger = getLogger(__name__)\n\nclass Async(Agent):\n  def __init__(self, sess, pred_network, env, stat, conf, target_network=None):\n    super(DeepQ, self).__init__(sess, pred_network, target_network, env, stat, conf)\n\n    raise Exception(""[!] Not fully implemented yet"")\n\n    # Optimizer\n    with tf.variable_scope(\'optimizer\'):\n      self.targets = tf.placeholder(\'float32\', [None], name=\'target_q_t\')\n      self.actions = tf.placeholder(\'int64\', [None], name=\'action\')\n\n      actions_one_hot = tf.one_hot(self.actions, self.env.action_size, 1.0, 0.0, name=\'action_one_hot\')\n      pred_q = tf.reduce_sum(self.pred_network.outputs * actions_one_hot, reduction_indices=1, name=\'q_acted\')\n\n      self.delta = self.targets - pred_q\n      if self.max_delta and self.min_delta:\n        self.delta = tf.clip_by_value(self.delta, self.min_delta, self.max_delta, name=\'clipped_delta\')\n\n      self.loss = tf.reduce_mean(tf.square(self.delta), name=\'loss\')\n\n      self.learning_rate_op = tf.maximum(self.learning_rate_minimum,\n          tf.train.exponential_decay(\n              self.learning_rate,\n              self.stat.t_op,\n              self.learning_rate_decay_step,\n              self.learning_rate_decay,\n              staircase=True))\n\n      optimizer = tf.train.RMSPropOptimizer(\n        self.learning_rate_op, momentum=0.95, epsilon=0.01)\n      \n      grads_and_vars = optimizer.compute_gradients(self.loss)\n      for idx, (grad, var) in enumerate(grads_and_vars):\n        if grad is not None:\n          grads_and_vars[idx] = (tf.clip_by_norm(grad, self.max_grad_norm), var)\n      self.optim = optimizer.apply_gradients(grads_and_vars)\n\n\n  def train(self, max_t, global_t):\n    self.global_t = global_t\n\n    # 0. Prepare training\n    state, reward, terminal = self.env.new_random_game()\n    self.observe(state, reward, terminal)\n\n    while True:\n      if global_t[0] > self.t_train_max:\n        break\n\n      # 1. Predict\n      action = self.predict(state)\n      # 2. Step\n      state, reward, terminal = self.env.step(action, is_training=True)\n      # 3. Observe\n      self.observe(state, reward, terminal)\n\n      if terminal:\n        observation, reward, terminal = self.new_game()\n\n      global_t[0] += 1\n\n  def train_with_log(self, max_t, global_t):\n    from tqdm import tqdm\n\n    for _ in tqdm(range(max_t), ncols=70, initial=int(global_t[0])):\n      if global_t[0] > self.t_train_max:\n        break\n\n      # 1. Predict\n      action = self.predict(state)\n      # 2. Step\n      state, reward, terminal = self.env.step(-1, is_training=True)\n      # 3. Observe\n      self.observe(state, reward, terminal)\n\n      if terminal:\n        observation, reward, terminal = self.new_game()\n\n      global_t[0] += 1\n\n    if self.stat:\n      self.stat.on_step(self.t, action, reward, terminal,\n                        ep, q, loss, is_update, self.learning_rate_op)\n\n  def observe(self, s_t, r_t, terminal):\n    self.prev_r[self.t] = max(self.min_reward, min(self.max_reward, r_t))\n\n    if (terminal and self.t_start < self.t) or self.t - self.t_start == self.t_max:\n      r = {}\n\n      lr = (self.t_train_max - self.global_t[0] + 1) / self.t_train_max * self.learning_rate\n\n      if terminal:\n        r[self.t] = 0.\n      else:\n        r[self.t] = self.sess.partial_run(\n            self.partial_graph,\n            self.networks[self.t_start - self.t].value,\n        )[0][0]\n\n      for t in range(self.t - 1, self.t_start - 1, -1):\n        r[t] = self.prev_r[t] + self.gamma * r[t + 1]\n\n      data = {}\n      data.update({\n        self.networks[t].R: [r[t + self.t_start]] for t in range(len(self.prev_r) - 1)\n      })\n      data.update({\n        self.networks[t].true_log_policy:\n          [self.prev_log_policy[t + self.t_start]] for t in range(len(self.prev_r) - 1)\n      })\n      data.update({\n        self.learning_rate_op: lr,\n      })\n\n      # 1. Update accumulated gradients\n      if not self.writer:\n        self.sess.partial_run(self.partial_graph,\n            [self.add_accum_grads[t] for t in range(len(self.prev_r) - 1)], data)\n      else:\n        results = self.sess.partial_run(self.partial_graph,\n            [self.value_policy_summary] + [self.add_accum_grads[t] for t in range(len(self.prev_r) - 1)], data)\n\n        summary_str = results[0]\n        self.writer.add_summary(summary_str, self.global_t[0])\n\n      # 2. Update global w with accumulated gradients\n      self.sess.run(self.apply_gradient, {\n        self.learning_rate_op: lr,\n      })\n\n      # 3. Reset accumulated gradients to zero\n      self.sess.run(self.reset_accum_grad)\n\n      # 4. Copy weights of global_network to local_network\n      self.networks[0].copy_from_global()\n\n      self.prev_r = {self.t: self.prev_r[self.t]}\n      self.t_start = self.t\n\n      del self.partial_graph\n'"
agents/_n_step_q.py,17,"b'import os\nimport time\nimport numpy as np\nimport tensorflow as tf\nfrom logging import getLogger\n\nfrom .agent import Agent\nfrom .history import History\nfrom .experience import Experience\n\nlogger = getLogger(__name__)\n\nclass NStepQ(Agent):\n  def __init__(self, sess, pred_network, env, stat, conf, target_network=None):\n    super(DeepQ, self).__init__(sess, pred_network, target_network, env, stat, conf)\n\n    raise Exception(""[!] Not fully implemented yet"")\n\n    # Optimizer\n    with tf.variable_scope(\'optimizer\'):\n      self.targets = tf.placeholder(\'float32\', [None], name=\'target_q_t\')\n      self.actions = tf.placeholder(\'int64\', [None], name=\'action\')\n\n      actions_one_hot = tf.one_hot(self.actions, self.env.action_size, 1.0, 0.0, name=\'action_one_hot\')\n      pred_q = tf.reduce_sum(self.pred_network.outputs * actions_one_hot, reduction_indices=1, name=\'q_acted\')\n\n      self.delta = self.targets - pred_q\n      if self.max_delta and self.min_delta:\n        self.delta = tf.clip_by_value(self.delta, self.min_delta, self.max_delta, name=\'clipped_delta\')\n\n      self.loss = tf.reduce_mean(tf.square(self.delta), name=\'loss\')\n\n      self.learning_rate_op = tf.maximum(self.learning_rate_minimum,\n          tf.train.exponential_decay(\n              self.learning_rate,\n              self.stat.t_op,\n              self.learning_rate_decay_step,\n              self.learning_rate_decay,\n              staircase=True))\n\n      optimizer = tf.train.RMSPropOptimizer(\n        self.learning_rate_op, momentum=0.95, epsilon=0.01)\n      \n      grads_and_vars = optimizer.compute_gradients(self.loss)\n      for idx, (grad, var) in enumerate(grads_and_vars):\n        if grad is not None:\n          grads_and_vars[idx] = (tf.clip_by_norm(grad, self.max_grad_norm), var)\n      self.optim = optimizer.apply_gradients(grads_and_vars)\n\n  # Add accumulated gradients for n-step Q-learning\n  def make_accumulated_gradients(self):\n    reset_accum_grads = []\n    new_grads_and_vars = []\n\n    # 1. Prepare accum_grads\n    self.accum_grads = {}\n    self.add_accum_grads = {}\n\n    for step, network in enumerate(self.networks):\n      grads_and_vars = self.global_optim.compute_gradients(network.total_loss, network.w.values())\n      _add_accum_grads = []\n\n      for grad, var in tuple(grads_and_vars):\n        if grad is not None:\n          shape = grad.get_shape().as_list()\n\n          name = \'accum/%s\' % ""/"".join(var.name.split(\':\')[0].split(\'/\')[-3:])\n          if step == 0:\n            self.accum_grads[name] = tf.Variable(\n                tf.zeros(shape), trainable=False, name=name)\n\n            global_v = global_var[re.sub(r\'.*\\/A3C_\\d+\\/\', \'\', var.name)]\n            new_grads_and_vars.append((tf.clip_by_norm(self.accum_grads[name].ref(), self.max_grad_norm), global_v))\n\n            reset_accum_grads.append(self.accum_grads[name].assign(tf.zeros(shape)))\n\n          _add_accum_grads.append(tf.assign_add(self.accum_grads[name], grad))\n\n      # 2. Add gradient to accum_grads\n      self.add_accum_grads[step] = tf.group(*_add_accum_grads)\n\n  def observe(self, observation, reward, action, terminal):\n    reward = max(self.min_r, min(self.max_r, reward))\n\n    self.history.add(observation)\n    self.experience.add(observation, reward, action, terminal)\n\n    # q, loss, is_update\n    result = [], 0, False\n\n    if self.t > self.t_learn_start:\n      if self.t % self.t_train_freq == 0:\n        result = self.q_learning_minibatch()\n\n      if self.t % self.t_target_q_update_freq == self.t_target_q_update_freq - 1:\n        self.update_target_q_network()\n\n    return result\n\n  def q_learning_minibatch(self):\n    if self.experience.count < self.history_length:\n      return [], 0, False\n    else:\n      s_t, action, reward, s_t_plus_1, terminal = self.experience.sample()\n\n    terminal = np.array(terminal) + 0.\n\n    # Deep Q-learning\n    max_q_t_plus_1 = self.target_network.calc_max_outputs(s_t_plus_1)\n    target_q_t = (1. - terminal) * self.discount_r * max_q_t_plus_1 + reward\n\n    _, q_t, loss = self.sess.run([self.optim, self.pred_network.outputs, self.loss], {\n      self.targets: target_q_t,\n      self.actions: action,\n      self.pred_network.inputs: s_t,\n    })\n\n    return q_t, loss, True\n'"
agents/_sarsa.py,11,"b'import os\nimport time\nimport numpy as np\nimport tensorflow as tf\nfrom logging import getLogger\n\nfrom .agent import Agent\nfrom .history import History\nfrom .experience import Experience\n\nlogger = getLogger(__name__)\n\nclass SARSA(Agent):\n  def __init__(self, sess, pred_network, \n               env, stat, conf, target_network=None, policy_network=None):\n    super(DeepQ, self).__init__(sess, pred_network, target_network, policy_network, env, stat, conf)\n\n    raise Exception(""policy is not implemented yet"")\n\n    # Optimizer\n    with tf.variable_scope(\'optimizer\'):\n      self.targets = tf.placeholder(\'float32\', [None], name=\'target_q_t\')\n      self.actions = tf.placeholder(\'int64\', [None], name=\'action\')\n\n      actions_one_hot = tf.one_hot(self.actions, self.env.action_size, 1.0, 0.0, name=\'action_one_hot\')\n      pred_q = tf.reduce_sum(self.pred_network.outputs * actions_one_hot, reduction_indices=1, name=\'q_acted\')\n\n      self.delta = self.targets - pred_q\n      if self.max_delta and self.min_delta:\n        self.delta = tf.clip_by_value(self.delta, self.min_delta, self.max_delta, name=\'clipped_delta\')\n\n      self.loss = tf.reduce_mean(tf.square(self.delta), name=\'loss\')\n\n      self.learning_rate_op = tf.maximum(self.learning_rate_minimum,\n          tf.train.exponential_decay(\n              self.learning_rate,\n              self.stat.t_op,\n              self.learning_rate_decay_step,\n              self.learning_rate_decay,\n              staircase=True))\n\n      optimizer = tf.train.RMSPropOptimizer(\n        self.learning_rate_op, momentum=0.95, epsilon=0.01)\n      \n      grads_and_vars = optimizer.compute_gradients(self.loss)\n      for idx, (grad, var) in enumerate(grads_and_vars):\n        if grad is not None:\n          grads_and_vars[idx] = (tf.clip_by_norm(grad, self.max_grad_norm), var)\n      self.optim = optimizer.apply_gradients(grads_and_vars)\n\n  def observe(self, observation, reward, action, terminal):\n    reward = max(self.min_r, min(self.max_r, reward))\n\n    self.history.add(observation)\n    self.experience.add(observation, reward, action, terminal)\n\n    # q, loss, is_update\n    result = [], 0, False\n\n    if self.t > self.t_learn_start:\n      if self.t % self.t_train_freq == 0:\n        result = self.q_learning_minibatch()\n\n      if self.t % self.t_target_q_update_freq == self.t_target_q_update_freq - 1:\n        self.update_target_q_network()\n\n    return result\n\n  def q_learning_minibatch(self):\n    if self.experience.count < self.history_length:\n      return [], 0, False\n    else:\n      s_t, action, reward, s_t_plus_1, terminal = self.experience.sample()\n\n    terminal = np.array(terminal) + 0.\n\n    # Deep Q-learning\n    max_q_t_plus_1 = self.target_network.calc_max_outputs(s_t_plus_1)\n    target_q_t = (1. - terminal) * self.discount_r * max_q_t_plus_1 + reward\n\n    _, q_t, loss = self.sess.run([self.optim, self.pred_network.outputs, self.loss], {\n      self.targets: target_q_t,\n      self.actions: action,\n      self.pred_network.inputs: s_t,\n    })\n\n    return q_t, loss, True\n'"
agents/agent.py,2,"b'import gym\nimport time\nimport random\nimport numpy as np\nfrom tqdm import tqdm\nimport tensorflow as tf\nfrom logging import getLogger\n\nfrom .history import History\nfrom .experience import Experience\n\nlogger = getLogger(__name__)\n\ndef get_time():\n  return time.strftime(""%Y-%m-%d_%H:%M:%S"", time.gmtime())\n\nclass Agent(object):\n  def __init__(self, sess, pred_network, env, stat, conf, target_network=None):\n    self.sess = sess\n    self.stat = stat\n\n    self.ep_start = conf.ep_start\n    self.ep_end = conf.ep_end\n    self.history_length = conf.history_length\n    self.t_ep_end = conf.t_ep_end\n    self.t_learn_start = conf.t_learn_start\n    self.t_train_freq = conf.t_train_freq\n    self.t_target_q_update_freq = conf.t_target_q_update_freq\n    self.env_name = conf.env_name\n\n    self.discount_r = conf.discount_r\n    self.min_r = conf.min_r\n    self.max_r = conf.max_r\n    self.min_delta = conf.min_delta\n    self.max_delta = conf.max_delta\n    self.max_grad_norm = conf.max_grad_norm\n    self.observation_dims = conf.observation_dims\n\n    self.learning_rate = conf.learning_rate\n    self.learning_rate_minimum = conf.learning_rate_minimum\n    self.learning_rate_decay = conf.learning_rate_decay\n    self.learning_rate_decay_step = conf.learning_rate_decay_step\n\n    # network\n    self.double_q = conf.double_q\n    self.pred_network = pred_network\n    self.target_network = target_network\n    self.target_network.create_copy_op(self.pred_network)\n\n    self.env = env \n    self.history = History(conf.data_format,\n        conf.batch_size, conf.history_length, conf.observation_dims)\n    self.experience = Experience(conf.data_format,\n        conf.batch_size, conf.history_length, conf.memory_size, conf.observation_dims)\n\n    if conf.random_start:\n      self.new_game = self.env.new_random_game\n    else:\n      self.new_game = self.env.new_game\n\n  def train(self, t_max):\n    tf.global_variables_initializer().run()\n\n    self.stat.load_model()\n    self.target_network.run_copy()\n\n    start_t = self.stat.get_t()\n    observation, reward, terminal = self.new_game()\n\n    for _ in range(self.history_length):\n      self.history.add(observation)\n\n    for self.t in tqdm(range(start_t, t_max), ncols=70, initial=start_t):\n      ep = (self.ep_end +\n          max(0., (self.ep_start - self.ep_end)\n            * (self.t_ep_end - max(0., self.t - self.t_learn_start)) / self.t_ep_end))\n\n      # 1. predict\n      action = self.predict(self.history.get(), ep)\n      # 2. act\n      observation, reward, terminal, info = self.env.step(action, is_training=True)\n      # 3. observe\n      q, loss, is_update = self.observe(observation, reward, action, terminal)\n\n      logger.debug(""a: %d, r: %d, t: %d, q: %.4f, l: %.2f"" % \\\n          (action, reward, terminal, np.mean(q), loss))\n\n      if self.stat:\n        self.stat.on_step(self.t, action, reward, terminal,\n                          ep, q, loss, is_update, self.learning_rate_op)\n      if terminal:\n        observation, reward, terminal = self.new_game()\n\n  def play(self, test_ep, n_step=10000, n_episode=100):\n    tf.initialize_all_variables().run()\n\n    self.stat.load_model()\n    self.target_network.run_copy()\n\n    if not self.env.display:\n      gym_dir = \'/tmp/%s-%s\' % (self.env_name, get_time())\n      env = gym.wrappers.Monitor(self.env.env, gym_dir)\n\n    best_reward, best_idx, best_count = 0, 0, 0\n    try:\n      itr = xrange(n_episode)\n    except NameError:\n      itr = range(n_episode)\n    for idx in itr:\n      observation, reward, terminal = self.new_game()\n      current_reward = 0\n\n      for _ in range(self.history_length):\n        self.history.add(observation)\n\n      for self.t in tqdm(range(n_step), ncols=70):\n        # 1. predict\n        action = self.predict(self.history.get(), test_ep)\n        # 2. act\n        observation, reward, terminal, info = self.env.step(action, is_training=False)\n        # 3. observe\n        q, loss, is_update = self.observe(observation, reward, action, terminal)\n\n        logger.debug(""a: %d, r: %d, t: %d, q: %.4f, l: %.2f"" % \\\n            (action, reward, terminal, np.mean(q), loss))\n        current_reward += reward\n\n        if terminal:\n          break\n\n      if current_reward > best_reward:\n        best_reward = current_reward\n        best_idx = idx\n        best_count = 0\n      elif current_reward == best_reward:\n        best_count += 1\n\n      print (""=""*30)\n      print ("" [%d] Best reward : %d (dup-percent: %d/%d)"" % (best_idx, best_reward, best_count, n_episode))\n      print (""=""*30)\n\n    #if not self.env.display:\n      #gym.upload(gym_dir, writeup=\'https://github.com/devsisters/DQN-tensorflow\', api_key=\'\')\n\n  def predict(self, s_t, ep):\n    if random.random() < ep:\n      action = random.randrange(self.env.action_size)\n    else:\n      action = self.pred_network.calc_actions([s_t])[0]\n    return action\n\n  def q_learning_minibatch_test(self):\n    s_t = np.array([[[ 0., 0., 0., 0.],\n                     [ 0., 0., 0., 0.],\n                     [ 0., 0., 0., 0.],\n                     [ 1., 0., 0., 0.]]], dtype=np.uint8)\n    s_t_plus_1 = np.array([[[ 0., 0., 0., 0.],\n                            [ 0., 0., 0., 0.],\n                            [ 1., 0., 0., 0.],\n                            [ 0., 0., 0., 0.]]], dtype=np.uint8)\n    s_t = s_t.reshape([1, 1] + self.observation_dims)\n    s_t_plus_1 = s_t_plus_1.reshape([1, 1] + self.observation_dims)\n\n    action = [3]\n    reward = [1]\n    terminal = [0]\n\n    terminal = np.array(terminal) + 0.\n    max_q_t_plus_1 = self.target_network.calc_max_outputs(s_t_plus_1)\n    target_q_t = (1. - terminal) * self.discount_r * max_q_t_plus_1 + reward\n\n    _, q_t, a, loss = self.sess.run([\n        self.optim, self.pred_network.outputs, self.pred_network.actions, self.loss\n      ], {\n        self.targets: target_q_t,\n        self.actions: action,\n        self.pred_network.inputs: s_t,\n      })\n\n    logger.info(""q: %s, a: %d, l: %.2f"" % (q_t, a, loss))\n\n  def update_target_q_network(self):\n    assert self.target_network != None\n    self.target_network.run_copy()\n'"
agents/deep_q.py,13,"b""import os\nimport time\nimport numpy as np\nimport tensorflow as tf\nfrom logging import getLogger\n\nfrom .agent import Agent\n\nlogger = getLogger(__name__)\n\nclass DeepQ(Agent):\n  def __init__(self, sess, pred_network, env, stat, conf, target_network=None):\n    super(DeepQ, self).__init__(sess, pred_network, env, stat, conf, target_network=target_network)\n\n    # Optimizer\n    with tf.variable_scope('optimizer'):\n      self.targets = tf.placeholder('float32', [None], name='target_q_t')\n      self.actions = tf.placeholder('int64', [None], name='action')\n\n      actions_one_hot = tf.one_hot(self.actions, self.env.action_size, 1.0, 0.0, name='action_one_hot')\n      pred_q = tf.reduce_sum(self.pred_network.outputs * actions_one_hot, reduction_indices=1, name='q_acted')\n\n      self.delta = self.targets - pred_q\n      self.clipped_error = tf.where(tf.abs(self.delta) < 1.0,\n                                    0.5 * tf.square(self.delta),\n                                    tf.abs(self.delta) - 0.5, name='clipped_error')\n\n      self.loss = tf.reduce_mean(self.clipped_error, name='loss')\n\n      self.learning_rate_op = tf.maximum(self.learning_rate_minimum,\n          tf.train.exponential_decay(\n              self.learning_rate,\n              self.stat.t_op,\n              self.learning_rate_decay_step,\n              self.learning_rate_decay,\n              staircase=True))\n\n      optimizer = tf.train.RMSPropOptimizer(\n        self.learning_rate_op, momentum=0.95, epsilon=0.01)\n      \n      if self.max_grad_norm != None:\n        grads_and_vars = optimizer.compute_gradients(self.loss)\n        for idx, (grad, var) in enumerate(grads_and_vars):\n          if grad is not None:\n            grads_and_vars[idx] = (tf.clip_by_norm(grad, self.max_grad_norm), var)\n        self.optim = optimizer.apply_gradients(grads_and_vars)\n      else:\n        self.optim = optimizer.minimize(self.loss)\n\n  def observe(self, observation, reward, action, terminal):\n    reward = max(self.min_r, min(self.max_r, reward))\n\n    self.history.add(observation)\n    self.experience.add(observation, reward, action, terminal)\n\n    # q, loss, is_update\n    result = [], 0, False\n\n    if self.t > self.t_learn_start:\n      if self.t % self.t_train_freq == 0:\n        result = self.q_learning_minibatch()\n\n      if self.t % self.t_target_q_update_freq == self.t_target_q_update_freq - 1:\n        self.update_target_q_network()\n\n    return result\n\n  def q_learning_minibatch(self):\n    if self.experience.count < self.history_length:\n      return [], 0, False\n    else:\n      s_t, action, reward, s_t_plus_1, terminal = self.experience.sample()\n\n    terminal = np.array(terminal) + 0.\n\n    if self.double_q:\n      # Double Q-learning\n      pred_action = self.pred_network.calc_actions(s_t_plus_1)\n      q_t_plus_1_with_pred_action = self.target_network.calc_outputs_with_idx(\n          s_t_plus_1, [[idx, pred_a] for idx, pred_a in enumerate(pred_action)])\n      target_q_t = (1. - terminal) * self.discount_r * q_t_plus_1_with_pred_action + reward\n    else:\n      # Deep Q-learning\n      max_q_t_plus_1 = self.target_network.calc_max_outputs(s_t_plus_1)\n      target_q_t = (1. - terminal) * self.discount_r * max_q_t_plus_1 + reward\n\n    _, q_t, loss = self.sess.run([self.optim, self.pred_network.outputs, self.loss], {\n      self.targets: target_q_t,\n      self.actions: action,\n      self.pred_network.inputs: s_t,\n    })\n\n    return q_t, loss, True\n"""
agents/experience.py,0,"b'""""""Modification of https://github.com/tambetm/simple_dqn/blob/master/src/replay_memory.py""""""\n\nimport random\nimport numpy as np\n\nclass Experience(object):\n  def __init__(self, data_format, batch_size, history_length, memory_size, observation_dims):\n    self.data_format = data_format\n    self.batch_size = batch_size\n    self.history_length = history_length\n    self.memory_size = memory_size\n\n    self.actions = np.empty(self.memory_size, dtype=np.uint8)\n    self.rewards = np.empty(self.memory_size, dtype=np.int8)\n    self.observations = np.empty([self.memory_size] + observation_dims, dtype=np.uint8)\n    self.terminals = np.empty(self.memory_size, dtype=np.bool)\n\n    # pre-allocate prestates and poststates for minibatch\n    self.prestates = np.empty([self.batch_size, self.history_length] + observation_dims, dtype = np.float16)\n    self.poststates = np.empty([self.batch_size, self.history_length] + observation_dims, dtype = np.float16)\n\n    self.count = 0\n    self.current = 0\n\n  def add(self, observation, reward, action, terminal):\n    self.actions[self.current] = action\n    self.rewards[self.current] = reward\n    self.observations[self.current, ...] = observation\n    self.terminals[self.current] = terminal\n    self.count = max(self.count, self.current + 1)\n    self.current = (self.current + 1) % self.memory_size\n\n  def sample(self):\n    indexes = []\n    while len(indexes) < self.batch_size:\n      while True:\n        index = random.randint(self.history_length, self.count - 1)\n        if index >= self.current and index - self.history_length < self.current:\n          continue\n        if self.terminals[(index - self.history_length):index].any():\n          continue\n        break\n      \n      self.prestates[len(indexes), ...] = self.retreive(index - 1)\n      self.poststates[len(indexes), ...] = self.retreive(index)\n      indexes.append(index)\n\n    actions = self.actions[indexes]\n    rewards = self.rewards[indexes]\n    terminals = self.terminals[indexes]\n\n    if self.data_format == \'NHWC\' and len(self.prestates.shape) == 4:\n      return np.transpose(self.prestates, (0, 2, 3, 1)), actions, \\\n        rewards, np.transpose(self.poststates, (0, 2, 3, 1)), terminals\n    else:\n      return self.prestates, actions, rewards, self.poststates, terminals\n\n  def retreive(self, index):\n    index = index % self.count\n    if index >= self.history_length - 1:\n      return self.observations[(index - (self.history_length - 1)):(index + 1), ...]\n    else:\n      indexes = [(index - i) % self.count for i in reversed(range(self.history_length))]\n      return self.observations[indexes, ...]\n'"
agents/history.py,0,"b""import numpy as np\n\nclass History:\n  def __init__(self, data_format, batch_size, history_length, screen_dims):\n    self.data_format = data_format\n    self.history = np.zeros([history_length] + screen_dims, dtype=np.float32)\n\n  def add(self, screen):\n    self.history[:-1] = self.history[1:]\n    self.history[-1] = screen\n\n  def reset(self):\n    self.history *= 0\n\n  def get(self):\n    if self.data_format == 'NHWC' and len(self.history.shape) == 3:\n      return np.transpose(self.history, (1, 2, 0))\n    else:\n      return self.history\n"""
agents/statistic.py,10,"b'import os\nimport numpy as np\nimport tensorflow as tf\n\nclass Statistic(object):\n  def __init__(self, sess, t_test, t_learn_start, model_dir, variables, max_to_keep=20):\n    self.sess = sess\n    self.t_test = t_test\n    self.t_learn_start = t_learn_start\n\n    self.reset()\n    self.max_avg_ep_reward = 0\n\n    with tf.variable_scope(\'t\'):\n      self.t_op = tf.Variable(0, trainable=False, name=\'t\')\n      self.t_add_op = self.t_op.assign_add(1)\n\n    self.model_dir = model_dir\n    self.saver = tf.train.Saver(list(variables) + [self.t_op], max_to_keep=max_to_keep)\n    self.writer = tf.summary.FileWriter(\'./logs/%s\' % self.model_dir, self.sess.graph)\n\n    with tf.variable_scope(\'summary\'):\n      scalar_summary_tags = [\n        \'average/reward\', \'average/loss\', \'average/q\',\n        \'episode/max_reward\', \'episode/min_reward\', \'episode/avg_reward\',\n        \'episode/num_of_game\', \'training/learning_rate\', \'training/epsilon\',\n      ]\n\n      self.summary_placeholders = {}\n      self.summary_ops = {}\n\n      for tag in scalar_summary_tags:\n        self.summary_placeholders[tag] = tf.placeholder(\'float32\', None, name=tag.replace(\' \', \'_\'))\n        self.summary_ops[tag]  = tf.summary.scalar(tag, self.summary_placeholders[tag])\n\n      histogram_summary_tags = [\'episode/rewards\', \'episode/actions\']\n\n      for tag in histogram_summary_tags:\n        self.summary_placeholders[tag] = tf.placeholder(\'float32\', None, name=tag.replace(\' \', \'_\'))\n        self.summary_ops[tag]  = tf.summary.histogram(tag, self.summary_placeholders[tag])\n\n\n  def reset(self):\n    self.num_game = 0\n    self.update_count = 0\n    self.ep_reward = 0.\n    self.total_loss = 0.\n    self.total_reward = 0.\n    self.actions = []\n    self.total_q = []\n    self.ep_rewards = []\n\n  def on_step(self, t, action, reward, terminal, \n              ep, q, loss, is_update, learning_rate_op):\n    if t >= self.t_learn_start:\n      self.total_q.extend(q)\n      self.actions.append(action)\n\n      self.total_loss += loss\n      self.total_reward += reward\n\n      if terminal:\n        self.num_game += 1\n        self.ep_rewards.append(self.ep_reward)\n        self.ep_reward = 0.\n      else:\n        self.ep_reward += reward\n\n      if is_update:\n        self.update_count += 1\n\n      if t % self.t_test == self.t_test - 1 and self.update_count != 0:\n        avg_q = np.mean(self.total_q)\n        avg_loss = self.total_loss / self.update_count\n        avg_reward = self.total_reward / self.t_test\n\n        try:\n          max_ep_reward = np.max(self.ep_rewards)\n          min_ep_reward = np.min(self.ep_rewards)\n          avg_ep_reward = np.mean(self.ep_rewards)\n        except:\n          max_ep_reward, min_ep_reward, avg_ep_reward = 0, 0, 0\n\n        print (\'\\navg_r: %.4f, avg_l: %.6f, avg_q: %3.6f, avg_ep_r: %.4f, max_ep_r: %.4f, min_ep_r: %.4f, # game: %d\' \\\n            % (avg_reward, avg_loss, avg_q, avg_ep_reward, max_ep_reward, min_ep_reward, self.num_game))\n\n        if self.max_avg_ep_reward * 0.9 <= avg_ep_reward:\n          assert t == self.get_t()\n\n          self.save_model(t)\n\n          self.max_avg_ep_reward = max(self.max_avg_ep_reward, avg_ep_reward)\n\n        self.inject_summary({\n            \'average/q\': avg_q,\n            \'average/loss\': avg_loss,\n            \'average/reward\': avg_reward,\n            \'episode/max_reward\': max_ep_reward,\n            \'episode/min_reward\': min_ep_reward,\n            \'episode/avg_reward\': avg_ep_reward,\n            \'episode/num_of_game\': self.num_game,\n            \'episode/actions\': self.actions,\n            \'episode/rewards\': self.ep_rewards,\n            \'training/learning_rate\': learning_rate_op.eval(session=self.sess),\n            \'training/epsilon\': ep,\n          }, t)\n\n        self.reset()\n\n    self.t_add_op.eval(session=self.sess)\n\n  def inject_summary(self, tag_dict, t):\n    summary_str_lists = self.sess.run([self.summary_ops[tag] for tag in tag_dict.keys()], {\n      self.summary_placeholders[tag]: value for tag, value in tag_dict.items()\n    })\n    for summary_str in summary_str_lists:\n      self.writer.add_summary(summary_str, t)\n\n  def get_t(self):\n    return self.t_op.eval(session=self.sess)\n\n  def save_model(self, t):\n    print("" [*] Saving checkpoints..."")\n    model_name = type(self).__name__\n\n    if not os.path.exists(self.model_dir):\n      os.makedirs(self.model_dir)\n    self.saver.save(self.sess, self.model_dir, global_step=t)\n\n  def load_model(self):\n    ckpt = tf.train.get_checkpoint_state(self.model_dir)\n    if ckpt and ckpt.model_checkpoint_path:\n      ckpt_name = os.path.basename(ckpt.model_checkpoint_path)\n      fname = os.path.join(self.model_dir, ckpt_name)\n      self.saver.restore(self.sess, fname)\n      print("" [*] Load SUCCESS: %s"" % fname)\n      return True\n    else:\n      print("" [!] Load FAILED: %s"" % self.model_dir)\n      return False\n'"
environments/__init__.py,0,b''
environments/corridor.py,0,"b'import numpy as np\nimport sys\ntry:\n    from StringIO import StringIO\nexcept ImportError:\n    from io import StringIO\nfrom gym import utils\nfrom gym import spaces\nfrom gym.envs.toy_text import discrete\nfrom gym.envs.registration import register\n\nMAPS = {\n  ""4x4"": [\n    ""HHHD"",\n    ""FFFF"",\n    ""FHHH"",\n    ""SHHH"",\n  ],\n  ""9x9"": [\n    ""HHHHHHHHD"",\n    ""HHHHHHHHF"",\n    ""HHHHHHHHF"",\n    ""HHHHHHHHF"",\n    ""FFFFFFFFF"",\n    ""FHHHHHHHH"",\n    ""FHHHHHHHH"",\n    ""FHHHHHHHH"",\n    ""SHHHHHHHH"",\n  ],\n}\n\nclass CorridorEnv(discrete.DiscreteEnv):\n  """"""\n  The surface is described using a grid like the following\n\n    HHHD\n    FFFF\n    SHHH\n    AHHH\n\n  S : starting point, safe\n  F : frozen surface, safe\n  H : hole, fall to your doom\n  A : adjacent goal\n  D : distant goal\n\n  The episode ends when you reach the goal or fall in a hole.\n  You receive a reward of 1 if you reach the adjacent goal, \n  10 if you reach the distant goal, and zero otherwise.\n  """"""\n  metadata = {\'render.modes\': [\'human\', \'ansi\']}\n\n  def __init__(self, desc=None, map_name=""9x9"", n_actions=5):\n    if desc is None and map_name is None:\n      raise ValueError(\'Must provide either desc or map_name\')\n    elif desc is None:\n      desc = MAPS[map_name]\n    self.desc = desc = np.asarray(desc, dtype=\'c\')\n    self.nrow, self.ncol = nrow, ncol = desc.shape\n\n    self.action_space = spaces.Discrete(n_actions)\n    self.observation_space = spaces.Discrete(desc.size)\n\n    n_state = nrow * ncol\n\n    isd = (desc == \'S\').ravel().astype(\'float64\')\n    isd /= isd.sum()\n\n    P = {s : {a : [] for a in xrange(n_actions)} for s in xrange(n_state)}\n\n    def to_s(row, col):\n      return row*ncol + col\n    def inc(row, col, a):\n      if a == 0: # left\n        col = max(col-1,0)\n      elif a == 1: # down\n        row = min(row+1, nrow-1)\n      elif a == 2: # right\n        col = min(col+1, ncol-1)\n      elif a == 3: # up\n        row = max(row-1, 0)\n\n      return (row, col)\n\n    for row in xrange(nrow):\n      for col in xrange(ncol):\n        s = to_s(row, col)\n        for a in xrange(n_actions):\n          li = P[s][a]\n          newrow, newcol = inc(row, col, a)\n          newstate = to_s(newrow, newcol)\n          letter = desc[newrow, newcol]\n          done = letter in \'DAH\'\n          rew = 1.0 if letter == \'A\' \\\n              else 10.0 if letter == \'D\' \\\n              else -1.0 if letter == \'H\' \\\n              else 1.0 if (newrow != row or newcol != col) and letter == \'F\' \\\n              else 0.0\n          li.append((1.0/3.0, newstate, rew, done))\n\n    super(CorridorEnv, self).__init__(nrow * ncol, n_actions, P, isd)\n\n  def _render(self, mode=\'human\', close=False):\n    if close:\n      return\n\n    outfile = StringIO.StringIO() if mode == \'ansi\' else sys.stdout\n\n    row, col = self.s // self.ncol, self.s % self.ncol\n    desc = self.desc.tolist()\n    desc[row][col] = utils.colorize(desc[row][col], ""red"", highlight=True)\n\n    outfile.write(""\\n"".join("""".join(row) for row in desc)+""\\n"")\n    if self.lastaction is not None:\n      outfile.write(""  ({})\\n"".format(self.get_action_meanings()[self.lastaction]))\n    else:\n      outfile.write(""\\n"")\n\n    return outfile\n\n  def get_action_meanings(self):\n    return [[""Left"", ""Down"", ""Right"", ""Up""][i] if i < 4 else ""NoOp"" for i in xrange(self.action_space.n)]\n\nregister(\n  id=\'CorridorSmall-v5\',\n  entry_point=\'environments.corridor:CorridorEnv\',\n  kwargs={\n    \'map_name\': \'4x4\',\n    \'n_actions\': 5\n  },\n  timestep_limit=100,\n)\n\nregister(\n  id=\'CorridorSmall-v10\',\n  entry_point=\'environments.corridor:CorridorEnv\',\n  kwargs={\n    \'map_name\': \'4x4\',\n    \'n_actions\': 10\n  },\n  timestep_limit=100,\n)\n\nregister(\n  id=\'CorridorBig-v5\',\n  entry_point=\'environments.corridor:CorridorEnv\',\n  kwargs={\n    \'map_name\': \'9x9\',\n    \'n_actions\': 5\n  },\n  timestep_limit=100,\n)\n\nregister(\n  id=\'CorridorBig-v10\',\n  entry_point=\'environments.corridor:CorridorEnv\',\n  kwargs={\n    \'map_name\': \'9x9\',\n    \'n_actions\': 10\n  },\n  timestep_limit=100,\n)\n'"
environments/environment.py,0,"b'import gym\nimport random\nimport logging\nimport numpy as np\n\nfrom .corridor import CorridorEnv\n\ntry:\n  import scipy.misc\n  imresize = scipy.misc.imresize\n  imwrite = scipy.misc.imsave\nexcept:\n  import cv2\n  imresize = cv2.resize\n  imwrite = cv2.imwrite\n\nlogger = logging.getLogger(__name__)\n\nclass Environment(object):\n  def __init__(self, env_name, n_action_repeat, max_random_start,\n               observation_dims, data_format, display, use_cumulated_reward=False):\n    self.env = gym.make(env_name)\n\n    self.n_action_repeat = n_action_repeat\n    self.max_random_start = max_random_start\n    self.action_size = self.env.action_space.n\n\n    self.display = display\n    self.data_format = data_format\n    self.observation_dims = observation_dims\n    self.use_cumulated_reward = use_cumulated_reward\n\n    if hasattr(self.env, \'get_action_meanings\'):\n      logger.info(""Using %d actions : %s"" % (self.action_size, "", "".join(self.env.get_action_meanings())))\n\n  def new_game(self):\n    return self.preprocess(self.env.reset()), 0, False\n\n  def new_random_game(self):\n    return self.new_game()\n\n  def step(self, action, is_training=False):\n    observation, reward, terminal, info = self.env.step(action)\n    if self.display: self.env.render()\n    return self.preprocess(observation), reward, terminal, info\n\n  def preprocess(self):\n    raise NotImplementedError()\n\nclass ToyEnvironment(Environment):\n  def preprocess(self, obs):\n    new_obs = np.zeros([self.env.observation_space.n])\n    new_obs[obs] = 1\n    return new_obs\n\nclass AtariEnvironment(Environment):\n  def __init__(self, env_name, n_action_repeat, max_random_start,\n               observation_dims, data_format, display, use_cumulated_reward):\n    super(AtariEnvironment, self).__init__(env_name, \n        n_action_repeat, max_random_start, observation_dims, data_format, display, use_cumulated_reward)\n\n  def new_game(self, from_random_game=False):\n    screen = self.env.reset()\n    screen, reward, terminal, _ = self.env.step(0)\n\n    if self.display:\n      self.env.render()\n\n    if from_random_game:\n      return screen, 0, False\n    else:\n      self.lives = self.env.unwrapped.ale.lives()\n      terminal = False\n      return self.preprocess(screen, terminal), 0, terminal\n\n  def new_random_game(self):\n    screen, reward, terminal = self.new_game(True)\n\n    for idx in range(random.randrange(self.max_random_start)):\n      screen, reward, terminal, _ = self.env.step(0)\n\n      if terminal: logger.warning(""warning: terminal signal received after %d 0-steps"", idx)\n\n    if self.display:\n      self.env.render()\n\n    self.lives = self.env.unwrapped.ale.lives()\n\n    terminal = False\n    return self.preprocess(screen, terminal), 0, terminal\n\n  def step(self, action, is_training):\n    if action == -1:\n      # Step with random action\n      action = self.env.action_space.sample()\n\n    cumulated_reward = 0\n\n    for _ in range(self.n_action_repeat):\n      screen, reward, terminal, _ = self.env.step(action)\n      cumulated_reward += reward\n      current_lives = self.env.unwrapped.ale.lives()\n\n      if is_training and self.lives > current_lives:\n        terminal = True\n\n      if terminal: break\n\n    if self.display:\n      self.env.render()\n\n    if not terminal:\n      self.lives = current_lives\n\n    if self.use_cumulated_reward:\n      return self.preprocess(screen, terminal), cumulated_reward, terminal, {}\n    else:\n      return self.preprocess(screen, terminal), reward, terminal, {}\n\n  def preprocess(self, raw_screen, terminal):\n    y = 0.2126 * raw_screen[:, :, 0] + 0.7152 * raw_screen[:, :, 1] + 0.0722 * raw_screen[:, :, 2]\n    y = y.astype(np.uint8)\n    y_screen = imresize(y, self.observation_dims)\n    return y_screen\n'"
networks/__init__.py,0,b''
networks/cnn.py,6,"b'import os\nimport tensorflow as tf\n\nfrom .layers import *\nfrom .network import Network\n\nclass CNN(Network):\n  def __init__(self, sess,\n               data_format,\n               history_length,\n               observation_dims,\n               output_size, \n               trainable=True,\n               hidden_activation_fn=tf.nn.relu,\n               output_activation_fn=None,\n               weights_initializer=initializers.xavier_initializer(),\n               biases_initializer=tf.constant_initializer(0.1),\n               value_hidden_sizes=[512],\n               advantage_hidden_sizes=[512],\n               network_output_type=\'dueling\',\n               network_header_type=\'nips\',\n               name=\'CNN\'):\n    super(CNN, self).__init__(sess, name)\n\n    if data_format == \'NHWC\':\n      self.inputs = tf.placeholder(\'float32\',\n          [None] + observation_dims + [history_length], name=\'inputs\')\n    elif data_format == \'NCHW\':\n      self.inputs = tf.placeholder(\'float32\',\n          [None, history_length] + observation_dims, name=\'inputs\')\n    else:\n      raise ValueError(""unknown data_format : %s"" % data_format)\n\n    self.var = {}\n    self.l0 = tf.div(self.inputs, 255.)\n\n    with tf.variable_scope(name):\n      if network_header_type.lower() == \'nature\':\n        self.l1, self.var[\'l1_w\'], self.var[\'l1_b\'] = conv2d(self.l0,\n            32, [8, 8], [4, 4], weights_initializer, biases_initializer,\n            hidden_activation_fn, data_format, name=\'l1_conv\')\n        self.l2, self.var[\'l2_w\'], self.var[\'l2_b\'] = conv2d(self.l1,\n            64, [4, 4], [2, 2], weights_initializer, biases_initializer,\n            hidden_activation_fn, data_format, name=\'l2_conv\')\n        self.l3, self.var[\'l3_w\'], self.var[\'l3_b\'] = conv2d(self.l2,\n            64, [3, 3], [1, 1], weights_initializer, biases_initializer,\n            hidden_activation_fn, data_format, name=\'l3_conv\')\n        self.l4, self.var[\'l4_w\'], self.var[\'l4_b\'] = \\\n            linear(self.l3, 512, weights_initializer, biases_initializer,\n            hidden_activation_fn, data_format, name=\'l4_conv\')\n        layer = self.l4\n      elif network_header_type.lower() == \'nips\':\n        self.l1, self.var[\'l1_w\'], self.var[\'l1_b\'] = conv2d(self.l0,\n            16, [8, 8], [4, 4], weights_initializer, biases_initializer,\n            hidden_activation_fn, data_format, name=\'l1_conv\')\n        self.l2, self.var[\'l2_w\'], self.var[\'l2_b\'] = conv2d(self.l1,\n            32, [4, 4], [2, 2], weights_initializer, biases_initializer,\n            hidden_activation_fn, data_format, name=\'l2_conv\')\n        self.l3, self.var[\'l3_w\'], self.var[\'l3_b\'] = \\\n            linear(self.l2, 256, weights_initializer, biases_initializer,\n            hidden_activation_fn, data_format, name=\'l3_conv\')\n        layer = self.l3\n      else:\n        raise ValueError(\'Wrong DQN type: %s\' % network_header_type)\n\n      self.build_output_ops(layer, network_output_type,\n          value_hidden_sizes, advantage_hidden_sizes, output_size,\n          weights_initializer, biases_initializer, hidden_activation_fn,\n          output_activation_fn, trainable)\n'"
networks/layers.py,19,"b""import tensorflow as tf\nfrom functools import reduce\nfrom tensorflow.contrib.layers.python.layers import initializers\n\ndef conv2d(x,\n           output_dim,\n           kernel_size,\n           stride,\n           weights_initializer=tf.contrib.layers.xavier_initializer(),\n           biases_initializer=tf.zeros_initializer,\n           activation_fn=tf.nn.relu,\n           data_format='NHWC',\n           padding='VALID',\n           name='conv2d',\n           trainable=True):\n  with tf.variable_scope(name):\n    if data_format == 'NCHW':\n      stride = [1, 1, stride[0], stride[1]]\n      kernel_shape = [kernel_size[0], kernel_size[1], x.get_shape()[1], output_dim]\n    elif data_format == 'NHWC':\n      stride = [1, stride[0], stride[1], 1]\n      kernel_shape = [kernel_size[0], kernel_size[1], x.get_shape()[-1], output_dim]\n\n    w = tf.get_variable('w', kernel_shape, \n        tf.float32, initializer=weights_initializer, trainable=trainable)\n    conv = tf.nn.conv2d(x, w, stride, padding, data_format=data_format)\n\n    b = tf.get_variable('b', [output_dim],\n        tf.float32, initializer=biases_initializer, trainable=trainable)\n    out = tf.nn.bias_add(conv, b, data_format)\n\n  if activation_fn != None:\n    out = activation_fn(out)\n\n  return out, w, b\n\ndef linear(input_,\n           output_size,\n           weights_initializer=initializers.xavier_initializer(),\n           biases_initializer=tf.zeros_initializer,\n           activation_fn=None,\n           trainable=True,\n           name='linear'):\n  shape = input_.get_shape().as_list()\n\n  if len(shape) > 2:\n    input_ = tf.reshape(input_, [-1, reduce(lambda x, y: x * y, shape[1:])])\n    shape = input_.get_shape().as_list()\n\n  with tf.variable_scope(name):\n    w = tf.get_variable('w', [shape[1], output_size], tf.float32,\n        initializer=weights_initializer, trainable=trainable)\n    b = tf.get_variable('b', [output_size],\n        initializer=biases_initializer, trainable=trainable)\n    out = tf.nn.bias_add(tf.matmul(input_, w), b)\n\n    if activation_fn != None:\n      return activation_fn(out), w, b\n    else:\n      return out, w, b\n\ndef batch_sample(probs, name='batch_sample'):\n  with tf.variable_scope(name):\n    uniform = tf.random_uniform(tf.shape(probs), minval=0, maxval=1)\n    samples = tf.argmax(probs - uniform, dimension=1)\n  return samples\n"""
networks/mlp.py,6,"b'import tensorflow as tf\n\nfrom .layers import *\nfrom .network import Network\n\nclass MLPSmall(Network):\n  def __init__(self, sess,\n               data_format,\n               observation_dims,\n               history_length,\n               output_size,\n               trainable=True,\n               batch_size=None,\n               weights_initializer=initializers.xavier_initializer(),\n               biases_initializer=tf.zeros_initializer,\n               hidden_activation_fn=tf.nn.relu,\n               output_activation_fn=None,\n               hidden_sizes=[50, 50, 50],\n               value_hidden_sizes=[25],\n               advantage_hidden_sizes=[25],\n               network_output_type=\'dueling\',\n               name=\'MLPSmall\'):\n    super(MLPSmall, self).__init__(sess, name)\n\n    with tf.variable_scope(name):\n      if data_format == \'NHWC\':\n        layer = self.inputs = tf.placeholder(\n          \'float32\', [batch_size] + observation_dims + [history_length], \'inputs\')\n      elif data_format == \'NCHW\':\n        layer = self.inputs = tf.placeholder(\n          \'float32\', [batch_size, history_length] + observation_dims, \'inputs\')\n      else:\n        raise ValueError(""unknown data_format : %s"" % data_format)\n\n      if len(layer.get_shape().as_list()) == 3:\n        assert layer.get_shape().as_list()[1] == 1\n        layer = tf.reshape(layer, [-1] + layer.get_shape().as_list()[2:])\n\n      for idx, hidden_size in enumerate(hidden_sizes):\n        w_name, b_name = \'w_%d\' % idx, \'b_%d\' % idx\n\n        layer, self.var[w_name], self.var[b_name] = \\\n            linear(layer, hidden_size, weights_initializer, \n              biases_initializer, hidden_activation_fn, trainable, name=\'lin_%d\' % idx)\n\n      self.build_output_ops(layer, network_output_type, \n          value_hidden_sizes, advantage_hidden_sizes, output_size, \n          weights_initializer, biases_initializer, hidden_activation_fn,\n          output_activation_fn, trainable)\n'"
networks/network.py,8,"b'import tensorflow as tf\n\nfrom .layers import *\n\nclass Network(object):\n  def __init__(self, sess, name):\n    self.sess = sess\n    self.copy_op = None\n    self.name = name\n    self.var = {}\n\n  def build_output_ops(self, input_layer, network_output_type, \n      value_hidden_sizes, advantage_hidden_sizes, output_size, \n      weights_initializer, biases_initializer, hidden_activation_fn, \n      output_activation_fn, trainable):\n    if network_output_type == \'normal\':\n      self.outputs, self.var[\'w_out\'], self.var[\'b_out\'] = \\\n          linear(input_layer, output_size, weights_initializer,\n                 biases_initializer, output_activation_fn, trainable, name=\'out\')\n    elif network_output_type == \'dueling\':\n      # Dueling Network\n      assert len(value_hidden_sizes) != 0 and len(advantage_hidden_sizes) != 0\n\n      layer = input_layer\n      for idx, hidden_size in enumerate(value_hidden_sizes):\n        w_name, b_name = \'val_w_%d\' % idx, \'val_b_%d\' % idx\n\n        layer, self.var[w_name], self.var[b_name] = \\\n            linear(layer, hidden_size, weights_initializer,\n              biases_initializer, hidden_activation_fn, trainable, name=\'val_lin_%d\' % idx)\n\n      self.value, self.var[\'val_w_out\'], self.var[\'val_w_b\'] = \\\n          linear(layer, 1, weights_initializer,\n            biases_initializer, output_activation_fn, trainable, name=\'val_lin_out\')\n\n      layer = input_layer\n      for idx, hidden_size in enumerate(advantage_hidden_sizes):\n        w_name, b_name = \'adv_w_%d\' % idx, \'adv_b_%d\' % idx\n\n        layer, self.var[w_name], self.var[b_name] = \\\n            linear(layer, hidden_size, weights_initializer,\n              biases_initializer, hidden_activation_fn, trainable, name=\'adv_lin_%d\' % idx)\n\n      self.advantage, self.var[\'adv_w_out\'], self.var[\'adv_w_b\'] = \\\n          linear(layer, output_size, weights_initializer,\n            biases_initializer, output_activation_fn, trainable, name=\'adv_lin_out\')\n\n      # Simple Dueling\n      # self.outputs = self.value + self.advantage\n\n      # Max Dueling\n      # self.outputs = self.value + (self.advantage - \n      #     tf.reduce_max(self.advantage, reduction_indices=1, keep_dims=True))\n\n      # Average Dueling\n      self.outputs = self.value + (self.advantage - \n          tf.reduce_mean(self.advantage, reduction_indices=1, keepdims=True))\n\n    self.max_outputs = tf.reduce_max(self.outputs, reduction_indices=1)\n    self.outputs_idx = tf.placeholder(\'int32\', [None, None], \'outputs_idx\')\n    self.outputs_with_idx = tf.gather_nd(self.outputs, self.outputs_idx)\n    self.actions = tf.argmax(self.outputs, axis=1)\n\n  def run_copy(self):\n    if self.copy_op is None:\n      raise Exception(""run `create_copy_op` first before copy"")\n    else:\n      self.sess.run(self.copy_op)\n\n  def create_copy_op(self, network):\n    with tf.variable_scope(self.name):\n      copy_ops = []\n\n      for name in self.var.keys():\n        copy_op = self.var[name].assign(network.var[name])\n        copy_ops.append(copy_op)\n\n      self.copy_op = tf.group(*copy_ops, name=\'copy_op\')\n\n  def calc_actions(self, observation):\n    return self.actions.eval({self.inputs: observation}, session=self.sess)\n\n  def calc_outputs(self, observation):\n    return self.outputs.eval({self.inputs: observation}, session=self.sess)\n\n  def calc_max_outputs(self, observation):\n    return self.max_outputs.eval({self.inputs: observation}, session=self.sess)\n\n  def calc_outputs_with_idx(self, observation, idx):\n    return self.outputs_with_idx.eval(\n        {self.inputs: observation, self.outputs_idx: idx}, session=self.sess)\n'"
