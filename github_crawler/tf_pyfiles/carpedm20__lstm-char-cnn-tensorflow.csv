file_path,api_count,code
batch_loader.py,0,"b'import re\nimport os\nimport math\nimport pickle\nimport numpy as np\n\n\ndef save(fname, obj):\n  with open(fname, \'w\') as f:\n    pickle.dump(obj, f)\n\n\ndef load(fname):\n  with open(fname, \'r\') as f:\n    return pickle.load(f)\n\n\nclass BatchLoader(object):\n  \n  def __init__(self, data_dir, dataset_name, batch_size, seq_length, max_word_length):\n    train_fname = os.path.join(data_dir, dataset_name, \'train.txt\')\n    valid_fname = os.path.join(data_dir, dataset_name, \'valid.txt\')\n    test_fname = os.path.join(data_dir, dataset_name, \'test.txt\')\n    input_fnames = [train_fname, valid_fname, test_fname]\n\n    vocab_fname = os.path.join(data_dir, dataset_name, \'vocab.pkl\')\n    tensor_fname = os.path.join(data_dir, dataset_name, \'data.pkl\')\n    char_fname = os.path.join(data_dir, dataset_name, \'data_char.pkl\')\n\n    if not os.path.exists(vocab_fname) or not os.path.exists(tensor_fname) or not os.path.exists(char_fname):\n      print(""Creating vocab..."")\n      self.text_to_tensor(input_fnames, vocab_fname, tensor_fname, char_fname, max_word_length)\n\n    print(""Loading vocab..."")\n    all_data = load(tensor_fname)\n    all_data_char = load(char_fname)\n    self.idx2word, self.word2idx, self.idx2char, self.char2idx = load(vocab_fname)\n    vocab_size = len(self.idx2word)\n\n    print(""Word vocab size: %d, Char vocab size: %d"" % (len(self.idx2word), len(self.idx2char)))\n    self.max_word_length = all_data_char[0].shape[1]\n    self.sizes = []\n    self.all_batches = []\n\n    print(""Reshaping tensors..."")\n    for split, data in enumerate(all_data): # split = 0:train, 1:valid, 2:test\n      length = data.shape[0]\n      #if length % (batch_size * seq_length) != 0 and split < 2:\n      data = data[: batch_size * seq_length * math.floor(length / (batch_size * seq_length))]\n      ydata = np.zeros_like(data)\n      ydata[:-1] = data[1:].copy()\n      ydata[-1] = data[0].copy()\n      data_char = np.zeros([data.shape[0], self.max_word_length])\n\n      for idx in xrange(data.shape[0]):\n        data_char[idx] = all_data_char[split][idx]\n\n      if split < 2:\n        x_batches = list(data.reshape([-1, batch_size, seq_length]))\n        y_batches = list(ydata.reshape([-1, batch_size, seq_length]))\n        x_char_batches = list(data_char.reshape([-1, batch_size, seq_length, self.max_word_length]))\n        self.sizes.append(len(x_batches))\n      else:\n        x_batches = list(data.reshape([-1, batch_size, seq_length]))\n        y_batches = list(ydata.reshape([-1, batch_size, seq_length]))\n        x_char_batches = list(data_char.reshape([-1, batch_size, seq_length, self.max_word_length]))\n        self.sizes.append(len(x_batches))\n        # x_batches = np.tile(data, (batch_size, 1))\n        # y_batches = np.tile(ydata, (batch_size, 1))\n        # x_char_batches = np.tile(data_char, (batch_size, 1)).reshape(batch_size, -1, data_char.shape[1])\n        # self.sizes.append(1)\n      self.all_batches.append([x_batches, y_batches, x_char_batches])\n\n    self.batch_idx = [0, 0, 0]\n    print(""data load done. Number of batches in train: %d, val: %d, test: %d"" \\\n        % (self.sizes[0], self.sizes[1], self.sizes[2]))\n\n  def next_batch(self, split_idx):\n    # cycle around to beginning\n    if self.batch_idx[split_idx] >= self.sizes[split_idx]:\n      self.batch_idx[split_idx] = 0\n    idx = self.batch_idx[split_idx]\n    self.batch_idx[split_idx] = self.batch_idx[split_idx] + 1\n    return self.all_batches[split_idx][0][idx], \\\n           self.all_batches[split_idx][1][idx], \\\n           self.all_batches[split_idx][2][idx]\n\n  def reset_batch_pointer(self, split_idx, batch_idx=None):\n    if batch_idx == None:\n      batch_idx = 0\n    self.batch_idx[split_idx] = batch_idx\n\n  def text_to_tensor(self, input_files, vocab_fname, tensor_fname, char_fname, max_word_length):\n    max_word_length_tmp = 0\n    counts = []\n\n    for input_file in input_files:\n      count = 0\n\n      with open(input_file) as f:\n        for line in f:\n          line = line.replace(\'<unk>\', \'|\')\n          line = line.replace(\'}\', \'\')\n          line = line.replace(\'{\', \'\')\n          for word in line.split():\n            max_word_length_tmp = max(max_word_length_tmp, len(word) + 2)\n            count += 1\n\n          count += 1 # for \\n\n      counts.append(count)\n\n    print(""After first pass of data, max word length is: %d"" % max_word_length_tmp)\n    print(""Token count: train %d, val %d, test %d"" % (counts[0], counts[1], counts[2]))\n\n    max_word_length = min(max_word_length_tmp, max_word_length)\n\n    char2idx = {\' \':0, \'{\': 1, \'}\': 2}\n    word2idx = {\'<unk>\': 0}\n    idx2char = [\' \', \'{\', \'}\']\n    idx2word = [\'<unk>\']\n\n    output_tensors = []\n    output_chars = []\n\n    for idx, input_file in enumerate(input_files):\n      count = 0\n\n      with open(input_file) as f:\n        output_tensor = np.ndarray(counts[idx])\n        output_char = np.ones([counts[idx], max_word_length])\n\n        word_num = 0\n        for line in f:\n          line = line.replace(\'<unk>\', \'|\')\n          line = line.replace(\'}\', \'\')\n          line = line.replace(\'{\', \'\')\n\n          for word in line.split() + [\'+\']:\n            chars = [char2idx[\'{\']]\n            if word[0] == \'|\' and len(word) > 1:\n              word = word[2:]\n              output_tensor[word_num] = word2idx[\'|\']\n            else:\n              if not word2idx.has_key(word):\n                idx2word.append(word)\n                word2idx[word] = len(idx2word) - 1\n              output_tensor[word_num] = word2idx[word]\n\n            for char in word:\n              if not char2idx.has_key(char):\n                idx2char.append(char)\n                char2idx[char] = len(idx2char) - 1\n              chars.append(char2idx[char])\n            chars.append(char2idx[\'}\'])\n\n            if len(chars) == max_word_length:\n              chars[-1] = char2idx[\'}\']\n\n            for idx in xrange(min(len(chars), max_word_length)):\n              output_char[word_num][idx] = chars[idx]\n            word_num += 1\n\n        output_tensors.append(output_tensor)\n        output_chars.append(output_char)\n\n    save(vocab_fname, [idx2word, word2idx, idx2char, char2idx])\n    save(tensor_fname, output_tensors)\n    save(char_fname, output_chars)\n'"
main.py,3,"b'import os\nimport numpy as np\nimport tensorflow as tf\n\nfrom models import LSTMTDNN\nfrom utils import *\n\nfrom utils import pp\n\nflags = tf.app.flags\nflags.DEFINE_integer(""epoch"", 25, ""Epoch to train [25]"")\nflags.DEFINE_integer(""word_embed_dim"", 650, ""The dimension of word embedding matrix [650]"")\nflags.DEFINE_integer(""char_embed_dim"", 15, ""The dimension of char embedding matrix [15]"")\nflags.DEFINE_integer(""max_word_length"", 65, ""The maximum length of word [65]"")\nflags.DEFINE_integer(""batch_size"", 100, ""The size of batch images [100]"")\nflags.DEFINE_integer(""seq_length"", 35, ""The # of timesteps to unroll for [35]"")\nflags.DEFINE_float(""learning_rate"", 1.0, ""Learning rate [1.0]"")\nflags.DEFINE_float(""decay"", 0.5, ""Decay of SGD [0.5]"")\nflags.DEFINE_float(""dropout_prob"", 0.5, ""Probability of dropout layer [0.5]"")\nflags.DEFINE_string(""feature_maps"", ""[50,100,150,200,200,200,200]"", ""The # of feature maps in CNN [50,100,150,200,200,200,200]"")\nflags.DEFINE_string(""kernels"", ""[1,2,3,4,5,6,7]"", ""The width of CNN kernels [1,2,3,4,5,6,7]"")\nflags.DEFINE_string(""model"", ""LSTMTDNN"", ""The type of model to train and test [LSTM, LSTMTDNN]"")\nflags.DEFINE_string(""data_dir"", ""data"", ""The name of data directory [data]"")\nflags.DEFINE_string(""dataset"", ""ptb"", ""The name of dataset [ptb]"")\nflags.DEFINE_string(""checkpoint_dir"", ""checkpoint"", ""Directory name to save the checkpoints [checkpoint]"")\nflags.DEFINE_boolean(""forward_only"", False, ""True for forward only, False for training [False]"")\nflags.DEFINE_boolean(""use_char"", True, ""Use character-level language model [True]"")\nflags.DEFINE_boolean(""use_word"", False, ""Use word-level language [False]"")\nFLAGS = flags.FLAGS\n\nmodel_dict = {\n  \'LSTM\': None,\n  \'LSTMTDNN\': LSTMTDNN,\n}\n\ndef main(_):\n  pp.pprint(flags.FLAGS.__flags)\n\n  if not os.path.exists(FLAGS.checkpoint_dir):\n    print("" [*] Creating checkpoint directory..."")\n    os.makedirs(FLAGS.checkpoint_dir)\n\n  with tf.Session() as sess:\n    model = model_dict[FLAGS.model](sess, checkpoint_dir=FLAGS.checkpoint_dir,\n                                    seq_length=FLAGS.seq_length,\n                                    word_embed_dim=FLAGS.word_embed_dim,\n                                    char_embed_dim=FLAGS.char_embed_dim,\n                                    feature_maps=eval(FLAGS.feature_maps),\n                                    kernels=eval(FLAGS.kernels),\n                                    batch_size=FLAGS.batch_size,\n                                    dropout_prob=FLAGS.dropout_prob,\n                                    max_word_length=FLAGS.max_word_length,\n                                    forward_only=FLAGS.forward_only,\n                                    dataset_name=FLAGS.dataset,\n                                    use_char=FLAGS.use_char,\n                                    use_word=FLAGS.use_word,\n                                    data_dir=FLAGS.data_dir)\n\n    if not FLAGS.forward_only:\n      model.run(FLAGS.epoch, FLAGS.learning_rate, FLAGS.decay)\n    else:\n      test_loss = model.test(2)\n      print("" [*] Test loss: %2.6f, perplexity: %2.6f"" % (test_loss, np.exp(test_loss)))\n\nif __name__ == \'__main__\':\n  tf.app.run()\n'"
utils.py,0,"b'import sys\nimport pprint\n\ntry:\n    xrange\nexcept NameError:\n    xrange = range\n\npp = pprint.PrettyPrinter()\n\ndef progress(progress, status=""""):\n    barLength = 20\n    if isinstance(progress, int):\n        progress = float(progress)\n    if not isinstance(progress, float):\n        progress = 0\n        status = ""error: progress var must be float\\r\\n""\n    if progress < 0:\n        progress = 0\n        status = ""Halt...\\r\\n""\n    if progress >= 1:\n        progress = 1\n        status = ""Finished.\\r\\n""\n    block = int(round(barLength*progress))\n    text = ""\\rPercent: [%s] %.2f%% | %s"" % (""#""*block + "" ""*(barLength-block), progress*100, status)\n    sys.stdout.write(text)\n    sys.stdout.flush()\n\n'"
models/LSTMTDNN.py,40,"b'import sys\nimport numpy as np\nimport tensorflow as tf\n\nfrom TDNN import TDNN\nfrom base import Model\n\nfrom utils import progress\nfrom batch_loader import BatchLoader\nfrom ops import conv2d, batch_norm, highway\n\nclass LSTMTDNN(Model):\n  """"""\n  Time-delayed Neural Network (cf. http://arxiv.org/abs/1508.06615v4)\n  """"""\n  def __init__(self, sess,\n               batch_size=100, rnn_size=650, layer_depth=2,\n               word_embed_dim=650, char_embed_dim=15,\n               feature_maps=[50, 100, 150, 200, 200, 200, 200],\n               kernels=[1,2,3,4,5,6,7], seq_length=35, max_word_length=65,\n               use_word=False, use_char=True, hsm=0, max_grad_norm=5,\n               highway_layers=2, dropout_prob=0.5, use_batch_norm=True,\n               checkpoint_dir=""checkpoint"", forward_only=False,\n               data_dir=""data"", dataset_name=""pdb"", use_progressbar=False):\n    """"""\n    Initialize the parameters for LSTM TDNN\n\n    Args:\n      rnn_size: the dimensionality of hidden layers\n      layer_depth: # of depth in LSTM layers\n      batch_size: size of batch per epoch\n      word_embed_dim: the dimensionality of word embeddings\n      char_embed_dim: the dimensionality of character embeddings\n      feature_maps: list of feature maps (for each kernel width)\n      kernels: list of kernel widths\n      seq_length: max length of a word\n      use_word: whether to use word embeddings or not\n      use_char: whether to use character embeddings or not\n      highway_layers: # of highway layers to use\n      dropout_prob: the probability of dropout\n      use_batch_norm: whether to use batch normalization or not\n      hsm: whether to use hierarchical softmax\n    """"""\n    self.sess = sess\n\n    self.batch_size = batch_size\n    self.seq_length = seq_length\n\n    # RNN\n    self.rnn_size = rnn_size\n    self.layer_depth = layer_depth\n\n    # CNN\n    self.use_word = use_word\n    self.use_char = use_char\n    self.word_embed_dim = word_embed_dim\n    self.char_embed_dim = char_embed_dim\n    self.feature_maps = feature_maps\n    self.kernels = kernels\n\n    # General\n    self.highway_layers = highway_layers\n    self.dropout_prob = dropout_prob\n    self.use_batch_norm = use_batch_norm\n\n    # Training\n    self.max_grad_norm = max_grad_norm\n    self.max_word_length = max_word_length\n    self.hsm = hsm\n\n    self.data_dir = data_dir\n    self.dataset_name = dataset_name\n    self.checkpoint_dir = checkpoint_dir\n\n    self.forward_only = forward_only\n    self.use_progressbar = use_progressbar\n\n    self.loader = BatchLoader(self.data_dir, self.dataset_name, self.batch_size, self.seq_length, self.max_word_length)\n    print(\'Word vocab size: %d, Char vocab size: %d, Max word length (incl. padding): %d\' % \\\n        (len(self.loader.idx2word), len(self.loader.idx2char), self.loader.max_word_length))\n\n    self.max_word_length = self.loader.max_word_length\n    self.char_vocab_size = len(self.loader.idx2char)\n    self.word_vocab_size = len(self.loader.idx2word)\n\n    # build LSTMTDNN model\n    self.prepare_model()\n\n    # load checkpoints\n    if self.forward_only == True:\n      if self.load(self.checkpoint_dir, self.dataset_name):\n        print(""[*] SUCCESS to load model for %s."" % self.dataset_name)\n      else:\n        print(""[!] Failed to load model for %s."" % self.dataset_name)\n        sys.exit(1)\n\n  def prepare_model(self):\n    with tf.variable_scope(""LSTMTDNN""):\n      self.char_inputs = []\n      self.word_inputs = []\n      self.cnn_outputs = []\n\n      if self.use_char:\n        char_W = tf.get_variable(""char_embed"",\n            [self.char_vocab_size, self.char_embed_dim])\n      if self.use_word:\n        word_W = tf.get_variable(""word_embed"",\n            [self.word_vocab_size, self.word_embed_dim])\n\n      with tf.variable_scope(""CNN"") as scope:\n        self.char_inputs = tf.placeholder(tf.int32, [self.batch_size, self.seq_length, self.max_word_length])\n        self.word_inputs = tf.placeholder(tf.int32, [self.batch_size, self.seq_length])\n\n        char_indices = tf.split(1, self.seq_length, self.char_inputs)\n        word_indices = tf.split(1, self.seq_length, tf.expand_dims(self.word_inputs, -1))\n\n        for idx in xrange(self.seq_length):\n          char_index = tf.reshape(char_indices[idx], [-1, self.max_word_length])\n          word_index = tf.reshape(word_indices[idx], [-1, 1])\n\n          if idx != 0:\n            scope.reuse_variables()\n\n          if self.use_char:\n            # [batch_size x word_max_length, char_embed]\n            char_embed = tf.nn.embedding_lookup(char_W, char_index)\n\n            char_cnn = TDNN(char_embed, self.char_embed_dim, self.feature_maps, self.kernels)\n\n            if self.use_word:\n              word_embed = tf.nn.embedding_lookup(word_W, word_index)\n              cnn_output = tf.concat(1, [char_cnn.output, tf.squeeze(word_embed, [1])])\n            else:\n              cnn_output = char_cnn.output\n          else:\n            cnn_output = tf.squeeze(tf.nn.embedding_lookup(word_W, word_index))\n\n          if self.use_batch_norm:\n            bn = batch_norm()\n            norm_output = bn(tf.expand_dims(tf.expand_dims(cnn_output, 1), 1))\n            cnn_output = tf.squeeze(norm_output)\n\n          if highway:\n            #cnn_output = highway(input_, input_dim_length, self.highway_layers, 0)\n            cnn_output = highway(cnn_output, cnn_output.get_shape()[1], self.highway_layers, 0)\n\n          self.cnn_outputs.append(cnn_output)\n\n      with tf.variable_scope(""LSTM"") as scope:\n        self.cell = tf.nn.rnn_cell.BasicLSTMCell(self.rnn_size)\n        self.stacked_cell = tf.nn.rnn_cell.MultiRNNCell([self.cell] * self.layer_depth)\n\n        outputs, _ = tf.nn.rnn(self.stacked_cell,\n                               self.cnn_outputs,\n                               dtype=tf.float32)\n\n        self.lstm_outputs = []\n        self.true_outputs = tf.placeholder(tf.int64,\n            [self.batch_size, self.seq_length])\n\n        loss = 0\n        true_outputs = tf.split(1, self.seq_length, self.true_outputs)\n\n        for idx, (top_h, true_output) in enumerate(zip(outputs, true_outputs)):\n          if self.dropout_prob > 0:\n            top_h = tf.nn.dropout(top_h, self.dropout_prob)\n\n          if self.hsm > 0:\n            self.lstm_outputs.append(top_h)\n          else:\n            if idx != 0:\n              scope.reuse_variables()\n            proj = tf.nn.rnn_cell._linear(top_h, self.word_vocab_size, 0)\n            self.lstm_outputs.append(proj)\n\n          loss += tf.nn.sparse_softmax_cross_entropy_with_logits(self.lstm_outputs[idx], tf.squeeze(true_output))\n\n        self.loss = tf.reduce_mean(loss) / self.seq_length\n\n        tf.scalar_summary(""loss"", self.loss)\n        tf.scalar_summary(""perplexity"", tf.exp(self.loss))\n\n  def train(self, epoch):\n    cost = 0\n    target = np.zeros([self.batch_size, self.seq_length]) \n\n    N = self.loader.sizes[0]\n    for idx in xrange(N):\n      target.fill(0)\n      x, y, x_char = self.loader.next_batch(0)\n      for b in xrange(self.batch_size):\n        for t, w in enumerate(y[b]):\n          target[b][t] = w\n\n      feed_dict = {\n          self.word_inputs: x,\n          self.char_inputs: x_char,\n          self.true_outputs: target,\n      }\n\n      _, loss, step, summary_str = self.sess.run(\n          [self.optim, self.loss, self.global_step, self.merged_summary], feed_dict=feed_dict)\n\n      self.writer.add_summary(summary_str, step)\n\n      if idx % 50 == 0:\n        if self.use_progressbar:\n          progress(idx/N, ""epoch: [%2d] [%4d/%4d] loss: %2.6f"" % (epoch, idx, N, loss))\n        else:\n          print(""epoch: [%2d] [%4d/%4d] loss: %2.6f"" % (epoch, idx, N, loss))\n\n      cost += loss\n    return cost / N\n\n  def test(self, split_idx, max_batches=None):\n    if split_idx == 1:\n      set_name = \'Valid\'\n    else:\n      set_name = \'Test\'\n\n    N = self.loader.sizes[split_idx]\n    if max_batches != None:\n      N = min(max_batches, N)\n\n    self.loader.reset_batch_pointer(split_idx)\n    target = np.zeros([self.batch_size, self.seq_length]) \n\n    cost = 0\n    for idx in xrange(N):\n      target.fill(0)\n\n      x, y, x_char = self.loader.next_batch(split_idx)\n      for b in xrange(self.batch_size):\n        for t, w in enumerate(y[b]):\n          target[b][t] = w\n\n      feed_dict = {\n          self.word_inputs: x,\n          self.char_inputs: x_char,\n          self.true_outputs: target,\n      }\n\n      loss = self.sess.run(self.loss, feed_dict=feed_dict)\n\n      if idx % 50 == 0:\n        if self.use_progressbar:\n          progress(idx/N, ""> %s: loss: %2.6f, perplexity: %2.6f"" % (set_name, loss, np.exp(loss)))\n        else:\n          print("" > %s: loss: %2.6f, perplexity: %2.6f"" % (set_name, loss, np.exp(loss)))\n\n      cost += loss\n\n    cost = cost / N\n    return cost\n\n  def run(self, epoch=25, \n          learning_rate=1, learning_rate_decay=0.5):\n    self.current_lr = learning_rate\n\n    self.lr = tf.Variable(learning_rate, trainable=False)\n    self.opt = tf.train.GradientDescentOptimizer(self.lr)\n    #self.opt = tf.train.AdamOptimizer(learning_rate, beta1=0.5).minimize(self.loss)\n\n    # clip gradients\n    params = tf.trainable_variables()\n    grads = []\n    for grad in tf.gradients(self.loss, params):\n      if grad is not None:\n        grads.append(tf.clip_by_norm(grad, self.max_grad_norm))\n      else:\n        grads.append(grad)\n\n    self.global_step = tf.Variable(0, name=""global_step"", trainable=False)\n    self.optim = self.opt.apply_gradients(zip(grads, params),\n                                          global_step=self.global_step)\n\n    # ready for train\n    tf.initialize_all_variables().run()\n\n    if self.load(self.checkpoint_dir, self.dataset_name):\n      print(""[*] SUCCESS to load model for %s."" % self.dataset_name)\n    else:\n      print(""[!] Failed to load model for %s."" % self.dataset_name)\n\n    self.saver = tf.train.Saver()\n    self.merged_summary = tf.merge_all_summaries()\n    self.writer = tf.train.SummaryWriter(""./logs"", self.sess.graph_def)\n\n    self.log_loss = []\n    self.log_perp = []\n\n    if not self.forward_only:\n      for idx in xrange(epoch):\n        train_loss = self.train(idx)\n        valid_loss = self.test(1)\n\n        # Logging\n        self.log_loss.append([train_loss, valid_loss])\n        self.log_perp.append([np.exp(train_loss), np.exp(valid_loss)])\n\n        state = {\n          \'perplexity\': np.exp(train_loss),\n          \'epoch\': idx,\n          \'learning_rate\': self.current_lr,\n          \'valid_perplexity\': np.exp(valid_loss)\n        }\n        print(state)\n\n        # Learning rate annealing\n        if len(self.log_loss) > 1 and self.log_loss[idx][1] > self.log_loss[idx-1][1] * 0.9999:\n          self.current_lr = self.current_lr * learning_rate_decay\n          self.lr.assign(self.current_lr).eval()\n        if self.current_lr < 1e-5: break\n\n        if idx % 2 == 0:\n          self.save(self.checkpoint_dir, self.dataset_name)\n\n    test_loss = self.test(2)\n    print(""[*] Test loss: %2.6f, perplexity: %2.6f"" % (test_loss, np.exp(test_loss)))\n'"
models/TDNN.py,4,"b'import tensorflow as tf\n\nfrom .ops import conv2d\nfrom base import Model\n\nclass TDNN(Model):\n  """"""Time-delayed Nueral Network (cf. http://arxiv.org/abs/1508.06615v4)\n  """"""\n  def __init__(self, input_, embed_dim=650,\n               feature_maps=[50, 100, 150, 200, 200, 200, 200],\n               kernels=[1,2,3,4,5,6,7], checkpoint_dir=""checkpoint"",\n               forward_only=False):\n    """"""Initialize the parameters for TDNN\n\n    Args:\n      embed_dim: the dimensionality of the inputs\n      feature_maps: list of feature maps (for each kernel width)\n      kernels: list of # of kernels (width)\n    """"""\n    self.embed_dim = embed_dim\n    self.feature_maps = feature_maps\n    self.kernels = kernels\n\n    # [batch_size x seq_length x embed_dim x 1]\n    input_ = tf.expand_dims(input_, -1)\n\n    layers = []\n    for idx, kernel_dim in enumerate(kernels):\n      reduced_length = input_.get_shape()[1] - kernel_dim + 1\n\n      # [batch_size x seq_length x embed_dim x feature_map_dim]\n      conv = conv2d(input_, feature_maps[idx], kernel_dim , self.embed_dim,\n                    name=""kernel%d"" % idx)\n\n      # [batch_size x 1 x 1 x feature_map_dim]\n      pool = tf.nn.max_pool(tf.tanh(conv), [1, reduced_length, 1, 1], [1, 1, 1, 1], \'VALID\')\n\n      layers.append(tf.squeeze(pool))\n\n    if len(kernels) > 1:\n      self.output = tf.concat(1, layers)\n    else:\n      self.output = layers[0]\n'"
models/__init__.py,0,b'from TDNN import TDNN\nfrom LSTMTDNN import LSTMTDNN\n'
models/base.py,3,"b'import os\nfrom glob import glob\nimport tensorflow as tf\n\nclass Model(object):\n  """"""Abstract object representing an Reader model.""""""\n  def __init__(self):\n    self.vocab = None\n    self.data = None\n\n  def save(self, checkpoint_dir, dataset_name):\n    self.saver = tf.train.Saver()\n\n    print("" [*] Saving checkpoints..."")\n    model_name = type(self).__name__ or ""Reader""\n    if self.batch_size:\n      model_dir = ""%s_%s"" % (dataset_name, self.batch_size)\n    else:\n      model_dir = dataset_name\n\n    checkpoint_dir = os.path.join(checkpoint_dir, model_dir)\n    if not os.path.exists(checkpoint_dir):\n      os.makedirs(checkpoint_dir)\n    self.saver.save(self.sess, os.path.join(checkpoint_dir, model_name))\n\n  def load(self, checkpoint_dir, dataset_name):\n    self.saver = tf.train.Saver()\n\n    print("" [*] Loading checkpoints..."")\n    if self.batch_size:\n      model_dir = ""%s_%s"" % (dataset_name, self.batch_size)\n    else:\n      model_dir = dataset_name\n    checkpoint_dir = os.path.join(checkpoint_dir, model_dir)\n\n    ckpt = tf.train.get_checkpoint_state(checkpoint_dir)\n    if ckpt and ckpt.model_checkpoint_path:\n      ckpt_name = os.path.basename(ckpt.model_checkpoint_path)\n      self.saver.restore(self.sess, os.path.join(checkpoint_dir, ckpt_name))\n      return True\n    else:\n      return False\n'"
models/ops.py,17,"b'import math\nimport numpy as np\nimport tensorflow as tf\n\nfrom tensorflow.python.framework import ops\n\nfrom utils import *\n\ndef conv2d(input_, output_dim, k_h, k_w,\n           stddev=0.02, name=""conv2d""):\n  with tf.variable_scope(name):\n    w = tf.get_variable(\'w\', [k_h, k_w, input_.get_shape()[-1], output_dim],\n              initializer=tf.truncated_normal_initializer(stddev=stddev))\n    conv = tf.nn.conv2d(input_, w, strides=[1, 1, 1, 1], padding=\'VALID\')\n    return conv\n\ndef highway(input_, size, layer_size=1, bias=-2, f=tf.nn.relu):\n  """"""Highway Network (cf. http://arxiv.org/abs/1505.00387).\n  \n  t = sigmoid(Wy + b)\n  z = t * g(Wy + b) + (1 - t) * y\n\n  where g is nonlinearity, t is transform gate, and (1 - t) is carry gate.\n  """"""\n  output = input_\n  for idx in xrange(layer_size):\n    output = f(tf.nn.rnn_cell._linear(output, size, 0, scope=\'output_lin_%d\' % idx))\n\n    transform_gate = tf.sigmoid(\n        tf.nn.rnn_cell._linear(input_, size, 0, scope=\'transform_lin_%d\' % idx) + bias)\n    carry_gate = 1. - transform_gate\n\n    output = transform_gate * output + carry_gate * input_\n\n  return output\n\nclass batch_norm(object):\n  """"""Code modification of http://stackoverflow.com/a/33950177""""""\n  def __init__(self, epsilon=1e-5, momentum = 0.1, name=""batch_norm""):\n    with tf.variable_scope(name) as scope:\n      self.epsilon = epsilon\n      self.momentum = momentum\n\n      self.ema = tf.train.ExponentialMovingAverage(decay=self.momentum)\n      self.name=name\n\n  def __call__(self, x, train=True):\n    shape = x.get_shape().as_list()\n\n    with tf.variable_scope(self.name) as scope:\n      self.gamma = tf.get_variable(""gamma"", [shape[-1]],\n          initializer=tf.random_normal_initializer(1., 0.02))\n      self.beta = tf.get_variable(""beta"", [shape[-1]],\n          initializer=tf.constant_initializer(0.))\n\n      mean, variance = tf.nn.moments(x, [0, 1, 2])\n\n      return tf.nn.batch_norm_with_global_normalization(\n        x, mean, variance, self.beta, self.gamma, self.epsilon,\n        scale_after_normalization=True)\n'"
