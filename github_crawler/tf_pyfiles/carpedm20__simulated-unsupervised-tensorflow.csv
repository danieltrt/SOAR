file_path,api_count,code
buffer.py,0,"b'import numpy as np\n\nclass Buffer(object):\n  def __init__(self, config, rng):\n    self.rng = rng\n    self.buffer_size = config.buffer_size\n    self.batch_size = config.batch_size\n\n    image_dims = [\n        config.input_height,\n        config.input_width,\n        config.input_channel,\n    ]\n\n    self.idx = 0\n    self.data = np.zeros([self.buffer_size] + image_dims)\n\n  def push(self, batches):\n    batch_size = len(batches)\n    if self.idx + batch_size > self.buffer_size:\n      random_idx1 = self.rng.choice(self.idx, self.batch_size/2)\n      random_idx2 = self.rng.choice(batch_size, self.batch_size/2)\n      self.data[random_idx1] = batches[random_idx2]\n    else:\n      self.data[self.idx:self.idx+batch_size] = batches\n      self.idx += batch_size\n\n  def sample(self, n=None):\n    assert self.idx > n, ""not enough data is pushed""\n    if n is None:\n      n = self.batch_size/2\n    random_idx = self.rng.choice(self.idx, n)\n    return self.data[random_idx]\n'"
config.py,0,"b'#-*- coding: utf-8 -*-\nimport argparse\n\ndef str2bool(v):\n  return v.lower() in (\'true\', \'1\')\n\narg_lists = []\nparser = argparse.ArgumentParser()\n\ndef add_argument_group(name):\n  arg = parser.add_argument_group(name)\n  arg_lists.append(arg)\n  return arg\n\n# Network\nnet_arg = add_argument_group(\'Network\')\nnet_arg.add_argument(\'--kernel_dims\', type=eval, default=\'[]\', help=\'\')\nnet_arg.add_argument(\'--stride_size\', type=eval, default=\'[]\', help=\'\')\nnet_arg.add_argument(\'--channel_dims\', type=eval, default=\'[]\', help=\'\')\n\n# Data\ndata_arg = add_argument_group(\'Data\')\ndata_arg.add_argument(\'--data_set\', type=str, default=\'gaze\')\ndata_arg.add_argument(\'--data_dir\', type=str, default=\'data\')\ndata_arg.add_argument(\'--input_height\', type=int, default=35)\ndata_arg.add_argument(\'--input_width\', type=int, default=55)\ndata_arg.add_argument(\'--input_channel\', type=int, default=1)\ndata_arg.add_argument(\'--max_synthetic_num\', type=int, default=-1)\ndata_arg.add_argument(\'--real_image_dir\', type=str, default=""MPIIGaze"")\ndata_arg.add_argument(\'--synthetic_image_dir\', type=str, default=""UnityEyes"")\n\n# Training / test parameters\ntrain_arg = add_argument_group(\'Training\')\ntrain_arg.add_argument(\'--task\', type=str, default=\'generative\', \n                       choices=[\'generative\', \'estimation\'], help=\'\')\ntrain_arg.add_argument(\'--is_train\', type=str2bool, default=True, help=\'\')\ntrain_arg.add_argument(\'--max_step\', type=int, default=10000, help=\'\')\ntrain_arg.add_argument(\'--reg_scale\', type=float, default=0.5, help=\'\')\ntrain_arg.add_argument(\'--initial_K_d\', type=int, default=200, help=\'\')\ntrain_arg.add_argument(\'--initial_K_g\', type=int, default=1000, help=\'\')\ntrain_arg.add_argument(\'--K_d\', type=int, default=1, help=\'\')\ntrain_arg.add_argument(\'--K_g\', type=int, default=2, help=\'\')\ntrain_arg.add_argument(\'--batch_size\', type=int, default=512, help=\'\')\ntrain_arg.add_argument(\'--buffer_size\', type=int, default=25600, help=\'\')\ntrain_arg.add_argument(\'--num_epochs\', type=int, default=12, help=\'\')\ntrain_arg.add_argument(\'--random_seed\', type=int, default=123, help=\'\')\ntrain_arg.add_argument(\'--learning_rate\', type=float, default=0.001, help=\'\')\ntrain_arg.add_argument(\'--checkpoint_secs\', type=int, default=300, help=\'\')\ntrain_arg.add_argument(\'--max_grad_norm\', type=float, default=50, help=\'\')\ntrain_arg.add_argument(\'--optimizer\', type=str, default=\'adam\', choices=[\'adam\', \'sgd\'], help=\'\')\n\n# Misc\nmisc_arg = add_argument_group(\'Misc\')\nmisc_arg.add_argument(\'--log_step\', type=int, default=20, help=\'\')\nmisc_arg.add_argument(\'--log_dir\', type=str, default=\'logs\')\nmisc_arg.add_argument(\'--sample_dir\', type=str, default=\'samples\')\nmisc_arg.add_argument(\'--output_dir\', type=str, default=\'outputs\')\nmisc_arg.add_argument(\'--load_path\', type=str, default=\'\')\nmisc_arg.add_argument(\'--debug\', type=str2bool, default=False)\nmisc_arg.add_argument(\'--gpu_memory_fraction\', type=float, default=1.0)\nmisc_arg.add_argument(\'--max_image_summary\', type=int, default=7)\nmisc_arg.add_argument(\'--sample_image_grid\', type=eval, default=\'[8, 8]\')\n\ndef get_config():\n  config, unparsed = parser.parse_known_args()\n  return config, unparsed\n'"
layers.py,14,"b'import tensorflow as tf\nimport tensorflow.contrib.slim as slim\nfrom tensorflow.contrib.framework import add_arg_scope\n\nSE_loss = tf.nn.sparse_softmax_cross_entropy_with_logits\n\ndef int_shape(x):\n  return list(map(int, x.get_shape()[1: ]))\n\ndef normalize(layer):\n  return layer/127.5 - 1.\n\ndef denormalize(layer):\n  return (layer + 1.)/2.\n\ndef _update_dict(layer_dict, scope, layer):\n  name = ""{}/{}"".format(tf.get_variable_scope().name, scope)\n  layer_dict[name] = layer\n\ndef image_from_paths(paths, shape, is_grayscale=True, seed=None):\n  filename_queue = tf.train.string_input_producer(list(paths), shuffle=False, seed=seed)\n  reader = tf.WholeFileReader()\n  filename, data = reader.read(filename_queue)\n  image = tf.image.decode_png(data, channels=3, dtype=tf.uint8)\n  if is_grayscale:\n    image = tf.image.rgb_to_grayscale(image)\n  image.set_shape(shape)\n  return filename, tf.to_float(image)\n\n@add_arg_scope\ndef resnet_block(\n    inputs, scope, num_outputs=64, kernel_size=[3, 3],\n    stride=[1, 1], padding=""SAME"", layer_dict={}):\n  with tf.variable_scope(scope):\n    layer = conv2d(\n        inputs, num_outputs, kernel_size, stride,\n        padding=padding, activation_fn=tf.nn.relu, scope=""conv1"")\n    layer = conv2d(\n        layer, num_outputs, kernel_size, stride,\n        padding=padding, activation_fn=None, scope=""conv2"")\n    outputs = tf.nn.relu(tf.add(inputs, layer))\n  _update_dict(layer_dict, scope, outputs)\n  return outputs\n\n@add_arg_scope\ndef repeat(inputs, repetitions, layer, layer_dict={}, **kargv):\n  outputs = slim.repeat(inputs, repetitions, layer, **kargv)\n  _update_dict(layer_dict, kargv[\'scope\'], outputs)\n  return outputs\n\n@add_arg_scope\ndef conv2d(inputs, num_outputs, kernel_size, stride,\n           layer_dict={}, activation_fn=None,\n           #weights_initializer=tf.random_normal_initializer(0, 0.001),\n           weights_initializer=tf.contrib.layers.xavier_initializer(),\n           scope=None, name="""", **kargv):\n  outputs = slim.conv2d(\n      inputs, num_outputs, kernel_size,\n      stride, activation_fn=activation_fn, \n      weights_initializer=weights_initializer,\n      biases_initializer=tf.zeros_initializer(dtype=tf.float32), scope=scope, **kargv)\n  if name:\n    scope = ""{}/{}"".format(name, scope)\n  _update_dict(layer_dict, scope, outputs)\n  return outputs\n\n@add_arg_scope\ndef max_pool2d(inputs, kernel_size=[3, 3], stride=[1, 1],\n               layer_dict={}, scope=None, name="""", **kargv):\n  outputs = slim.max_pool2d(inputs, kernel_size, stride, **kargv)\n  if name:\n    scope = ""{}/{}"".format(name, scope)\n  _update_dict(layer_dict, scope, outputs)\n  return outputs\n\n@add_arg_scope\ndef tanh(inputs, layer_dict={}, name=None, **kargv):\n  outputs = tf.nn.tanh(inputs, name=name, **kargv)\n  _update_dict(layer_dict, name, outputs)\n  return outputs\n'"
main.py,2,"b'import sys\nimport numpy as np\nimport tensorflow as tf\n\nfrom trainer import Trainer\nfrom config import get_config\nfrom utils import prepare_dirs, save_config\n\nconfig = None\n\ndef main(_):\n  prepare_dirs(config)\n\n  rng = np.random.RandomState(config.random_seed)\n  tf.set_random_seed(config.random_seed)\n\n  trainer = Trainer(config, rng)\n  save_config(config.model_dir, config)\n\n  if config.is_train:\n    trainer.train()\n  else:\n    if not config.load_path:\n      raise Exception(""[!] You should specify `load_path` to load a pretrained model"")\n    trainer.test()\n\nif __name__ == ""__main__"":\n  config, unparsed = get_config()\n  tf.app.run(main=main, argv=[sys.argv[0]] + unparsed)\n'"
model.py,69,"b'import tensorflow as tf\nfrom tensorflow.contrib.framework import arg_scope\n\nfrom layers import *\nfrom utils import show_all_variables\n\nclass Model(object):\n  def __init__(self, config, data_loader):\n    self.data_loader = data_loader\n\n    self.task = config.task\n    self.debug = config.debug\n    self.config = config\n\n    self.input_height = config.input_height\n    self.input_width = config.input_width\n    self.input_channel = config.input_channel\n\n    self.reg_scale = config.reg_scale\n    self.learning_rate = config.learning_rate\n    self.max_grad_norm = config.max_grad_norm\n    self.batch_size = config.batch_size\n\n    self.layer_dict = {}\n\n    self._build_placeholders()\n    self._build_model()\n    self._build_steps()\n    self._build_optim()\n\n    show_all_variables()\n\n  def _build_placeholders(self):\n    image_dims = [self.input_height, self.input_width, self.input_channel]\n\n    min_after_dequeue = 5000\n    capacity = min_after_dequeue + 3 * self.batch_size\n\n    self.synthetic_batch_size = tf.placeholder(tf.int32, [], ""synthetic_batch_size"")\n    self.synthetic_filenames, self.synthetic_images = \\\n        image_from_paths(self.data_loader.synthetic_data_paths,\n                         self.data_loader.synthetic_data_dims, seed=self.config.random_seed)\n\n    self.x_filename, self.x = tf.train.shuffle_batch(\n        [self.synthetic_filenames, self.synthetic_images],\n        batch_size=self.synthetic_batch_size,\n        num_threads=4, capacity=capacity,\n        min_after_dequeue=min_after_dequeue, name=\'synthetic_inputs\')\n\n    self.test_x_filename, self.test_x = tf.train.batch(\n        [self.synthetic_filenames, self.synthetic_images],\n        batch_size=self.synthetic_batch_size,\n        num_threads=1, capacity=capacity,\n        name=\'synthetic_test_inputs\')\n\n    if not self.config.is_train:\n      self.x_filename, self.x = \\\n          self.test_x_filename, self.test_x\n\n    self.y = tf.placeholder(\n        tf.uint8, [None, None, None, self.input_channel], name=\'real_inputs\')\n    self.R_x_history = tf.placeholder(\n        tf.float32, [None, None, None, self.input_channel], \'R_x_history\')\n\n    resize_dim = [self.input_height, self.input_width]\n    self.resized_x = tf.image.resize_images(self.x, resize_dim)\n    self.resized_y = tf.image.resize_images(self.y, resize_dim)\n    self.resized_test_x = tf.image.resize_images(self.test_x, resize_dim)\n\n    self.normalized_x = normalize(self.resized_x)\n    self.normalized_y = normalize(self.resized_y)\n\n    self.refiner_step = tf.Variable(0, name=\'refiner_step\', trainable=False)\n    self.discrim_step = tf.Variable(0, name=\'discrim_step\', trainable=False)\n\n  def _build_optim(self):\n    def minimize(loss, step, var_list):\n      if self.config.optimizer == ""sgd"":\n        optim = tf.train.GradientDescentOptimizer(self.learning_rate)\n      elif self.config.optimizer == ""adam"":\n        optim = tf.train.AdamOptimizer(self.learning_rate)\n      else:\n        raise Exception(""[!] Unkown optimizer: {}"".format(self.config.optimizer))\n\n      if self.max_grad_norm != None:\n        grads_and_vars = optim.compute_gradients(loss)\n        new_grads_and_vars = []\n        for idx, (grad, var) in enumerate(grads_and_vars):\n          if grad is not None and var in var_list:\n            new_grads_and_vars.append((tf.clip_by_norm(grad, self.max_grad_norm), var))\n        return optim.apply_gradients(new_grads_and_vars,\n                                     global_step=step)\n      else:\n        return optim.minimize(loss, global_step=step, var_list=var_list)\n\n    if self.task == ""generative"":\n      self.refiner_optim = minimize(\n          self.refiner_loss, self.refiner_step, self.refiner_vars)\n\n      self.discrim_optim = minimize(\n          self.discrim_loss, self.discrim_step, self.discrim_vars)\n\n      self.discrim_optim_with_history = minimize(\n          self.discrim_loss_with_history, self.discrim_step, self.discrim_vars)\n    elif self.task == ""estimate"":\n      raise Exception(""[!] Not implemented yet"")\n\n  def _build_model(self):\n    with arg_scope([resnet_block, conv2d, max_pool2d, tanh],\n                   layer_dict=self.layer_dict):\n      self.R_x = self._build_refiner(self.normalized_x)\n      self.denormalized_R_x = denormalize(self.R_x)\n\n      self.D_y, self.D_y_logits = \\\n          self._build_discrim(self.normalized_y, name=""D_y"")\n      self.D_R_x, self.D_R_x_logits = \\\n          self._build_discrim(self.R_x, name=""D_R_x"", reuse=True)\n      self.D_R_x_history, self.D_R_x_history_logits = \\\n          self._build_discrim(self.R_x_history,\n                              name=""D_R_x_history"", reuse=True)\n\n      #self.estimate_outputs = self._build_estimation_network()\n    self._build_loss()\n\n  def _build_loss(self):\n    # Refiner loss\n    def fake_label(layer):\n      return tf.zeros_like(layer, dtype=tf.int32)[:,:,:,0]\n\n    def real_label(layer):\n      return tf.ones_like(layer, dtype=tf.int32)[:,:,:,0]\n\n    def log_loss(logits, label, name):\n      return tf.reduce_sum(SE_loss(logits=logits, labels=label), [1, 2], name=name)\n\n    with tf.name_scope(""refiner""):\n      self.realism_loss = log_loss(\n          self.D_R_x_logits, real_label(self.D_R_x_logits), ""realism_loss"")\n      self.regularization_loss = \\\n          self.reg_scale * tf.reduce_sum(\n              tf.abs(self.R_x - self.normalized_x), [1, 2, 3],\n              name=""regularization_loss"")\n\n      self.refiner_loss = tf.reduce_mean(\n          self.realism_loss + self.regularization_loss,\n          name=""refiner_loss"")\n\n      if self.debug:\n        self.refiner_loss = tf.Print(\n            self.refiner_loss, [self.R_x], ""R_x"")\n        self.refiner_loss = tf.Print(\n            self.refiner_loss, [self.D_R_x], ""D_R_x"")\n        self.refiner_loss = tf.Print(\n            self.refiner_loss, [self.normalized_x], ""normalized_x"")\n        self.refiner_loss = tf.Print(\n            self.refiner_loss, [self.denormalized_R_x], ""denormalized_R_x"")\n        self.refiner_loss = tf.Print(\n            self.refiner_loss, [self.regularization_loss], ""reg_loss"")\n\n    self.refiner_summary = tf.summary.merge([\n        #tf.summary.image(""synthetic_images"",\n        #                 self.x, max_outputs=self.config.max_image_summary),\n        #tf.summary.image(""refined_images"",\n        #                 self.denormalized_R_x, max_outputs=self.config.max_image_summary),\n        tf.summary.scalar(""refiner/realism_loss"",\n                          tf.reduce_mean(self.realism_loss)),\n        tf.summary.scalar(""refiner/regularization_loss"",\n                          tf.reduce_mean(self.regularization_loss)),\n        tf.summary.scalar(""refiner/loss"",\n                          tf.reduce_mean(self.refiner_loss)),\n    ])\n\n    # Discriminator loss\n    with tf.name_scope(""discriminator""):\n      self.refiner_d_loss = log_loss(\n          self.D_R_x_logits, fake_label(self.D_R_x_logits), ""refiner_d_loss"")\n      self.synthetic_d_loss = log_loss(\n          self.D_y_logits, real_label(self.D_y_logits), ""synthetic_d_loss"")\n\n      self.discrim_loss = tf.reduce_mean(\n          self.refiner_d_loss + \\\n              self.synthetic_d_loss, name=""discrim_loss"")\n\n      # with history\n      self.refiner_d_loss_with_history = log_loss(\n          self.D_R_x_history_logits,\n          fake_label(self.D_R_x_history_logits),\n          ""refiner_d_loss_with_history"")\n      self.discrim_loss_with_history = tf.reduce_mean(\n          tf.concat([self.refiner_d_loss, self.refiner_d_loss_with_history], axis=0) + \\\n              self.synthetic_d_loss, name=""discrim_loss_with_history"")\n\n      if self.debug:\n        self.discrim_loss_with_history = tf.Print(\n            self.discrim_loss_with_history, [self.D_R_x_logits], ""D_R_x_logits"")\n        self.discrim_loss_with_history = tf.Print(\n            self.discrim_loss_with_history, [self.D_y_logits], ""D_y_logits"")\n        self.discrim_loss_with_history = tf.Print(\n            self.discrim_loss_with_history, [self.refiner_d_loss], ""refiner_d_loss"")\n        self.discrim_loss_with_history = tf.Print(\n            self.discrim_loss_with_history, [self.refiner_d_loss_with_history], ""refiner_d_loss_with_history"")\n        self.discrim_loss_with_history = tf.Print(\n            self.discrim_loss_with_history, [self.synthetic_d_loss], ""synthetic_d_loss"")\n        self.discrim_loss_with_history = tf.Print(\n            self.discrim_loss_with_history, [self.D_R_x_history_logits], ""D_R_x_history_logits"")\n        self.discrim_loss_with_history = tf.Print(\n            self.discrim_loss_with_history, [self.D_y_logits], ""D_y_logits"")\n\n      self.discrim_summary = tf.summary.merge([\n          #tf.summary.image(""real_images"",\n          #                  self.resized_y, max_outputs=self.config.max_image_summary),\n          tf.summary.scalar(""synthetic_d_loss"",\n                            tf.reduce_mean(self.synthetic_d_loss)),\n          tf.summary.scalar(""refiner_d_loss"",\n                            tf.reduce_mean(self.refiner_d_loss)),\n          tf.summary.scalar(""discrim_loss"",\n                            tf.reduce_mean(self.discrim_loss)),\n      ])\n      self.discrim_summary_with_history = tf.summary.merge([\n          #tf.summary.image(""real_images"",\n          #                 self.resized_y, max_outputs=self.config.max_image_summary),\n          tf.summary.scalar(""synthetic_d_loss"",\n                            tf.reduce_mean(self.synthetic_d_loss)),\n          tf.summary.scalar(""refiner_d_loss_with_history"",\n                            tf.reduce_mean(self.refiner_d_loss_with_history)),\n          tf.summary.scalar(""discrim_loss_with_history"",\n                            tf.reduce_mean(self.discrim_loss_with_history)),\n      ])\n\n  def _build_steps(self):\n    def run(sess, feed_dict, fetch,\n            summary_op, summary_writer, output_op=None):\n      if summary_writer is not None:\n        fetch[\'summary\'] = summary_op\n      if output_op is not None:\n        fetch[\'output\'] = output_op\n\n      result = sess.run(fetch, feed_dict=feed_dict)\n      if result.has_key(\'summary\'):\n        summary_writer.add_summary(result[\'summary\'], result[\'step\'])\n        summary_writer.flush()\n      return result\n\n    def train_refiner(sess, feed_dict, summary_writer=None, with_output=False):\n      fetch = {\n          \'loss\': self.refiner_loss,\n          \'optim\': self.refiner_optim,\n          \'step\': self.refiner_step,\n      }\n      return run(sess, feed_dict, fetch,\n                 self.refiner_summary, summary_writer,\n                 output_op=self.R_x if with_output else None)\n\n    def test_refiner(sess, feed_dict, summary_writer=None, with_output=False):\n      fetch = {\n          \'filename\': self.x_filename,\n          \'loss\': self.refiner_loss,\n          \'step\': self.refiner_step,\n      }\n      return run(sess, feed_dict, fetch,\n                 self.refiner_summary, summary_writer,\n                 output_op=self.R_x if with_output else None)\n\n    def train_discrim(sess, feed_dict, summary_writer=None,\n                      with_history=False, with_output=False):\n      fetch = {\n          \'loss\': self.discrim_loss_with_history,\n          \'optim\': self.discrim_optim_with_history,\n          \'step\': self.discrim_step,\n      }\n      return run(sess, feed_dict, fetch,\n                 self.discrim_summary_with_history if with_history \\\n                     else self.discrim_summary, summary_writer,\n                 output_op=self.D_R_x if with_output else None)\n\n    def test_discrim(sess, feed_dict, summary_writer=None,\n                     with_history=False, with_output=False):\n      fetch = {\n          \'loss\': self.discrim_loss,\n          \'step\': self.discrim_step,\n      }\n      return run(sess, feed_dict, fetch,\n                 self.discrim_summary_with_history if with_history \\\n                     else self.discrim_summary, summary_writer,\n                 output_op=self.D_R_x if with_output else None)\n\n    self.train_refiner = train_refiner\n    self.test_refiner = test_refiner\n    self.train_discrim = train_discrim\n    self.test_discrim = test_discrim\n\n  def _build_refiner(self, layer):\n    with tf.variable_scope(""refiner"") as sc:\n      layer = conv2d(layer, 64, 3, 1, scope=""conv_1"")\n      layer = repeat(layer, 4, resnet_block, scope=""resnet"")\n      layer = conv2d(layer, 1, 1, 1, \n                     activation_fn=None, scope=""conv_2"")\n      output = tanh(layer, name=""tanh"")\n      self.refiner_vars = tf.contrib.framework.get_variables(sc)\n    return output \n\n  def _build_discrim(self, layer, name, reuse=False):\n    with tf.variable_scope(""discriminator"", reuse=reuse) as sc:\n      layer = conv2d(layer, 96, 3, 2, scope=""conv_1"", name=name)\n      layer = conv2d(layer, 64, 3, 2, scope=""conv_2"", name=name)\n      layer = max_pool2d(layer, 3, 1, scope=""max_1"", name=name)\n      layer = conv2d(layer, 32, 3, 1, scope=""conv_3"", name=name)\n      layer = conv2d(layer, 32, 1, 1, scope=""conv_4"", name=name)\n      logits = conv2d(layer, 2, 1, 1, scope=""conv_5"", name=name)\n      output = tf.nn.softmax(logits, name=""softmax"")\n      self.discrim_vars = tf.contrib.framework.get_variables(sc)\n    return output, logits\n\n  def _build_estimation_network(self):\n    layer = self.normalized_x\n    with tf.variable_scope(""estimation""):\n      layer = conv2d(layer, 96, 3, 2, scope=""conv_1"")\n      layer = conv2d(layer, 64, 3, 2, scope=""conv_2"")\n      layer = max_pool2d(layer, 64, 3, scope=""max_1"")\n      layer = conv2d(layer, 32, 3, 1, scope=""conv_3"")\n      layer = conv2d(layer, 32, 1, 1, scope=""conv_4"")\n      layer = conv2d(layer, 2, 1, 1, activation_fn=slim.softmax)\n    return layer\n'"
trainer.py,7,"b'import os\nimport numpy as np\nfrom tqdm import trange\nimport tensorflow as tf\nfrom tensorflow.contrib.framework.python.ops import arg_scope\n\nfrom model import Model\nfrom buffer import Buffer\nimport data.gaze_data as gaze_data\nimport data.hand_data as hand_data\nfrom utils import imwrite, imread, img_tile\n\nclass Trainer(object):\n  def __init__(self, config, rng):\n    self.config = config\n    self.rng = rng\n\n    self.task = config.task\n    self.model_dir = config.model_dir\n    self.gpu_memory_fraction = config.gpu_memory_fraction\n\n    self.log_step = config.log_step\n    self.max_step = config.max_step\n\n    self.K_d = config.K_d\n    self.K_g = config.K_g\n    self.initial_K_d = config.initial_K_d\n    self.initial_K_g = config.initial_K_g\n    self.checkpoint_secs = config.checkpoint_secs\n\n    DataLoader = {\n        \'gaze\': gaze_data.DataLoader,\n        \'hand\': hand_data.DataLoader,\n    }[config.data_set]\n    self.data_loader = DataLoader(config, rng=self.rng)\n\n    self.model = Model(config, self.data_loader)\n    self.history_buffer = Buffer(config, self.rng)\n\n    self.summary_ops = {\n        \'test_synthetic_images\': {\n            \'summary\': tf.summary.image(""test_synthetic_images"",\n                                        self.model.resized_x,\n                                        max_outputs=config.max_image_summary),\n            \'output\': self.model.resized_x,\n        },\n        \'test_refined_images\': {\n            \'summary\': tf.summary.image(""test_refined_images"",\n                                        self.model.denormalized_R_x,\n                                        max_outputs=config.max_image_summary),\n            \'output\': self.model.denormalized_R_x,\n        }\n    }\n\n    self.saver = tf.train.Saver()\n    self.summary_writer = tf.summary.FileWriter(self.model_dir)\n\n    sv = tf.train.Supervisor(logdir=self.model_dir,\n                             is_chief=True,\n                             saver=self.saver,\n                             summary_op=None,\n                             summary_writer=self.summary_writer,\n                             save_summaries_secs=300,\n                             save_model_secs=self.checkpoint_secs,\n                             global_step=self.model.discrim_step)\n\n    gpu_options = tf.GPUOptions(\n        per_process_gpu_memory_fraction=self.gpu_memory_fraction,\n        allow_growth=True) # seems to be not working\n    sess_config = tf.ConfigProto(allow_soft_placement=True,\n                                 gpu_options=gpu_options)\n\n    self.sess = sv.prepare_or_wait_for_session(config=sess_config)\n\n  def train(self):\n    print(""[*] Training starts..."")\n    self._summary_writer = None\n\n    sample_num = reduce(lambda x, y: x*y, self.config.sample_image_grid)\n    idxs = self.rng.choice(len(self.data_loader.synthetic_data_paths), sample_num)\n    test_samples = np.expand_dims(np.stack(\n        [imread(path) for path in \\\n            self.data_loader.synthetic_data_paths[idxs]]\n    ), -1)\n\n    def train_refiner(push_buffer=False):\n      feed_dict = {\n        self.model.synthetic_batch_size: self.data_loader.batch_size,\n      }\n      res = self.model.train_refiner(\n          self.sess, feed_dict, self._summary_writer, with_output=True)\n      self._summary_writer = self._get_summary_writer(res)\n\n      if push_buffer:\n        self.history_buffer.push(res[\'output\'])\n\n      if res[\'step\'] % self.log_step == 0:\n        feed_dict = {\n            self.model.x: test_samples,\n        }\n        self._inject_summary(\n          \'test_refined_images\', feed_dict, res[\'step\'])\n\n        if res[\'step\'] / float(self.log_step) == 1.:\n          self._inject_summary(\n              \'test_synthetic_images\', feed_dict, res[\'step\'])\n\n    def train_discrim():\n      feed_dict = {\n        self.model.synthetic_batch_size: self.data_loader.batch_size/2,\n        self.model.R_x_history: self.history_buffer.sample(),\n        self.model.y: self.data_loader.next(),\n      }\n      res = self.model.train_discrim(\n          self.sess, feed_dict, self._summary_writer, with_history=True, with_output=False)\n      self._summary_writer = self._get_summary_writer(res)\n\n    for k in trange(self.initial_K_g, desc=""Train refiner""):\n      train_refiner(push_buffer=k > self.initial_K_g * 0.9)\n\n    for k in trange(self.initial_K_d, desc=""Train discrim""):\n      train_discrim()\n\n    for step in trange(self.max_step, desc=""Train both""):\n      for k in xrange(self.K_g):\n        train_refiner(push_buffer=True)\n\n      for k in xrange(self.K_d):\n        train_discrim()\n\n  def test(self):\n    batch_size = self.data_loader.batch_size\n    num_epoch = len(self.data_loader.synthetic_data_paths) / batch_size\n\n    for idx in trange(num_epoch, desc=""Refine all synthetic images""):\n      feed_dict = {\n        self.model.synthetic_batch_size: batch_size,\n      }\n      res = self.model.test_refiner(\n          self.sess, feed_dict, None, with_output=True)\n\n      for image, filename in zip(res[\'output\'], res[\'filename\']):\n        basename = os.path.basename(filename).replace(""_cropped"", ""_refined"")\n        path = os.path.join(self.config.output_model_dir, basename)\n        imwrite(path, image[:,:,0])\n\n  def _inject_summary(self, tag, feed_dict, step):\n    summaries = self.sess.run(self.summary_ops[tag], feed_dict)\n    self.summary_writer.add_summary(summaries[\'summary\'], step)\n\n    path = os.path.join(\n        self.config.sample_model_dir, ""{}.png"".format(step))\n    imwrite(path, img_tile(summaries[\'output\'],\n            tile_shape=self.config.sample_image_grid)[:,:,0])\n\n  def _get_summary_writer(self, result):\n    if result[\'step\'] % self.log_step == 0:\n      return self.summary_writer\n    else:\n      return None\n'"
utils.py,1,"b'import os\nimport json\nimport numpy as np\nfrom datetime import datetime\n\nimport tensorflow as tf\nimport tensorflow.contrib.slim as slim\n\ntry:\n  import scipy.misc\n  imread = scipy.misc.imread\n  imresize = scipy.misc.imresize\n  imwrite = scipy.misc.imsave\nexcept:\n  import cv2\n  imread = cv2.imread\n  imresize = cv2.resize\n  imwrite = cv2.imwrite\n\nimport scipy.io as sio\nloadmat = sio.loadmat\n\ndef prepare_dirs(config):\n  if config.load_path:\n    if config.load_path.startswith(config.task):\n      config.model_name = config.load_path\n    else:\n      config.model_name = ""{}_{}"".format(config.task, config.load_path)\n  else:\n    config.model_name = ""{}_{}"".format(config.task, get_time())\n\n  config.model_dir = os.path.join(config.log_dir, config.model_name)\n  config.sample_model_dir = os.path.join(config.sample_dir, config.model_name)\n  config.output_model_dir = os.path.join(config.output_dir, config.model_name)\n\n  for path in [config.log_dir, config.data_dir,\n               config.sample_dir, config.sample_model_dir,\n               config.output_dir, config.output_model_dir]:\n    if not os.path.exists(path):\n      os.makedirs(path)\n\ndef get_time():\n  return datetime.now().strftime(""%Y-%m-%d_%H-%M-%S"")\n\ndef show_all_variables():\n  model_vars = tf.trainable_variables()\n  slim.model_analyzer.analyze_vars(model_vars, print_info=True)\n\ndef img_tile(imgs, aspect_ratio=1.0, tile_shape=None, border=1,\n             border_color=0):\n  \'\'\' from https://github.com/openai/pixel-cnn/blob/master/pixel_cnn_pp/plotting.py\n  \'\'\'\n\n  if imgs.ndim != 3 and imgs.ndim != 4:\n    raise ValueError(\'imgs has wrong number of dimensions.\')\n  n_imgs = imgs.shape[0]\n\n  # Grid shape\n  img_shape = np.array(imgs.shape[1:3])\n  if tile_shape is None:\n    img_aspect_ratio = img_shape[1] / float(img_shape[0])\n    aspect_ratio *= img_aspect_ratio\n    tile_height = int(np.ceil(np.sqrt(n_imgs * aspect_ratio)))\n    tile_width = int(np.ceil(np.sqrt(n_imgs / aspect_ratio)))\n    grid_shape = np.array((tile_height, tile_width))\n  else:\n    assert len(tile_shape) == 2\n    grid_shape = np.array(tile_shape)\n\n  # Tile image shape\n  tile_img_shape = np.array(imgs.shape[1:])\n  tile_img_shape[:2] = (img_shape[:2] + border) * grid_shape[:2] - border\n\n  # Assemble tile image\n  tile_img = np.empty(tile_img_shape)\n  tile_img[:] = border_color\n  for i in range(grid_shape[0]):\n    for j in range(grid_shape[1]):\n      img_idx = j + i*grid_shape[1]\n      if img_idx >= n_imgs:\n        # No more images - stop filling out the grid.\n        break\n      img = imgs[img_idx]\n      yoff = (img_shape[0] + border) * i\n      xoff = (img_shape[1] + border) * j\n      tile_img[yoff:yoff+img_shape[0], xoff:xoff+img_shape[1], ...] = img\n\n  return tile_img\n\ndef save_config(model_dir, config):\n  param_path = os.path.join(model_dir, ""params.json"")\n\n  print(""[*] MODEL dir: %s"" % model_dir)\n  print(""[*] PARAM path: %s"" % param_path)\n\n  with open(param_path, \'w\') as fp:\n    json.dump(config.__dict__, fp,  indent=4, sort_keys=True)\n'"
data/__init__.py,0,b''
data/gaze_data.py,0,"b'import os\nimport sys\nimport json\nimport fnmatch\nimport tarfile\nfrom PIL import Image\nfrom glob import glob\nfrom tqdm import tqdm\nfrom six.moves import urllib\n\nimport numpy as np\n\nfrom utils import loadmat, imread, imwrite\n\nDATA_FNAME = \'gaze.npz\'\n\ndef save_array_to_grayscale_image(array, path):\n  Image.fromarray(array).convert(\'L\').save(path)\n\ndef process_json_list(json_list, img):\n  ldmks = [eval(s) for s in json_list]\n  return np.array([(x, img.shape[0]-y, z) for (x,y,z) in ldmks])\n\ndef maybe_download_and_extract(\n    config,\n    data_path,\n    url=\'http://datasets.d2.mpi-inf.mpg.de/MPIIGaze/MPIIGaze.tar.gz\'):\n  if not os.path.exists(os.path.join(data_path, config.real_image_dir)):\n    if not os.path.exists(data_path):\n      os.makedirs(data_path)\n\n    filename = os.path.basename(url)\n    filepath = os.path.join(data_path, filename)\n\n    if not os.path.exists(filepath):\n      def _progress(count, block_size, total_size):\n        sys.stdout.write(\'\\r>> Downloading %s %.1f%%\' % (filename,\n          float(count * block_size) / float(total_size) * 100.0))\n        sys.stdout.flush()\n\n      filepath, _ = urllib.request.urlretrieve(url, filepath, _progress)\n      statinfo = os.stat(filepath)\n      print(\'\\nSuccessfully downloaded {} {} bytes.\'.format(filename, statinfo.st_size))\n      tarfile.open(filepath, \'r:gz\').extractall(data_path)\n\ndef maybe_preprocess(config, data_path, sample_path=None):\n  if config.max_synthetic_num < 0:\n    max_synthetic_num = None\n  else:\n    max_synthetic_num = config.max_synthetic_num\n  \n  # MPIIGaze dataset\n  base_path = os.path.join(data_path, \'{}/Data/Normalized\'.format(config.real_image_dir))\n  npz_path = os.path.join(data_path, DATA_FNAME)\n\n  if not os.path.exists(npz_path):\n    mat_paths = []\n    for root, dirnames, filenames in os.walk(base_path):\n      for filename in fnmatch.filter(filenames, \'*.mat\'):\n        mat_paths.append(os.path.join(root, filename))\n\n    print(""[*] Preprocessing real `gaze` data..."")\n\n    real_images =[]\n    for mat_path in tqdm(mat_paths):\n      mat = loadmat(mat_path)\n      # Left eye (batch_size, height, width)\n      real_images.extend(mat[\'data\'][0][0][0][0][0][1])\n      # Right eye\n      real_images.extend(mat[\'data\'][0][0][1][0][0][1])\n\n    real_data = np.stack(real_images, axis=0)\n    np.savez(npz_path, real=real_data)\n\n  # UnityEyes dataset\n  synthetic_image_path_candidates = [\n      config.synthetic_image_dir,\n      os.path.join(data_path, config.synthetic_image_dir),\n  ]\n  for synthetic_image_path in synthetic_image_path_candidates:\n    jpg_paths = glob(os.path.join(synthetic_image_path, \'*.jpg\'))\n    cropped_jpg_paths = glob(os.path.join(synthetic_image_path, \'*_cropped.png\'))\n\n    if len(jpg_paths) == 0:\n      print(""[!] No images in ./{}. Skip."".format(synthetic_image_path))\n      continue\n    else:\n      print(""[!] Found images in ./{}."".format(synthetic_image_path))\n      if len(cropped_jpg_paths) != len(jpg_paths):\n        json_paths = glob(os.path.join(\n            data_path, \'{}/*.json\'.format(config.synthetic_image_dir)))\n\n        assert len(jpg_paths) >= max_synthetic_num, \\\n            ""[!] # of synthetic data ({}) is smaller than max_synthetic_num ({})"". \\\n                format(len(jpg_paths), max_synthetic_num)\n\n        json_paths = json_paths[:max_synthetic_num]\n        for json_path in tqdm(json_paths):\n          jpg_path = json_path.replace(\'json\', \'jpg\')\n\n          if not os.path.exists(jpg_path):\n            continue\n\n          with open(json_path) as json_f:\n            img = imread(jpg_path)\n            j = json.loads(json_f.read())\n\n            key = ""interior_margin_2d""\n            j[key] = process_json_list(j[key], img)\n\n            x_min, x_max = int(min(j[key][:,0])), int(max(j[key][:,0]))\n            y_min, y_max = int(min(j[key][:,1])), int(max(j[key][:,1]))\n\n            x_center, y_center = (x_min + x_max)/2, (y_min + y_max)/2\n\n            cropped_img = img[y_center-42: y_center+42, x_center-70:x_center+70]\n            img_path = jpg_path.replace("".jpg"", ""_cropped.png"")\n\n            save_array_to_grayscale_image(cropped_img, img_path)\n\n      jpg_paths = glob(os.path.join(synthetic_image_path, \'*.jpg\'))\n      cropped_jpg_paths = glob(os.path.join(synthetic_image_path, \'*_cropped.png\'))\n\n      print(""[*] # of synthetic data: {}, # of cropped_data: {}"". \\\n          format(len(jpg_paths), len(cropped_jpg_paths)))\n      print(""[*] Finished preprocessing synthetic `gaze` data."")\n\n      return synthetic_image_path\n\n  raise Exception(""[!] Failed to found proper synthetic_image_path in {}"" \\\n      .format(synthetic_image_path_candidates))\n\ndef load(config, data_path, sample_path, rng):\n  if not os.path.exists(data_path):\n    print(\'creating folder\', data_path)\n    os.makedirs(data_path)\n\n  maybe_download_and_extract(config, data_path)\n  synthetic_image_path = maybe_preprocess(config, data_path, sample_path)\n\n  gaze_data = np.load(os.path.join(data_path, DATA_FNAME))\n  real_data = gaze_data[\'real\']\n\n  if not os.path.exists(sample_path):\n    os.makedirs(sample_path)\n\n  print(""[*] Save samples images in {}"".format(data_path))\n  random_idxs = rng.choice(len(real_data), 100)\n  for idx, random_idx in enumerate(random_idxs):\n    image_path = os.path.join(sample_path, ""real_{}.png"".format(idx))\n    imwrite(image_path, real_data[random_idx])\n\n  return real_data, synthetic_image_path\n\nclass DataLoader(object):\n  def __init__(self, config, rng=None):\n    self.rng = np.random.RandomState(1) if rng is None else rng\n\n    self.data_path = os.path.join(config.data_dir, \'gaze\')\n    self.sample_path = os.path.join(self.data_path, config.sample_dir)\n    self.batch_size = config.batch_size\n    self.debug = config.debug\n\n    self.real_data, synthetic_image_path = load(config, self.data_path, self.sample_path, rng)\n\n    self.synthetic_data_paths = np.array(glob(os.path.join(synthetic_image_path, \'*_cropped.png\')))\n    self.synthetic_data_dims = list(imread(self.synthetic_data_paths[0]).shape) + [1]\n\n    self.synthetic_data_paths.sort()\n\n    if np.rank(self.real_data) == 3:\n      self.real_data = np.expand_dims(self.real_data, -1)\n    \n    self.real_p = 0\n\n  def get_observation_size(self):\n    return self.real_data.shape[1:]\n\n  def get_num_labels(self):\n    return np.amax(self.labels) + 1\n\n  def reset(self):\n    self.real_p = 0\n\n  def __iter__(self):\n    return self\n\n  def __next__(self, n=None):\n    """""" n is the number of examples to fetch """"""\n    if n is None: n = self.batch_size\n\n    if self.real_p == 0:\n      inds = self.rng.permutation(self.real_data.shape[0])\n      self.real_data = self.real_data[inds]\n\n    if self.real_p + n > self.real_data.shape[0]:\n      self.reset()\n\n    x = self.real_data[self.real_p : self.real_p + n]\n    self.real_p += self.batch_size\n\n    return x\n\n  next = __next__\n'"
data/hand_data.py,0,b'\nclass DataLoader(object):\n  def __init__(self):\n    pass\n'
data/utils.py,1,"b'import os\nimport json\nimport numpy as np\nfrom datetime import datetime\n\nimport tensorflow as tf\nimport tensorflow.contrib.slim as slim\n\ntry:\n  import scipy.misc\n  imread = scipy.misc.imread\n  imresize = scipy.misc.imresize\n  imwrite = scipy.misc.imsave\nexcept:\n  import cv2\n  imread = cv2.imread\n  imresize = cv2.resize\n  imwrite = cv2.imwrite\n\nimport scipy.io as sio\nloadmat = sio.loadmat\n\ndef prepare_dirs(config):\n  if config.load_path:\n    if config.load_path.startswith(config.task):\n      config.model_name = config.load_path\n    else:\n      config.model_name = ""{}_{}"".format(config.task, config.load_path)\n  else:\n    config.model_name = ""{}_{}"".format(config.task, get_time())\n\n  config.model_dir = os.path.join(config.log_dir, config.model_name)\n  config.sample_model_dir = os.path.join(config.sample_dir, config.model_name)\n  config.output_model_dir = os.path.join(config.output_dir, config.model_name)\n\n  for path in [config.log_dir, config.data_dir,\n               config.sample_dir, config.sample_model_dir,\n               config.output_dir, config.output_model_dir]:\n    if not os.path.exists(path):\n      os.makedirs(path)\n\ndef get_time():\n  return datetime.now().strftime(""%Y-%m-%d_%H-%M-%S"")\n\ndef show_all_variables():\n  model_vars = tf.trainable_variables()\n  slim.model_analyzer.analyze_vars(model_vars, print_info=True)\n\ndef img_tile(imgs, aspect_ratio=1.0, tile_shape=None, border=1,\n             border_color=0):\n  \'\'\' from https://github.com/openai/pixel-cnn/blob/master/pixel_cnn_pp/plotting.py\n  \'\'\'\n\n  if imgs.ndim != 3 and imgs.ndim != 4:\n    raise ValueError(\'imgs has wrong number of dimensions.\')\n  n_imgs = imgs.shape[0]\n\n  # Grid shape\n  img_shape = np.array(imgs.shape[1:3])\n  if tile_shape is None:\n    img_aspect_ratio = img_shape[1] / float(img_shape[0])\n    aspect_ratio *= img_aspect_ratio\n    tile_height = int(np.ceil(np.sqrt(n_imgs * aspect_ratio)))\n    tile_width = int(np.ceil(np.sqrt(n_imgs / aspect_ratio)))\n    grid_shape = np.array((tile_height, tile_width))\n  else:\n    assert len(tile_shape) == 2\n    grid_shape = np.array(tile_shape)\n\n  # Tile image shape\n  tile_img_shape = np.array(imgs.shape[1:])\n  tile_img_shape[:2] = (img_shape[:2] + border) * grid_shape[:2] - border\n\n  # Assemble tile image\n  tile_img = np.empty(tile_img_shape)\n  tile_img[:] = border_color\n  for i in range(grid_shape[0]):\n    for j in range(grid_shape[1]):\n      img_idx = j + i*grid_shape[1]\n      if img_idx >= n_imgs:\n        # No more images - stop filling out the grid.\n        break\n      img = imgs[img_idx]\n      yoff = (img_shape[0] + border) * i\n      xoff = (img_shape[1] + border) * j\n      tile_img[yoff:yoff+img_shape[0], xoff:xoff+img_shape[1], ...] = img\n\n  return tile_img\n\ndef save_config(model_dir, config):\n  param_path = os.path.join(model_dir, ""params.json"")\n\n  print(""[*] MODEL dir: %s"" % model_dir)\n  print(""[*] PARAM path: %s"" % param_path)\n\n  with open(param_path, \'w\') as fp:\n    json.dump(config.__dict__, fp,  indent=4, sort_keys=True)\n'"
