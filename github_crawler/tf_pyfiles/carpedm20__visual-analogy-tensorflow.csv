file_path,api_count,code
loader.py,0,"b'import os\nimport scipy.io\nimport scipy.misc\nimport numpy as np\nfrom time import gmtime, strftime\nfrom numpy.random import choice\n\nclass Loader(object):\n\n  def __init__(self, dataset, batch_size):\n    self.dataset = dataset\n    self.batch_size = batch_size\n    self.options = [\'rotate\', \'scale\', \'xpos\', \'ypos\']\n\n    if dataset == ""shape"":\n      mat_fname = ""shapes48.mat""\n    elif dataset == ""sprite"":\n      mat_fname = ""shapes48.mat""\n    else:\n      raise Exception("" [!] No dataset exists for %s."" % dataset)\n\n    mat_path = os.path.join(""data"", mat_fname)\n    print ("" [*] loading %s"" % mat_path)\n    mat = scipy.io.loadmat(mat_path)\n\n    if dataset == ""shape"":\n      self.data = mat[\'M\']\n      self.data_shape = self.data.shape\n      self.data = self.data.reshape(list(self.data.shape[:3]) + [-1])\n\n      self.width, self.height, self.channel, self.color, \\\n          self.shape, self.scale, self.angle, self.xpos, self.ypos = self.data_shape\n\n      num_id = self.color * self.shape\n      pair_matrix = np.eye(num_id).flatten()\n\n      num_train = 800\n      num_test = 224\n      \n      random_idx = choice(range(pair_matrix.size), num_train, replace=False)\n      pair_matrix[random_idx] = 1\n\n      pair_matrix = pair_matrix.reshape([num_id, num_id])\n      self.train_pairs = np.array(zip(*np.nonzero(pair_matrix)))\n      self.test_pairs = np.array(zip(*(pair_matrix == 0)))\n\n      self.tests = {}\n      for option in self.options:\n        test_a, test_b, test_c, test_d = self.next_test(set_option=option)\n        self.tests[option] = [test_a, test_b, test_c, test_d]\n\n    elif dataset == ""sprites"":\n      pass\n\n  def next(self, set_option=None):\n    return self.get_set_from_pairs(self.train_pairs, set_option)\n\n  def next_test(self, set_option=None):\n    return self.get_set_from_pairs(self.test_pairs, set_option)\n\n  def get_set_from_pairs(self, pairs, set_option):\n    idxes = choice(range(len(pairs)), self.batch_size)\n\n    cur_pairs = pairs[idxes]\n    cur_pairs_idx1 = cur_pairs[:,0]\n    cur_pairs_idx2 = cur_pairs[:,1]\n\n    default_angle1 = choice(self.angle, self.batch_size)\n    default_scale1 = choice(self.scale, self.batch_size)\n    default_xpos1 = choice(self.xpos, self.batch_size)\n    default_ypos1 = choice(self.ypos, self.batch_size)\n\n    default_angle2 = choice(self.angle, self.batch_size)\n    default_scale2 = choice(self.scale, self.batch_size)\n    default_xpos2 = choice(self.xpos, self.batch_size)\n    default_ypos2 = choice(self.ypos, self.batch_size)\n\n    angle1 = default_angle1\n    angle2 = default_angle1\n    angle3 = default_angle2\n    angle4 = default_angle2\n    scale1 = default_scale1\n    scale2 = default_scale1\n    scale3 = default_scale2\n    scale4 = default_scale2\n\n    xpos1 = default_xpos1\n    xpos2 = default_xpos1\n    xpos3 = default_xpos2\n    xpos4 = default_xpos2\n    ypos1 = default_ypos1\n    ypos2 = default_ypos1\n    ypos3 = default_ypos2\n    ypos4 = default_ypos2\n\n    if set_option != None:\n      to_change = set_option\n    else:\n      to_change = choice(self.options)\n\n    if to_change == ""rotate"":\n      offset = choice(range(-2, 3), self.batch_size)\n\n      angle1 = choice(self.angle, self.batch_size)\n      angle2 = angle1 + offset\n      angle2[angle2 < 0] += self.angle\n      angle2[angle2 >= self.angle] -= self.angle\n\n      angle3 = choice(range(self.angle), self.batch_size)\n      angle4 = angle3 + offset\n      angle4[angle4 < 0] += self.angle\n      angle4[angle4 >= self.angle] -= self.angle\n    elif to_change == ""scale"":\n      offset = choice(range(-1, 2), self.batch_size)\n\n      scale1 = choice(self.scale, self.batch_size)\n      scale2 = scale1 + offset\n\n      bound_idx = np.logical_or(scale2 < 0, scale2 >= self.scale)\n      offset[bound_idx] *= -1\n      scale2[bound_idx] = scale1[bound_idx] + offset[bound_idx]\n\n      scale3 = choice(range(self.scale), self.batch_size)\n      under_idx = np.logical_and(scale3 == 0, offset == -1)\n      upper_idx = np.logical_and(scale3 == self.scale - 1, offset == 1) \n      scale3[under_idx] = choice(range(1, self.scale), np.sum(under_idx))\n      scale3[upper_idx] = choice(range(0, self.scale - 1), np.sum(upper_idx))\n      scale4 = scale3 + offset\n    elif to_change == ""xpos"":\n      offset = choice(range(-1, 2), self.batch_size)\n\n      xpos1 = choice(self.xpos, self.batch_size)\n      xpos2 = xpos1 + offset\n\n      bound_idx = np.logical_or(xpos2 < 0, xpos2 >= self.xpos)\n      offset[bound_idx] *= -1\n      xpos2[bound_idx] = xpos1[bound_idx] + offset[bound_idx]\n\n      xpos3 = choice(range(self.xpos), self.batch_size)\n      under_idx = np.logical_and(xpos3 == 0, offset == -1)\n      upper_idx = np.logical_and(xpos3 == self.xpos - 1, offset == 1) \n      xpos3[under_idx] = choice(range(1, self.xpos), np.sum(under_idx))\n      xpos3[upper_idx] = choice(range(0, self.xpos - 1), np.sum(upper_idx))\n      xpos4 = xpos3 + offset\n    elif to_change == ""ypos"":\n      offset = choice(range(-1, 2), self.batch_size)\n\n      ypos1 = choice(self.ypos, self.batch_size)\n      ypos2 = ypos1 + offset\n\n      bound_idx = np.logical_or(ypos2 < 0, ypos2 >= self.ypos)\n      offset[bound_idx] *= -1\n      ypos2[bound_idx] = ypos1[bound_idx] + offset[bound_idx]\n\n      ypos3 = choice(range(self.ypos), self.batch_size)\n      under_idx = np.logical_and(ypos3 == 0, offset == -1)\n      upper_idx = np.logical_and(ypos3 == self.ypos - 1, offset == 1) \n      ypos3[under_idx] = choice(range(1, self.ypos), np.sum(under_idx))\n      ypos3[upper_idx] = choice(range(0, self.ypos - 1), np.sum(upper_idx))\n      ypos4 = ypos3 + offset\n    else:\n      raise Exception("" [!] Wrong option %s"" % to_change)\n    \n    color1, shape1 = np.unravel_index(cur_pairs_idx1, [self.color, self.shape])\n    color2, shape2 = np.unravel_index(cur_pairs_idx2, [self.color, self.shape])\n\n    shape = self.data_shape[3:]\n    idx1 =  np.ravel_multi_index([color1, shape1, scale1, angle1, xpos1, ypos1], shape)\n    idx2 =  np.ravel_multi_index([color1, shape1, scale2, angle2, xpos2, ypos2], shape)\n    idx3 =  np.ravel_multi_index([color2, shape2, scale3, angle3, xpos3, ypos3], shape)\n    idx4 =  np.ravel_multi_index([color2, shape2, scale4, angle4, xpos4, ypos4], shape)\n\n    a = np.rollaxis(self.data[:,:,:,idx1], 3)\n    b = np.rollaxis(self.data[:,:,:,idx2], 3)\n    c = np.rollaxis(self.data[:,:,:,idx3], 3)\n    d = np.rollaxis(self.data[:,:,:,idx4], 3)\n\n    if False: # only sued for debugging\n      t = strftime(""%Y-%m-%d %H:%M:%S"", gmtime())\n      self._get_image(a, ""test/%s_1.png"" % t)\n      self._get_image(b, ""test/%s_2.png"" % t)\n      self._get_image(c, ""test/%s_3.png"" % t)\n      self._get_image(d, ""test/%s_4.png"" % t)\n\n    return a, b, c, d\n\n  def _get_image(self, imgs, fname):\n    for idx, img in enumerate(imgs):\n      scipy.misc.imsave(fname.replace(""."", ""_%s."" % idx).replace("" "", ""_""), img)\n'"
main.py,3,"b'import os\nimport tensorflow as tf\n\nfrom model import ShapeAnalogy, SpriteAnalogy\nfrom utils import pp\n\nflags = tf.app.flags\nflags.DEFINE_float(""learning_rate"", 0.0001, ""Learning rate of for adam [0.0001]"")\nflags.DEFINE_float(""alpha"", 0.001, ""The importance of regularizer term [0.01]"")\nflags.DEFINE_integer(""max_iter"", 450000, ""The size of total iterations [450000]"")\nflags.DEFINE_integer(""batch_size"", 25, ""The size of batch images [25]"")\nflags.DEFINE_integer(""image_size"", 48, ""The size of width or height of image to use [48]"")\nflags.DEFINE_string(""dataset"", ""shape"", ""The name of dataset [shape, sprite]"")\nflags.DEFINE_string(""model_type"", ""deep"", ""The type of the model [add, deep]"")\nflags.DEFINE_string(""checkpoint_dir"", ""checkpoint"", ""Directory name to save the checkpoints [checkpoint]"")\nflags.DEFINE_boolean(""is_train"", False, ""True for training, False for testing [False]"")\nFLAGS = flags.FLAGS\n\nmodel_dict = {\n  ""shape"": ShapeAnalogy,\n  ""sprite"": SpriteAnalogy\n}\n\ndef main(_):\n  pp.pprint(flags.FLAGS.__flags)\n\n  if not os.path.exists(FLAGS.checkpoint_dir):\n    os.makedirs(FLAGS.checkpoint_dir)\n\n  Analogy = model_dict[FLAGS.dataset]\n\n  with tf.Session() as sess:\n    analogy = Analogy(sess, image_size=FLAGS.image_size, model_type=FLAGS.model_type,\n                      batch_size=FLAGS.batch_size, dataset=FLAGS.dataset)\n\n    if FLAGS.is_train:\n      analogy.train(max_iter=FLAGS.max_iter, alpha=FLAGS.alpha,\n                    learning_rate=FLAGS.learning_rate, checkpoint_dir=FLAGS.checkpoint_dir)\n    else:\n      analogy.load(FLAGS.checkpoint_dir)\n\n    analogy.test()\n\nif __name__ == \'__main__\':\n  tf.app.run()\n'"
utils.py,0,"b'import pprint\nimport numpy as np\nimport scipy.misc\nfrom time import gmtime, strftime\n\npp = pprint.PrettyPrinter()\n\nstrfnow = lambda: strftime(""%Y-%m-%d %H:%M:%S"", gmtime())\n\ndef merge(*images):\n  images = list(images)\n  # For difference between target and inferenced image\n  # images.append(abs(images[-2] - images[-1]))\n\n  h, w = images[0].shape[1], images[0].shape[2]\n  h_count, w_count = len(images), len(images[0])\n  img = np.zeros((h * h_count, w * w_count, 3))\n\n  for idx, image_set in enumerate(zip(*(images))):\n    for jdx, image in enumerate(image_set):\n      copy_img = image.copy()\n      copy_img[[0,-1],:,:]=1\n      copy_img[:,[0,-1],:]=1\n      img[jdx*h:jdx*h + h, idx*w:idx*w + w, :] = copy_img\n  return img\n\ndef imsave(path, image):\n  print("" [*] Save %s"" % path)\n  image[image>1] = 1\n  image[image<0] = 0\n  return scipy.misc.imsave(path, image)\n\ndef make_gif(images, fname, duration=2, true_image=False):\n  import moviepy.editor as mpy\n\n  def make_frame(t):\n    try:\n      x = images[int(len(images)/duration*t)]\n    except:\n      x = images[-1]\n\n    if true_image:\n      return x.astype(np.uint8)\n    else:\n      return ((x+1)/2*255).astype(np.uint8)\n\n  clip = mpy.VideoClip(make_frame, duration=duration)\n  clip.write_gif(fname, fps = len(images) / duration)\n'"
model/__init__.py,0,b'from .shape import ShapeAnalogy\nfrom .sprite import SpriteAnalogy\n'
model/base.py,3,"b'import os\nfrom glob import glob\nimport tensorflow as tf\n\nclass Model(object):\n  """"""Abstract object representing an Reader model.""""""\n  def __init__(self):\n    self.vocab = None\n    self.data = None\n\n  def get_model_dir(self):\n    model_dir = self.dataset\n    for attr in self._attrs:\n      if hasattr(self, attr):\n        model_dir += ""_%s:%s"" % (attr, getattr(self, attr))\n    return model_dir\n\n  def save(self, checkpoint_dir, global_step=None):\n    self.saver = tf.train.Saver()\n\n    print("" [*] Saving checkpoints..."")\n    model_name = type(self).__name__ or ""Reader""\n    model_dir = self.get_model_dir()\n\n    checkpoint_dir = os.path.join(checkpoint_dir, model_dir)\n    if not os.path.exists(checkpoint_dir):\n      os.makedirs(checkpoint_dir)\n    self.saver.save(self.sess, \n        os.path.join(checkpoint_dir, model_name), global_step=global_step)\n\n  def load(self, checkpoint_dir):\n    self.saver = tf.train.Saver()\n\n    print("" [*] Loading checkpoints..."")\n    model_dir = self.get_model_dir()\n    checkpoint_dir = os.path.join(checkpoint_dir, model_dir)\n\n    ckpt = tf.train.get_checkpoint_state(checkpoint_dir)\n    if ckpt and ckpt.model_checkpoint_path:\n      ckpt_name = os.path.basename(ckpt.model_checkpoint_path)\n      self.saver.restore(self.sess, os.path.join(checkpoint_dir, ckpt_name))\n      print("" [*] Load SUCCESS"")\n      return True\n    else:\n      print("" [!] Load failed..."")\n      return False\n'"
model/ops.py,6,"b'import math\nimport numpy as np \nimport tensorflow as tf\n\ndef conv2d(input_, output_dim, \n           k_h=5, k_w=5, d_h=2, d_w=2, stddev=0.02,\n           name=""conv2d""):\n  with tf.variable_scope(name):\n    w = tf.get_variable(\'w\', [k_h, k_w, input_.get_shape()[-1], output_dim],\n              initializer=tf.truncated_normal_initializer(stddev=stddev))\n    conv = tf.nn.conv2d(input_, w, strides=[1, d_h, d_w, 1], padding=\'SAME\')\n\n    biases = tf.get_variable(\'biases\', [output_dim], initializer=tf.constant_initializer(0.0))\n    conv = tf.reshape(tf.nn.bias_add(conv, biases), conv.get_shape())\n\n    return conv\n'"
model/shape.py,46,"b'import os\nimport time\nimport tensorflow as tf\n\nfrom .base import Model\nfrom loader import Loader\nfrom utils import merge, imsave, strfnow\n\nclass ShapeAnalogy(Model):\n  """"""Deep Visual Analogy Network.""""""\n  def __init__(self, sess, image_size=48, model_type=""deep"",\n               batch_size=25, dataset=""shape""):\n    """"""Initialize the parameters for an Deep Visual Analogy network.\n\n    Args:\n      image_size: int, The size of width and height of input image\n      model_type: string, The type of increment function [""add"", ""deep""]\n      batch_size: int, The size of a batch [25]\n      dataset: str, The name of dataset [""shape"", """"]\n    """"""\n    self.sess = sess\n\n    self.image_size = image_size\n    self.model_type = model_type\n    self.batch_size = batch_size\n    self.dataset = dataset\n    self.loader = Loader(self.dataset, self.batch_size)\n\n    self.sample_dir = ""samples""\n    if not os.path.exists(self.sample_dir):\n      os.makedirs(self.sample_dir)\n\n    # parameters used to save a checkpoint\n    self._attrs = [\'batch_size\', \'model_type\', \'image_size\']\n    self.options = [\'rotate\', \'scale\', \'xpos\', \'ypos\']\n\n    self.build_model()\n\n  def build_model(self):\n    self.a = tf.placeholder(tf.float32, [None, self.image_size, self.image_size, 3])\n    self.b = tf.placeholder(tf.float32, [None, self.image_size, self.image_size, 3])\n    self.c = tf.placeholder(tf.float32, [None, self.image_size, self.image_size, 3])\n    self.d = tf.placeholder(tf.float32, [None, self.image_size, self.image_size, 3])\n\n    a = tf.reshape(self.a, [self.batch_size, self.image_size * self.image_size * 3])\n    b = tf.reshape(self.b, [self.batch_size, self.image_size * self.image_size * 3])\n    c = tf.reshape(self.c, [self.batch_size, self.image_size * self.image_size * 3])\n    d = tf.reshape(self.d, [self.batch_size, self.image_size * self.image_size * 3])\n\n    enc_w1 = tf.get_variable(""enc_w1"", [self.image_size * self.image_size * 3, 4096])\n    enc_w2 = tf.get_variable(""enc_w2"", [4096, 1024])\n    enc_w3 = tf.get_variable(""enc_w3"", [1024, 512])\n\n    enc_b1 = tf.get_variable(""enc_b1"", [4096])\n    enc_b2 = tf.get_variable(""enc_b2"", [1024])\n    enc_b3 = tf.get_variable(""enc_b3"", [512])\n\n    f = tf.nn.relu\n    m = tf.matmul\n\n    f_a = m(f(m(f(m(a, enc_w1) + enc_b1), enc_w2) + enc_b2), enc_w3) + enc_b3\n    f_b = m(f(m(f(m(b, enc_w1) + enc_b1), enc_w2) + enc_b2), enc_w3) + enc_b3\n    f_c = m(f(m(f(m(c, enc_w1) + enc_b1), enc_w2) + enc_b2), enc_w3) + enc_b3\n    f_d = m(f(m(f(m(d, enc_w1) + enc_b1), enc_w2) + enc_b2), enc_w3) + enc_b3\n\n    if self.model_type == ""add"":\n      T = (f_b - f_a)\n    elif self.model_type == ""deep"":\n      T_input = tf.concat(1, [f_b - f_a, f_c])\n\n      deep_w1 = tf.get_variable(""deep_w1"", [1024, 512])\n      deep_w2 = tf.get_variable(""deep_w2"", [512, 256])\n      deep_w3 = tf.get_variable(""deep_w3"", [256, 512])\n\n      deep_b1 = tf.get_variable(""deep_b1"", [512])\n      deep_b2 = tf.get_variable(""deep_b2"", [256])\n      deep_b3 = tf.get_variable(""deep_b3"", [512])\n\n      T = m(f(m(f(m(T_input, deep_w1) + deep_b1), deep_w2) + deep_b2), deep_w3) + deep_b3\n    else:\n      raise Exception("" [!] Wrong model type : %s"" % self.model_type)\n\n    dec_w1 = tf.get_variable(""dec_w1"", [T.get_shape()[-1], 1024])\n    dec_w2 = tf.get_variable(""dec_w2"", [1024, 4096])\n    dec_w3 = tf.get_variable(""dec_w3"", [4096, self.image_size * self.image_size * 3])\n\n    dec_b1 = tf.get_variable(""dec_b1"", [1024])\n    dec_b2 = tf.get_variable(""dec_b2"", [4096])\n    dec_b3 = tf.get_variable(""dec_b3"", [self.image_size * self.image_size * 3])\n\n    self.g1 = m(f(m(f(m(T + f_c, dec_w1) + dec_b1), dec_w2) + dec_b2), dec_w3) + dec_b3\n    self.g2 = m(f(m(f(m(2*T + f_c, dec_w1) + dec_b1), dec_w2) + dec_b2), dec_w3) + dec_b3\n    self.g3 = m(f(m(f(m(3*T + f_c, dec_w1) + dec_b1), dec_w2) + dec_b2), dec_w3) + dec_b3\n\n    self.g1_img = tf.reshape(self.g1, [self.batch_size, self.image_size, self.image_size, 3])\n    self.g2_img = tf.reshape(self.g2, [self.batch_size, self.image_size, self.image_size, 3])\n    self.g3_img = tf.reshape(self.g3, [self.batch_size, self.image_size, self.image_size, 3])\n    _ = tf.image_summary(""g"", self.g1_img, max_images=5)\n\n    self.l = tf.nn.l2_loss(d - self.g1) / self.batch_size\n    _ = tf.scalar_summary(""loss"", self.l)\n\n    self.r = tf.nn.l2_loss(f_d - f_c - T) / self.batch_size\n    _ = tf.scalar_summary(""regularizer"", self.r)\n\n  def train(self, max_iter=450000,\n            alpha=0.01, learning_rate=0.001,\n            checkpoint_dir=""checkpoint""):\n    """"""Train an Deep Visual Analogy network.\n\n    Args:\n      max_iter: int, The size of total iterations [450000]\n      alpha: float, The importance of regularizer term [0.01]\n      learning_rate: float, The learning rate of SGD [0.001]\n      checkpoint_dir: str, The path for checkpoints to be saved [checkpoint]\n    """"""\n    self.max_iter = max_iter\n    self.alpha = alpha\n    self.learning_rate = learning_rate\n    self.checkpoint_dir = checkpoint_dir\n\n    self.step = tf.Variable(0, trainable=False)\n\n    self.loss = (self.l + self.alpha * self.r)\n    _ = tf.scalar_summary(""l_plus_r"", self.loss)\n\n    self.lr = tf.train.exponential_decay(self.learning_rate,\n                                         global_step=self.step,\n                                         decay_steps=100000,\n                                         decay_rate=0.999)\n    self.optim = tf.train.MomentumOptimizer(self.lr, momentum=0.9) \\\n                         .minimize(self.loss, global_step=self.step)\n    #self.optim = tf.train.AdamOptimizer(self.lr, beta1=0.5) \\\n    #                     .minimize(self.loss, global_step=self.step)\n    #self.optim = tf.train.RMSPropOptimizer(self.lr, momentum=0.9, decay=0.95) \\\n    #                     .minimize(self.loss, global_step=self.step)\n\n    merged_sum = tf.merge_all_summaries()\n    writer = tf.train.SummaryWriter(""./logs"", self.sess.graph_def)\n\n    tf.initialize_all_variables().run()\n    self.load(self.checkpoint_dir)\n\n    start_time = time.time()\n    start_iter = self.step.eval()\n\n    test_a, test_b, test_c, test_d = self.loader.tests[\'rotate\']\n\n    for step in xrange(start_iter, start_iter + self.max_iter):\n      if step != 0 and step % 10000 == 0:\n        self.test(fixed=True)\n        self.save(checkpoint_dir, step)\n\n      if step % 5  == 1:\n        feed = {self.a: test_a, self.b: test_b, self.c: test_c, self.d: test_d}\n\n        summary_str, loss = self.sess.run([merged_sum, self.loss], feed_dict=feed)\n        writer.add_summary(summary_str, step)\n\n        if step % 50 == 1:\n          print(""Epoch: [%2d/%7d] time: %4.4f, loss: %.8f"" % (step, self.max_iter, time.time() - start_time, loss))\n\n      a, b, c, d = self.loader.next()\n\n      feed = {self.a: a,\n              self.b: b,\n              self.c: c,\n              self.d: d}\n      self.sess.run(self.optim, feed_dict=feed)\n\n  def test(self, name=""test"", options=None, fixed=False):\n    if options == None:\n      options = self.options\n\n    t = strfnow()\n\n    for option in options:\n      if fixed == True:\n        a, b, c, d = self.loader.tests[option]\n      else:\n        a, b, c, d = self.loader.next(set_option=option)\n\n      feed = {self.a: a,\n              self.b: b,\n              self.c: c,\n              self.d: d}\n\n      fname = ""%s/%s_option:%s_time:%s.png"" % (self.sample_dir, name, option, t)\n      g_img, g2_img, g3_img = self.sess.run([self.g1_img, self.g2_img, self.g3_img], feed_dict=feed)\n\n      imsave(fname, merge(a, b, c, d, g_img, g2_img, g3_img))\n'"
model/sprite.py,34,"b'import time\nimport tensorflow as tf\n\nfrom .ops import conv2d\nfrom .base import Model\nfrom loader import Loader\n\nclass SpriteAnalogy(Model):\n  """"""Deep Visual Analogy Network.""""""\n  def __init__(self, sess, image_size=48, num_hid=512,\n               model_type=""dis+cls"", batch_size=25, dataset=""shape""):\n    """"""Initialize the parameters for an Deep Visual Analogy network.\n\n    Args:\n      image_size: int, The size of width and height of input image\n      model_type: string, The type of increment function [""add"", ""deep""]\n      batch_size: int, The size of a batch [25]\n      dataset: str, The name of dataset [""shape"", """"]\n    """"""\n    self.sess = sess\n\n    self.image_size = image_size\n    self.model_type = model_type\n    self.batch_size = batch_size\n    self.dataset = dataset\n    self.num_hid = num_hid\n    #self.loader = Loader(self.dataset, self.batch_size)\n\n    self.cards = [2, 4, 3, 6, 2, 2, 2]\n    num_categorical = 0\n    for card in self.cards:\n        num_categorical = num_categorical + card\n    self.cards[6] = 3;\n    num_categorical = num_categorical + 1\n    self.num_categorical = num_categorical\n\n    self.id_idxes = range(0, self.num_categorical - 1)\n    self.pose_idxes = range(self.num_categorical, self.num_hid)\n\n    # parameters used to save a checkpoint\n    self._attrs = [\'max_iter\', \'batch_size\', \'alpha\', \'learning_rate\']\n\n    self.build_model()\n\n  def build_model(self):\n    self.a = tf.placeholder(tf.float32, [self.batch_size, self.image_size, self.image_size, 3])\n    self.b = tf.placeholder(tf.float32, [self.batch_size, self.image_size, self.image_size, 3])\n    self.c = tf.placeholder(tf.float32, [self.batch_size, self.image_size, self.image_size, 3])\n    self.d = tf.placeholder(tf.float32, [self.batch_size, self.image_size, self.image_size, 3])\n\n    f = tf.nn.relu\n    m = tf.matmul\n\n    with tf.variable_scope(""encoder"") as scope:\n      enc_w1 = tf.get_variable(""enc_w1"", [4608, 2048])\n      enc_w2 = tf.get_variable(""enc_w2"", [2048, 512])\n\n      enc_b1 = tf.get_variable(""enc_b1"", [2048])\n      enc_b2 = tf.get_variable(""enc_b2"", [512])\n\n      conv_a = tf.reshape(f(conv2d(f(conv2d(self.a, 64, name=""conv1"")), 32, name=""conv2"")), [self.batch_size, -1])\n      f_a = (m(f(m(conv_a, enc_w1) + enc_b1), enc_w2) + enc_b2)\n\n      scope.reuse_variables()\n\n      conv_b = tf.reshape(f(conv2d(f(conv2d(self.b, 64, name=""conv1"")), 32, name=""conv2"")), [self.batch_size, -1])\n      f_b = (m(f(m(conv_b, enc_w1) + enc_b1), enc_w2) + enc_b2)\n      conv_c = tf.reshape(f(conv2d(f(conv2d(self.c, 64, name=""conv1"")), 32, name=""conv2"")), [self.batch_size, -1])\n      f_c = (m(f(m(conv_c, enc_w1) + enc_b1), enc_w2) + enc_b2)\n      conv_d = tf.reshape(f(conv2d(f(conv2d(self.d, 64, name=""conv1"")), 32, name=""conv2"")), [self.batch_size, -1])\n      f_d = (m(f(m(conv_d, enc_w1) + enc_b1), enc_w2) + enc_b2)\n\n    # Transform\n    T = f_b - f_a\n    T = T[self.pose_idxes, :]\n\n    top_pose = f_c[self.pose_idxes, :]\n    sw = np.ones_like(T)\n\n    g_input = T + f_c\n\n    dec_w1 = tf.get_variable(""dec_w1"", [g_input.get_shape()[-1], 1024])\n    dec_w2 = tf.get_variable(""dec_w2"", [1024, 4096])\n    dec_w3 = tf.get_variable(""dec_w3"", [4096, self.image_size * self.image_size * 3])\n\n    dec_b1 = tf.get_variable(""dec_b1"", [1024])\n    dec_b2 = tf.get_variable(""dec_b2"", [4096])\n    dec_b3 = tf.get_variable(""dec_b3"", [self.image_size * self.image_size * 3])\n\n    self.g = f(m(f(m(f(m(T, dec_w1) + dec_b1), dec_w2) + dec_b2), dec_w3) + dec_b3)\n\n    self.g_img = tf.reshape(self.g, [self.batch_size, self.image_size, self.image_size, 3])\n    _ = tf.image_summary(""g"", self.g_img, max_images=5)\n\n    self.l = tf.nn.l2_loss(d - self.g)\n    _ = tf.scalar_summary(""loss"", self.l)\n\n    self.r = tf.nn.l2_loss(f_d - f_c - T)\n    _ = tf.scalar_summary(""regularizer"", self.r)\n\n  def train(self, max_iter=450000,\n            alpha=0.01, learning_rate=0.001,\n            checkpoint_dir=""checkpoint""):\n    """"""Train an Deep Visual Analogy network.\n\n    Args:\n      max_iter: int, The size of total iterations [450000]\n      alpha: float, The importance of regularizer term [0.01]\n      learning_rate: float, The learning rate of SGD [0.001]\n      checkpoint_dir: str, The path for checkpoints to be saved [checkpoint]\n    """"""\n    self.max_iter = max_iter\n    self.alpha = alpha\n    self.learning_rate = learning_rate\n    self.checkpoint_dir = checkpoint_dir\n\n    self.step = tf.Variable(0, trainable=False)\n\n    self.loss = (self.l + self.alpha * self.r) / self.batch_size\n    _ = tf.scalar_summary(""l_plus_r"", self.loss)\n\n    self.lr = tf.train.exponential_decay(self.learning_rate,\n                                         global_step=self.step,\n                                         decay_steps=100000,\n                                         decay_rate=0.999)\n    self.optim = tf.train.MomentumOptimizer(self.lr, momentum=0.9) \\\n                         .minimize(self.loss, global_step=self.step)\n\n    merged_sum = tf.merge_all_summaries()\n    writer = tf.train.SummaryWriter(""./logs"", self.sess.graph_def)\n\n    tf.initialize_all_variables().run()\n\n    start_time = time.time()\n    for step in xrange(self.max_iter):\n      if step % 1000  == 0:\n        self.save(checkpoint_dir)\n\n      if step % 2  == 0:\n        feed = {self.a: self.loader.test_a,\n                self.b: self.loader.test_b,\n                self.c: self.loader.test_c,\n                self.d: self.loader.test_d}\n\n        summary_str, loss = self.sess.run([merged_sum, self.loss], feed_dict=feed)\n        writer.add_summary(summary_str, step)\n\n        if step % 50 == 0:\n          print(""Epoch: [%2d/%7d] time: %4.4f, loss: %.8f"" % (step, self.max_iter, time.time() - start_time, loss))\n\n      a, b, c, d = self.loader.next()\n\n      feed = {self.a: a,\n              self.b: b,\n              self.c: c,\n              self.d: d}\n      self.sess.run(self.optim, feed_dict=feed)\n'"
