file_path,api_count,code
download_dataset.py,0,"b'import requests\nimport os\n\ndef download_file_from_google_drive(id, destination):\n    URL = ""https://docs.google.com/uc?export=download""\n\n    session = requests.Session()\n\n    response = session.get(URL, params = { \'id\' : id }, stream = True)\n    token = get_confirm_token(response)\n\n    if token:\n        params = { \'id\' : id, \'confirm\' : token }\n        response = session.get(URL, params = params, stream = True)\n\n    save_response_content(response, destination)    \n\ndef get_confirm_token(response):\n    for key, value in response.cookies.items():\n        if key.startswith(\'download_warning\'):\n            return value\n\n    return None\n\ndef save_response_content(response, destination):\n    CHUNK_SIZE = 32768\n\n    with open(destination, ""wb"") as f:\n        for chunk in response.iter_content(CHUNK_SIZE):\n            if chunk: # filter out keep-alive new chunks\n                f.write(chunk)\n\n\n\nprint(\'Dowloading Sony subset... (25GB)\')\ndownload_file_from_google_drive(\'10kpAcvldtcb9G2ze5hTcF1odzu4V_Zvh\', \'dataset/Sony.zip\')\n\nprint(\'Dowloading Fuji subset... (52GB)\')\ndownload_file_from_google_drive(\'12hvKCjwuilKTZPe9EZ7ZTb-azOmUA3HT\', \'dataset/Fuji.zip\')\n\nos.system(\'unzip dataset/Sony.zip -d dataset\')\nos.system(\'unzip dataset/Fuji.zip -d dataset\')\n'"
download_models.py,0,"b'import requests\n\ndef download_file_from_google_drive(id, destination):\n    URL = ""https://docs.google.com/uc?export=download""\n\n    session = requests.Session()\n\n    response = session.get(URL, params = { \'id\' : id }, stream = True)\n    token = get_confirm_token(response)\n\n    if token:\n        params = { \'id\' : id, \'confirm\' : token }\n        response = session.get(URL, params = params, stream = True)\n\n    save_response_content(response, destination)    \n\ndef get_confirm_token(response):\n    for key, value in response.cookies.items():\n        if key.startswith(\'download_warning\'):\n            return value\n\n    return None\n\ndef save_response_content(response, destination):\n    CHUNK_SIZE = 32768\n\n    with open(destination, ""wb"") as f:\n        for chunk in response.iter_content(CHUNK_SIZE):\n            if chunk: # filter out keep-alive new chunks\n                f.write(chunk)\n\n\n\nprint(\'Dowloading Sony Model (84Mb)\')\ndownload_file_from_google_drive(\'1wmx7AM6XWHjHIvpErmIouQgbQoMxAymG\', \'checkpoint/Sony/model.ckpt.data-00000-of-00001\')\ndownload_file_from_google_drive(\'1OmrGMng1QuwUa8lf-_wBVvbRJwBr0ETr\', \'checkpoint/Sony/model.ckpt.meta\')\n\nprint(\'Dowloading Fuji Model (84Mb)\')\ndownload_file_from_google_drive(\'1PX5wA89d-JLmwQHqpBnyTYJxDVzC1gpt\', \'checkpoint/Fuji/model.ckpt.data-00000-of-00001\')\ndownload_file_from_google_drive(\'1VzyzQ9JglcxxqUe8yn3-cAeB1pJ4jxf4\', \'checkpoint/Fuji/model.ckpt.meta\')\n\n'"
test_Fuji.py,11,"b""from __future__ import division\nimport os, scipy.io\nimport tensorflow as tf\nimport tensorflow.contrib.slim as slim\nimport numpy as np\nimport rawpy\nimport glob\n\ninput_dir = './dataset/Fuji/short/'\ngt_dir = './dataset/Fuji/long/'\ncheckpoint_dir = './checkpoint/Fuji/'\nresult_dir = './result_Fuji/'\n\n# get test IDs\ntest_fns = glob.glob(gt_dir + '1*.RAF')\ntest_ids = [int(os.path.basename(test_fn)[0:5]) for test_fn in test_fns]\n\n\ndef lrelu(x):\n    return tf.maximum(x * 0.2, x)\n\n\ndef upsample_and_concat(x1, x2, output_channels, in_channels):\n    pool_size = 2\n    deconv_filter = tf.Variable(tf.truncated_normal([pool_size, pool_size, output_channels, in_channels], stddev=0.02))\n    deconv = tf.nn.conv2d_transpose(x1, deconv_filter, tf.shape(x2), strides=[1, pool_size, pool_size, 1])\n\n    deconv_output = tf.concat([deconv, x2], 3)\n    deconv_output.set_shape([None, None, None, output_channels * 2])\n\n    return deconv_output\n\n\ndef network(input):  # Unet\n    conv1 = slim.conv2d(input, 32, [3, 3], rate=1, activation_fn=lrelu, scope='g_conv1_1')\n    conv1 = slim.conv2d(conv1, 32, [3, 3], rate=1, activation_fn=lrelu, scope='g_conv1_2')\n    pool1 = slim.max_pool2d(conv1, [2, 2], padding='SAME')\n\n    conv2 = slim.conv2d(pool1, 64, [3, 3], rate=1, activation_fn=lrelu, scope='g_conv2_1')\n    conv2 = slim.conv2d(conv2, 64, [3, 3], rate=1, activation_fn=lrelu, scope='g_conv2_2')\n    pool2 = slim.max_pool2d(conv2, [2, 2], padding='SAME')\n\n    conv3 = slim.conv2d(pool2, 128, [3, 3], rate=1, activation_fn=lrelu, scope='g_conv3_1')\n    conv3 = slim.conv2d(conv3, 128, [3, 3], rate=1, activation_fn=lrelu, scope='g_conv3_2')\n    pool3 = slim.max_pool2d(conv3, [2, 2], padding='SAME')\n\n    conv4 = slim.conv2d(pool3, 256, [3, 3], rate=1, activation_fn=lrelu, scope='g_conv4_1')\n    conv4 = slim.conv2d(conv4, 256, [3, 3], rate=1, activation_fn=lrelu, scope='g_conv4_2')\n    pool4 = slim.max_pool2d(conv4, [2, 2], padding='SAME')\n\n    conv5 = slim.conv2d(pool4, 512, [3, 3], rate=1, activation_fn=lrelu, scope='g_conv5_1')\n    conv5 = slim.conv2d(conv5, 512, [3, 3], rate=1, activation_fn=lrelu, scope='g_conv5_2')\n\n    up6 = upsample_and_concat(conv5, conv4, 256, 512)\n    conv6 = slim.conv2d(up6, 256, [3, 3], rate=1, activation_fn=lrelu, scope='g_conv6_1')\n    conv6 = slim.conv2d(conv6, 256, [3, 3], rate=1, activation_fn=lrelu, scope='g_conv6_2')\n\n    up7 = upsample_and_concat(conv6, conv3, 128, 256)\n    conv7 = slim.conv2d(up7, 128, [3, 3], rate=1, activation_fn=lrelu, scope='g_conv7_1')\n    conv7 = slim.conv2d(conv7, 128, [3, 3], rate=1, activation_fn=lrelu, scope='g_conv7_2')\n\n    up8 = upsample_and_concat(conv7, conv2, 64, 128)\n    conv8 = slim.conv2d(up8, 64, [3, 3], rate=1, activation_fn=lrelu, scope='g_conv8_1')\n    conv8 = slim.conv2d(conv8, 64, [3, 3], rate=1, activation_fn=lrelu, scope='g_conv8_2')\n\n    up9 = upsample_and_concat(conv8, conv1, 32, 64)\n    conv9 = slim.conv2d(up9, 32, [3, 3], rate=1, activation_fn=lrelu, scope='g_conv9_1')\n    conv9 = slim.conv2d(conv9, 32, [3, 3], rate=1, activation_fn=lrelu, scope='g_conv9_2')\n\n    conv10 = slim.conv2d(conv9, 27, [1, 1], rate=1, activation_fn=None, scope='g_conv10')\n    out = tf.depth_to_space(conv10, 3)\n    return out\n\n\ndef pack_raw(raw):\n    # pack X-Trans image to 9 channels\n    im = raw.raw_image_visible.astype(np.float32)\n    im = np.maximum(im - 1024, 0) / (16383 - 1024)  # subtract the black level\n\n    img_shape = im.shape\n    H = (img_shape[0] // 6) * 6\n    W = (img_shape[1] // 6) * 6\n\n    out = np.zeros((H // 3, W // 3, 9))\n\n    # 0 R\n    out[0::2, 0::2, 0] = im[0:H:6, 0:W:6]\n    out[0::2, 1::2, 0] = im[0:H:6, 4:W:6]\n    out[1::2, 0::2, 0] = im[3:H:6, 1:W:6]\n    out[1::2, 1::2, 0] = im[3:H:6, 3:W:6]\n\n    # 1 G\n    out[0::2, 0::2, 1] = im[0:H:6, 2:W:6]\n    out[0::2, 1::2, 1] = im[0:H:6, 5:W:6]\n    out[1::2, 0::2, 1] = im[3:H:6, 2:W:6]\n    out[1::2, 1::2, 1] = im[3:H:6, 5:W:6]\n\n    # 1 B\n    out[0::2, 0::2, 2] = im[0:H:6, 1:W:6]\n    out[0::2, 1::2, 2] = im[0:H:6, 3:W:6]\n    out[1::2, 0::2, 2] = im[3:H:6, 0:W:6]\n    out[1::2, 1::2, 2] = im[3:H:6, 4:W:6]\n\n    # 4 R\n    out[0::2, 0::2, 3] = im[1:H:6, 2:W:6]\n    out[0::2, 1::2, 3] = im[2:H:6, 5:W:6]\n    out[1::2, 0::2, 3] = im[5:H:6, 2:W:6]\n    out[1::2, 1::2, 3] = im[4:H:6, 5:W:6]\n\n    # 5 B\n    out[0::2, 0::2, 4] = im[2:H:6, 2:W:6]\n    out[0::2, 1::2, 4] = im[1:H:6, 5:W:6]\n    out[1::2, 0::2, 4] = im[4:H:6, 2:W:6]\n    out[1::2, 1::2, 4] = im[5:H:6, 5:W:6]\n\n    out[:, :, 5] = im[1:H:3, 0:W:3]\n    out[:, :, 6] = im[1:H:3, 1:W:3]\n    out[:, :, 7] = im[2:H:3, 0:W:3]\n    out[:, :, 8] = im[2:H:3, 1:W:3]\n    return out\n\n\nsess = tf.Session()\nin_image = tf.placeholder(tf.float32, [None, None, None, 9])\ngt_image = tf.placeholder(tf.float32, [None, None, None, 3])\nout_image = network(in_image)\n\nsaver = tf.train.Saver()\nsess.run(tf.global_variables_initializer())\nckpt = tf.train.get_checkpoint_state(checkpoint_dir)\nif ckpt:\n    print('loaded ' + ckpt.model_checkpoint_path)\n    saver.restore(sess, ckpt.model_checkpoint_path)\n\nif not os.path.isdir(result_dir + 'final/'):\n    os.makedirs(result_dir + 'final/')\n\nfor test_id in test_ids:\n    # test the first image in each sequence\n    in_files = glob.glob(input_dir + '%05d_00*.RAF' % test_id)\n    for k in range(len(in_files)):\n        in_path = in_files[k]\n        in_fn = os.path.basename(in_path)\n        print(in_fn)\n        gt_files = glob.glob(gt_dir + '%05d_00*.RAF' % test_id)\n        gt_path = gt_files[0]\n        gt_fn = os.path.basename(gt_path)\n        in_exposure = float(in_fn[9:-5])\n        gt_exposure = float(gt_fn[9:-5])\n        ratio = min(gt_exposure / in_exposure, 300)\n\n        raw = rawpy.imread(in_path)\n        input_full = np.expand_dims(pack_raw(raw), axis=0) * ratio\n        im = raw.postprocess(use_camera_wb=True, half_size=False, no_auto_bright=True, output_bps=16)\n        # scale_full = np.expand_dims(np.float32(im/65535.0),axis = 0)*ratio #scale the low-light image using the same ratio\n        scale_full = np.expand_dims(np.float32(im / 65535.0), axis=0)\n\n        gt_raw = rawpy.imread(gt_path)\n        im = gt_raw.postprocess(use_camera_wb=True, half_size=False, no_auto_bright=True, output_bps=16)\n        gt_full = np.expand_dims(np.float32(im / 65535.0), axis=0)\n\n        input_full = np.minimum(input_full, 1.0)\n\n        output = sess.run(out_image, feed_dict={in_image: input_full})\n        output = np.minimum(np.maximum(output, 0), 1)\n\n        _, H, W, _ = output.shape\n\n        output = output[0, :, :, :]\n        gt_full = gt_full[0, 0:H, 0:W, :]\n        scale_full = scale_full[0, 0:H, 0:W, :]\n        scale_full = scale_full * np.mean(gt_full) / np.mean(\n            scale_full)  # scale the low-light image to the same mean of the groundtruth\n\n        scipy.misc.toimage(output * 255, high=255, low=0, cmin=0, cmax=255).save(\n            result_dir + 'final/%5d_00_%d_out.png' % (test_id, ratio))\n        scipy.misc.toimage(scale_full * 255, high=255, low=0, cmin=0, cmax=255).save(\n            result_dir + 'final/%5d_00_%d_scale.png' % (test_id, ratio))\n        scipy.misc.toimage(gt_full * 255, high=255, low=0, cmin=0, cmax=255).save(\n            result_dir + 'final/%5d_00_%d_gt.png' % (test_id, ratio))\n"""
test_Sony.py,11,"b""# uniform content loss + adaptive threshold + per_class_input + recursive G\n# improvement upon cqf37\nfrom __future__ import division\nimport os, scipy.io\nimport tensorflow as tf\nimport tensorflow.contrib.slim as slim\nimport numpy as np\nimport rawpy\nimport glob\n\ninput_dir = './dataset/Sony/short/'\ngt_dir = './dataset/Sony/long/'\ncheckpoint_dir = './checkpoint/Sony/'\nresult_dir = './result_Sony/'\n\n# get test IDs\ntest_fns = glob.glob(gt_dir + '/1*.ARW')\ntest_ids = [int(os.path.basename(test_fn)[0:5]) for test_fn in test_fns]\n\nDEBUG = 0\nif DEBUG == 1:\n    save_freq = 2\n    test_ids = test_ids[0:5]\n\n\ndef lrelu(x):\n    return tf.maximum(x * 0.2, x)\n\n\ndef upsample_and_concat(x1, x2, output_channels, in_channels):\n    pool_size = 2\n    deconv_filter = tf.Variable(tf.truncated_normal([pool_size, pool_size, output_channels, in_channels], stddev=0.02))\n    deconv = tf.nn.conv2d_transpose(x1, deconv_filter, tf.shape(x2), strides=[1, pool_size, pool_size, 1])\n\n    deconv_output = tf.concat([deconv, x2], 3)\n    deconv_output.set_shape([None, None, None, output_channels * 2])\n\n    return deconv_output\n\n\ndef network(input):\n    conv1 = slim.conv2d(input, 32, [3, 3], rate=1, activation_fn=lrelu, scope='g_conv1_1')\n    conv1 = slim.conv2d(conv1, 32, [3, 3], rate=1, activation_fn=lrelu, scope='g_conv1_2')\n    pool1 = slim.max_pool2d(conv1, [2, 2], padding='SAME')\n\n    conv2 = slim.conv2d(pool1, 64, [3, 3], rate=1, activation_fn=lrelu, scope='g_conv2_1')\n    conv2 = slim.conv2d(conv2, 64, [3, 3], rate=1, activation_fn=lrelu, scope='g_conv2_2')\n    pool2 = slim.max_pool2d(conv2, [2, 2], padding='SAME')\n\n    conv3 = slim.conv2d(pool2, 128, [3, 3], rate=1, activation_fn=lrelu, scope='g_conv3_1')\n    conv3 = slim.conv2d(conv3, 128, [3, 3], rate=1, activation_fn=lrelu, scope='g_conv3_2')\n    pool3 = slim.max_pool2d(conv3, [2, 2], padding='SAME')\n\n    conv4 = slim.conv2d(pool3, 256, [3, 3], rate=1, activation_fn=lrelu, scope='g_conv4_1')\n    conv4 = slim.conv2d(conv4, 256, [3, 3], rate=1, activation_fn=lrelu, scope='g_conv4_2')\n    pool4 = slim.max_pool2d(conv4, [2, 2], padding='SAME')\n\n    conv5 = slim.conv2d(pool4, 512, [3, 3], rate=1, activation_fn=lrelu, scope='g_conv5_1')\n    conv5 = slim.conv2d(conv5, 512, [3, 3], rate=1, activation_fn=lrelu, scope='g_conv5_2')\n\n    up6 = upsample_and_concat(conv5, conv4, 256, 512)\n    conv6 = slim.conv2d(up6, 256, [3, 3], rate=1, activation_fn=lrelu, scope='g_conv6_1')\n    conv6 = slim.conv2d(conv6, 256, [3, 3], rate=1, activation_fn=lrelu, scope='g_conv6_2')\n\n    up7 = upsample_and_concat(conv6, conv3, 128, 256)\n    conv7 = slim.conv2d(up7, 128, [3, 3], rate=1, activation_fn=lrelu, scope='g_conv7_1')\n    conv7 = slim.conv2d(conv7, 128, [3, 3], rate=1, activation_fn=lrelu, scope='g_conv7_2')\n\n    up8 = upsample_and_concat(conv7, conv2, 64, 128)\n    conv8 = slim.conv2d(up8, 64, [3, 3], rate=1, activation_fn=lrelu, scope='g_conv8_1')\n    conv8 = slim.conv2d(conv8, 64, [3, 3], rate=1, activation_fn=lrelu, scope='g_conv8_2')\n\n    up9 = upsample_and_concat(conv8, conv1, 32, 64)\n    conv9 = slim.conv2d(up9, 32, [3, 3], rate=1, activation_fn=lrelu, scope='g_conv9_1')\n    conv9 = slim.conv2d(conv9, 32, [3, 3], rate=1, activation_fn=lrelu, scope='g_conv9_2')\n\n    conv10 = slim.conv2d(conv9, 12, [1, 1], rate=1, activation_fn=None, scope='g_conv10')\n    out = tf.depth_to_space(conv10, 2)\n    return out\n\n\ndef pack_raw(raw):\n    # pack Bayer image to 4 channels\n    im = raw.raw_image_visible.astype(np.float32)\n    im = np.maximum(im - 512, 0) / (16383 - 512)  # subtract the black level\n\n    im = np.expand_dims(im, axis=2)\n    img_shape = im.shape\n    H = img_shape[0]\n    W = img_shape[1]\n\n    out = np.concatenate((im[0:H:2, 0:W:2, :],\n                          im[0:H:2, 1:W:2, :],\n                          im[1:H:2, 1:W:2, :],\n                          im[1:H:2, 0:W:2, :]), axis=2)\n    return out\n\n\nsess = tf.Session()\nin_image = tf.placeholder(tf.float32, [None, None, None, 4])\ngt_image = tf.placeholder(tf.float32, [None, None, None, 3])\nout_image = network(in_image)\n\nsaver = tf.train.Saver()\nsess.run(tf.global_variables_initializer())\nckpt = tf.train.get_checkpoint_state(checkpoint_dir)\nif ckpt:\n    print('loaded ' + ckpt.model_checkpoint_path)\n    saver.restore(sess, ckpt.model_checkpoint_path)\n\nif not os.path.isdir(result_dir + 'final/'):\n    os.makedirs(result_dir + 'final/')\n\nfor test_id in test_ids:\n    # test the first image in each sequence\n    in_files = glob.glob(input_dir + '%05d_00*.ARW' % test_id)\n    for k in range(len(in_files)):\n        in_path = in_files[k]\n        in_fn = os.path.basename(in_path)\n        print(in_fn)\n        gt_files = glob.glob(gt_dir + '%05d_00*.ARW' % test_id)\n        gt_path = gt_files[0]\n        gt_fn = os.path.basename(gt_path)\n        in_exposure = float(in_fn[9:-5])\n        gt_exposure = float(gt_fn[9:-5])\n        ratio = min(gt_exposure / in_exposure, 300)\n\n        raw = rawpy.imread(in_path)\n        input_full = np.expand_dims(pack_raw(raw), axis=0) * ratio\n\n        im = raw.postprocess(use_camera_wb=True, half_size=False, no_auto_bright=True, output_bps=16)\n        # scale_full = np.expand_dims(np.float32(im/65535.0),axis = 0)*ratio\n        scale_full = np.expand_dims(np.float32(im / 65535.0), axis=0)\n\n        gt_raw = rawpy.imread(gt_path)\n        im = gt_raw.postprocess(use_camera_wb=True, half_size=False, no_auto_bright=True, output_bps=16)\n        gt_full = np.expand_dims(np.float32(im / 65535.0), axis=0)\n\n        input_full = np.minimum(input_full, 1.0)\n\n        output = sess.run(out_image, feed_dict={in_image: input_full})\n        output = np.minimum(np.maximum(output, 0), 1)\n\n        output = output[0, :, :, :]\n        gt_full = gt_full[0, :, :, :]\n        scale_full = scale_full[0, :, :, :]\n        scale_full = scale_full * np.mean(gt_full) / np.mean(\n            scale_full)  # scale the low-light image to the same mean of the groundtruth\n\n        scipy.misc.toimage(output * 255, high=255, low=0, cmin=0, cmax=255).save(\n            result_dir + 'final/%5d_00_%d_out.png' % (test_id, ratio))\n        scipy.misc.toimage(scale_full * 255, high=255, low=0, cmin=0, cmax=255).save(\n            result_dir + 'final/%5d_00_%d_scale.png' % (test_id, ratio))\n        scipy.misc.toimage(gt_full * 255, high=255, low=0, cmin=0, cmax=255).save(\n            result_dir + 'final/%5d_00_%d_gt.png' % (test_id, ratio))\n"""
train_Fuji.py,15,"b""from __future__ import division\nimport os, time, scipy.io\nimport tensorflow as tf\nimport tensorflow.contrib.slim as slim\nimport numpy as np\nimport rawpy\nimport glob\n\ninput_dir = './dataset/Fuji/short/'\ngt_dir = './dataset/Fuji/long/'\ncheckpoint_dir = './result_Fuji/'\nresult_dir = './result_Fuji/'\n\n# get train IDs\ntrain_fns = glob.glob(gt_dir + '0*.RAF')\ntrain_ids = [int(os.path.basename(train_fn)[0:5]) for train_fn in train_fns]\n\nps = 512  # patch size for training\nsave_freq = 500\n\n\ndef lrelu(x):\n    return tf.maximum(x * 0.2, x)\n\n\ndef upsample_and_concat(x1, x2, output_channels, in_channels):\n    pool_size = 2\n    deconv_filter = tf.Variable(tf.truncated_normal([pool_size, pool_size, output_channels, in_channels], stddev=0.02))\n    deconv = tf.nn.conv2d_transpose(x1, deconv_filter, tf.shape(x2), strides=[1, pool_size, pool_size, 1])\n\n    deconv_output = tf.concat([deconv, x2], 3)\n    deconv_output.set_shape([None, None, None, output_channels * 2])\n\n    return deconv_output\n\n\ndef network(input):  # Unet\n    conv1 = slim.conv2d(input, 32, [3, 3], rate=1, activation_fn=lrelu, scope='g_conv1_1')\n    conv1 = slim.conv2d(conv1, 32, [3, 3], rate=1, activation_fn=lrelu, scope='g_conv1_2')\n    pool1 = slim.max_pool2d(conv1, [2, 2], padding='SAME')\n\n    conv2 = slim.conv2d(pool1, 64, [3, 3], rate=1, activation_fn=lrelu, scope='g_conv2_1')\n    conv2 = slim.conv2d(conv2, 64, [3, 3], rate=1, activation_fn=lrelu, scope='g_conv2_2')\n    pool2 = slim.max_pool2d(conv2, [2, 2], padding='SAME')\n\n    conv3 = slim.conv2d(pool2, 128, [3, 3], rate=1, activation_fn=lrelu, scope='g_conv3_1')\n    conv3 = slim.conv2d(conv3, 128, [3, 3], rate=1, activation_fn=lrelu, scope='g_conv3_2')\n    pool3 = slim.max_pool2d(conv3, [2, 2], padding='SAME')\n\n    conv4 = slim.conv2d(pool3, 256, [3, 3], rate=1, activation_fn=lrelu, scope='g_conv4_1')\n    conv4 = slim.conv2d(conv4, 256, [3, 3], rate=1, activation_fn=lrelu, scope='g_conv4_2')\n    pool4 = slim.max_pool2d(conv4, [2, 2], padding='SAME')\n\n    conv5 = slim.conv2d(pool4, 512, [3, 3], rate=1, activation_fn=lrelu, scope='g_conv5_1')\n    conv5 = slim.conv2d(conv5, 512, [3, 3], rate=1, activation_fn=lrelu, scope='g_conv5_2')\n\n    up6 = upsample_and_concat(conv5, conv4, 256, 512)\n    conv6 = slim.conv2d(up6, 256, [3, 3], rate=1, activation_fn=lrelu, scope='g_conv6_1')\n    conv6 = slim.conv2d(conv6, 256, [3, 3], rate=1, activation_fn=lrelu, scope='g_conv6_2')\n\n    up7 = upsample_and_concat(conv6, conv3, 128, 256)\n    conv7 = slim.conv2d(up7, 128, [3, 3], rate=1, activation_fn=lrelu, scope='g_conv7_1')\n    conv7 = slim.conv2d(conv7, 128, [3, 3], rate=1, activation_fn=lrelu, scope='g_conv7_2')\n\n    up8 = upsample_and_concat(conv7, conv2, 64, 128)\n    conv8 = slim.conv2d(up8, 64, [3, 3], rate=1, activation_fn=lrelu, scope='g_conv8_1')\n    conv8 = slim.conv2d(conv8, 64, [3, 3], rate=1, activation_fn=lrelu, scope='g_conv8_2')\n\n    up9 = upsample_and_concat(conv8, conv1, 32, 64)\n    conv9 = slim.conv2d(up9, 32, [3, 3], rate=1, activation_fn=lrelu, scope='g_conv9_1')\n    conv9 = slim.conv2d(conv9, 32, [3, 3], rate=1, activation_fn=lrelu, scope='g_conv9_2')\n\n    conv10 = slim.conv2d(conv9, 27, [1, 1], rate=1, activation_fn=None, scope='g_conv10')\n    out = tf.depth_to_space(conv10, 3)\n    return out\n\n\ndef pack_raw(raw):\n    # pack X-Trans image to 9 channels\n    im = raw.raw_image_visible.astype(np.float32)\n    im = np.maximum(im - 1024, 0) / (16383 - 1024)  # subtract the black level\n\n    img_shape = im.shape\n    H = (img_shape[0] // 6) * 6\n    W = (img_shape[1] // 6) * 6\n\n    out = np.zeros((H // 3, W // 3, 9))\n\n    # 0 R\n    out[0::2, 0::2, 0] = im[0:H:6, 0:W:6]\n    out[0::2, 1::2, 0] = im[0:H:6, 4:W:6]\n    out[1::2, 0::2, 0] = im[3:H:6, 1:W:6]\n    out[1::2, 1::2, 0] = im[3:H:6, 3:W:6]\n\n    # 1 G\n    out[0::2, 0::2, 1] = im[0:H:6, 2:W:6]\n    out[0::2, 1::2, 1] = im[0:H:6, 5:W:6]\n    out[1::2, 0::2, 1] = im[3:H:6, 2:W:6]\n    out[1::2, 1::2, 1] = im[3:H:6, 5:W:6]\n\n    # 1 B\n    out[0::2, 0::2, 2] = im[0:H:6, 1:W:6]\n    out[0::2, 1::2, 2] = im[0:H:6, 3:W:6]\n    out[1::2, 0::2, 2] = im[3:H:6, 0:W:6]\n    out[1::2, 1::2, 2] = im[3:H:6, 4:W:6]\n\n    # 4 R\n    out[0::2, 0::2, 3] = im[1:H:6, 2:W:6]\n    out[0::2, 1::2, 3] = im[2:H:6, 5:W:6]\n    out[1::2, 0::2, 3] = im[5:H:6, 2:W:6]\n    out[1::2, 1::2, 3] = im[4:H:6, 5:W:6]\n\n    # 5 B\n    out[0::2, 0::2, 4] = im[2:H:6, 2:W:6]\n    out[0::2, 1::2, 4] = im[1:H:6, 5:W:6]\n    out[1::2, 0::2, 4] = im[4:H:6, 2:W:6]\n    out[1::2, 1::2, 4] = im[5:H:6, 5:W:6]\n\n    out[:, :, 5] = im[1:H:3, 0:W:3]\n    out[:, :, 6] = im[1:H:3, 1:W:3]\n    out[:, :, 7] = im[2:H:3, 0:W:3]\n    out[:, :, 8] = im[2:H:3, 1:W:3]\n    return out\n\n\nsess = tf.Session()\nin_image = tf.placeholder(tf.float32, [None, None, None, 9])\ngt_image = tf.placeholder(tf.float32, [None, None, None, 3])\nout_image = network(in_image)\n\nG_loss = tf.reduce_mean(tf.abs(out_image - gt_image))\n\nt_vars = tf.trainable_variables()\nlr = tf.placeholder(tf.float32)\nG_opt = tf.train.AdamOptimizer(learning_rate=lr).minimize(G_loss)\n\nsaver = tf.train.Saver()\nsess.run(tf.global_variables_initializer())\nckpt = tf.train.get_checkpoint_state(checkpoint_dir)\nif ckpt:\n    print('loaded ' + ckpt.model_checkpoint_path)\n    saver.restore(sess, ckpt.model_checkpoint_path)\n\n# Raw data takes long time to load. Keep them in memory after loaded.\ngt_images = [None] * 6000\nin_images = {}\nin_images['300'] = [None] * len(train_ids)\nin_images['250'] = [None] * len(train_ids)\nin_images['100'] = [None] * len(train_ids)\n\ng_loss = np.zeros((5000, 1))\n\nallfolders = glob.glob(result_dir + '*0')\nlastepoch = 0\nfor folder in allfolders:\n    lastepoch = np.maximum(lastepoch, int(folder[-4:]))\n\nlearning_rate = 1e-4\nfor epoch in range(lastepoch, 4001):\n    if os.path.isdir(result_dir + '%04d' % epoch):\n        continue\n    cnt = 0\n    if epoch > 2000:\n        learning_rate = 1e-5\n\n    for ind in np.random.permutation(len(train_ids)):\n        # get the path from image id\n        train_id = train_ids[ind]\n        in_files = glob.glob(input_dir + '%05d_00*.RAF' % train_id)\n        in_path = in_files[np.random.random_integers(0, len(in_files) - 1)]\n        in_fn = os.path.basename(in_path)\n\n        gt_files = glob.glob(gt_dir + '%05d_00*.RAF' % train_id)\n        gt_path = gt_files[0]\n        gt_fn = os.path.basename(gt_path)\n        in_exposure = float(in_fn[9:-5])\n        gt_exposure = float(gt_fn[9:-5])\n        ratio = min(gt_exposure / in_exposure, 300)\n\n        st = time.time()\n        cnt += 1\n\n        if in_images[str(ratio)[0:3]][ind] is None:\n            raw = rawpy.imread(in_path)\n            in_images[str(ratio)[0:3]][ind] = np.expand_dims(pack_raw(raw), axis=0) * ratio\n\n            gt_raw = rawpy.imread(gt_path)\n            im = gt_raw.postprocess(use_camera_wb=True, half_size=False, no_auto_bright=True, output_bps=16)\n            gt_images[ind] = np.expand_dims(np.float32(im / 65535.0), axis=0)\n\n        # crop\n        H = in_images[str(ratio)[0:3]][ind].shape[1]\n        W = in_images[str(ratio)[0:3]][ind].shape[2]\n\n        xx = np.random.randint(0, W - ps)\n        yy = np.random.randint(0, H - ps)\n        input_patch = in_images[str(ratio)[0:3]][ind][:, yy:yy + ps, xx:xx + ps, :]\n        gt_patch = gt_images[ind][:, yy * 3:yy * 3 + ps * 3, xx * 3:xx * 3 + ps * 3, :]\n\n        if np.random.randint(2, size=1)[0] == 1:  # random flip\n            input_patch = np.flip(input_patch, axis=1)\n            gt_patch = np.flip(gt_patch, axis=1)\n        if np.random.randint(2, size=1)[0] == 1:\n            input_patch = np.flip(input_patch, axis=2)\n            gt_patch = np.flip(gt_patch, axis=2)\n        if np.random.randint(2, size=1)[0] == 1:  # random transpose\n            input_patch = np.transpose(input_patch, (0, 2, 1, 3))\n            gt_patch = np.transpose(gt_patch, (0, 2, 1, 3))\n\n        input_patch = np.minimum(input_patch, 1.0)\n\n        _, G_current, output = sess.run([G_opt, G_loss, out_image],\n                                        feed_dict={in_image: input_patch, gt_image: gt_patch, lr: learning_rate})\n        output = np.minimum(np.maximum(output, 0), 1)\n        g_loss[ind] = G_current\n\n        print('%d %d Loss=%.3f Time=%.3f' % (epoch, cnt, np.mean(g_loss[np.where(g_loss)]), time.time() - st))\n\n        if epoch % save_freq == 0:\n            if not os.path.isdir(result_dir + '%04d' % epoch):\n                os.makedirs(result_dir + '%04d' % epoch)\n\n            temp = np.concatenate((gt_patch[0, :, :, :], output[0, :, :, :]), axis=1)\n            scipy.misc.toimage(temp * 255, high=255, low=0, cmin=0, cmax=255).save(\n                result_dir + '%04d/%05d_00_train_%d.jpg' % (epoch, train_id, ratio))\n\n    saver.save(sess, checkpoint_dir + 'model.ckpt')\n"""
train_Sony.py,15,"b'# uniform content loss + adaptive threshold + per_class_input + recursive G\n# improvement upon cqf37\nfrom __future__ import division\nimport os, time, scipy.io\nimport tensorflow as tf\nimport tensorflow.contrib.slim as slim\nimport numpy as np\nimport rawpy\nimport glob\n\ninput_dir = \'./dataset/Sony/short/\'\ngt_dir = \'./dataset/Sony/long/\'\ncheckpoint_dir = \'./result_Sony/\'\nresult_dir = \'./result_Sony/\'\n\n# get train IDs\ntrain_fns = glob.glob(gt_dir + \'0*.ARW\')\ntrain_ids = [int(os.path.basename(train_fn)[0:5]) for train_fn in train_fns]\n\nps = 512  # patch size for training\nsave_freq = 500\n\nDEBUG = 0\nif DEBUG == 1:\n    save_freq = 2\n    train_ids = train_ids[0:5]\n\n\ndef lrelu(x):\n    return tf.maximum(x * 0.2, x)\n\n\ndef upsample_and_concat(x1, x2, output_channels, in_channels):\n    pool_size = 2\n    deconv_filter = tf.Variable(tf.truncated_normal([pool_size, pool_size, output_channels, in_channels], stddev=0.02))\n    deconv = tf.nn.conv2d_transpose(x1, deconv_filter, tf.shape(x2), strides=[1, pool_size, pool_size, 1])\n\n    deconv_output = tf.concat([deconv, x2], 3)\n    deconv_output.set_shape([None, None, None, output_channels * 2])\n\n    return deconv_output\n\n\ndef network(input):\n    conv1 = slim.conv2d(input, 32, [3, 3], rate=1, activation_fn=lrelu, scope=\'g_conv1_1\')\n    conv1 = slim.conv2d(conv1, 32, [3, 3], rate=1, activation_fn=lrelu, scope=\'g_conv1_2\')\n    pool1 = slim.max_pool2d(conv1, [2, 2], padding=\'SAME\')\n\n    conv2 = slim.conv2d(pool1, 64, [3, 3], rate=1, activation_fn=lrelu, scope=\'g_conv2_1\')\n    conv2 = slim.conv2d(conv2, 64, [3, 3], rate=1, activation_fn=lrelu, scope=\'g_conv2_2\')\n    pool2 = slim.max_pool2d(conv2, [2, 2], padding=\'SAME\')\n\n    conv3 = slim.conv2d(pool2, 128, [3, 3], rate=1, activation_fn=lrelu, scope=\'g_conv3_1\')\n    conv3 = slim.conv2d(conv3, 128, [3, 3], rate=1, activation_fn=lrelu, scope=\'g_conv3_2\')\n    pool3 = slim.max_pool2d(conv3, [2, 2], padding=\'SAME\')\n\n    conv4 = slim.conv2d(pool3, 256, [3, 3], rate=1, activation_fn=lrelu, scope=\'g_conv4_1\')\n    conv4 = slim.conv2d(conv4, 256, [3, 3], rate=1, activation_fn=lrelu, scope=\'g_conv4_2\')\n    pool4 = slim.max_pool2d(conv4, [2, 2], padding=\'SAME\')\n\n    conv5 = slim.conv2d(pool4, 512, [3, 3], rate=1, activation_fn=lrelu, scope=\'g_conv5_1\')\n    conv5 = slim.conv2d(conv5, 512, [3, 3], rate=1, activation_fn=lrelu, scope=\'g_conv5_2\')\n\n    up6 = upsample_and_concat(conv5, conv4, 256, 512)\n    conv6 = slim.conv2d(up6, 256, [3, 3], rate=1, activation_fn=lrelu, scope=\'g_conv6_1\')\n    conv6 = slim.conv2d(conv6, 256, [3, 3], rate=1, activation_fn=lrelu, scope=\'g_conv6_2\')\n\n    up7 = upsample_and_concat(conv6, conv3, 128, 256)\n    conv7 = slim.conv2d(up7, 128, [3, 3], rate=1, activation_fn=lrelu, scope=\'g_conv7_1\')\n    conv7 = slim.conv2d(conv7, 128, [3, 3], rate=1, activation_fn=lrelu, scope=\'g_conv7_2\')\n\n    up8 = upsample_and_concat(conv7, conv2, 64, 128)\n    conv8 = slim.conv2d(up8, 64, [3, 3], rate=1, activation_fn=lrelu, scope=\'g_conv8_1\')\n    conv8 = slim.conv2d(conv8, 64, [3, 3], rate=1, activation_fn=lrelu, scope=\'g_conv8_2\')\n\n    up9 = upsample_and_concat(conv8, conv1, 32, 64)\n    conv9 = slim.conv2d(up9, 32, [3, 3], rate=1, activation_fn=lrelu, scope=\'g_conv9_1\')\n    conv9 = slim.conv2d(conv9, 32, [3, 3], rate=1, activation_fn=lrelu, scope=\'g_conv9_2\')\n\n    conv10 = slim.conv2d(conv9, 12, [1, 1], rate=1, activation_fn=None, scope=\'g_conv10\')\n    out = tf.depth_to_space(conv10, 2)\n    return out\n\n\ndef pack_raw(raw):\n    # pack Bayer image to 4 channels\n    im = raw.raw_image_visible.astype(np.float32)\n    im = np.maximum(im - 512, 0) / (16383 - 512)  # subtract the black level\n\n    im = np.expand_dims(im, axis=2)\n    img_shape = im.shape\n    H = img_shape[0]\n    W = img_shape[1]\n\n    out = np.concatenate((im[0:H:2, 0:W:2, :],\n                          im[0:H:2, 1:W:2, :],\n                          im[1:H:2, 1:W:2, :],\n                          im[1:H:2, 0:W:2, :]), axis=2)\n    return out\n\n\nsess = tf.Session()\nin_image = tf.placeholder(tf.float32, [None, None, None, 4])\ngt_image = tf.placeholder(tf.float32, [None, None, None, 3])\nout_image = network(in_image)\n\nG_loss = tf.reduce_mean(tf.abs(out_image - gt_image))\n\nt_vars = tf.trainable_variables()\nlr = tf.placeholder(tf.float32)\nG_opt = tf.train.AdamOptimizer(learning_rate=lr).minimize(G_loss)\n\nsaver = tf.train.Saver()\nsess.run(tf.global_variables_initializer())\nckpt = tf.train.get_checkpoint_state(checkpoint_dir)\nif ckpt:\n    print(\'loaded \' + ckpt.model_checkpoint_path)\n    saver.restore(sess, ckpt.model_checkpoint_path)\n\n# Raw data takes long time to load. Keep them in memory after loaded.\ngt_images = [None] * 6000\ninput_images = {}\ninput_images[\'300\'] = [None] * len(train_ids)\ninput_images[\'250\'] = [None] * len(train_ids)\ninput_images[\'100\'] = [None] * len(train_ids)\n\ng_loss = np.zeros((5000, 1))\n\nallfolders = glob.glob(result_dir + \'*0\')\nlastepoch = 0\nfor folder in allfolders:\n    lastepoch = np.maximum(lastepoch, int(folder[-4:]))\n\nlearning_rate = 1e-4\nfor epoch in range(lastepoch, 4001):\n    if os.path.isdir(result_dir + \'%04d\' % epoch):\n        continue\n    cnt = 0\n    if epoch > 2000:\n        learning_rate = 1e-5\n\n    for ind in np.random.permutation(len(train_ids)):\n        # get the path from image id\n        train_id = train_ids[ind]\n        in_files = glob.glob(input_dir + \'%05d_00*.ARW\' % train_id)\n        in_path = in_files[np.random.random_integers(0, len(in_files) - 1)]\n        in_fn = os.path.basename(in_path)\n\n        gt_files = glob.glob(gt_dir + \'%05d_00*.ARW\' % train_id)\n        gt_path = gt_files[0]\n        gt_fn = os.path.basename(gt_path)\n        in_exposure = float(in_fn[9:-5])\n        gt_exposure = float(gt_fn[9:-5])\n        ratio = min(gt_exposure / in_exposure, 300)\n\n        st = time.time()\n        cnt += 1\n\n        if input_images[str(ratio)[0:3]][ind] is None:\n            raw = rawpy.imread(in_path)\n            input_images[str(ratio)[0:3]][ind] = np.expand_dims(pack_raw(raw), axis=0) * ratio\n\n            gt_raw = rawpy.imread(gt_path)\n            im = gt_raw.postprocess(use_camera_wb=True, half_size=False, no_auto_bright=True, output_bps=16)\n            gt_images[ind] = np.expand_dims(np.float32(im / 65535.0), axis=0)\n\n        # crop\n        H = input_images[str(ratio)[0:3]][ind].shape[1]\n        W = input_images[str(ratio)[0:3]][ind].shape[2]\n\n        xx = np.random.randint(0, W - ps)\n        yy = np.random.randint(0, H - ps)\n        input_patch = input_images[str(ratio)[0:3]][ind][:, yy:yy + ps, xx:xx + ps, :]\n        gt_patch = gt_images[ind][:, yy * 2:yy * 2 + ps * 2, xx * 2:xx * 2 + ps * 2, :]\n\n        if np.random.randint(2, size=1)[0] == 1:  # random flip\n            input_patch = np.flip(input_patch, axis=1)\n            gt_patch = np.flip(gt_patch, axis=1)\n        if np.random.randint(2, size=1)[0] == 1:\n            input_patch = np.flip(input_patch, axis=2)\n            gt_patch = np.flip(gt_patch, axis=2)\n        if np.random.randint(2, size=1)[0] == 1:  # random transpose\n            input_patch = np.transpose(input_patch, (0, 2, 1, 3))\n            gt_patch = np.transpose(gt_patch, (0, 2, 1, 3))\n\n        input_patch = np.minimum(input_patch, 1.0)\n\n        _, G_current, output = sess.run([G_opt, G_loss, out_image],\n                                        feed_dict={in_image: input_patch, gt_image: gt_patch, lr: learning_rate})\n        output = np.minimum(np.maximum(output, 0), 1)\n        g_loss[ind] = G_current\n\n        print(""%d %d Loss=%.3f Time=%.3f"" % (epoch, cnt, np.mean(g_loss[np.where(g_loss)]), time.time() - st))\n\n        if epoch % save_freq == 0:\n            if not os.path.isdir(result_dir + \'%04d\' % epoch):\n                os.makedirs(result_dir + \'%04d\' % epoch)\n\n            temp = np.concatenate((gt_patch[0, :, :, :], output[0, :, :, :]), axis=1)\n            scipy.misc.toimage(temp * 255, high=255, low=0, cmin=0, cmax=255).save(\n                result_dir + \'%04d/%05d_00_train_%d.jpg\' % (epoch, train_id, ratio))\n\n    saver.save(sess, checkpoint_dir + \'model.ckpt\')\n'"
