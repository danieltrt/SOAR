file_path,api_count,code
setup.py,0,"b'""""""Setup-module for DistKeras.\n\nThis software enables distrubuted Machine Learning on Apache Spark using Keras.\n\nSee:\nhttps://github.com/JoeriHermans/dist-keras/\nhttp://joerihermans.com/\n""""""\n\nfrom setuptools import setup\nfrom setuptools import find_packages\n\nsetup(name=\'dist-keras\',\n      description=\'Distributed Deep learning with Apache Spark with Keras.\',\n      url=\'https://github.com/JoeriHermans/dist-keras\',\n      author=\'Joeri Hermans\',\n      version=\'0.2.1\',\n      author_email=\'joeri@joerihermans.com\',\n      license=\'GPLv3\',\n      install_requires=[\'theano\', \'tensorflow\', \'keras\', \'flask\'],\n      packages=[\'distkeras\'],\n      package_data={\'distkeras\': [\'distkeras/*.py\']},\n      # Keywords related to the project.\n      keywords=[\'Keras\', \'Deep Learning\', \'Machine Learning\', \'Theano\', \'Tensorflow\', \'Distributed\', \'Apache Spark\'],\n)\n'"
distkeras/__init__.py,0,b''
distkeras/evaluators.py,0,"b'""""""Evaluation module.\n\nAn evaluator will evaluate a dataframe according to specific requirements.\n""""""\n\nclass Evaluator(object):\n    """"""An evaluator is an abstract class which will, given a label and a prediction,\n       will compute an evaluation metric.\n\n    # Arguments\n        label_col: string. Column name of the label.\n        prediction_col: string. Column name of the prediction.\n    """"""\n\n    def __init__(self, label_col=""label"", prediction_col=""prediction""):\n        self.label_column = label_col\n        self.prediction_column = prediction_col\n\n    def evaluate(self, dataframe):\n        """"""Evalutes the specified dataframe.\n\n        # Arguments\n            dataframe: dataframe. Spark Dataframe.\n        """"""\n        raise NotImplementedError\n\n\nclass AccuracyEvaluator(Evaluator):\n    """"""Computes the accuracy of the prediction based on the label.\n\n    # Arguments\n        label_col: string. Label column.\n        prediction_col: string. Prediction column.\n    """"""\n\n    def __init__(self, label_col=""label"", prediction_col=""prediction""):\n        # Initialize the parent structure.\n        super(AccuracyEvaluator, self).__init__(label_col, prediction_col)\n\n    def evaluate(self, dataframe):\n        # Count the total number of instances.\n        num_instances = dataframe.count()\n        # Extract the matching indexes.\n        cleaned = dataframe.where(dataframe[self.prediction_column] == dataframe[self.label_column])\n        # Fetch the number of correctly guessed instances.\n        validated_instances = cleaned.count()\n\n        return float(validated_instances) / float(num_instances)\n'"
distkeras/job_deployment.py,0,"b'""""""Module which facilitates job deployment on remote Spark clusters.\nThis allows you to build models and architectures on, for example, remote\nnotebook servers, and submit the large scale training job on remote\nHadoop / Spark clusters.""""""\n\n## BEGIN Imports. ##############################################################\n\nfrom distkeras.utils import deserialize_keras_model\nfrom distkeras.utils import get_os_username\nfrom distkeras.utils import pickle_object\nfrom distkeras.utils import serialize_keras_model\nfrom distkeras.utils import unpickle_object\n\nfrom flask import Flask\nfrom flask import request\n\nfrom os.path import expanduser\n\nfrom threading import Lock\n\nimport base64\n\nimport json\n\nimport os\n\nimport subprocess\n\nimport threading\n\nimport time\n\nimport urllib2\n\n## END Imports. ################################################################\n\nclass Punchcard(object):\n\n    def __init__(self, secrets_path=""secrets.json"", port=80):\n        self.application = Flask(__name__)\n        self.secrets_path = secrets_path\n        self.port = port\n        self.mutex = threading.Lock()\n        self.jobs = {}\n\n    def read_secrets(self):\n        with open(self.secrets_path) as f:\n            secrets_raw = f.read()\n        secrets = json.loads(secrets_raw)\n\n        return secrets\n\n    def valid_secret(self, secret, secrets):\n        num_secrets = len(secrets)\n        for i in range(0, num_secrets):\n            description = secrets[i]\n            if description[\'secret\'] == secret:\n                return True\n        return False\n\n    def secret_in_use(self, secret):\n        return secret in self.jobs\n\n    def set_trained_model(self, job, model):\n        with self.mutex:\n            self.models[job.get_secret()] = model\n\n    def get_submitted_job(self, secret):\n        with self.mutex:\n            if self.secret_in_use(secret):\n                job = self.jobs[secret]\n            else:\n                job = None\n\n        return job\n\n    def define_routes(self):\n\n        ## BEGIN Route definitions. ############################################\n\n        @self.application.route(\'/api/submit\', methods=[\'POST\'])\n        def submit_job():\n            # Parse the incoming JSON data.\n            data = json.loads(request.data)\n            # Fetch the required job arguments.\n            secret = data[\'secret\']\n            job_name = data[\'job_name\']\n            num_executors = data[\'num_executors\']\n            num_processes = data[\'num_processes\']\n            data_path = data[\'data_path\']\n            trainer = unpickle_object(data[\'trainer\'].decode(\'hex_codec\'))\n            # Fetch the parameters for the job.\n            secrets = self.read_secrets()\n            with self.mutex:\n                if self.valid_secret(secret, secrets) and not self.secret_in_use(secret):\n                    job = PunchcardJob(secret, job_name, data_path, num_executors, num_processes, trainer)\n                    self.jobs[secret] = job\n                    job.start()\n                    return \'\', 200\n\n            return \'\', 403\n\n        @self.application.route(\'/api/state\')\n        def job_state():\n            secret = request.args.get(\'secret\')\n            job = self.get_submitted_job(secret)\n            # Check if the job exists.\n            if job is not None:\n                d = {}\n                d[\'job_name\'] = job.get_job_name()\n                d[\'running\'] = job.running()\n                return json.dumps(d), 200\n\n            return \'\', 404\n\n        @self.application.route(\'/api/cancel\')\n        def cancel():\n            secret = request.args.get(\'secret\')\n            job = self.get_submitted_job(secret)\n            if job is not None and job.running():\n                with self.mutex:\n                    job.cancel()\n                    del self.jobs[secret]\n\n            return \'\', 200\n\n        @self.application.route(\'/api/destroy\')\n        def destroy_job():\n            secret = request.args.get(\'secret\')\n            job = self.get_submitted_job(secret)\n            if job is not None and not job.running():\n                with self.mutex:\n                    model = self.jobs[secret].get_trained_model()\n                    history = self.jobs[secret].get_history()\n                    model = pickle_object(serialize_keras_model(model)).encode(\'hex_codec\')\n                    history = pickle_object(history).encode(\'hex_codec\')\n                    d = {}\n                    d[\'model\'] = model\n                    d[\'history\'] = history\n                    del self.jobs[secret]\n                return json.dumps(d), 200\n\n            return \'\', 400\n\n        ## END Route definitions. ##############################################\n\n    def run(self):\n        self.define_routes()\n        self.application.run(\'0.0.0.0\', self.port)\n\n\nclass PunchcardJob(object):\n\n    def __init__(self, secret, job_name, data_path, num_executors, num_processes, trainer):\n        self.secret = secret\n        self.job_name = job_name\n        self.data_path = data_path\n        self.num_executors = num_executors\n        self.num_processes = num_processes\n        self.trainer = trainer\n        self.is_running = True\n        self.thread = None\n        self.trained_model = None\n        self.history = None\n\n    def get_job_name(self):\n        return self.job_name\n\n    def get_secret(self):\n        return self.secret\n\n    def get_history(self):\n        return self.history\n\n    def get_trained_model(self):\n        return self.trained_model\n\n    def start(self):\n        self.trainer.determine_new_master()\n        self.thread = threading.Thread(target=self.run)\n        self.thread.setDaemon(True)\n        self.thread.start()\n\n    def cancel(self):\n        self.thread.exit()\n\n    def running(self):\n        return self.is_running\n\n    def join(self):\n        self.thread.join()\n\n    def run_job(self):\n        os.system(""python ~/jobs/"" + self.secret + "".py"")\n\n    def clean_up(self):\n        home = expanduser(""~"")\n        os.remove(home + ""/models/"" + self.secret)\n        os.remove(home + ""/histories/"" + self.secret)\n        os.remove(home + ""/trainers/"" + self.secret)\n\n    def read_trained_model(self):\n        home = expanduser(""~"")\n        with open(home + ""/models/"" + self.secret, ""r"") as f:\n            self.trained_model = deserialize_keras_model(unpickle_object(f.read()))\n\n    def read_history(self):\n        home = expanduser(""~"")\n        with open(home + ""/histories/"" + self.secret, ""r"") as f:\n            self.history = unpickle_object(f.read())\n\n    def serialize_trainer(self):\n        trainer = pickle_object(self.trainer)\n        home = expanduser(""~"")\n        with open(home + ""/trainers/"" + self.secret, ""w"") as f:\n            f.write(trainer)\n\n    def generate_code(self):\n        source = """"""\nfrom distkeras.evaluators import *\nfrom distkeras.predictors import *\nfrom distkeras.trainers import *\nfrom distkeras.trainers import *\nfrom distkeras.transformers import *\nfrom distkeras.utils import *\nfrom keras import *\nfrom pyspark import SparkConf\nfrom pyspark import SparkContext\nfrom pyspark import SQLContext\nfrom os.path import expanduser\nsecret = \'{secret}\'\napplication_name = \'{job_name}\'\nnum_executors = {num_executors}\nnum_processes = {num_processes}\npath_data = \'{data_path}\'\nnum_workers = num_processes * num_executors\n# Allocate a Spark Context, and a Spark SQL context.\nconf = SparkConf()\nconf.set(""spark.app.name"", application_name)\nconf.set(""spark.master"", ""yarn-client"")\nconf.set(""spark.executor.cores"", num_processes)\nconf.set(""spark.executor.instances"", num_executors)\nconf.set(""spark.executor.memory"", ""5g"")\nconf.set(""spark.locality.wait"", ""0"")\nconf.set(""spark.serializer"", ""org.apache.spark.serializer.KryoSerializer"");\nsc = SparkContext(conf=conf)\nsqlContext = SQLContext(sc)\n# Read the dataset from HDFS. For now we assume Parquet files.\ndataset = sqlContext.read.parquet(path_data).repartition(num_workers)\n# Deserialize the trainer object.\nhome = expanduser(""~"")\nwith open(home + ""/trainers/"" + secret, ""r"") as f:\n    trainer = unpickle_object(f.read())\n# Train the model, and save it afterwards.\ntrained_model = trainer.train(dataset)\nwith open(home + ""/models/"" + secret, ""w"") as f:\n    f.write(pickle_object(serialize_keras_model(trained_model)))\n# Save the history of the training process.\nhistories = trainer.get_history()\nwith open(home + ""/histories/"" + secret, ""w"") as f:\n    f.write(pickle_object(histories))\nsc.stop()\n        """""".format(\n            secret=self.secret,\n            job_name=self.job_name,\n            num_executors=self.num_executors,\n            num_processes=self.num_processes,\n            data_path=self.data_path\n        )\n        home = expanduser(""~"")\n        with open(home + ""/jobs/"" + self.secret + "".py"", ""w"") as f:\n            f.write(source)\n\n    def run(self):\n        self.serialize_trainer()\n        self.generate_code()\n        self.run_job()\n        self.read_trained_model()\n        self.read_history()\n        self.clean_up()\n        self.is_running = False\n\n\nclass Job(object):\n\n    def __init__(self, secret, job_name, data_path, num_executors, num_processes, trainer):\n        self.secret = secret\n        self.job_name = job_name\n        self.num_executors = 20\n        self.num_processes = 1\n        self.data_path = data_path\n        self.trainer = trainer\n        self.trained_model = None\n        self.history = None\n        self.address = None\n\n    def set_num_executors(self, num_executors):\n        self.num_executors = num_executors\n\n    def set_num_processes(self, num_processes):\n        self.num_processes = num_processes\n\n    def get_trained_model(self):\n        return self.trained_model\n\n    def get_history(self):\n        return self.history\n\n    def is_finished(self):\n        address = self.address + \'/api/state?secret=\' + self.secret\n        request = urllib2.Request(address)\n        response = urllib2.urlopen(request)\n        data = json.load(response)\n\n        return not data[\'running\']\n\n    def destroy_remote_job(self):\n        address = self.address + \'/api/destroy?secret=\' + self.secret\n        request = urllib2.Request(address)\n        response = urllib2.urlopen(request)\n        data = json.load(response)\n        model = unpickle_object(data[\'model\'].decode(\'hex_codec\'))\n        self.trained_model = deserialize_keras_model(model)\n        self.history = unpickle_object(data[\'history\'].decode(\'hex_codec\'))\n\n    def start(self):\n        self.thread = threading.Thread(target=self.run)\n        self.thread.start()\n\n    def wait_completion(self):\n        self.thread.join()\n\n    def cancel(self):\n        address = self.address + \'/api/cancel?secret=\' + self.secret\n        request = urllib2.Request(address)\n        urllib2.urlopen(request)\n\n    def send(self, address):\n        data = {}\n        data[\'secret\'] = self.secret\n        data[\'job_name\'] = self.job_name\n        data[\'num_executors\'] = self.num_executors\n        data[\'num_processes\'] = self.num_processes\n        data[\'data_path\'] = self.data_path\n        data[\'trainer\'] = pickle_object(self.trainer).encode(\'hex_codec\')\n        request = urllib2.Request(address + ""/api/submit"")\n        request.add_header(\'Content-Type\', \'application/json\')\n        urllib2.urlopen(request, json.dumps(data))\n        self.address = address\n        self.start()\n\n    def run(self):\n        time.sleep(1)\n        while not self.is_finished():\n            time.sleep(10)\n        self.destroy_remote_job()\n'"
distkeras/networking.py,0,"b'""""""Networking utility functions.""""""\n\n## BEGIN Imports. ##############################################################\n\nimport pickle\n\nimport socket\n\n## END Imports. ################################################################\n\ndef determine_host_address():\n    """"""Determines the human-readable host address of the local machine.""""""\n    host_address = socket.gethostbyname(socket.gethostname())\n\n    return host_address\n\n\ndef recvall(connection, num_bytes):\n    """"""Reads `num_bytes` bytes from the specified connection.\n\n    # Arguments\n        connection: socket. Opened socket.\n        num_bytes: int. Number of bytes to read.\n    """"""\n    byte_buffer = b\'\'\n    buffer_size = 0\n    bytes_left = num_bytes\n    # Iterate until we received all data.\n    while buffer_size < num_bytes:\n        # Fetch the next frame from the network.\n        data = connection.recv(bytes_left)\n        # Compute the size of the frame.\n        delta = len(data)\n        buffer_size += delta\n        bytes_left -= delta\n        # Append the data to the buffer.\n        byte_buffer += data\n\n    return byte_buffer\n\n\ndef recv_data(connection):\n    """"""Will fetch the next data frame from the connection.\n\n    The protocol for reading is structured as follows:\n    1. The first 20 bytes represents a string which holds the next number of bytes to read.\n    2. We convert the 20 byte string to an integer (e.g. \'00000000000000000011\' -> 11).\n    3. We read `num_bytes` from the socket (which is in our example 11).\n    4. Deserialize the retrieved string.\n\n    # Arguments\n        connection: socket. Opened socket.\n    """"""\n    data = b\'\'\n    # Fetch the serialized data length.\n    length = int(recvall(connection, 20).decode())\n    # Fetch the serialized data.\n    serialized_data = recvall(connection, length)\n    # Deserialize the data.\n    data = pickle.loads(serialized_data)\n\n    return data\n\n\ndef send_data(connection, data):\n    """"""Sends the data to the other endpoint of the socket using our protocol.\n\n    The protocol for sending is structured as follows:\n    1. Serialize the data.\n    2. Obtain the buffer-size of the serialized data.\n    3. Serialize the buffer-size in 20 bytes (e.g. 11 -> \'00000000000000000011\').\n    4. Send the serialized buffer size.\n    5. Send the serialized data.\n\n    # Arguments\n        connection: socket. Opened socket.\n        data: any. Data to send.\n    """"""\n    # Serialize the data.\n    serialized_data = pickle.dumps(data, -1)\n    length = len(serialized_data)\n    # Serialize the number of bytes in the data.\n    serialized_length = str(length).zfill(20)\n    # Send the data over the provided socket.\n    connection.sendall(serialized_length.encode())\n    connection.sendall(serialized_data)\n\n\ndef connect(host, port, disable_nagle=True):\n    fd = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n    # Check if Nagle\'s algorithm needs to be disabled.\n    if disable_nagle:\n        fd.setsockopt(socket.IPPROTO_TCP, socket.TCP_NODELAY, 1)\n    else:\n        fd.setsockopt(socket.IPPROTO_TCP, socket.TCP_NODELAY, 0)\n    # Connect to the specified URI.\n    fd.connect((host, port))\n\n    return fd\n'"
distkeras/parameter_servers.py,0,"b'""""""Parameter servers.\n\nA parameter server is a process which will aggregate all the incoming gradient\nor parameter updates of the workers and incorperate it into a single center variable.\nThis center variable will eventually be the produced model of the trainer.\n""""""\n\n## BEGIN Imports. ##############################################################\n\nimport copy\n\nimport math\n\nimport numpy as np\n\nimport socket\n\nimport threading\n\nfrom distkeras.networking import recv_data\nfrom distkeras.networking import send_data\nfrom distkeras.utils import deserialize_keras_model\n\n## END Imports. ################################################################\n\nclass ParameterServer(object):\n    """"""Abstract class which provides basic attributed and methods for all\n       parameter servers.\n\n    # Arguments\n        model: string. Serialized Keras model.\n               See: distkeras.utils.serialize_keras_model\n    """"""\n\n    def __init__(self, model):\n        self.model = deserialize_keras_model(model)\n        self.num_updates = 1\n\n    def initialize(self):\n        """"""Initializes the parameter server.\n\n        This method is called after self.start().\n        """"""\n        raise NotImplementedError\n\n    def start(self):\n        """"""Starts the parameter server in a new thread.""""""\n        raise NotImplementedError\n\n    def run(self):\n        """"""Main event loop of the parameter server.""""""\n        raise NotImplementedError\n\n    def stop(self):\n        """"""Notifies the parameter server thread to stop.""""""\n        raise NotImplementedError\n\n    def get_model(self):\n        """"""Returns the Keras model which will be trained by the workers.""""""\n        return self.model\n\n    def next_update(self):\n        """"""Increments the number of model updates by 1.""""""\n        self.num_updates += 1\n\n    def reset_update_counter(self):\n        """"""Resets the model update counter.""""""\n        self.num_updates = 0\n\n    def get_num_updates(self):\n        """"""Returns the number of model updates the parameter server has performed.""""""\n        return self.num_updates\n\n\nclass SocketParameterServer(ParameterServer):\n    """"""Abstract class of a parameter server which is based on a socket implementation.\n\n    This means that this parameter server accepts multiple TCP connections from multiple\n    workers, and uses a costum protocol to transmit and receive the model parameters. This\n    is done by implementing a custom protocol. Which is fully described in the\n    distkeras.networking module.\n\n    # Arguments\n        model: string. Serialized Keras model.\n               See: distkeras.utils.serialize_keras_model\n        port: int. Listing port number.\n    """"""\n\n    def __init__(self, model, port=5000):\n        super(SocketParameterServer, self).__init__(model)\n        self.master_port = port\n        self.socket = None\n        self.running = False\n        self.connections = []\n        self.mutex = threading.Lock()\n\n    def initialize(self):\n        """"""Sets up the listing port.""""""\n        # Reset the running flag.\n        self.running = True\n        # Prepare a socket.\n        file_descriptor = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n        # Disable Nagle\'s algorithm.\n        file_descriptor.setsockopt(socket.IPPROTO_TCP, socket.TCP_NODELAY, 1)\n        # Check if the master port needs to be assigned by the OS.\n        if self.master_port is None:\n            file_descriptor.bind((\'0.0.0.0\', 0))\n            # Retrieve the port assigned by the OS.\n            self.master_port = int(file_descriptor.getsockname()[1])\n        else:\n            file_descriptor.bind((\'0.0.0.0\', self.master_port))\n        # Listen to the socket.\n        file_descriptor.listen(5)\n        # Assign the socket.\n        self.socket = file_descriptor\n\n    def handle_commit(self, conn, addr):\n        """"""Handles parameter updates coming from the workers.\n\n        # Arguments:\n            conn: socket. The opened connection.\n            addr: addr. Address of the remote host.\n        """"""\n        raise NotImplementedError\n\n    def handle_pull(self, conn, addr):\n        """"""Handles parameter requests coming from the workers. This will\n        actually send the model parameters to the requesting host.\n\n        # Arguments:\n            conn: socket. The opened connection.\n            addr: addr. Address of the remote host.\n        """"""\n        # Fetch the raw center variables.\n        with self.mutex:\n            center_variable = self.model.get_weights()\n            cv = copy.deepcopy(center_variable)\n        # Send the data over the socket.\n        send_data(conn, cv)\n\n    def cancel_accept(self):\n        """"""This method will cancel the accept procedure. The method\n        is meant to be executed by the stop() procedure.\n        """"""\n        file_descriptor = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n        try:\n            # Connect to the listening socket to cancel the accept.\n            file_descriptor.connect((""localhost"", self.master_port))\n            file_descriptor.close()\n        except Exception as e:\n            print(e)\n\n    def handle_connection(self, conn, addr):\n        """"""\n        A parameter server has two main functionalities. Nodes are able to\n        pull (p) the current state, or \'commit\' a state. This is implemented\n        in the following functionality. Classes which implement these interfaces\n        should not worry about connection handling.\n        """"""\n        try:\n            while self.running:\n                # Fetch the current action.\n                action = conn.recv(1).decode()\n                # Check if the action is a commit (most of the cases).\n                if action == \'c\':\n                    # Handle the commit.\n                    self.handle_commit(conn, addr)\n                elif action == \'p\':\n                    # Handle the pull.\n                    self.handle_pull(conn, addr)\n        except Exception as e:\n            print(e)\n\n    def start(self):\n        """"""Starts the parameter server.""""""\n        # Set the running flag.\n        self.running = True\n\n    def run(self):\n        """"""Main event loop of the parameter server.""""""\n        # Listen for incoming connections.\n        while self.running:\n            try:\n                # Accept incoming connections.\n                conn, addr = self.socket.accept()\n                # Handle the connection.\n                thread = threading.Thread(target=self.handle_connection, args=(conn, addr))\n                thread.start()\n                # Store the connection in the dictionary.\n                self.connections.append(thread)\n            except Exception as e:\n                print(e)\n\n    def stop(self):\n        """"""Stop the parameter server. This will also cleanup all existing connections.""""""\n        self.running = False\n        # Check if a socket is allocated.\n        if self.socket:\n            self.cleanup_connections()\n            self.finalize()\n            self.socket.close()\n            self.cancel_accept()\n            self.socket = None\n        self.connections = []\n\n    def finalize(self):\n        """"""Method that is called when the parameter server stops.""""""\n        print(""Not executed"")\n\n    def cleanup_connections(self):\n        """"""Clean all existing connections up.""""""\n        # Iterate over all connections.\n        for thread in self.connections:\n            # Fetch the thread object.\n            thread.join()\n            del thread\n\n\nclass DeltaParameterServer(SocketParameterServer):\n    """"""A parameter server which integrates all incoming deltas into the model.\n\n    # Arguments\n        model: string. Serialized Keras model.\n               See: distkeras.utils.serialize_keras_model\n        master_port: int. Port number of the parameter server.\n    """"""\n\n    def __init__(self, model, master_port):\n        super(DeltaParameterServer, self).__init__(model, master_port)\n        self.center_variable = np.asarray(self.model.get_weights())\n\n    def handle_commit(self, conn, addr):\n        # Receive the parameters from the remote node.\n        data = recv_data(conn)\n        # Extract the delta from the dictionary.\n        delta = data[\'delta\']\n        # Update the center variable with the delta.\n        with self.mutex:\n            self.center_variable = self.center_variable + delta\n        # Next iteration.\n        self.next_update()\n\n    def handle_pull(self, conn, addr):\n        """"""Handles parameter requests coming from the workers. This will\n        actually send the model parameters to the requesting host.\n\n        # Arguments:\n            conn: socket. The opened connection.\n            addr: addr. Address of the remote host.\n        """"""\n        # Fetch the raw center variables.\n        with self.mutex:\n            cv = copy.deepcopy(self.center_variable)\n        # Send the data over the socket.\n        send_data(conn, cv)\n\n    def finalize(self):\n        # Set the final weights of the model.\n        self.model.set_weights(self.center_variable)\n\n\nclass ADAGParameterServer(SocketParameterServer):\n    """"""A parameter server which integrates the incoming gradient residuals into\n       the model, and integrates them using the ADAG scheme.\n\n    # Arguments\n        model: string. Keras model.\n               See: distkeras.utils.serialize_keras_model\n        master_port: int. Port number of the parameter server.\n    """"""\n\n    def __init__(self, model, master_port):\n        super(ADAGParameterServer, self).__init__(model, master_port)\n        self.center_variable = np.asarray(self.model.get_weights())\n\n    def handle_commit(self, conn, addr):\n        # Receive the parameters from the remote node.\n        data = recv_data(conn)\n        # Extract the data from the dictionary.\n        r = data[\'residual\']\n        with self.mutex:\n            # Update the center variable.\n            self.center_variable = self.center_variable + r\n        # Increment the number of parameter server updates.\n        self.next_update()\n\n    def handle_pull(self, conn, addr):\n        """"""Handles parameter requests coming from the workers. This will\n        actually send the model parameters to the requesting host.\n\n        # Arguments:\n            conn: socket. The opened connection.\n            addr: addr. Address of the remote host.\n        """"""\n        # Fetch the raw center variables.\n        with self.mutex:\n            cv = copy.deepcopy(self.center_variable)\n        # Send the data over the socket.\n        send_data(conn, cv)\n\n    def finalize(self):\n        # Set the weights of the model.\n        self.model.set_weights(self.center_variable)\n\n\nclass DynSGDParameterServer(SocketParameterServer):\n    """"""DynSGD parameter server, keeps track of the staleness between updates\n    to maintain dynamic worker learning rates based on staleness.\n\n    # Arguments\n        model: string. Keras model\n               See: distkeras.utils.serialize_keras_model\n        master_port: int. Port number of the parameter server.\n    """"""\n\n    def __init__(self, model, master_port):\n        super(DynSGDParameterServer, self).__init__(model, master_port)\n\n    def handle_pull(self, conn, addr):\n        """"""Handles parameter requests coming from the workers. This will\n        actually send the model parameters to the requesting host.\n\n        This is a specific implementation for DynSGD.\n\n        # Arguments:\n            conn: socket. The opened connection.\n            addr: addr. Address of the remote host.\n        """"""\n        # Allocate a new dictionary.\n        data = {}\n        # Fetch the raw center variables.\n        with self.mutex:\n            center_variable = self.model.get_weights()\n            cv = copy.deepcopy(center_variable)\n            # Store the number of updates (u) the PS executed.\n            data[\'update\'] = self.num_updates\n        # Store the model (m).\n        data[\'model\'] = cv\n        # Send the data over the socket.\n        send_data(conn, data)\n\n    def handle_commit(self, conn, addr):\n        data = recv_data(conn)\n        r = data[\'residual\']\n        # Fetch the last iteration number\n        last_update = data[\'last_update\']\n        du = (self.num_updates - last_update) + 1\n        r /= du\n        with self.mutex:\n            center_variable = self.model.get_weights()\n            center_variable = center_variable + r\n            self.model.set_weights(center_variable)\n        # Increment the number of parameter server updates.\n        self.next_update()\n\n\nclass ExperimentalParameterServer(SocketParameterServer):\n    """"""A parameter server which integrates the incoming gradient residuals into\n       the model, and integrates them using the ADAG scheme.\n\n    # Arguments\n        model: string. Keras model.\n               See: distkeras.utils.serialize_keras_model\n        master_port: int. Port number of the parameter server.\n    """"""\n\n    def __init__(self, model, master_port, learning_rate):\n        super(ExperimentalParameterServer, self).__init__(model, master_port)\n        self.center_variable = np.asarray(self.model.get_weights())\n        self.inverse_learning_rate = 1.0 / learning_rate\n\n    def handle_commit(self, conn, addr):\n        # Receive the parameters from the remote node.\n        data = recv_data(conn)\n        # Extract the data from the dictionary.\n        r = data[\'residual\']\n        worker_id = data[\'worker_id\']\n        stale_cv = data[\'stale_center_variable\']\n        with self.mutex:\n            diff_cv = np.subtract(self.center_variable, stale_cv)\n            d = 1 / (self.inverse_learning_rate * np.power(diff_cv, 2) + 1)\n            r = np.multiply(d, r)\n            # Update the center variable.\n            self.center_variable = self.center_variable + r\n        # Increment the number of parameter server updates.\n        self.next_update()\n\n    def handle_pull(self, conn, addr):\n        """"""Handles parameter requests coming from the workers. This will\n        actually send the model parameters to the requesting host.\n\n        # Arguments:\n            conn: socket. The opened connection.\n            addr: addr. Address of the remote host.\n        """"""\n        # Fetch the raw center variables.\n        with self.mutex:\n            cv = copy.deepcopy(self.center_variable)\n        # Send the data over the socket.\n        send_data(conn, cv)\n\n    def finalize(self):\n        # Set the weights of the model.\n        self.model.set_weights(self.center_variable)\n'"
distkeras/predictors.py,0,"b'""""""Predictors take a model and will transform the Dataframe by adding a prediction column.""""""\n\n## BEGIN Imports. ##############################################################\n\nimport numpy as np\n\nfrom pyspark.mllib.linalg import DenseVector\n\nfrom distkeras.utils import serialize_keras_model\nfrom distkeras.utils import deserialize_keras_model\nfrom distkeras.utils import new_dataframe_row\n\n## END Imports. ################################################################\n\nclass Predictor(object):\n    """"""Abstract predictor class.\n\n    # Arguments\n        keras_model: Keras Model.\n    """"""\n\n    def __init__(self, keras_model):\n        self.model = serialize_keras_model(keras_model)\n\n    def predict(self, dataframe):\n        """"""Transforms the dataframe to add a prediction.\n\n        # Arguments\n            dataframe: dataframe. Spark Dataframe.\n        """"""\n        raise NotImplementedError\n\n\nclass ModelPredictor(Predictor):\n    """"""Takes a Keras model and adds a prediction column to the dataframe\n       given a features column.\n\n    # Arguments\n        keras_model: Keras model.\n        features_col: string. Name of the features column.\n        output_col: string. Name of the prediction column.\n    """"""\n\n    def __init__(self, keras_model, features_col=""features"", output_col=""prediction""):\n        super(ModelPredictor, self).__init__(keras_model)\n        assert isinstance(features_col, (str, list)), ""\'features_col\' must be a string or a list of strings""\n        self.features_column = [features_col] if isinstance(features_col, str) else features_col\n        self.output_column = output_col\n\n    def _predict(self, iterator):\n        """"""Lambda method which will append a prediction column to the provided rows.\n\n        # Arguments:\n            iterator: iterator. Spark Row iterator.\n        """"""\n        model = deserialize_keras_model(self.model)\n        for row in iterator:\n            features = [np.asarray([row[c]]) for c in self.features_column]\n            prediction = model.predict(features)\n            dense_prediction = DenseVector(prediction[0])\n            new_row = new_dataframe_row(row, self.output_column, dense_prediction)\n            yield new_row\n\n    def predict(self, dataframe):\n        """"""Returns a dataframe which is the old dataframe with an additional\n        prediction column.\n        """"""\n        return dataframe.rdd.mapPartitions(self._predict).toDF()\n'"
distkeras/schemes.py,0,"b'""""""Schemes module.\n\nModule with schemes to automatize a distributed learning process. These schemes will automatically\nadjust the hyperparameters to improve training performance.\n""""""\n\n## BEGIN Imports. ##############################################################\n\nimport math\n\n## END Imports. ################################################################\n\nclass Scheme(object):\n    """"""A \'Scheme\' is way to describe how a distributed optimization sequence\n    should perform. For example, it is responsible for adjusting the learning\n    rate of the parameter server if it notices that the loss doesn\'t decay.\n    However, this is only one of the possible solutions. Others include the\n    optimization of other hyperparameters such as the number of workers.\n\n    # Arguments\n        optimizer: trainer. A distributed optimizer.\n        num_epoch: int. Total number of epoch.\n        evaluation_frequency: int. Frequency of hyperparameter evaluation.\n    """"""\n\n    def __init__(self, optimizer, num_epoch=15, evaluation_frequency=5):\n        self.optimizer = optimizer\n        self.num_epoch = num_epoch\n        self.evaluation_frequency = evaluation_frequency\n        self.epoch_over_eval_frequency = int(self.num_epoch / self.evaluation_frequency)\n        self.initialize()\n\n    def initialize(self):\n        """"""Initializes the hyperparameters to follow the scheme parameters.""""""\n        self.optimizer.set_num_epoch(self.get_epoch_over_evaluation_frequency())\n\n    def get_epoch_over_evaluation_frequency(self):\n        """"""Returns the number of epochs per evaluation frequency.""""""\n        return self.epoch_over_eval_frequency\n\n    def optimize(self, training_set, validation_set):\n        raise NotImplementedError\n\n\nclass Emperor(Scheme):\n    """"""The \'Emporor\' optimization schema will make hyperparameter changes based\n    on the loss derrivatives of the validation set.\n\n    # Arguments\n        optimizer: trainer. A distributed optimizer.\n        evaluate_loss: function. Function which evaluates the loss. This\n                       function should accept a model, and a dataframe.\n        num_epoch: int. Total number of epoch.\n        evaluation_frequency: int. Frequency of hyperparameter evaluation.\n    """"""\n\n    def __init__(self, optimizer, evaluate_loss, num_epoch=15, evaluation_frequency=5,\n                 loss_threshold=0.005):\n        super(Emperor, self).__init__(optimizer, num_epoch, evaluation_frequency)\n        self.previous_loss = float(\'inf\')\n        self.loss_threshold = loss_threshold\n        self.evaluate_loss = evaluate_loss\n\n    def optimize(self, training_set, validation_set):\n        trained_model = None\n\n        # Fetch the number of evaluations, to match the number of epochs.\n        num_evaluations = self.get_epoch_over_evaluation_frequency() + 1\n        # Iterate over the number of evaluation epochs.\n        for i in range(0, num_evaluations):\n            # Train the model.\n            trained_model = self.optimizer.train(training_set)\n            self.optimizer.set_model(trained_model)\n            # Evaluate the training set, and fetch the loss.\n            loss = self.evaluate_loss(trained_model, validation_set)\n            print(""Current loss: "" + str(loss))\n            dl = math.fabs(loss - self.previous_loss)\n            self.previous_loss = loss\n            if dl <= self.loss_threshold:\n                print(""Lowering learning rate."")\n                print(""Old learning rate: "" + str(self.optimizer.get_learning_rate()))\n                # Modify the learning rate.\n                learning_rate = self.optimizer.get_learning_rate()\n                learning_rate /= 10\n                self.optimizer.set_learning_rate(learning_rate)\n                print(""New learning rate: ""+ str(self.optimizer.get_learning_rate()))\n\n        return trained_model\n'"
distkeras/trainers.py,0,"b'""""""Model optimizers. Depending on the implementation, these classes will optimize the\nKeras model in a distributed manner (with exception of the SingleTrainer).""""""\n\n## BEGIN Imports. ##############################################################\n\nimport numpy as np\n\nimport threading\n\nimport time\n\nfrom distkeras.parameter_servers import ADAGParameterServer\nfrom distkeras.parameter_servers import DeltaParameterServer\nfrom distkeras.parameter_servers import DynSGDParameterServer\nfrom distkeras.parameter_servers import ExperimentalParameterServer\n\nfrom distkeras.utils import deserialize_keras_model\nfrom distkeras.utils import history_executor\nfrom distkeras.utils import history_executors_average\nfrom distkeras.utils import pickle_object\nfrom distkeras.utils import serialize_keras_model\nfrom distkeras.utils import set_keras_base_directory\nfrom distkeras.utils import unpickle_object\n\nfrom distkeras.networking import determine_host_address\n\nfrom distkeras.workers import ADAGWorker\nfrom distkeras.workers import AEASGDWorker\nfrom distkeras.workers import DOWNPOURWorker\nfrom distkeras.workers import DynSGDWorker\nfrom distkeras.workers import ExperimentalWorker\nfrom distkeras.workers import EAMSGDWorker\nfrom distkeras.workers import SequentialWorker\n\nfrom keras import backend as K\n\n## END Imports. ################################################################\n\nclass Trainer(object):\n    """"""Abstract trainer class. This class provides all base functionality which\n    all optimizers need to implement.\n\n    # Arguments\n        keras_model: Keras model.\n        loss: string. String representing the loss.\n              See: https://keras.io/objectives/\n        worker_optimizer: string. String representing worker optimizer.\n                          See https://keras.io/optimizers/\n        metrics: list of strings representing model evaluation metrics. Default is [""accuracy""].\n                 See: https://keras.io/metrics/\n        loss_weights: optional list or dict specifying weights for different losses.\n    """"""\n\n    def __init__(self, keras_model, loss, worker_optimizer, metrics=[""accuracy""], loss_weights=None):\n        set_keras_base_directory()\n        self.master_model = serialize_keras_model(keras_model)\n        self.loss = loss\n        self.loss_weights = loss_weights\n        self.worker_optimizer = worker_optimizer\n        self.metrics = metrics\n        self.history = []\n        self.training_time_start = 0\n        self.training_time_end = 0\n        self.training_time = 0\n        self.max_mini_batches_prefetch = 100\n\n    def set_max_prefetch(self, max_mini_batches):\n        """"""Sets the maximum amount of mini-batches that can be prefetched by a worker.""""""\n        self.max_mini_batches_prefetch = max_mini_batches\n\n    def set_model(self, model):\n        """"""Sets the master model to be used by the trainer.""""""\n        self.master_model = serialize_keras_model(model)\n\n    def record_training_start(self):\n        """"""Records the start of the training.\n\n        This private function is called when the training process starts.\n        """"""\n        self.training_time = 0\n        self.training_time_start = time.time()\n\n    def record_training_end(self):\n        """"""Records the end of the traing.\n\n        This private function is called when the training process is terminated.\n        """"""\n        self.training_time_end = time.time()\n        self.training_time = self.training_time_end - self.training_time_start\n\n    def get_training_time(self):\n        """"""Returns the told training time.""""""\n        return self.training_time\n\n    def get_history(self):\n        """"""Returns all history object aggregated during training.""""""\n        return self.history\n\n    def get_averaged_history(self):\n        """"""Returns the averaged history of the center variable.""""""\n        return history_executors_average(self.history)\n\n    def get_executor_history(self, executor_id):\n        """"""Returns the history of a specific executor.""""""\n        return history_executor(self.history, executor_id)\n\n    def train(self, dataframe, shuffle=False):\n        """"""Trains the specified model using the specified dataframe.\n\n        # Arguments\n            dataframe: dataframe. A Spark Dataframe containing the training data.\n            shuffle: boolean. Tells to shuffle the dataframe before training.\n                     Warning: this will tell Spark to shuffle all partitions over\n                     the network. It is recommended to shuffle the dataframe before\n                     training and store it.\n        """"""\n        raise NotImplementedError\n\n    def serialize(self):\n        return pickle_object(self)\n\n\nclass SingleTrainer(Trainer):\n    """"""An optimizer which will train a network on a single machine.\n\n    # Arguments\n        keras_model: model. Keras model to train.\n        worker_optimizer: string. String representing worker optimizer.\n                          See https://keras.io/optimizers/\n        loss: string. String representing the loss.\n              See: https://keras.io/objectives/\n        metrics: list of strings representing model evaluation metrics. Default is [""accuracy""].\n                 See: https://keras.io/metrics/\n        features_col: string or list of strings. Name(s) of the features column(s).\n        label_col: string or list of strings. Name(s) of the label column(s).\n        num_epoch: int. Number of epochs.\n        batch_size: int. Mini-batch size.\n        loss_weights: optional list or dict specifying weights for different losses.\n    """"""\n\n    def __init__(self, keras_model, worker_optimizer, loss, metrics=[""accuracy""], features_col=""features"",\n                 label_col=""label"", num_epoch=1, batch_size=32, loss_weights=None):\n        super(SingleTrainer, self).__init__(keras_model, loss, worker_optimizer, metrics, loss_weights)\n        self.features_column = features_col\n        self.label_column = label_col\n        self.num_epoch = num_epoch\n        self.batch_size = batch_size\n\n    def allocate_worker(self):\n        """"""Allocates a worker for the Single Trainer instance.\n\n        Only for internal use.\n        """"""\n        worker = SequentialWorker(model=self.master_model, features_col=self.features_column,\n                                  label_col=self.label_column, batch_size=self.batch_size, num_epoch = self.num_epoch,\n                                  optimizer=self.worker_optimizer, loss=self.loss, loss_weights=self.loss_weights, \n                                  metrics = self.metrics)\n\n        return worker\n\n    def train(self, dataframe, shuffle=False):\n        """"""See distkeras.trainers.Trainer.train\n\n        # Arguments\n            dataframe: dataframe. A Spark Dataframe containing the training data.\n            shuffle: boolean. Tells to shuffle the dataframe before training.\n                     Warning: this will tell Spark to shuffle all partitions over\n                     the network. It is recommended to shuffle the dataframe before\n                     training and store it.\n        """"""\n        # Check if the data needs to be shuffled.\n        if shuffle:\n            dataframe = shuffle(dataframe)\n        # Collect the dataframe on a single worker node.\n        dataframe = dataframe.coalesce(1)\n        # Cache the dataframe.\n        dataframe.cache()\n        # Allocate a worker.\n        worker = self.allocate_worker()\n        # Set the maximum number of mini-batches.\n        worker.set_max_prefetch(self.max_mini_batches_prefetch)\n        # Start recording training time.\n        self.record_training_start()\n        # Fetch the trained model.\n        self.master_model = dataframe.rdd.mapPartitionsWithIndex(worker.train).collect()[0]\n        # Stop recording of training time.\n        self.record_training_end()\n\n        return deserialize_keras_model(self.master_model)\n\n\nclass AveragingTrainer(Trainer):\n    """"""A trainer which implements a data parallel technique using model averaging.\n\n    In this implementation, the model replicas are averages after every epoch.\n    # Arguments\n        keras_model: model. Keras model to train.\n        worker_optimizer: string. String representing worker optimizer.\n                          See https://keras.io/optimizers/\n        loss: string. String representing the loss.\n              See: https://keras.io/objectives/\n        metrics: list of strings representing model evaluation metrics. Default is [""accuracy""].\n                 See: https://keras.io/metrics/\n        features_col: string or list of strings. Name(s) of the features column(s).\n        label_col: string or list of strings. Name(s) of the label column(s).\n        num_epoch: int. Number of epochs.\n        batch_size: int. Mini-batch size.\n        num_workers: int. Number of model replicas to train in parallel.\n        loss_weights: optional list or dict specifying weights for different losses.\n    """"""\n\n    def __init__(self, keras_model, worker_optimizer, loss, metrics=[""accuracy""], features_col=""features"",\n                 label_col=""label"", num_epoch=1, batch_size=32, num_workers=2, loss_weights=None):\n        super(AveragingTrainer, self).__init__(keras_model, loss, worker_optimizer, metrics, loss_weights)\n        self.features_column = features_col\n        self.label_column = label_col\n        self.num_epoch = num_epoch\n        self.batch_size = batch_size\n        self.num_workers = num_workers\n        self.parameter_buffer = np.asarray(keras_model.get_weights())\n        self.parameter_buffer.fill(0.0)\n\n    def average_models(self, models):\n        """"""Averages the specified list of Keras models, and assigns the\n        averaged model as the master model.\n\n        # Arguments:\n            models: list. A list of serialized Keras models.\n        """"""\n        num_models = len(models)\n        # Get all weights of the models.\n        for i in range(0, num_models):\n            weights = np.asarray(deserialize_keras_model(models[i]).get_weights())\n            self.parameter_buffer += weights\n        # Average the parameters.\n        self.parameter_buffer /= num_models\n        temp_model = deserialize_keras_model(self.master_model)\n        temp_model.set_weights(self.parameter_buffer)\n        self.master_model = serialize_keras_model(temp_model)\n\n\n    def allocate_worker(self):\n        """"""Allocates the AveragingWorker for internal use.""""""\n        worker = SequentialWorker(model=self.master_model, features_col=self.features_column,\n                                  label_col=self.label_column, batch_size=self.batch_size, num_epoch = 1,\n                                  optimizer=self.worker_optimizer, loss=self.loss, loss_weights=self.loss_weights, metrics = self.metrics)\n\n        return worker\n\n    def train(self, dataframe, shuffle=False):\n        """"""Applies model averaging to the model replicas distributed over the specified\n        number of Spark executors.\n\n        # Arguments\n            dataframe: dataframe: A Spark Dataframe containing the training data.\n            shuffle: boolean. Tells to shuffle the dataframe before training.\n                     Warning: this will tell Spark to shuffle all partitions over\n                     the network. It is recommended to shuffle the dataframe before\n                     training and store it.\n        """"""\n        # Repartition the data in order to fit the number of workers.\n        num_partitions = dataframe.rdd.getNumPartitions()\n        # Check if the dataframe needs to be shuffled.\n        if shuffle:\n            dataframe = shuffle(dataframe)\n        # Check if we need to repartition the dataframe.\n        if num_partitions >= self.num_workers:\n            dataframe = dataframe.coalesce(self.num_workers)\n        else:\n            dataframe = dataframe.repartition(self.num_workers)\n        # Start the training procedure.\n        self.record_training_start()\n        for i in range(0, self.num_epoch):\n            worker = self.allocate_worker()\n            # Set the maximum number of mini-batches.\n            worker.set_max_prefetch(self.max_mini_batches_prefetch)\n            models = dataframe.rdd.mapPartitionsWithIndex(worker.train).collect()\n            self.average_models(models)\n        # End the training procedure.\n        self.record_training_end()\n\n        return deserialize_keras_model(self.master_model)\n\n\nclass EnsembleTrainer(Trainer):\n    """"""Utility trainer which will train ensemble methods in parallel.\n\n    # Arguments\n        keras_model: model. Keras model to train.\n        worker_optimizer: string. String representing worker optimizer.\n                          See https://keras.io/optimizers/\n        loss: string. String representing the loss.\n              See: https://keras.io/objectives/\n        metrics: list of strings representing model evaluation metrics. Default is [""accuracy""].\n                 See: https://keras.io/metrics/\n        features_col: string or list of strings. Name(s) of the features column(s).\n        label_col: string or list of strings. Name(s) of the label column(s).\n        batch_size: int. Mini-batch size.\n        num_ensembles: int. Number of ensembles to train.\n        loss_weights: optional list or dict specifying weights for different losses.\n    # Note\n        This will note employ a data-parallell approach for the ensembles.\n    """"""\n\n    def __init__(self, keras_model, worker_optimizer, loss, metrics=[""accuracy""], features_col=""features"",\n                 label_col=""label"", batch_size=32, num_ensembles=2, loss_weights=None):\n        super(EnsembleTrainer, self).__init__(keras_model, loss, worker_optimizer, metrics, loss_weights)\n        self.features_column = features_col\n        self.label_column = label_col\n        self.batch_size = batch_size\n        self.num_ensembles = num_ensembles\n\n    def allocate_worker(self):\n        """"""Allocates the EnsembleWorker for internal use.""""""\n        worker = SequentialWorker(model=self.master_model, features_col=self.features_column,\n                                  label_col=self.label_column, batch_size=self.batch_size, num_epoch = self.num_epoch,\n                                  optimizer=self.worker_optimizer, loss=self.loss, loss_weights=self.loss_weights, metrics=self.metrics)\n\n        return worker\n\n    def train(self, dataframe, shuffle=False):\n        """"""Trains the specified number of ensemble models using the specified dataframe.\n\n        # Arguments\n            dataframe: dataframe. A Spark Dataframe containing the training data.\n            shuffle: boolean. Tells to shuffle the dataframe before training.\n                     Warning: this will tell Spark to shuffle all partitions over\n                     the network. It is recommended to shuffle the dataframe before\n                     training and store it.\n        """"""\n        # Allocate a worker.\n        worker = self.allocate_worker()\n        # Set the maximum number of mini-batches.\n        worker.set_max_prefetch(self.max_mini_batches_prefetch)\n        # Repartition in order to fit the number of workers.\n        num_partitions = dataframe.rdd.getNumPartitions()\n        # Check if the dataframe needs to be shuffled before training.\n        if shuffle:\n            dataframe = shuffle(dataframe)\n        # Check if we need to repartition the dataframe.\n        if num_partitions >= self.num_workers:\n            dataframe = dataframe.coalesce(self.num_workers)\n        else:\n            dataframe = dataframe.repartition(self.num_workers)\n        # Start the training procedure.\n        self.record_training_start()\n        # Train the models in parallel.\n        models = dataframe.rdd.mapPartitionsWithIndex(worker.train).collect()\n        # End the training procedure.\n        self.record_training_end()\n\n        return models\n\n\nclass DistributedTrainer(Trainer):\n    """"""Abstract class which describes the properties of a distributed optimizer.\n\n    # Arguments\n        keras_model: model. Keras model to train.\n        worker_optimizer: string. String representing worker optimizer.\n                          See https://keras.io/optimizers/\n        loss: string. String representing the loss.\n              See: https://keras.io/objectives/\n        metrics: list of strings representing model evaluation metrics. Default is [""accuracy""].\n                 See: https://keras.io/metrics/\n        features_col: string or list of strings. Name(s) of the features column(s).\n        label_col: string or list of strings. Name(s) of the label column(s).\n        num_epoch: int. Number of epochs.\n        batch_size: int. Mini-batch size.\n        num_workers: int. Number of distributed workers.\n        master_port: int. port number for the parameter server.\n        loss_weights: optional list or dict specifying weights for different losses.\n    """"""\n\n    def __init__(self, keras_model, worker_optimizer, loss, metrics=[""accuracy""], num_workers=2, batch_size=32,\n                 features_col=""features"", label_col=""label"", num_epoch=1, master_port=5000, loss_weights=None):\n        super(DistributedTrainer, self).__init__(keras_model, loss, worker_optimizer, metrics, loss_weights)\n        self.num_workers = num_workers\n        self.batch_size = batch_size\n        self.features_column = features_col\n        self.label_column = label_col\n        self.num_epoch = num_epoch\n        self.parameter_server = None\n        self.parameter_server_thread = None\n        self.master_host = determine_host_address()\n        self.master_port = master_port\n        self.learning_rate = 1.0\n\n    def set_minibatch_size(self, size):\n        """"""Sets the size of the mini-batch.""""""\n        self.batch_size = size\n\n    def get_minibatch_size(self):\n        """"""Returns the size of the mini-batch.""""""\n        return self.batch_size\n\n    def get_features_column(self):\n        """"""Returns the name of the features column.""""""\n        return self.features_column\n\n    def get_label_column(self):\n        """"""Returns the name of the label column.""""""\n        return self.label_column\n\n    def get_learning_rate(self):\n        """"""Returns the learning rate of the worker which can be tuned by\n        the parameter server, or optimization scheme.\n\n        Note: this learning rate is independent of the learning rate of the optimizer.\n        """"""\n        return self.learning_rate\n\n    def set_learning_rate(self, learning_rate):\n        """"""Sets the learning rate which can be tuned by the parameter server,\n        or optimization scheme.\n\n        Note: this learning rate is independent of the learning rate of the optimizer.\n        """"""\n        self.learning_rate = learning_rate\n\n    def set_num_epoch(self, num_epoch):\n        """"""Sets the number of epochs.""""""\n        self.num_epoch = num_epoch\n\n    def get_num_epoch(self):\n        """"""Returns the number of epochs.""""""\n        return self.num_epoch\n\n    def allocate_worker(self):\n        """"""Allocates the worker implementation.\n\n        Implement this method in subclasses.\n        """"""\n        raise NotImplementedError\n\n    def set_master(self, master):\n        """"""Sets the master address of the parameter server.""""""\n        self.master_host = master\n\n    def determine_new_master(self):\n        """"""Sets the new master address to the current host.""""""\n        self.master_host = determine_host_address()\n\n    def allocate_parameter_server(self):\n        """"""Allocates the parameter server.\n\n        If an other type of parameter server is required, you can overwrite\n        this implementation.\n        """"""\n        parameter_server = DeltaParameterServer(self.master_model, self.master_port)\n\n        return parameter_server\n\n    def set_num_workers(self, num_workers):\n        """"""Sets the number of parallel workers to use.""""""\n        self.num_workers = num_workers\n\n    def get_num_workers(self):\n        """"""Returns the number of parallel workers.""""""\n        return self.num_workers\n\n    def num_updates(self):\n        """"""Returns the number of model updates the parameter server performed.""""""\n        return self.parameter_server.num_updates()\n\n    def service(self):\n        """"""Executes the parameter server service.""""""\n        self.parameter_server.start()\n        self.parameter_server.initialize()\n        self.parameter_server.run()\n\n    def stop_service(self):\n        """"""Stops the parameter server service.""""""\n        self.parameter_server.stop()\n        self.parameter_server_thread.join()\n        self.parameter_server_thread = None\n\n    def start_service(self):\n        """"""Starts the parameter server service.""""""\n        # Check if a parameter server thread is already allocated.\n        if not self.parameter_server_thread is None:\n            # Stop the parameter server service.\n            self.stop_service()\n        # Allocate a new parameter service thread.\n        self.parameter_server_thread = threading.Thread(target=self.service)\n        self.parameter_server_thread.start()\n\n    def train(self, dataframe, shuffle=False):\n        """"""Training procedure of a distributed optimization process.\n\n        # Arguments\n            dataframe: dataframe. A Spark Dataframe containing the training data.\n            shuffle: boolean. Tells to shuffle the dataframe before training.\n                     Warning: this will tell Spark to shuffle all partitions over\n                     the network. It is recommended to shuffle the dataframe before\n                     training and store it.\n        """"""\n        # Check if a parameter server has been allocated.\n        if self.parameter_server is not None:\n            # Cleanup the old parameter server.\n            self.parameter_server.stop()\n            self.parameter_server = None\n        # Allocate the parameter server.\n        self.parameter_server = self.allocate_parameter_server()\n        # Start the communication service.\n        self.start_service()\n        # Allocate a worker.\n        worker = self.allocate_worker()\n        # Set the maximum number of mini-batches.\n        worker.set_max_prefetch(self.max_mini_batches_prefetch)\n        # Repartition in order to fit the number of workers.\n        num_partitions = dataframe.rdd.getNumPartitions()\n        # Check if the dataframe needs to be shuffled before training.\n        if shuffle:\n            dataframe = shuffle(dataframe)\n        # Check if we need to repartition the dataframe.\n        if num_partitions >= self.num_workers:\n            dataframe = dataframe.coalesce(self.num_workers)\n        else:\n            dataframe = dataframe.repartition(self.num_workers)\n        # Cache the dataframe.\n        dataframe.cache()\n        # Start the training procedure.\n        self.record_training_start()\n        # Iterate through the epochs.\n        self.history = dataframe.rdd.mapPartitionsWithIndex(worker.train).collect()\n        # End the training procedure.\n        self.record_training_end()\n        # Stop the communication service.\n        self.stop_service()\n\n        return self.parameter_server.get_model()\n\n\nclass AsynchronousDistributedTrainer(DistributedTrainer):\n    """"""Abstract class for an asynchronous distributed trainer.\n\n    This trainer also allows us to set a parallelism factor. This parallelism factor allows\n    us to further parallelize the Spark job. For example, imagine having n machines optimizing\n    a model in an asynchronous distributed setting. If for some, but likely reason, some machines\n    are performing worse compared to others. It will cause the complete learning procedure to be\n    stuck on this one particular machine since every machine will be assigned a single partition.\n    In order to resolve this, we added a parallelization factor. This factor indicates the ratio\n    of the number of jobs per machine (executor). For small dataframes, we recommend that this factor\n    is set to 1. However, this effect really is prominent when the dataframe is large. In this case\n    we recommend that the ratio is 2 or 3.\n\n    # Arguments\n        keras_model: model. Keras model to train.\n        worker_optimizer: string. String representing worker optimizer.\n                          See https://keras.io/optimizers/\n        loss: string. String representing the loss.\n              See: https://keras.io/objectives/\n        metrics: list of strings representing model evaluation metrics. Default is [""accuracy""].\n                 See: https://keras.io/metrics/\n        features_col: string or list of strings. Name(s) of the features column(s).\n        label_col: string or list of strings. Name(s) of the label column(s).\n        num_epoch: int. Number of epochs.\n        batch_size: int. Mini-batch size.\n        num_workers: int. Number of distributed workers.\n        master_port: int. port number for the parameter server.\n        loss_weights: optional list or dict specifying weights for different losses.\n\n    # Note\n        By default, the parallelization factor is set to 1.\n    """"""\n\n    def __init__(self, keras_model, worker_optimizer, loss, metrics=[""accuracy""], num_workers=2, batch_size=32,\n                 features_col=""features"", label_col=""label"", num_epoch=1, master_port=5000, loss_weights=None):\n        super(AsynchronousDistributedTrainer, self).__init__(keras_model, worker_optimizer, loss, metrics, \n                                                             num_workers, batch_size, features_col,\n                                                             label_col, num_epoch, master_port, loss_weights)\n        # Initialize asynchronous methods variables.\n        self.parallelism_factor = 1\n\n    def allocate_worker(self):\n        """"""Allocates the worker implementation.\n\n        Implement this method in subclasses.\n        """"""\n        raise NotImplementedError\n\n    def set_parallelism_factor(self, factor):\n        """"""Sets the parallelization factor.\n\n        # Arguments\n            factor: int. The new parallelization factor.\n        """"""\n        self.parallelism_factor = factor\n\n    def get_parallelism_factor(self):\n        """"""Returns the parallelization factor.""""""\n        return self.parallelism_factor\n\n    def train(self, dataframe, shuffle=False):\n        """"""Training procedure of an asynchronous distributed optimization process.\n\n        # Arguments\n            dataframe: dataframe. A Spark Dataframe containing the training data.\n            shuffle: boolean. Tells to shuffle the dataframe before training.\n                     Warning: this will tell Spark to shuffle all partitions over\n                     the network. It is recommended to shuffle the dataframe before\n                     training and store it.\n        """"""\n        # Check if a parameter server has been allocated.\n        if self.parameter_server is not None:\n            # Cleanup the old parameter server.\n            self.parameter_server.stop()\n            self.parameter_server = None\n        # Allocate the parameter server.\n        self.parameter_server = self.allocate_parameter_server()\n        # Start the communication service.\n        self.start_service()\n        # Allocate a worker.\n        worker = self.allocate_worker()\n        # Set the maximum number of mini-batches.\n        worker.set_max_prefetch(self.max_mini_batches_prefetch)\n        # Repartition in order to fit the number of workers.\n        num_partitions = dataframe.rdd.getNumPartitions()\n        # Check if the dataframe needs to be shuffled before training.\n        if shuffle:\n            dataframe = shuffle(dataframe)\n        # Indicate the parallelism (number of worker times parallelism factor).\n        parallelism = self.parallelism_factor * self.num_workers\n        # Check if we need to repartition the dataframe.\n        if num_partitions >= parallelism:\n            dataframe = dataframe.coalesce(parallelism)\n        else:\n            dataframe = dataframe.repartition(parallelism)\n        # Start the training procedure.\n        self.record_training_start()\n        # Iterate through the epochs.\n        self.history = dataframe.rdd.mapPartitionsWithIndex(worker.train).collect()\n        # End the training procedure.\n        self.record_training_end()\n        # Stop the communication service.\n        self.stop_service()\n\n        return self.parameter_server.get_model()\n\n\nclass AEASGD(AsynchronousDistributedTrainer):\n    """"""Asynchronous Elastic Averaging SGD optimizer.\n    Introduced by Zhang et al.\n    https://arxiv.org/pdf/1412.6651.pdf\n    # Arguments\n        keras_model: model. Keras model to train.\n        worker_optimizer: string. String representing worker optimizer.\n                          See https://keras.io/optimizers/\n        loss: string. String representing the loss.\n              See: https://keras.io/objectives/\n        metrics: list of strings representing model evaluation metrics. Default is [""accuracy""].\n                 See: https://keras.io/metrics/\n        features_col: string or list of strings. Name(s) of the features column(s).\n        label_col: string or list of strings. Name(s) of the label column(s).\n        num_epoch: int. Number of epochs.\n        batch_size: int. Mini-batch size.\n        num_workers: int. Number of distributed workers.\n        communication_window: int. Staleness parameter.\n                              This parameter describes the number of mini-batches that will be\n                              computed before updating the center variable. For EASGD based\n                              algorithms we recommend large communication windows.\n        learning_rate: float. Learning rate.\n        rho: float. Elastic ""exploration"" variable.\n                    Higher values mean that the model is allowed to ""explore"" its surroundings.\n                    Smaller values are correlated with less exploration. We use the value\n                    recommend by the authors.\n        master_port: int. port number for the parameter server.\n        loss_weights: optional list or dict specifying weights for different losses.\n    """"""\n\n    def __init__(self, keras_model, worker_optimizer, loss, metrics=[""accuracy""], num_workers=2, batch_size=32,\n                 features_col=""features"", label_col=""label"", num_epoch=1, communication_window=32,\n                 rho=5.0, learning_rate=0.1, master_port=5000, loss_weights=None):\n        super(AEASGD, self).__init__(keras_model, worker_optimizer, loss, metrics, num_workers,\n                                     batch_size, features_col, label_col, num_epoch, master_port, loss_weights)\n        self.communication_window = communication_window\n        self.rho = rho\n        self.learning_rate = learning_rate\n\n    def allocate_worker(self):\n        """"""Allocates the asynchronous EASGD worker.""""""\n        # Allocate a AEASGD worker.\n        worker = AEASGDWorker(self.master_model, self.worker_optimizer, self.loss, self.loss_weights, self.metrics,\n                              self.features_column, self.label_column, self.batch_size, self.num_epoch,\n                              self.master_host, self.master_port, self.rho, self.learning_rate,\n                              self.communication_window)\n\n        return worker\n\n\nclass DOWNPOUR(AsynchronousDistributedTrainer):\n    """"""DOWNPOUR Optimizer.\n\n    Asynchronous data-parallel optimizer introduced by Dean et al.\n    http://static.googleusercontent.com/media/research.google.com/en/archive/large_deep_networks_nips2012.pdf\n\n    # Arguments\n        keras_model: model. Keras model to train.\n        worker_optimizer: string. String representing worker optimizer.\n                          See https://keras.io/optimizers/\n        loss: string. String representing the loss.\n              See: https://keras.io/objectives/\n        metrics: list of strings representing model evaluation metrics. Default is [""accuracy""].\n                 See: https://keras.io/metrics/\n        features_col: string or list of strings. Name(s) of the features column(s).\n        label_col: string or list of strings. Name(s) of the label column(s).\n        num_epoch: int. Number of epochs.\n        batch_size: int. Mini-batch size.\n        num_workers: int. Number of distributed workers.\n        communication_window: int. Staleness parameter.\n                              This parameter describes the number of mini-batches that will be\n                              computed before updating the center variable. For DOWNPOUR we\n                              recommend small communication windows.\n        learning_rate: float. Learning rate.\n        master_port: int. port number for the parameter server.\n        loss_weights: optional list or dict specifying weights for different losses.\n    """"""\n\n    def __init__(self, keras_model, worker_optimizer, loss, metrics=[""accuracy""], num_workers=2, batch_size=32,\n                 features_col=""features"", label_col=""label"", num_epoch=1, communication_window=5, master_port=5000, loss_weights=None):\n        super(DOWNPOUR, self).__init__(keras_model, worker_optimizer, loss, metrics, num_workers,\n                                       batch_size, features_col, label_col, num_epoch, master_port, loss_weights)\n        self.communication_window = communication_window\n\n    def allocate_worker(self):\n        """"""Allocates the DOWNPOUR worker.""""""\n        # Allocate DOWNPOUR worker.\n        worker = DOWNPOURWorker(self.master_model, self.worker_optimizer, self.loss, self.loss_weights, self.metrics,\n                                self.features_column, self.label_column, self.batch_size, self.num_epoch,\n                                self.master_host, self.master_port, self.communication_window)\n\n        return worker\n\n\nclass EAMSGD(AsynchronousDistributedTrainer):\n    """"""Asynchronous Elastic Averaging w/ Momentum SGD optimizer.\n\n    Introduced by Zhang et al.\n    https://arxiv.org/pdf/1412.6651.pdf\n\n    # Arguments\n        keras_model: model. Keras model to train.\n        worker_optimizer: string. String representing worker optimizer.\n                          See https://keras.io/optimizers/\n        loss: string. String representing the loss.\n              See: https://keras.io/objectives/\n        metrics: list of strings representing model evaluation metrics. Default is [""accuracy""].\n                 See: https://keras.io/metrics/\n        features_col: string or list of strings. Name(s) of the features column(s).\n        label_col: string or list of strings. Name(s) of the label column(s).\n        num_epoch: int. Number of epochs.\n        batch_size: int. Mini-batch size.\n        num_workers: int. Number of distributed workers.\n        communication_window: int. Staleness parameter.\n                              This parameter describes the number of mini-batches that will be\n                              computed before updating the center variable. For EASGD based\n                              algorithms we recommend large communication windows.\n        learning_rate: float. Learning rate.\n        rho: float. Elastic ""exploration"" variable.\n                    Higher values mean that the model is allowed to ""explore"" its surroundings.\n                    Smaller values are correlated with less exploration. We use the value\n                    recommend by the authors.\n        momentum: float. Momentum term.\n        master_port: int. port number for the parameter server.\n        loss_weights: optional list or dict specifying weights for different losses.\n    """"""\n\n    def __init__(self, keras_model, worker_optimizer, loss, metrics=[""accuracy""], num_workers=2, batch_size=32,\n                 features_col=""features"", label_col=""label"", num_epoch=1, communication_window=32,\n                 rho=5.0, learning_rate=0.1, momentum=0.9, master_port=5000, loss_weights=None):\n        super(EAMSGD, self).__init__(keras_model, worker_optimizer, loss, metrics, num_workers,\n                                     batch_size, features_col, label_col, num_epoch, master_port, loss_weights)\n        self.communication_window = communication_window\n        self.rho = rho\n        self.learning_rate = learning_rate\n        self.momentum = momentum\n\n    def allocate_worker(self):\n        """"""Allocates the asynchronous EAMSGD worker.""""""\n        # Allocate a EAMSGD REST worker.\n        worker = EAMSGDWorker(self.master_model, self.worker_optimizer, self.loss, self.loss_weights, self.metrics,\n                              self.features_column, self.label_column, self.batch_size, self.num_epoch,\n                              self.master_host, self.master_port, self.rho, self.learning_rate,\n                              self.momentum, self.communication_window)\n\n        return worker\n\n\nclass ADAG(AsynchronousDistributedTrainer):\n    """"""Asynchronous Distributed Adaptive Gradient (Stochastic Gradient Descent).\n\n    Introduced by Hermans et al.\n\n    # Arguments:\n        keras_model: model. Keras model to train.\n        worker_optimizer: string. String representing worker optimizer.\n                          See: https://keras.io/optimizers/\n        loss: string. String representing the loss function.\n              See: https://keras.io/objectives/\n        metrics: list of strings representing model evaluation metrics. Default is [""accuracy""].\n                 See: https://keras.io/metrics/\n        features_col: string or list of strings. Name(s) of the features column(s).\n        num_epoch: int. Number of epochs.\n        batch_size: int. Mini-batch size.\n        num_workers: int. Number of distributed workers.\n        communication_window: int. Staleness parameter.\n                              This parameter describes the number of mini-batches that will be\n                              computed before updating the center variable. For DOWNPOUR based\n                              algorithms we recommend large communication windows.\n        master_port: int. port number for the parameter server.\n        loss_weights: optional list or dict specifying weights for different losses.\n    """"""\n\n    def __init__(self, keras_model, worker_optimizer, loss, metrics=[""accuracy""], num_workers=2, batch_size=32,\n                 features_col=""features"", label_col=""label"", num_epoch=1, communication_window=12, master_port=5000, loss_weights=None):\n        # Initialize the parent object.\n        super(ADAG, self).__init__(keras_model, worker_optimizer, loss, metrics, num_workers,\n                                   batch_size, features_col, label_col, num_epoch, master_port, loss_weights)\n        # Set algorithm parameters.\n        self.communication_window = communication_window\n\n    def allocate_worker(self):\n        """"""Allocate an Adag worker.""""""\n        worker = ADAGWorker(self.master_model, self.worker_optimizer, self.loss, self.loss_weights, self.metrics,\n                            self.features_column, self.label_column, self.batch_size, self.num_epoch,\n                            self.master_host, self.master_port, self.communication_window)\n\n        return worker\n\n    def allocate_parameter_server(self):\n        """"""Allocate the Adag parameter server.""""""\n        parameter_server = ADAGParameterServer(self.master_model, self.master_port)\n\n        return parameter_server\n\n\nclass DynSGD(AsynchronousDistributedTrainer):\n    """"""Dynamic SGD, dynamically maintains learning rate for every worker\n    and incorperates staleness.\n\n    Introduced in SIGMOD 2017 ""Heterogenity-aware Parameter Servers""\n    http://net.pku.edu.cn/~cuibin/Papers/2017SIGMOD.pdf\n\n    # Arguments:\n        keras_model: model. Keras model to train.\n        worker_optimizer: string. String representing worker optimizer.\n                          See: https://keras.io/optimizers/\n        loss: string. String representing the loss function.\n              See: https://keras.io/objectives/\n        metrics: list of strings representing model evaluation metrics. Default is [""accuracy""].\n                 See: https://keras.io/metrics/\n        features_col: string or list of strings. Name(s) of the features column(s).\n        num_epoch: int. Number of epochs.\n        batch_size: int. Mini-batch size.\n        num_workers: int. Number of distributed workers.\n        communication_window: int. Staleness parameter.\n                              This parameter describes the number of mini-batches that will be\n                              computed before updating the center variable. For DOWNPOUR based\n                              algorithms we recommend large communication windows.\n        master_port: int. port number for the parameter server.\n        loss_weights: optional list or dict specifying weights for different losses.\n    """"""\n\n    def __init__(self, keras_model, worker_optimizer, loss, metrics=[""accuracy""], num_workers=2, batch_size=32,\n                 features_col=""features"", label_col=""label"", num_epoch=1, communication_window=5, master_port=5000, loss_weights=None):\n        # Initialize the parent object.\n        super(DynSGD, self).__init__(keras_model, worker_optimizer, loss, metrics, num_workers,\n                                     batch_size, features_col, label_col, num_epoch, master_port, loss_weights)\n        # Set algorithm parameters.\n        self.communication_window = communication_window\n\n    def allocate_worker(self):\n        """"""Allocate DYNSGD worker.""""""\n        worker = DynSGDWorker(self.master_model, self.worker_optimizer, self.loss, self.loss_weights, self.metrics,\n                              self.features_column, self.label_column, self.batch_size, self.num_epoch,\n                              self.master_host, self.master_port, self.communication_window)\n\n        return worker\n\n    def allocate_parameter_server(self):\n        """"""Allocate DYNSGD parameter server.""""""\n        parameter_server = DynSGDParameterServer(self.master_model, self.master_port)\n\n        return parameter_server\n\n\nclass Experimental(AsynchronousDistributedTrainer):\n    """"""Experimental optimization scheme for development purposes.""""""\n\n    def __init__(self, keras_model, worker_optimizer, loss, metrics=[""accuracy""], num_workers=2, batch_size=32,\n                 features_col=""features"", label_col=""label"", num_epoch=1, communication_window=5,\n                 learning_rate=1.0, master_port=5000, loss_weights=None):\n        # Initialize the parent object.\n        super(Experimental, self).__init__(keras_model, worker_optimizer, loss, metrics, num_workers,\n                                           batch_size, features_col, label_col, num_epoch, master_port, loss_weights)\n        # Set the algorithm parameters.\n        self.communication_window = communication_window\n        self.learning_rate = learning_rate\n\n    def allocate_worker(self):\n        """"""Allocate experimental worker.""""""\n        worker = ExperimentalWorker(self.master_model, self.worker_optimizer, self.loss, self.loss_weights, self.metrics,\n                                    self.features_column, self.label_column, self.batch_size, self.num_epoch,\n                                    self.master_host, self.master_port, self.communication_window,\n                                    self.num_workers, self.learning_rate)\n\n        return worker\n\n    def allocate_parameter_server(self):\n        """"""Allocate experimental parameter server.""""""\n        parameter_server = ExperimentalParameterServer(self.master_model, self.master_port, self.learning_rate)\n\n        return parameter_server\n'"
distkeras/transformers.py,0,"b'""""""Commonly used Dataframe transformers.\n\nA transformer will ""transform"" a Spark dataframe from one form into\nthe other. For example, mapping the column to an other value, or adding\na column to a dataframe based on a collection of specified values.\n""""""\n\n## BEGIN Imports. ##############################################################\n\nimport numpy as np\n\nfrom distkeras.utils import new_dataframe_row\nfrom distkeras.utils import to_one_hot_encoded_dense\n\nfrom pyspark.mllib.linalg import DenseMatrix\nfrom pyspark.mllib.linalg import DenseVector\n\nfrom pyspark.sql.functions import mean\nfrom pyspark.sql.functions import stddev_pop\n\n## END Imports. ################################################################\n\nclass Transformer(object):\n    """"""Interface which defines a transformer object.""""""\n\n    def transform(self, dataframe):\n        """"""Transforms the dataframe into an other dataframe.\n\n        # Returns\n            The transformed dataframe.\n        """"""\n        raise NotImplementedError\n\n\nclass MinMaxTransformer(Transformer):\n    """"""Will transform every feature of an instance between a specified range.\n\n    # Arguments\n        o_min: float. Original minimum of dataset.\n        o_max: float. Original maximum of dataset.\n        n_min: float. New minimum of dataset.\n        n_max: float. New maximum of dataset.\n        input_col: string. Name of input column.\n        output_col: string. Name of output column.\n        is_vector. boolean. Indicates if the data element is a vector or\n                            a singular value.\n\n    # Summary\n        New range: [o_min; o_max]\n        Old range: [n_min; n_max]\n    """"""\n\n    def __init__(self, o_min, o_max, n_min, n_max, input_col, output_col, is_vector=True):\n        self.o_min = float(o_min)\n        self.o_max = float(o_max)\n        self.n_min = float(n_min)\n        self.n_max = float(n_max)\n        self.scale = (self.n_max - self.n_min) / (self.o_max - self.o_min)\n        self.input_column = input_col\n        self.output_column = output_col\n        self.is_vector = is_vector\n\n    def _transform(self, row):\n        """"""Rescale every instance like this:\n\n        x\' = \\frac{x - min}{max - min}\n        """"""\n        if self.is_vector:\n            vector = row[self.input_column].toArray()\n            vector = self.scale * (vector - self.o_max) + self.n_max\n            new_value = DenseVector(vector)\n        else:\n            value = row[self.input_column]\n            new_value = self.scale * (value - self.o_max) + self.n_max\n        # Construct a new row with the normalized vector.\n        new_row = new_dataframe_row(row, self.output_column, new_value)\n\n        return new_row\n\n    def transform(self, dataframe):\n        """"""Applies the min-max transformation to every row in the dataframe.\n\n        # Arguments\n            dataframe: dataframe. Spark Dataframe.\n        """"""\n        return dataframe.rdd.map(self._transform).toDF()\n\n\nclass BinaryLabelTransformer(Transformer):\n    """"""Transformers the specified a column to a binary label, i.e., [0, 1] give\n    a specific label name. Given the specified label, this transformer will generate\n    [1,0], in the other case [0,1].\n\n    # Arguments:\n        input_column: string. Column name of the label identifier.\n        output_column: string. Name of the new label which contains the binary label.\n        label: string. Name of the label which needs to serve as 1.\n    """"""\n\n    def __init__(self, input_column, output_column, label):\n        self.input_column = input_column\n        self.output_column = output_column\n        self.label = label\n\n    def _transform(self, row):\n        """"""Appends the desired binary label column.""""""\n        value = row[self.input_column]\n        vector = np.zeros(2)\n        # Check if the name matches.\n        if value == self.label:\n            vector[0] = 1.0\n        else:\n            vector[1] = 1.0\n        # Convert to a Spark DenseVector\n        vector = DenseVector(vector)\n\n        return new_dataframe_row(row, self.output_column, vector)\n\n    def transform(self, dataframe):\n        """"""Applies the binary label transformation to the applied dataframe.\n\n        # Arguments\n            dataframe: dataframe. Spark Dataframe.\n        """"""\n        return dataframe.rdd.map(self._transform).toDF()\n\n\nclass StandardTransformer(Transformer):\n    """"""Will transform the specified columns to unit standard deviation (if specified),\n    and centers the data to mean 0 (if specified).\n\n    # Arguments\n        columns: list. List of columns.\n        suffix: string. Suffix name of the column after processing.\n    # Note\n        We assume equal probability of the rows.\n    """"""\n\n    def __init__(self, columns, suffix=""_normalized""):\n        self.columns = columns\n        self.column_suffix = suffix\n        self.current_column = None\n        self.means = {}\n        self.stddevs = {}\n\n    def clean_mean_keys(self, means):\n        """"""Cleans the keys of the specified dictionary (mean).""""""\n        new_means = {}\n\n        for k in means:\n            new_means[k[4:-1]] = means[k]\n\n        return new_means\n\n    def clean_stddev_keys(self, stddevs):\n        """"""Cleans the keys of the specified dictionary (stddev).""""""\n        new_stddevs = {}\n\n        for k in stddevs:\n            new_stddevs[k[11:-5]] = stddevs[k]\n\n        return new_stddevs\n\n    def _transform(self, row):\n        """"""Take the column, and normalize it with the computed means and std devs.""""""\n        mean = self.means[self.current_column]\n        stddev = self.stddevs[self.current_column]\n        x = row[self.current_column]\n        x_normalized = (x - mean) / stddev\n        output_column = self.current_column + self.column_suffix\n        new_row = new_dataframe_row(row, output_column, x_normalized)\n\n        return new_row\n\n    def transform(self, dataframe):\n        """"""Applies standardization to the specified columns.\n\n        # Arguments\n            dataframe: dataframe. Spark Dataframe.\n        """"""\n        # Compute the means of the specified columns.\n        means = [mean(x) for x in self.columns]\n        means = dataframe.select(means).collect()[0].asDict()\n        self.means = self.clean_mean_keys(means)\n        # Compute the standard deviation of the specified columns.\n        stddevs = [stddev_pop(x) for x in self.columns]\n        stddevs = dataframe.select(stddevs).collect()[0].asDict()\n        self.stddevs = self.clean_stddev_keys(stddevs)\n        # For every feature, add a new column to the dataframe.\n        for column in self.columns:\n            self.current_column = column\n            dataframe = dataframe.rdd.map(self._transform).toDF()\n\n        return dataframe\n\n\nclass DenseTransformer(Transformer):\n    """"""Transformes sparse vectors into dense vectors.\n\n    # Arguments\n        input_col: string. Name of the input column of the sparse vector.\n        output_col: string. Name of the output column.\n    """"""\n\n    def __init__(self, input_col, output_col):\n        self.input_column = input_col\n        self.output_column = output_col\n\n    def _transform(self, row):\n        """"""Transforms the sparse vector to a dense vector while putting it in a new column.""""""\n        sparse_vector = row[self.input_column]\n        dense_vector = DenseVector(sparse_vector.toArray())\n        new_row = new_dataframe_row(row, self.output_column, dense_vector)\n\n        return new_row\n\n    def transform(self, dataframe):\n        """"""Transforms every sparse vector in the input column to a dense vector.\n\n        # Arguments\n            dataframe: dataframe. Spark Dataframe.\n        # Returns\n            A transformed Spark Dataframe.\n        """"""\n        return dataframe.rdd.map(self._transform).toDF()\n\n\nclass ReshapeTransformer(Transformer):\n    """"""Transforms vectors into other dense shapes.\n\n    # Note:\n        Only use this transformer in the last stage of the processing pipeline.\n        Since the arbitrary vector shapes will be directly passed on to the models.\n\n    # Arguments:\n        input_col: string. Name of the input column containing the vector.\n        output_col: string. Name of the output column.\n        shape: tuple. Shape of the matrix.\n    """"""\n\n    def __init__(self, input_col, output_col, shape):\n        self.input_column = input_col\n        self.output_column = output_col\n        self.shape = shape\n\n    def _transform(self, row):\n        """"""Transforms the vector to a dense matrix while putting it in a new column.""""""\n        vector = row[self.input_column]\n        vector = np.asarray(vector)\n        reshaped = vector.reshape(self.shape).tolist()\n        new_row = new_dataframe_row(row, self.output_column, reshaped)\n\n        return new_row\n\n    def transform(self, dataframe):\n        """"""Transforms every vector in the input column to a dense vector.\n\n        # Arguments\n            dataframe: dataframe. Spark Dataframe.\n        # Returns\n            A transformed Spark Dataframe.\n        """"""\n        return dataframe.rdd.map(self._transform).toDF()\n\n\nclass OneHotTransformer(Transformer):\n    """"""Transformer which transforms an integer index into a vector using one-hot-encoding.\n\n    # Arguments\n        output_dim: int. Dimension of output vector.\n        input_col: string. Name of input column.\n        output_col: string. Name of output column.\n    """"""\n\n    def __init__(self, output_dim, input_col, output_col):\n        self.input_column = input_col\n        self.output_column = output_col\n        self.output_dimensionality = output_dim\n\n    def _transform(self, row):\n        """"""Transforms every individual row.\n\n        Only for internal use.\n        """"""\n        label = row[self.input_column]\n        vector = to_one_hot_encoded_dense(label, self.output_dimensionality)\n        new_row = new_dataframe_row(row, self.output_column, vector.tolist())\n\n        return new_row\n\n    def transform(self, dataframe):\n        """"""Applies One-Hot encoding to every row in the dataframe.\n\n        # Arguments\n            dataframe: dataframe. A Spark Dataframe.\n        # Returns\n            A Spark Dataframe with one-hot encoded features.\n        """"""\n        return dataframe.rdd.map(self._transform).toDF()\n\n\nclass LabelIndexTransformer(Transformer):\n    """"""Transformer which will transform a prediction vector into an integer label.\n\n    # Arguments\n        output_dim: int. Dimension of output vector.\n        input_col: string. Name of the input column.\n        output_col: string. Name of the output column.\n        default_index: int. Default ""answer"".\n        activation_threshold: float. Threshold of immediate activation.\n    """"""\n\n    def __init__(self, output_dim, input_col=""prediction"", output_col=""prediction_index"",\n                 default_index=0, activation_threshold=0.55):\n        self.input_column = input_col\n        self.output_column = output_col\n        self.output_dimensionality = output_dim\n        self.activation_threshold = activation_threshold\n        self.default_index = default_index\n\n    def get_index(self, vector):\n        """"""Returns the index with the highest value or with activation threshold.""""""\n        max = 0.0\n        max_index = self.default_index\n        for index in range(0, self.output_dimensionality):\n            if vector[index] >= self.activation_threshold:\n                return index\n            if vector[index] > max:\n                max = vector[index]\n                max_index = index\n\n        return max_index\n\n    def _transform(self, row):\n        """"""Transforms every row by adding a ""predicted index"" column to the dataframe. """"""\n        prediction = row[self.input_column]\n        index = float(self.get_index(prediction))\n        new_row = new_dataframe_row(row, self.output_column, index)\n\n        return new_row\n\n    def transform(self, dataframe):\n        """"""Transforms the dataframe by adding a predicted index.\n\n       # Arguments\n            dataframe: dataframe. A Spark Dataframe.\n        # Returns\n            A Spark Dataframe with a ""predicted"" index.\n        """"""\n        return dataframe.rdd.map(self._transform).toDF()\n'"
distkeras/utils.py,0,"b'""""""Utility functions used throughout Distributed Keras.""""""\n\n## BEGIN Import. ###############################################################\n\nfrom keras import backend as K\n\nfrom keras.models import model_from_json\n\nfrom keras import backend as K\n\nfrom pyspark.mllib.linalg import DenseVector\nfrom pyspark.sql import Row\nfrom pyspark.sql.functions import rand\n\nimport pickle\n\nimport json\n\nimport numpy as np\n\nimport os\n\nimport pwd\n\n## END Import. #################################################################\n\n\ndef get_os_username():\n    """"""Returns the username of user on the operating system.\n\n    From: http://stackoverflow.com/questions/842059/is-there-a-portable-way-to-get-the-current-username-in-python\n    """"""\n    return pwd.getpwuid(os.getuid())[0]\n\n\ndef set_keras_base_directory(base_dir=\'/tmp/\' + get_os_username()):\n    """"""Sets the base directory of Keras.""""""\n    K._keras_base_dir = base_dir\n\n\ndef to_one_hot_encoded_dense(value, n_dim=2):\n    """"""Converts the value to a one-hot encoded vector.\n\n    # Arguments\n        value: float. Value of the single ""hot"" value.\n        n_dim: int. Dimension of the output vector.\n    """"""\n    value = int(value)\n    vector = np.zeros(n_dim)\n    vector[value] = 1.0\n\n    return vector\n\n\ndef new_dataframe_row(old_row, column_name, column_value):\n    """"""Constructs a new Spark Row based on the old row, and a new column name and value.""""""\n    row = Row(*(old_row.__fields__ + [column_name]))(*(old_row + (column_value, )))\n\n    return row\n\n\ndef json_to_dataframe_row(string):\n    """"""Converts a JSON String to a Spark Dataframe row.""""""\n    dictionary = json.loads(string)\n    row = Row(**dictionary)\n\n    return row\n\n\ndef pickle_object(o):\n    """"""Pickles the specified model and its weights.""""""\n    return pickle.dumps(o, -1)\n\n\ndef unpickle_object(string):\n    """"""Unpickles the specified string into a model.""""""\n    return pickle.loads(string)\n\n\ndef serialize_keras_model(model):\n    """"""Serializes the specified Keras model into a dictionary.""""""\n    dictionary = {}\n    dictionary[\'model\'] = model.to_json()\n    dictionary[\'weights\'] = model.get_weights()\n\n    return dictionary\n\n\ndef history_executors_average(history):\n    """"""Returns the averaged training metrics for all the executors.""""""\n    max_iteration = max(history, key=lambda x: x[\'iteration\'])[\'iteration\']\n    max_executor = max(history, key=lambda x: x[\'worker_id\'])[\'worker_id\']\n    histories = []\n    averaged_history = []\n    # Fetch the histories of the individual executors.\n    for i in range(0, max_executor):\n        histories.append(history_executor(history, i))\n    # Construct the averaged history.\n    for i in range(0, max_iteration):\n        num_executors = 0\n        sum = np.zeros(2)\n        for j in range(0, max_executor):\n            if len(histories[j]) - 1 >= i:\n                num_executors += 1\n                sum += histories[j][i][\'history\']\n        # Average the history.\n        sum /= num_executors\n        averaged_history.append(sum)\n\n    return averaged_history\n\n\ndef history_executor(history, id):\n    """"""Returns the history of a specific executor.""""""\n    executor_history = [h for h in history if h[\'worker_id\'] == id]\n    executor_history.sort(key=lambda x: x[\'iteration\'])\n\n    return executor_history\n\n\ndef deserialize_keras_model(dictionary):\n    """"""Deserialized the Keras model using the specified dictionary.""""""\n    architecture = dictionary[\'model\']\n    weights = dictionary[\'weights\']\n    model = model_from_json(architecture)\n    model.set_weights(weights)\n\n    return model\n\n\ndef uniform_weights(model, constraints=[-0.5, 0.5]):\n    """"""Initializes the parameters of the specified Keras model with uniform\n    weights between the specified ranges.\n\n    # Arguments\n        model: Keras model.\n        constraints: array. An array with two elements which defines the range\n                     of the uniform initalization.\n    """"""\n    # We assume the following: Keras will return a list of weight matrices.\n    # All layers, even the activiation layers, will be randomly initialized.\n    weights = model.get_weights()\n    for layer in weights:\n        shape = layer.shape\n        if len(shape) > 1:\n            # Fill the matrix with random numbers.\n            n_rows = shape[0]\n            n_columns = shape[1]\n            for i in range(0, n_rows):\n                for j in range(0, n_columns):\n                    layer[i][j] = np.random.uniform(low=constraints[0], high=constraints[1])\n        else:\n            # Fill the vector with random numbers.\n            n_elements = shape[0]\n            for i in range(0, n_elements):\n                layer[i] = np.random.uniform(low=constraints[0], high=constraints[1])\n    # Set the new weights in the model.\n    model.set_weights(weights)\n\n\ndef shuffle(dataset):\n    """"""Shuffles the rows in the specified Spark Dataframe.\n\n    # Arguments\n        dataset: dataframe. A Spark Dataframe.\n    """"""\n    dataset = dataset.orderBy(rand())\n    dataset.cache()\n\n    return dataset\n\n\ndef precache(dataset, num_workers):\n    """"""Precaches the specified dataset.\n\n    Make sure the specified dataframe has the desired partitioning scheme.\n\n    # Arguments\n        dataset: dataframe. A Spark Dataframe.\n        num_workers: int. Number of workers you are going to use.\n    """"""\n    dataset = dataset.repartition(num_workers)\n    dataset.cache()\n    dataset.count()\n\n    return dataset\n'"
distkeras/workers.py,2,"b'""""""Workers module.\n\nThis module contains all worker specific implementations for different optimization\nalgorithms.\n""""""\n\n## BEGIN Imports. ##############################################################\n\nfrom distkeras.networking import connect\nfrom distkeras.networking import recv_data\nfrom distkeras.networking import send_data\n\nfrom distkeras.utils import deserialize_keras_model\nfrom distkeras.utils import serialize_keras_model\nfrom distkeras.utils import set_keras_base_directory\nfrom distkeras.utils import shuffle\nfrom distkeras.utils import uniform_weights\n\nfrom keras.optimizers import Optimizer, serialize, deserialize\nimport keras.backend as K\n\nfrom itertools import tee\n\nfrom multiprocessing import Pool\n\nimport numpy as np\n\nimport threading\n\nimport tensorflow as tf\n\nimport sys\n\n# ""queue"" module in python 3 is named ""Queue"" in python 2\nuse_python3 = sys.version_info[0] == 3\nif use_python3:\n    import queue\nelse:\n    import Queue as queue\n\nimport random\n\nimport socket\n\nimport time\n\n## END Imports. ################################################################\n\nclass Worker(object):\n    """"""Abstract class of a worker.\n\n    This class provides basic functionality and properties all workers share.\n    """"""\n\n    def __init__(self, model, optimizer, loss, loss_weights, metrics=[""accuracy""], features_col=""features"", label_col=""label"",\n                 batch_size=32, num_epoch=1, learning_rate=1.0):\n        assert isinstance(optimizer, (str, Optimizer)), ""\'optimizer\' must be a string or a Keras Optimizer instance""\n        assert isinstance(features_col, (str, list)), ""\'features_col\' must be a string or a list of strings""\n        assert isinstance(label_col, (str, list)), ""\'label_col\' must be a string or a list of strings""\n        self.model = model\n        self.optimizer = {\'class_name\': optimizer, \'config\': {}} if isinstance(optimizer, str) else serialize(optimizer)\n        self.loss = loss\n        self.loss_weights = loss_weights\n        self.metrics= metrics\n        self.features_column = [features_col] if isinstance(features_col, str) else features_col\n        self.label_column = [label_col] if isinstance(label_col, str) else label_col\n        self.batch_size = batch_size\n        self.num_epoch = num_epoch\n        self.max_mini_batches = 100\n        self.prefetching_thread = None\n        self.mini_batches = None\n        self.is_prefetching = True\n        self.worker_id = -1\n        self.learning_rate = learning_rate\n        self.num_inputs = len(self.features_column)\n        self.num_outputs = len(self.label_column)\n        self.current_epoch = 0\n\n    def set_max_prefetch(self, max_mini_batches):\n        """"""Sets the maximum number of mini-batches that can be prefetched.""""""\n        self.max_mini_batches = max_mini_batches\n\n    def set_learning_rate(self, learning_rate):\n        """"""Sets the learning rate of the worker.""""""\n        self.learning_rate = learning_rate\n\n    def get_learning_rate(self):\n        """"""Returns the learning rate of the worker.""""""\n        return self.learning_rate\n\n    def set_worker_id(self, worker_id):\n        """"""Sets the worker id.\n\n        # Arguments\n            worker_id: int. Worker identifier.\n        """"""\n        self.worker_id = worker_id\n\n    def get_worker_id(self):\n        """"""Returns the worker id.""""""\n        return self.worker_id\n\n    def prepare_model(self):\n        """"""Prepares the model for training.""""""\n        # Set the Keras directory.\n        set_keras_base_directory()\n        if K.backend() == \'tensorflow\':\n            # set GPU option allow_growth to False for GPU-enabled tensorflow\n            config = tf.ConfigProto()\n            config.gpu_options.allow_growth = False\n            sess = tf.Session(config=config)\n            K.set_session(sess)\n\n        # Deserialize the Keras model.\n        self.model = deserialize_keras_model(self.model)\n        self.optimizer = deserialize(self.optimizer)\n        # Compile the model with the specified loss and optimizer.\n        self.model.compile(loss=self.loss, loss_weights = self.loss_weights, \n            optimizer=self.optimizer, metrics=self.metrics)\n\n    def get_next_minibatch(self):\n        """"""Returns the next mini-batch.""""""\n        return self.mini_batches.get(timeout=10)\n\n    def start_prefetching_thread(self, iterator):\n        """"""Starts the data prefetching thread.""""""\n        self.mini_batches = queue.Queue()\n        self.iterator = iterator\n        self.prefetching_thread = threading.Thread(target=self.prefetching)\n        self.prefetching_thread.start()\n\n    def prefetching(self):\n        partition_iterators_all_epochs = tee(self.iterator, self.num_epoch)\n        for iter_one_epoch in partition_iterators_all_epochs:\n            self.current_epoch += 1\n            self.is_prefetching = True\n            try:\n                while self.is_prefetching:\n                    if self.mini_batches.qsize() < self.max_mini_batches:\n                        batch = [next(iter_one_epoch) for _ in range(self.batch_size)]\n                        batch_iterator_copies = tee(batch, self.num_inputs + self.num_outputs)\n                        feature_iterators = batch_iterator_copies[:self.num_inputs]\n                        label_iterators = batch_iterator_copies[self.num_inputs:]\n                        X = [np.asarray([x[self.features_column[i]] for x in iterator]) \n                            for i, iterator in enumerate(feature_iterators)]\n                        Y = [np.asarray([x[self.label_column[i]] for x in iterator])\n                            for i, iterator in enumerate(label_iterators)]\n                        self.mini_batches.put([X, Y])\n            except Exception as e:\n                print(e)\n                self.is_prefetching = False\n\n    def optimize(self):\n        """"""Optimization procedure of a worker.""""""\n        raise NotImplementedError\n\n    def train(self, worker_id, iterator):\n        """"""Training procedure for the worker node.\n\n        # Arguments\n            worker_id: int. Partition index provided by Spark. Can be used as a worker_id.\n            iterator: iterator. Data iterator.\n        """"""\n        # Prepare the optimization procedure.\n        self.start_prefetching_thread(iterator)\n        self.set_worker_id(worker_id)\n        self.prepare_model()\n        # Start the optimization procedure.\n        try:\n            self.optimize()\n        except Exception as e:\n            # Stop the prefetching process.\n            self.is_prefetching = False\n            print(e)\n        # Wait for the prefetching thread to stop.\n        self.prefetching_thread.join()\n\n        return iter([serialize_keras_model(self.model)])\n\n\nclass SequentialWorker(Worker):\n    """"""Implementation for sequential gradient updates on a single worker.\n\n    Will train a model on a single worker node.\n    """"""\n\n    def __init__(self, model, optimizer, loss, loss_weights, metrics=[""accuracy""], \n                 features_col=""features"", label_col=""label"", batch_size=32, num_epoch=1):\n        # Initialize the parent class.\n        super(SequentialWorker, self).__init__(model, optimizer, loss, loss_weights, metrics, features_col,\n                                               label_col, batch_size, num_epoch)\n\n    def optimize(self):\n        """"""Training procedure with sequential gradient updates.\n\n        # Returns\n            Trained serialized Keras model.\n        """"""\n        while True:\n            X, Y = self.get_next_minibatch()\n            h = self.model.train_on_batch(X, Y)\n            self.add_history(h)\n\n\nclass NetworkWorker(Worker):\n    """"""Abstract class of a worker who shares the variables using the network.""""""\n\n    def __init__(self, model, optimizer, loss, loss_weights, metrics=[""accuracy""], features_col=""features"", label_col=""label"",\n                 batch_size=32, num_epoch=1, master_host=""localhost"", master_port=5000, learning_rate=1.0):\n        super(NetworkWorker, self).__init__(model, optimizer, loss, loss_weights, metrics, features_col,\n                                            label_col, batch_size, num_epoch, learning_rate)\n        self.master_host = master_host\n        self.master_port = master_port\n        self.socket = None\n        self.center_variable = None\n        self.disable_nagle = True\n        self.training_history = []\n        self.worker_id = 0\n\n    def connect(self):\n        """"""Connect with the remote parameter server.""""""\n        self.socket = connect(self.master_host, self.master_port, self.disable_nagle)\n\n    def pull(self):\n        """"""Requests the center variable from the parameter server.""""""\n        # Request a pull from the parameter server.\n        self.socket.sendall(b\'p\')\n        # Fetch the center variable from the parameter server.\n        self.center_variable = np.asarray(recv_data(self.socket))\n\n    def commit(self, residual):\n        """"""Sends the gradient residual to the parameter server.""""""\n        # Prepare the datastructure.\n        data = {}\n        data[\'worker_id\'] = self.get_worker_id()\n        data[\'delta\'] = residual\n        # Request a commit from the parameter server.\n        self.socket.sendall(b\'c\')\n        # Send the data to the paramter server.\n        send_data(self.socket, data)\n\n    def set_tcp_no_delay(self, flag):\n        """"""Disables or enables Nagle\'s algorithm.\n        (True -> TCP_NODELAY = 1)\n        (False -> TCP_NODELAY = 0)\n\n        # Arguments:\n            flag: boolean. Indicates if Nagle\'s algorithm should be disabled.\n        """"""\n        self.disable_nagle = flag\n\n    def tcp_no_delay(self):\n        """"""Returns the value TCP_NODELAY of the flag (Nagle\'s algorithm).\n\n        # Returns\n            True, if Nagle\'s algorithm is disabled. False otherwise.\n        """"""\n        return self.disable_nagle\n\n    def get_master_host(self):\n        """"""Returns the host address of the master parameter server.""""""\n        return self.master_host\n\n    def get_master_port(self):\n        """"""Returns the port of the master parameter server.""""""\n        return self.master_port\n\n    def add_history(self, h):\n        """"""Appends the specified history data.""""""\n        d = {}\n        d[\'history\'] = h\n        d[\'worker_id\'] = self.worker_id\n        d[\'iteration\'] = self.iteration\n        d[\'timestamp\'] = time.time()\n        self.training_history.append(d)\n\n    def optimize(self):\n        """"""Optimization procedure of a network worker.""""""\n        raise NotImplementedError\n\n    def train(self, worker_id, iterator):\n        """"""Training procedure of a networked worker with a parameter server.""""""\n        self.start_prefetching_thread(iterator)\n        self.set_worker_id(worker_id)\n        self.prepare_model()\n        self.connect()\n        self.pull()\n        self.model.set_weights(self.center_variable)\n        try:\n            self.optimize()\n        except Exception as e:\n            # Stop the prefetching process.\n            self.is_prefetching = False\n            print(e)\n        self.socket.close()\n        self.prefetching_thread.join(timeout=1)\n\n        return iter(self.training_history)\n\n\nclass ADAGWorker(NetworkWorker):\n    """"""Implements the training procedure for ADAG.\n\n    Introduced by Hermans et al.\n    """"""\n\n    def __init__(self, model, optimizer, loss, loss_weights, metrics=[""accuracy""], features_col=""features"", label_col=""label"",\n                 batch_size=32, num_epoch=1, master_host=""localhost"", master_port=5000, communication_window=5):\n        # Initialize the parent object.\n        super(ADAGWorker, self).__init__(model, optimizer, loss, loss_weights, metrics, features_col, label_col,\n                                         batch_size, num_epoch, master_host, master_port)\n        # Initialize ADAG parameters.\n        self.communication_window = communication_window\n        self.iteration = 1\n\n    def commit(self, residual):\n        """"""Sends the gradient residual to the parameter server.""""""\n        # Prepare the datastructure.\n        data = {}\n        data[\'worker_id\'] = self.get_worker_id()\n        data[\'residual\'] = residual\n        # Request a commit from the parameter server.\n        self.socket.sendall(b\'c\')\n        # Send the data to the paramter server.\n        send_data(self.socket, data)\n\n    def optimize(self):\n        """"""Optimization procedure of ADAG.""""""\n        W1 = np.asarray(self.model.get_weights())\n        while True:\n            X, Y = self.get_next_minibatch()\n            h = self.model.train_on_batch(X, Y)\n            self.add_history(h)\n            if self.iteration % self.communication_window == 0:\n                W2 = np.asarray(self.model.get_weights())\n                delta = W2 - W1\n                delta /= self.communication_window\n                self.commit(delta)\n                self.pull()\n                self.model.set_weights(self.center_variable)\n                W1 = self.center_variable\n            self.iteration += 1\n\n\nclass DOWNPOURWorker(NetworkWorker):\n    """"""Implements the training procedure for the distributed DOWNPOUR optimizer.\n\n    Introduced by Dean et al.\n    http://static.googleusercontent.com/media/research.google.com/en//archive/large_deep_networks_nips2012.pdf\n    """"""\n\n    def __init__(self, model, optimizer, loss, loss_weights, metrics=[""accuracy""], features_col=""features"", label_col=""label"",\n                 batch_size=32, num_epoch=1, master_host=""localhost"", master_port=5000, communication_window=3):\n        # Initialize the parent object.\n        super(DOWNPOURWorker, self).__init__(model, optimizer, loss, loss_weights, metrics, features_col, label_col,\n                                             batch_size, num_epoch, master_host, master_port)\n        self.communication_window = communication_window\n        self.iteration = 1\n\n    def optimize(self):\n        """"""Specific optimization procedure for DOWNPOUR.""""""\n        W1 = np.asarray(self.model.get_weights())\n        while True:\n            X, Y = self.get_next_minibatch()\n            if self.iteration % self.communication_window == 0:\n                W2 = np.asarray(self.model.get_weights())\n                delta = W2 - W1\n                self.commit(delta)\n                self.pull()\n                self.model.set_weights(self.center_variable)\n                W1 = self.center_variable\n            h = self.model.train_on_batch(X, Y)\n            self.add_history(h)\n            self.iteration += 1\n\n\nclass AEASGDWorker(NetworkWorker):\n    """"""Implementation of asynchronous EASGD worker.\n\n    Introduced by Zhang et al.\n    https://arxiv.org/pdf/1412.6651.pdf\n    """"""\n\n    def __init__(self, model, optimizer, loss, loss_weights, metrics=[\'accuracy\'], features_col=""features"", label_col=""label"",\n                 batch_size=32, num_epoch=1, master_host=""localhost"", master_port=5000, rho=5.0,\n                 learning_rate=0.01, communication_window=32):\n        # Initialize the parent object.\n        super(AEASGDWorker, self).__init__(model, optimizer, loss, loss_weights, metrics, features_col, label_col,\n                                           batch_size, num_epoch, master_host, master_port)\n        # Initialize AEASGD specific variables.\n        self.rho = rho\n        self.learning_rate = learning_rate\n        self.communication_window = communication_window\n        self.alpha = self.rho * self.learning_rate\n        self.iteration = 1\n\n    def optimize(self):\n        """"""Specific training procedure for AEASGD.""""""\n        while True:\n            X, Y = self.get_next_minibatch()\n            if self.iteration % self.communication_window == 0:\n                self.pull()\n                W = np.asarray(self.model.get_weights())\n                E = self.alpha * (W - self.center_variable)\n                W = W - E\n                self.model.set_weights(W)\n                self.commit(E)\n            h = self.model.train_on_batch(X, Y)\n            self.add_history(h)\n            self.iteration += 1\n\n\nclass EAMSGDWorker(NetworkWorker):\n    """"""Worker implementation of Asynchronous EA Momentum SGD.\n\n    Introduced by Zhang et al.\n    https://arxiv.org/pdf/1412.6651.pdf\n    """"""\n\n    def __init__(self, model, optimizer, loss, loss_weights, metrics=[\'accuracy\'], features_col=""features"", label_col=""label"",\n                 batch_size=32, num_epoch=1, master_host=""localhost"", master_port=5000, rho=5.0,\n                 learning_rate=0.01, momentum=0.9, communication_window=32):\n        # Initialize the parent object.\n        super(EAMSGDWorker, self).__init__(model, optimizer, loss, loss_weights, metrics, features_col, label_col,\n                                           batch_size, num_epoch, master_host, master_port)\n        # Initialize EAMSGD specific variables.\n        self.rho = rho\n        self.learning_rate = learning_rate\n        self.momentum = momentum\n        self.communication_window = communication_window\n        self.alpha = self.learning_rate * self.rho\n        self.iteration = 1\n\n    def optimize(self):\n        """"""Specific training procedure of asynchronous EAMSGD.""""""\n        r = np.asarray(self.model.get_weights())\n        r.fill(0.0)\n        while True:\n            X, Y = self.get_next_minibatch()\n            if self.iteration % self.communication_window == 0:\n                self.pull()\n                W = np.asarray(self.model.get_weights())\n                E = self.alpha * (W - self.center_variable)\n                W = W - E\n                self.model.set_weights(W)\n                self.commit(E)\n            r_t = self.momentum * r\n            W_copy = np.asarray(self.model.get_weights())\n            W = np.asarray(self.model.get_weights())\n            W += r_t\n            self.model.set_weights(W)\n            h = self.model.train_on_batch(X, Y)\n            self.add_history(h)\n            gradient = np.asarray(self.model.get_weights()) - W\n            r = r_t - self.learning_rate * gradient\n            W_copy -= r\n            self.model.set_weights(W_copy)\n            self.iteration += 1\n\n\nclass DynSGDWorker(NetworkWorker):\n    """"""Implements the training procedure for DynSGD.""""""\n\n    def __init__(self, model, optimizer, loss, loss_weights, metrics=[""accuracy""], features_col=""features"", label_col=""label"",\n                 batch_size=32, num_epoch=1, master_host=""localhost"", master_port=5000, communication_window=5):\n        # Initialize the parent object.\n        super(DynSGDWorker, self).__init__(model, optimizer, loss, loss_weights, metrics, features_col, label_col,\n                                           batch_size, num_epoch, master_host, master_port)\n        # Initialize DynSGD parameters.\n        self.communication_window = communication_window\n        self.iteration = 1\n        self.last_update = 0\n\n    def pull(self):\n        """"""Requests the center variable and last update from the parameter server.""""""\n        # Request a pull from the parameter server.\n        self.socket.sendall(b\'p\')\n        # Fetch the dictionary from the parameter server.\n        data = recv_data(self.socket)\n        self.center_variable = np.asarray(data[\'model\'])\n        self.last_update = data[\'update\']\n\n    def commit(self, residual):\n        """"""Sends the gradient residual to the parameter server.""""""\n        # Prepare the datastructure.\n        data = {}\n        data[\'worker_id\'] = self.get_worker_id()\n        data[\'residual\'] = residual\n        data[\'last_update\'] = self.last_update\n        # Request a commit from the parameter server.\n        self.socket.sendall(b\'c\')\n        # Send the data to the paramter server.\n        send_data(self.socket, data)\n\n    def optimize(self):\n        """"""Optimization procedure of DynSGD.""""""\n        W1 = np.asarray(self.model.get_weights())\n        while True:\n            X, Y = self.get_next_minibatch()\n            h = self.model.train_on_batch(X, Y)\n            self.add_history(h)\n            if self.iteration % self.communication_window == 0:\n                W2 = np.asarray(self.model.get_weights())\n                delta = W2 - W1\n                self.commit(delta)\n                self.pull()\n                self.model.set_weights(self.center_variable)\n                W1 = self.center_variable\n            self.iteration += 1\n\n\nclass ExperimentalWorker(NetworkWorker):\n    """"""Implements the training procedure for ADAG.\n\n    Introduced by Hermans et al.\n    """"""\n\n    def __init__(self, model, optimizer, loss, loss_weights, metrics=[""accuracy""], features_col=""features"", label_col=""label"",\n                 batch_size=32, num_epoch=1, master_host=""localhost"", master_port=5000, communication_window=5,\n                 num_workers=2, learning_rate=1.0):\n        # Initialize the parent object.\n        super(ExperimentalWorker, self).__init__(model, optimizer, loss, loss_weights, metrics, features_col, label_col,\n                                                 batch_size, num_epoch, master_host, master_port, learning_rate)\n        # Initialize ADAG parameters.\n        self.communication_window = communication_window\n        self.num_workers = num_workers\n        self.current_num_workers = self.num_workers\n        self.inverse_learning_rate = 1 / self.learning_rate\n        self.iteration = 1\n\n    def commit(self, residual):\n        """"""Sends the gradient residual to the parameter server.""""""\n        # Prepare the datastructure.\n        data = {}\n        data[\'worker_id\'] = self.get_worker_id()\n        data[\'residual\'] = residual\n        data[\'stale_center_variable\'] = self.center_variable\n        # Request a commit from the parameter server.\n        self.socket.sendall(b\'c\')\n        # Send the data to the paramter server.\n        send_data(self.socket, data)\n\n    def pull(self):\n        """"""Requests the center variable from the parameter server.""""""\n        # Request a pull from the parameter server.\n        self.socket.sendall(b\'p\')\n        # Fetch the center variable from the parameter server.\n        self.center_variable = np.asarray(recv_data(self.socket))\n\n    def optimize(self):\n        """"""Optimization procedure of ADAG.""""""\n        W1 = np.asarray(self.model.get_weights())\n        while True:\n            X, Y = self.get_next_minibatch()\n            h = self.model.train_on_batch(X, Y)\n            self.add_history(h)\n            if self.iteration % self.communication_window == 0:\n                W2 = np.asarray(self.model.get_weights())\n                delta = W2 - W1\n                delta /= self.communication_window\n                self.commit(delta)\n                self.pull()\n                self.model.set_weights(self.center_variable)\n                W1 = self.center_variable\n            self.iteration += 1\n'"
examples/kafka_producer.py,0,"b'""""""\nThis example will be used as a Kafka producer to generate dummy\ndata for our Spark Streaming example.\n""""""\n\n## BEGIN Imports. ##############################################################\n\nfrom kafka import *\n\nimport sys\n\nimport pandas\n\nimport time\n\nimport json\n\n## END Imports. ################################################################\n\ndef usage():\n    print(""Distributed Keras Example: Kafka Producer"")\n    print("""")\n    print(""Usage:"")\n    print(""python kafka_producer.py [bootstrap_server]"")\n    exit(0)\n\ndef allocate_producer(bootstrap_server):\n    producer = KafkaProducer(bootstrap_servers=[bootstrap_server])\n\n    return producer\n\ndef read_data():\n    path = \'data/atlas_higgs.csv\'\n    data = []\n    # Use Pandas to infer the types.\n    data = pandas.read_csv(path)\n    # Remove the unneeded columns.\n    del data[\'Label\']\n    del data[\'Weight\']\n    # Convert the data to a list of dictionaries.\n    data = data.transpose().to_dict().values()\n\n    return data\n\ndef produce(producer, topic, data):\n    for row in data:\n        producer.send(topic, json.dumps(row))\n\ndef main():\n    # Check if the required number of arguments has been specified.\n    if len(sys.argv) != 2:\n        usage()\n    # Fetch the bootstrap server from the arguments.\n    bootstrap_server = sys.argv[1]\n    # Allocate the producer.\n    producer = allocate_producer(bootstrap_server)\n    # Read the data from the CSV file.\n    data = read_data()\n    iteration = 1\n    # Transmit the data in a continous loop while waiting for 5 seconds after every iteration.\n    while True:\n        print(""Iteration "" + str(iteration) + ""."")\n        produce(producer, \'Machine_Learning\', data)\n        iteration += 1\n        time.sleep(5)\n\nif __name__ == ""__main__"":\n    main()\n'"
examples/mnist.py,0,"b'""""""MNIST classification using Distributed Keras.\n\nATTENTION:\nBefore running this example, make sure you put the MNIST dataset\non HDFS.\n1. unzip mnist.zip\n2. hdfs dfs -mkdir data\n3. hdfs dfs -copyFromLocal mnist_train.csv data/mnist_train.csv\n4. hdfs dfs -copyFromLocal mnist_test.csv data/mnist_test.csv\n""""""\n\nfrom distkeras.evaluators import *\nfrom distkeras.predictors import *\nfrom distkeras.trainers import *\nfrom distkeras.transformers import *\nfrom distkeras.utils import *\n\nfrom keras.layers.convolutional import *\nfrom keras.layers.core import *\nfrom keras.models import Sequential\nfrom keras.optimizers import *\n\nfrom pyspark import SparkConf\nfrom pyspark import SparkContext\n\nfrom pyspark.ml.evaluation import MulticlassClassificationEvaluator\nfrom pyspark.ml.feature import OneHotEncoder\nfrom pyspark.ml.feature import StandardScaler\nfrom pyspark.ml.feature import StringIndexer\nfrom pyspark.ml.feature import VectorAssembler\n\nimport pwd\nimport os\n\n\n# First, setup the Spark variables. You can modify them to your needs.\napplication_name = ""Distributed Keras MNIST Notebook""\nusing_spark_2 = False\nlocal = False\npath_train = ""data/mnist_train.csv""\npath_test = ""data/mnist_test.csv""\nif local:\n    # Tell master to use local resources.\n    master = ""local[*]""\n    num_processes = 3\n    num_executors = 1\nelse:\n    # Tell master to use YARN.\n    master = ""yarn-client""\n    num_executors = 20\n    num_processes = 1\n\n# This variable is derived from the number of cores and executors, and will be used to assign the number of model trainers.\nnum_workers = num_executors * num_processes\n\nprint(""Number of desired executors: "" + `num_executors`)\nprint(""Number of desired processes / executor: "" + `num_processes`)\nprint(""Total number of workers: "" + `num_workers`)\n\n# Use the DataBricks CSV reader, this has some nice functionality regarding invalid values.\nos.environ[\'PYSPARK_SUBMIT_ARGS\'] = \'--packages com.databricks:spark-csv_2.10:1.4.0 pyspark-shell\'\n\nconf = SparkConf()\nconf.set(""spark.app.name"", application_name)\nconf.set(""spark.master"", master)\nconf.set(""spark.executor.cores"", `num_processes`)\nconf.set(""spark.executor.instances"", `num_executors`)\nconf.set(""spark.executor.memory"", ""4g"")\nconf.set(""spark.locality.wait"", ""0"")\nconf.set(""spark.serializer"", ""org.apache.spark.serializer.KryoSerializer"");\nconf.set(""spark.local.dir"", ""/tmp/"" + get_os_username() + ""/dist-keras"");\n\n# Check if the user is running Spark 2.0 +\nif using_spark_2:\n    sc = SparkSession.builder.config(conf=conf) \\\n            .appName(application_name) \\\n            .getOrCreate()\nelse:\n    # Create the Spark context.\n    sc = SparkContext(conf=conf)\n    # Add the missing imports\n    from pyspark import SQLContext\n    sqlContext = SQLContext(sc)\n\n# Check if we are using Spark 2.0\nif using_spark_2:\n    reader = sc\nelse:\n    reader = sqlContext\n# Read the training dataset.\nraw_dataset_train = reader.read.format(\'com.databricks.spark.csv\') \\\n                          .options(header=\'true\', inferSchema=\'true\') \\\n                          .load(path_train)\n# Read the testing dataset.\nraw_dataset_test = reader.read.format(\'com.databricks.spark.csv\') \\\n                         .options(header=\'true\', inferSchema=\'true\') \\\n                         .load(path_test)\n\n# First, we would like to extract the desired features from the raw dataset.\n# We do this by constructing a list with all desired columns.\n# This is identical for the test set.\nfeatures = raw_dataset_train.columns\nfeatures.remove(\'label\')\n\n# Next, we use Spark\'s VectorAssembler to ""assemble"" (create) a vector of all desired features.\n# http://spark.apache.org/docs/latest/ml-features.html#vectorassembler\nvector_assembler = VectorAssembler(inputCols=features, outputCol=""features"")\n# This transformer will take all columns specified in features, and create an additional column\n# ""features"" which will contain all the desired features aggregated into a single vector.\ndataset_train = vector_assembler.transform(raw_dataset_train)\ndataset_test = vector_assembler.transform(raw_dataset_test)\n\n# Define the number of output classes.\nnb_classes = 10\nencoder = OneHotTransformer(nb_classes, input_col=""label"", output_col=""label_encoded"")\ndataset_train = encoder.transform(dataset_train)\ndataset_test = encoder.transform(dataset_test)\n\n# Allocate a MinMaxTransformer from Distributed Keras to normalize the features..\n# o_min -> original_minimum\n# n_min -> new_minimum\ntransformer = MinMaxTransformer(n_min=0.0, n_max=1.0, \\\n                                o_min=0.0, o_max=250.0, \\\n                                input_col=""features"", \\\n                                output_col=""features_normalized"")\n# Transform the dataset.\ndataset_train = transformer.transform(dataset_train)\ndataset_test = transformer.transform(dataset_test)\n\n# Keras expects the vectors to be in a particular shape, we can reshape the\n# vectors using Spark.\nreshape_transformer = ReshapeTransformer(""features_normalized"", ""matrix"", (28, 28, 1))\ndataset_train = reshape_transformer.transform(dataset_train)\ndataset_test = reshape_transformer.transform(dataset_test)\n\n# Now, create a Keras model.\n# Taken from Keras MNIST example.\n\n# Declare model parameters.\nimg_rows, img_cols = 28, 28\n# number of convolutional filters to use\nnb_filters = 32\n# size of pooling area for max pooling\npool_size = (2, 2)\n# convolution kernel size\nkernel_size = (3, 3)\ninput_shape = (img_rows, img_cols, 1)\n\n# Construct the model.\nconvnet = Sequential()\nconvnet.add(Convolution2D(nb_filters, kernel_size[0], kernel_size[1],\n                          border_mode=\'valid\',\n                          input_shape=input_shape))\nconvnet.add(Activation(\'relu\'))\nconvnet.add(Convolution2D(nb_filters, kernel_size[0], kernel_size[1]))\nconvnet.add(Activation(\'relu\'))\nconvnet.add(MaxPooling2D(pool_size=pool_size))\nconvnet.add(Flatten())\nconvnet.add(Dense(225))\nconvnet.add(Activation(\'relu\'))\nconvnet.add(Dense(nb_classes))\nconvnet.add(Activation(\'softmax\'))\n\n# Define the optimizer and the loss.\noptimizer_convnet = \'adam\'\nloss_convnet = \'categorical_crossentropy\'\n\n# Print the summary.\nconvnet.summary()\n\n# We can also evaluate the dataset in a distributed manner.\n# However, for this we need to specify a procedure how to do this.\ndef evaluate_accuracy(model, test_set, features=""matrix""):\n    evaluator = AccuracyEvaluator(prediction_col=""prediction_index"", label_col=""label"")\n    predictor = ModelPredictor(keras_model=model, features_col=features)\n    transformer = LabelIndexTransformer(output_dim=nb_classes)\n    test_set = test_set.select(features, ""label"")\n    test_set = predictor.predict(test_set)\n    test_set = transformer.transform(test_set)\n    score = evaluator.evaluate(test_set)\n\n    return score\n\n# Select the desired columns, this will reduce network usage.\ndataset_train = dataset_train.select(""features_normalized"", ""matrix"",""label"", ""label_encoded"")\ndataset_test = dataset_test.select(""features_normalized"", ""matrix"",""label"", ""label_encoded"")\n# Keras expects DenseVectors.\ndense_transformer = DenseTransformer(input_col=""features_normalized"", output_col=""features_normalized_dense"")\ndataset_train = dense_transformer.transform(dataset_train)\ndataset_test = dense_transformer.transform(dataset_test)\ndataset_train.repartition(num_workers)\ndataset_test.repartition(num_workers)\n# Assing the training and test set.\ntraining_set = dataset_train.repartition(num_workers)\ntest_set = dataset_test.repartition(num_workers)\n# Cache them.\ntraining_set.cache()\ntest_set.cache()\n\n# Precache the trainingset on the nodes using a simple count.\nprint(training_set.count())\n\n# Use the ADAG optimizer. You can also use a SingleWorker for testing purposes -> traditional\n# non-distributed gradient descent.\ntrainer = ADAG(keras_model=convnet, worker_optimizer=optimizer_convnet, loss=loss_convnet,\n               num_workers=num_workers, batch_size=16, communication_window=5, num_epoch=5,\n               features_col=""matrix"", label_col=""label_encoded"")\ntrained_model = trainer.train(training_set)\n\nprint(""Training time: "" + str(trainer.get_training_time()))\nprint(""Accuracy: "" + str(evaluate_accuracy(trained_model, test_set)))\nprint(""Number of parameter server updates: "" + str(trainer.parameter_server.num_updates))\n'"
scripts/generate_secret.py,0,"b'""""""Generates a JSON structure that needs to be added to the\nsecrets file.\n\nAuthor: Joeri Hermans\n""""""\n\n## BEGIN Imports. ##############################################################\n\nimport json\n\nimport optparse\n\nimport random\n\nimport string\n\n## END Imports. ################################################################\n\ndef generate_secret(identity):\n    secret = \'\'.join(random.SystemRandom().choice(string.ascii_uppercase + string.digits) for _ in range(64))\n    d = {}\n    d[\'secret\'] = secret\n    d[\'identity\'] = identity\n    print(json.dumps(d))\n\ndef parse_arguments():\n    parser = optparse.OptionParser()\n    parser.set_defaults(identity=None)\n    parser.add_option(\'--identity\', action=\'store\', dest=\'identity\', type=\'string\')\n    (options, args) = parser.parse_args()\n\n    return options\n\ndef main():\n    # Parse the options.\n    options = parse_arguments()\n    # Check if an identity has been provided.\n    if options.identity is not None:\n        generate_secret(options.identity)\n    else:\n        print(""Please specify an identity (--identity)."")\n\nif __name__ == \'__main__\':\n    main()\n'"
scripts/punchcard.py,0,"b'""""""Script which starts the Punchcard daemon. Punchcard will accept remote job\nrequests and execute them on the local cluster.\n\nAuthor: Joeri Hermans\n""""""\n\n## BEGIN Imports. ##############################################################\n\nfrom distkeras.job_deployment import Job\nfrom distkeras.job_deployment import Punchcard\n\nimport os\n\nimport sys\n\nimport optparse\n\n## END Imports. ################################################################\n\ndef parse_arguments():\n    parser = optparse.OptionParser()\n    parser.set_defaults(port=8000, secrets_path=\'secrets.json\')\n    parser.add_option(\'--port\', action=\'store\', dest=\'port\', type=\'int\')\n    parser.add_option(\'--secrets\', action=\'store\', dest=\'secrets_path\', type=\'string\')\n    (options, args) = parser.parse_args()\n\n    return options\n\ndef start_punchcard(port, secrets):\n    punchcard = Punchcard(secrets, port)\n    punchcard.run()\n\ndef main():\n    # Parse the program arguments.\n    options = parse_arguments()\n    port = options.port\n    secrets_path = options.secrets_path\n    # Start the Punchcard instance.\n    start_punchcard(port, secrets_path)\n\nif __name__ == \'__main__\':\n    main()\n'"
