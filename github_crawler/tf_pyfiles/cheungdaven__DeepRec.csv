file_path,api_count,code
models/__init__.py,0,b''
test/__init__.py,0,b''
test/testSeqRec.py,2,"b'import argparse\nimport tensorflow as tf\nimport sys\nimport os\n\nsys.path.append(os.path.join(os.path.dirname(__file__), \'..\'))\nfrom models.seq_rec.Caser import Caser\nfrom models.seq_rec.AttRec import AttRec\nfrom models.seq_rec.PRME import PRME\nfrom utils.load_data.load_data_seq import DataSet\nfrom utils.load_data.load_data_ranking import *\n\n\ndef parse_args():\n    parser = argparse.ArgumentParser(description=\'DeepRec\')\n    parser.add_argument(\'--model\', choices=[\'Caser\',\'PRME\', \'AttRec\'], default = \'AttRec\')\n    parser.add_argument(\'--epochs\', type=int, default=1000)\n    parser.add_argument(\'--num_factors\', type=int, default=10)\n    parser.add_argument(\'--display_step\', type=int, default=256)\n    parser.add_argument(\'--batch_size\', type=int, default=1024 ) #128 for unlimpair\n    parser.add_argument(\'--learning_rate\', type=float, default=1e-3) #1e-4 for unlimpair\n    parser.add_argument(\'--reg_rate\', type=float, default=0.1) #0.01 for unlimpair\n    return parser.parse_args()\n\n\nif __name__ == \'__main__\':\n    args = parse_args()\n    epochs = args.epochs\n    learning_rate = args.learning_rate\n    reg_rate = args.reg_rate\n    num_factors = args.num_factors\n    display_step = args.display_step\n    batch_size = args.batch_size\n\n\n    config = tf.ConfigProto()\n    config.gpu_options.allow_growth = True\n\n\n\n    with tf.Session(config=config) as sess:\n        model = None\n        # Model selection\n\n        if args.model == ""Caser"":\n            train_data = DataSet(path=""../data/ml100k/temp/train.dat"", sep=""\\t"",\n                                 header=[\'user\', \'item\', \'rating\', \'time\'],\n                                 isTrain=True, seq_len=5, target_len=3, num_users=943, num_items=1682)\n            test_data = DataSet(path=""../data/ml100k/temp/test.dat"", sep=""\\t"",\n                                header=[\'user\', \'item\', \'rating\', \'time\'],\n                                user_map=train_data.user_map, item_map=train_data.item_map)\n            # train_data = DataSet(path=""../Data/ml100k/seq/train.dat"",  isTrain=True)\n            # test_data = DataSet(path=""../Data/ml100k/seq/test.dat"", user_map=train_data.user_map, item_map=train_data.item_map)\n            model = Caser(sess, train_data.num_user,  train_data.num_item)\n            model.build_network(L = train_data.sequences.L, num_T=train_data.sequences.T)\n            model.execute(train_data, test_data)\n        if args.model == ""PRME"":\n            train_data = DataSet(path=""../data/ml100k/temp/train.dat"", sep=""\\t"",header=[\'user\', \'item\', \'rating\', \'time\'],isTrain=True, seq_len=1, target_len=1)\n            test_data = DataSet(path=""../data/ml100k/temp/test.dat"", sep=""\\t"", header=[\'user\', \'item\', \'rating\', \'time\'], user_map=train_data.user_map, item_map=train_data.item_map)\n            model = PRME(sess, train_data.num_user,  train_data.num_item)\n            model.build_network(L = train_data.sequences.L, num_T=train_data.sequences.T)\n            model.execute(train_data, test_data)\n        if args.model == ""AttRec"":\n            train_data = DataSet(path=""../data/ml100k/temp/train.dat"", sep=""\\t"",header=[\'user\', \'item\', \'rating\', \'time\'],isTrain=True, seq_len=5, target_len=3, num_users=943, num_items=1682)\n            test_data = DataSet(path=""../data/ml100k/temp/test.dat"", sep=""\\t"", header=[\'user\', \'item\', \'rating\', \'time\'], user_map=train_data.user_map, item_map=train_data.item_map)\n            model = AttRec(sess, train_data.num_user,  train_data.num_item)\n            # print(train_data.user_map)\n            # print(train_data.item_map)\n            model.build_network(L = train_data.sequences.L, num_T=train_data.sequences.T)\n            model.execute(train_data, test_data)\n'"
test/test_item_ranking.py,2,"b'import argparse\nimport tensorflow as tf\nimport sys\nimport os.path\n\nsys.path.append(os.path.join(os.path.dirname(__file__), \'..\'))\n\nfrom models.item_ranking.cdae import ICDAE\nfrom models.item_ranking.bprmf import BPRMF\nfrom models.item_ranking.cml import CML\nfrom models.item_ranking.neumf import NeuMF\nfrom models.item_ranking.gmf import GMF\nfrom models.item_ranking.jrl import JRL\nfrom models.item_ranking.mlp import MLP\nfrom models.item_ranking.lrml import LRML\n\nfrom utils.load_data.load_data_ranking import *\n\n\ndef parse_args():\n    parser = argparse.ArgumentParser(description=\'DeepRec\')\n    parser.add_argument(\'--model\', choices=[\'CDAE\', \'CML\', \'NeuMF\', \'GMF\', \'MLP\', \'BPRMF\', \'JRL\', \'LRML\'],\n                        default=\'LRML\')\n    parser.add_argument(\'--epochs\', type=int, default=1000)\n    parser.add_argument(\'--num_factors\', type=int, default=10)\n    parser.add_argument(\'--display_step\', type=int, default=1000)\n    parser.add_argument(\'--batch_size\', type=int, default=1024)  # 128 for unlimpair\n    parser.add_argument(\'--learning_rate\', type=float, default=1e-3)  # 1e-4 for unlimpair\n    parser.add_argument(\'--reg_rate\', type=float, default=0.1)  # 0.01 for unlimpair\n    return parser.parse_args()\n\n\nif __name__ == \'__main__\':\n    args = parse_args()\n    epochs = args.epochs\n    learning_rate = args.learning_rate\n    reg_rate = args.reg_rate\n    num_factors = args.num_factors\n    display_step = args.display_step\n    batch_size = args.batch_size\n\n    train_data, test_data, n_user, n_item = load_data_neg(test_size=0.2, sep=""\\t"")\n\n    try:\n        gpus = tf.config.experimental.list_physical_devices(\'GPU\')\n        tf.config.experimental.set_memory_growth(gpus[0], True)\n    except:\n        # Invalid device or cannot modify virtual devices once initialized.\n        pass\n\n    model = None\n    # Model selection\n    if args.model == ""CDAE"":\n        train_data, test_data, n_user, n_item = load_data_all(test_size=0.2, sep=""\\t"")\n        model = ICDAE(n_user, n_item)\n    if args.model == ""CML"":\n        model = CML(n_user, n_item)\n    if args.model == ""LRML"":\n        model = LRML(n_user, n_item)\n    if args.model == ""BPRMF"":\n        model = BPRMF(n_user, n_item)\n    if args.model == ""NeuMF"":\n        model = NeuMF(n_user, n_item)\n    if args.model == ""GMF"":\n        model = GMF(n_user, n_item)\n    if args.model == ""MLP"":\n        model = MLP(n_user, n_item)\n    if args.model == ""JRL"":\n        model = JRL(n_user, n_item)\n    # build and execute the model\n    if model is not None:\n        model.build_network()\n        model.execute(train_data, test_data)\n'"
test/test_rating_pred.py,2,"b'import argparse\nimport sys\nimport os.path\nsys.path.append(os.path.join(os.path.dirname(__file__), \'..\'))\n\nfrom models.rating_prediction.fm import FM\nfrom models.rating_prediction.nnmf import NNMF\nfrom models.rating_prediction.mf import MF\nfrom models.rating_prediction.nrr import NRR\nfrom models.rating_prediction.autorec import *\nfrom models.rating_prediction.nfm import NFM\nfrom models.rating_prediction.deepfm import DeepFM\nfrom models.rating_prediction.afm import AFM\nfrom utils.load_data.load_data_rating import *\nfrom utils.load_data.load_data_content import *\n\n\ndef parse_args():\n    parser = argparse.ArgumentParser(description=\'nnRec\')\n    parser.add_argument(\'--model\', choices=[\'MF\', \'NNMF\', \'NRR\', \'I-AutoRec\', \'U-AutoRec\',\n                                            \'FM\', \'NFM\', \'AFM\', \'DEEP-FM\'],\n                        default=\'DEEP-FM\')\n    parser.add_argument(\'--epochs\', type=int, default=200)\n    parser.add_argument(\'--batch_size\', type=int, default=256)  # 128 for unlimpair\n    parser.add_argument(\'--learning_rate\', type=float, default=1e-3)  # 1e-4 for unlimpair\n    parser.add_argument(\'--reg_rate\', type=float, default=0.1)  # 0.01 for unlimpair\n    parser.add_argument(\'--num_factors\', type=int, default=10)\n    parser.add_argument(\'--display_step\', type=int, default=1000)\n    parser.add_argument(\'--show_time\', type=bool, default=False)\n    parser.add_argument(\'--T\', type=int, default=2)\n    parser.add_argument(\'--deep_layers\', type=str, default=""200, 200, 200"")\n    parser.add_argument(\'--field_size\', type=int, default=10)\n\n    return parser.parse_args()\n\n\nif __name__ == \'__main__\':\n    args = parse_args()\n    epochs = args.epochs\n    batch_size = args.batch_size\n    learning_rate = args.learning_rate\n    reg_rate = args.reg_rate\n    num_factors = args.num_factors\n    display_step = args.display_step\n    show_time = args.show_time,\n\n    kws = {\n        \'epochs\': epochs,\n        \'batch_size\': batch_size,\n        \'learning_rate\': learning_rate,\n        \'reg_rate\': reg_rate,\n        \'num_factors\': num_factors,\n        \'display_step\': display_step,\n        \'show_time\': show_time[0],\n        \'T\': args.T,\n        \'layers\': list(map(int, args.deep_layers.split(\',\'))),\n        \'field_size\': args.field_size\n\n    }\n\n    train_data, test_data, n_user, n_item = load_data_rating(path=""../Data/ml100k/movielens_100k.dat"",\n                                                             header=[\'user_id\', \'item_id\', \'rating\', \'t\'],\n                                                             test_size=0.1, sep=""\\t"")\n\n    config = tf.ConfigProto()\n    config.gpu_options.allow_growth = True\n\n    with tf.Session(config=config) as sess:\n        model = None\n        # Model selection\n        if args.model == ""MF"":\n            model = MF(sess, n_user, n_item, batch_size=batch_size)\n        if args.model == ""NNMF"":\n            model = NNMF(sess, n_user, n_item, learning_rate=learning_rate)\n        if args.model == ""NRR"":\n            model = NRR(sess, n_user, n_item)\n        if args.model == ""I-AutoRec"":\n            model = IAutoRec(sess, n_user, n_item)\n        if args.model == ""U-AutoRec"":\n            model = UAutoRec(sess, n_user, n_item)\n        if args.model == ""NFM"":\n            train_data, test_data, feature_M = load_data_fm()\n            n_user = 957\n            n_item = 4082\n            model = NFM(sess, n_user, n_item, epoch=2)\n            model.build_network(feature_M)\n        if args.model == ""FM"":\n            train_data, test_data, feature_M = load_data_fm()\n            n_user = 957\n            n_item = 4082\n            model = FM(sess, n_user, n_item, learning_rate=learning_rate, reg_rate=reg_rate, epoch=epochs,\n                       batch_size=batch_size, display_step=display_step)\n            model.build_network(feature_M)\n\n        if args.model == ""DEEP-FM"":\n            train_data, test_data, feature_M = load_data_fm()\n            n_user = 957\n            n_item = 4082\n            model = DeepFM(sess, n_user, n_item, **kws)\n            model.build_network(feature_M)\n\n        if args.model == ""AFM"":\n            train_data, test_data, feature_M = load_data_fm()\n            n_user = 957\n            n_item = 4082\n            model = AFM(sess, n_user, n_item, learning_rate=learning_rate, reg_rate=reg_rate, epoch=epochs,\n                        batch_size=batch_size, display_step=display_step)\n            model.build_network(feature_M)\n\n        # build and execute the model\n        if model is not None:\n            if args.model in (\'FM\', \'NFM\', \'DEEP-FM\', \'AFM\'):\n                model.execute(train_data, test_data)\n            else:\n                model.build_network()\n                model.execute(train_data, test_data)\n'"
utils/__init__.py,0,b''
models/item_ranking/__init__.py,0,b''
models/item_ranking/bprmf.py,30,"b'#!/usr/bin/env python\n""""""Implementation of Bayesain Personalized Ranking Model.\nReference: Rendle, Steffen, et al. ""BPR: Bayesian personalized ranking from implicit feedback."" Proceedings of\nthe twenty-fifth conference on uncertainty in artificial intelligence. AUAI Press, 2009..\n""""""\n\nimport time\n\nimport tensorflow as tf\nfrom tensorflow.keras import Input, Model\nfrom tensorflow.keras.layers import Dense, Lambda\n\nfrom utils.evaluation.RankingMetrics import *\n\n__author__ = ""Shuai Zhang""\n__copyright__ = ""Copyright 2018, The DeepRec Project""\n\n__license__ = ""GPL""\n__version__ = ""1.0.0""\n__maintainer__ = ""Shuai Zhang""\n__email__ = ""cheungdaven@gmail.com""\n__status__ = ""Development""\n\n\nclass EmbeddingLookup(tf.keras.layers.Layer):\n    def __init__(self, input_embedding, **kwargs):\n        super(EmbeddingLookup, self).__init__(**kwargs)\n        self.input_embedding = input_embedding\n\n    # def build(self, input_shape):\n    #     self.embeddings = self.add_weight(\n    #         shape=(self.input_dim, self.output_dim),\n    #         initializer=\'random_normal\',\n    #         dtype=\'float32\')\n\n    def call(self, inputs):\n        return tf.nn.embedding_lookup(self.input_embedding, inputs)\n\n\nclass BPRMF(object):\n    def __init__(self, num_user, num_item, learning_rate=0.001, reg_rate=0.1, epoch=500, batch_size=1024,\n                 verbose=False, t=5, display_step=1000):\n        self.num_user = num_user\n        self.num_item = num_item\n        self.learning_rate = learning_rate\n        self.reg_rate = reg_rate\n        self.epochs = epoch\n        self.batch_size = batch_size\n        self.verbose = verbose\n        self.T = t\n        self.display_step = display_step\n\n        self.user_id = Input(shape=(1,), dtype=tf.int32, name=\'user_id\')\n        self.item_id = Input(shape=(1,), dtype=tf.int32, name=\'item_id\')\n        self.neg_item_id = Input(shape=(1,), dtype=tf.int32, name=\'neg_item_id\')\n        self.P = None\n        self.Q = None\n        self.pred_y = None\n        self.pred_y_neg = None\n        self.loss = None\n        self.loss_estimator = tf.keras.metrics.Mean(name=\'train_loss\')\n        self.optimizer = tf.keras.optimizers.Adam(learning_rate=self.learning_rate)\n\n        self.test_data = None\n        self.user = None\n        self.item = None\n        self.neg_items = None\n        self.test_users = None\n\n        self.num_training = None\n        self.total_batch = None\n        print(""You are running BPRMF."")\n\n    def build_network(self, num_factor=30):\n        self.P = tf.Variable(tf.random.normal([self.num_user, num_factor], stddev=0.01))\n        self.Q = tf.Variable(tf.random.normal([self.num_item, num_factor], stddev=0.01))\n        # user_id = tf.squeeze(self.user_id)\n        # item_id = tf.squeeze(self.item_id)\n        # neg_item_id = tf.squeeze(self.neg_item_id)\n        user_id = Lambda(lambda x: tf.squeeze(x))(self.user_id)\n        item_id = Lambda(lambda x: tf.squeeze(x))(self.item_id)\n        neg_item_id = Lambda(lambda x: tf.squeeze(x))(self.neg_item_id)\n\n        user_latent_factor = EmbeddingLookup(self.P)(user_id)\n        item_latent_factor = EmbeddingLookup(self.Q)(item_id)\n        neg_item_latent_factor = EmbeddingLookup(self.Q)(neg_item_id)\n        # user_latent_factor = tf.nn.embedding_lookup(self.P, user_id)\n        # item_latent_factor = tf.nn.embedding_lookup(self.Q, item_id)\n        # neg_item_latent_factor = tf.nn.embedding_lookup(self.Q, neg_item_id)\n\n        pred_y = tf.reduce_sum(tf.multiply(user_latent_factor, item_latent_factor), 1)\n        pred_y_neg = tf.reduce_sum(tf.multiply(user_latent_factor, neg_item_latent_factor), 1)\n        self.model = Model(inputs=[self.user_id, self.item_id, self.neg_item_id],\n                           outputs=[pred_y, pred_y_neg])\n        # self.optimizer = tf.train.AdamOptimizer(learning_rate=self.learning_rate).minimize(self.loss)\n\n    def prepare_data(self, train_data, test_data):\n        """"""\n        You must prepare the data before train and test the model\n        :param train_data:\n        :param test_data:\n        :return:\n        """"""\n        t = train_data.tocoo()\n        self.user = t.row.reshape(-1)\n        self.item = t.col.reshape(-1)\n        self.num_training = len(self.item)\n        self.test_data = test_data\n        self.total_batch = int(self.num_training / self.batch_size)\n        self.neg_items = self._get_neg_items(train_data.tocsr())\n        self.test_users = set([u for u in self.test_data.keys() if len(self.test_data[u]) > 0])\n        print(""data preparation finished."")\n\n    @tf.function\n    def train_op(self, batch_user, batch_item, batch_item_neg):\n        with tf.GradientTape() as tape:\n            pred_y, pred_y_neg = self.model([batch_item, batch_user, batch_item_neg])\n            loss = - tf.reduce_sum(\n                tf.math.log(tf.sigmoid(pred_y - pred_y_neg))) + \\\n                   self.reg_rate * (tf.nn.l2_loss(self.P) + tf.nn.l2_loss(self.Q))\n        gradients_of_model = tape.gradient(loss, self.model.trainable_variables)\n        self.optimizer.apply_gradients(zip(gradients_of_model, self.model.trainable_variables))\n        self.loss_estimator(loss)\n\n    def train(self):\n        idxs = np.random.permutation(self.num_training)  # shuffled ordering\n        user_random = list(self.user[idxs])\n        item_random = list(self.item[idxs])\n        item_random_neg = []\n        for u in user_random:\n            neg_i = self.neg_items[u]\n            s = np.random.randint(len(neg_i))\n            item_random_neg.append(neg_i[s])\n\n        # train\n        for i in range(self.total_batch):\n            start_time = time.time()\n            batch_user = np.array(user_random[i * self.batch_size:(i + 1) * self.batch_size])\n            batch_item = item_random[i * self.batch_size:(i + 1) * self.batch_size]\n            batch_item_neg = item_random_neg[i * self.batch_size:(i + 1) * self.batch_size]\n\n            np_batch_user = np.expand_dims(np.array(batch_user), -1)\n            np_batch_item = np.expand_dims(np.array(batch_item), -1)\n            np_batch_item_neg = np.expand_dims(np.array(batch_item_neg), -1)\n            self.train_op(np_batch_user, np_batch_item, np_batch_item_neg)\n\n            if i % self.display_step == 0:\n                if self.verbose:\n                    print(""Index: %04d; cost= %.9f"" % (i + 1, self.loss_estimator.result()))\n                    print(""one iteration: %s seconds."" % (time.time() - start_time))\n\n    def test(self):\n        evaluate(self)\n\n    def execute(self, train_data, test_data):\n        self.prepare_data(train_data, test_data)\n\n        for epoch in range(self.epochs):\n            self.train()\n            if epoch % self.T == 0:\n                print(""Epoch: %04d; "" % epoch, end=\'\')\n                self.test()\n\n    def save(self, path):\n        saver = tf.train.Saver()\n        saver.save(self.sess, path)\n\n    def predict(self, user_id, item_id):\n        user_id = tf.expand_dims(tf.convert_to_tensor(user_id), -1)\n        item_id = tf.expand_dims(tf.convert_to_tensor(item_id), -1)\n        dummy_neg_id = tf.zeros(item_id.shape, tf.int32)\n        pred_y, pred_y_neg = self.model([user_id, item_id, dummy_neg_id])\n        return pred_y.numpy()\n\n    def _get_neg_items(self, data):\n        all_items = set(np.arange(self.num_item))\n        neg_items = {}\n        for u in range(self.num_user):\n            neg_items[u] = list(all_items - set(data.getrow(u).nonzero()[1]))\n\n        return neg_items\n'"
models/item_ranking/cdae.py,38,"b'#!/usr/bin/env python\n""""""Implementation of CDAE.\nReference: Wu, Yao, et al. ""Collaborative denoising auto-encoders for top-n recommender systems."" Proceedings\nof the Ninth ACM International Conference on Web Search and Data Mining. ACM, 2016.\n""""""\n\nimport tensorflow as tf\nimport time\nimport numpy as np\n\nfrom utils.evaluation.RankingMetrics import evaluate\n\n__author__ = ""Shuai Zhang""\n__copyright__ = ""Copyright 2018, The DeepRec Project""\n\n__license__ = ""GPL""\n__version__ = ""1.0.0""\n__maintainer__ = ""Shuai Zhang""\n__email__ = ""cheungdaven@gmail.com""\n__status__ = ""Development""\n\n\nclass CDAE(object):\n    def __init__(self, sess, num_user, num_item, learning_rate=0.01, reg_rate=0.01, epoch=500, batch_size=100,\n                 verbose=False, t=1, display_step=1000):\n        self.sess = sess\n        self.num_user = num_user\n        self.num_item = num_item\n        self.learning_rate = learning_rate\n        self.reg_rate = reg_rate\n        self.epochs = epoch\n        self.batch_size = batch_size\n        self.verbose = verbose\n        self.T = t\n        self.display_step = display_step\n\n        self.user_id = None\n        self.corrupted_rating_matrix = None\n        self.rating_matrix = None\n        self.corruption_level = None\n        self.layer_2 = None\n        self.loss = None\n        self.optimizer = None\n        self.train_data = None\n        self.neg_items = None\n        self.num_training = None\n        self.total_batch = None\n        self.test_data = None\n        self.test_users = None\n        self.reconstruction = None\n        print(""You are running CDAE."")\n\n    def build_network(self, hidden_neuron=500, corruption_level=0):\n        self.corrupted_rating_matrix = tf.placeholder(dtype=tf.float32, shape=[None, self.num_item])\n        self.rating_matrix = tf.placeholder(dtype=tf.float32, shape=[None, self.num_item])\n        self.user_id = tf.placeholder(dtype=tf.int32, shape=[None])\n        self.corruption_level = corruption_level\n\n        _W = tf.Variable(tf.random_normal([self.num_item, hidden_neuron], stddev=0.01))\n        _W_prime = tf.Variable(tf.random_normal([hidden_neuron, self.num_item], stddev=0.01))\n        _V = tf.Variable(tf.random_normal([self.num_user, hidden_neuron], stddev=0.01))\n\n        b = tf.Variable(tf.random_normal([hidden_neuron], stddev=0.01))\n        b_prime = tf.Variable(tf.random_normal([self.num_item], stddev=0.01))\n        print(np.shape(tf.matmul(self.corrupted_rating_matrix, _W)))\n        print(np.shape(tf.nn.embedding_lookup(_V, self.user_id)))\n        layer_1 = tf.sigmoid(tf.matmul(self.corrupted_rating_matrix, _W) + tf.nn.embedding_lookup(_V, self.user_id) + b)\n        self.layer_2 = tf.sigmoid(tf.matmul(layer_1, _W_prime) + b_prime)\n\n        self.loss = - tf.reduce_sum(\n            self.rating_matrix * tf.log(self.layer_2) + (1 - self.rating_matrix) * tf.log(1 - self.layer_2)) + \\\n            self.reg_rate * (tf.nn.l2_loss(_W) + tf.nn.l2_loss(_W_prime) + tf.nn.l2_loss(_V) +\n                             tf.nn.l2_loss(b) + tf.nn.l2_loss(b_prime))\n\n        self.optimizer = tf.train.AdamOptimizer(learning_rate=self.learning_rate).minimize(self.loss)\n\n    def prepare_data(self, train_data, test_data):\n        self.train_data = self._data_process(train_data)\n        self.neg_items = self._get_neg_items(train_data)\n        self.num_training = self.num_user\n        self.total_batch = int(self.num_training / self.batch_size)\n        self.test_data = test_data\n        self.test_users = set([u for u in self.test_data.keys() if len(self.test_data[u]) > 0])\n        print(""data preparation finished."")\n\n    def train(self):\n        idxs = np.random.permutation(self.num_training)  # shuffled ordering\n\n        for i in range(self.total_batch):\n            start_time = time.time()\n            if i == self.total_batch - 1:\n                batch_set_idx = idxs[i * self.batch_size:]\n            elif i < self.total_batch - 1:\n                batch_set_idx = idxs[i * self.batch_size: (i + 1) * self.batch_size]\n\n            _, loss = self.sess.run([self.optimizer, self.loss], feed_dict={\n                self.corrupted_rating_matrix: self._get_corrupted_input(self.train_data[batch_set_idx, :],\n                                                                        self.corruption_level),\n                self.rating_matrix: self.train_data[batch_set_idx, :],\n                self.user_id: batch_set_idx\n                })\n            if self.verbose and i % self.display_step == 0:\n                print(""Index: %04d; cost= %.9f"" % (i + 1, np.mean(loss)))\n                if self.verbose:\n                    print(""one iteration: %s seconds."" % (time.time() - start_time))\n\n    def test(self):\n        self.reconstruction = self.sess.run(self.layer_2, feed_dict={self.corrupted_rating_matrix: self.train_data,\n                                                                     self.user_id: range(self.num_user)})\n\n        evaluate(self)\n\n    def execute(self, train_data, test_data):\n        self.prepare_data(train_data, test_data)\n        init = tf.global_variables_initializer()\n        self.sess.run(init)\n        for epoch in range(self.epochs):\n            self.train()\n            if epoch % self.T == 0:\n                print(""Epoch: %04d; "" % epoch, end=\'\')\n                self.test()\n\n    def save(self, path):\n        saver = tf.train.Saver()\n        saver.save(self.sess, path)\n\n    def predict(self, user_id, item_id):\n        return np.array(self.reconstruction[np.array(user_id), np.array(item_id)])\n\n    @staticmethod\n    def _data_process(data):\n        return np.asmatrix(data)\n\n    def _get_neg_items(self, data):\n        neg_items = {}\n        for u in range(self.num_user):\n            neg_items[u] = [k for k, i in enumerate(data[u]) if data[u][k] == 0]\n            # print(neg_items[u])\n\n        return neg_items\n\n    @staticmethod\n    def _get_corrupted_input(input_train_data, corruption_level):\n        return np.random.binomial(n=1, p=1 - corruption_level) * input_train_data\n\n\nclass ICDAE(object):\n    """"""\n    Based on CDAE and I-AutoRec, I designed the following item based CDAE, it seems to perform better than CDAE\n    slightly.\n    """"""\n    def __init__(self, sess, num_user, num_item, learning_rate=0.01, reg_rate=0.01, epoch=500, batch_size=300,\n                 verbose=False, t=2, display_step=1000):\n        self.sess = sess\n        self.num_user = num_user\n        self.num_item = num_item\n        self.learning_rate = learning_rate\n        self.reg_rate = reg_rate\n        self.epochs = epoch\n        self.batch_size = batch_size\n        self.verbose = verbose\n        self.T = t\n        self.display_step = display_step\n\n        self.corrupted_interact_matrix = None\n        self.interact_matrix = None\n        self.item_id = None\n        self.corruption_level = None\n        self.layer_2 = None\n        self.loss = None\n        self.optimizer = None\n        self.train_data = None\n        self.neg_items = None\n        self.num_training = None\n        self.total_batch = None\n        self.test_data = None\n        self.test_users = None\n        self.reconstruction = None\n        print(""Item based CDAE."")\n\n    def build_network(self, hidden_neuron=500, corruption_level=0):\n        self.corrupted_interact_matrix = tf.placeholder(dtype=tf.float32, shape=[None, self.num_user])\n        self.interact_matrix = tf.placeholder(dtype=tf.float32, shape=[None, self.num_user])\n        self.item_id = tf.placeholder(dtype=tf.int32, shape=[None])\n        self.corruption_level = corruption_level\n\n        _W = tf.Variable(tf.random_normal([self.num_user, hidden_neuron], stddev=0.01))\n        _W_prime = tf.Variable(tf.random_normal([hidden_neuron, self.num_user], stddev=0.01))\n        _V = tf.Variable(tf.random_normal([self.num_item, hidden_neuron], stddev=0.01))\n\n        b = tf.Variable(tf.random_normal([hidden_neuron], stddev=0.01))\n        b_prime = tf.Variable(tf.random_normal([self.num_user], stddev=0.01))\n        # print(np.shape(tf.matmul(self.corrupted_interact_matrix, _W)))\n        # print(np.shape( tf.nn.embedding_lookup(_V, self.item_id)))\n        layer_1 = tf.sigmoid(tf.matmul(self.corrupted_interact_matrix, _W) + b)\n        self.layer_2 = tf.sigmoid(tf.matmul(layer_1, _W_prime) + b_prime)\n\n        self.loss = - tf.reduce_sum(\n            self.interact_matrix * tf.log(self.layer_2) + (1 - self.interact_matrix) * tf.log(1 - self.layer_2)) + \\\n            self.reg_rate * (tf.nn.l2_loss(_W) + tf.nn.l2_loss(_W_prime) +\n                             tf.nn.l2_loss(b) + tf.nn.l2_loss(b_prime))\n\n        self.optimizer = tf.train.AdamOptimizer(learning_rate=self.learning_rate).minimize(self.loss)\n\n    def prepare_data(self, train_data, test_data):\n        self.train_data = self._data_process(train_data).transpose()\n        self.neg_items = self._get_neg_items(train_data)\n        self.num_training = self.num_item\n        self.total_batch = int(self.num_training / self.batch_size)\n        self.test_data = test_data\n        self.test_users = set([u for u in self.test_data.keys() if len(self.test_data[u]) > 0])\n        print(""data preparation finished."")\n\n    def train(self):\n        idxs = np.random.permutation(self.num_training)  # shuffled ordering\n\n        for i in range(self.total_batch):\n            start_time = time.time()\n            if i == self.total_batch - 1:\n                batch_set_idx = idxs[i * self.batch_size:]\n            elif i < self.total_batch - 1:\n                batch_set_idx = idxs[i * self.batch_size: (i + 1) * self.batch_size]\n\n            _, loss = self.sess.run([self.optimizer, self.loss], feed_dict={\n                self.corrupted_interact_matrix: self._get_corrupted_input(self.train_data[batch_set_idx, :],\n                                                                          self.corruption_level),\n                self.interact_matrix: self.train_data[batch_set_idx, :],\n                self.item_id: batch_set_idx\n                })\n            if self.verbose and i % self.display_step == 0:\n                print(""Index: %04d; cost= %.9f"" % (i + 1, np.mean(loss)))\n                if self.verbose:\n                    print(""one iteration: %s seconds."" % (time.time() - start_time))\n\n    def test(self):\n        self.reconstruction = self.sess.run(self.layer_2, feed_dict={self.corrupted_interact_matrix: self.train_data,\n                                                                     self.item_id: range(self.num_item)}).transpose()\n\n        evaluate(self)\n\n    def execute(self, train_data, test_data):\n        self.prepare_data(train_data, test_data)\n        init = tf.global_variables_initializer()\n        self.sess.run(init)\n        for epoch in range(self.epochs):\n            self.train()\n            if epoch % self.T == 0:\n                print(""Epoch: %04d; "" % epoch, end=\'\')\n                self.test()\n\n    def save(self, path):\n        saver = tf.train.Saver()\n        saver.save(self.sess, path)\n\n    def predict(self, user_id, item_id):\n        return np.array(self.reconstruction[np.array(user_id), np.array(item_id)])\n\n    @staticmethod\n    def _data_process(data):\n        return np.asmatrix(data)\n\n    def _get_neg_items(self, data):\n        neg_items = {}\n        for u in range(self.num_user):\n            neg_items[u] = [k for k, i in enumerate(data[u]) if data[u][k] == 0]\n            # print(neg_items[u])\n\n        return neg_items\n\n    @staticmethod\n    def _get_corrupted_input(input_train_data, corruption_level):\n        return np.random.binomial(n=1, p=1 - corruption_level) * input_train_data\n'"
models/item_ranking/cml.py,37,"b'#!/usr/bin/env python\n""""""Implementation of Collaborative Metric Learning.\nReference: Hsieh, Cheng-Kang, et al. ""Collaborative metric learning."" Proceedings of the 26th International\nConference on World Wide Web. International World Wide Web Conferences Steering Committee, 2017.\n""""""\n\nimport tensorflow as tf\nimport time\n\nfrom utils.evaluation.RankingMetrics import *\n\n__author__ = ""Shuai Zhang""\n__copyright__ = ""Copyright 2018, The DeepRec Project""\n\n__license__ = ""GPL""\n__version__ = ""1.0.0""\n__maintainer__ = ""Shuai Zhang""\n__email__ = ""cheungdaven@gmail.com""\n__status__ = ""Development""\n\n\nclass CML(object):\n    def __init__(self, sess, num_user, num_item, learning_rate=0.1, reg_rate=0.1, epoch=500, batch_size=500,\n                 verbose=False, t=5, display_step=1000):\n        self.sess = sess\n        self.num_user = num_user\n        self.num_item = num_item\n        self.learning_rate = learning_rate\n        self.reg_rate = reg_rate\n        self.epochs = epoch\n        self.batch_size = batch_size\n        self.verbose = verbose\n        self.T = t\n        self.display_step = display_step\n\n        self.user_id = None\n        self.item_id = None\n        self.neg_item_id = None\n        self.keep_rate = None\n        self.pred_distance = None\n        self.pred_distance_neg = None\n        self.loss = None\n        self.optimizer = None\n        self.clip_P = None\n        self.clip_Q = None\n\n        self.user = None\n        self.item = None\n        self.num_training = None\n        self.test_data = None\n        self.total_batch = None\n        self.neg_items = None\n        self.test_users = None\n        print(""You are running CML."")\n\n    def build_network(self, num_factor=100, margin=0.5, norm_clip_value=1):\n\n        self.user_id = tf.placeholder(dtype=tf.int32, shape=[None], name=\'user_id\')\n        self.item_id = tf.placeholder(dtype=tf.int32, shape=[None], name=\'item_id\')\n        self.neg_item_id = tf.placeholder(dtype=tf.int32, shape=[None], name=\'neg_item_id\')\n        self.keep_rate = tf.placeholder(tf.float32)\n\n        _P = tf.Variable(\n            tf.random_normal([self.num_user, num_factor], stddev=1 / (num_factor ** 0.5)), dtype=tf.float32)\n        _Q = tf.Variable(\n            tf.random_normal([self.num_item, num_factor], stddev=1 / (num_factor ** 0.5)), dtype=tf.float32)\n\n        user_embedding = tf.nn.embedding_lookup(_P, self.user_id)\n        item_embedding = tf.nn.embedding_lookup(_Q, self.item_id)\n        neg_item_embedding = tf.nn.embedding_lookup(_Q, self.neg_item_id)\n\n        self.pred_distance = tf.reduce_sum(\n            tf.nn.dropout(tf.squared_difference(user_embedding, item_embedding), self.keep_rate), 1)\n        self.pred_distance_neg = tf.reduce_sum(\n            tf.nn.dropout(tf.squared_difference(user_embedding, neg_item_embedding), self.keep_rate), 1)\n\n        self.loss = tf.reduce_sum(tf.maximum(self.pred_distance - self.pred_distance_neg + margin, 0))\n\n        self.optimizer = tf.train.AdagradOptimizer(self.learning_rate).minimize(self.loss, var_list=[_P, _Q])\n        self.clip_P = tf.assign(_P, tf.clip_by_norm(_P, norm_clip_value, axes=[1]))\n        self.clip_Q = tf.assign(_Q, tf.clip_by_norm(_Q, norm_clip_value, axes=[1]))\n\n        return self\n\n    def prepare_data(self, train_data, test_data):\n        """"""\n        You must prepare the data before train and test the model\n        :param train_data:\n        :param test_data:\n        :return:\n        """"""\n        t = train_data.tocoo()\n        self.user = t.row.reshape(-1)\n        self.item = t.col.reshape(-1)\n        self.num_training = len(self.item)\n        self.test_data = test_data\n        self.total_batch = int(self.num_training / self.batch_size)\n        self.neg_items = self._get_neg_items(train_data.tocsr())\n        self.test_users = set([u for u in self.test_data.keys() if len(self.test_data[u]) > 0])\n        print(self.total_batch)\n        print(""data preparation finished."")\n\n    def train(self):\n        idxs = np.random.permutation(self.num_training)  # shuffled ordering\n        user_random = list(self.user[idxs])\n        item_random = list(self.item[idxs])\n        item_random_neg = []\n        for u in user_random:\n            neg_i = self.neg_items[u]\n            s = np.random.randint(len(neg_i))\n            item_random_neg.append(neg_i[s])\n\n        # train\n        for i in range(self.total_batch):\n            start_time = time.time()\n            batch_user = user_random[i * self.batch_size:(i + 1) * self.batch_size]\n            batch_item = item_random[i * self.batch_size:(i + 1) * self.batch_size]\n            batch_item_neg = item_random_neg[i * self.batch_size:(i + 1) * self.batch_size]\n\n            _, loss, _, _ = self.sess.run((self.optimizer, self.loss, self.clip_P, self.clip_Q),\n                                          feed_dict={self.user_id: batch_user,\n                                                     self.item_id: batch_item,\n                                                     self.neg_item_id: batch_item_neg,\n                                                     self.keep_rate: 0.98})\n\n            if i % self.display_step == 0:\n                if self.verbose:\n                    print(""Index: %04d; cost= %.9f"" % (i + 1, np.mean(loss)))\n                    print(""one iteration: %s seconds."" % (time.time() - start_time))\n\n    def test(self):\n        evaluate(self)\n\n    def execute(self, train_data, test_data):\n\n        self.prepare_data(train_data, test_data)\n\n        init = tf.global_variables_initializer()\n        self.sess.run(init)\n\n        for epoch in range(self.epochs):\n            self.train()\n            if epoch % self.T == 0:\n                print(""Epoch: %04d; "" % epoch, end=\'\')\n                self.test()\n\n    def save(self, path):\n        saver = tf.train.Saver()\n        saver.save(self.sess, path)\n\n    def predict(self, user_id, item_id):\n        return -self.sess.run([self.pred_distance],\n                              feed_dict={self.user_id: user_id, self.item_id: item_id, self.keep_rate: 1})[0]\n\n    def _get_neg_items(self, data):\n        all_items = set(np.arange(self.num_item))\n        neg_items = {}\n        for u in range(self.num_user):\n            neg_items[u] = list(all_items - set(data.getrow(u).nonzero()[1]))\n\n        return neg_items\n\n\nclass CMLwarp(object):\n    """"""\n    To appear.\n\n\n    """"""\n\n    def __init__(self, sess, num_user, num_item, learning_rate=0.1, reg_rate=0.1, epoch=500, batch_size=500,\n                 verbose=False, t=5, display_step=1000):\n        self.sess = sess\n        self.num_user = num_user\n        self.num_item = num_item\n        self.learning_rate = learning_rate\n        self.reg_rate = reg_rate\n        self.epochs = epoch\n        self.batch_size = batch_size\n        self.verbose = verbose\n        self.T = t\n        self.display_step = display_step\n\n        self.user_id = None\n        self.item_id = None\n        self.neg_item_id = None\n        self.keep_rate = None\n        self.pred_distance = None\n        self.pred_distance_neg = None\n        self.loss = None\n        self.optimizer = None\n        self.clip_P = None\n        self.clip_Q = None\n\n        self.user = None\n        self.item = None\n        self.num_training = None\n        self.test_data = None\n        self.total_batch = None\n        self.neg_items = None\n        self.test_users = None\n        print(""CML warp loss."")\n\n    def build_network(self, num_factor=100, margin=0.5, norm_clip_value=1):\n        self.user_id = tf.placeholder(dtype=tf.int32, shape=[None], name=\'user_id\')\n        self.item_id = tf.placeholder(dtype=tf.int32, shape=[None], name=\'item_id\')\n        self.neg_item_id = tf.placeholder(dtype=tf.int32, shape=[None], name=\'neg_item_id\')\n\n        _P = tf.Variable(tf.random_normal([self.num_user, num_factor], stddev=1 / (num_factor ** 0.5)))\n        _Q = tf.Variable(tf.random_normal([self.num_item, num_factor], stddev=1 / (num_factor ** 0.5)))\n\n        user_embedding = tf.nn.embedding_lookup(_P, self.user_id)\n        item_embedding = tf.nn.embedding_lookup(_Q, self.item_id)\n        neg_item_embedding = tf.nn.embedding_lookup(_Q, self.neg_item_id)\n\n        self.pred_distance = tf.reduce_sum(tf.squared_difference(user_embedding, item_embedding), 1)\n        self.pred_distance_neg = tf.reduce_sum(tf.squared_difference(user_embedding, neg_item_embedding), 1)\n\n        self.loss = tf.reduce_sum(tf.maximum(self.pred_distance - self.pred_distance_neg + margin, 0))\n\n        self.optimizer = tf.train.AdagradOptimizer(self.learning_rate).minimize(self.loss, var_list=[_P, _Q])\n        self.clip_P = tf.assign(_P, tf.clip_by_norm(_P, norm_clip_value, axes=[1]))\n        self.clip_Q = tf.assign(_Q, tf.clip_by_norm(_Q, norm_clip_value, axes=[1]))\n\n        return self\n\n    def prepare_data(self, train_data, test_data):\n        """"""\n        You must prepare the data before train and test the model\n        :param train_data:\n        :param test_data:\n        :return:\n        """"""\n        t = train_data.tocoo()\n        self.user = t.row.reshape(-1)\n        self.item = t.col.reshape(-1)\n        self.num_training = len(self.item)\n        self.test_data = test_data\n        self.total_batch = int(self.num_training / self.batch_size)\n        self.neg_items = self._get_neg_items(train_data.tocsr())\n        self.test_users = set([u for u in self.test_data.keys() if len(self.test_data[u]) > 0])\n        print(""data preparation finished."")\n\n    def train(self):\n        idxs = np.random.permutation(self.num_training)  # shuffled ordering\n        user_random = list(self.user[idxs])\n        item_random = list(self.item[idxs])\n        item_random_neg = []\n        for u in user_random:\n            neg_i = self.neg_items[u]\n            s = np.random.randint(len(neg_i))\n            item_random_neg.append(neg_i[s])\n\n        # train\n        for i in range(self.total_batch):\n            start_time = time.time()\n            batch_user = user_random[i * self.batch_size:(i + 1) * self.batch_size]\n            batch_item = item_random[i * self.batch_size:(i + 1) * self.batch_size]\n            batch_item_neg = item_random_neg[i * self.batch_size:(i + 1) * self.batch_size]\n\n            _, loss, _, _ = self.sess.run((self.optimizer, self.loss, self.clip_P, self.clip_Q),\n                                          feed_dict={self.user_id: batch_user,\n                                                     self.item_id: batch_item,\n                                                     self.neg_item_id: batch_item_neg})\n\n            if i % self.display_step == 0:\n                if self.verbose:\n                    print(""Index: %04d; cost= %.9f"" % (i + 1, np.mean(loss)))\n                    print(""one iteration: %s seconds."" % (time.time() - start_time))\n\n    def test(self):\n        evaluate(self)\n\n    def execute(self, train_data, test_data):\n\n        self.prepare_data(train_data, test_data)\n\n        init = tf.global_variables_initializer()\n        self.sess.run(init)\n\n        for epoch in range(self.epochs):\n            self.train()\n            if epoch % self.T == 0:\n                print(""Epoch: %04d; "" % epoch, end=\'\')\n                self.test()\n\n    def save(self, path):\n        saver = tf.train.Saver()\n        saver.save(self.sess, path)\n\n    def predict(self, user_id, item_id):\n        return - self.sess.run([self.pred_distance], feed_dict={self.user_id: user_id, self.item_id: item_id})[0]\n\n    def _get_neg_items(self, data):\n        all_items = set(np.arange(self.num_item))\n        neg_items = {}\n        for u in range(self.num_user):\n            neg_items[u] = list(all_items - set(data.getrow(u).nonzero()[1]))\n\n        return neg_items\n'"
models/item_ranking/dmf.py,0,b''
models/item_ranking/dssm.py,60,"b'#!/usr/bin/env python\n""""""Implementation of Deep Semantic Similarity Model with BPR.\nReference: Huang, Po-Sen, et al. ""Learning deep structured semantic models for web search using clickthrough data."" Proceedings of the 22nd ACM international conference on Conference on information & knowledge management. ACM, 2013.\n""""""\n\nimport tensorflow as tf\nimport time\nimport numpy as np\n\nfrom utils.evaluation.RankingMetrics import *\n\n__author__ = ""Shuai Zhang""\n__copyright__ = ""Copyright 2018, The DeepRec Project""\n\n__license__ = ""GPL""\n__version__ = ""1.0.0""\n__maintainer__ = ""Shuai Zhang""\n__email__ = ""cheungdaven@gmail.com""\n__status__ = ""Development""\n\n\nclass DSSM():\n    def __init__(self, sess, num_user, num_item, learning_rate=0.001, reg_rate=0.1, epoch=500, batch_size=1024,\n                 verbose=False, T=5, display_step=1000):\n        self.learning_rate = learning_rate\n        self.epochs = epoch\n        self.batch_size = batch_size\n        self.reg_rate = reg_rate\n        self.sess = sess\n        self.num_user = num_user\n        self.num_item = num_item\n        self.verbose = verbose\n        self.T = T\n        self.display_step = display_step\n        print(""BPRMF."")\n\n    def build_network(self, user_side_info, item_side_info, hidden_dim=100, output_size=30):\n\n        self.user_id = tf.placeholder(dtype=tf.int32, shape=[None], name=\'user_id\')\n        self.item_id = tf.placeholder(dtype=tf.int32, shape=[None], name=\'item_id\')\n        self.neg_item_id = tf.placeholder(dtype=tf.int32, shape=[None], name=\'neg_item_id\')\n        self.y = tf.placeholder(""float"", [None], \'rating\')\n\n        self.user_side_info = tf.constant(user_side_info, dtype=tf.float32)\n        self.item_side_info = tf.constant(item_side_info, dtype=tf.float32)\n\n        user_input_dim = len(user_side_info[0])\n        item_input_dim = len(item_side_info[0])\n\n        user_input = tf.gather(self.user_side_info, self.user_id, axis=0)\n        item_input = tf.gather(self.item_side_info, self.item_id, axis=0)\n        neg_item_input = tf.gather(self.item_side_info, self.neg_item_id, axis=0)\n\n        layer_1 = tf.layers.dense(inputs=user_input, units=user_input_dim,\n                                  bias_initializer=tf.random_normal_initializer,\n                                  kernel_initializer=tf.random_normal_initializer, activation=tf.sigmoid,\n                                  kernel_regularizer=tf.contrib.layers.l2_regularizer(scale=self.reg_rate))\n        layer_2 = tf.layers.dense(inputs=layer_1, units=hidden_dim, activation=tf.sigmoid,\n                                  bias_initializer=tf.random_normal_initializer,\n                                  kernel_initializer=tf.random_normal_initializer,\n                                  kernel_regularizer=tf.contrib.layers.l2_regularizer(scale=self.reg_rate))\n        layer_3 = tf.layers.dense(inputs=layer_2, units=hidden_dim, activation=tf.sigmoid,\n                                  bias_initializer=tf.random_normal_initializer,\n                                  kernel_initializer=tf.random_normal_initializer,\n                                  kernel_regularizer=tf.contrib.layers.l2_regularizer(scale=self.reg_rate))\n        layer_4 = tf.layers.dense(inputs=layer_3, units=hidden_dim, activation=tf.sigmoid,\n                                  bias_initializer=tf.random_normal_initializer,\n                                  kernel_initializer=tf.random_normal_initializer,\n                                  kernel_regularizer=tf.contrib.layers.l2_regularizer(scale=self.reg_rate))\n        user_output = tf.layers.dense(inputs=layer_4, units=output_size, activation=None,\n                                      bias_initializer=tf.random_normal_initializer,\n                                      kernel_initializer=tf.random_normal_initializer,\n                                      kernel_regularizer=tf.contrib.layers.l2_regularizer(scale=self.reg_rate))\n\n        layer_1 = tf.layers.dense(inputs=item_input, units=item_input_dim,\n                                  bias_initializer=tf.random_normal_initializer,\n                                  kernel_initializer=tf.random_normal_initializer, activation=tf.sigmoid,\n                                  kernel_regularizer=tf.contrib.layers.l2_regularizer(scale=self.reg_rate))\n        layer_2 = tf.layers.dense(inputs=layer_1, units=hidden_dim, activation=tf.sigmoid,\n                                  bias_initializer=tf.random_normal_initializer,\n                                  kernel_initializer=tf.random_normal_initializer,\n                                  kernel_regularizer=tf.contrib.layers.l2_regularizer(scale=self.reg_rate))\n        layer_3 = tf.layers.dense(inputs=layer_2, units=hidden_dim, activation=tf.sigmoid,\n                                  bias_initializer=tf.random_normal_initializer,\n                                  kernel_initializer=tf.random_normal_initializer,\n                                  kernel_regularizer=tf.contrib.layers.l2_regularizer(scale=self.reg_rate))\n        layer_4 = tf.layers.dense(inputs=layer_3, units=hidden_dim, activation=tf.sigmoid,\n                                  bias_initializer=tf.random_normal_initializer,\n                                  kernel_initializer=tf.random_normal_initializer,\n                                  kernel_regularizer=tf.contrib.layers.l2_regularizer(scale=self.reg_rate))\n        item_output = tf.layers.dense(inputs=layer_4, units=output_size, activation=None,\n                                      bias_initializer=tf.random_normal_initializer,\n                                      kernel_initializer=tf.random_normal_initializer,\n                                      kernel_regularizer=tf.contrib.layers.l2_regularizer(scale=self.reg_rate))\n\n        self.pred_rating = tf.reshape(output, [-1])\n\n        user_latent_factor = tf.nn.embedding_lookup(self.P, self.user_id)\n        item_latent_factor = tf.nn.embedding_lookup(self.Q, self.item_id)\n        neg_item_latent_factor = tf.nn.embedding_lookup(self.Q, self.neg_item_id)\n\n        self.pred_y = tf.reduce_sum(tf.multiply(user_latent_factor, item_latent_factor), 1)\n        self.pred_y_neg = tf.reduce_sum(tf.multiply(user_latent_factor, neg_item_latent_factor), 1)\n\n        self.loss = - tf.reduce_sum(tf.log(tf.sigmoid(self.pred_y - self.pred_y_neg))) + self.reg_rate * (\n        tf.norm(self.P) + tf.norm(self.Q))\n\n        self.optimizer = tf.train.AdamOptimizer(learning_rate=self.learning_rate).minimize(self.loss)\n\n        return self\n\n    def prepare_data(self, train_data, test_data):\n        \'\'\'\n        You must prepare the data before train and test the model\n        :param train_data:\n        :param test_data:\n        :return:\n        \'\'\'\n        t = train_data.tocoo()\n        self.user = t.row.reshape(-1)\n        self.item = t.col.reshape(-1)\n        self.num_training = len(self.item)\n        self.test_data = test_data\n        self.total_batch = int(self.num_training / self.batch_size)\n        self.neg_items = self._get_neg_items(train_data.tocsr())\n        self.test_users = set([u for u in self.test_data.keys() if len(self.test_data[u]) > 0])\n        print(""data preparation finished."")\n        return self\n\n    def train(self):\n        idxs = np.random.permutation(self.num_training)  # shuffled ordering\n        user_random = list(self.user[idxs])\n        item_random = list(self.item[idxs])\n        item_random_neg = []\n        for u in user_random:\n            neg_i = self.neg_items[u]\n            s = np.random.randint(len(neg_i))\n            item_random_neg.append(neg_i[s])\n\n        # train\n        for i in range(self.total_batch):\n            start_time = time.time()\n            batch_user = user_random[i * self.batch_size:(i + 1) * self.batch_size]\n            batch_item = item_random[i * self.batch_size:(i + 1) * self.batch_size]\n            batch_item_neg = item_random_neg[i * self.batch_size:(i + 1) * self.batch_size]\n\n            _, loss = self.sess.run((self.optimizer, self.loss), feed_dict={self.user_id: batch_user,\n                                                                            self.item_id: batch_item,\n                                                                            self.neg_item_id: batch_item_neg})\n\n            if i % self.display_step == 0:\n                if self.verbose:\n                    print(""Index: %04d; cost= %.9f"" % (i + 1, np.mean(loss)))\n                    print(""one iteration: %s seconds."" % (time.time() - start_time))\n\n    def test(self):\n        evaluate(self)\n\n    def execute(self, train_data, test_data):\n\n        self.prepare_data(train_data, test_data)\n\n        init = tf.global_variables_initializer()\n        self.sess.run(init)\n\n        for epoch in range(self.epochs):\n            self.train()\n            if (epoch) % self.T == 0:\n                print(""Epoch: %04d; "" % (epoch), end=\'\')\n                self.test()\n\n    def save(self, path):\n        saver = tf.train.Saver()\n        saver.save(self.sess, path)\n\n    def predict(self, user_id, item_id):\n        return self.sess.run([self.pred_y], feed_dict={self.user_id: user_id, self.item_id: item_id})[0]\n\n    def _get_neg_items(self, data):\n        all_items = set(np.arange(self.num_item))\n        neg_items = {}\n        for u in range(self.num_user):\n            neg_items[u] = list(all_items - set(data.getrow(u).nonzero()[1]))\n\n        return neg_items\n'"
models/item_ranking/gmf.py,15,"b'#!/usr/bin/env python\n""""""Implementation of Neural Collaborative Filtering.\nReference: He, Xiangnan, et al. ""Neural collaborative filtering."" Proceedings of the 26th International Conference\non World Wide Web. International World Wide Web Conferences Steering Committee, 2017.\n""""""\n\nimport tensorflow as tf\nimport time\nimport random\nfrom utils.evaluation.RankingMetrics import *\n\n__author__ = ""Shuai Zhang""\n__copyright__ = ""Copyright 2018, The DeepRec Project""\n\n__license__ = ""GPL""\n__version__ = ""1.0.0""\n__maintainer__ = ""Shuai Zhang""\n__email__ = ""cheungdaven@gmail.com""\n__status__ = ""Development""\n\n\nclass GMF(object):\n    def __init__(self, sess, num_user, num_item, learning_rate=0.5, reg_rate=0.01, epoch=500, batch_size=256,\n                 verbose=False, t=1, display_step=1000):\n        self.sess = sess\n        self.num_user = num_user\n        self.num_item = num_item\n        self.learning_rate = learning_rate\n        self.reg_rate = reg_rate\n        self.epochs = epoch\n        self.batch_size = batch_size\n        self.verbose = verbose\n        self.T = t\n        self.display_step = display_step\n\n        self.num_neg_sample = None\n        self.user_id = None\n        self.item_id = None\n        self.y = None\n        self.P = None\n        self.Q = None\n        self.pred_y = None\n        self.loss = None\n        self.optimizer = None\n\n        self.test_data = None\n        self.user = None\n        self.item = None\n        self.label = None\n        self.neg_items = None\n        self.test_users = None\n\n        self.num_training = None\n        self.total_batch = None\n        print(""You are running GMF."")\n\n    def build_network(self, num_factor=10, num_neg_sample=20):\n        self.num_neg_sample = num_neg_sample\n        self.user_id = tf.placeholder(dtype=tf.int32, shape=[None], name=\'user_id\')\n        self.item_id = tf.placeholder(dtype=tf.int32, shape=[None], name=\'item_id\')\n        self.y = tf.placeholder(dtype=tf.float32, shape=[None], name=\'y\')\n\n        self.P = tf.Variable(tf.random_normal([self.num_user, num_factor]), dtype=tf.float32)\n        self.Q = tf.Variable(tf.random_normal([self.num_item, num_factor]), dtype=tf.float32)\n\n        user_latent_factor = tf.nn.embedding_lookup(self.P, self.user_id)\n        item_latent_factor = tf.nn.embedding_lookup(self.Q, self.item_id)\n        _GMF = tf.multiply(user_latent_factor, item_latent_factor)\n\n        self.pred_y = tf.nn.sigmoid(tf.reduce_sum(_GMF, axis=1))\n\n        # -{y.log(p{y=1}) + (1-y).log(1 - p{y=1})} + {regularization loss...}\n        self.loss = - tf.reduce_sum(\n            self.y * tf.log(self.pred_y + 1e-10) + (1 - self.y) * tf.log(1 - self.pred_y + 1e-10)) + \\\n            self.reg_rate * (tf.nn.l2_loss(self.P) + tf.nn.l2_loss(self.Q))\n\n        self.optimizer = tf.train.AdagradOptimizer(self.learning_rate).minimize(self.loss)\n\n        return self\n\n    def prepare_data(self, train_data, test_data):\n        """"""\n        You must prepare the data before train and test the model.\n        :param train_data:\n        :param test_data:\n        :return:\n        """"""\n        t = train_data.tocoo()\n        self.user = list(t.row.reshape(-1))\n        self.item = list(t.col.reshape(-1))\n        self.label = list(t.data)\n        self.test_data = test_data\n\n        self.neg_items = self._get_neg_items(train_data.tocsr())\n        self.test_users = set([u for u in self.test_data.keys() if len(self.test_data[u]) > 0])\n\n        print(""data preparation finished."")\n        return self\n\n    def train(self):\n        item_temp = self.item[:]\n        user_temp = self.user[:]\n        labels_temp = self.label[:]\n\n        user_append = []\n        item_append = []\n        values_append = []\n        for u in self.user:\n            list_of_random_items = random.sample(self.neg_items[u], self.num_neg_sample)\n            user_append += [u] * self.num_neg_sample\n            item_append += list_of_random_items\n            values_append += [0] * self.num_neg_sample\n\n        item_temp += item_append\n        user_temp += user_append\n        labels_temp += values_append\n\n        self.num_training = len(item_temp)\n        self.total_batch = int(self.num_training / self.batch_size)\n        # print(self.total_batch)\n        idxs = np.random.permutation(self.num_training)  # shuffled ordering\n        user_random = list(np.array(user_temp)[idxs])\n        item_random = list(np.array(item_temp)[idxs])\n        labels_random = list(np.array(labels_temp)[idxs])\n\n        # train\n        for i in range(self.total_batch):\n            start_time = time.time()\n            batch_user = user_random[i * self.batch_size:(i + 1) * self.batch_size]\n            batch_item = item_random[i * self.batch_size:(i + 1) * self.batch_size]\n            batch_label = labels_random[i * self.batch_size:(i + 1) * self.batch_size]\n\n            _, loss = self.sess.run((self.optimizer, self.loss),\n                                    feed_dict={self.user_id: batch_user, self.item_id: batch_item, self.y: batch_label})\n\n            if i % self.display_step == 0:\n                if self.verbose:\n                    print(""Index: %04d; cost= %.9f"" % (i + 1, np.mean(loss)))\n                    print(""one iteration: %s seconds."" % (time.time() - start_time))\n\n    def test(self):\n        evaluate(self)\n\n    def execute(self, train_data, test_data):\n        self.prepare_data(train_data, test_data)\n\n        init = tf.global_variables_initializer()\n        self.sess.run(init)\n\n        for epoch in range(self.epochs):\n            self.train()\n            if epoch % self.T == 0:\n                print(""Epoch: %04d; "" % epoch, end=\'\')\n                self.test()\n\n    def save(self, path):\n        saver = tf.train.Saver()\n        saver.save(self.sess, path)\n\n    def predict(self, user_id, item_id):\n        return self.sess.run([self.pred_y], feed_dict={self.user_id: user_id, self.item_id: item_id})[0]\n\n    def _get_neg_items(self, data):\n        all_items = set(np.arange(self.num_item))\n        neg_items = {}\n        for u in range(self.num_user):\n            neg_items[u] = list(all_items - set(data.getrow(u).nonzero()[1]))\n\n        return neg_items\n'"
models/item_ranking/jrl.py,31,"b'#!/usr/bin/env python\n""""""Implementation of Joint Representation Learning .\nReference: Zhang, Yongfeng, et al. ""Joint representation learning for top-n recommendation with heterogeneous information sources."" Proceedings of the 2017 ACM on Conference on Information and Knowledge Management. ACM, 2017.\n""""""\n\nimport tensorflow as tf\nimport time\n\nimport random\n\nfrom utils.evaluation.RankingMetrics import *\n\n__author__ = ""Shuai Zhang""\n__copyright__ = ""Copyright 2018, The DeepRec Project""\n\n__license__ = ""GPL""\n__version__ = ""1.0.0""\n__maintainer__ = ""Shuai Zhang""\n__email__ = ""cheungdaven@gmail.com""\n__status__ = ""Development""\n\n\nclass JRL():\n    """"""\n    Here we do not use the side information.\n    """"""\n\n    def __init__(self, sess, num_user, num_item, learning_rate=0.5, reg_rate=0.01, epoch=500, batch_size=256,\n                 verbose=False, T=1, display_step=1000):\n        self.learning_rate = learning_rate\n        self.epochs = epoch\n        self.batch_size = batch_size\n        self.reg_rate = reg_rate\n        self.sess = sess\n        self.num_user = num_user\n        self.num_item = num_item\n        self.verbose = verbose\n        self.T = T\n        self.display_step = display_step\n        print(""NeuMF."")\n\n    def build_network(self, num_factor=10, num_neg_sample=20, hidden_dimension=10):\n        self.num_neg_sample = num_neg_sample\n        self.user_id = tf.placeholder(dtype=tf.int32, shape=[None], name=\'user_id\')\n        self.item_id = tf.placeholder(dtype=tf.int32, shape=[None], name=\'item_id\')\n        self.y = tf.placeholder(dtype=tf.float32, shape=[None], name=\'y\')\n\n        self.P = tf.Variable(tf.random_normal([self.num_user, num_factor]), dtype=tf.float32)\n        self.Q = tf.Variable(tf.random_normal([self.num_item, num_factor]), dtype=tf.float32)\n\n        user_latent_factor = tf.nn.embedding_lookup(self.P, self.user_id)\n        item_latent_factor = tf.nn.embedding_lookup(self.Q, self.item_id)\n        GMF = tf.multiply(user_latent_factor, item_latent_factor)\n\n        layer_1 = tf.layers.dense(inputs=GMF, units=num_factor, kernel_initializer=tf.random_normal_initializer,\n                                  activation=tf.sigmoid,\n                                  kernel_regularizer=tf.contrib.layers.l2_regularizer(scale=self.reg_rate))\n        layer_2 = tf.layers.dense(inputs=layer_1, units=hidden_dimension, activation=tf.sigmoid,\n                                  kernel_initializer=tf.random_normal_initializer,\n                                  kernel_regularizer=tf.contrib.layers.l2_regularizer(scale=self.reg_rate))\n        layer_3 = tf.layers.dense(inputs=layer_2, units=hidden_dimension, activation=tf.sigmoid,\n                                  kernel_initializer=tf.random_normal_initializer,\n                                  kernel_regularizer=tf.contrib.layers.l2_regularizer(scale=self.reg_rate))\n        layer_4 = tf.layers.dense(inputs=layer_3, units=hidden_dimension, activation=tf.sigmoid,\n                                  kernel_initializer=tf.random_normal_initializer,\n                                  kernel_regularizer=tf.contrib.layers.l2_regularizer(scale=self.reg_rate))\n        output = tf.layers.dense(inputs=layer_4, units=hidden_dimension, activation=tf.sigmoid,\n                                 kernel_initializer=tf.random_normal_initializer,\n                                 kernel_regularizer=tf.contrib.layers.l2_regularizer(scale=self.reg_rate))\n\n        self.pred_y = tf.nn.sigmoid(tf.reduce_sum(output, 1))\n\n        self.loss = - tf.reduce_sum(\n            self.y * tf.log(self.pred_y + 1e-10) + (1 - self.y) * tf.log(1 - self.pred_y + 1e-10)) \\\n                    + tf.losses.get_regularization_loss() + self.reg_rate * (\n        tf.nn.l2_loss(self.P) + tf.nn.l2_loss(self.Q))\n\n        self.optimizer = tf.train.AdagradOptimizer(self.learning_rate).minimize(self.loss)\n\n        return self\n\n    def prepare_data(self, train_data, test_data):\n        \'\'\'\n        You must prepare the data before train and test the model\n        :param train_data:\n        :param test_data:\n        :return:\n        \'\'\'\n        t = train_data.tocoo()\n        self.user = list(t.row.reshape(-1))\n        self.item = list(t.col.reshape(-1))\n        self.label = list(t.data)\n        self.test_data = test_data\n\n        self.neg_items = self._get_neg_items(train_data.tocsr())\n        self.test_users = set([u for u in self.test_data.keys() if len(self.test_data[u]) > 0])\n\n        print(""data preparation finished."")\n        return self\n\n    def train(self):\n\n        item_temp = self.item[:]\n        user_temp = self.user[:]\n        labels_temp = self.label[:]\n\n        user_append = []\n        item_append = []\n        values_append = []\n        for u in self.user:\n            list_of_random_items = random.sample(self.neg_items[u], self.num_neg_sample)\n            user_append += [u] * self.num_neg_sample\n            item_append += list_of_random_items\n            values_append += [0] * self.num_neg_sample\n\n        item_temp += item_append\n        user_temp += user_append\n        labels_temp += values_append\n\n        self.num_training = len(item_temp)\n        self.total_batch = int(self.num_training / self.batch_size)\n        # print(self.total_batch)\n        idxs = np.random.permutation(self.num_training)  # shuffled ordering\n        user_random = list(np.array(user_temp)[idxs])\n        item_random = list(np.array(item_temp)[idxs])\n        labels_random = list(np.array(labels_temp)[idxs])\n\n        # train\n        for i in range(self.total_batch):\n            start_time = time.time()\n            batch_user = user_random[i * self.batch_size:(i + 1) * self.batch_size]\n            batch_item = item_random[i * self.batch_size:(i + 1) * self.batch_size]\n            batch_label = labels_random[i * self.batch_size:(i + 1) * self.batch_size]\n\n            _, loss = self.sess.run((self.optimizer, self.loss),\n                                    feed_dict={self.user_id: batch_user, self.item_id: batch_item, self.y: batch_label})\n\n            if i % self.display_step == 0:\n                if self.verbose:\n                    print(""Index: %04d; cost= %.9f"" % (i + 1, np.mean(loss)))\n                    print(""one iteration: %s seconds."" % (time.time() - start_time))\n\n    def test(self):\n        evaluate(self)\n\n    def execute(self, train_data, test_data):\n\n        self.prepare_data(train_data, test_data)\n\n        init = tf.global_variables_initializer()\n        self.sess.run(init)\n\n        for epoch in range(self.epochs):\n            self.train()\n            if (epoch) % self.T == 0:\n                print(""Epoch: %04d; "" % (epoch), end=\'\')\n                self.test()\n\n    def save(self, path):\n        saver = tf.train.Saver()\n        saver.save(self.sess, path)\n\n    def predict(self, user_id, item_id):\n        return self.sess.run([self.pred_y], feed_dict={self.user_id: user_id, self.item_id: item_id})[0]\n\n    def _get_neg_items(self, data):\n        all_items = set(np.arange(self.num_item))\n        neg_items = {}\n        for u in range(self.num_user):\n            neg_items[u] = list(all_items - set(data.getrow(u).nonzero()[1]))\n\n        return neg_items\n'"
models/item_ranking/lrml.py,29,"b'#!/usr/bin/env python\n""""""Implementation of Latent Relational Metric Learning (LRML)\nWWW 2018. Authors - Yi Tay, Luu Anh Tuan, Siu Cheung Hui\n""""""\n\nimport tensorflow as tf\nimport time\nimport numpy as np\n\nfrom utils.evaluation.RankingMetrics import *\n\n__author__ = ""Yi Tay""\n__copyright__ = ""Copyright 2018, The DeepRec Project""\n\n__license__ = ""GPL""\n__version__ = ""1.0.0""\n__maintainer__ = ""Yi Tay""\n__email__ = ""ytay017@gmail.com""\n__status__ = ""Development""\n\n\nclass LRML():\n    """""" This is a reference implementation of the LRML model\n    proposed in WWW\'18.\n    Note: This was mainly adapted for the DeepRec repository\n    and is copied from the first author\'s\n    private code repository. This has NOT undergone sanity checks.\n    """"""\n\n    def __init__(self, sess, num_user, num_item, learning_rate=0.1,\n                 reg_rate=0.1, epoch=500, batch_size=500,\n                 verbose=False, T=5, display_step=1000, mode=1,\n                 copy_relations=True, dist=\'L1\', num_mem=100):\n        """""" This model takes after the CML structure implemented by Shuai.\n        There are several new hyperparameters introduced which are explained\n        as follows:\n        Args:\n            mode:`int`.1 or 2. varies the attention computation.\n                    2 corresponds to the implementation in the paper.\n                    But 1 seems to produce better results.\n            copy_relations: `bool`. Reuse relation vector for negative sample.\n            dist: `str`. L1 or L2. Use L1 or L2 distance.\n            num_mem: `int`. Controls the number of memory rows.\n        """"""\n        self.learning_rate = learning_rate\n        self.epochs = epoch\n        self.batch_size = batch_size\n        self.reg_rate = reg_rate\n        self.sess = sess\n        self.num_user = num_user\n        self.num_item = num_item\n        self.verbose = verbose\n        self.T = T\n        self.mode = mode\n        self.display_step = display_step\n        # self.init = 1 / (num_factor ** 0.5)\n        self.num_mem = num_mem\n        self.copy_relations = copy_relations\n        self.dist = dist\n        print(""LRML."")\n\n    def lram(self, a, b,\n             reuse=None, initializer=None, k=10, relation=None):\n        """""" Generates relation given user (a) and item(b)\n        """"""\n        with tf.variable_scope(\'lrml\', reuse=reuse) as scope:\n            if (relation is None):\n                _dim = a.get_shape().as_list()[1]\n                key_matrix = tf.get_variable(\'key_matrix\', [_dim, k],\n                                             initializer=initializer)\n                memories = tf.get_variable(\'memory\', [_dim, k],\n                                           initializer=initializer)\n                user_item_key = a * b\n                key_attention = tf.matmul(user_item_key, key_matrix)\n                key_attention = tf.nn.softmax(key_attention)  # bsz x k\n                if (self.mode == 1):\n                    relation = tf.matmul(key_attention, memories)\n                elif (self.mode == 2):\n                    key_attention = tf.expand_dims(key_attention, 1)\n                    relation = key_attention * memories\n                    relation = tf.reduce_sum(relation, 2)\n        return relation\n\n    def build_network(self, num_factor=100, margin=0.5, norm_clip_value=1):\n        """""" Main computational graph\n        """"""\n        # stddev initialize\n        init = 1 / (num_factor ** 0.5)\n\n        self.user_id = tf.placeholder(dtype=tf.int32, shape=[None], name=\'user_id\')\n        self.item_id = tf.placeholder(dtype=tf.int32, shape=[None], name=\'item_id\')\n        self.neg_item_id = tf.placeholder(dtype=tf.int32, shape=[None], name=\'neg_item_id\')\n        self.keep_rate = tf.placeholder(tf.float32)\n\n        P = tf.Variable(tf.random_normal([self.num_user, num_factor], stddev=init), dtype=tf.float32)\n        Q = tf.Variable(tf.random_normal([self.num_item, num_factor], stddev=init), dtype=tf.float32)\n\n        user_embedding = tf.nn.embedding_lookup(P, self.user_id)\n        item_embedding = tf.nn.embedding_lookup(Q, self.item_id)\n        neg_item_embedding = tf.nn.embedding_lookup(Q, self.neg_item_id)\n\n        selected_memory = self.lram(user_embedding, item_embedding,\n                                    reuse=None,\n                                    initializer=tf.random_normal_initializer(init),\n                                    k=self.num_mem)\n        if (self.copy_relations == False):\n            selected_memory_neg = self.lram(user_embedding, neg_item_embedding,\n                                            reuse=True,\n                                            initializer=tf.random_normal_initializer(init),\n                                            k=self.num_mem)\n        else:\n            selected_memory_neg = selected_memory\n\n        energy_pos = item_embedding - (user_embedding + selected_memory)\n        energy_neg = neg_item_embedding - (user_embedding + selected_memory_neg)\n\n        if (self.dist == \'L2\'):\n            pos_dist = tf.sqrt(tf.reduce_sum(tf.square(energy_pos), 1) + 1E-3)\n            neg_dist = tf.sqrt(tf.reduce_sum(tf.square(energy_neg), 1) + 1E-3)\n        elif (self.dist == \'L1\'):\n            pos_dist = tf.reduce_sum(tf.abs(energy_pos), 1)\n            neg_dist = tf.reduce_sum(tf.abs(energy_neg), 1)\n\n        self.pred_distance = pos_dist\n        self.pred_distance_neg = neg_dist\n\n        self.loss = tf.reduce_sum(tf.maximum(self.pred_distance - self.pred_distance_neg + margin, 0))\n\n        self.optimizer = tf.train.AdagradOptimizer(self.learning_rate).minimize(self.loss, var_list=[P, Q])\n        self.clip_P = tf.assign(P, tf.clip_by_norm(P, norm_clip_value, axes=[1]))\n        self.clip_Q = tf.assign(Q, tf.clip_by_norm(Q, norm_clip_value, axes=[1]))\n\n        return self\n\n    def prepare_data(self, train_data, test_data):\n        \'\'\'\n        You must prepare the data before train and test the model\n        :param train_data:\n        :param test_data:\n        :return:\n        \'\'\'\n        t = train_data.tocoo()\n        self.user = t.row.reshape(-1)\n        self.item = t.col.reshape(-1)\n        self.num_training = len(self.item)\n        self.test_data = test_data\n        self.total_batch = int(self.num_training / self.batch_size)\n        self.neg_items = self._get_neg_items(train_data.tocsr())\n        self.test_users = set([u for u in self.test_data.keys() if len(self.test_data[u]) > 0])\n        print(self.total_batch)\n        print(""data preparation finished."")\n        return self\n\n    def train(self):\n        idxs = np.random.permutation(self.num_training)  # shuffled ordering\n        user_random = list(self.user[idxs])\n        item_random = list(self.item[idxs])\n        item_random_neg = []\n        for u in user_random:\n            neg_i = self.neg_items[u]\n            s = np.random.randint(len(neg_i))\n            item_random_neg.append(neg_i[s])\n\n        # train\n        for i in range(self.total_batch):\n            start_time = time.time()\n            batch_user = user_random[i * self.batch_size:(i + 1) * self.batch_size]\n            batch_item = item_random[i * self.batch_size:(i + 1) * self.batch_size]\n            batch_item_neg = item_random_neg[i * self.batch_size:(i + 1) * self.batch_size]\n\n            _, loss, _, _ = self.sess.run((self.optimizer, self.loss, self.clip_P, self.clip_Q),\n                                          feed_dict={self.user_id: batch_user,\n                                                     self.item_id: batch_item,\n                                                     self.neg_item_id: batch_item_neg,\n                                                     self.keep_rate: 0.98})\n\n            if i % self.display_step == 0:\n                if self.verbose:\n                    print(""Index: %04d; cost= %.9f"" % (i + 1, np.mean(loss)))\n                    print(""one iteration: %s seconds."" % (time.time() - start_time))\n\n    def test(self):\n        evaluate(self)\n\n    def execute(self, train_data, test_data):\n\n        self.prepare_data(train_data, test_data)\n\n        init = tf.global_variables_initializer()\n        self.sess.run(init)\n\n        for epoch in range(self.epochs):\n            self.train()\n            if (epoch) % self.T == 0:\n                print(""Epoch: %04d; "" % (epoch), end="""")\n                self.test()\n\n    def save(self, path):\n        saver = tf.train.Saver()\n        saver.save(self.sess, path)\n\n    def predict(self, user_id, item_id):\n        return -self.sess.run([self.pred_distance],\n                              feed_dict={self.user_id: user_id,\n                                         self.item_id: item_id, self.keep_rate: 1})[0]\n\n    def _get_neg_items(self, data):\n        all_items = set(np.arange(self.num_item))\n        neg_items = {}\n        for u in range(self.num_user):\n            neg_items[u] = list(all_items - set(data.getrow(u).nonzero()[1]))\n\n        return neg_items\n'"
models/item_ranking/mlp.py,29,"b'#!/usr/bin/env python\n""""""Implementation of Neural Collaborative Filtering.\nReference: He, Xiangnan, et al. ""Neural collaborative filtering."" Proceedings of the 26th International Conference\non World Wide Web. International World Wide Web Conferences Steering Committee, 2017.\n""""""\n\nimport tensorflow as tf\nimport time\nimport numpy as np\nimport random\n\nfrom utils.evaluation.RankingMetrics import *\n\n__author__ = ""Shuai Zhang""\n__copyright__ = ""Copyright 2018, The DeepRec Project""\n\n__license__ = ""GPL""\n__version__ = ""1.0.0""\n__maintainer__ = ""Shuai Zhang""\n__email__ = ""cheungdaven@gmail.com""\n__status__ = ""Development""\n\n\nclass MLP(object):\n    def __init__(self, sess, num_user, num_item, learning_rate=0.5, reg_rate=0.001, epoch=500, batch_size=256,\n                 verbose=False, t=5, display_step=1000):\n        self.sess = sess\n        self.num_user = num_user\n        self.num_item = num_item\n        self.learning_rate = learning_rate\n        self.reg_rate = reg_rate\n        self.epochs = epoch\n        self.batch_size = batch_size\n        self.verbose = verbose\n        self.T = t\n        self.display_step = display_step\n\n        self.num_neg_sample = None \n        self.user_id = None  \n        self.item_id = None  \n        self.y = None  \n        self.P = None  \n        self.Q = None  \n        self.mlp_P = None \n        self.mlp_Q = None\n        self.pred_y = None \n        self.loss = None\n        self.optimizer = None\n\n        self.test_data = None\n        self.user = None\n        self.item = None\n        self.label = None\n        self.neg_items = None  \n        self.test_users = None\n\n        self.num_training = None\n        self.total_batch = None\n        print(""You are running MLP."")\n\n    def build_network(self, num_factor_mlp=10, hidden_dimension=10, num_neg_sample=2):\n        self.num_neg_sample = num_neg_sample\n        self.user_id = tf.placeholder(dtype=tf.int32, shape=[None], name=\'user_id\')\n        self.item_id = tf.placeholder(dtype=tf.int32, shape=[None], name=\'item_id\')\n        self.y = tf.placeholder(dtype=tf.float32, shape=[None], name=\'y\')\n\n        self.mlp_P = tf.Variable(tf.random_normal([self.num_user, num_factor_mlp]), dtype=tf.float32)\n        self.mlp_Q = tf.Variable(tf.random_normal([self.num_item, num_factor_mlp]), dtype=tf.float32)\n\n        mlp_user_latent_factor = tf.nn.embedding_lookup(self.mlp_P, self.user_id)\n        mlp_item_latent_factor = tf.nn.embedding_lookup(self.mlp_Q, self.item_id)\n\n        layer_1 = tf.layers.dense(\n            inputs=tf.concat([mlp_item_latent_factor, mlp_user_latent_factor], axis=1),\n            units=num_factor_mlp * 2,\n            kernel_initializer=tf.random_normal_initializer,\n            activation=tf.nn.relu,\n            kernel_regularizer=tf.contrib.layers.l2_regularizer(scale=self.reg_rate))\n\n        layer_2 = tf.layers.dense(\n            inputs=layer_1,\n            units=hidden_dimension * 2,\n            activation=tf.nn.relu,\n            kernel_initializer=tf.random_normal_initializer,\n            kernel_regularizer=tf.contrib.layers.l2_regularizer(scale=self.reg_rate))\n\n        _MLP = tf.layers.dense(\n            inputs=layer_2,\n            units=hidden_dimension,\n            activation=tf.nn.relu,\n            kernel_initializer=tf.random_normal_initializer,\n            kernel_regularizer=tf.contrib.layers.l2_regularizer(scale=self.reg_rate))\n\n        self.pred_y = tf.nn.sigmoid(tf.reduce_sum(_MLP, axis=1))\n        # self.pred_y = tf.layers.dense(inputs=MLP, units=1, activation=tf.sigmoid)\n\n        # -{y.log(p{y=1}) + (1-y).log(1 - p{y=1})} + {regularization loss...}\n        self.loss = - tf.reduce_sum(\n            self.y * tf.log(self.pred_y + 1e-10) + (1 - self.y) * tf.log(1 - self.pred_y + 1e-10)) + \\\n            tf.losses.get_regularization_loss() + \\\n            self.reg_rate * (tf.nn.l2_loss(self.mlp_P) + tf.nn.l2_loss(self.mlp_Q))\n\n        self.optimizer = tf.train.AdagradOptimizer(self.learning_rate).minimize(self.loss)\n\n        return self\n\n    def prepare_data(self, train_data, test_data):\n        """"""\n        You must prepare the data before train and test the model.\n        :param train_data:\n        :param test_data:\n        :return:\n        """"""\n        t = train_data.tocoo()\n        self.user = list(t.row.reshape(-1))\n        self.item = list(t.col.reshape(-1))\n        self.label = list(t.data)\n        self.test_data = test_data\n\n        self.neg_items = self._get_neg_items(train_data.tocsr())\n        self.test_users = set([u for u in self.test_data.keys() if len(self.test_data[u]) > 0])\n\n        print(""data preparation finished."")\n        return self\n\n    def train(self):\n        item_temp = self.item[:]\n        user_temp = self.user[:]\n        labels_temp = self.label[:]\n\n        user_append = []\n        item_append = []\n        values_append = []\n        for u in self.user:\n            list_of_random_items = random.sample(self.neg_items[u], self.num_neg_sample)\n            user_append += [u] * self.num_neg_sample\n            item_append += list_of_random_items\n            values_append += [0] * self.num_neg_sample\n\n        item_temp += item_append\n        user_temp += user_append\n        labels_temp += values_append\n\n        self.num_training = len(item_temp)\n        self.total_batch = int(self.num_training / self.batch_size)\n        # print(self.total_batch)\n        idxs = np.random.permutation(self.num_training)  # shuffled ordering\n        user_random = list(np.array(user_temp)[idxs])\n        item_random = list(np.array(item_temp)[idxs])\n        labels_random = list(np.array(labels_temp)[idxs])\n\n        # train\n        for i in range(self.total_batch):\n            start_time = time.time()\n            batch_user = user_random[i * self.batch_size:(i + 1) * self.batch_size]\n            batch_item = item_random[i * self.batch_size:(i + 1) * self.batch_size]\n            batch_label = labels_random[i * self.batch_size:(i + 1) * self.batch_size]\n\n            _, loss = self.sess.run((self.optimizer, self.loss),\n                                    feed_dict={self.user_id: batch_user, self.item_id: batch_item, self.y: batch_label})\n\n            if i % self.display_step == 0:\n                if self.verbose:\n                    print(""Index: %04d; cost= %.9f"" % (i + 1, np.mean(loss)))\n                    print(""one iteration: %s seconds."" % (time.time() - start_time))\n\n    def test(self):\n        evaluate(self)\n\n    def execute(self, train_data, test_data):\n        self.prepare_data(train_data, test_data)\n\n        init = tf.global_variables_initializer()\n        self.sess.run(init)\n\n        for epoch in range(self.epochs):\n            self.train()\n            if epoch % self.T == 0:\n                print(""Epoch: %04d; "" % epoch, end=\'\')\n                self.test()\n\n    def save(self, path):\n        saver = tf.train.Saver()\n        saver.save(self.sess, path)\n\n    def predict(self, user_id, item_id):\n        return self.sess.run([self.pred_y], feed_dict={self.user_id: user_id, self.item_id: item_id})[0]\n\n    def _get_neg_items(self, data):\n        all_items = set(np.arange(self.num_item))\n        neg_items = {}\n        for u in range(self.num_user):\n            neg_items[u] = list(all_items - set(data.getrow(u).nonzero()[1]))\n\n        return neg_items\n'"
models/item_ranking/neumf.py,46,"b'#!/usr/bin/env python\n""""""Implementation of Neural Collaborative Filtering.\nReference: He, Xiangnan, et al. ""Neural collaborative filtering."" Proceedings of the 26th International Conference\non World Wide Web. International World Wide Web Conferences Steering Committee, 2017.\n""""""\n\nimport tensorflow as tf\nimport time\nimport numpy as np\nimport random\n\nfrom utils.evaluation.RankingMetrics import *\n\n__author__ = ""Shuai Zhang""\n__copyright__ = ""Copyright 2018, The DeepRec Project""\n\n__license__ = ""GPL""\n__version__ = ""1.0.0""\n__maintainer__ = ""Shuai Zhang""\n__email__ = ""cheungdaven@gmail.com""\n__status__ = ""Development""\n\n\nclass NeuMF(object):\n    def __init__(self, sess, num_user, num_item, learning_rate=0.5, reg_rate=0.01, epoch=500, batch_size=256,\n                 verbose=False, t=1, display_step=1000):\n        self.sess = sess\n        self.num_user = num_user\n        self.num_item = num_item\n        self.learning_rate = learning_rate\n        self.reg_rate = reg_rate\n        self.epochs = epoch\n        self.batch_size = batch_size\n        self.verbose = verbose\n        self.T = t\n        self.display_step = display_step\n\n        self.num_neg_sample = None\n        self.user_id = None\n        self.item_id = None\n        self.y = None\n        self.P = None\n        self.Q = None\n        self.mlp_P = None\n        self.mlp_Q = None\n        self.pred_y = None\n        self.loss = None\n        self.optimizer = None\n\n        self.test_data = None\n        self.user = None\n        self.item = None\n        self.label = None\n        self.neg_items = None\n        self.test_users = None\n\n        self.num_training = None\n        self.total_batch = None\n        print(""You are running NeuMF."")\n\n    def build_network(self, num_factor=10, num_factor_mlp=64, hidden_dimension=10, num_neg_sample=30):\n        self.num_neg_sample = num_neg_sample\n        self.user_id = tf.placeholder(dtype=tf.int32, shape=[None], name=\'user_id\')\n        self.item_id = tf.placeholder(dtype=tf.int32, shape=[None], name=\'item_id\')\n        self.y = tf.placeholder(dtype=tf.float32, shape=[None], name=\'y\')\n\n        self.P = tf.Variable(tf.random_normal([self.num_user, num_factor]), dtype=tf.float32)\n        self.Q = tf.Variable(tf.random_normal([self.num_item, num_factor]), dtype=tf.float32)\n\n        self.mlp_P = tf.Variable(tf.random_normal([self.num_user, num_factor_mlp]), dtype=tf.float32)\n        self.mlp_Q = tf.Variable(tf.random_normal([self.num_item, num_factor_mlp]), dtype=tf.float32)\n\n        user_latent_factor = tf.nn.embedding_lookup(self.P, self.user_id)\n        item_latent_factor = tf.nn.embedding_lookup(self.Q, self.item_id)\n        mlp_user_latent_factor = tf.nn.embedding_lookup(self.mlp_P, self.user_id)\n        mlp_item_latent_factor = tf.nn.embedding_lookup(self.mlp_Q, self.item_id)\n\n        _GMF = tf.multiply(user_latent_factor, item_latent_factor)\n\n        layer_1 = tf.layers.dense(\n            inputs=tf.concat([mlp_item_latent_factor, mlp_user_latent_factor], axis=1),\n            units=num_factor_mlp * 2,\n            kernel_initializer=tf.random_normal_initializer,\n            activation=tf.nn.relu,\n            kernel_regularizer=tf.contrib.layers.l2_regularizer(scale=self.reg_rate))\n\n        layer_2 = tf.layers.dense(\n            inputs=layer_1,\n            units=hidden_dimension * 8,\n            activation=tf.nn.relu,\n            kernel_initializer=tf.random_normal_initializer,\n            kernel_regularizer=tf.contrib.layers.l2_regularizer(scale=self.reg_rate))\n\n        layer_3 = tf.layers.dense(\n            inputs=layer_2,\n            units=hidden_dimension * 4,\n            activation=tf.nn.relu,\n            kernel_initializer=tf.random_normal_initializer,\n            kernel_regularizer=tf.contrib.layers.l2_regularizer(scale=self.reg_rate))\n\n        layer_4 = tf.layers.dense(\n            inputs=layer_3,\n            units=hidden_dimension * 2,\n            activation=tf.nn.relu,\n            kernel_initializer=tf.random_normal_initializer,\n            kernel_regularizer=tf.contrib.layers.l2_regularizer(scale=self.reg_rate))\n\n        _MLP = tf.layers.dense(\n                inputs=layer_4,\n                units=hidden_dimension,\n                activation=tf.nn.relu,\n                kernel_initializer=tf.random_normal_initializer,\n                kernel_regularizer=tf.contrib.layers.l2_regularizer(scale=self.reg_rate))\n\n        self.pred_y = tf.nn.sigmoid(tf.reduce_sum(tf.concat([_GMF, _MLP], axis=1), 1))\n\n        # self.pred_y = tf.layers.dense(\n        #     inputs=tf.concat([_GMF, _MLP], axis=1), units=1, activation=tf.sigmoid,\n        #     kernel_initializer=tf.random_normal_initializer,\n        #     kernel_regularizer= tf.contrib.layers.l2_regularizer(scale=self.reg_rate))\n\n        # -{y.log(p{y=1}) + (1-y).log(1 - p{y=1})} + {regularization loss...}\n        self.loss = - tf.reduce_sum(\n            self.y * tf.log(self.pred_y + 1e-10) + (1 - self.y) * tf.log(1 - self.pred_y + 1e-10)) + \\\n            tf.losses.get_regularization_loss() + \\\n            self.reg_rate * (tf.nn.l2_loss(self.P) + tf.nn.l2_loss(self.Q) +\n                             tf.nn.l2_loss(self.mlp_P) + tf.nn.l2_loss(self.mlp_Q))\n\n        self.optimizer = tf.train.AdagradOptimizer(self.learning_rate).minimize(self.loss)\n\n        return self\n\n    def prepare_data(self, train_data, test_data):\n        """"""\n        You must prepare the data before train and test the model.\n\n        :param train_data:\n        :param test_data:\n        :return:\n        """"""\n        t = train_data.tocoo()\n        self.user = list(t.row.reshape(-1))\n        self.item = list(t.col.reshape(-1))\n        self.label = list(t.data)\n        self.test_data = test_data\n\n        self.neg_items = self._get_neg_items(train_data.tocsr())\n        self.test_users = set([u for u in self.test_data.keys() if len(self.test_data[u]) > 0])\n\n        print(""data preparation finished."")\n        return self\n\n    def train(self):\n        item_temp = self.item[:]\n        user_temp = self.user[:]\n        labels_temp = self.label[:]\n\n        user_append = []\n        item_append = []\n        values_append = []\n        for u in self.user:\n            list_of_random_items = random.sample(self.neg_items[u], self.num_neg_sample)\n            user_append += [u] * self.num_neg_sample\n            item_append += list_of_random_items\n            values_append += [0] * self.num_neg_sample\n\n        item_temp += item_append\n        user_temp += user_append\n        labels_temp += values_append\n\n        self.num_training = len(item_temp)\n        self.total_batch = int(self.num_training / self.batch_size)\n        # print(self.total_batch)\n        idxs = np.random.permutation(self.num_training)  # shuffled ordering\n        user_random = list(np.array(user_temp)[idxs])\n        item_random = list(np.array(item_temp)[idxs])\n        labels_random = list(np.array(labels_temp)[idxs])\n\n        # train\n        for i in range(self.total_batch):\n            start_time = time.time()\n            batch_user = user_random[i * self.batch_size:(i + 1) * self.batch_size]\n            batch_item = item_random[i * self.batch_size:(i + 1) * self.batch_size]\n            batch_label = labels_random[i * self.batch_size:(i + 1) * self.batch_size]\n\n            _, loss = self.sess.run((self.optimizer, self.loss),\n                                    feed_dict={self.user_id: batch_user, self.item_id: batch_item, self.y: batch_label})\n\n            if i % self.display_step == 0:\n                if self.verbose:\n                    print(""Index: %04d; cost= %.9f"" % (i + 1, np.mean(loss)))\n                    print(""one iteration: %s seconds."" % (time.time() - start_time))\n\n    def test(self):\n        evaluate(self)\n\n    def execute(self, train_data, test_data):\n        self.prepare_data(train_data, test_data)\n\n        init = tf.global_variables_initializer()\n        self.sess.run(init)\n\n        for epoch in range(self.epochs):\n            self.train()\n            if epoch % self.T == 0:\n                print(""Epoch: %04d; "" % epoch, end=\'\')\n                self.test()\n\n    def save(self, path):\n        saver = tf.train.Saver()\n        saver.save(self.sess, path)\n\n    def predict(self, user_id, item_id):\n        return self.sess.run([self.pred_y], feed_dict={self.user_id: user_id, self.item_id: item_id})[0]\n\n    def _get_neg_items(self, data):\n        all_items = set(np.arange(self.num_item))\n        neg_items = {}\n        for u in range(self.num_user):\n            neg_items[u] = list(all_items - set(data.getrow(u).nonzero()[1]))\n\n        return neg_items\n'"
models/item_ranking/neurec.py,0,b''
models/item_ranking/widedeep.py,0,b''
models/rating_prediction/__init__.py,0,b''
models/rating_prediction/afm.py,42,"b'#!/usr/bin/env python\n""""""Implementation of Attention Factorization Machine.\nReference: Xiao, Jun, et al. ""Attentional factorization machines: Learning the weight of feature interactions\nvia attention networks."" arXiv preprint arXiv:1708.04617 (2017).\n\nOriginal Implementation:\n""""""\n\nimport tensorflow as tf\nimport time\nfrom sklearn.metrics import mean_squared_error\nimport math\n\nfrom utils.evaluation.RatingMetrics import *\n\n__author__ = ""Shuai Zhang""\n__copyright__ = ""Copyright 2018, The DeepRec Project""\n\n__license__ = ""GPL""\n__version__ = ""1.0.0""\n__maintainer__ = ""Shuai Zhang""\n__email__ = ""cheungdaven@gmail.com""\n__status__ = ""Development""\n\n\nclass AFM(object):\n    def __init__(self, sess, num_user, num_item, learning_rate=0.01, reg_rate=0.1, epoch=500, batch_size=4096,\n                 show_time=False, t=2, display_step=1000):\n        self.sess = sess\n        self.num_user = num_user\n        self.num_item = num_item\n        self.learning_rate = learning_rate\n        self.reg_rate = reg_rate\n        self.epochs = epoch\n        self.batch_size = batch_size\n        self.show_time = show_time\n        self.T = t\n        self.display_step = display_step\n\n        self.train_features = None\n        self.y = None\n        self.dropout_keep = None\n        self.feature_embeddings = None\n        self.feature_bias = None\n        self.bias = None\n        self.pred_weight = None\n        self.attention_W = None\n        self.attention_b = None\n        self.attention_p = None\n        self.element_wise_product = None\n        self.interactions = None\n        self.attention_mul = None\n        self.attention_relu = None\n        self.attention_out = None\n        self.AFM = None\n        self.AFM_FM = None\n        self.prediction = None\n        self.f_b = None\n        self.pred_rating = None\n        self.loss = None\n        self.optimizer = None\n        self.num_training = None\n        print(""Attention Factorization Machine."")\n\n    def build_network(self, feature_m, valid_dim=3, num_factor=16):\n        # model dependent arguments\n        self.train_features = tf.placeholder(tf.int32, shape=[None, None])\n        self.y = tf.placeholder(tf.float32, shape=[None, 1])\n        self.dropout_keep = tf.placeholder(tf.float32)\n\n        self.feature_embeddings = tf.Variable(tf.random_normal([feature_m, num_factor], mean=0.0, stddev=0.01))\n\n        self.feature_bias = tf.Variable(tf.random_uniform([feature_m, 1], 0.0, 0.0))\n        self.bias = tf.Variable(tf.constant(0.0))\n        self.pred_weight = tf.Variable(np.ones((num_factor, 1), dtype=np.float32))\n        glorot = np.sqrt(2.0 / (num_factor + num_factor))\n        self.attention_W = tf.Variable(np.random.normal(loc=0, scale=glorot, size=(num_factor, num_factor)),\n                                       dtype=np.float32, name=""attention_W"")  # K * AK\n        self.attention_b = tf.Variable(np.random.normal(loc=0, scale=glorot, size=(1, num_factor)),\n                                       dtype=np.float32, name=""attention_b"")  # 1 * AK\n        self.attention_p = tf.Variable(np.random.normal(loc=0, scale=1, size=num_factor),\n                                       dtype=np.float32, name=""attention_p"")\n\n        nonzero_embeddings = tf.nn.embedding_lookup(self.feature_embeddings, self.train_features)\n\n        element_wise_product_list = []\n        count = 0\n\n        for i in range(0, valid_dim):\n            for j in range(i+1, valid_dim):\n                element_wise_product_list.append(tf.multiply(nonzero_embeddings[:, i, :], nonzero_embeddings[:, j, :]))\n                count += 1\n\n        self.element_wise_product = tf.stack(element_wise_product_list)\n        self.element_wise_product = tf.transpose(self.element_wise_product, perm=[1, 0, 2])\n        self.interactions = tf.reduce_sum(self.element_wise_product, 2)\n\n        num_interactions = int(valid_dim * (valid_dim - 1) / 2)\n\n        self.attention_mul = tf.reshape(tf.matmul(tf.reshape(self.element_wise_product, shape=[-1, num_factor]),\n                                                  self.attention_W), shape=[-1, num_interactions, num_factor])\n        self.attention_relu = tf.reduce_sum(\n            tf.multiply(self.attention_p, tf.nn.relu(self.attention_mul + self.attention_b)), 2, keep_dims=True)\n        self.attention_out = tf.nn.softmax(self.attention_relu)\n        self.attention_out = tf.nn.dropout(self.attention_out, self.dropout_keep)\n\n        self.AFM = tf.reduce_sum(tf.multiply(self.attention_out, self.element_wise_product), 1)\n        self.AFM_FM = tf.reduce_sum(self.element_wise_product, 1) / num_interactions\n        self.AFM = tf.nn.dropout(self.AFM, self.dropout_keep)\n\n        #\n        # self.summed_features_embedding = tf.reduce_sum(nonzero_embeddings, 1)\n        # self.squared_summed_features_embedding = tf.square(self.summed_features_embedding)\n        # self.squared_features_embedding = tf.square(nonzero_embeddings)\n        # self.summed_squared_features_embedding = tf.reduce_sum(self.squared_features_embedding, 1)\n        #\n        # self.FM = 0.5 * tf.subtract(self.summed_squared_features_embedding, self.squared_summed_features_embedding)\n        # # if batch_norm:\n        # #     self.FM = self\n        # layer_1 = tf.layers.dense(inputs=self.FM, units=num_hidden,\n        #                           bias_initializer=tf.random_normal_initializer,\n        #                           kernel_initializer=tf.random_normal_initializer, activation=tf.nn.relu,\n        #                           kernel_regularizer=tf.contrib.layers.l2_regularizer(scale=self.reg_rate))\n\n        self.prediction = tf.matmul(self.AFM, self.pred_weight)\n\n        bilinear = tf.reduce_sum(self.prediction, 1, keep_dims=True)\n        self.f_b = tf.reduce_sum(tf.nn.embedding_lookup(self.feature_bias, self.train_features), 1)\n        b = self.bias * tf.ones_like(self.y)\n        self.pred_rating = tf.add_n([bilinear, self.f_b, b])\n\n        self.loss = tf.nn.l2_loss(tf.subtract(self.y, self.pred_rating)) + \\\n            tf.contrib.layers.l2_regularizer(self.reg_rate)(self.feature_embeddings)\n        self.optimizer = tf.train.AdagradOptimizer(self.learning_rate).minimize(self.loss)\n\n    # def prepare_data(self, train_data, test_data):\n    #     print(""data preparation finished."")\n    #     return self\n\n    def train(self, train_data):\n        self.num_training = len(train_data[\'Y\'])\n        total_batch = int(self.num_training / self.batch_size)\n\n        rng_state = np.random.get_state()\n        np.random.shuffle(train_data[\'Y\'])\n        np.random.set_state(rng_state)\n        np.random.shuffle(train_data[\'X\'])\n        # train\n        for i in range(total_batch):\n            start_time = time.time()\n            batch_y = train_data[\'Y\'][i * self.batch_size:(i + 1) * self.batch_size]\n            batch_x = train_data[\'X\'][i * self.batch_size:(i + 1) * self.batch_size]\n\n            loss, opt = self.sess.run((self.loss, self.optimizer), feed_dict={self.train_features: batch_x,\n                                                                              self.y: batch_y,\n                                                                              self.dropout_keep: 0.5})\n            if i % self.display_step == 0:\n                print(""Index: %04d; cost= %.9f"" % (i + 1, np.mean(loss)))\n                if self.show_time:\n                    print(""one iteration: %s seconds."" % (time.time() - start_time))\n\n    def test(self, test_data):\n        # error = 0\n        # error_mae = 0\n        # test_set = list(test_data.keys())\n        # for (u, i) in test_set:\n        #     pred_rating_test = self.predict([u], [i])\n        #     error += (float(test_data.get((u, i))) - pred_rating_test) ** 2\n        #     error_mae += (np.abs(float(test_data.get((u, i))) - pred_rating_test))\n        num_example = len(test_data[\'Y\'])\n        feed_dict = {self.train_features: test_data[\'X\'], self.y: test_data[\'Y\'], self.dropout_keep: 1.0}\n        predictions = self.sess.run([self.pred_rating], feed_dict=feed_dict)\n        y_pred = np.reshape(predictions, (num_example,))\n        y_true = np.reshape(test_data[\'Y\'], (num_example,))\n        predictions_bounded = np.maximum(y_pred, np.ones(num_example) * min(y_true))  # bound the lower values\n        predictions_bounded = np.minimum(predictions_bounded,\n                                         np.ones(num_example) * max(y_true))  # bound the higher values\n        _RMSE = math.sqrt(mean_squared_error(y_true, predictions_bounded))\n\n        print(""RMSE:"" + str(_RMSE))\n\n    def execute(self, train_data, test_data):\n        init = tf.global_variables_initializer()\n        self.sess.run(init)\n\n        for epoch in range(self.epochs):\n            print(""Epoch: %04d;"" % epoch)\n            self.train(train_data)\n            if epoch % self.T == 0:\n                self.test(test_data)\n\n    def save(self, path):\n        saver = tf.train.Saver()\n        saver.save(self.sess, path)\n\n    def predict(self, user_id, item_id):\n        return self.sess.run([self.pred_rating], feed_dict={self.user_id: user_id, self.item_id: item_id})[0]\n'"
models/rating_prediction/autorec.py,30,"b'#!/usr/bin/env python\n""""""Implementation of Item based AutoRec and user based AutoRec.\nReference: Sedhain, Suvash, et al. ""Autorec: Autoencoders meet collaborative filtering."" Proceedings of the 24th International Conference on World Wide Web. ACM, 2015.\n""""""\n\nimport tensorflow as tf\nimport time\nimport numpy as np\nimport scipy\n\nfrom utils.evaluation.RatingMetrics import *\n\n__author__ = ""Shuai Zhang""\n__copyright__ = ""Copyright 2018, The DeepRec Project""\n\n__license__ = ""GPL""\n__version__ = ""1.0.0""\n__maintainer__ = ""Shuai Zhang""\n__email__ = ""cheungdaven@gmail.com""\n__status__ = ""Development""\n\n\nclass IAutoRec():\n    def __init__(self, sess, num_user, num_item, learning_rate=0.001, reg_rate=0.1, epoch=500, batch_size=500,\n                 verbose=False, T=3, display_step=1000):\n        self.learning_rate = learning_rate\n        self.epochs = epoch\n        self.batch_size = batch_size\n        self.reg_rate = reg_rate\n        self.sess = sess\n        self.num_user = num_user\n        self.num_item = num_item\n        self.verbose = verbose\n        self.T = T\n        self.display_step = display_step\n        print(""IAutoRec."")\n\n    def build_network(self, hidden_neuron=500):\n\n        self.rating_matrix = tf.placeholder(dtype=tf.float32, shape=[self.num_user, None])\n        self.rating_matrix_mask = tf.placeholder(dtype=tf.float32, shape=[self.num_user, None])\n        self.keep_rate_net = tf.placeholder(tf.float32)\n        self.keep_rate_input = tf.placeholder(tf.float32)\n\n        V = tf.Variable(tf.random_normal([hidden_neuron, self.num_user], stddev=0.01))\n        W = tf.Variable(tf.random_normal([self.num_user, hidden_neuron], stddev=0.01))\n\n        mu = tf.Variable(tf.random_normal([hidden_neuron], stddev=0.01))\n        b = tf.Variable(tf.random_normal([self.num_user], stddev=0.01))\n        layer_1 = tf.nn.dropout(tf.sigmoid(tf.expand_dims(mu, 1) + tf.matmul(V, self.rating_matrix)),\n                                self.keep_rate_net)\n        self.layer_2 = tf.matmul(W, layer_1) + tf.expand_dims(b, 1)\n        self.loss = tf.reduce_mean(tf.square(\n            tf.norm(tf.multiply((self.rating_matrix - self.layer_2), self.rating_matrix_mask)))) + self.reg_rate * (\n        tf.square(tf.norm(W)) + tf.square(tf.norm(V)))\n        self.optimizer = tf.train.AdamOptimizer(learning_rate=self.learning_rate).minimize(self.loss)\n\n    def train(self, train_data):\n        self.num_training = self.num_item\n        total_batch = int(self.num_training / self.batch_size)\n        idxs = np.random.permutation(self.num_training)  # shuffled ordering\n\n        for i in range(total_batch):\n            start_time = time.time()\n            if i == total_batch - 1:\n                batch_set_idx = idxs[i * self.batch_size:]\n            elif i < total_batch - 1:\n                batch_set_idx = idxs[i * self.batch_size: (i + 1) * self.batch_size]\n\n            _, loss = self.sess.run([self.optimizer, self.loss],\n                                    feed_dict={self.rating_matrix: self.train_data[:, batch_set_idx],\n                                               self.rating_matrix_mask: self.train_data_mask[:, batch_set_idx],\n                                               self.keep_rate_net: 0.95\n                                               })\n            if i % self.display_step == 0:\n                if self.verbose:\n                    print(""Index: %04d; cost= %.9f"" % (i + 1, np.mean(loss)))\n                    print(""one iteration: %s seconds."" % (time.time() - start_time))\n\n    def test(self, test_data):\n        self.reconstruction = self.sess.run(self.layer_2, feed_dict={self.rating_matrix: self.train_data,\n                                                                     self.rating_matrix_mask: self.train_data_mask,\n                                                                     self.keep_rate_net: 1})\n        error = 0\n        error_mae = 0\n        test_set = list(test_data.keys())\n        for (u, i) in test_set:\n            pred_rating_test = self.predict(u, i)\n            error += (float(test_data.get((u, i))) - pred_rating_test) ** 2\n            error_mae += (np.abs(float(test_data.get((u, i))) - pred_rating_test))\n        print(""RMSE:"" + str(RMSE(error, len(test_set))) + ""; MAE:"" + str(MAE(error_mae, len(test_set))))\n\n    def execute(self, train_data, test_data):\n        self.train_data = self._data_process(train_data)\n        self.train_data_mask = scipy.sign(self.train_data)\n        init = tf.global_variables_initializer()\n        self.sess.run(init)\n        for epoch in range(self.epochs):\n            if self.verbose:\n                print(""Epoch: %04d;"" % (epoch))\n            self.train(train_data)\n            if (epoch) % self.T == 0:\n                print(""Epoch: %04d; "" % (epoch), end=\'\')\n                self.test(test_data)\n\n    def save(self, path):\n        saver = tf.train.Saver()\n        saver.save(self.sess, path)\n\n    def predict(self, user_id, item_id):\n        return self.reconstruction[user_id, item_id]\n\n    def _data_process(self, data):\n        output = np.zeros((self.num_user, self.num_item))\n        for u in range(self.num_user):\n            for i in range(self.num_item):\n                output[u, i] = data.get((u, i))\n        return output\n\n\nclass UAutoRec():\n    def __init__(self, sess, num_user, num_item, learning_rate=0.001, reg_rate=0.1, epoch=500, batch_size=200,\n                 verbose=False, T=3, display_step=1000):\n        self.learning_rate = learning_rate\n        self.epochs = epoch\n        self.batch_size = batch_size\n        self.reg_rate = reg_rate\n        self.sess = sess\n        self.num_user = num_user\n        self.num_item = num_item\n        self.verbose = verbose\n        self.T = T\n        self.display_step = display_step\n        print(""UAutoRec."")\n\n    def build_network(self, hidden_neuron=500):\n\n        self.rating_matrix = tf.placeholder(dtype=tf.float32, shape=[self.num_item, None])\n        self.rating_matrix_mask = tf.placeholder(dtype=tf.float32, shape=[self.num_item, None])\n\n        V = tf.Variable(tf.random_normal([hidden_neuron, self.num_item], stddev=0.01))\n        W = tf.Variable(tf.random_normal([self.num_item, hidden_neuron], stddev=0.01))\n\n        mu = tf.Variable(tf.random_normal([hidden_neuron], stddev=0.01))\n        b = tf.Variable(tf.random_normal([self.num_item], stddev=0.01))\n        layer_1 = tf.sigmoid(tf.expand_dims(mu, 1) + tf.matmul(V, self.rating_matrix))\n        self.layer_2 = tf.matmul(W, layer_1) + tf.expand_dims(b, 1)\n        self.loss = tf.reduce_mean(tf.square(\n            tf.norm(tf.multiply((self.rating_matrix - self.layer_2), self.rating_matrix_mask)))) + self.reg_rate * (\n        tf.square(tf.norm(W)) + tf.square(tf.norm(V)))\n        self.optimizer = tf.train.AdamOptimizer(learning_rate=self.learning_rate).minimize(self.loss)\n\n    def train(self, train_data):\n        self.num_training = self.num_user\n        total_batch = int(self.num_training / self.batch_size)\n        idxs = np.random.permutation(self.num_training)  # shuffled ordering\n\n        for i in range(total_batch):\n            start_time = time.time()\n            if i == total_batch - 1:\n                batch_set_idx = idxs[i * self.batch_size:]\n            elif i < total_batch - 1:\n                batch_set_idx = idxs[i * self.batch_size: (i + 1) * self.batch_size]\n\n            _, loss = self.sess.run([self.optimizer, self.loss],\n                                    feed_dict={self.rating_matrix: self.train_data[:, batch_set_idx],\n                                               self.rating_matrix_mask: self.train_data_mask[:, batch_set_idx]\n                                               })\n            if self.verbose and i % self.display_step == 0:\n                print(""Index: %04d; cost= %.9f"" % (i + 1, np.mean(loss)))\n                if self.verbose:\n                    print(""one iteration: %s seconds."" % (time.time() - start_time))\n\n    def test(self, test_data):\n        self.reconstruction = self.sess.run(self.layer_2, feed_dict={self.rating_matrix: self.train_data,\n                                                                     self.rating_matrix_mask:\n                                                                         self.train_data_mask})\n        error = 0\n        error_mae = 0\n        test_set = list(test_data.keys())\n        for (u, i) in test_set:\n            pred_rating_test = self.predict(u, i)\n            error += (float(test_data.get((u, i))) - pred_rating_test) ** 2\n            error_mae += (np.abs(float(test_data.get((u, i))) - pred_rating_test))\n        print(""RMSE:"" + str(RMSE(error, len(test_set))) + ""; MAE:"" + str(MAE(error_mae, len(test_set))))\n\n    def execute(self, train_data, test_data):\n        self.train_data = self._data_process(train_data.transpose())\n        self.train_data_mask = scipy.sign(self.train_data)\n        init = tf.global_variables_initializer()\n        self.sess.run(init)\n        for epoch in range(self.epochs):\n            self.train(train_data)\n            if (epoch) % self.T == 0:\n                print(""Epoch: %04d; "" % (epoch), end=\'\')\n                self.test(test_data)\n\n    def save(self, path):\n        saver = tf.train.Saver()\n        saver.save(self.sess, path)\n\n    def predict(self, user_id, item_id):\n        return self.reconstruction[item_id, user_id]\n\n    def _data_process(self, data):\n        output = np.zeros((self.num_item, self.num_user))\n        for u in range(self.num_user):\n            for i in range(self.num_item):\n                output[i, u] = data.get((i, u))\n        return output\n'"
models/rating_prediction/deepfm.py,33,"b'#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n\n""""""\nImplementation of DeepFM with tensorflow.\n\nReference:\n[1] DeepFM: A Factorization-Machine based Neural Network for CTR Prediction,\n    Huifeng Guo\x03, Ruiming Tang, Yunming Yey, Zhenguo Li, Xiuqiang He.\n""""""\n\nimport math\nimport time\nimport tensorflow as tf\nfrom sklearn.metrics import mean_squared_error\nfrom utils.evaluation.RatingMetrics import *\n\n__author__ = ""Buracag Yang""\n__copyright__ = ""Copyright 2018, The DeepRec Project""\n\n__license__ = ""GPL""\n__version__ = ""1.0.0""\n__maintainer__ = ""Buracag Yang""\n__email__ = ""15591875898@163.com""\n__status__ = ""Development""\n\n\nclass DeepFM(object):\n    def __init__(self, sess, num_user, num_item, **kwargs):\n        self.sess = sess\n        self.num_user = num_user\n        self.num_item = num_item\n        self.epochs = kwargs[\'epochs\']\n        self.batch_size = kwargs[\'batch_size\']\n        self.learning_rate = kwargs[\'learning_rate\']\n        self.reg_rate = kwargs[\'reg_rate\']\n        self.num_factors = kwargs[\'num_factors\']\n        self.display_step = kwargs[\'display_step\']\n        self.show_time = kwargs[\'show_time\']\n        self.T = kwargs[\'T\']\n        self.layers = kwargs[\'layers\']\n        self.field_size = kwargs[\'field_size\']\n\n        self.train_features = None\n        self.y = None\n        self.dropout_keep = None\n        self.first_oder_weight = None\n        self.feature_embeddings = None\n        self.feature_bias = None\n        self.bias = None\n        self.pred_rating = None\n        self.pred = None\n        self.loss = None\n        self.optimizer = None\n        self.num_training = None\n        print(""You are running DeepFM."")\n\n    def build_network(self, feature_size):\n        self.train_features = tf.placeholder(tf.int32, shape=[None, None])\n        self.y = tf.placeholder(tf.float32, shape=[None, 1])\n        self.dropout_keep = tf.placeholder(tf.float32)\n        self.first_oder_weight = tf.Variable(tf.random_normal([feature_size], mean=0.0, stddev=0.01))\n        self.feature_embeddings = tf.Variable(tf.random_normal([feature_size, self.num_factors], mean=0.0, stddev=0.01))\n        self.feature_bias = tf.Variable(tf.random_uniform([feature_size, 1], 0.0, 0.0))\n        self.bias = tf.Variable(tf.constant(0.0))\n\n        # f(x)\n        with tf.variable_scope(""First-order""):\n            y1 = tf.reduce_sum(tf.nn.embedding_lookup(self.first_oder_weight, self.train_features), 1, keepdims=True)\n\n        with tf.variable_scope(""Second-order""):\n            nonzero_embeddings = tf.nn.embedding_lookup(self.feature_embeddings, self.train_features)\n            sum_square = tf.square(tf.reduce_sum(nonzero_embeddings, 1))\n            square_sum = tf.reduce_sum(tf.square(nonzero_embeddings), 1)\n            y_fm = 0.5 * tf.reduce_sum(tf.subtract(sum_square, square_sum), 1, keepdims=True)\n            y_fm = tf.nn.dropout(y_fm, self.dropout_keep)\n\n        with tf.variable_scope(""Deep_part""):\n            deep_inputs = tf.reshape(nonzero_embeddings, shape=[-1, self.field_size*self.num_factors])  # None * (F*K)\n            for i in range(len(self.layers)):\n                deep_inputs = tf.contrib.layers.fully_connected(\n                    inputs=deep_inputs, num_outputs=self.layers[i],\n                    weights_regularizer=tf.contrib.layers.l2_regularizer(self.reg_rate), scope=\'mlp%d\' % i)\n                # TODO: dropout\n\n            y_deep = tf.contrib.layers.fully_connected(\n                inputs=deep_inputs, num_outputs=1, activation_fn=tf.nn.relu,\n                weights_regularizer=tf.contrib.layers.l2_regularizer(self.reg_rate),\n                scope=\'deep_out\')\n            y_d = tf.reshape(y_deep, shape=[-1, 1])\n\n        with tf.variable_scope(""DeepFM-out""):\n            f_b = tf.reduce_sum(tf.nn.embedding_lookup(self.feature_bias, self.train_features), 1)\n            b = self.bias * tf.ones_like(self.y)\n            self.pred_rating = tf.add_n([y1, y_fm, y_d, f_b, b])\n            self.pred = tf.sigmoid(self.pred_rating)\n\n        self.loss = tf.nn.l2_loss(tf.subtract(self.y, self.pred_rating)) + \\\n            tf.contrib.layers.l2_regularizer(self.reg_rate)(self.feature_embeddings)\n\n        self.optimizer = tf.train.AdagradOptimizer(self.learning_rate).minimize(self.loss)\n\n    def train(self, train_data):\n        self.num_training = len(train_data[\'Y\'])\n        total_batch = int(self.num_training / self.batch_size)\n        rng_state = np.random.get_state()\n        np.random.shuffle(train_data[\'Y\'])\n        np.random.set_state(rng_state)\n        np.random.shuffle(train_data[\'X\'])\n\n        # train\n        for i in range(total_batch):\n            start_time = time.time()\n            batch_y = train_data[\'Y\'][i * self.batch_size:(i + 1) * self.batch_size]\n            batch_x = train_data[\'X\'][i * self.batch_size:(i + 1) * self.batch_size]\n            loss, _ = self.sess.run((self.loss, self.optimizer),\n                                    feed_dict={self.train_features: batch_x,\n                                               self.y: batch_y,\n                                               self.dropout_keep: 0.5})\n            if i % self.display_step == 0:\n                print(""Index: %04d; cost= %.9f"" % (i + 1, np.mean(loss)))\n                if self.show_time:\n                    print(""one iteration: %s seconds."" % (time.time() - start_time))\n\n    def test(self, test_data):\n        num_example = len(test_data[\'Y\'])\n        feed_dict = {self.train_features: test_data[\'X\'], self.y: test_data[\'Y\'], self.dropout_keep: 1.0}\n        predictions = self.sess.run(self.pred_rating, feed_dict=feed_dict)\n        y_pred = np.reshape(predictions, (num_example,))\n        y_true = np.reshape(test_data[\'Y\'], (num_example,))\n        predictions_bounded = np.maximum(y_pred, np.ones(num_example) * min(y_true))  # bound the lower values\n        predictions_bounded = np.minimum(predictions_bounded, np.ones(num_example) * max(y_true))\n        _RMSE = math.sqrt(mean_squared_error(y_true, predictions_bounded))\n        print(""RMSE:"" + str(_RMSE))\n\n    def execute(self, train_data, test_data):\n        init = tf.global_variables_initializer()\n        self.sess.run(init)\n\n        for epoch in range(self.epochs):\n            print(""Epoch: %04d;"" % epoch)\n            self.train(train_data)\n            if epoch % self.T == 0:\n                self.test(test_data)\n\n    def save(self, path):\n        saver = tf.train.Saver()\n        saver.save(self.sess, path)\n\n    # def predict(self, user_id, item_id):\n    #     return self.sess.run([self.pred_rating], feed_dict={self.user_id: user_id, self.item_id: item_id})[0]\n'"
models/rating_prediction/fm.py,22,"b'#!/usr/bin/env python\n""""""Implementation of Matrix Factorization with tensorflow.\nReference: Koren, Yehuda, Robert Bell, and Chris Volinsky. ""Matrix factorization techniques for recommender systems."" Computer 42.8 (2009).\nOrginal Implementation:\n""""""\n\nimport tensorflow as tf\nimport time\nfrom sklearn.metrics import mean_squared_error\nimport math\n\nfrom utils.evaluation.RatingMetrics import *\n\n__author__ = ""Shuai Zhang""\n__copyright__ = ""Copyright 2018, The DeepRec Project""\n\n__license__ = ""GPL""\n__version__ = ""1.0.0""\n__maintainer__ = ""Shuai Zhang""\n__email__ = ""cheungdaven@gmail.com""\n__status__ = ""Development""\n\nclass FM():\n\n    def __init__(self, sess, num_user, num_item, learning_rate = 0.05, reg_rate = 0.01, epoch = 500, batch_size = 128, show_time = False, T =2, display_step= 1000):\n        self.learning_rate = learning_rate\n        self.epochs = epoch\n        self.batch_size = batch_size\n        self.reg_rate = reg_rate\n        self.sess = sess\n        self.num_user = num_user\n        self.num_item = num_item\n        self.show_time = show_time\n        self.T = T\n        self.display_step = display_step\n        print(""FM."")\n\n\n    def build_network(self, feature_M, num_factor = 64):\n\n\n        # model dependent arguments\n        self.train_features = tf.placeholder(tf.int32, shape=[None, None])\n        self.y = tf.placeholder(tf.float32, shape=[None, 1])\n        self.dropout_keep = tf.placeholder(tf.float32)\n\n        self.feature_embeddings = tf.Variable(tf.random_normal([feature_M, num_factor], mean=0.0, stddev=0.01))\n\n        self.feature_bias = tf.Variable(tf.random_uniform([feature_M, 1], 0.0, 0.0))\n        self.bias = tf.Variable(tf.constant(0.0))\n\n\n        nonzero_embeddings = tf.nn.embedding_lookup(self.feature_embeddings, self.train_features)\n\n        self.summed_features_embedding = tf.reduce_sum(nonzero_embeddings, 1)\n        self.squared_summed_features_embedding = tf.square(self.summed_features_embedding)\n        self.squared_features_embedding = tf.square(nonzero_embeddings)\n        self.summed_squared_features_embedding = tf.reduce_sum(self.squared_features_embedding, 1)\n\n        self.FM = 0.5 * tf.subtract(self.summed_squared_features_embedding, self.squared_summed_features_embedding)\n        # if batch_norm:\n        #     self.FM = self\n        self.FM = tf.nn.dropout(self.FM, self.dropout_keep)\n\n        bilinear = tf.reduce_sum(self.FM, 1, keep_dims=True)\n        self.f_b = tf.reduce_sum(tf.nn.embedding_lookup(self.feature_bias, self.train_features), 1)\n        b = self.bias * tf.ones_like(self.y)\n        self.pred_rating = tf.add_n([bilinear, self.f_b, b])\n\n        self.loss = tf.nn.l2_loss(tf.subtract(self.y, self.pred_rating)) \\\n                    + tf.contrib.layers.l2_regularizer(self.reg_rate)(self.feature_embeddings)\n\n\n        self.optimizer = tf.train.AdagradOptimizer(self.learning_rate).minimize(self.loss)\n\n    def prepare_data(self, train_data, test_data):\n\n\n        print(""data preparation finished."")\n        return self\n\n\n    def train(self, train_data):\n        self.num_training = len(train_data[\'Y\'])\n        total_batch = int( self.num_training/ self.batch_size)\n\n        rng_state = np.random.get_state()\n        np.random.shuffle(train_data[\'Y\'])\n        np.random.set_state(rng_state)\n        np.random.shuffle(train_data[\'X\'])\n        # train\n        for i in range(total_batch):\n            start_time = time.time()\n            batch_y = train_data[\'Y\'][i * self.batch_size:(i + 1) * self.batch_size]\n            batch_x = train_data[\'X\'][i * self.batch_size:(i + 1) * self.batch_size]\n            # print(batch_x)\n            loss, opt = self.sess.run((self.loss, self.optimizer), feed_dict={self.train_features: batch_x,\n                                                                              self.y: batch_y,\n                                                                              self.dropout_keep:0.5})\n            if i % self.display_step == 0:\n                print(""Index: %04d; cost= %.9f"" % (i + 1, np.mean(loss)))\n                if self.show_time:\n                    print(""one iteration: %s seconds."" % (time.time() - start_time))\n\n    def test(self, test_data):\n        num_example = len(test_data[\'Y\'])\n        feed_dict = {self.train_features: test_data[\'X\'], self.y: test_data[\'Y\'],self.dropout_keep: 1.0}\n        predictions = self.sess.run((self.pred_rating), feed_dict=feed_dict)\n        y_pred = np.reshape(predictions, (num_example,))\n        y_true = np.reshape(test_data[\'Y\'], (num_example,))\n        predictions_bounded = np.maximum(y_pred, np.ones(num_example) * min(y_true))  # bound the lower values\n        predictions_bounded = np.minimum(predictions_bounded,\n                                         np.ones(num_example) * max(y_true))  # bound the higher values\n        RMSE = math.sqrt(mean_squared_error(y_true, predictions_bounded))\n\n        print(""RMSE:"" + str(RMSE))\n\n    def execute(self, train_data, test_data):\n\n        init = tf.global_variables_initializer()\n        self.sess.run(init)\n\n        for epoch in range(self.epochs):\n            print(""Epoch: %04d;"" % (epoch))\n            self.train(train_data)\n            if (epoch) % self.T == 0:\n                self.test(test_data)\n\n    def save(self, path):\n        saver = tf.train.Saver()\n        saver.save(self.sess, path)\n\n    def predict(self, user_id, item_id):\n        return self.sess.run([self.pred_rating], feed_dict={self.user_id: user_id, self.item_id: item_id})[0]\n\n'"
models/rating_prediction/fml.py,0,b''
models/rating_prediction/mf.py,20,"b'#!/usr/bin/env python\n""""""Implementation of Matrix Factorization with tensorflow.\nReference: Koren, Yehuda, Robert Bell, and Chris Volinsky. ""Matrix factorization techniques for recommender systems."" Computer 42.8 (2009).\n""""""\n\nimport tensorflow as tf\nimport time\nimport numpy as np\n\nfrom utils.evaluation.RatingMetrics import *\n\n__author__ = ""Shuai Zhang""\n__copyright__ = ""Copyright 2018, The DeepRec Project""\n\n__license__ = ""GPL""\n__version__ = ""1.0.0""\n__maintainer__ = ""Shuai Zhang""\n__email__ = ""cheungdaven@gmail.com""\n__status__ = ""Development""\n\n\nclass MF():\n    def __init__(self, sess, num_user, num_item, learning_rate=0.001, reg_rate=0.01, epoch=500, batch_size=128,\n                 show_time=False, T=2, display_step=1000):\n        self.learning_rate = learning_rate\n        self.epochs = epoch\n        self.batch_size = batch_size\n        self.reg_rate = reg_rate\n        self.sess = sess\n        self.num_user = num_user\n        self.num_item = num_item\n        self.show_time = show_time\n        self.T = T\n        self.display_step = display_step\n        print(""MF."")\n\n    def build_network(self, num_factor=30):\n\n        # model dependent arguments\n        self.user_id = tf.placeholder(dtype=tf.int32, shape=[None], name=\'user_id\')\n        self.item_id = tf.placeholder(dtype=tf.int32, shape=[None], name=\'item_id\')\n        self.y = tf.placeholder(""float"", [None], \'rating\')\n\n        self.P = tf.Variable(tf.random_normal([self.num_user, num_factor], stddev=0.01))\n        self.Q = tf.Variable(tf.random_normal([self.num_item, num_factor], stddev=0.01))\n\n        self.B_U = tf.Variable(tf.random_normal([self.num_user], stddev=0.01))\n        self.B_I = tf.Variable(tf.random_normal([self.num_item], stddev=0.01))\n\n        user_latent_factor = tf.nn.embedding_lookup(self.P, self.user_id)\n        item_latent_factor = tf.nn.embedding_lookup(self.Q, self.item_id)\n        user_bias = tf.nn.embedding_lookup(self.B_U, self.user_id)\n        item_bias = tf.nn.embedding_lookup(self.B_I, self.item_id)\n\n        self.pred_rating = tf.reduce_sum(tf.multiply(user_latent_factor, item_latent_factor), 1) + user_bias + item_bias\n\n    def train(self, train_data):\n        self.num_training = len(self.rating)\n        total_batch = int(self.num_training / self.batch_size)\n        idxs = np.random.permutation(self.num_training)  # shuffled ordering\n        user_random = list(self.user[idxs])\n        item_random = list(self.item[idxs])\n        rating_random = list(self.rating[idxs])\n        # train\n        for i in range(total_batch):\n            start_time = time.time()\n            batch_user = user_random[i * self.batch_size:(i + 1) * self.batch_size]\n            batch_item = item_random[i * self.batch_size:(i + 1) * self.batch_size]\n            batch_rating = rating_random[i * self.batch_size:(i + 1) * self.batch_size]\n\n            _, loss = self.sess.run([self.optimizer, self.loss], feed_dict={self.user_id: batch_user,\n                                                                            self.item_id: batch_item,\n                                                                            self.y: batch_rating\n                                                                            })\n            if i % self.display_step == 0:\n                print(""Index: %04d; cost= %.9f"" % (i + 1, np.mean(loss)))\n                if self.show_time:\n                    print(""one iteration: %s seconds."" % (time.time() - start_time))\n\n    def test(self, test_data):\n        error = 0\n        error_mae = 0\n        test_set = list(test_data.keys())\n        for (u, i) in test_set:\n            pred_rating_test = self.predict([u], [i])\n            error += (float(test_data.get((u, i))) - pred_rating_test) ** 2\n            error_mae += (np.abs(float(test_data.get((u, i))) - pred_rating_test))\n        print(""RMSE:"" + str(RMSE(error, len(test_set))[0]) + ""; MAE:"" + str(MAE(error_mae, len(test_set))[0]))\n\n    def execute(self, train_data, test_data):\n\n        t = train_data.tocoo()\n        self.user = t.row.reshape(-1)\n        self.item = t.col.reshape(-1)\n        self.rating = t.data\n        self.pred_rating += np.mean(list(self.rating))\n        self.loss = tf.reduce_sum(tf.square(self.y - self.pred_rating)) \\\n                    + self.reg_rate * (\n        tf.nn.l2_loss(self.B_I) + tf.nn.l2_loss(self.B_U) + tf.nn.l2_loss(self.P) + tf.nn.l2_loss(self.Q))\n        # tf.norm(self.B_I) +  tf.norm(self.B_U) + tf.norm(self.P) +  tf.norm(self.Q))\n        # tf.reduce_sum(tf.square(P))\n        # tf.reduce_sum(tf.multiply(P,P))\n        self.optimizer = tf.train.AdamOptimizer(learning_rate=self.learning_rate).minimize(self.loss)\n        init = tf.global_variables_initializer()\n        self.sess.run(init)\n\n        for epoch in range(self.epochs):\n            print(""Epoch: %04d;"" % (epoch))\n            self.train(train_data)\n            if (epoch) % self.T == 0:\n                self.test(test_data)\n\n    def save(self, path):\n        saver = tf.train.Saver()\n        saver.save(self.sess, path)\n\n    def predict(self, user_id, item_id):\n        return self.sess.run([self.pred_rating], feed_dict={self.user_id: user_id, self.item_id: item_id})[0]\n'"
models/rating_prediction/nfm.py,27,"b'#!/usr/bin/env python\n""""""Implementation of Matrix Factorization with tensorflow.\nReference: Koren, Yehuda, Robert Bell, and Chris Volinsky. ""Matrix factorization techniques for recommender systems."" Computer 42.8 (2009).\nOrginal Implementation:\n""""""\n\nimport tensorflow as tf\nimport time\nfrom sklearn.metrics import mean_squared_error\nimport math\n\nfrom utils.evaluation.RatingMetrics import *\n\n__author__ = ""Shuai Zhang""\n__copyright__ = ""Copyright 2018, The DeepRec Project""\n\n__license__ = ""GPL""\n__version__ = ""1.0.0""\n__maintainer__ = ""Shuai Zhang""\n__email__ = ""cheungdaven@gmail.com""\n__status__ = ""Development""\n\nclass NFM():\n\n    def __init__(self, sess, num_user, num_item, learning_rate = 0.05, reg_rate = 0.01, epoch = 500, batch_size = 128, show_time = False, T =2, display_step= 1000):\n        self.learning_rate = learning_rate\n        self.epochs = epoch\n        self.batch_size = batch_size\n        self.reg_rate = reg_rate\n        self.sess = sess\n        self.num_user = num_user\n        self.num_item = num_item\n        self.show_time = show_time\n        self.T = T\n        self.display_step = display_step\n        print(""NFM."")\n\n\n    def build_network(self, feature_M, num_factor = 128, num_hidden = 128):\n\n\n        # model dependent arguments\n        self.train_features = tf.placeholder(tf.int32, shape=[None, None])\n        self.y = tf.placeholder(tf.float32, shape=[None, 1])\n        self.dropout_keep = tf.placeholder(tf.float32)\n\n        self.feature_embeddings = tf.Variable(tf.random_normal([feature_M, num_factor], mean=0.0, stddev=0.01))\n\n        self.feature_bias = tf.Variable(tf.random_uniform([feature_M, 1], 0.0, 0.0))\n        self.bias = tf.Variable(tf.constant(0.0))\n        self.pred_weight = tf.Variable(np.random.normal(loc=0, scale= np.sqrt(2.0 / (num_factor + num_hidden)), size=(num_hidden, 1)),\n                                                dtype=np.float32)\n\n        nonzero_embeddings = tf.nn.embedding_lookup(self.feature_embeddings, self.train_features)\n\n        self.summed_features_embedding = tf.reduce_sum(nonzero_embeddings, 1)\n        self.squared_summed_features_embedding = tf.square(self.summed_features_embedding)\n        self.squared_features_embedding = tf.square(nonzero_embeddings)\n        self.summed_squared_features_embedding = tf.reduce_sum(self.squared_features_embedding, 1)\n\n        self.FM = 0.5 * tf.subtract( self.squared_summed_features_embedding, self.summed_squared_features_embedding)\n        # if batch_norm:\n        #     self.FM = self\n        layer_1 = tf.layers.dense(inputs=self.FM, units=num_hidden,\n                                  bias_initializer=tf.random_normal_initializer,\n                                  kernel_initializer=tf.random_normal_initializer, activation=tf.nn.relu,\n                                  kernel_regularizer=tf.contrib.layers.l2_regularizer(scale=self.reg_rate))\n\n        self.FM =  tf.matmul(tf.nn.dropout(layer_1, 0.8), self.pred_weight)\n\n\n\n        bilinear = tf.reduce_sum(self.FM, 1, keep_dims=True)\n        self.f_b = tf.reduce_sum(tf.nn.embedding_lookup(self.feature_bias, self.train_features), 1)\n        b = self.bias * tf.ones_like(self.y)\n        self.pred_rating = tf.add_n([bilinear, self.f_b, b])\n\n        self.loss = tf.nn.l2_loss(tf.subtract(self.y, self.pred_rating)) \\\n                    + tf.contrib.layers.l2_regularizer(self.reg_rate)(self.feature_embeddings)\n\n\n        self.optimizer = tf.train.AdagradOptimizer(self.learning_rate).minimize(self.loss)\n\n    def prepare_data(self, train_data, test_data):\n\n\n        print(""data preparation finished."")\n        return self\n\n\n    def train(self, train_data):\n        self.num_training = len(train_data[\'Y\'])\n        total_batch = int( self.num_training/ self.batch_size)\n\n        rng_state = np.random.get_state()\n        np.random.shuffle(train_data[\'Y\'])\n        np.random.set_state(rng_state)\n        np.random.shuffle(train_data[\'X\'])\n        # train\n        for i in range(total_batch):\n            start_time = time.time()\n            batch_y = train_data[\'Y\'][i * self.batch_size:(i + 1) * self.batch_size]\n            batch_x = train_data[\'X\'][i * self.batch_size:(i + 1) * self.batch_size]\n\n            loss, opt = self.sess.run((self.loss, self.optimizer), feed_dict={self.train_features: batch_x,\n                                                                              self.y: batch_y,\n                                                                              self.dropout_keep:0.5})\n            if i % self.display_step == 0:\n                print(""Index: %04d; cost= %.9f"" % (i + 1, np.mean(loss)))\n                if self.show_time:\n                    print(""one iteration: %s seconds."" % (time.time() - start_time))\n\n    def test(self, test_data):\n        # error = 0\n        # error_mae = 0\n        # test_set = list(test_data.keys())\n        # for (u, i) in test_set:\n        #     pred_rating_test = self.predict([u], [i])\n        #     error += (float(test_data.get((u, i))) - pred_rating_test) ** 2\n        #     error_mae += (np.abs(float(test_data.get((u, i))) - pred_rating_test))\n        num_example = len(test_data[\'Y\'])\n        feed_dict = {self.train_features: test_data[\'X\'], self.y: test_data[\'Y\'],self.dropout_keep: 1.0}\n        predictions = self.sess.run((self.pred_rating), feed_dict=feed_dict)\n        y_pred = np.reshape(predictions, (num_example,))\n        y_true = np.reshape(test_data[\'Y\'], (num_example,))\n        predictions_bounded = np.maximum(y_pred, np.ones(num_example) * min(y_true))  # bound the lower values\n        predictions_bounded = np.minimum(predictions_bounded,\n                                         np.ones(num_example) * max(y_true))  # bound the higher values\n        RMSE = math.sqrt(mean_squared_error(y_true, predictions_bounded))\n\n        print(""RMSE:"" + str(RMSE))\n\n    def execute(self, train_data, test_data):\n\n        init = tf.global_variables_initializer()\n        self.sess.run(init)\n\n        for epoch in range(self.epochs):\n            print(""Epoch: %04d;"" % (epoch))\n            self.train(train_data)\n            if (epoch) % self.T == 0 and epoch > 100:\n                self.test(test_data)\n\n    def save(self, path):\n        saver = tf.train.Saver()\n        saver.save(self.sess, path)\n\n    def predict(self, user_id, item_id):\n        return self.sess.run([self.pred_rating], feed_dict={self.user_id: user_id, self.item_id: item_id})[0]\n\n'"
models/rating_prediction/nnmf.py,39,"b'#!/usr/bin/env python\n""""""Implementation of Neural Network Matrix Factorization.\nReference: Dziugaite, Gintare Karolina, and Daniel M. Roy. ""Neural network matrix factorization."" arXiv preprint arXiv:1511.06443 (2015).\n""""""\n\nimport tensorflow as tf\nimport time\nimport numpy as np\n\nfrom utils.evaluation.RatingMetrics import *\n\n__author__ = ""Shuai Zhang""\n__copyright__ = ""Copyright 2018, The DeepRec Project""\n\n__license__ = ""GPL""\n__version__ = ""1.0.0""\n__maintainer__ = ""Shuai Zhang""\n__email__ = ""cheungdaven@gmail.com""\n__status__ = ""Development""\n\n\nclass NNMF():\n    def __init__(self, sess, num_user, num_item, learning_rate=0.001, reg_rate=0.01, epoch=500, batch_size=256,\n                 show_time=False, T=1, display_step=1000):\n        self.learning_rate = learning_rate\n        self.epochs = epoch\n        self.batch_size = batch_size\n        self.reg_rate = reg_rate\n        self.sess = sess\n        self.num_user = num_user\n        self.num_item = num_item\n        self.show_time = show_time\n        self.T = T\n        self.display_step = display_step\n        print(""NNMF."")\n\n    def build_network(self, num_factor_1=100, num_factor_2=10, hidden_dimension=50):\n        print(""num_factor_1=%d, num_factor_2=%d, hidden_dimension=%d"" % (num_factor_1, num_factor_2, hidden_dimension))\n\n        # model dependent arguments\n        self.user_id = tf.placeholder(dtype=tf.int32, shape=[None], name=\'user_id\')\n        self.item_id = tf.placeholder(dtype=tf.int32, shape=[None], name=\'item_id\')\n        self.y = tf.placeholder(""float"", [None], \'rating\')\n\n        P = tf.Variable(tf.random_normal([self.num_user, num_factor_1], stddev=0.01))\n        Q = tf.Variable(tf.random_normal([self.num_item, num_factor_1], stddev=0.01))\n\n        U = tf.Variable(tf.random_normal([self.num_user, num_factor_2], stddev=0.01))\n        V = tf.Variable(tf.random_normal([self.num_item, num_factor_2], stddev=0.01))\n\n        input = tf.concat(values=[tf.nn.embedding_lookup(P, self.user_id),\n                                  tf.nn.embedding_lookup(Q, self.item_id),\n                                  tf.multiply(tf.nn.embedding_lookup(U, self.user_id),\n                                              tf.nn.embedding_lookup(V, self.item_id))\n                                  ], axis=1)\n\n        layer_1 = tf.layers.dense(inputs=input, units=2 * num_factor_1 + num_factor_2,\n                                  bias_initializer=tf.random_normal_initializer,\n                                  kernel_initializer=tf.random_normal_initializer, activation=tf.sigmoid,\n                                  kernel_regularizer=tf.contrib.layers.l2_regularizer(scale=self.reg_rate))\n        layer_2 = tf.layers.dense(inputs=layer_1, units=hidden_dimension, activation=tf.sigmoid,\n                                  bias_initializer=tf.random_normal_initializer,\n                                  kernel_initializer=tf.random_normal_initializer,\n                                  kernel_regularizer=tf.contrib.layers.l2_regularizer(scale=self.reg_rate))\n        layer_3 = tf.layers.dense(inputs=layer_2, units=hidden_dimension, activation=tf.sigmoid,\n                                  bias_initializer=tf.random_normal_initializer,\n                                  kernel_initializer=tf.random_normal_initializer,\n                                  kernel_regularizer=tf.contrib.layers.l2_regularizer(scale=self.reg_rate))\n        layer_4 = tf.layers.dense(inputs=layer_3, units=hidden_dimension, activation=tf.sigmoid,\n                                  bias_initializer=tf.random_normal_initializer,\n                                  kernel_initializer=tf.random_normal_initializer,\n                                  kernel_regularizer=tf.contrib.layers.l2_regularizer(scale=self.reg_rate))\n        output = tf.layers.dense(inputs=layer_4, units=1, activation=None,\n                                 bias_initializer=tf.random_normal_initializer,\n                                 kernel_initializer=tf.random_normal_initializer,\n                                 kernel_regularizer=tf.contrib.layers.l2_regularizer(scale=self.reg_rate))\n        self.pred_rating = tf.reshape(output, [-1])\n\n        # print(np.shape(output))\n        # reg_losses = tf.reduce_sum(tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES))\n        self.loss = tf.reduce_sum(tf.square(self.y - self.pred_rating)) \\\n                    + tf.losses.get_regularization_loss() + self.reg_rate * (\n        tf.norm(U) + tf.norm(V) + tf.norm(P) + tf.norm(Q))\n        self.optimizer = tf.train.RMSPropOptimizer(learning_rate=self.learning_rate).minimize(self.loss)\n\n    def train(self, train_data):\n        self.num_training = len(self.rating)\n        total_batch = int(self.num_training / self.batch_size)\n        idxs = np.random.permutation(self.num_training)  # shuffled ordering\n        user_random = list(self.user[idxs])\n        item_random = list(self.item[idxs])\n        rating_random = list(self.rating[idxs])\n        # train\n        for i in range(total_batch):\n            start_time = time.time()\n            batch_user = user_random[i * self.batch_size:(i + 1) * self.batch_size]\n            batch_item = item_random[i * self.batch_size:(i + 1) * self.batch_size]\n            batch_rating = rating_random[i * self.batch_size:(i + 1) * self.batch_size]\n\n            _, loss = self.sess.run([self.optimizer, self.loss], feed_dict={self.user_id: batch_user,\n                                                                            self.item_id: batch_item,\n                                                                            self.y: batch_rating\n                                                                            })\n            if i % self.display_step == 0:\n                print(""Index: %04d; cost= %.9f"" % (i + 1, np.mean(loss)))\n                if self.show_time:\n                    print(""one iteration: %s seconds."" % (time.time() - start_time))\n\n    def test(self, test_data):\n        error = 0\n        error_mae = 0\n        test_set = list(test_data.keys())\n        # users, items = map(list, zip(*[(1, 2), (3, 4), (5, 6)]))\n        for (u, i) in test_set:\n            pred_rating_test = self.predict([u], [i])\n            error += (float(test_data.get((u, i))) - pred_rating_test) ** 2\n            error_mae += (np.abs(float(test_data.get((u, i))) - pred_rating_test))\n        print(""RMSE:"" + str(RMSE(error, len(test_set))) + ""; MAE:"" + str(MAE(error_mae, len(test_set))))\n\n    def execute(self, train_data, test_data):\n        init = tf.global_variables_initializer()\n        t = train_data.tocoo()\n        self.user = t.row.reshape(-1)\n        self.item = t.col.reshape(-1)\n        self.rating = t.data\n        self.sess.run(init)\n        for epoch in range(self.epochs):\n            print(""Epoch: %04d;"" % (epoch))\n            self.train(train_data)\n            if (epoch) % self.T == 0:\n                self.test(test_data)\n\n    def save(self, path):\n        saver = tf.train.Saver()\n        saver.save(self.sess, path)\n\n    def predict(self, user_id, item_id):\n        return self.sess.run([self.pred_rating], feed_dict={self.user_id: user_id, self.item_id: item_id})[0]\n'"
models/rating_prediction/nrr.py,38,"b'#!/usr/bin/env python\n""""""Implementation of Neural Rating Regression.\nReference: Piji Li, Zihao Wang, Zhaochun Ren, Lidong Bing, Wai Lam. ""Neural Rating Regression with Abstractive Tips Generation for Recommendation\nAuthors."" https://arxiv.org/pdf/1708.00154.pdf\n""""""\n\n\n\n\nimport tensorflow as tf\nimport time\nimport numpy as np\n\nfrom utils.evaluation.RatingMetrics import *\n\n__author__ = ""Shuai Zhang""\n__copyright__ = ""Copyright 2018, The DeepRec Project""\n\n__license__ = ""GPL""\n__version__ = ""1.0.0""\n__maintainer__ = ""Shuai Zhang""\n__email__ = ""cheungdaven@gmail.com""\n__status__ = ""Development""\n\n\nclass NRR():\n    def __init__(self, sess, num_user, num_item, learning_rate=0.001, reg_rate=0.1, epoch=50, batch_size=256,\n                 show_time=False, T=1, display_step=1000):\n        self.learning_rate = learning_rate\n        self.epochs = epoch\n        self.batch_size = batch_size\n        self.reg_rate = reg_rate\n        self.sess = sess\n        self.num_user = num_user\n        self.num_item = num_item\n        self.show_time = show_time\n        self.T = T\n        self.display_step = display_step\n        print(""NRR."")\n\n    def build_network(self, num_factor_user=40, num_factor_item=40, d=50, hidden_dimension=40):\n\n        # model dependent arguments\n        self.user_id = tf.placeholder(dtype=tf.int32, shape=[None], name=\'user_id\')\n        self.item_id = tf.placeholder(dtype=tf.int32, shape=[None], name=\'item_id\')\n        self.y = tf.placeholder(""float"", [None], \'rating\')\n\n        U = tf.Variable(tf.random_normal([self.num_user, num_factor_user], stddev=0.01))\n        V = tf.Variable(tf.random_normal([self.num_item, num_factor_item], stddev=0.01))\n        b = tf.Variable(tf.random_normal([d]))\n\n        user_latent_factor = tf.nn.embedding_lookup(U, self.user_id)\n        item_latent_factor = tf.nn.embedding_lookup(V, self.item_id)\n\n        W_User = tf.Variable(tf.random_normal([num_factor_user, d], stddev=0.01))\n        W_Item = tf.Variable(tf.random_normal([num_factor_item, d], stddev=0.01))\n\n        input = tf.matmul(user_latent_factor, W_User) + tf.matmul(item_latent_factor, W_Item) + b\n\n        layer_1 = tf.layers.dense(inputs=input, units=d, bias_initializer=tf.random_normal_initializer,\n                                  kernel_initializer=tf.random_normal_initializer, activation=tf.sigmoid,\n                                  kernel_regularizer=tf.contrib.layers.l2_regularizer(scale=self.reg_rate))\n        layer_2 = tf.layers.dense(inputs=layer_1, units=hidden_dimension, activation=tf.sigmoid,\n                                  bias_initializer=tf.random_normal_initializer,\n                                  kernel_initializer=tf.random_normal_initializer,\n                                  kernel_regularizer=tf.contrib.layers.l2_regularizer(scale=self.reg_rate))\n        layer_3 = tf.layers.dense(inputs=layer_2, units=hidden_dimension, activation=tf.sigmoid,\n                                  bias_initializer=tf.random_normal_initializer,\n                                  kernel_initializer=tf.random_normal_initializer,\n                                  kernel_regularizer=tf.contrib.layers.l2_regularizer(scale=self.reg_rate))\n        layer_4 = tf.layers.dense(inputs=layer_3, units=hidden_dimension, activation=tf.sigmoid,\n                                  bias_initializer=tf.random_normal_initializer,\n                                  kernel_initializer=tf.random_normal_initializer,\n                                  kernel_regularizer=tf.contrib.layers.l2_regularizer(scale=self.reg_rate))\n        output = tf.layers.dense(inputs=layer_4, units=1, activation=None,\n                                 bias_initializer=tf.random_normal_initializer,\n                                 kernel_initializer=tf.random_normal_initializer,\n                                 kernel_regularizer=tf.contrib.layers.l2_regularizer(scale=self.reg_rate))\n        self.pred_rating = tf.reshape(output, [-1])\n\n        # print(np.shape(output))\n        reg_losses = tf.reduce_sum(tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES))\n        self.loss = tf.reduce_sum(tf.square(self.y - self.pred_rating)) \\\n                    + tf.losses.get_regularization_loss() + self.reg_rate * (\n        tf.norm(U) + tf.norm(V) + tf.norm(b) + tf.norm(W_Item) + tf.norm(W_User))\n        self.optimizer = tf.train.AdamOptimizer(learning_rate=self.learning_rate).minimize(self.loss)\n\n    def train(self, train_data):\n        self.num_training = len(self.rating)\n        total_batch = int(self.num_training / self.batch_size)\n        idxs = np.random.permutation(self.num_training)  # shuffled ordering\n        user_random = list(self.user[idxs])\n        item_random = list(self.item[idxs])\n        rating_random = list(self.rating[idxs])\n\n        # train\n        for i in range(total_batch):\n            start_time = time.time()\n            batch_user = user_random[i * self.batch_size:(i + 1) * self.batch_size]\n            batch_item = item_random[i * self.batch_size:(i + 1) * self.batch_size]\n            batch_rating = rating_random[i * self.batch_size:(i + 1) * self.batch_size]\n\n            _, loss = self.sess.run([self.optimizer, self.loss], feed_dict={self.user_id: batch_user,\n                                                                            self.item_id: batch_item,\n                                                                            self.y: batch_rating\n                                                                            })\n            if i % self.display_step == 0:\n                print(""Index: %04d; cost= %.9f"" % (i + 1, np.mean(loss)))\n                if self.show_time:\n                    print(""one iteration: %s seconds."" % (time.time() - start_time))\n\n    def test(self, test_data):\n        error = 0\n        error_mae = 0\n        test_set = list(test_data.keys())\n        for (u, i) in test_set:\n            pred_rating_test = self.predict([u], [i], False)\n            error += (float(test_data.get((u, i))) - pred_rating_test) ** 2\n            error_mae += (np.abs(float(test_data.get((u, i))) - pred_rating_test))\n        print(""RMSE:"" + str(RMSE(error, len(test_set))) + ""; MAE:"" + str(MAE(error_mae, len(test_set))))\n\n    def execute(self, train_data, test_data):\n        init = tf.global_variables_initializer()\n        t = train_data.tocoo()\n        self.user = t.row.reshape(-1)\n        self.item = t.col.reshape(-1)\n        self.rating = t.data\n        self.sess.run(init)\n\n        for epoch in range(self.epochs):\n            print(""Epoch: %04d;"" % (epoch))\n            self.train(train_data)\n            if (epoch) % self.T == 0:\n                self.test(test_data)\n\n    def save(self, path):\n        saver = tf.train.Saver()\n        saver.save(self.sess, path)\n\n    def predict(self, user_id, item_id, prt=True):\n        score = self.sess.run([self.pred_rating], feed_dict={self.user_id: user_id, self.item_id: item_id})[0]\n        if prt:\n            print(score)\n        return score\n'"
models/seq_rec/AttRec.py,174,"b'#!/usr/bin/env python\n""""""Implementation of Caser.\nReference: Next Item Recommendation with Self-Attentive Metric Learning, Shuai Zhang etc. , AAAI workshop\'18.\n""""""\n\nimport tensorflow as tf\nimport time\nimport numpy as np\nfrom utils.evaluation.SeqRecMetrics import *\nimport numpy as np\n\nnp.set_printoptions(threshold=np.inf)\n__author__ = ""Shuai Zhang""\n__copyright__ = ""Copyright 2018, The DeepRec Project""\n\n__license__ = ""GPL""\n__version__ = ""1.0.0""\n__maintainer__ = ""Shuai Zhang""\n__email__ = ""cheungdaven@gmail.com""\n__status__ = ""Development""\n\n\nclass AttRec():\n    def __init__(self, sess, num_user, num_item, learning_rate=0.05, reg_rate=1e-2, epoch=5000, batch_size=1000,\n                 show_time=False, T=1, display_step=1000, verbose=False):\n        self.learning_rate = learning_rate\n        self.epochs = epoch\n        self.batch_size = batch_size\n        self.reg_rate = reg_rate\n        self.sess = sess\n        self.num_user = num_user\n        self.num_item = num_item\n        self.show_time = show_time\n        self.verbose = verbose\n        self.T = T\n        self.display_step = display_step\n\n        self.neg_items = dict()\n        print(""AttSeqRec."")\n\n    def build_network(self, L, num_T, num_factor=150, num_neg=1):\n\n        self.L = L\n        self.num_T = num_T\n        self.num_factor = num_factor\n        self.num_neg = num_neg\n\n        self.user_id = tf.placeholder(dtype=tf.int32, shape=[None], name=\'user_id\')\n        self.item_seq = tf.placeholder(dtype=tf.int32, shape=[None, L], name=\'item_seq\')\n        self.item_id = tf.placeholder(dtype=tf.int32, shape=[None, self.num_T], name=\'item_id\')\n        self.item_id_test = tf.placeholder(dtype=tf.int32, shape=[None, 1], name=\'item_id_test\')\n        self.neg_item_id = tf.placeholder(dtype=tf.int32, shape=[None, self.num_T * self.num_neg], name=\'item_id_neg\')\n        self.isTrain = tf.placeholder(tf.bool, shape=())\n\n        # self.y = tf.placeholder(""float"", [None], \'rating\')\n        print(np.shape(self.user_id))\n\n        # initializer = tf.contrib.layers.xavier_initializer()\n        # self.P = tf.Variable(initializer([self.num_user, num_factor] ))\n        # self.V = tf.Variable(initializer([self.num_user, num_factor * 1] ))\n        # self.Q = tf.Variable(initializer([self.num_item, num_factor] ))\n        # self.X = tf.Variable(initializer([self.num_item, num_factor] ))\n\n        self.P = tf.Variable(tf.truncated_normal([self.num_user, num_factor], stddev=0.001))\n        self.V = tf.Variable(tf.truncated_normal([self.num_user, num_factor * 1], stddev=0.001))\n        self.Q = tf.Variable(tf.truncated_normal([self.num_item, num_factor], stddev=0.001))\n        self.X = tf.Variable(tf.truncated_normal([self.num_item, num_factor], stddev=0.001))\n        self.A = tf.Variable(tf.truncated_normal([self.num_item, num_factor], stddev=0.001))\n\n\n        item_latent_factor_neg = tf.nn.embedding_lookup(self.Q, self.neg_item_id)\n        self.W = tf.Variable(tf.random_normal([self.num_item, self.num_factor * (1 + 1)], stddev=0.01))\n        self.b = tf.Variable(tf.random_normal([self.num_item], stddev=0.01))\n        # vertical conv layer\n        # self.conv_v = tf.nn.conv2d(1, n_v, (L, 1))\n\n\n\n        self.target_prediction = self._distance_self_attention(self.item_seq, self.user_id, self.item_id)\n        self.negative_prediction = self._distance_self_attention(self.item_seq, self.user_id, self.neg_item_id)\n        self.test_prediction = self._distance_self_attention(self.item_seq, self.user_id, self.item_id_test, isTrain=False)\n\n        self.user_param = self.user_latent_factor\n        self.user_param_2 = self.user_specific_bias\n\n        self.seq_param = self.out\n        self.seq_weight = self.weights\n        self.item_param_1, self.item_param_2 = self._getItemParam(self.item_id_test)\n\n        # - tf.reduce_sum(tf.log(tf.sigmoid(- self.target_prediction + self.negative_prediction)) )\n        # - tf.reduce_mean(tf.log(tf.sigmoid(self.target_prediction) + 1e-10)) - tf.reduce_mean( tf.log( tf.sigmoid(1 - self.negative_prediction) + 1e-10))\n        # tf.reduce_mean(tf.square(1 - self.negative_prediction)) + tf.reduce_mean(tf.square(self.target_prediction))\n        # tf.reduce_sum(tf.maximum(self.target_prediction - self.negative_prediction + 0.5, 0))\n\n\n\n        self.loss = tf.reduce_sum(tf.maximum(self.target_prediction - self.negative_prediction + 0.5, 0)) \\\n                    + tf.losses.get_regularization_loss() + 0.001 * (\n        tf.nn.l2_loss(self.P) + tf.nn.l2_loss(self.V) + tf.nn.l2_loss(self.X) + tf.nn.l2_loss(self.Q))\n\n\n        norm_clip_value = 1\n        self.clip_P = tf.assign(self.P, tf.clip_by_norm(self.P, norm_clip_value, axes=[1]))\n        self.clip_Q = tf.assign(self.Q, tf.clip_by_norm(self.Q, norm_clip_value, axes=[1]))\n        self.clip_V = tf.assign(self.V, tf.clip_by_norm(self.V, norm_clip_value, axes=[1]))\n        self.clip_X = tf.assign(self.X, tf.clip_by_norm(self.X, norm_clip_value, axes=[1]))\n\n\n\n        self.optimizer = tf.train.AdagradOptimizer(self.learning_rate, initial_accumulator_value=0.05).minimize(self.loss)  # GradientDescentOptimizer\n\n        return self\n\n\n\n    def getUserParam(self, user_id):\n        params = self.sess.run([self.user_param, self.seq_param,  self.user_param_2, self.seq_weight], feed_dict={self.user_id: user_id,\n                                                           self.item_seq: self.test_sequences[user_id, :]})\n        return params[0], params[1], params[2], params[3]\n\n    def getItemParam(self, item_id):\n        params = self.sess.run([self.item_param_1, self.item_param_2,  self.bias_item], feed_dict={self.item_id_test: item_id})\n        return np.squeeze(params[0]), np.squeeze(params[1]), params[2]\n\n    def _getItemParam(self, item_id):\n        w_items = tf.nn.embedding_lookup(self.X, item_id)\n        w_items_2 = tf.nn.embedding_lookup(self.Q, item_id)\n        return w_items, w_items_2\n\n\n\n    def _distance_self_attention(self, item_seq, user_id, item_id, isTrain=True):\n        # horizontal conv layer\n        lengths = [i + 1 for i in range(self.L)]\n\n        out, out_h, out_v = None, None, None\n        # print(np.shape(item_seq)[1])\n\n\n        # item_latent_factor = self.add_timing_signal(item_latent_factor)\n\n        item_latent_factor =  tf.nn.embedding_lookup(self.Q, item_seq) #+ self.user_specific_bias\n\n\n        item_latent_factor_2 = tf.nn.embedding_lookup(self.X, item_seq)\n\n        query = key = value = self.add_timing_signal(item_latent_factor)\n\n        if isTrain:\n            query = tf.layers.dense(inputs=query, name=""linear_project"", units=self.num_factor, activation=tf.nn.relu,\n                                    use_bias=False,\n                                    kernel_initializer=tf.contrib.layers.xavier_initializer(), reuse=tf.AUTO_REUSE,\n                                    kernel_regularizer=tf.contrib.layers.l2_regularizer(scale=self.reg_rate)\n                                    )\n            #query = tf.layers.dropout(query, rate=0.0)\n            key = tf.layers.dense(inputs=key, name=""linear_project"", units=self.num_factor, activation=tf.nn.relu,\n                                  use_bias=False,\n                                  kernel_initializer=tf.contrib.layers.xavier_initializer(), reuse=tf.AUTO_REUSE,\n                                  kernel_regularizer=tf.contrib.layers.l2_regularizer(scale=self.reg_rate),\n                                  )\n            #key = tf.layers.dropout(key, rate=0.3)\n        else:\n            query = tf.layers.dense(inputs=query, name=""linear_project"", units=self.num_factor, activation=tf.nn.relu,\n                                    use_bias=False,\n                                    kernel_initializer=tf.contrib.layers.xavier_initializer(), reuse=tf.AUTO_REUSE,\n                                    kernel_regularizer=tf.contrib.layers.l2_regularizer(scale=self.reg_rate)\n                                    )\n            key = tf.layers.dense(inputs=key, name=""linear_project"", units=self.num_factor, activation=tf.nn.relu,\n                                  use_bias=False,\n                                  kernel_initializer=tf.contrib.layers.xavier_initializer(), reuse=tf.AUTO_REUSE,\n                                  kernel_regularizer=tf.contrib.layers.l2_regularizer(scale=self.reg_rate)\n                                  )\n\n        logits = tf.matmul(query, key, transpose_b=True) / np.sqrt(self.num_factor)\n\n        print(np.shape(logits))\n        weights = tf.nn.softmax(logits, dim=-1, name=""attention_weights"")\n        mask = tf.ones([self.L, self.L])\n        mask = tf.matrix_set_diag(mask, tf.zeros([self.L]))\n        weights = weights * mask\n\n        out =   tf.matmul(weights, item_latent_factor )\n        self.weights = weights\n\n        print(np.shape(item_latent_factor))\n        self.out = tf.reduce_mean(out, 1)\n        print(np.shape(self.out))\n\n\n        w_items =  tf.nn.embedding_lookup(self.X, item_id)\n        w_items_2 = tf.nn.embedding_lookup(self.Q, item_id)\n        w_items_3 = tf.nn.embedding_lookup(self.V, user_id)#tf.nn.embedding_lookup(self.A, item_id)\n        self.bias_item = tf.nn.embedding_lookup(self.b, item_id)\n\n\n        x_tmp = []\n        for i in range(np.shape(item_id)[1]):\n            x_tmp.append(self.out)\n        x = tf.stack(x_tmp)\n        print(np.shape(x))\n        print(np.shape(w_items))\n        x = tf.transpose(x, [1, 0, 2])\n\n\n        self.user_latent_factor = tf.nn.embedding_lookup(self.P, user_id)\n\n\n        u_tmp = []\n        for i in range(np.shape(item_id)[1]):\n            u_tmp.append(self.user_latent_factor)\n        u = tf.stack(u_tmp)\n        print(np.shape(u))\n        u = tf.transpose(u, [1, 0, 2])\n\n        self.user_specific_bias = tf.nn.embedding_lookup(self.V, user_id)\n        u_tmp_2 = []\n        for i in range(np.shape(item_id)[1]):\n            u_tmp_2.append(self.user_specific_bias)\n        u_2 = tf.stack(u_tmp_2)\n        print(np.shape(u_2))\n        u_2 = tf.transpose(u_2, [1, 0, 2])\n\n\n        self.alpha = 0.2\n        if isTrain:\n            res = self.alpha * tf.reduce_sum(tf.nn.dropout(tf.square(w_items - u), 1), 2) + (1-self.alpha) * tf.reduce_sum(tf.nn.dropout(tf.square(x -w_items_2  ),1),2)   #+ 0.1 * tf.reduce_sum(tf.square(x - u), 2)\n        else:\n            res = self.alpha * tf.reduce_sum(tf.square(w_items - u), 2) + (1 - self.alpha) * tf.reduce_sum(\n                tf.square(x - w_items_2 ), 2)\n\n        print(np.shape(res))\n        return tf.squeeze(res)\n\n\n    def _distance_multihead(self, item_seq, user_latent_factor, item_id):\n        # horizontal conv layer\n        lengths = [i + 1 for i in range(self.L)]\n\n        out, out_h, out_v = None, None, None\n\n        # item_latent_factor = self.add_timing_signal(item_latent_factor)\n        item_latent_factor = tf.nn.embedding_lookup(self.Q, item_seq)\n        item_latent_factor_2 = tf.nn.embedding_lookup(self.X, item_seq)\n\n        query = key = self.add_timing_signal(item_latent_factor)\n\n        out = self.multihead_attention(queries=query, keys=key, value=item_latent_factor, reuse=tf.AUTO_REUSE)\n        out = tf.reduce_mean(out, 1)\n\n        query_2 = key_2 = out\n        query_2 = tf.layers.dense(inputs=query_2, name=""linear_project1"", units=self.num_factor, activation=None,\n                                  use_bias=False,\n                                  kernel_initializer=tf.contrib.layers.xavier_initializer(), reuse=tf.AUTO_REUSE,\n                                  kernel_regularizer=tf.contrib.layers.l2_regularizer(scale=self.reg_rate)\n                                  )\n        key_2 = tf.layers.dense(inputs=key_2, name=""linear_project1"", units=self.num_factor, activation=None,\n                                use_bias=False,\n                                kernel_initializer=tf.contrib.layers.xavier_initializer(), reuse=tf.AUTO_REUSE,\n                                kernel_regularizer=tf.contrib.layers.l2_regularizer(scale=self.reg_rate)\n                                )\n        # value = tf.layers.dense(inputs= key, name=""linear_project"", units = seq_len_item, activation = None,  kernel_initializer=tf.random_normal_initializer, reuse=True)\n        # b =  tf.Variable(tf.random_normal([seq_len_user], stddev=1))\n        logits_2 = tf.matmul(query_2, key_2, transpose_b=True) / np.sqrt(self.num_factor)\n        weights_2 = tf.nn.softmax(logits_2, name=""attention_weights1"")\n        mask_2 = tf.ones([self.L, self.L])\n        mask_2 = tf.matrix_set_diag(mask_2, tf.zeros([self.L]))\n        weights_2 = weights_2 * mask_2\n        out_2 = tf.reduce_mean(tf.matmul(weights_2, out) , 1)\n\n        print(""--------------"")\n        print(np.shape(out))\n        print(np.shape(out_2))\n\n\n\n\n        w_items = tf.nn.embedding_lookup(self.X, item_id)\n        w_items_2 = tf.nn.embedding_lookup(self.Q, item_id)\n        b_items = tf.nn.embedding_lookup(self.b, item_id)\n        item_specific_bias = tf.nn.embedding_lookup(self.X, item_id)\n\n\n        x_tmp = []\n        for i in range(np.shape(w_items)[1]):\n            x_tmp.append(out)\n        x = tf.stack(x_tmp)\n        print(np.shape(x))\n        print(np.shape(w_items))\n        x = tf.transpose(x, [1, 0, 2])\n\n        u_tmp = []\n        for i in range(np.shape(w_items)[1]):\n            u_tmp.append(user_latent_factor)\n        u = tf.stack(u_tmp)\n        print(np.shape(u))\n        u = tf.transpose(u, [1, 0, 2])\n\n        # res = tf.reduce_sum(tf.multiply(x, w_items), 2) + b_items\n        res = 0.2 * tf.reduce_sum(tf.square(w_items - u), 2) + 0.8 * tf.reduce_sum(tf.square(x- w_items_2),2)   # + 0.1 * tf.reduce_sum(tf.square(x - u), 2)\n\n\n        print(np.shape(res))\n        return tf.squeeze(res)\n\n    def execute(self, train_data, test_data):\n\n        self.prepare_data(train_data, test_data)\n        init = tf.global_variables_initializer()\n        self.sess.run(init)\n        for epoch in range(self.epochs):\n            if self.verbose:\n                print(""Epoch: %04d;"" % (epoch))\n            self.train(train_data)\n            if (epoch) % self.T == 0 and epoch >= 5:\n                print(""Epoch: %04d; "" % (epoch), end=\'\')\n                self.test(test_data)\n\n\n    def prepare_data(self, train_data, test_data):\n        self.sequences = train_data.sequences.sequences\n        # print(self.sequences)\n        self.targets = train_data.sequences.targets\n        self.users = train_data.sequences.user_ids.reshape(-1, 1)\n        all_items = set(np.arange(self.num_item - 1) + 1)\n        self.all_items = all_items\n        # print(all_items) # from 1 to 1679\n\n\n        self.test_data = dict()\n        test = test_data.tocsr()\n\n        for user, row in enumerate(test):\n            self.test_data[user] = list(set(row.indices))\n\n        self.x = []\n        for i, u in enumerate(self.users.squeeze()):\n            tar = set([int(t) for t in self.targets[i]])\n            # print(tar)\n            seq = set([int(t) for t in self.sequences[i]])\n            self.x.append(list(all_items - tar))\n            # print(self.test_data[u][0] in  self.x[i])\n        self.x = np.array(self.x)\n\n        if not self.neg_items:\n            # all_items = set(np.arange(self.num_item - 1) + 1)\n            train = train_data.tocsr()\n\n            for user, row in enumerate(train):\n                # print(user)\n                # print(row.indices)\n                # print(0 in row.indices)\n                self.neg_items[user] = list(all_items - set(row.indices))\n                # print(self.test_data[user][0] in self.neg_items[user])\n        print(""Data Preparation Finish."")\n\n    def train(self, train_data):\n\n        # print(users)\n\n        self.num_training = len(self.sequences)\n        self.total_batch = int(self.num_training / self.batch_size)\n        L, T = train_data.sequences.L, train_data.sequences.T\n        self.test_sequences = train_data.test_sequences.sequences\n        # print(self.test_sequences)\n        idxs = np.random.permutation(\n            self.num_training)  # shuffled ordering np.random.choice(self.num_training, self.num_training, replace=True) #\n\n        sequences_random = [i.tolist() for i in list(self.sequences[idxs])]\n        targets_random = list(self.targets[idxs])\n        users_random = [i[0] for i in list(self.users[idxs])]\n        self.x_random = list(self.x[idxs])\n        item_random_neg = self._get_neg_items_sbpr(self.users.squeeze(), train_data, self.num_neg * self.num_T)\n\n        # # train\n        for i in range(self.total_batch):\n            start_time = time.time()\n            batch_user = users_random[i * self.batch_size:(i + 1) * self.batch_size]\n            batch_seq = sequences_random[i * self.batch_size:(i + 1) * self.batch_size]\n            batch_item = targets_random[i * self.batch_size:(i + 1) * self.batch_size]\n            batch_item_neg = item_random_neg[i * self.batch_size:(i + 1) * self.batch_size]\n            # print(batch_item_neg)\n            #, self.clip_P, self.clip_Q, self.clip_X\n            _, loss = self.sess.run((self.optimizer, self.loss), feed_dict={self.user_id: batch_user,\n                                                                            self.item_seq: batch_seq,\n                                                                            self.item_id: batch_item,\n                                                                            self.neg_item_id: batch_item_neg,\n                                                                            self.isTrain: True})\n            #\n            if i % self.display_step == 0:\n                if self.verbose:\n                    print(""Index: %04d; cost= %.9f"" % (i + 1, np.mean(loss)))\n                    #             print(""one iteration: %s seconds."" % (time.time() - start_time))\n\n    def test(self, test_data):\n        # print(test_data.user_map)\n\n        # print(self.test_data)\n        self.test_users = []\n        for i in range(self.num_user):\n            self.test_users.append(i)\n        # print(self.test_users)\n        evaluate1(self)\n\n    def save(self, path):\n        saver = tf.train.Saver()\n        saver.save(self.sess, path)\n\n    def predict(self, user_id, item_id):\n        # print(user_id)\n        # print(len(self.test_sequences))\n        # print(self.test_sequences[user_id, :])\n        # user_id_2 = [i+1 for i in user_id]\n        item_id = [[i] for i in item_id]\n        # print(len(item_id))\n\n        return -self.sess.run([self.test_prediction], feed_dict={self.user_id: user_id,\n                                                                  self.item_seq: self.test_sequences[user_id, :],\n                                                                  self.item_id_test: item_id})[0]\n\n    def _weight_variable(self, shape):\n        initial = tf.random_normal(shape, stddev=0.1)\n        return tf.Variable(initial)\n\n    def _bias_variable(self, shape):\n        initial = tf.constant(0.1, shape=shape)\n        return tf.Variable(initial)\n\n    def _get_neg_items(self, users, interactions, n):\n\n        # users = users.squeeze()\n        print(users.shape[0])\n        neg_items_samples = np.zeros((users.shape[0], n))\n\n        # if not self.neg_items:\n        #     all_items = set(np.arange(self.num_item - 1) + 1)\n        #     train = interactions.tocsr()\n        #\n        #     for user, row in enumerate(train):\n        #         self.neg_items[user] = list(all_items - set(row.indices))\n\n        for i, u in enumerate(users):\n            for j in range(n):\n                x = self.neg_items[u]\n                neg_items_samples[i, j] = x[np.random.randint(len(x))]\n\n        return neg_items_samples\n\n    def _get_neg_items_sbpr(self, users, interactions, n):\n        # print(""start sampling"")\n        # print(targets)\n        # users = users.squeeze()\n        neg_items_samples = np.zeros((users.shape[0], n))\n        # all_items = None\n        # if not self.neg_items:\n        #     all_items = set(np.arange(self.num_item - 1) + 1)\n        #     train = interactions.tocsr()\n        #\n        #     for user, row in enumerate(train):\n        #         self.neg_items[user] = list(all_items - set(row.indices))\n        print(len(users))\n        for i, u in enumerate(users):\n            for j in range(n):\n                # print(int(targets[i][0]))\n                neg_items_samples[i, j] = self.x_random[i][np.random.randint(len(self.x_random[i]))]\n        # print(""end sampling"")\n        return neg_items_samples\n\n    def add_timing_signal(self, x, min_timescale=1.0, max_timescale=1.0e4):\n        """"""Adds a bunch of sinusoids of different frequencies to a Tensor.\n        Each channel of the input Tensor is incremented by a sinusoid of a\n        different frequency and phase.\n        This allows attention to learn to use absolute and relative positions.\n        Timing signals should be added to some precursors of both the query and the\n        memory inputs to attention.\n        The use of relative position is possible because sin(x+y) and cos(x+y) can\n        be experessed in terms of y, sin(x) and cos(x).\n        In particular, we use a geometric sequence of timescales starting with\n        min_timescale and ending with max_timescale.  The number of different\n        timescales is equal to channels / 2. For each timescale, we\n        generate the two sinusoidal signals sin(timestep/timescale) and\n        cos(timestep/timescale).  All of these sinusoids are concatenated in\n        the channels dimension.\n        Args:\n            x: a Tensor with shape [batch, length, channels]\n            min_timescale: a float\n            max_timescale: a float\n        Returns:\n            a Tensor the same shape as x.\n        """"""\n        with tf.name_scope(""add_timing_signal"", values=[x]):\n            length = tf.shape(x)[1]\n            channels = tf.shape(x)[2]\n            position = tf.to_float(tf.range(length))\n            num_timescales = channels // 2\n\n            log_timescale_increment = (\n                math.log(float(max_timescale) / float(min_timescale)) /\n                (tf.to_float(num_timescales) - 1)\n            )\n            inv_timescales = min_timescale * tf.exp(\n                tf.to_float(tf.range(num_timescales)) * -log_timescale_increment\n            )\n\n            scaled_time = (tf.expand_dims(position, 1) *\n                           tf.expand_dims(inv_timescales, 0))\n            signal = tf.concat([tf.sin(scaled_time), tf.cos(scaled_time)], axis=1)\n            signal = tf.pad(signal, [[0, 0], [0, tf.mod(channels, 2)]])\n            signal = tf.reshape(signal, [1, length, channels])\n\n            return x + signal\n\n    def normalize(self, inputs,\n                  epsilon=1e-8,\n                  scope=""ln"",\n                  reuse=None):\n        \'\'\'Applies layer normalization.\n        Args:\n          inputs: A tensor with 2 or more dimensions, where the first dimension has\n            `batch_size`.\n          epsilon: A floating number. A very small number for preventing ZeroDivision Error.\n          scope: Optional scope for `variable_scope`.\n          reuse: Boolean, whether to reuse the weights of a previous layer\n            by the same name.\n        Returns:\n          A tensor with the same shape and data dtype as `inputs`.\n        \'\'\'\n        with tf.variable_scope(scope, reuse=reuse):\n            inputs_shape = inputs.get_shape()\n            params_shape = inputs_shape[-1:]\n\n            mean, variance = tf.nn.moments(inputs, [-1], keep_dims=True)\n            beta = tf.Variable(tf.zeros(params_shape))\n            gamma = tf.Variable(tf.ones(params_shape))\n            normalized = (inputs - mean) / ((variance + epsilon) ** (.5))\n            outputs = gamma * normalized + beta\n\n        return outputs\n\n    def multihead_attention(self, queries, keys, value, num_units=None, num_heads=2, dropout_rate=0, is_training=True, causality=False, scope=""multihead_attention"", reuse=None):\n        with tf.variable_scope(scope, reuse=reuse):\n            if num_units is None:\n                num_units = queries.get_shape().as_list()[-1]\n\n\n            Q = tf.layers.dense(queries, num_units, name=""project_q"", activation=tf.nn.relu,\n                                use_bias=False,\n                                kernel_initializer=tf.contrib.layers.xavier_initializer(),\n                                kernel_regularizer=tf.contrib.layers.l2_regularizer(scale=self.reg_rate)) # (batch size, sequence length, dim)\n            K = tf.layers.dense(keys, num_units, name=""project_k"", activation=tf.nn.relu,\n                                use_bias=False,\n                                kernel_initializer=tf.contrib.layers.xavier_initializer(),\n                                kernel_regularizer=tf.contrib.layers.l2_regularizer(scale=self.reg_rate))\n            V = tf.layers.dense(value, num_units, name=""project_v"",activation=tf.nn.relu,\n                                use_bias=False,\n                                kernel_initializer=tf.contrib.layers.xavier_initializer(),\n                                kernel_regularizer=tf.contrib.layers.l2_regularizer(scale=self.reg_rate))\n\n            Q_ = tf.concat(tf.split(Q, num_heads, axis=2),axis=0) #( h * batch size, seq len, dim/h)\n            K_ = tf.concat(tf.split(K, num_heads, axis=2),axis=0)\n            V_ = tf.concat(tf.split(value, num_heads, axis=2),axis=0)\n\n\n            outputs = tf.matmul(Q_, tf.transpose(K_, [0, 2, 1]))\n\n            # scale\n            outputs = outputs / ( K_.get_shape().as_list()[-1] ** 0.5)\n\n            # key masking\n            key_masks = tf.sign(tf.abs(tf.reduce_sum(keys, axis=-1))) # (batch size, seq len)\n            key_masks = tf.tile(key_masks, [num_heads, 1])\n            key_masks = tf.tile(tf.expand_dims(key_masks, 1), [ 1, tf.shape(queries)[1], 1])\n\n            paddings = tf.ones_like(outputs) * (-2**32 + 1)\n\n            outputs = tf.where(tf.equal(key_masks, 0), paddings, outputs)\n\n\n            if causality:\n                diag_vals = tf.ones_like(outputs[0,:,:])\n                tril = tf.contrib.linalg.LinearOperatorTriL(diag_vals).to_dense()\n                masks = tf.tile(tf.expand_dims(tril, 0), [tf.shape(outputs)[0], 1, 1])\n\n                paddings = tf.ones_like(masks) * ( -2**32 +1 )\n                outputs = tf.where(tf.equal(key_masks, 0), paddings, outputs)\n\n            outputs = tf.nn.sigmoid(outputs)\n\n            query_masks = tf.sign(tf.abs(tf.reduce_sum(queries, axis=-1)))\n            query_masks = tf.tile(query_masks, [num_heads, 1])\n            query_masks = tf.tile(tf.expand_dims(query_masks, -1), [1,1, tf.shape(keys)[1]])\n            outputs *= query_masks\n\n            outputs = tf.layers.dropout(outputs, rate=dropout_rate, training=tf.convert_to_tensor(is_training))\n\n            outputs = tf.matmul(outputs, V_)\n\n            outputs = tf.concat(tf.split(outputs, num_heads, axis=0), axis=2)\n\n            #outputs += value\n\n            #outputs = self.normalize(outputs, reuse=tf.AUTO_REUSE)\n        return outputs\n\n\n    def feedforward(self, inputs,\n                    num_units=[400, 100],\n                    scope=""multihead_attention"",\n                    reuse=None):\n        \'\'\'Point-wise feed forward net.\n\n        Args:\n          inputs: A 3d tensor with shape of [N, T, C].\n          num_units: A list of two integers.\n          scope: Optional scope for `variable_scope`.\n          reuse: Boolean, whether to reuse the weights of a previous layer\n            by the same name.\n\n        Returns:\n          A 3d tensor with the same shape and dtype as inputs\n        \'\'\'\n        with tf.variable_scope(scope, reuse=reuse):\n            # Inner layer\n            params = {""inputs"": inputs, ""filters"": num_units[0], ""kernel_size"": 1,\n                      ""activation"": tf.nn.relu, ""use_bias"": True}\n            outputs = tf.layers.conv1d(**params,\n                                       kernel_initializer=tf.contrib.layers.xavier_initializer(),\n                                kernel_regularizer=tf.contrib.layers.l2_regularizer(scale=self.reg_rate))\n\n            # Readout layer\n            params = {""inputs"": outputs, ""filters"": num_units[1], ""kernel_size"": 1,\n                      ""activation"": None, ""use_bias"": True}\n            outputs = tf.layers.conv1d(**params, kernel_initializer=tf.contrib.layers.xavier_initializer(),\n                                kernel_regularizer=tf.contrib.layers.l2_regularizer(scale=self.reg_rate))\n\n            # Residual connection\n            outputs += inputs\n\n            # Normalize\n            outputs = self.normalize(outputs, reuse=tf.AUTO_REUSE)\n\n        return outputs\n\n    def add_timing_signal_nd(self, x, min_timescale=1.0, max_timescale=1.0e4):\n        """""" Adds a bunch of sinusoids of different frequencies to a Tensor.\n            Each channel of the input Tensor is incremented by a sinusoid of a\n            different frequency and phase in one of the positional dimensions.\n            This allows attention to learn to use absolute and relative positions.\n            Timing signals should be added to some precursors of both the query and\n            the memory inputs to attention.\n            The use of relative position is possible because sin(a+b) and cos(a+b)\n            can be experessed in terms of b, sin(a) and cos(a).\n            x is a Tensor with n ""positional"" dimensions, e.g. one dimension for a\n            sequence or two dimensions for an image\n            We use a geometric sequence of timescales starting with min_timescale\n            and ending with max_timescale.  The number of different timescales is\n            equal to channels // (n * 2). For each timescale, we generate the two\n            sinusoidal signals sin(timestep/timescale) and cos(timestep/timescale).\n            All of these sinusoids are concatenated in the channels dimension.\n            Args:\n                x: a Tensor with shape [batch, d1 ... dn, channels]\n                min_timescale: a float\n                max_timescale: a float\n            Returns:\n                a Tensor the same shape as x.\n        """"""\n        static_shape = x.get_shape().as_list()\n        num_dims = len(static_shape) - 2\n        channels = tf.shape(x)[-1]\n        num_timescales = channels // (num_dims * 2)\n        log_timescale_increment = (\n            math.log(float(max_timescale) / float(min_timescale)) /\n            (tf.to_float(num_timescales) - 1)\n        )\n        inv_timescales = min_timescale * tf.exp(\n              tf.to_float(tf.range(num_timescales)) * -log_timescale_increment\n        )\n        for dim in range(num_dims):\n            length = tf.shape(x)[dim + 1]\n            position = tf.to_float(tf.range(length))\n            scaled_time = tf.expand_dims(position, 1) * tf.expand_dims(\n                inv_timescales, 0)\n            signal = tf.concat([tf.sin(scaled_time), tf.cos(scaled_time)], axis=1)\n            prepad = dim * 2 * num_timescales\n            postpad = channels - (dim + 1) * 2 * num_timescales\n            signal = tf.pad(signal, [[0, 0], [prepad, postpad]])\n            for _ in range(1 + dim):\n                signal = tf.expand_dims(signal, 0)\n            for _ in range(num_dims - 1 - dim):\n                signal = tf.expand_dims(signal, -2)\n            x += signal\n\n        return x\n'"
models/seq_rec/Caser.py,60,"b'#!/usr/bin/env python\n""""""Implementation of Caser.\nReference: Personalized Top-N Sequential Recommendation via Convolutional Sequence Embedding, Jiaxi Tang and Ke Wang , WSDM \'18.\n""""""\n\nimport tensorflow as tf\nimport time\nimport numpy as np\nfrom utils.evaluation.SeqRecMetrics import *\n\n__author__ = ""Shuai Zhang""\n__copyright__ = ""Copyright 2018, The DeepRec Project""\n\n__license__ = ""GPL""\n__version__ = ""1.0.0""\n__maintainer__ = ""Shuai Zhang""\n__email__ = ""cheungdaven@gmail.com""\n__status__ = ""Development""\n\n\nclass Caser():\n    def __init__(self, sess, num_user, num_item, learning_rate=0.001, reg_rate=1e-6, epoch=500, batch_size=1000,\n                 show_time=False, T=1, display_step=1000, verbose=False):\n        self.learning_rate = learning_rate\n        self.epochs = epoch\n        self.batch_size = batch_size\n        self.reg_rate = reg_rate\n        self.sess = sess\n        self.num_user = num_user\n        self.num_item = num_item\n        self.show_time = show_time\n        self.verbose = verbose\n        self.T = T\n        self.display_step = display_step\n\n        self.neg_items = dict()\n        print(""Caser."")\n\n    def build_network(self, L, num_T, n_h=16, n_v=4, num_factor=150, num_neg=2):\n        self.n_h = n_h\n        self.n_v = n_v\n        self.L = L\n        self.num_T = num_T\n        self.num_factor = num_factor\n        self.num_neg = num_neg\n\n        self.user_id = tf.placeholder(dtype=tf.int32, shape=[None], name=\'user_id\')\n        self.item_seq = tf.placeholder(dtype=tf.int32, shape=[None, L], name=\'item_seq\')\n        self.item_id = tf.placeholder(dtype=tf.int32, shape=[None,  self.num_T], name=\'item_id\')\n        self.item_id_test = tf.placeholder(dtype=tf.int32, shape=[None, 1], name=\'item_id_test\')\n        self.neg_item_id = tf.placeholder(dtype=tf.int32, shape=[None, self.num_T * self.num_neg], name=\'item_id_neg\')\n        self.isTrain = tf.placeholder(tf.bool, shape=())\n\n        # self.y = tf.placeholder(""float"", [None], \'rating\')\n        print(np.shape(self.user_id))\n\n        self.P = tf.Variable(tf.random_normal([self.num_user, num_factor], stddev=0.01))\n        self.V = tf.Variable(tf.random_normal([self.num_user, num_factor * self.L], stddev=0.01))\n        self.Q = tf.Variable(tf.random_normal([self.num_item, num_factor], stddev=0.01))\n\n        user_latent_factor = tf.nn.embedding_lookup(self.P, self.user_id)\n        self.user_specific_bias =  tf.nn.embedding_lookup(self.V, self.user_id)\n        item_latent_factor = tf.nn.embedding_lookup(self.Q, self.item_seq)\n        item_latent_factor_neg = tf.nn.embedding_lookup(self.Q, self.neg_item_id)\n        self.W = tf.Variable(tf.random_normal([self.num_item, self.num_factor * 2 ], stddev=0.01))\n        self.b = tf.Variable(tf.random_normal([self.num_item], stddev=0.01))\n        # vertical conv layer\n        # self.conv_v = tf.nn.conv2d(1, n_v, (L, 1))\n\n        self.fc1_dim_v = self.n_v * self.num_factor\n        self.fc1_dim_h = self.n_h * self.num_factor\n\n        self.target_prediction = self._forward(item_latent_factor, user_latent_factor, self.item_id)\n        self.negative_prediction = self._forward(item_latent_factor, user_latent_factor, self.neg_item_id)\n        self.test_prediction = self._forward(item_latent_factor, user_latent_factor, self.item_id_test)\n\n\n\n        self.loss = - tf.reduce_mean(tf.log(tf.sigmoid(self.target_prediction) + 1e-10)) - tf.reduce_mean(\n            tf.log(1 - tf.sigmoid(self.negative_prediction) + 1e-10)) + self.reg_rate * (\n            tf.nn.l2_loss(self.P) + tf.nn.l2_loss(self.Q) + tf.nn.l2_loss(self.V)+ tf.nn.l2_loss(self.W) + tf.nn.l2_loss(\n                self.b)) + tf.losses.get_regularization_loss()\n\n\n        self.optimizer = tf.train.AdamOptimizer(self.learning_rate).minimize(self.loss)\n\n        return self\n\n\n    def getUserParam(self, user_id):\n        params = self.sess.run([self.user_emb], feed_dict={self.user_id: user_id, self.item_seq: self.test_sequences[user_id, :]})\n        return params[0]\n\n    def getItemParam(self, item_id):\n        params = self.sess.run([self.w_items, self.b_items], feed_dict={self.item_id_test: item_id})\n        return np.squeeze(params[0]), np.squeeze(params[1])\n\n    def _forward(self, item_latent_factor, user_latent_factor, item_id):\n        # horizontal conv layer\n        lengths = [i + 1 for i in range(self.L)]\n        # self.conv_h = []\n        #\n        # for i in lengths:\n        #     self.conv_h.append(tf.nn.conv2d(1, n_h, (i, num_factor)))\n\n\n        # fully connected layer\n\n\n        out, out_h, out_v = None, None, None\n        if self.n_v:\n            # print(tf.shape(tf.Variable(tf.random_normal(shape=[self.L, 1, 1, self.n_v], stddev=0.1))))\n            # print(tf.expand_dims(item_latent_factor, 1))\n            # out_v = tf.squeeze(tf.nn.conv2d(input=tf.expand_dims(item_latent_factor,3),\n            #                                 filter=self._weight_variable(shape=[self.L, 1, 1, self.n_v]),\n            #                                 strides=[1,1,1,1],\n            #                                 padding=\'SAME\'), 2)\n            out_v = tf.squeeze(tf.layers.conv2d(inputs=tf.expand_dims(item_latent_factor, 3),\n                                                filters=self.n_v,\n                                                kernel_size=(self.L, 1),\n                                                padding=\'valid\',\n                                                kernel_initializer=tf.random_normal_initializer,\n                                                kernel_regularizer=tf.contrib.layers.l2_regularizer(\n                                                    scale=self.reg_rate),\n                                                data_format=\'channels_last\',\n                                                reuse=tf.AUTO_REUSE,\n                                                name=""Convv""), 1)\n            print("";;;;;;;;;;;;;;;;"")\n\n            out_v = tf.reshape(out_v, [-1, self.fc1_dim_v])\n            print(np.shape(out_v))  # (?, 200)\n        out_hs = list()\n        if self.n_h:\n            for i in lengths:\n                conv_out = tf.nn.relu(tf.squeeze(tf.layers.conv2d(inputs=tf.expand_dims(item_latent_factor, 3),\n                                                                  filters=self.n_h,\n                                                                  kernel_size=(i, self.num_factor),\n                                                                  padding=\'valid\',\n                                                                  kernel_initializer=tf.random_normal_initializer,\n                                                                  kernel_regularizer=tf.contrib.layers.l2_regularizer(\n                                                                      scale=self.reg_rate),\n                                                                  data_format=\'channels_last\',\n                                                                  reuse=tf.AUTO_REUSE,\n                                                                  name=""Convh"" + str(i)), 2))\n                # print(np.shape(conv_out))\n                # print(np.shape(conv_out)[1])\n                # print(tf.shape(conv_out))\n                # conv_out = tf.transpose(conv_out, [0,2,1])\n                pool_out = tf.squeeze(\n                    tf.layers.max_pooling1d(conv_out, [np.shape(conv_out)[1]], data_format=\'channels_last\',\n                                            padding=\'valid\', strides=1), 1)\n                print(np.shape(pool_out))  # (?, 16)\n                out_hs.append(pool_out)\n            out_h = tf.concat(out_hs, axis=1)\n            print(np.shape(out_h))  # (?, 80)\n\n        out = tf.concat(values=[out_v, out_h], axis=1)\n        print(np.shape(out))  # (?, 280)\n        # fc1_dim_in = self.fc1_dim_h + self.fc1_dim_v\n\n        # self.fc1 = tf.layers.dense(out, self.num_factor)\n        # w and b are item specific\n\n\n        self.w_items = tf.nn.embedding_lookup(self.W, item_id)\n        self.b_items = tf.nn.embedding_lookup(self.b, item_id)\n\n        z = tf.layers.dense(out, units=self.num_factor, activation=tf.nn.relu,\n                            kernel_initializer=tf.random_normal_initializer,\n                            kernel_regularizer=tf.contrib.layers.l2_regularizer(scale=self.reg_rate),\n                            name=""full"",\n                            reuse=tf.AUTO_REUSE)\n        print(np.shape(z))\n\n        # x = tf.concat(values=[z, user_latent_factor], axis=1)\n        # res = []\n        # for i in range(np.shape(w_items)[1]):\n        #     result = (x * w_items[:, i, :]) + b_items[:, i, 0]\n        #     res.append(result)\n\n\n        # x = tf.reshape(tf.tile(tf.concat(values=[z, user_latent_factor], axis=1),[1, np.shape(w_items)[1]]), [-1, np.shape(w_items)[1], self.num_factor * 2])\n\n        self.user_emb = x = tf.concat(values=[z, user_latent_factor], axis=1)\n        x_tmp = []\n        for i in range(np.shape(self.w_items)[1]):\n            x_tmp.append(x)\n        x = tf.stack(x_tmp)\n        print(np.shape(x))\n        x = tf.transpose(x, [1, 0, 2])\n\n        res = tf.reduce_sum(tf.multiply(x, self.w_items), 2) + self.b_items\n        print(""......"")\n        print(np.shape(res))\n\n        return res\n\n    def execute(self, train_data, test_data):\n        self.sequences = train_data.sequences.sequences\n        self.targets = train_data.sequences.targets\n        self.users = train_data.sequences.user_ids.reshape(-1, 1)\n\n        all_items = set(np.arange(self.num_item - 1) + 1)\n        self.x = []\n        for i, u in enumerate(self.users.squeeze()):\n            tar = set([int(t) for t in self.targets[i]])\n            seq = set([int(t) for t in self.sequences[i]])\n            self.x.append(list(all_items - tar))\n        self.x = np.array(self.x)\n        if not self.neg_items:\n            all_items = set(np.arange(self.num_item - 1) + 1)\n\n            train = train_data.tocsr()\n\n            for user, row in enumerate(train):\n                self.neg_items[user] = list(all_items - set(row.indices))\n\n        init = tf.global_variables_initializer()\n        self.sess.run(init)\n        for epoch in range(self.epochs):\n            if self.verbose:\n                print(""Epoch: %04d;"" % (epoch))\n            self.train(train_data)\n            if (epoch) % self.T == 0:\n                print(""Epoch: %04d; "" % (epoch), end=\'\')\n                self.test(test_data)\n\n    def train(self, train_data):\n\n        # print(users)\n\n        self.num_training = len(self.sequences)\n        self.total_batch = int(self.num_training / self.batch_size)\n        L, T = train_data.sequences.L, train_data.sequences.T\n        self.test_sequences = train_data.test_sequences.sequences\n        # print(self.test_sequences)\n        idxs = np.random.permutation(self.num_training)  # shuffled ordering\n\n        sequences_random = [i.tolist() for i in list(self.sequences[idxs])]\n        targets_random = list(self.targets[idxs])\n        users_random = [i[0] for i in list(self.users[idxs])]\n        self.x_random = list(self.x[idxs])\n        item_random_neg =  self._get_neg_items( self.users.squeeze(), train_data, self.num_neg * self.num_T)\n        # # train\n        for i in range(self.total_batch):\n            start_time = time.time()\n            batch_user = users_random[i * self.batch_size:(i + 1) * self.batch_size]\n            batch_seq = sequences_random[i * self.batch_size:(i + 1) * self.batch_size]\n            batch_item = targets_random[i * self.batch_size:(i + 1) * self.batch_size]\n            batch_item_neg = item_random_neg[i * self.batch_size:(i + 1) * self.batch_size]\n            # print(batch_item_neg)\n\n            _, loss = self.sess.run((self.optimizer, self.loss), feed_dict={self.user_id: batch_user,\n                                                                            self.item_seq: batch_seq,\n                                                                            self.item_id: batch_item,\n                                                                            self.neg_item_id: batch_item_neg,\n                                                                            self.isTrain: True})\n            #\n            if i % self.display_step == 0:\n                if self.verbose:\n                    print(""Index: %04d; cost= %.9f"" % (i + 1, np.mean(loss)))\n                    #             print(""one iteration: %s seconds."" % (time.time() - start_time))\n\n    def test(self, test_data):\n        # print(test_data.user_map)\n        self.test_data = dict()\n        test = test_data.tocsr()\n\n        for user, row in enumerate(test):\n            self.test_data[user] = set(row.indices)\n            # print(self.test_data[user])\n\n        self.test_users = []\n        for i in range(self.num_user):\n            self.test_users.append(i)\n        # print(self.test_users)\n        evaluate_caser(self)\n\n    def save(self, path):\n        saver = tf.train.Saver()\n        saver.save(self.sess, path)\n\n    def predict(self, user_id, item_id):\n        # print(user_id)\n        #print(self.test_sequences[user_id, :])\n        # user_id_2 = [i+1 for i in user_id]\n        item_id = [[i] for i in item_id]\n        # print(len(item_id))\n\n        return self.sess.run([self.test_prediction], feed_dict={self.user_id: user_id,\n                                                                self.item_seq: self.test_sequences[user_id, :],\n                                                                self.item_id_test: item_id})[0]\n\n    def _weight_variable(self, shape):\n        initial = tf.random_normal(shape, stddev=0.1)\n        return tf.Variable(initial)\n\n    def _bias_variable(self, shape):\n        initial = tf.constant(0.1, shape=shape)\n        return tf.Variable(initial)\n\n    def _get_neg_items(self, users, interactions, n):\n\n        # users = users.squeeze()\n        neg_items_samples = np.zeros((users.shape[0], n))\n\n        # if not self.neg_items:\n        #     all_items = set(np.arange(self.num_item - 1) + 1)\n        #     train = interactions.tocsr()\n        #\n        #     for user, row in enumerate(train):\n        #         self.neg_items[user] = list(all_items - set(row.indices))\n\n        for i, u in enumerate(users):\n            for j in range(n):\n                x = self.neg_items[u]\n                neg_items_samples[i, j] = x[np.random.randint(len(x))]\n\n        return neg_items_samples\n\n    def _get_neg_items_sbpr(self, users, interactions, n):\n        # print(""start sampling"")\n        # print(targets)\n        # users = users.squeeze()\n        neg_items_samples = np.zeros((users.shape[0], n))\n        # all_items = None\n        # if not self.neg_items:\n        #     all_items = set(np.arange(self.num_item - 1) + 1)\n        #     train = interactions.tocsr()\n        #\n        #     for user, row in enumerate(train):\n        #         self.neg_items[user] = list(all_items - set(row.indices))\n        print(len(users))\n        for i, u in enumerate(users):\n            for j in range(n):\n                # print(int(targets[i][0]))\n                neg_items_samples[i, j] = self.x_random[i][np.random.randint(len(self.x_random[i]))]\n        # print(""end sampling"")\n        return neg_items_samples\n\n\n\n\n'"
models/seq_rec/PRME.py,47,"b'#!/usr/bin/env python\n""""""Implementation of Caser.\nReference: Personalized Ranking Metric Embedding for Next New POI Recommendation, Shanshan Feng, IJCAI 2015.\n""""""\n\nimport tensorflow as tf\nimport time\nimport numpy as np\nfrom utils.evaluation.SeqRecMetrics import *\nimport numpy as np\n\nnp.set_printoptions(threshold=np.inf)\n__author__ = ""Shuai Zhang""\n__copyright__ = ""Copyright 2018, The DeepRec Project""\n\n__license__ = ""GPL""\n__version__ = ""1.0.0""\n__maintainer__ = ""Shuai Zhang""\n__email__ = ""cheungdaven@gmail.com""\n__status__ = ""Development""\n\n\nclass PRME():\n    def __init__(self, sess, num_user, num_item, learning_rate=0.001, reg_rate=1e-2, epoch=5000, batch_size=1000,\n                 show_time=False, T=1, display_step=1000, verbose=False):\n        self.learning_rate = learning_rate\n        self.epochs = epoch\n        self.batch_size = batch_size\n        self.reg_rate = reg_rate\n        self.sess = sess\n        self.num_user = num_user\n        self.num_item = num_item\n        self.show_time = show_time\n        self.verbose = verbose\n        self.T = T\n        self.display_step = display_step\n\n        self.neg_items = dict()\n        print(""PRME."")\n\n    def build_network(self, L, num_T, num_factor=100, num_neg=1):\n\n        self.L = L\n        self.num_T = num_T\n        self.num_factor = num_factor\n        self.num_neg = num_neg\n\n        self.user_id = tf.placeholder(dtype=tf.int32, shape=[None], name=\'user_id\')\n        self.item_seq = tf.placeholder(dtype=tf.int32, shape=[None, L], name=\'item_seq\')\n        self.item_id = tf.placeholder(dtype=tf.int32, shape=[None, self.num_T], name=\'item_id\')\n        self.item_id_test = tf.placeholder(dtype=tf.int32, shape=[None, 1], name=\'item_id_test\')\n        self.neg_item_id = tf.placeholder(dtype=tf.int32, shape=[None, self.num_T * self.num_neg], name=\'item_id_neg\')\n        self.isTrain = tf.placeholder(tf.bool, shape=())\n\n\n        print(np.shape(self.user_id))\n        self.P = tf.Variable(tf.random_normal([self.num_user, num_factor], stddev=0.001))\n        self.V = tf.Variable(tf.random_normal([self.num_user, num_factor * 1], stddev=0.001))\n        self.Q = tf.Variable(tf.random_normal([self.num_item, num_factor], stddev=0.001))\n        self.X = tf.Variable(tf.random_normal([self.num_item, num_factor], stddev=0.001))\n\n        user_latent_factor = tf.nn.embedding_lookup(self.P, self.user_id)\n        self.user_specific_bias = tf.nn.embedding_lookup(self.V, self.user_id)\n\n\n\n        self.target_prediction = self._distance(self.item_seq, user_latent_factor, self.item_id)\n        self.negative_prediction = self._distance(self.item_seq, user_latent_factor, self.neg_item_id)\n        self.test_prediction = self._distance(self.item_seq, user_latent_factor, self.item_id_test)\n\n        # - tf.reduce_sum(tf.log(tf.sigmoid(self.target_prediction - self.negative_prediction)) )\n        # - tf.reduce_mean(tf.log(tf.sigmoid(self.target_prediction) + 1e-10)) - tf.reduce_mean( tf.log( tf.sigmoid(1 - self.negative_prediction) + 1e-10))\n        self.loss = - tf.reduce_sum(tf.log(tf.sigmoid(- self.target_prediction + self.negative_prediction))) + 0.01 * (\n            tf.nn.l2_loss(self.P) + tf.nn.l2_loss(self.X) + tf.nn.l2_loss(\n                self.Q))  # + tf.losses.get_regularization_loss()\n        \'\'\'\n        self.loss =  tf.reduce_sum(tf.maximum(self.target_prediction - self.negative_prediction + 0.5, 0))   + self.reg_rate * ( tf.nn.l2_loss(self.P)   + tf.nn.l2_loss(self.V) +tf.nn.l2_loss(\n                self.b)) + tf.losses.get_regularization_loss()\n\n\n        self.loss = - tf.reduce_sum(tf.log(tf.sigmoid(self.target_prediction - self.negative_prediction)) ) + self.reg_rate * (\n            tf.nn.l2_loss(self.P) +    tf.nn.l2_loss(self.V) + tf.nn.l2_loss(\n                self.W) + tf.nn.l2_loss(\n                self.b)) + tf.losses.get_regularization_loss()\n\n\n        \'\'\'\n        norm_clip_value = 1\n        self.clip_P = tf.assign(self.P, tf.clip_by_norm(self.P, norm_clip_value, axes=[1]))\n        self.clip_Q = tf.assign(self.Q, tf.clip_by_norm(self.Q, norm_clip_value, axes=[1]))\n        self.clip_V = tf.assign(self.V, tf.clip_by_norm(self.V, norm_clip_value, axes=[1]))\n        self.clip_X = tf.assign(self.X, tf.clip_by_norm(self.X, norm_clip_value, axes=[1]))\n\n        # self.loss = tf.reduce_sum(tf.square( 1 - self.target_prediction) ) + tf.reduce_sum(tf.square( self.negative_prediction)) + self.reg_rate * (\n        #     tf.nn.l2_loss(self.P) + tf.nn.l2_loss(self.Q) + tf.nn.l2_loss(self.V)+ tf.nn.l2_loss(self.W) + tf.nn.l2_loss(\n        #         self.b)) + tf.losses.get_regularization_loss()\n\n        self.optimizer = tf.train.AdamOptimizer(self.learning_rate).minimize(self.loss)  # GradientDescentOptimizer\n\n        return self\n\n\n\n    def _distance(self, item_seq, user_latent_factor, item_id):\n\n        item_latent_factor = tf.nn.embedding_lookup(self.Q, item_seq)\n\n        out = tf.squeeze(item_latent_factor, 1)\n\n        w_items = tf.nn.embedding_lookup(self.X, item_id)\n        w_items_2 = tf.nn.embedding_lookup(self.Q, item_id)\n\n        x_tmp = []\n        for i in range(np.shape(w_items)[1]):\n            x_tmp.append(out)\n        x = tf.stack(x_tmp)\n        print(np.shape(x))\n        print(np.shape(w_items))\n        x = tf.transpose(x, [1, 0, 2])\n\n\n        u_tmp = []\n        for i in range(np.shape(w_items)[1]):\n            u_tmp.append(user_latent_factor)\n        u = tf.stack(u_tmp)\n        print(np.shape(u))\n        u = tf.transpose(u, [1, 0, 2])\n\n        res = 0.2 * tf.reduce_sum(tf.square(w_items - u), 2) + 0.8 * tf.reduce_sum(tf.square(x - w_items_2),2)\n\n        return tf.squeeze(res)\n\n\n    def prepare_data(self, train_data, test_data):\n        self.sequences = train_data.sequences.sequences\n        self.test_sequences = train_data.test_sequences.sequences\n        self.targets = train_data.sequences.targets\n        self.users = train_data.sequences.user_ids.reshape(-1, 1)\n        all_items = set(np.arange(self.num_item - 1) + 1)\n        self.all_items = all_items\n        # print(all_items) # from 1 to 1679\n        self.x = []\n        for i, u in enumerate(self.users.squeeze()):\n            tar = set([int(t) for t in self.targets[i]])\n            seq = set([int(t) for t in self.sequences[i]])\n            self.x.append(list(all_items - tar))\n        self.x = np.array(self.x)\n\n        self.test_data = dict()\n        test = test_data.tocsr()\n\n        for user, row in enumerate(test):\n            self.test_data[user] = list(set(row.indices))\n\n        if not self.neg_items:\n            # all_items = set(np.arange(self.num_item - 1) + 1)\n            train = train_data.tocsr()\n\n            for user, row in enumerate(train):\n                # print(user)\n                # print(row.indices)\n                # print(0 in row.indices)\n                self.neg_items[user] = list(all_items - set(row.indices))\n                # print(self.test_data[user][0] in self.neg_items[user])\n\n    def execute(self, train_data, test_data):\n        self.prepare_data(train_data, test_data)\n        init = tf.global_variables_initializer()\n        self.sess.run(init)\n        for epoch in range(self.epochs):\n            if self.verbose:\n                print(""Epoch: %04d;"" % (epoch))\n            self.train(train_data)\n            if (epoch) % self.T == 0:\n                print(""Epoch: %04d; "" % (epoch), end=\'\')\n                self.test(test_data)\n\n    def train(self, train_data):\n\n        # print(users)\n\n        self.num_training = len(self.sequences)\n        self.total_batch = int(self.num_training / self.batch_size)\n\n        # print(self.test_sequences)\n        idxs = np.random.permutation(\n            self.num_training)  # shuffled ordering np.random.choice(self.num_training, self.num_training, replace=True) #\n\n        sequences_random = [i.tolist() for i in list(self.sequences[idxs])]\n        targets_random = list(self.targets[idxs])\n        users_random = [i[0] for i in list(self.users[idxs])]\n        self.x_random = list(self.x[idxs])\n        item_random_neg = self._get_neg_items_sbpr(self.users.squeeze(), train_data, self.num_neg * self.num_T)\n\n        # # train\n        for i in range(self.total_batch):\n            start_time = time.time()\n            batch_user = users_random[i * self.batch_size:(i + 1) * self.batch_size]\n            batch_seq = sequences_random[i * self.batch_size:(i + 1) * self.batch_size]\n            batch_item = targets_random[i * self.batch_size:(i + 1) * self.batch_size]\n            batch_item_neg = item_random_neg[i * self.batch_size:(i + 1) * self.batch_size]\n            # print(batch_item_neg)\n\n            _, loss = self.sess.run((self.optimizer, self.loss), feed_dict={self.user_id: batch_user,\n                                                                            self.item_seq: batch_seq,\n                                                                            self.item_id: batch_item,\n                                                                            self.neg_item_id: batch_item_neg,\n                                                                            self.isTrain: True})\n            #\n            if i % self.display_step == 0:\n                if self.verbose:\n                    print(""Index: %04d; cost= %.9f"" % (i + 1, np.mean(loss)))\n                    #             print(""one iteration: %s seconds."" % (time.time() - start_time))\n\n    def test(self, test_data):\n        # print(test_data.user_map)\n\n        # print(self.test_data)\n        self.test_users = []\n        for i in range(self.num_user):\n            self.test_users.append(i)\n        # print(self.test_users)\n        evaluate(self)\n\n    def save(self, path):\n        saver = tf.train.Saver()\n        saver.save(self.sess, path)\n\n    def predict(self, user_id, item_id):\n        # print(user_id)\n        # print(len(self.test_sequences))\n        # print(self.test_sequences[user_id, :])\n        # user_id_2 = [i+1 for i in user_id]\n        item_id = [[i] for i in item_id]\n        # print(len(item_id))\n\n        return - self.sess.run([self.test_prediction], feed_dict={self.user_id: user_id,\n                                                                  self.item_seq: self.test_sequences[user_id, :],\n                                                                  self.item_id_test: item_id})[0]\n\n    def _weight_variable(self, shape):\n        initial = tf.random_normal(shape, stddev=0.1)\n        return tf.Variable(initial)\n\n    def _bias_variable(self, shape):\n        initial = tf.constant(0.1, shape=shape)\n        return tf.Variable(initial)\n\n\n\n    def _get_neg_items_sbpr(self, users, interactions, n):\n        # print(""start sampling"")\n        # print(targets)\n        # users = users.squeeze()\n        neg_items_samples = np.zeros((users.shape[0], n))\n        # all_items = None\n        # if not self.neg_items:\n        #     all_items = set(np.arange(self.num_item - 1) + 1)\n        #     train = interactions.tocsr()\n        #\n        #     for user, row in enumerate(train):\n        #         self.neg_items[user] = list(all_items - set(row.indices))\n        print(len(users))\n        for i, u in enumerate(users):\n            for j in range(n):\n                # print(int(targets[i][0]))\n                neg_items_samples[i, j] = self.x_random[i][np.random.randint(len(self.x_random[i]))]\n        # print(""end sampling"")\n        return neg_items_samples\n'"
models/seq_rec/gru4rec.py,0,b''
utils/config/config.py,0,b''
utils/evaluation/RankingMetrics.py,0,"b'#!/usr/bin/env python\n""""""\nEvaluation Metrics for Top N Recommendation\n""""""\n\nimport numpy as np\n\n__author__ = ""Shuai Zhang""\n__copyright__ = ""Copyright 2018, The DeepRec Project""\n\n__license__ = ""GPL""\n__version__ = ""1.0.0""\n__maintainer__ = ""Shuai Zhang""\n__email__ = ""cheungdaven@gmail.com""\n__status__ = ""Development""\n\nimport math\n\n\n# efficient version\ndef precision_recall_ndcg_at_k(k, rankedlist, test_matrix):\n    idcg_k = 0\n    dcg_k = 0\n    n_k = k if len(test_matrix) > k else len(test_matrix)\n    for i in range(n_k):\n        idcg_k += 1 / math.log(i + 2, 2)\n\n    b1 = rankedlist\n    b2 = test_matrix\n    s2 = set(b2)\n    hits = [(idx, val) for idx, val in enumerate(b1) if val in s2]\n    count = len(hits)\n\n    for c in range(count):\n        dcg_k += 1 / math.log(hits[c][0] + 2, 2)\n\n    return float(count / k), float(count / len(test_matrix)), float(dcg_k / idcg_k)\n\n\ndef map_mrr_ndcg(rankedlist, test_matrix):\n    ap = 0\n    map = 0\n    dcg = 0\n    idcg = 0\n    mrr = 0\n    for i in range(len(test_matrix)):\n        idcg += 1 / math.log(i + 2, 2)\n\n    b1 = rankedlist\n    b2 = test_matrix\n    s2 = set(b2)\n    hits = [(idx, val) for idx, val in enumerate(b1) if val in s2]\n    count = len(hits)\n\n    for c in range(count):\n        ap += (c + 1) / (hits[c][0] + 1)\n        dcg += 1 / math.log(hits[c][0] + 2, 2)\n\n    if count != 0:\n        mrr = 1 / (hits[0][0] + 1)\n\n    if count != 0:\n        map = ap / count\n\n    return map, mrr, float(dcg / idcg)\n\n\ndef evaluate(self):\n    pred_ratings_10 = {}\n    pred_ratings_5 = {}\n    pred_ratings = {}\n    ranked_list = {}\n    p_at_5 = []\n    p_at_10 = []\n    r_at_5 = []\n    r_at_10 = []\n    map = []\n    mrr = []\n    ndcg = []\n    ndcg_at_5 = []\n    ndcg_at_10 = []\n    for u in self.test_users:\n        user_ids = []\n        user_neg_items = self.neg_items[u]\n        item_ids = []\n        # scores = []\n        for j in user_neg_items:\n            item_ids.append(j)\n            user_ids.append(u)\n\n        scores = self.predict(user_ids, item_ids)\n        # print(type(scores))\n        # print(scores)\n        # print(np.shape(scores))\n        # print(ratings)\n        neg_item_index = list(zip(item_ids, scores))\n\n        ranked_list[u] = sorted(neg_item_index, key=lambda tup: tup[1], reverse=True)\n        pred_ratings[u] = [r[0] for r in ranked_list[u]]\n        pred_ratings_5[u] = pred_ratings[u][:5]\n        pred_ratings_10[u] = pred_ratings[u][:10]\n\n        p_5, r_5, ndcg_5 = precision_recall_ndcg_at_k(5, pred_ratings_5[u], self.test_data[u])\n        p_at_5.append(p_5)\n        r_at_5.append(r_5)\n        ndcg_at_5.append(ndcg_5)\n        p_10, r_10, ndcg_10 = precision_recall_ndcg_at_k(10, pred_ratings_10[u], self.test_data[u])\n        p_at_10.append(p_10)\n        r_at_10.append(r_10)\n        ndcg_at_10.append(ndcg_10)\n        map_u, mrr_u, ndcg_u = map_mrr_ndcg(pred_ratings[u], self.test_data[u])\n        map.append(map_u)\n        mrr.append(mrr_u)\n        ndcg.append(ndcg_u)\n\n    print(""------------------------"")\n    print(""precision@10:"" + str(np.mean(p_at_10)))\n    print(""recall@10:"" + str(np.mean(r_at_10)))\n    print(""precision@5:"" + str(np.mean(p_at_5)))\n    print(""recall@5:"" + str(np.mean(r_at_5)))\n    print(""map:"" + str(np.mean(map)))\n    print(""mrr:"" + str(np.mean(mrr)))\n    print(""ndcg:"" + str(np.mean(ndcg)))\n    print(""ndcg@5:"" + str(np.mean(ndcg_at_5)))\n    print(""ndcg@10:"" + str(np.mean(ndcg_at_10)))\n'"
utils/evaluation/RatingMetrics.py,0,"b'import numpy as np\n\n\ndef RMSE(error, num):\n    return np.sqrt(error / num)\n\n\ndef MAE(error_mae, num):\n    return (error_mae / num)\n'"
utils/evaluation/SeqRecMetrics.py,1,"b'#!/usr/bin/env python\n""""""\nEvaluation Metrics for Top N Recommendation\n""""""\n\nimport numpy as np\nimport time\nfrom numpy.linalg import norm\n\n__author__ = ""Shuai Zhang""\n__copyright__ = ""Copyright 2018, The DeepRec Project""\n\n__license__ = ""GPL""\n__version__ = ""1.0.0""\n__maintainer__ = ""Shuai Zhang""\n__email__ = ""cheungdaven@gmail.com""\n__status__ = ""Development""\n\n\nimport math\n\n# efficient version\ndef precision_recall_ndcg_at_k(k, rankedlist, test_matrix):\n    idcg_k = 0\n    dcg_k = 0\n    n_k = k if len(test_matrix) > k else len(test_matrix)\n    for i in range(n_k):\n        idcg_k += 1 / math.log(i + 2, 2)\n\n    b1 = rankedlist\n    b2 = test_matrix\n    s2 = set(b2)\n    hits = [ (idx, val) for idx, val in enumerate(b1) if val in s2]\n\n    count = len(hits)\n\n    count_test = len(test_matrix)\n\n\n    for c in range(count):\n        dcg_k += 1 / math.log(hits[c][0] + 2, 2)\n\n    return count, float(count / len(test_matrix)), float(dcg_k / idcg_k)\n\n# def hitratio(k, rankedlist, test_matrix)\n\ndef map_mrr_ndcg(rankedlist, test_matrix):\n    ap = 0\n    map = 0\n    dcg = 0\n    idcg = 0\n    mrr = 0\n    for i in range(len(test_matrix)):\n        idcg += 1 / math.log(i + 2, 2)\n\n    b1 = rankedlist\n    b2 = test_matrix\n    s2 = set(b2)\n    hits = [ (idx, val) for idx, val in enumerate(b1) if val in s2]\n    # for idx, vale in enumerate(b1):\n    #     print(idx, vale)\n    count = len(hits)\n\n\n    for c in range(count):\n        ap += (c+1) / (hits[c][0] + 1)\n        dcg += 1 / math.log(hits[c][0] + 2, 2)\n\n    if count != 0:\n        mrr = 1 / (hits[0][0] + 1)\n\n    if count != 0:\n        map = ap / count\n\n    max = len(b1) - 1\n    if count != 0:\n        count_test = max - hits[0][0]\n    else:\n        count_test = 0\n    auc = 1.0 * count_test /max\n    return auc, mrr, float(dcg / idcg)\n\n\ndef hitratio_at_k():\n    print("" test"")\n\ndef ndcg_at_k():\n    print(""test"")\n\n\ndef evaluate1(self):\n    pred_ratings_10 = {}\n    pred_ratings_50 = {}\n    pred_ratings = {}\n    ranked_list = {}\n    p_at_5 = []\n    hr_at_50 = []\n    r_at_5 = []\n    r_at_10 = []\n    map = []\n    mrr = []\n    auc = []\n    ndcg = []\n    ndcg_at_5 = []\n    ndcg_at_10 = []\n    start_time = time.time()\n    user_factors, seq_factors, user_factors_2, _ = self.getUserParam(self.test_users)\n    item_factors_1, item_factors_2, bias_item = self.getItemParam(np.expand_dims(np.arange(self.num_item), axis=1))\n\n    #\n    print(np.shape(user_factors))\n    # print(np.shape(seq_factors))\n    print(np.shape(item_factors_1))\n    # print(np.shape(item_factors_2))\n    # print(type(user_factors))\n    # print(type(seq_factors))\n    # print(type(item_factors_1))\n    # print(type(item_factors_2))\n\n    # print(np.shape(user_factors[:,None]- item_factors_1))\n    # print(np.shape(bias_item))\n    results =  - self.alpha * np.sum((user_factors[:,None]- item_factors_1)**2, axis=2) \\\n               - (1-  self.alpha) *  np.sum((seq_factors[:,None]  - item_factors_2)**2, axis=2)\n    #results =  -   np.sum((seq_factors[:,None] +  user_factors[:,None]- item_factors_2)**2, axis=2)\n\n    #- np.reshape(bias_item, [ np.shape(bias_item)[1], np.shape(bias_item)[0]])\n    # print(np.shape(results))\n    # print(time.time() - start_time)\n    for u in self.test_users:\n        user_ids = []\n        user_neg_items = self.neg_items[u]\n        item_ids = []\n        scores = []\n\n\n        for j in user_neg_items:\n            item_ids.append(j)\n            user_ids.append(u)\n\n\n            scores.append(results[u, j])\n\n\n        #scores = self.predict(user_ids, item_ids)\n        #print(np.shape(scores))\n        #print( scores)\n        #\n\n        # print(type(scores))\n        # print(scores)\n        # print(np.shape(scores))\n        # print(ratings)\n        neg_item_index = list(zip(item_ids, scores))\n\n        ranked_list[u] = sorted(neg_item_index, key=lambda tup: tup[1], reverse=True)\n\n        # print(ranked_list[u])\n        pred_ratings[u] = [r[0] for r in ranked_list[u]]\n        pred_ratings_50[u] = pred_ratings[u][:50]\n\n\n        hr, _, _ = precision_recall_ndcg_at_k(50, pred_ratings_50[u], self.test_data[u])\n        # if hr > 0:\n        #     print(u)\n        #     print(self.test_sequences[u, :])\n        #     print( self.test_data[u])\n        #     print(seq_weights[u])\n        auc_t, mrr_t, _ = map_mrr_ndcg(pred_ratings[u], self.test_data[u])\n\n        hr_at_50.append(hr)\n        mrr.append(mrr_t)\n        auc.append(auc_t)\n    print(np.sum(hr_at_50))\n    print(""------------------------"")\n    print(""HR@50:"" + str(np.mean(hr_at_50)))\n    print(""MRR:"" + str(np.mean(mrr)))\n    print(""AUC:"" + str(np.mean(auc)))\n\ndef evaluate_caser(self):\n    pred_ratings_10 = {}\n    pred_ratings_50 = {}\n    pred_ratings = {}\n    ranked_list = {}\n    hr_at_50 = []\n    p_at_10 = []\n    r_at_5 = []\n    r_at_10 = []\n    map = []\n    mrr = []\n    ndcg = []\n    ndcg_at_5 = []\n    ndcg_at_10 = []\n    all_users = np.arange(500)\n\n    user_factors = self.getUserParam(self.test_users)\n    item_factors, bias_item = self.getItemParam(np.expand_dims(np.arange(self.num_item), axis=1))\n\n    #\n\n    # print(np.shape(item_factors_1))\n    # print(np.shape(item_factors_2))\n    # print(type(user_factors))\n    # print(type(seq_factors))\n    # print(type(item_factors_1))\n    # print(type(item_factors_2))\n\n    # print(np.shape(user_factors[:,None]- item_factors_1))\n    # print( bias_item[0] )\n    #res = tf.reduce_sum(tf.multiply(x, self.w_items), 2) + self.b_items\n\n    # print(np.shape(bias_item))\n    # print(np.shape( np.dot(user_factors, item_factors.T)))\n    results = np.dot(user_factors, item_factors.T) + bias_item\n\n    # - self.alpha * np.sum((user_factors[:, None] - item_factors_1) ** 2, axis=2) \\\n    #       - (1 - self.alpha) * np.sum((seq_factors[:, None] - item_factors_2) ** 2, axis=2)\n\n    for u in self.test_users:#all_users:#\n        user_ids = []\n        user_neg_items = self.neg_items[u] # self.all_items\n        item_ids = []\n        scores = []\n        for j in user_neg_items:\n            item_ids.append(j)\n            user_ids.append(u)\n            scores.append(results[u, j])\n        #scores = self.predict(user_ids, item_ids)\n        # print(type(scores))\n        # print(scores)\n        # print(np.shape(scores))\n        # print(ratings)\n        neg_item_index = list(zip(item_ids, scores))\n\n        ranked_list[u] = sorted(neg_item_index, key=lambda tup: tup[1], reverse=True)\n\n        # print(ranked_list[u])\n        pred_ratings[u] = [r[0] for r in ranked_list[u]]\n        pred_ratings_50[u] = pred_ratings[u][:50]\n\n        hr, _, _ = precision_recall_ndcg_at_k(50, pred_ratings_50[u], self.test_data[u])\n        _, mrr_t, _ = map_mrr_ndcg(pred_ratings[u], self.test_data[u])\n\n        hr_at_50.append(hr)\n        mrr.append(mrr_t)\n    print(np.sum(hr_at_50))\n    print(""------------------------"")\n    print(""HR@50:"" + str(np.mean(hr_at_50)))\n    print(""MRR:"" + str(np.mean(mrr)))\n\ndef evaluate(self):\n    pred_ratings_10 = {}\n    pred_ratings_50 = {}\n    pred_ratings = {}\n    ranked_list = {}\n    hr_at_50 = []\n    p_at_10 = []\n    r_at_5 = []\n    r_at_10 = []\n    map = []\n    mrr = []\n    ndcg = []\n    ndcg_at_5 = []\n    ndcg_at_10 = []\n    all_users = np.arange(500)\n    for u in self.test_users:#all_users:#\n        user_ids = []\n        user_neg_items = self.neg_items[u] # self.all_items\n        item_ids = []\n        #scores = []\n        for j in user_neg_items:\n            item_ids.append(j)\n            user_ids.append(u)\n\n        scores = self.predict(user_ids, item_ids)\n        # print(type(scores))\n        # print(scores)\n        # print(np.shape(scores))\n        # print(ratings)\n        neg_item_index = list(zip(item_ids, scores))\n\n        ranked_list[u] = sorted(neg_item_index, key=lambda tup: tup[1], reverse=True)\n\n        # print(ranked_list[u])\n        pred_ratings[u] = [r[0] for r in ranked_list[u]]\n        pred_ratings_50[u] = pred_ratings[u][:50]\n\n        hr, _, _ = precision_recall_ndcg_at_k(50, pred_ratings_50[u], self.test_data[u])\n        _, mrr_t, _ = map_mrr_ndcg(pred_ratings[u], self.test_data[u])\n\n        hr_at_50.append(hr)\n        mrr.append(mrr_t)\n    print(np.sum(hr_at_50))\n    print(""------------------------"")\n    print(""HR@50:"" + str(np.mean(hr_at_50)))\n    print(""MRR:"" + str(np.mean(mrr)))\n'"
utils/evaluation/__init__.py,0,b''
utils/load_data/__init__.py,0,b''
utils/load_data/load_data_content.py,0,"b'import pandas as pd\nimport numpy as np\n\nfrom sklearn.model_selection import train_test_split\nfrom scipy.sparse import csr_matrix\n\nfeatures = {}\n\ndef load_data_fm(path=""""):\n\n    train_file = ""../Data/frappe/frappe.train.libfm""\n    test_file = ""../Data/frappe/frappe.test.libfm""\n\n\n    count_num_feature_field(train_file)\n    count_num_feature_field(test_file)\n    features_M = len(features)\n\n    train_data = read_data(train_file)\n    test_data = read_data(test_file)\n\n    return train_data, test_data, features_M\n\n\ndef count_num_feature_field(file):\n    f = open(file)\n    line = f.readline()\n    i = len(features)\n    while line:\n         elements = line.strip().split(\' \')\n         for e in elements[1:]:\n             if e not in features:\n                 features[e] = i\n                 i = i + 1\n         line = f.readline()\n    f.close()\n\ndef read_data(file):\n    f = open(file)\n    X = []\n    Y = []\n\n    line = f.readline()\n    while line:\n        elements = line.strip().split(\' \')\n        Y.append([float(elements[0])])\n        X.append([ features[e] for e in elements[1:]])\n        line = f.readline()\n    f.close()\n    Data_Dict = {}\n    Data_Dict[\'Y\'] = Y\n    Data_Dict[\'X\'] = X\n    return Data_Dict\n\n\n\n\n\n\n\n\n\n\n'"
utils/load_data/load_data_ranking.py,0,"b'import pandas as pd\nimport numpy as np\n\nfrom sklearn.model_selection import train_test_split\nfrom scipy.sparse import csr_matrix\n\n\ndef load_data_all(path=""../data/ml100k/movielens_100k.dat"", header=[\'user_id\', \'item_id\', \'rating\', \'time\'],\n                  test_size=0.2, sep=""\\t""):\n    df = pd.read_csv(path, sep=sep, names=header, engine=\'python\')\n\n    n_users = df.user_id.unique().shape[0]\n    n_items = df.item_id.unique().shape[0]\n\n    train_data, test_data = train_test_split(df, test_size=test_size)\n    train_data = pd.DataFrame(train_data)\n    test_data = pd.DataFrame(test_data)\n\n    train_row = []\n    train_col = []\n    train_rating = []\n\n    train_dict = {}\n    for line in train_data.itertuples():\n        u = line[1] - 1\n        i = line[2] - 1\n        train_dict[(u, i)] = 1\n\n    for u in range(n_users):\n        for i in range(n_items):\n            train_row.append(u)\n            train_col.append(i)\n            if (u, i) in train_dict.keys():\n                train_rating.append(1)\n            else:\n                train_rating.append(0)\n    train_matrix = csr_matrix((train_rating, (train_row, train_col)), shape=(n_users, n_items))\n    all_items = set(np.arange(n_items))\n\n    neg_items = {}\n    train_interaction_matrix = []\n    for u in range(n_users):\n        neg_items[u] = list(all_items - set(train_matrix.getrow(u).nonzero()[1]))\n        train_interaction_matrix.append(list(train_matrix.getrow(u).toarray()[0]))\n\n    test_row = []\n    test_col = []\n    test_rating = []\n    for line in test_data.itertuples():\n        test_row.append(line[1] - 1)\n        test_col.append(line[2] - 1)\n        test_rating.append(1)\n    test_matrix = csr_matrix((test_rating, (test_row, test_col)), shape=(n_users, n_items))\n\n    test_dict = {}\n    for u in range(n_users):\n        test_dict[u] = test_matrix.getrow(u).nonzero()[1]\n\n    print(""Load data finished. Number of users:"", n_users, ""Number of items:"", n_items)\n\n    return train_interaction_matrix, test_dict, n_users, n_items\n\n\ndef load_data_neg(path=""../data/ml100k/movielens_100k.dat"", header=[\'user_id\', \'item_id\', \'rating\', \'category\'],\n                  test_size=0.2, sep=""\\t""):\n    df = pd.read_csv(path, sep=sep, names=header, engine=\'python\')\n\n    n_users = df.user_id.unique().shape[0]\n    n_items = df.item_id.unique().shape[0]\n\n    train_data, test_data = train_test_split(df, test_size=test_size)\n    train_data = pd.DataFrame(train_data)\n    test_data = pd.DataFrame(test_data)\n\n    train_row = []\n    train_col = []\n    train_rating = []\n\n    for line in train_data.itertuples():\n        u = line[1] - 1\n        i = line[2] - 1\n        train_row.append(u)\n        train_col.append(i)\n        train_rating.append(1)\n    train_matrix = csr_matrix((train_rating, (train_row, train_col)), shape=(n_users, n_items))\n\n    # all_items = set(np.arange(n_items))\n    # neg_items = {}\n    # for u in range(n_users):\n    #     neg_items[u] = list(all_items - set(train_matrix.getrow(u).nonzero()[1]))\n\n    test_row = []\n    test_col = []\n    test_rating = []\n    for line in test_data.itertuples():\n        test_row.append(line[1] - 1)\n        test_col.append(line[2] - 1)\n        test_rating.append(1)\n    test_matrix = csr_matrix((test_rating, (test_row, test_col)), shape=(n_users, n_items))\n\n    test_dict = {}\n    for u in range(n_users):\n        test_dict[u] = test_matrix.getrow(u).nonzero()[1]\n\n    print(""Load data finished. Number of users:"", n_users, ""Number of items:"", n_items)\n    return train_matrix.todok(), test_dict, n_users, n_items\n\n\ndef load_data_separately(path_train=None, path_test=None, path_val=None, header=[\'user_id\', \'item_id\', \'rating\'],\n                         sep="" "", n_users=0, n_items=0):\n    n_users = n_users\n    n_items = n_items\n    print(""start"")\n    train_matrix = None\n    if path_train is not None:\n        train_data = pd.read_csv(path_train, sep=sep, names=header, engine=\'python\')\n        print(""Load data finished. Number of users:"", n_users, ""Number of items:"", n_items)\n\n        train_row = []\n        train_col = []\n        train_rating = []\n\n        for line in train_data.itertuples():\n            u = line[1]  # - 1\n            i = line[2]  # - 1\n            train_row.append(u)\n            train_col.append(i)\n            train_rating.append(1)\n\n        train_matrix = csr_matrix((train_rating, (train_row, train_col)), shape=(n_users, n_items))\n\n    print(""Load data finished. Number of users:"", n_users, ""Number of items:"", n_items)\n    test_dict = None\n    if path_test is not None:\n        test_data = pd.read_csv(path_test, sep=sep, names=header, engine=\'python\')\n        test_row = []\n        test_col = []\n        test_rating = []\n        for line in test_data.itertuples():\n            test_row.append(line[1])\n            i = line[2]  # - 1\n            test_col.append(i)\n            test_rating.append(1)\n\n        test_matrix = csr_matrix((test_rating, (test_row, test_col)), shape=(n_users, n_items))\n\n        test_dict = {}\n        for u in range(n_users):\n            test_dict[u] = test_matrix.getrow(u).nonzero()[1]\n    all_items = set(np.arange(n_items))\n    train_interaction_matrix = []\n    for u in range(n_users):\n        train_interaction_matrix.append(list(train_matrix.getrow(u).toarray()[0]))\n\n    if path_val is not None:\n        val_data = pd.read_csv(path_val, sep=sep, names=header, engine=\'python\')\n\n    print(""end"")\n    return train_interaction_matrix, test_dict, n_users, n_items\n'"
utils/load_data/load_data_rating.py,0,"b'import pandas as pd\nimport numpy as np\n\nfrom sklearn.model_selection import train_test_split\nfrom scipy.sparse import csr_matrix\n\n\ndef load_data_rating(path=""../data/ml100k/movielens_100k.dat"", header=[\'user_id\', \'item_id\', \'rating\', \'category\'],\n                     test_size=0.1, sep=""\\t""):\n    \'\'\'\n    Loading the data for rating prediction task\n    :param path: the path of the dataset, datasets should be in the CSV format\n    :param header: the header of the CSV format, the first three should be: user_id, item_id, rating\n    :param test_size: the test ratio, default 0.1\n    :param sep: the seperator for csv colunms, defalut space\n    :return:\n    \'\'\'\n\n    df = pd.read_csv(path, sep=sep, names=header, engine=\'python\')\n\n    n_users = df.user_id.unique().shape[0]\n    n_items = df.item_id.unique().shape[0]\n\n    train_data, test_data = train_test_split(df, test_size=test_size)\n    train_data = pd.DataFrame(train_data)\n    test_data = pd.DataFrame(test_data)\n\n    train_row = []\n    train_col = []\n    train_rating = []\n\n    for line in train_data.itertuples():\n        u = line[1] - 1\n        i = line[2] - 1\n        train_row.append(u)\n        train_col.append(i)\n        train_rating.append(line[3])\n    train_matrix = csr_matrix((train_rating, (train_row, train_col)), shape=(n_users, n_items))\n\n    test_row = []\n    test_col = []\n    test_rating = []\n    for line in test_data.itertuples():\n        test_row.append(line[1] - 1)\n        test_col.append(line[2] - 1)\n        test_rating.append(line[3])\n    test_matrix = csr_matrix((test_rating, (test_row, test_col)), shape=(n_users, n_items))\n    print(""Load data finished. Number of users:"", n_users, ""Number of items:"", n_items)\n    return train_matrix.todok(), test_matrix.todok(), n_users, n_items\n'"
utils/load_data/load_data_seq.py,0,"b'import pandas as pd\nimport numpy as np\n\nfrom sklearn.model_selection import train_test_split\nfrom scipy.sparse import csr_matrix\n\n\nclass DataSet():\n    class SeqData():\n\n        def __init__(self, user_ids, sequences, targets=None):\n            self.user_ids = user_ids\n            self.sequences = sequences\n            self.targets = targets\n            self.L = sequences.shape[1]\n            self.T = None\n\n            if np.any(targets):\n                self.T = targets.shape[1]\n\n    def __init__(self, path=""../../Data/ml1m/seq/train.txt"", header=[\'user\', \'item\', \'rating\'], sep="" "", seq_len=1,\n                 target_len=1, isTrain=False, user_map=None,\n                 item_map=None, num_users=None, num_items=None):\n        self.path = path\n        self.header = header\n        self.sep = sep\n        self.seq_len = seq_len\n        self.target_len = target_len\n        self.isTrain = isTrain\n\n        if not user_map and not item_map:\n            user_map = dict()\n            item_map = dict()\n\n            self.num_user = 0\n            self.num_item = 0\n        else:\n            self.num_user = len(user_map)\n            self.num_item = len(item_map)\n\n        # TODO: 1. remove cold start user with less than 5 items;\n\n        # TODO: 2.split the data into 70-10-20 based on the timestamp\n\n        df_train = pd.read_csv(self.path, sep=self.sep, names=self.header)\n        \'\'\'\n        if not num_users and not num_items:\n            n_users = df_train.user.unique().shape[0]\n            n_items = df_train.item.unique().shape[0]\n        else:\n            n_users = num_users #7390 #43117 #df_train.user.unique().shape[0]\n            n_items = num_items #10159#26018 #df_train.item.unique().shape[0]\n\n        print(""Load data finished. Number of users:"", n_users, ""Number of items:"", n_items)\n        \'\'\'\n        train_data = pd.DataFrame(df_train)\n\n        self.user_ids = list()\n        self.item_ids = list()\n\n        train_row = []\n        train_col = []\n        train_rating = []\n\n        for line in train_data.itertuples():\n            self.user_ids.append(line[1])\n            self.item_ids.append(line[2])\n\n        for u in self.user_ids:\n            if u not in user_map:\n                user_map[u] = self.num_user\n                self.num_user += 1\n\n        for i in self.item_ids:\n            if i not in item_map:\n                item_map[i] = self.num_item\n                self.num_item += 1\n        if num_users and num_items:\n            self.num_user = num_users\n            self.num_item = num_items\n        print(""....Load data finished. Number of users:"", self.num_user, ""Number of items:"", self.num_item)\n        self.user_map = user_map\n        self.item_map = item_map\n\n        self.user_ids = np.array([self.user_map[u] for u in self.user_ids])\n        self.item_ids = np.array([self.item_map[i] for i in self.item_ids])\n        print(len(self.item_ids))\n\n        if isTrain:\n            self.load_data_seq()\n            # else:\n            #    self.num_item += 1\n\n    def load_data_seq(self):\n        for k, v in self.item_map.items():\n            self.item_map[k] = v + 1\n        self.item_ids = self.item_ids + 1\n        self.num_item += 1\n\n        max_seq_len = self.seq_len + self.target_len\n\n        sort_indices = np.lexsort((self.user_ids,))\n\n        u_ids = self.user_ids[sort_indices]\n        i_ids = self.item_ids[sort_indices]\n\n        u_ids, indices, counts = np.unique(u_ids, return_index=True, return_counts=True)\n\n        num_subsequences = sum([c - max_seq_len + 1 if c >= max_seq_len else 1 for c in counts])\n\n        sequences = np.zeros((num_subsequences, self.seq_len))\n        sequences_targets = np.zeros((num_subsequences, self.target_len))\n\n        sequence_users = np.empty(num_subsequences)\n\n        test_sequences = np.zeros((self.num_user, self.seq_len))\n        test_users = np.empty(self.num_user)\n\n        _uid = None\n        # print(u_ids)\n        # print(len(i_ids))\n        for i, (uid, item_seq) in enumerate(self._generate_sequences(u_ids,\n                                                                     i_ids,\n                                                                     indices,\n                                                                     max_seq_len)):\n            if uid != _uid:\n                test_sequences[uid][:] = item_seq[-self.seq_len:]\n                test_users[uid] = uid\n                _uid = uid\n            sequences_targets[i][:] = item_seq[-self.target_len:]\n            sequences[i][:] = item_seq[:self.seq_len]\n            sequence_users[i] = uid\n\n        self.sequences = self.SeqData(sequence_users, sequences, sequences_targets)\n        self.test_sequences = self.SeqData(test_users, test_sequences)\n\n    # user_seq = []\n    # for i in range(len(indices)):\n    #     start_idx = indices[1]\n    #\n    #     if i >= len(indices) - 1:\n    #         stop_idx = None\n    #     else:\n    #         stop_idx = indices[ i + 1]\n    #\n    #     # seq = []\n    #     tensor = i_ids[start_idx:stop_idx]\n    #     if len(tensor) - max_seq_len >= 0:\n    #         for j in range(len(tensor), 0, -1):\n    #             if j - max_seq_len >= 0:\n    #                 user_seq.append((u_ids[i], tensor[j - max_seq_len:j]))\n    #             else:\n    #                 break\n    #     else:\n    #         user_seq.append((u_ids[i],tensor))\n    #\n    # _uid = None\n    # for i, (uid, item_seq) in enumerate(user_seq):\n    #     if uid != _uid:\n    #         test_sequences[uid][:] = item_seq[-sequence_len:]\n    #         test_users[uid] = uid\n    #         _uid = uid\n    #     sequence_targets[i][:] = item_seq[-target_len:]\n    #     sequences[i][:] = item_seq[:sequence_len]\n    #     sequence_users[i] = uid\n\n\n\n    def _sliding_window(self, tensor, window_size, step_size=1):\n        if len(tensor) - window_size >= 0:\n            for i in range(len(tensor), 0, -step_size):\n                if i - window_size >= 0:\n                    yield tensor[i - window_size:i]\n                else:\n                    break\n        else:\n            yield tensor\n\n    def _generate_sequences(self, user_ids, item_ids,\n                            indices,\n                            max_sequence_length):\n        for i in range(len(indices)):\n\n            start_idx = indices[i]\n\n            if i >= len(indices) - 1:\n                stop_idx = None\n            else:\n                stop_idx = indices[i + 1]\n\n            for seq in self._sliding_window(item_ids[start_idx:stop_idx],\n                                            max_sequence_length):\n                yield (user_ids[i], seq)\n\n    def tocsr(self):\n\n        row = self.user_ids\n        col = self.item_ids\n        data = np.ones(len(row))\n\n        return csr_matrix((data, (row, col)), shape=(self.num_user, self.num_item))\n\n\n\n\n\n        #\n        # for u in range(n_users):\n        #     for i in range(n_items):\n        #         train_row.append(u)\n        #         train_col.append(i)\n        #         if (u, i) in train_dict.keys():\n        #             train_rating.append(1)\n        #         else:\n        #             train_rating.append(0)\n        #\n        # all_items = set(np.arange(n_items))\n        #\n        # neg_items = {}\n        # train_interaction_matrix = []\n        # for u in range(n_users):\n        #     neg_items[u] = list(all_items - set(train_matrix.getrow(u).nonzero()[1]))\n        #     train_interaction_matrix.append(list(train_matrix.getrow(u).toarray()[0]))\n        #\n        # test_row = []\n        # test_col = []\n        # test_rating = []\n        # for line in test_data.itertuples():\n        #     test_row.append(line[1] - 1)\n        #     test_col.append(line[2] - 1)\n        #     test_rating.append(1)\n        # test_matrix = csr_matrix((test_rating, (test_row, test_col)), shape=(n_users, n_items))\n        #\n        # test_dict = {}\n        # for u in range(n_users):\n        #     test_dict[u] = test_matrix.getrow(u).nonzero()[1]\n        #\n        # print(""Load data finished. Number of users:"", n_users, ""Number of items:"", n_items)\n        #\n        #\n        #\n        # num_subsequences = sum([ c - max])\n        #\n        # return train_interaction_matrix, test_dict, n_users, n_items\n\n# def remove_cold_start_user():\n#     print()\n#\n#\n# def train_valid_test_split(time_order=True):\n#     # if time_order:\n#\n#     print()\n\n# if __name__ == \'__main__\':\n#\n\ndef data_preprocess(path, path_save, sep=""\\t"", header = [\'user_id\', \'item_id\', \'rating\', \'timestampe\']):\n\n    #TODO: leave the recent one for test, seperately the data into two parts.\n    df = pd.read_csv(path, sep=sep, names=header, engine=\'python\')\n    test_items = {}\n    n_users = df.user_id.unique().shape[0]  # 943  # 6040 #.user_id.unique().shape[0]\n    n_items = df.item_id.unique().shape[0]  # 1682 # 3952 ##df.item_id.unique().shape[0]\n    print(""Number of users: %d; Number of items: %d;"" % (n_users, n_items))\n    train_items = {}\n    user_set = set()\n    for line in df.itertuples():\n        u = line[1]\n        i = line[2]\n        user_set.add(u)\n        train_items.setdefault(u, []).append((u, i, line[3],line[4]))\n        if u not in test_items:\n            test_items[u] = (i, line[3], line[4])\n        else:\n            if test_items[u][2] < line[4]:\n                test_items[u] = (i, line[3], line[4])\n\n\n    test_data = [(key, value[0], value[1],  value[2]) for key, value in test_items.items()]\n    test_data_map = {}\n    for i in range(len(test_data)):\n        test_data_map[test_data[i][0]] = test_data[i]\n\n    test_file = open(path_save+""test.dat"", \'a\', encoding=\'utf-8\')\n    test_writer = csv.writer(test_file, delimiter=\'\\t\', lineterminator=\'\\n\', quoting=csv.QUOTE_MINIMAL)\n    for i in test_data:\n        #test_writer.writerow([i[0] - 1 , i[1]-1 ,   i[2]])\n        test_writer.writerow([i[0]  , i[1] , i[2],  i[3]])\n\n    train_file = open(path_save+""train.dat"", \'a\', encoding=\'utf-8\')\n    train_writer = csv.writer(train_file, delimiter=\'\\t\', lineterminator=\'\\n\', quoting=csv.QUOTE_MINIMAL)\n\n    for u in user_set:\n        sorted_items = sorted(train_items[u ], key=lambda tup: tup[3], reverse=False)\n        #print(sorted_items)\n\n        for i in sorted_items:\n            #print(test_data[u])\n            #print(sorted_items[i])\n            #print(u)\n            if i != test_data_map[u]:\n                train_writer.writerow([u , i[1], i[2], i[3]])\n'"
utils/log/Log.py,0,b''
