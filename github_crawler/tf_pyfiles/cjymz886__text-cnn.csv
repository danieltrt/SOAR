file_path,api_count,code
loader.py,0,"b'#encoding:utf-8\nfrom collections import  Counter\nimport tensorflow.contrib.keras as kr\nimport numpy as np\nimport codecs\nimport re\nimport jieba\n\n\ndef read_file(filename):\n    """"""\n    Args:\n        filename:trian_filename,test_filename,val_filename \n    Returns:\n        two list where the first is lables and the second is contents cut by jieba\n        \n    """"""\n    re_han = re.compile(u""([\\u4E00-\\u9FD5a-zA-Z]+)"")  # the method of cutting text by punctuation\n    contents,labels=[],[]\n    with codecs.open(filename,\'r\',encoding=\'utf-8\') as f:\n        for line in f:\n            try:\n                line=line.rstrip()\n                assert len(line.split(\'\\t\'))==2\n                label,content=line.split(\'\\t\')\n                labels.append(label)\n                blocks = re_han.split(content)\n                word = []\n                for blk in blocks:\n                    if re_han.match(blk):\n                        for w in jieba.cut(blk):\n                            if len(w)>=2:\n                                word.append(w)\n                contents.append(word)\n            except:\n                pass\n    return labels,contents\n\ndef build_vocab(filenames,vocab_dir,vocab_size=8000):\n    """"""\n    Args:\n        filename:trian_filename,test_filename,val_filename\n        vocab_dir:path of vocab_filename\n        vocab_size:number of vocabulary\n    Returns:\n        writting vocab to vocab_filename\n\n    """"""\n    all_data = []\n    for filename in filenames:\n        _,data_train=read_file(filename)\n        for content in data_train:\n            all_data.extend(content)\n    counter=Counter(all_data)\n    count_pairs=counter.most_common(vocab_size-1)\n    words,_=list(zip(*count_pairs))\n    words=[\'<PAD>\']+list(words)\n\n    with codecs.open(vocab_dir,\'w\',encoding=\'utf-8\') as f:\n        f.write(\'\\n\'.join(words)+\'\\n\')\n\ndef read_vocab(vocab_dir):\n    """"""\n    Args:\n        filename:path of vocab_filename\n    Returns:\n        words: a list of vocab\n        word_to_id: a dict of word to id\n        \n    """"""\n    words=codecs.open(vocab_dir,\'r\',encoding=\'utf-8\').read().strip().split(\'\\n\')\n    word_to_id=dict(zip(words,range(len(words))))\n    return words,word_to_id\n\ndef read_category():\n    """"""\n    Args:\n        None\n    Returns:\n        categories: a list of label\n        cat_to_id: a dict of label to id\n\n    """"""\n    categories = [\'\xe4\xbd\x93\xe8\x82\xb2\', \'\xe8\xb4\xa2\xe7\xbb\x8f\', \'\xe6\x88\xbf\xe4\xba\xa7\', \'\xe5\xae\xb6\xe5\xb1\x85\', \'\xe6\x95\x99\xe8\x82\xb2\', \'\xe7\xa7\x91\xe6\x8a\x80\', \'\xe6\x97\xb6\xe5\xb0\x9a\', \'\xe6\x97\xb6\xe6\x94\xbf\', \'\xe6\xb8\xb8\xe6\x88\x8f\', \'\xe5\xa8\xb1\xe4\xb9\x90\']\n    cat_to_id=dict(zip(categories,range(len(categories))))\n    return categories,cat_to_id\n\ndef process_file(filename,word_to_id,cat_to_id,max_length=600):\n    """"""\n    Args:\n        filename:train_filename or test_filename or val_filename\n        word_to_id:get from def read_vocab()\n        cat_to_id:get from def read_category()\n        max_length:allow max length of sentence \n    Returns:\n        x_pad: sequence data from  preprocessing sentence \n        y_pad: sequence data from preprocessing label\n\n    """"""\n    labels,contents=read_file(filename)\n    data_id,label_id=[],[]\n    for i in range(len(contents)):\n        data_id.append([word_to_id[x] for x in contents[i] if x in word_to_id])\n        label_id.append(cat_to_id[labels[i]])\n    x_pad=kr.preprocessing.sequence.pad_sequences(data_id,max_length,padding=\'post\', truncating=\'post\')\n    y_pad=kr.utils.to_categorical(label_id)\n    return x_pad,y_pad\n\ndef batch_iter(x,y,batch_size=64):\n    """"""\n    Args:\n        x: x_pad get from def process_file()\n        y:y_pad get from def process_file()\n    Yield:\n        input_x,input_y by batch size\n\n    """"""\n\n    data_len=len(x)\n    num_batch=int((data_len-1)/batch_size)+1\n\n    indices=np.random.permutation(np.arange(data_len))\n    x_shuffle=x[indices]\n    y_shuffle=y[indices]\n\n    for i in range(num_batch):\n        start_id=i*batch_size\n        end_id=min((i+1)*batch_size,data_len)\n        yield x_shuffle[start_id:end_id],y_shuffle[start_id:end_id]\n\ndef export_word2vec_vectors(vocab, word2vec_dir,trimmed_filename):\n    """"""\n    Args:\n        vocab: word_to_id \n        word2vec_dir:file path of have trained word vector by word2vec\n        trimmed_filename:file path of changing word_vector to numpy file\n    Returns:\n        save vocab_vector to numpy file\n        \n    """"""\n    file_r = codecs.open(word2vec_dir, \'r\', encoding=\'utf-8\')\n    line = file_r.readline()\n    voc_size, vec_dim = map(int, line.split(\' \'))\n    embeddings = np.zeros([len(vocab), vec_dim])\n    line = file_r.readline()\n    while line:\n        try:\n            items = line.split(\' \')\n            word = items[0]\n            vec = np.asarray(items[1:], dtype=\'float32\')\n            if word in vocab:\n                word_idx = vocab[word]\n                embeddings[word_idx] = np.asarray(vec)\n        except:\n            pass\n        line = file_r.readline()\n    np.savez_compressed(trimmed_filename, embeddings=embeddings)\n\ndef get_training_word2vec_vectors(filename):\n    """"""\n    Args:\n        filename:numpy file\n    Returns:\n        data[""embeddings""]: a matrix of vocab vector\n    """"""\n    with np.load(filename) as data:\n        return data[""embeddings""]\n'"
text_model.py,40,"b'#encoding:utf-8\nimport  tensorflow as tf\n\nclass TextConfig():\n\n    embedding_size=100     #dimension of word embedding\n    vocab_size=8000        #number of vocabulary\n    pre_trianing = None   #use vector_char trained by word2vec\n\n    seq_length=600         #max length of sentence\n    num_classes=10         #number of labels\n\n    num_filters=128        #number of convolution kernel\n    filter_sizes=[2,3,4]   #size of convolution kernel\n\n\n    keep_prob=0.5          #droppout\n    lr= 1e-3               #learning rate\n    lr_decay= 0.9          #learning rate decay\n    clip= 6.0              #gradient clipping threshold\n    l2_reg_lambda=0.01     #l2 regularization lambda\n\n    num_epochs=10          #epochs\n    batch_size=64         #batch_size\n    print_per_batch =100   #print result\n\n    train_filename=\'./data/cnews.train.txt\'  #train data\n    test_filename=\'./data/cnews.test.txt\'    #test data\n    val_filename=\'./data/cnews.val.txt\'      #validation data\n    vocab_filename=\'./data/vocab.txt\'        #vocabulary\n    vector_word_filename=\'./data/vector_word.txt\'  #vector_word trained by word2vec\n    vector_word_npz=\'./data/vector_word.npz\'   # save vector_word to numpy file\n\nclass TextCNN(object):\n\n    def __init__(self,config):\n\n        self.config=config\n\n        self.input_x=tf.placeholder(tf.int32,shape=[None,self.config.seq_length],name=\'input_x\')\n        self.input_y=tf.placeholder(tf.float32,shape=[None,self.config.num_classes],name=\'input_y\')\n        self.keep_prob=tf.placeholder(tf.float32,name=\'dropout\')\n        self.global_step = tf.Variable(0, trainable=False, name=\'global_step\')\n        self.l2_loss = tf.constant(0.0)\n\n        self.cnn()\n    def cnn(self):\n\n        with tf.device(\'/cpu:0\'):\n            self.embedding = tf.get_variable(""embeddings"", shape=[self.config.vocab_size, self.config.embedding_size],\n                                             initializer=tf.constant_initializer(self.config.pre_trianing))\n            self.embedding_inputs= tf.nn.embedding_lookup(self.embedding, self.input_x)\n            self.embedding_inputs_expanded = tf.expand_dims(self.embedding_inputs, -1)\n\n        with tf.name_scope(\'cnn\'):\n            pooled_outputs = []\n            for i, filter_size in enumerate(self.config.filter_sizes):\n                with tf.name_scope(""conv-maxpool-%s"" % filter_size):\n\n                    filter_shape = [filter_size, self.config.embedding_size, 1, self.config.num_filters]\n                    W = tf.Variable(tf.truncated_normal(filter_shape, stddev=0.1), name=""W"")\n                    b = tf.Variable(tf.constant(0.1, shape=[self.config.num_filters]), name=""b"")\n                    conv = tf.nn.conv2d(\n                        self.embedding_inputs_expanded,\n                        W,\n                        strides=[1, 1, 1, 1],\n                        padding=""VALID"",\n                        name=""conv"")\n                    h = tf.nn.relu(tf.nn.bias_add(conv, b), name=""relu"")\n                    pooled = tf.nn.max_pool(\n                        h,\n                        ksize=[1, self.config.seq_length - filter_size + 1, 1, 1],\n                        strides=[1, 1, 1, 1],\n                        padding=\'VALID\',\n                        name=""pool"")\n                    pooled_outputs.append(pooled)\n\n            num_filters_total = self.config.num_filters * len(self.config.filter_sizes)\n            self.h_pool = tf.concat(pooled_outputs, 3)\n            self.outputs= tf.reshape(self.h_pool, [-1, num_filters_total])\n\n\n        with tf.name_scope(""dropout""):\n            self.final_output = tf.nn.dropout(self.outputs, self.keep_prob)\n\n        with tf.name_scope(\'output\'):\n            fc_w = tf.get_variable(\'fc_w\', shape=[self.final_output.shape[1].value, self.config.num_classes],\n                                   initializer=tf.contrib.layers.xavier_initializer())\n            fc_b = tf.Variable(tf.constant(0.1, shape=[self.config.num_classes]), name=\'fc_b\')\n            self.logits = tf.matmul(self.final_output, fc_w) + fc_b\n            self.prob=tf.nn.softmax(self.logits)\n            self.y_pred_cls = tf.argmax(self.logits, 1, name=\'predictions\')\n\n        with tf.name_scope(\'loss\'):\n            cross_entropy = tf.nn.softmax_cross_entropy_with_logits(logits=self.logits, labels=self.input_y)\n            self.l2_loss += tf.nn.l2_loss(fc_w)\n            self.l2_loss += tf.nn.l2_loss(fc_b)\n            self.loss = tf.reduce_mean(cross_entropy) + self.config.l2_reg_lambda * self.l2_loss\n            self.loss = tf.reduce_mean(cross_entropy)\n\n        with tf.name_scope(\'optimizer\'):\n            optimizer = tf.train.AdamOptimizer(self.config.lr)\n            gradients, variables = zip(*optimizer.compute_gradients(self.loss))\n            gradients, _ = tf.clip_by_global_norm(gradients, self.config.clip)\n            self.optim = optimizer.apply_gradients(zip(gradients, variables), global_step=self.global_step)\n\n        with tf.name_scope(\'accuracy\'):\n            correct_pred=tf.equal(tf.argmax(self.input_y,1),self.y_pred_cls)\n            self.acc=tf.reduce_mean(tf.cast(correct_pred,tf.float32))\n\n\n'"
text_predict.py,4,"b'#encoding:utf-8\nfrom text_model import *\nimport  tensorflow as tf\nimport tensorflow.contrib.keras as kr\nimport os\nimport numpy as np\nimport jieba\nimport re\nimport heapq\nimport codecs\n\n\n\ndef predict(sentences):\n    config = TextConfig()\n    config.pre_trianing = get_training_word2vec_vectors(config.vector_word_npz)\n    model = TextCNN(config)\n    save_dir = \'./checkpoints/textcnn\'\n    save_path = os.path.join(save_dir, \'best_validation\')\n\n    _,word_to_id=read_vocab(config.vocab_filename)\n    input_x= process_file(sentences,word_to_id,max_length=config.seq_length)\n    labels = {0:\'\xe4\xbd\x93\xe8\x82\xb2\',\n              1:\'\xe8\xb4\xa2\xe7\xbb\x8f\',\n              2:\'\xe6\x88\xbf\xe4\xba\xa7\',\n              3:\'\xe5\xae\xb6\xe5\xb1\x85\',\n              4:\'\xe6\x95\x99\xe8\x82\xb2\',\n              5:\'\xe7\xa7\x91\xe6\x8a\x80\',\n              6:\'\xe6\x97\xb6\xe5\xb0\x9a\',\n              7:\'\xe6\x97\xb6\xe6\x94\xbf\',\n              8:\'\xe6\xb8\xb8\xe6\x88\x8f\',\n              9:\'\xe5\xa8\xb1\xe4\xb9\x90\'\n              }\n\n    feed_dict = {\n        model.input_x: input_x,\n        model.keep_prob: 1,\n    }\n    session = tf.Session()\n    session.run(tf.global_variables_initializer())\n    saver = tf.train.Saver()\n    saver.restore(sess=session, save_path=save_path)\n    y_prob=session.run(model.prob, feed_dict=feed_dict)\n    y_prob=y_prob.tolist()\n    cat=[]\n    for prob in y_prob:\n        top2= list(map(prob.index, heapq.nlargest(1, prob)))\n        cat.append(labels[top2[0]])\n    tf.reset_default_graph()\n    return  cat\n\ndef sentence_cut(sentences):\n    """"""\n    Args:\n        sentence: a list of text need to segment\n    Returns:\n        seglist:  a list of sentence cut by jieba \n\n    """"""\n    re_han = re.compile(u""([\\u4E00-\\u9FD5a-zA-Z0-9+#&\\._%]+)"")  # the method of cutting text by punctuation\n    seglist=[]\n    for sentence in sentences:\n        words=[]\n        blocks = re_han.split(sentence)\n        for blk in blocks:\n            if re_han.match(blk):\n                words.extend(jieba.lcut(blk))\n        seglist.append(words)\n    return  seglist\n\n\ndef process_file(sentences,word_to_id,max_length=600):\n    """"""\n    Args:\n        sentence: a text need to predict\n        word_to_id:get from def read_vocab()\n        max_length:allow max length of sentence \n    Returns:\n        x_pad: sequence data from  preprocessing sentence \n\n    """"""\n    data_id=[]\n    seglist=sentence_cut(sentences)\n    for i in range(len(seglist)):\n        data_id.append([word_to_id[x] for x in seglist[i] if x in word_to_id])\n    x_pad=kr.preprocessing.sequence.pad_sequences(data_id,max_length)\n    return x_pad\n\n\ndef read_vocab(vocab_dir):\n    """"""\n    Args:\n        filename:path of vocab_filename\n    Returns:\n        words: a list of vocab\n        word_to_id: a dict of word to id\n\n    """"""\n    words = codecs.open(vocab_dir, \'r\', encoding=\'utf-8\').read().strip().split(\'\\n\')\n    word_to_id = dict(zip(words, range(len(words))))\n    return words, word_to_id\n\ndef get_training_word2vec_vectors(filename):\n    """"""\n    Args:\n        filename:numpy file\n    Returns:\n        data[""embeddings""]: a matrix of vocab vector\n    """"""\n    with np.load(filename) as data:\n        return data[""embeddings""]\n\nif __name__ == \'__main__\':\n    print(\'predict random five samples in test data.... \')\n    import random\n    sentences=[]\n    labels=[]\n    with codecs.open(\'./data/cnews.test.txt\',\'r\',encoding=\'utf-8\') as f:\n        sample=random.sample(f.readlines(),5)\n        for line in sample:\n            try:\n                line=line.rstrip().split(\'\\t\')\n                assert len(line)==2\n                sentences.append(line[1])\n                labels.append(line[0])\n            except:\n                pass\n    cat=predict(sentences)\n    for i,sentence in enumerate(sentences,0):\n        print (\'----------------------the text-------------------------\')\n        print (sentence[:50]+\'....\')\n        print(\'the orginal label:%s\'%labels[i])\n        print(\'the predict label:%s\'%cat[i])\n\n'"
text_test.py,3,"b'#encoding:utf-8\nfrom __future__ import print_function\nfrom text_model import *\nfrom loader import *\nfrom sklearn import metrics\nimport sys\nimport os\nimport time\nfrom datetime import timedelta\n\n\ndef evaluate(sess, x_, y_):\n    data_len = len(x_)\n    batch_eval = batch_iter(x_, y_, 128)\n    total_loss = 0.0\n    total_acc = 0.0\n    for x_batch, y_batch in batch_eval:\n        batch_len = len(x_batch)\n        feed_dict = feed_data(x_batch, y_batch, 1.0)\n        loss, acc = sess.run([model.loss, model.acc], feed_dict=feed_dict)\n        total_loss += loss * batch_len\n        total_acc += acc * batch_len\n\n    return total_loss / data_len, total_acc / data_len\n\ndef feed_data(x_batch, y_batch, keep_prob):\n    feed_dict = {\n        model.input_x: x_batch,\n        model.input_y: y_batch,\n        model.keep_prob:keep_prob\n    }\n    return feed_dict\n\ndef test():\n    print(""Loading test data..."")\n    t1=time.time()\n    x_test,y_test=process_file(config.test_filename,word_to_id,cat_to_id,config.seq_length)\n\n    session=tf.Session()\n    session.run(tf.global_variables_initializer())\n    saver=tf.train.Saver()\n    saver.restore(sess=session,save_path=save_path)\n\n    print(\'Testing...\')\n    test_loss,test_accuracy = evaluate(session,x_test,y_test)\n    msg = \'Test Loss: {0:>6.2}, Test Acc: {1:>7.2%}\'\n    print(msg.format(test_loss, test_accuracy))\n\n    batch_size=config.batch_size\n    data_len=len(x_test)\n    num_batch=int((data_len-1)/batch_size)+1\n    y_test_cls=np.argmax(y_test,1)\n    y_pred_cls=np.zeros(shape=len(x_test),dtype=np.int32)\n\n    for i in range(num_batch):\n        start_id=i*batch_size\n        end_id=min((i+1)*batch_size,data_len)\n        feed_dict={\n            model.input_x:x_test[start_id:end_id],\n            model.keep_prob:1.0,\n        }\n        y_pred_cls[start_id:end_id]=session.run(model.y_pred_cls,feed_dict=feed_dict)\n\n    #evaluate\n    print(""Precision, Recall and F1-Score..."")\n    print(metrics.classification_report(y_test_cls, y_pred_cls, target_names=categories))\n\n    print(""Confusion Matrix..."")\n    cm = metrics.confusion_matrix(y_test_cls, y_pred_cls)\n    print(cm)\n\n    print(""Time usage:%.3f seconds...\\n""%(time.time() - t1))\n\nif __name__ == \'__main__\':\n    print(\'Configuring CNN model...\')\n    config = TextConfig()\n    filenames = [config.train_filename, config.test_filename, config.val_filename]\n    if not os.path.exists(config.vocab_filename):\n        build_vocab(filenames, config.vocab_filename, config.vocab_size)\n    #read vocab and categories\n    categories,cat_to_id = read_category()\n    words,word_to_id = read_vocab(config.vocab_filename)\n    config.vocab_size = len(words)\n\n    # trans vector file to numpy file\n    if not os.path.exists(config.vector_word_npz):\n        export_word2vec_vectors(word_to_id, config.vector_word_filename, config.vector_word_npz)\n    config.pre_trianing = get_training_word2vec_vectors(config.vector_word_npz)\n    model = TextCNN(config)\n\n    save_dir = \'./checkpoints/textcnn\'\n    save_path = os.path.join(save_dir, \'best_validation\')\n    test()'"
text_train.py,7,"b'#encoding:utf-8\nfrom __future__ import print_function\nfrom text_model import *\nfrom loader import *\nfrom sklearn import metrics\nimport sys\nimport os\nimport time\nfrom datetime import timedelta\n\n\n\n\ndef evaluate(sess, x_, y_):\n    data_len = len(x_)\n    batch_eval = batch_iter(x_, y_, 128)\n    total_loss = 0.0\n    total_acc = 0.0\n    for x_batch, y_batch in batch_eval:\n        batch_len = len(x_batch)\n        feed_dict = feed_data(x_batch, y_batch, 1.0)\n        loss, acc = sess.run([model.loss, model.acc], feed_dict=feed_dict)\n        total_loss += loss * batch_len\n        total_acc += acc * batch_len\n\n    return total_loss / data_len, total_acc / data_len\n\n\n\ndef feed_data(x_batch, y_batch, keep_prob):\n    feed_dict = {\n        model.input_x: x_batch,\n        model.input_y: y_batch,\n        model.keep_prob:keep_prob\n    }\n    return feed_dict\n\ndef train():\n    print(""Configuring TensorBoard and Saver..."")\n    tensorboard_dir = \'./tensorboard/textcnn\'\n    save_dir = \'./checkpoints/textcnn\'\n    if not os.path.exists(tensorboard_dir):\n        os.makedirs(tensorboard_dir)\n    if not os.path.exists(save_dir):\n        os.makedirs(save_dir)\n    save_path = os.path.join(save_dir, \'best_validation\')\n\n    print(""Loading training and validation data..."")\n    start_time = time.time()\n    x_train, y_train = process_file(config.train_filename, word_to_id, cat_to_id, config.seq_length)\n    x_val, y_val = process_file(config.val_filename, word_to_id, cat_to_id, config.seq_length)\n    print(""Time cost: %.3f seconds...\\n"" % (time.time() - start_time))\n\n    tf.summary.scalar(""loss"", model.loss)\n    tf.summary.scalar(""accuracy"", model.acc)\n    merged_summary = tf.summary.merge_all()\n    writer = tf.summary.FileWriter(tensorboard_dir)\n    saver = tf.train.Saver()\n\n    session = tf.Session()\n    session.run(tf.global_variables_initializer())\n    writer.add_graph(session.graph)\n\n    print(\'Training and evaluating...\')\n    best_val_accuracy = 0\n    last_improved = 0  # record global_step at best_val_accuracy\n    require_improvement = 1000  # break training if not having improvement over 1000 iter\n    flag=False\n\n    for epoch in range(config.num_epochs):\n        batch_train = batch_iter(x_train, y_train, config.batch_size)\n        start = time.time()\n        print(\'Epoch:\', epoch + 1)\n        for x_batch, y_batch in batch_train:\n            feed_dict = feed_data(x_batch, y_batch, config.keep_prob)\n            _, global_step, train_summaries, train_loss, train_accuracy = session.run([model.optim, model.global_step,\n                                                                                    merged_summary, model.loss,\n                                                                                    model.acc], feed_dict=feed_dict)\n            if global_step % config.print_per_batch == 0:\n                end = time.time()\n                val_loss, val_accuracy = evaluate(session, x_val, y_val)\n                writer.add_summary(train_summaries, global_step)\n\n                # If improved, save the model\n                if val_accuracy > best_val_accuracy:\n                    saver.save(session, save_path)\n                    best_val_accuracy = val_accuracy\n                    last_improved=global_step\n                    improved_str = \'*\'\n                else:\n                    improved_str = \'\'\n                print(""step: {},train loss: {:.3f}, train accuracy: {:.3f}, val loss: {:.3f}, val accuracy: {:.3f},training speed: {:.3f}sec/batch {}\\n"".format(\n                        global_step, train_loss, train_accuracy, val_loss, val_accuracy,\n                        (end - start) / config.print_per_batch,improved_str))\n                start = time.time()\n\n            if global_step - last_improved > require_improvement:\n                print(""No optimization over 1000 steps, stop training"")\n                flag = True\n                break\n        if flag:\n            break\n        config.lr *= config.lr_decay\n\nif __name__ == \'__main__\':\n    print(\'Configuring CNN model...\')\n    config = TextConfig()\n    filenames = [config.train_filename, config.test_filename, config.val_filename]\n    if not os.path.exists(config.vocab_filename):\n        build_vocab(filenames, config.vocab_filename, config.vocab_size)\n\n    #read vocab and categories\n    categories,cat_to_id = read_category()\n    words,word_to_id = read_vocab(config.vocab_filename)\n    config.vocab_size = len(words)\n\n    # trans vector file to numpy file\n    if not os.path.exists(config.vector_word_npz):\n        export_word2vec_vectors(word_to_id, config.vector_word_filename, config.vector_word_npz)\n    config.pre_trianing = get_training_word2vec_vectors(config.vector_word_npz)\n\n    model = TextCNN(config)\n    train()\n'"
train_word2vec.py,0,"b'#encoding:utf-8\nimport logging\nimport time\nimport codecs\nimport sys\nimport re\nimport jieba\nfrom gensim.models import word2vec\nfrom text_model import TextConfig\n\n\nre_han= re.compile(u""([\\u4E00-\\u9FD5a-zA-Z]+)"") # the method of cutting text by punctuation\n\nclass Get_Sentences(object):\n    \'\'\'\n\n    Args:\n         filenames: a list of train_filename,test_filename,val_filename\n    Yield:\n        word:a list of word cut by jieba\n\n    \'\'\'\n\n    def __init__(self,filenames):\n        self.filenames= filenames\n\n    def __iter__(self):\n        for filename in self.filenames:\n            with codecs.open(filename, \'r\', encoding=\'utf-8\') as f:\n                for _,line in enumerate(f):\n                    try:\n                        line=line.strip()\n                        line=line.split(\'\\t\')\n                        assert len(line)==2\n                        blocks=re_han.split(line[1])\n                        word=[]\n                        for blk in blocks:\n                            if re_han.match(blk):\n                                word.extend(jieba.lcut(blk))\n                        yield word\n                    except:\n                        pass\n\ndef train_word2vec(filenames):\n    \'\'\'\n    use word2vec train word vector\n    argv:\n        filenames: a list of train_filename,test_filename,val_filename\n    return: \n        save word vector to config.vector_word_filename\n\n    \'\'\'\n    t1 = time.time()\n    sentences = Get_Sentences(filenames)\n    logging.basicConfig(format=\'%(asctime)s : %(levelname)s : %(message)s\', level=logging.INFO)\n    model = word2vec.Word2Vec(sentences, size=100, window=5, min_count=1, workers=6)\n    model.wv.save_word2vec_format(config.vector_word_filename, binary=False)\n    print(\'-------------------------------------------\')\n    print(""Training word2vec model cost %.3f seconds...\\n"" % (time.time() - t1))\n\nif __name__ == \'__main__\':\n    config=TextConfig()\n    filenames=[config.train_filename,config.test_filename,config.val_filename]\n    train_word2vec(filenames)\n\n'"
