file_path,api_count,code
trainer/setup.py,0,"b'""""""Setup script""""""\n\nimport tarfile\nimport zipfile\nimport shutil\nimport argparse\nimport os\n\nfrom setuptools import setup, find_packages\nfrom setuptools.command.sdist import sdist\nfrom subprocess import call\n\n# There is some custom code in `model_main.py` to change up the logging\n# verbosity and other things like how many steps to print accuracy.\nPULL_OBJECT_DETECTION = """"""\n    svn export -r 11689 https://github.com/tensorflow/models/trunk/research/object_detection src/object_detection &&\n    svn export -r 11689 https://github.com/tensorflow/models/trunk/research/slim src/object_detection/slim &&\n    cd src && protoc object_detection/protos/*.proto --python_out=. && cd ..\n""""""\n\nPULL_CLASSIFICATION = """"""\n    svn export -r 308 https://github.com/tensorflow/hub/trunk/examples/image_retraining src/classification\n    echo > src/classification/__init__.py\n""""""\n\nOUTPUT_FILE = ""training.zip""\n\n\nparser = argparse.ArgumentParser()\nparser.add_argument(""type"")\nargs = parser.parse_args()\n\n\nclass InstallCommand(sdist):\n    def initialize_options(self):\n        sdist.initialize_options(self)\n        self.custom_option = None\n\n    def finalize_options(self):\n        sdist.finalize_options(self)\n\n    def run(self):\n        sdist.run(self)\n\n\ndef move_zip(zip_file, name):\n    with tarfile.open(""dist/{}-0.1.tar.gz"".format(name)) as tar:\n        for member in tar.getmembers():\n            if (\n                member.name.startswith(""{}-0.1/{}/"".format(name, name))\n                and member.isfile()\n            ):\n                print(""in: {}"".format(member.name))\n                f = tar.extractfile(member)\n                rel_path = os.path.relpath(member.name, ""{}-0.1"".format(name))\n                zip_file.writestr(rel_path, f.read())\n\n\nif args.type == ""all"":\n    call(PULL_CLASSIFICATION, shell=True)\n    call(PULL_OBJECT_DETECTION, shell=True)\n\nif args.type == ""object_detection"":\n    call(PULL_OBJECT_DETECTION, shell=True)\n\nif args.type == ""classification"":\n    call(PULL_CLASSIFICATION, shell=True)\n\n\nshutil.copy(""src/model_main_override.py"", ""src/object_detection/model_main.py"")\n\n\nsetup(\n    name=""src"",\n    version=""0.1"",\n    include_package_data=True,\n    packages=[p for p in find_packages() if p.startswith(""src"")],\n    description=""tf-training"",\n    cmdclass={args.type: InstallCommand},\n)\nshutil.rmtree(""src.egg-info"")\n\n\nwith zipfile.ZipFile(OUTPUT_FILE, ""w"", zipfile.ZIP_DEFLATED) as tf_model:\n    move_zip(tf_model, ""src"")\n\n\nshutil.rmtree(""dist"")\n'"
trainer/src/__init__.py,0,b''
trainer/src/export_labels.py,4,"b'import json\n\nimport tensorflow as tf\nfrom object_detection.utils.label_map_util import get_label_map_dict\n\n\nflags = tf.app.flags\nflags.DEFINE_string(""label_map_path"", None, ""Path to label map."")\nflags.DEFINE_string(""output_label_path"", None, ""Path to write labels."")\ntf.app.flags.mark_flag_as_required(""label_map_path"")\ntf.app.flags.mark_flag_as_required(""output_label_path"")\nFLAGS = flags.FLAGS\n\n\ndef main(_):\n    label_map = get_label_map_dict(FLAGS.label_map_path)\n    label_array = [k for k in sorted(label_map, key=label_map.get)]\n    with open(FLAGS.output_label_path, ""w"") as f:\n        json.dump(label_array, f)\n\n\nif __name__ == ""__main__"":\n    tf.app.run(main)\n'"
trainer/src/get_latest_checkpoint.py,3,"b'import os\nimport re\n\nimport tensorflow as tf\n\n\nflags = tf.app.flags\nflags.DEFINE_string(""checkpoint_path"", None, ""Path to checkpoints."")\ntf.app.flags.mark_flag_as_required(""checkpoint_path"")\nFLAGS = flags.FLAGS\n\n\ndef main(_):\n    regex = re.compile(r""model\\.ckpt-([0-9]+)\\.index"")\n    numbers = [\n        int(regex.search(f).group(1))\n        for f in os.listdir(FLAGS.checkpoint_path)\n        if regex.search(f)\n    ]\n    if not numbers:\n        print(""No checkpoint found!"")\n        exit(1)\n    trained_checkpoint_prefix = os.path.join(\n        FLAGS.checkpoint_path, ""model.ckpt-{}"".format(max(numbers))\n    )\n    print(trained_checkpoint_prefix)\n\n\nif __name__ == ""__main__"":\n    tf.app.run(main)\n'"
trainer/src/model_main_override.py,5,"b'# Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Binary to run train and evaluation on object detection model.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom absl import flags\n\nimport tensorflow as tf\n\nfrom object_detection import model_hparams\nfrom object_detection import model_lib\n\ntf.logging.set_verbosity(tf.logging.INFO)\n\n\nflags.DEFINE_string(\n    ""model_dir"",\n    None,\n    ""Path to output model directory ""\n    ""where event and checkpoint files will be written."",\n)\nflags.DEFINE_string(""pipeline_config_path"", None, ""Path to pipeline config "" ""file."")\nflags.DEFINE_integer(""num_train_steps"", None, ""Number of train steps."")\nflags.DEFINE_boolean(\n    ""eval_training_data"",\n    False,\n    ""If training data should be evaluated for this job. Note ""\n    ""that one call only use this in eval-only mode, and ""\n    ""`checkpoint_dir` must be supplied."",\n)\nflags.DEFINE_integer(\n    ""sample_1_of_n_eval_examples"",\n    1,\n    ""Will sample one of "" ""every n eval input examples, where n is provided."",\n)\nflags.DEFINE_integer(\n    ""sample_1_of_n_eval_on_train_examples"",\n    5,\n    ""Will sample ""\n    ""one of every n train input examples for evaluation, ""\n    ""where n is provided. This is only used if ""\n    ""`eval_training_data` is True."",\n)\nflags.DEFINE_integer(\n    ""log_step_count_steps"",\n    100,\n    ""The frequency, in number ""\n    ""of global steps, that the global step and the loss will ""\n    ""be logged during training. Also controls the frequency ""\n    ""that the global steps / s will be logged (and written to ""\n    ""summary) during training."",\n)\nflags.DEFINE_string(\n    ""hparams_overrides"",\n    None,\n    ""Hyperparameter overrides, ""\n    ""represented as a string containing comma-separated ""\n    ""hparam_name=value pairs."",\n)\nflags.DEFINE_string(\n    ""checkpoint_dir"",\n    None,\n    ""Path to directory holding a checkpoint.  If ""\n    ""`checkpoint_dir` is provided, this binary operates in eval-only mode, ""\n    ""writing resulting metrics to `model_dir`."",\n)\nflags.DEFINE_boolean(\n    ""run_once"",\n    False,\n    ""If running in eval-only mode, whether to run just ""\n    ""one round of eval vs running continuously (default)."",\n)\nFLAGS = flags.FLAGS\n\n\ndef main(unused_argv):\n    flags.mark_flag_as_required(""model_dir"")\n    flags.mark_flag_as_required(""pipeline_config_path"")\n    config = tf.estimator.RunConfig(\n        model_dir=FLAGS.model_dir, log_step_count_steps=FLAGS.log_step_count_steps\n    )\n\n    train_and_eval_dict = model_lib.create_estimator_and_inputs(\n        run_config=config,\n        hparams=model_hparams.create_hparams(FLAGS.hparams_overrides),\n        pipeline_config_path=FLAGS.pipeline_config_path,\n        train_steps=FLAGS.num_train_steps,\n        sample_1_of_n_eval_examples=FLAGS.sample_1_of_n_eval_examples,\n        sample_1_of_n_eval_on_train_examples=(\n            FLAGS.sample_1_of_n_eval_on_train_examples\n        ),\n    )\n    estimator = train_and_eval_dict[""estimator""]\n    train_input_fn = train_and_eval_dict[""train_input_fn""]\n    eval_input_fns = train_and_eval_dict[""eval_input_fns""]\n    eval_on_train_input_fn = train_and_eval_dict[""eval_on_train_input_fn""]\n    predict_input_fn = train_and_eval_dict[""predict_input_fn""]\n    train_steps = train_and_eval_dict[""train_steps""]\n\n    if FLAGS.checkpoint_dir:\n        if FLAGS.eval_training_data:\n            name = ""training_data""\n            input_fn = eval_on_train_input_fn\n        else:\n            name = ""validation_data""\n            # The first eval input will be evaluated.\n            input_fn = eval_input_fns[0]\n        if FLAGS.run_once:\n            estimator.evaluate(\n                input_fn,\n                steps=None,\n                checkpoint_path=tf.train.latest_checkpoint(FLAGS.checkpoint_dir),\n            )\n        else:\n            model_lib.continuous_eval(\n                estimator, FLAGS.checkpoint_dir, input_fn, train_steps, name\n            )\n    else:\n        train_spec, eval_specs = model_lib.create_train_and_eval_specs(\n            train_input_fn,\n            eval_input_fns,\n            eval_on_train_input_fn,\n            predict_input_fn,\n            train_steps,\n            eval_on_train_data=False,\n        )\n\n        # Currently only a single Eval Spec is allowed.\n        tf.estimator.train_and_evaluate(estimator, train_spec, eval_specs[0])\n\n\nif __name__ == ""__main__"":\n    tf.app.run()\n'"
trainer/src/convert/TFLiteConverter.py,4,"b'import tensorflow as tf\nfrom pkg_resources import parse_version\n\n\nprint(""Using TensorFlow version: {}"".format(parse_version(tf.__version__)))\n\n# TensorFlow 1.12\nif parse_version(tf.__version__) < parse_version(""1.13""):\n    print(""Using contrib TFLiteConverter"")\n    convert = tf.contrib.lite.TFLiteConverter\n\n# Current\nelse:\n    print(""Using current TFLiteConverter"")\n    convert = tf.lite.TFLiteConverter\n'"
trainer/src/convert/__init__.py,0,b''
trainer/src/convert/build_decoder.py,0,"b'import coremltools\nimport numpy as np\n\nfrom coremltools.models import datatypes\nfrom coremltools.models import neural_network\n\n\ndef build_decoder(anchors, num_classes, num_anchors):\n    # MLMultiArray inputs of neural networks must have 1 or 3 dimensions.\n    # We only have 2, so add an unused dimension of size one at the back.\n    input_features = [\n        (""scores"", datatypes.Array(num_classes + 1, num_anchors, 1)),\n        (""boxes"", datatypes.Array(4, num_anchors, 1)),\n    ]\n\n    # The outputs of the decoder model should match the inputs of the next\n    # model in the pipeline, NonMaximumSuppression. This expects the number\n    # of bounding boxes in the first dimension.\n    output_features = [\n        (""raw_confidence"", datatypes.Array(num_anchors, num_classes)),\n        (""raw_coordinates"", datatypes.Array(num_anchors, 4)),\n    ]\n\n    builder = neural_network.NeuralNetworkBuilder(input_features, output_features)\n\n    # (num_classes+1, num_anchors, 1) --> (1, num_anchors, num_classes+1)\n    builder.add_permute(\n        name=""permute_scores"",\n        dim=(0, 3, 2, 1),\n        input_name=""scores"",\n        output_name=""permute_scores_output"",\n    )\n\n    # Strip off the ""unknown"" class (at index 0).\n    builder.add_slice(\n        name=""slice_scores"",\n        input_name=""permute_scores_output"",\n        output_name=""raw_confidence"",\n        axis=""width"",\n        start_index=1,\n        end_index=num_classes + 1,\n    )\n\n    # Grab the y, x coordinates (channels 0-1).\n    builder.add_slice(\n        name=""slice_yx"",\n        input_name=""boxes"",\n        output_name=""slice_yx_output"",\n        axis=""channel"",\n        start_index=0,\n        end_index=2,\n    )\n\n    # boxes_yx / 10\n    builder.add_elementwise(\n        name=""scale_yx"",\n        input_names=""slice_yx_output"",\n        output_name=""scale_yx_output"",\n        mode=""MULTIPLY"",\n        alpha=0.1,\n    )\n\n    # Split the anchors into two (2, 1917, 1) arrays.\n    anchors_yx = np.expand_dims(anchors[:2, :], axis=-1)\n    anchors_hw = np.expand_dims(anchors[2:, :], axis=-1)\n\n    builder.add_load_constant(\n        name=""anchors_yx"",\n        output_name=""anchors_yx"",\n        constant_value=anchors_yx,\n        shape=[2, num_anchors, 1],\n    )\n\n    builder.add_load_constant(\n        name=""anchors_hw"",\n        output_name=""anchors_hw"",\n        constant_value=anchors_hw,\n        shape=[2, num_anchors, 1],\n    )\n\n    # (boxes_yx / 10) * anchors_hw\n    builder.add_elementwise(\n        name=""yw_times_hw"",\n        input_names=[""scale_yx_output"", ""anchors_hw""],\n        output_name=""yw_times_hw_output"",\n        mode=""MULTIPLY"",\n    )\n\n    # (boxes_yx / 10) * anchors_hw + anchors_yx\n    builder.add_elementwise(\n        name=""decoded_yx"",\n        input_names=[""yw_times_hw_output"", ""anchors_yx""],\n        output_name=""decoded_yx_output"",\n        mode=""ADD"",\n    )\n\n    # Grab the height and width (channels 2-3).\n    builder.add_slice(\n        name=""slice_hw"",\n        input_name=""boxes"",\n        output_name=""slice_hw_output"",\n        axis=""channel"",\n        start_index=2,\n        end_index=4,\n    )\n\n    # (boxes_hw / 5)\n    builder.add_elementwise(\n        name=""scale_hw"",\n        input_names=""slice_hw_output"",\n        output_name=""scale_hw_output"",\n        mode=""MULTIPLY"",\n        alpha=0.2,\n    )\n\n    # exp(boxes_hw / 5)\n    builder.add_unary(\n        name=""exp_hw"",\n        input_name=""scale_hw_output"",\n        output_name=""exp_hw_output"",\n        mode=""exp"",\n    )\n\n    # exp(boxes_hw / 5) * anchors_hw\n    builder.add_elementwise(\n        name=""decoded_hw"",\n        input_names=[""exp_hw_output"", ""anchors_hw""],\n        output_name=""decoded_hw_output"",\n        mode=""MULTIPLY"",\n    )\n\n    # The coordinates are now (y, x) and (height, width) but NonMaximumSuppression\n    # wants them as (x, y, width, height). So create four slices and then concat\n    # them into the right order.\n    builder.add_slice(\n        name=""slice_y"",\n        input_name=""decoded_yx_output"",\n        output_name=""slice_y_output"",\n        axis=""channel"",\n        start_index=0,\n        end_index=1,\n    )\n\n    builder.add_slice(\n        name=""slice_x"",\n        input_name=""decoded_yx_output"",\n        output_name=""slice_x_output"",\n        axis=""channel"",\n        start_index=1,\n        end_index=2,\n    )\n\n    builder.add_slice(\n        name=""slice_h"",\n        input_name=""decoded_hw_output"",\n        output_name=""slice_h_output"",\n        axis=""channel"",\n        start_index=0,\n        end_index=1,\n    )\n\n    builder.add_slice(\n        name=""slice_w"",\n        input_name=""decoded_hw_output"",\n        output_name=""slice_w_output"",\n        axis=""channel"",\n        start_index=1,\n        end_index=2,\n    )\n\n    builder.add_elementwise(\n        name=""concat"",\n        input_names=[\n            ""slice_x_output"",\n            ""slice_y_output"",\n            ""slice_w_output"",\n            ""slice_h_output"",\n        ],\n        output_name=""concat_output"",\n        mode=""CONCAT"",\n    )\n\n    # (4, num_anchors, 1) --> (1, num_anchors, 4)\n    builder.add_permute(\n        name=""permute_output"",\n        dim=(0, 3, 2, 1),\n        input_name=""concat_output"",\n        output_name=""raw_coordinates"",\n    )\n\n    return coremltools.models.MLModel(builder.spec)\n'"
trainer/src/convert/build_nms.py,0,"b'import coremltools\n\n\ndef build_nms(\n    decoder_model, labels, default_iou_threshold=0.5, default_confidence_threshold=0.5\n):\n    nms_spec = coremltools.proto.Model_pb2.Model()\n    nms_spec.specificationVersion = 3\n\n    for i in range(2):\n        decoder_output = decoder_model._spec.description.output[i].SerializeToString()\n\n        nms_spec.description.input.add()\n        nms_spec.description.input[i].ParseFromString(decoder_output)\n\n        nms_spec.description.output.add()\n        nms_spec.description.output[i].ParseFromString(decoder_output)\n\n    nms_spec.description.output[0].name = ""confidence""\n    nms_spec.description.output[1].name = ""coordinates""\n\n    output_sizes = [len(labels), 4]\n    for i in range(2):\n        ma_type = nms_spec.description.output[i].type.multiArrayType\n        ma_type.shapeRange.sizeRanges.add()\n        ma_type.shapeRange.sizeRanges[0].lowerBound = 0\n        ma_type.shapeRange.sizeRanges[0].upperBound = -1\n        ma_type.shapeRange.sizeRanges.add()\n        ma_type.shapeRange.sizeRanges[1].lowerBound = output_sizes[i]\n        ma_type.shapeRange.sizeRanges[1].upperBound = output_sizes[i]\n        del ma_type.shape[:]\n\n    nms = nms_spec.nonMaximumSuppression\n    nms.confidenceInputFeatureName = ""raw_confidence""\n    nms.coordinatesInputFeatureName = ""raw_coordinates""\n    nms.confidenceOutputFeatureName = ""confidence""\n    nms.coordinatesOutputFeatureName = ""coordinates""\n    nms.iouThresholdInputFeatureName = ""iouThreshold""\n    nms.confidenceThresholdInputFeatureName = ""confidenceThreshold""\n\n    nms.iouThreshold = default_iou_threshold\n    nms.confidenceThreshold = default_confidence_threshold\n\n    nms.pickTop.perClass = True\n    nms.stringClassLabels.vector.extend(labels)\n\n    return coremltools.models.MLModel(nms_spec)\n'"
trainer/src/convert/convert.py,7,"b'import os\nimport argparse\n\nimport numpy as np\nimport tensorflow as tf\n\nfrom tensorflow.python.platform import gfile\nfrom tensorflow.python.framework import dtypes\nfrom tensorflow.python.tools import strip_unused_lib\n\n\ntf.enable_eager_execution()\n\n\nparser = argparse.ArgumentParser()\n# export types\nparser.add_argument(""--coreml"", action=""store_true"")\nparser.add_argument(""--tflite"", action=""store_true"")\nparser.add_argument(""--tfjs"", action=""store_true"")\n\nparser.add_argument(""--model-type"", type=str)\n\n# import paths\nparser.add_argument(""--saved-model"", type=str)\n\n# export paths\nparser.add_argument(""--mlmodel-path"", type=str)\nparser.add_argument(""--tflite-path"", type=str)\nparser.add_argument(""--tfjs-path"", type=str)\nargs = parser.parse_args()\n\n\ndef print_header(msg):\n    print("" "" * 80)\n    print(""_"" * 80)\n    print(msg)\n\n\ndef print_footer(msg):\n    print(msg)\n    print(""_"" * 80)\n    print("" "" * 80)\n\n\ndef attempt_conversion(model_type, model_format):\n    def attempt_conversion(convert):\n        try:\n            print_header(f""Converting {model_type} model to {model_format}"")\n            convert()\n            print_footer(f""Successfully converted to {model_format}"")\n        except Exception as e:\n            print(e)\n            print_footer(f""Unable to convert to {model_format}"")\n\n    return attempt_conversion\n\n\ndef get_anchors(graph):\n    """"""\n    Computes the list of anchor boxes by sending a fake image through the graph.\n    Outputs an array of size (4, num_anchors) where each element is an anchor box\n    given as [ycenter, xcenter, height, width] in normalized coordinates.\n    """"""\n    with tf.Session(graph=graph) as sess:\n        anchors_tensor = ""Concatenate/concat:0""\n        image_tensor = graph.get_tensor_by_name(""image_tensor:0"")\n        box_corners_tensor = graph.get_tensor_by_name(anchors_tensor)\n        box_corners = sess.run(\n            box_corners_tensor, feed_dict={image_tensor: np.zeros((1, 300, 300, 3))}\n        )\n\n        # The TensorFlow graph gives each anchor box as [ymin, xmin, ymax, xmax].\n        # Convert these min/max values to a center coordinate, width and height.\n        ymin, xmin, ymax, xmax = np.transpose(box_corners)\n        width = xmax - xmin\n        height = ymax - ymin\n        ycenter = ymin + height / 2.0\n        xcenter = xmin + width / 2.0\n        return np.stack([ycenter, xcenter, height, width])\n\n\ndef strip_and_freeze_model(\n    saved_model, output_path, input_node_names=[], output_node_names=[]\n):\n    graph = tf.Graph()\n    with tf.Session(graph=graph) as sess:\n        print(""loading model..."")\n        tf.saved_model.loader.load(sess, [tf.saved_model.SERVING], saved_model)\n\n        print(""stripping unused ops..."")\n        gdef = strip_unused_lib.strip_unused(\n            input_graph_def=tf.get_default_graph().as_graph_def(),\n            input_node_names=input_node_names,\n            output_node_names=output_node_names,\n            placeholder_type_enum=dtypes.float32.as_datatype_enum,\n        )\n\n        gdef = tf.graph_util.convert_variables_to_constants(\n            sess, gdef, output_node_names\n        )\n\n        with gfile.GFile(output_path, ""wb"") as f:\n            print(""writing frozen model..."")\n            f.write(gdef.SerializeToString())\n    return graph\n\n\nos.makedirs("".tmp"", exist_ok=True)\n\n\n################################################################################\n# Object Detection\n################################################################################\nif args.model_type == ""localization"":\n    labels_path = os.path.join(args.saved_model, ""labels.json"")\n\n    @attempt_conversion(""object detection"", ""Core ML"")\n    def convert_object_detection_coreml():\n        if args.coreml:\n            from convert.convert_to_core_ml import convert_localization\n\n            frozen_model = "".tmp/coreml_frozen_model.pb""\n\n            graph = strip_and_freeze_model(\n                saved_model=args.saved_model,\n                output_path=frozen_model,\n                input_node_names=[""Preprocessor/sub""],\n                output_node_names=[""Squeeze"", ""Postprocessor/convert_scores""],\n            )\n\n            anchors = get_anchors(graph)\n\n            convert_localization(\n                frozen_model=frozen_model,\n                labels_path=labels_path,\n                output_path=args.mlmodel_path,\n                anchors=anchors,\n            )\n\n    @attempt_conversion(""object detection"", ""TensorFlow Lite"")\n    def convert_object_detection_tflite():\n        if args.tflite:\n            from convert.convert_to_tflite import convert_localization\n\n            frozen_model = "".tmp/tflite_frozen_model.pb""\n\n            graph = strip_and_freeze_model(\n                saved_model=args.saved_model,\n                output_path=frozen_model,\n                input_node_names=[""Preprocessor/sub""],\n                output_node_names=[""Squeeze"", ""Postprocessor/convert_scores""],\n            )\n\n            anchors = get_anchors(graph)\n\n            convert_localization(\n                frozen_model=frozen_model,\n                labels_path=labels_path,\n                output_path=args.tflite_path,\n                anchors=anchors,\n            )\n\n    @attempt_conversion(""object detection"", ""TensorFlow.js"")\n    def convert_object_detection_tfjs():\n        if args.tfjs:\n            from convert.convert_to_tfjs import convert_localization\n\n            frozen_model = "".tmp/tfjs_frozen_model.pb""\n\n            strip_and_freeze_model(\n                saved_model=args.saved_model,\n                output_path=frozen_model,\n                input_node_names=[],\n                output_node_names=[""Postprocessor/ExpandDims_1"", ""Postprocessor/Slice""],\n            )\n\n            convert_localization(\n                frozen_model=frozen_model,\n                labels_path=labels_path,\n                output_path=args.tfjs_path,\n            )\n\n\n################################################################################\n# Classification\n################################################################################\nif args.model_type == ""classification"":\n    labels_path = os.path.join(args.saved_model, ""labels.txt"")\n\n    @attempt_conversion(""classification"", ""Core ML"")\n    def convert_classification_coreml():\n        if args.coreml:\n            from convert.convert_to_core_ml import convert_classification\n\n            frozen_model = "".tmp/coreml_frozen_model.pb""\n\n            strip_and_freeze_model(\n                saved_model=args.saved_model,\n                output_path=frozen_model,\n                input_node_names=[""Placeholder""],\n                output_node_names=[""final_result""],\n            )\n\n            convert_classification(\n                frozen_model=frozen_model,\n                labels_path=labels_path,\n                output_path=args.mlmodel_path,\n            )\n\n    @attempt_conversion(""classification"", ""TensorFlow Lite"")\n    def convert_classification_tflite():\n        if args.tflite:\n            from convert.convert_to_tflite import convert_classification\n\n            frozen_model = "".tmp/tflite_frozen_model.pb""\n\n            strip_and_freeze_model(\n                saved_model=args.saved_model,\n                output_path=frozen_model,\n                input_node_names=[""Placeholder""],\n                output_node_names=[""final_result""],\n            )\n\n            convert_classification(\n                frozen_model=frozen_model,\n                labels_path=labels_path,\n                output_path=args.tflite_path,\n            )\n\n    @attempt_conversion(""classification"", ""TensorFlow.js"")\n    def convert_classification_tfjs():\n        if args.tfjs:\n            from convert.convert_to_tfjs import convert_classification\n\n            frozen_model = "".tmp/tfjs_frozen_model.pb""\n\n            strip_and_freeze_model(\n                saved_model=args.saved_model,\n                output_path=frozen_model,\n                input_node_names=[""Placeholder""],\n                output_node_names=[""final_result""],\n            )\n\n            convert_classification(\n                frozen_model=frozen_model,\n                labels_path=labels_path,\n                output_path=args.tfjs_path,\n            )\n'"
trainer/src/convert/convert_to_core_ml.py,0,"b'import os\nimport json\n\nimport tfcoreml\nimport coremltools\n\nfrom convert.build_nms import build_nms\nfrom convert.build_decoder import build_decoder\nfrom coremltools.models.pipeline import Pipeline\nfrom coremltools.models import datatypes\n\n\ndef _convert_multiarray_to_float32(feature):\n    from coremltools.proto import FeatureTypes_pb2 as ft\n\n    if feature.type.HasField(""multiArrayType""):\n        feature.type.multiArrayType.dataType = ft.ArrayFeatureType.DOUBLE\n\n\ndef convert_localization(frozen_model, labels_path, output_path, anchors):\n    os.makedirs(output_path, exist_ok=True)\n\n    num_anchors = 1917\n\n    with open(labels_path) as f:\n        labels = json.load(f)\n\n    # Strip the model down to something usable by Core ML.\n    # Instead of `concat_1`, use `Postprocessor/convert_scores`, because it\n    # applies the sigmoid to the class scores.\n    input_node = ""Preprocessor/sub""\n    bbox_output_node = ""Squeeze""\n    class_output_node = ""Postprocessor/convert_scores""\n\n    # Convert to Core ML model.\n    ssd_model = tfcoreml.convert(\n        tf_model_path=frozen_model,\n        input_name_shape_dict={input_node: [1, 300, 300, 3]},\n        image_input_names=[input_node],\n        output_feature_names=[bbox_output_node, class_output_node],\n        is_bgr=False,\n        red_bias=-1.0,\n        green_bias=-1.0,\n        blue_bias=-1.0,\n        image_scale=2.0 / 255,\n        minimum_ios_deployment_target=""13"",\n    )\n\n    spec = ssd_model.get_spec()\n\n    # Rename the inputs and outputs to something more readable.\n    spec.description.input[0].name = ""image""\n    spec.description.input[0].shortDescription = ""Input image""\n    spec.neuralNetwork.preprocessing[0].featureName = ""image""\n\n    for i in range(len(spec.description.output)):\n        if spec.description.output[i].name == bbox_output_node:\n            spec.description.output[i].name = ""boxes""\n            spec.description.output[\n                i\n            ].shortDescription = ""Predicted coordinates for each bounding box""\n            spec.description.output[i].type.multiArrayType.shape[:] = [\n                4,\n                num_anchors,\n                1,\n            ]\n\n        if spec.description.output[i].name == class_output_node:\n            spec.description.output[i].name = ""scores""\n            spec.description.output[\n                i\n            ].shortDescription = ""Predicted class scores for each bounding box""\n            spec.description.output[i].type.multiArrayType.shape[:] = [\n                len(labels) + 1,\n                num_anchors,\n                1,\n            ]\n\n    for i in range(len(spec.neuralNetwork.layers)):\n        # Assumes everything only has 1 input or output...\n        if spec.neuralNetwork.layers[i].input[0] == input_node:\n            spec.neuralNetwork.layers[i].input[0] = ""image""\n        if spec.neuralNetwork.layers[i].output[0] == class_output_node:\n            spec.neuralNetwork.layers[i].output[0] = ""scores""\n        if spec.neuralNetwork.layers[i].output[0] == bbox_output_node:\n            spec.neuralNetwork.layers[i].output[0] = ""boxes""\n\n    for input_ in spec.description.input:\n        _convert_multiarray_to_float32(input_)\n    for output_ in spec.description.output:\n        _convert_multiarray_to_float32(output_)\n\n    # Convert weights to 16-bit floats to make the model smaller.\n    spec = coremltools.utils.convert_neural_network_spec_weights_to_fp16(spec)\n\n    input_features = [\n        (""image"", datatypes.Array(3, 300, 300)),\n        (""iouThreshold"", datatypes.Double()),\n        (""confidenceThreshold"", datatypes.Double()),\n    ]\n\n    output_features = [""confidence"", ""coordinates""]\n\n    pipeline = Pipeline(input_features, output_features)\n\n    # Create a new MLModel from the modified spec and save it.\n    ssd_model = coremltools.models.MLModel(spec)\n    decoder_model = build_decoder(anchors, len(labels), num_anchors)\n    nms_model = build_nms(decoder_model, labels)\n\n    pipeline.add_model(ssd_model)\n    pipeline.add_model(decoder_model)\n    pipeline.add_model(nms_model)\n\n    # The `image` input should really be an image, not a multi-array.\n    pipeline.spec.description.input[0].ParseFromString(\n        ssd_model._spec.description.input[0].SerializeToString()\n    )\n\n    # Copy the declarations of the `confidence` and `coordinates` outputs.\n    # The Pipeline makes these strings by default.\n    pipeline.spec.description.output[0].ParseFromString(\n        nms_model._spec.description.output[0].SerializeToString()\n    )\n    pipeline.spec.description.output[1].ParseFromString(\n        nms_model._spec.description.output[1].SerializeToString()\n    )\n\n    # Add descriptions to the inputs and outputs.\n    pipeline.spec.description.input[\n        1\n    ].shortDescription = ""(optional) IOU Threshold override""\n    pipeline.spec.description.input[\n        2\n    ].shortDescription = ""(optional) Confidence Threshold override""\n    pipeline.spec.description.output[\n        0\n    ].shortDescription = u""Boxes \\xd7 Class confidence""\n    pipeline.spec.description.output[\n        1\n    ].shortDescription = u""Boxes \\xd7 [x, y, width, height] (relative to image size)""\n\n    # Add metadata to the model.\n    pipeline.spec.description.metadata.versionString = ""ssd_mobilenet""\n    pipeline.spec.description.metadata.shortDescription = ""MobileNet + SSD""\n    pipeline.spec.description.metadata.author = (\n        ""Converted to Core ML by Cloud Annotations""\n    )\n    pipeline.spec.description.metadata.license = (\n        ""https://github.com/tensorflow/models/blob/master/research/object_detection""\n    )\n\n    # Add the list of class labels and the default threshold values too.\n    user_defined_metadata = {\n        ""iou_threshold"": str(0.5),\n        ""confidence_threshold"": str(0.5),\n        ""classes"": "","".join(labels),\n    }\n    pipeline.spec.description.metadata.userDefined.update(user_defined_metadata)\n\n    pipeline.spec.specificationVersion = 4\n\n    final_model = coremltools.models.MLModel(pipeline.spec)\n    final_model.save(os.path.join(output_path, ""Model.mlmodel""))\n\n\ndef convert_classification(frozen_model, labels_path, output_path):\n    os.makedirs(output_path, exist_ok=True)\n\n    tfcoreml.convert(\n        tf_model_path=frozen_model,\n        mlmodel_path=os.path.join(output_path, ""Model.mlmodel""),\n        input_name_shape_dict={\n            ""Placeholder"": [1, 224, 224, 3],\n            ""input/BottleneckInputPlaceholder"": [-1, 1024],\n        },\n        image_input_names=[""Placeholder""],\n        output_feature_names=[""final_result""],\n        class_labels=labels_path,\n        is_bgr=False,\n        red_bias=-1.0,\n        green_bias=-1.0,\n        blue_bias=-1.0,\n        image_scale=2.0 / 255,\n        minimum_ios_deployment_target=""13"",\n    )\n'"
trainer/src/convert/convert_to_tfjs.py,0,"b'import os\nimport json\nimport shutil\n\nfrom tensorflowjs.converters import tf_saved_model_conversion_v2\n\n\ndef convert_localization(frozen_model, labels_path, output_path):\n    tf_saved_model_conversion_v2.convert_tf_frozen_model(\n        frozen_model,\n        ""Postprocessor/ExpandDims_1,Postprocessor/Slice"",\n        output_path,\n        quantization_dtype=None,\n        skip_op_check=False,\n        strip_debug_ops=True,\n    )\n\n    # Move the labels to the model directory.\n    shutil.copy2(labels_path, output_path)\n\n\ndef convert_classification(frozen_model, labels_path, output_path):\n    tf_saved_model_conversion_v2.convert_tf_frozen_model(\n        frozen_model,\n        ""final_result"",\n        output_path,\n        quantization_dtype=None,\n        skip_op_check=False,\n        strip_debug_ops=True,\n    )\n\n    # Move the labels to the model directory.\n    with open(labels_path, ""r"") as f:\n        labels = f.read()\n        labels = list(filter(bool, [s.strip() for s in labels.splitlines()]))\n    with open(os.path.join(output_path, ""labels.json""), ""w"") as f:\n        json.dump(labels, f)\n'"
trainer/src/convert/convert_to_tflite.py,0,"b'import os\nimport json\nimport shutil\n\nimport numpy as np\n\nfrom convert.TFLiteConverter import convert\n\n\ndef convert_localization(frozen_model, labels_path, output_path, anchors):\n    os.makedirs(output_path, exist_ok=True)\n\n    converter = convert.from_frozen_graph(\n        frozen_model,\n        input_arrays=[""Preprocessor/sub""],\n        output_arrays=[""Squeeze"", ""Postprocessor/convert_scores""],\n        input_shapes={""Preprocessor/sub"": [1, 300, 300, 3]},\n    )\n\n    # Write tflite model\n    tflite_model = converter.convert()\n    with open(os.path.join(output_path, ""model.tflite""), ""wb"") as f:\n        f.write(tflite_model)\n\n    # Write anchors\n    anchors = np.swapaxes(anchors, 0, 1)\n    with open(os.path.join(output_path, ""anchors.json""), ""w"") as f:\n        json.dump(anchors.tolist(), f)\n\n    # Move the labels to the model directory.\n    shutil.copy2(labels_path, output_path)\n\n\ndef convert_classification(frozen_model, labels_path, output_path):\n    os.makedirs(output_path, exist_ok=True)\n\n    converter = convert.from_frozen_graph(\n        frozen_model,\n        input_arrays=[""Placeholder""],\n        output_arrays=[""final_result""],\n        input_shapes={""Placeholder"": [1, 224, 224, 3]},\n    )\n\n    tflite_model = converter.convert()\n    with open(os.path.join(output_path, ""model.tflite""), ""wb"") as f:\n        f.write(tflite_model)\n\n    # Move the labels to the model directory.\n    with open(labels_path, ""r"") as f:\n        labels = f.read()\n        labels = list(filter(bool, [s.strip() for s in labels.splitlines()]))\n    with open(os.path.join(output_path, ""labels.json""), ""w"") as f:\n        json.dump(labels, f)\n'"
trainer/src/data/__init__.py,0,b''
trainer/src/data/prepare_data_classification.py,0,"b'from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport os\nimport shutil\nimport json\n\nread_dir = """"\nwrite_dir = """"\ntry:\n    read_dir = os.environ[""DATA_DIR""]\n    write_dir = os.environ[""RESULT_DIR""]\nexcept Exception:\n    pass\n\n\ndef main(read_bucket=read_dir, write_bucket=write_dir):\n    def create_dir(base, dirName):\n        path = os.path.join(base, dirName)\n        if os.path.exists(path) and os.path.isdir(path):\n            shutil.rmtree(path)\n        os.makedirs(path)\n        return path\n\n    data_dir = create_dir("""", ""data"")\n\n    annotations_file = os.path.join(read_bucket, ""_annotations.json"")\n\n    with open(annotations_file) as f:\n        annotations = json.load(f)[""annotations""]\n\n    labels = list(\n        {annotation[""label""] for image in annotations.values() for annotation in image}\n    )\n\n    for label in labels:\n        file_list = [\n            image_name\n            for image_name in annotations.keys()\n            for annotation in annotations[image_name]\n            if annotation[""label""] == label\n        ]\n\n        # Make directory for labels, if they don\'t exist.\n        train_label_dir = os.path.join(data_dir, label)\n        if not os.path.exists(train_label_dir):\n            os.makedirs(train_label_dir)\n\n        # move files to their proper labels.\n        for f in file_list:\n            try:\n                shutil.copy2(os.path.join(read_bucket, f), train_label_dir)\n            except Exception as err:\n                print(""Error: {}, skipping {}..."".format(err, f))\n\n\nif __name__ == ""__main__"":\n    main()\n'"
trainer/src/data/prepare_data_object_detection.py,3,"b'import os\nimport io\nimport json\nimport random\nimport hashlib\nimport shutil\nimport tarfile\n\nimport six.moves.urllib as urllib\n\nimport contextlib2\nimport PIL.Image\nimport tensorflow as tf\n\nfrom object_detection.dataset_tools import tf_record_creation_util\nfrom object_detection.utils import dataset_util\n\nread_dir = """"\nwrite_dir = """"\ntry:\n    read_dir = os.environ[""DATA_DIR""]\n    write_dir = os.environ[""RESULT_DIR""]\nexcept Exception:\n    pass\n\n\ndef main(read_bucket=read_dir, write_bucket=write_dir):\n    ############################################################################\n    # Prepare Directories\n    ############################################################################\n    def create_dir(base, dirName):\n        path = os.path.join(base, dirName)\n        if os.path.exists(path) and os.path.isdir(path):\n            shutil.rmtree(path)\n        os.makedirs(path)\n        return path\n\n    data_dir = create_dir("""", ""data"")\n    checkpoint_dir = create_dir(write_dir, ""checkpoints"")\n\n    ############################################################################\n    # Create LabelMap Proto\n    ############################################################################\n    annotations_file = os.path.join(read_bucket, ""_annotations.json"")\n\n    with open(annotations_file) as f:\n        annotations = json.load(f)[""annotations""]\n\n    labels = list(\n        {annotation[""label""] for image in annotations.values() for annotation in image}\n    )\n\n    label_map_path = os.path.join(data_dir, ""label_map.pbtxt"")\n\n    with open(label_map_path, ""w"") as file:\n        for idx, label in enumerate(labels):\n            file.write(""item {\\n"")\n            file.write(""\\tname: \'{}\'\\n"".format(label))\n            file.write(""\\tid: {}\\n"".format(idx + 1))  # indexes must start at 1.\n            file.write(""}\\n"")\n\n    ############################################################################\n    # Create TF Records\n    ############################################################################\n    image_files = [image for image in annotations.keys()]\n\n    train_shards = 10\n    val_shards = 10\n    random.seed(42)\n    random.shuffle(image_files)\n    num_train = int(0.7 * len(image_files))\n    train_examples = image_files[:num_train]\n    val_examples = image_files[num_train:]\n    print(\n        ""{} training and {} validation examples."".format(\n            len(train_examples), len(val_examples)\n        )\n    )\n\n    train_output_path = os.path.join(data_dir, ""train.record"")\n    val_output_path = os.path.join(data_dir, ""val.record"")\n\n    def create_tf_record(output_filename, num_shards, examples):\n        with contextlib2.ExitStack() as tf_record_close_stack:\n            output_tfrecords = tf_record_creation_util.open_sharded_output_tfrecords(\n                tf_record_close_stack, output_filename, num_shards\n            )\n            for idx, example in enumerate(examples):\n                try:\n                    img_path = os.path.join(read_bucket, example)\n                    if not os.path.isfile(img_path):\n                        continue\n                    with tf.gfile.GFile(img_path, ""rb"") as fid:\n                        encoded_jpg = fid.read()\n                    encoded_jpg_io = io.BytesIO(encoded_jpg)\n                    image = PIL.Image.open(encoded_jpg_io)\n                    if image.format != ""JPEG"":\n                        raise ValueError(""Image format not JPEG"")\n                    key = hashlib.sha256(encoded_jpg).hexdigest()\n\n                    width, height = image.size\n\n                    xmins = []\n                    xmaxs = []\n                    ymins = []\n                    ymaxs = []\n                    classes_text = []  # \'coke\', \'pepsi\', \'coke\'...\n                    classes = []  # 1, 2, 1...\n                    difficult_obj = []\n                    truncated = []\n                    poses = []\n\n                    for annotation in annotations[example]:\n                        if (\n                            ""x"" in annotation\n                            and ""x2"" in annotation\n                            and ""y"" in annotation\n                            and ""y2"" in annotation\n                        ):\n                            xmins.append(annotation[""x""])\n                            xmaxs.append(annotation[""x2""])\n                            ymins.append(annotation[""y""])\n                            ymaxs.append(annotation[""y2""])\n                            classes_text.append(annotation[""label""].encode(""utf8""))\n                            classes.append(\n                                1\n                            )  # temporary, I need to assign labels to actual ids\n                            difficult_obj.append(0)\n                            truncated.append(0)\n                            poses.append("""".encode(""utf8""))\n\n                    feature_dict = {\n                        ""image/height"": dataset_util.int64_feature(height),\n                        ""image/width"": dataset_util.int64_feature(width),\n                        ""image/filename"": dataset_util.bytes_feature(\n                            example.encode(""utf8"")\n                        ),\n                        ""image/source_id"": dataset_util.bytes_feature(\n                            example.encode(""utf8"")\n                        ),\n                        ""image/key/sha256"": dataset_util.bytes_feature(\n                            key.encode(""utf8"")\n                        ),\n                        ""image/encoded"": dataset_util.bytes_feature(encoded_jpg),\n                        ""image/format"": dataset_util.bytes_feature(\n                            ""jpeg"".encode(""utf8"")\n                        ),\n                        ""image/object/bbox/xmin"": dataset_util.float_list_feature(\n                            xmins\n                        ),\n                        ""image/object/bbox/xmax"": dataset_util.float_list_feature(\n                            xmaxs\n                        ),\n                        ""image/object/bbox/ymin"": dataset_util.float_list_feature(\n                            ymins\n                        ),\n                        ""image/object/bbox/ymax"": dataset_util.float_list_feature(\n                            ymaxs\n                        ),\n                        ""image/object/class/text"": dataset_util.bytes_list_feature(\n                            classes_text\n                        ),\n                        ""image/object/class/label"": dataset_util.int64_list_feature(\n                            classes\n                        ),\n                        ""image/object/difficult"": dataset_util.int64_list_feature(\n                            difficult_obj\n                        ),\n                        ""image/object/truncated"": dataset_util.int64_list_feature(\n                            truncated\n                        ),\n                        ""image/object/view"": dataset_util.bytes_list_feature(poses),\n                    }\n                    tf_example = tf.train.Example(\n                        features=tf.train.Features(feature=feature_dict)\n                    )\n                    if tf_example:\n                        shard_idx = idx % num_shards\n                        output_tfrecords[shard_idx].write(\n                            tf_example.SerializeToString()\n                        )\n                except ValueError:\n                    print(""Invalid example, ignoring."")\n\n    create_tf_record(train_output_path, train_shards, train_examples)\n    create_tf_record(val_output_path, val_shards, val_examples)\n\n    ############################################################################\n    # Extract Model Checkpoint\n    ############################################################################\n    download_base = ""https://max-cdn.cdn.appdomain.cloud/max-object-detector/1.0.1/""\n    model_file = ""ssd_mobilenet_v1_coco_2018_01_28.tar.gz""\n\n    tar_path = os.path.join("""", model_file)\n\n    if not os.path.exists(tar_path):\n        print(""Downloading model checkpoint..."")\n        opener = urllib.request.URLopener()\n        opener.retrieve(download_base + model_file, tar_path)\n    else:\n        print(""Model checkpoint found."")\n\n    with tarfile.open(tar_path) as tar:\n        for member in tar.getmembers():\n            # Flatten the directory.\n            member.name = os.path.basename(member.name)\n            if ""model.ckpt"" in member.name:\n                print(""Extracting {}..."".format(member.name))\n                tar.extract(member, path=checkpoint_dir)\n\n    ############################################################################\n    # Create pipeline.config\n    ############################################################################\n    fill_num_classes = str(len(labels))\n    fill_label_map = label_map_path\n    fill_train_record = train_output_path + ""-?????-of-{:05}"".format(train_shards)\n    fill_val_record = val_output_path + ""-?????-of-{:05}"".format(val_shards)\n    fill_checkpoint = os.path.join(checkpoint_dir, ""model.ckpt"")\n\n    skeleton_path = ""pipeline_skeleton.config""\n    pipeline_path = ""pipeline.config""\n\n    with open(skeleton_path, ""r"") as skeleton:\n        with open(pipeline_path, ""w"") as pipeline:\n            for line in skeleton:\n                new_line = line.replace(""${NUM_CLASSES}"", fill_num_classes)\n                new_line = new_line.replace(""${LABEL_MAP}"", fill_label_map)\n                new_line = new_line.replace(""${TRAIN_RECORD}"", fill_train_record)\n                new_line = new_line.replace(""${VAL_RECORD}"", fill_val_record)\n                new_line = new_line.replace(""${CHECKPOINT}"", fill_checkpoint)\n                pipeline.write(new_line)\n\n\nif __name__ == ""__main__"":\n    main()\n'"
