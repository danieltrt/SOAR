file_path,api_count,code
main_classification.py,0,"b'# coding=utf8\n# This code is adapted from the https://github.com/tensorflow/models/tree/master/official/r1/resnet.\n# ==========================================================================================\n# NAVER\xe2\x80\x99s modifications are Copyright 2020 NAVER corp. All rights reserved.\n# ==========================================================================================\n# Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom absl import app as absl_app\nfrom absl import flags\n\nfrom functions import input_fns, data_config, model_fns\nfrom nets import run_loop_classification\nfrom official.utils.flags import core as flags_core\nfrom official.utils.logs import logger\nfrom utils import log_utils\nfrom utils import config_utils\n\ndef define_flags():\n  run_loop_classification.define_resnet_flags(\n    resnet_size_choices=[\'18\', \'34\', \'50\', \'101\', \'152\', \'200\'])\n  flags.adopt_module_key_flags(run_loop_classification)\n  flags_core.set_defaults(train_epochs=90)\n\n\ndef run(flags_obj):\n  """"""Run ResNet ImageNet training and eval loop.\n\n  Args:\n    flags_obj: An object containing parsed flag values.\n  """"""\n  if not flags_obj.eval_only:\n    config_utils.dump_hparam()\n  dataset = data_config.get_config(flags_obj.dataset_name)\n\n  run_loop_classification.resnet_main(\n    flags_obj, model_fns.model_fn_cls, input_fns.input_fn_cls, dataset.dataset_name,\n    shape=[dataset.default_image_size, dataset.default_image_size, dataset.num_channels],\n    num_images=dataset.num_images, zeroshot_eval=flags_obj.zeroshot_eval)\n\ndef main(_):\n  with logger.benchmark_context(flags.FLAGS):\n    run(flags.FLAGS)\n\nif __name__ == \'__main__\':\n  # \xeb\xa1\x9c\xea\xb7\xb8 \xeb\x91\x90\xeb\xb2\x88 \xeb\x82\x98\xec\x98\xa4\xeb\x8a\x94 \xea\xb2\x83 fix\n  log_utils.define_log_level()\n  define_flags()\n  absl_app.run(main)\n'"
datasets/__init__.py,0,b''
datasets/build_cars196.py,4,"b'#!/usr/bin/env python\n# coding=utf8\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\n""""""CAR196 \xeb\xa5\xbc \xec\x83\x9d\xec\x84\xb1\xed\x95\x9c\xeb\x8b\xa4.\nexample:\nCUDA_VISIBLE_DEVICES="""" python build_cars196.py \\\n--data_dir=cars196 \\\n--output_dir=cars196/tfrecord_cls \\\n--use_bbox=False \n""""""\n\nimport os\nimport argparse\nimport sys\n\nsys.path.append(""../datasets"")\nsys.path.append(""./datasets"")\nsys.path.append("".."")\nsys.path.append(""."")\nfrom datetime import datetime\nfrom datasets import dataset_utils\nfrom datasets import image_coder as coder\nfrom utils import data_util\nfrom scipy.io import loadmat\nimport tarfile\nimport subprocess\n\nimport tensorflow as tf\nimport numpy as np\nimport random\n\n_NUM_CLASSES = 196\n\n# Just disables the warning, doesn\'t enable AVX/FMA\nos.environ[\'TF_CPP_MIN_LOG_LEVEL\'] = \'2\'\n\nparser = argparse.ArgumentParser()\n\nparser.add_argument(\'-d\', \'--data_dir\', type=str, default=None, help=\'Input data directory.\')\nparser.add_argument(\'-o\', \'--output_dir\', type=str, default=None, help=\'Output data directory.\')\nparser.add_argument(\'--train_shards\', type=int, default=128, help=\'Number of shards in training TFRecord files.\')\nparser.add_argument(\'--validation_shards\', type=int, default=16, help=\'Number of shards in validation TFRecord files.\')\nparser.add_argument(\'--num_threads\', type=int, default=8, help=\'Number of threads to preprocess the images.\')\nparser.add_argument(\'--use_bbox\', type=dataset_utils.str2bool, nargs=\'?\', const=True, default=False,\n                    help=\'Whether to use bounding boxes or not.\')\n\n\ndef _write_label_id_to_name(name, data_dir, id_to_name):\n  output_filename = \'%s_labels.txt\' % (name)\n  output_file = os.path.join(data_dir, output_filename)\n  with open(output_file, \'w\') as f:\n    for index in sorted(id_to_name):\n      f.write(\'%d:%s\\n\' % (index, id_to_name[index]))\n\n\ndef _get_bbox_info(dataset_name=""cars196""):\n  fuel_data_path = dataset_name\n  label_filepath = os.path.join(fuel_data_path, ""cars_annos.mat"")\n  # Extract class labels\n  cars_annos = loadmat(label_filepath)\n  annotations = cars_annos[""annotations""].ravel()\n  annotations = sorted(annotations, key=lambda a: str(a[0][0]))\n\n  bbox_info = {}\n  for i in range(len(annotations)):\n    filename = annotations[i][0][0]\n    file_id = os.path.splitext(os.path.basename(filename))[0]\n    #   print(file_id)\n    xmin = annotations[i][1][0][0]\n    ymin = annotations[i][2][0][0]\n    xmax = annotations[i][3][0][0]\n    ymax = annotations[i][4][0][0]\n    bbox = [int(xmin), int(ymin), int(xmax), int(ymax)]\n    #   print(bbox)\n    bbox_info[file_id] = bbox\n  return bbox_info\n\n\ndef _find_image_files(dataset_name=""cars196"", archive_basename=""car_ims""):\n  fuel_data_path = dataset_name\n  image_filepath = os.path.join(fuel_data_path, archive_basename + "".tgz"")\n  label_filepath = os.path.join(fuel_data_path, ""cars_annos.mat"")\n  # Extract class labels\n  cars_annos = loadmat(label_filepath)\n  annotations = cars_annos[""annotations""].ravel()\n  annotations = sorted(annotations, key=lambda a: str(a[0][0]))\n  class_labels = []\n  for annotation in annotations:\n    class_label = int(annotation[5])\n    class_labels.append(class_label)\n\n  if not os.path.exists(os.path.join(fuel_data_path, archive_basename)):\n    subprocess.call([""tar"", ""zxvf"", image_filepath.replace(""\\\\"", ""/""),\n                     ""-C"", fuel_data_path.replace(""\\\\"", ""/""),\n                     ""--force-local""])\n\n  train_filenames = []\n  val_filenames = []\n  train_labels = []\n  val_labels = []\n\n  def append_data(filenames, labels, label_idx, fn, fuel_data_path):\n    # if not filenames[label_idx]:\n    #   filenames[label_idx] = []\n    # if not labels[label_idx]:\n    #   labels[label_idx] = []\n\n    filenames.append(os.path.join(os.getcwd(), fuel_data_path, fn))\n    labels.append(label_idx)\n    return filenames, labels\n\n  for annotation in annotations:\n    label_idx = int(annotation[5]) - 1\n    fn = annotation[0][0]\n    if int(annotation[6]) == 0:\n      train_filenames, train_labels = append_data(train_filenames, train_labels, label_idx, fn, fuel_data_path)\n    else:\n      val_filenames, val_labels = append_data(val_filenames, val_labels, label_idx, fn, fuel_data_path)\n\n  shuffled_index = list(range(len(train_filenames)))\n  random.seed(12345)\n  random.shuffle(shuffled_index)\n  train_filenames = [train_filenames[i] for i in shuffled_index]\n  train_labels = [train_labels[i] for i in shuffled_index]\n\n  sum_train = len(train_filenames)\n  sum_val = len(val_filenames)\n\n  # sum_train += len(f)\n  # for f in val_filenames:\n  #   sum_val += len(f)\n\n  return train_filenames, val_filenames, train_labels, val_labels, sum_train, sum_val\n\n\ndef _process_image(filename, bbox=None):\n  """"""\n  \xec\x9d\xb4\xeb\xaf\xb8\xec\xa7\x80 \xed\x8c\x8c\xec\x9d\xbc\xec\x9d\x84 \xec\x9d\xbd\xec\x96\xb4\xeb\x93\xa4\xec\x97\xac RGB \xed\x83\x80\xec\x9e\x85\xec\x9c\xbc\xeb\xa1\x9c \xeb\xb3\x80\xed\x99\x98\xed\x95\x98\xec\x97\xac \xeb\xb0\x98\xed\x99\x98\xed\x95\x9c\xeb\x8b\xa4.\n  :param filename: string, \xec\x9d\xbd\xec\x96\xb4\xeb\x93\xa4\xec\x9d\xbc \xec\x9d\xb4\xeb\xaf\xb8\xec\xa7\x80 \xed\x8c\x8c\xec\x9d\xbc \xea\xb2\xbd\xeb\xa1\x9c. e.g., \'/path/to/example.JPG\'.\n  :param bbox: [xmin, ymin, xmax, ymax] \xed\x98\x95\xec\x8b\x9d\xec\x9d\x98 bbox \xeb\x8d\xb0\xec\x9d\xb4\xed\x84\xb0 \xeb\x98\x90\xeb\x8a\x94 None\n  :return:\n    image_data: \xec\x9d\xb4\xeb\xaf\xb8\xec\xa7\x80 \xeb\x8d\xb0\xec\x9d\xb4\xed\x84\xb0\xeb\xa1\x9c jpg \xed\x8f\xac\xeb\xa7\xb7\xec\x9d\x98 \xeb\x8d\xb0\xec\x9d\xb4\xed\x84\xb0.\n    height: \xec\x9d\xb4\xeb\xaf\xb8\xec\xa7\x80 height\n    width: \xec\x9d\xb4\xeb\xaf\xb8\xec\xa7\x80 width\n  """"""\n  with tf.gfile.GFile(filename, \'rb\') as f:\n    image_data = f.read()\n    image_format = dataset_utils.get_image_file_format(filename)\n\n    # try:\n    image = coder.decode_jpg(image_data)\n    height = image.shape[0]\n    width = image.shape[1]\n    # except tf.errors.InvalidArgumentError:\n    #   raise ValueError(""Invalid decode in {}"".format(filename))\n\n  if bbox is None:\n    return image_data, height, width, image_format\n  else:\n    # change bbox to [y, x, h, w]\n    crop_window = [bbox[1], bbox[0], bbox[3] - bbox[1], bbox[2] - bbox[0]]\n\n    # \xec\xa0\x84\xec\xb2\xb4 \xec\x9d\xb4\xeb\xaf\xb8\xec\xa7\x80 \xed\x81\xac\xea\xb8\xb0\xeb\xb3\xb4\xeb\x8b\xa4 bbox \xec\x98\x81\xec\x97\xad\xec\x9d\xb4 \xeb\x84\x98\xec\x96\xb4\xea\xb0\x80\xeb\x8a\x94 \xea\xb2\xbd\xec\x9a\xb0 \xec\x97\x90\xeb\x9f\xac\xea\xb0\x80 \xeb\xb0\x9c\xec\x83\x9d\xed\x95\x9c\xeb\x8b\xa4.\n    # \xec\x9d\xb4\xeb\xa5\xbc \xeb\xa7\x89\xea\xb8\xb0\xec\x9c\x84\xed\x95\x9c \xeb\xb3\xb4\xec\xa0\x95 \xec\xbd\x94\xeb\x93\x9c\xeb\xa5\xbc \xec\xb6\x94\xea\xb0\x80\xed\x95\x9c\xeb\x8b\xa4.\n    h_gap = crop_window[2] + crop_window[0] - height\n    w_gap = crop_window[3] + crop_window[1] - width\n    if h_gap > 0:\n      crop_window[2] -= h_gap\n    if w_gap > 0:\n      crop_window[3] -= w_gap\n    assert crop_window[2] > 0\n    assert crop_window[3] > 0\n\n    image = coder.crop_bbox(image, crop_window)\n    image_data = coder.encode_jpg(image)\n    image_format = \'jpg\'\n    return image_data, crop_window[2], crop_window[3], image_format\n\n\ndef _process_image_files_batch(thread_index, offsets, output_filenames, filenames, labels):\n  """"""\n  \xed\x95\x98\xeb\x82\x98\xec\x9d\x98 \xec\x8a\xa4\xeb\xa0\x88\xeb\x93\x9c \xeb\x8b\xa8\xec\x9c\x84\xec\x97\x90\xec\x84\x9c \xec\x9d\xb4\xeb\xaf\xb8\xec\xa7\x80 \xeb\xa6\xac\xec\x8a\xa4\xed\x8a\xb8\xeb\xa5\xbc \xec\x9d\xbd\xec\x96\xb4 TRRecord \xed\x83\x80\xec\x9e\x85\xec\x9c\xbc\xeb\xa1\x9c \xeb\xb3\x80\xed\x99\x98\xed\x95\x98\xeb\x8a\x94 \xed\x95\xa8\xec\x88\x98\n  :param thread_index: \xed\x98\x84\xec\x9e\xac \xec\x9e\x91\xec\x97\x85\xec\xa4\x91\xec\x9d\xb8 thread \xeb\xb2\x88\xed\x98\xb8.\n  :param offsets: offset list. \xec\x9d\xb4\xeb\xaf\xb8\xec\xa7\x80 \xeb\xaa\xa9\xeb\xa1\x9d \xec\xa4\x91 \xed\x98\x84\xec\x9e\xac \xec\x8a\xa4\xeb\xa0\x88\xeb\x93\x9c\xec\x97\x90\xec\x84\x9c \xec\xb2\x98\xeb\xa6\xac\xed\x95\xb4\xec\x95\xbc \xed\x95\xa0 offset \xea\xb0\x92\xec\x9c\xbc\xeb\xa1\x9c shard \xea\xb0\xaf\xec\x88\x98\xeb\xa7\x8c\xed\x81\xbc \xeb\xa6\xac\xec\x8a\xa4\xed\x8a\xb8\xeb\xa1\x9c \xec\xa0\x9c\xea\xb3\xb5\n  :param output_filenames: \xec\xb6\x9c\xeb\xa0\xa5 \xed\x8c\x8c\xec\x9d\xbc \xec\x9d\xb4\xeb\xa6\x84\xec\x9c\xbc\xeb\xa1\x9c shard \xea\xb0\xaf\xec\x88\x98\xeb\xa7\x8c\xed\x81\xbc \xeb\xa6\xac\xec\x8a\xa4\xed\x8a\xb8\xeb\xa1\x9c \xec\xa0\x9c\xea\xb3\xb5.\n  :param filenames: \xec\xb2\x98\xeb\xa6\xac\xed\x95\xb4\xec\x95\xbc \xed\x95\xa0 \xec\xa0\x84\xec\xb2\xb4 \xec\x9d\xb4\xeb\xaf\xb8\xec\xa7\x80 \xed\x8c\x8c\xec\x9d\xbc \xeb\xa6\xac\xec\x8a\xa4\xed\x8a\xb8\n  :param labels: \xec\xb2\x98\xeb\xa6\xac\xed\x95\xb4\xec\x95\xbc \xed\x95\xa0 \xec\xa0\x84\xec\xb2\xb4 \xec\x9d\xb4\xeb\xaf\xb8\xec\xa7\x80 \xeb\xa0\x88\xec\x9d\xb4\xeb\xb8\x94 \xeb\xa6\xac\xec\x8a\xa4\xed\x8a\xb8\n  """"""\n  assert len(offsets) == len(output_filenames)\n  assert len(filenames) == len(labels)\n\n  num_files_in_thread = offsets[-1][1] - offsets[0][0]\n  counter = 0\n  # \xed\x95\x98\xeb\x82\x98\xec\x9d\x98 thread \xec\x97\x90\xeb\x8a\x94 \xec\x97\xac\xeb\x9f\xac \xea\xb0\x9c\xec\x9d\x98 shard \xea\xb0\x80 \xed\x95\xa0\xeb\x8b\xb9\xeb\x90\xa0 \xec\x88\x98 \xec\x9e\x88\xeb\x8b\xa4.\n  for offset, output_filename in zip(offsets, output_filenames):\n    output_file = os.path.join(FLAGS.output_dir, output_filename)\n    writer = tf.python_io.TFRecordWriter(output_file)\n\n    # offset \xec\x97\x90\xeb\x8a\x94 \xed\x98\x84\xec\x9e\xac shard \xec\x97\x90 \xeb\x8c\x80\xed\x95\x9c (start, end) offset\xec\x9d\xb4 \xec\xa0\x80\xec\x9e\xa5\xeb\x90\x98\xec\x96\xb4 \xec\x9e\x88\xec\x9d\x8c.\n    files_in_shard = np.arange(offset[0], offset[1], dtype=int)\n    shard_counter = 0\n    for i in files_in_shard:\n      filename = filenames[i]\n      label = labels[i]\n\n      # try:\n      image_data, height, width, image_format = _process_image(filename)\n      # except ValueError:\n      #   dataset_utils.log(\'[thread %2d]: Invalid image found. %s - [skip].\' % (thread_index, filename))\n      #   continue\n\n      example = data_util.convert_to_example_without_bbox(image_data, \'jpg\', label, height, width)\n      writer.write(example.SerializeToString())\n\n      counter += 1\n      shard_counter += 1\n      if not counter % 1000:\n        dataset_utils.log(\'%s [thread %2d]: Processed %d of %d images in thread batch.\' %\n                          (datetime.now(), thread_index, counter, num_files_in_thread))\n\n    writer.close()\n    dataset_utils.log(\'%s [thread %2d]: Wrote %d images to %s\' %\n                      (datetime.now(), thread_index, shard_counter, output_file))\n\n\ndef _process_dataset(name, filenames, labels, num_shards):\n  """"""\n  \xec\x9d\xb4\xeb\xaf\xb8\xec\xa7\x80 \xed\x8c\x8c\xec\x9d\xbc \xeb\xaa\xa9\xeb\xa1\x9d\xec\x9d\x84 \xec\x9d\xbd\xec\x96\xb4\xeb\x93\xa4\xec\x97\xac TFRecord \xea\xb0\x9d\xec\xb2\xb4\xeb\xa1\x9c \xeb\xb3\x80\xed\x99\x98\xed\x95\x98\xeb\x8a\x94 \xed\x95\xa8\xec\x88\x98\n  :param name: string, \xeb\x8d\xb0\xec\x9d\xb4\xed\x84\xb0 \xea\xb3\xa0\xec\x9c\xa0 \xeb\xac\xb8\xec\x9e\x90\xec\x97\xb4 (train, validation \xeb\x93\xb1)\n  :param filenames: list of strings; \xec\x9d\xb4\xeb\xaf\xb8\xec\xa7\x80 \xed\x8c\x8c\xec\x9d\xbc \xea\xb2\xbd\xeb\xa1\x9c \xeb\xa6\xac\xec\x8a\xa4\xed\x8a\xb8.\n  :param labels: list of integer; \xec\x9d\xb4\xeb\xaf\xb8\xec\xa7\x80\xec\x97\x90 \xeb\x8c\x80\xed\x95\x9c \xec\xa0\x95\xec\x88\x98\xed\x99\x94\xeb\x90\x9c \xec\xa0\x95\xeb\x8b\xb5 \xeb\xa0\x88\xec\x9d\xb4\xeb\xb8\x94 \xeb\xa6\xac\xec\x8a\xa4\xed\x8a\xb8\n  :param num_shards: \xeb\x8d\xb0\xec\x9d\xb4\xed\x84\xb0 \xec\xa7\x91\xed\x95\xa9\xec\x9d\x84 \xec\x83\xa4\xeb\x94\xa9\xed\x95\xa0 \xea\xb0\xaf\xec\x88\x98.\n  """"""\n  assert len(filenames) == len(labels)\n\n  shard_offsets = dataset_utils.make_shard_offsets(len(filenames), FLAGS.num_threads, num_shards)\n  shard_output_filenames = dataset_utils.make_shard_filenames(name, len(filenames), FLAGS.num_threads, num_shards)\n\n  def _process_batch(thread_index):\n    offsets = shard_offsets[thread_index]\n    output_filenames = shard_output_filenames[thread_index]\n    _process_image_files_batch(thread_index, offsets, output_filenames, filenames, labels)\n\n  dataset_utils.thread_execute(FLAGS.num_threads, _process_batch)\n  dataset_utils.log(\'%s: Finished writing all %d images in data set.\' % (datetime.now(), len(filenames)))\n\n\ndef main(unused_argv):\n  if (FLAGS.data_dir is None) or (FLAGS.output_dir is None):\n    parser.print_help()\n    return\n\n  if not os.path.exists(FLAGS.output_dir):\n    os.makedirs(FLAGS.output_dir)\n\n  if FLAGS.use_bbox:\n    bbox_info = _get_bbox_info()\n    dataset_utils.log(\' - Use bounding box info. (opt. ON)\')\n  else:\n    bbox_info = None\n\n  filenames_train, filenames_val, labels_train, labels_val, total_train, total_val \\\n    = _find_image_files(FLAGS.data_dir)\n  # _process_dataset(\'train\', filenames_train, labels_train, bbox_info)\n  # # _write_label_id_to_name(\'train\', FLAGS.output_dir, id_to_name)\n  # dataset_utils.log(\'Finished writing all %d images in train data set.\' % total_train)\n  #\n  # _process_dataset(\'validation\', filenames_val, labels_val, bbox_info)\n  # # _write_label_id_to_name(\'validation\', FLAGS.output_dir, id_to_name)\n  # dataset_utils.log(\'Finished writing all %d images in validation data set.\' % total_val)\n\n  print(\'filenames_train\', len(filenames_train))\n  dataset_utils.log(\'Convert [train] dataset.\')\n  _process_dataset(\'train\', filenames_train, labels_train, FLAGS.train_shards)\n\n  dataset_utils.log(\'Convert [validation] dataset.\')\n  _process_dataset(\'validation\', filenames_val, labels_val, FLAGS.validation_shards)\n\n\nif __name__ == \'__main__\':\n  FLAGS, unparsed = parser.parse_known_args()\n  tf.app.run(argv=[sys.argv[0]] + unparsed)\n'"
datasets/build_cars196_zeroshot.py,5,"b'#!/usr/bin/env python\n# coding=utf8\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\n""""""HDML \xeb\x85\xbc\xeb\xac\xb8\xec\x9d\x98 \xec\x84\xb1\xeb\x8a\xa5 \xeb\xb9\x84\xea\xb5\x90\xeb\xa5\xbc \xec\x9c\x84\xed\x95\xb4 CAR196 \xeb\xa5\xbc \xec\x83\x9d\xec\x84\xb1\xed\x95\x9c\xeb\x8b\xa4.\nexample:\nCUDA_VISIBLE_DEVICES="""" python datasets/build_cars196.py \\\n--data_dir=cars196 \\\n--output_dir=cars196/tfrecord_crop \\\n--use_bbox=True \n""""""\n\nimport os\nimport argparse\nimport sys\nimport random\n\nsys.path.append(""../datasets"")\nsys.path.append(""./datasets"")\nsys.path.append("".."")\nsys.path.append(""."")\nfrom datetime import datetime\nfrom datasets import dataset_utils\nfrom datasets import image_coder as coder\nfrom utils import data_util\nfrom scipy.io import loadmat\nimport tarfile\nimport subprocess\n\nimport tensorflow as tf\nimport numpy as np\n\n_NUM_CLASSES = 196\n_TRAIN_CLASS_RANGE = list(range(0, 98))\n_VALIDATION_CLASS_RANGE = list(range(98, 196))\n\n# Just disables the warning, doesn\'t enable AVX/FMA\nos.environ[\'TF_CPP_MIN_LOG_LEVEL\'] = \'2\'\n\n\ndef _write_label_id_to_name(name, data_dir, id_to_name):\n  output_filename = \'%s_labels.txt\' % (name)\n  output_file = os.path.join(data_dir, output_filename)\n  with open(output_file, \'w\') as f:\n    for index in sorted(id_to_name):\n      f.write(\'%d:%s\\n\' % (index, id_to_name[index]))\n\n\ndef _get_bbox_info(dataset_name=""cars196""):\n  fuel_data_path = dataset_name\n  label_filepath = os.path.join(fuel_data_path, ""cars_annos.mat"")\n  # Extract class labels\n  cars_annos = loadmat(label_filepath)\n  annotations = cars_annos[""annotations""].ravel()\n  annotations = sorted(annotations, key=lambda a: str(a[0][0]))\n\n  bbox_info = {}\n  for i in range(len(annotations)):\n    filename = annotations[i][0][0]\n    file_id = os.path.splitext(os.path.basename(filename))[0]\n    #   print(file_id)\n    xmin = annotations[i][1][0][0]\n    ymin = annotations[i][2][0][0]\n    xmax = annotations[i][3][0][0]\n    ymax = annotations[i][4][0][0]\n    bbox = [int(xmin), int(ymin), int(xmax), int(ymax)]\n    #   print(bbox)\n    bbox_info[file_id] = bbox\n  return bbox_info\n\n\ndef _find_image_files(name, data_dir=""cars196"", archive_basename=""car_ims""):\n  if name == \'train\':\n    label_index_range = _TRAIN_CLASS_RANGE\n  elif name == \'validation\':\n    label_index_range = _VALIDATION_CLASS_RANGE\n  else:\n    raise ValueError(\'Invalid index range\')\n\n  fuel_data_path = data_dir\n  image_filepath = os.path.join(fuel_data_path, archive_basename + "".tgz"")\n  label_filepath = os.path.join(fuel_data_path, ""cars_annos.mat"")\n  # Extract class labels\n  cars_annos = loadmat(label_filepath)\n  annotations = cars_annos[""annotations""].ravel()\n  annotations = sorted(annotations, key=lambda a: str(a[0][0]))\n  class_labels = []\n  for annotation in annotations:\n    class_label = int(annotation[5])\n    class_labels.append(class_label)\n\n  with tarfile.open(image_filepath, ""r"") as tf:\n    jpg_filenames = [fn for fn in tf.getnames() if fn.endswith("".jpg"")]\n\n  jpg_filenames.sort()\n\n  # if not os.path.exists(os.path.join(fuel_data_path, archive_basename)):\n  #   subprocess.call([""tar"", ""zxvf"", image_filepath.replace(""\\\\"", ""/""),\n  #                    ""-C"", fuel_data_path.replace(""\\\\"", ""/""),\n  #                    ""--force-local""])\n\n  num_classes = max(class_labels)\n\n  filenames_car = [None] * num_classes\n  labels_car = [None] * num_classes\n  for i in range(len(class_labels)):\n    label_idx = class_labels[i] - 1\n    fn = jpg_filenames[i]\n\n    if label_idx not in label_index_range:\n      continue\n\n    if not filenames_car[label_idx]:\n      filenames_car[label_idx] = []\n    if not labels_car[label_idx]:\n      labels_car[label_idx] = []\n\n    filenames_car[label_idx].append(os.path.join(os.getcwd(), fuel_data_path, fn))\n    labels_car[label_idx].append(label_idx)\n\n  filenames = []\n  labels = []\n  total = 0\n  # print(\'filenames_car\', filenames_car)\n  for label_idx in range(num_classes):\n    if label_idx not in label_index_range:\n      continue\n    total += len(filenames_car[label_idx])\n    filenames.extend(filenames_car[label_idx])\n    labels.extend(labels_car[label_idx])\n\n  shuffled_index = list(range(len(filenames)))\n  random.seed(12345)\n  random.shuffle(shuffled_index)\n  filenames = [filenames[i] for i in shuffled_index]\n  labels = [labels[i] for i in shuffled_index]\n\n  return filenames, labels, total\n\n\ndef _process_image(filename, bbox=None):\n  """"""\n  \xec\x9d\xb4\xeb\xaf\xb8\xec\xa7\x80 \xed\x8c\x8c\xec\x9d\xbc\xec\x9d\x84 \xec\x9d\xbd\xec\x96\xb4\xeb\x93\xa4\xec\x97\xac RGB \xed\x83\x80\xec\x9e\x85\xec\x9c\xbc\xeb\xa1\x9c \xeb\xb3\x80\xed\x99\x98\xed\x95\x98\xec\x97\xac \xeb\xb0\x98\xed\x99\x98\xed\x95\x9c\xeb\x8b\xa4.\n  :param filename: string, \xec\x9d\xbd\xec\x96\xb4\xeb\x93\xa4\xec\x9d\xbc \xec\x9d\xb4\xeb\xaf\xb8\xec\xa7\x80 \xed\x8c\x8c\xec\x9d\xbc \xea\xb2\xbd\xeb\xa1\x9c. e.g., \'/path/to/example.JPG\'.\n  :param bbox: [xmin, ymin, xmax, ymax] \xed\x98\x95\xec\x8b\x9d\xec\x9d\x98 bbox \xeb\x8d\xb0\xec\x9d\xb4\xed\x84\xb0 \xeb\x98\x90\xeb\x8a\x94 None\n  :return:\n    image_data: \xec\x9d\xb4\xeb\xaf\xb8\xec\xa7\x80 \xeb\x8d\xb0\xec\x9d\xb4\xed\x84\xb0\xeb\xa1\x9c jpg \xed\x8f\xac\xeb\xa7\xb7\xec\x9d\x98 \xeb\x8d\xb0\xec\x9d\xb4\xed\x84\xb0.\n    height: \xec\x9d\xb4\xeb\xaf\xb8\xec\xa7\x80 height\n    width: \xec\x9d\xb4\xeb\xaf\xb8\xec\xa7\x80 width\n  """"""\n  with tf.gfile.GFile(filename, \'rb\') as f:\n    image_data = f.read()\n    image_format = dataset_utils.get_image_file_format(filename)\n\n    # try:\n    image = coder.decode_jpg(image_data)\n    height = image.shape[0]\n    width = image.shape[1]\n    # except tf.errors.InvalidArgumentError:\n    #   raise ValueError(""Invalid decode in {}"".format(filename))\n\n  if bbox is None:\n    return image_data, height, width, image_format\n  else:\n    # change bbox to [y, x, h, w]\n    crop_window = [bbox[1], bbox[0], bbox[3] - bbox[1], bbox[2] - bbox[0]]\n\n    # \xec\xa0\x84\xec\xb2\xb4 \xec\x9d\xb4\xeb\xaf\xb8\xec\xa7\x80 \xed\x81\xac\xea\xb8\xb0\xeb\xb3\xb4\xeb\x8b\xa4 bbox \xec\x98\x81\xec\x97\xad\xec\x9d\xb4 \xeb\x84\x98\xec\x96\xb4\xea\xb0\x80\xeb\x8a\x94 \xea\xb2\xbd\xec\x9a\xb0 \xec\x97\x90\xeb\x9f\xac\xea\xb0\x80 \xeb\xb0\x9c\xec\x83\x9d\xed\x95\x9c\xeb\x8b\xa4.\n    # \xec\x9d\xb4\xeb\xa5\xbc \xeb\xa7\x89\xea\xb8\xb0\xec\x9c\x84\xed\x95\x9c \xeb\xb3\xb4\xec\xa0\x95 \xec\xbd\x94\xeb\x93\x9c\xeb\xa5\xbc \xec\xb6\x94\xea\xb0\x80\xed\x95\x9c\xeb\x8b\xa4.\n    h_gap = crop_window[2] + crop_window[0] - height\n    w_gap = crop_window[3] + crop_window[1] - width\n    if h_gap > 0:\n      crop_window[2] -= h_gap\n    if w_gap > 0:\n      crop_window[3] -= w_gap\n    assert crop_window[2] > 0\n    assert crop_window[3] > 0\n\n    image = coder.crop_bbox(image, crop_window)\n    image_data = coder.encode_jpg(image)\n    image_format = \'jpg\'\n    return image_data, crop_window[2], crop_window[3], image_format\n\n\ndef _process_image_files_batch(thread_index, offsets, output_filenames, filenames, labels, bbox_info):\n  """"""\n  \xed\x95\x98\xeb\x82\x98\xec\x9d\x98 \xec\x8a\xa4\xeb\xa0\x88\xeb\x93\x9c \xeb\x8b\xa8\xec\x9c\x84\xec\x97\x90\xec\x84\x9c \xec\x9d\xb4\xeb\xaf\xb8\xec\xa7\x80 \xeb\xa6\xac\xec\x8a\xa4\xed\x8a\xb8\xeb\xa5\xbc \xec\x9d\xbd\xec\x96\xb4 TRRecord \xed\x83\x80\xec\x9e\x85\xec\x9c\xbc\xeb\xa1\x9c \xeb\xb3\x80\xed\x99\x98\xed\x95\x98\xeb\x8a\x94 \xed\x95\xa8\xec\x88\x98\n  :param thread_index: \xed\x98\x84\xec\x9e\xac \xec\x9e\x91\xec\x97\x85\xec\xa4\x91\xec\x9d\xb8 thread \xeb\xb2\x88\xed\x98\xb8.\n  :param offsets: offset list. \xec\x9d\xb4\xeb\xaf\xb8\xec\xa7\x80 \xeb\xaa\xa9\xeb\xa1\x9d \xec\xa4\x91 \xed\x98\x84\xec\x9e\xac \xec\x8a\xa4\xeb\xa0\x88\xeb\x93\x9c\xec\x97\x90\xec\x84\x9c \xec\xb2\x98\xeb\xa6\xac\xed\x95\xb4\xec\x95\xbc \xed\x95\xa0 offset \xea\xb0\x92\xec\x9c\xbc\xeb\xa1\x9c shard \xea\xb0\xaf\xec\x88\x98\xeb\xa7\x8c\xed\x81\xbc \xeb\xa6\xac\xec\x8a\xa4\xed\x8a\xb8\xeb\xa1\x9c \xec\xa0\x9c\xea\xb3\xb5\n  :param output_filenames: \xec\xb6\x9c\xeb\xa0\xa5 \xed\x8c\x8c\xec\x9d\xbc \xec\x9d\xb4\xeb\xa6\x84\xec\x9c\xbc\xeb\xa1\x9c shard \xea\xb0\xaf\xec\x88\x98\xeb\xa7\x8c\xed\x81\xbc \xeb\xa6\xac\xec\x8a\xa4\xed\x8a\xb8\xeb\xa1\x9c \xec\xa0\x9c\xea\xb3\xb5.\n  :param filenames: \xec\xb2\x98\xeb\xa6\xac\xed\x95\xb4\xec\x95\xbc \xed\x95\xa0 \xec\xa0\x84\xec\xb2\xb4 \xec\x9d\xb4\xeb\xaf\xb8\xec\xa7\x80 \xed\x8c\x8c\xec\x9d\xbc \xeb\xa6\xac\xec\x8a\xa4\xed\x8a\xb8\n  :param labels: \xec\xb2\x98\xeb\xa6\xac\xed\x95\xb4\xec\x95\xbc \xed\x95\xa0 \xec\xa0\x84\xec\xb2\xb4 \xec\x9d\xb4\xeb\xaf\xb8\xec\xa7\x80 \xeb\xa0\x88\xec\x9d\xb4\xeb\xb8\x94 \xeb\xa6\xac\xec\x8a\xa4\xed\x8a\xb8\n  """"""\n  assert len(offsets) == len(output_filenames)\n  assert len(filenames) == len(labels)\n\n  num_files_in_thread = offsets[-1][1] - offsets[0][0]\n  counter = 0\n  # \xed\x95\x98\xeb\x82\x98\xec\x9d\x98 thread \xec\x97\x90\xeb\x8a\x94 \xec\x97\xac\xeb\x9f\xac \xea\xb0\x9c\xec\x9d\x98 shard \xea\xb0\x80 \xed\x95\xa0\xeb\x8b\xb9\xeb\x90\xa0 \xec\x88\x98 \xec\x9e\x88\xeb\x8b\xa4.\n  for offset, output_filename in zip(offsets, output_filenames):\n    output_file = os.path.join(FLAGS.output_dir, output_filename)\n    writer = tf.python_io.TFRecordWriter(output_file)\n\n    # offset \xec\x97\x90\xeb\x8a\x94 \xed\x98\x84\xec\x9e\xac shard \xec\x97\x90 \xeb\x8c\x80\xed\x95\x9c (start, end) offset\xec\x9d\xb4 \xec\xa0\x80\xec\x9e\xa5\xeb\x90\x98\xec\x96\xb4 \xec\x9e\x88\xec\x9d\x8c.\n    files_in_shard = np.arange(offset[0], offset[1], dtype=int)\n    shard_counter = 0\n    for i in files_in_shard:\n      filename = filenames[i]\n      label = labels[i]\n\n      file_id = os.path.splitext(os.path.basename(filename))[0]\n      if bbox_info is None:\n        bbox = None\n      else:\n        bbox = bbox_info[file_id]\n\n      # try:\n      image_data, height, width, image_format = _process_image(filename, bbox)\n      # except ValueError:\n      #   dataset_utils.log(\'[thread %2d]: Invalid image found. %s - [skip].\' % (thread_index, filename))\n      #   continue\n\n      example = data_util.convert_to_example_without_bbox(image_data, \'jpg\', label, height, width)\n      writer.write(example.SerializeToString())\n\n      counter += 1\n      shard_counter += 1\n      if not counter % 1000:\n        dataset_utils.log(\'%s [thread %2d]: Processed %d of %d images in thread batch.\' %\n                          (datetime.now(), thread_index, counter, num_files_in_thread))\n\n    writer.close()\n    dataset_utils.log(\'%s [thread %2d]: Wrote %d images to %s\' %\n                      (datetime.now(), thread_index, shard_counter, output_file))\n\n\ndef _process_dataset(name, filenames, labels, bbox_info, num_shards=128):\n  """"""\n  \xec\x9d\xb4\xeb\xaf\xb8\xec\xa7\x80 \xed\x8c\x8c\xec\x9d\xbc \xeb\xaa\xa9\xeb\xa1\x9d\xec\x9d\x84 \xec\x9d\xbd\xec\x96\xb4\xeb\x93\xa4\xec\x97\xac TFRecord \xea\xb0\x9d\xec\xb2\xb4\xeb\xa1\x9c \xeb\xb3\x80\xed\x99\x98\xed\x95\x98\xeb\x8a\x94 \xed\x95\xa8\xec\x88\x98\n  :param name: string, \xeb\x8d\xb0\xec\x9d\xb4\xed\x84\xb0 \xea\xb3\xa0\xec\x9c\xa0 \xeb\xac\xb8\xec\x9e\x90\xec\x97\xb4 (train, validation \xeb\x93\xb1)\n  :param filenames: list of strings; \xec\x9d\xb4\xeb\xaf\xb8\xec\xa7\x80 \xed\x8c\x8c\xec\x9d\xbc \xea\xb2\xbd\xeb\xa1\x9c \xeb\xa6\xac\xec\x8a\xa4\xed\x8a\xb8.\n  :param labels: list of integer; \xec\x9d\xb4\xeb\xaf\xb8\xec\xa7\x80\xec\x97\x90 \xeb\x8c\x80\xed\x95\x9c \xec\xa0\x95\xec\x88\x98\xed\x99\x94\xeb\x90\x9c \xec\xa0\x95\xeb\x8b\xb5 \xeb\xa0\x88\xec\x9d\xb4\xeb\xb8\x94 \xeb\xa6\xac\xec\x8a\xa4\xed\x8a\xb8\n  :param num_shards: \xeb\x8d\xb0\xec\x9d\xb4\xed\x84\xb0 \xec\xa7\x91\xed\x95\xa9\xec\x9d\x84 \xec\x83\xa4\xeb\x94\xa9\xed\x95\xa0 \xea\xb0\xaf\xec\x88\x98.\n  """"""\n  assert len(filenames) == len(labels)\n\n  shard_offsets = dataset_utils.make_shard_offsets(len(filenames), FLAGS.num_threads, num_shards)\n  shard_output_filenames = dataset_utils.make_shard_filenames(name, len(filenames), FLAGS.num_threads, num_shards)\n\n  def _process_batch(thread_index):\n    offsets = shard_offsets[thread_index]\n    output_filenames = shard_output_filenames[thread_index]\n    _process_image_files_batch(thread_index, offsets, output_filenames, filenames, labels, bbox_info)\n\n  dataset_utils.thread_execute(FLAGS.num_threads, _process_batch)\n  dataset_utils.log(\'%s: Finished writing all %d images in data set.\' % (datetime.now(), len(filenames)))\n\n\ndef main(unused_argv):\n  if (FLAGS.data_dir is None) or (FLAGS.output_dir is None):\n    parser.print_help()\n    return\n\n  if not os.path.exists(FLAGS.output_dir):\n    os.makedirs(FLAGS.output_dir)\n\n  if FLAGS.use_bbox:\n    bbox_info = _get_bbox_info()\n    dataset_utils.log(\' - Use bounding box info. (opt. ON)\')\n  else:\n    bbox_info = None\n\n  filenames, labels, total = _find_image_files(\'train\', FLAGS.data_dir)\n  _process_dataset(\'train\', filenames, labels, bbox_info)\n\n  filenames, labels, total = _find_image_files(\'validation\', FLAGS.data_dir)\n  _process_dataset(\'validation\', filenames, labels, bbox_info)\n\n\nparser = argparse.ArgumentParser()\n\nparser.add_argument(\'-d\', \'--data_dir\', type=str, default=None, help=\'Input data directory.\')\nparser.add_argument(\'-o\', \'--output_dir\', type=str, default=None, help=\'Output data directory.\')\nparser.add_argument(\'--num_threads\', type=int, default=16, help=\'Number of threads to preprocess the images.\')\nparser.add_argument(\'--use_bbox\', type=dataset_utils.str2bool, nargs=\'?\', const=True, default=False,\n                    help=\'Whether to use bounding boxes or not.\')\n\nif __name__ == \'__main__\':\n  FLAGS, unparsed = parser.parse_known_args()\n  tf.app.run(argv=[sys.argv[0]] + unparsed)\n'"
datasets/build_cub_bird200_zeroshot.py,5,"b'#!/usr/bin/env python\n# coding=utf8\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\n""""""BFE \xeb\x85\xbc\xeb\xac\xb8\xec\x9d\x98 \xec\x84\xb1\xeb\x8a\xa5 \xeb\xb9\x84\xea\xb5\x90\xeb\xa5\xbc \xec\x9c\x84\xed\x95\xb4 CUB BIRD200-2011 \xeb\xa5\xbc \xec\x83\x9d\xec\x84\xb1\xed\x95\x9c\xeb\x8b\xa4.""""""\n\nimport os\nimport random\nimport argparse\nimport sys\n\nsys.path.append(\'./\')\nsys.path.append(\'../\')\n\nfrom datetime import datetime\nfrom datasets import dataset_utils\nfrom datasets import image_coder as coder\nfrom utils import data_util\n\nimport tensorflow as tf\nimport numpy as np\n\n# Just disables the warning, doesn\'t enable AVX/FMA\nos.environ[\'TF_CPP_MIN_LOG_LEVEL\'] = \'2\'\n\n_NUM_CLASSES = 200\n_TRAIN_CLASS_RANGE = list(range(0, 100))\n_VALIDATION_CLASS_RANGE = list(range(100, 200))\n\ndef _write_label_id_to_name(name, data_dir, id_to_name):\n  output_filename = \'%s_labels.txt\' % (name)\n  output_file = os.path.join(data_dir, output_filename)\n  with open(output_file, \'w\') as f:\n    for index in sorted(id_to_name):\n      f.write(\'%d:%s\\n\' % (index, id_to_name[index]))\n\n\ndef _get_bbox_info(input_dir):\n  box_file = os.path.join(input_dir, \'bounding_boxes.txt\')\n  imageid_file = os.path.join(input_dir, \'images.txt\')\n\n  imageid_to_labelstr = {}\n  with open(imageid_file) as fp:\n    for line in fp:\n      token = line.strip().split()\n      imageid_to_labelstr[token[0]] = token[1]\n\n  bbox_info = {}\n  with open(box_file) as fp:\n    for line in fp:\n      imageid, x, y, w, h = line.strip().split()\n      xmin = float(x)\n      xmax = float(x) + float(w)\n      ymin = float(y)\n      ymax = float(y) + float(h)\n      bbox = [int(xmin), int(ymin), int(xmax), int(ymax)]\n      filename = imageid_to_labelstr[imageid]\n      file_id = os.path.splitext(os.path.basename(filename))[0]\n      bbox_info[file_id] = bbox\n  return bbox_info\n\n\ndef _find_image_files(name, data_dir):\n  """"""\n  Build a list of all images files and labels in the data set.\n\n  :param: data_dir: string, path to the root directory of images.\n  :return:\n    filenames: double list of strings; each string is a path to an image file by label.\n    labels: double list of integer; each integer identifies the ground truth by label.\n    total: total number of images that was founded inside data_dir.\n  """"""\n  print(\'Determining list of input files and labels from %s.\' % data_dir)\n\n  # BFE \xec\x97\x90\xec\x84\x9c\xeb\x8a\x94 0-99 \xea\xb9\x8c\xec\xa7\x80\xeb\xa5\xbc training, 100-199 \xea\xb9\x8c\xec\xa7\x80\xeb\xa5\xbc test \xeb\xa1\x9c \xec\x82\xac\xec\x9a\xa9\xed\x95\x9c\xeb\x8b\xa4.\n  if name == \'train\':\n    label_index_range = _TRAIN_CLASS_RANGE\n  elif name == \'validation\':\n    label_index_range = _VALIDATION_CLASS_RANGE\n  else:\n    raise ValueError(\'Invalid index range\')\n\n  labels = []\n  filenames = []\n  total = 0\n  label_index = 0\n  id_to_name = {}\n\n  for label_name in sorted(os.listdir(data_dir)):\n    filenames_in_label = []\n    labels_in_label = []\n\n    if label_index not in label_index_range:\n      label_index += 1\n      continue\n\n    path = os.path.join(data_dir, label_name)\n    if os.path.isdir(path):\n      image_file_path = \'%s/*\' % (path)\n      matching_files = tf.gfile.Glob(image_file_path)\n      id_to_name[label_index] = label_name\n      total += len(matching_files)\n\n      labels_in_label.extend([label_index] * len(matching_files))\n      filenames_in_label.extend(matching_files)\n      label_index += 1\n\n      # \xed\x8a\xb9\xec\xa0\x95 label \xeb\x82\xb4\xec\x9d\x98 \xec\x9d\xb4\xeb\xaf\xb8\xec\xa7\x80\xec\x97\x90 \xeb\x8c\x80\xed\x95\xb4\xec\x84\x9c\xeb\xa7\x8c shuffle \xec\xb2\x98\xeb\xa6\xac\xeb\xa5\xbc \xec\x88\x98\xed\x96\x89.\n      shuffled_index = list(range(len(filenames_in_label)))\n      random.seed(12345)\n      random.shuffle(shuffled_index)\n\n      filenames_in_label = [filenames_in_label[i] for i in shuffled_index]\n      labels_in_label = [labels_in_label[i] for i in shuffled_index]\n\n      filenames.extend(filenames_in_label)\n      labels.extend(labels_in_label)\n\n  print(\'Found %d image files across %d labels inside %s.\' % (total, label_index, data_dir))\n\n  shuffled_index = list(range(len(filenames)))\n  random.seed(12345)\n  random.shuffle(shuffled_index)\n  filenames = [filenames[i] for i in shuffled_index]\n  labels = [labels[i] for i in shuffled_index]\n\n  return filenames, labels, id_to_name, total\n\n\ndef _process_image(filename, bbox=None):\n  """"""\n  \xec\x9d\xb4\xeb\xaf\xb8\xec\xa7\x80 \xed\x8c\x8c\xec\x9d\xbc\xec\x9d\x84 \xec\x9d\xbd\xec\x96\xb4\xeb\x93\xa4\xec\x97\xac RGB \xed\x83\x80\xec\x9e\x85\xec\x9c\xbc\xeb\xa1\x9c \xeb\xb3\x80\xed\x99\x98\xed\x95\x98\xec\x97\xac \xeb\xb0\x98\xed\x99\x98\xed\x95\x9c\xeb\x8b\xa4.\n  :param filename: string, \xec\x9d\xbd\xec\x96\xb4\xeb\x93\xa4\xec\x9d\xbc \xec\x9d\xb4\xeb\xaf\xb8\xec\xa7\x80 \xed\x8c\x8c\xec\x9d\xbc \xea\xb2\xbd\xeb\xa1\x9c. e.g., \'/path/to/example.JPG\'.\n  :param bbox: [xmin, ymin, xmax, ymax] \xed\x98\x95\xec\x8b\x9d\xec\x9d\x98 bbox \xeb\x8d\xb0\xec\x9d\xb4\xed\x84\xb0 \xeb\x98\x90\xeb\x8a\x94 None\n  :return:\n    image_data: \xec\x9d\xb4\xeb\xaf\xb8\xec\xa7\x80 \xeb\x8d\xb0\xec\x9d\xb4\xed\x84\xb0\xeb\xa1\x9c jpg \xed\x8f\xac\xeb\xa7\xb7\xec\x9d\x98 \xeb\x8d\xb0\xec\x9d\xb4\xed\x84\xb0.\n    height: \xec\x9d\xb4\xeb\xaf\xb8\xec\xa7\x80 height\n    width: \xec\x9d\xb4\xeb\xaf\xb8\xec\xa7\x80 width\n  """"""\n  with tf.gfile.GFile(filename, \'rb\') as f:\n    image_data = f.read()\n    image_format = dataset_utils.get_image_file_format(filename)\n\n    # try:\n    image = coder.decode_jpg(image_data)\n    height = image.shape[0]\n    width = image.shape[1]\n    # except tf.errors.InvalidArgumentError:\n    #   raise ValueError(""Invalid decode in {}"".format(filename))\n\n  if bbox is None:\n    return image_data, height, width, image_format\n  else:\n    # change bbox to [y, x, h, w]\n    crop_window = [bbox[1], bbox[0], bbox[3] - bbox[1], bbox[2] - bbox[0]]\n\n    # \xec\xa0\x84\xec\xb2\xb4 \xec\x9d\xb4\xeb\xaf\xb8\xec\xa7\x80 \xed\x81\xac\xea\xb8\xb0\xeb\xb3\xb4\xeb\x8b\xa4 bbox \xec\x98\x81\xec\x97\xad\xec\x9d\xb4 \xeb\x84\x98\xec\x96\xb4\xea\xb0\x80\xeb\x8a\x94 \xea\xb2\xbd\xec\x9a\xb0 \xec\x97\x90\xeb\x9f\xac\xea\xb0\x80 \xeb\xb0\x9c\xec\x83\x9d\xed\x95\x9c\xeb\x8b\xa4.\n    # \xec\x9d\xb4\xeb\xa5\xbc \xeb\xa7\x89\xea\xb8\xb0\xec\x9c\x84\xed\x95\x9c \xeb\xb3\xb4\xec\xa0\x95 \xec\xbd\x94\xeb\x93\x9c\xeb\xa5\xbc \xec\xb6\x94\xea\xb0\x80\xed\x95\x9c\xeb\x8b\xa4.\n    h_gap = crop_window[2] + crop_window[0] - height\n    w_gap = crop_window[3] + crop_window[1] - width\n    if h_gap > 0:\n      crop_window[2] -= h_gap\n    if w_gap > 0:\n      crop_window[3] -= w_gap\n    assert crop_window[2] > 0\n    assert crop_window[3] > 0\n\n    image = coder.crop_bbox(image, crop_window)\n    image_data = coder.encode_jpg(image)\n    image_format = \'jpg\'\n    return image_data, crop_window[2], crop_window[3], image_format\n\n\ndef _process_image_files_batch(thread_index, offsets, output_filenames, filenames, labels, bbox_info):\n  """"""\n  \xed\x95\x98\xeb\x82\x98\xec\x9d\x98 \xec\x8a\xa4\xeb\xa0\x88\xeb\x93\x9c \xeb\x8b\xa8\xec\x9c\x84\xec\x97\x90\xec\x84\x9c \xec\x9d\xb4\xeb\xaf\xb8\xec\xa7\x80 \xeb\xa6\xac\xec\x8a\xa4\xed\x8a\xb8\xeb\xa5\xbc \xec\x9d\xbd\xec\x96\xb4 TRRecord \xed\x83\x80\xec\x9e\x85\xec\x9c\xbc\xeb\xa1\x9c \xeb\xb3\x80\xed\x99\x98\xed\x95\x98\xeb\x8a\x94 \xed\x95\xa8\xec\x88\x98\n  :param thread_index: \xed\x98\x84\xec\x9e\xac \xec\x9e\x91\xec\x97\x85\xec\xa4\x91\xec\x9d\xb8 thread \xeb\xb2\x88\xed\x98\xb8.\n  :param offsets: offset list. \xec\x9d\xb4\xeb\xaf\xb8\xec\xa7\x80 \xeb\xaa\xa9\xeb\xa1\x9d \xec\xa4\x91 \xed\x98\x84\xec\x9e\xac \xec\x8a\xa4\xeb\xa0\x88\xeb\x93\x9c\xec\x97\x90\xec\x84\x9c \xec\xb2\x98\xeb\xa6\xac\xed\x95\xb4\xec\x95\xbc \xed\x95\xa0 offset \xea\xb0\x92\xec\x9c\xbc\xeb\xa1\x9c shard \xea\xb0\xaf\xec\x88\x98\xeb\xa7\x8c\xed\x81\xbc \xeb\xa6\xac\xec\x8a\xa4\xed\x8a\xb8\xeb\xa1\x9c \xec\xa0\x9c\xea\xb3\xb5\n  :param output_filenames: \xec\xb6\x9c\xeb\xa0\xa5 \xed\x8c\x8c\xec\x9d\xbc \xec\x9d\xb4\xeb\xa6\x84\xec\x9c\xbc\xeb\xa1\x9c shard \xea\xb0\xaf\xec\x88\x98\xeb\xa7\x8c\xed\x81\xbc \xeb\xa6\xac\xec\x8a\xa4\xed\x8a\xb8\xeb\xa1\x9c \xec\xa0\x9c\xea\xb3\xb5.\n  :param filenames: \xec\xb2\x98\xeb\xa6\xac\xed\x95\xb4\xec\x95\xbc \xed\x95\xa0 \xec\xa0\x84\xec\xb2\xb4 \xec\x9d\xb4\xeb\xaf\xb8\xec\xa7\x80 \xed\x8c\x8c\xec\x9d\xbc \xeb\xa6\xac\xec\x8a\xa4\xed\x8a\xb8\n  :param labels: \xec\xb2\x98\xeb\xa6\xac\xed\x95\xb4\xec\x95\xbc \xed\x95\xa0 \xec\xa0\x84\xec\xb2\xb4 \xec\x9d\xb4\xeb\xaf\xb8\xec\xa7\x80 \xeb\xa0\x88\xec\x9d\xb4\xeb\xb8\x94 \xeb\xa6\xac\xec\x8a\xa4\xed\x8a\xb8\n  """"""\n  assert len(offsets) == len(output_filenames)\n  assert len(filenames) == len(labels)\n\n  num_files_in_thread = offsets[-1][1] - offsets[0][0]\n  counter = 0\n  # \xed\x95\x98\xeb\x82\x98\xec\x9d\x98 thread \xec\x97\x90\xeb\x8a\x94 \xec\x97\xac\xeb\x9f\xac \xea\xb0\x9c\xec\x9d\x98 shard \xea\xb0\x80 \xed\x95\xa0\xeb\x8b\xb9\xeb\x90\xa0 \xec\x88\x98 \xec\x9e\x88\xeb\x8b\xa4.\n  for offset, output_filename in zip(offsets, output_filenames):\n    output_file = os.path.join(FLAGS.output_dir, output_filename)\n    writer = tf.python_io.TFRecordWriter(output_file)\n\n    # offset \xec\x97\x90\xeb\x8a\x94 \xed\x98\x84\xec\x9e\xac shard \xec\x97\x90 \xeb\x8c\x80\xed\x95\x9c (start, end) offset\xec\x9d\xb4 \xec\xa0\x80\xec\x9e\xa5\xeb\x90\x98\xec\x96\xb4 \xec\x9e\x88\xec\x9d\x8c.\n    files_in_shard = np.arange(offset[0], offset[1], dtype=int)\n    shard_counter = 0\n    for i in files_in_shard:\n      filename = filenames[i]\n      label = labels[i]\n\n      file_id = os.path.splitext(os.path.basename(filename))[0]\n      if bbox_info is None:\n        bbox = None\n      else:\n        bbox = bbox_info[file_id]\n\n      # try:\n      image_data, height, width, image_format = _process_image(filename, bbox)\n      # except ValueError:\n      #   dataset_utils.log(\'[thread %2d]: Invalid image found. %s - [skip].\' % (thread_index, filename))\n      #   continue\n\n      example = data_util.convert_to_example_without_bbox(image_data, \'jpg\', label, height, width)\n      writer.write(example.SerializeToString())\n\n      counter += 1\n      shard_counter += 1\n      if not counter % 1000:\n        dataset_utils.log(\'%s [thread %2d]: Processed %d of %d images in thread batch.\' %\n                          (datetime.now(), thread_index, counter, num_files_in_thread))\n\n    writer.close()\n    dataset_utils.log(\'%s [thread %2d]: Wrote %d images to %s\' %\n                      (datetime.now(), thread_index, shard_counter, output_file))\n\n\ndef _process_dataset(name, filenames, labels, bbox_info, num_shards=128):\n  """"""\n  \xec\x9d\xb4\xeb\xaf\xb8\xec\xa7\x80 \xed\x8c\x8c\xec\x9d\xbc \xeb\xaa\xa9\xeb\xa1\x9d\xec\x9d\x84 \xec\x9d\xbd\xec\x96\xb4\xeb\x93\xa4\xec\x97\xac TFRecord \xea\xb0\x9d\xec\xb2\xb4\xeb\xa1\x9c \xeb\xb3\x80\xed\x99\x98\xed\x95\x98\xeb\x8a\x94 \xed\x95\xa8\xec\x88\x98\n  :param name: string, \xeb\x8d\xb0\xec\x9d\xb4\xed\x84\xb0 \xea\xb3\xa0\xec\x9c\xa0 \xeb\xac\xb8\xec\x9e\x90\xec\x97\xb4 (train, validation \xeb\x93\xb1)\n  :param filenames: list of strings; \xec\x9d\xb4\xeb\xaf\xb8\xec\xa7\x80 \xed\x8c\x8c\xec\x9d\xbc \xea\xb2\xbd\xeb\xa1\x9c \xeb\xa6\xac\xec\x8a\xa4\xed\x8a\xb8.\n  :param labels: list of integer; \xec\x9d\xb4\xeb\xaf\xb8\xec\xa7\x80\xec\x97\x90 \xeb\x8c\x80\xed\x95\x9c \xec\xa0\x95\xec\x88\x98\xed\x99\x94\xeb\x90\x9c \xec\xa0\x95\xeb\x8b\xb5 \xeb\xa0\x88\xec\x9d\xb4\xeb\xb8\x94 \xeb\xa6\xac\xec\x8a\xa4\xed\x8a\xb8\n  :param num_shards: \xeb\x8d\xb0\xec\x9d\xb4\xed\x84\xb0 \xec\xa7\x91\xed\x95\xa9\xec\x9d\x84 \xec\x83\xa4\xeb\x94\xa9\xed\x95\xa0 \xea\xb0\xaf\xec\x88\x98.\n  """"""\n  assert len(filenames) == len(labels)\n\n  shard_offsets = dataset_utils.make_shard_offsets(len(filenames), FLAGS.num_threads, num_shards)\n  shard_output_filenames = dataset_utils.make_shard_filenames(name, len(filenames), FLAGS.num_threads, num_shards)\n\n  def _process_batch(thread_index):\n    offsets = shard_offsets[thread_index]\n    output_filenames = shard_output_filenames[thread_index]\n    _process_image_files_batch(thread_index, offsets, output_filenames, filenames, labels, bbox_info)\n\n  dataset_utils.thread_execute(FLAGS.num_threads, _process_batch)\n  dataset_utils.log(\'%s: Finished writing all %d images in data set.\' % (datetime.now(), len(filenames)))\n\ndef main(unused_argv):\n  if (FLAGS.data_dir is None) or (FLAGS.output_dir is None):\n    parser.print_help()\n    return\n\n  dataset_utils.log(\'Make Naver-Food TFRecord dataset by label.\')\n\n  if not os.path.exists(FLAGS.output_dir):\n    os.makedirs(FLAGS.output_dir)\n\n  if FLAGS.use_bbox:\n    bbox_info = _get_bbox_info(FLAGS.data_dir)\n    dataset_utils.log(\' - Use bounding box info. (opt. ON)\')\n  else:\n    bbox_info = None\n\n  source_dir = os.path.join(FLAGS.data_dir, \'images\')\n\n  filenames_train, labels_train, id_to_name, total = _find_image_files(\'train\', source_dir)\n  dataset_utils.log(\'Convert [train] dataset.\')\n  _process_dataset(\'train\', filenames_train, labels_train, bbox_info, 128)\n\n  filenames_val, labels_val, id_to_name, total = _find_image_files(\'validation\', source_dir)\n  dataset_utils.log(\'Convert [validation] dataset.\')\n  _process_dataset(\'validation\', filenames_val, labels_val, bbox_info, 16)\n\n\nparser = argparse.ArgumentParser()\n\nparser.add_argument(\'-d\', \'--data_dir\', type=str, default=None, help=\'Input data directory.\')\nparser.add_argument(\'-o\', \'--output_dir\', type=str, default=None, help=\'Output data directory.\')\nparser.add_argument(\'--num_threads\', type=int, default=16, help=\'Number of threads to preprocess the images.\')\n\nparser.add_argument(\n  \'--use_bbox\', type=dataset_utils.str2bool, nargs=\'?\', const=True, default=False,\n  help=\'Whether to use bounding boxes or not.\')\n\nif __name__ == \'__main__\':\n  FLAGS, unparsed = parser.parse_known_args()\n  tf.app.run(argv=[sys.argv[0]] + unparsed)\n'"
datasets/build_ethz_food101.py,4,"b'#!/usr/bin/env python\n# coding=utf8\n""""""\nUPMC-food101 \xeb\x8d\xb0\xec\x9d\xb4\xed\x84\xb0 \xec\xa7\x91\xed\x95\xa9\xec\x9d\x84 TFRecord \xed\x8c\x8c\xec\x9d\xbc \xed\x8f\xac\xeb\xa7\xb7\xec\x9d\x98 Example \xea\xb0\x9d\xec\xb2\xb4\xeb\xa1\x9c \xeb\xb3\x80\xed\x99\x98\xed\x95\x9c\xeb\x8b\xa4.\n""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport argparse\nimport os\nimport random\nimport sys\nfrom datetime import datetime\n\nimport numpy as np\nimport tensorflow as tf\n\nfrom datasets import dataset_utils\nfrom datasets import image_coder as coder\nfrom utils import data_util\n\n# Just disables the warning, doesn\'t enable AVX/FMA\nos.environ[\'TF_CPP_MIN_LOG_LEVEL\'] = \'2\'\n\nparser = argparse.ArgumentParser()\n\nparser.add_argument(\'-d\', \'--data_dir\', type=str, default=None, help=\'Input data directory.\')\nparser.add_argument(\'-o\', \'--output_dir\', type=str, default=None, help=\'Output data directory.\')\nparser.add_argument(\'--train_shards\', type=int, default=128, help=\'Number of shards in training TFRecord files.\')\nparser.add_argument(\'--validation_shards\', type=int, default=16, help=\'Number of shards in validation TFRecord files.\')\nparser.add_argument(\'--num_threads\', type=int, default=8, help=\'Number of threads to preprocess the images.\')\n\n\ndef _process_image(filename):\n  """"""\n  \xec\x9d\xb4\xeb\xaf\xb8\xec\xa7\x80 \xed\x8c\x8c\xec\x9d\xbc\xec\x9d\x84 \xec\x9d\xbd\xec\x96\xb4\xeb\x93\xa4\xec\x97\xac RGB \xed\x83\x80\xec\x9e\x85\xec\x9c\xbc\xeb\xa1\x9c \xeb\xb3\x80\xed\x99\x98\xed\x95\x98\xec\x97\xac \xeb\xb0\x98\xed\x99\x98\xed\x95\x9c\xeb\x8b\xa4.\n  :param filename: string, \xec\x9d\xbd\xec\x96\xb4\xeb\x93\xa4\xec\x9d\xbc \xec\x9d\xb4\xeb\xaf\xb8\xec\xa7\x80 \xed\x8c\x8c\xec\x9d\xbc \xea\xb2\xbd\xeb\xa1\x9c.\n  :return:\n    image_data: \xec\x9d\xb4\xeb\xaf\xb8\xec\xa7\x80 \xeb\x8d\xb0\xec\x9d\xb4\xed\x84\xb0\xeb\xa1\x9c jpg \xed\x8f\xac\xeb\xa7\xb7\xec\x9d\x98 \xeb\x8d\xb0\xec\x9d\xb4\xed\x84\xb0.\n    height: \xec\x9d\xb4\xeb\xaf\xb8\xec\xa7\x80 height\n    width: \xec\x9d\xb4\xeb\xaf\xb8\xec\xa7\x80 width\n  """"""\n  # Read the image file.\n  with tf.gfile.FastGFile(filename, \'rb\') as f:\n    image_data = f.read()\n    try:\n      image = coder.decode_jpg(image_data)\n      height = image.shape[0]\n      width = image.shape[1]\n    except tf.errors.InvalidArgumentError:\n      raise ValueError(""Invalid decode in {}"".format(filename))\n    return image_data, height, width\n\n\ndef _process_image_files_batch(thread_index, offsets, output_filenames, filenames, labels):\n  """"""\n  \xed\x95\x98\xeb\x82\x98\xec\x9d\x98 \xec\x8a\xa4\xeb\xa0\x88\xeb\x93\x9c \xeb\x8b\xa8\xec\x9c\x84\xec\x97\x90\xec\x84\x9c \xec\x9d\xb4\xeb\xaf\xb8\xec\xa7\x80 \xeb\xa6\xac\xec\x8a\xa4\xed\x8a\xb8\xeb\xa5\xbc \xec\x9d\xbd\xec\x96\xb4 TRRecord \xed\x83\x80\xec\x9e\x85\xec\x9c\xbc\xeb\xa1\x9c \xeb\xb3\x80\xed\x99\x98\xed\x95\x98\xeb\x8a\x94 \xed\x95\xa8\xec\x88\x98\n  :param thread_index: \xed\x98\x84\xec\x9e\xac \xec\x9e\x91\xec\x97\x85\xec\xa4\x91\xec\x9d\xb8 thread \xeb\xb2\x88\xed\x98\xb8.\n  :param offsets: offset list. \xec\x9d\xb4\xeb\xaf\xb8\xec\xa7\x80 \xeb\xaa\xa9\xeb\xa1\x9d \xec\xa4\x91 \xed\x98\x84\xec\x9e\xac \xec\x8a\xa4\xeb\xa0\x88\xeb\x93\x9c\xec\x97\x90\xec\x84\x9c \xec\xb2\x98\xeb\xa6\xac\xed\x95\xb4\xec\x95\xbc \xed\x95\xa0 offset \xea\xb0\x92\xec\x9c\xbc\xeb\xa1\x9c shard \xea\xb0\xaf\xec\x88\x98\xeb\xa7\x8c\xed\x81\xbc \xeb\xa6\xac\xec\x8a\xa4\xed\x8a\xb8\xeb\xa1\x9c \xec\xa0\x9c\xea\xb3\xb5\n  :param output_filenames: \xec\xb6\x9c\xeb\xa0\xa5 \xed\x8c\x8c\xec\x9d\xbc \xec\x9d\xb4\xeb\xa6\x84\xec\x9c\xbc\xeb\xa1\x9c shard \xea\xb0\xaf\xec\x88\x98\xeb\xa7\x8c\xed\x81\xbc \xeb\xa6\xac\xec\x8a\xa4\xed\x8a\xb8\xeb\xa1\x9c \xec\xa0\x9c\xea\xb3\xb5.\n  :param filenames: \xec\xb2\x98\xeb\xa6\xac\xed\x95\xb4\xec\x95\xbc \xed\x95\xa0 \xec\xa0\x84\xec\xb2\xb4 \xec\x9d\xb4\xeb\xaf\xb8\xec\xa7\x80 \xed\x8c\x8c\xec\x9d\xbc \xeb\xa6\xac\xec\x8a\xa4\xed\x8a\xb8\n  :param labels: \xec\xb2\x98\xeb\xa6\xac\xed\x95\xb4\xec\x95\xbc \xed\x95\xa0 \xec\xa0\x84\xec\xb2\xb4 \xec\x9d\xb4\xeb\xaf\xb8\xec\xa7\x80 \xeb\xa0\x88\xec\x9d\xb4\xeb\xb8\x94 \xeb\xa6\xac\xec\x8a\xa4\xed\x8a\xb8\n  """"""\n  assert len(offsets) == len(output_filenames)\n  assert len(filenames) == len(labels)\n\n  num_files_in_thread = offsets[-1][1] - offsets[0][0]\n  counter = 0\n  # \xed\x95\x98\xeb\x82\x98\xec\x9d\x98 thread \xec\x97\x90\xeb\x8a\x94 \xec\x97\xac\xeb\x9f\xac \xea\xb0\x9c\xec\x9d\x98 shard \xea\xb0\x80 \xed\x95\xa0\xeb\x8b\xb9\xeb\x90\xa0 \xec\x88\x98 \xec\x9e\x88\xeb\x8b\xa4.\n  for offset, output_filename in zip(offsets, output_filenames):\n    output_file = os.path.join(FLAGS.output_dir, output_filename)\n    writer = tf.python_io.TFRecordWriter(output_file)\n\n    # offset \xec\x97\x90\xeb\x8a\x94 \xed\x98\x84\xec\x9e\xac shard \xec\x97\x90 \xeb\x8c\x80\xed\x95\x9c (start, end) offset\xec\x9d\xb4 \xec\xa0\x80\xec\x9e\xa5\xeb\x90\x98\xec\x96\xb4 \xec\x9e\x88\xec\x9d\x8c.\n    files_in_shard = np.arange(offset[0], offset[1], dtype=int)\n    shard_counter = 0\n    for i in files_in_shard:\n      filename = filenames[i]\n      label = labels[i]\n\n      try:\n        image_data, height, width = _process_image(filename)\n      except ValueError:\n        dataset_utils.log(\'[thread %2d]: Invalid image found. %s - [skip].\' % (thread_index, filename))\n        continue\n\n      example = data_util.convert_to_example_without_bbox(image_data, \'jpg\', label, height, width)\n      writer.write(example.SerializeToString())\n\n      counter += 1\n      shard_counter += 1\n      if not counter % 1000:\n        dataset_utils.log(\'%s [thread %2d]: Processed %d of %d images in thread batch.\' %\n                          (datetime.now(), thread_index, counter, num_files_in_thread))\n\n    writer.close()\n    dataset_utils.log(\'%s [thread %2d]: Wrote %d images to %s\' %\n                      (datetime.now(), thread_index, shard_counter, output_file))\n\n\ndef _process_dataset(name, filenames, labels, num_shards):\n  """"""\n  \xec\x9d\xb4\xeb\xaf\xb8\xec\xa7\x80 \xed\x8c\x8c\xec\x9d\xbc \xeb\xaa\xa9\xeb\xa1\x9d\xec\x9d\x84 \xec\x9d\xbd\xec\x96\xb4\xeb\x93\xa4\xec\x97\xac TFRecord \xea\xb0\x9d\xec\xb2\xb4\xeb\xa1\x9c \xeb\xb3\x80\xed\x99\x98\xed\x95\x98\xeb\x8a\x94 \xed\x95\xa8\xec\x88\x98\n  :param name: string, \xeb\x8d\xb0\xec\x9d\xb4\xed\x84\xb0 \xea\xb3\xa0\xec\x9c\xa0 \xeb\xac\xb8\xec\x9e\x90\xec\x97\xb4 (train, validation \xeb\x93\xb1)\n  :param filenames: list of strings; \xec\x9d\xb4\xeb\xaf\xb8\xec\xa7\x80 \xed\x8c\x8c\xec\x9d\xbc \xea\xb2\xbd\xeb\xa1\x9c \xeb\xa6\xac\xec\x8a\xa4\xed\x8a\xb8.\n  :param labels: list of integer; \xec\x9d\xb4\xeb\xaf\xb8\xec\xa7\x80\xec\x97\x90 \xeb\x8c\x80\xed\x95\x9c \xec\xa0\x95\xec\x88\x98\xed\x99\x94\xeb\x90\x9c \xec\xa0\x95\xeb\x8b\xb5 \xeb\xa0\x88\xec\x9d\xb4\xeb\xb8\x94 \xeb\xa6\xac\xec\x8a\xa4\xed\x8a\xb8\n  :param num_shards: \xeb\x8d\xb0\xec\x9d\xb4\xed\x84\xb0 \xec\xa7\x91\xed\x95\xa9\xec\x9d\x84 \xec\x83\xa4\xeb\x94\xa9\xed\x95\xa0 \xea\xb0\xaf\xec\x88\x98.\n  """"""\n  assert len(filenames) == len(labels)\n\n  shard_offsets = dataset_utils.make_shard_offsets(len(filenames), FLAGS.num_threads, num_shards)\n  shard_output_filenames = dataset_utils.make_shard_filenames(name, len(filenames), FLAGS.num_threads, num_shards)\n\n  def _process_batch(thread_index):\n    offsets = shard_offsets[thread_index]\n    output_filenames = shard_output_filenames[thread_index]\n    _process_image_files_batch(thread_index, offsets, output_filenames, filenames, labels)\n\n  dataset_utils.thread_execute(FLAGS.num_threads, _process_batch)\n  dataset_utils.log(\'%s: Finished writing all %d images in data set.\' % (datetime.now(), len(filenames)))\n\n\ndef _get_filenames_and_labels(data_dir, filenames_path, label_path, shuffle=True):\n  # label data\n  labels_to_ids = {}\n  with open(label_path, \'r\') as f:\n    labels = [line.strip().lower().replace("" "", ""_"") for line in f]\n    for id, label in enumerate(labels):\n      labels_to_ids[label] = id\n\n  image_path = os.path.join(data_dir, \'images\')\n\n  labels = []\n  with open(filenames_path, \'r\') as f:\n    filenames = [os.path.join(image_path, line.strip() + \'.jpg\') for line in f]\n\n  for filename in filenames:\n    label = os.path.basename(os.path.dirname(filename))\n    label_id = labels_to_ids[label]\n    labels.append(label_id)\n\n  if shuffle:\n    shuffled_index = list(range(len(filenames)))\n    random.seed(12345)\n    random.shuffle(shuffled_index)\n    filenames = [filenames[i] for i in shuffled_index]\n    labels = [labels[i] for i in shuffled_index]\n\n  return filenames, labels\n\n\ndef main(_):\n  if (FLAGS.data_dir is None) or (FLAGS.output_dir is None):\n    parser.print_help()\n    return\n\n  assert not FLAGS.train_shards % FLAGS.num_threads, (\n    \'Please make the FLAGS.num_threads commensurate with FLAGS.train_shards\')\n  assert not FLAGS.validation_shards % FLAGS.num_threads, (\n    \'Please make the FLAGS.num_threads commensurate with FLAGS.validation_shards\')\n  print(\'Saving results to {}\'.format(FLAGS.output_dir))\n\n  dataset_utils.log(\'Make UEC-food100 TFRecord dataset.\')\n\n  if not os.path.exists(FLAGS.output_dir):\n    os.makedirs(FLAGS.output_dir)\n\n  label_path = os.path.join(FLAGS.data_dir, format(""meta/labels.txt""))\n  train_path = os.path.join(FLAGS.data_dir, format(""meta/train.txt""))\n  validation_path = os.path.join(FLAGS.data_dir, format(""meta/test.txt""))\n\n  train_filenames, train_labels = _get_filenames_and_labels(\n    FLAGS.data_dir, train_path, label_path, shuffle=True)\n\n  validation_filenames, validation_labels = _get_filenames_and_labels(\n    FLAGS.data_dir, validation_path, label_path, shuffle=False)\n\n  dataset_utils.log(\'Convert [train] dataset.\')\n  _process_dataset(\'train\', train_filenames, train_labels, FLAGS.train_shards)\n\n  dataset_utils.log(\'Convert [validation] dataset.\')\n  _process_dataset(\'validation\', validation_filenames, validation_labels, FLAGS.validation_shards)\n\n\nif __name__ == \'__main__\':\n  FLAGS, unparsed = parser.parse_known_args()\n  tf.app.run(argv=[sys.argv[0]] + unparsed)\n'"
datasets/build_fgvc_aircraft.py,4,"b'#!/usr/bin/env python\n# coding=utf8\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\n""""""\nexample:\nCUDA_VISIBLE_DEVICES="""" python build_fgvc_aircraft.py \\\n--data_dir=fgvc_aircraft \\\n--output_dir=fgvc_aircraft/tfrecord \n""""""\n\nimport os\nimport argparse\nimport sys\n\nsys.path.append(""../datasets"")\nsys.path.append(""./datasets"")\nsys.path.append("".."")\nsys.path.append(""."")\nfrom datetime import datetime\nfrom datasets import dataset_utils\nfrom datasets import image_coder as coder\nfrom utils import data_util\nimport subprocess\n\nimport tensorflow as tf\nimport numpy as np\nimport random\n\n_NUM_CLASSES = 196\n\n# Just disables the warning, doesn\'t enable AVX/FMA\nos.environ[\'TF_CPP_MIN_LOG_LEVEL\'] = \'2\'\n\n\ndef _write_label_id_to_name(name, data_dir, id_to_name):\n  output_filename = \'%s_labels.txt\' % (name)\n  output_file = os.path.join(data_dir, output_filename)\n  with open(output_file, \'w\') as f:\n    for index in sorted(id_to_name):\n      f.write(\'%d:%s\\n\' % (index, id_to_name[index]))\n\n\ndef _get_filenames_and_labels(data_dir, image, label, label_set, shuffle=True):\n  image_file = os.path.join(data_dir, image)\n  with open(image_file, \'r\') as f:\n    file_list = [line.strip() for line in f]\n\n  label_set_file = os.path.join(data_dir, label_set)\n  label_file = os.path.join(data_dir, label)\n  image_id_to_label = {}\n  labels_dict = {}\n  label_id = 0\n  with open(label_set_file, \'r\') as f:\n    for line in f:\n      label = line.strip()\n      if label not in labels_dict:\n        labels_dict[label] = label_id\n        label_id += 1\n  print(\'labels_dict\', labels_dict)\n  with open(label_file, \'r\') as f:\n    for line in f:\n      image_id = line.strip()[:7]\n      label = line.strip()[8:]\n      if image_id not in image_id_to_label:\n        image_id_to_label[image_id] = label\n  #   print(\'image_id_to_label\',image_id_to_label)\n  filenames = []\n  labels = []\n  #   print(\'file_list\',file_list)\n  for l in file_list:\n    filenames.append(os.path.join(os.path.abspath(data_dir), \'data/images\', l + \'.jpg\'))\n    labels.append(labels_dict[image_id_to_label[l]])\n\n  return filenames, labels\n\n\ndef _process_image(filename, bbox=None):\n  """"""\n  \xec\x9d\xb4\xeb\xaf\xb8\xec\xa7\x80 \xed\x8c\x8c\xec\x9d\xbc\xec\x9d\x84 \xec\x9d\xbd\xec\x96\xb4\xeb\x93\xa4\xec\x97\xac RGB \xed\x83\x80\xec\x9e\x85\xec\x9c\xbc\xeb\xa1\x9c \xeb\xb3\x80\xed\x99\x98\xed\x95\x98\xec\x97\xac \xeb\xb0\x98\xed\x99\x98\xed\x95\x9c\xeb\x8b\xa4.\n  :param filename: string, \xec\x9d\xbd\xec\x96\xb4\xeb\x93\xa4\xec\x9d\xbc \xec\x9d\xb4\xeb\xaf\xb8\xec\xa7\x80 \xed\x8c\x8c\xec\x9d\xbc \xea\xb2\xbd\xeb\xa1\x9c. e.g., \'/path/to/example.JPG\'.\n  :param bbox: [xmin, ymin, xmax, ymax] \xed\x98\x95\xec\x8b\x9d\xec\x9d\x98 bbox \xeb\x8d\xb0\xec\x9d\xb4\xed\x84\xb0 \xeb\x98\x90\xeb\x8a\x94 None\n  :return:\n    image_data: \xec\x9d\xb4\xeb\xaf\xb8\xec\xa7\x80 \xeb\x8d\xb0\xec\x9d\xb4\xed\x84\xb0\xeb\xa1\x9c jpg \xed\x8f\xac\xeb\xa7\xb7\xec\x9d\x98 \xeb\x8d\xb0\xec\x9d\xb4\xed\x84\xb0.\n    height: \xec\x9d\xb4\xeb\xaf\xb8\xec\xa7\x80 height\n    width: \xec\x9d\xb4\xeb\xaf\xb8\xec\xa7\x80 width\n  """"""\n  with tf.gfile.GFile(filename, \'rb\') as f:\n    image_data = f.read()\n    image_format = dataset_utils.get_image_file_format(filename)\n\n    # try:\n    image = coder.decode_jpg(image_data)\n    height = image.shape[0]\n    width = image.shape[1]\n    # except tf.errors.InvalidArgumentError:\n    #   raise ValueError(""Invalid decode in {}"".format(filename))\n\n  if bbox is None:\n    return image_data, height, width, image_format\n  else:\n    # change bbox to [y, x, h, w]\n    crop_window = [bbox[1], bbox[0], bbox[3] - bbox[1], bbox[2] - bbox[0]]\n\n    # \xec\xa0\x84\xec\xb2\xb4 \xec\x9d\xb4\xeb\xaf\xb8\xec\xa7\x80 \xed\x81\xac\xea\xb8\xb0\xeb\xb3\xb4\xeb\x8b\xa4 bbox \xec\x98\x81\xec\x97\xad\xec\x9d\xb4 \xeb\x84\x98\xec\x96\xb4\xea\xb0\x80\xeb\x8a\x94 \xea\xb2\xbd\xec\x9a\xb0 \xec\x97\x90\xeb\x9f\xac\xea\xb0\x80 \xeb\xb0\x9c\xec\x83\x9d\xed\x95\x9c\xeb\x8b\xa4.\n    # \xec\x9d\xb4\xeb\xa5\xbc \xeb\xa7\x89\xea\xb8\xb0\xec\x9c\x84\xed\x95\x9c \xeb\xb3\xb4\xec\xa0\x95 \xec\xbd\x94\xeb\x93\x9c\xeb\xa5\xbc \xec\xb6\x94\xea\xb0\x80\xed\x95\x9c\xeb\x8b\xa4.\n    h_gap = crop_window[2] + crop_window[0] - height\n    w_gap = crop_window[3] + crop_window[1] - width\n    if h_gap > 0:\n      crop_window[2] -= h_gap\n    if w_gap > 0:\n      crop_window[3] -= w_gap\n    assert crop_window[2] > 0\n    assert crop_window[3] > 0\n\n    image = coder.crop_bbox(image, crop_window)\n    image_data = coder.encode_jpg(image)\n    image_format = \'jpg\'\n    return image_data, crop_window[2], crop_window[3], image_format\n\n\ndef _process_image_files_batch(thread_index, offsets, output_filenames, filenames, labels):\n  """"""\n  \xed\x95\x98\xeb\x82\x98\xec\x9d\x98 \xec\x8a\xa4\xeb\xa0\x88\xeb\x93\x9c \xeb\x8b\xa8\xec\x9c\x84\xec\x97\x90\xec\x84\x9c \xec\x9d\xb4\xeb\xaf\xb8\xec\xa7\x80 \xeb\xa6\xac\xec\x8a\xa4\xed\x8a\xb8\xeb\xa5\xbc \xec\x9d\xbd\xec\x96\xb4 TRRecord \xed\x83\x80\xec\x9e\x85\xec\x9c\xbc\xeb\xa1\x9c \xeb\xb3\x80\xed\x99\x98\xed\x95\x98\xeb\x8a\x94 \xed\x95\xa8\xec\x88\x98\n  :param thread_index: \xed\x98\x84\xec\x9e\xac \xec\x9e\x91\xec\x97\x85\xec\xa4\x91\xec\x9d\xb8 thread \xeb\xb2\x88\xed\x98\xb8.\n  :param offsets: offset list. \xec\x9d\xb4\xeb\xaf\xb8\xec\xa7\x80 \xeb\xaa\xa9\xeb\xa1\x9d \xec\xa4\x91 \xed\x98\x84\xec\x9e\xac \xec\x8a\xa4\xeb\xa0\x88\xeb\x93\x9c\xec\x97\x90\xec\x84\x9c \xec\xb2\x98\xeb\xa6\xac\xed\x95\xb4\xec\x95\xbc \xed\x95\xa0 offset \xea\xb0\x92\xec\x9c\xbc\xeb\xa1\x9c shard \xea\xb0\xaf\xec\x88\x98\xeb\xa7\x8c\xed\x81\xbc \xeb\xa6\xac\xec\x8a\xa4\xed\x8a\xb8\xeb\xa1\x9c \xec\xa0\x9c\xea\xb3\xb5\n  :param output_filenames: \xec\xb6\x9c\xeb\xa0\xa5 \xed\x8c\x8c\xec\x9d\xbc \xec\x9d\xb4\xeb\xa6\x84\xec\x9c\xbc\xeb\xa1\x9c shard \xea\xb0\xaf\xec\x88\x98\xeb\xa7\x8c\xed\x81\xbc \xeb\xa6\xac\xec\x8a\xa4\xed\x8a\xb8\xeb\xa1\x9c \xec\xa0\x9c\xea\xb3\xb5.\n  :param filenames: \xec\xb2\x98\xeb\xa6\xac\xed\x95\xb4\xec\x95\xbc \xed\x95\xa0 \xec\xa0\x84\xec\xb2\xb4 \xec\x9d\xb4\xeb\xaf\xb8\xec\xa7\x80 \xed\x8c\x8c\xec\x9d\xbc \xeb\xa6\xac\xec\x8a\xa4\xed\x8a\xb8\n  :param labels: \xec\xb2\x98\xeb\xa6\xac\xed\x95\xb4\xec\x95\xbc \xed\x95\xa0 \xec\xa0\x84\xec\xb2\xb4 \xec\x9d\xb4\xeb\xaf\xb8\xec\xa7\x80 \xeb\xa0\x88\xec\x9d\xb4\xeb\xb8\x94 \xeb\xa6\xac\xec\x8a\xa4\xed\x8a\xb8\n  """"""\n  assert len(offsets) == len(output_filenames)\n  assert len(filenames) == len(labels)\n\n  num_files_in_thread = offsets[-1][1] - offsets[0][0]\n  counter = 0\n  # \xed\x95\x98\xeb\x82\x98\xec\x9d\x98 thread \xec\x97\x90\xeb\x8a\x94 \xec\x97\xac\xeb\x9f\xac \xea\xb0\x9c\xec\x9d\x98 shard \xea\xb0\x80 \xed\x95\xa0\xeb\x8b\xb9\xeb\x90\xa0 \xec\x88\x98 \xec\x9e\x88\xeb\x8b\xa4.\n  for offset, output_filename in zip(offsets, output_filenames):\n    output_file = os.path.join(FLAGS.output_dir, output_filename)\n    writer = tf.python_io.TFRecordWriter(output_file)\n\n    # offset \xec\x97\x90\xeb\x8a\x94 \xed\x98\x84\xec\x9e\xac shard \xec\x97\x90 \xeb\x8c\x80\xed\x95\x9c (start, end) offset\xec\x9d\xb4 \xec\xa0\x80\xec\x9e\xa5\xeb\x90\x98\xec\x96\xb4 \xec\x9e\x88\xec\x9d\x8c.\n    files_in_shard = np.arange(offset[0], offset[1], dtype=int)\n    shard_counter = 0\n    for i in files_in_shard:\n      filename = filenames[i]\n      label = labels[i]\n\n      # try:\n      image_data, height, width, image_format = _process_image(filename)\n      # except ValueError:\n      #   dataset_utils.log(\'[thread %2d]: Invalid image found. %s - [skip].\' % (thread_index, filename))\n      #   continue\n\n      example = data_util.convert_to_example_without_bbox(image_data, \'jpg\', label, height, width)\n      writer.write(example.SerializeToString())\n\n      counter += 1\n      shard_counter += 1\n      if not counter % 1000:\n        dataset_utils.log(\'%s [thread %2d]: Processed %d of %d images in thread batch.\' %\n                          (datetime.now(), thread_index, counter, num_files_in_thread))\n\n    writer.close()\n    dataset_utils.log(\'%s [thread %2d]: Wrote %d images to %s\' %\n                      (datetime.now(), thread_index, shard_counter, output_file))\n\n\ndef _process_dataset(name, filenames, labels, num_shards):\n  """"""\n  \xec\x9d\xb4\xeb\xaf\xb8\xec\xa7\x80 \xed\x8c\x8c\xec\x9d\xbc \xeb\xaa\xa9\xeb\xa1\x9d\xec\x9d\x84 \xec\x9d\xbd\xec\x96\xb4\xeb\x93\xa4\xec\x97\xac TFRecord \xea\xb0\x9d\xec\xb2\xb4\xeb\xa1\x9c \xeb\xb3\x80\xed\x99\x98\xed\x95\x98\xeb\x8a\x94 \xed\x95\xa8\xec\x88\x98\n  :param name: string, \xeb\x8d\xb0\xec\x9d\xb4\xed\x84\xb0 \xea\xb3\xa0\xec\x9c\xa0 \xeb\xac\xb8\xec\x9e\x90\xec\x97\xb4 (train, validation \xeb\x93\xb1)\n  :param filenames: list of strings; \xec\x9d\xb4\xeb\xaf\xb8\xec\xa7\x80 \xed\x8c\x8c\xec\x9d\xbc \xea\xb2\xbd\xeb\xa1\x9c \xeb\xa6\xac\xec\x8a\xa4\xed\x8a\xb8.\n  :param labels: list of integer; \xec\x9d\xb4\xeb\xaf\xb8\xec\xa7\x80\xec\x97\x90 \xeb\x8c\x80\xed\x95\x9c \xec\xa0\x95\xec\x88\x98\xed\x99\x94\xeb\x90\x9c \xec\xa0\x95\xeb\x8b\xb5 \xeb\xa0\x88\xec\x9d\xb4\xeb\xb8\x94 \xeb\xa6\xac\xec\x8a\xa4\xed\x8a\xb8\n  :param num_shards: \xeb\x8d\xb0\xec\x9d\xb4\xed\x84\xb0 \xec\xa7\x91\xed\x95\xa9\xec\x9d\x84 \xec\x83\xa4\xeb\x94\xa9\xed\x95\xa0 \xea\xb0\xaf\xec\x88\x98.\n  """"""\n  assert len(filenames) == len(labels)\n\n  shard_offsets = dataset_utils.make_shard_offsets(len(filenames), FLAGS.num_threads, num_shards)\n  shard_output_filenames = dataset_utils.make_shard_filenames(name, len(filenames), FLAGS.num_threads, num_shards)\n\n  def _process_batch(thread_index):\n    offsets = shard_offsets[thread_index]\n    output_filenames = shard_output_filenames[thread_index]\n    _process_image_files_batch(thread_index, offsets, output_filenames, filenames, labels)\n\n  dataset_utils.thread_execute(FLAGS.num_threads, _process_batch)\n  dataset_utils.log(\'%s: Finished writing all %d images in data set.\' % (datetime.now(), len(filenames)))\n\n\nparser = argparse.ArgumentParser()\n\nparser.add_argument(\'-d\', \'--data_dir\', type=str, default=None, help=\'Input data directory.\')\nparser.add_argument(\'-o\', \'--output_dir\', type=str, default=None, help=\'Output data directory.\')\nparser.add_argument(\'--train_shards\', type=int, default=128, help=\'Number of shards in training TFRecord files.\')\nparser.add_argument(\'--validation_shards\', type=int, default=16, help=\'Number of shards in validation TFRecord files.\')\nparser.add_argument(\'--num_threads\', type=int, default=8, help=\'Number of threads to preprocess the images.\')\n\n\ndef main(unused_argv):\n  if (FLAGS.data_dir is None) or (FLAGS.output_dir is None):\n    parser.print_help()\n    return\n\n  if not os.path.exists(FLAGS.output_dir):\n    os.makedirs(FLAGS.output_dir)\n\n  data_path = FLAGS.data_dir\n  image_filepath = os.path.join(data_path, ""fgvc-aircraft-2013b.tar.gz"")\n\n  if not os.path.exists(os.path.join(data_path, \'fgvc-aircraft-2013b\')):\n    subprocess.call([""tar"", ""zxvf"", image_filepath.replace(""\\\\"", ""/""),\n                     ""-C"", data_path.replace(""\\\\"", ""/""),\n                     ""--force-local""])\n\n  data_path = os.path.join(data_path, \'fgvc-aircraft-2013b\')\n  train_filenames, train_labels = _get_filenames_and_labels(data_path, \'data/images_train.txt\',\n                                                            \'data/images_variant_train.txt\',\n                                                            \'data/variants.txt\')\n\n  train_filenames2, train_labels2 = _get_filenames_and_labels(data_path, \'data/images_val.txt\',\n                                                              \'data/images_variant_val.txt\',\n                                                              \'data/variants.txt\')\n  train_filenames.extend(train_filenames2)\n  train_labels.extend(train_labels2)\n\n  shuffled_index = list(range(len(train_filenames)))\n  random.seed(12345)\n  random.shuffle(shuffled_index)\n  train_filenames = [train_filenames[i] for i in shuffled_index]\n  train_labels = [train_labels[i] for i in shuffled_index]\n\n  validation_filenames, validation_labels = _get_filenames_and_labels(data_path, \'data/images_test.txt\',\n                                                                      \'data/images_variant_test.txt\',\n                                                                      \'data/variants.txt\', shuffle=False)\n\n  dataset_utils.log(\'Convert [train] dataset.\')\n  _process_dataset(\'train\', train_filenames, train_labels, FLAGS.train_shards)\n\n  dataset_utils.log(\'Convert [validation] dataset.\')\n  _process_dataset(\'validation\', validation_filenames, validation_labels, FLAGS.validation_shards)\n\n\nif __name__ == \'__main__\':\n  FLAGS, unparsed = parser.parse_known_args()\n  tf.app.run(argv=[sys.argv[0]] + unparsed)\n'"
datasets/build_imagenet_data.py,36,"b'#!/usr/bin/python\n# Copyright 2016 Google Inc. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Converts ImageNet data to TFRecords file format with Example protos.\n\nThe raw ImageNet data set is expected to reside in JPEG files located in the\nfollowing directory structure.\n\n  data_dir/n01440764/ILSVRC2012_val_00000293.JPEG\n  data_dir/n01440764/ILSVRC2012_val_00000543.JPEG\n  ...\n\nwhere \'n01440764\' is the unique synset label associated with\nthese images.\n\nThe training data set consists of 1000 sub-directories (i.e. labels)\neach containing 1200 JPEG images for a total of 1.2M JPEG images.\n\nThe evaluation data set consists of 1000 sub-directories (i.e. labels)\neach containing 50 JPEG images for a total of 50K JPEG images.\n\nThis TensorFlow script converts the training and evaluation data into\na sharded data set consisting of 1024 and 128 TFRecord files, respectively.\n\n  train_directory/train-00000-of-01024\n  train_directory/train-00001-of-01024\n  ...\n  train_directory/train-01023-of-01024\n\nand\n\n  validation_directory/validation-00000-of-00128\n  validation_directory/validation-00001-of-00128\n  ...\n  validation_directory/validation-00127-of-00128\n\nEach validation TFRecord file contains ~390 records. Each training TFREcord\nfile contains ~1250 records. Each record within the TFRecord file is a\nserialized Example proto. The Example proto contains the following fields:\n\n  image/encoded: string containing JPEG encoded image in RGB colorspace\n  image/height: integer, image height in pixels\n  image/width: integer, image width in pixels\n  image/colorspace: string, specifying the colorspace, always \'RGB\'\n  image/channels: integer, specifying the number of channels, always 3\n  image/format: string, specifying the format, always \'JPEG\'\n\n  image/filename: string containing the basename of the image file\n            e.g. \'n01440764_10026.JPEG\' or \'ILSVRC2012_val_00000293.JPEG\'\n  image/class/label: integer specifying the index in a classification layer.\n    The label ranges from [1, 1000] where 0 is not used.\n  image/class/synset: string specifying the unique ID of the label,\n    e.g. \'n01440764\'\n  image/class/text: string specifying the human-readable version of the label\n    e.g. \'red fox, Vulpes vulpes\'\n\n  image/object/bbox/xmin: list of integers specifying the 0+ human annotated\n    bounding boxes\n  image/object/bbox/xmax: list of integers specifying the 0+ human annotated\n    bounding boxes\n  image/object/bbox/ymin: list of integers specifying the 0+ human annotated\n    bounding boxes\n  image/object/bbox/ymax: list of integers specifying the 0+ human annotated\n    bounding boxes\n  image/object/bbox/label: integer specifying the index in a classification\n    layer. The label ranges from [1, 1000] where 0 is not used. Note this is\n    always identical to the image label.\n\nNote that the length of xmin is identical to the length of xmax, ymin and ymax\nfor each example.\n\nRunning this script using 16 threads may take around ~2.5 hours on an HP Z420.\n\npython datasets/build_imagenet_data.py \\\n--train_directory=imagenet_raw/train \\\n--validation_directory=imagenet_raw/validation \\\n--output_directory=imagenet_kd_resnet152_m16_sk \\\n--imagenet_metadata_file=datasets/imagenet_metadata.txt \\\n--labels_file=datasets/imagenet_lsvrc_2015_synsets.txt \\\n--bounding_box_file=datasets/imagenet_2012_bounding_boxes.csv \\\n--logits_file_path=resnet152_m16_sk_logits\n\npython datasets/build_imagenet_data.py \\\n--train_directory=imagenet_raw/train \\\n--validation_directory=imagenet_raw/validation \\\n--make_train=False \\\n--output_directory=imagenet_kd_amoebanet_10crop \\\n--imagenet_metadata_file=datasets/imagenet_metadata.txt \\\n--labels_file=datasets/imagenet_lsvrc_2015_synsets.txt \\\n--bounding_box_file=datasets/imagenet_2012_bounding_boxes.csv \\\n--logits_file_path=amoebanet_logits\n\n""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom datetime import datetime\nimport os\nimport random\nimport sys\nimport threading\n\nimport numpy as np\nimport six\nimport tensorflow as tf\n\nsys.path.append(\'./\')\nsys.path.append(\'../\')\n\ntf.app.flags.DEFINE_string(\'train_directory\', \'/tmp/\',\n                           \'Training data directory\')\ntf.app.flags.DEFINE_string(\'validation_directory\', \'/tmp/\',\n                           \'Validation data directory\')\ntf.app.flags.DEFINE_string(\'output_directory\', \'/tmp/\',\n                           \'Output data directory\')\n\ntf.app.flags.DEFINE_integer(\'train_shards\', 1024,\n                            \'Number of shards in training TFRecord files.\')\ntf.app.flags.DEFINE_integer(\'validation_shards\', 128,\n                            \'Number of shards in validation TFRecord files.\')\n\ntf.app.flags.DEFINE_integer(\'num_threads\', 8,\n                            \'Number of threads to preprocess the images.\')\n\ntf.app.flags.DEFINE_boolean(\'make_val\', True, \'\')\ntf.app.flags.DEFINE_boolean(\'make_train\', True, \'\')\n\n# The labels file contains a list of valid labels are held in this file.\n# Assumes that the file contains entries as such:\n#   n01440764\n#   n01443537\n#   n01484850\n# where each line corresponds to a label expressed as a synset. We map\n# each synset contained in the file to an integer (based on the alphabetical\n# ordering). See below for details.\ntf.app.flags.DEFINE_string(\'labels_file\',\n                           \'imagenet_lsvrc_2015_synsets.txt\',\n                           \'Labels file\')\n\n# This file containing mapping from synset to human-readable label.\n# Assumes each line of the file looks like:\n#\n#   n02119247    black fox\n#   n02119359    silver fox\n#   n02119477    red fox, Vulpes fulva\n#\n# where each line corresponds to a unique mapping. Note that each line is\n# formatted as <synset>\\t<human readable label>.\ntf.app.flags.DEFINE_string(\'imagenet_metadata_file\',\n                           \'imagenet_metadata.txt\',\n                           \'ImageNet metadata file\')\n\n# This file is the output of process_bounding_box.py\n# Assumes each line of the file looks like:\n#\n#   n00007846_64193.JPEG,0.0060,0.2620,0.7545,0.9940\n#\n# where each line corresponds to one bounding box annotation associated\n# with an image. Each line can be parsed as:\n#\n#   <JPEG file name>, <xmin>, <ymin>, <xmax>, <ymax>\n#\n# Note that there might exist mulitple bounding box annotations associated\n# with an image file.\ntf.app.flags.DEFINE_string(\'bounding_box_file\',\n                           \'./imagenet_2012_bounding_boxes.csv\',\n                           \'Bounding box file\')\ntf.app.flags.DEFINE_string(\'logits_file_path\', \'amoebanet_logits\', \'\')\n\nFLAGS = tf.app.flags.FLAGS\n\n\ndef _int64_feature(value):\n  """"""Wrapper for inserting int64 features into Example proto.""""""\n  if not isinstance(value, list):\n    value = [value]\n  return tf.train.Feature(int64_list=tf.train.Int64List(value=value))\n\n\ndef _float_feature(value):\n  """"""Wrapper for inserting float features into Example proto.""""""\n  if not isinstance(value, list):\n    value = [value]\n  return tf.train.Feature(float_list=tf.train.FloatList(value=value))\n\n\ndef _bytes_feature(value):\n  """"""Wrapper for inserting bytes features into Example proto.""""""\n  if six.PY3 and isinstance(value, six.text_type):\n    value = six.binary_type(value, encoding=\'utf-8\')\n  return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))\n\n\ndef _convert_to_example(filename, image_buffer, label, synset, human, bbox,\n                        height, width, logit):\n  """"""Build an Example proto for an example.\n\n  Args:\n    filename: string, path to an image file, e.g., \'/path/to/example.JPG\'\n    image_buffer: string, JPEG encoding of RGB image\n    label: integer, identifier for the ground truth for the network\n    synset: string, unique WordNet ID specifying the label, e.g., \'n02323233\'\n    human: string, human-readable label, e.g., \'red fox, Vulpes vulpes\'\n    bbox: list of bounding boxes; each box is a list of integers\n      specifying [xmin, ymin, xmax, ymax]. All boxes are assumed to belong to\n      the same label as the image label.\n    height: integer, image height in pixels\n    width: integer, image width in pixels\n  Returns:\n    Example proto\n  """"""\n  xmin = []\n  ymin = []\n  xmax = []\n  ymax = []\n  for b in bbox:\n    assert len(b) == 4\n    # pylint: disable=expression-not-assigned\n    [l.append(point) for l, point in zip([xmin, ymin, xmax, ymax], b)]\n    # pylint: enable=expression-not-assigned\n\n  colorspace = \'RGB\'\n  channels = 3\n  image_format = \'JPEG\'\n\n  example = tf.train.Example(features=tf.train.Features(feature={\n    \'image/height\': _int64_feature(height),\n    \'image/width\': _int64_feature(width),\n    \'image/colorspace\': _bytes_feature(colorspace),\n    \'image/channels\': _int64_feature(channels),\n    \'image/class/label\': _int64_feature(label),\n    \'image/class/synset\': _bytes_feature(synset),\n    \'image/class/text\': _bytes_feature(human),\n    \'image/object/bbox/xmin\': _float_feature(xmin),\n    \'image/object/bbox/xmax\': _float_feature(xmax),\n    \'image/object/bbox/ymin\': _float_feature(ymin),\n    \'image/object/bbox/ymax\': _float_feature(ymax),\n    \'image/logit\': _float_feature(logit),\n    \'image/object/bbox/label\': _int64_feature([label] * len(xmin)),\n    \'image/format\': _bytes_feature(image_format),\n    \'image/filename\': _bytes_feature(os.path.basename(filename)),\n    \'image/encoded\': _bytes_feature(image_buffer)}))\n  return example\n\n\nclass ImageCoder(object):\n  """"""Helper class that provides TensorFlow image coding utilities.""""""\n\n  def __init__(self):\n    # Create a single Session to run all image coding calls.\n    self._sess = tf.Session()\n\n    # Initializes function that converts PNG to JPEG data.\n    self._png_data = tf.placeholder(dtype=tf.string)\n    image = tf.image.decode_png(self._png_data, channels=3)\n    self._png_to_jpeg = tf.image.encode_jpeg(image, format=\'rgb\', quality=100)\n\n    # Initializes function that converts CMYK JPEG data to RGB JPEG data.\n    self._cmyk_data = tf.placeholder(dtype=tf.string)\n    image = tf.image.decode_jpeg(self._cmyk_data, channels=0)\n    self._cmyk_to_rgb = tf.image.encode_jpeg(image, format=\'rgb\', quality=100)\n\n    # Initializes function that decodes RGB JPEG data.\n    self._decode_jpeg_data = tf.placeholder(dtype=tf.string)\n    self._decode_jpeg = tf.image.decode_jpeg(self._decode_jpeg_data, channels=3)\n\n  def png_to_jpeg(self, image_data):\n    return self._sess.run(self._png_to_jpeg,\n                          feed_dict={self._png_data: image_data})\n\n  def cmyk_to_rgb(self, image_data):\n    return self._sess.run(self._cmyk_to_rgb,\n                          feed_dict={self._cmyk_data: image_data})\n\n  def decode_jpeg(self, image_data):\n    image = self._sess.run(self._decode_jpeg,\n                           feed_dict={self._decode_jpeg_data: image_data})\n    assert len(image.shape) == 3\n    assert image.shape[2] == 3\n    return image\n\n\ndef _is_png(filename):\n  """"""Determine if a file contains a PNG format image.\n\n  Args:\n    filename: string, path of the image file.\n\n  Returns:\n    boolean indicating if the image is a PNG.\n  """"""\n  # File list from:\n  # https://groups.google.com/forum/embed/?place=forum/torch7#!topic/torch7/fOSTXHIESSU\n  return \'n02105855_2933.JPEG\' in filename\n\n\ndef _is_cmyk(filename):\n  """"""Determine if file contains a CMYK JPEG format image.\n\n  Args:\n    filename: string, path of the image file.\n\n  Returns:\n    boolean indicating if the image is a JPEG encoded with CMYK color space.\n  """"""\n  # File list from:\n  # https://github.com/cytsai/ilsvrc-cmyk-image-list\n  blacklist = [\'n01739381_1309.JPEG\', \'n02077923_14822.JPEG\',\n               \'n02447366_23489.JPEG\', \'n02492035_15739.JPEG\',\n               \'n02747177_10752.JPEG\', \'n03018349_4028.JPEG\',\n               \'n03062245_4620.JPEG\', \'n03347037_9675.JPEG\',\n               \'n03467068_12171.JPEG\', \'n03529860_11437.JPEG\',\n               \'n03544143_17228.JPEG\', \'n03633091_5218.JPEG\',\n               \'n03710637_5125.JPEG\', \'n03961711_5286.JPEG\',\n               \'n04033995_2932.JPEG\', \'n04258138_17003.JPEG\',\n               \'n04264628_27969.JPEG\', \'n04336792_7448.JPEG\',\n               \'n04371774_5854.JPEG\', \'n04596742_4225.JPEG\',\n               \'n07583066_647.JPEG\', \'n13037406_4650.JPEG\']\n  return filename.split(\'/\')[-1] in blacklist\n\n\ndef _process_image(filename, coder):\n  """"""Process a single image file.\n\n  Args:\n    filename: string, path to an image file e.g., \'/path/to/example.JPG\'.\n    coder: instance of ImageCoder to provide TensorFlow image coding utils.\n  Returns:\n    image_buffer: string, JPEG encoding of RGB image.\n    height: integer, image height in pixels.\n    width: integer, image width in pixels.\n  """"""\n  # Read the image file.\n  with tf.gfile.FastGFile(filename, \'rb\') as f:\n    image_data = f.read()\n\n  # Clean the dirty data.\n  if _is_png(filename):\n    # 1 image is a PNG.\n    print(\'Converting PNG to JPEG for %s\' % filename)\n    image_data = coder.png_to_jpeg(image_data)\n  elif _is_cmyk(filename):\n    # 22 JPEG images are in CMYK colorspace.\n    print(\'Converting CMYK to RGB for %s\' % filename)\n    image_data = coder.cmyk_to_rgb(image_data)\n\n  # Decode the RGB JPEG.\n  image = coder.decode_jpeg(image_data)\n\n  # Check that image converted to RGB\n  assert len(image.shape) == 3\n  height = image.shape[0]\n  width = image.shape[1]\n  assert image.shape[2] == 3\n\n  return image_data, height, width\n\n\ndef _process_image_files_batch(coder, thread_index, ranges, name, filenames,\n                               synsets, labels, humans, bboxes, num_shards, logits):\n  """"""Processes and saves list of images as TFRecord in 1 thread.\n\n  Args:\n    coder: instance of ImageCoder to provide TensorFlow image coding utils.\n    thread_index: integer, unique batch to run index is within [0, len(ranges)).\n    ranges: list of pairs of integers specifying ranges of each batches to\n      analyze in parallel.\n    name: string, unique identifier specifying the data set\n    filenames: list of strings; each string is a path to an image file\n    synsets: list of strings; each string is a unique WordNet ID\n    labels: list of integer; each integer identifies the ground truth\n    humans: list of strings; each string is a human-readable label\n    bboxes: list of bounding boxes for each image. Note that each entry in this\n      list might contain from 0+ entries corresponding to the number of bounding\n      box annotations for the image.\n    num_shards: integer number of shards for this data set.\n  """"""\n  # Each thread produces N shards where N = int(num_shards / num_threads).\n  # For instance, if num_shards = 128, and the num_threads = 2, then the first\n  # thread would produce shards [0, 64).\n  num_threads = len(ranges)\n  assert not num_shards % num_threads\n  num_shards_per_batch = int(num_shards / num_threads)\n\n  shard_ranges = np.linspace(ranges[thread_index][0],\n                             ranges[thread_index][1],\n                             num_shards_per_batch + 1).astype(int)\n  num_files_in_thread = ranges[thread_index][1] - ranges[thread_index][0]\n\n  counter = 0\n  for s in range(num_shards_per_batch):\n    # Generate a sharded version of the file name, e.g. \'train-00002-of-00010\'\n    shard = thread_index * num_shards_per_batch + s\n    output_filename = \'%s-%.5d-of-%.5d\' % (name, shard, num_shards)\n    output_file = os.path.join(FLAGS.output_directory, output_filename)\n    writer = tf.python_io.TFRecordWriter(output_file)\n\n    shard_counter = 0\n    files_in_shard = np.arange(shard_ranges[s], shard_ranges[s + 1], dtype=int)\n    for i in files_in_shard:\n      filename = filenames[i]\n      label = labels[i]\n      synset = synsets[i]\n      human = humans[i]\n      bbox = bboxes[i]\n      logit = logits[i]\n\n      image_buffer, height, width = _process_image(filename, coder)\n\n      example = _convert_to_example(filename, image_buffer, label,\n                                    synset, human, bbox,\n                                    height, width, logit)\n      writer.write(example.SerializeToString())\n      shard_counter += 1\n      counter += 1\n\n      if not counter % 1000:\n        print(\'%s [thread %d]: Processed %d of %d images in thread batch.\' %\n              (datetime.now(), thread_index, counter, num_files_in_thread))\n        sys.stdout.flush()\n\n    writer.close()\n    print(\'%s [thread %d]: Wrote %d images to %s\' %\n          (datetime.now(), thread_index, shard_counter, output_file))\n    sys.stdout.flush()\n    shard_counter = 0\n  print(\'%s [thread %d]: Wrote %d images to %d shards.\' %\n        (datetime.now(), thread_index, counter, num_files_in_thread))\n  sys.stdout.flush()\n\n\ndef _process_image_files(name, filenames, synsets, labels, humans,\n                         bboxes, num_shards, logits):\n  """"""Process and save list of images as TFRecord of Example protos.\n\n  Args:\n    name: string, unique identifier specifying the data set\n    filenames: list of strings; each string is a path to an image file\n    synsets: list of strings; each string is a unique WordNet ID\n    labels: list of integer; each integer identifies the ground truth\n    humans: list of strings; each string is a human-readable label\n    bboxes: list of bounding boxes for each image. Note that each entry in this\n      list might contain from 0+ entries corresponding to the number of bounding\n      box annotations for the image.\n    num_shards: integer number of shards for this data set.\n  """"""\n  assert len(filenames) == len(synsets)\n  assert len(filenames) == len(labels)\n  assert len(filenames) == len(humans)\n  assert len(filenames) == len(bboxes)\n\n  # Break all images into batches with a [ranges[i][0], ranges[i][1]].\n  spacing = np.linspace(0, len(filenames), FLAGS.num_threads + 1).astype(np.int)\n  ranges = []\n  threads = []\n  for i in range(len(spacing) - 1):\n    ranges.append([spacing[i], spacing[i + 1]])\n\n  # Launch a thread for each batch.\n  print(\'Launching %d threads for spacings: %s\' % (FLAGS.num_threads, ranges))\n  sys.stdout.flush()\n\n  # Create a mechanism for monitoring when all threads are finished.\n  coord = tf.train.Coordinator()\n\n  # Create a generic TensorFlow-based utility for converting all image codings.\n  coder = ImageCoder()\n\n  threads = []\n  for thread_index in range(len(ranges)):\n    args = (coder, thread_index, ranges, name, filenames,\n            synsets, labels, humans, bboxes, num_shards, logits)\n    t = threading.Thread(target=_process_image_files_batch, args=args)\n    t.start()\n    threads.append(t)\n\n  # Wait for all the threads to terminate.\n  coord.join(threads)\n  print(\'%s: Finished writing all %d images in data set.\' %\n        (datetime.now(), len(filenames)))\n  sys.stdout.flush()\n\n\ndef _find_image_files(data_dir, labels_file):\n  """"""Build a list of all images files and labels in the data set.\n\n  Args:\n    data_dir: string, path to the root directory of images.\n\n      Assumes that the ImageNet data set resides in JPEG files located in\n      the following directory structure.\n\n        data_dir/n01440764/ILSVRC2012_val_00000293.JPEG\n        data_dir/n01440764/ILSVRC2012_val_00000543.JPEG\n\n      where \'n01440764\' is the unique synset label associated with these images.\n\n    labels_file: string, path to the labels file.\n\n      The list of valid labels are held in this file. Assumes that the file\n      contains entries as such:\n        n01440764\n        n01443537\n        n01484850\n      where each line corresponds to a label expressed as a synset. We map\n      each synset contained in the file to an integer (based on the alphabetical\n      ordering) starting with the integer 1 corresponding to the synset\n      contained in the first line.\n\n      The reason we start the integer labels at 1 is to reserve label 0 as an\n      unused background class.\n\n  Returns:\n    filenames: list of strings; each string is a path to an image file.\n    synsets: list of strings; each string is a unique WordNet ID.\n    labels: list of integer; each integer identifies the ground truth.\n  """"""\n  print(\'Determining list of input files and labels from %s.\' % data_dir)\n  challenge_synsets = [l.strip() for l in\n                       tf.gfile.FastGFile(labels_file, \'r\').readlines()]\n\n  labels = []\n  filenames = []\n  synsets = []\n\n  # Leave label index 0 empty as a background class.\n  label_index = 1\n\n  # Construct the list of JPEG files and labels.\n  for synset in challenge_synsets:\n    jpeg_file_path = \'%s/%s/*.JPEG\' % (data_dir, synset)\n    matching_files = tf.gfile.Glob(jpeg_file_path)\n\n    labels.extend([label_index] * len(matching_files))\n    synsets.extend([synset] * len(matching_files))\n    filenames.extend(matching_files)\n\n    if not label_index % 100:\n      print(\'Finished finding files in %d of %d classes.\' % (\n        label_index, len(challenge_synsets)))\n    label_index += 1\n\n  # Shuffle the ordering of all image files in order to guarantee\n  # random ordering of the images with respect to label in the\n  # saved TFRecord files. Make the randomization repeatable.\n  shuffled_index = list(range(len(filenames)))\n  random.seed(12345)\n  random.shuffle(shuffled_index)\n\n  filenames = [filenames[i] for i in shuffled_index]\n  synsets = [synsets[i] for i in shuffled_index]\n  labels = [labels[i] for i in shuffled_index]\n\n  print(\'Found %d JPEG files across %d labels inside %s.\' %\n        (len(filenames), len(challenge_synsets), data_dir))\n  return filenames, synsets, labels\n\n\ndef _find_human_readable_labels(synsets, synset_to_human):\n  """"""Build a list of human-readable labels.\n\n  Args:\n    synsets: list of strings; each string is a unique WordNet ID.\n    synset_to_human: dict of synset to human labels, e.g.,\n      \'n02119022\' --> \'red fox, Vulpes vulpes\'\n\n  Returns:\n    List of human-readable strings corresponding to each synset.\n  """"""\n  humans = []\n  for s in synsets:\n    assert s in synset_to_human, (\'Failed to find: %s\' % s)\n    humans.append(synset_to_human[s])\n  return humans\n\n\ndef _find_image_bounding_boxes(filenames, image_to_bboxes):\n  """"""Find the bounding boxes for a given image file.\n\n  Args:\n    filenames: list of strings; each string is a path to an image file.\n    image_to_bboxes: dictionary mapping image file names to a list of\n      bounding boxes. This list contains 0+ bounding boxes.\n  Returns:\n    List of bounding boxes for each image. Note that each entry in this\n    list might contain from 0+ entries corresponding to the number of bounding\n    box annotations for the image.\n  """"""\n  num_image_bbox = 0\n  bboxes = []\n  for f in filenames:\n    basename = os.path.basename(f)\n    if basename in image_to_bboxes:\n      bboxes.append(image_to_bboxes[basename])\n      num_image_bbox += 1\n    else:\n      bboxes.append([])\n  print(\'Found %d images with bboxes out of %d images\' % (\n    num_image_bbox, len(filenames)))\n  return bboxes\n\n\ndef _find_image_teacher_logits(filenames, filenames_to_logits):\n  num_image_logits = 0\n  logits = []\n  for f in filenames:\n    basename = os.path.basename(f)\n    if basename in filenames_to_logits:\n      logits.append(filenames_to_logits[basename])\n      num_image_logits += 1\n    else:\n      raise ValueError(\'There is missing logits: %s\', basename)\n  return logits\n\n\ndef _process_dataset(name, directory, num_shards, synset_to_human,\n                     image_to_bboxes, filenames_to_logits):\n  """"""Process a complete data set and save it as a TFRecord.\n\n  Args:\n    name: string, unique identifier specifying the data set.\n    directory: string, root path to the data set.\n    num_shards: integer number of shards for this data set.\n    synset_to_human: dict of synset to human labels, e.g.,\n      \'n02119022\' --> \'red fox, Vulpes vulpes\'\n    image_to_bboxes: dictionary mapping image file names to a list of\n      bounding boxes. This list contains 0+ bounding boxes.\n  """"""\n  filenames, synsets, labels = _find_image_files(directory, FLAGS.labels_file)\n  humans = _find_human_readable_labels(synsets, synset_to_human)\n  bboxes = _find_image_bounding_boxes(filenames, image_to_bboxes)\n  logits = _find_image_teacher_logits(filenames, filenames_to_logits)\n  _process_image_files(name, filenames, synsets, labels,\n                       humans, bboxes, num_shards, logits)\n\n\ndef _build_synset_lookup(imagenet_metadata_file):\n  """"""Build lookup for synset to human-readable label.\n\n  Args:\n    imagenet_metadata_file: string, path to file containing mapping from\n      synset to human-readable label.\n\n      Assumes each line of the file looks like:\n\n        n02119247    black fox\n        n02119359    silver fox\n        n02119477    red fox, Vulpes fulva\n\n      where each line corresponds to a unique mapping. Note that each line is\n      formatted as <synset>\\t<human readable label>.\n\n  Returns:\n    Dictionary of synset to human labels, such as:\n      \'n02119022\' --> \'red fox, Vulpes vulpes\'\n  """"""\n  lines = tf.gfile.FastGFile(imagenet_metadata_file, \'r\').readlines()\n  synset_to_human = {}\n  for l in lines:\n    if l:\n      parts = l.strip().split(\'\\t\')\n      assert len(parts) == 2\n      synset = parts[0]\n      human = parts[1]\n      synset_to_human[synset] = human\n  return synset_to_human\n\n\ndef _build_bounding_box_lookup(bounding_box_file):\n  """"""Build a lookup from image file to bounding boxes.\n\n  Args:\n    bounding_box_file: string, path to file with bounding boxes annotations.\n\n      Assumes each line of the file looks like:\n\n        n00007846_64193.JPEG,0.0060,0.2620,0.7545,0.9940\n\n      where each line corresponds to one bounding box annotation associated\n      with an image. Each line can be parsed as:\n\n        <JPEG file name>, <xmin>, <ymin>, <xmax>, <ymax>\n\n      Note that there might exist mulitple bounding box annotations associated\n      with an image file. This file is the output of process_bounding_boxes.py.\n\n  Returns:\n    Dictionary mapping image file names to a list of bounding boxes. This list\n    contains 0+ bounding boxes.\n  """"""\n  lines = tf.gfile.FastGFile(bounding_box_file, \'r\').readlines()\n  images_to_bboxes = {}\n  num_bbox = 0\n  num_image = 0\n  for l in lines:\n    if l:\n      parts = l.split(\',\')\n      assert len(parts) == 5, (\'Failed to parse: %s\' % l)\n      filename = parts[0]\n      xmin = float(parts[1])\n      ymin = float(parts[2])\n      xmax = float(parts[3])\n      ymax = float(parts[4])\n      box = [xmin, ymin, xmax, ymax]\n\n      if filename not in images_to_bboxes:\n        images_to_bboxes[filename] = []\n        num_image += 1\n      images_to_bboxes[filename].append(box)\n      num_bbox += 1\n\n  print(\'Successfully read %d bounding boxes \'\n        \'across %d images.\' % (num_bbox, num_image))\n  return images_to_bboxes\n\n\ndef _build_kd_embbeddings_lookup(logits_file_path):\n  matching_files = tf.gfile.Glob(logits_file_path + ""*"")\n  filenames_to_logits = {}\n  num_image = 0\n  for filename in matching_files:\n    lines = tf.gfile.FastGFile(filename, \'r\').readlines()\n    for l in lines:\n      if l:\n        parts = l.split(\',\')\n        assert len(parts) == 1002, (\'Failed to parse: %s\' % l)\n        filename = parts[0]\n        logits = [float(v) for v in parts[1:1002]]\n\n        if filename not in filenames_to_logits:\n          filenames_to_logits[filename] = logits\n          num_image += 1\n        else:\n          raise ValueError(\'There is a duplicated image.\')\n    print(\'Successfully read filenames_to_logits \'\n          \'across %d images.\' % (num_image))\n\n  print(\'Successfully read filenames_to_logits \'\n        \'across %d images.\' % (num_image))\n  return filenames_to_logits\n\n\ndef main(unused_argv):\n  assert not FLAGS.train_shards % FLAGS.num_threads, (\n    \'Please make the FLAGS.num_threads commensurate with FLAGS.train_shards\')\n  assert not FLAGS.validation_shards % FLAGS.num_threads, (\n    \'Please make the FLAGS.num_threads commensurate with \'\n    \'FLAGS.validation_shards\')\n  print(\'Saving results to %s\' % FLAGS.output_directory)\n\n  # Build a map from synset to human-readable label.\n  synset_to_human = _build_synset_lookup(FLAGS.imagenet_metadata_file)\n  image_to_bboxes = _build_bounding_box_lookup(FLAGS.bounding_box_file)\n\n  # Run it!\n  if FLAGS.make_val:\n    filenames_to_logits_validation = _build_kd_embbeddings_lookup(FLAGS.logits_file_path + \'/validation\')\n    _process_dataset(\'validation\', FLAGS.validation_directory,\n                     FLAGS.validation_shards, synset_to_human, image_to_bboxes, filenames_to_logits_validation)\n  if FLAGS.make_train:\n    filenames_to_logits_train = _build_kd_embbeddings_lookup(FLAGS.logits_file_path + \'/train\')\n    _process_dataset(\'train\', FLAGS.train_directory, FLAGS.train_shards,\n                     synset_to_human, image_to_bboxes, filenames_to_logits_train)\n\n\nif __name__ == \'__main__\':\n  tf.app.run()\n'"
datasets/build_oxford_iiit_pet.py,4,"b'#!/usr/bin/env python\n# coding=utf8\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\n""""""\nexample:\nCUDA_VISIBLE_DEVICES="""" python build_oxford_iiit_pet.py \\\n--data_dir=oxford_iiit_pet \\\n--output_dir=oxford_iiit_pet/tfrecord \n""""""\n\nimport os\nimport argparse\nimport sys\n\nsys.path.append(""../datasets"")\nsys.path.append(""./datasets"")\nsys.path.append("".."")\nsys.path.append(""."")\nfrom datetime import datetime\nfrom datasets import dataset_utils\nfrom datasets import image_coder as coder\nfrom utils import data_util\nimport subprocess\n\nimport tensorflow as tf\nimport numpy as np\nimport random\n\n_NUM_CLASSES = 196\n\n# Just disables the warning, doesn\'t enable AVX/FMA\nos.environ[\'TF_CPP_MIN_LOG_LEVEL\'] = \'2\'\n\n\ndef _write_label_id_to_name(name, data_dir, id_to_name):\n  output_filename = \'%s_labels.txt\' % (name)\n  output_file = os.path.join(data_dir, output_filename)\n  with open(output_file, \'w\') as f:\n    for index in sorted(id_to_name):\n      f.write(\'%d:%s\\n\' % (index, id_to_name[index]))\n\n\ndef _get_filenames_and_labels(data_dir, annotation, shuffle=True):\n  annotation_file = os.path.join(data_dir, annotation)\n  with open(annotation_file, \'r\') as f:\n    file_list = [line.strip() for line in f]\n\n  filenames = []\n  labels = []\n  for l in file_list:\n    filenames.append(os.path.join(os.getcwd(), \'oxford_iiit_pet/images\', l.split(\' \')[0] + \'.jpg\'))\n    labels.append(int(l.split(\' \')[1]) - 1)\n\n  if shuffle:\n    shuffled_index = list(range(len(filenames)))\n    random.seed(12345)\n    random.shuffle(shuffled_index)\n    filenames = [filenames[i] for i in shuffled_index]\n    labels = [labels[i] for i in shuffled_index]\n\n  return filenames, labels\n\n\ndef _process_image(filename, bbox=None):\n  """"""\n  \xec\x9d\xb4\xeb\xaf\xb8\xec\xa7\x80 \xed\x8c\x8c\xec\x9d\xbc\xec\x9d\x84 \xec\x9d\xbd\xec\x96\xb4\xeb\x93\xa4\xec\x97\xac RGB \xed\x83\x80\xec\x9e\x85\xec\x9c\xbc\xeb\xa1\x9c \xeb\xb3\x80\xed\x99\x98\xed\x95\x98\xec\x97\xac \xeb\xb0\x98\xed\x99\x98\xed\x95\x9c\xeb\x8b\xa4.\n  :param filename: string, \xec\x9d\xbd\xec\x96\xb4\xeb\x93\xa4\xec\x9d\xbc \xec\x9d\xb4\xeb\xaf\xb8\xec\xa7\x80 \xed\x8c\x8c\xec\x9d\xbc \xea\xb2\xbd\xeb\xa1\x9c. e.g., \'/path/to/example.JPG\'.\n  :param bbox: [xmin, ymin, xmax, ymax] \xed\x98\x95\xec\x8b\x9d\xec\x9d\x98 bbox \xeb\x8d\xb0\xec\x9d\xb4\xed\x84\xb0 \xeb\x98\x90\xeb\x8a\x94 None\n  :return:\n    image_data: \xec\x9d\xb4\xeb\xaf\xb8\xec\xa7\x80 \xeb\x8d\xb0\xec\x9d\xb4\xed\x84\xb0\xeb\xa1\x9c jpg \xed\x8f\xac\xeb\xa7\xb7\xec\x9d\x98 \xeb\x8d\xb0\xec\x9d\xb4\xed\x84\xb0.\n    height: \xec\x9d\xb4\xeb\xaf\xb8\xec\xa7\x80 height\n    width: \xec\x9d\xb4\xeb\xaf\xb8\xec\xa7\x80 width\n  """"""\n  with tf.gfile.GFile(filename, \'rb\') as f:\n    image_data = f.read()\n    image_format = dataset_utils.get_image_file_format(filename)\n\n    # try:\n    image = coder.decode_jpg(image_data)\n    height = image.shape[0]\n    width = image.shape[1]\n    # except tf.errors.InvalidArgumentError:\n    #   raise ValueError(""Invalid decode in {}"".format(filename))\n\n  if bbox is None:\n    return image_data, height, width, image_format\n  else:\n    # change bbox to [y, x, h, w]\n    crop_window = [bbox[1], bbox[0], bbox[3] - bbox[1], bbox[2] - bbox[0]]\n\n    # \xec\xa0\x84\xec\xb2\xb4 \xec\x9d\xb4\xeb\xaf\xb8\xec\xa7\x80 \xed\x81\xac\xea\xb8\xb0\xeb\xb3\xb4\xeb\x8b\xa4 bbox \xec\x98\x81\xec\x97\xad\xec\x9d\xb4 \xeb\x84\x98\xec\x96\xb4\xea\xb0\x80\xeb\x8a\x94 \xea\xb2\xbd\xec\x9a\xb0 \xec\x97\x90\xeb\x9f\xac\xea\xb0\x80 \xeb\xb0\x9c\xec\x83\x9d\xed\x95\x9c\xeb\x8b\xa4.\n    # \xec\x9d\xb4\xeb\xa5\xbc \xeb\xa7\x89\xea\xb8\xb0\xec\x9c\x84\xed\x95\x9c \xeb\xb3\xb4\xec\xa0\x95 \xec\xbd\x94\xeb\x93\x9c\xeb\xa5\xbc \xec\xb6\x94\xea\xb0\x80\xed\x95\x9c\xeb\x8b\xa4.\n    h_gap = crop_window[2] + crop_window[0] - height\n    w_gap = crop_window[3] + crop_window[1] - width\n    if h_gap > 0:\n      crop_window[2] -= h_gap\n    if w_gap > 0:\n      crop_window[3] -= w_gap\n    assert crop_window[2] > 0\n    assert crop_window[3] > 0\n\n    image = coder.crop_bbox(image, crop_window)\n    image_data = coder.encode_jpg(image)\n    image_format = \'jpg\'\n    return image_data, crop_window[2], crop_window[3], image_format\n\n\ndef _process_image_files_batch(thread_index, offsets, output_filenames, filenames, labels):\n  """"""\n  \xed\x95\x98\xeb\x82\x98\xec\x9d\x98 \xec\x8a\xa4\xeb\xa0\x88\xeb\x93\x9c \xeb\x8b\xa8\xec\x9c\x84\xec\x97\x90\xec\x84\x9c \xec\x9d\xb4\xeb\xaf\xb8\xec\xa7\x80 \xeb\xa6\xac\xec\x8a\xa4\xed\x8a\xb8\xeb\xa5\xbc \xec\x9d\xbd\xec\x96\xb4 TRRecord \xed\x83\x80\xec\x9e\x85\xec\x9c\xbc\xeb\xa1\x9c \xeb\xb3\x80\xed\x99\x98\xed\x95\x98\xeb\x8a\x94 \xed\x95\xa8\xec\x88\x98\n  :param thread_index: \xed\x98\x84\xec\x9e\xac \xec\x9e\x91\xec\x97\x85\xec\xa4\x91\xec\x9d\xb8 thread \xeb\xb2\x88\xed\x98\xb8.\n  :param offsets: offset list. \xec\x9d\xb4\xeb\xaf\xb8\xec\xa7\x80 \xeb\xaa\xa9\xeb\xa1\x9d \xec\xa4\x91 \xed\x98\x84\xec\x9e\xac \xec\x8a\xa4\xeb\xa0\x88\xeb\x93\x9c\xec\x97\x90\xec\x84\x9c \xec\xb2\x98\xeb\xa6\xac\xed\x95\xb4\xec\x95\xbc \xed\x95\xa0 offset \xea\xb0\x92\xec\x9c\xbc\xeb\xa1\x9c shard \xea\xb0\xaf\xec\x88\x98\xeb\xa7\x8c\xed\x81\xbc \xeb\xa6\xac\xec\x8a\xa4\xed\x8a\xb8\xeb\xa1\x9c \xec\xa0\x9c\xea\xb3\xb5\n  :param output_filenames: \xec\xb6\x9c\xeb\xa0\xa5 \xed\x8c\x8c\xec\x9d\xbc \xec\x9d\xb4\xeb\xa6\x84\xec\x9c\xbc\xeb\xa1\x9c shard \xea\xb0\xaf\xec\x88\x98\xeb\xa7\x8c\xed\x81\xbc \xeb\xa6\xac\xec\x8a\xa4\xed\x8a\xb8\xeb\xa1\x9c \xec\xa0\x9c\xea\xb3\xb5.\n  :param filenames: \xec\xb2\x98\xeb\xa6\xac\xed\x95\xb4\xec\x95\xbc \xed\x95\xa0 \xec\xa0\x84\xec\xb2\xb4 \xec\x9d\xb4\xeb\xaf\xb8\xec\xa7\x80 \xed\x8c\x8c\xec\x9d\xbc \xeb\xa6\xac\xec\x8a\xa4\xed\x8a\xb8\n  :param labels: \xec\xb2\x98\xeb\xa6\xac\xed\x95\xb4\xec\x95\xbc \xed\x95\xa0 \xec\xa0\x84\xec\xb2\xb4 \xec\x9d\xb4\xeb\xaf\xb8\xec\xa7\x80 \xeb\xa0\x88\xec\x9d\xb4\xeb\xb8\x94 \xeb\xa6\xac\xec\x8a\xa4\xed\x8a\xb8\n  """"""\n  assert len(offsets) == len(output_filenames)\n  assert len(filenames) == len(labels)\n\n  num_files_in_thread = offsets[-1][1] - offsets[0][0]\n  counter = 0\n  # \xed\x95\x98\xeb\x82\x98\xec\x9d\x98 thread \xec\x97\x90\xeb\x8a\x94 \xec\x97\xac\xeb\x9f\xac \xea\xb0\x9c\xec\x9d\x98 shard \xea\xb0\x80 \xed\x95\xa0\xeb\x8b\xb9\xeb\x90\xa0 \xec\x88\x98 \xec\x9e\x88\xeb\x8b\xa4.\n  for offset, output_filename in zip(offsets, output_filenames):\n    output_file = os.path.join(FLAGS.output_dir, output_filename)\n    writer = tf.python_io.TFRecordWriter(output_file)\n\n    # offset \xec\x97\x90\xeb\x8a\x94 \xed\x98\x84\xec\x9e\xac shard \xec\x97\x90 \xeb\x8c\x80\xed\x95\x9c (start, end) offset\xec\x9d\xb4 \xec\xa0\x80\xec\x9e\xa5\xeb\x90\x98\xec\x96\xb4 \xec\x9e\x88\xec\x9d\x8c.\n    files_in_shard = np.arange(offset[0], offset[1], dtype=int)\n    shard_counter = 0\n    for i in files_in_shard:\n      filename = filenames[i]\n      label = labels[i]\n\n      # try:\n      image_data, height, width, image_format = _process_image(filename)\n      # except ValueError:\n      #   dataset_utils.log(\'[thread %2d]: Invalid image found. %s - [skip].\' % (thread_index, filename))\n      #   continue\n\n      example = data_util.convert_to_example_without_bbox(image_data, \'jpg\', label, height, width)\n      writer.write(example.SerializeToString())\n\n      counter += 1\n      shard_counter += 1\n      if not counter % 1000:\n        dataset_utils.log(\'%s [thread %2d]: Processed %d of %d images in thread batch.\' %\n                          (datetime.now(), thread_index, counter, num_files_in_thread))\n\n    writer.close()\n    dataset_utils.log(\'%s [thread %2d]: Wrote %d images to %s\' %\n                      (datetime.now(), thread_index, shard_counter, output_file))\n\n\ndef _process_dataset(name, filenames, labels, num_shards):\n  """"""\n  \xec\x9d\xb4\xeb\xaf\xb8\xec\xa7\x80 \xed\x8c\x8c\xec\x9d\xbc \xeb\xaa\xa9\xeb\xa1\x9d\xec\x9d\x84 \xec\x9d\xbd\xec\x96\xb4\xeb\x93\xa4\xec\x97\xac TFRecord \xea\xb0\x9d\xec\xb2\xb4\xeb\xa1\x9c \xeb\xb3\x80\xed\x99\x98\xed\x95\x98\xeb\x8a\x94 \xed\x95\xa8\xec\x88\x98\n  :param name: string, \xeb\x8d\xb0\xec\x9d\xb4\xed\x84\xb0 \xea\xb3\xa0\xec\x9c\xa0 \xeb\xac\xb8\xec\x9e\x90\xec\x97\xb4 (train, validation \xeb\x93\xb1)\n  :param filenames: list of strings; \xec\x9d\xb4\xeb\xaf\xb8\xec\xa7\x80 \xed\x8c\x8c\xec\x9d\xbc \xea\xb2\xbd\xeb\xa1\x9c \xeb\xa6\xac\xec\x8a\xa4\xed\x8a\xb8.\n  :param labels: list of integer; \xec\x9d\xb4\xeb\xaf\xb8\xec\xa7\x80\xec\x97\x90 \xeb\x8c\x80\xed\x95\x9c \xec\xa0\x95\xec\x88\x98\xed\x99\x94\xeb\x90\x9c \xec\xa0\x95\xeb\x8b\xb5 \xeb\xa0\x88\xec\x9d\xb4\xeb\xb8\x94 \xeb\xa6\xac\xec\x8a\xa4\xed\x8a\xb8\n  :param num_shards: \xeb\x8d\xb0\xec\x9d\xb4\xed\x84\xb0 \xec\xa7\x91\xed\x95\xa9\xec\x9d\x84 \xec\x83\xa4\xeb\x94\xa9\xed\x95\xa0 \xea\xb0\xaf\xec\x88\x98.\n  """"""\n  assert len(filenames) == len(labels)\n\n  shard_offsets = dataset_utils.make_shard_offsets(len(filenames), FLAGS.num_threads, num_shards)\n  shard_output_filenames = dataset_utils.make_shard_filenames(name, len(filenames), FLAGS.num_threads, num_shards)\n\n  def _process_batch(thread_index):\n    offsets = shard_offsets[thread_index]\n    output_filenames = shard_output_filenames[thread_index]\n    _process_image_files_batch(thread_index, offsets, output_filenames, filenames, labels)\n\n  dataset_utils.thread_execute(FLAGS.num_threads, _process_batch)\n  dataset_utils.log(\'%s: Finished writing all %d images in data set.\' % (datetime.now(), len(filenames)))\n\n\nparser = argparse.ArgumentParser()\n\nparser.add_argument(\'-d\', \'--data_dir\', type=str, default=None, help=\'Input data directory.\')\nparser.add_argument(\'-o\', \'--output_dir\', type=str, default=None, help=\'Output data directory.\')\nparser.add_argument(\'--train_shards\', type=int, default=128, help=\'Number of shards in training TFRecord files.\')\nparser.add_argument(\'--validation_shards\', type=int, default=16, help=\'Number of shards in validation TFRecord files.\')\nparser.add_argument(\'--num_threads\', type=int, default=8, help=\'Number of threads to preprocess the images.\')\n\n\ndef main(unused_argv):\n  if (FLAGS.data_dir is None) or (FLAGS.output_dir is None):\n    parser.print_help()\n    return\n\n  if not os.path.exists(FLAGS.output_dir):\n    os.makedirs(FLAGS.output_dir)\n\n  data_path = FLAGS.data_dir\n  image_filepath = os.path.join(data_path, ""images.tar.gz"")\n  label_filepath = os.path.join(data_path, ""annotations.tar.gz"")\n\n  if not os.path.exists(os.path.join(data_path, \'images\')):\n    subprocess.call([""tar"", ""zxvf"", image_filepath.replace(""\\\\"", ""/""),\n                     ""-C"", data_path.replace(""\\\\"", ""/""),\n                     ""--force-local""])\n\n  if not os.path.exists(os.path.join(data_path, \'annotations\')):\n    subprocess.call([""tar"", ""zxvf"", label_filepath.replace(""\\\\"", ""/""),\n                     ""-C"", data_path.replace(""\\\\"", ""/""),\n                     ""--force-local""])\n\n  train_filenames, train_labels = _get_filenames_and_labels(\n    FLAGS.data_dir, \'annotations/trainval.txt\', shuffle=True)\n\n  validation_filenames, validation_labels = _get_filenames_and_labels(\n    FLAGS.data_dir, \'annotations/test.txt\', shuffle=False)\n\n  dataset_utils.log(\'Convert [train] dataset.\')\n  _process_dataset(\'train\', train_filenames, train_labels, FLAGS.train_shards)\n\n  dataset_utils.log(\'Convert [validation] dataset.\')\n  _process_dataset(\'validation\', validation_filenames, validation_labels, FLAGS.validation_shards)\n\n\nif __name__ == \'__main__\':\n  FLAGS, unparsed = parser.parse_known_args()\n  tf.app.run(argv=[sys.argv[0]] + unparsed)\n'"
datasets/build_sop.py,4,"b'#!/usr/bin/env python\n# coding=utf8\n""""""\nStanford Online Product \xeb\x8d\xb0\xec\x9d\xb4\xed\x84\xb0 \xec\xa7\x91\xed\x95\xa9\xec\x9d\x84 TFRecord \xed\x8c\x8c\xec\x9d\xbc \xed\x8f\xac\xeb\xa7\xb7\xec\x9d\x98 Example \xea\xb0\x9d\xec\xb2\xb4\xeb\xa1\x9c \xeb\xb3\x80\xed\x99\x98\xed\x95\x9c\xeb\x8b\xa4.\n\nExample:\npython datasets/build_sop.py \\\n--input_dir=/home1/irteam/user/jklee/dataset/Stanford_Online_Products/Stanford_Online_Products \\\n--output_dir=sop_tfrecord\n""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport argparse\nimport os\nimport random\nimport sys\n\nsys.path.append(\'./\')\nsys.path.append(\'../\')\n\nfrom datetime import datetime\n\nimport numpy as np\nimport tensorflow as tf\n\nfrom datasets import dataset_utils\nfrom datasets import image_coder as coder\nfrom utils import data_util\n\n# Just disables the warning, doesn\'t enable AVX/FMA\nos.environ[\'TF_CPP_MIN_LOG_LEVEL\'] = \'2\'\n\nparser = argparse.ArgumentParser()\n\nparser.add_argument(\'-i\', \'--input_dir\', type=str, default=None, help=\'Input data directory.\')\nparser.add_argument(\'-o\', \'--output_dir\', type=str, default=None, help=\'Output data directory.\')\nparser.add_argument(\'--train_shards\', type=int, default=128, help=\'Number of shards in training TFRecord files.\')\nparser.add_argument(\'--validation_shards\', type=int, default=16, help=\'Number of shards in validation TFRecord files.\')\nparser.add_argument(\'--num_threads\', type=int, default=8, help=\'Number of threads to preprocess the images.\')\n\n\ndef _process_image(filename):\n  """"""\n  \xec\x9d\xb4\xeb\xaf\xb8\xec\xa7\x80 \xed\x8c\x8c\xec\x9d\xbc\xec\x9d\x84 \xec\x9d\xbd\xec\x96\xb4\xeb\x93\xa4\xec\x97\xac RGB \xed\x83\x80\xec\x9e\x85\xec\x9c\xbc\xeb\xa1\x9c \xeb\xb3\x80\xed\x99\x98\xed\x95\x98\xec\x97\xac \xeb\xb0\x98\xed\x99\x98\xed\x95\x9c\xeb\x8b\xa4.\n  :param filename: string, \xec\x9d\xbd\xec\x96\xb4\xeb\x93\xa4\xec\x9d\xbc \xec\x9d\xb4\xeb\xaf\xb8\xec\xa7\x80 \xed\x8c\x8c\xec\x9d\xbc \xea\xb2\xbd\xeb\xa1\x9c.\n  :return:\n    image_data: \xec\x9d\xb4\xeb\xaf\xb8\xec\xa7\x80 \xeb\x8d\xb0\xec\x9d\xb4\xed\x84\xb0\xeb\xa1\x9c jpg \xed\x8f\xac\xeb\xa7\xb7\xec\x9d\x98 \xeb\x8d\xb0\xec\x9d\xb4\xed\x84\xb0.\n    height: \xec\x9d\xb4\xeb\xaf\xb8\xec\xa7\x80 height\n    width: \xec\x9d\xb4\xeb\xaf\xb8\xec\xa7\x80 width\n  """"""\n  # Read the image file.\n  with tf.gfile.FastGFile(filename, \'rb\') as f:\n    image_data = f.read()\n    try:\n      image = coder.decode_jpg(image_data)\n      height = image.shape[0]\n      width = image.shape[1]\n    except tf.errors.InvalidArgumentError:\n      raise ValueError(""Invalid decode in {}"".format(filename))\n    return image_data, height, width\n\n\ndef _process_image_files_batch(thread_index, offsets, output_filenames, filenames, labels):\n  """"""\n  \xed\x95\x98\xeb\x82\x98\xec\x9d\x98 \xec\x8a\xa4\xeb\xa0\x88\xeb\x93\x9c \xeb\x8b\xa8\xec\x9c\x84\xec\x97\x90\xec\x84\x9c \xec\x9d\xb4\xeb\xaf\xb8\xec\xa7\x80 \xeb\xa6\xac\xec\x8a\xa4\xed\x8a\xb8\xeb\xa5\xbc \xec\x9d\xbd\xec\x96\xb4 TRRecord \xed\x83\x80\xec\x9e\x85\xec\x9c\xbc\xeb\xa1\x9c \xeb\xb3\x80\xed\x99\x98\xed\x95\x98\xeb\x8a\x94 \xed\x95\xa8\xec\x88\x98\n  :param thread_index: \xed\x98\x84\xec\x9e\xac \xec\x9e\x91\xec\x97\x85\xec\xa4\x91\xec\x9d\xb8 thread \xeb\xb2\x88\xed\x98\xb8.\n  :param offsets: offset list. \xec\x9d\xb4\xeb\xaf\xb8\xec\xa7\x80 \xeb\xaa\xa9\xeb\xa1\x9d \xec\xa4\x91 \xed\x98\x84\xec\x9e\xac \xec\x8a\xa4\xeb\xa0\x88\xeb\x93\x9c\xec\x97\x90\xec\x84\x9c \xec\xb2\x98\xeb\xa6\xac\xed\x95\xb4\xec\x95\xbc \xed\x95\xa0 offset \xea\xb0\x92\xec\x9c\xbc\xeb\xa1\x9c shard \xea\xb0\xaf\xec\x88\x98\xeb\xa7\x8c\xed\x81\xbc \xeb\xa6\xac\xec\x8a\xa4\xed\x8a\xb8\xeb\xa1\x9c \xec\xa0\x9c\xea\xb3\xb5\n  :param output_filenames: \xec\xb6\x9c\xeb\xa0\xa5 \xed\x8c\x8c\xec\x9d\xbc \xec\x9d\xb4\xeb\xa6\x84\xec\x9c\xbc\xeb\xa1\x9c shard \xea\xb0\xaf\xec\x88\x98\xeb\xa7\x8c\xed\x81\xbc \xeb\xa6\xac\xec\x8a\xa4\xed\x8a\xb8\xeb\xa1\x9c \xec\xa0\x9c\xea\xb3\xb5.\n  :param filenames: \xec\xb2\x98\xeb\xa6\xac\xed\x95\xb4\xec\x95\xbc \xed\x95\xa0 \xec\xa0\x84\xec\xb2\xb4 \xec\x9d\xb4\xeb\xaf\xb8\xec\xa7\x80 \xed\x8c\x8c\xec\x9d\xbc \xeb\xa6\xac\xec\x8a\xa4\xed\x8a\xb8\n  :param labels: \xec\xb2\x98\xeb\xa6\xac\xed\x95\xb4\xec\x95\xbc \xed\x95\xa0 \xec\xa0\x84\xec\xb2\xb4 \xec\x9d\xb4\xeb\xaf\xb8\xec\xa7\x80 \xeb\xa0\x88\xec\x9d\xb4\xeb\xb8\x94 \xeb\xa6\xac\xec\x8a\xa4\xed\x8a\xb8\n  """"""\n  assert len(offsets) == len(output_filenames)\n  assert len(filenames) == len(labels)\n\n  num_files_in_thread = offsets[-1][1] - offsets[0][0]\n  counter = 0\n  # \xed\x95\x98\xeb\x82\x98\xec\x9d\x98 thread \xec\x97\x90\xeb\x8a\x94 \xec\x97\xac\xeb\x9f\xac \xea\xb0\x9c\xec\x9d\x98 shard \xea\xb0\x80 \xed\x95\xa0\xeb\x8b\xb9\xeb\x90\xa0 \xec\x88\x98 \xec\x9e\x88\xeb\x8b\xa4.\n  for offset, output_filename in zip(offsets, output_filenames):\n    output_file = os.path.join(FLAGS.output_dir, output_filename)\n    writer = tf.python_io.TFRecordWriter(output_file)\n\n    # offset \xec\x97\x90\xeb\x8a\x94 \xed\x98\x84\xec\x9e\xac shard \xec\x97\x90 \xeb\x8c\x80\xed\x95\x9c (start, end) offset\xec\x9d\xb4 \xec\xa0\x80\xec\x9e\xa5\xeb\x90\x98\xec\x96\xb4 \xec\x9e\x88\xec\x9d\x8c.\n    files_in_shard = np.arange(offset[0], offset[1], dtype=int)\n    shard_counter = 0\n    for i in files_in_shard:\n      filename = filenames[i]\n      label = labels[i]\n\n      try:\n        image_data, height, width = _process_image(filename)\n      except ValueError:\n        dataset_utils.log(\'[thread %2d]: Invalid image found. %s - [skip].\' % (thread_index, filename))\n        continue\n\n      example = data_util.convert_to_example_without_bbox(image_data, \'jpg\', label, height, width)\n      writer.write(example.SerializeToString())\n\n      counter += 1\n      shard_counter += 1\n      if not counter % 1000:\n        dataset_utils.log(\'%s [thread %2d]: Processed %d of %d images in thread batch.\' %\n                          (datetime.now(), thread_index, counter, num_files_in_thread))\n\n    writer.close()\n    dataset_utils.log(\'%s [thread %2d]: Wrote %d images to %s\' %\n                      (datetime.now(), thread_index, shard_counter, output_file))\n\n\ndef _process_dataset(name, filenames, labels, num_shards):\n  """"""\n  \xec\x9d\xb4\xeb\xaf\xb8\xec\xa7\x80 \xed\x8c\x8c\xec\x9d\xbc \xeb\xaa\xa9\xeb\xa1\x9d\xec\x9d\x84 \xec\x9d\xbd\xec\x96\xb4\xeb\x93\xa4\xec\x97\xac TFRecord \xea\xb0\x9d\xec\xb2\xb4\xeb\xa1\x9c \xeb\xb3\x80\xed\x99\x98\xed\x95\x98\xeb\x8a\x94 \xed\x95\xa8\xec\x88\x98\n  :param name: string, \xeb\x8d\xb0\xec\x9d\xb4\xed\x84\xb0 \xea\xb3\xa0\xec\x9c\xa0 \xeb\xac\xb8\xec\x9e\x90\xec\x97\xb4 (train, validation \xeb\x93\xb1)\n  :param filenames: list of strings; \xec\x9d\xb4\xeb\xaf\xb8\xec\xa7\x80 \xed\x8c\x8c\xec\x9d\xbc \xea\xb2\xbd\xeb\xa1\x9c \xeb\xa6\xac\xec\x8a\xa4\xed\x8a\xb8.\n  :param labels: list of integer; \xec\x9d\xb4\xeb\xaf\xb8\xec\xa7\x80\xec\x97\x90 \xeb\x8c\x80\xed\x95\x9c \xec\xa0\x95\xec\x88\x98\xed\x99\x94\xeb\x90\x9c \xec\xa0\x95\xeb\x8b\xb5 \xeb\xa0\x88\xec\x9d\xb4\xeb\xb8\x94 \xeb\xa6\xac\xec\x8a\xa4\xed\x8a\xb8\n  :param num_shards: \xeb\x8d\xb0\xec\x9d\xb4\xed\x84\xb0 \xec\xa7\x91\xed\x95\xa9\xec\x9d\x84 \xec\x83\xa4\xeb\x94\xa9\xed\x95\xa0 \xea\xb0\xaf\xec\x88\x98.\n  """"""\n  assert len(filenames) == len(labels)\n\n  shard_offsets = dataset_utils.make_shard_offsets(len(filenames), FLAGS.num_threads, num_shards)\n  shard_output_filenames = dataset_utils.make_shard_filenames(name, len(filenames), FLAGS.num_threads, num_shards)\n\n  def _process_batch(thread_index):\n    offsets = shard_offsets[thread_index]\n    output_filenames = shard_output_filenames[thread_index]\n    _process_image_files_batch(thread_index, offsets, output_filenames, filenames, labels)\n\n  dataset_utils.thread_execute(FLAGS.num_threads, _process_batch)\n  dataset_utils.log(\'%s: Finished writing all %d images in data set.\' % (datetime.now(), len(filenames)))\n\n\ndef get_filenames_and_labels(data_dir, is_training=True):\n  if is_training:\n    txt = data_dir + \'/Ebay_train.txt\'\n  else:\n    txt = data_dir + \'/Ebay_test.txt\'\n\n  with open(txt, \'r\') as f:\n    filenames = []\n    labels = []\n    for line in f:\n      token = line.strip().split()\n      if token[0] == \'image_id\':\n        continue\n      class_id = int(token[1]) - 1\n\n      path = token[3]\n      path = os.path.join(data_dir, path)\n      filenames.append(path)\n      labels.append(class_id)\n\n  if is_training:\n    shuffled_index = list(range(len(filenames)))\n    random.seed(12345)\n    random.shuffle(shuffled_index)\n    filenames = [filenames[i] for i in shuffled_index]\n    labels = [labels[i] for i in shuffled_index]\n\n  return filenames, labels\n\n\ndef main(_):\n  if (FLAGS.input_dir is None) or (FLAGS.output_dir is None):\n    parser.print_help()\n    return\n\n  assert not FLAGS.train_shards % FLAGS.num_threads, (\n    \'Please make the FLAGS.num_threads commensurate with FLAGS.train_shards\')\n  assert not FLAGS.validation_shards % FLAGS.num_threads, (\n    \'Please make the FLAGS.num_threads commensurate with FLAGS.validation_shards\')\n  print(\'Saving results to {}\'.format(FLAGS.output_dir))\n\n  dataset_utils.log(\'Make UEC-food100 TFRecord dataset.\')\n\n  if not os.path.exists(FLAGS.output_dir):\n    os.makedirs(FLAGS.output_dir)\n\n  # train_txt = \'/home1/irteam/user/jklee/dataset/Stanford_Online_Products/Stanford_Online_Products/Ebay_train.txt\'\n  # test_txt = \'/home1/irteam/user/jklee/dataset/Stanford_Online_Products/Stanford_Online_Products/Ebay_test.txt\'\n\n  train_filenames, train_labels = get_filenames_and_labels(FLAGS.input_dir)\n  validation_filenames, validation_labels = get_filenames_and_labels(FLAGS.input_dir, is_training=False)\n\n  # train_filenames, train_labels = _get_filenames_and_labels(\n  #   FLAGS.data_dir, train_path, label_path, shuffle=True)\n  #\n  # validation_filenames, validation_labels = _get_filenames_and_labels(\n  #   FLAGS.data_dir, validation_path, label_path, shuffle=False)\n\n  dataset_utils.log(\'Convert [train] dataset.\')\n  _process_dataset(\'train\', train_filenames, train_labels, FLAGS.train_shards)\n\n  dataset_utils.log(\'Convert [validation] dataset.\')\n  _process_dataset(\'validation\', validation_filenames, validation_labels, FLAGS.validation_shards)\n\n\nif __name__ == \'__main__\':\n  FLAGS, unparsed = parser.parse_known_args()\n  tf.app.run(argv=[sys.argv[0]] + unparsed)\n'"
datasets/build_uec_food100.py,5,"b'#!/usr/bin/env python\n# coding=utf8\n""""""\nUEC-food100 \xeb\x8d\xb0\xec\x9d\xb4\xed\x84\xb0 \xec\xa7\x91\xed\x95\xa9\xec\x9d\x84 TFRecord \xed\x8c\x8c\xec\x9d\xbc \xed\x8f\xac\xeb\xa7\xb7\xec\x9d\x98 Example \xea\xb0\x9d\xec\xb2\xb4\xeb\xa1\x9c \xeb\xb3\x80\xed\x99\x98\xed\x95\x9c\xeb\x8b\xa4.\n\xeb\x8d\xb0\xec\x9d\xb4\xed\x84\xb0\xeb\x8a\x94 `UECFOOD100/{label_no}/11500.jpg` \xec\x99\x80 \xea\xb0\x99\xec\x9d\x80 \xed\x98\x95\xec\x8b\x9d\xec\x9c\xbc\xeb\xa1\x9c \xec\xa0\x80\xec\x9e\xa5\xeb\x90\x98\xec\x96\xb4 \xec\x9e\x88\xeb\x8b\xa4.\n""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport argparse\nimport os\nimport random\nimport sys\nfrom datetime import datetime\n\nimport numpy as np\nimport tensorflow as tf\n\nfrom datasets import dataset_utils\nfrom datasets import image_coder as coder\nfrom utils import data_util\n\n# Just disables the warning, doesn\'t enable AVX/FMA\nos.environ[\'TF_CPP_MIN_LOG_LEVEL\'] = \'2\'\n\n_UEC_FOOD100_ROOT_DIR = \'UECFOOD100\'\n_NUM_CLASSES = 100\n\nparser = argparse.ArgumentParser()\n\nparser.add_argument(\'-d\', \'--data_dir\', type=str, default=None, help=\'Input data directory.\')\nparser.add_argument(\'-o\', \'--output_dir\', type=str, default=None, help=\'Output data directory.\')\nparser.add_argument(\'--train_shards\', type=int, default=64, help=\'Number of shards in training TFRecord files.\')\nparser.add_argument(\'--validation_shards\', type=int, default=16, help=\'Number of shards in validation TFRecord files.\')\nparser.add_argument(\'--num_threads\', type=int, default=16, help=\'Number of threads to preprocess the images.\')\n\nparser.add_argument(\n  \'--use_bbox\', type=dataset_utils.str2bool, nargs=\'?\', const=True, default=False,\n  help=\'Whether to use bounding boxes or not.\')\n\n\ndef _get_bbox_info(input_dir):\n  """"""\n  bb_info.txt \xed\x8c\x8c\xec\x9d\xbc\xec\x9d\x84 \xec\x9d\xbd\xec\x96\xb4\xeb\x93\xa4\xec\x97\xac \xea\xb0\x81 \xec\x9d\xb4\xeb\xaf\xb8\xec\xa7\x80\xec\x9d\x98 \xeb\x8c\x80\xed\x95\x9c bbox \xec\xa0\x95\xeb\xb3\xb4\xeb\xa5\xbc \xec\x9d\xbd\xec\x96\xb4\xeb\x93\xa4\xec\x9d\xb8\xeb\x8b\xa4.\n  :param input_dir: \xec\x9e\x85\xeb\xa0\xa5 \xed\x8c\x8c\xec\x9d\xbc \xeb\x94\x94\xeb\xa0\x89\xed\x86\xa0\xeb\xa6\xac.\n  :return: double dict \xed\x83\x80\xec\x9e\x85\xec\x9d\x98 \xea\xb0\x9d\xec\xb2\xb4\xeb\xa1\x9c bbox \xec\xa0\x95\xeb\xb3\xb4\xeb\xa5\xbc \xea\xb0\x80\xec\xa7\x80\xea\xb3\xa0 \xec\x9e\x88\xeb\x8b\xa4. {label_id, {file_id, [x1, y1, x2, y2]}}\n  """"""\n  bbox_info = dict()\n  for label_id in range(0, _NUM_CLASSES):\n    label_dir = str(label_id + 1)  # from one-base index to zero-base\n    box_file = os.path.join(input_dir, _UEC_FOOD100_ROOT_DIR, label_dir, \'bb_info.txt\')\n    bbox_info_by_label = dict()\n    with tf.gfile.GFile(box_file, \'r\') as f:\n      next(f)  # skip first line (column info : \'img x1 y1 x2 y2\')\n      for line in f:\n        sp = line.rstrip(\'\\n\').split(\' \')\n        if len(sp) == 0:\n          continue\n        file_id, bbox_location = sp[0], [int(i) for i in sp[1:]]\n        assert len(bbox_location) == 4\n        bbox_info_by_label[file_id] = bbox_location\n      bbox_info[label_id] = bbox_info_by_label\n  return bbox_info\n\n\ndef _get_filenames_and_labels(val_index, shuffle=True):\n  """"""\xec\x9d\xb4\xeb\xaf\xb8\xec\xa7\x80 \xed\x8c\x8c\xec\x9d\xbc \xec\x9d\xb4\xeb\xa6\x84\xea\xb3\xbc \xec\x9d\xb4\xec\x97\x90 \xeb\x8c\x80\xed\x95\x9c \xeb\xa0\x88\xec\x9d\xb4\xeb\xb8\x94 \xec\xa0\x95\xeb\xb3\xb4\xeb\xa5\xbc \xeb\xb0\x98\xed\x99\x98\xed\x95\x9c\xeb\x8b\xa4.""""""\n  output_filenames = []\n  output_labels = []\n\n  # make filenames\n  for i in val_index:\n    target_file = os.path.join(FLAGS.data_dir, \'val{}.txt\'.format(str(i)))\n    with open(target_file, \'r\') as f:\n      image_files = [os.path.join(FLAGS.data_dir, line.strip()) for line in f]\n      output_filenames.extend(image_files)\n\n  # make labels\n  for filename in output_filenames:\n    label = int(os.path.basename(os.path.dirname(filename))) - 1\n    output_labels.append(label)\n\n  if shuffle:\n    shuffled_index = list(range(len(output_filenames)))\n    random.seed(12345)\n    random.shuffle(shuffled_index)\n    output_filenames = [output_filenames[i] for i in shuffled_index]\n    output_labels = [output_labels[i] for i in shuffled_index]\n\n  return output_filenames, output_labels\n\n\ndef _process_image(filename, bbox):\n  """"""\n  \xec\x9d\xb4\xeb\xaf\xb8\xec\xa7\x80 \xed\x8c\x8c\xec\x9d\xbc\xec\x9d\x84 \xec\x9d\xbd\xec\x96\xb4\xeb\x93\xa4\xec\x97\xac RGB \xed\x83\x80\xec\x9e\x85\xec\x9c\xbc\xeb\xa1\x9c \xeb\xb3\x80\xed\x99\x98\xed\x95\x98\xec\x97\xac \xeb\xb0\x98\xed\x99\x98\xed\x95\x9c\xeb\x8b\xa4.\n  :param filename: string, \xec\x9d\xbd\xec\x96\xb4\xeb\x93\xa4\xec\x9d\xbc \xec\x9d\xb4\xeb\xaf\xb8\xec\xa7\x80 \xed\x8c\x8c\xec\x9d\xbc \xea\xb2\xbd\xeb\xa1\x9c. e.g., \'/path/to/example.JPG\'.\n  :param bbox: [xmin, ymin, xmax, ymax] \xed\x98\x95\xec\x8b\x9d\xec\x9d\x98 bbox \xeb\x8d\xb0\xec\x9d\xb4\xed\x84\xb0 \xeb\x98\x90\xeb\x8a\x94 None\n  :return:\n    image_data: \xec\x9d\xb4\xeb\xaf\xb8\xec\xa7\x80 \xeb\x8d\xb0\xec\x9d\xb4\xed\x84\xb0\xeb\xa1\x9c jpg \xed\x8f\xac\xeb\xa7\xb7\xec\x9d\x98 \xeb\x8d\xb0\xec\x9d\xb4\xed\x84\xb0.\n    height: \xec\x9d\xb4\xeb\xaf\xb8\xec\xa7\x80 height\n    width: \xec\x9d\xb4\xeb\xaf\xb8\xec\xa7\x80 width\n  """"""\n  with tf.gfile.GFile(filename, \'rb\') as f:\n    image_data = f.read()\n    try:\n      image = coder.decode_jpg(image_data)\n\n      height = image.shape[0]\n      width = image.shape[1]\n    except tf.errors.InvalidArgumentError:\n      raise ValueError(""Invalid decode in {}"".format(filename))\n\n  if bbox is None:\n    return image_data, height, width\n  else:\n    # change bbox to [y, x, h, w]\n    crop_window = [bbox[1], bbox[0], bbox[3] - bbox[1], bbox[2] - bbox[0]]\n\n    # food100 bbox \xec\xa0\x95\xeb\xb3\xb4 \xec\xa4\x91 \xec\x9d\xbc\xeb\xb6\x80 bbox \xec\xa0\x95\xeb\xb3\xb4\xea\xb0\x80 \xec\x9e\x98\xeb\xaa\xbb\xeb\x90\x9c \xea\xb2\xbd\xec\x9a\xb0\xea\xb0\x80 \xec\x9e\x88\xeb\x8b\xa4.\n    # \xec\xa0\x84\xec\xb2\xb4 \xec\x9d\xb4\xeb\xaf\xb8\xec\xa7\x80 \xed\x81\xac\xea\xb8\xb0\xeb\xb3\xb4\xeb\x8b\xa4 bbox \xec\x98\x81\xec\x97\xad\xec\x9d\xb4 \xeb\x84\x98\xec\x96\xb4\xea\xb0\x80\xeb\x8a\x94 \xea\xb2\xbd\xec\x9a\xb0 \xec\x97\x90\xeb\x9f\xac\xea\xb0\x80 \xeb\xb0\x9c\xec\x83\x9d\xed\x95\x9c\xeb\x8b\xa4.\n    # \xec\x9d\xb4\xeb\xa5\xbc \xeb\xa7\x89\xea\xb8\xb0\xec\x9c\x84\xed\x95\x9c \xeb\xb3\xb4\xec\xa0\x95 \xec\xbd\x94\xeb\x93\x9c\xeb\xa5\xbc \xec\xb6\x94\xea\xb0\x80\xed\x95\x9c\xeb\x8b\xa4.\n    h_gap = crop_window[2] + crop_window[0] - height\n    w_gap = crop_window[3] + crop_window[1] - width\n    if h_gap > 0:\n      crop_window[2] -= h_gap\n    if w_gap > 0:\n      crop_window[3] -= w_gap\n    assert crop_window[2] > 0\n    assert crop_window[3] > 0\n\n    image = coder.crop_bbox(image, crop_window)\n    image_data = coder.encode_jpg(image)\n    return image_data, crop_window[2], crop_window[3]\n\n\ndef _process_image_files_batch(thread_index, offsets, output_filenames, filenames, labels, bbox_info):\n  """"""\n  \xed\x95\x98\xeb\x82\x98\xec\x9d\x98 \xec\x8a\xa4\xeb\xa0\x88\xeb\x93\x9c \xeb\x8b\xa8\xec\x9c\x84\xec\x97\x90\xec\x84\x9c \xec\x9d\xb4\xeb\xaf\xb8\xec\xa7\x80 \xeb\xa6\xac\xec\x8a\xa4\xed\x8a\xb8\xeb\xa5\xbc \xec\x9d\xbd\xec\x96\xb4 TRRecord \xed\x83\x80\xec\x9e\x85\xec\x9c\xbc\xeb\xa1\x9c \xeb\xb3\x80\xed\x99\x98\xed\x95\x98\xeb\x8a\x94 \xed\x95\xa8\xec\x88\x98\n  :param thread_index: \xed\x98\x84\xec\x9e\xac \xec\x9e\x91\xec\x97\x85\xec\xa4\x91\xec\x9d\xb8 thread \xeb\xb2\x88\xed\x98\xb8.\n  :param offsets: offset list. \xec\x9d\xb4\xeb\xaf\xb8\xec\xa7\x80 \xeb\xaa\xa9\xeb\xa1\x9d \xec\xa4\x91 \xed\x98\x84\xec\x9e\xac \xec\x8a\xa4\xeb\xa0\x88\xeb\x93\x9c\xec\x97\x90\xec\x84\x9c \xec\xb2\x98\xeb\xa6\xac\xed\x95\xb4\xec\x95\xbc \xed\x95\xa0 offset \xea\xb0\x92\xec\x9c\xbc\xeb\xa1\x9c shard \xea\xb0\xaf\xec\x88\x98\xeb\xa7\x8c\xed\x81\xbc \xeb\xa6\xac\xec\x8a\xa4\xed\x8a\xb8\xeb\xa1\x9c \xec\xa0\x9c\xea\xb3\xb5\n  :param output_filenames: \xec\xb6\x9c\xeb\xa0\xa5 \xed\x8c\x8c\xec\x9d\xbc \xec\x9d\xb4\xeb\xa6\x84\xec\x9c\xbc\xeb\xa1\x9c shard \xea\xb0\xaf\xec\x88\x98\xeb\xa7\x8c\xed\x81\xbc \xeb\xa6\xac\xec\x8a\xa4\xed\x8a\xb8\xeb\xa1\x9c \xec\xa0\x9c\xea\xb3\xb5.\n  :param filenames: \xec\xb2\x98\xeb\xa6\xac\xed\x95\xb4\xec\x95\xbc \xed\x95\xa0 \xec\xa0\x84\xec\xb2\xb4 \xec\x9d\xb4\xeb\xaf\xb8\xec\xa7\x80 \xed\x8c\x8c\xec\x9d\xbc \xeb\xa6\xac\xec\x8a\xa4\xed\x8a\xb8\n  :param labels: \xec\xb2\x98\xeb\xa6\xac\xed\x95\xb4\xec\x95\xbc \xed\x95\xa0 \xec\xa0\x84\xec\xb2\xb4 \xec\x9d\xb4\xeb\xaf\xb8\xec\xa7\x80 \xeb\xa0\x88\xec\x9d\xb4\xeb\xb8\x94 \xeb\xa6\xac\xec\x8a\xa4\xed\x8a\xb8\n  :param bbox_info: \xec\xa0\x84\xec\xb2\xb4 \xec\x9d\xb4\xeb\xaf\xb8\xec\xa7\x80\xec\x97\x90 \xeb\x8c\x80\xed\x95\x9c bbox \xec\xa0\x95\xeb\xb3\xb4\xeb\xa1\x9c \xec\x82\xac\xec\x9a\xa9\xed\x95\x98\xec\xa7\x80 \xec\x95\x8a\xec\x9d\x84 \xea\xb2\xbd\xec\x9a\xb0 None \xec\x9e\x85\xeb\xa0\xa5\n  """"""\n  assert len(offsets) == len(output_filenames)\n  assert len(filenames) == len(labels)\n\n  num_files_in_thread = offsets[-1][1] - offsets[0][0]\n  counter = 0\n  # \xed\x95\x98\xeb\x82\x98\xec\x9d\x98 thread \xec\x97\x90\xeb\x8a\x94 \xec\x97\xac\xeb\x9f\xac \xea\xb0\x9c\xec\x9d\x98 shard \xea\xb0\x80 \xed\x95\xa0\xeb\x8b\xb9\xeb\x90\xa0 \xec\x88\x98 \xec\x9e\x88\xeb\x8b\xa4.\n  for offset, output_filename in zip(offsets, output_filenames):\n    output_file = os.path.join(FLAGS.output_dir, output_filename)\n    writer = tf.python_io.TFRecordWriter(output_file)\n\n    # offset \xec\x97\x90\xeb\x8a\x94 \xed\x98\x84\xec\x9e\xac shard \xec\x97\x90 \xeb\x8c\x80\xed\x95\x9c (start, end) offset\xec\x9d\xb4 \xec\xa0\x80\xec\x9e\xa5\xeb\x90\x98\xec\x96\xb4 \xec\x9e\x88\xec\x9d\x8c.\n    files_in_shard = np.arange(offset[0], offset[1], dtype=int)\n    shard_counter = 0\n    for i in files_in_shard:\n      filename = filenames[i]\n      label = labels[i]\n\n      file_id = os.path.splitext(os.path.basename(filename))[0]\n      if bbox_info is None:\n        bbox = None  # bbox_info \xea\xb0\x80 \xec\x97\x86\xeb\x8a\x94 \xea\xb2\xbd\xec\x9a\xb0 bbox \xec\xa0\x95\xeb\xb3\xb4\xeb\xa1\x9c crop \xed\x95\x98\xeb\x8a\x94 \xea\xb3\xbc\xec\xa0\x95\xec\x9d\x84 \xec\x83\x9d\xeb\x9e\xb5.\n      else:\n        bbox = bbox_info[label][file_id]\n      try:\n        image_data, height, width = _process_image(filename, bbox)\n      except ValueError:\n        dataset_utils.log(\'[thread %2d]: Invalid image found. %s - [skip].\' % (thread_index, filename))\n        continue\n\n      # crop \xec\x9d\xb4 \xed\x95\x84\xec\x9a\x94\xed\x95\x9c \xea\xb2\xbd\xec\x9a\xb0 bbox \xec\xa0\x95\xeb\xb3\xb4\xeb\xa5\xbc \xec\xb6\x94\xea\xb0\x80\xed\x95\x98\xeb\x8a\x94 \xeb\x8c\x80\xec\x8b\xa0 \xec\x9d\xb4\xeb\xaf\xb8\xec\xa7\x80\xeb\xa5\xbc \xec\xa7\x81\xec\xa0\x91 crop \xeb\x92\xa4\xec\x97\x90 \xec\xb6\x94\xea\xb0\x80\xed\x95\x9c\xeb\x8b\xa4.\n      example = data_util.convert_to_example_without_bbox(image_data, \'jpg\', label, height, width)\n      writer.write(example.SerializeToString())\n\n      counter += 1\n      shard_counter += 1\n      if not counter % 1000:\n        dataset_utils.log(\'%s [thread %2d]: Processed %d of %d images in thread batch.\' %\n                          (datetime.now(), thread_index, counter, num_files_in_thread))\n\n    writer.close()\n    dataset_utils.log(\'%s [thread %2d]: Wrote %d images to %s\' %\n                      (datetime.now(), thread_index, shard_counter, output_file))\n\n\ndef _process_dataset(name, filenames, labels, bbox_info, num_shards):\n  """"""\n  \xec\x9d\xb4\xeb\xaf\xb8\xec\xa7\x80 \xed\x8c\x8c\xec\x9d\xbc \xeb\xaa\xa9\xeb\xa1\x9d\xec\x9d\x84 \xec\x9d\xbd\xec\x96\xb4\xeb\x93\xa4\xec\x97\xac TFRecord \xea\xb0\x9d\xec\xb2\xb4\xeb\xa1\x9c \xeb\xb3\x80\xed\x99\x98\xed\x95\x98\xeb\x8a\x94 \xed\x95\xa8\xec\x88\x98\n  :param name: string, \xeb\x8d\xb0\xec\x9d\xb4\xed\x84\xb0 \xea\xb3\xa0\xec\x9c\xa0 \xeb\xac\xb8\xec\x9e\x90\xec\x97\xb4 (train, validation \xeb\x93\xb1)\n  :param filenames: list of strings; \xec\x9d\xb4\xeb\xaf\xb8\xec\xa7\x80 \xed\x8c\x8c\xec\x9d\xbc \xea\xb2\xbd\xeb\xa1\x9c \xeb\xa6\xac\xec\x8a\xa4\xed\x8a\xb8.\n  :param labels: list of integer; \xec\x9d\xb4\xeb\xaf\xb8\xec\xa7\x80\xec\x97\x90 \xeb\x8c\x80\xed\x95\x9c \xec\xa0\x95\xec\x88\x98\xed\x99\x94\xeb\x90\x9c \xec\xa0\x95\xeb\x8b\xb5 \xeb\xa0\x88\xec\x9d\xb4\xeb\xb8\x94 \xeb\xa6\xac\xec\x8a\xa4\xed\x8a\xb8\n  :param bbox_info: double dict of label id to file id. \xec\x82\xac\xec\x9a\xa9\xed\x95\x98\xec\xa7\x80 \xec\x95\x8a\xeb\x8a\x94 \xea\xb2\xbd\xec\x9a\xb0 None \xec\x9d\x84 \xec\x9e\x85\xeb\xa0\xa5\n  :param num_shards: \xeb\x8d\xb0\xec\x9d\xb4\xed\x84\xb0 \xec\xa7\x91\xed\x95\xa9\xec\x9d\x84 \xec\x83\xa4\xeb\x94\xa9\xed\x95\xa0 \xea\xb0\xaf\xec\x88\x98.\n  """"""\n  assert len(filenames) == len(labels)\n\n  shard_offsets = dataset_utils.make_shard_offsets(len(filenames), FLAGS.num_threads, num_shards)\n  shard_output_filenames = dataset_utils.make_shard_filenames(name, len(filenames), FLAGS.num_threads, num_shards)\n\n  def _process_batch(thread_index):\n    offsets = shard_offsets[thread_index]\n    output_filenames = shard_output_filenames[thread_index]\n    _process_image_files_batch(thread_index, offsets, output_filenames, filenames, labels, bbox_info)\n\n  dataset_utils.thread_execute(FLAGS.num_threads, _process_batch)\n  dataset_utils.log(\'%s: Finished writing all %d images in data set.\' % (datetime.now(), len(filenames)))\n\n\ndef main(_):\n  if (FLAGS.data_dir is None) or (FLAGS.output_dir is None):\n    parser.print_help()\n    return\n\n  assert not FLAGS.train_shards % FLAGS.num_threads, (\n    \'Please make the FLAGS.num_threads commensurate with FLAGS.train_shards\')\n  assert not FLAGS.validation_shards % FLAGS.num_threads, (\n    \'Please make the FLAGS.num_threads commensurate with FLAGS.validation_shards\')\n  print(\'Saving results to {}\'.format(FLAGS.output_dir))\n\n  dataset_utils.log(\'Make UEC-food100 TFRecord dataset.\')\n\n  if not os.path.exists(FLAGS.output_dir):\n    os.makedirs(FLAGS.output_dir)\n\n  if FLAGS.use_bbox:\n    bbox_info = _get_bbox_info(FLAGS.data_dir)\n    dataset_utils.log(\' - Use bounding box info. (opt. ON)\')\n  else:\n    bbox_info = None\n\n  # val{}.txt \xed\x8c\x8c\xec\x9d\xbc \xec\xa4\x91 0~3 \xec\x9d\x80 train set \xec\x9c\xbc\xeb\xa1\x9c \xec\x82\xac\xec\x9a\xa9\xed\x95\x98\xea\xb3\xa0 4\xeb\x8a\x94 validation set \xec\x9c\xbc\xeb\xa1\x9c \xec\x82\xac\xec\x9a\xa9.\n  train_filenames, train_labels = _get_filenames_and_labels([0, 1, 2, 3], shuffle=True)\n  validation_filenames, validation_labels = _get_filenames_and_labels([4], shuffle=False)\n\n  dataset_utils.log(\'Convert [train] dataset.\')\n  _process_dataset(\'train\', train_filenames, train_labels, bbox_info, FLAGS.train_shards)\n\n  dataset_utils.log(\'Convert [validation] dataset.\')\n  _process_dataset(\'validation\', validation_filenames, validation_labels, bbox_info, FLAGS.validation_shards)\n\n  dataset_utils.log(\'Make UEC-food100 TFRecord dataset. [OK]\')\n\n\nif __name__ == \'__main__\':\n  FLAGS, unparsed = parser.parse_known_args()\n  tf.app.run(argv=[sys.argv[0]] + unparsed)\n'"
datasets/dataset_utils.py,3,"b'#!/usr/bin/env python\n# coding=utf8\n""""""Contains utilities for downloading and converting datasets.""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport argparse\nimport gzip\nimport os\nimport shutil\nimport ssl\nimport sys\nimport tarfile\nimport threading\nimport zipfile\nfrom datetime import datetime\n\nimport numpy as np\nimport tensorflow as tf\nfrom six.moves import urllib\n\n\ndef log(msg, *args):\n  msg = \'[{}] \' + msg\n  print(msg.format(datetime.now(), *args))\n  sys.stdout.flush()\n\n\ndef str2bool(v):\n  y = [\'yes\', \'true\', \'t\', \'y\', \'1\']\n  n = [\'no\', \'false\', \'f\', \'n\', \'0\']\n  if v.lower() in y:\n    return True\n  elif v.lower() in n:\n    return False\n  else:\n    raise argparse.ArgumentTypeError(\'Boolean value expected.\')\n\n\ndef get_image_file_format(filename):\n  image_name = filename.rsplit(\'.\', 1)\n  if len(image_name) <= 1:\n    return \'jpg\'  # default format\n  image_format = image_name[-1].lower()\n  if image_format in [\'jpg\', \'jpeg\']:\n    return \'jpg\'\n  elif image_format in [\'bmp\', \'png\', \'gif\']:\n    return image_format\n  return """"\n\n\ndef download_file(url, data_dir, filename):\n  """"""\n  URL\xeb\xa1\x9c\xeb\xb6\x80\xed\x84\xb0 \xeb\x8d\xb0\xec\x9d\xb4\xed\x84\xb0\xeb\xa5\xbc \xeb\x8b\xa4\xec\x9a\xb4\xeb\xa1\x9c\xeb\x93\x9c\xed\x95\x9c\xeb\x8b\xa4.\n  :param url: \xec\xa0\x80\xec\x9e\xa5\xed\x95\xa0 \xed\x8c\x8c\xec\x9d\xbc\xec\x9d\x84 \xea\xb0\x80\xeb\xa6\xac\xed\x82\xa4\xeb\x8a\x94 URL.\n  :param data_dir: \xed\x8c\x8c\xec\x9d\xbc\xec\x9d\x84 \xec\xa0\x80\xec\x9e\xa5\xed\x95\xa0 \xeb\x94\x94\xeb\xa0\x89\xed\x86\xa0\xeb\xa6\xac \xea\xb2\xbd\xeb\xa1\x9c.\n  :param filename: \xec\xa0\x80\xec\x9e\xa5\xed\x95\xa0 \xed\x8c\x8c\xec\x9d\xbc \xec\x9d\xb4\xeb\xa6\x84.\n  :return: \xec\xa0\x80\xec\x9e\xa5\xeb\x90\x9c \xed\x8c\x8c\xec\x9d\xbc\xec\x9d\x98 \xea\xb2\xbd\xeb\xa1\x9c \xeb\x94\x94\xeb\xa0\x89\xed\x86\xa0\xeb\xa6\xac.\n  """"""\n  if not os.path.exists(data_dir):\n    os.makedirs(data_dir)\n\n  filepath = os.path.join(data_dir, filename)\n\n  def _progress(count, block_size, total_size):\n    if total_size > 0:\n      sys.stdout.write(\'\\r>> Downloading %s %.1f%%\' % (filename, float(count * block_size) / float(total_size) * 100.0))\n    else:\n      sys.stdout.write(\'\\r>> Downloading %s %s\' % (filename, \'.\' * (count % 20)))\n    sys.stdout.flush()\n\n  # This is the way to allow unverified SSL\n  ssl._create_default_https_context = ssl._create_unverified_context\n  filepath, _ = urllib.request.urlretrieve(url, filepath, _progress)\n  statinfo = os.stat(filepath)\n  print()\n  log(\'Successfully downloaded\', filename, statinfo.st_size, \'bytes.\')\n  return filepath\n\n\ndef download_and_uncompress_tarball(tarball_url, data_dir, filename):\n  """"""\n  tar \xed\x98\x95\xec\x8b\x9d\xec\x9c\xbc\xeb\xa1\x9c \xec\xa0\x80\xec\x9e\xa5\xeb\x90\x9c URL \xed\x8c\x8c\xec\x9d\xbc\xec\x9d\x84 \xeb\x8b\xa4\xec\x9a\xb4\xeb\xb0\x9b\xec\x95\x84 \xec\x95\x95\xec\xb6\x95\xec\x9d\x84 \xed\x92\x80\xec\x96\xb4 \xec\xa0\x80\xec\x9e\xa5\xed\x95\x9c\xeb\x8b\xa4.\n  :param tarball_url: tarball \xed\x8c\x8c\xec\x9d\xbc URL.\n  :param data_dir: The directory where the output files are stored.\n  :param filename: String, path to a output file.\n  :return: The directory where the outputfiles are stored.\n  """"""\n  filepath = download_file(tarball_url, data_dir, filename)\n  if filepath.endswith(\'tar\'):\n    tarfile.open(filepath, \'r:\').extractall(data_dir)\n  elif filepath.endswith(\'tar.gz\'):\n    tarfile.open(filepath, \'r:gz\').extractall(data_dir)\n  elif filepath.endswith(\'tgz\'):\n    tarfile.open(filepath, \'r:gz\').extractall(data_dir)\n  return data_dir\n\n\ndef download_and_uncompress_zip(zip_url, data_dir, zipped_filename):\n  """"""\n  zip \xed\x98\x95\xec\x8b\x9d\xec\x9c\xbc\xeb\xa1\x9c \xec\xa0\x80\xec\x9e\xa5\xeb\x90\x9c URL \xed\x8c\x8c\xec\x9d\xbc\xec\x9d\x84 \xeb\x8b\xa4\xec\x9a\xb4\xeb\xb0\x9b\xec\x95\x84 \xec\x95\x95\xec\xb6\x95\xec\x9d\x84 \xed\x92\x80\xec\x96\xb4 \xec\xa0\x80\xec\x9e\xa5\xed\x95\x9c\xeb\x8b\xa4.\n  :param zip_url: The URL of zip file.\n  :param data_dir: The directory where the output files are stored.\n  :param zipped_filename: String, path to a output file.\n  :return: Uncompredded file path.\n  """"""\n  zip_suffix = \'.zip\'\n  zip_len = len(zip_suffix)\n  assert len(zipped_filename) >= zip_len and zipped_filename[-zip_len:] == zip_suffix\n\n  zipped_filepath = download_file(zip_url, data_dir, zipped_filename)\n  zip_ref = zipfile.ZipFile(zipped_filepath, \'r\')\n  zip_ref.extractall(data_dir)\n  zip_ref.close()\n  return zipped_filepath\n\n\ndef download_and_uncompress_gzip(gzip_url, data_dir, zipped_filename):\n  """"""\n  Downloads the `gzip_url` and uncompresses it locally.\n  :param gzip_url: The URL of gzip file.\n  :param data_dir: The directory where the output files are stored.\n  :param zipped_filename: String, path to a output file.\n  :return: Uncompredded file path.\n  """"""\n  zip_suffix = \'.gz\'\n  zip_len = len(zip_suffix)\n  assert len(zipped_filename) >= zip_len and zipped_filename[-zip_len:] == zip_suffix\n\n  zipped_filepath = download_file(gzip_url, data_dir, zipped_filename)\n  filepath = zipped_filepath[:-zip_len]\n  with gzip.open(zipped_filepath, \'rb\') as f_in:\n    # gzip only suppport single file.\n    with tf.gfile.Open(filepath, \'wb\') as f_out:\n      shutil.copyfileobj(f_in, f_out)\n  return filepath\n\n\ndef thread_execute(num_threads, fn):\n  """"""\n  Thread \xeb\x8b\xa8\xec\x9c\x84\xeb\xa1\x9c fn \xec\x9d\x84 \xeb\xb3\x91\xeb\xa0\xac \xec\x88\x98\xed\x96\x89\xed\x95\x9c\xeb\x8b\xa4.\n  :param num_threads: thread \xea\xb0\xaf\xec\x88\x98\n  :param target_fn: thread \xea\xb0\x80 \xec\x88\x98\xed\x96\x89\xed\x95\xa0 \xed\x95\xa8\xec\x88\x98\xeb\xa1\x9c \xec\xb2\xab\xeb\xb2\x88\xec\xa7\xb8 \xec\x9d\xb8\xec\x9e\x90\xec\x97\x90 thread index\xeb\xa5\xbc \xeb\x84\x98\xea\xb8\xb4\xeb\x8b\xa4.\n  """"""\n  assert num_threads > 0\n  # Create a mechanism for monitoring when all threads are finished.\n  coord = tf.train.Coordinator()\n  threads = []\n  for idx in range(num_threads):\n    t = threading.Thread(target=fn, args=(idx,))\n    t.start()\n    threads.append(t)\n  coord.join(threads)  # Wait for all the threads to terminate.\n\n\ndef split(contents, num_split, start_index=0):\n  """"""\n  contents \xeb\xa5\xbc num_split \xea\xb0\x92\xeb\xa7\x8c\xed\x81\xbc \xeb\xb6\x84\xed\x95\xa0\xed\x95\x98\xec\x97\xac \xeb\xa6\xac\xec\x8a\xa4\xed\x8a\xb8\xeb\xa1\x9c \xeb\xb0\x98\xed\x99\x98.\n  :param contents: \xeb\xb6\x84\xed\x95\xa0\xed\x95\x98\xea\xb3\xa0\xec\x9e\x90 \xed\x95\x98\xeb\x8a\x94 \xeb\x8d\xb0\xec\x9d\xb4\xed\x84\xb0 \xeb\xa6\xac\xec\x8a\xa4\xed\x8a\xb8\n  :param num_split: \xeb\xb6\x84\xed\x95\xa0 \xea\xb0\xaf\xec\x88\x98\n  :param start_index: contents \xec\x8b\x9c\xec\x9e\x91 \xec\x9d\xb8\xeb\x8d\xb1\xec\x8a\xa4 \xeb\xb2\x88\xed\x98\xb8\xeb\xa1\x9c default \xea\xb0\x92\xec\x9c\xbc\xeb\xa1\x9c 0\xec\x9d\x84 \xec\x82\xac\xec\x9a\xa9\n  :return: split \xea\xb0\xaf\xec\x88\x98 \xed\x81\xac\xea\xb8\xb0\xec\x9d\x98 \xeb\x8d\x94\xeb\xb8\x94 \xeb\xa6\xac\xec\x8a\xa4\xed\x8a\xb8. [[...],[...],...]\n  """"""\n  rs = np.linspace(start_index, len(contents), num_split + 1).astype(np.int)\n  result = [contents[rs[i]:rs[i + 1]] for i in range(len(rs) - 1)]\n  return result\n\n\ndef split_range(total, num_split, start_index=0):\n  """"""\n  \xec\xa0\x95\xec\x88\x98 \xeb\xb2\x94\xec\x9c\x84\xec\x9d\x98 \xea\xb0\x92\xec\x9d\x84 num_split \xea\xb0\x92\xeb\xa7\x8c\xed\x81\xbc \xeb\xb6\x84\xed\x95\xa0\xed\x95\x98\xec\x97\xac \xed\x95\xb4\xeb\x8b\xb9 start, end \xec\x9d\xb8\xeb\x8d\xb1\xec\x8a\xa4\xeb\xa5\xbc \xeb\xb0\x98\xed\x99\x98.\n  :param total: \xeb\xb6\x84\xed\x95\xa0\xed\x95\x98\xea\xb3\xa0\xec\x9e\x90 \xed\x95\x98\xeb\x8a\x94 max \xea\xb0\x92\n  :param num_split: \xeb\xb6\x84\xed\x95\xa0 \xea\xb0\xaf\xec\x88\x98\n  :param start_index: contents \xec\x8b\x9c\xec\x9e\x91 \xec\x9d\xb8\xeb\x8d\xb1\xec\x8a\xa4 \xeb\xb2\x88\xed\x98\xb8\xeb\xa1\x9c default \xea\xb0\x92\xec\x9c\xbc\xeb\xa1\x9c 0\xec\x9d\x84 \xec\x82\xac\xec\x9a\xa9\n  :return: split \xea\xb0\xaf\xec\x88\x98 \xed\x81\xac\xea\xb8\xb0\xec\x9d\x98 \xeb\xa6\xac\xec\x8a\xa4\xed\x8a\xb8\xeb\xa1\x9c start/end \xec\x9d\xb8\xeb\x8d\xb1\xec\x8a\xa4 \xed\x8a\x9c\xed\x94\x8c\xec\x9d\x84 \xec\x9b\x90\xec\x86\x8c\xeb\xa1\x9c \xea\xb0\x80\xec\xa7\x90.  [(s,e),(s,e),...]\n  """"""\n  rs = np.linspace(start_index, total, num_split + 1).astype(np.int)\n  result = [(rs[i], rs[i + 1]) for i in range(len(rs) - 1)]\n  return result\n\n\ndef make_shard_offsets(total, num_threads, num_shards):\n  """"""\n  Thread \xec\x99\x80 thread \xeb\x82\xb4 shard \xec\x97\x90\xec\x84\x9c \xec\x82\xac\xec\x9a\xa9\xed\x95\xa0 \xec\x9d\xb8\xeb\x8d\xb1\xec\x8a\xa4 \xeb\xb2\x94\xec\x9c\x84\xeb\xa5\xbc \xec\x83\x9d\xec\x84\xb1\n  :param total: \xeb\xb6\x84\xed\x95\xa0\xed\x95\x98\xea\xb3\xa0\xec\x9e\x90 \xed\x95\x98\xeb\x8a\x94 max \xea\xb0\x92\n  :param num_threads: thread \xea\xb0\xaf\xec\x88\x98\n  :param num_shards: \xec\xb4\x9d shard \xec\x88\x98\xeb\xa1\x9c (num_threads * num_shards_per_thread)\xec\x99\x80 \xea\xb0\x99\xeb\x8b\xa4.\n  :return: [[(s,e),(s,e)...],[()()...],...] \xec\x99\x80 \xea\xb0\x99\xec\x9d\x80 \xed\x98\x95\xed\x83\x9c\xec\x9d\x98 \xeb\x8d\x94\xeb\xb8\x94 \xeb\xa6\xac\xec\x8a\xa4\xed\x8a\xb8.\n  """"""\n  assert total > 0\n  assert num_threads > 0\n  assert num_shards > 0\n  assert not num_shards % num_threads\n\n  num_shards_per_batch = int(num_shards / num_threads)\n  thread_range = split_range(total, num_threads)\n  offsets = []\n  for start, end in thread_range:\n    offsets.append(split_range(end, num_shards_per_batch, start_index=start))\n  return offsets\n\n\ndef make_shard_filenames(name, total, num_threads, num_shards):\n  assert total > 0\n  assert num_threads > 0\n  assert num_shards > 0\n\n  offsets = make_shard_offsets(total, num_threads, num_shards)\n  filenames = []\n  shard_idx = 0\n  for thread_offsets in offsets:\n    shard_filenames = []\n    for _ in thread_offsets:\n      filename = \'%s-%.5d-of-%.5d\' % (name, shard_idx, num_shards)\n      shard_idx += 1\n      shard_filenames.append(filename)\n    filenames.append(shard_filenames)\n  return filenames\n\n\ndef make_label_id_to_name(data_dir, start_index=0):\n  """"""\n  \xeb\xa0\x88\xec\x9d\xb4\xeb\xb8\x94 \xec\x9d\xb4\xeb\xa6\x84\xec\x9d\xb4 \xeb\x94\x94\xeb\xa0\x89\xed\x86\xa0\xeb\xa6\xac\xec\x9d\xb8 \xea\xb2\xbd\xec\x9a\xb0 \xec\x9d\xb4\xeb\xa5\xbc \xec\x9d\xbd\xec\x96\xb4 \xed\x95\x99\xec\x8a\xb5\xec\x97\x90 \xec\x82\xac\xec\x9a\xa9\xed\x95\xa0 label id \xeb\xa1\x9c \xeb\xa7\xa4\xed\x95\x91\xed\x95\x98\xeb\x8a\x94 \xed\x95\xa8\xec\x88\x98.\n  \xec\x95\x84\xeb\x9e\x98\xec\x99\x80 \xea\xb0\x99\xec\x9d\x80 \xed\x98\x95\xec\x8b\x9d\xec\x9d\x98 \xeb\x8d\xb0\xec\x9d\xb4\xed\x84\xb0\xeb\xa5\xbc {0:labelA, 1:labelB} \xec\x99\x80 \xea\xb0\x99\xec\x9d\x80 dict \xea\xb0\x9d\xec\xb2\xb4\xeb\xa1\x9c \xeb\xb3\x80\xed\x99\x98\xed\x95\x9c\xeb\x8b\xa4.\n    data_dir/labelA/xxx.jpg \n    data_dir/labelB/yyy.jpg\n  :param data_dir: \xeb\xa0\x88\xec\x9d\xb4\xeb\xb8\x94 \xeb\x94\x94\xeb\xa0\x89\xed\x86\xa0\xeb\xa6\xac\xea\xb0\x80 \xed\x8f\xac\xed\x95\xa8\xeb\x90\x9c \xec\x83\x81\xec\x9c\x84 \xeb\x94\x94\xeb\xa0\x89\xed\x86\xa0\xeb\xa6\xac \xec\x9d\xb4\xeb\xa6\x84.\n  :return: name[id] \xed\x98\x95\xec\x8b\x9d\xec\x9d\x98 dict \xea\xb0\x9d\xec\xb2\xb4.\n  """"""\n  id_to_name = {}\n  label_index = 0 + start_index\n  # os.listdir()\xec\x9d\x80 \xec\x88\x9c\xec\x84\x9c\xeb\xa5\xbc \xeb\xb3\xb4\xec\x9e\xa5\xed\x95\x98\xec\xa7\x80 \xec\x95\x8a\xec\x9c\xbc\xeb\xaf\x80\xeb\xa1\x9c \xeb\xb0\x98\xeb\x93\x9c\xec\x8b\x9c sort\xed\x95\x98\xec\x97\xac \xec\x82\xac\xec\x9a\xa9.\n  for label_name in sorted(os.listdir(data_dir)):\n    path = os.path.join(data_dir, label_name)\n    if os.path.isdir(path):\n      image_file_path = \'%s/*\' % (path)\n      matching_files = tf.gfile.Glob(image_file_path)\n      id_to_name[label_index] = label_name\n      label_index += 1\n  return id_to_name\n\n\ndef make_label_name_to_id(data_dir, start_index=0):\n  """"""\n  \xeb\xa0\x88\xec\x9d\xb4\xeb\xb8\x94 \xec\x9d\xb4\xeb\xa6\x84\xec\x9d\xb4 \xeb\x94\x94\xeb\xa0\x89\xed\x86\xa0\xeb\xa6\xac\xec\x9d\xb8 \xea\xb2\xbd\xec\x9a\xb0 \xec\x9d\xb4\xeb\xa5\xbc \xec\x9d\xbd\xec\x96\xb4 \xed\x95\x99\xec\x8a\xb5\xec\x97\x90 \xec\x82\xac\xec\x9a\xa9\xed\x95\xa0 label id \xeb\xa1\x9c \xeb\xa7\xa4\xed\x95\x91\xed\x95\x98\xeb\x8a\x94 \xed\x95\xa8\xec\x88\x98.\n  \xec\x95\x84\xeb\x9e\x98\xec\x99\x80 \xea\xb0\x99\xec\x9d\x80 \xed\x98\x95\xec\x8b\x9d\xec\x9d\x98 \xeb\x8d\xb0\xec\x9d\xb4\xed\x84\xb0\xeb\xa5\xbc {labelA:0, labelB:1} \xec\x99\x80 \xea\xb0\x99\xec\x9d\x80 dict \xea\xb0\x9d\xec\xb2\xb4\xeb\xa1\x9c \xeb\xb3\x80\xed\x99\x98\xed\x95\x9c\xeb\x8b\xa4.\n    data_dir/labelA/xxx.jpg \n    data_dir/labelB/yyy.jpg\n  :param data_dir: \xeb\xa0\x88\xec\x9d\xb4\xeb\xb8\x94 \xeb\x94\x94\xeb\xa0\x89\xed\x86\xa0\xeb\xa6\xac\xea\xb0\x80 \xed\x8f\xac\xed\x95\xa8\xeb\x90\x9c \xec\x83\x81\xec\x9c\x84 \xeb\x94\x94\xeb\xa0\x89\xed\x86\xa0\xeb\xa6\xac \xec\x9d\xb4\xeb\xa6\x84.\n  :return: id[name] \xed\x98\x95\xec\x8b\x9d\xec\x9d\x98 dict \xea\xb0\x9d\xec\xb2\xb4.\n  """"""\n  name_to_id = {}\n  label_index = 0 + start_index\n  # os.listdir()\xec\x9d\x80 \xec\x88\x9c\xec\x84\x9c\xeb\xa5\xbc \xeb\xb3\xb4\xec\x9e\xa5\xed\x95\x98\xec\xa7\x80 \xec\x95\x8a\xec\x9c\xbc\xeb\xaf\x80\xeb\xa1\x9c \xeb\xb0\x98\xeb\x93\x9c\xec\x8b\x9c sort\xed\x95\x98\xec\x97\xac \xec\x82\xac\xec\x9a\xa9.\n  for label_name in sorted(os.listdir(data_dir)):\n    path = os.path.join(data_dir, label_name)\n    if os.path.isdir(path):\n      name_to_id[label_name] = label_index\n      label_index += 1\n  return name_to_id\n\n\ndef write_label_id_to_name(name, data_dir, output_dir=None, start_index=0):\n  """"""\n  id_to_name \xec\xa0\x95\xeb\xb3\xb4\xeb\xa5\xbc data_dir \xec\x97\x90 \xea\xb8\xb0\xeb\xa1\x9d\xed\x95\x9c\xeb\x8b\xa4.\n  :param name: \xec\xa0\x80\xec\x9e\xa5\xed\x95\xa0 \xeb\x8d\xb0\xec\x9d\xb4\xed\x84\xb0\xec\x9d\x98 \xed\x83\x80\xec\x9e\x85. ex) train, validation\n  :param data_dir: \xeb\xa0\x88\xec\x9d\xb4\xeb\xb8\x94 \xeb\x94\x94\xeb\xa0\x89\xed\x86\xa0\xeb\xa6\xac\xea\xb0\x80 \xed\x8f\xac\xed\x95\xa8\xeb\x90\x9c \xec\x83\x81\xec\x9c\x84 \xeb\x94\x94\xeb\xa0\x89\xed\x86\xa0\xeb\xa6\xac \xec\x9d\xb4\xeb\xa6\x84.\n  :param output_dir: \xed\x8c\x8c\xec\x9d\xbc\xec\x9d\x84 \xea\xb8\xb0\xeb\xa1\x9d\xed\x95\xa0 \xea\xb2\xbd\xeb\xa1\x9c \xeb\x94\x94\xeb\xa0\x89\xed\x86\xa0\xeb\xa6\xac. None \xec\x9d\xb8 \xea\xb2\xbd\xec\x9a\xb0 data_dir\xec\x9d\x84 \xec\x82\xac\xec\x9a\xa9.\n  """"""\n  id_to_name = make_label_id_to_name(data_dir, start_index)\n  output_filename = \'%s_labels.txt\' % (name)\n  if not output_dir:\n    output_dir = data_dir\n  output_file = os.path.join(output_dir, output_filename)\n  with open(output_file, \'w\') as f:\n    for index in sorted(id_to_name):\n      f.write(\'%d:%s\\n\' % (index, id_to_name[index]))\n\n\ndef check_label_id_to_name_files(data_dir, train_name=\'train\', validation_name=\'validation\'):\n  train_filename = os.path.join(data_dir, \'%s_labels.txt\' % (train_name))\n  validation_filename = os.path.join(data_dir, \'%s_labels.txt\' % (validation_name))\n\n  def _load_id_to_name(filename):\n    id_to_name = {}\n    with open(filename, \'r\') as f:\n      labels = f.readlines()\n      for label in labels:\n        label_id, name = label.strip().split(\':\', 2)\n        id_to_name[int(label_id)] = name\n      return id_to_name\n\n  train = _load_id_to_name(train_filename)\n  validation = _load_id_to_name(validation_filename)\n\n  for key, val in train.items():\n    if not key in validation:\n      log(""Warn: The label index (%d:%s) is not exist in validation but train."" % (key, val))\n    elif val != validation[key]:\n      msg = ""train(%d:%s) vs. validation(%d:%s)"" % (key, val, key, validation[key])\n      raise ValueError(""Invalid label : {}"".format(msg))\n\n  for key, val in validation.items():\n    if not key in train:\n      msg = ""valdation(%d:%s) is not exist in train label."" % (key, val)\n      raise ValueError(""Invalid label : {}"".format(msg))\n\n\ndef write_tfrecord_info(output_dir, num_train, num_validation):\n  """"""\n  train, validation \xec\xa0\x95\xeb\xb3\xb4\xeb\xa5\xbc \xed\x8c\x8c\xec\x9d\xbc\xec\x97\x90 \xea\xb8\xb0\xeb\xa1\x9d\xed\x95\x9c\xeb\x8b\xa4.\n  :param output_dir: \xed\x8c\x8c\xec\x9d\xbc\xec\x9d\x84 \xea\xb8\xb0\xeb\xa1\x9d\xed\x95\xa0 \xea\xb2\xbd\xeb\xa1\x9c \xeb\x94\x94\xeb\xa0\x89\xed\x86\xa0\xeb\xa6\xac. None \xec\x9d\xb8 \xea\xb2\xbd\xec\x9a\xb0 data_dir\xec\x9d\x84 \xec\x82\xac\xec\x9a\xa9.\n  :param num_train: train \xeb\x8d\xb0\xec\x9d\xb4\xed\x84\xb0 \xea\xb0\xaf\xec\x88\x98\n  :param num_validation: validation \xeb\x8d\xb0\xec\x9d\xb4\xed\x84\xb0 \xea\xb0\xaf\xec\x88\x98\n  """"""\n  id_to_name = make_label_id_to_name(output_dir)\n  output_filename = \'tfrecord_info.txt\'\n  output_file = os.path.join(output_dir, output_filename)\n  with open(output_file, \'w\') as f:\n    f.write(\'tfrecord info\\n\')\n    f.write(\'- train: %s\\n\' % (num_train))\n    f.write(\'- validation: %s\\n\' % (num_validation))\n'"
datasets/download_cars196.py,0,"b'# -*- coding: utf-8 -*-\nimport os\nimport contextlib\nimport time\nfrom six.moves.urllib import request\nfrom progressbar import ProgressBar, Percentage, Bar, ETA, FileTransferSpeed\n\nimport sys\n\nsys.path.append(""../"")\n\nfuel_root_path = \'.\'\nbase_url = ""http://imagenet.stanford.edu/internal/car196/""\nfilenames = [""car_ims.tgz"", ""cars_annos.mat""]\nurls = [base_url + f for f in filenames]\n\nfuel_data_path = os.path.join(fuel_root_path, ""cars196"")\nos.mkdir(fuel_data_path)\n\nfor filename in filenames:\n  url = base_url + filename\n  filepath = os.path.join(fuel_data_path, filename)\n\n  with contextlib.closing(request.urlopen(url)) as f:\n    expected_filesize = int(f.headers[""content-length""])\n    print(expected_filesize)\n  time.sleep(5)\n\n  widgets = [\'{}: \'.format(filename), Percentage(), \' \', Bar(), \' \', ETA(),\n             \' \', FileTransferSpeed()]\n  progress_bar = ProgressBar(widgets=widgets,\n                             maxval=expected_filesize).start()\n\n\n  def reporthook(count, blockSize, totalSize):\n    progress_bar.update(min(count * blockSize, totalSize))\n\n\n  request.urlretrieve(url, filepath, reporthook=reporthook)\n  progress_bar.finish()\n\n  downloaded_filesize = os.path.getsize(filepath)\n  assert expected_filesize == downloaded_filesize, "" "".join((\n    ""expected file size is {}, but the actual size of the downloaded file"",\n    ""is {}."")).format(expected_filesize, downloaded_filesize)\n'"
datasets/download_fgvc_aircraft.py,0,"b'# -*- coding: utf-8 -*-\nimport os\nimport contextlib\nimport time\nfrom six.moves.urllib import request\nfrom progressbar import ProgressBar, Percentage, Bar, ETA, FileTransferSpeed\n\nimport sys\n\nsys.path.append(""../datasets"")\n\nfuel_root_path = \'.\'\nbase_url = ""http://www.robots.ox.ac.uk/~vgg/data/fgvc-aircraft/archives/""\nfilenames = [""fgvc-aircraft-2013b.tar.gz""]\n\nurls = [base_url + f for f in filenames]\n\nfuel_data_path = os.path.join(fuel_root_path, ""fgvc_aircraft"")\nos.mkdir(fuel_data_path)\n\nfor filename in filenames:\n  url = base_url + filename\n  filepath = os.path.join(fuel_data_path, filename)\n\n  with contextlib.closing(request.urlopen(url)) as f:\n    expected_filesize = int(f.headers[""content-length""])\n    print(expected_filesize)\n  time.sleep(5)\n\n  widgets = [\'{}: \'.format(filename), Percentage(), \' \', Bar(), \' \', ETA(),\n             \' \', FileTransferSpeed()]\n  progress_bar = ProgressBar(widgets=widgets,\n                             maxval=expected_filesize).start()\n\n\n  def reporthook(count, blockSize, totalSize):\n    progress_bar.update(min(count * blockSize, totalSize))\n\n\n  request.urlretrieve(url, filepath, reporthook=reporthook)\n  progress_bar.finish()\n\n  downloaded_filesize = os.path.getsize(filepath)\n  assert expected_filesize == downloaded_filesize, "" "".join((\n    ""expected file size is {}, but the actual size of the downloaded file"",\n    ""is {}."")).format(expected_filesize, downloaded_filesize)\n'"
datasets/download_oxford_iiit_pet.py,0,"b'# -*- coding: utf-8 -*-\nimport os\nimport contextlib\nimport time\nfrom six.moves.urllib import request\nfrom progressbar import ProgressBar, Percentage, Bar, ETA, FileTransferSpeed\n\nimport sys\n\nsys.path.append(""../datasets"")\n\nfuel_root_path = \'.\'\nbase_url = ""http://www.robots.ox.ac.uk/~vgg/data/pets/data/""\nfilenames = [""images.tar.gz"", ""annotations.tar.gz""]\n\nurls = [base_url + f for f in filenames]\n\nfuel_data_path = os.path.join(fuel_root_path, ""oxford_iiit_pet"")\nos.mkdir(fuel_data_path)\n\nfor filename in filenames:\n  url = base_url + filename\n  filepath = os.path.join(fuel_data_path, filename)\n\n  with contextlib.closing(request.urlopen(url)) as f:\n    expected_filesize = int(f.headers[""content-length""])\n    print(expected_filesize)\n  time.sleep(5)\n\n  widgets = [\'{}: \'.format(filename), Percentage(), \' \', Bar(), \' \', ETA(),\n             \' \', FileTransferSpeed()]\n  progress_bar = ProgressBar(widgets=widgets,\n                             maxval=expected_filesize).start()\n\n\n  def reporthook(count, blockSize, totalSize):\n    progress_bar.update(min(count * blockSize, totalSize))\n\n\n  request.urlretrieve(url, filepath, reporthook=reporthook)\n  progress_bar.finish()\n\n  downloaded_filesize = os.path.getsize(filepath)\n  assert expected_filesize == downloaded_filesize, "" "".join((\n    ""expected file size is {}, but the actual size of the downloaded file"",\n    ""is {}."")).format(expected_filesize, downloaded_filesize)\n'"
datasets/image_coder.py,12,"b'#!/usr/bin/env python\n# coding=utf8\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport numpy as np\n# use eager execution\nimport tensorflow as tf\n\ntf.enable_eager_execution()\n\n\n# decorator\ndef with_tf_cpu(fn):\n  def wrapper_fn(*args, **kwargs):\n    with tf.device(\'CPU:0\'):\n      return fn(*args, **kwargs)\n\n  return wrapper_fn\n\n\n@with_tf_cpu\ndef decode_png(contents):\n  image = tf.image.decode_png(contents, channels=3)\n  return image.numpy()\n\n\n@with_tf_cpu\ndef decode_jpg(contents):\n  # \xec\xa0\x95\xed\x99\x95\xed\x95\x9c \xec\x84\xb1\xeb\x8a\xa5\xec\x9d\x84 \xeb\x82\xb4\xea\xb8\xb0 \xec\x9c\x84\xed\x95\xb4\xec\x84\x9c\xeb\x8a\x94 dct_method=\'INTEGER_ACCURATE\' \xeb\xa5\xbc \xec\x82\xac\xec\x9a\xa9\xed\x95\xb4\xec\x95\xbc \xed\x95\x9c\xeb\x8b\xa4.\n  image = tf.image.decode_jpeg(contents, channels=3, dct_method=\'INTEGER_ACCURATE\')\n  return image.numpy()\n\n\n@with_tf_cpu\ndef decode_bmp(contents):\n  image = tf.image.decode_bmp(contents, channels=3)\n  return image.numpy()\n\n\n@with_tf_cpu\ndef decode_gif(contents):\n  image = tf.image.decode_gif(contents).numpy()\n  if len(image.shape) == 4:\n    return image[0,]\n  return image\n\n\n@with_tf_cpu\ndef decode(contents, image_type=None):\n  if image_type == \'jpg\':\n    return decode_jpg(contents)\n  elif image_type == \'png\':\n    return decode_png(contents)\n  elif image_type == \'bmp\':\n    return decode_bmp(contents)\n  elif image_type == \'gif\':\n    return decode_gif(contents)\n  image = tf.image.decode_image(contents).numpy()\n  if len(image.shape) == 4:\n    return image[0,]\n  return image\n\n\n@with_tf_cpu\ndef encode_png(image):\n  image_data = tf.image.encode_png(image)\n  return image_data.numpy()\n\n\n@with_tf_cpu\ndef encode_jpg(image):\n  image_data = tf.image.encode_jpeg(image, format=\'rgb\', quality=100)\n  return image_data.numpy()\n\n\ndef crop(image, offset_height, offset_width, crop_height, crop_width):\n  assert len(image.shape) == 3\n  assert offset_height >= 0\n  assert offset_width >= 0\n  assert crop_height > 0\n  assert crop_width > 0\n  assert (offset_height + crop_height) <= image.shape[0]\n  assert (offset_width + crop_width) <= image.shape[1]\n  return image[offset_height:(offset_height + crop_height),\n         offset_width:(offset_width + crop_width)]\n\n\ndef crop_bbox(image, bbox=None):\n  if bbox is None:\n    return image\n  return crop(image, bbox[0], bbox[1], bbox[2], bbox[3])\n\n\n@with_tf_cpu\ndef central_crop(image, central_fraction):\n  cropped_image = tf.image.central_crop(image, central_fraction)\n  return cropped_image.numpy()\n\n\ndef _smallest_size_at_least(height, width, smallest_side):\n  """"""\n  Computes new shape with the smallest side equal to `smallest_side`.\n  :param height: A python integer indicating the current height.\n  :param width: A python integer indicating the current width.\n  :param smallest_side: A python integer indicating the size of the smallest side after resize.\n  :return: tuple (new_height, new_width) of int type.\n  """"""\n  assert height > 0\n  assert width > 0\n  assert smallest_side > 0\n  scale = float(smallest_side) / min(height, width)\n  new_height = int(np.rint(height * scale))\n  new_width = int(np.rint(width * scale))\n  return (new_height, new_width)\n\n\n@with_tf_cpu\ndef resize(image, height, width):\n  image = tf.image.resize_images(\n    image, [height, width],\n    method=tf.image.ResizeMethod.BILINEAR,\n    align_corners=False)\n  return image.numpy().astype(np.uint8)\n\n\n@with_tf_cpu\ndef aspect_preserving_resize(image, resize_min):\n  """"""\n  Resize images preserving the original aspect ratio.\n\n  :param image: A 3-D image `Tensor`.\n  :param resize_min: A python integer or scalar `Tensor` indicating the size ofthe smallest side after resize.\n  :return: A 3-D tensor containing the resized image.\n  """"""\n  height, width = image.shape[0], image.shape[1]\n  new_height, new_width = _smallest_size_at_least(height, width, resize_min)\n  return resize(image, new_height, new_width), new_height, new_width\n'"
datasets/preprocess_imagenet_validation_data.py,0,"b'#!/usr/bin/python\n# Copyright 2016 Google Inc. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Process the ImageNet Challenge bounding boxes for TensorFlow model training.\n\nAssociate the ImageNet 2012 Challenge validation data set with labels.\n\nThe raw ImageNet validation data set is expected to reside in JPEG files\nlocated in the following directory structure.\n\n data_dir/ILSVRC2012_val_00000001.JPEG\n data_dir/ILSVRC2012_val_00000002.JPEG\n ...\n data_dir/ILSVRC2012_val_00050000.JPEG\n\nThis script moves the files into a directory structure like such:\n data_dir/n01440764/ILSVRC2012_val_00000293.JPEG\n data_dir/n01440764/ILSVRC2012_val_00000543.JPEG\n ...\nwhere \'n01440764\' is the unique synset label associated with\nthese images.\n\nThis directory reorganization requires a mapping from validation image\nnumber (i.e. suffix of the original file) to the associated label. This\nis provided in the ImageNet development kit via a Matlab file.\n\nIn order to make life easier and divorce ourselves from Matlab, we instead\nsupply a custom text file that provides this mapping for us.\n\nSample usage:\n  ./preprocess_imagenet_validation_data.py ILSVRC2012_img_val \\\n  imagenet_2012_validation_synset_labels.txt\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport os\nimport errno\nimport os.path\nimport sys\n\nif __name__ == \'__main__\':\n  if len(sys.argv) < 3:\n    print(\'Invalid usage\\n\'\n          \'usage: preprocess_imagenet_validation_data.py \'\n          \'<validation data dir> <validation labels file>\')\n    sys.exit(-1)\n  data_dir = sys.argv[1]\n  validation_labels_file = sys.argv[2]\n\n  # Read in the 50000 synsets associated with the validation data set.\n  labels = [l.strip() for l in open(validation_labels_file).readlines()]\n  unique_labels = set(labels)\n\n  # Make all sub-directories in the validation data dir.\n  for label in unique_labels:\n    labeled_data_dir = os.path.join(data_dir, label)\n    # Catch error if sub-directory exists\n    try:\n      os.makedirs(labeled_data_dir)\n    except OSError as e:\n      # Raise all errors but \'EEXIST\'\n      if e.errno != errno.EEXIST:\n        raise\n\n  # Move all of the image to the appropriate sub-directory.\n  for i in range(len(labels)):\n    basename = \'ILSVRC2012_val_000%.5d.JPEG\' % (i + 1)\n    original_filename = os.path.join(data_dir, basename)\n    if not os.path.exists(original_filename):\n      print(\'Failed to find: %s\' % original_filename)\n      sys.exit(-1)\n    new_filename = os.path.join(data_dir, labels[i], basename)\n    os.rename(original_filename, new_filename)\n'"
datasets/process_bounding_boxes.py,0,"b'#!/usr/bin/python\n# Copyright 2016 Google Inc. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Process the ImageNet Challenge bounding boxes for TensorFlow model training.\n\nThis script is called as\n\nprocess_bounding_boxes.py <dir> [synsets-file]\n\nWhere <dir> is a directory containing the downloaded and unpacked bounding box\ndata. If [synsets-file] is supplied, then only the bounding boxes whose\nsynstes are contained within this file are returned. Note that the\n[synsets-file] file contains synset ids, one per line.\n\nThe script dumps out a CSV text file in which each line contains an entry.\n  n00007846_64193.JPEG,0.0060,0.2620,0.7545,0.9940\n\nThe entry can be read as:\n  <JPEG file name>, <xmin>, <ymin>, <xmax>, <ymax>\n\nThe bounding box for <JPEG file name> contains two points (xmin, ymin) and\n(xmax, ymax) specifying the lower-left corner and upper-right corner of a\nbounding box in *relative* coordinates.\n\nThe user supplies a directory where the XML files reside. The directory\nstructure in the directory <dir> is assumed to look like this:\n\n<dir>/nXXXXXXXX/nXXXXXXXX_YYYY.xml\n\nEach XML file contains a bounding box annotation. The script:\n\n (1) Parses the XML file and extracts the filename, label and bounding box info.\n\n (2) The bounding box is specified in the XML files as integer (xmin, ymin) and\n    (xmax, ymax) *relative* to image size displayed to the human annotator. The\n    size of the image displayed to the human annotator is stored in the XML file\n    as integer (height, width).\n\n    Note that the displayed size will differ from the actual size of the image\n    downloaded from image-net.org. To make the bounding box annotation useable,\n    we convert bounding box to floating point numbers relative to displayed\n    height and width of the image.\n\n    Note that each XML file might contain N bounding box annotations.\n\n    Note that the points are all clamped at a range of [0.0, 1.0] because some\n    human annotations extend outside the range of the supplied image.\n\n    See details here: http://image-net.org/download-bboxes\n\n(3) By default, the script outputs all valid bounding boxes. If a\n    [synsets-file] is supplied, only the subset of bounding boxes associated\n    with those synsets are outputted. Importantly, one can supply a list of\n    synsets in the ImageNet Challenge and output the list of bounding boxes\n    associated with the training images of the ILSVRC.\n\n    We use these bounding boxes to inform the random distortion of images\n    supplied to the network.\n\nIf you run this script successfully, you will see the following output\nto stderr:\n> Finished processing 544546 XML files.\n> Skipped 0 XML files not in ImageNet Challenge.\n> Skipped 0 bounding boxes not in ImageNet Challenge.\n> Wrote 615299 bounding boxes from 544546 annotated images.\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport glob\nimport os.path\nimport sys\nimport xml.etree.ElementTree as ET\n\n\nclass BoundingBox(object):\n  pass\n\n\ndef GetItem(name, root, index=0):\n  count = 0\n  for item in root.iter(name):\n    if count == index:\n      return item.text\n    count += 1\n  # Failed to find ""index"" occurrence of item.\n  return -1\n\n\ndef GetInt(name, root, index=0):\n  # In some XML annotation files, the point values are not integers, but floats.\n  # So we add a float function to avoid ValueError.\n  return int(float(GetItem(name, root, index)))\n\n\ndef FindNumberBoundingBoxes(root):\n  index = 0\n  while True:\n    if GetInt(\'xmin\', root, index) == -1:\n      break\n    index += 1\n  return index\n\n\ndef ProcessXMLAnnotation(xml_file):\n  """"""Process a single XML file containing a bounding box.""""""\n  # pylint: disable=broad-except\n  try:\n    tree = ET.parse(xml_file)\n  except Exception:\n    print(\'Failed to parse: \' + xml_file, file=sys.stderr)\n    return None\n  # pylint: enable=broad-except\n  root = tree.getroot()\n\n  num_boxes = FindNumberBoundingBoxes(root)\n  boxes = []\n\n  for index in range(num_boxes):\n    box = BoundingBox()\n    # Grab the \'index\' annotation.\n    box.xmin = GetInt(\'xmin\', root, index)\n    box.ymin = GetInt(\'ymin\', root, index)\n    box.xmax = GetInt(\'xmax\', root, index)\n    box.ymax = GetInt(\'ymax\', root, index)\n\n    box.width = GetInt(\'width\', root)\n    box.height = GetInt(\'height\', root)\n    box.filename = GetItem(\'filename\', root) + \'.JPEG\'\n    box.label = GetItem(\'name\', root)\n\n    xmin = float(box.xmin) / float(box.width)\n    xmax = float(box.xmax) / float(box.width)\n    ymin = float(box.ymin) / float(box.height)\n    ymax = float(box.ymax) / float(box.height)\n\n    # Some images contain bounding box annotations that\n    # extend outside of the supplied image. See, e.g.\n    # n03127925/n03127925_147.xml\n    # Additionally, for some bounding boxes, the min > max\n    # or the box is entirely outside of the image.\n    min_x = min(xmin, xmax)\n    max_x = max(xmin, xmax)\n    box.xmin_scaled = min(max(min_x, 0.0), 1.0)\n    box.xmax_scaled = min(max(max_x, 0.0), 1.0)\n\n    min_y = min(ymin, ymax)\n    max_y = max(ymin, ymax)\n    box.ymin_scaled = min(max(min_y, 0.0), 1.0)\n    box.ymax_scaled = min(max(max_y, 0.0), 1.0)\n\n    boxes.append(box)\n\n  return boxes\n\n\nif __name__ == \'__main__\':\n  if len(sys.argv) < 2 or len(sys.argv) > 3:\n    print(\'Invalid usage\\n\'\n          \'usage: process_bounding_boxes.py <dir> [synsets-file]\',\n          file=sys.stderr)\n    sys.exit(-1)\n\n  xml_files = glob.glob(sys.argv[1] + \'/*/*.xml\')\n  print(\'Identified %d XML files in %s\' % (len(xml_files), sys.argv[1]),\n        file=sys.stderr)\n\n  if len(sys.argv) == 3:\n    labels = set([l.strip() for l in open(sys.argv[2]).readlines()])\n    print(\'Identified %d synset IDs in %s\' % (len(labels), sys.argv[2]),\n          file=sys.stderr)\n  else:\n    labels = None\n\n  skipped_boxes = 0\n  skipped_files = 0\n  saved_boxes = 0\n  saved_files = 0\n  for file_index, one_file in enumerate(xml_files):\n    # Example: <...>/n06470073/n00141669_6790.xml\n    label = os.path.basename(os.path.dirname(one_file))\n\n    # Determine if the annotation is from an ImageNet Challenge label.\n    if labels is not None and label not in labels:\n      skipped_files += 1\n      continue\n\n    bboxes = ProcessXMLAnnotation(one_file)\n    assert bboxes is not None, \'No bounding boxes found in \' + one_file\n\n    found_box = False\n    for bbox in bboxes:\n      if labels is not None:\n        if bbox.label != label:\n          # Note: There is a slight bug in the bounding box annotation data.\n          # Many of the dog labels have the human label \'Scottish_deerhound\'\n          # instead of the synset ID \'n02092002\' in the bbox.label field. As a\n          # simple hack to overcome this issue, we only exclude bbox labels\n          # *which are synset ID\'s* that do not match original synset label for\n          # the XML file.\n          if bbox.label in labels:\n            skipped_boxes += 1\n            continue\n\n      # Guard against improperly specified boxes.\n      if (bbox.xmin_scaled >= bbox.xmax_scaled or\n              bbox.ymin_scaled >= bbox.ymax_scaled):\n        skipped_boxes += 1\n        continue\n\n      # Note bbox.filename occasionally contains \'%s\' in the name. This is\n      # data set noise that is fixed by just using the basename of the XML file.\n      image_filename = os.path.splitext(os.path.basename(one_file))[0]\n      print(\'%s.JPEG,%.4f,%.4f,%.4f,%.4f\' %\n            (image_filename,\n             bbox.xmin_scaled, bbox.ymin_scaled,\n             bbox.xmax_scaled, bbox.ymax_scaled))\n\n      saved_boxes += 1\n      found_box = True\n    if found_box:\n      saved_files += 1\n    else:\n      skipped_files += 1\n\n    if not file_index % 5000:\n      print(\'--> processed %d of %d XML files.\' %\n            (file_index + 1, len(xml_files)),\n            file=sys.stderr)\n      print(\'--> skipped %d boxes and %d XML files.\' %\n            (skipped_boxes, skipped_files), file=sys.stderr)\n\n  print(\'Finished processing %d XML files.\' % len(xml_files), file=sys.stderr)\n  print(\'Skipped %d XML files not in ImageNet Challenge.\' % skipped_files,\n        file=sys.stderr)\n  print(\'Skipped %d bounding boxes not in ImageNet Challenge.\' % skipped_boxes,\n        file=sys.stderr)\n  print(\'Wrote %d bounding boxes from %d annotated images.\' %\n        (saved_boxes, saved_files),\n        file=sys.stderr)\n  print(\'Finished.\', file=sys.stderr)\n'"
functions/__init__.py,0,b''
functions/data_config.py,0,"b'# ==============================================================================\n# Copyright 2020-present NAVER Corp.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\n\nclass Default(object):\n  shuffle_buffer = 1000\n  ir_shuffle_buffer = 100\n  default_image_size = 224\n  num_channels = 3\n  num_train_files = 128\n  num_val_files = 16\n\n\nclass Food101(Default):\n  shuffle_buffer = 1000\n  shuffle_buffer_unsup = 10000\n  num_classes = 101\n  num_images = {\n    \'train\': 75750,\n    \'validation\': 25250,\n  }\n  dataset_name = \'food101\'\n\n\nclass ImageNet(Default):\n  shuffle_buffer = 10000\n  num_classes = 1001\n  num_images = {\n    \'train\': 1281167,\n    \'validation\': 50000,\n  }\n  num_train_files = 1024\n  dataset_name = \'imagenet\'\n\nclass CUB_200_2011(Default):\n  shuffle_buffer = 100\n  num_classes = 100\n  num_images = {\n    \'train\': 5864,\n    \'validation\': 5924,\n  }\n  num_train_files = 100\n  dataset_name = \'cub_200_2011\'\n\nclass iFood2019(Default):\n  num_classes = 251\n  num_images = {\n    \'train\': 118475,\n    \'validation\': 11994,\n  }\n  dataset_name = \'iFood2019\'\n\n\nclass StanfordOnlineProduct(Default):\n  shuffle_buffer = 50000\n  num_classes = 11318\n  num_images = {\n    \'train\': 59551,\n    \'validation\': 60502,\n  }\n  dataset_name = \'SOP\'\n\n\nclass Flower102(Default):\n  num_classes = 102\n  num_images = {\n    \'train\': 2040,\n    \'validation\': 6149,\n  }\n  dataset_name = \'oxford_flowers102\'\n\n\nclass Cars196(Default):\n  num_classes = 196\n  num_images = {\n    \'train\': 8144,\n    \'validation\': 8041,\n  }\n  dataset_name = \'cars196\'\n\n\nclass Cars196ZeroShot(Default):\n  num_classes = 196\n  num_images = {\n    \'train\': 8054,\n    \'validation\': 8131,\n  }\n  dataset_name = \'cars196_zeroshot\'\n\n\nclass OxfordPet(Default):\n  num_classes = 37\n  num_images = {\n    \'train\': 3680,\n    \'validation\': 3669,\n  }\n  dataset_name = \'oxford_iiit_pet\'\n\n\nclass Aircraft(Default):\n  num_classes = 100\n  num_images = {\n    \'train\': 6667,\n    \'validation\': 3333,\n  }\n  dataset_name = \'fgvc_aircraft\'\n\n\ndef get_config(data_name):\n  if data_name == \'imagenet\':\n    return ImageNet()\n  elif data_name == \'cars196\':\n    return Cars196()\n  elif data_name == \'food101\':\n    return Food101()\n  elif data_name == \'oxford_flowers102\':\n    return Flower102()\n  elif data_name == \'oxford_iiit_pet\':\n    return OxfordPet()\n  elif data_name == \'fgvc_aircraft\':\n    return Aircraft()\n  elif data_name == \'cub_200_2011\':\n    return CUB_200_2011()\n  elif data_name == \'cars196_zeroshot\':\n    return Cars196ZeroShot()\n  elif data_name == \'SOP\':\n    return StanfordOnlineProduct()\n  elif data_name == \'iFood2019\':\n    return iFood2019()\n  else:\n    raise ValueError(""Unable to support {} dataset."".format(data_name))\n'"
functions/input_fns.py,14,"b'# coding=utf8\n# ==============================================================================\n# Copyright 2020-present NAVER Corp.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\nfrom official.utils.misc import distribution_utils\nfrom official.utils.flags import core as flags_core\nfrom functions import data_config\nfrom utils import data_util\n\n\ndef get_tf_version():\n  tf_version = tf.VERSION\n  tf_major_version, tf_minor_version, _ = tf_version.split(\'.\')\n  return int(tf_major_version), int(tf_minor_version)\n\ndef input_fn(is_training,\n             filenames,\n             use_random_crop,\n             batch_size,\n             num_train_files,\n             num_images,\n             shuffle_buffer,\n             num_channels,\n             num_epochs=1,\n             num_gpus=None,\n             dtype=tf.float32,\n             autoaugment_type=None,\n             with_drawing_bbox=False,\n             preprocessing_type=\'imagenet\',\n             drop_remainder=False,\n             dct_method="""",\n             return_logits=False,\n             return_filename=False,\n             parse_record_fn=data_util.parse_record):\n  dataset = tf.data.Dataset.from_tensor_slices(filenames)\n\n  if is_training:\n    # Shuffle the input files\n    dataset = dataset.shuffle(buffer_size=num_train_files)\n\n  # Convert to individual records.\n  # cycle_length = 10 means 10 files will be read and deserialized in parallel.\n  # This number is low enough to not cause too much contention on small systems\n  # but high enough to provide the benefits of parallelization. You may want\n  # to increase this number if you have a large number of CPU cores.\n  # dataset = dataset.apply(tf.data.experimental.parallel_interleave(\n  #   tf.data.TFRecordDataset, cycle_length=20))\n\n  tf_major_version, tf_minor_version = get_tf_version()\n  if tf_major_version == 1 and tf_minor_version <= 12:\n    dataset = dataset.apply(tf.contrib.data.parallel_interleave(\n      tf.data.TFRecordDataset, cycle_length=20))\n  else:\n    dataset = dataset.interleave(\n      tf.data.TFRecordDataset,\n      cycle_length=20,\n      num_parallel_calls=tf.data.experimental.AUTOTUNE)\n\n  return data_util.process_record_dataset(\n    dataset=dataset,\n    is_training=is_training,\n    batch_size=batch_size,\n    shuffle_buffer=shuffle_buffer,\n    parse_record_fn=parse_record_fn,\n    num_epochs=num_epochs,\n    num_gpus=num_gpus,\n    num_channels=num_channels,\n    examples_per_epoch=num_images if is_training else None,\n    dtype=dtype,\n    use_random_crop=use_random_crop,\n    dct_method=dct_method,\n    autoaugment_type=autoaugment_type,\n    preprocessing_type=preprocessing_type,\n    drop_remainder=drop_remainder,\n    with_drawing_bbox=with_drawing_bbox,\n    return_logits=return_logits,\n    return_filename=return_filename)\n\n\ndef input_fn_cls(is_training, use_random_crop, num_epochs, flags_obj):\n  if flags_obj.mixup_type == 1 and is_training:\n    batch_size = flags_obj.batch_size * 2\n    num_epochs = num_epochs * 2\n  else:\n    batch_size = flags_obj.batch_size\n\n  batch_size = distribution_utils.per_device_batch_size(batch_size, flags_core.get_num_gpus(flags_obj))\n  filenames_sup = data_util.get_filenames(is_training, flags_obj.data_dir,\n                                          train_regex=flags_obj.train_regex,\n                                          val_regex=flags_obj.val_regex)\n  tf.logging.info(\'The # of Supervised tfrecords: {}\'.format(len(filenames_sup)))\n  dataset_meta = data_config.get_config(flags_obj.dataset_name)\n  datasets = []\n  dataset_sup = input_fn(is_training,\n                         filenames_sup,\n                         use_random_crop,\n                         batch_size,\n                         dataset_meta.num_train_files,\n                         dataset_meta.num_images[\'train\'],\n                         dataset_meta.shuffle_buffer,\n                         dataset_meta.num_channels,\n                         num_epochs,\n                         flags_core.get_num_gpus(flags_obj),\n                         flags_core.get_tf_dtype(flags_obj),\n                         autoaugment_type=flags_obj.autoaugment_type,\n                         with_drawing_bbox=flags_obj.with_drawing_bbox,\n                         drop_remainder=False,\n                         preprocessing_type=flags_obj.preprocessing_type,\n                         return_logits=flags_obj.kd_temp > 0,\n                         dct_method=flags_obj.dct_method,\n                         parse_record_fn=data_util.parse_record_sup)\n  datasets.append(dataset_sup)\n\n  def flatten_input(*features):\n    images_dict = {}\n    for feature in features:\n      for key in feature:\n        if key == \'label\':\n          label = feature[key]\n        else:\n          images_dict[key] = feature[key]\n    return images_dict, label\n\n  dataset = tf.data.Dataset.zip(tuple(datasets))\n  dataset = dataset.map(flatten_input)\n  tf.logging.info(\'dataset = dataset.map(flatten_input)\')\n  tf.logging.info(dataset)\n  return dataset\n\n\ndef input_fn_ir_eval(is_training,\n                     data_dir,\n                     batch_size,\n                     num_epochs=1,\n                     num_gpus=0,\n                     dtype=tf.float32,\n                     preprocessing_type=\'imagenet\',\n                     dataset_name=None,\n                     dct_method="""",\n                     val_regex=\'validation-*\'):\n  filenames = data_util.get_filenames(is_training, data_dir, val_regex=val_regex)\n  assert len(filenames) > 0\n  dataset_config = data_config.get_config(dataset_name)\n\n  return input_fn(is_training, filenames, False, batch_size,\n                  dataset_config.num_train_files, dataset_config.num_images[\'validation\'],\n                  dataset_config.shuffle_buffer, dataset_config.num_channels, num_epochs, num_gpus, dtype,\n                  preprocessing_type=preprocessing_type, dct_method=dct_method)\n'"
functions/model_fns.py,9,"b'# ==============================================================================\n# Copyright 2020-present NAVER Corp.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\n\nfrom functions import data_config\nfrom nets import resnet_model\nfrom nets import run_loop_classification\n\ndef keep_prob_decay(starter_kp, end_kp, decay_steps):\n  def keep_prob_decay_fn(global_step):\n    kp = tf.train.polynomial_decay(starter_kp, global_step, decay_steps,\n                                   end_kp, power=1.0,\n                                   cycle=False)\n    return kp\n\n  return keep_prob_decay_fn\n\n\ndef learning_rate_with_decay(learning_rate_decay_type,\n                             batch_size, batch_denom, num_images, num_epochs_per_decay,\n                             learning_rate_decay_factor, end_learning_rate, piecewise_lr_boundary_epochs,\n                             piecewise_lr_decay_rates, base_lr, warmup_epochs=0, train_epochs=None):\n  """"""Get a learning rate that decays step-wise as training progresses.\n  Args:\n    batch_size: the number of examples processed in each training batch.\n    batch_denom: this value will be used to scale the base learning rate.\n      `0.1 * batch size` is divided by this number, such that when\n      batch_denom == batch_size, the initial learning rate will be 0.1.\n    num_images: total number of images that will be used for training.\n    num_epochs_per_decay: number of epochs after which learning rate decays.\n    learning_rate_decay_factor: number of epochs after which learning rate decays.\n    end_learning_rate: the minimal end learning rate used by a polynomial decay learning rate.\n    piecewise_lr_boundary_epochs: A list of ints with strictly increasing entries to reduce the learning rate at certain epochs.\n    piecewise_lr_decay_rates: A list of floats that specifies the decay rates for the intervals defined by piecewise_lr_boundary_epochs. It should have one more element than piecewise_lr_boundary_epochs.\n    base_lr: initial learning rate scaled based on batch_denom.\n    warmup_epochs: Run the number of epoch warmup to the initial lr.\n    train_epochs: The number of train epochs\n  Returns:\n    Returns a function that takes a single argument - the number of batches\n    trained so far (global_step)- and returns the learning rate to be used\n    for training the next batch.\n  """"""\n  tf.logging.debug(""learning_rate_decay_type=({})"".format(learning_rate_decay_type))\n  initial_learning_rate = base_lr * batch_size / batch_denom\n  batches_per_epoch = num_images / batch_size\n  decay_steps = int(batches_per_epoch * num_epochs_per_decay)\n\n  def learning_rate_fn(global_step):\n    warmup_steps = int(batches_per_epoch * warmup_epochs)\n\n    if learning_rate_decay_type == \'exponential\':\n      lr = tf.train.exponential_decay(initial_learning_rate, global_step - warmup_steps, decay_steps,\n                                      learning_rate_decay_factor, staircase=True,\n                                      name=\'exponential_decay_learning_rate\')\n    elif learning_rate_decay_type == \'fixed\':\n      lr = tf.constant(base_lr, name=\'fixed_learning_rate\')\n    elif learning_rate_decay_type == \'polynomial\':\n      lr = tf.train.polynomial_decay(initial_learning_rate, global_step - warmup_steps, decay_steps,\n                                     end_learning_rate, power=1.0,\n                                     cycle=False, name=\'polynomial_decay_learning_rate\')\n    elif learning_rate_decay_type == \'piecewise\':\n      boundaries = [int(batches_per_epoch * epoch) for epoch in piecewise_lr_boundary_epochs]\n      vals = [initial_learning_rate * float(decay) for decay in piecewise_lr_decay_rates]\n      lr = tf.train.piecewise_constant(global_step, boundaries, vals)\n    elif learning_rate_decay_type == \'cosine\':\n      total_batches = int(batches_per_epoch * train_epochs) - warmup_steps\n      global_step_except_warmup_step = global_step - warmup_steps\n      lr = tf.train.cosine_decay(initial_learning_rate, global_step_except_warmup_step, total_batches)\n    else:\n      raise NotImplementedError\n\n    if warmup_steps > 0:\n      warmup_lr = (initial_learning_rate * tf.cast(global_step, tf.float32) / tf.cast(warmup_steps, tf.float32))\n      return tf.cond(global_step < warmup_steps, lambda: warmup_lr, lambda: lr)\n\n    return lr\n\n  return learning_rate_fn\n\n\ndef get_block_sizes(resnet_size, resnet_version=1):\n  """"""Retrieve the size of each block_layer in the ResNet model.\n\n  The number of block layers used for the Resnet model varies according\n  to the size of the model. This helper grabs the layer set we want, throwing\n  an error if a non-standard size has been selected.\n\n  Args:\n    resnet_size: The number of convolutional layers needed in the model.\n\n  Returns:\n    A list of block sizes to use in building the model.\n\n  Raises:\n    KeyError: if invalid resnet_size is received.\n  """"""\n  # In the case of bl-resnet, the number of blocks in each stage is different from the original.\n  if resnet_version == 2:\n    choices = {\n      50: [3, 4, 6, 3],\n      101: [4, 8, 18, 3],\n      152: [5, 12, 30, 3],\n    }\n  else:\n    choices = {\n      50: [3, 4, 6, 3],\n      101: [3, 4, 23, 3],\n      152: [3, 8, 36, 3],\n      200: [3, 24, 36, 3]\n    }\n\n  try:\n    return choices[resnet_size]\n  except KeyError:\n    err = (\'Could not find layers for selected Resnet size.\\n\'\n           \'Size received: {}; sizes allowed: {}.\'.format(\n      resnet_size, choices.keys()))\n    raise ValueError(err)\n\n\nclass Model(resnet_model.Model):\n  """"""Model class with appropriate defaults for Imagenet data.""""""\n\n  def __init__(self, resnet_size,\n               data_format=None,\n               num_classes=None,\n               resnet_version=resnet_model.DEFAULT_VERSION,\n               dtype=resnet_model.DEFAULT_DTYPE,\n               no_downsample=False,\n               zero_gamma=False,\n               use_se_block=False,\n               use_sk_block=False,\n               bn_momentum=0.997,\n               embedding_size=0,\n               anti_alias_filter_size=0,\n               anti_alias_type="""",\n               pool_type=\'gap\',\n               loss_type=\'softmax\',\n               bl_alpha=2,\n               bl_beta=4):\n\n    # For bigger models, we want to use ""bottleneck"" layers\n    if resnet_size < 50:\n      bottleneck = False\n    else:\n      bottleneck = True\n\n    if resnet_version == 2:\n      block_strides = [2, 2, 1, 2]\n    else:\n      block_strides = [1, 2, 2, 2]\n\n    if no_downsample:\n      block_strides[-1] = 1\n\n    super(Model, self).__init__(\n      resnet_size=resnet_size,\n      bottleneck=bottleneck,\n      num_classes=num_classes,\n      num_filters=64,\n      kernel_size=7,\n      conv_stride=2,\n      first_pool_size=3,\n      first_pool_stride=2,\n      block_sizes=get_block_sizes(resnet_size, resnet_version),\n      block_strides=block_strides,\n      resnet_version=resnet_version,\n      data_format=data_format,\n      dtype=dtype,\n      zero_gamma=zero_gamma,\n      use_se_block=use_se_block,\n      use_sk_block=use_sk_block,\n      bn_momentum=bn_momentum,\n      embedding_size=embedding_size,\n      anti_alias_filter_size=anti_alias_filter_size,\n      anti_alias_type=anti_alias_type,\n      pool_type=pool_type,\n      bl_alpha=bl_alpha,\n      bl_beta=bl_beta,\n      loss_type=loss_type,\n    )\n\n\ndef model_fn_cls(features, labels, mode, params):\n  if int(params[\'resnet_size\']) < 50:\n    assert not params[\'use_dropblock\']\n    assert not params[\'use_se_block\']\n    assert not params[\'use_sk_block\']\n    assert not params[\'use_resnet_d\']\n\n  dataset = data_config.get_config(params[\'dataset_name\'])\n  learning_rate_fn = learning_rate_with_decay(\n    learning_rate_decay_type=params[\'learning_rate_decay_type\'],\n    batch_size=params[\'batch_size\'], batch_denom=params[\'batch_size\'],\n    num_images=dataset.num_images[\'train\'], num_epochs_per_decay=params[\'num_epochs_per_decay\'],\n    learning_rate_decay_factor=params[\'learning_rate_decay_factor\'],\n    end_learning_rate=params[\'end_learning_rate\'],\n    piecewise_lr_boundary_epochs=params[\'piecewise_lr_boundary_epochs\'],\n    piecewise_lr_decay_rates=params[\'piecewise_lr_decay_rates\'],\n    base_lr=params[\'base_learning_rate\'],\n    train_epochs=params[\'train_epochs\'],\n    warmup_epochs=params[\'lr_warmup_epochs\'])\n\n  if params[\'use_dropblock\']:\n    starter_kp = params[\'dropblock_kp\'][0]\n    end_kp = params[\'dropblock_kp\'][1]\n    batches_per_epoch = dataset.num_images[\'train\'] / params[\'batch_size\']\n    decay_steps = int(params[\'train_epochs\'] * batches_per_epoch)\n    keep_prob_fn = keep_prob_decay(starter_kp, end_kp, decay_steps)\n  else:\n    keep_prob_fn = None\n\n  return run_loop_classification.resnet_model_fn(\n    features=features,\n    labels=labels,\n    num_classes=dataset.num_classes,\n    mode=mode,\n    model_class=Model,\n    learning_rate_fn=learning_rate_fn,\n    keep_prob_fn=keep_prob_fn,\n    loss_filter_fn=None,\n    p=params)\n'"
kd/eval_amoebanet.py,8,"b'# ==============================================================================\n# Copyright 2020-present NAVER Corp.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom absl import flags\nimport tensorflow_hub as hub\nimport numpy as np\nfrom tqdm import tqdm\n\nimport sys\n\nsys.path.append(\'./\')\nsys.path.append(\'../\')\n\nfrom functions.input_fns import *\nfrom functions import data_config\nfrom utils import data_util\n\n\nimport os\n\ntry:\n  import urllib2 as urllib\nexcept ImportError:\n  import urllib.request as urllib\n\n""""""\nExamples:\npython kd/eval_amoebanet.py  \n""""""\n\nflags.DEFINE_integer(\'batch_size\', 32, \'The number of samples in each batch.\')\nflags.DEFINE_string(\'data_dir\', \'imagenet\', \'path to the root directory of images\')\nflags.DEFINE_string(\'gpu_to_use\', \'5\', \'\')\nflags.DEFINE_string(\'data_name\', \'imagenet\', \'dataset name to use.\')\nflags.DEFINE_string(\'preprocessing_type\', \'inception_331\', \'dataset name to use.\')\nflags.DEFINE_string(\'model_dir\', \'tmp/tfmodel/\', \'Create a flag for specifying the model file directory.\')\n\nFLAGS = flags.FLAGS\n\n\ndef input_fn_amoabanet(is_training,\n                       use_random_crop,\n                       data_dir,\n                       batch_size,\n                       num_epochs=1,\n                       num_gpus=None,\n                       dtype=tf.float32,\n                       with_drawing_bbox=False,\n                       autoaugment_type=None,\n                       dataset_name=None,\n                       drop_remainder=False,\n                       preprocessing_type=\'imagenet\',\n                       return_logits=False,\n                       dct_method="""",\n                       train_regex=\'train*\',\n                       val_regex=\'validation*\'):\n  filenames = data_util.get_filenames(is_training, data_dir,\n                                      train_regex=train_regex,\n                                      val_regex=val_regex)\n  dataset = data_config.get_config(dataset_name)\n\n  return input_fn(is_training, filenames, use_random_crop, batch_size,\n                  dataset.num_train_files, dataset.num_images[\'train\'],\n                  dataset.shuffle_buffer, dataset.num_channels, num_epochs,\n                  num_gpus, dtype,\n                  autoaugment_type=autoaugment_type,\n                  with_drawing_bbox=with_drawing_bbox,\n                  drop_remainder=drop_remainder,\n                  preprocessing_type=preprocessing_type,\n                  return_logits=return_logits,\n                  dct_method=dct_method)\n\ndef main(unused_argv):\n  os.environ[\'CUDA_VISIBLE_DEVICES\'] = FLAGS.gpu_to_use\n  dconf = data_config.get_config(FLAGS.data_name)\n  dataset = input_fn_amoabanet(False, False, FLAGS.data_dir, FLAGS.batch_size,\n                               dataset_name=FLAGS.data_name,\n                               preprocessing_type=FLAGS.preprocessing_type)\n  iterator = dataset.make_one_shot_iterator()\n  images, labels = iterator.get_next()\n\n  module = hub.Module(""https://tfhub.dev/google/imagenet/amoebanet_a_n18_f448/classification/1"")\n\n  logits = module(images)  # Logits with shape [batch_size, num_classes].\n  pred = tf.nn.softmax(logits)\n  top1 = tf.argmax(logits, axis=1)\n\n  np_preds = np.zeros(dconf.num_images[\'validation\'], dtype=np.int64)\n  np_labels = np.zeros(dconf.num_images[\'validation\'], dtype=np.int64)\n\n  np_i = 0\n  with tf.Session() as sess:\n    sess.run(tf.global_variables_initializer())\n    sess.run(tf.local_variables_initializer())\n    n_loop = dconf.num_images[\'validation\'] // FLAGS.batch_size\n    for _ in tqdm(range(n_loop + 1)):\n      try:\n        _pred, _top1, _labels = sess.run([pred, top1, labels])\n        np_preds[np_i:np_i + _pred.shape[0]] = _top1\n        np_labels[np_i:np_i + _labels.shape[0]] = _labels\n        np_i += _pred.shape[0]\n      except tf.errors.OutOfRangeError:\n        break\n\n  print(\'Accuracy:\')\n  print(np.count_nonzero(np_preds == np_labels) / dconf.num_images[\'validation\'])\n\n\nif __name__ == \'__main__\':\n  tf.app.run()\n'"
kd/extract_embeddings.py,9,"b'# ==============================================================================\n# Copyright 2020-present NAVER Corp.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom absl import flags\nimport tensorflow_hub as hub\nimport numpy as np\n\nimport sys\n\nsys.path.append(\'./\')\nsys.path.append(\'../\')\n\nfrom functions.input_fns import *\nimport os\nfrom tqdm import tqdm\nimport pandas as pd\n\ntry:\n  import urllib2 as urllib\nexcept ImportError:\n  import urllib.request as urllib\n\nflags.DEFINE_integer(\'batch_size\', 64, \'The number of samples in each batch.\')\nflags.DEFINE_integer(\'total_num_shard\', 128, \'\')\nflags.DEFINE_integer(\'n_tfrecord\', 1024, \'\')\nflags.DEFINE_integer(\'no_shard\', 0, \'\')\nflags.DEFINE_boolean(\'is_training\', True, \'\')\nflags.DEFINE_string(\'data_dir\', \'imagenet\', \'path to the root directory of images\')\nflags.DEFINE_string(\'output_dir\', \'amoebanet_logits\', \'path to the root directory of images\')\nflags.DEFINE_string(\'gpu_to_use\', \'0\', \'\')\nflags.DEFINE_string(\'data_name\', \'imagenet\', \'dataset name to use.\')\nflags.DEFINE_string(\'net_name\', \'amoebanet\', \'\')\nflags.DEFINE_integer(\'offset\', 0, \'\')\n\nFLAGS = flags.FLAGS\n\n\ndef input_fn_cls(is_training,\n                 use_random_crop,\n                 data_dir,\n                 batch_size,\n                 num_epochs=1,\n                 num_gpus=None,\n                 dtype=tf.float32,\n                 with_drawing_bbox=False,\n                 autoaugment_type=None,\n                 dataset_name=None,\n                 drop_remainder=False,\n                 preprocessing_type=\'imagenet\',\n                 dct_method=""""):\n  tfr_range_low = FLAGS.no_shard * (FLAGS.n_tfrecord / FLAGS.total_num_shard)\n  tfr_range_high = (FLAGS.no_shard + 1) * (FLAGS.n_tfrecord / FLAGS.total_num_shard)\n  print(\'tfr_range_high\', tfr_range_high)\n  print(\'tfr_range_low\', tfr_range_low)\n  tf.logging.info(\'data_dir is {}\'.format(data_dir))\n  filenames = data_util.get_filenames(FLAGS.is_training, data_dir)\n\n  filenames = [fn for fn in filenames if\n               tfr_range_low <= int(fn.split(\'/\')[-1].split(""-"")[1]) < tfr_range_high]\n  print(\'len(filenames)\', len(filenames))\n  print(\'filenames\', filenames)\n  dataset = data_config.get_config(dataset_name)\n  return input_fn(is_training, filenames, use_random_crop, batch_size,\n                  dataset.num_train_files, dataset.num_images[\'train\'], dataset.shuffle_buffer,\n                  dataset.num_channels, num_epochs, num_gpus, dtype,\n                  autoaugment_type=autoaugment_type,\n                  with_drawing_bbox=with_drawing_bbox,\n                  drop_remainder=drop_remainder,\n                  preprocessing_type=preprocessing_type,\n                  dct_method=dct_method,\n                  return_filename=True)\n\n\ndef main(unused_argv):\n  if FLAGS.net_name == \'amoebanet\':\n    hub_address = ""https://tfhub.dev/google/imagenet/amoebanet_a_n18_f448/classification/1""\n    preprocessing_type = \'inception_331\'\n  elif FLAGS.net_name == \'efficientnet\':\n    hub_address = ""https://tfhub.dev/google/efficientnet/b7/classification/1""\n    preprocessing_type = \'inception_600\'\n  else:\n    raise NotImplementedError\n  os.environ[\'CUDA_VISIBLE_DEVICES\'] = FLAGS.gpu_to_use\n  dataset = input_fn_cls(False, False, FLAGS.data_dir, FLAGS.batch_size,\n                         dataset_name=FLAGS.data_name,\n                         preprocessing_type=preprocessing_type)\n  iterator = dataset.make_one_shot_iterator()\n  images, labels, filename = iterator.get_next()\n  print(\'images, labels, filename\', images, labels, filename)\n\n  module = hub.Module(hub_address)\n\n  logits = module(images)  # Logits with shape [batch_size, num_classes].\n\n  # Get the number of images\n  num_images = 0\n  with tf.Session() as sess:\n    while True:\n      try:\n        _filename = sess.run(filename)\n        num_images += _filename.shape[0]\n      except tf.errors.OutOfRangeError:\n        break\n\n  file2emb_dict = {}\n  with tf.Session() as sess:\n    sess.run(tf.global_variables_initializer())\n    sess.run(tf.local_variables_initializer())\n    n_loop = num_images // (FLAGS.batch_size)\n    for _ in tqdm(range(n_loop + 1)):\n\n      try:\n        _, _logits, _filename = sess.run([labels, logits, filename])\n\n        for l, fn in zip(_logits, _filename):\n          result = np.zeros(l.shape[0] + FLAGS.offset)\n          result[FLAGS.offset:] = l\n          file2emb_dict[fn.decode(""utf-8"")] = result\n\n      except tf.errors.OutOfRangeError:\n        break\n\n  df = pd.DataFrame(data=file2emb_dict).T\n  if not os.path.exists(FLAGS.output_dir):\n    os.mkdir(FLAGS.output_dir)\n  if FLAGS.is_training:\n    output_file = os.path.join(FLAGS.output_dir, \'train-{}.csv\'.format(str(FLAGS.no_shard)))\n  else:\n    output_file = os.path.join(FLAGS.output_dir, \'validation-{}.csv\'.format(str(FLAGS.no_shard)))\n\n  df.to_csv(output_file, sep=\',\', header=False)\n\n\nif __name__ == \'__main__\':\n  tf.app.run()\n'"
losses/__init__.py,0,b''
losses/cls_losses.py,5,"b'# coding=utf8\n# ==============================================================================\n# Copyright 2020-present NAVER Corp.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\n\ndef get_sup_loss(logits, onehot_labels, global_step, num_classes, p):\n  cross_entropy = None\n  weights = 1.0\n\n  tf.logging.info(\'[Classfication loss type; {}]\'.format(p[\'cls_loss_type\']))\n  if p[\'cls_loss_type\'] == \'softmax\':\n    if not isinstance(weights, float):\n      weights = tf.reduce_mean(weights, axis=1)\n    cross_entropy = tf.losses.softmax_cross_entropy(\n      logits=logits, onehot_labels=onehot_labels,  # 128\n      label_smoothing=p[\'label_smoothing\'], weights=weights)\n  elif p[\'cls_loss_type\'] == \'sigmoid\':\n    cross_entropy = weights * tf.nn.sigmoid_cross_entropy_with_logits(\n      labels=onehot_labels, logits=logits)\n    # Normalize by the total number of positive samples.\n    cross_entropy = tf.reduce_sum(cross_entropy) / tf.reduce_sum(onehot_labels)\n\n  assert cross_entropy is not None\n  return cross_entropy\n'"
mce/__init__.py,0,b''
mce/eval_robustness.py,25,"b'# coding=utf8\n# ==============================================================================\n# Copyright 2020-present NAVER Corp.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport os\nimport sys\n\nsys.path.append(\'./\')\nsys.path.append(\'../\')\n\nfrom absl import flags\nimport tensorflow as tf\nimport numpy as np\nimport glob\n\nfrom functions import model_fns\nfrom preprocessing import imagenet_preprocessing\nimport threading\n\nflags.DEFINE_string(\n  \'robustness_type\', \'ce\',\n  \'[ce|fr]\')\n\nflags.DEFINE_string(\n  \'gpu_index\', \'0\', \'The index of gpus.\')\n\nflags.DEFINE_integer(\n  \'label_offset\', 1, \'\')\n\nflags.DEFINE_integer(\n  \'num_classes\', \'1001\', \'The number of classes.\')\n\nflags.DEFINE_integer(\n  \'batch_size\', 128, \'The number of samples in each batch.\')\n\nflags.DEFINE_integer(\n  \'image_size\', 224, \'The number of samples in each batch.\')\n\nflags.DEFINE_integer(\n  \'resnet_size\', 50, \'The number of convolutional layers needed in the model.\')\n\nflags.DEFINE_integer(\n  \'resnet_version\', 1, \'Resnet version\')\n\nflags.DEFINE_string(\n  \'data_format\', \'channels_first\',\n  \'[channels_fisrt|channels_last]\')\n\nflags.DEFINE_boolean(name=\'use_resnet_d\', default=False, help=\'\')\nflags.DEFINE_boolean(name=\'use_se_block\', default=False, help=\'\')\nflags.DEFINE_boolean(name=\'use_sk_block\', default=False, help=\'\')\nflags.DEFINE_boolean(name=\'zero_gamma\', default=True, help=\'\')\nflags.DEFINE_boolean(name=\'no_downsample\', default=False, help=\'\')\nflags.DEFINE_integer(name=\'anti_alias_filter_size\', default=0, help=\'\')\nflags.DEFINE_string(name=\'anti_alias_type\', default="""", help=\'\')\nflags.DEFINE_integer(name=\'bl_alpha\', default=2, help=\'\')\nflags.DEFINE_integer(name=\'bl_beta\', default=4, help=\'\')\nflags.DEFINE_integer(name=\'embedding_size\', default=0, help=\'\')\nflags.DEFINE_string(name=\'pool_type\', default=\'gap\', help=\'\')\n\nflags.DEFINE_string(\n  \'data_dir\', \'./imagenet-C\',\n  \'path to the root directory of images\')\n\nflags.DEFINE_string(\n  \'label_file\', \'./label.txt\',\n  \'path to the label file of imagenet\')\n\nflags.DEFINE_string(\n  \'model_dir\', \'tmp/tfmodel/\',\n  \'Create a flag for specifying the model file directory.\')\n\nFLAGS = flags.FLAGS\n\nALEXNET_CE = {\n  \'gaussian_noise\': 0.8864, \'shot_noise\': 0.8945, \'impulse_noise\': 0.9226,\n  \'defocus_blur\': 0.8199, \'glass_blur\': 0.8263, \'motion_blur\': 0.7860, \'zoom_blur\': 0.7984,\n  \'snow\': 0.8668, \'frost\': 0.8266, \'fog\': 0.8193, \'brightness\': 0.5646,\n  \'contrast\': 0.8532, \'elastic_transform\': 0.6461, \'pixelate\': 0.7178, \'jpeg_compression\': 0.6065,\n  \'speckle_noise\': 0.8454, \'gaussian_blur\': 0.7871, \'spatter\': 0.7175, \'saturate\': 0.6583\n}\n\nOURS_CE = {\n  \'gaussian_noise\': 0.0, \'shot_noise\': 0.0, \'impulse_noise\': 0.0,\n  \'defocus_blur\': 0.0, \'glass_blur\': 0.0, \'motion_blur\': 0.0, \'zoom_blur\': 0.0,\n  \'snow\': 0.0, \'frost\': 0.0, \'fog\': 0.0, \'brightness\': 0.0,\n  \'contrast\': 0.0, \'elastic_transform\': 0.0, \'pixelate\': 0.0, \'jpeg_compression\': 0.0,\n  \'speckle_noise\': 0.0, \'gaussian_blur\': 0.0, \'spatter\': 0.0, \'saturate\': 0.0\n}\n\nABSOLUTE_CE = {\n  \'gaussian_noise\': 0.0, \'shot_noise\': 0.0, \'impulse_noise\': 0.0,\n  \'defocus_blur\': 0.0, \'glass_blur\': 0.0, \'motion_blur\': 0.0, \'zoom_blur\': 0.0,\n  \'snow\': 0.0, \'frost\': 0.0, \'fog\': 0.0, \'brightness\': 0.0,\n  \'contrast\': 0.0, \'elastic_transform\': 0.0, \'pixelate\': 0.0, \'jpeg_compression\': 0.0,\n  \'speckle_noise\': 0.0, \'gaussian_blur\': 0.0, \'spatter\': 0.0, \'saturate\': 0.0\n}\n\nALEXNET_FP = {\n  \'gaussian_noise\': 0.23653, \'shot_noise\': 0.30062, \'motion_blur\': 0.09297,\n  \'zoom_blur\': 0.05942, \'spatter\': 0.05053, \'brightness\': 0.04889,\n  \'translate\': 0.11007, \'rotate\': 0.13104, \'tilt\': 0.07050, \'scale\': 0.23531,\n  \'speckle_noise\': 0.18649, \'gaussian_blur\': 0.02775, \'snow\': 0.11928, \'shear\': 0.10658\n}\n\n\ndef input_fn_imagenet_c(image_files, batch_size):\n  def _parse_function(image_file):\n    image_buffer = tf.read_file(image_file)\n    image = tf.image.decode_jpeg(image_buffer, channels=3, dct_method=""INTEGER_ACCURATE"")\n\n    def bypass(imagel):\n      imagel = tf.cast(imagel, tf.float32)\n      imagel.set_shape([FLAGS.image_size, FLAGS.image_size, 3])\n      imagel = imagenet_preprocessing.mean_image_subtraction(imagel, imagenet_preprocessing.CHANNEL_MEANS, 3)\n      return imagel\n\n    image = bypass(image)\n    image = tf.cast(image, tf.float32)\n\n    return image, image_file\n\n  dataset = tf.data.Dataset.from_tensor_slices(image_files)\n  dataset = dataset.prefetch(buffer_size=batch_size)\n  dataset = dataset.apply(\n    tf.contrib.data.map_and_batch(\n      _parse_function,\n      batch_size=batch_size,\n      num_parallel_batches=1,\n      drop_remainder=False))\n\n  return dataset\n\n\ndef get_synsets2idx(labels_file):\n  challenge_synsets = [l.strip().replace(\' \', \'_\').lower() for l in\n                       tf.gfile.GFile(labels_file, \'r\').readlines()]\n  synsets2idx = {}\n  for i, synset in enumerate(challenge_synsets):\n    synsets2idx[synset] = i\n\n  return synsets2idx\n\n\ndef show_corruption_error_by_distortion(distortion_name, current_ckpt, gpu_id):\n  synsets2idx = get_synsets2idx(FLAGS.label_file)\n\n  distortion_dir = os.path.join(FLAGS.data_dir, distortion_name)\n\n  with tf.device(\'/gpu:%d\' % gpu_id):\n    model = model_fns.Model(resnet_size=FLAGS.resnet_size,\n                            num_classes=FLAGS.num_classes,\n                            resnet_version=FLAGS.resnet_version,\n                            use_se_block=FLAGS.use_se_block,\n                            use_sk_block=FLAGS.use_sk_block,\n                            zero_gamma=FLAGS.zero_gamma,\n                            data_format=FLAGS.data_format,\n                            no_downsample=FLAGS.no_downsample,\n                            anti_alias_filter_size=FLAGS.anti_alias_filter_size,\n                            anti_alias_type=FLAGS.anti_alias_type,\n                            embedding_size=FLAGS.embedding_size,\n                            pool_type=FLAGS.pool_type,\n                            bl_alpha=FLAGS.bl_alpha,\n                            bl_beta=FLAGS.bl_beta,\n                            dtype=tf.float32)\n    images = tf.placeholder(tf.float32, [None, FLAGS.image_size, FLAGS.image_size, 3])\n    logits = model(inputs=images, training=False,\n                   use_resnet_d=FLAGS.use_resnet_d,\n                   reuse=tf.AUTO_REUSE)\n    softmax = tf.nn.softmax(logits)\n    sm_top1 = tf.nn.top_k(softmax, 1)\n\n  saver = tf.train.Saver()\n\n  image_files = []\n  for severity in range(1, 6):\n    severity_dir = os.path.join(distortion_dir, str(severity))\n    for d in os.listdir(severity_dir):\n      for f in glob.glob(os.path.join(severity_dir, d) + \'/*\'):\n        image_files.append(f)\n\n  dataset = input_fn_imagenet_c(image_files, FLAGS.batch_size)\n\n  iterator = dataset.make_one_shot_iterator()\n  next_element = iterator.get_next()\n\n  with tf.Session() as sess:\n    saver.restore(sess, current_ckpt)\n\n    num_of_images = 0\n    correct = 0\n\n    while True:\n      try:\n        images_tensor, image_files = next_element\n        images_input, image_files = sess.run([images_tensor, image_files])\n\n        result = sess.run(sm_top1, feed_dict={images: images_input})\n\n        for i in range(len(result.indices)):\n          fn = os.path.splitext(image_files[i].decode(""utf-8""))[0]\n          synset = os.path.basename(os.path.dirname(fn))\n          gt = synsets2idx[synset] + FLAGS.label_offset\n\n          # top1\n          pred = result.indices[i][0]\n\n          num_of_images += 1\n          if pred == gt:\n            correct += 1\n\n      except tf.errors.OutOfRangeError:\n        break\n\n  assert (num_of_images > 0)\n  err = (1 - 1. * correct / num_of_images)\n\n  return err\n\n\ndef thread_execute(num_threads, fn, current_ckpt):\n  assert num_threads > 0\n  # Create a mechanism for monitoring when all threads are finished.\n  coord = tf.train.Coordinator()\n  threads = []\n  for idx in range(num_threads):\n    t = threading.Thread(target=fn, args=(idx, current_ckpt))\n    t.start()\n    threads.append(t)\n  coord.join(threads)  # Wait for all the threads to terminate.\n\n\ndef process(no_thread, current_ckpt):\n  print(\'!!!!!!!!!!!!!!!!!! start\', no_thread)\n  distortions = list(ALEXNET_CE.keys())\n  num_distortions = len(distortions)\n\n  num_gpus = len(FLAGS.gpu_index.split(\',\'))\n  quota_base = num_distortions // num_gpus\n  overtime_task = num_distortions % num_gpus\n\n  quota = []\n  base = 0\n  for i in range(num_gpus):\n    if i < overtime_task:\n      quota.append(base + quota_base + 1)\n      base += quota_base + 1\n    else:\n      quota.append(base + quota_base)\n      base += quota_base\n\n  print(\'quota\', quota)\n  # assert sum(quota) == num_distortions\n\n  start_idx = 0 if no_thread is 0 else quota[no_thread - 1]\n  end_idx = quota[no_thread]\n  quota_distortions = distortions[start_idx:end_idx]\n  print(\'quota_distortions\', quota_distortions)\n\n  for distortion_name in quota_distortions:\n    rate = show_corruption_error_by_distortion(distortion_name, current_ckpt, no_thread)\n    ce = rate / ALEXNET_CE[distortion_name]\n    ABSOLUTE_CE[distortion_name] = rate\n    # error_rates.append(ce)\n    OURS_CE[distortion_name] = ce\n    print(\'Distortion: {:15s}  | CE (unnormalized) (%): {:.2f}  | CE (normalized) (%): {:.2f}\'.format(distortion_name,\n                                                                                                      100 * rate,\n                                                                                                      100 * ce))\n\n\ndef show_corruption_error():\n  num_gpus = len(FLAGS.gpu_index.split(\',\'))\n\n  if tf.gfile.IsDirectory(FLAGS.model_dir):\n    current_ckpt = tf.train.latest_checkpoint(FLAGS.model_dir)\n  else:\n    current_ckpt = FLAGS.model_dir\n\n  thread_execute(num_gpus, process, current_ckpt)\n\n  error_rates = list(OURS_CE.values())\n  abs_error_rates = list(ABSOLUTE_CE.values())\n  m_ce = np.mean(error_rates)\n  abs_m_ce = np.mean(abs_error_rates)\n  print(\'mCE (normalized by AlexNet errors) (%): {:.2f}\'.format(100 * m_ce))\n  print(\'mCE (unnormalized) (%): {:.2f}\'.format(100 * abs_m_ce))\n\n\n  tf.summary.scalar(\'mCE\', tf.constant(m_ce))\n  merged = tf.summary.merge_all()\n\n  model_dir = os.path.dirname(os.path.splitext(current_ckpt)[0])\n  test_writer = tf.summary.FileWriter(model_dir + \'/eval\')\n\n  with tf.Session() as sess:\n    sess.run(tf.global_variables_initializer())\n    summary = sess.run(merged)\n    global_step = int(os.path.basename(current_ckpt).split(\'-\')[1])\n    test_writer.add_summary(summary, global_step)\n\n\ndef main(_):\n  os.environ[\'CUDA_VISIBLE_DEVICES\'] = FLAGS.gpu_index\n\n  if FLAGS.robustness_type == \'ce\':\n    show_corruption_error()\n  elif FLAGS.robustness_type == \'fr\':\n    raise ValueError(\'not yet supported ({}\'.format(FLAGS.robustness_type))\n  else:\n    raise ValueError(\'invalid type ({})\'.format(FLAGS.robustness_type))\n\n\nif __name__ == \'__main__\':\n  tf.app.run()\n'"
metric/__init__.py,0,b''
metric/ece_metric.py,1,"b'from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\n\nfrom tensorflow.python.framework import dtypes\nfrom tensorflow.python.framework import ops\nfrom tensorflow.python.ops import array_ops\nfrom tensorflow.python.ops import confusion_matrix\nfrom tensorflow.python.ops import control_flow_ops\nfrom tensorflow.python.ops import math_ops\nfrom tensorflow.python.ops import metrics_impl\nfrom tensorflow.python.ops import state_ops\nfrom tensorflow.python.ops import variable_scope\n\nimport tensorflow as tf\n\ndef get_tf_version():\n  tf_version = tf.VERSION\n  tf_major_version, tf_minor_version, _ = tf_version.split(\'.\')\n  return int(tf_major_version), int(tf_minor_version)\n\ntf_major_version, tf_minor_version = get_tf_version()\nif tf_major_version == 1 and tf_minor_version <= 12:\n  from tensorflow.python.training import distribution_strategy_context\nelse:\n  from tensorflow.python.distribute import distribution_strategy_context\n\ndef _remove_squeezable_dimensions(predictions, labels, weights):\n  """"""Squeeze or expand last dim if needed.\n\n  Squeezes last dim of `predictions` or `labels` if their rank differs by 1\n  (using confusion_matrix.remove_squeezable_dimensions).\n  Squeezes or expands last dim of `weights` if its rank differs by 1 from the\n  new rank of `predictions`.\n\n  If `weights` is scalar, it is kept scalar.\n\n  This will use static shape if available. Otherwise, it will add graph\n  operations, which could result in a performance hit.\n\n  Args:\n    predictions: Predicted values, a `Tensor` of arbitrary dimensions.\n    labels: Optional label `Tensor` whose dimensions match `predictions`.\n    weights: Optional weight scalar or `Tensor` whose dimensions match\n      `predictions`.\n\n  Returns:\n    Tuple of `predictions`, `labels` and `weights`. Each of them possibly has\n    the last dimension squeezed, `weights` could be extended by one dimension.\n  """"""\n  predictions = ops.convert_to_tensor(predictions)\n  if labels is not None:\n    labels, predictions = confusion_matrix.remove_squeezable_dimensions(\n      labels, predictions)\n    predictions.get_shape().assert_is_compatible_with(labels.get_shape())\n\n  if weights is None:\n    return predictions, labels, None\n\n  weights = ops.convert_to_tensor(weights)\n  weights_shape = weights.get_shape()\n  weights_rank = weights_shape.ndims\n  if weights_rank == 0:\n    return predictions, labels, weights\n\n  predictions_shape = predictions.get_shape()\n  predictions_rank = predictions_shape.ndims\n  if (predictions_rank is not None) and (weights_rank is not None):\n    # Use static rank.\n    if weights_rank - predictions_rank == 1:\n      weights = array_ops.squeeze(weights, [-1])\n    elif predictions_rank - weights_rank == 1:\n      weights = array_ops.expand_dims(weights, [-1])\n  else:\n    # Use dynamic rank.\n    weights_rank_tensor = array_ops.rank(weights)\n    rank_diff = weights_rank_tensor - array_ops.rank(predictions)\n\n    def _maybe_expand_weights():\n      return control_flow_ops.cond(\n        math_ops.equal(rank_diff, -1),\n        lambda: array_ops.expand_dims(weights, [-1]), lambda: weights)\n\n    # Don\'t attempt squeeze if it will fail based on static check.\n    if ((weights_rank is not None) and\n            (not weights_shape.dims[-1].is_compatible_with(1))):\n      maybe_squeeze_weights = lambda: weights\n    else:\n      maybe_squeeze_weights = lambda: array_ops.squeeze(weights, [-1])\n\n    def _maybe_adjust_weights():\n      return control_flow_ops.cond(\n        math_ops.equal(rank_diff, 1), maybe_squeeze_weights,\n        _maybe_expand_weights)\n\n    # If weights are scalar, do nothing. Otherwise, try to add or remove a\n    # dimension to match predictions.\n    weights = control_flow_ops.cond(\n      math_ops.equal(weights_rank_tensor, 0), lambda: weights,\n      _maybe_adjust_weights)\n  return predictions, labels, weights\n\ndef _aggregate_across_towers(metrics_collections, metric_value_fn, *args):\n  """"""Aggregate metric value across towers.""""""\n  def fn(distribution, *a):\n    """"""Call `metric_value_fn` in the correct control flow context.""""""\n    if hasattr(distribution, \'_outer_control_flow_context\'):\n      # If there was an outer context captured before this method was called,\n      # then we enter that context to create the metric value op. If the\n      # caputred context is `None`, ops.control_dependencies(None) gives the\n      # desired behavior. Else we use `Enter` and `Exit` to enter and exit the\n      # captured context.\n      # This special handling is needed because sometimes the metric is created\n      # inside a while_loop (and perhaps a TPU rewrite context). But we don\'t\n      # want the value op to be evaluated every step or on the TPU. So we\n      # create it outside so that it can be evaluated at the end on the host,\n      # once the update ops have been evaluted.\n\n      # pylint: disable=protected-access\n      if distribution._outer_control_flow_context is None:\n        with ops.control_dependencies(None):\n          metric_value = metric_value_fn(distribution, *a)\n      else:\n        distribution._outer_control_flow_context.Enter()\n        metric_value = metric_value_fn(distribution, *a)\n        distribution._outer_control_flow_context.Exit()\n        # pylint: enable=protected-access\n    else:\n      metric_value = metric_value_fn(distribution, *a)\n    if metrics_collections:\n      ops.add_to_collections(metrics_collections, metric_value)\n    return metric_value\n\n  return distribution_strategy_context.get_tower_context().merge_call(fn, *args)\n\ndef _aggregate_across_replicas(metrics_collections, metric_value_fn, *args):\n  """"""Aggregate metric value across replicas.""""""\n  def fn(distribution, *a):\n    """"""Call `metric_value_fn` in the correct control flow context.""""""\n    if hasattr(distribution.extended, \'_outer_control_flow_context\'):\n      # If there was an outer context captured before this method was called,\n      # then we enter that context to create the metric value op. If the\n      # caputred context is `None`, ops.control_dependencies(None) gives the\n      # desired behavior. Else we use `Enter` and `Exit` to enter and exit the\n      # captured context.\n      # This special handling is needed because sometimes the metric is created\n      # inside a while_loop (and perhaps a TPU rewrite context). But we don\'t\n      # want the value op to be evaluated every step or on the TPU. So we\n      # create it outside so that it can be evaluated at the end on the host,\n      # once the update ops have been evaluted.\n\n      # pylint: disable=protected-access\n      if distribution.extended._outer_control_flow_context is None:\n        with ops.control_dependencies(None):\n          metric_value = metric_value_fn(distribution, *a)\n      else:\n        distribution.extended._outer_control_flow_context.Enter()\n        metric_value = metric_value_fn(distribution, *a)\n        distribution.extended._outer_control_flow_context.Exit()\n        # pylint: enable=protected-access\n    else:\n      metric_value = metric_value_fn(distribution, *a)\n    if metrics_collections:\n      ops.add_to_collections(metrics_collections, metric_value)\n    return metric_value\n\n  return distribution_strategy_context.get_replica_context().merge_call(\n      fn, args=args)\n\ndef ece(conf, pred, label, num_thresholds=10,\n        metrics_collections=None, updates_collections=None, name=None):\n  """"""\n  Calculate expected calibration error(ece).\n  :param conf: The confidence values a `Tensor` of any shape.\n  :param pred: The predicted values, a whose shape matches `conf`.\n  :param label: The ground truth values, a `Tensor` whose shape matches `conf`.\n  :param num_thresholds: The number of thresholds to use when discretizing reliability diagram.\n  :param metrics_collections: An optional list of collections that `ece` should be added to.\n  :param updates_collections: An optional list of collections that `update_op` should be added to.\n  :param name: An optional variable_scope name.\n  :return:\n    ece: A scalar `Tensor` representing the current `ece` score\n    update_op: An operation that increments the `ece` score\n  """"""\n\n  with variable_scope.variable_scope(\n          name, \'ece\', (conf, pred, label)):\n\n    pred, label, conf = _remove_squeezable_dimensions(\n      predictions=pred, labels=label, weights=conf)\n\n    if pred.dtype != label.dtype:\n      pred = math_ops.cast(pred, label.dtype)\n\n    conf_2d = array_ops.reshape(conf, [-1, 1])\n    pred_2d = array_ops.reshape(pred, [-1, 1])\n    true_2d = array_ops.reshape(label, [-1, 1])\n\n    # Use static shape if known.\n    num_predictions = conf_2d.get_shape().as_list()[0]\n\n    # Otherwise use dynamic shape.\n    if num_predictions is None:\n      num_predictions = array_ops.shape(conf_2d)[0]\n\n    # To account for floating point imprecisions / avoid division by zero.\n    epsilon = 1e-7\n    thresholds = [(i + 1) * 1.0 / (num_thresholds)\n                  for i in range(num_thresholds - 1)]\n    thresholds = [0.0 - epsilon] + thresholds + [1.0 + epsilon]\n\n    min_th = thresholds[0:num_thresholds]\n    max_th = thresholds[1:num_thresholds + 1]\n\n    min_thresh_tiled = array_ops.tile(\n      array_ops.expand_dims(array_ops.constant(min_th), [1]),\n      array_ops.stack([1, num_predictions]))\n    max_thresh_tiled = array_ops.tile(\n      array_ops.expand_dims(array_ops.constant(max_th), [1]),\n      array_ops.stack([1, num_predictions]))\n\n    conf_is_greater_th = math_ops.greater(\n      array_ops.tile(array_ops.transpose(conf_2d), [num_thresholds, 1]),\n      min_thresh_tiled)\n    conf_is_less_equal_th = math_ops.less_equal(\n      array_ops.tile(array_ops.transpose(conf_2d), [num_thresholds, 1]),\n      max_thresh_tiled)\n\n    # The `which_bin_conf_include` is num_thresholds x num_predictions (i, j) matrix\n    # which if j-th prediction is included in i-th threshold, it is True\n    which_bin_conf_include = math_ops.logical_and(conf_is_greater_th, conf_is_less_equal_th)\n\n    # The `pred_2d_tiled` and `true_2d_tiled` is num_thresholds x num_predictions (i, j) matrix\n    conf_2d_tiled = array_ops.tile(array_ops.transpose(conf_2d), [num_thresholds, 1])\n    pred_2d_tiled = array_ops.tile(array_ops.transpose(pred_2d), [num_thresholds, 1])\n    true_2d_tiled = array_ops.tile(array_ops.transpose(true_2d), [num_thresholds, 1])\n\n    is_correct = math_ops.equal(pred_2d_tiled, true_2d_tiled)\n\n    # The sum of correct answers count per threshold bin\n    is_correct_per_bin = math_ops.reduce_sum(\n      math_ops.to_float(math_ops.logical_and(is_correct, which_bin_conf_include)),\n      1\n    )\n\n    # The sum of confidence per threshold bin\n    conf_per_bin = math_ops.multiply(conf_2d_tiled, math_ops.cast(which_bin_conf_include, dtypes.float32))\n    sum_conf_per_bin = math_ops.reduce_sum(conf_per_bin, 1)\n\n    # The number of predictions per threshold bin\n    len_per_bin = math_ops.reduce_sum(math_ops.to_float(which_bin_conf_include), 1)\n\n    accumulated_correct = metrics_impl.metric_variable(\n      [num_thresholds], dtypes.float32, name=\'accuracy_per_bin\')\n    accumulated_conf = metrics_impl.metric_variable(\n      [num_thresholds], dtypes.float32, name=\'confidence_per_bin\')\n    accumulated_cnt = metrics_impl.metric_variable(\n      [num_thresholds], dtypes.float32, name=\'count_per_bin\')\n\n    update_ops = {}\n    update_ops[\'correct\'] = state_ops.assign_add(accumulated_correct, is_correct_per_bin)\n    update_ops[\'conf\'] = state_ops.assign_add(accumulated_conf, sum_conf_per_bin)\n    update_ops[\'cnt\'] = state_ops.assign_add(accumulated_cnt, len_per_bin)\n\n    values = {}\n    values[\'correct\'] = accumulated_correct\n    values[\'conf\'] = accumulated_conf\n    values[\'cnt\'] = accumulated_cnt\n\n    def compute_ece(correct, conf, cnt, name):\n      acc = math_ops.div(correct, epsilon + cnt, name=\'avg_acc_per_bin_\' + name)\n      avg_conf = math_ops.div(conf, epsilon + cnt, name=\'avg_conf_per_bin_\' + name)\n      abs_err = math_ops.abs(acc - avg_conf)\n      sum_cnt = array_ops.reshape(math_ops.reduce_sum(cnt), [-1, ])\n      sum_cnt_tiled = array_ops.tile(sum_cnt, [num_thresholds, ])\n      weight = math_ops.div(cnt, sum_cnt_tiled)\n      weighted_abs_err = math_ops.multiply(weight, abs_err)\n      return math_ops.reduce_sum(weighted_abs_err)\n\n    def ece_across_towers(_, correct, conf, cnt):\n      ece = compute_ece(correct=correct, conf=conf,\n                        cnt=cnt, name=\'value\')\n      return ece\n\n    if tf_major_version == 1 and tf_minor_version <= 12:\n      ece = _aggregate_across_towers(metrics_collections, ece_across_towers,\n                                 values[\'correct\'], values[\'conf\'], values[\'cnt\'])\n    else:\n      ece = _aggregate_across_replicas(metrics_collections, ece_across_towers,\n                                 values[\'correct\'], values[\'conf\'], values[\'cnt\'])\n\n    update_op = compute_ece(correct=update_ops[\'correct\'], conf=update_ops[\'conf\'],\n                            cnt=update_ops[\'cnt\'], name=\'update\')\n    if updates_collections:\n      ops.add_to_collections(updates_collections, update_op)\n\n    return ece, update_op\n'"
metric/recall_metric.py,46,"b""# coding=utf8\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport os\n\nimport numpy as np\nimport tensorflow as tf\nfrom sklearn.metrics.pairwise import euclidean_distances\nfrom sklearn.preprocessing import normalize\n\ndef recall_at_k(flags_obj,\n                flags_core,\n                input_function_eval,\n                model_for_eval,\n                num_val_images,\n                return_embedding=False,\n                eval_similarity='cosine',\n                ):\n  # When evaluating N-pair, use classfication validation.\n  datasets = input_function_eval(\n    is_training=False, data_dir=flags_obj.data_dir,\n    batch_size=int(flags_obj.val_batch_size),\n    num_epochs=1,\n    dct_method=flags_obj.dct_method,\n    dataset_name=flags_obj.dataset_name,\n    dtype=flags_core.get_tf_dtype(flags_obj),\n    preprocessing_type=flags_obj.preprocessing_type,\n    val_regex=flags_obj.val_regex)\n\n  one_shot_iter = datasets.make_one_shot_iterator()\n  image_and_label = one_shot_iter.get_next()\n\n  num_gpus = 1\n\n  input_images = image_and_label[0]\n\n  batch_size = tf.shape(input_images)[0]\n\n  mod_val = tf.mod(batch_size, num_gpus)\n\n  # In order for the number of images to be multiples of the integer of gpus, input_images[:num_gpus - mod_val] temporarily added!\n  input_images = tf.cond(tf.equal(mod_val,0), lambda: input_images, lambda: tf.concat([input_images, input_images[:num_gpus - mod_val]], axis=0))\n\n  split_input_images = tf.split(input_images, num_gpus, axis=0)\n\n  embedding_list = []\n  for i in range(num_gpus):\n    with tf.device('/gpu:%d' % i):\n      if return_embedding:\n        embedding = model_for_eval(split_input_images[i],\n                                   False,\n                                   use_resnet_d=flags_obj.use_resnet_d,\n                                   return_embedding=True)\n        tf.logging.info('Evaluation using Embedding')\n      else:\n        embedding = model_for_eval(split_input_images[i],\n                                   False,\n                                   use_resnet_d=flags_obj.use_resnet_d)\n      tf.get_variable_scope().reuse_variables()\n      embedding_list.append(embedding)\n\n  # then, we only needs embeddings as many as the number of input images. We simply sliced embeddings using [:batch_size]\n  embedding = tf.concat(embedding_list, axis=0)[:batch_size]\n\n  # muti-gpu sorting\n  if flags_obj.dtype == 'fp32':\n    ph_query = tf.placeholder(tf.float32, shape=[None, None], name='ph_query')\n    ph_index = tf.placeholder(tf.float32, shape=[None, None], name='ph_index')\n  else:\n    ph_query = tf.placeholder(tf.float16, shape=[None, None], name='ph_query')\n    ph_index = tf.placeholder(tf.float16, shape=[None, None], name='ph_index')\n\n  k_list = flags_obj.recall_at_k\n\n  query_size = tf.shape(ph_query)[0]\n  batch_size = tf.shape(ph_index)[0]\n\n  mod_val = tf.mod(batch_size, num_gpus)\n\n  # dummy features\n  # In order for the number of index features to be multiples of the integer of gpus, dummy features temporarily added!\n  # We use zero-valued embeddings as dummy features due to sorting a similarity matrix.\n  if flags_obj.dtype == 'fp32':\n      dummy_features = tf.zeros([num_gpus - mod_val, tf.shape(ph_index)[1]])\n  else:\n      dummy_features = tf.zeros([num_gpus - mod_val, tf.shape(ph_index)[1]], dtype=tf.float16)\n  index_features = tf.cond(tf.equal(mod_val,0), lambda: ph_index, lambda: tf.concat([ph_index, dummy_features], axis=0))\n\n  split_index_features = tf.split(index_features, num_gpus, axis=0)\n\n  check_split = tf.shape(split_index_features[0])[0]\n\n  split_top_k_indices_list = []\n  split_top_k_values_list = []\n\n  if eval_similarity == 'cosine':\n    l2_query = tf.nn.l2_normalize(ph_query, axis=1)\n  for i in range(num_gpus):\n    with tf.device('/gpu:%d' % i):\n      if eval_similarity == 'cosine':\n        l2_index = tf.nn.l2_normalize(split_index_features[i], axis=1)\n        split_mat = tf.matmul(l2_query, l2_index, transpose_b=True)\n      elif eval_similarity == 'euclidean':\n        split_mat = -tf_simple_pairwise_distance(ph_query, split_index_features[i])\n      else:\n        raise NotImplementedError\n\n      split_top_k_values, split_top_k_indices = tf.nn.top_k(split_mat, k=np.max(k_list) + 1, sorted=True)\n      split_top_k_indices += i * check_split\n      split_top_k_values_list.append(split_top_k_values)\n      split_top_k_indices_list.append(split_top_k_indices)\n\n  pre_top_k_indices = tf.concat(split_top_k_indices_list, axis=1)\n  pre_top_k_values = tf.concat(split_top_k_values_list, axis=1)\n\n  post_top_k_values, post_top_k_indices = tf.nn.top_k(pre_top_k_values, k=np.max(k_list) + 1, sorted=True)\n\n  ii, _ = tf.meshgrid(tf.range(query_size, dtype=tf.int32), tf.range(np.max(k_list) + 1, dtype=tf.int32), indexing='ij')\n\n  indices = tf.stack([ii, post_top_k_indices], axis=-1)\n  top_k_indices = tf.gather_nd(pre_top_k_indices, indices)\n\n  saver = tf.train.Saver()\n\n  if tf.gfile.IsDirectory(flags_obj.model_dir):\n    checkpoint_path = tf.train.latest_checkpoint(flags_obj.model_dir)\n  else:\n    checkpoint_path = flags_obj.model_dir\n\n  np_features = np.zeros((num_val_images, int(embedding.get_shape()[1])), dtype=np.float32)\n  np_labels = np.zeros(num_val_images, dtype=np.int64)\n  np_i = 0\n  with tf.Session() as sess:\n    sess.run(tf.global_variables_initializer())\n    saver.restore(sess, checkpoint_path)\n\n    while True:\n      try:\n        np_predict, np_label = sess.run([embedding, image_and_label[1]])\n        np_features[np_i:np_i + np_predict.shape[0], :] = np_predict\n        np_labels[np_i:np_i + np_label.shape[0]] = np_label\n        np_i += np_predict.shape[0]\n\n      except tf.errors.OutOfRangeError:\n        break\n    assert np_i == num_val_images\n\n    # split distractors and others\n    not_dist_idx = (np_labels != -1)\n    dist_idx = np.logical_not(not_dist_idx)\n    np_query = np_features[not_dist_idx]\n    np_query_labels = np_labels[not_dist_idx]\n\n    np_dist = np_features[dist_idx]\n    np_dist_labels = np_labels[dist_idx]\n\n    sorted_idx = get_sorted_idx(np_query, np_features, sess, ph_query, ph_index, top_k_indices)\n\n  recall_dict = get_recall(sorted_idx, np_query_labels, np_labels, k_list)\n\n  for key in recall_dict.keys():\n    tf.summary.scalar('recall_at_%d' % (key), tf.constant(recall_dict[key]))\n\n  merged = tf.summary.merge_all()\n\n  test_writer = tf.summary.FileWriter(flags_obj.model_dir + '/eval')\n\n  with tf.Session() as sess:\n    sess.run(tf.global_variables_initializer())\n    summary = sess.run(merged)\n    global_step = int(os.path.basename(checkpoint_path).split('-')[1])\n    test_writer.add_summary(summary, global_step)\n  eval_results = {}\n\n  for key in recall_dict.keys():\n    eval_results['recall_at_%d' % (key)] = recall_dict[key]\n  eval_results['global_step'] = global_step\n  return eval_results\n\ndef get_sorted_idx(np_query, np_features, sess, ph_query, ph_index, top_k_indices):\n  num_val_images = len(np_query)\n\n  if num_val_images < 50000:\n    sorted_idx = sess.run(top_k_indices, feed_dict={ph_query: np_query, ph_index: np_features})\n  else:\n    # do split sorting\n    check_size = 10000\n    how_many = num_val_images // check_size\n    sorted_idx = []\n\n    for idx in range(how_many + 1):\n      if (idx + 1) * check_size >= num_val_images:\n        selected_query_features = np_query[idx * check_size:]\n      else:\n        selected_query_features = np_query[idx * check_size:(idx+1) * check_size]\n      if len(selected_query_features) == 0:\n        break\n\n      sorted_idx.append(sess.run(top_k_indices, feed_dict={ph_query: selected_query_features, ph_index: np_features}))\n\n    sorted_idx = np.concatenate(sorted_idx, axis=0)\n\n  return sorted_idx\n\ndef tf_simple_pairwise_distance(query, index):\n  a = tf.reduce_sum(tf.square(query), axis=1, keepdims=True)\n  b = tf.reduce_sum(tf.square(tf.transpose(index)), axis=0, keepdims=True)\n\n  ab = tf.matmul(query, index, transpose_b=True)\n\n  l2_distance_squared = tf.add(a, b) - 2.0 * ab\n\n  return l2_distance_squared\n\ndef get_recall(sorted_idx, np_query_labels, np_labels, k_list=[1, 5]):\n  num_val_images = len(np_query_labels)\n  count_dict = { k:0. for k in k_list}\n\n  for query_idx, top_k in enumerate(sorted_idx):\n    top_k = list(filter(lambda x: x != query_idx, top_k))\n    sorted_labels = [np_labels[i] for i in top_k]\n    for k in k_list:\n      if np_query_labels[query_idx] in sorted_labels[:k]:\n        count_dict[k] += 1.\n\n  return {k: v / num_val_images for k, v in count_dict.items()}\n"""
nets/__init__.py,0,b''
nets/blocks.py,59,"b'# coding=utf8\n# ==============================================================================\n# Copyright 2020-present NAVER Corp.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\nfrom nets.model_helper import *\nimport numpy as np\nimport math\n\n\ndef generalized_mean_pooling(x, p=3, data_format=\'channels_first\'):\n  if data_format == \'channels_first\':\n    _, c, h, w = x.shape.as_list()\n    reduce_axis = [2, 3]\n  else:\n    _, h, w, c = x.shape.as_list()\n    reduce_axis = [1, 2]\n\n  N = tf.to_float(tf.multiply(h, w))\n  if x.dtype == tf.float16:\n    # For numerical stability, cast to fp32, calculate, and then change back to fp16.\n    x = tf.cast(x, tf.float32)\n\n  epsilon = 1e-6\n  x = tf.clip_by_value(x, epsilon, 1e12)\n  x_p = tf.pow(x, p)\n  x_p_sum = tf.maximum(tf.reduce_sum(x_p, axis=reduce_axis, keep_dims=True), epsilon)\n  pooled_x = tf.pow(N, -1.0 / p) * tf.pow(x_p_sum, 1 / p)\n  if x.dtype == tf.float16:\n    pooled_x = tf.cast(pooled_x, tf.float16)\n  return pooled_x\n\n\ndef anti_aliased_downsample(inp, data_format=\'channels_first\',\n                            filt_size=3, stride=2, name=None,\n                            pad_off=0):\n  pading_size = int(1. * (filt_size - 1) / 2)\n  if data_format == \'channels_first\':\n    pad_sizes = [[0, 0], [0, 0], [pading_size + pad_off, pading_size + pad_off],\n                 [pading_size + pad_off, pading_size + pad_off]]\n  else:\n    pad_sizes = [[0, 0], [pading_size + pad_off, pading_size + pad_off], [pading_size + pad_off, pading_size + pad_off],\n                 [0, 0]]\n\n  if filt_size == 1:\n    a = np.array([1., ])\n  elif filt_size == 2:\n    a = np.array([1., 1.])\n  elif filt_size == 3:\n    a = np.array([1., 2., 1.])\n  elif filt_size == 4:\n    a = np.array([1., 3., 3., 1.])\n  elif filt_size == 5:\n    a = np.array([1., 4., 6., 4., 1.])\n  elif filt_size == 6:\n    a = np.array([1., 5., 10., 10., 5., 1.])\n  elif filt_size == 7:\n    a = np.array([1., 6., 15., 20., 15., 6., 1.])\n\n  channel_axis = 1 if data_format == \'channels_first\' else 3\n  G = inp.shape[channel_axis]\n\n  filt = tf.constant(a[:, None] * a[None, :], inp.dtype)\n  filt = filt / tf.reduce_sum(filt)\n  filt = tf.reshape(filt, [filt_size, filt_size, 1, 1])\n  filt = tf.tile(filt, [1, 1, 1, G])\n\n  if filt_size == 1:\n    if pad_off == 0:\n      return inp[:, :, ::stride, ::stride]\n    else:\n      padded = tf.pad(inp, pad_sizes, ""REFLECT"")\n      return padded[:, :, ::stride, ::stride]\n  else:\n    inp = tf.pad(inp, pad_sizes, ""REFLECT"")\n    data_format = ""NCHW"" if data_format == \'channels_first\' else ""NHWC""\n\n    strides = [1, 1, stride, stride] if data_format == \'NCHW\' else [1, stride, stride, 1]\n\n    with tf.variable_scope(name, ""anti_alias"", [inp]) as name:\n      try:\n        output = tf.nn.conv2d(inp, filt,\n                              strides=strides,\n                              padding=\'VALID\',\n                              data_format=data_format)\n      except:  # When not support group conv\n        tf.logging.info(\'Group conv by looping\')\n        inp = tf.split(inp, G, axis=channel_axis)\n        filters = tf.split(filt, G, axis=3)\n        output = tf.concat(\n          [tf.nn.conv2d(i, f,\n                        strides=strides,\n                        padding=\'VALID\',\n                        data_format=data_format) for i, f in zip(inp, filters)], axis=1 if data_format == \'NCHW\' else 3)\n\n    return output\n\n\ndef sk_conv2d(inputs, filters, strides, r=2, L=32, data_format=\'channels_first\', training=True, name=None,\n              bn_momentum=0.997):\n  channel_axis = 1 if data_format == \'channels_first\' else 3\n  inputs = conv2d_fixed_padding(\n    inputs=inputs, filters=filters * 2, kernel_size=3, strides=strides,\n    data_format=data_format)\n  inputs = batch_norm(inputs, training, data_format, momentum=bn_momentum)\n  inputs = tf.nn.relu(inputs)\n\n  if data_format == \'channels_last\':\n    _, height, width, channel = inputs.shape.as_list()\n  elif data_format == \'channels_first\':\n    _, channel, height, width = inputs.shape.as_list()\n  else:\n    raise NotImplementedError\n\n  # feas_shape = [filters, height, width] if data_format == \'channels_first\' else [height, width, filters]\n  # feas = tf.reshape(inputs, [2, -1] + feas_shape)\n  feas = tf.split(inputs, num_or_size_splits=2, axis=channel_axis)\n  fea_U = tf.reduce_sum(feas, axis=0)\n\n  pooling_axes = [2, 3] if data_format == \'channels_first\' else [1, 2]\n  fea_s = tf.reduce_mean(fea_U, pooling_axes, keepdims=True)\n\n  d = max(int(filters / r), L)\n  with tf.variable_scope(name, ""sk_block"", [inputs]) as name:\n    fea_z = tf.layers.conv2d(\n      inputs=fea_s, filters=d, kernel_size=1, strides=1,\n      kernel_initializer=tf.variance_scaling_initializer(),\n      use_bias=False,\n      data_format=data_format, name=\'sk_fc_1\')\n    fea_z = batch_norm(fea_z, training, data_format, momentum=bn_momentum)\n    fea_z = tf.nn.relu(fea_z)\n    attention = tf.layers.conv2d(\n      inputs=fea_z, filters=filters * 2, kernel_size=1, strides=1,\n      kernel_initializer=tf.variance_scaling_initializer(),\n      use_bias=False,\n      data_format=data_format, name=\'sk_fc_2\')\n    # attention_shape = [filters, 1, 1] if data_format == \'channels_first\' else [1, 1, filters]\n    # attention = tf.reshape(attention, [2, -1] + attention_shape)\n    attention = tf.split(attention, num_or_size_splits=2, axis=channel_axis)\n    attention = tf.nn.softmax(attention, axis=0)\n    fea_v = tf.reduce_sum(feas * attention, axis=0)\n\n  return fea_v\n\ndef se_block(x, name=None, ratio=16, data_format=\'channels_last\'):\n  """""" https://arxiv.org/abs/1709.01507.\n  """"""\n  # h, w = x.size()[-2:]\n  if data_format == \'channels_last\':\n    _, height, width, channel = x.shape.as_list()\n    reduce_spatial = [1, 2]\n  elif data_format == \'channels_first\':\n    _, channel, height, width = x.shape.as_list()\n    reduce_spatial = [2, 3]\n  else:\n    raise NotImplementedError\n\n  with tf.variable_scope(name, ""se_block"", [x]) as name:\n    squeeze = tf.reduce_mean(x, axis=reduce_spatial, keepdims=True)\n    excitation = tf.layers.conv2d(\n      inputs=squeeze, filters=channel // ratio, kernel_size=1, strides=1,\n      kernel_initializer=tf.variance_scaling_initializer(),\n      use_bias=False,\n      data_format=data_format, name=\'seblock_dense_1\')\n    excitation = tf.nn.relu(excitation)\n    excitation = tf.layers.conv2d(\n      inputs=excitation, filters=channel, kernel_size=1, strides=1,\n      kernel_initializer=tf.variance_scaling_initializer(),\n      use_bias=False,\n      data_format=data_format, name=\'seblock_dense_2\')\n    excitation = tf.nn.sigmoid(excitation)\n    scale = x * excitation\n  return scale\n\n\ndef _bernoulli(shape, mean, seed=None, dtype=tf.float32):\n  return tf.nn.relu(tf.sign(mean - tf.random_uniform(shape, minval=0, maxval=1, dtype=dtype, seed=seed)))\n\n\ndef dropblock(x, keep_prob, block_size, gamma_scale=1.0, seed=None, name=None,\n              data_format=\'channels_last\', is_training=True):  # pylint: disable=invalid-name\n  """"""\n  Dropblock layer. For more details, refer to https://arxiv.org/abs/1810.12890\n  :param x: A floating point tensor.\n  :param keep_prob: A scalar Tensor with the same type as x. The probability that each element is kept.\n  :param block_size: The block size to drop\n  :param gamma_scale: The multiplier to gamma.\n  :param seed:  Python integer. Used to create random seeds.\n  :param name: A name for this operation (optional)\n  :param data_format: \'channels_last\' or \'channels_first\'\n  :param is_training: If False, do nothing.\n  :return: A Tensor of the same shape of x.\n  """"""\n  if not is_training:\n    return x\n\n  # Early return if nothing needs to be dropped.\n  if (isinstance(keep_prob, float) and keep_prob == 1) or gamma_scale == 0:\n    return x\n\n  with tf.name_scope(name, ""dropblock"", [x]) as name:\n    if not x.dtype.is_floating:\n      raise ValueError(""x has to be a floating point tensor since it\'s going to""\n                       "" be scaled. Got a %s tensor instead."" % x.dtype)\n    if isinstance(keep_prob, float) and not 0 < keep_prob <= 1:\n      raise ValueError(""keep_prob must be a scalar tensor or a float in the ""\n                       ""range (0, 1], got %g"" % keep_prob)\n\n    br = (block_size - 1) // 2\n    tl = (block_size - 1) - br\n    if data_format == \'channels_last\':\n      _, h, w, c = x.shape.as_list()\n      sampling_mask_shape = tf.stack([1, h - block_size + 1, w - block_size + 1, c])\n      pad_shape = [[0, 0], [tl, br], [tl, br], [0, 0]]\n    elif data_format == \'channels_first\':\n      _, c, h, w = x.shape.as_list()\n      sampling_mask_shape = tf.stack([1, c, h - block_size + 1, w - block_size + 1])\n      pad_shape = [[0, 0], [0, 0], [tl, br], [tl, br]]\n    else:\n      raise NotImplementedError\n\n    gamma = (1. - keep_prob) * (w * h) / (block_size ** 2) / ((w - block_size + 1) * (h - block_size + 1))\n    gamma = gamma_scale * gamma\n    mask = _bernoulli(sampling_mask_shape, gamma, seed, tf.float32)\n    mask = tf.pad(mask, pad_shape)\n\n    xdtype_mask = tf.cast(mask, x.dtype)\n    xdtype_mask = tf.layers.max_pooling2d(\n      inputs=xdtype_mask, pool_size=block_size,\n      strides=1, padding=\'SAME\',\n      data_format=data_format)\n\n    xdtype_mask = 1 - xdtype_mask\n    fp32_mask = tf.cast(xdtype_mask, tf.float32)\n    ret = tf.multiply(x, xdtype_mask)\n    float32_mask_size = tf.cast(tf.size(fp32_mask), tf.float32)\n    float32_mask_reduce_sum = tf.reduce_sum(fp32_mask)\n    normalize_factor = tf.cast(float32_mask_size / (float32_mask_reduce_sum + 1e-8), x.dtype)\n    ret = ret * normalize_factor\n    return ret\n'"
nets/hparams_config.py,0,"b'# coding=utf8\n# ==============================================================================\n# Copyright 2020-present NAVER Corp.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom official.utils.flags import core as flags_core\n\ndef define_metric_learning_flags(flags):\n  define_common_flags(flags)\n\n  flags.DEFINE_boolean(\n    name=\'return_embedding\', default=False,\n    help=flags_core.help_wrap(\'Do not use the last fc.\'))\n\ndef define_common_flags(flags):\n  flags.DEFINE_string(\n    name=\'dataset_name\', default=None,\n    help=flags_core.help_wrap(\'imagenet, food100, food101, cub_200_2011\'))\n\n  ################################################################################\n  # Basic Hyperparameters\n  ################################################################################\n  flags.DEFINE_integer(\n    name=\'val_batch_size\', default=256,\n    help=flags_core.help_wrap(\n      \'The number of steps to warmup learning rate.\'))\n  flags.DEFINE_string(\n    name=\'pretrained_model_checkpoint_path\', short_name=\'pmcp\', default=None,\n    help=flags_core.help_wrap(\n      \'If not None initialize all the network except the final layer with \'\n      \'these values\'))\n  flags.DEFINE_float(\n    name=\'num_epochs_per_decay\', short_name=\'nepd\', default=2.0,\n    help=flags_core.help_wrap(\n      \'Number of epochs after which learning rate decays.\'))\n  flags.DEFINE_float(\n    name=\'learning_rate_decay_factor\', short_name=\'lrdf\', default=0.94,\n    help=flags_core.help_wrap(\n      \'Number of epochs after which learning rate decays.\'))\n  flags.DEFINE_float(\n    name=\'base_learning_rate\', short_name=\'blr\', default=0.01,\n    help=flags_core.help_wrap(\n      \'Base learning rate.\'))\n  flags.DEFINE_float(\n    name=\'end_learning_rate\', short_name=\'elr\', default=0.0001,\n    help=flags_core.help_wrap(\n      \'The minimal end learning rate used by a polynomial decay learning rate.\'))\n  flags.DEFINE_string(\n    name=\'learning_rate_decay_type\', short_name=\'lrdt\', default=\'exponential\',\n    help=flags_core.help_wrap(\n      \'Specifies how the learning rate is decayed. One of \'\n      \'""fixed"", ""exponential"", ""polynomial"", ""piecewise"", ""cosine""\'))\n  flags.DEFINE_float(\n    name=\'momentum\', short_name=\'mmt\', default=0.9,\n    help=flags_core.help_wrap(\n      \'The momentum for the MomentumOptimizer.\'))\n  flags.DEFINE_float(\n    name=\'bn_momentum\', default=0.997,\n    help=flags_core.help_wrap(\n      \'batch normalization momentum.\'))\n  flags.DEFINE_integer(\n    name=\'embedding_size\', default=0,\n    help=flags_core.help_wrap(\'When embedding_size> 0, add embedding between global average pool and fc.\'))\n  flags.DEFINE_list(\n    name=\'piecewise_lr_boundary_epochs\', short_name=\'pwlrb\', default=[30, 60, 80, 90],\n    help=flags_core.help_wrap(\n      \'A list of ints with strictly increasing entries to reduce the learning rate at certain epochs. \'\n      \'It\\\'s for piecewise lr decay type purpose only.\'))\n  flags.DEFINE_list(\n    name=\'piecewise_lr_decay_rates\', short_name=\'pwlrd\',\n    default=[1, 0.1, 0.01, 0.001, 1e-4],\n    help=flags_core.help_wrap(\n      \'A list of floats that specifies the decay rates for the intervals defined by piecewise_lr_boundary_epochs. \'\n      \'It should have one more element than piecewise_lr_boundary_epochs. It\\\'s for piecewise lr decay type purpose only.\'))\n  flags.DEFINE_string(\n    name=\'eval_similarity\', default=\'cosine\',\n    help=flags_core.help_wrap(\'cosine or euclidean\'))\n\n  ################################################################################\n  # Loss related\n  ################################################################################\n  flags.DEFINE_string(\n    name=\'cls_loss_type\', default=\'softmax\',\n    help=flags_core.help_wrap(\n      \'`softmax`, `sigmoid`\'))\n\n  flags.DEFINE_string(\n    name=\'logit_type\', default=None,\n    help=flags_core.help_wrap(\'Logit type `arc_margin` or `None`.\'))\n\n  flags.DEFINE_float(\n    name=\'arc_s\', default=80.0,\n    help=flags_core.help_wrap(\n      \'s of arc-margin loss\'))\n\n  flags.DEFINE_float(\n    name=\'arc_m\', default=0.15,\n    help=flags_core.help_wrap(\n      \'margin m of arc-margin loss\'))\n\n  flags.DEFINE_string(\n    name=\'pool_type\', default=\'gap\',\n    help=flags_core.help_wrap(\'`gap` or `gem` or `flatten`\'))\n\n  flags.DEFINE_boolean(\n    name=\'no_downsample\', default=False,\n    help=flags_core.help_wrap(\'If true, remove downsample in group 4\'))\n\n  ################################################################################\n  # Data augmentation\n  ################################################################################\n  flags.DEFINE_string(\n    name=\'preprocessing_type\', default=\'imagenet\',\n    help=flags_core.help_wrap(\' ""imagenet"" or ""cub""\'))\n\n  flags.DEFINE_string(\n    name=\'autoaugment_type\', default=None,\n    help=flags_core.help_wrap(\n      \'Specifies auto augmentation type. One of ""imagenet"", ""svhn"", ""cifar"", ""good""\'\n      \'To use numpy implementation, prefix ""np_"" to the type.\'))\n\n  flags.DEFINE_integer(\n    name=\'mixup_type\', short_name=\'mixup_type\', default=0,\n    help=flags_core.help_wrap(\n      \'Use mixup data augmentation. For more details, refer to https://arxiv.org/abs/1710.09412\'\n      \'If type is 0, do not use mixup.\'\n      \'If the type is 1, mix up twice the batch_size to produce batch_size data.\'\n      \'If the type is 2, it mixes up as much as batch_size and produces as much data as batch_size. \'\n      \'Faster than Type 1, but may be less accurate\'))\n\n  ################################################################################\n  # Network Tweak\n  ################################################################################\n  flags.DEFINE_boolean(\n    name=\'use_resnet_d\', default=False,\n    help=flags_core.help_wrap(\'Use resnet_d architecture. \'\n                              \'For more details, refer to https://arxiv.org/abs/1812.01187\'))\n  flags.DEFINE_boolean(\n    name=\'use_se_block\', default=False,\n    help=flags_core.help_wrap(\'Use SE block. \'\n                              \'For more details, refer to https://arxiv.org/abs/1709.01507\'))\n  flags.DEFINE_boolean(\n    name=\'use_sk_block\', default=False,\n    help=flags_core.help_wrap(\'Use SK block.\'))\n  flags.DEFINE_integer(\n    name=\'anti_alias_filter_size\', default=0,\n    help=flags_core.help_wrap(\'Anti-aliasing filter size, One of 2, 3, 4, 5, 6, 7\'))\n\n  flags.DEFINE_string(\n    name=\'anti_alias_type\', default="""",\n    help=flags_core.help_wrap(\n      \'Specifies auto anti alias type. For example,  ""max,proj,sconv"" is fully anti-alias, \'\n      \'""sconv"" means that only strided conv is applied. \'))\n\n  flags.DEFINE_enum(\n    name=\'resnet_version\', short_name=\'rv\', default=\'1\',\n    enum_values=[\'1\', \'2\'],\n    help=flags_core.help_wrap(\n      \'1 is original ResNet structure.\'\n      \'2 is BigLittleNet structure.\'))\n\n  flags.DEFINE_integer(\n    name=\'bl_alpha\', default=2,\n    help=flags_core.help_wrap(\'\'))\n  flags.DEFINE_integer(\n    name=\'bl_beta\', default=4,\n    help=flags_core.help_wrap(\'\'))\n\n  ################################################################################\n  # Regularization\n  ################################################################################\n  flags.DEFINE_float(\n    name=\'weight_decay\', short_name=\'wd\', default=0.00004,\n    help=flags_core.help_wrap(\n      \'The weight decay on the model weights.\'))\n  flags.DEFINE_boolean(\n    name=\'use_dropblock\', default=False,\n    help=flags_core.help_wrap(\'Use dropblock. \'\n                              \'For more details, refer to https://arxiv.org/abs/1810.12890\'))\n  flags.DEFINE_list(\n    name=\'dropblock_kp\', short_name=\'drblkp\',\n    default=[1.0, 0.9],\n    help=flags_core.help_wrap(\n      \'Initial keep_prob and end keep_prob of dropblock.\'))\n  flags.DEFINE_float(\n    name=\'label_smoothing\', short_name=\'lblsm\', default=0.0,\n    help=flags_core.help_wrap(\n      \'If greater than 0 then smooth the labels.\'))\n  flags.DEFINE_float(\n    name=\'kd_temp\', default=0,\n    help=flags_core.help_wrap(\'Use knowledge distillation.\'))\n\n  ################################################################################\n  # Tricks to Learn the Models\n  ################################################################################\n  flags.DEFINE_integer(\n    name=\'lr_warmup_epochs\', default=0,\n    help=flags_core.help_wrap(\'The number of learning rate warmup epochs. If 0 do not use warmup\'))\n  flags.DEFINE_boolean(\n    name=\'zero_gamma\', default=False,\n    help=flags_core.help_wrap(\n      \'If True, we initialize gamma = 0 for all BN layers that sit at the end of a residual block\'))\n\n  ################################################################################\n  # Others\n  ################################################################################\n  flags.DEFINE_string(\n    name=\'dct_method\', short_name=\'dctm\', default=\'INTEGER_ACCURATE\',\n    help=flags_core.help_wrap(\n      \'An optional `string`. \'\n      \'Defaults to `""""`. \'\n      \'string specifying a hint about the algorithm used for decompression.  \'\n      \'Defaults to """" which maps to a system-specific default.  \'\n      \'Currently valid values are [""INTEGER_FAST"", ""INTEGER_ACCURATE""].  \'\n      \'The hint may be ignored (e.g., the internal jpeg library changes to a version \'\n      \'that does not have that specific option.)\'))\n  flags.DEFINE_float(\n    name=\'use_ranking_loss\', default=None, help=flags_core.help_wrap(\n      \'if use_ranking_loss  > 0 use softmax + ranking loss\'))\n  flags.DEFINE_boolean(\n    name=\'with_drawing_bbox\', default=False,\n    help=flags_core.help_wrap(\'If True, display raw images with bounding box in tensorboard\'))\n  flags.DEFINE_boolean(\n    name=\'use_hyperdash\', default=True,\n    help=flags_core.help_wrap(\'Use hyperdash(https://hyperdash.io/) \'))\n  flags.DEFINE_integer(\n    name=\'num_best_ckpt_to_keep\', short_name=\'nbck\', default=3,\n    help=flags_core.help_wrap(\n      \'The number of best performance checkpoint to keep.\'))\n  flags.DEFINE_boolean(\n    name=\'keep_ckpt_every_eval\', default=True,\n    help=flags_core.help_wrap(\'If True, checkpoints are saved for each evaluation.\'))\n  flags.DEFINE_integer(\n    name=\'keep_checkpoint_max\', default=20,\n    help=flags_core.help_wrap(\'keep checkpoint max.\'))\n  flags.DEFINE_boolean(\n    name=\'eval_only\', default=False,\n    help=flags_core.help_wrap(\'Skip training and only perform evaluation on \'\n                              \'the latest checkpoint.\'))\n  flags.DEFINE_boolean(\n    name=\'training_random_crop\', default=True,\n    help=flags_core.help_wrap(\'Whether to randomly crop training images\'))\n  flags.DEFINE_boolean(\n    name=\'export_only\', default=False,\n    help=flags_core.help_wrap(\'Skip training and evaluations.\'\n                              \'Only export the latest checkpoint.\'))\n  flags.DEFINE_string(\n    name=\'export_decoder_type\', default=\'jpeg\',\n    help=flags_core.help_wrap(\'Specify the image decoder for binary input pb (jpeg | webp)\'))\n  flags.DEFINE_float(\n    name=\'save_checkpoints_epochs\', default=1.0,\n    help=flags_core.help_wrap(\'Save checkpoint every save_checkpoints_epochs\'))\n  flags.DEFINE_float(\n    name=\'ratio_fine_eval\', default=1.0,\n    help=flags_core.help_wrap(\'From `train_epochs` *` ratio_fine_eval`, \'\n                              \'it evaluates every 1 epoch.\'))\n  flags.DEFINE_boolean(\n    name=\'zeroshot_eval\', default=False,\n    help=flags_core.help_wrap(\'zeroshot evaluation\'))\n  flags.DEFINE_string(\n    name=\'label_file\', default=None,\n    help=flags_core.help_wrap(\'use label ids which has been generated by building tfrecord. \'\n                              \'If None, it put the label ids in alphabetical order.\'))\n  flags.DEFINE_string(\n    name=\'train_regex\', default=\'train-*\',\n    help=flags_core.help_wrap(\'use label ids which has been generated by building tfrecord. \'\n                              \'If None, it put the label ids in alphabetical order.\'))\n  flags.DEFINE_string(\n    name=\'val_regex\', default=\'validation-*\',\n    help=flags_core.help_wrap(\'use label ids which has been generated by building tfrecord. \'\n                              \'If None, it put the label ids in alphabetical order.\'))\n  flags.DEFINE_list(\n    name=\'recall_at_k\',\n    default=[1, 5],\n    help=flags_core.help_wrap(\n      \'A list of int that specifies the k values of the recall_at_k\'))\n  \n'"
nets/model_helper.py,8,"b'# coding=utf8\n# ==============================================================================\n# Copyright 2020-present NAVER Corp.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\n\n################################################################################\n# Convenience functions for building the ResNet model.\n################################################################################\ndef batch_norm(inputs, training, data_format, zero_gamma=False, momentum=0.997, epsilon=1e-5, name=None):\n  """"""Performs a batch normalization using a standard set of parameters.""""""\n  # We set fused=True for a significant performance boost. See\n  # https://www.tensorflow.org/performance/performance_guide#common_fused_ops\n  if zero_gamma:\n    gamma_initializer = tf.zeros_initializer()\n  else:\n    gamma_initializer = tf.ones_initializer()\n  return tf.layers.batch_normalization(\n    inputs=inputs, axis=1 if data_format == \'channels_first\' else 3,\n    momentum=momentum, epsilon=epsilon, center=True,\n    scale=True, training=training, fused=True, gamma_initializer=gamma_initializer, name=name)\n\n\ndef fixed_padding(inputs, kernel_size, data_format):\n  """"""Pads the input along the spatial dimensions independently of input size.\n\n  Args:\n    inputs: A tensor of size [batch, channels, height_in, width_in] or\n      [batch, height_in, width_in, channels] depending on data_format.\n    kernel_size: The kernel to be used in the conv2d or max_pool2d operation.\n                 Should be a positive integer.\n    data_format: The input format (\'channels_last\' or \'channels_first\').\n\n  Returns:\n    A tensor with the same format as the input with the data either intact\n    (if kernel_size == 1) or padded (if kernel_size > 1).\n  """"""\n  pad_total = kernel_size - 1\n  pad_beg = pad_total // 2\n  pad_end = pad_total - pad_beg\n\n  if data_format == \'channels_first\':\n    padded_inputs = tf.pad(inputs, [[0, 0], [0, 0],\n                                    [pad_beg, pad_end], [pad_beg, pad_end]])\n  else:\n    padded_inputs = tf.pad(inputs, [[0, 0], [pad_beg, pad_end],\n                                    [pad_beg, pad_end], [0, 0]])\n  return padded_inputs\n\n\ndef conv2d_fixed_padding(inputs, filters, kernel_size, strides, data_format):\n  """"""Strided 2-D convolution with explicit padding.""""""\n  # The padding is consistent and is based only on `kernel_size`, not on the\n  # dimensions of `inputs` (as opposed to using `tf.layers.conv2d` alone).\n  if strides > 1:\n    inputs = fixed_padding(inputs, kernel_size, data_format)\n\n  return tf.layers.conv2d(\n    inputs=inputs, filters=filters, kernel_size=kernel_size, strides=strides,\n    padding=(\'SAME\' if strides == 1 else \'VALID\'), use_bias=False,\n    kernel_initializer=tf.variance_scaling_initializer(),\n    data_format=data_format)\n'"
nets/optimizer_setting.py,3,"b'# coding=utf8\n# ==============================================================================\n# Copyright 2020-present NAVER Corp.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\n\ndef get_train_op(loss,\n                 global_step,\n                 learning_rate,\n                 momentum,\n                 loss_scale,\n                 ):\n  optimizer = tf.train.MomentumOptimizer(learning_rate=learning_rate, momentum=momentum)\n  scaled_grad_vars = optimizer.compute_gradients(loss * loss_scale)\n  if loss_scale != 1:\n    scaled_grad_vars = [(grad / loss_scale, var)\n                        for grad, var in scaled_grad_vars]\n  minimize_op = optimizer.apply_gradients(scaled_grad_vars, global_step)\n\n  update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n  train_op = tf.group(minimize_op, update_ops)\n  return train_op\n'"
nets/resnet_model.py,70,"b'#!/usr/bin/env python\n# coding=utf8\n# This code is adapted from the https://github.com/tensorflow/models/tree/master/official/r1/resnet.\n# ==========================================================================================\n# NAVER\xe2\x80\x99s modifications are Copyright 2020 NAVER corp. All rights reserved.\n# ==========================================================================================\n# Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom nets import blocks\nfrom nets.model_helper import *\nimport numpy as np\n\nDEFAULT_VERSION = 1\nDEFAULT_DTYPE = tf.float32\nCASTABLE_TYPES = (tf.float16,)\nALLOWED_TYPES = (DEFAULT_DTYPE,) + CASTABLE_TYPES\n\ndef _bottleneck_block_v1(inputs, filters, training, projection_shortcut,\n                         strides, data_format, zero_gamma=False,\n                         dropblock_fn=None, se_block_fn=None, use_sk_block=False,\n                         bn_momentum=0.997, anti_alias_filter_size=0, anti_alias_type="""",\n                         last_relu=True, block_expansion=4):\n  shortcut = inputs\n\n  if projection_shortcut is not None:\n    shortcut = projection_shortcut(inputs)\n    shortcut = batch_norm(inputs=shortcut, training=training,\n                          data_format=data_format, momentum=bn_momentum)\n    if dropblock_fn:\n      shortcut = dropblock_fn(shortcut)\n\n  inputs = conv2d_fixed_padding(\n    inputs=inputs, filters=filters, kernel_size=1, strides=1,\n    data_format=data_format)\n  inputs = batch_norm(inputs, training, data_format, momentum=bn_momentum)\n  if dropblock_fn:\n    inputs = dropblock_fn(inputs)\n  inputs = tf.nn.relu(inputs)\n\n  if use_sk_block:\n    inputs = blocks.sk_conv2d(inputs, filters, strides=1 if \'sconv\' in anti_alias_type else strides,\n                              training=training,\n                              data_format=data_format,\n                              bn_momentum=bn_momentum)\n    if dropblock_fn:\n      inputs = dropblock_fn(inputs)\n  else:\n    inputs = conv2d_fixed_padding(\n      inputs=inputs, filters=filters, kernel_size=3, strides=1 if \'sconv\' in anti_alias_type else strides,\n      data_format=data_format)\n\n    inputs = batch_norm(inputs, training, data_format, momentum=bn_momentum)\n    if dropblock_fn:\n      inputs = dropblock_fn(inputs)\n    inputs = tf.nn.relu(inputs)\n\n  if \'sconv\' in anti_alias_type and strides != 1:\n    tf.logging.info(\'Anti-Alias stridedConv2 On\')\n    inputs = blocks.anti_aliased_downsample(inputs,\n                                            data_format=data_format,\n                                            stride=strides,\n                                            filt_size=anti_alias_filter_size)\n\n  inputs = conv2d_fixed_padding(\n    inputs=inputs, filters=block_expansion * filters, kernel_size=1, strides=1,\n    data_format=data_format)\n  inputs = batch_norm(inputs, training, data_format, zero_gamma=zero_gamma, momentum=bn_momentum)\n\n  if dropblock_fn:\n    inputs = dropblock_fn(inputs)\n\n  if se_block_fn:\n    inputs = se_block_fn(inputs)\n\n  inputs += shortcut\n\n  if last_relu:\n    inputs = tf.nn.relu(inputs)\n\n  return inputs\n\ndef block_layer(inputs, filters, bottleneck, block_fn, num_blocks, strides,\n                training, name, data_format, zero_gamma=False, use_resnet_d=False,\n                dropblock_fn=None, se_block_fn=None, use_sk_block=False, bn_momentum=0.997,\n                anti_alias_filter_size=0, anti_alias_type="""", expansion=4,\n                use_bl=False, last_relu=True):\n  # Bottleneck blocks end with 4x the number of filters as they start with\n  filters_out = filters * expansion if bottleneck else filters\n\n  def projection_shortcut(inputs):\n    if \'proj\' in anti_alias_type and strides != 1:\n      tf.logging.info(\'Anti-Alias Projection Conv On\')\n      inputs = blocks.anti_aliased_downsample(inputs,\n                                              data_format=data_format,\n                                              stride=strides,\n                                              filt_size=anti_alias_filter_size)\n      return conv2d_fixed_padding(\n        inputs=inputs, filters=filters_out, kernel_size=1, strides=1,\n        data_format=data_format)\n\n    else:\n      return conv2d_fixed_padding(\n        inputs=inputs, filters=filters_out, kernel_size=1, strides=strides,\n        data_format=data_format)\n\n  def resnet_d_projection_shortcut(inputs):\n    if strides > 1:\n      inputs = fixed_padding(inputs, 2, data_format)\n    inputs = tf.layers.average_pooling2d(inputs, pool_size=2, strides=strides,\n                                         padding=\'SAME\' if strides == 1 else \'VALID\',\n                                         data_format=data_format)\n    return conv2d_fixed_padding(\n      inputs=inputs, filters=filters_out, kernel_size=1, strides=1,\n      data_format=data_format)\n\n  def bl_projection_shortcut(inputs):\n    if strides > 1:\n      inputs = fixed_padding(inputs, 3, data_format)\n      inputs = tf.layers.average_pooling2d(inputs, pool_size=3, strides=strides,\n                                           padding=\'SAME\' if strides == 1 else \'VALID\',\n                                           data_format=data_format)\n    return conv2d_fixed_padding(\n      inputs=inputs, filters=filters_out, kernel_size=1, strides=1,\n      data_format=data_format)\n\n  # Only the first block per block_layer uses projection_shortcut and strides\n  if use_resnet_d:\n    projection_shortcut_fn = resnet_d_projection_shortcut\n  elif use_bl:  # resnet_d\xec\x99\x80 \xec\x9c\xa0\xec\x82\xac\xed\x95\x9c\xeb\x8d\xb0, average pooling\xec\x9d\x98 kernel_size\xeb\xa7\x8c 2\xec\x97\x90\xec\x84\x9c 3\xec\x9c\xbc\xeb\xa1\x9c \xeb\xb0\x94\xeb\x80\x9c,\n    projection_shortcut_fn = bl_projection_shortcut\n  else:\n    projection_shortcut_fn = projection_shortcut\n\n  inputs = block_fn(inputs, filters, training, projection_shortcut_fn, strides,\n                    data_format, zero_gamma=zero_gamma, dropblock_fn=dropblock_fn,\n                    se_block_fn=se_block_fn, use_sk_block=use_sk_block, bn_momentum=bn_momentum,\n                    anti_alias_filter_size=anti_alias_filter_size, anti_alias_type=anti_alias_type,\n                    block_expansion=expansion)\n\n  for i in range(1, num_blocks):\n    inputs = block_fn(inputs, filters, training, None, 1, data_format, zero_gamma,\n                      dropblock_fn=dropblock_fn, se_block_fn=se_block_fn, use_sk_block=use_sk_block,\n                      bn_momentum=bn_momentum, block_expansion=expansion,\n                      last_relu=last_relu if i == num_blocks - 1 else True)\n\n  return tf.identity(inputs, name)\n\n\nclass Model(object):\n  """"""Base class for building the Resnet Model.""""""\n\n  def __init__(self, resnet_size,\n               bottleneck,\n               num_classes,\n               num_filters,\n               kernel_size,\n               conv_stride,\n               first_pool_size,\n               first_pool_stride,\n               block_sizes,\n               block_strides,\n               zero_gamma=False,\n               resnet_version=DEFAULT_VERSION,\n               data_format=None,\n               num_feature=None,\n               use_se_block=False,\n               use_sk_block=False,\n               bn_momentum=0.997,\n               embedding_size=0,\n               anti_alias_filter_size=0,\n               anti_alias_type="""",\n               pool_type=\'gap\',\n               bl_alpha=2,\n               bl_beta=4,\n               loss_type=\'softmax\',\n               dtype=DEFAULT_DTYPE):\n    self.resnet_size = resnet_size\n\n    if not data_format:\n      data_format = (\n        \'channels_first\' if tf.test.is_built_with_cuda() else \'channels_last\')\n\n    self.resnet_version = resnet_version\n    if resnet_version not in (1, 2):\n      raise ValueError(\n        \'Resnet version should be 1 or 2. See README for citations.\')\n\n    self.bottleneck = bottleneck\n    if bottleneck:\n      if resnet_version == 1 or resnet_version == 2:\n        self.block_fn = _bottleneck_block_v1\n      else:\n        raise NotImplementedError\n    else:\n      raise NotImplementedError\n\n    if dtype not in ALLOWED_TYPES:\n      raise ValueError(\'dtype must be one of: {}\'.format(ALLOWED_TYPES))\n\n    self.data_format = data_format\n    self.num_classes = num_classes\n    self.num_filters = num_filters\n    self.kernel_size = kernel_size\n    self.conv_stride = conv_stride\n    self.first_pool_size = first_pool_size\n    self.first_pool_stride = first_pool_stride\n    self.block_sizes = block_sizes\n    self.block_strides = block_strides\n    self.dtype = dtype\n    self.zero_gamma = zero_gamma\n    self.embedding_size = num_feature\n    self.use_se_block = use_se_block\n    self.use_sk_block = use_sk_block\n    self.bn_momentum = bn_momentum\n    self.embedding_size = embedding_size\n    self.anti_alias_filter_size = anti_alias_filter_size\n    self.anti_alias_type = anti_alias_type\n    self.pool_type = pool_type\n    self.alpha = bl_alpha\n    self.beta = bl_beta\n    tf.logging.info(\'[loss type] {}\'.format(loss_type))\n    if loss_type == \'softmax\':\n      self.dense_bias_initializer = tf.zeros_initializer()\n    elif loss_type == \'sigmoid\' or loss_type == \'focal\':\n      tf.logging.info(\'[Dense layer bias initializer: -np.log(num_classes - 1)]\')\n      self.dense_bias_initializer = tf.constant_initializer(-np.log(num_classes - 1))\n    elif loss_type == \'anchor\':\n      tf.logging.info(\'anchor loss warmup epoch: %d\' % (anchor_loss_warmup_epochs))\n      tf.logging.info(\'[Dense layer bias initializer: -np.log(num_classes - 1)]\')\n      self.dense_bias_initializer = tf.constant_initializer(-np.log(num_classes - 1))\n    else:\n      raise NotImplementedError\n\n  def _custom_dtype_getter(self, getter, name, shape=None, dtype=DEFAULT_DTYPE,\n                           *args, **kwargs):\n    """"""Creates variables in fp32, then casts to fp16 if necessary.\n\n    This function is a custom getter. A custom getter is a function with the\n    same signature as tf.get_variable, except it has an additional getter\n    parameter. Custom getters can be passed as the `custom_getter` parameter of\n    tf.variable_scope. Then, tf.get_variable will call the custom getter,\n    instead of directly getting a variable itself. This can be used to change\n    the types of variables that are retrieved with tf.get_variable.\n    The `getter` parameter is the underlying variable getter, that would have\n    been called if no custom getter was used. Custom getters typically get a\n    variable with `getter`, then modify it in some way.\n\n    This custom getter will create an fp32 variable. If a low precision\n    (e.g. float16) variable was requested it will then cast the variable to the\n    requested dtype. The reason we do not directly create variables in low\n    precision dtypes is that applying small gradients to such variables may\n    cause the variable not to change.\n\n    Args:\n      getter: The underlying variable getter, that has the same signature as\n        tf.get_variable and returns a variable.\n      name: The name of the variable to get.\n      shape: The shape of the variable to get.\n      dtype: The dtype of the variable to get. Note that if this is a low\n        precision dtype, the variable will be created as a tf.float32 variable,\n        then cast to the appropriate dtype\n      *args: Additional arguments to pass unmodified to getter.\n      **kwargs: Additional keyword arguments to pass unmodified to getter.\n\n    Returns:\n      A variable which is cast to fp16 if necessary.\n    """"""\n\n    if dtype in CASTABLE_TYPES:\n      var = getter(name, shape, tf.float32, *args, **kwargs)\n      return tf.cast(var, dtype=dtype, name=name + \'_cast\')\n    else:\n      return getter(name, shape, dtype, *args, **kwargs)\n\n  def _model_variable_scope(self, reuse=False):\n    """"""Returns a variable scope that the model should be created under.\n\n    If self.dtype is a castable type, model variable will be created in fp32\n    then cast to self.dtype before being used.\n\n    Returns:\n      A variable scope for the model.\n    """"""\n\n    return tf.variable_scope(\'resnet_model\', reuse=reuse,\n                             custom_getter=self._custom_dtype_getter)\n\n  def __call__(self, inputs,\n               training,\n               reuse=False,\n               use_resnet_d=False,\n               keep_prob=1.0,\n               return_embedding=False):\n    """"""Add operations to classify a batch of input images.\n\n    Args:\n      inputs: A Tensor representing a batch of input images.\n      training: A boolean. Set to True to add operations required only when\n        training the classifier.\n      keep_prob: The keep_prob of Dropblock.\n    Returns:\n      A logits Tensor with shape [<batch_size>, self.num_classes].\n    """"""\n\n    with self._model_variable_scope(reuse):\n      if self.data_format == \'channels_first\':\n        # Convert the inputs from channels_last (NHWC) to channels_first (NCHW).\n        # This provides a large performance boost on GPU. See\n        # https://www.tensorflow.org/performance/performance_guide#data_formats\n        inputs = tf.transpose(inputs, [0, 3, 1, 2])\n      if use_resnet_d and self.resnet_version == 1:\n        inputs = conv2d_fixed_padding(\n          inputs=inputs, filters=self.num_filters // 2, kernel_size=3,\n          strides=self.conv_stride, data_format=self.data_format)\n        inputs = batch_norm(inputs, training, self.data_format, momentum=self.bn_momentum)\n        inputs = tf.nn.relu(inputs)\n        inputs = conv2d_fixed_padding(\n          inputs=inputs, filters=self.num_filters // 2, kernel_size=3,\n          strides=1, data_format=self.data_format)\n        inputs = batch_norm(inputs, training, self.data_format, momentum=self.bn_momentum)\n        inputs = tf.nn.relu(inputs)\n        inputs = conv2d_fixed_padding(\n          inputs=inputs, filters=self.num_filters, kernel_size=3,\n          strides=1, data_format=self.data_format)\n      elif use_resnet_d and self.resnet_version == 2:\n        tf.logging.warn(""use_resnet_d + blresnet may causes models\' predictive ""\n                        ""performance degradation."")\n        with tf.variable_scope(None, \'stage{}\'.format(0)):\n          inputs = conv2d_fixed_padding(\n            inputs=inputs, filters=self.num_filters // 2, kernel_size=3,\n            strides=self.conv_stride, data_format=self.data_format)\n          inputs = batch_norm(inputs, training, self.data_format, momentum=self.bn_momentum)\n          inputs = tf.nn.relu(inputs)\n          inputs = conv2d_fixed_padding(\n            inputs=inputs, filters=self.num_filters // 2, kernel_size=3,\n            strides=1, data_format=self.data_format)\n          inputs = batch_norm(inputs, training, self.data_format, momentum=self.bn_momentum)\n          inputs = tf.nn.relu(inputs)\n          inputs = conv2d_fixed_padding(\n            inputs=inputs, filters=self.num_filters, kernel_size=3,\n            strides=1, data_format=self.data_format)\n      elif self.resnet_version == 2:\n        with tf.variable_scope(None, \'stage{}\'.format(0)):\n          inputs = conv2d_fixed_padding(\n            inputs=inputs, filters=self.num_filters, kernel_size=self.kernel_size,\n            strides=self.conv_stride, data_format=self.data_format)\n      else:\n        inputs = conv2d_fixed_padding(\n          inputs=inputs, filters=self.num_filters, kernel_size=self.kernel_size,\n          strides=self.conv_stride, data_format=self.data_format)\n\n      inputs = tf.identity(inputs, \'initial_conv\')\n\n      # We do not include batch normalization or activation functions in V2\n      # for the initial conv1 because the first ResNet unit will perform these\n      # for both the shortcut and non-shortcut paths as part of the first\n      # block\'s projection. Cf. Appendix of [2].\n      if self.resnet_version == 1:\n        inputs = batch_norm(inputs, training, self.data_format, momentum=self.bn_momentum)\n        inputs = tf.nn.relu(inputs)\n      elif self.resnet_version == 2:\n        with tf.variable_scope(None, \'stage{}\'.format(0)):\n          inputs = batch_norm(inputs, training, self.data_format, momentum=self.bn_momentum)\n          inputs = tf.nn.relu(inputs)\n\n\n      if self.first_pool_size:\n        if self.resnet_version == 2:\n          # conv \xea\xb5\xac\xec\x84\xb1\n          # big, 3x3, 64, s2\n          tf.logging.info(\'blModule0 On\')\n          with tf.variable_scope(None, \'stage{}/pool\'.format(0)):\n            big0 = conv2d_fixed_padding(\n              inputs=inputs, filters=self.num_filters, kernel_size=3,\n              strides=2, data_format=self.data_format)\n            big0 = batch_norm(big0, training, self.data_format, momentum=self.bn_momentum)\n\n            # conv \xea\xb5\xac\xec\x84\xb1\n            # little 3x3, 32\n            # little 3x3, 32, s2\n            # little 1x1, 64\n            little0 = conv2d_fixed_padding(\n              inputs=inputs, filters=self.num_filters // self.alpha, kernel_size=3,\n              strides=1, data_format=self.data_format)\n            little0 = batch_norm(little0, training, self.data_format, momentum=self.bn_momentum)\n            little0 = tf.nn.relu(little0)\n            little0 = conv2d_fixed_padding(\n              inputs=little0, filters=self.num_filters // self.alpha, kernel_size=3,\n              strides=2, data_format=self.data_format)\n            little0 = batch_norm(little0, training, self.data_format, momentum=self.bn_momentum)\n            little0 = tf.nn.relu(little0)\n            little0 = conv2d_fixed_padding(\n              inputs=little0, filters=self.num_filters, kernel_size=1,\n              strides=1, data_format=self.data_format)\n            little0 = batch_norm(little0, training, self.data_format, momentum=self.bn_momentum)\n\n            inputs = tf.nn.relu(big0 + little0)\n            inputs = conv2d_fixed_padding(\n              inputs=inputs, filters=self.num_filters, kernel_size=1,\n              strides=1, data_format=self.data_format)\n            inputs = batch_norm(inputs, training, self.data_format, momentum=self.bn_momentum)\n            inputs = tf.nn.relu(inputs)\n        else:\n          inputs = tf.layers.max_pooling2d(\n            inputs=inputs, pool_size=self.first_pool_size,\n            strides=self.first_pool_stride, padding=\'SAME\',\n            data_format=self.data_format)\n          inputs = tf.identity(inputs, \'initial_max_pool\')\n\n      data_format = self.data_format\n\n      def se_block(inputs):\n        return blocks.se_block(inputs, ratio=16, data_format=data_format)\n\n      def dropblock_for_group3(inputs):\n        return blocks.dropblock(inputs, keep_prob, is_training=training,\n                                block_size=7, gamma_scale=0.25, data_format=data_format)\n\n      def dropblock_for_group4(inputs):\n        return blocks.dropblock(inputs, keep_prob, is_training=training,\n                                block_size=7, gamma_scale=1.0, data_format=data_format)\n\n      if self.use_se_block:\n        se_block_fn = se_block\n      else:\n        se_block_fn = None\n\n      for i, num_blocks in enumerate(self.block_sizes):\n        num_filters = self.num_filters * (2 ** i)\n\n        if i == 2:\n          dropblock_fn = dropblock_for_group3\n        elif i == 3:\n          dropblock_fn = dropblock_for_group4\n        else:\n          dropblock_fn = None\n\n        if self.resnet_version == 2 and i < 3:\n          tf.logging.info(\'blModule{} On\'.format(i + 1))\n          with tf.variable_scope(None, \'stage{}\'.format(i + 1)):\n            with tf.variable_scope(None, \'big{}\'.format(i + 1)):\n              big = block_layer(\n                inputs=inputs, filters=num_filters, bottleneck=self.bottleneck,\n                block_fn=self.block_fn, num_blocks=num_blocks - 1,\n                strides=2, training=training,\n                name=\'big{}\'.format(i + 1),\n                data_format=self.data_format,\n                zero_gamma=self.zero_gamma,\n                dropblock_fn=dropblock_fn,\n                se_block_fn=se_block_fn,\n                use_sk_block=self.use_sk_block,\n                bn_momentum=self.bn_momentum,\n                anti_alias_filter_size=self.anti_alias_filter_size,\n                anti_alias_type=self.anti_alias_type,\n                last_relu=False,\n                use_bl=True\n              )\n\n            with tf.variable_scope(None, \'little{}\'.format(i + 1)):\n              little = block_layer(\n                inputs=inputs, filters=num_filters // self.alpha, bottleneck=self.bottleneck,\n                block_fn=self.block_fn, num_blocks=max(1, num_blocks // self.beta - 1),\n                strides=1, training=training,\n                name=\'little{}\'.format(i + 1),\n                data_format=self.data_format,\n                zero_gamma=self.zero_gamma,\n                dropblock_fn=dropblock_fn,\n                se_block_fn=se_block_fn,\n                use_sk_block=self.use_sk_block,\n                bn_momentum=self.bn_momentum,\n                anti_alias_filter_size=self.anti_alias_filter_size,\n                anti_alias_type=self.anti_alias_type,\n                use_bl=True\n              )\n\n              little_e = conv2d_fixed_padding(\n                inputs=little, filters=num_filters * 4, kernel_size=1,\n                strides=1, data_format=self.data_format)\n              little_e = batch_norm(little_e, training, self.data_format, momentum=self.bn_momentum)\n\n            with tf.variable_scope(None, \'merge{}\'.format(i + 1)):\n              big_e = tf.keras.layers.UpSampling2D((2, 2), data_format=self.data_format)(big)\n\n              inputs = tf.nn.relu(little_e + big_e)\n\n              inputs = block_layer(\n                inputs=inputs, filters=num_filters, bottleneck=self.bottleneck,\n                block_fn=self.block_fn, num_blocks=1,\n                strides=self.block_strides[i], training=training,\n                name=\'merge{}\'.format(i + 1), data_format=self.data_format,\n                zero_gamma=self.zero_gamma,\n                dropblock_fn=dropblock_fn,\n                se_block_fn=se_block_fn,\n                use_sk_block=self.use_sk_block,\n                bn_momentum=self.bn_momentum,\n                anti_alias_filter_size=self.anti_alias_filter_size,\n                anti_alias_type=self.anti_alias_type,\n                use_bl=True\n              )\n            # print(\'x{}\'.format(i + 1), inputs)\n        elif self.resnet_version == 2 and i == 3:\n          with tf.variable_scope(None, \'stage{}\'.format(i + 1)):\n            inputs = block_layer(\n              inputs=inputs, filters=num_filters, bottleneck=self.bottleneck,\n              block_fn=self.block_fn, num_blocks=num_blocks,\n              strides=self.block_strides[i], training=training,\n              name=\'block_layer{}\'.format(i + 1), data_format=self.data_format,\n              zero_gamma=self.zero_gamma,\n              use_resnet_d=use_resnet_d,\n              dropblock_fn=dropblock_fn,\n              se_block_fn=se_block_fn,\n              use_sk_block=self.use_sk_block,\n              bn_momentum=self.bn_momentum,\n              anti_alias_filter_size=self.anti_alias_filter_size,\n              anti_alias_type=self.anti_alias_type,\n              use_bl=True,\n            )\n        else:\n          inputs = block_layer(\n            inputs=inputs, filters=num_filters, bottleneck=self.bottleneck,\n            block_fn=self.block_fn, num_blocks=num_blocks,\n            strides=self.block_strides[i], training=training,\n            name=\'block_layer{}\'.format(i + 1), data_format=self.data_format,\n            zero_gamma=self.zero_gamma,\n            use_resnet_d=use_resnet_d,\n            dropblock_fn=dropblock_fn,\n            se_block_fn=se_block_fn,\n            use_sk_block=self.use_sk_block,\n            bn_momentum=self.bn_momentum,\n            anti_alias_filter_size=self.anti_alias_filter_size,\n            anti_alias_type=self.anti_alias_type,\n          )\n      # The current top layer has shape\n      # `batch_size x pool_size x pool_size x final_size`.\n      # ResNet does an Average Pooling layer over pool_size,\n      # but that is the same as doing a reduce_mean. We do a reduce_mean\n      # here because it performs better than AveragePooling2D.\n      axes = [2, 3] if self.data_format == \'channels_first\' else [1, 2]\n\n      if self.block_strides[-1] == 1:\n        tf.logging.info(\'[No downsample at final stage]\')\n\n      if self.pool_type == \'gap\':\n        inputs = tf.reduce_mean(inputs, axes, keepdims=True)\n      elif self.pool_type == \'gem\':\n        tf.logging.info(\'GeM POOLING ON\')\n        inputs = blocks.generalized_mean_pooling(inputs, data_format=data_format)\n      elif self.pool_type == \'flatten\':\n        tf.logging.info(\'Use FLATTEN for POOLING\')\n        inputs = tf.layers.flatten(inputs)\n        for axis_idx in axes:\n          inputs = tf.expand_dims(inputs, axis=axis_idx)\n      else:\n        raise NotImplementedError\n      global_pool = tf.identity(inputs, \'final_reduce_mean\')\n\n      if self.embedding_size > 0:\n        tf.logging.info(\'[Added embedding layer before final dense layer]\')\n        embeddings = tf.layers.conv2d(\n          inputs=global_pool, filters=self.embedding_size, kernel_size=1, strides=1,\n          padding=\'SAME\', use_bias=False,\n          kernel_initializer=tf.variance_scaling_initializer(),\n          data_format=data_format, name=\'embedding_dense\')\n        \n        embeddings = batch_norm(embeddings, training, self.data_format,\n                                momentum=self.bn_momentum,\n                                name=\'embedding_dense_batch_normalization\')\n        squeezed_global_pool = tf.squeeze(embeddings, axes)\n      else:\n        squeezed_global_pool = tf.squeeze(global_pool, axes)\n\n      if return_embedding:\n        return squeezed_global_pool\n      tf.logging.info(\'[Added final dense layer]\')\n      if self.embedding_size > 0:\n        squeezed_global_pool = tf.nn.relu(squeezed_global_pool)\n\n      final_dense = tf.layers.dense(inputs=squeezed_global_pool,\n                                    units=self.num_classes,\n                                    bias_initializer=self.dense_bias_initializer)\n      final_dense = tf.identity(final_dense, \'final_dense\')\n      return final_dense\n'"
nets/run_loop_classification.py,56,"b'# coding=utf8\n# This code is adapted from the https://github.com/tensorflow/models/tree/master/official/r1/resnet.\n# ==========================================================================================\n# NAVER\xe2\x80\x99s modifications are Copyright 2020 NAVER corp. All rights reserved.\n# ==========================================================================================\n\n# Copyright 2019 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport math\nimport os\n\n# pylint: disable=g-bad-import-order\nfrom absl import flags\n\nfrom metric import ece_metric\nfrom nets import hparams_config\nfrom nets.optimizer_setting import *\nfrom official.utils.flags import core as flags_core\nfrom official.utils.logs import hooks_helper\nfrom official.utils.logs import logger\nfrom official.utils.misc import model_helpers\nfrom utils import checkpoint_utils\nfrom utils import data_util\nfrom utils import export_utils\nfrom utils.hook_utils import *\nfrom utils import config_utils\nfrom metric import recall_metric\nfrom functions import data_config, model_fns, input_fns\nfrom nets import blocks\nfrom losses import cls_losses\n\ntry:\n  from hyperdash import Experiment\n\n  install_hyperdash = True\nexcept:\n  install_hyperdash = False\n\n\n# pylint: enable=g-bad-import-order\n################################################################################\n# Functions for running training/eval/validation loops for the model.\n################################################################################\ndef resnet_model_fn(features, labels, num_classes, mode, model_class,\n                    learning_rate_fn, keep_prob_fn, loss_filter_fn, p):\n  global_step = tf.train.get_or_create_global_step()\n\n  if keep_prob_fn:\n    keep_prob = keep_prob_fn(global_step)\n  else:\n    keep_prob = 1.0\n\n  model = model_class(p[\'resnet_size\'], p[\'data_format\'],\n                      num_classes=num_classes,\n                      resnet_version=p[\'resnet_version\'],\n                      zero_gamma=p[\'zero_gamma\'],\n                      use_se_block=p[\'use_se_block\'],\n                      use_sk_block=p[\'use_sk_block\'],\n                      no_downsample=p[\'no_downsample\'],\n                      anti_alias_filter_size=p[\'anti_alias_filter_size\'],\n                      anti_alias_type=p[\'anti_alias_type\'],\n                      bn_momentum=p[\'bn_momentum\'],\n                      embedding_size=p[\'embedding_size\'],\n                      pool_type=p[\'pool_type\'],\n                      bl_alpha=p[\'bl_alpha\'],\n                      bl_beta=p[\'bl_beta\'],\n                      dtype=p[\'dtype\'],\n                      loss_type=p[\'cls_loss_type\'])\n\n  is_training = (mode == tf.estimator.ModeKeys.TRAIN)\n\n  if mode != tf.estimator.ModeKeys.PREDICT:\n    features_sup_may_mixuped = features[""image""]\n    if p[\'kd_temp\'] > 0:\n      # The kd label contains the supervised label and the teacher label.\n      # And unlike usual, it comes in one-hot form.\n      # Split this and apply a temperatured softmax to the teacher label.\n      onehot_labels, teacher_logits = tf.split(labels, 2, axis=1)\n      teacher_labels = tf.nn.softmax(teacher_logits / p[\'kd_temp\'])\n      labels = tf.argmax(onehot_labels, axis=1)\n    else:\n      onehot_labels = tf.one_hot(labels, model.num_classes)\n      teacher_labels = None\n\n    if p[\'mixup_type\'] == 1 and mode == tf.estimator.ModeKeys.TRAIN:\n      tf.logging.info(\'MIXUP on: {}\'.format(p[\'mixup_type\']))\n      features_sup_may_mixuped, onehot_labels, teacher_labels = data_util.mixup(features_sup_may_mixuped, onehot_labels,\n                                                                                keep_batch_size=False,\n                                                                                y_t=teacher_labels)\n    elif p[\'mixup_type\'] == 2 and mode == tf.estimator.ModeKeys.TRAIN:\n      tf.logging.info(\'MIXUP on: {}\'.format(p[\'mixup_type\']))\n      features_sup_may_mixuped, onehot_labels, teacher_labels = data_util.mixup(features_sup_may_mixuped, onehot_labels,\n                                                                                y_t=teacher_labels)\n  else:\n    onehot_labels = None\n    features_sup_may_mixuped = features\n\n  tf.summary.image(\'image\', features_sup_may_mixuped, max_outputs=3)\n  all_images = features_sup_may_mixuped\n\n  assert all_images.dtype == p[\'dtype\']\n\n  sup_bsz = tf.shape(features_sup_may_mixuped)[0]\n\n  all_logits = model(all_images, mode == tf.estimator.ModeKeys.TRAIN, False,\n                     use_resnet_d=p[\'use_resnet_d\'], keep_prob=keep_prob)\n  all_logits = tf.cast(all_logits, tf.float32)\n  logits = all_logits[:sup_bsz]\n\n  predictions = {\n    \'classes\': tf.argmax(logits, axis=1),\n    \'probabilities\': tf.nn.softmax(logits, name=\'softmax_tensor\'),\n    \'probabilities_sigmoid\': tf.nn.sigmoid(logits, name=\'sigmoid_tensor\')\n  }\n\n  if mode == tf.estimator.ModeKeys.PREDICT:\n    # Return the predictions and the specification for serving a SavedModel\n    return tf.estimator.EstimatorSpec(\n      mode=mode,\n      predictions=predictions,\n      export_outputs={\n        \'predict\': tf.estimator.export.PredictOutput(predictions)\n      })\n\n  ###########################\n  # Calculate cross entropy.\n  ###########################\n  cross_entropy = cls_losses.get_sup_loss(logits, onehot_labels, global_step, num_classes, p)\n  # Create a tensor named cross_entropy for logging purposes.\n  tf.identity(cross_entropy, name=\'cross_entropy\')\n  tf.summary.scalar(\'cross_entropy\', cross_entropy)\n\n  sup_prob = tf.nn.softmax(logits, axis=-1)\n  tf.summary.scalar(""sup/pred_prob"", tf.reduce_mean(tf.reduce_max(sup_prob, axis=-1)))\n\n\n  ##############################\n  # Knowledge Distillation loss\n  ##############################\n  if p[\'kd_temp\'] > 0:\n    cross_entropy_kd = p[\'kd_temp\'] * p[\'kd_temp\'] * tf.losses.softmax_cross_entropy(\n      logits=logits / p[\'kd_temp\'], onehot_labels=teacher_labels)\n    tf.identity(cross_entropy_kd, name=\'cross_entropy_kd\')\n    tf.summary.scalar(\'cross_entropy_kd\', cross_entropy_kd)\n  else:\n    cross_entropy_kd = 0\n\n  # If no loss_filter_fn is passed, assume we want the default behavior,\n  # which is that batch_normalization variables are excluded from loss.\n  def exclude_batch_norm(name):\n    return \'batch_normalization\' not in name\n\n  loss_filter_fn = loss_filter_fn or exclude_batch_norm\n\n  ###############################\n  # Add weight decay to the loss.\n  ###############################\n  l2_loss = p[\'weight_decay\'] * tf.add_n(\n    # loss is computed using fp32 for numerical stability.\n    [tf.nn.l2_loss(tf.cast(v, tf.float32)) for v in tf.trainable_variables()\n     if loss_filter_fn(v.name)])\n  tf.summary.scalar(\'l2_loss\', l2_loss)\n  loss = cross_entropy + l2_loss + cross_entropy_kd\n\n  if mode == tf.estimator.ModeKeys.TRAIN:\n\n    learning_rate = learning_rate_fn(global_step)\n\n    # Create a tensor named learning_rate and keep_prob for logging purposes\n    tf.identity(learning_rate, name=\'learning_rate\')\n    tf.summary.scalar(\'learning_rate\', learning_rate)\n\n    tf.identity(keep_prob, name=\'dropblock_kp\')\n    tf.summary.scalar(\'dropblock_kp\', keep_prob)\n\n    train_op = get_train_op(loss=loss,\n                            global_step=global_step,\n                            learning_rate=learning_rate,\n                            momentum=p[\'momentum\'],\n                            loss_scale=p[\'loss_scale\'],\n                            )\n  else:\n    train_op = None\n\n  if p[\'mixup_type\'] > 0 and mode == tf.estimator.ModeKeys.TRAIN:\n    # mixup \xec\x8b\x9c\xec\x97\x90\xeb\x8a\x94 \xeb\xa0\x88\xec\x9d\xb4\xeb\xb8\x94 \xeb\x91\x90\xea\xb0\x9c \xec\x84\x9e\xec\x97\xac\xec\x84\x9c \xec\xa0\x95\xeb\x8b\xb5\xec\x9d\xb4 \xeb\x90\x98\xea\xb8\xb0 \xeb\x95\x8c\xeb\xac\xb8\xec\x97\x90 train accuracy\xeb\xa5\xbc \xea\xb5\xac\xed\x95\xa0 \xec\x88\x98 \xec\x97\x86\xeb\x8b\xa4.\n    metrics = None\n    tf.identity(0, name=\'train_accuracy\')\n    tf.identity(0, name=\'train_accuracy_top_5\')\n    tf.identity(0, name=\'train_ece\')\n  else:\n    accuracy = tf.metrics.accuracy(labels, predictions[\'classes\'])\n\n    accuracy_top_5 = tf.metrics.mean(tf.nn.in_top_k(predictions=logits,\n                                                    targets=labels,\n                                                    k=5,\n                                                    name=\'top_5_op\'))\n    conf = tf.reduce_max(predictions[\'probabilities\'], axis=1)\n    ece = ece_metric.ece(conf, predictions[\'classes\'], labels)\n    metrics = {\'accuracy\': accuracy,\n               \'accuracy_top_5\': accuracy_top_5,\n               \'ece\': ece,\n               }\n\n    # Create a tensor named train_accuracy for logging purposes\n    tf.identity(accuracy[1], name=\'train_accuracy\')\n    tf.identity(accuracy_top_5[1], name=\'train_accuracy_top_5\')\n    tf.identity(ece[1], name=\'train_ece\')\n    tf.summary.scalar(\'train_accuracy\', accuracy[1])\n    tf.summary.scalar(\'train_accuracy_top_5\', accuracy_top_5[1])\n    tf.summary.scalar(\'train_ece\', ece[1])\n\n  return tf.estimator.EstimatorSpec(\n    mode=mode,\n    predictions=predictions,\n    loss=loss,\n    train_op=train_op,\n    eval_metric_ops=metrics)\n\n\ndef resnet_main(flags_obj,\n                model_function,\n                input_function,\n                dataset_name,\n                shape=None,\n                num_images=None,\n                zeroshot_eval=False):\n  model_helpers.apply_clean(flags.FLAGS)\n\n  # Using the Winograd non-fused algorithms provides a small performance boost.\n  os.environ[\'TF_ENABLE_WINOGRAD_NONFUSED\'] = \'1\'\n\n  # Create session config based on values of inter_op_parallelism_threads and\n  # intra_op_parallelism_threads. Note that we default to having\n  # allow_soft_placement = True, which is required for multi-GPU and not\n  # harmful for other modes.\n  session_config = config_utils.get_session_config(flags_obj)\n  run_config = config_utils.get_run_config(flags_obj, flags_core, session_config, num_images[\'train\'])\n\n  def gen_estimator(period=None):\n    resnet_size = int(flags_obj.resnet_size)\n    data_format = flags_obj.data_format\n    batch_size = flags_obj.batch_size\n    resnet_version = int(flags_obj.resnet_version)\n    loss_scale = flags_core.get_loss_scale(flags_obj)\n    dtype_tf = flags_core.get_tf_dtype(flags_obj)\n    num_epochs_per_decay = flags_obj.num_epochs_per_decay\n    learning_rate_decay_factor = flags_obj.learning_rate_decay_factor\n    end_learning_rate = flags_obj.end_learning_rate\n    learning_rate_decay_type = flags_obj.learning_rate_decay_type\n    weight_decay = flags_obj.weight_decay\n    zero_gamma = flags_obj.zero_gamma\n    lr_warmup_epochs = flags_obj.lr_warmup_epochs\n    base_learning_rate = flags_obj.base_learning_rate\n    use_resnet_d = flags_obj.use_resnet_d\n    use_dropblock = flags_obj.use_dropblock\n    dropblock_kp = [float(be) for be in flags_obj.dropblock_kp]\n    label_smoothing = flags_obj.label_smoothing\n    momentum = flags_obj.momentum\n    bn_momentum = flags_obj.bn_momentum\n    train_epochs = flags_obj.train_epochs\n    piecewise_lr_boundary_epochs = [int(be) for be in flags_obj.piecewise_lr_boundary_epochs]\n    piecewise_lr_decay_rates = [float(dr) for dr in flags_obj.piecewise_lr_decay_rates]\n    use_ranking_loss = flags_obj.use_ranking_loss\n    use_se_block = flags_obj.use_se_block\n    use_sk_block = flags_obj.use_sk_block\n    mixup_type = flags_obj.mixup_type\n    dataset_name = flags_obj.dataset_name\n    kd_temp = flags_obj.kd_temp\n    no_downsample = flags_obj.no_downsample\n    anti_alias_filter_size = flags_obj.anti_alias_filter_size\n    anti_alias_type = flags_obj.anti_alias_type\n    cls_loss_type = flags_obj.cls_loss_type\n    logit_type = flags_obj.logit_type\n    embedding_size = flags_obj.embedding_size\n    pool_type = flags_obj.pool_type\n    arc_s = flags_obj.arc_s\n    arc_m = flags_obj.arc_m\n    bl_alpha = flags_obj.bl_alpha\n    bl_beta = flags_obj.bl_beta\n    exp = None\n\n    if install_hyperdash and flags_obj.use_hyperdash:\n      exp = Experiment(flags_obj.model_dir.split(""/"")[-1])\n      resnet_size = exp.param(""resnet_size"", int(flags_obj.resnet_size))\n      batch_size = exp.param(""batch_size"", flags_obj.batch_size)\n      exp.param(""dtype"", flags_obj.dtype)\n      learning_rate_decay_type = exp.param(""learning_rate_decay_type"", flags_obj.learning_rate_decay_type)\n      weight_decay = exp.param(""weight_decay"", flags_obj.weight_decay)\n      zero_gamma = exp.param(""zero_gamma"", flags_obj.zero_gamma)\n      lr_warmup_epochs = exp.param(""lr_warmup_epochs"", flags_obj.lr_warmup_epochs)\n      base_learning_rate = exp.param(""base_learning_rate"", flags_obj.base_learning_rate)\n      use_dropblock = exp.param(""use_dropblock"", flags_obj.use_dropblock)\n      dropblock_kp = exp.param(""dropblock_kp"", [float(be) for be in flags_obj.dropblock_kp])\n      piecewise_lr_boundary_epochs = exp.param(""piecewise_lr_boundary_epochs"",\n                                               [int(be) for be in flags_obj.piecewise_lr_boundary_epochs])\n      piecewise_lr_decay_rates = exp.param(""piecewise_lr_decay_rates"",\n                                           [float(dr) for dr in flags_obj.piecewise_lr_decay_rates])\n      mixup_type = exp.param(""mixup_type"", flags_obj.mixup_type)\n      dataset_name = exp.param(""dataset_name"", flags_obj.dataset_name)\n      exp.param(""autoaugment_type"", flags_obj.autoaugment_type)\n\n    classifier = tf.estimator.Estimator(\n      model_fn=model_function, model_dir=flags_obj.model_dir,\n      config=run_config,\n      params={\n        \'resnet_size\': resnet_size,\n        \'data_format\': data_format,\n        \'batch_size\': batch_size,\n        \'resnet_version\': resnet_version,\n        \'loss_scale\': loss_scale,\n        \'dtype\': dtype_tf,\n        \'num_epochs_per_decay\': num_epochs_per_decay,\n        \'learning_rate_decay_factor\': learning_rate_decay_factor,\n        \'end_learning_rate\': end_learning_rate,\n        \'learning_rate_decay_type\': learning_rate_decay_type,\n        \'weight_decay\': weight_decay,\n        \'zero_gamma\': zero_gamma,\n        \'lr_warmup_epochs\': lr_warmup_epochs,\n        \'base_learning_rate\': base_learning_rate,\n        \'use_resnet_d\': use_resnet_d,\n        \'use_dropblock\': use_dropblock,\n        \'dropblock_kp\': dropblock_kp,\n        \'label_smoothing\': label_smoothing,\n        \'momentum\': momentum,\n        \'bn_momentum\': bn_momentum,\n        \'embedding_size\': embedding_size,\n        \'train_epochs\': train_epochs,\n        \'piecewise_lr_boundary_epochs\': piecewise_lr_boundary_epochs,\n        \'piecewise_lr_decay_rates\': piecewise_lr_decay_rates,\n        \'with_drawing_bbox\': flags_obj.with_drawing_bbox,\n        \'use_ranking_loss\': use_ranking_loss,\n        \'use_se_block\': use_se_block,\n        \'use_sk_block\': use_sk_block,\n        \'mixup_type\': mixup_type,\n        \'kd_temp\': kd_temp,\n        \'no_downsample\': no_downsample,\n        \'dataset_name\': dataset_name,\n        \'anti_alias_filter_size\': anti_alias_filter_size,\n        \'anti_alias_type\': anti_alias_type,\n        \'cls_loss_type\': cls_loss_type,\n        \'logit_type\': logit_type,\n        \'arc_s\': arc_s,\n        \'arc_m\': arc_m,\n        \'pool_type\': pool_type,\n        \'bl_alpha\': bl_alpha,\n        \'bl_beta\': bl_beta,\n        \'train_steps\': total_train_steps,\n\n      })\n    return classifier, exp\n\n  run_params = {\n    \'batch_size\': flags_obj.batch_size,\n    \'dtype\': flags_core.get_tf_dtype(flags_obj),\n    \'resnet_size\': flags_obj.resnet_size,\n    \'resnet_version\': flags_obj.resnet_version,\n    \'synthetic_data\': flags_obj.use_synthetic_data,\n    \'train_epochs\': flags_obj.train_epochs,\n  }\n  if flags_obj.use_synthetic_data:\n    dataset_name = dataset_name + \'-synthetic\'\n\n  benchmark_logger = logger.get_benchmark_logger()\n  benchmark_logger.log_run_info(\'resnet\', dataset_name, run_params,\n                                test_id=flags_obj.benchmark_test_id)\n\n  train_hooks = hooks_helper.get_train_hooks(\n    flags_obj.hooks,\n    model_dir=flags_obj.model_dir,\n    batch_size=flags_obj.batch_size)\n\n  def input_fn_train(num_epochs):\n    return input_function(\n      is_training=True,\n      use_random_crop=flags_obj.training_random_crop,\n      num_epochs=num_epochs,\n      flags_obj=flags_obj\n    )\n\n  def input_fn_eval():\n    return input_function(\n      is_training=False,\n      use_random_crop=False,\n      num_epochs=1,\n      flags_obj=flags_obj)\n\n  ckpt_keeper = checkpoint_utils.CheckpointKeeper(\n    save_dir=flags_obj.model_dir,\n    num_to_keep=flags_obj.num_best_ckpt_to_keep,\n    keep_epoch=flags_obj.keep_ckpt_every_eval,\n    maximize=True\n  )\n\n  if zeroshot_eval:\n    dataset = data_config.get_config(dataset_name)\n    model = model_fns.Model(int(flags_obj.resnet_size),\n                            flags_obj.data_format,\n                            resnet_version=int(flags_obj.resnet_version),\n                            num_classes=dataset.num_classes,\n                            zero_gamma=flags_obj.zero_gamma,\n                            use_se_block=flags_obj.use_se_block,\n                            use_sk_block=flags_obj.use_sk_block,\n                            no_downsample=flags_obj.no_downsample,\n                            anti_alias_filter_size=flags_obj.anti_alias_filter_size,\n                            anti_alias_type=flags_obj.anti_alias_type,\n                            bn_momentum=flags_obj.bn_momentum,\n                            embedding_size=flags_obj.embedding_size,\n                            pool_type=flags_obj.pool_type,\n                            bl_alpha=flags_obj.bl_alpha,\n                            bl_beta=flags_obj.bl_beta,\n                            dtype=flags_core.get_tf_dtype(flags_obj),\n                            loss_type=flags_obj.cls_loss_type)\n  def train_and_evaluate(hooks):\n    tf.logging.info(\'Starting cycle: %d/%d\', cycle_index, int(n_loops))\n\n    if num_train_epochs:\n      classifier.train(input_fn=lambda: input_fn_train(num_train_epochs), hooks=hooks,\n                       steps=flags_obj.max_train_steps)\n\n    tf.logging.info(\'Starting to evaluate.\')\n\n    if zeroshot_eval:\n      tf.reset_default_graph()\n      eval_results = recall_metric.recall_at_k(flags_obj, flags_core,\n                                               input_fns.input_fn_ir_eval, model,\n                                               num_images[\'validation\'],\n                                               eval_similarity=flags_obj.eval_similarity,\n                                               return_embedding=True)\n    else:\n      eval_results = classifier.evaluate(input_fn=input_fn_eval,\n                                         steps=flags_obj.max_train_steps)\n\n\n    return eval_results\n\n  total_train_steps = flags_obj.train_epochs * int(num_images[\'train\'] / flags_obj.batch_size)\n\n  if flags_obj.eval_only or not flags_obj.train_epochs:\n    # If --eval_only is set, perform a single loop with zero train epochs.\n    schedule, n_loops = [0], 1\n  elif flags_obj.export_only:\n    schedule, n_loops = [], 0\n  else:\n    n_loops = math.ceil(flags_obj.train_epochs / flags_obj.epochs_between_evals)\n    schedule = [flags_obj.epochs_between_evals for _ in range(int(n_loops))]\n    schedule[-1] = flags_obj.train_epochs - sum(schedule[:-1])  # over counting.\n\n    schedule = config_utils.get_epoch_schedule(flags_obj, schedule, num_images)\n    tf.logging.info(\'epoch schedule:\')\n    tf.logging.info(schedule)\n\n  classifier, exp = gen_estimator()\n  if flags_obj.pretrained_model_checkpoint_path:\n    warm_start_hook = WarmStartHook(flags_obj.pretrained_model_checkpoint_path)\n    train_hooks.append(warm_start_hook)\n\n  for cycle_index, num_train_epochs in enumerate(schedule):\n    eval_results = train_and_evaluate(train_hooks)\n    if zeroshot_eval:\n      metric = eval_results[\'recall_at_1\']\n    else:\n      metric = eval_results[\'accuracy\']\n    ckpt_keeper.save(metric, flags_obj.model_dir)\n    if exp:\n      exp.metric(""accuracy"", metric)\n    benchmark_logger.log_evaluation_result(eval_results)\n    if model_helpers.past_stop_threshold(\n            flags_obj.stop_threshold, metric):\n      break\n    if model_helpers.past_stop_threshold(\n            total_train_steps, eval_results[\'global_step\']):\n      break\n\n  if exp:\n    exp.end()\n\n  if flags_obj.export_dir is not None:\n    export_utils.export_pb(flags_core, flags_obj, shape, classifier)\n\ndef define_resnet_flags(resnet_size_choices=None):\n  """"""Add flags and validators for ResNet.""""""\n  flags_core.define_base()\n  flags_core.define_performance(num_parallel_calls=False)\n  flags_core.define_image()\n  flags_core.define_benchmark()\n  flags.adopt_module_key_flags(flags_core)\n\n  hparams_config.define_common_flags(flags)\n\n  choice_kwargs = dict(\n    name=\'resnet_size\', short_name=\'rs\', default=\'50\',\n    help=flags_core.help_wrap(\'The size of the ResNet model to use.\'))\n\n  if resnet_size_choices is None:\n    flags.DEFINE_string(**choice_kwargs)\n  else:\n    flags.DEFINE_enum(enum_values=resnet_size_choices, **choice_kwargs)\n'"
official/__init__.py,0,b''
preprocessing/__init__.py,0,b''
preprocessing/autoaugment.py,98,"b'# This code is adapted from the  https://github.com/tensorflow/tpu/blob/master/models/official/efficientnet/autoaugment.py\n# ==========================================================================================\n# NAVER\xe2\x80\x99s modifications are Copyright 2020 NAVER corp. All rights reserved.\n# ==========================================================================================\n\n# Copyright 2019 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport inspect\nimport math\nimport tensorflow as tf\n\n_MAX_LEVEL = 10.\n\n\ndef cifar10_policies():\n  exp0_0 = [\n    [(\'Invert\', 0.1, 7), (\'Contrast\', 0.2, 6)],\n    [(\'Rotate\', 0.7, 2), (\'TranslateX\', 0.3, 9)],\n    [(\'Sharpness\', 0.8, 1), (\'Sharpness\', 0.9, 3)],\n    [(\'ShearY\', 0.5, 8), (\'TranslateY\', 0.7, 9)],\n    [(\'AutoContrast\', 0.5, 8), (\'Equalize\', 0.9, 2)]]\n\n  exp1_0 = [\n    [(\'ShearY\', 0.2, 7), (\'Posterize\', 0.3, 7)],\n    [(\'Color\', 0.4, 3), (\'Brightness\', 0.6, 7)],\n    [(\'Sharpness\', 0.3, 9), (\'Brightness\', 0.7, 9)],\n    [(\'Equalize\', 0.6, 5), (\'Equalize\', 0.5, 1)],\n    [(\'Contrast\', 0.6, 7), (\'Sharpness\', 0.6, 5)]]\n\n  exp2_0 = [\n    [(\'Color\', 0.7, 7), (\'TranslateX\', 0.5, 8)],\n    [(\'Equalize\', 0.3, 7), (\'AutoContrast\', 0.4, 8)],\n    [(\'TranslateY\', 0.4, 3), (\'Sharpness\', 0.2, 6)],\n    [(\'Brightness\', 0.9, 6), (\'Color\', 0.2, 8)],\n    [(\'Solarize\', 0.5, 2), (\'Invert\', 0.0, 3)]]\n\n  exp3_0 = [\n    [(\'Equalize\', 0.2, 0), (\'AutoContrast\', 0.6, 0)],\n    [(\'Equalize\', 0.2, 8), (\'Equalize\', 0.8, 4)],\n    [(\'Color\', 0.9, 9), (\'Equalize\', 0.6, 6)],\n    [(\'AutoContrast\', 0.8, 4), (\'Solarize\', 0.2, 8)],\n    [(\'Brightness\', 0.1, 3), (\'Color\', 0.7, 0)]]\n\n  exp4_0 = [\n    [(\'Solarize\', 0.4, 5), (\'AutoContrast\', 0.9, 3)],\n    [(\'TranslateY\', 0.9, 9), (\'TranslateY\', 0.7, 9)],\n    [(\'AutoContrast\', 0.9, 2), (\'Solarize\', 0.8, 3)],\n    [(\'Equalize\', 0.8, 8), (\'Invert\', 0.1, 3)],\n    [(\'TranslateY\', 0.7, 9), (\'AutoContrast\', 0.9, 1)]]\n\n  return exp0_0 + exp1_0 + exp2_0 + exp3_0 + exp4_0\n\n\ndef imagenet_policies():\n  exp0_0 = [\n    [(\'Posterize\', 0.4, 8), (\'Rotate\', 0.6, 9)],\n    [(\'Solarize\', 0.6, 5), (\'AutoContrast\', 0.6, 5)],\n    [(\'Equalize\', 0.8, 8), (\'Equalize\', 0.6, 3)],\n    [(\'Posterize\', 0.6, 7), (\'Posterize\', 0.6, 6)],\n    [(\'Equalize\', 0.4, 7), (\'Solarize\', 0.2, 4)]]\n\n  exp1_0 = [\n    [(\'Equalize\', 0.4, 4), (\'Rotate\', 0.8, 8)],\n    [(\'Solarize\', 0.6, 3), (\'Equalize\', 0.6, 7)],\n    [(\'Posterize\', 0.8, 5), (\'Equalize\', 1.0, 2)],\n    [(\'Rotate\', 0.2, 3), (\'Solarize\', 0.6, 8)],\n    [(\'Equalize\', 0.6, 8), (\'Posterize\', 0.4, 6)]]\n\n  exp2_0 = [\n    [(\'Rotate\', 0.8, 8), (\'Color\', 0.4, 0)],\n    [(\'Rotate\', 0.4, 9), (\'Equalize\', 0.6, 2)],\n    [(\'Equalize\', 0.0, 7), (\'Equalize\', 0.8, 8)],\n    [(\'Invert\', 0.6, 4), (\'Equalize\', 1.0, 8)],\n    [(\'Color\', 0.6, 4), (\'Contrast\', 1.0, 8)]]\n\n  exp3_0 = [\n    [(\'Rotate\', 0.8, 8), (\'Color\', 1.0, 2)],\n    [(\'Color\', 0.8, 8), (\'Solarize\', 0.8, 7)],\n    [(\'Sharpness\', 0.4, 7), (\'Invert\', 0.6, 8)],\n    [(\'ShearX\', 0.6, 5), (\'Equalize\', 1.0, 9)],\n    [(\'Color\', 0.4, 0), (\'Equalize\', 0.6, 3)]]\n\n  exp4_0 = [\n    [(\'Equalize\', 0.4, 7), (\'Solarize\', 0.2, 4)],\n    [(\'Solarize\', 0.6, 5), (\'AutoContrast\', 0.6, 5)],\n    [(\'Invert\', 0.6, 4), (\'Equalize\', 1.0, 8)],\n    [(\'Color\', 0.6, 4), (\'Contrast\', 1.0, 8)],\n    [(\'Equalize\', 0.8, 8), (\'Equalize\', 0.6, 3)]]\n\n  return exp0_0 + exp1_0 + exp2_0 + exp3_0 + exp4_0\n\n\ndef SVHN_policies():\n  exp0_0 = [\n    [(\'ShearX\', 0.9, 4), (\'Invert\', 0.2, 3)],\n    [(\'ShearY\', 0.9, 8), (\'Invert\', 0.7, 5)],\n    [(\'Equalize\', 0.6, 5), (\'Solarize\', 0.6, 6)],\n    [(\'Invert\', 0.9, 3), (\'Equalize\', 0.6, 3)],\n    [(\'Equalize\', 0.6, 1), (\'Rotate\', 0.9, 3)]]\n\n  exp1_0 = [\n    [(\'ShearX\', 0.9, 4), (\'AutoContrast\', 0.8, 3)],\n    [(\'ShearY\', 0.9, 8), (\'Invert\', 0.4, 5)],\n    [(\'ShearY\', 0.9, 5), (\'Solarize\', 0.2, 6)],\n    [(\'Invert\', 0.9, 6), (\'AutoContrast\', 0.8, 1)],\n    [(\'Equalize\', 0.6, 3), (\'Rotate\', 0.9, 3)]]\n\n  exp2_0 = [\n    [(\'ShearX\', 0.9, 4), (\'Solarize\', 0.3, 3)],\n    [(\'ShearY\', 0.8, 8), (\'Invert\', 0.7, 4)],\n    [(\'Equalize\', 0.9, 5), (\'TranslateY\', 0.6, 6)],\n    [(\'Invert\', 0.9, 4), (\'Equalize\', 0.6, 7)],\n    [(\'contrast\', 0.3, 3), (\'Rotate\', 0.8, 4)]]\n\n  exp3_0 = [\n    [(\'Invert\', 0.8, 5), (\'TranslateY\', 0.0, 2)],\n    [(\'ShearY\', 0.7, 6), (\'Solarize\', 0.4, 8)],\n    [(\'Invert\', 0.6, 4), (\'Rotate\', 0.8, 4)],\n    [(\'ShearY\', 0.3, 7), (\'TranslateX\', 0.9, 3)],\n    [(\'ShearX\', 0.1, 6), (\'Invert\', 0.6, 5)]]\n\n  exp4_0 = [\n    [(\'Solarize\', 0.7, 2), (\'TranslateY\', 0.6, 7)],\n    [(\'ShearY\', 0.8, 4), (\'Invert\', 0.8, 8)],\n    [(\'ShearX\', 0.7, 9), (\'TranslateY\', 0.8, 3)],\n    [(\'ShearY\', 0.8, 5), (\'AutoContrast\', 0.7, 3)],\n    [(\'ShearX\', 0.7, 2), (\'Invert\', 0.1, 5)]]\n\n  return exp0_0 + exp1_0 + exp2_0 + exp3_0 + exp4_0\n\n\ndef good_policies():\n  """"""AutoAugment policies found on Cifar.""""""\n  exp0_0 = [\n    [(\'Invert\', 0.1, 7), (\'Contrast\', 0.2, 6)],\n    [(\'Rotate\', 0.7, 2), (\'TranslateX\', 0.3, 9)],\n    [(\'Sharpness\', 0.8, 1), (\'Sharpness\', 0.9, 3)],\n    [(\'ShearY\', 0.5, 8), (\'TranslateY\', 0.7, 9)],\n    [(\'AutoContrast\', 0.5, 8), (\'Equalize\', 0.9, 2)]]\n  exp0_1 = [\n    [(\'Solarize\', 0.4, 5), (\'AutoContrast\', 0.9, 3)],\n    [(\'TranslateY\', 0.9, 9), (\'TranslateY\', 0.7, 9)],\n    [(\'AutoContrast\', 0.9, 2), (\'Solarize\', 0.8, 3)],\n    [(\'Equalize\', 0.8, 8), (\'Invert\', 0.1, 3)],\n    [(\'TranslateY\', 0.7, 9), (\'AutoContrast\', 0.9, 1)]]\n  exp0_2 = [\n    [(\'Solarize\', 0.4, 5), (\'AutoContrast\', 0.0, 2)],\n    [(\'TranslateY\', 0.7, 9), (\'TranslateY\', 0.7, 9)],\n    [(\'AutoContrast\', 0.9, 0), (\'Solarize\', 0.4, 3)],\n    [(\'Equalize\', 0.7, 5), (\'Invert\', 0.1, 3)],\n    [(\'TranslateY\', 0.7, 9), (\'TranslateY\', 0.7, 9)]]\n  exp0_3 = [\n    [(\'Solarize\', 0.4, 5), (\'AutoContrast\', 0.9, 1)],\n    [(\'TranslateY\', 0.8, 9), (\'TranslateY\', 0.9, 9)],\n    [(\'AutoContrast\', 0.8, 0), (\'TranslateY\', 0.7, 9)],\n    [(\'TranslateY\', 0.2, 7), (\'Color\', 0.9, 6)],\n    [(\'Equalize\', 0.7, 6), (\'Color\', 0.4, 9)]]\n  exp1_0 = [\n    [(\'ShearY\', 0.2, 7), (\'Posterize\', 0.3, 7)],\n    [(\'Color\', 0.4, 3), (\'Brightness\', 0.6, 7)],\n    [(\'Sharpness\', 0.3, 9), (\'Brightness\', 0.7, 9)],\n    [(\'Equalize\', 0.6, 5), (\'Equalize\', 0.5, 1)],\n    [(\'Contrast\', 0.6, 7), (\'Sharpness\', 0.6, 5)]]\n  exp1_1 = [\n    [(\'Brightness\', 0.3, 7), (\'AutoContrast\', 0.5, 8)],\n    [(\'AutoContrast\', 0.9, 4), (\'AutoContrast\', 0.5, 6)],\n    [(\'Solarize\', 0.3, 5), (\'Equalize\', 0.6, 5)],\n    [(\'TranslateY\', 0.2, 4), (\'Sharpness\', 0.3, 3)],\n    [(\'Brightness\', 0.0, 8), (\'Color\', 0.8, 8)]]\n  exp1_2 = [\n    [(\'Solarize\', 0.2, 6), (\'Color\', 0.8, 6)],\n    [(\'Solarize\', 0.2, 6), (\'AutoContrast\', 0.8, 1)],\n    [(\'Solarize\', 0.4, 1), (\'Equalize\', 0.6, 5)],\n    [(\'Brightness\', 0.0, 0), (\'Solarize\', 0.5, 2)],\n    [(\'AutoContrast\', 0.9, 5), (\'Brightness\', 0.5, 3)]]\n  exp1_3 = [\n    [(\'Contrast\', 0.7, 5), (\'Brightness\', 0.0, 2)],\n    [(\'Solarize\', 0.2, 8), (\'Solarize\', 0.1, 5)],\n    [(\'Contrast\', 0.5, 1), (\'TranslateY\', 0.2, 9)],\n    [(\'AutoContrast\', 0.6, 5), (\'TranslateY\', 0.0, 9)],\n    [(\'AutoContrast\', 0.9, 4), (\'Equalize\', 0.8, 4)]]\n  exp1_4 = [\n    [(\'Brightness\', 0.0, 7), (\'Equalize\', 0.4, 7)],\n    [(\'Solarize\', 0.2, 5), (\'Equalize\', 0.7, 5)],\n    [(\'Equalize\', 0.6, 8), (\'Color\', 0.6, 2)],\n    [(\'Color\', 0.3, 7), (\'Color\', 0.2, 4)],\n    [(\'AutoContrast\', 0.5, 2), (\'Solarize\', 0.7, 2)]]\n  exp1_5 = [\n    [(\'AutoContrast\', 0.2, 0), (\'Equalize\', 0.1, 0)],\n    [(\'ShearY\', 0.6, 5), (\'Equalize\', 0.6, 5)],\n    [(\'Brightness\', 0.9, 3), (\'AutoContrast\', 0.4, 1)],\n    [(\'Equalize\', 0.8, 8), (\'Equalize\', 0.7, 7)],\n    [(\'Equalize\', 0.7, 7), (\'Solarize\', 0.5, 0)]]\n  exp1_6 = [\n    [(\'Equalize\', 0.8, 4), (\'TranslateY\', 0.8, 9)],\n    [(\'TranslateY\', 0.8, 9), (\'TranslateY\', 0.6, 9)],\n    [(\'TranslateY\', 0.9, 0), (\'TranslateY\', 0.5, 9)],\n    [(\'AutoContrast\', 0.5, 3), (\'Solarize\', 0.3, 4)],\n    [(\'Solarize\', 0.5, 3), (\'Equalize\', 0.4, 4)]]\n  exp2_0 = [\n    [(\'Color\', 0.7, 7), (\'TranslateX\', 0.5, 8)],\n    [(\'Equalize\', 0.3, 7), (\'AutoContrast\', 0.4, 8)],\n    [(\'TranslateY\', 0.4, 3), (\'Sharpness\', 0.2, 6)],\n    [(\'Brightness\', 0.9, 6), (\'Color\', 0.2, 8)],\n    [(\'Solarize\', 0.5, 2), (\'Invert\', 0.0, 3)]]\n  exp2_1 = [\n    [(\'AutoContrast\', 0.1, 5), (\'Brightness\', 0.0, 0)],\n    [(\'Cutout\', 0.2, 4), (\'Equalize\', 0.1, 1)],\n    [(\'Equalize\', 0.7, 7), (\'AutoContrast\', 0.6, 4)],\n    [(\'Color\', 0.1, 8), (\'ShearY\', 0.2, 3)],\n    [(\'ShearY\', 0.4, 2), (\'Rotate\', 0.7, 0)]]\n  exp2_2 = [\n    [(\'ShearY\', 0.1, 3), (\'AutoContrast\', 0.9, 5)],\n    [(\'TranslateY\', 0.3, 6), (\'Cutout\', 0.3, 3)],\n    [(\'Equalize\', 0.5, 0), (\'Solarize\', 0.6, 6)],\n    [(\'AutoContrast\', 0.3, 5), (\'Rotate\', 0.2, 7)],\n    [(\'Equalize\', 0.8, 2), (\'Invert\', 0.4, 0)]]\n  exp2_3 = [\n    [(\'Equalize\', 0.9, 5), (\'Color\', 0.7, 0)],\n    [(\'Equalize\', 0.1, 1), (\'ShearY\', 0.1, 3)],\n    [(\'AutoContrast\', 0.7, 3), (\'Equalize\', 0.7, 0)],\n    [(\'Brightness\', 0.5, 1), (\'Contrast\', 0.1, 7)],\n    [(\'Contrast\', 0.1, 4), (\'Solarize\', 0.6, 5)]]\n  exp2_4 = [\n    [(\'Solarize\', 0.2, 3), (\'ShearX\', 0.0, 0)],\n    [(\'TranslateX\', 0.3, 0), (\'TranslateX\', 0.6, 0)],\n    [(\'Equalize\', 0.5, 9), (\'TranslateY\', 0.6, 7)],\n    [(\'ShearX\', 0.1, 0), (\'Sharpness\', 0.5, 1)],\n    [(\'Equalize\', 0.8, 6), (\'Invert\', 0.3, 6)]]\n  exp2_5 = [\n    [(\'AutoContrast\', 0.3, 9), (\'Cutout\', 0.5, 3)],\n    [(\'ShearX\', 0.4, 4), (\'AutoContrast\', 0.9, 2)],\n    [(\'ShearX\', 0.0, 3), (\'Posterize\', 0.0, 3)],\n    [(\'Solarize\', 0.4, 3), (\'Color\', 0.2, 4)],\n    [(\'Equalize\', 0.1, 4), (\'Equalize\', 0.7, 6)]]\n  exp2_6 = [\n    [(\'Equalize\', 0.3, 8), (\'AutoContrast\', 0.4, 3)],\n    [(\'Solarize\', 0.6, 4), (\'AutoContrast\', 0.7, 6)],\n    [(\'AutoContrast\', 0.2, 9), (\'Brightness\', 0.4, 8)],\n    [(\'Equalize\', 0.1, 0), (\'Equalize\', 0.0, 6)],\n    [(\'Equalize\', 0.8, 4), (\'Equalize\', 0.0, 4)]]\n  exp2_7 = [\n    [(\'Equalize\', 0.5, 5), (\'AutoContrast\', 0.1, 2)],\n    [(\'Solarize\', 0.5, 5), (\'AutoContrast\', 0.9, 5)],\n    [(\'AutoContrast\', 0.6, 1), (\'AutoContrast\', 0.7, 8)],\n    [(\'Equalize\', 0.2, 0), (\'AutoContrast\', 0.1, 2)],\n    [(\'Equalize\', 0.6, 9), (\'Equalize\', 0.4, 4)]]\n  exp0s = exp0_0 + exp0_1 + exp0_2 + exp0_3\n  exp1s = exp1_0 + exp1_1 + exp1_2 + exp1_3 + exp1_4 + exp1_5 + exp1_6\n  exp2s = exp2_0 + exp2_1 + exp2_2 + exp2_3 + exp2_4 + exp2_5 + exp2_6 + exp2_7\n  return exp0s + exp1s + exp2s\n\n\ndef policy_v0():\n  """"""Autoaugment policy that was used in AutoAugment Paper.""""""\n  # Each tuple is an augmentation operation of the form\n  # (operation, probability, magnitude). Each element in policy is a\n  # sub-policy that will be applied sequentially on the image.\n  policy = [\n    [(\'Equalize\', 0.8, 1), (\'ShearY\', 0.8, 4)],\n    [(\'Color\', 0.4, 9), (\'Equalize\', 0.6, 3)],\n    [(\'Color\', 0.4, 1), (\'Rotate\', 0.6, 8)],\n    [(\'Solarize\', 0.8, 3), (\'Equalize\', 0.4, 7)],\n    [(\'Solarize\', 0.4, 2), (\'Solarize\', 0.6, 2)],\n    [(\'Color\', 0.2, 0), (\'Equalize\', 0.8, 8)],\n    [(\'Equalize\', 0.4, 8), (\'SolarizeAdd\', 0.8, 3)],\n    [(\'ShearX\', 0.2, 9), (\'Rotate\', 0.6, 8)],\n    [(\'Color\', 0.6, 1), (\'Equalize\', 1.0, 2)],\n    [(\'Invert\', 0.4, 9), (\'Rotate\', 0.6, 0)],\n    [(\'Equalize\', 1.0, 9), (\'ShearY\', 0.6, 3)],\n    [(\'Color\', 0.4, 7), (\'Equalize\', 0.6, 0)],\n    [(\'Posterize\', 0.4, 6), (\'AutoContrast\', 0.4, 7)],\n    [(\'Solarize\', 0.6, 8), (\'Color\', 0.6, 9)],\n    [(\'Solarize\', 0.2, 4), (\'Rotate\', 0.8, 9)],\n    [(\'Rotate\', 1.0, 7), (\'TranslateY\', 0.8, 9)],\n    [(\'ShearX\', 0.0, 0), (\'Solarize\', 0.8, 4)],\n    [(\'ShearY\', 0.8, 0), (\'Color\', 0.6, 4)],\n    [(\'Color\', 1.0, 0), (\'Rotate\', 0.6, 2)],\n    [(\'Equalize\', 0.8, 4), (\'Equalize\', 0.0, 8)],\n    [(\'Equalize\', 1.0, 4), (\'AutoContrast\', 0.6, 2)],\n    [(\'ShearY\', 0.4, 7), (\'SolarizeAdd\', 0.6, 7)],\n    [(\'Posterize\', 0.8, 2), (\'Solarize\', 0.6, 10)],\n    [(\'Solarize\', 0.6, 8), (\'Equalize\', 0.6, 1)],\n    [(\'Color\', 0.8, 6), (\'Rotate\', 0.4, 5)],\n  ]\n  return policy\n\n\ndef policy_vtest():\n  """"""Autoaugment test policy for debugging.""""""\n  # Each tuple is an augmentation operation of the form\n  # (operation, probability, magnitude). Each element in policy is a\n  # sub-policy that will be applied sequentially on the image.\n  policy = [\n    [(\'TranslateX\', 1.0, 4), (\'Equalize\', 1.0, 10)],\n  ]\n  return policy\n\n\ndef blend(image1, image2, factor):\n  """"""Blend image1 and image2 using \'factor\'.\n\n  Factor can be above 0.0.  A value of 0.0 means only image1 is used.\n  A value of 1.0 means only image2 is used.  A value between 0.0 and\n  1.0 means we linearly interpolate the pixel values between the two\n  images.  A value greater than 1.0 ""extrapolates"" the difference\n  between the two pixel values, and we clip the results to values\n  between 0 and 255.\n\n  Args:\n    image1: An image Tensor of type uint8.\n    image2: An image Tensor of type uint8.\n    factor: A floating point value above 0.0.\n\n  Returns:\n    A blended image Tensor of type uint8.\n  """"""\n  if factor == 0.0:\n    return tf.convert_to_tensor(image1)\n  if factor == 1.0:\n    return tf.convert_to_tensor(image2)\n\n  image1 = tf.to_float(image1)\n  image2 = tf.to_float(image2)\n\n  difference = image2 - image1\n  scaled = factor * difference\n\n  # Do addition in float.\n  temp = tf.to_float(image1) + scaled\n\n  # Interpolate\n  if factor > 0.0 and factor < 1.0:\n    # Interpolation means we always stay within 0 and 255.\n    return tf.cast(temp, tf.uint8)\n\n  # Extrapolate:\n  #\n  # We need to clip and then cast.\n  return tf.cast(tf.clip_by_value(temp, 0.0, 255.0), tf.uint8)\n\n\ndef cutout(image, pad_size, replace=0):\n  """"""Apply cutout (https://arxiv.org/abs/1708.04552) to image.\n\n  This operation applies a (2*pad_size x 2*pad_size) mask of zeros to\n  a random location within `img`. The pixel values filled in will be of the\n  value `replace`. The located where the mask will be applied is randomly\n  chosen uniformly over the whole image.\n\n  Args:\n    image: An image Tensor of type uint8.\n    pad_size: Specifies how big the zero mask that will be generated is that\n      is applied to the image. The mask will be of size\n      (2*pad_size x 2*pad_size).\n    replace: What pixel value to fill in the image in the area that has\n      the cutout mask applied to it.\n\n  Returns:\n    An image Tensor that is of type uint8.\n  """"""\n  image_height = tf.shape(image)[0]\n  image_width = tf.shape(image)[1]\n\n  # Sample the center location in the image where the zero mask will be applied.\n  cutout_center_height = tf.random_uniform(\n    shape=[], minval=0, maxval=image_height,\n    dtype=tf.int32)\n\n  cutout_center_width = tf.random_uniform(\n    shape=[], minval=0, maxval=image_width,\n    dtype=tf.int32)\n\n  lower_pad = tf.maximum(0, cutout_center_height - pad_size)\n  upper_pad = tf.maximum(0, image_height - cutout_center_height - pad_size)\n  left_pad = tf.maximum(0, cutout_center_width - pad_size)\n  right_pad = tf.maximum(0, image_width - cutout_center_width - pad_size)\n\n  cutout_shape = [image_height - (lower_pad + upper_pad),\n                  image_width - (left_pad + right_pad)]\n  padding_dims = [[lower_pad, upper_pad], [left_pad, right_pad]]\n  mask = tf.pad(\n    tf.zeros(cutout_shape, dtype=image.dtype),\n    padding_dims, constant_values=1)\n  mask = tf.expand_dims(mask, -1)\n  mask = tf.tile(mask, [1, 1, 3])\n  image = tf.where(\n    tf.equal(mask, 0),\n    tf.ones_like(image, dtype=image.dtype) * replace,\n    image)\n  return image\n\n\ndef solarize(image, threshold=128):\n  # For each pixel in the image, select the pixel\n  # if the value is less than the threshold.\n  # Otherwise, subtract 255 from the pixel.\n  return tf.where(image < threshold, image, 255 - image)\n\n\ndef solarize_add(image, addition=0, threshold=128):\n  # For each pixel in the image less than threshold\n  # we add \'addition\' amount to it and then clip the\n  # pixel value to be between 0 and 255. The value\n  # of \'addition\' is between -128 and 128.\n  added_image = tf.cast(image, tf.int64) + addition\n  added_image = tf.cast(tf.clip_by_value(added_image, 0, 255), tf.uint8)\n  return tf.where(image < threshold, added_image, image)\n\n\ndef color(image, factor):\n  """"""Equivalent of PIL Color.""""""\n  degenerate = tf.image.grayscale_to_rgb(tf.image.rgb_to_grayscale(image))\n  return blend(degenerate, image, factor)\n\n\ndef contrast(image, factor):\n  """"""Equivalent of PIL Contrast.""""""\n  degenerate = tf.image.rgb_to_grayscale(image)\n  # Cast before calling tf.histogram.\n  degenerate = tf.cast(degenerate, tf.int32)\n\n  # Compute the grayscale histogram, then compute the mean pixel value,\n  # and create a constant image size of that value.  Use that as the\n  # blending degenerate target of the original image.\n  hist = tf.histogram_fixed_width(degenerate, [0, 255], nbins=256)\n  mean = tf.reduce_sum(tf.cast(hist, tf.float32)) / 256.0\n  degenerate = tf.ones_like(degenerate, dtype=tf.float32) * mean\n  degenerate = tf.clip_by_value(degenerate, 0.0, 255.0)\n  degenerate = tf.image.grayscale_to_rgb(tf.cast(degenerate, tf.uint8))\n  return blend(degenerate, image, factor)\n\n\ndef brightness(image, factor):\n  """"""Equivalent of PIL Brightness.""""""\n  degenerate = tf.zeros_like(image)\n  return blend(degenerate, image, factor)\n\n\ndef posterize(image, bits):\n  """"""Equivalent of PIL Posterize.""""""\n  shift = 8 - bits\n  return tf.bitwise.left_shift(tf.bitwise.right_shift(image, shift), shift)\n\n\ndef rotate(image, degrees, replace):\n  """"""Rotates the image by degrees either clockwise or counterclockwise.\n\n  Args:\n    image: An image Tensor of type uint8.\n    degrees: Float, a scalar angle in degrees to rotate all images by. If\n      degrees is positive the image will be rotated clockwise otherwise it will\n      be rotated counterclockwise.\n    replace: A one or three value 1D tensor to fill empty pixels caused by\n      the rotate operation.\n\n  Returns:\n    The rotated version of image.\n  """"""\n  # Convert from degrees to radians.\n  degrees_to_radians = math.pi / 180.0\n  radians = degrees * degrees_to_radians\n\n  # In practice, we should randomize the rotation degrees by flipping\n  # it negatively half the time, but that\'s done on \'degrees\' outside\n  # of the function.\n  image = tf.contrib.image.rotate(wrap(image), radians)\n  return unwrap(image, replace)\n\n\ndef translate_x(image, pixels, replace):\n  """"""Equivalent of PIL Translate in X dimension.""""""\n  image = tf.contrib.image.translate(wrap(image), [-pixels, 0])\n  return unwrap(image, replace)\n\n\ndef translate_y(image, pixels, replace):\n  """"""Equivalent of PIL Translate in Y dimension.""""""\n  image = tf.contrib.image.translate(wrap(image), [0, -pixels])\n  return unwrap(image, replace)\n\n\ndef shear_x(image, level, replace):\n  """"""Equivalent of PIL Shearing in X dimension.""""""\n  # Shear parallel to x axis is a projective transform\n  # with a matrix form of:\n  # [1  level\n  #  0  1].\n  image = tf.contrib.image.transform(\n    wrap(image), [1., level, 0., 0., 1., 0., 0., 0.])\n  return unwrap(image, replace)\n\n\ndef shear_y(image, level, replace):\n  """"""Equivalent of PIL Shearing in Y dimension.""""""\n  # Shear parallel to y axis is a projective transform\n  # with a matrix form of:\n  # [1  0\n  #  level  1].\n  image = tf.contrib.image.transform(\n    wrap(image), [1., 0., 0., level, 1., 0., 0., 0.])\n  return unwrap(image, replace)\n\n\ndef autocontrast(image):\n  """"""Implements Autocontrast function from PIL using TF ops.\n\n  Args:\n    image: A 3D uint8 tensor.\n\n  Returns:\n    The image after it has had autocontrast applied to it and will be of type\n    uint8.\n  """"""\n\n  def scale_channel(image):\n    """"""Scale the 2D image using the autocontrast rule.""""""\n    # A possibly cheaper version can be done using cumsum/unique_with_counts\n    # over the histogram values, rather than iterating over the entire image.\n    # to compute mins and maxes.\n    lo = tf.to_float(tf.reduce_min(image))\n    hi = tf.to_float(tf.reduce_max(image))\n\n    # Scale the image, making the lowest value 0 and the highest value 255.\n    def scale_values(im):\n      scale = 255.0 / (hi - lo)\n      offset = -lo * scale\n      im = tf.to_float(im) * scale + offset\n      im = tf.clip_by_value(im, 0.0, 255.0)\n      return tf.cast(im, tf.uint8)\n\n    result = tf.cond(hi > lo, lambda: scale_values(image), lambda: image)\n    return result\n\n  # Assumes RGB for now.  Scales each channel independently\n  # and then stacks the result.\n  s1 = scale_channel(image[:, :, 0])\n  s2 = scale_channel(image[:, :, 1])\n  s3 = scale_channel(image[:, :, 2])\n  image = tf.stack([s1, s2, s3], 2)\n  return image\n\n\ndef sharpness(image, factor):\n  """"""Implements Sharpness function from PIL using TF ops.""""""\n  orig_image = image\n  image = tf.cast(image, tf.float32)\n  # Make image 4D for conv operation.\n  image = tf.expand_dims(image, 0)\n  # SMOOTH PIL Kernel.\n  kernel = tf.constant(\n    [[1, 1, 1], [1, 5, 1], [1, 1, 1]], dtype=tf.float32,\n    shape=[3, 3, 1, 1]) / 13.\n  # Tile across channel dimension.\n  kernel = tf.tile(kernel, [1, 1, 3, 1])\n  strides = [1, 1, 1, 1]\n  degenerate = tf.nn.depthwise_conv2d(\n    image, kernel, strides, padding=\'VALID\', rate=[1, 1])\n  degenerate = tf.clip_by_value(degenerate, 0.0, 255.0)\n  degenerate = tf.squeeze(tf.cast(degenerate, tf.uint8), [0])\n\n  # For the borders of the resulting image, fill in the values of the\n  # original image.\n  mask = tf.ones_like(degenerate)\n  padded_mask = tf.pad(mask, [[1, 1], [1, 1], [0, 0]])\n  padded_degenerate = tf.pad(degenerate, [[1, 1], [1, 1], [0, 0]])\n  result = tf.where(tf.equal(padded_mask, 1), padded_degenerate, orig_image)\n\n  # Blend the final result.\n  return blend(result, orig_image, factor)\n\n\ndef equalize(image):\n  """"""Implements Equalize function from PIL using TF ops.""""""\n\n  def scale_channel(im, c):\n    """"""Scale the data in the channel to implement equalize.""""""\n    im = tf.cast(im[:, :, c], tf.int32)\n    # Compute the histogram of the image channel.\n    histo = tf.histogram_fixed_width(im, [0, 255], nbins=256)\n\n    # For the purposes of computing the step, filter out the nonzeros.\n    nonzero = tf.where(tf.not_equal(histo, 0))\n    nonzero_histo = tf.reshape(tf.gather(histo, nonzero), [-1])\n    step = (tf.reduce_sum(nonzero_histo) - nonzero_histo[-1]) // 255\n\n    def build_lut(histo, step):\n      # Compute the cumulative sum, shifting by step // 2\n      # and then normalization by step.\n      lut = (tf.cumsum(histo) + (step // 2)) // step\n      # Shift lut, prepending with 0.\n      lut = tf.concat([[0], lut[:-1]], 0)\n      # Clip the counts to be in range.  This is done\n      # in the C code for image.point.\n      return tf.clip_by_value(lut, 0, 255)\n\n    # If step is zero, return the original image.  Otherwise, build\n    # lut from the full histogram and step and then index from it.\n    result = tf.cond(tf.equal(step, 0),\n                     lambda: im,\n                     lambda: tf.gather(build_lut(histo, step), im))\n\n    return tf.cast(result, tf.uint8)\n\n  # Assumes RGB for now.  Scales each channel independently\n  # and then stacks the result.\n  s1 = scale_channel(image, 0)\n  s2 = scale_channel(image, 1)\n  s3 = scale_channel(image, 2)\n  image = tf.stack([s1, s2, s3], 2)\n  return image\n\n\ndef invert(image):\n  """"""Inverts the image pixels.""""""\n  image = tf.convert_to_tensor(image)\n  return 255 - image\n\n\ndef wrap(image):\n  """"""Returns \'image\' with an extra channel set to all 1s.""""""\n  shape = tf.shape(image)\n  extended_channel = tf.ones([shape[0], shape[1], 1], image.dtype)\n  extended = tf.concat([image, extended_channel], 2)\n  return extended\n\n\ndef unwrap(image, replace):\n  """"""Unwraps an image produced by wrap.\n\n  Where there is a 0 in the last channel for every spatial position,\n  the rest of the three channels in that spatial dimension are grayed\n  (set to 128).  Operations like translate and shear on a wrapped\n  Tensor will leave 0s in empty locations.  Some transformations look\n  at the intensity of values to do preprocessing, and we want these\n  empty pixels to assume the \'average\' value, rather than pure black.\n\n\n  Args:\n    image: A 3D Image Tensor with 4 channels.\n    replace: A one or three value 1D tensor to fill empty pixels.\n\n  Returns:\n    image: A 3D image Tensor with 3 channels.\n  """"""\n  image_shape = tf.shape(image)\n  # Flatten the spatial dimensions.\n  flattened_image = tf.reshape(image, [-1, image_shape[2]])\n\n  # Find all pixels where the last channel is zero.\n  alpha_channel = flattened_image[:, 3]\n\n  replace = tf.concat([replace, tf.ones([1], image.dtype)], 0)\n\n  # Where they are zero, fill them in with \'replace\'.\n  flattened_image = tf.where(\n    tf.equal(alpha_channel, 0),\n    tf.ones_like(flattened_image, dtype=image.dtype) * replace,\n    flattened_image)\n\n  image = tf.reshape(flattened_image, image_shape)\n  image = tf.slice(image, [0, 0, 0], [image_shape[0], image_shape[1], 3])\n  return image\n\n\nNAME_TO_FUNC = {\n  \'AutoContrast\': autocontrast,\n  \'Equalize\': equalize,\n  \'Invert\': invert,\n  \'Rotate\': rotate,\n  \'Posterize\': posterize,\n  \'Solarize\': solarize,\n  \'SolarizeAdd\': solarize_add,\n  \'Color\': color,\n  \'Contrast\': contrast,\n  \'Brightness\': brightness,\n  \'Sharpness\': sharpness,\n  \'ShearX\': shear_x,\n  \'ShearY\': shear_y,\n  \'TranslateX\': translate_x,\n  \'TranslateY\': translate_y,\n  \'Cutout\': cutout,\n}\n\n\ndef _randomly_negate_tensor(tensor):\n  """"""With 50% prob turn the tensor negative.""""""\n  should_flip = tf.cast(tf.floor(tf.random_uniform([]) + 0.5), tf.bool)\n  final_tensor = tf.cond(should_flip, lambda: tensor, lambda: -tensor)\n  return final_tensor\n\n\ndef _rotate_level_to_arg(level):\n  level = (level / _MAX_LEVEL) * 30.\n  level = _randomly_negate_tensor(level)\n  return (level,)\n\n\ndef _shrink_level_to_arg(level):\n  """"""Converts level to ratio by which we shrink the image content.""""""\n  if level == 0:\n    return (1.0,)  # if level is zero, do not shrink the image\n  # Maximum shrinking ratio is 2.9.\n  level = 2. / (_MAX_LEVEL / level) + 0.9\n  return (level,)\n\n\ndef _enhance_level_to_arg(level):\n  return ((level / _MAX_LEVEL) * 1.8 + 0.1,)\n\n\ndef _shear_level_to_arg(level):\n  level = (level / _MAX_LEVEL) * 0.3\n  # Flip level to negative with 50% chance.\n  level = _randomly_negate_tensor(level)\n  return (level,)\n\n\ndef _translate_level_to_arg(level, translate_const):\n  level = (level / _MAX_LEVEL) * float(translate_const)\n  # Flip level to negative with 50% chance.\n  level = _randomly_negate_tensor(level)\n  return (level,)\n\n\ndef level_to_arg(hparams):\n  return {\n    \'AutoContrast\': lambda level: (),\n    \'Equalize\': lambda level: (),\n    \'Invert\': lambda level: (),\n    \'Rotate\': _rotate_level_to_arg,\n    \'Posterize\': lambda level: (int((level / _MAX_LEVEL) * 4),),\n    \'Solarize\': lambda level: (int((level / _MAX_LEVEL) * 256),),\n    \'SolarizeAdd\': lambda level: (int((level / _MAX_LEVEL) * 110),),\n    \'Color\': _enhance_level_to_arg,\n    \'Contrast\': _enhance_level_to_arg,\n    \'Brightness\': _enhance_level_to_arg,\n    \'Sharpness\': _enhance_level_to_arg,\n    \'ShearX\': _shear_level_to_arg,\n    \'ShearY\': _shear_level_to_arg,\n    \'Cutout\': lambda level: (int((level / _MAX_LEVEL) * hparams.cutout_const),),\n    # pylint:disable=g-long-lambda\n    \'TranslateX\': lambda level: _translate_level_to_arg(\n      level, hparams.translate_const),\n    \'TranslateY\': lambda level: _translate_level_to_arg(\n      level, hparams.translate_const),\n    # pylint:enable=g-long-lambda\n  }\n\n\ndef _parse_policy_info(name, prob, level, replace_value, augmentation_hparams):\n  """"""Return the function that corresponds to `name` and update `level` param.""""""\n  func = NAME_TO_FUNC[name]\n  args = level_to_arg(augmentation_hparams)[name](level)\n\n  # Check to see if prob is passed into function. This is used for operations\n  # where we alter bboxes independently.\n  # pytype:disable=wrong-arg-types\n  if \'prob\' in inspect.getargspec(func)[0]:\n    args = tuple([prob] + list(args))\n  # pytype:enable=wrong-arg-types\n\n  # Add in replace arg if it is required for the function that is being called.\n  if \'replace\' in inspect.getargspec(func)[0]:\n    # Make sure replace is the final argument\n    assert \'replace\' == inspect.getargspec(func)[0][-1]\n    args = tuple(list(args) + [replace_value])\n\n  return (func, prob, args)\n\n\ndef _apply_func_with_prob(func, image, args, prob):\n  """"""Apply `func` to image w/ `args` as input with probability `prob`.""""""\n  assert isinstance(args, tuple)\n\n  # If prob is a function argument, then this randomness is being handled\n  # inside the function, so make sure it is always called.\n  if \'prob\' in inspect.getargspec(func)[0]:\n    prob = 1.0\n\n  # Apply the function with probability `prob`.\n  should_apply_op = tf.cast(\n    tf.floor(tf.random_uniform([], dtype=tf.float32) + prob), tf.bool)\n  augmented_image = tf.cond(\n    should_apply_op,\n    lambda: func(image, *args),\n    lambda: image)\n  return augmented_image\n\n\ndef select_and_apply_random_policy(policies, image):\n  """"""Select a random policy from `policies` and apply it to `image`.""""""\n  policy_to_select = tf.random_uniform([], maxval=len(policies), dtype=tf.int32)\n  # Note that using tf.case instead of tf.conds would result in significantly\n  # larger graphs and would even break export for some larger policies.\n  for (i, policy) in enumerate(policies):\n    image = tf.cond(\n      tf.equal(i, policy_to_select),\n      lambda selected_policy=policy: selected_policy(image),\n      lambda: image)\n  return image\n\n\ndef build_and_apply_nas_policy(policies, image,\n                               augmentation_hparams):\n  """"""Build a policy from the given policies passed in and apply to image.\n\n  Args:\n    policies: list of lists of tuples in the form `(func, prob, level)`, `func`\n      is a string name of the augmentation function, `prob` is the probability\n      of applying the `func` operation, `level` is the input argument for\n      `func`.\n    image: tf.Tensor that the resulting policy will be applied to.\n    augmentation_hparams: Hparams associated with the NAS learned policy.\n\n  Returns:\n    A version of image that now has data augmentation applied to it based on\n    the `policies` pass into the function.\n  """"""\n  replace_value = [128, 128, 128]\n\n  # func is the string name of the augmentation function, prob is the\n  # probability of applying the operation and level is the parameter associated\n  # with the tf op.\n\n  # tf_policies are functions that take in an image and return an augmented\n  # image.\n  tf_policies = []\n  for policy in policies:\n    tf_policy = []\n    # Link string name to the correct python function and make sure the correct\n    # argument is passed into that function.\n    for policy_info in policy:\n      policy_info = list(policy_info) + [replace_value, augmentation_hparams]\n\n      tf_policy.append(_parse_policy_info(*policy_info))\n\n    # Now build the tf policy that will apply the augmentation procedue\n    # on image.\n    def make_final_policy(tf_policy_):\n      def final_policy(image_):\n        for func, prob, args in tf_policy_:\n          image_ = _apply_func_with_prob(\n            func, image_, args, prob)\n        return image_\n\n      return final_policy\n\n    tf_policies.append(make_final_policy(tf_policy))\n\n  augmented_image = select_and_apply_random_policy(\n    tf_policies, image)\n  return augmented_image\n\n\ndef distort_image_with_autoaugment(image, augmentation_name):\n  """"""Applies the AutoAugment policy to `image`.\n\n  AutoAugment is from the paper: https://arxiv.org/abs/1805.09501.\n\n  Args:\n    image: `Tensor` of shape [height, width, 3] representing an image.\n    augmentation_name: The name of the AutoAugment policy to use. The available\n      options are `v0` and `test`. `v0` is the policy used for\n      all of the results in the paper and was found to achieve the best results\n      on the COCO dataset. `v1`, `v2` and `v3` are additional good policies\n      found on the COCO dataset that have slight variation in what operations\n      were used during the search procedure along with how many operations are\n      applied in parallel to a single image (2 vs 3).\n\n  Returns:\n    A tuple containing the augmented versions of `image`.\n  """"""\n  available_policies = {\'v0\': policy_v0,\n                        \'imagenet\': imagenet_policies,\n                        \'good\': good_policies,\n                        \'test\': policy_vtest}\n  if augmentation_name not in available_policies:\n    raise ValueError(\'Invalid augmentation_name: {}\'.format(augmentation_name))\n\n  policy = available_policies[augmentation_name]()\n  # Hparams that will be used for AutoAugment.\n  augmentation_hparams = tf.contrib.training.HParams(\n    cutout_max_pad_fraction=0.75,\n    cutout_const=100, translate_const=250)\n\n  return build_and_apply_nas_policy(policy, image, augmentation_hparams)\n'"
preprocessing/imagenet_preprocessing.py,38,"b'# This code is adapted from the https://github.com/tensorflow/models/tree/master/official/r1/resnet.\n# ==========================================================================================\n# NAVER\xe2\x80\x99s modifications are Copyright 2020 NAVER corp. All rights reserved.\n# ==========================================================================================\n# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Provides utilities to preprocess images.\n\nTraining images are sampled using the provided bounding boxes, and subsequently\ncropped to the sampled bounding box. Images are additionally flipped randomly,\nthen resized to the target output size (without aspect-ratio preservation).\n\nImages used during evaluation are resized (with aspect-ratio preservation) and\ncentrally cropped.\n\nAll images undergo mean color subtraction.\n\nNote that these steps are colloquially referred to as ""ResNet preprocessing,""\nand they differ from ""VGG preprocessing,"" which does not use bounding boxes\nand instead does an aspect-preserving resize followed by random crop during\ntraining. (These both differ from ""Inception preprocessing,"" which introduces\ncolor distortion steps.)\n\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\n\nfrom preprocessing import autoaugment\n\n_R_MEAN = 123.68\n_G_MEAN = 116.78\n_B_MEAN = 103.94\nCHANNEL_MEANS = [_R_MEAN, _G_MEAN, _B_MEAN]\n\n# The lower bound for the smallest side of the image for aspect-preserving\n# resizing. For example, if an image is 500 x 1000, it will be resized to\n# _RESIZE_MIN x (_RESIZE_MIN * 2).\n_RESIZE_MIN = 256\n\n\ndef _decode_crop_and_flip(image_buffer, bbox, num_channels, use_random_crop,\n                          with_drawing_bbox=False, dct_method=\'\'):\n  # A large fraction of image datasets contain a human-annotated bounding box\n  # delineating the region of the image containing the object of interest.  We\n  # choose to create a new bounding box for the object which is a randomly\n  # distorted version of the human-annotated bounding box that obeys an\n  # allowed range of aspect ratios, sizes and overlap with the human-annotated\n  # bounding box. If no box is supplied, then we assume the bounding box is\n  # the entire image.\n  if use_random_crop:\n    min_object_covered = 0.1\n  else:\n    min_object_covered = 1.0\n  sample_distorted_bounding_box = tf.image.sample_distorted_bounding_box(\n    tf.image.extract_jpeg_shape(image_buffer),\n    bounding_boxes=bbox,\n    min_object_covered=min_object_covered,\n    aspect_ratio_range=[0.75, 1.33],\n    area_range=[0.05, 1.0],\n    max_attempts=100,\n    use_image_if_no_bounding_boxes=True)\n  bbox_begin, bbox_size, bbox_for_draw = sample_distorted_bounding_box\n\n  if with_drawing_bbox:\n    image_float32 = tf.cast(tf.image.decode_jpeg(image_buffer, channels=num_channels), tf.float32)\n    raw_image_with_bbox = tf.image.draw_bounding_boxes(tf.expand_dims(image_float32, 0), bbox_for_draw)[0]\n  else:\n    raw_image_with_bbox = None\n\n  # Reassemble the bounding box in the format the crop op requires.\n  offset_y, offset_x, _ = tf.unstack(bbox_begin)\n  target_height, target_width, _ = tf.unstack(bbox_size)\n  crop_window = tf.stack([offset_y, offset_x, target_height, target_width])\n\n  # Use the fused decode and crop op here, which is faster than each in series.\n  cropped = tf.image.decode_and_crop_jpeg(\n    image_buffer, crop_window, channels=num_channels, dct_method=dct_method)\n\n  # Flip to add a little more random distortion in.\n  cropped = tf.image.random_flip_left_right(cropped)\n  return cropped, raw_image_with_bbox\n\n\ndef central_crop(image, crop_height, crop_width):\n  """"""Performs central crops of the given image list.\n\n  Args:\n    image: a 3-D image tensor\n    crop_height: the height of the image following the crop.\n    crop_width: the width of the image following the crop.\n\n  Returns:\n    3-D tensor with cropped image.\n  """"""\n  shape = tf.shape(image)\n  height, width = shape[0], shape[1]\n\n  amount_to_be_cropped_h = (height - crop_height)\n  crop_top = amount_to_be_cropped_h // 2\n  amount_to_be_cropped_w = (width - crop_width)\n  crop_left = amount_to_be_cropped_w // 2\n  return tf.slice(\n    image, [crop_top, crop_left, 0], [crop_height, crop_width, -1])\n\n\ndef mean_image_subtraction(image, means, num_channels):\n  """"""Subtracts the given means from each image channel.\n\n  For example:\n    means = [123.68, 116.779, 103.939]\n    image = mean_image_subtraction(image, means)\n\n  Note that the rank of `image` must be known.\n\n  Args:\n    image: a tensor of size [height, width, C].\n    means: a C-vector of values to subtract from each channel.\n    num_channels: number of color channels in the image that will be distorted.\n\n  Returns:\n    the centered image.\n\n  Raises:\n    ValueError: If the rank of `image` is unknown, if `image` has a rank other\n      than three or if the number of channels in `image` doesn\'t match the\n      number of values in `means`.\n  """"""\n  if image.get_shape().ndims != 3:\n    raise ValueError(\'Input must be of size [height, width, C>0]\')\n\n  if len(means) != num_channels:\n    raise ValueError(\'len(means) must match the number of channels\')\n\n  # We have a 1-D tensor of means; convert to 3-D.\n  # Note(b/130245863): we explicitly call `broadcast` instead of simply\n  # expanding dimensions for better performance.\n  means = tf.broadcast_to(means, tf.shape(image))\n\n  return image - means\n\n\ndef _smallest_size_at_least(height, width, resize_min):\n  """"""Computes new shape with the smallest side equal to `smallest_side`.\n\n  Computes new shape with the smallest side equal to `smallest_side` while\n  preserving the original aspect ratio.\n\n  Args:\n    height: an int32 scalar tensor indicating the current height.\n    width: an int32 scalar tensor indicating the current width.\n    resize_min: A python integer or scalar `Tensor` indicating the size of\n      the smallest side after resize.\n\n  Returns:\n    new_height: an int32 scalar tensor indicating the new height.\n    new_width: an int32 scalar tensor indicating the new width.\n  """"""\n  resize_min = tf.cast(resize_min, tf.float32)\n\n  # Convert to floats to make subsequent calculations go smoothly.\n  height, width = tf.cast(height, tf.float32), tf.cast(width, tf.float32)\n\n  smaller_dim = tf.minimum(height, width)\n  scale_ratio = resize_min / smaller_dim\n\n  # Convert back to ints to make heights and widths that TF ops will accept.\n  new_height = tf.cast(height * scale_ratio, tf.int32)\n  new_width = tf.cast(width * scale_ratio, tf.int32)\n\n  return new_height, new_width\n\n\ndef _aspect_preserving_resize(image, resize_min):\n  """"""Resize images preserving the original aspect ratio.\n\n  Args:\n    image: A 3-D image `Tensor`.\n    resize_min: A python integer or scalar `Tensor` indicating the size of\n      the smallest side after resize.\n\n  Returns:\n    resized_image: A 3-D tensor containing the resized image.\n  """"""\n  shape = tf.shape(image)\n  height, width = shape[0], shape[1]\n\n  new_height, new_width = _smallest_size_at_least(height, width, resize_min)\n\n  return _resize_image(image, new_height, new_width)\n\n\ndef _resize_image(image, height, width):\n  """"""Simple wrapper around tf.resize_images.\n\n  This is primarily to make sure we use the same `ResizeMethod` and other\n  details each time.\n\n  Args:\n    image: A 3-D image `Tensor`.\n    height: The target height for the resized image.\n    width: The target width for the resized image.\n\n  Returns:\n    resized_image: A 3-D tensor containing the resized image. The first two\n      dimensions have the shape [height, width].\n  """"""\n  return tf.image.resize_images(\n    image, [height, width], method=tf.image.ResizeMethod.BILINEAR,\n    align_corners=False)\n\n\ndef _ten_crop(image, crop_h, crop_w):\n  def _crop(img, center_offset):\n    # input img shape is [h,w,c]\n    img = tf.image.extract_glimpse(\n      [img], [crop_w, crop_h], offsets=tf.to_float([center_offset]),\n      centered=False, normalized=False)\n    return tf.squeeze(img, 0)\n\n  def _crop5(img):\n    # img shape is [h,w,c]\n    im_shape = tf.shape(image)\n    height, width = im_shape[0], im_shape[1]\n    ch, cw = tf.to_int32(height / 2), tf.to_int32(width / 2)  # center offset\n    hh, hw = tf.to_int32(crop_h / 2), tf.to_int32(crop_w / 2)  # half crop size\n    ct = _crop(img, [ch, cw])\n    lu = _crop(img, [hh, hw])\n    ld = _crop(img, [height - hh, hw])\n    ru = _crop(img, [hh, width - hw])\n    rd = _crop(img, [height - hh, width - hw])\n\n    org_images = tf.stack([lu, ru, ld, rd, ct])\n\n    return org_images, tf.stack([lu, ru, ld, rd, ct])\n\n  lhs, aa_lhs = _crop5(image)\n  rhs = tf.image.flip_left_right(lhs)\n\n  return tf.concat([lhs, rhs], axis=0)\n\n\ndef preprocess_image_ten_crop(image_buffer, output_height, output_width, num_channels):\n  image = tf.image.decode_jpeg(image_buffer, channels=num_channels, dct_method=\'INTEGER_ACCURATE\')\n  image = _aspect_preserving_resize(image, _RESIZE_MIN)\n  images = _ten_crop(image, output_height, output_width)\n  num_crops = 10\n  images.set_shape([num_crops, output_height, output_width, num_channels])\n  images = tf.map_fn(lambda x: mean_image_subtraction(x, CHANNEL_MEANS, num_channels), images)\n\n  return images\n\n\ndef preprocess_image(image_buffer, bbox, output_height,\n                     output_width, num_channels, is_training=False,\n                     use_random_crop=True, autoaugment_type=None,\n                     with_drawing_bbox=False, crop_type=0, dct_method=\'\'):\n  if is_training:\n    # For training, we want to randomize some of the distortions.\n    image, image_with_bbox = _decode_crop_and_flip(image_buffer, bbox, num_channels,\n                                                   use_random_crop,\n                                                   dct_method=dct_method,\n                                                   with_drawing_bbox=with_drawing_bbox)\n\n    image = _resize_image(image, output_height, output_width)\n\n    if autoaugment_type:\n      tf.logging.info(\'Apply AutoAugment policy {}\'.format(autoaugment_type))\n      image = tf.clip_by_value(image, 0.0, 255.0)\n      dtype = image.dtype\n      image = tf.cast(image, dtype=tf.uint8)\n      image = autoaugment.distort_image_with_autoaugment(\n        image, autoaugment_type)\n      image = tf.cast(image, dtype=dtype)\n\n    if with_drawing_bbox:\n      image_with_bbox = _resize_image(image_with_bbox, output_height, output_width)\n      image_with_bbox.set_shape([output_height, output_width, num_channels])\n\n  else:\n    image = tf.image.decode_jpeg(image_buffer, channels=num_channels, dct_method=dct_method)\n\n    if with_drawing_bbox:\n      image_with_bbox = image\n      image_with_bbox = _resize_image(image_with_bbox, output_height, output_width)\n      image_with_bbox.set_shape([output_height, output_width, num_channels])\n    else:\n      image_with_bbox = None\n\n    if crop_type == 1:\n      image = _aspect_preserving_resize(image, int(min(output_height, output_width) + 1))\n      image = central_crop(image, output_height, output_width)\n    else:\n      image = _aspect_preserving_resize(image, int(min(output_height, output_width) * (1.0 / 0.875)))\n      image = central_crop(image, output_height, output_width)\n\n  image.set_shape([output_height, output_width, num_channels])\n  return mean_image_subtraction(image, CHANNEL_MEANS, num_channels), image_with_bbox\n\n'"
preprocessing/inception_preprocessing.py,66,"b'# This code is adapted from the https://github.com/tensorflow/models/tree/master/official/r1/resnet.\n# ==========================================================================================\n# NAVER\xe2\x80\x99s modifications are Copyright 2020 NAVER corp. All rights reserved.\n# ==========================================================================================\n# Copyright 2018 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Provides utilities to preprocess images for the Inception networks.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\nfrom absl import flags\nfrom tensorflow.python.ops import control_flow_ops\nfrom tensorflow.python.ops import random_ops\n\nflags.DEFINE_float(\n  \'cb_distortion_range\', 0.1, \'Cb distortion range +/-\')\n\nflags.DEFINE_float(\n  \'cr_distortion_range\', 0.1, \'Cr distortion range +/-\')\n\nflags.DEFINE_boolean(\n  \'use_fast_color_distort\', True,\n  \'apply fast color/chroma distortion if True, else apply\'\n  \'brightness/saturation/hue/contrast distortion\')\n\nFLAGS = flags.FLAGS\n\n\ndef apply_with_random_selector(x, func, num_cases):\n  """"""Computes func(x, sel), with sel sampled from [0...num_cases-1].\n\n  Args:\n    x: input Tensor.\n    func: Python function to apply.\n    num_cases: Python int32, number of cases to sample sel from.\n\n  Returns:\n    The result of func(x, sel), where func receives the value of the\n    selector as a python integer, but sel is sampled dynamically.\n  """"""\n  sel = tf.random_uniform([], maxval=num_cases, dtype=tf.int32)\n  # Pass the real x only to one of the func calls.\n  return control_flow_ops.merge([\n    func(control_flow_ops.switch(x, tf.equal(sel, case))[1], case)\n    for case in range(num_cases)])[0]\n\n\ndef distort_color(image, color_ordering=0, fast_mode=True, scope=None, deno=255.):\n  """"""Distort the color of a Tensor image.\n\n  Each color distortion is non-commutative and thus ordering of the color ops\n  matters. Ideally we would randomly permute the ordering of the color ops.\n  Rather then adding that level of complication, we select a distinct ordering\n  of color ops for each preprocessing thread.\n\n  Args:\n    image: 3-D Tensor containing single image in [0, 1].\n    color_ordering: Python int, a type of distortion (valid values: 0-3).\n    fast_mode: Avoids slower ops (random_hue and random_contrast)\n    scope: Optional scope for name_scope.\n  Returns:\n    3-D Tensor color-distorted image on range [0, 1]\n  Raises:\n    ValueError: if color_ordering not in [0, 3]\n  """"""\n  with tf.name_scope(scope, \'distort_color\', [image]):\n    if fast_mode:\n      if color_ordering == 0:\n        image = tf.image.random_brightness(image, max_delta=32. / deno)\n        image = tf.image.random_saturation(image, lower=0.5, upper=1.5)\n      else:\n        image = tf.image.random_saturation(image, lower=0.5, upper=1.5)\n        image = tf.image.random_brightness(image, max_delta=32. / deno)\n    else:\n      if color_ordering == 0:\n        image = tf.image.random_brightness(image, max_delta=32. / deno)\n        image = tf.image.random_saturation(image, lower=0.5, upper=1.5)\n        image = tf.image.random_hue(image, max_delta=0.2)\n        image = tf.image.random_contrast(image, lower=0.5, upper=1.5)\n      elif color_ordering == 1:\n        image = tf.image.random_saturation(image, lower=0.5, upper=1.5)\n        image = tf.image.random_brightness(image, max_delta=32. / deno)\n        image = tf.image.random_contrast(image, lower=0.5, upper=1.5)\n        image = tf.image.random_hue(image, max_delta=0.2)\n      elif color_ordering == 2:\n        image = tf.image.random_contrast(image, lower=0.5, upper=1.5)\n        image = tf.image.random_hue(image, max_delta=0.2)\n        image = tf.image.random_brightness(image, max_delta=32. / deno)\n        image = tf.image.random_saturation(image, lower=0.5, upper=1.5)\n      elif color_ordering == 3:\n        image = tf.image.random_hue(image, max_delta=0.2)\n        image = tf.image.random_saturation(image, lower=0.5, upper=1.5)\n        image = tf.image.random_contrast(image, lower=0.5, upper=1.5)\n        image = tf.image.random_brightness(image, max_delta=32. / deno)\n      else:\n        raise ValueError(\'color_ordering must be in [0, 3]\')\n\n    # The random_* ops do not necessarily clamp.\n    return tf.minimum(tf.maximum(image, 0.0), 255. / deno)\n\n\ndef distort_color_fast(image, scope=None):\n  """"""Distort the color of a Tensor image.\n\n  Distort brightness and chroma values of input image\n\n  Args:\n    image: 3-D Tensor containing single image in [0, 1].\n    scope: Optional scope for name_scope.\n  Returns:\n    3-D Tensor color-distorted image on range [0, 1]\n  """"""\n  with tf.name_scope(scope, \'distort_color\', [image]):\n    br_delta = random_ops.random_uniform([], -32. / 255., 32. / 255., seed=None)\n    cb_factor = random_ops.random_uniform(\n      [], -FLAGS.cb_distortion_range, FLAGS.cb_distortion_range, seed=None)\n    cr_factor = random_ops.random_uniform(\n      [], -FLAGS.cr_distortion_range, FLAGS.cr_distortion_range, seed=None)\n\n    channels = tf.split(axis=2, num_or_size_splits=3, value=image)\n    red_offset = 1.402 * cr_factor + br_delta\n    green_offset = -0.344136 * cb_factor - 0.714136 * cr_factor + br_delta\n    blue_offset = 1.772 * cb_factor + br_delta\n    channels[0] += red_offset\n    channels[1] += green_offset\n    channels[2] += blue_offset\n    image = tf.concat(axis=2, values=channels)\n    image = tf.minimum(tf.maximum(image, 0.), 1.)\n    return image\n\n\ndef distorted_bounding_box_crop(image,\n                                bbox,\n                                min_object_covered=0.1,\n                                aspect_ratio_range=(3. / 4., 4. / 3.),\n                                area_range=(0.05, 1.0),\n                                max_attempts=100,\n                                scope=None):\n  """"""Generates cropped_image using a one of the bboxes randomly distorted.\n\n  See `tf.image.sample_distorted_bounding_box` for more documentation.\n\n  Args:\n    image: 3-D Tensor of image (it will be converted to floats in [0, 1]).\n    bbox: 3-D float Tensor of bounding boxes arranged [1, num_boxes, coords]\n      where each coordinate is [0, 1) and the coordinates are arranged\n      as [ymin, xmin, ymax, xmax]. If num_boxes is 0 then it would use the whole\n      image.\n    min_object_covered: An optional `float`. Defaults to `0.1`. The cropped\n      area of the image must contain at least this fraction of any bounding box\n      supplied.\n    aspect_ratio_range: An optional list of `floats`. The cropped area of the\n      image must have an aspect ratio = width / height within this range.\n    area_range: An optional list of `floats`. The cropped area of the image\n      must contain a fraction of the supplied image within in this range.\n    max_attempts: An optional `int`. Number of attempts at generating a cropped\n      region of the image of the specified constraints. After `max_attempts`\n      failures, return the entire image.\n    scope: Optional scope for name_scope.\n  Returns:\n    A tuple, a 3-D Tensor cropped_image and the distorted bbox\n  """"""\n  with tf.name_scope(scope, \'distorted_bounding_box_crop\', [image, bbox]):\n    # Each bounding box has shape [1, num_boxes, box coords] and\n    # the coordinates are ordered [ymin, xmin, ymax, xmax].\n\n    # A large fraction of image datasets contain a human-annotated bounding\n    # box delineating the region of the image containing the object of interest.\n    # We choose to create a new bounding box for the object which is a randomly\n    # distorted version of the human-annotated bounding box that obeys an\n    # allowed range of aspect ratios, sizes and overlap with the human-annotated\n    # bounding box. If no box is supplied, then we assume the bounding box is\n    # the entire image.\n    sample_distorted_bounding_box = tf.image.sample_distorted_bounding_box(\n      tf.shape(image),\n      bounding_boxes=bbox,\n      min_object_covered=min_object_covered,\n      aspect_ratio_range=aspect_ratio_range,\n      area_range=area_range,\n      max_attempts=max_attempts,\n      use_image_if_no_bounding_boxes=True)\n    bbox_begin, bbox_size, distort_bbox = sample_distorted_bounding_box\n\n    # Crop the image to the specified bounding box.\n    cropped_image = tf.slice(image, bbox_begin, bbox_size)\n    return cropped_image, distort_bbox\n\n\ndef preprocess_for_train(image, height, width, bbox,\n                         min_object_covered=0.1,\n                         fast_mode=True,\n                         scope=None,\n                         add_image_summaries=True):\n  """"""Distort one image for training a network.\n\n  Distorting images provides a useful technique for augmenting the data\n  set during training in order to make the network invariant to aspects\n  of the image that do not effect the label.\n\n  Additionally it would create image_summaries to display the different\n  transformations applied to the image.\n\n  Args:\n    image: 3-D Tensor of image. If dtype is tf.float32 then the range should be\n      [0, 1], otherwise it would converted to tf.float32 assuming that the range\n      is [0, MAX], where MAX is largest positive representable number for\n      int(8/16/32) data type (see `tf.image.convert_image_dtype` for details).\n    height: integer\n    width: integer\n    bbox: 3-D float Tensor of bounding boxes arranged [1, num_boxes, coords]\n      where each coordinate is [0, 1) and the coordinates are arranged\n      as [ymin, xmin, ymax, xmax].\n    min_object_covered: An optional `float`. Defaults to `0.1`. The cropped\n      area of the image must contain at least this fraction of any bounding box\n      supplied.\n    fast_mode: Optional boolean, if True avoids slower transformations (i.e.\n      bi-cubic resizing, random_hue or random_contrast).\n    scope: Optional scope for name_scope.\n    add_image_summaries: Enable image summaries.\n  Returns:\n    3-D float Tensor of distorted image used for training with range [-1, 1].\n  """"""\n  with tf.name_scope(scope, \'distort_image\', [image, height, width, bbox]):\n    if bbox is None:\n      bbox = tf.constant([0.0, 0.0, 1.0, 1.0],\n                         dtype=tf.float32,\n                         shape=[1, 1, 4])\n    image = tf.image.decode_jpeg(image, channels=3)\n    if image.dtype != tf.float32:\n      image = tf.image.convert_image_dtype(image, dtype=tf.float32)\n    if add_image_summaries:\n      # Each bounding box has shape [1, num_boxes, box coords] and\n      # the coordinates are ordered [ymin, xmin, ymax, xmax].\n      image_with_box = tf.image.draw_bounding_boxes(tf.expand_dims(image, 0),\n                                                    bbox)\n      tf.summary.image(\'image_with_bounding_boxes\', image_with_box)\n\n    distorted_image, distorted_bbox = distorted_bounding_box_crop(\n      image,\n      bbox,\n      min_object_covered=min_object_covered,\n      area_range=(min_object_covered, 1.0))\n    # Restore the shape since the dynamic slice based upon the bbox_size loses\n    # the third dimension.\n    distorted_image.set_shape([None, None, 3])\n    if add_image_summaries:\n      image_with_distorted_box = tf.image.draw_bounding_boxes(\n        tf.expand_dims(image, 0), distorted_bbox)\n      tf.summary.image(\'images_with_distorted_bounding_box\',\n                       image_with_distorted_box)\n\n    # This resizing operation may distort the images because the aspect\n    # ratio is not respected. We select a resize method in a round robin\n    # fashion based on the thread number.\n    # Note that ResizeMethod contains 4 enumerated resizing methods.\n\n    # We select only 1 case for fast_mode bilinear.\n    num_resize_cases = 1 if fast_mode else 4\n    distorted_image = apply_with_random_selector(\n      distorted_image,\n      lambda x, method: tf.image.resize_images(x, [height, width], method),\n      num_cases=num_resize_cases)\n\n    if add_image_summaries:\n      tf.summary.image(\'cropped_resized_image\',\n                       tf.expand_dims(distorted_image, 0))\n\n    # Randomly flip the image horizontally.\n    distorted_image = tf.image.random_flip_left_right(distorted_image)\n\n    # Randomly distort the colors. There are 1 or 4 ways to do it.\n    if FLAGS.use_fast_color_distort:\n      distorted_image = distort_color_fast(distorted_image)\n    else:\n      num_distort_cases = 1 if fast_mode else 4\n      distorted_image = apply_with_random_selector(\n        distorted_image,\n        lambda x, ordering: distort_color(x, ordering, fast_mode),\n        num_cases=num_distort_cases)\n\n    if add_image_summaries:\n      tf.summary.image(\'final_distorted_image\',\n                       tf.expand_dims(distorted_image, 0))\n    return distorted_image\n\n\ndef preprocess_for_eval(image, height, width,\n                        central_fraction=0.875, scope=None):\n  """"""Prepare one image for evaluation.\n\n  If height and width are specified it would output an image with that size by\n  applying resize_bilinear.\n\n  If central_fraction is specified it would crop the central fraction of the\n  input image.\n\n  Args:\n    image: 3-D Tensor of image. If dtype is tf.float32 then the range should be\n      [0, 1], otherwise it would converted to tf.float32 assuming that the range\n      is [0, MAX], where MAX is largest positive representable number for\n      int(8/16/32) data type (see `tf.image.convert_image_dtype` for details).\n    height: integer\n    width: integer\n    central_fraction: Optional Float, fraction of the image to crop.\n    scope: Optional scope for name_scope.\n  Returns:\n    3-D float Tensor of prepared image.\n  """"""\n  with tf.name_scope(scope, \'eval_image\', [image, height, width]):\n    image = tf.image.decode_jpeg(image, channels=3)\n    if image.dtype != tf.float32:\n      image = tf.image.convert_image_dtype(image, dtype=tf.float32)\n    # Crop the central region of the image with an area containing 87.5% of\n    # the original image.\n    if central_fraction:\n      image = tf.image.central_crop(image, central_fraction=central_fraction)\n\n    if height and width:\n      # Resize the image to the specified height and width.\n      image = tf.expand_dims(image, 0)\n      image = tf.image.resize_bilinear(image, [height, width],\n                                       align_corners=False)\n      image = tf.squeeze(image, [0])\n    image.set_shape([height, width, 3])\n    return image\n\n\ndef preprocess_image(image,\n                     output_height,\n                     output_width,\n                     is_training=False,\n                     scaled_images=False,\n                     bbox=None,\n                     min_object_covered=0.1,\n                     fast_mode=True,\n                     add_image_summaries=False):\n\n\n  if is_training:\n    image = preprocess_for_train(\n      image,\n      output_height,\n      output_width,\n      bbox,\n      min_object_covered,\n      fast_mode,\n      add_image_summaries=add_image_summaries)\n  else:\n    image = preprocess_for_eval(image, output_height, output_width)\n  if scaled_images:\n    image = tf.subtract(image, 0.5)\n    image = tf.multiply(image, 2.0)\n  return image\n'"
preprocessing/reid_preprocessing.py,95,"b'# This code is adapted from the https://github.com/tensorflow/models/tree/master/official/r1/resnet.\n# ==========================================================================================\n# NAVER\xe2\x80\x99s modifications are Copyright 2020 NAVER corp. All rights reserved.\n# ==========================================================================================\n# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Provides utilities to preprocess images.\n\nTraining images are sampled using the provided bounding boxes, and subsequently\ncropped to the sampled bounding box. Images are additionally flipped randomly,\nthen resized to the target output size (without aspect-ratio preservation).\n\nImages used during evaluation are resized (with aspect-ratio preservation) and\ncentrally cropped.\n\nAll images undergo mean color subtraction.\n\nNote that these steps are colloquially referred to as ""ResNet preprocessing,""\nand they differ from ""VGG preprocessing,"" which does not use bounding boxes\nand instead does an aspect-preserving resize followed by random crop during\ntraining. (These both differ from ""Inception preprocessing,"" which introduces\ncolor distortion steps.)\n\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\nfrom tensorflow.python.ops import control_flow_ops\n\nfrom preprocessing import autoaugment\n\n_R_MEAN = 123.68\n_G_MEAN = 116.78\n_B_MEAN = 103.94\n_CHANNEL_MEANS = [_R_MEAN, _G_MEAN, _B_MEAN]\n\n_MEAN = [0.485, 0.456, 0.406]\n_STD = [0.229, 0.224, 0.225]\n# The lower bound for the smallest side of the image for aspect-preserving\n# resizing. For example, if an image is 500 x 1000, it will be resized to\n# _RESIZE_MIN x (_RESIZE_MIN * 2).\n_RESIZE_MIN = 256\n\ndef central_crop(image, crop_height, crop_width):\n  """"""Performs central crops of the given image list.\n\n  Args:\n    image: a 3-D image tensor\n    crop_height: the height of the image following the crop.\n    crop_width: the width of the image following the crop.\n\n  Returns:\n    3-D tensor with cropped image.\n  """"""\n  shape = tf.shape(image)\n  height, width = shape[0], shape[1]\n\n  amount_to_be_cropped_h = (height - crop_height)\n  crop_top = amount_to_be_cropped_h // 2\n  amount_to_be_cropped_w = (width - crop_width)\n  crop_left = amount_to_be_cropped_w // 2\n  return tf.slice(\n    image, [crop_top, crop_left, 0], [crop_height, crop_width, -1])\n\n\ndef _mean_image_subtraction(image, means, num_channels):\n  """"""Subtracts the given means from each image channel.\n\n  For example:\n    means = [123.68, 116.779, 103.939]\n    image = _mean_image_subtraction(image, means)\n\n  Note that the rank of `image` must be known.\n\n  Args:\n    image: a tensor of size [height, width, C].\n    means: a C-vector of values to subtract from each channel.\n    num_channels: number of color channels in the image that will be distorted.\n\n  Returns:\n    the centered image.\n\n  Raises:\n    ValueError: If the rank of `image` is unknown, if `image` has a rank other\n      than three or if the number of channels in `image` doesn\'t match the\n      number of values in `means`.\n  """"""\n  if image.get_shape().ndims != 3:\n    raise ValueError(\'Input must be of size [height, width, C>0]\')\n\n  if len(means) != num_channels:\n    raise ValueError(\'len(means) must match the number of channels\')\n\n  # We have a 1-D tensor of means; convert to 3-D.\n  # Note(b/130245863): we explicitly call `broadcast` instead of simply\n  # expanding dimensions for better performance.\n  means = tf.broadcast_to(means, tf.shape(image))\n\n  return image - means\n\n\ndef _normalization(image, means, stds, num_channels):\n  """"""Subtracts the given means from each image channel.\n\n  For example:\n    means = [123.68, 116.779, 103.939]\n    image = _mean_image_subtraction(image, means)\n\n  Note that the rank of `image` must be known.\n\n  Args:\n    image: a tensor of size [height, width, C].\n    means: a C-vector of values to subtract from each channel.\n    num_channels: number of color channels in the image that will be distorted.\n\n  Returns:\n    the centered image.\n\n  Raises:\n    ValueError: If the rank of `image` is unknown, if `image` has a rank other\n      than three or if the number of channels in `image` doesn\'t match the\n      number of values in `means`.\n  """"""\n  if image.get_shape().ndims != 3:\n    raise ValueError(\'Input must be of size [height, width, C>0]\')\n\n  if len(means) != num_channels:\n    raise ValueError(\'len(means) must match the number of channels\')\n\n  # We have a 1-D tensor of means; convert to 3-D.\n  # Note(b/130245863): we explicitly call `broadcast` instead of simply\n  # expanding dimensions for better performance.\n  means = tf.broadcast_to(means, tf.shape(image))\n  stds = tf.broadcast_to(stds, tf.shape(image))\n\n  return (image - means) / stds\n\n\ndef _smallest_size_at_least(height, width, resize_min):\n  """"""Computes new shape with the smallest side equal to `smallest_side`.\n\n  Computes new shape with the smallest side equal to `smallest_side` while\n  preserving the original aspect ratio.\n\n  Args:\n    height: an int32 scalar tensor indicating the current height.\n    width: an int32 scalar tensor indicating the current width.\n    resize_min: A python integer or scalar `Tensor` indicating the size of\n      the smallest side after resize.\n\n  Returns:\n    new_height: an int32 scalar tensor indicating the new height.\n    new_width: an int32 scalar tensor indicating the new width.\n  """"""\n  resize_min = tf.cast(resize_min, tf.float32)\n\n  # Convert to floats to make subsequent calculations go smoothly.\n  height, width = tf.cast(height, tf.float32), tf.cast(width, tf.float32)\n\n  smaller_dim = tf.minimum(height, width)\n  scale_ratio = resize_min / smaller_dim\n\n  # Convert back to ints to make heights and widths that TF ops will accept.\n  new_height = tf.cast(height * scale_ratio, tf.int32)\n  new_width = tf.cast(width * scale_ratio, tf.int32)\n\n  return new_height, new_width\n\n\ndef _aspect_preserving_resize(image, resize_min):\n  """"""Resize images preserving the original aspect ratio.\n\n  Args:\n    image: A 3-D image `Tensor`.\n    resize_min: A python integer or scalar `Tensor` indicating the size of\n      the smallest side after resize.\n\n  Returns:\n    resized_image: A 3-D tensor containing the resized image.\n  """"""\n  shape = tf.shape(image)\n  height, width = shape[0], shape[1]\n\n  new_height, new_width = _smallest_size_at_least(height, width, resize_min)\n\n  return _resize_image(image, new_height, new_width)\n\n\ndef _resize_image(image, height, width):\n  """"""Simple wrapper around tf.resize_images.\n\n  This is primarily to make sure we use the same `ResizeMethod` and other\n  details each time.\n\n  Args:\n    image: A 3-D image `Tensor`.\n    height: The target height for the resized image.\n    width: The target width for the resized image.\n\n  Returns:\n    resized_image: A 3-D tensor containing the resized image. The first two\n      dimensions have the shape [height, width].\n  """"""\n  return tf.image.resize_images(\n    image, [height, width], method=tf.image.ResizeMethod.BILINEAR,\n    align_corners=False)\n\n\ndef _ten_crop(image, crop_h, crop_w):\n  def _crop(img, center_offset):\n    # input img shape is [h,w,c]\n    img = tf.image.extract_glimpse(\n      [img], [crop_w, crop_h], offsets=tf.to_float([center_offset]),\n      centered=False, normalized=False)\n    return tf.squeeze(img, 0)\n\n  def _crop5(img):\n    # img shape is [h,w,c]\n    im_shape = tf.shape(image)\n    height, width = im_shape[0], im_shape[1]\n    ch, cw = tf.to_int32(height / 2), tf.to_int32(width / 2)  # center offset\n    hh, hw = tf.to_int32(crop_h / 2), tf.to_int32(crop_w / 2)  # half crop size\n    ct = _crop(img, [ch, cw])\n    lu = _crop(img, [hh, hw])\n    ld = _crop(img, [height - hh, hw])\n    ru = _crop(img, [hh, width - hw])\n    rd = _crop(img, [height - hh, width - hw])\n    return tf.stack([lu, ru, ld, rd, ct])\n\n  lhs = _crop5(image)\n  rhs = tf.image.flip_left_right(lhs)\n  return tf.concat([lhs, rhs], axis=0)\n\n\ndef preprocess_image_ten_crop(image_buffer, output_height, output_width, num_channels):\n  image = tf.image.decode_jpeg(image_buffer, channels=num_channels)\n  image = _aspect_preserving_resize(image, _RESIZE_MIN)\n  images = _ten_crop(image, output_height, output_width)\n  images.set_shape([10, output_height, output_width, num_channels])\n  images = tf.map_fn(lambda x: _mean_image_subtraction(x, _CHANNEL_MEANS, num_channels), images)\n  return images\n\ndef _crop(image, offset_height, offset_width, crop_height, crop_width):\n  """"""Crops the given image using the provided offsets and sizes.\n\n  Note that the method doesn\'t assume we know the input image size but it does\n  assume we know the input image rank.\n\n  Args:\n    image: an image of shape [height, width, channels].\n    offset_height: a scalar tensor indicating the height offset.\n    offset_width: a scalar tensor indicating the width offset.\n    crop_height: the height of the cropped image.\n    crop_width: the width of the cropped image.\n\n  Returns:\n    the cropped (and resized) image.\n\n  Raises:\n    InvalidArgumentError: if the rank is not 3 or if the image dimensions are\n      less than the crop size.\n  """"""\n  original_shape = tf.shape(image)\n\n  rank_assertion = tf.Assert(\n    tf.equal(tf.rank(image), 3),\n    [\'Rank of image must be equal to 3.\'])\n  with tf.control_dependencies([rank_assertion]):\n    cropped_shape = tf.stack([crop_height, crop_width, original_shape[2]])\n\n  size_assertion = tf.Assert(\n    tf.logical_and(\n      tf.greater_equal(original_shape[0], crop_height),\n      tf.greater_equal(original_shape[1], crop_width)),\n    [\'Crop size greater than the image size.\'])\n\n  offsets = tf.to_int32(tf.stack([offset_height, offset_width, 0]))\n\n  # Use tf.slice instead of crop_to_bounding box as it accepts tensors to\n  # define the crop size.\n  with tf.control_dependencies([size_assertion]):\n    image = tf.slice(image, offsets, cropped_shape)\n  return tf.reshape(image, cropped_shape)\n\n\ndef _get_random_crop_coord(image_list, crop_height, crop_width):\n  """"""Crops the given list of images.\n  The function applies the same crop to each image in the list. This can be\n  effectively applied when there are multiple image inputs of the same\n  dimension such as:\n    image, depths, normals = _random_crop([image, depths, normals], 120, 150)\n  Args:\n    image_list: a list of image tensors of the same dimension but possibly\n      varying channel.\n    crop_height: the new height.\n    crop_width: the new width.\n  Returns:\n    the image_list with cropped images.\n  Raises:\n    ValueError: if there are multiple image inputs provided with different size\n      or the images are smaller than the crop dimensions.\n  """"""\n  if not image_list:\n    raise ValueError(\'Empty image_list.\')\n\n  # Compute the rank assertions.\n  rank_assertions = []\n  for i in range(len(image_list)):\n    image_rank = tf.rank(image_list[i])\n    rank_assert = tf.Assert(\n      tf.equal(image_rank, 3),\n      [\'Wrong rank for tensor  %s [expected] [actual]\',\n       image_list[i].name, 3, image_rank])\n    rank_assertions.append(rank_assert)\n\n  image_shape = control_flow_ops.with_dependencies(\n    [rank_assertions[0]],\n    tf.shape(image_list[0]))\n  image_height = image_shape[0]\n  image_width = image_shape[1]\n  crop_size_assert = tf.Assert(\n    tf.logical_and(\n      tf.greater_equal(image_height, crop_height),\n      tf.greater_equal(image_width, crop_width)),\n    [\'Crop size greater than the image size.\'])\n\n  asserts = [rank_assertions[0], crop_size_assert]\n\n  for i in range(1, len(image_list)):\n    image = image_list[i]\n    asserts.append(rank_assertions[i])\n    shape = control_flow_ops.with_dependencies([rank_assertions[i]],\n                                               tf.shape(image))\n    height = shape[0]\n    width = shape[1]\n\n    height_assert = tf.Assert(\n      tf.equal(height, image_height),\n      [\'Wrong height for tensor %s [expected][actual]\',\n       image.name, height, image_height])\n    width_assert = tf.Assert(\n      tf.equal(width, image_width),\n      [\'Wrong width for tensor %s [expected][actual]\',\n       image.name, width, image_width])\n    asserts.extend([height_assert, width_assert])\n\n  # Create a random bounding box.\n  #\n  # Use tf.random_uniform and not numpy.random.rand as doing the former would\n  # generate random numbers at graph eval time, unlike the latter which\n  # generates random numbers at graph definition time.\n  max_offset_height = control_flow_ops.with_dependencies(\n    asserts, tf.reshape(image_height - crop_height + 1, []))\n  max_offset_width = control_flow_ops.with_dependencies(\n    asserts, tf.reshape(image_width - crop_width + 1, []))\n  offset_height = tf.random_uniform(\n    [], maxval=max_offset_height, dtype=tf.int32)\n  offset_width = tf.random_uniform(\n    [], maxval=max_offset_width, dtype=tf.int32)\n\n  return tf.stack([offset_height, offset_width, crop_height, crop_width])\n\n\ndef _random_crop(image_list, crop_height, crop_width):\n  """"""Crops the given list of images.\n  The function applies the same crop to each image in the list. This can be\n  effectively applied when there are multiple image inputs of the same\n  dimension such as:\n    image, depths, normals = _random_crop([image, depths, normals], 120, 150)\n  Args:\n    image_list: a list of image tensors of the same dimension but possibly\n      varying channel.\n    crop_height: the new height.\n    crop_width: the new width.\n  Returns:\n    the image_list with cropped images.\n  Raises:\n    ValueError: if there are multiple image inputs provided with different size\n      or the images are smaller than the crop dimensions.\n  """"""\n  if not image_list:\n    raise ValueError(\'Empty image_list.\')\n\n  # Compute the rank assertions.\n  rank_assertions = []\n  for i in range(len(image_list)):\n    image_rank = tf.rank(image_list[i])\n    rank_assert = tf.Assert(\n      tf.equal(image_rank, 3),\n      [\'Wrong rank for tensor  %s [expected] [actual]\',\n       image_list[i].name, 3, image_rank])\n    rank_assertions.append(rank_assert)\n\n  image_shape = control_flow_ops.with_dependencies(\n    [rank_assertions[0]],\n    tf.shape(image_list[0]))\n  image_height = image_shape[0]\n  image_width = image_shape[1]\n  crop_size_assert = tf.Assert(\n    tf.logical_and(\n      tf.greater_equal(image_height, crop_height),\n      tf.greater_equal(image_width, crop_width)),\n    [\'Crop size greater than the image size.\'])\n\n  asserts = [rank_assertions[0], crop_size_assert]\n\n  for i in range(1, len(image_list)):\n    image = image_list[i]\n    asserts.append(rank_assertions[i])\n    shape = control_flow_ops.with_dependencies([rank_assertions[i]],\n                                               tf.shape(image))\n    height = shape[0]\n    width = shape[1]\n\n    height_assert = tf.Assert(\n      tf.equal(height, image_height),\n      [\'Wrong height for tensor %s [expected][actual]\',\n       image.name, height, image_height])\n    width_assert = tf.Assert(\n      tf.equal(width, image_width),\n      [\'Wrong width for tensor %s [expected][actual]\',\n       image.name, width, image_width])\n    asserts.extend([height_assert, width_assert])\n\n  # Create a random bounding box.\n  #\n  # Use tf.random_uniform and not numpy.random.rand as doing the former would\n  # generate random numbers at graph eval time, unlike the latter which\n  # generates random numbers at graph definition time.\n  max_offset_height = control_flow_ops.with_dependencies(\n    asserts, tf.reshape(image_height - crop_height + 1, []))\n  max_offset_width = control_flow_ops.with_dependencies(\n    asserts, tf.reshape(image_width - crop_width + 1, []))\n  offset_height = tf.random_uniform(\n    [], maxval=max_offset_height, dtype=tf.int32)\n  offset_width = tf.random_uniform(\n    [], maxval=max_offset_width, dtype=tf.int32)\n\n  return [_crop(image, offset_height, offset_width,\n                crop_height, crop_width) for image in image_list]\n\n\ndef pad_shorter(image):\n  shape = tf.shape(image)\n  height, width = shape[0], shape[1]\n  larger_dim = tf.maximum(height, width)\n  h1 = (larger_dim - height) // 2\n  h2 = (larger_dim - height) - h1\n  w1 = tf.maximum((larger_dim - width) // 2, 0)\n  w2 = (larger_dim - width) - w1\n  pad_shape = [[h1, h2], [w1, w2], [0, 0]]\n  return tf.pad(image, pad_shape)\n\n\ndef apply_with_random_selector(x, func, num_cases):\n  """"""Computes func(x, sel), with sel sampled from [0...num_cases-1].\n\n  Args:\n    x: input Tensor.\n    func: Python function to apply.\n    num_cases: Python int32, number of cases to sample sel from.\n\n  Returns:\n    The result of func(x, sel), where func receives the value of the\n    selector as a python integer, but sel is sampled dynamically.\n  """"""\n  sel = tf.random_uniform([], maxval=num_cases, dtype=tf.int32)\n  # Pass the real x only to one of the func calls.\n  return control_flow_ops.merge([\n    func(control_flow_ops.switch(x, tf.equal(sel, case))[1], case)\n    for case in range(num_cases)])[0]\n\n\ndef resize_func(image, size, method):\n  if method == 0:\n    image = _resize_image(image, _RESIZE_MIN, _RESIZE_MIN)\n    image = _random_crop([image], size[0], size[1])[0]\n  else:\n    image = _resize_image(image, size[0], size[1])\n  return image\n\ndef preprocess_image(image_buffer,\n                     output_height,\n                     output_width,\n                     num_channels,\n                     dct_method=\'\',\n                     is_training=False,\n                     autoaugment_type=None,\n                     eval_large_resolution=True):\n\n  if is_training:\n    image = tf.image.decode_jpeg(image_buffer, channels=num_channels, dct_method=dct_method)\n\n    image = apply_with_random_selector(\n      image,\n      lambda x, method: resize_func(x, [output_height, output_width], method),\n      num_cases=2)\n\n    image.set_shape([output_height, output_width, 3])\n    image = tf.to_float(image)\n    image = tf.image.random_flip_left_right(image)\n\n    if autoaugment_type:\n      tf.logging.info(\'Apply AutoAugment policy {}\'.format(autoaugment_type))\n      image = tf.clip_by_value(image, 0.0, 255.0)\n      dtype = image.dtype\n      image = tf.cast(image, dtype=tf.uint8)\n      image = autoaugment.distort_image_with_autoaugment(\n        image, autoaugment_type)\n      image = tf.cast(image, dtype=dtype)\n\n    image.set_shape([output_height, output_width, num_channels])\n\n  else:\n    if eval_large_resolution:\n      output_height = int(output_height * (1.0 / 0.875))\n      output_width = int(output_width * (1.0 / 0.875))\n    # For validation, we want to decode, resize, then just crop the middle.\n    image = tf.image.decode_jpeg(image_buffer, channels=num_channels, dct_method=dct_method)\n\n    image = _resize_image(image, output_height, output_width)\n    image = tf.to_float(image)\n    image.set_shape([output_height, output_width, num_channels])\n\n  return _mean_image_subtraction(image, _CHANNEL_MEANS, num_channels)\n'"
utils/__init__.py,0,b''
utils/checkpoint_utils.py,3,"b'# ==============================================================================\n# Copyright 2020-present NAVER Corp.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\nimport glob\nimport json\nimport os\nfrom shutil import copyfile\n\nimport tensorflow as tf\n\n\nclass CheckpointKeeper(object):\n  def __init__(self, save_dir, num_to_keep=1, keep_epoch=False, maximize=True):\n    """"""Creates a `BestCheckpointSaver`\n\n    Args:\n        save_dir: The directory in which the checkpoint files will be saved\n        num_to_keep: The number of best checkpoint files to retain\n        keep_epoch: If True, checkpoints are saved for each evaluation.\n        maximize: Define \'best\' values to be the highest values.  For example,\n          set this to True if selecting for the checkpoints with the highest\n          given accuracy.  Or set to False to select for checkpoints with the\n          lowest given error rate.\n    """"""\n    self._num_to_keep = num_to_keep\n    self._save_dir = save_dir\n    self._best_save_path = os.path.join(save_dir, \'best\')\n    self._periodical_save_path = os.path.join(save_dir, \'periodical\')\n    self._maximize = maximize\n    self._keep_epoch = keep_epoch\n\n    if not os.path.exists(self._best_save_path):\n      os.makedirs(self._best_save_path)\n\n    if self._keep_epoch:\n      if not os.path.exists(self._periodical_save_path):\n        os.makedirs(self._periodical_save_path)\n\n    self.best_checkpoints_file = os.path.join(self._best_save_path, \'best_checkpoints\')\n\n  def _keep_ckpt(self, checkpoint_path, mode=\'best\'):\n    if mode == \'best\':\n      save_path = self._best_save_path\n    elif mode == \'periodical\':\n      save_path = self._periodical_save_path\n    else:\n      raise AssertionError(\'mode must be ""best"" or ""periodical""\')\n\n    for ckpt_file in glob.glob(os.path.join(self._save_dir, checkpoint_path) + ""*""):\n      file_name = os.path.basename(ckpt_file)\n      copyfile(ckpt_file, os.path.join(save_path, file_name))\n\n  def save(self, value, current_ckpt):\n    """"""Updates the set of best checkpoints based on the given result.\n\n    Args:\n        value: The value by which to rank the checkpoint.\n    """"""\n    if tf.gfile.IsDirectory(current_ckpt):\n      current_ckpt = tf.train.latest_checkpoint(current_ckpt)\n\n    if current_ckpt.startswith(\'hdfs\'):\n      # Not support hdfs yet. @TODO\n      return\n\n    # The codes next is assumed to work only with the filename, so remove path except file name.\n    current_ckpt = os.path.basename(current_ckpt)\n\n    value = float(value)\n    if not os.path.exists(self.best_checkpoints_file):\n      self._save_best_checkpoints_file({current_ckpt: value})\n      self._keep_ckpt(current_ckpt)\n      return\n\n    best_checkpoints = self._load_best_checkpoints_file()\n\n    if len(best_checkpoints) < self._num_to_keep:\n      best_checkpoints[current_ckpt] = value\n      self._save_best_checkpoints_file(best_checkpoints)\n      self._keep_ckpt(current_ckpt)\n      return\n\n    if self._maximize:\n      should_save = not all(current_best >= value\n                            for current_best in best_checkpoints.values())\n    else:\n      should_save = not all(current_best <= value\n                            for current_best in best_checkpoints.values())\n    if should_save:\n      best_checkpoint_list = self._sort(best_checkpoints)\n\n      worst_checkpoint = os.path.join(self._best_save_path,\n                                      best_checkpoint_list.pop(-1)[0])\n      tf.logging.debug(worst_checkpoint)\n      self._remove_outdated_checkpoint_files(worst_checkpoint)\n\n      best_checkpoints = dict(best_checkpoint_list)\n      best_checkpoints[current_ckpt] = value\n      self._save_best_checkpoints_file(best_checkpoints)\n      self._keep_ckpt(current_ckpt)\n\n    if self._keep_epoch:\n      self._keep_ckpt(current_ckpt, mode=\'periodical\')\n\n  def _save_best_checkpoints_file(self, updated_best_checkpoints):\n    with open(self.best_checkpoints_file, \'w\') as f:\n      json.dump(updated_best_checkpoints, f, indent=3)\n\n  def _remove_outdated_checkpoint_files(self, worst_checkpoint):\n    for ckpt_file in glob.glob(worst_checkpoint + \'.*\'):\n      os.remove(ckpt_file)\n\n  def _load_best_checkpoints_file(self):\n    with open(self.best_checkpoints_file, \'r\') as f:\n      best_checkpoints = json.load(f)\n    return best_checkpoints\n\n  def _sort(self, best_checkpoints):\n    best_checkpoints = [\n      (ckpt, best_checkpoints[ckpt])\n      for ckpt in sorted(best_checkpoints,\n                         key=best_checkpoints.get,\n                         reverse=self._maximize)\n    ]\n    return best_checkpoints\n'"
utils/config_utils.py,10,"b'# coding=utf8\n# This code is adapted from the https://github.com/tensorflow/models/tree/master/official/r1/resnet.\n# ==========================================================================================\n# NAVER\xe2\x80\x99s modifications are Copyright 2020 NAVER corp. All rights reserved.\n# ==========================================================================================\n# Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\nfrom tensorflow.core.protobuf import rewriter_config_pb2\nfrom official.utils.misc import distribution_utils\nimport json\nimport os\n\ndef _monkey_patch_org_assert_broadcastable():\n  """"""Monkey-patch `assert_broadcast` op to avoid OOM when enabling XLA.""""""\n  def no_op_assert_broadcastable(weights, values):\n    del weights, values\n    tf.logging.info(\n        \'Using monkey-patched version of assert_broadcastable op, which always \'\n        \'returns an no_op. It should be removed after XLA OOM issue is fixed.\')\n    return tf.constant([], dtype=tf.float32)\n\n  from tensorflow.python.ops import weights_broadcast_ops  # pylint: disable=g-import-not-at-top\n  if not hasattr(weights_broadcast_ops, \'org_assert_broadcastable\'):\n    weights_broadcast_ops.org_assert_broadcastable = (\n        weights_broadcast_ops.assert_broadcastable)\n  weights_broadcast_ops.assert_broadcastable = no_op_assert_broadcastable\n\n\ndef get_session_config(flags_obj):\n  """"""Return config proto according to flag settings, or None to use default.""""""\n  config = tf.ConfigProto(\n    inter_op_parallelism_threads=flags_obj.inter_op_parallelism_threads,\n    intra_op_parallelism_threads=flags_obj.intra_op_parallelism_threads,\n    allow_soft_placement=True)\n  return config\n\ndef get_run_config(flags_obj, flags_core, session_config, num_images_train):\n  distribution_strategy = distribution_utils.get_distribution_strategy(\n    flags_core.get_num_gpus(flags_obj), flags_obj.all_reduce_alg)\n\n  steps_per_epoch = flags_obj.save_checkpoints_epochs \\\n                    * int(num_images_train // int(flags_obj.batch_size))\n\n  run_config = tf.estimator.RunConfig(\n    train_distribute=distribution_strategy, session_config=session_config,\n    keep_checkpoint_max=flags_obj.keep_checkpoint_max,\n    save_checkpoints_steps=int(steps_per_epoch),\n    save_checkpoints_secs=None,\n  )\n  return run_config\n\n\ndef get_epoch_schedule(flags_obj, schedule, num_images):\n  if tf.gfile.Exists(flags_obj.model_dir):\n    ckpt = tf.train.latest_checkpoint(flags_obj.model_dir)\n    if not ckpt:\n      cur_epoch = 0\n    else:\n      global_steps = int(ckpt.split(""-"")[-1])\n      cur_epoch = global_steps // int(num_images[\'train\'] / flags_obj.batch_size)\n  else:\n    cur_epoch = 0\n\n  accumulated_epoch = 0\n  fine_eval_start_epoch = int(flags_obj.train_epochs * flags_obj.ratio_fine_eval)\n  # print(\'fine_eval_start_epoch\', fine_eval_start_epoch)\n  # print(\'cur_epoch\', cur_epoch)\n  new_schedule = []\n  for num_train_epochs in schedule:\n    #   print(num_train_epochs)\n    accumulated_epoch += num_train_epochs\n    if accumulated_epoch <= cur_epoch:\n      continue\n    if accumulated_epoch > fine_eval_start_epoch:\n      for i in range(num_train_epochs):\n        new_schedule.append(1)\n    else:\n      new_schedule.append(num_train_epochs)\n  return new_schedule\n\n\ndef dump_hparam():\n  flags_dict = tf.app.flags.FLAGS.flag_values_dict()\n  tf.logging.info(flags_dict[\'model_dir\'])\n  tf.gfile.MakeDirs(flags_dict[\'model_dir\'])\n  with tf.gfile.Open(os.path.join(flags_dict[\'model_dir\'], ""hparams.json""), ""w"") as out:\n    json.dump(flags_dict, out)\n'"
utils/data_util.py,65,"b'#/usr/bin/env python\n# coding=utf8\n# This code is adapted from the https://github.com/tensorflow/models/tree/master/official/r1/resnet.\n# ==========================================================================================\n# NAVER\xe2\x80\x99s modifications are Copyright 2020 NAVER corp. All rights reserved.\n# ==========================================================================================\n# Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport os\n\nimport six\nimport re\nimport tensorflow as tf\n\nfrom preprocessing import imagenet_preprocessing\nfrom preprocessing import inception_preprocessing\nfrom preprocessing import reid_preprocessing\n\ndef get_tf_version():\n  tf_version = tf.VERSION\n  tf_major_version, tf_minor_version, _ = tf_version.split(\'.\')\n  return int(tf_major_version), int(tf_minor_version)\n\ndef int64_feature(values):\n  """"""Wrapper for inserting int64 features into Example proto.""""""\n  if not isinstance(values, (tuple, list)):\n    values = [values]\n  return tf.train.Feature(int64_list=tf.train.Int64List(value=values))\n\n\ndef bytes_feature(values):\n  """"""Wrapper for inserting bytes features into Example proto.""""""\n  if six.PY3 and isinstance(values, six.text_type):\n    values = six.binary_type(values, encoding=\'utf-8\')\n  return tf.train.Feature(bytes_list=tf.train.BytesList(value=[values]))\n\n\ndef float_feature(values):\n  """"""Wrapper for inserting floats features into Example proto.""""""\n  if not isinstance(values, (tuple, list)):\n    values = [values]\n  return tf.train.Feature(float_list=tf.train.FloatList(value=values))\n\n\ndef convert_to_example(image_data, image_format, class_id, height, width, bbox=None):\n  assert height > 0\n  assert width > 0\n  (xmin, ymin, xmax, ymax) = ([], [], [], [])\n  bbox = [] if bbox is None else bbox\n  for b in bbox:\n    assert len(b) == 4\n    [l.append(p) for l, p in zip([xmin, ymin, xmax, ymax], b)]\n\n  example = tf.train.Example(features=tf.train.Features(feature={\n    \'image/encoded\': bytes_feature(image_data),\n    \'image/height\': int64_feature(height),\n    \'image/width\': int64_feature(width),\n    \'image/class/label\': int64_feature(class_id),\n    \'image/format\': bytes_feature(image_format),\n    \'image/object/bbox/xmin\': float_feature(xmin),\n    \'image/object/bbox/xmax\': float_feature(xmax),\n    \'image/object/bbox/ymin\': float_feature(ymin),\n    \'image/object/bbox/ymax\': float_feature(ymax)\n  }))\n  return example\n\n\ndef convert_to_example_without_bbox(image_data, image_format, class_id, height, width):\n  assert height > 0\n  assert width > 0\n  example = tf.train.Example(features=tf.train.Features(feature={\n    \'image/encoded\': bytes_feature(image_data),\n    \'image/height\': int64_feature(height),\n    \'image/width\': int64_feature(width),\n    \'image/class/label\': int64_feature(class_id),\n    \'image/format\': bytes_feature(image_format),\n  }))\n  return example\n\n\ndef mixup(x, y, alpha=0.2, keep_batch_size=True, y_t=None):\n  dist = tf.contrib.distributions.Beta(alpha, alpha)\n\n  _, h, w, c = x.get_shape().as_list()\n\n  batch_size = tf.shape(x)[0]\n  num_class = y.get_shape().as_list()[1]\n\n  lam1 = dist.sample([batch_size // 2])\n\n  if x.dtype == tf.float16:\n    lam1 = tf.cast(lam1, dtype=tf.float16)\n    y = tf.cast(y, dtype=tf.float16)\n    if y_t is not None:\n      y_t = tf.cast(y_t, dtype=tf.float16)\n\n  x1, x2 = tf.split(x, 2, axis=0)\n  y1, y2 = tf.split(y, 2, axis=0)\n\n  lam1_x = tf.tile(tf.reshape(lam1, [batch_size // 2, 1, 1, 1]), [1, h, w, c])\n  lam1_y = tf.tile(tf.reshape(lam1, [batch_size // 2, 1]), [1, num_class])\n\n  mixed_sx1 = lam1_x * x1 + (1. - lam1_x) * x2\n  mixed_sy1 = lam1_y * y1 + (1. - lam1_y) * y2\n  mixed_sx1 = tf.stop_gradient(mixed_sx1)\n  mixed_sy1 = tf.stop_gradient(mixed_sy1)\n\n  if y_t is not None:\n    y1_t, y2_t = tf.split(y_t, 2, axis=0)\n    mixed_sy1_t = lam1_y * y1_t + (1. - lam1_y) * y2_t\n    mixed_sy1_t = tf.stop_gradient(mixed_sy1_t)\n  else:\n    mixed_sy1_t = None\n\n  if keep_batch_size:\n    lam2 = dist.sample([batch_size // 2])\n\n    if x.dtype == tf.float16:\n      lam2 = tf.cast(lam2, dtype=tf.float16)\n\n    lam2_x = tf.tile(tf.reshape(lam2, [batch_size // 2, 1, 1, 1]), [1, h, w, c])\n    lam2_y = tf.tile(tf.reshape(lam2, [batch_size // 2, 1]), [1, num_class])\n\n    x3 = tf.reverse(x2, [0])\n    y3 = tf.reverse(y2, [0])\n\n    mixed_sx2 = lam2_x * x1 + (1. - lam2_x) * x3\n    mixed_sy2 = lam2_y * y1 + (1. - lam2_y) * y3\n\n    mixed_sx2 = tf.stop_gradient(mixed_sx2)\n    mixed_sy2 = tf.stop_gradient(mixed_sy2)\n\n    mixed_sx1 = tf.concat([mixed_sx1, mixed_sx2], axis=0)\n    mixed_sy1 = tf.concat([mixed_sy1, mixed_sy2], axis=0)\n\n    if y_t is not None:\n      y3_t = tf.reverse(y2_t, [0])\n      mixed_sy2_t = lam2_y * y1 + (1. - lam2_y) * y3_t\n      mixed_sy2_t = tf.stop_gradient(mixed_sy2_t)\n      mixed_sy1_t = tf.concat([mixed_sy1_t, mixed_sy2_t], axis=0)\n\n  return mixed_sx1, mixed_sy1, mixed_sy1_t\n\n\ndef get_filenames(is_training, data_dir, train_regex=\'train-*\', val_regex=\'validation-*\'):\n  """"""Return filenames for dataset.""""""\n  if is_training:\n    path = os.path.join(data_dir, train_regex)\n    matching_files = tf.gfile.Glob(path)\n    return matching_files\n  else:\n    path = os.path.join(data_dir, val_regex)\n    matching_files = tf.gfile.Glob(path)\n    return matching_files\n\n\ndef parse_example_proto(example_serialized, ret_dict=False):\n  # Dense features in Example proto.\n  feature_map = {\n    \'image/encoded\': tf.FixedLenFeature([], dtype=tf.string, default_value=\'\'),\n    \'image/filename\': tf.FixedLenFeature([], dtype=tf.string, default_value=\'\'),\n    \'image/class/label\': tf.FixedLenFeature([], dtype=tf.int64, default_value=-1),\n    \'image/logit\': tf.VarLenFeature(dtype=tf.float32)\n  }\n  sparse_float32 = tf.VarLenFeature(dtype=tf.float32)\n  # Sparse features in Example proto.\n  feature_map.update(\n    {k: sparse_float32 for k in [\'image/object/bbox/xmin\',\n                                 \'image/object/bbox/ymin\',\n                                 \'image/object/bbox/xmax\',\n                                 \'image/object/bbox/ymax\']})\n\n  features = tf.parse_single_example(example_serialized, feature_map)\n  label = tf.cast(features[\'image/class/label\'], dtype=tf.int32)\n\n  xmin = tf.expand_dims(features[\'image/object/bbox/xmin\'].values, 0)\n  ymin = tf.expand_dims(features[\'image/object/bbox/ymin\'].values, 0)\n  xmax = tf.expand_dims(features[\'image/object/bbox/xmax\'].values, 0)\n  ymax = tf.expand_dims(features[\'image/object/bbox/ymax\'].values, 0)\n\n  # Note that we impose an ordering of (y, x) just to make life difficult.\n  bbox = tf.concat([ymin, xmin, ymax, xmax], 0)\n\n  # Force the variable number of bounding boxes into the shape\n  # [1, num_boxes, coords].\n  bbox = tf.expand_dims(bbox, 0)\n  bbox = tf.transpose(bbox, [0, 2, 1])\n\n  if ret_dict:\n    return features\n  else:\n    return features[\'image/encoded\'], label, bbox, features[\'image/filename\'], features[\'image/logit\'].values\n\ndef parse_record_sup(raw_record, is_training, num_channels, dtype,\n                     use_random_crop=True, image_size=224,\n                     autoaugment_type=None, with_drawing_bbox=False,\n                     dct_method="""", preprocessing_type=\'imagenet\',\n                     return_logits=False, num_classes=1001, return_filename=False):\n  features = parse_example_proto(raw_record, ret_dict=True)\n  results = {}\n  sup_image = preprocess_image(image_buffer=features[\'image/encoded\'],\n                               is_training=is_training,\n                               num_channels=num_channels,\n                               dtype=dtype,\n                               use_random_crop=use_random_crop,\n                               image_size=image_size,\n                               autoaugment_type=autoaugment_type,\n                               dct_method=dct_method,\n                               preprocessing_type=preprocessing_type)\n\n  results[\'image\'] = sup_image\n  if return_logits:\n    label = tf.one_hot(tf.cast(features[\'image/class/label\'], dtype=tf.int32), num_classes)\n    logit = tf.reshape(features[\'image/logit\'].values, [num_classes])\n    label = tf.concat([label, logit], axis=0)\n  else:\n    label = tf.cast(features[\'image/class/label\'], dtype=tf.int32)\n  results[\'label\'] = label\n  return results\n\ndef parse_record(raw_record, is_training, num_channels, dtype,\n                 use_random_crop=True, image_size=224,\n                 autoaugment_type=None, with_drawing_bbox=False,\n                 dct_method="""", preprocessing_type=\'imagenet\',\n                 return_logits=False, num_classes=1001, return_filename=False):\n\n  image_buffer, label, bbox, filename, logit = parse_example_proto(raw_record)\n  if return_logits:\n    assert num_classes == 1001, \'Only support ImageNet for Knowledge Distillation yet\'\n    label = tf.one_hot(label, num_classes)\n    logit = tf.reshape(logit, [num_classes])\n    label = tf.concat([label, logit], axis=0)\n\n  image = preprocess_image(image_buffer=image_buffer,\n                           is_training=is_training,\n                           num_channels=num_channels,\n                           dtype=dtype,\n                           use_random_crop=use_random_crop,\n                           image_size=image_size,\n                           bbox=bbox,\n                           autoaugment_type=autoaugment_type,\n                           with_drawing_bbox=with_drawing_bbox,\n                           dct_method=dct_method,\n                           preprocessing_type=preprocessing_type)\n\n  if return_filename:\n    return image, label, filename\n  else:\n    return image, label\n\ndef preprocess_image(image_buffer, is_training, num_channels, dtype,\n                     use_random_crop=True, image_size=224, bbox=None,\n                     autoaugment_type=None, with_drawing_bbox=False,\n                     dct_method="""", decoder_name=\'jpeg\',\n                     preprocessing_type=\'imagenet\'):\n  raw_image_with_bbox = None\n  bbox = tf.zeros([0, 0, 4], tf.float32) if bbox is None else bbox\n  crop_type_check = re.compile(\'imagenet_[0-9]{3}\')\n  crop_type_check_a = re.compile(\'imagenet_[0-9]{3}a\')\n  if preprocessing_type == \'imagenet\':\n    image, raw_image_with_bbox = imagenet_preprocessing.preprocess_image(\n      image_buffer=image_buffer,\n      bbox=bbox,\n      output_height=image_size,\n      output_width=image_size,\n      num_channels=num_channels,\n      is_training=is_training,\n      use_random_crop=use_random_crop,\n      autoaugment_type=autoaugment_type,\n      dct_method=dct_method,\n      with_drawing_bbox=with_drawing_bbox)\n  elif preprocessing_type == \'imagenet_224_256\':\n    # 224\xeb\xa1\x9c \xed\x95\x99\xec\x8a\xb5\xed\x95\x98\xea\xb3\xa0 256 \xed\x95\xb4\xec\x83\x81\xeb\x8f\x84\xeb\xa1\x9c \xed\x8f\x89\xea\xb0\x80 type1\n    image, raw_image_with_bbox = imagenet_preprocessing.preprocess_image(\n      image_buffer=image_buffer,\n      bbox=bbox,\n      output_height=224 if is_training else 256,\n      output_width=224 if is_training else 256,\n      num_channels=num_channels,\n      is_training=is_training,\n      use_random_crop=use_random_crop,\n      autoaugment_type=autoaugment_type,\n      with_drawing_bbox=with_drawing_bbox,\n      dct_method=dct_method,\n      crop_type=0,\n    )\n  elif preprocessing_type == \'imagenet_224_256a\':\n    # 224\xeb\xa1\x9c \xed\x95\x99\xec\x8a\xb5\xed\x95\x98\xea\xb3\xa0 256 \xed\x95\xb4\xec\x83\x81\xeb\x8f\x84\xeb\xa1\x9c \xed\x8f\x89\xea\xb0\x80 type2\n    image, raw_image_with_bbox = imagenet_preprocessing.preprocess_image(\n      image_buffer=image_buffer,\n      bbox=bbox,\n      output_height=224 if is_training else 256,\n      output_width=224 if is_training else 256,\n      num_channels=num_channels,\n      is_training=is_training,\n      use_random_crop=use_random_crop,\n      autoaugment_type=autoaugment_type,\n      with_drawing_bbox=with_drawing_bbox,\n      dct_method=dct_method,\n      crop_type=1,\n    )\n  elif crop_type_check_a.match(preprocessing_type):\n    image_size = int(preprocessing_type.split(""_"")[1][0:3])\n    image, raw_image_with_bbox = imagenet_preprocessing.preprocess_image(\n      image_buffer=image_buffer,\n      bbox=bbox,\n      output_height=image_size,\n      output_width=image_size,\n      num_channels=num_channels,\n      is_training=is_training,\n      use_random_crop=use_random_crop,\n      autoaugment_type=autoaugment_type,\n      with_drawing_bbox=with_drawing_bbox,\n      dct_method=dct_method,\n      crop_type=1)\n  elif crop_type_check.match(preprocessing_type):\n    image_size = int(preprocessing_type.split(""_"")[1])\n    image, raw_image_with_bbox = imagenet_preprocessing.preprocess_image(\n      image_buffer=image_buffer,\n      bbox=bbox,\n      output_height=image_size,\n      output_width=image_size,\n      num_channels=num_channels,\n      is_training=is_training,\n      use_random_crop=use_random_crop,\n      autoaugment_type=autoaugment_type,\n      with_drawing_bbox=with_drawing_bbox,\n      dct_method=dct_method,\n      crop_type=0)\n  elif preprocessing_type == \'reid\':\n    image = reid_preprocessing.preprocess_image(\n      image_buffer=image_buffer,\n      output_height=image_size,\n      output_width=image_size,\n      num_channels=num_channels,\n      is_training=is_training,\n      dct_method=dct_method,\n      autoaugment_type=autoaugment_type)\n  elif preprocessing_type == \'reid_224\':\n    image = reid_preprocessing.preprocess_image(\n      image_buffer=image_buffer,\n      output_height=image_size,\n      output_width=image_size,\n      num_channels=num_channels,\n      is_training=is_training,\n      autoaugment_type=autoaugment_type,\n      dct_method=dct_method,\n      eval_large_resolution=False)\n  elif preprocessing_type == \'inception_331\':\n    image = inception_preprocessing.preprocess_image(\n      image=image_buffer,\n      output_height=331,\n      output_width=331,\n      scaled_images=False,\n      is_training=is_training)\n  elif preprocessing_type == \'inception_600\':\n    image = inception_preprocessing.preprocess_image(\n      image=image_buffer,\n      output_height=600,\n      output_width=600,\n      scaled_images=False,\n      is_training=is_training)\n  else:\n    raise NotImplementedError\n\n  image = tf.cast(image, dtype)\n  if with_drawing_bbox and not raw_image_with_bbox:\n    image = tf.stack([image, raw_image_with_bbox])\n\n  return image\n\n\ndef process_record_dataset(dataset, is_training, batch_size, shuffle_buffer, num_channels, parse_record_fn, num_epochs,\n                           num_gpus, examples_per_epoch, dtype, use_random_crop=False, with_drawing_bbox=False,\n                           autoaugment_type=None, dct_method=\'\', preprocessing_type=\'imagenet\', drop_remainder=False,\n                           return_logits=False, return_filename=False):\n\n  # Disable intra-op parallelism to optimize for throughput instead of latency.\n  tf_major_version, tf_minor_version = get_tf_version()\n  if tf_major_version == 1 and tf_minor_version <= 12:\n    pass\n  else:\n    options = tf.data.Options()\n    options.experimental_threading.max_intra_op_parallelism = 1\n    dataset = dataset.with_options(options)\n\n  # We prefetch a batch at a time, This can help smooth out the time taken to\n  # load input files as we go through shuffling and processing.\n  dataset = dataset.prefetch(buffer_size=batch_size)\n\n  if is_training:\n    # Shuffle the records. Note that we shuffle before repeating to ensure\n    # that the shuffling respects epoch boundaries.\n    dataset = dataset.shuffle(buffer_size=shuffle_buffer)\n\n  # If we are training over multiple epochs before evaluating, repeat the\n  # dataset for the appropriate number of epochs.\n  dataset = dataset.repeat(num_epochs)\n\n  if is_training and num_gpus and examples_per_epoch:\n    total_examples = num_epochs * examples_per_epoch\n    # Force the number of batches to be divisible by the number of devices.\n    # This prevents some devices from receiving batches while others do not,\n    # which can lead to a lockup. This case will soon be handled directly by\n    # distribution strategies, at which point this .take() operation will no\n    # longer be needed.\n    total_batches = total_examples // batch_size // num_gpus * num_gpus\n    dataset.take(total_batches * batch_size)\n\n  # Parse the raw records into images and labels. Testing has shown that setting\n  # num_parallel_batches > 1 produces no improvement in throughput, since\n  # batch_size is almost always much greater than the number of CPU cores.\n  if tf_major_version == 1 and tf_minor_version <= 12:\n    dataset = dataset.apply(\n      tf.contrib.data.map_and_batch(\n        lambda value: parse_record_fn(value,\n                                      is_training,\n                                      num_channels,\n                                      dtype,\n                                      dct_method=dct_method,\n                                      use_random_crop=use_random_crop,\n                                      autoaugment_type=autoaugment_type,\n                                      preprocessing_type=preprocessing_type,\n                                      with_drawing_bbox=with_drawing_bbox,\n                                      return_logits=return_logits,\n                                      return_filename=return_filename),\n        batch_size=batch_size,\n        num_parallel_batches=1,\n        drop_remainder=drop_remainder))\n  else:\n    dataset = dataset.map(\n      lambda value: parse_record_fn(value,\n                                    is_training,\n                                    num_channels,\n                                    dtype,\n                                    dct_method=dct_method,\n                                    use_random_crop=use_random_crop,\n                                    autoaugment_type=autoaugment_type,\n                                    preprocessing_type=preprocessing_type,\n                                    with_drawing_bbox=with_drawing_bbox,\n                                    return_logits=return_logits,\n                                    return_filename=return_filename),\n      num_parallel_calls=tf.data.experimental.AUTOTUNE)\n    dataset = dataset.batch(batch_size, drop_remainder=drop_remainder)\n\n  # Operations between the final prefetch and the get_next call to the iterator\n  # will happen synchronously during run time. We prefetch here again to\n  # background all of the above processing work and keep it out of the\n  # critical training path. Setting buffer_size to tf.contrib.data.AUTOTUNE\n  # allows DistributionStrategies to adjust how many batches to fetch based\n  # on how many devices are present.\n  if tf_major_version == 1 and tf_minor_version <= 12:\n    dataset = dataset.prefetch(buffer_size=tf.contrib.data.AUTOTUNE)\n  else:\n    dataset = dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n\n  return dataset\n'"
utils/export_utils.py,24,"b'# coding=utf8\n# This code is adapted from the https://github.com/tensorflow/models/tree/master/official/r1/resnet.\n# ==========================================================================================\n# NAVER\xe2\x80\x99s modifications are Copyright 2020 NAVER corp. All rights reserved.\n# ==========================================================================================\n# Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport functools\nimport os\n\nimport tensorflow as tf\n\nfrom official.utils.export import export\nfrom utils import data_util\nfrom functions import data_config\nimport numpy as np\nfrom tqdm import tqdm\n\n\ndef export_test(bin_export_path, flags_obj, ir_eval):\n  ds = tf.data.Dataset.list_files(flags_obj.data_dir + \'/\' + flags_obj.val_regex)\n  ds = ds.interleave(tf.data.TFRecordDataset, cycle_length=10)\n\n  def parse_tfr(example_proto):\n    feature_def = {\'image/class/label\': tf.FixedLenFeature([], dtype=tf.int64, default_value=-1),\n                   \'image/encoded\': tf.FixedLenFeature([], dtype=tf.string, default_value=\'\')}\n    features = tf.io.parse_single_example(serialized=example_proto, features=feature_def)\n    return features[\'image/encoded\'], features[\'image/class/label\']\n\n  ds = ds.map(parse_tfr)\n  ds = ds.batch(flags_obj.val_batch_size)\n  iterator = ds.make_one_shot_iterator()\n  images, labels = iterator.get_next()\n  dconf = data_config.get_config(flags_obj.dataset_name)\n  num_val_images = dconf.num_images[\'validation\']\n  if flags_obj.zeroshot_eval or ir_eval:\n    feature_dim = flags_obj.embedding_size if flags_obj.embedding_size > 0 else flags_obj.num_features\n    np_features = np.zeros((num_val_images, feature_dim), dtype=np.float32)\n    np_labels = np.zeros(num_val_images, dtype=np.int64)\n    np_i = 0\n    with tf.Session() as sess:\n      sess.run(tf.global_variables_initializer())\n      sess.run(tf.local_variables_initializer())\n      tf.saved_model.load(sess=sess, export_dir=bin_export_path, tags={""serve""})\n      for _ in tqdm(range(int(num_val_images / flags_obj.val_batch_size) + 1)):\n        try:\n          np_image, np_label = sess.run([images, labels])\n          np_predict = sess.run(\'embedding_tensor:0\',\n                                feed_dict={\'input_tensor:0\': np_image})\n          np_features[np_i:np_i + np_predict.shape[0], :] = np_predict\n          np_labels[np_i:np_i + np_label.shape[0]] = np_label\n          np_i += np_predict.shape[0]\n\n        except tf.errors.OutOfRangeError:\n          break\n      assert np_i == num_val_images\n\n    from sklearn.preprocessing import normalize\n\n    x = normalize(np_features)\n    np_sim = x.dot(x.T)\n    np.fill_diagonal(np_sim, -10)  # removing similarity for query.\n    num_correct = 0\n    for i in range(num_val_images):\n      cur_label = np_labels[i]\n      rank1_label = np_labels[np.argmax(np_sim[i, :])]\n      if rank1_label == cur_label:\n        num_correct += 1\n    recall_at_1 = num_correct / num_val_images\n    metric = recall_at_1\n  else:\n    np_i = 0\n    correct_cnt = 0\n    with tf.Session() as sess:\n      sess.run(tf.global_variables_initializer())\n      sess.run(tf.local_variables_initializer())\n      tf.saved_model.load(sess=sess, export_dir=bin_export_path, tags={""serve""})\n      for _ in tqdm(range(int(num_val_images / flags_obj.val_batch_size) + 1)):\n        try:\n          np_image, np_label = sess.run([images, labels])\n          np_predict = sess.run(\'ArgMax:0\',\n                                feed_dict={\'input_tensor:0\': np_image})\n          np_i += np_predict.shape[0]\n          correct_cnt += np.sum(np_predict == np_label)\n        except tf.errors.OutOfRangeError:\n          break\n      assert np_i == num_val_images\n\n      metric = correct_cnt / np_i\n  return metric\n\ndef image_bytes_serving_input_fn(image_shape, decoder_name, dtype=tf.float32, pptype=\'imagenet\'):\n  """"""Serving input fn for raw jpeg images.""""""\n\n  def _preprocess_image(image_bytes):\n    """"""Preprocess a single raw image.""""""\n    # Bounding box around the whole image.\n    bbox = tf.constant([0.0, 0.0, 1.0, 1.0], dtype=dtype, shape=[1, 1, 4])\n    _, _, num_channels = image_shape\n    tf.logging.info(""!!!!!!!!!! Preprocessing type for exporting pb: {} and decoder type: {}"".format(pptype, decoder_name))\n    image = data_util.preprocess_image(\n      image_buffer=image_bytes, is_training=False, bbox=bbox,\n      num_channels=num_channels, dtype=dtype, use_random_crop=False,\n      decoder_name=decoder_name, dct_method=\'INTEGER_ACCURATE\', preprocessing_type=pptype)\n    return image\n\n  image_bytes_list = tf.placeholder(\n    shape=[None], dtype=tf.string, name=\'input_tensor\')\n  images = tf.map_fn(\n    _preprocess_image, image_bytes_list, back_prop=False, dtype=dtype)\n  return tf.estimator.export.TensorServingInputReceiver(\n    images, {\'image_bytes\': image_bytes_list})\n\n\ndef export_pb(flags_core, flags_obj, shape, classifier, ir_eval=False):\n  export_dtype = flags_core.get_tf_dtype(flags_obj)\n\n  if not flags_obj.data_format:\n    raise ValueError(\'The `data_format` must be specified: channels_first or channels_last \')\n\n  bin_export_path = os.path.join(flags_obj.export_dir, flags_obj.data_format, \'binary_input\')\n  bin_input_receiver_fn = functools.partial(image_bytes_serving_input_fn, shape, flags_obj.export_decoder_type,\n                                            dtype=export_dtype, pptype=flags_obj.preprocessing_type)\n\n  pp_export_path = os.path.join(flags_obj.export_dir, flags_obj.data_format, \'preprocessed_input\')\n  pp_input_receiver_fn = export.build_tensor_serving_input_receiver_fn(\n    shape, batch_size=None, dtype=export_dtype)\n\n  result_bin_export_path = classifier.export_savedmodel(bin_export_path, bin_input_receiver_fn)\n  classifier.export_savedmodel(pp_export_path, pp_input_receiver_fn)\n\n  if flags_obj.export_decoder_type == \'jpeg\':\n    metric = export_test(result_bin_export_path, flags_obj, ir_eval)\n    msg = \'IMPOTANT! Evaluation metric of exported saved_model.pb is {}\'.format(metric)\n    tf.logging.info(msg)\n    with tf.gfile.Open(result_bin_export_path.decode(""utf-8"") + \'/model_performance.txt\', \'w\') as fp:\n      fp.write(msg)\n'"
utils/hook_utils.py,8,"b'# coding=utf8\n# ==============================================================================\n# Copyright 2020-present NAVER Corp.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\n\ntry:\n  from functions import model_fns\nexcept:\n  from functions import *\nfrom utils import log_utils\n\nclass WarmStartHook(tf.train.SessionRunHook):\n  def __init__(self, checkpoint_path):\n    self.checkpoint_path = checkpoint_path\n    self.initialized = False\n    self.var_list_warm_start = []\n    self.saver = None\n\n  def begin(self):\n    var_list_all = tf.contrib.framework.get_trainable_variables()\n\n    for v in var_list_all:\n      # Dense layers in se_block is included in warm-start variables.\n      if \'dense\' in v.name and not \'se_block\' in v.name:\n        continue\n      else:\n        self.var_list_warm_start.append(v)\n\n    if tf.gfile.IsDirectory(self.checkpoint_path):\n      self.checkpoint_path = tf.train.latest_checkpoint(self.checkpoint_path)\n\n    self.saver = tf.train.Saver(self.var_list_warm_start)\n\n  def after_create_session(self, session, coord=None):\n    tf.logging.info(\'Session created.\')\n    if self.checkpoint_path and session.run(tf.train.get_or_create_global_step()) == 0:\n      log_utils.log_var_list_by_line(self.var_list_warm_start, \'var_list_warm_start\')\n      tf.logging.info(\'Fine-tuning from %s\' % self.checkpoint_path)\n      self.saver.restore(session, self.checkpoint_path)\n\n'"
utils/log_utils.py,4,"b'# ==============================================================================\n# Copyright 2020-present NAVER Corp.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport logging\nimport sys\n\nimport tensorflow as tf\n\n\ndef log_var_list_by_line(var_list, var_list_name):\n  tf.logging.info(\'*********** begin %s **************\', var_list_name)\n  for v in var_list:\n    tf.logging.info(v.name)\n  tf.logging.info(\'*********** end   %s **************\', var_list_name)\n\n\ndef define_log_level():\n  tf_logger = logging.getLogger(\'tensorflow\')\n  tf_logger.propagate = False\n  handler = logging.StreamHandler(sys.stdout)\n  formatter = logging.Formatter(""%(asctime)s.%(msecs).3d %(levelname).1s: %(message)s"", datefmt=""%Y-%m-%d %H:%M:%S"")\n  handler.setFormatter(formatter)\n  tf_logger.handlers = [handler]\n  tf_logger.setLevel(tf.logging.INFO)\n'"
datasets/CE_dataset/CE_validation_generator.py,0,"b'# -*- coding: utf-8 -*-\n# This code is adapted from the https://github.com/hendrycks/robustness.\n# ==========================================================================================\n# NAVER\xe2\x80\x99s modifications are Copyright 2020 NAVER corp. All rights reserved.\n# ==========================================================================================\n\nimport os\nfrom PIL import Image\nimport os.path\nimport time\nimport numpy as np\nimport PIL\nfrom multiprocessing import Pool\nimport argparse\n\n# RESIZE_SIZE = 256\n# CROP_SIZE = 256\n\nparser = argparse.ArgumentParser()\n\nparser.add_argument(\'-v\', \'--validation_dir\', type=str, default=None, help=\'Validation data directory.\')\nparser.add_argument(\'-o\', \'--output_dir\', type=str, default=None, help=\'Output data directory.\')\nparser.add_argument(\'-f\', \'--frost_dir\', type=str, default=\'./frost\', help=\'frost img file directory.\')\nparser.add_argument(\'--num_workers\', type=int, default=20, help=\'Number of processes to preprocess the images.\')\nparser.add_argument(\'--RESIZE_SIZE\', type=int, default=256, help=\'Resize size\')\nparser.add_argument(\'--CROP_SIZE\', type=int, default=224, help=\'Center crop size\')\n\nargs = parser.parse_args()\n# /////////////// Data Loader ///////////////\n\n\nIMG_EXTENSIONS = [\'.jpg\', \'.jpeg\', \'.png\', \'.ppm\', \'.bmp\', \'.pgm\', \'.gif\']\n\n\ndef is_image_file(filename):\n  """"""Checks if a file is an image.\n  Args:\n    filename (string): path to a file\n  Returns:\n    bool: True if the filename ends with a known image extension\n  """"""\n  filename_lower = filename.lower()\n  return any(filename_lower.endswith(ext) for ext in IMG_EXTENSIONS)\n\ndef make_dataset(directory):\n  images = []\n  directory = os.path.expanduser(directory)\n  for target in sorted(os.listdir(directory)):\n    d = os.path.join(directory, target)\n    if not os.path.isdir(d):\n      continue\n\n    for root, _, fnames in sorted(os.walk(d)):\n      for fname in sorted(fnames):\n        if is_image_file(fname):\n          path = os.path.join(root, fname)\n          item = (path, target)\n          images.append(item)\n\n  return images\n\n\ndef count_dataset(directory):\n  img_cnt = 0\n  directory = os.path.expanduser(directory)\n  for target in sorted(os.listdir(directory)):\n    d = os.path.join(directory, target)\n    if not os.path.isdir(d):\n      continue\n    \n    for root, _, fnames in sorted(os.walk(d)):\n      for fname in sorted(fnames):\n        if is_image_file(fname):\n          img_cnt += 1\n\n  return img_cnt\n\n\ndef pil_loader(path):\n  # open path as file to avoid ResourceWarning (https://github.com/python-pillow/Pillow/issues/835)\n  with open(path, \'rb\') as f:\n    img = Image.open(f)\n    return img.convert(\'RGB\')\n\n\ndef default_loader(path):\n  return pil_loader(path)\n\n\ndef resize_and_center_crop(img, resize_size, crop_size):\n  w = img.size[0]\n  h = img.size[1]\n  # resize image\n  if h > w:\n    new_h = int(resize_size * (h/w))\n    new_w = resize_size\n  else:\n    new_h = resize_size\n    new_w = int(resize_size * (w/h))\n  \n  resized_img = img.resize((new_w, new_h), resample=PIL.Image.BILINEAR)\n  # crop image\n  h_start = int((new_h-crop_size)/2)\n  w_start = int((new_w-crop_size)/2)\n  cropped_img = resized_img.crop((w_start, h_start,\n                                  w_start+crop_size, h_start+crop_size))\n  \n  return cropped_img\n\n\nclass DistortImageFolder():\n  def __init__(self, root, method, severity, start_idx, end_idx,\n               transform=\'imagenet\', loader=default_loader):\n    imgs = make_dataset(root)\n    imgs = imgs[start_idx:end_idx]\n    if len(imgs) == 0:\n      raise (RuntimeError(""Found 0 images in subfolders of: "" + root + ""\\n""\n                                       ""Supported image extensions are: "" + "","".join(\n        IMG_EXTENSIONS)))\n\n    self.root = root\n    self.method = method\n    self.severity = severity\n    self.imgs = imgs\n    self.transform = transform\n    self.loader = loader\n\n  def __getitem__(self, index):\n    path, target = self.imgs[index]\n    img = self.loader(path)\n    # default trasformation is set to imagenet preprocessing\n    if self.transform == \'imagenet\':\n      img = resize_and_center_crop(img, resize_size=args.RESIZE_SIZE, crop_size=args.CROP_SIZE)\n    img = self.method(img, self.severity)\n\n    save_path = os.path.join(args.output_dir, self.method.__name__,\n                             str(self.severity), target)\n\n    if not os.path.exists(save_path):\n      os.makedirs(save_path)\n\n    save_path += path[path.rindex(\'/\'):]\n\n    Image.fromarray(np.uint8(img)).save(save_path, quality=85, optimize=True)\n\n    return 0  # we do not care about returning the data\n\n  def __len__(self):\n    return len(self.imgs)\n\n\n# /////////////// Distortion Helpers ///////////////\n\nimport skimage as sk\nfrom skimage.filters import gaussian\nfrom io import BytesIO\nfrom wand.image import Image as WandImage\nfrom wand.api import library as wandlibrary\nimport wand.color as WandColor\nimport ctypes\nfrom PIL import Image as PILImage\nimport cv2\nfrom scipy.ndimage import zoom as scizoom\nfrom scipy.ndimage.interpolation import map_coordinates\nimport warnings\n\nwarnings.simplefilter(""ignore"", UserWarning)\n\n\ndef auc(errs):  # area under the alteration error curve\n  area = 0\n  for i in range(1, len(errs)):\n    area += (errs[i] + errs[i - 1]) / 2\n  area /= len(errs) - 1\n  return area\n\n\ndef disk(radius, alias_blur=0.1, dtype=np.float32):\n  if radius <= 8:\n    L = np.arange(-8, 8 + 1)\n    ksize = (3, 3)\n  else:\n    L = np.arange(-radius, radius + 1)\n    ksize = (5, 5)\n  X, Y = np.meshgrid(L, L)\n  aliased_disk = np.array((X ** 2 + Y ** 2) <= radius ** 2, dtype=dtype)\n  aliased_disk /= np.sum(aliased_disk)\n\n  # supersample disk to antialias\n  return cv2.GaussianBlur(aliased_disk, ksize=ksize, sigmaX=alias_blur)\n\n\n# Tell Python about the C method\nwandlibrary.MagickMotionBlurImage.argtypes = (ctypes.c_void_p,  # wand\n                        ctypes.c_double,  # radius\n                        ctypes.c_double,  # sigma\n                        ctypes.c_double)  # angle\n\n\n# Extend wand.image.Image class to include method signature\nclass MotionImage(WandImage):\n  def motion_blur(self, radius=0.0, sigma=0.0, angle=0.0):\n    wandlibrary.MagickMotionBlurImage(self.wand, radius, sigma, angle)\n\n\n# modification of https://github.com/FLHerne/mapgen/blob/master/diamondsquare.py\ndef plasma_fractal(mapsize=256, wibbledecay=3):\n  """"""\n  Generate a heightmap using diamond-square algorithm.\n  Return square 2d array, side length \'mapsize\', of floats in range 0-255.\n  \'mapsize\' must be a power of two.\n  """"""\n  assert (mapsize & (mapsize - 1) == 0)\n  maparray = np.empty((mapsize, mapsize), dtype=np.float_)\n  maparray[0, 0] = 0\n  stepsize = mapsize\n  wibble = 100\n\n  def wibbledmean(array):\n    return array / 4 + wibble * np.random.uniform(-wibble, wibble, array.shape)\n\n  def fillsquares():\n    """"""For each square of points stepsize apart,\n       calculate middle value as mean of points + wibble""""""\n    cornerref = maparray[0:mapsize:stepsize, 0:mapsize:stepsize]\n    squareaccum = cornerref + np.roll(cornerref, shift=-1, axis=0)\n    squareaccum += np.roll(squareaccum, shift=-1, axis=1)\n    maparray[stepsize // 2:mapsize:stepsize,\n    stepsize // 2:mapsize:stepsize] = wibbledmean(squareaccum)\n\n  def filldiamonds():\n    """"""For each diamond of points stepsize apart,\n       calculate middle value as mean of points + wibble""""""\n    mapsize = maparray.shape[0]\n    drgrid = maparray[stepsize // 2:mapsize:stepsize, stepsize // 2:mapsize:stepsize]\n    ulgrid = maparray[0:mapsize:stepsize, 0:mapsize:stepsize]\n    ldrsum = drgrid + np.roll(drgrid, 1, axis=0)\n    lulsum = ulgrid + np.roll(ulgrid, -1, axis=1)\n    ltsum = ldrsum + lulsum\n    maparray[0:mapsize:stepsize, stepsize // 2:mapsize:stepsize] = wibbledmean(ltsum)\n    tdrsum = drgrid + np.roll(drgrid, 1, axis=1)\n    tulsum = ulgrid + np.roll(ulgrid, -1, axis=0)\n    ttsum = tdrsum + tulsum\n    maparray[stepsize // 2:mapsize:stepsize, 0:mapsize:stepsize] = wibbledmean(ttsum)\n\n  while stepsize >= 2:\n    fillsquares()\n    filldiamonds()\n    stepsize //= 2\n    wibble /= wibbledecay\n\n  maparray -= maparray.min()\n  return maparray / maparray.max()\n\n\ndef clipped_zoom(img, zoom_factor):\n  h = img.shape[0]\n  w = img.shape[1]\n  # ceil crop height(= crop width)\n  ch = int(np.ceil(h / zoom_factor))\n  cw = int(np.ceil(w / zoom_factor))\n\n  top_h = (h - ch) // 2\n  top_w = (w - cw) // 2\n\n  img = scizoom(img[top_h:top_h + ch, top_w:top_w + cw], (zoom_factor, zoom_factor, 1), order=1)\n  # trim off any extra pixels\n  trim_top_h = (img.shape[0] - h) // 2\n  trim_top_w = (img.shape[1] - w) // 2\n\n  return img[trim_top_h:trim_top_h + h, trim_top_w:trim_top_w + w]\n\n\n# /////////////// End Distortion Helpers ///////////////\n\n\n# /////////////// Distortions ///////////////\n\ndef gaussian_noise(x, severity=1):\n  c = [.08, .12, 0.18, 0.26, 0.38][severity - 1]\n\n  x = np.array(x) / 255.\n  return np.clip(x + np.random.normal(size=x.shape, scale=c), 0, 1) * 255\n\n\ndef shot_noise(x, severity=1):\n  c = [60, 25, 12, 5, 3][severity - 1]\n\n  x = np.array(x) / 255.\n  return np.clip(np.random.poisson(x * c) / c, 0, 1) * 255\n\n\ndef impulse_noise(x, severity=1):\n  c = [.03, .06, .09, 0.17, 0.27][severity - 1]\n\n  x = sk.util.random_noise(np.array(x) / 255., mode=\'s&p\', amount=c)\n  return np.clip(x, 0, 1) * 255\n\n\ndef speckle_noise(x, severity=1):\n  c = [.15, .2, 0.35, 0.45, 0.6][severity - 1]\n\n  x = np.array(x) / 255.\n  return np.clip(x + x * np.random.normal(size=x.shape, scale=c), 0, 1) * 255\n\n\ndef gaussian_blur(x, severity=1):\n  c = [1, 2, 3, 4, 6][severity - 1]\n\n  x = gaussian(np.array(x) / 255., sigma=c, multichannel=True)\n  return np.clip(x, 0, 1) * 255\n\n\ndef glass_blur(x, severity=1):\n  # sigma, max_delta, iterations\n  c = [(0.7, 1, 2), (0.9, 2, 1), (1, 2, 3), (1.1, 3, 2), (1.5, 4, 2)][severity - 1]\n\n  x = np.uint8(gaussian(np.array(x) / 255., sigma=c[0], multichannel=True) * 255)\n\n  # locally shuffle pixels\n  for i in range(c[2]):\n    for h in range(args.CROP_SIZE - c[1], c[1], -1):\n      for w in range(args.CROP_SIZE - c[1], c[1], -1):\n        dx, dy = np.random.randint(-c[1], c[1], size=(2,))\n        h_prime, w_prime = h + dy, w + dx\n        # swap\n        x[h, w], x[h_prime, w_prime] = x[h_prime, w_prime], x[h, w]\n\n  return np.clip(gaussian(x / 255., sigma=c[0], multichannel=True), 0, 1) * 255\n\n\ndef defocus_blur(x, severity=1):\n  c = [(3, 0.1), (4, 0.5), (6, 0.5), (8, 0.5), (10, 0.5)][severity - 1]\n\n  x = np.array(x) / 255.\n  kernel = disk(radius=c[0], alias_blur=c[1])\n\n  channels = []\n  for d in range(3):\n    channels.append(cv2.filter2D(x[:, :, d], -1, kernel))\n  channels = np.array(channels).transpose((1, 2, 0))  # 3xCROP_SIZExCROP_SIZE -> CROP_SIZExCROP_SIZEx3\n\n  return np.clip(channels, 0, 1) * 255\n\n\ndef motion_blur(x, severity=1):\n  c = [(10, 3), (15, 5), (15, 8), (15, 12), (20, 15)][severity - 1]\n\n  output = BytesIO()\n  x.save(output, format=\'PNG\')\n  x = MotionImage(blob=output.getvalue())\n\n  x.motion_blur(radius=c[0], sigma=c[1], angle=np.random.uniform(-45, 45))\n\n  x = cv2.imdecode(np.fromstring(x.make_blob(), np.uint8),\n           cv2.IMREAD_UNCHANGED)\n\n  if x.shape != (args.CROP_SIZE, args.CROP_SIZE):\n    return np.clip(x[..., [2, 1, 0]], 0, 255)  # BGR to RGB\n  else:  # greyscale to RGB\n    return np.clip(np.array([x, x, x]).transpose((1, 2, 0)), 0, 255)\n\n\ndef zoom_blur(x, severity=1):\n  c = [np.arange(1, 1.11, 0.01),\n     np.arange(1, 1.16, 0.01),\n     np.arange(1, 1.21, 0.02),\n     np.arange(1, 1.26, 0.02),\n     np.arange(1, 1.31, 0.03)][severity - 1]\n\n  x = (np.array(x) / 255.).astype(np.float32)\n  out = np.zeros_like(x)\n  for zoom_factor in c:\n    out += clipped_zoom(x, zoom_factor)\n\n  x = (x + out) / (len(c) + 1)\n  return np.clip(x, 0, 1) * 255\n\n\ndef fog(x, severity=1):\n  c = [(1.5, 2), (2, 2), (2.5, 1.7), (2.5, 1.5), (3, 1.4)][severity - 1]\n\n  x = np.array(x) / 255.\n  max_val = x.max()\n  x += c[0] * plasma_fractal(wibbledecay=c[1])[:args.CROP_SIZE, :args.CROP_SIZE][..., np.newaxis]\n  return np.clip(x * max_val / (max_val + c[0]), 0, 1) * 255\n\n\ndef frost(x, severity=1):\n  c = [(1, 0.4),\n     (0.8, 0.6),\n     (0.7, 0.7),\n     (0.65, 0.7),\n     (0.6, 0.75)][severity - 1]\n  idx = np.random.randint(5)\n  filename = [\'frost1.png\', \'frost2.png\', \'frost3.png\', \'frost4.jpg\', \'frost5.jpg\', \'frost6.jpg\'][idx]\n  frost = cv2.imread(os.path.join(args.frost_dir, filename))\n  # randomly crop and convert to rgb\n  x_start, y_start = np.random.randint(0, frost.shape[0] - args.CROP_SIZE), np.random.randint(0, frost.shape[1] - args.CROP_SIZE)\n  frost = frost[x_start:x_start + args.CROP_SIZE, y_start:y_start + args.CROP_SIZE][..., [2, 1, 0]]\n\n  return np.clip(c[0] * np.array(x) + c[1] * frost, 0, 255)\n\n\ndef snow(x, severity=1):\n  c = [(0.1, 0.3, 3, 0.5, 10, 4, 0.8),\n     (0.2, 0.3, 2, 0.5, 12, 4, 0.7),\n     (0.55, 0.3, 4, 0.9, 12, 8, 0.7),\n     (0.55, 0.3, 4.5, 0.85, 12, 8, 0.65),\n     (0.55, 0.3, 2.5, 0.85, 12, 12, 0.55)][severity - 1]\n\n  x = np.array(x, dtype=np.float32) / 255.\n  snow_layer = np.random.normal(size=x.shape[:2], loc=c[0], scale=c[1])  # [:2] for monochrome\n\n  snow_layer = clipped_zoom(snow_layer[..., np.newaxis], c[2])\n  snow_layer[snow_layer < c[3]] = 0\n\n  snow_layer = PILImage.fromarray((np.clip(snow_layer.squeeze(), 0, 1) * 255).astype(np.uint8), mode=\'L\')\n  output = BytesIO()\n  snow_layer.save(output, format=\'PNG\')\n  snow_layer = MotionImage(blob=output.getvalue())\n\n  snow_layer.motion_blur(radius=c[4], sigma=c[5], angle=np.random.uniform(-135, -45))\n\n  snow_layer = cv2.imdecode(np.fromstring(snow_layer.make_blob(), np.uint8),\n                cv2.IMREAD_UNCHANGED) / 255.\n  snow_layer = snow_layer[..., np.newaxis]\n\n  x = c[6] * x + (1 - c[6]) * np.maximum(x, cv2.cvtColor(x, cv2.COLOR_RGB2GRAY).reshape(args.CROP_SIZE, args.CROP_SIZE,\n                                                                                        1) * 1.5 + 0.5)\n  return np.clip(x + snow_layer + np.rot90(snow_layer, k=2), 0, 1) * 255\n\n\ndef spatter(x, severity=1):\n  c = [(0.65, 0.3, 4, 0.69, 0.6, 0),\n     (0.65, 0.3, 3, 0.68, 0.6, 0),\n     (0.65, 0.3, 2, 0.68, 0.5, 0),\n     (0.65, 0.3, 1, 0.65, 1.5, 1),\n     (0.67, 0.4, 1, 0.65, 1.5, 1)][severity - 1]\n  x = np.array(x, dtype=np.float32) / 255.\n\n  liquid_layer = np.random.normal(size=x.shape[:2], loc=c[0], scale=c[1])\n\n  liquid_layer = gaussian(liquid_layer, sigma=c[2])\n  liquid_layer[liquid_layer < c[3]] = 0\n  if c[5] == 0:\n    liquid_layer = (liquid_layer * 255).astype(np.uint8)\n    dist = 255 - cv2.Canny(liquid_layer, 50, 150)\n    dist = cv2.distanceTransform(dist, cv2.DIST_L2, 5)\n    _, dist = cv2.threshold(dist, 20, 20, cv2.THRESH_TRUNC)\n    dist = cv2.blur(dist, (3, 3)).astype(np.uint8)\n    dist = cv2.equalizeHist(dist)\n    #   ker = np.array([[-1,-2,-3],[-2,0,0],[-3,0,1]], dtype=np.float32)\n    #   ker -= np.mean(ker)\n    ker = np.array([[-2, -1, 0], [-1, 1, 1], [0, 1, 2]])\n    dist = cv2.filter2D(dist, cv2.CV_8U, ker)\n    dist = cv2.blur(dist, (3, 3)).astype(np.float32)\n\n    m = cv2.cvtColor(liquid_layer * dist, cv2.COLOR_GRAY2BGRA)\n    m /= np.max(m, axis=(0, 1))\n    m *= c[4]\n\n    # water is pale turqouise\n    color = np.concatenate((175 / 255. * np.ones_like(m[..., :1]),\n                238 / 255. * np.ones_like(m[..., :1]),\n                238 / 255. * np.ones_like(m[..., :1])), axis=2)\n\n    color = cv2.cvtColor(color, cv2.COLOR_BGR2BGRA)\n    x = cv2.cvtColor(x, cv2.COLOR_BGR2BGRA)\n\n    return cv2.cvtColor(np.clip(x + m * color, 0, 1), cv2.COLOR_BGRA2BGR) * 255\n  else:\n    m = np.where(liquid_layer > c[3], 1, 0)\n    m = gaussian(m.astype(np.float32), sigma=c[4])\n    m[m < 0.8] = 0\n    #     m = np.abs(m) ** (1/c[4])\n\n    # mud brown\n    color = np.concatenate((63 / 255. * np.ones_like(x[..., :1]),\n                42 / 255. * np.ones_like(x[..., :1]),\n                20 / 255. * np.ones_like(x[..., :1])), axis=2)\n\n    color *= m[..., np.newaxis]\n    x *= (1 - m[..., np.newaxis])\n\n    return np.clip(x + color, 0, 1) * 255\n\n\ndef contrast(x, severity=1):\n  c = [0.4, .3, .2, .1, .05][severity - 1]\n\n  x = np.array(x) / 255.\n  means = np.mean(x, axis=(0, 1), keepdims=True)\n  return np.clip((x - means) * c + means, 0, 1) * 255\n\n\ndef brightness(x, severity=1):\n  c = [.1, .2, .3, .4, .5][severity - 1]\n\n  x = np.array(x) / 255.\n  x = sk.color.rgb2hsv(x)\n  x[:, :, 2] = np.clip(x[:, :, 2] + c, 0, 1)\n  x = sk.color.hsv2rgb(x)\n\n  return np.clip(x, 0, 1) * 255\n\n\ndef saturate(x, severity=1):\n  c = [(0.3, 0), (0.1, 0), (2, 0), (5, 0.1), (20, 0.2)][severity - 1]\n\n  x = np.array(x) / 255.\n  x = sk.color.rgb2hsv(x)\n  x[:, :, 1] = np.clip(x[:, :, 1] * c[0] + c[1], 0, 1)\n  x = sk.color.hsv2rgb(x)\n\n  return np.clip(x, 0, 1) * 255\n\n\ndef jpeg_compression(x, severity=1):\n  c = [25, 18, 15, 10, 7][severity - 1]\n\n  output = BytesIO()\n  x.save(output, \'JPEG\', quality=c)\n  x = PILImage.open(output)\n\n  return x\n\n\ndef pixelate(x, severity=1):\n  c = [0.6, 0.5, 0.4, 0.3, 0.25][severity - 1]\n\n  x = x.resize((int(args.CROP_SIZE * c), int(args.CROP_SIZE * c)), PILImage.BOX)\n  x = x.resize((args.CROP_SIZE, args.CROP_SIZE), PILImage.BOX)\n\n  return x\n\n\n# mod of https://gist.github.com/erniejunior/601cdf56d2b424757de5\ndef elastic_transform(image, severity=1):\n  c = [(244 * 2, 244 * 0.7, 244 * 0.1),  # CROP_SIZE should have been CROP_SIZE, but ultimately nothing is incorrect\n       (244 * 2, 244 * 0.08, 244 * 0.2),\n       (244 * 0.05, 244 * 0.01, 244 * 0.02),\n       (244 * 0.07, 244 * 0.01, 244 * 0.02),\n       (244 * 0.12, 244 * 0.01, 244 * 0.02)][severity - 1]\n\n  image = np.array(image, dtype=np.float32) / 255.\n  shape = image.shape\n  shape_size = shape[:2]\n\n  # random affine\n  center_square = np.float32(shape_size) // 2\n  square_size = min(shape_size) // 3\n  pts1 = np.float32([center_square + square_size,\n             [center_square[0] + square_size, center_square[1] - square_size],\n             center_square - square_size])\n  pts2 = pts1 + np.random.uniform(-c[2], c[2], size=pts1.shape).astype(np.float32)\n  M = cv2.getAffineTransform(pts1, pts2)\n  image = cv2.warpAffine(image, M, shape_size[::-1], borderMode=cv2.BORDER_REFLECT_101)\n\n  dx = (gaussian(np.random.uniform(-1, 1, size=shape[:2]),\n           c[1], mode=\'reflect\', truncate=3) * c[0]).astype(np.float32)\n  dy = (gaussian(np.random.uniform(-1, 1, size=shape[:2]),\n           c[1], mode=\'reflect\', truncate=3) * c[0]).astype(np.float32)\n  dx, dy = dx[..., np.newaxis], dy[..., np.newaxis]\n\n  x, y, z = np.meshgrid(np.arange(shape[1]), np.arange(shape[0]), np.arange(shape[2]))\n  indices = np.reshape(y + dy, (-1, 1)), np.reshape(x + dx, (-1, 1)), np.reshape(z, (-1, 1))\n  return np.clip(map_coordinates(image, indices, order=1, mode=\'reflect\').reshape(shape), 0, 1) * 255\n\n\n# /////////////// End Distortions ///////////////\n\n\n# /////////////// Further Setup ///////////////\n\ndef split_range(total, num_split, start_index=0):\n  rs = np.linspace(start_index, total, num_split + 1).astype(np.int)\n  result = [[rs[i], rs[i + 1]] for i in range(len(rs) - 1)]\n  return result\n\n\ndef distort_iterate_and_save(method, severity, start_idx, end_idx):\n  distorted_dataset = DistortImageFolder(\n    root=args.validation_dir,\n    method=method, severity=severity,\n    start_idx=start_idx, end_idx=end_idx,\n    transform=\'imagenet\')\n  # iterate to save distorted images\n  for _ in distorted_dataset:\n    continue\n  \n  return 0\n\n  \ndef save_distorted(total_img_cnt, method=gaussian_noise, num_process=20):\n  for severity in range(1, 6):\n    print(method.__name__, severity)\n    start = time.time()\n    ranges = split_range(total_img_cnt, num_process)\n    input_list = [(method, severity, idxs[0], idxs[1]) for idxs in ranges]\n    \n    pool = Pool(num_process)\n    pool.starmap(distort_iterate_and_save, input_list)\n    \n    end = time.time()\n    print(\'%f secs taken for %s %s\' % (end-start, method.__name__, severity))\n\n\n# /////////////// End Further Setup ///////////////\n\n\n# /////////////// Display Results ///////////////\nimport collections\n\nd = collections.OrderedDict()\nd[\'Gaussian Noise\'] = gaussian_noise\nd[\'Shot Noise\'] = shot_noise\nd[\'Impulse Noise\'] = impulse_noise\nd[\'Defocus Blur\'] = defocus_blur\nd[\'Glass Blur\'] = glass_blur\nd[\'Motion Blur\'] = motion_blur\nd[\'Zoom Blur\'] = zoom_blur\nd[\'Snow\'] = snow\nd[\'Frost\'] = frost\nd[\'Fog\'] = fog\nd[\'Brightness\'] = brightness\nd[\'Contrast\'] = contrast\nd[\'Elastic\'] = elastic_transform\nd[\'Pixelate\'] = pixelate\nd[\'JPEG\'] = jpeg_compression\n\nd[\'Speckle Noise\'] = speckle_noise\nd[\'Gaussian Blur\'] = gaussian_blur\nd[\'Spatter\'] = spatter\nd[\'Saturate\'] = saturate\n\n\n# count total number of validation images first.\ntotal_img_cnt = count_dataset(args.validation_dir)\n\nprint(\'\\nTotal %d validation images. Distortion started.\' % total_img_cnt)\n\n# start distortion process\nstart = time.time()\n\nfor method_name in d.keys():\n  save_distorted(total_img_cnt, d[method_name], num_process=args.num_workers)\n\nend = time.time()\n\nprint(\'Total %f secs taken.\' % (end-start))\n'"
official/utils/__init__.py,0,b''
official/utils/accelerator/__init__.py,0,b''
official/utils/accelerator/tpu.py,17,"b'# Copyright 2018 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Functions specific to running TensorFlow on TPUs.""""""\n\nimport tensorflow as tf\n\n\n# ""local"" is a magic word in the TPU cluster resolver; it informs the resolver\n# to use the local CPU as the compute device. This is useful for testing and\n# debugging; the code flow is ostensibly identical, but without the need to\n# actually have a TPU on the other end.\nLOCAL = ""local""\n\n\ndef construct_scalar_host_call(metric_dict, model_dir, prefix=""""):\n  """"""Construct a host call to log scalars when training on TPU.\n\n  Args:\n    metric_dict: A dict of the tensors to be logged.\n    model_dir: The location to write the summary.\n    prefix: The prefix (if any) to prepend to the metric names.\n\n  Returns:\n    A tuple of (function, args_to_be_passed_to_said_function)\n  """"""\n  # type: (dict, str) -> (function, list)\n  metric_names = list(metric_dict.keys())\n\n  def host_call_fn(global_step, *args):\n    """"""Training host call. Creates scalar summaries for training metrics.\n\n    This function is executed on the CPU and should not directly reference\n    any Tensors in the rest of the `model_fn`. To pass Tensors from the\n    model to the `metric_fn`, provide as part of the `host_call`. See\n    https://www.tensorflow.org/api_docs/python/tf/contrib/tpu/TPUEstimatorSpec\n    for more information.\n\n    Arguments should match the list of `Tensor` objects passed as the second\n    element in the tuple passed to `host_call`.\n\n    Args:\n      global_step: `Tensor with shape `[batch]` for the global_step\n      *args: Remaining tensors to log.\n\n    Returns:\n      List of summary ops to run on the CPU host.\n    """"""\n    step = global_step[0]\n    with tf.contrib.summary.create_file_writer(\n        logdir=model_dir, filename_suffix="".host_call"").as_default():\n      with tf.contrib.summary.always_record_summaries():\n        for i, name in enumerate(metric_names):\n          tf.contrib.summary.scalar(prefix + name, args[i][0], step=step)\n\n        return tf.contrib.summary.all_summary_ops()\n\n  # To log the current learning rate, and gradient norm for Tensorboard, the\n  # summary op needs to be run on the host CPU via host_call. host_call\n  # expects [batch_size, ...] Tensors, thus reshape to introduce a batch\n  # dimension. These Tensors are implicitly concatenated to\n  # [params[\'batch_size\']].\n  global_step_tensor = tf.reshape(tf.train.get_or_create_global_step(), [1])\n  other_tensors = [tf.reshape(metric_dict[key], [1]) for key in metric_names]\n\n  return host_call_fn, [global_step_tensor] + other_tensors\n\n\ndef embedding_matmul(embedding_table, values, mask, name=""embedding_matmul""):\n  """"""Performs embedding lookup via a matmul.\n\n  The matrix to be multiplied by the embedding table Tensor is constructed\n  via an implementation of scatter based on broadcasting embedding indices\n  and performing an equality comparison against a broadcasted\n  range(num_embedding_table_rows). All masked positions will produce an\n  embedding vector of zeros.\n\n  Args:\n    embedding_table: Tensor of embedding table.\n      Rank 2 (table_size x embedding dim)\n    values: Tensor of embedding indices. Rank 2 (batch x n_indices)\n    mask: Tensor of mask / weights. Rank 2 (batch x n_indices)\n    name: Optional name scope for created ops\n\n  Returns:\n    Rank 3 tensor of embedding vectors.\n  """"""\n\n  with tf.name_scope(name):\n    n_embeddings = embedding_table.get_shape().as_list()[0]\n    batch_size, padded_size = values.shape.as_list()\n\n    emb_idcs = tf.tile(\n        tf.reshape(values, (batch_size, padded_size, 1)), (1, 1, n_embeddings))\n    emb_weights = tf.tile(\n        tf.reshape(mask, (batch_size, padded_size, 1)), (1, 1, n_embeddings))\n    col_idcs = tf.tile(\n        tf.reshape(tf.range(n_embeddings), (1, 1, n_embeddings)),\n        (batch_size, padded_size, 1))\n    one_hot = tf.where(\n        tf.equal(emb_idcs, col_idcs), emb_weights,\n        tf.zeros((batch_size, padded_size, n_embeddings)))\n\n    return tf.tensordot(one_hot, embedding_table, 1)\n'"
official/utils/accelerator/tpu_test.py,8,"b'# Copyright 2018 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Test TPU optimized matmul embedding.""""""\n\nimport numpy as np\nimport tensorflow as tf\n\nfrom official.utils.accelerator import tpu as tpu_utils\n\n\nTEST_CASES = [\n    dict(embedding_dim=256, vocab_size=1000, sequence_length=64,\n         batch_size=32, seed=54131),\n    dict(embedding_dim=8, vocab_size=15, sequence_length=12,\n         batch_size=256, seed=536413),\n    dict(embedding_dim=2048, vocab_size=512, sequence_length=50,\n         batch_size=8, seed=35124)\n]\n\n\nclass TPUBaseTester(tf.test.TestCase):\n  def construct_embedding_and_values(self, embedding_dim, vocab_size,\n                                     sequence_length, batch_size, seed):\n    np.random.seed(seed)\n\n    embeddings = np.random.random(size=(vocab_size, embedding_dim))\n    embedding_table = tf.convert_to_tensor(embeddings, dtype=tf.float32)\n\n    tokens = np.random.randint(low=1, high=vocab_size-1,\n                               size=(batch_size, sequence_length))\n    for i in range(batch_size):\n      tokens[i, np.random.randint(low=0, high=sequence_length-1):] = 0\n    values = tf.convert_to_tensor(tokens, dtype=tf.int32)\n    mask = tf.to_float(tf.not_equal(values, 0))\n    return embedding_table, values, mask\n\n  def _test_embedding(self, embedding_dim, vocab_size,\n                      sequence_length, batch_size, seed):\n    """"""Test that matmul embedding matches embedding lookup (gather).""""""\n\n    with self.test_session():\n      embedding_table, values, mask = self.construct_embedding_and_values(\n          embedding_dim=embedding_dim,\n          vocab_size=vocab_size,\n          sequence_length=sequence_length,\n          batch_size=batch_size,\n          seed=seed\n      )\n\n      embedding = (tf.nn.embedding_lookup(params=embedding_table, ids=values) *\n                   tf.expand_dims(mask, -1))\n\n      matmul_embedding = tpu_utils.embedding_matmul(\n          embedding_table=embedding_table, values=values, mask=mask)\n\n      self.assertAllClose(embedding, matmul_embedding)\n\n  def _test_masking(self, embedding_dim, vocab_size,\n                    sequence_length, batch_size, seed):\n    """"""Test that matmul embedding properly zeros masked positions.""""""\n    with self.test_session():\n      embedding_table, values, mask = self.construct_embedding_and_values(\n          embedding_dim=embedding_dim,\n          vocab_size=vocab_size,\n          sequence_length=sequence_length,\n          batch_size=batch_size,\n          seed=seed\n      )\n\n      matmul_embedding = tpu_utils.embedding_matmul(\n          embedding_table=embedding_table, values=values, mask=mask)\n\n      self.assertAllClose(matmul_embedding,\n                          matmul_embedding * tf.expand_dims(mask, -1))\n\n  def test_embedding_0(self):\n    self._test_embedding(**TEST_CASES[0])\n\n  def test_embedding_1(self):\n    self._test_embedding(**TEST_CASES[1])\n\n  def test_embedding_2(self):\n    self._test_embedding(**TEST_CASES[2])\n\n  def test_masking_0(self):\n    self._test_masking(**TEST_CASES[0])\n\n  def test_masking_1(self):\n    self._test_masking(**TEST_CASES[1])\n\n  def test_masking_2(self):\n    self._test_masking(**TEST_CASES[2])\n\n\nif __name__ == ""__main__"":\n  tf.test.main()\n'"
official/utils/data/__init__.py,0,b''
official/utils/data/file_io.py,19,"b'# Copyright 2018 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Convenience functions for managing dataset file buffers.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport atexit\nimport multiprocessing\nimport os\nimport tempfile\nimport uuid\n\nimport numpy as np\nimport six\n\nimport tensorflow as tf\n\n\nclass _GarbageCollector(object):\n  """"""Deletes temporary buffer files at exit.\n\n  Certain tasks (such as NCF Recommendation) require writing buffers to\n  temporary files. (Which may be local or distributed.) It is not generally safe\n  to delete these files during operation, but they should be cleaned up. This\n  class keeps track of temporary files created, and deletes them at exit.\n  """"""\n  def __init__(self):\n    self.temp_buffers = []\n\n  def register(self, filepath):\n    self.temp_buffers.append(filepath)\n\n  def purge(self):\n    try:\n      for i in self.temp_buffers:\n        if tf.gfile.Exists(i):\n          tf.gfile.Remove(i)\n          tf.logging.info(""Buffer file {} removed"".format(i))\n    except Exception as e:\n      tf.logging.error(""Failed to cleanup buffer files: {}"".format(e))\n\n\n_GARBAGE_COLLECTOR = _GarbageCollector()\natexit.register(_GARBAGE_COLLECTOR.purge)\n\n_ROWS_PER_CORE = 50000\n\n\ndef write_to_temp_buffer(dataframe, buffer_folder, columns):\n  if buffer_folder is None:\n    _, buffer_path = tempfile.mkstemp()\n  else:\n    tf.gfile.MakeDirs(buffer_folder)\n    buffer_path = os.path.join(buffer_folder, str(uuid.uuid4()))\n  _GARBAGE_COLLECTOR.register(buffer_path)\n\n  return write_to_buffer(dataframe, buffer_path, columns)\n\n\ndef iter_shard_dataframe(df, rows_per_core=1000):\n  """"""Two way shard of a dataframe.\n\n  This function evenly shards a dataframe so that it can be mapped efficiently.\n  It yields a list of dataframes with length equal to the number of CPU cores,\n  with each dataframe having rows_per_core rows. (Except for the last batch\n  which may have fewer rows in the dataframes.) Passing vectorized inputs to\n  a multiprocessing pool is much more effecient than iterating through a\n  dataframe in serial and passing a list of inputs to the pool.\n\n  Args:\n    df: Pandas dataframe to be sharded.\n    rows_per_core: Number of rows in each shard.\n\n  Returns:\n    A list of dataframe shards.\n  """"""\n  n = len(df)\n  num_cores = min([multiprocessing.cpu_count(), n])\n\n  num_blocks = int(np.ceil(n / num_cores / rows_per_core))\n  max_batch_size = num_cores * rows_per_core\n  for i in range(num_blocks):\n    min_index = i * max_batch_size\n    max_index = min([(i + 1) * max_batch_size, n])\n    df_shard = df[min_index:max_index]\n    n_shard = len(df_shard)\n    boundaries = np.linspace(0, n_shard, num_cores + 1, dtype=np.int64)\n    yield [df_shard[boundaries[j]:boundaries[j+1]] for j in range(num_cores)]\n\n\ndef _shard_dict_to_examples(shard_dict):\n  """"""Converts a dict of arrays into a list of example bytes.""""""\n  n = [i for i in shard_dict.values()][0].shape[0]\n  feature_list = [{} for _ in range(n)]\n  for column, values in shard_dict.items():\n    if len(values.shape) == 1:\n      values = np.reshape(values, values.shape + (1,))\n\n    if values.dtype.kind == ""i"":\n      feature_map = lambda x: tf.train.Feature(\n          int64_list=tf.train.Int64List(value=x))\n    elif values.dtype.kind == ""f"":\n      feature_map = lambda x: tf.train.Feature(\n          float_list=tf.train.FloatList(value=x))\n    else:\n      raise ValueError(""Invalid dtype"")\n    for i in range(n):\n      feature_list[i][column] = feature_map(values[i])\n  examples = [\n      tf.train.Example(features=tf.train.Features(feature=example_features))\n      for example_features in feature_list\n  ]\n\n  return [e.SerializeToString() for e in examples]\n\n\ndef _serialize_shards(df_shards, columns, pool, writer):\n  """"""Map sharded dataframes to bytes, and write them to a buffer.\n\n  Args:\n    df_shards: A list of pandas dataframes. (Should be of similar size)\n    columns: The dataframe columns to be serialized.\n    pool: A multiprocessing pool to serialize in parallel.\n    writer: A TFRecordWriter to write the serialized shards.\n  """"""\n  # Pandas does not store columns of arrays as nd arrays. stack remedies this.\n  map_inputs = [{c: np.stack(shard[c].values, axis=0) for c in columns}\n                for shard in df_shards]\n\n  # Failure within pools is very irksome. Thus, it is better to thoroughly check\n  # inputs in the main process.\n  for inp in map_inputs:\n    # Check that all fields have the same number of rows.\n    assert len(set([v.shape[0] for v in inp.values()])) == 1\n    for val in inp.values():\n      assert hasattr(val, ""dtype"")\n      assert hasattr(val.dtype, ""kind"")\n      assert val.dtype.kind in (""i"", ""f"")\n      assert len(val.shape) in (1, 2)\n  shard_bytes = pool.map(_shard_dict_to_examples, map_inputs)\n  for s in shard_bytes:\n    for example in s:\n      writer.write(example)\n\ndef write_to_buffer(dataframe, buffer_path, columns, expected_size=None):\n  """"""Write a dataframe to a binary file for a dataset to consume.\n\n  Args:\n    dataframe: The pandas dataframe to be serialized.\n    buffer_path: The path where the serialized results will be written.\n    columns: The dataframe columns to be serialized.\n    expected_size: The size in bytes of the serialized results. This is used to\n      lazily construct the buffer.\n\n  Returns:\n    The path of the buffer.\n  """"""\n  if tf.gfile.Exists(buffer_path) and tf.gfile.Stat(buffer_path).length > 0:\n    actual_size = tf.gfile.Stat(buffer_path).length\n    if expected_size == actual_size:\n      return buffer_path\n    tf.logging.warning(\n        ""Existing buffer {} has size {}. Expected size {}. Deleting and ""\n        ""rebuilding buffer."".format(buffer_path, actual_size, expected_size))\n    tf.gfile.Remove(buffer_path)\n\n  if dataframe is None:\n    raise ValueError(\n        ""dataframe was None but a valid existing buffer was not found."")\n\n  tf.gfile.MakeDirs(os.path.split(buffer_path)[0])\n\n  tf.logging.info(""Constructing TFRecordDataset buffer: {}"".format(buffer_path))\n\n  count = 0\n  pool = multiprocessing.Pool(multiprocessing.cpu_count())\n  try:\n    with tf.python_io.TFRecordWriter(buffer_path) as writer:\n      for df_shards in iter_shard_dataframe(df=dataframe,\n                                            rows_per_core=_ROWS_PER_CORE):\n        _serialize_shards(df_shards, columns, pool, writer)\n        count += sum([len(s) for s in df_shards])\n        tf.logging.info(""{}/{} examples written.""\n                        .format(str(count).ljust(8), len(dataframe)))\n  finally:\n    pool.terminate()\n\n  tf.logging.info(""Buffer write complete."")\n  return buffer_path\n'"
official/utils/data/file_io_test.py,10,"b'# Copyright 2018 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Tests for binary data file utilities.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport contextlib\nimport multiprocessing\n\n# pylint: disable=wrong-import-order\nimport numpy as np\nimport pandas as pd\nimport tensorflow as tf\n# pylint: enable=wrong-import-order\n\nfrom official.utils.data import file_io\n\n\n_RAW_ROW = ""raw_row""\n_DUMMY_COL = ""column_0""\n_DUMMY_VEC_COL = ""column_1""\n_DUMMY_VEC_LEN = 4\n\n_ROWS_PER_CORE = 4\n_TEST_CASES = [\n    # One batch of one\n    dict(row_count=1, cpu_count=1, expected=[\n        [[0]]\n    ]),\n\n    dict(row_count=10, cpu_count=1, expected=[\n        [[0, 1, 2, 3]], [[4, 5, 6, 7]], [[8, 9]]\n    ]),\n\n    dict(row_count=21, cpu_count=1, expected=[\n        [[0, 1, 2, 3]], [[4, 5, 6, 7]], [[8, 9, 10, 11]],\n        [[12, 13, 14, 15]], [[16, 17, 18, 19]], [[20]]\n    ]),\n\n    dict(row_count=1, cpu_count=4, expected=[\n        [[0]]\n    ]),\n\n    dict(row_count=10, cpu_count=4, expected=[\n        [[0, 1], [2, 3, 4], [5, 6], [7, 8, 9]]\n    ]),\n\n    dict(row_count=21, cpu_count=4, expected=[\n        [[0, 1, 2, 3], [4, 5, 6, 7], [8, 9, 10, 11], [12, 13, 14, 15]],\n        [[16], [17], [18], [19, 20]]\n    ]),\n\n    dict(row_count=10, cpu_count=8, expected=[\n        [[0], [1], [2], [3, 4], [5], [6], [7], [8, 9]]\n    ]),\n\n    dict(row_count=40, cpu_count=8, expected=[\n        [[0, 1, 2, 3], [4, 5, 6, 7], [8, 9, 10, 11], [12, 13, 14, 15],\n         [16, 17, 18, 19], [20, 21, 22, 23], [24, 25, 26, 27],\n         [28, 29, 30, 31]],\n        [[32], [33], [34], [35], [36], [37], [38], [39]]\n    ]),\n]\n\n_FEATURE_MAP = {\n    _RAW_ROW: tf.FixedLenFeature([1], dtype=tf.int64),\n    _DUMMY_COL: tf.FixedLenFeature([1], dtype=tf.int64),\n    _DUMMY_VEC_COL: tf.FixedLenFeature([_DUMMY_VEC_LEN], dtype=tf.float32)\n}\n\n\n@contextlib.contextmanager\ndef fixed_core_count(cpu_count):\n  """"""Override CPU count.\n\n  file_io.py uses the cpu_count function to scale to the size of the instance.\n  However, this is not desirable for testing because it can make the test flaky.\n  Instead, this context manager fixes the count for more robust testing.\n\n  Args:\n    cpu_count: How many cores multiprocessing claims to have.\n\n  Yields:\n    Nothing. (for context manager only)\n  """"""\n  old_count_fn = multiprocessing.cpu_count\n  multiprocessing.cpu_count = lambda: cpu_count\n  yield\n  multiprocessing.cpu_count = old_count_fn\n\n\nclass BaseTest(tf.test.TestCase):\n\n  def _test_sharding(self, row_count, cpu_count, expected):\n    df = pd.DataFrame({_DUMMY_COL: list(range(row_count))})\n    with fixed_core_count(cpu_count):\n      shards = list(file_io.iter_shard_dataframe(df, _ROWS_PER_CORE))\n    result = [[j[_DUMMY_COL].tolist() for j in i] for i in shards]\n    self.assertAllEqual(expected, result)\n\n  def test_tiny_rows_low_core(self):\n    self._test_sharding(**_TEST_CASES[0])\n\n  def test_small_rows_low_core(self):\n    self._test_sharding(**_TEST_CASES[1])\n\n  def test_large_rows_low_core(self):\n    self._test_sharding(**_TEST_CASES[2])\n\n  def test_tiny_rows_medium_core(self):\n    self._test_sharding(**_TEST_CASES[3])\n\n  def test_small_rows_medium_core(self):\n    self._test_sharding(**_TEST_CASES[4])\n\n  def test_large_rows_medium_core(self):\n    self._test_sharding(**_TEST_CASES[5])\n\n  def test_small_rows_large_core(self):\n    self._test_sharding(**_TEST_CASES[6])\n\n  def test_large_rows_large_core(self):\n    self._test_sharding(**_TEST_CASES[7])\n\n  def _serialize_deserialize(self, num_cores=1, num_rows=20):\n    np.random.seed(1)\n    df = pd.DataFrame({\n        # Serialization order is only deterministic for num_cores=1. raw_row is\n        # used in validation after the deserialization.\n        _RAW_ROW: np.array(range(num_rows), dtype=np.int64),\n        _DUMMY_COL: np.random.randint(0, 35, size=(num_rows,)),\n        _DUMMY_VEC_COL: [\n            np.array([np.random.random() for _ in range(_DUMMY_VEC_LEN)])\n            for i in range(num_rows)  # pylint: disable=unused-variable\n        ]\n    })\n\n    with fixed_core_count(num_cores):\n      buffer_path = file_io.write_to_temp_buffer(\n          df, self.get_temp_dir(), [_RAW_ROW, _DUMMY_COL, _DUMMY_VEC_COL])\n\n    with self.test_session(graph=tf.Graph()) as sess:\n      dataset = tf.data.TFRecordDataset(buffer_path)\n      dataset = dataset.batch(1).map(\n          lambda x: tf.parse_example(x, _FEATURE_MAP))\n\n      data_iter = dataset.make_one_shot_iterator()\n      seen_rows = set()\n      for i in range(num_rows+5):\n        row = data_iter.get_next()\n        try:\n          row_id, val_0, val_1 = sess.run(\n              [row[_RAW_ROW], row[_DUMMY_COL], row[_DUMMY_VEC_COL]])\n          row_id, val_0, val_1 = row_id[0][0], val_0[0][0], val_1[0]\n          assert row_id not in seen_rows\n          seen_rows.add(row_id)\n\n          self.assertEqual(val_0, df[_DUMMY_COL][row_id])\n          self.assertAllClose(val_1, df[_DUMMY_VEC_COL][row_id])\n\n          self.assertLess(i, num_rows, msg=""Too many rows."")\n        except tf.errors.OutOfRangeError:\n          self.assertGreaterEqual(i, num_rows, msg=""Too few rows."")\n\n    file_io._GARBAGE_COLLECTOR.purge()\n    assert not tf.gfile.Exists(buffer_path)\n\n  def test_serialize_deserialize_0(self):\n    self._serialize_deserialize(num_cores=1)\n\n  def test_serialize_deserialize_1(self):\n    self._serialize_deserialize(num_cores=2)\n\n  def test_serialize_deserialize_2(self):\n    self._serialize_deserialize(num_cores=8)\n\n\nif __name__ == ""__main__"":\n  tf.test.main()\n'"
official/utils/export/__init__.py,0,b''
official/utils/export/export.py,4,"b'# Copyright 2018 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Convenience functions for exporting models as SavedModels or other types.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\n\ndef build_tensor_serving_input_receiver_fn(shape, dtype=tf.float32,\n                                           batch_size=1):\n  """"""Returns a input_receiver_fn that can be used during serving.\n\n  This expects examples to come through as float tensors, and simply\n  wraps them as TensorServingInputReceivers.\n\n  Arguably, this should live in tf.estimator.export. Testing here first.\n\n  Args:\n    shape: list representing target size of a single example.\n    dtype: the expected datatype for the input example\n    batch_size: number of input tensors that will be passed for prediction\n\n  Returns:\n    A function that itself returns a TensorServingInputReceiver.\n  """"""\n  def serving_input_receiver_fn():\n    # Prep a placeholder where the input example will be fed in\n    features = tf.placeholder(\n        dtype=dtype, shape=[batch_size] + shape, name=\'input_tensor\')\n\n    return tf.estimator.export.TensorServingInputReceiver(\n        features=features, receiver_tensors=features)\n\n  return serving_input_receiver_fn\n'"
official/utils/export/export_test.py,15,"b'# Copyright 2018 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Tests for exporting utils.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf  # pylint: disable=g-bad-import-order\n\nfrom official.utils.export import export\n\n\nclass ExportUtilsTest(tf.test.TestCase):\n  """"""Tests for the ExportUtils.""""""\n\n  def test_build_tensor_serving_input_receiver_fn(self):\n    receiver_fn = export.build_tensor_serving_input_receiver_fn(shape=[4, 5])\n    with tf.Graph().as_default():\n      receiver = receiver_fn()\n      self.assertIsInstance(\n          receiver, tf.estimator.export.TensorServingInputReceiver)\n\n      self.assertIsInstance(receiver.features, tf.Tensor)\n      self.assertEqual(receiver.features.shape, tf.TensorShape([1, 4, 5]))\n      self.assertEqual(receiver.features.dtype, tf.float32)\n      self.assertIsInstance(receiver.receiver_tensors, dict)\n      # Note that Python 3 can no longer index .values() directly; cast to list.\n      self.assertEqual(list(receiver.receiver_tensors.values())[0].shape,\n                       tf.TensorShape([1, 4, 5]))\n\n  def test_build_tensor_serving_input_receiver_fn_batch_dtype(self):\n    receiver_fn = export.build_tensor_serving_input_receiver_fn(\n        shape=[4, 5], dtype=tf.int8, batch_size=10)\n\n    with tf.Graph().as_default():\n      receiver = receiver_fn()\n      self.assertIsInstance(\n          receiver, tf.estimator.export.TensorServingInputReceiver)\n\n      self.assertIsInstance(receiver.features, tf.Tensor)\n      self.assertEqual(receiver.features.shape, tf.TensorShape([10, 4, 5]))\n      self.assertEqual(receiver.features.dtype, tf.int8)\n      self.assertIsInstance(receiver.receiver_tensors, dict)\n      # Note that Python 3 can no longer index .values() directly; cast to list.\n      self.assertEqual(list(receiver.receiver_tensors.values())[0].shape,\n                       tf.TensorShape([10, 4, 5]))\n\n\nif __name__ == ""__main__"":\n  tf.test.main()\n'"
official/utils/flags/__init__.py,0,b''
official/utils/flags/_base.py,1,"b'# Copyright 2018 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Flags which will be nearly universal across models.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom absl import flags\nimport tensorflow as tf\n\nfrom official.utils.flags._conventions import help_wrap\nfrom official.utils.logs import hooks_helper\n\n\ndef define_base(data_dir=True, model_dir=True, clean=True, train_epochs=True,\n                epochs_between_evals=True, stop_threshold=True, batch_size=True,\n                num_gpu=True, hooks=True, export_dir=True):\n  """"""Register base flags.\n\n  Args:\n    data_dir: Create a flag for specifying the input data directory.\n    model_dir: Create a flag for specifying the model file directory.\n    train_epochs: Create a flag to specify the number of training epochs.\n    epochs_between_evals: Create a flag to specify the frequency of testing.\n    stop_threshold: Create a flag to specify a threshold accuracy or other\n      eval metric which should trigger the end of training.\n    batch_size: Create a flag to specify the batch size.\n    num_gpu: Create a flag to specify the number of GPUs used.\n    hooks: Create a flag to specify hooks for logging.\n    export_dir: Create a flag to specify where a SavedModel should be exported.\n\n  Returns:\n    A list of flags for core.py to marks as key flags.\n  """"""\n  key_flags = []\n\n  if data_dir:\n    flags.DEFINE_string(\n        name=""data_dir"", short_name=""dd"", default=""/tmp"",\n        help=help_wrap(""The location of the input data.""))\n    key_flags.append(""data_dir"")\n\n  if model_dir:\n    flags.DEFINE_string(\n        name=""model_dir"", short_name=""md"", default=""/tmp"",\n        help=help_wrap(""The location of the model checkpoint files.""))\n    key_flags.append(""model_dir"")\n\n  if clean:\n    flags.DEFINE_boolean(\n        name=""clean"", default=False,\n        help=help_wrap(""If set, model_dir will be removed if it exists.""))\n    key_flags.append(""clean"")\n\n  if train_epochs:\n    flags.DEFINE_integer(\n        name=""train_epochs"", short_name=""te"", default=1,\n        help=help_wrap(""The number of epochs used to train.""))\n    key_flags.append(""train_epochs"")\n\n  if epochs_between_evals:\n    flags.DEFINE_integer(\n        name=""epochs_between_evals"", short_name=""ebe"", default=1,\n        help=help_wrap(""The number of training epochs to run between ""\n                       ""evaluations.""))\n    key_flags.append(""epochs_between_evals"")\n\n  if stop_threshold:\n    flags.DEFINE_float(\n        name=""stop_threshold"", short_name=""st"",\n        default=None,\n        help=help_wrap(""If passed, training will stop at the earlier of ""\n                       ""train_epochs and when the evaluation metric is  ""\n                       ""greater than or equal to stop_threshold.""))\n\n  if batch_size:\n    flags.DEFINE_integer(\n        name=""batch_size"", short_name=""bs"", default=32,\n        help=help_wrap(""Batch size for training and evaluation. When using ""\n                       ""multiple gpus, this is the global batch size for ""\n                       ""all devices. For example, if the batch size is 32 ""\n                       ""and there are 4 GPUs, each GPU will get 8 examples on ""\n                       ""each step.""))\n    key_flags.append(""batch_size"")\n\n  if num_gpu:\n    flags.DEFINE_integer(\n        name=""num_gpus"", short_name=""ng"",\n        default=1 if tf.test.is_gpu_available() else 0,\n        help=help_wrap(\n            ""How many GPUs to use with the DistributionStrategies API. The ""\n            ""default is 1 if TensorFlow can detect a GPU, and 0 otherwise.""))\n\n  if hooks:\n    # Construct a pretty summary of hooks.\n    hook_list_str = (\n        u""\\ufeff  Hook:\\n"" + u""\\n"".join([u""\\ufeff    {}"".format(key) for key\n                                         in hooks_helper.HOOKS]))\n    flags.DEFINE_list(\n        name=""hooks"", short_name=""hk"", default=""LoggingTensorHook"",\n        help=help_wrap(\n            u""A list of (case insensitive) strings to specify the names of ""\n            u""training hooks.\\n{}\\n\\ufeff  Example: `--hooks ProfilerHook,""\n            u""ExamplesPerSecondHook`\\n See official.utils.logs.hooks_helper ""\n            u""for details."".format(hook_list_str))\n    )\n    key_flags.append(""hooks"")\n\n  if export_dir:\n    flags.DEFINE_string(\n        name=""export_dir"", short_name=""ed"", default=None,\n        help=help_wrap(""If set, a SavedModel serialization of the model will ""\n                       ""be exported to this directory at the end of training. ""\n                       ""See the README for more details and relevant links."")\n    )\n    key_flags.append(""export_dir"")\n\n  return key_flags\n\n\ndef get_num_gpus(flags_obj):\n  """"""Treat num_gpus=-1 as \'use all\'.""""""\n  if flags_obj.num_gpus != -1:\n    return flags_obj.num_gpus\n\n  from tensorflow.python.client import device_lib  # pylint: disable=g-import-not-at-top\n  local_device_protos = device_lib.list_local_devices()\n  return sum([1 for d in local_device_protos if d.device_type == ""GPU""])\n'"
official/utils/flags/_benchmark.py,0,"b'# Copyright 2018 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Flags for benchmarking models.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom absl import flags\n\nfrom official.utils.flags._conventions import help_wrap\n\n\ndef define_benchmark(benchmark_log_dir=True, bigquery_uploader=True):\n  """"""Register benchmarking flags.\n\n  Args:\n    benchmark_log_dir: Create a flag to specify location for benchmark logging.\n    bigquery_uploader: Create flags for uploading results to BigQuery.\n\n  Returns:\n    A list of flags for core.py to marks as key flags.\n  """"""\n\n  key_flags = []\n\n  flags.DEFINE_enum(\n      name=""benchmark_logger_type"", default=""BaseBenchmarkLogger"",\n      enum_values=[""BaseBenchmarkLogger"", ""BenchmarkFileLogger"",\n                   ""BenchmarkBigQueryLogger""],\n      help=help_wrap(""The type of benchmark logger to use. Defaults to using ""\n                     ""BaseBenchmarkLogger which logs to STDOUT. Different ""\n                     ""loggers will require other flags to be able to work.""))\n  flags.DEFINE_string(\n      name=""benchmark_test_id"", short_name=""bti"", default=None,\n      help=help_wrap(""The unique test ID of the benchmark run. It could be the ""\n                     ""combination of key parameters. It is hardware ""\n                     ""independent and could be used compare the performance ""\n                     ""between different test runs. This flag is designed for ""\n                     ""human consumption, and does not have any impact within ""\n                     ""the system.""))\n\n  if benchmark_log_dir:\n    flags.DEFINE_string(\n        name=""benchmark_log_dir"", short_name=""bld"", default=None,\n        help=help_wrap(""The location of the benchmark logging."")\n    )\n\n  if bigquery_uploader:\n    flags.DEFINE_string(\n        name=""gcp_project"", short_name=""gp"", default=None,\n        help=help_wrap(\n            ""The GCP project name where the benchmark will be uploaded.""))\n\n    flags.DEFINE_string(\n        name=""bigquery_data_set"", short_name=""bds"", default=""test_benchmark"",\n        help=help_wrap(\n            ""The Bigquery dataset name where the benchmark will be uploaded.""))\n\n    flags.DEFINE_string(\n        name=""bigquery_run_table"", short_name=""brt"", default=""benchmark_run"",\n        help=help_wrap(""The Bigquery table name where the benchmark run ""\n                       ""information will be uploaded.""))\n\n    flags.DEFINE_string(\n        name=""bigquery_run_status_table"", short_name=""brst"",\n        default=""benchmark_run_status"",\n        help=help_wrap(""The Bigquery table name where the benchmark run ""\n                       ""status information will be uploaded.""))\n\n    flags.DEFINE_string(\n        name=""bigquery_metric_table"", short_name=""bmt"",\n        default=""benchmark_metric"",\n        help=help_wrap(""The Bigquery table name where the benchmark metric ""\n                       ""information will be uploaded.""))\n\n  @flags.multi_flags_validator(\n      [""benchmark_logger_type"", ""benchmark_log_dir""],\n      message=""--benchmark_logger_type=BenchmarkFileLogger will require ""\n              ""--benchmark_log_dir being set"")\n  def _check_benchmark_log_dir(flags_dict):\n    benchmark_logger_type = flags_dict[""benchmark_logger_type""]\n    if benchmark_logger_type == ""BenchmarkFileLogger"":\n      return flags_dict[""benchmark_log_dir""]\n    return True\n\n  return key_flags\n'"
official/utils/flags/_conventions.py,0,"b'# Copyright 2018 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Central location for shared arparse convention definitions.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport codecs\nimport functools\n\nfrom absl import app as absl_app\nfrom absl import flags\n\n\n# This codifies help string conventions and makes it easy to update them if\n# necessary. Currently the only major effect is that help bodies start on the\n# line after flags are listed. All flag definitions should wrap the text bodies\n# with help wrap when calling DEFINE_*.\n_help_wrap = functools.partial(flags.text_wrap, length=80, indent="""",\n                               firstline_indent=""\\n"")\n\n\n# Pretty formatting causes issues when utf-8 is not installed on a system.\ntry:\n  codecs.lookup(""utf-8"")\n  help_wrap = _help_wrap\nexcept LookupError:\n  def help_wrap(text, *args, **kwargs):\n    return _help_wrap(text, *args, **kwargs).replace(""\\ufeff"", """")\n\n\n# Replace None with h to also allow -h\nabsl_app.HelpshortFlag.SHORT_NAME = ""h""\n'"
official/utils/flags/_device.py,1,"b'# Copyright 2018 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Flags for managing compute devices. Currently only contains TPU flags.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom absl import flags\nimport tensorflow as tf\n\nfrom official.utils.flags._conventions import help_wrap\n\n\ndef require_cloud_storage(flag_names):\n  """"""Register a validator to check directory flags.\n  Args:\n    flag_names: An iterable of strings containing the names of flags to be\n      checked.\n  """"""\n  msg = ""TPU requires GCS path for {}"".format("", "".join(flag_names))\n  @flags.multi_flags_validator([""tpu""] + flag_names, message=msg)\n  def _path_check(flag_values):  # pylint: disable=missing-docstring\n    if flag_values[""tpu""] is None:\n      return True\n\n    valid_flags = True\n    for key in flag_names:\n      if not flag_values[key].startswith(""gs://""):\n        tf.logging.error(""{} must be a GCS path."".format(key))\n        valid_flags = False\n\n    return valid_flags\n\n\ndef define_device(tpu=True):\n  """"""Register device specific flags.\n  Args:\n    tpu: Create flags to specify TPU operation.\n  Returns:\n    A list of flags for core.py to marks as key flags.\n  """"""\n\n  key_flags = []\n\n  if tpu:\n    flags.DEFINE_string(\n        name=""tpu"", default=None,\n        help=help_wrap(\n            ""The Cloud TPU to use for training. This should be either the name ""\n            ""used when creating the Cloud TPU, or a ""\n            ""grpc://ip.address.of.tpu:8470 url. Passing `local` will use the""\n            ""CPU of the local instance instead. (Good for debugging.)""))\n    key_flags.append(""tpu"")\n\n    flags.DEFINE_string(\n        name=""tpu_zone"", default=None,\n        help=help_wrap(\n            ""[Optional] GCE zone where the Cloud TPU is located in. If not ""\n            ""specified, we will attempt to automatically detect the GCE ""\n            ""project from metadata.""))\n\n    flags.DEFINE_string(\n        name=""tpu_gcp_project"", default=None,\n        help=help_wrap(\n            ""[Optional] Project name for the Cloud TPU-enabled project. If not ""\n            ""specified, we will attempt to automatically detect the GCE ""\n            ""project from metadata.""))\n\n    flags.DEFINE_integer(name=""num_tpu_shards"", default=8,\n                         help=help_wrap(""Number of shards (TPU chips).""))\n\n  return key_flags\n'"
official/utils/flags/_misc.py,0,"b'# Copyright 2018 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Misc flags.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom absl import flags\n\nfrom official.utils.flags._conventions import help_wrap\n\n\ndef define_image(data_format=True):\n  """"""Register image specific flags.\n\n  Args:\n    data_format: Create a flag to specify image axis convention.\n\n  Returns:\n    A list of flags for core.py to marks as key flags.\n  """"""\n\n  key_flags = []\n\n  if data_format:\n    flags.DEFINE_enum(\n        name=""data_format"", short_name=""df"", default=None,\n        enum_values=[""channels_first"", ""channels_last""],\n        help=help_wrap(\n            ""A flag to override the data format used in the model. ""\n            ""channels_first provides a performance boost on GPU but is not ""\n            ""always compatible with CPU. If left unspecified, the data format ""\n            ""will be chosen automatically based on whether TensorFlow was ""\n            ""built for CPU or GPU.""))\n    key_flags.append(""data_format"")\n\n  return key_flags\n'"
official/utils/flags/_performance.py,3,"b'# Copyright 2018 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Register flags for optimizing performance.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport multiprocessing\n\nimport tensorflow as tf  # pylint: disable=g-bad-import-order\nfrom absl import flags  # pylint: disable=g-bad-import-order\n\nfrom official.utils.flags._conventions import help_wrap\n\n# Map string to (TensorFlow dtype, default loss scale)\nDTYPE_MAP = {\n    ""fp16"": (tf.float16, 128),\n    ""fp32"": (tf.float32, 1),\n}\n\n\ndef get_tf_dtype(flags_obj):\n  return DTYPE_MAP[flags_obj.dtype][0]\n\n\ndef get_loss_scale(flags_obj):\n  if flags_obj.loss_scale is not None:\n    return flags_obj.loss_scale\n  return DTYPE_MAP[flags_obj.dtype][1]\n\n\ndef define_performance(num_parallel_calls=True, inter_op=True, intra_op=True,\n                       synthetic_data=True, max_train_steps=True, dtype=True,\n                       all_reduce_alg=True):\n  """"""Register flags for specifying performance tuning arguments.\n\n  Args:\n    num_parallel_calls: Create a flag to specify parallelism of data loading.\n    inter_op: Create a flag to allow specification of inter op threads.\n    intra_op: Create a flag to allow specification of intra op threads.\n    synthetic_data: Create a flag to allow the use of synthetic data.\n    max_train_steps: Create a flags to allow specification of maximum number\n      of training steps\n    dtype: Create flags for specifying dtype.\n\n  Returns:\n    A list of flags for core.py to marks as key flags.\n  """"""\n\n  key_flags = []\n  if num_parallel_calls:\n    flags.DEFINE_integer(\n        name=""num_parallel_calls"", short_name=""npc"",\n        default=multiprocessing.cpu_count(),\n        help=help_wrap(""The number of records that are  processed in parallel ""\n                       ""during input processing. This can be optimized per ""\n                       ""data set but for generally homogeneous data sets, ""\n                       ""should be approximately the number of available CPU ""\n                       ""cores. (default behavior)""))\n\n  if inter_op:\n    flags.DEFINE_integer(\n        name=""inter_op_parallelism_threads"", short_name=""inter"", default=0,\n        help=help_wrap(""Number of inter_op_parallelism_threads to use for CPU. ""\n                       ""See TensorFlow config.proto for details."")\n    )\n\n  if intra_op:\n    flags.DEFINE_integer(\n        name=""intra_op_parallelism_threads"", short_name=""intra"", default=0,\n        help=help_wrap(""Number of intra_op_parallelism_threads to use for CPU. ""\n                       ""See TensorFlow config.proto for details.""))\n\n  if synthetic_data:\n    flags.DEFINE_bool(\n        name=""use_synthetic_data"", short_name=""synth"", default=False,\n        help=help_wrap(\n            ""If set, use fake data (zeroes) instead of a real dataset. ""\n            ""This mode is useful for performance debugging, as it removes ""\n            ""input processing steps, but will not learn anything.""))\n\n  if max_train_steps:\n    flags.DEFINE_integer(\n        name=""max_train_steps"", short_name=""mts"", default=None, help=help_wrap(\n            ""The model will stop training if the global_step reaches this ""\n            ""value. If not set, training will run until the specified number ""\n            ""of epochs have run as usual. It is generally recommended to set ""\n            ""--train_epochs=1 when using this flag.""\n        ))\n\n  if dtype:\n    flags.DEFINE_enum(\n        name=""dtype"", short_name=""dt"", default=""fp32"",\n        enum_values=DTYPE_MAP.keys(),\n        help=help_wrap(""The TensorFlow datatype used for calculations. ""\n                       ""Variables may be cast to a higher precision on a ""\n                       ""case-by-case basis for numerical stability.""))\n\n    flags.DEFINE_integer(\n        name=""loss_scale"", short_name=""ls"", default=None,\n        help=help_wrap(\n            ""The amount to scale the loss by when the model is run. Before ""\n            ""gradients are computed, the loss is multiplied by the loss scale, ""\n            ""making all gradients loss_scale times larger. To adjust for this, ""\n            ""gradients are divided by the loss scale before being applied to ""\n            ""variables. This is mathematically equivalent to training without ""\n            ""a loss scale, but the loss scale helps avoid some intermediate ""\n            ""gradients from underflowing to zero. If not provided the default ""\n            ""for fp16 is 128 and 1 for all other dtypes.""))\n\n    loss_scale_val_msg = ""loss_scale should be a positive integer.""\n    @flags.validator(flag_name=""loss_scale"", message=loss_scale_val_msg)\n    def _check_loss_scale(loss_scale):  # pylint: disable=unused-variable\n      if loss_scale is None:\n        return True  # null case is handled in get_loss_scale()\n\n      return loss_scale > 0\n\n  if all_reduce_alg:\n    flags.DEFINE_string(\n        name=""all_reduce_alg"", short_name=""ara"", default=None,\n        help=help_wrap(""Defines the algorithm to use for performing all-reduce.""\n                       ""See tf.contrib.distribute.AllReduceCrossTowerOps for ""\n                       ""more details and available options.""))\n\n\n  return key_flags\n'"
official/utils/flags/core.py,0,"b'# Copyright 2018 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Public interface for flag definition.\n\nSee _example.py for detailed instructions on defining flags.\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport functools\nimport sys\n\nfrom absl import app as absl_app\nfrom absl import flags\n\nfrom official.utils.flags import _base\nfrom official.utils.flags import _benchmark\nfrom official.utils.flags import _conventions\nfrom official.utils.flags import _device\nfrom official.utils.flags import _misc\nfrom official.utils.flags import _performance\n\n\ndef set_defaults(**kwargs):\n  for key, value in kwargs.items():\n    flags.FLAGS.set_default(name=key, value=value)\n\n\ndef parse_flags(argv=None):\n  """"""Reset flags and reparse. Currently only used in testing.""""""\n  flags.FLAGS.unparse_flags()\n  absl_app.parse_flags_with_usage(argv or sys.argv)\n\n\ndef register_key_flags_in_core(f):\n  """"""Defines a function in core.py, and registers its key flags.\n\n  absl uses the location of a flags.declare_key_flag() to determine the context\n  in which a flag is key. By making all declares in core, this allows model\n  main functions to call flags.adopt_module_key_flags() on core and correctly\n  chain key flags.\n\n  Args:\n    f:  The function to be wrapped\n\n  Returns:\n    The ""core-defined"" version of the input function.\n  """"""\n\n  def core_fn(*args, **kwargs):\n    key_flags = f(*args, **kwargs)\n    [flags.declare_key_flag(fl) for fl in key_flags]  # pylint: disable=expression-not-assigned\n  return core_fn\n\n\ndefine_base = register_key_flags_in_core(_base.define_base)\n# Remove options not relevant for Eager from define_base().\ndefine_base_eager = register_key_flags_in_core(functools.partial(\n    _base.define_base, epochs_between_evals=False, stop_threshold=False,\n    hooks=False))\ndefine_benchmark = register_key_flags_in_core(_benchmark.define_benchmark)\ndefine_device = register_key_flags_in_core(_device.define_device)\ndefine_image = register_key_flags_in_core(_misc.define_image)\ndefine_performance = register_key_flags_in_core(_performance.define_performance)\n\n\nhelp_wrap = _conventions.help_wrap\n\n\nget_num_gpus = _base.get_num_gpus\nget_tf_dtype = _performance.get_tf_dtype\nget_loss_scale = _performance.get_loss_scale\nDTYPE_MAP = _performance.DTYPE_MAP\nrequire_cloud_storage = _device.require_cloud_storage\n'"
official/utils/flags/flags_test.py,2,"b'# Copyright 2018 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nimport unittest\n\nfrom absl import flags\nimport tensorflow as tf\n\nfrom official.utils.flags import core as flags_core  # pylint: disable=g-bad-import-order\n\n\ndef define_flags():\n  flags_core.define_base(num_gpu=False)\n  flags_core.define_performance()\n  flags_core.define_image()\n  flags_core.define_benchmark()\n\n\nclass BaseTester(unittest.TestCase):\n\n  @classmethod\n  def setUpClass(cls):\n    super(BaseTester, cls).setUpClass()\n    define_flags()\n\n  def test_default_setting(self):\n    """"""Test to ensure fields exist and defaults can be set.\n    """"""\n\n    defaults = dict(\n        data_dir=""dfgasf"",\n        model_dir=""dfsdkjgbs"",\n        train_epochs=534,\n        epochs_between_evals=15,\n        batch_size=256,\n        hooks=[""LoggingTensorHook""],\n        num_parallel_calls=18,\n        inter_op_parallelism_threads=5,\n        intra_op_parallelism_threads=10,\n        data_format=""channels_first""\n    )\n\n    flags_core.set_defaults(**defaults)\n    flags_core.parse_flags()\n\n    for key, value in defaults.items():\n      assert flags.FLAGS.get_flag_value(name=key, default=None) == value\n\n  def test_benchmark_setting(self):\n    defaults = dict(\n        hooks=[""LoggingMetricHook""],\n        benchmark_log_dir=""/tmp/12345"",\n        gcp_project=""project_abc"",\n    )\n\n    flags_core.set_defaults(**defaults)\n    flags_core.parse_flags()\n\n    for key, value in defaults.items():\n      assert flags.FLAGS.get_flag_value(name=key, default=None) == value\n\n  def test_booleans(self):\n    """"""Test to ensure boolean flags trigger as expected.\n    """"""\n\n    flags_core.parse_flags([__file__, ""--use_synthetic_data""])\n\n    assert flags.FLAGS.use_synthetic_data\n\n  def test_parse_dtype_info(self):\n    for dtype_str, tf_dtype, loss_scale in [[""fp16"", tf.float16, 128],\n                                            [""fp32"", tf.float32, 1]]:\n      flags_core.parse_flags([__file__, ""--dtype"", dtype_str])\n\n      self.assertEqual(flags_core.get_tf_dtype(flags.FLAGS), tf_dtype)\n      self.assertEqual(flags_core.get_loss_scale(flags.FLAGS), loss_scale)\n\n      flags_core.parse_flags(\n          [__file__, ""--dtype"", dtype_str, ""--loss_scale"", ""5""])\n\n      self.assertEqual(flags_core.get_loss_scale(flags.FLAGS), 5)\n\n    with self.assertRaises(SystemExit):\n      flags_core.parse_flags([__file__, ""--dtype"", ""int8""])\n\n\nif __name__ == ""__main__"":\n  unittest.main()\n'"
official/utils/logs/__init__.py,0,b''
official/utils/logs/cloud_lib.py,0,"b'# Copyright 2018 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n""""""Utilities that interact with cloud service.\n""""""\n\nimport requests\n\nGCP_METADATA_URL = ""http://metadata/computeMetadata/v1/instance/hostname""\nGCP_METADATA_HEADER = {""Metadata-Flavor"": ""Google""}\n\n\ndef on_gcp():\n  """"""Detect whether the current running environment is on GCP.""""""\n  try:\n    # Timeout in 5 seconds, in case the test environment has connectivity issue.\n    # There is not default timeout, which means it might block forever.\n    response = requests.get(\n        GCP_METADATA_URL, headers=GCP_METADATA_HEADER, timeout=5)\n    return response.status_code == 200\n  except requests.exceptions.RequestException:\n    return False\n'"
official/utils/logs/cloud_lib_test.py,0,"b'# This code is adapted from the https://github.com/tensorflow/models/tree/master/official/r1/resnet.\n# ==========================================================================================\n# NAVER\xe2\x80\x99s modifications are Copyright 2020 NAVER corp. All rights reserved.\n# ==========================================================================================\n# Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n""""""Tests for cloud_lib.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport unittest\n\nimport mock\nimport requests\n\nfrom official.utils.logs import cloud_lib\n\n\nclass CloudLibTest(unittest.TestCase):\n\n  @mock.patch(""requests.get"")\n  def test_on_gcp(self, mock_requests_get):\n    mock_response = mock.MagicMock()\n    mock_requests_get.return_value = mock_response\n    mock_response.status_code = 200\n\n    self.assertEqual(cloud_lib.on_gcp(), True)\n\n  @mock.patch(""requests.get"")\n  def test_not_on_gcp(self, mock_requests_get):\n    mock_requests_get.side_effect = requests.exceptions.ConnectionError()\n\n    self.assertEqual(cloud_lib.on_gcp(), False)\n\n\nif __name__ == ""__main__"":\n  unittest.main()\n'"
official/utils/logs/hooks.py,4,"b'# This code is adapted from the https://github.com/tensorflow/models/tree/master/official/r1/resnet.\n# ==========================================================================================\n# NAVER\xe2\x80\x99s modifications are Copyright 2020 NAVER corp. All rights reserved.\n# ==========================================================================================\n# Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n""""""Hook that counts examples per second every N steps or seconds.""""""\n\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf  # pylint: disable=g-bad-import-order\n\nfrom official.utils.logs import logger\n\n\nclass ExamplesPerSecondHook(tf.train.SessionRunHook):\n  """"""Hook to print out examples per second.\n\n  Total time is tracked and then divided by the total number of steps\n  to get the average step time and then batch_size is used to determine\n  the running average of examples per second. The examples per second for the\n  most recent interval is also logged.\n  """"""\n\n  def __init__(self,\n               batch_size,\n               every_n_steps=None,\n               every_n_secs=None,\n               warm_steps=0,\n               metric_logger=None):\n    """"""Initializer for ExamplesPerSecondHook.\n\n    Args:\n      batch_size: Total batch size across all workers used to calculate\n        examples/second from global time.\n      every_n_steps: Log stats every n steps.\n      every_n_secs: Log stats every n seconds. Exactly one of the\n        `every_n_steps` or `every_n_secs` should be set.\n      warm_steps: The number of steps to be skipped before logging and running\n        average calculation. warm_steps steps refers to global steps across all\n        workers, not on each worker\n      metric_logger: instance of `BenchmarkLogger`, the benchmark logger that\n          hook should use to write the log. If None, BaseBenchmarkLogger will\n          be used.\n\n    Raises:\n      ValueError: if neither `every_n_steps` or `every_n_secs` is set, or\n      both are set.\n    """"""\n\n    if (every_n_steps is None) == (every_n_secs is None):\n      raise ValueError(""exactly one of every_n_steps""\n                       "" and every_n_secs should be provided."")\n\n    self._logger = metric_logger or logger.BaseBenchmarkLogger()\n\n    self._timer = tf.train.SecondOrStepTimer(\n        every_steps=every_n_steps, every_secs=every_n_secs)\n\n    self._step_train_time = 0\n    self._total_steps = 0\n    self._batch_size = batch_size\n    self._warm_steps = warm_steps\n\n  def begin(self):\n    """"""Called once before using the session to check global step.""""""\n    self._global_step_tensor = tf.train.get_global_step()\n    if self._global_step_tensor is None:\n      raise RuntimeError(\n          ""Global step should be created to use StepCounterHook."")\n\n  def before_run(self, run_context):  # pylint: disable=unused-argument\n    """"""Called before each call to run().\n\n    Args:\n      run_context: A SessionRunContext object.\n\n    Returns:\n      A SessionRunArgs object or None if never triggered.\n    """"""\n    return tf.train.SessionRunArgs(self._global_step_tensor)\n\n  def after_run(self, run_context, run_values):  # pylint: disable=unused-argument\n    """"""Called after each call to run().\n\n    Args:\n      run_context: A SessionRunContext object.\n      run_values: A SessionRunValues object.\n    """"""\n    global_step = run_values.results\n\n    if self._timer.should_trigger_for_step(\n        global_step) and global_step > self._warm_steps:\n      elapsed_time, elapsed_steps = self._timer.update_last_triggered_step(\n          global_step)\n      if elapsed_time is not None:\n        self._step_train_time += elapsed_time\n        self._total_steps += elapsed_steps\n\n        # average examples per second is based on the total (accumulative)\n        # training steps and training time so far\n        average_examples_per_sec = self._batch_size * (\n            self._total_steps / self._step_train_time)\n        # current examples per second is based on the elapsed training steps\n        # and training time per batch\n        current_examples_per_sec = self._batch_size * (\n            elapsed_steps / elapsed_time)\n\n        self._logger.log_metric(\n            ""average_examples_per_sec"", average_examples_per_sec,\n            global_step=global_step)\n\n        self._logger.log_metric(\n            ""current_examples_per_sec"", current_examples_per_sec,\n            global_step=global_step)\n'"
official/utils/logs/hooks_helper.py,3,"b'# This code is adapted from the https://github.com/tensorflow/models/tree/master/official/r1/resnet.\n# ==========================================================================================\n# NAVER\xe2\x80\x99s modifications are Copyright 2020 NAVER corp. All rights reserved.\n# ==========================================================================================\n# Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n""""""Hooks helper to return a list of TensorFlow hooks for training by name.\n\nMore hooks can be added to this set. To add a new hook, 1) add the new hook to\nthe registry in HOOKS, 2) add a corresponding function that parses out necessary\nparameters.\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf  # pylint: disable=g-bad-import-order\n\nfrom official.utils.logs import hooks\nfrom official.utils.logs import logger\nfrom official.utils.logs import metric_hook\n\n_TENSORS_TO_LOG = dict((x, x) for x in [\'learning_rate\',\n                                        \'cross_entropy\',\n                                        \'train_ece\',\n                                        \'train_accuracy\'])\n\n\ndef get_train_hooks(name_list, use_tpu=False, **kwargs):\n  """"""Factory for getting a list of TensorFlow hooks for training by name.\n\n  Args:\n    name_list: a list of strings to name desired hook classes. Allowed:\n      LoggingTensorHook, ProfilerHook, ExamplesPerSecondHook, which are defined\n      as keys in HOOKS\n    use_tpu: Boolean of whether computation occurs on a TPU. This will disable\n      hooks altogether.\n    **kwargs: a dictionary of arguments to the hooks.\n\n  Returns:\n    list of instantiated hooks, ready to be used in a classifier.train call.\n\n  Raises:\n    ValueError: if an unrecognized name is passed.\n  """"""\n\n  if not name_list:\n    return []\n\n  if use_tpu:\n    tf.logging.warning(""hooks_helper received name_list `{}`, but a TPU is ""\n                       ""specified. No hooks will be used."".format(name_list))\n    return []\n\n  train_hooks = []\n  for name in name_list:\n    hook_name = HOOKS.get(name.strip().lower())\n    if hook_name is None:\n      raise ValueError(\'Unrecognized training hook requested: {}\'.format(name))\n    else:\n      train_hooks.append(hook_name(**kwargs))\n\n  return train_hooks\n\n\ndef get_logging_tensor_hook(every_n_iter=100, tensors_to_log=None, **kwargs):  # pylint: disable=unused-argument\n  """"""Function to get LoggingTensorHook.\n\n  Args:\n    every_n_iter: `int`, print the values of `tensors` once every N local\n      steps taken on the current worker.\n    tensors_to_log: List of tensor names or dictionary mapping labels to tensor\n      names. If not set, log _TENSORS_TO_LOG by default.\n    **kwargs: a dictionary of arguments to LoggingTensorHook.\n\n  Returns:\n    Returns a LoggingTensorHook with a standard set of tensors that will be\n    printed to stdout.\n  """"""\n  if tensors_to_log is None:\n    tensors_to_log = _TENSORS_TO_LOG\n\n  return tf.train.LoggingTensorHook(\n      tensors=tensors_to_log,\n      every_n_iter=every_n_iter)\n\n\ndef get_profiler_hook(model_dir, save_steps=1000, **kwargs):  # pylint: disable=unused-argument\n  """"""Function to get ProfilerHook.\n\n  Args:\n    model_dir: The directory to save the profile traces to.\n    save_steps: `int`, print profile traces every N steps.\n    **kwargs: a dictionary of arguments to ProfilerHook.\n\n  Returns:\n    Returns a ProfilerHook that writes out timelines that can be loaded into\n    profiling tools like chrome://tracing.\n  """"""\n  return tf.train.ProfilerHook(save_steps=save_steps, output_dir=model_dir)\n\n\ndef get_examples_per_second_hook(every_n_steps=100,\n                                 batch_size=128,\n                                 warm_steps=5,\n                                 **kwargs):  # pylint: disable=unused-argument\n  """"""Function to get ExamplesPerSecondHook.\n\n  Args:\n    every_n_steps: `int`, print current and average examples per second every\n      N steps.\n    batch_size: `int`, total batch size used to calculate examples/second from\n      global time.\n    warm_steps: skip this number of steps before logging and running average.\n    **kwargs: a dictionary of arguments to ExamplesPerSecondHook.\n\n  Returns:\n    Returns a ProfilerHook that writes out timelines that can be loaded into\n    profiling tools like chrome://tracing.\n  """"""\n  return hooks.ExamplesPerSecondHook(\n      batch_size=batch_size, every_n_steps=every_n_steps,\n      warm_steps=warm_steps, metric_logger=logger.get_benchmark_logger())\n\n\ndef get_logging_metric_hook(tensors_to_log=None,\n                            every_n_secs=600,\n                            **kwargs):  # pylint: disable=unused-argument\n  """"""Function to get LoggingMetricHook.\n\n  Args:\n    tensors_to_log: List of tensor names or dictionary mapping labels to tensor\n      names. If not set, log _TENSORS_TO_LOG by default.\n    every_n_secs: `int`, the frequency for logging the metric. Default to every\n      10 mins.\n\n  Returns:\n    Returns a LoggingMetricHook that saves tensor values in a JSON format.\n  """"""\n  if tensors_to_log is None:\n    tensors_to_log = _TENSORS_TO_LOG\n  return metric_hook.LoggingMetricHook(\n      tensors=tensors_to_log,\n      metric_logger=logger.get_benchmark_logger(),\n      every_n_secs=every_n_secs)\n\n\n# A dictionary to map one hook name and its corresponding function\nHOOKS = {\n    \'loggingtensorhook\': get_logging_tensor_hook,\n    \'profilerhook\': get_profiler_hook,\n    \'examplespersecondhook\': get_examples_per_second_hook,\n    \'loggingmetrichook\': get_logging_metric_hook,\n}\n'"
official/utils/logs/hooks_helper_test.py,2,"b'# This code is adapted from the https://github.com/tensorflow/models/tree/master/official/r1/resnet.\n# ==========================================================================================\n# NAVER\xe2\x80\x99s modifications are Copyright 2020 NAVER corp. All rights reserved.\n# ==========================================================================================\n# Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n""""""Tests for hooks_helper.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport unittest\n\nimport tensorflow as tf  # pylint: disable=g-bad-import-order\n\nfrom official.utils.logs import hooks_helper\n\n\nclass BaseTest(unittest.TestCase):\n\n  def test_raise_in_non_list_names(self):\n    with self.assertRaises(ValueError):\n      hooks_helper.get_train_hooks(\n          \'LoggingTensorHook, ProfilerHook\', model_dir="""", batch_size=256)\n\n  def test_raise_in_invalid_names(self):\n    invalid_names = [\'StepCounterHook\', \'StopAtStepHook\']\n    with self.assertRaises(ValueError):\n      hooks_helper.get_train_hooks(invalid_names, model_dir="""", batch_size=256)\n\n  def validate_train_hook_name(self,\n                               test_hook_name,\n                               expected_hook_name,\n                               **kwargs):\n    returned_hook = hooks_helper.get_train_hooks(\n        [test_hook_name], model_dir="""", **kwargs)\n    self.assertEqual(len(returned_hook), 1)\n    self.assertIsInstance(returned_hook[0], tf.train.SessionRunHook)\n    self.assertEqual(returned_hook[0].__class__.__name__.lower(),\n                     expected_hook_name)\n\n  def test_get_train_hooks_logging_tensor_hook(self):\n    self.validate_train_hook_name(\'LoggingTensorHook\', \'loggingtensorhook\')\n\n  def test_get_train_hooks_profiler_hook(self):\n    self.validate_train_hook_name(\'ProfilerHook\', \'profilerhook\')\n\n  def test_get_train_hooks_examples_per_second_hook(self):\n    self.validate_train_hook_name(\'ExamplesPerSecondHook\',\n                                  \'examplespersecondhook\')\n\n  def test_get_logging_metric_hook(self):\n    test_hook_name = \'LoggingMetricHook\'\n    self.validate_train_hook_name(test_hook_name, \'loggingmetrichook\')\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
official/utils/logs/hooks_test.py,11,"b'# This code is adapted from the https://github.com/tensorflow/models/tree/master/official/r1/resnet.\n# ==========================================================================================\n# NAVER\xe2\x80\x99s modifications are Copyright 2020 NAVER corp. All rights reserved.\n# ==========================================================================================\n# Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n""""""Tests for hooks.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport time\n\nimport tensorflow as tf  # pylint: disable=g-bad-import-order\n\nfrom official.utils.logs import hooks\nfrom official.utils.testing import mock_lib\n\ntf.logging.set_verbosity(tf.logging.DEBUG)\n\n\nclass ExamplesPerSecondHookTest(tf.test.TestCase):\n  """"""Tests for the ExamplesPerSecondHook.\n\n  In the test, we explicitly run global_step tensor after train_op in order to\n  keep the global_step value and the train_op (which increase the glboal_step\n  by 1) consistent. This is to correct the discrepancies in reported global_step\n  value when running on GPUs.\n  """"""\n\n  def setUp(self):\n    """"""Mock out logging calls to verify if correct info is being monitored.""""""\n    self._logger = mock_lib.MockBenchmarkLogger()\n\n    self.graph = tf.Graph()\n    with self.graph.as_default():\n      tf.train.create_global_step()\n      self.train_op = tf.assign_add(tf.train.get_global_step(), 1)\n      self.global_step = tf.train.get_global_step()\n\n  def test_raise_in_both_secs_and_steps(self):\n    with self.assertRaises(ValueError):\n      hooks.ExamplesPerSecondHook(\n          batch_size=256,\n          every_n_steps=10,\n          every_n_secs=20,\n          metric_logger=self._logger)\n\n  def test_raise_in_none_secs_and_steps(self):\n    with self.assertRaises(ValueError):\n      hooks.ExamplesPerSecondHook(\n          batch_size=256,\n          every_n_steps=None,\n          every_n_secs=None,\n          metric_logger=self._logger)\n\n  def _validate_log_every_n_steps(self, every_n_steps, warm_steps):\n    hook = hooks.ExamplesPerSecondHook(\n        batch_size=256,\n        every_n_steps=every_n_steps,\n        warm_steps=warm_steps,\n        metric_logger=self._logger)\n\n    with tf.train.MonitoredSession(\n        tf.train.ChiefSessionCreator(), [hook]) as mon_sess:\n      for _ in range(every_n_steps):\n        # Explicitly run global_step after train_op to get the accurate\n        # global_step value\n        mon_sess.run(self.train_op)\n        mon_sess.run(self.global_step)\n        # Nothing should be in the list yet\n        self.assertFalse(self._logger.logged_metric)\n\n      mon_sess.run(self.train_op)\n      global_step_val = mon_sess.run(self.global_step)\n\n      if global_step_val > warm_steps:\n        self._assert_metrics()\n      else:\n        # Nothing should be in the list yet\n        self.assertFalse(self._logger.logged_metric)\n\n      # Add additional run to verify proper reset when called multiple times.\n      prev_log_len = len(self._logger.logged_metric)\n      mon_sess.run(self.train_op)\n      global_step_val = mon_sess.run(self.global_step)\n\n      if every_n_steps == 1 and global_step_val > warm_steps:\n        # Each time, we log two additional metrics. Did exactly 2 get added?\n        self.assertEqual(len(self._logger.logged_metric), prev_log_len + 2)\n      else:\n        # No change in the size of the metric list.\n        self.assertEqual(len(self._logger.logged_metric), prev_log_len)\n\n  def test_examples_per_sec_every_1_steps(self):\n    with self.graph.as_default():\n      self._validate_log_every_n_steps(1, 0)\n\n  def test_examples_per_sec_every_5_steps(self):\n    with self.graph.as_default():\n      self._validate_log_every_n_steps(5, 0)\n\n  def test_examples_per_sec_every_1_steps_with_warm_steps(self):\n    with self.graph.as_default():\n      self._validate_log_every_n_steps(1, 10)\n\n  def test_examples_per_sec_every_5_steps_with_warm_steps(self):\n    with self.graph.as_default():\n      self._validate_log_every_n_steps(5, 10)\n\n  def _validate_log_every_n_secs(self, every_n_secs):\n    hook = hooks.ExamplesPerSecondHook(\n        batch_size=256,\n        every_n_steps=None,\n        every_n_secs=every_n_secs,\n        metric_logger=self._logger)\n\n    with tf.train.MonitoredSession(\n        tf.train.ChiefSessionCreator(), [hook]) as mon_sess:\n      # Explicitly run global_step after train_op to get the accurate\n      # global_step value\n      mon_sess.run(self.train_op)\n      mon_sess.run(self.global_step)\n      # Nothing should be in the list yet\n      self.assertFalse(self._logger.logged_metric)\n      time.sleep(every_n_secs)\n\n      mon_sess.run(self.train_op)\n      mon_sess.run(self.global_step)\n      self._assert_metrics()\n\n  def test_examples_per_sec_every_1_secs(self):\n    with self.graph.as_default():\n      self._validate_log_every_n_secs(1)\n\n  def test_examples_per_sec_every_5_secs(self):\n    with self.graph.as_default():\n      self._validate_log_every_n_secs(5)\n\n  def _assert_metrics(self):\n    metrics = self._logger.logged_metric\n    self.assertEqual(metrics[-2][""name""], ""average_examples_per_sec"")\n    self.assertEqual(metrics[-1][""name""], ""current_examples_per_sec"")\n\n\nif __name__ == ""__main__"":\n  tf.test.main()\n'"
official/utils/logs/logger.py,15,"b'# Copyright 2018 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n""""""Logging utilities for benchmark.\n\nFor collecting local environment metrics like CPU and memory, certain python\npackages need be installed. See README for details.\n""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport contextlib\nimport datetime\nimport json\nimport multiprocessing\nimport numbers\nimport os\nimport threading\nimport uuid\n\nfrom six.moves import _thread as thread\nfrom absl import flags\nimport tensorflow as tf\nfrom tensorflow.python.client import device_lib\n\nfrom official.utils.logs import cloud_lib\n\nMETRIC_LOG_FILE_NAME = ""metric.log""\nBENCHMARK_RUN_LOG_FILE_NAME = ""benchmark_run.log""\n_DATE_TIME_FORMAT_PATTERN = ""%Y-%m-%dT%H:%M:%S.%fZ""\nGCP_TEST_ENV = ""GCP""\nRUN_STATUS_SUCCESS = ""success""\nRUN_STATUS_FAILURE = ""failure""\nRUN_STATUS_RUNNING = ""running""\n\n\nFLAGS = flags.FLAGS\n\n# Don\'t use it directly. Use get_benchmark_logger to access a logger.\n_benchmark_logger = None\n_logger_lock = threading.Lock()\n\n\ndef config_benchmark_logger(flag_obj=None):\n  """"""Config the global benchmark logger.""""""\n  _logger_lock.acquire()\n  try:\n    global _benchmark_logger\n    if not flag_obj:\n      flag_obj = FLAGS\n\n    if (not hasattr(flag_obj, ""benchmark_logger_type"") or\n        flag_obj.benchmark_logger_type == ""BaseBenchmarkLogger""):\n      _benchmark_logger = BaseBenchmarkLogger()\n    elif flag_obj.benchmark_logger_type == ""BenchmarkFileLogger"":\n      _benchmark_logger = BenchmarkFileLogger(flag_obj.benchmark_log_dir)\n    elif flag_obj.benchmark_logger_type == ""BenchmarkBigQueryLogger"":\n      from official.benchmark import benchmark_uploader as bu  # pylint: disable=g-import-not-at-top\n      bq_uploader = bu.BigQueryUploader(gcp_project=flag_obj.gcp_project)\n      _benchmark_logger = BenchmarkBigQueryLogger(\n          bigquery_uploader=bq_uploader,\n          bigquery_data_set=flag_obj.bigquery_data_set,\n          bigquery_run_table=flag_obj.bigquery_run_table,\n          bigquery_run_status_table=flag_obj.bigquery_run_status_table,\n          bigquery_metric_table=flag_obj.bigquery_metric_table,\n          run_id=str(uuid.uuid4()))\n    else:\n      raise ValueError(""Unrecognized benchmark_logger_type: %s""\n                       % flag_obj.benchmark_logger_type)\n\n  finally:\n    _logger_lock.release()\n  return _benchmark_logger\n\n\ndef get_benchmark_logger():\n  if not _benchmark_logger:\n    config_benchmark_logger()\n  return _benchmark_logger\n\n\n@contextlib.contextmanager\ndef benchmark_context(flag_obj):\n  """"""Context of benchmark, which will update status of the run accordingly.""""""\n  benchmark_logger = config_benchmark_logger(flag_obj)\n  try:\n    yield\n    benchmark_logger.on_finish(RUN_STATUS_SUCCESS)\n  except Exception:  # pylint: disable=broad-except\n    # Catch all the exception, update the run status to be failure, and re-raise\n    benchmark_logger.on_finish(RUN_STATUS_FAILURE)\n    raise\n\n\nclass BaseBenchmarkLogger(object):\n  """"""Class to log the benchmark information to STDOUT.""""""\n\n  def log_evaluation_result(self, eval_results):\n    """"""Log the evaluation result.\n\n    The evaluate result is a dictionary that contains metrics defined in\n    model_fn. It also contains a entry for global_step which contains the value\n    of the global step when evaluation was performed.\n\n    Args:\n      eval_results: dict, the result of evaluate.\n    """"""\n    if not isinstance(eval_results, dict):\n      tf.logging.warning(""eval_results should be dictionary for logging. ""\n                         ""Got %s"", type(eval_results))\n      return\n    global_step = eval_results[tf.GraphKeys.GLOBAL_STEP]\n    for key in sorted(eval_results):\n      if key != tf.GraphKeys.GLOBAL_STEP:\n        self.log_metric(key, eval_results[key], global_step=global_step)\n\n  def log_metric(self, name, value, unit=None, global_step=None, extras=None):\n    """"""Log the benchmark metric information to local file.\n\n    Currently the logging is done in a synchronized way. This should be updated\n    to log asynchronously.\n\n    Args:\n      name: string, the name of the metric to log.\n      value: number, the value of the metric. The value will not be logged if it\n        is not a number type.\n      unit: string, the unit of the metric, E.g ""image per second"".\n      global_step: int, the global_step when the metric is logged.\n      extras: map of string:string, the extra information about the metric.\n    """"""\n    metric = _process_metric_to_json(name, value, unit, global_step, extras)\n    if metric:\n      tf.logging.info(""Benchmark metric: %s"", metric)\n\n  def log_run_info(self, model_name, dataset_name, run_params, test_id=None):\n    tf.logging.info(""Benchmark run: %s"",\n                    _gather_run_info(model_name, dataset_name, run_params,\n                                     test_id))\n\n  def on_finish(self, status):\n    pass\n\n\nclass BenchmarkFileLogger(BaseBenchmarkLogger):\n  """"""Class to log the benchmark information to local disk.""""""\n\n  def __init__(self, logging_dir):\n    super(BenchmarkFileLogger, self).__init__()\n    self._logging_dir = logging_dir\n    if not tf.gfile.IsDirectory(self._logging_dir):\n      tf.gfile.MakeDirs(self._logging_dir)\n    self._metric_file_handler = tf.gfile.GFile(\n        os.path.join(self._logging_dir, METRIC_LOG_FILE_NAME), ""a"")\n\n  def log_metric(self, name, value, unit=None, global_step=None, extras=None):\n    """"""Log the benchmark metric information to local file.\n\n    Currently the logging is done in a synchronized way. This should be updated\n    to log asynchronously.\n\n    Args:\n      name: string, the name of the metric to log.\n      value: number, the value of the metric. The value will not be logged if it\n        is not a number type.\n      unit: string, the unit of the metric, E.g ""image per second"".\n      global_step: int, the global_step when the metric is logged.\n      extras: map of string:string, the extra information about the metric.\n    """"""\n    metric = _process_metric_to_json(name, value, unit, global_step, extras)\n    if metric:\n      try:\n        json.dump(metric, self._metric_file_handler)\n        self._metric_file_handler.write(""\\n"")\n        self._metric_file_handler.flush()\n      except (TypeError, ValueError) as e:\n        tf.logging.warning(""Failed to dump metric to log file: ""\n                           ""name %s, value %s, error %s"", name, value, e)\n\n  def log_run_info(self, model_name, dataset_name, run_params, test_id=None):\n    """"""Collect most of the TF runtime information for the local env.\n\n    The schema of the run info follows official/benchmark/datastore/schema.\n\n    Args:\n      model_name: string, the name of the model.\n      dataset_name: string, the name of dataset for training and evaluation.\n      run_params: dict, the dictionary of parameters for the run, it could\n        include hyperparameters or other params that are important for the run.\n      test_id: string, the unique name of the test run by the combination of key\n        parameters, eg batch size, num of GPU. It is hardware independent.\n    """"""\n    run_info = _gather_run_info(model_name, dataset_name, run_params, test_id)\n\n    with tf.gfile.GFile(os.path.join(\n        self._logging_dir, BENCHMARK_RUN_LOG_FILE_NAME), ""w"") as f:\n      try:\n        json.dump(run_info, f)\n        f.write(""\\n"")\n      except (TypeError, ValueError) as e:\n        tf.logging.warning(""Failed to dump benchmark run info to log file: %s"",\n                           e)\n\n  def on_finish(self, status):\n    self._metric_file_handler.flush()\n    self._metric_file_handler.close()\n\n\nclass BenchmarkBigQueryLogger(BaseBenchmarkLogger):\n  """"""Class to log the benchmark information to BigQuery data store.""""""\n\n  def __init__(self,\n               bigquery_uploader,\n               bigquery_data_set,\n               bigquery_run_table,\n               bigquery_run_status_table,\n               bigquery_metric_table,\n               run_id):\n    super(BenchmarkBigQueryLogger, self).__init__()\n    self._bigquery_uploader = bigquery_uploader\n    self._bigquery_data_set = bigquery_data_set\n    self._bigquery_run_table = bigquery_run_table\n    self._bigquery_run_status_table = bigquery_run_status_table\n    self._bigquery_metric_table = bigquery_metric_table\n    self._run_id = run_id\n\n  def log_metric(self, name, value, unit=None, global_step=None, extras=None):\n    """"""Log the benchmark metric information to bigquery.\n\n    Args:\n      name: string, the name of the metric to log.\n      value: number, the value of the metric. The value will not be logged if it\n        is not a number type.\n      unit: string, the unit of the metric, E.g ""image per second"".\n      global_step: int, the global_step when the metric is logged.\n      extras: map of string:string, the extra information about the metric.\n    """"""\n    metric = _process_metric_to_json(name, value, unit, global_step, extras)\n    if metric:\n      # Starting new thread for bigquery upload in case it might take long time\n      # and impact the benchmark and performance measurement. Starting a new\n      # thread might have potential performance impact for model that run on\n      # CPU.\n      thread.start_new_thread(\n          self._bigquery_uploader.upload_benchmark_metric_json,\n          (self._bigquery_data_set,\n           self._bigquery_metric_table,\n           self._run_id,\n           [metric]))\n\n  def log_run_info(self, model_name, dataset_name, run_params, test_id=None):\n    """"""Collect most of the TF runtime information for the local env.\n\n    The schema of the run info follows official/benchmark/datastore/schema.\n\n    Args:\n      model_name: string, the name of the model.\n      dataset_name: string, the name of dataset for training and evaluation.\n      run_params: dict, the dictionary of parameters for the run, it could\n        include hyperparameters or other params that are important for the run.\n      test_id: string, the unique name of the test run by the combination of key\n        parameters, eg batch size, num of GPU. It is hardware independent.\n    """"""\n    run_info = _gather_run_info(model_name, dataset_name, run_params, test_id)\n    # Starting new thread for bigquery upload in case it might take long time\n    # and impact the benchmark and performance measurement. Starting a new\n    # thread might have potential performance impact for model that run on CPU.\n    thread.start_new_thread(\n        self._bigquery_uploader.upload_benchmark_run_json,\n        (self._bigquery_data_set,\n         self._bigquery_run_table,\n         self._run_id,\n         run_info))\n    thread.start_new_thread(\n        self._bigquery_uploader.insert_run_status,\n        (self._bigquery_data_set,\n         self._bigquery_run_status_table,\n         self._run_id,\n         RUN_STATUS_RUNNING))\n\n  def on_finish(self, status):\n    thread.start_new_thread(\n        self._bigquery_uploader.update_run_status,\n        (self._bigquery_data_set,\n         self._bigquery_run_status_table,\n         self._run_id,\n         status))\n\n\ndef _gather_run_info(model_name, dataset_name, run_params, test_id):\n  """"""Collect the benchmark run information for the local environment.""""""\n  run_info = {\n      ""model_name"": model_name,\n      ""dataset"": {""name"": dataset_name},\n      ""machine_config"": {},\n      ""test_id"": test_id,\n      ""run_date"": datetime.datetime.utcnow().strftime(\n          _DATE_TIME_FORMAT_PATTERN)}\n  _collect_tensorflow_info(run_info)\n  _collect_tensorflow_environment_variables(run_info)\n  _collect_run_params(run_info, run_params)\n  _collect_cpu_info(run_info)\n  _collect_gpu_info(run_info)\n  _collect_memory_info(run_info)\n  _collect_test_environment(run_info)\n  return run_info\n\n\ndef _process_metric_to_json(\n    name, value, unit=None, global_step=None, extras=None):\n  """"""Validate the metric data and generate JSON for insert.""""""\n  if not isinstance(value, numbers.Number):\n    tf.logging.warning(\n        ""Metric value to log should be a number. Got %s"", type(value))\n    return None\n\n  extras = _convert_to_json_dict(extras)\n  return {\n      ""name"": name,\n      ""value"": float(value),\n      ""unit"": unit,\n      ""global_step"": global_step,\n      ""timestamp"": datetime.datetime.utcnow().strftime(\n          _DATE_TIME_FORMAT_PATTERN),\n      ""extras"": extras}\n\n\ndef _collect_tensorflow_info(run_info):\n  run_info[""tensorflow_version""] = {\n      ""version"": tf.VERSION, ""git_hash"": tf.GIT_VERSION}\n\n\ndef _collect_run_params(run_info, run_params):\n  """"""Log the parameter information for the benchmark run.""""""\n  def process_param(name, value):\n    type_check = {\n        str: {""name"": name, ""string_value"": value},\n        int: {""name"": name, ""long_value"": value},\n        bool: {""name"": name, ""bool_value"": str(value)},\n        float: {""name"": name, ""float_value"": value},\n    }\n    return type_check.get(type(value),\n                          {""name"": name, ""string_value"": str(value)})\n  if run_params:\n    run_info[""run_parameters""] = [\n        process_param(k, v) for k, v in sorted(run_params.items())]\n\n\ndef _collect_tensorflow_environment_variables(run_info):\n  run_info[""tensorflow_environment_variables""] = [\n      {""name"": k, ""value"": v}\n      for k, v in sorted(os.environ.items()) if k.startswith(""TF_"")]\n\n\n# The following code is mirrored from tensorflow/tools/test/system_info_lib\n# which is not exposed for import.\ndef _collect_cpu_info(run_info):\n  """"""Collect the CPU information for the local environment.""""""\n  cpu_info = {}\n\n  cpu_info[""num_cores""] = multiprocessing.cpu_count()\n\n  try:\n    # Note: cpuinfo is not installed in the TensorFlow OSS tree.\n    # It is installable via pip.\n    import cpuinfo    # pylint: disable=g-import-not-at-top\n\n    info = cpuinfo.get_cpu_info()\n    cpu_info[""cpu_info""] = info[""brand""]\n    cpu_info[""mhz_per_cpu""] = info[""hz_advertised_raw""][0] / 1.0e6\n\n    run_info[""machine_config""][""cpu_info""] = cpu_info\n  except ImportError:\n    tf.logging.warn(""\'cpuinfo\' not imported. CPU info will not be logged."")\n\n\ndef _collect_gpu_info(run_info):\n  """"""Collect local GPU information by TF device library.""""""\n  gpu_info = {}\n  local_device_protos = device_lib.list_local_devices()\n\n  gpu_info[""count""] = len([d for d in local_device_protos\n                           if d.device_type == ""GPU""])\n  # The device description usually is a JSON string, which contains the GPU\n  # model info, eg:\n  # ""device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0""\n  for d in local_device_protos:\n    if d.device_type == ""GPU"":\n      gpu_info[""model""] = _parse_gpu_model(d.physical_device_desc)\n      # Assume all the GPU connected are same model\n      break\n  run_info[""machine_config""][""gpu_info""] = gpu_info\n\n\ndef _collect_memory_info(run_info):\n  try:\n    # Note: psutil is not installed in the TensorFlow OSS tree.\n    # It is installable via pip.\n    import psutil   # pylint: disable=g-import-not-at-top\n    vmem = psutil.virtual_memory()\n    run_info[""machine_config""][""memory_total""] = vmem.total\n    run_info[""machine_config""][""memory_available""] = vmem.available\n  except ImportError:\n    tf.logging.warn(""\'psutil\' not imported. Memory info will not be logged."")\n\n\ndef _collect_test_environment(run_info):\n  """"""Detect the local environment, eg GCE, AWS or DGX, etc.""""""\n  if cloud_lib.on_gcp():\n    run_info[""test_environment""] = GCP_TEST_ENV\n  # TODO(scottzhu): Add more testing env detection for other platform\n\n\ndef _parse_gpu_model(physical_device_desc):\n  # Assume all the GPU connected are same model\n  for kv in physical_device_desc.split("",""):\n    k, _, v = kv.partition("":"")\n    if k.strip() == ""name"":\n      return v.strip()\n  return None\n\n\ndef _convert_to_json_dict(input_dict):\n  if input_dict:\n    return [{""name"": k, ""value"": v} for k, v in sorted(input_dict.items())]\n  else:\n    return []\n'"
official/utils/logs/logger_test.py,28,"b'# Copyright 2018 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n""""""Tests for benchmark logger.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport json\nimport os\nimport tempfile\nimport time\nimport unittest\n\nimport mock\nfrom absl.testing import flagsaver\nimport tensorflow as tf  # pylint: disable=g-bad-import-order\n\ntry:\n  from google.cloud import bigquery\nexcept ImportError:\n  bigquery = None\n\nfrom official.utils.flags import core as flags_core\nfrom official.utils.logs import logger\n\n\nclass BenchmarkLoggerTest(tf.test.TestCase):\n\n  @classmethod\n  def setUpClass(cls):  # pylint: disable=invalid-name\n    super(BenchmarkLoggerTest, cls).setUpClass()\n    flags_core.define_benchmark()\n\n  def test_get_default_benchmark_logger(self):\n    with flagsaver.flagsaver(benchmark_logger_type=\'foo\'):\n      self.assertIsInstance(logger.get_benchmark_logger(),\n                            logger.BaseBenchmarkLogger)\n\n  def test_config_base_benchmark_logger(self):\n    with flagsaver.flagsaver(benchmark_logger_type=\'BaseBenchmarkLogger\'):\n      logger.config_benchmark_logger()\n      self.assertIsInstance(logger.get_benchmark_logger(),\n                            logger.BaseBenchmarkLogger)\n\n  def test_config_benchmark_file_logger(self):\n    # Set the benchmark_log_dir first since the benchmark_logger_type will need\n    # the value to be set when it does the validation.\n    with flagsaver.flagsaver(benchmark_log_dir=\'/tmp\'):\n      with flagsaver.flagsaver(benchmark_logger_type=\'BenchmarkFileLogger\'):\n        logger.config_benchmark_logger()\n        self.assertIsInstance(logger.get_benchmark_logger(),\n                              logger.BenchmarkFileLogger)\n\n  @unittest.skipIf(bigquery is None, \'Bigquery dependency is not installed.\')\n  @mock.patch.object(bigquery, ""Client"")\n  def test_config_benchmark_bigquery_logger(self, mock_bigquery_client):\n    with flagsaver.flagsaver(benchmark_logger_type=\'BenchmarkBigQueryLogger\'):\n      logger.config_benchmark_logger()\n      self.assertIsInstance(logger.get_benchmark_logger(),\n                            logger.BenchmarkBigQueryLogger)\n\n  @mock.patch(""official.utils.logs.logger.config_benchmark_logger"")\n  def test_benchmark_context(self, mock_config_benchmark_logger):\n    mock_logger = mock.MagicMock()\n    mock_config_benchmark_logger.return_value = mock_logger\n    with logger.benchmark_context(None):\n      tf.logging.info(""start benchmarking"")\n    mock_logger.on_finish.assert_called_once_with(logger.RUN_STATUS_SUCCESS)\n\n  @mock.patch(""official.utils.logs.logger.config_benchmark_logger"")\n  def test_benchmark_context_failure(self, mock_config_benchmark_logger):\n    mock_logger = mock.MagicMock()\n    mock_config_benchmark_logger.return_value = mock_logger\n    with self.assertRaises(RuntimeError):\n      with logger.benchmark_context(None):\n        raise RuntimeError(""training error"")\n    mock_logger.on_finish.assert_called_once_with(logger.RUN_STATUS_FAILURE)\n\n\nclass BaseBenchmarkLoggerTest(tf.test.TestCase):\n\n  def setUp(self):\n    super(BaseBenchmarkLoggerTest, self).setUp()\n    self._actual_log = tf.logging.info\n    self.logged_message = None\n\n    def mock_log(*args, **kwargs):\n      self.logged_message = args\n      self._actual_log(*args, **kwargs)\n\n    tf.logging.info = mock_log\n\n  def tearDown(self):\n    super(BaseBenchmarkLoggerTest, self).tearDown()\n    tf.logging.info = self._actual_log\n\n  def test_log_metric(self):\n    log = logger.BaseBenchmarkLogger()\n    log.log_metric(""accuracy"", 0.999, global_step=1e4, extras={""name"": ""value""})\n\n    expected_log_prefix = ""Benchmark metric:""\n    self.assertRegexpMatches(str(self.logged_message), expected_log_prefix)\n\n\nclass BenchmarkFileLoggerTest(tf.test.TestCase):\n\n  def setUp(self):\n    super(BenchmarkFileLoggerTest, self).setUp()\n    # Avoid pulling extra env vars from test environment which affects the test\n    # result, eg. Kokoro test has a TF_PKG env which affect the test case\n    # test_collect_tensorflow_environment_variables()\n    self.original_environ = dict(os.environ)\n    os.environ.clear()\n\n  def tearDown(self):\n    super(BenchmarkFileLoggerTest, self).tearDown()\n    tf.gfile.DeleteRecursively(self.get_temp_dir())\n    os.environ.clear()\n    os.environ.update(self.original_environ)\n\n  def test_create_logging_dir(self):\n    non_exist_temp_dir = os.path.join(self.get_temp_dir(), ""unknown_dir"")\n    self.assertFalse(tf.gfile.IsDirectory(non_exist_temp_dir))\n\n    logger.BenchmarkFileLogger(non_exist_temp_dir)\n    self.assertTrue(tf.gfile.IsDirectory(non_exist_temp_dir))\n\n  def test_log_metric(self):\n    log_dir = tempfile.mkdtemp(dir=self.get_temp_dir())\n    log = logger.BenchmarkFileLogger(log_dir)\n    log.log_metric(""accuracy"", 0.999, global_step=1e4, extras={""name"": ""value""})\n\n    metric_log = os.path.join(log_dir, ""metric.log"")\n    self.assertTrue(tf.gfile.Exists(metric_log))\n    with tf.gfile.GFile(metric_log) as f:\n      metric = json.loads(f.readline())\n      self.assertEqual(metric[""name""], ""accuracy"")\n      self.assertEqual(metric[""value""], 0.999)\n      self.assertEqual(metric[""unit""], None)\n      self.assertEqual(metric[""global_step""], 1e4)\n      self.assertEqual(metric[""extras""], [{""name"": ""name"", ""value"": ""value""}])\n\n  def test_log_multiple_metrics(self):\n    log_dir = tempfile.mkdtemp(dir=self.get_temp_dir())\n    log = logger.BenchmarkFileLogger(log_dir)\n    log.log_metric(""accuracy"", 0.999, global_step=1e4, extras={""name"": ""value""})\n    log.log_metric(""loss"", 0.02, global_step=1e4)\n\n    metric_log = os.path.join(log_dir, ""metric.log"")\n    self.assertTrue(tf.gfile.Exists(metric_log))\n    with tf.gfile.GFile(metric_log) as f:\n      accuracy = json.loads(f.readline())\n      self.assertEqual(accuracy[""name""], ""accuracy"")\n      self.assertEqual(accuracy[""value""], 0.999)\n      self.assertEqual(accuracy[""unit""], None)\n      self.assertEqual(accuracy[""global_step""], 1e4)\n      self.assertEqual(accuracy[""extras""], [{""name"": ""name"", ""value"": ""value""}])\n\n      loss = json.loads(f.readline())\n      self.assertEqual(loss[""name""], ""loss"")\n      self.assertEqual(loss[""value""], 0.02)\n      self.assertEqual(loss[""unit""], None)\n      self.assertEqual(loss[""global_step""], 1e4)\n      self.assertEqual(loss[""extras""], [])\n\n  def test_log_non_number_value(self):\n    log_dir = tempfile.mkdtemp(dir=self.get_temp_dir())\n    log = logger.BenchmarkFileLogger(log_dir)\n    const = tf.constant(1)\n    log.log_metric(""accuracy"", const)\n\n    metric_log = os.path.join(log_dir, ""metric.log"")\n    self.assertFalse(tf.gfile.Exists(metric_log))\n\n  def test_log_evaluation_result(self):\n    eval_result = {""loss"": 0.46237424,\n                   ""global_step"": 207082,\n                   ""accuracy"": 0.9285}\n    log_dir = tempfile.mkdtemp(dir=self.get_temp_dir())\n    log = logger.BenchmarkFileLogger(log_dir)\n    log.log_evaluation_result(eval_result)\n\n    metric_log = os.path.join(log_dir, ""metric.log"")\n    self.assertTrue(tf.gfile.Exists(metric_log))\n    with tf.gfile.GFile(metric_log) as f:\n      accuracy = json.loads(f.readline())\n      self.assertEqual(accuracy[""name""], ""accuracy"")\n      self.assertEqual(accuracy[""value""], 0.9285)\n      self.assertEqual(accuracy[""unit""], None)\n      self.assertEqual(accuracy[""global_step""], 207082)\n\n      loss = json.loads(f.readline())\n      self.assertEqual(loss[""name""], ""loss"")\n      self.assertEqual(loss[""value""], 0.46237424)\n      self.assertEqual(loss[""unit""], None)\n      self.assertEqual(loss[""global_step""], 207082)\n\n  def test_log_evaluation_result_with_invalid_type(self):\n    eval_result = ""{\'loss\': 0.46237424, \'global_step\': 207082}""\n    log_dir = tempfile.mkdtemp(dir=self.get_temp_dir())\n    log = logger.BenchmarkFileLogger(log_dir)\n    log.log_evaluation_result(eval_result)\n\n    metric_log = os.path.join(log_dir, ""metric.log"")\n    self.assertFalse(tf.gfile.Exists(metric_log))\n\n  @mock.patch(""official.utils.logs.logger._gather_run_info"")\n  def test_log_run_info(self, mock_gather_run_info):\n    log_dir = tempfile.mkdtemp(dir=self.get_temp_dir())\n    log = logger.BenchmarkFileLogger(log_dir)\n    run_info = {""model_name"": ""model_name"",\n                ""dataset"": ""dataset_name"",\n                ""run_info"": ""run_value""}\n    mock_gather_run_info.return_value = run_info\n    log.log_run_info(""model_name"", ""dataset_name"", {})\n\n    run_log = os.path.join(log_dir, ""benchmark_run.log"")\n    self.assertTrue(tf.gfile.Exists(run_log))\n    with tf.gfile.GFile(run_log) as f:\n      run_info = json.loads(f.readline())\n      self.assertEqual(run_info[""model_name""], ""model_name"")\n      self.assertEqual(run_info[""dataset""], ""dataset_name"")\n      self.assertEqual(run_info[""run_info""], ""run_value"")\n\n  def test_collect_tensorflow_info(self):\n    run_info = {}\n    logger._collect_tensorflow_info(run_info)\n    self.assertNotEqual(run_info[""tensorflow_version""], {})\n    self.assertEqual(run_info[""tensorflow_version""][""version""], tf.VERSION)\n    self.assertEqual(run_info[""tensorflow_version""][""git_hash""], tf.GIT_VERSION)\n\n  def test_collect_run_params(self):\n    run_info = {}\n    run_parameters = {\n        ""batch_size"": 32,\n        ""synthetic_data"": True,\n        ""train_epochs"": 100.00,\n        ""dtype"": ""fp16"",\n        ""resnet_size"": 50,\n        ""random_tensor"": tf.constant(2.0)\n    }\n    logger._collect_run_params(run_info, run_parameters)\n    self.assertEqual(len(run_info[""run_parameters""]), 6)\n    self.assertEqual(run_info[""run_parameters""][0],\n                     {""name"": ""batch_size"", ""long_value"": 32})\n    self.assertEqual(run_info[""run_parameters""][1],\n                     {""name"": ""dtype"", ""string_value"": ""fp16""})\n    self.assertEqual(run_info[""run_parameters""][2],\n                     {""name"": ""random_tensor"", ""string_value"":\n                          ""Tensor(\\""Const:0\\"", shape=(), dtype=float32)""})\n    self.assertEqual(run_info[""run_parameters""][3],\n                     {""name"": ""resnet_size"", ""long_value"": 50})\n    self.assertEqual(run_info[""run_parameters""][4],\n                     {""name"": ""synthetic_data"", ""bool_value"": ""True""})\n    self.assertEqual(run_info[""run_parameters""][5],\n                     {""name"": ""train_epochs"", ""float_value"": 100.00})\n\n  def test_collect_tensorflow_environment_variables(self):\n    os.environ[""TF_ENABLE_WINOGRAD_NONFUSED""] = ""1""\n    os.environ[""TF_OTHER""] = ""2""\n    os.environ[""OTHER""] = ""3""\n\n    run_info = {}\n    logger._collect_tensorflow_environment_variables(run_info)\n    self.assertIsNotNone(run_info[""tensorflow_environment_variables""])\n    expected_tf_envs = [\n        {""name"": ""TF_ENABLE_WINOGRAD_NONFUSED"", ""value"": ""1""},\n        {""name"": ""TF_OTHER"", ""value"": ""2""},\n    ]\n    self.assertEqual(run_info[""tensorflow_environment_variables""],\n                     expected_tf_envs)\n\n  @unittest.skipUnless(tf.test.is_built_with_cuda(), ""requires GPU"")\n  def test_collect_gpu_info(self):\n    run_info = {""machine_config"": {}}\n    logger._collect_gpu_info(run_info)\n    self.assertNotEqual(run_info[""machine_config""][""gpu_info""], {})\n\n  def test_collect_memory_info(self):\n    run_info = {""machine_config"": {}}\n    logger._collect_memory_info(run_info)\n    self.assertIsNotNone(run_info[""machine_config""][""memory_total""])\n    self.assertIsNotNone(run_info[""machine_config""][""memory_available""])\n\n\n@unittest.skipIf(bigquery is None, \'Bigquery dependency is not installed.\')\nclass BenchmarkBigQueryLoggerTest(tf.test.TestCase):\n\n  def setUp(self):\n    super(BenchmarkBigQueryLoggerTest, self).setUp()\n    # Avoid pulling extra env vars from test environment which affects the test\n    # result, eg. Kokoro test has a TF_PKG env which affect the test case\n    # test_collect_tensorflow_environment_variables()\n    self.original_environ = dict(os.environ)\n    os.environ.clear()\n\n    self.mock_bq_uploader = mock.MagicMock()\n    self.logger = logger.BenchmarkBigQueryLogger(\n        self.mock_bq_uploader, ""dataset"", ""run_table"", ""run_status_table"",\n        ""metric_table"", ""run_id"")\n\n  def tearDown(self):\n    super(BenchmarkBigQueryLoggerTest, self).tearDown()\n    tf.gfile.DeleteRecursively(self.get_temp_dir())\n    os.environ.clear()\n    os.environ.update(self.original_environ)\n\n  def test_log_metric(self):\n    self.logger.log_metric(\n        ""accuracy"", 0.999, global_step=1e4, extras={""name"": ""value""})\n    expected_metric_json = [{\n        ""name"": ""accuracy"",\n        ""value"": 0.999,\n        ""unit"": None,\n        ""global_step"": 1e4,\n        ""timestamp"": mock.ANY,\n        ""extras"": [{""name"": ""name"", ""value"": ""value""}]\n    }]\n    # log_metric will call upload_benchmark_metric_json in a separate thread.\n    # Give it some grace period for the new thread before assert.\n    time.sleep(1)\n    self.mock_bq_uploader.upload_benchmark_metric_json.assert_called_once_with(\n        ""dataset"", ""metric_table"", ""run_id"", expected_metric_json)\n\n  @mock.patch(""official.utils.logs.logger._gather_run_info"")\n  def test_log_run_info(self, mock_gather_run_info):\n    run_info = {""model_name"": ""model_name"",\n                ""dataset"": ""dataset_name"",\n                ""run_info"": ""run_value""}\n    mock_gather_run_info.return_value = run_info\n    self.logger.log_run_info(""model_name"", ""dataset_name"", {})\n    # log_metric will call upload_benchmark_metric_json in a separate thread.\n    # Give it some grace period for the new thread before assert.\n    time.sleep(1)\n    self.mock_bq_uploader.upload_benchmark_run_json.assert_called_once_with(\n        ""dataset"", ""run_table"", ""run_id"", run_info)\n    self.mock_bq_uploader.insert_run_status.assert_called_once_with(\n        ""dataset"", ""run_status_table"", ""run_id"", ""running"")\n\n  def test_on_finish(self):\n    self.logger.on_finish(logger.RUN_STATUS_SUCCESS)\n    # log_metric will call upload_benchmark_metric_json in a separate thread.\n    # Give it some grace period for the new thread before assert.\n    time.sleep(1)\n    self.mock_bq_uploader.update_run_status.assert_called_once_with(\n        ""dataset"", ""run_status_table"", ""run_id"", logger.RUN_STATUS_SUCCESS)\n\n\nif __name__ == ""__main__"":\n  tf.test.main()\n'"
official/utils/logs/metric_hook.py,3,"b'# Copyright 2018 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Session hook for logging benchmark metric.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf  # pylint: disable=g-bad-import-order\n\n\nclass LoggingMetricHook(tf.train.LoggingTensorHook):\n  """"""Hook to log benchmark metric information.\n\n  This hook is very similar as tf.train.LoggingTensorHook, which logs given\n  tensors every N local steps, every N seconds, or at the end. The metric\n  information will be logged to given log_dir or via metric_logger in JSON\n  format, which can be consumed by data analysis pipeline later.\n\n  Note that if `at_end` is True, `tensors` should not include any tensor\n  whose evaluation produces a side effect such as consuming additional inputs.\n  """"""\n\n  def __init__(self, tensors, metric_logger=None,\n               every_n_iter=None, every_n_secs=None, at_end=False):\n    """"""Initializer for LoggingMetricHook.\n\n    Args:\n      tensors: `dict` that maps string-valued tags to tensors/tensor names,\n          or `iterable` of tensors/tensor names.\n      metric_logger: instance of `BenchmarkLogger`, the benchmark logger that\n          hook should use to write the log.\n      every_n_iter: `int`, print the values of `tensors` once every N local\n          steps taken on the current worker.\n      every_n_secs: `int` or `float`, print the values of `tensors` once every N\n          seconds. Exactly one of `every_n_iter` and `every_n_secs` should be\n          provided.\n      at_end: `bool` specifying whether to print the values of `tensors` at the\n          end of the run.\n\n    Raises:\n      ValueError:\n        1. `every_n_iter` is non-positive, or\n        2. Exactly one of every_n_iter and every_n_secs should be provided.\n        3. Exactly one of log_dir and metric_logger should be provided.\n    """"""\n    super(LoggingMetricHook, self).__init__(\n        tensors=tensors,\n        every_n_iter=every_n_iter,\n        every_n_secs=every_n_secs,\n        at_end=at_end)\n\n    if metric_logger is None:\n      raise ValueError(""metric_logger should be provided."")\n    self._logger = metric_logger\n\n  def begin(self):\n    super(LoggingMetricHook, self).begin()\n    self._global_step_tensor = tf.train.get_global_step()\n    if self._global_step_tensor is None:\n      raise RuntimeError(\n          ""Global step should be created to use LoggingMetricHook."")\n    if self._global_step_tensor.name not in self._current_tensors:\n      self._current_tensors[self._global_step_tensor.name] = (\n          self._global_step_tensor)\n\n  def after_run(self, unused_run_context, run_values):\n    # should_trigger is a internal state that populated at before_run, and it is\n    # using self_timer to determine whether it should trigger.\n    if self._should_trigger:\n      self._log_metric(run_values.results)\n\n    self._iter_count += 1\n\n  def end(self, session):\n    if self._log_at_end:\n      values = session.run(self._current_tensors)\n      self._log_metric(values)\n\n  def _log_metric(self, tensor_values):\n    self._timer.update_last_triggered_step(self._iter_count)\n    global_step = tensor_values[self._global_step_tensor.name]\n    # self._tag_order is populated during the init of LoggingTensorHook\n    for tag in self._tag_order:\n      self._logger.log_metric(tag, tensor_values[tag], global_step=global_step)\n'"
official/utils/logs/metric_hook_test.py,30,"b'# Copyright 2018 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Tests for metric_hook.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tempfile\nimport time\n\nimport tensorflow as tf  # pylint: disable=g-bad-import-order\nfrom tensorflow.python.training import monitored_session  # pylint: disable=g-bad-import-order\n\nfrom official.utils.logs import metric_hook\nfrom official.utils.testing import mock_lib\n\n\nclass LoggingMetricHookTest(tf.test.TestCase):\n  """"""Tests for LoggingMetricHook.""""""\n\n  def setUp(self):\n    super(LoggingMetricHookTest, self).setUp()\n\n    self._log_dir = tempfile.mkdtemp(dir=self.get_temp_dir())\n    self._logger = mock_lib.MockBenchmarkLogger()\n\n  def tearDown(self):\n    super(LoggingMetricHookTest, self).tearDown()\n    tf.gfile.DeleteRecursively(self.get_temp_dir())\n\n  def test_illegal_args(self):\n    with self.assertRaisesRegexp(ValueError, ""nvalid every_n_iter""):\n      metric_hook.LoggingMetricHook(tensors=[""t""], every_n_iter=0)\n    with self.assertRaisesRegexp(ValueError, ""nvalid every_n_iter""):\n      metric_hook.LoggingMetricHook(tensors=[""t""], every_n_iter=-10)\n    with self.assertRaisesRegexp(ValueError, ""xactly one of""):\n      metric_hook.LoggingMetricHook(\n          tensors=[""t""], every_n_iter=5, every_n_secs=5)\n    with self.assertRaisesRegexp(ValueError, ""xactly one of""):\n      metric_hook.LoggingMetricHook(tensors=[""t""])\n    with self.assertRaisesRegexp(ValueError, ""metric_logger""):\n      metric_hook.LoggingMetricHook(tensors=[""t""], every_n_iter=5)\n\n  def test_print_at_end_only(self):\n    with tf.Graph().as_default(), tf.Session() as sess:\n      tf.train.get_or_create_global_step()\n      t = tf.constant(42.0, name=""foo"")\n      train_op = tf.constant(3)\n      hook = metric_hook.LoggingMetricHook(\n          tensors=[t.name], at_end=True, metric_logger=self._logger)\n      hook.begin()\n      mon_sess = monitored_session._HookedSession(sess, [hook])  # pylint: disable=protected-access\n      sess.run(tf.global_variables_initializer())\n\n      for _ in range(3):\n        mon_sess.run(train_op)\n        self.assertEqual(self._logger.logged_metric, [])\n\n      hook.end(sess)\n      self.assertEqual(len(self._logger.logged_metric), 1)\n      metric = self._logger.logged_metric[0]\n      self.assertRegexpMatches(metric[""name""], ""foo"")\n      self.assertEqual(metric[""value""], 42.0)\n      self.assertEqual(metric[""unit""], None)\n      self.assertEqual(metric[""global_step""], 0)\n\n  def test_global_step_not_found(self):\n    with tf.Graph().as_default():\n      t = tf.constant(42.0, name=""foo"")\n      hook = metric_hook.LoggingMetricHook(\n          tensors=[t.name], at_end=True, metric_logger=self._logger)\n\n      with self.assertRaisesRegexp(\n          RuntimeError, ""should be created to use LoggingMetricHook.""):\n        hook.begin()\n\n  def test_log_tensors(self):\n    with tf.Graph().as_default(), tf.Session() as sess:\n      tf.train.get_or_create_global_step()\n      t1 = tf.constant(42.0, name=""foo"")\n      t2 = tf.constant(43.0, name=""bar"")\n      train_op = tf.constant(3)\n      hook = metric_hook.LoggingMetricHook(\n          tensors=[t1, t2], at_end=True, metric_logger=self._logger)\n      hook.begin()\n      mon_sess = monitored_session._HookedSession(sess, [hook])  # pylint: disable=protected-access\n      sess.run(tf.global_variables_initializer())\n\n      for _ in range(3):\n        mon_sess.run(train_op)\n        self.assertEqual(self._logger.logged_metric, [])\n\n      hook.end(sess)\n      self.assertEqual(len(self._logger.logged_metric), 2)\n      metric1 = self._logger.logged_metric[0]\n      self.assertRegexpMatches(str(metric1[""name""]), ""foo"")\n      self.assertEqual(metric1[""value""], 42.0)\n      self.assertEqual(metric1[""unit""], None)\n      self.assertEqual(metric1[""global_step""], 0)\n\n      metric2 = self._logger.logged_metric[1]\n      self.assertRegexpMatches(str(metric2[""name""]), ""bar"")\n      self.assertEqual(metric2[""value""], 43.0)\n      self.assertEqual(metric2[""unit""], None)\n      self.assertEqual(metric2[""global_step""], 0)\n\n  def _validate_print_every_n_steps(self, sess, at_end):\n    t = tf.constant(42.0, name=""foo"")\n\n    train_op = tf.constant(3)\n    hook = metric_hook.LoggingMetricHook(\n        tensors=[t.name], every_n_iter=10, at_end=at_end,\n        metric_logger=self._logger)\n    hook.begin()\n    mon_sess = monitored_session._HookedSession(sess, [hook])  # pylint: disable=protected-access\n    sess.run(tf.global_variables_initializer())\n    mon_sess.run(train_op)\n    self.assertRegexpMatches(str(self._logger.logged_metric), t.name)\n    for _ in range(3):\n      self._logger.logged_metric = []\n      for _ in range(9):\n        mon_sess.run(train_op)\n        # assertNotRegexpMatches is not supported by python 3.1 and later\n        self.assertEqual(str(self._logger.logged_metric).find(t.name), -1)\n      mon_sess.run(train_op)\n      self.assertRegexpMatches(str(self._logger.logged_metric), t.name)\n\n    # Add additional run to verify proper reset when called multiple times.\n    self._logger.logged_metric = []\n    mon_sess.run(train_op)\n    # assertNotRegexpMatches is not supported by python 3.1 and later\n    self.assertEqual(str(self._logger.logged_metric).find(t.name), -1)\n\n    self._logger.logged_metric = []\n    hook.end(sess)\n    if at_end:\n      self.assertRegexpMatches(str(self._logger.logged_metric), t.name)\n    else:\n      # assertNotRegexpMatches is not supported by python 3.1 and later\n      self.assertEqual(str(self._logger.logged_metric).find(t.name), -1)\n\n  def test_print_every_n_steps(self):\n    with tf.Graph().as_default(), tf.Session() as sess:\n      tf.train.get_or_create_global_step()\n      self._validate_print_every_n_steps(sess, at_end=False)\n      # Verify proper reset.\n      self._validate_print_every_n_steps(sess, at_end=False)\n\n  def test_print_every_n_steps_and_end(self):\n    with tf.Graph().as_default(), tf.Session() as sess:\n      tf.train.get_or_create_global_step()\n      self._validate_print_every_n_steps(sess, at_end=True)\n      # Verify proper reset.\n      self._validate_print_every_n_steps(sess, at_end=True)\n\n  def _validate_print_every_n_secs(self, sess, at_end):\n    t = tf.constant(42.0, name=""foo"")\n    train_op = tf.constant(3)\n\n    hook = metric_hook.LoggingMetricHook(\n        tensors=[t.name], every_n_secs=1.0, at_end=at_end,\n        metric_logger=self._logger)\n    hook.begin()\n    mon_sess = monitored_session._HookedSession(sess, [hook])  # pylint: disable=protected-access\n    sess.run(tf.global_variables_initializer())\n\n    mon_sess.run(train_op)\n    self.assertRegexpMatches(str(self._logger.logged_metric), t.name)\n\n    # assertNotRegexpMatches is not supported by python 3.1 and later\n    self._logger.logged_metric = []\n    mon_sess.run(train_op)\n    self.assertEqual(str(self._logger.logged_metric).find(t.name), -1)\n    time.sleep(1.0)\n\n    self._logger.logged_metric = []\n    mon_sess.run(train_op)\n    self.assertRegexpMatches(str(self._logger.logged_metric), t.name)\n\n    self._logger.logged_metric = []\n    hook.end(sess)\n    if at_end:\n      self.assertRegexpMatches(str(self._logger.logged_metric), t.name)\n    else:\n      # assertNotRegexpMatches is not supported by python 3.1 and later\n      self.assertEqual(str(self._logger.logged_metric).find(t.name), -1)\n\n  def test_print_every_n_secs(self):\n    with tf.Graph().as_default(), tf.Session() as sess:\n      tf.train.get_or_create_global_step()\n      self._validate_print_every_n_secs(sess, at_end=False)\n      # Verify proper reset.\n      self._validate_print_every_n_secs(sess, at_end=False)\n\n  def test_print_every_n_secs_and_end(self):\n    with tf.Graph().as_default(), tf.Session() as sess:\n      tf.train.get_or_create_global_step()\n      self._validate_print_every_n_secs(sess, at_end=True)\n      # Verify proper reset.\n      self._validate_print_every_n_secs(sess, at_end=True)\n\n\nif __name__ == ""__main__"":\n  tf.test.main()\n'"
official/utils/misc/__init__.py,0,b''
official/utils/misc/distribution_utils.py,6,"b'# Copyright 2018 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Helper functions for running models in a distributed setting.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\n\n\ndef get_distribution_strategy(num_gpus, all_reduce_alg=None):\n  """"""Return a DistributionStrategy for running the model.\n\n  Args:\n    num_gpus: Number of GPUs to run this model.\n    all_reduce_alg: Specify which algorithm to use when performing all-reduce.\n      See tf.contrib.distribute.AllReduceCrossTowerOps for available algorithms.\n      If None, DistributionStrategy will choose based on device topology.\n\n  Returns:\n    tf.contrib.distribute.DistibutionStrategy object.\n  """"""\n  if num_gpus == 0:\n    return tf.contrib.distribute.OneDeviceStrategy(""device:CPU:0"")\n  else:\n    if all_reduce_alg:\n      return tf.contrib.distribute.MirroredStrategy(\n          num_gpus=num_gpus,\n          cross_tower_ops=tf.contrib.distribute.AllReduceCrossTowerOps(\n              all_reduce_alg, num_packs=num_gpus))\n    else:\n      return tf.contrib.distribute.MirroredStrategy(num_gpus=num_gpus)\n\n\ndef per_device_batch_size(batch_size, num_gpus):\n  """"""For multi-gpu, batch-size must be a multiple of the number of GPUs.\n\n  Note that this should eventually be handled by DistributionStrategies\n  directly. Multi-GPU support is currently experimental, however,\n  so doing the work here until that feature is in place.\n\n  Args:\n    batch_size: Global batch size to be divided among devices. This should be\n      equal to num_gpus times the single-GPU batch_size for multi-gpu training.\n    num_gpus: How many GPUs are used with DistributionStrategies.\n\n  Returns:\n    Batch size per device.\n\n  Raises:\n    ValueError: if batch_size is not divisible by number of devices\n  """"""\n  if num_gpus <= 1:\n    return batch_size\n\n  remainder = batch_size % num_gpus\n  if remainder:\n    err = (""When running with multiple GPUs, batch size ""\n           ""must be a multiple of the number of available GPUs. Found {} ""\n           ""GPUs with a batch size of {}; try --batch_size={} instead.""\n          ).format(num_gpus, batch_size, batch_size - remainder)\n    raise ValueError(err)\n  return int(batch_size / num_gpus)\n'"
official/utils/misc/distribution_utils_test.py,3,"b'# Copyright 2018 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n"""""" Tests for distribution util functions.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf  # pylint: disable=g-bad-import-order\n\nfrom official.utils.misc import distribution_utils\n\n\nclass GetDistributionStrategyTest(tf.test.TestCase):\n  """"""Tests for get_distribution_strategy.""""""\n  def test_one_device_strategy_cpu(self):\n    ds = distribution_utils.get_distribution_strategy(0)\n    self.assertTrue(ds.is_single_tower)\n    self.assertEquals(ds.num_towers, 1)\n    self.assertEquals(len(ds.worker_devices), 1)\n    self.assertIn(\'CPU\', ds.worker_devices[0])\n\n  def test_one_device_strategy_gpu(self):\n    ds = distribution_utils.get_distribution_strategy(1)\n    self.assertTrue(ds.is_single_tower)\n    self.assertEquals(ds.num_towers, 1)\n    self.assertEquals(len(ds.worker_devices), 1)\n    self.assertIn(\'GPU\', ds.worker_devices[0])\n\n  def test_mirrored_strategy(self):\n    ds = distribution_utils.get_distribution_strategy(5)\n    self.assertFalse(ds.is_single_tower)\n    self.assertEquals(ds.num_towers, 5)\n    self.assertEquals(len(ds.worker_devices), 5)\n    for device in ds.worker_devices:\n      self.assertIn(\'GPU\', device)\n\n\nclass PerDeviceBatchSizeTest(tf.test.TestCase):\n  """"""Tests for per_device_batch_size.""""""\n\n  def test_batch_size(self):\n    self.assertEquals(\n        distribution_utils.per_device_batch_size(147, num_gpus=0), 147)\n    self.assertEquals(\n        distribution_utils.per_device_batch_size(147, num_gpus=1), 147)\n    self.assertEquals(\n        distribution_utils.per_device_batch_size(147, num_gpus=7), 21)\n\n  def test_batch_size_with_remainder(self):\n    with self.assertRaises(ValueError):\n        distribution_utils.per_device_batch_size(147, num_gpus=5)\n\n\nif __name__ == ""__main__"":\n  tf.test.main()\n'"
official/utils/misc/model_helpers.py,9,"b'# Copyright 2018 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Miscellaneous functions that can be called by models.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport numbers\n\nimport tensorflow as tf\nfrom tensorflow.python.util import nest\n\n\ndef past_stop_threshold(stop_threshold, eval_metric):\n  """"""Return a boolean representing whether a model should be stopped.\n\n  Args:\n    stop_threshold: float, the threshold above which a model should stop\n      training.\n    eval_metric: float, the current value of the relevant metric to check.\n\n  Returns:\n    True if training should stop, False otherwise.\n\n  Raises:\n    ValueError: if either stop_threshold or eval_metric is not a number\n  """"""\n  if stop_threshold is None:\n    return False\n\n  if not isinstance(stop_threshold, numbers.Number):\n    raise ValueError(""Threshold for checking stop conditions must be a number."")\n  if not isinstance(eval_metric, numbers.Number):\n    raise ValueError(""Eval metric being checked against stop conditions ""\n                     ""must be a number."")\n\n  if eval_metric >= stop_threshold:\n    tf.logging.info(\n        ""Stop threshold of {} was passed with metric value {}."".format(\n            stop_threshold, eval_metric))\n    return True\n\n  return False\n\n\ndef generate_synthetic_data(\n    input_shape, input_value=0, input_dtype=None, label_shape=None,\n    label_value=0, label_dtype=None):\n  """"""Create a repeating dataset with constant values.\n\n  Args:\n    input_shape: a tf.TensorShape object or nested tf.TensorShapes. The shape of\n      the input data.\n    input_value: Value of each input element.\n    input_dtype: Input dtype. If None, will be inferred by the input value.\n    label_shape: a tf.TensorShape object or nested tf.TensorShapes. The shape of\n      the label data.\n    label_value: Value of each input element.\n    label_dtype: Input dtype. If None, will be inferred by the target value.\n\n  Returns:\n    Dataset of tensors or tuples of tensors (if label_shape is set).\n  """"""\n  # TODO(kathywu): Replace with SyntheticDataset once it is in contrib.\n  element = input_element = nest.map_structure(\n      lambda s: tf.constant(input_value, input_dtype, s), input_shape)\n\n  if label_shape:\n    label_element = nest.map_structure(\n        lambda s: tf.constant(label_value, label_dtype, s), label_shape)\n    element = (input_element, label_element)\n\n  return tf.data.Dataset.from_tensors(element).repeat()\n\n\ndef apply_clean(flags_obj):\n  if flags_obj.clean and tf.gfile.Exists(flags_obj.model_dir):\n    tf.logging.info(""--clean flag set. Removing existing model dir: {}"".format(\n        flags_obj.model_dir))\n    tf.gfile.DeleteRecursively(flags_obj.model_dir)\n'"
official/utils/misc/model_helpers_test.py,13,"b'# Copyright 2018 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n"""""" Tests for Model Helper functions.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf  # pylint: disable=g-bad-import-order\n\nfrom official.utils.misc import model_helpers\n\n\nclass PastStopThresholdTest(tf.test.TestCase):\n  """"""Tests for past_stop_threshold.""""""\n\n  def test_past_stop_threshold(self):\n    """"""Tests for normal operating conditions.""""""\n    self.assertTrue(model_helpers.past_stop_threshold(0.54, 1))\n    self.assertTrue(model_helpers.past_stop_threshold(54, 100))\n    self.assertFalse(model_helpers.past_stop_threshold(0.54, 0.1))\n    self.assertFalse(model_helpers.past_stop_threshold(-0.54, -1.5))\n    self.assertTrue(model_helpers.past_stop_threshold(-0.54, 0))\n    self.assertTrue(model_helpers.past_stop_threshold(0, 0))\n    self.assertTrue(model_helpers.past_stop_threshold(0.54, 0.54))\n\n  def test_past_stop_threshold_none_false(self):\n    """"""Tests that check None returns false.""""""\n    self.assertFalse(model_helpers.past_stop_threshold(None, -1.5))\n    self.assertFalse(model_helpers.past_stop_threshold(None, None))\n    self.assertFalse(model_helpers.past_stop_threshold(None, 1.5))\n    # Zero should be okay, though.\n    self.assertTrue(model_helpers.past_stop_threshold(0, 1.5))\n\n  def test_past_stop_threshold_not_number(self):\n    """"""Tests for error conditions.""""""\n    with self.assertRaises(ValueError):\n      model_helpers.past_stop_threshold(""str"", 1)\n\n    with self.assertRaises(ValueError):\n      model_helpers.past_stop_threshold(""str"", tf.constant(5))\n\n    with self.assertRaises(ValueError):\n      model_helpers.past_stop_threshold(""str"", ""another"")\n\n    with self.assertRaises(ValueError):\n      model_helpers.past_stop_threshold(0, None)\n\n    with self.assertRaises(ValueError):\n      model_helpers.past_stop_threshold(0.7, ""str"")\n\n    with self.assertRaises(ValueError):\n      model_helpers.past_stop_threshold(tf.constant(4), None)\n\n\nclass SyntheticDataTest(tf.test.TestCase):\n  """"""Tests for generate_synthetic_data.""""""\n\n  def test_generate_synethetic_data(self):\n    input_element, label_element = model_helpers.generate_synthetic_data(\n        input_shape=tf.TensorShape([5]),\n        input_value=123,\n        input_dtype=tf.float32,\n        label_shape=tf.TensorShape([]),\n        label_value=456,\n        label_dtype=tf.int32).make_one_shot_iterator().get_next()\n\n    with self.test_session() as sess:\n      for n in range(5):\n        inp, lab = sess.run((input_element, label_element))\n        self.assertAllClose(inp, [123., 123., 123., 123., 123.])\n        self.assertEquals(lab, 456)\n\n  def test_generate_only_input_data(self):\n    d = model_helpers.generate_synthetic_data(\n        input_shape=tf.TensorShape([4]),\n        input_value=43.5,\n        input_dtype=tf.float32)\n\n    element = d.make_one_shot_iterator().get_next()\n    self.assertFalse(isinstance(element, tuple))\n\n    with self.test_session() as sess:\n      inp = sess.run(element)\n      self.assertAllClose(inp, [43.5, 43.5, 43.5, 43.5])\n\n  def test_generate_nested_data(self):\n    d = model_helpers.generate_synthetic_data(\n        input_shape={\'a\': tf.TensorShape([2]),\n                     \'b\': {\'c\': tf.TensorShape([3]), \'d\': tf.TensorShape([])}},\n        input_value=1.1)\n\n    element = d.make_one_shot_iterator().get_next()\n    self.assertIn(\'a\', element)\n    self.assertIn(\'b\', element)\n    self.assertEquals(len(element[\'b\']), 2)\n    self.assertIn(\'c\', element[\'b\'])\n    self.assertIn(\'d\', element[\'b\'])\n    self.assertNotIn(\'c\', element)\n\n    with self.test_session() as sess:\n      inp = sess.run(element)\n      self.assertAllClose(inp[\'a\'], [1.1, 1.1])\n      self.assertAllClose(inp[\'b\'][\'c\'], [1.1, 1.1, 1.1])\n      self.assertAllClose(inp[\'b\'][\'d\'], 1.1)\n\n\nif __name__ == ""__main__"":\n  tf.test.main()\n'"
official/utils/testing/__init__.py,0,b''
official/utils/testing/integration.py,0,"b'# Copyright 2018 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Helper code to run complete models from within python.\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport os\nimport shutil\nimport sys\nimport tempfile\n\nfrom absl import flags\n\nfrom official.utils.flags import core as flags_core\n\n\ndef run_synthetic(main, tmp_root, extra_flags=None, synth=True, max_train=1):\n  """"""Performs a minimal run of a model.\n\n    This function is intended to test for syntax errors throughout a model. A\n  very limited run is performed using synthetic data.\n\n  Args:\n    main: The primary function used to exercise a code path. Generally this\n      function is ""<MODULE>.main(argv)"".\n    tmp_root: Root path for the temp directory created by the test class.\n    extra_flags: Additional flags passed by the caller of this function.\n    synth: Use synthetic data.\n    max_train: Maximum number of allowed training steps.\n  """"""\n\n  extra_flags = [] if extra_flags is None else extra_flags\n\n  model_dir = tempfile.mkdtemp(dir=tmp_root)\n\n  args = [sys.argv[0], ""--model_dir"", model_dir, ""--train_epochs"", ""1"",\n          ""--epochs_between_evals"", ""1""] + extra_flags\n\n  if synth:\n    args.append(""--use_synthetic_data"")\n\n  if max_train is not None:\n    args.extend([""--max_train_steps"", str(max_train)])\n\n  try:\n    flags_core.parse_flags(argv=args)\n    main(flags.FLAGS)\n  finally:\n    if os.path.exists(model_dir):\n      shutil.rmtree(model_dir)\n'"
official/utils/testing/mock_lib.py,0,"b'# Copyright 2018 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n""""""Mock objects and related functions for testing.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\n\nclass MockBenchmarkLogger(object):\n  """"""This is a mock logger that can be used in dependent tests.""""""\n\n  def __init__(self):\n    self.logged_metric = []\n\n  def log_metric(self, name, value, unit=None, global_step=None,\n                 extras=None):\n    self.logged_metric.append({\n        ""name"": name,\n        ""value"": float(value),\n        ""unit"": unit,\n        ""global_step"": global_step,\n        ""extras"": extras})\n'"
official/utils/testing/reference_data.py,17,"b'# Copyright 2018 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""TensorFlow testing subclass to automate numerical testing.\n\nReference tests determine when behavior deviates from some ""gold standard,"" and\nare useful for determining when layer definitions have changed without\nperforming full regression testing, which is generally prohibitive. This class\nhandles the symbolic graph comparison as well as loading weights to avoid\nrelying on random number generation, which can change.\n\nThe tests performed by this class are:\n\n1) Compare a generated graph against a reference graph. Differences are not\n   necessarily fatal.\n2) Attempt to load known weights for the graph. If this step succeeds but\n   changes are present in the graph, a warning is issued but does not raise\n   an exception.\n3) Perform a calculation and compare the result to a reference value.\n\nThis class also provides a method to generate reference data.\n\nNote:\n  The test class is responsible for fixing the random seed during graph\n  definition. A convenience method name_to_seed() is provided to make this\n  process easier.\n\nThe test class should also define a .regenerate() class method which (usually)\njust calls the op definition function with test=False for all relevant tests.\n\nA concise example of this class in action is provided in:\n  official/utils/testing/reference_data_test.py\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport argparse\nimport hashlib\nimport json\nimport os\nimport shutil\nimport sys\n\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow.python import pywrap_tensorflow\n\n\nclass BaseTest(tf.test.TestCase):\n  """"""TestCase subclass for performing reference data tests.""""""\n\n  def regenerate(self):\n    """"""Subclasses should override this function to generate a new reference.""""""\n    raise NotImplementedError\n\n  @property\n  def test_name(self):\n    """"""Subclass should define its own name.""""""\n    raise NotImplementedError\n\n  @property\n  def data_root(self):\n    """"""Use the subclass directory rather than the parent directory.\n\n    Returns:\n      The path prefix for reference data.\n    """"""\n    return os.path.join(os.path.split(\n        os.path.abspath(__file__))[0], ""reference_data"", self.test_name)\n\n  ckpt_prefix = ""model.ckpt""\n\n  @staticmethod\n  def name_to_seed(name):\n    """"""Convert a string into a 32 bit integer.\n\n    This function allows test cases to easily generate random fixed seeds by\n    hashing the name of the test. The hash string is in hex rather than base 10\n    which is why there is a 16 in the int call, and the modulo projects the\n    seed from a 128 bit int to 32 bits for readability.\n\n    Args:\n      name: A string containing the name of a test.\n\n    Returns:\n      A pseudo-random 32 bit integer derived from name.\n    """"""\n    seed = hashlib.md5(name.encode(""utf-8"")).hexdigest()\n    return int(seed, 16) % (2**32 - 1)\n\n  @staticmethod\n  def common_tensor_properties(input_array):\n    """"""Convenience function for matrix testing.\n\n    In tests we wish to determine whether a result has changed. However storing\n    an entire n-dimensional array is impractical. A better approach is to\n    calculate several values from that array and test that those derived values\n    are unchanged. The properties themselves are arbitrary and should be chosen\n    to be good proxies for a full equality test.\n\n    Args:\n      input_array: A numpy array from which key values are extracted.\n\n    Returns:\n      A list of values derived from the input_array for equality tests.\n    """"""\n    output = list(input_array.shape)\n    flat_array = input_array.flatten()\n    output.extend([float(i) for i in\n                   [flat_array[0], flat_array[-1], np.sum(flat_array)]])\n    return output\n\n  def default_correctness_function(self, *args):\n    """"""Returns a vector with the concatenation of common properties.\n\n    This function simply calls common_tensor_properties() for every element.\n    It is useful as it allows one to easily construct tests of layers without\n    having to worry about the details of result checking.\n\n    Args:\n      *args: A list of numpy arrays corresponding to tensors which have been\n        evaluated.\n\n    Returns:\n      A list of values containing properties for every element in args.\n    """"""\n    output = []\n    for arg in args:\n      output.extend(self.common_tensor_properties(arg))\n    return output\n\n  def _construct_and_save_reference_files(\n      self, name, graph, ops_to_eval, correctness_function):\n    """"""Save reference data files.\n\n    Constructs a serialized graph_def, layer weights, and computation results.\n    It then saves them to files which are read at test time.\n\n    Args:\n      name: String defining the run. This will be used to define folder names\n        and will be used for random seed construction.\n      graph: The graph in which the test is conducted.\n      ops_to_eval: Ops which the user wishes to be evaluated under a controlled\n        session.\n      correctness_function: This function accepts the evaluated results of\n        ops_to_eval, and returns a list of values. This list must be JSON\n        serializable; in particular it is up to the user to convert numpy\n        dtypes into builtin dtypes.\n    """"""\n    data_dir = os.path.join(self.data_root, name)\n\n    # Make sure there is a clean space for results.\n    if os.path.exists(data_dir):\n      shutil.rmtree(data_dir)\n    os.makedirs(data_dir)\n\n    # Serialize graph for comparison.\n    graph_bytes = graph.as_graph_def().SerializeToString()\n    expected_file = os.path.join(data_dir, ""expected_graph"")\n    with tf.gfile.Open(expected_file, ""wb"") as f:\n      f.write(graph_bytes)\n\n    with graph.as_default():\n      init = tf.global_variables_initializer()\n      saver = tf.train.Saver()\n\n    with self.test_session(graph=graph) as sess:\n      sess.run(init)\n      saver.save(sess=sess, save_path=os.path.join(data_dir, self.ckpt_prefix))\n\n      # These files are not needed for this test.\n      os.remove(os.path.join(data_dir, ""checkpoint""))\n      os.remove(os.path.join(data_dir, self.ckpt_prefix + "".meta""))\n\n      # ops are evaluated even if there is no correctness function to ensure\n      # that they can be evaluated.\n      eval_results = [op.eval() for op in ops_to_eval]\n\n      if correctness_function is not None:\n        results = correctness_function(*eval_results)\n        with tf.gfile.Open(os.path.join(data_dir, ""results.json""), ""w"") as f:\n          json.dump(results, f)\n\n      with tf.gfile.Open(os.path.join(data_dir, ""tf_version.json""), ""w"") as f:\n        json.dump([tf.VERSION, tf.GIT_VERSION], f)\n\n  def _evaluate_test_case(self, name, graph, ops_to_eval, correctness_function):\n    """"""Determine if a graph agrees with the reference data.\n\n    Args:\n      name: String defining the run. This will be used to define folder names\n        and will be used for random seed construction.\n      graph: The graph in which the test is conducted.\n      ops_to_eval: Ops which the user wishes to be evaluated under a controlled\n        session.\n      correctness_function: This function accepts the evaluated results of\n        ops_to_eval, and returns a list of values. This list must be JSON\n        serializable; in particular it is up to the user to convert numpy\n        dtypes into builtin dtypes.\n    """"""\n    data_dir = os.path.join(self.data_root, name)\n\n    # Serialize graph for comparison.\n    graph_bytes = graph.as_graph_def().SerializeToString()\n    expected_file = os.path.join(data_dir, ""expected_graph"")\n    with tf.gfile.Open(expected_file, ""rb"") as f:\n      expected_graph_bytes = f.read()\n      # The serialization is non-deterministic byte-for-byte. Instead there is\n      # a utility which evaluates the semantics of the two graphs to test for\n      # equality. This has the added benefit of providing some information on\n      # what changed.\n      #   Note: The summary only show the first difference detected. It is not\n      #         an exhaustive summary of differences.\n    differences = pywrap_tensorflow.EqualGraphDefWrapper(\n        graph_bytes, expected_graph_bytes).decode(""utf-8"")\n\n    with graph.as_default():\n      init = tf.global_variables_initializer()\n      saver = tf.train.Saver()\n\n    with tf.gfile.Open(os.path.join(data_dir, ""tf_version.json""), ""r"") as f:\n      tf_version_reference, tf_git_version_reference = json.load(f)  # pylint: disable=unpacking-non-sequence\n\n    tf_version_comparison = """"\n    if tf.GIT_VERSION != tf_git_version_reference:\n      tf_version_comparison = (\n          ""Test was built using:     {} (git = {})\\n""\n          ""Local TensorFlow version: {} (git = {})""\n          .format(tf_version_reference, tf_git_version_reference,\n                  tf.VERSION, tf.GIT_VERSION)\n      )\n\n    with self.test_session(graph=graph) as sess:\n      sess.run(init)\n      try:\n        saver.restore(sess=sess, save_path=os.path.join(\n            data_dir, self.ckpt_prefix))\n        if differences:\n          tf.logging.warn(\n              ""The provided graph is different than expected:\\n  {}\\n""\n              ""However the weights were still able to be loaded.\\n{}"".format(\n                  differences, tf_version_comparison)\n          )\n      except:  # pylint: disable=bare-except\n        raise self.failureException(\n            ""Weight load failed. Graph comparison:\\n  {}{}""\n            .format(differences, tf_version_comparison))\n\n      eval_results = [op.eval() for op in ops_to_eval]\n      if correctness_function is not None:\n        results = correctness_function(*eval_results)\n        with tf.gfile.Open(os.path.join(data_dir, ""results.json""), ""r"") as f:\n          expected_results = json.load(f)\n        self.assertAllClose(results, expected_results)\n\n  def _save_or_test_ops(self, name, graph, ops_to_eval=None, test=True,\n                        correctness_function=None):\n    """"""Utility function to automate repeated work of graph checking and saving.\n\n    The philosophy of this function is that the user need only define ops on\n    a graph and specify which results should be validated. The actual work of\n    managing snapshots and calculating results should be automated away.\n\n    Args:\n      name: String defining the run. This will be used to define folder names\n        and will be used for random seed construction.\n      graph: The graph in which the test is conducted.\n      ops_to_eval: Ops which the user wishes to be evaluated under a controlled\n        session.\n      test: Boolean. If True this function will test graph correctness, load\n        weights, and compute numerical values. If False the necessary test data\n        will be generated and saved.\n      correctness_function: This function accepts the evaluated results of\n        ops_to_eval, and returns a list of values. This list must be JSON\n        serializable; in particular it is up to the user to convert numpy\n        dtypes into builtin dtypes.\n    """"""\n\n    ops_to_eval = ops_to_eval or []\n\n    if test:\n      try:\n        self._evaluate_test_case(\n            name=name, graph=graph, ops_to_eval=ops_to_eval,\n            correctness_function=correctness_function\n        )\n      except:\n        tf.logging.error(""Failed unittest {}"".format(name))\n        raise\n    else:\n      self._construct_and_save_reference_files(\n          name=name, graph=graph, ops_to_eval=ops_to_eval,\n          correctness_function=correctness_function\n      )\n\n\nclass ReferenceDataActionParser(argparse.ArgumentParser):\n  """"""Minimal arg parser so that test regeneration can be called from the CLI.""""""\n\n  def __init__(self):\n    super(ReferenceDataActionParser, self).__init__()\n    self.add_argument(\n        ""--regenerate"", ""-regen"",\n        action=""store_true"",\n        help=""Enable this flag to regenerate test data. If not set unit tests""\n             ""will be run.""\n    )\n\n\ndef main(argv, test_class):\n  """"""Simple switch function to allow test regeneration from the CLI.""""""\n  flags = ReferenceDataActionParser().parse_args(argv[1:])\n  if flags.regenerate:\n    if sys.version_info[0] == 2:\n      raise NameError(""\\nPython2 unittest does not support being run as a ""\n                      ""standalone class.\\nAs a result tests must be ""\n                      ""regenerated using Python3.\\n""\n                      ""Tests can be run under 2 or 3."")\n    test_class().regenerate()\n  else:\n    tf.test.main()\n'"
official/utils/testing/reference_data_test.py,13,"b'# Copyright 2018 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""This module tests generic behavior of reference data tests.\n\nThis test is not intended to test every layer of interest, and models should\ntest the layers that affect them. This test is primarily focused on ensuring\nthat reference_data.BaseTest functions as intended. If there is a legitimate\nchange such as a change to TensorFlow which changes graph construction, tests\ncan be regenerated with the following command:\n\n  $ python3 reference_data_test.py -regen\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport sys\nimport unittest\nimport warnings\n\nimport tensorflow as tf  # pylint: disable=g-bad-import-order\nfrom official.utils.testing import reference_data\n\n\nclass GoldenBaseTest(reference_data.BaseTest):\n  """"""Class to ensure that reference data testing runs properly.""""""\n\n  @property\n  def test_name(self):\n    return ""reference_data_test""\n\n  def _uniform_random_ops(self, test=False, wrong_name=False, wrong_shape=False,\n                          bad_seed=False, bad_function=False):\n    """"""Tests number generation and failure modes.\n\n    This test is of a very simple graph: the generation of a 1x1 random tensor.\n    However, it is also used to confirm that the tests are actually checking\n    properly by failing in predefined ways.\n\n    Args:\n      test: Whether or not to run as a test case.\n      wrong_name: Whether to assign the wrong name to the tensor.\n      wrong_shape: Whether to create a tensor with the wrong shape.\n      bad_seed: Whether or not to perturb the random seed.\n      bad_function: Whether to perturb the correctness function.\n    """"""\n    name = ""uniform_random""\n\n    g = tf.Graph()\n    with g.as_default():\n      seed = self.name_to_seed(name)\n      seed = seed + 1 if bad_seed else seed\n      tf.set_random_seed(seed)\n      tensor_name = ""wrong_tensor"" if wrong_name else ""input_tensor""\n      tensor_shape = (1, 2) if wrong_shape else (1, 1)\n      input_tensor = tf.get_variable(\n          tensor_name, dtype=tf.float32,\n          initializer=tf.random_uniform(tensor_shape, maxval=1)\n      )\n\n    def correctness_function(tensor_result):\n      result = float(tensor_result[0, 0])\n      result = result + 0.1 if bad_function else result\n      return [result]\n\n    self._save_or_test_ops(\n        name=name, graph=g, ops_to_eval=[input_tensor], test=test,\n        correctness_function=correctness_function\n    )\n\n  def _dense_ops(self, test=False):\n    name = ""dense""\n\n    g = tf.Graph()\n    with g.as_default():\n      tf.set_random_seed(self.name_to_seed(name))\n      input_tensor = tf.get_variable(\n          ""input_tensor"", dtype=tf.float32,\n          initializer=tf.random_uniform((1, 2), maxval=1)\n      )\n      layer = tf.layers.dense(inputs=input_tensor, units=4)\n      layer = tf.layers.dense(inputs=layer, units=1)\n\n    self._save_or_test_ops(\n        name=name, graph=g, ops_to_eval=[layer], test=test,\n        correctness_function=self.default_correctness_function\n    )\n\n  def test_uniform_random(self):\n    self._uniform_random_ops(test=True)\n\n  def test_tensor_name_error(self):\n    with self.assertRaises(AssertionError):\n      self._uniform_random_ops(test=True, wrong_name=True)\n\n  def test_tensor_shape_error(self):\n    with self.assertRaises(AssertionError):\n      self._uniform_random_ops(test=True, wrong_shape=True)\n\n  @unittest.skipIf(sys.version_info[0] == 2,\n                   ""catch_warning doesn\'t catch tf.logging.warn in py 2."")\n  def test_bad_seed(self):\n    with warnings.catch_warnings(record=True) as warn_catch:\n      self._uniform_random_ops(test=True, bad_seed=True)\n      assert len(warn_catch) == 1, ""Test did not warn of minor graph change.""\n\n  def test_incorrectness_function(self):\n    with self.assertRaises(AssertionError):\n      self._uniform_random_ops(test=True, bad_function=True)\n\n  def test_dense(self):\n    self._dense_ops(test=True)\n\n  def regenerate(self):\n    self._uniform_random_ops(test=False)\n    self._dense_ops(test=False)\n\n\nif __name__ == ""__main__"":\n  reference_data.main(argv=sys.argv, test_class=GoldenBaseTest)\n'"
