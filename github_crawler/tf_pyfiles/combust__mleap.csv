file_path,api_count,code
python/setup.py,0,"b'#!/usr/bin/env python\n\n#\n# Licensed to the Apache Software Foundation (ASF) under one or more\n# contributor license agreements.  See the NOTICE file distributed with\n# this work for additional information regarding copyright ownership.\n# The ASF licenses this file to You under the Apache License, Version 2.0\n# (the ""License""); you may not use this file except in compliance with\n# the License.  You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom __future__ import print_function\nfrom mleap.version import version\nimport sys\nfrom setuptools import setup, find_packages\n\nif sys.version_info < (2, 7):\n    print(""Python versions prior to 2.7 are not supported for pip installed MLeap."",\n          file=sys.stderr)\n    exit(-1)\n\ntry:\n    exec(open(\'mleap/version.py\').read())\nexcept IOError:\n    print(""Failed to load MLeap version file for packaging. You must be in MLeap\'s python directory."",\n          file=sys.stderr)\n    sys.exit(-1)\n\nVERSION = version\n\nnumpy_version = ""1.8.2""\n\nREQUIRED_PACKAGES = [\n      \'numpy >= %s\' % numpy_version,\n      \'six >= 1.10.0\',\n      \'scipy>=0.13.0b1\',\n      \'pandas>=0.18.1\',\n      \'scikit-learn>=0.18.dev0\',\n]\n\nTESTS_REQUIRED_PACKAGES = [\n      \'nose-exclude>=0.5.0\'\n]\n\nsetup(name=\'mleap\',\n      version=VERSION,\n      description=\'MLeap Python API\',\n      author=\'MLeap Developers\',\n      author_email=\'combust@combust.ml\',\n      url=\'https://github.com/combust/mleap/tree/master/python\',\n      packages=find_packages(),\n      zip_safe=False,\n      install_requires=REQUIRED_PACKAGES,\n      tests_require=TESTS_REQUIRED_PACKAGES,\n      classifiers=[\n        \'Development Status :: 5 - Production/Stable\',\n        \'Intended Audience :: Developers\',\n        \'License :: OSI Approved :: Apache Software License\',\n        \'Operating System :: OS Independent\',\n        \'Topic :: Software Development :: Libraries :: Python Modules\',\n        \'Topic :: Communications :: Chat\',\n        \'Topic :: Internet\',\n        \'Programming Language :: Python\',\n        \'Programming Language :: Python :: 2\',\n        \'Programming Language :: Python :: 2.7\',\n        \'Programming Language :: Python :: 3\',\n        \'Programming Language :: Python :: 3.5\',\n    ],\n     )\n'"
python/mleap/__init__.py,0,b'from mleap.version import version\n\n__version__ = version'
python/mleap/version.py,0,"b'#!/usr/bin/env python\n\n#\n# Licensed to the Apache Software Foundation (ASF) under one or more\n# contributor license agreements.  See the NOTICE file distributed with\n# this work for additional information regarding copyright ownership.\n# The ASF licenses this file to You under the Apache License, Version 2.0\n# (the ""License""); you may not use this file except in compliance with\n# the License.  You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nversion = ""0.17.0-SNAPSHOT""'"
python/tests/__init__.py,0,b''
python/tests/gensim_test.py,0,"b'#\n# Licensed to the Apache Software Foundation (ASF) under one or more\n# contributor license agreements.  See the NOTICE file distributed with\n# this work for additional information regarding copyright ownership.\n# The ASF licenses this file to You under the Apache License, Version 2.0\n# (the ""License""); you may not use this file except in compliance with\n# the License.  You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\nimport shutil\nimport json\nimport os\nimport tempfile\nimport unittest\nimport uuid\n\nfrom mleap.gensim.word2vec import Word2Vec\n\n\nclass TransformerTests(unittest.TestCase):\n\n    def setUp(self):\n        self.tmp_dir = tempfile.mkdtemp()\n\n    def tearDown(self):\n        shutil.rmtree(self.tmp_dir)\n\n    def test_word2vec(self):\n\n        sentences4word2vec_ = [\n            [\'call\', \'me\', \'tomorrow\'],\n            [\'give\', \'me\', \'a\', \'call\', \'in\',\' the\', \'after\', \'noon\'],\n            [\'when\', \'can\', \'i\', \'call\'],\n            [\'when\', \'is\', \'the\', \'best\', \'time\', \'to\', \'call\'],\n            [\'call\', \'me\', \'tomorrow\', \'after\', \'noon\'],\n            [\'i\', \'would\', \'like\', \'a\', \'call\', \'tomorrow\'],\n            [\'do\', \'not\', \'call\', \'until\', \'tomorrow\'],\n            [\'best\', \'time\', \'is\', \'tomorrow\', \'after\', \'noon\'],\n            [\'call\', \'tomorrow\', \'after\', \'lunch\'],\n            [\'call\', \'after\', \'lunch\', \'time\'],\n            [\'make\', \'the\', \'call\', \'tomorrow\'],\n            [\'make\', \'the\', \'call\', \'tomorrow\', \'after\', \'noon\'],\n            [\'make\', \'the\' \'call\', \'after\', \'lunch\', \'time\']\n        ]\n\n        size_ = 5\n        window_ = 2\n\n        model_ = Word2Vec(sentences4word2vec_, min_count=2, size=size_, window=window_)\n        model_.mlinit(input_features=[\'input\'], prediction_column = \'sentence_vector\')\n\n        model_.serialize_to_bundle(self.tmp_dir, model_.name)\n\n        res = model_.sent2vec([\'call\', \'me\', \'on\', \'my\', \'cell\', \'phone\'])\n\n\n        with open(\'{}/{}.node/node.json\'.format(self.tmp_dir, model_.name)) as node_json:\n            node = json.load(node_json)\n\n        with open(\'{}/{}.node/model.json\'.format(self.tmp_dir, model_.name)) as model_json:\n            model = json.load(model_json)\n\n\n        self.assertEqual(5, res.size)\n        self.assertEqual(node[\'shape\'][\'inputs\'][0][\'name\'], [\'input\'])\n        self.assertEqual(node[\'shape\'][\'outputs\'][0][\'name\'], \'sentence_vector\')\n        self.assertEqual(model[\'op\'], \'word2vec\')\n'"
python/mleap/bundle/__init__.py,0,b''
python/mleap/bundle/serialize.py,0,"b'import os\nimport shutil\nimport six\nimport json\nimport numpy as np\n\n_type_map = {\n    int: \'long\',\n    float: \'double\',\n    str: \'string\',\n    np.float64: \'double\'\n}\n\n\nclass Vector(object):\n    def __init__(self, values):\n        self.values = values\n\n\nclass MLeapSerializer(object):\n    """"""\n    Base class to serialize transformers and estimators to a bundle.ml file. Main components that get serialized are:\n        - Model: Contains the data needed for the transformer. For example, if the transformer is a linear regression,\n                then we serialize the coefficients and the intercept of the model.\n        - Node: Contains the definition of the input and output data.\n    """"""\n    def __init__(self):\n        pass\n\n    def get_mleap_model(self, transformer, attributes_to_serialize=None):\n        """"""\n        Generates the model.json given a list of attributes, which are a tuple comprised of:\n            - name\n            - value\n        Type is figured out automatically, but we should consider specifying it explicitly.\n        Note: this only supports doubles and tensors that are vectors/lists of doubles.\n        :param transformer:\n        :param attributes_to_serialize: Tuple of (name, value)\n        :return:\n        """"""\n        js = {\n            \'op\': transformer.op\n        }\n\n        # If the transformer doesn\'t have any attributes, return just the op name\n        if attributes_to_serialize is None:\n            return js\n\n        attributes = {}\n\n        for name, value in attributes_to_serialize:\n            if isinstance(value, float):\n                attributes[name] = {\n                    ""double"": value\n                }\n\n            elif isinstance(value, bool) and value in [True, False]:\n                attributes[name] = {\n                    ""boolean"": value\n                }\n\n            elif isinstance(value, int):\n                attributes[name] = {\n                    ""long"": value\n                }\n            elif isinstance(value, Vector):\n                attributes[name] = {\n                    ""type"": ""list"",\n                    ""double"": value.values\n                }\n            elif isinstance(value, list) and (isinstance(value[0], np.float64) or isinstance(value[0], np.float32) or isinstance(value[0], float)):\n                base = type(value[0])\n                attributes[name] = {\n                    _type_map[base]: value,\n                    ""shape"": {\n                        ""dimensions"": [{\n                            ""size"": len(value),\n                            ""name"": """"\n                        }]\n                    },\n                    ""type"": ""tensor""\n                }\n            elif isinstance(value, list) and isinstance(value[0], str):\n                attributes[name] = {\n                    ""type"": ""list"",\n                    ""string"": value\n                }\n\n            elif isinstance(value, np.ndarray):\n                attributes[name] = {\n                    ""double"": list(value.flatten()),\n                    ""shape"": {\n                        ""dimensions"": [{\n                            ""size"": dim,\n                            ""name"": """"\n                        } for dim in value.shape]\n                    },\n                    ""type"": ""tensor""\n                }\n\n            elif isinstance(value, str):\n                attributes[name] = {\n                    \'string\': value\n                }\n\n            elif isinstance(value, dict):\n                shapes = list()\n                for shape in value[\'data_shape\']:\n                    if shape[\'shape\'] == \'scalar\':\n                        shapes.append(({""base"": ""scalar"",\n                                        ""isNullable"": False}))\n                    elif shape[\'shape\'] == \'tensor\':\n                        shapes.append(({\n                            ""base"": ""tensor"",\n                            ""isNullable"": False,\n                            ""tensorShape"": {\n                                ""dimensions"": [{\n                                    ""size"": shape[\'tensor_shape\'][\'dimensions\'][0][\'size\'],\n                                    ""name"": """"\n                                }]\n                            }\n                        }))\n                attributes[name] = {\n                    \'type\': \'list\',\n                    \'data_shape\': shapes\n                }\n\n        js[\'attributes\'] = attributes\n\n        return js\n\n    def get_mleap_node(self, transformer, inputs, outputs):\n        js = {\n            ""name"": transformer.name,\n            ""shape"": {\n                ""inputs"": inputs,\n                ""outputs"": outputs\n            }\n        }\n        return js\n\n    def serialize(self, transformer, path, model_name, attributes, inputs, outputs, node=True, model=True):\n        # If bundle path already exists, delete it and create a clean directory\n        if node:\n            if os.path.exists(""{}/{}.node"".format(path, model_name)):\n                shutil.rmtree(""{}/{}.node"".format(path, model_name))\n\n            model_dir = ""{}/{}.node"".format(path, model_name)\n        else:\n            if os.path.exists(""{}/{}"".format(path, model_name)):\n                shutil.rmtree(""{}/{}"".format(path, model_name))\n\n            model_dir = ""{}/{}"".format(path, model_name)\n\n        os.mkdir(model_dir)\n\n        if model:\n            # Write bundle file\n            with open(""{}/{}"".format(model_dir, \'model.json\'), \'w\') as outfile:\n                json.dump(self.get_mleap_model(transformer, attributes), outfile, indent=3)\n\n        if node:\n            # Write node file\n            with open(""{}/{}"".format(model_dir, \'node.json\'), \'w\') as outfile:\n                json.dump(self.get_mleap_node(transformer, inputs, outputs), outfile, indent=3)\n\n\nclass MLeapDeserializer(object):\n\n    def deserialize_from_bundle(self, transformer, node_path, node_name):\n        """"""\n        :type node_path: str\n        :type node_name: str\n        :type transformer: StandardScaler\n        :param transformer:\n        :param node_path:\n        :param node_name:\n        :return:\n        """"""\n        NotImplementedError()\n\n    @staticmethod\n    def _node_features_format(x):\n        if isinstance(x, six.string_types):\n            return [str(x)]\n        return x\n\n    def deserialize_single_input_output(self, transformer, node_path, attributes_map=None):\n        """"""\n        :attributes_map: Map of attributes names. For example StandardScaler has `mean_` but is serialized as `mean`\n        :param transformer: Scikit or Pandas transformer\n        :param node: bundle.ml node json file\n        :param model: bundle.ml model json file\n        :return: Transformer\n        """"""\n        # Load the model file\n        with open(""{}/model.json"".format(node_path)) as json_data:\n            model_j = json.load(json_data)\n\n        # Set Transformer Attributes\n        attributes = model_j[\'attributes\']\n        for attribute in attributes.keys():\n            value_key = [key for key in attributes[attribute].keys()\n                         if key in [\'string\', \'boolean\', \'long\', \'double\', \'data_shape\']][0]\n            if attributes_map is not None and attribute in attributes_map.keys():\n                setattr(transformer, attributes_map[attribute], attributes[attribute][value_key])\n            else:\n                setattr(transformer, attribute, attributes[attribute][value_key])\n\n        transformer.op = model_j[\'op\']\n\n        # Load the node file\n        with open(""{}/node.json"".format(node_path)) as json_data:\n            node_j = json.load(json_data)\n\n        transformer.name = node_j[\'name\']\n        transformer.input_features = self._node_features_format(node_j[\'shape\'][\'inputs\'][0][\'name\'])\n        transformer.output_features = self._node_features_format(node_j[\'shape\'][\'outputs\'][0][\'name\'])\n\n        return transformer\n'"
python/mleap/gensim/__init__.py,0,b''
python/mleap/gensim/word2vec.py,0,"b'#\n# Licensed to the Apache Software Foundation (ASF) under one or more\n# contributor license agreements.  See the NOTICE file distributed with\n# this work for additional information regarding copyright ownership.\n# The ASF licenses this file to You under the Apache License, Version 2.0\n# (the ""License""); you may not use this file except in compliance with\n# the License.  You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\nfrom gensim.models import Word2Vec\nfrom mleap.bundle.serialize import MLeapSerializer\nimport uuid\nimport numpy as np\n\n\ndef serialize_to_bundle(self, path, model_name):\n    serializer = SimpleSparkSerializer()\n    return serializer.serialize_to_bundle(self, path, model_name)\n\n\ndef sent2vec(self, words):\n    serializer = SimpleSparkSerializer()\n    return serializer.sent2vec(words, self)\n\n\ndef mleap_init(self, input_features, prediction_column):\n    self.input_features = input_features\n    self.prediction_column = prediction_column\n    self.name = ""{}_{}"".format(self.op, uuid.uuid4())\n\nsetattr(Word2Vec, \'op\', \'word2vec\')\nsetattr(Word2Vec, \'mlinit\', mleap_init)\nsetattr(Word2Vec, \'serialize_to_bundle\', serialize_to_bundle)\nsetattr(Word2Vec, \'serializable\', True)\nsetattr(Word2Vec, \'sent2vec\', sent2vec)\n\n\nclass SimpleSparkSerializer(MLeapSerializer):\n    def __init__(self):\n        super(SimpleSparkSerializer, self).__init__()\n\n    @staticmethod\n    def set_prediction_column(transformer, prediction_column):\n        transformer.prediction_column = prediction_column\n\n    @staticmethod\n    def set_input_features(transformer, input_features):\n        transformer.input_features = input_features\n\n    def serialize_to_bundle(self, transformer, path, model_name):\n\n        # compile tuples of model attributes to serialize\n        attributes = list()\n        attributes.append((\'words\', transformer.wv.index2word))\n\n        # indices = [np.float64(x) for x in list(range(len(transformer.wv.index2word)))]\n        word_vectors = np.array([float(y) for x in [transformer.wv.word_vec(w) for w in transformer.wv.index2word] for y in x])\n\n        # attributes.append((\'indices\', indices))\n        # Excluding indices because they are 0 - N\n        attributes.append((\'word_vectors\', word_vectors))\n        attributes.append((\'kernel\', \'sqrt\'))\n\n        # define node inputs and outputs\n        inputs = [{\n                  ""name"": transformer.input_features,\n                  ""port"": ""input""\n                  }]\n\n        outputs = [{\n                  ""name"": transformer.prediction_column,\n                  ""port"": ""output""\n                   }]\n\n        self.serialize(transformer, path, model_name, attributes, inputs, outputs)\n\n    def sent2vec(self, words, transformer):\n        """"""\n        Used with sqrt kernel\n        :param words:\n        :param transformer:\n        :return:\n        """"""\n        sent_vec = np.zeros(transformer.vector_size)\n        numw = 0\n        for w in words:\n            try:\n                sent_vec = np.add(sent_vec, transformer.wv[w])\n                numw += 1\n            except:\n                continue\n        return sent_vec / np.sqrt(sent_vec.dot(sent_vec))\n'"
python/mleap/pyspark/__init__.py,0,"b""import sys\n\nimport pyspark.ml\n\nimport mleap.pyspark\nimport mleap.pyspark.feature\nfrom mleap.pyspark.feature.string_map import StringMap\nfrom mleap.pyspark.feature.math_binary import MathBinary\nfrom mleap.pyspark.feature.math_unary import MathUnary\n\nsys.modules['pyspark.ml.mleap'] = mleap\nsys.modules['pyspark.ml.mleap.pyspark'] = sys.modules['mleap.pyspark']\nsys.modules['pyspark.ml.mleap.feature'] = sys.modules['mleap.pyspark.feature']\n\nsys.modules['pyspark.ml'].mleap = mleap\nsys.modules['pyspark.ml'].mleap.feature = sys.modules['mleap.pyspark.feature']\nsys.modules['pyspark.ml'].mleap.feature.StringMap = StringMap\nsys.modules['pyspark.ml'].mleap.feature.MathUnary = MathUnary\nsys.modules['pyspark.ml'].mleap.feature.MathBinary = MathBinary\n"""
python/mleap/pyspark/py2scala.py,0,"b'from pyspark.ml.util import _jvm\n\n\ndef jvm_scala_object(jpkg, obj):\n    """"""\n    This accesses a scala object. For example, when passing:\n        jpkg = _jvm().ml.combust.mleap.core.feature.UnaryOperation\n        obj = Sin$\n    it accesses:\n        ml.combust.mleap.core.feature.UnaryOperation.Sin$.MODULE$\n\n    or the scala case object `UnaryOperation.Sin`\n        (for reference see file ml.combust.mleap.core.feature.MathUnaryModel)\n    """"""\n    return getattr(\n        getattr(jpkg, obj + ""$""),  # JavaClass\n        ""MODULE$"",  # JavaObject\n    )\n\ndef Some(value):\n    """"""\n    Instantiate a scala Some object. Useful when scala code takes in\n    an Option[<value>]\n    """"""\n    return _jvm().scala.Some(value)\n'"
python/mleap/pyspark/spark_support.py,0,"b'#\n# Licensed to the Apache Software Foundation (ASF) under one or more\n# contributor license agreements.  See the NOTICE file distributed with\n# this work for additional information regarding copyright ownership.\n# The ASF licenses this file to You under the Apache License, Version 2.0\n# (the ""License""); you may not use this file except in compliance with\n# the License.  You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\nfrom pyspark.ml.wrapper import JavaTransformer\nfrom pyspark.ml.base import Transformer\nfrom pyspark.ml.util import _jvm\n\n\ndef serializeToBundle(self, path, dataset=None):\n    serializer = SimpleSparkSerializer()\n    serializer.serializeToBundle(self, path, dataset=dataset)\n\n\ndef deserializeFromBundle(path):\n    serializer = SimpleSparkSerializer()\n    return serializer.deserializeFromBundle(path)\n\nsetattr(Transformer, \'serializeToBundle\', serializeToBundle)\nsetattr(Transformer, \'deserializeFromBundle\', staticmethod(deserializeFromBundle))\n\n\nclass SimpleSparkSerializer(object):\n    def __init__(self):\n        super(SimpleSparkSerializer, self).__init__()\n        self._java_obj = _jvm().ml.combust.mleap.spark.SimpleSparkSerializer()\n\n    def serializeToBundle(self, transformer, path, dataset):\n        self._java_obj.serializeToBundle(transformer._to_java(), path, dataset._jdf)\n\n    def deserializeFromBundle(self, path):\n        return JavaTransformer._from_java(self._java_obj.deserializeFromBundle(path))\n'"
python/mleap/sklearn/__init__.py,0,b''
python/mleap/sklearn/base.py,0,"b'#\n# Licensed to the Apache Software Foundation (ASF) under one or more\n# contributor license agreements.  See the NOTICE file distributed with\n# this work for additional information regarding copyright ownership.\n# The ASF licenses this file to You under the Apache License, Version 2.0\n# (the ""License""); you may not use this file except in compliance with\n# the License.  You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\nfrom sklearn.linear_model import LinearRegression\nfrom mleap.bundle.serialize import MLeapSerializer, MLeapDeserializer\nimport uuid\nimport os\nimport numpy as np\n\n\ndef serialize_to_bundle(self, path, model_name):\n    serializer = SimpleSerializer()\n    return serializer.serialize_to_bundle(self, path, model_name)\n\n\ndef deserialize_from_bundle(self, path, node_name):\n    serializer = SimpleSerializer()\n    return serializer.deserialize_from_bundle(self, path, node_name)\n\n\ndef mleap_init(self, input_features, prediction_column):\n    self.input_features = input_features\n    self.prediction_column = prediction_column\n    self.name = ""{}_{}"".format(self.op, uuid.uuid1())\n\n\nsetattr(LinearRegression, \'op\', \'linear_regression\')\nsetattr(LinearRegression, \'mlinit\', mleap_init)\nsetattr(LinearRegression, \'serialize_to_bundle\', serialize_to_bundle)\nsetattr(LinearRegression, \'deserialize_from_bundle\', deserialize_from_bundle)\nsetattr(LinearRegression, \'serializable\', True)\n\n\nclass SimpleSerializer(MLeapSerializer, MLeapDeserializer):\n    def __init__(self):\n        super(SimpleSerializer, self).__init__()\n\n    @staticmethod\n    def set_prediction_column(transformer, prediction_column):\n        transformer.prediction_column = prediction_column\n\n    @staticmethod\n    def set_input_features(transformer, input_features):\n        transformer.input_features = input_features\n\n    def serialize_to_bundle(self, transformer, path, model_name):\n\n        # compile tuples of model attributes to serialize\n        attributes = list()\n        attributes.append((\'intercept\', transformer.intercept_.tolist()[0]))\n        attributes.append((\'coefficients\', transformer.coef_.tolist()[0]))\n\n        # define node inputs and outputs\n        inputs = [{\n                  ""name"": transformer.input_features,\n                  ""port"": ""features""\n                  }]\n\n        outputs = [{\n                  ""name"": transformer.prediction_column,\n                  ""port"": ""prediction""\n                   }]\n\n        self.serialize(transformer, path, model_name, attributes, inputs, outputs)\n\n    def deserialize_from_bundle(self, transformer, node_path, node_name):\n\n        attributes_map = {\n            \'coefficients\': \'coef_\',\n            \'intercept\': \'intercept_\'\n        }\n\n        # Set serialized attributes\n        full_node_path = os.path.join(node_path, node_name)\n        transformer = self.deserialize_single_input_output(transformer, full_node_path, attributes_map)\n\n        # Set Additional Attributes\n        if \'intercept_\' in transformer.__dict__:\n            transformer.fit_intercept = True\n        else:\n            transformer.fit_intercept = False\n\n        transformer.coef_ = np.array([transformer.coef_])\n\n        return transformer\n'"
python/mleap/sklearn/feature_union.py,0,"b'#\n# Licensed to the Apache Software Foundation (ASF) under one or more\n# contributor license agreements.  See the NOTICE file distributed with\n# this work for additional information regarding copyright ownership.\n# The ASF licenses this file to You under the Apache License, Version 2.0\n# (the ""License""); you may not use this file except in compliance with\n# the License.  You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\nfrom sklearn.pipeline import FeatureUnion\nimport os\nimport shutil\nimport uuid\n\n\ndef serialize_to_bundle(self, path, model_name):\n    serializer = SimplekSerializer()\n    serializer.serialize_to_bundle(self, path, model_name)\n\n\ndef deserialize_from_bundle(self, path):\n    serializer = SimplekSerializer()\n    return serializer.deserialize_from_bundle(path)\n\n\ndef mleap_init(self):\n    self.name = ""{}_{}"".format(self.op, uuid.uuid1())\n\nsetattr(FeatureUnion, \'serialize_to_bundle\', serialize_to_bundle)\nsetattr(FeatureUnion, \'deserialize_from_bundle\', deserialize_from_bundle)\nsetattr(FeatureUnion, \'op\', \'feature_union\')\nsetattr(FeatureUnion, \'mlinit\', mleap_init)\nsetattr(FeatureUnion, \'serializable\', True)\n\n\nclass SimplekSerializer(object):\n    def __init__(self):\n        super(SimplekSerializer, self).__init__()\n\n    @staticmethod\n    def serialize_to_bundle(transformer, path, model_name):\n\n        for transformer in [x[1] for x in transformer.transformer_list]:\n\n            if os.path.exists(""{}/{}.node"".format(path, transformer.name)):\n                shutil.rmtree(""{}/{}.node"".format(path, transformer.name))\n\n            model_dir = ""{}/{}.node"".format(path, transformer.name)\n            os.mkdir(model_dir)\n\n            if transformer.op == \'pipeline\':\n                # Write bundle file\n                transformer.serialize_to_bundle(model_dir, transformer.name)\n#\n            if isinstance(transformer, list):\n                pass\n\n    @staticmethod\n    def deserialize_from_bundle(self, path):\n        return NotImplementedError\n'"
python/mleap/sklearn/logistic.py,0,"b'#\n# Licensed to the Apache Software Foundation (ASF) under one or more\n# contributor license agreements.  See the NOTICE file distributed with\n# this work for additional information regarding copyright ownership.\n# The ASF licenses this file to You under the Apache License, Version 2.0\n# (the ""License""); you may not use this file except in compliance with\n# the License.  You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.linear_model import LogisticRegressionCV\nfrom mleap.bundle.serialize import MLeapSerializer, MLeapDeserializer\nimport uuid\nimport os\nimport numpy as np\n\n\ndef mleap_init(self, input_features, prediction_column):\n    self.input_features = input_features\n    self.prediction_column = prediction_column\n    self.name = ""{}_{}"".format(self.op, uuid.uuid1())\n\n\ndef serialize_to_bundle(self, path, model_name):\n    serializer = SimpleSerializer()\n    return serializer.serialize_to_bundle(self, path, model_name)\n\n\ndef deserialize_from_bundle(self, path, node_name):\n    serializer = SimpleSerializer()\n    return serializer.deserialize_from_bundle(self, path, node_name)\n\nsetattr(LogisticRegression, \'op\', \'logistic_regression\')\nsetattr(LogisticRegression, \'mlinit\', mleap_init)\nsetattr(LogisticRegression, \'serialize_to_bundle\', serialize_to_bundle)\nsetattr(LogisticRegression, \'deserialize_from_bundle\', deserialize_from_bundle)\nsetattr(LogisticRegression, \'serializable\', True)\n\nsetattr(LogisticRegressionCV, \'op\', \'logistic_regression\')\nsetattr(LogisticRegressionCV, \'mlinit\', mleap_init)\nsetattr(LogisticRegressionCV, \'serialize_to_bundle\', serialize_to_bundle)\nsetattr(LogisticRegressionCV, \'deserialize_from_bundle\', deserialize_from_bundle)\nsetattr(LogisticRegressionCV, \'serializable\', True)\n\n\nclass SimpleSerializer(MLeapSerializer, MLeapDeserializer):\n    def __init__(self):\n        super(SimpleSerializer, self).__init__()\n\n    @staticmethod\n    def set_prediction_column(transformer, prediction_column):\n        transformer.prediction_column = prediction_column\n\n    @staticmethod\n    def set_input_features(transformer, input_features):\n        transformer.input_features = input_features\n\n    def serialize_to_bundle(self, transformer, path, model_name):\n\n        num_classes = len(transformer.classes_)\n\n        # compile tuples of model attributes to serialize\n        attributes = list()\n        if num_classes > 2:\n            attributes.append((\'coefficient_matrix\', transformer.coef_))\n            attributes.append((\'intercept_vector\', transformer.intercept_))\n        else:\n            attributes.append((\'coefficients\', transformer.coef_.tolist()[0]))\n            attributes.append((\'intercept\', transformer.intercept_.tolist()[0]))\n        attributes.append((\'num_classes\', num_classes))\n\n        # define node inputs and outputs\n        inputs = [{\n                  ""name"": transformer.input_features,\n                  ""port"": ""features""\n                }]\n\n        outputs = [{\n                  ""name"": transformer.prediction_column,\n                  ""port"": ""prediction""\n                }]\n\n        self.serialize(transformer, path, model_name, attributes, inputs, outputs)\n\n    def deserialize_from_bundle(self, transformer, node_path, node_name):\n\n        attributes_map = {\n            \'coefficients\': \'coef_\',\n            \'coefficient_matrix\': \'coef_\',\n            \'intercept\': \'intercept_\',\n            \'intercept_vector\': \'intercept_\',\n        }\n\n        # Set serialized attributes\n        full_node_path = os.path.join(node_path, node_name)\n        transformer = self.deserialize_single_input_output(transformer, full_node_path, attributes_map)\n\n        # Set Additional Attributes\n        if \'intercept_\' in transformer.__dict__:\n            transformer.fit_intercept = True\n        else:\n            transformer.fit_intercept = False\n\n        if transformer.num_classes > 2:\n            transformer.coef_ = np.reshape(transformer.coef_, (transformer.num_classes, -1))\n        else:\n            transformer.coef_ = np.reshape(transformer.coef_, (1, -1))\n\n        transformer.classes_ = np.array(range(transformer.num_classes))\n\n        return transformer\n'"
python/mleap/sklearn/pipeline.py,0,"b'#\n# Licensed to the Apache Software Foundation (ASF) under one or more\n# contributor license agreements.  See the NOTICE file distributed with\n# this work for additional information regarding copyright ownership.\n# The ASF licenses this file to You under the Apache License, Version 2.0\n# (the ""License""); you may not use this file except in compliance with\n# the License.  You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\nfrom sklearn.pipeline import Pipeline\nfrom mleap import __version__\nimport os\nimport json\nimport shutil\nimport uuid\nimport zipfile\nimport datetime\n\ndef serialize_to_bundle(self, path, model_name, init=False):\n    serializer = SimpleSerializer()\n    serializer.serialize_to_bundle(self, path, model_name, init)\n\n\ndef deserialize_from_bundle(self, path):\n    serializer = SimpleSerializer()\n    return serializer.deserialize_from_bundle(path)\n\n\ndef mleap_init(self):\n    self.name = ""{}_{}"".format(self.op, uuid.uuid1())\n\n\nsetattr(Pipeline, \'serialize_to_bundle\', serialize_to_bundle)\nsetattr(Pipeline, \'deserialize_from_bundle\', deserialize_from_bundle)\nsetattr(Pipeline, \'op\', \'pipeline\')\nsetattr(Pipeline, \'mlinit\', mleap_init)\nsetattr(Pipeline, \'serializable\', True)\n\n\nclass SimpleSerializer(object):\n    def __init__(self):\n        super(SimpleSerializer, self).__init__()\n\n    def serialize_to_bundle(self, transformer, path, model_name, init=False):\n\n        model_dir = path\n        if init:\n            # If bundle path already exists, delte it and create a clean directory\n            if os.path.exists(""{}/{}"".format(path, model_name)):\n                shutil.rmtree(""{}/{}"".format(path, model_name))\n\n            # make pipeline directory\n            model_dir = ""{}/{}"".format(path, model_name)\n            os.mkdir(model_dir)\n\n            # make pipeline root directory\n            root_dir = ""{}/root"".format(model_dir)\n            os.mkdir(root_dir)\n\n            # Write Pipeline Bundle file\n            with open(""{}/{}"".format(model_dir, \'bundle.json\'), \'w\') as outfile:\n                json.dump(self.get_bundle(transformer), outfile, indent=3)\n\n            model_dir = root_dir\n\n            # Write the model and node files\n            with open(""{}/{}"".format(model_dir, \'model.json\'), \'w\') as outfile:\n                json.dump(self.get_model(transformer), outfile, indent=3)\n\n            with open(""{}/{}"".format(model_dir, \'node.json\'), \'w\') as outfile:\n                json.dump(self.get_node(transformer), outfile, indent=3)\n\n        else:\n            # Write model file\n            with open(""{}/{}"".format(model_dir, \'model.json\'), \'w\') as outfile:\n                json.dump(self.get_model(transformer), outfile, indent=3)\n\n            # Write node file\n            with open(""{}/{}"".format(model_dir, \'node.json\'), \'w\') as outfile:\n                json.dump(self.get_node(transformer), outfile, indent=3)\n\n        for step in [x[1] for x in transformer.steps if hasattr(x[1], \'serialize_to_bundle\')]:\n            name = step.name\n\n            if step.op == \'pipeline\':\n                # Create the node directory\n                bundle_dir = ""{}/{}.node"".format(model_dir, name)\n                os.mkdir(bundle_dir)\n\n                # Write model file\n                with open(""{}/{}"".format(bundle_dir, \'model.json\'), \'w\') as outfile:\n                    json.dump(self.get_model(step), outfile, indent=3)\n\n                # Write node file\n                with open(""{}/{}"".format(bundle_dir, \'node.json\'), \'w\') as outfile:\n                    json.dump(self.get_node(step), outfile, indent=3)\n\n                for step_i in [x[1] for x in step.steps]:\n                    step_i.serialize_to_bundle(bundle_dir, step_i.name)\n\n            elif step.op == \'feature_union\':\n                step.serialize_to_bundle(model_dir, step.name)\n            else:\n                step.serialize_to_bundle(model_dir, step.name)\n\n            if isinstance(step, list):\n                pass\n\n        if init:\n            zip_pipeline(path, model_name)\n\n    def deserialize_from_bundle(self, path):\n        return NotImplementedError\n\n    @staticmethod\n    def get_bundle(transformer):\n        js = {\n          ""name"": transformer.name,\n          ""format"": ""json"",\n          ""version"": __version__,\n          ""timestamp"": datetime.datetime.now().isoformat(),\n          ""uid"": ""{}"".format(uuid.uuid4())\n        }\n        return js\n\n    @staticmethod\n    def get_node(transformer):\n        js = {\n          ""name"": transformer.name,\n          ""shape"": {\n            ""inputs"": [],\n            ""outputs"": []\n          }\n        }\n        return js\n\n    def get_model(self, transformer):\n        js = {\n          ""op"": transformer.op,\n            ""attributes"": {\n                ""nodes"": {\n                    ""type"": ""list"",\n                    ""string"": self._extract_nodes(transformer.steps)\n                }\n            }\n        }\n        return js\n\n    @staticmethod\n    def _extract_nodes(steps):\n        pipeline_steps = []\n        for name, step in steps:\n            if step.op == \'feature_union\':\n                union_steps = [x[1].name for x in step.transformer_list if hasattr(x[1], \'serialize_to_bundle\') and x[1].serializable]\n                pipeline_steps += union_steps\n            elif hasattr(step, \'serialize_to_bundle\') and step.serializable:\n                pipeline_steps.append(step.name)\n        return pipeline_steps\n\n\ndef zip_pipeline(path, name):\n    zip_file = zipfile.ZipFile(""{}/{}.zip"".format(path, name), \'w\', zipfile.ZIP_DEFLATED)\n    abs_src = os.path.abspath(""{}/{}"".format(path, name))\n    for root, dirs, files in os.walk(""{}/{}"".format(path, name)):\n        for file in files:\n            absname = os.path.abspath(os.path.join(root, file))\n            arcname = absname[len(abs_src) + 1:]\n\n            zip_file.write(os.path.join(root, file), arcname)\n    zip_file.close()'"
python/mleap/sklearn/svm.py,0,"b'#\n# Licensed to the Apache Software Foundation (ASF) under one or more\n# contributor license agreements.  See the NOTICE file distributed with\n# this work for additional information regarding copyright ownership.\n# The ASF licenses this file to You under the Apache License, Version 2.0\n# (the ""License""); you may not use this file except in compliance with\n# the License.  You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\nfrom sklearn.svm import SVC\nfrom sklearn.svm import LinearSVC\nfrom mleap.bundle.serialize import MLeapSerializer, MLeapDeserializer\nimport uuid\nimport numpy as np\nimport os\n\n\ndef mleap_init(self, input_features, prediction_column):\n    self.input_features = input_features\n    self.prediction_column = prediction_column\n    self.name = ""{}_{}"".format(self.op, uuid.uuid1())\n\n\ndef serialize_to_bundle(self, path, model_name):\n    serializer = SimpleSerializer()\n    return serializer.serialize_to_bundle(self, path, model_name)\n\n\ndef deserialize_from_bundle(self, path, node_name):\n    serializer = SimpleSerializer()\n    return serializer.deserialize_from_bundle(self, path, node_name)\n\nsetattr(SVC, \'op\', \'svm\')\nsetattr(SVC, \'mlinit\', mleap_init)\nsetattr(SVC, \'serialize_to_bundle\', serialize_to_bundle)\nsetattr(SVC, \'deserialize_from_bundle\', deserialize_from_bundle)\nsetattr(SVC, \'serializable\', True)\n\nsetattr(LinearSVC, \'op\', \'svm\')\nsetattr(LinearSVC, \'mlinit\', mleap_init)\nsetattr(LinearSVC, \'serialize_to_bundle\', serialize_to_bundle)\nsetattr(LinearSVC, \'deserialize_from_bundle\', deserialize_from_bundle)\nsetattr(LinearSVC, \'serializable\', True)\n\n\nclass SimpleSerializer(MLeapSerializer, MLeapDeserializer):\n    def __init__(self):\n        super(SimpleSerializer, self).__init__()\n\n    @staticmethod\n    def set_prediction_column(transformer, prediction_column):\n        transformer.prediction_column = prediction_column\n\n    @staticmethod\n    def set_input_features(transformer, input_features):\n        transformer.input_features = input_features\n\n    def serialize_to_bundle(self, transformer, path, model_name):\n\n        # compile tuples of model attributes to serialize\n        attributes = list()\n        attributes.append((\'intercept\', transformer.intercept_.tolist()[0]))\n        attributes.append((\'coefficients\', transformer.coef_.tolist()[0]))\n        attributes.append((\'num_classes\', len(transformer.classes_))) # TODO: get number of classes from the transformer\n\n        # define node inputs and outputs\n        inputs = [{\n                  ""name"": transformer.input_features,\n                  ""port"": ""features""\n                  }]\n\n        outputs = [{\n                  ""name"": transformer.prediction_column,\n                  ""port"": ""prediction""\n                }]\n\n        self.serialize(transformer, path, model_name, attributes, inputs, outputs)\n\n    def deserialize_from_bundle(self, transformer, node_path, node_name):\n\n        attributes_map = {\n            \'coefficients\': \'coef_\',\n            \'intercept\': \'intercept_\'\n        }\n\n        # Set serialized attributes\n        full_node_path = os.path.join(node_path, node_name)\n        transformer = self.deserialize_single_input_output(transformer, full_node_path, attributes_map)\n\n        # Set Additional Attributes\n        if \'intercept_\' in transformer.__dict__:\n            transformer.fit_intercept = True\n        else:\n            transformer.fit_intercept = False\n\n        transformer.coef_ = np.array([transformer.coef_])\n        transformer.classes_ = np.array([0, 1])\n\n        return transformer\n'"
python/mleap/tensorflow/__init__.py,0,b''
python/mleap/tensorflow/test.py,7,"b'class NodeLookup(object):\n    """"""Converts integer node ID\'s to human readable labels.""""""\n\n    def __init__(self,\n                 label_lookup_path=None,\n                 uid_lookup_path=None):\n        if not label_lookup_path:\n            label_lookup_path = os.path.join(\n                    FLAGS.model_dir, \'imagenet_2012_challenge_label_map_proto.pbtxt\')\n        if not uid_lookup_path:\n            uid_lookup_path = os.path.join(\n                    FLAGS.model_dir, \'imagenet_synset_to_human_label_map.txt\')\n        self.node_lookup = self.load(label_lookup_path, uid_lookup_path)\n\n    def load(self, label_lookup_path, uid_lookup_path):\n        """"""Loads a human readable English name for each softmax node.\n        Args:\n          label_lookup_path: string UID to integer node ID.\n          uid_lookup_path: string UID to human-readable string.\n        Returns:\n          dict from integer node ID to human-readable string.\n        """"""\n        if not tf.gfile.Exists(uid_lookup_path):\n            tf.logging.fatal(\'File does not exist %s\', uid_lookup_path)\n        if not tf.gfile.Exists(label_lookup_path):\n            tf.logging.fatal(\'File does not exist %s\', label_lookup_path)\n\n        # Loads mapping from string UID to human-readable string\n        proto_as_ascii_lines = tf.gfile.GFile(uid_lookup_path).readlines()\n        uid_to_human = {}\n        p = re.compile(r\'[n\\d]*[ \\S,]*\')\n        for line in proto_as_ascii_lines:\n            parsed_items = p.findall(line)\n            uid = parsed_items[0]\n            human_string = parsed_items[2]\n            uid_to_human[uid] = human_string\n\n        # Loads mapping from string UID to integer node ID.\n        node_id_to_uid = {}\n        proto_as_ascii = tf.gfile.GFile(label_lookup_path).readlines()\n        for line in proto_as_ascii:\n            if line.startswith(\'  target_class:\'):\n                target_class = int(line.split(\': \')[1])\n            if line.startswith(\'  target_class_string:\'):\n                target_class_string = line.split(\': \')[1]\n                node_id_to_uid[target_class] = target_class_string[1:-2]\n\n        # Loads the final mapping of integer node ID to human-readable string\n        node_id_to_name = {}\n        for key, val in node_id_to_uid.items():\n            if val not in uid_to_human:\n                tf.logging.fatal(\'Failed to locate: %s\', val)\n            name = uid_to_human[val]\n            node_id_to_name[key] = name\n\n        return node_id_to_name\n\n    def id_to_string(self, node_id):\n        if node_id not in self.node_lookup:\n            return \'\'\n        return self.node_lookup[node_id]\n'"
python/tests/pyspark/__init__.py,0,"b'import logging\n\n# prevent py4j from flooding the output with warnings upon failing tests\nlogging.getLogger(""py4j"").setLevel(logging.ERROR)\n'"
python/tests/sklearn/__init__.py,0,b''
python/tests/sklearn/base_tests.py,8,"b'\n#\n# Licensed to the Apache Software Foundation (ASF) under one or more\n# contributor license agreements.  See the NOTICE file distributed with\n# this work for additional information regarding copyright ownership.\n# The ASF licenses this file to You under the Apache License, Version 2.0\n# (the ""License""); you may not use this file except in compliance with\n# the License.  You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\nimport unittest\nimport pandas as pd\nimport numpy as np\nimport os\nimport shutil\nimport json\nimport uuid\nimport tempfile\n\nfrom sklearn.linear_model import LinearRegression, LogisticRegression, LogisticRegressionCV\nfrom sklearn.preprocessing import Binarizer\nfrom mleap.sklearn.base import LinearRegression\nfrom mleap.sklearn.logistic import LogisticRegression, LogisticRegressionCV\nfrom mleap.sklearn.preprocessing.data import FeatureExtractor, Binarizer\n\n\ndef to_standard_normal_quartile(rand_num):\n    """"""Retrieve the quartile of a data point sampled from the standard normal distribution\n\n    Useful for assigning multi-class labels to random data during tests\n    Such tests should probably use sklearn.preprocessing.KBinsDiscretizer instead\n    But they can\'t since scikit-learn is pinned < 0.20.0\n    https://github.com/combust/mleap/pull/431\n\n    """"""\n    if rand_num < -0.67448:\n        return 0\n    elif rand_num < 0:\n        return 1\n    elif rand_num < 0.67448:\n        return 2\n    else:\n        return 3\n\n\nclass TransformerTests(unittest.TestCase):\n    def setUp(self):\n        self.df = pd.DataFrame(np.random.randn(100, 5), columns=[\'a\', \'b\', \'c\', \'d\', \'e\'])\n        self.tmp_dir = tempfile.mkdtemp()\n\n    def tearDown(self):\n        shutil.rmtree(self.tmp_dir)\n\n    def test_linear_regression_serializer(self):\n\n        linear_regression = LinearRegression(fit_intercept=True, normalize=False)\n        linear_regression.mlinit(input_features=\'a\',\n                                 prediction_column=\'e\')\n\n        linear_regression.fit(self.df[[\'a\']], self.df[[\'e\']])\n\n        linear_regression.serialize_to_bundle(self.tmp_dir, linear_regression.name)\n\n\n        # Test model.json\n        with open(""{}/{}.node/model.json"".format(self.tmp_dir, linear_regression.name)) as json_data:\n            model = json.load(json_data)\n\n        self.assertEqual(model[\'op\'], \'linear_regression\')\n        self.assertTrue(model[\'attributes\'][\'intercept\'][\'double\'] is not None)\n\n    def test_linear_regression_deserializer(self):\n\n        linear_regression = LinearRegression(fit_intercept=True, normalize=False)\n        linear_regression.mlinit(input_features=\'a\',\n                                 prediction_column=\'e\')\n\n        linear_regression.fit(self.df[[\'a\']], self.df[[\'e\']])\n\n        linear_regression.serialize_to_bundle(self.tmp_dir, linear_regression.name)\n\n        # Test model.json\n        with open(""{}/{}.node/model.json"".format(self.tmp_dir, linear_regression.name)) as json_data:\n            model = json.load(json_data)\n\n        # Now deserialize it back\n        node_name = ""{}.node"".format(linear_regression.name)\n        linear_regression_tf = LinearRegression()\n        linear_regression_tf = linear_regression_tf.deserialize_from_bundle(self.tmp_dir, node_name)\n\n        res_a = linear_regression.predict(self.df[[\'a\']])\n        res_b = linear_regression_tf.predict(self.df[[\'a\']])\n\n        self.assertEqual(res_a[0], res_b[0])\n        self.assertEqual(res_a[1], res_b[1])\n        self.assertEqual(res_a[2], res_b[2])\n\n    def test_logistic_regression_serializer(self):\n\n        logistic_regression = LogisticRegression(fit_intercept=True)\n        logistic_regression.mlinit(input_features=\'a\',\n                                 prediction_column=\'e_binary\')\n\n        extract_features = [\'e\']\n        feature_extractor = FeatureExtractor(input_scalars=[\'e\'],\n                                         output_vector=\'extracted_e_output\',\n                                         output_vector_items=[""{}_out"".format(x) for x in extract_features])\n\n        binarizer = Binarizer(threshold=0.0)\n        binarizer.mlinit(prior_tf=feature_extractor,\n                         output_features=\'e_binary\')\n\n        Xres = binarizer.fit_transform(self.df[[\'a\']])\n\n        logistic_regression.fit(self.df[[\'a\']], Xres)\n\n        logistic_regression.serialize_to_bundle(self.tmp_dir, logistic_regression.name)\n\n\n        # Test model.json\n        with open(""{}/{}.node/model.json"".format(self.tmp_dir, logistic_regression.name)) as json_data:\n            model = json.load(json_data)\n\n        self.assertEqual(model[\'op\'], \'logistic_regression\')\n        self.assertTrue(model[\'attributes\'][\'intercept\'][\'double\'] is not None)\n\n    def test_logistic_regression_deserializer(self):\n\n        logistic_regression = LogisticRegression(fit_intercept=True)\n        logistic_regression.mlinit(input_features=\'a\',\n                                   prediction_column=\'e_binary\')\n\n        extract_features = [\'e\']\n        feature_extractor = FeatureExtractor(input_scalars=[\'e\'],\n                                         output_vector=\'extracted_e_output\',\n                                         output_vector_items=[""{}_out"".format(x) for x in extract_features])\n\n        binarizer = Binarizer(threshold=0.0)\n        binarizer.mlinit(prior_tf=feature_extractor,\n                         output_features=\'e_binary\')\n\n        Xres = binarizer.fit_transform(self.df[[\'a\']])\n\n        logistic_regression.fit(self.df[[\'a\']], Xres)\n\n        logistic_regression.serialize_to_bundle(self.tmp_dir, logistic_regression.name)\n\n        # Test model.json\n        with open(""{}/{}.node/model.json"".format(self.tmp_dir, logistic_regression.name)) as json_data:\n            model = json.load(json_data)\n\n        # Now deserialize it back\n        node_name = ""{}.node"".format(logistic_regression.name)\n        logistic_regression_tf = LogisticRegression()\n        logistic_regression_tf = logistic_regression_tf.deserialize_from_bundle(self.tmp_dir, node_name)\n\n        res_a = logistic_regression.predict(self.df[[\'a\']])\n        res_b = logistic_regression_tf.predict(self.df[[\'a\']])\n\n        self.assertEqual(res_a[0], res_b[0])\n        self.assertEqual(res_a[1], res_b[1])\n        self.assertEqual(res_a[2], res_b[2])\n\n    def test_multinomial_logistic_regression_serializer(self):\n\n        logistic_regression = LogisticRegression(fit_intercept=True)\n        logistic_regression.mlinit(\n            input_features=\'a\',\n            prediction_column=\'prediction\'\n        )\n\n        X = self.df[[\'a\']]\n        y = np.array([to_standard_normal_quartile(elem) for elem in X.to_numpy()])\n\n        logistic_regression.fit(X, y)\n        logistic_regression.serialize_to_bundle(self.tmp_dir, logistic_regression.name)\n\n        with open(""{}/{}.node/model.json"".format(self.tmp_dir, logistic_regression.name)) as json_data:\n            model = json.load(json_data)\n\n        self.assertEqual(model[\'op\'], \'logistic_regression\')\n        self.assertEqual(model[\'attributes\'][\'num_classes\'][\'long\'], 4)\n\n        # assert 4x1 coefficient matrix\n        self.assertEqual(len(model[\'attributes\'][\'coefficient_matrix\'][\'double\']), 4)\n        self.assertEqual(len(model[\'attributes\'][\'coefficient_matrix\'][\'shape\'][\'dimensions\']), 2)\n        self.assertEqual(model[\'attributes\'][\'coefficient_matrix\'][\'shape\'][\'dimensions\'][0][\'size\'], 4)\n        self.assertEqual(model[\'attributes\'][\'coefficient_matrix\'][\'shape\'][\'dimensions\'][1][\'size\'], 1)\n\n        # assert 4x0 intercept vector\n        self.assertEqual(len(model[\'attributes\'][\'intercept_vector\'][\'double\']), 4)\n        self.assertEqual(len(model[\'attributes\'][\'intercept_vector\'][\'shape\'][\'dimensions\']), 1)\n        self.assertEqual(model[\'attributes\'][\'intercept_vector\'][\'shape\'][\'dimensions\'][0][\'size\'], 4)\n\n    def test_multinomial_logistic_regression_deserializer(self):\n\n        logistic_regression = LogisticRegression(fit_intercept=True)\n        logistic_regression.mlinit(\n            input_features=\'a\',\n            prediction_column=\'prediction\'\n        )\n\n        X = self.df[[\'a\']]\n        y = np.array([to_standard_normal_quartile(elem) for elem in X.to_numpy()])\n\n        logistic_regression.fit(X, y)\n        logistic_regression.serialize_to_bundle(self.tmp_dir, logistic_regression.name)\n\n        node_name = ""{}.node"".format(logistic_regression.name)\n        logistic_regression_tf = LogisticRegression()\n        logistic_regression_tf = logistic_regression_tf.deserialize_from_bundle(self.tmp_dir, node_name)\n\n        expected = logistic_regression.predict(self.df[[\'a\']])\n        actual = logistic_regression_tf.predict(self.df[[\'a\']])\n\n        np.testing.assert_array_equal(expected, actual)\n\n    def test_logistic_regression_cv_serializer(self):\n\n        logistic_regression = LogisticRegressionCV(fit_intercept=True)\n        logistic_regression.mlinit(input_features=\'a\',\n                                 prediction_column=\'e_binary\')\n\n        extract_features = [\'e\']\n        feature_extractor = FeatureExtractor(input_scalars=[\'e\'],\n                                             output_vector=\'extracted_e_output\',\n                                             output_vector_items=[""{}_out"".format(x) for x in extract_features])\n\n        binarizer = Binarizer(threshold=0.0)\n        binarizer.mlinit(prior_tf=feature_extractor,\n                         output_features=\'e_binary\')\n\n        Xres = binarizer.fit_transform(self.df[[\'a\']])\n\n        logistic_regression.fit(self.df[[\'a\']], Xres)\n\n        logistic_regression.serialize_to_bundle(self.tmp_dir, logistic_regression.name)\n\n\n        # Test model.json\n        with open(""{}/{}.node/model.json"".format(self.tmp_dir, logistic_regression.name)) as json_data:\n            model = json.load(json_data)\n\n        self.assertEqual(model[\'op\'], \'logistic_regression\')\n        self.assertTrue(model[\'attributes\'][\'intercept\'][\'double\'] is not None)\n\n    def test_logistic_regression_cv_deserializer(self):\n\n        logistic_regression = LogisticRegressionCV(fit_intercept=True)\n        logistic_regression.mlinit(input_features=\'a\',\n                                   prediction_column=\'e_binary\')\n\n        extract_features = [\'e\']\n        feature_extractor = FeatureExtractor(input_scalars=[\'e\'],\n                                             output_vector=\'extracted_e_output\',\n                                             output_vector_items=[""{}_out"".format(x) for x in extract_features])\n\n        binarizer = Binarizer(threshold=0.0)\n        binarizer.mlinit(prior_tf=feature_extractor,\n                         output_features=\'e_binary\')\n\n        Xres = binarizer.fit_transform(self.df[[\'a\']])\n\n        logistic_regression.fit(self.df[[\'a\']], Xres)\n\n        logistic_regression.serialize_to_bundle(self.tmp_dir, logistic_regression.name)\n\n        # Test model.json\n        with open(""{}/{}.node/model.json"".format(self.tmp_dir, logistic_regression.name)) as json_data:\n            model = json.load(json_data)\n\n        # Now deserialize it back\n        node_name = ""{}.node"".format(logistic_regression.name)\n        logistic_regression_tf = LogisticRegressionCV()\n        logistic_regression_tf = logistic_regression_tf.deserialize_from_bundle(self.tmp_dir, node_name)\n\n        res_a = logistic_regression.predict(self.df[[\'a\']])\n        res_b = logistic_regression_tf.predict(self.df[[\'a\']])\n\n        self.assertEqual(res_a[0], res_b[0])\n        self.assertEqual(res_a[1], res_b[1])\n        self.assertEqual(res_a[2], res_b[2])\n'"
python/mleap/pyspark/feature/__init__.py,0,b''
python/mleap/pyspark/feature/math_binary.py,0,"b'from enum import Enum\n\nfrom pyspark import keyword_only\nfrom pyspark.ml.param.shared import HasInputCol\nfrom pyspark.ml.param.shared import HasOutputCol\nfrom pyspark.ml.param.shared import Param\nfrom pyspark.ml.param.shared import Params\nfrom pyspark.ml.param.shared import TypeConverters\nfrom pyspark.ml.util import JavaMLReadable\nfrom pyspark.ml.util import JavaMLWritable\nfrom pyspark.ml.util import _jvm\nfrom pyspark.ml.wrapper import JavaTransformer\n\nfrom mleap.pyspark.py2scala import jvm_scala_object\nfrom mleap.pyspark.py2scala import Some\n\n\nclass BinaryOperation(Enum):\n    Add = 1\n    Subtract = 2\n    Multiply = 3\n    Divide = 4\n    Remainder = 5\n    LogN = 6\n    Pow = 7\n\n\nclass MathBinary(JavaTransformer, HasOutputCol, JavaMLReadable, JavaMLWritable):\n\n    inputA = Param(\n        Params._dummy(),\n        ""inputA"",\n        ""input for left side of binary operation"",\n        typeConverter=TypeConverters.toString,\n    )\n\n    inputB = Param(\n        Params._dummy(),\n        ""inputB"",\n        ""input for right side of binary operation"",\n        typeConverter=TypeConverters.toString,\n    )\n\n    @keyword_only\n    def __init__(\n        self,\n        operation=None,\n        inputA=None,\n        inputB=None,\n        outputCol=None,\n    ):\n        """"""\n        Computes the mathematical binary `operation` over\n        the input columns A and B.\n\n        :param operation: BinaryOperation to specify the operation type\n        :param inputA: column name for the left side of operation (string)\n        :param inputB: column name for the right side of operation (string)\n        :param outputCol: output column name (string)\n\n        NOTE: `operation` is not a JavaParam because the underlying MathBinary\n        scala object uses a MathBinaryModel to store the info about the binary\n        operation.\n\n        `operation` has a None default value even though it should *never* be\n        None. A None value is necessary upon deserialization to instantiate a\n        MathBinary without errors. Afterwards, pyspark sets the _java_obj to\n        the deserialized scala object, which encodes the operation.\n        """"""\n        super(MathBinary, self).__init__()\n\n        # if operation=None, it means that pyspark is reloading the model\n        # from disk and calling this method without args. In such case we don\'t\n        # need to set _java_obj here because pyspark will set it after creation\n        #\n        # if operation is not None, we can proceed to instantiate the scala classes\n        if operation:\n            scalaBinaryOperation = jvm_scala_object(\n                _jvm().ml.combust.mleap.core.feature.BinaryOperation,\n                operation.name\n            )\n\n            # IMPORTANT: defaults for missing values are forced to None.\n            # I\'ve found an issue when setting default values for A and B,\n            # Remember to treat your missing values before the MathBinary\n            # (for example, you could use an Imputer)\n            scalaMathBinaryModel = _jvm().ml.combust.mleap.core.feature.MathBinaryModel(\n                scalaBinaryOperation, Some(None), Some(None)\n            )\n\n            self._java_obj = self._new_java_obj(\n                ""org.apache.spark.ml.mleap.feature.MathBinary"",\n                self.uid,\n                scalaMathBinaryModel,\n            )\n\n        self._setDefault()\n        self.setParams(inputA=inputA, inputB=inputB, outputCol=outputCol)\n\n    @keyword_only\n    def setParams(self, inputA=None, inputB=None, outputCol=None):\n        """"""\n        Sets params for this MathBinary.\n        """"""\n        kwargs = self._input_kwargs\n        return self._set(**kwargs)\n\n    def setInputA(self, value):\n        """"""\n        Sets the value of :py:attr:`inputA`.\n        """"""\n        return self._set(inputA=value)\n\n    def setInputB(self, value):\n        """"""\n        Sets the value of :py:attr:`inputB`.\n        """"""\n        return self._set(inputB=value)\n\n    def setOutputCol(self, value):\n        """"""\n        Sets the value of :py:attr:`outputCol`.\n        """"""\n        return self._set(outputCol=value)\n\n'"
python/mleap/pyspark/feature/math_unary.py,0,"b'from enum import Enum\n\nfrom pyspark import keyword_only\nfrom pyspark.ml.param.shared import HasInputCol, HasOutputCol\nfrom pyspark.ml.util import JavaMLReadable\nfrom pyspark.ml.util import JavaMLWritable\nfrom pyspark.ml.util import _jvm\nfrom pyspark.ml.wrapper import JavaTransformer\n\nfrom mleap.pyspark.py2scala import jvm_scala_object\n\n\nclass UnaryOperation(Enum):\n    Sin = 1\n    Cos = 2\n    Tan = 3\n    Log = 4\n    Exp = 5\n    Abs = 6\n    Sqrt = 7\n\n\nclass MathUnary(JavaTransformer, HasInputCol, HasOutputCol, JavaMLReadable, JavaMLWritable):\n\n    @keyword_only\n    def __init__(self, operation=None, inputCol=None, outputCol=None):\n        """"""\n        Computes the mathematical unary `operation` over the input column.\n\n        NOTE: `operation` is not a JavaParam because the underlying \n        MathUnary scala object uses a MathUnaryModel to store the info about\n        the unary operation (sin, tan, etc.), not a JavaParam string.\n\n        `operation` has a None default value even though it should *never* be\n        None. A None value is necessary upon deserialization to instantiate a\n        MathUnary without errors. Afterwards, pyspark sets the _java_obj to\n        the deserialized scala object, which encodes the operation.\n        """"""\n        super(MathUnary, self).__init__()\n\n        # if operation=None, it means that pyspark is reloading the model\n        # from disk and calling this method without args. In such case we don\'t\n        # need to set _java_obj here because pyspark will set it after creation\n        #\n        # if operation is not None, we can proceed to instantiate the scala classes\n        if operation:\n            scalaUnaryOperation = jvm_scala_object(\n                _jvm().ml.combust.mleap.core.feature.UnaryOperation,\n                operation.name\n            )\n\n            scalaMathUnaryModel = _jvm().ml.combust.mleap.core.feature.MathUnaryModel(scalaUnaryOperation)\n\n            self._java_obj = self._new_java_obj(\n                ""org.apache.spark.ml.mleap.feature.MathUnary"",\n                self.uid,\n                scalaMathUnaryModel,\n            )\n\n        self._setDefault()\n        self.setParams(inputCol=inputCol, outputCol=outputCol)\n\n    @keyword_only\n    def setParams(self, inputCol=None, outputCol=None):\n        """"""\n        Sets params for this MathUnary.\n        """"""\n        kwargs = self._input_kwargs\n        return self._set(**kwargs)\n\n    def setInputCol(self, value):\n        """"""\n        Sets the value of :py:attr:`inputCol`.\n        """"""\n        return self._set(inputCol=value)\n\n    def setOutputCol(self, value):\n        """"""\n        Sets the value of :py:attr:`outputCol`.\n        """"""\n        return self._set(outputCol=value)\n'"
python/mleap/pyspark/feature/string_map.py,0,"b'import six\nfrom pyspark import keyword_only\nfrom pyspark.ml.util import JavaMLReadable, JavaMLWritable, _jvm\nfrom pyspark.ml.wrapper import JavaTransformer\nfrom pyspark.ml.param.shared import HasInputCol, HasOutputCol\nfrom pyspark.sql import DataFrame\n\nfrom mleap.pyspark.py2scala import jvm_scala_object\n\n\nclass StringMap(JavaTransformer, HasInputCol, HasOutputCol, JavaMLReadable, JavaMLWritable):\n\n    @keyword_only\n    def __init__(self, labels={}, inputCol=None, outputCol=None, handleInvalid=\'error\', defaultValue=0.0):\n        """"""\n        :param labels: a dict {string: double}\n        :param handleInvalid: how to handle missing labels: \'error\' (throw), or \'keep\' (map to defaultValue)\n        :param defaultValue: value to use if key is not found in labels\n        """"""\n        """"""\n        labels must be a dict {string: double} or a spark DataFrame with columns inputCol & outputCol\n        handleInvalid: \n        """"""\n        super(StringMap, self).__init__()\n\n        def validate_args():\n            """"""\n            validate args early to avoid failing at Py4j with some hard to interpret error message\n            """"""\n            assert handleInvalid in [\'error\', \'keep\'], \'Invalid value for handleInvalid: {}\'.format(handleInvalid)\n            assert isinstance(labels, dict), \'labels must be a dict, got: {}\'.format(type(labels))\n            for (key, value) in labels.items():\n                assert isinstance(key, six.string_types), \\\n                    \'label keys must be a string type, got: {}\'.format(type(key))\n                assert isinstance(value, float), \'label values must be float, got: {}\'.format(type(key))\n\n        validate_args()\n\n        labels_scala_map = _jvm() \\\n            .scala \\\n            .collection \\\n            .JavaConverters \\\n            .mapAsScalaMapConverter(labels) \\\n            .asScala() \\\n            .toMap(_jvm().scala.Predef.conforms())\n\n        handle_invalid_jvm = jvm_scala_object(\n            _jvm().ml.combust.mleap.core.feature.HandleInvalid,\n            handleInvalid.capitalize(),\n        )\n\n        string_map_model = _jvm().ml.combust.mleap.core.feature.StringMapModel(\n            labels_scala_map,\n            handle_invalid_jvm,\n            defaultValue,\n        )\n\n        self._java_obj = self._new_java_obj(\n            ""org.apache.spark.ml.mleap.feature.StringMap"",\n            self.uid,\n            string_map_model\n        )\n\n        self._setDefault()\n        self.setParams(inputCol=inputCol, outputCol=outputCol)\n\n    @keyword_only\n    def setParams(self, inputCol=None, outputCol=None):\n        """"""\n        setParams(self, inputCol=None, outputCol=None)\n        Sets params for this StringMap.\n        """"""\n        kwargs = self._input_kwargs\n        return self._set(**kwargs)\n\n    def setInputCol(self, value):\n        """"""\n        Sets the value of :py:attr:`inputCol`.\n        """"""\n        return self._set(inputCol=value)\n\n    def setOutputCol(self, value):\n        """"""\n        Sets the value of :py:attr:`outputCol`.\n        """"""\n        return self._set(outputCol=value)\n\n    @classmethod\n    def from_dataframe(cls, labels_df, inputCol, outputCol, handleInvalid=\'error\', defaultValue=0.0):\n        """"""\n        :param labels_df: a spark DataFrame whose columns include inputCol:string & outputCol:double.\n        See StringMap() for other params.\n        """"""\n        assert isinstance(labels_df, DataFrame), \'labels must be a DataFrame, got: {}\'.format(type(labels_df))\n        labels_dict = {\n            row[0]: float(row[1])\n            for row in\n            labels_df.select([inputCol, outputCol]).collect()\n        }\n\n        return cls(\n            labels=labels_dict,\n            inputCol=inputCol,\n            outputCol=outputCol,\n            handleInvalid=handleInvalid,\n            defaultValue=defaultValue\n        )\n\n'"
python/mleap/sklearn/decomposition/__init__.py,0,b''
python/mleap/sklearn/decomposition/pca.py,0,"b'#\n# Licensed to the Apache Software Foundation (ASF) under one or more\n# contributor license agreements.  See the NOTICE file distributed with\n# this work for additional information regarding copyright ownership.\n# The ASF licenses this file to You under the Apache License, Version 2.0\n# (the ""License""); you may not use this file except in compliance with\n# the License.  You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\nfrom sklearn.decomposition.pca import PCA\nfrom mleap.bundle.serialize import MLeapSerializer\nimport uuid\n\n\ndef serialize_to_bundle(self, path, model_name):\n    serializer = SimpleSparkSerializer()\n    return serializer.serialize_to_bundle(self, path, model_name)\n\n\ndef mleap_init(self, input_features, prediction_column):\n    self.input_features = input_features\n    self.prediction_column = prediction_column\n    self.name = ""{}_{}"".format(self.op, uuid.uuid4())\n\n\nsetattr(PCA, \'op\', \'pca\')\nsetattr(PCA, \'mlinit\', mleap_init)\nsetattr(PCA, \'serialize_to_bundle\', serialize_to_bundle)\nsetattr(PCA, \'serializable\', True)\n\n\nclass SimpleSparkSerializer(MLeapSerializer):\n    def __init__(self):\n        super(SimpleSparkSerializer, self).__init__()\n\n    @staticmethod\n    def set_prediction_column(transformer, prediction_column):\n        transformer.prediction_column = prediction_column\n\n    @staticmethod\n    def set_input_features(transformer, input_features):\n        transformer.input_features = input_features\n\n    def serialize_to_bundle(self, transformer, path, model_name):\n\n        # compile tuples of model attributes to serialize\n        attributes = list()\n        attributes.append((\'principal_components\', transformer.components_))\n\n        # define node inputs and outputs\n        inputs = [{\n                  ""name"": transformer.input_features,\n                  ""port"": ""input""\n                  }]\n\n        outputs = [{\n                  ""name"": transformer.prediction_column,\n                  ""port"": ""output""\n                   }]\n\n        self.serialize(transformer, path, model_name, attributes, inputs, outputs)\n'"
python/mleap/sklearn/ensemble/__init__.py,0,b''
python/mleap/sklearn/ensemble/forest.py,0,"b'#\n# Licensed to the Apache Software Foundation (ASF) under one or more\n# contributor license agreements.  See the NOTICE file distributed with\n# this work for additional information regarding copyright ownership.\n# The ASF licenses this file to You under the Apache License, Version 2.0\n# (the ""License""); you may not use this file except in compliance with\n# the License.  You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\nfrom sklearn.ensemble.forest import RandomForestRegressor\nfrom sklearn.ensemble.forest import RandomForestClassifier\nfrom mleap.bundle.serialize import MLeapSerializer\nfrom mleap.bundle.serialize import Vector\nimport mleap.sklearn.tree.tree\nimport uuid\n\n\ndef mleap_init(self, input_features, prediction_column, feature_names):\n    self.input_features = input_features\n    self.prediction_column = prediction_column\n    self.feature_names = feature_names\n    self.name = ""{}_{}"".format(self.op, uuid.uuid1())\n\n\ndef serialize_to_bundle(self, path, model_name):\n    serializer = SimpleSerializer()\n    return serializer.serialize_to_bundle(self, path, model_name)\n\n\nsetattr(RandomForestRegressor, \'op\', \'random_forest_regression\')\nsetattr(RandomForestRegressor, \'mlinit\', mleap_init)\nsetattr(RandomForestRegressor, \'serialize_to_bundle\', serialize_to_bundle)\nsetattr(RandomForestRegressor, \'serializable\', True)\n\nsetattr(RandomForestClassifier, \'op\', \'random_forest_classifier\')\nsetattr(RandomForestClassifier, \'mlinit\', mleap_init)\nsetattr(RandomForestClassifier, \'serialize_to_bundle\', serialize_to_bundle)\nsetattr(RandomForestClassifier, \'serializable\', True)\n\n\nclass SimpleSerializer(MLeapSerializer):\n    def __init__(self):\n        super(SimpleSerializer, self).__init__()\n\n    def serialize_to_bundle(self, transformer, path, model):\n        """"""\n        :param transformer: Random Forest Regressor or Classifier\n        :param path: Root path where to serialize the model\n        :param model: Name of the model to be serialized\n        :type transformer: sklearn.ensemble.forest.BaseForest\n        :type path: str\n        :type model: str\n        :return: None\n        """"""\n\n        # Define Node Inputs and Outputs\n        inputs = [{\n                  ""name"": transformer.input_features,\n                  ""port"": ""features""\n                }]\n\n        outputs = list()\n        outputs.append({\n                  ""name"": transformer.prediction_column,\n                  ""port"": ""prediction""\n                })\n\n        outputs.append({\n              ""name"": ""raw_prediction"",\n              ""port"": ""raw_prediction""\n             })\n\n        outputs.append({\n              ""name"": ""probability"",\n              ""port"": ""probability""\n            })\n\n        # compile tuples of model attributes to serialize\n        tree_weights = Vector([1.0 for x in range(0, len(transformer.estimators_))])\n        attributes = list()\n        attributes.append((\'num_features\', transformer.n_features_))\n        attributes.append((\'tree_weights\', tree_weights))\n        attributes.append((\'trees\', [""tree{}"".format(x) for x in range(0, len(transformer.estimators_))]))\n        if isinstance(transformer, RandomForestClassifier):\n            attributes.append((\'num_classes\', transformer.n_classes_)) # TODO: get number of classes from the transformer\n\n        self.serialize(transformer, path, model, attributes, inputs, outputs)\n\n        rf_path = ""{}/{}.node"".format(path, model)\n\n        estimators = transformer.estimators_\n\n        i = 0\n        for estimator in estimators:\n            estimator.mlinit(input_features = transformer.input_features, prediction_column = transformer.prediction_column, feature_names=transformer.feature_names)\n            model_name = ""tree{}"".format(i)\n            estimator.serialize_to_bundle(rf_path, model_name, serialize_node=False)\n\n            i += 1\n'"
python/mleap/sklearn/extensions/__init__.py,0,b''
python/mleap/sklearn/extensions/data.py,0,"b'#\n# Licensed to the Apache Software Foundation (ASF) under one or more\n# contributor license agreements.  See the NOTICE file distributed with\n# this work for additional information regarding copyright ownership.\n# The ASF licenses this file to You under the Apache License, Version 2.0\n# (the ""License""); you may not use this file except in compliance with\n# the License.  You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\nfrom sklearn.preprocessing.data import BaseEstimator, TransformerMixin\nfrom mleap.sklearn.preprocessing.data import MLeapSerializer, FeatureExtractor\nimport uuid\nfrom sklearn.preprocessing import Imputer as SklearnImputer\nfrom mleap.sklearn.preprocessing.data import ImputerSerializer\nimport pandas as pd\n\nclass DefineEstimator(BaseEstimator, TransformerMixin):\n    def __init__(self, transformer):\n        """"""\n        Selects all but the last column of a matrix and passes it as the X variable into the transformer,\n        and the last column as the y variable.\n\n        This transformer is useful when we need to run a transformer or a series of transformers on the y-variable\n        :type transformer: BaseEstimator\n        :param transformer: Estimator (linear regression, random forest, etc)\n        :return: Estimator\n        """"""\n        self.op = \'define_estimator\'\n        self.name = ""{}_{}"".format(self.op, uuid.uuid4())\n        self.transformer = transformer\n        self.serializable=False\n\n    def fit(self, X, y, sample_weight=None):\n        return self.transformer.fit(X[:, :-1], X[:, -1:])\n\n    def transform(self, X):\n        return self.transformer.predict(X[:,:-1])\n\n    def fit_transform(self, X, y=None, **fit_params):\n        return self.transformer.fit_transform(X[:,:-1], X[:,-1:])\n\n    def predict(self, X):\n        return self.transformer.predict(X[:,:-1])\n\n\n""""""\nWrapper around the sklearn Imputer so that it can be used in pipelines. \nDelegates fit() and transform() methods to the sklearn transformer and uses the ImputerSerializer to serialize to bundle.\n\nInstead of putting a FeatureExtractor ahead of the Imputer, we add the equivalent of FeatureExtractor\'s transform() method\nin the fit() and transform() methods.\n\nThis is because the Imputer both in Spark and MLeap operates on a scalar value and if we were to add a feature extractor in\nfront of it, then it would serialize as operating on a tensor and thus, fail at scoring time. \n""""""\nclass Imputer(SklearnImputer):\n\n    def __init__(self, missing_values=""NaN"", strategy=""mean"",\n                 axis=0, verbose=0, copy=True, input_features=None, output_features=None):\n        self.name = ""{}_{}"".format(self.op, uuid.uuid1())\n        self.input_features = input_features\n        self.output_features = output_features\n        self.input_shapes = {\'data_shape\': [{\'shape\': \'scalar\'}]}\n        self.feature_extractor = FeatureExtractor(input_scalars=[input_features],\n                                                  output_vector=\'extracted_\' + output_features,\n                                                  output_vector_items=[output_features])\n        SklearnImputer.__init__(self, missing_values, strategy, axis, verbose, copy)\n\n    def fit(self, X, y=None):\n        super(Imputer, self).fit(self.feature_extractor.transform(X))\n        return self\n\n    def transform(self, X):\n        return pd.DataFrame(super(Imputer, self).transform(self.feature_extractor.transform(X)))\n\n    def serialize_to_bundle(self, path, model_name):\n        ImputerSerializer().serialize_to_bundle(self, path, model_name)'"
python/mleap/sklearn/feature_extraction/__init__.py,0,b''
python/mleap/sklearn/feature_extraction/text.py,0,"b'#\n# Licensed to the Apache Software Foundation (ASF) under one or more\n# contributor license agreements.  See the NOTICE file distributed with\n# this work for additional information regarding copyright ownership.\n# The ASF licenses this file to You under the Apache License, Version 2.0\n# (the ""License""); you may not use this file except in compliance with\n# the License.  You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\nfrom mleap.sklearn.pipeline import SimpleSerializer as PipelineSerializer\nfrom mleap.bundle.serialize import MLeapSerializer\nimport uuid\n\n\nclass ops(object):\n    def __init__(self):\n        self.COUNT_VECTORIZER = \'tokenizer\'\n        self.TF_IDF_VECTORIZER = \'pipeline\'\n\n\nops = ops()\n\n\ndef serialize_to_bundle(self, path, model_name):\n    serializer = SimpleSerializer()\n    return serializer.serialize_to_bundle(self, path, model_name)\n\n\ndef mleap_init(self, input_features, prediction_column):\n    name = ""{}_{}""\n\n    if isinstance(self, TfidfVectorizer):\n        name = ""tfidf_{}_{}""\n\n    self.name = name.format(self.op, uuid.uuid4())\n    self.input_features = input_features\n    self.prediction_column = prediction_column\n\n\nsetattr(TfidfVectorizer, \'op\', ops.TF_IDF_VECTORIZER)\nsetattr(TfidfVectorizer, \'mlinit\', mleap_init)\nsetattr(TfidfVectorizer, \'serialize_to_bundle\', serialize_to_bundle)\nsetattr(TfidfVectorizer, \'serializable\', True)\n\nsetattr(CountVectorizer, \'op\', ops.COUNT_VECTORIZER)\nsetattr(CountVectorizer, \'mlinit\', mleap_init)\nsetattr(CountVectorizer, \'serialize_to_bundle\', serialize_to_bundle)\nsetattr(CountVectorizer, \'serializable\', True)\n\n\nclass SimpleSerializer(object):\n    def __init__(self):\n        super(SimpleSerializer, self).__init__()\n\n    @staticmethod\n    def _choose_serializer(transformer):\n        serializer = None\n\n        if transformer.op == ops.COUNT_VECTORIZER:\n            serializer = CountVectorizerSerializer()\n        elif transformer.op == ops.TF_IDF_VECTORIZER:\n            serializer = TfidfVectorizerSerializer()\n\n        return serializer\n\n    @staticmethod\n    def set_prediction_column(transformer, prediction_column):\n        transformer.prediction_column = prediction_column\n\n    @staticmethod\n    def set_input_features(transformer, input_features):\n        transformer.input_features = input_features\n\n    def serialize_to_bundle(self, transformer, path, model_name):\n        serializer = self._choose_serializer(transformer)\n        serializer.serialize_to_bundle(transformer, path, model_name)\n\n\nclass CountVectorizerSerializer(MLeapSerializer):\n    def __init__(self):\n        super(CountVectorizerSerializer, self).__init__()\n\n    def serialize_to_bundle(self, transformer, path, model_name):\n\n        # compile tuples of model attributes to serialize\n        attributes = None\n\n        # define node inputs and outputs\n        inputs = [{\n                  ""name"": transformer.input_features,\n                  ""port"": ""input""\n                  }]\n\n        outputs = [{\n                  ""name"": transformer.prediction_column,\n                  ""port"": ""output""\n                   }]\n\n        self.serialize(transformer, path, model_name, attributes, inputs, outputs)\n\n\nclass TfidfVectorizerSerializer(MLeapSerializer):\n    pipeline_serializer = PipelineSerializer()\n\n    def __init__(self):\n        super(TfidfVectorizerSerializer, self).__init__()\n\n    @staticmethod\n    def set_prediction_column(transformer, prediction_column):\n        transformer.prediction_column = prediction_column\n\n    @staticmethod\n    def set_input_features(transformer, input_features):\n        transformer.input_features = input_features\n\n    def serialize_to_bundle(self, transformer, path, model_name):\n        num_features = transformer.idf_.shape[0]\n        vocabulary = [None] * num_features\n\n        for term, termidx in transformer.vocabulary_.items():\n            vocabulary[termidx] = str(term)\n\n        tf_attributes = [\n            (\'vocabulary\', vocabulary),\n            (\'binary\', transformer.binary),\n            # no concept of \'min_tf\' in scikit\'s TfidfVectorizer\n            (\'min_tf\', 0)\n        ]\n        tf_inputs = [{\n            ""name"": transformer.input_features,\n            ""port"": ""input""\n        }]\n        tf_outputs = [{\n            ""name"": ""token_counts"",\n            ""port"": ""output""\n        }]\n        tf_step = TfidfStep(transformer, \'count_vectorizer\', tf_attributes, tf_inputs, tf_outputs)\n\n        idf_attributes = [\n            (\'idf\', transformer.idf_.tolist())\n        ]\n        idf_inputs = [{\n            ""name"": tf_outputs[0][\'name\'],\n            ""port"": ""input""\n        }]\n        idf_outputs = [{\n            ""name"": transformer.prediction_column,\n            ""port"": ""output""\n        }]\n        idf_step = TfidfStep(transformer, \'idf\', idf_attributes, idf_inputs, idf_outputs)\n\n        pipeline_step = TfidfStep(transformer, \'pipeline\', None, None, None)\n        pipeline_step.steps = [(\'tf\', tf_step), (\'idf\', idf_step)]\n        self.pipeline_serializer.serialize_to_bundle(pipeline_step, path, model_name, True)\n\n\nclass TfidfStep(MLeapSerializer):\n    def __init__(self, original_transformer, op, attributes, inputs, outputs):\n        super(TfidfStep, self).__init__()\n\n        self.op = op\n        self.attributes = attributes\n        self.inputs = inputs\n        self.outputs = outputs\n        self.name = ""{}_{}"".format(original_transformer.name, op)\n        self.serializable = True\n\n    def serialize_to_bundle(self, path, model_name):\n        self.serialize(self, path, model_name, self.attributes, self.inputs, self.outputs)\n'"
python/mleap/sklearn/preprocessing/__init__.py,0,b''
python/mleap/sklearn/preprocessing/data.py,13,"b'#\n# Licensed to the Apache Software Foundation (ASF) under one or more\n# contributor license agreements.  See the NOTICE file distributed with\n# this work for additional information regarding copyright ownership.\n# The ASF licenses this file to You under the Apache License, Version 2.0\n# (the ""License""); you may not use this file except in compliance with\n# the License.  You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\nimport os\nimport json\nimport shutil\nimport uuid\nimport warnings\nfrom collections import OrderedDict\n\nimport numpy as np\nimport pandas as pd\nfrom sklearn.preprocessing.data import BaseEstimator, TransformerMixin\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler, Imputer, Binarizer, PolynomialFeatures\nfrom sklearn.preprocessing.data import OneHotEncoder\nfrom sklearn.preprocessing.label import LabelEncoder\nfrom mleap.bundle.serialize import MLeapSerializer, MLeapDeserializer, Vector\nfrom sklearn.utils import column_or_1d\nfrom sklearn.utils.validation import check_is_fitted\nfrom sklearn.utils.fixes import np_version\n\n\nclass ops(object):\n    def __init__(self):\n        self.STANDARD_SCALER = \'standard_scaler\'\n        self.MIN_MAX_SCALER = \'min_max_scaler\'\n        self.LABEL_ENCODER = \'string_indexer\'\n        self.ONE_HOT_ENCODER = \'one_hot_encoder\'\n        self.IMPUTER = \'imputer\'\n        self.NDARRAYTODATAFRAME = \'one_dim_array_to_dataframe\'\n        self.TODENSE = \'dense_transformer\'\n        self.BINARIZER = \'sklearn_binarizer\'\n        self.POLYNOMIALEXPANSION = \'sklearn_polynomial_expansion\'\n\nops = ops()\n\n\ndef _check_numpy_unicode_bug(labels):\n    """"""Check that user is not subject to an old numpy bug\n\n    Fixed in master before 1.7.0:\n\n      https://github.com/numpy/numpy/pull/243\n\n    """"""\n    if np_version[:3] < (1, 7, 0) and labels.dtype.kind == \'U\':\n        raise RuntimeError(""NumPy < 1.7.0 does not implement searchsorted""\n                           "" on unicode data correctly. Please upgrade""\n                           "" NumPy to use LabelEncoder with unicode inputs."")\n\n\ndef serialize_to_bundle(self, path, model_name):\n    serializer = SimpleSerializer()\n    return serializer.serialize_to_bundle(self, path, model_name)\n\n\ndef deserialize_from_bundle(self, path, node_name):\n    serializer = SimpleSerializer()\n    return serializer.deserialize_from_bundle(self, path, node_name)\n\n\ndef mleap_init(self, prior_tf, output_features=None):\n\n    self.name = ""{}_{}"".format(self.op, uuid.uuid1())\n\n    if prior_tf.op == \'vector_assembler\':\n        self.input_features = prior_tf.output_vector\n        output_shape = \'tensor\'\n        output_size = len(prior_tf.input_features)\n        output_feature_name = prior_tf.output_vector\n\n    else:\n        self.input_features = prior_tf.output_features\n        output_shape = \'scalar\'\n        output_size = 1\n        output_feature_name = prior_tf.output_features\n\n    class_name = self.__class__.__name__\n\n    if output_features is not None:\n        self.output_features = output_features\n    else:\n        self.output_features = ""{}_{}"".format(output_feature_name, class_name.lower())\n\n    if class_name == \'PolynomialFeatures\':\n        self.input_size = len(prior_tf.input_features)\n\n    elif output_shape == \'tensor\':\n        self.input_shapes = {\'data_shape\': [{\'shape\':\'tensor\',\n                                                 ""tensor_shape"": {""dimensions"": [{""size"": output_size}]}}]}\n    else:\n        self.input_shapes = {\'data_shape\': [{\'shape\': output_shape}]}\n\n\nsetattr(StandardScaler, \'op\', ops.STANDARD_SCALER)\nsetattr(StandardScaler, \'serialize_to_bundle\', serialize_to_bundle)\nsetattr(StandardScaler, \'deserialize_from_bundle\', deserialize_from_bundle)\nsetattr(StandardScaler, \'mlinit\', mleap_init)\nsetattr(StandardScaler, \'serializable\', True)\n\nsetattr(MinMaxScaler, \'op\', ops.MIN_MAX_SCALER)\nsetattr(MinMaxScaler, \'mlinit\', mleap_init)\nsetattr(MinMaxScaler, \'serialize_to_bundle\', serialize_to_bundle)\nsetattr(MinMaxScaler, \'deserialize_from_bundle\', deserialize_from_bundle)\nsetattr(MinMaxScaler, \'serializable\', True)\n\nsetattr(Imputer, \'op\', ops.IMPUTER)\nsetattr(Imputer, \'mlinit\', mleap_init)\nsetattr(Imputer, \'serialize_to_bundle\', serialize_to_bundle)\nsetattr(Imputer, \'serializable\', True)\n\nsetattr(OneHotEncoder, \'op\', ops.ONE_HOT_ENCODER)\nsetattr(OneHotEncoder, \'mlinit\', mleap_init)\nsetattr(OneHotEncoder, \'serialize_to_bundle\', serialize_to_bundle)\nsetattr(OneHotEncoder, \'deserialize_from_bundle\', deserialize_from_bundle)\nsetattr(OneHotEncoder, \'serializable\', True)\n\nsetattr(Binarizer, \'op\', ops.BINARIZER)\nsetattr(Binarizer, \'mlinit\', mleap_init)\nsetattr(Binarizer, \'serialize_to_bundle\', serialize_to_bundle)\nsetattr(Binarizer, \'deserialize_from_bundle\', deserialize_from_bundle)\nsetattr(Binarizer, \'serializable\', True)\n\nsetattr(PolynomialFeatures, \'op\', ops.POLYNOMIALEXPANSION)\nsetattr(PolynomialFeatures, \'mlinit\', mleap_init)\nsetattr(PolynomialFeatures, \'serialize_to_bundle\', serialize_to_bundle)\nsetattr(PolynomialFeatures, \'deserialize_from_bundle\', deserialize_from_bundle)\nsetattr(PolynomialFeatures, \'serializable\', True)\n\n\nclass SimpleSerializer(object):\n    """"""\n    Extends base scikit-learn transformers to include:\n        - set_input_features\n        - set_output_features\n        - serialize_to_bundle\n    methods. The set input/output features are required to maintain parity with the Spark/MLeap execution engine.\n    """"""\n    def __init__(self):\n        super(SimpleSerializer, self).__init__()\n\n    @staticmethod\n    def _choose_serializer(transformer):\n        serializer = None\n        if transformer.op == ops.STANDARD_SCALER:\n            serializer = StandardScalerSerializer()\n        elif transformer.op == ops.MIN_MAX_SCALER:\n            serializer = MinMaxScalerSerializer()\n        elif transformer.op == ops.ONE_HOT_ENCODER:\n            serializer = OneHotEncoderSerializer()\n        elif transformer.op == ops.IMPUTER:\n            serializer = ImputerSerializer()\n        elif transformer.op == ops.BINARIZER:\n            serializer = BinarizerSerializer()\n        elif transformer.op == ops.POLYNOMIALEXPANSION:\n            serializer = PolynomialExpansionSerializer()\n        return serializer\n\n    @staticmethod\n    def set_input_features(transformer, input_features):\n        transformer.input_features = input_features\n\n    @staticmethod\n    def set_output_features(transformer, output_features):\n        transformer.output_features = output_features\n\n    def serialize_to_bundle(self, transformer, path, model_name):\n        serializer = self._choose_serializer(transformer)\n        serializer.serialize_to_bundle(transformer, path, model_name)\n\n    def deserialize_from_bundle(self, transformer, path, node_name):\n        deserializer = self._choose_serializer(transformer)\n        transformer = deserializer.deserialize_from_bundle(transformer, path, node_name)\n        return transformer\n\n\ndef _to_list(x):\n    if isinstance(x, list):\n        return x\n    return list(x)\n\n\nclass FeatureExtractor(BaseEstimator, TransformerMixin, MLeapSerializer):\n    def __init__(self, input_scalars=None, input_vectors=None, output_vector=None, output_vector_items=None):\n        """"""\n        Selects a subset of features from a pandas dataframe that are then passed into a subsequent transformer.\n        MLeap treats this transformer like a VectorAssembler equivalent in spark.\n        >>> data = pd.DataFrame([[\'a\', 0, 1], [\'b\', 1, 2], [\'c\', 3, 4]], columns=[\'col_a\', \'col_b\', \'col_c\'])\n        >>> vector_items = [\'col_b\', \'col_c\']\n        >>> input_shapes = {\'data_shape\': [{\'shape\':\'scalar\'}, {\'shape\':\'scalar\'}]}\n        >>> input_shapes = {\'data_shape\': [{\'shape\':\'tensor\', ""tensor_shape"": {""dimensions"": [{""size"": 23}]}}]}\n        >>> feature_extractor2_tf = FeatureExtractor(vector_items, \'continuous_features\', vector_items, input_shapes)\n        >>> feature_extractor2_tf.fit_transform(data).head(1).values\n        >>> array([[0, 1]])\n        :param input_vectors: List of scalar feature names that are being extracted from a DataFrame\n        :param input_vectors: List of FeatureExtractors that were used to generate the input vectors\n        :param input_features: List of features to extracts from a pandas data frame\n        :param output_vector: Name of the output vector, only used for serialization\n        :param output_vector_items: List of output feature names\n        :param input_shapes: the shape of each input feature, whether it is scalar or a vector\n        if it\'s a vector, then we include size information of the vector\n        :return:\n        """"""\n        self.input_scalars = input_scalars\n        self.input_vectors = input_vectors\n        self.input_features = self.get_input_features(input_scalars, input_vectors)\n        self.output_vector_items = output_vector_items\n        self.output_vector = output_vector\n        self.op = \'vector_assembler\'\n        self.name = ""{}_{}"".format(self.op, uuid.uuid1())\n        self.dtypes = None\n        self.serializable = True\n        self.skip_fit_transform = False\n        self.input_size = None\n        self.input_shapes = None\n\n    def get_input_features(self, input_scalars, input_vectors):\n        if input_scalars is not None:\n            return input_scalars\n        elif input_vectors is not None and isinstance(input_vectors, list) and len(input_vectors)>0:\n            features = list()\n            for x in input_vectors:\n                if \'output_vector\' in x.__dict__:\n                    features.append(x.output_vector)\n                elif \'output_features\' in x.__dict__:\n                    features.append(x.output_features)\n            return features\n        else:\n            raise BaseException(""Unable To Define Input Features"")\n\n    def transform(self, df, **params):\n        if not self.skip_fit_transform:\n            return df[self.input_features]\n        return df\n\n    def assign_input_shapes(self, X):\n        # Figure out the data shape\n        if self.input_scalars is not None and isinstance(X, pd.DataFrame):\n            self.input_shapes = {\'data_shape\': [{\'shape\': \'scalar\'}]*len(self.input_features)}\n\n        elif self.input_vectors is not None:\n            self.input_shapes = {\'data_shape\': []}\n            for vector in self.input_vectors:\n                if vector.op not in [ops.ONE_HOT_ENCODER, ops.POLYNOMIALEXPANSION]:\n                    shape = {\'shape\': \'tensor\', ""tensor_shape"": {""dimensions"": [{""size"": len(vector.input_features)}]}}\n                    self.input_shapes[\'data_shape\'].append(shape)\n                elif vector.op == ops.ONE_HOT_ENCODER:\n                    shape = {\'shape\': \'tensor\', ""tensor_shape"": {""dimensions"": [{""size"": int(vector.n_values_[0] - 1)}]}}\n                    self.input_shapes[\'data_shape\'].append(shape)\n        return self\n\n    def fit(self, X, y=None, **fit_params):\n        self.assign_input_shapes(X)\n        if not self.skip_fit_transform:\n            self.dtypes = X[self.input_features].dtypes.to_dict()\n            if len([x for x in self.dtypes.values() if x.type == np.object_]) != 0:\n                self.serializable = False\n        return self\n\n    def fit_transform(self, X, y=None, **fit_params):\n        if not self.skip_fit_transform:\n            self.fit(X)\n        else:\n            self.assign_input_shapes(X)\n\n        df_subset = self.transform(X)\n        return df_subset\n\n    def serialize_to_bundle(self, path, model_name):\n\n        # compile tuples of model attributes to serialize\n        attributes = list()\n        attributes.append((""input_shapes"", self.input_shapes))\n\n        # define node inputs and outputs\n        inputs = [{\'name\': x, \'port\': \'input{}\'.format(self.input_features.index(x))} for x in self.input_features]\n\n        outputs = [{\n                  ""name"": self.output_vector,\n                  ""port"": ""output""\n                }]\n\n        self.serialize(self, path, model_name, attributes, inputs, outputs)\n\n\nclass LabelEncoder(BaseEstimator, TransformerMixin, MLeapSerializer, MLeapDeserializer):\n    """"""\n    Copied from sklearn, but enables passing X and Y features, which allows this transformer\n    to be used in Pipelines.\n\n    Converts categorical values of a single column into categorical indices. This transformer should be followed by a\n    NDArrayToDataFrame transformer to maintain a data structure required by scikit pipelines.\n\n    NOTE: You can only LabelEncode/String Index one feature at a time!!!\n\n    >>> data = pd.DataFrame([[\'a\', 0], [\'b\', 1], [\'b\', 3], [\'c\', 1]], columns=[\'col_a\', \'col_b\'])\n    >>> # Label Encoder for x1 Label\n    >>> label_encoder_tf = LabelEncoder(input_features = [\'col_a\'] , output_features=\'col_a_label_le\')\n    >>> # Convert output of Label Encoder to Data Frame instead of 1d-array\n    >>> n_dim_array_to_df_tf = NDArrayToDataFrame(\'col_a_label_le\')\n    >>> n_dim_array_to_df_tf.fit_transform(label_encoder_tf.fit_transform(data[\'col_a\']))\n\n    Encode labels with value between 0 and n_classes-1.\n\n    Read more in the :ref:`User Guide <preprocessing_targets>`.\n\n    Attributes\n    ----------\n    classes_ : array of shape (n_class,)\n        Holds the label for each class.\n\n    Examples\n    --------\n    `LabelEncoder` can be used to normalize labels.\n\n    >>> from sklearn import preprocessing\n    >>> le = preprocessing.LabelEncoder()\n    >>> le.fit([1, 2, 2, 6])\n    LabelEncoder()\n    >>> le.classes_\n    array([1, 2, 6])\n    >>> le.transform([1, 1, 2, 6]) #doctest: +ELLIPSIS\n    array([0, 0, 1, 2]...)\n    >>> le.inverse_transform([0, 0, 1, 2])\n    array([1, 1, 2, 6])\n\n    It can also be used to transform non-numerical labels (as long as they are\n    hashable and comparable) to numerical labels.\n\n    >>> le = preprocessing.LabelEncoder()\n    >>> le.fit([""paris"", ""paris"", ""tokyo"", ""amsterdam""])\n    LabelEncoder()\n    >>> list(le.classes_)\n    [\'amsterdam\', \'paris\', \'tokyo\']\n    >>> le.transform([""tokyo"", ""tokyo"", ""paris""]) #doctest: +ELLIPSIS\n    array([2, 2, 1]...)\n    >>> list(le.inverse_transform([2, 2, 1]))\n    [\'tokyo\', \'tokyo\', \'paris\']\n\n    """"""\n    def __init__(self, input_features=None, output_features=None):\n        self.input_features = input_features\n        self.output_features = output_features\n        self.op = \'string_indexer\'\n        self.name = ""{}_{}"".format(self.op, uuid.uuid1())\n        self.serializable = True\n\n\n    def fit(self, X):\n        """"""Fit label encoder\n\n        Parameters\n        ----------\n        y : array-like of shape (n_samples,)\n            Target values.\n\n        Returns\n        -------\n        self : returns an instance of self.\n        """"""\n        X = column_or_1d(X, warn=True)\n        _check_numpy_unicode_bug(X)\n        self.classes_ = np.unique(X)\n        return self\n\n    def fit_transform(self, X, y=None, **fit_params):\n        """"""Fit label encoder and return encoded labels\n\n        Parameters\n        ----------\n        y : array-like of shape [n_samples]\n            Target values.\n\n        Returns\n        -------\n        y : array-like of shape [n_samples]\n        """"""\n        y = column_or_1d(X, warn=True)\n        _check_numpy_unicode_bug(X)\n        self.classes_, X = np.unique(X, return_inverse=True)\n        return X\n\n    def transform(self, y):\n        """"""Transform labels to normalized encoding.\n\n        Parameters\n        ----------\n        y : array-like of shape [n_samples]\n            Target values.\n\n        Returns\n        -------\n        y : array-like of shape [n_samples]\n        """"""\n        check_is_fitted(self, \'classes_\')\n        y = column_or_1d(y, warn=True)\n\n        classes = np.unique(y)\n        _check_numpy_unicode_bug(classes)\n        if len(np.intersect1d(classes, self.classes_)) < len(classes):\n            diff = np.setdiff1d(classes, self.classes_)\n            raise ValueError(""y contains new labels: %s"" % str(diff))\n        return np.searchsorted(self.classes_, y)\n\n    def inverse_transform(self, y):\n        """"""Transform labels back to original encoding.\n\n        Parameters\n        ----------\n        y : numpy array of shape [n_samples]\n            Target values.\n\n        Returns\n        -------\n        y : numpy array of shape [n_samples]\n        """"""\n        check_is_fitted(self, \'classes_\')\n\n        diff = np.setdiff1d(y, np.arange(len(self.classes_)))\n        if diff:\n            raise ValueError(""y contains new labels: %s"" % str(diff))\n        y = np.asarray(y)\n        return self.classes_[y]\n\n    def serialize_to_bundle(self, path, model_name):\n\n        # compile tuples of model attributes to serialize\n        attributes = list()\n        attributes.append((\'labels\', self.classes_.tolist()))\n        attributes.append((\'nullable_input\', False))\n\n        # define node inputs and outputs\n        inputs = [{\n                  ""name"": self.input_features[0],\n                  ""port"": ""input""\n                }]\n\n        outputs = [{\n                  ""name"": self.output_features,\n                  ""port"": ""output""\n                }]\n\n        self.serialize(self, path, model_name, attributes, inputs, outputs)\n\n    def deserialize_from_bundle(self, node_path, node_name):\n\n        attributes_map = {\n            \'labels\': \'classes_\'\n        }\n\n        full_node_path = os.path.join(node_path, node_name)\n        transformer = self.deserialize_single_input_output(self, full_node_path, attributes_map)\n\n        return transformer\n\n\nclass MinMaxScalerSerializer(MLeapSerializer, MLeapDeserializer):\n    """"""\n    Scales features by the range of values using calculated min and max from training data.\n\n    >>> data = pd.DataFrame([[1], [5], [6], [1]], columns=[\'col_a\'])\n    >>> minmax_scaler_tf = MinMaxScaler()\n    >>> minmax_scaler_tf.mlinit(input_features=\'col_a\', output_features=\'scaled_cont_features\')\n\n    >>> minmax_scaler_tf.fit_transform(data)\n    >>> array([[ 0.],\n    >>>      [ 0.8],\n    >>>      [ 1.],\n    >>>      [ 0.]])\n    """"""\n    def __init__(self):\n        super(MinMaxScalerSerializer, self).__init__()\n\n    def serialize_to_bundle(self, transformer, path, model_name):\n\n        # compile tuples of model attributes to serialize\n        attributes = list()\n        attributes.append((\'min\', _to_list(transformer.data_min_)))\n        attributes.append((\'max\', _to_list(transformer.data_max_)))\n\n        # define node inputs and outputs\n        inputs = [{\n                  ""name"": transformer.input_features,\n                  ""port"": ""input""\n                }]\n\n        outputs = [{\n                  ""name"": transformer.output_features,\n                  ""port"": ""output""\n                }]\n\n        self.serialize(transformer, path, model_name, attributes, inputs, outputs)\n\n    def deserialize_from_bundle(self, transformer, node_path, node_name):\n\n        # Default (and only range supported)\n        feature_range = (0, 1)\n\n        attributes_map = {\n            \'min\': \'data_min_\',\n            \'max\': \'data_max_\'\n        }\n\n        full_node_path = os.path.join(node_path, node_name)\n        transformer = self.deserialize_single_input_output(transformer, full_node_path, attributes_map)\n        transformer.data_range_ = np.array(transformer.data_max_) - np.array(transformer.data_min_)\n\n        transformer.scale_ = ((feature_range[1] - feature_range[0]) / transformer.data_range_)\n\n        transformer.min_ = feature_range[0] - transformer.data_min_ * transformer.scale_\n\n        return transformer\n\n\nclass ImputerSerializer(MLeapSerializer):\n    def __init__(self):\n        super(ImputerSerializer, self).__init__()\n        self.serializable = False\n\n    def serialize_to_bundle(self, transformer, path, model_name):\n\n        # compile tuples of model attributes to serialize\n        attributes = list()\n        attributes.append((\'strategy\', transformer.strategy))\n        attributes.append((\'surrogate_value\', transformer.statistics_.tolist()[0]))\n        if transformer.missing_values != ""NaN"":\n            attributes.append((\'missing_value\', transformer.missing_values))\n\n        # define node inputs and outputs\n        inputs = [{\n                  ""name"": transformer.input_features,\n                  ""port"": ""input""\n                }]\n\n        outputs = [{\n                  ""name"": transformer.output_features,\n                  ""port"": ""output""\n                }]\n\n        self.serialize(transformer, path, model_name, attributes, inputs, outputs)\n\n\nclass StandardScalerSerializer(MLeapSerializer, MLeapDeserializer):\n    """"""\n    Standardizes features by removing the mean and scaling to unit variance using mean and standard deviation from\n    training data.\n    >>> data = pd.DataFrame([[1], [5], [6], [1]], columns=[\'col_a\'])\n    >>> standard_scaler_tf = StandardScaler()\n    >>> standard_scaler_tf.mlinit(input_features=\'col_a\', output_features=\'scaled_cont_features\')\n    >>> standard_scaler_tf.fit_transform(data)\n    >>> array([[-0.98787834],\n    >>>         [ 0.76834982],\n    >>>         [ 1.20740686],\n    >>>         [-0.98787834]])\n    """"""\n    def __init__(self):\n        super(StandardScalerSerializer, self).__init__()\n\n    def serialize_to_bundle(self, transformer, path, model_name):\n\n        # compile tuples of model attributes to serialize\n        attributes = list()\n        attributes.append((\'mean\', transformer.mean_.tolist()))\n        attributes.append((\'std\', [np.sqrt(x) for x in transformer.var_]))\n\n        # define node inputs and outputs\n        inputs = [{\n                  ""name"": transformer.input_features,\n                  ""port"": ""input""\n                }]\n\n        outputs = [{\n                  ""name"": transformer.output_features,\n                  ""port"": ""output""\n                }]\n\n        self.serialize(transformer, path, model_name, attributes, inputs, outputs)\n\n    def deserialize_from_bundle(self, transformer, node_path, node_name):\n\n        attributes_map = {\n            \'mean\': \'mean_\',\n            \'std\': \'scale_\'\n        }\n\n        # Set serialized attributes\n        full_node_path = os.path.join(node_path, node_name)\n        transformer = self.deserialize_single_input_output(transformer, full_node_path, attributes_map)\n\n        # Set Additional Attributes\n        if \'mean_\' in transformer.__dict__:\n            transformer.with_mean = True\n        else:\n            transformer.with_mean = False\n\n        if \'scale_\' in transformer.__dict__:\n            transformer.with_std = True\n            transformer.var = np.square(transformer.scale_)\n        else:\n            transformer.with_std = False\n\n        return transformer\n\n\nclass OneHotEncoderSerializer(MLeapSerializer, MLeapDeserializer):\n    """"""\n    A one-hot encoder maps a single column of categorical indices to a\n    column of binary vectors, which can be re-assamble back to a DataFrame using a ToDense transformer.\n    """"""\n    def __init__(self):\n        super(OneHotEncoderSerializer, self).__init__()\n\n    def serialize_to_bundle(self, transformer, path, model_name):\n\n        # compile tuples of model attributes to serialize\n        attributes = list()\n        attributes.append((\'size\', transformer.n_values_.tolist()[0]))\n        # setting drop_last to True, so to that 1HE maintains parity with MLeap/Spark\n        attributes.append((\'drop_last\', True))\n        if transformer.handle_unknown == \'ignore\':\n            attributes.append((\'handle_invalid\', \'keep\'))\n        else:\n            attributes.append((\'handle_invalid\', \'error\'))\n\n        # define node inputs and outputs\n        inputs = [{\n                  ""name"": transformer.input_features,\n                  ""port"": ""input""\n                }]\n\n        outputs = [{\n                  ""name"": transformer.output_features,\n                  ""port"": ""output""\n                }]\n\n        self.serialize(transformer, path, model_name, attributes, inputs, outputs)\n\n    def deserialize_from_bundle(self, transformer, node_path, node_name):\n\n        attributes_map = {\n            \'size\': \'n_values_\'\n        }\n\n        full_node_path = os.path.join(node_path, node_name)\n        transformer = self.deserialize_single_input_output(transformer, full_node_path, attributes_map)\n\n        # Set Sparse = False\n        transformer.sparse = False\n\n        # Set Feature Indices\n        n_values = np.hstack([[0], [transformer.n_values_]])\n        indices = np.cumsum(n_values)\n        transformer.feature_indices_ = indices\n        transformer.active_features_ = range(0, transformer.n_values_)\n\n        return transformer\n\n\nclass BinarizerSerializer(MLeapSerializer, MLeapDeserializer):\n    def __init__(self):\n        super(BinarizerSerializer, self).__init__()\n\n\n\n    def serialize_to_bundle(self, transformer, path, model_name):\n\n        # compile tuples of model attributes to serialize\n        attributes = list()\n        attributes.append((\'threshold\', float(transformer.threshold)))\n        attributes.append((""input_shapes"", transformer.input_shapes))\n\n        # define node inputs and outputs\n        inputs = [{\n                  ""name"": transformer.input_features,\n                  ""port"": ""input""\n                }]\n\n        outputs = [{\n                  ""name"": transformer.output_features,\n                  ""port"": ""output""\n                }]\n\n        self.serialize(transformer, path, model_name, attributes, inputs, outputs)\n\n    def deserialize_from_bundle(self, transformer, node_path, node_name):\n\n        full_node_path = os.path.join(node_path, node_name)\n        transformer = self.deserialize_single_input_output(transformer, full_node_path)\n\n        return transformer\n\n\nclass PolynomialExpansionSerializer(MLeapSerializer, MLeapDeserializer):\n    def __init__(self):\n        super(PolynomialExpansionSerializer, self).__init__()\n\n    def serialize_to_bundle(self, transformer, path, model_name):\n\n        # compile tuples of model attributes to serialize\n        attributes = list()\n        attributes.append((\'combinations\', str(transformer.get_feature_names()).replace(""\'"", """").replace("", "", "","")))\n\n        # define node inputs and outputs\n        inputs = [{\n                  ""name"": transformer.input_features,\n                  ""port"": ""input""\n                }]\n\n        outputs = [{\n                  ""name"": transformer.output_features,\n                  ""port"": ""output""\n                }]\n\n        self.serialize(transformer, path, model_name, attributes, inputs, outputs)\n\n    def deserialize_from_bundle(self, transformer, node_path, node_name):\n\n        full_node_path = os.path.join(node_path, node_name)\n        transformer = self.deserialize_single_input_output(transformer, full_node_path)\n        transformer.include_bias = False\n\n        # Get number of input features\n        with open(""{}/node.json"".format(full_node_path)) as json_data:\n            node_j = json.load(json_data)\n\n        # Set powers\n        transformer.n_input_features_ = len(node_j[\'shape\'][\'inputs\'][0][\'name\']) # TODO: Make this dynamic\n        transformer.interaction_only = False\n\n        transformer.n_output_features_ = sum(1 for _ in transformer._combinations(transformer.n_input_features_,\n                                                                                  transformer.degree,\n                                                                                  transformer.interaction_only,\n                                                                                  transformer.include_bias))\n        return transformer\n\n\nclass ReshapeArrayToN1(BaseEstimator, TransformerMixin):\n    def __init__(self):\n        self.serializable=False\n        self.op=\'reshape_array\'\n        self.name = ""{}_{}"".format(self.op, uuid.uuid1())\n\n    def transform(self, X, **params):\n        """"""\n        :type X: np.ndarray\n        :param X:\n        :param params:\n        :return:\n        """"""\n        return X.reshape(X.size, 1)\n\n    def fit(self, df, y=None, **fit_params):\n        return self\n\n    def fit_transform(self, X, y=None, **fit_params):\n        return self.transform(X)\n\n\nclass NDArrayToDataFrame(BaseEstimator, TransformerMixin):\n    def __init__(self, input_features):\n        self.input_features = input_features\n        self.output_features = input_features\n        self.op = \'one_dim_array_to_dataframe\'\n        self.name = ""{}_{}"".format(self.op, uuid.uuid1())\n\n    def transform(self, X, **params):\n        if isinstance(X, np.ndarray):\n            return pd.DataFrame(X, columns=[self.input_features])\n        return pd.DataFrame(X.todense(), columns=[self.input_features])\n\n    def fit(self, df, y=None, **fit_params):\n        return self\n\n    def fit_transform(self, X, y=None, **fit_params):\n        return self.transform(X)\n\n    def get_mleap_model(self):\n        js = {\n          ""op"": self.op\n        }\n        return js\n\n    def get_mleap_node(self):\n        js = {\n          ""name"": self.name,\n          ""shape"": {\n            ""inputs"": [{\'name\': x, \'port\': \'input{}\'.format(self.input_features.index(x))} for x in self.input_features],\n            ""outputs"": [{\n              ""name"": self.output_features,\n              ""port"": ""output""\n            }]\n          }\n        }\n        return js\n\n    def _serialize_to_bundle(self, path, model_name):\n\n        # If bundle path already exists, delte it and create a clean directory\n        if os.path.exists(""{}/{}"".format(path, model_name)):\n            shutil.rmtree(""{}/{}"".format(path, model_name))\n\n        model_dir = ""{}/{}"".format(path, model_name)\n        os.mkdir(model_dir)\n\n        # Write bundle file\n        with open(""{}/{}"".format(model_dir, \'model.json\'), \'w\') as outfile:\n            json.dump(self.get_mleap_model(), outfile, indent=3)\n\n        # Write node file\n        with open(""{}/{}"".format(model_dir, \'node.json\'), \'w\') as outfile:\n            json.dump(self.get_mleap_node(), outfile, indent=3)\n\n\nclass ToDense(BaseEstimator, TransformerMixin):\n    def __init__(self, input_features):\n        self.op = \'dense_transformer\'\n        self.name = ""{}_{}"".format(self.op, uuid.uuid1())\n        self.input_features = input_features\n        self.output_features = input_features\n\n    def transform(self, X, **params):\n        return X.todense()\n\n    def fit(self, df, y=None, **fit_params):\n        return self\n\n    def fit_transform(self, X, y=None, **fit_params):\n        return self.transform(X)\n\n    def get_mleap_model(self):\n        js = {\n          ""op"": self.op\n        }\n        return js\n\n    def get_mleap_node(self):\n        js = {\n          ""name"": self.name,\n          ""shape"": {\n            ""inputs"": {\'name\': self.input_features, \'port\': \'input0\'},\n            ""outputs"": [{\n              ""name"": self.output_features,\n              ""port"": ""output""\n            }]\n          }\n        }\n        return js\n\n    def _serialize_to_bundle(self, path, model_name):\n        # If bundle path already exists, delte it and create a clean directory\n        if os.path.exists(""{}/{}"".format(path, model_name)):\n            shutil.rmtree(""{}/{}"".format(path, model_name))\n\n        model_dir = ""{}/{}"".format(path, model_name)\n        os.mkdir(model_dir)\n\n        # Write bundle file\n        with open(""{}/{}"".format(model_dir, \'model.json\'), \'w\') as outfile:\n            json.dump(self.get_mleap_model(), outfile, indent=3)\n\n        # Write node file\n        with open(""{}/{}"".format(model_dir, \'node.json\'), \'w\') as outfile:\n            json.dump(self.get_mleap_node(), outfile, indent=3)\n\n\nclass MathUnary(BaseEstimator, TransformerMixin, MLeapSerializer, MLeapDeserializer):\n    """"""\n    Performs basic math operations on a single feature (column of a DataFrame). Supported operations include:\n        - log\n        - exp\n        - sqrt\n        - sin\n        - cos\n        - tan\n    Note, currently we only support 1d-arrays.\n    Inputs need to be floats.\n    """"""\n    def __init__(self, input_features=None, output_features=None, transform_type=None):\n        self.valid_transforms = [\'log\', \'exp\', \'sqrt\', \'sin\', \'cos\', \'tan\', \'abs\']\n        self.op = \'math_unary\'\n        self.name = ""{}_{}"".format(self.op, uuid.uuid4())\n        self.input_features = input_features\n        self.output_features = output_features\n        self.transform_type = transform_type\n        self.serializable = True\n\n    def fit(self, X, y=None, **fit_params):\n        """"""\n        Fit Unary Math Operator\n        :param X:\n        :return:\n        """"""\n        X = column_or_1d(X, warn=True)\n        if self.transform_type not in self.valid_transforms:\n                warnings.warn(""Invalid transform type."", stacklevel=2)\n        return self\n\n    def transform(self, y):\n        """"""\n        Transform features per specified math function.\n        :param y:\n        :return:\n        """"""\n        if self.transform_type == \'log\':\n            return np.log(y)\n        elif self.transform_type == \'exp\':\n            return np.exp(y)\n        elif self.transform_type == \'sqrt\':\n            return np.sqrt(y)\n        elif self.transform_type == \'sin\':\n            return np.sin(y)\n        elif self.transform_type == \'cos\':\n            return np.cos(y)\n        elif self.transform_type == \'tan\':\n            return np.tan(y)\n        elif self.transform_type == \'abs\':\n             return np.abs(y)\n\n    def fit_transform(self, X, y=None, **fit_params):\n        """"""\n        Transform data per specified math function.\n        :param X:\n        :param y:\n        :param fit_params:\n        :return:\n        """"""\n        self.fit(X)\n\n        return self.transform(X)\n\n    def serialize_to_bundle(self, path, model_name):\n\n        # compile tuples of model attributes to serialize\n        attributes = list()\n        attributes.append((\'operation\', self.transform_type))\n\n        # define node inputs and outputs\n        inputs = [{\n                  ""name"": self.input_features[0],\n                  ""port"": ""input""\n                }]\n\n        outputs = [{\n                  ""name"": self.output_features,\n                  ""port"": ""output""\n                }]\n\n        self.serialize(self, path, model_name, attributes, inputs, outputs)\n\n    def deserialize_from_bundle(self, node_path, node_name):\n        attributes_map = {\n            \'operation\': \'transform_type\'\n        }\n        full_node_path = os.path.join(node_path, node_name)\n        transformer = self.deserialize_single_input_output(self, full_node_path, attributes_map)\n        return transformer\n\n\nclass MathBinary(BaseEstimator, TransformerMixin, MLeapSerializer, MLeapDeserializer):\n    """"""\n    Performs basic math operations on two features (columns of a DataFrame). Supported operations include:\n        - add: Add x + y\n        - sub: Subtract x - y\n        - mul: Multiply x * y\n        - div: Divide x / y\n        - rem: Remainder x % y\n        - logn: LogN log(x) / log(y)\n        - pow: Power x^y\n    These transforms work on 2-dimensional arrays/vectors, where the the first column is x and second column is y.\n    Inputs need to be floats.\n    """"""\n    def __init__(self, input_features=None, output_features=None, transform_type=None):\n        self.valid_transforms = [\'add\', \'sub\', \'mul\', \'div\', \'rem\', \'logn\', \'pow\']\n        self.op = \'math_binary\'\n        self.name = ""{}_{}"".format(self.op, uuid.uuid4())\n        self.input_features = input_features\n        self.output_features = output_features\n        self.transform_type = transform_type\n        self.serializable = True\n\n    def _return(self, y):\n        if type(y, np.ndarray):\n            return\n\n    def fit(self, X, y=None, **fit_params):\n        """"""\n        Fit Unary Math Operator\n        :param y:\n        :return:\n        """"""\n        if self.transform_type not in self.valid_transforms:\n                warnings.warn(""Invalid transform type."", stacklevel=2)\n        return self\n\n    def transform(self, y):\n        """"""\n        Transform features per specified math function.\n        :param y:\n        :return:\n        """"""\n        if isinstance(y, pd.DataFrame):\n            x = y.ix[:,0]\n            y = y.ix[:,1]\n        else:\n            x = y[:,0]\n            y = y[:,1]\n        if self.transform_type == \'add\':\n            return pd.DataFrame(np.add(x, y))\n        elif self.transform_type == \'sub\':\n            return pd.DataFrame(np.subtract(x, y))\n        elif self.transform_type == \'mul\':\n            return pd.DataFrame(np.multiply(x, y))\n        elif self.transform_type == \'div\':\n            return pd.DataFrame(np.divide(x, y))\n        elif self.transform_type == \'rem\':\n            return pd.DataFrame(np.remainder(x, y))\n        elif self.transform_type == \'pow\':\n            return pd.DataFrame(x**y)\n\n    def fit_transform(self, X, y=None, **fit_params):\n        """"""\n        Transform data per specified math function.\n        :param X:\n        :param y:\n        :param fit_params:\n        :return:\n        """"""\n        self.fit(X)\n\n        return self.transform(X)\n\n    def serialize_to_bundle(self, path, model_name):\n\n        # compile tuples of model attributes to serialize\n        attributes = list()\n        attributes.append((\'operation\', self.transform_type))\n\n        # define node inputs and outputs\n        inputs = [{\n                  ""name"": self.input_features[0],\n                  ""port"": ""input_a""\n                },\n                {\n                    ""name"": self.input_features[1],\n                    ""port"": ""input_b""\n                }]\n\n        outputs = [{\n                  ""name"": self.output_features,\n                  ""port"": ""output""\n                }]\n\n        self.serialize(self, path, model_name, attributes, inputs, outputs)\n\n    def deserialize_from_bundle(self, node_path, node_name):\n        attributes_map = {\n            \'operation\': \'transform_type\'\n        }\n        full_node_path = os.path.join(node_path, node_name)\n        transformer = self.deserialize_single_input_output(self, full_node_path, attributes_map)\n        return transformer\n\nclass StringMap(BaseEstimator, TransformerMixin, MLeapSerializer, MLeapDeserializer):\n\n    def __init__(self, input_features=None, output_features=None, labels=None):\n        self.op = \'string_map\'\n        self.name = ""{}_{}"".format(self.op, uuid.uuid4())\n        self.input_features = input_features\n        self.output_features = output_features\n        self.serializable = True\n        self.labels = labels\n        if labels is not None:\n            if not isinstance(self.labels, OrderedDict):\n                self.labels = OrderedDict(\n                    sorted(self.labels.items(), key=lambda x: x[0])\n                )\n            self.label_keys = self.labels.keys\n            self.label_values = self.labels.values\n\n    def fit(self, X, y=None, **fit_params):\n        if self.labels is None:\n            self.labels = dict(zip(self.label_keys, self.label_values))\n        return self\n\n    def transform(self, y):\n       return y.applymap(lambda input : self.labels[input]).values\n\n    def fit_transform(self, X, y=None, **fit_params):\n        self.fit(X)\n        return self.transform(X)\n\n    def serialize_to_bundle(self, path, model_name):\n        # compile tuples of model attributes to serialize\n        attributes = list()\n        attributes.append((""labels"", list(self.labels.keys())))\n        attributes.append((""values"", Vector(list(self.labels.values()))))\n\n        # define node inputs and outputs\n        inputs = [{\n            ""name"": self.input_features[0],\n            ""port"": ""input""\n        }]\n\n        outputs = [{\n            ""name"": self.output_features,\n            ""port"": ""output""\n        }]\n\n        self.serialize(self, path, model_name, attributes, inputs, outputs)\n\n    def deserialize_from_bundle(self, node_path, node_name):\n        attributes_map = {\n            \'labels\': \'label_keys\',\n            \'values\': \'label_values\'\n        }\n\n        full_node_path = os.path.join(node_path, node_name)\n        transformer = self.deserialize_single_input_output(self, full_node_path, attributes_map)\n        return transformer\n'"
python/mleap/sklearn/tree/__init__.py,0,b''
python/mleap/sklearn/tree/tree.py,0,"b'#\n# Licensed to the Apache Software Foundation (ASF) under one or more\n# contributor license agreements.  See the NOTICE file distributed with\n# this work for additional information regarding copyright ownership.\n# The ASF licenses this file to You under the Apache License, Version 2.0\n# (the ""License""); you may not use this file except in compliance with\n# the License.  You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.tree import DecisionTreeRegressor\nfrom mleap.bundle.serialize import MLeapSerializer\nfrom sklearn.tree import _tree\nimport json\nimport uuid\n\n\ndef mleap_init(self, input_features, prediction_column, feature_names):\n    self.input_features = input_features\n    self.prediction_column = prediction_column\n    self.feature_names = feature_names\n    self.name = ""{}_{}"".format(self.op, uuid.uuid1())\n\n\ndef serialize_to_bundle(self, path, model_name, serialize_node=True):\n    serializer = SimpleSerializer()\n    return serializer.serialize_to_bundle(self, path, model_name, serialize_node=serialize_node)\n\n\nsetattr(DecisionTreeRegressor, \'op\', \'decision_tree_regression\')\nsetattr(DecisionTreeRegressor, \'mlinit\', mleap_init)\nsetattr(DecisionTreeRegressor, \'serialize_to_bundle\', serialize_to_bundle)\nsetattr(DecisionTreeRegressor, \'serializable\', True)\n\nsetattr(DecisionTreeClassifier, \'op\', \'decision_tree_classifier\')\nsetattr(DecisionTreeClassifier, \'mlinit\', mleap_init)\nsetattr(DecisionTreeClassifier, \'serialize_to_bundle\', serialize_to_bundle)\nsetattr(DecisionTreeClassifier, \'serializable\', True)\n\n\nclass SimpleSerializer(MLeapSerializer):\n    def __init__(self):\n        super(SimpleSerializer, self).__init__()\n\n    @staticmethod\n    def serialize_tree(tree, feature_names, outfile):\n        """"""\n        :type feature_names: list\n        :type tree: sklearn.tree.tree.BaseDecisionTree\n        :param tree: sklearn.tree.tree\n        :param feature_names:\n        :return:\n        """"""\n\n        tree_ = tree.tree_\n        feature_name = [feature_names[i] if i != _tree.TREE_UNDEFINED else \'n/a\' for i in tree_.feature]\n\n        def traverse(node, depth, outfile):\n            if tree_.feature[node] != _tree.TREE_UNDEFINED:\n                name = feature_name[node]\n                threshold = tree_.threshold[node]\n\n                # Define internal node for serialization\n                internal_node = {\n                    \'type\': \'internal\',\n                    \'split\': {\n                        \'type\': \'continuous\',\n                        \'featureIndex\': feature_names.index(name),\n                        \'threshold\': threshold\n                    }\n                }\n\n                # Serialize the internal Node\n                json.dump(internal_node, outfile)\n                outfile.write(\'\\n\')\n\n                # Traverse Left\n                traverse(tree_.children_left[node], depth + 1, outfile)\n\n                # Traverse Rigiht\n                traverse(tree_.children_right[node], depth + 1, outfile)\n            else:\n                leaf_node = {\n                    \'type\': \'leaf\',\n                    \'values\': tree_.value[node].tolist()[0]\n                }\n\n                # Serialize the leaf node\n                json.dump(leaf_node, outfile)\n                outfile.write(\'\\n\')\n\n        traverse(0, 1, outfile)\n\n    def serialize_to_bundle(self, transformer, path, model_name, serialize_node=True):\n        """"""\n        :type transformer: sklearn.tree.tree.BaseDecisionTree\n        :type path: str\n        :type model_name: str\n        :type serialize_node: bool\n        :param transformer:\n        :param path:\n        :param model_name:\n        :return:\n        """"""\n\n        # Define attributes\n        attributes = list()\n        attributes.append((\'num_features\', transformer.n_features_))\n        if isinstance(transformer, DecisionTreeClassifier):\n            attributes.append((\'num_classes\', int(transformer.n_classes_)))\n\n        inputs = []\n        outputs = []\n        if serialize_node:\n            # define node inputs and outputs\n            inputs = [{\n                      ""name"": transformer.input_features,\n                      ""port"": ""features""\n                    }]\n\n            outputs = [{\n                      ""name"": transformer.prediction_column,\n                      ""port"": ""prediction""\n                    }]\n\n        self.serialize(transformer, path, model_name, attributes, inputs, outputs, node=serialize_node)\n\n        # Serialize tree.json\n        tree_path = ""{}/{}.node/tree.json"".format(path, model_name)\n        if not serialize_node:\n            tree_path = ""{}/{}/tree.json"".format(path, model_name)\n        with open(tree_path, \'w\') as outfile:\n            self.serialize_tree(transformer, transformer.feature_names, outfile)\n'"
python/tests/pyspark/feature/__init__.py,0,b''
python/tests/pyspark/feature/math_binary_test.py,0,"b'import math\nimport os\nimport shutil\nimport tempfile\nimport unittest\n\nimport mleap.pyspark  # noqa\nfrom mleap.pyspark.spark_support import SimpleSparkSerializer  # noqa\n\nimport pandas as pd\nfrom pandas.testing import assert_frame_equal\nfrom pyspark.ml import Pipeline\nfrom pyspark.sql.types import FloatType\nfrom pyspark.sql.types import StructType\nfrom pyspark.sql.types import StructField\n\nfrom mleap.pyspark.feature.math_binary import MathBinary\nfrom mleap.pyspark.feature.math_binary import BinaryOperation\nfrom tests.pyspark.lib.spark_session import spark_session\n\n\nINPUT_SCHEMA = StructType([\n    StructField(\'f1\', FloatType()),\n    StructField(\'f2\', FloatType()),\n])\n\n\nclass MathBinaryTest(unittest.TestCase):\n\n    @classmethod\n    def setUpClass(cls):\n        cls.spark = spark_session()\n\n    @classmethod\n    def tearDownClass(cls):\n        cls.spark.stop()\n\n    def setUp(self):\n        self.input = self.spark.createDataFrame([\n            (\n                float(i),\n                float(i * 2),\n            )\n            for i in range(1, 10)\n        ], INPUT_SCHEMA)\n\n        self.expected_add = pd.DataFrame(\n            [(\n                float(i + i * 2)\n            )\n            for i in range(1, 10)],\n            columns=[\'add(f1, f2)\'],\n        )\n\n        self.tmp_dir = tempfile.mkdtemp()\n\n    def tearDown(self):\n        shutil.rmtree(self.tmp_dir)\n\n    def _new_add_math_binary(self):\n        return MathBinary(\n            operation=BinaryOperation.Add,\n            inputA=""f1"",\n            inputB=""f2"",\n            outputCol=""add(f1, f2)"",\n        )\n\n    def test_add_math_binary(self):\n        add_transformer = self._new_add_math_binary()\n        result = add_transformer.transform(self.input).toPandas()[[\'add(f1, f2)\']]\n        assert_frame_equal(self.expected_add, result)\n\n    def test_math_binary_pipeline(self):\n        add_transformer = self._new_add_math_binary()\n\n        mul_transformer = MathBinary(\n            operation=BinaryOperation.Multiply,\n            inputA=""f1"",\n            inputB=""add(f1, f2)"",\n            outputCol=""mul(f1, add(f1, f2))"",\n        )\n\n        expected = pd.DataFrame(\n            [(\n                float(i * (i + i * 2))\n            )\n            for i in range(1, 10)],\n            columns=[\'mul(f1, add(f1, f2))\'],\n        )\n\n        pipeline = Pipeline(\n            stages=[add_transformer, mul_transformer]\n        )\n\n        pipeline_model = pipeline.fit(self.input)\n        result = pipeline_model.transform(self.input).toPandas()[[\'mul(f1, add(f1, f2))\']]\n        assert_frame_equal(expected, result)\n\n    def test_can_instantiate_all_math_binary(self):\n        for binary_operation in BinaryOperation:\n            transformer = MathBinary(\n                operation=binary_operation,\n                inputA=""f1"",\n                inputB=""f2"",\n                outputCol=""operation"",\n            )\n\n    def test_serialize_deserialize_math_binary(self):\n        add_transformer = self._new_add_math_binary()\n\n        file_path = \'{}{}\'.format(\'jar:file:\', os.path.join(self.tmp_dir, \'math_binary.zip\'))\n\n        add_transformer.serializeToBundle(file_path, self.input)\n        deserialized_math_binary = SimpleSparkSerializer().deserializeFromBundle(file_path)\n\n        result = deserialized_math_binary.transform(self.input).toPandas()[[\'add(f1, f2)\']]\n        assert_frame_equal(self.expected_add, result)\n\n    def test_serialize_deserialize_pipeline(self):\n        add_transformer = self._new_add_math_binary()\n\n        mul_transformer = MathBinary(\n            operation=BinaryOperation.Multiply,\n            inputA=""f1"",\n            inputB=""add(f1, f2)"",\n            outputCol=""mul(f1, add(f1, f2))"",\n        )\n\n        expected = pd.DataFrame(\n            [(\n                float(i * (i + i * 2))\n            )\n            for i in range(1, 10)],\n            columns=[\'mul(f1, add(f1, f2))\'],\n        )\n\n        pipeline = Pipeline(\n            stages=[add_transformer, mul_transformer]\n        )\n\n        pipeline_model = pipeline.fit(self.input)\n\n        file_path = \'{}{}\'.format(\'jar:file:\', os.path.join(self.tmp_dir, \'math_binary_pipeline.zip\'))\n\n        pipeline_model.serializeToBundle(file_path, self.input)\n        deserialized_pipeline = SimpleSparkSerializer().deserializeFromBundle(file_path)\n\n        result = pipeline_model.transform(self.input).toPandas()[[\'mul(f1, add(f1, f2))\']]\n        assert_frame_equal(expected, result)\n'"
python/tests/pyspark/feature/math_unary_test.py,0,"b'import math\nimport os\nimport shutil\nimport tempfile\nimport unittest\n\nimport mleap.pyspark  # noqa\nfrom mleap.pyspark.spark_support import SimpleSparkSerializer  # noqa\n\nimport pandas as pd\nfrom pandas.testing import assert_frame_equal\nfrom pyspark.ml import Pipeline\nfrom pyspark.sql.types import FloatType\nfrom pyspark.sql.types import StructType\nfrom pyspark.sql.types import StructField\n\nfrom mleap.pyspark.feature.math_unary import MathUnary\nfrom mleap.pyspark.feature.math_unary import UnaryOperation\nfrom tests.pyspark.lib.spark_session import spark_session\n\n\nINPUT_SCHEMA = StructType([\n    StructField(\'f1\', FloatType()),\n])\n\n\nclass MathUnaryTest(unittest.TestCase):\n\n    @classmethod\n    def setUpClass(cls):\n        cls.spark = spark_session()\n\n    @classmethod\n    def tearDownClass(cls):\n        cls.spark.stop()\n\n    def setUp(self):\n        self.input = self.spark.createDataFrame([\n            (\n                float(i),\n            )\n            for i in range(1, 10)\n        ], INPUT_SCHEMA)\n\n        self.expected_sin = pd.DataFrame(\n            [(\n                math.sin(i),\n            )\n            for i in range(1, 10)],\n            columns=[\'sin(f1)\'],\n        )\n        self.tmp_dir = tempfile.mkdtemp()\n\n    def tearDown(self):\n        shutil.rmtree(self.tmp_dir)\n\n    def test_sin_math_unary(self):\n        sin_transformer = MathUnary(\n            operation=UnaryOperation.Sin,\n            inputCol=""f1"",\n            outputCol=""sin(f1)"",\n        )\n\n        result = sin_transformer.transform(self.input).toPandas()[[\'sin(f1)\']]\n        assert_frame_equal(self.expected_sin, result)\n\n    def test_math_unary_pipeline(self):\n        sin_transformer = MathUnary(\n            operation=UnaryOperation.Sin,\n            inputCol=""f1"",\n            outputCol=""sin(f1)"",\n        )\n\n        exp_transformer = MathUnary(\n            operation=UnaryOperation.Exp,\n            inputCol=""sin(f1)"",\n            outputCol=""exp(sin(f1))"",\n        )\n\n        expected = pd.DataFrame(\n            [(\n                math.exp(math.sin(i)),\n            )\n            for i in range(1, 10)],\n            columns=[\'exp(sin(f1))\'],\n        )\n\n        pipeline = Pipeline(\n            stages=[sin_transformer, exp_transformer]\n        )\n\n        pipeline_model = pipeline.fit(self.input)\n        result = pipeline_model.transform(self.input).toPandas()[[\'exp(sin(f1))\']]\n        assert_frame_equal(expected, result)\n\n    def test_can_instantiate_all_math_unary(self):\n        for unary_operation in UnaryOperation:\n            transformer = MathUnary(\n                operation=unary_operation,\n                inputCol=""f1"",\n                outputCol=""operation"",\n            )\n\n    def test_serialize_deserialize_math_unary(self):\n        sin_transformer = MathUnary(\n            operation=UnaryOperation.Sin,\n            inputCol=""f1"",\n            outputCol=""sin(f1)"",\n        )\n\n        file_path = \'{}{}\'.format(\'jar:file:\', os.path.join(self.tmp_dir, \'math_unary.zip\'))\n\n        sin_transformer.serializeToBundle(file_path, self.input)\n        deserialized_math_unary = SimpleSparkSerializer().deserializeFromBundle(file_path)\n\n        result = deserialized_math_unary.transform(self.input).toPandas()[[\'sin(f1)\']]\n        assert_frame_equal(self.expected_sin, result)\n\n    def test_serialize_deserialize_pipeline(self):\n        sin_transformer = MathUnary(\n            operation=UnaryOperation.Sin,\n            inputCol=""f1"",\n            outputCol=""sin(f1)"",\n        )\n\n        exp_transformer = MathUnary(\n            operation=UnaryOperation.Exp,\n            inputCol=""sin(f1)"",\n            outputCol=""exp(sin(f1))"",\n        )\n\n        expected = pd.DataFrame(\n            [(\n                math.exp(math.sin(i)),\n            )\n            for i in range(1, 10)],\n            columns=[\'exp(sin(f1))\'],\n        )\n\n        pipeline = Pipeline(\n            stages=[sin_transformer, exp_transformer]\n        )\n\n        pipeline_model = pipeline.fit(self.input)\n\n        file_path = \'{}{}\'.format(\'jar:file:\', os.path.join(self.tmp_dir, \'math_unary_pipeline.zip\'))\n\n        pipeline_model.serializeToBundle(file_path, self.input)\n        deserialized_pipeline = SimpleSparkSerializer().deserializeFromBundle(file_path)\n\n        result = pipeline_model.transform(self.input).toPandas()[[\'exp(sin(f1))\']]\n        assert_frame_equal(expected, result)\n\n'"
python/tests/pyspark/feature/string_map_test.py,0,"b'import os\nimport tempfile\nimport unittest\n\nfrom py4j.protocol import Py4JJavaError\nfrom pyspark.ml import Pipeline\nfrom pyspark.sql import types as t\n\nfrom mleap.pyspark.feature.string_map import StringMap\nfrom mleap.pyspark.spark_support import SimpleSparkSerializer\nfrom tests.pyspark.lib.assertions import assert_df\nfrom tests.pyspark.lib.spark_session import spark_session\n\n\nINPUT_SCHEMA = t.StructType([t.StructField(\'key_col\', t.StringType(), False),\n                             t.StructField(\'extra_col\', t.StringType(), False)])\n\nOUTPUT_SCHEMA = t.StructType([t.StructField(\'key_col\', t.StringType(), False),\n                              t.StructField(\'extra_col\', t.StringType(), False),\n                              t.StructField(\'value_col\', t.DoubleType(), False)])\n\n\nclass StringMapTest(unittest.TestCase):\n\n    @classmethod\n    def setUpClass(cls):\n        cls.spark = spark_session()\n\n    @classmethod\n    def tearDownClass(cls):\n        cls.spark.stop()\n\n    def setUp(self):\n        self.input = StringMapTest.spark.createDataFrame([[\'a\', \'b\']], INPUT_SCHEMA)\n\n    def test_map(self):\n        result = StringMap(\n            labels={\'a\': 1.0},\n            inputCol=\'key_col\',\n            outputCol=\'value_col\',\n        ).transform(self.input)\n        expected = StringMapTest.spark.createDataFrame([[\'a\', \'b\', 1.0]], OUTPUT_SCHEMA)\n        assert_df(expected, result)\n\n    def test_map_default_value(self):\n        result = StringMap(\n            labels={\'z\': 1.0},\n            inputCol=\'key_col\',\n            outputCol=\'value_col\',\n            handleInvalid=\'keep\',\n        ).transform(self.input)\n        expected = StringMapTest.spark.createDataFrame([[\'a\', \'b\', 0.0]], OUTPUT_SCHEMA)\n        assert_df(expected, result)\n\n    def test_map_custom_default_value(self):\n        result = StringMap(\n            labels={\'z\': 1.0},\n            inputCol=\'key_col\',\n            outputCol=\'value_col\',\n            handleInvalid=\'keep\',\n            defaultValue=-1.0\n        ).transform(self.input)\n        expected = StringMapTest.spark.createDataFrame([[\'a\', \'b\', -1.0]], OUTPUT_SCHEMA)\n        assert_df(expected, result)\n\n    def test_map_missing_value_error(self):\n        with self.assertRaises(Py4JJavaError) as error:\n            StringMap(\n                labels={\'z\': 1.0},\n                inputCol=\'key_col\',\n                outputCol=\'value_col\'\n            ).transform(self.input).collect()\n        self.assertIn(\'java.util.NoSuchElementException: Missing label: a\', str(error.exception))\n\n    def test_map_from_dataframe(self):\n        labels_df = StringMapTest.spark.createDataFrame([[\'a\', 1.0]], \'key_col: string, value_col: double\')\n        result = StringMap.from_dataframe(\n            labels_df=labels_df,\n            inputCol=\'key_col\',\n            outputCol=\'value_col\'\n        ).transform(self.input)\n        expected = StringMapTest.spark.createDataFrame([[\'a\', \'b\', 1.0]], OUTPUT_SCHEMA)\n        assert_df(expected, result)\n\n    def test_serialize_to_bundle(self):\n        string_map = StringMap(\n            labels={\'a\': 1.0},\n            inputCol=\'key_col\',\n            outputCol=\'value_col\',\n        )\n        pipeline = Pipeline(stages=[string_map]).fit(self.input)\n        serialization_dataset = pipeline.transform(self.input)\n\n        jar_file_path = _serialize_to_file(pipeline, serialization_dataset)\n        deserialized_pipeline = _deserialize_from_file(jar_file_path)\n\n        result = deserialized_pipeline.transform(self.input)\n        expected = StringMapTest.spark.createDataFrame([[\'a\', \'b\', 1.0]], OUTPUT_SCHEMA)\n        assert_df(expected, result)\n\n    @staticmethod\n    def test_validate_handleInvalid_ok():\n        StringMap(labels={}, handleInvalid=\'error\')\n\n    def test_validate_handleInvalid_bad(self):\n        with self.assertRaises(AssertionError):\n            StringMap(labels=None, inputCol=dict(), outputCol=None, handleInvalid=\'invalid\')\n\n    def test_validate_labels_type_fails(self):\n        with self.assertRaises(AssertionError):\n            StringMap(labels=None, inputCol=set(), outputCol=None)\n\n    def test_validate_labels_key_fails(self):\n        with self.assertRaises(AssertionError):\n            StringMap(labels=None, inputCol={False: 0.0}, outputCol=None)\n\n    def test_validate_labels_value_fails(self):\n        with self.assertRaises(AssertionError):\n            StringMap(labels=None, inputCol={\'valid_key_type\': \'invalid_value_type\'}, outputCol=None)\n\n\ndef _serialize_to_file(model, df_for_serializing):\n    jar_file_path = _to_jar_file_path(\n        os.path.join(tempfile.mkdtemp(), \'test_serialize_to_bundle-pipeline.zip\'))\n    SimpleSparkSerializer().serializeToBundle(model, jar_file_path, df_for_serializing)\n    return jar_file_path\n\n\ndef _to_jar_file_path(path):\n    return ""jar:file:"" + path\n\n\ndef _deserialize_from_file(path):\n    return SimpleSparkSerializer().deserializeFromBundle(path)\n'"
python/tests/pyspark/lib/__init__.py,0,b''
python/tests/pyspark/lib/assertions.py,0,"b'import re\nfrom pandas.testing import assert_frame_equal\n\n\ndef _print_dfs(actual, expected):\n    print(\'actual.show(100):\')\n    actual.show(100, False)\n    print(\'expected.show(100):\')\n    expected.show(100, False)\n\n\ndef assert_df(actual, expected, sort=True):\n    """"""\n    For more elaborate DFs comparison, e.g. ignoring sorting or less precise.\n    Default precision for float comparison is 5 digits.\n    """"""\n    __tracebackhide__ = True\n\n    if sort:\n        actual = actual.orderBy(actual.columns)\n        expected = expected.orderBy(actual.columns)\n\n    try:\n        assert_frame_equal(actual.toPandas(), expected.toPandas())\n    except AssertionError as e:\n        _print_dfs(actual, expected)\n        # assert_frame_equal doesn\'t print the column name, only the index\n        #   -> get the index from message with regex & resolve column name\n        matcher = re.search(r\'iloc\\[:, (\\d+)\\]\', e.args[0])\n        if matcher:\n            iloc = matcher.group(1)\n            if iloc is not None:\n                raise AssertionError(""failed assert on column \'{}\': {}"".format(actual.columns[int(iloc)], e))\n        # couldn\'t extract column name. unexpected.\n        raise e\n'"
python/tests/pyspark/lib/spark_session.py,0,"b'import os\nfrom pyspark.sql import SparkSession\n\n\ndef spark_session():\n    """"""\n    Minimize parallelism etc. to speed up execution.\n    Test data is small & local, so just avoid all kind of overhead.\n    """"""\n    builder = SparkSession.builder \\\n        .config(\'spark.sql.shuffle.partitions\', 1) \\\n        .config(\'spark.default.parallelism\', 1) \\\n        .config(\'spark.shuffle.compress\', False) \\\n        .config(\'spark.rdd.compress\', False)\n\n    # mleap pyspark wrappers have not been tested against scala 2.12. However,\n    # they may still work as long as all jars are with the same scala version.\n    classpath = os.environ[\'SCALA_CLASS_PATH\']\n\n    return builder \\\n        .config(\'spark.driver.extraClassPath\', classpath) \\\n        .config(\'spark.executor.extraClassPath\', classpath) \\\n        .getOrCreate()\n'"
python/tests/sklearn/extensions/__init__.py,0,b''
python/tests/sklearn/extensions/data_test.py,0,"b'import json\nimport numpy as np\nimport pandas as pd\nimport shutil\nimport tempfile\nimport unittest\n\nfrom mleap.sklearn.extensions.data import Imputer\n\nclass ExtensionsTests(unittest.TestCase):\n    def setUp(self):\n        self.df = pd.DataFrame(np.random.randn(10, 5), columns=[\'a\', \'b\', \'c\', \'d\', \'e\'])\n        self.tmp_dir = tempfile.mkdtemp()\n\n    def tearDown(self):\n        shutil.rmtree(self.tmp_dir)\n\n    def imputer_ext_test(self):\n\n        def _set_nulls(df):\n            row = df[\'index\']\n            if row in [2,5]:\n                return np.NaN\n            return df.a\n\n        imputer = Imputer(strategy=\'mean\', input_features=\'a\', output_features=\'a_imputed\')\n\n        df2 = self.df\n        df2.reset_index(inplace=True)\n        df2[\'a\'] = df2.apply(_set_nulls, axis=1)\n\n        imputer.fit(df2)\n\n        self.assertAlmostEqual(imputer.statistics_[0], df2.a.mean(), places = 7)\n\n        imputer.serialize_to_bundle(self.tmp_dir, imputer.name)\n\n        expected_model = {\n            ""op"": ""imputer"",\n            ""attributes"": {\n                ""surrogate_value"": {\n                    ""double"": df2.a.mean()\n                },\n                ""strategy"": {\n                    ""string"": ""mean""\n                }\n            }\n        }\n\n        # Test model.json\n        with open(""{}/{}.node/model.json"".format(self.tmp_dir, imputer.name)) as json_data:\n            model = json.load(json_data)\n\n        self.assertEqual(expected_model[\'attributes\'][\'strategy\'][\'string\'], model[\'attributes\'][\'strategy\'][\'string\'])\n        self.assertAlmostEqual(expected_model[\'attributes\'][\'surrogate_value\'][\'double\'], model[\'attributes\'][\'surrogate_value\'][\'double\'], places = 7)\n\n        # Test node.json\n        with open(""{}/{}.node/node.json"".format(self.tmp_dir, imputer.name)) as json_data:\n            node = json.load(json_data)\n\n        self.assertEqual(imputer.name, node[\'name\'])\n        self.assertEqual(imputer.input_features, node[\'shape\'][\'inputs\'][0][\'name\'])\n        self.assertEqual(imputer.output_features, node[\'shape\'][\'outputs\'][0][\'name\'])\n'"
python/tests/sklearn/feature_extraction/__init__.py,0,b''
python/tests/sklearn/feature_extraction/text_test.py,0,"b""import json\nimport os\nimport shutil\nimport tempfile\nimport unittest\nimport uuid\n\nfrom mleap.sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n\n\nclass TransformerTests(unittest.TestCase):\n    def setUp(self):\n        self.tmp_dir = tempfile.mkdtemp()\n\n        self.docs = ['test']\n\n        self.tfidf = TfidfVectorizer(binary=False)\n        self.tfidf.mlinit(input_features='some_text', prediction_column='features')\n        self.tfidf.fit(self.docs)\n\n        self.assertEqual(self.tfidf.vocabulary_, {u'test': 0})\n        self.assertEqual(self.tfidf.idf_, [1.0])\n\n        self.tfidf.serialize_to_bundle(self.tmp_dir, self.tfidf.name)\n        self.pipe_model = self.load_pipeline_model()\n\n    def tearDown(self):\n        shutil.rmtree(self.tmp_dir)\n\n    def test_countvectorizer_serializer(self):\n        count = CountVectorizer()\n        count.mlinit(input_features='input', prediction_column='pred')\n        count.fit(self.docs)\n        count.serialize_to_bundle(self.tmp_dir, count.name)\n\n        with open('{}/{}.node/node.json'.format(self.tmp_dir, count.name)) as node_json:\n            node = json.load(node_json)\n\n        with open('{}/{}.node/model.json'.format(self.tmp_dir, count.name)) as model_json:\n            model = json.load(model_json)\n\n        self.assertEqual(node['shape']['inputs'][0]['name'], 'input')\n        self.assertEqual(node['shape']['outputs'][0]['name'], 'pred')\n        self.assertEqual(model['op'], 'tokenizer')\n\n    def test_tfidf_vectorizer_serializer_pipeline_part(self):\n        self.assertEqual(self.pipe_model['op'], 'pipeline')\n\n    def test_tfidf_vectorizer_serializer_tf_part(self):\n        nodes = self.pipe_model['attributes']['nodes']['string']\n        tf_node_name = nodes[0]\n\n        with open('{}/{}/root/{}.node/model.json'.format(self.tmp_dir, self.tfidf.name, tf_node_name)) as tf_model_json:\n            tf_model = json.load(tf_model_json)\n\n        self.assertEqual(tf_model['attributes']['binary']['boolean'], False)\n        self.assertEqual(tf_model['attributes']['vocabulary']['string'], self.docs)\n        self.assertEqual(tf_model['op'], 'count_vectorizer')\n\n        with open('{}/{}/root/{}.node/node.json'.format(self.tmp_dir, self.tfidf.name, tf_node_name)) as tf_node_json:\n            tf_node = json.load(tf_node_json)\n\n        self.assertEqual(tf_node['shape']['inputs'][0]['name'], 'some_text')\n        self.assertEqual(tf_node['shape']['outputs'][0]['name'], 'token_counts')\n\n    def test_tfidf_vectorizer_serializer_idf_part(self):\n        nodes = self.pipe_model['attributes']['nodes']['string']\n        idf_node_name = nodes[1]\n\n        with open('{}/{}/root/{}.node/node.json'.format(self.tmp_dir, self.tfidf.name, idf_node_name)) as idf_node_json:\n            idf_node = json.load(idf_node_json)\n\n        self.assertEqual(idf_node['shape']['inputs'][0]['name'], 'token_counts')\n        self.assertEqual(idf_node['shape']['outputs'][0]['name'], 'features')\n\n        idf_model_file = '{}/{}/root/{}.node/model.json'.format(self.tmp_dir, self.tfidf.name, idf_node_name)\n        with open(idf_model_file) as idf_model_json:\n            idf_model = json.load(idf_model_json)\n\n        idf_shape = {\n            'dimensions': [\n                {\n                    'name': '',\n                    'size': 1\n                }\n            ]\n        }\n\n        self.assertEqual(idf_model['attributes']['idf']['double'], [1.0])\n        self.assertEqual(idf_model['attributes']['idf']['shape'], idf_shape)\n        self.assertEqual(idf_model['attributes']['idf']['type'], 'tensor')\n        self.assertEqual(idf_model['op'], 'idf')\n\n    def load_pipeline_model(self):\n        with open('{}/{}/root/model.json'.format(self.tmp_dir, self.tfidf.name)) as pipeline_model_json:\n            return json.load(pipeline_model_json)\n"""
python/tests/sklearn/preprocessing/__init__.py,0,b''
python/tests/sklearn/preprocessing/data_test.py,109,"b'\n#\n# Licensed to the Apache Software Foundation (ASF) under one or more\n# contributor license agreements.  See the NOTICE file distributed with\n# this work for additional information regarding copyright ownership.\n# The ASF licenses this file to You under the Apache License, Version 2.0\n# (the ""License""); you may not use this file except in compliance with\n# the License.  You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\nimport unittest\nimport pandas as pd\nimport numpy as np\nimport os\nimport shutil\nimport json\nimport uuid\nimport tempfile\n\nfrom mleap.sklearn.preprocessing.data import FeatureExtractor, MathUnary, MathBinary, StringMap\nfrom mleap.sklearn.preprocessing.data import StandardScaler, MinMaxScaler, LabelEncoder, Imputer, Binarizer, PolynomialFeatures\nfrom mleap.sklearn.preprocessing.data import OneHotEncoder\n\nfrom pandas.util.testing import assert_frame_equal\n\nclass TransformerTests(unittest.TestCase):\n    def setUp(self):\n        self.df = pd.DataFrame(np.random.randn(10, 5), columns=[\'a\', \'b\', \'c\', \'d\', \'e\'])\n        self.tmp_dir = tempfile.mkdtemp()\n\n    def tearDown(self):\n        shutil.rmtree(self.tmp_dir)\n\n    def test_standard_scaler_serializer(self):\n\n        standard_scaler = StandardScaler(with_mean=True,\n                                         with_std=True\n                                         )\n\n        extract_features = [\'a\']\n        feature_extractor = FeatureExtractor(input_scalars=[\'a\'],\n                                             output_vector=\'extracted_a_output\',\n                                             output_vector_items=[""{}_out"".format(x) for x in extract_features])\n\n        standard_scaler.mlinit(prior_tf=feature_extractor,\n                               output_features=\'a_scaled\')\n\n        standard_scaler.fit(self.df[[\'a\']])\n\n        standard_scaler.serialize_to_bundle(self.tmp_dir, standard_scaler.name)\n\n        expected_mean = self.df.a.mean()\n        expected_std = np.sqrt(np.var(self.df.a))\n\n        expected_model = {\n            ""op"": ""standard_scaler"",\n            ""attributes"": {\n                ""mean"": {\n                    ""double"": [expected_mean],\n                    ""shape"": {\n                        ""dimensions"": [{\n                            ""size"": 1,\n                            ""name"": """"\n                        }]\n                    },\n                    ""type"": ""tensor""\n                },\n                ""std"": {\n                    ""double"": [expected_std],\n                    ""shape"": {\n                        ""dimensions"": [{\n                            ""size"": 1,\n                            ""name"": """"\n                        }]\n                    },\n                    ""type"": ""tensor""\n                }\n            }\n        }\n\n        self.assertAlmostEqual(expected_mean, standard_scaler.mean_.tolist()[0], places = 7)\n        self.assertAlmostEqual(expected_std, np.sqrt(standard_scaler.var_.tolist()[0]), places = 7)\n\n        # Test model.json\n        with open(""{}/{}.node/model.json"".format(self.tmp_dir, standard_scaler.name)) as json_data:\n            model = json.load(json_data)\n\n        self.assertEqual(standard_scaler.op, expected_model[\'op\'])\n        self.assertEqual(expected_model[\'attributes\'][\'mean\'][\'shape\'][\'dimensions\'][0][\'size\'], model[\'attributes\'][\'mean\'][\'shape\'][\'dimensions\'][0][\'size\'])\n        self.assertEqual(expected_model[\'attributes\'][\'std\'][\'shape\'][\'dimensions\'][0][\'size\'], model[\'attributes\'][\'std\'][\'shape\'][\'dimensions\'][0][\'size\'])\n        self.assertAlmostEqual(expected_model[\'attributes\'][\'mean\'][\'double\'][0], model[\'attributes\'][\'mean\'][\'double\'][0], places = 7)\n        self.assertAlmostEqual(expected_model[\'attributes\'][\'std\'][\'double\'][0], model[\'attributes\'][\'std\'][\'double\'][0], places = 7)\n\n        # Test node.json\n        with open(""{}/{}.node/node.json"".format(self.tmp_dir, standard_scaler.name)) as json_data:\n            node = json.load(json_data)\n\n        self.assertEqual(standard_scaler.name, node[\'name\'])\n        self.assertEqual(standard_scaler.input_features, node[\'shape\'][\'inputs\'][0][\'name\'])\n        self.assertEqual(standard_scaler.output_features, node[\'shape\'][\'outputs\'][0][\'name\'])\n\n    def test_standard_scaler_deserializer(self):\n\n        extract_features = [\'a\']\n        feature_extractor = FeatureExtractor(input_scalars=[\'a\'],\n                                         output_vector=\'extracted_a_output\',\n                                         output_vector_items=[""{}_out"".format(x) for x in extract_features])\n\n        # Serialize a standard scaler to a bundle\n        standard_scaler = StandardScaler(with_mean=True,\n                                         with_std=True\n                                         )\n\n        standard_scaler.mlinit(prior_tf=feature_extractor,\n                               output_features=\'a_scaled\')\n\n        standard_scaler.fit(self.df[[\'a\']])\n\n        standard_scaler.serialize_to_bundle(self.tmp_dir, standard_scaler.name)\n\n        # Now deserialize it back\n\n        node_name = ""{}.node"".format(standard_scaler.name)\n\n        standard_scaler_tf = StandardScaler()\n\n        standard_scaler_tf = standard_scaler_tf.deserialize_from_bundle(self.tmp_dir, node_name)\n\n        # Transform some sample data\n        res_a = standard_scaler.transform(self.df[[\'a\']])\n        res_b = standard_scaler_tf.transform(self.df[[\'a\']])\n\n        self.assertEqual(res_a[0], res_b[0])\n        self.assertEqual(standard_scaler.name, standard_scaler_tf.name)\n        self.assertEqual(standard_scaler.op, standard_scaler_tf.op)\n        self.assertEqual(standard_scaler.mean_, standard_scaler_tf.mean_)\n        self.assertEqual(standard_scaler.scale_, standard_scaler_tf.scale_)\n\n    def test_standard_scaler_multi_deserializer(self):\n\n        extract_features = [\'a\', \'b\']\n        feature_extractor = FeatureExtractor(input_scalars=[\'a\', \'b\'],\n                                             output_vector=\'extracted_multi_outputs\',\n                                             output_vector_items=[""{}_out"".format(x) for x in extract_features])\n\n        # Serialize a standard scaler to a bundle\n        standard_scaler = StandardScaler(with_mean=True,\n                                         with_std=True\n                                         )\n\n        standard_scaler.mlinit(prior_tf=feature_extractor,\n                               output_features=[\'a_scaled\', \'b_scaled\'])\n\n        standard_scaler.fit(self.df[[\'a\', \'b\']])\n\n        standard_scaler.serialize_to_bundle(self.tmp_dir, standard_scaler.name)\n\n        # Now deserialize it back\n\n        node_name = ""{}.node"".format(standard_scaler.name)\n\n        standard_scaler_tf = StandardScaler()\n\n        standard_scaler_tf = standard_scaler_tf.deserialize_from_bundle(self.tmp_dir, node_name)\n\n        # Transform some sample data\n        res_a = standard_scaler.transform(self.df[[\'a\', \'b\']])\n        res_b = standard_scaler_tf.transform(self.df[[\'a\', \'b\']])\n\n        self.assertEqual(res_a[0][0], res_b[0][0])\n        self.assertEqual(res_a[0][1], res_b[0][1])\n        self.assertEqual(standard_scaler.name, standard_scaler_tf.name)\n        self.assertEqual(standard_scaler.op, standard_scaler_tf.op)\n        self.assertEqual(standard_scaler.mean_[0], standard_scaler_tf.mean_[0])\n        self.assertEqual(standard_scaler.mean_[1], standard_scaler_tf.mean_[1])\n        self.assertEqual(standard_scaler.scale_[0], standard_scaler_tf.scale_[0])\n        self.assertEqual(standard_scaler.scale_[1], standard_scaler_tf.scale_[1])\n\n    def test_min_max_scaler_serializer(self):\n\n        extract_features = [\'a\']\n        feature_extractor = FeatureExtractor(input_scalars=[\'a\'],\n                                         output_vector=\'extracted_a_output\',\n                                         output_vector_items=[""{}_out"".format(x) for x in extract_features])\n\n        scaler = MinMaxScaler()\n        scaler.mlinit(prior_tf = feature_extractor,\n                      output_features=\'a_scaled\')\n\n        scaler.fit(self.df[[\'a\']])\n\n        scaler.serialize_to_bundle(self.tmp_dir, scaler.name)\n\n        expected_min = self.df.a.min()\n        expected_max = self.df.a.max()\n\n        expected_model = {\n           ""op"": ""min_max_scaler"",\n            ""attributes"": {\n                ""min"": {\n                    ""double"": [expected_min],\n                    ""shape"": {\n                        ""dimensions"": [{\n                            ""size"": 1,\n                            ""name"": """"\n                        }]\n                    },\n                    ""type"": ""tensor""\n                },\n                ""max"": {\n                    ""double"": [expected_max],\n                    ""shape"": {\n                        ""dimensions"": [{\n                            ""size"": 1,\n                            ""name"": """"\n                        }]\n                    },\n                    ""type"": ""tensor""\n                }\n            }\n        }\n\n        self.assertEqual(expected_min, scaler.data_min_.tolist()[0])\n        self.assertEqual(expected_max, scaler.data_max_.tolist()[0])\n\n        # Test model.json\n        with open(""{}/{}.node/model.json"".format(self.tmp_dir, scaler.name)) as json_data:\n            model = json.load(json_data)\n\n        self.assertEqual(scaler.op, expected_model[\'op\'])\n        self.assertEqual(expected_model[\'attributes\'][\'min\'][\'shape\'][\'dimensions\'][0][\'size\'], model[\'attributes\'][\'min\'][\'shape\'][\'dimensions\'][0][\'size\'])\n        self.assertEqual(expected_model[\'attributes\'][\'max\'][\'shape\'][\'dimensions\'][0][\'size\'], model[\'attributes\'][\'max\'][\'shape\'][\'dimensions\'][0][\'size\'])\n        self.assertEqual(expected_model[\'attributes\'][\'min\'][\'double\'][0], model[\'attributes\'][\'min\'][\'double\'][0])\n        self.assertEqual(expected_model[\'attributes\'][\'max\'][\'double\'][0], model[\'attributes\'][\'max\'][\'double\'][0])\n\n        # Test node.json\n        with open(""{}/{}.node/node.json"".format(self.tmp_dir, scaler.name)) as json_data:\n            node = json.load(json_data)\n\n        self.assertEqual(scaler.name, node[\'name\'])\n        self.assertEqual(scaler.input_features, node[\'shape\'][\'inputs\'][0][\'name\'])\n        self.assertEqual(scaler.output_features, node[\'shape\'][\'outputs\'][0][\'name\'])\n\n    def test_min_max_scaler_deserializer(self):\n\n        extract_features = [\'a\']\n        feature_extractor = FeatureExtractor(input_scalars=[\'a\'],\n                                             output_vector=\'extracted_a_output\',\n                                             output_vector_items=[""{}_out"".format(x) for x in extract_features])\n\n        scaler = MinMaxScaler()\n        scaler.mlinit(prior_tf=feature_extractor,\n                      output_features=\'a_scaled\')\n\n        scaler.fit(self.df[[\'a\']])\n\n        scaler.serialize_to_bundle(self.tmp_dir, scaler.name)\n\n        # Deserialize the MinMaxScaler\n        node_name = ""{}.node"".format(scaler.name)\n        min_max_scaler_tf = MinMaxScaler()\n        min_max_scaler_tf.deserialize_from_bundle(self.tmp_dir, node_name)\n\n        # Transform some sample data\n        res_a = scaler.transform(self.df[[\'a\']])\n        res_b = min_max_scaler_tf.transform(self.df[[\'a\']])\n\n        self.assertEqual(res_a[0], res_b[0])\n\n        self.assertEqual(scaler.name, min_max_scaler_tf.name)\n        self.assertEqual(scaler.op, min_max_scaler_tf.op)\n\n    def test_min_max_scaler_multi_deserializer(self):\n\n        extract_features = [\'a\', \'b\']\n        feature_extractor = FeatureExtractor(input_scalars=[\'a\', \'b\'],\n                                             output_vector=\'extracted_multi_outputs\',\n                                             output_vector_items=[""{}_out"".format(x) for x in extract_features])\n\n        scaler = MinMaxScaler()\n        scaler.mlinit(prior_tf=feature_extractor,\n                      output_features=[\'a_scaled\', \'b_scaled\'])\n\n        scaler.fit(self.df[[\'a\']])\n\n        scaler.serialize_to_bundle(self.tmp_dir, scaler.name)\n\n        # Deserialize the MinMaxScaler\n        node_name = ""{}.node"".format(scaler.name)\n        min_max_scaler_tf = MinMaxScaler()\n        min_max_scaler_tf.deserialize_from_bundle(self.tmp_dir, node_name)\n\n        # Transform some sample data\n        res_a = scaler.transform(self.df[[\'a\', \'b\']])\n        res_b = min_max_scaler_tf.transform(self.df[[\'a\', \'b\']])\n\n        self.assertEqual(res_a[0][0], res_b[0][0])\n        self.assertEqual(res_a[0][1], res_b[0][1])\n\n        self.assertEqual(scaler.name, min_max_scaler_tf.name)\n        self.assertEqual(scaler.op, min_max_scaler_tf.op)\n\n    def label_encoder_test(self):\n\n        labels = [\'a\', \'b\', \'c\']\n\n        le = LabelEncoder(input_features=[\'label_feature\'],\n                  output_features=\'label_feature_le_encoded\')\n\n        le.fit(labels)\n\n        self.assertEqual(labels, le.classes_.tolist())\n\n        le.serialize_to_bundle(self.tmp_dir, le.name)\n\n        # Test model.json\n        with open(""{}/{}.node/model.json"".format(self.tmp_dir, le.name)) as json_data:\n            model = json.load(json_data)\n\n        self.assertEqual(le.op, model[\'op\'])\n        self.assertTrue(\'nullable_input\' in model[\'attributes\'])\n        self.assertTrue(\'labels\' in model[\'attributes\'])\n\n        # Test node.json\n        with open(""{}/{}.node/node.json"".format(self.tmp_dir, le.name)) as json_data:\n            node = json.load(json_data)\n\n        self.assertEqual(le.name, node[\'name\'])\n        self.assertEqual(le.input_features[0], node[\'shape\'][\'inputs\'][0][\'name\'])\n        self.assertEqual(le.output_features, node[\'shape\'][\'outputs\'][0][\'name\'])\n\n    def label_encoder_deserializer_test(self):\n\n        labels = [\'a\', \'b\', \'c\']\n        le = LabelEncoder(input_features=[\'label_feature\'],\n                          output_features=\'label_feature_le_encoded\')\n\n        le.fit(labels)\n\n        self.assertEqual(labels, le.classes_.tolist())\n\n        le.serialize_to_bundle(self.tmp_dir, le.name)\n\n        # Test model.json\n        with open(""{}/{}.node/model.json"".format(self.tmp_dir, le.name)) as json_data:\n            model = json.load(json_data)\n\n        # Deserialize the LabelEncoder\n        node_name = ""{}.node"".format(le.name)\n        label_encoder_tf = LabelEncoder()\n        label_encoder_tf.deserialize_from_bundle(self.tmp_dir, node_name)\n\n        # Transform some sample data\n        res_a = le.transform(labels)\n        res_b = label_encoder_tf.transform(labels)\n        print(""le.output_features: {}"".format(le.output_features))\n        print(""label_encoder_tf.output_features: {}"".format(label_encoder_tf.output_features))\n        self.assertEqual(res_a[0], res_b[0])\n        self.assertEqual(res_a[1], res_b[1])\n        self.assertEqual(res_a[2], res_b[2])\n        self.assertEqual(le.input_features, label_encoder_tf.input_features)\n        self.assertEqual(le.output_features, label_encoder_tf.output_features[0])\n\n    def one_hot_encoder_serializer_test(self):\n\n        labels = [\'a\', \'b\', \'c\']\n\n        le = LabelEncoder(input_features=[\'label_feature\'],\n                          output_features=\'label_feature_le_encoded\')\n\n        oh_data = le.fit_transform(labels).reshape(3, 1)\n\n        one_hot_encoder_tf = OneHotEncoder(sparse=False)\n        one_hot_encoder_tf.mlinit(prior_tf=le,\n                                  output_features=\'{}_one_hot_encoded\'.format(le.output_features))\n        one_hot_encoder_tf.fit(oh_data)\n\n        one_hot_encoder_tf.serialize_to_bundle(self.tmp_dir, one_hot_encoder_tf.name)\n\n        # Test model.json\n        with open(""{}/{}.node/model.json"".format(self.tmp_dir, one_hot_encoder_tf.name)) as json_data:\n            model = json.load(json_data)\n\n        self.assertEqual(one_hot_encoder_tf.op, model[\'op\'])\n        self.assertEqual(3, model[\'attributes\'][\'size\'][\'long\'])\n        self.assertEqual(True, model[\'attributes\'][\'drop_last\'][\'boolean\'])\n        self.assertEqual(\'error\', model[\'attributes\'][\'handle_invalid\'][\'string\'])\n\n    def one_hot_encoder_deserializer_test(self):\n\n        labels = [\'a\', \'b\', \'c\']\n\n        le = LabelEncoder(input_features=[\'label_feature\'],\n                          output_features=\'label_feature_le_encoded\')\n\n        oh_data = le.fit_transform(labels).reshape(3, 1)\n\n        one_hot_encoder_tf = OneHotEncoder(sparse=False)\n        one_hot_encoder_tf.mlinit(prior_tf = le,\n                                  output_features=\'{}_one_hot_encoded\'.format(le.output_features))\n        one_hot_encoder_tf.fit(oh_data)\n\n        one_hot_encoder_tf.serialize_to_bundle(self.tmp_dir, one_hot_encoder_tf.name)\n\n        # Deserialize the OneHotEncoder\n        node_name = ""{}.node"".format(one_hot_encoder_tf.name)\n        one_hot_encoder_tf_ds = OneHotEncoder()\n        one_hot_encoder_tf_ds.deserialize_from_bundle(self.tmp_dir, node_name)\n\n        # Transform some sample data\n        res_a = one_hot_encoder_tf.transform(oh_data)\n        res_b = one_hot_encoder_tf_ds.transform(oh_data)\n\n        self.assertEqual(res_a[0][0], res_b[0][0])\n        self.assertEqual(res_a[1][0], res_b[1][0])\n        self.assertEqual(res_a[2][0], res_b[2][0])\n\n        # Test node.json\n        with open(""{}/{}.node/node.json"".format(self.tmp_dir, one_hot_encoder_tf.name)) as json_data:\n            node = json.load(json_data)\n\n        self.assertEqual(one_hot_encoder_tf_ds.name, node[\'name\'])\n        self.assertEqual(one_hot_encoder_tf_ds.input_features[0], node[\'shape\'][\'inputs\'][0][\'name\'])\n        self.assertEqual(one_hot_encoder_tf_ds.output_features[0], node[\'shape\'][\'outputs\'][0][\'name\'])\n\n    def feature_extractor_test(self):\n\n        extract_features = [\'a\', \'d\']\n\n        feature_extractor = FeatureExtractor(input_scalars=extract_features,\n                                             output_vector=\'extract_features_output\',\n                                             output_vector_items=[""{}_out"".format(x) for x in extract_features])\n\n        res = feature_extractor.fit_transform(self.df)\n\n        self.assertEqual(len(res.columns), 2)\n\n        feature_extractor.serialize_to_bundle(self.tmp_dir, feature_extractor.name)\n\n        # Test node.json\n        with open(""{}/{}.node/node.json"".format(self.tmp_dir, feature_extractor.name)) as json_data:\n            node = json.load(json_data)\n\n        self.assertEqual(feature_extractor.name, node[\'name\'])\n        self.assertEqual(feature_extractor.input_features[0], node[\'shape\'][\'inputs\'][0][\'name\'])\n        self.assertEqual(feature_extractor.input_features[1], node[\'shape\'][\'inputs\'][1][\'name\'])\n        self.assertEqual(feature_extractor.output_vector, node[\'shape\'][\'outputs\'][0][\'name\'])\n\n        # Test model.json\n        with open(""{}/{}.node/model.json"".format(self.tmp_dir, feature_extractor.name)) as json_data:\n            model = json.load(json_data)\n\n        expected_model = {\n            ""op"": ""vector_assembler"",\n            ""attributes"": {\n                ""input_shapes"": {\n                    ""data_shape"": [\n                        {\n                        ""base"": ""scalar"",\n                        ""isNullable"": False\n                        },\n                        {\n                        ""base"": ""scalar"",\n                        ""isNullable"": False\n                        }],\n                    ""type"": ""list""\n                }\n            }\n        }\n\n        self.assertEqual(expected_model[\'op\'], model[\'op\'])\n        self.assertEqual(expected_model[\'attributes\'][\'input_shapes\'][\'data_shape\'][0][\'base\'],\n                         model[\'attributes\'][\'input_shapes\'][\'data_shape\'][0][\'base\'])\n        self.assertEqual(expected_model[\'attributes\'][\'input_shapes\'][\'data_shape\'][0][\'isNullable\'],\n                         model[\'attributes\'][\'input_shapes\'][\'data_shape\'][0][\'isNullable\'])\n        self.assertEqual(expected_model[\'attributes\'][\'input_shapes\'][\'data_shape\'][1][\'base\'],\n                         model[\'attributes\'][\'input_shapes\'][\'data_shape\'][1][\'base\'])\n        self.assertEqual(expected_model[\'attributes\'][\'input_shapes\'][\'data_shape\'][1][\'isNullable\'],\n                     model[\'attributes\'][\'input_shapes\'][\'data_shape\'][1][\'isNullable\'])\n\n    def imputer_test(self):\n\n        def _set_nulls(df):\n            row = df[\'index\']\n            if row in [2,5]:\n                return np.NaN\n            return df.a\n\n        extract_features = [\'a\']\n        feature_extractor = FeatureExtractor(input_scalars=[\'a\'],\n                                         output_vector=\'extracted_a_output\',\n                                         output_vector_items=[""{}_out"".format(x) for x in extract_features])\n\n        imputer = Imputer(strategy=\'mean\')\n        imputer.mlinit(prior_tf=feature_extractor,\n                       output_features=\'a_imputed\')\n\n        df2 = self.df\n        df2.reset_index(inplace=True)\n        df2[\'a\'] = df2.apply(_set_nulls, axis=1)\n\n        imputer.fit(df2[[\'a\']])\n\n        self.assertAlmostEqual(imputer.statistics_[0], df2.a.mean(), places = 7)\n\n        imputer.serialize_to_bundle(self.tmp_dir, imputer.name)\n\n        expected_model = {\n          ""op"": ""imputer"",\n          ""attributes"": {\n            ""surrogate_value"": {\n              ""double"": df2.a.mean()\n            },\n            ""strategy"": {\n              ""string"": ""mean""\n            }\n          }\n        }\n\n        # Test model.json\n        with open(""{}/{}.node/model.json"".format(self.tmp_dir, imputer.name)) as json_data:\n            model = json.load(json_data)\n\n        self.assertEqual(expected_model[\'attributes\'][\'strategy\'][\'string\'], model[\'attributes\'][\'strategy\'][\'string\'])\n        self.assertAlmostEqual(expected_model[\'attributes\'][\'surrogate_value\'][\'double\'], model[\'attributes\'][\'surrogate_value\'][\'double\'], places = 7)\n\n        # Test node.json\n        with open(""{}/{}.node/node.json"".format(self.tmp_dir, imputer.name)) as json_data:\n            node = json.load(json_data)\n\n        self.assertEqual(imputer.name, node[\'name\'])\n        self.assertEqual(imputer.input_features, node[\'shape\'][\'inputs\'][0][\'name\'])\n        self.assertEqual(imputer.output_features, node[\'shape\'][\'outputs\'][0][\'name\'])\n\n    def binarizer_test(self):\n\n        extract_features = [\'a\']\n        feature_extractor = FeatureExtractor(input_scalars=[\'a\'],\n                                         output_vector=\'extracted_a_output\',\n                                         output_vector_items=[""{}_out"".format(x) for x in extract_features])\n\n        binarizer = Binarizer(threshold=0)\n        binarizer.mlinit(prior_tf=feature_extractor,\n                         output_features=\'a_binary\')\n\n        Xres = binarizer.fit_transform(self.df[[\'a\']])\n\n        # Test that the binarizer functions as expected\n        self.assertEqual(float(len(self.df[self.df.a >= 0]))/10.0, Xres.mean())\n\n        binarizer.serialize_to_bundle(self.tmp_dir, binarizer.name)\n\n        expected_model = {\n          ""op"": ""sklearn_binarizer"",\n          ""attributes"": {\n            ""threshold"": {\n              ""double"": 0.0\n            }\n          }\n        }\n\n        # Test model.json\n        with open(""{}/{}.node/model.json"".format(self.tmp_dir, binarizer.name)) as json_data:\n            model = json.load(json_data)\n\n        self.assertEqual(expected_model[\'attributes\'][\'threshold\'][\'double\'],\n                         model[\'attributes\'][\'threshold\'][\'double\'])\n        self.assertEqual(expected_model[\'op\'], model[\'op\'])\n\n        # Test node.json\n        with open(""{}/{}.node/node.json"".format(self.tmp_dir, binarizer.name)) as json_data:\n            node = json.load(json_data)\n\n        self.assertEqual(binarizer.name, node[\'name\'])\n        self.assertEqual(binarizer.input_features, node[\'shape\'][\'inputs\'][0][\'name\'])\n        self.assertEqual(binarizer.output_features, node[\'shape\'][\'outputs\'][0][\'name\'])\n\n    def binarizer_deserializer_test(self):\n\n        extract_features = [\'a\']\n        feature_extractor = FeatureExtractor(input_scalars=[\'a\'],\n                                         output_vector=\'extracted_a_output\',\n                                         output_vector_items=[""{}_out"".format(x) for x in extract_features])\n\n        binarizer = Binarizer(threshold=0.0)\n        binarizer.mlinit(prior_tf=feature_extractor,\n                         output_features=\'a_binary\')\n\n        Xres = binarizer.fit_transform(self.df[[\'a\']])\n\n        # Test that the binarizer functions as expected\n        self.assertEqual(float(len(self.df[self.df.a >= 0]))/10.0, Xres.mean())\n\n        binarizer.serialize_to_bundle(self.tmp_dir, binarizer.name)\n\n        # Deserialize the Binarizer\n        node_name = ""{}.node"".format(binarizer.name)\n        binarizer_tf_ds = Binarizer()\n        binarizer_tf_ds.deserialize_from_bundle(self.tmp_dir, node_name)\n\n        # Transform some sample data\n        res_a = binarizer.transform(self.df[[\'a\']])\n        res_b = binarizer_tf_ds.transform(self.df[[\'a\']])\n\n        self.assertEqual(res_a[0][0], res_b[0][0])\n        self.assertEqual(res_a[1][0], res_b[1][0])\n        self.assertEqual(res_a[2][0], res_b[2][0])\n        self.assertEqual(res_a[3][0], res_b[3][0])\n\n    def polynomial_expansion_test(self):\n\n        extract_features = [\'a\']\n        feature_extractor = FeatureExtractor(input_scalars=[\'a\'],\n                                         output_vector=\'extracted_a_output\',\n                                         output_vector_items=[""{}_out"".format(x) for x in extract_features])\n\n        polynomial_exp = PolynomialFeatures(degree=2, include_bias=False)\n        polynomial_exp.mlinit(prior_tf=feature_extractor,\n                              output_features=\'poly\')\n\n        Xres = polynomial_exp.fit_transform(self.df[[\'a\']])\n\n        self.assertEqual(Xres[0][1], Xres[0][0] * Xres[0][0])\n\n        polynomial_exp.serialize_to_bundle(self.tmp_dir, polynomial_exp.name)\n\n        expected_model = {\n          ""op"": ""sklearn_polynomial_expansion"",\n          ""attributes"": {\n              ""combinations"": {\n                  ""string"": ""[x0,x0^2]""\n              }\n          }\n        }\n\n        # Test model.json\n        with open(""{}/{}.node/model.json"".format(self.tmp_dir, polynomial_exp.name)) as json_data:\n            model = json.load(json_data)\n\n        self.assertEqual(expected_model[\'op\'], model[\'op\'])\n        self.assertEqual(expected_model[\'attributes\'][\'combinations\'][\'string\'], model[\'attributes\'][\'combinations\'][\'string\'])\n\n        # Test node.json\n        with open(""{}/{}.node/node.json"".format(self.tmp_dir, polynomial_exp.name)) as json_data:\n            node = json.load(json_data)\n\n        self.assertEqual(polynomial_exp.name, node[\'name\'])\n        self.assertEqual(polynomial_exp.input_features, node[\'shape\'][\'inputs\'][0][\'name\'])\n        self.assertEqual(polynomial_exp.output_features, node[\'shape\'][\'outputs\'][0][\'name\'])\n\n    def math_unary_exp_test(self):\n\n        math_unary_tf = MathUnary(input_features=[\'a\'], output_features=\'exp_a\', transform_type=\'exp\')\n\n        Xres = math_unary_tf.fit_transform(self.df.a)\n\n        self.assertEqual(np.exp(self.df.a[0]), Xres[0])\n\n        math_unary_tf.serialize_to_bundle(self.tmp_dir, math_unary_tf.name)\n\n        expected_model = {\n          ""op"": ""math_unary"",\n          ""attributes"": {\n            ""operation"": {\n              ""string"": \'exp\'\n            }\n          }\n        }\n\n        # Test model.json\n        with open(""{}/{}.node/model.json"".format(self.tmp_dir, math_unary_tf.name)) as json_data:\n            model = json.load(json_data)\n\n        self.assertEqual(expected_model[\'attributes\'][\'operation\'][\'string\'], model[\'attributes\'][\'operation\'][\'string\'])\n\n        # Test node.json\n        with open(""{}/{}.node/node.json"".format(self.tmp_dir, math_unary_tf.name)) as json_data:\n            node = json.load(json_data)\n\n        self.assertEqual(math_unary_tf.name, node[\'name\'])\n        self.assertEqual(math_unary_tf.input_features[0], node[\'shape\'][\'inputs\'][0][\'name\'])\n        self.assertEqual(math_unary_tf.output_features, node[\'shape\'][\'outputs\'][0][\'name\'])\n\n    def math_unary_deserialize_exp_test(self):\n\n        math_unary_tf = MathUnary(input_features=[\'a\'], output_features=\'exp_a\', transform_type=\'exp\')\n\n        Xres = math_unary_tf.fit_transform(self.df.a)\n\n        self.assertEqual(np.exp(self.df.a[0]), Xres[0])\n\n        math_unary_tf.serialize_to_bundle(self.tmp_dir, math_unary_tf.name)\n\n        node_name = ""{}.node"".format(math_unary_tf.name)\n        math_unary_ds_tf = MathUnary()\n        math_unary_ds_tf = math_unary_ds_tf.deserialize_from_bundle(self.tmp_dir, node_name)\n\n        with open(""{}/{}.node/model.json"".format(self.tmp_dir, math_unary_tf.name)) as json_data:\n            model = json.load(json_data)\n\n        res_a = math_unary_tf.transform(self.df[\'a\'])\n        res_b = math_unary_ds_tf.transform(self.df[\'a\'])\n\n        self.assertEqual(res_a[0], res_b[0])\n\n    def math_unary_sin_test(self):\n\n        math_unary_tf = MathUnary(input_features=[\'a\'], output_features=\'sin_a\', transform_type=\'sin\')\n\n        Xres = math_unary_tf.fit_transform(self.df.a)\n\n        self.assertEqual(np.sin(self.df.a[0]), Xres[0])\n\n        math_unary_tf.serialize_to_bundle(self.tmp_dir, math_unary_tf.name)\n\n        expected_model = {\n          ""op"": ""math_unary"",\n          ""attributes"": {\n            ""operation"": {\n              ""string"": \'sin\'\n            }\n          }\n        }\n\n        # Test model.json\n        with open(""{}/{}.node/model.json"".format(self.tmp_dir, math_unary_tf.name)) as json_data:\n            model = json.load(json_data)\n\n        self.assertEqual(expected_model[\'attributes\'][\'operation\'][\'string\'], model[\'attributes\'][\'operation\'][\'string\'])\n\n        # Test node.json\n        with open(""{}/{}.node/node.json"".format(self.tmp_dir, math_unary_tf.name)) as json_data:\n            node = json.load(json_data)\n\n        self.assertEqual(math_unary_tf.name, node[\'name\'])\n        self.assertEqual(math_unary_tf.input_features[0], node[\'shape\'][\'inputs\'][0][\'name\'])\n        self.assertEqual(math_unary_tf.output_features, node[\'shape\'][\'outputs\'][0][\'name\'])\n\n    def math_binary_test(self):\n\n        math_binary_tf = MathBinary(input_features=[\'a\', \'b\'], output_features=\'a_plus_b\', transform_type=\'add\')\n\n        Xres = math_binary_tf.fit_transform(self.df[[\'a\', \'b\']])\n\n        assert_frame_equal(pd.DataFrame(self.df.a + self.df.b, columns=[\'a\']), Xres)\n\n        math_binary_tf.serialize_to_bundle(self.tmp_dir, math_binary_tf.name)\n\n        expected_model = {\n          ""op"": ""math_binary"",\n          ""attributes"": {\n            ""operation"": {\n              ""string"": \'add\'\n            }\n          }\n        }\n\n        # Test model.json\n        with open(""{}/{}.node/model.json"".format(self.tmp_dir, math_binary_tf.name)) as json_data:\n            model = json.load(json_data)\n\n        self.assertEqual(expected_model[\'attributes\'][\'operation\'][\'string\'], model[\'attributes\'][\'operation\'][\'string\'])\n\n        # Test node.json\n        with open(""{}/{}.node/node.json"".format(self.tmp_dir, math_binary_tf.name)) as json_data:\n            node = json.load(json_data)\n\n        self.assertEqual(math_binary_tf.name, node[\'name\'])\n        self.assertEqual(math_binary_tf.input_features[0], node[\'shape\'][\'inputs\'][0][\'name\'])\n        self.assertEqual(math_binary_tf.input_features[1], node[\'shape\'][\'inputs\'][1][\'name\'])\n        self.assertEqual(math_binary_tf.output_features, node[\'shape\'][\'outputs\'][0][\'name\'])\n\n    def math_binary_deserialize_add_test(self):\n\n        math_binary_tf = MathBinary(input_features=[\'a\', \'b\'], output_features=\'a_plus_b\', transform_type=\'add\')\n\n        Xres = math_binary_tf.fit_transform(self.df[[\'a\', \'b\']])\n\n        assert_frame_equal(pd.DataFrame(self.df.a + self.df.b, columns=[\'a\']), Xres)\n\n        math_binary_tf.serialize_to_bundle(self.tmp_dir, math_binary_tf.name)\n\n        node_name = ""{}.node"".format(math_binary_tf.name)\n        math_binary_ds_tf = MathBinary()\n        math_binary_ds_tf = math_binary_ds_tf.deserialize_from_bundle(self.tmp_dir, node_name)\n\n        res_a = math_binary_tf.transform(self.df[[\'a\', \'b\']])\n        res_b = math_binary_ds_tf.transform(self.df[[\'a\', \'b\']])\n        assert_frame_equal(res_a, res_b)\n\n    def math_binary_subtract_test(self):\n\n        math_binary_tf = MathBinary(input_features=[\'a\', \'b\'], output_features=\'a_less_b\', transform_type=\'sub\')\n\n        Xres = math_binary_tf.fit_transform(self.df[[\'a\', \'b\']])\n\n        assert_frame_equal(pd.DataFrame(self.df.a - self.df.b, columns=[\'a\']), Xres)\n\n        math_binary_tf.serialize_to_bundle(self.tmp_dir, math_binary_tf.name)\n\n        expected_model = {\n          ""op"": ""math_binary"",\n          ""attributes"": {\n            ""operation"": {\n              ""string"": \'sub\'\n            }\n          }\n        }\n\n        # Test model.json\n        with open(""{}/{}.node/model.json"".format(self.tmp_dir, math_binary_tf.name)) as json_data:\n            model = json.load(json_data)\n\n        self.assertEqual(expected_model[\'attributes\'][\'operation\'][\'string\'], model[\'attributes\'][\'operation\'][\'string\'])\n\n        # Test node.json\n        with open(""{}/{}.node/node.json"".format(self.tmp_dir, math_binary_tf.name)) as json_data:\n            node = json.load(json_data)\n\n        self.assertEqual(math_binary_tf.name, node[\'name\'])\n        self.assertEqual(math_binary_tf.input_features[0], node[\'shape\'][\'inputs\'][0][\'name\'])\n        self.assertEqual(math_binary_tf.input_features[1], node[\'shape\'][\'inputs\'][1][\'name\'])\n        self.assertEqual(math_binary_tf.output_features, node[\'shape\'][\'outputs\'][0][\'name\'])\n\n    def math_binary_multiply_test(self):\n\n        math_binary_tf = MathBinary(input_features=[\'a\', \'b\'], output_features=\'a_mul_b\', transform_type=\'mul\')\n\n        Xres = math_binary_tf.fit_transform(self.df[[\'a\', \'b\']])\n\n        assert_frame_equal(pd.DataFrame(self.df.a * self.df.b, columns=[\'a\']), Xres)\n\n        math_binary_tf.serialize_to_bundle(self.tmp_dir, math_binary_tf.name)\n\n        expected_model = {\n          ""op"": ""math_binary"",\n          ""attributes"": {\n            ""operation"": {\n              ""string"": \'mul\'\n            }\n          }\n        }\n\n        # Test model.json\n        with open(""{}/{}.node/model.json"".format(self.tmp_dir, math_binary_tf.name)) as json_data:\n            model = json.load(json_data)\n\n        self.assertEqual(expected_model[\'attributes\'][\'operation\'][\'string\'], model[\'attributes\'][\'operation\'][\'string\'])\n\n        # Test node.json\n        with open(""{}/{}.node/node.json"".format(self.tmp_dir, math_binary_tf.name)) as json_data:\n            node = json.load(json_data)\n\n        self.assertEqual(math_binary_tf.name, node[\'name\'])\n        self.assertEqual(math_binary_tf.input_features[0], node[\'shape\'][\'inputs\'][0][\'name\'])\n        self.assertEqual(math_binary_tf.input_features[1], node[\'shape\'][\'inputs\'][1][\'name\'])\n        self.assertEqual(math_binary_tf.output_features, node[\'shape\'][\'outputs\'][0][\'name\'])\n\n    def math_binary_divide_test(self):\n\n        math_binary_tf = MathBinary(input_features=[\'a\', \'b\'], output_features=\'a_mul_b\', transform_type=\'div\')\n\n        Xres = math_binary_tf.fit_transform(self.df[[\'a\', \'b\']])\n\n        assert_frame_equal(pd.DataFrame(self.df.a / self.df.b, columns=[\'a\']), Xres)\n\n        math_binary_tf.serialize_to_bundle(self.tmp_dir, math_binary_tf.name)\n\n        expected_model = {\n          ""op"": ""math_binary"",\n          ""attributes"": {\n            ""operation"": {\n              ""string"": \'div\'\n            }\n          }\n        }\n\n        # Test model.json\n        with open(""{}/{}.node/model.json"".format(self.tmp_dir, math_binary_tf.name)) as json_data:\n            model = json.load(json_data)\n\n        self.assertEqual(expected_model[\'attributes\'][\'operation\'][\'string\'], model[\'attributes\'][\'operation\'][\'string\'])\n\n        # Test node.json\n        with open(""{}/{}.node/node.json"".format(self.tmp_dir, math_binary_tf.name)) as json_data:\n            node = json.load(json_data)\n\n        self.assertEqual(math_binary_tf.name, node[\'name\'])\n        self.assertEqual(math_binary_tf.input_features[0], node[\'shape\'][\'inputs\'][0][\'name\'])\n        self.assertEqual(math_binary_tf.input_features[1], node[\'shape\'][\'inputs\'][1][\'name\'])\n        self.assertEqual(math_binary_tf.output_features, node[\'shape\'][\'outputs\'][0][\'name\'])\n\n    def string_map_test(self):\n\n        df = pd.DataFrame([\'test_one\', \'test_two\', \'test_one\', \'test_one\', \'test_two\'], columns=[\'a\'])\n        string_map_tf = StringMap(input_features=[\'a\'], output_features=\'a_mapped\', labels={""test_one"":1.0, ""test_two"": 0.0})\n\n        Xres = string_map_tf.fit_transform(df)\n        self.assertEqual(1.0, Xres[0])\n        self.assertEqual(0.0, Xres[1])\n        self.assertEqual(1.0, Xres[2])\n        self.assertEqual(1.0, Xres[3])\n        self.assertEqual(0.0, Xres[4])\n\n        string_map_tf.serialize_to_bundle(self.tmp_dir, string_map_tf.name)\n\n        expected_model = {\n            ""op"": ""string_map"",\n            ""attributes"": {\n                ""labels"": {\n                    ""type"": ""list"",\n                    ""string"": [""test_one"", ""test_two""]\n                },\n                ""values"": {\n                    ""type"": ""list"",\n                    ""double"": [1.0, 0.0]\n                }\n            }\n        }\n        #\n        # Test model.json\n        with open(""{}/{}.node/model.json"".format(self.tmp_dir, string_map_tf.name)) as json_data:\n            model = json.load(json_data)\n\n        self.assertEqual(expected_model[\'attributes\'][\'labels\'][\'string\'], model[\'attributes\'][\'labels\'][\'string\'])\n        self.assertEqual(expected_model[\'attributes\'][\'values\'][\'double\'], model[\'attributes\'][\'values\'][\'double\'])\n\n        # Test node.json\n        with open(""{}/{}.node/node.json"".format(self.tmp_dir, string_map_tf.name)) as json_data:\n            node = json.load(json_data)\n\n        self.assertEqual(string_map_tf.name, node[\'name\'])\n        self.assertEqual(string_map_tf.input_features[0], node[\'shape\'][\'inputs\'][0][\'name\'])\n        self.assertEqual(string_map_tf.output_features, node[\'shape\'][\'outputs\'][0][\'name\'])\n\n    def string_map_deserializer_test(self):\n\n        df = pd.DataFrame([\'test_one\', \'test_two\', \'test_one\', \'test_one\', \'test_two\'], columns=[\'a\'])\n        string_map = StringMap(input_features=[\'a\'], output_features=\'a_mapped\', labels={""test_one"":1.0, ""test_two"": 0.0})\n        string_map.serialize_to_bundle(self.tmp_dir, string_map.name)\n\n        # Now deserialize it back\n        node_name = ""{}.node"".format(string_map.name)\n        string_map_tf = StringMap()\n        string_map_tf = string_map_tf.deserialize_from_bundle(self.tmp_dir, node_name)\n\n        # Transform some sample data\n        res_a = string_map.fit_transform(df)\n        res_b = string_map_tf.fit_transform(df)\n\n        self.assertEqual(res_a[0], res_b[0])\n        self.assertEqual(res_a[1], res_b[1])\n        self.assertEqual(res_a[2], res_b[2])\n        self.assertEqual(res_a[3], res_b[3])\n        self.assertEqual(res_a[4], res_b[4])\n        self.assertEqual(string_map.name, string_map_tf.name)\n        self.assertEqual(string_map.op, string_map_tf.op)\n        self.assertEqual(string_map.labels, string_map_tf.labels)\n'"
python/tests/sklearn/tree/__init__.py,0,b''
python/tests/sklearn/tree/tree_test.py,0,"b'import unittest\nimport os\nimport shutil\nimport json\nimport tempfile\nimport uuid\n\nfrom mleap.sklearn.tree.tree import DecisionTreeClassifier, DecisionTreeRegressor\nfrom sklearn.datasets import load_iris\n\nclass TransformerTests(unittest.TestCase):\n    def setUp(self):\n        self.tmp_dir = tempfile.mkdtemp()\n\n    def tearDown(self):\n        shutil.rmtree(self.tmp_dir)\n\n    def test_decision_tree_classifier(self):\n        X = [[0, 1], [1, 1]]\n        Y = [0, 0]\n\n        dt_classifier = DecisionTreeClassifier()\n        dt_classifier = dt_classifier.fit(X, Y)\n        dt_classifier.mlinit(input_features =\'feature\', prediction_column = \'pred\', feature_names = [\'a\'])\n        dt_classifier.serialize_to_bundle(self.tmp_dir, dt_classifier.name)\n\n        expected_model = {\n            ""attributes"": {\n                ""num_features"": {\n                    ""long"": 2\n                },\n                ""num_classes"": {\n                    ""long"": 1\n                }\n            },\n            ""op"": ""decision_tree_classifier""\n        }\n\n        # Test model.json\n        with open(""{}/{}.node/model.json"".format(self.tmp_dir, dt_classifier.name)) as json_data:\n            model = json.load(json_data)\n\n        self.assertEqual(dt_classifier.op, expected_model[\'op\'])\n        self.assertEqual(expected_model[\'attributes\'][\'num_features\'][\'long\'], model[\'attributes\'][\'num_features\'][\'long\'])\n        self.assertEqual(expected_model[\'attributes\'][\'num_classes\'][\'long\'], model[\'attributes\'][\'num_classes\'][\'long\'])\n\n        # Test node.json\n        with open(""{}/{}.node/node.json"".format(self.tmp_dir, dt_classifier.name)) as json_data:\n            node = json.load(json_data)\n\n        self.assertEqual(dt_classifier.name, node[\'name\'])\n        self.assertEqual(dt_classifier.input_features, node[\'shape\'][\'inputs\'][0][\'name\'])\n        self.assertEqual(dt_classifier.prediction_column, node[\'shape\'][\'outputs\'][0][\'name\'])\n\n    def test_decision_tree_classifier_with_iris_dataset(self):\n        iris = load_iris()\n\n        dt_classifier = DecisionTreeClassifier()\n        dt_classifier.mlinit(input_features = \'features\', prediction_column = \'species\', feature_names = iris.feature_names)\n        dt_classifier = dt_classifier.fit(iris.data, iris.target)\n        dt_classifier.serialize_to_bundle(self.tmp_dir, dt_classifier.name)\n\n        expected_model = {\n            ""attributes"": {\n                ""num_features"": {\n                    ""long"": 4\n                },\n                ""num_classes"": {\n                    ""long"": 3\n                }\n            },\n            ""op"": ""decision_tree_classifier""\n        }\n\n        # Test model.json\n        with open(""{}/{}.node/model.json"".format(self.tmp_dir, dt_classifier.name)) as json_data:\n            model = json.load(json_data)\n\n        self.assertEqual(dt_classifier.op, expected_model[\'op\'])\n        self.assertEqual(expected_model[\'attributes\'][\'num_features\'][\'long\'], model[\'attributes\'][\'num_features\'][\'long\'])\n        self.assertEqual(expected_model[\'attributes\'][\'num_classes\'][\'long\'], model[\'attributes\'][\'num_classes\'][\'long\'])\n\n        # Test node.json\n        with open(""{}/{}.node/node.json"".format(self.tmp_dir, dt_classifier.name)) as json_data:\n            node = json.load(json_data)\n\n        self.assertEqual(dt_classifier.name, node[\'name\'])\n        self.assertEqual(dt_classifier.input_features, node[\'shape\'][\'inputs\'][0][\'name\'])\n        self.assertEqual(dt_classifier.prediction_column, node[\'shape\'][\'outputs\'][0][\'name\'])\n'"
