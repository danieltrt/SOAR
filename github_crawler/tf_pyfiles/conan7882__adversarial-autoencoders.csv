file_path,api_count,code
experiment/__init__.py,0,b''
experiment/aae_mnist.py,22,"b'#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n# File: aae_mnist.py\n# Author: Qian Ge <geqian1001@gmail.com>\n\nimport sys\nimport argparse\nimport numpy as np\nimport tensorflow as tf\n\nsys.path.append(\'../\')\nfrom src.dataflow.mnist import MNISTData\nfrom src.models.aae import AAE\nfrom src.helper.trainer import Trainer\nfrom src.helper.generator import Generator\nfrom src.helper.visualizer import Visualizer\n\n\nDATA_PATH = \'/home/qge2/workspace/data/MNIST_data/\'\nSAVE_PATH = \'/home/qge2/workspace/data/out/vae/\'\n\ndef get_args():\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\'--train\', action=\'store_true\',\n                        help=\'Train the model of Fig 1 and 3 in the paper.\')\n    parser.add_argument(\'--train_supervised\', action=\'store_true\',\n                        help=\'Train the model of Fig 6 in the paper.\')\n    parser.add_argument(\'--train_semisupervised\', action=\'store_true\',\n                        help=\'Train the model of Fig 8 in the paper.\')\n    parser.add_argument(\'--label\', action=\'store_true\',\n                        help=\'Incorporate label info (Fig 3 in the paper).\')\n    parser.add_argument(\'--generate\', action=\'store_true\',\n                        help=\'Sample images from trained model.\')\n    parser.add_argument(\'--viz\', action=\'store_true\',\n                        help=\'Visualize learned model when ncode=2.\')\n    parser.add_argument(\'--supervise\', action=\'store_true\',\n                        help=\'Sampling from supervised model (Fig 6 in the paper).\')\n    parser.add_argument(\'--load\', type=int, default=99,\n                        help=\'The epoch ID of pre-trained model to be restored.\')\n    \n    parser.add_argument(\'--ncode\', type=int, default=2,\n                        help=\'Dimension of code\')\n    parser.add_argument(\'--dist_type\', type=str, default=\'gaussian\',\n                        help=\'Prior distribution to be imposed on latent z (gaussian and gmm).\')\n    parser.add_argument(\'--noise\', action=\'store_true\',\n                        help=\'Add noise to encoder input (Gaussian with std=0.6).\')\n    \n    parser.add_argument(\'--lr\', type=float, default=2e-4,\n                        help=\'Initial learning rate\')\n    parser.add_argument(\'--dropout\', type=float, default=1.0,\n                        help=\'Keep probability for dropout\')\n    parser.add_argument(\'--bsize\', type=int, default=128,\n                        help=\'Batch size\')\n    parser.add_argument(\'--maxepoch\', type=int, default=100,\n                        help=\'Max number of epochs\')\n\n    parser.add_argument(\'--encw\', type=float, default=1.,\n                        help=\'Weight of autoencoder loss\')\n    parser.add_argument(\'--genw\', type=float, default=6.,\n                        help=\'Weight of z generator loss\')\n    parser.add_argument(\'--disw\', type=float, default=6.,\n                        help=\'Weight of z discriminator loss\')\n\n    parser.add_argument(\'--clsw\', type=float, default=1.,\n                        help=\'Weight of semi-supervised loss\')\n    parser.add_argument(\'--ygenw\', type=float, default=6.,\n                        help=\'Weight of y generator loss\')\n    parser.add_argument(\'--ydisw\', type=float, default=6.,\n                        help=\'Weight of y discriminator loss\')\n    \n    return parser.parse_args()\n\n\ndef preprocess_im(im):\n    """""" normalize input image to [-1., 1.] """"""\n    im = im / 255. * 2. - 1.\n    return im\n\ndef read_train_data(batch_size, n_use_label=None, n_use_sample=None):\n    """""" Function for load training data \n\n    If n_use_label or n_use_sample is not None, samples will be\n    randomly picked to have a balanced number of examples\n\n    Args:\n        batch_size (int): batch size\n        n_use_label (int): how many labels are used for training\n        n_use_sample (int): how many samples are used for training\n\n    Retuns:\n        MNISTData\n\n    """"""\n    data = MNISTData(\'train\',\n                     data_dir=DATA_PATH,\n                     shuffle=True,\n                     pf=preprocess_im,\n                     n_use_label=n_use_label,\n                     n_use_sample=n_use_sample,\n                     batch_dict_name=[\'im\', \'label\'])\n    data.setup(epoch_val=0, batch_size=batch_size)\n    return data\n\ndef read_valid_data(batch_size):\n    """""" Function for load validation data """"""\n    data = MNISTData(\'test\',\n                     data_dir=DATA_PATH,\n                     shuffle=True,\n                     pf=preprocess_im,\n                     batch_dict_name=[\'im\', \'label\'])\n    data.setup(epoch_val=0, batch_size=batch_size)\n    return data\n\ndef semisupervised_train():\n    """""" Function for semisupervised training (Fig 8 in the paper)\n\n    Validation will be processed after each epoch of training \n    Loss of each modules will be averaged and saved in summaries\n    every 100 steps.\n    """"""\n\n    FLAGS = get_args()\n    # load dataset\n    train_data_unlabel = read_train_data(FLAGS.bsize)\n    train_data_label = read_train_data(FLAGS.bsize, n_use_sample=1280)\n    train_data = {\'unlabeled\': train_data_unlabel, \'labeled\': train_data_label}\n    valid_data = read_valid_data(FLAGS.bsize)\n\n    # create an AAE model for semisupervised training\n    train_model = AAE(\n        n_code=FLAGS.ncode, wd=0, n_class=10, add_noise=FLAGS.noise,\n        enc_weight=FLAGS.encw, gen_weight=FLAGS.genw, dis_weight=FLAGS.disw,\n        cat_dis_weight=FLAGS.ydisw, cat_gen_weight=FLAGS.ygenw, cls_weight=FLAGS.clsw)\n    train_model.create_semisupervised_train_model()\n\n    # create an separated AAE model for semisupervised validation\n    # shared weights with training model\n    cls_valid_model = AAE(n_code=FLAGS.ncode, n_class=10)\n    cls_valid_model.create_semisupervised_test_model()\n\n    # initialize a trainer for training\n    trainer = Trainer(train_model,\n                      cls_valid_model=cls_valid_model,\n                      generate_model=None,\n                      train_data=train_data,\n                      init_lr=FLAGS.lr,\n                      save_path=SAVE_PATH)\n\n    sessconfig = tf.ConfigProto()\n    sessconfig.gpu_options.allow_growth = True\n    with tf.Session(config=sessconfig) as sess:\n        writer = tf.summary.FileWriter(SAVE_PATH)\n        sess.run(tf.global_variables_initializer())\n        writer.add_graph(sess.graph)\n        for epoch_id in range(FLAGS.maxepoch):\n            trainer.train_semisupervised_epoch(\n                sess, ae_dropout=FLAGS.dropout, summary_writer=writer)\n            trainer.valid_semisupervised_epoch(\n                sess, valid_data, summary_writer=writer)\n    \ndef supervised_train():\n    """""" Function for supervised training (Fig 6 in the paper)\n\n    Validation will be processed after each epoch of training.\n    Loss of each modules will be averaged and saved in summaries\n    every 100 steps. Every 10 epochs, 10 different style for 10 digits\n    will be saved.\n    """"""\n\n    FLAGS = get_args()\n    # load dataset\n    train_data = read_train_data(FLAGS.bsize)\n    valid_data = read_valid_data(FLAGS.bsize)\n\n    # create an AAE model for supervised training\n    model = AAE(n_code=FLAGS.ncode, wd=0, n_class=10, \n                use_supervise=True, add_noise=FLAGS.noise,\n                enc_weight=FLAGS.encw, gen_weight=FLAGS.genw,\n                dis_weight=FLAGS.disw)\n    model.create_train_model()\n\n    # Create an separated AAE model for supervised validation\n    # shared weights with training model. This model is used to\n    # generate 10 different style for 10 digits for every 10 epochs.\n    valid_model = AAE(n_code=FLAGS.ncode, use_supervise=True, n_class=10)\n    valid_model.create_generate_style_model(n_sample=10)\n\n    # initialize a trainer for training\n    trainer = Trainer(model, valid_model, train_data,\n                      init_lr=FLAGS.lr, save_path=SAVE_PATH)\n    # initialize a generator for generating style images\n    generator = Generator(\n        generate_model=valid_model, save_path=SAVE_PATH, n_labels=10)\n\n    sessconfig = tf.ConfigProto()\n    sessconfig.gpu_options.allow_growth = True\n    with tf.Session(config=sessconfig) as sess:\n        writer = tf.summary.FileWriter(SAVE_PATH)\n        saver = tf.train.Saver()\n        sess.run(tf.global_variables_initializer())\n        writer.add_graph(sess.graph)\n\n        for epoch_id in range(FLAGS.maxepoch):\n            trainer.train_z_gan_epoch(\n                sess, ae_dropout=FLAGS.dropout, summary_writer=writer)\n            trainer.valid_epoch(sess, dataflow=valid_data, summary_writer=writer)\n            \n            if epoch_id % 10 == 0:\n                saver.save(sess, \'{}aae-epoch-{}\'.format(SAVE_PATH, epoch_id))\n                generator.sample_style(sess, valid_data, plot_size=10,\n                                       file_id=epoch_id, n_sample=10)\n        saver.save(sess, \'{}aae-epoch-{}\'.format(SAVE_PATH, epoch_id))\n\ndef train():\n    """""" Function for unsupervised training and incorporate\n        label info in adversarial regularization \n        (Fig 1 and 3 in the paper)\n\n    Validation will be processed after each epoch of training.\n    Loss of each modules will be averaged and saved in summaries\n    every 100 steps. Random samples and learned latent space will\n    be saved for every 10 epochs.\n    """"""\n\n    FLAGS = get_args()\n    # image size for visualization. plot_size * plot_size digits will be visualized.\n    plot_size = 20\n\n    # Use 10000 labels info to train latent space\n    n_use_label = 10000\n\n    # load data\n    train_data = read_train_data(FLAGS.bsize, n_use_label=n_use_label)\n    valid_data = read_valid_data(FLAGS.bsize)\n\n    # create an AAE model for training\n    model = AAE(n_code=FLAGS.ncode, wd=0, n_class=10, \n                use_label=FLAGS.label, add_noise=FLAGS.noise,\n                enc_weight=FLAGS.encw, gen_weight=FLAGS.genw,\n                dis_weight=FLAGS.disw)\n    model.create_train_model()\n\n    # Create an separated AAE model for validation shared weights \n    # with training model. This model is used to\n    # randomly sample model data every 10 epoches.\n    valid_model = AAE(n_code=FLAGS.ncode, n_class=10)\n    valid_model.create_generate_model(b_size=400)\n\n    # initialize a trainer for training\n    trainer = Trainer(model, valid_model, train_data,\n                      distr_type=FLAGS.dist_type, use_label=FLAGS.label,\n                      init_lr=FLAGS.lr, save_path=SAVE_PATH)\n    # Initialize a visualizer and a generator to monitor learned\n    # latent space and data generation.\n    # Latent space visualization only for code dim = 2\n    if FLAGS.ncode == 2:\n        visualizer = Visualizer(model, save_path=SAVE_PATH)\n    generator = Generator(generate_model=valid_model, save_path=SAVE_PATH,\n                          distr_type=FLAGS.dist_type, n_labels=10,\n                          use_label=FLAGS.label)\n\n    sessconfig = tf.ConfigProto()\n    sessconfig.gpu_options.allow_growth = True\n    with tf.Session(config=sessconfig) as sess:\n        writer = tf.summary.FileWriter(SAVE_PATH)\n        saver = tf.train.Saver()\n        sess.run(tf.global_variables_initializer())\n        writer.add_graph(sess.graph)\n\n        for epoch_id in range(FLAGS.maxepoch):\n            trainer.train_z_gan_epoch(sess, ae_dropout=FLAGS.dropout, summary_writer=writer)\n            trainer.valid_epoch(sess, dataflow=valid_data, summary_writer=writer)\n            \n            if epoch_id % 10 == 0:\n                saver.save(sess, \'{}aae-epoch-{}\'.format(SAVE_PATH, epoch_id))\n                generator.generate_samples(sess, plot_size=plot_size, file_id=epoch_id)\n                if FLAGS.ncode == 2:\n                    visualizer.viz_2Dlatent_variable(sess, valid_data, file_id=epoch_id)\n        saver.save(sess, \'{}aae-epoch-{}\'.format(SAVE_PATH, epoch_id))\n\ndef generate():\n    """""" function for sampling images from trained model """"""\n    FLAGS = get_args()\n    plot_size = 20\n\n    # Greate model for sampling\n    generate_model = AAE(n_code=FLAGS.ncode, n_class=10)\n    \n    if FLAGS.supervise:\n        # create samping model of Fig 6 in the paper\n        generate_model.create_generate_style_model(n_sample=10)\n    else:\n        # create samping model of Fig 1 and 3 in the paper\n        generate_model.create_generate_model(b_size=plot_size*plot_size)\n\n    # initalize the Generator for sampling\n    generator = Generator(generate_model=generate_model, save_path=SAVE_PATH,\n                          distr_type=FLAGS.dist_type, n_labels=10, use_label=FLAGS.label)\n\n    sessconfig = tf.ConfigProto()\n    sessconfig.gpu_options.allow_growth = True\n    with tf.Session(config=sessconfig) as sess:\n        saver = tf.train.Saver()\n        sess.run(tf.global_variables_initializer())\n        saver.restore(sess, \'{}aae-epoch-{}\'.format(SAVE_PATH, FLAGS.load))\n        if FLAGS.supervise:\n            generator.sample_style(sess, plot_size=10, n_sample=10)\n        else:\n            generator.generate_samples(sess, plot_size=plot_size)\n\ndef visualize():\n    """""" function for visualize latent space of trained model when ncode = 2 """"""\n    FLAGS = get_args()\n    if FLAGS.ncode != 2:\n        raise ValueError(\'Visualization only for ncode = 2!\')\n\n    plot_size = 20\n\n    # read validation set\n    valid_data = MNISTData(\'test\',\n                            data_dir=DATA_PATH,\n                            shuffle=True,\n                            pf=preprocess_im,\n                            batch_dict_name=[\'im\', \'label\'])\n    valid_data.setup(epoch_val=0, batch_size=FLAGS.bsize)\n\n    # create model for computing the latent z\n    model = AAE(n_code=FLAGS.ncode, use_label=FLAGS.label, n_class=10)\n    model.create_train_model()\n\n    # create model for sampling images \n    valid_model = AAE(n_code=FLAGS.ncode)\n    valid_model.create_generate_model(b_size=400)\n\n    # initialize Visualizer and Generator\n    visualizer = Visualizer(model, save_path=SAVE_PATH)\n    generator = Generator(generate_model=valid_model, save_path=SAVE_PATH,\n                          distr_type=FLAGS.dist_type, n_labels=10,\n                          use_label=FLAGS.label)\n\n    sessconfig = tf.ConfigProto()\n    sessconfig.gpu_options.allow_growth = True\n    with tf.Session(config=sessconfig) as sess:\n        saver = tf.train.Saver()\n        sess.run(tf.global_variables_initializer())\n        saver.restore(sess, \'{}aae-epoch-{}\'.format(SAVE_PATH, FLAGS.load))\n        # visulize the learned latent space\n        visualizer.viz_2Dlatent_variable(sess, valid_data)\n        # visulize the learned manifold\n        generator.generate_samples(sess, plot_size=plot_size, manifold=True)\n\nif __name__ == \'__main__\':\n    FLAGS = get_args()\n\n    if FLAGS.train:\n        train()\n    elif FLAGS.train_supervised:\n        supervised_train()\n    elif FLAGS.train_semisupervised:\n        semisupervised_train()\n    elif FLAGS.generate:\n        generate()\n    elif FLAGS.viz:\n        visualize()\n'"
experiment/vae_mnist.py,18,"b""#!/usr/bin/env python\r\n# -*- coding: utf-8 -*-\r\n# File: vae_mnist.py\r\n# Author: Qian Ge <geqian1001@gmail.com>\r\n\r\nimport os\r\nimport sys\r\nimport numpy as np\r\nimport tensorflow as tf\r\nimport platform\r\nimport scipy.misc\r\nimport argparse\r\nimport matplotlib.pyplot as plt\r\n\r\nsys.path.append('../')\r\nfrom src.dataflow.mnist import MNISTData\r\nfrom src.models.vae import VAE\r\nfrom src.helper.trainer import Trainer\r\nfrom src.helper.generator import Generator\r\nfrom src.helper.visualizer import Visualizer\r\nimport src.models.distribution as distribution\r\n\r\nDATA_PATH = '/home/qge2/workspace/data/MNIST_data/'\r\nSAVE_PATH = '/home/qge2/workspace/data/out/vae/vae/'\r\n\r\n\r\ndef get_args():\r\n    parser = argparse.ArgumentParser()\r\n    parser.add_argument('--train', action='store_true',\r\n                        help='Train the model')\r\n    parser.add_argument('--generate', action='store_true',\r\n                        help='generate')\r\n    parser.add_argument('--viz', action='store_true',\r\n                        help='visualize')\r\n    parser.add_argument('--test', action='store_true',\r\n                        help='test')\r\n    parser.add_argument('--load', type=int, default=99,\r\n                        help='Load step of pre-trained')\r\n    parser.add_argument('--lr', type=float, default=1e-3,\r\n                        help='Init learning rate')\r\n    parser.add_argument('--ncode', type=int, default=2,\r\n                        help='number of code')\r\n\r\n    parser.add_argument('--bsize', type=int, default=128,\r\n                        help='Init learning rate')\r\n    parser.add_argument('--maxepoch', type=int, default=100,\r\n                        help='Max iteration')\r\n\r\n    \r\n    return parser.parse_args()\r\n\r\n\r\ndef preprocess_im(im):\r\n    im = im / 255.\r\n    return im\r\n\r\ndef train():\r\n    FLAGS = get_args()\r\n    train_data = MNISTData('train',\r\n                            data_dir=DATA_PATH,\r\n                            shuffle=True,\r\n                            pf=preprocess_im,\r\n                            batch_dict_name=['im', 'label'])\r\n    train_data.setup(epoch_val=0, batch_size=FLAGS.bsize)\r\n    valid_data = MNISTData('test',\r\n                            data_dir=DATA_PATH,\r\n                            shuffle=True,\r\n                            pf=preprocess_im,\r\n                            batch_dict_name=['im', 'label'])\r\n    valid_data.setup(epoch_val=0, batch_size=FLAGS.bsize)\r\n\r\n    with tf.variable_scope('VAE') as scope:\r\n        model = VAE(n_code=FLAGS.ncode, wd=0)\r\n        model.create_train_model()\r\n\r\n    with tf.variable_scope('VAE') as scope:\r\n        scope.reuse_variables()\r\n        valid_model = VAE(n_code=FLAGS.ncode, wd=0)\r\n        valid_model.create_generate_model(b_size=400)\r\n\r\n    trainer = Trainer(model, valid_model, train_data, init_lr=FLAGS.lr, save_path=SAVE_PATH)\r\n    if FLAGS.ncode == 2:\r\n        z = distribution.interpolate(plot_size=20)\r\n        z = np.reshape(z, (400, 2))\r\n        visualizer = Visualizer(model, save_path=SAVE_PATH)\r\n    else:\r\n        z = None\r\n    generator = Generator(generate_model=valid_model, save_path=SAVE_PATH)\r\n\r\n    sessconfig = tf.ConfigProto()\r\n    sessconfig.gpu_options.allow_growth = True\r\n    with tf.Session(config=sessconfig) as sess:\r\n        writer = tf.summary.FileWriter(SAVE_PATH)\r\n        saver = tf.train.Saver()\r\n        sess.run(tf.global_variables_initializer())\r\n        writer.add_graph(sess.graph)\r\n\r\n        for epoch_id in range(FLAGS.maxepoch):\r\n            trainer.train_epoch(sess, summary_writer=writer)\r\n            trainer.valid_epoch(sess, summary_writer=writer)\r\n            if epoch_id % 10 == 0:\r\n                saver.save(sess, '{}vae-epoch-{}'.format(SAVE_PATH, epoch_id))\r\n                if FLAGS.ncode == 2:\r\n                    generator.generate_samples(sess, plot_size=20, z=z, file_id=epoch_id)\r\n                    visualizer.viz_2Dlatent_variable(sess, valid_data, file_id=epoch_id)\r\n\r\ndef generate():\r\n    FLAGS = get_args()\r\n    plot_size = 20\r\n\r\n    with tf.variable_scope('VAE') as scope:\r\n        # scope.reuse_variables()\r\n        generate_model = VAE(n_code=FLAGS.ncode, wd=0)\r\n        generate_model.create_generate_model(b_size=plot_size*plot_size)\r\n\r\n    generator = Generator(generate_model=generate_model, save_path=SAVE_PATH)\r\n\r\n    sessconfig = tf.ConfigProto()\r\n    sessconfig.gpu_options.allow_growth = True\r\n    with tf.Session(config=sessconfig) as sess:\r\n        saver = tf.train.Saver()\r\n        sess.run(tf.global_variables_initializer())\r\n        saver.restore(sess, '{}vae-epoch-{}'.format(SAVE_PATH, FLAGS.load))\r\n        generator.generate_samples(sess, plot_size=plot_size, z=None)\r\n\r\ndef visualize():\r\n    FLAGS = get_args()\r\n    plot_size = 20\r\n\r\n    valid_data = MNISTData('test',\r\n                            data_dir=DATA_PATH,\r\n                            shuffle=True,\r\n                            pf=preprocess_im,\r\n                            batch_dict_name=['im', 'label'])\r\n    valid_data.setup(epoch_val=0, batch_size=FLAGS.bsize)\r\n\r\n    with tf.variable_scope('VAE') as scope:\r\n        model = VAE(n_code=FLAGS.ncode, wd=0)\r\n        model.create_train_model()\r\n\r\n    with tf.variable_scope('VAE') as scope:\r\n        scope.reuse_variables()\r\n        valid_model = VAE(n_code=FLAGS.ncode, wd=0)\r\n        valid_model.create_generate_model(b_size=400)\r\n\r\n    visualizer = Visualizer(model, save_path=SAVE_PATH)\r\n    generator = Generator(generate_model=valid_model, save_path=SAVE_PATH)\r\n\r\n    z = distribution.interpolate(plot_size=plot_size)\r\n    z = np.reshape(z, (plot_size*plot_size, 2))\r\n\r\n    sessconfig = tf.ConfigProto()\r\n    sessconfig.gpu_options.allow_growth = True\r\n    with tf.Session(config=sessconfig) as sess:\r\n        saver = tf.train.Saver()\r\n        sess.run(tf.global_variables_initializer())\r\n        saver.restore(sess, '{}vae-epoch-{}'.format(SAVE_PATH, FLAGS.load))\r\n        visualizer.viz_2Dlatent_variable(sess, valid_data)\r\n        generator.generate_samples(sess, plot_size=plot_size, z=z)\r\n\r\ndef test():\r\n    valid_data = MNISTData('test',\r\n                            data_dir=DATA_PATH,\r\n                            shuffle=True,\r\n                            pf=preprocess_im,\r\n                            batch_dict_name=['im', 'label'])\r\n    batch_data = valid_data.next_batch_dict()\r\n    plt.figure()\r\n    plt.imshow(np.squeeze(batch_data['im'][0]))\r\n    plt.show()\r\n    print(batch_data['label'])\r\n\r\nif __name__ == '__main__':\r\n    FLAGS = get_args()\r\n\r\n    if FLAGS.train:\r\n        train()\r\n    elif FLAGS.generate:\r\n        generate()\r\n    elif FLAGS.viz:\r\n        visualize()\r\n    elif FLAGS.test:\r\n        test()\r\n\r\n"""
src/__init__.py,0,b''
src/dataflow/__init__.py,0,b''
src/dataflow/mnist.py,0,"b'#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n# File: mnist.py\n# Author: Qian Ge <geqian1001@gmail.com>\n\n# from tensorflow.examples.tutorials.mnist import input_data\nimport os\nimport gzip\nimport struct\nimport numpy as np \nimport tensorflow as tf\n\nfrom src.utils.dataflow import get_rng\n\ndef identity(im):\n    return im\n\nclass MNISTData(object):\n    def __init__(self, name, data_dir=\'\', n_use_label=None, n_use_sample=None,\n                 batch_dict_name=None, shuffle=True, pf=identity):\n        assert os.path.isdir(data_dir)\n        self._data_dir = data_dir\n\n        self._shuffle = shuffle\n        self._pf = pf\n\n        if not isinstance(batch_dict_name, list):\n            batch_dict_name = [batch_dict_name]\n        self._batch_dict_name = batch_dict_name\n\n        assert name in [\'train\', \'test\', \'val\']\n        self.setup(epoch_val=0, batch_size=1)\n\n        self._load_files(name, n_use_label, n_use_sample)\n        self._image_id = 0\n\n    def next_batch_dict(self):\n        batch_data = self.next_batch()\n        data_dict = {key: data for key, data in zip(self._batch_dict_name, batch_data)}\n        return data_dict\n\n    def _load_files(self, name, n_use_label, n_use_sample):\n        if name == \'train\':\n            image_name = \'train-images-idx3-ubyte.gz\'\n            label_name = \'train-labels-idx1-ubyte.gz\'\n        else:\n            image_name = \'t10k-images-idx3-ubyte.gz\'\n            label_name = \'t10k-labels-idx1-ubyte.gz\'\n\n        image_path = os.path.join(self._data_dir, image_name)\n        label_path = os.path.join(self._data_dir, label_name)\n\n        with gzip.open(label_path) as f:\n            magic = struct.unpack(\'>I\', f.read(4))\n            if magic[0] != 2049:\n                raise Exception(\'Invalid file: unexpected magic number.\')\n            n_label = struct.unpack(\'>I\', f.read(4))\n            label_list = np.fromstring(f.read(n_label[0]), dtype = np.uint8)\n\n        with gzip.open(image_path) as f:\n            magic = struct.unpack(\'>I\', f.read(4))\n            if magic[0] != 2051:\n                raise Exception(\'Invalid file: unexpected magic number.\')\n            n_im, rows, cols = struct.unpack(\'>III\', f.read(12))\n            image_list = np.fromstring(f.read(n_im * rows * cols), dtype = np.uint8)\n            image_list = np.reshape(image_list, (n_im, rows, cols, 1))\n            # image_list = image_list.astype(np.float32)\n            im_list = []\n            if n_use_sample is not None and n_use_sample < len(label_list):\n                remain_sample = n_use_sample // 10 * 10\n                left_sample = n_use_sample - remain_sample\n                keep_sign = [0 for i in range(10)]\n                data_idx = 0\n                new_label_list = []\n                for idx, im in enumerate(image_list):\n\n                    if remain_sample > 0:\n                        if keep_sign[label_list[idx]] < (n_use_sample // 10):\n                            keep_sign[label_list[idx]] += 1\n                            im_list.append(self._pf(im))\n                            new_label_list.append(label_list[idx])\n                            remain_sample -= 1\n                    else:\n                        break\n                im_list.extend(image_list[idx:idx + left_sample])\n                new_label_list.extend(label_list[idx:idx + left_sample])\n                label_list = new_label_list\n\n            else:\n                for im in image_list:\n                    im_list.append(self._pf(im))\n\n        self.im_list = np.array(im_list)\n        self.label_list = np.array(label_list)\n\n        if n_use_label is not None and n_use_label < self.size():\n            remain_sample = n_use_label // 10 * 10\n            left_sample = n_use_label - remain_sample\n            keep_sign = [0 for i in range(10)]\n            data_idx = 0\n            while remain_sample > 0:\n                if keep_sign[self.label_list[data_idx]] < (n_use_label // 10):\n                    keep_sign[self.label_list[data_idx]] += 1\n                    remain_sample -= 1\n                else:\n                    self.label_list[data_idx] = 10\n                data_idx += 1\n\n            self.label_list[data_idx + left_sample:] = 10\n        self._suffle_files()\n\n    def _suffle_files(self):\n        if self._shuffle:\n            idxs = np.arange(self.size())\n\n            self.rng.shuffle(idxs)\n            self.im_list = self.im_list[idxs]\n            self.label_list = self.label_list[idxs]\n\n    def size(self):\n        return self.im_list.shape[0]\n\n    def next_batch(self):\n        assert self._batch_size <= self.size(), \\\n          ""batch_size {} cannot be larger than data size {}"".\\\n           format(self._batch_size, self.size())\n        start = self._image_id\n        self._image_id += self._batch_size\n        end = self._image_id\n        batch_files = self.im_list[start:end]\n        batch_label = self.label_list[start:end]\n\n        if self._image_id + self._batch_size > self.size():\n            self._epochs_completed += 1\n            self._image_id = 0\n            self._suffle_files()\n        return [batch_files, batch_label]\n\n    def setup(self, epoch_val, batch_size, **kwargs):\n        self.reset_epochs_completed(epoch_val)\n        self.set_batch_size(batch_size)\n        self.reset_state()\n        try:\n            self._suffle_files()\n        except AttributeError:\n            pass\n\n    def reset_epoch(self):\n        self._epochs_completed = 0\n\n    @property\n    def batch_size(self):\n        return self._batch_size\n\n    @property\n    def epochs_completed(self):\n        return self._epochs_completed\n\n    def set_batch_size(self, batch_size):\n        self._batch_size = batch_size\n\n    def reset_epochs_completed(self, epoch_val):\n        self._epochs_completed  = epoch_val\n\n    def reset_state(self):\n        self.rng = get_rng(self)\n        \n\n\n'"
src/helper/__init__.py,0,b''
src/helper/generator.py,0,"b""#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n# File: generator.py\n# Author: Qian Ge <geqian1001@gmail.com>\n\nimport os\nimport math\nimport numpy as np\nimport tensorflow as tf\n\nimport src.utils.viz as viz\nimport src.models.distribution as distribution\n\n\nclass Generator(object):\n    def __init__(self, generate_model, distr_type='gaussian',\n                 n_labels=None, use_label=False, save_path=None):\n\n        self._save_path = save_path\n        self._g_model = generate_model\n        self._generate_op = generate_model.layers['generate']\n\n        self._dist = distr_type\n        self._n_labels = n_labels\n        self._use_label = use_label\n        if self._use_label:\n            assert self._n_labels is not None\n\n    def sample_style(self, sess, plot_size, n_sample=10, file_id=None):\n        # epochs_completed, batch_size = dataflow.epochs_completed, dataflow.batch_size \n        # dataflow.setup(epoch_val=0, batch_size=n_sample)\n\n        # batch_data = dataflow.next_batch_dict()\n        # latent_var = sess.run(\n        #     self._latent_op, \n        #     feed_dict={self._g_model.encoder_in: batch_data['im'],\n        #                self._g_model.keep_prob: 1.})\n\n        # label = []\n        # for i in range(n_labels):\n        #     label.extend([i for k in range(n_sample)])\n        # code = np.tile(latent_var, [n_labels, 1]) # [n_class*10, n_code]\n        # print(batch_data['label'])\n        gen_im = sess.run(self._g_model.layers['generate'],\n                          feed_dict={\n                                     # self._g_model.image: batch_data['im'],\n                                     # self._g_model.label: label,\n                                     # self._g_model.keep_prob: 1.\n                                     })\n\n        if self._save_path:\n            if file_id is not None:\n                im_save_path = os.path.join(\n                    self._save_path, 'sample_style_{}.png'.format(file_id))\n            else:\n                im_save_path = os.path.join(\n                    self._save_path, 'sample_style.png')\n\n            n_sample = len(gen_im)\n            plot_size = int(min(plot_size, math.sqrt(n_sample)))\n            viz.viz_batch_im(batch_im=gen_im, grid_size=[plot_size, plot_size],\n                             save_path=im_save_path, gap=0, gap_color=0,\n                             shuffle=False)\n\n#         dataflow.setup(epoch_val=epochs_completed, batch_size=batch_size)\n\n    def generate_samples(self, sess, plot_size, manifold=False, file_id=None):\n        # if z is None:\n        #     gen_im = sess.run(self._generate_op)\n        # else:\n        n_samples = plot_size * plot_size\n\n        label_indices = None\n        if self._use_label:\n            cur_r = 0\n            label_indices = []\n            cur_label = -1\n            while cur_r < plot_size:\n                cur_label = cur_label + 1 if cur_label < self._n_labels - 1 else 0\n                row_label = np.ones(plot_size) * cur_label\n                label_indices.extend(row_label)\n                cur_r += 1\n\n        if manifold:\n            if self._dist ==  'gaussian':\n                random_code = distribution.interpolate(\n                    plot_size=plot_size, interpolate_range=[-3, 3, -3, 3])\n                self.viz_samples(sess, random_code, plot_size, file_id=file_id)\n            else:\n                for mode_id in range(self._n_labels):\n                    random_code = distribution.interpolate_gm(\n                        plot_size=plot_size, interpolate_range=[-1., 1., -0.2, 0.2],\n                        mode_id=mode_id, n_mode=self._n_labels)\n                    self.viz_samples(sess, random_code, plot_size,\n                                     file_id='{}_{}'.format(file_id, mode_id))\n        else:\n            if self._dist ==  'gaussian':\n                random_code = distribution.diagonal_gaussian(\n                    n_samples, self._g_model.n_code, mean=0, var=1.0)\n            else:\n                random_code = distribution.gaussian_mixture(\n                    n_samples, n_dim=self._g_model.n_code, n_labels=self._n_labels,\n                    x_var=0.5, y_var=0.1, label_indices=label_indices)\n\n            self.viz_samples(sess, random_code, plot_size, file_id=file_id)\n\n    def viz_samples(self, sess, random_code, plot_size, file_id=None):\n        gen_im = sess.run(self._generate_op, feed_dict={self._g_model.z: random_code})\n        if self._save_path:\n            if file_id is not None:\n                im_save_path = os.path.join(\n                    self._save_path, 'generate_im_{}.png'.format(file_id))\n            else:\n                im_save_path = os.path.join(\n                    self._save_path, 'generate_im.png')\n\n            n_sample = len(gen_im)\n            plot_size = int(min(plot_size, math.sqrt(n_sample)))\n            viz.viz_batch_im(batch_im=gen_im, grid_size=[plot_size, plot_size],\n                            save_path=im_save_path, gap=0, gap_color=0,\n                            shuffle=False)\n\n\n\n            \n\n"""
src/helper/trainer.py,1,"b""#!/usr/bin/env python\r\n# -*- coding: utf-8 -*-\r\n# File: trainer.py\r\n# Author: Qian Ge <geqian1001@gmail.com>\r\n\r\nimport os\r\n# import scipy.misc\r\nimport numpy as np\r\nimport tensorflow as tf\r\n\r\nimport src.utils.viz as viz\r\nimport src.models.distribution as distribution\r\n\r\n\r\ndef display(global_step,\r\n            step,\r\n            scaler_sum_list,\r\n            name_list,\r\n            collection,\r\n            summary_val=None,\r\n            summary_writer=None,\r\n            ):\r\n    print('[step: {}]'.format(global_step), end='')\r\n    for val, name in zip(scaler_sum_list, name_list):\r\n        print(' {}: {:.4f}'.format(name, val * 1. / step), end='')\r\n    print('')\r\n    if summary_writer is not None:\r\n        s = tf.Summary()\r\n        for val, name in zip(scaler_sum_list, name_list):\r\n            s.value.add(tag='{}/{}'.format(collection, name),\r\n                        simple_value=val * 1. / step)\r\n        summary_writer.add_summary(s, global_step)\r\n        if summary_val is not None:\r\n            summary_writer.add_summary(summary_val, global_step)\r\n\r\nclass Trainer(object):\r\n    def __init__(self, train_model, generate_model, train_data, cls_valid_model=None, distr_type='gaussian',\r\n                 use_label=False, init_lr=1e-3, save_path=None):\r\n\r\n        self._save_path = save_path\r\n\r\n        self._t_model = train_model\r\n        \r\n        self._train_data = train_data\r\n        self._lr = init_lr\r\n        self._use_label = use_label\r\n        self._dist = distr_type\r\n\r\n        self._train_op = train_model.get_reconstruction_train_op()\r\n        self._loss_op = train_model.get_reconstruction_loss()\r\n        self._train_summary_op = train_model.get_train_summary()\r\n        self._valid_summary_op = train_model.get_valid_summary()\r\n\r\n        try:\r\n            self._train_d_op = train_model.get_latent_discrimator_train_op()\r\n            self._train_g_op = train_model.get_latent_generator_train_op()\r\n            self._d_loss_op = train_model.latent_d_loss\r\n            self._g_loss_op = train_model.latent_g_loss\r\n        except (AttributeError, KeyError):\r\n            pass\r\n\r\n        try:\r\n            self._train_cat_d_op = train_model.get_cat_discrimator_train_op()\r\n            self._train_cat_g_op = train_model.get_cat_generator_train_op()\r\n            self._cat_d_loss_op = train_model.cat_d_loss\r\n            self._cat_g_loss_op = train_model.cat_g_loss\r\n        except (AttributeError, KeyError):\r\n            pass\r\n\r\n        try:\r\n            self._cls_train_op = train_model.get_cls_train_op()\r\n            self._cls_loss_op = train_model.get_cls_loss()\r\n            self._cls_accuracy_op = train_model.get_cls_accuracy()\r\n        except (AttributeError, KeyError):\r\n            pass\r\n\r\n        if generate_model is not None:\r\n            self._g_model = generate_model\r\n            self._generate_op = generate_model.layers['generate']\r\n            self._generate_summary_op = generate_model.get_generate_summary()\r\n\r\n        if cls_valid_model is not None:\r\n            self._cls_v_model = cls_valid_model\r\n            self._cls_valid_loss_op = cls_valid_model.get_cls_loss()\r\n            self._cls_v_accuracy_op = cls_valid_model.get_cls_accuracy()\r\n\r\n        self.global_step = 0\r\n        self.epoch_id = 0\r\n\r\n    def valid_semisupervised_epoch(self, sess, dataflow, summary_writer=None):\r\n        dataflow.setup(epoch_val=0, batch_size=dataflow.batch_size)\r\n        display_name_list = ['cls_loss', 'cls_accuracy']\r\n        cur_summary = None\r\n        step = 0\r\n        cls_loss_sum = 0\r\n        cls_accuracy_sum = 0\r\n        while dataflow.epochs_completed < 1:\r\n            step += 1\r\n            batch_data = dataflow.next_batch_dict()\r\n            im = batch_data['im']\r\n            label = batch_data['label']\r\n\r\n            cls_loss, cls_accuracy = sess.run(\r\n                [self._cls_valid_loss_op, self._cls_v_accuracy_op],\r\n                feed_dict={self._cls_v_model.image: im,\r\n                           self._cls_v_model.label: label})\r\n            cls_loss_sum += cls_loss\r\n            cls_accuracy_sum += cls_accuracy\r\n\r\n        print('[Valid]: ', end='')\r\n        display(self.global_step,\r\n                step,\r\n                [cls_loss_sum, cls_accuracy_sum],\r\n                display_name_list,\r\n                'valid',\r\n                summary_val=cur_summary,\r\n                summary_writer=summary_writer)\r\n\r\n\r\n    def train_semisupervised_epoch(self, sess, ae_dropout=1.0, summary_writer=None):\r\n        label_data = self._train_data['labeled']\r\n        unlabel_data = self._train_data['unlabeled']\r\n        display_name_list = ['loss', 'z_d_loss', 'z_g_loss', 'y_d_loss', 'y_g_loss',\r\n                             'cls_loss', 'cls_accuracy']\r\n        cur_summary = None\r\n        cur_epoch = unlabel_data.epochs_completed\r\n\r\n        self.epoch_id += 1\r\n\r\n        if self.epoch_id == 150:\r\n            self._lr = self._lr / 10\r\n        if self.epoch_id == 200:\r\n            self._lr = self._lr / 10\r\n\r\n        step = 0\r\n        loss_sum = 0\r\n        z_d_loss_sum = 0\r\n        z_g_loss_sum = 0\r\n        y_d_loss_sum = 0\r\n        y_g_loss_sum = 0\r\n        cls_loss_sum = 0\r\n        cls_accuracy_sum = 0\r\n        while cur_epoch == unlabel_data.epochs_completed:\r\n            self.global_step += 1\r\n            step += 1\r\n\r\n            batch_data = unlabel_data.next_batch_dict()\r\n            im = batch_data['im']\r\n            label = batch_data['label']\r\n\r\n            z_real_sample = distribution.diagonal_gaussian(\r\n                len(im), self._t_model.n_code, mean=0, var=1.0)\r\n\r\n            y_real_sample = np.random.choice(self._t_model.n_class, len(im))\r\n            # a = np.array([1, 0, len(im)])\r\n            # b = np.zeros((len(im), self._t_model.n_class))\r\n            # b[np.arange(len(im)), y_real_sample] = 1\r\n            # y_real_sample = b\r\n            # print(y_real_sample)\r\n\r\n            # train autoencoder\r\n            _, loss, cur_summary = sess.run(\r\n                [self._train_op, self._loss_op, self._train_summary_op], \r\n                feed_dict={self._t_model.image: im,\r\n                           self._t_model.lr: self._lr,\r\n                           self._t_model.keep_prob: ae_dropout,\r\n                           self._t_model.label: label,\r\n                           self._t_model.real_distribution: z_real_sample,\r\n                           self._t_model.real_y: y_real_sample})\r\n\r\n            # z discriminator\r\n            _, z_d_loss = sess.run(\r\n                [self._train_d_op, self._d_loss_op], \r\n                feed_dict={self._t_model.image: im,\r\n                           # self._t_model.label: label,\r\n                           self._t_model.lr: self._lr,\r\n                           self._t_model.keep_prob: 1.,\r\n                           self._t_model.real_distribution: z_real_sample})\r\n\r\n            # z generator\r\n            _, z_g_loss = sess.run(\r\n                [self._train_g_op, self._g_loss_op], \r\n                feed_dict={self._t_model.image: im,\r\n                           # self._t_model.label: label,\r\n                           self._t_model.lr: self._lr,\r\n                           self._t_model.keep_prob: 1.})\r\n\r\n            # y discriminator\r\n            _, y_d_loss = sess.run(\r\n                [self._train_cat_d_op, self._cat_d_loss_op], \r\n                feed_dict={self._t_model.image: im,\r\n                           # self._t_model.label: label,\r\n                           self._t_model.lr: self._lr,\r\n                           self._t_model.keep_prob: 1.,\r\n                           self._t_model.real_y: y_real_sample})\r\n\r\n            # y generator\r\n            _, y_g_loss = sess.run(\r\n                [self._train_cat_g_op, self._cat_g_loss_op], \r\n                feed_dict={self._t_model.image: im,\r\n                           # self._t_model.label: label,\r\n                           self._t_model.lr: self._lr,\r\n                           self._t_model.keep_prob: 1.})\r\n\r\n            batch_data = label_data.next_batch_dict()\r\n            im = batch_data['im']\r\n            label = batch_data['label']\r\n            # semisupervise\r\n            if self.global_step % 10 == 0:\r\n                _, cls_loss, cls_accuracy = sess.run(\r\n                    [self._cls_train_op, self._cls_loss_op, self._cls_accuracy_op], \r\n                    feed_dict={self._t_model.image: im,\r\n                               self._t_model.label: label,\r\n                               self._t_model.lr: self._lr,\r\n                               self._t_model.keep_prob: 1.})\r\n                cls_loss_sum += cls_loss\r\n                cls_accuracy_sum += cls_accuracy\r\n            \r\n\r\n            loss_sum += loss\r\n            z_d_loss_sum += z_d_loss\r\n            z_g_loss_sum += z_g_loss\r\n            y_d_loss_sum += y_d_loss\r\n            y_g_loss_sum += y_g_loss\r\n            \r\n            \r\n            if step % 100 == 0:\r\n                display(self.global_step,\r\n                    step,\r\n                    [loss_sum, z_d_loss_sum, z_g_loss_sum, y_d_loss_sum, y_g_loss_sum,\r\n                     cls_loss_sum * 10, cls_accuracy_sum * 10],\r\n                    display_name_list,\r\n                    'train',\r\n                    summary_val=cur_summary,\r\n                    summary_writer=summary_writer)\r\n\r\n        print('==== epoch: {}, lr:{} ===='.format(cur_epoch, self._lr))\r\n        display(self.global_step,\r\n                step,\r\n                [loss_sum, z_d_loss_sum, z_g_loss_sum, y_d_loss_sum, y_g_loss_sum],\r\n                display_name_list,\r\n                'train',\r\n                summary_val=cur_summary,\r\n                summary_writer=summary_writer)\r\n\r\n\r\n    def train_z_gan_epoch(self, sess, ae_dropout=1.0, summary_writer=None):\r\n        self._t_model.set_is_training(True)\r\n        display_name_list = ['loss', 'd_loss', 'g_loss']\r\n        cur_summary = None\r\n        # if self.epoch_id == 50:\r\n        #     self._lr = self._lr / 10\r\n        # if self.epoch_id == 200:\r\n        #     self._lr = self._lr / 10\r\n        if self.epoch_id == 100:\r\n            self._lr = self._lr / 10\r\n        if self.epoch_id == 300:\r\n            self._lr = self._lr / 10\r\n\r\n        cur_epoch = self._train_data.epochs_completed\r\n\r\n        step = 0\r\n        loss_sum = 0\r\n        d_loss_sum = 0\r\n        g_loss_sum = 0\r\n        self.epoch_id += 1\r\n        while cur_epoch == self._train_data.epochs_completed:\r\n            self.global_step += 1\r\n            step += 1\r\n\r\n            # batch_data = self._train_data.next_batch_dict()\r\n            # im = batch_data['im']\r\n            # label = batch_data['label']\r\n\r\n            # _, d_loss = sess.run(\r\n            #     [self._train_d_op, self._d_loss_op], \r\n            #     feed_dict={self._t_model.image: im,\r\n            #                self._t_model.lr: self._lr,\r\n            #                self._t_model.keep_prob: 1.})\r\n\r\n            batch_data = self._train_data.next_batch_dict()\r\n            im = batch_data['im']\r\n            label = batch_data['label']\r\n\r\n            if self._use_label:\r\n                label_indices = label\r\n            else:\r\n                label_indices = None\r\n\r\n            if self._dist == 'gmm':\r\n                real_sample = distribution.gaussian_mixture(\r\n                    len(im), n_dim=self._t_model.n_code, n_labels=10,\r\n                    x_var=0.5, y_var=0.1, label_indices=label_indices)\r\n            else:\r\n                real_sample = distribution.diagonal_gaussian(\r\n                    len(im), self._t_model.n_code, mean=0, var=1.0)\r\n\r\n            # train autoencoder\r\n            _, loss, cur_summary = sess.run(\r\n                [self._train_op, self._loss_op, self._train_summary_op], \r\n                feed_dict={self._t_model.image: im,\r\n                           self._t_model.lr: self._lr,\r\n                           self._t_model.keep_prob: ae_dropout,\r\n                           self._t_model.label: label,\r\n                           self._t_model.real_distribution: real_sample})\r\n\r\n            # train discriminator\r\n            \r\n            _, d_loss = sess.run(\r\n                [self._train_d_op, self._d_loss_op], \r\n                feed_dict={self._t_model.image: im,\r\n                           self._t_model.label: label,\r\n                           self._t_model.lr: self._lr,\r\n                           self._t_model.keep_prob: 1.,\r\n                           self._t_model.real_distribution: real_sample})\r\n\r\n            # train generator\r\n            _, g_loss = sess.run(\r\n                [self._train_g_op, self._g_loss_op], \r\n                feed_dict={self._t_model.image: im,\r\n                           self._t_model.label: label,\r\n                           self._t_model.lr: self._lr,\r\n                           self._t_model.keep_prob: 1.})\r\n\r\n            # batch_data = self._train_data.next_batch_dict()\r\n            # im = batch_data['im']\r\n            # label = batch_data['label']\r\n            loss_sum += loss\r\n            d_loss_sum += d_loss\r\n            g_loss_sum += g_loss\r\n\r\n            if step % 100 == 0:\r\n                display(self.global_step,\r\n                    step,\r\n                    [loss_sum, d_loss_sum, g_loss_sum],\r\n                    display_name_list,\r\n                    'train',\r\n                    summary_val=cur_summary,\r\n                    summary_writer=summary_writer)\r\n\r\n        print('==== epoch: {}, lr:{} ===='.format(cur_epoch, self._lr))\r\n        display(self.global_step,\r\n                step,\r\n                [loss_sum, d_loss_sum, g_loss_sum],\r\n                display_name_list,\r\n                'train',\r\n                summary_val=cur_summary,\r\n                summary_writer=summary_writer)\r\n\r\n    def train_epoch(self, sess, summary_writer=None):\r\n        self._t_model.set_is_training(True)\r\n        display_name_list = ['loss']\r\n        cur_summary = None\r\n\r\n        cur_epoch = self._train_data.epochs_completed\r\n\r\n        step = 0\r\n        loss_sum = 0\r\n        self.epoch_id += 1\r\n        while cur_epoch == self._train_data.epochs_completed:\r\n            self.global_step += 1\r\n            step += 1\r\n\r\n            batch_data = self._train_data.next_batch_dict()\r\n            im = batch_data['im']\r\n            label = batch_data['label']\r\n            _, loss, cur_summary = sess.run(\r\n                [self._train_op, self._loss_op, self._train_summary_op], \r\n                feed_dict={self._t_model.image: im,\r\n                           self._t_model.lr: self._lr,\r\n                           self._t_model.keep_prob: 0.9})\r\n\r\n            loss_sum += loss\r\n\r\n            if step % 100 == 0:\r\n                display(self.global_step,\r\n                    step,\r\n                    [loss_sum],\r\n                    display_name_list,\r\n                    'train',\r\n                    summary_val=cur_summary,\r\n                    summary_writer=summary_writer)\r\n\r\n        print('==== epoch: {}, lr:{} ===='.format(cur_epoch, self._lr))\r\n        display(self.global_step,\r\n                step,\r\n                [loss_sum],\r\n                display_name_list,\r\n                'train',\r\n                summary_val=cur_summary,\r\n                summary_writer=summary_writer)\r\n\r\n    def valid_epoch(self, sess, dataflow=None, moniter_generation=False, summary_writer=None):\r\n        # self._g_model.set_is_training(True)\r\n        # display_name_list = ['loss']\r\n        # cur_summary = None\r\n        \r\n        dataflow.setup(epoch_val=0, batch_size=dataflow.batch_size)\r\n        display_name_list = ['loss']\r\n\r\n        step = 0\r\n        loss_sum = 0\r\n        while dataflow.epochs_completed == 0:\r\n            step += 1\r\n\r\n            batch_data = dataflow.next_batch_dict()\r\n            im = batch_data['im']\r\n            label = batch_data['label']\r\n            loss, valid_summary = sess.run(\r\n                [self._loss_op, self._valid_summary_op],\r\n                feed_dict={self._t_model.encoder_in: im,\r\n                           self._t_model.image: im,\r\n                           self._t_model.keep_prob: 1.0,\r\n                           self._t_model.label: label,\r\n                           })\r\n            loss_sum += loss\r\n\r\n        print('[Valid]: ', end='')\r\n        display(self.global_step,\r\n                step,\r\n                [loss_sum],\r\n                display_name_list,\r\n                'valid',\r\n                summary_val=None,\r\n                summary_writer=summary_writer)\r\n        dataflow.setup(epoch_val=0, batch_size=dataflow.batch_size)\r\n\r\n        gen_im = sess.run(self._generate_op)\r\n        if moniter_generation and self._save_path:\r\n            im_save_path = os.path.join(self._save_path,\r\n                                        'generate_step_{}.png'.format(self.global_step))\r\n            viz.viz_batch_im(batch_im=gen_im, grid_size=[10, 10],\r\n                             save_path=im_save_path, gap=0, gap_color=0,\r\n                             shuffle=False)\r\n        if summary_writer:\r\n            cur_summary = sess.run(self._generate_summary_op)\r\n            summary_writer.add_summary(cur_summary, self.global_step)\r\n            summary_writer.add_summary(valid_summary, self.global_step)\r\n"""
src/helper/visualizer.py,0,"b'#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n# File: visualizer.py\n# Author: Qian Ge <geqian1001@gmail.com>\n\nimport os\nimport numpy as np\nimport tensorflow as tf\nimport matplotlib.patches as mpatches\n\n# import src.utils.viz as viz\n\nclass Visualizer(object):\n    def __init__(self, model, save_path=None):\n\n        self._save_path = save_path\n        self._model = model\n        self._latent_op = model.layers[\'z\']\n\n    def viz_2Dlatent_variable(self, sess, dataflow, batch_size=128, file_id=None):\n        """"""\n        modify from:\n        https://github.com/fastforwardlabs/vae-tf/blob/master/plot.py#L45\n        """"""\n        import matplotlib as mpl\n        mpl.use(\'Agg\')\n        import matplotlib.pyplot as plt\n        self._model.set_is_training(False)\n\n        dataflow.setup(epoch_val=0, batch_size=batch_size)\n        latent_var_list = []\n        label_list = []\n        \n        while dataflow.epochs_completed == 0:\n            batch_data = dataflow.next_batch_dict()\n            im = batch_data[\'im\']\n            labels = batch_data[\'label\']\n            latent_var = sess.run(\n                self._latent_op, \n                feed_dict={self._model.encoder_in: im,\n                           self._model.keep_prob: 1.})\n            latent_var_list.extend(latent_var)\n#             try:           \n#                 latent_var_list.extend(latent_var[:, pick_dim])\n#             except UnboundLocalError:\n#                 pick_dim = np.random.choice(len(latent_var[0]), 2, replace=False)\n#                 pick_dim = sorted(pick_dim)\n#                 latent_var_list.extend(latent_var[:, pick_dim])\n\n            label_list.extend(labels)\n            # print(latent_var)\n\n        xs, ys = np.array(latent_var_list).T\n\n        plt.figure()\n        # plt.title(""round {}: {} in latent space"".format(model.step, title))\n        kwargs = {\'alpha\': 0.8}\n\n        classes = set(label_list)\n        if classes:\n            colormap = plt.cm.rainbow(np.linspace(0, 1, len(classes)))\n            kwargs[\'c\'] = [colormap[i] for i in label_list]\n\n            # make room for legend\n            ax = plt.subplot(111, aspect=\'equal\')\n            box = ax.get_position()\n            ax.set_position([box.x0, box.y0, box.width * 0.8, box.height])\n            handles = [mpatches.Circle((0,0), label=class_, color=colormap[i])\n                        for i, class_ in enumerate(classes)]\n            ax.legend(handles=handles, shadow=True, bbox_to_anchor=(1.05, 0.45),\n                      fancybox=True, loc=\'center left\')\n\n        plt.scatter(xs, ys, s=2, **kwargs)\n\n        ax.set_xlim([-3.5, 3.5])\n        ax.set_ylim([-3.5, 3.5])\n\n        if file_id is not None:\n            fig_save_path = os.path.join(self._save_path, \'latent_{}.png\'.format(file_id))\n        else:\n            fig_save_path = os.path.join(self._save_path, \'latent.png\')\n        plt.savefig(fig_save_path, bbox_inches=""tight"")\n\n'"
src/models/__init__.py,0,b''
src/models/aae.py,99,"b'#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n# File: aae.py\n# Author: Qian Ge <geqian1001@gmail.com>\n\nimport tensorflow as tf\nfrom src.models.base import BaseModel\nimport src.models.layers as L\nimport src.models.modules as modules\nimport src.models.ops as ops\n\nINIT_W = tf.contrib.layers.variance_scaling_initializer()\n\nclass AAE(BaseModel):\n    """""" model of Adversarical Autoencoders """"""\n\n    def __init__(self, im_size=[28, 28], n_channel=1, n_class=None, n_code=1000,\n                 use_label=False, use_supervise=False, add_noise=False, wd=0,\n                 enc_weight=1., gen_weight=1., dis_weight=1.,\n                 cat_dis_weight=1., cat_gen_weight=1., cls_weight=1.):\n        """"""\n            Args:\n                im_size (int or list of length 2): size of input image\n                n_channel (int): number of input image channel (1 or 3)\n                n_class (int): number of classes\n                n_code (int): dimension of code\n                use_label (bool): whether incoporate label information\n                    in the adversarial regularization or not\n                use_supervise (bool): whether supervised training or not\n                add_noise (bool): whether add noise to encoder input or not\n                wd (float): weight decay\n                enc_weight (float): weight of autoencoder loss\n                gen_weight (float): weight of latent z generator loss\n                dis_weight (float): weight of latent z discriminator loss\n                cat_gen_weight (float): weight of label y generator loss\n                cat_dis_weight (float): weight of label y discriminator loss\n                cls_weight (float): weight of classification loss\n        """"""\n        self._n_channel = n_channel\n        self._wd = wd\n        self.n_code = n_code\n        self._im_size = im_size\n        if use_supervise:\n            use_label = False\n        self._flag_label = use_label\n        self._flag_supervise = use_supervise\n        self._flag_noise = add_noise\n        self.n_class = n_class\n        self._enc_w = enc_weight\n        self._gen_w = gen_weight\n        self._dis_w = dis_weight\n        self._cat_dis_w = cat_dis_weight\n        self._cat_gen_w = cat_gen_weight\n        self._cls_w = cls_weight\n        self.layers = {}\n\n    def _create_train_input(self):\n        """""" create input for training model in fig 1, 3, 6 and 8 in the paper """"""\n        self.image = tf.placeholder(\n            tf.float32, name=\'image\',\n            shape=[None, self._im_size[0], self._im_size[1], self._n_channel])\n        self.label = tf.placeholder(\n            tf.int64, name=\'label\', shape=[None])\n        self.real_distribution = tf.placeholder(\n            tf.float32, name=\'real_distribution\', shape=[None, self.n_code])\n        self.real_y = tf.placeholder(\n            tf.int64, name=\'real_y\', shape=[None])\n        self.lr = tf.placeholder(tf.float32, name=\'lr\')\n        self.keep_prob = tf.placeholder(tf.float32, name=\'keep_prob\')\n\n    def create_semisupervised_train_model(self):\n        """""" create training model in fig 8 in the paper """"""\n        self.set_is_training(True)\n        self._create_train_input()\n        with tf.variable_scope(\'AE\', reuse=tf.AUTO_REUSE):\n            encoder_in = self.image\n            if self._flag_noise:\n                # add gaussian noise to encoder input\n                encoder_in += tf.random_normal(\n                    tf.shape(encoder_in), mean=0.0, stddev=0.6, dtype=tf.float32)\n            self.encoder_in = encoder_in\n            self.layers[\'encoder_out\'] = self.encoder(self.encoder_in)\n            # continuous latent variable\n            self.layers[\'z\'], self.layers[\'z_mu\'], self.layers[\'z_std\'], self.layers[\'z_log_std\'] =\\\n                self.sample_latent(self.layers[\'encoder_out\'])\n            # discrete class variable\n            self.layers[\'cls_logits\'] = self.cls_layer(self.layers[\'encoder_out\'])\n\n            self.layers[\'y\'] = tf.argmax(self.layers[\'cls_logits\'], axis=-1,\n                                         name=\'label_predict\')\n            # one hot label is approximated by output of softmax for back-prop\n            self.layers[\'one_hot_y_approx\'] = tf.nn.softmax(self.layers[\'cls_logits\'], axis=-1)\n            \n            decoder_in = tf.concat((self.layers[\'z\'], self.layers[\'one_hot_y_approx\']), axis=-1)\n            self.layers[\'decoder_out\'] = self.decoder(decoder_in)\n            self.layers[\'sample_im\'] = (self.layers[\'decoder_out\'] + 1. ) / 2.\n\n        with tf.variable_scope(\'regularization_z\'):\n            fake_in = self.layers[\'z\']\n            real_in = self.real_distribution\n            self.layers[\'fake_z\'] = self.latent_discriminator(fake_in)\n            self.layers[\'real_z\'] = self.latent_discriminator(real_in)\n\n        with tf.variable_scope(\'regularization_y\'):\n            fake_in = self.layers[\'one_hot_y_approx\']\n            real_in = tf.one_hot(self.real_y, self.n_class)\n            self.layers[\'fake_y\'] = self.cat_discriminator(fake_in)\n            self.layers[\'real_y\'] = self.cat_discriminator(real_in)\n\n    def create_train_model(self):\n        """""" create training model in fig 1, 3 and 6 in the paper """"""\n        self.set_is_training(True)\n        self._create_train_input()\n        with tf.variable_scope(\'AE\', reuse=tf.AUTO_REUSE):\n            encoder_in = self.image\n            if self._flag_noise:\n                # add gaussian noise to encoder input\n                encoder_in += tf.random_normal(\n                    tf.shape(encoder_in), mean=0.0, stddev=0.6, dtype=tf.float32)\n            self.encoder_in = encoder_in\n            self.layers[\'encoder_out\'] = self.encoder(self.encoder_in)\n            self.layers[\'z\'], self.layers[\'z_mu\'], self.layers[\'z_std\'], self.layers[\'z_log_std\'] =\\\n                self.sample_latent(self.layers[\'encoder_out\'])\n\n            self.decoder_in = self.layers[\'z\']\n            if self._flag_supervise:\n                one_hot_label = tf.one_hot(self.label, self.n_class)\n                self.decoder_in = tf.concat((self.decoder_in, one_hot_label), axis=-1)\n            self.layers[\'decoder_out\'] = self.decoder(self.decoder_in)\n            self.layers[\'sample_im\'] = (self.layers[\'decoder_out\'] + 1. ) / 2.\n\n        with tf.variable_scope(\'regularization_z\'):\n            fake_in = self.layers[\'z\']\n            real_in = self.real_distribution\n            self.layers[\'fake_z\'] = self.latent_discriminator(fake_in)\n            self.layers[\'real_z\'] = self.latent_discriminator(real_in)\n\n    def _create_generate_input(self):\n        """""" create input for sampling model in fig 1, 3 and 6 in the paper """"""\n        self.z = tf.placeholder(\n            tf.float32, name=\'latent_z\',\n            shape=[None, self.n_code])\n        self.keep_prob = 1.\n        self.image = tf.placeholder(\n            tf.float32, name=\'image\',\n            shape=[None, self._im_size[0], self._im_size[1], self._n_channel])\n\n    def create_generate_style_model(self, n_sample):\n        """""" create samping model in fig 6 in the paper """"""\n        self.set_is_training(False)\n        with tf.variable_scope(\'AE\', reuse=tf.AUTO_REUSE):\n            self._create_generate_input()\n            label = []\n            for i in range(self.n_class):\n                label.extend([i for k in range(n_sample)])\n            label = tf.convert_to_tensor(label) # [n_class]\n            one_hot_label = tf.one_hot(label, self.n_class) # [n_class*n_sample, n_class]\n\n            z = ops.tf_sample_standard_diag_guassian(n_sample, self.n_code)\n            z = tf.tile(z, [self.n_class, 1]) # [n_class*n_sample, n_code]\n            decoder_in = tf.concat((z, one_hot_label), axis=-1)\n            self.layers[\'generate\'] = (self.decoder(decoder_in) + 1. ) / 2.\n\n    def create_generate_model(self, b_size):\n        """""" create samping model in fig 1 and 3 in the paper """"""\n        self.set_is_training(False)\n        with tf.variable_scope(\'AE\', reuse=tf.AUTO_REUSE):\n            self._create_generate_input()\n            # if self.z is not fed in, just sample from diagonal Gaussian\n            self.z = ops.tf_sample_standard_diag_guassian(b_size, self.n_code)\n            decoder_in = self.z\n            self.layers[\'generate\'] = (self.decoder(decoder_in) + 1. ) / 2.\n\n    def _create_cls_input(self):\n        """""" create input for testing model in fig 8 in the paper """"""\n        self.keep_prob = 1.\n        self.label = tf.placeholder(tf.int64, name=\'label\', shape=[None])\n        self.image = tf.placeholder(\n            tf.float32, name=\'image\',\n            shape=[None, self._im_size[0], self._im_size[1], self._n_channel])\n\n    def create_semisupervised_test_model(self):\n        """""" create testing model in fig 8 in the paper """"""\n        self.set_is_training(False)\n        self._create_cls_input()\n        with tf.variable_scope(\'AE\', reuse=tf.AUTO_REUSE):\n            encoder_in = self.image\n            self.encoder_in = encoder_in\n            self.layers[\'encoder_out\'] = self.encoder(self.encoder_in)\n            # discrete class variable\n            self.layers[\'cls_logits\'] = self.cls_layer(self.layers[\'encoder_out\'])\n            self.layers[\'y\'] = tf.argmax(self.layers[\'cls_logits\'], axis=-1,\n                                         name=\'label_predict\')\n\n    def encoder(self, inputs):\n        with tf.variable_scope(\'encoder\'):\n            fc_out = modules.encoder_FC(\n                inputs, self.is_training, keep_prob=self.keep_prob,\n                wd=self._wd, name=\'encoder_FC\', init_w=INIT_W)\n            return fc_out\n\n    def decoder(self, inputs):\n        with tf.variable_scope(\'decoder\'):\n            fc_out = modules.decoder_FC(\n                inputs, self.is_training, keep_prob=self.keep_prob,\n                wd=self._wd, name=\'decoder_FC\', init_w=INIT_W)\n            out_dim = self._im_size[0] * self._im_size[1] * self._n_channel\n            decoder_out = L.linear(\n                out_dim=out_dim, layer_dict=self.layers,\n                inputs=fc_out, init_w=None, wd=self._wd, name=\'decoder_linear\')\n            decoder_out = tf.reshape(\n                decoder_out, (-1, self._im_size[0], self._im_size[1], self._n_channel))\n            return tf.tanh(decoder_out)\n\n    def cls_layer(self, encoder_out):\n        """""" estimate digit label for semi-supervised model """"""\n        cls_logits = L.linear(\n            out_dim=self.n_class, layer_dict=self.layers,\n            inputs=encoder_out, init_w=INIT_W, wd=self._wd, name=\'cls_layer\')\n        return cls_logits\n\n    def sample_latent(self, encoder_out):\n        with tf.variable_scope(\'sample_latent\'):\n            encoder_out = encoder_out\n            \n            z_mean = L.linear(\n                out_dim=self.n_code, layer_dict=self.layers,\n                inputs=encoder_out, init_w=INIT_W, wd=self._wd, name=\'latent_mean\')\n            z_std = L.linear(\n                out_dim=self.n_code, layer_dict=self.layers, nl=L.softplus,\n                inputs=encoder_out, init_w=INIT_W, wd=self._wd, name=\'latent_std\')\n            z_log_std = tf.log(z_std + 1e-8)\n\n            b_size = tf.shape(encoder_out)[0]\n            z = ops.tf_sample_diag_guassian(z_mean, z_std, b_size, self.n_code)\n            return z, z_mean, z_std, z_log_std\n\n    def latent_discriminator(self, inputs):\n        with tf.variable_scope(\'latent_discriminator\', reuse=tf.AUTO_REUSE):\n            fc_out = modules.discriminator_FC(\n                inputs, self.is_training, nl=L.leaky_relu,\n                wd=self._wd, name=\'latent_discriminator_FC\', init_w=INIT_W)\n            return fc_out\n\n    def cat_discriminator(self, inputs):\n        with tf.variable_scope(\'cat_discriminator\', reuse=tf.AUTO_REUSE):\n            fc_out = modules.discriminator_FC(\n                inputs, self.is_training, nl=L.leaky_relu,\n                wd=self._wd, name=\'cat_discriminator_FC\', init_w=INIT_W)\n            return fc_out\n\n    def get_generate_summary(self):\n        with tf.name_scope(\'generate\'):\n            tf.summary.image(\n                \'image\',\n                tf.cast(self.layers[\'generate\'], tf.float32),\n                collections=[\'generate\'])\n        return tf.summary.merge_all(key=\'generate\')\n\n    def get_valid_summary(self):\n        with tf.name_scope(\'valid\'):\n            tf.summary.image(\n                \'encoder input\',\n                tf.cast(self.encoder_in, tf.float32),\n                collections=[\'valid\'])\n            tf.summary.image(\n                \'decoder output\',\n                tf.cast(self.layers[\'sample_im\'], tf.float32),\n                collections=[\'valid\'])  \n            return tf.summary.merge_all(key=\'valid\')\n\n    def get_train_summary(self):\n        with tf.name_scope(\'train\'):\n            tf.summary.image(\n                \'input image\',\n                tf.cast(self.image, tf.float32),\n                collections=[\'train\'])\n            tf.summary.image(\n                \'encoder input\',\n                tf.cast(self.encoder_in, tf.float32),\n                collections=[\'train\'])\n            tf.summary.image(\n                \'decoder output\',\n                tf.cast(self.layers[\'sample_im\'], tf.float32),\n                collections=[\'train\'])\n\n            tf.summary.histogram(\n                name=\'z real distribution\', values=self.real_distribution,\n                collections=[\'train\'])\n            tf.summary.histogram(\n                name=\'z encoder distribution\', values=self.layers[\'z\'],\n                collections=[\'train\'])\n            try:\n                tf.summary.histogram(\n                    name=\'y encoder distribution\', values=self.layers[\'y\'],\n                    collections=[\'train\'])\n                tf.summary.histogram(\n                    name=\'y real distribution\', values=self.real_y,\n                    collections=[\'train\'])\n            except KeyError:\n                pass\n        \n        return tf.summary.merge_all(key=\'train\')\n\n    def _get_reconstruction_loss(self):\n        with tf.name_scope(\'reconstruction_loss\'):\n            p_hat = self.layers[\'decoder_out\']\n            p = self.image\n            autoencoder_loss = 0.5 * tf.reduce_mean(tf.reduce_sum(tf.square(p - p_hat), axis=[1,2,3]))\n\n            return autoencoder_loss * self._enc_w\n\n    def get_reconstruction_loss(self):\n        try:\n            return self._reconstr_loss\n        except AttributeError:\n            self._reconstr_loss = self._get_reconstruction_loss()\n        return self._reconstr_loss\n\n    def get_reconstruction_train_op(self):\n        with tf.name_scope(\'reconstruction_train\'):\n            opt = tf.train.AdamOptimizer(self.lr, beta1=0.5)\n            loss = self.get_reconstruction_loss()\n            var_list = tf.trainable_variables(scope=\'AE\')\n            # print(var_list)\n            grads = tf.gradients(loss, var_list)\n            return opt.apply_gradients(zip(grads, var_list))\n\n    def get_latent_generator_train_op(self):\n        var_list = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=\'AE/encoder\') +\\\n                   tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=\'AE/sample_latent\')\n        self.latent_g_loss, train_op = modules.train_generator(\n            fake_in=self.layers[\'fake_z\'],\n            loss_weight=self._gen_w,\n            opt=tf.train.AdamOptimizer(self.lr, beta1=0.5),\n            var_list=var_list,\n            name=\'z_generate_train_op\')\n        return train_op\n\n    def get_cat_generator_train_op(self):\n        var_list = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=\'AE/encoder\') +\\\n                   tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=\'AE/cls_layer\')\n        self.cat_g_loss, train_op = modules.train_generator(\n            fake_in=self.layers[\'fake_y\'],\n            loss_weight=self._cat_gen_w,\n            opt=tf.train.AdamOptimizer(self.lr, beta1=0.5),\n            var_list=var_list,\n            name=\'y_generate_train_op\')\n        return train_op\n\n    def get_latent_discrimator_train_op(self):\n        self.latent_d_loss, train_op = modules.train_discrimator(\n            fake_in=self.layers[\'fake_z\'],\n            real_in=self.layers[\'real_z\'],\n            loss_weight=self._dis_w,\n            opt=tf.train.AdamOptimizer(self.lr, beta1=0.5),\n            var_list=tf.trainable_variables(scope=\'regularization_z\'),\n            name=\'z_discrimator_train_op\')\n        return train_op\n\n    def get_cat_discrimator_train_op(self):\n        self.cat_d_loss, train_op = modules.train_discrimator(\n            fake_in=self.layers[\'fake_y\'],\n            real_in=self.layers[\'real_y\'],\n            loss_weight=self._cat_dis_w,\n            opt=tf.train.AdamOptimizer(self.lr, beta1=0.5),\n            var_list=tf.trainable_variables(scope=\'regularization_y\'),\n            name=\'y_discrimator_train_op\')\n        return train_op\n\n    def get_cls_train_op(self):\n        with tf.name_scope(\'cls_train_op\'):\n            var_list = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=\'AE/encoder\') +\\\n                       tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=\'AE/cls_layer\')\n            loss = self.get_cls_loss()\n            opt=tf.train.AdamOptimizer(self.lr, beta1=0.5)\n            grads = tf.gradients(loss, var_list)\n            return opt.apply_gradients(zip(grads, var_list))\n\n    def _get_cls_loss(self):\n        with tf.name_scope(\'cls_loss\'):\n            logits=self.layers[\'cls_logits\'],\n            labels=self.label,\n            cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(\n                labels=labels,\n                logits=logits,\n                name=\'cross_entropy\')\n            cross_entropy = tf.reduce_mean(cross_entropy)\n\n            return cross_entropy * self._cls_w\n\n    def get_cls_loss(self):\n        try:\n            return self._cls_loss\n        except AttributeError:\n            self._cls_loss = self._get_cls_loss()\n        return self._cls_loss\n\n    def get_cls_accuracy(self):\n        with tf.name_scope(\'cls_accuracy\'):\n            labels = self.label\n            cls_predict = self.layers[\'y\']\n            num_correct = tf.cast(tf.equal(labels, cls_predict), tf.float32)\n            return tf.reduce_mean(num_correct)\n'"
src/models/base.py,4,"b'#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n# File: base.py\n# Author: Qian Ge <geqian1001@gmail.com>\n\nimport tensorflow as tf\nfrom abc import abstractmethod\n\n\nclass BaseModel(object):\n    """""" Model with single loss and single optimizer """"""\n\n    def set_is_training(self, is_training=True):\n        self.is_training = is_training\n\n    def get_loss(self):\n        try:\n            return self._loss\n        except AttributeError:\n            self._loss = self._get_loss()\n        return self._loss\n\n    def _get_loss(self):\n        raise NotImplementedError()\n\n    def get_optimizer(self):\n        try:\n            return self.optimizer\n        except AttributeError:\n            self.optimizer = self._get_optimizer()\n        return self.optimizer\n\n    def _get_optimizer(self):\n        raise NotImplementedError()\n\n    def get_train_op(self):\n        with tf.name_scope(\'train\'):\n            opt = self.get_optimizer()\n            loss = self.get_loss()\n            var_list = tf.trainable_variables()\n            grads = tf.gradients(loss, var_list)\n            # [tf.summary.histogram(\'gradient/\' + var.name, grad, \n            #  collections=[\'train\']) for grad, var in zip(grads, var_list)]\n            return opt.apply_gradients(zip(grads, var_list))\n\n\n'"
src/models/distribution.py,0,"b'#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n# File: distribution.py\n# Author: Qian Ge <geqian1001@gmail.com>\n\nimport numpy as np\nfrom math import sin,cos,sqrt\n\n\ndef interpolate(plot_size=20, interpolate_range=[-3, 3, -3, 3]):\n    assert len(interpolate_range) == 4\n    nx = plot_size\n    ny = plot_size\n    min_x = interpolate_range[0]\n    max_x = interpolate_range[1]\n    min_y = interpolate_range[2]\n    max_y = interpolate_range[3]\n    \n    zs = np.rollaxis(np.mgrid[min_x: max_x: nx*1j, max_y:min_y: ny*1j], 0, 3)\n    zs = zs.transpose(1, 0, 2)\n    return np.reshape(zs, (plot_size*plot_size, 2))\n\ndef interpolate_gm(plot_size=20, interpolate_range=[-1., 1., -0.2, 0.2],\n                   mode_id=0, n_mode=10):\n    n_samples = plot_size * plot_size\n    def sample(x, y, mode_id, n_mode):\n        shift = 1.4\n        r = 2.0 * np.pi / float(n_mode) * float(mode_id)\n        new_x = x * cos(r) - y * sin(r)\n        new_y = x * sin(r) + y * cos(r)\n        new_x += shift * cos(r)\n        new_y += shift * sin(r)\n        return np.array([new_x, new_y]).reshape((2,))\n\n    interp_grid = interpolate(plot_size=20, interpolate_range=interpolate_range)\n    x = interp_grid[:, 0]\n    y = interp_grid[:, 1]\n\n    z = np.empty((n_samples, 2), dtype=np.float32)\n    for i in range(n_samples):\n        z[i, :2] = sample(x[i], y[i], mode_id, n_mode)\n    return z\n\ndef gaussian(batch_size, n_dim, mean=0, var=1.):\n    z = np.random.normal(mean, var, (batch_size, n_dim)).astype(np.float32)\n    return z\n\ndef diagonal_gaussian(batch_size, n_dim, mean=0, var=1.):\n    cov_mat = np.diag([var for i in range(n_dim)])\n    mean_vec = [mean for i in range(n_dim)]\n    z = np.random.multivariate_normal(\n        mean_vec, cov_mat, (batch_size,)).astype(np.float32)\n    return z\n\ndef gaussian_mixture(batch_size, n_dim=2, n_labels=10,\n                     x_var=0.5, y_var=0.1, label_indices=None):\n    # borrow from:\n    # https://github.com/nicklhy/AdversarialAutoEncoder/blob/master/data_factory.py#L40\n    if n_dim % 2 != 0:\n        raise Exception(""n_dim must be a multiple of 2."")\n\n    def sample(x, y, label, n_labels):\n        shift = 1.4\n        if label >= n_labels:\n            label =  np.random.randint(0, n_labels)\n        r = 2.0 * np.pi / float(n_labels) * float(label)\n        new_x = x * cos(r) - y * sin(r)\n        new_y = x * sin(r) + y * cos(r)\n        new_x += shift * cos(r)\n        new_y += shift * sin(r)\n        return np.array([new_x, new_y]).reshape((2,))\n\n    x = np.random.normal(0, x_var, (batch_size, n_dim // 2))\n    y = np.random.normal(0, y_var, (batch_size, n_dim // 2))\n    z = np.empty((batch_size, n_dim), dtype=np.float32)\n    for batch in range(batch_size):\n        for zi in range(n_dim // 2):\n            if label_indices is not None:\n                z[batch, zi*2:zi*2+2] = sample(x[batch, zi], y[batch, zi], label_indices[batch], n_labels)\n            else:\n                z[batch, zi*2:zi*2+2] = sample(x[batch, zi], y[batch, zi], np.random.randint(0, n_labels), n_labels)\n\n    return z\n'"
src/models/layers.py,46,"b'#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n# File: layers.py\n# Author: Qian Ge <geqian1001@gmail.com>\n\nimport math\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow.contrib.framework import add_arg_scope\n\n\ndef get_shape4D(in_val):\n    """"""\n    Return a 4D shape\n    Args:\n        in_val (int or list with length 2)\n    Returns:\n        list with length 4\n    """"""\n    # if isinstance(in_val, int):\n    return [1] + get_shape2D(in_val) + [1]\n\ndef get_shape2D(in_val):\n    """"""\n    Return a 2D shape \n    Args:\n        in_val (int or list with length 2) \n    Returns:\n        list with length 2\n    """"""\n    in_val = int(in_val)\n    if isinstance(in_val, int):\n        return [in_val, in_val]\n    if isinstance(in_val, list):\n        assert len(in_val) == 2\n        return in_val\n    raise RuntimeError(\'Illegal shape: {}\'.format(in_val))\n\ndef batch_flatten(x):\n    """"""\n    Flatten the tensor except the first dimension.\n    """"""\n    shape = x.get_shape().as_list()[1:]\n    if None not in shape:\n        return tf.reshape(x, [-1, int(np.prod(shape))])\n    return tf.reshape(x, tf.stack([tf.shape(x)[0], -1]))\n\ndef softplus(inputs, name):\n    return tf.log(1 + tf.exp(inputs), name=name)\n\ndef softmax(logits, axis=-1, name=\'softmax\'):\n    with tf.name_scope(name):\n        max_in = tf.reduce_max(logits, axis=axis, keepdims=True)\n        stable_in = logits - max_in\n        normal_p = tf.reduce_sum(tf.exp(stable_in), axis=axis, keepdims=True)\n \n        return tf.exp(stable_in) / normal_p\n\ndef leaky_relu(x, leak=0.2, name=\'LeakyRelu\'):\n    """""" \n    leaky_relu \n        Allow a small non-zero gradient when the unit is not active\n    Args:\n        x (tf.tensor): a tensor \n        leak (float): Default to 0.2\n    Returns:\n        tf.tensor with name \'name\'\n    """"""\n    return tf.maximum(x, leak*x, name=name)\n\n\n@add_arg_scope\ndef linear(out_dim,\n           layer_dict=None,\n           inputs=None,\n           init_w=None,\n           init_b=tf.zeros_initializer(),\n           wd=0,\n           name=\'Linear\',\n           nl=tf.identity):\n    with tf.variable_scope(name):\n        if inputs is None:\n            assert layer_dict is not None\n            inputs = layer_dict[\'cur_input\']\n        inputs = batch_flatten(inputs)\n        in_dim = inputs.get_shape().as_list()[1]\n        if wd > 0:\n            regularizer = tf.contrib.layers.l2_regularizer(scale=wd)\n        else:\n            regularizer=None\n        weights = tf.get_variable(\'weights\',\n                                  shape=[in_dim, out_dim],\n                                  # dtype=None,\n                                  initializer=init_w,\n                                  regularizer=regularizer,\n                                  trainable=True)\n        biases = tf.get_variable(\'biases\',\n                                  shape=[out_dim],\n                                  # dtype=None,\n                                  initializer=init_b,\n                                  regularizer=None,\n                                  trainable=True)\n        # print(\'init: {}\'.format(weights))\n        act = tf.nn.xw_plus_b(inputs, weights, biases)\n        result = nl(act, name=\'output\')\n        if layer_dict is not None:\n            layer_dict[\'cur_input\'] = result\n            \n        return result\n\n@add_arg_scope\ndef transpose_conv(\n                   filter_size,\n                   out_dim,\n                   layer_dict,\n                   inputs=None,\n                   out_shape=None,\n                   stride=2,\n                   padding=\'SAME\',\n                   trainable=True,\n                   nl=tf.identity,\n                   init_w=None,\n                   init_b=tf.zeros_initializer(),\n                   wd=0,\n                   constant_init=False,\n                   name=\'dconv\'):\n    if inputs is None:\n        inputs = layer_dict[\'cur_input\']\n    stride = get_shape4D(stride)\n    in_dim = inputs.get_shape().as_list()[-1]\n\n    # TODO other ways to determine the output shape \n    x_shape = tf.shape(inputs)\n    # assume output shape is input_shape*stride\n    if out_shape is None:\n        out_shape = tf.stack([x_shape[0],\n                              tf.multiply(x_shape[1], stride[1]), \n                              tf.multiply(x_shape[2], stride[2]),\n                              out_dim])        \n\n    filter_shape = get_shape2D(filter_size) + [out_dim, in_dim]\n\n    with tf.variable_scope(name) as scope:\n        if wd > 0:\n            regularizer = tf.contrib.layers.l2_regularizer(scale=wd)\n        else:\n            regularizer=None\n        weights = tf.get_variable(\'weights\',\n                                  filter_shape,\n                                  initializer=init_w,\n                                  trainable=trainable,\n                                  regularizer=regularizer)\n        biases = tf.get_variable(\'biases\',\n                                 [out_dim],\n                                 initializer=init_b,\n                                 trainable=trainable)\n        \n        output = tf.nn.conv2d_transpose(inputs,\n                                        weights, \n                                        output_shape=out_shape, \n                                        strides=stride, \n                                        padding=padding, \n                                        name=scope.name)\n\n        output = tf.nn.bias_add(output, biases)\n        output.set_shape([None, None, None, out_dim])\n        output = nl(output, name=\'output\')\n        layer_dict[\'cur_input\'] = output\n        return output\n\ndef max_pool(layer_dict,\n             inputs=None,\n             name=\'max_pool\',\n             filter_size=2,\n             stride=None,\n             padding=\'SAME\',\n             switch=False):\n    """""" \n    Max pooling layer \n    Args:\n        x (tf.tensor): a tensor \n        name (str): name scope of the layer\n        filter_size (int or list with length 2): size of filter\n        stride (int or list with length 2): Default to be the same as shape\n        padding (str): \'VALID\' or \'SAME\'. Use \'SAME\' for FCN.\n    Returns:\n        tf.tensor with name \'name\'\n    """"""\n    if inputs is not None:\n        layer_dict[\'cur_input\'] = inputs\n    padding = padding.upper()\n    filter_shape = get_shape4D(filter_size)\n    if stride is None:\n        stride = filter_shape\n    else:\n        stride = get_shape4D(stride)\n\n    if switch == True:\n        layer_dict[\'cur_input\'], switch_s = tf.nn.max_pool_with_argmax(\n            layer_dict[\'cur_input\'],\n            ksize=filter_shape, \n            strides=stride, \n            padding=padding,\n            Targmax=tf.int64,\n            name=name)\n        return layer_dict[\'cur_input\'], switch_s\n    else:\n        layer_dict[\'cur_input\'] = tf.nn.max_pool(\n            layer_dict[\'cur_input\'],\n            ksize=filter_shape, \n            strides=stride, \n            padding=padding,\n            name=name)\n        return layer_dict[\'cur_input\'], None\n\n@add_arg_scope\ndef conv(filter_size,\n         out_dim,\n         layer_dict,\n         inputs=None,\n         pretrained_dict=None,\n         stride=1,\n         dilations=[1, 1, 1, 1],\n         bn=False,\n         nl=tf.identity,\n         init_w=None,\n         init_b=tf.zeros_initializer(),\n         use_bias=True,\n         padding=\'SAME\',\n         pad_type=\'ZERO\',\n         trainable=True,\n         is_training=None,\n         wd=0,\n         name=\'conv\',\n         add_summary=False):\n    if inputs is None:\n        inputs = layer_dict[\'cur_input\']\n    stride = get_shape4D(stride)\n    in_dim = inputs.get_shape().as_list()[-1]\n    filter_shape = get_shape2D(filter_size) + [in_dim, out_dim]\n\n    if padding == \'SAME\' and pad_type == \'REFLECT\':\n        pad_size_1 = int((filter_shape[0] - 1) / 2)\n        pad_size_2 = int((filter_shape[1] - 1) / 2)\n        inputs = tf.pad(\n            inputs,\n            [[0, 0], [pad_size_1, pad_size_1], [pad_size_2, pad_size_2], [0, 0]],\n            ""REFLECT"")\n        padding = \'VALID\'\n\n    with tf.variable_scope(name):\n        if wd > 0:\n            regularizer = tf.contrib.layers.l2_regularizer(scale=wd)\n        else:\n            regularizer=None\n\n        if pretrained_dict is not None and name in pretrained_dict:\n            try:\n                load_w = pretrained_dict[name][0]\n            except KeyError:\n                load_w = pretrained_dict[name][\'weights\']\n            print(\'Load {} weights!\'.format(name))\n\n            load_w = np.reshape(load_w, filter_shape)\n            init_w = tf.constant_initializer(load_w)\n\n        weights = tf.get_variable(\'weights\',\n                                  filter_shape,\n                                  initializer=init_w,\n                                  trainable=trainable,\n                                  regularizer=regularizer)\n        if add_summary:\n            tf.summary.histogram(\n                \'weights/{}\'.format(name), weights, collections = [\'train\'])\n\n        outputs = tf.nn.conv2d(inputs,\n                               filter=weights,\n                               strides=stride,\n                               padding=padding,\n                               use_cudnn_on_gpu=True,\n                               data_format=""NHWC"",\n                               dilations=dilations,\n                               name=\'conv2d\')\n\n        if use_bias:\n            if pretrained_dict is not None and name in pretrained_dict:\n                try:\n                    load_b = pretrained_dict[name][1]\n                except KeyError:\n                    load_b = pretrained_dict[name][\'biases\']\n                print(\'Load {} biases!\'.format(name))\n\n                load_b = np.reshape(load_b, [out_dim])\n                init_b = tf.constant_initializer(load_b)\n\n            biases = tf.get_variable(\'biases\',\n                                 [out_dim],\n                                 initializer=init_b,\n                                 trainable=trainable)\n            outputs += biases\n\n        # if bn is True:\n        #     outputs = layers.batch_norm(outputs, train=is_training, name=\'bn\')\n\n        layer_dict[\'cur_input\'] = nl(outputs)\n        layer_dict[name] = layer_dict[\'cur_input\']\n        return layer_dict[\'cur_input\']\n\ndef drop_out(layer_dict, is_training, inputs=None, keep_prob=0.5):\n    if inputs is None:\n        inputs = layer_dict[\'cur_input\']\n    if is_training:\n        layer_dict[\'cur_input\'] = tf.nn.dropout(inputs, keep_prob=keep_prob)\n    else:\n        layer_dict[\'cur_input\'] = inputs\n    return layer_dict[\'cur_input\']\n\n'"
src/models/modules.py,45,"b""#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n# File: modules.py\n# Author: Qian Ge <geqian1001@gmail.com>\n\nimport tensorflow as tf\nimport src.models.layers as L\n\n\ndef encoder_FC(inputs, is_training, n_hidden=1000, nl=tf.nn.relu,\n               keep_prob=0.5, wd=0, name='encoder_FC', init_w=None):\n    layer_dict = {}\n    layer_dict['cur_input'] = inputs\n    with tf.variable_scope(name):\n        arg_scope = tf.contrib.framework.arg_scope\n        with arg_scope([L.linear],\n                       out_dim=n_hidden, layer_dict=layer_dict, init_w=init_w,\n                       wd=wd):\n            L.linear(name='linear1', nl=nl)\n            L.drop_out(layer_dict, is_training, keep_prob=keep_prob)\n            L.linear(name='linear2', nl=nl)\n            L.drop_out(layer_dict, is_training, keep_prob=keep_prob)\n\n        return layer_dict['cur_input']\n        \ndef decoder_FC(inputs, is_training, n_hidden=1000, nl=tf.nn.relu,\n               keep_prob=0.5, wd=0, name='decoder_FC', init_w=None):\n    layer_dict = {}\n    layer_dict['cur_input'] = inputs\n    with tf.variable_scope(name):\n        arg_scope = tf.contrib.framework.arg_scope\n        with arg_scope([L.linear],\n                       out_dim=n_hidden, layer_dict=layer_dict, init_w=init_w,\n                       wd=wd):\n            L.linear(name='linear1', nl=nl)\n            L.drop_out(layer_dict, is_training, keep_prob=keep_prob)\n            L.linear(name='linear2', nl=nl)\n            L.drop_out(layer_dict, is_training, keep_prob=keep_prob)\n\n        return layer_dict['cur_input']\n\ndef discriminator_FC(inputs, is_training, n_hidden=1000, nl=tf.nn.relu,\n                     wd=0, name='discriminator_FC', init_w=None):\n    layer_dict = {}\n    layer_dict['cur_input'] = inputs\n    with tf.variable_scope(name):\n        arg_scope = tf.contrib.framework.arg_scope\n        with arg_scope([L.linear],\n                       layer_dict=layer_dict, init_w=init_w,\n                       wd=wd):\n            L.linear(name='linear1', nl=nl, out_dim=n_hidden)\n            L.linear(name='linear2', nl=nl, out_dim=n_hidden)\n            L.linear(name='output', out_dim=1)\n\n        return layer_dict['cur_input']\n\ndef encoder_CNN(inputs, is_training, wd=0, bn=False, name='encoder_CNN',\n                init_w=tf.keras.initializers.he_normal()):\n    # init_w = tf.keras.initializers.he_normal()\n    layer_dict = {}\n    layer_dict['cur_input'] = inputs\n    with tf.variable_scope(name):\n        arg_scope = tf.contrib.framework.arg_scope\n        with arg_scope([L.conv],\n                        layer_dict=layer_dict, bn=bn, nl=tf.nn.relu,\n                        init_w=init_w, padding='SAME', pad_type='ZERO',\n                        is_training=is_training, wd=0):\n            \n            L.conv(filter_size=5, out_dim=32, name='conv1', add_summary=False)\n            L.max_pool(layer_dict, name='pool1')\n            L.conv(filter_size=3, out_dim=64, name='conv2', add_summary=False)\n            L.max_pool(layer_dict, name='pool2')\n            # L.conv(filter_size=3, out_dim=128, name='conv3', add_summary=False)\n            # L.max_pool(layer_dict, name='pool3')\n\n            return layer_dict['cur_input']\n\ndef decoder_CNN(inputs, is_training, out_channel=1, wd=0, bn=False, name='decoder_CNN',\n                init_w=tf.keras.initializers.he_normal()):\n    # init_w = tf.keras.initializers.he_normal()\n    layer_dict = {}\n    layer_dict['cur_input'] = inputs\n    with tf.variable_scope(name):\n        arg_scope = tf.contrib.framework.arg_scope\n        with arg_scope([L.transpose_conv],\n                        layer_dict=layer_dict, nl=tf.nn.relu, stride=2,\n                        init_w=init_w, wd=0):\n            \n            # L.transpose_conv(filter_size=3, out_dim=64, name='deconv1')\n            L.transpose_conv(filter_size=3, out_dim=32, name='deconv2')\n            L.transpose_conv(filter_size=3, out_dim=out_channel, name='deconv3')\n\n            return layer_dict['cur_input']\n\ndef train_discrimator(fake_in, real_in, loss_weight, opt, var_list, name):\n    with tf.name_scope(name):\n        with tf.name_scope('discrimator_loss'):\n            loss_real = tf.nn.sigmoid_cross_entropy_with_logits(\n                labels=tf.ones_like(real_in),\n                logits=real_in,\n                name='loss_real')\n            loss_fake = tf.nn.sigmoid_cross_entropy_with_logits(\n                labels=tf.zeros_like(fake_in),\n                logits=fake_in,\n                name='loss_fake')\n            d_loss = tf.reduce_mean(loss_real) + tf.reduce_mean(loss_fake)\n                \n            # opt = tf.train.AdamOptimizer(lr, beta1=0.5)\n            # opt = tf.train.MomentumOptimizer(self.lr, momentum=0.1)\n            # dc_var = [var for var in all_variables if 'dc_' in var.name]\n            # var_list = tf.trainable_variables(scope='discriminator')\n            # print(tf.trainable_variables())\n            # print(var_list)\n            grads = tf.gradients(d_loss * loss_weight, var_list)\n            # [tf.summary.histogram('gradient/' + var.name, grad, \n            #  collections=['train']) for grad, var in zip(grads, var_list)]\n        train_op = opt.apply_gradients(zip(grads, var_list))\n\n        return d_loss, train_op\n\ndef train_generator(fake_in, loss_weight, opt, var_list, name):\n     with tf.name_scope(name):\n        with tf.name_scope('generator_loss'):\n            loss_fake = tf.nn.sigmoid_cross_entropy_with_logits(\n                labels=tf.ones_like(fake_in),\n                logits=fake_in,\n                name='loss_fake')\n            g_loss = tf.reduce_mean(loss_fake)\n        # opt = tf.train.AdamOptimizer(lr, beta1=0.5)\n        # print(var_list)\n        grads = tf.gradients(g_loss * loss_weight, var_list)\n        train_op = opt.apply_gradients(zip(grads, var_list))\n\n        return g_loss, train_op\n\ndef train_by_cross_entropy_loss(logits, labels, loss_weight, opt, var_list, name):\n    with tf.name_scope(name):\n        with tf.name_scope('cross_entropy_loss'):\n            cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(\n                labels=labels,\n                logits=logits,\n                name='cross_entropy')\n            cross_entropy = tf.reduce_mean(cross_entropy)\n        # opt = tf.train.AdamOptimizer(lr, beta1=0.5)\n        # print(var_list)\n        grads = tf.gradients(cross_entropy * loss_weight, var_list)\n        train_op = opt.apply_gradients(zip(grads, var_list))\n\n        return cross_entropy, train_op\n\n"""
src/models/ops.py,1,"b""#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n# File: ops.py\n# Author: Qian Ge <geqian1001@gmail.com>\n\nimport tensorflow as tf\nimport tensorflow_probability as tfp\n\n\ndef tf_sample_standard_diag_guassian(b_size, n_code):\n    mean_list = [0.0 for i in range(0, n_code)]\n    std_list = [1.0 for i in range(0, n_code)]\n    mvn = tfp.distributions.MultivariateNormalDiag(\n        loc=mean_list,\n        scale_diag=std_list)\n    samples = mvn.sample(sample_shape=(b_size,), seed=None, name='sample')\n    return samples\n\ndef tf_sample_diag_guassian(mean, std, b_size, n_code):\n    mean_list = [0.0 for i in range(0, n_code)]\n    std_list = [1.0 for i in range(0, n_code)]\n    mvn = tfp.distributions.MultivariateNormalDiag(\n        loc=mean_list,\n        scale_diag=std_list)\n    samples = mvn.sample(sample_shape=(b_size,), seed=None, name='sample')\n    samples = mean +  tf.multiply(std, samples)\n\n    return samples"""
src/models/vae.py,39,"b""#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n# File: vae.py\n# Author: Qian Ge <geqian1001@gmail.com>\n\nimport tensorflow as tf\nfrom src.models.base import BaseModel\nimport src.models.layers as L\nimport src.models.modules as modules\nimport src.models.ops as ops\n\n# INIT_W = tf.keras.initializers.he_normal()\nINIT_W = tf.contrib.layers.variance_scaling_initializer()\n\n\nclass VAE(BaseModel):\n    def __init__(self, im_size=[28, 28], n_code=1000, n_channel=1, wd=0):\n        self._n_channel = n_channel\n        self._wd = wd\n        self._n_code = n_code\n        self._im_size = im_size\n        self.layers = {}\n        \n    def create_generate_model(self, b_size):\n        self.set_is_training(False)\n        self._create_generate_input()\n        self.z = ops.tf_sample_standard_diag_guassian(b_size, self._n_code)\n        self.layers['generate'] = tf.nn.sigmoid(self.decoder(self.z))\n\n    def _create_generate_input(self):\n        self.z = tf.placeholder(\n            tf.float32, name='latent_z',\n            shape=[None, self._n_code])\n        self.keep_prob = 1.\n\n    def create_train_model(self):\n        self.set_is_training(True)\n        self._create_train_input()\n        self.layers['encoder_out'] = self.encoder()\n        self.layers['z'], self.layers['z_mu'], self.layers['z_std'], self.layers['z_log_std'] =\\\n            self.sample_latent()\n        self.layers['decoder_out'] = self.decoder(self.layers['z'])\n\n    def _create_train_input(self):\n        self.image = tf.placeholder(\n            tf.float32, name='image',\n            shape=[None, self._im_size[0], self._im_size[1], self._n_channel])\n        self.lr = tf.placeholder(tf.float32, name='lr')\n        self.keep_prob = tf.placeholder(tf.float32, name='keep_prob')\n\n    def encoder(self):\n        with tf.variable_scope('encoder'):\n            # cnn_out = modules.encoder_CNN(\n            #     self.image, is_training=self.is_training, init_w=INIT_W,\n            #     wd=self._wd, bn=False, name='encoder_CNN')\n\n            fc_out = modules.encoder_FC(self.image, self.is_training, keep_prob=self.keep_prob, wd=self._wd, name='encoder_FC', init_w=INIT_W)\n\n            # fc_out = L.linear(\n            #     out_dim=self._n_code*2, layer_dict=self.layers,\n            #     inputs=cnn_out, init_w=INIT_W, wd=self._wd, name='Linear')\n\n            return fc_out\n\n    def sample_latent(self):\n        with tf.variable_scope('sample_latent'):\n            cnn_out = self.layers['encoder_out']\n            \n            z_mean = L.linear(\n                out_dim=self._n_code, layer_dict=self.layers,\n                inputs=cnn_out, init_w=INIT_W, wd=self._wd, name='latent_mean')\n            z_std = L.linear(\n                out_dim=self._n_code, layer_dict=self.layers, nl=L.softplus,\n                inputs=cnn_out, init_w=INIT_W, wd=self._wd, name='latent_std')\n            z_log_std = tf.log(z_std + 1e-8)\n\n            b_size = tf.shape(cnn_out)[0]\n            z = ops.tf_sample_diag_guassian(z_mean, z_std, b_size, self._n_code)\n            return z, z_mean, z_std, z_log_std\n\n    def decoder(self, inputs):\n        with tf.variable_scope('decoder'):\n            # out_h = int(self._im_size[0] / 4)\n            # out_w = int(self._im_size[1] / 4)\n            # out_dim = out_h * out_w * 64\n\n            # z_linear = L.linear(\n            #     out_dim=out_dim, layer_dict=self.layers, nl=tf.nn.relu,\n            #     inputs=inputs, init_w=INIT_W, wd=self._wd, name='z_linear')\n            # z_linear = tf.reshape(z_linear, (-1, out_h, out_w, 64))\n\n            # decoder_out = modules.decoder_CNN(\n            #     z_linear, is_training=self.is_training, out_channel=self._n_channel, \n            #     wd=self._wd, bn=False, name='decoder_CNN')\n\n            fc_out = modules.decoder_FC(inputs, self.is_training, keep_prob=self.keep_prob, wd=self._wd, name='decoder_FC', init_w=INIT_W)\n            out_dim = self._im_size[0] * self._im_size[1] * self._n_channel\n            decoder_out = L.linear(\n                out_dim=out_dim, layer_dict=self.layers,\n                inputs=fc_out, init_w=None, wd=self._wd, name='decoder_linear')\n            decoder_out = tf.reshape(decoder_out, (-1, self._im_size[0], self._im_size[1], self._n_channel))\n\n            return decoder_out\n\n    def _get_loss(self):\n        with tf.name_scope('loss'):\n            with tf.name_scope('likelihood'):\n                p_hat = tf.nn.sigmoid(self.layers['decoder_out'], name='estimate_prob')\n                p = self.image\n                cross_entropy = p * tf.log(p_hat + 1e-6) + (1 - p) * tf.log(1 - p_hat + 1e-6)\n                # likelihood_loss = tf.nn.sigmoid_cross_entropy_with_logits(\n                #     labels=label,\n                #     logits=logits,\n                #     name='likelihood_loss')\n                cross_entropy_loss = -tf.reduce_mean(tf.reduce_sum(cross_entropy, axis=[1,2,3]))\n\n            with tf.name_scope('KL'):\n                kl_loss = tf.reduce_sum(\n                    tf.square(self.layers['z_mu'])\n                    + tf.square(self.layers['z_std'])\n                    - 2 * self.layers['z_log_std'],\n                    axis=1)\n\n                kl_loss = 0.5 * kl_loss\n                kl_loss = tf.reduce_mean(kl_loss)\n\n            return cross_entropy_loss + kl_loss\n\n    def _get_optimizer(self):\n        return tf.train.AdamOptimizer(self.lr)\n\n    def get_valid_summary(self):\n        with tf.name_scope('generate'):\n            tf.summary.image(\n                'image',\n                tf.cast(self.layers['generate'], tf.float32),\n                collections=['generate'])\n        return tf.summary.merge_all(key='generate')\n\n    def get_train_summary(self):\n        tf.summary.image(\n            'input_image',\n            tf.cast(self.image, tf.float32),\n            collections=['train'])\n        tf.summary.image(\n            'out_image',\n            tf.cast(tf.nn.sigmoid(self.layers['decoder_out']), tf.float32),\n            collections=['train'])\n        tf.summary.histogram(\n            name='encoder distribution', values=self.layers['z'],\n            collections=['train'])\n        return tf.summary.merge_all(key='train')\n\n"""
src/utils/__init__.py,0,b''
src/utils/dataflow.py,0,"b'#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n# File: dataflow.py\n# Author: Qian Ge <geqian1001@gmail.com>\n\nimport os\nimport scipy.misc\nimport numpy as np\nfrom datetime import datetime\n\n\n_RNG_SEED = None\n\ndef get_rng(obj=None):\n    """"""\n    This function is copied from `tensorpack\n    <https://github.com/ppwwyyxx/tensorpack/blob/master/tensorpack/utils/utils.py>`__.\n    Get a good RNG seeded with time, pid and the object.\n    Args:\n        obj: some object to use to generate random seed.\n    Returns:\n        np.random.RandomState: the RNG.\n    """"""\n    seed = (id(obj) + os.getpid() +\n            int(datetime.now().strftime(""%Y%m%d%H%M%S%f""))) % 4294967295\n    if _RNG_SEED is not None:\n        seed = _RNG_SEED\n    return np.random.RandomState(seed)\n\n\ndef get_file_list(file_dir, file_ext, sub_name=None):\n    re_list = []\n\n    if sub_name is None:\n        return np.array([os.path.join(root, name)\n            for root, dirs, files in os.walk(file_dir) \n            for name in sorted(files) if name.endswith(file_ext)])\n    else:\n        return np.array([os.path.join(root, name)\n            for root, dirs, files in os.walk(file_dir) \n            for name in sorted(files) if name.endswith(file_ext) and sub_name in name])\n'"
src/utils/utils.py,0,"b'#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n# File: utils.py\n# Author: Qian Ge <geqian1001@gmail.com>\n\n\ndef make_list(inputs):\n    if not isinstance(inputs, list):\n        return [inputs]\n    else:\n        return inputs\n\ndef assert_len(check_list):\n    for ele in check_list[1:]:\n        assert len(check_list[0]) == len(ele)'"
src/utils/viz.py,0,"b'#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n# File: viz.py\n# Author: Qian Ge <geqian1001@gmail.com>\n\nimport numpy as np\nimport imageio\n\n\ndef viz_batch_im(batch_im, grid_size, save_path,\n                gap=0, gap_color=0, shuffle=False):\n\n    batch_im = np.array(batch_im)\n    if len(batch_im.shape) == 4:\n        n_channel = batch_im.shape[-1]\n    elif len(batch_im.shape) == 3:\n        n_channel = 1\n        batch_im = np.expand_dims(batch_im, axis=-1)\n    assert len(grid_size) == 2\n\n    h = batch_im.shape[1]\n    w = batch_im.shape[2]\n\n    merge_im = np.zeros((h * grid_size[0] + (grid_size[0] + 1) * gap,\n                         w * grid_size[1] + (grid_size[1] + 1) * gap,\n                         n_channel)) + gap_color\n\n    n_viz_filter = min(batch_im.shape[0], grid_size[0] * grid_size[1])\n    if shuffle == True:\n        pick_id = np.random.permutation(batch_im.shape[0])\n    else:\n        pick_id = range(0, batch_im.shape[0])\n    for idx in range(0, n_viz_filter):\n        i = idx % grid_size[1]\n        j = idx // grid_size[1]\n        cur_filter = batch_im[pick_id[idx], :, :, :]\n        merge_im[j * (h + gap) + gap: j * (h + gap) + h + gap,\n                 i * (w + gap) + gap: i * (w + gap) + w + gap, :]\\\n            = (cur_filter)\n    imageio.imwrite(save_path, np.squeeze(merge_im))\n\n\n'"
