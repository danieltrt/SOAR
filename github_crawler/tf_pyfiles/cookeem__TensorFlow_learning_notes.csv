file_path,api_count,code
Chapter08/reader.py,12,"b'# Copyright 2015 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n\n""""""Utilities for parsing PTB text files.""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport collections\nimport os\n\nimport tensorflow as tf\n\n\ndef _read_words(filename):\n  with tf.gfile.GFile(filename, ""r"") as f:\n    return f.read().decode(""utf-8"").replace(""\\n"", ""<eos>"").split()\n\n\ndef _build_vocab(filename):\n  data = _read_words(filename)\n\n  counter = collections.Counter(data)\n  count_pairs = sorted(counter.items(), key=lambda x: (-x[1], x[0]))\n\n  words, _ = list(zip(*count_pairs))\n  word_to_id = dict(zip(words, range(len(words))))\n\n  return word_to_id\n\n\ndef _file_to_word_ids(filename, word_to_id):\n  data = _read_words(filename)\n  return [word_to_id[word] for word in data if word in word_to_id]\n\n\ndef ptb_raw_data(data_path=None):\n  """"""Load PTB raw data from data directory ""data_path"".\n\n  Reads PTB text files, converts strings to integer ids,\n  and performs mini-batching of the inputs.\n\n  The PTB dataset comes from Tomas Mikolov\'s webpage:\n\n  http://www.fit.vutbr.cz/~imikolov/rnnlm/simple-examples.tgz\n\n  Args:\n    data_path: string path to the directory where simple-examples.tgz has\n      been extracted.\n\n  Returns:\n    tuple (train_data, valid_data, test_data, vocabulary)\n    where each of the data objects can be passed to PTBIterator.\n  """"""\n\n  train_path = os.path.join(data_path, ""ptb.train.txt"")\n  valid_path = os.path.join(data_path, ""ptb.valid.txt"")\n  test_path = os.path.join(data_path, ""ptb.test.txt"")\n\n  word_to_id = _build_vocab(train_path)\n  train_data = _file_to_word_ids(train_path, word_to_id)\n  valid_data = _file_to_word_ids(valid_path, word_to_id)\n  test_data = _file_to_word_ids(test_path, word_to_id)\n  vocabulary = len(word_to_id)\n  return train_data, valid_data, test_data, vocabulary\n\n\ndef ptb_producer(raw_data, batch_size, num_steps, name=None):\n  """"""Iterate on the raw PTB data.\n\n  This chunks up raw_data into batches of examples and returns Tensors that\n  are drawn from these batches.\n\n  Args:\n    raw_data: one of the raw data outputs from ptb_raw_data.\n    batch_size: int, the batch size.\n    num_steps: int, the number of unrolls.\n    name: the name of this operation (optional).\n\n  Returns:\n    A pair of Tensors, each shaped [batch_size, num_steps]. The second element\n    of the tuple is the same data time-shifted to the right by one.\n\n  Raises:\n    tf.errors.InvalidArgumentError: if batch_size or num_steps are too high.\n  """"""\n  with tf.name_scope(name, ""PTBProducer"", [raw_data, batch_size, num_steps]):\n    raw_data = tf.convert_to_tensor(raw_data, name=""raw_data"", dtype=tf.int32)\n\n    data_len = tf.size(raw_data)\n    batch_len = data_len // batch_size\n    data = tf.reshape(raw_data[0 : batch_size * batch_len],\n                      [batch_size, batch_len])\n\n    epoch_size = (batch_len - 1) // num_steps\n    assertion = tf.assert_positive(\n        epoch_size,\n        message=""epoch_size == 0, decrease batch_size or num_steps"")\n    with tf.control_dependencies([assertion]):\n      epoch_size = tf.identity(epoch_size, name=""epoch_size"")\n\n    i = tf.train.range_input_producer(epoch_size, shuffle=False).dequeue()\n    x = tf.strided_slice(data, [0, i * num_steps],\n                         [batch_size, (i + 1) * num_steps])\n    x.set_shape([batch_size, num_steps])\n    y = tf.strided_slice(data, [0, i * num_steps + 1],\n                         [batch_size, (i + 1) * num_steps + 1])\n    y.set_shape([batch_size, num_steps])\n    return x, y\n'"
Chapter09/mnist_inference.py,8,"b'import tensorflow as tf\n\nINPUT_NODE = 784\nOUTPUT_NODE = 10\nLAYER1_NODE = 500\n\ndef get_weight_variable(shape, regularizer):\n    weights = tf.get_variable(""weights"", shape, initializer=tf.truncated_normal_initializer(stddev=0.1))\n    if regularizer != None: tf.add_to_collection(\'losses\', regularizer(weights))\n    return weights\n\n\ndef inference(input_tensor, regularizer):\n    with tf.variable_scope(\'layer1\'):\n\n        weights = get_weight_variable([INPUT_NODE, LAYER1_NODE], regularizer)\n        biases = tf.get_variable(""biases"", [LAYER1_NODE], initializer=tf.constant_initializer(0.0))\n        layer1 = tf.nn.relu(tf.matmul(input_tensor, weights) + biases)\n\n    with tf.variable_scope(\'layer2\'):\n        weights = get_weight_variable([LAYER1_NODE, OUTPUT_NODE], regularizer)\n        biases = tf.get_variable(""biases"", [OUTPUT_NODE], initializer=tf.constant_initializer(0.0))\n        layer2 = tf.matmul(layer1, weights) + biases\n\n    return layer2'"
Chapter10/1. GPU基本操作.py,14,"b'# coding=utf-8\nimport tensorflow as tf\n\na = tf.constant([1.0, 2.0, 3.0], shape=[3], name=\'a\')\nb = tf.constant([1.0, 2.0, 3.0], shape=[3], name=\'b\')\nc = a + b\n\n# \xe9\x80\x9a\xe8\xbf\x87log_device_placement\xe5\x8f\x82\xe6\x95\xb0\xe6\x9d\xa5\xe8\xae\xb0\xe5\xbd\x95\xe8\xbf\x90\xe8\xa1\x8c\xe6\xaf\x8f\xe4\xb8\x80\xe4\xb8\xaa\xe8\xbf\x90\xe7\xae\x97\xe7\x9a\x84\xe8\xae\xbe\xe5\xa4\x87\xe3\x80\x82\nsess = tf.Session(config=tf.ConfigProto(log_device_placement=True))\nprint sess.run(c)\n\n\n# \xe9\x80\x9a\xe8\xbf\x87tf.device\xe5\xb0\x86\xe8\xbf\x90\xe7\xae\x97\xe6\x8c\x87\xe5\xae\x9a\xe5\x88\xb0\xe7\x89\xb9\xe5\xae\x9a\xe7\x9a\x84\xe8\xae\xbe\xe5\xa4\x87\xe4\xb8\x8a\xe3\x80\x82\nwith tf.device(\'/cpu:0\'):\n\ta = tf.constant([1.0, 2.0, 3.0], shape=[3], name=\'a\')\n\tb = tf.constant([1.0, 2.0, 3.0], shape=[3], name=\'b\')\nwith tf.device(\'/gpu:1\'):\n    c = a + b\n\nsess = tf.Session(config=tf.ConfigProto(log_device_placement=True))\nprint sess.run(c)\n\n\na_cpu = tf.Variable(0, name=""a_cpu"")\nwith tf.device(\'/gpu:0\'):\n\ta_gpu = tf.Variable(0, name=""a_gpu"")\n\t# \xe9\x80\x9a\xe8\xbf\x87allow_soft_placement\xe5\x8f\x82\xe6\x95\xb0\xe8\x87\xaa\xe5\x8a\xa8\xe5\xb0\x86\xe6\x97\xa0\xe6\xb3\x95\xe6\x94\xbe\xe5\x9c\xa8GPU\xe4\xb8\x8a\xe7\x9a\x84\xe6\x93\x8d\xe4\xbd\x9c\xe6\x94\xbe\xe5\x9b\x9eCPU\xe4\xb8\x8a\xe3\x80\x82\nsess = tf.Session(config=tf.ConfigProto(allow_soft_placement=True, log_device_placement=True))\nsess.run(tf.global_variables_initializer())\n\n\n'"
Chapter10/2. 多GPU并行.py,38,"b'# coding=utf-8\nfrom datetime import datetime\nimport os\nimport time\n\nimport tensorflow as tf\nimport mnist_inference\n\n# \xe5\xae\x9a\xe4\xb9\x89\xe8\xae\xad\xe7\xbb\x83\xe7\xa5\x9e\xe7\xbb\x8f\xe7\xbd\x91\xe7\xbb\x9c\xe6\x97\xb6\xe9\x9c\x80\xe8\xa6\x81\xe7\x94\xa8\xe5\x88\xb0\xe7\x9a\x84\xe5\x8f\x82\xe6\x95\xb0\xe3\x80\x82\nBATCH_SIZE = 100 \nLEARNING_RATE_BASE = 0.001\nLEARNING_RATE_DECAY = 0.99\nREGULARAZTION_RATE = 0.0001\nTRAINING_STEPS = 1000\nMOVING_AVERAGE_DECAY = 0.99 \nN_GPU = 4\n\n# \xe5\xae\x9a\xe4\xb9\x89\xe6\x97\xa5\xe5\xbf\x97\xe5\x92\x8c\xe6\xa8\xa1\xe5\x9e\x8b\xe8\xbe\x93\xe5\x87\xba\xe7\x9a\x84\xe8\xb7\xaf\xe5\xbe\x84\xe3\x80\x82\nMODEL_SAVE_PATH = ""logs_and_models/""\nMODEL_NAME = ""model.ckpt""\nDATA_PATH = ""output.tfrecords"" \n\n# \xe5\xae\x9a\xe4\xb9\x89\xe8\xbe\x93\xe5\x85\xa5\xe9\x98\x9f\xe5\x88\x97\xe5\xbe\x97\xe5\x88\xb0\xe8\xae\xad\xe7\xbb\x83\xe6\x95\xb0\xe6\x8d\xae\xef\xbc\x8c\xe5\x85\xb7\xe4\xbd\x93\xe7\xbb\x86\xe8\x8a\x82\xe5\x8f\xaf\xe4\xbb\xa5\xe5\x8f\x82\xe8\x80\x83\xe7\xac\xac\xe4\xb8\x83\xe7\xab\xa0\xe3\x80\x82\ndef get_input():\n    filename_queue = tf.train.string_input_producer([DATA_PATH]) \n    reader = tf.TFRecordReader()\n\t_, serialized_example = reader.read(filename_queue)\n\n\t# \xe5\xae\x9a\xe4\xb9\x89\xe6\x95\xb0\xe6\x8d\xae\xe8\xa7\xa3\xe6\x9e\x90\xe6\xa0\xbc\xe5\xbc\x8f\xe3\x80\x82\n    features = tf.parse_single_example(\n        serialized_example,\n        features={\n            \'image_raw\': tf.FixedLenFeature([], tf.string),\n            \'pixels\': tf.FixedLenFeature([], tf.int64),\n            \'label\': tf.FixedLenFeature([], tf.int64),\n        })\n\n    # \xe8\xa7\xa3\xe6\x9e\x90\xe5\x9b\xbe\xe7\x89\x87\xe5\x92\x8c\xe6\xa0\x87\xe7\xad\xbe\xe4\xbf\xa1\xe6\x81\xaf\xe3\x80\x82\n    decoded_image = tf.decode_raw(features[\'image_raw\'], tf.uint8)\n    reshaped_image = tf.reshape(decoded_image, [784])\n    retyped_image = tf.cast(reshaped_image, tf.float32)\n    label = tf.cast(features[\'label\'], tf.int32)\n    \n    # \xe5\xae\x9a\xe4\xb9\x89\xe8\xbe\x93\xe5\x85\xa5\xe9\x98\x9f\xe5\x88\x97\xe5\xb9\xb6\xe8\xbf\x94\xe5\x9b\x9e\xe3\x80\x82\n    min_after_dequeue = 10000\n    capacity = min_after_dequeue + 3 * BATCH_SIZE\n\treturn tf.train.shuffle_batch(\n    \t[retyped_image, label], \n    \tbatch_size=BATCH_SIZE, \n    \tcapacity=capacity, \n    \tmin_after_dequeue=min_after_dequeue)\n\n# \xe5\xae\x9a\xe4\xb9\x89\xe6\x8d\x9f\xe5\xa4\xb1\xe5\x87\xbd\xe6\x95\xb0\xe3\x80\x82\ndef get_loss(x, y_, regularizer, scope, reuse_variables=None):\n    with tf.variable_scope(tf.get_variable_scope(), reuse=reuse_variables):\n        y = mnist_inference.inference(x, regularizer)\n    cross_entropy = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(y, y_))\n    regularization_loss = tf.add_n(tf.get_collection(\'losses\', scope))\n    loss = cross_entropy + regularization_loss\n    return loss\n\n# \xe8\xae\xa1\xe7\xae\x97\xe6\xaf\x8f\xe4\xb8\x80\xe4\xb8\xaa\xe5\x8f\x98\xe9\x87\x8f\xe6\xa2\xaf\xe5\xba\xa6\xe7\x9a\x84\xe5\xb9\xb3\xe5\x9d\x87\xe5\x80\xbc\xe3\x80\x82\ndef average_gradients(tower_grads):\n    average_grads = []\n\n    # \xe6\x9e\x9a\xe4\xb8\xbe\xe6\x89\x80\xe6\x9c\x89\xe7\x9a\x84\xe5\x8f\x98\xe9\x87\x8f\xe5\x92\x8c\xe5\x8f\x98\xe9\x87\x8f\xe5\x9c\xa8\xe4\xb8\x8d\xe5\x90\x8cGPU\xe4\xb8\x8a\xe8\xae\xa1\xe7\xae\x97\xe5\xbe\x97\xe5\x87\xba\xe7\x9a\x84\xe6\xa2\xaf\xe5\xba\xa6\xe3\x80\x82\n    for grad_and_vars in zip(*tower_grads):\n        # \xe8\xae\xa1\xe7\xae\x97\xe6\x89\x80\xe6\x9c\x89GPU\xe4\xb8\x8a\xe7\x9a\x84\xe6\xa2\xaf\xe5\xba\xa6\xe5\xb9\xb3\xe5\x9d\x87\xe5\x80\xbc\xe3\x80\x82\n        grads = []\n        for g, _ in grad_and_vars:\n            expanded_g = tf.expand_dims(g, 0)\n            grads.append(expanded_g)\n        grad = tf.concat(grads, 0)\n        grad = tf.reduce_mean(grad, 0)\n\n        v = grad_and_vars[0][1]\n        grad_and_var = (grad, v)\n        # \xe5\xb0\x86\xe5\x8f\x98\xe9\x87\x8f\xe5\x92\x8c\xe5\xae\x83\xe7\x9a\x84\xe5\xb9\xb3\xe5\x9d\x87\xe6\xa2\xaf\xe5\xba\xa6\xe5\xaf\xb9\xe5\xba\x94\xe8\xb5\xb7\xe6\x9d\xa5\xe3\x80\x82\n        average_grads.append(grad_and_var)\n    # \xe8\xbf\x94\xe5\x9b\x9e\xe6\x89\x80\xe6\x9c\x89\xe5\x8f\x98\xe9\x87\x8f\xe7\x9a\x84\xe5\xb9\xb3\xe5\x9d\x87\xe6\xa2\xaf\xe5\xba\xa6\xef\xbc\x8c\xe8\xbf\x99\xe4\xb8\xaa\xe5\xb0\x86\xe8\xa2\xab\xe7\x94\xa8\xe4\xba\x8e\xe5\x8f\x98\xe9\x87\x8f\xe7\x9a\x84\xe6\x9b\xb4\xe6\x96\xb0\xe3\x80\x82\n    return average_grads\n\n# \xe4\xb8\xbb\xe8\xae\xad\xe7\xbb\x83\xe8\xbf\x87\xe7\xa8\x8b\xe3\x80\x82\ndef main(argv=None): \n    # \xe5\xb0\x86\xe7\xae\x80\xe5\x8d\x95\xe7\x9a\x84\xe8\xbf\x90\xe7\xae\x97\xe6\x94\xbe\xe5\x9c\xa8CPU\xe4\xb8\x8a\xef\xbc\x8c\xe5\x8f\xaa\xe6\x9c\x89\xe7\xa5\x9e\xe7\xbb\x8f\xe7\xbd\x91\xe7\xbb\x9c\xe7\x9a\x84\xe8\xae\xad\xe7\xbb\x83\xe8\xbf\x87\xe7\xa8\x8b\xe6\x94\xbe\xe5\x9c\xa8GPU\xe4\xb8\x8a\xe3\x80\x82\n\twith tf.Graph().as_default(), tf.device(\'/cpu:0\'):\n \t\t\n \t\t# \xe5\xae\x9a\xe4\xb9\x89\xe5\x9f\xba\xe6\x9c\xac\xe7\x9a\x84\xe8\xae\xad\xe7\xbb\x83\xe8\xbf\x87\xe7\xa8\x8b\n        x, y_ = get_input()\n        regularizer = tf.contrib.layers.l2_regularizer(REGULARAZTION_RATE)\n        \n        global_step = tf.get_variable(\'global_step\', [], initializer=tf.constant_initializer(0), trainable=False)\n        learning_rate = tf.train.exponential_decay(\n            LEARNING_RATE_BASE, global_step, 60000 / BATCH_SIZE, LEARNING_RATE_DECAY)       \n        \n        opt = tf.train.GradientDescentOptimizer(learning_rate)\n        \n        tower_grads = []\n        reuse_variables = False\n        # \xe5\xb0\x86\xe7\xa5\x9e\xe7\xbb\x8f\xe7\xbd\x91\xe7\xbb\x9c\xe7\x9a\x84\xe4\xbc\x98\xe5\x8c\x96\xe8\xbf\x87\xe7\xa8\x8b\xe8\xb7\x91\xe5\x9c\xa8\xe4\xb8\x8d\xe5\x90\x8c\xe7\x9a\x84GPU\xe4\xb8\x8a\xe3\x80\x82\n        for i in range(N_GPU):\n            # \xe5\xb0\x86\xe4\xbc\x98\xe5\x8c\x96\xe8\xbf\x87\xe7\xa8\x8b\xe6\x8c\x87\xe5\xae\x9a\xe5\x9c\xa8\xe4\xb8\x80\xe4\xb8\xaaGPU\xe4\xb8\x8a\xe3\x80\x82\n            with tf.device(\'/gpu:%d\' % i):\n                with tf.name_scope(\'GPU_%d\' % i) as scope:\n                    cur_loss = get_loss(x, y_, regularizer, scope, reuse_variables)\n                    reuse_variables = True\n                    grads = opt.compute_gradients(cur_loss)\n                    tower_grads.append(grads)\n        \n        # \xe8\xae\xa1\xe7\xae\x97\xe5\x8f\x98\xe9\x87\x8f\xe7\x9a\x84\xe5\xb9\xb3\xe5\x9d\x87\xe6\xa2\xaf\xe5\xba\xa6\xe3\x80\x82\n        grads = average_gradients(tower_grads)\n        for grad, var in grads:\n            if grad is not None:\n            \ttf.histogram_summary(\'gradients_on_average/%s\' % var.op.name, grad)\n\n        # \xe4\xbd\xbf\xe7\x94\xa8\xe5\xb9\xb3\xe5\x9d\x87\xe6\xa2\xaf\xe5\xba\xa6\xe6\x9b\xb4\xe6\x96\xb0\xe5\x8f\x82\xe6\x95\xb0\xe3\x80\x82\n        apply_gradient_op = opt.apply_gradients(grads, global_step=global_step)\n        for var in tf.trainable_variables():\n            tf.histogram_summary(var.op.name, var)\n\n        # \xe8\xae\xa1\xe7\xae\x97\xe5\x8f\x98\xe9\x87\x8f\xe7\x9a\x84\xe6\xbb\x91\xe5\x8a\xa8\xe5\xb9\xb3\xe5\x9d\x87\xe5\x80\xbc\xe3\x80\x82\n        variable_averages = tf.train.ExponentialMovingAverage(MOVING_AVERAGE_DECAY, global_step)\n        variables_to_average = (tf.trainable_variables() +tf.moving_average_variables())\n        variables_averages_op = variable_averages.apply(variables_to_average)\n        # \xe6\xaf\x8f\xe4\xb8\x80\xe8\xbd\xae\xe8\xbf\xad\xe4\xbb\xa3\xe9\x9c\x80\xe8\xa6\x81\xe6\x9b\xb4\xe6\x96\xb0\xe5\x8f\x98\xe9\x87\x8f\xe7\x9a\x84\xe5\x8f\x96\xe5\x80\xbc\xe5\xb9\xb6\xe6\x9b\xb4\xe6\x96\xb0\xe5\x8f\x98\xe9\x87\x8f\xe7\x9a\x84\xe6\xbb\x91\xe5\x8a\xa8\xe5\xb9\xb3\xe5\x9d\x87\xe5\x80\xbc\xe3\x80\x82\n        train_op = tf.group(apply_gradient_op, variables_averages_op)\n\n        saver = tf.train.Saver(tf.all_variables())\n        summary_op = tf.merge_all_summaries()        \n        init = tf.initialize_all_variables()\n        with tf.Session(config=tf.ConfigProto(\n                allow_soft_placement=True, log_device_placement=True)) as sess:\n            # \xe5\x88\x9d\xe5\xa7\x8b\xe5\x8c\x96\xe6\x89\x80\xe6\x9c\x89\xe5\x8f\x98\xe9\x87\x8f\xe5\xb9\xb6\xe5\x90\xaf\xe5\x8a\xa8\xe9\x98\x9f\xe5\x88\x97\xe3\x80\x82\n            init.run()\n            coord = tf.train.Coordinator()\n            threads = tf.train.start_queue_runners(sess=sess, coord=coord)\n            summary_writer = tf.train.SummaryWriter(MODEL_SAVE_PATH, sess.graph)\n\n            for step in range(TRAINING_STEPS):\n                # \xe6\x89\xa7\xe8\xa1\x8c\xe7\xa5\x9e\xe7\xbb\x8f\xe7\xbd\x91\xe7\xbb\x9c\xe8\xae\xad\xe7\xbb\x83\xe6\x93\x8d\xe4\xbd\x9c\xef\xbc\x8c\xe5\xb9\xb6\xe8\xae\xb0\xe5\xbd\x95\xe8\xae\xad\xe7\xbb\x83\xe6\x93\x8d\xe4\xbd\x9c\xe7\x9a\x84\xe8\xbf\x90\xe8\xa1\x8c\xe6\x97\xb6\xe9\x97\xb4\xe3\x80\x82\n                start_time = time.time()\n                _, loss_value = sess.run([train_op, cur_loss])\n                duration = time.time() - start_time\n                \n                # \xe6\xaf\x8f\xe9\x9a\x94\xe4\xb8\x80\xe6\xae\xb5\xe6\x97\xb6\xe9\x97\xb4\xe6\x95\xb0\xe6\x8d\xae\xe5\xbd\x93\xe5\x89\x8d\xe7\x9a\x84\xe8\xae\xad\xe7\xbb\x83\xe8\xbf\x9b\xe5\xba\xa6\xef\xbc\x8c\xe5\xb9\xb6\xe7\xbb\x9f\xe8\xae\xa1\xe8\xae\xad\xe7\xbb\x83\xe9\x80\x9f\xe5\xba\xa6\xe3\x80\x82\n                if step != 0 and step % 10 == 0:\n                    # \xe8\xae\xa1\xe7\xae\x97\xe4\xbd\xbf\xe7\x94\xa8\xe8\xbf\x87\xe7\x9a\x84\xe8\xae\xad\xe7\xbb\x83\xe6\x95\xb0\xe6\x8d\xae\xe4\xb8\xaa\xe6\x95\xb0\xe3\x80\x82\n                    num_examples_per_step = BATCH_SIZE * N_GPU\n                    examples_per_sec = num_examples_per_step / duration\n                    sec_per_batch = duration / N_GPU\n    \n                    # \xe8\xbe\x93\xe5\x87\xba\xe8\xae\xad\xe7\xbb\x83\xe4\xbf\xa1\xe6\x81\xaf\xe3\x80\x82\n                    format_str = (\'%s: step %d, loss = %.2f (%.1f examples/sec; %.3f sec/batch)\')\n                    print (format_str % (datetime.now(), step, loss_value, examples_per_sec, sec_per_batch))\n                    \n                    # \xe9\x80\x9a\xe8\xbf\x87TensorBoard\xe5\x8f\xaf\xe8\xa7\x86\xe5\x8c\x96\xe8\xae\xad\xe7\xbb\x83\xe8\xbf\x87\xe7\xa8\x8b\xe3\x80\x82\n                    summary = sess.run(summary_op)\n                    summary_writer.add_summary(summary, step)\n    \n                # \xe6\xaf\x8f\xe9\x9a\x94\xe4\xb8\x80\xe6\xae\xb5\xe6\x97\xb6\xe9\x97\xb4\xe4\xbf\x9d\xe5\xad\x98\xe5\xbd\x93\xe5\x89\x8d\xe7\x9a\x84\xe6\xa8\xa1\xe5\x9e\x8b\xe3\x80\x82\n                if step % 1000 == 0 or (step + 1) == TRAINING_STEPS:\n                    checkpoint_path = os.path.join(MODEL_SAVE_PATH, MODEL_NAME)\n                    saver.save(sess, checkpoint_path, global_step=step)\n        \n            coord.request_stop()\n            coord.join(threads)\n        \nif __name__ == \'__main__\':\n\ttf.app.run()\n\n\n\n'"
Chapter10/3. 分布式TensorFlow.py,11,"b'import tensorflow as tf\n\n# \xe5\x88\x9b\xe5\xbb\xba\xe4\xb8\x80\xe4\xb8\xaa\xe6\x9c\xac\xe5\x9c\xb0\xe9\x9b\x86\xe7\xbe\xa4\xe3\x80\x82\nc = tf.constant(""Hello, distributed TensorFlow!"")\nserver = tf.train.Server.create_local_server()\nsess = tf.Session(server.target)\nprint sess.run(c)\n\n\n# \xe5\x88\x9b\xe5\xbb\xba\xe4\xb8\xa4\xe4\xb8\xaa\xe9\x9b\x86\xe7\xbe\xa4\nc = tf.constant(""Hello from server1!"")\ncluster = tf.train.ClusterSpec({""local"": [""localhost:2222"", ""localhost:2223""]})\nserver = tf.train.Server(cluster, job_name=""local"", task_index=0)\nsess = tf.Session(server.target, config=tf.ConfigProto(log_device_placement=True)) \nprint sess.run(c)\nserver.join()\n\n\nimport tensorflow as tf\nc = tf.constant(""Hello from server2!"")\ncluster = tf.train.ClusterSpec({""local"": [""localhost:2222"", ""localhost:2223""]})\nserver = tf.train.Server(cluster, job_name=""local"", task_index=1)\nsess = tf.Session(server.target, config=tf.ConfigProto(log_device_placement=True)) \nprint sess.run(c)\nserver.join()\n\n\n'"
Chapter10/4. 异步更新模式样例程序.py,33,"b'# coding=utf-8\nimport time\nimport tensorflow as tf\nfrom tensorflow.examples.tutorials.mnist import input_data\n\nimport mnist_inference\n\n# \xe9\x85\x8d\xe7\xbd\xae\xe7\xa5\x9e\xe7\xbb\x8f\xe7\xbd\x91\xe7\xbb\x9c\xe7\x9a\x84\xe5\x8f\x82\xe6\x95\xb0\xe3\x80\x82\nBATCH_SIZE = 100\nLEARNING_RATE_BASE = 0.01\nLEARNING_RATE_DECAY = 0.99\nREGULARAZTION_RATE = 0.0001\nTRAINING_STEPS = 1000\nMOVING_AVERAGE_DECAY = 0.99\n\n# \xe6\xa8\xa1\xe5\x9e\x8b\xe4\xbf\x9d\xe5\xad\x98\xe7\x9a\x84\xe8\xb7\xaf\xe5\xbe\x84\xe5\x92\x8c\xe6\x96\x87\xe4\xbb\xb6\xe5\x90\x8d\xe3\x80\x82\nMODEL_SAVE_PATH = ""logs/log_async""\nDATA_PATH = ""../../datasets/MNIST_data""\n\nFLAGS = tf.app.flags.FLAGS\n\n# \xe6\x8c\x87\xe5\xae\x9a\xe5\xbd\x93\xe5\x89\x8d\xe7\xa8\x8b\xe5\xba\x8f\xe6\x98\xaf\xe5\x8f\x82\xe6\x95\xb0\xe6\x9c\x8d\xe5\x8a\xa1\xe5\x99\xa8\xe8\xbf\x98\xe6\x98\xaf\xe8\xae\xa1\xe7\xae\x97\xe6\x9c\x8d\xe5\x8a\xa1\xe5\x99\xa8\xe3\x80\x82\ntf.app.flags.DEFINE_string(\'job_name\', \'worker\', \' ""ps"" or ""worker"" \')\n# \xe6\x8c\x87\xe5\xae\x9a\xe9\x9b\x86\xe7\xbe\xa4\xe4\xb8\xad\xe7\x9a\x84\xe5\x8f\x82\xe6\x95\xb0\xe6\x9c\x8d\xe5\x8a\xa1\xe5\x99\xa8\xe5\x9c\xb0\xe5\x9d\x80\xe3\x80\x82\ntf.app.flags.DEFINE_string(\n    \'ps_hosts\', \' tf-ps0:2222,tf-ps1:1111\',\n    \'Comma-separated list of hostname:port for the parameter server jobs. e.g. ""tf-ps0:2222,tf-ps1:1111"" \')\n# \xe6\x8c\x87\xe5\xae\x9a\xe9\x9b\x86\xe7\xbe\xa4\xe4\xb8\xad\xe7\x9a\x84\xe8\xae\xa1\xe7\xae\x97\xe6\x9c\x8d\xe5\x8a\xa1\xe5\x99\xa8\xe5\x9c\xb0\xe5\x9d\x80\xe3\x80\x82\ntf.app.flags.DEFINE_string(\n    \'worker_hosts\', \' tf-worker0:2222,tf-worker1:1111\',\n    \'Comma-separated list of hostname:port for the worker jobs. e.g. ""tf-worker0:2222,tf-worker1:1111"" \')\n# \xe6\x8c\x87\xe5\xae\x9a\xe5\xbd\x93\xe5\x89\x8d\xe7\xa8\x8b\xe5\xba\x8f\xe7\x9a\x84\xe4\xbb\xbb\xe5\x8a\xa1ID\xe3\x80\x82\ntf.app.flags.DEFINE_integer(\'task_id\', 0, \'Task ID of the worker/replica running the training.\')\n\n# \xe5\xae\x9a\xe4\xb9\x89TensorFlow\xe7\x9a\x84\xe8\xae\xa1\xe7\xae\x97\xe5\x9b\xbe\xef\xbc\x8c\xe5\xb9\xb6\xe8\xbf\x94\xe5\x9b\x9e\xe6\xaf\x8f\xe4\xb8\x80\xe8\xbd\xae\xe8\xbf\xad\xe4\xbb\xa3\xe6\x97\xb6\xe9\x9c\x80\xe8\xa6\x81\xe8\xbf\x90\xe8\xa1\x8c\xe7\x9a\x84\xe6\x93\x8d\xe4\xbd\x9c\xe3\x80\x82\ndef build_model(x, y_, is_chief):\n    regularizer = tf.contrib.layers.l2_regularizer(REGULARAZTION_RATE)\n    # \xe9\x80\x9a\xe8\xbf\x87\xe5\x92\x8c5.5\xe8\x8a\x82\xe7\xbb\x99\xe5\x87\xba\xe7\x9a\x84mnist_inference.py\xe4\xbb\xa3\xe7\xa0\x81\xe8\xae\xa1\xe7\xae\x97\xe7\xa5\x9e\xe7\xbb\x8f\xe7\xbd\x91\xe7\xbb\x9c\xe5\x89\x8d\xe5\x90\x91\xe4\xbc\xa0\xe6\x92\xad\xe7\x9a\x84\xe7\xbb\x93\xe6\x9e\x9c\xe3\x80\x82\n    y = mnist_inference.inference(x, regularizer)\n    global_step = tf.Variable(0, trainable=False)\n\n    # \xe8\xae\xa1\xe7\xae\x97\xe6\x8d\x9f\xe5\xa4\xb1\xe5\x87\xbd\xe6\x95\xb0\xe5\xb9\xb6\xe5\xae\x9a\xe4\xb9\x89\xe5\x8f\x8d\xe5\x90\x91\xe4\xbc\xa0\xe6\x92\xad\xe8\xbf\x87\xe7\xa8\x8b\xe3\x80\x82\n    cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=y, labels=tf.argmax(y_, 1))\n    cross_entropy_mean = tf.reduce_mean(cross_entropy)\n    loss = cross_entropy_mean + tf.add_n(tf.get_collection(\'losses\'))\n    learning_rate = tf.train.exponential_decay(\n        LEARNING_RATE_BASE, global_step, 60000 / BATCH_SIZE, LEARNING_RATE_DECAY)\n    train_op = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step=global_step)\n\n    # \xe5\xae\x9a\xe4\xb9\x89\xe6\xaf\x8f\xe4\xb8\x80\xe8\xbd\xae\xe8\xbf\xad\xe4\xbb\xa3\xe9\x9c\x80\xe8\xa6\x81\xe8\xbf\x90\xe8\xa1\x8c\xe7\x9a\x84\xe6\x93\x8d\xe4\xbd\x9c\xe3\x80\x82\n    if is_chief:\n        # \xe8\xae\xa1\xe7\xae\x97\xe5\x8f\x98\xe9\x87\x8f\xe7\x9a\x84\xe6\xbb\x91\xe5\x8a\xa8\xe5\xb9\xb3\xe5\x9d\x87\xe5\x80\xbc\xe3\x80\x82   \n        variable_averages = tf.train.ExponentialMovingAverage(MOVING_AVERAGE_DECAY, global_step)\n        variables_averages_op = variable_averages.apply(tf.trainable_variables())\n        with tf.control_dependencies([variables_averages_op, train_op]):\n            train_op = tf.no_op()\n    return global_step, loss, train_op\n\n\ndef main(argv=None):\n    # \xe8\xa7\xa3\xe6\x9e\x90flags\xe5\xb9\xb6\xe9\x80\x9a\xe8\xbf\x87tf.train.ClusterSpec\xe9\x85\x8d\xe7\xbd\xaeTensorFlow\xe9\x9b\x86\xe7\xbe\xa4\xe3\x80\x82\n    ps_hosts = FLAGS.ps_hosts.split(\',\')\n    worker_hosts = FLAGS.worker_hosts.split(\',\')\n    cluster = tf.train.ClusterSpec({""ps"": ps_hosts, ""worker"": worker_hosts})\n    # \xe9\x80\x9a\xe8\xbf\x87tf.train.ClusterSpec\xe4\xbb\xa5\xe5\x8f\x8a\xe5\xbd\x93\xe5\x89\x8d\xe4\xbb\xbb\xe5\x8a\xa1\xe5\x88\x9b\xe5\xbb\xbatf.train.Server\xe3\x80\x82\n    server = tf.train.Server(cluster, job_name = FLAGS.job_name, task_index=FLAGS.task_id)\n\n    # \xe5\x8f\x82\xe6\x95\xb0\xe6\x9c\x8d\xe5\x8a\xa1\xe5\x99\xa8\xe5\x8f\xaa\xe9\x9c\x80\xe8\xa6\x81\xe7\xae\xa1\xe7\x90\x86TensorFlow\xe4\xb8\xad\xe7\x9a\x84\xe5\x8f\x98\xe9\x87\x8f\xef\xbc\x8c\xe4\xb8\x8d\xe9\x9c\x80\xe8\xa6\x81\xe6\x89\xa7\xe8\xa1\x8c\xe8\xae\xad\xe7\xbb\x83\xe7\x9a\x84\xe8\xbf\x87\xe7\xa8\x8b\xe3\x80\x82server.join()\xe4\xbc\x9a\n    # \xe4\xb8\x80\xe8\x87\xb4\xe5\x81\x9c\xe5\x9c\xa8\xe8\xbf\x99\xe6\x9d\xa1\xe8\xaf\xad\xe5\x8f\xa5\xe4\xb8\x8a\xe3\x80\x82\n    if FLAGS.job_name == \'ps\':\n        with tf.device(""/cpu:0""):\n            server.join()\n\n    # \xe5\xae\x9a\xe4\xb9\x89\xe8\xae\xa1\xe7\xae\x97\xe6\x9c\x8d\xe5\x8a\xa1\xe5\x99\xa8\xe9\x9c\x80\xe8\xa6\x81\xe8\xbf\x90\xe8\xa1\x8c\xe7\x9a\x84\xe6\x93\x8d\xe4\xbd\x9c\xe3\x80\x82\n    is_chief = (FLAGS.task_id == 0)\n    mnist = input_data.read_data_sets(DATA_PATH, one_hot=True)\n\n    device_setter = tf.train.replica_device_setter(worker_device=""/job:worker/task:%d"" % FLAGS.task_id, cluster=cluster)\n    with tf.device(device_setter):\n\n        # \xe5\xae\x9a\xe4\xb9\x89\xe8\xbe\x93\xe5\x85\xa5\xe5\xb9\xb6\xe5\xbe\x97\xe5\x88\xb0\xe6\xaf\x8f\xe4\xb8\x80\xe8\xbd\xae\xe8\xbf\xad\xe4\xbb\xa3\xe9\x9c\x80\xe8\xa6\x81\xe8\xbf\x90\xe8\xa1\x8c\xe7\x9a\x84\xe6\x93\x8d\xe4\xbd\x9c\xe3\x80\x82\n        x = tf.placeholder(tf.float32, [None, mnist_inference.INPUT_NODE], name=\'x-input\')\n        y_ = tf.placeholder(tf.float32, [None, mnist_inference.OUTPUT_NODE], name=\'y-input\')\n        global_step, loss, train_op = build_model(x, y_, is_chief)\n\n        # \xe5\xae\x9a\xe4\xb9\x89\xe7\x94\xa8\xe4\xba\x8e\xe4\xbf\x9d\xe5\xad\x98\xe6\xa8\xa1\xe5\x9e\x8b\xe7\x9a\x84saver\xe3\x80\x82\n        saver = tf.train.Saver()\n        # \xe5\xae\x9a\xe4\xb9\x89\xe6\x97\xa5\xe5\xbf\x97\xe8\xbe\x93\xe5\x87\xba\xe6\x93\x8d\xe4\xbd\x9c\xe3\x80\x82\n        summary_op = tf.summary.merge_all()\n        # \xe5\xae\x9a\xe4\xb9\x89\xe5\x8f\x98\xe9\x87\x8f\xe5\x88\x9d\xe5\xa7\x8b\xe5\x8c\x96\xe6\x93\x8d\xe4\xbd\x9c\xe3\x80\x82\n        init_op = tf.global_variables_initializer()\n        # \xe9\x80\x9a\xe8\xbf\x87tf.train.Supervisor\xe7\xae\xa1\xe7\x90\x86\xe8\xae\xad\xe7\xbb\x83\xe6\xb7\xb1\xe5\xba\xa6\xe5\xad\xa6\xe4\xb9\xa0\xe6\xa8\xa1\xe5\x9e\x8b\xe6\x97\xb6\xe7\x9a\x84\xe9\x80\x9a\xe7\x94\xa8\xe5\x8a\x9f\xe8\x83\xbd\xe3\x80\x82\n        sv = tf.train.Supervisor(\n            is_chief=is_chief,\n            logdir=MODEL_SAVE_PATH,\n            init_op=init_op,\n            summary_op=summary_op,\n            saver=saver,\n            global_step=global_step,\n            save_model_secs=60,\n            save_summaries_secs=60)\n\n        sess_config = tf.ConfigProto(allow_soft_placement=True, log_device_placement=False)\n        # \xe9\x80\x9a\xe8\xbf\x87tf.train.Supervisor\xe7\x94\x9f\xe6\x88\x90\xe4\xbc\x9a\xe8\xaf\x9d\xe3\x80\x82\n        sess = sv.prepare_or_wait_for_session(server.target, config=sess_config)\n\n        step = 0\n        start_time = time.time()\n\n        # \xe6\x89\xa7\xe8\xa1\x8c\xe8\xbf\xad\xe4\xbb\xa3\xe8\xbf\x87\xe7\xa8\x8b\xe3\x80\x82\n        while not sv.should_stop():\n            xs, ys = mnist.train.next_batch(BATCH_SIZE)\n            _, loss_value, global_step_value = sess.run([train_op, loss, global_step], feed_dict={x: xs, y_: ys})\n            if global_step_value >= TRAINING_STEPS: break\n\n           # \xe6\xaf\x8f\xe9\x9a\x94\xe4\xb8\x80\xe6\xae\xb5\xe6\x97\xb6\xe9\x97\xb4\xe8\xbe\x93\xe5\x87\xba\xe8\xae\xad\xe7\xbb\x83\xe4\xbf\xa1\xe6\x81\xaf\xe3\x80\x82\n            if step > 0 and step % 100 == 0:\n                duration = time.time() - start_time\n                sec_per_batch = duration / global_step_value\n                format_str = ""After %d training steps (%d global steps), loss on training batch is %g.  (%.3f sec/batch)""\n                print format_str % (step, global_step_value, loss_value, sec_per_batch)\n            step += 1\n    sv.stop()\n\nif __name__ == ""__main__"":\n    tf.app.run()\n\n\n\n\n'"
Chapter10/5. 同步更新模式样例程序.py,34,"b'# coding=utf-8\nimport time\nimport tensorflow as tf\nfrom tensorflow.examples.tutorials.mnist import input_data\n\nimport mnist_inference\n\n# \xe9\x85\x8d\xe7\xbd\xae\xe7\xa5\x9e\xe7\xbb\x8f\xe7\xbd\x91\xe7\xbb\x9c\xe7\x9a\x84\xe5\x8f\x82\xe6\x95\xb0\xe3\x80\x82\nBATCH_SIZE = 100 \nLEARNING_RATE_BASE = 0.8\nLEARNING_RATE_DECAY = 0.99\nREGULARAZTION_RATE = 0.0001\nTRAINING_STEPS = 10000\nMOVING_AVERAGE_DECAY = 0.99 \nMODEL_SAVE_PATH = ""logs/log_sync""\nDATA_PATH = ""../../datasets/MNIST_data""\n\n\n# \xe5\x92\x8c\xe5\xbc\x82\xe6\xad\xa5\xe6\xa8\xa1\xe5\xbc\x8f\xe7\xb1\xbb\xe4\xbc\xbc\xe7\x9a\x84\xe8\xae\xbe\xe7\xbd\xaeflags\xe3\x80\x82\nFLAGS = tf.app.flags.FLAGS\n\ntf.app.flags.DEFINE_string(\'job_name\', \'worker\', \' ""ps"" or ""worker"" \')\ntf.app.flags.DEFINE_string(\n    \'ps_hosts\', \' tf-ps0:2222,tf-ps1:1111\',\n    \'Comma-separated list of hostname:port for the parameter server jobs. e.g. ""tf-ps0:2222,tf-ps1:1111"" \')\ntf.app.flags.DEFINE_string(\n    \'worker_hosts\', \' tf-worker0:2222,tf-worker1:1111\',\n\'Comma-separated list of hostname:port for the worker jobs. e.g. ""tf-worker0:2222,tf-worker1:1111"" \')\ntf.app.flags.DEFINE_integer(\'task_id\', 0, \'Task ID of the worker/replica running the training.\')\n\n# \xe5\x92\x8c\xe5\xbc\x82\xe6\xad\xa5\xe6\xa8\xa1\xe5\xbc\x8f\xe7\xb1\xbb\xe4\xbc\xbc\xe7\x9a\x84\xe5\xae\x9a\xe4\xb9\x89TensorFlow\xe7\x9a\x84\xe8\xae\xa1\xe7\xae\x97\xe5\x9b\xbe\xe3\x80\x82\xe5\x94\xaf\xe4\xb8\x80\xe7\x9a\x84\xe5\x8c\xba\xe5\x88\xab\xe5\x9c\xa8\xe4\xba\x8e\xe4\xbd\xbf\xe7\x94\xa8\n# tf.train.SyncReplicasOptimizer\xe5\x87\xbd\xe6\x95\xb0\xe5\xa4\x84\xe7\x90\x86\xe5\x90\x8c\xe6\xad\xa5\xe6\x9b\xb4\xe6\x96\xb0\xe3\x80\x82\ndef build_model(x, y_, n_workers, is_chief):\n    regularizer = tf.contrib.layers.l2_regularizer(REGULARAZTION_RATE)\n    y = mnist_inference.inference(x, regularizer)\n    global_step = tf.Variable(0, trainable=False)\n\n    variable_averages = tf.train.ExponentialMovingAverage(MOVING_AVERAGE_DECAY, global_step)\n    variables_averages_op = variable_averages.apply(tf.trainable_variables())\n\n    cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=y, labels=tf.argmax(y_, 1))\n    cross_entropy_mean = tf.reduce_mean(cross_entropy)\n    loss = cross_entropy_mean + tf.add_n(tf.get_collection(\'losses\'))\n    learning_rate = tf.train.exponential_decay(\n        LEARNING_RATE_BASE, global_step, 60000 / BATCH_SIZE, LEARNING_RATE_DECAY)\n   \n    # \xe9\x80\x9a\xe8\xbf\x87tf.train.SyncReplicasOptimizer\xe5\x87\xbd\xe6\x95\xb0\xe5\xae\x9e\xe7\x8e\xb0\xe5\x90\x8c\xe6\xad\xa5\xe6\x9b\xb4\xe6\x96\xb0\xe3\x80\x82\n    opt = tf.train.SyncReplicasOptimizer(\n        tf.train.GradientDescentOptimizer(learning_rate),\n        replicas_to_aggregate=n_workers,\n        total_num_replicas=n_workers)\n\n    train_op = opt.minimize(loss, global_step=global_step)     \n    if is_chief:\n        variable_averages = tf.train.ExponentialMovingAverage(MOVING_AVERAGE_DECAY, global_step)\n        variables_averages_op = variable_averages.apply(tf.trainable_variables())\n        with tf.control_dependencies([variables_averages_op, train_op]):\n            train_op = tf.no_op()\n\n    return global_step, loss, train_op, opt\n\ndef main(argv=None): \n    # \xe5\x92\x8c\xe5\xbc\x82\xe6\xad\xa5\xe6\xa8\xa1\xe5\xbc\x8f\xe7\xb1\xbb\xe4\xbc\xbc\xe7\x9a\x84\xe5\x88\x9b\xe5\xbb\xbaTensorFlow\xe9\x9b\x86\xe7\xbe\xa4\xe3\x80\x82\n    ps_hosts = FLAGS.ps_hosts.split(\',\')\n    worker_hosts = FLAGS.worker_hosts.split(\',\')\n    print (\'PS hosts are: %s\' % ps_hosts)\n    print (\'Worker hosts are: %s\' % worker_hosts)\n    n_workers = len(worker_hosts)\n\n    cluster = tf.train.ClusterSpec({""ps"": ps_hosts, ""worker"": worker_hosts})\n    server = tf.train.Server(\n        cluster, job_name = FLAGS.job_name, task_index=FLAGS.task_id)\n\n    if FLAGS.job_name == \'ps\':\n        with tf.device(""/cpu:0""):\n            server.join()\n\n    is_chief = (FLAGS.task_id == 0)\n    mnist = input_data.read_data_sets(DATA_PATH, one_hot=True)\n   \n    with tf.device(tf.train.replica_device_setter(\n            worker_device=""/job:worker/task:%d"" % FLAGS.task_id, cluster=cluster)):\n        x = tf.placeholder(tf.float32, [None, mnist_inference.INPUT_NODE], name=\'x-input\')\n        y_ = tf.placeholder(tf.float32, [None, mnist_inference.OUTPUT_NODE], name=\'y-input\')\n        global_step, loss, train_op, opt = build_model(x, y_, n_workers, is_chief)\n        # \xe5\x92\x8c\xe5\xbc\x82\xe6\xad\xa5\xe6\xa8\xa1\xe5\xbc\x8f\xe7\xb1\xbb\xe4\xbc\xbc\xe7\x9a\x84\xe5\xa3\xb0\xe6\x98\x8e\xe4\xb8\x80\xe4\xba\x9b\xe8\xbe\x85\xe5\x8a\xa9\xe5\x87\xbd\xe6\x95\xb0\xe3\x80\x82\n        saver = tf.train.Saver()\n        summary_op = tf.summary.merge_all()\n        init_op = tf.global_variables_initializer()\n\n        # \xe5\x9c\xa8\xe5\x90\x8c\xe6\xad\xa5\xe6\xa8\xa1\xe5\xbc\x8f\xe4\xb8\x8b\xef\xbc\x8c\xe4\xb8\xbb\xe8\xae\xa1\xe7\xae\x97\xe6\x9c\x8d\xe5\x8a\xa1\xe5\x99\xa8\xe9\x9c\x80\xe8\xa6\x81\xe5\x8d\x8f\xe8\xb0\x83\xe4\xb8\x8d\xe5\x90\x8c\xe8\xae\xa1\xe7\xae\x97\xe6\x9c\x8d\xe5\x8a\xa1\xe5\x99\xa8\xe8\xae\xa1\xe7\xae\x97\xe5\xbe\x97\xe5\x88\xb0\xe7\x9a\x84\xe5\x8f\x82\xe6\x95\xb0\xe6\xa2\xaf\xe5\xba\xa6\xe5\xb9\xb6\xe6\x9c\x80\xe7\xbb\x88\xe6\x9b\xb4\xe6\x96\xb0\xe5\x8f\x82\xe6\x95\xb0\xe3\x80\x82\n        # \xe8\xbf\x99\xe9\x9c\x80\xe8\xa6\x81\xe4\xb8\xbb\xe8\xae\xa1\xe7\xae\x97\xe6\x9c\x8d\xe5\x8a\xa1\xe5\x99\xa8\xe5\xae\x8c\xe6\x88\x90\xe4\xb8\x80\xe4\xba\x9b\xe9\xa2\x9d\xe5\xa4\x96\xe7\x9a\x84\xe5\x88\x9d\xe5\xa7\x8b\xe5\x8c\x96\xe5\xb7\xa5\xe4\xbd\x9c\xe3\x80\x82\n        if is_chief:\n            # \xe8\x8e\xb7\xe5\x8f\x96\xe5\x8d\x8f\xe8\xb0\x83\xe4\xb8\x8d\xe5\x90\x8c\xe8\xae\xa1\xe7\xae\x97\xe6\x9c\x8d\xe5\x8a\xa1\xe5\x99\xa8\xe7\x9a\x84\xe9\x98\x9f\xe5\x88\x97\xe3\x80\x82\xe5\x9c\xa8\xe6\x9b\xb4\xe6\x96\xb0\xe5\x8f\x82\xe6\x95\xb0\xe4\xb9\x8b\xe5\x89\x8d\xef\xbc\x8c\xe4\xb8\xbb\xe8\xae\xa1\xe7\xae\x97\xe6\x9c\x8d\xe5\x8a\xa1\xe5\x99\xa8\xe9\x9c\x80\xe8\xa6\x81\xe5\x85\x88\xe5\x90\xaf\xe5\x8a\xa8\xe8\xbf\x99\xe4\xba\x9b\xe9\x98\x9f\xe5\x88\x97\xe3\x80\x82\n            chief_queue_runner = opt.get_chief_queue_runner()\n            # \xe5\x88\x9d\xe5\xa7\x8b\xe5\x8c\x96\xe5\x90\x8c\xe6\xad\xa5\xe6\x9b\xb4\xe6\x96\xb0\xe9\x98\x9f\xe5\x88\x97\xe7\x9a\x84\xe6\x93\x8d\xe4\xbd\x9c\xe3\x80\x82\n            init_tokens_op = opt.get_init_tokens_op(0)\n     \n        # \xe5\x92\x8c\xe5\xbc\x82\xe6\xad\xa5\xe6\xa8\xa1\xe5\xbc\x8f\xe7\xb1\xbb\xe4\xbc\xbc\xe7\x9a\x84\xe5\xa3\xb0\xe6\x98\x8etf.train.Supervisor\xe3\x80\x82\n        sv = tf.train.Supervisor(is_chief=is_chief,\n                                logdir=MODEL_SAVE_PATH,\n                                init_op=init_op,\n                                summary_op=summary_op,\n                                saver = saver,\n                                global_step=global_step,\n                                save_model_secs=60,\n                                save_summaries_secs=60)\n        sess_config = tf.ConfigProto(allow_soft_placement=True, log_device_placement=False)\n        sess = sv.prepare_or_wait_for_session(server.target, config=sess_config)        \n\n        # \xe5\x9c\xa8\xe4\xb8\xbb\xe8\xae\xa1\xe7\xae\x97\xe6\x9c\x8d\xe5\x8a\xa1\xe5\x99\xa8\xe4\xb8\x8a\xe5\x90\xaf\xe5\x8a\xa8\xe5\x8d\x8f\xe8\xb0\x83\xe5\x90\x8c\xe6\xad\xa5\xe6\x9b\xb4\xe6\x96\xb0\xe7\x9a\x84\xe9\x98\x9f\xe5\x88\x97\xe5\xb9\xb6\xe6\x89\xa7\xe8\xa1\x8c\xe5\x88\x9d\xe5\xa7\x8b\xe5\x8c\x96\xe6\x93\x8d\xe4\xbd\x9c\xe3\x80\x82\n        if is_chief:\n            sv.start_queue_runners(sess, [chief_queue_runner])\n            sess.run(init_tokens_op)\n     \n        # \xe5\x92\x8c\xe5\xbc\x82\xe6\xad\xa5\xe6\xa8\xa1\xe5\xbc\x8f\xe7\xb1\xbb\xe4\xbc\xbc\xe7\x9a\x84\xe8\xbf\x90\xe8\xa1\x8c\xe8\xbf\xad\xe4\xbb\xa3\xe7\x9a\x84\xe8\xae\xad\xe7\xbb\x83\xe8\xbf\x87\xe7\xa8\x8b\xe3\x80\x82\n        step = 0\n        start_time = time.time()\n        while not sv.should_stop():\n            xs, ys = mnist.train.next_batch(BATCH_SIZE)\n            _, loss_value, global_step_value = sess.run([train_op, loss, global_step], feed_dict={x: xs, y_: ys})\n            if global_step_value >= TRAINING_STEPS: break\n\n            if step > 0 and step % 100 == 0:\n                duration = time.time() - start_time\n                sec_per_batch = duration / (global_step_value * n_workers)\n                format_str = ""After %d training steps (%d global steps), loss on training batch is %g.  (%.3f sec/batch)""\n                print format_str % (step, global_step_value, loss_value, sec_per_batch)\n            step += 1\n    sv.stop()\n       \nif __name__ == ""__main__"":\n    tf.app.run()\n'"
Chapter10/mnist_inference.py,8,"b'import tensorflow as tf\n\nINPUT_NODE = 784\nOUTPUT_NODE = 10\nLAYER1_NODE = 500\n\ndef get_weight_variable(shape, regularizer):\n    weights = tf.get_variable(""weights"", shape, initializer=tf.truncated_normal_initializer(stddev=0.1))\n    if regularizer != None: tf.add_to_collection(\'losses\', regularizer(weights))\n    return weights\n\n\ndef inference(input_tensor, regularizer):\n    with tf.variable_scope(\'layer1\'):\n\n        weights = get_weight_variable([INPUT_NODE, LAYER1_NODE], regularizer)\n        biases = tf.get_variable(""biases"", [LAYER1_NODE], initializer=tf.constant_initializer(0.0))\n        layer1 = tf.nn.relu(tf.matmul(input_tensor, weights) + biases)\n\n    with tf.variable_scope(\'layer2\'):\n        weights = get_weight_variable([LAYER1_NODE, OUTPUT_NODE], regularizer)\n        biases = tf.get_variable(""biases"", [OUTPUT_NODE], initializer=tf.constant_initializer(0.0))\n        layer2 = tf.matmul(layer1, weights) + biases\n\n    return layer2'"
Chapter05/5. MNIST最佳实践/mnist_inference.py,8,"b'import tensorflow as tf\n\nINPUT_NODE = 784\nOUTPUT_NODE = 10\nLAYER1_NODE = 500\n\ndef get_weight_variable(shape, regularizer):\n    weights = tf.get_variable(""weights"", shape, initializer=tf.truncated_normal_initializer(stddev=0.1))\n    if regularizer != None: tf.add_to_collection(\'losses\', regularizer(weights))\n    return weights\n\n\ndef inference(input_tensor, regularizer):\n    with tf.variable_scope(\'layer1\'):\n\n        weights = get_weight_variable([INPUT_NODE, LAYER1_NODE], regularizer)\n        biases = tf.get_variable(""biases"", [LAYER1_NODE], initializer=tf.constant_initializer(0.0))\n        layer1 = tf.nn.relu(tf.matmul(input_tensor, weights) + biases)\n\n    with tf.variable_scope(\'layer2\'):\n        weights = get_weight_variable([LAYER1_NODE, OUTPUT_NODE], regularizer)\n        biases = tf.get_variable(""biases"", [OUTPUT_NODE], initializer=tf.constant_initializer(0.0))\n        layer2 = tf.matmul(layer1, weights) + biases\n\n    return layer2'"
Chapter05/5. MNIST最佳实践/mnist_train.py,17,"b'import tensorflow as tf\nfrom tensorflow.examples.tutorials.mnist import input_data\nimport mnist_inference\nimport os\n\nBATCH_SIZE = 100\nLEARNING_RATE_BASE = 0.8\nLEARNING_RATE_DECAY = 0.99\nREGULARIZATION_RATE = 0.0001\nTRAINING_STEPS = 30000\nMOVING_AVERAGE_DECAY = 0.99\nMODEL_SAVE_PATH=""MNIST_model/""\nMODEL_NAME=""mnist_model""\n\n\ndef train(mnist):\n\n    x = tf.placeholder(tf.float32, [None, mnist_inference.INPUT_NODE], name=\'x-input\')\n    y_ = tf.placeholder(tf.float32, [None, mnist_inference.OUTPUT_NODE], name=\'y-input\')\n\n    regularizer = tf.contrib.layers.l2_regularizer(REGULARIZATION_RATE)\n    y = mnist_inference.inference(x, regularizer)\n    global_step = tf.Variable(0, trainable=False)\n\n\n    variable_averages = tf.train.ExponentialMovingAverage(MOVING_AVERAGE_DECAY, global_step)\n    variables_averages_op = variable_averages.apply(tf.trainable_variables())\n    cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=y, labels=tf.argmax(y_, 1))\n    cross_entropy_mean = tf.reduce_mean(cross_entropy)\n    loss = cross_entropy_mean + tf.add_n(tf.get_collection(\'losses\'))\n    learning_rate = tf.train.exponential_decay(\n        LEARNING_RATE_BASE,\n        global_step,\n        mnist.train.num_examples / BATCH_SIZE, LEARNING_RATE_DECAY,\n        staircase=True)\n    train_step = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step=global_step)\n    with tf.control_dependencies([train_step, variables_averages_op]):\n        train_op = tf.no_op(name=\'train\')\n\n\n    saver = tf.train.Saver()\n    with tf.Session() as sess:\n        tf.global_variables_initializer().run()\n\n        for i in range(TRAINING_STEPS):\n            xs, ys = mnist.train.next_batch(BATCH_SIZE)\n            _, loss_value, step = sess.run([train_op, loss, global_step], feed_dict={x: xs, y_: ys})\n            if i % 1000 == 0:\n                print(""After %d training step(s), loss on training batch is %g."" % (step, loss_value))\n                saver.save(sess, os.path.join(MODEL_SAVE_PATH, MODEL_NAME), global_step=global_step)\n\n\ndef main(argv=None):\n    mnist = input_data.read_data_sets(""../../datasets/MNIST_data"", one_hot=True)\n    train(mnist)\n\nif __name__ == \'__main__\':\n    tf.app.run()\n\n\n'"
Chapter06/LeNet-5/LeNet5_infernece.py,30,"b'import tensorflow as tf\n\nINPUT_NODE = 784\nOUTPUT_NODE = 10\n\nIMAGE_SIZE = 28\nNUM_CHANNELS = 1\nNUM_LABELS = 10\n\nCONV1_DEEP = 32\nCONV1_SIZE = 5\n\nCONV2_DEEP = 64\nCONV2_SIZE = 5\n\nFC_SIZE = 512\n\ndef inference(input_tensor, train, regularizer):\n    with tf.variable_scope(\'layer1-conv1\'):\n        conv1_weights = tf.get_variable(\n            ""weight"", [CONV1_SIZE, CONV1_SIZE, NUM_CHANNELS, CONV1_DEEP],\n            initializer=tf.truncated_normal_initializer(stddev=0.1))\n        conv1_biases = tf.get_variable(""bias"", [CONV1_DEEP], initializer=tf.constant_initializer(0.0))\n        conv1 = tf.nn.conv2d(input_tensor, conv1_weights, strides=[1, 1, 1, 1], padding=\'SAME\')\n        relu1 = tf.nn.relu(tf.nn.bias_add(conv1, conv1_biases))\n\n    with tf.name_scope(""layer2-pool1""):\n        pool1 = tf.nn.max_pool(relu1, ksize = [1,2,2,1],strides=[1,2,2,1],padding=""SAME"")\n\n    with tf.variable_scope(""layer3-conv2""):\n        conv2_weights = tf.get_variable(\n            ""weight"", [CONV2_SIZE, CONV2_SIZE, CONV1_DEEP, CONV2_DEEP],\n            initializer=tf.truncated_normal_initializer(stddev=0.1))\n        conv2_biases = tf.get_variable(""bias"", [CONV2_DEEP], initializer=tf.constant_initializer(0.0))\n        conv2 = tf.nn.conv2d(pool1, conv2_weights, strides=[1, 1, 1, 1], padding=\'SAME\')\n        relu2 = tf.nn.relu(tf.nn.bias_add(conv2, conv2_biases))\n\n    with tf.name_scope(""layer4-pool2""):\n        pool2 = tf.nn.max_pool(relu2, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding=\'SAME\')\n        pool_shape = pool2.get_shape().as_list()\n        nodes = pool_shape[1] * pool_shape[2] * pool_shape[3]\n        reshaped = tf.reshape(pool2, [pool_shape[0], nodes])\n\n    with tf.variable_scope(\'layer5-fc1\'):\n        fc1_weights = tf.get_variable(""weight"", [nodes, FC_SIZE],\n                                      initializer=tf.truncated_normal_initializer(stddev=0.1))\n        if regularizer != None: tf.add_to_collection(\'losses\', regularizer(fc1_weights))\n        fc1_biases = tf.get_variable(""bias"", [FC_SIZE], initializer=tf.constant_initializer(0.1))\n\n        fc1 = tf.nn.relu(tf.matmul(reshaped, fc1_weights) + fc1_biases)\n        if train: fc1 = tf.nn.dropout(fc1, 0.5)\n\n    with tf.variable_scope(\'layer6-fc2\'):\n        fc2_weights = tf.get_variable(""weight"", [FC_SIZE, NUM_LABELS],\n                                      initializer=tf.truncated_normal_initializer(stddev=0.1))\n        if regularizer != None: tf.add_to_collection(\'losses\', regularizer(fc2_weights))\n        fc2_biases = tf.get_variable(""bias"", [NUM_LABELS], initializer=tf.constant_initializer(0.1))\n        logit = tf.matmul(fc1, fc2_weights) + fc2_biases\n\n    return logit'"
