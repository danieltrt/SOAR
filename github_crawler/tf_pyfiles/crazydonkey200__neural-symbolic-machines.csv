file_path,api_count,code
setup.py,0,"b""#!/usr/bin/env python\n\nfrom setuptools import setup, find_packages\nimport sys\n\nwith open('README.md') as f:\n    readme = f.read()\n\nwith open('LICENSE') as f:\n    license = f.read()\n\nwith open('requirements.txt') as f:\n    reqs = f.read()\n\nsetup(\n    name='nsm',\n    version='0.1.0',\n    description='Neural Symbolic Machines',\n    long_description=readme,\n    license=license,\n    python_requires='==2.7',\n    packages=find_packages(exclude=('examples')),\n    install_requires=reqs.strip().split('\\n'),\n)\n"""
nsm/__init__.py,0,b''
nsm/agent_factory.py,2,"b'""""""Implementation of RL agents.""""""\n\nimport collections\nimport os\nimport logging\nimport time\nimport shutil\nimport pprint\nimport heapq\nimport random\n\nimport tensorflow as tf\nimport numpy as np\n\nimport data_utils\nimport graph_factory\nimport env_factory\n\n\n# To suppress division by zero in np.log, because the\n# behaviour np.log(0.0) = -inf is correct in this context.\nnp.warnings.filterwarnings(\'ignore\')\n\n\ndef scale_probs(samples, scale):\n  ""Weight each samples with the weight. Reflected on probs.""\n  scaled_probs = [scale * s.prob for s in samples]\n  new_samples = []\n  for s, p in zip(samples, scaled_probs):\n    new_samples.append(Sample(traj=s.traj, prob=p))\n  return new_samples\n\n\ndef scale_rewards(samples, scale):\n  ""Weight each samples with the weight. Reflected on rewards.""\n  scaled_rewards = [list(scale * np.array(s.rewards)) for s in samples]\n  new_samples = []\n  for s, p in zip(samples, scaled_rewards):\n    new_samples.append(sample._replace(traj=traj._replace(rewards=scaled_rewards)))\n  return new_samples\n  \n\ndef normalize_probs(samples, smoothing=1e-8):\n  ""Normalize the probability of the samples (in each env) to sum to 1.0.""\n  sum_prob_dict = {}\n  for s in samples:\n    name = s.traj.env_name\n    if name in sum_prob_dict:\n      sum_prob_dict[name] += s.prob + smoothing\n    else:\n      sum_prob_dict[name] = s.prob + smoothing\n  new_samples = []\n  for s in samples:\n    new_prob = (s.prob + smoothing) / sum_prob_dict[s.traj.env_name]\n    new_samples.append(Sample(traj=s.traj, prob=new_prob))\n  return new_samples\n\n\ndef compute_baselines(returns, probs, env_names):\n  ""Compute baseline for samples.""\n  baseline_dict = {}\n  sum_dict = {}\n  for ret, p, name in zip(returns, probs, env_names):\n    if name not in sum_dict:\n      sum_dict[name] = ret[-1] * p\n    else:\n      sum_dict[name] += ret[-1] * p\n  for name, _ in sum_dict.iteritems():\n    # Assume probabilities are already normalized.\n    baseline_dict[name] = sum_dict[name]\n  return baseline_dict\n\n\nclass PGAgent(object):\n  ""Agent trained by policy gradient.""\n  def __init__(self, model, train_writer=None, discount_factor=1.0):\n\n    self.model = model\n    self.discount_factor = discount_factor\n\n    self.train_writer = train_writer\n\n    self.monitor_graph = graph_factory.MonitorGraph()\n    for name in [\'avg_return\', \'min_return\', \'max_return\', \'std_return\',\n                 \'avg_len\', \'min_len\', \'max_len\', \'std_len\',\n                 \'clip_frac\']:\n      self.monitor_graph.add_scalar_monitor(name, dtype=tf.float32)\n\n    self.monitor_graph.launch()\n\n  def generate_samples(self, envs, n_samples=1, greedy=False,\n                       use_cache=False, filter_error=True): #, parameters=None):\n    """"""Returns Actions, rewards, obs, other info.""""""\n    samples = sampling(\n      self.model, envs, n_samples=n_samples, greedy=greedy, use_cache=use_cache,\n      filter_error=filter_error)\n    return samples\n\n  def beam_search(self, envs=None, beam_size=1, use_cache=False, greedy=False):\n    """"""Returns Actions, rewards, obs and probs.""""""    \n    samples = beam_search(self.model, envs, beam_size=beam_size,\n                          use_cache=use_cache, greedy=greedy)\n    return samples\n\n  def update_replay_prob(self, samples, min_replay_weight=0.0):\n    """"""Update the probability of the replay samples and recompute the weights (prob).""""""\n    prob_sum_dict = {}\n    trajs_to_update = [\n      sample.traj for sample in samples if sample.prob is None]\n    new_probs = self.compute_probs(trajs_to_update)\n    for traj, prob in zip(trajs_to_update, new_probs):\n      name = traj.env_name\n      if name in prob_sum_dict:\n        prob_sum_dict[name] += prob\n      else:\n        prob_sum_dict[name] = prob\n\n    i = 0\n    new_samples = []\n    for sample in samples:\n      name = sample.traj.env_name\n      if name in prob_sum_dict:\n        w = max(prob_sum_dict[name], min_replay_weight)\n      else:\n        w = 0.0\n      if sample.prob is None:\n        new_samples.append(\n          sample._replace(prob=new_probs[i] * w / prob_sum_dict[name]))\n        i += 1\n      else:\n        prob = sample.prob * (1.0 - w)\n        new_samples.append(sample._replace(prob=prob))\n    assert i == len(trajs_to_update)\n\n    return new_samples\n\n  def train(self, samples, use_baseline=False,\n            debug=False, parameters=None, min_prob=0.0, scale=1.0,\n            behaviour_logprobs=None, use_importance_sampling=False,\n            ppo_epsilon=0.2, weight_by_target_prob=True,\n            de_vocab=None):\n\n    trajs = [s.traj for s in samples]\n    probs = [s.prob for s in samples]\n    env_names = [t.env_name for t in trajs]\n    returns = [compute_returns(t.rewards, self.discount_factor) for t in trajs]\n\n    if use_baseline:\n      baseline_dict = compute_baselines(returns, probs, env_names)\n      advantages = []\n      for t, r in zip(trajs, returns):\n        advantages.append(\n          list(np.array(r) - baseline_dict[t.env_name]))\n    else:\n      advantages = returns\n\n    obs = [t.obs for t in trajs]\n    actions = [t.actions for t in trajs]\n    rewards = [t.rewards for t in trajs]\n    context = [t.context for t in trajs]\n\n    weights = [list(np.array(ad) * p * scale) for ad, p in zip(advantages, probs)]\n\n    if not use_importance_sampling and min_prob > 0.0:\n      model_probs = self.compute_probs(trajs)\n      for i, mp in enumerate(model_probs):\n        # If the probabiity of the example is already small\n        # and the advantage is still negative, then stop\n        # punishing it because it might cause numerical\n        # instability.\n        if mp < min_prob and len(advantages[i]) > 0 and advantages[i][0] < 0.0:\n          weights[i] = len(weights[i]) * [0.0]\n\n    if use_importance_sampling:\n      assert behaviour_logprobs is not None\n      new_weights = []\n      target_logprobs = self.compute_step_logprobs(trajs)\n      ratio = [list(np.exp(np.array(tlp) - np.array(blp))) for blp, tlp in\n               zip(behaviour_logprobs, target_logprobs)]\n      if debug:\n        print \'ratio: \'\n        pprint.pprint(ratio)\n      # print \'advantages: \'\n      # print advantages      \n      for i in xrange(len(weights)):\n        new_ws = []\n        for w, ad, blp, tlp in zip(\n            weights[i], advantages[i],\n            behaviour_logprobs[i], target_logprobs[i]):\n          if weight_by_target_prob:\n            if ad > 0.0:\n              new_ws.append(max(np.exp(tlp), 0.1) * ad)\n            else:\n              new_ws.append(np.exp(tlp) * ad)\n          elif ad >= 0 and (tlp - blp) > np.log(1 + ppo_epsilon):\n            new_ws.append(0.0)\n          elif ad < 0 and (tlp - blp) < np.log(1 - ppo_epsilon):\n            new_ws.append(0.0)\n          elif tlp > np.log(1 - 1e-4) and ad > 0.0:\n            new_ws.append(0.0)\n          elif tlp < np.log(1e-4) and ad < 0.0:\n            new_ws.append(0.0)\n          else:\n           new_ws.append(w * np.exp(tlp - blp))\n        if min_prob > 0.0 and sum(target_logprobs[i]) < np.log(min_prob):\n          new_ws = [0.0] * len(new_ws)\n        new_weights.append(new_ws)\n      weights = new_weights\n      if debug:\n        print \'new_weights: \'\n        pprint.pprint(weights)\n\n    if debug:\n      print(\'+\' * 50)      \n      model_probs = self.compute_probs(trajs)\n      print(\'scale: {}, min_prob: {}\'.format(scale, min_prob))\n      for i, (name, c, o, a, r, ad, p, mp, w, traj) in enumerate(zip(\n          env_names, context, obs, actions, returns, advantages,\n          probs, model_probs, weights, trajs)):\n        print((\'sample {}, name: {}, return: {}, advantage: {}, \'\n               \'prob: {}, model prob: {}, weight: {}\').format(\n                 i, name, r[0], ad[0], p, mp, w[0]))\n        if de_vocab is not None:\n          print(\' \'.join(traj_to_program(traj, de_vocab)))\n      print(\'+\' * 50)\n      \n    self.model.train(obs, actions, weights=weights, context=context,\n                     parameters=parameters, writer=self.train_writer)\n\n  def compute_probs(self, trajs):\n    obs = [s.obs for s in trajs]\n    actions = [s.actions for s in trajs]\n    context = [s.context for s in trajs]\n    probs = self.model.compute_probs(obs, actions, context=context)\n    return probs\n\n  def compute_step_logprobs(self, trajs):\n    obs = [s.obs for s in trajs]\n    actions = [s.actions for s in trajs]\n    context = [s.context for s in trajs]\n    logprobs = self.model.compute_step_logprobs(obs, actions, context=context)\n    return logprobs\n    \n  def evaluate(self, samples, n_samples=1, verbose=0, writer=None, true_n=None,\n               clip_frac=0.0):\n    ""Evaluate the agent on the envs.""\n\n    trajs = [s.traj for s in samples]\n    actions = [t.actions for t in trajs]\n    probs = [s.prob for s in samples]\n  \n    returns = [compute_returns(t.rewards, self.discount_factor)[0] for t in trajs]\n\n    avg_return, std_return, max_return, min_return, n_w = compute_weighted_stats(\n      returns, probs)\n\n    if true_n is not None:\n      # Account for the fact that some environment doesn\'t\n      # generate any valid samples, but we still need to\n      # consider them when computing returns.\n      new_avg_return = avg_return * n_w / true_n\n      tf.logging.info(\n        \'avg return adjusted from {} to {} based on true n\'.format(\n          avg_return, new_avg_return))\n      avg_return = new_avg_return\n    \n    lens = [len(acs) for acs in actions]\n    avg_len, std_len, max_len, min_len, _ = compute_weighted_stats(lens, probs)\n    \n    if verbose > 0:\n      print (\'return: avg {} +- {}, max {}, min {}\\n\'\n             \'length: avg {} +- {}, max {}, min {}\').format(\n               avg_return, return_array.std(),\n               return_array.max(), return_array.min(),\n               avg_len, len_array.std(), len_array.max(), len_array.min())\n\n    if writer is not None:\n      feed_dict = dict(\n        avg_return=avg_return, max_return=max_return,\n        min_return=min_return, std_return=std_return,\n        avg_len=avg_len, max_len=max_len,\n        min_len=min_len, std_len=std_len,\n        clip_frac=clip_frac)\n      self.write_to_monitor(feed_dict, writer)\n      # summary = self.monitor_graph.generate_summary(feed_dict)\n      # writer.add_summary(summary, self.model.get_global_step())\n      # writer.flush()\n    return avg_return, avg_len\n\n  def write_to_monitor(self, feed_dict, writer):\n    summary = self.monitor_graph.generate_summary(feed_dict)\n    writer.add_summary(summary, self.model.get_global_step())\n    writer.flush()\n\n\ndef compute_weighted_stats(array, weight):\n  n = len(array)\n  if n < 1:\n    return (0.0, 0.0, 0.0, 0.0, 0.0)\n  sum_ = 0.0\n  min_ = array[0]\n  max_ = array[0]\n  n_w = sum(weight)\n  for a, w in zip(array, weight):\n    min_ = min([min_, a])\n    max_ = max([max_, a])\n    sum_ += a * w\n  mean = sum_ / n_w\n  sum_square_std = 0.0\n  for a in array:\n    sum_square_std += (a - mean) ** 2 * w\n  std = np.sqrt(sum_square_std / n_w)\n  return (mean, std, max_, min_, n_w)\n\n\ndef compute_returns(rewards, discount_factor=1.0):\n  ""Compute returns of a trace (sum of discounted rewards).""\n  returns = []\n  t = len(rewards)\n  returns = [0.0] * t\n  sum_return_so_far = 0.0\n  for i in xrange(t):\n    returns[-i-1] = sum_return_so_far * discount_factor + rewards[-i-1]\n    sum_return_so_far = returns[-1-i]\n  return returns\n\n\ndef compute_td_errors(values, rewards, discount_factor=1.0, td_n=0):\n  ""Compute TD errors.""\n  td_errors = []\n  td_n += 1\n  backup_values = compute_backup_values(values, rewards, discount_factor, td_n)\n  for vs, bvs in zip(values, backup_values):\n    td_errors.append((np.array(bvs) - np.array(vs)).tolist())\n  return td_errors\n\n\ndef compute_backup_values(values, rewards, discount_factor=1.0, n_steps=1):\n  ""Compute backup value.""\n  backup_values = []\n  for vs, rs in zip(values, rewards):\n    bvs = []\n    T = len(vs)\n    for i in xrange(T):\n      end = min(i + n_steps, T)\n      if end == T:\n        bv = 0.0\n      else:\n        bv = vs[end] * (discount_factor ** (end - i))\n      for t in xrange(i, end):\n        bv += rs[t] * (discount_factor ** (t-i))\n      bvs.append(bv)\n    backup_values.append(bvs)\n  return backup_values\n\n\nclass ReplayBuffer(object):\n\n  def save(self, samples):\n    pass\n\n  def replay(self, envs):\n    pass\n\n\nclass AllGoodReplayBuffer(ReplayBuffer):\n  def __init__(self, agent=None, de_vocab=None, discount_factor=1.0, debug=False):\n    self._buffer = dict()\n    self.discount_factor = discount_factor\n    self.agent = agent\n    self.de_vocab = de_vocab\n    self.program_prob_dict = dict()\n    self.prob_sum_dict = dict()\n\n  def has_found_solution(self, env_name):\n    return env_name in self._buffer and self._buffer[env_name]\n\n  def contain(self, traj):\n    name = traj.env_name\n    if name not in self.program_prob_dict:\n      return False\n    program = traj_to_program(traj, self.de_vocab)\n    program_str = u\' \'.join(program)\n    if program_str in self.program_prob_dict[name]:\n      return True\n    else:\n      return False\n\n  @property\n  def size(self):\n    n = 0\n    for _, v in self._buffer.iteritems():\n      n += len(v)\n    return n\n      \n  def save(self, samples):\n    trajs = [s.traj for s in samples]\n    self.save_trajs(trajs)\n\n  def save_trajs(self, trajs):\n    total_returns = [\n      compute_returns(t.rewards, self.discount_factor)[0] for t in trajs]\n    for t, return_ in zip(trajs, total_returns):\n      name = t.env_name\n      program = traj_to_program(t, self.de_vocab)\n      program_str = \' \'.join(program)\n      if (return_ > 0.5 and\n          len(program) > 0 and\n          (program[-1] == self.de_vocab.end_tk) and\n          (not (name in self.program_prob_dict and\n                (program_str in self.program_prob_dict[name])))):\n        if name in self.program_prob_dict:\n          self.program_prob_dict[name][program_str] = True\n        else:\n          self.program_prob_dict[name] = {program_str: True}\n        if name in self._buffer:\n          self._buffer[name].append(t)\n        else:\n          self._buffer[name] = [t]\n\n  def all_samples(self, envs, agent=None):\n    select_env_names = set([e.name for e in envs])\n    trajs = []\n    # Collect all the trajs for the selected envs.\n    for name in select_env_names:\n      if name in self._buffer:\n        trajs += self._buffer[name]\n    if agent is None:\n      # All traj has the same probability, since it will be\n      # normalized later, we just assign them all as 1.0.\n      probs = [1.0] * len(trajs)\n    else:\n      # Otherwise use the agent to compute the prob for each\n      # traj.\n      probs = agent.compute_probs(trajs)\n    samples = [Sample(traj=t, prob=p) for t, p in zip(trajs, probs)]\n    return samples\n\n  def replay(self, envs, n_samples=1, use_top_k=False, agent=None, truncate_at_n=0):\n    select_env_names = set([e.name for e in envs])\n    trajs = []\n    # Collect all the trajs for the selected envs.\n    for name in select_env_names:\n      if name in self._buffer:\n        trajs += self._buffer[name]\n    if agent is None:\n      # All traj has the same probability, since it will be\n      # normalized later, we just assign them all as 1.0.\n      probs = [1.0] * len(trajs)\n    else:\n      # Otherwise use the agent to compute the prob for each\n      # traj.\n      probs = agent.compute_probs(trajs)\n\n    # Put the samples into an dictionary keyed by env names.\n    samples = [Sample(traj=t, prob=p) for t, p in zip(trajs, probs)]\n    env_sample_dict = dict(\n      [(name, []) for name in select_env_names\n       if name in self._buffer])\n    for s in samples:\n      name = s.traj.env_name\n      env_sample_dict[name].append(s)\n      \n    replay_samples = []\n    for name, samples in env_sample_dict.iteritems():\n      n = len(samples)\n      # Truncated the number of samples in the selected\n      # samples and in the buffer.\n      if truncate_at_n > 0 and n > truncate_at_n:\n        # Randomize the samples before truncation in case\n        # when no prob information is provided and the trajs\n        # need to be truncated randomly.\n        random.shuffle(samples)\n        samples = heapq.nlargest(\n          truncate_at_n, samples, key=lambda s: s.prob)\n        self._buffer[name] = [sample.traj for sample in samples]\n\n      # Compute the sum of prob of replays in the buffer.\n      self.prob_sum_dict[name] = sum([sample.prob for sample in samples])\n\n      if use_top_k:\n        # Select the top k samples weighted by their probs.\n        selected_samples = heapq.nlargest(\n          n_samples, samples, key=lambda s: s.prob)\n        replay_samples += normalize_probs(selected_samples)\n      else:\n        # Randomly samples according to their probs.\n        samples = normalize_probs(samples)\n        selected_samples = [samples[i] for i in np.random.choice(\n          len(samples), n_samples, p=[sample.prob for sample in samples])]\n        replay_samples += [\n          Sample(traj=s.traj, prob=1.0 / n_samples) for s in selected_samples]\n\n    return replay_samples\n\n\ndef traj_to_program(traj, de_vocab):\n  program = []\n  for a, ob in zip(traj.actions, traj.obs):\n    ob = ob[0]\n    token = de_vocab.lookup(ob.valid_indices[a], reverse=True)\n    program.append(token)\n  return program\n  \n    \nTraj = collections.namedtuple(\n  \'Traj\', \'obs actions rewards context env_name answer\')\n\nSample = collections.namedtuple(\n  \'Sample\', \'traj prob\')\n\n\ndef sampling(model, envs, temperature=1.0, use_encode=True,\n             greedy=False, n_samples=1, debug=False, use_cache=False,\n             filter_error=True):\n\n  if not envs:\n    raise ValueError(\'No environment provided!\')\n\n  if use_cache:\n    # if already explored everything, then don\'t explore this environment anymore.\n    envs = [env for env in envs if not env.cache.is_full()]\n\n  duplicated_envs = []\n  for env in envs:\n    for i in range(n_samples):\n      duplicated_envs.append(env.clone())\n      \n  envs = duplicated_envs\n  \n  for env in envs:\n    env.use_cache = use_cache\n  \n  if use_encode:\n    env_context = [env.get_context() for env in envs]\n    encoded_context, initial_state = model.encode(env_context)\n  else:\n    # env_context = [None for env in envs]\n    encoded_context, initial_state = None, None\n\n  obs = [[env.start_ob] for env in envs]\n  state = initial_state\n  \n  while True:\n    outputs, state = model.step(obs, state, context=encoded_context)\n    \n    if greedy:\n      actions = model.predict(cell_outputs=outputs)\n    else:\n      actions = model.sampling(\n        cell_outputs=outputs, temperature=temperature)\n\n    if debug:\n      print \'*\' * 50\n      print \'actions: \'\n      pprint.pprint(actions)\n      print \'action_prob: \'\n      action_prob = model.predict_prob(cell_outputs=outputs)\n      pprint.pprint(action_prob)\n      print \'*\' * 50\n\n    # Get rid of the time dimension so that actions is just one list.\n    actions = [a[0] for a in actions]\n    action_probs = model.predict_prob(cell_outputs=outputs)\n    action_probs = [ap[0] for ap in action_probs]\n\n    obs = []\n    for env, action, p in zip(envs, actions, action_probs):\n      try:\n        ob, _, _, info = env.step(action)\n        obs.append([ob])\n      except IndexError:\n        print p\n        raise IndexError\n    step_pairs = [\n      x for x in zip(obs, state, encoded_context, envs) if not x[-1].done]\n    if len(step_pairs) > 0:\n      obs, state, encoded_context, envs = zip(*step_pairs)\n      obs = list(obs)\n      state = list(state)\n      envs = list(envs)\n      encoded_context = list(encoded_context)\n      assert len(obs) == len(state)\n      assert len(obs) == len(encoded_context)\n      assert len(obs) == len(envs)\n    else:\n      break\n\n  obs, actions, rewards = zip(*[(env.obs, env.actions, env.rewards)\n                                for env in duplicated_envs])\n  env_names = [env.name for env in duplicated_envs]\n  answers = [env.interpreter.result for env in duplicated_envs]\n  \n  samples = []\n  for i, env in enumerate(duplicated_envs):\n    if not (filter_error and env.error):\n      samples.append(\n        Sample(\n          traj=Traj(obs=env.obs, actions=env.actions, rewards=env.rewards,\n                    context=env_context[i], env_name=env_names[i],\n                    answer=answers[i]),\n          prob=1.0 / n_samples))\n  return samples\n\n\nHyph = collections.namedtuple(\'Hyph\', [\'state\', \'env\', \'score\'])\nCandidate = collections.namedtuple(\'Candidate\', [\'state\', \'env\', \'score\', \'action\'])\n\n\ndef beam_search(model, envs, use_encode=True, beam_size=1, debug=False, renorm=True,\n                use_cache=False, filter_error=True, greedy=False):\n  if use_cache:\n    # if already explored everything, then don\'t explore this environment anymore.\n    envs = [env for env in envs if not env.cache.is_full()]\n    \n  if use_encode:\n    env_context = [env.get_context() for env in envs]\n    encoded_context, initial_state = model.encode(env_context)\n    env_context_dict = dict(\n      [(env.name, env.get_context()) for env in envs])\n    context_dict = dict(\n      [(env.name, c) for env, c in zip(envs, encoded_context)])\n    beam = [Hyph(s, env.clone(), 0.0)\n            for env, s in zip(envs, initial_state)]\n    state = initial_state\n    context = encoded_context\n  else:\n    beam = [Hyph(None, env.clone(), 0.0) for env in envs]\n    state = None\n    context = None\n    env_context_dict = dict(\n      [(env.name, None) for env in envs])\n\n  for hyp in beam:\n    hyp.env.use_cache = use_cache      \n    \n  finished_dict = dict([(env.name, []) for env in envs])\n  obs = [[h.env.start_ob] for h in beam]\n\n  while beam:\n    if debug:\n      print \'@\' * 50\n      print \'beam is\'\n      for h in beam:\n        print \'env {}\'.format(h.env.name)\n        print h.env.show()\n        print h.score\n        print\n  \n    # Run the model for one step to get probabilities for new actions.\n    outputs, state = model.step(obs, state, context=context)\n\n    probs = model.predict_prob(outputs)\n    scores = (np.log(np.array(probs)) + np.array([[[h.score]] for h in beam]))\n\n    # Collect candidates.\n    candidate_dict = {}\n    for hyph, st, score in zip(beam, state, scores):\n      env_name = hyph.env.name\n      if env_name not in candidate_dict:\n        candidate_dict[env_name] = []\n      for action, s in enumerate(score[0]):\n        if not s == -np.inf:\n          candidate_dict[env_name].append(Candidate(st, hyph.env, s, action))\n\n    if debug:\n      print \'*\' * 20\n      print \'candidates are\'\n      for k, v in candidate_dict.iteritems():      \n        print \'env {}\'.format(k)\n        for x in v:\n          print x.env.show()\n          print x.action\n          print x.score\n          print type(x)\n          print isinstance(x, Candidate)\n          print\n        \n    # Collect the new beam.\n    new_beam = []\n    obs = []\n    for env_name, candidates in candidate_dict.iteritems():\n      # Find the top k from the union of candidates and\n      # finished hypotheses.\n      all_candidates = candidates + finished_dict[env_name]\n      topk = heapq.nlargest(\n        beam_size, all_candidates, key=lambda x: x.score)\n\n      # Step the environment and collect the hypotheses into\n      # new beam (unfinished hypotheses) or finished_dict\n      finished_dict[env_name] = []\n      for c in topk:\n        if isinstance(c, Hyph):\n          finished_dict[env_name].append(c)\n        else:\n          env = c.env.clone()\n          #print \'action\', c.action\n          ob, _, done, info = env.step(c.action)\n          #pprint.pprint(env.de_vocab.lookup(info[\'valid_actions\'], reverse=True))\n          #pprint.pprint(env.de_vocab.lookup(info[\'new_var_id\'], reverse=True))\n          new_hyph = Hyph(c.state, env, c.score)\n          if not done:\n            obs.append([ob])\n            new_beam.append(new_hyph)\n          else:\n            if not (filter_error and new_hyph.env.error):\n              finished_dict[env_name].append(new_hyph)\n\n    if debug:\n      print \'#\' * 20\n      print \'finished programs are\'\n      for k, v in finished_dict.iteritems():\n        print \'env {}\'.format(k)\n        for x in v:\n          print x.env.show()\n          print x.score\n          print type(x)\n          print isinstance(x, Hyph)\n          print\n            \n    beam = new_beam\n    \n    if use_encode:\n      state = [h.state for h in beam]\n      context = [context_dict[h.env.name] for h in beam]\n    else:\n      state = None\n      context = None\n\n  final = []\n  env_names = [env.name for env in envs]\n  for name in env_names:\n    sorted_final = sorted(\n      finished_dict[name],\n      key=lambda h: h.score, reverse=True)\n    if greedy:\n      # Consider the time when sorted_final is empty (didn\'t\n      # find any programs without error).\n      if sorted_final:\n        final += [sorted_final[0]]\n    else:\n      final += sorted_final\n\n  # Collect the training examples.\n  obs, actions, rewards, env_names, scores = zip(\n    *[(h.env.obs, h.env.actions, h.env.rewards, h.env.name, h.score)\n      for h in final])\n  answers = [h.env.interpreter.result for h in final]\n\n  samples = []\n  for i, name in enumerate(env_names):\n    samples.append(\n      Sample(\n        traj=Traj(obs=obs[i], actions=actions[i], rewards=rewards[i],\n                  context=env_context_dict[name], env_name=name,\n                  answer=answers[i]),\n        prob=np.exp(scores[i])))\n\n  if renorm:\n    samples = normalize_probs(samples)\n  return samples\n\n\n# A random agent.\n\nclass RandomAgent(object):\n  def __init__(self, discount_factor=1.0):\n    self.discount_factor = discount_factor\n\n  def generate_samples(self, envs, n_samples=1, use_cache=False):      \n    if use_cache:\n      # if already explored everything, then don\'t explore this environment anymore.\n      envs = [env for env in envs if not env.cache.is_full()]\n\n    for env in envs:\n      env.use_cache = use_cache\n    \n    duplicated_envs = []\n    for env in envs:\n      for i in range(n_samples):\n        duplicated_envs.append(env.clone())\n\n    envs = duplicated_envs\n\n    for env in envs:\n      ob = env.start_ob\n      while not env.done:\n        valid_actions = ob[0].valid_indices\n        action = np.random.randint(0, len(valid_actions))\n        ob, _, _, _ = env.step(action)\n\n    env_context = [env.get_context() for env in envs]\n    env_names = [env.name for env in envs]\n    samples = []\n    for i, env in enumerate(envs):\n      samples.append(\n        Sample(\n          traj=Traj(obs=env.obs, actions=env.actions, rewards=env.rewards,\n                    context=env_context[i], env_name=env_names[i]),\n          prob=1.0 / n_samples))\n    return samples\n\n  def evaluate(self, samples):\n    trajs = [s.traj for s in samples]\n    actions = [t.actions for t in trajs]\n\n    probs = [s.prob for s in samples]\n  \n    returns = [compute_returns(t.rewards, self.discount_factor)[0] for t in trajs]\n\n    avg_return, std_return, max_return, min_return = compute_weighted_stats(\n      returns, probs)\n\n    lens = [len(acs) for acs in actions]\n    avg_len, std_len, max_len, min_len = compute_weighted_stats(lens, probs)\n    return avg_return, avg_len\n'"
nsm/computer_factory.py,0,"b'""""""Computers can read in tokens, parse them into a program, and execute it.""""""\n\nfrom collections import OrderedDict\nimport re\nimport copy\nimport sys\nimport os\nimport data_utils\nimport pprint\n\nEND_TK = data_utils.END_TK  # End of program token\nERROR_TK = \'<ERROR>\'\n# SPECIAL_TKS = [END_TK, ERROR_TK, \'(\', \')\']\nSPECIAL_TKS = [ERROR_TK, \'(\', \')\']\n\nclass LispInterpreter(object):\n  """"""Interpreter reads in tokens, parse them into a program and execute it.""""""\n\n  def __init__(self, type_hierarchy, max_mem, max_n_exp, assisted=True):\n    """"""\n    max_mem: maximum number of memory slots.\n    max_n_exp: maximum number of expressions.\n    assisted: whether to provide assistance to the programmer (used for neural programmer).\n    """"""\n    # Create namespace.\n    self.namespace = Namespace()\n\n    self.assisted = assisted\n    # Configs.\n    # Functions used to call\n    # Signature: autocomplete(evaled_exp, valid_tokens, evaled_tokens)\n    # return a subset of valid_tokens that passed the\n    # filter. Used to implement filtering with denotation.\n    self.type_hierarchy = type_hierarchy\n    self.type_ancestry = create_type_ancestry(type_hierarchy)\n    self.max_mem = max_mem\n    self.max_n_exp = max_n_exp\n\n    # Initialize the parser state.\n    self.n_exp = 0\n    self.history = []\n    self.exp_stack = []\n    self.done = False\n    self.result = None\n\n  @property\n  def primitive_names(self):\n    primitive_names = []\n    for k, v in self.namespace.iteritems():\n      if (\'property\' in self.type_ancestry[v[\'type\']] or\n          \'primitive_function\' in self.type_ancestry[v[\'type\']]):\n        primitive_names.append(k)\n    return primitive_names\n\n  @property\n  def primitives(self):\n    primitives = []\n    for k, v in self.namespace.iteritems():\n      if (\'property\' in self.type_ancestry[v[\'type\']] or\n          \'primitive_function\' in self.type_ancestry[v[\'type\']]):\n        primitives.append(v)\n    return primitives  \n\n  def add_constant(self, value, type, name=None):\n    """"""Generate the code and variables to hold the constants.""""""\n    if name is None:\n      name = self.namespace.generate_new_name()\n    self.namespace[name] = dict(\n      value=value, type=type,\n      is_constant=True)\n    return name\n\n  def add_function(self, name, value, args, return_type,\n                   autocomplete, type):\n    """"""Add function into the namespace.""""""\n    if name in self.namespace:\n      raise ValueError(\'Name %s is already used.\' % name)\n    else:\n      self.namespace[name] = dict(\n        value=value, type=type,\n        autocomplete=autocomplete,\n        return_type=return_type, args=args)\n\n  def autocomplete(self, exp, tokens, token_vals, namespace):\n    func = exp[0]\n    exp = [x[\'value\'] for x in exp]\n    token_vals = [x[\'value\'] for x in token_vals]\n    if func[\'type\'] == \'global_primitive_function\':\n      return func[\'autocomplete\'](\n        exp, tokens, token_vals, namespace=namespace)\n    else:\n      return func[\'autocomplete\'](exp, tokens, token_vals)\n      \n  def reset(self, only_reset_variables=False):\n    """"""Reset all the interpreter state.""""""\n    if only_reset_variables:\n      self.namespace.reset_variables()\n    else:\n      self.namespace = Namespace()\n    self.history = []\n    self.n_exp = 0\n    self.exp_stack = []\n    self.done = False\n    self.result = None\n    \n  def read_token_id(self, token_id):\n    token = self.rev_vocab[token_id]\n    return self.read_token(token)\n\n  def read_token(self, token):\n    """"""Read in one token, parse and execute the expression if completed.""""""\n    if ((self.n_exp >= self.max_n_exp) or\n        (self.namespace.n_var >= self.max_mem)):\n      token = END_TK    \n    new_exp = self.parse_step(token)\n    # If reads in end of program, then return the last value as result.\n    if token == END_TK:\n      self.done = True\n      self.result = self.namespace.get_last_value()\n      return self.result\n    elif new_exp:\n      if self.assisted:\n        name = self.namespace.generate_new_name()\n        result = self.eval([\'define\', name, new_exp])\n        self.n_exp += 1\n        # If there are errors in the execution, self.eval\n        # will return None. We can also give a separate negative\n        # reward for errors.\n        if result is None:\n          self.namespace.n_var -= 1\n          self.done = True\n          self.result = [ERROR_TK]\n        #   result = self.eval([\'define\', name, ERROR_TK])\n      else:\n        result = self.eval(new_exp)\n      return result\n    else:\n      return None\n\n  def valid_tokens(self):\n    """"""Return valid tokens for the next step for programmer to pick.""""""\n    # If already exceeded max memory or max expression\n    # limit, then must end the program.\n    if ((self.n_exp >= self.max_n_exp) or\n        (self.namespace.n_var >= self.max_mem)):\n      result = [END_TK]\n    # If last expression is finished, either start a new one\n    # or end the program.\n    elif not self.history:\n      result = [\'(\'] \n    # If not in an expression, either start a new expression or end the program. \n    elif not self.exp_stack:\n      result = [\'(\', END_TK]\n    # If currently in an expression.\n    else:\n      exp = self.exp_stack[-1]\n      # If in the middle of a new expression.\n      if exp:\n        # Use number of arguments to check if all arguments are there.\n        head = exp[0]\n        args = self.namespace[head][\'args\']\n        pos = len(exp) - 1\n        if pos == len(args):\n          result = [\')\']\n        else:\n          result = self.namespace.valid_tokens(\n            args[pos], self.get_type_ancestors)\n          if self.autocomplete is not None:\n            valid_tokens = result\n            evaled_exp = [self.eval(item) for item in exp]\n            evaled_tokens = [self.eval(tk) for tk in valid_tokens]\n            result = self.autocomplete(\n              evaled_exp, valid_tokens, evaled_tokens, self.namespace)\n      # If at the beginning of a new expression.\n      else:\n        result = self.namespace.valid_tokens(\n          {\'types\': [\'head\']}, self.get_type_ancestors)\n    return result\n      \n  def parse_step(self, token):\n    """"""Run the parser for one step with given token which parses tokens into expressions.""""""\n    self.history.append(token)\n    if token == END_TK:\n      self.done = True\n    elif token == \'(\':\n      self.exp_stack.append([])\n    elif token == \')\':\n      # One list is finished.\n      new_exp = self.exp_stack.pop()\n      if self.exp_stack:\n        self.exp_stack[-1].append(new_exp)\n      else:\n        self.exp_stack = []\n        return new_exp\n    elif self.exp_stack:\n      self.exp_stack[-1].append(token)\n    else:\n      # Atom expression.\n      return token\n      \n  def tokenize(self, chars):\n    """"""Convert a string of characters into a list of tokens.""""""\n    return chars.replace(\'(\', \' ( \').replace(\')\', \' ) \').split()\n      \n  def get_type_ancestors(self, type):\n    return self.type_ancestry[type]\n\n  def infer_type(self, return_type, arg_types):\n    """"""Infer the type of the returned value of a function.""""""\n    if hasattr(return_type, \'__call__\'):\n      return return_type(*arg_types)\n    else:\n      return return_type\n\n  def eval(self, x, namespace=None):\n    """"""Another layer above _eval to handle exceptions.""""""\n    try:\n      result = self._eval(x, namespace)\n    except Exception as e:\n      print \'Error: \', e\n      exc_type, exc_obj, exc_tb = sys.exc_info()\n      fname = os.path.split(exc_tb.tb_frame.f_code.co_filename)[1]\n      print(exc_type, fname, exc_tb.tb_lineno)\n      print \'when evaluating \', x\n      print self.history\n      pprint.pprint(self.namespace)\n      raise e\n      result = None\n    return result\n\n  def _eval(self, x, namespace=None):\n    """"""Evaluate an expression in an namespace.""""""\n    if namespace is None:\n      namespace = self.namespace\n    if is_symbol(x):      # variable reference\n      return namespace.get_object(x).copy()\n    elif x[0] == \'define\':         # (define name exp)\n      (_, name, exp) = x\n      obj = self._eval(exp, namespace)\n      namespace[name] = obj\n      return obj\n    else:\n      # Execute a function.\n      proc = self._eval(x[0], namespace)\n      args = [self._eval(exp, namespace) for exp in x[1:]]\n      arg_values = [arg[\'value\'] for arg in args]\n      if proc[\'type\'] == \'global_primitive_function\':\n        arg_values += [self.namespace]\n      value = proc[\'value\'](*(arg_values))\n      arg_types = [arg[\'type\'] for arg in args]\n      type = self.infer_type(proc[\'return_type\'], arg_types)\n      return {\'value\': value, \'type\': type, \'is_constant\': False}\n\n  def step(self, token):\n    """"""Open AI gym inferface.""""""\n    result = self.read_token(token)\n    observation = token\n    reward = 0.0\n    done = self.done\n    if (result is None) or self.done:\n      write_pos = None\n    else:\n      write_pos = self.namespace.n_var - 1\n\n    info = {\'result\': result,\n            \'write_pos\': write_pos}\n    return observation, reward, done, info\n\n  def get_last_var_loc(self):\n    return self.namespace.n_var - 1\n\n  def interactive(self, prompt=\'> \', assisted=False):\n    """"""A prompt-read-eval-print loop.""""""\n    temp = self.assisted\n    try:\n      self.assisted = assisted\n      while True:\n        try:\n          tokens = self.tokenize(raw_input(prompt))\n          for tk in tokens:\n            result = self.read_token(tk)\n          print result[\'value\']\n        except Exception as e:\n          print e\n          continue\n    finally:\n      self.assisted = temp\n\n  def has_extra_work(self):\n    """"""Check if the current solution contains some extra/wasted work.""""""\n    all_var_names = [\'v{}\'.format(i)\n                     for i in range(self.namespace.n_var)]\n    for var_name in all_var_names:\n      obj = self.namespace.get_object(var_name)\n      # If some variable is not given as constant, not used\n      # in other expressions and not the last one, then\n      # generating it is some extra work that should not be\n      # done.\n      if ((not obj[\'is_constant\']) and\n          (var_name not in self.history) and\n          (var_name != self.namespace.last_var)):\n        return True\n    return False\n\n  def clone(self):\n    """"""Make a copy of itself, used in search.""""""\n    new = LispInterpreter(\n      self.type_hierarchy, self.max_mem, self.max_n_exp, self.assisted)\n\n    new.history = self.history[:]\n    new.exp_stack = copy.deepcopy(self.exp_stack)\n    new.n_exp = self.n_exp\n    new.namespace = self.namespace.clone()\n    return new\n\n  def get_vocab(self):\n    mem_tokens = []\n    for i in range(self.max_mem):\n      mem_tokens.append(\'v{}\'.format(i))\n    vocab = data_utils.Vocab(\n      self.namespace.get_all_names() + SPECIAL_TKS + mem_tokens)\n    return vocab\n\n\nclass Namespace(OrderedDict):\n  """"""Namespace is a mapping from names to values.\n\n  Namespace maintains the mapping from names to their\n  values. It also generates new variable names for memory\n  slots (v0, v1...), and support finding a subset of\n  variables that fulfill some type constraints, (for\n  example, find all the functions or find all the entity\n  lists).\n  """"""\n  def __init__(self, *args, **kwargs):\n    """"""Initialize the namespace with a list of functions.""""""\n    # params = dict(zip(names, objs))\n    super(Namespace, self).__init__(*args, **kwargs)\n    self.n_var = 0\n    self.last_var = None\n\n  def clone(self):\n    new = Namespace(self)\n    new.n_var = self.n_var\n    new.last_var = self.last_var\n    return new\n\n  def generate_new_name(self):\n    """"""Create and return a new variable.""""""\n    name = \'v{}\'.format(self.n_var)\n    self.last_var = name\n    self.n_var += 1\n    return name\n\n  def valid_tokens(self, constraint, get_type_ancestors):\n    """"""Return all the names/tokens that fulfill the constraint.""""""\n    return [k for k, v in self.iteritems()\n            if self._is_token_valid(v, constraint, get_type_ancestors)]\n\n  def _is_token_valid(self, token, constraint, get_type_ancestors):\n    """"""Determine if the token fulfills the given constraint.""""""\n    type = token[\'type\']\n    return set(get_type_ancestors(type) + [type]).intersection(constraint[\'types\'])\n\n  def get_value(self, name):\n    return self[name][\'value\']\n\n  def get_object(self, name):\n    return self[name]\n\n  def get_last_value(self):\n    if self.last_var is None:\n      return None\n    else:\n      return self.get_value(self.last_var)\n\n  def get_all_names(self):\n    return self.keys()\n\n  def reset_variables(self):\n    for k in self.keys():\n      if re.match(r\'v\\d+\', k):\n        del self[k]\n    self.n_var = 0\n    self.last_var = None\n      \n\ndef is_symbol(x):\n  return isinstance(x, str) or isinstance(x, unicode)\n\n\ndef create_type_ancestry(type_tree):\n  type_ancestry = {}\n  for type, _ in type_tree.iteritems():\n    _get_type_ancestors(type, type_tree, type_ancestry)\n  return type_ancestry\n\n\ndef _get_type_ancestors(type, type_hrchy, type_ancestry):\n  """"""Compute the ancestors of a type with memorization.""""""\n  if type in type_ancestry:\n    return type_ancestry[type]\n  else:\n    parents = type_hrchy[type]\n    result = parents[:]\n    for p in parents:\n      ancestors = _get_type_ancestors(p, type_hrchy, type_ancestry)\n      for a in ancestors:\n        if a not in result:\n          result.append(a)\n    type_ancestry[type] = result\n    return result\n'"
nsm/data_utils.py,0,"b'""""""Utility fuctions to preprocess data.""""""\n\nimport random\nimport six\nimport collections\n\nimport numpy as np\n\nimport nlp_utils\n\nDECODE_TK = \'<DECODE>\'\nUNK_TK = \'<UNK>\'\nSTART_TK = \'<START>\'\nEND_TK = \'<END>\'\n\n\n# Utility functions to deal with input data.\ndef read_seq_dataset_from_file(\n    filename, max_vocab_size=1000000,\n    min_count=0, unk_tk=UNK_TK,\n    start_tk=START_TK, decode_tk=DECODE_TK,\n    end_tk=END_TK, tokenize=True):\n  """"""Get the sequences and vocab from a file.\n\n  Args:\n    filename: name of file.\n    max_vocab_size: the maximum number of tokens in the vocab.\n    min_count: the minimum number of appearance for a token to\n    be added into the vocab.\n    unk_tk: the unknown token.\n    start_tk: the start of sentence token.\n    decode_tk: the start of decoding token.\n    end_tk: the end of decoding token.\n    tokenize: Whether to tokenize the text in the file.\n\n  Returns:\n    seqs: a list of lists of tokens.\n    vocab: a Vocab object created from the file.\n  """"""\n  vocab = generate_vocab_from_file(\n      filename, tokenize=tokenize,\n      max_vocab_size=max_vocab_size,\n      min_count=min_count, unk_tk=unk_tk,\n      start_tk=start_tk, decode_tk=decode_tk,\n      end_tk=end_tk)\n  seqs = []\n  with open(filename, \'r\') as f:\n    for line in f:\n      if tokenize:\n        tokens = nlp_utils.tokenize(line)\n      else:\n        tokens = line.strip().split()\n      seqs.append(tokens)\n  return seqs, vocab\n\n\ndef create_lm_inputs_labels(dataset, vocab):\n  """"""Create inputs and labels for language modelling.""""""\n  decode_id = vocab.decode_id\n  end_id = vocab.end_id\n  inputs = [[decode_id] + vocab.lookup(seq) for seq in dataset]\n  labels = [vocab.lookup(seq) + [end_id] for seq in dataset]\n  return inputs, labels\n\n\ndef create_seq2seq_inputs(en_dataset, en_vocab, de_dataset, de_vocab):\n  """"""Create encoder inputs, decoder inputs and targets for seq2seq training.""""""\n  start_id = en_vocab.start_id\n  en_inputs = [[start_id] + en_vocab.lookup(seq) for seq in en_dataset]\n\n  decode_id = de_vocab.decode_id\n  end_id = de_vocab.end_id\n  inputs = [[decode_id] + de_vocab.lookup(seq) for seq in de_dataset]\n  targets = [de_vocab.lookup(seq) + [end_id] for seq in de_dataset]\n  return en_inputs, inputs, targets\n\n\n# Utilities for generating and using vocabulary.\ndef generate_vocab_from_file(\n    filename, tokenize=True, max_vocab_size=1000000, min_count=0,\n    unk_tk=UNK_TK, start_tk=START_TK,\n    decode_tk=DECODE_TK, end_tk=END_TK):\n  """"""Create vocab from a given file.""""""\n  with open(filename, \'r\') as f:\n    vocab = generate_vocab_from_stream(\n        f, tokenize=tokenize, max_vocab_size=max_vocab_size,\n        min_count=min_count, unk_tk=unk_tk,\n        start_tk=start_tk, decode_tk=decode_tk,\n        end_tk=end_tk)\n  return vocab\n\n\ndef generate_vocab_from_stream(\n    text_stream, max_vocab_size=1000000, min_count=0,\n    unk_tk=UNK_TK, start_tk=START_TK,\n    decode_tk=DECODE_TK, end_tk=END_TK, tokenize=True):\n  """"""Create a vocab from a given text stream.""""""\n  token_list = []\n  for line in text_stream:\n    if tokenize:\n      new_list = nlp_utils.tokenize(line)\n    else:\n      new_list = line.strip().split()\n    token_list += new_list\n  return generate_vocab_from_list(\n      token_list, max_vocab_size=max_vocab_size,\n      min_count=min_count, unk_tk=unk_tk,\n      start_tk=start_tk, decode_tk=decode_tk,\n      end_tk=end_tk)\n\n\ndef generate_vocab_from_list(\n    token_list, max_vocab_size=1000000, min_count=0,\n    unk_tk=UNK_TK, start_tk=START_TK,\n    decode_tk=DECODE_TK, end_tk=END_TK):\n  """"""Create a vocab from a list of tokens.""""""\n  token_count = {}\n  for tk in token_list:\n    try:\n      token_count[tk] += 1\n    except KeyError:\n      token_count[tk] = 1\n  return generate_vocab_from_token_count(\n      token_count, max_vocab_size=max_vocab_size,\n      min_count=min_count, unk_tk=unk_tk,\n      start_tk=start_tk, decode_tk=decode_tk,\n      end_tk=end_tk)\n\n\ndef sort_kv_pairs_by_value(d):\n  """"""Turn a dict into a list of key-value pairs, sorted by value.""""""\n  return [(k, v) for v, k in\n          sorted([(v, k) for k, v in d.items()], reverse=True)]\n\n\ndef vocab_lookup(item, vocab, unknown):\n  """"""Look up the iterm from the vocabulary.\n\n  Args:\n    item: a string, an integer or a nested sequence or numpy\n      arrays of strings or integers. \n    vocab: a Vocab object.\n    unknown: Any value. This will be used when a integer or\n      string is not in Vocab. \n\n  Returns:\n    result: same structure as item, with the integer or\n      string replaced by the corresponding lookup in the Vocab.\n  """"""\n  if (isinstance(item, str) or isinstance(item, int)\n      or isinstance(item, unicode)):\n    result = vocab.get(item, unknown)\n  elif is_sequence(item) or isinstance(item, np.ndarray):\n    result = [vocab_lookup(x, vocab, unknown) for x in item]\n  else:\n    raise ValueError(\'Can not handle type {}\'.format(type(item)))\n  return result\n\n\ndef generate_vocab_from_token_count(\n    token_count, max_vocab_size=1000000,\n    min_count=0, unk_tk=UNK_TK,\n    start_tk=START_TK, decode_tk=DECODE_TK,\n    end_tk=END_TK):\n  """"""Generate vocabulary from token count information.""""""\n  special_tks = [unk_tk, start_tk, decode_tk, end_tk]\n  token_count_pairs = sort_kv_pairs_by_value(token_count)\n  token_count_pairs = [(tk, count) for (tk, count)\n                       in token_count_pairs\n                       if (count >= min_count) and tk not in special_tks]\n  token_count_pairs = token_count_pairs[:max_vocab_size-4]\n  # The vocab are organized as: first special tokens, then\n  # tokens ordered by frequency.\n  tokens = [p[0] for p in token_count_pairs]\n  return Vocab(\n      tokens, unk_tk=UNK_TK, start_tk=START_TK, decode_tk=DECODE_TK,\n      end_tk=END_TK)\n\n\nclass Vocab(object):\n  """"""A vocabulary used in language tasks.""""""\n\n  def __init__(self, tokens, unk_tk=UNK_TK,\n               start_tk=START_TK, decode_tk=DECODE_TK,\n               end_tk=END_TK):\n    special_tks = [unk_tk, start_tk, decode_tk, end_tk]\n    self.unk_tk = unk_tk\n    self.unk_id = 0\n    self.start_tk = start_tk\n    self.start_id = 1\n    self.decode_tk = decode_tk\n    self.decode_id = 2\n    self.end_tk = end_tk\n    self.end_id = 3\n    self.special_tks = special_tks\n\n    all_tokens = special_tks + tokens\n    self.vocab = {}\n    self.rev_vocab = {}\n    for i, token in enumerate(all_tokens):\n      if token in self.vocab:\n        raise ValueError(\'token {} repeated\'.format(token))\n      self.vocab[token] = i\n      self.rev_vocab[i] = token\n\n    self.size = len(self.vocab)\n\n  def lookup(self, item, reverse=False):\n    """"""Lookup the id/token of the token/id.""""""\n    if reverse:\n      result = vocab_lookup(item, self.rev_vocab, None)\n    else:\n      result = vocab_lookup(item, self.vocab, self.unk_id)\n    return result\n\n  def load_vocab(self, vocab):\n    self.vocab = vocab\n    self.rev_vocab = {}\n    for token, i in vocab.iteritems():\n      self.rev_vocab[i] = token\n\n    self.size = len(self.vocab)\n\n\n# Utilities for batching. \nclass BatchIterator(object):\n  def __init__(self, feed_dict, shuffle=False, batch_size=32):\n    self.batch_size = batch_size\n    kv_pairs = [(k, v) for k, v in feed_dict.items() if v is not None]\n    self.keys = [p[0] for p in kv_pairs]\n    self.examples = zip(*[p[1] for p in kv_pairs])\n    self.n_examples = len(self.examples)\n    if shuffle:\n      random.shuffle(self.examples)\n\n  def __iter__(self):\n    def _iterator():\n      bs = self.batch_size\n      idx = 0\n      while idx < self.n_examples:\n        batch_examples = self.examples[idx:idx+bs]\n        idx += bs\n        unzipped_batch_examples = list(zip(*batch_examples))\n        batch_feed_dict = dict(\n          [(k, list(v)) for k, v in zip(self.keys, unzipped_batch_examples)])\n        batch_feed_dict[\'batch_size\'] = len(batch_examples)\n        batch_feed_dict[\'max_batch_size\'] = self.batch_size\n        yield batch_feed_dict\n    return _iterator()\n\n  \ndef convert_seqs_to_batch(seqs):\n  n_seqs = len(seqs)\n  sequence_length = []\n  for seq in seqs:\n    sequence_length.append(len(seq))\n  max_len = max(sequence_length)\n  one_step = seqs[0][0]\n  try:\n    step_shape = one_step.shape\n  except AttributeError:\n    step_shape = ()\n  # The batch matrix is padded with all 0s.\n  batch = np.zeros((n_seqs, max_len) + step_shape)\n  for i, (seq, seq_len) in enumerate(\n      zip(seqs, sequence_length)):\n    if seq_len > 0:\n      batch[i, 0:seq_len] = seq\n  sequence_length = np.array(sequence_length)\n  return (batch, sequence_length)\n\n\ndef convert_batch_to_seqs(batch):\n  array = batch.tensor\n  sequence_length = batch.sequence_length\n  seqs = np.vsplit(array, array.shape[0])\n  result = []\n  for seq, seq_len in zip(seqs, sequence_length):\n    result.append(list(seq[0][:seq_len]))\n  return result  \n\n\ndef deep_vstack(structs):\n  """"""Turn tuples of arrays into one tuple of stacked arrays.""""""\n  if len(structs) == 0:\n    raise \'No structs available.\'\n  flat_structs = [flatten(struct) for struct in structs]\n  flat_result = [np.vstack(args) for args in zip(*flat_structs)]\n  result = pack_sequence_as(structs[0], flat_result)\n  return result\n\n\ndef deep_split(struct):\n  flat_struct = flatten(struct)\n  new_flat_structs = zip(*[np.split(x, x.shape[0]) for x in flat_struct])\n  new_structs = [pack_sequence_as(struct, x) for x in new_flat_structs]\n  return new_structs\n\n\nclass BatchConverter(object):\n  """"""BatchConverter converts input data into dictionaries of\n  batches of data (by stacking and padding) that is ready to\n  feed into TF graphs.\n\n  """"""\n  def __init__(self, tuple_keys=None, seq_keys=None, preprocess_fn=None):\n    if tuple_keys == None:\n      self.tuple_keys = []\n    else:\n      self.tuple_keys = tuple_keys\n\n    if seq_keys == None:\n      self.seq_keys = []\n    else:\n      self.seq_keys = seq_keys\n\n    self.preprocess_fn = preprocess_fn\n\n  def add_preprocess(self, preprocess_fn):\n    self.preprocess_fn = preprocess_fn\n\n  def convert(self, batch_dict):\n    if self.preprocess_fn is not None:\n      self.preprocess_fn(batch_dict)\n    for k, v in batch_dict.iteritems():\n      if k in self.tuple_keys:\n        batch_dict[k] = deep_vstack(v)\n      elif k in self.seq_keys:\n        batch_dict[k] = convert_seqs_to_batch(v)\n    return batch_dict\n\n\nclass BatchAggregator(object):\n  def __init__(self, tuple_keys=None, seq_keys=None, num_keys=None, keep_keys=None):\n\n    if tuple_keys == None:\n      self.tuple_keys = set()\n    else:\n      self.tuple_keys = set(tuple_keys)\n\n    if seq_keys == None:\n      self.seq_keys = set()\n    else:\n      self.seq_keys = set(seq_keys)\n\n    if num_keys == None:\n      self.num_keys = set()\n    else:\n      self.num_keys = set(num_keys)\n\n    if keep_keys == None:\n      self.keep_keys = set()\n    else:\n      self.keep_keys = set(keep_keys)\n      \n    self.all_keys = set.union(\n      self.seq_keys, self.tuple_keys, self.num_keys, self.keep_keys)\n\n    self.result_dict = {}\n\n  def reset(self):\n    self.result_dict = {}\n\n  def merge(self, batch_dict):\n    for k in self.all_keys:\n      b = batch_dict[k]\n      if k in self.seq_keys:\n        v = convert_batch_to_seqs(b)\n      elif k in self.tuple_keys:\n        v = deep_split(b)\n      elif k in self.num_keys:\n        v = b\n      elif k in self.keep_keys:\n        v = [b]\n      if k in self.result_dict:\n        self.result_dict[k] += v\n      else:\n        self.result_dict[k] = v\n    \n  @property\n  def result(self):\n    return self.result_dict\n\n\n# Utilities for dealing with data with internal structures. \ndef zero_struct_like(struct):\n  return constant_struct_like(struct, 0.0)\n\n\ndef constant_struct_like(struct, constant):\n  flat_struct = flatten(struct)\n  new_flat_struct = [np.ones_like(x) * constant for x in flat_struct]\n  return pack_sequence_as(struct, new_flat_struct)\n\n    \n# The following code are copied from TensorFlow source code. \ndef is_sequence(seq):\n  """"""Returns a true if its input is a collections.Sequence (except strings).\n\n  Args:\n    seq: an input sequence.\n\n  Returns:\n    True if the sequence is a not a string and is a collections.Sequence.\n  """"""\n  return (isinstance(seq, collections.Sequence)\n          and not isinstance(seq, six.string_types))\n\n\ndef flatten(nest):\n  """"""Returns a flat sequence from a given nested structure.\n\n  If `nest` is not a sequence, this returns a single-element list: `[nest]`.\n\n  Args:\n    nest: an arbitrarily nested structure or a scalar object.\n      Note, numpy arrays are considered scalars.\n\n  Returns:\n    A Python list, the flattened version of the input.\n  """"""\n  return list(_yield_flat_nest(nest)) if is_sequence(nest) else [nest]\n\n\ndef _yield_flat_nest(nest):\n  for n in nest:\n    if is_sequence(n):\n      for ni in _yield_flat_nest(n):\n        yield ni\n    else:\n      yield n\n\n\ndef _packed_nest_with_indices(structure, flat, index):\n  """"""Helper function for pack_nest_as.\n\n  Args:\n    structure: Substructure (tuple of elements and/or tuples) to mimic\n    flat: Flattened values to output substructure for.\n    index: Index at which to start reading from flat.\n\n  Returns:\n    The tuple (new_index, child), where:\n      * new_index - the updated index into `flat` having processed `structure`.\n      * packed - the subset of `flat` corresponding to `structure`,\n                 having started at `index`, and packed into the same nested\n                 format.\n\n  Raises:\n    ValueError: if `structure` contains more elements than `flat`\n      (assuming indexing starts from `index`).\n  """"""\n  packed = []\n  for s in structure:\n    if is_sequence(s):\n      new_index, child = _packed_nest_with_indices(s, flat, index)\n      packed.append(_sequence_like(s, child))\n      index = new_index\n    else:\n      packed.append(flat[index])\n      index += 1\n  return index, packed\n\n\ndef pack_sequence_as(structure, flat_sequence):\n  """"""Returns a given flattened sequence packed into a nest.\n\n  If `structure` is a scalar, `flat_sequence` must be a single-element list;\n  in this case the return value is `flat_sequence[0]`.\n\n  Args:\n    structure: tuple or list constructed of scalars and/or other tuples/lists,\n      or a scalar.  Note: numpy arrays are considered scalars.\n    flat_sequence: flat sequence to pack.\n\n  Returns:\n    packed: `flat_sequence` converted to have the same recursive structure as\n      `structure`.\n\n  Raises:\n    ValueError: If nest and structure have different element counts.\n  """"""\n  if not is_sequence(flat_sequence):\n    raise TypeError(\'flat_sequence must be a sequence\')\n\n  if not is_sequence(structure):\n    if len(flat_sequence) != 1:\n      raise ValueError(\'Structure is a scalar but len(flat_sequence) == %d > 1\'\n                       % len(flat_sequence))\n    return flat_sequence[0]\n\n  flat_structure = flatten(structure)\n  if len(flat_structure) != len(flat_sequence):\n    raise ValueError(\n        \'Could not pack sequence. Structure had %d elements, but flat_sequence \'\n        \'had %d elements.  Structure: %s, flat_sequence: %s.\'\n        % (len(flat_structure), len(flat_sequence), structure, flat_sequence))\n\n  _, packed = _packed_nest_with_indices(structure, flat_sequence, 0)\n  return _sequence_like(structure, packed)\n\n\ndef _sequence_like(instance, args):\n  """"""Converts the sequence `args` to the same type as `instance`.\n\n  Args:\n    instance: an instance of `tuple`, `list`, or a `namedtuple` class.\n    args: elements to be converted to a sequence.\n\n  Returns:\n    `args` with the type of `instance`.\n  """"""\n  if (isinstance(instance, tuple) and\n      hasattr(instance, \'_fields\') and\n      isinstance(instance._fields, collections.Sequence) and\n      all(isinstance(f, six.string_types) for f in instance._fields)):\n    # This is a namedtuple\n    return type(instance)(*args)\n  else:\n    # Not a namedtuple\n    return type(instance)(args)\n\n\ndef map_structure(func, *structure):\n  """"""Applies `func` to each entry in `structure` and returns a new structure.\n\n  Applies `func(x[0], x[1], ...)` where x[i] is an entry in\n  `structure[i]`.  All structures in `structure` must have the same arity,\n  and the return value will contain the results in the same structure.\n\n  Args:\n    func: A callable that acceps as many arguments are there are structures.\n    *structure: scalar, or tuple or list of constructed scalars and/or other\n      tuples/lists, or scalars.  Note: numpy arrays are considered scalars.\n\n  Returns:\n    A new structure with the same arity as `structure`, whose values correspond\n    to `func(x[0], x[1], ...)` where `x[i]` is a value in the corresponding\n    location in `structure[i]`.\n\n  Raises:\n    TypeError: If `func` is not callable or if the structures do not match\n      each other by depth tree.\n    ValueError: If no structure is provided or if the structures do not match\n      each other by type.\n  """"""\n  if not callable(func):\n    raise TypeError(\'func must be callable, got: %s\' % func)\n\n  if not structure:\n    raise ValueError(\'Must provide at least one structure\')\n\n  for other in structure[1:]:\n    assert_same_structure(structure[0], other)\n\n  flat_structure = [flatten(s) for s in structure]\n  entries = zip(*flat_structure)\n\n  return pack_sequence_as(\n      structure[0], [func(*x) for x in entries])\n\n\n# Check the same structure.\ndef _recursive_assert_same_structure(nest1, nest2):\n  is_sequence_nest1 = is_sequence(nest1)\n  if is_sequence_nest1 != is_sequence(nest2):\n    raise ValueError(\n        ""The two structures don\'t have the same nested structure. ""\n        ""First structure: %s, second structure: %s."" % (nest1, nest2))\n\n  if is_sequence_nest1:\n    type_nest1 = type(nest1)\n    type_nest2 = type(nest2)\n    if type_nest1 != type_nest2:\n      raise TypeError(\n          ""The two structures don\'t have the same sequence type. First ""\n          ""structure has type %s, while second structure has type %s.""\n          % (type_nest1, type_nest2))\n\n    for n1, n2 in zip(nest1, nest2):\n      _recursive_assert_same_structure(n1, n2)\n\n\ndef assert_same_structure(nest1, nest2):\n  """"""Asserts that two structures are nested in the same way.\n\n  Args:\n    nest1: an arbitrarily nested structure.\n    nest2: an arbitrarily nested structure.\n\n  Raises:\n    ValueError: If the two structures do not have the same number of elements or\n      if the two structures are not nested in the same way.\n    TypeError: If the two structures differ in the type of sequence in any of\n      their substructures.\n  """"""\n  len_nest1 = len(flatten(nest1)) if is_sequence(nest1) else 1\n  len_nest2 = len(flatten(nest2)) if is_sequence(nest2) else 1\n  if len_nest1 != len_nest2:\n    raise ValueError(""The two structures don\'t have the same number of ""\n                     ""elements. First structure: %s, second structure: %s.""\n                     % (nest1, nest2))\n  _recursive_assert_same_structure(nest1, nest2)\n    \n'"
nsm/env_factory.py,1,"b'""A collections of environments of sequence generations tasks.""\n\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport collections\nimport pprint\n\nimport nlp_utils\nimport tf_utils\nimport computer_factory\n\nimport bloom_filter\nimport tensorflow as tf\n\n\nclass Environment(object):\n  """"""Environment with OpenAI Gym like interface.""""""\n  \n  def step(self, action):\n    """"""\n    Args:\n      action: an action to execute against the environment.\n\n    Returns:\n      observation:\n      reward:\n      done:\n      info: \n    """"""\n    raise NotImplementedError\n\n\n# Use last action and the new variable\'s memory location as input.\nProgrammingObservation = collections.namedtuple(\n  \'ProgramObservation\', [\'last_actions\', \'output\', \'valid_actions\'])\n\n\nclass QAProgrammingEnv(Environment):\n  """"""An RL environment wrapper around an interpreter to\n  learn to write programs based on question.\n  """"""\n  def __init__(self, en_vocab, de_vocab,\n               question_annotation, answer,\n               constant_value_embedding_fn, \n               score_fn, interpreter, constants=None,\n               punish_extra_work=True,\n               init_interp=True, trigger_words_dict=None,\n               max_cache_size=1e4,\n               name=\'qa_programming\'):\n    self.name=name\n    self.en_vocab = en_vocab\n    self.de_vocab = de_vocab\n    self.end_action = self.de_vocab.end_id\n    self.score_fn = score_fn\n    self.interpreter = interpreter\n    self.answer = answer\n    self.question_annotation = question_annotation\n    self.constant_value_embedding_fn = constant_value_embedding_fn\n    self.constants = constants\n    self.punish_extra_work = punish_extra_work\n    self.error = False\n    self.trigger_words_dict = trigger_words_dict\n    tokens = question_annotation[\'tokens\']\n\n    en_inputs = en_vocab.lookup(tokens)\n    self.n_builtin = len(de_vocab.vocab) - interpreter.max_mem\n    self.n_mem = interpreter.max_mem\n    self.n_exp = interpreter.max_n_exp\n    max_n_constants = self.n_mem - self.n_exp\n\n    constant_spans = []\n    constant_values = []\n    if constants is None:\n      constants = []\n    for c in constants:\n      constant_spans.append([-1, -1])\n      constant_values.append(c[\'value\'])\n      if init_interp:\n        self.interpreter.add_constant(\n          value=c[\'value\'], type=c[\'type\'])\n\n    for entity in question_annotation[\'entities\']:\n      # Use encoder output at start and end (inclusive) step\n      # to create span embedding.\n      constant_spans.append(\n        [entity[\'token_start\'], entity[\'token_end\'] - 1])\n      constant_values.append(entity[\'value\'])\n      if init_interp:\n        self.interpreter.add_constant(\n          value=entity[\'value\'], type=entity[\'type\'])\n\n    constant_value_embeddings = [\n      constant_value_embedding_fn(value) for value in constant_values]\n\n    if len(constant_values) > (self.n_mem - self.n_exp):\n      tf.logging.info(\n        \'Not enough memory slots for example {}, which has {} constants.\'.format(\n          self.name, len(constant_values)))\n\n    constant_spans = constant_spans[:max_n_constants]\n    constant_value_embeddings = constant_value_embeddings[:max_n_constants]\n    self.context = (en_inputs, constant_spans, constant_value_embeddings,\n                    question_annotation[\'features\'],\n                    question_annotation[\'tokens\'])\n\n    # Create output features.\n    prop_features = question_annotation[\'prop_features\']\n    self.id_feature_dict = {}\n    for name, id in de_vocab.vocab.iteritems():\n      self.id_feature_dict[id] = [0]\n      if name in self.interpreter.namespace:\n        val = self.interpreter.namespace[name][\'value\']\n        if ((isinstance(val, str) or isinstance(val, unicode)) and\n            val in prop_features):\n          self.id_feature_dict[id] = prop_features[val]\n      \n    self.cache = SearchCache(name=name, max_elements=max_cache_size)\n    self.use_cache = False\n    self.reset()\n    \n  def get_context(self):\n    return self.context\n\n  def step(self, action, debug=False):\n    self.actions.append(action)\n    if debug:\n      print(\'-\' * 50)\n      print(self.de_vocab.lookup(self.valid_actions, reverse=True))\n      print(\'pick #{} valid action\'.format(action))\n      print(\'history:\')\n      print(self.de_vocab.lookup(self.mapped_actions, reverse=True))\n      print(\'env: {}, cache size: {}\'.format(self.name, len(self.cache._set)))\n      print(\'obs\')\n      pprint.pprint(self.obs)\n\n    if action < len(self.valid_actions) and action >= 0: \n      mapped_action = self.valid_actions[action]\n    else:\n      print(\'-\' * 50)\n      # print(\'env: {}, cache size: {}\'.format(self.name, len(self.cache._set)))\n      print(\'action out of range.\')\n      print(\'action:\')\n      print(action)\n      print(\'valid actions:\')\n      print(self.de_vocab.lookup(self.valid_actions, reverse=True))\n      print(\'pick #{} valid action\'.format(action))\n      print(\'history:\')\n      print(self.de_vocab.lookup(self.mapped_actions, reverse=True))\n      print(\'obs\')\n      pprint.pprint(self.obs)\n      print(\'-\' * 50)\n      mapped_action = self.valid_actions[action]\n      \n    self.mapped_actions.append(mapped_action)\n\n    result = self.interpreter.read_token(\n      self.de_vocab.lookup(mapped_action, reverse=True))\n\n    self.done = self.interpreter.done\n    # Only when the proram is finished and it doesn\'t have\n    # extra work or we don\'t care, its result will be\n    # scored, and the score will be used as reward. \n    if self.done and not (self.punish_extra_work and self.interpreter.has_extra_work()):\n      reward = self.score_fn(self.interpreter.result, self.answer)\n    else:\n      reward = 0.0\n\n    if self.done and self.interpreter.result == [computer_factory.ERROR_TK]:\n      self.error = True\n\n    if result is None or self.done:\n      new_var_id = -1\n    else:\n      new_var_id = self.de_vocab.lookup(self.interpreter.namespace.last_var)\n    valid_tokens = self.interpreter.valid_tokens()\n    valid_actions = self.de_vocab.lookup(valid_tokens)\n\n    # For each action, check the cache for the program, if\n    # already tried, then not valid anymore.\n    if self.use_cache:\n      new_valid_actions = []\n      cached_actions = []\n      partial_program = self.de_vocab.lookup(self.mapped_actions, reverse=True)\n      for ma in valid_actions:\n        new_program = partial_program + [self.de_vocab.lookup(ma, reverse=True)]\n        if not self.cache.check(new_program):\n          new_valid_actions.append(ma)\n        else:\n          cached_actions.append(ma)\n      valid_actions = new_valid_actions\n\n    self.valid_actions = valid_actions\n    self.rewards.append(reward)\n    ob = (tf_utils.MemoryInputTuple(\n      read_ind=mapped_action, write_ind=new_var_id, valid_indices=self.valid_actions),\n          [self.id_feature_dict[a] for a in valid_actions])\n\n    # If no valid actions are available, then stop.\n    if not self.valid_actions:\n      self.done = True\n      self.error = True\n\n    # If the program is not finished yet, collect the\n    # observation.\n    if not self.done:\n      # Add the actions that are filtered by cache into the\n      # training example because at test time, they will be\n      # there (no cache is available).\n      if self.use_cache:\n        valid_actions = self.valid_actions + cached_actions\n        true_ob = (tf_utils.MemoryInputTuple(\n          read_ind=mapped_action, write_ind=new_var_id,\n          valid_indices=valid_actions),\n                   [self.id_feature_dict[a] for a in valid_actions])\n        self.obs.append(true_ob)\n      else:\n        self.obs.append(ob)\n    elif self.use_cache:\n      # If already finished, save it in the cache.\n      self.cache.save(self.de_vocab.lookup(self.mapped_actions, reverse=True))\n\n    return ob, reward, self.done, {}\n      #\'valid_actions\': valid_actions, \'new_var_id\': new_var_id}\n\n  def reset(self):\n    self.actions = []\n    self.mapped_actions = []\n    self.rewards = []\n    self.done = False\n    valid_actions = self.de_vocab.lookup(self.interpreter.valid_tokens())\n    if self.use_cache:\n      new_valid_actions = []\n      for ma in valid_actions:\n        partial_program = self.de_vocab.lookup(\n          self.mapped_actions + [ma], reverse=True)\n        if not self.cache.check(partial_program):\n          new_valid_actions.append(ma)\n      valid_actions = new_valid_actions\n    self.valid_actions = valid_actions\n    self.start_ob = (tf_utils.MemoryInputTuple(\n      self.de_vocab.decode_id, -1, valid_actions),\n                     [self.id_feature_dict[a] for a in valid_actions])\n    self.obs = [self.start_ob]\n\n  def interactive(self):\n    self.interpreter.interactive()\n    print(\'reward is: %s\' % score_fn(self.interpreter))\n\n  def clone(self):\n    new_interpreter = self.interpreter.clone()\n    new = QAProgrammingEnv(\n      self.en_vocab, self.de_vocab, score_fn=self.score_fn,\n      question_annotation=self.question_annotation,\n      constant_value_embedding_fn=self.constant_value_embedding_fn,\n      constants=self.constants,\n      answer=self.answer, interpreter=new_interpreter,\n      init_interp=False)\n    new.actions = self.actions[:]\n    new.mapped_actions = self.mapped_actions[:]\n    new.rewards = self.rewards[:]\n    new.obs = self.obs[:]\n    new.done = self.done\n    new.name = self.name\n    # Cache is shared among all copies of this environment.\n    new.cache = self.cache\n    new.use_cache = self.use_cache\n    new.valid_actions = self.valid_actions\n    new.error = self.error\n    new.id_feature_dict = self.id_feature_dict\n    new.punish_extra_work = self.punish_extra_work\n    new.trigger_words_dict = self.trigger_words_dict\n    return new\n\n  def show(self):\n    program = \' \'.join(\n      self.de_vocab.lookup([o.read_ind for o in self.obs], reverse=True))\n    valid_tokens = \' \'.join(self.de_vocab.lookup(self.valid_actions, reverse=True))\n    return \'program: {}\\nvalid tokens: {}\'.format(program, valid_tokens)\n\n\nclass SearchCache(object):\n  def __init__(self, name, size=None, max_elements=1e4, error_rate=1e-8):\n    self.name = name\n    self.max_elements = max_elements\n    self.error_rate = error_rate\n    self._set = bloom_filter.BloomFilter(\n      max_elements=max_elements, error_rate=error_rate)\n    \n  def check(self, tokens):\n    return \' \'.join(tokens) in self._set\n\n  def save(self, tokens):\n    string = \' \'.join(tokens)\n    self._set.add(string)\n    \n  def is_full(self):\n    return \'(\' in self._set\n\n  def reset(self):\n    self._set = bloom_filter.BloomFilter(\n      max_elements=self.max_elements, error_rate=self.error_rate)\n'"
nsm/executor_factory.py,0,"b'""""""Utility functions to interact with knowledge graph.""""""\n\nimport functools\nimport pprint\nimport time\nimport collections\n\nclass Executor(object):\n  """"""Executors implements the basic subroutines and provide\n  the API to the computer.\n  """"""\n  def get_api(self, config):\n    \'Provide API to the computer.\'\n    raise NotImplementedError()\n\n\ndef get_simple_type_hierarchy():\n  type_hierarchy = {\n    \'entity_list\': [\'atom_list\'], \'list\': [],\n    \'num_list\': [\'ordered_list\'],\n    \'datetime_list\': [\'ordered_list\'],\n    \'ordered_list\': [\'atom_list\'],\n    \'atom_list\': [\'list\'],\n    \'string_list\': [\'list\'],\n    \'string_property\': [\'property\'],\n    # Atom as it doesn\'t have sub-parts.\n    \'ordered_property\': [\'atom_property\'],\n    \'entity_property\': [\'atom_property\'],\n    \'atom_property\': [\'property\'],\n    \'datetime_property\': [\'ordered_property\'],\n    \'num_property\': [\'ordered_property\'],\n    \'num\': [], \'int\': [\'num\'], \'property\': [], \'symbol\': [],\n    \'function\': [\'head\'],\n    \'head\': [],\n    \'primitive_function\': [\'function\'],\n    \'global_primitive_function\': [\'primitive_function\'],\n    \'<ERROR>\': []}\n  return type_hierarchy\n\n\nclass SimpleKGExecutor(Executor):\n  """"""This executor assumes that the knowledge graph is\n  encoded as a dictionary.\n  """"""\n  def __init__(self, kg_info):\n    """"""Given a knowledge graph, the number properties and\n    the datetime properties, initialize an executor that\n    implements the basic subroutines.\n\n    Args:\n      kg_info: a dictionary with three keys.\n    """"""\n    self.kg = kg_info[\'kg\']\n    self.num_props = kg_info[\'num_props\']\n    self.datetime_props = kg_info[\'datetime_props\']\n    self.props = kg_info[\'props\']\n    \n  def hop(self, entities, prop, keep_dup=False):\n    """"""Get the property of a list of entities.""""""\n    if keep_dup:\n      result = []\n    else:\n      result = set()\n    for ent in entities:\n      try:\n        if keep_dup:\n          result += self.kg[ent][prop]\n        else:\n          result = result.union(self.kg[ent][prop])\n      except KeyError:\n        continue\n    return list(result)\n    \n  def filter_equal(self, ents_1, ents_2, prop):\n    """"""From ents_1, filter out the entities whose property equal to ents_2.""""""\n    result = []\n    for ent in ents_1:\n      if set(self.hop([ent], prop)) == set(ents_2):\n        result.append(ent)\n    return result\n\n  def filter_not_equal(self, ents_1, ents_2, prop):\n    """"""From ents_1, filter out the entities whose property equal to ents_2.""""""\n    result = []\n    for ent in ents_1:\n      if set(self.hop([ent], prop)) != set(ents_2):\n        result.append(ent)\n    return result\n    \n  def get_num_prop_val(self, ent, prop):\n    """"""Get the value of an entities\' number property. """"""\n    # If there are multiple values, then take the first one.\n    prop_str_list = self.hop([ent], prop)\n    try:\n      prop_str = prop_str_list[0]\n      prop_val = float(prop_str)\n    except (ValueError, IndexError):\n      prop_val = None\n    return prop_val\n\n  def get_datetime_prop_val(self, ent, prop):\n    """"""Get the value of an entities\' date time property. """"""\n    # If there are multiple values, then take the first one.\n    prop_str_list = self.hop([ent], prop)\n    try:\n      prop_str = prop_str_list[0]\n      if prop_str[0] == \'-\':\n        sign = -1\n        prop_str = prop_str[1:]\n      else:\n        sign = 1\n      result = [float(n) for n in prop_str.replace(\'x\', \'0\').split(\'-\')]\n      day = 0\n      for n, unit in zip(result, [365, 30, 1]):\n        day += n*unit\n      day *= sign\n      prop_val = day\n    except (ValueError, IndexError):\n      prop_val = None\n    return prop_val\n\n  def sort_select(self, entities, prop, ind):\n    """"""Sort the entities using prop then select the i-th one.""""""\n    if prop in self.num_props:\n      get_val = self.get_num_prop_val\n    elif prop in self.datetime_props:\n      get_val = self.get_datetime_prop_val\n    else:\n      raise(ValueError(prop))\n    vals = []\n    new_ents = []\n    for ent in entities:\n      val = get_val(ent, prop)\n      if val is not None:\n        new_ents.append(ent)\n        vals.append(val)\n    ent_vals = zip(new_ents, vals)\n    best_ent_val = sorted(\n      ent_vals,\n      key=lambda x: x[1])[ind]\n    best_score = best_ent_val[1]\n    result = [ent for ent, val in ent_vals if val == best_score]\n    return result\n    # return [best_ent_val[0]]\n\n  def argmax(self, entities, prop):\n    return self.sort_select(entities, prop, -1)\n\n  def argmin(self, entities, prop):\n    return self.sort_select(entities, prop, 0)\n\n  def valid_props(self, source_mids, token_val_dict, target_mids=None, condition_fn=None):\n    connected_props = self.get_props(source_mids, target_mids, condition_fn=condition_fn)\n    valid_tks = []\n    for tk, prop in token_val_dict.iteritems():\n      if prop in connected_props:\n        valid_tks.append(tk)\n    return valid_tks\n\n  def is_connected(self, source_ents, target_ents, prop):\n    return set(self.hop(source_ents, prop)) == set(target_ents)\n\n  def get_props(\n      self, source_ents, target_ents=None, debug=False, condition_fn=None):\n    """"""Get the properties that goes from source to targets.""""""\n    props = set()\n    if condition_fn is None:\n      condition_fn = self.is_connected\n    if debug:\n      print \'=\' * 100\n    for ent in source_ents:\n      if debug:\n        print \'@\' * 20\n        print ent\n      if ent in self.kg:\n        ent_props = self.kg[ent].keys()\n        if target_ents is not None:\n          for p in ent_props:\n            if debug:\n              print\n              print p\n              print self.hop([ent], p)\n            # if set(self.hop([ent], p)) == set(target_ents):\n            if condition_fn([ent], target_ents, p):\n              props.add(p)\n        else:\n          props = props.union(ent_props)\n    if debug:\n      print \'in get props\'\n      print source_ents\n      print target_ents\n      print props\n      print \'=\' * 100\n    return list(props)\n    \n  def autocomplete_hop(self, exp, tokens, token_vals):\n    l = len(exp)\n    if l == 2:  # second argument is a property.\n      source_mids = exp[1]\n      token_val_dict = dict(zip(tokens, token_vals))      \n      valid_tks = self.valid_props(source_mids, token_val_dict)\n    else:\n      valid_tks = tokens\n    return valid_tks\n\n  def autocomplete_argm(self, exp, tokens, token_vals, debug=False):\n    l = len(exp)\n    if l == 1: # first argument has more than one entity.\n      valid_tks = [tk for tk, val in zip(tokens, token_vals)\n                   if len(val) > 1]\n    elif l == 2:  # second argument is a property.\n      source_mids = exp[1]\n      token_val_dict = dict(zip(tokens, token_vals))\n      valid_tks = self.valid_props(source_mids, token_val_dict)\n    else:\n      valid_tks = tokens\n    if debug:\n      print \'*\' * 30\n      print exp\n      print tokens\n      print valid_tks\n      print \'*\' * 30\n    return valid_tks\n\n  def autocomplete_filter_equal(self, exp, tokens, token_vals, debug=False):\n    l = len(exp)\n    if l == 1:\n      valid_tks = [tk for tk, val in zip(tokens, token_vals)\n                   if len(val) > 1]\n    elif l == 2:\n      valid_tks = []\n      for tk, val in zip(tokens, token_vals):\n        # The second argument must have some connection with\n        # the first argument.\n        if self.get_props(exp[1], val):\n          valid_tks.append(tk)\n    elif l == 3:\n      token_val_dict = dict(zip(tokens, token_vals))\n      valid_tks = self.valid_props(exp[1], token_val_dict, exp[2])\n    else:\n      raise ValueError(\'Expression is too long: {}\'.format(l))\n\n    if debug:\n      print\n      print \'+\' * 30\n      print \'in filter equal\'\n      print exp\n      print tokens\n      print valid_tks\n      print \'+\' * 30\n\n    return valid_tks\n\n  def get_api(self):\n    func_dict = collections.OrderedDict()\n    func_dict[\'hop\'] = dict(\n      name=\'hop\',\n      args=[{\'types\': [\'entity_list\']},\n            {\'types\': [\'property\']}],\n      return_type=\'entity_list\',\n      autocomplete=self.autocomplete_hop,\n      value=self.hop)\n\n    func_dict[\'filter_equal\'] = dict(\n      name=\'filter_equal\',\n      args=[{\'types\': [\'entity_list\']},\n            {\'types\': [\'entity_list\']},\n            {\'types\': [\'property\']}],\n      return_type=\'entity_list\',\n      autocomplete=self.autocomplete_filter_equal,\n      value=self.filter_equal)\n\n    func_dict[\'argmax\'] = dict(\n      name=\'argmax\',\n      args=[{\'types\': [\'entity_list\']},\n            {\'types\': [\'ordered_property\']}],\n      return_type=\'entity_list\',\n      autocomplete=self.autocomplete_argm,\n      value=self.argmax)\n\n    func_dict[\'argmin\'] = dict(\n      name=\'argmin\',\n      args=[{\'types\': [\'entity_list\']},\n            {\'types\': [\'ordered_property\']}],\n      return_type=\'entity_list\',\n      autocomplete=self.autocomplete_argm,\n      value=self.argmin)\n\n    constant_dict = collections.OrderedDict()\n\n    for p in self.props:\n      if p in self.num_props:\n        tp = \'num_property\'\n      elif p in self.datetime_props:\n        tp = \'datetime_property\'\n      else:\n        tp = \'string_property\'\n\n      constant_dict[p] = dict(\n        value=p, type=tp, name=p)\n\n    type_hierarchy = get_simple_type_hierarchy()\n    return dict(type_hierarchy=type_hierarchy,\n                func_dict=func_dict,\n                constant_dict=constant_dict)\n\n\nclass TableExecutor(SimpleKGExecutor):\n  """"""The executor for writing programs that processes simple Tables.""""""\n  def __init__(self, table_info):\n    super(TableExecutor, self).__init__(table_info)\n    self.n_rows = len(table_info[\'row_ents\'])    \n\n  def filter_ge(self, ents_1, nums, prop):\n    """"""Filter out entities whose prop >= nums.""""""\n    result = []\n    for ent in ents_1:\n      vals = set(self.hop([ent], prop))\n      for val in vals:\n        if all([(val >= x) for x in nums]):\n          result.append(ent)\n          break\n    return result\n\n  def filter_greater(self, ents_1, nums, prop):\n    """"""Filter out entities whose prop > nums.""""""\n    result = []\n    for ent in ents_1:\n      vals = set(self.hop([ent], prop))\n      for val in vals:\n        if all([(val > x) for x in nums]):\n          result.append(ent)\n          break\n    return result\n    \n  def filter_le(self, ents_1, nums, prop):\n    """"""Filter out entities whose prop <= nums.""""""\n    result = []\n    for ent in ents_1:\n      vals = set(self.hop([ent], prop))\n      for val in vals:\n        if all([(val <= x) for x in nums]):\n          result.append(ent)\n          break\n    return result\n\n  def filter_less(self, ents_1, nums, prop):\n    """"""Filter out entities whose prop < nums.""""""\n    result = []\n    for ent in ents_1:\n      vals = set(self.hop([ent], prop))\n      for val in vals:\n        if all([(val < x) for x in nums]):\n          result.append(ent)\n          break\n    return result\n\n  def filter_str_contain_any(self, ents, string_list, prop):\n    """"""Filter out entities whose prop contains any of the strings.""""""\n    result = []\n    for ent in ents:\n      str_val_list = self.hop([ent], prop)\n      assert len(str_val_list) == 1\n      str_val = str_val_list[0]\n      for string in string_list:\n        if string in str_val:\n          result.append(ent)\n          break\n    return result    \n\n  def filter_str_contain_not_any(self, ents, string_list, prop):\n    """"""Filter out entities, whose prop doesn\'t contain any of the strings.""""""\n    result = []\n    for ent in ents:\n      str_val_list = self.hop([ent], prop)\n      # Make sure that entity only has one value for the prop.\n      assert len(str_val_list) == 1\n      str_val = str_val_list[0]\n      # If any one of the string is contained by the cell,\n      # then pass. Only add to the result when none of the\n      # string is in the cell.\n      for string in string_list:\n        if string in str_val:\n          break\n      else:\n        result.append(ent)\n    return result    \n\n  def autocomplete_filter_str_contain_any(\n      self, exp, tokens, token_vals, debug=False):\n    """"""Auto-complete for filter_str_contain_any function.""""""\n    l = len(exp)\n    if l == 1:\n      valid_tks = [tk for tk, val in zip(tokens, token_vals)\n                   if len(val) > 1]\n    elif l == 2:\n      # Since all the strings are in the table, no need to\n      # filter any more. Keep the ones that have at least\n      # one string in it.\n      valid_tks = [tk for tk, val in zip(tokens, token_vals)\n                   if len(val) > 0]\n    elif l == 3:\n      valid_tks = []\n      token_val_dict = dict(zip(tokens, token_vals))\n      source_ents = exp[1]\n      string_list = exp[2]\n      for tk in tokens:\n        is_valid = False\n        prop = token_val_dict[tk]\n        str_val_list = self.hop(source_ents, prop)\n        # If one of the str_val contains any one of the\n        # string, then we can use this property.\n        for str_val in str_val_list:\n          for string in string_list:\n            if string in str_val:\n              is_valid = True\n              break\n          if is_valid:\n            valid_tks.append(tk)\n            break          \n    else:\n      raise ValueError(\'Expression is too long: {}\'.format(l))\n\n    if debug:\n      print\n      print \'+\' * 30\n      print \'in filter equal\'\n      print exp\n      print tokens\n      print valid_tks\n      print \'+\' * 30\n\n    return valid_tks\n\n  # Next and previous\n  def next(self, rows):\n    """"""Select all the rows that is right below the given rows respectively.""""""\n    assert rows\n    assert rows[0][:4] == \'row_\'\n    # row are in the pattern of row_0, row_1.\n    row_ids = [int(row_str[4:]) for row_str in rows]\n    new_row_ids = [(i+1) for i in row_ids if i+1 < self.n_rows]\n    if new_row_ids:\n      result_rows = [\'row_{}\'.format(i) for i in new_row_ids]\n      # result_rows = [\'row_{}\'.format(max(new_row_ids))]\n    else:\n      result_rows = []\n    return result_rows\n    \n  def previous(self, rows):\n    """"""Select all the rows that is right above the given rows respectively.""""""\n    assert rows\n    assert rows[0][:4] == \'row_\'\n    row_ids = [int(row_str[4:]) for row_str in rows]\n    new_row_ids = [(i-1) for i in row_ids if i-1 >= 0]\n    if new_row_ids:\n      result_rows = [\'row_{}\'.format(i) for i in new_row_ids]\n      # result_rows = [\'row_{}\'.format(min(new_row_ids))]\n    else:\n      result_rows = []\n    return result_rows\n\n  def autocomplete_next(self, exp, tokens, token_vals):\n    """"""Autocompletion for next function.""""""\n    l = len(exp)\n    if l == 1:\n      # If there are any non-empty result, then it is available.\n      valid_tks = []\n      for tk, val in zip(tokens, token_vals):\n        if len(val) > 0 and tk != \'all_rows\' and self.next(val):\n          valid_tks.append(tk)\n    else:\n      raise ValueError(\'Wrong length: {}.\'.format(l))\n    return valid_tks      \n\n  def autocomplete_previous(self, exp, tokens, token_vals):\n    """"""Autocompletion for previous function.""""""\n    l = len(exp)\n    if l == 1:\n      # If there are any non-empty result, then it is available.\n      valid_tks = []\n      for tk, val in zip(tokens, token_vals):\n        if len(val) > 0 and tk != \'all_rows\' and self.previous(val):\n          valid_tks.append(tk)\n    else:\n      raise ValueError(\'Wrong length: {}.\'.format(l))\n    return valid_tks      \n    \n  # First and last\n  def first(self, rows):\n    """"""Take the first row (the one with minimum index) in all the rows.""""""\n    assert len(rows) > 1\n    assert rows[0][:4] == \'row_\'\n    # Return the row with the smallest id.\n    row_ids = [int(row_str[4:]) for row_str in rows]\n    result_row_id = min(row_ids)\n    result_rows = [\'row_{}\'.format(result_row_id)]\n    return result_rows    \n\n  def last(self, rows):\n    """"""Take the last row (the one with maximum index) in all the rows.""""""\n    assert len(rows) > 1\n    assert rows[0][:4] == \'row_\'\n    # Return the row with the largest id.\n    row_ids = [int(row_str[4:]) for row_str in rows]\n    result_row_id = max(row_ids)\n    result_rows = [\'row_{}\'.format(result_row_id)]\n    return result_rows\n\n  def autocomplete_first_last(self, exp, tokens, token_vals):\n    """"""Autocompletion for both first and last.""""""\n    l = len(exp)\n    if l == 1:\n      # Only use first or last when you have more than one\n      # entity.\n      valid_tks = [tk for tk, val in zip(tokens, token_vals) if len(val) > 1]\n    else:\n      raise ValueError(\'Wrong length: {}.\'.format(l))\n    return valid_tks      \n\n  # Aggregation functions.\n  def count(self, ents):\n    return [len(ents)]\n\n  def maximum(self, ents, prop):\n    vals = self.hop(ents, prop)\n    return [max(vals)]\n\n  def minimum(self, ents, prop):\n    vals = self.hop(ents, prop)\n    try:\n      result = [min(vals)]\n    except Exception as e:\n      print ents, prop\n      raise e\n    return result\n\n  def mode(self, ents, prop):\n    """"""Return the value that appears the most in the prop of the entities.""""""\n    vals = self.hop(ents, prop, keep_dup=True)\n    count_dict = {}\n    for v in vals:\n      if v in count_dict:\n        count_dict[v] += 1\n      else:\n        count_dict[v] = 1\n    max_count = 0\n    max_val_list = []\n    for val, count in count_dict.iteritems():\n      if count > max_count:\n        max_count = count\n        max_val_list = [val]\n      elif count == max_count:\n        max_val_list.append(val)\n    return max_val_list\n\n  def sum(self, ents, prop):\n    vals = self.hop(ents, prop, keep_dup=True)\n    return [sum(vals)]\n\n  def average(self, ents, prop):\n    vals = self.hop(ents, prop, keep_dup=True)\n    return [float(sum(vals)) / len(vals)]\n    \n  def autocomplete_aggregation(self, exp, tokens, token_vals):\n    """"""Autocomplete for aggregation functions.""""""\n    l = len(exp)\n    if l == 1:\n      # Only use aggregation when you have more than one\n      # entity, otherwise just use hop.\n      valid_tks = [tk for tk, val in zip(tokens, token_vals) if len(val) > 1]\n    else:\n      # For the second argument, all the props with the\n      # right type (ordered_property) can be used.\n      props = set(self.get_props(exp[1]))\n      valid_tks = []\n      for tk, val in zip(tokens, token_vals):\n        if val in props:\n          valid_tks.append(tk)\n    return valid_tks\n\n  def same(self, ents, prop, namespace):\n    """"""Find the entities that has the prop as the given entity.""""""\n    # Can only work with one entity.\n    assert len(ents) == 1\n    vals_1 = self.hop(ents, prop)\n    all_rows = namespace[\'all_rows\'][\'value\']\n    same_ents = self.filter_equal(all_rows, vals_1, prop)\n    # Remove itself. \n    same_ents.remove(ents[0])\n    return same_ents\n\n  def autocomplete_same(self, exp, tokens, token_vals, namespace):\n    """"""Autocomplete for same function.""""""\n    l = len(exp)\n    if l == 1:\n      valid_tks = [\n        tk for tk, val in zip(tokens, token_vals)\n        if len(val) == 1]\n    elif l == 2:\n      props = set(self.get_props(exp[1]))\n      valid_tks = []\n      for tk, val in zip(tokens, token_vals):\n        if val in props:\n          valid_tks.append(tk)\n    else:\n      raise ValueError(\'Wrong length {}\'.format(l))\n    return valid_tks\n\n  def diff(self, ents_1, ents_2, prop):\n    """"""Return the difference of two entities in prop.""""""\n    assert len(ents_1) == 1\n    assert len(ents_2) == 1\n    val_1 = self.hop(ents_1, prop)[0]\n    val_2 = self.hop(ents_2, prop)[0]\n    return [abs(val_1 - val_2)]\n\n  def autocomplete_diff(self, exp, tokens, token_vals):\n    """"""Autocomplete for diff function.""""""\n    l = len(exp)\n    if l == 1:\n      valid_tks = [\n        tk for tk, val in zip(tokens, token_vals)\n        if len(val) == 1]\n      # There must be at least two valid variables to apply\n      # diff.\n      if len(valid_tks) < 2:\n        valid_tks = []\n    elif l == 2:\n      valid_tks = [\n        tk for tk, val in zip(tokens, token_vals)\n        if (len(val) == 1 and val != exp[1])]\n    else:\n      props = set(self.get_props(exp[1]))\n      props = props.intersection(self.get_props(exp[2]))\n      valid_tks = []\n      for tk, val in zip(tokens, token_vals):\n        if val in props:\n          valid_tks.append(tk)\n    return valid_tks\n\n  def return_all_tokens(self, unused_exp, tokens, unused_token_vals):\n    return tokens\n\n  def get_api(self):\n    """"""Get the functions, constants and type hierarchy.""""""\n    func_dict = collections.OrderedDict()\n\n    def hop_return_type_fn(arg1_type, arg2_type):\n      if arg2_type == \'num_property\':\n        return \'num_list\'\n      elif arg2_type == \'string_property\':\n        return \'string_list\'\n      elif arg2_type == \'datetime_property\':\n        return \'datetime_list\'\n      elif arg2_type == \'entity_property\':\n        return \'entity_list\'\n      else:\n        raise ValueError(\'Unknown type {}\'.format(arg2_type))\n    \n    func_dict[\'hop\'] = dict(\n      name=\'hop\',\n      args=[{\'types\': [\'entity_list\']},\n            {\'types\': [\'property\']}],\n      return_type=hop_return_type_fn,\n      autocomplete=self.autocomplete_hop,\n      type=\'primitive_function\',\n      value=self.hop)\n\n    # Only use filter equal for number and date and\n    # entities. Use filter_str_contain for string values.\n    func_dict[\'filter_eq\'] = dict(\n      name=\'filter_eq\',\n      args=[{\'types\': [\'entity_list\']},\n            {\'types\': [\'ordered_list\']},\n            {\'types\': [\'ordered_property\']}],\n      return_type=\'entity_list\',\n      autocomplete=self.autocomplete_filter_equal,\n      type=\'primitive_function\',\n      value=self.filter_equal)\n\n    func_dict[\'filter_not_eq\'] = dict(\n      name=\'filter_not_eq\',\n      args=[{\'types\': [\'entity_list\']},\n            {\'types\': [\'ordered_list\']},\n            {\'types\': [\'ordered_property\']}],\n      return_type=\'entity_list\',\n      autocomplete=self.autocomplete_filter_equal,\n      type=\'primitive_function\',\n      value=self.filter_not_equal)\n    \n    func_dict[\'argmax\'] = dict(\n      name=\'argmax\',\n      args=[{\'types\': [\'entity_list\']},\n            {\'types\': [\'ordered_property\']}],\n      return_type=\'entity_list\',\n      autocomplete=self.autocomplete_argm,\n      type=\'primitive_function\',\n      value=self.argmax)\n\n    func_dict[\'argmin\'] = dict(\n      name=\'argmin\',\n      args=[{\'types\': [\'entity_list\']},\n            {\'types\': [\'ordered_property\']}],\n      return_type=\'entity_list\',\n      autocomplete=self.autocomplete_argm,\n      type=\'primitive_function\',\n      value=self.argmin)    \n\n    func_dict[\'same\'] = dict(\n      name=\'same\',\n      args=[{\'types\': [\'entity_list\']},\n            {\'types\': [\'property\']}],\n      return_type=\'entity_list\',\n      autocomplete=self.autocomplete_same,\n      type=\'global_primitive_function\',\n      value=self.same)\n    \n    func_dict[\'first\'] = dict(\n      name=\'first\',\n      args=[{\'types\': [\'entity_list\']}],\n      return_type=\'entity_list\',\n      autocomplete=self.autocomplete_first_last,\n      type=\'primitive_function\',\n      value=self.first)\n\n    func_dict[\'last\'] = dict(\n      name=\'last\',\n      args=[{\'types\': [\'entity_list\']}],\n      return_type=\'entity_list\',\n      autocomplete=self.autocomplete_first_last,\n      type=\'primitive_function\',\n      value=self.last)\n\n    func_dict[\'next\'] = dict(\n      name=\'next\',\n      args=[{\'types\': [\'entity_list\']}],\n      return_type=\'entity_list\',\n      autocomplete=self.autocomplete_next,\n      type=\'primitive_function\',\n      value=self.next)\n\n    func_dict[\'previous\'] = dict(\n      name=\'previous\',\n      args=[{\'types\': [\'entity_list\']}],\n      return_type=\'entity_list\',\n      autocomplete=self.autocomplete_previous,\n      type=\'primitive_function\',\n      value=self.previous)\n    \n    func_dict[\'count\'] = dict(\n      name=\'count\',\n      args=[{\'types\': [\'entity_list\']}],\n      return_type=\'num\',\n      autocomplete=self.return_all_tokens,\n      type=\'primitive_function\',\n      value=self.count)\n\n    func_dict[\'filter_str_contain_any\'] = dict(\n      name=\'filter_str_contain_any\',\n      args=[{\'types\': [\'entity_list\']},\n            {\'types\': [\'string_list\']},\n            {\'types\': [\'string_property\']}],\n      return_type=\'entity_list\',\n      autocomplete=self.autocomplete_filter_str_contain_any,\n      type=\'primitive_function\',\n      value=self.filter_str_contain_any)\n\n    func_dict[\'filter_str_contain_not_any\'] = dict(\n      name=\'filter_str_contain_not_any\',\n      args=[{\'types\': [\'entity_list\']},\n            {\'types\': [\'string_list\']},\n            {\'types\': [\'string_property\']}],\n      return_type=\'entity_list\',\n      autocomplete=self.autocomplete_filter_str_contain_any,\n      type=\'primitive_function\',\n      value=self.filter_str_contain_not_any)\n    \n    func_dict[\'filter_ge\'] = dict(\n      name=\'filter_ge\',\n      args=[{\'types\': [\'entity_list\']},\n            {\'types\': [\'ordered_list\']},\n            {\'types\': [\'ordered_property\']}],\n      return_type=\'entity_list\',\n      autocomplete=self.return_all_tokens,\n      type=\'primitive_function\',\n      value=self.filter_ge)\n\n    func_dict[\'filter_greater\'] = dict(\n      name=\'filter_greater\',\n      args=[{\'types\': [\'entity_list\']},\n            {\'types\': [\'ordered_list\']},\n            {\'types\': [\'ordered_property\']}],\n      return_type=\'entity_list\',\n      autocomplete=self.return_all_tokens,\n      type=\'primitive_function\',\n      value=self.filter_greater)    \n    \n    func_dict[\'filter_le\'] = dict(\n      name=\'filter_le\',\n      args=[{\'types\': [\'entity_list\']},\n            {\'types\': [\'ordered_list\']},\n            {\'types\': [\'ordered_property\']}],\n      return_type=\'entity_list\',\n      autocomplete=self.return_all_tokens,\n      type=\'primitive_function\',\n      value=self.filter_le)\n\n    func_dict[\'filter_less\'] = dict(\n      name=\'filter_less\',\n      args=[{\'types\': [\'entity_list\']},\n            {\'types\': [\'ordered_list\']},\n            {\'types\': [\'ordered_property\']}],\n      return_type=\'entity_list\',\n      autocomplete=self.return_all_tokens,\n      type=\'primitive_function\',\n      value=self.filter_less)\n\n    # aggregation functions.\n    for k, f in zip([\'maximum\', \'minimum\'],\n                    [self.maximum, self.minimum]):\n      func_dict[k] = dict(\n        name=k,\n        args=[{\'types\': [\'entity_list\']},\n              {\'types\': [\'ordered_property\']}],\n        return_type=\'ordered_list\',\n        autocomplete=self.autocomplete_aggregation,\n        type=\'primitive_function\',\n        value=f)\n\n    func_dict[\'mode\'] = dict(\n      name=\'mode\',\n      args=[{\'types\': [\'entity_list\']},\n            {\'types\': [\'property\']}],\n      return_type=hop_return_type_fn,\n      autocomplete=self.autocomplete_aggregation,\n      type=\'primitive_function\',\n      value=self.mode)\n\n    func_dict[\'average\'] = dict(\n      name=\'average\',\n      args=[{\'types\': [\'entity_list\']},\n            {\'types\': [\'num_property\']}],\n      return_type=\'num\',\n      autocomplete=self.autocomplete_aggregation,\n      type=\'primitive_function\',\n      value=self.average)\n\n    func_dict[\'sum\'] = dict(\n      name=\'sum\',\n      args=[{\'types\': [\'entity_list\']},\n            {\'types\': [\'num_property\']}],\n      return_type=\'num\',\n      autocomplete=self.autocomplete_aggregation,\n      type=\'primitive_function\',\n      value=self.sum)\n    \n    func_dict[\'diff\'] = dict(\n      name=\'diff\',\n      args=[{\'types\': [\'entity_list\']},\n            {\'types\': [\'entity_list\']},\n            {\'types\': [\'num_property\']}],\n      return_type=\'num\',\n      autocomplete=self.autocomplete_diff,\n      type=\'primitive_function\',\n      value=self.diff)\n\n    constant_dict = collections.OrderedDict()\n\n    for p in self.props:\n      if p in self.num_props:\n        tp = \'num_property\'\n      elif p in self.datetime_props:\n        tp = \'datetime_property\'\n      elif p.split(\'-\')[-1] == \'entity\':\n        tp = \'entity_property\'\n      else:\n        tp = \'string_property\'\n\n      constant_dict[p] = dict(\n        value=p, type=tp, name=p)\n\n    type_hierarchy = get_simple_type_hierarchy()\n    return dict(type_hierarchy=type_hierarchy,\n                func_dict=func_dict,\n                constant_dict=constant_dict)\n\n    \ndef is_number(obj):\n  return isinstance(obj, int) or isinstance(obj, float)\n\n\nclass WikiTableExecutor(TableExecutor):\n  pass\n\n\nclass WikiSQLExecutor(TableExecutor):\n    \n  def __init__(self, table_info, use_filter_str_contain=True, use_filter_str_equal=False):\n    super(TableExecutor, self).__init__(table_info)\n    self.n_rows = len(table_info[\'row_ents\'])\n    self.use_filter_str_equal = use_filter_str_equal\n    self.use_filter_str_contain = use_filter_str_contain\n\n  def hop(self, entities, prop, keep_dup=True):\n    """"""Get the property of a list of entities.""""""\n    # Note this changes keep_dup=True as default, which is\n    # different from WikiTableQuestions experiments.\n    if keep_dup:\n      result = []\n    else:\n      result = set()\n    for ent in entities:\n      try:\n        if keep_dup:\n          result += self.kg[ent][prop]\n        else:\n          result = result.union(self.kg[ent][prop])\n      except KeyError:\n        continue\n    return list(result)\n    \n  def get_api(self):\n    """"""Get the functions, constants and type hierarchy.""""""\n    func_dict = collections.OrderedDict()\n\n    def hop_return_type_fn(arg1_type, arg2_type):\n      if arg2_type == \'num_property\':\n        return \'num_list\'\n      elif arg2_type == \'string_property\':\n        return \'string_list\'\n      elif arg2_type == \'datetime_property\':\n        return \'datetime_list\'\n      elif arg2_type == \'entity_property\':\n        return \'entity_list\'\n      else:\n        raise ValueError(\'Unknown type {}\'.format(arg2_type))\n    \n    func_dict[\'hop\'] = dict(\n      name=\'hop\',\n      args=[{\'types\': [\'entity_list\']},\n            {\'types\': [\'property\']}],\n      return_type=hop_return_type_fn,\n      autocomplete=self.autocomplete_hop,\n      type=\'primitive_function\',\n      value=self.hop)\n\n    if self.use_filter_str_equal:\n      # Allow equal to work on every type.\n      func_dict[\'filter_eq\'] = dict(\n        name=\'filter_eq\',\n        args=[{\'types\': [\'entity_list\']},\n              {\'types\': [\'entity_list\']},\n              {\'types\': [\'property\']}],\n        return_type=\'entity_list\',\n        autocomplete=self.autocomplete_filter_equal,\n        type=\'primitive_function\',\n        value=self.filter_equal)      \n    else:\n      # Only use filter equal for number and date and\n      # entities. Use filter_str_contain for string values.\n      func_dict[\'filter_eq\'] = dict(\n        name=\'filter_eq\',\n        args=[{\'types\': [\'entity_list\']},\n              {\'types\': [\'ordered_list\']},\n              {\'types\': [\'ordered_property\']}],\n        return_type=\'entity_list\',\n        autocomplete=self.autocomplete_filter_equal,\n        type=\'primitive_function\',\n        value=self.filter_equal)\n\n    if self.use_filter_str_contain:\n      func_dict[\'filter_str_contain_any\'] = dict(\n        name=\'filter_str_contain_any\',\n        args=[{\'types\': [\'entity_list\']},\n              {\'types\': [\'string_list\']},\n              {\'types\': [\'string_property\']}],\n        return_type=\'entity_list\',\n        autocomplete=self.autocomplete_filter_str_contain_any,\n        type=\'primitive_function\',\n        value=self.filter_str_contain_any)\n    \n    func_dict[\'filter_greater\'] = dict(\n      name=\'filter_greater\',\n      args=[{\'types\': [\'entity_list\']},\n            {\'types\': [\'ordered_list\']},\n            {\'types\': [\'ordered_property\']}],\n      return_type=\'entity_list\',\n      autocomplete=self.return_all_tokens,\n      type=\'primitive_function\',\n      value=self.filter_greater)    \n    \n    func_dict[\'filter_less\'] = dict(\n      name=\'filter_less\',\n      args=[{\'types\': [\'entity_list\']},\n            {\'types\': [\'ordered_list\']},\n            {\'types\': [\'ordered_property\']}],\n      return_type=\'entity_list\',\n      autocomplete=self.return_all_tokens,\n      type=\'primitive_function\',\n      value=self.filter_less)\n\n    func_dict[\'count\'] = dict(\n      name=\'count\',\n      args=[{\'types\': [\'entity_list\']}],\n      return_type=\'num\',\n      autocomplete=self.return_all_tokens,\n      type=\'primitive_function\',\n      value=self.count)\n    \n    # aggregation functions.\n    for k, f in zip([\'maximum\', \'minimum\'],\n                    [self.maximum, self.minimum]):\n      func_dict[k] = dict(\n        name=k,\n        args=[{\'types\': [\'entity_list\']},\n              {\'types\': [\'ordered_property\']}],\n        return_type=\'ordered_list\',\n        autocomplete=self.autocomplete_aggregation,\n        type=\'primitive_function\',\n        value=f)\n\n    func_dict[\'average\'] = dict(\n      name=\'average\',\n      args=[{\'types\': [\'entity_list\']},\n            {\'types\': [\'num_property\']}],\n      return_type=\'num\',\n      autocomplete=self.autocomplete_aggregation,\n      type=\'primitive_function\',\n      value=self.average)\n\n    func_dict[\'sum\'] = dict(\n      name=\'sum\',\n      args=[{\'types\': [\'entity_list\']},\n            {\'types\': [\'num_property\']}],\n      return_type=\'num\',\n      autocomplete=self.autocomplete_aggregation,\n      type=\'primitive_function\',\n      value=self.sum)\n\n    constant_dict = collections.OrderedDict()\n\n    for p in self.props:\n      if p in self.num_props:\n        tp = \'num_property\'\n      elif p in self.datetime_props:\n        tp = \'datetime_property\'\n      elif p.split(\'-\')[-1] == \'entity\':\n        tp = \'entity_property\'\n      else:\n        tp = \'string_property\'\n\n      constant_dict[p] = dict(\n        value=p, type=tp, name=p)\n\n    type_hierarchy = get_simple_type_hierarchy()\n    return dict(type_hierarchy=type_hierarchy,\n                func_dict=func_dict,\n                constant_dict=constant_dict)\n'"
nsm/graph_factory.py,251,"b'""Implements several tensorflow graphs and capsulate them as Graph.""\n\nimport abc\nimport six\nimport collections\nimport os\nimport pprint\n\nimport numpy as np\nimport tensorflow as tf\n\nimport tf_utils\nimport data_utils\n\nos.environ[\'TF_CPP_MIN_LOG_LEVEL\'] = \'2\'\n\nRNN_CELL_DICT = dict(\n  rnn=tf.contrib.rnn.RNNCell,\n  lstm=tf.contrib.rnn.BasicLSTMCell,\n  layernorm_lstm=tf.contrib.rnn.LayerNormBasicLSTMCell,\n  gru=tf.contrib.rnn.GRUCell)\n\nOPTIMIZER_DICT = dict(\n  sgd=tf.train.GradientDescentOptimizer,\n  adam=tf.train.AdamOptimizer,\n  adagrad=tf.train.AdagradOptimizer,\n  rmsprop=tf.train.RMSPropOptimizer)\n\nACTIVATION_DICT = dict(\n  relu=tf.nn.relu, sigmoid=tf.nn.sigmoid,\n  tanh=tf.nn.tanh)\n\n# Bind a variable length tensor with its sequence_length.\nSeqTensor = collections.namedtuple(\'SeqTensor\', [\'tensor\', \'sequence_length\'])\n\n\ndef with_graph_variable_scope(func):\n  def func_wrapper(*args, **kwargs):\n    self = args[0]\n    with self._graph.as_default():\n      pid = os.getpid()\n      container_name = \'worker{}\'.format(pid)\n      # print(container_name)\n      with self._graph.container(container_name):\n        with tf.variable_scope(self.vs):\n          return func(*args, **kwargs)\n  return func_wrapper\n\n  \nclass Graph(object):\n  """"""A TensorFlow graph with simpler interface to interact with it.\n\n  The neural network architecture (basically all the\n  tensorflow code) should live within this class. A new\n  architecture (for example, Seq2seq) should implement a new\n  subclass (Seq2seqGraph).\n  """"""\n  def __init__(self, name, meta_graph_fn=\'\'):\n    self.node_dict = {\'summaries\': []}\n    self._graph = tf.Graph()\n    self.vs_name = name\n    self.use_gpu = False\n    with tf.variable_scope(name) as vs:\n      self.vs = vs\n\n  @with_graph_variable_scope\n  def launch(self, init_model_path=\'\'):\n    ""Launch and initialize the graph.""\n    if self.use_gpu:\n      n_gpu = 1\n    else:\n      n_gpu = 0\n    session_config = tf.ConfigProto(\n      device_count={\'GPU\' : n_gpu},\n      allow_soft_placement=False,\n      log_device_placement=False,\n    )\n    if n_gpu:\n      session_config.gpu_options.allow_growth = True\n    tf.logging.info(\'number of gpu used {}\'.format(n_gpu))\n\n    self.session = tf.Session(\n      graph=self._graph, config=session_config)\n    self.saver = tf.train.Saver(tf.global_variables(), max_to_keep=3)\n    self.best_saver = tf.train.Saver(tf.global_variables(), max_to_keep=1)\n\n    if init_model_path:\n      self._graph.finalize()\n      self.saver.restore(self.session, init_model_path)\n    else:\n      init = tf.global_variables_initializer()\n      self._graph.finalize()\n      self.session.run(init)\n    return self.session\n\n  def restore(self, model_path):\n    self.saver.restore(self.session, model_path)\n\n  def save(self, model_path, global_step):\n    return self.saver.save(self.session, model_path, global_step)\n\n  def save_best(self, model_path, global_step):\n    return self.best_saver.save(self.session, model_path, global_step)\n    \n  def run(self, fetch_list, feed_dict, writer=None):\n    """"""Main interface to interact with the tensorflow graph.\n\n    Args:\n      fetch_list: a list of names (strings) indicating\n        the name of result operations.\n      feed_dict: a dictionary with the names of the nodes as keys\n        and the corresponding values that are fed as values.\n      writer: a tensorflow summary writer\n\n    Returns:\n      outputs: a dictionary with the names in the fetch_list as\n        keys, and the outputs from the executing graph as values.      \n    """"""\n    fetch_dict = dict([(name, self.node_dict[name])\n                       for name in fetch_list if name in self.node_dict])\n\n    if writer is not None:\n      fetch_dict[\'summaries\'] = self.node_dict[\'summaries\']\n      fetch_dict[\'global_step\'] = self.node_dict[\'global_step\']\n      \n    outputs = self.session.run(\n      fetch_dict, map_dict(self.node_dict, feed_dict))\n    \n    if writer is not None:\n      writer.add_summary(\n        outputs[\'summaries\'], outputs[\'global_step\'])\n      writer.flush()\n\n    return outputs\n\n  @with_graph_variable_scope\n  def add_train(self, aux_loss_list=None, optimizer=\'adam\',\n                learning_rate=0.01, max_grad_norm=5.0,\n                decay_after_n_steps=1000, decay_every_n_steps=1000,\n                lr_decay_factor=1.0, avg_loss_by_n=False, debug=False,\n                l2_coeff=0.0,\n                adam_beta1=0.9,\n                name=\'Training\'):\n    ""Construct part of the graph that controlls training (SGD optimization).""\n    self.node_dict[\'max_batch_size\'] = tf.placeholder(tf.int32, None)\n    with tf.variable_scope(name):\n      all_summaries = []\n      batch_size = tf.cast(self.node_dict[\'max_batch_size\'],\n                           dtype=tf.float32)\n      loss = self.node_dict[\'loss\'] / batch_size\n      \n      all_summaries.append(\n        tf.summary.scalar(self.vs_name + \'/\' + \'loss\', loss))\n      total_loss = loss\n\n      if aux_loss_list is not None:\n        for loss_name, w in aux_loss_list:\n          if w > 0.0:\n            aux_loss = self.node_dict[loss_name]\n            total_loss += aux_loss * w / batch_size\n            aux_loss_summary = tf.summary.scalar(\n              self.vs_name + \'/\' + loss_name, aux_loss)\n            all_summaries.append(aux_loss_summary)\n\n      if debug:\n        total_loss= tf.Print(\n          total_loss, [self.node_dict[\'sequence_loss\']], message=\'seq_loss:\', summarize=10000)\n\n        total_loss= tf.Print(\n          total_loss, [self.node_dict[\'weights\'].tensor], message=\'weights:\', summarize=10000)\n\n        total_loss= tf.Print(\n          total_loss, [self.node_dict[\'targets\'].tensor], message=\'targets:\', summarize=10000)\n\n        total_loss= tf.Print(\n          total_loss, [self.node_dict[\'probs\'].tensor], message=\'probs:\', summarize=10000)\n\n        total_loss= tf.Print(\n          total_loss, [self.node_dict[\'step_loss\'].tensor],\n          message=\'step_loss:\', summarize=10000)\n\n        total_loss= tf.Print(\n          total_loss, [self.node_dict[\'logits\'].tensor], message=\'logits:\', summarize=10000)\n      \n      total_loss_summary = tf.summary.scalar(\n        self.vs_name + \'/\' + \'total_loss\', total_loss)\n      all_summaries.append(total_loss_summary)\n      \n      lr = tf.Variable(\n        float(learning_rate), trainable=False)\n\n      new_lr = tf.placeholder(dtype=tf.float32, shape=None, name=\'new_lr\')\n      update_lr = lr.assign(new_lr)\n\n      global_step = tf.Variable(0, trainable=False)\n      decay_step = tf.maximum(\n        0, global_step - decay_after_n_steps)\n      decay_exponent = (tf.cast(decay_step, tf.float32) /\n                        tf.cast(decay_every_n_steps, tf.float32))\n      decay = lr_decay_factor ** decay_exponent\n      decayed_lr = lr * decay\n\n      lr_summary = tf.summary.scalar(self.vs_name + \'/\' + \'learning_rate\', decayed_lr)\n      all_summaries.append(lr_summary)\n      \n      params = tf.get_collection(\n        tf.GraphKeys.TRAINABLE_VARIABLES, scope=self.vs_name)\n      n_params = 0\n      tf.logging.info(\'trainable parameters:\')\n      for tv in params:\n        n_tv_params = np.product(tv.get_shape().as_list())\n        n_params += n_tv_params\n        tf.logging.info(\'{}: {}\'.format(tv.name, n_tv_params))\n        if \'weights\' in tv.name or \'kernel\' in tv.name:\n          total_loss += tf.reduce_sum(tf.nn.l2_loss(tv)) * l2_coeff\n      tf.logging.info(\'total number of trainable parameters {}\'.format(n_params))\n\n      if optimizer == \'adam\':\n        tf.logging.info(\'adam beta1: {}\'.format(adam_beta1))\n        opt = OPTIMIZER_DICT[optimizer](decayed_lr, beta1=adam_beta1)\n      else:\n        opt = OPTIMIZER_DICT[optimizer](decayed_lr)\n\n      gradients = tf.gradients(total_loss, params)\n      clipped_gradients, grad_norm = tf.clip_by_global_norm(\n        gradients, max_grad_norm)\n      update = opt.apply_gradients(\n        zip(clipped_gradients, params), global_step=global_step)\n            \n      grad_norm_summary = tf.summary.scalar(\n        self.vs_name + \'/\' + \'grad_norm\', grad_norm)\n      all_summaries.append(grad_norm_summary)\n\n      if debug:\n        _, clipped_grad_norm = tf.clip_by_global_norm(\n          clipped_gradients, max_grad_norm)\n        clipped_grad_norm_summary = tf.summary.scalar(\n          self.vs_name + \'/\' + \'clipped_grad_norm\', clipped_grad_norm)\n        n_summary = tf.summary.scalar(\n          self.vs_name + \'/\' + \'n\', self.node_dict[\'n\'])\n        seq_loss_summary = tf.summary.histogram(\n          self.vs_name + \'/\' + \'seq_loss\', self.node_dict[\'sequence_loss\'])\n        step_loss_summary = tf.summary.histogram(\n          self.vs_name + \'/\' + \'step_loss\', self.node_dict[\'step_loss\'].tensor)\n        weights_summary = tf.summary.histogram(\n          self.vs_name + \'/\' + \'weights\', self.node_dict[\'weights\'].tensor)\n        all_summaries += [\n          clipped_grad_norm_summary, n_summary,\n          step_loss_summary, seq_loss_summary, weights_summary]\n\n      batch_size_summary = tf.summary.scalar(\n        self.vs_name + \'/\' + \'batch_size\', self.node_dict[\'batch_size\'])\n      all_summaries.append(batch_size_summary)\n\n      if \'ent_reg\' in self.node_dict:\n        if avg_loss_by_n:\n          ent_reg = (self.node_dict[\'ent_reg\'] /\n                     tf.cast(self.node_dict[\'n\'],\n                             dtype=tf.float32))\n        else:\n          ent_reg = self.node_dict[\'ent_reg\'] / batch_size\n        ent_reg_summary = tf.summary.scalar(\n          self.vs_name + \'/\' + \'polic_entropy\',\n          (self.node_dict[\'ent_reg\'] /\n           tf.cast(self.node_dict[\'n\'], tf.float32)))\n        ent_reg_ppl_summary = tf.summary.scalar(\n          self.vs_name + \'/\' + \'policy_entropy_ppl\',\n          tf.exp(self.node_dict[\'ent_reg\'] /\n                 tf.cast(self.node_dict[\'n\'], tf.float32)))\n        all_summaries.append(ent_reg_summary)\n        all_summaries.append(ent_reg_ppl_summary)\n\n    for s in self.node_dict[\'summaries\']:\n      all_summaries.append(s)\n\n    merged = tf.summary.merge(inputs=all_summaries)\n    self.node_dict.update(\n      train=update, global_step=global_step,\n      summaries=merged,\n      update_lr=update_lr, new_lr=new_lr)\n\n  @property\n  def final_state(self):\n    return \'final_state\'\n\n  @property\n  def outputs(self):\n    return \'outputs\'\n\n  @property\n  def initial_state(self):\n    return \'initial_state\'\n\n  @property\n  def en_outputs(self):\n    return \'en_outputs\'\n\n  @property\n  def n_examples(self):\n    return \'n_examples\'\n\n  @property\n  def prediction_probs(self):\n    return \'probs\'\n\n  @property\n  def samples(self):\n    return \'samples\'\n\n  @property\n  def predictions(self):\n    return \'predictions\'\n\n  @property\n  def en_initial_state(self):\n    return \'en_initial_state\'\n\n  def add_outputs(self, output_type, output_config):\n    ""Create part of the graph that compute final outputs from the RNN output.""\n    if output_type == \'softmax\':\n      self.add_softmax_outputs(**output_config)\n    elif output_type == \'regression\':\n      self.add_regression_outputs(**output_config)\n    else:\n      raise NotImplementedError(\'Output type {} not supported!\'.format(\n        output_type))\n    \n  @with_graph_variable_scope\n  def add_softmax_outputs(self, output_vocab_size=None, use_logits=None, name=\'Softmax\'):\n    ""Add softmax layer on top of RNN outputs.""\n    with tf.variable_scope(name):\n      seq_targets = create_seq_inputs(\n        shape=tf.TensorShape([None, None]), dtype=tf.int32)\n      seq_weights = create_seq_inputs(\n        shape=tf.TensorShape([None, None]), dtype=tf.float32)      \n\n      if use_logits:\n        # Feeding logits instead of outputs (thus no linear transformation needed).\n        logits, probs, predictions, samples, temperature = create_softmax_from_logits(\n          self.node_dict[\'outputs\'].tensor)\n        sequence_length = self.node_dict[\'outputs\'].sequence_length\n      else:\n        logits, probs, predictions, samples, temperature = create_softmax(\n          self.node_dict[\'outputs\'].tensor, output_vocab_size=output_vocab_size)\n        sequence_length = self.node_dict[\'outputs\'].sequence_length\n\n      # From openai baselines to avoid numerical issue. \n      a0 = logits - tf.reduce_max(logits, axis=-1, keepdims=True)\n      ea0 = tf.exp(a0)\n      z0 = tf.reduce_sum(ea0, axis=-1, keepdims=True)\n      p0 = ea0 / z0\n\n      clipped_entropy = p0 * (tf.log(z0) - a0)\n      \n      seq_entropy = SeqTensor(\n        tf.reduce_sum(clipped_entropy, axis=-1) *\n        tf.sequence_mask(\n          sequence_length, dtype=tf.float32),\n        sequence_length)\n\n      policy_entropy = tf.reduce_sum(\n        tf.reduce_sum(clipped_entropy, axis=-1) *\n        tf.sequence_mask(\n          sequence_length, dtype=tf.float32))\n\n      # Compute sequence cross entropy loss.\n      seq_logits, seq_probs, seq_predictions, seq_samples = [\n        SeqTensor(x, sequence_length)\n        for x in (logits, probs, predictions, samples)]\n      xent_loss, sequence_loss, step_loss, sequence_probs, step_logprobs = create_seq_xent_loss(\n        seq_logits.tensor,\n        seq_targets.tensor,\n        seq_weights.tensor,\n        sequence_length)\n\n    seq_step_loss = SeqTensor(step_loss, sequence_length)\n      \n    # Add new nodes to the node_dict.\n    self.node_dict.update(\n      targets=seq_targets, weights=seq_weights,\n      temperature=temperature,\n      sequence_loss=sequence_loss,\n      step_loss=seq_step_loss,\n      loss=xent_loss, \n      ent_reg=policy_entropy,\n      seq_entropy=seq_entropy,\n      probs=seq_probs,\n      sequence_probs=sequence_probs,\n      step_logprobs=step_logprobs,\n      samples=seq_samples,\n      predictions=seq_predictions, logits=seq_logits)\n\n\n  @with_graph_variable_scope\n  def add_regression_outputs(self, hidden_sizes=(), activation=\'relu\',\n                             stop_gradient=False, name=\'Regression\'):\n    ""Add regression layer (linear transformation and MSE loss).""\n    with tf.variable_scope(name):\n      seq_targets = create_seq_inputs(\n        shape=tf.TensorShape([None, None]), dtype=tf.float32)\n      seq_weights = create_seq_inputs(\n        shape=tf.TensorShape([None, None]), dtype=tf.float32)\n      dropout = tf.placeholder_with_default(\n        0.0, shape=None, name=\'regression_dropout\')\n\n      # Last dimension is the output, thus only one dimension.\n      output_tensor = self.node_dict[\'outputs\'].tensor\n      if stop_gradient:\n        output_tensor = tf.stop_gradient(output_tensor)\n      if hidden_sizes:\n          h = create_multilayer_fnn(\n              output_tensor, dropout, list(hidden_sizes),\n              activation=activation)\n      else:\n          h = output_tensor\n\n      predictions = tf.layers.dense(inputs=h, units=1, name=\'regression_final\')\n      \n      # Turn shape from (batch_size, timesteps, 1) to (batch_size, timesteps).\n      predictions = tf.squeeze(predictions, axis=-1)\n      sequence_length = self.node_dict[\'outputs\'].sequence_length\n\n      seq_predictions = SeqTensor(predictions, sequence_length)                         \n\n      mse_loss = create_seq_mse_loss(\n        seq_predictions.tensor, seq_targets.tensor, seq_weights.tensor, sequence_length)\n\n    self.node_dict.update(\n      targets=seq_targets, weights=seq_weights, regression_dropout=dropout,\n      loss=mse_loss, predictions=seq_predictions)\n\n\nclass SeqGraph(Graph):\n  ""TensorFlow graph for RNN sequence model.""  \n  def __init__(self, graph_config, name=\'seq_graph\'):\n    super(SeqGraph, self).__init__(name)\n    self.add_seq(**graph_config[\'core_config\'])\n    self.add_outputs(graph_config[\'output_type\'], graph_config[\'output_config\'])\n    self.add_train(**graph_config[\'train_config\'])\n      \n  @with_graph_variable_scope\n  def add_seq(self, input_shape, input_vocab_size=None,\n              hidden_size=128, n_layers=2,\n              cell_type=\'lstm\', bidirectional=False,\n              dropout=0.0, use_embeddings=True,\n              embedding_size=64, name=\'Sequence\'):\n    with tf.variable_scope(name):\n      batch_size = tf.placeholder(\n        dtype=tf.int32, shape=(), name=\'batch_size\')\n      if use_embeddings:\n        embeddings = tf.get_variable(\n          \'embeddings\', shape=(input_vocab_size, embedding_size),\n          initializer=tf.truncated_normal_initializer(mean=0.0, stddev=0.1))\n      else:\n        embeddings = None        \n      (seq_inputs, initial_state, seq_outputs, final_state,\n       input_dropout, rnn_dropout, _) = create_seq_graph(\n         input_shape, batch_size=batch_size,\n         hidden_size=hidden_size, n_layers=n_layers,\n         cell_type=cell_type, bidirectional=bidirectional,\n         embeddings=embeddings)\n\n      n = tf.reduce_sum(seq_inputs.sequence_length)\n\n    self.node_dict.update(inputs=seq_inputs, \n                          rnn_dropout=rnn_dropout,\n                          input_dropout=input_dropout,\n                          embeddings=embeddings,\n                          batch_size=batch_size,\n                          final_state=final_state,\n                          outputs=seq_outputs, n=n,\n                          initial_state=initial_state)\n\nclass Seq2seqGraph(Graph):\n  """"""TensorFlow graph for seq2seq model.\n\n  A basic seq2seq model with attention. The model supports\n  all the common specifications for a seq2seq model such as\n  number of layers, whether to use bidirectional encoder,\n  attention type, etc.\n\n  """"""\n  def __init__(self, graph_config, name=\'seq2seq_graph\'):\n    super(Seq2seqGraph, self).__init__(name)\n    self.add_seq2seq(**graph_config[\'core_config\'])\n    self.add_outputs(graph_config[\'output_type\'], graph_config[\'output_config\'])\n    self.add_train(**graph_config[\'train_config\'])\n  \n  @with_graph_variable_scope\n  def add_seq2seq(self, en_input_shape, input_shape,\n                  use_attn=True,\n                  attn_size=128, attn_vec_size=128,\n                  en_input_vocab_size=None,\n                  input_vocab_size=None,\n                  en_hidden_size=128, en_n_layers=2,\n                  hidden_size=128, n_layers=2,\n                  cell_type=\'lstm\',\n                  en_bidirectional=False,\n                  en_use_embeddings=True,\n                  use_embeddings=True,\n                  en_embedding_size=64,\n                  embedding_size=64, name=\'Seq2seq\'):\n    with tf.variable_scope(name) as scope:\n      batch_size = tf.placeholder(\n        dtype=tf.int32, shape=[], name=\'batch_size\')\n\n      # Create encoder.\n      with tf.variable_scope(\'Encoder\'):\n        if en_use_embeddings:\n          en_embeddings = tf.get_variable(\n            \'embeddings\', shape=(en_input_vocab_size, en_embedding_size),\n            initializer=tf.truncated_normal_initializer(mean=0.0, stddev=0.1))\n        else:\n          en_embeddings = None        \n        (en_seq_inputs, en_initial_state, en_seq_outputs, en_final_state,\n         en_input_dropout, en_rnn_dropout, _) = create_seq_graph(\n           en_input_shape, batch_size=batch_size,\n           hidden_size=en_hidden_size, n_layers=en_n_layers,\n           cell_type=cell_type, bidirectional=en_bidirectional,\n           embeddings=en_embeddings, output_proj_size=en_hidden_size)\n\n      if use_attn:\n        attn_inputs = en_seq_outputs.tensor\n      else:\n        attn_inputs = None\n\n      if en_bidirectional:\n        en_final_state = en_final_state[0]\n\n      # Create decoder.\n      with tf.variable_scope(\'Decoder\'):\n        if use_embeddings:\n          embeddings = tf.get_variable(\n            \'embeddings\', shape=(input_vocab_size, embedding_size),\n            initializer=tf.truncated_normal_initializer(mean=0.0, stddev=0.1))\n        else:\n          embeddings = None        \n        (seq_inputs, initial_state, seq_outputs, final_state,\n         input_dropout, rnn_dropout, _) = create_seq_graph(\n           input_shape, batch_size=batch_size,\n           hidden_size=hidden_size, n_layers=n_layers,\n           cell_type=cell_type, bidirectional=False,\n           embeddings=embeddings,\n           attn_size=attn_size,\n           attn_vec_size=attn_vec_size,\n           attn_inputs=attn_inputs,\n           initial_state=en_final_state)\n\n      # Count number of steps.\n      n = tf.reduce_sum(seq_inputs.sequence_length)\n\n    self.node_dict.update(\n      en_inputs=en_seq_inputs,\n      en_rnn_dropout=en_rnn_dropout,\n      en_input_dropout=en_input_dropout,\n      en_outputs=en_seq_outputs,\n      en_initial_state=en_initial_state,\n      en_final_state=en_final_state,\n      inputs=seq_inputs,\n      rnn_dropout=rnn_dropout,\n      input_dropout=input_dropout,\n      outputs=seq_outputs,\n      batch_size=batch_size,\n      final_state=final_state,\n      initial_state=initial_state, n=n,\n      encoded_context=en_seq_outputs, context=en_seq_inputs,\n      en_embeddings=en_embeddings,\n      embeddings=embeddings)\n\n    if use_attn:\n      self.node_dict[\'attn_inputs\'] = attn_inputs\n\n\nclass MemorySeq2seqGraph(Graph):\n  def __init__(self, graph_config, name=\'memory_seq2seq_graph\'):\n    super(MemorySeq2seqGraph, self).__init__(name)\n    self.use_gpu = graph_config[\'use_gpu\']\n    if self.use_gpu:\n      os.environ[""CUDA_VISIBLE_DEVICES""] = graph_config[\'gpu_id\']\n    else:\n      os.environ[""CUDA_VISIBLE_DEVICES""] = \'\'\n\n    self.add_memory_seq2seq(**graph_config[\'core_config\'])\n    self.add_outputs(graph_config[\'output_type\'], graph_config[\'output_config\'])\n    self.add_train(**graph_config[\'train_config\'])\n    self.config = graph_config\n    \n  @with_graph_variable_scope\n  def add_memory_seq2seq(\n      self, max_n_valid_indices=None,\n      n_mem=None,\n      n_builtin=None, \n      use_attn=True,\n      attn_size=128, attn_vec_size=128,\n      en_input_vocab_size=None,\n      input_vocab_size=None,\n      en_hidden_size=128, en_n_layers=2,\n      hidden_size=128, n_layers=2,\n      cell_type=\'lstm\',\n      en_bidirectional=False,\n      en_use_embeddings=True,\n      en_embedding_size=4,\n      value_embedding_size=128,\n      en_pretrained_vocab_size=None,\n      en_pretrained_embedding_size=-1,\n      tie_en_embeddings=True,\n      add_lm_loss=False,\n      n_en_input_features=1,\n      n_de_output_features=1,\n      en_attn_on_constants=False,\n      name=\'MemorySeq2seq\'):\n    """"""Create seq2seq with key variable memory.\n\n    Seq2seq with key variable memory is used for semantic\n    parsing (generating programs from natural language\n    instructions/questions).\n\n    A MemorySeq2seq Model uses a memory cell in decoder.\n    \n    There are 3 types of tokens in a program:\n\n    1) constants that are provided at the before the program\n    is generated (added before decoding, different for\n    different examples); 2) variables that saves the results\n    from executing past expressions (added during decoding,\n    different for different examples); 3) language\n    primitives such as built-in functions and reserved\n    tokens (for example, ""("", "")""). (the same for different\n    examples).\n\n    There are two kinds of constants: 1) constants from the\n    question, whose representation is from the span the\n    annotated constants; 2) constants from the context,\n    whose representation is from the constant value\n    embeddings, for example, table columns. \n    \n    So the decoder vocab is organized as\n    [primitives, constants, variables].\n    \n    For a constant, its embedding is computed as sum of two\n    parts: 1) embedding of the span (from encoder) on which\n    the constant is annotated with, for example the span\n    ""barack obama"" in ""who is barack obama\'s wife"" or the\n    span ""one"" in ""what is one plus one""; 2) embedding of\n    the constant, for example, the embedding of the entity\n    Obama or the embedding of the number one.\n\n    For a variable, its embedding is the decoder RNN output\n    at the step where the variable is created.\n\n    For a primitive, its embedding is initialized randomly\n    and tuned by SGD.\n\n    Inspired by the code asistance (such as autocompletion)\n    in modern IDE, we also apply semantic and syntax\n    constraint on the decoder vocabulary so that at each\n    step, only some of the tokens are valid. So the decoder\n    has a dynamic vocabulary that is changing through\n    different steps.\n\n    """"""\n    input_shape = tf_utils.MemoryInputTuple(\n      tf.TensorShape([None, None]),\n      tf.TensorShape([None, None]),\n      tf.TensorShape([None, None, max_n_valid_indices]))\n    \n    input_dtype=tf_utils.MemoryInputTuple(\n      tf.int32, tf.int32, tf.int32)\n\n    en_input_shape=tf.TensorShape([None, None])\n    \n    constant_span_shape=tf.TensorShape([None, n_mem, 2])\n    constant_value_embedding_shape=tf.TensorShape(\n      [None, n_mem, value_embedding_size])\n    builtin_de_embeddings_shape=tf.TensorShape([n_builtin, hidden_size])\n\n    with tf.variable_scope(\'ConstantInput\'):\n      # constant_span_embedding encodes the information\n      # from the span where the constant is referred to,\n      # for example the span ""obama"" in ""who is the wife\n      # of obama"".\n\n      # constant_value_embedding encodes the information\n      # from the value of the constant, for example, the\n      # embedding of the entity Obama.\n\n      # constant_span: (B, n_mem, 2)\n      constant_spans_placeholder = tf.placeholder(tf.int32, constant_span_shape)\n      constant_spans = constant_spans_placeholder\n      n_constants_placeholder = tf.placeholder(tf.int32, [None, 1])\n      n_constants = tf.squeeze(n_constants_placeholder, [-1])\n\n      # constant_spans: (B, n_mem, 1)\n      # 0.0 if the span is [-1, -1], else 1.0.\n      constant_span_masks = tf.cast(\n        tf.greater(\n          tf.reduce_sum(constant_spans, axis=2), 0), tf.float32)\n      constant_span_masks = tf.expand_dims(constant_span_masks, -1)\n\n      # constant_spans: (B, n_mem, 2, 1)\n      constant_spans = tf.maximum(constant_spans, 0)\n      constant_spans = tf.expand_dims(constant_spans, axis=-1)\n\n      if constant_value_embedding_shape is not None:\n        constant_value_embeddings_placeholder = tf.placeholder(\n          tf.float32, shape=constant_value_embedding_shape)\n        constant_value_embeddings = constant_value_embeddings_placeholder\n        constant_value_embeddings = tf.layers.dense(\n          constant_value_embeddings, hidden_size, use_bias=True)\n        constant_value_masks = tf.squeeze(1 - constant_span_masks, [-1])\n\n    if n_en_input_features > 0:\n      en_input_features_shape = tf.TensorShape([None, None, n_en_input_features])\n    else:\n      en_input_features_shape = None\n\n    with tf.variable_scope(name) as scope:\n      batch_size = tf.placeholder(\n        dtype=tf.int32, shape=[], name=\'batch_size\')\n      with tf.variable_scope(\'Encoder\'):\n        if en_use_embeddings:\n          if en_pretrained_embedding_size < 0:\n            en_embeddings = tf.get_variable(\n              \'embeddings\', shape=(en_input_vocab_size, en_embedding_size),\n              initializer=tf.truncated_normal_initializer(mean=0.0, stddev=0.1))\n          else:\n            en_embeddings = tf.get_variable(\n              \'embeddings\', shape=(\n                en_input_vocab_size - en_pretrained_vocab_size, en_embedding_size),\n              initializer=tf.truncated_normal_initializer(mean=0.0, stddev=0.1))\n            en_pretrained_embeddings = tf.get_variable(\n              \'pretrained_embeddings\', shape=(\n                en_pretrained_vocab_size, en_pretrained_embedding_size),\n              trainable=False,\n              initializer=tf.zeros_initializer())\n            en_pretrained_embeddings_placeholder = tf.placeholder(\n              tf.float32, [en_pretrained_vocab_size, en_pretrained_embedding_size])\n            en_pretrained_embeddings_init = en_pretrained_embeddings.assign(\n              en_pretrained_embeddings_placeholder)\n            en_pretrained_embeddings = tf.layers.dense(\n              inputs=en_pretrained_embeddings, units=en_embedding_size,\n              use_bias=True)\n            en_embeddings = tf.concat(\n              values=[en_embeddings, en_pretrained_embeddings], axis=0)\n        else:\n          en_embeddings = None\n\n        if en_attn_on_constants: \n          tf.logging.info(\'Using attention in encoder!!!\')\n          (en_seq_inputs, en_initial_state, en_seq_outputs, en_final_state,\n           en_input_dropout, en_rnn_dropout, en_rnn_outputs) = create_seq_graph(\n             en_input_shape, batch_size=batch_size,\n             hidden_size=en_hidden_size, n_layers=en_n_layers,\n             cell_type=cell_type, bidirectional=en_bidirectional,\n             embeddings=en_embeddings, output_proj_size=en_hidden_size,\n             input_features_shape=en_input_features_shape,\n             attn_inputs=constant_value_embeddings,\n             attn_masks=constant_value_masks,\n             attn_size=attn_size, attn_vec_size=attn_vec_size)\n        else:\n          (en_seq_inputs, en_initial_state, en_seq_outputs, en_final_state,\n           en_input_dropout, en_rnn_dropout, en_rnn_outputs) = create_seq_graph(\n             en_input_shape, batch_size=batch_size,\n             hidden_size=en_hidden_size, n_layers=en_n_layers,\n             cell_type=cell_type, bidirectional=en_bidirectional,\n             embeddings=en_embeddings, output_proj_size=en_hidden_size,\n             input_features_shape=en_input_features_shape)\n\n        if n_en_input_features > 0:\n          en_seq_input_features = SeqTensor(\n            en_seq_inputs.tensor[1], tf.placeholder(tf.int32, [None]))\n          en_seq_inputs = SeqTensor(\n            en_seq_inputs.tensor[0], en_seq_inputs.sequence_length)\n      \n      if add_lm_loss:\n        sequence_length = tf.maximum(en_seq_inputs.sequence_length - 1, 0)\n        en_n = tf.cast(tf.reduce_sum(sequence_length), tf.float32)\n        mask = tf.sequence_mask(sequence_length, dtype=tf.float32)\n        if en_bidirectional:\n          en_fw_outputs = en_rnn_outputs[0]\n          en_bw_outputs = en_rnn_outputs[1]\n\n          if tie_en_embeddings:\n            en_fw_logits = tf_utils.tensormul(\n              en_fw_outputs[:, :-1, :], tf.transpose(en_embeddings))\n            en_bw_logits = tf_utils.tensormul(\n              en_bw_outputs[:, 1:, :], tf.transpose(en_embeddings))\n          else:\n            # Use 0 to n-2 to compute logits.\n            en_fw_logits = tf.layers.dense(\n              en_fw_outputs[:, :-1, :], en_input_vocab_size, use_bias=True)\n            en_bw_logits = tf.layers.dense(\n              en_bw_outputs[:, 1:, :], en_input_vocab_size, use_bias=True)\n          \n          # Use 1 to n-1 as targets.\n          en_fw_lm_loss = tf.nn.sparse_softmax_cross_entropy_with_logits(\n            labels=en_seq_inputs.tensor[:, 1:], logits=en_fw_logits) * mask\n          en_bw_lm_loss = tf.nn.sparse_softmax_cross_entropy_with_logits(\n            labels=en_seq_inputs.tensor[:, :-1], logits=en_bw_logits) * mask\n          \n          en_lm_loss = tf.reduce_sum(en_fw_lm_loss + en_bw_lm_loss) / en_n\n        else:\n          en_fw_outputs = en_rnn_outputs\n          if tie_en_embeddings:\n            en_fw_logits = tf_utils.tensormul(\n              en_fw_outputs[:, :-1, :], tf.transpose(en_embeddings))\n          else:\n            en_fw_logits = tf.layers.dense(\n              en_fw_outputs[:, :-1, :], en_input_vocab_size, use_bias=True)\n          en_fw_lm_loss = tf.nn.sparse_softmax_cross_entropy_with_logits(\n            labels=en_seq_inputs.tensor[:, 1:], logits=en_fw_logits) * mask\n          en_lm_step_loss = en_fw_lm_loss\n          en_lm_loss = tf.reduce_sum(en_lm_step_loss) / en_n\n        \n      if use_attn:\n        attn_inputs = en_seq_outputs.tensor\n        attn_masks = tf.sequence_mask(\n          en_seq_outputs.sequence_length, dtype=tf.float32)\n      else:\n        attn_inputs = None\n        attn_masks = None\n\n      with tf.variable_scope(\'ConstantEncoder\'):\n        batch_ind = tf.range(batch_size)\n\n        # batch_ind: (B, 1, 1, 1)\n        for i in range(3):\n          batch_ind = tf.expand_dims(batch_ind, axis=-1)\n          \n        # batch_ind: (B, n_mem, 2, 1)\n        batch_ind = tf.tile(batch_ind, [1, n_mem, 2, 1])\n\n        # constant_span: (B, n_mem, 2, 2)\n        constant_spans = tf.concat([batch_ind, constant_spans], axis=-1)\n        \n        # constant_span_embedding: (B, n_mem, 2, en_output_size)\n        constant_span_embeddings = tf.gather_nd(en_seq_outputs.tensor, constant_spans)\n\n        # constant_embedding: (B, n_mem, en_output_size)\n        constant_embeddings = tf.reduce_mean(constant_span_embeddings, axis=2)\n        constant_embeddings = constant_embeddings * constant_span_masks\n\n        if constant_value_embedding_shape is not None:\n          constant_embeddings = constant_embeddings + constant_value_embeddings\n          \n        # mask out the bad constants.\n        # constant mask: (B, n_mem)\n        constant_masks = tf.sequence_mask(\n          n_constants, maxlen=n_mem, dtype=tf.float32)\n        # constant mask: (B, n_mem, 1)\n        constant_masks = tf.expand_dims(constant_masks, -1)\n        constant_masks = tf.tile(constant_masks, [1, 1, hidden_size])\n        # constant_embeddings: (B, n_mem, hidden_size)\n        constant_embeddings = constant_embeddings * constant_masks\n\n        # builtin_de_embeddings: (n_builtin, embed_size)\n        builtin_de_embeddings = tf.get_variable(\n          \'builtin_de_embeddings\', builtin_de_embeddings_shape,\n          initializer=tf.truncated_normal_initializer(mean=0.0, stddev=0.1))\n        # builtin_de_embeddings: (1, n_builtin, embed_size)\n        builtin_de_embeddings = tf.expand_dims(builtin_de_embeddings, axis=0) \n        # builtin_de_embeddings: (B, n_builtin, embed_size)\n        builtin_de_embeddings = tf.tile(\n          builtin_de_embeddings, [batch_size] + [1] * 2)\n\n        # initial_memory: (B, n_builtin + n_mem, embed_size)\n        initial_memory = tf.concat(\n          [builtin_de_embeddings, constant_embeddings], axis=1)\n\n        # concatenate static and constant embeddings to form\n        # new memory to create initial states.\n        if en_bidirectional:\n          initial_state = en_final_state[0]\n        else:\n          initial_state = en_final_state\n\n      with tf.variable_scope(\'Decoder\'):\n        initial_state = tf_utils.MemoryStateTuple(\n          initial_memory, initial_state)  \n        \n        seq_inputs = create_seq_inputs(shape=input_shape, dtype=input_dtype)\n        inputs = seq_inputs.tensor\n        sequence_length = seq_inputs.sequence_length\n\n        rnn_dropout = tf.placeholder_with_default(\n          0.0, shape=None, name=\'rnn_dropout\')        \n        \n        # Create multilayer attention cell then wrap with memory cell.\n        cell = multilayer_dropout_cell(\n          cell_fn=RNN_CELL_DICT[cell_type], hidden_size=hidden_size,\n          n_layers=n_layers, dropout=rnn_dropout)\n        \n        if attn_inputs is not None:\n          cell = tf_utils.SeqAttentionCellWrapper(\n            cell, attn_inputs=attn_inputs,\n            attn_size=attn_size, attn_vec_size=attn_vec_size,\n            output_size=hidden_size, attn_masks=attn_masks)\n\n        mem_size = builtin_de_embeddings_shape[0] + constant_span_shape[1]\n        embed_size = hidden_size\n        cell = tf_utils.MemoryWrapper(\n          cell, mem_size, embed_size, max_n_valid_indices)\n\n        flat_inputs = data_utils.flatten(inputs)\n        flat_inputs = [\n          tf.expand_dims(in_, -1) for in_ in flat_inputs[:2]] + flat_inputs[2:]\n        inputs = data_utils.pack_sequence_as(inputs, flat_inputs)\n        outputs, final_state = tf.nn.dynamic_rnn(\n          cell, inputs, sequence_length=sequence_length,\n          initial_state=initial_state, dtype=tf.float32)\n\n        if n_de_output_features > 0:\n          de_seq_output_features = create_seq_inputs(\n            shape=tf.TensorShape(\n              [None, None, max_n_valid_indices, n_de_output_features]),\n            dtype=tf.int32, name=\'de_output_features\')\n          output_feature_weights = tf.get_variable(\n            \'de_output_feature_weights\',\n            shape=tf.TensorShape([n_de_output_features, 1]),\n            initializer=tf.zeros_initializer())\n          outputs = outputs + tf.squeeze(tf_utils.tensormul(\n            tf.cast(de_seq_output_features.tensor, tf.float32),\n            output_feature_weights), axis=-1)\n\n        seq_outputs = SeqTensor(outputs, sequence_length)\n            \n      n = tf.reduce_sum(seq_inputs.sequence_length)\n\n    self.node_dict.update(\n      en_inputs=en_seq_inputs,\n      en_rnn_dropout=en_rnn_dropout,\n      en_input_dropout=en_input_dropout,\n      en_outputs=en_seq_outputs,\n      en_initial_state=en_initial_state,\n      en_final_state=en_final_state,      \n      inputs=seq_inputs,\n      constant_spans=constant_spans_placeholder,\n      constant_embeddings=constant_embeddings,\n      constant_masks=constant_masks,\n      n_constants=n_constants_placeholder,\n      rnn_dropout=rnn_dropout,\n      # input_dropout=input_dropout,\n      outputs=seq_outputs,\n      batch_size=batch_size,\n      final_state=final_state,\n      initial_state=initial_state, n=n,\n      encoded_context=en_seq_outputs,\n      context=en_seq_inputs,\n      en_embeddings=en_embeddings)\n\n    if en_pretrained_embedding_size > 0:\n      self.node_dict[\'en_pretrained_embeddings\'] = en_pretrained_embeddings_placeholder\n      self.node_dict[\'en_pretrained_embeddings_init\'] = en_pretrained_embeddings_init\n  \n    if constant_value_embedding_shape is not None:\n      self.node_dict[\'constant_value_embeddings\'] = constant_value_embeddings_placeholder\n\n    if add_lm_loss:\n      self.node_dict[\'en_lm_loss\'] = en_lm_loss\n      # self.node_dict[\'en_lm_step_loss\'] = en_lm_step_loss\n\n    if use_attn:\n      self.node_dict[\'attn_inputs\'] = attn_inputs\n\n    if n_en_input_features > 0:\n      self.node_dict[\'en_input_features\'] = en_seq_input_features\n      self.node_dict[\'summaries\'].append(\n        tf.summary.scalar(\n          self.vs_name + \'/\' + \'en_input_features_sum\',\n          tf.reduce_sum(en_seq_input_features.tensor)))\n\n    if n_de_output_features > 0:\n      self.node_dict[\'output_features\'] = de_seq_output_features\n      self.node_dict[\'output_feature_weights\'] = output_feature_weights\n      self.node_dict[\'summaries\'].append(\n        tf.summary.scalar(\n          self.vs_name + \'/\' + \'output_feature_weights_0\',\n          output_feature_weights[0][0]))\n      self.node_dict[\'summaries\'].append(\n        tf.summary.scalar(\n          self.vs_name + \'/\' + \'output_features_sum\',\n          tf.reduce_sum(de_seq_output_features.tensor)))\n\n\nclass MonitorGraph(object):\n  """"""A tensorflow graph to monitor some values during training.\n\n  Generate tensorflow summaries for the values to monitor\n  them through tensorboard.\n  """"""\n  def __init__(self):\n    self.node_dict = {}\n    self._graph = tf.Graph()\n\n  def launch(self):\n    with self._graph.as_default():\n      self.merged = tf.summary.merge_all()\n      init = tf.global_variables_initializer()\n    self.session = tf.Session(graph=self._graph)\n    self.session.run(init)\n\n  def add_scalar_monitor(self, name, dtype):\n    with self._graph.as_default():\n      x = tf.placeholder(\n        dtype=dtype, shape=None, name=name)\n      tf.summary.scalar(name, x)\n    self.node_dict[name] = x\n\n  def generate_summary(self, feed_dict):\n    summary_str = self.session.run(\n      self.merged, map_dict(self.node_dict, feed_dict))\n    return summary_str\n      \n\n# Utility functions for creating TensorFlow graphs.\n\n# FNN\ndef create_multilayer_fnn(inputs, dropout, hidden_sizes, activation=\'relu\'):\n  x = inputs\n  for size in hidden_sizes:\n    x = tf.nn.dropout(x, 1 - dropout)\n    x = tf.layers.dense(inputs=x, units=size, activation=ACTIVATION_DICT[activation])\n  return x\n\n\n# Loss \ndef create_seq_mse_loss(outputs, targets, weights, sequence_length):\n  mask = tf.sequence_mask(\n    sequence_length, dtype=tf.float32)\n  loss = tf.reduce_sum(tf.squared_difference(outputs, targets) * weights * mask)\n  return loss\n\n\ndef create_seq_xent_loss(logits, targets, weights, sequence_length):\n  mask = tf.sequence_mask(\n    sequence_length, maxlen=tf.reduce_max(sequence_length),\n    dtype=tf.float32)\n  step_neg_logprobs = tf.nn.sparse_softmax_cross_entropy_with_logits(\n    labels=targets, logits=logits) * mask\n  step_logprobs = -1 * step_neg_logprobs\n  sequence_probs = tf.exp(tf.reduce_sum(step_logprobs, axis=1))\n  step_loss =  step_neg_logprobs * weights\n  sequence_loss = tf.reduce_sum(step_loss, axis=1)\n  xent_loss = tf.reduce_sum(sequence_loss)\n  return xent_loss, sequence_loss, step_loss, sequence_probs, step_logprobs\n\n\ndef create_softmax(\n    inputs, softmax_w=None,\n    output_vocab_size=None, use_bias=False, name=\'Softmax_layer\'):\n  ""Create nodes for linear transformation of inputs and softmax computation.""\n  with tf.name_scope(name):\n    # inputs = tf.nn.dropout(inputs, 1-dropout)\n    if softmax_w is None:\n      logits = tf.layers.dense(\n        inputs=inputs, units=output_vocab_size,\n        use_bias=use_bias)\n    else:\n      logits = tf_utils.tensormul(inputs, softmax_w)\n      if use_bias:\n        softmax_b = tf.Variable(\n          initial_value=np.zeros(\n            (1, output_vocab_size), dtype=tf.float32),\n          name=\'softmax_bias\')\n        logits += softmax_b\n    return create_softmax_from_logits(logits)\n\n\ndef create_softmax_from_logits(logits):\n  ""Create nodes for softmax computation from logits.""\n  temperature = tf.placeholder_with_default(\n    1.0, shape=(), name=\'temperature\')\n  logits = logits / temperature\n\n  logits_shape = tf.shape(logits)\n  logits_dim = logits_shape[-1]\n  logits_2d = tf.reshape(logits, [-1, logits_dim])\n  samples = tf.multinomial(logits_2d, 1)\n  samples = tf.reshape(samples, logits_shape[:-1])\n\n  probs = tf.nn.softmax(logits)\n  predictions = tf.argmax(probs, axis=2)\n    \n  return logits, probs, predictions, samples, temperature\n\n\n# Embedding\ndef embed_inputs(\n    inputs, embeddings, name=\'Embedding_layer\'):\n  with tf.name_scope(name):\n    embedded_inputs = tf.nn.embedding_lookup(embeddings, inputs)\n    return embedded_inputs\n\n\n# RNN\ndef create_rnn(\n    cell, initial_state, inputs, sequence_length,\n    hidden_size, bidirectional, cell_bw=None,\n    name=\'RNN\'):\n  with tf.name_scope(name):\n    if bidirectional:\n      # Note that you can\'t use bidirectional RNN if you\n      # want to do decoding.\n      initial_state_fw = initial_state[0]\n      initial_state_bw = initial_state[1]\n      outputs, final_state = tf.nn.bidirectional_dynamic_rnn(\n        cell, cell_bw, inputs, sequence_length=sequence_length,\n        initial_state_fw=initial_state_fw,\n        initial_state_bw=initial_state_bw,\n        dtype=tf.float32)\n    else:\n      outputs, final_state = tf.nn.dynamic_rnn(\n        cell, inputs, sequence_length=sequence_length,\n        initial_state=initial_state, dtype=tf.float32)\n  return outputs, final_state\n\n    \n# RNN Cell\ndef multilayer_dropout_cell(\n    cell_fn, hidden_size, n_layers, dropout,\n    use_skip_connection=True):\n  """"""Create multilayer RNN cell with dropout.""""""\n  cells = []\n  for i in xrange(n_layers):\n    cell = cell_fn(hidden_size)\n    if i > 0 and use_skip_connection:\n      cell = tf.nn.rnn_cell.ResidualWrapper(cell)\n    cell = tf.contrib.rnn.DropoutWrapper(\n      cell, output_keep_prob=1.0-dropout)\n    # variational_recurrent=True,\n    # state_keep_prob = 1.0 - dropout,\n    # dtype=tf.float32)\n    cells.append(cell)\n\n  final_cell = tf.contrib.rnn.MultiRNNCell(cells)\n  return final_cell\n\n\n# Input placeholders.\ndef create_seq_inputs(shape, dtype=tf.float32, name=\'inputs\'):\n  with tf.name_scope(name):\n    if isinstance(shape, tuple):\n      flat_input_shape = data_utils.flatten(shape)\n      assert isinstance(dtype, tuple)\n      flat_dtype = data_utils.flatten(dtype)\n      flat_inputs = [tf.placeholder(\n        dt, sh, name=\'inputs\') for dt, sh in zip(flat_dtype, flat_input_shape)]\n      inputs = data_utils.pack_sequence_as(shape, flat_inputs)\n    else:\n      inputs = tf.placeholder(dtype, shape)\n    sequence_length = tf.placeholder(\n      tf.int32, [None], name=\'sequence_length\')\n  return SeqTensor(inputs, sequence_length)\n  \ndef create_tuple_placeholders_with_default(inputs, extra_dims, shape):\n  if isinstance(shape, int):\n    result = tf.placeholder_with_default(\n      inputs, list(extra_dims) + [shape])\n  else:\n    subplaceholders = [create_tuple_placeholders_with_default(\n      subinputs, extra_dims, subshape)\n                       for subinputs, subshape in zip(inputs, shape)]\n    t = type(shape)\n    if t == tuple:\n      result = t(subplaceholders)\n    else:\n      result = t(*subplaceholders)    \n  return result\n\n        \ndef create_tuple_placeholders(dtype, extra_dims, shape):\n  if isinstance(shape, int):\n    result = tf.placeholder(dtype, list(extra_dims) + [shape])\n  else:\n    subplaceholders = [create_tuple_placeholders(dtype, extra_dims, subshape)\n                       for subshape in shape]\n    t = type(shape)\n\n    # Handles both tuple and LSTMStateTuple.\n    if t == tuple:\n      result = t(subplaceholders)\n    else:\n      result = t(*subplaceholders)\n  return result\n\n\n# Sequence models.\ndef create_seq_graph(input_shape, batch_size=None, \n                     # input_vocab_size=None,\n                     attn_inputs=None,\n                     attn_size=128, attn_vec_size=128,\n                     # output_size=128,\n                     input_size=None, \n                     hidden_size=128, n_layers=2,\n                     cell_type=\'lstm\', bidirectional=False,\n                     initial_state=None,\n                     #use_embeddings=True,\n                     #embedding_size=64,\n                     embeddings=None,\n                     output_proj_size=None,\n                     input_features_shape=None,\n                     attn_masks=None):\n  # Create inputs.\n  seq_inputs = create_seq_inputs(\n    shape=input_shape, dtype=tf.int32 if embeddings is not None else tf.float32)\n\n  rnn_dropout = tf.placeholder_with_default(\n    0.0, shape=None, name=\'rnn_dropout\')\n\n  # Create embedding layer.\n  if embeddings is not None:\n    embedded_inputs = embed_inputs(\n      seq_inputs.tensor, embeddings=embeddings)\n  else:\n    embedded_inputs = seq_inputs.tensor\n    \n  input_dropout = tf.placeholder_with_default(\n    0.0, shape=None, name=\'input_dropout\')\n    \n  embedded_inputs = tf.nn.dropout(\n    embedded_inputs, 1 - input_dropout)\n\n  # If we include features in inputs, then add them here.\n  if input_features_shape is not None:\n    seq_input_features = create_seq_inputs(\n      shape=input_features_shape, dtype=tf.int32)\n    embedded_inputs = tf.concat(\n      [embedded_inputs, tf.cast(seq_input_features.tensor, tf.float32)],\n      axis=-1)\n    seq_inputs = SeqTensor(\n      (seq_inputs.tensor, seq_input_features.tensor), seq_inputs.sequence_length)\n  else:\n    seq_input_features = None\n  \n  embedded_seq_inputs = SeqTensor(embedded_inputs, seq_inputs.sequence_length)\n    \n  # Create RNN cell\n  cell = multilayer_dropout_cell(\n    RNN_CELL_DICT[cell_type],\n    hidden_size, n_layers, rnn_dropout)\n\n  if bidirectional:\n    cell_bw = multilayer_dropout_cell(\n      RNN_CELL_DICT[cell_type],\n      hidden_size, n_layers, rnn_dropout)\n  else:\n    cell_bw = None\n\n  # Add attention.\n  if attn_inputs is not None:\n    cell = tf_utils.SeqAttentionCellWrapper(\n      cell, attn_inputs=attn_inputs,\n      attn_size=attn_size, attn_vec_size=attn_vec_size,\n      output_size=hidden_size, attn_masks=attn_masks)\n    if bidirectional:\n      cell_bw = tf_utils.SeqAttentionCellWrapper(\n        cell_bw, attn_inputs=attn_inputs,\n        attn_size=attn_size, attn_vec_size=attn_vec_size,\n        output_size=hidden_size, attn_masks=attn_masks)\n    \n  if initial_state is None:\n    # Create zero state.\n    zero_state = cell.zero_state(batch_size, tf.float32)\n    \n    if bidirectional:\n      zero_state_bw = cell_bw.zero_state(batch_size, tf.float32)\n      zero_state = (zero_state, zero_state_bw)\n\n    initial_state = zero_state\n      \n  # Create RNN.\n  outputs, final_state = create_rnn(\n    cell, initial_state, embedded_seq_inputs.tensor,\n    embedded_seq_inputs.sequence_length,\n    hidden_size=hidden_size,\n    bidirectional=bidirectional, cell_bw=cell_bw)\n  rnn_outputs = outputs\n\n  if bidirectional:\n    outputs = tf.concat(outputs, axis=2)\n    hidden_size *= 2\n    \n  # Whether to add linear transformation to outputs. \n  if output_proj_size is not None:\n    outputs = tf.layers.dense(\n      inputs=outputs, units=output_proj_size, use_bias=True)\n\n  seq_outputs = SeqTensor(\n    outputs,\n    tf.placeholder_with_default(\n      seq_inputs.sequence_length, shape=[None]))\n  \n  return (seq_inputs, initial_state, seq_outputs,\n          final_state, input_dropout, rnn_dropout,\n          rnn_outputs)\n\n\n# General utility functions.\n\ndef map_dict(dict_1, main_dict):\n  new_dict = {}  \n  for k, v in main_dict.iteritems():\n    if k in dict_1:\n      new_dict[dict_1[k]] = main_dict[k]\n  return new_dict\n'"
nsm/model_factory.py,1,"b'""""""A scikit-lean like interface around tensorflow graphs.""""""\nimport abc\nimport time\nimport six\nimport pprint\n\nimport numpy as np\nimport tensorflow as tf\n\nimport data_utils\nimport tf_utils\n\n\n@six.add_metaclass(abc.ABCMeta)\nclass SeqModel(object):\n  """"""Abstract class for a sequence model.""""""\n\n  @abc.abstractmethod\n  def step(self, inputs, state, context, parameters):\n    raise NotImplementedError\n\n  @abc.abstractmethod\n  def train(self, inputs, targets, context, parameters):\n    raise NotImplementedError\n\n\nclass RNNSeqModel(SeqModel):\n  """"""A scikit-learn like interface to a RNN sequence model.\n\n  The model handles the batching and padding for sequence data.\n\n  B is the batch size, T is the time steps or sequence length.\n  ""..."" means scalar, arrays, or tuples.\n\n  Conceptually, the input should contain 4 parts:\n    1) inputs that is different at every timestep. shape: (B, T, ...)\n    2) initial states that is different for each example. shape: (B, ...)\n    3) context that is different at every example, but the same at different\n       timestep. shape: (B, ...), may be different for training\n       (input sequence for backprop to encoder) and inference\n       (encoded sequence).\n    4) parameters that is the same for each example. For example,\n       the dropout rate. Usually a scalar. shape: (...)\n\n  The output usually contains 2 parts:\n    1) outputs at each step, shape: (B, T, ...).\n    2) final states. shape: (B, ...)\n\n  In terms of implementation, we use list to represent\n  variable length inputs.\n\n  Assume:\n  Atom = np.array or float or integer or tuple\n\n  For normal inputs (handled by data_utils.BatchGenerator):\n  inputs = [[Atom1, Atom2, ...]]\n  size is (B, ...)\n\n  For sequence inputs (handled by data_utils.SeqBatchGenerator):\n  inputs = [[Atom_11, Atom_12, ...],\n            [Atom_21, Atom_22, ...], ...]\n  size is (B, T, ...)\n  """"""\n\n  def __init__(self, graph, batch_size=32):\n    """"""Creates a RNN sequence model for a given Graph instance.""""""\n    self.graph = graph\n    self.session = graph.session\n    self.saver = graph.saver\n    self.batch_size = batch_size\n    self._outputs = graph.outputs\n    self._final_state = graph.final_state\n    self._n_examples = graph.n_examples\n    self._predictions = graph.predictions\n    self._probs = graph.prediction_probs\n    self._samples = graph.samples\n\n    self._loss =\'loss\'\n    self._train =\'train\'\n    self._count =\'n\'\n    self._policy_ent =\'ent_reg\'\n\n    self._step_bc = data_utils.BatchConverter(\n      tuple_keys=[\'initial_state\'], seq_keys=[\'inputs\', \'encoded_context\'])\n    self._step_ba = data_utils.BatchAggregator(\n      tuple_keys=[self._final_state], seq_keys=[self._outputs])\n\n    self._train_bc = data_utils.BatchConverter(\n      [\'initial_state\'], \'inputs targets weights context\'.split())\n    self._train_ba = data_utils.BatchAggregator(\n      num_keys = [self._loss, self._policy_ent, self._count])\n    \n  def set_lr(self, new_lr):\n    """"""Set the learning rate to a new value.""""""\n    self.graph.run([\'update_lr\'], feed_dict=dict(new_lr=new_lr))\n\n  def get_global_step(self):\n    global_step = self.graph.run([\'global_step\'], {})[\'global_step\']\n    return global_step\n\n  def run_epoch(self, fetch_list, feed_dict, batch_converter,\n                batch_aggregator, shuffle=False, parameters=None,\n                verbose=1,\n                writer=None):\n    """"""Run the TF graph for one pass through the data in feed_dict.\n\n    Args:\n      fetch_list: A list of the names of the nodes to be fetched.\n      feed_dict: A dictionary with names of the nodes to be feed\n        to as keys. Contains the fixed length data.\n      prepare_feed_dict_fn: A function to prepare a batch of examples\n        to a feed dict for TF graph.\n      reduce_result_dict_fn: A reducer to collect results from\n        each iteration.\n      shuffle: whether to shuffle the data.\n      parameters: A dictionary of parameters.\n      writer: A TF Filewriter to write summaries.\n\n    Returns:\n      epoch_result_dict: A dictionary with keys from fetch_list and\n        the outputs collected through the epoch.\n    """"""\n    batch_iterator = data_utils.BatchIterator(\n      feed_dict, shuffle=shuffle, batch_size=self.batch_size)\n    batch_aggregator.reset()\n    for batch_data in batch_iterator:\n      batch_feed_dict = batch_converter.convert(batch_data)\n      if parameters is not None:\n        batch_feed_dict.update(parameters)\n      result_dict = self.graph.run(fetch_list, batch_feed_dict, writer=writer)\n      batch_aggregator.merge(result_dict)\n    return batch_aggregator.result    \n\n  def step(self, inputs, state=None, parameters=None, context=None):\n    """"""Step the RNN with the given inputs and state.""""""\n    feed_dict = dict(initial_state=state, inputs=inputs, encoded_context=context)\n    fetch_list = [self._outputs, self._final_state]\n    result_dict = self.run_epoch(\n      fetch_list, feed_dict,\n      self._step_bc, self._step_ba,\n      parameters=parameters)\n    outputs = result_dict[self._outputs]\n    final_state = result_dict[self._final_state]\n    return outputs, final_state\n\n  def train(self, inputs, targets, weights=None, context=None,\n            initial_state=None, shuffle=True, update=True,\n            n_epochs=1, parameters=None,\n            writer=None):\n    if weights is None:\n      weights = data_utils.constant_struct_like(targets, 1.0)\n\n    feed_dict = dict(\n      initial_state=initial_state, inputs=inputs, targets=targets,\n      weights=weights, context=context)\n\n    for _ in xrange(n_epochs):\n      t1 = time.time()\n      fetch_list = [self._loss, self._count, self._policy_ent]\n      if update:\n        fetch_list.append(self._train)\n      result_dict = self.run_epoch(\n        fetch_list, feed_dict,\n        self._train_bc, self._train_ba,\n        shuffle=shuffle,\n        parameters=parameters, writer=writer)\n      t2 = time.time()\n      tf.logging.info(\'{} sec used in one epoch\'.format(t2 - t1))\n      total_loss = result_dict[self._loss]\n      total_n = result_dict[self._count]\n      avg_loss = total_loss / total_n\n      wps = total_n / (t2 - t1)\n    result = dict(loss=avg_loss, wps=wps)\n    result[\'policy_entropy\'] = - result_dict[self._policy_ent] / total_n\n    return result\n\n  def compute_probs(self, inputs, targets, context=None,\n                    initial_state=None, parameters=None):\n    weights = data_utils.constant_struct_like(targets, 1.0)\n    feed_dict = dict(\n      initial_state=initial_state, inputs=inputs, targets=targets,\n      weights=weights, context=context)\n    ba = data_utils.BatchAggregator(tuple_keys=[\'sequence_probs\'])\n    t1 = time.time()\n    fetch_list = [\'sequence_probs\']\n    result_dict = self.run_epoch(\n      fetch_list, feed_dict,\n      self._train_bc, ba,\n      parameters=parameters)\n    t2 = time.time()\n    probs = [l[0] for l in result_dict.get(\'sequence_probs\', [])]\n    return probs\n\n  def compute_step_logprobs(self, inputs, targets, context=None,\n                            initial_state=None, parameters=None):\n    weights = data_utils.constant_struct_like(targets, 1.0)\n    feed_dict = dict(\n      initial_state=initial_state, inputs=inputs, targets=targets,\n      weights=weights, context=context)\n    ba = data_utils.BatchAggregator(seq_keys=[\'step_logprobs\'])\n    t1 = time.time()\n    fetch_list = [\'step_logprobs\']\n    result_dict = self.run_epoch(\n      fetch_list, feed_dict,\n      self._train_bc, ba,\n      parameters=parameters)\n    t2 = time.time()\n    logprobs = result_dict.get(\'step_logprobs\', [])\n    return logprobs\n    \n  def evaluate(self, inputs, targets, weights=None, context=None,\n               initial_state=None, writer=None):\n    return self.train(inputs, targets, weights, context=context,\n                      initial_state=initial_state, shuffle=False,\n                      update=False, n_epochs=1, writer=writer)\n\n  def _predict(self, cell_outputs, predictions_node, temperature=1.0):\n    fetch_list = [predictions_node]\n    feed_dict = {self._outputs: cell_outputs}\n\n    bc = data_utils.BatchConverter(seq_keys=[self._outputs])\n    ba = data_utils.BatchAggregator(seq_keys=[predictions_node])\n      \n    result_dict = self.run_epoch(\n        fetch_list, feed_dict, bc, ba,\n        parameters=dict(temperature=temperature))\n    outputs = result_dict[predictions_node]\n    return outputs\n\n  def predict(self, cell_outputs):\n    outputs = self._predict(cell_outputs, predictions_node=self._predictions)\n    return outputs\n\n  def predict_prob(self, cell_outputs, temperature=1.0):\n    return self._predict(\n        cell_outputs, predictions_node=self._probs, temperature=temperature)\n\n  def sampling(self, cell_outputs, temperature=1.0):\n    return self._predict(\n        cell_outputs, predictions_node=self._samples, temperature=temperature)\n\n\nclass RNNSeq2seqModel(RNNSeqModel):\n  """"""Basic seq2seq model.""""""\n\n  def __init__(self, graph, batch_size=32):\n    """"""Creates a RNN seq2seq model for a given Graph object.""""""\n    super(RNNSeq2seqModel, self).__init__(graph, batch_size=batch_size)\n    self._en_outputs = graph.en_outputs\n    self._initial_state = graph.initial_state\n    self._en_initial_state = graph.en_initial_state\n    self._encode_bc = data_utils.BatchConverter(\n      tuple_keys=[self._en_initial_state], seq_keys=[\'context\'])\n    self._encode_ba = data_utils.BatchAggregator(\n      tuple_keys=[self._initial_state], seq_keys=[self._en_outputs])\n\n  def encode(self, en_inputs, en_initial_state=None,\n             parameters=None):\n    # The returned outputs and states can be directly used\n    # in step as en_outputs (for attention) and initial\n    # state (the attention context vector is already concatenated).\n    feed_dict = {self._en_initial_state: en_initial_state, \'context\': en_inputs}\n    fetch_list = [self._en_outputs, self._initial_state]\n    result_dict = self.run_epoch(\n      fetch_list, feed_dict,\n      self._encode_bc, self._encode_ba,\n      parameters=parameters)\n    outputs = result_dict[self._en_outputs]\n    final_state = result_dict[self._initial_state]\n    return outputs, final_state  \n\n\nclass MemorySeq2seqModel(RNNSeq2seqModel):\n  """"""Seq2seq model with augmented with key-variable memory.""""""\n\n  def __init__(self, graph, batch_size=32):\n    super(MemorySeq2seqModel, self).__init__(graph, batch_size=batch_size)\n    self.max_n_valid_indices = graph.config[\'core_config\'][\'max_n_valid_indices\']\n    self.n_mem = graph.config[\'core_config\'][\'n_mem\']\n    self.hidden_size = graph.config[\'core_config\'][\'hidden_size\']\n    self.value_embedding_size = graph.config[\'core_config\'][\'value_embedding_size\']\n    self._encode_bc = data_utils.BatchConverter(\n      seq_keys=[\'en_inputs\', \'en_input_features\'], tuple_keys=[\n        \'en_initial_state\', \'n_constants\', \'constant_spans\',\n        \'constant_value_embeddings\'],\n      preprocess_fn=self._preprocess)\n    self._step_bc = data_utils.BatchConverter(\n      tuple_keys=[\'initial_state\'], seq_keys=[\'encoded_context\'],\n      preprocess_fn=self._preprocess)\n    self._train_bc = data_utils.BatchConverter(\n      tuple_keys=[\'n_constants\', \'constant_spans\', \'constant_value_embeddings\'],\n      seq_keys=[\'targets\', \'weights\', \'en_inputs\', \'en_input_features\'],\n      preprocess_fn=self._preprocess)\n    \n  def init_pretrained_embeddings(self, pretrained_embeddings):\n    self.graph.run(\n      [\'en_pretrained_embeddings_init\'],\n      feed_dict={\'en_pretrained_embeddings\': pretrained_embeddings})\n    \n  def _preprocess(self, batch_dict):\n    if \'context\' in batch_dict:\n      packed_context = batch_dict[\'context\']\n      del batch_dict[\'context\']\n      batch_dict[\'en_inputs\'] = [x[0] for x in packed_context]\n      constant_value_embeddings = [x[2] for x in packed_context]\n      constant_value_embeddings = [\n        _pad_list(cs, np.zeros(self.value_embedding_size), self.n_mem)\n        for cs in constant_value_embeddings]\n      batch_dict[\'constant_value_embeddings\'] = [\n        np.array([x]) for x in constant_value_embeddings]\n      batch_dict[\'n_constants\'] = [len(x[1]) for x in packed_context]\n      constant_spans = [\n        _pad_list(x[1], [-1, -1], self.n_mem) for x in packed_context]\n      batch_dict[\'constant_spans\'] = [np.array([x]) for x in constant_spans]\n      batch_dict[\'en_input_features\'] = [np.array(x[3]) for x in packed_context]\n    if \'inputs\' in batch_dict:\n      processed_step_inputs = self._process_step_inputs(batch_dict[\'inputs\'])\n      batch_dict[\'inputs\'] = processed_step_inputs[0]\n      batch_dict[\'output_features\'] = processed_step_inputs[1]      \n\n  def _process_step_inputs(self, inputs):\n    """"""Turn a list of MemoryInputTuple into one MemoryInputTuple.\n\n    Args:\n      inputs: a list of MemoryInputTuple, like\n        [MemTuple(1, 2, [1,2,3]), MemTuple(1, 2, [1,2,3])...].\n\n    Returns:\n      processed_inputs: a MemoryInputTuple like\n        MemTuple(np.array([1, 1, ...]), np.array([2, 2, ...]),\n                 np.array([[1, 2, 3, -1, ...], [1, 2, 3, -1,...]))).\n    """"""\n    read_ind = np.array([[x[0].read_ind for x in seq] for seq in inputs])\n    write_ind = np.array([[x[0].write_ind for x in seq] for seq in inputs])\n    valid_indices = np.array([\n      [_pad_list(x[0].valid_indices, -1, self.max_n_valid_indices) for x in seq]\n      for seq in inputs])\n    output_features = np.array(\n      [[_pad_list(x[1], [0], self.max_n_valid_indices) for x in seq]\n       for seq in inputs])\n    \n    read_ind_batch, sequence_length = data_utils.convert_seqs_to_batch(read_ind)\n    output_feature_batch, _ = data_utils.convert_seqs_to_batch(output_features)\n    write_ind_batch, _ = data_utils.convert_seqs_to_batch(write_ind)\n    valid_indices_batch, _ = data_utils.convert_seqs_to_batch(valid_indices)\n    processed_inputs = tf_utils.MemoryInputTuple(\n      read_ind_batch, write_ind_batch, valid_indices_batch)\n    return (processed_inputs, sequence_length), (\n      output_feature_batch, sequence_length)\n\n\ndef _pad_list(lst, pad, length):\n  return np.array(lst + (length - len(lst)) * [pad])\n'"
nsm/nlp_utils.py,0,"b'""""""NLP utility functions.""""""\nimport nltk\n\n\ndef tokenize(string):\n  return nltk.word_tokenize(string)\n\n\ndef bleu_score(sentence, gold_sentence):\n  return nltk.translate.bleu_score.sentence_bleu(\n      [gold_sentence], sentence)\n\n\ndef edit_distance(s1, s2, substitution_cost=1, transpositions=False):\n  return nltk.metrics.distance.edit_distance(\n      s1, s2, substitution_cost=substitution_cost,\n      transpositions=transpositions)\n'"
nsm/tf_utils.py,48,"b'""TensorFlow utilities.""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport collections\nimport contextlib\nimport hashlib\nimport math\nimport numbers\n\nimport tensorflow as tf\n\nfrom tensorflow.contrib.compiler import jit\nfrom tensorflow.contrib.layers.python.layers import layers\nfrom tensorflow.contrib import rnn\nfrom tensorflow.python.framework import dtypes\nfrom tensorflow.python.framework import op_def_registry\nfrom tensorflow.python.framework import ops\nfrom tensorflow.python.ops import array_ops\nfrom tensorflow.python.ops import clip_ops\nfrom tensorflow.python.ops import init_ops\nfrom tensorflow.python.ops import math_ops\nfrom tensorflow.python.ops import nn_ops\nfrom tensorflow.python.ops import variable_scope as vs\nfrom tensorflow.python.platform import tf_logging as logging\nfrom tensorflow.python.util import nest\n\n\n_BIAS_VARIABLE_NAME = ""biases""\n_WEIGHTS_VARIABLE_NAME = ""weights""\n\n\ndef tensormul(t1, t2):\n  """"""Basically matmul, but t1 can have more dimensions than\n  t2.\n  """"""\n  dim1 = t1.get_shape().as_list()[-1]\n  dim2 = t2.get_shape().as_list()[-1]\n  result_shape_tensors = tf.unstack(tf.shape(t1))\n  result_shape_tensors[-1] = dim2\n  result_shape_tensor = tf.stack(result_shape_tensors)\n  t1 = tf.reshape(t1, [-1, dim1])\n  result = tf.matmul(t1, t2)\n  result = tf.reshape(result, result_shape_tensors)\n  return result\n\n\n@contextlib.contextmanager\ndef _checked_scope(cell, scope, reuse=None, **kwargs):\n  if reuse is not None:\n    kwargs[""reuse""] = reuse\n  with vs.variable_scope(scope, **kwargs) as checking_scope:\n    scope_name = checking_scope.name\n    if hasattr(cell, ""_scope""):\n      cell_scope = cell._scope  # pylint: disable=protected-access\n      if cell_scope.name != checking_scope.name:\n        raise ValueError(\n            ""Attempt to reuse RNNCell %s with a different variable scope than ""\n            ""its first use.  First use of cell was with scope \'%s\', this ""\n            ""attempt is with scope \'%s\'.  Please create a new instance of the ""\n            ""cell if you would like it to use a different set of weights.  ""\n            ""If before you were using: MultiRNNCell([%s(...)] * num_layers), ""\n            ""change to: MultiRNNCell([%s(...) for _ in range(num_layers)]).  ""\n            ""If before you were using the same cell instance as both the ""\n            ""forward and reverse cell of a bidirectional RNN, simply create ""\n            ""two instances (one for forward, one for reverse).  ""\n            ""In May 2017, we will start transitioning this cell\'s behavior ""\n            ""to use existing stored weights, if any, when it is called ""\n            ""with scope=None (which can lead to silent model degradation, so ""\n            ""this error will remain until then.)""\n            % (cell, cell_scope.name, scope_name, type(cell).__name__,\n               type(cell).__name__))\n    else:\n      weights_found = False\n      try:\n        with vs.variable_scope(checking_scope, reuse=True):\n          vs.get_variable(_WEIGHTS_VARIABLE_NAME)\n        weights_found = True\n      except ValueError:\n        pass\n      if weights_found and reuse is None:\n        raise ValueError(\n            ""Attempt to have a second RNNCell use the weights of a variable ""\n            ""scope that already has weights: \'%s\'; and the cell was not ""\n            ""constructed as %s(..., reuse=True).  ""\n            ""To share the weights of an RNNCell, simply ""\n            ""reuse it in your second calculation, or create a new one with ""\n            ""the argument reuse=True."" % (scope_name, type(cell).__name__))\n\n    # Everything is OK.  Update the cell\'s scope and yield it.\n    cell._scope = checking_scope  # pylint: disable=protected-access\n    yield checking_scope\n\n\nclass SeqAttentionCellWrapper(tf.nn.rnn_cell.RNNCell):\n  """"""Basic attention cell wrapper.\n  Implementation based on https://arxiv.org/abs/1409.0473.\n  """"""\n  def __init__(self, cell, attn_inputs, \n               attn_size, attn_vec_size,\n               output_size=None, input_size=None, \n               state_is_tuple=True, attn_masks=None,\n               merge_output_attn=\'linear\',\n               reuse=None):\n    """"""Create a cell with attention.\n    Args:\n      cell: an RNNCell, an attention is added to it.\n      attn_inputs: a Tensor.\n      attn_size: integer, the size of an attention vector. Equal to\n        cell.output_size by default.\n      attn_vec_size: integer, the number of convolutional features calculated\n        on attention state and a size of the hidden layer built from\n        base cell state. Equal to attn_size by default.\n      input_size: integer, the size of a hidden linear layer,\n        built from inputs and attention. Derived from the input tensor\n        by default.\n      state_is_tuple: If True, accepted and returned states are n-tuples, where\n        `n = len(cells)`.  By default (False), the states are all\n        concatenated along the column axis.\n      attn_mask: mask that should be applied to attention. If None, no masks\n         will be applied.\n      reuse: (optional) Python boolean describing whether to reuse variables\n        in an existing scope.  If not `True`, and the existing scope already has\n        the given variables, an error is raised.\n    Raises:\n      TypeError: if cell is not an RNNCell.\n      ValueError: if cell returns a state tuple but the flag\n          `state_is_tuple` is `False` or if attn_length is zero or less.\n    """"""\n    if not isinstance(cell, rnn.RNNCell):\n      raise TypeError(""The parameter cell is not RNNCell."")\n    if nest.is_sequence(cell.state_size) and not state_is_tuple:\n      raise ValueError(""Cell returns tuple of states, but the flag ""\n                       ""state_is_tuple is not set. State size is: %s""\n                       % str(cell.state_size))\n    if not state_is_tuple:\n      logging.warn(\n          ""%s: Using a concatenated state is slower and will soon be ""\n          ""deprecated.  Use state_is_tuple=True."", self)\n\n    self._state_is_tuple = state_is_tuple\n\n    if not state_is_tuple:\n      raise NotImplementedError\n\n    self._cell = cell\n    self._input_size = input_size\n    self._output_size = output_size\n    if output_size is None:\n      self._output_size = cell.output_size\n    self._attn_size = attn_size\n    self._reuse = reuse\n    self._attn_inputs = attn_inputs\n    self._attn_vec_size = attn_vec_size\n    self.attn_masks = attn_masks\n    self.merge_output_attn = merge_output_attn\n\n  @property\n  def state_size(self):\n    return self._cell.state_size\n\n  @property\n  def output_size(self):\n    return self._output_size\n\n  def zero_state(self, batch_size, dtype=tf.float32):\n    zero_state = self._cell.zero_state(batch_size, dtype=dtype)\n    return zero_state\n    \n  def __call__(self, inputs, state, scope=None):\n    """"""Seq Attention wrapper.""""""\n    with _checked_scope(\n        self, scope or ""attention_cell_wrapper"",\n        reuse=self._reuse):\n      inner_output, new_state = self._cell(inputs, state)\n      new_attns = self._attention(inner_output, self._attn_inputs)\n      if self.merge_output_attn == \'linear\':\n        with vs.variable_scope(""attn_output_projection""):\n          output = linear([inner_output, new_attns], self._output_size, True)\n      elif self.merge_output_attn == \'concat\':\n        output = tf.concat([inner_output, new_attns], axis=-1)\n      else:\n        raise ValueError(\'Unknown method to merge output and attention: {}\'.format(\n          self.merge_output_attn))\n      return output, new_state\n\n  def _attention(self, query, attn_inputs):\n    with vs.variable_scope(""attention""):\n      attn_query = tf.layers.dense(\n        inputs=query, units=self._attn_vec_size, use_bias=True)\n      attn_keys = tf.layers.dense(\n        inputs=attn_inputs, units=self._attn_vec_size, use_bias=True)\n      attn_contents = tf.layers.dense(\n        inputs=attn_inputs, units=self._attn_size, use_bias=True)\n      \n      v_attn = vs.get_variable(""attn_v"", [self._attn_vec_size])\n      scores = attn_sum_bahdanau(v_attn, attn_keys, attn_query)\n      \n      if self.attn_masks is not None:\n        score_masks = self.attn_masks\n        scores = scores * score_masks + (1.0 - score_masks) * tf.float32.min\n\n      attn_weights = nn_ops.softmax(scores)\n      new_attns = math_ops.reduce_sum(\n        tf.expand_dims(attn_weights, -1) * attn_contents, [1])\n      return new_attns\n\n\ndef attn_sum_bahdanau(v_attn, keys, query):\n  """"""Calculates a batch and timewise dot product with a variable""""""\n  return tf.reduce_sum(v_attn * tf.tanh(keys + tf.expand_dims(query, 1)), [2])\n\n\ndef attn_sum_dot(keys, query):\n  """"""Calculates a batch and timewise dot product""""""\n  return tf.reduce_sum(keys * tf.expand_dims(query, 1), [2])\n\n\ndef linear(args, output_size, bias, bias_start=0.0):\n  """"""Linear map: sum_i(args[i] * W[i]), where W[i] is a variable.\n  Args:\n    args: a 2D Tensor or a list of 2D, batch x n, Tensors.\n    output_size: int, second dimension of W[i].\n    bias: boolean, whether to add a bias term or not.\n    bias_start: starting value to initialize the bias; 0 by default.\n  Returns:\n    A 2D Tensor with shape [batch x output_size] equal to\n    sum_i(args[i] * W[i]), where W[i]s are newly created matrices.\n  Raises:\n    ValueError: if some of the arguments has unspecified or wrong shape.\n  """"""\n  if args is None or (nest.is_sequence(args) and not args):\n    raise ValueError(""`args` must be specified"")\n  if not nest.is_sequence(args):\n    args = [args]\n\n  # Calculate the total size of arguments on dimension 1.\n  total_arg_size = 0\n  shapes = [a.get_shape() for a in args]\n  for shape in shapes:\n    if shape.ndims != 2:\n      raise ValueError(""linear is expecting 2D arguments: %s"" % shapes)\n    if shape[1].value is None:\n      raise ValueError(""linear expects shape[1] to be provided for shape %s, ""\n                       ""but saw %s"" % (shape, shape[1]))\n    else:\n      total_arg_size += shape[1].value\n\n  dtype = [a.dtype for a in args][0]\n\n  # Now the computation.\n  scope = vs.get_variable_scope()\n  with vs.variable_scope(scope) as outer_scope:\n    weights = vs.get_variable(\n        _WEIGHTS_VARIABLE_NAME, [total_arg_size, output_size], dtype=dtype)\n    if len(args) == 1:\n      res = math_ops.matmul(args[0], weights)\n    else:\n      res = math_ops.matmul(array_ops.concat(args, 1), weights)\n    if not bias:\n      return res\n    with vs.variable_scope(outer_scope) as inner_scope:\n      inner_scope.set_partitioner(None)\n      biases = vs.get_variable(\n          _BIAS_VARIABLE_NAME, [output_size],\n          dtype=dtype,\n          initializer=init_ops.constant_initializer(bias_start, dtype=dtype))\n    return nn_ops.bias_add(res, biases)\n\n\nMemoryStateTuple = collections.namedtuple(\'MemoryStateTuple\', (\'memory\', \'inner_state\'))\nMemoryInputTuple = collections.namedtuple(\n  \'MemoryInputTuple\', (\'read_ind\', \'write_ind\', \'valid_indices\'))\n\n\nclass MemoryWrapper(tf.nn.rnn_cell.RNNCell):\n  """"""Augment RNNCell with a memory that the RNN can write to and read from.\n\n  Each time step, 3 things are happening:\n\n  1) the RNNCell reads from one memory location (read_ind)\n  as input to the inner RNN.\n\n  2) It also writes the output of the inner RNN to one\n  memory location (write_ind). 1 indicates no writing.\n\n  3) It use the output of the inner RNN to compute the\n  logits for the valid_indices, which will be used as input\n  to compute a softmax distribution over them. Note that\n  valid_indices always has the dimension\n  max_n_valid_indices, use -1 to pad the dimensions the\n  actual number of valid indices are less.\n\n  """"""\n\n  def __init__(self, cell, mem_size, embed_size, max_n_valid_indices):\n    """"""Constructs a `ResidualWrapper` for `cell`.\n    Args:\n      cell: An instance of `RNNCell`.\n      mem_size: size of the memory.\n      embed_size: the size/dimension of the embedding in each memory location.\n      max_n_valid_indices: maximum number of valid_indices.\n    """"""\n    self._cell = cell\n    self._mem_size = mem_size\n    self._embed_size = embed_size\n    self._max_n_valid_indices = max_n_valid_indices\n\n  @property\n  def state_size(self):\n    # This will be used to create zero states.\n    return MemoryStateTuple(\n      tf.TensorShape([self._mem_size, self._embed_size]), self._cell.state_size)\n\n  @property\n  def output_size(self):\n    # The output is the logits of the valid_dices.\n    return self._max_n_valid_indices\n\n  def __call__(self, inputs, state, scope=None, debug=False):\n    """"""Unroll the memory augmented cell for one step.\n    \n    Args:\n      inputs: (Possibly nested tuple of) Tensor, the input at this time step.\n      state: An instance of MemoryStateTuple containing\n        tensors from the previous time step.\n    """"""\n    # B is batch size.\n\n    # 1) Use read_ind to find memory location to read from\n    # as input.\n\n    # inputs.read_ind: (B, 1)\n    # memory: (B, mem_size, embed_size)\n    read_ind = tf.to_int32(inputs.read_ind)\n    batch_size = tf.shape(read_ind)[0]\n    if debug:\n      print(\'batch size is\', batch_size)\n      print(\'read ind is\', read_ind)\n    mem_ind = tf.range(batch_size)\n\n    # read_mem_ind: (B, 1)\n    read_mem_ind = tf.expand_dims(mem_ind, axis=1)\n    # read_ind: (B, 2)\n    read_ind = tf.concat([read_mem_ind, read_ind], axis=1)\n    if debug:\n      print(\'processed read ind is\', read_ind)\n\n    # inner_inputs: (B, embed_size)\n    inner_inputs = tf.gather_nd(state.memory, read_ind)\n    if debug:\n      print(\'inner_inputs is\', inner_inputs)\n\n    inner_state = state.inner_state\n\n    # 2) Run the inner RNNCell.\n    \n    # inner_outputs: (B, embed_size)\n    inner_outputs, new_inner_state = self._cell(inner_inputs, inner_state, scope=scope)\n\n    if debug:\n      print(\'inner_outputs is\', inner_outputs)\n\n    # 3) Compute logits for valid indices (using logit_masks\n    # to mask out padded valid indices (-1)).\n      \n    # valid_indices: (B, max_n_valid_indices)\n    valid_indices = tf.to_int32(inputs.valid_indices)\n    if debug:\n      print(\'valid_indices is\', valid_indices)\n    \n    # Logit mask: (B, max_n_valid_indices)\n    logit_masks = tf.greater_equal(inputs.valid_indices, 0)\n    logit_masks = tf.cast(logit_masks, tf.float32)\n\n    if debug:\n      print(\'logit_masks is\', logit_masks)\n\n    # Normalize indices to be at least 0.\n    valid_indices = tf.maximum(valid_indices, 0)\n\n    # valid_indices: (B, max_n_valid_indices, 1)\n    valid_indices = tf.expand_dims(valid_indices, -1)\n    if debug:\n      print(\'valid_indices is\', valid_indices)\n      print(\'mem_ind is\', mem_ind)\n\n    valid_mem_ind = tf.expand_dims(mem_ind, axis=1)\n\n    # valid_mem_ind: (B, 1, 1)\n    valid_mem_ind = tf.expand_dims(valid_mem_ind, axis=2)\n    if debug:\n      print(\'valid_mem_ind is\', valid_mem_ind)\n\n    # valid_mem_ind: (B, max_n_valid_indices, 1)\n    valid_mem_ind = tf.tile(valid_mem_ind, [1, self._max_n_valid_indices, 1])\n\n    # valid_indices: (B, max_n_valid_indices, 2)\n    # Third dimension of valid_indices is [b_i, valid_index] so that it can\n    # index into the right memory location.\n    valid_indices = tf.concat([valid_mem_ind, valid_indices], axis=2)\n\n    if debug:\n      print(\'valid_indices is\', valid_indices)\n\n    # select all the valid slots. \n    # valid_values: (B, max_n_valid_indices, embed_size)\n    valid_values = tf.gather_nd(state.memory, valid_indices)\n    if debug:\n      print(\'valid_values is\', valid_values)\n    \n    # expanded_inner_outputs: (B, 1, embed_size)\n    expanded_inner_outputs = tf.expand_dims(inner_outputs, 1)\n    if debug:\n      print(\'expanded_inner_outputs is\', expanded_inner_outputs)\n\n    # valid_values: (B, embed_size, max_n_valid_indices)\n    valid_values = tf.transpose(valid_values, [0, 2, 1])\n    if debug:\n      print(\'valid_values is\', valid_values)\n\n    # logits: (B, 1, max_n_valid_indices)\n    logits = tf.matmul(expanded_inner_outputs, valid_values)\n      \n    if debug:\n      print(\'logits is\', logits)\n\n    # logits: (B, max_n_valid_indices)\n    logits = tf.squeeze(logits, axis=[1])\n    if debug:\n      print(\'logits is\', logits)\n\n    # masked_logits = (logits * logit_masks) - (1 - logit_masks) * 1e6\n    masked_logits = logits * logit_masks + (1 - logit_masks) * tf.float32.min\n    # masked_logits = tf.Print(masked_logits, [masked_logits], message=\'masked_logits\')\n\n    outputs = masked_logits\n    # 4) Write the output of the inner RNN to a memory\n    # location (write_ind), using write_masks to mask out\n    # padded write_ind (-1).\n    \n    # write_ind: (B, 1)\n    write_ind = tf.cast(inputs.write_ind, tf.int32)\n    if debug:\n      print(\'write_ind is\', write_ind)\n\n    # write mask: (B, 1)\n    write_masks = tf.greater_equal(inputs.write_ind, 0)\n    if debug:\n      print(\'write_masks greater_equal\', write_masks)\n\n    write_masks = tf.cast(write_masks, tf.float32)\n\n    # write mask: (B, 1, 1)\n    write_masks = tf.expand_dims(write_masks, [-1])\n\n    if debug:\n      print(\'write_masks is\', write_masks)\n\n    # Normalize write_ind to be above 0.\n    # write_ind: (B, 1)\n    write_ind = tf.maximum(write_ind, 0)\n\n    # write_mem_ind: (B, 1)\n    write_mem_ind = tf.expand_dims(mem_ind, axis=1)\n\n    # write_ind: (B, 2)\n    # Second dimension is [b_i, write_index]\n    write_ind = tf.concat([write_mem_ind, write_ind], axis=1)\n    if debug:\n      print(\'write_ind is\', write_ind)\n\n    if debug:\n      print(\'masked_logits is\', masked_logits)\n      print(\'memory is\', state.memory)\n\n    # write_mat: (B, mem_size, embed_size)\n    write_mat = tf.scatter_nd(\n      write_ind, inner_outputs, shape=tf.shape(state.memory))\n\n    if debug:\n      print(\'@\' * 50)\n      print(\'write_mat is\', write_mat)\n      print(\'write_mask is\', write_masks)\n      print(\'@\' * 50)\n\n    masked_write_mat = write_mat * write_masks\n    new_memory = state.memory + masked_write_mat\n    \n    state = MemoryStateTuple(new_memory, new_inner_state)\n    if debug:\n      print(\'state is\', state)\n      \n    return (outputs, state)\n'"
nsm/word_embeddings.py,0,"b""import json\nimport numpy as np\nimport gensim\n\n\nclass EmbeddingModel(object):\n  def __init__(\n      self, vocab_file, embedding_file, normalize_embeddings=True):\n    with open(embedding_file, 'rb') as f:\n      self.embedding_mat = np.load(f)\n    if normalize_embeddings:\n      self.embedding_mat = self.embedding_mat / np.linalg.norm(\n        self.embedding_mat, axis=1, keepdims=True)\n    with open(vocab_file, 'r') as f:\n      tks = json.load(f)\n    self.vocab = dict(zip(tks, range(len(tks))))\n\n  def __contains__(self, word):\n    return word in self.vocab\n    \n  def __getitem__(self, word):\n    if word in self.vocab:\n      index = self.vocab[word]\n      return self.embedding_mat[index]\n    else:\n      raise KeyError\n"""
table/__init__.py,0,b'\n'
table/experiment.py,166,"b'import shutil\nimport json\nimport time\nimport os\nimport sys\nimport re\nimport random\nimport cPickle as pkl\nimport functools\nimport pprint\nimport codecs\nimport multiprocessing\n\nimport copy\nimport numpy as np\nimport tensorflow as tf\n\nimport nsm\nfrom nsm import data_utils\nfrom nsm import env_factory\nfrom nsm import graph_factory\nfrom nsm import model_factory\nfrom nsm import agent_factory\nfrom nsm import executor_factory\nfrom nsm import computer_factory\nfrom nsm import word_embeddings\n\nimport utils\n\n\n# FLAGS\nFLAGS = tf.app.flags.FLAGS  \n\n\n# Experiment name\ntf.flags.DEFINE_string(\'output_dir\', \'\', \'output folder.\')\ntf.flags.DEFINE_string(\'experiment_name\', \'experiment\',\n                       \'All outputs of this experiment is\'\n                       \' saved under a folder with the same name.\')\n\n\n# Tensorboard logging\ntf.app.flags.DEFINE_string(\n  \'tb_log_dir\', \'tb_log\', \'Path for saving tensorboard logs.\')\n\n\n# Tensorflow model checkpoint.\ntf.app.flags.DEFINE_string(\n  \'saved_model_dir\', \'saved_model\', \'Path for saving models.\')\ntf.app.flags.DEFINE_string(\n  \'best_model_dir\', \'best_model\', \'Path for saving best models.\')\ntf.app.flags.DEFINE_string(\n  \'init_model_path\', \'\', \'Path for saving best models.\')\ntf.app.flags.DEFINE_string(\n  \'meta_graph_path\', \'\', \'Path for meta graph.\')\ntf.app.flags.DEFINE_string(\'experiment_to_eval\', \'\', \'.\')\n\n\n# Model\n## Computer\ntf.app.flags.DEFINE_integer(\n  \'max_n_mem\', 100, \'Max number of memory slots in the ""computer"".\')\ntf.app.flags.DEFINE_integer(\n  \'max_n_exp\', 3, \'Max number of expressions allowed in a program.\')\ntf.app.flags.DEFINE_integer(\n  \'max_n_valid_indices\', 100, \'Max number of valid tokens during decoding.\')\ntf.app.flags.DEFINE_bool(\n  \'use_cache\', False, \'Use cache to avoid generating the same samples.\')\ntf.app.flags.DEFINE_string(\n  \'en_vocab_file\', \'\', \'.\')\ntf.app.flags.DEFINE_string(\n  \'executor\', \'wtq\', \'Which executor to use, wtq or wikisql.\')\n\n\n## neural network\ntf.app.flags.DEFINE_integer(\n  \'hidden_size\', 100, \'Number of hidden units.\')\ntf.app.flags.DEFINE_integer(\n  \'attn_size\', 100, \'Size of attention vector.\')\ntf.app.flags.DEFINE_integer(\n  \'attn_vec_size\', 100, \'Size of the vector parameter for computing attention.\')\ntf.app.flags.DEFINE_integer(\n  \'n_layers\', 1, \'Number of layers in decoder.\')\ntf.app.flags.DEFINE_integer(\n  \'en_n_layers\', 1, \'Number of layers in encoder.\')\ntf.app.flags.DEFINE_integer(\n  \'en_embedding_size\', 100, \'Size of encoder input embedding.\')\ntf.app.flags.DEFINE_integer(\n  \'value_embedding_size\', 300, \'Size of value embedding for the constants.\')\ntf.app.flags.DEFINE_bool(\n  \'en_bidirectional\', False, \'Whether to use bidirectional RNN in encoder.\')\ntf.app.flags.DEFINE_bool(\n  \'en_attn_on_constants\', False, \'.\')\ntf.app.flags.DEFINE_bool(\n  \'use_pretrained_embeddings\', False, \'Whether to use pretrained embeddings.\')\ntf.app.flags.DEFINE_integer(\n  \'pretrained_embedding_size\', 300, \'Size of pretrained embedding.\')\n\n\n# Features\ntf.app.flags.DEFINE_integer(\n  \'n_de_output_features\', 1,\n  \'Number of features in decoder output softmax.\')\ntf.app.flags.DEFINE_integer(\n  \'n_en_input_features\', 1,\n  \'Number of features in encoder inputs.\')\n\n\n# Data\ntf.app.flags.DEFINE_string(\n  \'table_file\', \'\', \'Path to the file of wikitables, a jsonl file.\')\ntf.app.flags.DEFINE_string(\n  \'train_file\', \'\', \'Path to the file of training examples, a jsonl file.\')\ntf.app.flags.DEFINE_string(\n  \'dev_file\', \'\', \'Path to the file of training examples, a jsonl file.\')\ntf.app.flags.DEFINE_string(\n  \'eval_file\', \'\', \'Path to the file of test examples, a jsonl file.\')\ntf.app.flags.DEFINE_string(\n  \'embedding_file\', \'\', \'Path to the file of pretrained embeddings, a npy file.\')\ntf.app.flags.DEFINE_string(\n  \'vocab_file\', \'\', \'Path to the vocab file for the pretrained embeddings, a json file.\')\ntf.app.flags.DEFINE_string(\n  \'train_shard_dir\', \'\', \'Folder containing the sharded training data.\')\ntf.app.flags.DEFINE_string(\n  \'train_shard_prefix\', \'\', \'The prefix for the sharded files.\')\ntf.app.flags.DEFINE_integer(\n  \'n_train_shard\', 90, \'Number of shards in total.\')\ntf.app.flags.DEFINE_integer(\n  \'shard_start\', 0,\n  \'Start id of the shard to use.\')\ntf.app.flags.DEFINE_integer(\n  \'shard_end\', 90, \'End id of the shard to use.\')\n\n\n# Load saved samples.\ntf.app.flags.DEFINE_bool(\n  \'load_saved_programs\', False,\n  \'Whether to use load saved programs from exploration.\')\ntf.app.flags.DEFINE_string(\n  \'saved_program_file\', \'\', \'Saved program file.\')\n\n\n# Training\ntf.app.flags.DEFINE_integer(\n  \'n_steps\', 100000, \'Maximum number of steps in training.\')\ntf.app.flags.DEFINE_integer(\n  \'n_explore_samples\', 1, \'Number of exploration samples per env per epoch.\')\ntf.app.flags.DEFINE_integer(\n  \'n_extra_explore_for_hard\', 0, \'Number of exploration samples for hard envs.\')\ntf.app.flags.DEFINE_float(\n  \'learning_rate\', 0.001, \'Initial learning rate.\')\ntf.app.flags.DEFINE_float(\n  \'max_grad_norm\', 5.0, \'Maximum gradient norm.\')\ntf.app.flags.DEFINE_float(\n  \'l2_coeff\', 0.0, \'l2 regularization coefficient.\')\ntf.app.flags.DEFINE_float(\n  \'dropout\', 0.0, \'Dropout rate.\')\ntf.app.flags.DEFINE_integer(\n  \'batch_size\', 10, \'Model batch size.\')\ntf.app.flags.DEFINE_integer(\n  \'n_actors\', 3, \'Number of actors for generating samples.\')\ntf.app.flags.DEFINE_integer(\n  \'save_every_n\', -1,\n  \'Save model to a ckpt every n train steps, -1 means save every epoch.\')\ntf.app.flags.DEFINE_bool(\n  \'save_replay_buffer_at_end\', True,\n  \'Whether to save the full replay buffer for each actor at the \'\n  \'end of training or not\')\ntf.app.flags.DEFINE_integer(\n  \'log_samples_every_n_epoch\', 0,\n  \'Log samples every n epochs.\')\ntf.app.flags.DEFINE_bool(\n  \'greedy_exploration\', False,\n  \'Whether to use a greedy policy when doing systematic exploration.\')\ntf.app.flags.DEFINE_bool(\n  \'use_baseline\', False,\n  \'Whether to use baseline during policy gradient.\')\ntf.app.flags.DEFINE_float(\n  \'min_prob\', 0.0,\n  (\'Minimum probability of a negative\'\n   \'example for it to be punished to avoid numerical issue.\'))\ntf.app.flags.DEFINE_float(\n  \'lm_loss_coeff\', 0.0,\n  \'Weight for lm loss.\')\ntf.app.flags.DEFINE_float(\n  \'entropy_reg_coeff\', 0.0,\n  \'Weight for entropy regularization.\')\ntf.app.flags.DEFINE_string(\n  \'optimizer\', \'adam\', \'.\')\ntf.app.flags.DEFINE_float(\n  \'adam_beta1\', 0.9, \'adam beta1 parameter.\')\ntf.app.flags.DEFINE_bool(\n  \'sample_other\', False, \'Whether to use a greedy policy during training.\')\ntf.app.flags.DEFINE_bool(\n  \'use_replay_samples_in_train\', False,\n  \'Whether to use replay samples for training.\')\ntf.app.flags.DEFINE_bool(\n  \'random_replay_samples\', False,\n  \'randomly pick a replay samples as ML baseline.\')\ntf.app.flags.DEFINE_bool(\n  \'use_policy_samples_in_train\', False,\n  \'Whether to use on-policy samples for training.\')\ntf.app.flags.DEFINE_bool(\n  \'use_nonreplay_samples_in_train\', False,\n  \'Whether to use a negative samples for training.\')\ntf.app.flags.DEFINE_integer(\n  \'n_replay_samples\', 5, \'Number of replay samples drawn.\')\ntf.app.flags.DEFINE_integer(\n  \'n_policy_samples\', 5, \'Number of on-policy samples drawn.\')\ntf.app.flags.DEFINE_bool(\n  \'use_top_k_replay_samples\', False,\n  (\'Whether to use the top k most probable (model probability) replay samples\'\n   \' or to sample from the replay samples.\'))\ntf.app.flags.DEFINE_bool(\n  \'use_top_k_policy_samples\', False,\n  (\'Whether to use the top k most probable (from beam search) samples\'\n   \' or to sample from the replay samples.\'))\ntf.app.flags.DEFINE_float(\n  \'fixed_replay_weight\', 0.5, \'Weight for replay samples between 0.0 and 1.0.\')\ntf.app.flags.DEFINE_bool(\n  \'use_replay_prob_as_weight\', False,\n  \'Whether or not use replay probability as weight for replay samples.\')\ntf.app.flags.DEFINE_float(\n  \'min_replay_weight\', 0.1, \'minimum replay weight.\')\ntf.app.flags.DEFINE_bool(\n  \'use_importance_sampling\', False, \'\')\ntf.app.flags.DEFINE_float(\n  \'ppo_epsilon\', 0.2, \'\')\ntf.app.flags.DEFINE_integer(\n  \'truncate_replay_buffer_at_n\', 0,\n  \'Whether truncate the replay buffer to the top n highest prob trajs.\')\ntf.app.flags.DEFINE_bool(\n  \'use_trainer_prob\', False,\n  \'Whether to supply all the replay buffer for training.\')\ntf.app.flags.DEFINE_bool(\n  \'show_log\', False,\n  \'Whether to show logging info.\')\n\n\n# Eval\ntf.app.flags.DEFINE_integer(\n  \'eval_beam_size\', 5,\n  \'Beam size when evaluating on development set.\')\n\ntf.app.flags.DEFINE_integer(\n  \'eval_batch_size\', 50,\n  \'Batch size when evaluating on development set.\')\n\ntf.app.flags.DEFINE_bool(\n  \'eval_only\', False, \'only run evaluator.\')\n\ntf.app.flags.DEFINE_bool(\n  \'debug\', False, \'Whether to output debug information.\')\n\n\n# Device placement.\ntf.app.flags.DEFINE_bool(\n  \'train_use_gpu\', False, \'Whether to output debug information.\')\ntf.app.flags.DEFINE_integer(\n  \'train_gpu_id\', 0, \'Id of the gpu used for training.\')\n\ntf.app.flags.DEFINE_bool(\n  \'eval_use_gpu\', False, \'Whether to output debug information.\')\ntf.app.flags.DEFINE_integer(\n  \'eval_gpu_id\', 1, \'Id of the gpu used for eval.\')\n\ntf.app.flags.DEFINE_bool(\n  \'actor_use_gpu\', False, \'Whether to output debug information.\')\ntf.app.flags.DEFINE_integer(\n  \'actor_gpu_start_id\', 0,\n  \'Id of the gpu for the first actor, gpu for other actors will follow.\')\n\n\n# Testing\ntf.app.flags.DEFINE_bool(\n  \'unittest\', False, \'.\')\n\ntf.app.flags.DEFINE_integer(\n  \'n_opt_step\', 1, \'Number of optimization steps per training batch.\')\n\n\n\ndef get_experiment_dir():\n  experiment_dir = os.path.join(FLAGS.output_dir, FLAGS.experiment_name)\n  if not tf.gfile.IsDirectory(FLAGS.output_dir):\n    tf.gfile.MkDir(FLAGS.output_dir)\n  if not tf.gfile.IsDirectory(experiment_dir):\n    tf.gfile.MkDir(experiment_dir)\n  return experiment_dir  \n\n\ndef get_init_model_path():\n  if FLAGS.init_model_path:\n    return FLAGS.init_model_path\n  elif FLAGS.experiment_to_eval:\n    with open(os.path.join(\n        FLAGS.output_dir,\n        FLAGS.experiment_to_eval,\n        \'best_model_info.json\'), \'r\') as f:\n      best_model_info = json.load(f)\n      best_model_path = os.path.expanduser(\n        best_model_info[\'best_model_path\'])\n      return best_model_path\n  else:\n    return \'\'\n\n\ndef get_saved_graph_config():\n  if FLAGS.experiment_to_eval:\n    with open(os.path.join(\n        FLAGS.output_dir,\n        FLAGS.experiment_to_eval,\n        \'graph_config.json\'), \'r\') as f:\n      graph_config = json.load(f)\n      return graph_config\n  else:\n    return None\n\n\ndef get_saved_experiment_config():\n  if FLAGS.experiment_to_eval:\n    with open(os.path.join(\n        FLAGS.output_dir,\n        FLAGS.experiment_to_eval,\n        \'experiment_config.json\'), \'r\') as f:\n      experiment_config = json.load(f)\n      return experiment_config\n  else:\n    return None\n    \n\ndef show_samples(samples, de_vocab, env_dict=None):\n  string = \'\'\n  for sample in samples:\n    traj = sample.traj\n    actions = traj.actions\n    obs = traj.obs\n    pred_answer = traj.answer\n    string += u\'\\n\'\n    env_name = traj.env_name\n    string += u\'env {}\\n\'.format(env_name)\n    if env_dict is not None:\n      string += u\'question: {}\\n\'.format(env_dict[env_name].question_annotation[\'question\'])\n      string += u\'answer: {}\\n\'.format(env_dict[env_name].question_annotation[\'answer\'])\n    tokens = []\n    program = []\n    for t, (a, ob) in enumerate(zip(actions, obs)):\n      ob = ob[0]\n      valid_tokens = de_vocab.lookup(ob.valid_indices, reverse=True)\n      token = valid_tokens[a]\n      program.append(token)\n    program_str = \' \'.join(program)\n    if env_dict:\n      program_str = unpack_program(program_str, env_dict[env_name])\n    string += u\'program: {}\\n\'.format(program_str)\n    string += u\'prediction: {}\\n\'.format(pred_answer)\n    string += u\'return: {}\\n\'.format(sum(traj.rewards))\n    string += u\'prob is {}\\n\'.format(sample.prob)\n  return string\n\n\ndef collect_traj_for_program(env, program):\n  env = env.clone()\n  env.use_cache = False\n  ob = env.start_ob\n\n  for tk in program:\n    valid_actions = list(ob[0].valid_indices)\n    mapped_action = env.de_vocab.lookup(tk)\n    try:\n      action = valid_actions.index(mapped_action)\n    except Exception:\n      return None\n    ob, _, _, _ = env.step(action)\n  traj = agent_factory.Traj(\n    obs=env.obs, actions=env.actions, rewards=env.rewards,\n    context=env.get_context(), env_name=env.name, answer=env.interpreter.result)\n  return traj\n\n\ndef unpack_program(program_str, env):\n  ns = env.interpreter.namespace\n  processed_program = []\n  for tk in program_str.split():\n    if tk[:1] == \'v\' and tk in ns:\n      processed_program.append(unicode(ns[tk][\'value\']))\n    else:\n      processed_program.append(tk)\n  return \' \'.join(processed_program)\n\n\ndef load_programs(envs, replay_buffer, fn):\n  if not tf.gfile.Exists(fn):\n    return \n  with open(fn, \'r\') as f:\n    program_dict = json.load(f)\n  trajs = []\n  n = 0\n  total_env = 0\n  n_found = 0\n  for env in envs:\n    total_env += 1\n    found = False\n    if env.name in program_dict:\n      program_str_list = program_dict[env.name]\n      n += len(program_str_list)\n      env.cache._set = set(program_str_list)\n      for program_str in program_str_list:\n        program = program_str.split()\n        traj = collect_traj_for_program(env, program)\n        if traj is not None:\n          trajs.append(traj)\n          if not found:\n            found = True\n            n_found += 1\n  tf.logging.info(\'@\' * 100)\n  tf.logging.info(\'loading programs from file {}\'.format(fn))\n  tf.logging.info(\'at least 1 solution found fraction: {}\'.format(\n    float(n_found) / total_env))\n  replay_buffer.save_trajs(trajs)\n  n_trajs_buffer = 0\n  for k, v in replay_buffer._buffer.iteritems():\n    n_trajs_buffer += len(v)\n  tf.logging.info(\'{} programs in the file\'.format(n))\n  tf.logging.info(\'{} programs extracted\'.format(len(trajs)))\n  tf.logging.info(\'{} programs in the buffer\'.format(n_trajs_buffer))\n  tf.logging.info(\'@\' * 100)\n\n\ndef get_program_shard_path(i):\n  return os.path.join(\n    FLAGS.saved_programs_dir, FLAGS.program_shard_prefix + str(i) + \'.json\')\n\n\ndef get_train_shard_path(i):\n  return os.path.join(\n    FLAGS.train_shard_dir, FLAGS.train_shard_prefix + str(i) + \'.jsonl\')\n\n\ndef load_jsonl(fn):\n  result = []\n  with open(fn, \'r\') as f:\n    for line in f:\n      data = json.loads(line)\n      result.append(data)\n  return result\n\n\ndef create_envs(table_dict, data_set, en_vocab, embedding_model):\n  all_envs = []\n  t1 = time.time()\n  if FLAGS.executor == \'wtq\':\n    score_fn = utils.wtq_score\n    process_answer_fn = lambda x: x\n    executor_fn = executor_factory.WikiTableExecutor\n  elif FLAGS.executor == \'wikisql\':\n    score_fn = utils.wikisql_score\n    process_answer_fn = utils.wikisql_process_answer\n    executor_fn = executor_factory.WikiSQLExecutor\n  else:\n    raise ValueError(\'Unknown executor {}\'.format(FLAGS.executor))\n\n  for i, example in enumerate(data_set):\n      if i % 100 == 0:\n          tf.logging.info(\'creating environment #{}\'.format(i))\n      kg_info = table_dict[example[\'context\']]\n      executor = executor_fn(kg_info)\n      api = executor.get_api()\n      type_hierarchy = api[\'type_hierarchy\']\n      func_dict = api[\'func_dict\']\n      constant_dict = api[\'constant_dict\']\n      interpreter = computer_factory.LispInterpreter(\n        type_hierarchy=type_hierarchy, \n        max_mem=FLAGS.max_n_mem, max_n_exp=FLAGS.max_n_exp, assisted=True)\n      for v in func_dict.values():\n          interpreter.add_function(**v)\n\n      interpreter.add_constant(\n        value=kg_info[\'row_ents\'], type=\'entity_list\', name=\'all_rows\')\n\n      de_vocab = interpreter.get_vocab()\n\n      constant_value_embedding_fn = lambda x: utils.get_embedding_for_constant(\n        x, embedding_model, embedding_size=FLAGS.pretrained_embedding_size)\n      env = env_factory.QAProgrammingEnv(\n        en_vocab, de_vocab, question_annotation=example,\n        answer=process_answer_fn(example[\'answer\']),\n        constants=constant_dict.values(),\n        interpreter=interpreter,\n        constant_value_embedding_fn=constant_value_embedding_fn,\n        score_fn=score_fn,\n        name=example[\'id\'])\n      all_envs.append(env)\n  return all_envs\n\n\ndef create_agent(graph_config, init_model_path,\n                 pretrained_embeddings=None):\n  tf.logging.info(\'Start creating and initializing graph\')\n  t1 = time.time()\n  graph = graph_factory.MemorySeq2seqGraph(graph_config)\n  graph.launch(init_model_path=init_model_path)\n  t2 = time.time()\n  tf.logging.info(\'{} sec used to create and initialize graph\'.format(t2 - t1))\n\n  tf.logging.info(\'Start creating model and agent\')\n  t1 = time.time()\n  model = model_factory.MemorySeq2seqModel(graph, batch_size=FLAGS.batch_size)\n\n  if pretrained_embeddings is not None:\n    model.init_pretrained_embeddings(pretrained_embeddings)\n  agent = agent_factory.PGAgent(model)\n  t2 = time.time()\n  tf.logging.info(\'{} sec used to create model and agent\'.format(t2 - t1))\n  return agent  \n  \n\ndef init_experiment(fns, use_gpu=False, gpu_id=\'0\'):\n  dataset = []\n  for fn in fns:\n    dataset += load_jsonl(fn)\n  tf.logging.info(\'{} examples in dataset.\'.format(len(dataset)))\n  tables = load_jsonl(FLAGS.table_file)\n  table_dict = dict([(table[\'name\'], table) for table in tables])\n  tf.logging.info(\'{} tables.\'.format(len(table_dict)))\n\n  # Load pretrained embeddings.\n  embedding_model = word_embeddings.EmbeddingModel(\n    FLAGS.vocab_file, FLAGS.embedding_file)\n\n  with open(FLAGS.en_vocab_file, \'r\') as f:\n    vocab = json.load(f)\n  en_vocab = data_utils.Vocab([])\n  en_vocab.load_vocab(vocab)\n  tf.logging.info(\'{} unique tokens in encoder vocab\'.format(\n    len(en_vocab.vocab)))\n  tf.logging.info(\'{} examples in the dataset\'.format(len(dataset)))\n  \n  # Create environments.   \n  envs = create_envs(table_dict, dataset, en_vocab, embedding_model)\n  if FLAGS.unittest:\n    envs = envs[:25]\n  tf.logging.info(\'{} environments in total\'.format(len(envs)))\n\n  graph_config = get_saved_graph_config()\n  if graph_config:\n    # If evaluating an saved model, just load its graph\n    # config.\n    agent = create_agent(graph_config, get_init_model_path())\n  else:\n    if FLAGS.use_pretrained_embeddings:\n      tf.logging.info(\'Using pretrained embeddings!\')\n      pretrained_embeddings = []\n      for i in xrange(len(en_vocab.special_tks), en_vocab.size):\n        pretrained_embeddings.append(\n          utils.average_token_embedding(\n            utils.find_tk_in_model(\n              en_vocab.lookup(i, reverse=True), embedding_model),\n            embedding_model,\n            embedding_size=FLAGS.pretrained_embedding_size))\n      pretrained_embeddings = np.vstack(pretrained_embeddings)\n    else:\n      pretrained_embeddings = None\n\n    # Model configuration and initialization.\n    de_vocab = envs[0].de_vocab\n    n_mem = FLAGS.max_n_mem\n    n_builtin = de_vocab.size - n_mem\n    en_pretrained_vocab_size = en_vocab.size - len(en_vocab.special_tks)\n\n    graph_config = {}\n    graph_config[\'core_config\'] = dict(\n      max_n_valid_indices=FLAGS.max_n_valid_indices,\n      n_mem=n_mem,\n      n_builtin=n_builtin,\n      use_attn=True, \n      attn_size=FLAGS.attn_size,\n      attn_vec_size=FLAGS.attn_vec_size,\n      input_vocab_size=de_vocab.size,\n      en_input_vocab_size=en_vocab.size,\n      hidden_size=FLAGS.hidden_size, n_layers=FLAGS.n_layers,\n      en_hidden_size=FLAGS.hidden_size, en_n_layers=FLAGS.en_n_layers,\n      en_use_embeddings=True,\n      en_embedding_size=FLAGS.en_embedding_size,\n      value_embedding_size=FLAGS.value_embedding_size,\n      en_pretrained_vocab_size=en_pretrained_vocab_size,\n      en_pretrained_embedding_size=FLAGS.pretrained_embedding_size,\n      add_lm_loss=FLAGS.lm_loss_coeff > 0.0,\n      en_bidirectional=FLAGS.en_bidirectional,\n      en_attn_on_constants=FLAGS.en_attn_on_constants)\n\n    graph_config[\'use_gpu\'] = use_gpu\n    graph_config[\'gpu_id\'] = gpu_id\n\n    graph_config[\'output_type\'] = \'softmax\'\n    graph_config[\'output_config\'] = dict(\n      output_vocab_size=de_vocab.size, use_logits=True)\n    aux_loss_list = [(\'ent_reg\', FLAGS.entropy_reg_coeff),]\n\n    if FLAGS.lm_loss_coeff > 0.0:\n      aux_loss_list.append((\'en_lm_loss\', FLAGS.lm_loss_coeff))\n    graph_config[\'train_config\'] = dict(\n      aux_loss_list=aux_loss_list,\n      learning_rate=FLAGS.learning_rate,\n      max_grad_norm=FLAGS.max_grad_norm,\n      adam_beta1=FLAGS.adam_beta1,\n      l2_coeff=FLAGS.l2_coeff,\n      optimizer=FLAGS.optimizer, avg_loss_by_n=False)\n\n    agent = create_agent(\n      graph_config, get_init_model_path(),\n      pretrained_embeddings=pretrained_embeddings)\n\n  with open(os.path.join(get_experiment_dir(), \'graph_config.json\'), \'w\') as f:\n    json.dump(graph_config, f, sort_keys=True, indent=2)\n    \n  return agent, envs\n\n\ndef compress_home_path(path):\n  home_folder = os.path.expanduser(\'~\')\n  n = len(home_folder)\n  if path[:n] == home_folder:\n    return \'~\' + path[n:]\n  else:\n    return path\n\n\ndef create_experiment_config():\n  experiment_config = get_saved_experiment_config()\n  if experiment_config:\n    FLAGS.embedding_file = os.path.expanduser(experiment_config[\'embedding_file\'])\n    FLAGS.vocab_file = os.path.expanduser(experiment_config[\'vocab_file\'])\n    FLAGS.en_vocab_file = os.path.expanduser(experiment_config[\'en_vocab_file\'])\n    FLAGS.table_file = os.path.expanduser(experiment_config[\'table_file\'])\n\n  experiment_config = {\n    \'embedding_file\': compress_home_path(FLAGS.embedding_file),\n    \'vocab_file\': compress_home_path(FLAGS.vocab_file),\n    \'en_vocab_file\': compress_home_path(FLAGS.en_vocab_file),\n    \'table_file\': compress_home_path(FLAGS.table_file)}\n\n  return experiment_config\n\n\ndef run_experiment():\n  print(\'=\' * 100)\n  if FLAGS.show_log:\n    tf.logging.set_verbosity(tf.logging.INFO)\n\n  experiment_dir = get_experiment_dir()\n  if tf.gfile.Exists(experiment_dir):\n    tf.gfile.DeleteRecursively(experiment_dir)\n  tf.gfile.MkDir(experiment_dir)\n\n  experiment_config = create_experiment_config()\n  \n  with open(os.path.join(\n      get_experiment_dir(), \'experiment_config.json\'), \'w\') as f:\n    json.dump(experiment_config, f)\n\n  ckpt_queue = multiprocessing.Queue()\n  train_queue = multiprocessing.Queue()\n  eval_queue = multiprocessing.Queue()\n  replay_queue = multiprocessing.Queue()\n\n  run_type = \'evaluation\' if FLAGS.eval_only else \'experiment\'\n  print(\'Start {} {}.\'.format(run_type, FLAGS.experiment_name))\n  print(\'The data of this {} is saved in {}.\'.format(run_type, experiment_dir))\n\n  if FLAGS.eval_only:\n    print(\'Start evaluating the best model {}.\'.format(get_init_model_path()))\n  else:\n    print(\'Start distributed training.\')\n\n  print(\'Start evaluator.\')\n  evaluator = Evaluator(\n    \'Evaluator\',\n    [FLAGS.eval_file if FLAGS.eval_only else FLAGS.dev_file])\n  evaluator.start()\n\n  if not FLAGS.eval_only:\n    actors = []\n    actor_shard_dict = dict([(i, []) for i in range(FLAGS.n_actors)])\n    for i in xrange(FLAGS.shard_start, FLAGS.shard_end):\n      actor_num = i % FLAGS.n_actors\n      actor_shard_dict[actor_num].append(i)\n\n    for k in xrange(FLAGS.n_actors):\n      name = \'actor_{}\'.format(k)\n      actor = Actor(\n        name, k, actor_shard_dict[k],\n        ckpt_queue, train_queue, eval_queue, replay_queue)\n      actors.append(actor)\n      actor.start()\n    print(\'Start {} actors.\'.format(len(actors)))\n    print(\'Start learner.\')\n    learner = Learner(\n      \'Learner\', [FLAGS.dev_file], ckpt_queue,\n      train_queue, eval_queue, replay_queue)\n    learner.start()\n    print(\'Use tensorboard to monitor the training progress (see README).\')\n    for actor in actors:\n      actor.join()\n    print(\'All actors finished\')\n    # Send learner the signal that all the actors have finished.\n    train_queue.put(None)\n    eval_queue.put(None)\n    replay_queue.put(None)\n    learner.join()\n    print(\'Learner finished\')\n\n  evaluator.join()\n  print(\'Evaluator finished\')\n  print(\'=\' * 100)\n\n\nclass Actor(multiprocessing.Process):\n    \n  def __init__(\n      self, name, actor_id, shard_ids, ckpt_queue, train_queue, eval_queue, replay_queue):\n    multiprocessing.Process.__init__(self)\n    self.ckpt_queue = ckpt_queue\n    self.eval_queue = eval_queue\n    self.train_queue = train_queue\n    self.replay_queue = replay_queue\n    self.name = name\n    self.shard_ids = shard_ids\n    self.actor_id = actor_id\n      \n  def run(self):\n    agent, envs = init_experiment(\n      [get_train_shard_path(i) for i in self.shard_ids],\n      use_gpu=FLAGS.actor_use_gpu,\n      gpu_id=str(self.actor_id + FLAGS.actor_gpu_start_id))\n    graph = agent.model.graph\n    current_ckpt = get_init_model_path()\n\n    env_dict = dict([(env.name, env) for env in envs])\n    replay_buffer = agent_factory.AllGoodReplayBuffer(agent, envs[0].de_vocab)\n\n    # Load saved programs to warm start the replay buffer. \n    if FLAGS.load_saved_programs:\n      load_programs(\n        envs, replay_buffer, FLAGS.saved_program_file)\n\n    if FLAGS.save_replay_buffer_at_end:\n      replay_buffer_copy = agent_factory.AllGoodReplayBuffer(de_vocab=envs[0].de_vocab)\n      replay_buffer_copy.program_prob_dict = copy.deepcopy(replay_buffer.program_prob_dict)\n\n    i = 0\n    while True:\n      # Create the logging files. \n      if FLAGS.log_samples_every_n_epoch > 0 and i % FLAGS.log_samples_every_n_epoch == 0:\n        f_replay = codecs.open(os.path.join(\n          get_experiment_dir(), \'replay_samples_{}_{}.txt\'.format(self.name, i)),\n                               \'w\', encoding=\'utf-8\')\n        f_policy = codecs.open(os.path.join(\n          get_experiment_dir(), \'policy_samples_{}_{}.txt\'.format(self.name, i)),\n                               \'w\', encoding=\'utf-8\')\n        f_train = codecs.open(os.path.join(\n          get_experiment_dir(), \'train_samples_{}_{}.txt\'.format(self.name, i)),\n                              \'w\', encoding=\'utf-8\')\n\n      n_train_samples = 0\n      if FLAGS.use_replay_samples_in_train:\n        n_train_samples += FLAGS.n_replay_samples\n\n      if FLAGS.use_policy_samples_in_train and FLAGS.use_nonreplay_samples_in_train:\n        raise ValueError(\n          \'Cannot use both on-policy samples and nonreplay samples for training!\')\n        \n      if FLAGS.use_policy_samples_in_train or FLAGS.use_nonreplay_samples_in_train:\n        # Note that nonreplay samples are drawn by rejection\n        # sampling from on-policy samples.\n        n_train_samples += FLAGS.n_policy_samples\n\n      # Make sure that all the samples from the env batch\n      # fits into one batch for training.\n      if FLAGS.batch_size < n_train_samples:\n        raise ValueError(\n            \'One batch have to at least contain samples from one environment.\')\n\n      env_batch_size = FLAGS.batch_size / n_train_samples\n      \n      env_iterator = data_utils.BatchIterator(\n        dict(envs=envs), shuffle=True,\n        batch_size=env_batch_size)\n\n      for j, batch_dict in enumerate(env_iterator):\n        batch_envs = batch_dict[\'envs\']\n        tf.logging.info(\'=\' * 50)\n        tf.logging.info(\'{} iteration {}, batch {}: {} envs\'.format(\n            self.name, i, j, len(batch_envs)))\n  \n        t1 = time.time()\n        # Generate samples with cache and save to replay buffer.\n        t3 = time.time()\n        n_explore = 0\n        for _ in xrange(FLAGS.n_explore_samples):\n          explore_samples = agent.generate_samples(\n            batch_envs, n_samples=1, use_cache=FLAGS.use_cache,\n            greedy=FLAGS.greedy_exploration)\n          replay_buffer.save(explore_samples)\n          n_explore += len(explore_samples)\n\n        if FLAGS.n_extra_explore_for_hard > 0:\n          hard_envs = [env for env in batch_envs\n                       if not replay_buffer.has_found_solution(env.name)]\n          if hard_envs:\n            for _ in xrange(FLAGS.n_extra_explore_for_hard):\n              explore_samples = agent.generate_samples(\n                hard_envs, n_samples=1, use_cache=FLAGS.use_cache,\n                greedy=FLAGS.greedy_exploration)\n              replay_buffer.save(explore_samples)\n              n_explore += len(explore_samples)\n\n        t4 = time.time()\n        tf.logging.info(\'{} sec used generating {} exploration samples.\'.format(\n          t4 - t3, n_explore))\n\n        tf.logging.info(\'{} samples saved in the replay buffer.\'.format(\n          replay_buffer.size))\n        \n        t3 = time.time()\n        replay_samples = replay_buffer.replay(\n          batch_envs, FLAGS.n_replay_samples,\n          use_top_k=FLAGS.use_top_k_replay_samples,\n          agent=None if FLAGS.random_replay_samples else agent,\n          truncate_at_n=FLAGS.truncate_replay_buffer_at_n)\n        t4 = time.time()\n        tf.logging.info(\'{} sec used selecting {} replay samples.\'.format(\n          t4 - t3, len(replay_samples)))\n          \n        t3 = time.time()\n        if FLAGS.use_top_k_policy_samples:\n          if FLAGS.n_policy_samples == 1:\n            policy_samples = agent.generate_samples(\n              batch_envs, n_samples=FLAGS.n_policy_samples,\n              greedy=True)\n          else:\n            policy_samples = agent.beam_search(\n              batch_envs, beam_size=FLAGS.n_policy_samples)\n        else:\n          policy_samples = agent.generate_samples(\n            batch_envs, n_samples=FLAGS.n_policy_samples,\n            greedy=False)\n        t4 = time.time()\n        tf.logging.info(\'{} sec used generating {} on-policy samples\'.format(\n          t4-t3, len(policy_samples)))\n\n        t2 = time.time()\n        tf.logging.info(\n          (\'{} sec used generating replay and on-policy samples,\'\n           \' {} iteration {}, batch {}: {} envs\').format(\n            t2-t1, self.name, i, j, len(batch_envs)))\n\n        t1 = time.time()\n        self.eval_queue.put((policy_samples, len(batch_envs)))\n        self.replay_queue.put((replay_samples, len(batch_envs)))\n\n        assert (FLAGS.fixed_replay_weight >= 0.0 and FLAGS.fixed_replay_weight <= 1.0)\n\n        if FLAGS.use_replay_prob_as_weight:\n          new_samples = []\n          for sample in replay_samples:\n            name = sample.traj.env_name\n            if name in replay_buffer.prob_sum_dict:\n              replay_prob = max(\n                replay_buffer.prob_sum_dict[name], FLAGS.min_replay_weight)\n            else:\n              replay_prob = 0.0\n            scale = replay_prob\n            new_samples.append(\n              agent_factory.Sample(\n                traj=sample.traj,\n                prob=sample.prob * scale))\n          replay_samples = new_samples\n        else:\n          replay_samples = agent_factory.scale_probs(\n            replay_samples, FLAGS.fixed_replay_weight)\n\n        replay_samples = sorted(\n          replay_samples, key=lambda x: x.traj.env_name)\n\n        policy_samples = sorted(\n          policy_samples, key=lambda x: x.traj.env_name)\n\n        if FLAGS.use_nonreplay_samples_in_train:\n          nonreplay_samples = []\n          for sample in policy_samples:\n            if not replay_buffer.contain(sample.traj):\n              nonreplay_samples.append(sample)\n\n        replay_buffer.save(policy_samples)\n\n        def weight_samples(samples):\n          if FLAGS.use_replay_prob_as_weight:\n            new_samples = []\n            for sample in samples:\n              name = sample.traj.env_name\n              if name in replay_buffer.prob_sum_dict:\n                replay_prob = max(\n                  replay_buffer.prob_sum_dict[name],\n                  FLAGS.min_replay_weight)\n              else:\n                replay_prob = 0.0\n              scale = 1.0 - replay_prob\n              new_samples.append(\n                agent_factory.Sample(\n                  traj=sample.traj,\n                  prob=sample.prob * scale))\n          else:\n            new_samples = agent_factory.scale_probs(\n              samples, 1 - FLAGS.fixed_replay_weight)\n          return new_samples\n\n        train_samples = []\n        if FLAGS.use_replay_samples_in_train:\n          if FLAGS.use_trainer_prob:\n            replay_samples = [\n              sample._replace(prob=None) for sample in replay_samples]\n          train_samples += replay_samples\n\n        if FLAGS.use_policy_samples_in_train:\n          train_samples += weight_samples(policy_samples)\n\n        if FLAGS.use_nonreplay_samples_in_train:\n          train_samples += weight_samples(nonreplay_samples)\n        \n        train_samples = sorted(train_samples, key=lambda x: x.traj.env_name)\n        tf.logging.info(\'{} train samples\'.format(len(train_samples)))\n\n        if FLAGS.use_importance_sampling:\n          step_logprobs = agent.compute_step_logprobs(\n            [s.traj for s in train_samples])\n        else:\n          step_logprobs = None\n\n        if FLAGS.use_replay_prob_as_weight:\n          n_clip = 0\n          for env in batch_envs:\n            name = env.name\n            if (name in replay_buffer.prob_sum_dict and\n                replay_buffer.prob_sum_dict[name] < FLAGS.min_replay_weight):\n              n_clip += 1\n          clip_frac = float(n_clip) / len(batch_envs)\n        else:\n          clip_frac = 0.0\n  \n        self.train_queue.put((train_samples, step_logprobs, clip_frac))\n        t2 = time.time()\n        tf.logging.info(\n          (\'{} sec used preparing and enqueuing samples, {}\'\n           \' iteration {}, batch {}: {} envs\').format(\n             t2-t1, self.name, i, j, len(batch_envs)))\n\n        t1 = time.time()\n        # Wait for a ckpt that still exist or it is the same\n        # ckpt (no need to load anything).\n        while True:\n          new_ckpt = self.ckpt_queue.get()\n          new_ckpt_file = new_ckpt + \'.meta\'\n          if new_ckpt == current_ckpt or tf.gfile.Exists(new_ckpt_file):\n            break\n        t2 = time.time()\n        tf.logging.info(\'{} sec waiting {} iteration {}, batch {}\'.format(\n          t2-t1, self.name, i, j))\n\n        if new_ckpt != current_ckpt:\n          # If the ckpt is not the same, then restore the new\n          # ckpt.\n          tf.logging.info(\'{} loading ckpt {}\'.format(self.name, new_ckpt))\n          t1 = time.time()          \n          graph.restore(new_ckpt)\n          t2 = time.time()\n          tf.logging.info(\'{} sec used {} restoring ckpt {}\'.format(\n            t2-t1, self.name, new_ckpt))\n          current_ckpt = new_ckpt\n\n        if FLAGS.log_samples_every_n_epoch > 0 and i % FLAGS.log_samples_every_n_epoch == 0:\n          f_replay.write(show_samples(replay_samples, envs[0].de_vocab, env_dict))\n          f_policy.write(show_samples(policy_samples, envs[0].de_vocab, env_dict))\n          f_train.write(show_samples(train_samples, envs[0].de_vocab, env_dict))\n\n      if FLAGS.log_samples_every_n_epoch > 0 and i % FLAGS.log_samples_every_n_epoch == 0:\n        f_replay.close()\n        f_policy.close()\n        f_train.close()\n\n      if agent.model.get_global_step() >= FLAGS.n_steps:\n        if FLAGS.save_replay_buffer_at_end:\n          all_replay = os.path.join(get_experiment_dir(),\n                                    \'all_replay_samples_{}.txt\'.format(self.name))\n          with codecs.open(all_replay, \'w\', encoding=\'utf-8\') as f:\n            samples = replay_buffer.all_samples(envs, agent=None)\n            samples = [s for s in samples if not replay_buffer_copy.contain(s.traj)]\n            f.write(show_samples(samples, envs[0].de_vocab, None))\n        tf.logging.info(\'{} finished\'.format(self.name))\n        return\n      i += 1\n\n\ndef select_top(samples):\n  top_dict = {}\n  for sample in samples:\n    name = sample.traj.env_name\n    prob = sample.prob\n    if name not in top_dict or prob > top_dict[name].prob:\n      top_dict[name] = sample    \n  return agent_factory.normalize_probs(top_dict.values())\n\n\ndef beam_search_eval(agent, envs, writer=None):\n    env_batch_size = FLAGS.eval_batch_size\n    env_iterator = data_utils.BatchIterator(\n      dict(envs=envs), shuffle=False,\n      batch_size=env_batch_size)\n    dev_samples = []\n    dev_samples_in_beam = []\n    for j, batch_dict in enumerate(env_iterator):\n      t1 = time.time()\n      batch_envs = batch_dict[\'envs\']\n      tf.logging.info(\'=\' * 50)\n      tf.logging.info(\'eval, batch {}: {} envs\'.format(j, len(batch_envs)))\n      new_samples_in_beam = agent.beam_search(\n        batch_envs, beam_size=FLAGS.eval_beam_size)\n      dev_samples_in_beam += new_samples_in_beam\n      tf.logging.info(\'{} samples in beam, batch {}.\'.format(\n        len(new_samples_in_beam), j))\n      t2 = time.time()\n      tf.logging.info(\'{} sec used in evaluator batch {}.\'.format(t2 - t1, j))\n\n    # Account for beam search where the beam doesn\'t\n    # contain any examples without error, which will make\n    # len(dev_samples) smaller than len(envs).\n    dev_samples = select_top(dev_samples_in_beam)\n    dev_avg_return, dev_avg_len = agent.evaluate(\n      dev_samples, writer=writer, true_n=len(envs))\n    tf.logging.info(\'{} samples in non-empty beam.\'.format(len(dev_samples)))\n    tf.logging.info(\'true n is {}\'.format(len(envs)))\n    tf.logging.info(\'{} questions in dev set.\'.format(len(envs)))\n    tf.logging.info(\'{} dev avg return.\'.format(dev_avg_return))\n    tf.logging.info(\'dev: avg return: {}, avg length: {}.\'.format(\n      dev_avg_return, dev_avg_len))\n\n    return dev_avg_return, dev_samples, dev_samples_in_beam\n\n\nclass Evaluator(multiprocessing.Process):\n    \n  def __init__(self, name, fns):\n    multiprocessing.Process.__init__(self)\n    self.name = name\n    self.fns = fns\n\n  def run(self):\n    agent, envs = init_experiment(self.fns, FLAGS.eval_use_gpu, gpu_id=str(FLAGS.eval_gpu_id))\n    for env in envs:\n      env.punish_extra_work = False\n    graph = agent.model.graph\n    dev_writer = tf.summary.FileWriter(os.path.join(\n      get_experiment_dir(), FLAGS.tb_log_dir, \'dev\'))\n    best_dev_avg_return = 0.0\n    best_model_path = \'\'\n    best_model_dir = os.path.join(get_experiment_dir(), FLAGS.best_model_dir)\n    if not tf.gfile.Exists(best_model_dir):\n      tf.gfile.MkDir(best_model_dir)\n    i = 0\n    current_ckpt = get_init_model_path()\n    env_dict = dict([(env.name, env) for env in envs])\n    while True:\n      t1 = time.time()\n      tf.logging.info(\'dev: iteration {}, evaluating {}.\'.format(i, current_ckpt))\n\n      dev_avg_return, dev_samples, dev_samples_in_beam = beam_search_eval(\n        agent, envs, writer=dev_writer)\n      \n      if dev_avg_return > best_dev_avg_return:\n        best_model_path = graph.save_best(\n          os.path.join(best_model_dir, \'model\'),\n          agent.model.get_global_step())\n        best_dev_avg_return = dev_avg_return\n        tf.logging.info(\'New best dev avg returns is {}\'.format(best_dev_avg_return))\n        tf.logging.info(\'New best model is saved in {}\'.format(best_model_path))\n        with open(os.path.join(get_experiment_dir(), \'best_model_info.json\'), \'w\') as f:\n          result = {\'best_model_path\': compress_home_path(best_model_path)}\n          if FLAGS.eval_only:\n            result[\'best_eval_avg_return\'] = best_dev_avg_return\n          else:\n            result[\'best_dev_avg_return\'] = best_dev_avg_return\n          json.dump(result, f)\n\n      if FLAGS.eval_only:\n        # Save the decoding results for further. \n        dev_programs_in_beam_dict = {}\n        for sample in dev_samples_in_beam:\n          name = sample.traj.env_name\n          program = agent_factory.traj_to_program(sample.traj, envs[0].de_vocab)\n          answer = sample.traj.answer\n          reward = sum(sample.traj.rewards)\n          if name in dev_programs_in_beam_dict:\n            dev_programs_in_beam_dict[name].append((program, answer, sample.prob, reward))\n          else:\n            dev_programs_in_beam_dict[name] = [(program, answer, sample.prob, reward)]\n\n        t3 = time.time()\n        with open(\n            os.path.join(get_experiment_dir(), \'dev_programs_in_beam_{}.json\'.format(i)),\n            \'w\') as f:\n          json.dump(dev_programs_in_beam_dict, f)\n        t4 = time.time()\n        tf.logging.info(\'{} sec used dumping programs in beam in eval iteration {}.\'.format(\n          t4 - t3, i))\n\n        t3 = time.time()\n        with codecs.open(\n            os.path.join(\n              get_experiment_dir(), \'dev_samples_{}.txt\'.format(i)),\n            \'w\', encoding=\'utf-8\') as f:\n          for sample in dev_samples:\n            f.write(show_samples([sample], envs[0].de_vocab, env_dict))\n        t4 = time.time()\n        tf.logging.info(\'{} sec used logging dev samples in eval iteration {}.\'.format(\n          t4 - t3, i))\n\n      t2 = time.time()\n      tf.logging.info(\'{} sec used in eval iteration {}.\'.format(\n        t2 - t1, i))\n\n      if FLAGS.eval_only or agent.model.get_global_step() >= FLAGS.n_steps:\n        tf.logging.info(\'{} finished\'.format(self.name))\n        if FLAGS.eval_only:\n          print(\'Eval average return (accuracy) of the best model is {}\'.format(\n            best_dev_avg_return))\n        else:\n          print(\'Best dev average return (accuracy) is {}\'.format(best_dev_avg_return))\n          print(\'Best model is saved in {}\'.format(best_model_path))\n        return\n\n      # Reload on the latest model.\n      new_ckpt = None\n      t1 = time.time()\n      while new_ckpt is None or new_ckpt == current_ckpt:\n        time.sleep(1)\n        new_ckpt = tf.train.latest_checkpoint(\n          os.path.join(get_experiment_dir(), FLAGS.saved_model_dir))\n      t2 = time.time()\n      tf.logging.info(\'{} sec used waiting for new checkpoint in evaluator.\'.format(\n        t2-t1))\n      \n      tf.logging.info(\'lastest ckpt to evaluate is {}.\'.format(new_ckpt))\n      tf.logging.info(\'{} loading ckpt {}\'.format(self.name, new_ckpt))\n      t1 = time.time()\n      graph.restore(new_ckpt)\n      t2 = time.time()\n      tf.logging.info(\'{} sec used {} loading ckpt {}\'.format(\n        t2-t1, self.name, new_ckpt))\n      current_ckpt = new_ckpt\n\n\nclass Learner(multiprocessing.Process):\n    \n  def __init__(\n      self, name, fns, ckpt_queue,\n      train_queue, eval_queue, replay_queue):\n    multiprocessing.Process.__init__(self)\n    self.ckpt_queue = ckpt_queue\n    self.eval_queue = eval_queue\n    self.train_queue = train_queue\n    self.replay_queue = replay_queue\n    self.name = name\n    self.save_every_n = FLAGS.save_every_n\n    self.fns = fns\n      \n  def run(self):\n    # Writers to record training and replay information.\n    train_writer = tf.summary.FileWriter(os.path.join(\n      get_experiment_dir(), FLAGS.tb_log_dir, \'train\'))\n    replay_writer = tf.summary.FileWriter(os.path.join(\n      get_experiment_dir(), FLAGS.tb_log_dir, \'replay\'))\n    saved_model_dir = os.path.join(get_experiment_dir(), FLAGS.saved_model_dir)\n    if not tf.gfile.Exists(saved_model_dir):\n      tf.gfile.MkDir(saved_model_dir)\n    agent, envs = init_experiment(self.fns, FLAGS.train_use_gpu, gpu_id=str(FLAGS.train_gpu_id))\n    agent.train_writer = train_writer\n    graph = agent.model.graph\n    current_ckpt = get_init_model_path()\n\n    i = 0\n    n_save = 0\n    while True:\n      tf.logging.info(\'Start train step {}\'.format(i))\n      t1 = time.time()\n      train_samples, behaviour_logprobs, clip_frac  = self.train_queue.get()\n      eval_samples, eval_true_n = self.eval_queue.get()\n      replay_samples, replay_true_n = self.replay_queue.get()\n      t2 = time.time()\n      tf.logging.info(\'{} secs used waiting in train step {}.\'.format(\n        t2-t1, i))\n      t1 = time.time()\n      n_train_samples = 0\n      if FLAGS.use_replay_samples_in_train:\n        n_train_samples += FLAGS.n_replay_samples\n      if FLAGS.use_policy_samples_in_train and FLAGS.use_nonreplay_samples_in_train:\n        raise ValueError(\n          \'Cannot use both on-policy samples and nonreplay samples for training!\')\n      if FLAGS.use_policy_samples_in_train:\n        n_train_samples += FLAGS.n_policy_samples\n\n      if train_samples:\n        if FLAGS.use_trainer_prob:\n          train_samples = agent.update_replay_prob(\n            train_samples, min_replay_weight=FLAGS.min_replay_weight)\n        for _ in xrange(FLAGS.n_opt_step):\n          agent.train(\n            train_samples,\n            parameters=dict(en_rnn_dropout=FLAGS.dropout,rnn_dropout=FLAGS.dropout),\n            use_baseline=FLAGS.use_baseline,\n            min_prob=FLAGS.min_prob,\n            scale=n_train_samples,\n            behaviour_logprobs=behaviour_logprobs,\n            use_importance_sampling=FLAGS.use_importance_sampling,\n            ppo_epsilon=FLAGS.ppo_epsilon,\n            de_vocab=envs[0].de_vocab,\n            debug=FLAGS.debug)\n\n      avg_return, avg_len = agent.evaluate(\n        eval_samples, writer=train_writer, true_n=eval_true_n,\n        clip_frac=clip_frac)\n      tf.logging.info(\'train: avg return: {}, avg length: {}.\'.format(\n        avg_return, avg_len))\n      avg_return, avg_len = agent.evaluate(\n        replay_samples, writer=replay_writer, true_n=replay_true_n)\n      tf.logging.info(\'replay: avg return: {}, avg length: {}.\'.format(avg_return, avg_len))\n      t2 = time.time()\n      tf.logging.info(\'{} sec used in training train iteration {}, {} samples.\'.format(\n        t2-t1, i, len(train_samples)))\n      i += 1\n      if i % self.save_every_n == 0:\n        t1 = time.time()\n        current_ckpt = graph.save(\n          os.path.join(saved_model_dir, \'model\'),\n          agent.model.get_global_step())\n        t2 = time.time()\n        tf.logging.info(\'{} sec used saving model to {}, train iteration {}.\'.format(\n          t2-t1, current_ckpt, i))\n        self.ckpt_queue.put(current_ckpt)\n        if agent.model.get_global_step() >= FLAGS.n_steps:\n          t1 = time.time()\n          while True:\n            train_data = self.train_queue.get()\n            _ = self.eval_queue.get()\n            _ = self.replay_queue.get()\n            self.ckpt_queue.put(current_ckpt)\n            # Get the signal that all the actors have\n            # finished.\n            if train_data is None:\n              t2 = time.time()\n              tf.logging.info(\'{} finished, {} sec used waiting for actors\'.format(\n                self.name, t2-t1))\n              return\n      else:\n        # After training on one set of samples, put one ckpt\n        # back so that the ckpt queue is always full.\n        self.ckpt_queue.put(current_ckpt)\n\n\ndef main(unused_argv):\n  run_experiment()\n\n\nif __name__ == \'__main__\':  \n    tf.app.run()\n'"
table/random_explore.py,31,"b'import json\nimport time\nimport os\nimport random\nimport multiprocessing\n\nimport numpy as np\nimport tensorflow as tf\n\nimport nsm\nfrom nsm import data_utils\nfrom nsm import env_factory\nfrom nsm import graph_factory\nfrom nsm import model_factory\nfrom nsm import agent_factory\nfrom nsm import executor_factory\nfrom nsm import computer_factory\n\nimport utils\n\n\n# FLAGS\nFLAGS = tf.app.flags.FLAGS  \n\n# Experiment name\ntf.flags.DEFINE_string(\'output_dir\', \'\', \'output directory\')\ntf.flags.DEFINE_string(\'experiment_name\', \'experiment\',\n                       \'All outputs of this experiment is\'\n                       \' saved under a folder with the same name.\')\ntf.app.flags.DEFINE_integer(\n  \'n_epoch\', 1000, \'Max number of valid tokens during decoding.\')\n\n\n# Data\ntf.app.flags.DEFINE_string(\n  \'table_file\', \'\', \'.\')\ntf.app.flags.DEFINE_string(\n  \'train_file_tmpl\', \'\', \'Path to the file of training examples, a jsonl file.\')\n\n\n# Model\n## Computer\ntf.app.flags.DEFINE_integer(\n  \'max_n_mem\', 100, \'Max number of memory slots in the ""computer"".\')\ntf.app.flags.DEFINE_integer(\n  \'max_n_exp\', 3, \'Max number of expressions allowed in a program.\')\ntf.app.flags.DEFINE_integer(\n  \'max_n_valid_indices\', 100, \'Max number of valid tokens during decoding.\')\ntf.app.flags.DEFINE_string(\n  \'executor\', \'wtq\', \'Which executor to use, wtq or wikisql.\')\n\n\n# Exploration\ntf.app.flags.DEFINE_integer(\n  \'n_explore_samples\', 50, \'.\')\ntf.app.flags.DEFINE_integer(\'save_every_n\', 10, \'.\')\ntf.app.flags.DEFINE_integer(\'id_start\', 0, \'.\')\ntf.app.flags.DEFINE_integer(\'id_end\', 0, \'.\')\n\ntf.app.flags.DEFINE_string(\n  \'trigger_word_file\', \'\', \'.\')\n\ntf.app.flags.DEFINE_integer(\'n_process\', -1, \'.\')\n\ntf.logging.set_verbosity(tf.logging.INFO)\n\n\ndef get_experiment_dir():\n  experiment_dir = os.path.join(FLAGS.output_dir, FLAGS.experiment_name)\n  return experiment_dir\n\n\ndef random_explore(env, use_cache=True, trigger_dict=None):\n  env = env.clone()\n  env.use_cache = use_cache\n  question_tokens = env.question_annotation[\'tokens\']\n  if \'pos_tags\' in env.question_annotation:\n    pos_tags = env.question_annotation[\'pos_tags\']\n    tokens = question_tokens + pos_tags\n  else:\n    tokens = question_tokens\n  invalid_functions = []\n  if trigger_dict is not None:\n    for function, trigger_words in trigger_dict.iteritems():\n      if not set(trigger_words) & set(tokens):\n        invalid_functions.append(function)\n  ob = env.start_ob\n  while not env.done:\n    invalid_actions = env.de_vocab.lookup(invalid_functions)\n    valid_actions = ob[0].valid_indices\n    new_valid_actions = list(set(valid_actions) - set(invalid_actions))\n    # No action available anymore. \n    if len(new_valid_actions) <= 0:\n      return None\n    new_action = np.random.randint(0, len(new_valid_actions))\n    action = valid_actions.index(new_valid_actions[new_action])\n    ob, _, _, _ = env.step(action)\n\n  if sum(env.rewards) >= 1.0:\n    return env.de_vocab.lookup(env.mapped_actions, reverse=True)\n  else:\n    return None\n\n\ndef run_random_exploration(shard_id):\n  experiment_dir = get_experiment_dir()\n  if not tf.gfile.Exists(experiment_dir):\n    tf.gfile.MkDir(experiment_dir)\n\n  if FLAGS.trigger_word_file:\n    with open(FLAGS.trigger_word_file, \'r\') as f:\n      trigger_dict = json.load(f)\n      print \'use trigger words in {}\'.format(FLAGS.trigger_word_file)\n  else:\n    trigger_dict = None\n\n  # Load dataset.\n  train_set = []\n  with open(FLAGS.train_file_tmpl.format(shard_id), \'r\') as f:\n    for line in f:\n      example = json.loads(line)\n      train_set.append(example)\n  tf.logging.info(\'{} examples in training set.\'.format(len(train_set)))\n    \n  table_dict = {}\n  with open(FLAGS.table_file) as f:\n    for line in f:\n      table = json.loads(line)\n      table_dict[table[\'name\']] = table\n  tf.logging.info(\'{} tables.\'.format(len(table_dict)))\n\n  if FLAGS.executor == \'wtq\':\n    score_fn = utils.wtq_score\n    process_answer_fn = lambda x: x\n    executor_fn = executor_factory.WikiTableExecutor\n  elif FLAGS.executor == \'wikisql\':\n    score_fn = utils.wikisql_score\n    process_answer_fn = utils.wikisql_process_answer\n    executor_fn = executor_factory.WikiSQLExecutor\n  else:\n    raise ValueError(\'Unknown executor {}\'.format(FLAGS.executor))\n  \n  all_envs = []\n  t1 = time.time()\n  for i, example in enumerate(train_set):\n      if i % 100 == 0:\n        tf.logging.info(\'creating environment #{}\'.format(i))\n      kg_info = table_dict[example[\'context\']]\n      executor = executor_fn(kg_info)\n      api = executor.get_api()\n      type_hierarchy = api[\'type_hierarchy\']\n      func_dict = api[\'func_dict\']\n      constant_dict = api[\'constant_dict\']\n      interpreter = computer_factory.LispInterpreter(\n        type_hierarchy=type_hierarchy, \n        max_mem=FLAGS.max_n_mem, max_n_exp=FLAGS.max_n_exp, assisted=True)\n      for v in func_dict.values():\n        interpreter.add_function(**v)\n        \n      interpreter.add_constant(\n        value=kg_info[\'row_ents\'], type=\'entity_list\', name=\'all_rows\')\n\n      de_vocab = interpreter.get_vocab()\n      env = env_factory.QAProgrammingEnv(\n        data_utils.Vocab([]), de_vocab, question_annotation=example,\n        answer=process_answer_fn(example[\'answer\']),\n        constants=constant_dict.values(),\n        interpreter=interpreter,\n        constant_value_embedding_fn=lambda x: None,\n        score_fn=score_fn,\n        max_cache_size=FLAGS.n_explore_samples * FLAGS.n_epoch * 10,\n        name=example[\'id\'])\n      all_envs.append(env)\n\n  program_dict = dict([(env.name, []) for env in all_envs])\n  for i in xrange(1, FLAGS.n_epoch + 1):\n    tf.logging.info(\'iteration {}\'.format(i))\n    t1 = time.time()\n    for env in all_envs:\n      for _ in xrange(FLAGS.n_explore_samples):\n        program = random_explore(env, trigger_dict=trigger_dict)\n        if program is not None:\n          program_dict[env.name].append(program)\n    t2 = time.time()\n    tf.logging.info(\'{} sec used in iteration {}\'.format(t2 - t1, i))\n\n    if i % FLAGS.save_every_n == 0:\n      tf.logging.info(\'saving programs and cache in iteration {}\'.format(i))\n      t1 = time.time()\n      with open(os.path.join(\n          get_experiment_dir(), \'program_shard_{}-{}.json\'.format(shard_id, i)), \'w\') as f:\n        program_str_dict = dict([(k, [\' \'.join(p) for p in v]) for k, v\n                                 in program_dict.iteritems()])\n        json.dump(program_str_dict, f, sort_keys=True, indent=2)\n\n      # cache_dict = dict([(env.name, list(env.cache._set)) for env in all_envs])\n      t2 = time.time()\n      tf.logging.info(\n        \'{} sec used saving programs and cache in iteration {}\'.format(\n          t2 - t1, i))\n\n    n = len(all_envs)\n    solution_ratio = len([env for env in all_envs if program_dict[env.name]]) * 1.0 / n\n    tf.logging.info(\n      \'At least one solution found ratio: {}\'.format(solution_ratio))\n    n_programs_per_env = np.array([len(program_dict[env.name]) for env in all_envs])\n    tf.logging.info(\n      \'number of solutions found per example: max {}, min {}, avg {}, std {}\'.format(\n        n_programs_per_env.max(), n_programs_per_env.min(), n_programs_per_env.mean(),\n        n_programs_per_env.std()))\n\n    # Macro average length.\n    mean_length = np.mean([np.mean([len(p) for p in program_dict[env.name]]) for env in all_envs\n                           if program_dict[env.name]])\n    tf.logging.info(\'macro average program length: {}\'.format(\n      mean_length))\n    # avg_cache_size = sum([len(env.cache._set) for env in all_envs]) * 1.0 / len(all_envs)\n    # tf.logging.info(\'average cache size: {}\'.format(\n    #  avg_cache_size))\n\n\ndef collect_programs():\n  saved_programs = {}\n  for i in xrange(FLAGS.id_start, FLAGS.id_end):\n    with open(os.path.join(\n        get_experiment_dir(),\n        \'program_shard_{}-{}.json\'.format(i, FLAGS.n_epoch)), \'r\') as f:\n      program_shard = json.load(f)\n      saved_programs.update(program_shard)\n  saved_program_path = os.path.join(get_experiment_dir(), \'saved_programs.json\')\n  with open(saved_program_path, \'w\') as f:\n    json.dump(saved_programs, f)\n  print \'saved programs are aggregated in {}\'.format(saved_program_path)\n\n\ndef main(unused_argv):\n  ps = []\n  for idx in xrange(FLAGS.id_start, FLAGS.id_end):\n    p = multiprocessing.Process(target=run_random_exploration, args=(idx,))\n    p.start()\n    ps.append(p)\n  for p in ps:\n    p.join()\n  collect_programs()\n\n\nif __name__ == \'__main__\':  \n  tf.app.run()\n\n'"
table/utils.py,0,"b'import re\nimport numpy as np\n\nimport babel\nfrom babel import numbers\nfrom wtq import evaluator\n\n\ndef average_token_embedding(tks, model, embedding_size=300):\n  arrays = []\n  for tk in tks:\n    if tk in model:\n      arrays.append(model[tk])\n    else:\n      arrays.append(np.zeros(embedding_size))\n  return np.average(np.vstack(arrays), axis=0)\n\n\ndef get_embedding_for_constant(value, model, embedding_size=300):\n  if isinstance(value, list):\n    # Use zero embeddings for values from the question to\n    # avoid overfitting\n    return np.zeros(embedding_size)\n  elif value[:2] == \'r.\':\n    value_split = value.split(\'-\')\n    type_str = value_split[-1]\n    type_embedding = average_token_embedding([type_str], model)\n    value_split = value_split[:-1]\n    value = \'-\'.join(value_split)\n    raw_tks = value[2:].split(\'_\')\n    tks = []\n    for tk in raw_tks:\n      valid_tks = find_tk_in_model(tk, model)\n      tks += valid_tks\n    val_embedding = average_token_embedding(tks or raw_tks, model)\n    return (val_embedding + type_embedding) / 2\n  else:\n    raise NotImplementedError(\'Unexpected value: {}\'.format(value))\n\n\ndef find_tk_in_model(tk, model):\n    special_tk_dict = {\'-lrb-\': \'(\', \'-rrb-\': \')\'}\n    if tk in model:\n        return [tk]\n    elif tk in special_tk_dict:\n        return [special_tk_dict[tk]]\n    elif tk.upper() in model:\n        return [tk.upper()]\n    elif tk[:1].upper() + tk[1:] in model:\n        return [tk[:1].upper() + tk[1:]]\n    elif re.search(\'\\\\/\', tk):\n        tks = tk.split(\'\\\\\\\\/\')\n        if len(tks) == 1:\n          return []\n        valid_tks = []\n        for tk in tks:\n            valid_tk = find_tk_in_model(tk, model)\n            if valid_tk:\n                valid_tks += valid_tk\n        return valid_tks\n    else:\n        return []\n\n\n# WikiSQL evaluation utility functions.\ndef wikisql_normalize(val):\n  """"""Normalize the val for wikisql experiments.""""""\n  if (isinstance(val, float) or isinstance(val, int)):\n    return val\n  elif isinstance(val, str) or isinstance(val, unicode):\n    try:\n      val = babel.numbers.parse_decimal(val)\n    except (babel.numbers.NumberFormatError, UnicodeEncodeError): \n      val = val.lower()\n    return val\n  else:\n    return None\n\n\ndef wikisql_process_answer(answer):\n  processed_answer = []\n  for a in answer:\n    normalized_val = wikisql_normalize(a)\n    # Ignore None value and normalize the rest, keep the\n    # order.\n    if normalized_val is not None:\n      processed_answer.append(normalized_val)\n  return processed_answer\n\n\ndef wikisql_score(prediction, answer):\n  prediction = wikisql_process_answer(prediction)\n  if prediction == answer:\n    return 1.0\n  else:\n    return 0.0\n\n\n# WikiTableQuestions evaluation function.\ndef wtq_score(prediction, answer):\n    processed_answer = evaluator.target_values_map(*answer)\n    correct = evaluator.check_prediction(\n      [unicode(p) for p in prediction], processed_answer)\n    if correct:\n        return 1.0\n    else:\n        return 0.0    \n'"
table/wikisql/preprocess.py,8,"b'# -*- coding: utf-8 -*-\nimport json\nimport babel\nfrom babel import numbers\nfrom babel.numbers import parse_decimal, NumberFormatError\n\nimport nltk\nimport os\nimport unicodedata\nimport re\nimport time\n\nimport nsm\nfrom nsm import word_embeddings\nfrom nsm import data_utils\n\nimport tensorflow as tf\n\n# FLAGS\nFLAGS = tf.app.flags.FLAGS  \ntf.flags.DEFINE_string(\'raw_input_dir\', \'\',\n                       \'path.\')\ntf.flags.DEFINE_string(\'processed_input_dir\', \'\',\n                       \'path to the folder to save all the processed data.\')\ntf.flags.DEFINE_integer(\'n_train_shard\', 30, \'.\')\n\n\n# Copied from utils to avoid relative import.\n# [TODO] Use a cleaner solution.\ndef find_tk_in_model(tk, model):\n    special_tk_dict = {\'-lrb-\': \'(\', \'-rrb-\': \')\'}\n    if tk in model:\n        return [tk]\n    elif tk in special_tk_dict:\n        return [special_tk_dict[tk]]\n    elif tk.upper() in model:\n        return [tk.upper()]\n    elif tk[:1].upper() + tk[1:] in model:\n        return [tk[:1].upper() + tk[1:]]\n    elif re.search(\'\\\\/\', tk):\n        tks = tk.split(\'\\\\\\\\/\')\n        if len(tks) == 1:\n          return []\n        valid_tks = []\n        for tk in tks:\n            valid_tk = find_tk_in_model(tk, model)\n            if valid_tk:\n                valid_tks += valid_tk\n        return valid_tks\n    else:\n        return []\n\n\ndef tokens_contain(string_1, string_2):\n    tks_1 = nltk.tokenize.word_tokenize(string_1)\n    tks_2 = nltk.tokenize.word_tokenize(string_2)\n    return set(tks_2).issubset(set(tks_1))\n\n\n# preprocess the questions.\ndef string_in_table(string, kg):\n    # print string\n    for k, node in kg[\'kg\'].iteritems():\n        for prop, val in node.iteritems():\n            if (isinstance(val[0], str) or isinstance(val[0], unicode)) and string in val[0]:\n                return True\n    else:\n        return False\n\n\ndef num_in_table(ent, kg):\n    props_set = set(kg[\'num_props\'])\n    for k, node in kg[\'kg\'].iteritems():\n        for prop, val in node.iteritems():\n            if prop in props_set and ent == val:\n                return True\n    else:\n        return False        \n\n\ndef prop_in_question_score(prop, example, stop_words, binary=True):\n    question = example[\'question\']\n    prop = prop[2:]\n    prop = u\'-\'.join(prop.split(u\'-\')[:-1])\n    prop_tks = prop.split(u\'_\')\n    n_in_question = 0\n    for tk in prop_tks:\n        # tk = tk.decode(\'utf-8\')\n        if tk not in stop_words and tk in question:\n            n_in_question += 1\n    if binary:\n        n_in_question = min(n_in_question, 1)\n    return n_in_question\n\n\ndef expand_entities(e, table_dict):\n    ents = [ent for ent in e[\'entities\']\n            if ent[\'type\'] == \'string_list\' and ent[\'value\'][0]]\n    other_ents = [ent for ent in e[\'entities\'] if ent[\'type\'] != \'string_list\']\n    kg = table_dict[e[\'context\']]\n    l = len(ents)\n    new_ents = []\n    i = 0\n    tokens = e[\'tokens\']\n    for ent in ents:\n        # relies on order. \n        if new_ents and ent[\'token_end\'] <= new_ents[-1][\'token_end\']:\n            continue\n        else:\n            ent[\'value\'][0] = tokens[ent[\'token_start\']]\n            new_ents.append(ent)\n            while True and ent[\'token_end\'] < len(tokens):\n                new_str_list = (\n                  [s.join([to_unicode(ent[\'value\'][0]),\n                           to_unicode(tokens[ent[\'token_end\']])]) \n                   for s in [u\' \', u\'-\', u\'\']] + \n                  [s.join([to_unicode(ent[\'value\'][0]),\n                           to_unicode(normalize(tokens[ent[\'token_end\']]))])\n                   for s in [u\' \', u\'-\', u\'\']] + \n                  [s.join([to_unicode(normalize(ent[\'value\'][0])),\n                           to_unicode(tokens[ent[\'token_end\']])]) \n                   for s in [u\' \', u\'-\', u\'\']] + \n                  [s.join([to_unicode(normalize(ent[\'value\'][0])),\n                           to_unicode(normalize(tokens[ent[\'token_end\']]))])\n                   for s in [u\' \', u\'-\', u\'\']])\n                for new_str in new_str_list:\n                  if string_in_table(new_str, kg):\n                    ent[\'token_end\'] += 1\n                    ent[\'value\'] = [new_str]\n                    break\n                else:\n                  break\n            ent[\'value\'] = [normalize(ent[\'value\'][0])]\n    e[\'entities\'] = new_ents + other_ents        \n\n\ndef to_unicode(tk, encoding=\'utf-8\'):\n  if isinstance(tk, str):\n    return tk.decode(encoding)\n  else:\n    return tk\n\n\ndef normalize(x):\n    if not isinstance(x, unicode):\n        x = x.decode(\'utf8\', errors=\'ignore\')\n    # Remove diacritics\n    x = \'\'.join(c for c in unicodedata.normalize(\'NFKD\', x)\n                if unicodedata.category(c) != \'Mn\')\n    # Normalize quotes and dashes\n    x = re.sub(ur""[\xe2\x80\x98\xe2\x80\x99\xc2\xb4`]"", ""\'"", x)\n    x = re.sub(ur""[\xe2\x80\x9c\xe2\x80\x9d]"", ""\\"""", x)\n    x = re.sub(ur""[\xe2\x80\x90\xe2\x80\x91\xe2\x80\x92\xe2\x80\x93\xe2\x80\x94\xe2\x88\x92]"", ""-"", x)\n    while True:\n        old_x = x\n        # Remove citations\n        x = re.sub(ur""((?<!^)\\[[^\\]]*\\]|\\[\\d+\\]|[\xe2\x80\xa2\xe2\x99\xa6\xe2\x80\xa0\xe2\x80\xa1*#+])*$"", """", x.strip())\n        # Remove details in parenthesis\n        x = re.sub(ur""(?<!^)( \\([^)]*\\))*$"", """", x.strip())\n        # Remove outermost quotation mark\n        x = re.sub(ur\'^""([^""]*)""$\', r\'\\1\', x.strip())\n        if x == old_x:\n            break\n    # Remove final \'.\'\n    if x and x[-1] == \'.\':\n        x = x[:-1]\n    # Collapse whitespaces and convert to lower case\n    x = re.sub(ur\'\\s+\', \' \', x, flags=re.U).lower().strip()\n    return x\n        \n\ndef process_prop(prop):\n    return prop.replace(\' \', \'_\').lower()\n\n\ndef table2kg(table):\n    tkg = {}\n    tkg[u\'datetime_props\'] = []\n    num_props = []\n    props = []\n    for p, tp in zip(table[\'header\'], table[\'types\']):\n        prop = process_prop(p)\n        if tp == \'real\':\n            prop = u\'r.{}-number\'.format(prop)\n            num_props.append(prop)\n        else:\n            prop = u\'r.{}-string\'.format(prop)\n        props.append(prop)\n    tkg[u\'props\'] = props\n    tkg[u\'num_props\'] = num_props\n    tkg[u\'name\'] = table[u\'id\']\n    rows = table[u\'rows\']\n    tkg[u\'row_ents\'] = [u\'row_{}\'.format(i) for i in xrange(len(rows))]\n    kg = {}\n    for i, row in enumerate(rows):\n        processed_vals = []\n        for j, val in enumerate(row):\n            if table[\'types\'][j] == \'real\':\n                if isinstance(val, int) or isinstance(val, float):\n                    processed_vals.append([val])\n                else:\n                    processed_vals.append([float(babel.numbers.parse_decimal(val))])\n            else:\n                processed_vals.append([val.lower()])\n        kg[u\'row_{}\'.format(i)] = dict(zip(props, processed_vals))\n    tkg[u\'kg\'] = kg\n    return tkg\n\n\ndef annotate_question(q, id, kg_dict, stop_words):\n    tokens = nltk.tokenize.word_tokenize(q[\'question\'])\n    tokens = [tk.lower() for tk in tokens]\n    e = {}\n    e[\'tokens\'] = tokens\n    e[\'question\'] = q[\'question\']\n    e[\'context\'] = q[\'table_id\']\n    e[\'sql\'] = q[\'sql\']\n    e[\'answer\'] = q[\'answer\']\n    e[\'id\'] = id\n    e[\'entities\'] = []\n    e[\'in_table\'] = [0] * len(tokens)\n    # entities are normalized tokens\n    e[\'processed_tokens\'] = tokens[:]\n    kg = kg_dict[q[\'table_id\']]\n    for i, tk in enumerate(tokens):\n        if tk not in stop_words:\n            if string_in_table(normalize(tk), kg):\n                e[\'entities\'].append(\n                    dict(value=[normalize(tk)], token_start=i, token_end=i+1,\n                         type=\'string_list\'))\n                e[\'in_table\'][i] = 1\n        try:\n            val = float(babel.numbers.parse_decimal(tk))\n            if val is not None:\n                e[\'entities\'].append(\n                    dict(value=[val], token_start=i, token_end=i+1,\n                         type=\'num_list\'))\n                if num_in_table(val, kg):\n                    e[\'in_table\'][i] = 1\n                e[\'processed_tokens\'][i] = \'<{}>\'.format(\'NUMBER\')\n        except NumberFormatError:\n            pass\n    e[\'features\'] = [[it] for it in e[\'in_table\']]\n    e[\'prop_features\'] = dict(\n        [(prop, [prop_in_question_score(\n            prop, e, stop_words, \n            binary=False)])\n         for prop in kg[\'props\']])\n    return e\n\n\ndef create_vocab(examples, embedding_model, min_count):\n    token_count = {}\n    for e in examples:\n      for tk in e[\'tokens\']:\n        # Token must be in glove and also appears more than min_count.\n        if find_tk_in_model(tk, embedding_model):\n          try:\n            token_count[tk] += 1\n          except KeyError:\n            token_count[tk] = 1\n    en_vocab = data_utils.generate_vocab_from_token_count(\n      token_count, min_count=min_count)\n    return en_vocab\n\n\ndef dump_examples(examples, fn):\n    t1 = time.time()\n    with open(fn, \'w\') as f:\n        for i, e in enumerate(examples):\n            f.write(json.dumps(e))\n            f.write(\'\\n\')\n    t2 = time.time()\n    print \'{} sec used dumping {} examples.\'.format(t2 - t1, len(examples))\n \n\ndef main(unused_argv):\n    assert tf.gfile.Exists(FLAGS.raw_input_dir)\n    if not tf.gfile.Exists(FLAGS.processed_input_dir):\n        tf.gfile.MkDir(FLAGS.processed_input_dir)\n\n    table_file = os.path.join(FLAGS.processed_input_dir, \'tables.jsonl\')\n    stop_words_file = os.path.join(FLAGS.raw_input_dir, \'stop_words.json\')\n\n    with open(stop_words_file, \'r\') as f:\n        stop_words = json.load(f)\n\n    # Load datasets. \n    train_set = []\n    with open(os.path.join(FLAGS.raw_input_dir, \'train.jsonl\'), \'r\') as f:\n        for line in f:\n            train_set.append(json.loads(line))\n\n    dev_set = []\n    with open(os.path.join(FLAGS.raw_input_dir, \'dev.jsonl\'), \'r\') as f:\n        for line in f:\n            dev_set.append(json.loads(line))\n\n    test_set = []\n    with open(os.path.join(FLAGS.raw_input_dir, \'test.jsonl\'), \'r\') as f:\n        for line in f:\n            test_set.append(json.loads(line))\n\n    # Load tables.\n    train_table_dict = {}\n    with open(os.path.join(FLAGS.raw_input_dir, \'train.tables.jsonl\'), \'r\') as f:\n        for line in f:\n            _table = json.loads(line)\n            train_table_dict[_table[\'id\']] = _table\n\n    dev_table_dict = {}\n    with open(os.path.join(FLAGS.raw_input_dir, \'dev.tables.jsonl\'), \'r\') as f:\n        for line in f:\n            _table = json.loads(line)\n            dev_table_dict[_table[\'id\']] = _table\n\n    test_table_dict = {}\n    with open(os.path.join(FLAGS.raw_input_dir, \'test.tables.jsonl\'), \'r\') as f:\n        for line in f:\n            _table = json.loads(line)\n            test_table_dict[_table[\'id\']] = _table\n\n    # Collect all the tables.\n    print \'Start collecting all the tables.\'\n    kg_dict = {}\n    for tb_dict in [dev_table_dict, train_table_dict, test_table_dict]:\n        for i, (k, v) in enumerate(tb_dict.iteritems()):\n            if i % 1000 == 0:\n                print i\n            kg_dict[k] = table2kg(v)\n\n    # Check if the string or number value has the correct type. \n    for kg in kg_dict.values():\n        for _, v in kg[\'kg\'].iteritems():\n            for prop, val in v.iteritems():\n                if prop[-7:] == \'-number\':\n                    for num in val:\n                        if not (isinstance(num, int) or isinstance(num, float)):\n                            print kg\n                            raise ValueError\n                if prop[-7:] == \'-string\':\n                    for num in val:\n                        if not isinstance(num, unicode):\n                            print kg\n                            raise ValueError\n\n    # Save the tables. \n    with open(table_file, \'w\') as f:\n        for _, v in kg_dict.iteritems():\n            f.write(json.dumps(v) + \'\\n\')\n\n    # Load the gold answers.\n    with open(os.path.join(FLAGS.raw_input_dir, \'dev_gold.json\'), \'r\') as f:\n        dev_answers = json.load(f)\n\n    for q, ans in zip(dev_set, dev_answers):\n        q[\'answer\'] = ans\n\n    with open(os.path.join(FLAGS.raw_input_dir, \'train_gold.json\'), \'r\') as f:\n        train_answers = json.load(f)\n\n    for q, ans in zip(train_set, train_answers):\n        q[\'answer\'] = ans\n\n    with open(os.path.join(FLAGS.raw_input_dir, \'test_gold.json\'), \'r\') as f:\n        test_answers = json.load(f)\n\n    for q, ans in zip(test_set, test_answers):\n        q[\'answer\'] = ans\n\n    # Annotate the examples and dump to files.\n    train_split_jsonl = os.path.join(\n        FLAGS.processed_input_dir, \'train_split.jsonl\')\n    dev_split_jsonl = os.path.join(\n        FLAGS.processed_input_dir, \'dev_split.jsonl\')\n    test_split_jsonl = os.path.join(\n        FLAGS.processed_input_dir, \'test_split.jsonl\')\n\n    t1 = time.time()\n    dev_examples = []\n    print \'start annotating dev examples.\'\n    for i, q in enumerate(dev_set):\n        if i % 500 == 0:\n            print i\n        e = annotate_question(q, \'dev-{}\'.format(i), kg_dict, stop_words)\n        expand_entities(e, kg_dict)\n        dev_examples.append(e)\n    t2 = time.time()\n    print \'{} sec used annotating dev examples.\'.format(t2 - t1)\n    dump_examples(dev_examples, dev_split_jsonl)\n\n    t1 = time.time()\n    train_examples = []\n    print \'start annotating train examples.\'\n    for i, q in enumerate(train_set):\n        if i % 500 == 0:\n            print i\n        e = annotate_question(q, \'train-{}\'.format(i), kg_dict, stop_words)\n        expand_entities(e, kg_dict)\n        train_examples.append(e)\n    t2 = time.time()\n    print \'{} sec used annotating train examples.\'.format(t2 - t1)\n    dump_examples(train_examples, train_split_jsonl)\n\n\n    t1 = time.time()\n    test_examples = []\n    print \'start annotating test examples.\'\n    for i, q in enumerate(test_set):\n        if i % 500 == 0:\n            print i\n        e = annotate_question(q, \'test-{}\'.format(i), kg_dict, stop_words)\n        expand_entities(e, kg_dict)\n        test_examples.append(e)\n    t2 = time.time()\n    print \'{} sec used annotating test examples.\'.format(t2 - t1)\n    dump_examples(test_examples, test_split_jsonl)\n\n    train_shards = []\n    for i in range(FLAGS.n_train_shard):\n        train_shards.append([])\n    for i, e in enumerate(train_examples):\n        train_shards[i % FLAGS.n_train_shard].append(e)\n\n    for i, sh in enumerate(train_shards):\n        train_shard_jsonl = os.path.join(\n            FLAGS.processed_input_dir, \'train_split_shard_{}-{}.jsonl\'.format(\n                FLAGS.n_train_shard, i))\n        dump_examples(sh, train_shard_jsonl)\n\n    # Load pretrained embeddings.\n    vocab_file = os.path.join(\n        FLAGS.raw_input_dir, ""wikisql_glove_vocab.json"")\n    embedding_file = os.path.join(\n        FLAGS.raw_input_dir, ""wikisql_glove_embedding_mat.npy"")\n    embedding_model = word_embeddings.EmbeddingModel(vocab_file, embedding_file)\n\n    for i in xrange(1, 11):\n        en_vocab = create_vocab(\n          train_examples + dev_examples, embedding_model, i)\n        vocab_file = os.path.join(\n          FLAGS.processed_input_dir, ""en_vocab_min_count_{}.json"".format(i))\n        with open(vocab_file, \'w\') as f:\n          json.dump(en_vocab.vocab, f, sort_keys=True, indent=2)\n        print \'min_tk_count: {}, vocab size: {}\'.format(i, len(en_vocab.vocab))    \n\n\nif __name__ == \'__main__\':  \n    tf.app.run()\n'"
table/wtq/__init__.py,0,b'\n'
table/wtq/ensemble.py,4,"b""import codecs\nimport glob\nimport json\nimport os\n\nimport tensorflow as tf\n\n\n# FLAGS\nFLAGS = tf.app.flags.FLAGS  \n\ntf.flags.DEFINE_string('eval_output_dir', '',\n                       'Path to the folder with all the eval outputs.')\n\ntf.flags.DEFINE_string('ensemble_output', '',\n                       'Path to save the ensemble output')\n\n\ndef main(unused_argv):\n    fns = [y for x in os.walk(FLAGS.eval_output_dir) for y in glob.glob(\n        os.path.join(x[0], '*dev_programs_in_beam_0.json'))]\n\n    model_predictions = []\n    print('{} eval output found in {}'.format(\n        len(fns), FLAGS.eval_output_dir))\n    print('=' * 100)\n    for fn in fns:\n        print fn\n        with open(fn, 'r') as f:\n            model_predictions.append(json.load(f))\n    print('=' * 100)\n    ensemble_pred = {}\n    for mp in model_predictions:\n        for name, candidates in mp.iteritems():\n            if name not in ensemble_pred:\n                ensemble_pred[name] = {}\n            for c in candidates:\n                prob = c[2]\n                prog = ' '.join(c[0])\n                answer = c[1]\n                if prog in ensemble_pred[name]:\n                    ensemble_pred[name][prog]['prob'] += prob\n                else:\n                    ensemble_pred[name][prog] = dict(prob=prob, answer=answer)\n\n\n    with codecs.open(FLAGS.ensemble_output, 'w', encoding='utf-8') as f:\n        for i in xrange(4344):\n            name = u'nu-{}'.format(i)\n            if name in ensemble_pred:\n                answer = []\n                max_prob = -1\n                for c, info in ensemble_pred[name].iteritems():\n                    if info['prob'] > max_prob:\n                        answer = info['answer']\n                        max_prob = info['prob']\n                string = name\n                for ans in answer:\n                    if isinstance(ans, unicode):\n                        string += u'\\t{}'.format(ans)\n                    else:\n                        string += u'\\t{}'.format(unicode(ans))\n            else:\n                string = name\n            f.write(string + u'\\n')\n    \n\nif __name__ == '__main__':  \n    tf.app.run()\n"""
table/wtq/evaluator.py,0,"b'#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n""""""Copy and extended from WikiTableQuestions dataset release.""""""\n\nu""""""Official Evaluator for WikiTableQuestions Dataset\n\nThere are 3 value types\n1. String (unicode)\n2. Number (float)\n3. Date (a struct with 3 fields: year, month, and date)\n   Some fields (but not all) can be left unspecified. However, if only the year\n   is specified, the date is automatically converted into a number.\n\nTarget denotation = a set of items\n- Each item T is a raw unicode string from Mechanical Turk\n- If T can be converted to a number or date (via Stanford CoreNLP), the\n    converted value (number T_N or date T_D) is precomputed\n\nPredicted denotation = a set of items\n- Each item P is a string, a number, or a date\n- If P is read from a text file, assume the following\n  - A string that can be converted into a number (float) is converted into a\n    number\n  - A string of the form ""yyyy-mm-dd"" is converted into a date. Unspecified\n    fields can be marked as ""xx"". For example, ""xx-01-02"" represents the date\n    January 2nd of an unknown year.\n  - Otherwise, it is kept as a string\n\nThe predicted denotation is correct if\n1. The sizes of the target denotation and the predicted denotation are equal\n2. Each item in the target denotation matches an item in the predicted\n    denotation\n\nA target item T matches a predicted item P if one of the following is true:\n1. normalize(raw string of T) and normalize(string form of P) are identical.\n   The normalize method performs the following normalizations on strings:\n   - Remove diacritics (\xc3\xa9 \xe2\x86\x92 e)\n   - Convert smart quotes (\xe2\x80\x98\xe2\x80\x99\xc2\xb4`\xe2\x80\x9c\xe2\x80\x9d) and dashes (\xe2\x80\x90\xe2\x80\x91\xe2\x80\x92\xe2\x80\x93\xe2\x80\x94\xe2\x88\x92) into ASCII ones\n   - Remove citations (trailing \xe2\x80\xa2\xe2\x99\xa6\xe2\x80\xa0\xe2\x80\xa1*#+ or [...])\n   - Remove details in parenthesis (trailing (...))\n   - Remove outermost quotation marks\n   - Remove trailing period (.)\n   - Convert to lowercase\n   - Collapse multiple whitespaces and strip outermost whitespaces\n2. T can be interpreted as a number T_N, P is a number, and P = T_N\n3. T can be interpreted as a date T_D, P is a date, and P = T_D\n   (exact match on all fields; e.g., xx-01-12 and 1990-01-12 do not match)\n""""""\n__version__ = \'1.0.2\'\n\nimport sys, os, re, argparse\nimport unicodedata\nfrom codecs import open\nfrom math import isnan, isinf\nfrom abc import ABCMeta, abstractmethod\n\n################ String Normalization ################\n\ndef normalize(x):\n    if not isinstance(x, unicode):\n        x = x.decode(\'utf8\', errors=\'ignore\')\n    # Remove diacritics\n    x = \'\'.join(c for c in unicodedata.normalize(\'NFKD\', x)\n                if unicodedata.category(c) != \'Mn\')\n    # Normalize quotes and dashes\n    x = re.sub(ur""[\xe2\x80\x98\xe2\x80\x99\xc2\xb4`]"", ""\'"", x)\n    x = re.sub(ur""[\xe2\x80\x9c\xe2\x80\x9d]"", ""\\"""", x)\n    x = re.sub(ur""[\xe2\x80\x90\xe2\x80\x91\xe2\x80\x92\xe2\x80\x93\xe2\x80\x94\xe2\x88\x92]"", ""-"", x)\n    while True:\n        old_x = x\n        # Remove citations\n        x = re.sub(ur""((?<!^)\\[[^\\]]*\\]|\\[\\d+\\]|[\xe2\x80\xa2\xe2\x99\xa6\xe2\x80\xa0\xe2\x80\xa1*#+])*$"", """", x.strip())\n        # Remove details in parenthesis\n        x = re.sub(ur""(?<!^)( \\([^)]*\\))*$"", """", x.strip())\n        # Remove outermost quotation mark\n        x = re.sub(ur\'^""([^""]*)""$\', r\'\\1\', x.strip())\n        if x == old_x:\n            break\n    # Remove final \'.\'\n    if x and x[-1] == \'.\':\n        x = x[:-1]\n    # Collapse whitespaces and convert to lower case\n    x = re.sub(ur\'\\s+\', \' \', x, flags=re.U).lower().strip()\n    return x\n\n\n################ Value Types ################\n\nclass Value(object):\n    __metaclass__ = ABCMeta\n\n    # Should be populated with the normalized string\n    _normalized = None\n\n    @abstractmethod\n    def match(self, other):\n        """"""Return True if the value matches the other value.\n\n        Args:\n            other (Value)\n        Returns:\n            a boolean\n        """"""\n        pass\n\n    @property\n    def normalized(self):\n        return self._normalized\n\n\nclass StringValue(Value):\n\n    def __init__(self, content):\n        assert isinstance(content, basestring)\n        self._normalized = normalize(content)\n        self._hash = hash(self._normalized)\n\n    def __eq__(self, other):\n        return isinstance(other, StringValue) and self.normalized == other.normalized\n\n    def __hash__(self):\n        return self._hash\n\n    def __str__(self):\n        return \'S\' +  str([self.normalized])\n    __repr__ = __str__\n\n    def match(self, other):\n        assert isinstance(other, Value)\n        return self.normalized == other.normalized\n\n\nclass NumberValue(Value):\n\n    def __init__(self, amount, original_string=None):\n        assert isinstance(amount, (int, long, float))\n        if abs(amount - round(amount)) < 1e-6:\n            self._amount = int(amount)\n        else:\n            self._amount = float(amount)\n        if not original_string:\n            self._normalized = unicode(self._amount)\n        else:\n            self._normalized = normalize(original_string)\n        self._hash = hash(self._amount)\n\n    @property\n    def amount(self):\n        return self._amount\n\n    def __eq__(self, other):\n        return isinstance(other, NumberValue) and self.amount == other.amount\n\n    def __hash__(self):\n        return self._hash\n\n    def __str__(self):\n        return (\'N(%f)\' % self.amount) + str([self.normalized])\n    __repr__ = __str__\n\n    def match(self, other):\n        assert isinstance(other, Value)\n        if self.normalized == other.normalized:\n            return True\n        if isinstance(other, NumberValue):\n            return abs(self.amount - other.amount) < 1e-6\n        return False\n\n    @staticmethod\n    def parse(text):\n        """"""Try to parse into a number.\n\n        Return:\n            the number (int or float) if successful; otherwise None.\n        """"""\n        try:\n            return int(text)\n        except:\n            try:\n                amount = float(text)\n                assert not isnan(amount) and not isinf(amount)\n                return amount\n            except:\n                return None\n\n\nclass DateValue(Value):\n\n    def __init__(self, year, month, day, original_string=None):\n        """"""Create a new DateValue. Placeholders are marked as -1.""""""\n        assert isinstance(year, int)\n        assert isinstance(month, int) and (month == -1 or 1 <= month <= 12)\n        assert isinstance(day, int) and (day == -1 or 1 <= day <= 31)\n        assert not (year == month == day == -1)\n        self._year = year\n        self._month = month\n        self._day = day\n        if not original_string:\n            self._normalized = \'{}-{}-{}\'.format(\n                year if year != -1 else \'xx\',\n                month if month != -1 else \'xx\',\n                day if day != \'-1\' else \'xx\')\n        else:\n            self._normalized = normalize(original_string)\n        self._hash = hash((self._year, self._month, self._day))\n\n    @property\n    def ymd(self):\n        return (self._year, self._month, self._day)\n\n    def __eq__(self, other):\n        return isinstance(other, DateValue) and self.ymd == other.ymd\n\n    def __hash__(self):\n        return self._hash\n\n    def __str__(self):\n        return ((\'D(%d,%d,%d)\' % (self._year, self._month, self._day))\n                + str([self._normalized]))\n    __repr__ = __str__\n\n    def match(self, other):\n        assert isinstance(other, Value)\n        if self.normalized == other.normalized:\n            return True\n        if isinstance(other, DateValue):\n            return self.ymd == other.ymd\n        return False\n\n    @staticmethod\n    def parse(text):\n        """"""Try to parse into a date.\n\n        Return:\n            tuple (year, month, date) if successful; otherwise None.\n        """"""\n        try:\n            ymd = text.lower().split(\'-\')\n            assert len(ymd) == 3\n            year = -1 if ymd[0] in (\'xx\', \'xxxx\') else int(ymd[0])\n            month = -1 if ymd[1] == \'xx\' else int(ymd[1])\n            day = -1 if ymd[2] == \'xx\' else int(ymd[2])\n            assert not (year == month == day == -1)\n            assert month == -1 or 1 <= month <= 12\n            assert day == -1 or 1 <= day <= 31\n            return (year, month, day)\n        except:\n            return None\n\n\n################ Value Instantiation ################\n\ndef to_value(original_string, corenlp_value=None):\n    """"""Convert the string to Value object.\n\n    Args:\n        original_string (basestring): Original string\n        corenlp_value (basestring): Optional value returned from CoreNLP\n    Returns:\n        Value\n    """"""\n    if isinstance(original_string, Value):\n        # Already a Value\n        return original_string\n    if not corenlp_value:\n        corenlp_value = original_string\n    # Number?\n    amount = NumberValue.parse(corenlp_value)\n    if amount is not None:\n        return NumberValue(amount, original_string)\n    # Date?\n    ymd = DateValue.parse(corenlp_value)\n    if ymd is not None:\n        if ymd[1] == ymd[2] == -1:\n            return NumberValue(ymd[0], original_string)\n        else:\n            return DateValue(ymd[0], ymd[1], ymd[2], original_string)\n    # String.\n    return StringValue(original_string)\n\ndef to_value_list(original_strings, corenlp_values=None):\n    """"""Convert a list of strings to a list of Values\n\n    Args:\n        original_strings (list[basestring])\n        corenlp_values (list[basestring or None])\n    Returns:\n        list[Value]\n    """"""\n    assert isinstance(original_strings, (list, tuple, set))\n    if corenlp_values is not None:\n        assert isinstance(corenlp_values, (list, tuple, set))\n        assert len(original_strings) == len(corenlp_values)\n        return list(set(to_value(x, y) for (x, y)\n                in zip(original_strings, corenlp_values)))\n    else:\n        return list(set(to_value(x) for x in original_strings))\n\n\n################ Check the Predicted Denotations ################\n\ndef check_denotation(target_values, predicted_values):\n    """"""Return True if the predicted denotation is correct.\n    \n    Args:\n        target_values (list[Value])\n        predicted_values (list[Value])\n    Returns:\n        bool\n    """"""\n    # Check size\n    if len(target_values) != len(predicted_values):\n        return False\n    # Check items\n    for target in target_values:\n        if not any(target.match(pred) for pred in predicted_values):\n            return False\n    return True\n\n\n################ Batch Mode ################\n\ndef tsv_unescape(x):\n    """"""Unescape strings in the TSV file.\n    Escaped characters include:\n        newline (0x10) -> backslash + n\n        vertical bar (0x7C) -> backslash + p\n        backslash (0x5C) -> backslash + backslash\n\n    Args:\n        x (str or unicode)\n    Returns:\n        a unicode\n    """"""\n    return x.replace(r\'\\n\', \'\\n\').replace(r\'\\p\', \'|\').replace(\'\\\\\\\\\', \'\\\\\')\n\ndef tsv_unescape_list(x):\n    """"""Unescape a list in the TSV file.\n    List items are joined with vertical bars (0x5C)\n\n    Args:\n        x (str or unicode)\n    Returns:\n        a list of unicodes\n    """"""\n    return [tsv_unescape(y) for y in x.split(\'|\')]\n\ndef main():\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\'-t\', \'--tagged-dataset-path\',\n            default=os.path.join(\'.\', \'tagged\', \'data\'),\n            help=\'Directory containing CoreNLP-tagged dataset TSV file\')\n    parser.add_argument(\'prediction_path\',\n            help=\'Path to the prediction file. Each line contains \'\n                 \'ex_id <tab> item1 <tab> item2 <tab> ...\')\n    args = parser.parse_args()\n    \n    # ID string --> list[Value]\n    target_values_map = {}\n    for filename in os.listdir(args.tagged_dataset_path):\n        filename = os.path.join(args.tagged_dataset_path, filename)\n        print >> sys.stderr, \'Reading dataset from\', filename\n        with open(filename, \'r\', \'utf8\') as fin:\n            header = fin.readline().rstrip(\'\\n\').split(\'\\t\')\n            for line in fin:\n                stuff = dict(zip(header, line.rstrip(\'\\n\').split(\'\\t\')))\n                ex_id = stuff[\'id\']\n                original_strings = tsv_unescape_list(stuff[\'targetValue\'])\n                canon_strings = tsv_unescape_list(stuff[\'targetCanon\'])\n                target_values_map[ex_id] = to_value_list(\n                        original_strings, canon_strings)\n    print >> sys.stderr, \'Read\', len(target_values_map), \'examples\'\n\n    print >> sys.stderr, \'Reading predictions from\', args.prediction_path\n    num_examples, num_correct = 0, 0\n    with open(args.prediction_path, \'r\', \'utf8\') as fin:\n        for line in fin:\n            line = line.rstrip(\'\\n\').split(\'\\t\')\n            ex_id = line[0]\n            if ex_id not in target_values_map:\n                print \'WARNING: Example ID ""%s"" not found\' % ex_id\n            else:\n                target_values = target_values_map[ex_id]\n                predicted_values = to_value_list(line[1:])\n                correct = check_denotation(target_values, predicted_values)\n                print u\'%s\\t%s\\t%s\\t%s\' % (ex_id, correct,\n                        target_values, predicted_values)\n                num_examples += 1\n                if correct:\n                    num_correct += 1\n    print >> sys.stderr, \'Examples:\', num_examples\n    print >> sys.stderr, \'Correct:\', num_correct\n    print >> sys.stderr, \'Accuracy:\', round(\n            (num_correct + 1e-9) / (num_examples + 1e-9), 4)\n\n\n# Added utility functions for computing preprocessing answers and computing rewards.\ndef target_values_map(targetValue, targetCanon):\n    original_strings = tsv_unescape_list(targetValue)\n    canon_strings = tsv_unescape_list(targetCanon)\n    target_values  = to_value_list(original_strings, canon_strings)\n    return target_values\n\ndef check_prediction(ts_prediction_string, target_values):\n    predicted_values = to_value_list(ts_prediction_string)\n    correct = check_denotation(target_values, predicted_values)\n    return correct\n\n\nif __name__ == \'__main__\':\n    main()\n'"
table/wtq/preprocess.py,23,"b'# -*- coding: utf-8 -*-\nimport argparse\nimport csv\nimport nltk\nimport pandas\nimport os\nimport re\nimport time\nimport json\nimport pprint\n\nimport unicodedata\nfrom codecs import open\n\nimport nsm\nfrom nsm import word_embeddings\nfrom nsm import data_utils\n\nimport evaluator\n\nimport tensorflow as tf\n\n\n# FLAGS\nFLAGS = tf.app.flags.FLAGS  \n\ntf.flags.DEFINE_string(\'raw_input_dir\', \'\',\n                       \'path.\')\ntf.flags.DEFINE_string(\'processed_input_dir\', \'\',\n                       \'path to the folder to save all the processed data.\')\ntf.flags.DEFINE_integer(\'n_train_shard\', 90, \'.\')\ntf.flags.DEFINE_integer(\'max_n_tokens_for_num_prop\', 10, \'.\')\ntf.flags.DEFINE_float(\'min_frac_for_ordered_prop\', 0.2, \'.\')\ntf.flags.DEFINE_integer(\'en_min_tk_count\', 10, \'.\')\ntf.flags.DEFINE_bool(\'use_prop_match_count_feature\', False, \'.\')\ntf.flags.DEFINE_bool(\'anonymize_in_table_tokens\', False, \'.\')\ntf.flags.DEFINE_bool(\'anonymize_datetime_and_number_entities\', False, \'.\')\ntf.flags.DEFINE_bool(\'merge_entities\', False, \'.\')\ntf.flags.DEFINE_bool(\'process_conjunction\', False, \'.\')\ntf.flags.DEFINE_bool(\'expand_entities\', False, \'.\')\ntf.flags.DEFINE_bool(\'use_tokens_contain\', False, \'.\')\n\n\n# Copied from WikiTableQuestions dataset official evaluator. \ndef normalize(x):\n    if not isinstance(x, unicode):\n        x = x.decode(\'utf8\', errors=\'ignore\')\n    # Remove diacritics\n    x = \'\'.join(c for c in unicodedata.normalize(\'NFKD\', x)\n                if unicodedata.category(c) != \'Mn\')\n    # Normalize quotes and dashes\n    x = re.sub(ur""[\xe2\x80\x98\xe2\x80\x99\xc2\xb4`]"", ""\'"", x)\n    x = re.sub(ur""[\xe2\x80\x9c\xe2\x80\x9d]"", ""\\"""", x)\n    x = re.sub(ur""[\xe2\x80\x90\xe2\x80\x91\xe2\x80\x92\xe2\x80\x93\xe2\x80\x94\xe2\x88\x92]"", ""-"", x)\n    while True:\n        old_x = x\n        # Remove citations\n        x = re.sub(ur""((?<!^)\\[[^\\]]*\\]|\\[\\d+\\]|[\xe2\x80\xa2\xe2\x99\xa6\xe2\x80\xa0\xe2\x80\xa1*#+])*$"", """", x.strip())\n        # Remove details in parenthesis\n        x = re.sub(ur""(?<!^)( \\([^)]*\\))*$"", """", x.strip())\n        # Remove outermost quotation mark\n        x = re.sub(ur\'^""([^""]*)""$\', r\'\\1\', x.strip())\n        if x == old_x:\n            break\n    # Remove final \'.\'\n    if x and x[-1] == \'.\':\n        x = x[:-1]\n    # Collapse whitespaces and convert to lower case\n    x = re.sub(ur\'\\s+\', \' \', x, flags=re.U).lower().strip()\n    return x\n\n\n# Normalize the date time value from corenlp annotation.\ndef normalize_date_nervalue(val_string):\n    if re.match(\'\\A[\\d]{4}$\', val_string):\n        string = val_string + \'-XX-XX\'\n    elif re.match(\'\\A[\\dX]{4}-[\\dX]{2}$\', val_string):\n        string = val_string + \'-XX\'\n    else:\n        string = val_string\n    val = evaluator.to_value(string)\n    if isinstance(val, evaluator.DateValue):\n        return val.normalized\n    else:\n        return None\n\n\n# Normalize the number value from corenlp annotation.\ndef normalize_number_nervalue(val_string):\n    m = re.search(r\'\\d+(\\.\\d+)*\', val_string)\n    if m is None:\n        return None\n    string = m.group()\n    val = evaluator.to_value(string)\n    if isinstance(val, evaluator.NumberValue):\n        return val.amount\n    else:\n        return None\n\n# # Preprocess the tables\n\n# Turn table into KG (implemented as a dictionary).\nn_total_num = 0\nn_filtered_num = 0\nn_date_and_num = 0\nn_too_few_num_date = 0\n\n\ndef table2kg(tab_name, data_folder,\n             max_n_tokens_for_num_prop=10, min_frac_for_ordered_prop=0.2):\n    kg = {}\n    node_info = {}\n    tab_ids = tab_name.split(\'_\')\n    col_num_to_name = {}\n    num_props = set()\n    datetime_props = set()\n    props = set()\n    fn = os.path.join(data_folder, tab_ids[1] + \'-tagged\', tab_ids[2] + \'.tagged\')\n    \n    with open(fn, \'r\') as f:\n        reader = csv.reader(f, delimiter=\'\\t\', quoting=csv.QUOTE_NONE)\n        # headers are ""row col id content tokens lemmaTokens\n        # posTags nerTags nerValues number date num2 list\n        # listId"" in order. \n        header = reader.next()\n        # Collect all the column names.\n        for row in reader:\n            # Assume all the column names are defined in the\n            # first row (row index is -1).\n            if row[0] != \'-1\':\n                break\n            raw_col_name = row[2]\n            col_num = row[1]\n            # All the column names start with ""fb:row.row.""\n            # like ""fb:row.row.title"".\n            col_name = \'r.\' + raw_col_name[11:]\n            col_num_to_name[col_num] = col_name\n    \n    with open(fn, \'r\') as f:\n        reader = csv.reader(f, delimiter=\'\\t\', quoting=csv.QUOTE_NONE)\n        header = reader.next()\n        row_id = \'\'\n        row_node = None\n        for row in reader:\n            # Ignore all the -1 row.\n            if row[0] == \'-1\':\n                continue\n            # Create a new row node when a new row starts.\n            if row[0] != row_id:\n                row_id = row[0]\n                row_name = \'row_{}\'.format(row_id)\n                # Create new row node.\n                row_node = dict()\n                kg[row_name] = row_node\n            col_name = col_num_to_name[row[1]]\n            cell_name = \'cell_{}_{}\'.format(row[0], row[1])\n            node_info[cell_name] = dict(zip(header, row))\n            prop_name = col_name + \'-string\'\n            row_node[prop_name] = [normalize(node_info[cell_name][\'content\'])]\n            props.add(prop_name)\n\n            # Number of tokens in this cell.\n            n_tokens = len(node_info[cell_name][\'tokens\'].split(\'|\'))\n            if node_info[cell_name][\'number\']:\n                global n_total_num\n                n_total_num += 1\n            if node_info[cell_name][\'number\'] and n_tokens >= max_n_tokens_for_num_prop:\n                global n_filtered_num\n                n_filtered_num += 1\n            if node_info[cell_name][\'date\'] and node_info[cell_name][\'number\']:\n                global n_date_and_num\n                n_date_and_num += 1\n\n            # If there are too many tokens in the cell, then\n            # it is probably not a number cell, should\n            # ignore the numbers extracted by corenlp. \n            if n_tokens < max_n_tokens_for_num_prop:\n                num = node_info[cell_name][\'number\']\n                num2 = node_info[cell_name][\'num2\']\n            else:\n                num = None\n                num2 = None\n            date = node_info[cell_name][\'date\']\n            if date:\n                prop_name = col_name + \'-date\'\n                row_node[prop_name] = [date]\n                datetime_props.add(prop_name)\n            if num:\n                prop_name = col_name + \'-number\'\n                row_node[prop_name] = [float(num)]\n                num_props.add(prop_name)\n            if num2:\n                prop_name = col_name + \'-num2\'\n                row_node[prop_name] = [float(num2)]\n                num_props.add(prop_name)\n\n        row_names = [k for k in kg.keys() if k[:4] == \'row_\']\n        num_props = list(num_props)\n        datetime_props = list(datetime_props)\n        \n    nodes = kg.values()\n    total_n = len(nodes)\n    \n    for prop in (num_props + datetime_props):\n        nodes_with_prop = [node for node in nodes if prop in node]\n        n = len(nodes_with_prop)\n        # If a large fraction of the column don\'t have\n        # number or date, then this column is probably not\n        # really number or date.\n        if n * 1.0 / total_n < min_frac_for_ordered_prop:\n            for node in nodes_with_prop:\n                del node[prop]\n\n    return dict(kg=kg, row_ents=row_names,\n                props=(list(props) + num_props + datetime_props),\n                num_props=num_props, datetime_props=datetime_props)\n\n\n# Don\'t use dataframe.from_csv because quotes might be dropped. \ndef create_df_from_wtq_questions(fn):\n    df_dict = {}\n    with open(fn, \'r\') as f:\n        reader = csv.reader(f, delimiter=\'\\t\', quotechar=None)\n        header = reader.next()\n        for col in header:\n            df_dict[col] = []\n        for line in reader:\n            for col, val in zip(header, line):\n                df_dict[col].append(val)\n    df = pandas.DataFrame(df_dict)\n    return df\n\n\ndef tokens_contain(string_1, string_2):\n  tks_1 = nltk.tokenize.word_tokenize(string_1)\n  tks_2 = nltk.tokenize.word_tokenize(string_2)\n  return set(tks_2).issubset(set(tks_1))\n\n# preprocess the questions.\ndef string_in_table_tk(string, kg):\n  for k, node in kg[\'kg\'].iteritems():\n    for prop, val in node.iteritems():\n      if ((isinstance(val[0], str) or isinstance(val[0], unicode)) and\n          tokens_contain(val[0], string)):\n        return True\n      else:\n        return False\n    \n# preprocess the questions.\ndef string_in_table_str(string, kg):\n    for k, node in kg[\'kg\'].iteritems():\n        for prop, val in node.iteritems():\n            if (isinstance(val[0], str) or isinstance(val[0], unicode)) and string in val[0]:\n                return True\n    else:\n        return False\n\ndef string_in_table(string, kg):\n  if FLAGS.use_tokens_contain:\n    return string_in_table_tk(string, kg)\n  else:\n    return string_in_table_str(string, kg)\n\n\ndef date_in_table(ent, kg):\n    props_set = set(kg[\'datetime_props\'])\n    for k, node in kg[\'kg\'].iteritems():\n        for prop, val in node.iteritems():\n            if prop in props_set and ent == val:\n                return True\n    else:\n        return False\n\n        \ndef num_in_table(ent, kg):\n    props_set = set(kg[\'num_props\'])\n    for k, node in kg[\'kg\'].iteritems():\n        for prop, val in node.iteritems():\n            if prop in props_set and ent == val:\n                return True\n    else:\n        return False        \n\n\ndef prop_in_question_score(prop, example, stop_words, binary=True):\n    question = example[\'question\'].decode(\'utf-8\')\n    prop = prop[2:]\n    prop = u\'-\'.join(prop.split(u\'-\')[:-1])\n    prop_tks = prop.split(u\'_\')\n    n_in_question = 0\n    for tk in prop_tks:\n        tk = tk.decode(\'utf-8\')\n        if tk not in stop_words and tk in question:\n            n_in_question += 1\n    if binary:\n        n_in_question = min(n_in_question, 1)\n    return n_in_question\n\n\ndef collect_examples_from_df(df, kg_dict, stop_words):\n    examples = []\n    for index, row in df.iterrows():\n        #print row[\'utterance\'], row[\'tokens\'], index\n        match = re.match(r\'csv/(?P<first>\\d+)-csv/(?P<second>\\d+).csv\', row[\'context\'])\n        context = \'t_{}_{}\'.format(*match.groups())\n        tks = row[\'tokens\'].split(\'|\')\n        pos_tags = row[\'posTags\'].split(\'|\')\n        vals = row[\'nerValues\'].split(\'|\')\n        tags = row[\'nerTags\'].split(\'|\')\n        e = dict(id=row[\'id\'], question=row[\'utterance\'],tokens=tks,\n                 context=context, pos_tags=pos_tags)\n        answer = (row[\'targetValue\'], row[\'targetCanon\'])\n        e[\'answer\'] = answer\n        e[\'entities\'] = []\n        # entities are normalized tokens\n        e[\'processed_tokens\'] = tks[:]\n        e[\'in_table\'] = [0] * len(tks)\n        for i, (tk, tag, val) in enumerate(zip(tks, tags, vals)):\n            kg = kg_dict[context]\n            if tk not in stop_words:\n                if string_in_table(normalize(tk), kg):\n                    e[\'entities\'].append(\n                        dict(value=[normalize(tk)], token_start=i, token_end=i+1,\n                             type=\'string_list\'))\n                    e[\'in_table\'][i] = 1\n            if tag == \'DATE\':\n                nerVal = normalize_date_nervalue(val)\n                if nerVal is not None:\n                    e[\'entities\'].append(\n                        dict(value=[nerVal], token_start=i, token_end=i+1,\n                             type=\'datetime_list\'))\n                    if date_in_table(nerVal, kg):\n                        e[\'in_table\'][i] = 1\n                        e[\'processed_tokens\'][i] = \'<{}>\'.format(tag)\n                    #e[\'processed_tokens\'][i] += \' <DATE>\'\n            elif tag == \'NUMBER\':\n                nerVal = normalize_number_nervalue(val)\n                if nerVal is not None:\n                    e[\'entities\'].append(\n                        dict(value=[nerVal], token_start=i, token_end=i+1,\n                             type=\'num_list\'))\n                    if num_in_table(nerVal, kg):\n                        e[\'in_table\'][i] = 1\n                        e[\'processed_tokens\'][i] = \'<{}>\'.format(tag)\n                    #e[\'processed_tokens\'][i] += \' <NUMBER>\'\n            elif tag != \'O\':\n                e[\'processed_tokens\'][i] = \'<{}>\'.format(tag)\n        e[\'features\'] = [[it] for it in e[\'in_table\']]\n        e[\'prop_features\'] = dict(\n          [(prop, [prop_in_question_score(\n              prop, e, stop_words,\n              binary=not FLAGS.use_prop_match_count_feature)])\n           for prop in kg[\'props\']])\n        examples.append(e)        \n                \n    avg_n_ent = (sum([len(e[\'entities\']) for e in examples]) * 1.0 /\n                 len(examples))\n\n    print \'Average number of entities is {}\'.format(avg_n_ent)\n    if FLAGS.expand_entities:\n      expand_entities(examples, kg_dict)\n      avg_n_ent = (sum([len(e[\'entities\']) for e in examples]) * 1.0 /\n                   len(examples))\n      print \'After expanding, average number of entities is {}\'.format(avg_n_ent)\n\n    for e in examples:\n      e[\'tmp_tokens\'] = e[\'tokens\'][:]\n\n    if FLAGS.anonymize_datetime_and_number_entities:\n        for e in examples:\n            for ent in e[\'entities\']:\n                if ent[\'type\'] == \'datetime_list\':\n                  for t in xrange(ent[\'token_start\'], ent[\'token_end\']):\n                    e[\'tmp_tokens\'][t] = \'<DECODE>\'\n                elif ent[\'type\'] == \'num_list\':\n                  for t in xrange(ent[\'token_start\'], ent[\'token_end\']):\n                    e[\'tmp_tokens\'][t] = \'<START>\'\n      \n    # if FLAGS.merge_entities:\n    #   merge_entities(examples, kg_dict)\n    #   avg_n_ent = (sum([len(e[\'entities\']) for e in examples]) * 1.0 /\n    #                len(examples))\n    #   print \'After merging, average number of entities is {}\'.format(avg_n_ent)\n\n    if FLAGS.process_conjunction:\n      n_conjunction = process_conjunction(examples, \'or\')\n      tf.logging.info(\'{} conjunctions processed.\'.format(n_conjunction))\n      avg_n_ent = (sum([len(e[\'entities\']) for e in examples]) * 1.0 /\n                   len(examples))\n      print \'After processing conjunction, average number of entities is {}\'.format(\n        avg_n_ent)\n\n    for e in examples:\n      e[\'tokens\'] = e[\'tmp_tokens\']\n      \n    return examples\n\n\n# Save the preprocessed examples. \n\ndef dump_examples(examples, fn):\n    t1 = time.time()\n    with open(fn, \'w\') as f:\n        for i, e in enumerate(examples):\n            f.write(json.dumps(e))\n            f.write(\'\\n\')\n    t2 = time.time()\n    print \'{} sec used dumping {} examples.\'.format(t2 - t1, len(examples))\n\n\ndef to_unicode(tk, encoding=\'utf-8\'):\n  if isinstance(tk, str):\n    return tk.decode(encoding)\n  else:\n    return tk\n\n\ndef merge_entities(examples, table_dict):\n    for e in examples:\n        ents = [ent for ent in e[\'entities\']\n                if ent[\'type\'] == \'string_list\' and ent[\'value\'][0]]\n        other_ents = [ent for ent in e[\'entities\'] if ent[\'type\'] != \'string_list\']\n        kg = table_dict[e[\'context\']]\n        l = len(ents)\n        new_ents = []\n        i = 0\n        merged = False\n        while i < l:\n            top_ent = ents[i].copy()\n            new_ents.append(top_ent)\n            i += 1\n            while i < l:\n                if ents[i][\'token_start\'] - top_ent[\'token_end\'] <= 2:\n                    # print e[\'tokens\'][top_ent[\'token_start\']:ents[i][\'token_end\']]\n                    tokens = [to_unicode(tk) for tk in\n                              e[\'tokens\'][top_ent[\'token_start\']:ents[i][\'token_end\']]]\n                    ent_tokens = [to_unicode(top_ent[\'value\'][0]),\n                                  to_unicode(ents[i][\'value\'][0])]\n                    new_str_1 = u\' \'.join(tokens)\n                    new_str_2 = u\' \'.join(ent_tokens)\n                    new_str_3 = u\'-\'.join(tokens)\n                    new_str_4 = u\'-\'.join(ent_tokens)\n                    new_str_5 = u\'\'.join(tokens)\n                    new_str_6 = u\'\'.join(ent_tokens)\n                    # print new_str_1\n                    if string_in_table(new_str_1, kg):\n                        new_str = new_str_1\n                    elif string_in_table(new_str_2, kg):\n                        new_str = new_str_2\n                    elif string_in_table(new_str_3, kg):\n                        new_str = new_str_3\n                    elif string_in_table(new_str_4, kg):\n                        new_str = new_str_4\n                    elif string_in_table(new_str_5, kg):\n                        new_str = new_str_5\n                    elif string_in_table(new_str_6, kg):\n                        new_str = new_str_6\n                    else:\n                        new_str = \'\'\n                    if new_str:\n                        top_ent = dict(value=[new_str], type=\'string_list\', \n                                       token_start=top_ent[\'token_start\'],\n                                       token_end=ents[i][\'token_end\'])\n                        new_ents[-1] = top_ent\n                        i += 1\n                    else:\n                        break\n                else:\n                    break\n        e[\'entities\'] = new_ents + other_ents\n        for ent in e[\'entities\']:\n            for t in xrange(ent[\'token_start\'], ent[\'token_end\']):\n                e[\'features\'][t] = [1]\n\ndef expand_entities(examples, table_dict):\n    for e in examples:\n    # for e in [example_dict[\'nt-11874\']]:\n        ents = [ent for ent in e[\'entities\']\n                if ent[\'type\'] == \'string_list\' and ent[\'value\'][0]]\n        other_ents = [ent for ent in e[\'entities\'] if ent[\'type\'] != \'string_list\']\n        kg = table_dict[e[\'context\']]\n        l = len(ents)\n        new_ents = []\n        i = 0\n        tokens = e[\'tokens\']\n        for ent in ents:\n            # relies on order. \n            if new_ents and ent[\'token_end\'] <= new_ents[-1][\'token_end\']:\n                continue\n            else:\n                ent[\'value\'][0] = tokens[ent[\'token_start\']]\n                new_ents.append(ent)\n                while True and ent[\'token_end\'] < len(tokens):\n                    new_str_list = (\n                      [s.join([to_unicode(ent[\'value\'][0]),\n                               to_unicode(tokens[ent[\'token_end\']])]) \n                       for s in [u\' \', u\'-\', u\'\']] + \n                      [s.join([to_unicode(ent[\'value\'][0]),\n                               to_unicode(normalize(tokens[ent[\'token_end\']]))])\n                       for s in [u\' \', u\'-\', u\'\']] + \n                      [s.join([to_unicode(normalize(ent[\'value\'][0])),\n                               to_unicode(tokens[ent[\'token_end\']])]) \n                       for s in [u\' \', u\'-\', u\'\']] + \n                      [s.join([to_unicode(normalize(ent[\'value\'][0])),\n                               to_unicode(normalize(tokens[ent[\'token_end\']]))])\n                       for s in [u\' \', u\'-\', u\'\']])\n                    for new_str in new_str_list:\n                      if string_in_table(new_str, kg):\n                        ent[\'token_end\'] += 1\n                        ent[\'value\'] = [new_str]\n                        break\n                    else:\n                      break\n                ent[\'value\'] = [normalize(ent[\'value\'][0])]\n        e[\'entities\'] = new_ents + other_ents\n\n\ndef process_conjunction(examples, conjunction_word, other_words=None):\n    i = 0\n    for e in examples:\n        str_ents = [ent for ent in e[\'entities\'] if ent[\'type\'] == \'string_list\']\n        other_ents = [ent for ent in e[\'entities\'] if ent[\'type\'] != \'string_list\']\n        if other_words is not None:\n            extra_condition = any([w in e[\'tokens\'] for w in other_words])\n        else:\n            extra_condition = True\n        if str_ents and conjunction_word in e[\'tokens\'] and extra_condition:\n            or_idx = e[\'tokens\'].index(conjunction_word)\n            before_ent = None\n            before_id = None\n            after_ent = None\n            after_id = None\n            for k, ent in enumerate(str_ents):\n                if ent[\'token_end\'] <= or_idx:\n                    before_ent = ent\n                    before_id = k\n                    before_distance = abs(ent[\'token_end\'] - or_idx)\n                if after_ent is None and ent[\'token_start\'] > or_idx:\n                    after_ent = ent\n                    after_id = k\n                    after_distance = abs(ent[\'token_start\'] - or_idx)\n            if (not before_ent is None and not after_ent is None and\n                before_distance <= 2 and after_distance <= 2):\n                i += 1\n                new_ent = dict(\n                  value=before_ent[\'value\'] + after_ent[\'value\'],\n                  type=\'string_list\',\n                  token_start=before_ent[\'token_start\'], \n                  token_end=after_ent[\'token_end\'])\n                str_ents[before_id] = new_ent\n                del str_ents[after_id]\n                e[\'entities\'] = str_ents + other_ents\n    return i\n\n\ndef main(unused_argv):\n    assert tf.gfile.Exists(FLAGS.raw_input_dir)\n    if not tf.gfile.Exists(FLAGS.processed_input_dir):\n        tf.gfile.MkDir(FLAGS.processed_input_dir)\n\n    data_folder = os.path.join(FLAGS.raw_input_dir, \'WikiTableQuestions/tagged\')\n    table_file = os.path.join(FLAGS.processed_input_dir, \'tables.jsonl\')\n    test_table_file = os.path.join(FLAGS.processed_input_dir, \'test_table.json\')\n    stop_words_file = os.path.join(FLAGS.raw_input_dir, \'stop_words.json\')\n    train_file = os.path.join(FLAGS.processed_input_dir, \'train_examples.jsonl\')\n\n    train_tagged = os.path.join(\n        FLAGS.raw_input_dir, \'WikiTableQuestions/tagged/data/training.tagged\')\n    test_tagged = os.path.join(\n        FLAGS.raw_input_dir, \'WikiTableQuestions/tagged/data/pristine-unseen-tables.tagged\')\n\n    # Preprocess the tables.\n    subdirs = os.listdir(data_folder)\n    subdirs.remove(\'data\')\n\n    # Preprocess the tables. \n    table_dict = {}\n    folders = []\n    t1 = time.time()\n    for d in subdirs:\n        for fn in os.listdir(os.path.join(data_folder, d)):\n            full_path = os.path.join(data_folder, d, fn)\n            m = re.match(r\'.*/(?P<first>[0-9]*)-tagged/(?P<second>[0-9]*)\\.tagged\', full_path)\n            folders.append(full_path)\n            table_name = \'t_{}_{}\'.format(m.group(\'first\'), m.group(\'second\'))\n            kg = table2kg(\n                table_name, data_folder,\n                max_n_tokens_for_num_prop=FLAGS.max_n_tokens_for_num_prop,\n                min_frac_for_ordered_prop=FLAGS.min_frac_for_ordered_prop)\n            kg[\'name\'] = table_name\n            table_dict[table_name] = kg\n    t2 = time.time()\n    print(\'{} sec used processing the tables.\'.format(t2 - t1))\n    print \'total number of number cells: {}\'.format(n_total_num)\n    print \'total number of filtered number cells: {}\'.format(n_filtered_num)\n    print \'filtered ration: {}\'.format(n_filtered_num * 1.0 / n_total_num)\n    print \'date and number ratio: {}\'.format(n_date_and_num * 1.0 / n_total_num)\n\n    # Save the preprocessed test table. \n    with open(test_table_file, \'w\') as f:\n        json.dump({\'t_203_375\': table_dict[\'t_203_375\']}, f)\n\n    # Save the preprocessed table. \n    t1 = time.time()\n    with open(table_file, \'w\') as f:\n        for i, (k, v) in enumerate(table_dict.iteritems()):\n            if i % 1000 == 0:\n                print \'number {}\'.format(i)\n            f.write(json.dumps(v))\n            f.write(\'\\n\')\n    t2 = time.time()\n    print \'{} sec used dumping tables\'.format(t2 - t1)\n\n    df = create_df_from_wtq_questions(train_tagged)\n\n    with open(stop_words_file, \'r\') as f:\n        stop_words_list = json.load(f)\n    stop_words = set(stop_words_list)\n    \n    t1 = time.time()    \n    examples = collect_examples_from_df(\n        df, table_dict, stop_words)\n    t2 = time.time()\n    print \'{} sec used collecting train examples.\'.format(t2 - t1)\n    \n    dump_examples(examples, train_file)\n\n    # Save all the train data.\n    processed_input_dir = os.path.join(\n        FLAGS.processed_input_dir, \'no_split\')\n    if not tf.gfile.Exists(processed_input_dir):\n        tf.gfile.MkDir(processed_input_dir)\n    train_shards = []\n    for i in range(FLAGS.n_train_shard):\n        train_shards.append([])\n    for i, e in enumerate(examples):\n        train_shards[i % FLAGS.n_train_shard].append(e)\n    for i, sh in enumerate(train_shards):\n        train_shard_jsonl = os.path.join(\n            processed_input_dir, \'train_split_shard_{}-{}.jsonl\'.format(\n                FLAGS.n_train_shard, i))\n        dump_examples(sh, train_shard_jsonl)\n\n    # Save each split.\n    for split_id in xrange(1, 6):\n        processed_input_dir = os.path.join(\n            FLAGS.processed_input_dir, \'data_split_{}\'.format(split_id))\n        if not tf.gfile.Exists(processed_input_dir):\n            tf.gfile.MkDir(processed_input_dir)\n        \n        train_split_tsv = os.path.join(\n            FLAGS.raw_input_dir,\n            \'WikiTableQuestions/data/random-split-{}-train.tsv\'.format(split_id))\n        dev_split_tsv = os.path.join(\n            FLAGS.raw_input_dir,\n            \'WikiTableQuestions/data/random-split-{}-dev.tsv\'.format(split_id))\n\n        # Create all the splitted datasets.\n        train_df = create_df_from_wtq_questions(train_split_tsv)\n        dev_df = create_df_from_wtq_questions(dev_split_tsv)\n\n        assert len(train_df) + len(dev_df) == len(df)\n\n        train_ids = set(train_df[\'id\'])\n        train_examples = []\n        dev_ids = set(dev_df[\'id\'])\n        dev_examples = []\n        for e in examples:\n            if e[\'id\'] in train_ids:\n                train_examples.append(e)\n            elif e[\'id\'] in dev_ids:\n                dev_examples.append(e)\n            else:\n                raise ValueError(\'id {} not found\'.format(e[\'id\']))\n        assert len(train_examples) + len(dev_examples) == len(df)\n\n        train_split_jsonl = os.path.join(\n            processed_input_dir, \'train_split.jsonl\')\n        dev_split_jsonl = os.path.join(\n            processed_input_dir, \'dev_split.jsonl\')\n\n        dump_examples(train_examples, train_split_jsonl)\n        dump_examples(dev_examples, dev_split_jsonl)\n\n        train_shards = []\n        for i in range(FLAGS.n_train_shard):\n            train_shards.append([]) \n        for i, e in enumerate(train_examples):\n            train_shards[i % FLAGS.n_train_shard].append(e)\n\n        for i, sh in enumerate(train_shards):\n            train_shard_jsonl = os.path.join(\n                processed_input_dir, \'train_split_shard_{}-{}.jsonl\'.format(\n                    FLAGS.n_train_shard, i))\n            dump_examples(sh, train_shard_jsonl)        \n\n    test_df = create_df_from_wtq_questions(test_tagged)\n    t1 = time.time()    \n    test_examples = collect_examples_from_df(\n        test_df, table_dict, stop_words)\n    t2 = time.time()\n    print \'{} sec used collecting test examples.\'.format(t2 - t1)\n    \n    test_split_jsonl = os.path.join(FLAGS.processed_input_dir, \'test_split.jsonl\')\n    dump_examples(test_examples, test_split_jsonl)\n\n    # Load pretrained embeddings.\n    vocab_file = os.path.join(\n        FLAGS.raw_input_dir, ""wikitable_glove_vocab.json"")\n    embedding_file = os.path.join(\n        FLAGS.raw_input_dir, ""wikitable_glove_embedding_mat.npy"")\n    embedding_model = word_embeddings.EmbeddingModel(vocab_file, embedding_file)\n\n    def create_vocab(examples, embedding_model, min_count):\n        token_count = {}\n        for e in examples:\n          for tk in e[\'tokens\']:\n            # Token must be in glove and also appears more than min_count.\n            if find_tk_in_model(tk, embedding_model):\n              try:\n                token_count[tk] += 1\n              except KeyError:\n                token_count[tk] = 1\n        en_vocab = data_utils.generate_vocab_from_token_count(\n          token_count, min_count=min_count)\n        return en_vocab\n\n    for i in xrange(1, 11):\n        en_vocab = create_vocab(\n          train_examples + dev_examples, embedding_model, i)\n        vocab_file = os.path.join(\n          FLAGS.processed_input_dir, ""en_vocab_min_count_{}.json"".format(i))\n        with open(vocab_file, \'w\') as f:\n          json.dump(en_vocab.vocab, f, sort_keys=True, indent=2)\n        print \'min_tk_count: {}, vocab size: {}\'.format(i, len(en_vocab.vocab))    \n\n\n# Copied from utils to avoid relative import.\n# [TODO] Use a cleaner solution.\ndef find_tk_in_model(tk, model):\n    special_tk_dict = {\'-lrb-\': \'(\', \'-rrb-\': \')\'}\n    if tk in model:\n        return [tk]\n    elif tk in special_tk_dict:\n        return [special_tk_dict[tk]]\n    elif tk.upper() in model:\n        return [tk.upper()]\n    elif tk[:1].upper() + tk[1:] in model:\n        return [tk[:1].upper() + tk[1:]]\n    elif re.search(\'\\\\/\', tk):\n        tks = tk.split(\'\\\\\\\\/\')\n        if len(tks) == 1:\n          return []\n        valid_tks = []\n        for tk in tks:\n            valid_tk = find_tk_in_model(tk, model)\n            if valid_tk:\n                valid_tks += valid_tk\n        return valid_tks\n    else:\n        return []\n        \n\nif __name__ == \'__main__\':  \n    tf.app.run()\n'"
