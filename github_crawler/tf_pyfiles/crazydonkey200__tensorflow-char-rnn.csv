file_path,api_count,code
char_rnn_model.py,57,"b'import os\nos.environ[\'TF_CPP_MIN_LOG_LEVEL\']=\'2\'\nimport logging\nimport time\nimport numpy as np\nimport tensorflow as tf\n\n# Disable Tensorflow logging messages.\nlogging.getLogger(\'tensorflow\').setLevel(logging.WARNING)\n\nclass CharRNN(object):\n  """"""Character RNN model.""""""\n  \n  def __init__(self, is_training, batch_size, num_unrollings, vocab_size, \n               hidden_size, max_grad_norm, embedding_size, num_layers,\n               learning_rate, model, dropout=0.0, input_dropout=0.0, use_batch=True):\n    self.batch_size = batch_size\n    self.num_unrollings = num_unrollings\n    if not use_batch:\n      self.batch_size = 1\n      self.num_unrollings = 1\n    self.hidden_size = hidden_size\n    self.vocab_size = vocab_size\n    self.max_grad_norm = max_grad_norm\n    self.num_layers = num_layers\n    self.embedding_size = embedding_size\n    self.model = model\n    self.dropout = dropout\n    self.input_dropout = input_dropout\n    if embedding_size <= 0:\n      self.input_size = vocab_size\n      # Don\'t do dropout on one hot representation.\n      self.input_dropout = 0.0\n    else:\n      self.input_size = embedding_size\n    self.model_size = (embedding_size * vocab_size + # embedding parameters\n                       # lstm parameters\n                       4 * hidden_size * (hidden_size + self.input_size + 1) +\n                       # softmax parameters\n                       vocab_size * (hidden_size + 1) +\n                       # multilayer lstm parameters for extra layers.\n                       (num_layers - 1) * 4 * hidden_size *\n                       (hidden_size + hidden_size + 1))\n    # self.decay_rate = decay_rate\n\n    # Placeholder to feed in input and targets/labels data.\n    self.input_data = tf.placeholder(tf.int64,\n                                     [self.batch_size, self.num_unrollings],\n                                     name=\'inputs\')\n    self.targets = tf.placeholder(tf.int64,\n                                  [self.batch_size, self.num_unrollings],\n                                  name=\'targets\')\n\n    if self.model == \'rnn\':\n      cell_fn = tf.contrib.rnn.BasicRNNCell\n    elif self.model == \'lstm\':\n      cell_fn = tf.contrib.rnn.BasicLSTMCell\n    elif self.model == \'gru\':\n      cell_fn = tf.contrib.rnn.GRUCell\n\n    # params = {\'input_size\': self.input_size}\n    params = {}\n    if self.model == \'lstm\':\n      # add bias to forget gate in lstm.\n      params[\'forget_bias\'] = 0.0\n      params[\'state_is_tuple\'] = True\n    # Create multilayer cell.\n    cell = cell_fn(\n        self.hidden_size, reuse=tf.get_variable_scope().reuse,\n        **params)\n\n    cells = [cell]\n    # params[\'input_size\'] = self.hidden_size\n    # more explicit way to create cells for MultiRNNCell than\n    # [higher_layer_cell] * (self.num_layers - 1)\n    for i in range(self.num_layers-1):\n      higher_layer_cell = cell_fn(\n          self.hidden_size, reuse=tf.get_variable_scope().reuse,\n          **params)\n      cells.append(higher_layer_cell)\n\n    if is_training and self.dropout > 0:\n      cells = [tf.contrib.rnn.DropoutWrapper(\n        cell,\n        output_keep_prob=1.0-self.dropout)\n               for cell in cells]\n\n    multi_cell = tf.contrib.rnn.MultiRNNCell(cells)\n\n    with tf.name_scope(\'initial_state\'):\n      # zero_state is used to compute the intial state for cell.\n      self.zero_state = multi_cell.zero_state(self.batch_size, tf.float32)\n      # Placeholder to feed in initial state.\n      # self.initial_state = tf.placeholder(\n      #   tf.float32,\n      #   [self.batch_size, multi_cell.state_size],\n      #   \'initial_state\')\n\n      self.initial_state = create_tuple_placeholders_with_default(\n        multi_cell.zero_state(batch_size, tf.float32),\n        extra_dims=(None,),\n        shape=multi_cell.state_size)      \n      \n\n    # Embeddings layers.\n    with tf.name_scope(\'embedding_layer\'):\n      if embedding_size > 0:\n        self.embedding = tf.get_variable(\n          \'embedding\', [self.vocab_size, self.embedding_size])\n      else:\n        self.embedding = tf.constant(np.eye(self.vocab_size), dtype=tf.float32)\n\n      inputs = tf.nn.embedding_lookup(self.embedding, self.input_data)\n      if is_training and self.input_dropout > 0:\n        inputs = tf.nn.dropout(inputs, 1 - self.input_dropout)\n\n    with tf.name_scope(\'slice_inputs\'):\n      # Slice inputs into a list of shape [batch_size, 1] data colums.\n      sliced_inputs = [tf.squeeze(input_, [1])\n                       for input_ in tf.split(axis=1, num_or_size_splits=self.num_unrollings, value=inputs)]\n      \n    # Copy cell to do unrolling and collect outputs.\n    outputs, final_state = tf.contrib.rnn.static_rnn(\n      multi_cell, sliced_inputs,\n      initial_state=self.initial_state)\n\n    self.final_state = final_state\n\n    with tf.name_scope(\'flatten_ouputs\'):\n      # Flatten the outputs into one dimension.\n      flat_outputs = tf.reshape(tf.concat(axis=1, values=outputs), [-1, hidden_size])\n\n    with tf.name_scope(\'flatten_targets\'):\n      # Flatten the targets too.\n      flat_targets = tf.reshape(tf.concat(axis=1, values=self.targets), [-1])\n    \n    # Create softmax parameters, weights and bias.\n    with tf.variable_scope(\'softmax\') as sm_vs:\n      softmax_w = tf.get_variable(""softmax_w"", [hidden_size, vocab_size])\n      softmax_b = tf.get_variable(""softmax_b"", [vocab_size])\n      self.logits = tf.matmul(flat_outputs, softmax_w) + softmax_b\n      self.probs = tf.nn.softmax(self.logits)\n\n    with tf.name_scope(\'loss\'):\n      # Compute mean cross entropy loss for each output.\n      loss = tf.nn.sparse_softmax_cross_entropy_with_logits(\n        logits=self.logits, labels=flat_targets)\n      self.mean_loss = tf.reduce_mean(loss)\n\n    with tf.name_scope(\'loss_monitor\'):\n      # Count the number of elements and the sum of mean_loss\n      # from each batch to compute the average loss.\n      count = tf.Variable(1.0, name=\'count\')\n      sum_mean_loss = tf.Variable(1.0, name=\'sum_mean_loss\')\n      \n      self.reset_loss_monitor = tf.group(sum_mean_loss.assign(0.0),\n                                         count.assign(0.0),\n                                         name=\'reset_loss_monitor\')\n      self.update_loss_monitor = tf.group(sum_mean_loss.assign(sum_mean_loss +\n                                                               self.mean_loss),\n                                          count.assign(count + 1),\n                                          name=\'update_loss_monitor\')\n      with tf.control_dependencies([self.update_loss_monitor]):\n        self.average_loss = sum_mean_loss / count\n        self.ppl = tf.exp(self.average_loss)\n\n      # Monitor the loss.\n      loss_summary_name = ""average loss""\n      ppl_summary_name = ""perplexity""\n  \n      average_loss_summary = tf.summary.scalar(loss_summary_name, self.average_loss)\n      ppl_summary = tf.summary.scalar(ppl_summary_name, self.ppl)\n\n    # Monitor the loss.\n    self.summaries = tf.summary.merge([average_loss_summary, ppl_summary],\n                                      name=\'loss_monitor\')\n    \n    self.global_step = tf.get_variable(\'global_step\', [],\n                                       initializer=tf.constant_initializer(0.0))\n\n    self.learning_rate = tf.constant(learning_rate)\n    if is_training:\n      # learning_rate = tf.train.exponential_decay(1.0, self.global_step,\n      #                                            5000, 0.1, staircase=True)\n      tvars = tf.trainable_variables()\n      grads, _ = tf.clip_by_global_norm(tf.gradients(self.mean_loss, tvars),\n                                        self.max_grad_norm)\n      # optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n      # optimizer = tf.train.RMSPropOptimizer(learning_rate, decay_rate)\n      optimizer = tf.train.AdamOptimizer(self.learning_rate)\n\n      self.train_op = optimizer.apply_gradients(zip(grads, tvars),\n                                                global_step=self.global_step)\n      \n  def run_epoch(self, session, data_size, batch_generator, is_training,\n                verbose=0, freq=10, summary_writer=None, debug=False, divide_by_n=1):\n    """"""Runs the model on the given data for one full pass.""""""\n    # epoch_size = ((data_size // self.batch_size) - 1) // self.num_unrollings\n    epoch_size = data_size // (self.batch_size * self.num_unrollings)\n    if data_size % (self.batch_size * self.num_unrollings) != 0:\n        epoch_size += 1\n\n    if verbose > 0:\n        logging.info(\'epoch_size: %d\', epoch_size)\n        logging.info(\'data_size: %d\', data_size)\n        logging.info(\'num_unrollings: %d\', self.num_unrollings)\n        logging.info(\'batch_size: %d\', self.batch_size)\n\n    if is_training:\n      extra_op = self.train_op\n    else:\n      extra_op = tf.no_op()\n\n    # Prepare initial state and reset the average loss\n    # computation.\n    state = session.run(self.zero_state)\n    self.reset_loss_monitor.run()\n    start_time = time.time()\n    for step in range(epoch_size // divide_by_n):\n      # Generate the batch and use [:-1] as inputs and [1:] as targets.\n      data = batch_generator.next()\n      inputs = np.array(data[:-1]).transpose()\n      targets = np.array(data[1:]).transpose()\n\n      ops = [self.average_loss, self.final_state, extra_op,\n             self.summaries, self.global_step, self.learning_rate]\n\n      feed_dict = {self.input_data: inputs, self.targets: targets,\n                   self.initial_state: state}\n\n      results = session.run(ops, feed_dict)\n      average_loss, state, _, summary_str, global_step, lr = results\n      \n      ppl = np.exp(average_loss)\n      if (verbose > 0) and ((step+1) % freq == 0):\n        logging.info(""%.1f%%, step:%d, perplexity: %.3f, speed: %.0f words"",\n                     (step + 1) * 1.0 / epoch_size * 100, step, ppl,\n                     (step + 1) * self.batch_size * self.num_unrollings /\n                     (time.time() - start_time))\n\n    logging.info(""Perplexity: %.3f, speed: %.0f words per sec"",\n                 ppl, (step + 1) * self.batch_size * self.num_unrollings /\n                 (time.time() - start_time))\n    return ppl, summary_str, global_step\n\n  def sample_seq(self, session, length, start_text, vocab_index_dict,\n                 index_vocab_dict, temperature=1.0, max_prob=True):\n\n    state = session.run(self.zero_state)\n\n    # use start_text to warm up the RNN.\n    if start_text is not None and len(start_text) > 0:\n      seq = list(start_text)\n      for char in start_text[:-1]:\n        x = np.array([[char2id(char, vocab_index_dict)]])\n        state = session.run(self.final_state,\n                            {self.input_data: x,\n                             self.initial_state: state})\n      x = np.array([[char2id(start_text[-1], vocab_index_dict)]])\n    else:\n      vocab_size = len(vocab_index_dict.keys())\n      x = np.array([[np.random.randint(0, vocab_size)]])\n      seq = []\n\n    for i in range(length):\n      state, logits = session.run([self.final_state,\n                                   self.logits],\n                                  {self.input_data: x,\n                                   self.initial_state: state})\n      unnormalized_probs = np.exp((logits - np.max(logits)) / temperature)\n      probs = unnormalized_probs / np.sum(unnormalized_probs)\n\n      if max_prob:\n        sample = np.argmax(probs[0])\n      else:\n        sample = np.random.choice(self.vocab_size, 1, p=probs[0])[0]\n\n      seq.append(id2char(sample, index_vocab_dict))\n      x = np.array([[sample]])\n    return \'\'.join(seq)\n      \n        \nclass BatchGenerator(object):\n    """"""Generate and hold batches.""""""\n    def __init__(self, text, batch_size, n_unrollings, vocab_size,\n                 vocab_index_dict, index_vocab_dict):\n      self._text = text\n      self._text_size = len(text)\n      self._batch_size = batch_size\n      self.vocab_size = vocab_size\n      self._n_unrollings = n_unrollings\n      self.vocab_index_dict = vocab_index_dict\n      self.index_vocab_dict = index_vocab_dict\n      \n      segment = self._text_size // batch_size\n\n      # number of elements in cursor list is the same as\n      # batch_size.  each batch is just the collection of\n      # elements in where the cursors are pointing to.\n      self._cursor = [ offset * segment for offset in range(batch_size)]\n      self._last_batch = self._next_batch()\n      \n    def _next_batch(self):\n      """"""Generate a single batch from the current cursor position in the data.""""""\n      batch = np.zeros(shape=(self._batch_size), dtype=np.float)\n      for b in range(self._batch_size):\n        batch[b] = char2id(self._text[self._cursor[b]], self.vocab_index_dict)\n        self._cursor[b] = (self._cursor[b] + 1) % self._text_size\n      return batch\n\n    def next(self):\n      """"""Generate the next array of batches from the data. The array consists of\n      the last batch of the previous array, followed by num_unrollings new ones.\n      """"""\n      batches = [self._last_batch]\n      for step in range(self._n_unrollings):\n        batches.append(self._next_batch())\n      self._last_batch = batches[-1]\n      return batches\n\n\n# Utility functions\ndef batches2string(batches, index_vocab_dict):\n  """"""Convert a sequence of batches back into their (most likely) string\n  representation.""""""\n  s = [\'\'] * batches[0].shape[0]\n  for b in batches:\n    s = [\'\'.join(x) for x in zip(s, id2char_list(b, index_vocab_dict))]\n  return s\n\n\ndef characters(probabilities):\n  """"""Turn a 1-hot encoding or a probability distribution over the possible\n  characters back into its (most likely) character representation.""""""\n  return [id2char(c) for c in np.argmax(probabilities, 1)]\n\n\ndef char2id(char, vocab_index_dict):\n  try:\n    return vocab_index_dict[char]\n  except KeyError:\n    logging.info(\'Unexpected char %s\', char)\n    return 0\n\n\ndef id2char(index, index_vocab_dict):\n  return index_vocab_dict[index]\n\n    \ndef id2char_list(lst, index_vocab_dict):\n  return [id2char(i, index_vocab_dict) for i in lst]\n\n\ndef create_tuple_placeholders_with_default(inputs, extra_dims, shape):\n  if isinstance(shape, int):\n    result = tf.placeholder_with_default(\n      inputs, list(extra_dims) + [shape])\n  else:\n    subplaceholders = [create_tuple_placeholders_with_default(\n      subinputs, extra_dims, subshape)\n                       for subinputs, subshape in zip(inputs, shape)]\n    t = type(shape)\n    if t == tuple:\n      result = t(subplaceholders)\n    else:\n      result = t(*subplaceholders)    \n  return result\n\n        \ndef create_tuple_placeholders(dtype, extra_dims, shape):\n  if isinstance(shape, int):\n    result = tf.placeholder(dtype, list(extra_dims) + [shape])\n  else:\n    subplaceholders = [create_tuple_placeholders(dtype, extra_dims, subshape)\n                       for subshape in shape]\n    t = type(shape)\n\n    # Handles both tuple and LSTMStateTuple.\n    if t == tuple:\n      result = t(subplaceholders)\n    else:\n      result = t(*subplaceholders)\n  return result\n  \n'"
sample.py,5,"b""import os\nos.environ['TF_CPP_MIN_LOG_LEVEL']='2'\n\nimport argparse\nimport codecs\nimport json\nimport os\n\nimport numpy as np\nfrom char_rnn_model import *\nfrom train import load_vocab\n\ndef main():\n    parser = argparse.ArgumentParser()\n    \n    # Parameters for using saved best models.\n    parser.add_argument('--init_dir', type=str, default='',\n                        help='continue from the outputs in the given directory')\n\n    # Parameters for picking which model to use. \n    parser.add_argument('--model_path', type=str, default='',\n                        help='path to the model file like output/best_model/model-40.')\n\n    # Parameters for sampling.\n    parser.add_argument('--temperature', type=float,\n                        default=1.0,\n                        help=('Temperature for sampling from softmax: '\n                              'higher temperature, more random; '\n                              'lower temperature, more greedy.'))\n\n    parser.add_argument('--max_prob', dest='max_prob', action='store_true',\n                        help='always pick the most probable next character in sampling')\n\n    parser.set_defaults(max_prob=False)\n\n    parser.add_argument('--start_text', type=str,\n                        default='The meaning of life is ',\n                        help='the text to start with')\n\n    parser.add_argument('--length', type=int,\n                        default=100,\n                        help='length of sampled sequence')\n\n    parser.add_argument('--seed', type=int,\n                        default=-1,\n                        help=('seed for sampling to replicate results, '\n                              'an integer between 0 and 4294967295.'))\n\n    # Parameters for evaluation (computing perplexity of given text).\n    parser.add_argument('--evaluate', dest='evaluate', action='store_true',\n                        help='compute the perplexity of given text')\n    parser.set_defaults(evaluate=False)\n    parser.add_argument('--example_text', type=str,\n                        default='The meaning of life is 42.',\n                        help='compute the perplexity of given example text.')\n\n    # Parameters for debugging.\n    parser.add_argument('--debug', dest='debug', action='store_true',\n                        help='show debug information')\n    parser.set_defaults(debug=False)\n    \n    args = parser.parse_args()\n\n    # Prepare parameters.\n    with open(os.path.join(args.init_dir, 'result.json'), 'r') as f:\n        result = json.load(f)\n    params = result['params']\n\n    if args.model_path:    \n        best_model = args.model_path\n    else:\n        best_model = result['best_model']\n\n    best_valid_ppl = result['best_valid_ppl']\n    if 'encoding' in result:\n        args.encoding = result['encoding']\n    else:\n        args.encoding = 'utf-8'\n    args.vocab_file = os.path.join(args.init_dir, 'vocab.json')\n    vocab_index_dict, index_vocab_dict, vocab_size = load_vocab(args.vocab_file, args.encoding)\n\n    # Create graphs\n    logging.info('Creating graph')\n    graph = tf.Graph()\n    with graph.as_default():\n        with tf.name_scope('evaluation'):\n            test_model = CharRNN(is_training=False, use_batch=False, **params)\n            saver = tf.train.Saver(name='checkpoint_saver')\n\n    if args.evaluate:\n        example_batches = BatchGenerator(args.example_text, 1, 1, vocab_size,\n                                         vocab_index_dict, index_vocab_dict)\n        with tf.Session(graph=graph) as session:\n            saver.restore(session, best_model)\n            ppl = test_model.run_epoch(session, len(args.example_text),\n                                        example_batches,\n                                        is_training=False)[0]\n            print('Example text is: %s' % args.example_text)\n            print('Perplexity is: %s' % ppl)\n    else:\n        if args.seed >= 0:\n            np.random.seed(args.seed)\n        # Sampling a sequence \n        with tf.Session(graph=graph) as session:\n            saver.restore(session, best_model)\n            sample = test_model.sample_seq(session, args.length, args.start_text,\n                                            vocab_index_dict, index_vocab_dict,\n                                            temperature=args.temperature,\n                                            max_prob=args.max_prob)\n            print('Sampled text is:\\n%s' % sample)\n        return sample\n\nif __name__ == '__main__':\n    main()\n"""
train.py,11,"b""import os\nos.environ['TF_CPP_MIN_LOG_LEVEL']='2'\n\nimport argparse\nimport codecs\nimport json\nimport logging\nimport os\nimport shutil\nimport sys\n\nimport numpy as np\nfrom char_rnn_model import *\nfrom six import iteritems\n\n\ndef main():\n    parser = argparse.ArgumentParser()\n\n    # Data and vocabulary file\n    parser.add_argument('--data_file', type=str,\n                        default='data/tiny_shakespeare.txt',\n                        help='data file')\n\n    parser.add_argument('--encoding', type=str,\n                        default='utf-8',\n                        help='the encoding of the data file.')\n\n    # Parameters for saving models.\n    parser.add_argument('--output_dir', type=str, default='output',\n                        help=('directory to store final and'\n                              ' intermediate results and models.'))\n    parser.add_argument('--n_save', type=int, default=1,\n                        help='how many times to save the model during each epoch.')\n    parser.add_argument('--max_to_keep', type=int, default=5,\n                        help='how many recent models to keep.')\n    \n    # Parameters to configure the neural network.\n    parser.add_argument('--hidden_size', type=int, default=128,\n                        help='size of RNN hidden state vector')\n    parser.add_argument('--embedding_size', type=int, default=0,\n                        help='size of character embeddings')\n    parser.add_argument('--num_layers', type=int, default=2,\n                        help='number of layers in the RNN')\n    parser.add_argument('--num_unrollings', type=int, default=10,\n                        help='number of unrolling steps.')\n    parser.add_argument('--model', type=str, default='lstm',\n                        help='which model to use (rnn, lstm or gru).')\n    \n    # Parameters to control the training.\n    parser.add_argument('--num_epochs', type=int, default=50,\n                        help='number of epochs')\n    parser.add_argument('--batch_size', type=int, default=20,\n                        help='minibatch size')\n    parser.add_argument('--train_frac', type=float, default=0.9,\n                        help='fraction of data used for training.')\n    parser.add_argument('--valid_frac', type=float, default=0.05,\n                        help='fraction of data used for validation.')\n    # test_frac is computed as (1 - train_frac - valid_frac).\n    parser.add_argument('--dropout', type=float, default=0.0,\n                        help='dropout rate, default to 0 (no dropout).')\n\n    parser.add_argument('--input_dropout', type=float, default=0.0,\n                        help=('dropout rate on input layer, default to 0 (no dropout),'\n                              'and no dropout if using one-hot representation.'))\n\n    # Parameters for gradient descent.\n    parser.add_argument('--max_grad_norm', type=float, default=5.,\n                        help='clip global grad norm')\n    parser.add_argument('--learning_rate', type=float, default=2e-3,\n                        help='initial learning rate')\n    parser.add_argument('--decay_rate', type=float, default=0.95,\n                        help='decay rate')\n\n    # Parameters for logging.\n    parser.add_argument('--log_to_file', dest='log_to_file', action='store_true',\n                        help=('whether the experiment log is stored in a file under'\n                              '  output_dir or printed at stdout.'))\n    parser.set_defaults(log_to_file=False)\n    \n    parser.add_argument('--progress_freq', type=int,\n                        default=100,\n                        help=('frequency for progress report in training'\n                              ' and evalution.'))\n\n    parser.add_argument('--verbose', type=int,\n                        default=0,\n                        help=('whether to show progress report in training'\n                              ' and evalution.'))\n\n    # Parameters to feed in the initial model and current best model.\n    parser.add_argument('--init_model', type=str,\n                        default='',\n                        help=('initial model'))\n    parser.add_argument('--best_model', type=str,\n                        default='',\n                        help=('current best model'))\n    parser.add_argument('--best_valid_ppl', type=float,\n                        default=np.Inf,\n                        help=('current valid perplexity'))\n    \n    # Parameters for using saved best models.\n    parser.add_argument('--init_dir', type=str, default='',\n                        help='continue from the outputs in the given directory')\n\n    # Parameters for debugging.\n    parser.add_argument('--debug', dest='debug', action='store_true',\n                        help='show debug information')\n    parser.set_defaults(debug=False)\n\n    # Parameters for unittesting the implementation.\n    parser.add_argument('--test', dest='test', action='store_true',\n                        help=('use the first 1000 character to as data'\n                              ' to test the implementation'))\n    parser.set_defaults(test=False)\n    \n    args = parser.parse_args()\n\n    # Specifying location to store model, best model and tensorboard log.\n    args.save_model = os.path.join(args.output_dir, 'save_model/model')\n    args.save_best_model = os.path.join(args.output_dir, 'best_model/model')\n    args.tb_log_dir = os.path.join(args.output_dir, 'tensorboard_log/')\n    args.vocab_file = ''\n\n    # Create necessary directories.\n    if args.init_dir:\n        args.output_dir = args.init_dir\n    else:\n        if os.path.exists(args.output_dir):\n            shutil.rmtree(args.output_dir)\n        for paths in [args.save_model, args.save_best_model,\n                      args.tb_log_dir]:\n            os.makedirs(os.path.dirname(paths))\n\n    # Specify logging config.\n    if args.log_to_file:\n        args.log_file = os.path.join(args.output_dir, 'experiment_log.txt')\n    else:\n        args.log_file = 'stdout'\n\n    # Set logging file.\n    if args.log_file == 'stdout':\n        logging.basicConfig(stream=sys.stdout,\n                            format='%(asctime)s %(levelname)s:%(message)s', \n                            level=logging.INFO,\n                            datefmt='%I:%M:%S')\n    else:\n        logging.basicConfig(filename=args.log_file,\n                            format='%(asctime)s %(levelname)s:%(message)s', \n                            level=logging.INFO,\n                            datefmt='%I:%M:%S')\n\n    print('=' * 60)\n    print('All final and intermediate outputs will be stored in %s/' % args.output_dir)\n    print('All information will be logged to %s' % args.log_file)\n    print('=' * 60 + '\\n')\n    \n    if args.debug:\n        logging.info('args are:\\n%s', args)\n\n    # Prepare parameters.\n    if args.init_dir:\n        with open(os.path.join(args.init_dir, 'result.json'), 'r') as f:\n            result = json.load(f)\n        params = result['params']\n        args.init_model = result['latest_model']\n        best_model = result['best_model']\n        best_valid_ppl = result['best_valid_ppl']\n        if 'encoding' in result:\n            args.encoding = result['encoding']\n        else:\n            args.encoding = 'utf-8'\n        args.vocab_file = os.path.join(args.init_dir, 'vocab.json')\n    else:\n        params = {'batch_size': args.batch_size,\n                  'num_unrollings': args.num_unrollings,\n                  'hidden_size': args.hidden_size,\n                  'max_grad_norm': args.max_grad_norm,\n                  'embedding_size': args.embedding_size,\n                  'num_layers': args.num_layers,\n                  'learning_rate': args.learning_rate,\n                  'model': args.model,\n                  'dropout': args.dropout,\n                  'input_dropout': args.input_dropout}\n        best_model = ''\n    logging.info('Parameters are:\\n%s\\n', json.dumps(params, sort_keys=True, indent=4))\n\n    # Read and split data.\n    logging.info('Reading data from: %s', args.data_file)\n    with codecs.open(args.data_file, 'r', encoding=args.encoding) as f:\n        text = f.read()\n\n    if args.test:\n        text = text[:1000]\n    logging.info('Number of characters: %s', len(text))\n\n    if args.debug:\n        n = 10        \n        logging.info('First %d characters: %s', n, text[:n])\n\n    logging.info('Creating train, valid, test split')\n    train_size = int(args.train_frac * len(text))\n    valid_size = int(args.valid_frac * len(text))\n    test_size = len(text) - train_size - valid_size\n    train_text = text[:train_size]\n    valid_text = text[train_size:train_size + valid_size]\n    test_text = text[train_size + valid_size:]\n\n    if args.vocab_file:\n        vocab_index_dict, index_vocab_dict, vocab_size = load_vocab(\n          args.vocab_file, args.encoding)\n    else:\n        logging.info('Creating vocabulary')\n        vocab_index_dict, index_vocab_dict, vocab_size = create_vocab(text)\n        vocab_file = os.path.join(args.output_dir, 'vocab.json')\n        save_vocab(vocab_index_dict, vocab_file, args.encoding)\n        logging.info('Vocabulary is saved in %s', vocab_file)\n        args.vocab_file = vocab_file\n\n    params['vocab_size'] = vocab_size\n    logging.info('Vocab size: %d', vocab_size)\n\n    # Create batch generators.\n    batch_size = params['batch_size']\n    num_unrollings = params['num_unrollings']\n    train_batches = BatchGenerator(train_text, batch_size, num_unrollings, vocab_size, \n                                   vocab_index_dict, index_vocab_dict)\n    # valid_batches = BatchGenerator(valid_text, 1, 1, vocab_size,\n    #                                vocab_index_dict, index_vocab_dict)\n    valid_batches = BatchGenerator(valid_text, batch_size, num_unrollings, vocab_size,\n                                   vocab_index_dict, index_vocab_dict)\n\n    test_batches = BatchGenerator(test_text, 1, 1, vocab_size,\n                                  vocab_index_dict, index_vocab_dict)\n\n    if args.debug:\n        logging.info('Test batch generators')\n        logging.info(batches2string(train_batches.next(), index_vocab_dict))\n        logging.info(batches2string(valid_batches.next(), index_vocab_dict))\n        logging.info('Show vocabulary')\n        logging.info(vocab_index_dict)\n        logging.info(index_vocab_dict)\n        \n    # Create graphs\n    logging.info('Creating graph')\n    graph = tf.Graph()\n    with graph.as_default():\n        with tf.name_scope('training'):\n            train_model = CharRNN(is_training=True, use_batch=True, **params)\n        tf.get_variable_scope().reuse_variables()\n        with tf.name_scope('validation'):\n            valid_model = CharRNN(is_training=False, use_batch=True, **params)\n        with tf.name_scope('evaluation'):\n            test_model = CharRNN(is_training=False, use_batch=False, **params)\n            saver = tf.train.Saver(name='checkpoint_saver', max_to_keep=args.max_to_keep)\n            best_model_saver = tf.train.Saver(name='best_model_saver')\n\n    logging.info('Model size (number of parameters): %s\\n', train_model.model_size)\n    logging.info('Start training\\n')\n\n    result = {}\n    result['params'] = params\n    result['vocab_file'] = args.vocab_file\n    result['encoding'] = args.encoding\n\n    try:\n        # Use try and finally to make sure that intermediate\n        # results are saved correctly so that training can\n        # be continued later after interruption.\n        with tf.Session(graph=graph) as session:\n            graph_info = session.graph\n\n            train_writer = tf.summary.FileWriter(args.tb_log_dir + 'train/', graph_info)\n            valid_writer = tf.summary.FileWriter(args.tb_log_dir + 'valid/', graph_info)\n\n            # load a saved model or start from random initialization.\n            if args.init_model:\n                saver.restore(session, args.init_model)\n            else:\n                tf.global_variables_initializer().run()\n            for i in range(args.num_epochs):\n                for j in range(args.n_save):\n                    logging.info(\n                        '=' * 19 + ' Epoch %d: %d/%d' + '=' * 19 + '\\n', i+1, j+1, args.n_save)\n                    logging.info('Training on training set')\n                    # training step\n                    ppl, train_summary_str, global_step = train_model.run_epoch(\n                        session,\n                        train_size,\n                        train_batches,\n                        is_training=True,\n                        verbose=args.verbose,\n                        freq=args.progress_freq,\n                        divide_by_n=args.n_save)\n                    # record the summary\n                    train_writer.add_summary(train_summary_str, global_step)\n                    train_writer.flush()\n                    # save model\n                    saved_path = saver.save(session, args.save_model,\n                                            global_step=train_model.global_step)\n                    logging.info('Latest model saved in %s\\n', saved_path)\n                    logging.info('Evaluate on validation set')\n\n                    # valid_ppl, valid_summary_str, _ = valid_model.run_epoch(\n                    valid_ppl, valid_summary_str, _ = valid_model.run_epoch(\n                        session,\n                        valid_size,\n                        valid_batches, \n                        is_training=False,\n                        verbose=args.verbose,\n                        freq=args.progress_freq)\n\n                    # save and update best model\n                    if (not best_model) or (valid_ppl < best_valid_ppl):\n                        best_model = best_model_saver.save(\n                            session,\n                            args.save_best_model,\n                            global_step=train_model.global_step)\n                        best_valid_ppl = valid_ppl\n                    valid_writer.add_summary(valid_summary_str, global_step)\n                    valid_writer.flush()\n                    logging.info('Best model is saved in %s', best_model)\n                    logging.info('Best validation ppl is %f\\n', best_valid_ppl)\n                    result['latest_model'] = saved_path\n                    result['best_model'] = best_model\n                    # Convert to float because numpy.float is not json serializable.\n                    result['best_valid_ppl'] = float(best_valid_ppl)\n                    result_path = os.path.join(args.output_dir, 'result.json')\n                    if os.path.exists(result_path):\n                        os.remove(result_path)\n                    with open(result_path, 'w') as f:\n                        json.dump(result, f, indent=2, sort_keys=True)\n\n            logging.info('Latest model is saved in %s', saved_path)\n            logging.info('Best model is saved in %s', best_model)\n            logging.info('Best validation ppl is %f\\n', best_valid_ppl)\n            logging.info('Evaluate the best model on test set')\n            saver.restore(session, best_model)\n            test_ppl, _, _ = test_model.run_epoch(session, test_size, test_batches,\n                                                   is_training=False,\n                                                   verbose=args.verbose,\n                                                   freq=args.progress_freq)\n            result['test_ppl'] = float(test_ppl)\n    finally:\n        result_path = os.path.join(args.output_dir, 'result.json')\n        if os.path.exists(result_path):\n            os.remove(result_path)\n        with open(result_path, 'w') as f:\n            json.dump(result, f, indent=2, sort_keys=True)\n\n\ndef create_vocab(text):\n    unique_chars = list(set(text))\n    vocab_size = len(unique_chars)\n    vocab_index_dict = {}\n    index_vocab_dict = {}\n    for i, char in enumerate(unique_chars):\n        vocab_index_dict[char] = i\n        index_vocab_dict[i] = char\n    return vocab_index_dict, index_vocab_dict, vocab_size\n\n\ndef load_vocab(vocab_file, encoding):\n    with codecs.open(vocab_file, 'r', encoding=encoding) as f:\n        vocab_index_dict = json.load(f)\n    index_vocab_dict = {}\n    vocab_size = 0\n    for char, index in iteritems(vocab_index_dict):\n        index_vocab_dict[index] = char\n        vocab_size += 1\n    return vocab_index_dict, index_vocab_dict, vocab_size\n\n\ndef save_vocab(vocab_index_dict, vocab_file, encoding):\n    with codecs.open(vocab_file, 'w', encoding=encoding) as f:\n        json.dump(vocab_index_dict, f, indent=2, sort_keys=True)\n        \nif __name__ == '__main__':\n    main()\n"""
