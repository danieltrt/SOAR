file_path,api_count,code
src/causal_prod2vec.py,16,"b'# Causal-Prod2vec\nfrom __future__ import absolute_import\nfrom __future__ import print_function\n\nimport time\nimport os\nimport tensorflow as tf\nimport numpy as np\nimport utils as ut\nfrom models import CausalProd2Vec\nfrom tensorflow.contrib.tensorboard.plugins import projector\n\n# Hyper-Parameters\nflags = tf.app.flags\nFLAGS = flags.FLAGS\nflags.DEFINE_string(\'data_set\', \'user_prod_dict.skew.\', \'Dataset string.\')\nflags.DEFINE_integer(\'num_products\', 1683, \'How many products in the dataset.\')\nflags.DEFINE_integer(\'num_users\', 944, \'How many users in the dataset.\')\nflags.DEFINE_string(\'adapt_stat\', \'adapt_0\', \'Adapt String.\')\nflags.DEFINE_string(\'model_name\', \'cp2v\', \'Name of the model for saving.\')\nflags.DEFINE_string(\'logging_dir\', \'/tmp/tensorboard\', \'Name of the model for saving.\')\nflags.DEFINE_float(\'learning_rate\', 1.0, \'Initial learning rate.\')\nflags.DEFINE_float(\'l2_pen\', 0.0, \'L2 learning rate penalty.\')\nflags.DEFINE_integer(\'num_epochs\', 1, \'Number of epochs to train.\')\nflags.DEFINE_integer(\'batch_size\', 512, \'How big is a batch of training.\')\nflags.DEFINE_integer(\'num_steps\', 500, \'Number of steps after which to test.\')\nflags.DEFINE_bool(\'early_stopping_enabled\', False, \'Enable early stopping.\')\nflags.DEFINE_bool(\'plot_gradients\', False, \'Plot the gradients in Tensorboard.\')\nflags.DEFINE_integer(\'early_stopping\', 200, \'Tolerance for early stopping (# of epochs).\')\nflags.DEFINE_integer(\'seed\', 123, \'Set for reproducibility.\')\nflags.DEFINE_integer(\'embedding_size\', 50, \'Size of each embedding vector.\')\nflags.DEFINE_float(\'cf_pen\', 1.0, \'Imbalance loss.\')\nflags.DEFINE_string(\'cf_distance\', \'l1\', \'Use L1 or L2 for the loss .\')\n\ntrain_data_set_location = ""./Data/"" + FLAGS.data_set +  ""train."" + FLAGS.adapt_stat + "".csv"" # Location of train dataset\ntest_data_set_location = ""./Data/"" + FLAGS.data_set +  ""test."" + FLAGS.adapt_stat + "".csv"" # Location of the test dataset\nvalidation_test_set_location = ""./Data/"" + FLAGS.data_set +  ""valid_test."" + FLAGS.adapt_stat + "".csv"" # Location of the validation dataset\nvalidation_train_set_location = ""./Data/"" + FLAGS.data_set +  ""valid_train."" + FLAGS.adapt_stat + "".csv"" #Location of the validation dataset\n\nmodel_name = FLAGS.model_name + "".ckpt""\ncost_val = []\n\n# Create graph object\ngraph = tf.Graph()\nwith graph.as_default():\n\n    with tf.device(\'/cpu:0\'):\n\n        tf.set_random_seed(FLAGS.seed)\n        \n        # Create the model object\n        model = CausalProd2Vec(FLAGS)\n\n        # Get train data batch from queue\n        next_batch = ut.load_train_dataset(train_data_set_location, FLAGS.batch_size, FLAGS.num_epochs)\n        test_user_batch, test_product_batch, test_label_batch, test_cr = ut.load_test_dataset(test_data_set_location)\n        val_test_user_batch, val_test_product_batch, val_test_label_batch, val_cr = ut.load_test_dataset(validation_test_set_location)\n        val_train_user_batch, val_train_product_batch, val_train_label_batch, val_cr = ut.load_test_dataset(validation_train_set_location)\n\n        # create the empirical CR test logits \n        test_logits = np.empty(len(test_label_batch))\n        test_logits.fill(test_cr)\n\n# Launch the Session\nwith tf.Session(graph=graph, config=tf.ConfigProto(allow_soft_placement=True, log_device_placement=False)) as sess:\n\n    # initialise all the TF variables\n    init_op = tf.global_variables_initializer()\n    sess.run(init_op)\n\n    # Plot the gradients if required.\n    if FLAGS.plot_gradients:\n        # Create summaries to visualize weights\n        for var in tf.trainable_variables():\n            tf.summary.histogram(var.name, var)\n        # Summarize all gradients\n        for grad, var in model.grads:\n            tf.summary.histogram(var.name + \'/gradient\', grad)\n\n    # Setup tensorboard\n    time_tb = str(time.ctime(int(time.time())))\n    train_writer = tf.summary.FileWriter(\'/tmp/tensorboard\' + \'/train\' + time_tb, sess.graph)\n    test_writer = tf.summary.FileWriter(\'/tmp/tensorboard\' + \'/test\' + time_tb, sess.graph)\n    merged = tf.summary.merge_all()\n\n    # Embeddings viz (Possible to add labels for embeddings later)\n    saver = tf.train.Saver()\n    config = projector.ProjectorConfig()\n    embedding = config.embeddings.add()\n    embedding.tensor_name = model.product_embeddings.name\n    projector.visualize_embeddings(train_writer, config)\n\n    # Variables used in the training loop\n    t = time.time()\n    step = 0\n    average_loss = 0\n    average_mse_loss = 0\n    average_log_loss = 0\n\n    # Start the training loop---------------------------------------------------------------------------------------------\n    print(""Starting Training On Causal Prod2Vec"")\n    print(""Num Epochs = "", FLAGS.num_epochs)\n    print(""Learning Rate = "", FLAGS.learning_rate)\n    try:\n        while True:\n\n            # Run the TRAIN for this step batch ---------------------------------------------------------------------\n            with tf.device(\'/cpu:0\'):\n                # Construct the feed_dict\n                user_batch, product_batch, label_batch = sess.run(next_batch)\n                feed_dict = {model.user_list_placeholder : user_batch, model.product_list_placeholder: product_batch, model.label_list_placeholder: label_batch}\n\n                # Run the graph\n                _, sum_str, loss_val, mse_loss_val, log_loss_val = sess.run([model.apply_grads, merged, model.loss, model.mse_loss, model.log_loss], feed_dict=feed_dict)\n                \n            step +=1\n            average_loss += loss_val\n            average_mse_loss += mse_loss_val\n            average_log_loss += log_loss_val\n\n            # Every num_steps print average loss\n            if step % FLAGS.num_steps == 0:\n                if step > FLAGS.num_steps:\n                    # The average loss is an estimate of the loss over the last set batches.\n                    average_loss /= FLAGS.num_steps\n                    average_mse_loss /= FLAGS.num_steps\n                    average_log_loss /= FLAGS.num_steps\n                print(""Average Training Loss on S_c (FULL, MSE, NLL) at step "", step, "": "", average_loss, "": "", average_mse_loss, "": "", average_log_loss, ""Time taken (S) = "" + str(round(time.time() - t, 1)))\n\n                average_loss = 0\n                t = time.time() # reset the time\n                train_writer.add_summary(sum_str, step) # Write the summary\n\n                # Run the VALIDATION for this step batch ---------------------------------------------------------------------\n                feed_dict_test = {model.user_list_placeholder : val_test_user_batch, model.product_list_placeholder: val_test_product_batch, model.label_list_placeholder: val_test_label_batch}\n                feed_dict_train = {model.user_list_placeholder : val_train_user_batch, model.product_list_placeholder: val_train_product_batch, model.label_list_placeholder: val_train_label_batch}\n                \n                sum_str, loss_val, mse_loss_val, log_loss_val = sess.run([merged, model.loss, model.mse_loss, model.log_loss], feed_dict=feed_dict_test)\n                cost_val.append(loss_val)\n                print(""Validation loss on S_t(FULL, MSE, NLL) at step "", step, "": "", loss_val, "": "", mse_loss_val, "": "", log_loss_val)\n\n                sum_str, loss_val, mse_loss_val, log_loss_val = sess.run([merged, model.loss, model.mse_loss, model.log_loss], feed_dict=feed_dict_train)\n                print(""Validation loss on S_c (FULL, MSE, NLL) at step "", step, "": "", loss_val, "": "", mse_loss_val, "": "", log_loss_val)\n                print(""####################################################################################################################"")   \n                \n                test_writer.add_summary(sum_str, step) # Write the summary\n\n                # If condition for the early stopping condition\n                if FLAGS.early_stopping_enabled and step > FLAGS.early_stopping and cost_val[-1] > np.mean(cost_val[-(FLAGS.early_stopping+1):-1]):\n                    print(""Early stopping..."")\n                    saver.save(sess, os.path.join(FLAGS.logging_dir, model_name), model.global_step) # Save model\n                    break\n\n    except tf.errors.OutOfRangeError:\n        print(""Reached the required number of epochs"")\n\n    finally:\n        with tf.device(\'/cpu:0\'):\n            saver.save(sess, os.path.join(FLAGS.logging_dir, model_name), model.global_step) # Save model\n\n    train_writer.close()\n    print(""Training Complete"")\n\n    # Run the test set for the trained model --------------------------------------------------------------------------\n    print(""Running Test Set"")\n    feed_dict = {model.user_list_placeholder : test_user_batch, model.product_list_placeholder: test_product_batch, model.label_list_placeholder: test_label_batch}\n    loss_val, mse_loss_val, log_loss_val = sess.run([model.loss, model.mse_loss, model.log_loss], feed_dict=feed_dict)\n    print(""Test loss (CE, MSE, NLL) = "", loss_val, "": "", mse_loss_val , "": "",log_loss_val)\n\n    # Run the bootstrap for this model ---------------------------------------------------------------------------------------------------------------\n    print(""Begin Bootstrap process..."")\n    ut.compute_bootstraps(sess, model, test_user_batch, test_product_batch, test_label_batch, test_logits, model.ap_mse_loss, model.ap_log_loss)'"
src/causal_prod2vec2i.py,16,"b'# Causal-Prod2vec2i \nfrom __future__ import absolute_import\nfrom __future__ import print_function\n\nimport time\nimport os\nimport tensorflow as tf\nimport numpy as np\nimport utils as ut\nfrom models import CausalProd2Vec2i\nfrom tensorflow.contrib.tensorboard.plugins import projector\n\n# Hyper-Parameters\nflags = tf.app.flags\nFLAGS = flags.FLAGS\nflags.DEFINE_string(\'data_set\', \'user_prod_dict.skew.\', \'Dataset string.\')\nflags.DEFINE_integer(\'num_products\', 1683, \'How many products in the dataset.\')\nflags.DEFINE_integer(\'num_users\', 944, \'How many users in the dataset.\')\nflags.DEFINE_string(\'adapt_stat\', \'adapt_2i\', \'Adapt String.\')\nflags.DEFINE_string(\'model_name\', \'cp2v\', \'Name of the model for saving.\')\nflags.DEFINE_string(\'logging_dir\', \'/tmp/tensorboard\', \'Name of the model for saving.\')\nflags.DEFINE_float(\'learning_rate\', 1.0, \'Initial learning rate.\')\nflags.DEFINE_float(\'l2_pen\', 0.0, \'L2 learning rate penalty.\')\nflags.DEFINE_integer(\'num_epochs\', 1, \'Number of epochs to train.\')\nflags.DEFINE_integer(\'batch_size\', 512, \'How big is a batch of training.\')\nflags.DEFINE_integer(\'num_steps\', 500, \'Number of steps after which to test.\')\nflags.DEFINE_bool(\'early_stopping_enabled\', False, \'Enable early stopping.\')\nflags.DEFINE_bool(\'plot_gradients\', False, \'Plot the gradients in Tensorboard.\')\nflags.DEFINE_integer(\'early_stopping\', 200, \'Tolerance for early stopping (# of epochs).\')\nflags.DEFINE_integer(\'seed\', 123, \'Set for reproducibility.\')\nflags.DEFINE_integer(\'embedding_size\', 50, \'Size of each embedding vector.\')\nflags.DEFINE_float(\'cf_pen\', 1.0, \'Imbalance loss.\')\nflags.DEFINE_string(\'cf_distance\', \'l1\', \'Use L1 or L2 for the loss .\')\n\ntrain_data_set_location = ""./Data/"" + FLAGS.data_set +  ""train."" + FLAGS.adapt_stat + "".csv"" # Location of train dataset\ntest_data_set_location = ""./Data/"" + FLAGS.data_set +  ""test."" + FLAGS.adapt_stat + "".csv"" # Location of the test dataset\nvalidation_test_set_location = ""./Data/"" + FLAGS.data_set +  ""valid_test."" + FLAGS.adapt_stat + "".csv"" # Location of the validation dataset\nvalidation_train_set_location = ""./Data/"" + FLAGS.data_set +  ""valid_train."" + FLAGS.adapt_stat + "".csv"" #Location of the validation dataset\n\nmodel_name = FLAGS.model_name + "".ckpt""\ncost_val = []\nFLAGS.num_products = FLAGS.num_products*2\n\n# Create graph object\ngraph = tf.Graph()\nwith graph.as_default():\n\n    with tf.device(\'/cpu:0\'):\n\n        tf.set_random_seed(FLAGS.seed)\n        \n        # Load the model\n        model = CausalProd2Vec2i(FLAGS)\n\n        # Get train data batch from queue\n        next_batch = ut.load_train_dataset(train_data_set_location, FLAGS.batch_size, FLAGS.num_epochs)\n        test_user_batch, test_product_batch, test_label_batch, test_cr = ut.load_test_dataset(test_data_set_location)\n        val_test_user_batch, val_test_product_batch, val_test_label_batch, val_cr = ut.load_test_dataset(validation_test_set_location)\n        val_train_user_batch, val_train_product_batch, val_train_label_batch, val_cr = ut.load_test_dataset(validation_train_set_location)\n\n        # create the empirical CR test logits \n        test_logits = np.empty(len(test_label_batch))\n        test_logits.fill(test_cr)\n\n# Launch the Session\nwith tf.Session(graph=graph, config=tf.ConfigProto(allow_soft_placement=True, log_device_placement=False)) as sess:\n\n    # initialise all the TF variables\n    init_op = tf.global_variables_initializer()\n    sess.run(init_op)\n\n    # Plot the gradients if required.\n    if FLAGS.plot_gradients:\n        # Create summaries to visualize weights\n        for var in tf.trainable_variables():\n            tf.summary.histogram(var.name, var)\n        # Summarize all gradients\n        for grad, var in model.grads:\n            tf.summary.histogram(var.name + \'/gradient\', grad)\n\n    # Setup tensorboard\n    time_tb = str(time.ctime(int(time.time())))\n    train_writer = tf.summary.FileWriter(\'/tmp/tensorboard\' + \'/train\' + time_tb, sess.graph)\n    test_writer = tf.summary.FileWriter(\'/tmp/tensorboard\' + \'/test\' + time_tb, sess.graph)\n    merged = tf.summary.merge_all()\n\n    # Embeddings viz (Possible to add labels for embeddings later)\n    saver = tf.train.Saver()\n    config = projector.ProjectorConfig()\n    embedding = config.embeddings.add()\n    embedding.tensor_name = model.product_embeddings.name\n    projector.visualize_embeddings(train_writer, config)\n\n    # Variables used in the training loop\n    t = time.time()\n    step = 0\n    average_loss = 0\n    average_mse_loss = 0\n    average_log_loss = 0\n\n    # Start the training loop---------------------------------------------------------------------------------------------\n    print(""Starting Training On Causal Prod2Vec"")\n    print(""Num Epochs = "", FLAGS.num_epochs)\n    print(""Learning Rate = "", FLAGS.learning_rate)\n    print(""L2 Reg = "", FLAGS.l2_pen)\n\n    try:\n        while True:\n\n            # Run the TRAIN for this step batch ---------------------------------------------------------------------\n            with tf.device(\'/cpu:0\'):\n                # Construct the feed_dict\n                user_batch, product_batch, label_batch = sess.run(next_batch)\n                # Treatment is the small set of samples from St, Control is the larger set of samples from Sc\n                reg_ids = ut.compute_2i_regularization_id(product_batch, (FLAGS.num_products/2)) # Compute the product ID\'s for regularization\n                feed_dict = {model.user_list_placeholder : user_batch, model.product_list_placeholder: product_batch, model.reg_list_placeholder: reg_ids, model.label_list_placeholder: label_batch}\n            \n                # Run the graph\n                _, sum_str, loss_val, mse_loss_val, log_loss_val = sess.run([model.apply_grads, merged, model.loss, model.mse_loss, model.log_loss], feed_dict=feed_dict)\n\n            step +=1\n            average_loss += loss_val\n            average_mse_loss += mse_loss_val\n            average_log_loss += log_loss_val\n\n            # Every num_steps print average loss\n            if step % FLAGS.num_steps == 0:\n                if step > FLAGS.num_steps:\n                    # The average loss is an estimate of the loss over the last set batches.\n                    average_loss /= FLAGS.num_steps\n                    average_mse_loss /= FLAGS.num_steps\n                    average_log_loss /= FLAGS.num_steps\n                print(""Average Training Loss on S_c (FULL, MSE, NLL) at step "", step, "": "", average_loss, "": "", average_mse_loss, "": "", average_log_loss, ""Time taken (S) = "" + str(round(time.time() - t, 1)))\n\n                average_loss = 0\n                t = time.time() # reset the time\n                train_writer.add_summary(sum_str, step) # Write the summary\n\n                # Run the VALIDATION for this step batch ---------------------------------------------------------------------\n                val_train_product_batch = np.asarray(val_train_product_batch, dtype=np.float32)\n                val_test_product_batch = np.asarray(val_test_product_batch, dtype=np.float32)\n\n                vaL_train_reg_ids = ut.compute_2i_regularization_id(val_train_product_batch, (FLAGS.num_products/2)) # Compute the product ID\'s for regularization\n                vaL_test_reg_ids = ut.compute_2i_regularization_id(val_test_product_batch, (FLAGS.num_products/2)) # Compute the product ID\'s for regularization\n\n                feed_dict_test = {model.user_list_placeholder : val_test_user_batch, model.product_list_placeholder: val_test_product_batch, model.reg_list_placeholder: vaL_test_reg_ids,  model.label_list_placeholder: val_test_label_batch}\n                feed_dict_train = {model.user_list_placeholder : val_train_user_batch, model.product_list_placeholder: val_train_product_batch, model.reg_list_placeholder: vaL_train_reg_ids, model.label_list_placeholder: val_train_label_batch}\n\n                # Run TEST distribution validation \n                sum_str, loss_val, mse_loss_val, log_loss_val = sess.run([merged, model.loss, model.mse_loss, model.log_loss], feed_dict=feed_dict_test)\n                cost_val.append(loss_val)\n                print(""Validation loss on S_t(FULL, MSE, NLL) at step "", step, "": "", loss_val, "": "", mse_loss_val, "": "", log_loss_val)\n\n                # Run TRAIN distribution validation \n                sum_str, loss_val, mse_loss_val, log_loss_val = sess.run([merged, model.loss, model.mse_loss, model.log_loss], feed_dict=feed_dict_train)\n                print(""Validation loss on S_c (FULL, MSE, NLL) at step "", step, "": "", loss_val, "": "", mse_loss_val, "": "", log_loss_val)\n                print(""####################################################################################################################"")   \n          \n                test_writer.add_summary(sum_str, step) # Write the summary\n\n                # If condition for the early stopping condition\n                if FLAGS.early_stopping_enabled and step > FLAGS.early_stopping and cost_val[-1] > np.mean(cost_val[-(FLAGS.early_stopping+1):-1]):\n                    print(""Early stopping..."")\n                    saver.save(sess, os.path.join(FLAGS.logging_dir , model_name), model.global_step) # Save model\n                    break\n                    \n    except tf.errors.OutOfRangeError:\n        print(""Reached the number of epochs"")\n\n    finally:\n        with tf.device(\'/cpu:0\'):\n            saver.save(sess, os.path.join(FLAGS.logging_dir, model_name), model.global_step) # Save model\n\n    train_writer.close()\n    print(""Training Complete"")\n\n    # Run the test set for the trained model --------------------------------------------------------------------------\n\n    # If we want to use the Treatment embedding here we can just add productid_size to the test_product_ids. \n    #test_product_batch = [int(x) + productid_size for x in test_product_batch]\n\n    print(""Running Test Set"")\n    feed_dict = {model.user_list_placeholder : test_user_batch, model.product_list_placeholder: test_product_batch, model.reg_list_placeholder: test_product_batch, model.label_list_placeholder: test_label_batch}\n    loss_val, mse_loss_val, log_loss_val = sess.run([model.loss, model.mse_loss, model.log_loss], feed_dict=feed_dict)\n    print(""Test loss (CE, MSE, NLL) = "", loss_val, "": "", mse_loss_val , "": "",log_loss_val)\n\n    # Run the bootstrap for this model ---------------------------------------------------------------------------------------------------------------\n    print(""Begin Bootstrap process..."")\n    print(""Running BootStrap On The Control Representations"")\n    ut.compute_bootstraps_2i(sess, model, test_user_batch, test_product_batch, test_label_batch, test_logits, model.ap_mse_loss, model.ap_log_loss)\n\n    print(""Running BootStrap On The Treatment Representations"")\n    test_product_batch = [int(x) + (FLAGS.num_products/2) for x in test_product_batch]\n    ut.compute_bootstraps_2i(sess, model, test_user_batch, test_product_batch, test_label_batch, test_logits, model.ap_mse_loss, model.ap_log_loss)'"
src/models.py,58,"b'from __future__ import absolute_import\nfrom __future__ import print_function\n\nimport tensorflow as tf\n\nclass SupervisedProd2vec():\n    def __init__(self, FLAGS):\n        super().__init__()\n\n        self.num_users = FLAGS.num_users\n        self.num_products = FLAGS.num_products\n        self.embedding_dim = FLAGS.embedding_size\n        self.l2_pen = FLAGS.l2_pen\n        self.learning_rate = FLAGS.learning_rate\n        self.plot_gradients = FLAGS.plot_gradients\n        self.cf_pen = FLAGS.cf_pen\n        self.cf_distance = FLAGS.cf_distance\n        self.cf_loss = 0\n\n        # Build the graph\n        self.create_placeholders()\n        self.build_graph()\n        self.create_control_embeddings()\n        self.create_counter_factual_loss()\n        self.create_losses()\n        self.add_optimizer()\n        self.add_average_predictor()\n        self.add_summaries()\n\n    def create_placeholders(self):\n        """"""Create the placeholders to be used """"""\n        \n        self.user_list_placeholder = tf.placeholder(tf.int32, [None], name=""user_list_placeholder"")\n        self.product_list_placeholder = tf.placeholder(tf.int32, [None], name=""product_list_placeholder"")\n        self.label_list_placeholder = tf.placeholder(tf.float32, [None, 1], name=""label_list_placeholder"")\n        self.reg_list_placeholder = tf.placeholder(tf.int32, [None], name=""reg_list_placeholder"")\n\n        # logits placeholder used to store the test CR for the bootstrapping process\n        self.logits_placeholder = tf.placeholder(tf.float32, [None], name=""logits_placeholder"")\n\n    def build_graph(self):\n        """"""Build the main tensorflow graph with embedding layers""""""\n\n        with tf.name_scope(\'embedding_layer\'):\n            # User matrix and current batch\n            self.user_embeddings = tf.get_variable(""user_embeddings"", shape=[self.num_users, self.embedding_dim], initializer=tf.contrib.layers.xavier_initializer(), trainable=True)\n            self.user_embed = tf.nn.embedding_lookup(self.user_embeddings, self.user_list_placeholder)\n            self.user_b = tf.Variable(tf.zeros([self.num_users]), name=\'user_b\', trainable=True)\n            self.user_bias_embed = tf.nn.embedding_lookup(self.user_b, self.user_list_placeholder)\n\n            # Product embedding\n            self.product_embeddings = tf.get_variable(""product_embeddings"", shape=[self.num_products, self.embedding_dim], initializer=tf.contrib.layers.xavier_initializer(), trainable=True)\n            self.product_embed = tf.nn.embedding_lookup(self.product_embeddings, self.product_list_placeholder)\n            self.prod_b = tf.Variable(tf.zeros([self.num_products]), name=\'prod_b\', trainable=True)\n            self.prod_bias_embed = tf.nn.embedding_lookup(self.prod_b, self.product_list_placeholder)\n\n        with tf.variable_scope(\'logits\'):\n\n            self.global_bias = tf.get_variable(\'global_bias\', [1], initializer=tf.constant_initializer(0.0, dtype=tf.float32), trainable=True)\n            self.alpha = tf.get_variable(\'alpha\', [], initializer=tf.constant_initializer(0.00000001, dtype=tf.float32), trainable=True)\n            emb_logits = self.alpha * tf.reshape(tf.reduce_sum(tf.multiply(self.user_embed, self.product_embed), 1), [tf.shape(self.user_list_placeholder)[0], 1])\n            logits = tf.reshape(tf.add(self.prod_bias_embed, self.user_bias_embed), [tf.shape(self.user_list_placeholder)[0], 1]) + self.global_bias\n            self.logits = emb_logits + logits\n\n            self.prediction = tf.sigmoid(self.logits, name=\'sigmoid_prediction\')\n\n    def create_control_embeddings(self):\n        """"""Create the control embeddings""""""\n\n        pass\n\n    def create_counter_factual_loss(self):\n        """"""Create the counter factual loss to add to main loss""""""\n\n        pass\n\n    def create_losses(self):\n        """"""Create the losses""""""\n\n        with tf.name_scope(\'losses\'):\n            # Sigmoid loss between the logits and labels\n            self.log_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=self.logits, labels=self.label_list_placeholder))\n\n            # Adding the regularizer term on user vct and prod vct and their bias terms\n            reg_term = self.l2_pen * ( tf.nn.l2_loss(self.user_embeddings) + tf.nn.l2_loss(self.product_embeddings) )\n            reg_term_biases = self.l2_pen * ( tf.nn.l2_loss(self.prod_b) + tf.nn.l2_loss(self.user_b) )\n            self.factual_loss = self.log_loss + reg_term + reg_term_biases\n\n            # Adding the counter-factual loss\n            self.loss = self.factual_loss + (self.cf_pen * self.cf_loss) # Imbalance loss\n            self.mse_loss = tf.losses.mean_squared_error(labels=self.label_list_placeholder, predictions=self.prediction)\n\n    def add_optimizer(self):\n        """"""Add the required optimiser to the graph""""""\n\n        with tf.name_scope(\'optimizer\'):\n            # Global step variable to keep track of the number of training steps\n            self.global_step = tf.Variable(0, dtype=tf.int32, trainable=False, name=\'global_step\')\n\n            # Gradient Descent\n            if self.plot_gradients:\n                optimizer = tf.train.GradientDescentOptimizer(self.learning_rate)\n                # Op to calculate every variable gradient\n                grads = tf.gradients(self.loss, tf.trainable_variables())\n                grads = list(zip(grads, tf.trainable_variables()))\n                # Op to update all variables according to their gradient\n                self.apply_grads = optimizer.apply_gradients(grads_and_vars=grads)\n            else:\n                self.apply_grads = tf.train.GradientDescentOptimizer(self.learning_rate).minimize(self.loss, global_step=self.global_step)\n\n    def add_average_predictor(self):\n        """"""""Add the average predictors to the graph""""""\n\n        with tf.variable_scope(\'ap_logits\'):\n\n            ap_logits = tf.reshape(self.logits_placeholder, [tf.shape(self.label_list_placeholder)[0], 1])\n\n        with tf.name_scope(\'ap_losses\'):\n            \n            self.ap_mse_loss = tf.losses.mean_squared_error(labels=self.label_list_placeholder, predictions=ap_logits)\n            self.ap_log_loss =  tf.losses.log_loss(labels=self.label_list_placeholder, predictions=ap_logits)\n\n    def add_summaries(self):\n        """"""Add the required summaries to the graph""""""\n\n        with tf.name_scope(\'summaries\'):\n            # Add loss to the summaries\n            tf.summary.scalar(\'total_loss\', self.loss)\n            tf.summary.histogram(\'histogram_total_loss\', self.loss)\n\n            # Add weights to the summaries\n            tf.summary.histogram(\'user_embedding_weights\', self.user_embeddings)\n            tf.summary.histogram(\'product_embedding_weights\', self.product_embeddings)\n            tf.summary.histogram(\'logits\', self.logits)\n            tf.summary.histogram(\'prod_b\', self.prod_b)\n            tf.summary.histogram(\'user_b\', self.user_b)\n            tf.summary.histogram(\'global_bias\', self.global_bias)\n\n            tf.summary.scalar(\'alpha\', self.alpha)\n\nclass CausalProd2Vec(SupervisedProd2vec):\n    def __init__(self, FLAGS):\n\n        super().__init__(FLAGS)\n        \n    def create_control_embeddings(self):\n        """"""Create the control embeddings""""""\n\n        with tf.name_scope(\'control_embedding\'):\n            # Get the control embedding at id 0\n            self.control_embed = tf.stop_gradient(tf.nn.embedding_lookup(self.product_embeddings, 0))\n    \n    def create_counter_factual_loss(self):\n        """"""Create the counter factual loss to add to main loss""""""\n        \n        with tf.name_scope(\'counter_factual\'):\n            # Take the mean of the difference between treatment and the control embedding\n            if self.cf_distance == ""l1"":\n                print(""Using L1 difference between treatment and control embeddings"")\n                self.cf_loss = tf.reduce_mean(tf.reduce_sum(tf.abs(tf.subtract(tf.nn.l2_normalize(self.product_embed, axis=1), tf.nn.l2_normalize(self.control_embed, axis=0))), axis=1))\n            \n            elif self.cf_distance == ""l2"":\n                print(""Using L2 difference between treatment and control embeddings"")\n                self.cf_loss = tf.sqrt(tf.reduce_sum(tf.square(tf.subtract(tf.nn.l2_normalize(self.product_embed,axis=1), tf.nn.l2_normalize(self.control_embed,axis=0)))))\n\n            elif self.cf_distance == ""cos"":\n                print(""Using Cosine difference between treatment and control embeddings"")\n                self.cf_loss = tf.losses.cosine_distance(tf.nn.l2_normalize(self.control_embed,axis=0), tf.nn.l2_normalize(self.product_embed,axis=1), axis=0)\n\nclass CausalProd2Vec2i(SupervisedProd2vec):\n    def __init__(self, FLAGS):\n\n        super().__init__(FLAGS)\n\n    def create_control_embeddings(self):\n        """"""Create the control embeddings""""""\n\n        with tf.name_scope(\'control_embedding\'):\n            # Get the treatment representations for the products\n            self.control_embed = tf.stop_gradient(tf.nn.embedding_lookup(self.product_embeddings, self.reg_list_placeholder))\n            \n    def create_counter_factual_loss(self):\n        """"""Create the counter factual loss to add to main loss""""""\n        \n        with tf.name_scope(\'counter_factual\'):\n            # Take the mean of the difference between treatment and the control embedding\n            if self.cf_distance == ""l1"":\n                print(""Using L1 difference between treatment and control embeddings"")\n                self.cf_loss = tf.reduce_mean(tf.reduce_sum(tf.abs(tf.subtract(self.product_embed, self.control_embed)), axis=1))\n            \n            elif self.cf_distance == ""l2"":\n                print(""Using L2 difference between treatment and control embeddings"")\n                self.cf_loss = tf.sqrt(tf.reduce_sum(tf.square(tf.subtract(tf.nn.l2_normalize(self.product_embed,axis=1), tf.nn.l2_normalize(self.control_embed,axis=0)))))\n\n            elif self.cf_distance == ""cos"":\n                print(""Using Cosine difference between treatment and control embeddings"")\n                self.cf_loss = tf.losses.cosine_distance(tf.nn.l2_normalize(self.control_embed,axis=0), tf.nn.l2_normalize(self.product_embed,axis=1), axis=0)'"
src/utils.py,3,"b'from __future__ import absolute_import, print_function\n\nimport csv\nimport random\n\nimport numpy as np\nimport tensorflow as tf\nfrom sklearn import metrics\n\ndef load_train_dataset(dataset_location, batch_size, num_epochs):\n    """"""Load the training data using TF Dataset API""""""\n\n    with tf.name_scope(\'train_dataset_loading\'):\n\n        record_defaults = [[1], [1], [0.]] # Sets the type of the resulting tensors and default values\n        # Dataset is in the format - UserID ProductID Rating\n        dataset = tf.data.TextLineDataset(dataset_location).map(lambda line: tf.decode_csv(line, record_defaults=record_defaults))\n        dataset = dataset.shuffle(buffer_size=10000)\n        dataset = dataset.batch(batch_size)\n        dataset = dataset.prefetch(5)\n        dataset = dataset.cache()\n        dataset = dataset.repeat(num_epochs)\n        iterator = dataset.make_one_shot_iterator()\n        user_batch, product_batch, label_batch = iterator.get_next()\n        label_batch = tf.expand_dims(label_batch, 1)\n\n    return user_batch, product_batch, label_batch\n\ndef load_test_dataset(dataset_location):\n    """"""Load the test and validation datasets""""""\n\n    user_list = []\n    product_list = []\n    labels = []\n\n    with open(dataset_location, \'r\') as f:\n        reader = csv.reader(f)\n        for row in reader:\n            user_list.append(row[0])\n            product_list.append(row[1])\n            labels.append(row[2])\n\n    labels = np.reshape(labels, [-1, 1])\n    cr = compute_empircal_cr(labels)\n\n    return user_list, product_list, labels, cr\n\ndef generate_bootstrap_batch(seed, data_set_size):\n    """"""Generate the IDs for the bootstap""""""\n\n    random.seed(seed)\n    ids = [random.randint(0, data_set_size-1) for j in range(int(data_set_size*0.8))]\n\n    return ids\n\ndef compute_empircal_cr(labels):\n    """"""Compute the cr from the empirical data""""""\n\n    labels = labels.astype(np.float)\n    clicks = np.count_nonzero(labels)\n    views = len(np.where(labels==0)[0])\n    cr = float(clicks)/float(views)\n\n    return cr\n\ndef compute_2i_regularization_id(prods, num_products):\n    """"""Compute the ID for the regularization for the 2i approach""""""\n\n    reg_ids = []\n    # Loop through batch and compute if the product ID is greater than the number of products\n    for x in np.nditer(prods):\n        if x >= num_products:\n            reg_ids.append(x)\n        elif x < num_products:\n            reg_ids.append(x + num_products) # Add number of products to create the 2i representation \n\n    return np.asarray(reg_ids)\n\ndef compute_treatment_or_control(prods, num_products):\n    """"""Compute if product is in treatment or control""""""\n    # Return the control product places and treatment places as 1\'s in a binary matrix.\n    ids = []\n    for x in np.nditer(prods):\n        # Greater than the number of products\n        if x >= num_products:\n            ids.append(0)\n        elif x < num_products:\n            ids.append(1)\n    # create the binary mask and return \n    return np.asarray(ids), np.logical_not(np.asarray(ids)).astype(int)\n\ndef compute_bootstraps_2i(sess, model, test_user_batch, test_product_batch, test_label_batch, test_logits, ap_mse_loss, ap_log_loss):\n    """"""Compute the bootstraps for the 2i model""""""\n\n    data_set_size = len(test_user_batch)\n    mse = []\n    llh = []\n    ap_mse = []\n    ap_llh = []\n    auc_list = []\n    mse_diff = []\n    llh_diff = []\n\n    # Compute the bootstrap values for the test split - this compute the empirical CR as well for comparision\n    for i in range(30):\n\n        ids = generate_bootstrap_batch(i*2, data_set_size)\n        test_user_batch = np.asarray(test_user_batch)\n        test_product_batch = np.asarray(test_product_batch)\n        test_label_batch = np.asarray(test_label_batch)\n\n        # Construct the feed-dict for the model and the average predictor\n        feed_dict = {model.user_list_placeholder : test_user_batch[ids], model.product_list_placeholder: test_product_batch[ids], model.label_list_placeholder: test_label_batch[ids], model.logits_placeholder: test_logits[ids], model.reg_list_placeholder: test_product_batch[ids]}\n\n        # Run the model test step updating the AUC object\n        loss_val, mse_loss_val, log_loss_val, pred = sess.run([model.loss, model.mse_loss, model.log_loss, model.prediction], feed_dict=feed_dict)\n\n        # Run the Average Predictor graph\n        ap_mse_val, ap_log_val = sess.run([ap_mse_loss, ap_log_loss], feed_dict=feed_dict)\n\n        mse.append(mse_loss_val)\n        llh.append(log_loss_val)\n        ap_mse.append(ap_mse_val)\n        ap_llh.append(ap_log_val)\n        auc_list.append(metrics.roc_auc_score(y_true=test_label_batch[ids].astype(int), y_score=pred))\n\n    for i in range(30):\n        mse_diff.append((ap_mse[i]-mse[i]) / ap_mse[i])\n        llh_diff.append((ap_llh[i]-llh[i]) / ap_llh[i])\n\n    print(""MSE Mean Score On The Bootstrap = "", np.mean(mse))\n    print(""MSE Mean Lift Over Average Predictor (%) = "", np.round(np.mean(mse_diff)*100, decimals=2))\n    print(""MSE STD (%) ="" , np.round(np.std(mse_diff)*100, decimals=2))\n\n    print(""LLH Mean Over Average Predictor (%) ="", np.round(np.mean(llh_diff)*100, decimals=2))\n    print(""LLH STD (%) = "", np.round(np.std(llh_diff)*100, decimals=2))\n\n    print(""Mean AUC Score On The Bootstrap = "",  np.round(np.mean(auc_list), decimals=4), ""+/-"", np.round(np.std(auc_list), decimals=4))\n\ndef compute_bootstraps(sess, model, test_user_batch, test_product_batch, test_label_batch, test_logits, ap_mse_loss, ap_log_loss):\n    """"""Compute the bootstraps for the 0 indexed model""""""\n\n    data_set_size = len(test_user_batch)\n    mse = []\n    llh = []\n    ap_mse = []\n    ap_llh = []\n    auc_list = []\n    mse_diff = []\n    llh_diff = []\n\n    # Compute the bootstrap values for the test split - this compute the empirical CR as well for comparision\n    for i in range(30):\n\n        ids = generate_bootstrap_batch(i*2, data_set_size)\n        test_user_batch = np.asarray(test_user_batch)\n        test_product_batch = np.asarray(test_product_batch)\n        test_label_batch = np.asarray(test_label_batch)\n\n        # Construct the feed-dict for the model and the average predictor\n        feed_dict = {model.user_list_placeholder : test_user_batch[ids], model.product_list_placeholder: test_product_batch[ids], model.label_list_placeholder: test_label_batch[ids], model.logits_placeholder: test_logits[ids]}\n\n        # Run the model test step updating the AUC object\n        loss_val, mse_loss_val, log_loss_val, pred = sess.run([model.loss, model.mse_loss, model.log_loss, model.prediction], feed_dict=feed_dict)\n\n        # Run the Average Predictor graph\n        ap_mse_val, ap_log_val = sess.run([ap_mse_loss, ap_log_loss], feed_dict=feed_dict)\n\n        mse.append(mse_loss_val)\n        llh.append(log_loss_val)\n        ap_mse.append(ap_mse_val)\n        ap_llh.append(ap_log_val)\n        auc_list.append(metrics.roc_auc_score(y_true=test_label_batch[ids].astype(int), y_score=pred))\n\n\n    for i in range(30):\n        mse_diff.append((ap_mse[i]-mse[i]) / ap_mse[i])\n        llh_diff.append((ap_llh[i]-llh[i]) / ap_llh[i])\n\n    print(""MSE Mean Score On The Bootstrap = "", np.mean(mse))\n    print(""MSE Mean Lift Over Average Predictor (%) = "", np.round(np.mean(mse_diff)*100, decimals=2))\n    print(""MSE STD (%) ="" , np.round(np.std(mse_diff)*100, decimals=2))\n\n    print(""LLH Mean Over Average Predictor (%) ="", np.round(np.mean(llh_diff)*100, decimals=2))\n    print(""LLH STD (%) = "", np.round(np.std(llh_diff)*100, decimals=2))\n\n    print(""Mean AUC Score On The Bootstrap = "",  np.round(np.mean(auc_list), decimals=4), ""+/-"", np.round(np.std(auc_list), decimals=4))\n'"
src/Data/dataset_loading.py,0,"b'from __future__ import absolute_import\nfrom __future__ import print_function\n\nimport numpy as np\nimport math as mt\nimport pandas as pd\nfrom scipy.sparse import csr_matrix\n\ndef load_movielens(dataset_location):\n    """"""Load the movielens 100K dataset from disk""""""\n\n    split_characters = ""\\t""\n    userid = []\n    productid = []\n    rating = []\n\n    with open(dataset_location, ""r"") as dataset_file:\n        for line in dataset_file:\n            f = line.rstrip(\'\\r\\n\').split(split_characters)\n            userid.append(int(f[0]))\n            productid.append(int(f[1]))\n            rating.append(int(f[2]))\n\n    # Make the sequence being at zero and contain no missing IDs\n    _, userid = np.unique(np.asarray(userid), return_inverse=1)\n    _, productid = np.unique(np.asarray(productid), return_inverse=1)\n\n    return userid, productid, rating\n\ndef load_movielens10M(dataset_location):\n    """"""Load the movielens dataset from disk""""""\n\n    split_characters = ""::""\n    userid = []\n    productid = []\n    rating = []\n\n    with open(dataset_location, ""r"") as dataset_file:\n        for line in dataset_file:\n            f = line.rstrip(\'\\r\\n\').split(split_characters)\n            userid.append(int(f[0]))\n            productid.append(int(f[1]))\n            rating.append(float(f[2]))\n\n        # Make the sequence being at zero and contain no missing IDs\n    _, userid = np.unique(np.asarray(userid), return_inverse=1)\n    _, productid = np.unique(np.asarray(productid), return_inverse=1)\n\n    print(""Dataset loaded"")\n\n    return userid, productid, rating\n\ndef create_movielens_userproduct_matrix(userid, productid, rating):\n    """"""Create the matrix from the movielens dataset""""""\n\n    model = ""expomf""\n    ratings_threshold = 5\n\n    num_unique_users = int(np.amax(userid)+1)\n    num_unique_products = int(np.amax(productid)+1)\n\n    print(num_unique_users)\n    print(num_unique_products)\n\n    view_matrix = np.zeros(shape=(num_unique_users, num_unique_products))\n    view_matrix[:] = np.NAN\n\n    for i in range(userid.shape[0]):\n\n        if rating[i] < ratings_threshold:\n            view_matrix[userid[i], productid[i]] = 0\n\n        elif rating[i] >= ratings_threshold:\n            view_matrix[userid[i], productid[i]] = 1\n\n        #return view_matrix\n        np.savetxt(\'ML_view.txt\', view_matrix)\n\n    print(""Data Saved"")\n\ndef load_netflix_dataset(dataset_location):\n\n    df1 = pd.read_csv(dataset_location, header = None, names = [\'Cust_Id\', \'Rating\'], usecols = [0,1]) \n    df2 = pd.read_csv(\'combined_data_2.txt\', header = None, names = [\'Cust_Id\', \'Rating\'], usecols = [0,1])\n    df3 = pd.read_csv(\'combined_data_3.txt\', header = None, names = [\'Cust_Id\', \'Rating\'], usecols = [0,1])\n    df4 = pd.read_csv(\'combined_data_4.txt\', header = None, names = [\'Cust_Id\', \'Rating\'], usecols = [0,1])\n\n    df = df1\n    df = df1.append(df2)\n    df = df.append(df3)\n    df = df.append(df4)\n\n    df.index = np.arange(0, len(df))\n\n    print(""Netflix data loaded into Pandas"")\n\n    df_nan = pd.DataFrame(pd.isnull(df.Rating))\n    df_nan = df_nan[df_nan[\'Rating\'] == True]\n    df_nan = df_nan.reset_index()\n\n    movie_np = []\n    movie_id = 1\n\n    for i,j in zip(df_nan[\'index\'][1:], df_nan[\'index\'][:-1]):\n        # numpy approach\n        temp = np.full((1,i-j-1), movie_id)\n        movie_np = np.append(movie_np, temp)\n        movie_id += 1\n\n    # Account for last record and corresponding length\n    # numpy approach\n    last_record = np.full((1, len(df) - df_nan.iloc[-1, 0] - 1),movie_id)\n    movie_np = np.append(movie_np, last_record)\n\n    # remove those Movie ID rows\n    df = df[pd.notnull(df[\'Rating\'])]\n\n    df[\'Movie_Id\'] = movie_np.astype(int)\n    df[\'Cust_Id\'] = df[\'Cust_Id\'].astype(int)\n\n    productid = df[\'Movie_Id\'].values\n    userid = df[\'Cust_Id\'].values\n    ratings = df[\'Rating\'].values\n\n    _, userid = np.unique(userid, return_inverse=1)\n    _, productid = np.unique(np.asarray(productid), return_inverse=1)\n\n    return userid, productid, ratings\n\nif __name__ == ""__main__"":\n\n    #userid, productid, rating = load_movielens(""/Users/s.bonner/Downloads/ml-100k/u.data"")\n    userid, productid, rating = load_movielens10M(""/home/ubuntu/ml-10M100K/ratings.dat"")\n    create_movielens_userproduct_matrix(userid, productid, rating)'"
