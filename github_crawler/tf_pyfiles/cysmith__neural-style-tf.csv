file_path,api_count,code
neural_style.py,31,"b'import tensorflow as tf\nimport numpy as np \nimport scipy.io  \nimport argparse \nimport struct\nimport errno\nimport time                       \nimport cv2\nimport os\n\n\'\'\'\n  parsing and configuration\n\'\'\'\ndef parse_args():\n\n  desc = ""TensorFlow implementation of \'A Neural Algorithm for Artistic Style\'""  \n  parser = argparse.ArgumentParser(description=desc)\n\n  # options for single image\n  parser.add_argument(\'--verbose\', action=\'store_true\',\n    help=\'Boolean flag indicating if statements should be printed to the console.\')\n\n  parser.add_argument(\'--img_name\', type=str, \n    default=\'result\',\n    help=\'Filename of the output image.\')\n\n  parser.add_argument(\'--style_imgs\', nargs=\'+\', type=str,\n    help=\'Filenames of the style images (example: starry-night.jpg)\', \n    required=True)\n  \n  parser.add_argument(\'--style_imgs_weights\', nargs=\'+\', type=float,\n    default=[1.0],\n    help=\'Interpolation weights of each of the style images. (example: 0.5 0.5)\')\n  \n  parser.add_argument(\'--content_img\', type=str,\n    help=\'Filename of the content image (example: lion.jpg)\')\n\n  parser.add_argument(\'--style_imgs_dir\', type=str,\n    default=\'./styles\',\n    help=\'Directory path to the style images. (default: %(default)s)\')\n\n  parser.add_argument(\'--content_img_dir\', type=str,\n    default=\'./image_input\',\n    help=\'Directory path to the content image. (default: %(default)s)\')\n  \n  parser.add_argument(\'--init_img_type\', type=str, \n    default=\'content\',\n    choices=[\'random\', \'content\', \'style\'], \n    help=\'Image used to initialize the network. (default: %(default)s)\')\n  \n  parser.add_argument(\'--max_size\', type=int, \n    default=512,\n    help=\'Maximum width or height of the input images. (default: %(default)s)\')\n  \n  parser.add_argument(\'--content_weight\', type=float, \n    default=5e0,\n    help=\'Weight for the content loss function. (default: %(default)s)\')\n  \n  parser.add_argument(\'--style_weight\', type=float, \n    default=1e4,\n    help=\'Weight for the style loss function. (default: %(default)s)\')\n  \n  parser.add_argument(\'--tv_weight\', type=float, \n    default=1e-3,\n    help=\'Weight for the total variational loss function. Set small (e.g. 1e-3). (default: %(default)s)\')\n\n  parser.add_argument(\'--temporal_weight\', type=float, \n    default=2e2,\n    help=\'Weight for the temporal loss function. (default: %(default)s)\')\n\n  parser.add_argument(\'--content_loss_function\', type=int,\n    default=1,\n    choices=[1, 2, 3],\n    help=\'Different constants for the content layer loss function. (default: %(default)s)\')\n  \n  parser.add_argument(\'--content_layers\', nargs=\'+\', type=str, \n    default=[\'conv4_2\'],\n    help=\'VGG19 layers used for the content image. (default: %(default)s)\')\n  \n  parser.add_argument(\'--style_layers\', nargs=\'+\', type=str,\n    default=[\'relu1_1\', \'relu2_1\', \'relu3_1\', \'relu4_1\', \'relu5_1\'],\n    help=\'VGG19 layers used for the style image. (default: %(default)s)\')\n  \n  parser.add_argument(\'--content_layer_weights\', nargs=\'+\', type=float, \n    default=[1.0], \n    help=\'Contributions (weights) of each content layer to loss. (default: %(default)s)\')\n  \n  parser.add_argument(\'--style_layer_weights\', nargs=\'+\', type=float, \n    default=[0.2, 0.2, 0.2, 0.2, 0.2],\n    help=\'Contributions (weights) of each style layer to loss. (default: %(default)s)\')\n    \n  parser.add_argument(\'--original_colors\', action=\'store_true\',\n    help=\'Transfer the style but not the colors.\')\n\n  parser.add_argument(\'--color_convert_type\', type=str,\n    default=\'yuv\',\n    choices=[\'yuv\', \'ycrcb\', \'luv\', \'lab\'],\n    help=\'Color space for conversion to original colors (default: %(default)s)\')\n\n  parser.add_argument(\'--color_convert_time\', type=str,\n    default=\'after\',\n    choices=[\'after\', \'before\'],\n    help=\'Time (before or after) to convert to original colors (default: %(default)s)\')\n\n  parser.add_argument(\'--style_mask\', action=\'store_true\',\n    help=\'Transfer the style to masked regions.\')\n\n  parser.add_argument(\'--style_mask_imgs\', nargs=\'+\', type=str, \n    default=None,\n    help=\'Filenames of the style mask images (example: face_mask.png) (default: %(default)s)\')\n  \n  parser.add_argument(\'--noise_ratio\', type=float, \n    default=1.0, \n    help=""Interpolation value between the content image and noise image if the network is initialized with \'random\'."")\n\n  parser.add_argument(\'--seed\', type=int, \n    default=0,\n    help=\'Seed for the random number generator. (default: %(default)s)\')\n  \n  parser.add_argument(\'--model_weights\', type=str, \n    default=\'imagenet-vgg-verydeep-19.mat\',\n    help=\'Weights and biases of the VGG-19 network.\')\n  \n  parser.add_argument(\'--pooling_type\', type=str,\n    default=\'avg\',\n    choices=[\'avg\', \'max\'],\n    help=\'Type of pooling in convolutional neural network. (default: %(default)s)\')\n  \n  parser.add_argument(\'--device\', type=str, \n    default=\'/gpu:0\',\n    choices=[\'/gpu:0\', \'/cpu:0\'],\n    help=\'GPU or CPU mode.  GPU mode requires NVIDIA CUDA. (default|recommended: %(default)s)\')\n  \n  parser.add_argument(\'--img_output_dir\', type=str, \n    default=\'./image_output\',\n    help=\'Relative or absolute directory path to output image and data.\')\n  \n  # optimizations\n  parser.add_argument(\'--optimizer\', type=str, \n    default=\'lbfgs\',\n    choices=[\'lbfgs\', \'adam\'],\n    help=\'Loss minimization optimizer.  L-BFGS gives better results.  Adam uses less memory. (default|recommended: %(default)s)\')\n  \n  parser.add_argument(\'--learning_rate\', type=float, \n    default=1e0, \n    help=\'Learning rate parameter for the Adam optimizer. (default: %(default)s)\')\n  \n  parser.add_argument(\'--max_iterations\', type=int, \n    default=1000,\n    help=\'Max number of iterations for the Adam or L-BFGS optimizer. (default: %(default)s)\')\n\n  parser.add_argument(\'--print_iterations\', type=int, \n    default=50,\n    help=\'Number of iterations between optimizer print statements. (default: %(default)s)\')\n  \n  # options for video frames\n  parser.add_argument(\'--video\', action=\'store_true\', \n    help=\'Boolean flag indicating if the user is generating a video.\')\n\n  parser.add_argument(\'--start_frame\', type=int, \n    default=1,\n    help=\'First frame number.\')\n  \n  parser.add_argument(\'--end_frame\', type=int, \n    default=1,\n    help=\'Last frame number.\')\n  \n  parser.add_argument(\'--first_frame_type\', type=str,\n    choices=[\'random\', \'content\', \'style\'], \n    default=\'content\',\n    help=\'Image used to initialize the network during the rendering of the first frame.\')\n  \n  parser.add_argument(\'--init_frame_type\', type=str, \n    choices=[\'prev_warped\', \'prev\', \'random\', \'content\', \'style\'], \n    default=\'prev_warped\',\n    help=\'Image used to initialize the network during the every rendering after the first frame.\')\n  \n  parser.add_argument(\'--video_input_dir\', type=str, \n    default=\'./video_input\',\n    help=\'Relative or absolute directory path to input frames.\')\n  \n  parser.add_argument(\'--video_output_dir\', type=str, \n    default=\'./video_output\',\n    help=\'Relative or absolute directory path to output frames.\')\n  \n  parser.add_argument(\'--content_frame_frmt\', type=str, \n    default=\'frame_{}.ppm\',\n    help=\'Filename format of the input content frames.\')\n  \n  parser.add_argument(\'--backward_optical_flow_frmt\', type=str, \n    default=\'backward_{}_{}.flo\',\n    help=\'Filename format of the backward optical flow files.\')\n  \n  parser.add_argument(\'--forward_optical_flow_frmt\', type=str, \n    default=\'forward_{}_{}.flo\',\n    help=\'Filename format of the forward optical flow files\')\n  \n  parser.add_argument(\'--content_weights_frmt\', type=str, \n    default=\'reliable_{}_{}.txt\',\n    help=\'Filename format of the optical flow consistency files.\')\n  \n  parser.add_argument(\'--prev_frame_indices\', nargs=\'+\', type=int, \n    default=[1],\n    help=\'Previous frames to consider for longterm temporal consistency.\')\n\n  parser.add_argument(\'--first_frame_iterations\', type=int, \n    default=2000,\n    help=\'Maximum number of optimizer iterations of the first frame. (default: %(default)s)\')\n  \n  parser.add_argument(\'--frame_iterations\', type=int, \n    default=800,\n    help=\'Maximum number of optimizer iterations for each frame after the first frame. (default: %(default)s)\')\n\n  args = parser.parse_args()\n\n  # normalize weights\n  args.style_layer_weights   = normalize(args.style_layer_weights)\n  args.content_layer_weights = normalize(args.content_layer_weights)\n  args.style_imgs_weights    = normalize(args.style_imgs_weights)\n\n  # create directories for output\n  if args.video:\n    maybe_make_directory(args.video_output_dir)\n  else:\n    maybe_make_directory(args.img_output_dir)\n\n  return args\n\n\'\'\'\n  pre-trained vgg19 convolutional neural network\n  remark: layers are manually initialized for clarity.\n\'\'\'\n\ndef build_model(input_img):\n  if args.verbose: print(\'\\nBUILDING VGG-19 NETWORK\')\n  net = {}\n  _, h, w, d     = input_img.shape\n  \n  if args.verbose: print(\'loading model weights...\')\n  vgg_rawnet     = scipy.io.loadmat(args.model_weights)\n  vgg_layers     = vgg_rawnet[\'layers\'][0]\n  if args.verbose: print(\'constructing layers...\')\n  net[\'input\']   = tf.Variable(np.zeros((1, h, w, d), dtype=np.float32))\n\n  if args.verbose: print(\'LAYER GROUP 1\')\n  net[\'conv1_1\'] = conv_layer(\'conv1_1\', net[\'input\'], W=get_weights(vgg_layers, 0))\n  net[\'relu1_1\'] = relu_layer(\'relu1_1\', net[\'conv1_1\'], b=get_bias(vgg_layers, 0))\n\n  net[\'conv1_2\'] = conv_layer(\'conv1_2\', net[\'relu1_1\'], W=get_weights(vgg_layers, 2))\n  net[\'relu1_2\'] = relu_layer(\'relu1_2\', net[\'conv1_2\'], b=get_bias(vgg_layers, 2))\n  \n  net[\'pool1\']   = pool_layer(\'pool1\', net[\'relu1_2\'])\n\n  if args.verbose: print(\'LAYER GROUP 2\')  \n  net[\'conv2_1\'] = conv_layer(\'conv2_1\', net[\'pool1\'], W=get_weights(vgg_layers, 5))\n  net[\'relu2_1\'] = relu_layer(\'relu2_1\', net[\'conv2_1\'], b=get_bias(vgg_layers, 5))\n  \n  net[\'conv2_2\'] = conv_layer(\'conv2_2\', net[\'relu2_1\'], W=get_weights(vgg_layers, 7))\n  net[\'relu2_2\'] = relu_layer(\'relu2_2\', net[\'conv2_2\'], b=get_bias(vgg_layers, 7))\n  \n  net[\'pool2\']   = pool_layer(\'pool2\', net[\'relu2_2\'])\n  \n  if args.verbose: print(\'LAYER GROUP 3\')\n  net[\'conv3_1\'] = conv_layer(\'conv3_1\', net[\'pool2\'], W=get_weights(vgg_layers, 10))\n  net[\'relu3_1\'] = relu_layer(\'relu3_1\', net[\'conv3_1\'], b=get_bias(vgg_layers, 10))\n\n  net[\'conv3_2\'] = conv_layer(\'conv3_2\', net[\'relu3_1\'], W=get_weights(vgg_layers, 12))\n  net[\'relu3_2\'] = relu_layer(\'relu3_2\', net[\'conv3_2\'], b=get_bias(vgg_layers, 12))\n\n  net[\'conv3_3\'] = conv_layer(\'conv3_3\', net[\'relu3_2\'], W=get_weights(vgg_layers, 14))\n  net[\'relu3_3\'] = relu_layer(\'relu3_3\', net[\'conv3_3\'], b=get_bias(vgg_layers, 14))\n\n  net[\'conv3_4\'] = conv_layer(\'conv3_4\', net[\'relu3_3\'], W=get_weights(vgg_layers, 16))\n  net[\'relu3_4\'] = relu_layer(\'relu3_4\', net[\'conv3_4\'], b=get_bias(vgg_layers, 16))\n\n  net[\'pool3\']   = pool_layer(\'pool3\', net[\'relu3_4\'])\n\n  if args.verbose: print(\'LAYER GROUP 4\')\n  net[\'conv4_1\'] = conv_layer(\'conv4_1\', net[\'pool3\'], W=get_weights(vgg_layers, 19))\n  net[\'relu4_1\'] = relu_layer(\'relu4_1\', net[\'conv4_1\'], b=get_bias(vgg_layers, 19))\n\n  net[\'conv4_2\'] = conv_layer(\'conv4_2\', net[\'relu4_1\'], W=get_weights(vgg_layers, 21))\n  net[\'relu4_2\'] = relu_layer(\'relu4_2\', net[\'conv4_2\'], b=get_bias(vgg_layers, 21))\n\n  net[\'conv4_3\'] = conv_layer(\'conv4_3\', net[\'relu4_2\'], W=get_weights(vgg_layers, 23))\n  net[\'relu4_3\'] = relu_layer(\'relu4_3\', net[\'conv4_3\'], b=get_bias(vgg_layers, 23))\n\n  net[\'conv4_4\'] = conv_layer(\'conv4_4\', net[\'relu4_3\'], W=get_weights(vgg_layers, 25))\n  net[\'relu4_4\'] = relu_layer(\'relu4_4\', net[\'conv4_4\'], b=get_bias(vgg_layers, 25))\n\n  net[\'pool4\']   = pool_layer(\'pool4\', net[\'relu4_4\'])\n\n  if args.verbose: print(\'LAYER GROUP 5\')\n  net[\'conv5_1\'] = conv_layer(\'conv5_1\', net[\'pool4\'], W=get_weights(vgg_layers, 28))\n  net[\'relu5_1\'] = relu_layer(\'relu5_1\', net[\'conv5_1\'], b=get_bias(vgg_layers, 28))\n\n  net[\'conv5_2\'] = conv_layer(\'conv5_2\', net[\'relu5_1\'], W=get_weights(vgg_layers, 30))\n  net[\'relu5_2\'] = relu_layer(\'relu5_2\', net[\'conv5_2\'], b=get_bias(vgg_layers, 30))\n\n  net[\'conv5_3\'] = conv_layer(\'conv5_3\', net[\'relu5_2\'], W=get_weights(vgg_layers, 32))\n  net[\'relu5_3\'] = relu_layer(\'relu5_3\', net[\'conv5_3\'], b=get_bias(vgg_layers, 32))\n\n  net[\'conv5_4\'] = conv_layer(\'conv5_4\', net[\'relu5_3\'], W=get_weights(vgg_layers, 34))\n  net[\'relu5_4\'] = relu_layer(\'relu5_4\', net[\'conv5_4\'], b=get_bias(vgg_layers, 34))\n\n  net[\'pool5\']   = pool_layer(\'pool5\', net[\'relu5_4\'])\n\n  return net\n\ndef conv_layer(layer_name, layer_input, W):\n  conv = tf.nn.conv2d(layer_input, W, strides=[1, 1, 1, 1], padding=\'SAME\')\n  if args.verbose: print(\'--{} | shape={} | weights_shape={}\'.format(layer_name, \n    conv.get_shape(), W.get_shape()))\n  return conv\n\ndef relu_layer(layer_name, layer_input, b):\n  relu = tf.nn.relu(layer_input + b)\n  if args.verbose: \n    print(\'--{} | shape={} | bias_shape={}\'.format(layer_name, relu.get_shape(), \n      b.get_shape()))\n  return relu\n\ndef pool_layer(layer_name, layer_input):\n  if args.pooling_type == \'avg\':\n    pool = tf.nn.avg_pool(layer_input, ksize=[1, 2, 2, 1], \n      strides=[1, 2, 2, 1], padding=\'SAME\')\n  elif args.pooling_type == \'max\':\n    pool = tf.nn.max_pool(layer_input, ksize=[1, 2, 2, 1], \n      strides=[1, 2, 2, 1], padding=\'SAME\')\n  if args.verbose: \n    print(\'--{}   | shape={}\'.format(layer_name, pool.get_shape()))\n  return pool\n\ndef get_weights(vgg_layers, i):\n  weights = vgg_layers[i][0][0][2][0][0]\n  W = tf.constant(weights)\n  return W\n\ndef get_bias(vgg_layers, i):\n  bias = vgg_layers[i][0][0][2][0][1]\n  b = tf.constant(np.reshape(bias, (bias.size)))\n  return b\n\n\'\'\'\n  \'a neural algorithm for artistic style\' loss functions\n\'\'\'\ndef content_layer_loss(p, x):\n  _, h, w, d = p.get_shape()\n  M = h.value * w.value\n  N = d.value\n  if args.content_loss_function   == 1:\n    K = 1. / (2. * N**0.5 * M**0.5)\n  elif args.content_loss_function == 2:\n    K = 1. / (N * M)\n  elif args.content_loss_function == 3:  \n    K = 1. / 2.\n  loss = K * tf.reduce_sum(tf.pow((x - p), 2))\n  return loss\n\ndef style_layer_loss(a, x):\n  _, h, w, d = a.get_shape()\n  M = h.value * w.value\n  N = d.value\n  A = gram_matrix(a, M, N)\n  G = gram_matrix(x, M, N)\n  loss = (1./(4 * N**2 * M**2)) * tf.reduce_sum(tf.pow((G - A), 2))\n  return loss\n\ndef gram_matrix(x, area, depth):\n  F = tf.reshape(x, (area, depth))\n  G = tf.matmul(tf.transpose(F), F)\n  return G\n\ndef mask_style_layer(a, x, mask_img):\n  _, h, w, d = a.get_shape()\n  mask = get_mask_image(mask_img, w.value, h.value)\n  mask = tf.convert_to_tensor(mask)\n  tensors = []\n  for _ in range(d.value): \n    tensors.append(mask)\n  mask = tf.stack(tensors, axis=2)\n  mask = tf.stack(mask, axis=0)\n  mask = tf.expand_dims(mask, 0)\n  a = tf.multiply(a, mask)\n  x = tf.multiply(x, mask)\n  return a, x\n\ndef sum_masked_style_losses(sess, net, style_imgs):\n  total_style_loss = 0.\n  weights = args.style_imgs_weights\n  masks = args.style_mask_imgs\n  for img, img_weight, img_mask in zip(style_imgs, weights, masks):\n    sess.run(net[\'input\'].assign(img))\n    style_loss = 0.\n    for layer, weight in zip(args.style_layers, args.style_layer_weights):\n      a = sess.run(net[layer])\n      x = net[layer]\n      a = tf.convert_to_tensor(a)\n      a, x = mask_style_layer(a, x, img_mask)\n      style_loss += style_layer_loss(a, x) * weight\n    style_loss /= float(len(args.style_layers))\n    total_style_loss += (style_loss * img_weight)\n  total_style_loss /= float(len(style_imgs))\n  return total_style_loss\n\ndef sum_style_losses(sess, net, style_imgs):\n  total_style_loss = 0.\n  weights = args.style_imgs_weights\n  for img, img_weight in zip(style_imgs, weights):\n    sess.run(net[\'input\'].assign(img))\n    style_loss = 0.\n    for layer, weight in zip(args.style_layers, args.style_layer_weights):\n      a = sess.run(net[layer])\n      x = net[layer]\n      a = tf.convert_to_tensor(a)\n      style_loss += style_layer_loss(a, x) * weight\n    style_loss /= float(len(args.style_layers))\n    total_style_loss += (style_loss * img_weight)\n  total_style_loss /= float(len(style_imgs))\n  return total_style_loss\n\ndef sum_content_losses(sess, net, content_img):\n  sess.run(net[\'input\'].assign(content_img))\n  content_loss = 0.\n  for layer, weight in zip(args.content_layers, args.content_layer_weights):\n    p = sess.run(net[layer])\n    x = net[layer]\n    p = tf.convert_to_tensor(p)\n    content_loss += content_layer_loss(p, x) * weight\n  content_loss /= float(len(args.content_layers))\n  return content_loss\n\n\'\'\'\n  \'artistic style transfer for videos\' loss functions\n\'\'\'\ndef temporal_loss(x, w, c):\n  c = c[np.newaxis,:,:,:]\n  D = float(x.size)\n  loss = (1. / D) * tf.reduce_sum(c * tf.nn.l2_loss(x - w))\n  loss = tf.cast(loss, tf.float32)\n  return loss\n\ndef get_longterm_weights(i, j):\n  c_sum = 0.\n  for k in range(args.prev_frame_indices):\n    if i - k > i - j:\n      c_sum += get_content_weights(i, i - k)\n  c = get_content_weights(i, i - j)\n  c_max = tf.maximum(c - c_sum, 0.)\n  return c_max\n\ndef sum_longterm_temporal_losses(sess, net, frame, input_img):\n  x = sess.run(net[\'input\'].assign(input_img))\n  loss = 0.\n  for j in range(args.prev_frame_indices):\n    prev_frame = frame - j\n    w = get_prev_warped_frame(frame)\n    c = get_longterm_weights(frame, prev_frame)\n    loss += temporal_loss(x, w, c)\n  return loss\n\ndef sum_shortterm_temporal_losses(sess, net, frame, input_img):\n  x = sess.run(net[\'input\'].assign(input_img))\n  prev_frame = frame - 1\n  w = get_prev_warped_frame(frame)\n  c = get_content_weights(frame, prev_frame)\n  loss = temporal_loss(x, w, c)\n  return loss\n\n\'\'\'\n  utilities and i/o\n\'\'\'\ndef read_image(path):\n  # bgr image\n  img = cv2.imread(path, cv2.IMREAD_COLOR)\n  check_image(img, path)\n  img = img.astype(np.float32)\n  img = preprocess(img)\n  return img\n\ndef write_image(path, img):\n  img = postprocess(img)\n  cv2.imwrite(path, img)\n\ndef preprocess(img):\n  imgpre = np.copy(img)\n  # bgr to rgb\n  imgpre = imgpre[...,::-1]\n  # shape (h, w, d) to (1, h, w, d)\n  imgpre = imgpre[np.newaxis,:,:,:]\n  imgpre -= np.array([123.68, 116.779, 103.939]).reshape((1,1,1,3))\n  return imgpre\n\ndef postprocess(img):\n  imgpost = np.copy(img)\n  imgpost += np.array([123.68, 116.779, 103.939]).reshape((1,1,1,3))\n  # shape (1, h, w, d) to (h, w, d)\n  imgpost = imgpost[0]\n  imgpost = np.clip(imgpost, 0, 255).astype(\'uint8\')\n  # rgb to bgr\n  imgpost = imgpost[...,::-1]\n  return imgpost\n\ndef read_flow_file(path):\n  with open(path, \'rb\') as f:\n    # 4 bytes header\n    header = struct.unpack(\'4s\', f.read(4))[0]\n    # 4 bytes width, height    \n    w = struct.unpack(\'i\', f.read(4))[0]\n    h = struct.unpack(\'i\', f.read(4))[0]   \n    flow = np.ndarray((2, h, w), dtype=np.float32)\n    for y in range(h):\n      for x in range(w):\n        flow[0,y,x] = struct.unpack(\'f\', f.read(4))[0]\n        flow[1,y,x] = struct.unpack(\'f\', f.read(4))[0]\n  return flow\n\ndef read_weights_file(path):\n  lines = open(path).readlines()\n  header = list(map(int, lines[0].split(\' \')))\n  w = header[0]\n  h = header[1]\n  vals = np.zeros((h, w), dtype=np.float32)\n  for i in range(1, len(lines)):\n    line = lines[i].rstrip().split(\' \')\n    vals[i-1] = np.array(list(map(np.float32, line)))\n    vals[i-1] = list(map(lambda x: 0. if x < 255. else 1., vals[i-1]))\n  # expand to 3 channels\n  weights = np.dstack([vals.astype(np.float32)] * 3)\n  return weights\n\ndef normalize(weights):\n  denom = sum(weights)\n  if denom > 0.:\n    return [float(i) / denom for i in weights]\n  else: return [0.] * len(weights)\n\ndef maybe_make_directory(dir_path):\n  if not os.path.exists(dir_path):  \n    os.makedirs(dir_path)\n\ndef check_image(img, path):\n  if img is None:\n    raise OSError(errno.ENOENT, ""No such file"", path)\n\n\'\'\'\n  rendering -- where the magic happens\n\'\'\'\ndef stylize(content_img, style_imgs, init_img, frame=None):\n  with tf.device(args.device), tf.Session() as sess:\n    # setup network\n    net = build_model(content_img)\n    \n    # style loss\n    if args.style_mask:\n      L_style = sum_masked_style_losses(sess, net, style_imgs)\n    else:\n      L_style = sum_style_losses(sess, net, style_imgs)\n    \n    # content loss\n    L_content = sum_content_losses(sess, net, content_img)\n    \n    # denoising loss\n    L_tv = tf.image.total_variation(net[\'input\'])\n    \n    # loss weights\n    alpha = args.content_weight\n    beta  = args.style_weight\n    theta = args.tv_weight\n    \n    # total loss\n    L_total  = alpha * L_content\n    L_total += beta  * L_style\n    L_total += theta * L_tv\n    \n    # video temporal loss\n    if args.video and frame > 1:\n      gamma      = args.temporal_weight\n      L_temporal = sum_shortterm_temporal_losses(sess, net, frame, init_img)\n      L_total   += gamma * L_temporal\n\n    # optimization algorithm\n    optimizer = get_optimizer(L_total)\n\n    if args.optimizer == \'adam\':\n      minimize_with_adam(sess, net, optimizer, init_img, L_total)\n    elif args.optimizer == \'lbfgs\':\n      minimize_with_lbfgs(sess, net, optimizer, init_img)\n    \n    output_img = sess.run(net[\'input\'])\n    \n    if args.original_colors:\n      output_img = convert_to_original_colors(np.copy(content_img), output_img)\n\n    if args.video:\n      write_video_output(frame, output_img)\n    else:\n      write_image_output(output_img, content_img, style_imgs, init_img)\n\ndef minimize_with_lbfgs(sess, net, optimizer, init_img):\n  if args.verbose: print(\'\\nMINIMIZING LOSS USING: L-BFGS OPTIMIZER\')\n  init_op = tf.global_variables_initializer()\n  sess.run(init_op)\n  sess.run(net[\'input\'].assign(init_img))\n  optimizer.minimize(sess)\n\ndef minimize_with_adam(sess, net, optimizer, init_img, loss):\n  if args.verbose: print(\'\\nMINIMIZING LOSS USING: ADAM OPTIMIZER\')\n  train_op = optimizer.minimize(loss)\n  init_op = tf.global_variables_initializer()\n  sess.run(init_op)\n  sess.run(net[\'input\'].assign(init_img))\n  iterations = 0\n  while (iterations < args.max_iterations):\n    sess.run(train_op)\n    if iterations % args.print_iterations == 0 and args.verbose:\n      curr_loss = loss.eval()\n      print(""At iterate {}\\tf=  {}"".format(iterations, curr_loss))\n    iterations += 1\n\ndef get_optimizer(loss):\n  print_iterations = args.print_iterations if args.verbose else 0\n  if args.optimizer == \'lbfgs\':\n    optimizer = tf.contrib.opt.ScipyOptimizerInterface(\n      loss, method=\'L-BFGS-B\',\n      options={\'maxiter\': args.max_iterations,\n                  \'disp\': print_iterations})\n  elif args.optimizer == \'adam\':\n    optimizer = tf.train.AdamOptimizer(args.learning_rate)\n  return optimizer\n\ndef write_video_output(frame, output_img):\n  fn = args.content_frame_frmt.format(str(frame).zfill(4))\n  path = os.path.join(args.video_output_dir, fn)\n  write_image(path, output_img)\n\ndef write_image_output(output_img, content_img, style_imgs, init_img):\n  out_dir = os.path.join(args.img_output_dir, args.img_name)\n  maybe_make_directory(out_dir)\n  img_path = os.path.join(out_dir, args.img_name+\'.png\')\n  content_path = os.path.join(out_dir, \'content.png\')\n  init_path = os.path.join(out_dir, \'init.png\')\n\n  write_image(img_path, output_img)\n  write_image(content_path, content_img)\n  write_image(init_path, init_img)\n  index = 0\n  for style_img in style_imgs:\n    path = os.path.join(out_dir, \'style_\'+str(index)+\'.png\')\n    write_image(path, style_img)\n    index += 1\n  \n  # save the configuration settings\n  out_file = os.path.join(out_dir, \'meta_data.txt\')\n  f = open(out_file, \'w\')\n  f.write(\'image_name: {}\\n\'.format(args.img_name))\n  f.write(\'content: {}\\n\'.format(args.content_img))\n  index = 0\n  for style_img, weight in zip(args.style_imgs, args.style_imgs_weights):\n    f.write(\'styles[\'+str(index)+\']: {} * {}\\n\'.format(weight, style_img))\n    index += 1\n  index = 0\n  if args.style_mask_imgs is not None:\n    for mask in args.style_mask_imgs:\n      f.write(\'style_masks[\'+str(index)+\']: {}\\n\'.format(mask))\n      index += 1\n  f.write(\'init_type: {}\\n\'.format(args.init_img_type))\n  f.write(\'content_weight: {}\\n\'.format(args.content_weight))\n  f.write(\'style_weight: {}\\n\'.format(args.style_weight))\n  f.write(\'tv_weight: {}\\n\'.format(args.tv_weight))\n  f.write(\'content_layers: {}\\n\'.format(args.content_layers))\n  f.write(\'style_layers: {}\\n\'.format(args.style_layers))\n  f.write(\'optimizer_type: {}\\n\'.format(args.optimizer))\n  f.write(\'max_iterations: {}\\n\'.format(args.max_iterations))\n  f.write(\'max_image_size: {}\\n\'.format(args.max_size))\n  f.close()\n\n\'\'\'\n  image loading and processing\n\'\'\'\ndef get_init_image(init_type, content_img, style_imgs, frame=None):\n  if init_type == \'content\':\n    return content_img\n  elif init_type == \'style\':\n    return style_imgs[0]\n  elif init_type == \'random\':\n    init_img = get_noise_image(args.noise_ratio, content_img)\n    return init_img\n  # only for video frames\n  elif init_type == \'prev\':\n    init_img = get_prev_frame(frame)\n    return init_img\n  elif init_type == \'prev_warped\':\n    init_img = get_prev_warped_frame(frame)\n    return init_img\n\ndef get_content_frame(frame):\n  fn = args.content_frame_frmt.format(str(frame).zfill(4))\n  path = os.path.join(args.video_input_dir, fn)\n  img = read_image(path)\n  return img\n\ndef get_content_image(content_img):\n  path = os.path.join(args.content_img_dir, content_img)\n   # bgr image\n  img = cv2.imread(path, cv2.IMREAD_COLOR)\n  check_image(img, path)\n  img = img.astype(np.float32)\n  h, w, d = img.shape\n  mx = args.max_size\n  # resize if > max size\n  if h > w and h > mx:\n    w = (float(mx) / float(h)) * w\n    img = cv2.resize(img, dsize=(int(w), mx), interpolation=cv2.INTER_AREA)\n  if w > mx:\n    h = (float(mx) / float(w)) * h\n    img = cv2.resize(img, dsize=(mx, int(h)), interpolation=cv2.INTER_AREA)\n  img = preprocess(img)\n  return img\n\ndef get_style_images(content_img):\n  _, ch, cw, cd = content_img.shape\n  style_imgs = []\n  for style_fn in args.style_imgs:\n    path = os.path.join(args.style_imgs_dir, style_fn)\n    # bgr image\n    img = cv2.imread(path, cv2.IMREAD_COLOR)\n    check_image(img, path)\n    img = img.astype(np.float32)\n    img = cv2.resize(img, dsize=(cw, ch), interpolation=cv2.INTER_AREA)\n    img = preprocess(img)\n    style_imgs.append(img)\n  return style_imgs\n\ndef get_noise_image(noise_ratio, content_img):\n  np.random.seed(args.seed)\n  noise_img = np.random.uniform(-20., 20., content_img.shape).astype(np.float32)\n  img = noise_ratio * noise_img + (1.-noise_ratio) * content_img\n  return img\n\ndef get_mask_image(mask_img, width, height):\n  path = os.path.join(args.content_img_dir, mask_img)\n  img = cv2.imread(path, cv2.IMREAD_GRAYSCALE)\n  check_image(img, path)\n  img = cv2.resize(img, dsize=(width, height), interpolation=cv2.INTER_AREA)\n  img = img.astype(np.float32)\n  mx = np.amax(img)\n  img /= mx\n  return img\n\ndef get_prev_frame(frame):\n  # previously stylized frame\n  prev_frame = frame - 1\n  fn = args.content_frame_frmt.format(str(prev_frame).zfill(4))\n  path = os.path.join(args.video_output_dir, fn)\n  img = cv2.imread(path, cv2.IMREAD_COLOR)\n  check_image(img, path)\n  return img\n\ndef get_prev_warped_frame(frame):\n  prev_img = get_prev_frame(frame)\n  prev_frame = frame - 1\n  # backwards flow: current frame -> previous frame\n  fn = args.backward_optical_flow_frmt.format(str(frame), str(prev_frame))\n  path = os.path.join(args.video_input_dir, fn)\n  flow = read_flow_file(path)\n  warped_img = warp_image(prev_img, flow).astype(np.float32)\n  img = preprocess(warped_img)\n  return img\n\ndef get_content_weights(frame, prev_frame):\n  forward_fn = args.content_weights_frmt.format(str(prev_frame), str(frame))\n  backward_fn = args.content_weights_frmt.format(str(frame), str(prev_frame))\n  forward_path = os.path.join(args.video_input_dir, forward_fn)\n  backward_path = os.path.join(args.video_input_dir, backward_fn)\n  forward_weights = read_weights_file(forward_path)\n  backward_weights = read_weights_file(backward_path)\n  return forward_weights #, backward_weights\n\ndef warp_image(src, flow):\n  _, h, w = flow.shape\n  flow_map = np.zeros(flow.shape, dtype=np.float32)\n  for y in range(h):\n    flow_map[1,y,:] = float(y) + flow[1,y,:]\n  for x in range(w):\n    flow_map[0,:,x] = float(x) + flow[0,:,x]\n  # remap pixels to optical flow\n  dst = cv2.remap(\n    src, flow_map[0], flow_map[1], \n    interpolation=cv2.INTER_CUBIC, borderMode=cv2.BORDER_TRANSPARENT)\n  return dst\n\ndef convert_to_original_colors(content_img, stylized_img):\n  content_img  = postprocess(content_img)\n  stylized_img = postprocess(stylized_img)\n  if args.color_convert_type == \'yuv\':\n    cvt_type = cv2.COLOR_BGR2YUV\n    inv_cvt_type = cv2.COLOR_YUV2BGR\n  elif args.color_convert_type == \'ycrcb\':\n    cvt_type = cv2.COLOR_BGR2YCR_CB\n    inv_cvt_type = cv2.COLOR_YCR_CB2BGR\n  elif args.color_convert_type == \'luv\':\n    cvt_type = cv2.COLOR_BGR2LUV\n    inv_cvt_type = cv2.COLOR_LUV2BGR\n  elif args.color_convert_type == \'lab\':\n    cvt_type = cv2.COLOR_BGR2LAB\n    inv_cvt_type = cv2.COLOR_LAB2BGR\n  content_cvt = cv2.cvtColor(content_img, cvt_type)\n  stylized_cvt = cv2.cvtColor(stylized_img, cvt_type)\n  c1, _, _ = cv2.split(stylized_cvt)\n  _, c2, c3 = cv2.split(content_cvt)\n  merged = cv2.merge((c1, c2, c3))\n  dst = cv2.cvtColor(merged, inv_cvt_type).astype(np.float32)\n  dst = preprocess(dst)\n  return dst\n\ndef render_single_image():\n  content_img = get_content_image(args.content_img)\n  style_imgs = get_style_images(content_img)\n  with tf.Graph().as_default():\n    print(\'\\n---- RENDERING SINGLE IMAGE ----\\n\')\n    init_img = get_init_image(args.init_img_type, content_img, style_imgs)\n    tick = time.time()\n    stylize(content_img, style_imgs, init_img)\n    tock = time.time()\n    print(\'Single image elapsed time: {}\'.format(tock - tick))\n\ndef render_video():\n  for frame in range(args.start_frame, args.end_frame+1):\n    with tf.Graph().as_default():\n      print(\'\\n---- RENDERING VIDEO FRAME: {}/{} ----\\n\'.format(frame, args.end_frame))\n      if frame == 1:\n        content_frame = get_content_frame(frame)\n        style_imgs = get_style_images(content_frame)\n        init_img = get_init_image(args.first_frame_type, content_frame, style_imgs, frame)\n        args.max_iterations = args.first_frame_iterations\n        tick = time.time()\n        stylize(content_frame, style_imgs, init_img, frame)\n        tock = time.time()\n        print(\'Frame {} elapsed time: {}\'.format(frame, tock - tick))\n      else:\n        content_frame = get_content_frame(frame)\n        style_imgs = get_style_images(content_frame)\n        init_img = get_init_image(args.init_frame_type, content_frame, style_imgs, frame)\n        args.max_iterations = args.frame_iterations\n        tick = time.time()\n        stylize(content_frame, style_imgs, init_img, frame)\n        tock = time.time()\n        print(\'Frame {} elapsed time: {}\'.format(frame, tock - tick))\n\ndef main():\n  global args\n  args = parse_args()\n  if args.video: render_video()\n  else: render_single_image()\n\nif __name__ == \'__main__\':\n  main()\n'"
