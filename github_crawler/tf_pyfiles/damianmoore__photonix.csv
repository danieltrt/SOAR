file_path,api_count,code
setup.py,0,"b'from setuptools import find_packages, setup\n\ninstall_requires = [\n    \'django\', \'ipython\', \'pillow\', \'psutil\', \'psycopg2-binary\',\n    \'python-redis-lock\', \'requests\', \'redis\', \'rq\', \'tensorflow\', \'inotify\',\n    \'graphene-django\', \'django-filter\', \'pyshp\', \'matplotlib\', \'pytest\',\n    \'pytest-django\', \'codecov\', \'gunicorn\', \'pip\'\n]\n\nsetup(\n    name=\'photonix\',\n    version=\'0.2\',\n    packages=find_packages(),\n    description=(""WIP""),\n    author=""Damian Moore"",\n    author_email=""damian@epixstudios.co.uk"",\n    include_package_data=True,\n    zip_safe=False,\n    classifiers=[\n        ""Programming Language :: Python"",\n        ""Programming Language :: Python :: 3.6"",\n        ""Programming Language :: Python :: 3.7"",\n    ],\n    install_requires=install_requires, )\n'"
test.py,0,"b""import os\nfrom pathlib import Path\nimport sys\n\nimport pytest\nimport tensorflow as tf\n\n\nsys.path.insert(0, str(Path(__file__).parent.resolve()))\nos.environ['ENV'] = 'test'\n\n\nif __name__ == '__main__':\n    exit(pytest.main(['tests']))\n"""
photonix/manage.py,0,"b'#!/usr/bin/env python\nimport os\nimport sys\n\nif __name__ == ""__main__"":\n    os.environ.setdefault(""DJANGO_SETTINGS_MODULE"", ""photonix.web.settings"")\n    try:\n        from django.core.management import execute_from_command_line\n    except ImportError:\n        # The above import may fail for some other reason. Ensure that the\n        # issue is really that Django is missing to avoid masking other\n        # exceptions on Python 2.\n        try:\n            import django\n        except ImportError:\n            raise ImportError(\n                ""Couldn\'t import Django. Are you sure it\'s installed and ""\n                ""available on your PYTHONPATH environment variable? Did you ""\n                ""forget to activate a virtual environment?""\n            )\n        raise\n    execute_from_command_line(sys.argv)\n'"
tests/__init__.py,0,b''
tests/conftest.py,0,"b'import json\nimport os\n\nfrom django.conf import settings\nfrom django.core.serializers.json import DjangoJSONEncoder\nfrom django.contrib.auth.models import AnonymousUser\nfrom django.shortcuts import reverse\nfrom django.test.client import MULTIPART_CONTENT, Client\nimport mock\nimport pytest\n\n\nAPI_PATH = reverse(\'api\')\n\n\n@pytest.fixture(scope=\'session\')\ndef django_db_modify_db_settings(django_db_modify_db_settings,):\n    os.environ[\'ENV\'] = \'test\'\n    settings.DATABASES[\'default\'] = {\n        \'ENGINE\':   \'django.db.backends.sqlite3\',\n        \'NAME\':     \':memory:\'\n    }\n\n\n@pytest.fixture(autouse=True)\ndef mock_redis(request):\n    mocks = [\'photonix.classifiers.base_model.Lock\',\n             \'photonix.classifiers.style.model.Lock\',\n             \'photonix.classifiers.object.model.Lock\']\n    mocks = [mock.patch(x, mock.MagicMock()) for x in mocks]\n    for m in mocks:\n        m.start()\n    yield\n    for m in mocks:\n        m.stop()\n\n\n# These fixtures come from the Saleor project and are licensed as BSD-3-Clause\n# https://github.com/mirumee/saleor/blob/master/tests/api/conftest.py\n\nclass ApiClient(Client):\n    """"""GraphQL API client.""""""\n\n    def __init__(self, *args, **kwargs):\n        user = kwargs.pop(""user"")\n        self.user = user\n        if not user.is_anonymous:\n            self.token = get_token(user)\n        super().__init__(*args, **kwargs)\n\n    def _base_environ(self, **request):\n        environ = super()._base_environ(**request)\n        if not self.user.is_anonymous:\n            environ.update({""HTTP_AUTHORIZATION"": ""JWT %s"" % self.token})\n        return environ\n\n    def post(self, data=None, **kwargs):\n        """"""Send a POST request.\n        This wrapper sets the `application/json` content type which is\n        more suitable for standard GraphQL requests and doesn\'t mismatch with\n        handling multipart requests in Graphene.\n        """"""\n        if data:\n            data = json.dumps(data, cls=DjangoJSONEncoder)\n        kwargs[""content_type""] = ""application/json""\n        return super().post(API_PATH, data, **kwargs)\n\n    def post_graphql(\n        self,\n        query,\n        variables=None,\n        permissions=None,\n        check_no_permissions=True,\n        **kwargs,\n    ):\n        """"""Dedicated helper for posting GraphQL queries.\n        Sets the `application/json` content type and json.dumps the variables\n        if present.\n        """"""\n        data = {""query"": query}\n        if variables is not None:\n            data[""variables""] = variables\n        if data:\n            data = json.dumps(data, cls=DjangoJSONEncoder)\n        kwargs[""content_type""] = ""application/json""\n\n        if permissions:\n            if check_no_permissions:\n                response = super().post(API_PATH, data, **kwargs)\n                assert_no_permission(response)\n            self.user.user_permissions.add(*permissions)\n        return super().post(API_PATH, data, **kwargs)\n\n    def post_multipart(self, *args, permissions=None, **kwargs):\n        """"""Send a multipart POST request.\n        This is used to send multipart requests to GraphQL API when e.g.\n        uploading files.\n        """"""\n        kwargs[""content_type""] = MULTIPART_CONTENT\n\n        if permissions:\n            response = super().post(API_PATH, *args, **kwargs)\n            assert_no_permission(response)\n            self.user.user_permissions.add(*permissions)\n        return super().post(API_PATH, *args, **kwargs)\n\n\n@pytest.fixture\ndef api_client():\n    return ApiClient(user=AnonymousUser())\n'"
tests/test_classifier_batch.py,0,"b""from datetime import datetime\nfrom pathlib import Path\nimport queue\nimport threading\nfrom time import time\nimport uuid\n\nfrom django.utils import timezone\nimport factory\nimport pytest\n\nfrom photonix.classifiers.color import ColorModel, run_on_photo\nfrom photonix.classifiers.style import StyleModel, run_on_photo\nfrom photonix.photos.models import Task, Photo, PhotoFile, Tag, PhotoTag\nfrom photonix.photos.utils.classification import ThreadedQueueProcessor\n\n\n# pytestmark = pytest.mark.django_db\n\n\nmodel = StyleModel()\n\n\n@pytest.fixture\ndef photo_fixture_snow(db):\n    from photonix.photos.utils.db import record_photo\n    snow_path = str(Path(__file__).parent / 'photos' / 'snow.jpg')\n    return record_photo(snow_path)\n\n\nclass PhotoFileFactory(factory.django.DjangoModelFactory):\n    class Meta:\n        model = PhotoFile\n\n    path                = str(Path(__file__).parent / 'photos' / 'snow.jpg')\n    mimetype            = 'image/jpeg'\n    bytes               = 1000\n    file_modified_at    = timezone.now()\n\n\nclass PhotoFactory(factory.django.DjangoModelFactory):\n    class Meta:\n        model = Photo\n\n\nclass TaskFactory(factory.django.DjangoModelFactory):\n    class Meta:\n        model = Task\n\n    type = 'classify.style'\n    status = 'P'\n\n\ndef test_classifier_batch(photo_fixture_snow):\n    photo = PhotoFactory()\n    PhotoFileFactory(photo=photo)\n\n    for i in range(4):\n        TaskFactory(subject_id=photo.id)\n\n    start = time()\n\n    threaded_queue_processor = ThreadedQueueProcessor(model, 'classify.style', run_on_photo, 1, 64)\n    threaded_queue_processor.run(loop=False)\n\n    assert time() - start > 0\n    assert time() - start < 100\n    assert photo.photo_tags.count() == 1\n    assert photo.photo_tags.all()[0].tag.name == 'serene'\n    assert photo.photo_tags.all()[0].confidence > 0.9\n"""
tests/test_classifier_models.py,0,"b""import os\nimport time\nfrom datetime import datetime\nfrom pathlib import Path\n\n\ndef test_downloading(tmpdir):\n    from photonix.classifiers.style.model import StyleModel\n\n    model_dir = tmpdir\n    start = time.mktime(datetime.now().timetuple())\n    model = StyleModel(lock_name=None, model_dir=model_dir)\n\n    graph_path = str(Path(model_dir) / 'style' / 'graph.pb')\n    assert os.stat(graph_path).st_size > 1024 * 10 * 10\n    assert os.stat(graph_path).st_mtime > start\n    with open(str(Path(model_dir) / 'style' / 'version.txt')) as f:\n        content = f.read()\n        assert content.strip() == str(model.version)\n\n\ndef test_color_predict():\n    from photonix.classifiers.color.model import ColorModel\n\n    model = ColorModel()\n    snow = str(Path(__file__).parent / 'photos' / 'snow.jpg')\n    result = model.predict(snow)\n    expected = [('Violet', '0.094'), ('Gray', '0.018'), ('Black', '0.006'), ('White', '0.005'), ('Pale pink', '0.001'), ('Dark orange', '0.000'), ('Dark lime green', '0.000')]\n    actual = [(x, '{:.3f}'.format(y)) for x, y in result]\n    assert expected == actual\n\n\ndef test_location_predict():\n    from photonix.classifiers.location.model import LocationModel\n\n    model = LocationModel()\n\n    # London, UK - Tests multiple polygons of the UK\n    result = model.predict(location=[51.5304213, -0.1286445])\n    assert result['country']['name'] == 'United Kingdom'\n    assert result['city']['name'] == 'London'\n    assert result['city']['distance'] == 1405\n    assert result['city']['population'] == 7556900\n\n    # In the sea near Oia, Santorini, Greece - Country is inferred from city\n    result = model.predict(location=[36.4396445,25.3560936])\n    assert result['country']['name'] == 'Greece'\n    assert result['city']['name'] == 'O\xc3\xada'\n    assert result['city']['distance'] == 3132\n    assert result['city']['population'] == 3376\n\n    # Too far off the coast of John o' Groats, Scotland, UK - No match\n    result = model.predict(location=[58.6876742,-3.4206862])\n    assert result['country'] == None\n    assert result['city'] == None\n\n    # Vernier, Switzerland - Tests country code mainly (CH can be China in some codings)\n    result = model.predict(location=[46.1760906,5.9929043])\n    assert result['country']['name'] == 'Switzerland'\n    assert result['country']['code'] == 'CH'\n    assert result['city']['country_name'] == 'Switzerland'\n    assert result['city']['country_code'] == 'CH'\n\n    # In France but close to a 'city' in Belgium - City should be limited to within border of country\n    result = model.predict(location=[51.074323, 2.547278])\n    assert result['country']['name'] == 'France'\n    assert result['city']['country_name'] == 'France'\n    assert result['city']['name'] == 'T\xc3\xa9teghem'\n\n\ndef test_object_predict():\n    from photonix.classifiers.object.model import ObjectModel\n\n    model = ObjectModel()\n    snow = str(Path(__file__).parent / 'photos' / 'snow.jpg')\n    result = model.predict(snow)\n#     import pdb; pdb.set_trace()\n\n    assert len(result) == 3\n\n    assert result[0]['label'] == 'Tree'\n    assert '{0:.3f}'.format(result[0]['score']) == '0.602'\n    assert '{0:.3f}'.format(result[0]['significance']) == '0.134'\n    assert '{0:.3f}'.format(result[0]['x']) == '0.787'\n    assert '{0:.3f}'.format(result[0]['y']) == '0.374'\n    assert '{0:.3f}'.format(result[0]['width']) == '0.340'\n    assert '{0:.3f}'.format(result[0]['height']) == '0.655'\n\n    assert result[1]['label'] == 'Tree'\n    assert '{0:.3f}'.format(result[1]['score']) == '0.525'\n    assert '{0:.3f}'.format(result[1]['significance']) == '0.016'\n\n    assert result[2]['label'] == 'Tree'\n    assert '{0:.3f}'.format(result[2]['score']) == '0.453'\n    assert '{0:.3f}'.format(result[2]['significance']) == '0.025'\n\n\ndef test_style_predict():\n    from photonix.classifiers.style.model import StyleModel\n\n    model = StyleModel()\n    snow = str(Path(__file__).parent / 'photos' / 'snow.jpg')\n    result = model.predict(snow)\n\n    assert len(result) == 1\n    assert result[0][0] == 'serene'\n    assert '{0:.3f}'.format(result[0][1]) == '0.962'\n"""
tests/test_classifier_runners.py,0,"b""from pathlib import Path\n\nimport pytest\n\n\n# pytestmark = pytest.mark.django_db\n\n\n@pytest.fixture\ndef photo_fixture_snow(db):\n    from photonix.photos.utils.db import record_photo\n    snow_path = str(Path(__file__).parent / 'photos' / 'snow.jpg')\n    return record_photo(snow_path)\n\n\n@pytest.fixture\ndef photo_fixture_tree(db):\n    from photonix.photos.utils.db import record_photo\n    tree_path = str(Path(__file__).parent / 'photos' / 'tree.jpg')\n    return record_photo(tree_path)\n\n\ndef test_color_via_runner(photo_fixture_snow):\n    from photonix.classifiers.color.model import run_on_photo\n\n    # Path on it's own returns a None Photo object along with the result\n    snow = str(Path(__file__).parent / 'photos' / 'snow.jpg')\n    photo, result = run_on_photo(snow)\n\n    assert photo is None\n    assert len(result) == 7\n    assert result[0][0] == 'Violet'\n    assert '{0:.3f}'.format(result[0][1]) == '0.094'\n\n    # Passing in a Photo object should tag the object\n    assert photo_fixture_snow.photo_tags.count() == 0\n    photo, result = run_on_photo(photo_fixture_snow.id)\n    assert photo_fixture_snow.photo_tags.count() == 7\n    assert photo_fixture_snow.photo_tags.all()[0].tag.name == 'Violet'\n    assert photo_fixture_snow.photo_tags.all()[0].tag.type == 'C'\n    assert '{0:.3f}'.format(photo_fixture_snow.photo_tags.all()[0].significance) == '0.094'\n\n\ndef test_location_via_runner(photo_fixture_tree):\n    from photonix.classifiers.location.model import run_on_photo\n\n    # Path on it's own returns a None Photo object along with the result\n    snow = str(Path(__file__).parent / 'photos' / 'snow.jpg')\n    photo, result = run_on_photo(snow)\n\n    # This photo has no GPS coordinates\n    assert photo is None\n    assert result['city'] is None\n    assert result['country'] is None\n\n    # Path which does have GPS coordinates\n    tree = str(Path(__file__).parent / 'photos' / 'tree.jpg')\n    photo, result = run_on_photo(tree)\n    assert result['country']['name'] == 'Greece'\n    assert result['country']['code'] == 'GR'\n    assert result['city']['name'] == 'Fir\xc3\xa1'\n    assert result['city']['country_name'] == 'Greece'\n\n    # Photo object with location to tag should have tags for country and city\n    assert photo_fixture_tree.photo_tags.count() == 0\n    photo, result = run_on_photo(photo_fixture_tree.id)\n    assert photo.photo_tags.all().count() == 2\n    assert photo.photo_tags.all()[0].tag.name == 'Greece'\n    assert photo.photo_tags.all()[0].confidence == 1.0\n    assert photo.photo_tags.all()[0].significance == 1.0\n    assert photo.photo_tags.all()[1].tag.name == 'Fir\xc3\xa1'\n    assert photo.photo_tags.all()[1].confidence == 0.5\n    assert photo.photo_tags.all()[1].significance == 0.5\n    assert photo.photo_tags.all()[1].tag.parent.name == 'Greece'\n\n\ndef test_object_via_runner(photo_fixture_snow):\n    from photonix.classifiers.object.model import run_on_photo\n\n    # Path on it's own returns a None Photo object along with the result\n    snow = str(Path(__file__).parent / 'photos' / 'snow.jpg')\n    photo, result = run_on_photo(snow)\n\n    assert photo is None\n    assert len(result) == 3\n    assert result[0]['label'] == 'Tree'\n    assert '{0:.3f}'.format(result[0]['significance']) == '0.134'\n\n    # Passing in a Photo object should tag the object\n    assert photo_fixture_snow.photo_tags.count() == 0\n    photo, result = run_on_photo(photo_fixture_snow.id)\n    assert photo_fixture_snow.photo_tags.count() == 3\n    assert photo_fixture_snow.photo_tags.all()[0].tag.name == 'Tree'\n    assert photo_fixture_snow.photo_tags.all()[0].tag.type == 'O'\n    assert '{0:.3f}'.format(photo_fixture_snow.photo_tags.all()[0].significance) == '0.134'\n\n\ndef test_style_via_runner(photo_fixture_snow):\n    from photonix.classifiers.style.model import run_on_photo\n\n    # Path on it's own returns a None Photo object along with the result\n    snow = str(Path(__file__).parent / 'photos' / 'snow.jpg')\n    photo, result = run_on_photo(snow)\n\n    assert photo is None\n    assert len(result) == 1\n    assert result[0][0] == 'serene'\n    assert '{0:.3f}'.format(result[0][1]) == '0.915'\n\n    # Passing in a Photo object should tag the object\n    assert photo_fixture_snow.photo_tags.count() == 0\n    photo, result = run_on_photo(photo_fixture_snow.id)\n    assert photo_fixture_snow.photo_tags.count() == 1\n    assert photo_fixture_snow.photo_tags.all()[0].tag.name == 'serene'\n    assert photo_fixture_snow.photo_tags.all()[0].tag.type == 'S'\n    assert '{0:.3f}'.format(photo_fixture_snow.photo_tags.all()[0].significance) == '0.915'\n"""
tests/test_graphql.py,0,"b'import json\nfrom pathlib import Path\n\nimport graphene\nimport pytest\n\nfrom .utils import get_graphql_content\nfrom photonix.photos.models import Tag, PhotoTag\n\n\n@pytest.fixture\ndef photo_fixture_snow(db):\n    from photonix.photos.utils.db import record_photo\n    snow_path = str(Path(__file__).parent / \'photos\' / \'snow.jpg\')\n    return record_photo(snow_path)\n\n\n@pytest.fixture\ndef photo_fixture_tree(db):\n    from photonix.photos.utils.db import record_photo\n    tree_path = str(Path(__file__).parent / \'photos\' / \'tree.jpg\')\n    return record_photo(tree_path)\n\n\ndef test_get_photo(photo_fixture_snow, api_client):\n    query = """"""\n        query PhotoQuery($id: UUID) {\n            photo(id: $id) {\n                url\n            }\n        }\n    """"""\n    response = api_client.post_graphql(query, {\'id\': str(photo_fixture_snow.id)})\n    assert response.status_code == 200\n    data = get_graphql_content(response)\n    assert data[\'data\'][\'photo\'][\'url\'].startswith(\'/thumbnails\')\n\n\ndef test_get_photos(photo_fixture_snow, photo_fixture_tree, api_client):\n    query = """"""\n        {\n            allPhotos {\n                edges {\n                    node {\n                        url\n                    }\n                }\n            }\n        }\n    """"""\n    response = api_client.post_graphql(query, {\'id\': str(photo_fixture_snow.id)})\n    assert response.status_code == 200\n    data = get_graphql_content(response)\n    assert len(data[\'data\'][\'allPhotos\'][\'edges\']) == 2\n    assert data[\'data\'][\'allPhotos\'][\'edges\'][0][\'node\'][\'url\'].startswith(\'/thumbnails\')\n\n\ndef test_filter_photos(photo_fixture_snow, photo_fixture_tree, api_client):\n    tree_tag, _ = Tag.objects.get_or_create(name=\'Tree\', type=\'O\')\n    tree_photo_tag, _ = PhotoTag.objects.get_or_create(photo=photo_fixture_snow, tag=tree_tag, confidence=1.0)\n\n    multi_filter = f\'tag:{tree_tag.id}\'\n\n    query = """"""\n        query PhotoQuery($filters: String) {\n            allPhotos(multiFilter: $filters) {\n                edges {\n                    node {\n                        id\n                    }\n                }\n            }\n        }\n    """"""\n    response = api_client.post_graphql(query, {\'filters\': multi_filter})\n\n    assert response.status_code == 200\n    data = get_graphql_content(response)\n    assert len(data[\'data\'][\'allPhotos\'][\'edges\']) == 1\n    assert data[\'data\'][\'allPhotos\'][\'edges\'][0][\'node\'][\'id\'] == str(photo_fixture_snow.id)\n\n    # Add \'Tree\' tag to another photo. Querying again should return 2 photos\n    tree_photo_tag, _ = PhotoTag.objects.get_or_create(photo=photo_fixture_tree, tag=tree_tag, confidence=1.0)\n    response = api_client.post_graphql(query, {\'filters\': multi_filter})\n\n    assert response.status_code == 200\n    data = get_graphql_content(response)\n    assert len(data[\'data\'][\'allPhotos\'][\'edges\']) == 2\n\n    # Add \'Tree\' to the last photo again (allowed). Querying should not return duplicates\n    tree_photo_tag, _ = PhotoTag.objects.get_or_create(photo=photo_fixture_tree, tag=tree_tag, confidence=0.9)\n    response = api_client.post_graphql(query, {\'filters\': multi_filter})\n    assert response.status_code == 200\n    data = get_graphql_content(response)\n    assert len(data[\'data\'][\'allPhotos\'][\'edges\']) == 2'"
tests/test_metadata.py,0,"b'from pathlib import Path\n\nfrom photonix.photos.utils.metadata import PhotoMetadata, parse_gps_location, get_datetime\n\n\ndef test_metadata():\n    # General exif metadata\n    photo_path = str(Path(__file__).parent / \'photos\' / \'snow.jpg\')\n    metadata = PhotoMetadata(photo_path)\n    assert metadata.get(\'Image Size\') == \'800x600\'\n    assert metadata.get(\'Date Time\') == \'2018:02:28 07:16:25\'\n    assert metadata.get(\'Make\') == \'Xiaomi\'\n    assert metadata.get(\'ISO\') == \'100\'\n\n\ndef test_location():\n    # Conversion from GPS exif data to latitude/longitude\n    gps_position = \'64 deg 9\\\' 0.70"" N, 21 deg 56\\\' 3.47"" W\'\n    latitude, longitude = parse_gps_location(gps_position)\n    assert latitude == 64.15011666666668\n    assert longitude == -21.933911666666667\n\n\ndef test_datetime():\n    # Data from exif metadata\n    photo_path = str(Path(__file__).parent / \'photos\' / \'snow.jpg\')\n    parsed_datetime = get_datetime(photo_path)\n    assert parsed_datetime.year == 2018\n    assert parsed_datetime.isoformat() == \'2018-02-28T07:16:25+00:00\'\n\n    # Date extraction from filename\n    photo_path = str(Path(__file__).parent / \'photos\' / \'snow-1999-12-31.jpg\')\n    parsed_datetime = get_datetime(photo_path)\n    assert parsed_datetime.isoformat() == \'1999-12-31T00:00:00\'\n\n    photo_path = str(Path(__file__).parent / \'photos\' / \'snow-20100603.jpg\')\n    parsed_datetime = get_datetime(photo_path)\n    assert parsed_datetime.isoformat() == \'2010-06-03T00:00:00\'\n'"
tests/test_raw_processing.py,0,"b""import os\nfrom pathlib import Path\n\nfrom django.conf import settings\nfrom django.utils import timezone\nimport pytest\n\nfrom photonix.photos.models import PhotoFile, Task\nfrom photonix.photos.utils.fs import download_file\nfrom photonix.photos.utils.raw import generate_jpeg, ensure_raw_processing_tasks, identified_as_jpeg, process_raw_tasks\nfrom photonix.photos.utils.thumbnails import process_generate_thumbnails_tasks\n\n\nPHOTOS = [\n    # -e argument to dcraw means JPEG was extracted without any processing\n    ('Adobe DNG Converter - Canon EOS 5D Mark III - Lossy JPEG compression (3_2).DNG',  'dcraw -e', 1236950, ['https://epixstudios.co.uk/filer/canonical/1562608755/19/', 'https://raw.pixls.us/getfile.php/1023/nice/Adobe%20DNG%20Converter%20-%20Canon%20EOS%205D%20Mark%20III%20-%20Lossy%20JPEG%20compression%20(3:2).DNG']),\n    ('Apple - iPhone 8 - 16bit (4_3).dng',                                              'dcraw -w', 772618, ['https://epixstudios.co.uk/filer/canonical/1562608803/20/', 'https://raw.pixls.us/getfile.php/2835/nice/Apple%20-%20iPhone%208%20-%2016bit%20(4:3).dng']),  # No embedded JPEG\n    ('Canon - Canon PowerShot SX20 IS.DNG',                                             'dcraw -w', 1828344, ['https://epixstudios.co.uk/filer/canonical/1562608852/21/', 'https://raw.pixls.us/getfile.php/861/nice/Canon%20-%20Canon%20PowerShot%20SX20%20IS.DNG']),  # Embedded image but low resolution and not a JPEG\n    ('Canon - EOS 7D - sRAW2 (sRAW) (3:2).CR2',                                         'dcraw -e', 2264602, ['https://epixstudios.co.uk/filer/canonical/1562608895/22/', 'https://raw.pixls.us/getfile.php/129/nice/Canon%20-%20EOS%207D%20-%20sRAW2%20(sRAW)%20(3:2).CR2']),\n    ('Canon - Powershot SX110IS - CHDK.CR2',                                            'dcraw -w', 1493825, ['https://epixstudios.co.uk/filer/canonical/1562608935/23/', 'https://raw.pixls.us/getfile.php/144/nice/Canon%20-%20Powershot%20SX110IS%20-%20CHDK.CR2']),  # No embedded JPEG, No metadata about image dimensions for us to compare against\n    ('Leica - D-LUX 5 - 16_9.RWL',                                                      'dcraw -w', 1478207, ['https://epixstudios.co.uk/filer/canonical/1562608970/24/', 'https://raw.pixls.us/getfile.php/2808/nice/Leica%20-%20D-LUX%205%20-%2016:9.RWL']),  # Less common aspect ratio, fairly large embedded JPEG but not similar enough to the raw's dimensions\n    ('Nikon - 1 J1 - 12bit compressed (Lossy (type 2)) (3_2).NEF',                      'dcraw -e', 635217, ['https://epixstudios.co.uk/filer/canonical/1562608457/18/', 'https://raw.pixls.us/getfile.php/2956/nice/Nikon%20-%201%20J1%20-%2012bit%20compressed%20(Lossy%20(type%202))%20(3:2).NEF']),\n    ('Sony - SLT-A77 - 12bit compressed (3_2).ARW',                                     'dcraw -w', 859814, ['https://epixstudios.co.uk/filer/canonical/1562609121/25/', 'https://raw.pixls.us/getfile.php/2691/nice/Sony%20-%20SLT-A77%20-%2012bit%20compressed%20(3:2).ARW']),  # Large embedded JPEG but not the right aspect ratio and smaller than raw\n]\n\n\ndef test_extract_jpg():\n    for fn, intended_process_params, intended_filesize, urls in PHOTOS:\n        raw_photo_path = str(Path(__file__).parent / 'photos' / fn)\n        if not os.path.exists(raw_photo_path):\n            for url in urls:\n                try:\n                    download_file(url, raw_photo_path)\n                    if not os.path.exists(raw_photo_path) or os.stat(raw_photo_path).st_size < 1024 * 1024:\n                        try:\n                            os.remove(raw_photo_path)\n                        except:\n                            pass\n                    else:\n                        break\n                except:\n                    pass\n\n        output_path, _, process_params, _ = generate_jpeg(raw_photo_path)\n\n        assert process_params == intended_process_params\n        assert identified_as_jpeg(output_path) == True\n        filesizes = [intended_filesize, os.stat(output_path).st_size]\n        assert min(filesizes) / max(filesizes) > 0.8  # Within 20% of the intended JPEG filesize\n\n        os.remove(output_path)\n\n\n@pytest.fixture\ndef photo_fixture_raw(db):\n    from photonix.photos.utils.db import record_photo\n    photo_index = 4  # Photo selected because it doesn't have width and height metadata\n    raw_photo_path = str(Path(__file__).parent / 'photos' / PHOTOS[photo_index][0])\n    if not os.path.exists(raw_photo_path):\n        urls = PHOTOS[photo_index][3]\n        for url in urls:\n            try:\n                download_file(url, raw_photo_path)\n                break\n            except:\n                pass\n    return record_photo(raw_photo_path)\n\n\ndef test_task_raw_processing(photo_fixture_raw):\n    # Task should have been created for the fixture\n    task = Task.objects.get(type='ensure_raw_processed', status='P', subject_id=photo_fixture_raw.id)\n    assert (timezone.now() - task.created_at).seconds < 1\n    assert (timezone.now() - task.updated_at).seconds < 1\n    assert task.started_at == None\n    assert task.finished_at == None\n    assert task.status == 'P'\n    assert task.complete_with_children == True\n\n    # Calling this function should create a child task tp generate a JPEG from the raw file\n    ensure_raw_processing_tasks()\n    parent_task = Task.objects.get(type='ensure_raw_processed', subject_id=photo_fixture_raw.id)\n    child_task = Task.objects.get(type='process_raw', parent=parent_task)\n    assert parent_task.status == 'S'\n    assert child_task.status == 'P'\n\n    # PhotoFile should have been created widthout dimensions as metadata for this photo doesn't include it\n    photo_file = PhotoFile.objects.get(id=child_task.subject_id)\n    assert photo_file.width is None\n\n    # Call the processing function\n    process_raw_tasks()\n\n    # Tasks should be now marked as completed\n    parent_task = Task.objects.get(type='ensure_raw_processed', subject_id=photo_fixture_raw.id)\n    child_task = Task.objects.get(type='process_raw', parent=parent_task)\n    assert parent_task.status == 'C'\n    assert child_task.status == 'C'\n\n    # PhotoFile object should have been updated to show raw file has been processed\n    photo_file = PhotoFile.objects.get(id=child_task.subject_id)\n    assert photo_file.raw_processed == True\n    assert photo_file.raw_version == 20190305\n    assert photo_file.raw_external_params == 'dcraw -w'\n    assert '9.' in photo_file.raw_external_version\n    output_path = Path(settings.PHOTO_RAW_PROCESSED_DIR) / '{}.jpg'.format(photo_file.id)\n    assert os.path.exists(output_path)\n    assert os.path.exists(output_path) == os.path.exists(photo_fixture_raw.base_image_path)\n    assert os.stat(output_path).st_size > 1024 * 1024  # JPEG greater than 1MB in size\n    assert photo_file.width == 3684  # Width should now be set\n\n    # Thumbnailing task should have been created as ensure_raw_processed and process_raw have completed\n    assert Task.objects.filter(type='generate_thumbnails', subject_id=photo_fixture_raw.id).count() == 1\n    task = Task.objects.get(type='generate_thumbnails', subject_id=photo_fixture_raw.id)\n    assert (timezone.now() - task.created_at).seconds < 1\n    assert (timezone.now() - task.updated_at).seconds < 1\n    assert task.started_at == None\n    assert task.finished_at == None\n\n    # Process tasks to generate thumbnails which should add new task for classification\n    process_generate_thumbnails_tasks()\n    task = Task.objects.get(type='generate_thumbnails', subject_id=photo_fixture_raw.id)\n    assert task.status == 'C'\n    assert (timezone.now() - task.started_at).seconds < 10\n    assert (timezone.now() - task.finished_at).seconds < 1\n\n    # Make sure thumbnails got generated\n    for thumbnail in settings.THUMBNAIL_SIZES:\n        if thumbnail[4]:\n            path = photo_fixture_raw.thumbnail_path(thumbnail)\n            assert os.path.exists(path)\n    thumbnail_path = photo_fixture_raw.thumbnail_path((256, 256, 'cover', 50))\n    assert os.stat(thumbnail_path).st_size > 9463 * 0.8\n    assert os.stat(thumbnail_path).st_size < 9463 * 1.2\n\n    # Tidy up filesystem\n    os.remove(output_path)\n    for thumbnail in settings.THUMBNAIL_SIZES:\n        if thumbnail[4]:\n            path = photo_fixture_raw.thumbnail_path(thumbnail)\n            os.remove(path)\n"""
tests/test_task_queue.py,0,"b""from pathlib import Path\n\nimport pytest\n\nfrom django.utils import timezone\n\nfrom photonix.photos.models import Task\nfrom photonix.photos.utils.classification import process_classify_images_tasks\nfrom photonix.photos.utils.raw import ensure_raw_processing_tasks\nfrom photonix.photos.utils.thumbnails import process_generate_thumbnails_tasks\n\n# pytestmark = pytest.mark.django_db\n\n\n@pytest.fixture\ndef photo_fixture_snow(db):\n    from photonix.photos.utils.db import record_photo\n    snow_path = str(Path(__file__).parent / 'photos' / 'snow.jpg')\n    return record_photo(snow_path)\n\n\ndef test_tasks_created_updated(photo_fixture_snow):\n    # Task should have been created for the fixture\n    task = Task.objects.get(type='ensure_raw_processed', status='P', subject_id=photo_fixture_snow.id)\n    assert (timezone.now() - task.created_at).seconds < 1\n    assert (timezone.now() - task.updated_at).seconds < 1\n    assert task.started_at == None\n    assert task.finished_at == None\n\n    # Test manually starting makes intended changes\n    task.start()\n    assert task.status == 'S'\n    assert (timezone.now() - task.started_at).seconds < 1\n\n    # Undo last changes\n    task.status = 'P'\n    task.started_at = None\n    task.save()\n\n    # Calling this function should complete the task and queue up a new one for generating thumbnails\n    ensure_raw_processing_tasks()\n    task = Task.objects.get(type='ensure_raw_processed', subject_id=photo_fixture_snow.id)\n    assert task.status == 'C'\n    assert (timezone.now() - task.started_at).seconds < 1\n    assert (timezone.now() - task.finished_at).seconds < 1\n\n    # Check next task has been created\n    task = Task.objects.get(type='generate_thumbnails', subject_id=photo_fixture_snow.id)\n    assert task.status == 'P'\n    assert (timezone.now() - task.created_at).seconds < 1\n    assert (timezone.now() - task.updated_at).seconds < 1\n    assert task.started_at == None\n    assert task.finished_at == None\n\n    # Process tasks to generate thumbnails which should add new task for classification\n    process_generate_thumbnails_tasks()\n    task = Task.objects.get(type='generate_thumbnails', subject_id=photo_fixture_snow.id)\n    assert task.status == 'C'\n    assert (timezone.now() - task.started_at).seconds < 10\n    assert (timezone.now() - task.finished_at).seconds < 1\n\n    # Chekc next task has been added to classify images\n    task = Task.objects.get(type='classify_images', subject_id=photo_fixture_snow.id)\n    assert task.status == 'P'\n    assert (timezone.now() - task.created_at).seconds < 1\n    assert (timezone.now() - task.updated_at).seconds < 1\n    assert task.started_at == None\n    assert task.finished_at == None\n\n    # Processing the classification task should create child processes\n    assert task.complete_with_children == False\n    assert task.status == 'P'\n    process_classify_images_tasks()\n    task = Task.objects.get(type='classify_images', subject_id=photo_fixture_snow.id)\n    assert task.status == 'S'\n    assert task.children.count() == 4\n    assert task.complete_with_children == True\n\n    # Completing all the child processes should set the parent task to completed\n    for child in task.children.all():\n        assert child.status == 'P'\n        child.start()\n        assert task.status == 'S'\n        assert child.status == 'S'\n        child.complete()\n        assert child.status == 'C'\n    assert task.status == 'C'\n"""
tests/test_thumbnails.py,0,"b""import os\nfrom pathlib import Path\n\nimport pytest\n\nfrom django.conf import settings\nfrom django.test import Client\nfrom photonix.photos.utils.thumbnails import get_thumbnail, get_thumbnail_path\n\n\n@pytest.fixture\ndef photo_fixture_snow(db):\n    from photonix.photos.utils.db import record_photo\n    snow_path = str(Path(__file__).parent / 'photos' / 'snow.jpg')\n    return record_photo(snow_path)\n\n\ndef test_generate_thumbnail(photo_fixture_snow):\n    width, height, crop, quality, _ = settings.THUMBNAIL_SIZES[0]\n    assert width == 256\n    assert height == 256\n    assert crop == 'cover'\n    assert quality == 50\n\n    # Should generate image thumbnail and return bytes as we specified in the return_type\n    result = get_thumbnail(photo_fixture_snow.id, width, height, crop, quality, return_type='bytes')\n    assert result[:10] == b'\\xff\\xd8\\xff\\xe0\\x00\\x10JFIF'\n\n    # It should also have saved the thumbnail data down to disk\n    path = get_thumbnail_path(photo_fixture_snow, width, height, crop, quality)\n    assert os.path.exists(path)\n    assert path.endswith(str(Path('cache') / 'thumbnails' / '256x256_cover_q50' / '{}.jpg'.format(str(photo_fixture_snow))))\n    assert len(result) == os.stat(path).st_size\n    assert os.stat(path).st_size > 5929 * 0.8\n    assert os.stat(path).st_size < 5929 * 1.2\n\n\ndef test_view(photo_fixture_snow):\n    # Start with no thumbnail on disk\n    width, height, crop, quality, _ = settings.THUMBNAIL_SIZES[0]\n    path = get_thumbnail_path(photo_fixture_snow, width, height, crop, quality)\n    assert not os.path.exists(path)\n\n    # Make a web request to the thumbnail API\n    client = Client()\n    url = '/thumbnails/256x256_cover_q50/{}/'.format(photo_fixture_snow.id)\n    response = client.get(url)\n\n    # We should get thumbnail back\n    assert response.status_code == 200\n    assert response.content[:10] == b'\\xff\\xd8\\xff\\xe0\\x00\\x10JFIF'\n    assert response._headers['content-type'][1] == 'image/jpeg'\n    response_length = len(response.content)\n    assert response_length > 5929 * 0.8\n    assert response_length < 5929 * 1.2\n\n    # Thumbnail should be now on disk\n    assert os.path.exists(path)\n\n    # Now we check that if we make the same request again, the file on disk is used rather than re-generating\n    # Modify the file by appending 4 bytes\n    with open(path, 'a') as thumbfile:\n        thumbfile.write('test')\n\n    # Get and check the new file length\n    response = client.get(url)\n    assert len(response.content) == (response_length + 4)\n    os.remove(path)\n"""
tests/utils.py,0,"b'import json\n\n\n# These util functions come from the Saleor project and are licensed as BSD-3-Clause\n# https://github.com/mirumee/saleor/blob/master/tests/api/utils.py\n\ndef _get_graphql_content_from_response(response):\n    return json.loads(response.content.decode(""utf8""))\n\n\ndef get_graphql_content(response):\n    """"""Get\'s GraphQL content from the response, and optionally checks if it\n    contains any operating-related errors, eg. schema errors or lack of\n    permissions.\n    """"""\n    content = _get_graphql_content_from_response(response)\n    assert ""errors"" not in content, content[""errors""]\n    return content\n'"
photonix/classifiers/__init__.py,0,b''
photonix/classifiers/base_model.py,0,"b'import hashlib\nimport json\nimport os\nimport random\nimport shutil\nimport subprocess\nimport tempfile\nfrom pathlib import Path\nimport logging\n\nimport requests\n\nimport redis\nfrom redis_lock import Lock\n\ngraph_cache = {}\n\nlogger = logging.getLogger(__name__)\n\nclass BaseModel:\n    def __init__(self, model_dir=None):\n        global graph_cache\n        self.graph_cache = graph_cache\n\n        if model_dir:\n            self.model_dir = model_dir\n        else:\n            try:\n                from django.conf import settings\n                self.model_dir = settings.MODEL_DIR\n            except:\n                self.model_dir = str(Path(__file__).parent.parent.parent / \'data\' / \'models\')\n\n    @property\n    def graph_cache_key(self):\n        return \'{}:{}\'.format(self.name, self.model_dir)\n\n    def get_model_info(self):\n        from django.conf import settings\n        response = requests.get(settings.MODEL_INFO_URL)\n        models_info = json.loads(response.content)\n        model_info = models_info[self.name][str(self.version)]\n        return model_info\n\n    def ensure_downloaded(self, lock_name=None):\n        if self.graph_cache_key in self.graph_cache:\n            return True\n\n        version_file = os.path.join(self.model_dir, self.name, \'version.txt\')\n        if not lock_name:\n            lock_name = \'classifier_{}_download\'.format(self.name)\n\n        r = redis.Redis(host=os.environ.get(\'REDIS_HOST\', \'127.0.0.1\'))\n        with Lock(r, lock_name):\n            try:\n                with open(version_file) as f:\n                    if f.read().strip() == str(self.version):\n                        return True\n            except FileNotFoundError:\n                pass\n\n            model_info = self.get_model_info()\n            error = False\n\n            for file_data in model_info[\'files\']:\n                final_path = os.path.join(self.model_dir, self.name, file_data[\'filename\'])\n                if not os.path.exists(final_path):\n                    locations = file_data[\'locations\']\n                    index = random.choice(range(len(locations)))\n                    location = locations.pop(index)\n                    hash_sha256 = hashlib.sha256()\n                    request = requests.get(location, stream=True)\n\n                    if request.status_code != 200:\n                        error = True\n                        logger.error(f""Failed to fetch model for {location}: ""\n                                     f""Got {request.status_code}"")\n                        continue\n\n                    # Download file to temporary location\n                    with tempfile.NamedTemporaryFile(mode=\'w+b\', delete=False) as f:\n                        for chunk in request.iter_content(chunk_size=1024 * 1024):  # 1MB chunks\n                            if chunk:  # filter out keep-alive new chunks\n                                f.write(chunk)\n                                hash_sha256.update(chunk)\n\n                    # Move file to correct location if the hash matches\n                    if hash_sha256.hexdigest() == file_data[\'sha256\']:\n                        dirname = os.path.dirname(final_path)\n                        if not os.path.isdir(dirname):\n                            os.makedirs(dirname)\n\n                        if file_data.get(\'decompress\'):\n                            if file_data[\'filename\'].endswith(\'.xz\'):\n                                xz_path = \'{}.xz\'.format(f.name)\n                                shutil.move(f.name, xz_path)\n                                subprocess.run([\'unxz\', xz_path])\n                                final_path = final_path.replace(\'.xz\', \'\')\n\n                        shutil.move(f.name, final_path)\n                    else:\n                        error = True\n                        logger.error(f""File downloaded from {location} is ""\n                                     ""corrupt as indicated by bad hash"")\n                        # TODO: Delete badly downloaded file\n\n            # Write version file\n            with open(version_file, \'w\') as f:\n                if error:\n                    f.write(\'ERROR\\n\')\n                    return False\n                else:\n                    f.write(\'{}\\n\'.format(str(self.version)))\n                    return True\n'"
photonix/classifiers/runners.py,0,"b""import re\nfrom uuid import UUID\n\n\ndef get_or_create_tag(name, type, source, parent=None):\n    # get_or_create is not atomic so an instance could get created by another thread inbetween.\n    # This causes an IntegrityError due to the unique_together constraint.\n    from django.db import IntegrityError, transaction\n    from photonix.photos.models import Tag\n\n    try:\n        with transaction.atomic():\n            tag, _ = Tag.objects.get_or_create(name=name, type=type, source=source, parent=parent)\n    except IntegrityError:\n        tag = Tag.objects.get(name=name, type=type, source=source, parent=parent)\n    return tag\n\n\ndef results_for_model_on_photo(model, photo_id):\n    is_photo_instance = False\n    photo = None\n\n    if isinstance(photo_id, UUID):\n        is_photo_instance = True\n    elif isinstance(photo_id, str):\n        if re.match(r'\\b[0-9a-f]{8}\\b-[0-9a-f]{4}-[0-9a-f]{4}-[0-9a-f]{4}-\\b[0-9a-f]{12}\\b', photo_id):  # Is UUID\n            is_photo_instance = True\n    elif hasattr(photo_id, 'id'):\n        photo = photo_id\n\n    # Is an individual filename so return the prediction\n    if not is_photo_instance:\n        return None, model.predict(photo_id)\n\n    # Is a Photo model instance so needs saving\n    if not photo:\n        from photonix.photos.models import Photo\n        photo = Photo.objects.get(id=photo_id)\n\n    results = model.predict(photo.base_image_path)\n\n    return is_photo_instance and photo or None, results\n"""
photonix/common/__init__.py,0,b''
photonix/common/models.py,0,"b'import uuid\n\nfrom django.db import models\nfrom django.utils import timezone\n\n\nclass UUIDModel(models.Model):\n    id = models.UUIDField(primary_key=True, default=uuid.uuid4, editable=False)\n\n    class Meta:\n        abstract = True\n\n\nclass VersionedModel(models.Model):\n    created_at = models.DateTimeField(blank=True, db_index=True)\n    updated_at = models.DateTimeField(blank=True)\n\n    class Meta:\n        abstract = True\n\n    def save(self, *args, **kwargs):\n        now = timezone.now()\n        if not self.created_at:\n            self.created_at = now\n        self.updated_at = now\n        super(VersionedModel, self).save()\n'"
photonix/photos/__init__.py,0,"b""default_app_config = 'photonix.photos.app.PhotosConfig'\n"""
photonix/photos/admin.py,0,"b'from django.contrib import admin\n\nfrom .models import Camera, Lens, Photo, PhotoFile, Tag, PhotoTag\n\n\nclass CameraAdmin(admin.ModelAdmin):\n    pass\n\n\nclass LensAdmin(admin.ModelAdmin):\n    pass\n\n\nclass PhotoAdmin(admin.ModelAdmin):\n    pass\n\n\nclass PhotoFileAdmin(admin.ModelAdmin):\n    pass\n\n\nclass TagAdmin(admin.ModelAdmin):\n    pass\n\n\nclass PhotoTagAdmin(admin.ModelAdmin):\n    pass\n\n\nadmin.site.register(Camera, CameraAdmin)\nadmin.site.register(Lens, LensAdmin)\nadmin.site.register(Photo, PhotoAdmin)\nadmin.site.register(PhotoFile, PhotoFileAdmin)\nadmin.site.register(Tag, TagAdmin)\nadmin.site.register(PhotoTag, PhotoTagAdmin)\n'"
photonix/photos/app.py,0,"b""from django.apps import AppConfig\n\n\nclass PhotosConfig(AppConfig):\n    name = 'photonix.photos'\n    label = 'photos'\n    verbose_name = 'Photonix Photos'\n"""
photonix/photos/consumers.py,0,"b""import json\n\nfrom django.conf import settings\n\nfrom photonix.photos.utils.organise import import_photos_in_place\nfrom photonix.photos.utils.thumbnails import generate_thumbnails_for_photo\n\n\ndef rescan_photos(message):\n    paths = [item['PATH'] for item in settings.PHOTO_OUTPUT_DIRS]\n    for path in paths:\n        import_photos_in_place(path)\n\n\ndef photo_added(message):\n    if message:\n        data = json.loads(message['text'])\n        if data['id']:\n            generate_thumbnails_for_photo(data['id'])\n"""
photonix/photos/models.py,0,"b""from __future__ import unicode_literals\nfrom pathlib import Path\n\nfrom django.conf import settings\nfrom django.db import models, transaction\nfrom django.utils import timezone\n\nfrom photonix.common.models import UUIDModel, VersionedModel\n\n\nclass Camera(UUIDModel, VersionedModel):\n    make            = models.CharField(max_length=128)\n    model           = models.CharField(max_length=128)\n    earliest_photo  = models.DateTimeField()\n    latest_photo    = models.DateTimeField()\n\n    class Meta:\n        unique_together = [['make', 'model']]\n        ordering = ['make', 'model']\n        app_label = 'photos'\n\n    def __str__(self):\n        return '{} {}'.format(self.make, self.model)\n\n\nclass Lens(UUIDModel, VersionedModel):\n    name            = models.CharField(max_length=128)\n    earliest_photo  = models.DateTimeField()\n    latest_photo    = models.DateTimeField()\n\n    class Meta:\n        verbose_name_plural = 'lenses'\n        ordering = ['name']\n\n    def __str__(self):\n        return self.name\n\n\nclass Photo(UUIDModel, VersionedModel):\n    visible                             = models.BooleanField(default=False)\n    taken_at                            = models.DateTimeField(null=True)\n    taken_by                            = models.CharField(max_length=128, blank=True, null=True)\n    aperture                            = models.DecimalField(max_digits=3, decimal_places=1, null=True)\n    exposure                            = models.CharField(max_length=8, blank=True, null=True)\n    iso_speed                           = models.PositiveIntegerField(null=True)\n    focal_length                        = models.DecimalField(max_digits=4, decimal_places=1, null=True)\n    flash                               = models.NullBooleanField()\n    metering_mode                       = models.CharField(max_length=32, null=True)\n    drive_mode                          = models.CharField(max_length=32, null=True)\n    shooting_mode                       = models.CharField(max_length=32, null=True)\n    camera                              = models.ForeignKey(Camera, related_name='photos', null=True, on_delete=models.CASCADE)\n    lens                                = models.ForeignKey(Lens, related_name='photos', null=True, on_delete=models.CASCADE)\n    latitude                            = models.DecimalField(max_digits=9, decimal_places=6, null=True)\n    longitude                           = models.DecimalField(max_digits=9, decimal_places=6, null=True)\n    altitude                            = models.DecimalField(max_digits=6, decimal_places=1, null=True)\n\n    class Meta:\n        ordering = ['-taken_at']\n\n    def __str__(self):\n        return str(self.id)\n\n    # @property\n    # def country(self):\n    #     return country_from_point_field(self.location)\n\n    def thumbnail_url(self, thumbnail):\n        return '/thumbnails/{}x{}_{}_q{}/{}.jpg'.format(thumbnail[0], thumbnail[1], thumbnail[2], thumbnail[3], self.id)\n\n    def thumbnail_path(self, thumbnail):\n        return str(Path(settings.THUMBNAIL_ROOT) / '{}x{}_{}_q{}/{}.jpg'.format(thumbnail[0], thumbnail[1], thumbnail[2], thumbnail[3], self.id))\n\n    @property\n    def base_file(self):\n        preferred_files = self.files.filter(preferred=True)\n        if not preferred_files:\n            preferred_files = self.files.filter(raw_processed=True)\n        if not preferred_files:\n            preferred_files = self.files.filter(mimetype='image/jpeg').order_by('-created_at')\n        if preferred_files:\n            return preferred_files[0]\n        return None\n\n    @property\n    def base_image_path(self):\n        return self.base_file.base_image_path\n\n    @property\n    def dimensions(self):\n        file = self.base_file\n        if file:\n            return (file.width, file.height)\n        return (None, None)\n\n    def clear_tags(self, source, type):\n        self.photo_tags.filter(tag__source=source, tag__type=type).delete()\n\n\nclass PhotoFile(UUIDModel, VersionedModel):\n    photo                   = models.ForeignKey(Photo, related_name='files', on_delete=models.CASCADE)\n    path                    = models.CharField(max_length=512)\n    width                   = models.PositiveIntegerField(null=True)\n    height                  = models.PositiveIntegerField(null=True)\n    mimetype                = models.CharField(max_length=32, blank=True, null=True)\n    file_modified_at        = models.DateTimeField()\n    bytes                   = models.PositiveIntegerField()\n    preferred               = models.BooleanField(default=False)\n    raw_processed           = models.BooleanField(default=False)\n    raw_version             = models.PositiveIntegerField(null=True)\n    raw_external_params     = models.CharField(max_length=16, blank=True, null=True)\n    raw_external_version    = models.CharField(max_length=16, blank=True, null=True)\n\n    def __str__(self):\n        return str(self.path)\n\n    @property\n    def url(self):\n        return self.path.split('/data', 1)[1]\n\n    @property\n    def base_image_path(self):\n        if self.raw_processed:\n            return str(Path(settings.PHOTO_RAW_PROCESSED_DIR) / str('{}.jpg'.format(self.id)))\n        return self.path\n\n\nSOURCE_CHOICES = (\n    ('H', 'Human'),\n    ('C', 'Computer'),\n)\nTAG_TYPE_CHOICES = (\n    ('L', 'Location'),\n    ('O', 'Object'),\n    ('F', 'Face'),\n    ('C', 'Color'),\n    ('S', 'Style'),  # See Karayev et al.: Recognizing Image Style\n)\n\n\nclass Tag(UUIDModel, VersionedModel):\n    name            = models.CharField(max_length=128)\n    parent          = models.ForeignKey('Tag', related_name='+', null=True, on_delete=models.CASCADE)\n    type            = models.CharField(max_length=1, choices=TAG_TYPE_CHOICES, null=True)\n    source          = models.CharField(max_length=1, choices=SOURCE_CHOICES)\n\n    class Meta:\n        ordering = ['name']\n        unique_together = (('name', 'type', 'source'),)\n\n    def __str__(self):\n        return '{} ({})'.format(self.name, self.type)\n\n\nclass PhotoTag(UUIDModel, VersionedModel):\n    photo           = models.ForeignKey(Photo, related_name='photo_tags', on_delete=models.CASCADE)\n    tag             = models.ForeignKey(Tag, related_name='photo_tags', on_delete=models.CASCADE)\n    source          = models.CharField(max_length=1, choices=SOURCE_CHOICES)\n    model_version   = models.PositiveIntegerField(null=True)\n    confidence      = models.FloatField()\n    significance    = models.FloatField(null=True)\n    verified        = models.BooleanField(default=False)\n    hidden          = models.BooleanField(default=False)\n    # Optional bounding boxes from object detection or face detection\n    position_x      = models.FloatField(null=True)\n    position_y      = models.FloatField(null=True)\n    size_x          = models.FloatField(null=True)\n    size_y          = models.FloatField(null=True)\n\n    class Meta:\n        ordering = ['-significance']\n\n    def __str__(self):\n        return '{}: {}'.format(self.photo, self.tag)\n\n\nTASK_STATUS_CHOICES = (\n    ('P', 'Pending'),\n    ('S', 'Started'),\n    ('C', 'Completed'),\n    ('F', 'Failed'),\n)\n\nclass Task(UUIDModel, VersionedModel):\n    type                    = models.CharField(max_length=128, db_index=True)\n    subject_id              = models.UUIDField(db_index=True)\n    status                  = models.CharField(max_length=1, choices=TAG_TYPE_CHOICES, default='P', db_index=True)\n    started_at              = models.DateTimeField(null=True)\n    finished_at             = models.DateTimeField(null=True)\n    parent                  = models.ForeignKey('self', related_name='children', null=True, on_delete=models.CASCADE)\n    complete_with_children  = models.BooleanField(default=False)\n\n    class Meta:\n        ordering = ['created_at']\n\n    def __str__(self):\n        return '{}: {}'.format(self.type, self.created_at)\n\n    def start(self):\n        self.status = 'S'\n        self.started_at = timezone.now()\n        self.save()\n\n    def complete(self, next_type=None, next_subject_id=None):\n        # Set status of current task and queue up next task if appropriate\n        self.status = 'C'\n        self.finished_at = timezone.now()\n        self.save()\n\n        # Create next task in the chain if there should be one\n        if not self.parent and next_type:\n            Task(type=next_type, subject_id=next_subject_id).save()\n\n        if self.parent and self.parent.complete_with_children:\n            # If all siblings are complete, we should mark our parent as complete\n            with transaction.atomic():\n                # select_for_update() will block if another process is working with these children\n                siblings = self.parent.children.select_for_update().filter(status='C')\n                if siblings.count() == self.parent.children.count():\n                    self.parent.complete(next_type=next_type, next_subject_id=next_subject_id)\n\n    def failed(self):\n        self.status = 'F'\n        self.finished_at = timezone.now()\n        self.save()\n"""
photonix/photos/schema.py,0,"b""from django.conf import settings\nimport django_filters\nfrom django_filters import CharFilter\nimport graphene\nfrom graphene_django.filter import DjangoFilterConnectionField\nfrom graphene_django.types import DjangoObjectType\n\nfrom .models import Camera, Lens, Photo, Tag, PhotoTag\n\n\nclass CameraType(DjangoObjectType):\n    class Meta:\n        model = Camera\n\n\nclass LensType(DjangoObjectType):\n    class Meta:\n        model = Lens\n\nclass PhotoTagType(DjangoObjectType):\n    class Meta:\n        model = PhotoTag\n\n\nclass CustomNode(graphene.Node):\n\n    class Meta:\n        name = 'Node'\n\n    @staticmethod\n    def to_global_id(type, id):\n        return id\n\n\nclass PhotoInterface(graphene.Interface):\n    photo_tags__tag__id = graphene.String()\n    multi_filter = graphene.String()\n\n\nclass PhotoNode(DjangoObjectType):\n    url = graphene.String()\n    location = graphene.String()\n    location_tags = graphene.List(PhotoTagType)\n    object_tags = graphene.List(PhotoTagType)\n    color_tags = graphene.List(PhotoTagType)\n    style_tags = graphene.List(PhotoTagType)\n    width = graphene.Int()\n    height = graphene.Int()\n\n    class Meta:\n        model = Photo\n        interfaces = (CustomNode, PhotoInterface)\n\n    def resolve_location(self, info):\n        if self.latitude and self.longitude:\n            return '{},{}'.format(self.latitude, self.longitude)\n        return None\n\n    def resolve_url(self, info):\n        size = settings.THUMBNAIL_SIZES[-1]\n        return self.thumbnail_url(size)\n\n    def resolve_location_tags(self, info):\n        return self.photo_tags.filter(tag__type='L')\n\n    def resolve_object_tags(self, info):\n        return self.photo_tags.filter(tag__type='O')\n\n    def resolve_color_tags(self, info):\n        return self.photo_tags.filter(tag__type='C')\n\n    def resolve_style_tags(self, info):\n        return self.photo_tags.filter(tag__type='S')\n\n    def resolve_width(self, info):\n        return self.dimensions[0]\n\n    def resolve_height(self, info):\n        return self.dimensions[1]\n\n\nclass PhotoFilter(django_filters.FilterSet):\n    multi_filter = CharFilter(method='multi_filter_filter')\n\n    class Meta:\n        model = Photo\n        fields = {\n            'aperture': ['exact'],\n            'camera__id': ['exact'],\n            'camera__make': ['exact', 'icontains'],\n            'lens__id': ['exact'],\n            'photo_tags__tag__id': ['exact', 'in', 'icontains'],\n            'photo_tags__tag__name': ['exact', 'icontains', 'in'],\n        }\n\n    def sanitize(self, value_list):\n        return [v for v in value_list if v != '']  # Remove empty items\n\n    def customize(self, value):\n        return value\n\n    def multi_filter_filter(self, queryset, name, value):\n        filters = value.split(',')\n        filters = self.sanitize(filters)\n        filters = map(self.customize, filters)\n\n        has_tags = False\n        for filter_val in filters:\n            if ':' in filter_val:\n                key, val = filter_val.split(':')\n                if key == 'tag':\n                    queryset = queryset.filter(photo_tags__tag__id=val)\n                    has_tags = True\n                elif key == 'camera':\n                    queryset = queryset.filter(camera__id=val)\n                elif key == 'lens':\n                    queryset = queryset.filter(lens__id=val)\n                elif key == 'aperture':\n                    queryset = queryset.filter(aperture=val)\n                elif key == 'exposure':\n                    queryset = queryset.filter(exposure=val)\n                elif key == 'isoSpeed':\n                    queryset = queryset.filter(iso_speed=val)\n                elif key == 'focalLength':\n                    queryset = queryset.filter(focal_length=val)\n                elif key == 'flash':\n                    queryset = queryset.filter(flash=val == 'on' and True or False)\n                elif key == 'meeteringMode':\n                    queryset = queryset.filter(metering_mode=val)\n                elif key == 'driveMode':\n                    queryset = queryset.filter(drive_mode=val)\n                elif key == 'shootingMode':\n                    queryset = queryset.filter(shooting_mode=val)\n        if has_tags:\n            queryset.order_by('-photo_tags__significance')\n        return queryset.distinct()\n\n\nclass LocationTagType(DjangoObjectType):\n    class Meta:\n        model = Tag\n\n\nclass ObjectTagType(DjangoObjectType):\n    class Meta:\n        model = Tag\n\n\nclass PersonTagType(DjangoObjectType):\n    class Meta:\n        model = Tag\n\n\nclass ColorTagType(DjangoObjectType):\n    class Meta:\n        model = Tag\n\n\nclass StyleTagType(DjangoObjectType):\n    class Meta:\n        model = Tag\n\n\nclass Query(object):\n    camera = graphene.Field(CameraType, id=graphene.UUID(), make=graphene.String(), model=graphene.String())\n    all_cameras = graphene.List(CameraType)\n\n    lens = graphene.Field(LensType, id=graphene.UUID(), name=graphene.String())\n    all_lenses = graphene.List(LensType)\n\n    all_apertures = graphene.List(graphene.Float)\n    all_exposures = graphene.List(graphene.String)\n    all_iso_speeds = graphene.List(graphene.Int)\n    all_focal_lengths = graphene.List(graphene.Float)\n    all_metering_modes = graphene.List(graphene.String)\n    all_drive_modes = graphene.List(graphene.String)\n    all_shooting_modes = graphene.List(graphene.String)\n\n    photo = graphene.Field(PhotoNode, id=graphene.UUID())\n    all_photos = DjangoFilterConnectionField(PhotoNode, filterset_class=PhotoFilter)\n\n    all_location_tags = graphene.List(LocationTagType)\n    all_object_tags = graphene.List(ObjectTagType)\n    all_person_tags = graphene.List(PersonTagType)\n    all_color_tags = graphene.List(ColorTagType)\n    all_style_tags = graphene.List(StyleTagType)\n\n    def resolve_camera(self, info, **kwargs):\n        id = kwargs.get('id')\n        make = kwargs.get('make')\n        model = kwargs.get('model')\n\n        if id is not None:\n            return Camera.objects.get(pk=id)\n\n        if make is not None and model is not None:\n            return Camera.objects.get(make=make, model=model)\n\n        return None\n\n    def resolve_all_cameras(self, info, **kwargs):\n        return Camera.objects.all()\n\n    def resolve_lens(self, info, **kwargs):\n        id = kwargs.get('id')\n        name = kwargs.get('name')\n\n        if id is not None:\n            return Lens.objects.get(pk=id)\n\n        if name is not None:\n            return Lens.objects.get(name=name)\n\n        return None\n\n    def resolve_all_lenses(self, info, **kwargs):\n        return Lens.objects.all()\n\n    def resolve_all_apertures(self, info, **kwargs):\n        return Photo.objects.exclude(aperture__isnull=True).values_list('aperture', flat=True).distinct().order_by('aperture')\n\n    def resolve_all_exposures(self, info, **kwargs):\n        return Photo.objects.exclude(exposure__isnull=True).values_list('exposure', flat=True).distinct().order_by('exposure')\n\n    def resolve_all_iso_speeds(self, info, **kwargs):\n        return Photo.objects.exclude(iso_speed__isnull=True).values_list('iso_speed', flat=True).distinct().order_by('iso_speed')\n\n    def resolve_all_focal_lengths(self, info, **kwargs):\n        return Photo.objects.exclude(focal_length__isnull=True).values_list('focal_length', flat=True).distinct().order_by('focal_length')\n\n    def resolve_all_metering_modes(self, info, **kwargs):\n        return Photo.objects.exclude(metering_mode__isnull=True).values_list('metering_mode', flat=True).distinct().order_by('metering_mode')\n\n    def resolve_all_drive_modes(self, info, **kwargs):\n        return Photo.objects.exclude(drive_mode__isnull=True).values_list('drive_mode', flat=True).distinct().order_by('drive_mode')\n\n    def resolve_all_shooting_modes(self, info, **kwargs):\n        return Photo.objects.exclude(shooting_mode__isnull=True).values_list('shooting_mode', flat=True).distinct().order_by('shooting_mode')\n\n    def resolve_photo(self, info, **kwargs):\n        id = kwargs.get('id')\n        if id is not None:\n            return Photo.objects.get(pk=id)\n        return None\n\n    def resolve_all_location_tags(self, info, **kwargs):\n        return Tag.objects.filter(type='L')\n\n    def resolve_all_object_tags(self, info, **kwargs):\n        return Tag.objects.filter(type='O')\n\n    def resolve_all_person_tags(self, info, **kwargs):\n        return Tag.objects.filter(type='P')\n\n    def resolve_all_color_tags(self, info, **kwargs):\n        return Tag.objects.filter(type='C')\n\n    def resolve_all_style_tags(self, info, **kwargs):\n        return Tag.objects.filter(type='S')\n"""
photonix/photos/views.py,0,"b""from django.conf import settings\nfrom django.http import HttpResponse, HttpResponseNotFound\n\nfrom photonix.photos.utils.thumbnails import get_thumbnail\n\n\ndef thumbnail_view(request, photo_id, width, height, crop, quality):\n    width = int(width)\n    height = int(height)\n    quality = int(quality)\n\n    thumbnail_size_index = None\n    for i, thumbnail_size in enumerate(settings.THUMBNAIL_SIZES):\n        if width == thumbnail_size[0] and height == thumbnail_size[1] and crop == thumbnail_size[2] and quality == thumbnail_size[3]:\n            thumbnail_size_index = i\n            break\n\n    if thumbnail_size_index is None:\n        return HttpResponseNotFound('No photo thumbnail with these parameters')\n\n    img_bytes = get_thumbnail(photo_id, width, height, crop, quality, return_type='bytes')\n    response = HttpResponse(img_bytes, content_type='image/jpeg')\n    return response\n"""
photonix/web/__init__.py,0,b''
photonix/web/schema.py,0,"b'import graphene\n\nfrom photonix.photos.schema import Query as OtherQuery\n\n\nclass Query(OtherQuery, graphene.ObjectType):\n    # This class will inherit from multiple Queries\n    # as we begin to add more apps to our project\n    pass\n\nschema = graphene.Schema(query=Query)\n'"
photonix/web/settings.py,0,"b'""""""\nDjango settings for web project.\n\nGenerated by \'django-admin startproject\' using Django 1.10.5.\n\nFor more information on this file, see\nhttps://docs.djangoproject.com/en/1.10/topics/settings/\n\nFor the full list of settings and their values, see\nhttps://docs.djangoproject.com/en/1.10/ref/settings/\n""""""\n\nimport os\nfrom pathlib import Path\n\n\n# Build paths inside the project like this: os.path.join(BASE_DIR, ...)\nBASE_DIR = str(Path(__file__).parent.parent.resolve())\n\n# Quick-start development settings - unsuitable for production\n# See https://docs.djangoproject.com/en/1.10/howto/deployment/checklist/\n\n# SECURITY WARNING: keep the secret key used in production secret!\nSECRET_KEY = \'r*z#sh2aqb!zjz#s7h@5&toyx@t_r4nfrgwg%r$4)2@d@8ypyb\'\n\n# SECURITY WARNING: don\'t run with debug turned on in production!\nDEBUG = os.environ.get(\'ENV\', \'prd\') != \'prd\'\n\nALLOWED_HOSTS = os.environ.get(\'ALLOWED_HOSTS\', \'localhost,127.0.0.1,[::1]\').split(\',\')\n\n\n# Application definition\n\nINSTALLED_APPS = [\n    \'django.contrib.admin\',\n    \'django.contrib.auth\',\n    \'django.contrib.contenttypes\',\n    \'django.contrib.sessions\',\n    \'django.contrib.messages\',\n    \'django.contrib.staticfiles\',\n    \'photonix.common\',\n    \'photonix.photos\',\n    \'photonix.web\',\n    \'graphene_django\',\n    \'django_filters\',\n]\n\nMIDDLEWARE = [\n    \'django.middleware.security.SecurityMiddleware\',\n    \'django.contrib.sessions.middleware.SessionMiddleware\',\n    \'django.middleware.common.CommonMiddleware\',\n    \'django.middleware.csrf.CsrfViewMiddleware\',\n    \'django.contrib.auth.middleware.AuthenticationMiddleware\',\n    \'django.contrib.messages.middleware.MessageMiddleware\',\n    \'django.middleware.clickjacking.XFrameOptionsMiddleware\',\n]\n\nROOT_URLCONF = \'photonix.web.urls\'\n\nTEMPLATES = [\n    {\n        \'BACKEND\': \'django.template.backends.django.DjangoTemplates\',\n        \'DIRS\': [],\n        \'APP_DIRS\': True,\n        \'OPTIONS\': {\n            \'context_processors\': [\n                \'django.template.context_processors.debug\',\n                \'django.template.context_processors.request\',\n                \'django.contrib.auth.context_processors.auth\',\n                \'django.contrib.messages.context_processors.messages\',\n            ],\n        },\n    },\n]\n\nWSGI_APPLICATION = \'photonix.web.wsgi.application\'\n\n\nDATABASES = {\n    \'default\': {\n        \'ENGINE\':   \'django.db.backends.postgresql\',\n        \'HOST\':     os.environ.get(\'POSTGRES_HOST\', \'127.0.0.1\'),\n        \'NAME\':     os.environ.get(\'POSTGRES_DB\', \'photonix\'),\n        \'USER\':     os.environ.get(\'POSTGRES_USER\', \'postgres\'),\n        \'PASSWORD\': os.environ.get(\'POSTGRES_PASSWORD\', \'password\'),\n    }\n}\n\n\n# Password validation\n# https://docs.djangoproject.com/en/1.10/ref/settings/#auth-password-validators\n\nAUTH_PASSWORD_VALIDATORS = [\n    {\n        \'NAME\': \'django.contrib.auth.password_validation.UserAttributeSimilarityValidator\',\n    },\n    {\n        \'NAME\': \'django.contrib.auth.password_validation.MinimumLengthValidator\',\n    },\n    {\n        \'NAME\': \'django.contrib.auth.password_validation.CommonPasswordValidator\',\n    },\n    {\n        \'NAME\': \'django.contrib.auth.password_validation.NumericPasswordValidator\',\n    },\n]\n\n\n# Internationalization\n# https://docs.djangoproject.com/en/1.10/topics/i18n/\n\nLANGUAGE_CODE = \'en-gb\'\n\nTIME_ZONE = \'UTC\'\n\nUSE_I18N = True\n\nUSE_L10N = True\n\nUSE_TZ = True\n\n\nif os.path.exists(\'/data\'):\n    DATA_DIR = str(Path(\'/data\'))\nelse:\n    DATA_DIR = str(Path(BASE_DIR).parent / \'data\')\n\nCACHE_DIR = str(Path(DATA_DIR) / \'cache\')\nMODEL_DIR = str(Path(DATA_DIR) / \'models\')\n\nSTATIC_DIR = str(Path(BASE_DIR) / \'static\')\nSTATIC_URL = \'/static/\'\n\nMEDIA_ROOT = str(Path(BASE_DIR).parent / \'data\')\n\nTHUMBNAIL_ROOT = str(Path(CACHE_DIR) / \'thumbnails\')\n\nTHUMBNAIL_SIZES = [\n    # Width, height, crop method, JPEG quality, whether it should be generated upon upload\n    (256, 256, \'cover\', 50, True),  # Square thumbnails\n    # We use the largest dimension for both dimensions as they won\'t crop and some with in portrait mode \n    (960, 960, \'contain\', 75, False),  # 960px\n    (1920, 1920, \'contain\', 75, False),  # 2k\n    (3840, 3840, \'contain\', 75, False),  # 4k\n]\n\n\nPHOTO_INPUT_DIRS = [str(Path(BASE_DIR).parent.parent / \'photos_to_import\')]\nPHOTO_OUTPUT_DIRS = [\n    {\n        \'EXTENSIONS\': [\'jpg\', \'jpeg\', \'mov\', \'mp4\', \'m4v\', \'3gp\'],\n        \'PATH\': \'/data/photos\',\n    },\n    {\n        \'EXTENSIONS\': [\'cr2\'],\n        \'PATH\': \'/data/raw-photos\',\n    },\n]\nPHOTO_RAW_PROCESSED_DIR = \'/data/raw-photos-processed\'\n\nMODEL_INFO_URL = \'https://photonix.org/models.json\'\n\nGRAPHENE = {\n    \'SCHEMA\': \'photonix.web.schema.schema\'\n}\n'"
photonix/web/test_settings.py,0,"b""import tempfile\n\nfrom .settings import *\n\n\nDATABASES = {\n    'default': {\n        'ENGINE':   'django.db.backends.sqlite3',\n        'NAME':     ':memory:'\n    }\n}\n\nDATA_DIR = tempfile.mkdtemp()\nCACHE_DIR = str(Path(DATA_DIR) / 'cache')\nPHOTO_RAW_PROCESSED_DIR = str(Path(DATA_DIR) / 'raw-photos-processed')\nTHUMBNAIL_ROOT = str(Path(CACHE_DIR) / 'thumbnails')"""
photonix/web/urls.py,0,"b""from django.conf.urls import url\nfrom django.contrib import admin\nfrom django.views.decorators.csrf import csrf_exempt\nfrom graphene_django.views import GraphQLView\n\nfrom photonix.photos.views import thumbnail_view\n\nurlpatterns = [\n    url(r'^admin/', admin.site.urls),\n    url(r'^graphql', csrf_exempt(GraphQLView.as_view(graphiql=True)), name='api'),\n    url(r'^thumbnails/(?P<width>[0-9]+)x(?P<height>[0-9]+)_(?P<crop>cover|contain)_q(?P<quality>[0-9]+)/(?P<photo_id>[a-f0-9]{8}-?[a-f0-9]{4}-?4[a-f0-9]{3}-?[89ab][a-f0-9]{3}-?[a-f0-9]{12})/$', thumbnail_view),\n]\n"""
photonix/web/wsgi.py,0,"b'""""""\nWSGI config for web project.\n\nIt exposes the WSGI callable as a module-level variable named ``application``.\n\nFor more information on this file, see\nhttps://docs.djangoproject.com/en/1.10/howto/deployment/wsgi/\n""""""\n\nimport os\n\nfrom django.core.wsgi import get_wsgi_application\n\nos.environ.setdefault(""DJANGO_SETTINGS_MODULE"", ""photonix.web.settings"")\n\napplication = get_wsgi_application()\n'"
photonix/classifiers/color/__init__.py,0,"b'from .model import ColorModel, run_on_photo\n'"
photonix/classifiers/color/model.py,0,"b""import operator\nimport sys\nfrom collections import defaultdict\nfrom colorsys import rgb_to_hsv\nfrom pathlib import Path\n\nimport numpy as np\nfrom PIL import Image\n\n\nclass ColorModel:\n    version = 20181130\n    approx_ram_mb = 120\n    max_num_workers = 2\n\n    def __init__(self):\n        self.colors = {\n            # Name: (red, green, blue)\n\n            # 'Red':                  (255, 0, 0),\n            # 'Yellow':               (255, 255, 0),\n            # 'Green':                (0, 255, 0),\n            # 'Cyan':                 (0, 255, 255),\n            # 'Blue':                 (0, 0, 255),\n            # 'Magenta':              (255, 0, 255),\n\n            'Red':                  (225, 32, 0),\n            'Dark orange':          (162, 70, 21),\n            'Orange':               (255, 124, 0),\n            'Pale pink':            (255, 159, 156),\n            'Lemon yellow':         (255, 250, 0),\n            'School bus yellow':    (255, 207, 0),\n            'Green':                (144, 226, 0),\n            'Dark lime green':      (0, 171, 0),\n            'Cyan':                 (0, 178, 212),\n            'Blue':                 (0, 98, 198),\n            'Violet':               (140, 32, 186),\n            'Pink':                 (245, 35, 148),\n\n            'White':                (255, 255, 255),\n            'Gray':                 (124, 124, 124),\n            'Black':                (0, 0, 0),\n        }\n\n    def predict(self, image_file, image_size=32, min_score=0):\n        image = Image.open(image_file)\n        image = image.resize((1000, 1000), Image.BICUBIC)  # Remove sensor noise/grain\n        image = image.resize((image_size, image_size), Image.NEAREST)  # Get the interesting colors without muddying them\n        pixels = np.asarray(image)\n        pixels = [j for i in pixels for j in i]\n\n        summed_results = defaultdict(float)\n        for i, pixel in enumerate(pixels):\n            best_color = None\n            best_score = 0\n            for name, target in self.colors.items():\n                score = self.color_distance(pixel, target)\n                if score > best_score:\n                    best_color = name\n                    best_score = score\n            if best_color:\n                summed_results[best_color] += score\n\n        averaged_results = {}\n        for key, val in summed_results.items():\n            val = val / (image_size * image_size)\n            if val >= min_score:\n                averaged_results[key] = val\n\n        sorted_results = sorted(averaged_results.items(), key=operator.itemgetter(1), reverse=True)\n        return sorted_results\n\n    def color_distance(self, a, b):\n        # Colors are list of 3 floats (RGB) from 0.0 to 1.0\n        a_h, a_s, a_v = rgb_to_hsv(a[0] / 255, a[1] / 255, a[2] / 255)\n        b_h, b_s, b_v = rgb_to_hsv(b[0] / 255, b[1] / 255, b[2] / 255)\n        diff_h = 1 - abs(a_h - b_h)\n        diff_s = 1 - abs(a_s - b_s)\n        diff_v = 1 - abs(a_v - b_v)\n        score = diff_h * diff_s * diff_v\n        return score\n\n\ndef run_on_photo(photo_id):\n    model = ColorModel()\n    sys.path.insert(0, str(Path(__file__).resolve().parent.parent))\n    from photonix.classifiers.runners import results_for_model_on_photo, get_or_create_tag\n    photo, results = results_for_model_on_photo(model, photo_id)\n\n    if photo:\n        from django.utils import timezone\n        from photonix.photos.models import PhotoTag\n        photo.clear_tags(source='C', type='C')\n        for name, score in results:\n            tag = get_or_create_tag(name=name, type='C', source='C')\n            PhotoTag(photo=photo, tag=tag, source='C', confidence=score, significance=score).save()\n        photo.classifier_color_completed_at = timezone.now()\n        photo.classifier_color_version = getattr(model, 'version', 0)\n        photo.save()\n\n    return photo, results\n\n\nif __name__ == '__main__':\n    if len(sys.argv) != 2:\n        print('Argument required: image file path')\n        exit(1)\n\n    results = run_on_photo(sys.argv[1])\n\n    for result in results:\n        print('{} (score: {:0.10f})'.format(result[0], result[1]))\n"""
photonix/classifiers/location/__init__.py,0,"b'from .model import LocationModel, run_on_photo\n'"
photonix/classifiers/location/model.py,0,"b'import csv\nimport math\nfrom pathlib import Path\nimport sys\n\nimport matplotlib.path as mpltPath\nimport shapefile\n\nfrom photonix.photos.utils.metadata import PhotoMetadata, parse_gps_location\nfrom photonix.classifiers.base_model import BaseModel\n\n\nWORLD_FILE = Path(\'location\') / \'TM_WORLD_BORDERS-0.3.shp\'  # http://thematicmapping.org/downloads/world_borders.php\nCITIES_FILE = Path(\'location\') / \'cities1000.txt\'  # http://download.geonames.org/export/dump/\n\n\nclass LocationModel(BaseModel):\n    name = \'location\'\n    version = 20190109\n    approx_ram_mb = 100\n    max_num_workers = 4\n\n    def __init__(self, model_dir=None, world_file=WORLD_FILE, cities_file=CITIES_FILE, lock_name=None):\n        super().__init__(model_dir=model_dir)\n\n        world_file = str(Path(self.model_dir) / world_file)\n        cities_file = str(Path(self.model_dir) / cities_file)\n\n        if self.ensure_downloaded(lock_name=lock_name):\n            self.world = self.load_world(world_file)\n            self.cities = self.load_cities(cities_file)\n\n    def load_world(self, world_file):\n        return shapefile.Reader(world_file, encoding=\'latin1\').shapeRecords()\n\n    def load_cities(self, cities_file):\n        rows = []\n        with open(cities_file) as csvfile:\n            reader = csv.reader(csvfile, delimiter=\'\\t\')\n            for row in reader:\n                rows.append(row)\n        return rows\n\n    def predict(self, image_file=None, location=None):\n        if location:\n            lon, lat = location\n        else:\n            metadata = PhotoMetadata(image_file)\n            location = metadata.get(\'GPS Position\') and parse_gps_location(metadata.get(\'GPS Position\')) or None\n            if location:\n                lon, lat = location\n            else:\n                return {\n                    \'country\': None,\n                    \'city\': None,\n                }\n\n        country = self.get_country(lon=lon, lat=lat)\n        if country:\n            city = self.get_city(lon=lon, lat=lat, country_code=country[\'code\'])\n        else:\n            city = self.get_city(lon=lon, lat=lat)\n\n        if not country and city:\n            country = {\n                \'name\': city[\'country_name\'],\n            }\n\n        return {\n            \'country\': country,\n            \'city\': city,\n        }\n\n    def get_country(self, lon, lat):\n        # Using country border polygons, returns the country that contains the\n        # given point.\n        location = [[lat, lon]]\n        for shape_rec in self.world:\n            shape = shape_rec.shape\n            record = shape_rec.record\n            points = shape.points\n\n            if shape.shapeTypeName == \'POLYGON\':\n                polygons = self.split_country_points(points)\n                for polygon in polygons:\n                    path = mpltPath.Path(polygon)\n                    inside = path.contains_points(location)[0]\n                    if inside:\n                        return {\n                            \'name\': record[4],\n                            \'code\': record[1],\n                        }\n        return None\n\n    def get_city(self, lon, lat, country_code=None):\n        # Gets the city within a 10km radius that has the highest population.\n        # It can be limited to a particular country.\n        nearest_distance = None\n        largest_population = 0\n        largest_city = None\n        chosen_country_code = None\n        chosen_country_name = None\n        countries = {row.record[1]: row.record[4] for row in self.world}\n\n        for row in self.cities:\n            if not country_code or country_code == row[8]:\n                longitude = float(row[4])\n                latitude = float(row[5])\n\n                distance = int(self.haversine([lon, lat], [longitude, latitude]))\n                if distance < 10000:\n                    population = int(row[14])\n                    if population > largest_population:\n                        largest_population = population\n                        largest_city = row[1]\n                        chosen_country_code = row[8]\n                        chosen_country_name = countries[chosen_country_code]\n\n                if nearest_distance is None or distance < nearest_distance:\n                    nearest_distance = distance\n\n        if largest_city:\n            return {\n                \'name\': largest_city,\n                \'distance\': nearest_distance,\n                \'population\': largest_population,\n                \'country_code\': chosen_country_code,\n                \'country_name\': chosen_country_name,\n            }\n        return None\n\n    def split_country_points(self, points):\n        # The country shapes have multiple polygons within them. We split the\n        # polygons when we see the first point reoccur.\n        point_groups = []\n        pos = 0\n        try:\n            while True:\n                first_point = points[pos]\n                last_pos = points[pos + 1:].index(first_point) + pos + 1\n                point_groups.append(points[pos:last_pos])\n                pos = last_pos + 1\n\n                if pos >= len(points):\n                    break\n            return point_groups\n        except ValueError:  # No matching end point so return single polygon\n            return [points]\n\n    def haversine(self, coord1, coord2):\n        # Calculate distance in meters. This is a bit simplistic as it assumes\n        # a sherical world but we believe this to not have much impact for how\n        # we use it.\n        R = 6372800\n        lat1, lon1 = coord1\n        lat2, lon2 = coord2\n        \n        phi1, phi2 = math.radians(lat1), math.radians(lat2) \n        dphi = math.radians(lat2 - lat1)\n        dlambda = math.radians(lon2 - lon1)\n        \n        a = math.sin(dphi/2)**2 + math.cos(phi1)*math.cos(phi2)*math.sin(dlambda/2)**2\n        return 2*R*math.atan2(math.sqrt(a), math.sqrt(1 - a))\n\n    def export_country_kml(self, country, path):\n        # Useful for debugging country borders. The exported KML can be viewed\n        # online.\n        for shape_rec in self.world.shapeRecords():\n            shape = shape_rec.shape\n            record = shape_rec.record\n\n            if record[4] == country:\n                polygons = self.split_country_points(shape.points)\n                with open(path, \'w\') as f:\n                    f.write(\'\'\'<?xml version=""1.0"" encoding=""UTF-8""?>\n<kml xmlns=""http://www.opengis.net/kml/2.2"">\n  <Placemark>\n    <name>{}</name>\n    <MultiGeometry>\'\'\'.format(country))\n                    for polygon in polygons:\n                        f.write(\'\'\'\n      <Polygon>\n        <extrude>1</extrude>\n        <altitudeMode>relativeToGround</altitudeMode>\n        <outerBoundaryIs>\n          <LinearRing>\n            <coordinates>\\n\'\'\')\n                        for point in polygon:\n                            f.write(\'{},{},100\\n\'.format(point[0], point[1]))\n                        f.write(\'\'\'\n            </coordinates>\n          </LinearRing>\n        </outerBoundaryIs>\n      </Polygon>\'\'\')\n                    f.write(\'\'\'\n    </MultiGeometry>\n  </Placemark>\n</kml>\\n\'\'\')\n                break\n\n\ndef run_on_photo(photo_id):\n    model = LocationModel()\n    sys.path.insert(0, str(Path(__file__).resolve().parent.parent))\n    from photonix.classifiers.runners import results_for_model_on_photo, get_or_create_tag\n    photo, results = results_for_model_on_photo(model, photo_id)\n\n    if photo and results[\'country\']:\n        from django.utils import timezone\n        from photonix.photos.models import PhotoTag\n        photo.clear_tags(source=\'C\', type=\'L\')\n        country_tag = get_or_create_tag(name=results[\'country\'][\'name\'], type=\'L\', source=\'C\')\n        PhotoTag(photo=photo, tag=country_tag, source=\'C\', confidence=1.0, significance=1.0).save()\n        if results[\'city\']:\n            city_tag = get_or_create_tag(name=results[\'city\'][\'name\'], type=\'L\', source=\'C\', parent=country_tag)\n            PhotoTag(photo=photo, tag=city_tag, source=\'C\', confidence=0.5, significance=0.5).save()\n        photo.classifier_color_completed_at = timezone.now()\n        photo.classifier_color_version = getattr(model, \'version\', 0)\n        photo.save()\n\n    return photo, results\n\n\nif __name__ == \'__main__\':\n    model = LocationModel()\n    if len(sys.argv) != 2:\n        print(\'Argument required: image file path\')\n        exit(1)\n\n    if \',\' in sys.argv[1]:\n        location = sys.argv[1].split(\',\')\n        location = [float(loc) for loc in location]\n        result = model.predict(location=location)\n    else:\n        result = run_on_photo(sys.argv[1])\n\n    print(result)\n'"
photonix/classifiers/object/__init__.py,0,"b'from .model import ObjectModel, run_on_photo\n'"
photonix/classifiers/object/model.py,12,"b""import os\nimport sys\nfrom pathlib import Path\n\nimport numpy as np\nfrom PIL import Image\n\nimport redis\nfrom redis_lock import Lock\nimport tensorflow as tf\n\nfrom photonix.classifiers.object.utils import label_map_util\nfrom photonix.classifiers.base_model import BaseModel\n\nr = redis.Redis(host=os.environ.get('REDIS_HOST', '127.0.0.1'))\nGRAPH_FILE = os.path.join('object', 'ssd_mobilenet_v2_oid_v4_2018_12_12_frozen_inference_graph.pb')\nLABEL_FILE = os.path.join('object', 'oid_v4_label_map.pbtxt')\n\n\nclass ObjectModel(BaseModel):\n    name = 'object'\n    version = 20190407\n    approx_ram_mb = 2000\n\n    def __init__(self, model_dir=None, graph_file=GRAPH_FILE, label_file=LABEL_FILE, lock_name=None):\n        super().__init__(model_dir=model_dir)\n\n        graph_file = os.path.join(self.model_dir, graph_file)\n        label_file = os.path.join(self.model_dir, label_file)\n\n        if self.ensure_downloaded(lock_name=lock_name):\n            self.graph = self.load_graph(graph_file)\n            self.labels = self.load_labels(label_file)\n\n    def load_graph(self, graph_file):\n        with Lock(r, 'classifier_{}_load_graph'.format(self.name)):\n            if self.graph_cache_key in self.graph_cache:\n                return self.graph_cache[self.graph_cache_key]\n\n            graph = tf.Graph()\n            graph_def = tf.compat.v1.GraphDef()\n\n            with graph.as_default():\n                od_graph_def = tf.compat.v1.GraphDef()\n                with tf.io.gfile.GFile(graph_file, 'rb') as fid:\n                    serialized_graph = fid.read()\n                    od_graph_def.ParseFromString(serialized_graph)\n                    tf.import_graph_def(od_graph_def, name='')\n\n            self.graph_cache[self.graph_cache_key] = graph\n            return graph\n\n    def load_labels(self, label_file):\n        label_map = label_map_util.load_labelmap(label_file)\n        categories = label_map_util.convert_label_map_to_categories(label_map, max_num_classes=1000, use_display_name=True)\n        return label_map_util.create_category_index(categories)\n\n    def load_image_into_numpy_array(self, image):\n        (im_width, im_height) = image.size\n        return np.array(image.getdata()).reshape((im_height, im_width, 3)).astype(np.uint8)\n\n    def run_inference_for_single_image(self, image):\n        with self.graph.as_default():\n            with tf.compat.v1.Session() as sess:\n                # Get handles to input and output tensors\n                ops = tf.compat.v1.get_default_graph().get_operations()\n                all_tensor_names = {output.name for op in ops for output in op.outputs}\n                tensor_dict = {}\n                for key in [\n                    'num_detections', 'detection_boxes', 'detection_scores',\n                    'detection_classes', 'detection_masks'\n                ]:\n                    tensor_name = key + ':0'\n                    if tensor_name in all_tensor_names:\n                        tensor_dict[key] = tf.compat.v1.get_default_graph().get_tensor_by_name(tensor_name)\n                if 'detection_masks' in tensor_dict:\n                    # The following processing is only for single image\n                    detection_boxes = tf.squeeze(tensor_dict['detection_boxes'], [0])\n                    # Reframe is required to translate mask from box coordinates to image coordinates and fit the image size.\n                    real_num_detection = tf.cast(tensor_dict['num_detections'][0], tf.int32)\n                    detection_boxes = tf.slice(detection_boxes, [0, 0], [real_num_detection, -1])\n                image_tensor = tf.compat.v1.get_default_graph().get_tensor_by_name('image_tensor:0')\n\n                # Run inference\n                output_dict = sess.run(tensor_dict, feed_dict={image_tensor: np.expand_dims(image, 0)})\n\n                # all outputs are float32 numpy arrays, so convert types as appropriate\n                output_dict['num_detections'] = int(output_dict['num_detections'][0])\n                output_dict['detection_classes'] = output_dict['detection_classes'][0].astype(np.uint16)\n                output_dict['detection_boxes'] = output_dict['detection_boxes'][0]\n                output_dict['detection_scores'] = output_dict['detection_scores'][0]\n        return output_dict\n\n    def format_output(self, output_dict, min_score):\n        results = []\n        for i, score in enumerate(output_dict['detection_scores']):\n            if score < min_score:\n                break\n\n            box = list(output_dict['detection_boxes'][i])\n            width = box[3] - box[1]\n            height = box[2] - box[0]\n\n            results.append({\n                'label':        self.labels[output_dict['detection_classes'][i]]['name'],\n                'score':        score,\n                'x':            np.mean([box[1], box[3]]),\n                'y':            np.mean([box[0], box[2]]),\n                'width':        width,\n                'height':       height,\n                'significance': score * width * height,\n                'box':          box,\n            })\n        return results\n\n    def predict(self, image_file, min_score=0.1):\n        image = Image.open(image_file)\n        # the array based representation of the image will be used later in order to prepare the\n        # result image with boxes and labels on it.\n        image_np = self.load_image_into_numpy_array(image)\n        # Expand dimensions since the model expects images to have shape: [1, None, None, 3]\n        np.expand_dims(image_np, axis=0)\n        # Actual detection.\n        output_dict = self.run_inference_for_single_image(image_np)\n        return self.format_output(output_dict, min_score)\n\n\ndef run_on_photo(photo_id):\n    model = ObjectModel()\n    sys.path.insert(0, str(Path(__file__).resolve().parent.parent))\n    from photonix.classifiers.runners import results_for_model_on_photo, get_or_create_tag\n    photo, results = results_for_model_on_photo(model, photo_id)\n\n    if photo:\n        from django.utils import timezone\n        from photonix.photos.models import PhotoTag\n        photo.clear_tags(source='C', type='O')\n        for result in results:\n            tag = get_or_create_tag(name=result['label'], type='O', source='C')\n            PhotoTag(photo=photo, tag=tag, source='C', confidence=result['score'], significance=result['significance'], position_x=result['x'], position_y=result['y'], size_x=result['width'], size_y=result['height']).save()\n        photo.classifier_object_completed_at = timezone.now()\n        photo.classifier_object_version = getattr(model, 'version', 0)\n        photo.save()\n\n    return photo, results\n\nif __name__ == '__main__':\n    model = ObjectModel()\n    if len(sys.argv) != 2:\n        print('Argument required: image file path')\n        exit(1)\n\n    results = run_on_photo(sys.argv[1])\n\n    for result in results:\n        print('{} (score: {:0.5f}, significance: {:0.5f}, x: {:0.5f}, y: {:0.5f}, width: {:0.5f}, height: {:0.5f})'.format(result['label'], result['score'], result['significance'], result['x'], result['y'], result['width'], result['height']))\n"""
photonix/classifiers/style/__init__.py,0,"b'from .model import StyleModel, run_on_photo\n'"
photonix/classifiers/style/assemble_dataset.py,0,"b""import argparse\nimport os\nimport requests\nimport shutil\n\n\n# Mapping: (class, Name, groups)\nSTYLE_MAPPING = [\n    (0, 'Bokeh', ['1543486@N25']),\n    (1, 'Bright', ['799643@N24']),\n    (2, 'Depth_of_Field', ['75418467@N00', '407825@N20']),\n    (3, 'Detailed', ['1670588@N24', '1131378@N23']),\n    (4, 'Ethereal', ['907784@N22']),\n    (5, 'Geometric_Composition', ['46353124@N00']),\n    (6, 'Hazy', ['38694591@N00']),\n    (7, 'HDR', ['99275357@N00']),\n    (8, 'Horror', ['29561404@N00']),\n    (9, 'Long_Exposure', ['52240257802@N01']),\n    (10, 'Macro', ['52241335207@N01']),\n    (11, 'Melancholy', ['70495179@N00']),\n    (12, 'Minimal', ['42097308@N00']),\n    (13, 'Noir', ['42109523@N00']),\n    (14, 'Romantic', ['54284561@N00']),\n    (15, 'Serene', ['1081625@N25']),\n    (16, 'Pastel', ['1055565@N24', '1371818@N25']),\n    (17, 'Sunny', ['1242213@N23']),\n    (18, 'Texture', ['70176273@N00']),\n    (19, 'Vintage', ['1222306@N25', '1176551@N24']),\n]\n\n\ndef run(image_path, images_per_style=500):\n    for class_id, style, group_ids in STYLE_MAPPING:\n        print('Get images for style: {}'.format(style))\n        get_images_for_style(style, group_ids, image_path, images_per_style)\n\n    # fetch_images(url_file, img_info_file, image_path)\n    # generate_train_test_dataset(img_info_file, train_file, test_file, train_ratio=0.8)\n\n\ndef get_images_for_style(style, group_ids, image_path, num_images):\n    params = {\n        'api_key': 'd31c7cb60c57aa7483c5c80919df5371',\n        'per_page': 500,  # 500 is the maximum allowed\n        'content_type': 1,  # only photos\n    }\n\n    style_image_path = os.path.join(image_path, style)\n    if os.path.exists(style_image_path):\n        print('Directory already exists: {}'.format(style_image_path))\n        return\n\n    os.mkdir(style_image_path)\n    images_info = []\n\n    for page in range(10):\n        if len(images_info) >= num_images:\n            break\n\n        params['page'] = page\n\n        for group in group_ids:\n            if len(images_info) >= num_images:\n                break\n\n            params['group_id'] = group\n\n            url = ('https://api.flickr.com/services/rest/?'\n                   'method=flickr.photos.search&format=json&nojsoncallback=1'\n                   '&api_key={api_key}&content_type={content_type}'\n                   '&group_id={group_id}&page={page}&per_page={per_page}')\n            url = url.format(**params)\n\n            # Make the request and ensure it succeeds.\n            page_data = requests.get(url).json()\n            if page_data['stat'] != 'ok':\n                raise Exception('Something is wrong: API returned {}'.format(page_data['stat']))\n\n            for photo_item in page_data['photos']['photo']:\n                if len(images_info) >= num_images:\n                    break\n                image_url = _get_image_url(photo_item)\n                image_filename = '{}.jpg'.format(photo_item['id'])\n                image_file = os.path.join(style_image_path, image_filename)\n                download_image(image_url, image_file)\n                images_info.append((image_url, image_filename))\n\n    info_file_path = os.path.join(image_path, style, 'images.csv')\n    write_image_info(info_file_path, images_info)\n\n    if len(images_info) < num_images:\n        raise Exception('Not enough images, only find {}'.format(len(images_info)))\n\n\ndef _get_image_url(photo_item, size_flag=''):\n    '''\n    size_flag: string ['']\n        See http://www.flickr.com/services/api/misc.urls.html for options.\n            '': 500 px on longest side\n            '_m': 240px on longest side\n    '''\n    url = 'http://farm{farm}.staticflickr.com/{server}/{id}_{secret}{size}.jpg'\n    return url.format(size=size_flag, **photo_item)\n\n\ndef download_image(url, filename):\n    try:\n        if os.path.exists(filename):\n            return True\n\n        print(filename)\n        r = requests.get(url, stream=True)\n        if r.status_code == 200:\n            with open(filename, 'wb') as f:\n                r.raw.decode_content = True\n                shutil.copyfileobj(r.raw, f)\n                return True\n        else:\n            return False\n    except KeyboardInterrupt:\n        raise Exception()  # multiprocessing doesn't catch keyboard exceptions\n    except Exception:\n        return False\n\n\ndef write_image_info(path, images_info):\n    with open(path, 'w') as f:\n        for url, filename in images_info:\n            f.write('{},{}\\n'.format(url, filename))\n\n\nif __name__ == '__main__':\n    parser = argparse.ArgumentParser(prog='PROG')\n    parser.add_argument('--image-path', nargs='?', default=os.path.join(os.path.dirname(__file__), 'images'))\n    parser.add_argument('--images-per-style', nargs='?', default=1000)\n\n    vars = parser.parse_args()\n    if not os.path.exists(vars.image_path):\n        os.mkdir(vars.image_path)\n\n    run(vars.image_path, vars.images_per_style)\n"""
photonix/classifiers/style/model.py,16,"b'import os\nimport sys\nfrom pathlib import Path\n\nimport numpy as np\n\nimport redis\nimport tensorflow as tf\nfrom redis_lock import Lock\n\nfrom photonix.classifiers.base_model import BaseModel\n\n\nGRAPH_FILE = os.path.join(\'style\', \'graph.pb\')\nLABEL_FILE = os.path.join(\'style\', \'labels.txt\')\n\n\nclass StyleModel(BaseModel):\n    name = \'style\'\n    version = 20180624\n    approx_ram_mb = 100\n    max_num_workers = 2\n\n    def __init__(self, model_dir=None, graph_file=GRAPH_FILE, label_file=LABEL_FILE, lock_name=None):\n        super().__init__(model_dir=model_dir)\n\n        tf.compat.v1.disable_eager_execution()\n\n        graph_file = os.path.join(self.model_dir, graph_file)\n        label_file = os.path.join(self.model_dir, label_file)\n\n        if self.ensure_downloaded(lock_name=lock_name):\n            self.graph = self.load_graph(graph_file)\n            self.labels = self.load_labels(label_file)\n\n    def load_graph(self, graph_file):\n        r = redis.Redis(host=os.environ.get(\'REDIS_HOST\', \'127.0.0.1\'))\n        with Lock(r, \'classifier_{}_load_graph\'.format(self.name)):\n            if self.graph_cache_key in self.graph_cache:\n                return self.graph_cache[self.graph_cache_key]\n\n            graph = tf.Graph()\n            graph_def = tf.compat.v1.GraphDef()\n\n            with open(graph_file, \'rb\') as f:\n                graph_def.ParseFromString(f.read())\n            with graph.as_default():\n                tf.import_graph_def(graph_def)\n\n            self.graph_cache[self.graph_cache_key] = graph\n            return graph\n\n    def load_labels(self, label_file):\n        labels = []\n        proto_as_ascii_lines = tf.io.gfile.GFile(label_file).readlines()\n        for l in proto_as_ascii_lines:\n            labels.append(l.rstrip())\n        return labels\n\n    def predict(self, image_file, min_score=0.66):\n        input_height = 224\n        input_width = 224\n        input_mean = 128\n        input_std = 128\n        input_layer = ""input""\n        output_layer = ""final_result""\n\n        t = self.read_tensor_from_image_file(\n            image_file,\n            input_height=input_height,\n            input_width=input_width,\n            input_mean=input_mean,\n            input_std=input_std)\n\n        input_name = ""import/"" + input_layer\n        output_name = ""import/"" + output_layer\n        input_operation = self.graph.get_operation_by_name(input_name)\n        output_operation = self.graph.get_operation_by_name(output_name)\n\n        with tf.compat.v1.Session(graph=self.graph) as sess:\n            results = sess.run(output_operation.outputs[0], {input_operation.outputs[0]: t})\n        results = np.squeeze(results)\n\n        response = []\n        top_k = results.argsort()[-5:][::-1]\n        for i in top_k:\n            if results[i] >= min_score:\n                response.append((self.labels[i], results[i]))\n\n        return response\n\n    def read_tensor_from_image_file(self, file_name, input_height=299, input_width=299, input_mean=0, input_std=255):\n        input_name = ""file_reader""\n\n        file_reader = tf.io.read_file(file_name, input_name)\n        if file_name.endswith("".png""):\n            image_reader = tf.image.decode_png(file_reader, channels = 3, name=\'png_reader\')\n        elif file_name.endswith("".gif""):\n            image_reader = tf.squeeze(tf.image.decode_gif(file_reader, name=\'gif_reader\'))\n        elif file_name.endswith("".bmp""):\n            image_reader = tf.image.decode_bmp(file_reader, name=\'bmp_reader\')\n        else:\n            image_reader = tf.image.decode_jpeg(file_reader, channels = 3, name=\'jpeg_reader\')\n        float_caster = tf.cast(image_reader, tf.float32)\n        dims_expander = tf.expand_dims(float_caster, 0)\n        resized = tf.image.resize(dims_expander, [input_height, input_width], method=tf.image.ResizeMethod.BILINEAR, antialias=True)\n        normalized = tf.divide(tf.subtract(resized, [input_mean]), [input_std])\n        sess = tf.compat.v1.Session()\n        return sess.run(normalized)\n\n\ndef run_on_photo(photo_id):\n    model = StyleModel()\n    sys.path.insert(0, str(Path(__file__).resolve().parent.parent))\n    from photonix.classifiers.runners import results_for_model_on_photo, get_or_create_tag\n    photo, results = results_for_model_on_photo(model, photo_id)\n\n    if photo:\n        from django.utils import timezone\n        from photonix.photos.models import PhotoTag\n        photo.clear_tags(source=\'C\', type=\'S\')\n        for name, score in results:\n            tag = get_or_create_tag(name=name, type=\'S\', source=\'C\')\n            PhotoTag(photo=photo, tag=tag, source=\'C\', confidence=score, significance=score).save()\n        photo.classifier_style_completed_at = timezone.now()\n        photo.classifier_style_version = getattr(model, \'version\', 0)\n        photo.save()\n\n    return photo, results\n\n\nif __name__ == \'__main__\':\n    model = StyleModel()\n    if len(sys.argv) != 2:\n        print(\'Argument required: image file path\')\n        exit(1)\n\n    results = model.predict(sys.argv[1], min_score=0.01)\n\n    for label, score in results:\n        print(\'{} (score: {:0.5f})\'.format(label, score))\n'"
photonix/classifiers/style/train.py,109,"b'# Copyright 2015 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\nr""""""Simple transfer learning with Inception v3 or Mobilenet models.\n\nWith support for TensorBoard.\n\nThis example shows how to take a Inception v3 or Mobilenet model trained on\nImageNet images, and train a new top layer that can recognize other classes of\nimages.\n\nThe top layer receives as input a 2048-dimensional vector (1001-dimensional for\nMobilenet) for each image. We train a softmax layer on top of this\nrepresentation. Assuming the softmax layer contains N labels, this corresponds\nto learning N + 2048*N (or 1001*N)  model parameters corresponding to the\nlearned biases and weights.\n\nHere\'s an example, which assumes you have a folder containing class-named\nsubfolders, each full of images for each label. The example folder flower_photos\nshould have a structure like this:\n\n~/flower_photos/daisy/photo1.jpg\n~/flower_photos/daisy/photo2.jpg\n...\n~/flower_photos/rose/anotherphoto77.jpg\n...\n~/flower_photos/sunflower/somepicture.jpg\n\nThe subfolder names are important, since they define what label is applied to\neach image, but the filenames themselves don\'t matter. Once your images are\nprepared, you can run the training with a command like this:\n\n\n```bash\nbazel build tensorflow/examples/image_retraining:retrain && \\\nbazel-bin/tensorflow/examples/image_retraining/retrain \\\n    --image_dir ~/flower_photos\n```\n\nOr, if you have a pip installation of tensorflow, `retrain.py` can be run\nwithout bazel:\n\n```bash\npython tensorflow/examples/image_retraining/retrain.py \\\n    --image_dir ~/flower_photos\n```\n\nYou can replace the image_dir argument with any folder containing subfolders of\nimages. The label for each image is taken from the name of the subfolder it\'s\nin.\n\nThis produces a new model file that can be loaded and run by any TensorFlow\nprogram, for example the label_image sample code.\n\nBy default this script will use the high accuracy, but comparatively large and\nslow Inception v3 model architecture. It\'s recommended that you start with this\nto validate that you have gathered good training data, but if you want to deploy\non resource-limited platforms, you can try the `--architecture` flag with a\nMobilenet model. For example:\n\n```bash\npython tensorflow/examples/image_retraining/retrain.py \\\n    --image_dir ~/flower_photos --architecture mobilenet_1.0_224\n```\n\nThere are 32 different Mobilenet models to choose from, with a variety of file\nsize and latency options. The first number can be \'1.0\', \'0.75\', \'0.50\', or\n\'0.25\' to control the size, and the second controls the input image size, either\n\'224\', \'192\', \'160\', or \'128\', with smaller sizes running faster. See\nhttps://research.googleblog.com/2017/06/mobilenets-open-source-models-for.html\nfor more information on Mobilenet.\n\nTo use with TensorBoard:\n\nBy default, this script will log summaries to /tmp/retrain_logs directory\n\nVisualize the summaries with this command:\n\ntensorboard --logdir /tmp/retrain_logs\n\n""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport argparse\nimport collections\nfrom datetime import datetime\nimport hashlib\nimport os.path\nimport random\nimport re\nimport sys\nimport tarfile\n\nimport numpy as np\nfrom six.moves import urllib\nimport tensorflow as tf\n\nfrom tensorflow.python.framework import graph_util\nfrom tensorflow.python.framework import tensor_shape\nfrom tensorflow.python.platform import gfile\nfrom tensorflow.python.util import compat\n\nFLAGS = None\n\n# These are all parameters that are tied to the particular model architecture\n# we\'re using for Inception v3. These include things like tensor names and their\n# sizes. If you want to adapt this script to work with another model, you will\n# need to update these to reflect the values in the network you\'re using.\nMAX_NUM_IMAGES_PER_CLASS = 2 ** 27 - 1  # ~134M\n\nrandom.seed(\'1337\')\n\n\ndef create_image_lists(image_dir, testing_percentage, validation_percentage):\n  """"""Builds a list of training images from the file system.\n\n  Analyzes the sub folders in the image directory, splits them into stable\n  training, testing, and validation sets, and returns a data structure\n  describing the lists of images for each label and their paths.\n\n  Args:\n    image_dir: String path to a folder containing subfolders of images.\n    testing_percentage: Integer percentage of the images to reserve for tests.\n    validation_percentage: Integer percentage of images reserved for validation.\n\n  Returns:\n    A dictionary containing an entry for each label subfolder, with images split\n    into training, testing, and validation sets within each label.\n  """"""\n  if not gfile.Exists(image_dir):\n    tf.logging.error(""Image directory \'"" + image_dir + ""\' not found."")\n    return None\n  result = collections.OrderedDict()\n  sub_dirs = [\n    os.path.join(image_dir,item)\n    for item in gfile.ListDirectory(image_dir)]\n  sub_dirs = sorted(item for item in sub_dirs\n                    if gfile.IsDirectory(item))\n  for sub_dir in sub_dirs:\n    extensions = [\'jpg\', \'jpeg\', \'JPG\', \'JPEG\']\n    file_list = []\n    dir_name = os.path.basename(sub_dir)\n    if dir_name == image_dir:\n      continue\n    tf.logging.info(""Looking for images in \'"" + dir_name + ""\'"")\n    for extension in extensions:\n      file_glob = os.path.join(image_dir, dir_name, \'*.\' + extension)\n      file_list.extend(gfile.Glob(file_glob))\n    if not file_list:\n      tf.logging.warning(\'No files found\')\n      continue\n    if len(file_list) < 20:\n      tf.logging.warning(\n          \'WARNING: Folder has less than 20 images, which may cause issues.\')\n    elif len(file_list) > MAX_NUM_IMAGES_PER_CLASS:\n      tf.logging.warning(\n          \'WARNING: Folder {} has more than {} images. Some images will \'\n          \'never be selected.\'.format(dir_name, MAX_NUM_IMAGES_PER_CLASS))\n    label_name = re.sub(r\'[^a-z0-9]+\', \' \', dir_name.lower())\n    training_images = []\n    testing_images = []\n    validation_images = []\n    for file_name in file_list:\n      base_name = os.path.basename(file_name)\n      # We want to ignore anything after \'_nohash_\' in the file name when\n      # deciding which set to put an image in, the data set creator has a way of\n      # grouping photos that are close variations of each other. For example\n      # this is used in the plant disease data set to group multiple pictures of\n      # the same leaf.\n      hash_name = re.sub(r\'_nohash_.*$\', \'\', file_name)\n      # This looks a bit magical, but we need to decide whether this file should\n      # go into the training, testing, or validation sets, and we want to keep\n      # existing files in the same set even if more files are subsequently\n      # added.\n      # To do that, we need a stable way of deciding based on just the file name\n      # itself, so we do a hash of that and then use that to generate a\n      # probability value that we use to assign it.\n      hash_name_hashed = hashlib.sha1(compat.as_bytes(hash_name)).hexdigest()\n      percentage_hash = ((int(hash_name_hashed, 16) %\n                          (MAX_NUM_IMAGES_PER_CLASS + 1)) *\n                         (100.0 / MAX_NUM_IMAGES_PER_CLASS))\n      if percentage_hash < validation_percentage:\n        validation_images.append(base_name)\n      elif percentage_hash < (testing_percentage + validation_percentage):\n        testing_images.append(base_name)\n      else:\n        training_images.append(base_name)\n    result[label_name] = {\n        \'dir\': dir_name,\n        \'training\': training_images,\n        \'testing\': testing_images,\n        \'validation\': validation_images,\n    }\n  return result\n\n\ndef get_image_path(image_lists, label_name, index, image_dir, category):\n  """"""""Returns a path to an image for a label at the given index.\n\n  Args:\n    image_lists: Dictionary of training images for each label.\n    label_name: Label string we want to get an image for.\n    index: Int offset of the image we want. This will be moduloed by the\n    available number of images for the label, so it can be arbitrarily large.\n    image_dir: Root folder string of the subfolders containing the training\n    images.\n    category: Name string of set to pull images from - training, testing, or\n    validation.\n\n  Returns:\n    File system path string to an image that meets the requested parameters.\n\n  """"""\n  if label_name not in image_lists:\n    tf.logging.fatal(\'Label does not exist %s.\', label_name)\n  label_lists = image_lists[label_name]\n  if category not in label_lists:\n    tf.logging.fatal(\'Category does not exist %s.\', category)\n  category_list = label_lists[category]\n  if not category_list:\n    tf.logging.fatal(\'Label %s has no images in the category %s.\',\n                     label_name, category)\n  mod_index = index % len(category_list)\n  base_name = category_list[mod_index]\n  sub_dir = label_lists[\'dir\']\n  full_path = os.path.join(image_dir, sub_dir, base_name)\n  return full_path\n\n\ndef get_bottleneck_path(image_lists, label_name, index, bottleneck_dir,\n                        category, architecture):\n  """"""""Returns a path to a bottleneck file for a label at the given index.\n\n  Args:\n    image_lists: Dictionary of training images for each label.\n    label_name: Label string we want to get an image for.\n    index: Integer offset of the image we want. This will be moduloed by the\n    available number of images for the label, so it can be arbitrarily large.\n    bottleneck_dir: Folder string holding cached files of bottleneck values.\n    category: Name string of set to pull images from - training, testing, or\n    validation.\n    architecture: The name of the model architecture.\n\n  Returns:\n    File system path string to an image that meets the requested parameters.\n  """"""\n  return get_image_path(image_lists, label_name, index, bottleneck_dir,\n                        category) + \'_\' + architecture + \'.txt\'\n\n\ndef create_model_graph(model_info):\n  """"""""Creates a graph from saved GraphDef file and returns a Graph object.\n\n  Args:\n    model_info: Dictionary containing information about the model architecture.\n\n  Returns:\n    Graph holding the trained Inception network, and various tensors we\'ll be\n    manipulating.\n  """"""\n  with tf.Graph().as_default() as graph:\n    model_path = os.path.join(FLAGS.model_dir, model_info[\'model_file_name\'])\n    with gfile.FastGFile(model_path, \'rb\') as f:\n      graph_def = tf.GraphDef()\n      graph_def.ParseFromString(f.read())\n      bottleneck_tensor, resized_input_tensor = (tf.import_graph_def(\n          graph_def,\n          name=\'\',\n          return_elements=[\n              model_info[\'bottleneck_tensor_name\'],\n              model_info[\'resized_input_tensor_name\'],\n          ]))\n  return graph, bottleneck_tensor, resized_input_tensor\n\n\ndef run_bottleneck_on_image(sess, image_data, image_data_tensor,\n                            decoded_image_tensor, resized_input_tensor,\n                            bottleneck_tensor):\n  """"""Runs inference on an image to extract the \'bottleneck\' summary layer.\n\n  Args:\n    sess: Current active TensorFlow Session.\n    image_data: String of raw JPEG data.\n    image_data_tensor: Input data layer in the graph.\n    decoded_image_tensor: Output of initial image resizing and  preprocessing.\n    resized_input_tensor: The input node of the recognition graph.\n    bottleneck_tensor: Layer before the final softmax.\n\n  Returns:\n    Numpy array of bottleneck values.\n  """"""\n  # First decode the JPEG image, resize it, and rescale the pixel values.\n  resized_input_values = sess.run(decoded_image_tensor,\n                                  {image_data_tensor: image_data})\n  # Then run it through the recognition network.\n  bottleneck_values = sess.run(bottleneck_tensor,\n                               {resized_input_tensor: resized_input_values})\n  bottleneck_values = np.squeeze(bottleneck_values)\n  return bottleneck_values\n\n\ndef maybe_download_and_extract(data_url):\n  """"""Download and extract model tar file.\n\n  If the pretrained model we\'re using doesn\'t already exist, this function\n  downloads it from the TensorFlow.org website and unpacks it into a directory.\n\n  Args:\n    data_url: Web location of the tar file containing the pretrained model.\n  """"""\n  dest_directory = FLAGS.model_dir\n  if not os.path.exists(dest_directory):\n    os.makedirs(dest_directory)\n  filename = data_url.split(\'/\')[-1]\n  filepath = os.path.join(dest_directory, filename)\n  if not os.path.exists(filepath):\n\n    def _progress(count, block_size, total_size):\n      sys.stdout.write(\'\\r>> Downloading %s %.1f%%\' %\n                       (filename,\n                        float(count * block_size) / float(total_size) * 100.0))\n      sys.stdout.flush()\n\n    filepath, _ = urllib.request.urlretrieve(data_url, filepath, _progress)\n    print()\n    statinfo = os.stat(filepath)\n    tf.logging.info(\'Successfully downloaded\', filename, statinfo.st_size,\n                    \'bytes.\')\n  tarfile.open(filepath, \'r:gz\').extractall(dest_directory)\n\n\ndef ensure_dir_exists(dir_name):\n  """"""Makes sure the folder exists on disk.\n\n  Args:\n    dir_name: Path string to the folder we want to create.\n  """"""\n  if not os.path.exists(dir_name):\n    os.makedirs(dir_name)\n\n\nbottleneck_path_2_bottleneck_values = {}\n\n\ndef create_bottleneck_file(bottleneck_path, image_lists, label_name, index,\n                           image_dir, category, sess, jpeg_data_tensor,\n                           decoded_image_tensor, resized_input_tensor,\n                           bottleneck_tensor):\n  """"""Create a single bottleneck file.""""""\n  tf.logging.info(\'Creating bottleneck at \' + bottleneck_path)\n  image_path = get_image_path(image_lists, label_name, index,\n                              image_dir, category)\n  if not gfile.Exists(image_path):\n    tf.logging.fatal(\'File does not exist %s\', image_path)\n  image_data = gfile.FastGFile(image_path, \'rb\').read()\n  try:\n    bottleneck_values = run_bottleneck_on_image(\n        sess, image_data, jpeg_data_tensor, decoded_image_tensor,\n        resized_input_tensor, bottleneck_tensor)\n  except Exception as e:\n    raise RuntimeError(\'Error during processing file %s (%s)\' % (image_path,\n                                                                 str(e)))\n  bottleneck_string = \',\'.join(str(x) for x in bottleneck_values)\n  with open(bottleneck_path, \'w\') as bottleneck_file:\n    bottleneck_file.write(bottleneck_string)\n\n\ndef get_or_create_bottleneck(sess, image_lists, label_name, index, image_dir,\n                             category, bottleneck_dir, jpeg_data_tensor,\n                             decoded_image_tensor, resized_input_tensor,\n                             bottleneck_tensor, architecture):\n  """"""Retrieves or calculates bottleneck values for an image.\n\n  If a cached version of the bottleneck data exists on-disk, return that,\n  otherwise calculate the data and save it to disk for future use.\n\n  Args:\n    sess: The current active TensorFlow Session.\n    image_lists: Dictionary of training images for each label.\n    label_name: Label string we want to get an image for.\n    index: Integer offset of the image we want. This will be modulo-ed by the\n    available number of images for the label, so it can be arbitrarily large.\n    image_dir: Root folder string  of the subfolders containing the training\n    images.\n    category: Name string of which  set to pull images from - training, testing,\n    or validation.\n    bottleneck_dir: Folder string holding cached files of bottleneck values.\n    jpeg_data_tensor: The tensor to feed loaded jpeg data into.\n    decoded_image_tensor: The output of decoding and resizing the image.\n    resized_input_tensor: The input node of the recognition graph.\n    bottleneck_tensor: The output tensor for the bottleneck values.\n    architecture: The name of the model architecture.\n\n  Returns:\n    Numpy array of values produced by the bottleneck layer for the image.\n  """"""\n  label_lists = image_lists[label_name]\n  sub_dir = label_lists[\'dir\']\n  sub_dir_path = os.path.join(bottleneck_dir, sub_dir)\n  ensure_dir_exists(sub_dir_path)\n  bottleneck_path = get_bottleneck_path(image_lists, label_name, index,\n                                        bottleneck_dir, category, architecture)\n  if not os.path.exists(bottleneck_path):\n    create_bottleneck_file(bottleneck_path, image_lists, label_name, index,\n                           image_dir, category, sess, jpeg_data_tensor,\n                           decoded_image_tensor, resized_input_tensor,\n                           bottleneck_tensor)\n  with open(bottleneck_path, \'r\') as bottleneck_file:\n    bottleneck_string = bottleneck_file.read()\n  did_hit_error = False\n  try:\n    bottleneck_values = [float(x) for x in bottleneck_string.split(\',\')]\n  except ValueError:\n    tf.logging.warning(\'Invalid float found, recreating bottleneck\')\n    did_hit_error = True\n  if did_hit_error:\n    create_bottleneck_file(bottleneck_path, image_lists, label_name, index,\n                           image_dir, category, sess, jpeg_data_tensor,\n                           decoded_image_tensor, resized_input_tensor,\n                           bottleneck_tensor)\n    with open(bottleneck_path, \'r\') as bottleneck_file:\n      bottleneck_string = bottleneck_file.read()\n    # Allow exceptions to propagate here, since they shouldn\'t happen after a\n    # fresh creation\n    bottleneck_values = [float(x) for x in bottleneck_string.split(\',\')]\n  return bottleneck_values\n\n\ndef cache_bottlenecks(sess, image_lists, image_dir, bottleneck_dir,\n                      jpeg_data_tensor, decoded_image_tensor,\n                      resized_input_tensor, bottleneck_tensor, architecture):\n  """"""Ensures all the training, testing, and validation bottlenecks are cached.\n\n  Because we\'re likely to read the same image multiple times (if there are no\n  distortions applied during training) it can speed things up a lot if we\n  calculate the bottleneck layer values once for each image during\n  preprocessing, and then just read those cached values repeatedly during\n  training. Here we go through all the images we\'ve found, calculate those\n  values, and save them off.\n\n  Args:\n    sess: The current active TensorFlow Session.\n    image_lists: Dictionary of training images for each label.\n    image_dir: Root folder string of the subfolders containing the training\n    images.\n    bottleneck_dir: Folder string holding cached files of bottleneck values.\n    jpeg_data_tensor: Input tensor for jpeg data from file.\n    decoded_image_tensor: The output of decoding and resizing the image.\n    resized_input_tensor: The input node of the recognition graph.\n    bottleneck_tensor: The penultimate output layer of the graph.\n    architecture: The name of the model architecture.\n\n  Returns:\n    Nothing.\n  """"""\n  how_many_bottlenecks = 0\n  ensure_dir_exists(bottleneck_dir)\n  for label_name, label_lists in image_lists.items():\n    for category in [\'training\', \'testing\', \'validation\']:\n      category_list = label_lists[category]\n      for index, unused_base_name in enumerate(category_list):\n        get_or_create_bottleneck(\n            sess, image_lists, label_name, index, image_dir, category,\n            bottleneck_dir, jpeg_data_tensor, decoded_image_tensor,\n            resized_input_tensor, bottleneck_tensor, architecture)\n\n        how_many_bottlenecks += 1\n        if how_many_bottlenecks % 100 == 0:\n          tf.logging.info(\n              str(how_many_bottlenecks) + \' bottleneck files created.\')\n\n\ndef get_random_cached_bottlenecks(sess, image_lists, how_many, category,\n                                  bottleneck_dir, image_dir, jpeg_data_tensor,\n                                  decoded_image_tensor, resized_input_tensor,\n                                  bottleneck_tensor, architecture):\n  """"""Retrieves bottleneck values for cached images.\n\n  If no distortions are being applied, this function can retrieve the cached\n  bottleneck values directly from disk for images. It picks a random set of\n  images from the specified category.\n\n  Args:\n    sess: Current TensorFlow Session.\n    image_lists: Dictionary of training images for each label.\n    how_many: If positive, a random sample of this size will be chosen.\n    If negative, all bottlenecks will be retrieved.\n    category: Name string of which set to pull from - training, testing, or\n    validation.\n    bottleneck_dir: Folder string holding cached files of bottleneck values.\n    image_dir: Root folder string of the subfolders containing the training\n    images.\n    jpeg_data_tensor: The layer to feed jpeg image data into.\n    decoded_image_tensor: The output of decoding and resizing the image.\n    resized_input_tensor: The input node of the recognition graph.\n    bottleneck_tensor: The bottleneck output layer of the CNN graph.\n    architecture: The name of the model architecture.\n\n  Returns:\n    List of bottleneck arrays, their corresponding ground truths, and the\n    relevant filenames.\n  """"""\n  class_count = len(image_lists.keys())\n  bottlenecks = []\n  ground_truths = []\n  filenames = []\n  if how_many >= 0:\n    # Retrieve a random sample of bottlenecks.\n    for unused_i in range(how_many):\n      label_index = random.randrange(class_count)\n      label_name = list(image_lists.keys())[label_index]\n      image_index = random.randrange(MAX_NUM_IMAGES_PER_CLASS + 1)\n      image_name = get_image_path(image_lists, label_name, image_index,\n                                  image_dir, category)\n      bottleneck = get_or_create_bottleneck(\n          sess, image_lists, label_name, image_index, image_dir, category,\n          bottleneck_dir, jpeg_data_tensor, decoded_image_tensor,\n          resized_input_tensor, bottleneck_tensor, architecture)\n      ground_truth = np.zeros(class_count, dtype=np.float32)\n      ground_truth[label_index] = 1.0\n      bottlenecks.append(bottleneck)\n      ground_truths.append(ground_truth)\n      filenames.append(image_name)\n  else:\n    # Retrieve all bottlenecks.\n    for label_index, label_name in enumerate(image_lists.keys()):\n      for image_index, image_name in enumerate(\n          image_lists[label_name][category]):\n        image_name = get_image_path(image_lists, label_name, image_index,\n                                    image_dir, category)\n        bottleneck = get_or_create_bottleneck(\n            sess, image_lists, label_name, image_index, image_dir, category,\n            bottleneck_dir, jpeg_data_tensor, decoded_image_tensor,\n            resized_input_tensor, bottleneck_tensor, architecture)\n        ground_truth = np.zeros(class_count, dtype=np.float32)\n        ground_truth[label_index] = 1.0\n        bottlenecks.append(bottleneck)\n        ground_truths.append(ground_truth)\n        filenames.append(image_name)\n  return bottlenecks, ground_truths, filenames\n\n\ndef get_random_distorted_bottlenecks(\n    sess, image_lists, how_many, category, image_dir, input_jpeg_tensor,\n    distorted_image, resized_input_tensor, bottleneck_tensor):\n  """"""Retrieves bottleneck values for training images, after distortions.\n\n  If we\'re training with distortions like crops, scales, or flips, we have to\n  recalculate the full model for every image, and so we can\'t use cached\n  bottleneck values. Instead we find random images for the requested category,\n  run them through the distortion graph, and then the full graph to get the\n  bottleneck results for each.\n\n  Args:\n    sess: Current TensorFlow Session.\n    image_lists: Dictionary of training images for each label.\n    how_many: The integer number of bottleneck values to return.\n    category: Name string of which set of images to fetch - training, testing,\n    or validation.\n    image_dir: Root folder string of the subfolders containing the training\n    images.\n    input_jpeg_tensor: The input layer we feed the image data to.\n    distorted_image: The output node of the distortion graph.\n    resized_input_tensor: The input node of the recognition graph.\n    bottleneck_tensor: The bottleneck output layer of the CNN graph.\n\n  Returns:\n    List of bottleneck arrays and their corresponding ground truths.\n  """"""\n  class_count = len(image_lists.keys())\n  bottlenecks = []\n  ground_truths = []\n  for unused_i in range(how_many):\n    label_index = random.randrange(class_count)\n    label_name = list(image_lists.keys())[label_index]\n    image_index = random.randrange(MAX_NUM_IMAGES_PER_CLASS + 1)\n    image_path = get_image_path(image_lists, label_name, image_index, image_dir,\n                                category)\n    if not gfile.Exists(image_path):\n      tf.logging.fatal(\'File does not exist %s\', image_path)\n    jpeg_data = gfile.FastGFile(image_path, \'rb\').read()\n    # Note that we materialize the distorted_image_data as a numpy array before\n    # sending running inference on the image. This involves 2 memory copies and\n    # might be optimized in other implementations.\n    distorted_image_data = sess.run(distorted_image,\n                                    {input_jpeg_tensor: jpeg_data})\n    bottleneck_values = sess.run(bottleneck_tensor,\n                                 {resized_input_tensor: distorted_image_data})\n    bottleneck_values = np.squeeze(bottleneck_values)\n    ground_truth = np.zeros(class_count, dtype=np.float32)\n    ground_truth[label_index] = 1.0\n    bottlenecks.append(bottleneck_values)\n    ground_truths.append(ground_truth)\n  return bottlenecks, ground_truths\n\n\ndef should_distort_images(flip_left_right, random_crop, random_scale,\n                          random_brightness):\n  """"""Whether any distortions are enabled, from the input flags.\n\n  Args:\n    flip_left_right: Boolean whether to randomly mirror images horizontally.\n    random_crop: Integer percentage setting the total margin used around the\n    crop box.\n    random_scale: Integer percentage of how much to vary the scale by.\n    random_brightness: Integer range to randomly multiply the pixel values by.\n\n  Returns:\n    Boolean value indicating whether any distortions should be applied.\n  """"""\n  return (flip_left_right or (random_crop != 0) or (random_scale != 0) or\n          (random_brightness != 0))\n\n\ndef add_input_distortions(flip_left_right, random_crop, random_scale,\n                          random_brightness, input_width, input_height,\n                          input_depth, input_mean, input_std):\n  """"""Creates the operations to apply the specified distortions.\n\n  During training it can help to improve the results if we run the images\n  through simple distortions like crops, scales, and flips. These reflect the\n  kind of variations we expect in the real world, and so can help train the\n  model to cope with natural data more effectively. Here we take the supplied\n  parameters and construct a network of operations to apply them to an image.\n\n  Cropping\n  ~~~~~~~~\n\n  Cropping is done by placing a bounding box at a random position in the full\n  image. The cropping parameter controls the size of that box relative to the\n  input image. If it\'s zero, then the box is the same size as the input and no\n  cropping is performed. If the value is 50%, then the crop box will be half the\n  width and height of the input. In a diagram it looks like this:\n\n  <       width         >\n  +---------------------+\n  |                     |\n  |   width - crop%     |\n  |    <      >         |\n  |    +------+         |\n  |    |      |         |\n  |    |      |         |\n  |    |      |         |\n  |    +------+         |\n  |                     |\n  |                     |\n  +---------------------+\n\n  Scaling\n  ~~~~~~~\n\n  Scaling is a lot like cropping, except that the bounding box is always\n  centered and its size varies randomly within the given range. For example if\n  the scale percentage is zero, then the bounding box is the same size as the\n  input and no scaling is applied. If it\'s 50%, then the bounding box will be in\n  a random range between half the width and height and full size.\n\n  Args:\n    flip_left_right: Boolean whether to randomly mirror images horizontally.\n    random_crop: Integer percentage setting the total margin used around the\n    crop box.\n    random_scale: Integer percentage of how much to vary the scale by.\n    random_brightness: Integer range to randomly multiply the pixel values by.\n    graph.\n    input_width: Horizontal size of expected input image to model.\n    input_height: Vertical size of expected input image to model.\n    input_depth: How many channels the expected input image should have.\n    input_mean: Pixel value that should be zero in the image for the graph.\n    input_std: How much to divide the pixel values by before recognition.\n\n  Returns:\n    The jpeg input layer and the distorted result tensor.\n  """"""\n\n  jpeg_data = tf.placeholder(tf.string, name=\'DistortJPGInput\')\n  decoded_image = tf.image.decode_jpeg(jpeg_data, channels=input_depth)\n  decoded_image_as_float = tf.cast(decoded_image, dtype=tf.float32)\n  decoded_image_4d = tf.expand_dims(decoded_image_as_float, 0)\n  margin_scale = 1.0 + (random_crop / 100.0)\n  resize_scale = 1.0 + (random_scale / 100.0)\n  margin_scale_value = tf.constant(margin_scale)\n  resize_scale_value = tf.random_uniform(tensor_shape.scalar(),\n                                         minval=1.0,\n                                         maxval=resize_scale)\n  scale_value = tf.multiply(margin_scale_value, resize_scale_value)\n  precrop_width = tf.multiply(scale_value, input_width)\n  precrop_height = tf.multiply(scale_value, input_height)\n  precrop_shape = tf.stack([precrop_height, precrop_width])\n  precrop_shape_as_int = tf.cast(precrop_shape, dtype=tf.int32)\n  precropped_image = tf.image.resize_bilinear(decoded_image_4d,\n                                              precrop_shape_as_int)\n  precropped_image_3d = tf.squeeze(precropped_image, squeeze_dims=[0])\n  cropped_image = tf.random_crop(precropped_image_3d,\n                                 [input_height, input_width, input_depth])\n  if flip_left_right:\n    flipped_image = tf.image.random_flip_left_right(cropped_image)\n  else:\n    flipped_image = cropped_image\n  brightness_min = 1.0 - (random_brightness / 100.0)\n  brightness_max = 1.0 + (random_brightness / 100.0)\n  brightness_value = tf.random_uniform(tensor_shape.scalar(),\n                                       minval=brightness_min,\n                                       maxval=brightness_max)\n  brightened_image = tf.multiply(flipped_image, brightness_value)\n  offset_image = tf.subtract(brightened_image, input_mean)\n  mul_image = tf.multiply(offset_image, 1.0 / input_std)\n  distort_result = tf.expand_dims(mul_image, 0, name=\'DistortResult\')\n  return jpeg_data, distort_result\n\n\ndef variable_summaries(var):\n  """"""Attach a lot of summaries to a Tensor (for TensorBoard visualization).""""""\n  with tf.name_scope(\'summaries\'):\n    mean = tf.reduce_mean(var)\n    tf.summary.scalar(\'mean\', mean)\n    with tf.name_scope(\'stddev\'):\n      stddev = tf.sqrt(tf.reduce_mean(tf.square(var - mean)))\n    tf.summary.scalar(\'stddev\', stddev)\n    tf.summary.scalar(\'max\', tf.reduce_max(var))\n    tf.summary.scalar(\'min\', tf.reduce_min(var))\n    tf.summary.histogram(\'histogram\', var)\n\n\ndef add_final_training_ops(class_count, final_tensor_name, bottleneck_tensor,\n                           bottleneck_tensor_size):\n  """"""Adds a new softmax and fully-connected layer for training.\n\n  We need to retrain the top layer to identify our new classes, so this function\n  adds the right operations to the graph, along with some variables to hold the\n  weights, and then sets up all the gradients for the backward pass.\n\n  The set up for the softmax and fully-connected layers is based on:\n  https://www.tensorflow.org/versions/master/tutorials/mnist/beginners/index.html\n\n  Args:\n    class_count: Integer of how many categories of things we\'re trying to\n    recognize.\n    final_tensor_name: Name string for the new final node that produces results.\n    bottleneck_tensor: The output of the main CNN graph.\n    bottleneck_tensor_size: How many entries in the bottleneck vector.\n\n  Returns:\n    The tensors for the training and cross entropy results, and tensors for the\n    bottleneck input and ground truth input.\n  """"""\n  with tf.name_scope(\'input\'):\n    bottleneck_input = tf.placeholder_with_default(\n        bottleneck_tensor,\n        shape=[None, bottleneck_tensor_size],\n        name=\'BottleneckInputPlaceholder\')\n\n    ground_truth_input = tf.placeholder(tf.float32,\n                                        [None, class_count],\n                                        name=\'GroundTruthInput\')\n\n  # Organizing the following ops as `final_training_ops` so they\'re easier\n  # to see in TensorBoard\n  layer_name = \'final_training_ops\'\n  with tf.name_scope(layer_name):\n    with tf.name_scope(\'weights\'):\n      initial_value = tf.truncated_normal(\n          [bottleneck_tensor_size, class_count], stddev=0.001)\n\n      layer_weights = tf.Variable(initial_value, name=\'final_weights\')\n\n      variable_summaries(layer_weights)\n    with tf.name_scope(\'biases\'):\n      layer_biases = tf.Variable(tf.zeros([class_count]), name=\'final_biases\')\n      variable_summaries(layer_biases)\n    with tf.name_scope(\'Wx_plus_b\'):\n      logits = tf.matmul(bottleneck_input, layer_weights) + layer_biases\n      tf.summary.histogram(\'pre_activations\', logits)\n\n  final_tensor = tf.nn.softmax(logits, name=final_tensor_name)\n  tf.summary.histogram(\'activations\', final_tensor)\n\n  with tf.name_scope(\'cross_entropy\'):\n    cross_entropy = tf.nn.softmax_cross_entropy_with_logits(\n        labels=ground_truth_input, logits=logits)\n    with tf.name_scope(\'total\'):\n      cross_entropy_mean = tf.reduce_mean(cross_entropy)\n  tf.summary.scalar(\'cross_entropy\', cross_entropy_mean)\n\n  with tf.name_scope(\'train\'):\n    optimizer = tf.train.GradientDescentOptimizer(FLAGS.learning_rate)\n    train_step = optimizer.minimize(cross_entropy_mean)\n\n  return (train_step, cross_entropy_mean, bottleneck_input, ground_truth_input,\n          final_tensor)\n\n\ndef add_evaluation_step(result_tensor, ground_truth_tensor):\n  """"""Inserts the operations we need to evaluate the accuracy of our results.\n\n  Args:\n    result_tensor: The new final node that produces results.\n    ground_truth_tensor: The node we feed ground truth data\n    into.\n\n  Returns:\n    Tuple of (evaluation step, prediction).\n  """"""\n  with tf.name_scope(\'accuracy\'):\n    with tf.name_scope(\'correct_prediction\'):\n      prediction = tf.argmax(result_tensor, 1)\n      correct_prediction = tf.equal(\n          prediction, tf.argmax(ground_truth_tensor, 1))\n    with tf.name_scope(\'accuracy\'):\n      evaluation_step = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n  tf.summary.scalar(\'accuracy\', evaluation_step)\n  return evaluation_step, prediction\n\n\ndef save_graph_to_file(sess, graph, graph_file_name):\n  output_graph_def = graph_util.convert_variables_to_constants(\n      sess, graph.as_graph_def(), [FLAGS.final_tensor_name])\n  with gfile.FastGFile(graph_file_name, \'wb\') as f:\n    f.write(output_graph_def.SerializeToString())\n  return\n\n\ndef prepare_file_system():\n  # Setup the directory we\'ll write summaries to for TensorBoard\n  if tf.gfile.Exists(FLAGS.summaries_dir):\n    tf.gfile.DeleteRecursively(FLAGS.summaries_dir)\n  tf.gfile.MakeDirs(FLAGS.summaries_dir)\n  if FLAGS.intermediate_store_frequency > 0:\n    ensure_dir_exists(FLAGS.intermediate_output_graphs_dir)\n  return\n\n\ndef create_model_info(architecture):\n  """"""Given the name of a model architecture, returns information about it.\n\n  There are different base image recognition pretrained models that can be\n  retrained using transfer learning, and this function translates from the name\n  of a model to the attributes that are needed to download and train with it.\n\n  Args:\n    architecture: Name of a model architecture.\n\n  Returns:\n    Dictionary of information about the model, or None if the name isn\'t\n    recognized\n\n  Raises:\n    ValueError: If architecture name is unknown.\n  """"""\n  architecture = architecture.lower()\n  if architecture == \'inception_v3\':\n    # pylint: disable=line-too-long\n    data_url = \'http://download.tensorflow.org/models/image/imagenet/inception-2015-12-05.tgz\'\n    # pylint: enable=line-too-long\n    bottleneck_tensor_name = \'pool_3/_reshape:0\'\n    bottleneck_tensor_size = 2048\n    input_width = 299\n    input_height = 299\n    input_depth = 3\n    resized_input_tensor_name = \'Mul:0\'\n    model_file_name = \'classify_image_graph_def.pb\'\n    input_mean = 128\n    input_std = 128\n  elif architecture.startswith(\'mobilenet_\'):\n    parts = architecture.split(\'_\')\n    if len(parts) != 3 and len(parts) != 4:\n      tf.logging.error(""Couldn\'t understand architecture name \'%s\'"",\n                       architecture)\n      return None\n    version_string = parts[1]\n    if (version_string != \'1.0\' and version_string != \'0.75\' and\n        version_string != \'0.50\' and version_string != \'0.25\'):\n      tf.logging.error(\n          """"""""The Mobilenet version should be \'1.0\', \'0.75\', \'0.50\', or \'0.25\',\n  but found \'%s\' for architecture \'%s\'"""""",\n          version_string, architecture)\n      return None\n    size_string = parts[2]\n    if (size_string != \'224\' and size_string != \'192\' and\n        size_string != \'160\' and size_string != \'128\'):\n      tf.logging.error(\n          """"""The Mobilenet input size should be \'224\', \'192\', \'160\', or \'128\',\n but found \'%s\' for architecture \'%s\'"""""",\n          size_string, architecture)\n      return None\n    if len(parts) == 3:\n      is_quantized = False\n    else:\n      if parts[3] != \'quantized\':\n        tf.logging.error(\n            ""Couldn\'t understand architecture suffix \'%s\' for \'%s\'"", parts[3],\n            architecture)\n        return None\n      is_quantized = True\n    data_url = \'http://download.tensorflow.org/models/mobilenet_v1_\'\n    data_url += version_string + \'_\' + size_string + \'_frozen.tgz\'\n    bottleneck_tensor_name = \'MobilenetV1/Predictions/Reshape:0\'\n    bottleneck_tensor_size = 1001\n    input_width = int(size_string)\n    input_height = int(size_string)\n    input_depth = 3\n    resized_input_tensor_name = \'input:0\'\n    if is_quantized:\n      model_base_name = \'quantized_graph.pb\'\n    else:\n      model_base_name = \'frozen_graph.pb\'\n    model_dir_name = \'mobilenet_v1_\' + version_string + \'_\' + size_string\n    model_file_name = os.path.join(model_dir_name, model_base_name)\n    input_mean = 127.5\n    input_std = 127.5\n  else:\n    tf.logging.error(""Couldn\'t understand architecture name \'%s\'"", architecture)\n    raise ValueError(\'Unknown architecture\', architecture)\n\n  return {\n      \'data_url\': data_url,\n      \'bottleneck_tensor_name\': bottleneck_tensor_name,\n      \'bottleneck_tensor_size\': bottleneck_tensor_size,\n      \'input_width\': input_width,\n      \'input_height\': input_height,\n      \'input_depth\': input_depth,\n      \'resized_input_tensor_name\': resized_input_tensor_name,\n      \'model_file_name\': model_file_name,\n      \'input_mean\': input_mean,\n      \'input_std\': input_std,\n  }\n\n\ndef add_jpeg_decoding(input_width, input_height, input_depth, input_mean,\n                      input_std):\n  """"""Adds operations that perform JPEG decoding and resizing to the graph..\n\n  Args:\n    input_width: Desired width of the image fed into the recognizer graph.\n    input_height: Desired width of the image fed into the recognizer graph.\n    input_depth: Desired channels of the image fed into the recognizer graph.\n    input_mean: Pixel value that should be zero in the image for the graph.\n    input_std: How much to divide the pixel values by before recognition.\n\n  Returns:\n    Tensors for the node to feed JPEG data into, and the output of the\n      preprocessing steps.\n  """"""\n  jpeg_data = tf.placeholder(tf.string, name=\'DecodeJPGInput\')\n  decoded_image = tf.image.decode_jpeg(jpeg_data, channels=input_depth)\n  decoded_image_as_float = tf.cast(decoded_image, dtype=tf.float32)\n  decoded_image_4d = tf.expand_dims(decoded_image_as_float, 0)\n  resize_shape = tf.stack([input_height, input_width])\n  resize_shape_as_int = tf.cast(resize_shape, dtype=tf.int32)\n  resized_image = tf.image.resize_bilinear(decoded_image_4d,\n                                           resize_shape_as_int)\n  offset_image = tf.subtract(resized_image, input_mean)\n  mul_image = tf.multiply(offset_image, 1.0 / input_std)\n  return jpeg_data, mul_image\n\n\ndef main(_):\n  # Needed to make sure the logging output is visible.\n  # See https://github.com/tensorflow/tensorflow/issues/3047\n  tf.logging.set_verbosity(tf.logging.INFO)\n\n  # Prepare necessary directories  that can be used during training\n  prepare_file_system()\n\n  # Gather information about the model architecture we\'ll be using.\n  model_info = create_model_info(FLAGS.architecture)\n  if not model_info:\n    tf.logging.error(\'Did not recognize architecture flag\')\n    return -1\n\n  # Set up the pre-trained graph.\n  maybe_download_and_extract(model_info[\'data_url\'])\n  graph, bottleneck_tensor, resized_image_tensor = (\n      create_model_graph(model_info))\n\n  # Look at the folder structure, and create lists of all the images.\n  image_lists = create_image_lists(FLAGS.image_dir, FLAGS.testing_percentage,\n                                   FLAGS.validation_percentage)\n  class_count = len(image_lists.keys())\n  if class_count == 0:\n    tf.logging.error(\'No valid folders of images found at \' + FLAGS.image_dir)\n    return -1\n  if class_count == 1:\n    tf.logging.error(\'Only one valid folder of images found at \' +\n                     FLAGS.image_dir +\n                     \' - multiple classes are needed for classification.\')\n    return -1\n\n  # See if the command-line flags mean we\'re applying any distortions.\n  do_distort_images = should_distort_images(\n      FLAGS.flip_left_right, FLAGS.random_crop, FLAGS.random_scale,\n      FLAGS.random_brightness)\n\n  with tf.compat.v1.Session(graph=graph) as sess:\n    # Set up the image decoding sub-graph.\n    jpeg_data_tensor, decoded_image_tensor = add_jpeg_decoding(\n        model_info[\'input_width\'], model_info[\'input_height\'],\n        model_info[\'input_depth\'], model_info[\'input_mean\'],\n        model_info[\'input_std\'])\n\n    if do_distort_images:\n      # We will be applying distortions, so setup the operations we\'ll need.\n      (distorted_jpeg_data_tensor,\n       distorted_image_tensor) = add_input_distortions(\n           FLAGS.flip_left_right, FLAGS.random_crop, FLAGS.random_scale,\n           FLAGS.random_brightness, model_info[\'input_width\'],\n           model_info[\'input_height\'], model_info[\'input_depth\'],\n           model_info[\'input_mean\'], model_info[\'input_std\'])\n    else:\n      # We\'ll make sure we\'ve calculated the \'bottleneck\' image summaries and\n      # cached them on disk.\n      cache_bottlenecks(sess, image_lists, FLAGS.image_dir,\n                        FLAGS.bottleneck_dir, jpeg_data_tensor,\n                        decoded_image_tensor, resized_image_tensor,\n                        bottleneck_tensor, FLAGS.architecture)\n\n    # Add the new layer that we\'ll be training.\n    (train_step, cross_entropy, bottleneck_input, ground_truth_input,\n     final_tensor) = add_final_training_ops(\n         len(image_lists.keys()), FLAGS.final_tensor_name, bottleneck_tensor,\n         model_info[\'bottleneck_tensor_size\'])\n\n    # Create the operations we need to evaluate the accuracy of our new layer.\n    evaluation_step, prediction = add_evaluation_step(\n        final_tensor, ground_truth_input)\n\n    # Merge all the summaries and write them out to the summaries_dir\n    merged = tf.summary.merge_all()\n    train_writer = tf.summary.FileWriter(FLAGS.summaries_dir + \'/train\',\n                                         sess.graph)\n\n    validation_writer = tf.summary.FileWriter(\n        FLAGS.summaries_dir + \'/validation\')\n\n    # Set up all our weights to their initial default values.\n    init = tf.global_variables_initializer()\n    sess.run(init)\n\n    # Run the training for as many cycles as requested on the command line.\n    for i in range(FLAGS.how_many_training_steps):\n      # Get a batch of input bottleneck values, either calculated fresh every\n      # time with distortions applied, or from the cache stored on disk.\n      if do_distort_images:\n        (train_bottlenecks,\n         train_ground_truth) = get_random_distorted_bottlenecks(\n             sess, image_lists, FLAGS.train_batch_size, \'training\',\n             FLAGS.image_dir, distorted_jpeg_data_tensor,\n             distorted_image_tensor, resized_image_tensor, bottleneck_tensor)\n      else:\n        (train_bottlenecks,\n         train_ground_truth, _) = get_random_cached_bottlenecks(\n             sess, image_lists, FLAGS.train_batch_size, \'training\',\n             FLAGS.bottleneck_dir, FLAGS.image_dir, jpeg_data_tensor,\n             decoded_image_tensor, resized_image_tensor, bottleneck_tensor,\n             FLAGS.architecture)\n      # Feed the bottlenecks and ground truth into the graph, and run a training\n      # step. Capture training summaries for TensorBoard with the `merged` op.\n      train_summary, _ = sess.run(\n          [merged, train_step],\n          feed_dict={bottleneck_input: train_bottlenecks,\n                     ground_truth_input: train_ground_truth})\n      train_writer.add_summary(train_summary, i)\n\n      # Every so often, print out how well the graph is training.\n      is_last_step = (i + 1 == FLAGS.how_many_training_steps)\n      if (i % FLAGS.eval_step_interval) == 0 or is_last_step:\n        train_accuracy, cross_entropy_value = sess.run(\n            [evaluation_step, cross_entropy],\n            feed_dict={bottleneck_input: train_bottlenecks,\n                       ground_truth_input: train_ground_truth})\n        tf.logging.info(\'%s: Step %d: Train accuracy = %.1f%%\' %\n                        (datetime.now(), i, train_accuracy * 100))\n        tf.logging.info(\'%s: Step %d: Cross entropy = %f\' %\n                        (datetime.now(), i, cross_entropy_value))\n        validation_bottlenecks, validation_ground_truth, _ = (\n            get_random_cached_bottlenecks(\n                sess, image_lists, FLAGS.validation_batch_size, \'validation\',\n                FLAGS.bottleneck_dir, FLAGS.image_dir, jpeg_data_tensor,\n                decoded_image_tensor, resized_image_tensor, bottleneck_tensor,\n                FLAGS.architecture))\n        # Run a validation step and capture training summaries for TensorBoard\n        # with the `merged` op.\n        validation_summary, validation_accuracy = sess.run(\n            [merged, evaluation_step],\n            feed_dict={bottleneck_input: validation_bottlenecks,\n                       ground_truth_input: validation_ground_truth})\n        validation_writer.add_summary(validation_summary, i)\n        tf.logging.info(\'%s: Step %d: Validation accuracy = %.1f%% (N=%d)\' %\n                        (datetime.now(), i, validation_accuracy * 100,\n                         len(validation_bottlenecks)))\n\n      # Store intermediate results\n      intermediate_frequency = FLAGS.intermediate_store_frequency\n\n      if (intermediate_frequency > 0 and (i % intermediate_frequency == 0)\n          and i > 0):\n        intermediate_file_name = (FLAGS.intermediate_output_graphs_dir +\n                                  \'intermediate_\' + str(i) + \'.pb\')\n        tf.logging.info(\'Save intermediate result to : \' +\n                        intermediate_file_name)\n        save_graph_to_file(sess, graph, intermediate_file_name)\n\n    # We\'ve completed all our training, so run a final test evaluation on\n    # some new images we haven\'t used before.\n    test_bottlenecks, test_ground_truth, test_filenames = (\n        get_random_cached_bottlenecks(\n            sess, image_lists, FLAGS.test_batch_size, \'testing\',\n            FLAGS.bottleneck_dir, FLAGS.image_dir, jpeg_data_tensor,\n            decoded_image_tensor, resized_image_tensor, bottleneck_tensor,\n            FLAGS.architecture))\n    test_accuracy, predictions = sess.run(\n        [evaluation_step, prediction],\n        feed_dict={bottleneck_input: test_bottlenecks,\n                   ground_truth_input: test_ground_truth})\n    tf.logging.info(\'Final test accuracy = %.1f%% (N=%d)\' %\n                    (test_accuracy * 100, len(test_bottlenecks)))\n\n    if FLAGS.print_misclassified_test_images:\n      tf.logging.info(\'=== MISCLASSIFIED TEST IMAGES ===\')\n      for i, test_filename in enumerate(test_filenames):\n        if predictions[i] != test_ground_truth[i].argmax():\n          tf.logging.info(\'%70s  %s\' %\n                          (test_filename,\n                           list(image_lists.keys())[predictions[i]]))\n\n    # Write out the trained graph and labels with the weights stored as\n    # constants.\n    save_graph_to_file(sess, graph, FLAGS.output_graph)\n    with gfile.FastGFile(FLAGS.output_labels, \'w\') as f:\n      f.write(\'\\n\'.join(image_lists.keys()) + \'\\n\')\n\n\nif __name__ == \'__main__\':\n  parser = argparse.ArgumentParser()\n  parser.add_argument(\n      \'--image_dir\',\n      type=str,\n      default=os.path.join(os.path.dirname(__file__), \'images\'),\n      help=\'Path to folders of labeled images.\'\n  )\n  parser.add_argument(\n      \'--output_graph\',\n      type=str,\n      default=os.path.join(os.path.dirname(__file__), \'tf_files\', \'output_graph.pb\'),\n      help=\'Where to save the trained graph.\'\n  )\n  parser.add_argument(\n      \'--intermediate_output_graphs_dir\',\n      type=str,\n      default=\'/tmp/intermediate_graph/\',\n      help=\'Where to save the intermediate graphs.\'\n  )\n  parser.add_argument(\n      \'--intermediate_store_frequency\',\n      type=int,\n      default=0,\n      help=""""""\\\n         How many steps to store intermediate graph. If ""0"" then will not\n         store.\\\n      """"""\n  )\n  parser.add_argument(\n      \'--output_labels\',\n      type=str,\n      default=os.path.join(os.path.dirname(__file__), \'tf_files\', \'output_labels.txt\'),\n      help=\'Where to save the trained graph\\\'s labels.\'\n  )\n  parser.add_argument(\n      \'--summaries_dir\',\n      type=str,\n      default=os.path.join(os.path.dirname(__file__), \'tf_files\', \'training_summaries\'),\n      help=\'Where to save summary logs for TensorBoard.\'\n  )\n  parser.add_argument(\n      \'--how_many_training_steps\',\n      type=int,\n      default=40000,\n      help=\'How many training steps to run before ending.\'\n  )\n  parser.add_argument(\n      \'--learning_rate\',\n      type=float,\n      default=0.005,\n      help=\'How large a learning rate to use when training.\'\n  )\n  parser.add_argument(\n      \'--testing_percentage\',\n      type=int,\n      default=10,\n      help=\'What percentage of images to use as a test set.\'\n  )\n  parser.add_argument(\n      \'--validation_percentage\',\n      type=int,\n      default=10,\n      help=\'What percentage of images to use as a validation set.\'\n  )\n  parser.add_argument(\n      \'--eval_step_interval\',\n      type=int,\n      default=10,\n      help=\'How often to evaluate the training results.\'\n  )\n  parser.add_argument(\n      \'--train_batch_size\',\n      type=int,\n      default=100,\n      help=\'How many images to train on at a time.\'\n  )\n  parser.add_argument(\n      \'--test_batch_size\',\n      type=int,\n      default=-1,\n      help=""""""\\\n      How many images to test on. This test set is only used once, to evaluate\n      the final accuracy of the model after training completes.\n      A value of -1 causes the entire test set to be used, which leads to more\n      stable results across runs.\\\n      """"""\n  )\n  parser.add_argument(\n      \'--validation_batch_size\',\n      type=int,\n      default=100,\n      help=""""""\\\n      How many images to use in an evaluation batch. This validation set is\n      used much more often than the test set, and is an early indicator of how\n      accurate the model is during training.\n      A value of -1 causes the entire validation set to be used, which leads to\n      more stable results across training iterations, but may be slower on large\n      training sets.\\\n      """"""\n  )\n  parser.add_argument(\n      \'--print_misclassified_test_images\',\n      default=False,\n      help=""""""\\\n      Whether to print out a list of all misclassified test images.\\\n      """""",\n      action=\'store_true\'\n  )\n  parser.add_argument(\n      \'--model_dir\',\n      type=str,\n      default=os.path.join(os.path.dirname(__file__), \'tf_files\', \'models\'),\n      help=""""""\\\n      Path to classify_image_graph_def.pb,\n      imagenet_synset_to_human_label_map.txt, and\n      imagenet_2012_challenge_label_map_proto.pbtxt.\\\n      """"""\n  )\n  parser.add_argument(\n      \'--bottleneck_dir\',\n      type=str,\n      default=os.path.join(os.path.dirname(__file__), \'tf_files\', \'bottlenecks\'),\n      help=\'Path to cache bottleneck layer values as files.\'\n  )\n  parser.add_argument(\n      \'--final_tensor_name\',\n      type=str,\n      default=\'final_result\',\n      help=""""""\\\n      The name of the output classification layer in the retrained graph.\\\n      """"""\n  )\n  parser.add_argument(\n      \'--flip_left_right\',\n      default=False,\n      help=""""""\\\n      Whether to randomly flip half of the training images horizontally.\\\n      """""",\n      action=\'store_true\'\n  )\n  parser.add_argument(\n      \'--random_crop\',\n      type=int,\n      default=0,\n      help=""""""\\\n      A percentage determining how much of a margin to randomly crop off the\n      training images.\\\n      """"""\n  )\n  parser.add_argument(\n      \'--random_scale\',\n      type=int,\n      default=0,\n      help=""""""\\\n      A percentage determining how much to randomly scale up the size of the\n      training images by.\\\n      """"""\n  )\n  parser.add_argument(\n      \'--random_brightness\',\n      type=int,\n      default=0,\n      help=""""""\\\n      A percentage determining how much to randomly multiply the training image\n      input pixels up or down by.\\\n      """"""\n  )\n  parser.add_argument(\n      \'--architecture\',\n      type=str,\n      default=\'mobilenet_1.0_224\',\n      help=""""""\\\n      Which model architecture to use. \'inception_v3\' is the most accurate, but\n      also the slowest. For faster or smaller models, chose a MobileNet with the\n      form \'mobilenet_<parameter size>_<input_size>[_quantized]\'. For example,\n      \'mobilenet_1.0_224\' will pick a model that is 17 MB in size and takes 224\n      pixel input images, while \'mobilenet_0.25_128_quantized\' will choose a much\n      less accurate, but smaller and faster network that\'s 920 KB on disk and\n      takes 128x128 images. See https://research.googleblog.com/2017/06/mobilenets-open-source-models-for.html\n      for more information on Mobilenet.\\\n      """""")\n  FLAGS, unparsed = parser.parse_known_args()\n  tf.app.run(main=main, argv=[sys.argv[0]] + unparsed)\n'"
photonix/photos/management/__init__.py,0,b''
photonix/photos/migrations/0001_initial.py,0,"b""# -*- coding: utf-8 -*-\n# Generated by Django 1.11.16 on 2019-01-11 19:38\nfrom __future__ import unicode_literals\n\nfrom django.db import migrations, models\nimport django.db.models.deletion\nimport uuid\n\n\nclass Migration(migrations.Migration):\n\n    initial = True\n\n    dependencies = [\n    ]\n\n    operations = [\n        migrations.CreateModel(\n            name='Camera',\n            fields=[\n                ('id', models.UUIDField(default=uuid.uuid4, editable=False, primary_key=True, serialize=False)),\n                ('created_at', models.DateTimeField(blank=True)),\n                ('updated_at', models.DateTimeField(blank=True)),\n                ('make', models.CharField(max_length=128)),\n                ('model', models.CharField(max_length=128)),\n                ('earliest_photo', models.DateTimeField()),\n                ('latest_photo', models.DateTimeField()),\n            ],\n            options={\n                'ordering': ['make', 'model'],\n            },\n        ),\n        migrations.CreateModel(\n            name='Face',\n            fields=[\n                ('id', models.UUIDField(default=uuid.uuid4, editable=False, primary_key=True, serialize=False)),\n                ('created_at', models.DateTimeField(blank=True)),\n                ('updated_at', models.DateTimeField(blank=True)),\n                ('position_x', models.FloatField()),\n                ('position_y', models.FloatField()),\n                ('size_x', models.FloatField()),\n                ('size_y', models.FloatField()),\n                ('source', models.CharField(choices=[('H', 'Human'), ('C', 'Computer')], max_length=1)),\n                ('confidence', models.FloatField()),\n                ('verified', models.BooleanField(default=False)),\n                ('hidden', models.BooleanField(default=False)),\n            ],\n            options={\n                'abstract': False,\n            },\n        ),\n        migrations.CreateModel(\n            name='Lens',\n            fields=[\n                ('id', models.UUIDField(default=uuid.uuid4, editable=False, primary_key=True, serialize=False)),\n                ('created_at', models.DateTimeField(blank=True)),\n                ('updated_at', models.DateTimeField(blank=True)),\n                ('name', models.CharField(max_length=128)),\n                ('earliest_photo', models.DateTimeField()),\n                ('latest_photo', models.DateTimeField()),\n            ],\n            options={\n                'verbose_name_plural': 'lenses',\n                'ordering': ['name'],\n            },\n        ),\n        migrations.CreateModel(\n            name='Photo',\n            fields=[\n                ('id', models.UUIDField(default=uuid.uuid4, editable=False, primary_key=True, serialize=False)),\n                ('created_at', models.DateTimeField(blank=True)),\n                ('updated_at', models.DateTimeField(blank=True)),\n                ('taken_at', models.DateTimeField(null=True)),\n                ('taken_by', models.CharField(blank=True, max_length=128, null=True)),\n                ('aperture', models.DecimalField(decimal_places=1, max_digits=3, null=True)),\n                ('exposure', models.CharField(blank=True, max_length=8, null=True)),\n                ('iso_speed', models.PositiveIntegerField(null=True)),\n                ('focal_length', models.DecimalField(decimal_places=1, max_digits=4, null=True)),\n                ('flash', models.NullBooleanField()),\n                ('metering_mode', models.CharField(max_length=32, null=True)),\n                ('drive_mode', models.CharField(max_length=32, null=True)),\n                ('shooting_mode', models.CharField(max_length=32, null=True)),\n                ('latitude', models.DecimalField(decimal_places=6, max_digits=9, null=True)),\n                ('longitude', models.DecimalField(decimal_places=6, max_digits=9, null=True)),\n                ('altitude', models.DecimalField(decimal_places=1, max_digits=6, null=True)),\n                ('last_thumbnailed_version', models.PositiveIntegerField(null=True)),\n                ('last_thumbnailed_at', models.DateTimeField(null=True)),\n                ('classifier_color_version', models.PositiveIntegerField(null=True)),\n                ('classifier_color_queued_at', models.DateTimeField(null=True)),\n                ('classifier_color_completed_at', models.DateTimeField(null=True)),\n                ('classifier_location_version', models.PositiveIntegerField(null=True)),\n                ('classifier_location_queued_at', models.DateTimeField(null=True)),\n                ('classifier_location_completed_at', models.DateTimeField(null=True)),\n                ('classifier_object_version', models.PositiveIntegerField(null=True)),\n                ('classifier_object_queued_at', models.DateTimeField(null=True)),\n                ('classifier_object_completed_at', models.DateTimeField(null=True)),\n                ('classifier_person_version', models.PositiveIntegerField(null=True)),\n                ('classifier_person_queued_at', models.DateTimeField(null=True)),\n                ('classifier_person_completed_at', models.DateTimeField(null=True)),\n                ('classifier_style_version', models.PositiveIntegerField(null=True)),\n                ('classifier_style_queued_at', models.DateTimeField(null=True)),\n                ('classifier_style_completed_at', models.DateTimeField(null=True)),\n                ('camera', models.ForeignKey(null=True, on_delete=django.db.models.deletion.CASCADE, related_name='photos', to='photos.Camera')),\n                ('lens', models.ForeignKey(null=True, on_delete=django.db.models.deletion.CASCADE, related_name='photos', to='photos.Lens')),\n            ],\n            options={\n                'ordering': ['-taken_at'],\n            },\n        ),\n        migrations.CreateModel(\n            name='PhotoFile',\n            fields=[\n                ('id', models.UUIDField(default=uuid.uuid4, editable=False, primary_key=True, serialize=False)),\n                ('created_at', models.DateTimeField(blank=True)),\n                ('updated_at', models.DateTimeField(blank=True)),\n                ('path', models.CharField(max_length=512)),\n                ('width', models.PositiveSmallIntegerField()),\n                ('height', models.PositiveSmallIntegerField()),\n                ('mimetype', models.CharField(blank=True, max_length=32)),\n                ('file_modified_at', models.DateTimeField()),\n                ('bytes', models.PositiveIntegerField()),\n                ('preferred', models.BooleanField(default=False)),\n                ('photo', models.ForeignKey(on_delete=django.db.models.deletion.CASCADE, related_name='files', to='photos.Photo')),\n            ],\n            options={\n                'abstract': False,\n            },\n        ),\n        migrations.CreateModel(\n            name='PhotoTag',\n            fields=[\n                ('id', models.UUIDField(default=uuid.uuid4, editable=False, primary_key=True, serialize=False)),\n                ('created_at', models.DateTimeField(blank=True)),\n                ('updated_at', models.DateTimeField(blank=True)),\n                ('source', models.CharField(choices=[('H', 'Human'), ('C', 'Computer')], max_length=1)),\n                ('model_version', models.PositiveIntegerField(null=True)),\n                ('confidence', models.FloatField()),\n                ('significance', models.FloatField(null=True)),\n                ('verified', models.BooleanField(default=False)),\n                ('hidden', models.BooleanField(default=False)),\n                ('position_x', models.FloatField(null=True)),\n                ('position_y', models.FloatField(null=True)),\n                ('size_x', models.FloatField(null=True)),\n                ('size_y', models.FloatField(null=True)),\n                ('face', models.ForeignKey(null=True, on_delete=django.db.models.deletion.CASCADE, related_name='photo_tags', to='photos.Face')),\n                ('photo', models.ForeignKey(on_delete=django.db.models.deletion.CASCADE, related_name='photo_tags', to='photos.Photo')),\n            ],\n            options={\n                'ordering': ['-significance'],\n            },\n        ),\n        migrations.CreateModel(\n            name='Tag',\n            fields=[\n                ('id', models.UUIDField(default=uuid.uuid4, editable=False, primary_key=True, serialize=False)),\n                ('created_at', models.DateTimeField(blank=True)),\n                ('updated_at', models.DateTimeField(blank=True)),\n                ('name', models.CharField(max_length=128)),\n                ('type', models.CharField(choices=[('L', 'Location'), ('O', 'Object'), ('P', 'Person'), ('C', 'Color'), ('S', 'Style')], max_length=1, null=True)),\n                ('source', models.CharField(choices=[('H', 'Human'), ('C', 'Computer')], max_length=1)),\n                ('parent', models.ForeignKey(null=True, on_delete=django.db.models.deletion.CASCADE, related_name='+', to='photos.Tag')),\n            ],\n            options={\n                'ordering': ['name'],\n            },\n        ),\n        migrations.AddField(\n            model_name='phototag',\n            name='tag',\n            field=models.ForeignKey(on_delete=django.db.models.deletion.CASCADE, related_name='photo_tags', to='photos.Tag'),\n        ),\n        migrations.AddField(\n            model_name='face',\n            name='photo',\n            field=models.ForeignKey(on_delete=django.db.models.deletion.CASCADE, related_name='faces', to='photos.Photo'),\n        ),\n        migrations.AlterUniqueTogether(\n            name='tag',\n            unique_together=set([('name', 'type', 'source')]),\n        ),\n    ]\n"""
photonix/photos/migrations/0002_auto_20190224_2119.py,0,"b""# -*- coding: utf-8 -*-\n# Generated by Django 1.11.20 on 2019-02-24 21:19\nfrom __future__ import unicode_literals\n\nfrom django.db import migrations, models\nimport uuid\n\n\nclass Migration(migrations.Migration):\n\n    dependencies = [\n        ('photos', '0001_initial'),\n    ]\n\n    operations = [\n        migrations.CreateModel(\n            name='Task',\n            fields=[\n                ('id', models.UUIDField(default=uuid.uuid4, editable=False, primary_key=True, serialize=False)),\n                ('created_at', models.DateTimeField(blank=True)),\n                ('updated_at', models.DateTimeField(blank=True)),\n                ('type', models.CharField(db_index=True, max_length=128)),\n                ('subject_id', models.UUIDField()),\n                ('status', models.CharField(choices=[('L', 'Location'), ('O', 'Object'), ('F', 'Face'), ('C', 'Color'), ('S', 'Style')], db_index=True, default='P', max_length=1)),\n                ('started_at', models.DateTimeField(null=True)),\n                ('finished_at', models.DateTimeField(null=True)),\n            ],\n            options={\n                'abstract': False,\n            },\n        ),\n        migrations.RemoveField(\n            model_name='face',\n            name='photo',\n        ),\n        migrations.RemoveField(\n            model_name='photo',\n            name='classifier_color_completed_at',\n        ),\n        migrations.RemoveField(\n            model_name='photo',\n            name='classifier_color_queued_at',\n        ),\n        migrations.RemoveField(\n            model_name='photo',\n            name='classifier_color_version',\n        ),\n        migrations.RemoveField(\n            model_name='photo',\n            name='classifier_location_completed_at',\n        ),\n        migrations.RemoveField(\n            model_name='photo',\n            name='classifier_location_queued_at',\n        ),\n        migrations.RemoveField(\n            model_name='photo',\n            name='classifier_location_version',\n        ),\n        migrations.RemoveField(\n            model_name='photo',\n            name='classifier_object_completed_at',\n        ),\n        migrations.RemoveField(\n            model_name='photo',\n            name='classifier_object_queued_at',\n        ),\n        migrations.RemoveField(\n            model_name='photo',\n            name='classifier_object_version',\n        ),\n        migrations.RemoveField(\n            model_name='photo',\n            name='classifier_person_completed_at',\n        ),\n        migrations.RemoveField(\n            model_name='photo',\n            name='classifier_person_queued_at',\n        ),\n        migrations.RemoveField(\n            model_name='photo',\n            name='classifier_person_version',\n        ),\n        migrations.RemoveField(\n            model_name='photo',\n            name='classifier_style_completed_at',\n        ),\n        migrations.RemoveField(\n            model_name='photo',\n            name='classifier_style_queued_at',\n        ),\n        migrations.RemoveField(\n            model_name='photo',\n            name='classifier_style_version',\n        ),\n        migrations.RemoveField(\n            model_name='photo',\n            name='last_thumbnailed_at',\n        ),\n        migrations.RemoveField(\n            model_name='photo',\n            name='last_thumbnailed_version',\n        ),\n        migrations.RemoveField(\n            model_name='phototag',\n            name='face',\n        ),\n        migrations.AddField(\n            model_name='photo',\n            name='visible',\n            field=models.BooleanField(default=False),\n        ),\n        migrations.AlterField(\n            model_name='tag',\n            name='type',\n            field=models.CharField(choices=[('L', 'Location'), ('O', 'Object'), ('F', 'Face'), ('C', 'Color'), ('S', 'Style')], max_length=1, null=True),\n        ),\n        migrations.DeleteModel(\n            name='Face',\n        ),\n    ]\n"""
photonix/photos/migrations/0003_auto_20190225_0647.py,0,"b""# -*- coding: utf-8 -*-\n# Generated by Django 1.11.20 on 2019-02-25 06:47\nfrom __future__ import unicode_literals\n\nfrom django.db import migrations, models\n\n\nclass Migration(migrations.Migration):\n\n    dependencies = [\n        ('photos', '0002_auto_20190224_2119'),\n    ]\n\n    operations = [\n        migrations.AlterField(\n            model_name='camera',\n            name='created_at',\n            field=models.DateTimeField(blank=True, db_index=True),\n        ),\n        migrations.AlterField(\n            model_name='lens',\n            name='created_at',\n            field=models.DateTimeField(blank=True, db_index=True),\n        ),\n        migrations.AlterField(\n            model_name='photo',\n            name='created_at',\n            field=models.DateTimeField(blank=True, db_index=True),\n        ),\n        migrations.AlterField(\n            model_name='photofile',\n            name='created_at',\n            field=models.DateTimeField(blank=True, db_index=True),\n        ),\n        migrations.AlterField(\n            model_name='phototag',\n            name='created_at',\n            field=models.DateTimeField(blank=True, db_index=True),\n        ),\n        migrations.AlterField(\n            model_name='tag',\n            name='created_at',\n            field=models.DateTimeField(blank=True, db_index=True),\n        ),\n        migrations.AlterField(\n            model_name='task',\n            name='created_at',\n            field=models.DateTimeField(blank=True, db_index=True),\n        ),\n        migrations.AlterField(\n            model_name='task',\n            name='subject_id',\n            field=models.UUIDField(db_index=True),\n        ),\n    ]\n"""
photonix/photos/migrations/0004_auto_20190225_2147.py,0,"b""# -*- coding: utf-8 -*-\n# Generated by Django 1.11.20 on 2019-02-25 21:47\nfrom __future__ import unicode_literals\n\nfrom django.db import migrations, models\nimport django.db.models.deletion\n\n\nclass Migration(migrations.Migration):\n\n    dependencies = [\n        ('photos', '0003_auto_20190225_0647'),\n    ]\n\n    operations = [\n        migrations.AddField(\n            model_name='task',\n            name='complete_with_children',\n            field=models.BooleanField(default=False),\n        ),\n        migrations.AddField(\n            model_name='task',\n            name='parent',\n            field=models.ForeignKey(null=True, on_delete=django.db.models.deletion.CASCADE, related_name='children', to='photos.Task'),\n        ),\n    ]\n"""
photonix/photos/migrations/0005_auto_20190305_1837.py,0,"b""# -*- coding: utf-8 -*-\n# Generated by Django 1.11.20 on 2019-03-05 18:37\nfrom __future__ import unicode_literals\n\nfrom django.db import migrations, models\n\n\nclass Migration(migrations.Migration):\n\n    dependencies = [\n        ('photos', '0004_auto_20190225_2147'),\n    ]\n\n    operations = [\n        migrations.AlterModelOptions(\n            name='task',\n            options={'ordering': ['created_at']},\n        ),\n        migrations.AddField(\n            model_name='photofile',\n            name='raw_external_params',\n            field=models.CharField(blank=True, max_length=16, null=True),\n        ),\n        migrations.AddField(\n            model_name='photofile',\n            name='raw_external_version',\n            field=models.CharField(blank=True, max_length=16, null=True),\n        ),\n        migrations.AddField(\n            model_name='photofile',\n            name='raw_processed',\n            field=models.BooleanField(default=False),\n        ),\n        migrations.AddField(\n            model_name='photofile',\n            name='raw_version',\n            field=models.PositiveIntegerField(null=True),\n        ),\n        migrations.AlterField(\n            model_name='photofile',\n            name='mimetype',\n            field=models.CharField(blank=True, max_length=32, null=True),\n        ),\n    ]\n"""
photonix/photos/migrations/0006_auto_20190306_1827.py,0,"b""# -*- coding: utf-8 -*-\n# Generated by Django 1.11.20 on 2019-03-06 18:27\nfrom __future__ import unicode_literals\n\nfrom django.db import migrations, models\n\n\nclass Migration(migrations.Migration):\n\n    dependencies = [\n        ('photos', '0005_auto_20190305_1837'),\n    ]\n\n    operations = [\n        migrations.AlterField(\n            model_name='photofile',\n            name='height',\n            field=models.PositiveSmallIntegerField(null=True),\n        ),\n        migrations.AlterField(\n            model_name='photofile',\n            name='width',\n            field=models.PositiveSmallIntegerField(null=True),\n        ),\n    ]\n"""
photonix/photos/migrations/0007_auto_20190507_1655.py,0,"b""# -*- coding: utf-8 -*-\n# Generated by Django 1.11.20 on 2019-05-07 16:55\nfrom __future__ import unicode_literals\n\nfrom django.db import migrations\n\n\nclass Migration(migrations.Migration):\n\n    dependencies = [\n        ('photos', '0006_auto_20190306_1827'),\n    ]\n\n    operations = [\n        migrations.AlterUniqueTogether(\n            name='camera',\n            unique_together=set([('make', 'model')]),\n        ),\n    ]\n"""
photonix/photos/migrations/0008_auto_20191019_1851.py,0,"b""# -*- coding: utf-8 -*-\n# Generated by Django 1.11.23 on 2019-10-19 18:51\nfrom __future__ import unicode_literals\n\nfrom django.db import migrations, models\n\n\nclass Migration(migrations.Migration):\n\n    dependencies = [\n        ('photos', '0007_auto_20190507_1655'),\n    ]\n\n    operations = [\n        migrations.AlterField(\n            model_name='photofile',\n            name='height',\n            field=models.PositiveIntegerField(null=True),\n        ),\n        migrations.AlterField(\n            model_name='photofile',\n            name='width',\n            field=models.PositiveIntegerField(null=True),\n        ),\n    ]\n"""
photonix/photos/migrations/__init__.py,0,b''
photonix/photos/utils/__init__.py,0,b''
photonix/photos/utils/classification.py,0,"b""import queue\nimport threading\nfrom time import sleep\n\nfrom django.db import transaction\nfrom django.utils import timezone\nfrom photonix.photos.models import Task\nfrom photonix.photos.utils.tasks import requeue_stuck_tasks\n\nCLASSIFIERS = [\n    'color',\n    'location',\n    'object',\n    'style',\n]\n\n\ndef process_classify_images_tasks():\n    for task in Task.objects.filter(type='classify_images', status='P').order_by('created_at'):\n        photo_id = task.subject_id\n        generate_classifier_tasks_for_photo(photo_id, task)\n\n\ndef generate_classifier_tasks_for_photo(photo_id, task):\n    task.start()\n    started = timezone.now()\n\n    # Add task for each classifier on current photo\n    with transaction.atomic():\n        for classifier in CLASSIFIERS:\n            Task(type='classify.{}'.format(classifier), subject_id=photo_id, parent=task).save()\n        task.complete_with_children = True\n        task.save()\n\n\nclass ThreadedQueueProcessor:\n    def __init__(self, model=None, task_type=None, runner=None, num_workers=4, batch_size=64):\n        self.model = model\n        self.task_type = task_type\n        self.runner = runner\n        self.num_workers = num_workers\n        self.batch_size = batch_size\n        self.queue = queue.Queue()\n        self.threads = []\n\n    def __worker(self):\n        while True:\n            task = self.queue.get()\n\n            if task is None:\n                break\n\n            self.__process_task(task)\n\n            self.queue.task_done()\n\n    def __process_task(self, task):\n        try:\n            print('running task')\n            task.start()\n            self.runner(task.subject_id)\n            task.complete()\n        except:\n            task.failed()\n\n    def __clean_up(self):\n        # Shut down threads cleanly\n        for i in range(self.num_workers):\n            self.queue.put(None)\n        for t in self.threads:\n            t.join()\n\n    def run(self, loop=True):\n        print('Starting {} {} workers\\n'.format(self.num_workers, self.task_type))\n\n        if self.num_workers > 1:\n            for i in range(self.num_workers):\n                t = threading.Thread(target=self.__worker)\n                t.start()\n                self.threads.append(t)\n\n        try:\n            while True:\n                requeue_stuck_tasks(self.task_type)\n\n                for task in Task.objects.filter(type=self.task_type, status='P')[:64]:\n                    if self.num_workers > 1:\n                        print('putting task')\n                        self.queue.put(task)\n                    else:\n                        self.__process_task(task)\n\n                if self.num_workers > 1:\n                    self.queue.join()\n\n                if not loop:\n                    self.__clean_up()\n                    return\n                sleep(1)\n\n        except KeyboardInterrupt:\n            self.__clean_up()\n"""
photonix/photos/utils/db.py,0,"b""import mimetypes\nimport os\nfrom datetime import datetime\nfrom decimal import Decimal\n\nfrom django.utils.timezone import utc\nfrom photonix.photos.models import Camera, Lens, Photo, PhotoFile, Task\nfrom photonix.photos.utils.metadata import (PhotoMetadata, parse_datetime,\n                                            parse_gps_location)\n\n\ndef record_photo(path):\n    file_modified_at = datetime.fromtimestamp(os.stat(path).st_mtime, tz=utc)\n\n    try:\n        photo_file = PhotoFile.objects.get(path=path)\n    except PhotoFile.DoesNotExist:\n        photo_file = PhotoFile()\n\n    if photo_file and photo_file.file_modified_at == file_modified_at:\n        return False\n\n    metadata = PhotoMetadata(path)\n    date_taken = parse_datetime(metadata.get('Date/Time Original'))\n\n    camera = None\n    camera_make = metadata.get('Make')\n    camera_model = metadata.get('Camera Model Name')\n    if camera_model:\n        camera_model = camera_model.replace(camera_make, '').strip()\n    if camera_make and camera_model:\n        try:\n            camera = Camera.objects.get(make=camera_make, model=camera_model)\n            if date_taken < camera.earliest_photo:\n                camera.earliest_photo = date_taken\n                camera.save()\n            if date_taken > camera.latest_photo:\n                camera.latest_photo = date_taken\n                camera.save()\n        except Camera.DoesNotExist:\n            camera = Camera(make=camera_make, model=camera_model, earliest_photo=date_taken, latest_photo=date_taken)\n            camera.save()\n\n    lens = None\n    lens_name = metadata.get('Lens ID')\n    if lens_name:\n        try:\n            lens = Lens.objects.get(name=lens_name)\n            if date_taken < lens.earliest_photo:\n                lens.earliest_photo = date_taken\n                lens.save()\n            if date_taken > lens.latest_photo:\n                lens.latest_photo = date_taken\n                lens.save()\n        except Lens.DoesNotExist:\n            lens = Lens(name=lens_name, earliest_photo=date_taken, latest_photo=date_taken)\n            lens.save()\n\n    photo = None\n    if date_taken:\n        try:\n            # TODO: Match on file number/file name as well\n            photo = Photo.objects.get(taken_at=date_taken)\n        except Photo.DoesNotExist:\n            pass\n\n    latitude = None\n    longitude = None\n    if metadata.get('GPS Position'):\n        latitude, longitude = parse_gps_location(metadata.get('GPS Position'))\n\n\n    if not photo:\n        # Save Photo\n\n        aperture = None\n        aperturestr = metadata.get('Aperture')\n        if aperturestr:\n            try:\n                aperture = Decimal(aperturestr)\n                if aperture.is_infinite():\n                    aperture = None\n            except:\n                pass\n\n        photo = Photo(\n            taken_at=date_taken,\n            taken_by=metadata.get('Artist') or None,\n            aperture=aperture,\n            exposure=metadata.get('Exposure Time') or None,\n            iso_speed=metadata.get('ISO') and int(metadata.get('ISO')) or None,\n            focal_length=metadata.get('Focal Length') and metadata.get('Focal Length').split(' ', 1)[0] or None,\n            flash=metadata.get('Flash') and 'on' in metadata.get('Flash').lower() or False,\n            metering_mode=metadata.get('Metering Mode') or None,\n            drive_mode=metadata.get('Drive Mode') or None,\n            shooting_mode=metadata.get('Shooting Mode') or None,\n            camera=camera,\n            lens=lens,\n            latitude=latitude,\n            longitude=longitude,\n            altitude=metadata.get('GPS Altitude') and metadata.get('GPS Altitude').split(' ')[0]\n        )\n        photo.save()\n\n    width = metadata.get('Image Width')\n    height = metadata.get('Image Height')\n    if metadata.get('Orientation') in ['Rotate 90 CW', 'Rotate 270 CCW', 'Rotate 90 CCW', 'Rotate 270 CW']:\n        old_width = width\n        width = height\n        height = old_width\n\n    # Save PhotoFile\n    photo_file.photo = photo\n    photo_file.path             = path\n    photo_file.width            = width\n    photo_file.height           = height\n    photo_file.mimetype         = mimetypes.guess_type(path)[0]\n    photo_file.file_modified_at = file_modified_at\n    photo_file.bytes            = os.stat(path).st_size\n    photo_file.preferred        = False  # TODO\n    photo_file.save()\n\n    # Create task to ensure JPEG version of file exists (used for thumbnailing, analysing etc.)\n    Task(\n        type='ensure_raw_processed',\n        subject_id=photo.id,\n        complete_with_children=True\n    ).save()\n\n    return photo\n"""
photonix/photos/utils/fs.py,0,"b""import errno\nfrom hashlib import md5\nimport os\nimport shutil\nimport tempfile\n\nfrom django.conf import settings\nimport requests\n\n\ndef mkdir_p(path):\n    try:\n        os.makedirs(path)\n    except OSError as exc:\n        if exc.errno == errno.EEXIST and os.path.isdir(path):\n            pass\n        else:\n            raise\n\n\ndef determine_destination(fn):\n    extension = os.path.splitext(fn)[1][1:].lower()\n    for output_filter in settings.PHOTO_OUTPUT_DIRS:\n        if extension in output_filter['EXTENSIONS']:\n            return output_filter['PATH']\n    return None\n\n\ndef find_new_file_name(path):\n    '''\n    If a file already exists in the same place with the same name, this\n    function will find a new name to use, changing the extension to\n    '_1.jpg' or similar.\n    '''\n    counter = 1\n    fn, extension = os.path.splitext(path)\n    attempt = path\n    while os.path.exists(attempt):\n        attempt = '{}_{}{}'.format(fn, counter, extension)\n        counter += 1\n    return attempt\n\n\ndef download_file(url, destination_path):\n    temp_path = tempfile.mktemp()\n    with requests.get(url, stream=True) as r:\n        with open(temp_path, 'wb') as f:\n            for chunk in r.iter_content(chunk_size=32768):\n                if chunk:\n                    f.write(chunk)\n    shutil.move(temp_path, destination_path)\n    return destination_path\n\n\ndef md5sum(path):\n    hash_md5 = md5()\n    with open(path, 'rb') as f:\n        for chunk in iter(lambda: f.read(4096), b''):\n            hash_md5.update(chunk)\n    return hash_md5.hexdigest()\n"""
photonix/photos/utils/metadata.py,0,"b'import os\nimport re\nfrom subprocess import Popen, PIPE\nfrom datetime import datetime\nfrom dateutil.parser import parse as parse_date\n\nfrom django.utils.timezone import utc\n\n\nclass PhotoMetadata(object):\n    def __init__(self, path):\n        self.data = {}\n        result = Popen([\'exiftool\', path], stdout=PIPE, stdin=PIPE, stderr=PIPE).communicate()[0].decode(\'utf-8\')\n        for line in str(result).split(\'\\n\'):\n            if line:\n                k, v = line.split(\':\', 1)\n                self.data[k.strip()] = v.strip()\n\n    def get(self, attribute):\n        return self.data.get(attribute)\n\n\ndef parse_datetime(date_str):\n    if not date_str:\n        return None\n    if \'.\' in date_str:\n        date_str = date_str.split(\'.\', 1)[0]\n    try:\n        return datetime.strptime(date_str, \'%Y:%m:%d %H:%M:%S\').replace(tzinfo=utc)\n    except ValueError:\n        parsed_date = parse_date(date_str)\n        if not parsed_date.tzinfo:\n            parsed_date = parsed_date.replace(tzinfo=utc)\n        return parsed_date\n\n\ndef parse_gps_location(gps_str):\n    # 50 deg 49\' 9.53"" N, 0 deg 8\' 13.33"" W\n    regex = r\'\'\'(\\d{1,3}) deg (\\d{1,2})\' (\\d{1,2}).(\\d{2})"" ([N,S]), (\\d{1,3}) deg (\\d{1,2})\' (\\d{1,2}).(\\d{2})"" ([E,W])\'\'\'\n    m = re.search(regex, gps_str)\n\n    latitude = float(m.group(1)) + (float(m.group(2)) / 60) + (float(\'{}.{}\'.format(m.group(3), m.group(4))) / 60 / 100)\n    if m.group(5) == \'S\':\n        latitude *= -1\n\n    longitude = float(m.group(6)) + (float(m.group(7)) / 60) + (float(\'{}.{}\'.format(m.group(8), m.group(9))) / 60 / 100)\n    if m.group(10) == \'W\':\n        longitude *= -1\n\n    return (latitude, longitude)\n\n\ndef get_datetime(path):\n    \'\'\'\n    Tries to get date/time from EXIF data which works on JPEG and raw files.\n    Failing it that it tries to find the date in the filename.\n    \'\'\'\n    # TODO: Use \'GPS Date/Time\' if available as it\'s more accurate\n\n    # First try the date in the metadate\n    metadata = PhotoMetadata(path)\n    date_str = metadata.get(\'Date/Time Original\')\n    if date_str:\n        return parse_datetime(date_str)\n\n    # If there was not date metadata, try to infer it from filename\n    fn = os.path.split(path)[1]\n    matched = re.search(r\'((19|20)[0-9]{2})-([0-9]{2})-([0-9]{2})\\D\', fn)\n    if not matched:\n        matched = re.search(r\'\\D((19|20)[0-9]{2})([0-9]{2})([0-9]{2})\\D\', fn)\n    if matched:\n        # import pdb; pdb.set_trace()\n        date_str = \'{}-{}-{}\'.format(matched.group(1), matched.group(3), matched.group(4))\n        return datetime.strptime(date_str, \'%Y-%m-%d\')\n    return None\n\n\ndef get_dimensions(path):\n    metadata = PhotoMetadata(path)\n    if metadata.data.get(\'Image Width\') and metadata.data.get(\'Image Height\'):\n        return (int(metadata.data[\'Image Width\']), int(metadata.data[\'Image Height\']))\n    return (None, None)\n'"
photonix/photos/utils/organise.py,0,"b""import os\nimport shutil\nfrom hashlib import md5\nfrom io import StringIO\n\nfrom PIL import Image\n\nfrom photonix.photos.utils.db import record_photo\nfrom photonix.photos.utils.fs import (determine_destination,\n                                      find_new_file_name, mkdir_p)\nfrom photonix.photos.utils.metadata import get_datetime\n\n\nclass FileHashCache(object):\n    '''\n    Used with determine_same_file() function. Can keep hold of the previously\n    opened orig and dest file contents. Can keep hold of all file-based and\n    image-based hashes per file.\n    '''\n    file_hash_cache = {}\n    file_data = {'orig': (None, None), 'dest': (None, None)}\n\n    def reset(self):\n        self.file_hash_cache = {}\n\n    def get_file_hash(self, fn, hash_type):\n        if fn in self.file_hash_cache and hash_type in self.file_hash_cache[fn]:\n            return self.file_hash_cache[fn][hash_type]\n        return None\n\n    def set_file_hash(self, fn, hash_type, hash_val):\n        if fn not in self.file_hash_cache:\n            self.file_hash_cache[fn] = {}\n        self.file_hash_cache[fn][hash_type] = hash_val\n\n    def get_file(self, fn, file_type):\n        if self.file_data[file_type][0] != fn:\n            self.file_data[file_type] = (fn, open(fn, 'rb').read())\n        return self.file_data[file_type][1]\n\n\ndef determine_same_file(origpath, destpath, fhc=None):\n    '''\n    First check if hashes of the two files match. If they don't match, they\n    could still be the same image if metadata has changed so open the pixel\n    data using PIL and compare hashes of that.\n    '''\n    if not fhc:\n        fhc = FileHashCache()\n\n    if len(fhc.file_hash_cache) > 1000:\n        fhc.reset()\n\n    orig_hash = fhc.get_file_hash(origpath, 'file')\n    if not orig_hash:\n        orig_hash = md5(fhc.get_file(origpath, 'orig')).hexdigest()\n        fhc.set_file_hash(origpath, 'file', orig_hash)\n\n    dest_hash = fhc.get_file_hash(destpath, 'file')\n    if not dest_hash:\n        dest_hash = md5(fhc.get_file(destpath, 'dest')).hexdigest()\n        fhc.set_file_hash(destpath, 'file', dest_hash)\n\n    if orig_hash == dest_hash:\n        return True\n\n    # Try matching on image data (ignoring EXIF)\n    if os.path.splitext(origpath)[1][1:].lower() in ['jpg', 'jpeg', 'png', ]:\n        orig_hash = fhc.get_file_hash(origpath, 'image')\n        if not orig_hash:\n            orig_hash = md5(Image.open(StringIO(fhc.get_file(origpath, 'orig'))).tobytes()).hexdigest()\n            fhc.set_file_hash(origpath, 'image', orig_hash)\n\n        dest_hash = fhc.get_file_hash(destpath, 'image')\n        if not dest_hash:\n            dest_hash = md5(Image.open(StringIO(fhc.get_file(destpath, 'dest'))).tobytes()).hexdigest()\n            fhc.set_file_hash(destpath, 'image', dest_hash)\n\n        if orig_hash == dest_hash:\n            return True\n    # TODO: Convert raw photos into temp jpgs to do proper comparison\n    return False\n\n\ndef blacklisted_type(file):\n    if file[-4:].lower() == '.mov' or file[-4:].lower() == '.mp4' or file[-4:].lower() == '.mkv':\n        return True\n    if file == '.DS_Store':\n        return True\n    return False\n\ndef import_photos_from_dir(orig, move=False):\n    imported = 0\n    were_duplicates = 0\n    were_bad = 0\n\n    for r, d, f in os.walk(orig):\n        for fn in sorted(f):\n            filepath = os.path.join(r, fn)\n            dest = determine_destination(filepath)\n            if blacklisted_type(fn):\n                # Blacklisted type\n                were_bad += 1\n            elif not dest:\n                # No filters match this file type\n                pass\n            elif os.path.getsize(filepath) < 102400:\n                print('FILE VERY SMALL (<100k - PROBABLY THUMBNAIL), NOT IMPORTING {}'.format(filepath))\n                were_bad += 1\n            elif os.path.getsize(filepath) > 1073741824:\n                print('FILE VERY LARGE (>1G - PROBABLY VIDEO), NOT IMPORTING {}'.format(filepath))\n                were_bad += 1\n            else:\n                t = get_datetime(filepath)\n                if t:\n                    destpath = '%02d/%02d/%02d' % (t.year, t.month, t.day)\n                    destpath = os.path.join(dest, destpath)\n                    mkdir_p(destpath)\n                    destpath = os.path.join(destpath, fn)\n\n                    if filepath == destpath:\n                        # File is already in the right place so be very careful not to do anything like delete it\n                        pass\n                    elif not os.path.exists(destpath):\n                        if move:\n                            shutil.move(filepath, destpath)\n                        else:\n                            shutil.copyfile(filepath, destpath)\n                        record_photo(destpath)\n                        imported += 1\n                        print('IMPORTED  {} -> {}'.format(filepath, destpath))\n                    else:\n                        print('PATH EXISTS  {} -> {}'.format(filepath, destpath))\n                        same = determine_same_file(filepath, destpath)\n                        print('PHOTO IS THE SAME')\n                        if same:\n                            if move:\n                                os.remove(filepath)\n                                were_duplicates += 1\n                                print('DELETED FROM SOURCE')\n                        else:\n                            print('NEED TO IMPORT UNDER DIFFERENT NAME')\n                            exit(1)\n                            destpath = find_new_file_name(destpath)\n                            shutil.move(filepath, destpath)\n                            record_photo(destpath)\n                            imported += 1\n                            # print 'IMPORTED  {} -> {}'.format(filepath, destpath)\n\n                else:\n                    print('ERROR READING DATE: {}'.format(filepath))\n                    were_bad += 1\n\n    if imported or were_duplicates:\n        print('\\n{} PHOTOS IMPORTED\\n{} WERE DUPLICATES\\n{} WERE BAD'.format(imported, were_duplicates, were_bad))\n\n\ndef import_photos_in_place(orig):\n    imported = 0\n    were_bad = 0\n\n    for r, d, f in os.walk(orig):\n        for fn in sorted(f):\n            filepath = os.path.join(r, fn)\n            if blacklisted_type(fn):\n                # Blacklisted type\n                were_bad += 1\n            elif os.path.getsize(filepath) < 102400:\n                print('FILE VERY SMALL (<100k - PROBABLY THUMBNAIL), NOT IMPORTING {}'.format(filepath))\n                were_bad += 1\n            elif os.path.getsize(filepath) > 1073741824:\n                print('FILE VERY LARGE (>1G - PROBABLY VIDEO), NOT IMPORTING {}'.format(filepath))\n                were_bad += 1\n            else:\n                modified = record_photo(filepath)\n                if modified:\n                    imported += 1\n                    print('IMPORTED  {}'.format(filepath))\n\n    if imported:\n        print('\\n{} PHOTOS IMPORTED\\n{} WERE BAD'.format(imported, were_bad))\n"""
photonix/photos/utils/raw.py,0,"b'import os\nimport re\nimport shutil\nimport subprocess\nimport tempfile\nfrom pathlib import Path\n\nfrom PIL import Image\n\nfrom django.conf import settings\nfrom photonix.photos.models import Photo, PhotoFile, Task\n\nfrom .metadata import get_dimensions\n\nRAW_PROCESS_VERSION = \'20190305\'\nNON_RAW_MIMETYPES = [\n    \'image/jpeg\',\n]\n\n\ndef ensure_raw_processing_tasks():\n    for task in Task.objects.filter(type=\'ensure_raw_processed\', status=\'P\').order_by(\'created_at\'):\n        photo_id = task.subject_id\n        ensure_raw_processed(photo_id, task)\n\n\ndef ensure_raw_processed(photo_id, task):\n    task.start()\n    photo = Photo.objects.get(id=photo_id)\n    has_raw_photos = False\n\n    for photo_file in photo.files.all():\n        # TODO: Make raw photo detection better\n        if photo_file.mimetype not in NON_RAW_MIMETYPES:\n            has_raw_photos = True\n            Task(type=\'process_raw\', subject_id=photo_file.id, parent=task).save()\n\n    # Complete and add next task to generate thumbnails\n    if not has_raw_photos:\n        task.complete(next_type=\'generate_thumbnails\', next_subject_id=photo_id)\n\n\ndef process_raw_tasks():\n    for task in Task.objects.filter(type=\'process_raw\', status=\'P\').order_by(\'created_at\'):\n        photo_file_id = task.subject_id\n        process_raw_task(photo_file_id, task)\n\n\ndef process_raw_task(photo_file_id, task):\n    task.start()\n    photo_file = PhotoFile.objects.get(id=photo_file_id)\n    output_path, version, process_params, external_version = generate_jpeg(photo_file.path)\n\n    if not os.path.isdir(settings.PHOTO_RAW_PROCESSED_DIR):\n        os.mkdir(settings.PHOTO_RAW_PROCESSED_DIR)\n    destination_path = Path(settings.PHOTO_RAW_PROCESSED_DIR) / str(\'{}.jpg\'.format(photo_file.id))\n    shutil.move(output_path, str(destination_path))\n\n    photo_file.raw_processed = True\n    photo_file.raw_version = version\n    photo_file.raw_external_params = process_params\n    photo_file.raw_external_version = external_version\n\n    if not photo_file.width or not photo_file.height:\n        width, height = get_dimensions(photo_file.base_image_path)\n        photo_file.width = width\n        photo_file.height = height\n\n    photo_file.save()\n\n    task.complete(next_type=\'generate_thumbnails\', next_subject_id=photo_file.photo.id)\n\n\ndef __get_generated_image(temp_dir, basename):\n    for fn in os.listdir(temp_dir):\n        if fn != basename:\n            return Path(temp_dir) / fn\n\n\ndef __has_acceptable_dimensions(original_image_path, new_image_path, accept_empty_original_dimensions=False):\n    original_image_dimensions = get_dimensions(original_image_path)\n    new_image_dimensions = get_dimensions(new_image_path)\n\n    # We don\'t know the original dimensions so have nothing to compare to\n    if original_image_dimensions == (None, None):\n        if accept_empty_original_dimensions:\n            return True\n        else:\n            return False\n\n    # Embedded image can\'t be the full resolution\n    if new_image_dimensions[0] < 512 or new_image_dimensions[1] < 512:\n        return False\n\n    # Embedded image is exactly the same dimensions\n    if original_image_dimensions == new_image_dimensions:\n        return True\n\n    # Embedded image within 95% of the raw width and height\n    if original_image_dimensions[0] / new_image_dimensions[0] > 0.95 \\\n        and original_image_dimensions[1] / new_image_dimensions[1] > 0.95 \\\n        and new_image_dimensions[0] / original_image_dimensions[0] > 0.95 \\\n        and new_image_dimensions[1] / original_image_dimensions[1] > 0.95:\n        return True\n\n    return False\n\n\ndef identified_as_jpeg(path):\n    output = subprocess.Popen([\'file\', path], stdout=subprocess.PIPE, stdin=subprocess.PIPE, stderr=subprocess.PIPE).communicate()[0].decode(\'utf-8\')\n    return \'JPEG image data\' in output\n\n\ndef bitmap_to_jpeg(input_path, output_path, quality=75):\n    im = Image.open(input_path)\n    im = im.convert(\'RGB\')\n    im.save(output_path, format=\'JPEG\', quality=quality)\n\n\ndef __dcraw_version():\n    output = subprocess.Popen([\'dcraw\'], stdout=subprocess.PIPE, stdin=subprocess.PIPE, stderr=subprocess.PIPE).communicate()[0].decode(\'utf-8\')\n    for line in output.split(\'\\n\'):\n        if \'Raw photo decoder ""dcraw""\' in line:\n            try:\n                return re.search(r\'v([0-9]+.[0-9]+)\', line).group(1)\n            except AttributeError:\n                return\n\n\ndef generate_jpeg(path):\n    basename = os.path.basename(path)\n    temp_dir = tempfile.mkdtemp()\n    temp_input_path = Path(temp_dir) / basename\n    shutil.copyfile(path, temp_input_path)\n\n    valid_image = False\n    process_params = None\n\n    # First try to extract the JPEG that might be inside the raw file\n    subprocess.run([\'dcraw\', \'-e\', temp_input_path])\n    temp_output_path = __get_generated_image(temp_dir, basename)\n\n    # Check the JPEGs dimensions are close enough to the raw\'s dimensions\n    if temp_output_path:\n        if __has_acceptable_dimensions(temp_input_path, temp_output_path):\n            valid_image = True\n            process_params = \'dcraw -e\'\n        else:\n            os.remove(temp_output_path)\n\n    # Next try to use embedded profile to generate an image\n    if not valid_image:\n        subprocess.run([\'dcraw\', \'-p embed\', temp_input_path])\n        temp_output_path = __get_generated_image(temp_dir, basename)\n\n        if temp_output_path:\n            if __has_acceptable_dimensions(temp_input_path, temp_output_path):\n                valid_image = True\n                process_params = \'dcraw -p embed\'\n            else:\n                os.remove(temp_output_path)\n\n    # Finally try to use the embedded whitebalance to generate an image\n    if not valid_image:\n        subprocess.run([\'dcraw\', \'-w\', temp_input_path])\n        temp_output_path = __get_generated_image(temp_dir, basename)\n\n        if temp_output_path:\n            if __has_acceptable_dimensions(temp_input_path, temp_output_path, True):\n                valid_image = True\n                process_params = \'dcraw -w\'\n            else:\n                os.remove(temp_output_path)\n\n    # If extracted image isn\'t a JPEG then we need to convert it\n    if valid_image:\n        valid_image = identified_as_jpeg(temp_output_path)\n\n        if not valid_image:\n            jpeg_path = tempfile.mktemp()\n            bitmap_to_jpeg(temp_output_path, jpeg_path)\n\n            if identified_as_jpeg(jpeg_path):\n                temp_output_path = jpeg_path\n                valid_image = True\n\n    # Move the outputted file to a new temporary location\n    if valid_image:\n        final_path = tempfile.mktemp()\n        os.rename(temp_output_path, final_path)\n\n    # Delete the temporary working directory\n    shutil.rmtree(temp_dir)\n\n    if valid_image:\n        return (final_path, RAW_PROCESS_VERSION, process_params, __dcraw_version())\n    return (None, RAW_PROCESS_VERSION, None, None)\n'"
photonix/photos/utils/system.py,0,"b""from subprocess import Popen, PIPE\n\n\ndef missing_system_dependencies(commands):\n    missing = []\n    for dependency in commands:\n        result = Popen(['which', dependency], stdout=PIPE, stdin=PIPE, stderr=PIPE).communicate()[0]\n        if not result:\n            missing.append(dependency)\n    return missing\n"""
photonix/photos/utils/tasks.py,0,"b""from datetime import timedelta\n\nfrom django.utils import timezone\n\nfrom photonix.photos.models import Task\n\n\ndef requeue_stuck_tasks(task_type, age_hours=24, max_num=8):\n    # Set old, failed jobs to Pending\n    for task in Task.objects.filter(type=task_type, status='S', updated_at__lt=timezone.now() - timedelta(hours=24))[:8]:\n        task.status = 'P'\n        task.save()\n    for task in Task.objects.filter(type=task_type, status='F', updated_at__lt=timezone.now() - timedelta(hours=24))[:8]:\n        task.status = 'P'\n        task.save()\n"""
photonix/photos/utils/thumbnails.py,0,"b""\nimport io\nimport os\n\nfrom PIL import Image, ImageOps\n\nfrom django.conf import settings\nfrom django.utils import timezone\nfrom photonix.photos.models import Photo, Task\nfrom photonix.photos.utils.metadata import PhotoMetadata\n\n\ndef process_generate_thumbnails_tasks():\n    for task in Task.objects.filter(type='generate_thumbnails', status='P').order_by('created_at'):\n        photo_id = task.subject_id\n        generate_thumbnails_for_photo(photo_id, task)\n\n\ndef generate_thumbnails_for_photo(photo, task):\n    task.start()\n\n    if not isinstance(photo, Photo):\n        try:\n            photo = Photo.objects.get(id=photo)\n        except Photo.DoesNotExist:\n            task.failed()\n            return\n\n    # TODO: Put these tasks on a thumbnailing queue like the classification_scheduler so it can be done in parallel\n    for thumbnail in settings.THUMBNAIL_SIZES:\n        if thumbnail[4]:  # Required from the start\n            try:\n                get_thumbnail(photo, thumbnail[0], thumbnail[1], thumbnail[2], thumbnail[3], force_regenerate=True)\n            except (FileNotFoundError, IndexError):\n                task.failed()\n                return\n\n    # Complete task for photo and add next task for classifying images\n    task.complete(next_type='classify_images', next_subject_id=photo.id)\n\n\ndef get_thumbnail_path(photo, width=256, height=256, crop='cover', quality=75):\n    if not isinstance(photo, Photo):\n        photo = Photo.objects.get(id=photo)\n\n    directory = os.path.join(settings.THUMBNAIL_ROOT, '{}x{}_{}_q{}'.format(width, height, crop, quality))\n    if not os.path.exists(directory):\n        os.makedirs(directory)\n\n    return os.path.join(directory, '{}.jpg'.format(photo.id))\n\n\ndef get_thumbnail(photo, width=256, height=256, crop='cover', quality=75, return_type='path', force_regenerate=False):\n    if not isinstance(photo, Photo):\n        photo = Photo.objects.get(id=photo)\n\n    # If thumbnail image was previously generated and we weren't told to re-generate, return that one\n    output_path = get_thumbnail_path(photo, width, height, crop, quality)\n    if os.path.exists(output_path):\n        if return_type == 'bytes':\n            return open(output_path, 'rb').read()\n        else:\n            return output_path\n\n    # Read base image and metadata\n    input_path = photo.base_image_path\n    im = Image.open(input_path)\n\n    if im.mode != 'RGB':\n        im = im.convert('RGB')\n\n    metadata = PhotoMetadata(input_path)\n\n    # Perform rotations if decalared in metadata\n    if metadata.get('Orientation') in ['Rotate 90 CW', 'Rotate 270 CCW']:\n        im = im.rotate(-90, expand=True)\n    elif metadata.get('Orientation') in ['Rotate 90 CCW', 'Rotate 270 CW']:\n        im = im.rotate(90, expand=True)\n\n    # Crop / resize\n    if crop == 'cover':\n        im = ImageOps.fit(im, (width, height), Image.ANTIALIAS)\n    else:\n        im.thumbnail((width, height), Image.ANTIALIAS)\n\n    # Save to disk (keeping the bytes in memory if we need to return them)\n    if return_type == 'bytes':\n        img_byte_array = io.BytesIO()\n        im.save(img_byte_array, format='JPEG', quality=quality)\n        with open(output_path, 'wb') as f:\n            f.write(img_byte_array.getvalue())\n    else:\n        im.save(output_path, format='JPEG', quality=quality)\n\n    # Update Photo DB model\n    photo.last_thumbnailed_version = 0\n    photo.last_thumbnailed_at = timezone.now()\n    photo.save()\n\n    # Return accordingly\n    if return_type == 'bytes':\n        return img_byte_array.getvalue()\n    return output_path\n"""
photonix/classifiers/object/protos/string_int_label_map_pb2.py,0,"b'# Generated by the protocol buffer compiler.  DO NOT EDIT!\n# source: string_int_label_map.proto\n\nimport sys\n_b=sys.version_info[0]<3 and (lambda x:x) or (lambda x:x.encode(\'latin1\'))\nfrom google.protobuf import descriptor as _descriptor\nfrom google.protobuf import message as _message\nfrom google.protobuf import reflection as _reflection\nfrom google.protobuf import symbol_database as _symbol_database\nfrom google.protobuf import descriptor_pb2\n# @@protoc_insertion_point(imports)\n\n_sym_db = _symbol_database.Default()\n\n\n\n\nDESCRIPTOR = _descriptor.FileDescriptor(\n  name=\'string_int_label_map.proto\',\n  package=\'object_detection.protos\',\n  syntax=\'proto2\',\n  serialized_pb=_b(\'\\n\\x1astring_int_label_map.proto\\x12\\x17object_detection.protos\\""G\\n\\x15StringIntLabelMapItem\\x12\\x0c\\n\\x04name\\x18\\x01 \\x01(\\t\\x12\\n\\n\\x02id\\x18\\x02 \\x01(\\x05\\x12\\x14\\n\\x0c\\x64isplay_name\\x18\\x03 \\x01(\\t\\""Q\\n\\x11StringIntLabelMap\\x12<\\n\\x04item\\x18\\x01 \\x03(\\x0b\\x32..object_detection.protos.StringIntLabelMapItem\')\n)\n_sym_db.RegisterFileDescriptor(DESCRIPTOR)\n\n\n\n\n_STRINGINTLABELMAPITEM = _descriptor.Descriptor(\n  name=\'StringIntLabelMapItem\',\n  full_name=\'object_detection.protos.StringIntLabelMapItem\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'name\', full_name=\'object_detection.protos.StringIntLabelMapItem.name\', index=0,\n      number=1, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=_b("""").decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'id\', full_name=\'object_detection.protos.StringIntLabelMapItem.id\', index=1,\n      number=2, type=5, cpp_type=1, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'display_name\', full_name=\'object_detection.protos.StringIntLabelMapItem.display_name\', index=2,\n      number=3, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=_b("""").decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto2\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=55,\n  serialized_end=126,\n)\n\n\n_STRINGINTLABELMAP = _descriptor.Descriptor(\n  name=\'StringIntLabelMap\',\n  full_name=\'object_detection.protos.StringIntLabelMap\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'item\', full_name=\'object_detection.protos.StringIntLabelMap.item\', index=0,\n      number=1, type=11, cpp_type=10, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto2\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=128,\n  serialized_end=209,\n)\n\n_STRINGINTLABELMAP.fields_by_name[\'item\'].message_type = _STRINGINTLABELMAPITEM\nDESCRIPTOR.message_types_by_name[\'StringIntLabelMapItem\'] = _STRINGINTLABELMAPITEM\nDESCRIPTOR.message_types_by_name[\'StringIntLabelMap\'] = _STRINGINTLABELMAP\n\nStringIntLabelMapItem = _reflection.GeneratedProtocolMessageType(\'StringIntLabelMapItem\', (_message.Message,), dict(\n  DESCRIPTOR = _STRINGINTLABELMAPITEM,\n  __module__ = \'string_int_label_map_pb2\'\n  # @@protoc_insertion_point(class_scope:object_detection.protos.StringIntLabelMapItem)\n  ))\n_sym_db.RegisterMessage(StringIntLabelMapItem)\n\nStringIntLabelMap = _reflection.GeneratedProtocolMessageType(\'StringIntLabelMap\', (_message.Message,), dict(\n  DESCRIPTOR = _STRINGINTLABELMAP,\n  __module__ = \'string_int_label_map_pb2\'\n  # @@protoc_insertion_point(class_scope:object_detection.protos.StringIntLabelMap)\n  ))\n_sym_db.RegisterMessage(StringIntLabelMap)\n\n\n# @@protoc_insertion_point(module_scope)\n'"
photonix/classifiers/object/utils/label_map_util.py,1,"b'# Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n""""""Label map utility functions.""""""\n\nimport logging\n\nimport tensorflow as tf\nfrom google.protobuf import text_format\nfrom photonix.classifiers.object.protos import string_int_label_map_pb2\n\n\ndef _validate_label_map(label_map):\n  """"""Checks if a label map is valid.\n\n  Args:\n    label_map: StringIntLabelMap to validate.\n\n  Raises:\n    ValueError: if label map is invalid.\n  """"""\n  for item in label_map.item:\n    if item.id < 1:\n      raise ValueError(\'Label map ids should be >= 1.\')\n\n\ndef create_category_index(categories):\n  """"""Creates dictionary of COCO compatible categories keyed by category id.\n\n  Args:\n    categories: a list of dicts, each of which has the following keys:\n      \'id\': (required) an integer id uniquely identifying this category.\n      \'name\': (required) string representing category name\n        e.g., \'cat\', \'dog\', \'pizza\'.\n\n  Returns:\n    category_index: a dict containing the same entries as categories, but keyed\n      by the \'id\' field of each category.\n  """"""\n  category_index = {}\n  for cat in categories:\n    category_index[cat[\'id\']] = cat\n  return category_index\n\n\ndef convert_label_map_to_categories(label_map,\n                                    max_num_classes,\n                                    use_display_name=True):\n  """"""Loads label map proto and returns categories list compatible with eval.\n\n  This function loads a label map and returns a list of dicts, each of which\n  has the following keys:\n    \'id\': (required) an integer id uniquely identifying this category.\n    \'name\': (required) string representing category name\n      e.g., \'cat\', \'dog\', \'pizza\'.\n  We only allow class into the list if its id-label_id_offset is\n  between 0 (inclusive) and max_num_classes (exclusive).\n  If there are several items mapping to the same id in the label map,\n  we will only keep the first one in the categories list.\n\n  Args:\n    label_map: a StringIntLabelMapProto or None.  If None, a default categories\n      list is created with max_num_classes categories.\n    max_num_classes: maximum number of (consecutive) label indices to include.\n    use_display_name: (boolean) choose whether to load \'display_name\' field\n      as category name.  If False or if the display_name field does not exist,\n      uses \'name\' field as category names instead.\n  Returns:\n    categories: a list of dictionaries representing all possible categories.\n  """"""\n  categories = []\n  list_of_ids_already_added = []\n  if not label_map:\n    label_id_offset = 1\n    for class_id in range(max_num_classes):\n      categories.append({\n          \'id\': class_id + label_id_offset,\n          \'name\': \'category_{}\'.format(class_id + label_id_offset)\n      })\n    return categories\n  for item in label_map.item:\n    if not 0 < item.id <= max_num_classes:\n      logging.info(\'Ignore item %d since it falls outside of requested \'\n                   \'label range.\', item.id)\n      continue\n    if use_display_name and item.HasField(\'display_name\'):\n      name = item.display_name\n    else:\n      name = item.name\n    if item.id not in list_of_ids_already_added:\n      list_of_ids_already_added.append(item.id)\n      categories.append({\'id\': item.id, \'name\': name})\n  return categories\n\n\ndef load_labelmap(path):\n  """"""Loads label map proto.\n\n  Args:\n    path: path to StringIntLabelMap proto text file.\n  Returns:\n    a StringIntLabelMapProto\n  """"""\n  with tf.io.gfile.GFile(path, \'r\') as fid:\n    label_map_string = fid.read()\n    label_map = string_int_label_map_pb2.StringIntLabelMap()\n    try:\n      text_format.Merge(label_map_string, label_map)\n    except text_format.ParseError:\n      label_map.ParseFromString(label_map_string)\n  _validate_label_map(label_map)\n  return label_map\n\n\ndef get_label_map_dict(label_map_path, use_display_name=False):\n  """"""Reads a label map and returns a dictionary of label names to id.\n\n  Args:\n    label_map_path: path to label_map.\n    use_display_name: whether to use the label map items\' display names as keys.\n\n  Returns:\n    A dictionary mapping label names to id.\n  """"""\n  label_map = load_labelmap(label_map_path)\n  label_map_dict = {}\n  for item in label_map.item:\n    if use_display_name:\n      label_map_dict[item.display_name] = item.id\n    else:\n      label_map_dict[item.name] = item.id\n  return label_map_dict\n\n\ndef create_category_index_from_labelmap(label_map_path):\n  """"""Reads a label map and returns a category index.\n\n  Args:\n    label_map_path: Path to `StringIntLabelMap` proto text file.\n\n  Returns:\n    A category index, which is a dictionary that maps integer ids to dicts\n    containing categories, e.g.\n    {1: {\'id\': 1, \'name\': \'dog\'}, 2: {\'id\': 2, \'name\': \'cat\'}, ...}\n  """"""\n  label_map = load_labelmap(label_map_path)\n  max_num_classes = max(item.id for item in label_map.item)\n  categories = convert_label_map_to_categories(label_map, max_num_classes)\n  return create_category_index(categories)\n\n\ndef create_class_agnostic_category_index():\n  """"""Creates a category index with a single `object` class.""""""\n  return {1: {\'id\': 1, \'name\': \'object\'}}\n'"
photonix/photos/management/commands/__init__.py,0,b''
photonix/photos/management/commands/classification_color_processor.py,0,"b""from django.core.management.base import BaseCommand\n# Pre-load the model graphs so it doesn't have to be done for each job\nfrom photonix.classifiers.color import ColorModel, run_on_photo\nfrom photonix.photos.models import Task\nfrom photonix.photos.utils.classification import ThreadedQueueProcessor\n\n\nprint('Loading style color model')\nmodel = ColorModel()\n\n\nclass Command(BaseCommand):\n    help = 'Runs the workers with the color classification model.'\n\n    def run_processors(self):\n        num_workers = 4\n        batch_size = 64\n        threaded_queue_processor = ThreadedQueueProcessor(model, 'classify.color', run_on_photo, num_workers, batch_size)\n        threaded_queue_processor.run()\n\n    def handle(self, *args, **options):\n        self.run_processors()\n"""
photonix/photos/management/commands/classification_location_processor.py,0,"b""from django.core.management.base import BaseCommand\n# Pre-load the model graphs so it doesn't have to be done for each job\nfrom photonix.classifiers.location import LocationModel, run_on_photo\nfrom photonix.photos.models import Task\nfrom photonix.photos.utils.classification import ThreadedQueueProcessor\n\n\nprint('Loading object location model')\nmodel = LocationModel()\n\n\nclass Command(BaseCommand):\n    help = 'Runs the workers with the location classification model.'\n\n    def run_processors(self):\n        num_workers = 4\n        batch_size = 64\n        threaded_queue_processor = ThreadedQueueProcessor(model, 'classify.location', run_on_photo, num_workers, batch_size)\n        threaded_queue_processor.run()\n\n    def handle(self, *args, **options):\n        self.run_processors()\n"""
photonix/photos/management/commands/classification_object_processor.py,0,"b""from django.core.management.base import BaseCommand\n# Pre-load the model graphs so it doesn't have to be done for each job\nfrom photonix.classifiers.object import ObjectModel, run_on_photo\nfrom photonix.photos.models import Task\nfrom photonix.photos.utils.classification import ThreadedQueueProcessor\n\n\nprint('Loading object classification model')\nmodel = ObjectModel()\n\n\nclass Command(BaseCommand):\n    help = 'Runs the workers with the object classification model.'\n\n    def run_processors(self):\n        num_workers = 4\n        batch_size = 64\n        threaded_queue_processor = ThreadedQueueProcessor(model, 'classify.object', run_on_photo, num_workers, batch_size)\n        threaded_queue_processor.run()\n\n    def handle(self, *args, **options):\n        self.run_processors()\n"""
photonix/photos/management/commands/classification_scheduler.py,0,"b""from time import sleep\n\nfrom django.core.management.base import BaseCommand\n\nfrom photonix.photos.models import Task\nfrom photonix.photos.utils.classification import process_classify_images_tasks\n\n\nclass Command(BaseCommand):\n    help = 'Loads unclassified photos onto the classification queues for processing.'\n\n    def run_scheduler(self):\n        while True:\n            num_remaining = Task.objects.filter(type='classify_images', status='P').count()\n            if num_remaining:\n                print('{} photos remaining for classification'.format(num_remaining))\n                process_classify_images_tasks()\n            sleep(1)\n\n    def handle(self, *args, **options):\n        try:\n            self.run_scheduler()\n        except KeyboardInterrupt:\n            exit(0)\n"""
photonix/photos/management/commands/classification_style_processor.py,0,"b""from django.core.management.base import BaseCommand\n# Pre-load the model graphs so it doesn't have to be done for each job\nfrom photonix.classifiers.style import StyleModel, run_on_photo\nfrom photonix.photos.models import Task\nfrom photonix.photos.utils.classification import ThreadedQueueProcessor\n\n\nprint('Loading style classification model')\nmodel = StyleModel()\n\n\nclass Command(BaseCommand):\n    help = 'Runs the workers with the style classification model.'\n\n    def run_processors(self):\n        num_workers = 4\n        batch_size = 64\n        threaded_queue_processor = ThreadedQueueProcessor(model, 'classify.style', run_on_photo, num_workers, batch_size)\n        threaded_queue_processor.run()\n\n    def handle(self, *args, **options):\n        self.run_processors()\n"""
photonix/photos/management/commands/delete_all_photos.py,0,"b""import os\nimport shutil\n\nfrom django.conf import settings\nfrom django.core.management.base import BaseCommand\n\nfrom photonix.photos.models import Camera, Lens, Photo, PhotoFile, Tag\n\n\nclass Command(BaseCommand):\n    help = 'Deletes all photos and their related other models'\n\n    def clear_dir(self, path):\n        for the_file in os.listdir(path):\n            file_path = os.path.join(path, the_file)\n            try:\n                if os.path.isfile(file_path):\n                    os.remove(file_path)\n                elif os.path.isdir(file_path):\n                    shutil.rmtree(file_path)\n            except Exception as e:\n                print(e)\n\n    def delete_all_photos(self):\n        Camera.objects.all().delete()\n        Lens.objects.all().delete()\n        Photo.objects.all().delete()\n        PhotoFile.objects.all().delete()\n        Tag.objects.all().delete()\n\n        dirs = [section['PATH'] for section in settings.PHOTO_OUTPUT_DIRS] + [settings.THUMBNAIL_ROOT] + [settings.PHOTO_RAW_PROCESSED_DIR]\n        for path in dirs:\n            try:\n                self.clear_dir(path)\n            except OSError:\n                pass\n\n    def handle(self, *args, **options):\n        self.delete_all_photos()\n"""
photonix/photos/management/commands/import_demo_photos.py,0,"b""import os\nfrom pathlib import Path\n\nfrom django.core.management.base import BaseCommand\n\nfrom photonix.photos.utils.db import record_photo\nfrom photonix.photos.utils.fs import determine_destination, download_file\n\n\nURLS = [\n    'https://live.staticflickr.com/767/32439917684_25438930aa_o_d.jpg',\n    'https://live.staticflickr.com/3706/33154545171_ed2a4af283_o_d.jpg',\n    'https://live.staticflickr.com/3849/32440369924_b9e670290a_o_d.jpg',\n    'https://live.staticflickr.com/2892/33127783192_0f73b4aa12_o_d.jpg',\n    'https://live.staticflickr.com/657/32440441724_338aa20dae_o_d.jpg',\n    'https://live.staticflickr.com/744/33155094101_766415ab15_o_d.jpg',\n    'https://live.staticflickr.com/3750/33242383206_7bf8ba68f2_o_d.jpg',\n    'https://live.staticflickr.com/572/32440255904_7ea0605d9e_o_d.jpg',\n    'https://live.staticflickr.com/3771/32468418193_f8ea8caa32_o_d.jpg',\n    'https://live.staticflickr.com/736/33127591042_2b31fe5d58_o_d.jpg',\n]\n\n\nclass Command(BaseCommand):\n    help = 'Downloads sample photos for displaying on the demo site'\n\n    def import_photos(self):\n        for url in URLS:\n            dest_dir = determine_destination(url)\n            fn = url.split('/')[-1]\n            dest_path = str(Path(dest_dir) / fn)\n\n            if not os.path.exists(dest_path):\n                print('Fetching {} -> {}'.format(url, dest_path))\n                download_file(url, dest_path)\n                record_photo(dest_path)\n\n    def handle(self, *args, **options):\n        self.import_photos()\n"""
photonix/photos/management/commands/import_photos.py,0,"b""from django.core.management.base import BaseCommand\n\nfrom photonix.photos.utils.organise import import_photos_from_dir\nfrom photonix.photos.utils.system import missing_system_dependencies\n\n\nclass Command(BaseCommand):\n    help = 'Copies all photos from one directory into structured data folder hierchy and creates relevant database records'\n\n    def add_arguments(self, parser):\n        parser.add_argument('paths', nargs='+')\n\n    def import_photos(self, paths):\n        missing = missing_system_dependencies(['exiftool', ])\n        if missing:\n            print('Missing dependencies: {}'.format(missing))\n            exit(1)\n\n        for path in paths:\n            import_photos_from_dir(path)\n\n    def handle(self, *args, **options):\n        self.import_photos(options['paths'])\n"""
photonix/photos/management/commands/raw_processor.py,0,"b""import queue\nimport threading\nfrom multiprocessing import cpu_count\nfrom time import sleep\n\nfrom django.core.management.base import BaseCommand\n\nfrom photonix.photos.models import Task\nfrom photonix.photos.utils.raw import process_raw_task\nfrom photonix.photos.utils.tasks import requeue_stuck_tasks\n\nq = queue.Queue()\n\n\ndef worker():\n    while True:\n        task = q.get()\n\n        if task is None:\n            break\n\n        process_raw_task(task.subject_id, task)\n\n        q.task_done()\n\n\nclass Command(BaseCommand):\n    help = 'Processes raw photos into a JPEG we can use elsewhere.'\n\n    def run_processors(self):\n        num_workers = cpu_count()\n        threads = []\n\n        print('Starting {} raw processor workers\\n'.format(num_workers))\n\n        for i in range(num_workers):\n            t = threading.Thread(target=worker)\n            t.start()\n            threads.append(t)\n\n        try:\n            while True:\n                requeue_stuck_tasks('process_raw')\n\n                num_remaining = Task.objects.filter(type='process_raw', status='P').count()\n                if num_remaining:\n                    print('{} tasks remaining for raw processing'.format(num_remaining))\n\n                # Load 'Pending' tasks onto worker threads\n                for task in Task.objects.filter(type='process_raw', status='P')[:64]:\n                    q.put(task)\n                    print('Finished raw processing batch')\n\n                # Wait until all threads have finished\n                q.join()\n                sleep(1)\n\n        except KeyboardInterrupt:\n            # Shut down threads cleanly\n            for i in range(num_workers):\n                q.put(None)\n            for t in threads:\n                t.join()\n\n    def handle(self, *args, **options):\n        self.run_processors()\n"""
photonix/photos/management/commands/raw_scheduler.py,0,"b""from time import sleep\n\nfrom django.core.management.base import BaseCommand\n\nfrom photonix.photos.models import Task\nfrom photonix.photos.utils.raw import ensure_raw_processing_tasks\n\n\nclass Command(BaseCommand):\n    help = 'Loads raw photos onto the raw file processing queues.'\n\n    def run_scheduler(self):\n        while True:\n            num_remaining = Task.objects.filter(type='ensure_raw_processed', status='P').count()\n            if num_remaining:\n                print('{} tasks remaining for raw process scheduling'.format(num_remaining))\n                ensure_raw_processing_tasks()\n                print('Finished raw process scheduling')\n            sleep(1)\n\n    def handle(self, *args, **options):\n        try:\n            self.run_scheduler()\n        except KeyboardInterrupt:\n            exit(0)\n"""
photonix/photos/management/commands/rescan_photos.py,0,"b""from django.conf import settings\nfrom django.core.management.base import BaseCommand\n\nfrom photonix.photos.utils.organise import import_photos_in_place\nfrom photonix.photos.utils.system import missing_system_dependencies\n# from web.utils import notify_ui\n\n\nclass Command(BaseCommand):\n    help = 'Creates relevant database records for all photos that are in a folder.'\n\n    def add_arguments(self, parser):\n        parser.add_argument('--paths', nargs='+', default=[item['PATH'] for item in settings.PHOTO_OUTPUT_DIRS])\n\n    def rescan_photos(self, paths):\n        missing = missing_system_dependencies(['exiftool', ])\n        if missing:\n            print('Missing dependencies: {}'.format(missing))\n            exit(1)\n\n        for path in paths:\n            import_photos_in_place(path)\n\n    def handle(self, *args, **options):\n        # notify_ui('photo_dirs_scanning', True)\n        self.rescan_photos(options['paths'])\n        # notify_ui('photo_dirs_scanning', False)\n"""
photonix/photos/management/commands/rescan_photos_periodically.py,0,"b""from time import sleep\nfrom django.conf import settings\nfrom django.core.management.base import BaseCommand\n\nfrom photonix.photos.utils.organise import import_photos_in_place\nfrom photonix.photos.utils.system import missing_system_dependencies\n\n\nclass Command(BaseCommand):\n    help = 'Creates relevant database records for all photos that are in a folder.'\n\n    def add_arguments(self, parser):\n        parser.add_argument('--paths', nargs='+', default=[item['PATH'] for item in settings.PHOTO_OUTPUT_DIRS])\n\n    def rescan_photos(self, paths):\n        missing = missing_system_dependencies(['exiftool', ])\n        if missing:\n            print('Missing dependencies: {}'.format(missing))\n            exit(1)\n\n        for path in paths:\n            import_photos_in_place(path)\n\n    def handle(self, *args, **options):\n        while True:\n            # TODO: Add a lock in here because DB corruption occurs if rescan_photos is called while it's still already running\n            self.rescan_photos(options['paths'])\n\n            sleep(60 * 60)  # Sleep for an hour\n"""
photonix/photos/management/commands/reset_redis_locks.py,0,"b""import os\n\nimport redis_lock\n\nfrom django.core.management.base import BaseCommand\nimport redis\n\n\nr = redis.Redis(host=os.environ.get('REDIS_HOST', '127.0.0.1'))\n\n\nclass Command(BaseCommand):\n    help = 'Removes all Redis locks - intended to be run on server start.'\n\n    def handle(self, *args, **options):\n        redis_lock.reset_all(r)\n"""
photonix/photos/management/commands/thumbnail_processor.py,0,"b""import queue\nimport threading\nfrom multiprocessing import cpu_count\nfrom time import sleep\n\nfrom django.core.management.base import BaseCommand\n\nfrom photonix.photos.models import Task\nfrom photonix.photos.utils.tasks import requeue_stuck_tasks\nfrom photonix.photos.utils.thumbnails import generate_thumbnails_for_photo\n\nq = queue.Queue()\n\n\ndef worker():\n    while True:\n        task = q.get()\n\n        if task is None:\n            break\n\n        generate_thumbnails_for_photo(task.subject_id, task)\n\n        q.task_done()\n\n\nclass Command(BaseCommand):\n    help = 'Processes full-sized photos into thumbnails of various sizes.'\n\n    def run_processors(self):\n        num_workers = cpu_count()\n        threads = []\n\n        print('Starting {} thumbnail processor workers\\n'.format(num_workers))\n\n        for i in range(num_workers):\n            t = threading.Thread(target=worker)\n            t.start()\n            threads.append(t)\n\n        try:\n            while True:\n                requeue_stuck_tasks('generate_thumbnails')\n\n                num_remaining = Task.objects.filter(type='generate_thumbnails', status='P').count()\n                if num_remaining:\n                    print('{} tasks remaining for thumbnail processing'.format(num_remaining))\n\n                # Load 'Pending' tasks onto worker threads\n                for task in Task.objects.filter(type='generate_thumbnails', status='P')[:64]:\n                    q.put(task)\n                    print('Finished thumbnail processing batch')\n\n                # Wait until all threads have finished\n                q.join()\n                sleep(1)\n\n        except KeyboardInterrupt:\n            # Shut down threads cleanly\n            for i in range(num_workers):\n                q.put(None)\n            for t in threads:\n                t.join()\n\n\n    def handle(self, *args, **options):\n        self.run_processors()\n"""
photonix/photos/management/commands/watch_photos.py,0,"b""import inotify.adapters\n\nfrom django.conf import settings\nfrom django.core.management.base import BaseCommand\n\nfrom photonix.photos.utils.db import record_photo\n\n\nclass Command(BaseCommand):\n    help = 'Watches photo directories and creates relevant database records for all photos that are added or modified.'\n\n    def add_arguments(self, parser):\n        parser.add_argument('--paths', nargs='+', default=[item['PATH'] for item in settings.PHOTO_OUTPUT_DIRS])\n\n    def watch_photos(self, paths):\n        for path in paths:\n            print(path)\n            # TODO: Work out how to watch multiple paths at once\n            i = inotify.adapters.InotifyTree(path)\n\n            for event in i.event_gen():\n                if event is not None:\n                    (header, type_names, watch_path, filename) = event\n                    # if set(type_names).intersection(['IN_CLOSE_WRITE', 'IN_DELETE', 'IN_MOVED_FROM', 'IN_MOVED_TO']):  # TODO: Make moving photos really efficient by using the 'from' path\n                    if set(type_names).intersection(['IN_CLOSE_WRITE', 'IN_DELETE', 'IN_MOVED_TO']):\n                        photo_path = '{}/{}'.format(watch_path.decode('utf-8'), filename.decode('utf-8'))\n                        record_photo(photo_path)\n\n    def handle(self, *args, **options):\n        try:\n            self.watch_photos(options['paths'])\n        except KeyboardInterrupt:\n            exit(0)\n"""
