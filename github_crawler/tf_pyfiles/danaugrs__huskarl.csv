file_path,api_count,code
setup.py,0,"b""from setuptools import setup\nfrom setuptools import find_packages\n\n\nsetup(\n\tname='huskarl',\n\tversion='0.4',\n\tdescription='Deep Reinforcement Learning Framework',\n\tauthor='Daniel Salvadori',\n\tauthor_email='danaugrs@gmail.com',\n\turl='https://github.com/danaugrs/huskarl',\n\tclassifiers=[\n        'Development Status :: 3 - Alpha',\n        'Intended Audience :: Developers',\n        'Topic :: Scientific/Engineering :: Artificial Intelligence',\n        'License :: OSI Approved :: MIT License',\n        'Programming Language :: Python :: 3',\n    ],\n\tpython_requires='>=3.6',\n\tinstall_requires=[\n\t\t'cloudpickle',\n\t\t'tensorflow==2.0.0',\n\t\t'scipy',\n\t],\n\tpackages=find_packages()\n)\n"""
examples/a2c-cartpole.py,0,"b""from tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport gym\n\nimport huskarl as hk\n\nif __name__ == '__main__':\n\n\t# Setup gym environment\n\tcreate_env = lambda: gym.make('CartPole-v0').unwrapped\n\tdummy_env = create_env()\n\n\t# Build a simple neural network with 3 fully connected layers as our model\n\tmodel = Sequential([\n\t\tDense(16, activation='relu', input_shape=dummy_env.observation_space.shape),\n\t\tDense(16, activation='relu'),\n\t\tDense(16, activation='relu'),\n\t])\n\n\t# We will be running multiple concurrent environment instances\n\tinstances = 16\n\n\t# Create a policy for each instance with a different distribution for epsilon\n\tpolicy = [hk.policy.Greedy()] + [hk.policy.GaussianEpsGreedy(eps, 0.1) for eps in np.arange(0, 1, 1/(instances-1))]\n\n\t# Create Advantage Actor-Critic agent\n\tagent = hk.agent.A2C(model, actions=dummy_env.action_space.n, nsteps=2, instances=instances, policy=policy)\n\n\tdef plot_rewards(episode_rewards, episode_steps, done=False):\n\t\tplt.clf()\n\t\tplt.xlabel('Step')\n\t\tplt.ylabel('Reward')\n\t\tfor i, (ed, steps) in enumerate(zip(episode_rewards, episode_steps)):\n\t\t\tplt.plot(steps, ed, alpha=0.5 if i == 0 else 0.2, linewidth=2 if i == 0 else 1)\n\t\tplt.show() if done else plt.pause(0.001) # Pause a bit so that the graph is updated\n\n\t# Create simulation, train and then test\n\tsim = hk.Simulation(create_env, agent)\n\tsim.train(max_steps=5000, instances=instances, plot=plot_rewards)\n\tsim.test(max_steps=1000)\n"""
examples/ddpg-pendulum.py,0,"b'from tensorflow.keras.models import Sequential, Model\nfrom tensorflow.keras.layers import Dense, Input, Concatenate\n\nimport matplotlib.pyplot as plt\nimport gym\n\nimport huskarl as hk\n\nif __name__ == ""__main__"":\n\n\t# Setup gym environment\n\tcreate_env = lambda: gym.make(\'Pendulum-v0\')\n\tdummy_env = create_env()\n\taction_size = dummy_env.action_space.shape[0]\n\tstate_shape = dummy_env.observation_space.shape\n\n\t# Build a simple actor model\n\tactor = Sequential([\n\t\tDense(16, activation=\'relu\', input_shape=state_shape),\n\t\tDense(16, activation=\'relu\'),\n\t\tDense(16, activation=\'relu\'),\n\t\tDense(action_size, activation=\'linear\')\n\t])\n\n\t# Build a simple critic model\n\taction_input = Input(shape=(action_size,), name=\'action_input\')\n\tstate_input = Input(shape=state_shape, name=\'state_input\')\n\tx = Concatenate()([action_input, state_input])\n\tx = Dense(32, activation=\'relu\')(x)\n\tx = Dense(32, activation=\'relu\')(x)\n\tx = Dense(32, activation=\'relu\')(x)\n\tx = Dense(1, activation=\'linear\')(x)\n\tcritic = Model(inputs=[action_input, state_input], outputs=x)\n\n\t# Create Deep Deterministic Policy Gradient agent\n\tagent = hk.agent.DDPG(actor=actor, critic=critic, nsteps=2)\n\n\tdef plot_rewards(episode_rewards, episode_steps, done=False):\n\t\tplt.clf()\n\t\tplt.xlabel(\'Step\')\n\t\tplt.ylabel(\'Reward\')\n\t\tfor ed, steps in zip(episode_rewards, episode_steps):\n\t\t\tplt.plot(steps, ed)\n\t\tplt.show() if done else plt.pause(0.001) # Pause a bit so that the graph is updated\n\n\t# Create simulation, train and then test\n\tsim = hk.Simulation(create_env, agent)\n\tsim.train(max_steps=30_000, visualize=True, plot=plot_rewards)\n\tsim.test(max_steps=5_000)\n'"
examples/dqn-cartpole.py,0,"b""from tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense\n\nimport matplotlib.pyplot as plt\nimport gym\n\nimport huskarl as hk\n\nif __name__ == '__main__':\n\n\t# Setup gym environment\n\tcreate_env = lambda: gym.make('CartPole-v0').unwrapped\n\tdummy_env = create_env()\n\n\t# Build a simple neural network with 3 fully connected layers as our model\n\tmodel = Sequential([\n\t\tDense(16, activation='relu', input_shape=dummy_env.observation_space.shape),\n\t\tDense(16, activation='relu'),\n\t\tDense(16, activation='relu'),\n\t])\n\n\t# Create Deep Q-Learning Network agent\n\tagent = hk.agent.DQN(model, actions=dummy_env.action_space.n, nsteps=2)\n\n\tdef plot_rewards(episode_rewards, episode_steps, done=False):\n\t\tplt.clf()\n\t\tplt.xlabel('Step')\n\t\tplt.ylabel('Reward')\n\t\tfor ed, steps in zip(episode_rewards, episode_steps):\n\t\t\tplt.plot(steps, ed)\n\t\tplt.show() if done else plt.pause(0.001) # Pause a bit so that the graph is updated\n\n\t# Create simulation, train and then test\n\tsim = hk.Simulation(create_env, agent)\n\tsim.train(max_steps=3000, visualize=True, plot=plot_rewards)\n\tsim.test(max_steps=1000)\n"""
huskarl/__init__.py,0,b'from huskarl.core import HkException\nfrom huskarl.simulation import Simulation\nimport huskarl.agent'
huskarl/core.py,0,"b'\nclass HkException(Exception):\n\t""""""Basic exception for errors raised by Huskarl.""""""\n\n\nclass Agent:\n\t""""""Abstract base class for all implemented agents.\n\n\tDo not use this abstract base class directly but instead use one of the concrete agents implemented.\n\n\tTo implement your own agent, you have to implement the following methods:\n\t""""""\n\tdef save(self, filename, overwrite=False):\n\t\t""""""Saves the model parameters to the specified file.""""""\n\t\traise NotImplementedError()\n\n\tdef act(self, state, instance=0):\n\t\t""""""Returns the action to be taken given a state.""""""\n\t\traise NotImplementedError()\n\n\tdef push(self, transition, instance=0):\n\t\t""""""Stores the transition in memory.""""""\n\t\traise NotImplementedError()\n\n\tdef train(self, step):\n\t\t""""""Trains the agent for one step.""""""\n\t\traise NotImplementedError()\n'"
huskarl/memory.py,0,"b'from collections import namedtuple, deque\nimport random\nimport numpy as np\n\n\nTransition = namedtuple(\'Transition\', [\'state\', \'action\', \'reward\', \'next_state\'])\n\n\nclass Memory:\n\t""""""Abstract base class for all implemented memories.\n\n\tDo not use this abstract base class directly but instead use one of the concrete memories implemented.\n\n\tA memory stores interaction sequences between an agent and one or multiple environments.\n\tTo implement your own memory, you have to implement the following methods:\n\t""""""\n\tdef put(self, *args, **kwargs):\n\t\traise NotImplementedError()\n\n\tdef get(self, *args, **kwargs):\n\t\traise NotImplementedError()\n\n\tdef __len__(self):\n\t\traise NotImplementedError()\n\n\ndef unpack(traces):\n\t""""""Returns states, actions, rewards, end_states, and a mask for episode boundaries given traces.""""""\n\tstates = [t[0].state for t in traces]\n\tactions = [t[0].action for t in traces]\n\trewards = [[e.reward for e in t] for t in traces]\n\tend_states = [t[-1].next_state for t in traces]\n\tnot_done_mask = [[1 if n.next_state is not None else 0 for n in t] for t in traces]\n\treturn states, actions, rewards, end_states, not_done_mask\n\n\nclass OnPolicy(Memory):\n\t""""""Stores multiple steps of interaction with multiple environments.""""""\n\tdef __init__(self, steps=1, instances=1):\n\t\tself.buffers = [[] for _ in range(instances)]\n\t\tself.steps = steps\n\t\tself.instances = instances\n\n\tdef put(self, transition, instance=0):\n\t\t""""""Stores transition into the appropriate buffer.""""""\n\t\tself.buffers[instance].append(transition)\n\n\tdef get(self):\n\t\t""""""Returns all traces and clears the memory.""""""\n\t\ttraces = [list(tb) for tb in self.buffers]\n\t\tself.buffers = [[] for _ in range(self.instances)]\n\t\treturn unpack(traces)\n\n\tdef __len__(self):\n\t\t""""""Returns the number of traces stored.""""""\n\t\treturn sum([len(b) - self.steps + 1 for b in self.buffers])\n\n\nclass ExperienceReplay:\n\t""""""Stores interaction with an environment as a double-ended queue of Transition instances.\n\t\n\tProvides efficient sampling of multistep traces.\n\tIf exclude_boundaries==True, then traces are sampled such that they don\'t include episode boundaries.\n\t""""""\n\tdef __init__(self, capacity, steps=1, exclude_boundaries=False):\n\t\t""""""\n\t\tArgs:\n\t\t\tcapacity (int): The maximum number of traces the memory should be able to store.\n\t\t\tsteps (int): The number of steps (transitions) each sampled trace should include.\n\t\t\texclude_boundaries (bool): If True, sampled traces will not include episode boundaries.\n\t\t""""""\n\t\tself.traces = deque(maxlen=capacity)\n\t\tself.buffer = [] # Rolling buffer of size at most self.steps\n\t\tself.capacity = capacity\n\t\tself.steps = steps\n\t\tself.exclude_boundaries = exclude_boundaries\n\n\tdef put(self, transition):\n\t\t""""""Adds transition to memory.""""""\n\t\t# Append transition to temporary rolling buffer\n\t\tself.buffer.append(transition)\n\t\t# If buffer doesn\'t yet contain a full trace - return\n\t\tif len(self.buffer) < self.steps: return\n\t\t# If self.traces not at max capacity, append new trace and priority (use highest existing priority if available)\n\t\tself.traces.append(tuple(self.buffer))\n\t\t# If excluding boundaries and we\'ve reached a boundary - clear the buffer\n\t\tif self.exclude_boundaries and transition.next_state is None:\n\t\t\tself.buffer = []\n\t\t\treturn\n\t\t# Roll buffer\n\t\tself.buffer = self.buffer[1:]\n\n\tdef get(self, batch_size):\n\t\t""""""Samples the specified number of traces uniformly from the buffer.""""""\n\t\t# Sample batch_size traces\n\t\ttraces = random.sample(self.traces, batch_size)\n\t\treturn unpack(traces)\n\n\tdef __len__(self):\n\t\t""""""Returns the number of traces stored.""""""\n\t\treturn len(self.traces)\n\n\nEPS = 1e-3 # Constant added to all priorities to prevent them from being zero\n\n\nclass PrioritizedExperienceReplay:\n\t""""""Stores prioritized interaction with an environment in a priority queue implemented via a heap.\n\n\tProvides efficient prioritized sampling of multistep traces.\n\tIf exclude_boundaries==True, then traces are sampled such that they don\'t include episode boundaries.\n\tFor more information see ""Prioritized Experience Replay"" (Schaul et al., 2016).\n\t""""""\n\tdef __init__(self, capacity, steps=1, exclude_boundaries=False, prob_alpha=0.6):\n\t\t""""""\n\t\tArgs:\n\t\t\tcapacity (int): The maximum number of traces the memory should be able to store.\n\t\t\tsteps (int): The number of steps (transitions) each sampled trace should include.\n\t\t\texclude_boundaries (bool): If True, sampled traces will not include episode boundaries.\n\t\t\tprob_alpha (float): Value between 0 and 1 that specifies how strongly priorities are taken into account.\n\t\t""""""\n\t\tself.traces = [] # Each element is a tuple containing self.steps transitions\n\t\tself.priorities = np.array([]) # Each element is the priority for the same-index trace in self.traces\n\t\tself.buffer = [] # Rolling buffer of size at most self.steps\n\t\tself.capacity = capacity\n\t\tself.steps = steps\n\t\tself.exclude_boundaries = exclude_boundaries\n\t\tself.prob_alpha = prob_alpha\n\t\tself.traces_idxs = [] # Temporary list that contains the indexes associated to the last retrieved traces\n\n\tdef put(self, transition):\n\t\t""""""Adds transition to memory.""""""\n\t\t# Append transition to temporary rolling buffer\n\t\tself.buffer.append(transition)\n\t\t# If buffer doesn\'t yet contain a full trace - return\n\t\tif len(self.buffer) < self.steps: return\n\t\t# If self.traces not at max capacity, append new trace and priority (use highest existing priority if available)\n\t\tif len(self.traces) < self.capacity:\n\t\t\tself.traces.append(tuple(self.buffer))\n\t\t\tself.priorities = np.append(self.priorities, EPS if self.priorities.size == 0 else self.priorities.max())\n\t\telse:\n\t\t\t# If self.traces at max capacity, substitute lowest priority trace and use highest existing priority\n\t\t\tidx = np.argmin(self.priorities)\n\t\t\tself.traces[idx] = tuple(self.buffer)\n\t\t\tself.priorities[idx] = self.priorities.max()\n\t\t# If excluding boundaries and we\'ve reached a boundary - clear the buffer\n\t\tif self.exclude_boundaries and transition.next_state is None:\n\t\t\tself.buffer = []\n\t\t\treturn\n\t\t# Roll buffer\n\t\tself.buffer = self.buffer[1:]\n\n\tdef get(self, batch_size):\n\t\t""""""Samples the specified number of traces from the buffer according to the prioritization and prob_alpha.""""""\n\t\t# Transform priorities into probabilities using self.prob_alpha\n\t\tprobs = self.priorities ** self.prob_alpha\n\t\tprobs /= probs.sum()\n\t\t# Sample batch_size traces according to probabilities and store indexes\n\t\tself.traces_idxs = np.random.choice(len(self.traces), batch_size, p=probs, replace=False)\n\t\ttraces = [self.traces[idx] for idx in self.traces_idxs]\n\t\treturn unpack(traces)\n\n\tdef last_traces_idxs(self):\n\t\t""""""Returns the indexes associated with the last retrieved traces.""""""\n\t\treturn self.traces_idxs.copy()\n\n\tdef update_priorities(self, traces_idxs, new_priorities):\n\t\t""""""Updates the priorities of the traces with specified indexes.""""""\n\t\tself.priorities[traces_idxs] = new_priorities + EPS\n\n\tdef __len__(self):\n\t\t""""""Returns the number of traces stored.""""""\n\t\treturn len(self.traces)\n'"
huskarl/policy.py,0,"b'import random\nimport numpy as np\nfrom scipy.stats import truncnorm\n\n\nclass Policy:\n\t""""""Abstract base class for all implemented policies.\n\t\n\tDo not use this abstract base class directly but instead use one of the concrete policies implemented.\n\n\tA policy ultimately returns the action to be taken based on the output of the agent.\n\tThe policy is the place to implement action-space exploration strategies.\n\tIf the action space is discrete, the policy usually receives action values and has to pick an action/index.\n\tA discrete action-space policy can explore by pick an action at random with a small probability e.g. EpsilonGreedy.\n\tIf the action space is continuous, the policy usually receives a single action or a distribution over actions.\n\tA continuous action-space policy can simply sample from the distribution and/or add noise to the received action.\t\n\t\n\tTo implement your own policy, you have to implement the following method:\n\t""""""\n\tdef act(self, **kwargs):\n\t\traise NotImplementedError()\n\n\n# Discrete action-space policies =======================================================================================\n\n\nclass Greedy(Policy):\n\t""""""Greedy Policy\n\n\tThis policy always picks the action with largest value.\n\t""""""\n\tdef act(self, qvals):\n\t\treturn np.argmax(qvals)\n\n\nclass EpsGreedy(Policy):\n\t""""""Epsilon-Greedy Policy\n\t\n\tThis policy picks the action with largest value with probability 1-epsilon.\n\tIt picks a random action and therefore explores with probability epsilon.\n\t""""""\n\tdef __init__(self, eps):\n\t\tself.eps = eps\n\n\tdef act(self, qvals):\n\t\tif random.random() > self.eps:\n\t\t\treturn np.argmax(qvals)\n\t\treturn random.randrange(len(qvals))\n\n\nclass GaussianEpsGreedy(Policy):\n\t""""""Gaussian Epsilon-Greedy Policy\n\n\tLike the Epsilon-Greedy Policy except it samples epsilon from a [0,1]-truncated Gaussian distribution.\n\tThis method is used in ""Asynchronous Methods for Deep Reinforcement Learning"" (Mnih et al., 2016).\n\t""""""\n\tdef __init__(self, eps_mean, eps_std):\n\t\tself.eps_mean = eps_mean\n\t\tself.eps_std = eps_std\n\t\n\tdef act(self, qvals):\n\t\teps = truncnorm.rvs((0 - self.eps_mean) / self.eps_std, (1 - self.eps_mean) / self.eps_std)\n\t\tif random.random() > eps:\n\t\t\treturn np.argmax(qvals)\n\t\treturn random.randrange(len(qvals))\n\n\n# Continuous action-space policies (noise generators) ==================================================================\n\n\nclass PassThrough(Policy):\n\t""""""Pass-Through Policy\n\n\tThis policy simply outputs the model\'s output, unchanged.\n\t""""""\n\tdef act(self, action):\n\t\treturn action\n\n'"
huskarl/simulation.py,0,"b'from collections import namedtuple\nimport multiprocessing as mp\n\nimport cloudpickle # For pickling lambda functions and more\n\nfrom huskarl.memory import Transition\nfrom huskarl.core import HkException\n\n\n# Packet used to transmit experience from environment subprocesses to main process\n# The first packet of every episode will have reward set to None\n# The last packet of every episode will have state set to None\nRewardState = namedtuple(\'RewardState\', [\'reward\', \'state\'])\n\n\nclass Simulation:\n\t""""""Simulates an agent interacting with one of multiple environments.""""""\n\tdef __init__(self, create_env, agent, mapping=None):\n\t\tself.create_env = create_env\n\t\tself.agent = agent\n\t\tself.mapping = mapping\n\n\tdef train(self, max_steps=100_000, instances=1, visualize=False, plot=None, max_subprocesses=0):\n\t\t""""""Trains the agent on the specified number of environment instances.""""""\n\t\tself.agent.training = True\n\t\tif max_subprocesses == 0:\n\t\t\t# Use single process implementation\n\t\t\tself._sp_train(max_steps, instances, visualize, plot)\n\t\telif max_subprocesses is None or max_subprocesses > 0:\n\t\t\t# Use multiprocess implementation\n\t\t\tself._mp_train(max_steps, instances, visualize, plot, max_subprocesses)\n\t\telse:\n\t\t\traise HkException(f""Invalid max_subprocesses setting: {max_subprocesses}"")\n\n\tdef _sp_train(self, max_steps, instances, visualize, plot):\n\t\t""""""Trains using a single process.""""""\n\t\t# Keep track of rewards per episode per instance\n\t\tepisode_reward_sequences = [[] for i in range(instances)]\n\t\tepisode_step_sequences = [[] for i in range(instances)]\n\t\tepisode_rewards = [0] * instances\n\n\t\t# Create and initialize environment instances\n\t\tenvs = [self.create_env() for i in range(instances)]\n\t\tstates = [env.reset() for env in envs]\n\n\t\tfor step in range(max_steps):\n\t\t\tfor i in range(instances):\n\t\t\t\tif visualize: envs[i].render()\n\t\t\t\taction = self.agent.act(states[i], i)\n\t\t\t\tnext_state, reward, done, _ = envs[i].step(action)\n\t\t\t\tself.agent.push(Transition(states[i], action, reward, None if done else next_state), i)\n\t\t\t\tepisode_rewards[i] += reward\n\t\t\t\tif done:\n\t\t\t\t\tepisode_reward_sequences[i].append(episode_rewards[i])\n\t\t\t\t\tepisode_step_sequences[i].append(step)\n\t\t\t\t\tepisode_rewards[i] = 0\n\t\t\t\t\tif plot: plot(episode_reward_sequences, episode_step_sequences)\n\t\t\t\t\tstates[i] = envs[i].reset()\n\t\t\t\telse:\n\t\t\t\t\tstates[i] = next_state\n\t\t\t# Perform one step of the optimization\n\t\t\tself.agent.train(step)\n\n\t\tif plot: plot(episode_reward_sequences, episode_step_sequences, done=True)\n\n\tdef _mp_train(self, max_steps, instances, visualize, plot, max_subprocesses):\n\t\t""""""Trains using multiple processes.\n\t\t\n\t\tUseful to parallelize the computation of heavy environments.\n\t\t""""""\n\t\t# Unless specified set the maximum number of processes to be the number of cores in the machine\n\t\tif max_subprocesses is None:\n\t\t\tmax_subprocesses = mp.cpu_count()\n\t\tnprocesses = min(instances, max_subprocesses)\n\n\t\t# Split instances into processes as homogeneously as possibly\n\t\tinstances_per_process = [instances//nprocesses] * nprocesses\n\t\tleftover = instances % nprocesses\n\t\tif leftover > 0:\n\t\t\tfor i in range(leftover):\n\t\t\t\tinstances_per_process[i] += 1\n\n\t\t# Create a unique id (index) for each instance, grouped by process\n\t\tinstance_ids = [list(range(i, instances, nprocesses))[:ipp] for i, ipp in enumerate(instances_per_process)]\n\n\t\t# Create processes and pipes (one pipe for each environment instance)\n\t\tpipes = []\n\t\tprocesses = []\n\t\tfor i in range(nprocesses):\n\t\t\tchild_pipes = []\n\t\t\tfor j in range(instances_per_process[i]):\n\t\t\t\tparent, child = mp.Pipe()\n\t\t\t\tpipes.append(parent)\n\t\t\t\tchild_pipes.append(child)\n\t\t\tpargs = (cloudpickle.dumps(self.create_env), instance_ids[i], max_steps, child_pipes, visualize)\n\t\t\tprocesses.append(mp.Process(target=_train, args=pargs))\n\n\t\t# Start all processes\n\t\tprint(f""Starting {nprocesses} process(es) for {instances} environment instance(s)... {instance_ids}"")\n\t\tfor p in processes: p.start()\n\n\t\t# Keep track of rewards per episode per instance\n\t\tepisode_reward_sequences = [[] for i in range(instances)]\n\t\tepisode_step_sequences = [[] for i in range(instances)]\n\t\tepisode_rewards = [0] * instances\t\n\n\t\t# Temporarily record RewardState instances received from each subprocess\n\t\t# Each Transition instance requires two RewardState instances to be created\n\t\trss = [None] * instances\n\n\t\t# Keep track of last actions sent to subprocesses\n\t\tlast_actions = [None] * instances\n\n\t\tfor step in range(max_steps):\n\t\t\t\n\t\t\t# Keep track from which environments we have already constructed a full Transition instance\n\t\t\t# and sent it to agent. This is to synchronize steps.\n\t\t\tstep_done = [False] * instances\t\t\t\n\n\t\t\twhile sum(step_done) < instances: # Steps across environments are synchronized\n\n\t\t\t\t# Within each step, Transitions are received and processed on a first-come first-served basis\n\t\t\t\tawaiting_pipes = [p for iid, p in enumerate(pipes) if step_done[iid] == 0]\n\t\t\t\tready_pipes = mp.connection.wait(awaiting_pipes, timeout=None)\n\t\t\t\tpipe_indexes = [pipes.index(rp) for rp in ready_pipes]\n\n\t\t\t\t# Do a round-robin over processes to best divide computation\n\t\t\t\tpipe_indexes.sort()\n\t\t\t\tfor iid in pipe_indexes:\n\t\t\t\t\trs = pipes[iid].recv() # Receive a RewardState\n\n\t\t\t\t\t# If we already had a RewardState for this environment then we are able to create and push a Transition\n\t\t\t\t\tif rss[iid] is not None:\n\t\t\t\t\t\texp = Transition(rss[iid].state, last_actions[iid], rs.reward, rs.state)\n\t\t\t\t\t\tself.agent.push(exp, iid)\n\t\t\t\t\t\tstep_done[iid] = True\n\t\t\t\t\trss[iid] = rs\n\n\t\t\t\t\t# Check if episode is done\n\t\t\t\t\tif rs.state is None:\n\t\t\t\t\t\t# Episode is done - store rewards and update plot\n\t\t\t\t\t\trss[iid] = None\n\t\t\t\t\t\tepisode_reward_sequences[iid].append(episode_rewards[iid])\n\t\t\t\t\t\tepisode_step_sequences[iid].append(step)\n\t\t\t\t\t\tepisode_rewards[iid] = 0\n\t\t\t\t\t\tif plot: plot(episode_reward_sequences, episode_step_sequences)\n\t\t\t\t\telse:\n\t\t\t\t\t\t# Episode is NOT done - act according to state and send action to the subprocess\n\t\t\t\t\t\taction = self.agent.act(rs.state, iid)\n\t\t\t\t\t\tlast_actions[iid] = action\n\t\t\t\t\t\ttry:\n\t\t\t\t\t\t\tpipes[iid].send(action)\n\t\t\t\t\t\t# Disregard BrokenPipeError on last step\n\t\t\t\t\t\texcept BrokenPipeError as bpe:\n\t\t\t\t\t\t\tif step < (max_steps - 1): raise bpe\n\t\t\t\t\t\tif rs.reward: episode_rewards[iid] += rs.reward\n\n\t\t\t# Train the agent at the end of every synchronized step\n\t\t\tself.agent.train(step)\n\n\t\tif plot: plot(episode_reward_sequences, episode_step_sequences, done=True)\n\n\tdef test(self, max_steps, visualize=True):\n\t\t""""""Test the agent on the environment.""""""\n\t\tself.agent.training = False\n\n\t\t# Create and initialize environment\n\t\tenv = self.create_env()\n\t\tstate = env.reset()\n\n\t\tfor step in range(max_steps):\n\t\t\tif visualize: env.render()\n\t\t\taction = self.agent.act(state)\n\t\t\tnext_state, reward, done, _ = env.step(action)\n\t\t\tstate = env.reset() if done else next_state\n\n\ndef _train(create_env, instance_ids, max_steps, pipes, visualize):\n\t""""""This function is to be executed in a subprocess.""""""\n\tpipes = {iid: p for iid, p in zip(instance_ids, pipes)}\n\tactions = {iid: None for iid in instance_ids} # Reused dictionary of actions\n\n\t# Initialize environments and send initial state to agent in parent process\n\tcreate_env = cloudpickle.loads(create_env)\n\tenvs = {iid: create_env() for iid in instance_ids}\n\tfor iid in instance_ids:\n\t\tstate = envs[iid].reset()\n\t\tpipes[iid].send(RewardState(None, state))\n\n\t# Run for the specified number of steps\n\tfor step in range(max_steps):\n\t\tfor iid in instance_ids:\n\t\t\t# Get action from agent in main process via pipe\n\t\t\tactions[iid] = pipes[iid].recv()\n\t\t\tif visualize: envs[iid].render()\n\n\t\t\t# Step environment and send experience to agent in main process via pipe\n\t\t\tnext_state, reward, done, _ = envs[iid].step(actions[iid])\n\t\t\tpipes[iid].send(RewardState(reward, None if done else next_state))\n\n\t\t\t# If episode is over reset the environment and transmit initial state to agent\n\t\t\tif done:\n\t\t\t\tstate = envs[iid].reset()\n\t\t\t\tpipes[iid].send(RewardState(None, state))\n'"
huskarl/agent/__init__.py,0,b'from huskarl.agent.dqn import DQN\nfrom huskarl.agent.a2c import A2C\nfrom huskarl.agent.ddpg import DDPG'
huskarl/agent/a2c.py,4,"b'from tensorflow.keras.layers import Dense, Concatenate\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.optimizers import Adam\nimport tensorflow as tf\nimport numpy as np\n\nfrom huskarl.policy import Greedy, GaussianEpsGreedy\nfrom huskarl.core import Agent\nfrom huskarl import memory\n\n\nclass A2C(Agent):\n\t""""""Advantage Actor-Critic (A2C)\n\n\tA2C is a synchronous version of A3C which gives equal or better performance.\n\tFor more information on A2C refer to the OpenAI blog post: https://blog.openai.com/baselines-acktr-a2c/.\n\tThe A3C algorithm is described in ""Asynchronous Methods for Deep Reinforcement Learning"" (Mnih et al., 2016)\n\n\tSince this algorithm is on-policy, it can and should be trained with multiple simultaneous environment instances.\n\tThe parallelism decorrelates the agents\' data into a more stationary process which aids learning.\n\t""""""\n\tdef __init__(self, model, actions, optimizer=None, policy=None, test_policy=None,\n\t\t         gamma=0.99, instances=8, nsteps=1, value_loss=0.5, entropy_loss=0.01):\n\t\t""""""\n\t\tTODO: Describe parameters\n\t\t""""""\n\t\tself.actions = actions\n\t\tself.optimizer = Adam(lr=3e-3) if optimizer is None else optimizer\n\t\tself.memory = memory.OnPolicy(steps=nsteps, instances=instances)\n\n\t\tif policy is None:\n\t\t\t# Create one policy per instance, with varying exploration parameters\n\t\t\tself.policy = [Greedy()] + [GaussianEpsGreedy(eps, 0.1) for eps in np.arange(0, 1, 1/(instances-1))]\n\t\telse:\n\t\t\tself.policy = policy\n\t\tself.test_policy = Greedy() if test_policy is None else test_policy\n\n\t\tself.gamma = gamma\n\t\tself.instances = instances\n\t\tself.nsteps = nsteps\n\t\tself.value_loss = value_loss\n\t\tself.entropy_loss = entropy_loss\n\t\tself.training = True\n\n\t\t# Create output model layers based on number of actions\n\t\traw_output = model.layers[-1].output\n\t\tactor = Dense(actions, activation=\'softmax\')(raw_output) # Actor (Policy Network)\n\t\tcritic = Dense(1, activation=\'linear\')(raw_output) # Critic (Value Network)\n\t\toutput_layer = Concatenate()([actor, critic])\n\t\tself.model = Model(inputs=model.input, outputs=output_layer)\n\n\t\tdef a2c_loss(targets_actions, y_pred):\n\t\t\t# Unpack input\n\t\t\ttargets, actions = targets_actions[:,0], targets_actions[:,1:] # Unpack\n\t\t\tprobs, values = y_pred[:,:-1], y_pred[:,-1]\n\t\t\t# Compute advantages and logprobabilities\n\t\t\tadv = targets - values\n\t\t\tlogprob = tf.math.log(tf.reduce_sum(probs*actions, axis=1, keepdims=False) + 1e-10)\n\t\t\t# Compute composite loss\n\t\t\tloss_policy = -adv * logprob\n\t\t\tloss_value = self.value_loss * tf.square(adv)\n\t\t\tentropy = self.entropy_loss * tf.reduce_sum(probs * tf.math.log(probs + 1e-10), axis=1, keepdims=False)\n\t\t\treturn tf.reduce_mean(loss_policy + loss_value + entropy)\n\n\t\tself.model.compile(optimizer=self.optimizer, loss=a2c_loss)\n\n\tdef save(self, filename, overwrite=False):\n\t\t""""""Saves the model parameters to the specified file.""""""\n\t\tself.model.save_weights(filename, overwrite=overwrite)\n\n\tdef act(self, state, instance=0):\n\t\t""""""Returns the action to be taken given a state.""""""\n\t\tqvals = self.model.predict(np.array([state]))[0][:-1]\n\t\tif self.training:\n\t\t\treturn self.policy[instance].act(qvals) if isinstance(self.policy, list) else self.policy.act(qvals)\n\t\telse:\n\t\t\treturn self.test_policy[instance].act(qvals) if isinstance(self.test_policy, list) else self.test_policy.act(qvals)\n\n\tdef push(self, transition, instance=0):\n\t\t""""""Stores the transition in memory.""""""\n\t\tself.memory.put(transition, instance)\n\n\tdef train(self, step):\n\t\t""""""Trains the agent for one step.""""""\n\t\tif len(self.memory) < self.instances:\n\t\t\treturn\n\n\t\tstate_batch, action_batch, reward_batches, end_state_batch, not_done_mask = self.memory.get()\n\n\t\t# Compute the value of the last next states\n\t\ttarget_qvals = np.zeros(self.instances)\n\t\tnon_final_last_next_states = [es for es in end_state_batch if es is not None]\n\t\tif len(non_final_last_next_states) > 0:\n\t\t\tnon_final_mask = list(map(lambda s: s is not None, end_state_batch))\n\t\t\ttarget_qvals[non_final_mask] = self.model.predict_on_batch(np.array(non_final_last_next_states))[:,-1].squeeze()\n\n\t\t# Compute n-step discounted return\n\t\t# If episode ended within any sampled nstep trace - zero out remaining rewards\n\t\tfor n in reversed(range(self.nsteps)):\n\t\t\trewards = np.array([b[n] for b in reward_batches])\n\t\t\ttarget_qvals *= np.array([t[n] for t in not_done_mask])\n\t\t\ttarget_qvals = rewards + (self.gamma * target_qvals)\n\n\t\t# Prepare loss data: target Q-values and actions taken (as a mask)\n\t\tran = np.arange(self.instances)\n\t\ttargets_actions = np.zeros((self.instances, self.actions+1))\n\t\ttargets_actions[ran, 0] = target_qvals\n\t\ttargets_actions[ran, np.array(action_batch)+1] = 1\n\n\t\tself.model.train_on_batch(np.array(state_batch), targets_actions)\n'"
huskarl/agent/ddpg.py,6,"b'from tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.losses import MSE\nimport tensorflow as tf\nimport numpy as np\n\nfrom huskarl.policy import PassThrough\nfrom huskarl.core import Agent\nfrom huskarl import memory\n\n\nclass DDPG(Agent):\n\t""""""Deep Deterministic Policy Gradient\n\n\t""Continuous control with deep reinforcement learning"" (Lillicrap et al., 2015)\n\t""""""\n\tdef __init__(self, actor=None, critic=None, optimizer_critic=None, optimizer_actor=None,\n\t\t\t\t policy=None, test_policy=None, memsize=10_000, target_update=1e-3,\n\t\t\t\t gamma=0.99, batch_size=32, nsteps=1):\n\t\t""""""\n\t\tTODO: Describe parameters\n\t\t""""""\n\t\tself.actor = actor\n\t\tself.critic = critic\n\n\t\tself.optimizer_actor = Adam(lr=5e-3) if optimizer_actor is None else optimizer_actor\n\t\tself.optimizer_critic = Adam(lr=5e-3) if optimizer_critic is None else optimizer_critic\n\n\t\tself.policy = PassThrough() if policy is None else policy\n\t\tself.test_policy = PassThrough() if test_policy is None else test_policy\n\n\t\tself.memsize = memsize\n\t\tself.memory = memory.PrioritizedExperienceReplay(memsize, nsteps, prob_alpha=0.2)\n\n\t\tself.target_update = target_update\n\t\tself.gamma = gamma\n\t\tself.batch_size = batch_size\n\t\tself.nsteps = nsteps\n\t\tself.training = True\n\n\t\t# Clone models to use for delayed Q targets\n\t\tself.target_actor = tf.keras.models.clone_model(self.actor)\n\t\tself.target_critic = tf.keras.models.clone_model(self.critic)\n\n\t\t# Define loss function that computes the MSE between target Q-values and cumulative discounted rewards\n\t\t# If using PrioritizedExperienceReplay, the loss function also computes the TD error and updates the trace priorities\n\t\tdef q_loss(data, qvals):\n\t\t\t""""""Computes the MSE between the Q-values of the actions that were taken and\tthe cumulative discounted\n\t\t\trewards obtained after taking those actions. Updates trace priorities if using PrioritizedExperienceReplay.\n\t\t\t""""""\n\t\t\ttarget_qvals = data[:, 0, np.newaxis]\n\t\t\tif isinstance(self.memory, memory.PrioritizedExperienceReplay):\n\t\t\t\tdef update_priorities(_qvals, _target_qvals, _traces_idxs):\n\t\t\t\t\t""""""Computes the TD error and updates memory priorities.""""""\n\t\t\t\t\ttd_error = np.abs((_target_qvals - _qvals).numpy())[:, 0]\n\t\t\t\t\t_traces_idxs = (tf.cast(_traces_idxs, tf.int32)).numpy()\n\t\t\t\t\tself.memory.update_priorities(_traces_idxs, td_error)\n\t\t\t\t\treturn _qvals\n\t\t\t\tqvals = tf.py_function(func=update_priorities, inp=[qvals, target_qvals, data[:, 1]], Tout=tf.float32)\n\t\t\treturn MSE(target_qvals, qvals)\n\n\t\tself.critic.compile(optimizer=self.optimizer_critic, loss=q_loss)\n\n\t\t# To train the actor we want to maximize the critic\'s output (action value) given the predicted action as input\n\t\t# Namely we want to change the actor\'s weights such that it picks the action that has the highest possible value\n\t\tstate_input = self.critic.input[1]\n\t\tcritic_output = self.critic([self.actor(state_input), state_input])\n\t\tactor_loss = -tf.keras.backend.mean(critic_output)\n\t\tactor_updates = self.optimizer_actor.get_updates(params=self.actor.trainable_weights, loss=actor_loss)\n\t\tself.actor_train_on_batch = tf.keras.backend.function(inputs=[state_input], outputs=[self.actor(state_input)], updates=actor_updates)\n\n\tdef save(self, filename, overwrite=False):\n\t\t""""""Saves the model parameters to the specified file(s).""""""\n\t\tself.actor.save_weights(filename+""_actor"", overwrite=overwrite)\n\t\tself.critic.save_weights(filename+""_critic"", overwrite=overwrite)\n\n\tdef act(self, state, instance=0):\n\t\t""""""Returns the action to be taken given a state.""""""\n\t\taction = self.actor.predict(np.array([state]))[0]\n\t\treturn self.policy.act(action) if self.training else self.test_policy.act(action)\n\n\tdef push(self, transition, instance=0):\n\t\t""""""Stores the transition in memory.""""""\n\t\tself.memory.put(transition)\n\n\tdef train(self, step):\n\t\t""""""Trains the agent for one step.""""""\n\t\tif len(self.memory) == 0:\n\t\t\treturn\n\n\t\t# Update target network\n\t\tif self.target_update >= 1 and step % self.target_update == 0:\n\t\t\t# Perform a hard update\n\t\t\tself.target_actor.set_weights(self.actor.get_weights())\n\t\t\tself.target_critic.set_weights(self.critic.get_weights())\n\t\telif self.target_update < 1:\n\t\t\t# Perform a soft update\n\t\t\ta_w = np.array(self.actor.get_weights())\n\t\t\tta_w = np.array(self.target_actor.get_weights())\n\t\t\tself.target_actor.set_weights(self.target_update*a_w + (1-self.target_update)*ta_w)\n\t\t\tc_w = np.array(self.critic.get_weights())\n\t\t\ttc_w = np.array(self.target_critic.get_weights())\n\t\t\tself.target_critic.set_weights(self.target_update*c_w + (1-self.target_update)*tc_w)\n\n\t\t# Train even when memory has fewer than the specified batch_size\n\t\tbatch_size = min(len(self.memory), self.batch_size)\n\n\t\t# Sample batch_size traces from memory\n\t\tstate_batch, action_batch, reward_batches, end_state_batch, not_done_mask = self.memory.get(batch_size)\n\n\t\t# Compute the value of the last next states\n\t\ttarget_qvals = np.zeros(batch_size)\n\t\tnon_final_last_next_states = [es for es in end_state_batch if es is not None]\n\t\tif len(non_final_last_next_states) > 0:\n\t\t\tnon_final_mask = list(map(lambda s: s is not None, end_state_batch))\n\t\t\ttarget_actions = self.target_actor.predict_on_batch(np.array(non_final_last_next_states))\n\t\t\ttarget_qvals[non_final_mask] = self.target_critic.predict_on_batch([target_actions, np.array(non_final_last_next_states)]).squeeze()\n\n\t\t# Compute n-step discounted return\n\t\t# If episode ended within any sampled nstep trace - zero out remaining rewards\n\t\tfor n in reversed(range(self.nsteps)):\n\t\t\trewards = np.array([b[n] for b in reward_batches])\n\t\t\ttarget_qvals *= np.array([t[n] for t in not_done_mask])\n\t\t\ttarget_qvals = rewards + (self.gamma * target_qvals)\n\n\t\t# Train actor\n\t\tself.actor_train_on_batch([np.array(state_batch)])\n\n\t\t# Train critic\n\t\tPER = isinstance(self.memory, memory.PrioritizedExperienceReplay)\n\t\tcritic_loss_data = np.stack([target_qvals, self.memory.last_traces_idxs()], axis=1) if PER else target_qvals\n\t\tself.critic.train_on_batch([np.array(action_batch), np.array(state_batch)], critic_loss_data)\n'"
huskarl/agent/dqn.py,10,"b'from tensorflow.keras.layers import Dense, Lambda\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.optimizers import Adam\nimport tensorflow as tf\nimport numpy as np\n\nfrom huskarl.policy import EpsGreedy, Greedy\nfrom huskarl.core import Agent, HkException\nfrom huskarl import memory\n\n\nclass DQN(Agent):\n\t""""""Deep Q-Learning Network\n\n\tBase implementation:\n\t\t""Playing Atari with Deep Reinforcement Learning"" (Mnih et al., 2013)\n\n\tExtensions:\n\t\tMulti-step returns: ""Reinforcement Learning: An Introduction"" 2nd ed. (Sutton & Barto, 2018)\n\t\tDouble Q-Learning: ""Deep Reinforcement Learning with Double Q-learning"" (van Hasselt et al., 2015)\n\t\tDueling Q-Network: ""Dueling Network Architectures for Deep Reinforcement Learning"" (Wang et al., 2016)\n\t""""""\n\tdef __init__(self, model, actions, optimizer=None, policy=None, test_policy=None,\n\t\t\t\t memsize=10_000, target_update=10, gamma=0.99, batch_size=64, nsteps=1,\n\t\t\t\t enable_double_dqn=True, enable_dueling_network=False, dueling_type=\'avg\'):\n\t\t""""""\n\t\tTODO: Describe parameters\n\t\t""""""\n\t\tself.actions = actions\n\t\tself.optimizer = Adam(lr=3e-3) if optimizer is None else optimizer\n\n\t\tself.policy = EpsGreedy(0.1) if policy is None else policy\n\t\tself.test_policy = Greedy() if test_policy is None else test_policy\n\n\t\tself.memsize = memsize\n\t\tself.memory = memory.PrioritizedExperienceReplay(memsize, nsteps)\n\n\t\tself.target_update = target_update\n\t\tself.gamma = gamma\n\t\tself.batch_size = batch_size\n\t\tself.nsteps = nsteps\n\t\tself.training = True\n\n\t\t# Extension options\n\t\tself.enable_double_dqn = enable_double_dqn\n\t\tself.enable_dueling_network = enable_dueling_network\n\t\tself.dueling_type = dueling_type\n\n\t\t# Create output layer based on number of actions and (optionally) a dueling architecture\n\t\traw_output = model.layers[-1].output\n\t\tif self.enable_dueling_network:\n\t\t\t# ""Dueling Network Architectures for Deep Reinforcement Learning"" (Wang et al., 2016)\n\t\t\t# Output the state value (V) and the action-specific advantages (A) separately then compute the Q values: Q = A + V\n\t\t\tdueling_layer = Dense(self.actions + 1, activation=\'linear\')(raw_output)\n\t\t\tif   self.dueling_type == \'avg\':   f = lambda a: tf.expand_dims(a[:,0], -1) + a[:,1:] - tf.reduce_mean(a[:,1:], axis=1, keepdims=True)\n\t\t\telif self.dueling_type == \'max\':   f = lambda a: tf.expand_dims(a[:,0], -1) + a[:,1:] - tf.reduce_max(a[:,1:], axis=1, keepdims=True)\n\t\t\telif self.dueling_type == \'naive\': f = lambda a: tf.expand_dims(a[:,0], -1) + a[:,1:]\n\t\t\telse: raise HkException(""dueling_type must be one of {\'avg\',\'max\',\'naive\'}"")\n\t\t\toutput_layer = Lambda(f, output_shape=(self.actions,))(dueling_layer)\n\t\telse:\n\t\t\toutput_layer = Dense(self.actions, activation=\'linear\')(raw_output)\n\n\t\tself.model = Model(inputs=model.input, outputs=output_layer)\n\n\t\t# Define loss function that computes the MSE between target Q-values and cumulative discounted rewards\n\t\t# If using PrioritizedExperienceReplay, the loss function also computes the TD error and updates the trace priorities\n\t\tdef masked_q_loss(data, y_pred):\n\t\t\t""""""Computes the MSE between the Q-values of the actions that were taken and\tthe cumulative discounted\n\t\t\trewards obtained after taking those actions. Updates trace priorities if using PrioritizedExperienceReplay.\n\t\t\t""""""\n\t\t\taction_batch, target_qvals = data[:, 0], data[:, 1]\n\t\t\tseq = tf.cast(tf.range(0, tf.shape(action_batch)[0]), tf.int32)\n\t\t\taction_idxs = tf.transpose(tf.stack([seq, tf.cast(action_batch, tf.int32)]))\n\t\t\tqvals = tf.gather_nd(y_pred, action_idxs)\n\t\t\tif isinstance(self.memory, memory.PrioritizedExperienceReplay):\n\t\t\t\tdef update_priorities(_qvals, _target_qvals, _traces_idxs):\n\t\t\t\t\t""""""Computes the TD error and updates memory priorities.""""""\n\t\t\t\t\ttd_error = np.abs((_target_qvals - _qvals).numpy())\n\t\t\t\t\t_traces_idxs = (tf.cast(_traces_idxs, tf.int32)).numpy()\n\t\t\t\t\tself.memory.update_priorities(_traces_idxs, td_error)\n\t\t\t\t\treturn _qvals\n\t\t\t\tqvals = tf.py_function(func=update_priorities, inp=[qvals, target_qvals, data[:,2]], Tout=tf.float32)\n\t\t\treturn tf.keras.losses.mse(qvals, target_qvals)\n\n\t\tself.model.compile(optimizer=self.optimizer, loss=masked_q_loss)\n\n\t\t# Clone model to use for delayed Q targets\n\t\tself.target_model = tf.keras.models.clone_model(self.model)\n\t\tself.target_model.set_weights(self.model.get_weights())\n\n\tdef save(self, filename, overwrite=False):\n\t\t""""""Saves the model parameters to the specified file.""""""\n\t\tself.model.save_weights(filename, overwrite=overwrite)\n\n\tdef act(self, state, instance=0):\n\t\t""""""Returns the action to be taken given a state.""""""\n\t\tqvals = self.model.predict(np.array([state]))[0]\n\t\treturn self.policy.act(qvals) if self.training else self.test_policy.act(qvals)\n\n\tdef push(self, transition, instance=0):\n\t\t""""""Stores the transition in memory.""""""\n\t\tself.memory.put(transition)\n\n\tdef train(self, step):\n\t\t""""""Trains the agent for one step.""""""\n\t\tif len(self.memory) == 0:\n\t\t\treturn\n\n\t\t# Update target network\n\t\tif self.target_update >= 1 and step % self.target_update == 0:\n\t\t\t# Perform a hard update\n\t\t\tself.target_model.set_weights(self.model.get_weights())\n\t\telif self.target_update < 1:\n\t\t\t# Perform a soft update\n\t\t\tmw = np.array(self.model.get_weights())\n\t\t\ttmw = np.array(self.target_model.get_weights())\n\t\t\tself.target_model.set_weights(self.target_update * mw + (1 - self.target_update) * tmw)\n\n\t\t# Train even when memory has fewer than the specified batch_size\n\t\tbatch_size = min(len(self.memory), self.batch_size)\n\n\t\t# Sample batch_size traces from memory\n\t\tstate_batch, action_batch, reward_batches, end_state_batch, not_done_mask = self.memory.get(batch_size)\n\n\t\t# Compute the value of the last next states\n\t\ttarget_qvals = np.zeros(batch_size)\n\t\tnon_final_last_next_states = [es for es in end_state_batch if es is not None]\n\n\t\tif len(non_final_last_next_states) > 0:\t\t\n\t\t\tif self.enable_double_dqn:\n\t\t\t\t# ""Deep Reinforcement Learning with Double Q-learning"" (van Hasselt et al., 2015)\n\t\t\t\t# The online network predicts the actions while the target network is used to estimate the Q-values\n\t\t\t\tq_values = self.model.predict_on_batch(np.array(non_final_last_next_states))\n\t\t\t\tactions = np.argmax(q_values, axis=1)\n\t\t\t\t# Estimate Q-values using the target network but select the values with the\n\t\t\t\t# highest Q-value wrt to the online model (as computed above).\n\t\t\t\ttarget_q_values = self.target_model.predict_on_batch(np.array(non_final_last_next_states))\n\t\t\t\tselected_target_q_vals = target_q_values[range(len(target_q_values)), actions]\n\t\t\telse:\n\t\t\t\t# Use delayed target network to compute target Q-values\n\t\t\t\tselected_target_q_vals = self.target_model.predict_on_batch(np.array(non_final_last_next_states)).max(1)\n\t\t\tnon_final_mask = list(map(lambda s: s is not None, end_state_batch))\n\t\t\ttarget_qvals[non_final_mask] = selected_target_q_vals\n\n\t\t# Compute n-step discounted return\n\t\t# If episode ended within any sampled nstep trace - zero out remaining rewards\n\t\tfor n in reversed(range(self.nsteps)):\n\t\t\trewards = np.array([b[n] for b in reward_batches])\n\t\t\ttarget_qvals *= np.array([t[n] for t in not_done_mask])\n\t\t\ttarget_qvals = rewards + (self.gamma * target_qvals)\n\n\t\t# Compile information needed by the custom loss function\n\t\tloss_data = [action_batch, target_qvals]\n\n\t\t# If using PrioritizedExperienceReplay then we need to provide the trace indexes\n\t\t# to the loss function as well so we can update the priorities of the traces\n\t\tif isinstance(self.memory, memory.PrioritizedExperienceReplay):\n\t\t\tloss_data.append(self.memory.last_traces_idxs())\n\n\t\t# Train model\n\t\tself.model.train_on_batch(np.array(state_batch), np.stack(loss_data).transpose())\n'"
