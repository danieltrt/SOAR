file_path,api_count,code
setup.py,0,"b'from setuptools import setup, find_packages\n\nwith open(""README.md"", ""r"") as fh:\n    long_description = fh.read()\n\nsetup(\n    name=\'spektral\',\n    version=\'0.5.1\',\n    packages=find_packages(),\n    install_requires=[\'tensorflow>=2.1.0\',\n                      \'networkx\',\n                      \'pandas\',\n                      \'lxml\',\n                      \'joblib\',\n                      \'numpy\',\n                      \'scipy\',\n                      \'requests\',\n                      \'scikit-learn\'],\n    url=\'https://github.com/danielegrattarola/spektral\',\n    license=\'MIT\',\n    author=\'Daniele Grattarola\',\n    author_email=\'daniele.grattarola@gmail.com\',\n    description=\'Graph Neural Networks with Keras and Tensorflow 2.\',\n    long_description=long_description,\n    long_description_content_type=""text/markdown"",\n    classifiers=[\n        ""Programming Language :: Python :: 3.5""\n    ],\n)\n'"
docs/autogen.py,0,"b'from __future__ import print_function\nfrom __future__ import unicode_literals\n\nimport glob\nimport inspect\nimport os\nimport re\nimport shutil\nimport sys\n\nfrom spektral import chem\nfrom spektral import datasets\nfrom spektral import layers\nfrom spektral import utils\n\ntry:\n    reload(sys)\n    sys.setdefaultencoding(\'utf8\')\nexcept NameError:\n    pass\n\n\nEXCLUDE = {}\n\n# For each class to document, it is possible to:\n# 1) Document only the class: [classA, classB, ...]\n# 2) Document all its methods: [classA, (classB, ""*"")]\n# 3) Choose which methods to document (methods listed as strings):\n# [classA, (classB, [""method1"", ""method2"", ...]), ...]\n# 4) Choose which methods to document (methods listed as qualified names):\n# [classA, (classB, [module.classB.method1, module.classB.method2, ...]), ...]\n\nPAGES = [\n    # Layers ###################################################################\n    {\n        \'page\': \'layers/convolution.md\',\n        \'classes\': [\n            layers.GraphConv,\n            layers.ChebConv,\n            layers.GraphSageConv,\n            layers.ARMAConv,\n            layers.EdgeConditionedConv,\n            layers.GraphAttention,\n            layers.GraphConvSkip,\n            layers.APPNP,\n            layers.GINConv,\n            layers.DiffusionConv,\n            layers.GatedGraphConv,\n            layers.AGNNConv,\n            layers.TAGConv,\n            layers.CrystalConv,\n            layers.EdgeConv,\n            layers.MessagePassing,\n        ]\n    },\n    {\n        \'page\': \'layers/pooling.md\',\n        \'functions\': [],\n        \'methods\': [],\n        \'classes\': [\n            layers.DiffPool,\n            layers.MinCutPool,\n            layers.TopKPool,\n            layers.SAGPool,\n            layers.GlobalSumPool,\n            layers.GlobalAvgPool,\n            layers.GlobalMaxPool,\n            layers.GlobalAttentionPool,\n            layers.GlobalAttnSumPool,\n            layers.SortPool\n        ]\n    },\n    {\n        \'page\': \'layers/base.md\',\n        \'functions\': [],\n        \'methods\': [],\n        \'classes\': [\n            layers.InnerProduct,\n            layers.MinkowskiProduct\n        ]\n    },\n    # Datasets #################################################################\n    {\n        \'page\': \'datasets.md\',\n        \'functions\': [\n            datasets.citation.load_data\n        ],\n        \'methods\': [],\n        \'classes\': []\n    },\n    {\n        \'page\': \'datasets.md\',\n        \'functions\': [\n            datasets.graphsage.load_data\n        ],\n        \'methods\': [],\n        \'classes\': []\n    },\n    {\n        \'page\': \'datasets.md\',\n        \'functions\': [\n            datasets.tud.load_data\n        ],\n        \'methods\': [],\n        \'classes\': []\n    },\n    {\n        \'page\': \'datasets.md\',\n        \'functions\': [\n            datasets.ogb.graph_to_numpy,\n            datasets.ogb.dataset_to_numpy\n        ],\n        \'methods\': [],\n        \'classes\': []\n    },\n    {\n        \'page\': \'datasets.md\',\n        \'functions\': [\n            datasets.qm9.load_data\n        ],\n        \'methods\': [],\n        \'classes\': []\n    },\n    {\n        \'page\': \'datasets.md\',\n        \'functions\': [\n            datasets.mnist.load_data\n        ],\n        \'methods\': [],\n        \'classes\': []\n    },\n    {\n        \'page\': \'datasets.md\',\n        \'functions\': [\n            datasets.delaunay.generate_data\n        ],\n        \'methods\': [],\n        \'classes\': []\n    },\n    # Utils ####################################################################\n    {\n        \'page\': \'utils/data.md\',\n        \'functions\': [\n            utils.data.numpy_to_disjoint,\n            utils.data.numpy_to_batch,\n            utils.data.batch_iterator\n        ]\n    },\n    {\n        \'page\': \'utils/convolution.md\',\n        \'functions\': [\n            utils.convolution.degree_matrix,\n            utils.convolution.degree_power,\n            utils.convolution.normalized_adjacency,\n            utils.convolution.laplacian,\n            utils.convolution.normalized_laplacian,\n            utils.convolution.rescale_laplacian,\n            utils.convolution.localpooling_filter,\n            utils.convolution.chebyshev_polynomial,\n            utils.convolution.chebyshev_filter\n        ],\n        \'methods\': [],\n        \'classes\': []\n    },\n    {\n        \'page\': \'utils/misc.md\',\n        \'functions\': [\n            utils.misc.pad_jagged_array,\n            utils.misc.add_eye,\n            utils.misc.sub_eye,\n            utils.misc.add_eye_batch,\n            utils.misc.sub_eye_batch,\n            utils.misc.add_eye_jagged,\n            utils.misc.sub_eye_jagged,\n        ],\n        \'methods\': [],\n        \'classes\': []\n    },\n    {\n        \'page\': \'utils/conversion.md\',\n        \'functions\': [\n            utils.conversion.nx_to_adj,\n            utils.conversion.nx_to_node_features,\n            utils.conversion.nx_to_edge_features,\n            utils.conversion.nx_to_numpy,\n            utils.conversion.numpy_to_nx\n        ],\n        \'methods\': [],\n        \'classes\': []\n    },\n    # Chem #####################################################################\n    {\n        \'page\': \'chem.md\',\n        \'functions\': [\n            chem.numpy_to_rdkit,\n            chem.numpy_to_smiles,\n            chem.rdkit_to_smiles,\n            chem.sdf_to_nx,\n            chem.nx_to_sdf,\n            chem.validate_rdkit,\n            chem.get_atomic_symbol,\n            chem.get_atomic_num,\n            chem.valid_score,\n            chem.novel_score,\n            chem.unique_score,\n            chem.enable_rdkit_log,\n            chem.plot_rdkit,\n            chem.plot_rdkit_svg_grid\n        ],\n        \'methods\': [],\n        \'classes\': []\n    }\n]\n\nROOT = \'https://graphneural.network/\'\n\n\ndef get_function_signature(function, method=True):\n    wrapped = getattr(function, \'_original_function\', None)\n    if wrapped is None:\n        try:\n            signature = inspect.getargspec(function)\n        except ValueError:\n            signature = inspect.getfullargspec(function)\n    else:\n        signature = inspect.getargspec(wrapped)\n    if signature.defaults is None:\n        signature = signature._replace(defaults=[])\n    defaults = signature.defaults\n    if hasattr(signature, \'kwonlydefaults\'):\n        signature.args.extend(signature.kwonlyargs)\n        for kwa in signature.kwonlyargs:\n            defaults.append(signature.kwonlydefaults[kwa])\n    if method:\n        args = signature.args[1:]  # Remove self\n    else:\n        args = signature.args\n    if defaults:\n        kwargs = zip(args[-len(defaults):], defaults)\n        args = args[:-len(defaults)]\n    else:\n        kwargs = []\n    st = \'%s.%s(\' % (clean_module_name(function.__module__), function.__name__)\n\n    for a in args:\n        st += str(a) + \', \'\n    for a, v in kwargs:\n        if isinstance(v, str):\n            v = \'\\\'\' + v + \'\\\'\'\n        st += str(a) + \'=\' + str(v) + \', \'\n    if kwargs or args:\n        signature = st[:-2] + \')\'\n    else:\n        signature = st + \')\'\n    return post_process_signature(signature)\n\n\ndef get_class_signature(cls):\n    try:\n        class_signature = get_function_signature(cls.__init__)\n        class_signature = class_signature.replace(\'__init__\', cls.__name__)\n    except (TypeError, AttributeError):\n        # in case the class inherits from object and does not\n        # define __init__\n        class_signature = ""{clean_module_name}.{cls_name}()"".format(\n            clean_module_name=clean_module_name(cls.__module__),\n            cls_name=cls.__name__\n        )\n    return post_process_signature(class_signature)\n\n\ndef post_process_signature(signature):\n    parts = re.split(r\'\\.(?!\\d)\', signature)\n    if len(parts) >= 4:\n        if parts[1] == \'layers\':\n            signature = \'spektral.layers.\' + \'.\'.join(parts[3:])\n        if parts[1] == \'utils\':\n            signature = \'spektral.utils.\' + \'.\'.join(parts[3:])\n    return signature\n\n\ndef clean_module_name(name):\n    return name\n\n\ndef class_to_docs_link(cls):\n    module_name = clean_module_name(cls.__module__)\n    module_name = module_name[6:]\n    link = ROOT + module_name.replace(\'.\', \'/\') + \'#\' + cls.__name__.lower()\n    return link\n\n\ndef class_to_source_link(cls):\n    module_name = clean_module_name(cls.__module__)\n    path = module_name.replace(\'.\', \'/\')\n    path += \'.py\'\n    line = inspect.getsourcelines(cls)[-1]\n    link = (\'https://github.com/danielegrattarola/\'\n            \'spektral/blob/master/\' + path + \'#L\' + str(line))\n    return \'[[source]](\' + link + \')\'\n\n\ndef code_snippet(snippet):\n    result = \'```python\\n\'\n    result += snippet + \'\\n\'\n    result += \'```\\n\'\n    return result\n\n\ndef count_leading_spaces(s):\n    ws = re.search(r\'\\S\', s)\n    if ws:\n        return ws.start()\n    else:\n        return 0\n\n\ndef process_list_block(docstring, starting_point, leading_spaces, marker):\n    ending_point = docstring.find(\'\\n\\n\', starting_point)\n    block = docstring[starting_point:(\n        None if ending_point == -1 else ending_point - 1)]\n    # Place marker for later reinjection.\n    docstring = docstring.replace(block, marker)\n    lines = block.split(\'\\n\')\n    # Remove the computed number of leading white spaces from each line.\n    lines = [re.sub(\'^\' + \' \' * leading_spaces, \'\', line) for line in lines]\n    # Usually lines have at least 4 additional leading spaces.\n    # These have to be removed, but first the list roots have to be detected.\n    top_level_regex = r\'^    ([^\\s\\\\\\(]+):(.*)\'\n    top_level_replacement = r\'- __\\1__:\\2\'\n    lines = [re.sub(top_level_regex, top_level_replacement, line)\n             for line in lines]\n    # All the other lines get simply the 4 leading space (if present) removed\n    lines = [re.sub(r\'^    \', \'\', line) for line in lines]\n    # Fix text lines after lists\n    indent = 0\n    text_block = False\n    for i in range(len(lines)):\n        line = lines[i]\n        spaces = re.search(r\'\\S\', line)\n        if spaces:\n            # If it is a list element\n            if line[spaces.start()] == \'-\':\n                indent = spaces.start() + 1\n                if text_block:\n                    text_block = False\n                    lines[i] = \'\\n\' + line\n            elif spaces.start() < indent:\n                text_block = True\n                indent = spaces.start()\n                lines[i] = \'\\n\' + line\n        else:\n            text_block = False\n            indent = 0\n    block = \'\\n\'.join(lines)\n    return docstring, block\n\n\ndef process_docstring(docstring):\n    # First, extract code blocks and process them.\n    code_blocks = []\n    if \'```\' in docstring:\n        tmp = docstring[:]\n        while \'```\' in tmp:\n            tmp = tmp[tmp.find(\'```\'):]\n            index = tmp[3:].find(\'```\') + 6\n            snippet = tmp[:index]\n            # Place marker in docstring for later reinjection.\n            docstring = docstring.replace(\n                snippet, \'$CODE_BLOCK_%d\' % len(code_blocks))\n            snippet_lines = snippet.split(\'\\n\')\n            # Remove leading spaces.\n            num_leading_spaces = snippet_lines[-1].find(\'`\')\n            snippet_lines = ([snippet_lines[0]] +\n                             [line[num_leading_spaces:]\n                              for line in snippet_lines[1:]])\n            # Most code snippets have 3 or 4 more leading spaces\n            # on inner lines, but not all. Remove them.\n            inner_lines = snippet_lines[1:-1]\n            leading_spaces = None\n            for line in inner_lines:\n                if not line or line[0] == \'\\n\':\n                    continue\n                spaces = count_leading_spaces(line)\n                if leading_spaces is None:\n                    leading_spaces = spaces\n                if spaces < leading_spaces:\n                    leading_spaces = spaces\n            if leading_spaces:\n                snippet_lines = ([snippet_lines[0]] +\n                                 [line[leading_spaces:]\n                                  for line in snippet_lines[1:-1]] +\n                                 [snippet_lines[-1]])\n            snippet = \'\\n\'.join(snippet_lines)\n            code_blocks.append(snippet)\n            tmp = tmp[index:]\n\n    # Format docstring lists.\n    section_regex = r\'\\n( +)# (.*)\\n\'\n    section_idx = re.search(section_regex, docstring)\n    shift = 0\n    sections = {}\n    while section_idx and section_idx.group(2):\n        anchor = section_idx.group(2)\n        leading_spaces = len(section_idx.group(1))\n        shift += section_idx.end()\n        marker = \'$\' + anchor.replace(\' \', \'_\') + \'$\'\n        docstring, content = process_list_block(docstring,\n                                                shift,\n                                                leading_spaces,\n                                                marker)\n        sections[marker] = content\n        section_idx = re.search(section_regex, docstring[shift:])\n\n    # Format docstring section titles.\n    docstring = re.sub(r\'\\n(\\s+)# (.*)\\n\',\n                       r\'\\n\\1__\\2__\\n\\n\',\n                       docstring)\n\n    # Strip all remaining leading spaces.\n    lines = docstring.split(\'\\n\')\n    docstring = \'\\n\'.join([line.lstrip(\' \') for line in lines])\n\n    # Reinject list blocks.\n    for marker, content in sections.items():\n        docstring = docstring.replace(marker, content)\n\n    # Reinject code blocks.\n    for i, code_block in enumerate(code_blocks):\n        docstring = docstring.replace(\n            \'$CODE_BLOCK_%d\' % i, code_block)\n\n    # Spektral-specific code\n    docstring = re.sub(r\':param\', \'\\n**Arguments**  \\n:param\', docstring, 1)\n    docstring = re.sub(r\':param(.*):\', r\'\\n- `\\1`:\', docstring)\n    docstring = re.sub(\n        r\':return: ([a-z])\', lambda m: \':return: {}\'.format(m.group(1).upper()), docstring)\n    docstring = re.sub(r\':return:\', \'\\n**Return**  \\n\', docstring)\n\n    return docstring\n\n\nprint(\'Cleaning up existing sources directory.\')\nif os.path.exists(\'sources\'):\n    shutil.rmtree(\'sources\')\n\nprint(\'Populating sources directory with templates.\')\nfor subdir, dirs, fnames in os.walk(\'templates\'):\n    for fname in fnames:\n        new_subdir = subdir.replace(\'templates\', \'sources\')\n        if not os.path.exists(new_subdir):\n            os.makedirs(new_subdir)\n        if fname[-3:] == \'.md\':\n            fpath = os.path.join(subdir, fname)\n            new_fpath = fpath.replace(\'templates\', \'sources\')\n            shutil.copy(fpath, new_fpath)\n\n\ndef read_file(path):\n    with open(path) as f:\n        return f.read()\n\n\ndef collect_class_methods(cls, methods):\n    if isinstance(methods, (list, tuple)):\n        return [getattr(cls, m) if isinstance(m, str) else m for m in methods]\n    methods = []\n    for _, method in inspect.getmembers(cls, predicate=inspect.isroutine):\n        if method.__name__[0] == \'_\' or method.__name__ in EXCLUDE:\n            continue\n        methods.append(method)\n    return methods\n\n\ndef render_function(function, method=True):\n    subblocks = []\n    signature = get_function_signature(function, method=method)\n    if method:\n        signature = signature.replace(\n            clean_module_name(function.__module__) + \'.\', \'\')\n    subblocks.append(\'### \' + function.__name__ + \'\\n\')\n    subblocks.append(code_snippet(signature))\n    docstring = function.__doc__\n    if docstring:\n        subblocks.append(process_docstring(docstring))\n    return \'\\n\\n\'.join(subblocks)\n\n\ndef read_page_data(page_data, type):\n    assert type in [\'classes\', \'functions\', \'methods\']\n    data = page_data.get(type, [])\n    for module in page_data.get(\'all_module_{}\'.format(type), []):\n        module_data = []\n        for name in dir(module):\n            if name[0] == \'_\' or name in EXCLUDE:\n                continue\n            module_member = getattr(module, name)\n            if (inspect.isclass(module_member) and type == \'classes\' or\n                    inspect.isfunction(module_member) and type == \'functions\'):\n                instance = module_member\n                if module.__name__ in instance.__module__:\n                    if instance not in module_data:\n                        module_data.append(instance)\n        module_data.sort(key=lambda x: id(x))\n        data += module_data\n    return data\n\n\nif __name__ == \'__main__\':\n    readme = read_file(\'../README.md\')\n    index = read_file(\'templates/index.md\')\n    # index = index.replace(\'{{autogenerated}}\', readme[readme.find(\'##\'):])\n    index = index.replace(\'{{autogenerated}}\', readme)\n    with open(\'sources/index.md\', \'w\') as f:\n        f.write(index)\n\n    print(\'Generating docs for Spektral\')\n    for page_data in PAGES:\n        classes = read_page_data(page_data, \'classes\')\n\n        blocks = []\n        for element in classes:\n            if not isinstance(element, (list, tuple)):\n                element = (element, [])\n            cls = element[0]\n            subblocks = []\n            signature = get_class_signature(cls)\n            subblocks.append(\'<span style=""float:right;"">\' +\n                             class_to_source_link(cls) + \'</span>\')\n            if element[1]:\n                subblocks.append(\'## \' + cls.__name__ + \' class\\n\')\n            else:\n                subblocks.append(\'### \' + cls.__name__ + \'\\n\')\n            subblocks.append(code_snippet(signature))\n            docstring = cls.__doc__\n            if docstring:\n                subblocks.append(process_docstring(docstring))\n            methods = collect_class_methods(cls, element[1])\n            if methods:\n                subblocks.append(\'\\n---\')\n                subblocks.append(\'## \' + cls.__name__ + \' methods\\n\')\n                subblocks.append(\'\\n---\\n\'.join(\n                    [render_function(method, method=True) for method in methods]))\n            blocks.append(\'\\n\'.join(subblocks))\n\n        methods = read_page_data(page_data, \'methods\')\n\n        for method in methods:\n            blocks.append(render_function(method, method=True))\n\n        functions = read_page_data(page_data, \'functions\')\n\n        for function in functions:\n            blocks.append(render_function(function, method=False))\n\n        if not blocks:\n            # raise RuntimeError(\'Found no content for page \' + page_data[\'page\'])\n            blocks = []\n\n        mkdown = \'\\n----\\n\\n\'.join(blocks)\n        # save module page.\n        # Either insert content into existing page,\n        # or create page otherwise\n        page_name = page_data[\'page\']\n        path = os.path.join(\'sources\', page_name)\n        if os.path.exists(path):\n            template = read_file(path)\n            assert \'{{autogenerated}}\' in template, (\'Template found for \' + path +\n                                                     \' but missing {{autogenerated}}\'\n                                                     \' tag.\')\n            mkdown = template.replace(\'{{autogenerated}}\', mkdown, 1)\n            print(\'...inserting autogenerated content into template:\', path)\n        else:\n            print(\'...creating new page with autogenerated content:\', path)\n        subdir = os.path.dirname(path)\n        if not os.path.exists(subdir):\n            os.makedirs(subdir)\n        with open(path, \'w\') as f:\n            f.write(mkdown)\n        if not os.path.exists(\'sources/stylesheets/\'):\n            os.makedirs(\'sources/stylesheets/\')\n\n        if not os.path.exists(\'sources/js/\'):\n            os.makedirs(\'sources/js/\')\n\n        if not os.path.exists(\'sources/img/\'):\n            os.makedirs(\'sources/img/\')\n\n        if not os.path.exists(\'sources/custom_theme/img/\'):\n            os.makedirs(\'sources/custom_theme/img/\')\n\n        shutil.copy(\'./stylesheets/extra.css\',\n                    \'./sources/stylesheets/extra.css\')\n        shutil.copy(\'./js/macros.js\', \'./sources/js/macros.js\')\n        for file in glob.glob(r\'./img/*.svg\'):\n            shutil.copy(file, \'./sources/img/\')\n        shutil.copy(\'./img/favicon.ico\',\n                    \'./sources/custom_theme/img/favicon.ico\')\n        shutil.copy(\'./templates/google8a76765aa72fa8c1.html\',\n                    \'./sources/google8a76765aa72fa8c1.html\')\n'"
spektral/__init__.py,0,"b""from . import datasets\nfrom . import layers\nfrom . import utils\n\n__version__ = '0.5.1'\n"""
spektral/chem.py,0,"b'import networkx as nx\nimport numpy as np\ntry:\n    from rdkit import Chem as rdc\n    from rdkit.Chem import Draw\n    from rdkit import rdBase as rdb\n\n    rdb.DisableLog(\'rdApp.error\')  # RDKit logging is disabled by default\n    Draw.DrawingOptions.dblBondOffset = .1\n    BOND_MAP = {0: rdc.rdchem.BondType.ZERO,\n                1: rdc.rdchem.BondType.SINGLE,\n                2: rdc.rdchem.BondType.DOUBLE,\n                3: rdc.rdchem.BondType.TRIPLE,\n                4: rdc.rdchem.BondType.AROMATIC}\nexcept ImportError:\n    rdc = None\n    rdb = None\n\nNUM_TO_SYMBOL = {1: \'H\', 2: \'He\', 3: \'Li\', 4: \'Be\', 5: \'B\', 6: \'C\', 7: \'N\',\n                 8: \'O\', 9: \'F\', 10: \'Ne\', 11: \'Na\', 12: \'Mg\', 13: \'Al\',\n                 14: \'Si\', 15: \'P\', 16: \'S\', 17: \'Cl\', 18: \'Ar\', 19: \'K\',\n                 20: \'Ca\', 21: \'Sc\', 22: \'Ti\', 23: \'V\', 24: \'Cr\', 25: \'Mn\',\n                 26: \'Fe\', 27: \'Co\', 28: \'Ni\', 29: \'Cu\', 30: \'Zn\', 31: \'Ga\',\n                 32: \'Ge\', 33: \'As\', 34: \'Se\', 35: \'Br\', 36: \'Kr\', 37: \'Rb\',\n                 38: \'Sr\', 39: \'Y\', 40: \'Zr\', 41: \'Nb\', 42: \'Mo\', 43: \'Tc\',\n                 44: \'Ru\', 45: \'Rh\', 46: \'Pd\', 47: \'Ag\', 48: \'Cd\', 49: \'In\',\n                 50: \'Sn\', 51: \'Sb\', 52: \'Te\', 53: \'I\', 54: \'Xe\', 55: \'Cs\',\n                 56: \'Ba\', 57: \'La\', 58: \'Ce\', 59: \'Pr\', 60: \'Nd\', 61: \'Pm\',\n                 62: \'Sm\', 63: \'Eu\', 64: \'Gd\', 65: \'Tb\', 66: \'Dy\', 67: \'Ho\',\n                 68: \'Er\', 69: \'Tm\', 70: \'Yb\', 71: \'Lu\', 72: \'Hf\', 73: \'Ta\',\n                 74: \'W\', 75: \'Re\', 76: \'Os\', 77: \'Ir\', 78: \'Pt\', 79: \'Au\',\n                 80: \'Hg\', 81: \'Tl\', 82: \'Pb\', 83: \'Bi\', 84: \'Po\', 85: \'At\',\n                 86: \'Rn\', 87: \'Fr\', 88: \'Ra\', 89: \'Ac\', 90: \'Th\', 91: \'Pa\',\n                 92: \'U\', 93: \'Np\', 94: \'Pu\', 95: \'Am\', 96: \'Cm\', 97: \'Bk\',\n                 98: \'Cf\', 99: \'Es\', 100: \'Fm\', 101: \'Md\', 102: \'No\', 103: \'Lr\',\n                 104: \'Rf\', 105: \'Db\', 106: \'Sg\', 107: \'Bh\', 108: \'Hs\',\n                 109: \'Mt\', 110: \'Ds\', 111: \'Rg\', 112: \'Cn\', 113: \'Nh\',\n                 114: \'Fl\', 115: \'Mc\', 116: \'Lv\', 117: \'Ts\', 118: \'Og\'}\nSYMBOL_TO_NUM = {v: k for k, v in NUM_TO_SYMBOL.items()}\n\n\ndef numpy_to_rdkit(adj, nf, ef, sanitize=False):\n    """"""\n    Converts a molecule from numpy to RDKit format.\n    :param adj: binary numpy array of shape (N, N) \n    :param nf: numpy array of shape (N, F)\n    :param ef: numpy array of shape (N, N, S)\n    :param sanitize: whether to sanitize the molecule after conversion\n    :return: an RDKit molecule\n    """"""\n    if rdc is None:\n        raise ImportError(\'`numpy_to_rdkit` requires RDKit.\')\n    mol = rdc.RWMol()\n    for nf_ in nf:\n        atomic_num = int(nf_)\n        if atomic_num > 0:\n            mol.AddAtom(rdc.Atom(atomic_num))\n\n    for i, j in zip(*np.triu_indices(adj.shape[-1])):\n        if i != j and adj[i, j] == adj[j, i] == 1 and not mol.GetBondBetweenAtoms(int(i), int(j)):\n            bond_type_1 = BOND_MAP[int(ef[i, j, 0])]\n            bond_type_2 = BOND_MAP[int(ef[j, i, 0])]\n            if bond_type_1 == bond_type_2:\n                mol.AddBond(int(i), int(j), bond_type_1)\n\n    mol = mol.GetMol()\n    if sanitize:\n        rdc.SanitizeMol(mol)\n    return mol\n\n\ndef numpy_to_smiles(adj, nf, ef):\n    """"""\n    Converts a molecule from numpy to SMILES format.\n    :param adj: binary numpy array of shape (N, N) \n    :param nf: numpy array of shape (N, F)\n    :param ef: numpy array of shape (N, N, S) \n    :return: the SMILES string of the molecule\n    """"""\n    if rdc is None:\n        raise ImportError(\'`numpy_to_smiles` requires RDkit.\')\n    mol = numpy_to_rdkit(adj, nf, ef)\n    return rdkit_to_smiles(mol)\n\n\ndef rdkit_to_smiles(mol):\n    """"""\n    Returns the SMILES string representing an RDKit molecule.\n    :param mol: an RDKit molecule\n    :return: the SMILES string of the molecule \n    """"""\n    if rdc is None:\n        raise ImportError(\'`rdkit_to_smiles` requires RDkit.\')\n    return rdc.MolToSmiles(mol)\n\n\ndef sdf_to_nx(sdf, keep_hydrogen=False):\n    """"""\n    Converts molecules in SDF format to networkx Graphs.\n    :param sdf: a list of molecules (or individual molecule) in SDF format.\n    :param keep_hydrogen: whether to include hydrogen in the representation.\n    :return: list of nx.Graphs.\n    """"""\n    if not isinstance(sdf, list):\n        sdf = [sdf]\n\n    output = []\n    for sdf_ in sdf:\n        g = nx.Graph()\n\n        for atom in sdf_[\'atoms\']:\n            if atom[\'atomic_num\'] > 1 or keep_hydrogen:\n                g.add_node(atom[\'index\'], **atom)\n        for bond in sdf_[\'bonds\']:\n            start_atom_num = sdf_[\'atoms\'][bond[\'start_atom\']][\'atomic_num\']\n            end_atom_num = sdf_[\'atoms\'][bond[\'end_atom\']][\'atomic_num\']\n            if (start_atom_num > 1 and end_atom_num > 1) or keep_hydrogen:\n                g.add_edge(bond[\'start_atom\'], bond[\'end_atom\'], **bond)\n        output.append(g)\n\n    if len(output) == 1:\n        return output[0]\n    else:\n        return output\n\n\ndef nx_to_sdf(graphs):\n    """"""\n    Converts a list of nx.Graphs to the internal SDF format.\n    :param graphs: list of nx.Graphs.\n    :return: list of molecules in the internal SDF format.\n    """"""\n    if isinstance(graphs, nx.Graph):\n        graphs = [graphs]\n    output = []\n    for g in graphs:\n        sdf = {\'atoms\': [v for k, v in g.nodes.items()],\n               \'bonds\': [v for k, v in g.edges.items()],\n               \'comment\': \'\',\n               \'data\': [\'\'],\n               \'details\': \'\',\n               \'n_atoms\': -1,\n               \'n_bonds\': -1,\n               \'name\': \'\',\n               \'properties\': []}\n        output.append(sdf)\n    return output\n\n\ndef validate_rdkit_mol(mol):\n    """"""\n    Sanitizes an RDKit molecules and returns True if the molecule is chemically\n    valid.\n    :param mol: an RDKit molecule \n    :return: True if the molecule is chemically valid, False otherwise\n    """"""\n    if rdc is None:\n        raise ImportError(\'`validate_rdkit_mol` requires RDkit.\')\n    if len(rdc.GetMolFrags(mol)) > 1:\n        return False\n    try:\n        rdc.SanitizeMol(mol)\n        return True\n    except ValueError:\n        return False\n\n\ndef validate_rdkit(mol):\n    """"""\n    Validates RDKit molecules (single or in a list). \n    :param mol: an RDKit molecule or list/np.array thereof\n    :return: boolean array, True if the molecules are chemically valid, False \n    otherwise\n    """"""\n    if rdc is None:\n        raise ImportError(\'`validate_rdkit` requires RDkit.\')\n    if isinstance(mol, list) or isinstance(mol, np.ndarray):\n        return np.array([validate_rdkit_mol(m) for m in mol])\n    else:\n        return validate_rdkit_mol(mol)\n\n\ndef get_atomic_symbol(number):\n    """"""\n    Given an atomic number (e.g., 6), returns its atomic symbol (e.g., \'C\')\n    :param number: int <= 118\n    :return: string, atomic symbol\n    """"""\n    return NUM_TO_SYMBOL[number]\n\n\ndef get_atomic_num(symbol):\n    """"""\n    Given an atomic symbol (e.g., \'C\'), returns its atomic number (e.g., 6)\n    :param symbol: string, atomic symbol\n    :return: int <= 118\n    """"""\n    return SYMBOL_TO_NUM[symbol.lower().capitalize()]\n\n\ndef valid_score(molecules, from_numpy=False):\n    """"""\n    For a given list of molecules (RDKit or numpy format), returns a boolean \n    array representing the validity of each molecule.\n    :param molecules: list of molecules (RDKit or numpy format)\n    :param from_numpy: whether the molecules are in numpy format\n    :return: boolean array with the validity for each molecule\n    """"""\n    if rdc is None:\n        raise ImportError(\'`valid_score` requires RDkit.\')\n    valid = []\n    if from_numpy:\n        molecules = [numpy_to_rdkit(adj_p, nf_p, ef_p)\n                     for adj_p, nf_p, ef_p in molecules]\n    for mol_rdk in molecules:\n        valid.append(validate_rdkit_mol(mol_rdk))\n\n    return np.array(valid)\n\n\ndef novel_score(molecules, smiles, from_numpy=False):\n    """"""\n    For a given list of molecules (RDKit or numpy format), returns a boolean \n    array representing valid and novel molecules with respect to the list\n    of smiles provided (a molecule is novel if its SMILES is not in the list).\n    :param molecules: list of molecules (RDKit or numpy format)\n    :param smiles: list or set of smiles strings against which to check for \n    novelty\n    :param from_numpy: whether the molecules are in numpy format\n    :return: boolean array with the novelty for each valid molecule\n    """"""\n    if rdc is None:\n        raise ImportError(\'`novel_score` requires RDkit.\')\n    if from_numpy:\n        molecules = [numpy_to_rdkit(adj_p, nf_p, ef_p)\n                     for adj_p, nf_p, ef_p in molecules]\n    smiles = set(smiles)\n    novel = []\n    for mol in molecules:\n        is_valid = validate_rdkit_mol(mol)\n        is_novel = rdkit_to_smiles(mol) not in smiles\n        novel.append(is_valid and is_novel)\n\n    return np.array(novel)\n\n\ndef unique_score(molecules, from_numpy=False):\n    """"""\n    For a given list of molecules (RDKit or numpy format), returns the fraction\n    of unique and valid molecules w.r.t. to the number of valid molecules.\n    :param molecules: list of molecules (RDKit or numpy format)\n    :param from_numpy: whether the molecules are in numpy format\n    :return: fraction of unique valid molecules w.r.t. to valid molecules\n    """"""\n    if rdc is None:\n        raise ImportError(\'`unique_score` requires RDkit.\')\n    if from_numpy:\n        molecules = [numpy_to_rdkit(adj_p, nf_p, ef_p)\n                     for adj_p, nf_p, ef_p in molecules]\n    smiles = set()\n    n_valid = 0\n    for mol in molecules:\n        if validate_rdkit_mol(mol):\n            n_valid += 1\n            smiles.add(rdkit_to_smiles(mol))\n\n    return 0 if n_valid == 0 else (len(smiles) / n_valid)\n\n\ndef enable_rdkit_log():\n    """"""\n    Enables RDkit logging.\n    :return:\n    """"""\n    if rdb is None:\n        raise ImportError(\'`enable_rdkit_log` requires RDkit.\')\n    rdb.EnableLog(\'rdApp.error\')\n\n\ndef plot_rdkit(mol, filename=None):\n    """"""\n    Plots an RDKit molecule in Matplotlib\n    :param mol: an RDKit molecule \n    :param filename: save the image with the given filename \n    :return: the image as np.array\n    """"""\n    if rdc is None:\n        raise ImportError(\'`draw_rdkit_mol` requires RDkit.\')\n    if filename is not None:\n        Draw.MolToFile(mol, filename)\n    img = Draw.MolToImage(mol)\n    return img\n\n\ndef plot_rdkit_svg_grid(mols, mols_per_row=5, filename=None, **kwargs):\n    """"""\n    Plots a grid of RDKit molecules in SVG.\n    :param mols: a list of RDKit molecules\n    :param mols_per_row: size of the grid\n    :param filename: save an image with the given filename\n    :param kwargs: additional arguments for `RDKit.Chem.Draw.MolsToGridImage`\n    :return: the SVG as a string\n    """"""\n    if rdc is None:\n        raise ImportError(\'`draw_rdkit_mol` requires RDkit.\')\n    svg = Draw.MolsToGridImage(mols, molsPerRow=mols_per_row, useSVG=True, **kwargs)\n    if filename is not None:\n        if not filename.endswith(\'.svg\'):\n            filename += \'.svg\'\n        with open(filename, \'w\') as f:\n            f.write(svg)\n    return svg\n\n'"
tests/__init__.py,0,b''
tests/test_datasets.py,0,"b""from spektral.datasets import delaunay, qm9, citation, graphsage, mnist, tud\n\n\ndef correctly_padded(adj, nf, ef):\n    assert adj.ndim == 3\n    assert adj.shape[-1] == adj.shape[-2]\n    if nf is not None:\n        assert nf.ndim == 3\n        assert adj.shape[-1] == nf.shape[-2]\n    if ef is not None:\n        assert ef.ndim == 4\n        assert adj.shape[-1] == ef.shape[-2]\n        assert adj.shape[-1] == ef.shape[-3]\n\n\ndef test_citation():\n    for dataset_name in ['cora', 'citeseer', 'pubmed']:\n        citation.load_data(dataset_name)\n        citation.load_data(dataset_name, random_split=True)\n\n\ndef test_graphsage():\n    for dataset_name in ['ppi']:\n        # Test only PPI because Travis otherwise fails\n        graphsage.load_data(dataset_name)\n\n\ndef test_delaunay():\n    adj, nf, labels = delaunay.generate_data(return_type='numpy', classes=[0, 1, 2])\n    correctly_padded(adj, nf, None)\n    assert adj.shape[0] == labels.shape[0]\n\n    # Test that it doesn't crash\n    delaunay.generate_data(return_type='networkx')\n\n\ndef test_mnist():\n    mnist.load_data(k=8, noise_level=0.1)\n\n\ndef test_qm9():\n    adj, nf, ef, labels = qm9.load_data(return_type='numpy', amount=1000)\n    correctly_padded(adj, nf, ef)\n    assert adj.shape[0] == labels.shape[0]\n\n    # Test that it doesn't crash\n    qm9.load_data(return_type='networkx', amount=1000)\n    qm9.load_data(return_type='sdf', amount=1000)\n\n\ndef test_tud():\n    tud.load_data('PROTEINS', clean=False)\n    tud.load_data('ENZYMES', clean=True)\n"""
tests/test_utils.py,0,"b""from spektral.datasets import tud\nfrom spektral.utils import numpy_to_batch, numpy_to_disjoint\n\n\ndef test_utils_data():\n    # Load ENZYMES because we use it also in datasets tests\n    A_list, X_list, y = tud.load_data('ENZYMES', clean=True)\n\n    # Test numpy to batch\n    X_batch, A_batch = numpy_to_batch(X_list, A_list)\n    assert X_batch.ndim == 3\n    assert A_batch.ndim == 3\n    assert X_batch.shape[0] == A_batch.shape[0]\n    assert X_batch.shape[1] == A_batch.shape[1] == A_batch.shape[2]\n\n    # Test numpy to disjoint\n    X_disj, A_disj, I_disj = numpy_to_disjoint(X_list, A_list)\n    assert X_disj.ndim == 2\n    assert A_disj.ndim == 2\n    assert X_disj.shape[0] == A_disj.shape[0] == A_disj.shape[1]\n"""
examples/graph_prediction/BDGC_disjoint.py,7,"b'""""""\nThis example shows how to perform graph classification with a synthetic\nbenchmark dataset created by F. M. Bianchi (https://github.com/FilippoMB/Benchmark_dataset_for_graph_classification),\nusing a GNN with convolutional and pooling blocks in disjoint mode.\nThis is a more advanced example that also shows how to do validation and early\nstopping. For a beginner-level example, see qm9_disjoint.py.\n""""""\n\nimport os\n\nimport numpy as np\nimport requests\nimport tensorflow as tf\nfrom tensorflow.keras.layers import Input, Dense\nfrom tensorflow.keras.losses import CategoricalCrossentropy\nfrom tensorflow.keras.metrics import CategoricalAccuracy\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.optimizers import Adam\n\nfrom spektral.layers import GraphConvSkip, GlobalAvgPool\nfrom spektral.layers import ops\nfrom spektral.layers.pooling import TopKPool\nfrom spektral.utils.convolution import normalized_adjacency\nfrom spektral.utils.data import batch_iterator, numpy_to_disjoint\n\n\ndef evaluate(A_list, X_list, y_list, ops_list, batch_size):\n    batches = batch_iterator([X_list, A_list, y_list], batch_size=batch_size)\n    output = []\n    for b in batches:\n        X, A, I = numpy_to_disjoint(*b[:-1])\n        A = ops.sp_matrix_to_sp_tensor(A)\n        y = b[-1]\n        pred = model([X, A, I], training=False)\n        outs = [o(pred, y) for o in ops_list]\n        output.append(outs)\n    return np.mean(output, 0)\n\n\n################################################################################\n# PARAMETERS\n################################################################################\nlearning_rate = 1e-3       # Learning rate\nepochs = 500               # Number of training epochs\nes_patience = 50           # Patience for early stopping\nbatch_size = 16            # Batch size\ndata_url = \'https://github.com/FilippoMB/Benchmark_dataset_for_graph_classification/raw/master/datasets/\'\ndataset_name = \'easy.npz\'  # Dataset (\'easy.npz\' or \'hard.npz\')\n\n################################################################################\n# LOAD DATA\n################################################################################\n# Download graph classification data\nif not os.path.exists(dataset_name):\n    print(\'Downloading \' + dataset_name + \' from \' + data_url)\n    req = requests.get(data_url + dataset_name)\n    with open(dataset_name, \'wb\') as out_file:\n        out_file.write(req.content)\n\n# Load data\nloaded = np.load(dataset_name, allow_pickle=True)\nX_train, A_train, y_train = loaded[\'tr_feat\'], list(loaded[\'tr_adj\']), loaded[\'tr_class\']\nX_test, A_test, y_test = loaded[\'te_feat\'], list(loaded[\'te_adj\']), loaded[\'te_class\']\nX_val, A_val, y_val = loaded[\'val_feat\'], list(loaded[\'val_adj\']), loaded[\'val_class\']\n\n# Preprocessing\nA_train = [normalized_adjacency(a) for a in A_train]\nA_val = [normalized_adjacency(a) for a in A_val]\nA_test = [normalized_adjacency(a) for a in A_test]\n\n# Parameters\nF = X_train[0].shape[-1]  # Dimension of node features\nn_out = y_train[0].shape[-1]  # Dimension of the target\n\n################################################################################\n# BUILD MODEL\n################################################################################\nX_in = Input(shape=(F, ), name=\'X_in\')\nA_in = Input(shape=(None,), sparse=True)\nI_in = Input(shape=(), name=\'segment_ids_in\', dtype=tf.int32)\n\nX_1 = GraphConvSkip(32, activation=\'relu\')([X_in, A_in])\nX_1, A_1, I_1 = TopKPool(ratio=0.5)([X_1, A_in, I_in])\nX_2 = GraphConvSkip(32, activation=\'relu\')([X_1, A_1])\nX_2, A_2, I_2 = TopKPool(ratio=0.5)([X_2, A_1, I_1])\nX_3 = GraphConvSkip(32, activation=\'relu\')([X_2, A_2])\nX_3 = GlobalAvgPool()([X_3, I_2])\noutput = Dense(n_out, activation=\'softmax\')(X_3)\n\n# Build model\nmodel = Model(inputs=[X_in, A_in, I_in], outputs=output)\nopt = Adam(lr=learning_rate)\nloss_fn = CategoricalCrossentropy()\nacc_fn = CategoricalAccuracy()\n\n\n@tf.function(\n    input_signature=(tf.TensorSpec((None, F), dtype=tf.float64),\n                     tf.SparseTensorSpec((None, None), dtype=tf.float32),\n                     tf.TensorSpec((None,), dtype=tf.int32),\n                     tf.TensorSpec((None, n_out), dtype=tf.float64)),\n    experimental_relax_shapes=True)\ndef train_step(X_, A_, I_, y_):\n    with tf.GradientTape() as tape:\n        predictions = model([X_, A_, I_], training=True)\n        loss = loss_fn(y_, predictions)\n        loss += sum(model.losses)\n        acc = acc_fn(y_, predictions)\n    gradients = tape.gradient(loss, model.trainable_variables)\n    opt.apply_gradients(zip(gradients, model.trainable_variables))\n    return loss, acc\n\n\n################################################################################\n# FIT MODEL\n################################################################################\ncurrent_batch = 0\nepoch = 0\nmodel_loss = 0\nmodel_acc = 0\nbest_val_loss = np.inf\nbest_weights = None\npatience = es_patience\nbatches_in_epoch = np.ceil(y_train.shape[0] / batch_size)\n\nprint(\'Fitting model\')\nbatches = batch_iterator([X_train, A_train, y_train],\n                         batch_size=batch_size, epochs=epochs)\nfor b in batches:\n    current_batch += 1\n\n    X_, A_, I_ = numpy_to_disjoint(*b[:-1])\n    A_ = ops.sp_matrix_to_sp_tensor(A_)\n    y_ = b[-1]\n    outs = train_step(X_, A_, I_, y_)\n\n    model_loss += outs[0]\n    model_acc += outs[1]\n    if current_batch == batches_in_epoch:\n        epoch += 1\n        model_loss /= batches_in_epoch\n        model_acc /= batches_in_epoch\n\n        # Compute validation loss and accuracy\n        val_loss, val_acc = evaluate(A_val, X_val, y_val, [loss_fn, acc_fn], batch_size=batch_size)\n        print(\'Ep. {} - Loss: {:.2f} - Acc: {:.2f} - Val loss: {:.2f} - Val acc: {:.2f}\'\n              .format(epoch, model_loss, model_acc, val_loss, val_acc))\n\n        # Check if loss improved for early stopping\n        if val_loss < best_val_loss:\n            best_val_loss = val_loss\n            patience = es_patience\n            print(\'New best val_loss {:.3f}\'.format(val_loss))\n            best_weights = model.get_weights()\n        else:\n            patience -= 1\n            if patience == 0:\n                print(\'Early stopping (best val_loss: {})\'.format(best_val_loss))\n                break\n        model_loss = 0\n        model_acc = 0\n        current_batch = 0\n\n################################################################################\n# EVALUATE MODEL\n################################################################################\nprint(\'Testing model\')\nmodel.set_weights(best_weights)  # Load best model\ntest_loss, test_acc = evaluate(A_test, X_test, y_test, [loss_fn, acc_fn], batch_size=batch_size)\nprint(\'Done. Test loss: {:.4f}. Test acc: {:.2f}\'.format(test_loss, test_acc))\n'"
examples/graph_prediction/delaunay_batch.py,0,"b'""""""\nThis example shows how to perform graph classification with a synthetic dataset\nof Delaunay triangulations, using a graph attention network (Velickovic et al.)\nin batch mode.\n""""""\n\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom tensorflow.keras.layers import Input, Dense\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.regularizers import l2\n\nfrom spektral.datasets import delaunay\nfrom spektral.layers import GraphAttention, GlobalAttentionPool\n\n# Load data\nA, X, y = delaunay.generate_data(return_type=\'numpy\', classes=[0, 5])\n\n# Parameters\nN = X.shape[-2]          # Number of nodes in the graphs\nF = X.shape[-1]          # Original feature dimensionality\nn_classes = y.shape[-1]  # Number of classes\nl2_reg = 5e-4            # Regularization rate for l2\nlearning_rate = 1e-3     # Learning rate for Adam\nepochs = 20000           # Number of training epochs\nbatch_size = 32          # Batch size\nes_patience = 200        # Patience fot early stopping\n\n# Train/test split\nA_train, A_test, \\\nx_train, x_test, \\\ny_train, y_test = train_test_split(A, X, y, test_size=0.1)\n\n# Model definition\nX_in = Input(shape=(N, F))\nA_in = Input((N, N))\n\ngc1 = GraphAttention(32, activation=\'relu\', kernel_regularizer=l2(l2_reg))([X_in, A_in])\ngc2 = GraphAttention(32, activation=\'relu\', kernel_regularizer=l2(l2_reg))([gc1, A_in])\npool = GlobalAttentionPool(128)(gc2)\n\noutput = Dense(n_classes, activation=\'softmax\')(pool)\n\n# Build model\nmodel = Model(inputs=[X_in, A_in], outputs=output)\noptimizer = Adam(lr=learning_rate)\nmodel.compile(optimizer=optimizer, loss=\'categorical_crossentropy\', metrics=[\'acc\'])\nmodel.summary()\n\n# Train model\nmodel.fit([x_train, A_train],\n          y_train,\n          batch_size=batch_size,\n          validation_split=0.1,\n          epochs=epochs,\n          callbacks=[\n              EarlyStopping(patience=es_patience, restore_best_weights=True)\n          ])\n\n# Evaluate model\nprint(\'Evaluating model.\')\neval_results = model.evaluate([x_test, A_test],\n                              y_test,\n                              batch_size=batch_size)\nprint(\'Done. Test loss: {:.4f}. Test acc: {:.2f}\'.format(*eval_results))\n'"
examples/graph_prediction/ogbg-mol-esol_batch.py,0,"b'""""""\nThis example shows how to perform molecule regression with the\n[Open Graph Benchmark](https://ogb.stanford.edu) `mol-esol` dataset, using a\nsimple GIN-based GNN with MinCutPool in batch mode.\nExpect unstable training due to the small-ish size of the dataset.\n""""""\n\nfrom ogb.graphproppred import GraphPropPredDataset, Evaluator\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom tensorflow.keras.layers import Input, Dense\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.optimizers import Adam\n\nfrom spektral.datasets import ogb\nfrom spektral.layers import GINConv, MinCutPool, GlobalSumPool\nfrom spektral.utils import pad_jagged_array\n\n################################################################################\n# PARAMETERS\n################################################################################\nlearning_rate = 1e-3  # Learning rate\nepochs = 99999        # Number of training epochs\nbatch_size = 32       # Batch size\n\n################################################################################\n# LOAD DATA\n################################################################################\ndataset_name = \'ogbg-mol-esol\'\ndataset = GraphPropPredDataset(name=dataset_name)\nn_out = dataset.num_tasks\nN = max(g[0][\'num_nodes\'] for g in dataset)\n\nidx = dataset.get_idx_split()\ntr_idx, va_idx, te_idx = idx[""train""], idx[""valid""], idx[""test""]\n\nX, A, _, y = ogb.dataset_to_numpy(dataset, dtype=\'f8\')\nA = [a.toarray() for a in A]\nF = X[0].shape[-1]\nX = pad_jagged_array(X, (N, F))\nA = pad_jagged_array(A, (N, N))\nX_tr, A_tr, y_tr = X[tr_idx], A[tr_idx], y[tr_idx]\nX_va, A_va, y_va = X[va_idx], A[va_idx], y[va_idx]\nX_te, A_te, y_te = X[te_idx], A[te_idx], y[te_idx]\n\n################################################################################\n# BUILD MODEL\n################################################################################\nX_in = Input(shape=(N, F))\nA_in = Input(shape=(N, N))\n\nX_1 = GINConv(32, mlp_hidden=[32], activation=\'relu\')([X_in, A_in])\nX_1, A_1 = MinCutPool(N // 2)([X_1, A_in])\nX_2 = GINConv(32, mlp_hidden=[32], activation=\'relu\')([X_1, A_1])\nX_3 = GlobalSumPool()(X_2)\noutput = Dense(n_out)(X_3)\n\n# Build model\nmodel = Model(inputs=[X_in, A_in], outputs=output)\nopt = Adam(lr=learning_rate)\nmodel.compile(optimizer=opt, loss=\'mse\')\nmodel.summary()\n\n################################################################################\n# FIT MODEL\n################################################################################\nmodel.fit([X_tr, A_tr],\n          y_tr,\n          batch_size=batch_size,\n          validation_data=([X_va, A_va], y_va),\n          callbacks=[EarlyStopping(patience=200, restore_best_weights=True)],\n          epochs=epochs)\n\n################################################################################\n# EVALUATE MODEL\n################################################################################\nprint(\'Testing model\')\nevaluator = Evaluator(name=dataset_name)\ny_pred = model.predict([X_te, A_te], batch_size=batch_size)\nogb_score = evaluator.eval({\'y_true\': y_te, \'y_pred\': y_pred})\n\nprint(\'Done. RMSE: {:.4f}\'.format(ogb_score[\'rmse\']))\n'"
examples/graph_prediction/ogbg-mol-hiv_disjoint.py,8,"b'""""""\nThis example shows how to perform molecule classification with the\n[Open Graph Benchmark](https://ogb.stanford.edu) `mol-hiv` dataset, using a\nsimple ECC-based GNN in disjoint mode. The model does not perform really well\nbut should give you a starting point if you want to implement a more\nsophisticated one.\n""""""\n\nimport numpy as np\nimport tensorflow as tf\nfrom ogb.graphproppred import GraphPropPredDataset, Evaluator\nfrom tensorflow.keras.layers import Input, Dense\nfrom tensorflow.keras.losses import BinaryCrossentropy\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.optimizers import Adam\n\nfrom spektral.datasets import ogb\nfrom spektral.layers import EdgeConditionedConv, ops, GlobalSumPool\nfrom spektral.utils import batch_iterator, numpy_to_disjoint\n\n################################################################################\n# PARAMETERS\n################################################################################\nlearning_rate = 1e-3  # Learning rate\nepochs = 10           # Number of training epochs\nbatch_size = 32       # Batch size\n\n################################################################################\n# LOAD DATA\n################################################################################\ndataset_name = \'ogbg-mol-hiv\'\ndataset = GraphPropPredDataset(name=dataset_name)\nn_out = dataset.num_tasks\n\nidx = dataset.get_idx_split()\ntr_idx, va_idx, te_idx = idx[""train""], idx[""valid""], idx[""test""]\n\nX_tr, A_tr, E_tr, y_tr = ogb.dataset_to_numpy(dataset, tr_idx, dtype=\'f8\')\nX_va, A_va, E_va, y_va = ogb.dataset_to_numpy(dataset, va_idx, dtype=\'f8\')\nX_te, A_te, E_te, y_te = ogb.dataset_to_numpy(dataset, te_idx, dtype=\'f8\')\n\nF = X_tr[0].shape[-1]\nS = E_tr[0].shape[-1]\n\n################################################################################\n# BUILD MODEL\n################################################################################\nX_in = Input(shape=(F,))\nA_in = Input(shape=(None,), sparse=True)\nE_in = Input(shape=(S,))\nI_in = Input(shape=(), dtype=tf.int64)\n\nX_1 = EdgeConditionedConv(32, activation=\'relu\')([X_in, A_in, E_in])\nX_2 = EdgeConditionedConv(32, activation=\'relu\')([X_1, A_in, E_in])\nX_3 = GlobalSumPool()([X_2, I_in])\noutput = Dense(n_out, activation=\'sigmoid\')(X_3)\n\n# Build model\nmodel = Model(inputs=[X_in, A_in, E_in, I_in], outputs=output)\nopt = Adam(lr=learning_rate)\nloss_fn = BinaryCrossentropy()\n\n\n@tf.function(\n    input_signature=(tf.TensorSpec((None, F), dtype=tf.float64),\n                     tf.SparseTensorSpec((None, None), dtype=tf.float64),\n                     tf.TensorSpec((None, S), dtype=tf.float64),\n                     tf.TensorSpec((None,), dtype=tf.int32),\n                     tf.TensorSpec((None, n_out), dtype=tf.float64)),\n    experimental_relax_shapes=True)\ndef train_step(X_, A_, E_, I_, y_):\n    with tf.GradientTape() as tape:\n        predictions = model([X_, A_, E_, I_], training=True)\n        loss = loss_fn(y_, predictions)\n        loss += sum(model.losses)\n    gradients = tape.gradient(loss, model.trainable_variables)\n    opt.apply_gradients(zip(gradients, model.trainable_variables))\n    return loss\n\n\n################################################################################\n# FIT MODEL\n################################################################################\ncurrent_batch = 0\nmodel_loss = 0\nbatches_in_epoch = np.ceil(len(A_tr) / batch_size)\n\nprint(\'Fitting model\')\nbatches_train = batch_iterator([X_tr, A_tr, E_tr, y_tr],\n                               batch_size=batch_size, epochs=epochs)\nfor b in batches_train:\n    X_, A_, E_, I_ = numpy_to_disjoint(*b[:-1])\n    A_ = ops.sp_matrix_to_sp_tensor(A_)\n    y_ = b[-1]\n    outs = train_step(X_, A_, E_, I_, y_)\n\n    model_loss += outs.numpy()\n    current_batch += 1\n    if current_batch == batches_in_epoch:\n        print(\'Loss: {}\'.format(model_loss / batches_in_epoch))\n        model_loss = 0\n        current_batch = 0\n\n################################################################################\n# EVALUATE MODEL\n################################################################################\nprint(\'Testing model\')\nevaluator = Evaluator(name=dataset_name)\ny_pred = []\nbatches_test = batch_iterator([X_te, A_te, E_te], batch_size=batch_size)\nfor b in batches_test:\n    X_, A_, E_, I_ = numpy_to_disjoint(*b)\n    A_ = ops.sp_matrix_to_sp_tensor(A_)\n    p = model([X_, A_, E_, I_], training=False)\n    y_pred.append(p.numpy())\n\ny_pred = np.vstack(y_pred)\nmodel_loss = loss_fn(y_te, y_pred)\nogb_score = evaluator.eval({\'y_true\': y_te, \'y_pred\': y_pred})\n\nprint(\'Done. Test loss: {:.4f}. ROC-AUC: {:.2f}\'\n      .format(model_loss, ogb_score[\'rocauc\']))\n'"
examples/graph_prediction/qm9_batch.py,0,"b'""""""\nThis example shows how to perform regression of molecular properties with the\nQM9 database, using a GNN based on edge-conditioned convolutions in batch mode.\n""""""\n\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.keras.layers import Input, Dense\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.optimizers import Adam\n\nfrom spektral.datasets import qm9\nfrom spektral.layers import EdgeConditionedConv, GlobalSumPool\nfrom spektral.utils import label_to_one_hot\n\n################################################################################\n# PARAMETERS\n################################################################################\nlearning_rate = 1e-3  # Learning rate\nepochs = 10           # Number of training epochs\nbatch_size = 32           # Batch size\n\n################################################################################\n# LOAD DATA\n################################################################################\nA, X, E, y = qm9.load_data(return_type=\'numpy\',\n                           nf_keys=\'atomic_num\',\n                           ef_keys=\'type\',\n                           self_loops=True,\n                           amount=1000)  # Set to None to train on whole dataset\ny = y[[\'cv\']].values  # Heat capacity at 298.15K\n\n# Preprocessing\nX_uniq = np.unique(X)\nX_uniq = X_uniq[X_uniq != 0]\nE_uniq = np.unique(E)\nE_uniq = E_uniq[E_uniq != 0]\n\nX = label_to_one_hot(X, X_uniq)\nE = label_to_one_hot(E, E_uniq)\n\n# Parameters\nN = X.shape[-2]       # Number of nodes in the graphs\nF = X[0].shape[-1]    # Dimension of node features\nS = E[0].shape[-1]    # Dimension of edge features\nn_out = y.shape[-1]   # Dimension of the target\n\n# Train/test split\nA_train, A_test, \\\nX_train, X_test, \\\nE_train, E_test, \\\ny_train, y_test = train_test_split(A, X, E, y, test_size=0.1, random_state=0)\n\n################################################################################\n# BUILD MODEL\n################################################################################\nX_in = Input(shape=(N, F))\nA_in = Input(shape=(N, N))\nE_in = Input(shape=(N, N, S))\n\nX_1 = EdgeConditionedConv(32, activation=\'relu\')([X_in, A_in, E_in])\nX_2 = EdgeConditionedConv(32, activation=\'relu\')([X_1, A_in, E_in])\nX_3 = GlobalSumPool()(X_2)\noutput = Dense(n_out)(X_3)\n\n# Build model\nmodel = Model(inputs=[X_in, A_in, E_in], outputs=output)\noptimizer = Adam(lr=learning_rate)\nmodel.compile(optimizer=optimizer, loss=\'mse\')\nmodel.summary()\n\n################################################################################\n# FIT MODEL\n################################################################################\nmodel.fit([X_train, A_train, E_train],\n          y_train,\n          batch_size=batch_size,\n          epochs=epochs)\n\n################################################################################\n# EVALUATE MODEL\n################################################################################\nprint(\'Testing model\')\nmodel_loss = model.evaluate([X_test, A_test, E_test],\n                            y_test,\n                            batch_size=batch_size)\nprint(\'Done. Test loss: {}\'.format(model_loss))\n'"
examples/graph_prediction/qm9_disjoint.py,8,"b'""""""\nThis example shows how to perform regression of molecular properties with the\nQM9 database, using a simple GNN in disjoint mode .\nThe main training loop is written in TensorFlow, because we need to avoid the\nrestriction imposed by Keras that the input and the output have the same first\ndimension.\n""""""\n\nimport numpy as np\nimport tensorflow as tf\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.keras.layers import Input, Dense\nfrom tensorflow.keras.losses import MeanSquaredError\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.optimizers import Adam\n\nfrom spektral.datasets import qm9\nfrom spektral.layers import EdgeConditionedConv, ops, GlobalSumPool\nfrom spektral.utils import batch_iterator, numpy_to_disjoint\nfrom spektral.utils import label_to_one_hot\n\n################################################################################\n# PARAMETERS\n################################################################################\nlearning_rate = 1e-3  # Learning rate\nepochs = 10           # Number of training epochs\nbatch_size = 32       # Batch size\n\n################################################################################\n# LOAD DATA\n################################################################################\nA, X, E, y = qm9.load_data(return_type=\'numpy\',\n                           nf_keys=\'atomic_num\',\n                           ef_keys=\'type\',\n                           self_loops=False,\n                           auto_pad=False,\n                           amount=1000)  # Set to None to train on whole dataset\ny = y[[\'cv\']].values  # Heat capacity at 298.15K\n\n# Preprocessing\nX_uniq = np.unique([v for x in X for v in np.unique(x)])\nE_uniq = np.unique([v for e in E for v in np.unique(e)])\nX_uniq = X_uniq[X_uniq != 0]\nE_uniq = E_uniq[E_uniq != 0]\n\nX = [label_to_one_hot(x, labels=X_uniq) for x in X]\nE = [label_to_one_hot(e, labels=E_uniq) for e in E]\n\n# Parameters\nF = X[0].shape[-1]   # Dimension of node features\nS = E[0].shape[-1]   # Dimension of edge features\nn_out = y.shape[-1]  # Dimension of the target\n\n# Train/test split\nA_train, A_test, \\\nX_train, X_test, \\\nE_train, E_test, \\\ny_train, y_test = train_test_split(A, X, E, y, test_size=0.1, random_state=0)\n\n################################################################################\n# BUILD MODEL\n################################################################################\nX_in = Input(shape=(F,), name=\'X_in\')\nA_in = Input(shape=(None,), sparse=True, name=\'A_in\')\nE_in = Input(shape=(S,), name=\'E_in\')\nI_in = Input(shape=(), name=\'segment_ids_in\', dtype=tf.int32)\n\nX_1 = EdgeConditionedConv(32, activation=\'relu\')([X_in, A_in, E_in])\nX_2 = EdgeConditionedConv(32, activation=\'relu\')([X_1, A_in, E_in])\nX_3 = GlobalSumPool()([X_2, I_in])\noutput = Dense(n_out)(X_3)\n\n# Build model\nmodel = Model(inputs=[X_in, A_in, E_in, I_in], outputs=output)\nopt = Adam(lr=learning_rate)\nloss_fn = MeanSquaredError()\n\n\n@tf.function(\n    input_signature=(tf.TensorSpec((None, F), dtype=tf.float64),\n                     tf.SparseTensorSpec((None, None), dtype=tf.float64),\n                     tf.TensorSpec((None, S), dtype=tf.float64),\n                     tf.TensorSpec((None,), dtype=tf.int32),\n                     tf.TensorSpec((None, n_out), dtype=tf.float64)),\n    experimental_relax_shapes=True)\ndef train_step(X_, A_, E_, I_, y_):\n    with tf.GradientTape() as tape:\n        predictions = model([X_, A_, E_, I_], training=True)\n        loss = loss_fn(y_, predictions)\n        loss += sum(model.losses)\n    gradients = tape.gradient(loss, model.trainable_variables)\n    opt.apply_gradients(zip(gradients, model.trainable_variables))\n    return loss\n\n\n################################################################################\n# FIT MODEL\n################################################################################\ncurrent_batch = 0\nmodel_loss = 0\nbatches_in_epoch = np.ceil(len(A_train) / batch_size)\n\nprint(\'Fitting model\')\nbatches_train = batch_iterator([X_train, A_train, E_train, y_train],\n                               batch_size=batch_size, epochs=epochs)\nfor b in batches_train:\n    X_, A_, E_, I_ = numpy_to_disjoint(*b[:-1])\n    A_ = ops.sp_matrix_to_sp_tensor(A_)\n    y_ = b[-1]\n    outs = train_step(X_, A_, E_, I_, y_)\n\n    model_loss += outs.numpy()\n    current_batch += 1\n    if current_batch == batches_in_epoch:\n        print(\'Loss: {}\'.format(model_loss / batches_in_epoch))\n        model_loss = 0\n        current_batch = 0\n\n################################################################################\n# EVALUATE MODEL\n################################################################################\nprint(\'Testing model\')\nmodel_loss = 0\nbatches_in_epoch = np.ceil(len(A_test) / batch_size)\nbatches_test = batch_iterator([X_test, A_test, E_test, y_test], batch_size=batch_size)\nfor b in batches_test:\n    X_, A_, E_, I_ = numpy_to_disjoint(*b[:-1])\n    A_ = ops.sp_matrix_to_sp_tensor(A_)\n    y_ = b[3]\n\n    predictions = model([X_, A_, E_, I_], training=False)\n    model_loss += loss_fn(y_, predictions)\nmodel_loss /= batches_in_epoch\nprint(\'Done. Test loss: {}\'.format(model_loss))\n'"
examples/node_prediction/citation_arma.py,0,"b'""""""\nThis example implements the experiments on citation networks from the paper:\n\nGraph Neural Networks with convolutional ARMA filters (https://arxiv.org/abs/1901.01343)\nFilippo Maria Bianchi, Daniele Grattarola, Cesare Alippi, Lorenzo Livi\n""""""\n\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom tensorflow.keras.layers import Input, Dropout\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.regularizers import l2\n\nfrom spektral.datasets import citation\nfrom spektral.layers import ARMAConv\n\n# Load data\ndataset = \'cora\'\nA, X, y, train_mask, val_mask, test_mask = citation.load_data(dataset)\n\n# Parameters\nchannels = 16           # Number of channels in the first layer\niterations = 1          # Number of iterations to approximate each ARMA(1)\norder = 2               # Order of the ARMA filter (number of parallel stacks)\nshare_weights = True    # Share weights in each ARMA stack\nN = X.shape[0]          # Number of nodes in the graph\nF = X.shape[1]          # Original feature dimensionality\nn_classes = y.shape[1]  # Number of classes\ndropout = 0.5           # Dropout rate applied between layers\ndropout_skip = 0.75     # Dropout rate for the internal skip connection of ARMA\nl2_reg = 5e-5           # L2 regularization rate\nlearning_rate = 1e-2    # Learning rate\nepochs = 20000          # Number of training epochs\nes_patience = 100       # Patience for early stopping\n\n# Preprocessing operations\nfltr = ARMAConv.preprocess(A).astype(\'f4\')\nX = X.toarray()\n\n# Model definition\nX_in = Input(shape=(F, ))\nfltr_in = Input((N, ), sparse=True)\n\ngc_1 = ARMAConv(channels,\n                iterations=iterations,\n                order=order,\n                share_weights=share_weights,\n                dropout_rate=dropout_skip,\n                activation=\'elu\',\n                gcn_activation=\'elu\',\n                kernel_regularizer=l2(l2_reg))([X_in, fltr_in])\ngc_2 = Dropout(dropout)(gc_1)\ngc_2 = ARMAConv(n_classes,\n                iterations=1,\n                order=1,\n                share_weights=share_weights,\n                dropout_rate=dropout_skip,\n                activation=\'softmax\',\n                gcn_activation=None,\n                kernel_regularizer=l2(l2_reg))([gc_2, fltr_in])\n\n# Build model\nmodel = Model(inputs=[X_in, fltr_in], outputs=gc_2)\noptimizer = Adam(learning_rate=learning_rate)\nmodel.compile(optimizer=optimizer,\n              loss=\'categorical_crossentropy\',\n              weighted_metrics=[\'acc\'])\nmodel.summary()\n\n# Train model\nvalidation_data = ([X, fltr], y, val_mask)\nmodel.fit([X, fltr],\n          y,\n          sample_weight=train_mask,\n          epochs=epochs,\n          batch_size=N,\n          validation_data=validation_data,\n          shuffle=False,  # Shuffling data means shuffling the whole graph\n          callbacks=[\n              EarlyStopping(patience=es_patience,  restore_best_weights=True)\n          ])\n\n# Evaluate model\nprint(\'Evaluating model.\')\neval_results = model.evaluate([X, fltr],\n                              y,\n                              sample_weight=test_mask,\n                              batch_size=N)\nprint(\'Done.\\n\'\n      \'Test loss: {}\\n\'\n      \'Test accuracy: {}\'.format(*eval_results))\n'"
examples/node_prediction/citation_cheby.py,0,"b'""""""\nThis example implements the experiments on citation networks using convolutional\nlayers from the paper:\n\nConvolutional Neural Networks on Graphs with Fast Localized Spectral Filtering (https://arxiv.org/abs/1606.09375)\nMicha\xc3\xabl Defferrard, Xavier Bresson, Pierre Vandergheynst\n""""""\n\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom tensorflow.keras.layers import Input, Dropout\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.regularizers import l2\n\nfrom spektral.datasets import citation\nfrom spektral.layers import ChebConv\n\n# Load data\ndataset = \'cora\'\nA, X, y, train_mask, val_mask, test_mask = citation.load_data(dataset)\n\n# Parameters\nchannels = 16           # Number of channels in the first layer\nK = 2                   # Max degree of the Chebyshev polynomials\nN = X.shape[0]          # Number of nodes in the graph\nF = X.shape[1]          # Original size of node features\nn_classes = y.shape[1]  # Number of classes\ndropout = 0.5           # Dropout rate for the features\nl2_reg = 5e-4 / 2       # L2 regularization rate\nlearning_rate = 1e-2    # Learning rate\nepochs = 200            # Number of training epochs\nes_patience = 10        # Patience for early stopping\n\n# Preprocessing operations\nfltr = ChebConv.preprocess(A).astype(\'f4\')\nX = X.toarray()\n\n# Model definition\nX_in = Input(shape=(F, ))\nfltr_in = Input((N, ), sparse=True)\n\ndropout_1 = Dropout(dropout)(X_in)\ngraph_conv_1 = ChebConv(channels,\n                        K=K,\n                        activation=\'relu\',\n                        kernel_regularizer=l2(l2_reg),\n                        use_bias=False)([dropout_1, fltr_in])\ndropout_2 = Dropout(dropout)(graph_conv_1)\ngraph_conv_2 = ChebConv(n_classes,\n                        K=K,\n                        activation=\'softmax\',\n                        use_bias=False)([dropout_2, fltr_in])\n\n# Build model\nmodel = Model(inputs=[X_in, fltr_in], outputs=graph_conv_2)\noptimizer = Adam(lr=learning_rate)\nmodel.compile(optimizer=optimizer,\n              loss=\'categorical_crossentropy\',\n              weighted_metrics=[\'acc\'])\nmodel.summary()\n\n# Train model\nvalidation_data = ([X, fltr], y, val_mask)\nmodel.fit([X, fltr],\n          y,\n          sample_weight=train_mask,\n          epochs=epochs,\n          batch_size=N,\n          validation_data=validation_data,\n          shuffle=False,  # Shuffling data means shuffling the whole graph\n          callbacks=[\n              EarlyStopping(patience=es_patience,  restore_best_weights=True)\n          ])\n\n# Evaluate model\nprint(\'Evaluating model.\')\neval_results = model.evaluate([X, fltr],\n                              y,\n                              sample_weight=test_mask,\n                              batch_size=N)\nprint(\'Done.\\n\'\n      \'Test loss: {}\\n\'\n      \'Test accuracy: {}\'.format(*eval_results))\n'"
examples/node_prediction/citation_gat.py,0,"b'""""""\nThis example implements the experiments on citation networks from the paper:\n\nGraph Attention Networks (https://arxiv.org/abs/1710.10903)\nPetar Veli\xc4\x8dkovi\xc4\x87, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Li\xc3\xb2, Yoshua Bengio\n""""""\n\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom tensorflow.keras.layers import Input, Dropout\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.regularizers import l2\n\nfrom spektral.datasets import citation\nfrom spektral.layers import GraphAttention\n\n# Load data\ndataset = \'cora\'\nA, X, y, train_mask, val_mask, test_mask = citation.load_data(dataset)\n\n# Parameters\nchannels = 8            # Number of channel in each head of the first GAT layer\nn_attn_heads = 8        # Number of attention heads in first GAT layer\nN = X.shape[0]          # Number of nodes in the graph\nF = X.shape[1]          # Original size of node features\nn_classes = y.shape[1]  # Number of classes\ndropout = 0.6           # Dropout rate for the features and adjacency matrix\nl2_reg = 5e-6           # L2 regularization rate\nlearning_rate = 5e-3    # Learning rate\nepochs = 20000          # Number of training epochs\nes_patience = 100       # Patience for early stopping\n\n# Preprocessing operations\nA = A.astype(\'f4\')\nX = X.toarray()\n\n# Model definition\nX_in = Input(shape=(F, ))\nA_in = Input(shape=(N, ), sparse=True)\n\ndropout_1 = Dropout(dropout)(X_in)\ngraph_attention_1 = GraphAttention(channels,\n                                   attn_heads=n_attn_heads,\n                                   concat_heads=True,\n                                   dropout_rate=dropout,\n                                   activation=\'elu\',\n                                   kernel_regularizer=l2(l2_reg),\n                                   attn_kernel_regularizer=l2(l2_reg)\n                                   )([dropout_1, A_in])\ndropout_2 = Dropout(dropout)(graph_attention_1)\ngraph_attention_2 = GraphAttention(n_classes,\n                                   attn_heads=1,\n                                   concat_heads=False,\n                                   dropout_rate=dropout,\n                                   activation=\'softmax\',\n                                   kernel_regularizer=l2(l2_reg),\n                                   attn_kernel_regularizer=l2(l2_reg)\n                                   )([dropout_2, A_in])\n\n# Build model\nmodel = Model(inputs=[X_in, A_in], outputs=graph_attention_2)\noptimizer = Adam(lr=learning_rate)\nmodel.compile(optimizer=optimizer,\n              loss=\'categorical_crossentropy\',\n              weighted_metrics=[\'acc\'])\nmodel.summary()\n\n# Train model\nvalidation_data = ([X, A], y, val_mask)\nmodel.fit([X, A],\n          y,\n          sample_weight=train_mask,\n          epochs=epochs,\n          batch_size=N,\n          validation_data=validation_data,\n          shuffle=False,  # Shuffling data means shuffling the whole graph\n          callbacks=[\n              EarlyStopping(patience=es_patience, restore_best_weights=True)\n          ])\n\n# Evaluate model\nprint(\'Evaluating model.\')\neval_results = model.evaluate([X, A],\n                              y,\n                              sample_weight=test_mask,\n                              batch_size=N)\nprint(\'Done.\\n\'\n      \'Test loss: {}\\n\'\n      \'Test accuracy: {}\'.format(*eval_results))\n'"
examples/node_prediction/citation_gat_fast.py,3,"b'""""""\nThis script is an extension of the citation_gcn_fast.py script.\nIt shows how to train GAT (with the same experimental setting of the original\npaper), using faster training and test functions.\n""""""\n\nimport tensorflow as tf\nfrom tensorflow.keras.layers import Input, Dropout\nfrom tensorflow.keras.losses import CategoricalCrossentropy\nfrom tensorflow.keras.metrics import CategoricalAccuracy\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.regularizers import l2\n\nfrom spektral.datasets import citation\nfrom spektral.layers import GraphAttention\nfrom spektral.layers import ops\nfrom spektral.utils import tic, toc\n\n# Load data\nA, X, y, train_mask, val_mask, test_mask = citation.load_data(\'cora\')\nfltr = A.astype(\'f4\')\nfltr = ops.sp_matrix_to_sp_tensor(fltr)\nX = X.toarray()\n\n# Define model\nX_in = Input(shape=(X.shape[1], ))\nfltr_in = Input(shape=(X.shape[0], ), sparse=True)\nX_1 = Dropout(0.6)(X_in)\nX_1 = GraphAttention(8,\n                     attn_heads=8,\n                     concat_heads=True,\n                     dropout_rate=0.6,\n                     activation=\'elu\',\n                     kernel_regularizer=l2(5e-4),\n                     attn_kernel_regularizer=l2(5e-4),\n                     bias_regularizer=l2(5e-4))([X_1, fltr_in])\nX_2 = Dropout(0.6)(X_1)\nX_2 = GraphAttention(y.shape[1],\n                     attn_heads=1,\n                     concat_heads=True,\n                     dropout_rate=0.6,\n                     activation=\'softmax\',\n                     kernel_regularizer=l2(5e-4),\n                     attn_kernel_regularizer=l2(5e-4),\n                     bias_regularizer=l2(5e-4))([X_2, fltr_in])\n\n# Build model\nmodel = Model(inputs=[X_in, fltr_in], outputs=X_2)\noptimizer = Adam(lr=5e-3)\nloss_fn = CategoricalCrossentropy()\nacc_fn = CategoricalAccuracy()\n\n\n# Training step\n@tf.function\ndef train():\n    with tf.GradientTape() as tape:\n        predictions = model([X, fltr], training=True)\n        loss = loss_fn(y[train_mask], predictions[train_mask])\n        loss += sum(model.losses)\n    gradients = tape.gradient(loss, model.trainable_variables)\n    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n    return loss\n\n\n@tf.function\ndef evaluate():\n    predictions = model([X, fltr], training=False)\n    losses = []\n    accuracies = []\n    for mask in [train_mask, val_mask, test_mask]:\n        loss = loss_fn(y[mask], predictions[mask])\n        loss += sum(model.losses)\n        losses.append(loss)\n        acc = acc_fn(y[mask], predictions[mask])\n        accuracies.append(acc)\n    return losses, accuracies\n\n\nbest_val_loss = 99999\nbest_test_acc = 0\ncurrent_patience = patience = 100\ntic()\nfor epoch in range(1, 99999):\n    train()\n    l, a = evaluate()\n    print(\'Loss tr: {:.4f}, Acc tr: {:.4f}, \'\n          \'Loss va: {:.4f}, Acc va: {:.4f}, \'\n          \'Loss te: {:.4f}, Acc te: {:.4f}\'\n          .format(l[0], a[0], l[1], a[1], l[2], a[2]))\n    if l[1] < best_val_loss:\n        best_val_loss = l[1]\n        best_test_acc = a[2]\n        current_patience = patience\n        print(\'Improved\')\n    else:\n        current_patience -= 1\n        if current_patience == 0:\n            print(\'Best test acc: {}\'.format(best_test_acc))\n            break\ntoc(\'GAT ({} epochs)\'.format(epoch))\n'"
examples/node_prediction/citation_gcn.py,0,"b'""""""\nThis example implements the experiments on citation networks from the paper:\n\nSemi-Supervised Classification with Graph Convolutional Networks (https://arxiv.org/abs/1609.02907)\nThomas N. Kipf, Max Welling\n""""""\n\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom tensorflow.keras.layers import Input, Dropout\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.regularizers import l2\n\nfrom spektral.datasets import citation\nfrom spektral.layers import GraphConv\n\n# Load data\ndataset = \'cora\'\nA, X, y, train_mask, val_mask, test_mask = citation.load_data(dataset)\n\n# Parameters\nchannels = 16           # Number of channels in the first layer\nN = X.shape[0]          # Number of nodes in the graph\nF = X.shape[1]          # Original size of node features\nn_classes = y.shape[1]  # Number of classes\ndropout = 0.5           # Dropout rate for the features\nl2_reg = 5e-4 / 2       # L2 regularization rate\nlearning_rate = 1e-2    # Learning rate\nepochs = 200            # Number of training epochs\nes_patience = 10        # Patience for early stopping\n\n# Preprocessing operations\nfltr = GraphConv.preprocess(A).astype(\'f4\')\nX = X.toarray()\n\n# Model definition\nX_in = Input(shape=(F, ))\nfltr_in = Input((N, ), sparse=True)\n\ndropout_1 = Dropout(dropout)(X_in)\ngraph_conv_1 = GraphConv(channels,\n                         activation=\'relu\',\n                         kernel_regularizer=l2(l2_reg),\n                         use_bias=False)([dropout_1, fltr_in])\ndropout_2 = Dropout(dropout)(graph_conv_1)\ngraph_conv_2 = GraphConv(n_classes,\n                         activation=\'softmax\',\n                         use_bias=False)([dropout_2, fltr_in])\n\n# Build model\nmodel = Model(inputs=[X_in, fltr_in], outputs=graph_conv_2)\noptimizer = Adam(lr=learning_rate)\nmodel.compile(optimizer=optimizer,\n              loss=\'categorical_crossentropy\',\n              weighted_metrics=[\'acc\'])\nmodel.summary()\n\n# Train model\nvalidation_data = ([X, fltr], y, val_mask)\nmodel.fit([X, fltr],\n          y,\n          sample_weight=train_mask,\n          epochs=epochs,\n          batch_size=N,\n          validation_data=validation_data,\n          shuffle=False,  # Shuffling data means shuffling the whole graph\n          callbacks=[\n              EarlyStopping(patience=es_patience,  restore_best_weights=True)\n          ])\n\n# Evaluate model\nprint(\'Evaluating model.\')\neval_results = model.evaluate([X, fltr],\n                              y,\n                              sample_weight=test_mask,\n                              batch_size=N)\nprint(\'Done.\\n\'\n      \'Test loss: {}\\n\'\n      \'Test accuracy: {}\'.format(*eval_results))\n'"
examples/node_prediction/citation_gcn_fast.py,2,"b'""""""\nThis script is a proof of concept to train GCN as fast as possible and with as\nlittle lines of code as possible.\nIt uses a custom training function instead of the standard Keras fit(), and\ncan train GCN for 200 epochs in a few tenths of a second (0.32s on a GTX 1050).\nIn total, this script has 34 SLOC.\n""""""\nimport tensorflow as tf\nfrom tensorflow.keras.layers import Input, Dropout\nfrom tensorflow.keras.losses import CategoricalCrossentropy\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.regularizers import l2\n\nfrom spektral.datasets import citation\nfrom spektral.layers import GraphConv, ops\nfrom spektral.utils import tic, toc\n\n# Load data\nA, X, y, train_mask, val_mask, test_mask = citation.load_data(\'cora\')\nfltr = GraphConv.preprocess(A).astype(\'f4\')\nfltr = ops.sp_matrix_to_sp_tensor(fltr)\nX = X.toarray()\n\n# Define model\nX_in = Input(shape=(X.shape[1],))\nfltr_in = Input((X.shape[0],), sparse=True)\nX_1 = GraphConv(16, \'relu\', True, kernel_regularizer=l2(5e-4))([X_in, fltr_in])\nX_1 = Dropout(0.5)(X_1)\nX_2 = GraphConv(y.shape[1], \'softmax\', True)([X_1, fltr_in])\n\n# Build model\nmodel = Model(inputs=[X_in, fltr_in], outputs=X_2)\noptimizer = Adam(lr=1e-2)\nloss_fn = CategoricalCrossentropy()\n\n\n# Training step\n@tf.function\ndef train():\n    with tf.GradientTape() as tape:\n        predictions = model([X, fltr], training=True)\n        loss = loss_fn(y[train_mask], predictions[train_mask])\n        loss += sum(model.losses)\n    gradients = tape.gradient(loss, model.trainable_variables)\n    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n    return loss\n\n\n# Time the execution of 200 epochs of training\ntrain()  # Warm up to ignore tracing times when timing\ntic()\nfor epoch in range(1, 201):\n    train()\ntoc(\'Spektral - GCN (200 epochs)\')\n'"
examples/node_prediction/citation_simple_gc.py,0,"b'""""""\nThis example implements the experiments on citation networks from the paper:\n\nSimplifying Graph Convolutional Networks (https://arxiv.org/abs/1902.07153)\nFelix Wu, Tianyi Zhang, Amauri Holanda de Souza Jr., Christopher Fifty, Tao Yu, Kilian Q. Weinberger\n""""""\n\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom tensorflow.keras.layers import Input\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.regularizers import l2\n\nfrom spektral.datasets import citation\nfrom spektral.layers import GraphConv\nfrom spektral.utils.convolution import localpooling_filter\n\n# Load data\ndataset = \'cora\'\nA, X, y, train_mask, val_mask, test_mask = citation.load_data(dataset)\n\n# Parameters\nK = 2                   # Degree of propagation\nN = X.shape[0]          # Number of nodes in the graph\nF = X.shape[1]          # Original size of node features\nn_classes = y.shape[1]  # Number of classes\nl2_reg = 5e-6           # L2 regularization rate\nlearning_rate = 0.2     # Learning rate\nepochs = 20000          # Number of training epochs\nes_patience = 200       # Patience for early stopping\n\n# Preprocessing operations\nfltr = localpooling_filter(A).astype(\'f4\')\nX = X.toarray()\n\n# Pre-compute propagation\nfor i in range(K - 1):\n    fltr = fltr.dot(fltr)\nfltr.sort_indices()\n\n# Model definition\nX_in = Input(shape=(F, ))\nfltr_in = Input((N, ), sparse=True)\noutput = GraphConv(n_classes,\n                   activation=\'softmax\',\n                   kernel_regularizer=l2(l2_reg),\n                   use_bias=False)([X_in, fltr_in])\n\n# Build model\nmodel = Model(inputs=[X_in, fltr_in], outputs=output)\noptimizer = Adam(lr=learning_rate)\nmodel.compile(optimizer=optimizer,\n              loss=\'categorical_crossentropy\',\n              weighted_metrics=[\'acc\'])\nmodel.summary()\n\n# Train model\nvalidation_data = ([X, fltr], y, val_mask)\nmodel.fit([X, fltr],\n          y,\n          sample_weight=train_mask,\n          epochs=epochs,\n          batch_size=N,\n          validation_data=validation_data,\n          shuffle=False,  # Shuffling data means shuffling the whole graph\n          callbacks=[\n              EarlyStopping(patience=es_patience,  restore_best_weights=True)\n          ])\n\n# Evaluate model\nprint(\'Evaluating model.\')\neval_results = model.evaluate([X, fltr],\n                              y,\n                              sample_weight=test_mask,\n                              batch_size=N)\nprint(\'Done.\\n\'\n      \'Test loss: {}\\n\'\n      \'Test accuracy: {}\'.format(*eval_results))\n'"
examples/node_prediction/ogbn-proteins_gcn.py,0,"b'""""""\nThis example implements the same GCN example for node classification provided\nwith the [Open Graph Benchmark](https://ogb.stanford.edu).\nSee https://github.com/snap-stanford/ogb/blob/master/examples/nodeproppred/proteins/full_batch.py\nfor the reference implementation.\n""""""\nimport numpy as np\nfrom ogb.nodeproppred import NodePropPredDataset, Evaluator\nfrom tensorflow.keras.layers import Input\nfrom tensorflow.keras.losses import BinaryCrossentropy\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.optimizers import Adam\nfrom tqdm import tqdm\n\nfrom spektral.datasets import ogb\nfrom spektral.layers import GraphConv\n\n\ndef evaluate(X, fltr, y, model, masks, evaluator):\n    p = model.predict_on_batch([X, fltr]).numpy()\n    tr_mask, va_mask, te_mask = masks\n    tr_auc = evaluator.eval({\'y_true\': y[tr_mask],\n                             \'y_pred\': p[tr_mask]})[\'rocauc\']\n    va_auc = evaluator.eval({\'y_true\': y[va_mask],\n                             \'y_pred\': p[va_mask]})[\'rocauc\']\n    te_auc = evaluator.eval({\'y_true\': y[te_mask],\n                             \'y_pred\': p[te_mask]})[\'rocauc\']\n    return tr_auc, va_auc, te_auc\n\n\n# Load data\ndataset_name = \'ogbn-proteins\'\ndataset = NodePropPredDataset(dataset_name)\nevaluator = Evaluator(dataset_name)\ngraph, y = dataset[0]\nX, A, _ = ogb.graph_to_numpy(graph)\nN = A.shape[0]\n\n# Data splits\nidxs = dataset.get_idx_split()\ntr_idx, va_idx, te_idx = idxs[""train""], idxs[""valid""], idxs[""test""]\ntr_mask = np.zeros(N, dtype=bool)\ntr_mask[tr_idx] = True\nva_mask = np.zeros(N, dtype=bool)\nva_mask[va_idx] = True\nte_mask = np.zeros(N, dtype=bool)\nte_mask[te_idx] = True\nmasks = [tr_mask, va_mask, te_mask]\n\n# Parameters\nchannels = 256\nlearning_rate = 1e-2\nepochs = 200\nes_patience = 200\nF = X.shape[1]\nn_classes = y.shape[1]\n\n# Preprocessing operations\nfltr = GraphConv.preprocess(A).astype(\'f4\')\n\n# Model definition\nX_in = Input(shape=(F, ))\nfltr_in = Input((N, ), sparse=True)\ngraph_conv_1 = GraphConv(channels, activation=\'relu\')([X_in, fltr_in])\ngraph_conv_2 = GraphConv(channels, activation=\'relu\')([graph_conv_1, fltr_in])\ngraph_conv_3 = GraphConv(n_classes)([graph_conv_2, fltr_in])\n\n# Build model\nmodel = Model(inputs=[X_in, fltr_in], outputs=graph_conv_3)\noptimizer = Adam(lr=learning_rate)\nmodel.compile(optimizer=optimizer, loss=BinaryCrossentropy(from_logits=True))\nmodel.summary()\n\n# Train model\nfor i in tqdm(range(1, 1 + epochs)):\n    tr_loss = model.train_on_batch([X, fltr], y, sample_weight=tr_mask)\n    tr_auc, va_auc, te_auc = evaluate(X, fltr, y, model, masks, evaluator)\n    tqdm.write(\n        \'Ep. {} - Loss: {:.3f} - AUC: {:.3f} - Val AUC: {:.3f} - Test AUC: {:.3f}\'\n        .format(i, tr_loss, tr_auc, va_auc, te_auc)\n    )\n\n# Evaluate model\nprint(\'Evaluating model.\')\nte_loss = model.test_on_batch([X, fltr], y, sample_weight=te_mask)\ntr_auc, va_auc, te_auc = evaluate(X, fltr, y, model, masks, evaluator)\nprint(\'Done! Loss: {:.2f} - Test AUC: {:.3f}\'.format(te_loss, te_auc))\n'"
examples/other/graph_signal_classification_mnist.py,1,"b""import tensorflow as tf\nfrom tensorflow.keras import Input, Model\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom tensorflow.keras.layers import Dense, Flatten\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.regularizers import l2\n\nfrom spektral.datasets import mnist\nfrom spektral.layers import GraphConv\nfrom spektral.layers.ops import sp_matrix_to_sp_tensor\n\ntf.compat.v1.disable_eager_execution()\n\n# Parameters\nl2_reg = 5e-4         # Regularization rate for l2\nlearning_rate = 1e-3  # Learning rate for SGD\nbatch_size = 32       # Batch size\nepochs = 1000         # Number of training epochs\nes_patience = 10      # Patience fot early stopping\n\n# Load data\nX_train, y_train, X_val, y_val, X_test, y_test, A = mnist.load_data()\nX_train, X_val, X_test = X_train[..., None], X_val[..., None], X_test[..., None]\nN = X_train.shape[-2]      # Number of nodes in the graphs\nF = X_train.shape[-1]      # Node features dimensionality\nn_out = 10                 # Dimension of the target\n\nfltr = GraphConv.preprocess(A)\n\n# Model definition\nX_in = Input(shape=(N, F))\n# Pass A as a fixed tensor, otherwise Keras will complain about inputs of\n# different rank.\nA_in = Input(tensor=sp_matrix_to_sp_tensor(fltr))\n\ngraph_conv = GraphConv(32,\n                       activation='elu',\n                       kernel_regularizer=l2(l2_reg))([X_in, A_in])\ngraph_conv = GraphConv(32,\n                       activation='elu',\n                       kernel_regularizer=l2(l2_reg))([graph_conv, A_in])\nflatten = Flatten()(graph_conv)\nfc = Dense(512, activation='relu')(flatten)\noutput = Dense(n_out, activation='softmax')(fc)\n\n# Build model\nmodel = Model(inputs=[X_in, A_in], outputs=output)\noptimizer = Adam(lr=learning_rate)\nmodel.compile(optimizer=optimizer,\n              loss='sparse_categorical_crossentropy',\n              metrics=['acc'])\nmodel.summary()\n\n# Train model\nvalidation_data = (X_val, y_val)\nmodel.fit(X_train,\n          y_train,\n          batch_size=batch_size,\n          validation_data=validation_data,\n          epochs=epochs,\n          callbacks=[\n              EarlyStopping(patience=es_patience, restore_best_weights=True)\n          ])\n\n# Evaluate model\nprint('Evaluating model.')\neval_results = model.evaluate(X_test,\n                              y_test,\n                              batch_size=batch_size)\nprint('Done.\\n'\n      'Test loss: {}\\n'\n      'Test acc: {}'.format(*eval_results))\n"""
examples/other/node_clustering_mincut.py,3,"b'""""""\nThis example implements the experiments for node clustering on citation networks\nfrom the paper:\n\nMincut pooling in Graph Neural Networks (https://arxiv.org/abs/1907.00481)\nFilippo Maria Bianchi, Daniele Grattarola, Cesare Alippi\n""""""\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport tensorflow as tf\nfrom sklearn.metrics.cluster import v_measure_score, homogeneity_score, completeness_score\nfrom tensorflow.keras.layers import Input\nfrom tensorflow.keras.models import Model\nfrom tqdm import tqdm\n\nfrom spektral.datasets import citation\nfrom spektral.layers.convolutional import GraphConvSkip\nfrom spektral.layers.ops import sp_matrix_to_sp_tensor\nfrom spektral.layers.pooling import MinCutPool\nfrom spektral.utils.convolution import normalized_adjacency\n\n\n@tf.function\ndef train_step(inputs):\n    with tf.GradientTape() as tape:\n        _, S_pool = model(inputs, training=True)\n        loss = sum(model.losses)\n    gradients = tape.gradient(loss, model.trainable_variables)\n    opt.apply_gradients(zip(gradients, model.trainable_variables))\n    return model.losses[0], model.losses[1], S_pool\n\n\nnp.random.seed(1)\nepochs = 5000  # Training iterations\nlr = 5e-4      # Learning rate\n\n################################################################################\n# LOAD DATASET\n################################################################################\nA, X, y, _, _, _ = citation.load_data(\'cora\')\nA_norm = normalized_adjacency(A)\nX = X.todense()\nF = X.shape[-1]\ny = np.argmax(y, axis=-1)\nn_clusters = y.max() + 1\n\n################################################################################\n# MODEL\n################################################################################\nX_in = Input(shape=(F,), name=\'X_in\')\nA_in = Input(shape=(None, ), name=\'A_in\', sparse=True)\n\nX_1 = GraphConvSkip(16, activation=\'elu\')([X_in, A_in])\nX_1, A_1, S = MinCutPool(n_clusters, return_mask=True)([X_1, A_in])\n\nmodel = Model([X_in, A_in], [X_1, S])\n\n################################################################################\n# TRAINING\n################################################################################\n# Setup\ninputs = [X, sp_matrix_to_sp_tensor(A_norm)]\nopt = tf.keras.optimizers.Adam(learning_rate=lr)\n\n# Fit model\nloss_history = []\nnmi_history = []\nfor _ in tqdm(range(epochs)):\n    outs = train_step(inputs)\n    outs = [o.numpy() for o in outs]\n    loss_history.append((outs[0], outs[1], (outs[0] + outs[1])))\n    s = np.argmax(outs[2], axis=-1)\n    nmi_history.append(v_measure_score(y, s))\nloss_history = np.array(loss_history)\n\n################################################################################\n# RESULTS\n################################################################################\n_, S_ = model(inputs, training=False)\ns = np.argmax(S_, axis=-1)\nhom = homogeneity_score(y, s)\ncom = completeness_score(y, s)\nnmi = v_measure_score(y, s)\nprint(\'Homogeneity: {:.3f}; Completeness: {:.3f}; NMI: {:.3f}\'.format(hom, com, nmi))\n\n# Plots\nplt.figure(figsize=(10, 5))\n\nplt.subplot(121)\nplt.plot(loss_history[:, 0], label=\'MinCUT loss\')\nplt.plot(loss_history[:, 1], label=\'Ortho. loss\')\nplt.plot(loss_history[:, 2], label=\'Total loss\')\nplt.legend()\nplt.ylabel(\'Loss\')\nplt.xlabel(\'Iteration\')\n\nplt.subplot(122)\nplt.plot(nmi_history, label=\'NMI\')\nplt.legend()\nplt.ylabel(\'NMI\')\nplt.xlabel(\'Iteration\')\n\nplt.show()\n'"
spektral/datasets/__init__.py,0,b'from . import citation\nfrom . import delaunay\nfrom . import graphsage\nfrom . import mnist\nfrom . import ogb\nfrom . import qm9\nfrom . import tud\n'
spektral/datasets/citation.py,0,"b'""""""\nThe MIT License\n\nCopyright (c) 2016 Thomas Kipf\n\nPermission is hereby granted, free of charge, to any person obtaining a copy\nof this software and associated documentation files (the ""Software""), to deal\nin the Software without restriction, including without limitation the rights\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\ncopies of the Software, and to permit persons to whom the Software is\nfurnished to do so, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in\nall copies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN\nTHE SOFTWARE.\n\nThis code was taken almost verbatim from https://github.com/tkipf/gcn/ and\nadapted to work in Spektral.\n""""""\nimport os\n\nimport networkx as nx\nimport numpy as np\nimport requests\nimport scipy.sparse as sp\nfrom sklearn.model_selection import train_test_split\n\nfrom spektral.utils.io import load_binary\n\nDATA_PATH = os.path.expanduser(\'~/.spektral/datasets/\')\nAVAILABLE_DATASETS = {\'cora\', \'citeseer\', \'pubmed\'}\n\n\ndef load_data(dataset_name=\'cora\', normalize_features=True, random_split=False):\n    """"""\n    Loads a citation dataset (Cora, Citeseer or Pubmed) using the ""Planetoid""\n    splits intialliy defined in [Yang et al. (2016)](https://arxiv.org/abs/1603.08861).\n    The train, test, and validation splits are given as binary masks.\n\n    Node attributes are bag-of-words vectors representing the most common words\n    in the text document associated to each node.\n    Two papers are connected if either one cites the other.\n    Labels represent the class of the paper.\n\n    :param dataset_name: name of the dataset to load (`\'cora\'`, `\'citeseer\'`, or\n    `\'pubmed\'`);\n    :param normalize_features: if True, the node features are normalized;\n    :param random_split: if True, return a randomized split (20 nodes per class\n    for training, 30 nodes per class for validation and the remaining nodes for\n    testing, [Shchur et al. (2018)](https://arxiv.org/abs/1811.05868)).\n    :return:\n        - Adjacency matrix;\n        - Node features;\n        - Labels;\n        - Three binary masks for train, validation, and test splits.\n    """"""\n    if dataset_name not in AVAILABLE_DATASETS:\n        raise ValueError(\'Available datasets: {}\'.format(AVAILABLE_DATASETS))\n\n    if not os.path.exists(DATA_PATH + dataset_name):\n        _download_data(dataset_name)\n\n    print(\'Loading {} dataset\'.format(dataset_name))\n\n    names = [\'x\', \'y\', \'tx\', \'ty\', \'allx\', \'ally\', \'graph\']\n    objects = []\n    data_path = os.path.join(DATA_PATH, dataset_name)\n    for n in names:\n        filename = ""{}/ind.{}.{}"".format(data_path, dataset_name, n)\n        objects.append(load_binary(filename))\n\n    x, y, tx, ty, allx, ally, graph = tuple(objects)\n    adj = nx.adjacency_matrix(nx.from_dict_of_lists(graph))\n    test_idx_reorder = _parse_index_file(""{}/ind.{}.test.index"".format(data_path, dataset_name))\n    test_idx_range = np.sort(test_idx_reorder)\n\n    if dataset_name == \'citeseer\':\n        test_idx_range_full = range(min(test_idx_reorder),\n                                    max(test_idx_reorder) + 1)\n        tx_extended = sp.lil_matrix((len(test_idx_range_full), x.shape[1]))\n        tx_extended[test_idx_range - min(test_idx_range), :] = tx\n        tx = tx_extended\n        ty_extended = np.zeros((len(test_idx_range_full), y.shape[1]))\n        ty_extended[test_idx_range - min(test_idx_range), :] = ty\n        ty = ty_extended\n\n    features = sp.vstack((allx, tx)).tolil()\n    features[test_idx_reorder, :] = features[test_idx_range, :]\n\n    # Row-normalize the features\n    if normalize_features:\n        print(\'Pre-processing node features\')\n        features = _preprocess_features(features)\n\n    labels = np.vstack((ally, ty))\n    labels[test_idx_reorder, :] = labels[test_idx_range, :]\n\n    # Data splits\n    if random_split:\n        indices = np.arange(labels.shape[0])\n        n_classes = labels.shape[1]\n        idx_train, idx_test, y_train, y_test = train_test_split(indices, labels, train_size=20 * n_classes, stratify=labels)\n        idx_val, idx_test, y_val, y_test = train_test_split(idx_test, y_test, train_size=30 * n_classes, stratify=y_test)\n    else:\n        idx_test = test_idx_range.tolist()\n        idx_train = range(len(y))\n        idx_val = range(len(y), len(y) + 500)\n\n    train_mask = _sample_mask(idx_train, labels.shape[0])\n    val_mask = _sample_mask(idx_val, labels.shape[0])\n    test_mask = _sample_mask(idx_test, labels.shape[0])\n\n    return adj, features, labels, train_mask, val_mask, test_mask\n\n\ndef _parse_index_file(filename):\n    index = []\n    for line in open(filename):\n        index.append(int(line.strip()))\n    return index\n\n\ndef _sample_mask(idx, l):\n    mask = np.zeros(l)\n    mask[idx] = 1\n    return np.array(mask, dtype=np.bool)\n\n\ndef _preprocess_features(features):\n    rowsum = np.array(features.sum(1))\n    r_inv = np.power(rowsum, -1).flatten()\n    r_inv[np.isinf(r_inv)] = 0.\n    r_mat_inv = sp.diags(r_inv)\n    features = r_mat_inv.dot(features)\n    return features\n\n\ndef _download_data(dataset_name):\n    names = [\'x\', \'y\', \'tx\', \'ty\', \'allx\', \'ally\', \'graph\', \'test.index\']\n\n    os.makedirs(DATA_PATH + dataset_name + \'/\')\n    data_url = \'https://github.com/tkipf/gcn/raw/master/gcn/data/\'\n\n    print(\'Downloading \' + dataset_name + \'from \' + data_url)\n    for n in names:\n        f_name = \'ind.\' + dataset_name + \'.\' + n\n        req = requests.get(data_url + f_name)\n        with open(DATA_PATH + dataset_name + \'/\' + f_name, \'wb\') as out_file:\n            out_file.write(req.content)\n'"
spektral/datasets/delaunay.py,0,"b'import numpy as np\nfrom scipy.spatial import Delaunay\n\nfrom spektral.utils import label_to_one_hot, numpy_to_nx\n\nRETURN_TYPES = {\'numpy\', \'networkx\'}\n\n\ndef generate_data(classes=0, n_samples_in_class=1000, n_nodes=7, support_low=0.,\n                  support_high=10., drift_amount=1.0, one_hot_labels=True,\n                  support=None, seed=None, return_type=\'numpy\'):\n    """"""\n    Generates a dataset of Delaunay triangulations as described by\n    [Zambon et al. (2017)](https://arxiv.org/abs/1706.06941).\n\n    Node attributes are the 2D coordinates of the points.\n    Two nodes are connected if they share an edge in the Delaunay triangulation.\n    Labels represent the class of the graph (0 to 20, each class index i\n    represent the ""difficulty"" of the classification problem 0 v. i. In other\n    words, the higher the class index, the more similar the class is to class 0).\n\n    :param classes: indices of the classes to load (integer, or list of integers\n    between 0 and 20);\n    :param n_samples_in_class: number of generated samples per class;\n    :param n_nodes: number of nodes in a graph;\n    :param support_low: lower bound of the uniform distribution from which the \n    support is generated;\n    :param support_high: upper bound of the uniform distribution from which the \n    support is generated;\n    :param drift_amount: coefficient to control the amount of change between \n    classes;\n    :param one_hot_labels: one-hot encode dataset labels;\n    :param support: custom support to use instead of generating it randomly; \n    :param seed: random numpy seed;\n    :param return_type: `\'numpy\'` or `\'networkx\'`, data format to return;\n    :return:\n    - if `return_type=\'numpy\'`, the adjacency matrix, node features, and\n    an array containing labels;\n    - if `return_type=\'networkx\'`, a list of graphs in Networkx format, and an\n    array containing labels;\n    """"""\n    if return_type not in RETURN_TYPES:\n        raise ValueError(\'Possible return_type: {}\'.format(RETURN_TYPES))\n\n    if isinstance(classes, int):\n        classes = [classes]\n\n    if max(classes) > 20 or min(classes) < 0:\n        raise ValueError(\'Class indices must be between 0 and 20\')\n\n    r_classes = list(reversed(classes))\n    if r_classes[-1] == 0:\n        r_classes.insert(0, r_classes.pop(-1))\n\n    # Support points\n    np.random.seed(seed)\n    if support is None:\n        support = np.random.uniform(support_low, support_high, (1, n_nodes, 2))\n    else:\n        try:\n            assert support.shape == (1, n_nodes, 2)\n        except AssertionError:\n            print(\'The given support doesn\\\'t have shape (1, n_nodes, 2) as\'\n                  \'expected. Attempting to reshape.\')\n            support = support.reshape(1, n_nodes, 2)\n\n    # Compute node features\n    node_features = []\n    # Other node features\n    for idx, i in enumerate(r_classes):\n        if i == 0:\n            concept_0 = np.repeat(support, n_samples_in_class, 0)\n            noise_0 = np.random.normal(0, 1, (n_samples_in_class, n_nodes, 2))\n            class_0 = concept_0 + noise_0\n            node_features.append(class_0)\n        else:\n            radius = 10. * ((2./3.) ** (drift_amount * (i - 1)))\n            phase = np.random.uniform(0, 2 * np.pi, (n_nodes, 1))\n            perturb_i_x = radius * np.cos(phase)\n            perturb_i_y = radius * np.sin(phase)\n            perturb_i = np.concatenate((perturb_i_x, perturb_i_y), axis=-1)\n            support_i = support + perturb_i\n            concept_i = np.repeat(support_i, n_samples_in_class, 0)\n            noise_i = np.random.normal(0, 1, (n_samples_in_class, n_nodes, 2))\n            class_i = concept_i + noise_i\n            node_features.append(class_i)\n    node_features = np.array(node_features).reshape((-1, n_nodes, 2))\n\n    # Compute adjacency matrices\n    adjacency = []\n    for nf in node_features:\n        adj = _compute_adj(nf)\n        adjacency.append(adj)\n    adjacency = np.array(adjacency)\n\n    # Compute labels\n    labels = np.repeat(classes, n_samples_in_class)\n    if one_hot_labels:\n        labels = label_to_one_hot(labels, labels=classes)\n\n    if return_type is \'numpy\':\n        return adjacency, node_features, labels\n    elif return_type is \'networkx\':\n        graphs = numpy_to_nx(adjacency, node_features=node_features, nf_name=\'coords\')\n        return graphs, labels\n    else:\n        raise NotImplementedError\n\n\ndef _compute_adj(x):\n    """"""\n    Computes the Delaunay triangulation of the given points\n    :param x: array of shape (num_nodes, 2)\n    :return: the computed adjacency matrix\n    """"""\n    tri = Delaunay(x)\n    edges_explicit = np.concatenate((tri.vertices[:, :2],\n                                     tri.vertices[:, 1:],\n                                     tri.vertices[:, ::2]), axis=0)\n    adj = np.zeros((x.shape[0], x.shape[0]))\n    adj[edges_explicit[:, 0], edges_explicit[:, 1]] = 1.\n    return np.clip(adj + adj.T, 0, 1)\n'"
spektral/datasets/graphsage.py,0,"b'""""""\nThe MIT License\n\nCopyright (c) 2017 William L. Hamilton, Rex Ying\n\nPermission is hereby granted, free of charge, to any person obtaining a copy\nof this software and associated documentation files (the ""Software""), to deal\nin the Software without restriction, including without limitation the rights\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\ncopies of the Software, and to permit persons to whom the Software is\nfurnished to do so, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in\nall copies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN\nTHE SOFTWARE.\n\nPortions of this code base were orginally forked from: https://github.com/tkipf/gcn, which is under the following License:\n\nCopyright (c) 2016 Thomas Kipf\n\nPermission is hereby granted, free of charge, to any person obtaining a copy\nof this software and associated documentation files (the ""Software""), to deal\nin the Software without restriction, including without limitation the rights\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\ncopies of the Software, and to permit persons to whom the Software is\nfurnished to do so, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in\nall copies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN\nTHE SOFTWARE.\n\nNote from Spektral\'s authors: the code by Hamilton et al. was adapted and the\npresent version is not a verbatim copy.\n""""""\n\nimport json\nimport os\nimport zipfile\n\nimport numpy as np\nimport requests\nimport scipy.sparse as sp\nfrom networkx.readwrite import json_graph\n\nDATA_PATH = os.path.expanduser(\'~/.spektral/datasets/\')\nAVAILABLE_DATASETS = {\'ppi\', \'reddit\'}\n\n\ndef load_data(dataset_name, max_degree=-1, normalize_features=True):\n    """"""\n    Loads one of the datasets (PPI or Reddit) used in\n    [Hamilton & Ying (2017)](https://arxiv.org/abs/1706.02216).\n\n    The PPI dataset (originally [Stark et al. (2006)](https://www.ncbi.nlm.nih.gov/pubmed/16381927))\n    for inductive node classification uses positional gene sets, motif gene sets\n    and immunological signatures as features and gene ontology sets as labels.\n\n    The Reddit dataset consists of a graph made of Reddit posts in the month of\n    September, 2014. The label for each node is the community that a\n    post belongs to. The graph is built by sampling 50 large communities and\n    two nodes are connected if the same user commented on both. Node features\n    are obtained by concatenating the average GloVe CommonCrawl vectors of\n    the title and comments, the post\'s score and the number of comments.\n\n    The train, test, and validation splits are returned as binary masks.\n\n    :param dataset_name: name of the dataset to load (`\'ppi\'`, or `\'reddit\'`);\n    :param max_degree: int, if positive, subsample edges so that each node has\n    the specified maximum degree.\n    :param normalize_features: if True, the node features are normalized;\n    :return:\n        - Adjacency matrix;\n        - Node features;\n        - Labels;\n        - Three binary masks for train, validation, and test splits.\n    """"""\n    prefix = DATA_PATH + dataset_name + \'/\' + dataset_name\n    if max_degree == -1:\n        npz_file = prefix + \'.npz\'\n    else:\n        npz_file = \'{}_deg{}.npz\'.format(prefix, max_degree)\n\n    if not os.path.exists(prefix + ""-G.json""):\n        _download_data(dataset_name)\n\n    if os.path.exists(npz_file):\n        # Data already prepreccesed\n        print(\'Loading pre-processed dataset {}.\'.format(npz_file))\n        data = np.load(npz_file)\n        feats = data[\'feats\']\n        labels = data[\'labels\']\n        train_mask = data[\'train_mask\']\n        val_mask = data[\'val_mask\']\n        test_mask = data[\'test_mask\']\n        full_adj = sp.csr_matrix(\n            (data[\'full_adj_data\'], data[\'full_adj_indices\'], data[\'full_adj_indptr\']),\n            shape=data[\'full_adj_shape\']\n        )\n    else:\n        # Preprocess data\n        print(\'Loading dataset.\')\n        G_data = json.load(open(prefix + ""-G.json""))\n        G = json_graph.node_link_graph(G_data)\n        feats = np.load(prefix + ""-feats.npy"").astype(np.float32)\n        id_map = json.load(open(prefix + ""-id_map.json""))\n        if list(id_map.keys())[0].isdigit():\n            conversion = lambda n: int(n)\n        else:\n            conversion = lambda n: n\n        id_map = {conversion(k): int(v) for k, v in id_map.items()}\n        class_map = json.load(open(prefix + ""-class_map.json""))\n        if isinstance(list(class_map.values())[0], list):\n            lab_conversion = lambda n: n\n        else:\n            lab_conversion = lambda n: int(n)\n\n        class_map = {conversion(k): lab_conversion(v) for k, v in class_map.items()}\n\n        # Remove all nodes that do not have val/test annotations\n        # (necessary because of networkx weirdness with the Reddit data)\n        broken_count = 0\n        to_remove = []\n        for node in G.nodes():\n            if node not in id_map:\n                to_remove.append(node)\n                broken_count += 1\n        for node in to_remove:\n            G.remove_node(node)\n        print(\n            ""Removed {:d} nodes that lacked proper annotations due to networkx versioning issues""\n            .format(broken_count)\n        )\n\n        # Construct adjacency matrix\n        edges = []\n        for edge in G.edges():\n            if edge[0] in id_map and edge[1] in id_map:\n                edges.append((id_map[edge[0]], id_map[edge[1]]))\n        print(\'{} edges\'.format(len(edges)))\n        num_data = len(id_map)\n\n        # Subsample edges (optional)\n        if max_degree > -1:\n            print(\'Subsampling edges.\')\n            edges = _subsample_edges(edges, num_data, max_degree)\n\n        # Get train/val/test indexes\n        val_data = np.array([id_map[n] for n in G.nodes()\n                             if G.nodes[n][\'val\']], dtype=np.int32)\n        test_data = np.array([id_map[n] for n in G.nodes()\n                              if G.nodes[n][\'test\']], dtype=np.int32)\n        train_mask = np.ones((num_data), dtype=np.bool)\n        train_mask[val_data] = False\n        train_mask[test_data] = False\n        val_mask = np.zeros((num_data), dtype=np.bool)\n        val_mask[val_data] = True\n        test_mask = np.zeros((num_data), dtype=np.bool)\n        test_mask[test_data] = True\n\n        edges = np.array(edges, dtype=np.int32)\n\n        def _get_adj(edges):\n            adj = sp.csr_matrix((np.ones((edges.shape[0]), dtype=np.float32),\n                                 (edges[:, 0], edges[:, 1])), shape=(num_data, num_data))\n            adj = adj.maximum(adj.transpose())\n            return adj\n\n        full_adj = _get_adj(edges)\n\n        # Z-score on features (optional)\n        if normalize_features:\n            from sklearn.preprocessing import StandardScaler\n            train_ids = np.array([id_map[n] for n in G.nodes()\n                                  if not G.nodes[n][\'val\'] and not G.nodes[n][\'test\']])\n            train_feats = feats[train_ids]\n            scaler = StandardScaler()\n            scaler.fit(train_feats)\n            feats = scaler.transform(feats)\n\n        # Process labels\n        if isinstance(list(class_map.values())[0], list):\n            num_classes = len(list(class_map.values())[0])\n            labels = np.zeros((num_data, num_classes), dtype=np.float32)\n            for k in class_map.keys():\n                labels[id_map[k], :] = np.array(class_map[k])\n        else:\n            num_classes = len(set(class_map.values()))\n            labels = np.zeros((num_data, num_classes), dtype=np.float32)\n            for k in class_map.keys():\n                labels[id_map[k], class_map[k]] = 1\n\n        with open(npz_file, \'wb\') as fwrite:\n            print(\'Saving {} edges\'.format(full_adj.nnz))\n            np.savez(fwrite, num_data=num_data,\n                     full_adj_data=full_adj.data, full_adj_indices=full_adj.indices, full_adj_indptr=full_adj.indptr,\n                     full_adj_shape=full_adj.shape,\n                     feats=feats,\n                     labels=labels,\n                     train_mask=train_mask, val_mask=val_mask, test_mask=test_mask)\n\n    return full_adj, feats, labels, train_mask, val_mask, test_mask\n\n\ndef _download_data(dataset_name):\n    print(\'Dowloading \' + dataset_name + \' dataset.\')\n    if dataset_name == \'ppi\':\n        data_url = \'http://snap.stanford.edu/graphsage/ppi.zip\'\n    elif dataset_name == \'reddit\':\n        data_url = \'http://snap.stanford.edu/graphsage/reddit.zip\'\n    else:\n        raise ValueError(\'dataset_name must be one of: {}\'.format(AVAILABLE_DATASETS))\n    req = requests.get(data_url)\n\n    os.makedirs(DATA_PATH, exist_ok=True)\n    with open(DATA_PATH + dataset_name + \'.zip\', \'wb\') as out_file:\n        out_file.write(req.content)\n    with zipfile.ZipFile(DATA_PATH + dataset_name + \'.zip\', \'r\') as zip_ref:\n        zip_ref.extractall(DATA_PATH)\n\n\ndef _subsample_edges(edges, num_data, max_degree):\n    edges = np.array(edges, dtype=np.int32)\n    np.random.shuffle(edges)\n    degree = np.zeros(num_data, dtype=np.int32)\n\n    new_edges = []\n    for e in edges:\n        if degree[e[0]] < max_degree and degree[e[1]] < max_degree:\n            new_edges.append((e[0], e[1]))\n            degree[e[0]] += 1\n            degree[e[1]] += 1\n    return new_edges\n'"
spektral/datasets/mnist.py,0,"b'""""""\nThis code is largely take from M. Defferrard\'s Github\nhttps://github.com/mdeff/cnn_graph/blob/master/nips2016/mnist.ipynb.\n""""""\n\nimport numpy as np\nimport scipy.sparse as sp\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.neighbors import kneighbors_graph\nfrom tensorflow.keras.datasets import mnist as m\n\nMNIST_SIZE = 28\n\n\ndef load_data(k=8, noise_level=0.0):\n    """"""\n    Loads the MNIST dataset and a K-NN graph to perform graph signal\n    classification, as described by [Defferrard et al. (2016)](https://arxiv.org/abs/1606.09375).\n    The K-NN graph is statically determined from a regular grid of pixels using\n    the 2d coordinates.\n\n    The node features of each graph are the MNIST digits vectorized and rescaled\n    to [0, 1].\n    Two nodes are connected if they are neighbours according to the K-NN graph.\n    Labels are the MNIST class associated to each sample.\n\n    :param k: int, number of neighbours for each node;\n    :param noise_level: fraction of edges to flip (from 0 to 1 and vice versa);\n\n    :return:\n        - X_train, y_train: training node features and labels;\n        - X_val, y_val: validation node features and labels;\n        - X_test, y_test: test node features and labels;\n        - A: adjacency matrix of the grid;\n    """"""\n    A = _mnist_grid_graph(k)\n    A = _flip_random_edges(A, noise_level).astype(np.float32)\n\n    (X_train, y_train), (X_test, y_test) = m.load_data()\n    X_train, X_test = X_train / 255.0, X_test / 255.0\n    X_train = X_train.reshape(-1, MNIST_SIZE ** 2)\n    X_test = X_test.reshape(-1, MNIST_SIZE ** 2)\n\n    X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=10000)\n\n    return X_train, y_train, X_val, y_val, X_test, y_test, A\n\n\ndef _grid_coordinates(side):\n    """"""\n    Returns 2D coordinates for a square grid of equally spaced nodes.\n    :param side: int, the side of the grid (i.e., the grid has side * side nodes).\n    :return: np.array of shape (side * side, 2).\n    """"""\n    M = side ** 2\n    x = np.linspace(0, 1, side, dtype=np.float32)\n    y = np.linspace(0, 1, side, dtype=np.float32)\n    xx, yy = np.meshgrid(x, y)\n    z = np.empty((M, 2), np.float32)\n    z[:, 0] = xx.reshape(M)\n    z[:, 1] = yy.reshape(M)\n    return z\n\n\ndef _get_adj_from_data(X, k, **kwargs):\n    """"""\n    Computes adjacency matrix of a K-NN graph from the given data.\n    :param X: rank 1 np.array, the 2D coordinates of pixels on the grid.\n    :param kwargs: kwargs for sklearn.neighbors.kneighbors_graph (see docs\n    [here](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.kneighbors_graph.html)).\n    :return: scipy sparse matrix.\n    """"""\n    A = kneighbors_graph(X, k, **kwargs).toarray()\n    A = sp.csr_matrix(np.maximum(A, A.T))\n\n    return A\n\n\ndef _mnist_grid_graph(k):\n    """"""\n    Get the adjacency matrix for the KNN graph.\n    :param k: int, number of neighbours for each node;\n    :return:\n    """"""\n    X = _grid_coordinates(MNIST_SIZE)\n    A = _get_adj_from_data(\n        X, k, mode=\'connectivity\', metric=\'euclidean\', include_self=False\n    )\n\n    return A\n\n\ndef _flip_random_edges(A, percent):\n    """"""\n    Flips values of A randomly.\n    :param A: binary scipy sparse matrix.\n    :param percent: percent of the edges to flip.\n    :return: binary scipy sparse matrix.\n    """"""\n    if not A.shape[0] == A.shape[1]:\n        raise ValueError(\'A must be a square matrix.\')\n    dtype = A.dtype\n    A = sp.lil_matrix(A).astype(np.bool)\n    n_elem = A.shape[0] ** 2\n    n_elem_to_flip = round(percent * n_elem)\n    unique_idx = np.random.choice(n_elem, replace=False, size=n_elem_to_flip)\n    row_idx = unique_idx // A.shape[0]\n    col_idx = unique_idx % A.shape[0]\n    idxs = np.stack((row_idx, col_idx)).T\n    for i in idxs:\n        i = tuple(i)\n        A[i] = np.logical_not(A[i])\n    A = A.tocsr().astype(dtype)\n    A.eliminate_zeros()\n    return A\n'"
spektral/datasets/ogb.py,0,"b'import scipy.sparse as sp\nimport numpy as np\n\n\ndef graph_to_numpy(graph, dtype=None):\n    """"""\n    Converts a graph in OGB\'s library-agnostic format to a representation in\n    Numpy/Scipy. See the [Open Graph Benchmark\'s website](https://ogb.stanford.edu)\n    for more information.\n    :param graph: OGB library-agnostic graph;\n    :param dtype: if set, all output arrays will be cast to this dtype.\n    :return:\n        - X: np.array of shape (N, F) with the node features;\n        - A: scipy.sparse adjacency matrix of shape (N, N) in COOrdinate format;\n        - E: if edge features are available, np.array of shape (n_edges, S),\n            `None` otherwise.\n    """"""\n    N = graph[\'num_nodes\']\n    X = graph[\'node_feat\'].astype(dtype)\n    row, col = graph[\'edge_index\']\n    A = sp.coo_matrix((np.ones_like(row), (row, col)), shape=(N, N)).astype(dtype)\n    E = graph[\'edge_feat\'].astype(dtype)\n\n    return X, A, E\n\n\ndef dataset_to_numpy(dataset, indices=None, dtype=None):\n    """"""\n    Converts a dataset in OGB\'s library-agnostic version to lists of Numpy/Scipy\n    arrays. See the [Open Graph Benchmark\'s website](https://ogb.stanford.edu)\n    for more information.\n    :param dataset: OGB library-agnostic dataset (e.g., GraphPropPredDataset);\n    :param indices: optional, a list of integer indices; if provided, only these\n    graphs will be converted;\n    :param dtype: if set, the arrays in the returned lists will have this dtype.\n    :return:\n        - X_list: list of np.arrays of (variable) shape (N, F) with node features;\n        - A_list: list of scipy.sparse adjacency matrices of (variable) shape\n        (N, N);\n        - E_list: list of np.arrays of (variable) shape (n_nodes, S) with edge\n        attributes. If edge attributes are not available, a list of None.\n        - y_list: np.array of shape (n_graphs, n_tasks) with the task labels;\n    """"""\n    X_list = []\n    A_list = []\n    E_list = []\n    y_list = []\n    if indices is None:\n        indices = range(len(dataset))\n\n    for i in indices:\n        graph, label = dataset[int(i)]\n        X, A, E = graph_to_numpy(graph, dtype=dtype)\n        X_list.append(X)\n        A_list.append(A)\n        E_list.append(E)\n        y_list.append(label)\n\n    return X_list, A_list, E_list, np.array(y_list)'"
spektral/datasets/qm9.py,0,"b'import os\n\nfrom tensorflow.keras.utils import get_file\n\nfrom spektral.chem import sdf_to_nx\nfrom spektral.utils import nx_to_numpy\nfrom spektral.utils.io import load_csv, load_sdf\n\nDATA_PATH = os.path.expanduser(\'~/.spektral/datasets/qm9/\')\nDATASET_URL = \'http://deepchem.io.s3-website-us-west-1.amazonaws.com/datasets/gdb9.tar.gz\'\nRETURN_TYPES = {\'numpy\', \'networkx\', \'sdf\'}\nNODE_FEATURES = [\'atomic_num\', \'charge\', \'coords\', \'iso\']\nEDGE_FEATURES = [\'type\', \'stereo\']\nMAX_K = 9\n\n\ndef load_data(nf_keys=None, ef_keys=None, auto_pad=True, self_loops=False,\n              amount=None, return_type=\'numpy\'):\n    """"""\n    Loads the QM9 chemical data set of small molecules.\n\n    Nodes represent heavy atoms (hydrogens are discarded), edges represent\n    chemical bonds.\n\n    The node features represent the chemical properties of each atom, and are\n    loaded according to the `nf_keys` argument.\n    See `spektral.datasets.qm9.NODE_FEATURES` for possible node features, and\n    see [this link](http://www.nonlinear.com/progenesis/sdf-studio/v0.9/faq/sdf-file-format-guidance.aspx)\n    for the meaning of each property. Usually, it is sufficient to load the\n    atomic number.\n\n    The edge features represent the type and stereoscopy of each chemical bond\n    between two atoms.\n    See `spektral.datasets.qm9.EDGE_FEATURES` for possible edge features, and\n    see [this link](http://www.nonlinear.com/progenesis/sdf-studio/v0.9/faq/sdf-file-format-guidance.aspx)\n    for the meaning of each property. Usually, it is sufficient to load the\n    type of bond.\n\n    :param nf_keys: list or str, node features to return (see `qm9.NODE_FEATURES`\n    for available features);\n    :param ef_keys: list or str, edge features to return (see `qm9.EDGE_FEATURES`\n    for available features);\n    :param auto_pad: if `return_type=\'numpy\'`, zero pad graph matrices to have \n    the same number of nodes;\n    :param self_loops: if `return_type=\'numpy\'`, add self loops to adjacency \n    matrices;\n    :param amount: the amount of molecules to return (in ascending order by\n    number of atoms).\n    :param return_type: `\'numpy\'`, `\'networkx\'`, or `\'sdf\'`, data format to return;\n    :return:\n    - if `return_type=\'numpy\'`, the adjacency matrix, node features,\n    edge features, and a Pandas dataframe containing labels;\n    - if `return_type=\'networkx\'`, a list of graphs in Networkx format,\n    and a dataframe containing labels;   \n    - if `return_type=\'sdf\'`, a list of molecules in the internal SDF format and\n    a dataframe containing labels.\n    """"""\n    if return_type not in RETURN_TYPES:\n        raise ValueError(\'Possible return_type: {}\'.format(RETURN_TYPES))\n\n    if not os.path.exists(DATA_PATH):\n        _download_data()  # Try to download dataset\n\n    print(\'Loading QM9 dataset.\')\n    sdf_file = os.path.join(DATA_PATH, \'qm9.sdf\')\n    data = load_sdf(sdf_file, amount=amount)  # Internal SDF format\n\n    # Load labels\n    labels_file = os.path.join(DATA_PATH, \'qm9.sdf.csv\')\n    labels = load_csv(labels_file)\n    if amount is not None:\n        labels = labels[:amount]\n    if return_type is \'sdf\':\n        return data, labels\n    else:\n        # Convert to Networkx\n        data = [sdf_to_nx(_) for _ in data]\n\n    if return_type is \'numpy\':\n        if nf_keys is not None:\n            if isinstance(nf_keys, str):\n                nf_keys = [nf_keys]\n        else:\n            nf_keys = NODE_FEATURES\n        if ef_keys is not None:\n            if isinstance(ef_keys, str):\n                ef_keys = [ef_keys]\n        else:\n            ef_keys = EDGE_FEATURES\n\n        adj, nf, ef = nx_to_numpy(data,\n                                  auto_pad=auto_pad, self_loops=self_loops,\n                                  nf_keys=nf_keys, ef_keys=ef_keys)\n        return adj, nf, ef, labels\n    elif return_type is \'networkx\':\n        return data, labels\n    else:\n        # Should not get here\n        raise RuntimeError()\n\n\ndef _download_data():\n    _ = get_file(\n        \'qm9.tar.gz\', DATASET_URL,\n        extract=True, cache_dir=DATA_PATH, cache_subdir=DATA_PATH\n    )\n    os.rename(DATA_PATH + \'gdb9.sdf\', DATA_PATH + \'qm9.sdf\')\n    os.rename(DATA_PATH + \'gdb9.sdf.csv\', DATA_PATH + \'qm9.sdf.csv\')\n    os.remove(DATA_PATH + \'qm9.tar.gz\')\n'"
spektral/datasets/tud.py,0,"b'import glob\nimport os\nimport shutil\nimport zipfile\nfrom os import path as osp\nfrom urllib.error import URLError\n\nimport numpy as np\nimport pandas as pd\nimport requests\nimport scipy.sparse as sp\nfrom sklearn.preprocessing import OneHotEncoder, StandardScaler\n\nfrom spektral.utils import io\n\nDATASET_URL = \'https://ls11-www.cs.tu-dortmund.de/people/morris/graphkerneldatasets\'\nDATASET_CLEAN_URL = \'https://raw.githubusercontent.com/nd7141/graph_datasets/master/datasets\'\nDATA_PATH = osp.expanduser(\'~/.spektral/datasets/\')\ntry:\n    AVAILABLE_DATASETS = [\n        d[:-4]\n        for d in pd.read_html(DATASET_URL)[0].Name[2:-1].values.tolist()\n    ]\nexcept URLError:\n    # No internet, don\'t panic\n    AVAILABLE_DATASETS = []\n\n\ndef load_data(dataset_name, clean=False):\n    """"""\n    Loads one of the Benchmark Data Sets for Graph Kernels from TU Dortmund\n    ([link](https://ls11-www.cs.tu-dortmund.de/staff/morris/graphkerneldatasets)).\n    The node features are computed by concatenating the following features for\n    each node:\n\n    - node attributes, if available, normalized as specified in `normalize_features`;\n    - clustering coefficient, normalized with z-score;\n    - node degrees, normalized as specified in `normalize_features`;\n    - node labels, if available, one-hot encoded.\n    :param dataset_name: name of the dataset to load (see `spektral.datasets.tud.AVAILABLE_DATASETS`).\n    :param normalize_features: `None`, `\'zscore\'` or `\'ohe\'`, how to normalize\n    the node features (only works for node attributes).\n    :param clean: if True, return a version of the dataset with no isomorphic\n    graphs.\n    :return:\n    - a list of adjacency matrices;\n    - a list of node feature matrices;\n    - a numpy array containing the one-hot encoded targets.\n    """"""\n    if AVAILABLE_DATASETS and dataset_name not in AVAILABLE_DATASETS:\n        raise ValueError(\'Available datasets: {}\'.format(AVAILABLE_DATASETS))\n\n    if clean:\n        dataset_name += \'_clean\'\n    if not osp.exists(DATA_PATH + dataset_name):\n        _download_data(dataset_name)\n\n    # Read data\n    A_list, X_list, y = _read_graphs(dataset_name)\n\n    print(\'Successfully loaded {}.\'.format(dataset_name))\n\n    return A_list, X_list, y\n\n\ndef _read_graphs(dataset_name):\n    file_prefix = osp.join(DATA_PATH, dataset_name, dataset_name)\n    available = [\n        f.split(os.sep)[-1][len(dataset_name)+1:-4]\n        for f in glob.glob(\'{}_*.txt\'.format(file_prefix))\n    ]\n\n    I = io.load_txt(file_prefix + \'_graph_indicator.txt\').astype(int) - 1\n    unique_ids = np.unique(I)\n    num_graphs = len(unique_ids)\n    graph_sizes = np.bincount(I)\n    offsets = np.concatenate(([0], np.cumsum(graph_sizes)[:-1]))\n    edges = io.load_txt(file_prefix + \'_A.txt\', delimiter=\',\').astype(int) - 1\n\n    A_list = [[] for _ in range(num_graphs)]\n    for e in edges:\n        graph_id = I[e[0]]\n        A_list[graph_id].append(e - offsets[graph_id])\n    A_list = map(np.array, A_list)\n    A_list = [\n        sp.coo_matrix(\n            (np.ones_like(A[:, 0]), (A[:, 0], A[:, 1])),\n            shape=(graph_sizes[i], graph_sizes[i])\n        )\n        for i, A in enumerate(A_list)\n    ]\n\n    X = []\n    if \'node_attributes\' in available:\n        X_na = io.load_txt(file_prefix + \'_node_attributes.txt\', delimiter=\',\')\n        if X_na.ndim == 1:\n            X_na = X_na[:, None]\n        X.append(X_na)\n    if \'node_labels\' in available:\n        X_nl = io.load_txt(file_prefix + \'_node_labels.txt\')\n        X_nl = _normalize(X_nl.reshape(-1, 1), \'ohe\')\n        X.append(X_nl)\n    if len(X) > 0:\n        X = np.concatenate(X, -1)\n\n    X_list = []\n    start = offsets[0]\n    for i in range(num_graphs):\n        stop = offsets[i + 1] if i + 1 < len(offsets) else None\n        X_list.append(X[start:stop])\n        start = stop\n\n\n    y = None\n    if \'graph_attributes\' in available:\n        y = io.load_txt(file_prefix + \'_graph_attributes.txt\')\n    elif \'graph_labels\' in available:\n        y = io.load_txt(file_prefix + \'_graph_labels.txt\')\n        y = _normalize(y[:, None], \'ohe\')\n\n    return A_list, X_list, y\n\n\ndef _download_data(dataset_name):\n    print(\'Dowloading \' + dataset_name + \' dataset.\')\n    if dataset_name.endswith(\'_clean\'):\n        true_name = dataset_name[:-6]\n        url = DATASET_CLEAN_URL\n    else:\n        true_name = dataset_name\n        url = DATASET_URL\n\n    data_url = \'{}/{}.zip\'.format(url, true_name)\n    req = requests.get(data_url)\n\n    os.makedirs(DATA_PATH, exist_ok=True)\n    with open(DATA_PATH + dataset_name + \'.zip\', \'wb\') as out_file:\n        out_file.write(req.content)\n    with zipfile.ZipFile(DATA_PATH + dataset_name + \'.zip\', \'r\') as zip_ref:\n        zip_ref.extractall(DATA_PATH + dataset_name + \'/\')\n    os.remove(DATA_PATH + dataset_name + \'.zip\')\n\n    subfolder = osp.join(DATA_PATH, dataset_name, true_name)\n    parentfolder = osp.join(DATA_PATH, dataset_name)\n    for filename in os.listdir(subfolder):\n        try:\n            suffix = filename.split(true_name)[1]\n        except IndexError:\n            # Probably the README\n            continue\n        shutil.move(\n            osp.join(subfolder, filename),\n            osp.join(parentfolder, dataset_name + suffix)\n        )\n    shutil.rmtree(subfolder)\n\n\ndef _normalize(x, norm=None):\n    """"""\n    Apply one-hot encoding or z-score to a list of node features\n    """"""\n    if norm == \'ohe\':\n        fnorm = OneHotEncoder(sparse=False, categories=\'auto\')\n    elif norm == \'zscore\':\n        fnorm = StandardScaler()\n    else:\n        return x\n    return fnorm.fit_transform(x)\n'"
spektral/layers/__init__.py,0,b'from .base import *\nfrom .convolutional import *\nfrom .pooling import *\nfrom . import ops\n'
spektral/layers/base.py,3,"b'import numpy as np\nimport tensorflow as tf\nfrom tensorflow.python.framework import smart_cond\nfrom tensorflow.keras import activations, initializers, regularizers, constraints\nfrom tensorflow.keras import backend as K\nfrom tensorflow.keras.layers import Layer\n\n\nclass SparseDropout(Layer):\n    """"""Applies Dropout to the input.\n\n    Dropout consists in randomly setting\n    a fraction `rate` of input units to 0 at each update during training time,\n    which helps prevent overfitting.\n\n    Arguments:\n    rate: Float between 0 and 1. Fraction of the input units to drop.\n    noise_shape: 1D integer tensor representing the shape of the\n      binary dropout mask that will be multiplied with the input.\n      For instance, if your inputs have shape\n      `(batch_size, timesteps, features)` and\n      you want the dropout mask to be the same for all timesteps,\n      you can use `noise_shape=(batch_size, 1, features)`.\n    seed: A Python integer to use as random seed.\n\n    Call arguments:\n    inputs: Input tensor (of any rank).\n    training: Python boolean indicating whether the layer should behave in\n      training mode (adding dropout) or in inference mode (doing nothing).\n    """"""\n\n    def __init__(self, rate, noise_shape=None, seed=None, **kwargs):\n        super().__init__(**kwargs)\n        self.rate = rate\n        self.noise_shape = noise_shape\n        self.seed = seed\n        self.supports_masking = True\n\n    def _get_noise_shape(self, inputs):\n        return tf.shape(inputs.values)\n\n    def call(self, inputs, training=None):\n        if training is None:\n            training = K.learning_phase()\n\n        def dropped_inputs():\n            return self.sparse_dropout(\n              inputs,\n              noise_shape=self._get_noise_shape(inputs),\n              seed=self.seed,\n              rate=self.rate\n            )\n\n        output = smart_cond.smart_cond(training,\n                                       dropped_inputs,\n                                       lambda: inputs)\n        return output\n\n    def compute_output_shape(self, input_shape):\n        return input_shape\n\n    def get_config(self):\n        config = {\n            \'rate\': self.rate,\n            \'noise_shape\': self.noise_shape,\n            \'seed\': self.seed\n        }\n        base_config = super().get_config()\n        return dict(list(base_config.items()) + list(config.items()))\n\n    @staticmethod\n    def sparse_dropout(x, rate, noise_shape=None, seed=None):\n        random_tensor = tf.random.uniform(noise_shape, seed=seed, dtype=x.dtype)\n        keep_prob = 1 - rate\n        scale = 1 / keep_prob\n        keep_mask = random_tensor >= rate\n        output = tf.sparse.retain(x, keep_mask) * scale\n\n        return output\n\n\nclass InnerProduct(Layer):\n    r""""""\n    Computes the inner product between elements of a 2d Tensor:\n    $$\n        \\langle \\x, \\x \\rangle = \\x\\x^\\top.\n    $$\n\n    **Mode**: single.\n\n    **Input**\n\n    - Tensor of shape `(N, M)`;\n\n    **Output**\n\n    - Tensor of shape `(N, N)`.\n\n    :param trainable_kernel: add a trainable square matrix between the inner\n    product (e.g., `X @ W @ X.T`);\n    :param activation: activation function to use;\n    :param kernel_initializer: initializer for the weights;\n    :param kernel_regularizer: regularization applied to the kernel;\n    :param kernel_constraint: constraint applied to the kernel;\n    """"""\n    def __init__(self,\n                 trainable_kernel=False,\n                 activation=None,\n                 kernel_initializer=\'glorot_uniform\',\n                 kernel_regularizer=None,\n                 kernel_constraint=None,\n                 **kwargs):\n\n        super().__init__(**kwargs)\n        self.trainable_kernel = trainable_kernel\n        self.activation = activations.get(activation)\n        self.kernel_initializer = initializers.get(kernel_initializer)\n        self.kernel_regularizer = regularizers.get(kernel_regularizer)\n        self.kernel_constraint = constraints.get(kernel_constraint)\n\n    def build(self, input_shape):\n        assert len(input_shape) >= 2\n        if self.trainable_kernel:\n            features_dim = input_shape[-1]\n            self.kernel = self.add_weight(shape=(features_dim, features_dim),\n                                          name=\'kernel\',\n                                          initializer=self.kernel_initializer,\n                                          regularizer=self.kernel_regularizer,\n                                          constraint=self.kernel_constraint)\n        self.built = True\n\n    def call(self, inputs):\n        if self.trainable_kernel:\n            output = K.dot(K.dot(inputs, self.kernel), K.transpose(inputs))\n        else:\n            output = K.dot(inputs, K.transpose(inputs))\n        if self.activation is not None:\n            output = self.activation(output)\n        return output\n\n    def compute_output_shape(self, input_shape):\n        if len(input_shape) == 2:\n            return (None, None)\n        else:\n            return input_shape[:-1] + (input_shape[-2], )\n\n    def get_config(self, **kwargs):\n        config = {\n            \'trainable_kernel\': self.trainable_kernel,\n            \'activation\': self.activation,\n            \'kernel_initializer\': self.kernel_initializer,\n            \'kernel_regularizer\': self.kernel_regularizer,\n            \'kernel_constraint\': self.kernel_constraint,\n        }\n        base_config = super().get_config()\n        return dict(list(base_config.items()) + list(config.items()))\n\n\nclass MinkowskiProduct(Layer):\n    r""""""\n    Computes the hyperbolic inner product between elements of a rank 2 Tensor:\n    $$\n        \\langle \\x, \\x \\rangle = \\x \\,\n        \\begin{pmatrix}\n            \\I_{d \\times d} & 0 \\\\\n            0              & -1\n        \\end{pmatrix} \\, \\x^\\top.\n    $$\n\n    **Mode**: single.\n\n    **Input**\n\n    - Tensor of shape `(N, M)`;\n\n    **Output**\n\n    - Tensor of shape `(N, N)`.\n\n    :param input_dim_1: first dimension of the input Tensor; set this if you\n    encounter issues with shapes in your model, in order to provide an explicit\n    output shape for your layer.\n    :param activation: activation function to use;\n    """"""\n    def __init__(self,\n                 input_dim_1=None,\n                 activation=None,\n                 **kwargs):\n\n        super(MinkowskiProduct, self).__init__(**kwargs)\n        self.input_dim_1 = input_dim_1\n        self.activation = activations.get(activation)\n\n    def build(self, input_shape):\n        assert len(input_shape) >= 2\n        self.built = True\n\n    def call(self, inputs):\n        F = K.int_shape(inputs)[-1]\n        minkowski_prod_mat = np.eye(F)\n        minkowski_prod_mat[-1, -1] = -1.\n        minkowski_prod_mat = K.constant(minkowski_prod_mat)\n        output = K.dot(inputs, minkowski_prod_mat)\n        output = K.dot(output, K.transpose(inputs))\n        output = K.clip(output, -10e9, -1.)\n\n        if self.activation is not None:\n            output = self.activation(output)\n\n        return output\n\n    def compute_output_shape(self, input_shape):\n        if len(input_shape) == 2:\n            if self.input_dim_1 is None:\n                return (None, None)\n            else:\n                return (self.input_dim_1, self.input_dim_1)\n        else:\n            return input_shape[:-1] + (input_shape[-2], )\n\n    def get_config(self, **kwargs):\n        config = {\n            \'input_dim_1\': self.input_dim_1,\n            \'activation\': self.activation\n        }\n        base_config = super().get_config()\n        return dict(list(base_config.items()) + list(config.items()))\n'"
spektral/utils/__init__.py,0,b'from .conversion import *\nfrom .convolution import *\nfrom .logging import *\nfrom .misc import *\nfrom .io import *\nfrom .data import *\n'
spektral/utils/conversion.py,0,"b'import networkx as nx\nimport numpy as np\n\nfrom spektral.utils.misc import pad_jagged_array, add_eye_jagged, add_eye_batch, flatten_list\n\n\n# Available conversions: Numpy <-> Networkx <-> SDF\n\n\ndef nx_to_adj(graphs):\n    """"""\n    Converts a list of nx.Graphs to a rank 3 np.array of adjacency matrices\n    of shape `(num_graphs, num_nodes, num_nodes)`.\n    :param graphs: a nx.Graph, or list of nx.Graphs.\n    :return: a rank 3 np.array of adjacency matrices.\n    """"""\n    if isinstance(graphs, nx.Graph):\n        graphs = [graphs]\n    return np.array([nx.attr_sparse_matrix(g)[0].toarray() for g in graphs])\n\n\ndef nx_to_node_features(graphs, keys, post_processing=None):\n    """"""\n    Converts a list of nx.Graphs to a rank 3 np.array of node features matrices\n    of shape `(num_graphs, num_nodes, num_features)`. Optionally applies a\n    post-processing function to each individual attribute in the nx Graphs.\n    :param graphs: a nx.Graph, or a list of nx.Graphs;\n    :param keys: a list of keys with which to index node attributes in the nx\n    Graphs.\n    :param post_processing: a list of functions with which to post process each\n    attribute associated to a key. `None` can be passed as post-processing \n    function to leave the attribute unchanged.\n    :return: a rank 3 np.array of feature matrices\n    """"""\n    if post_processing is not None:\n        if len(post_processing) != len(keys):\n            raise ValueError(\'post_processing must contain an element for each key\')\n        for i in range(len(post_processing)):\n            if post_processing[i] is None:\n                post_processing[i] = lambda x: x\n\n    if isinstance(graphs, nx.Graph):\n        graphs = [graphs]\n\n    output = []\n    for g in graphs:\n        node_features = []\n        for v in g.nodes.values():\n            f = [v[key] for key in keys]\n            if post_processing is not None:\n                f = [op(_) for op, _ in zip(post_processing, f)]\n            f = flatten_list(f)\n            node_features.append(f)\n        output.append(np.array(node_features))\n\n    return np.array(output)\n\n\ndef nx_to_edge_features(graphs, keys, post_processing=None):\n    """"""\n    Converts a list of nx.Graphs to a rank 4 np.array of edge features matrices\n    of shape `(num_graphs, num_nodes, num_nodes, num_features)`.\n    Optionally applies a post-processing function to each attribute in the nx\n    graphs.\n    :param graphs: a nx.Graph, or a list of nx.Graphs;\n    :param keys: a list of keys with which to index edge attributes.\n    :param post_processing: a list of functions with which to post process each\n    attribute associated to a key. `None` can be passed as post-processing \n    function to leave the attribute unchanged.\n    :return: a rank 3 np.array of feature matrices\n    """"""\n    if post_processing is not None:\n        if len(post_processing) != len(keys):\n            raise ValueError(\'post_processing must contain an element for each key\')\n        for i in range(len(post_processing)):\n            if post_processing[i] is None:\n                post_processing[i] = lambda x: x\n\n    if isinstance(graphs, nx.Graph):\n        graphs = [graphs]\n\n    output = []\n    for g in graphs:\n        edge_features = []\n        for key in keys:\n            ef = nx.attr_sparse_matrix(g, edge_attr=key)[0].toarray()\n            if ef.ndim == 2:\n                ef = ef[..., None]  # Make it three dimensional to concatenate\n            edge_features.append(ef)\n        if post_processing is not None:\n            edge_features = [op(_) for op, _ in zip(post_processing, edge_features)]\n        if len(edge_features) > 1:\n            edge_features = np.concatenate(edge_features, axis=-1)\n        else:\n            edge_features = np.array(edge_features[0])\n        output.append(edge_features)\n\n    return np.array(output)\n\n\ndef nx_to_numpy(graphs, auto_pad=True, self_loops=True, nf_keys=None,\n                ef_keys=None, nf_postprocessing=None, ef_postprocessing=None):\n    """"""\n    Converts a list of nx.Graphs to numpy format (adjacency, node attributes,\n    and edge attributes matrices).\n    :param graphs: a nx.Graph, or list of nx.Graphs;\n    :param auto_pad: whether to zero-pad all matrices to have graphs with the\n    same dimension (set this to true if you don\'t want to deal with manual\n    batching for different-size graphs.\n    :param self_loops: whether to add self-loops to the graphs.\n    :param nf_keys: a list of keys with which to index node attributes. If None,\n    returns None as node attributes matrix.\n    :param ef_keys: a list of keys with which to index edge attributes. If None,\n    returns None as edge attributes matrix.\n    :param nf_postprocessing: a list of functions with which to post process each\n    node attribute associated to a key. `None` can be passed as post-processing\n    function to leave the attribute unchanged.\n    :param ef_postprocessing: a list of functions with which to post process each\n    edge attribute associated to a key. `None` can be passed as post-processing\n    function to leave the attribute unchanged.\n    :return:\n    - adjacency matrices of shape `(num_samples, num_nodes, num_nodes)`\n    - node attributes matrices of shape `(num_samples, num_nodes, node_features_dim)`\n    - edge attributes matrices of shape `(num_samples, num_nodes, num_nodes, edge_features_dim)`\n    """"""\n    adj = nx_to_adj(graphs)\n    if nf_keys is not None:\n        nf = nx_to_node_features(graphs, nf_keys, post_processing=nf_postprocessing)\n    else:\n        nf = None\n    if ef_keys is not None:\n        ef = nx_to_edge_features(graphs, ef_keys, post_processing=ef_postprocessing)\n    else:\n        ef = None\n\n    if self_loops:\n        if adj.ndim == 1:  # Jagged array\n            adj = add_eye_jagged(adj)\n            adj = np.array([np.clip(a_, 0, 1) for a_ in adj])\n        else:  # Rank 3 tensor\n            adj = add_eye_batch(adj)\n            adj = np.clip(adj, 0, 1)\n\n    if auto_pad:\n        # Pad all arrays to represent k-nodes graphs\n        k = max([_.shape[-1] for _ in adj])\n        adj = pad_jagged_array(adj, (k, k))\n        if nf is not None:\n            nf = pad_jagged_array(nf, (k, -1))\n        if ef is not None:\n            ef = pad_jagged_array(ef, (k, k, -1))\n\n    return adj, nf, ef\n\n\ndef numpy_to_nx(adj, node_features=None, edge_features=None, nf_name=None,\n                ef_name=None):\n    """"""\n    Converts graphs in numpy format to a list of nx.Graphs.\n    :param adj: adjacency matrices of shape `(num_samples, num_nodes, num_nodes)`.\n    If there is only one sample, the first dimension can be dropped.\n    :param node_features: optional node attributes matrices of shape `(num_samples, num_nodes, node_features_dim)`.\n    If there is only one sample, the first dimension can be dropped.\n    :param edge_features: optional edge attributes matrices of shape `(num_samples, num_nodes, num_nodes, edge_features_dim)`\n    If there is only one sample, the first dimension can be dropped.\n    :param nf_name: optional name to assign to node attributes in the nx.Graphs\n    :param ef_name: optional name to assign to edge attributes in the nx.Graphs\n    :return: a list of nx.Graphs (or a single nx.Graph is there is only one sample)\n    """"""\n    if adj.ndim == 2:\n        adj = adj[None, ...]\n        if node_features is not None:\n            if nf_name is None:\n                nf_name = \'node_features\'\n            node_features = node_features[None, ...]\n            if node_features.ndim != 3:\n                raise ValueError(\'node_features must have shape (batch, N, F) \'\n                                 \'or (N, F).\')\n        if edge_features is not None:\n            if ef_name is None:\n                ef_name = \'edge_features\'\n            edge_features = edge_features[None, ...]\n            if edge_features.ndim != 4:\n                raise ValueError(\'edge_features must have shape (batch, N, N, S) \'\n                                 \'or (N, N, S).\')\n\n    output = []\n    for i in range(adj.shape[0]):\n        g = nx.from_numpy_array(adj[i])\n        g.remove_nodes_from(list(nx.isolates(g)))\n\n        if node_features is not None:\n            node_attrs = {n: {nf_name: node_features[i, n]} for n in g.nodes}\n            nx.set_node_attributes(g, node_attrs, nf_name)\n        if edge_features is not None:\n            edge_attrs = {e: {ef_name: edge_features[i, e[0], e[1]]} for e in g.edges}\n            nx.set_edge_attributes(g, edge_attrs, ef_name)\n        output.append(g)\n\n    if len(output) == 1:\n        return output[0]\n    else:\n        return output\n'"
spektral/utils/convolution.py,0,"b'import numpy as np\nfrom scipy import sparse as sp\nfrom scipy.sparse.linalg import ArpackNoConvergence\n\n\ndef degree_matrix(A):\n    """"""\n    Computes the degree matrix of the given adjacency matrix.\n    :param A: rank 2 array or sparse matrix.\n    :return: if A is a dense array, a dense array; if A is sparse, a sparse\n    matrix in DIA format.\n    """"""\n    degrees = np.array(A.sum(1)).flatten()\n    if sp.issparse(A):\n        D = sp.diags(degrees)\n    else:\n        D = np.diag(degrees)\n    return D\n\n\ndef degree_power(A, k):\n    r""""""\n    Computes \\(\\D^{k}\\) from the given adjacency matrix. Useful for computing\n    normalised Laplacian.\n    :param A: rank 2 array or sparse matrix.\n    :param k: exponent to which elevate the degree matrix.\n    :return: if A is a dense array, a dense array; if A is sparse, a sparse\n    matrix in DIA format.\n    """"""\n    degrees = np.power(np.array(A.sum(1)), k).flatten()\n    degrees[np.isinf(degrees)] = 0.\n    if sp.issparse(A):\n        D = sp.diags(degrees)\n    else:\n        D = np.diag(degrees)\n    return D\n\n\ndef normalized_adjacency(A, symmetric=True):\n    r""""""\n    Normalizes the given adjacency matrix using the degree matrix as either\n    \\(\\D^{-1}\\A\\) or \\(\\D^{-1/2}\\A\\D^{-1/2}\\) (symmetric normalization).\n    :param A: rank 2 array or sparse matrix;\n    :param symmetric: boolean, compute symmetric normalization;\n    :return: the normalized adjacency matrix.\n    """"""\n    if symmetric:\n        normalized_D = degree_power(A, -0.5)\n        output = normalized_D.dot(A).dot(normalized_D)\n    else:\n        normalized_D = degree_power(A, -1.)\n        output = normalized_D.dot(A)\n    return output\n\n\ndef laplacian(A):\n    r""""""\n    Computes the Laplacian of the given adjacency matrix as \\(\\D - \\A\\).\n    :param A: rank 2 array or sparse matrix;\n    :return: the Laplacian.\n    """"""\n    return degree_matrix(A) - A\n\n\ndef normalized_laplacian(A, symmetric=True):\n    r""""""\n    Computes a  normalized Laplacian of the given adjacency matrix as\n    \\(\\I - \\D^{-1}\\A\\) or \\(\\I - \\D^{-1/2}\\A\\D^{-1/2}\\) (symmetric normalization).\n    :param A: rank 2 array or sparse matrix;\n    :param symmetric: boolean, compute symmetric normalization;\n    :return: the normalized Laplacian.\n    """"""\n    if sp.issparse(A):\n        I = sp.eye(A.shape[-1], dtype=A.dtype)\n    else:\n        I = np.eye(A.shape[-1], dtype=A.dtype)\n    normalized_adj = normalized_adjacency(A, symmetric=symmetric)\n    return I - normalized_adj\n\n\ndef rescale_laplacian(L, lmax=None):\n    """"""\n    Rescales the Laplacian eigenvalues in [-1,1], using lmax as largest eigenvalue.\n    :param L: rank 2 array or sparse matrix;\n    :param lmax: if None, compute largest eigenvalue with scipy.linalg.eisgh.\n    If the eigendecomposition fails, lmax is set to 2 automatically.\n    If scalar, use this value as largest eignevalue when rescaling.\n    :return:\n    """"""\n    if lmax is None:\n        try:\n            lmax = sp.linalg.eigsh(L, 1, which=\'LM\', return_eigenvectors=False)[0]\n        except ArpackNoConvergence:\n            lmax = 2\n    if sp.issparse(L):\n        I = sp.eye(L.shape[-1], dtype=L.dtype)\n    else:\n        I = np.eye(L.shape[-1], dtype=L.dtype)\n    L_scaled = (2. / lmax) * L - I\n    return L_scaled\n\n\ndef localpooling_filter(A, symmetric=True):\n    r""""""\n    Computes the graph filter described in\n    [Kipf & Welling (2017)](https://arxiv.org/abs/1609.02907).\n    :param A: array or sparse matrix with rank 2 or 3;\n    :param symmetric: boolean, whether to normalize the matrix as\n    \\(\\D^{-\\frac{1}{2}}\\A\\D^{-\\frac{1}{2}}\\) or as \\(\\D^{-1}\\A\\);\n    :return: array or sparse matrix with rank 2 or 3, same as A;\n    """"""\n    fltr = A.copy()\n    if sp.issparse(A):\n        I = sp.eye(A.shape[-1], dtype=A.dtype)\n    else:\n        I = np.eye(A.shape[-1], dtype=A.dtype)\n    if A.ndim == 3:\n        for i in range(A.shape[0]):\n            A_tilde = A[i] + I\n            fltr[i] = normalized_adjacency(A_tilde, symmetric=symmetric)\n    else:\n        A_tilde = A + I\n        fltr = normalized_adjacency(A_tilde, symmetric=symmetric)\n\n    if sp.issparse(fltr):\n        fltr.sort_indices()\n    return fltr\n\n\ndef chebyshev_polynomial(X, k):\n    """"""\n    Calculates Chebyshev polynomials of X, up to order k.\n    :param X: rank 2 array or sparse matrix;\n    :param k: the order up to which compute the polynomials,\n    :return: a list of k + 1 arrays or sparse matrices with one element for each\n    degree of the polynomial.\n    """"""\n    T_k = list()\n    if sp.issparse(X):\n        T_k.append(sp.eye(X.shape[0], dtype=X.dtype).tocsr())\n    else:\n        T_k.append(np.eye(X.shape[0], dtype=X.dtype))\n    T_k.append(X)\n\n    def chebyshev_recurrence(T_k_minus_one, T_k_minus_two, X):\n        if sp.issparse(X):\n            X_ = sp.csr_matrix(X, copy=True)\n        else:\n            X_ = np.copy(X)\n        return 2 * X_.dot(T_k_minus_one) - T_k_minus_two\n\n    for i in range(2, k + 1):\n        T_k.append(chebyshev_recurrence(T_k[-1], T_k[-2], X))\n\n    return T_k\n\n\ndef chebyshev_filter(A, k, symmetric=True):\n    r""""""\n    Computes the Chebyshev filter from the given adjacency matrix, as described\n    in [Defferrard et at. (2016)](https://arxiv.org/abs/1606.09375).\n    :param A: rank 2 array or sparse matrix;\n    :param k: integer, the order of the Chebyshev polynomial;\n    :param symmetric: boolean, whether to normalize the adjacency matrix as\n    \\(\\D^{-\\frac{1}{2}}\\A\\D^{-\\frac{1}{2}}\\) or as \\(\\D^{-1}\\A\\);\n    :return: a list of k + 1 arrays or sparse matrices with one element for each\n    degree of the polynomial.\n    """"""\n    normalized_adj = normalized_adjacency(A, symmetric)\n    if sp.issparse(A):\n        I = sp.eye(A.shape[0], dtype=A.dtype)\n    else:\n        I = np.eye(A.shape[0], dtype=A.dtype)\n    L = I - normalized_adj  # Compute Laplacian\n\n    # Rescale Laplacian\n    L_scaled = rescale_laplacian(L)\n\n    # Compute Chebyshev polynomial approximation\n    T_k = chebyshev_polynomial(L_scaled, k)\n\n    # Sort indices\n    if sp.issparse(T_k[0]):\n        for i in range(len(T_k)):\n            T_k[i].sort_indices()\n\n    return T_k\n\n\n'"
spektral/utils/data.py,0,"b'import numpy as np\nimport scipy.sparse as sp\n\nfrom spektral.utils import pad_jagged_array\n\n\ndef numpy_to_disjoint(X_list, A_list, E_list=None):\n    """"""\n    Converts a batch of graphs stored in lists (X, A, and optionally E) to the\n    [disjoint mode](https://danielegrattarola.github.io/spektral/data/#disjoint-mode).\n\n    Each entry i of the lists should be associated to the same graph, i.e.,\n    `X_list[i].shape[0] == A_list[i].shape[0] == E_list[i].shape[0]`.\n\n    :param X_list: a list of np.arrays of shape `(N, F)`;\n    :param A_list: a list of np.arrays or sparse matrices of shape `(N, N)`;\n    :param E_list: a list of np.arrays of shape `(N, N, S)`;\n    :return:\n        -  `X_out`: a rank 2 array of shape `(n_nodes, F)`;\n        -  `A_out`: a rank 2 array of shape `(n_nodes, n_nodes)`;\n        -  `E_out`: (only if `E_list` is given) a rank 2 array of shape\n        `(n_edges, S)`;\n    """"""\n    X_out = np.vstack(X_list)\n    A_list = [sp.coo_matrix(a) for a in A_list]\n    if E_list is not None:\n        if E_list[0].ndim == 3:\n            E_list = [e[a.row, a.col] for e, a in zip(E_list, A_list)]\n        E_out = np.vstack(E_list)\n    A_out = sp.block_diag(A_list)\n    n_nodes = np.array([x.shape[0] for x in X_list])\n    I_out = np.repeat(np.arange(len(n_nodes)), n_nodes)\n    if E_list is not None:\n        return X_out, A_out, E_out, I_out\n    else:\n        return X_out, A_out, I_out\n\n\ndef numpy_to_batch(X_list, A_list, E_list=None):\n    """"""\n    Converts a batch of graphs stored in lists (X, A, and optionally E) to the\n    [batch mode](https://danielegrattarola.github.io/spektral/data/#batch-mode)\n    by zero-padding all X, A and E matrices to have the same node dimensions\n    (`N_max`).\n\n    Each entry i of the lists should be associated to the same graph, i.e.,\n    `X_list[i].shape[0] == A_list[i].shape[0] == E_list[i].shape[0]`.\n\n    Note that if `A_list` contains sparse matrices, they will be converted to\n    dense np.arrays, which can be expensice.\n\n    :param X_list: a list of np.arrays of shape `(N, F)`;\n    :param A_list: a list of np.arrays or sparse matrices of shape `(N, N)`;\n    :param E_list: a list of np.arrays of shape `(N, N, S)`;\n    :return:\n        -  `X_out`: a rank 3 array of shape `(batch, N_max, F)`;\n        -  `A_out`: a rank 2 array of shape `(batch, N_max, N_max)`;\n        -  `E_out`: (only if `E_list` if given) a rank 2 array of shape\n        `(batch, N_max, N_max, S)`;\n    """"""\n    N_max = max([a.shape[-1] for a in A_list])\n    X_out = pad_jagged_array(X_list, (N_max, -1))\n    # Convert sparse matrices to dense\n    if hasattr(A_list[0], \'toarray\'):\n        A_list = [a.toarray() for a in A_list]\n    A_out = pad_jagged_array(A_list, (N_max, N_max))\n    if E_list is not None:\n        E_out = pad_jagged_array(E_list, (N_max, N_max, -1))\n        return X_out, A_out, E_out\n    else:\n        return X_out, A_out\n\n\ndef batch_iterator(data, batch_size=32, epochs=1, shuffle=True):\n    """"""\n    Iterates over the data for the given number of epochs, yielding batches of\n    size `batch_size`.\n    :param data: np.array or list of np.arrays with the same first dimension;\n    :param batch_size: number of samples in a batch;\n    :param epochs: number of times to iterate over the data;\n    :param shuffle: whether to shuffle the data at the beginning of each epoch\n    :return: batches of size `batch_size`.\n    """"""\n    if not isinstance(data, list):\n        data = [data]\n    if len(set([len(item) for item in data])) > 1:\n        raise ValueError(\'All arrays must have the same length\')\n\n    len_data = len(data[0])\n    batches_per_epoch = int(len_data / batch_size)\n    if len_data % batch_size != 0:\n        batches_per_epoch += 1\n    for epochs in range(epochs):\n        if shuffle:\n            shuffle_idx = np.random.permutation(np.arange(len_data))\n            data = [np.array(item)[shuffle_idx] for item in data]\n        for batch in range(batches_per_epoch):\n            start = batch * batch_size\n            stop = min(start + batch_size, len_data)\n            if len(data) > 1:\n                yield [item[start:stop] for item in data]\n            else:\n                yield data[0][start:stop]'"
spektral/utils/io.py,0,"b'import ast\nimport sys\n\nimport joblib\nimport networkx as nx\nimport numpy as np\nimport pandas as pd\n\nfrom spektral.chem import get_atomic_num\n\n\ndef load_binary(filename):\n    """"""\n    Loads a pickled file.\n    :param filename: a string or file-like object\n    :return: the loaded object\n    """"""\n    try:\n        return joblib.load(filename)\n    except ValueError:\n        import pickle\n        with open(filename, \'rb\') as f:\n            return pickle.load(f, encoding=\'latin1\')\n\n\ndef dump_binary(obj, filename):\n    """"""\n    Pickles and saves an object to file.\n    :param obj: the object to save\n    :param filename: a string or file-like object\n    """"""\n    joblib.dump(obj, filename)\n\n\ndef load_csv(filename, **kwargs):\n    """"""\n    Loads a csv file with pandas.\n    :param filename: a string or file-like object\n    :return: the loaded csv\n    """"""\n    return pd.read_csv(filename, **kwargs)\n\n\ndef dump_csv(df, filename, convert=False, **kwargs):\n    """"""\n    Dumps a pd.DataFrame to csv.\n    :param df: the pd.DataFrame to save or equivalent object\n    :param filename: a string or file-like object\n    :param convert: whether to attempt to convert the given object to\n    pd.DataFrame before saving the csv.\n    """"""\n    if convert:\n        df = pd.DataFrame(df)\n    assert hasattr(df, \'to_csv\'), \\\n        \'Trying to dump object of class {} to csv while pd.DataFrame is \' \\\n        \'expected. To attempt automatic conversion, set \' \\\n        \'convert=True.\'.format(df.__class__)\n    df.to_csv(filename, **kwargs)\n\n\ndef load_dot(filename, force_graph=True):\n    """"""\n    Loads a graph saved in .dot format.\n    :param filename: a string or file-like object\n    :param force_graph: whether to force a conversion to nx.Graph after loading.\n    This may be useful in the case of .dot files being loaded as nx.MultiGraph.\n    :return: the loaded graph\n    """"""\n    output = nx.nx_agraph.read_dot(filename)\n    if force_graph:\n        output = nx.Graph(output)\n\n    for elem in output.nodes().values():\n        for k, v in elem.items():\n            try:\n                elem[k] = ast.literal_eval(v)\n            except ValueError:\n                elem[k] = str(v)\n            except SyntaxError:\n                # Probably a numpy array\n                elem[k] = np.array(\' \'.join(v.lstrip(\'[\')\n                                             .rstrip(\']\')\n                                             .split())\n                                      .split(\' \')).astype(np.float)\n\n    for elem in output.edges().values():\n        for k, v in elem.items():\n            try:\n                elem[k] = ast.literal_eval(v)\n            except ValueError:\n                elem[k] = str(v)\n\n    return output\n\n\ndef dump_dot(obj, filename):\n    """"""\n    Dumps a nx.Graph to .dot file\n    :param obj: the nx.Graph (or equivalent) to save\n    :param filename: a string or file-like object\n    """"""\n    nx.nx_agraph.write_dot(obj, filename)\n\n\ndef load_npy(filename):\n    """"""\n    Loads a file saved by np.save.\n    :param filename: a string or file-like object\n    :return: the loaded object\n    """"""\n    if sys.version_info[0] == 3:\n        return np.load(filename, encoding=\'latin1\')\n    else:\n        return np.load(filename)\n\n\ndef dump_npy(obj, filename, zipped=False):\n    """"""\n    Saves an object to file using the numpy format.\n    :param obj: the object to save\n    :param filename: a string or file-like object\n    :param zipped: boolean, whether to save the object in the zipped format .npz\n    rather than .npy\n    """"""\n    if zipped:\n        np.savez(filename, obj)\n    else:\n        np.save(filename, obj)\n\n\ndef load_txt(filename, **kwargs):\n    """"""\n    Loads a txt file using np.loadtxt.\n    :param filename: a string or file-like object\n    :return: the loaded object\n    """"""\n    return np.loadtxt(filename, **kwargs)\n\n\ndef dump_txt(obj, filename, **kwargs):\n    """"""\n    Saves an object to text file using np.savetxt.\n    :param obj: the object to save\n    :param filename: a string or file-like object\n    """"""\n    np.savetxt(filename, obj, **kwargs)\n\n\n# Reference for implementation:\n# # http://www.nonlinear.com/progenesis/sdf-studio/v0.9/faq/sdf-file-format-guidance.aspx\n#\n# While parsing the SDF file, molecules are stored in a dictionary like this:\n#\n# {\'atoms\': [{\'atomic_num\': 7,\n#             \'charge\': 0,\n#             \'coords\': array([-0.0299,  1.2183,  0.2994]),\n#             \'index\': 0,\n#             \'info\': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n#             \'iso\': 0},\n#            ...,\n#            {\'atomic_num\': 1,\n#             \'charge\': 0,\n#             \'coords\': array([ 0.6896, -2.3002, -0.1042]),\n#             \'index\': 14,\n#             \'info\': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n#             \'iso\': 0}],\n#  \'bonds\': [{\'end_atom\': 13,\n#             \'info\': array([0, 0, 0]),\n#             \'start_atom\': 4,\n#             \'stereo\': 0,\n#             \'type\': 1},\n#            ...,\n#            {\'end_atom\': 8,\n#             \'info\': array([0, 0, 0]),\n#             \'start_atom\': 7,\n#             \'stereo\': 0,\n#             \'type\': 3}],\n#  \'comment\': \'\',\n#  \'data\': [\'\'],\n#  \'details\': \'-OEChem-03231823253D\',\n#  \'n_atoms\': 15,\n#  \'n_bonds\': 15,\n#  \'name\': \'gdb_54964\',\n#  \'properties\': []}\nHEADER_SIZE = 3\n\n\ndef _parse_header(sdf):\n    try:\n        return sdf[0].strip(), sdf[1].strip(), sdf[2].strip()\n    except IndexError:\n        print(sdf)\n\n\ndef _parse_counts_line(sdf):\n    # 12 fields\n    # First 11 are 3 characters long\n    # Last one is 6 characters long\n    # First two give the number of atoms and bonds\n\n    values = sdf[HEADER_SIZE]\n    n_atoms = int(values[:3])\n    n_bonds = int(values[3:6])\n\n    return n_atoms, n_bonds\n\n\ndef _parse_atoms_block(sdf, n_atoms):\n    # The first three fields, 10 characters long each, describe the atom\'s\n    # position in the X, Y, and Z dimensions.\n    # After that there is a space, and three characters for an atomic symbol.\n    # After the symbol, there are two characters for the mass difference from\n    # the monoisotope.\n    # Next you have three characters for the charge.\n    # There are ten more fields with three characters each, but these are all\n    # rarely used.\n\n    start = HEADER_SIZE + 1  # Add 1 for counts line\n    stop = start + n_atoms\n    values = sdf[start:stop]\n\n    atoms = []\n    for i, v in enumerate(values):\n        coords = np.array([float(v[pos:pos+10]) for pos in range(0, 30, 10)])\n        atomic_num = get_atomic_num(v[31:34].strip())\n        iso = int(v[34:36])\n        charge = int(v[36:39])\n        info = np.array([int(v[pos:pos+3]) for pos in range(39, len(v), 3)])\n        atoms.append({\'index\': i,\n                      \'coords\': coords,\n                      \'atomic_num\': atomic_num,\n                      \'iso\': iso,\n                      \'charge\': charge,\n                      \'info\': info})\n    return atoms\n\n\ndef _parse_bonds_block(sdf, n_atoms, n_bonds):\n    # The first two fields are the indexes of the atoms included in this bond\n    # (starting from 1). The third field defines the type of bond, and the\n    # fourth the stereoscopy of the bond.\n    # There are a further three fields, with 3 characters each, but these are\n    # rarely used and can be left blank.\n\n    start = HEADER_SIZE + n_atoms + 1  # Add 1 for counts line\n    stop = start + n_bonds\n    values = sdf[start:stop]\n\n    bonds = []\n    for v in values:\n        start_atom = int(v[:3]) - 1\n        end_atom = int(v[3:6]) - 1\n        type_ = int(v[6:9])\n        stereo = int(v[9:12])\n        info = np.array([int(v[pos:pos + 3]) for pos in range(12, len(v), 3)])\n        bonds.append({\'start_atom\': start_atom,\n                      \'end_atom\': end_atom,\n                      \'type\': type_,\n                      \'stereo\': stereo,\n                      \'info\': info})\n    return bonds\n\n\ndef _parse_properties(sdf, n_atoms, n_bonds):\n    # TODO This just returns a list of properties.\n    # See https://docs.chemaxon.com/display/docs/MDL+MOLfiles%2C+RGfiles%2C+SDfiles%2C+Rxnfiles%2C+RDfiles+formats\n    # for documentation.\n\n    start = HEADER_SIZE + n_atoms + n_bonds + 1  # Add 1 for counts line\n    stop = sdf.index(\'M  END\')\n\n    return sdf[start:stop]\n\n\ndef _parse_data_fields(sdf):\n    # TODO This just returns a list of data fields.\n\n    start = sdf.index(\'M  END\') + 1\n\n    return sdf[start:] if start < len(sdf) else []\n\n\ndef parse_sdf(sdf):\n    sdf_out = {}\n    sdf = sdf.split(\'\\n\')\n    sdf_out[\'name\'], sdf_out[\'details\'], sdf_out[\'comment\'] = _parse_header(sdf)\n    sdf_out[\'n_atoms\'], sdf_out[\'n_bonds\'] = _parse_counts_line(sdf)\n    sdf_out[\'atoms\'] = _parse_atoms_block(sdf, sdf_out[\'n_atoms\'])\n    sdf_out[\'bonds\'] = _parse_bonds_block(sdf, sdf_out[\'n_atoms\'], sdf_out[\'n_bonds\'])\n    sdf_out[\'properties\'] = _parse_properties(sdf, sdf_out[\'n_atoms\'], sdf_out[\'n_bonds\'])\n    sdf_out[\'data\'] = _parse_data_fields(sdf)\n    return sdf_out\n\n\ndef parse_sdf_file(sdf_file, amount=None):\n    data = sdf_file.read().split(\'$$$$\\n\')\n    if data[-1] == \'\':\n        data = data[:-1]\n    if amount is not None:\n        data = data[:amount]\n    output = [parse_sdf(sdf) for sdf in data]  # Parallel execution doesn\'t help\n    return output\n\n\ndef load_sdf(filename, amount=None):\n    """"""\n    Load an .sdf file and return a list of molecules in the internal SDF format.\n    :param filename: target SDF file\n    :param amount: only load the first `amount` molecules from the file\n    :return: a list of molecules in the internal SDF format (see documentation).\n    """"""\n    print(\'Reading SDF\')\n    with open(filename) as f:\n        return parse_sdf_file(f, amount=amount)\n'"
spektral/utils/keras.py,0,"b""from tensorflow.keras import initializers, regularizers, constraints, activations\n\nLAYER_KWARGS = {'activation', 'use_bias'}\nKERAS_KWARGS = {'activity_regularizer', 'autocast', 'batch_input_shape',\n                'batch_size', 'input_shape', 'weights'}\n\n\ndef is_layer_kwarg(key):\n    return (key.endswith('_initializer')\n            or key.endswith('_regularizer')\n            or key.endswith('_constraint')\n            or key in LAYER_KWARGS) and not key in KERAS_KWARGS\n\n\ndef is_keras_kwarg(key):\n    return key in KERAS_KWARGS\n\n\ndef deserialize_kwarg(key, attr):\n    if key.endswith('_initializer'):\n        return initializers.get(attr)\n    if key.endswith('_regularizer'):\n        return regularizers.get(attr)\n    if key.endswith('_constraint'):\n        return constraints.get(attr)\n    if key == 'activation':\n        return activations.get(attr)\n\n\ndef serialize_kwarg(key, attr):\n    if key.endswith('_initializer'):\n        return initializers.serialize(attr)\n    if key.endswith('_regularizer'):\n        return regularizers.serialize(attr)\n    if key.endswith('_constraint'):\n        return constraints.serialize(attr)\n    if key == 'activation':\n        return activations.serialize(attr)\n    if key == 'use_bias':\n        return attr"""
spektral/utils/logging.py,0,"b'import os\nimport time\nfrom pprint import pformat\n\nLOGFILE = None\nTIME_STACK = []\n\n\ndef init_logging(name=None):\n    """"""\n    Creates a log directory with an empty log.txt file. \n    :param name: custom name for the log directory (default \\""%Y-%m-%d-%H-%M-%S\\"")\n    :return: string, the relative path to the log directory\n    """"""\n    global LOGFILE\n    if name is None:\n        name = time.strftime(""%Y-%m-%d-%H-%M-%S"")\n    log_dir = \'./logs/%s/\' % name\n    if not os.path.isdir(log_dir):\n        os.makedirs(log_dir)\n    LOGFILE = log_dir + \'log.txt\'\n    return log_dir\n\n\ndef log(message, print_string=True):\n    """"""\n    Prints a message to stdout and writes it to the logfile (requires user to\n    call init_logging() at least once in order to save to file).\n    :param message: the string to log;\n    :param print_string: whether to print the string to stdout;\n    """"""\n    global LOGFILE\n    message = pformat(message) if isinstance(message, dict) else str(message)\n    if print_string:\n        print(message)\n    if not message.endswith(\'\\n\'):\n        message += \'\\n\'\n    if LOGFILE:\n        with open(LOGFILE, \'a\') as f:\n            f.write(message)\n\n\ndef tic(message=None, print_string=True):\n    """"""\n    Start counting time.\n    :param message: additional message to print;\n    :param print_string: whether to print the string to stdout;\n    """"""\n    TIME_STACK.append(time.time())\n    if message:\n        log(str(message), print_string=print_string)\n\n\ndef toc(message=None, print_string=True):\n    """"""\n    Stop counting time.\n    :param message: additional message to print;\n    :param print_string: whether to print the string to stdout;\n    """"""\n    fmt = \'Elapsed: {:.2f}s\'\n    try:\n        output = fmt.format(time.time() - TIME_STACK.pop())\n        if message:\n            output = str(message) + \'\\n\' + output\n        log(output, print_string=print_string)\n    except IndexError:\n        print(""You have to tic() before you toc()\\n"")\n\n\ndef model_to_str(model):\n    """"""\n    Converts a Keras model to a string.\n    :param model: a Keras model;\n    :return: the output of `model.summary()` as a string;\n    """"""\n    def to_str(line):\n        model_to_str.output += str(line) + \'\\n\'\n    model_to_str.output = \'\'\n    model.summary(print_fn=lambda x: to_str(x))\n    return model_to_str.output\n'"
spektral/utils/misc.py,0,"b'import numpy as np\nfrom scipy import sparse as sp\n\n\ndef pad_jagged_array(x, target_shape, dtype=np.float):\n    """"""\n    Given a jagged array of arbitrary dimensions, zero-pads all elements in the\n    array to match the provided `target_shape`.\n    :param x: a list or np.array of dtype object, containing np.arrays of\n    varying dimensions\n    :param target_shape: a tuple or list s.t. target_shape[i] >= x.shape[i]\n    for each x in X.\n    If `target_shape[i] = -1`, it will be automatically converted to X.shape[i], \n    so that passing a target shape of e.g. (-1, n, m) will leave the first \n    dimension of each element untouched (note that the creation of the output\n    array may fail if the result is again a jagged array). \n    :param dtype: the dtype of the returned np.array\n    :return: a zero-padded np.array of shape `(X.shape[0], ) + target_shape`\n    """"""\n    if isinstance(x, list):\n        x = np.array(x)\n    for i in range(len(x)):\n        shapes = []\n        for j in range(len(target_shape)):\n            ts = target_shape[j]\n            cs = x[i].shape[j]\n            shapes.append((cs if ts == -1 else ts, cs))\n        if x.ndim == 1:\n            x[i] = np.pad(x[i], [(0, ts - cs) for ts, cs in shapes], \'constant\')\n        else:\n            x = np.pad(x, [(0, 0)] + [(0, ts - cs) for ts, cs in shapes], \'constant\')\n\n    try:\n        return np.array(x, dtype=dtype)\n    except ValueError:\n        return np.array([_ for _ in x], dtype=dtype)\n\n\ndef add_eye(x):\n    """"""\n    Adds the identity matrix to the given matrix.\n    :param x: a rank 2 np.array or scipy.sparse matrix\n    :return: a rank 2 np.array or scipy.sparse matrix\n    """"""\n    if x.ndim != 2:\n        raise ValueError(\'X must be of rank 2 but has rank {}.\'.format(x.ndim))\n    if sp.issparse(x):\n        eye = sp.eye(x.shape[0])\n    else:\n        eye = np.eye(x.shape[0])\n    return x + eye\n\n\ndef sub_eye(x):\n    """"""\n    Subtracts the identity matrix from the given matrix.\n    :param x: a rank 2 np.array or scipy.sparse matrix\n    :return: a rank 2 np.array or scipy.sparse matrix\n    """"""\n    if x.ndim != 2:\n        raise ValueError(\'x must be of rank 2 but has rank {}.\'.format(x.ndim))\n    if sp.issparse(x):\n        eye = sp.eye(x.shape[0])\n    else:\n        eye = np.eye(x.shape[0])\n    return x - eye\n\n\ndef add_eye_batch(x):\n    """"""\n    Adds the identity matrix to each submatrix of the given rank 3 array.\n    :param x: a rank 3 np.array\n    :return: a rank 3 np.array\n    """"""\n    if x.ndim != 3:\n        raise ValueError(\'x must be of rank 3 but has rank {}.\'.format(x.ndim))\n    return x + np.eye(x.shape[1])[None, ...]\n\n\ndef sub_eye_batch(x):\n    """"""\n    Subtracts the identity matrix from each submatrix of the given rank 3\n    array.\n    :param x: a rank 3 np.array\n    :return: a rank 3 np.array\n    """"""\n    if x.ndim != 3:\n        raise ValueError(\'x must be of rank 3 but has rank {}.\'.format(x.ndim))\n    return x - np.repeat(np.eye(x.shape[1])[None, ...], x.shape[0], axis=0)\n\n\ndef add_eye_jagged(x):\n    """"""\n    Adds the identity matrix to each submatrix of the given rank 3 jagged array.\n    :param x: a rank 3 jagged np.array\n    :return: a rank 3 jagged np.array\n    """"""\n    x_out = x.copy()\n    for i in range(len(x)):\n        if x[i].ndim != 2:\n            raise ValueError(\'Jagged array must only contain 2d slices\')\n        x_out[i] = add_eye(x[i])\n    return x_out\n\n\ndef sub_eye_jagged(x):\n    """"""\n    Subtracts the identity matrix from each submatrix of the given rank 3\n    jagged array.\n    :param x: a rank 3 jagged np.array\n    :return: a rank 3 jagged np.array\n    """"""\n    x_out = x.copy()\n    for i in range(len(x)):\n        if x[i].ndim != 2:\n            raise ValueError(\'Jagged array must only contain 2d slices\')\n        x_out[i] = sub_eye(x[i])\n    return x_out\n\n\ndef int_to_one_hot(x, n=None):\n    """"""\n    Encodes x in a 1-of-n array. \n    :param x: an integer or array of integers, such that x < n\n    :param n: an integer\n    :return: an array of shape (x.shape[0], n) if x is an array, (n, ) if\n    x is an integer\n    """"""\n    if isinstance(x, int):\n        if n is None:\n            raise ValueError(\'n is required to one-hot encode a single integer\')\n        if x >= n:\n            raise ValueError(\'x must be smaller than n in order to one-hot encode\')\n        output = np.zeros((n,))\n        output[x] = 1\n    else:\n        if n is None:\n            n = int(np.max(x) + 1)\n        else:\n            if np.max(x) >= n:\n                raise ValueError(\'The maximum value in x ({}) is greater than \'\n                                 \'n ({}), therefore 1-of-n encoding is not \'\n                                 \'possible\'.format(np.max(x), n))\n        x = np.array(x, dtype=np.int)\n        if x.ndim is 1:\n            x = x[:, None]\n        orig_shp = x.shape\n        x = np.reshape(x, (-1, orig_shp[-1]))\n        output = np.zeros((x.shape[0], n))\n        output[np.arange(x.shape[0]), x.squeeze()] = 1\n        output = output.reshape(orig_shp[:-1] + (n,))\n\n    return output\n\n\ndef label_to_one_hot(x, labels=None):\n    """"""\n    Encodes x in a 1-of-n array. \n    :param x: any object or array of objects s.t. x is contained in `labels`. \n    The function may behave unexpectedly if x is a single object but \n    `hasattr(x, \'__len__\')`, and works best with integers or discrete entities.\n    :param labels: a list of n labels to compute the one-hot vector \n    :return: an array of shape (x.shape[0], n) if x is an array, (n, ) if\n    x is a single object\n    """"""\n    n = len(labels)\n    labels_idx = {l: i for i, l in enumerate(labels)}\n    if not hasattr(x, \'__len__\'):\n        output = np.zeros((n,))\n        output[labels_idx[x]] = 1\n    else:\n        x = np.array(x, dtype=np.int)\n        orig_shp = x.shape\n        x = np.reshape(x, (-1))\n        output = np.zeros((x.shape[0], n))\n        for i in range(len(x)):\n            try:\n                output[i, labels_idx[x[i]]] = 1\n            except KeyError:\n                pass\n        if len(orig_shp) == 1:\n            output_shape = orig_shp + (n,)\n        else:\n            output_shape = orig_shp[:-1] + (n,)\n        output = output.reshape(output_shape)\n\n    return output\n\n\ndef flatten_list_gen(alist):\n    """"""\n    Performs a depth-first visit of an arbitrarily nested list and yields its \n    element in order. \n    :param alist: a list or np.array (with at least one dimension), \n                  arbitrarily nested.\n    """"""\n    for item in alist:\n        if isinstance(item, list) or isinstance(item, np.ndarray):\n            for i in flatten_list_gen(item):\n                yield i\n        else:\n            yield item\n\n\ndef flatten_list(alist):\n    """"""\n    Flattens an arbitrarily nested list to 1D.\n    :param alist: a list or np.array (with at least one dimension), \n                  arbitrarily nested.\n    :return: a 1D Python list with the flattened elements as returned by a \n             depth-first search.\n    """"""\n    return list(flatten_list_gen(alist))\n\n\n'"
tests/test_layers/__init__.py,0,b''
tests/test_layers/test_convolutional.py,1,"b'import numpy as np\nimport tensorflow as tf\nfrom tensorflow.keras import Model, Input\n\nfrom spektral.layers import GraphConv, ChebConv, EdgeConditionedConv, GraphAttention, \\\n    GraphConvSkip, ARMAConv, APPNP, GraphSageConv, GINConv, DiffusionConv, \\\n    GatedGraphConv, AGNNConv, TAGConv, CrystalConv, MessagePassing, EdgeConv\nfrom spektral.layers.ops import sp_matrix_to_sp_tensor\n\ntf.keras.backend.set_floatx(\'float64\')\nSINGLE, BATCH, MIXED = 1, 2, 3  # Single, batch, mixed\nLAYER_K_, MODES_K_, KWARGS_K_ = \'layer\', \'modes\', \'kwargs\'\nbatch_size = 32\nN = 11\nF = 7\nS = 3\nA = np.ones((N, N))\nX = np.random.normal(size=(N, F))\nE = np.random.normal(size=(N, N, S))\nE_single = np.random.normal(size=(N * N, S))\n\n\n""""""\nEach entry in TESTS represent a test to be run for a particular Layer.\nEach config dictionary has the form: \n{\n    LAYER_K_: LayerClass,\n    MODES_K_: [...],\n    KWARGS_K_: {...},\n},\n\nLAYER_K_ is the class of the layer to be tested.\n \nMODES_K_ is a list containing the data modes supported by the model, and should \nbe at least one of: SINGLE, MIXED, BATCH. \n\nKWARGS_K_ is a dictionary containing: \n    - all keywords to be passed to the layer (including mandatory ones);\n    - an optional entry \'edges\': True if the layer supports edge attributes; \n    - an optional entry \'sparse\': [...], indicating whether the layer supports \n    sparse or dense inputs as a bool (e.g., \'sparse\': [False, True] will \n    test the layer on both dense and sparse adjacency matrix; \'sparse\': [True] \n    will only test for sparse). By default, each layer is tested only on dense\n    inputs. Batch mode only tests for dense inputs. \n\nThe testing loop will create a simple 1-layer model and run it in single, mixed, \nand batch mode according the what specified in MODES_K_ in the testing config. \nThe loop will check: \n    - that the model does not crash; \n    - that the output shape is pre-computed correctly; \n    - that the real output shape is correct; \n    - that the get_config() method works correctly (i.e., it is possible to \n    re-instatiate a layer using LayerClass(**layer_instance.get_config())).\n""""""\n\nTESTS = [\n    {\n        LAYER_K_: GraphConv,\n        MODES_K_: [SINGLE, BATCH, MIXED],\n        KWARGS_K_: {\'channels\': 8, \'activation\': \'relu\', \'sparse\': [False, True]},\n    },\n    {\n        LAYER_K_: ChebConv,\n        MODES_K_: [SINGLE, BATCH, MIXED],\n        KWARGS_K_: {\'K\': 3, \'channels\': 8, \'activation\': \'relu\', \'sparse\': [False, True]}\n    },\n    {\n        LAYER_K_: GraphSageConv,\n        MODES_K_: [SINGLE],\n        KWARGS_K_: {\'channels\': 8, \'activation\': \'relu\', \'sparse\': [False, True]}\n    },\n    {\n        LAYER_K_: EdgeConditionedConv,\n        MODES_K_: [SINGLE, BATCH],\n        KWARGS_K_: {\'kernel_network\': [8], \'channels\': 8, \'activation\': \'relu\',\n                    \'edges\': True, \'sparse\': [False, True]}\n    },\n    {\n        LAYER_K_: GraphAttention,\n        MODES_K_: [SINGLE, BATCH, MIXED],\n        KWARGS_K_: {\'channels\': 8, \'attn_heads\': 2, \'concat_heads\': False,\n                    \'activation\': \'relu\', \'sparse\': [False, True]}\n    },\n    {\n        LAYER_K_: GraphConvSkip,\n        MODES_K_: [SINGLE, BATCH, MIXED],\n        KWARGS_K_: {\'channels\': 8, \'activation\': \'relu\', \'sparse\': [False, True]}\n    },\n    {\n        LAYER_K_: ARMAConv,\n        MODES_K_: [SINGLE, BATCH, MIXED],\n        KWARGS_K_: {\'channels\': 8, \'activation\': \'relu\', \'order\': 2, \'iterations\': 2,\n                    \'share_weights\': True, \'sparse\': [False, True]}\n    },\n    {\n        LAYER_K_: APPNP,\n        MODES_K_: [SINGLE, BATCH, MIXED],\n        KWARGS_K_: {\'channels\': 8, \'activation\': \'relu\', \'mlp_hidden\': [16],\n                    \'sparse\': [False, True]}\n    },\n    {\n        LAYER_K_: GINConv,\n        MODES_K_: [SINGLE],\n        KWARGS_K_: {\'channels\': 8, \'activation\': \'relu\', \'mlp_hidden\': [16],\n                    \'sparse\': [True]}\n    },\n    {\n        LAYER_K_: DiffusionConv,\n        MODES_K_: [SINGLE, BATCH, MIXED],\n        KWARGS_K_: {\'channels\': 8, \'activation\': \'tanh\', \'num_diffusion_steps\': 5,\n                    \'sparse\': [False]}\n    },\n    {\n        LAYER_K_: GatedGraphConv,\n        MODES_K_: [SINGLE],\n        KWARGS_K_: {\'channels\': 10, \'n_layers\': 3, \'sparse\': [True]}\n    },\n    {\n        LAYER_K_: AGNNConv,\n        MODES_K_: [SINGLE],\n        KWARGS_K_: {\'channels\': F, \'trainable\': True, \'sparse\': [True]}\n    },\n    {\n        LAYER_K_: TAGConv,\n        MODES_K_: [SINGLE],\n        KWARGS_K_: {\'channels\': F, \'K\': 3, \'sparse\': [True]}\n    },\n    {\n        LAYER_K_: CrystalConv,\n        MODES_K_: [SINGLE],\n        KWARGS_K_: {\'channels\': F, \'edges\': True, \'sparse\': [True]}\n    },\n    {\n        LAYER_K_: EdgeConv,\n        MODES_K_: [SINGLE],\n        KWARGS_K_: {\'channels\': 8, \'activation\': \'relu\', \'mlp_hidden\': [16],\n                    \'sparse\': [True]}\n    },\n    {\n        LAYER_K_: MessagePassing,\n        MODES_K_: [SINGLE],\n        KWARGS_K_: {\'channels\': F, \'sparse\': [True]}\n    },\n]\n\n\ndef _test_single_mode(layer, **kwargs):\n    sparse = kwargs.pop(\'sparse\', False)\n    A_in = Input(shape=(None,), sparse=sparse)\n    X_in = Input(shape=(F,))\n    inputs = [X_in, A_in]\n    if sparse:\n        input_data = [X, sp_matrix_to_sp_tensor(A)]\n    else:\n        input_data = [X, A]\n\n    if kwargs.pop(\'edges\', None):\n        E_in = Input(shape=(S, ))\n        inputs.append(E_in)\n        input_data.append(E_single)\n\n    layer_instance = layer(**kwargs)\n    output = layer_instance(inputs)\n    model = Model(inputs, output)\n\n    output = model(input_data)\n\n    assert output.shape == (N, kwargs[\'channels\'])\n\n\ndef _test_batch_mode(layer, **kwargs):\n    A_batch = np.stack([A] * batch_size)\n    X_batch = np.stack([X] * batch_size)\n\n    A_in = Input(shape=(N, N))\n    X_in = Input(shape=(N, F))\n    inputs = [X_in, A_in]\n    input_data = [X_batch, A_batch]\n\n    if kwargs.pop(\'edges\', None):\n        E_batch = np.stack([E] * batch_size)\n        E_in = Input(shape=(N, N, S))\n        inputs.append(E_in)\n        input_data.append(E_batch)\n\n    layer_instance = layer(**kwargs)\n    output = layer_instance(inputs)\n    model = Model(inputs, output)\n\n    output = model(input_data)\n\n    assert output.shape == (batch_size, N, kwargs[\'channels\'])\n\n\ndef _test_mixed_mode(layer, **kwargs):\n    sparse = kwargs.pop(\'sparse\', False)\n    X_batch = np.stack([X] * batch_size)\n    A_in = Input(shape=(N,), sparse=sparse)\n    X_in = Input(shape=(N, F))\n    inputs = [X_in, A_in]\n    if sparse:\n        input_data = [X_batch, sp_matrix_to_sp_tensor(A)]\n    else:\n        input_data = [X_batch, A]\n\n    layer_instance = layer(**kwargs)\n    output = layer_instance(inputs)\n    model = Model(inputs, output)\n\n    output = model(input_data)\n\n    assert output.shape == (batch_size, N, kwargs[\'channels\'])\n\n\ndef _test_get_config(layer, **kwargs):\n    if kwargs.get(\'edges\'):\n        kwargs.pop(\'edges\')\n    layer_instance = layer(**kwargs)\n    config = layer_instance.get_config()\n    assert layer(**config)\n\n\ndef test_layers():\n    for test in TESTS:\n        for mode in test[MODES_K_]:\n            if mode == SINGLE:\n                if \'sparse\' in test[KWARGS_K_]:\n                    sparse = test[KWARGS_K_].pop(\'sparse\')\n                    for s in sparse:\n                        _test_single_mode(test[LAYER_K_], sparse=s, **test[KWARGS_K_])\n                else:\n                    _test_single_mode(test[LAYER_K_], **test[KWARGS_K_])\n            elif mode == BATCH:\n                _test_batch_mode(test[LAYER_K_], **test[KWARGS_K_])\n            elif mode == MIXED:\n                if \'sparse\' in test[KWARGS_K_]:\n                    sparse = test[KWARGS_K_].pop(\'sparse\')\n                    for s in sparse:\n                        _test_mixed_mode(test[LAYER_K_], sparse=s, **test[KWARGS_K_])\n                else:\n                    _test_mixed_mode(test[LAYER_K_], **test[KWARGS_K_])\n        _test_get_config(test[LAYER_K_], **test[KWARGS_K_])\n'"
tests/test_layers/test_global_pooling.py,1,"b""import numpy as np\nimport tensorflow as tf\nfrom tensorflow.keras import Input, Model\n\nfrom spektral.layers import GlobalSumPool, GlobalAttnSumPool, GlobalAttentionPool, GlobalAvgPool, GlobalMaxPool, SortPool\n\ntf.keras.backend.set_floatx('float64')\nbatch_size = 32\nN = 11\nF = 7\n\n\ndef _check_output_and_model_output_shapes(true_shape, model_shape):\n    assert len(true_shape) == len(model_shape)\n    for i in range(len(true_shape)):\n        if true_shape[i] == N:\n            assert model_shape[i] in {N, None}\n        elif true_shape[i] == batch_size:\n            assert model_shape[i] in {batch_size, None}\n        else:\n            assert model_shape[i] == true_shape[i]\n\n\ndef _test_single_mode(layer, **kwargs):\n    X = np.random.normal(size=(N, F))\n    X_in = Input(shape=(F,))\n    layer_instance = layer(**kwargs)\n    output = layer_instance(X_in)\n    model = Model(X_in, output)\n\n    output = model(X)\n    assert output.shape == (1, kwargs.get('channels', F))\n    assert output.shape == layer_instance.compute_output_shape(X.shape)\n    _check_output_and_model_output_shapes(output.shape, model.output_shape)\n\n\ndef _test_batch_mode(layer, **kwargs):\n    X = np.random.normal(size=(batch_size, N, F))\n    X_in = Input(shape=(N, F))\n    layer_instance = layer(**kwargs)\n    output = layer_instance(X_in)\n    model = Model(X_in, output)\n    output = model(X)\n    assert output.shape == (batch_size, kwargs.get('channels', F))\n    assert output.shape == layer_instance.compute_output_shape(X.shape)\n    _check_output_and_model_output_shapes(output.shape, model.output_shape)\n\n\ndef _test_graph_mode(layer, **kwargs):\n    X = np.random.normal(size=(batch_size * N, F))\n    S = np.repeat(np.arange(batch_size), N).astype(np.int)\n    X_in = Input(shape=(F,))\n    S_in = Input(shape=(), dtype=S.dtype)\n    layer_instance = layer(**kwargs)\n    output = layer_instance([X_in, S_in])\n    model = Model([X_in, S_in], output)\n    output = model([X, S])\n    assert output.shape == (batch_size, kwargs.get('channels', F))\n    # When creating actual graph, the bacth dimension is None\n    assert output.shape[1:] == layer_instance.compute_output_shape([X.shape, S.shape])[\n        1:]\n    _check_output_and_model_output_shapes(output.shape, model.output_shape)\n\n\ndef _test_sortpool_single(layer, k):\n    X = np.random.normal(size=(N, F))\n    X_in = Input(shape=(F,))\n    layer_instance = layer(k=k)\n    output = layer_instance(X_in)\n    model = Model(X_in, output)\n\n    output = model(X)\n    assert output.shape == (k, F)\n    assert output.shape == layer_instance.compute_output_shape(X.shape)\n\n\ndef _test_sortpool_batched(layer, k):\n    X = np.random.normal(size=(batch_size, N, F))\n    X_in = Input(shape=(N, F))\n    layer_instance = layer(k=k)\n    output = layer_instance(X_in)\n    model = Model(X_in, output)\n    output = model(X)\n    assert output.shape == (batch_size, k, F)\n    assert output.shape == layer_instance.compute_output_shape(X.shape)\n\n\ndef test_global_sum_pool():\n    _test_single_mode(GlobalSumPool)\n    _test_batch_mode(GlobalSumPool)\n    _test_graph_mode(GlobalSumPool)\n\n\ndef test_global_avg_pool():\n    _test_single_mode(GlobalAvgPool)\n    _test_batch_mode(GlobalAvgPool)\n    _test_graph_mode(GlobalAvgPool)\n\n\ndef test_global_max_pool():\n    _test_single_mode(GlobalMaxPool)\n    _test_batch_mode(GlobalMaxPool)\n    _test_graph_mode(GlobalMaxPool)\n\n\ndef test_global_node_attention_pool():\n    _test_single_mode(GlobalAttnSumPool)\n    _test_batch_mode(GlobalAttnSumPool)\n    _test_graph_mode(GlobalAttnSumPool)\n\n\ndef test_global_attention_pool():\n    F_ = 10\n    assert F_ != F\n    _test_single_mode(GlobalAttentionPool, channels=F_)\n    _test_batch_mode(GlobalAttentionPool, channels=F_)\n    _test_graph_mode(GlobalAttentionPool, channels=F_)\n\n\ndef test_global_sort_pool():\n    _test_sortpool_single(SortPool, k=6)\n    _test_sortpool_batched(SortPool, k=6)\n"""
tests/test_layers/test_ops.py,7,"b""import numpy as np\nimport tensorflow as tf\n\nfrom spektral.layers import ops\nfrom spektral.utils import convolution\n\nbatch_size = 10\nN = 3\ntol = 5.e-7\n\n\ndef _assert_all_close(output, expected_output):\n    try:\n        assert np.allclose(output, expected_output, atol=tol)\n    except AssertionError:\n        mean_diff = np.mean(np.absolute(output - expected_output))\n        max_diff = np.max(np.absolute(output - expected_output))\n        if mean_diff <= tol:\n            print('Max difference above tolerance, but mean OK.')\n        else:\n            raise AssertionError('Mean diff: {}, Max diff: {}'.format(\n                mean_diff, max_diff\n            ))\n\n\ndef _convert_to_sparse_tensor(x):\n    if x.ndim == 2:\n        return ops.sp_matrix_to_sp_tensor(x)\n    elif x.ndim == 3:\n        s1_, s2_, s3_ = x.shape\n        return ops.reshape(\n            ops.sp_matrix_to_sp_tensor(x.reshape(s1_ * s2_, s3_)),\n            (s1_, s2_, s3_)\n        )\n\n\ndef _cast_all_to_dtype(values, dtype):\n    return [tf.cast(v, dtype) for v in values]\n\n\ndef _check_op(op, numpy_inputs, expected_output, convert_to_sparse=None, **kwargs):\n    output = _check_op_dense(op, numpy_inputs, **kwargs)\n    _assert_all_close(output, expected_output)\n\n    if convert_to_sparse:\n        if isinstance(convert_to_sparse, list):\n            if isinstance(convert_to_sparse[0], bool):\n                # Make it into a list of list, always\n                convert_to_sparse = [convert_to_sparse]\n\n        for c_t_s in convert_to_sparse:\n            output = _check_op_sparse(op, numpy_inputs, c_t_s, **kwargs)\n            _assert_all_close(output, expected_output)\n\n\ndef _check_op_dense(op, numpy_inputs, **kwargs):\n    tf_inputs = [tf.convert_to_tensor(x) for x in numpy_inputs]\n    tf_inputs = _cast_all_to_dtype(tf_inputs, np.float32)\n\n    output = op(*tf_inputs, **kwargs)\n    if isinstance(output, tf.SparseTensor):\n        # Sometimes ops with dense inputs return sparse tensors\n        return tf.sparse.to_dense(output).numpy()\n    return np.asarray(output)\n\n\ndef _check_op_sparse(op, numpy_inputs, convert_to_sparse, **kwargs):\n    tf_inputs = []\n    for i in range(len(numpy_inputs)):\n        if convert_to_sparse[i]:\n            tf_inputs.append(\n                _convert_to_sparse_tensor(numpy_inputs[i])\n            )\n        else:\n            tf_inputs.append(\n                tf.convert_to_tensor(numpy_inputs[i])\n            )\n    tf_inputs = _cast_all_to_dtype(tf_inputs, np.float32)\n\n    output = op(*tf_inputs, **kwargs)\n\n    if hasattr(output, 'toarray'):\n        return output.toarray()\n    elif hasattr(output, 'numpy'):\n        return output.numpy()\n    elif isinstance(output, tf.SparseTensor):\n        return tf.sparse.to_dense(output).numpy()\n    else:\n        return np.asarray(output)\n\n\ndef test_matmul_ops_single_mode():\n    A = np.random.randn(N, N)\n    B = np.random.randn(N, N)\n    convert_to_sparse = [[True, False], [False, True], [True, True]]\n\n    _check_op(ops.matmul_A_B, [A, B], A.dot(B), convert_to_sparse)\n    _check_op(ops.matmul_AT_B_A, [A, B], A.T.dot(B).dot(A), convert_to_sparse)\n    _check_op(ops.matmul_AT_B, [A, B], A.T.dot(B), convert_to_sparse)\n    _check_op(ops.matmul_A_BT, [A, B], A.dot(B.T), convert_to_sparse)\n\n\ndef test_matmul_ops_mixed_mode():\n    A = np.random.randn(N, N)\n    B = np.random.randn(batch_size, N, N)\n    convert_to_sparse = [[True, False], [False, True], [True, True]]\n\n    # A * B\n    expected_output = np.array([A.dot(B[i]) for i in range(batch_size)])\n    _check_op(ops.matmul_A_B, [A, B], expected_output, convert_to_sparse)\n\n    # A.T * B * A\n    expected_output = np.array([A.T.dot(B[i]).dot(A) for i in range(batch_size)])\n    _check_op(ops.matmul_AT_B_A, [A, B], expected_output, convert_to_sparse)\n\n    # A.T * B\n    expected_output = np.array([A.T.dot(B[i]) for i in range(batch_size)])\n    _check_op(ops.matmul_AT_B, [A, B], expected_output, convert_to_sparse)\n\n    # A * B.T\n    expected_output = np.array([A.dot(B[i].T) for i in range(batch_size)])\n    _check_op(ops.matmul_A_BT, [A, B], expected_output, convert_to_sparse)\n\n\ndef test_matmul_ops_inv_mixed_mode():\n    A = np.random.randn(batch_size, N, N)\n    B = np.random.randn(N, N)\n    convert_to_sparse = [[True, False], [False, True], [True, True]]\n\n    # A * B\n    expected_output = np.array([A[i].dot(B) for i in range(batch_size)])\n    _check_op(ops.matmul_A_B, [A, B], expected_output, convert_to_sparse)\n\n    # A.T * B * A\n    expected_output = np.array([A[i].T.dot(B).dot(A[i]) for i in range(batch_size)])\n    _check_op(ops.matmul_AT_B_A, [A, B], expected_output, convert_to_sparse)\n\n    # A.T * B\n    expected_output = np.array([A[i].T.dot(B) for i in range(batch_size)])\n    _check_op(ops.matmul_AT_B, [A, B], expected_output, convert_to_sparse)\n\n    # A * B.T\n    expected_output = np.array([A[i].dot(B.T) for i in range(batch_size)])\n    _check_op(ops.matmul_A_BT, [A, B], expected_output, convert_to_sparse)\n\n\ndef test_matmul_ops_batch_mode():\n    A = np.random.randn(batch_size, N, N)\n    B = np.random.randn(batch_size, N, N)\n    convert_to_sparse = [[True, False], [False, True], [True, True]]\n\n    # A * B\n    expected_output = np.array([A[i].dot(B[i]) for i in range(batch_size)])\n    _check_op(ops.matmul_A_B, [A, B], expected_output, convert_to_sparse)\n\n    # A.T * B * A\n    expected_output = np.array([A[i].T.dot(B[i]).dot(A[i]) for i in range(batch_size)])\n    _check_op(ops.matmul_AT_B_A, [A, B], expected_output, convert_to_sparse)\n\n    # A.T * B\n    expected_output = np.array([A[i].T.dot(B[i]) for i in range(batch_size)])\n    _check_op(ops.matmul_AT_B, [A, B], expected_output, convert_to_sparse)\n\n    # A * B.T\n    expected_output = np.array([A[i].dot(B[i].T) for i in range(batch_size)])\n    _check_op(ops.matmul_A_BT, [A, B], expected_output, convert_to_sparse)\n\n\ndef test_graph_ops():\n    A = np.ones((N, N))\n    convert_to_sparse = [[True]]\n\n    expected_output = convolution.normalized_adjacency(A)\n    _check_op(ops.normalize_A, [A], expected_output, convert_to_sparse)\n\n    expected_output = convolution.degree_matrix(A).sum(-1)\n    _check_op(ops.degrees, [A], expected_output, convert_to_sparse)\n\n    expected_output = convolution.degree_matrix(A)\n    _check_op(ops.degree_matrix, [A], expected_output, convert_to_sparse)\n\n\ndef test_misc_ops():\n    convert_to_sparse = [[True]]\n\n    # Transpose\n    for perm in [(1, 0), (0, 2, 1), (2, 1, 0)]:\n        A = np.random.randn(*[N] * len(perm))\n        expected_output = np.transpose(A, axes=perm)\n        _check_op(ops.transpose, [A], expected_output, convert_to_sparse, perm=perm)\n\n    # Reshape\n    A = np.random.randn(4, 5)\n    for shape in [(-1, 4), (5, -1)]:\n        expected_output = np.reshape(A, shape)\n        _check_op(ops.reshape, [A], expected_output, convert_to_sparse, shape=shape)\n\n    # Matrix power\n    A = np.random.randn(N, N)\n    k = 4\n    expected_output = np.linalg.matrix_power(A, k)\n    _check_op(ops.matrix_power, [A], expected_output, convert_to_sparse, k=k)\n\n    A = np.random.randn(batch_size, N, N)\n    k = 4\n    expected_output = np.array([np.linalg.matrix_power(a, k) for a in A])\n    _check_op(ops.matrix_power, [A], expected_output, convert_to_sparse, k=k)"""
tests/test_layers/test_pooling.py,2,"b""import numpy as np\nimport scipy.sparse as sp\nimport tensorflow as tf\nfrom tensorflow.keras import Input, Model\n\nfrom spektral.layers import TopKPool, MinCutPool, DiffPool, SAGPool, SortPool\ntf.keras.backend.set_floatx('float64')\n\nSINGLE, BATCH, DISJOINT = 1, 2, 3  # Single, batch, disjoint\nLAYER_K_, MODES_K_, KWARGS_K_ = 'layer', 'modes', 'kwargs'\nTESTS = [\n    {\n        LAYER_K_: TopKPool,\n        MODES_K_: [SINGLE, DISJOINT],\n        KWARGS_K_: {'ratio': 0.5, 'return_mask': True, 'sparse': True}\n    },\n    {\n        LAYER_K_: SAGPool,\n        MODES_K_: [SINGLE, DISJOINT],\n        KWARGS_K_: {'ratio': 0.5, 'return_mask': True, 'sparse': True}\n    },\n    {\n        LAYER_K_: MinCutPool,\n        MODES_K_: [SINGLE, BATCH],\n        KWARGS_K_: {'k': 5, 'return_mask': True, 'sparse': True}\n    },\n    {\n        LAYER_K_: DiffPool,\n        MODES_K_: [SINGLE, BATCH],\n        KWARGS_K_: {'k': 5, 'return_mask': True, 'sparse': True}\n    },\n\n]\n\nbatch_size = 3\nN1, N2, N3 = 4, 5, 2\nN = N1 + N2 + N3\nF = 7\n\n\ndef _check_output_and_model_output_shapes(true_shape, model_shape):\n    assert len(true_shape) == len(model_shape)\n    for i in range(len(true_shape)):\n        assert len(true_shape[i]) == len(model_shape[i])\n        for j in range(len(true_shape[i])):\n            assert model_shape[i][j] in {true_shape[i][j], None}\n\n\ndef _check_number_of_nodes(N_pool_expected, N_pool_true):\n    if N_pool_expected is not None:\n        assert N_pool_expected == N_pool_true or N_pool_true is None\n\n\ndef _test_single_mode(layer, **kwargs):\n    A = np.ones((N, N))\n    X = np.random.normal(size=(N, F))\n    sparse = kwargs.pop('sparse', None) is not None\n\n    A_in = Input(shape=(None, ), sparse=sparse)\n    X_in = Input(shape=(F,))\n\n    layer_instance = layer(**kwargs)\n    output = layer_instance([X_in, A_in])\n    model = Model([X_in, A_in], output)\n    output = model([X, A])\n    X_pool, A_pool, mask = output\n\n    if 'ratio' in kwargs.keys():\n        N_exp = kwargs['ratio'] * N\n    elif 'k' in kwargs.keys():\n        N_exp = kwargs['k']\n    else:\n        raise ValueError('Need k or ratio.')\n    N_pool_expected = int(np.ceil(N_exp))\n    N_pool_true = A_pool.shape[-1]\n\n    _check_number_of_nodes(N_pool_expected, N_pool_true)\n\n    assert X_pool.shape == (N_pool_expected, F)\n    assert A_pool.shape == (N_pool_expected, N_pool_expected)\n\n    output_shape = [o.shape for o in output]\n    _check_output_and_model_output_shapes(output_shape, model.output_shape)\n\n\ndef _test_batch_mode(layer, **kwargs):\n    A = np.ones((batch_size, N, N))\n    X = np.random.normal(size=(batch_size, N, F))\n\n    A_in = Input(shape=(N, N))\n    X_in = Input(shape=(N, F))\n\n    layer_instance = layer(**kwargs)\n    output = layer_instance([X_in, A_in])\n    model = Model([X_in, A_in], output)\n    output = model([X, A])\n    X_pool, A_pool, mask = output\n\n    if 'ratio' in kwargs.keys():\n        N_exp = kwargs['ratio'] * N\n    elif 'k' in kwargs.keys():\n        N_exp = kwargs['k']\n    else:\n        raise ValueError('Need k or ratio.')\n    N_pool_expected = int(np.ceil(N_exp))\n    N_pool_true = A_pool.shape[-1]\n\n    _check_number_of_nodes(N_pool_expected, N_pool_true)\n\n    assert X_pool.shape == (batch_size, N_pool_expected, F)\n    assert A_pool.shape == (batch_size, N_pool_expected, N_pool_expected)\n\n    output_shape = [o.shape for o in output]\n    _check_output_and_model_output_shapes(output_shape, model.output_shape)\n\n\ndef _test_disjoint_mode(layer, **kwargs):\n    A = sp.block_diag([np.ones((N1, N1)), np.ones(\n        (N2, N2)), np.ones((N3, N3))]).todense()\n    X = np.random.normal(size=(N, F))\n    I = np.array([0] * N1 + [1] * N2 + [2] * N3).astype(int)\n    sparse = kwargs.pop('sparse', None) is not None\n\n    A_in = Input(shape=(None, ), sparse=sparse)\n    X_in = Input(shape=(F,))\n    I_in = Input(shape=(), dtype=tf.int32)\n\n    layer_instance = layer(**kwargs)\n    output = layer_instance([X_in, A_in, I_in])\n    model = Model([X_in, A_in, I_in], output)\n    output = model([X, A, I])\n    X_pool, A_pool, I_pool, mask = output\n\n    N_pool_expected = np.ceil(kwargs['ratio'] * N1) + \\\n        np.ceil(kwargs['ratio'] * N2) + \\\n        np.ceil(kwargs['ratio'] * N3)\n    N_pool_expected = int(N_pool_expected)\n    N_pool_true = A_pool.shape[0]\n\n    _check_number_of_nodes(N_pool_expected, N_pool_true)\n\n    assert X_pool.shape == (N_pool_expected, F)\n    assert A_pool.shape == (N_pool_expected, N_pool_expected)\n    assert I_pool.shape == (N_pool_expected, )\n\n    output_shape = [o.shape for o in output]\n    _check_output_and_model_output_shapes(output_shape, model.output_shape)\n\n\ndef _test_get_config(layer, **kwargs):\n    if kwargs.get('edges'):\n        kwargs.pop('edges')\n    layer_instance = layer(**kwargs)\n    config = layer_instance.get_config()\n    assert layer(**config)\n\n\ndef test_layers():\n    for test in TESTS:\n        for mode in test[MODES_K_]:\n            if mode == SINGLE:\n                _test_single_mode(test[LAYER_K_], **test[KWARGS_K_])\n                if test[KWARGS_K_].pop('sparse', None):\n                    _test_single_mode(test[LAYER_K_], **test[KWARGS_K_])\n            elif mode == BATCH:\n                _test_batch_mode(test[LAYER_K_], **test[KWARGS_K_])\n            elif mode == DISJOINT:\n                _test_disjoint_mode(test[LAYER_K_], **test[KWARGS_K_])\n                if test[KWARGS_K_].pop('sparse', None):\n                    _test_disjoint_mode(test[LAYER_K_], **test[KWARGS_K_])\n        _test_get_config(test[LAYER_K_], **test[KWARGS_K_])\n"""
spektral/layers/convolutional/__init__.py,0,b'from .agnn_conv import AGNNConv\nfrom .appnp import APPNP\nfrom .arma_conv import ARMAConv\nfrom .cheb_conv import ChebConv\nfrom .crystal_conv import CrystalConv\nfrom .diffusion_conv import DiffusionConv\nfrom .ecc_conv import EdgeConditionedConv\nfrom .edge_conv import EdgeConv\nfrom .gated_graph_conv import GatedGraphConv\nfrom .gin_conv import GINConv\nfrom .graph_attention import GraphAttention\nfrom .graph_conv import GraphConv\nfrom .graph_conv_skip import GraphConvSkip\nfrom .graphsage_conv import GraphSageConv\nfrom .message_passing import MessagePassing\nfrom .tag_conv import TAGConv\n'
spektral/layers/convolutional/agnn_conv.py,1,"b'import tensorflow as tf\nfrom tensorflow.keras import backend as K\n\nfrom spektral.layers import ops\nfrom spektral.layers.convolutional.message_passing import MessagePassing\n\n\nclass AGNNConv(MessagePassing):\n    r""""""\n    An Attention-based Graph Neural Network (AGNN) as presented by\n    [Thekumparampil et al. (2018)](https://arxiv.org/abs/1803.03735).\n\n    **Mode**: single, disjoint.\n\n    **This layer expects a sparse adjacency matrix.**\n\n    This layer computes:\n    $$\n        \\Z = \\P\\X\n    $$\n    where\n    $$\n        \\P_{ij} = \\frac{\n            \\exp \\left( \\beta \\cos \\left( \\X_i, \\X_j \\right) \\right)\n        }{\n            \\sum\\limits_{k \\in \\mathcal{N}(i) \\cup \\{ i \\}}\n            \\exp \\left( \\beta \\cos \\left( \\X_i, \\X_k \\right) \\right)\n        }\n    $$\n    and \\(\\beta\\) is a trainable parameter.\n\n    **Input**\n\n    - Node features of shape `(N, F)`;\n    - Binary adjacency matrix of shape `(N, N)`.\n\n    **Output**\n\n    - Node features with the same shape of the input.\n\n    **Arguments**\n\n    - `trainable`: boolean, if True, then beta is a trainable parameter.\n    Otherwise, beta is fixed to 1;\n    - `activation`: activation function to use;\n    """"""\n\n    def __init__(self, trainable=True, activation=None, **kwargs):\n        super().__init__(aggregate=\'sum\', activation=activation, **kwargs)\n        self.trainable = trainable\n\n    def build(self, input_shape):\n        assert len(input_shape) >= 2\n        if self.trainable:\n            self.beta = self.add_weight(shape=(1,), initializer=\'ones\', name=\'beta\')\n        else:\n            self.beta = K.constant(1.)\n        self.built = True\n\n    def call(self, inputs, **kwargs):\n        X, A, E = self.get_inputs(inputs)\n        X_norm = K.l2_normalize(X, axis=-1)\n        output = self.propagate(X, A, E, X_norm=X_norm)\n        output = self.activation(output)\n\n        return output\n\n    def message(self, X, X_norm=None):\n        X_j = self.get_j(X)\n        X_norm_i = self.get_i(X_norm)\n        X_norm_j = self.get_j(X_norm)\n        alpha = self.beta * tf.reduce_sum(X_norm_i * X_norm_j, axis=-1)\n        alpha = ops.unsorted_segment_softmax(alpha, self.index_i, self.N)\n        alpha = alpha[:, None]\n\n        return alpha * X_j\n\n    def get_config(self):\n        config = {\n            \'trainable\': self.trainable,\n        }\n        base_config = super().get_config()\n        base_config.pop(\'aggregate\')  # Remove it because it\'s defined by constructor\n\n        return {**base_config, **config}\n'"
spektral/layers/convolutional/appnp.py,0,"b'from tensorflow.keras import activations\nfrom tensorflow.keras.layers import Dropout, Dense\nfrom tensorflow.keras.models import Sequential\n\nfrom spektral.layers import ops\nfrom spektral.layers.convolutional.graph_conv import GraphConv\n\n\nclass APPNP(GraphConv):\n    r""""""\n    A graph convolutional layer implementing the APPNP operator, as presented by\n    [Klicpera et al. (2019)](https://arxiv.org/abs/1810.05997).\n\n    This layer computes:\n    $$\n        \\Z^{(0)} = \\textrm{MLP}(\\X); \\\\\n        \\Z^{(K)} = (1 - \\alpha) \\hat \\D^{-1/2} \\hat \\A \\hat \\D^{-1/2} \\Z^{(K - 1)} +\n                   \\alpha \\Z^{(0)},\n    $$\n    where \\(\\alpha\\) is the _teleport_ probability and \\(\\textrm{MLP}\\) is a\n    multi-layer perceptron.\n\n    **Mode**: single, disjoint, mixed, batch.\n\n    **Input**\n\n    - Node features of shape `([batch], N, F)`;\n    - Modified Laplacian of shape `([batch], N, N)`; can be computed with\n    `spektral.utils.convolution.localpooling_filter`.\n\n    **Output**\n\n    - Node features with the same shape as the input, but with the last\n    dimension changed to `channels`.\n\n    **Arguments**\n\n    - `channels`: number of output channels;\n    - `alpha`: teleport probability during propagation;\n    - `propagations`: number of propagation steps;\n    - `mlp_hidden`: list of integers, number of hidden units for each hidden\n    layer in the MLP (if None, the MLP has only the output layer);\n    - `mlp_activation`: activation for the MLP layers;\n    - `dropout_rate`: dropout rate for Laplacian and MLP layers;\n    - `activation`: activation function to use;\n    - `use_bias`: bool, add a bias vector to the output;\n    - `kernel_initializer`: initializer for the weights;\n    - `bias_initializer`: initializer for the bias vector;\n    - `kernel_regularizer`: regularization applied to the weights;\n    - `bias_regularizer`: regularization applied to the bias vector;\n    - `activity_regularizer`: regularization applied to the output;\n    - `kernel_constraint`: constraint applied to the weights;\n    - `bias_constraint`: constraint applied to the bias vector.\n    """"""\n\n    def __init__(self,\n                 channels,\n                 alpha=0.2,\n                 propagations=1,\n                 mlp_hidden=None,\n                 mlp_activation=\'relu\',\n                 dropout_rate=0.0,\n                 activation=None,\n                 use_bias=True,\n                 kernel_initializer=\'glorot_uniform\',\n                 bias_initializer=\'zeros\',\n                 kernel_regularizer=None,\n                 bias_regularizer=None,\n                 activity_regularizer=None,\n                 kernel_constraint=None,\n                 bias_constraint=None,\n                 **kwargs):\n        super().__init__(channels,\n                         activation=activation,\n                         use_bias=use_bias,\n                         kernel_initializer=kernel_initializer,\n                         bias_initializer=bias_initializer,\n                         kernel_regularizer=kernel_regularizer,\n                         bias_regularizer=bias_regularizer,\n                         activity_regularizer=activity_regularizer,\n                         kernel_constraint=kernel_constraint,\n                         bias_constraint=bias_constraint,\n                         **kwargs)\n        self.mlp_hidden = mlp_hidden if mlp_hidden else []\n        self.alpha = alpha\n        self.propagations = propagations\n        self.mlp_activation = activations.get(mlp_activation)\n        self.dropout_rate = dropout_rate\n\n    def build(self, input_shape):\n        assert len(input_shape) >= 2\n        layer_kwargs = dict(\n            kernel_initializer=self.kernel_initializer,\n            bias_initializer=self.bias_initializer,\n            kernel_regularizer=self.kernel_regularizer,\n            bias_regularizer=self.bias_regularizer,\n            kernel_constraint=self.kernel_constraint,\n            bias_constraint=self.bias_constraint\n        )\n        mlp_layers = []\n        for i, channels in enumerate(self.mlp_hidden):\n            mlp_layers.extend([\n                Dropout(self.dropout_rate),\n                Dense(channels, self.mlp_activation, **layer_kwargs)\n            ])\n        mlp_layers.append(\n            Dense(self.channels, \'linear\', **layer_kwargs)\n        )\n        self.mlp = Sequential(mlp_layers)\n        self.built = True\n\n    def call(self, inputs):\n        features = inputs[0]\n        fltr = inputs[1]\n\n        # Compute MLP hidden features\n        mlp_out = self.mlp(features)\n\n        # Propagation\n        Z = mlp_out\n        for k in range(self.propagations):\n            Z = (1 - self.alpha) * ops.filter_dot(fltr, Z) + self.alpha * mlp_out\n\n        if self.activation is not None:\n            output = self.activation(Z)\n        else:\n            output = Z\n        return output\n\n    def get_config(self):\n        config = {\n            \'alpha\': self.alpha,\n            \'propagations\': self.propagations,\n            \'mlp_hidden\': self.mlp_hidden,\n            \'mlp_activation\': activations.serialize(self.mlp_activation),\n            \'dropout_rate\': self.dropout_rate,\n        }\n        base_config = super().get_config()\n        return dict(list(base_config.items()) + list(config.items()))\n'"
spektral/layers/convolutional/arma_conv.py,0,"b'from tensorflow.keras import activations, backend as K\nfrom tensorflow.keras.layers import Dropout\n\nfrom spektral.layers import ops\nfrom spektral.layers.convolutional.graph_conv import GraphConv\nfrom spektral.utils import normalized_laplacian, rescale_laplacian\n\n\nclass ARMAConv(GraphConv):\n    r""""""\n    A graph convolutional layer with ARMA\\(_K\\) filters, as presented by\n    [Bianchi et al. (2019)](https://arxiv.org/abs/1901.01343).\n\n    **Mode**: single, disjoint, mixed, batch.\n\n    This layer computes:\n    $$\n        \\Z = \\frac{1}{K} \\sum\\limits_{k=1}^K \\bar\\X_k^{(T)},\n    $$\n    where \\(K\\) is the order of the ARMA\\(_K\\) filter, and where:\n    $$\n        \\bar \\X_k^{(t + 1)} =\n        \\sigma \\left(\\tilde \\L \\bar \\X^{(t)} \\W^{(t)} + \\X \\V^{(t)} \\right)\n    $$\n    is a recursive approximation of an ARMA\\(_1\\) filter, where\n    \\( \\bar \\X^{(0)} = \\X \\)\n    and\n    $$\n        \\tilde \\L =  \\frac{2}{\\lambda_{max}} \\cdot (\\I - \\D^{-1/2} \\A \\D^{-1/2}) - \\I\n    $$\n    is the normalized Laplacian with a rescaled spectrum.\n\n    **Input**\n\n    - Node features of shape `([batch], N, F)`;\n    - Normalized and rescaled Laplacian of shape `([batch], N, N)`; can be\n    computed with `spektral.utils.convolution.normalized_laplacian` and\n    `spektral.utils.convolution.rescale_laplacian`.\n\n    **Output**\n\n    - Node features with the same shape as the input, but with the last\n    dimension changed to `channels`.\n\n    **Arguments**\n\n    - `channels`: number of output channels;\n    - `order`: order of the full ARMA\\(_K\\) filter, i.e., the number of parallel\n    stacks in the layer;\n    - `iterations`: number of iterations to compute each ARMA\\(_1\\) approximation;\n    - `share_weights`: share the weights in each ARMA\\(_1\\) stack.\n    - `gcn_activation`: activation function to use to compute each ARMA\\(_1\\)\n    stack;\n    - `dropout_rate`: dropout rate for skip connection;\n    - `activation`: activation function to use;\n    - `use_bias`: bool, add a bias vector to the output;\n    - `kernel_initializer`: initializer for the weights;\n    - `bias_initializer`: initializer for the bias vector;\n    - `kernel_regularizer`: regularization applied to the weights;\n    - `bias_regularizer`: regularization applied to the bias vector;\n    - `activity_regularizer`: regularization applied to the output;\n    - `kernel_constraint`: constraint applied to the weights;\n    - `bias_constraint`: constraint applied to the bias vector.\n    """"""\n\n    def __init__(self,\n                 channels,\n                 order=1,\n                 iterations=1,\n                 share_weights=False,\n                 gcn_activation=\'relu\',\n                 dropout_rate=0.0,\n                 activation=None,\n                 use_bias=True,\n                 kernel_initializer=\'glorot_uniform\',\n                 bias_initializer=\'zeros\',\n                 kernel_regularizer=None,\n                 bias_regularizer=None,\n                 activity_regularizer=None,\n                 kernel_constraint=None,\n                 bias_constraint=None,\n                 **kwargs):\n        super().__init__(channels,\n                         activation=activation,\n                         use_bias=use_bias,\n                         kernel_initializer=kernel_initializer,\n                         bias_initializer=bias_initializer,\n                         kernel_regularizer=kernel_regularizer,\n                         bias_regularizer=bias_regularizer,\n                         activity_regularizer=activity_regularizer,\n                         kernel_constraint=kernel_constraint,\n                         bias_constraint=bias_constraint,\n                         **kwargs)\n        self.iterations = iterations\n        self.order = order\n        self.share_weights = share_weights\n        self.gcn_activation = activations.get(gcn_activation)\n        self.dropout_rate = dropout_rate\n\n    def build(self, input_shape):\n        assert len(input_shape) >= 2\n        F = input_shape[0][-1]\n\n        # Create weights for parallel stacks\n        # self.kernels[k][i] refers to the k-th stack, i-th iteration\n        self.kernels = []\n        for k in range(self.order):\n            kernel_stack = []\n            current_shape = F\n            for i in range(self.iterations):\n                kernel_stack.append(\n                    self.create_weights(current_shape, F, self.channels,\n                                        \'ARMA_GCS_{}{}\'.format(k, i))\n                )\n                current_shape = self.channels\n                if self.share_weights and i == 1:\n                    # No need to continue because all following weights will be shared\n                    break\n            self.kernels.append(kernel_stack)\n        self.built = True\n\n    def call(self, inputs):\n        features = inputs[0]\n        fltr = inputs[1]\n\n        # Convolution\n        output = []  # Stores the parallel filters\n        for k in range(self.order):\n            output_k = features\n            for i in range(self.iterations):\n                output_k = self.gcs([output_k, features, fltr], k, i)\n            output.append(output_k)\n\n        # Average stacks\n        output = K.stack(output, axis=-1)\n        output = K.mean(output, axis=-1)\n        output = self.activation(output)\n\n        return output\n\n    def create_weights(self, input_dim, input_dim_skip, channels, name):\n        """"""\n        Creates a set of weights for a GCN with skip connections.\n        :param input_dim: dimension of the input space\n        :param input_dim_skip: dimension of the input space for the skip connection\n        :param channels: dimension of the output space\n        :param name: name of the layer\n        :return:\n            - kernel_1, from input space of the layer to output space\n            - kernel_2, from input space of the skip connection to output space\n            - bias, bias vector on the output space if use_bias=True, None otherwise.\n        """"""\n        kernel_1 = self.add_weight(shape=(input_dim, channels),\n                                   name=name + \'_kernel_1\',\n                                   initializer=self.kernel_initializer,\n                                   regularizer=self.kernel_regularizer,\n                                   constraint=self.kernel_constraint)\n        kernel_2 = self.add_weight(shape=(input_dim_skip, channels),\n                                   name=name + \'_kernel_2\',\n                                   initializer=self.kernel_initializer,\n                                   regularizer=self.kernel_regularizer,\n                                   constraint=self.kernel_constraint)\n        if self.use_bias:\n            bias = self.add_weight(shape=(channels,),\n                                   name=name + \'_bias\',\n                                   initializer=self.bias_initializer,\n                                   regularizer=self.bias_regularizer,\n                                   constraint=self.bias_constraint)\n        else:\n            bias = None\n        return kernel_1, kernel_2, bias\n\n    def gcs(self, inputs, stack, iteration):\n        """"""\n        Creates a graph convolutional layer with a skip connection.\n        :param inputs: list of input Tensors, namely\n            - input node features\n            - input node features for the skip connection\n            - normalized adjacency matrix;\n        :param stack: int, current stack (used to retrieve kernels);\n        :param iteration: int, current iteration (used to retrieve kernels);\n        :return: output node features.\n        """"""\n        X = inputs[0]\n        X_skip = inputs[1]\n        fltr = inputs[2]\n\n        if self.share_weights and iteration >= 1:\n            iter = 1\n        else:\n            iter = iteration\n        kernel_1, kernel_2, bias = self.kernels[stack][iter]\n\n        # Convolution\n        output = K.dot(X, kernel_1)\n        output = ops.filter_dot(fltr, output)\n\n        # Skip connection\n        skip = K.dot(X_skip, kernel_2)\n        skip = Dropout(self.dropout_rate)(skip)\n        output += skip\n\n        if self.use_bias:\n            output = K.bias_add(output, bias)\n        output = self.gcn_activation(output)\n        return output\n\n    def get_config(self):\n        config = {\n            \'iterations\': self.iterations,\n            \'order\': self.order,\n            \'share_weights\': self.share_weights,\n            \'gcn_activation\': activations.serialize(self.gcn_activation),\n            \'dropout_rate\': self.dropout_rate,\n        }\n        base_config = super().get_config()\n        return dict(list(base_config.items()) + list(config.items()))\n\n    @staticmethod\n    def preprocess(A):\n        fltr = normalized_laplacian(A, symmetric=True)\n        fltr = rescale_laplacian(fltr, lmax=2)\n        return fltr\n'"
spektral/layers/convolutional/cheb_conv.py,0,"b'from tensorflow.keras import backend as K\n\nfrom spektral.layers import ops\nfrom spektral.layers.convolutional.graph_conv import GraphConv\nfrom spektral.utils import normalized_laplacian, rescale_laplacian\n\n\nclass ChebConv(GraphConv):\n    r""""""\n    A Chebyshev convolutional layer as presented by\n    [Defferrard et al. (2016)](https://arxiv.org/abs/1606.09375).\n\n    **Mode**: single, disjoint, mixed, batch.\n\n    This layer computes:\n    $$\n        \\Z = \\sum \\limits_{k=0}^{K - 1} \\T^{(k)} \\W^{(k)}  + \\b^{(k)},\n    $$\n    where \\( \\T^{(0)}, ..., \\T^{(K - 1)} \\) are Chebyshev polynomials of \\(\\tilde \\L\\)\n    defined as\n    $$\n        \\T^{(0)} = \\X \\\\\n        \\T^{(1)} = \\tilde \\L \\X \\\\\n        \\T^{(k \\ge 2)} = 2 \\cdot \\tilde \\L \\T^{(k - 1)} - \\T^{(k - 2)},\n    $$\n    where\n    $$\n        \\tilde \\L =  \\frac{2}{\\lambda_{max}} \\cdot (\\I - \\D^{-1/2} \\A \\D^{-1/2}) - \\I\n    $$\n    is the normalized Laplacian with a rescaled spectrum.\n\n    **Input**\n\n    - Node features of shape `([batch], N, F)`;\n    - A list of K Chebyshev polynomials of shape\n    `[([batch], N, N), ..., ([batch], N, N)]`; can be computed with\n    `spektral.utils.convolution.chebyshev_filter`.\n\n    **Output**\n\n    - Node features with the same shape of the input, but with the last\n    dimension changed to `channels`.\n\n    **Arguments**\n\n    - `channels`: number of output channels;\n    - `K`: order of the Chebyshev polynomials;\n    - `activation`: activation function to use;\n    - `use_bias`: bool, add a bias vector to the output;\n    - `kernel_initializer`: initializer for the weights;\n    - `bias_initializer`: initializer for the bias vector;\n    - `kernel_regularizer`: regularization applied to the weights;\n    - `bias_regularizer`: regularization applied to the bias vector;\n    - `activity_regularizer`: regularization applied to the output;\n    - `kernel_constraint`: constraint applied to the weights;\n    - `bias_constraint`: constraint applied to the bias vector.\n\n    """"""\n\n    def __init__(self,\n                 channels,\n                 K=1,\n                 activation=None,\n                 use_bias=True,\n                 kernel_initializer=\'glorot_uniform\',\n                 bias_initializer=\'zeros\',\n                 kernel_regularizer=None,\n                 bias_regularizer=None,\n                 activity_regularizer=None,\n                 kernel_constraint=None,\n                 bias_constraint=None,\n                 **kwargs):\n        super().__init__(channels,\n                         activation=activation,\n                         use_bias=use_bias,\n                         kernel_initializer=kernel_initializer,\n                         bias_initializer=bias_initializer,\n                         kernel_regularizer=kernel_regularizer,\n                         bias_regularizer=bias_regularizer,\n                         activity_regularizer=activity_regularizer,\n                         kernel_constraint=kernel_constraint,\n                         bias_constraint=bias_constraint,\n                         **kwargs)\n        self.K = K\n\n    def build(self, input_shape):\n        assert len(input_shape) >= 2\n        input_dim = input_shape[0][-1]\n        self.kernel = self.add_weight(shape=(self.K, input_dim, self.channels),\n                                      initializer=self.kernel_initializer,\n                                      name=\'kernel\',\n                                      regularizer=self.kernel_regularizer,\n                                      constraint=self.kernel_constraint)\n        if self.use_bias:\n            self.bias = self.add_weight(shape=(self.channels,),\n                                        initializer=self.bias_initializer,\n                                        name=\'bias\',\n                                        regularizer=self.bias_regularizer,\n                                        constraint=self.bias_constraint)\n        else:\n            self.bias = None\n        self.built = True\n\n    def call(self, inputs):\n        features = inputs[0]\n        laplacian = inputs[1]\n\n        # Convolution\n        T_0 = features\n        output = ops.dot(T_0, self.kernel[0])\n\n        if self.K > 1:\n            T_1 = ops.filter_dot(laplacian, features)\n            output += ops.dot(T_1, self.kernel[1])\n\n        for k in range(2, self.K):\n            T_2 = 2 * ops.filter_dot(laplacian, T_1) - T_0\n            output += ops.dot(T_2, self.kernel[k])\n            T_0, T_1 = T_1, T_2\n\n        if self.use_bias:\n            output = K.bias_add(output, self.bias)\n        if self.activation is not None:\n            output = self.activation(output)\n        return output\n\n    def get_config(self):\n        config = {\n            \'K\': self.K\n        }\n        base_config = super(ChebConv, self).get_config()\n        return dict(list(base_config.items()) + list(config.items()))\n\n    @staticmethod\n    def preprocess(A):\n        L = normalized_laplacian(A)\n        L = rescale_laplacian(L)\n        return L\n'"
spektral/layers/convolutional/crystal_conv.py,0,"b'from tensorflow.keras import backend as K\nfrom tensorflow.keras.layers import Dense\n\nfrom spektral.layers.convolutional.message_passing import MessagePassing\n\n\nclass CrystalConv(MessagePassing):\n    r""""""\n    A Crystal Graph Convolutional layer as presented by\n    [Xie & Grossman (2018)](https://journals.aps.org/prl/abstract/10.1103/PhysRevLett.120.145301).\n\n    **Mode**: single, disjoint.\n\n    **This layer expects a sparse adjacency matrix.**\n\n    This layer computes for each node \\(i\\):\n    $$\n        \\H_i = \\X_i +\n               \\sum\\limits_{j \\in \\mathcal{N}(i)}\n                    \\sigma \\left( \\z_{ij} \\W^{(f)} + \\b^{(f)} \\right)\n                    \\odot\n                    \\g \\left( \\z_{ij} \\W^{(s)} + \\b^{(s)} \\right)\n    $$\n    where \\(\\z_{ij} = \\X_i \\| \\X_j \\| \\E_{ij} \\), \\(\\sigma\\) is a sigmoid\n    activation, and \\(g\\) is the activation function (defined by the `activation`\n    argument).\n\n    **Input**\n\n    - Node features of shape `(N, F)`;\n    - Binary adjacency matrix of shape `(N, N)`.\n    - Edge features of shape `(num_edges, S)`.\n\n    **Output**\n\n    - Node features with the same shape of the input, but the last dimension\n    changed to `channels`.\n\n    **Arguments**\n\n    - `channels`: integer, number of output channels;\n    - `activation`: activation function to use;\n    - `use_bias`: bool, add a bias vector to the output;\n    - `kernel_initializer`: initializer for the weights;\n    - `bias_initializer`: initializer for the bias vector;\n    - `kernel_regularizer`: regularization applied to the weights;\n    - `bias_regularizer`: regularization applied to the bias vector;\n    - `activity_regularizer`: regularization applied to the output;\n    - `kernel_constraint`: constraint applied to the weights;\n    - `bias_constraint`: constraint applied to the bias vector.\n    """"""\n\n    def __init__(self,\n                 channels,\n                 activation=None,\n                 use_bias=True,\n                 kernel_initializer=\'glorot_uniform\',\n                 bias_initializer=\'zeros\',\n                 kernel_regularizer=None,\n                 bias_regularizer=None,\n                 activity_regularizer=None,\n                 kernel_constraint=None,\n                 bias_constraint=None,\n                 **kwargs):\n        super().__init__(aggregate=\'sum\',\n                         activation=activation,\n                         use_bias=use_bias,\n                         kernel_initializer=kernel_initializer,\n                         bias_initializer=bias_initializer,\n                         kernel_regularizer=kernel_regularizer,\n                         bias_regularizer=bias_regularizer,\n                         activity_regularizer=activity_regularizer,\n                         kernel_constraint=kernel_constraint,\n                         bias_constraint=bias_constraint,\n                         **kwargs)\n        self.channels = self.output_dim = channels\n\n    def build(self, input_shape):\n        assert len(input_shape) >= 2\n        layer_kwargs = dict(\n            kernel_initializer=self.kernel_initializer,\n            bias_initializer=self.bias_initializer,\n            kernel_regularizer=self.kernel_regularizer,\n            bias_regularizer=self.bias_regularizer,\n            kernel_constraint=self.kernel_constraint,\n            bias_constraint=self.bias_constraint\n        )\n        self.dense_f = Dense(self.channels, activation=\'sigmoid\', **layer_kwargs)\n        self.dense_s = Dense(self.channels, activation=self.activation, **layer_kwargs)\n\n        self.built = True\n\n    def message(self, X, E=None):\n        X_i = self.get_i(X)\n        X_j = self.get_j(X)\n        Z = K.concatenate((X_i, X_j, E), axis=-1)\n        output = self.dense_s(Z) * self.dense_f(Z)\n\n        return output\n\n    def update(self, embeddings, X=None):\n        return X + embeddings\n\n    def get_config(self):\n        config = {\n            \'channels\': self.channels\n        }\n        base_config = super().get_config()\n        base_config.pop(\'aggregate\')  # Remove it because it\'s defined by constructor\n        return {**base_config, **config}\n'"
spektral/layers/convolutional/diffusion_conv.py,6,"b'import tensorflow as tf\nimport tensorflow.keras.layers as layers\nfrom spektral.layers.convolutional.graph_conv import GraphConv\n\n\nclass DiffuseFeatures(layers.Layer):\n    r""""""Utility layer calculating a single channel of the\n    diffusional convolution.\n\n    Procedure is based on https://arxiv.org/abs/1707.01926\n\n    **Input**\n\n    - Node features of shape `([batch], N, F)`;\n    - Normalized adjacency or attention coef. matrix \\(\\hat \\A \\) of shape\n    `([batch], N, N)`; Use DiffusionConvolution.preprocess to normalize.\n\n    **Output**\n\n    - Node features with the same shape as the input, but with the last\n    dimension changed to \\(1\\).\n\n    **Arguments**\n\n    - `num_diffusion_steps`: How many diffusion steps to consider. \\(K\\) in paper.\n    - `kernel_initializer`: initializer for the weights;\n    - `kernel_regularizer`: regularization applied to the kernel vectors;\n    - `kernel_constraint`: constraint applied to the kernel vectors;\n    """"""\n\n    def __init__(\n        self,\n        num_diffusion_steps: int,\n        kernel_initializer,\n        kernel_regularizer,\n        kernel_constraint,\n        **kwargs\n    ):\n        super(DiffuseFeatures, self).__init__()\n\n        # number of diffusino steps (K in paper)\n        self.K = num_diffusion_steps\n\n        # get regularizer, initializer and constraint for kernel\n        self.kernel_initializer = kernel_initializer\n        self.kernel_regularizer = kernel_regularizer\n        self.kernel_constraint = kernel_constraint\n\n    def build(self, input_shape):\n\n        # Initializing the kernel vector (R^K)\n        # (theta in paper)\n        self.kernel = self.add_weight(\n            shape=(self.K,),\n            initializer=self.kernel_initializer,\n            regularizer=self.kernel_regularizer,\n            constraint=self.kernel_constraint,\n        )\n\n    def call(self, inputs):\n\n        # Get signal X and adjacency A\n        X, A = inputs\n\n        # Calculate diffusion matrix: sum kernel_k * Attention_t^k\n        # tf.polyval needs a list of tensors as the coeff. thus we\n        # unstack kernel\n        diffusion_matrix = tf.math.polyval(tf.unstack(self.kernel), A)\n\n        # Apply it to X to get a matrix C = [C_1, ..., C_F] (N x F)\n        # of diffused features\n        diffused_features = tf.matmul(diffusion_matrix, X)\n\n        # Now we add all diffused features (columns of the above matrix)\n        # and apply a non linearity to obtain H:,q (eq. 3 in paper)\n        H = tf.math.reduce_sum(diffused_features, axis=-1)\n\n        # H has shape ([batch], N) but as it is the sum of columns\n        # we reshape it to ([batch], N, 1)\n        return tf.expand_dims(H, -1)\n\n\nclass DiffusionConv(GraphConv):\n    r""""""Applies Graph Diffusion Convolution as descibed by\n    [Li et al. (2016)](https://arxiv.org/pdf/1707.01926.pdf)\n\n    **Mode**: single, disjoint, mixed, batch.\n\n    **This layer expects a dense adjacency matrix.**\n\n    Given a number of diffusion steps \\(K\\) and a row normalized adjacency matrix \\(\\hat \\A \\),\n    this layer calculates the q\'th channel as:\n    $$\n    \\mathbf{H}_{~:,~q} = \\sigma\\left(\n        \\sum_{f=1}^{F}\n            \\left(\n                \\sum_{k=0}^{K-1}\\theta_k {\\hat \\A}^k\n            \\right)\n        \\X_{~:,~f}\n    \\right)\n    $$\n\n    **Input**\n\n    - Node features of shape `([batch], N, F)`;\n    - Normalized adjacency or attention coef. matrix \\(\\hat \\A \\) of shape\n    `([batch], N, N)`; Use `DiffusionConvolution.preprocess` to normalize.\n\n    **Output**\n\n    - Node features with the same shape as the input, but with the last\n    dimension changed to `channels`.\n\n    **Arguments**\n\n    - `channels`: number of output channels;\n    - `num_diffusion_steps`: How many diffusion steps to consider. \\(K\\) in paper.\n    - `activation`: activation function \\(\\sigma\\); (\\(\\tanh\\) by default)\n    - `kernel_initializer`: initializer for the weights;\n    - `kernel_regularizer`: regularization applied to the weights;\n    - `kernel_constraint`: constraint applied to the weights;\n    """"""\n\n    def __init__(\n        self,\n        channels: int,\n        num_diffusion_steps: int = 6,\n        kernel_initializer=\'glorot_uniform\',\n        kernel_regularizer=None,\n        kernel_constraint=None,\n        activation=\'tanh\',\n        ** kwargs\n    ):\n        super().__init__(channels,\n                         activation=activation,\n                         kernel_initializer=kernel_initializer,\n                         kernel_regularizer=kernel_regularizer,\n                         kernel_constraint=kernel_constraint,\n                         **kwargs)\n\n        # number of features to generate (Q in paper)\n        assert channels > 0\n        self.Q = channels\n\n        # number of diffusion steps for each output feature\n        self.K = num_diffusion_steps + 1\n\n    def build(self, input_shape):\n\n        # We expect to receive (X, A)\n        # A - Adjacency ([batch], N, N)\n        # X - graph signal ([batch], N, F)\n        X_shape, A_shape = input_shape\n\n        # initialise Q diffusion convolution filters\n        self.filters = []\n\n        for _ in range(self.Q):\n            layer = DiffuseFeatures(\n                num_diffusion_steps=self.K,\n                kernel_initializer=self.kernel_initializer,\n                kernel_regularizer=self.kernel_regularizer,\n                kernel_constraint=self.kernel_constraint,\n            )\n            self.filters.append(layer)\n\n    def apply_filters(self, X, A):\n        """"""Applies diffusion convolution self.Q times to get a\n        ([batch], N, Q) diffused graph signal\n\n        """"""\n\n        # This will be a list of Q diffused features.\n        # Each diffused feature is a (batch, N, 1) tensor.\n        # Later we will concat all the features to get one\n        # (batch, N, Q) diffused graph signal\n        diffused_features = []\n\n        # Iterating over all Q diffusion filters\n        for diffusion in self.filters:\n            diffused_feature = diffusion((X, A))\n            diffused_features.append(diffused_feature)\n\n        # Concat them into ([batch], N, Q) diffused graph signal\n        H = tf.concat(diffused_features, -1)\n\n        return H\n\n    def call(self, inputs):\n\n        # Get graph signal X and adjacency tensor A\n        X, A = inputs\n\n        # \'single\', \'batch\' and \'mixed\' mode are supported by\n        # default, since we access the dimensions from the end\n        # and everything else is broadcasted accordingly\n        # if its missing.\n\n        H = self.apply_filters(X, A)\n        H = self.activation(H)\n\n        return H\n'"
spektral/layers/convolutional/ecc_conv.py,4,"b'import tensorflow as tf\nfrom tensorflow.keras import backend as K\nfrom tensorflow.keras.layers import Dense\n\nfrom spektral.layers import ops\nfrom spektral.layers.ops import modes\nfrom spektral.layers.convolutional.graph_conv import GraphConv\n\n\nclass EdgeConditionedConv(GraphConv):\n    r""""""\n    An edge-conditioned convolutional layer (ECC) as presented by\n    [Simonovsky & Komodakis (2017)](https://arxiv.org/abs/1704.02901).\n\n    **Mode**: single, disjoint, disjoint, batch.\n\n    **Notes**:\n        - This layer expects dense inputs and self-loops when working in batch mode.\n        - In single mode, if the adjacency matrix is dense it will be converted\n        to a SparseTensor automatically (which is an expensive operation).\n\n    For each node \\( i \\), this layer computes:\n    $$\n        \\Z_i =  \\frac{1}{\\mathcal{N}(i)} \\sum\\limits_{j \\in \\mathcal{N}(i)} \\textrm{MLP}(\\E_{ji}) \\X_{j} + \\b\n    $$\n    where \\(\\textrm{MLP}\\) is a multi-layer perceptron that outputs the\n    convolutional kernel \\(\\W\\) as a function of edge attributes.\n\n    **Input**\n\n    - Node features of shape `([batch], N, F)`;\n    - Binary adjacency matrices with self-loops, of shape `([batch], N, N)`;\n    - Edge features. In single mode, shape `(num_edges, S)`; in batch mode, shape\n    `(batch, N, N, S)`.\n\n    **Output**\n\n    - node features with the same shape of the input, but the last dimension\n    changed to `channels`.\n\n    **Arguments**\n\n    - `channels`: integer, number of output channels;\n    - `kernel_network`: a list of integers representing the hidden neurons of\n    the kernel-generating network;\n    - `activation`: activation function to use;\n    - `use_bias`: bool, add a bias vector to the output;\n    - `kernel_initializer`: initializer for the weights;\n    - `bias_initializer`: initializer for the bias vector;\n    - `kernel_regularizer`: regularization applied to the weights;\n    - `bias_regularizer`: regularization applied to the bias vector;\n    - `activity_regularizer`: regularization applied to the output;\n    - `kernel_constraint`: constraint applied to the weights;\n    - `bias_constraint`: constraint applied to the bias vector.\n\n    """"""\n\n    def __init__(self,\n                 channels,\n                 kernel_network=None,\n                 root=True,\n                 activation=None,\n                 use_bias=True,\n                 kernel_initializer=\'glorot_uniform\',\n                 bias_initializer=\'zeros\',\n                 kernel_regularizer=None,\n                 bias_regularizer=None,\n                 activity_regularizer=None,\n                 kernel_constraint=None,\n                 bias_constraint=None,\n                 **kwargs):\n        super().__init__(channels,\n                         activation=activation,\n                         use_bias=use_bias,\n                         kernel_initializer=kernel_initializer,\n                         bias_initializer=bias_initializer,\n                         kernel_regularizer=kernel_regularizer,\n                         bias_regularizer=bias_regularizer,\n                         activity_regularizer=activity_regularizer,\n                         kernel_constraint=kernel_constraint,\n                         bias_constraint=bias_constraint,\n                         **kwargs)\n        self.kernel_network = kernel_network\n        self.root = root\n\n    def build(self, input_shape):\n        F = input_shape[0][-1]\n        F_ = self.channels\n        self.kernel_network_layers = []\n        if self.kernel_network is not None:\n            for i, l in enumerate(self.kernel_network):\n                self.kernel_network_layers.append(\n                    Dense(l,\n                          name=\'FGN_{}\'.format(i),\n                          activation=\'relu\',\n                          use_bias=self.use_bias,\n                          kernel_initializer=self.kernel_initializer,\n                          bias_initializer=self.bias_initializer,\n                          kernel_regularizer=self.kernel_regularizer,\n                          bias_regularizer=self.bias_regularizer,\n                          kernel_constraint=self.kernel_constraint,\n                          bias_constraint=self.bias_constraint)\n                )\n        self.kernel_network_layers.append(Dense(F_ * F, name=\'FGN_out\'))\n        if self.root:\n            self.root_kernel = self.add_weight(name=\'root_kernel\',\n                                              shape=(F, F_),\n                                              initializer=self.kernel_initializer,\n                                              regularizer=self.kernel_regularizer,\n                                              constraint=self.kernel_constraint)\n        else:\n            self.root_kernel = None\n        if self.use_bias:\n            self.bias = self.add_weight(name=\'bias\',\n                                        shape=(self.channels,),\n                                        initializer=self.bias_initializer,\n                                        regularizer=self.bias_regularizer,\n                                        constraint=self.bias_constraint)\n        else:\n            self.bias = None\n        self.built = True\n\n    def call(self, inputs):\n        X = inputs[0]  # (batch_size, N, F)\n        A = inputs[1]  # (batch_size, N, N)\n        E = inputs[2]  # (n_edges, S) or (batch_size, N, N, S)\n\n        mode = ops.autodetect_mode(A, X)\n        if mode == modes.SINGLE:\n            return self._call_single(inputs)\n\n        # Parameters\n        N = K.shape(X)[-2]\n        F = K.int_shape(X)[-1]\n        F_ = self.channels\n\n        # Normalize adjacency matrix\n        A = ops.normalize_A(A)\n\n        # Filter network\n        kernel_network = E\n        for l in self.kernel_network_layers:\n            kernel_network = l(kernel_network)\n\n        # Convolution\n        target_shape = (-1, N, N, F_, F) if mode == modes.BATCH else (N, N, F_, F)\n        kernel = K.reshape(kernel_network, target_shape)\n        output = kernel * A[..., None, None]\n        output = tf.einsum(\'abicf,aif->abc\', output, X)\n\n        if self.use_bias:\n            output = K.bias_add(output, self.bias)\n        if self.activation is not None:\n            output = self.activation(output)\n\n        return output\n\n    def _call_single(self, inputs):\n        X = inputs[0]  # (N, F)\n        A = inputs[1]  # (N, N)\n        E = inputs[2]  # (n_edges, S)\n        assert K.ndim(E) == 2, \'In single mode, E must have shape (n_edges, S).\'\n\n        # Enforce sparse representation\n        if not K.is_sparse(A):\n            A = ops.dense_to_sparse(A)\n\n        # Parameters\n        N = tf.shape(X)[-2]\n        F = K.int_shape(X)[-1]\n        F_ = self.channels\n\n        # Filter network\n        kernel_network = E\n        for l in self.kernel_network_layers:\n            kernel_network = l(kernel_network)  # (n_edges, F * F_)\n        target_shape = (-1, F, F_)\n        kernel = tf.reshape(kernel_network, target_shape)\n\n        # Propagation\n        targets = A.indices[:, -2]\n        sources = A.indices[:, -1]\n        messages = tf.gather(X, sources)\n        messages = ops.dot(messages[:, None, :], kernel)[:, 0, :]\n        aggregated = ops.scatter_sum(messages, targets, N)\n\n        # Update\n        output = aggregated\n        if self.root_kernel is not None:\n            output += ops.dot(X, self.root_kernel)\n        if self.use_bias:\n            output = K.bias_add(output, self.bias)\n        if self.activation is not None:\n            output = self.activation(output)\n\n        return output\n\n    def get_config(self):\n        config = {\n            \'kernel_network\': self.kernel_network,\n        }\n        base_config = super().get_config()\n        return dict(list(base_config.items()) + list(config.items()))\n\n    @staticmethod\n    def preprocess(A):\n        return A\n'"
spektral/layers/convolutional/edge_conv.py,0,"b'from tensorflow.keras import activations, backend as K\nfrom tensorflow.keras.layers import Dense\nfrom tensorflow.keras.models import Sequential\n\nfrom spektral.layers.convolutional.message_passing import MessagePassing\n\n\nclass EdgeConv(MessagePassing):\n    r""""""\n    An Edge Convolutional layer as presented by\n    [Wang et al. (2018)](https://arxiv.org/abs/1801.07829).\n\n    **Mode**: single, disjoint.\n\n    **This layer expects a sparse adjacency matrix.**\n\n    This layer computes for each node \\(i\\):\n    $$\n        \\Z_i = \\sum\\limits_{j \\in \\mathcal{N}(i)} \\textrm{MLP}\\big( \\X_i \\| \\X_j - \\X_i \\big)\n    $$\n    where \\(\\textrm{MLP}\\) is a multi-layer perceptron.\n\n    **Input**\n\n    - Node features of shape `(N, F)`;\n    - Binary adjacency matrix of shape `(N, N)`.\n\n    **Output**\n\n    - Node features with the same shape of the input, but the last dimension\n    changed to `channels`.\n\n    **Arguments**\n\n    - `channels`: integer, number of output channels;\n    - `mlp_hidden`: list of integers, number of hidden units for each hidden\n    layer in the MLP (if None, the MLP has only the output layer);\n    - `mlp_activation`: activation for the MLP layers;\n    - `activation`: activation function to use;\n    - `use_bias`: bool, add a bias vector to the output;\n    - `kernel_initializer`: initializer for the weights;\n    - `bias_initializer`: initializer for the bias vector;\n    - `kernel_regularizer`: regularization applied to the weights;\n    - `bias_regularizer`: regularization applied to the bias vector;\n    - `activity_regularizer`: regularization applied to the output;\n    - `kernel_constraint`: constraint applied to the weights;\n    - `bias_constraint`: constraint applied to the bias vector.\n    """"""\n\n    def __init__(self,\n                 channels,\n                 mlp_hidden=None,\n                 mlp_activation=\'relu\',\n                 activation=None,\n                 use_bias=True,\n                 kernel_initializer=\'glorot_uniform\',\n                 bias_initializer=\'zeros\',\n                 kernel_regularizer=None,\n                 bias_regularizer=None,\n                 activity_regularizer=None,\n                 kernel_constraint=None,\n                 bias_constraint=None,\n                 **kwargs):\n        super().__init__(aggregate=\'sum\',\n                         activation=activation,\n                         use_bias=use_bias,\n                         kernel_initializer=kernel_initializer,\n                         bias_initializer=bias_initializer,\n                         kernel_regularizer=kernel_regularizer,\n                         bias_regularizer=bias_regularizer,\n                         activity_regularizer=activity_regularizer,\n                         kernel_constraint=kernel_constraint,\n                         bias_constraint=bias_constraint,\n                         **kwargs)\n        self.channels = self.output_dim = channels\n        self.mlp_hidden = mlp_hidden if mlp_hidden else []\n        self.mlp_activation = activations.get(mlp_activation)\n\n    def build(self, input_shape):\n        assert len(input_shape) >= 2\n        layer_kwargs = dict(\n            kernel_initializer=self.kernel_initializer,\n            bias_initializer=self.bias_initializer,\n            kernel_regularizer=self.kernel_regularizer,\n            bias_regularizer=self.bias_regularizer,\n            kernel_constraint=self.kernel_constraint,\n            bias_constraint=self.bias_constraint\n        )\n\n        self.mlp = Sequential([\n            Dense(channels, self.mlp_activation, **layer_kwargs)\n            for channels in self.mlp_hidden\n        ] + [Dense(self.channels, self.activation, use_bias=self.use_bias, **layer_kwargs)])\n\n        self.built = True\n\n    def message(self, X, **kwargs):\n        X_i = self.get_i(X)\n        X_j = self.get_j(X)\n        return self.mlp(K.concatenate((X_i, X_j - X_i)))\n\n    def get_config(self):\n        config = {\n            \'channels\': self.channels,\n            \'mlp_hidden\': self.mlp_hidden,\n            \'mlp_activation\': self.mlp_activation\n        }\n        base_config = super().get_config()\n        base_config.pop(\'aggregate\')  # Remove it because it\'s defined by constructor\n        return {**base_config, **config}\n'"
spektral/layers/convolutional/gated_graph_conv.py,2,"b'import tensorflow as tf\nfrom tensorflow.keras import backend as K\nfrom tensorflow.keras.layers import GRUCell\n\nfrom spektral.layers.convolutional.message_passing import MessagePassing\n\n\nclass GatedGraphConv(MessagePassing):\n    r""""""\n    A gated graph convolutional layer as presented by\n    [Li et al. (2018)](https://arxiv.org/abs/1511.05493).\n\n    **Mode**: single, disjoint.\n\n    **This layer expects a sparse adjacency matrix.**\n\n    This layer repeatedly applies a GRU cell \\(L\\) times to the node attributes\n    $$\n    \\begin{align}\n        & \\h^{(0)}_i = \\X_i \\| \\mathbf{0} \\\\\n        & \\m^{(l)}_i = \\sum\\limits_{j \\in \\mathcal{N}(i)} \\h^{(l - 1)}_j \\W \\\\\n        & \\h^{(l)}_i = \\textrm{GRU} \\left(\\m^{(l)}_i, \\h^{(l - 1)}_i \\right) \\\\\n        & \\Z_i = h^{(L)}_i\n    \\end{align}\n    $$\n    where \\(\\textrm{GRU}\\) is the GRU cell.\n\n    **Input**\n\n    - Node features of shape `(N, F)`; note that `F` must be smaller or equal\n    than `channels`.\n    - Binary adjacency matrix of shape `(N, N)`.\n\n    **Output**\n\n    - Node features with the same shape of the input, but the last dimension\n    changed to `channels`.\n\n    **Arguments**\n\n    - `channels`: integer, number of output channels;\n    - `n_layers`: integer, number of iterations with the GRU cell;\n    - `activation`: activation function to use;\n    - `use_bias`: bool, add a bias vector to the output;\n    - `kernel_initializer`: initializer for the weights;\n    - `bias_initializer`: initializer for the bias vector;\n    - `kernel_regularizer`: regularization applied to the weights;\n    - `bias_regularizer`: regularization applied to the bias vector;\n    - `activity_regularizer`: regularization applied to the output;\n    - `kernel_constraint`: constraint applied to the weights;\n    - `bias_constraint`: constraint applied to the bias vector.\n    """"""\n\n    def __init__(self,\n                 channels,\n                 n_layers,\n                 activation=None,\n                 use_bias=True,\n                 kernel_initializer=\'glorot_uniform\',\n                 bias_initializer=\'zeros\',\n                 kernel_regularizer=None,\n                 bias_regularizer=None,\n                 activity_regularizer=None,\n                 kernel_constraint=None,\n                 bias_constraint=None,\n                 **kwargs):\n        super().__init__(activation=activation,\n                         use_bias=use_bias,\n                         kernel_initializer=kernel_initializer,\n                         bias_initializer=bias_initializer,\n                         kernel_regularizer=kernel_regularizer,\n                         bias_regularizer=bias_regularizer,\n                         activity_regularizer=activity_regularizer,\n                         kernel_constraint=kernel_constraint,\n                         bias_constraint=bias_constraint,\n                         **kwargs)\n        self.channels = self.output_dim = channels\n        self.n_layers = n_layers\n\n    def build(self, input_shape):\n        assert len(input_shape) >= 2\n        F = input_shape[0][1]\n        if F > self.channels:\n            raise ValueError(\'channels ({}) must be greater than the number of \'\n                             \'input features ({}).\'.format(self.channels, F))\n\n        self.kernel = self.add_weight(name=\'kernel\',\n                                      shape=(self.n_layers, self.channels, self.channels),\n                                      initializer=self.kernel_initializer,\n                                      regularizer=self.kernel_regularizer,\n                                      constraint=self.kernel_constraint)\n        self.rnn = GRUCell(self.channels,\n                           kernel_initializer=self.kernel_initializer,\n                           bias_initializer=self.bias_initializer,\n                           kernel_regularizer=self.kernel_regularizer,\n                           bias_regularizer=self.bias_regularizer,\n                           activity_regularizer=self.activity_regularizer,\n                           kernel_constraint=self.kernel_constraint,\n                           bias_constraint=self.bias_constraint,\n                           use_bias=self.use_bias)\n        self.built = True\n\n    def call(self, inputs):\n        X, A, E = self.get_inputs(inputs)\n        F = K.int_shape(X)[-1]\n\n        to_pad = self.channels - F\n        output = tf.pad(X, [[0, 0], [0, to_pad]])\n        for i in range(self.n_layers):\n            m = tf.matmul(output, self.kernel[i])\n            m = self.propagate(m, A)\n            output = self.rnn(m, [output])[0]\n\n        output = self.activation(output)\n        return output\n\n    def get_config(self):\n        config = {\n            \'channels\': self.channels,\n            \'n_layers\': self.n_layers,\n        }\n        base_config = super().get_config()\n        return {**base_config, **config}\n'"
spektral/layers/convolutional/gin_conv.py,0,"b'from tensorflow.keras import activations, backend as K\nfrom tensorflow.keras.layers import Dense\nfrom tensorflow.keras.models import Sequential\n\nfrom spektral.layers.convolutional.message_passing import MessagePassing\n\n\nclass GINConv(MessagePassing):\n    r""""""\n    A Graph Isomorphism Network (GIN) as presented by\n    [Xu et al. (2018)](https://arxiv.org/abs/1810.00826).\n\n    **Mode**: single, disjoint.\n\n    **This layer expects a sparse adjacency matrix.**\n\n    This layer computes for each node \\(i\\):\n    $$\n        \\Z_i = \\textrm{MLP}\\big( (1 + \\epsilon) \\cdot \\X_i + \\sum\\limits_{j \\in \\mathcal{N}(i)} \\X_j \\big)\n    $$\n    where \\(\\textrm{MLP}\\) is a multi-layer perceptron.\n\n    **Input**\n\n    - Node features of shape `(N, F)`;\n    - Binary adjacency matrix of shape `(N, N)`.\n\n    **Output**\n\n    - Node features with the same shape of the input, but the last dimension\n    changed to `channels`.\n\n    **Arguments**\n\n    - `channels`: integer, number of output channels;\n    - `epsilon`: unnamed parameter, see\n    [Xu et al. (2018)](https://arxiv.org/abs/1810.00826), and the equation above.\n    By setting `epsilon=None`, the parameter will be learned (default behaviour).\n    If given as a value, the parameter will stay fixed.\n    - `mlp_hidden`: list of integers, number of hidden units for each hidden\n    layer in the MLP (if None, the MLP has only the output layer);\n    - `mlp_activation`: activation for the MLP layers;\n    - `activation`: activation function to use;\n    - `use_bias`: bool, add a bias vector to the output;\n    - `kernel_initializer`: initializer for the weights;\n    - `bias_initializer`: initializer for the bias vector;\n    - `kernel_regularizer`: regularization applied to the weights;\n    - `bias_regularizer`: regularization applied to the bias vector;\n    - `activity_regularizer`: regularization applied to the output;\n    - `kernel_constraint`: constraint applied to the weights;\n    - `bias_constraint`: constraint applied to the bias vector.\n    """"""\n\n    def __init__(self,\n                 channels,\n                 epsilon=None,\n                 mlp_hidden=None,\n                 mlp_activation=\'relu\',\n                 activation=None,\n                 use_bias=True,\n                 kernel_initializer=\'glorot_uniform\',\n                 bias_initializer=\'zeros\',\n                 kernel_regularizer=None,\n                 bias_regularizer=None,\n                 activity_regularizer=None,\n                 kernel_constraint=None,\n                 bias_constraint=None,\n                 **kwargs):\n        super().__init__(aggregate=\'sum\',\n                         activation=activation,\n                         use_bias=use_bias,\n                         kernel_initializer=kernel_initializer,\n                         bias_initializer=bias_initializer,\n                         kernel_regularizer=kernel_regularizer,\n                         bias_regularizer=bias_regularizer,\n                         activity_regularizer=activity_regularizer,\n                         kernel_constraint=kernel_constraint,\n                         bias_constraint=bias_constraint,\n                         **kwargs)\n        self.channels = self.output_dim = channels\n        self.epsilon = epsilon\n        self.mlp_hidden = mlp_hidden if mlp_hidden else []\n        self.mlp_activation = activations.get(mlp_activation)\n\n    def build(self, input_shape):\n        assert len(input_shape) >= 2\n        layer_kwargs = dict(\n            kernel_initializer=self.kernel_initializer,\n            bias_initializer=self.bias_initializer,\n            kernel_regularizer=self.kernel_regularizer,\n            bias_regularizer=self.bias_regularizer,\n            kernel_constraint=self.kernel_constraint,\n            bias_constraint=self.bias_constraint\n        )\n\n        self.mlp = Sequential([\n            Dense(channels, self.mlp_activation, **layer_kwargs)\n            for channels in self.mlp_hidden\n        ] + [Dense(self.channels, self.activation, use_bias=self.use_bias, **layer_kwargs)])\n\n        if self.epsilon is None:\n            self.eps = self.add_weight(shape=(1,),\n                                       initializer=\'zeros\',\n                                       name=\'eps\')\n        else:\n            # If epsilon is given, keep it constant\n            self.eps = K.constant(self.epsilon)\n\n        self.built = True\n\n    def call(self, inputs):\n        X, A, E = self.get_inputs(inputs)\n        output = self.mlp((1.0 + self.eps) * X + self.propagate(X, A, E))\n\n        return output\n\n    def get_config(self):\n        config = {\n            \'channels\': self.channels,\n            \'epsilon\': self.epsilon,\n            \'mlp_hidden\': self.mlp_hidden,\n            \'mlp_activation\': self.mlp_activation\n        }\n        base_config = super().get_config()\n        base_config.pop(\'aggregate\')  # Remove it because it\'s defined by constructor\n        return {**base_config, **config}\n'"
spektral/layers/convolutional/graph_attention.py,21,"b'import tensorflow as tf\nfrom tensorflow.keras import backend as K\nfrom tensorflow.keras import initializers, regularizers, constraints\nfrom tensorflow.keras.layers import Dropout\n\nfrom spektral.layers import ops\nfrom spektral.layers.convolutional.graph_conv import GraphConv\nfrom spektral.layers.ops import modes\n\n\nclass GraphAttention(GraphConv):\n    r""""""\n    A graph attention layer (GAT) as presented by\n    [Velickovic et al. (2017)](https://arxiv.org/abs/1710.10903).\n\n    **Mode**: single, disjoint, mixed, batch.\n\n    **This layer expects dense inputs when working in batch mode.**\n\n    This layer computes a convolution similar to `layers.GraphConv`, but\n    uses the attention mechanism to weight the adjacency matrix instead of\n    using the normalized Laplacian:\n    $$\n        \\Z = \\mathbf{\\alpha}\\X\\W + \\b\n    $$\n    where\n    $$\n        \\mathbf{\\alpha}_{ij} =\n            \\frac{\n                \\exp\\left(\n                    \\mathrm{LeakyReLU}\\left(\n                        \\a^{\\top} [(\\X\\W)_i \\, \\| \\, (\\X\\W)_j]\n                    \\right)\n                \\right)\n            }\n            {\\sum\\limits_{k \\in \\mathcal{N}(i) \\cup \\{ i \\}}\n                \\exp\\left(\n                    \\mathrm{LeakyReLU}\\left(\n                        \\a^{\\top} [(\\X\\W)_i \\, \\| \\, (\\X\\W)_k]\n                    \\right)\n                \\right)\n            }\n    $$\n    where \\(\\a \\in \\mathbb{R}^{2F\'}\\) is a trainable attention kernel.\n    Dropout is also applied to \\(\\alpha\\) before computing \\(\\Z\\).\n    Parallel attention heads are computed in parallel and their results are\n    aggregated by concatenation or average.\n\n    **Input**\n\n    - Node features of shape `([batch], N, F)`;\n    - Binary adjacency matrix of shape `([batch], N, N)`;\n\n    **Output**\n\n    - Node features with the same shape as the input, but with the last\n    dimension changed to `channels`;\n    - if `return_attn_coef=True`, a list with the attention coefficients for\n    each attention head. Each attention coefficient matrix has shape\n    `([batch], N, N)`.\n\n    **Arguments**\n\n    - `channels`: number of output channels;\n    - `attn_heads`: number of attention heads to use;\n    - `concat_heads`: bool, whether to concatenate the output of the attention\n     heads instead of averaging;\n    - `dropout_rate`: internal dropout rate for attention coefficients;\n    - `return_attn_coef`: if True, return the attention coefficients for\n    the given input (one N x N matrix for each head).\n    - `activation`: activation function to use;\n    - `use_bias`: bool, add a bias vector to the output;\n    - `kernel_initializer`: initializer for the weights;\n    - `attn_kernel_initializer`: initializer for the attention weights;\n    - `bias_initializer`: initializer for the bias vector;\n    - `kernel_regularizer`: regularization applied to the weights;\n    - `attn_kernel_regularizer`: regularization applied to the attention kernels;\n    - `bias_regularizer`: regularization applied to the bias vector;\n    - `activity_regularizer`: regularization applied to the output;\n    - `kernel_constraint`: constraint applied to the weights;\n    - `attn_kernel_constraint`: constraint applied to the attention kernels;\n    - `bias_constraint`: constraint applied to the bias vector.\n\n    """"""\n\n    def __init__(self,\n                 channels,\n                 attn_heads=1,\n                 concat_heads=True,\n                 dropout_rate=0.5,\n                 return_attn_coef=False,\n                 activation=None,\n                 use_bias=True,\n                 kernel_initializer=\'glorot_uniform\',\n                 bias_initializer=\'zeros\',\n                 attn_kernel_initializer=\'glorot_uniform\',\n                 kernel_regularizer=None,\n                 bias_regularizer=None,\n                 attn_kernel_regularizer=None,\n                 activity_regularizer=None,\n                 kernel_constraint=None,\n                 bias_constraint=None,\n                 attn_kernel_constraint=None,\n                 **kwargs):\n        super().__init__(channels,\n                         activation=activation,\n                         use_bias=use_bias,\n                         kernel_initializer=kernel_initializer,\n                         bias_initializer=bias_initializer,\n                         kernel_regularizer=kernel_regularizer,\n                         bias_regularizer=bias_regularizer,\n                         activity_regularizer=activity_regularizer,\n                         kernel_constraint=kernel_constraint,\n                         bias_constraint=bias_constraint,\n                         **kwargs)\n        self.attn_heads = attn_heads\n        self.concat_heads = concat_heads\n        self.dropout_rate = dropout_rate\n        self.return_attn_coef = return_attn_coef\n        self.attn_kernel_initializer = initializers.get(attn_kernel_initializer)\n        self.attn_kernel_regularizer = regularizers.get(attn_kernel_regularizer)\n        self.attn_kernel_constraint = constraints.get(attn_kernel_constraint)\n\n        if concat_heads:\n            # Output will have shape (..., attention_heads * channels)\n            self.output_dim = self.channels * self.attn_heads\n        else:\n            # Output will have shape (..., channels)\n            self.output_dim = self.channels\n\n    def build(self, input_shape):\n        assert len(input_shape) >= 2\n        input_dim = input_shape[0][-1]\n\n        self.kernel = self.add_weight(\n            name=\'kernel\',\n            shape=[input_dim, self.attn_heads, self.channels],\n            initializer=self.kernel_initializer,\n            regularizer=self.kernel_regularizer,\n            constraint=self.kernel_constraint,\n        )\n        self.attn_kernel_self = self.add_weight(\n            name=\'attn_kernel_self\',\n            shape=[self.channels, self.attn_heads, 1],\n            initializer=self.attn_kernel_initializer,\n            regularizer=self.attn_kernel_regularizer,\n            constraint=self.attn_kernel_constraint,\n        )\n        self.attn_kernel_neighs = self.add_weight(\n            name=\'attn_kernel_neigh\',\n            shape=[self.channels, self.attn_heads, 1],\n            initializer=self.attn_kernel_initializer,\n            regularizer=self.attn_kernel_regularizer,\n            constraint=self.attn_kernel_constraint,\n        )\n        if self.use_bias:\n            self.bias = self.add_weight(\n                shape=[self.output_dim],\n                initializer=self.bias_initializer,\n                regularizer=self.bias_regularizer,\n                constraint=self.bias_constraint,\n                name=\'bias\'\n            )\n\n        self.dropout = Dropout(self.dropout_rate)\n        self.built = True\n\n    def call(self, inputs):\n        X = inputs[0]\n        A = inputs[1]\n\n        mode = ops.autodetect_mode(A, X)\n        if mode == modes.SINGLE and K.is_sparse(A):\n            output, attn_coef = self._call_single(X, A)\n        else:\n            output, attn_coef = self._call_dense(X, A)\n\n        if self.concat_heads:\n            shape = output.shape[:-2] + [self.attn_heads * self.channels]\n            shape = [d if d is not None else -1 for d in shape]\n            output = tf.reshape(output, shape)\n        else:\n            output = tf.reduce_mean(output, axis=-2)\n\n        if self.use_bias:\n            output += self.bias\n\n        output = self.activation(output)\n\n        if self.return_attn_coef:\n            return output, attn_coef\n        else:\n            return output\n\n    def _call_single(self, X, A):\n        # Reshape kernels for efficient message-passing\n        kernel = tf.reshape(self.kernel, (-1, self.attn_heads * self.channels))\n        attn_kernel_self = ops.transpose(self.attn_kernel_self, (2, 1, 0))\n        attn_kernel_neighs = ops.transpose(self.attn_kernel_neighs, (2, 1, 0))\n\n        # Prepare message-passing\n        indices = A.indices\n        N = tf.shape(X, out_type=indices.dtype)[0]\n        indices = ops.sparse_add_self_loops(indices, N)\n        targets, sources = indices[:, -2], indices[:, -1]\n\n        # Update node features\n        X = ops.dot(X, kernel)\n        X = tf.reshape(X, (-1, self.attn_heads, self.channels))\n\n        # Compute attention\n        attn_for_self = tf.reduce_sum(X * attn_kernel_self, -1)\n        attn_for_self = tf.gather(attn_for_self, targets)\n        attn_for_neighs = tf.reduce_sum(X * attn_kernel_neighs, -1)\n        attn_for_neighs = tf.gather(attn_for_neighs, sources)\n\n        attn_coef = attn_for_self + attn_for_neighs\n        attn_coef = tf.nn.leaky_relu(attn_coef, alpha=0.2)\n        attn_coef = ops.unsorted_segment_softmax(attn_coef, targets, N)\n        attn_coef = self.dropout(attn_coef)\n        attn_coef = attn_coef[..., None]\n\n        # Update representation\n        output = attn_coef * tf.gather(X, sources)\n        output = ops.scatter_sum(output, targets, N)\n\n        return output, attn_coef\n\n    def _call_dense(self, X, A):\n        shape = tf.shape(A)[:-1]\n        A = tf.linalg.set_diag(A, tf.zeros(shape, A.dtype))\n        A = tf.linalg.set_diag(A, tf.ones(shape, A.dtype))\n        X = tf.einsum(""...NI , IHO -> ...NHO"", X, self.kernel)\n        attn_for_self = tf.einsum(""...NHI , IHO -> ...NHO"", X, self.attn_kernel_self)\n        attn_for_neighs = tf.einsum(""...NHI , IHO -> ...NHO"", X, self.attn_kernel_neighs)\n        attn_for_neighs = tf.einsum(""...ABC -> ...CBA"", attn_for_neighs)\n\n        attn_coef = attn_for_self + attn_for_neighs\n        attn_coef = tf.nn.leaky_relu(attn_coef, alpha=0.2)\n\n        mask = -10e9 * (1.0 - A)\n        attn_coef += mask[..., None, :]\n        attn_coef = tf.nn.softmax(attn_coef, axis=-1)\n        attn_coef_drop = self.dropout(attn_coef)\n\n        output = tf.einsum(""...NHM , ...MHI -> ...NHI"", attn_coef_drop, X)\n\n        return output, attn_coef\n\n    def compute_output_shape(self, input_shape):\n        output_shape = input_shape[0][:-1] + (self.output_dim,)\n        return output_shape\n\n    def get_config(self):\n        config = {\n            \'attn_heads\': self.attn_heads,\n            \'concat_heads\': self.concat_heads,\n            \'dropout_rate\': self.dropout_rate,\n            \'attn_kernel_initializer\': initializers.serialize(self.attn_kernel_initializer),\n            \'attn_kernel_regularizer\': regularizers.serialize(self.attn_kernel_regularizer),\n            \'attn_kernel_constraint\': constraints.serialize(self.attn_kernel_constraint),\n        }\n        base_config = super().get_config()\n        return dict(list(base_config.items()) + list(config.items()))\n\n    @staticmethod\n    def preprocess(A):\n        return A\n'"
spektral/layers/convolutional/graph_conv.py,0,"b'from tensorflow.keras import activations, initializers, regularizers, constraints\nfrom tensorflow.keras import backend as K\nfrom tensorflow.keras.layers import Layer\n\nfrom spektral.layers import ops\nfrom spektral.utils import localpooling_filter\n\n\nclass GraphConv(Layer):\n    r""""""\n    A graph convolutional layer (GCN) as presented by\n    [Kipf & Welling (2016)](https://arxiv.org/abs/1609.02907).\n\n    **Mode**: single, disjoint, mixed, batch.\n\n    This layer computes:\n    $$\n        \\Z = \\hat \\D^{-1/2} \\hat \\A \\hat \\D^{-1/2} \\X \\W + \\b\n    $$\n    where \\( \\hat \\A = \\A + \\I \\) is the adjacency matrix with added self-loops\n    and \\(\\hat\\D\\) is its degree matrix.\n\n    **Input**\n\n    - Node features of shape `([batch], N, F)`;\n    - Modified Laplacian of shape `([batch], N, N)`; can be computed with\n    `spektral.utils.convolution.localpooling_filter`.\n\n    **Output**\n\n    - Node features with the same shape as the input, but with the last\n    dimension changed to `channels`.\n\n    **Arguments**\n\n    - `channels`: number of output channels;\n    - `activation`: activation function to use;\n    - `use_bias`: bool, add a bias vector to the output;\n    - `kernel_initializer`: initializer for the weights;\n    - `bias_initializer`: initializer for the bias vector;\n    - `kernel_regularizer`: regularization applied to the weights;\n    - `bias_regularizer`: regularization applied to the bias vector;\n    - `activity_regularizer`: regularization applied to the output;\n    - `kernel_constraint`: constraint applied to the weights;\n    - `bias_constraint`: constraint applied to the bias vector.\n    """"""\n\n    def __init__(self,\n                 channels,\n                 activation=None,\n                 use_bias=True,\n                 kernel_initializer=\'glorot_uniform\',\n                 bias_initializer=\'zeros\',\n                 kernel_regularizer=None,\n                 bias_regularizer=None,\n                 activity_regularizer=None,\n                 kernel_constraint=None,\n                 bias_constraint=None,\n                 **kwargs):\n\n        super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n        self.channels = channels\n        self.activation = activations.get(activation)\n        self.use_bias = use_bias\n        self.kernel_initializer = initializers.get(kernel_initializer)\n        self.bias_initializer = initializers.get(bias_initializer)\n        self.kernel_regularizer = regularizers.get(kernel_regularizer)\n        self.bias_regularizer = regularizers.get(bias_regularizer)\n        self.kernel_constraint = constraints.get(kernel_constraint)\n        self.bias_constraint = constraints.get(bias_constraint)\n        self.supports_masking = False\n\n    def build(self, input_shape):\n        assert len(input_shape) >= 2\n        input_dim = input_shape[0][-1]\n        self.kernel = self.add_weight(shape=(input_dim, self.channels),\n                                      initializer=self.kernel_initializer,\n                                      name=\'kernel\',\n                                      regularizer=self.kernel_regularizer,\n                                      constraint=self.kernel_constraint)\n        if self.use_bias:\n            self.bias = self.add_weight(shape=(self.channels,),\n                                        initializer=self.bias_initializer,\n                                        name=\'bias\',\n                                        regularizer=self.bias_regularizer,\n                                        constraint=self.bias_constraint)\n        else:\n            self.bias = None\n        self.built = True\n\n    def call(self, inputs):\n        features = inputs[0]\n        fltr = inputs[1]\n\n        # Convolution\n        output = ops.dot(features, self.kernel)\n        output = ops.filter_dot(fltr, output)\n\n        if self.use_bias:\n            output = K.bias_add(output, self.bias)\n        if self.activation is not None:\n            output = self.activation(output)\n        return output\n\n    def compute_output_shape(self, input_shape):\n        features_shape = input_shape[0]\n        output_shape = features_shape[:-1] + (self.channels,)\n        return output_shape\n\n    def get_config(self):\n        config = {\n            \'channels\': self.channels,\n            \'activation\': activations.serialize(self.activation),\n            \'use_bias\': self.use_bias,\n            \'kernel_initializer\': initializers.serialize(self.kernel_initializer),\n            \'bias_initializer\': initializers.serialize(self.bias_initializer),\n            \'kernel_regularizer\': regularizers.serialize(self.kernel_regularizer),\n            \'bias_regularizer\': regularizers.serialize(self.bias_regularizer),\n            \'kernel_constraint\': constraints.serialize(self.kernel_constraint),\n            \'bias_constraint\': constraints.serialize(self.bias_constraint)\n        }\n        base_config = super().get_config()\n        return dict(list(base_config.items()) + list(config.items()))\n\n    @staticmethod\n    def preprocess(A):\n        return localpooling_filter(A)'"
spektral/layers/convolutional/graph_conv_skip.py,0,"b'from tensorflow.keras import backend as K\n\nfrom spektral.layers import ops\nfrom spektral.layers.convolutional.graph_conv import GraphConv\nfrom spektral.utils import normalized_adjacency\n\n\nclass GraphConvSkip(GraphConv):\n    r""""""\n    A simple convolutional layer with a skip connection.\n\n    **Mode**: single, disjoint, mixed, batch.\n\n    This layer computes:\n    $$\n        \\Z = \\D^{-1/2} \\A \\D^{-1/2} \\X \\W_1 + \\X \\W_2 + \\b\n    $$\n    where \\( \\A \\) does not have self-loops (unlike in GraphConv).\n\n    **Input**\n\n    - Node features of shape `([batch], N, F)`;\n    - Normalized adjacency matrix of shape `([batch], N, N)`; can be computed\n    with `spektral.utils.convolution.normalized_adjacency`.\n\n    **Output**\n\n    - Node features with the same shape as the input, but with the last\n    dimension changed to `channels`.\n\n    **Arguments**\n\n    - `channels`: number of output channels;\n    - `activation`: activation function to use;\n    - `use_bias`: bool, add a bias vector to the output;\n    - `kernel_initializer`: initializer for the weights;\n    - `bias_initializer`: initializer for the bias vector;\n    - `kernel_regularizer`: regularization applied to the weights;\n    - `bias_regularizer`: regularization applied to the bias vector;\n    - `activity_regularizer`: regularization applied to the output;\n    - `kernel_constraint`: constraint applied to the weights;\n    - `bias_constraint`: constraint applied to the bias vector.\n\n    """"""\n\n    def __init__(self,\n                 channels,\n                 activation=None,\n                 use_bias=True,\n                 kernel_initializer=\'glorot_uniform\',\n                 bias_initializer=\'zeros\',\n                 kernel_regularizer=None,\n                 bias_regularizer=None,\n                 activity_regularizer=None,\n                 kernel_constraint=None,\n                 bias_constraint=None,\n                 **kwargs):\n        super().__init__(channels,\n                         activation=activation,\n                         use_bias=use_bias,\n                         kernel_initializer=kernel_initializer,\n                         bias_initializer=bias_initializer,\n                         kernel_regularizer=kernel_regularizer,\n                         bias_regularizer=bias_regularizer,\n                         activity_regularizer=activity_regularizer,\n                         kernel_constraint=kernel_constraint,\n                         bias_constraint=bias_constraint,\n                         **kwargs)\n\n    def build(self, input_shape):\n        assert len(input_shape) >= 2\n        input_dim = input_shape[0][-1]\n\n        self.kernel_1 = self.add_weight(shape=(input_dim, self.channels),\n                                        initializer=self.kernel_initializer,\n                                        name=\'kernel_1\',\n                                        regularizer=self.kernel_regularizer,\n                                        constraint=self.kernel_constraint)\n        self.kernel_2 = self.add_weight(shape=(input_dim, self.channels),\n                                        initializer=self.kernel_initializer,\n                                        name=\'kernel_2\',\n                                        regularizer=self.kernel_regularizer,\n                                        constraint=self.kernel_constraint)\n        if self.use_bias:\n            self.bias = self.add_weight(shape=(self.channels,),\n                                        initializer=self.bias_initializer,\n                                        name=\'bias\',\n                                        regularizer=self.bias_regularizer,\n                                        constraint=self.bias_constraint)\n        else:\n            self.bias = None\n        self.built = True\n\n    def call(self, inputs):\n        features = inputs[0]\n        fltr = inputs[1]\n\n        # Convolution\n        output = K.dot(features, self.kernel_1)\n        output = ops.filter_dot(fltr, output)\n\n        # Skip connection\n        skip = K.dot(features, self.kernel_2)\n        output += skip\n\n        if self.use_bias:\n            output = K.bias_add(output, self.bias)\n        if self.activation is not None:\n            output = self.activation(output)\n        return output\n\n    @staticmethod\n    def preprocess(A):\n        return normalized_adjacency(A)\n'"
spektral/layers/convolutional/graphsage_conv.py,2,"b'import tensorflow as tf\nfrom tensorflow.keras import backend as K\n\nfrom spektral.layers import ops\nfrom spektral.layers.convolutional.graph_conv import GraphConv\n\n\nclass GraphSageConv(GraphConv):\n    r""""""\n    A GraphSAGE layer as presented by\n    [Hamilton et al. (2017)](https://arxiv.org/abs/1706.02216).\n\n    **Mode**: single, disjoint.\n\n    This layer computes:\n    $$\n        \\Z = \\big[ \\textrm{AGGREGATE}(\\X) \\| \\X \\big] \\W + \\b; \\\\\n        \\Z = \\frac{\\Z}{\\|\\Z\\|}\n    $$\n    where \\( \\textrm{AGGREGATE} \\) is a function to aggregate a node\'s\n    neighbourhood. The supported aggregation methods are: sum, mean,\n    max, min, and product.\n\n    **Input**\n\n    - Node features of shape `(N, F)`;\n    - Binary adjacency matrix of shape `(N, N)`.\n\n    **Output**\n\n    - Node features with the same shape as the input, but with the last\n    dimension changed to `channels`.\n\n    **Arguments**\n\n    - `channels`: number of output channels;\n    - `aggregate_op`: str, aggregation method to use (`\'sum\'`, `\'mean\'`,\n    `\'max\'`, `\'min\'`, `\'prod\'`);\n    - `activation`: activation function to use;\n    - `use_bias`: bool, add a bias vector to the output;\n    - `kernel_initializer`: initializer for the weights;\n    - `bias_initializer`: initializer for the bias vector;\n    - `kernel_regularizer`: regularization applied to the weights;\n    - `bias_regularizer`: regularization applied to the bias vector;\n    - `activity_regularizer`: regularization applied to the output;\n    - `kernel_constraint`: constraint applied to the weights;\n    - `bias_constraint`: constraint applied to the bias vector.\n\n    """"""\n\n    def __init__(self,\n                 channels,\n                 aggregate_op=\'mean\',\n                 activation=None,\n                 use_bias=True,\n                 kernel_initializer=\'glorot_uniform\',\n                 bias_initializer=\'zeros\',\n                 kernel_regularizer=None,\n                 bias_regularizer=None,\n                 activity_regularizer=None,\n                 kernel_constraint=None,\n                 bias_constraint=None,\n                 **kwargs):\n        super().__init__(channels,\n                         activation=activation,\n                         use_bias=use_bias,\n                         kernel_initializer=kernel_initializer,\n                         bias_initializer=bias_initializer,\n                         kernel_regularizer=kernel_regularizer,\n                         bias_regularizer=bias_regularizer,\n                         activity_regularizer=activity_regularizer,\n                         kernel_constraint=kernel_constraint,\n                         bias_constraint=bias_constraint,\n                         **kwargs)\n        if aggregate_op == \'sum\':\n            self.aggregate_op = ops.scatter_sum\n        elif aggregate_op == \'mean\':\n            self.aggregate_op = ops.scatter_mean\n        elif aggregate_op == \'max\':\n            self.aggregate_op = ops.scatter_max\n        elif aggregate_op == \'min\':\n            self.aggregate_op = ops.scatter_sum\n        elif aggregate_op == \'prod\':\n            self.aggregate_op = ops.scatter_prod\n        elif callable(aggregate_op):\n            self.aggregate_op = aggregate_op\n        else:\n            raise ValueError(\'Possbile aggragation methods: sum, mean, max, min, \'\n                             \'prod\')\n\n    def build(self, input_shape):\n        assert len(input_shape) >= 2\n        input_dim = input_shape[0][-1]\n        self.kernel = self.add_weight(shape=(2 * input_dim, self.channels),\n                                      initializer=self.kernel_initializer,\n                                      name=\'kernel\',\n                                      regularizer=self.kernel_regularizer,\n                                      constraint=self.kernel_constraint)\n        if self.use_bias:\n            self.bias = self.add_weight(shape=(self.channels,),\n                                        initializer=self.bias_initializer,\n                                        name=\'bias\',\n                                        regularizer=self.bias_regularizer,\n                                        constraint=self.bias_constraint)\n        else:\n            self.bias = None\n        self.built = True\n\n    def call(self, inputs):\n        features = inputs[0]\n        fltr = inputs[1]\n\n        # Enforce sparse representation\n        if not K.is_sparse(fltr):\n            fltr = ops.dense_to_sparse(fltr)\n\n        # Propagation\n        indices = fltr.indices\n        N = tf.shape(features, out_type=indices.dtype)[0]\n        indices = ops.sparse_add_self_loops(indices, N)\n        targets, sources = indices[:, -2], indices[:, -1]\n        messages = tf.gather(features, sources)\n        aggregated = self.aggregate_op(messages, targets, N)\n        output = K.concatenate([features, aggregated])\n        output = ops.dot(output, self.kernel)\n\n        if self.use_bias:\n            output = K.bias_add(output, self.bias)\n        output = K.l2_normalize(output, axis=-1)\n        if self.activation is not None:\n            output = self.activation(output)\n        return output\n\n    def get_config(self):\n        config = {\n            \'aggregate_op\': self.aggregate_op\n        }\n        base_config = super().get_config()\n        return dict(list(base_config.items()) + list(config.items()))\n\n    @staticmethod\n    def preprocess(A):\n        return A\n'"
spektral/layers/convolutional/message_passing.py,3,"b'import inspect\n\nimport tensorflow as tf\nfrom tensorflow.keras import backend as K\nfrom tensorflow.keras.layers import Layer\n\nfrom spektral.utils.keras import is_layer_kwarg, is_keras_kwarg, deserialize_kwarg, serialize_kwarg\nfrom spektral.layers.ops.scatter import deserialize_scatter\n\n\nclass MessagePassing(Layer):\n    r""""""\n    A general class for message passing as presented by\n    [Gilmer et al. (2017)](https://arxiv.org/abs/1704.01212).\n\n    **Mode**: single, disjoint.\n\n    **This layer and all of its extensions expect a sparse adjacency matrix.**\n\n    This layer computes:\n    $$\n        \\Z_i = \\gamma \\left( \\X_i, \\square_{j \\in \\mathcal{N}(i)} \\,\n        \\phi \\left(\\X_i, \\X_j, \\E_{j,i} \\right) \\right),\n    $$\n    \n    where \\( \\gamma \\) is a differentiable update function, \\( \\phi \\) is a\n    differentiable message function, \\( \\square \\) is a permutation-invariant\n    function to aggregate the messages (like the sum or the average), and\n    \\(\\E_{ij}\\) is the edge attribute of edge i-j.\n\n    By extending this class, it is possible to create any message-passing layer\n    in single/disjoint mode.\n\n    **API:**\n\n    - `propagate(X, A, E=None, **kwargs)`: propagate the messages and computes\n    embeddings for each node in the graph. `kwargs` will be propagated as\n    keyword arguments to `message()`, `aggregate()` and `update()`.\n    - `message(X, **kwargs)`: computes messages, equivalent to \\(\\phi\\) in the\n    definition.\n    Any extra keyword argument of this function will be  populated by\n    `propagate()` if a matching keyword is found.\n    Use `self.get_i()` and  `self.get_j()` to gather the elements using the\n    indices `i` or `j` of the adjacency matrix (e.g, `self.get_j(X)` will get\n    the features of the neighbours).\n    - `aggregate(messages, **kwargs)`: aggregates the messages, equivalent to\n    \\(\\square\\) in the definition.\n    The behaviour of this function can also be controlled using the `aggregate`\n    keyword in the constructor of the layer (supported aggregations: sum, mean,\n    max, min, prod).\n    Any extra keyword argument of this function will be  populated by\n    `propagate()` if a matching keyword is found.\n    - `update(embeddings, **kwargs)`: updates the aggregated messages to obtain\n    the final node embeddings, equivalent to \\(\\gamma\\) in the definition.\n    Any extra keyword argument of this function will be  populated by\n    `propagate()` if a matching keyword is found.\n\n    **Arguments**:\n\n    - `aggregate`: string or callable, an aggregate function. This flag can be\n    used to control the behaviour of `aggregate()` wihtout re-implementing it.\n    Supported aggregations: \'sum\', \'mean\', \'max\', \'min\', \'prod\'.\n    If callable, the function must have the signature `foo(updates, indices, N)`\n    and return a rank 2 tensor with shape `(N, ...)`.\n    """"""\n    def __init__(self, aggregate=\'sum\', **kwargs):\n        super().__init__(**{k: v for k, v in kwargs.items() if is_keras_kwarg(k)})\n        self.output_dim = None\n        self.kwargs_keys = []\n        for key in kwargs:\n            if is_layer_kwarg(key):\n                attr = kwargs[key]\n                attr = deserialize_kwarg(key, attr)\n                self.kwargs_keys.append(key)\n                setattr(self, key, attr)\n\n        self.msg_signature = inspect.signature(self.message).parameters\n        self.agg_signature = inspect.signature(self.aggregate).parameters\n        self.upd_signature = inspect.signature(self.update).parameters\n        self.agg = deserialize_scatter(aggregate)\n\n    def call(self, inputs, **kwargs):\n        X, A, E = self.get_inputs(inputs)\n        return self.propagate(X, A, E)\n\n    def build(self, input_shape):\n        self.built = True\n\n    def propagate(self, X, A, E=None, **kwargs):\n        self.N = tf.shape(X)[0]\n        self.index_i = A.indices[:, 0]\n        self.index_j = A.indices[:, 1]\n\n        # Message\n        msg_kwargs = self.get_kwargs(X, A, E, self.msg_signature, kwargs)\n        messages = self.message(X, **msg_kwargs)\n\n        # Aggregate\n        agg_kwargs = self.get_kwargs(X, A, E, self.agg_signature, kwargs)\n        embeddings = self.aggregate(messages, **agg_kwargs)\n\n        # Update\n        upd_kwargs = self.get_kwargs(X, A, E, self.upd_signature, kwargs)\n        output = self.update(embeddings, **upd_kwargs)\n\n        return output\n\n    def message(self, X, **kwargs):\n        return self.get_j(X)\n\n    def aggregate(self, messages, **kwargs):\n        return self.agg(messages, self.index_i, self.N)\n\n    def update(self, embeddings, **kwargs):\n        return embeddings\n\n    def get_i(self, x):\n        return tf.gather(x, self.index_i)\n\n    def get_j(self, x):\n        return tf.gather(x, self.index_j)\n\n    def get_kwargs(self, X, A, E, signature, kwargs):\n        output = {}\n        for k in signature.keys():\n            if signature[k].default is inspect.Parameter.empty or k == \'kwargs\':\n                pass\n            elif k == \'X\':\n                output[k] = X\n            elif k == \'A\':\n                output[k] = A\n            elif k == \'E\':\n                output[k] = E\n            elif k in kwargs:\n                output[k] = kwargs[k]\n            else:\n                raise ValueError(\'Missing key {} for signature {}\'\n                                 .format(k, signature))\n\n        return output\n\n    @staticmethod\n    def get_inputs(inputs):\n        if len(inputs) == 3:\n            X, A, E = inputs\n            assert K.ndim(E) == 2, \'E must have rank 2\'\n        elif len(inputs) == 2:\n            X, A = inputs\n            E = None\n        else:\n            raise ValueError(\'Expected 2 or 3 inputs tensors (X, A, E), got {}.\'\n                             .format(len(inputs)))\n        assert K.ndim(X) == 2, \'X must have rank 2\'\n        assert K.is_sparse(A), \'A must be a SparseTensor\'\n        assert K.ndim(A) == 2, \'A must have rank 2\'\n\n        return X, A, E\n\n    def compute_output_shape(self, input_shape):\n        if self.output_dim:\n            output_shape = input_shape[0][:-1] + (self.output_dim, )\n        else:\n            output_shape = input_shape[0]\n        return output_shape\n\n    def get_config(self):\n        config = {\n            \'aggregate\': self.agg,\n        }\n        for key in self.kwargs_keys:\n            config[key] = serialize_kwarg(key, getattr(self, key))\n        base_config = super().get_config()\n        return {**base_config, **config}\n\n    @staticmethod\n    def preprocess(A):\n        return A\n'"
spektral/layers/convolutional/tag_conv.py,0,"b'from tensorflow.keras import backend as K\nfrom tensorflow.keras.layers import Dense\n\nfrom spektral.layers.convolutional.message_passing import MessagePassing\nfrom spektral.utils import normalized_adjacency\n\n\nclass TAGConv(MessagePassing):\n    r""""""\n    A Topology Adaptive Graph Convolutional layer (TAG) as presented by\n    [Du et al. (2017)](https://arxiv.org/abs/1710.10370).\n\n    **Mode**: single, disjoint.\n\n    **This layer expects a sparse adjacency matrix.**\n\n    This layer computes:\n    $$\n        \\Z = \\sum\\limits_{k=0}^{K} \\D^{-1/2}\\A^k\\D^{-1/2}\\X\\W^{(k)}\n    $$\n\n    **Input**\n\n    - Node features of shape `(N, F)`;\n    - Binary adjacency matrix of shape `(N, N)`.\n\n    **Output**\n\n    - Node features with the same shape of the input, but the last dimension\n    changed to `channels`.\n\n    **Arguments**\n\n    - `channels`: integer, number of output channels;\n    - `K`: the order of the layer (i.e., the layer will consider a K-hop\n    neighbourhood for each node);\n    - `activation`: activation function to use;\n    - `use_bias`: bool, add a bias vector to the output;\n    - `kernel_initializer`: initializer for the weights;\n    - `bias_initializer`: initializer for the bias vector;\n    - `kernel_regularizer`: regularization applied to the weights;\n    - `bias_regularizer`: regularization applied to the bias vector;\n    - `activity_regularizer`: regularization applied to the output;\n    - `kernel_constraint`: constraint applied to the weights;\n    - `bias_constraint`: constraint applied to the bias vector.\n    """"""\n\n    def __init__(self,\n                 channels,\n                 K=3,\n                 activation=None,\n                 use_bias=True,\n                 kernel_initializer=\'glorot_uniform\',\n                 bias_initializer=\'zeros\',\n                 kernel_regularizer=None,\n                 bias_regularizer=None,\n                 activity_regularizer=None,\n                 kernel_constraint=None,\n                 bias_constraint=None,\n                 **kwargs):\n        super().__init__(aggregate=\'sum\',\n                         activation=activation,\n                         use_bias=use_bias,\n                         kernel_initializer=kernel_initializer,\n                         bias_initializer=bias_initializer,\n                         kernel_regularizer=kernel_regularizer,\n                         bias_regularizer=bias_regularizer,\n                         activity_regularizer=activity_regularizer,\n                         kernel_constraint=kernel_constraint,\n                         bias_constraint=bias_constraint,\n                         **kwargs)\n        self.channels = self.output_dim = channels\n        self.K = K\n        self.linear = Dense(channels,\n                            activation=activation,\n                            use_bias=use_bias,\n                            kernel_initializer=kernel_initializer,\n                            bias_initializer=bias_initializer,\n                            kernel_regularizer=kernel_regularizer,\n                            bias_regularizer=bias_regularizer,\n                            activity_regularizer=activity_regularizer,\n                            kernel_constraint=kernel_constraint,\n                            bias_constraint=bias_constraint)\n\n    def build(self, input_shape):\n        assert len(input_shape) >= 2\n        self.built = True\n\n    def call(self, inputs, **kwargs):\n        X, A, E = self.get_inputs(inputs)\n        edge_weight = A.values\n\n        output = [X]\n        for k in range(self.K):\n            output.append(self.propagate(X, A, E, edge_weight=edge_weight))\n        output = K.concatenate(output)\n\n        return self.linear(output)\n\n    def message(self, X, edge_weight=None):\n        X_j = self.get_j(X)\n        return edge_weight[:, None] * X_j\n\n    def get_config(self):\n        config = {\n            \'channels\': self.channels,\n        }\n        base_config = super().get_config()\n        base_config.pop(\'aggregate\')  # Remove it because it\'s defined by constructor\n        return {**base_config, **config}\n\n    @staticmethod\n    def preprocess(A):\n        return normalized_adjacency(A)\n'"
spektral/layers/ops/__init__.py,0,b'from .ops import *\nfrom .matmul import *\nfrom .graph import *\nfrom .modes import *\nfrom .scatter import *\nfrom .sparse import *\n\n'
spektral/layers/ops/graph.py,12,"b'import tensorflow as tf\nfrom tensorflow.keras import backend as K\n\nfrom . import ops as ops\n\n\ndef normalize_A(A):\n    """"""\n    Computes symmetric normalization of A, dealing with sparse A and batch mode\n    automatically.\n    :param A: Tensor or SparseTensor with rank k = {2, 3}.\n    :return: Tensor or SparseTensor of rank k.\n    """"""\n    D = degrees(A)\n    D = tf.sqrt(D)[:, None] + K.epsilon()\n    perm = (0, 2, 1) if K.ndim(A) == 3 else (1, 0)\n    output = (A / D) / ops.transpose(D, perm=perm)\n\n    return output\n\n\ndef degrees(A):\n    """"""\n    Computes the degrees of each node in A, dealing with sparse A and batch mode\n    automatically.\n    :param A: Tensor or SparseTensor with rank k = {2, 3}.\n    :return: Tensor or SparseTensor of rank k - 1.\n    """"""\n    if K.is_sparse(A):\n        D = tf.sparse.reduce_sum(A, axis=-1)\n    else:\n        D = tf.reduce_sum(A, axis=-1)\n\n    return D\n\n\ndef degree_matrix(A, return_sparse_batch=False):\n    """"""\n    Computes the degree matrix of A, deals with sparse A and batch mode\n    automatically.\n    :param A: Tensor or SparseTensor with rank k = {2, 3}.\n    :param return_sparse_batch: if operating in batch mode, return a\n    SparseTensor. Note that the sparse degree Tensor returned by this function\n    cannot be used for sparse matrix multiplication afterwards.\n    :return: SparseTensor of rank k.\n    """"""\n    D = degrees(A)\n\n    batch_mode = K.ndim(D) == 2\n    N = tf.shape(D)[-1]\n    batch_size = tf.shape(D)[0] if batch_mode else 1\n\n    inner_index = tf.tile(tf.stack([tf.range(N)] * 2, axis=1), (batch_size, 1))\n    if batch_mode:\n        if return_sparse_batch:\n            outer_index = ops.repeat(\n                tf.range(batch_size), tf.ones(batch_size) * tf.cast(N, tf.float32)\n            )\n            indices = tf.concat([outer_index[:, None], inner_index], 1)\n            dense_shape = (batch_size, N, N)\n        else:\n            return tf.linalg.diag(D)\n    else:\n        indices = inner_index\n        dense_shape = (N, N)\n\n    indices = tf.cast(indices, tf.int64)\n    values = tf.reshape(D, (-1, ))\n    return tf.SparseTensor(indices, values, dense_shape)'"
spektral/layers/ops/matmul.py,5,"b'import tensorflow as tf\nfrom tensorflow.keras import backend as K\nfrom tensorflow.python.ops.linalg.sparse import sparse as tfsp\n\nfrom . import modes as modes\nfrom . import ops as ops\n\n\ndef filter_dot(fltr, features):\n    """"""\n    Wrapper for matmul_A_B, specifically used to compute the matrix multiplication\n    between a graph filter and node features.\n    :param fltr:\n    :param features: the node features (N x F in single mode, batch x N x F in\n    mixed and batch mode).\n    :return: the filtered features.\n    """"""\n    mode = modes.autodetect_mode(fltr, features)\n    if mode == modes.SINGLE or mode == modes.BATCH:\n        return dot(fltr, features)\n    else:\n        # Mixed mode\n        return mixed_mode_dot(fltr, features)\n\n\ndef dot(a, b, transpose_a=False, transpose_b=False):\n    """"""\n    Dot product between a and b along innermost dimensions, for a and b with\n    same rank. Supports both dense and sparse multiplication (including\n    sparse-sparse).\n    :param a: Tensor or SparseTensor with rank 2 or 3.\n    :param b: Tensor or SparseTensor with same rank as a.\n    :param transpose_a: bool, transpose innermost two dimensions of a.\n    :param transpose_b: bool, transpose innermost two dimensions of b.\n    :return: Tensor or SparseTensor with rank 2 or 3.\n    """"""\n    a_is_sparse_tensor = isinstance(a, tf.SparseTensor)\n    b_is_sparse_tensor = isinstance(b, tf.SparseTensor)\n    if a_is_sparse_tensor:\n        a = tfsp.CSRSparseMatrix(a)\n    if b_is_sparse_tensor:\n        b = tfsp.CSRSparseMatrix(b)\n    out = tfsp.matmul(a, b, transpose_a=transpose_a, transpose_b=transpose_b)\n    if hasattr(out, \'to_sparse_tensor\'):\n        return out.to_sparse_tensor()\n\n    return out\n\n\ndef mixed_mode_dot(a, b):\n    """"""\n    Computes the equivalent of `tf.einsum(\'ij,bjk->bik\', a, b)`, but\n    works for both dense and sparse input filters.\n    :param a: rank 2 Tensor or SparseTensor.\n    :param b: rank 3 Tensor or SparseTensor.\n    :return: rank 3 Tensor or SparseTensor.\n    """"""\n    s_0_, s_1_, s_2_ = K.int_shape(b)\n    B_T = ops.transpose(b, (1, 2, 0))\n    B_T = ops.reshape(B_T, (s_1_, -1))\n    output = dot(a, B_T)\n    output = ops.reshape(output, (s_1_, s_2_, -1))\n    output = ops.transpose(output, (2, 0, 1))\n\n    return output\n\n\ndef matmul_A_B(a, b):\n    """"""\n    Computes A * B, dealing automatically with sparsity and data modes.\n    :param a: Tensor or SparseTensor with rank 2 or 3.\n    :param b: Tensor or SparseTensor with rank 2 or 3.\n    :return: Tensor or SparseTensor with rank = max(rank(a), rank(b)).\n    """"""\n    mode = modes.autodetect_mode(a, b)\n    if mode == modes.MIXED:\n        # Mixed mode (rank(a)=2, rank(b)=3)\n        output = mixed_mode_dot(a, b)\n    elif mode == modes.iMIXED:\n        # Inverted mixed (rank(a)=3, rank(b)=2)\n        s_1_a, s_2_a = tf.shape(a)[1], tf.shape(a)[2]\n        s_1_b = tf.shape(b)[1]\n        a_flat = ops.reshape(a, (-1, s_2_a))\n        output = dot(a_flat, b)\n        output = ops.reshape(output, (-1, s_1_a, s_1_b))\n    else:\n        # Single (rank(a)=2, rank(b)=2) and batch (rank(a)=3, rank(b)=3) mode\n        output = dot(a, b)\n\n    return output\n\n\ndef matmul_AT_B(a, b):\n    """"""\n    Computes A.T * B, dealing automatically with sparsity and data modes.\n    :param a: Tensor or SparseTensor with rank 2 or 3.\n    :param b: Tensor or SparseTensor with rank 2 or 3.\n    :return: Tensor or SparseTensor with rank = max(rank(a), rank(b)).\n    """"""\n    mode = modes.autodetect_mode(a, b)\n    if mode == modes.SINGLE or mode == modes.MIXED:\n        # Single (rank(a)=2, rank(b)=2)\n        # Mixed (rank(a)=2, rank(b)=3)\n        a_t = ops.transpose(a)\n    elif mode == modes.iMIXED or mode == modes.BATCH:\n        # Inverted mixed (rank(a)=3, rank(b)=2)\n        # Batch (rank(a)=3, rank(b)=3)\n        a_t = ops.transpose(a, (0, 2, 1))\n    else:\n        raise ValueError(\'Expected ranks to be 2 or 3, got {} and {}\'.format(\n            K.ndim(a), K.ndim(b)\n        ))\n\n    return matmul_A_B(a_t, b)\n\n\ndef matmul_A_BT(a, b):\n    """"""\n    Computes A * B.T, dealing automatically with sparsity and data modes.\n    :param a: Tensor or SparseTensor with rank 2 or 3.\n    :param b: Tensor or SparseTensor with rank 2 or 3.\n    :return: Tensor or SparseTensor with rank = max(rank(a), rank(b)).\n    """"""\n    mode = modes.autodetect_mode(a, b)\n    if mode == modes.SINGLE or mode == modes.iMIXED:\n        # Single (rank(a)=2, rank(b)=2)\n        # Inverted mixed (rank(a)=3, rank(b)=2)\n        b_t = ops.transpose(b)\n    elif mode == modes.MIXED or mode == modes.BATCH:\n        # Mixed (rank(a)=2, rank(b)=3)\n        # Batch (rank(a)=3, rank(b)=3)\n        b_t = ops.transpose(b, (0, 2, 1))\n    else:\n        raise ValueError(\'Expected ranks to be 2 or 3, got {} and {}\'.format(\n            K.ndim(a), K.ndim(b)\n        ))\n\n    return matmul_A_B(a, b_t)\n\n\ndef matmul_AT_B_A(a, b):\n    """"""\n    Computes A.T * B * A, dealing automatically with sparsity and data modes.\n    :param a: Tensor or SparseTensor with rank 2 or 3.\n    :param b: Tensor or SparseTensor with rank 2 or 3.\n    :return: Tensor or SparseTensor with rank = max(rank(a), rank(b)).\n    """"""\n    at_b = matmul_AT_B(a, b)\n    at_b_a = matmul_A_B(at_b, a)\n\n    return at_b_a\n\n\ndef matmul_A_B_AT(a, b):\n    """"""\n    Computes A * B * A.T, dealing automatically with sparsity and data modes.\n    :param a: Tensor or SparseTensor with rank 2 or 3.\n    :param b: Tensor or SparseTensor with rank 2 or 3.\n    :return: Tensor or SparseTensor with rank = max(rank(a), rank(b)).\n    """"""\n    b_at = matmul_A_BT(a, b)\n    a_b_at = matmul_A_B(a, b_at)\n\n    return a_b_at\n\n\ndef matrix_power(a, k):\n    """"""\n    If a is a square matrix, computes a^k. If a is a rank 3 Tensor of square\n    matrices, computes the exponent of each inner matrix.\n    :param a: Tensor or SparseTensor with rank 2 or 3. The innermost two\n    dimensions must be the same.\n    :param k: int, the exponent to which to raise the matrices.\n    :return: Tensor or SparseTensor with same rank as the input.\n    """"""\n    x_k = a\n    for _ in range(k - 1):\n        x_k = matmul_A_B(a, x_k)\n\n    return x_k'"
spektral/layers/ops/modes.py,0,"b'from tensorflow.keras import backend as K\n\nSINGLE  = 1   # Single         (rank(a)=2, rank(b)=2)\nMIXED   = 2   # Mixed          (rank(a)=2, rank(b)=3)\niMIXED  = 3   # Inverted mixed (rank(a)=3, rank(b)=2)\nBATCH   = 4   # Batch          (rank(a)=3, rank(b)=3)\nUNKNOWN = -1  # Unknown\n\ndef autodetect_mode(a, b):\n    """"""\n    Return a code identifying the mode of operation (single, mixed, inverted mixed and\n    batch), given a and b. See `ops.modes` for meaning of codes.\n    :param a: Tensor or SparseTensor.\n    :param b: Tensor or SparseTensor.\n    :return: mode of operation as an integer code.\n    """"""\n    a_dim = K.ndim(a)\n    b_dim = K.ndim(b)\n    if b_dim == 2:\n        if a_dim == 2:\n            return SINGLE\n        elif a_dim == 3:\n            return iMIXED\n    elif b_dim == 3:\n        if a_dim == 2:\n            return MIXED\n        elif a_dim == 3:\n            return BATCH\n    return UNKNOWN'"
spektral/layers/ops/ops.py,32,"b'import tensorflow as tf\nfrom tensorflow.keras import backend as K\n\n\ndef transpose(a, perm=None, name=None):\n    """"""\n    Transposes a according to perm, dealing automatically with sparsity.\n    :param a: Tensor or SparseTensor with rank k.\n    :param perm: permutation indices of size k.\n    :param name: name for the operation.\n    :return: Tensor or SparseTensor with rank k.\n    """"""\n    if K.is_sparse(a):\n        transpose_op = tf.sparse.transpose\n    else:\n        transpose_op = tf.transpose\n\n    if perm is None:\n        perm = (1, 0)  # Make explicit so that shape will always be preserved\n    return transpose_op(a, perm=perm, name=name)\n\n\ndef reshape(a, shape=None, name=None):\n    """"""\n    Reshapes a according to shape, dealing automatically with sparsity.\n    :param a: Tensor or SparseTensor.\n    :param shape: new shape.\n    :param name: name for the operation.\n    :return: Tensor or SparseTensor.\n    """"""\n    if K.is_sparse(a):\n        reshape_op = tf.sparse.reshape\n    else:\n        reshape_op = tf.reshape\n\n    return reshape_op(a, shape=shape, name=name)\n\n\ndef repeat(x, repeats):\n    """"""\n    Repeats elements of a Tensor (equivalent to np.repeat, but only for 1D\n    tensors).\n    :param x: rank 1 Tensor;\n    :param repeats: rank 1 Tensor with same shape as x, the number of\n    repetitions for each element;\n    :return: rank 1 Tensor, of shape `(sum(repeats), )`.\n    """"""\n    x = tf.expand_dims(x, 1)\n    max_repeats = tf.reduce_max(repeats)\n    tile_repeats = [1, max_repeats]\n    arr_tiled = tf.tile(x, tile_repeats)\n    mask = tf.less(tf.range(max_repeats), tf.expand_dims(repeats, 1))\n    result = tf.reshape(tf.boolean_mask(arr_tiled, mask), [-1])\n    return result\n\n\ndef segment_top_k(x, I, ratio, top_k_var):\n    """"""\n    Returns indices to get the top K values in x segment-wise, according to\n    the segments defined in I. K is not fixed, but it is defined as a ratio of\n    the number of elements in each segment.\n    :param x: a rank 1 Tensor;\n    :param I: a rank 1 Tensor with segment IDs for x;\n    :param ratio: float, ratio of elements to keep for each segment;\n    :param top_k_var: a tf.Variable created without shape validation (i.e.,\n    `tf.Variable(0.0, validate_shape=False)`);\n    :return: a rank 1 Tensor containing the indices to get the top K values of\n    each segment in x.\n    """"""\n    I = tf.cast(I, tf.int32)\n    num_nodes = tf.math.segment_sum(tf.ones_like(I), I)  # Number of nodes in each graph\n    cumsum = tf.cumsum(num_nodes)  # Cumulative number of nodes (A, A+B, A+B+C)\n    cumsum_start = cumsum - num_nodes  # Start index of each graph\n    n_graphs = tf.shape(num_nodes)[0]  # Number of graphs in batch\n    max_n_nodes = tf.reduce_max(num_nodes)  # Order of biggest graph in batch\n    batch_n_nodes = tf.shape(I)[0]  # Number of overall nodes in batch\n    to_keep = tf.math.ceil(ratio * tf.cast(num_nodes, tf.float32))\n    to_keep = tf.cast(to_keep, I.dtype)  # Nodes to keep in each graph\n\n    index = tf.range(batch_n_nodes)\n    index = (index - tf.gather(cumsum_start, I)) + (I * max_n_nodes)\n\n    y_min = tf.reduce_min(x)\n    dense_y = tf.ones((n_graphs * max_n_nodes,))\n    # subtract 1 to ensure that filler values do not get picked\n    dense_y = dense_y * tf.cast(y_min - 1, dense_y.dtype)\n    dense_y = tf.cast(dense_y, top_k_var.dtype)\n    # top_k_var is a variable with unknown shape defined in the elsewhere\n    top_k_var.assign(dense_y)\n    dense_y = tf.tensor_scatter_nd_update(top_k_var, index[..., None], tf.cast(x, top_k_var.dtype))\n    dense_y = tf.reshape(dense_y, (n_graphs, max_n_nodes))\n\n    perm = tf.argsort(dense_y, direction=\'DESCENDING\')\n    perm = perm + cumsum_start[:, None]\n    perm = tf.reshape(perm, (-1,))\n\n    to_rep = tf.tile(tf.constant([1., 0.]), (n_graphs,))\n    rep_times = tf.reshape(tf.concat((to_keep[:, None], (max_n_nodes - to_keep)[:, None]), -1), (-1,))\n    mask = repeat(to_rep, rep_times)\n\n    perm = tf.boolean_mask(perm, mask)\n\n    return perm\n'"
spektral/layers/ops/scatter.py,5,"b'import tensorflow as tf\n\n\ndef scatter_sum(updates, indices, N):\n    """"""\n    Sums updates along the first dimensions according to the indices, returns\n    a Tensor of the same rank as updates with shape `(N, ...)`.\n    If the result is empty for a given index `i`, `output[i] = 0`.\n    If a given index`i` is negative, the value is ignored.\n    :param updates: a Tensor.\n    :param indices: A Tensor with indices to index the updates.\n    :param N: first dimension the output (i.e., total number of segments).\n    :return: a Tensor with the same rank as updates, of shape\n    `(N, ) + updates.shape[1:]`.\n    """"""\n    return tf.math.unsorted_segment_sum(updates, indices, N)\n\n\ndef scatter_mean(updates, indices, N):\n    """"""\n    Averages updates along the first dimensions according to the indices,\n    returns a Tensor of the same rank as updates with shape `(N, ...)`.\n    If the result is empty for a given index `i`, `output[i] = 0`.\n    If a given index`i` is negative, the value is ignored.\n    :param updates: a Tensor.\n    :param indices: A Tensor with indices to index the updates.\n    :param N: first dimension the output (i.e., total number of segments).\n    :return: a Tensor with the same rank as updates, of shape\n    `(N, ) + updates.shape[1:]`.\n    """"""\n    return tf.math.unsorted_segment_mean(updates, indices, N)\n\n\n# Alias for scatter_mean for convenience\nscatter_avg = scatter_mean\n\n\ndef scatter_max(updates, indices, N):\n    """"""\n    Max-reduces updates along the first dimensions according to the indices,\n    returns a Tensor of the same rank as updates with shape `(N, ...)`.\n    If the result is empty for a given index `i`, `output[i] = 0`.\n    If a given index`i` is negative, the value is ignored.\n    :param updates: a Tensor.\n    :param indices: A Tensor with indices to index the updates.\n    :param N: first dimension the output (i.e., total number of segments).\n    :return: a Tensor with the same rank as updates, of shape\n    `(N, ) + updates.shape[1:]`.\n    """"""\n    return tf.math.unsorted_segment_max(updates, indices, N)\n\n\ndef scatter_min(updates, indices, N):\n    """"""\n    Min-reduces updates along the first dimensions according to the indices,\n    returns a Tensor of the same rank as updates with shape `(N, ...)`.\n    If the result is empty for a given index `i`, `output[i] = 0`.\n    If a given index`i` is negative, the value is ignored.\n    :param updates: a Tensor.\n    :param indices: A Tensor with indices to index the updates.\n    :param N: first dimension the output (i.e., total number of segments).\n    :return: a Tensor with the same rank as updates, of shape\n    `(N, ) + updates.shape[1:]`.\n    """"""\n    return tf.math.unsorted_segment_min(updates, indices, N)\n\n\ndef scatter_prod(updates, indices, N):\n    """"""\n    Multiplies updates along the first dimensions according to the indices,\n    returns a Tensor of the same rank as updates with shape `(N, ...)`.\n    If the result is empty for a given index `i`, `output[i] = 0`.\n    If a given index`i` is negative, the value is ignored.\n    :param updates: a Tensor.\n    :param indices: A Tensor with indices to index the updates.\n    :param N: first dimension the output (i.e., total number of segments).\n    :return: a Tensor with the same rank as updates, of shape\n    `(N, ) + updates.shape[1:]`.\n    """"""\n    return tf.math.unsorted_segment_prod(updates, indices, N)\n\n\nOP_DICT = {\n    \'sum\': scatter_sum,\n    \'mean\': scatter_mean,\n    \'avg\': scatter_avg,\n    \'max\': scatter_max,\n    \'min\': scatter_min,\n    \'prod\': scatter_prod\n}\n\n\ndef deserialize_scatter(scatter):\n    if isinstance(scatter, str):\n        if scatter in OP_DICT:\n            return OP_DICT[scatter]\n        else:\n            if callable(scatter):\n                return scatter\n            else:\n                raise ValueError(\'scatter must be callable or string in: {}.\'\n                                 .format(list(OP_DICT.keys())))\n'"
spektral/layers/ops/sparse.py,17,"b'import numpy as np\nimport tensorflow as tf\nfrom scipy import sparse as sp\nfrom tensorflow.python import gen_sparse_ops\n\n\ndef sp_matrix_to_sp_tensor(x):\n    """"""\n    Converts a Scipy sparse matrix to a SparseTensor.\n    :param x: a Scipy sparse matrix.\n    :return: a SparseTensor.\n    """"""\n    if not hasattr(x, \'tocoo\'):\n        try:\n            x = sp.coo_matrix(x)\n        except:\n            raise TypeError(\'x must be convertible to scipy.coo_matrix\')\n    else:\n        x = x.tocoo()\n    out = tf.SparseTensor(\n        indices=np.array([x.row, x.col]).T,\n        values=x.data,\n        dense_shape=x.shape\n    )\n    return tf.sparse.reorder(out)\n\n\ndef sp_batch_to_sp_tensor(a_list):\n    """"""\n    Converts a list of Scipy sparse matrices to a rank 3 SparseTensor.\n    :param a_list: list of Scipy sparse matrices with the same shape.\n    :return: SparseTensor of rank 3.\n    """"""\n    tensor_data = []\n    for i, a in enumerate(a_list):\n        values = a.tocoo().data\n        row = a.row\n        col = a.col\n        batch = np.ones_like(col) * i\n        tensor_data.append((values, batch, row, col))\n    tensor_data = list(map(np.concatenate, zip(*tensor_data)))\n\n    out = tf.SparseTensor(\n        indices=np.array(tensor_data[1:]).T,\n        values=tensor_data[0],\n        dense_shape=(len(a_list), ) + a_list[0].shape\n    )\n\n    return out\n\n\ndef dense_to_sparse(x):\n    """"""\n    Converts a Tensor to a SparseTensor.\n    :param x: a Tensor.\n    :return: a SparseTensor.\n    """"""\n    indices = tf.where(tf.not_equal(x, 0))\n    values = tf.gather_nd(x, indices)\n    shape = tf.shape(x, out_type=tf.int64)\n    return tf.SparseTensor(indices, values, shape)\n\n\ndef sparse_add_self_loops(indices, N=None):\n    """"""\n    Given the indices of a square SparseTensor, adds the diagonal entries (i, i)\n    and returns the reordered indices.\n    :param indices: Tensor of rank 2, the indices to a SparseTensor.\n    :param N: the size of the N x N SparseTensor indexed by the indices. If `None`,\n    N is calculated as the maximum entry in the indices plus 1.\n    :return: Tensor of rank 2, the indices to a SparseTensor.\n    """"""\n    N = tf.reduce_max(indices) + 1 if N is None else N\n    row, col = indices[..., 0], indices[..., 1]\n    mask = tf.ensure_shape(row != col, row.shape)\n    sl_indices = tf.range(N, dtype=row.dtype)[:, None]\n    sl_indices = tf.repeat(sl_indices, 2, -1)\n    indices = tf.concat((indices[mask], sl_indices), 0)\n    dummy_values = tf.ones_like(indices[:, 0])\n    indices, _ = gen_sparse_ops.sparse_reorder(indices, dummy_values, (N, N))\n    return indices\n\n\ndef unsorted_segment_softmax(x, indices, N=None):\n    """"""\n    Applies softmax along the segments of a Tensor. This operator is similar\n    to the tf.math.segment_* operators, which apply a certain reduction to the\n    segments. In this case, the output tensor is not reduced and maintains the\n    same shape as the input.\n    :param x: a Tensor. The softmax is applied along the first dimension.\n    :param indices: a Tensor, indices to the segments.\n    :param N: the number of unique segments in the indices. If `None`, N is\n    calculated as the maximum entry in the indices plus 1.\n    :return: a Tensor with the same shape as the input.\n    """"""\n    N = tf.reduce_max(indices) + 1 if N is None else N\n    e_x = tf.exp(x - tf.gather(tf.math.unsorted_segment_max(x, indices, N), indices))\n    e_x /= tf.gather(tf.math.unsorted_segment_sum(e_x, indices, N) + 1e-9, indices)\n    return e_x\n'"
spektral/layers/pooling/__init__.py,0,"b'from .diff_pool import DiffPool\nfrom .global_pool import (GlobalSumPool, GlobalAvgPool, GlobalMaxPool,\n                          GlobalAttnSumPool, GlobalAttentionPool, SortPool)\nfrom .mincut_pool import MinCutPool\nfrom .sag_pool import SAGPool\nfrom .topk_pool import TopKPool\n'"
spektral/layers/pooling/diff_pool.py,9,"b'import tensorflow as tf\nfrom tensorflow.keras import activations, initializers, regularizers, constraints\nfrom tensorflow.keras import backend as K\nfrom tensorflow.keras.layers import Layer\n\nfrom spektral.layers import ops\nfrom spektral.layers.ops import modes\n\n\nclass DiffPool(Layer):\n    r""""""\n    A DiffPool layer as presented by\n    [Ying et al. (2018)](https://arxiv.org/abs/1806.08804).\n\n    **Mode**: batch.\n\n    This layer computes a soft clustering \\(\\S\\) of the input graphs using a GNN,\n    and reduces graphs as follows:\n\n    $$\n        \\S = \\textrm{GNN}(\\A, \\X); \\\\\n        \\A\' = \\S^\\top \\A \\S; \\X\' = \\S^\\top \\X;\n    $$\n\n    where GNN consists of one GraphConv layer with softmax activation.\n    Two auxiliary loss terms are also added to the model: the _link prediction\n    loss_\n    $$\n        \\big\\| \\A - \\S\\S^\\top \\big\\|_F\n    $$\n    and the _entropy loss_\n    $$\n        - \\frac{1}{N} \\sum\\limits_{i = 1}^{N} \\S \\log (\\S).\n    $$\n\n    The layer also applies a 1-layer GCN to the input features, and returns\n    the updated graph signal (the number of output channels is controlled by\n    the `channels` parameter).\n    The layer can be used without a supervised loss, to compute node clustering\n    simply by minimizing the two auxiliary losses.\n\n    **Input**\n\n    - Node features of shape `([batch], N, F)`;\n    - Binary adjacency matrix of shape `([batch], N, N)`;\n\n    **Output**\n\n    - Reduced node features of shape `([batch], K, channels)`;\n    - Reduced adjacency matrix of shape `([batch], K, K)`;\n    - If `return_mask=True`, the soft clustering matrix of shape `([batch], N, K)`.\n\n    **Arguments**\n\n    - `k`: number of nodes to keep;\n    - `channels`: number of output channels (if None, the number of output\n    channels is assumed to be the same as the input);\n    - `return_mask`: boolean, whether to return the cluster assignment matrix;\n    - `kernel_initializer`: initializer for the weights;\n    - `kernel_regularizer`: regularization applied to the weights;\n    - `kernel_constraint`: constraint applied to the weights;\n    """"""\n\n    def __init__(self,\n                 k,\n                 channels=None,\n                 return_mask=False,\n                 activation=None,\n                 kernel_initializer=\'glorot_uniform\',\n                 kernel_regularizer=None,\n                 kernel_constraint=None,\n                 **kwargs):\n\n        super().__init__(**kwargs)\n        self.k = k\n        self.channels = channels\n        self.return_mask = return_mask\n        self.activation = activations.get(activation)\n        self.kernel_initializer = initializers.get(kernel_initializer)\n        self.kernel_regularizer = regularizers.get(kernel_regularizer)\n        self.kernel_constraint = constraints.get(kernel_constraint)\n\n    def build(self, input_shape):\n        assert isinstance(input_shape, list)\n        F = input_shape[0][-1]\n\n        if self.channels is None:\n            self.channels = F\n\n        self.kernel_emb = self.add_weight(shape=(F, self.channels),\n                                          name=\'kernel_emb\',\n                                          initializer=self.kernel_initializer,\n                                          regularizer=self.kernel_regularizer,\n                                          constraint=self.kernel_constraint)\n\n        self.kernel_pool = self.add_weight(shape=(F, self.k),\n                                           name=\'kernel_pool\',\n                                           initializer=self.kernel_initializer,\n                                           regularizer=self.kernel_regularizer,\n                                           constraint=self.kernel_constraint)\n\n        super().build(input_shape)\n\n    def call(self, inputs):\n        if len(inputs) == 3:\n            X, A, I = inputs\n            if K.ndim(I) == 2:\n                I = I[:, 0]\n        else:\n            X, A = inputs\n            I = None\n\n        N = K.shape(A)[-1]\n        # Check if the layer is operating in mixed or batch mode\n        mode = ops.autodetect_mode(A, X)\n        self.reduce_loss = mode in (modes.MIXED, modes.BATCH)\n\n        # Get normalized adjacency\n        if K.is_sparse(A):\n            I_ = tf.sparse.eye(N, dtype=A.dtype)\n            A_ = tf.sparse.add(A, I_)\n        else:\n            I_ = tf.eye(N, dtype=A.dtype)\n            A_ = A + I_\n        fltr = ops.normalize_A(A_)\n\n        # Node embeddings\n        Z = K.dot(X, self.kernel_emb)\n        Z = ops.filter_dot(fltr, Z)\n        if self.activation is not None:\n            Z = self.activation(Z)\n\n        # Compute cluster assignment matrix\n        S = K.dot(X, self.kernel_pool)\n        S = ops.filter_dot(fltr, S)\n        S = activations.softmax(S, axis=-1)  # softmax applied row-wise\n\n        # Link prediction loss\n        S_gram = ops.matmul_A_BT(S, S)\n        if mode == modes.MIXED:\n            A = tf.sparse.to_dense(A)[None, ...]\n        if K.is_sparse(A):\n            LP_loss = tf.sparse.add(A, -S_gram)  # A/tf.norm(A) - S_gram/tf.norm(S_gram)\n        else:\n            LP_loss = A - S_gram\n        LP_loss = tf.norm(LP_loss, axis=(-1, -2))\n        if self.reduce_loss:\n            LP_loss = K.mean(LP_loss)\n        self.add_loss(LP_loss)\n\n        # Entropy loss\n        entr = tf.negative(tf.reduce_sum(tf.multiply(S, K.log(S + K.epsilon())), axis=-1))\n        entr_loss = K.mean(entr, axis=-1)\n        if self.reduce_loss:\n            entr_loss = K.mean(entr_loss)\n        self.add_loss(entr_loss)\n\n        # Pooling\n        X_pooled = ops.matmul_AT_B(S, Z)\n        A_pooled = ops.matmul_AT_B_A(S, A)\n\n        output = [X_pooled, A_pooled]\n\n        if I is not None:\n            I_mean = tf.math.segment_mean(I, I)\n            I_pooled = ops.repeat(I_mean, tf.ones_like(I_mean) * self.k)\n            output.append(I_pooled)\n\n        if self.return_mask:\n            output.append(S)\n\n        return output\n\n    def compute_output_shape(self, input_shape):\n        X_shape = input_shape[0]\n        A_shape = input_shape[1]\n        X_shape_out = X_shape[:-2] + (self.k, self.channels)\n        if self.reduce_loss:\n            A_shape_out = X_shape[:-2] + (self.k, self.k)\n        else:\n            A_shape_out = A_shape[:-2] + (self.k, self.k)\n\n        output_shape = [X_shape_out, A_shape_out]\n\n        if len(input_shape) == 3:\n            I_shape_out = A_shape[:-2] + (self.k,)\n            output_shape.append(I_shape_out)\n\n        if self.return_mask:\n            S_shape_out = A_shape[:-1] + (self.k,)\n            output_shape.append(S_shape_out)\n\n        return output_shape\n\n    def get_config(self):\n        config = {\n            \'k\': self.k,\n            \'channels\': self.channels,\n            \'return_mask\': self.return_mask,\n            \'kernel_initializer\': initializers.serialize(self.kernel_initializer),\n            \'kernel_regularizer\': regularizers.serialize(self.kernel_regularizer),\n            \'kernel_constraint\': constraints.serialize(self.kernel_constraint),\n        }\n        base_config = super().get_config()\n        return dict(list(base_config.items()) + list(config.items()))'"
spektral/layers/pooling/global_pool.py,16,"b'import tensorflow as tf\nfrom tensorflow.keras import backend as K, initializers, regularizers, constraints\nfrom tensorflow.keras.layers import Layer, Dense\n\n\nclass GlobalPooling(Layer):\n    def __init__(self, **kwargs):\n\n        super().__init__(**kwargs)\n        self.supports_masking = True\n        self.pooling_op = None\n        self.batch_pooling_op = None\n\n    def build(self, input_shape):\n        if isinstance(input_shape, list) and len(input_shape) == 2:\n            self.data_mode = \'disjoint\'\n        else:\n            if len(input_shape) == 2:\n                self.data_mode = \'single\'\n            else:\n                self.data_mode = \'batch\'\n        super().build(input_shape)\n\n    def call(self, inputs):\n        if self.data_mode == \'disjoint\':\n            X = inputs[0]\n            I = inputs[1]\n            if K.ndim(I) == 2:\n                I = I[:, 0]\n        else:\n            X = inputs\n\n        if self.data_mode == \'disjoint\':\n            return self.pooling_op(X, I)\n        else:\n            return self.batch_pooling_op(X, axis=-2, keepdims=(self.data_mode == \'single\'))\n\n    def compute_output_shape(self, input_shape):\n        if self.data_mode == \'single\':\n            return (1,) + input_shape[-1:]\n        elif self.data_mode == \'batch\':\n            return input_shape[:-2] + input_shape[-1:]\n        else:\n            # Input shape is a list of shapes for X and I\n            return input_shape[0]\n\n    def get_config(self):\n        return super().get_config()\n\n\nclass GlobalSumPool(GlobalPooling):\n    """"""\n    A global sum pooling layer. Pools a graph by computing the sum of its node\n    features.\n\n    **Mode**: single, disjoint, mixed, batch.\n\n    **Input**\n\n    - Node features of shape `([batch], N, F)`;\n    - Graph IDs of shape `(N, )` (only in disjoint mode);\n\n    **Output**\n\n    - Pooled node features of shape `([batch], F)` (if single mode, shape will\n    be `(1, F)`).\n\n    **Arguments**\n\n    None.\n\n    """"""\n\n    def __init__(self, **kwargs):\n        super().__init__(**kwargs)\n        self.pooling_op = tf.math.segment_sum\n        self.batch_pooling_op = tf.reduce_sum\n\n\nclass GlobalAvgPool(GlobalPooling):\n    """"""\n    An average pooling layer. Pools a graph by computing the average of its node\n    features.\n\n    **Mode**: single, disjoint, mixed, batch.\n\n    **Input**\n\n    - Node features of shape `([batch], N, F)`;\n    - Graph IDs of shape `(N, )` (only in disjoint mode);\n\n    **Output**\n\n    - Pooled node features of shape `([batch], F)` (if single mode, shape will\n    be `(1, F)`).\n\n    **Arguments**\n\n    None.\n\n    """"""\n\n    def __init__(self, **kwargs):\n        super().__init__(**kwargs)\n        self.pooling_op = tf.math.segment_mean\n        self.batch_pooling_op = tf.reduce_mean\n\n\nclass GlobalMaxPool(GlobalPooling):\n    """"""\n    A max pooling layer. Pools a graph by computing the maximum of its node\n    features.\n\n    **Mode**: single, disjoint, mixed, batch.\n\n    **Input**\n\n    - Node features of shape `([batch], N, F)`;\n    - Graph IDs of shape `(N, )` (only in disjoint mode);\n\n    **Output**\n\n    - Pooled node features of shape `([batch], F)` (if single mode, shape will\n    be `(1, F)`).\n\n    **Arguments**\n\n    None.\n\n    """"""\n\n    def __init__(self, **kwargs):\n        super().__init__(**kwargs)\n        self.pooling_op = tf.math.segment_max\n        self.batch_pooling_op = tf.reduce_max\n\n\nclass GlobalAttentionPool(GlobalPooling):\n    r""""""\n    A gated attention global pooling layer as presented by\n    [Li et al. (2017)](https://arxiv.org/abs/1511.05493).\n\n    This layer computes:\n    $$\n        \\X\' = \\sum\\limits_{i=1}^{N} (\\sigma(\\X \\W_1 + \\b_1) \\odot (\\X \\W_2 + \\b_2))_i\n    $$\n    where \\(\\sigma\\) is the sigmoid activation function.\n\n    **Mode**: single, disjoint, mixed, batch.\n\n    **Input**\n\n    - Node features of shape `([batch], N, F)`;\n    - Graph IDs of shape `(N, )` (only in disjoint mode);\n\n    **Output**\n\n    - Pooled node features of shape `([batch], channels)` (if single mode,\n    shape will be `(1, channels)`).\n\n    **Arguments**\n\n    - `channels`: integer, number of output channels;\n    - `bias_initializer`: initializer for the bias vectors;\n    - `kernel_regularizer`: regularization applied to the kernel matrices;\n    - `bias_regularizer`: regularization applied to the bias vectors;\n    - `kernel_constraint`: constraint applied to the kernel matrices;\n    - `bias_constraint`: constraint applied to the bias vectors.\n    """"""\n\n    def __init__(self,\n                 channels,\n                 kernel_initializer=\'glorot_uniform\',\n                 bias_initializer=\'zeros\',\n                 kernel_regularizer=None,\n                 bias_regularizer=None,\n                 kernel_constraint=None,\n                 bias_constraint=None,\n                 **kwargs):\n        super().__init__(**kwargs)\n        self.channels = channels\n        self.kernel_initializer = initializers.get(kernel_initializer)\n        self.bias_initializer = initializers.get(bias_initializer)\n        self.kernel_regularizer = regularizers.get(kernel_regularizer)\n        self.bias_regularizer = regularizers.get(bias_regularizer)\n        self.kernel_constraint = constraints.get(kernel_constraint)\n        self.bias_constraint = constraints.get(bias_constraint)\n\n    def build(self, input_shape):\n        super().build(input_shape)\n        layer_kwargs = dict(\n            kernel_initializer=self.kernel_initializer,\n            bias_initializer=self.bias_initializer,\n            kernel_regularizer=self.kernel_regularizer,\n            bias_regularizer=self.bias_regularizer,\n            kernel_constraint=self.kernel_constraint,\n            bias_constraint=self.bias_constraint\n        )\n        self.features_layer = Dense(self.channels,\n                                    name=\'features_layer\',\n                                    **layer_kwargs)\n        self.attention_layer = Dense(self.channels,\n                                     activation=\'sigmoid\',\n                                     name=\'attn_layer\',\n                                     **layer_kwargs)\n        self.built = True\n\n    def call(self, inputs):\n        if self.data_mode == \'disjoint\':\n            X, I = inputs\n            if K.ndim(I) == 2:\n                I = I[:, 0]\n        else:\n            X = inputs\n        inputs_linear = self.features_layer(X)\n        attn = self.attention_layer(X)\n        masked_inputs = inputs_linear * attn\n        if self.data_mode in {\'single\', \'batch\'}:\n            output = K.sum(masked_inputs, axis=-2,\n                           keepdims=self.data_mode == \'single\')\n        else:\n            output = tf.math.segment_sum(masked_inputs, I)\n\n        return output\n\n    def compute_output_shape(self, input_shape):\n        if self.data_mode == \'single\':\n            return (1,) + (self.channels,)\n        elif self.data_mode == \'batch\':\n            return input_shape[:-2] + (self.channels,)\n        else:\n            output_shape = input_shape[0]\n            output_shape = output_shape[:-1] + (self.channels,)\n            return output_shape\n\n    def get_config(self):\n        config = {\n            \'channels\': self.channels,\n            \'kernel_initializer\': self.kernel_initializer,\n            \'bias_initializer\': self.bias_initializer,\n            \'kernel_regularizer\': self.kernel_regularizer,\n            \'bias_regularizer\': self.bias_regularizer,\n            \'kernel_constraint\': self.kernel_constraint,\n            \'bias_constraint\': self.bias_constraint,\n        }\n        base_config = super().get_config()\n        return dict(list(base_config.items()) + list(config.items()))\n\n\nclass GlobalAttnSumPool(GlobalPooling):\n    r""""""\n    A node-attention global pooling layer. Pools a graph by learning attention\n    coefficients to sum node features.\n\n    This layer computes:\n    $$\n        \\alpha = \\textrm{softmax}( \\X \\a); \\\\\n        \\X\' = \\sum\\limits_{i=1}^{N} \\alpha_i \\cdot \\X_i\n    $$\n    where \\(\\a \\in \\mathbb{R}^F\\) is a trainable vector. Note that the softmax\n    is applied across nodes, and not across features.\n\n    **Mode**: single, disjoint, mixed, batch.\n\n    **Input**\n\n    - Node features of shape `([batch], N, F)`;\n    - Graph IDs of shape `(N, )` (only in disjoint mode);\n\n    **Output**\n\n    - Pooled node features of shape `([batch], F)` (if single mode, shape will\n    be `(1, F)`).\n\n    **Arguments**\n\n    - `attn_kernel_initializer`: initializer for the attention weights;\n    - `attn_kernel_regularizer`: regularization applied to the attention kernel\n    matrix;\n    - `attn_kernel_constraint`: constraint applied to the attention kernel\n    matrix;\n    """"""\n\n    def __init__(self,\n                 attn_kernel_initializer=\'glorot_uniform\',\n                 attn_kernel_regularizer=None,\n                 attn_kernel_constraint=None,\n                 **kwargs):\n        super().__init__(**kwargs)\n        self.attn_kernel_initializer = initializers.get(\n            attn_kernel_initializer)\n        self.attn_kernel_regularizer = regularizers.get(\n            attn_kernel_regularizer)\n        self.attn_kernel_constraint = constraints.get(attn_kernel_constraint)\n\n    def build(self, input_shape):\n        assert len(input_shape) >= 2\n        if isinstance(input_shape, list) and len(input_shape) == 2:\n            self.data_mode = \'disjoint\'\n            F = input_shape[0][-1]\n        else:\n            if len(input_shape) == 2:\n                self.data_mode = \'single\'\n            else:\n                self.data_mode = \'batch\'\n            F = input_shape[-1]\n        # Attention kernels\n        self.attn_kernel = self.add_weight(shape=(F, 1),\n                                           initializer=self.attn_kernel_initializer,\n                                           regularizer=self.attn_kernel_regularizer,\n                                           constraint=self.attn_kernel_constraint,\n                                           name=\'attn_kernel\')\n        self.built = True\n\n    def call(self, inputs):\n        if self.data_mode == \'disjoint\':\n            X, I = inputs\n            if K.ndim(I) == 2:\n                I = I[:, 0]\n        else:\n            X = inputs\n        attn_coeff = K.dot(X, self.attn_kernel)\n        attn_coeff = K.squeeze(attn_coeff, -1)\n        attn_coeff = K.softmax(attn_coeff)\n        if self.data_mode == \'single\':\n            output = K.dot(attn_coeff[None, ...], X)\n        elif self.data_mode == \'batch\':\n            output = K.batch_dot(attn_coeff, X)\n        else:\n            output = attn_coeff[:, None] * X\n            output = tf.math.segment_sum(output, I)\n\n        return output\n\n    def get_config(self):\n        config = {\n            \'attn_kernel_initializer\': self.attn_kernel_initializer,\n            \'attn_kernel_regularizer\': self.attn_kernel_regularizer,\n            \'attn_kernel_constraint\': self.attn_kernel_constraint,\n        }\n        base_config = super().get_config()\n        return dict(list(base_config.items()) + list(config.items()))\n\n\nclass SortPool(Layer):\n    r""""""\n    SortPool layer pooling the top \\(k\\) most relevant nodes as described by\n    [Zhang et al.](https://www.cse.wustl.edu/~muhan/papers/AAAI_2018_DGCNN.pdf)\n    This layers takes a graph signal \\(\\mathbf{X}\\) and sorts the rows by the\n    elements of its last column. It then keeps the top \\(k\\) rows.\n    Should \\(\\mathbf{X}\\) have less than \\(k\\) rows, it sorts and then adds\n    rows full of zeros until \\(\\mathbf{X}\\) has \\(k\\) rows.\n\n    **Mode**: single, batch.\n\n    **Input**\n\n    - Node features of shape `([batch], N, F)`;\n\n    **Output**\n\n    - Pooled node features of shape `([batch], k, F)`;\n\n    **Arguments**\n    - `k`: number of nodes to keep;\n    """"""\n\n    def __init__(self, k: int):\n        super(SortPool, self).__init__()\n\n        # Number of nodes to be kept (k in paper)\n        k = int(k)\n        if k <= 0:\n            raise ValueError(""K must be a positive integer"")\n        self.k = k\n\n    def build(self, input_shape):\n\n        # check what mode we are in\n        if isinstance(input_shape, list) and len(input_shape) == 2:\n            raise NotImplementedError(""Disjoint mode not supported."")\n        else:\n            if len(input_shape) == 2:\n                self.data_mode = \'single\'\n            else:\n                self.data_mode = \'batch\'\n\n        # store number of features\n        self.F = input_shape[-1]\n\n    def call(self, inputs):\n\n        # Takes the (ideally concatenated & convolved) graph signal X\n        X = inputs\n\n        # turn to trivial batch if in ""single"" mode\n        # (N, F) -> (1, N, F)\n        if self.data_mode == \'single\':\n            X = tf.expand_dims(X, 0)\n\n        # get number of nodes\n        n = tf.shape(X)[-2]\n\n        # Sort last column and return permutation of indices\n        sort_perm = tf.argsort(X[..., -1], direction=\'DESCENDING\')\n\n        # Gather rows according to the sorting permutation\n        # thus sorting the rows according to the last column\n        X_sorted = tf.gather(X, sort_perm, axis=-2, batch_dims=1)\n\n        # cast X_sorted into float32\n        X_sorted = tf.cast(X_sorted, tf.float32)\n\n        def truncate():\n            """"""If we have more nodes than we want to keep,\n            then we simply truncate.\n            """"""\n\n            # trim number of nodes to k if k < n\n            X_out = X_sorted[..., : self.k, :]\n\n            return X_out\n\n        def pad():\n            """"""If we have less nodes than we would like to keep,\n            then we simply pad with empty nodes.\n            """"""\n\n            padding = [[0, 0], [0, self.k - n], [0, 0]]\n\n            # padded output\n            X_out = tf.pad(X_sorted, padding)\n\n            return X_out\n\n        X_out = tf.cond(tf.less_equal(self.k, n), truncate, pad)\n\n        # undo trivial batching if in ""single"" mode\n        if self.data_mode == \'single\':\n            X_out = tf.squeeze(X_out, [0])\n\n            # set shape manually, as tf is not able\n            # to infer the dimensions\n            X_out.set_shape((self.k, self.F))\n            return X_out\n\n        elif self.data_mode == \'batch\':\n\n            # set shape manually, as tf is not able\n            # to infer the dimensions\n            X_out.set_shape((None, self.k, self.F))\n            return X_out\n\n    def get_config(self):\n        config = {\n            \'k\': self.k\n        }\n        base_config = super().get_config()\n        return dict(list(base_config.items()) + list(config.items()))\n\n    def compute_output_shape(self, input_shape):\n        if self.data_mode == \'single\':\n            return (self.k, self.F)\n        elif self.data_mode == \'batch\':\n            return (input_shape[0], self.k, self.F)'"
spektral/layers/pooling/mincut_pool.py,9,"b'import tensorflow as tf\nfrom tensorflow.keras import Sequential\nfrom tensorflow.keras import activations, initializers, regularizers, constraints, backend as K\nfrom tensorflow.keras.layers import Layer, Dense\n\nfrom spektral.layers import ops\n\n\nclass MinCutPool(Layer):\n    r""""""\n    A minCUT pooling layer as presented by\n    [Bianchi et al. (2019)](https://arxiv.org/abs/1907.00481).\n\n    **Mode**: batch.\n\n    This layer computes a soft clustering \\(\\S\\) of the input graphs using a MLP,\n    and reduces graphs as follows:\n\n    $$\n        \\S = \\textrm{MLP}(\\X); \\\\\n        \\A\' = \\S^\\top \\A \\S; \\X\' = \\S^\\top \\X;\n    $$\n\n    where MLP is a multi-layer perceptron with softmax output.\n    Two auxiliary loss terms are also added to the model: the _minCUT loss_\n    $$\n        - \\frac{ \\mathrm{Tr}(\\S^\\top \\A \\S) }{ \\mathrm{Tr}(\\S^\\top \\D \\S) }\n    $$\n    and the _orthogonality loss_\n    $$\n        \\left\\|\n            \\frac{\\S^\\top \\S}{\\| \\S^\\top \\S \\|_F}\n            - \\frac{\\I_K}{\\sqrt{K}}\n        \\right\\|_F.\n    $$\n\n    The layer can be used without a supervised loss, to compute node clustering\n    simply by minimizing the two auxiliary losses.\n\n    **Input**\n\n    - Node features of shape `([batch], N, F)`;\n    - Binary adjacency matrix of shape `([batch], N, N)`;\n\n    **Output**\n\n    - Reduced node features of shape `([batch], K, F)`;\n    - Reduced adjacency matrix of shape `([batch], K, K)`;\n    - If `return_mask=True`, the soft clustering matrix of shape `([batch], N, K)`.\n\n    **Arguments**\n\n    - `k`: number of nodes to keep;\n    - `mlp_hidden`: list of integers, number of hidden units for each hidden\n    layer in the MLP used to compute cluster assignments (if None, the MLP has\n    only the output layer);\n    - `mlp_activation`: activation for the MLP layers;\n    - `return_mask`: boolean, whether to return the cluster assignment matrix;\n    - `kernel_initializer`: initializer for the weights;\n    - `kernel_regularizer`: regularization applied to the weights;\n    - `kernel_constraint`: constraint applied to the weights;\n    """"""\n\n    def __init__(self,\n                 k,\n                 mlp_hidden=None,\n                 mlp_activation=\'relu\',\n                 return_mask=False,\n                 activation=None,\n                 use_bias=True,\n                 kernel_initializer=\'glorot_uniform\',\n                 bias_initializer=\'zeros\',\n                 kernel_regularizer=None,\n                 bias_regularizer=None,\n                 kernel_constraint=None,\n                 bias_constraint=None,\n                 **kwargs):\n\n        super().__init__(**kwargs)\n        self.k = k\n        self.mlp_hidden = mlp_hidden if mlp_hidden else []\n        self.mlp_activation = mlp_activation\n        self.return_mask = return_mask\n        self.activation = activations.get(activation)\n        self.use_bias = use_bias\n        self.kernel_initializer = initializers.get(kernel_initializer)\n        self.bias_initializer = initializers.get(bias_initializer)\n        self.kernel_regularizer = regularizers.get(kernel_regularizer)\n        self.bias_regularizer = regularizers.get(bias_regularizer)\n        self.kernel_constraint = constraints.get(kernel_constraint)\n        self.bias_constraint = constraints.get(bias_constraint)\n\n    def build(self, input_shape):\n        assert isinstance(input_shape, list)\n        layer_kwargs = dict(\n            kernel_initializer=self.kernel_initializer,\n            bias_initializer=self.bias_initializer,\n            kernel_regularizer=self.kernel_regularizer,\n            bias_regularizer=self.bias_regularizer,\n            kernel_constraint=self.kernel_constraint,\n            bias_constraint=self.bias_constraint\n        )\n        mlp_layers = []\n        for i, channels in enumerate(self.mlp_hidden):\n            mlp_layers.append(\n                Dense(channels, self.mlp_activation, **layer_kwargs)\n            )\n        mlp_layers.append(\n            Dense(self.k, \'softmax\', **layer_kwargs)\n        )\n        self.mlp = Sequential(mlp_layers)\n\n        super().build(input_shape)\n\n    def call(self, inputs):\n        if len(inputs) == 3:\n            X, A, I = inputs\n            if K.ndim(I) == 2:\n                I = I[:, 0]\n        else:\n            X, A = inputs\n            I = None\n\n        # Check if the layer is operating in batch mode (X and A have rank 3)\n        batch_mode = K.ndim(X) == 3\n\n        # Compute cluster assignment matrix\n        S = self.mlp(X)\n\n        # MinCut regularization\n        A_pooled = ops.matmul_AT_B_A(S, A)\n        num = tf.linalg.trace(A_pooled)\n        D = ops.degree_matrix(A)\n        den = tf.linalg.trace(ops.matmul_AT_B_A(S, D)) + K.epsilon()\n        cut_loss = -(num / den)\n        if batch_mode:\n            cut_loss = K.mean(cut_loss)\n        self.add_loss(cut_loss)\n\n        # Orthogonality regularization\n        SS = ops.matmul_AT_B(S, S)\n        I_S = tf.eye(self.k, dtype=SS.dtype)\n        ortho_loss = tf.norm(\n            SS / tf.norm(SS, axis=(-1, -2), keepdims=True) - I_S / tf.norm(I_S),\n            axis=(-1, -2)\n        )\n        if batch_mode:\n            ortho_loss = K.mean(ortho_loss)\n        self.add_loss(ortho_loss)\n\n        # Pooling\n        X_pooled = ops.matmul_AT_B(S, X)\n        A_pooled = tf.linalg.set_diag(\n            A_pooled, tf.zeros(K.shape(A_pooled)[:-1], dtype=A_pooled.dtype)\n        )  # Remove diagonal\n        A_pooled = ops.normalize_A(A_pooled)\n\n        output = [X_pooled, A_pooled]\n\n        if I is not None:\n            I_mean = tf.math.segment_mean(I, I)\n            I_pooled = ops.repeat(I_mean, tf.ones_like(I_mean) * self.k)\n            output.append(I_pooled)\n\n        if self.return_mask:\n            output.append(S)\n\n        return output\n\n    def compute_output_shape(self, input_shape):\n        X_shape = input_shape[0]\n        A_shape = input_shape[1]\n        X_shape_out = X_shape[:-2] + (self.k,) + X_shape[-1:]\n        A_shape_out = A_shape[:-2] + (self.k, self.k)\n\n        output_shape = [X_shape_out, A_shape_out]\n\n        if len(input_shape) == 3:\n            I_shape_out = A_shape[:-2] + (self.k, )\n            output_shape.append(I_shape_out)\n\n        if self.return_mask:\n            S_shape_out = A_shape[:-1] + (self.k, )\n            output_shape.append(S_shape_out)\n\n        return output_shape\n\n    def get_config(self):\n        config = {\n            \'k\': self.k,\n            \'mlp_hidden\': self.mlp_hidden,\n            \'mlp_activation\': self.mlp_activation,\n            \'return_mask\': self.return_mask,\n            \'kernel_initializer\': initializers.serialize(self.kernel_initializer),\n            \'bias_initializer\': initializers.serialize(self.bias_initializer),\n            \'kernel_regularizer\': regularizers.serialize(self.kernel_regularizer),\n            \'bias_regularizer\': regularizers.serialize(self.bias_regularizer),\n            \'kernel_constraint\': constraints.serialize(self.kernel_constraint),\n            \'bias_constraint\': constraints.serialize(self.bias_constraint)\n        }\n        base_config = super().get_config()\n        return dict(list(base_config.items()) + list(config.items()))'"
spektral/layers/pooling/sag_pool.py,0,"b'from tensorflow.keras import backend as K\n\nfrom spektral.layers.pooling.topk_pool import ops, TopKPool\n\n\nclass SAGPool(TopKPool):\n    r""""""\n    A self-attention graph pooling layer as presented by\n    [Lee et al. (2019)](https://arxiv.org/abs/1904.08082).\n\n    **Mode**: single, disjoint.\n\n    This layer computes the following operations:\n\n    $$\n    \\y = \\textrm{GNN}(\\A, \\X); \\;\\;\\;\\;\n    \\i = \\textrm{rank}(\\y, K); \\;\\;\\;\\;\n    \\X\' = (\\X \\odot \\textrm{tanh}(\\y))_\\i; \\;\\;\\;\\;\n    \\A\' = \\A_{\\i, \\i}\n    $$\n\n    where \\( \\textrm{rank}(\\y, K) \\) returns the indices of the top K values of\n    \\(\\y\\), and \\(\\textrm{GNN}\\) consists of one GraphConv layer with no\n    activation. \\(K\\) is defined for each graph as a fraction of the number of\n    nodes.\n\n    This layer temporarily makes the adjacency matrix dense in order to compute\n    \\(\\A\'\\).\n    If memory is not an issue, considerable speedups can be achieved by using\n    dense graphs directly.\n    Converting a graph from sparse to dense and back to sparse is an expensive\n    operation.\n\n    **Input**\n\n    - Node features of shape `(N, F)`;\n    - Binary adjacency matrix of shape `(N, N)`;\n    - Graph IDs of shape `(N, )` (only in disjoint mode);\n\n    **Output**\n\n    - Reduced node features of shape `(ratio * N, F)`;\n    - Reduced adjacency matrix of shape `(ratio * N, ratio * N)`;\n    - Reduced graph IDs of shape `(ratio * N, )` (only in disjoint mode);\n    - If `return_mask=True`, the binary pooling mask of shape `(ratio * N, )`.\n\n    **Arguments**\n\n    - `ratio`: float between 0 and 1, ratio of nodes to keep in each graph;\n    - `return_mask`: boolean, whether to return the binary mask used for pooling;\n    - `sigmoid_gating`: boolean, use a sigmoid gating activation instead of a\n        tanh;\n    - `kernel_initializer`: initializer for the weights;\n    - `kernel_regularizer`: regularization applied to the weights;\n    - `kernel_constraint`: constraint applied to the weights;\n    """"""\n\n    def __init__(self,\n                 ratio,\n                 return_mask=False,\n                 sigmoid_gating=False,\n                 kernel_initializer=\'glorot_uniform\',\n                 kernel_regularizer=None,\n                 kernel_constraint=None,\n                 **kwargs):\n        super().__init__(ratio,\n                         return_mask=return_mask,\n                         sigmoid_gating=sigmoid_gating,\n                         kernel_initializer=kernel_initializer,\n                         kernel_regularizer=kernel_regularizer,\n                         kernel_constraint=kernel_constraint,\n                         **kwargs)\n\n    def compute_scores(self, X, A, I):\n        scores = K.dot(X, self.kernel)\n        scores = ops.filter_dot(A, scores)\n        return scores'"
spektral/layers/pooling/topk_pool.py,12,"b'import tensorflow as tf\nfrom tensorflow.keras import backend as K, initializers, regularizers, constraints\nfrom tensorflow.keras.layers import Layer\n\nfrom spektral.layers import ops\n\n\nclass TopKPool(Layer):\n    r""""""\n    A gPool/Top-K layer as presented by\n    [Gao & Ji (2019)](http://proceedings.mlr.press/v97/gao19a/gao19a.pdf) and\n    [Cangea et al. (2018)](https://arxiv.org/abs/1811.01287).\n\n    **Mode**: single, disjoint.\n\n    This layer computes the following operations:\n\n    $$\n    \\y = \\frac{\\X\\p}{\\|\\p\\|}; \\;\\;\\;\\;\n    \\i = \\textrm{rank}(\\y, K); \\;\\;\\;\\;\n    \\X\' = (\\X \\odot \\textrm{tanh}(\\y))_\\i; \\;\\;\\;\\;\n    \\A\' = \\A_{\\i, \\i}\n    $$\n\n    where \\( \\textrm{rank}(\\y, K) \\) returns the indices of the top K values of\n    \\(\\y\\), and \\(\\p\\) is a learnable parameter vector of size \\(F\\). \\(K\\) is\n    defined for each graph as a fraction of the number of nodes.\n    Note that the the gating operation \\(\\textrm{tanh}(\\y)\\) (Cangea et al.)\n    can be replaced with a sigmoid (Gao & Ji).\n\n    This layer temporarily makes the adjacency matrix dense in order to compute\n    \\(\\A\'\\).\n    If memory is not an issue, considerable speedups can be achieved by using\n    dense graphs directly.\n    Converting a graph from sparse to dense and back to sparse is an expensive\n    operation.\n\n    **Input**\n\n    - Node features of shape `(N, F)`;\n    - Binary adjacency matrix of shape `(N, N)`;\n    - Graph IDs of shape `(N, )` (only in disjoint mode);\n\n    **Output**\n\n    - Reduced node features of shape `(ratio * N, F)`;\n    - Reduced adjacency matrix of shape `(ratio * N, ratio * N)`;\n    - Reduced graph IDs of shape `(ratio * N, )` (only in disjoint mode);\n    - If `return_mask=True`, the binary pooling mask of shape `(ratio * N, )`.\n\n    **Arguments**\n\n    - `ratio`: float between 0 and 1, ratio of nodes to keep in each graph;\n    - `return_mask`: boolean, whether to return the binary mask used for pooling;\n    - `sigmoid_gating`: boolean, use a sigmoid gating activation instead of a\n        tanh;\n    - `kernel_initializer`: initializer for the weights;\n    - `kernel_regularizer`: regularization applied to the weights;\n    - `kernel_constraint`: constraint applied to the weights;\n    """"""\n\n    def __init__(self,\n                 ratio,\n                 return_mask=False,\n                 sigmoid_gating=False,\n                 kernel_initializer=\'glorot_uniform\',\n                 kernel_regularizer=None,\n                 kernel_constraint=None,\n                 **kwargs):\n        super().__init__(**kwargs)\n        self.ratio = ratio\n        self.return_mask = return_mask\n        self.sigmoid_gating = sigmoid_gating\n        self.gating_op = K.sigmoid if self.sigmoid_gating else K.tanh\n        self.kernel_initializer = initializers.get(kernel_initializer)\n        self.kernel_regularizer = regularizers.get(kernel_regularizer)\n        self.kernel_constraint = constraints.get(kernel_constraint)\n\n    def build(self, input_shape):\n        self.F = input_shape[0][-1]\n        self.N = input_shape[0][0]\n        self.kernel = self.add_weight(shape=(self.F, 1),\n                                      name=\'kernel\',\n                                      initializer=self.kernel_initializer,\n                                      regularizer=self.kernel_regularizer,\n                                      constraint=self.kernel_constraint)\n        self.top_k_var = tf.Variable(0.0,\n                                     trainable=False,\n                                     validate_shape=False,\n                                     dtype=tf.keras.backend.floatx(),\n                                     shape=tf.TensorShape(None))\n        super().build(input_shape)\n\n    def call(self, inputs):\n        if len(inputs) == 3:\n            X, A, I = inputs\n            self.data_mode = \'disjoint\'\n        else:\n            X, A = inputs\n            I = tf.zeros(tf.shape(X)[:1])\n            self.data_mode = \'single\'\n        if K.ndim(I) == 2:\n            I = I[:, 0]\n        I = tf.cast(I, tf.int32)\n\n        A_is_sparse = K.is_sparse(A)\n\n        # Get mask\n        y = self.compute_scores(X, A, I)\n        N = K.shape(X)[-2]\n        indices = ops.segment_top_k(y[:, 0], I, self.ratio, self.top_k_var)\n        mask = tf.scatter_nd(tf.expand_dims(indices, 1), tf.ones_like(indices), (N,))\n\n        # Multiply X and y to make layer differentiable\n        features = X * self.gating_op(y)\n\n        axis = 0 if len(K.int_shape(A)) == 2 else 1  # Cannot use negative axis in tf.boolean_mask\n        # Reduce X\n        X_pooled = tf.boolean_mask(features, mask, axis=axis)\n\n        # Reduce A\n        A_dense = tf.sparse.to_dense(A) if A_is_sparse else A\n        A_pooled = tf.boolean_mask(A_dense, mask, axis=axis)\n        A_pooled = tf.boolean_mask(A_pooled, mask, axis=axis + 1)\n        if A_is_sparse:\n            A_pooled = ops.dense_to_sparse(A_pooled)\n\n        output = [X_pooled, A_pooled]\n\n        # Reduce I\n        if self.data_mode == \'disjoint\':\n            I_pooled = tf.boolean_mask(I[:, None], mask)[:, 0]\n            output.append(I_pooled)\n\n        if self.return_mask:\n            output.append(mask)\n\n        return output\n\n    def compute_scores(self, X, A, I):\n        return K.dot(X, K.l2_normalize(self.kernel))\n\n    def compute_output_shape(self, input_shape):\n        output_shape = input_shape\n        if self.return_mask:\n            output_shape += [(input_shape[0][:-1])]\n        return output_shape\n\n    def get_config(self):\n        config = {\n            \'ratio\': self.ratio,\n            \'return_mask\': self.return_mask,\n            \'kernel_initializer\': initializers.serialize(self.kernel_initializer),\n            \'kernel_regularizer\': regularizers.serialize(self.kernel_regularizer),\n            \'kernel_constraint\': constraints.serialize(self.kernel_constraint),\n        }\n        base_config = super().get_config()\n        return dict(list(base_config.items()) + list(config.items()))'"
