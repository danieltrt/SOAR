file_path,api_count,code
examples/multi_layer_perceptron.py,0,"b'import matplotlib.pyplot as plt\nimport numpy as np\nimport tensorslow as ts\n\n# Create two clusters of red points centered at (0, 0) and (1, 1), respectively.\nred_points = np.concatenate((\n    0.2*np.random.randn(25, 2) + np.array([[0, 0]]*25),\n    0.2*np.random.randn(25, 2) + np.array([[1, 1]]*25)\n))\n\n# Create two clusters of blue points centered at (0, 1) and (1, 0), respectively.\nblue_points = np.concatenate((\n    0.2*np.random.randn(25, 2) + np.array([[0, 1]]*25),\n    0.2*np.random.randn(25, 2) + np.array([[1, 0]]*25)\n))\n\n# Plot them\nplt.scatter(red_points[:,0], red_points[:,1], color=\'red\')\nplt.scatter(blue_points[:,0], blue_points[:,1], color=\'blue\')\nplt.show()\n\n# Create a new graph\nts.Graph().as_default()\n\n# Create training input placeholder\nX = ts.placeholder()\n\n# Create placeholder for the training classes\nc = ts.placeholder()\n\n# Build a hidden layer\nW_hidden = ts.Variable(np.random.randn(2, 2))\nb_hidden = ts.Variable(np.random.randn(2))\np_hidden = ts.sigmoid(ts.add(ts.matmul(X, W_hidden), b_hidden))\n\n# Build the output layer\nW_output = ts.Variable(np.random.randn(2, 2))\nb_output = ts.Variable(np.random.randn(2))\np_output = ts.softmax(ts.add(ts.matmul(p_hidden, W_output), b_output))\n\n# Build cross-entropy loss\nJ = ts.negative(ts.reduce_sum(ts.reduce_sum(ts.multiply(c, ts.log(p_output)), axis=1)))\n\n# Build minimization op\nminimization_op = ts.train.GradientDescentOptimizer(learning_rate=0.03).minimize(J)\n\n# Build placeholder inputs\nfeed_dict = {\n    X: np.concatenate((blue_points, red_points)),\n    c:\n        [[1, 0]] * len(blue_points)\n        + [[0, 1]] * len(red_points)\n\n}\n\n# Create session\nsession = ts.Session()\n\n# Perform 100 gradient descent steps\nfor step in range(1000):\n    J_value = session.run(J, feed_dict)\n    if step % 100 == 0:\n        print(""Step:"", step, "" Loss:"", J_value)\n    session.run(minimization_op, feed_dict)\n\n# Print final result\nW_hidden_value = session.run(W_hidden)\nprint(""Hidden layer weight matrix:\\n"", W_hidden_value)\nb_hidden_value = session.run(b_hidden)\nprint(""Hidden layer bias:\\n"", b_hidden_value)\nW_output_value = session.run(W_output)\nprint(""Output layer weight matrix:\\n"", W_output_value)\nb_output_value = session.run(b_output)\nprint(""Output layer bias:\\n"", b_output_value)\n\n# Visualize classification boundary\nxs = np.linspace(-2, 2)\nys = np.linspace(-2, 2)\npred_classes = []\nfor x in xs:\n    for y in ys:\n        pred_class = session.run(p_output,\n                              feed_dict={X: [[x, y]]})[0]\n        pred_classes.append((x, y, pred_class.argmax()))\nxs_p, ys_p = [], []\nxs_n, ys_n = [], []\nfor x, y, c in pred_classes:\n    if c == 0:\n        xs_n.append(x)\n        ys_n.append(y)\n    else:\n        xs_p.append(x)\n        ys_p.append(y)\nplt.plot(xs_p, ys_p, \'ro\', xs_n, ys_n, \'bo\')\nplt.show()'"
examples/perceptron.py,0,"b'import matplotlib.pyplot as plt\nimport numpy as np\nimport tensorslow as ts\n\n# Create red points centered at (-2, -2)\nred_points = np.random.randn(50, 2) - 2*np.ones((50, 2))\n\n# Create blue points centered at (2, 2)\nblue_points = np.random.randn(50, 2) + 2*np.ones((50, 2))\n\n# Create a new graph\nts.Graph().as_default()\n\nX = ts.placeholder()\nc = ts.placeholder()\n\n# Initialize weights randomly\nW = ts.Variable(np.random.randn(2, 2))\nb = ts.Variable(np.random.randn(2))\n\n# Build perceptron\np = ts.softmax(ts.add(ts.matmul(X, W), b))\n\n# Build cross-entropy loss\nJ = ts.negative(ts.reduce_sum(ts.reduce_sum(ts.multiply(c, ts.log(p)), axis=1)))\n\n# Build minimization op\nminimization_op = ts.train.GradientDescentOptimizer(learning_rate=0.01).minimize(J)\n\n# Build placeholder inputs\nfeed_dict = {\n    X: np.concatenate((blue_points, red_points)),\n    c:\n        [[1, 0]] * len(blue_points)\n        + [[0, 1]] * len(red_points)\n\n}\n\n# Create session\nsession = ts.Session()\n\n# Perform 100 gradient descent steps\nfor step in range(100):\n    J_value = session.run(J, feed_dict)\n    if step % 10 == 0:\n        print(""Step:"", step, "" Loss:"", J_value)\n    session.run(minimization_op, feed_dict)\n\n# Print final result\nW_value = session.run(W)\nprint(""Weight matrix:\\n"", W_value)\nb_value = session.run(b)\nprint(""Bias:\\n"", b_value)\n\n# Plot a line y = -x\nx_axis = np.linspace(-4, 4, 100)\ny_axis = -W_value[0][0]/W_value[1][0] * x_axis - b_value[0]/W_value[1][0]\nplt.plot(x_axis, y_axis)\n\n# Add the red and blue points\nplt.scatter(red_points[:,0], red_points[:,1], color=\'red\')\nplt.scatter(blue_points[:,0], blue_points[:,1], color=\'blue\')\nplt.show()'"
tensorslow/__init__.py,0,b'from tensorslow.graph import Operation\nfrom tensorslow.graph import Variable\nfrom tensorslow.graph import placeholder\nfrom tensorslow.graph import Graph\nfrom tensorslow.operations import *\nfrom tensorslow.gradients import RegisterGradient\nfrom tensorslow.session import Session\nimport tensorslow.train'
tensorslow/gradients.py,0,"b'import numpy as np\nfrom tensorslow.operations import *\n\n# A dictionary that will map operations to gradient functions\n_gradient_registry = {}\n\n\nclass RegisterGradient:\n    """"""A decorator for registering the gradient function for an op type.\n    """"""\n\n    def __init__(self, op_type):\n        """"""Creates a new decorator with `op_type` as the Operation type.\n        Args:\n          op_type: The name of an operation\n        """"""\n        try:\n            self._op_type = eval(op_type)\n        except:\n            self._op_type = eval(op_type)\n\n    def __call__(self, f):\n        """"""Registers the function `f` as gradient function for `op_type`.""""""\n        _gradient_registry[self._op_type] = f\n        return f\n\n\n@RegisterGradient(""add"")\ndef _add_gradient(op, grad):\n    """"""Computes the gradients for `add`.\n\n    Args:\n      op: The `add` `Operation` that we are differentiating\n      grad: Gradient with respect to the output of the `add` op.\n\n    Returns:\n      Gradients with respect to the input of `add`.\n    """"""\n    a = op.inputs[0]\n    b = op.inputs[1]\n\n    grad_wrt_a = grad\n    while np.ndim(grad_wrt_a) > len(a.shape):\n        grad_wrt_a = np.sum(grad_wrt_a, axis=0)\n    for axis, size in enumerate(a.shape):\n        if size == 1:\n            grad_wrt_a = np.sum(grad_wrt_a, axis=axis, keepdims=True)\n\n    grad_wrt_b = grad\n    while np.ndim(grad_wrt_b) > len(b.shape):\n        grad_wrt_b = np.sum(grad_wrt_b, axis=0)\n    for axis, size in enumerate(b.shape):\n        if size == 1:\n            grad_wrt_b = np.sum(grad_wrt_b, axis=axis, keepdims=True)\n\n    return [grad_wrt_a, grad_wrt_b]\n\n\n@RegisterGradient(""matmul"")\ndef _matmul_gradient(op, grad):\n    """"""Computes the gradients for `matmul`.\n\n    Args:\n      op: The `matmul` `Operation` that we are differentiating\n      grad: Gradient with respect to the output of the `matmul` op.\n\n    Returns:\n      Gradients with respect to the input of `matmul`.\n    """"""\n\n    A = op.inputs[0]\n    B = op.inputs[1]\n\n    return [grad.dot(B.T), A.T.dot(grad)]\n\n\n@RegisterGradient(""sigmoid"")\ndef _sigmoid_gradient(op, grad):\n    """"""Computes the gradients for `sigmoid`.\n\n    Args:\n      op: The `sigmoid` `Operation` that we are differentiating\n      grad: Gradient with respect to the output of the `sigmoid` op.\n\n    Returns:\n      Gradients with respect to the input of `sigmoid`.\n    """"""\n\n    sigmoid = op.output\n\n    return grad * sigmoid * (1 - sigmoid)\n\n\n@RegisterGradient(""softmax"")\ndef _softmax_gradient(op, grad):\n    """"""Computes the gradients for `softmax`.\n\n    Args:\n      op: The `softmax` `Operation` that we are differentiating\n      grad: Gradient with respect to the output of the `softmax` op.\n\n    Returns:\n      Gradients with respect to the input of `softmax`.\n    """"""\n\n    softmax = op.output\n    return (grad - np.reshape(\n        np.sum(grad * softmax, 1),\n        [-1, 1]\n    )) * softmax\n\n\n\n@RegisterGradient(""log"")\ndef _log_gradient(op, grad):\n    """"""Computes the gradients for `log`.\n\n    Args:\n      op: The `log` `Operation` that we are differentiating\n      grad: Gradient with respect to the output of the `log` op.\n\n    Returns:\n      Gradients with respect to the input of `log`.\n    """"""\n    x = op.inputs[0]\n    return grad/x\n\n\n@RegisterGradient(""multiply"")\ndef _multiply_gradient(op, grad):\n    """"""Computes the gradients for `multiply`.\n\n    Args:\n      op: The `multiply` `Operation` that we are differentiating\n      grad: Gradient with respect to the output of the `multiply` op.\n\n    Returns:\n      Gradients with respect to the input of `multiply`.\n    """"""\n\n    A = op.inputs[0]\n    B = op.inputs[1]\n\n    return [grad * B, grad * A]\n\n\n@RegisterGradient(""reduce_sum"")\ndef _reduce_sum_gradient(op, grad):\n    """"""Computes the gradients for `reduce_sum`.\n\n    Args:\n      op: The `reduce_sum` `Operation` that we are differentiating\n      grad: Gradient with respect to the output of the `reduce_sum` op.\n\n    Returns:\n      Gradients with respect to the input of `reduce_sum`.\n    """"""\n    A = op.inputs[0]\n\n    output_shape = np.array(A.shape)\n    output_shape[op.axis] = 1\n    tile_scaling = A.shape // output_shape\n    grad = np.reshape(grad, output_shape)\n    return np.tile(grad, tile_scaling)\n\n\n@RegisterGradient(""negative"")\ndef _negative_gradient(op, grad):\n    """"""Computes the gradients for `negative`.\n\n    Args:\n      op: The `negative` `Operation` that we are differentiating\n      grad: Gradient with respect to the output of the `negative` op.\n\n    Returns:\n      Gradients with respect to the input of `negative`.\n    """"""\n    return -grad\n'"
tensorslow/graph.py,0,"b'class Graph:\n    """"""Represents a computational graph\n    """"""\n\n    def __init__(self):\n        """"""Construct Graph""""""\n        self.operations = []\n        self.placeholders = []\n        self.variables = []\n\n    def as_default(self):\n        global _default_graph\n        _default_graph = self\n\n\nclass Operation:\n    """"""Represents a graph node that performs a computation.\n\n    An `Operation` is a node in a `Graph` that takes zero or\n    more objects as input, and produces zero or more objects\n    as output.\n    """"""\n\n    def __init__(self, input_nodes=[]):\n        """"""Construct Operation\n        """"""\n        self.input_nodes = input_nodes\n\n        # Initialize list of consumers (i.e. nodes that receive this operation\'s output as input)\n        self.consumers = []\n\n        # Append this operation to the list of consumers of all input nodes\n        for input_node in input_nodes:\n            input_node.consumers.append(self)\n\n        # Append this operation to the list of operations in the currently active default graph\n        _default_graph.operations.append(self)\n\n    def compute(self):\n        """"""Computes the output of this operation.\n        """" Must be implemented by the particular operation.\n        """"""\n        pass\n\n\nclass placeholder:\n    """"""Represents a placeholder node that has to be provided with a value\n       when computing the output of a computational graph\n    """"""\n\n    def __init__(self):\n        """"""Construct placeholder\n        """"""\n        self.consumers = []\n\n        # Append this placeholder to the list of placeholders in the currently active default graph\n        _default_graph.placeholders.append(self)\n\n\nclass Variable:\n    """"""Represents a variable (i.e. an intrinsic, changeable parameter of a computational graph).\n    """"""\n\n    def __init__(self, initial_value=None):\n        """"""Construct Variable\n\n        Args:\n          initial_value: The initial value of this variable\n        """"""\n        self.value = initial_value\n        self.consumers = []\n\n        # Append this variable to the list of variables in the currently active default graph\n        _default_graph.variables.append(self)\n'"
tensorslow/operations.py,0,"b'import numpy as np\nfrom tensorslow.graph import Operation\n\nclass add(Operation):\n    """"""Returns x + y element-wise.\n    """"""\n\n    def __init__(self, x, y):\n        """"""Construct add\n\n        Args:\n          x: First summand node\n          y: Second summand node\n        """"""\n        super().__init__([x, y])\n\n    def compute(self, x_value, y_value):\n        """"""Compute the output of the add operation\n\n        Args:\n          x_value: First summand value\n          y_value: Second summand value\n        """"""\n        self.inputs = [x_value, y_value]\n        return x_value + y_value\n\n\nclass matmul(Operation):\n    """"""Multiplies matrix a by matrix b, producing a * b.\n    """"""\n\n    def __init__(self, a, b):\n        """"""Construct matmul\n\n        Args:\n          a: First matrix\n          b: Second matrix\n        """"""\n        super().__init__([a, b])\n\n    def compute(self, a_value, b_value):\n        """"""Compute the output of the matmul operation\n\n        Args:\n          a_value: First matrix value\n          b_value: Second matrix value\n        """"""\n        self.inputs = [a_value, b_value]\n        return a_value.dot(b_value)\n\n\nclass sigmoid(Operation):\n    """"""Returns the sigmoid of x element-wise.\n    """"""\n\n    def __init__(self, a):\n        """"""Construct sigmoid\n\n        Args:\n          a: Input node\n        """"""\n        super().__init__([a])\n\n    def compute(self, a_value):\n        """"""Compute the output of the sigmoid operation\n\n        Args:\n          a_value: Input value\n        """"""\n        return 1 / (1 + np.exp(-a_value))\n\n\nclass softmax(Operation):\n    """"""Returns the softmax of a.\n    """"""\n\n    def __init__(self, a):\n        """"""Construct softmax\n\n        Args:\n          a: Input node\n        """"""\n        super().__init__([a])\n\n    def compute(self, a_value):\n        """"""Compute the output of the softmax operation\n\n        Args:\n          a_value: Input value\n        """"""\n        return np.exp(a_value) / np.sum(np.exp(a_value), axis=1)[:, None]\n\n\nclass log(Operation):\n    """"""Computes the natural logarithm of x element-wise.\n    """"""\n\n    def __init__(self, x):\n        """"""Construct log\n\n        Args:\n          x: Input node\n        """"""\n        super().__init__([x])\n\n    def compute(self, x_value):\n        """"""Compute the output of the log operation\n\n        Args:\n          x_value: Input value\n        """"""\n        return np.log(x_value)\n\n\nclass multiply(Operation):\n    """"""Returns x * y element-wise.\n    """"""\n\n    def __init__(self, x, y):\n        """"""Construct multiply\n\n        Args:\n          x: First multiplicand node\n          y: Second multiplicand node\n        """"""\n        super().__init__([x, y])\n\n    def compute(self, x_value, y_value):\n        """"""Compute the output of the multiply operation\n\n        Args:\n          x_value: First multiplicand value\n          y_value: Second multiplicand value\n        """"""\n        return x_value * y_value\n\n\nclass reduce_sum(Operation):\n    """"""Computes the sum of elements across dimensions of a tensor.\n    """"""\n\n    def __init__(self, A, axis=None):\n        """"""Construct reduce_sum\n\n        Args:\n          A: The tensor to reduce.\n          axis: The dimensions to reduce. If `None` (the default), reduces all dimensions.\n        """"""\n        super().__init__([A])\n        self.axis = axis\n\n    def compute(self, A_value):\n        """"""Compute the output of the reduce_sum operation\n\n        Args:\n          A_value: Input tensor value\n        """"""\n        return np.sum(A_value, self.axis)\n\n\nclass negative(Operation):\n    """"""Computes the negative of x element-wise.\n    """"""\n\n    def __init__(self, x):\n        """"""Construct negative\n\n        Args:\n          x: Input node\n        """"""\n        super().__init__([x])\n\n    def compute(self, x_value):\n        """"""Compute the output of the negative operation\n\n        Args:\n          x_value: Input value\n        """"""\n        return -x_value\n'"
tensorslow/session.py,0,"b'import numpy as np\nfrom tensorslow.graph import Operation\nfrom tensorslow.graph import placeholder\nfrom tensorslow.graph import Variable\n\nclass Session:\n    """"""Represents a particular execution of a computational graph.\n    """"""\n\n    def run(self, operation, feed_dict={}):\n        """"""Computes the output of an operation\n\n        Args:\n          operation: The operation whose output we\'d like to compute.\n          feed_dict: A dictionary that maps placeholders to values for this session\n        """"""\n\n        # Perform a post-order traversal of the graph to bring the nodes into the right order\n        nodes_postorder = traverse_postorder(operation)\n\n        # Iterate all nodes to determine their value\n        for node in nodes_postorder:\n\n            if type(node) == placeholder:\n                # Set the node value to the placeholder value from feed_dict\n                node.output = feed_dict[node]\n            elif type(node) == Variable:\n                # Set the node value to the variable\'s value attribute\n                node.output = node.value\n            else:  # Operation\n                # Get the input values for this operation from node_values\n                node.inputs = [input_node.output for input_node in node.input_nodes]\n\n                # Compute the output of this operation\n                node.output = node.compute(*node.inputs)\n\n            # Convert lists to numpy arrays\n            if type(node.output) == list:\n                node.output = np.array(node.output)\n\n        # Return the requested node value\n        return operation.output\n\n\ndef traverse_postorder(operation):\n    """"""Performs a post-order traversal, returning a list of nodes\n    in the order in which they have to be computed\n\n    Args:\n       operation: The operation to start traversal at\n    """"""\n\n    nodes_postorder = []\n\n    def recurse(node):\n        if isinstance(node, Operation):\n            for input_node in node.input_nodes:\n                recurse(input_node)\n        nodes_postorder.append(node)\n\n    recurse(operation)\n    return nodes_postorder\n'"
tensorslow/train.py,0,"b'from tensorslow.graph import Operation\nfrom tensorslow.graph import Variable\nfrom tensorslow.gradients import _gradient_registry\n\nclass GradientDescentOptimizer:\n    def __init__(self, learning_rate):\n        self.learning_rate = learning_rate\n\n    def minimize(self, loss):\n        learning_rate = self.learning_rate\n\n        class MinimizationOperation(Operation):\n            def compute(self):\n                # Compute gradients\n                grad_table = compute_gradients(loss)\n\n                # Iterate all variables\n                for node in grad_table:\n                    if type(node) == Variable:\n                        # Retrieve gradient for this variable\n                        grad = grad_table[node]\n\n                        # Take a step along the direction of the negative gradient\n                        node.value -= learning_rate * grad\n\n        return MinimizationOperation()\n\n\nfrom queue import Queue\n\n\ndef compute_gradients(loss):\n    # grad_table[node] will contain the gradient of the loss w.r.t. the node\'s output\n    grad_table = {}\n\n    # The gradient of the loss with respect to the loss is just 1\n    grad_table[loss] = 1\n\n    # Perform a breadth-first search, backwards from the loss\n    visited = set()\n    queue = Queue()\n    visited.add(loss)\n    queue.put(loss)\n\n    while not queue.empty():\n        node = queue.get()\n\n        # If this node is not the loss\n        if node != loss:\n            #\n            # Compute the gradient of the loss with respect to this node\'s output\n            #\n            grad_table[node] = 0\n\n            # Iterate all consumers\n            for consumer in node.consumers:\n\n                # Retrieve the gradient of the loss w.r.t. consumer\'s output\n                lossgrad_wrt_consumer_output = grad_table[consumer]\n\n                # Retrieve the function which computes gradients with respect to\n                # consumer\'s inputs given gradients with respect to consumer\'s output.\n                consumer_op_type = consumer.__class__\n                bprop = _gradient_registry[consumer_op_type]\n\n                # Get the gradient of the loss with respect to all of consumer\'s inputs\n                lossgrads_wrt_consumer_inputs = bprop(consumer, lossgrad_wrt_consumer_output)\n\n                if len(consumer.input_nodes) == 1:\n                    # If there is a single input node to the consumer, lossgrads_wrt_consumer_inputs is a scalar\n                    grad_table[node] += lossgrads_wrt_consumer_inputs\n\n                else:\n                    # Otherwise, lossgrads_wrt_consumer_inputs is an array of gradients for each input node\n\n                    # Retrieve the index of node in consumer\'s inputs\n                    node_index_in_consumer_inputs = consumer.input_nodes.index(node)\n\n                    # Get the gradient of the loss with respect to node\n                    lossgrad_wrt_node = lossgrads_wrt_consumer_inputs[node_index_in_consumer_inputs]\n\n                    # Add to total gradient\n                    grad_table[node] += lossgrad_wrt_node\n\n        #\n        # Append each input node to the queue\n        #\n        if hasattr(node, ""input_nodes""):\n            for input_node in node.input_nodes:\n                if not input_node in visited:\n                    visited.add(input_node)\n                    queue.put(input_node)\n\n    # Return gradients for each visited node\n    return grad_table\n'"
