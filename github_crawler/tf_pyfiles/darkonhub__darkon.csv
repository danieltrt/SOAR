file_path,api_count,code
setup.py,0,"b""from setuptools import setup, find_packages\nimport json\n\n\ninfo = json.load(open('info.json'))\nwith open('requirements.txt') as f:\n    deps = [str(dep.strip()) for dep in f.readlines()]\n\nsetup(\n    name=info['name'],\n    packages=find_packages('.'),\n    version=info['version'],\n    description='Performance hacking for your deep learning models',\n    long_description=open('setup.rst').read(),\n    author=info['authors'],\n    url=info['github_url'],\n    download_url='{}/tarball/v{}'.format(info['github_url'], info['version']),\n    keywords=['AI', 'ML', 'DL', 'deep learning', 'machine learning', 'neural network',\n              'deep neural network', 'debug neural networks', 'performance hacking',\n              'tensorflow', 'tf'],\n    license='Apache License 2.0',\n    classifiers=[\n        'Development Status :: 2 - Pre-Alpha',\n        'Intended Audience :: Developers',\n        'Intended Audience :: Science/Research',\n        'License :: OSI Approved :: Apache Software License',\n        'Programming Language :: Python :: 2',\n        'Programming Language :: Python :: 2.7',\n        'Programming Language :: Python :: 3',\n        'Programming Language :: Python :: 3.6',\n        'Topic :: Scientific/Engineering :: Artificial Intelligence',\n        'Topic :: Software Development :: Debuggers',\n        'Topic :: Software Development :: Libraries :: Python Modules'\n    ],\n    install_requires=deps,\n    extras_require={\n        'tensorflow-gpu':  ['tensorflow-gpu>=1.3'],\n        'tensorflow': ['tensorflow>=1.3']\n    }\n)\n"""
darkon/__init__.py,0,"b'# Copyright 2017 Neosapience, Inc.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ========================================================================\nfrom .influence.influence import Influence  # noqa: ignore=F401\nfrom .influence.influence import InfluenceFeeder  # noqa: ignore=F401\nfrom .gradcam.gradcam import Gradcam  # noqa: ignore=F401\n\n__all__ = [""influence"", ""gradcam""]\n'"
darkon/log.py,0,"b'# Copyright 2017 Neosapience, Inc.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ========================================================================\nimport logging\nfrom logging.handlers import RotatingFileHandler\n\n\nclass DarkonLogger:\n    """""" darkon package logger\n    """"""\n    def __init__(self):\n        _formatter = logging.Formatter(\'%(asctime)s %(name)s %(levelname)-4s: %(message)s\')\n        # _file_handler = logging.handler.FileHandler(__package__ + \'.log\')\n        _file_handler = RotatingFileHandler(\'darkon.log\', maxBytes=1024*1024*100)\n        _file_handler.setFormatter(_formatter)\n        _file_handler.setLevel(logging.DEBUG)\n        _stream_handler = logging.StreamHandler()\n        _stream_handler.setFormatter(_formatter)\n        _stream_handler.setLevel(logging.INFO)\n\n        _logger = logging.getLogger(__package__)\n        _logger.setLevel(logging.DEBUG)\n        _logger.addHandler(_file_handler)\n        _logger.addHandler(_stream_handler)\n        self._logger = _logger\n\n        _logger.debug(\'----------------------------\')\n        _logger.debug(\'start logging darkon package\')\n\n    @property\n    def logger(self):\n        return self._logger\n\n\nlogger = DarkonLogger().logger\n'"
test/__init__.py,0,"b'# Copyright 2017 Neosapience, Inc.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ========================================================================\nfrom . import tf_util\n'"
test/test_gradcam.py,9,"b'# Copyright 2017 Neosapience, Inc.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ========================================================================\nimport unittest\n\nimport darkon\nimport tensorflow as tf\nimport numpy as np\nfrom tensorflow.contrib.slim.nets import resnet_v1\nfrom tensorflow.contrib.slim.nets import vgg\nimport tensorflow.contrib.slim as slim\nimport cv2\n\n\ndef load_image(image_path):\n    image = cv2.imread(image_path, cv2.IMREAD_COLOR)\n    resized_image = cv2.resize(image, (224, 224))\n    return resized_image.astype(np.float)\n\n\ndef load_expected_data(model_name, meta):\n    heatmap_path = \'test/data/gradcam/{}_gradcam_heatmap_{}.npy\'.format(meta, model_name)\n    guided_backprop_path = \'test/data/gradcam/{}_guided_backprop_{}.npy\'.format(meta, model_name)\n    gradcam_img_path = \'test/data/gradcam/{}_gradcam_img_{}.png\'.format(meta, model_name)\n    guided_gradcam_img_path = \'test/data/gradcam/{}_guided_gradcam_img_{}.png\'.format(meta, model_name)\n    return {\n        \'gradcam_img\': load_image(gradcam_img_path),\n        \'guided_gradcam_img\': load_image(guided_gradcam_img_path),\n        \'heatmap\': np.load(heatmap_path),\n        \'guided_backprop\': np.load(guided_backprop_path)\n    }\n\n\ndef save_expected_data(model_name, meta, ret):\n    heatmap_path = \'test/data/gradcam/{}_gradcam_heatmap_{}.npy\'.format(meta, model_name)\n    guided_backprop_path = \'test/data/gradcam/{}_guided_backprop_{}.npy\'.format(meta, model_name)\n    gradcam_img_path = \'test/data/gradcam/{}_gradcam_img_{}.png\'.format(meta, model_name)\n    guided_gradcam_img_path = \'test/data/gradcam/{}_guided_gradcam_img_{}.png\'.format(meta, model_name)\n\n    np.save(heatmap_path, ret[\'heatmap\'])\n    np.save(guided_backprop_path, ret[\'guided_backprop\'])\n    cv2.imwrite(gradcam_img_path, ret[\'gradcam_img\'])\n    cv2.imwrite(guided_gradcam_img_path, ret[\'guided_gradcam_img\'])\n\n\nclass TestGradcam(unittest.TestCase):\n    def setUp(self):\n        tf.reset_default_graph()\n        self.nbclasses = 1000\n        self.inputs = tf.placeholder(tf.float32, [1, 224, 224, 3])\n\n    def tearDown(self):\n        image = load_image(\'test/data/cat_dog.png\')\n\n        prob_op_name = darkon.Gradcam.candidate_predict_op_names(self.sess, self.nbclasses, self.graph_origin)[-1]\n        insp = darkon.Gradcam(self.inputs, self.nbclasses, self.target_op_name, prob_op_name, graph=self.graph_origin)\n        ret_top1 = insp.gradcam(self.sess, image)\n        ret_243 = insp.gradcam(self.sess, image, 243)\n\n        # save_expected_data(self.model_name, \'top1\', ret_top1)\n        # save_expected_data(self.model_name, \'243\', ret_243)\n\n        exp_top1 = load_expected_data(self.model_name, \'top1\')\n        for key in ret_top1.keys():\n            atol = 5 if \'img\' in key else 1e-6\n            print(key, atol)\n            self.assertTrue(np.allclose(ret_top1[key], exp_top1[key], atol=atol))\n\n        exp_243 = load_expected_data(self.model_name, \'243\')\n        for key in ret_243.keys():\n            atol = 5 if \'img\' in key else 1e-6\n            self.assertTrue(np.allclose(ret_243[key], exp_243[key], atol=atol))\n\n        # just check new output\n        # cv2.imwrite(\'test_{}.png\', ret[\'gradcam_img\'])\n        # cv2.imwrite(\'test_guided_{}.png\', ret[\'guided_gradcam_img\'])\n        # cv2.imwrite(\'test_guided_backprop_{}.png\', ret[\'guided_backprop_img\'])\n        # tf.summary.FileWriter(""./tmp/log-{}/"".format(self.model_name), sess.graph)\n        self.sess.close()\n\n    def test_resnet(self):\n        with slim.arg_scope(resnet_v1.resnet_arg_scope()):\n            net, end_points = resnet_v1.resnet_v1_50(self.inputs, self.nbclasses, is_training=False)\n        saver = tf.train.Saver(tf.global_variables())\n        check_point = \'test/data/resnet_v1_50.ckpt\'\n        sess = tf.InteractiveSession()\n        saver.restore(sess, check_point)\n\n        self.sess = sess\n        self.graph_origin = tf.get_default_graph()\n        self.target_op_name = darkon.Gradcam.candidate_featuremap_op_names(sess, self.graph_origin)[-1]\n        self.model_name = \'resnet\'\n        \n        self.assertEqual(\'resnet_v1_50/block4/unit_3/bottleneck_v1/Relu\', self.target_op_name) \n\n    def test_vgg(self):\n        with slim.arg_scope(vgg.vgg_arg_scope()):\n            net, end_points = vgg.vgg_16(self.inputs, self.nbclasses, is_training=False)\n            net = slim.softmax(net)\n        saver = tf.train.Saver(tf.global_variables())\n        check_point = \'test/data/vgg_16.ckpt\'\n\n        sess = tf.InteractiveSession()\n        saver.restore(sess, check_point)\n\n        self.sess = sess\n        self.graph_origin = tf.get_default_graph()\n        self.target_op_name = darkon.Gradcam.candidate_featuremap_op_names(sess, self.graph_origin)[-2]\n        self.model_name = \'vgg\'\n        self.assertEqual(\'vgg_16/conv5/conv5_3/Relu\', self.target_op_name)\n'"
test/test_gradcam_dangling.py,8,"b'# Copyright 2017 Neosapience, Inc.\r\n#\r\n# Licensed under the Apache License, Version 2.0 (the ""License"");\r\n# you may not use this file except in compliance with the License.\r\n# You may obtain a copy of the License at\r\n#\r\n#     http://www.apache.org/licenses/LICENSE-2.0\r\n#\r\n# Unless required by applicable law or agreed to in writing, software\r\n# distributed under the License is distributed on an ""AS IS"" BASIS,\r\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n# See the License for the specific language governing permissions and\r\n# limitations under the License.\r\n# ========================================================================\r\nimport unittest\r\n\r\nimport darkon\r\nimport tensorflow as tf\r\nimport numpy as np\r\nfrom tensorflow.contrib.slim.nets import resnet_v1\r\nimport tensorflow.contrib.slim as slim\r\nimport cv2\r\n\r\n\r\ndef load_image(image_path):\r\n    image = cv2.imread(image_path, cv2.IMREAD_COLOR)\r\n    resized_image = cv2.resize(image, (224, 224))\r\n    return resized_image.astype(np.float)\r\n\r\n\r\nclass TestGradcamDangling(unittest.TestCase):\r\n    def setUp(self):\r\n        tf.reset_default_graph()\r\n\r\n        self.nbclasses = 1000\r\n        inputs = tf.placeholder(tf.float32, [1, 224, 224, 3])\r\n        with slim.arg_scope(resnet_v1.resnet_arg_scope()):\r\n            net, end_points = resnet_v1.resnet_v1_50(inputs, self.nbclasses, is_training=False)\r\n        saver = tf.train.Saver(tf.global_variables())\r\n        check_point = \'test/data/resnet_v1_50.ckpt\'\r\n\r\n        sess = tf.InteractiveSession()\r\n        saver.restore(sess, check_point)\r\n\r\n        conv_name = \'resnet_v1_50/block4/unit_3/bottleneck_v1/Relu\'\r\n        \r\n        self.graph_origin = tf.get_default_graph().as_graph_def()\r\n        self.insp = darkon.Gradcam(inputs, self.nbclasses, conv_name)\r\n        self.sess = sess\r\n\r\n    def tearDown(self):\r\n        self.sess.close()\r\n\r\n    def test_dangling(self):\r\n        image = load_image(\'test/data/cat_dog.png\')\r\n\r\n        graph_influence_init = tf.get_default_graph().as_graph_def()\r\n        self.assertNotEqual(self.graph_origin, graph_influence_init)\r\n\r\n        _ = self.insp.gradcam(self.sess, image)\r\n\r\n        graph_first_executed = tf.get_default_graph().as_graph_def()\r\n        self.assertEqual(graph_influence_init, graph_first_executed)\r\n\r\n        _ = self.insp.gradcam(self.sess, image)\r\n\r\n        graph_second_executed = tf.get_default_graph().as_graph_def()\r\n        self.assertEqual(graph_first_executed, graph_second_executed)\r\n\r\n'"
test/test_gradcam_guided_backprop.py,31,"b'# Copyright 2017 Neosapience, Inc.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ========================================================================\nimport unittest\n\nimport darkon\nimport tensorflow as tf\nimport numpy as np\n\n_classes = 2\n\n\ndef nn_graph(activation):\n    # create graph\n    x = tf.placeholder(tf.float32, (1, 2, 2, 3), \'x_placeholder\')\n    y = tf.placeholder(tf.int32, name=\'y_placeholder\', shape=[1, 2])\n\n    with tf.name_scope(\'conv1\'):\n        conv_1 = tf.layers.conv2d(\n            inputs=x,\n            filters=10,\n            kernel_size=[2, 2],\n            padding=""same"",\n            activation=activation)\n\n    with tf.name_scope(\'fc2\'):\n        flatten = tf.layers.flatten(conv_1)\n        top = tf.layers.dense(flatten, _classes)\n\n    logits = tf.nn.softmax(top)\n    return x\n\n\nclass GradcamGuidedBackprop(unittest.TestCase):\n    def setUp(self):\n        tf.reset_default_graph()\n\n    def tearDown(self):\n        x = nn_graph(activation=self.activation_fn)\n        image = np.random.uniform(size=(2, 2, 3))\n\n        with tf.Session() as sess:\n            sess.run(tf.global_variables_initializer())\n\n            gradcam_ops = darkon.Gradcam.candidate_featuremap_op_names(sess)\n\n            if self.enable_guided_backprop:\n                _ = darkon.Gradcam(x, _classes, gradcam_ops[-1])\n\n            g = tf.get_default_graph()\n            from_ts = g.get_operation_by_name(gradcam_ops[-1]).outputs\n            to_ts = g.get_operation_by_name(gradcam_ops[-2]).outputs\n\n            max_output = tf.reduce_max(from_ts, axis=3)\n            y = tf.reduce_sum(-max_output * 1e2)\n\n            grad = tf.gradients(y, to_ts)[0]\n            grad_val = sess.run(grad, feed_dict={x: np.expand_dims(image, 0)})\n\n            if self.enable_guided_backprop:\n                self.assertTrue(not np.any(grad_val))\n            else:\n                self.assertTrue(np.any(grad_val))\n\n    def test_relu(self):\n        self.activation_fn = tf.nn.relu\n        self.enable_guided_backprop = False\n\n    def test_relu_guided(self):\n        self.activation_fn = tf.nn.relu\n        self.enable_guided_backprop = True\n\n    def test_tanh(self):\n        self.activation_fn = tf.nn.tanh\n        self.enable_guided_backprop = False\n\n    def test_tanh_guided(self):\n        self.activation_fn = tf.nn.tanh\n        self.enable_guided_backprop = True\n\n    def test_sigmoid(self):\n        self.activation_fn = tf.nn.sigmoid\n        self.enable_guided_backprop = False\n\n    def test_sigmoid_guided(self):\n        self.activation_fn = tf.nn.sigmoid\n        self.enable_guided_backprop = True\n\n    def test_relu6(self):\n        self.activation_fn = tf.nn.relu6\n        self.enable_guided_backprop = False\n\n    def test_relu6_guided(self):\n        self.activation_fn = tf.nn.relu6\n        self.enable_guided_backprop = True\n\n    def test_elu(self):\n        self.activation_fn = tf.nn.elu\n        self.enable_guided_backprop = False\n\n    def test_elu_guided(self):\n        self.activation_fn = tf.nn.elu\n        self.enable_guided_backprop = True\n\n    def test_selu(self):\n        self.activation_fn = tf.nn.selu\n        self.enable_guided_backprop = False\n\n    def test_selu_guided(self):\n        self.activation_fn = tf.nn.selu\n        self.enable_guided_backprop = True\n\n    def test_softplus(self):\n        self.activation_fn = tf.nn.softplus\n        self.enable_guided_backprop = False\n\n    def test_test_softplus_guided(self):\n        self.activation_fn = tf.nn.softplus\n        self.enable_guided_backprop = True\n\n    def test_softsign(self):\n        self.activation_fn = tf.nn.softsign\n        self.enable_guided_backprop = False\n\n    def test_softsign_guided(self):\n        self.activation_fn = tf.nn.softsign\n        self.enable_guided_backprop = True\n'"
test/test_gradcam_sequence.py,4,"b'# Copyright 2017 Neosapience, Inc.\r\n#\r\n# Licensed under the Apache License, Version 2.0 (the ""License"");\r\n# you may not use this file except in compliance with the License.\r\n# You may obtain a copy of the License at\r\n#\r\n#     http://www.apache.org/licenses/LICENSE-2.0\r\n#\r\n# Unless required by applicable law or agreed to in writing, software\r\n# distributed under the License is distributed on an ""AS IS"" BASIS,\r\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n# See the License for the specific language governing permissions and\r\n# limitations under the License.\r\n# ========================================================================\r\nimport unittest\r\n\r\nimport darkon\r\nimport tensorflow as tf\r\nimport numpy as np\r\nfrom tensorflow.contrib import learn\r\n\r\nclass TestGradcamSequence(unittest.TestCase):\r\n    def setUp(self):\r\n        tf.reset_default_graph()\r\n        x_raw = [""a masterpiece of four years in the making""]\r\n        vocab_path = ""test/data/sequence/vocab""\r\n        vocab_processor = learn.preprocessing.VocabularyProcessor.restore(vocab_path)\r\n        self.x_test_batch = np.array(list(vocab_processor.transform(x_raw)))\r\n        self.y_test_batch = [[1.0, 0.0]]\r\n        \r\n    def test_text(self):\r\n        sess = tf.InteractiveSession()\r\n        checkpoint_file = ""test/data/sequence/model-15000""\r\n        saver = tf.train.import_meta_graph(""{}.meta"".format(checkpoint_file))\r\n        saver.restore(sess, checkpoint_file)\r\n        graph = tf.get_default_graph()   \r\n        input_x = graph.get_operation_by_name(""input_x"").outputs[0]\r\n        dropout_keep_prob = graph.get_operation_by_name(""dropout_keep_prob"").outputs[0]\r\n        input_y = graph.get_operation_by_name(""input_y"").outputs[0]\r\n\r\n        conv_op_names = darkon.Gradcam.candidate_featuremap_op_names(sess, \r\n            feed_options={input_x: self.x_test_batch, input_y: self.y_test_batch ,dropout_keep_prob:1.0})\r\n                \r\n        prob_op_names = darkon.Gradcam.candidate_predict_op_names(sess, 2, \r\n            feed_options={input_x: self.x_test_batch, input_y: self.y_test_batch ,dropout_keep_prob:1.0})\r\n        \r\n        conv_name = conv_op_names[-7]\r\n        prob_name = prob_op_names[-1]\r\n        self.assertEqual(conv_name, ""conv-maxpool-3/relu"")\r\n        self.assertEqual(prob_name, ""output/scores"")\r\n            \r\n        insp = darkon.Gradcam(input_x, 2, conv_name, prob_name, graph=graph)\r\n        ret = insp.gradcam(sess, self.x_test_batch[0], feed_options={dropout_keep_prob: 1})'"
test/test_gradcam_util.py,16,"b'# Copyright 2017 Neosapience, Inc.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ========================================================================\nimport unittest\n\nimport darkon\nimport tensorflow as tf\nimport numpy as np\n\n_classes = 2\n\n\ndef nn_graph(activation):\n    # create graph\n    x = tf.placeholder(tf.float32, (1, 2, 2, 3), \'x_placeholder\')\n    y = tf.placeholder(tf.int32, name=\'y_placeholder\', shape=[1, 2])\n\n    with tf.name_scope(\'conv1\'):\n        conv_1 = tf.layers.conv2d(\n            inputs=x,\n            filters=10,\n            kernel_size=[2, 2],\n            padding=""same"",\n            activation=activation)\n\n    with tf.name_scope(\'fc2\'):\n        flatten = tf.layers.flatten(conv_1)\n        top = tf.layers.dense(flatten, _classes)\n\n    with tf.name_scope(\'logits\'):\n        logits = tf.nn.softmax(top)\n\n    with tf.name_scope(\'loss\'):\n        cross_entropy = tf.nn.softmax_cross_entropy_with_logits(labels=y, logits=logits)\n        cross_entropy = tf.reduce_mean(cross_entropy)\n\n    return x, y, cross_entropy\n\n\nclass GradcamUtil(unittest.TestCase):\n    def setUp(self):\n        tf.reset_default_graph()\n\n    def tearDown(self):\n        pass\n\n    def test_relu(self):\n        x, y, cross_entropy = nn_graph(activation=tf.nn.relu)\n        image = np.random.uniform(size=(2, 2, 3))\n\n        with tf.Session() as sess:\n            sess.run(tf.global_variables_initializer())\n\n            gradcam_ops = darkon.Gradcam.candidate_featuremap_op_names(sess)\n            prob_ops = darkon.Gradcam.candidate_predict_op_names(sess, _classes)\n\n            insp = darkon.Gradcam(x, _classes, gradcam_ops[-1], prob_ops[-1])\n            ret = insp.gradcam(sess, image)\n            self.assertTrue(ret)\n'"
test/test_influence.py,19,"b'# Copyright 2017 Neosapience, Inc.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ========================================================================\nimport unittest\n\nimport darkon\nimport tensorflow as tf\nimport numpy as np\nfrom .tf_util import weight_variable, bias_variable\n\n\n_num_train_data = 20\n_dim_features = 5\n_num_test_data = 3\n_classes = 2\n_batch_size = 4\n_num_iterations = 5\n\n\ndef nn_graph():\n    # create graph\n    x = tf.placeholder(tf.float32, name=\'x_placeholder\')\n    y = tf.placeholder(tf.int32, name=\'y_placeholder\')\n\n    with tf.name_scope(\'fc1\'):\n        W_fc1 = weight_variable([_dim_features, _classes], \'weight\')\n        b_fc1 = bias_variable([_classes], \'bias\')\n        op_fc1 = tf.add(tf.matmul(x, W_fc1), b_fc1)\n\n    # set loss function\n    cross_entropy = tf.nn.softmax_cross_entropy_with_logits(labels=y, logits=op_fc1)\n    cross_entropy = tf.reduce_mean(cross_entropy)\n    return x, y, cross_entropy\n\n\nclass TestInfluence(unittest.TestCase):\n    def setUp(self):\n        # init tf default graph\n        tf.reset_default_graph()\n\n        # dataset feeder\n        class MyFeeder(darkon.InfluenceFeeder):\n            def __init__(self):\n                self.train_x = np.random.uniform(size=_num_train_data * _dim_features).reshape([_num_train_data, -1])\n                self.train_y = np.random.randint(_classes, size=_num_train_data).reshape([-1])\n                self.test_x = np.random.uniform(size=_num_test_data * _dim_features).reshape([_num_test_data, -1])\n                self.test_y = np.random.randint(_classes, size=_num_test_data).reshape([-1])\n\n                self.train_y = np.eye(_classes)[self.train_y]\n                self.test_y = np.eye(_classes)[self.test_y]\n\n            def reset(self):\n                np.random.seed(97)\n\n            def train_batch(self, batch_size):\n                idx = np.random.choice(_num_train_data - batch_size + 1, 1)[0]\n                return self.train_x[idx:idx+batch_size], self.train_y[idx:idx+batch_size]\n\n            def train_one(self, index):\n                return self.train_x[index], self.train_y[index]\n\n            def test_indices(self, indices):\n                return self.test_x[indices], self.test_y[indices]\n\n        x, y, cross_entropy = nn_graph()\n\n        # open session\n        self.sess = tf.InteractiveSession()\n        saver = tf.train.Saver()\n        saver.restore(self.sess, tf.train.latest_checkpoint(\'test/data\'))\n\n        self.graph_origin = tf.get_default_graph().as_graph_def()\n        # initialize influence function\n        self.insp = darkon.Influence(workspace=\'./tmp\',\n                                     feeder=MyFeeder(),\n                                     loss_op_train=cross_entropy,\n                                     loss_op_test=cross_entropy,\n                                     x_placeholder=x,\n                                     y_placeholder=y)\n\n    def tearDown(self):\n        self.sess.close()\n\n    # def test_freeze_graph(self):\n    #     saver = tf.train.Saver()\n    #     with tf.Session() as sess:\n    #         # sess.run(tf.global_variables_initializer())\n    #         saver.restore(sess, tf.train.latest_checkpoint(\'test/data-origin\'))\n    #         saver.save(sess, \'test/data/model\', global_step=0)\n\n    def test_influence(self):\n        test_indices = [0]\n        approx_params = {\'scale\': 10,\n                         \'num_repeats\': 3,\n                         \'recursion_depth\': 2,\n                         \'recursion_batch_size\': _batch_size}\n\n        # get influence scores for all trainset\n        result = self.insp.upweighting_influence_batch(self.sess,\n                                                       test_indices=test_indices,\n                                                       test_batch_size=_batch_size,\n                                                       approx_params=approx_params,\n                                                       train_batch_size=_batch_size,\n                                                       train_iterations=_num_iterations,\n                                                       force_refresh=True)\n\n        # get influence scores for all trainset\n        result2 = self.insp.upweighting_influence_batch(self.sess,\n                                                        test_indices=test_indices,\n                                                        test_batch_size=_batch_size,\n                                                        approx_params=approx_params,\n                                                        train_batch_size=_batch_size,\n                                                        train_iterations=_num_iterations,\n                                                        force_refresh=False)\n\n        self.assertEqual(_batch_size * _num_iterations, len(result2))\n        self.assertTrue(np.all(result == result2))\n\n        selected_trainset = [2, 3, 0, 9, 14, 19, 8]\n        result_partial = self.insp.upweighting_influence(self.sess,\n                                                         test_indices=test_indices,\n                                                         test_batch_size=_batch_size,\n                                                         approx_params=approx_params,\n                                                         train_indices=selected_trainset,\n                                                         num_total_train_example=_num_train_data,\n                                                         force_refresh=False)\n        self.assertEqual(7, len(result_partial))\n\n    def test_influence_sampling(self):\n        test_indices = [0]\n        approx_batch_size = _batch_size\n        approx_params = {\'scale\': 10,\n                         \'num_repeats\': 3,\n                         \'recursion_depth\': 2,\n                         \'recursion_batch_size\': approx_batch_size}\n\n        result = self.insp.upweighting_influence_batch(self.sess,\n                                                       test_indices=test_indices,\n                                                       test_batch_size=_batch_size,\n                                                       approx_params=approx_params,\n                                                       train_batch_size=_batch_size,\n                                                       train_iterations=_num_iterations,\n                                                       force_refresh=False)\n        self.assertEqual(_batch_size * _num_iterations, len(result))\n\n        num_batch_sampling = 2\n        result2 = self.insp.upweighting_influence_batch(self.sess,\n                                                        test_indices=test_indices,\n                                                        test_batch_size=_batch_size,\n                                                        approx_params=approx_params,\n                                                        train_batch_size=_batch_size,\n                                                        train_iterations=_num_iterations,\n                                                        subsamples=num_batch_sampling,\n                                                        force_refresh=False)\n        self.assertEqual(num_batch_sampling * _num_iterations, len(result2))\n\n        result = result.reshape(_num_iterations, _batch_size)\n        result2 = result2.reshape(_num_iterations, num_batch_sampling)\n        result = result[:, :num_batch_sampling]\n        self.assertTrue(np.all(result == result2))\n\n    def test_unknown_approx_key(self):\n        test_indices = [0]\n        approx_params = {\'unknown_param\': 1}\n        self.assertRaises(RuntimeError,\n                          self.insp.upweighting_influence_batch,\n                          self.sess,\n                          test_indices=test_indices,\n                          test_batch_size=_batch_size,\n                          approx_params=approx_params,\n                          train_batch_size=_batch_size,\n                          train_iterations=_num_iterations)\n\n    def test_default_approx_params(self):\n        test_indices = [0]\n        r = self.insp.upweighting_influence_batch(self.sess,\n                                                  test_indices=test_indices,\n                                                  test_batch_size=_batch_size,\n                                                  approx_params=None,\n                                                  train_batch_size=_batch_size,\n                                                  train_iterations=_num_iterations)\n\n        r2 = self.insp.upweighting_influence_batch(self.sess,\n                                                   test_indices,\n                                                   _batch_size,\n                                                   None,\n                                                   _batch_size,\n                                                   _num_iterations)\n        self.assertTrue(np.all(r == r2))\n\n    def test_approx_filename(self):\n        test_indices = [0]\n        approx_params = {\'scale\': 10,\n                         \'num_repeats\': 3,\n                         \'recursion_depth\': 2,\n                         \'recursion_batch_size\': _batch_size}\n\n        inv_hvp_filename = \'ihvp.c089c98599898bfb0e7f920c9dfe533af38b5481.npz\'\n        self.insp.ihvp_config.update(approx_params)\n        self.assertEqual(inv_hvp_filename, self.insp._approx_filename(self.sess, test_indices))\n\n        test_indices = [1]\n        self.assertNotEqual(inv_hvp_filename, self.insp._approx_filename(self.sess, test_indices))\n\n        test_indices = [0]\n        self.insp.ihvp_config.update(scale=1)\n        self.assertNotEqual(inv_hvp_filename, self.insp._approx_filename(self.sess, test_indices))\n\n    def test_approx_filename_for_weight(self):\n        test_indices = [0]\n\n        filename_1 = self.insp._approx_filename(self.sess, test_indices)\n        filename_2 = self.insp._approx_filename(self.sess, test_indices)\n        self.assertEqual(filename_1, filename_2)\n\n        self.sess.run(tf.global_variables_initializer())\n        filename_3 = self.insp._approx_filename(self.sess, test_indices)\n        self.assertNotEqual(filename_1, filename_3)\n\n    def test_graph_dangling(self):\n        test_indices = [0]\n        approx_params = {\'scale\': 10,\n                         \'num_repeats\': 3,\n                         \'recursion_depth\': 2,\n                         \'recursion_batch_size\': _batch_size}\n\n        graph_influence_init = tf.get_default_graph().as_graph_def()\n        self.assertNotEqual(self.graph_origin, graph_influence_init)\n\n        self.insp.upweighting_influence(self.sess,\n                                        test_indices=test_indices,\n                                        test_batch_size=_batch_size,\n                                        approx_params=approx_params,\n                                        train_indices=[0],\n                                        num_total_train_example=_num_train_data,\n                                        force_refresh=True)\n\n        graph_first_executed = tf.get_default_graph().as_graph_def()\n        self.assertEqual(graph_influence_init, graph_first_executed)\n\n        self.insp.upweighting_influence(self.sess,\n                                        test_indices=test_indices,\n                                        test_batch_size=_batch_size,\n                                        approx_params=approx_params,\n                                        train_indices=[0],\n                                        num_total_train_example=_num_train_data,\n                                        force_refresh=True)\n\n        graph_second_executed = tf.get_default_graph().as_graph_def()\n        self.assertEqual(graph_first_executed, graph_second_executed)\n'"
test/test_influence_dropout.py,13,"b'# Copyright 2017 Neosapience, Inc.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ========================================================================\nimport unittest\n\nimport darkon\nimport tensorflow as tf\nimport numpy as np\nfrom .tf_util import weight_variable, bias_variable\n\n\n_num_train_data = 20\n_dim_features = 5\n_num_test_data = 3\n_classes = 2\n_batch_size = 4\n_num_iterations = 5\n\n\ndef nn_graph_dropout():\n    # create graph\n    x = tf.placeholder(tf.float32, name=\'x_placeholder\')\n    y = tf.placeholder(tf.int32, name=\'y_placeholder\')\n\n    with tf.name_scope(\'fc1\'):\n        W_fc1 = weight_variable([_dim_features, _classes], \'weight\')\n        b_fc1 = bias_variable([_classes], \'bias\')\n        op_fc1 = tf.add(tf.matmul(x, W_fc1), b_fc1)\n\n    with tf.name_scope(\'dropout\'):\n        keep_prob = tf.placeholder(tf.float32, name=\'keep_prob\')\n        drop_fc1 = tf.nn.dropout(op_fc1, keep_prob)\n\n    # set loss function\n    cross_entropy = tf.nn.softmax_cross_entropy_with_logits(labels=y, logits=drop_fc1)\n    cross_entropy = tf.reduce_mean(cross_entropy)\n    return x, y, cross_entropy, keep_prob\n\n\nclass TestInfluenceWithDropout(unittest.TestCase):\n    def tearDown(self):\n        # init tf default graph\n        tf.reset_default_graph()\n\n        # dataset feeder\n        class MyFeeder(darkon.InfluenceFeeder):\n            def __init__(self):\n                self.train_x = np.random.uniform(size=_num_train_data * _dim_features).reshape([_num_train_data, -1])\n                self.train_y = np.random.randint(_classes, size=_num_train_data).reshape([-1])\n                self.test_x = np.random.uniform(size=_num_test_data * _dim_features).reshape([_num_test_data, -1])\n                self.test_y = np.random.randint(_classes, size=_num_test_data).reshape([-1])\n\n                self.train_y = np.eye(_classes)[self.train_y]\n                self.test_y = np.eye(_classes)[self.test_y]\n\n            def reset(self):\n                np.random.seed(97)\n\n            def train_batch(self, batch_size):\n                idx = np.random.choice(_num_train_data - batch_size + 1, 1)[0]\n                return self.train_x[idx:idx+batch_size], self.train_y[idx:idx+batch_size]\n\n            def train_one(self, index):\n                return self.train_x[index], self.train_y[index]\n\n            def test_indices(self, indices):\n                return self.test_x[indices], self.test_y[indices]\n\n        x, y, cross_entropy, keep_prob = nn_graph_dropout()\n\n        self.insp = darkon.Influence(workspace=\'./tmp\',\n                                     feeder=MyFeeder(),\n                                     loss_op_train=cross_entropy,\n                                     loss_op_test=cross_entropy,\n                                     x_placeholder=x,\n                                     y_placeholder=y,\n                                     test_feed_options={keep_prob: 1.0},\n                                     train_feed_options={keep_prob: self.train_keep_prob})\n        # open session\n        with tf.Session() as sess:\n            saver = tf.train.Saver()\n            saver.restore(sess, tf.train.latest_checkpoint(\'test/data\'))\n\n            test_indices = [0]\n            approx_params = {\'scale\': 10,\n                             \'num_repeats\': 3,\n                             \'recursion_depth\': 2,\n                             \'recursion_batch_size\': _batch_size}\n\n            # get influence scores for all trainset\n            result = self.insp.upweighting_influence_batch(sess,\n                                                           test_indices=test_indices,\n                                                           test_batch_size=_batch_size,\n                                                           approx_params=approx_params,\n                                                           train_batch_size=_batch_size,\n                                                           train_iterations=_num_iterations,\n                                                           force_refresh=True)\n\n            result2 = self.insp.upweighting_influence_batch(sess,\n                                                            test_indices=test_indices,\n                                                            test_batch_size=_batch_size,\n                                                            approx_params=approx_params,\n                                                            train_batch_size=_batch_size,\n                                                            train_iterations=_num_iterations,\n                                                            force_refresh=False)\n\n            self.assertEqual(_batch_size * _num_iterations, len(result2))\n\n            # use dropout or not\n            if 1.0 > self.train_keep_prob:\n                self.assertFalse(np.all(result == result2))\n            else:\n                self.assertTrue(np.all(result == result2))\n\n    def test_with_dropout(self):\n        self.train_keep_prob = 0.5\n\n    def test_without_dropout(self):\n        self.train_keep_prob = 1.0\n'"
test/test_influence_feeder.py,0,"b'# Copyright 2017 Neosapience, Inc.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ========================================================================\nimport unittest\n\nimport darkon\nimport numpy as np\n\n_num_train_data = 20\n_dim_features = 5\n_num_test_data = 3\n_classes = 2\n\n\nclass MyFeeder(darkon.InfluenceFeeder):\n    def __init__(self):\n        self.train_x = np.random.uniform(size=_num_train_data * _dim_features).reshape([_num_train_data, -1])\n        self.train_y = np.random.randint(_classes, size=_num_train_data).reshape([-1])\n        self.test_x = np.random.uniform(size=_num_test_data * _dim_features).reshape([_num_test_data, -1])\n        self.test_y = np.random.randint(_classes, size=_num_test_data).reshape([-1])\n\n        self.train_y = np.eye(_classes)[self.train_y]\n        self.test_y = np.eye(_classes)[self.test_y]\n\n    def reset(self):\n        np.random.seed(97)\n\n    def train_batch(self, batch_size):\n        idx = np.random.choice(_num_train_data - batch_size + 1, 1)[0]\n        return self.train_x[idx:idx + batch_size], self.train_y[idx:idx + batch_size]\n\n    def train_one(self, index):\n        return self.train_x[index], self.train_y[index]\n\n    def test_indices(self, indices):\n        return self.test_x[indices], self.test_y[indices]\n\n\nclass TestInfluenceFeeder(unittest.TestCase):\n    def test_interface_without_implementation(self):\n        self.assertRaises(Exception, darkon.InfluenceFeeder)\n\n    def test_interface(self):\n        class ParentTestFeeder(darkon.InfluenceFeeder):\n            def reset(self):\n                return super(ParentTestFeeder, self).reset()\n\n            def train_batch(self, batch_size):\n                return super(ParentTestFeeder, self).train_batch(batch_size)\n\n            def train_one(self, index):\n                return super(ParentTestFeeder, self).train_batch(index)\n\n            def test_indices(self, indices):\n                return super(ParentTestFeeder, self).train_batch(indices)\n\n        feeder = ParentTestFeeder()\n        self.assertRaises(RuntimeError, feeder.reset)\n        self.assertRaises(RuntimeError, feeder.train_batch, 1)\n        self.assertRaises(RuntimeError, feeder.train_one, 0)\n        self.assertRaises(RuntimeError, feeder.test_indices, [0])\n\n    def test_reset(self):\n        feeder = MyFeeder()\n        feeder.reset()\n        data1 = np.array(feeder.train_batch(4)[0])\n        data1_next = np.array(feeder.train_batch(4)[0])\n        self.assertFalse(np.all(data1 == data1_next))\n\n        feeder.reset()\n        data2 = np.array(feeder.train_batch(4)[0])\n        data2_next = np.array(feeder.train_batch(4)[0])\n        self.assertTrue(np.all(data1 == data2))\n        self.assertTrue(np.all(data1_next == data2_next))\n\n    def test_train_batch(self):\n        feeder = MyFeeder()\n        data, label = feeder.train_batch(4)\n        self.assertEqual(4, len(data))\n        self.assertEqual(4, len(label))\n\n        data, label = feeder.train_batch(1)\n        self.assertEqual(1, len(data))\n        self.assertEqual(1, len(label))\n\n    def test_test_indices(self):\n        feeder = MyFeeder()\n        data, label = feeder.test_indices([2, 0])\n        self.assertEqual(2, len(data))\n        self.assertEqual(2, len(label))\n\n    def test_train_one(self):\n        feeder = MyFeeder()\n        data, label = feeder.train_one(2)\n        self.assertEqual(_dim_features, data.size)\n        self.assertEqual(_classes, label.size)\n'"
test/tf_util.py,4,"b'# Copyright 2017 Neosapience, Inc.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ========================================================================\nimport tensorflow as tf\n\n\ndef weight_variable(shape, name):\n    initial = tf.truncated_normal(shape, stddev=0.1)\n    return tf.Variable(initial, name=name)\n\n\ndef bias_variable(shape, name):\n    initial = tf.constant(0.1, shape=shape)\n    return tf.Variable(initial, name=name)\n'"
darkon/gradcam/__init__.py,0,"b'# Copyright 2017 Neosapience, Inc.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ========================================================================\nfrom . import gradcam\n# from . import guided_grad\n# from . import extract_ops\n\n__all__ = [""gradcam""]\n'"
darkon/gradcam/candidate_ops.py,4,"b'# Copyright 2017 Neosapience, Inc.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ========================================================================\nimport tensorflow as tf\nimport numpy as np\n\n_unusable_op_names = (\n    \'Shape\',\n    \'Reshape\',\n    \'Slice\',\n    \'Pack\',\n    \'Cast\',\n    \'ConcatV2\',\n    \'Const\',\n    \'Identity\',\n    \'ZerosLike\',\n    \'Assign\',\n    \'VariableV2\')\n\n\ndef _unusable_ops(op):\n    if len(op.outputs) == 0 \\\n            or \'save\' in op.name \\\n            or \'gradients/\' in op.name \\\n            or \'/Initializer\' in op.name \\\n            or op.op_def is None \\\n            or op.op_def.name in _unusable_op_names:\n        return True\n    else:\n        return False\n\n\ndef candidate_featuremap_op_names(sess, graph, feed_options):\n    operations = []\n    out_ranks = []\n    out_shapes = []\n\n    for op in graph.get_operations():\n        if _unusable_ops(op):\n            continue\n\n        out_ranks.append(tf.rank(op.outputs[0]))\n        out_shapes.append(tf.shape(op.outputs[0]))\n        operations.append(op)\n\n    out_ranks_val, out_shapes_val = sess.run([out_ranks, out_shapes], feed_dict=feed_options)\n\n    ret = []\n    for out_rank, out_shape, op in zip(out_ranks_val, out_shapes_val, operations):\n        if out_rank != 4 or (out_shape[1] == 1 and out_shape[2] == 1) or out_shape[0] != 1:\n            continue\n\n        ret.append(op.name)\n    return ret\n\n\ndef candidate_predict_op_names(sess, num_classes, graph, feed_options):\n    operations = []\n    out_ranks = []\n    out_shapes = []\n\n    for op in graph.get_operations():\n        if _unusable_ops(op):\n            continue\n\n        out_ranks.append(tf.rank(op.outputs[0]))\n        out_shapes.append(tf.shape(op.outputs[0]))\n        operations.append(op)\n\n    out_ranks_val, out_shapes_val = sess.run([out_ranks, out_shapes], feed_dict=feed_options)\n\n    ret = []\n    for out_rank, out_shape, op in zip(out_ranks_val, out_shapes_val, operations):\n        if out_rank == 1:\n            continue\n        if np.prod(out_shape) != num_classes:\n            continue\n\n        ret.append(op.name)\n    return ret\n'"
darkon/gradcam/gradcam.py,20,"b'# Copyright 2017 Neosapience, Inc.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ========================================================================\n""""""\nReferences\n----------\n.. [1] Ramprasaath R. Selvaraju, Michael Cogswell, Abhishek Das, Ramakrishna Vedantam, Devi Parikh, Dhruv Batra \\\n""Grad-CAM: Visual Explanations from Deep Networks via Gradient-based Localization"" ICCV2017\n\n""""""\nfrom __future__ import division\nfrom __future__ import print_function\nfrom __future__ import absolute_import\nfrom __future__ import unicode_literals\n\nimport numpy as np\nimport tensorflow as tf\nimport cv2\nfrom skimage.transform import resize as skimage_resize\n\nfrom .guided_grad import replace_grad_to_guided_grad\nfrom .candidate_ops import candidate_featuremap_op_names, candidate_predict_op_names\nfrom .candidate_ops import _unusable_ops\n\n\ndef _deprocess_image(x):\n    # Same normalization as in:\n    # https://github.com/fchollet/keras/blob/master/examples/conv_filter_visualization.py\n    if np.ndim(x) > 3:\n        x = np.squeeze(x)\n    # normalize tensor: center on 0., ensure std is 0.1\n    x -= x.mean()\n    x /= (x.std() + 1e-5)\n    x *= 0.1\n\n    # clip to [0, 1]\n    x += 0.5\n    x = np.clip(x, 0, 1)\n\n    # convert to RGB array\n    x *= 255\n    x = np.clip(x, 0, 255).astype(\'uint8\')\n    return x\n\n\nclass Gradcam:\n    """""" Gradcam Class\n\n    Parameters\n    ----------\n    x_placeholder : tf.Tensor\n        Data place holder\n        Tensor from tf.placeholder()\n    num_classes: int\n        number of classes\n    featuremap_op_name : str\n        Operation name of CNN feature map layer\n        To get the list of candidate names, use ``Gradcam.candidate_featuremap_op_names()``\n    predict_op_name : str\n        Operation name of prediction layer (decision output)\n        To get the list of candidate names, use ``Gradcam.candidate_predict_op_names()``\n    graph : tf.Graph\n        Tensorflow graph\n\n    """"""\n    def __init__(self, x_placeholder, num_classes, featuremap_op_name, predict_op_name=None, graph=None):\n        self._x_placeholder = x_placeholder\n        graph = graph if graph is not None else tf.get_default_graph()\n        self.graph = graph\n\n        predict_op_name = self._find_prob_layer(predict_op_name, graph)\n        self._prob_ts = graph.get_operation_by_name(predict_op_name).outputs[0]\n        self._target_ts = graph.get_operation_by_name(featuremap_op_name).outputs[0]\n\n        self._class_idx = tf.placeholder(tf.int32)\n        top1 = tf.argmax(tf.reshape(self._prob_ts, [-1]))\n\n        loss_by_idx = tf.reduce_sum(tf.multiply(self._prob_ts, tf.one_hot(self._class_idx, num_classes)), axis=1)\n        loss_by_top1 = tf.reduce_sum(tf.multiply(self._prob_ts, tf.one_hot(top1, num_classes)), axis=1)\n        self._grad_by_idx = self._normalize(tf.gradients(loss_by_idx, self._target_ts)[0])\n        self._grad_by_top1 = self._normalize(tf.gradients(loss_by_top1, self._target_ts)[0])\n\n        replace_grad_to_guided_grad(graph)\n\n        max_output = tf.reduce_max(self._target_ts, axis=2)\n        self._saliency_map = tf.gradients(tf.reduce_sum(max_output), x_placeholder)[0]\n\n    def gradcam(self, sess, input_data, target_index=None, feed_options=dict()):\n        """""" Calculate Grad-CAM (class activation map) and Guided Grad-CAM for given input on target class\n\n        Parameters\n        ----------\n        sess: tf.Session\n            Tensorflow session\n        input_data : numpy.ndarray\n            A single input instance\n        target_index : int\n            Target class index\n            If None, predicted class index is used\n        feed_options : dict\n            Optional parameters to graph\n\n        Returns\n        -------\n        dict\n\n        Note\n        ----\n        Keys in return:\n            * gradcam_img: Heatmap overlayed on input\n            * guided_gradcam_img: Guided Grad-CAM result\n            * heatmap: Heatmap of input on the target class\n            * guided_backprop: Guided backprop result\n\n        """"""\n        input_feed = np.expand_dims(input_data, axis=0)\n        if input_data.ndim == 3:\n            is_image = True\n            image_height, image_width = input_data.shape[:2]\n        if input_data.ndim == 1:\n            is_image = False\n            input_length = input_data.shape[0]\n\n        if target_index is not None:\n            feed_dict = {self._x_placeholder: input_feed, self._class_idx: target_index}\n            feed_dict.update(feed_options)\n            conv_out_eval, grad_eval = sess.run([self._target_ts, self._grad_by_idx], feed_dict=feed_dict)\n        else:\n            feed_dict = {self._x_placeholder: input_feed}\n            feed_dict.update(feed_options)\n            conv_out_eval, grad_eval = sess.run([self._target_ts, self._grad_by_top1], feed_dict=feed_dict)\n\n        weights = np.mean(grad_eval, axis=(0, 1, 2))\n        conv_out_eval = np.squeeze(conv_out_eval, axis=0)\n        cam = np.zeros(conv_out_eval.shape[:2], dtype=np.float32)\n\n        for i, w in enumerate(weights):\n            cam += w * conv_out_eval[:, :, i]\n\n        if is_image:\n            cam += 1\n            cam = cv2.resize(cam, (image_height, image_width))\n            saliency_val = sess.run(self._saliency_map, feed_dict={self._x_placeholder: input_feed})\n            saliency_val = np.squeeze(saliency_val, axis=0)\n        else:\n            cam = skimage_resize(cam, (input_length, 1), preserve_range=True, mode=\'reflect\')\n            cam = np.transpose(cam)\n\n        cam = np.maximum(cam, 0)\n        heatmap = cam / np.max(cam)\n\n        ret = {\'heatmap\': heatmap}\n\n        if is_image:\n            ret.update({\n                \'gradcam_img\': self.overlay_gradcam(input_data, heatmap),\n                \'guided_gradcam_img\': _deprocess_image(saliency_val * heatmap[..., None]),\n                \'guided_backprop\': saliency_val\n            })\n        return ret\n\n    @staticmethod\n    def candidate_featuremap_op_names(sess, graph=None, feed_options=None):\n        """""" Returns the list of candidates for operation names of CNN feature map layer\n\n        Parameters\n        ----------\n        sess: tf.Session\n            Tensorflow session\n        graph: tf.Graph\n            Tensorflow graph\n        feed_options: dict\n            Optional parameters to graph\n        Returns\n        -------\n        list\n            String list of candidates\n\n        """"""\n        graph = graph if graph is not None else tf.get_default_graph()\n        feed_options = feed_options if feed_options is not None else {}\n        return candidate_featuremap_op_names(sess, graph, feed_options)\n\n    @staticmethod\n    def candidate_predict_op_names(sess, num_classes, graph=None, feed_options=None):\n        """""" Returns the list of candidate for operation names of prediction layer\n\n        Parameters\n        ----------\n        sess: tf.Session\n            Tensorflow session\n        num_classes: int\n            Number of prediction classes\n        graph: tf.Graph\n            Tensorflow graph\n        feed_options: dict\n            Optional parameters to graph\n        Returns\n        -------\n        list\n            String list of candidates\n\n        """"""\n        graph = graph if graph is not None else tf.get_default_graph()\n        feed_options = feed_options if feed_options is not None else {}\n        return candidate_predict_op_names(sess, num_classes, graph, feed_options)\n\n    @staticmethod\n    def overlay_gradcam(image, heatmap):\n        """""" Overlay heatmap on input data\n        """"""\n        output_image = np.array(image)\n        output_image -= np.min(output_image)\n        output_image = np.minimum(output_image, 255)\n\n        cam = cv2.applyColorMap(np.uint8(255*heatmap), cv2.COLORMAP_JET)\n        output_image = np.float32(cam) + np.float32(output_image)\n        output_image = 255 * output_image / np.max(output_image)\n        output_image = np.uint8(output_image)\n        return output_image\n\n    @staticmethod\n    def _find_prob_layer(output_name, graph):\n        if output_name is not None:\n            return output_name\n\n        for op in graph.get_operations():\n            if _unusable_ops(op):\n                continue\n\n            output_name = op.name\n        return output_name\n\n    @staticmethod\n    def _normalize(x):\n        return tf.div(x, (tf.sqrt(tf.reduce_mean(tf.square(x), axis=1)) + 1e-5))\n'"
darkon/gradcam/guided_grad.py,2,"b'# Copyright 2017 Neosapience, Inc.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ========================================================================\nfrom __future__ import division\nfrom __future__ import print_function\nfrom __future__ import absolute_import\nfrom __future__ import unicode_literals\n\nimport tensorflow as tf\nfrom tensorflow.python.framework import ops\nfrom tensorflow.python.ops import gen_nn_ops\nfrom tensorflow.python.ops import math_grad\nfrom tensorflow.core.framework import attr_value_pb2\nfrom tensorflow.python.util import compat\nfrom tensorflow.contrib.graph_editor import subgraph\n\n\n_grad_override_map = {\n    \'Tanh\': \'GuidedTanh\',\n    \'Sigmoid\': \'GuidedSigmoid\',\n    \'Relu\': \'GuidedRelu\',\n    \'Relu6\': \'GuidedRelu6\',\n    \'Elu\': \'GuidedElu\',\n    \'Selu\': \'GuidedSelu\',\n    \'Softplus\': \'GuidedSoftplus\',\n    \'Softsign\': \'GuidedSoftsign\',\n}\n\n\ndef replace_grad_to_guided_grad(g):\n    sgv = subgraph.make_view(g)\n    with g.gradient_override_map(_grad_override_map):\n        for op in sgv.ops:\n            _replace_grad(g, op)\n\n\ndef _replace_grad(g, op):\n    # ref: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/framework/ops.py\n    # tf.Graph._gradient_override_map\n    try:\n        op_def = op._op_def\n        node_def = op._node_def\n\n        if op_def is not None:\n            mapped_op_type = g._gradient_override_map[op_def.name]\n            node_def.attr[""_gradient_op_type""].CopyFrom(\n                attr_value_pb2.AttrValue(s=compat.as_bytes(mapped_op_type)))\n    except KeyError:\n        pass\n\n\ndef guided_grad(grad):\n    return tf.where(0. < grad, grad, tf.zeros_like(grad))\n\n\n@ops.RegisterGradient(""GuidedTanh"")\ndef _guided_grad_tanh(op, grad):\n    return guided_grad(math_grad._TanhGrad(op, grad))\n\n\n@ops.RegisterGradient(""GuidedSigmoid"")\ndef _guided_grad_sigmoid(op, grad):\n    return guided_grad(math_grad._SigmoidGrad(op, grad))\n\n\n@ops.RegisterGradient(""GuidedRelu"")\ndef _guided_grad_relu(op, grad):\n    return guided_grad(gen_nn_ops._relu_grad(grad, op.outputs[0]))\n\n\n@ops.RegisterGradient(""GuidedRelu6"")\ndef _guided_grad_relu6(op, grad):\n    return guided_grad(gen_nn_ops._relu6_grad(grad, op.outputs[0]))\n\n\n@ops.RegisterGradient(""GuidedElu"")\ndef _guided_grad_elu(op, grad):\n    return guided_grad(gen_nn_ops._elu_grad(grad, op.outputs[0]))\n\n\n@ops.RegisterGradient(""GuidedSelu"")\ndef _guided_grad_selu(op, grad):\n    return guided_grad(gen_nn_ops._selu_grad(grad, op.outputs[0]))\n\n\n@ops.RegisterGradient(""GuidedSoftplus"")\ndef _guided_grad_softplus(op, grad):\n    return guided_grad(gen_nn_ops._softplus_grad(grad, op.outputs[0]))\n\n\n@ops.RegisterGradient(""GuidedSoftsign"")\ndef _guided_grad_softsign(op, grad):\n    return guided_grad(gen_nn_ops._softsign_grad(grad, op.outputs[0]))\n'"
darkon/influence/__init__.py,0,"b'# Copyright 2017 Neosapience, Inc.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ========================================================================\nfrom . import feeder\nfrom . import influence\n\n__all__ = [""feeder"", ""influence""]\n'"
darkon/influence/feeder.py,0,"b'# Copyright 2017 Neosapience, Inc.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ========================================================================\n"""""" Feeding interface for Influence class\n\n""""""\nfrom __future__ import absolute_import\nfrom __future__ import unicode_literals\n\nimport abc\nimport six\n\n\n@six.add_metaclass(abc.ABCMeta)\nclass InfluenceFeeder:\n    @abc.abstractmethod\n    def reset(self):\n        """""" reset dataset\n        """"""\n        raise RuntimeError(\'must be implemented\')\n\n    @abc.abstractmethod\n    def train_batch(self, batch_size):\n        """""" training data feeder by batch sampling\n\n        Parameters\n        ----------\n        batch_size : batch size\n\n        Returns\n        -------\n        xs : feed input values\n        ys : feed label values\n\n        """"""\n        raise RuntimeError(\'must be implemented\')\n\n    @abc.abstractmethod\n    def train_one(self, index):\n        """""" single training data feeder\n\n        Parameters\n        ----------\n        index : training sample index\n\n        Returns\n        -------\n        x : feed one input value\n        y : feed one label value\n\n        """"""\n        raise RuntimeError(\'must be implemented\')\n\n    @abc.abstractmethod\n    def test_indices(self, indices):\n        """""" test data feeder\n\n        Parameters\n        ----------\n        indices : testing sample index\n\n        Returns\n        -------\n        x : feed input values\n        y : feed label values\n\n        """"""\n        raise RuntimeError(\'must be implemented\')\n'"
darkon/influence/influence.py,26,"b'# Copyright 2017 Neosapience, Inc.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ========================================================================\n""""""\nReferences\n----------\n.. [1] Pang Wei Koh and Percy Liang ""Understanding Black-box Predictions via Influence Functions"" ICML2017\n\n""""""\nfrom __future__ import division\nfrom __future__ import print_function\nfrom __future__ import absolute_import\nfrom __future__ import unicode_literals\n\nfrom .feeder import InfluenceFeeder  # noqa: ignore=F401\nfrom ..log import logger\n\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow.python.ops.gradients_impl import _hessian_vector_product\n\nimport os\nimport time\nimport hashlib\nimport json\nfrom functools import wraps\n\n_using_fully_tf = True\n\n\ndef _timing(f):\n    @wraps(f)\n    def wrap(*args, **kwargs):\n        time1 = time.time()\n        ret = f(*args, **kwargs)\n        time2 = time.time()\n        logger.debug(\'* %s function took [%.3fs]\' % (f.__name__, time2-time1))\n        return ret\n    return wrap\n\n\nclass Influence:\n    """""" Influence Class\n\n    Parameters\n    ----------\n    workspace: str\n        Path for workspace directory\n    feeder : InfluenceFeeder\n        Dataset feeder\n    loss_op_train : tf.Operation\n        Tensor for loss function used for training. it may includes regularization.\n    loss_op_test : tf.Operation\n        Tensor for loss function for inference.\n    x_placeholder : tf.Tensor\n        Data place holder\n        Tensor from tf.placeholder()\n    y_placeholder : tf.Tensor\n        Target place holder\n        Tensor from tf.placeholder()\n    test_feed_options : dict\n        Optional parameters to run loss operation in testset\n    train_feed_options : dict\n        Optional parameters to run loss operation in trainset\n    trainable_variables : tuple, or list\n        Trainable variables to be used\n        If None, all variables are trainable\n        Default: None\n\n\n    """"""\n    def __init__(self, workspace, feeder, loss_op_train, loss_op_test, x_placeholder, y_placeholder,\n                 test_feed_options=None, train_feed_options=None, trainable_variables=None):\n        self.workspace = workspace\n        self.feeder = feeder\n        self.x_placeholder = x_placeholder\n        self.y_placeholder = y_placeholder\n        self.test_feed_options = test_feed_options if test_feed_options else dict()\n        self.train_feed_options = train_feed_options if train_feed_options else dict()\n\n        if trainable_variables is None:\n            trainable_variables = (\n                tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES) +\n                tf.get_collection(tf.GraphKeys.TRAINABLE_RESOURCE_VARIABLES))\n\n        self.loss_op_train = loss_op_train\n        self.grad_op_train = tf.gradients(loss_op_train, trainable_variables)\n        self.grad_op_test = tf.gradients(loss_op_test, trainable_variables)\n\n        self.v_cur_estimated = [tf.placeholder(tf.float32, shape=a.get_shape()) for a in trainable_variables]\n        self.v_test_grad = [tf.placeholder(tf.float32, shape=a.get_shape()) for a in trainable_variables]\n        self.v_ihvp = tf.placeholder(tf.float64, shape=[None])\n        self.v_param_damping = tf.placeholder(tf.float32)\n        self.v_param_scale = tf.placeholder(tf.float32)\n        self.v_param_total_trainset = tf.placeholder(tf.float64)\n\n        self.inverse_hvp = None\n        self.trainable_variables = trainable_variables\n\n        with tf.name_scope(\'darkon_ihvp\'):\n            self.hessian_vector_op = _hessian_vector_product(loss_op_train, trainable_variables, self.v_cur_estimated)\n            self.estimation_op = [\n                a + (b * self.v_param_damping) - (c / self.v_param_scale)\n                for a, b, c in zip(self.v_test_grad, self.v_cur_estimated, self.hessian_vector_op)\n            ]\n\n        with tf.name_scope(\'darkon_grad_diff\'):\n            flatten_inverse_hvp = tf.reshape(self.v_ihvp, shape=(-1, 1))\n            flatten_grads = tf.concat([tf.reshape(a, (-1,)) for a in self.grad_op_train], 0)\n            flatten_grads = tf.reshape(flatten_grads, shape=(1, -1,))\n            flatten_grads = tf.cast(flatten_grads, tf.float64)\n            flatten_grads /= self.v_param_total_trainset\n            self.grad_diff_op = tf.matmul(flatten_grads, flatten_inverse_hvp)\n\n        self.ihvp_config = {\n            \'scale\': 1e4,\n            \'damping\': 0.01,\n            \'num_repeats\': 1,\n            \'recursion_batch_size\': 10,\n            \'recursion_depth\': 10000\n        }\n\n        if not os.path.exists(self.workspace):\n            os.makedirs(self.workspace)\n\n    @_timing\n    def upweighting_influence(self, sess, test_indices, test_batch_size, approx_params,\n                              train_indices, num_total_train_example, force_refresh=False):\n        """""" Calculate influence score of given training samples that affect on the test samples\n         Negative value indicates bad effect on the test loss\n\n        Parameters\n        ----------\n        sess: tf.Session\n            Tensorflow session\n        test_indices: list\n            Test samples to be used. Influence on these samples are calculated.\n        test_batch_size: int\n            batch size for test samples\n        approx_params: dict\n            Parameters for inverse hessian vector product approximation\n            Default:\n                {\'scale\': 1e4,\n                \'damping\': 0.01,\n                \'num_repeats\': 1,\n                \'recursion_batch_size\': 10,\n                \'recursion_depth\': 10000}\n        train_indices: list\n            Training samples indices to be calculated.\n        num_total_train_example: int\n            Number of total training samples used for training,\n            which might be different from the size of train_indices\n        force_refresh: bool\n            If False, it calculates only when test samples and parameters are changed.\n            Default: False\n\n        Returns\n        -------\n        numpy.ndarray\n\n        """"""\n        self._prepare(sess, test_indices, test_batch_size, approx_params, force_refresh)\n\n        self.feeder.reset()\n        score = self._grad_diffs(sess, train_indices, num_total_train_example)\n        logger.info(\'Multiplying by %s train examples\' % score.size)\n        return score\n\n    @_timing\n    def upweighting_influence_batch(self, sess, test_indices, test_batch_size, approx_params,\n                                    train_batch_size, train_iterations, subsamples=-1, force_refresh=False):\n        """""" Iteratively calculate influence scores for training data sampled by batch sampler\n        Negative value indicates bad effect on the test loss\n\n        Parameters\n        ----------\n        sess: tf.Session\n            Tensorflow session\n        test_indices: list\n            Test samples to be used. Influence on these samples are calculated.\n        test_batch_size: int\n            batch size for test samples\n        approx_params: dict\n            Parameters for inverse hessian vector product approximation\n            Default:\n                {\'scale\': 1e4,\n                \'damping\': 0.01,\n                \'num_repeats\': 1,\n                \'recursion_batch_size\': 10,\n                \'recursion_depth\': 10000}\n        train_batch_size: int\n            Batch size of training samples\n        train_iterations: int\n            Number of iterations\n        subsamples: int\n            Number of training samples in a batch to be calculated.\n            If -1, all samples are calculated (no subsampling).\n            Default: -1\n        force_refresh: bool\n            If False, it calculates only when test samples and parameters are changed.\n            Default: False\n\n        Returns\n        -------\n        numpy.ndarray\n\n        """"""\n        self._prepare(sess, test_indices, test_batch_size, approx_params, force_refresh)\n\n        self.feeder.reset()\n        score = self._grad_diffs_all(sess, train_batch_size, train_iterations, subsamples)\n        logger.info(\'Multiplying by %s train examples\' % score.size)\n        return score\n\n    @_timing\n    def _prepare(self, sess, test_indices, test_batch_size, approx_params, force_refresh):\n        """""" Calculate inverse hessian vector product, and save it in workspace\n\n        Parameters\n        ----------\n        sess: tf.Session\n            Tensorflow session\n        test_indices: list\n            Test samples to be used. Influence on these samples are calculated.\n        test_batch_size: int\n            batch size for test samples\n        force_refresh: bool\n            If False, it calculates only when test samples and parameters are changed.\n            Default: False\n        approx_params: dict\n            Parameters for inverse hessian vector product approximation\n\n        """"""\n        # update ihvp approx params\n        if approx_params is not None:\n            for param_key in approx_params.keys():\n                if param_key not in self.ihvp_config:\n                    raise RuntimeError(\'unknown ihvp config param is approx_params\')\n            self.ihvp_config.update(approx_params)\n\n        inv_hvp_path = self._path(self._approx_filename(sess, test_indices))\n        if not os.path.exists(inv_hvp_path) or force_refresh:\n            self.feeder.reset()\n            test_grad_loss = self._get_test_grad_loss(sess, test_indices, test_batch_size)\n            logger.info(\'Norm of test gradient: %s\' % np.linalg.norm(np.concatenate([a.reshape(-1) for a in test_grad_loss])))\n            self.inverse_hvp = self._get_inverse_hvp_lissa(sess, test_grad_loss)\n            np.savez(inv_hvp_path, inverse_hvp=self.inverse_hvp, encoding=\'bytes\')\n            logger.info(\'Saved inverse HVP to %s\' % inv_hvp_path)\n        else:\n            self.inverse_hvp = np.load(inv_hvp_path, encoding=\'bytes\')[\'inverse_hvp\']\n            logger.info(\'Loaded inverse HVP from %s\' % inv_hvp_path)\n\n    def _get_test_grad_loss(self, sess, test_indices, test_batch_size):\n        if test_indices is not None:\n            num_iter = int(np.ceil(len(test_indices) / test_batch_size))\n            test_grad_loss = None\n            for i in range(num_iter):\n                start = i * test_batch_size\n                end = int(min((i + 1) * test_batch_size, len(test_indices)))\n                size = float(end - start)\n\n                test_feed_dict = self._make_test_feed_dict(*self.feeder.test_indices(test_indices[start:end]))\n                temp = sess.run(self.grad_op_test, feed_dict=test_feed_dict)\n                temp = np.asarray(temp)\n\n                temp *= size\n                if test_grad_loss is None:\n                    test_grad_loss = temp\n                else:\n                    test_grad_loss += temp\n\n            test_grad_loss /= len(test_indices)\n        else:\n            raise RuntimeError(\'unsupported yet\')\n        return test_grad_loss\n\n    def _approx_filename(self, sess, test_indices):\n        sha = hashlib.sha1()\n\n        # weights\n        vs = sess.run(self.trainable_variables)\n        for a in vs:\n            sha.update(a.data)\n\n        # test_indices\n        np_test_indices = np.array(list(test_indices))\n        sha.update(np_test_indices.data)\n\n        # approx_params\n        sha.update(json.dumps(self.ihvp_config, sort_keys=True).encode(\'utf-8\'))\n        return \'ihvp.\' + sha.hexdigest() + \'.npz\'\n\n    def _get_inverse_hvp_lissa(self, sess, test_grad_loss):\n        ihvp_config = self.ihvp_config\n        print_iter = ihvp_config[\'recursion_depth\'] / 10\n\n        inverse_hvp = None\n        for _ in range(ihvp_config[\'num_repeats\']):\n            cur_estimate = test_grad_loss\n            # debug_diffs_estimation = []\n            # prev_estimation_norm = np.linalg.norm(np.concatenate([a.reshape(-1) for a in cur_estimate]))\n\n            for j in range(ihvp_config[\'recursion_depth\']):\n                train_batch_data, train_batch_label = self.feeder.train_batch(ihvp_config[\'recursion_batch_size\'])\n                feed_dict = self._make_train_feed_dict(train_batch_data, train_batch_label)\n                feed_dict = self._update_feed_dict(feed_dict, cur_estimate, test_grad_loss)\n\n                if _using_fully_tf:\n                    feed_dict.update({\n                        self.v_param_damping: 1 - self.ihvp_config[\'damping\'],\n                        self.v_param_scale: self.ihvp_config[\'scale\']\n                    })\n                    cur_estimate = sess.run(self.estimation_op, feed_dict=feed_dict)\n                else:\n                    hessian_vector_val = sess.run(self.hessian_vector_op, feed_dict=feed_dict)\n                    hessian_vector_val = np.array(hessian_vector_val)\n                    cur_estimate = test_grad_loss + (1 - ihvp_config[\'damping\']) * cur_estimate - hessian_vector_val / ihvp_config[\'scale\']\n\n                # curr_estimation_norm = np.linalg.norm(np.concatenate([a.reshape(-1) for a in cur_estimate]))\n                # debug_diffs_estimation.append(curr_estimation_norm - prev_estimation_norm)\n                # prev_estimation_norm = curr_estimation_norm\n\n                if (j % print_iter == 0) or (j == ihvp_config[\'recursion_depth\'] - 1):\n                    logger.info(""Recursion at depth %s: norm is %.8lf"" %\n                                (j, np.linalg.norm(np.concatenate([a.reshape(-1) for a in cur_estimate]))))\n\n            if inverse_hvp is None:\n                inverse_hvp = np.array(cur_estimate) / ihvp_config[\'scale\']\n            else:\n                inverse_hvp += np.array(cur_estimate) / ihvp_config[\'scale\']\n\n            # np.savetxt(self._path(\'debug_diffs_estimation_{}.txt\'.format(sample_idx)), debug_diffs_estimation)\n\n        inverse_hvp /= ihvp_config[\'num_repeats\']\n        return inverse_hvp\n\n    def _update_feed_dict(self, feed_dict, cur_estimated, test_grad_loss):\n        for placeholder, var in zip(self.v_cur_estimated, cur_estimated):\n            feed_dict[placeholder] = var\n\n        for placeholder, var in zip(self.v_test_grad, test_grad_loss):\n            feed_dict[placeholder] = var\n        return feed_dict\n\n    @_timing\n    def _grad_diffs(self, sess, train_indices, num_total_train_example):\n        inverse_hvp = np.concatenate([a.reshape(-1) for a in self.inverse_hvp])\n\n        num_to_remove = len(train_indices)\n        predicted_grad_diffs = np.zeros([num_to_remove])\n\n        for counter, idx_to_remove in enumerate(train_indices):\n            single_data, single_label = self.feeder.train_one(idx_to_remove)\n            feed_dict = self._make_train_feed_dict([single_data], [single_label])\n            predicted_grad_diffs[counter] = self._grad_diff(sess, feed_dict, num_total_train_example, inverse_hvp)\n\n            if (counter % 1000) == 0:\n                logger.info(\'counter: {} / {}\'.format(counter, num_to_remove))\n\n        return predicted_grad_diffs\n\n    @_timing\n    def _grad_diffs_all(self, sess, train_batch_size, num_iters, num_subsampling):\n        num_total_train_example = num_iters * train_batch_size\n        if num_subsampling > 0:\n            num_diffs = num_iters * num_subsampling\n        else:\n            num_diffs = num_iters * train_batch_size\n\n        inverse_hvp = np.concatenate([a.reshape(-1) for a in self.inverse_hvp])\n        predicted_grad_diffs = np.zeros([num_diffs])\n\n        counter = 0\n        for it in range(num_iters):\n            train_batch_data, train_batch_label = self.feeder.train_batch(train_batch_size)\n\n            if num_subsampling > 0:\n                for idx in range(num_subsampling):\n                    feed_dict = self._make_train_feed_dict(train_batch_data[idx:idx + 1], train_batch_label[idx:idx + 1])\n                    predicted_grad_diffs[counter] = self._grad_diff(sess, feed_dict, num_total_train_example, inverse_hvp)\n                    counter += 1\n            else:\n                for single_data, single_label in zip(train_batch_data, train_batch_label):\n                    feed_dict = self._make_train_feed_dict([single_data], [single_label])\n                    predicted_grad_diffs[counter] = self._grad_diff(sess, feed_dict, num_total_train_example, inverse_hvp)\n                    counter += 1\n\n            if (it % 100) == 0:\n                logger.info(\'iter: {}/{}\'.format(it, num_iters))\n\n        return predicted_grad_diffs\n\n    def _grad_diff(self, sess, feed_dict, num_total_train_example, inverse_hvp):\n        if _using_fully_tf:\n            feed_dict.update({\n                self.v_ihvp: inverse_hvp,\n                self.v_param_total_trainset: num_total_train_example\n            })\n            return sess.run(self.grad_diff_op, feed_dict=feed_dict)\n        else:\n            train_grads = sess.run(self.grad_op_train, feed_dict=feed_dict)\n            train_grads = np.concatenate([a.reshape(-1) for a in train_grads])\n            train_grads /= num_total_train_example\n            return np.dot(inverse_hvp, train_grads)\n\n    def _make_test_feed_dict(self, xs, ys):\n        ret = {\n            self.x_placeholder: xs,\n            self.y_placeholder: ys,\n        }\n        ret.update(self.test_feed_options)\n        return ret\n\n    def _make_train_feed_dict(self, xs, ys):\n        ret = {\n            self.x_placeholder: xs,\n            self.y_placeholder: ys,\n        }\n        ret.update(self.train_feed_options)\n        return ret\n\n    def _path(self, *paths):\n        return os.path.join(self.workspace, *paths)\n'"
docs/src/conf.py,0,"b'# -*- coding: utf-8 -*-\n#\n# darkon documentation build configuration file, created by\n# sphinx-quickstart on Fri Nov 17 09:41:22 2017.\n#\n# This file is execfile()d with the current directory set to its\n# containing dir.\n#\n# Note that not all possible configuration values are present in this\n# autogenerated file.\n#\n# All configuration values have a default; values that are commented out\n# serve to show the default.\n\n# If extensions (or modules to document with autodoc) are in another directory,\n# add these directories to sys.path here. If the directory is relative to the\n# documentation root, use os.path.abspath to make it absolute, like shown here.\n#\nimport os\nimport sys\npkg_root = os.path.abspath(\'../../\')\nsys.path.insert(0, pkg_root)\nimport json\n\n# -- General configuration ------------------------------------------------\n\n# If your documentation needs a minimal Sphinx version, state it here.\n#\n# needs_sphinx = \'1.0\'\n\n# Add any Sphinx extension module names here, as strings. They can be\n# extensions coming with Sphinx (named \'sphinx.ext.*\') or your custom\n# ones.\nextensions = [\'sphinx.ext.autodoc\',\n    \'sphinx.ext.intersphinx\',\n    \'sphinx.ext.imgmath\',\n    \'sphinx.ext.viewcode\',\n    \'sphinx.ext.githubpages\']\n\nextensions += [\n        \'sphinx.ext.napoleon\',\n        \'m2r\'\n        ]\n\n# Napoleon settings\nnapoleon_google_docstring = True\nnapoleon_numpy_docstring = True\nnapoleon_include_init_with_doc = False\nnapoleon_include_private_with_doc = False\nnapoleon_include_special_with_doc = True\nnapoleon_use_admonition_for_examples = False\nnapoleon_use_admonition_for_notes = False\nnapoleon_use_admonition_for_references = False\nnapoleon_use_ivar = False\nnapoleon_use_param = True\nnapoleon_use_rtype = False\n\n# Add any paths that contain templates here, relative to this directory.\ntemplates_path = [\'_templates\']\n\n# The suffix(es) of source filenames.\n# You can specify multiple suffix as a list of string:\n#\nsource_suffix = [\'.rst\', \'.md\']\n#source_suffix = \'.rst\'\n\n# The master toctree document.\nmaster_doc = \'index\'\n\ninfo = json.load(open(os.path.join(pkg_root, \'info.json\')))\n\n# General information about the project.\nproject = info[\'name\']\ncopyright = info[\'copyright\']\nauthor = info[\'authors\']\n\n# The version info for the project you\'re documenting, acts as replacement for\n# |version| and |release|, also used in various other places throughout the\n# built documents.\n#\n# The short X.Y version.\nversion = info[\'version\']\n# The full version, including alpha/beta/rc tags.\nrelease = info[\'version\']\n\n# The language for content autogenerated by Sphinx. Refer to documentation\n# for a list of supported languages.\n#\n# This is also used if you do content translation via gettext catalogs.\n# Usually you set ""language"" from the command line for these cases.\nlanguage = None\n\n# List of patterns, relative to source directory, that match files and\n# directories to ignore when looking for source files.\n# This patterns also effect to html_static_path and html_extra_path\nexclude_patterns = [\n    \'_build\',\n    \'Thumbs.db\',\n    \'.DS_Store\',\n    \'README.md\'\n]\n\n# The name of the Pygments (syntax highlighting) style to use.\npygments_style = \'sphinx\'\n\n# If true, `todo` and `todoList` produce output, else they produce nothing.\ntodo_include_todos = False\n\n\n# -- Options for HTML output ----------------------------------------------\n\n# The theme to use for HTML and HTML Help pages.  See the documentation for\n# a list of builtin themes.\n#\nhtml_theme = \'alabaster\'\n\n# Theme options are theme-specific and customize the look and feel of a theme\n# further.  For a list of options available for each theme, see the\n# documentation.\n#\nhtml_theme_options = {\n    \'logo\': \'logo.png\',\n    \'github_user\': \'darkonhub\',\n    \'github_repo\': \'darkon\',\n    \'github_button\': True,\n    \'github_type\': \'star\',\n    \'github_banner\': True,\n    \'analytics_id\': \'UA-109936098-1\',\n    \'description\': \'Toolkit to Hack Your Deep Learning Models.\',\n    \'code_font_family\': (\'monaco\', \'Consolas\', \'Menlo\', \'Deja Vu Sans Mono\', \'Bitstream Vera Sans Mono\', \'monospace\'),\n    \'fixed_sidebar\': True,\n}\n# Add any paths that contain custom static files (such as style sheets) here,\n# relative to this directory. They are copied after the builtin static files,\n# so a file named ""default.css"" will overwrite the builtin ""default.css"".\nhtml_static_path = [\'_static\']\n\n# Custom sidebar templates, must be a dictionary that maps document names\n# to template names.\n#\n# This is required for the alabaster theme\n# refs: http://alabaster.readthedocs.io/en/latest/installation.html#sidebars\nhtml_sidebars = {\n    \'**\': [\n        \'about.html\',\n        \'navigation.html\',\n        \'relations.html\',  # needs \'show_related\': True theme option to display\n        \'searchbox.html\',\n    ]\n}\n\nhtml_favicon = ""_static/favicon.ico""\n\n\n# -- Options for HTMLHelp output ------------------------------------------\n\n# Output file base name for HTML help builder.\nhtmlhelp_basename = \'darkondoc\'\n\n\n# -- Options for LaTeX output ---------------------------------------------\n\nlatex_elements = {\n    # The paper size (\'letterpaper\' or \'a4paper\').\n    #\n    # \'papersize\': \'letterpaper\',\n\n    # The font size (\'10pt\', \'11pt\' or \'12pt\').\n    #\n    # \'pointsize\': \'10pt\',\n\n    # Additional stuff for the LaTeX preamble.\n    #\n    # \'preamble\': \'\',\n\n    # Latex figure (float) alignment\n    #\n    # \'figure_align\': \'htbp\',\n}\n\n# Grouping the document tree into LaTeX files. List of tuples\n# (source start file, target name, title,\n#  author, documentclass [howto, manual, or own class]).\nlatex_documents = [\n    (master_doc, \'darkon.tex\', u\'darkon Documentation\',\n     u\'Neosapience, Inc.\', \'manual\'),\n]\n\n\n# -- Options for manual page output ---------------------------------------\n\n# One entry per manual page. List of tuples\n# (source start file, name, description, authors, manual section).\nman_pages = [\n    (master_doc, \'darkon\', u\'darkon Documentation\',\n     [author], 1)\n]\n\n\n# -- Options for Texinfo output -------------------------------------------\n\n# Grouping the document tree into Texinfo files. List of tuples\n# (source start file, target name, title, author,\n#  dir menu entry, description, category)\ntexinfo_documents = [\n    (master_doc, \'darkon\', u\'darkon Documentation\',\n     author, \'darkon\', \'One line description of project.\',\n     \'Miscellaneous\'),\n]\n\n\n\n\n# Example configuration for intersphinx: refer to the Python standard library.\nintersphinx_mapping = {\n    \'https://docs.python.org/\': None,\n    \'numpy\': (\'http://docs.scipy.org/doc/numpy/\', None),\n}\n\n\nhighlight_language = \'python\'\nadd_module_names = False\n\n'"
