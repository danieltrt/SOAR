file_path,api_count,code
freeze_model.py,5,"b'import os, argparse\nimport tensorflow as tf\nfrom tensorflow.python.framework import graph_util\n\ndir = os.path.dirname(os.path.realpath(__file__))\n\n\ndef freeze_graph(model_folder):\n    # We retrieve our checkpoint fullpath\n    checkpoint = tf.train.get_checkpoint_state(model_folder)\n    input_checkpoint = checkpoint.model_checkpoint_path\n\n    # We precise the file fullname of our freezed graph\n    absolute_model_folder = \'/\'.join(input_checkpoint.split(\'/\')[:-1])\n    output_graph = absolute_model_folder + \'/frozen_model.pb\'\n\n    # Before exporting our graph, we need to precise what is our output node\n    # This is how TF decides what part of the Graph he has to keep and what part it can dump\n    # NOTE: this variable is plural, because you can have multiple output nodes\n    output_node_names = \'generate_output/output\'\n\n    # We clear devices to allow TensorFlow to control on which device it will load operations\n    clear_devices = True\n\n    # We import the meta graph and retrieve a Saver\n    saver = tf.train.import_meta_graph(input_checkpoint + \'.meta\', clear_devices=clear_devices)\n\n    # We retrieve the protobuf graph definition\n    graph = tf.get_default_graph()\n    input_graph_def = graph.as_graph_def()\n\n    # We start a session and restore the graph weights\n    with tf.Session() as sess:\n        saver.restore(sess, input_checkpoint)\n\n        # We use a built-in TF helper to export variables to constants\n        output_graph_def = graph_util.convert_variables_to_constants(\n            sess,  # The session is used to retrieve the weights\n            input_graph_def,  # The graph_def is used to retrieve the nodes\n            output_node_names.split("","")  # The output node names are used to select the usefull nodes\n        )\n\n        # Finally we serialize and dump the output graph to the filesystem\n        with tf.gfile.GFile(output_graph, \'wb\') as f:\n            f.write(output_graph_def.SerializeToString())\n        print(\'%d ops in the final graph.\' % len(output_graph_def.node))\n\n\nif __name__ == \'__main__\':\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\'--model-folder\', type=str, help=\'Model folder to export\')\n    args = parser.parse_args()\n\n    freeze_graph(args.model_folder)\n'"
generate_train_data.py,0,"b'import os\nimport cv2\nimport dlib\nimport time\nimport argparse\nimport numpy as np\nfrom imutils import video\n\nDOWNSAMPLE_RATIO = 4\n\n\ndef reshape_for_polyline(array):\n    return np.array(array, np.int32).reshape((-1, 1, 2))\n\n\ndef main():\n    os.makedirs(\'original\', exist_ok=True)\n    os.makedirs(\'landmarks\', exist_ok=True)\n\n    cap = cv2.VideoCapture(args.filename)\n    fps = video.FPS().start()\n\n    count = 0\n    while cap.isOpened():\n        ret, frame = cap.read()\n\n        frame_resize = cv2.resize(frame, None, fx=1 / DOWNSAMPLE_RATIO, fy=1 / DOWNSAMPLE_RATIO)\n        gray = cv2.cvtColor(frame_resize, cv2.COLOR_BGR2GRAY)\n        faces = detector(gray, 1)\n        black_image = np.zeros(frame.shape, np.uint8)\n\n        t = time.time()\n\n        # Perform if there is a face detected\n        if len(faces) == 1:\n            for face in faces:\n                detected_landmarks = predictor(gray, face).parts()\n                landmarks = [[p.x * DOWNSAMPLE_RATIO, p.y * DOWNSAMPLE_RATIO] for p in detected_landmarks]\n\n                jaw = reshape_for_polyline(landmarks[0:17])\n                left_eyebrow = reshape_for_polyline(landmarks[22:27])\n                right_eyebrow = reshape_for_polyline(landmarks[17:22])\n                nose_bridge = reshape_for_polyline(landmarks[27:31])\n                lower_nose = reshape_for_polyline(landmarks[30:35])\n                left_eye = reshape_for_polyline(landmarks[42:48])\n                right_eye = reshape_for_polyline(landmarks[36:42])\n                outer_lip = reshape_for_polyline(landmarks[48:60])\n                inner_lip = reshape_for_polyline(landmarks[60:68])\n\n                color = (255, 255, 255)\n                thickness = 3\n\n                cv2.polylines(black_image, [jaw], False, color, thickness)\n                cv2.polylines(black_image, [left_eyebrow], False, color, thickness)\n                cv2.polylines(black_image, [right_eyebrow], False, color, thickness)\n                cv2.polylines(black_image, [nose_bridge], False, color, thickness)\n                cv2.polylines(black_image, [lower_nose], True, color, thickness)\n                cv2.polylines(black_image, [left_eye], True, color, thickness)\n                cv2.polylines(black_image, [right_eye], True, color, thickness)\n                cv2.polylines(black_image, [outer_lip], True, color, thickness)\n                cv2.polylines(black_image, [inner_lip], True, color, thickness)\n\n            # Display the resulting frame\n            count += 1\n            print(count)\n            cv2.imwrite(""original/{}.png"".format(count), frame)\n            cv2.imwrite(""landmarks/{}.png"".format(count), black_image)\n            fps.update()\n\n            print(\'[INFO] elapsed time: {:.2f}\'.format(time.time() - t))\n\n            if count == args.number:  # only take 400 photos\n                break\n            elif cv2.waitKey(1) & 0xFF == ord(\'q\'):\n                break\n        else:\n            print(""No face detected"")\n\n    fps.stop()\n    print(\'[INFO] elapsed time (total): {:.2f}\'.format(fps.elapsed()))\n    print(\'[INFO] approx. FPS: {:.2f}\'.format(fps.fps()))\n\n    cap.release()\n    cv2.destroyAllWindows()\n\n\nif __name__ == \'__main__\':\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\'--file\', dest=\'filename\', type=str, help=\'Name of the video file.\')\n    parser.add_argument(\'--num\', dest=\'number\', type=int, help=\'Number of train data to be created.\')\n    parser.add_argument(\'--landmark-model\', dest=\'face_landmark_shape_file\', type=str, help=\'Face landmark model file.\')\n    args = parser.parse_args()\n\n    # Create the face predictor and landmark predictor\n    detector = dlib.get_frontal_face_detector()\n    predictor = dlib.shape_predictor(args.face_landmark_shape_file)\n\n    main()\n'"
reduce_model.py,46,"b'import argparse\nimport tensorflow as tf\n\nCROP_SIZE = 256  # scale_size = CROP_SIZE\nngf = 64\nndf = 64\n\n\ndef preprocess(image):\n    with tf.name_scope(\'preprocess\'):\n        # [0, 1] => [-1, 1]\n        return image * 2 - 1\n\n\ndef deprocess(image):\n    with tf.name_scope(\'deprocess\'):\n        # [-1, 1] => [0, 1]\n        return (image + 1) / 2\n\n\ndef gen_conv(batch_input, out_channels):\n    # [batch, in_height, in_width, in_channels] => [batch, out_height, out_width, out_channels]\n    initializer = tf.random_normal_initializer(0, 0.02)\n    # if a.separable_conv:\n    #     return tf.layers.separable_conv2d(batch_input, out_channels, kernel_size=4, strides=(2, 2), padding=""same"", depthwise_initializer=initializer, pointwise_initializer=initializer)\n    # else:\n    return tf.layers.conv2d(batch_input, out_channels, kernel_size=4, strides=(2, 2), padding=""same"", kernel_initializer=initializer)\n\n\ndef lrelu(x, a):\n    with tf.name_scope(\'lrelu\'):\n        # adding these together creates the leak part and linear part\n        # then cancels them out by subtracting/adding an absolute value term\n        # leak: a*x/2 - a*abs(x)/2\n        # linear: x/2 + abs(x)/2\n\n        # this block looks like it has 2 inputs on the graph unless we do this\n        x = tf.identity(x)\n        return (0.5 * (1 + a)) * x + (0.5 * (1 - a)) * tf.abs(x)\n\n\ndef batchnorm(inputs):\n    return tf.layers.batch_normalization(inputs, axis=3, epsilon=1e-5, momentum=0.1, training=True, gamma_initializer=tf.random_normal_initializer(1.0, 0.02))\n    # with tf.variable_scope(\'batchnorm\'):\n    #     # this block looks like it has 3 inputs on the graph unless we do this\n    #     input = tf.identity(input)\n    #\n    #     channels = input.get_shape()[3]\n    #     offset = tf.get_variable(\'offset\', [channels], dtype=tf.float32, initializer=tf.zeros_initializer())\n    #     scale = tf.get_variable(\'scale\', [channels], dtype=tf.float32,\n    #                             initializer=tf.random_normal_initializer(1.0, 0.02))\n    #     mean, variance = tf.nn.moments(input, axes=[0, 1, 2], keep_dims=False)\n    #     variance_epsilon = 1e-5\n    #     normalized = tf.nn.batch_normalization(input, mean, variance, offset, scale, variance_epsilon=variance_epsilon)\n    #     return normalized\n\n\ndef gen_deconv(batch_input, out_channels):\n    # [batch, in_height, in_width, in_channels] => [batch, out_height, out_width, out_channels]\n    initializer = tf.random_normal_initializer(0, 0.02)\n    # if a.separable_conv:\n    #     _b, h, w, _c = batch_input.shape\n    #     resized_input = tf.image.resize_images(batch_input, [h * 2, w * 2], method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n    #     return tf.layers.separable_conv2d(resized_input, out_channels, kernel_size=4, strides=(1, 1), padding=""same"", depthwise_initializer=initializer, pointwise_initializer=initializer)\n    # else:\n    return tf.layers.conv2d_transpose(batch_input, out_channels, kernel_size=4, strides=(2, 2), padding=""same"", kernel_initializer=initializer)\n\n\ndef process_image(x):\n    with tf.name_scope(\'load_images\'):\n        raw_input = tf.image.convert_image_dtype(x, dtype=tf.float32)\n\n        raw_input.set_shape([None, None, 3])\n\n        # break apart image pair and move to range [-1, 1]\n        width = tf.shape(raw_input)[1]  # [height, width, channels]\n        a_images = preprocess(raw_input[:, :width // 2, :])\n        b_images = preprocess(raw_input[:, width // 2:, :])\n\n    inputs, targets = [a_images, b_images]\n\n    # synchronize seed for image operations so that we do the same operations to both\n    # input and output images\n    def transform(image):\n        r = image\n\n        # area produces a nice downscaling, but does nearest neighbor for upscaling\n        # assume we\'re going to be doing downscaling here\n        r = tf.image.resize_images(r, [CROP_SIZE, CROP_SIZE], method=tf.image.ResizeMethod.AREA)\n\n        return r\n\n    with tf.name_scope(\'input_images\'):\n        input_images = tf.expand_dims(transform(inputs), 0)\n\n    with tf.name_scope(\'target_images\'):\n        target_images = tf.expand_dims(transform(targets), 0)\n\n    return input_images, target_images\n\n    # Tensor(\'batch:1\', shape=(1, 256, 256, 3), dtype=float32) -> 1 batch size\n\n\ndef create_generator(generator_inputs, generator_outputs_channels):\n    layers = []\n\n    # encoder_1: [batch, 256, 256, in_channels] => [batch, 128, 128, ngf]\n    with tf.variable_scope(\'encoder_1\'):\n        output = gen_conv(generator_inputs, ngf)\n        layers.append(output)\n\n    layer_specs = [\n        ngf * 2,  # encoder_2: [batch, 128, 128, ngf] => [batch, 64, 64, ngf * 2]\n        ngf * 4,  # encoder_3: [batch, 64, 64, ngf * 2] => [batch, 32, 32, ngf * 4]\n        ngf * 8,  # encoder_4: [batch, 32, 32, ngf * 4] => [batch, 16, 16, ngf * 8]\n        ngf * 8,  # encoder_5: [batch, 16, 16, ngf * 8] => [batch, 8, 8, ngf * 8]\n        ngf * 8,  # encoder_6: [batch, 8, 8, ngf * 8] => [batch, 4, 4, ngf * 8]\n        ngf * 8,  # encoder_7: [batch, 4, 4, ngf * 8] => [batch, 2, 2, ngf * 8]\n        ngf * 8,  # encoder_8: [batch, 2, 2, ngf * 8] => [batch, 1, 1, ngf * 8]\n    ]\n\n    for out_channels in layer_specs:\n        with tf.variable_scope(\'encoder_%d\' % (len(layers) + 1)):\n            rectified = lrelu(layers[-1], 0.2)\n            # [batch, in_height, in_width, in_channels] => [batch, in_height/2, in_width/2, out_channels]\n            convolved = gen_conv(rectified, out_channels)\n            output = batchnorm(convolved)\n            layers.append(output)\n\n    layer_specs = [\n        (ngf * 8, 0.5),  # decoder_8: [batch, 1, 1, ngf * 8] => [batch, 2, 2, ngf * 8 * 2]\n        (ngf * 8, 0.5),  # decoder_7: [batch, 2, 2, ngf * 8 * 2] => [batch, 4, 4, ngf * 8 * 2]\n        (ngf * 8, 0.5),  # decoder_6: [batch, 4, 4, ngf * 8 * 2] => [batch, 8, 8, ngf * 8 * 2]\n        (ngf * 8, 0.0),  # decoder_5: [batch, 8, 8, ngf * 8 * 2] => [batch, 16, 16, ngf * 8 * 2]\n        (ngf * 4, 0.0),  # decoder_4: [batch, 16, 16, ngf * 8 * 2] => [batch, 32, 32, ngf * 4 * 2]\n        (ngf * 2, 0.0),  # decoder_3: [batch, 32, 32, ngf * 4 * 2] => [batch, 64, 64, ngf * 2 * 2]\n        (ngf, 0.0),  # decoder_2: [batch, 64, 64, ngf * 2 * 2] => [batch, 128, 128, ngf * 2]\n    ]\n\n    num_encoder_layers = len(layers)\n    for decoder_layer, (out_channels, dropout) in enumerate(layer_specs):\n        skip_layer = num_encoder_layers - decoder_layer - 1\n        with tf.variable_scope(\'decoder_%d\' % (skip_layer + 1)):\n            if decoder_layer == 0:\n                # first decoder layer doesn\'t have skip connections\n                # since it is directly connected to the skip_layer\n                input = layers[-1]\n            else:\n                input = tf.concat([layers[-1], layers[skip_layer]], axis=3)\n\n            rectified = tf.nn.relu(input)\n            # [batch, in_height, in_width, in_channels] => [batch, in_height*2, in_width*2, out_channels]\n            output = gen_deconv(rectified, out_channels)\n            output = batchnorm(output)\n\n            if dropout > 0.0:\n                output = tf.nn.dropout(output, keep_prob=1 - dropout)\n\n            layers.append(output)\n\n    # decoder_1: [batch, 128, 128, ngf * 2] => [batch, 256, 256, generator_outputs_channels]\n    with tf.variable_scope(\'decoder_1\'):\n        input = tf.concat([layers[-1], layers[0]], axis=3)\n        rectified = tf.nn.relu(input)\n        output = gen_deconv(rectified, generator_outputs_channels)\n        output = tf.tanh(output)\n        layers.append(output)\n\n    return layers[-1]\n\n\ndef create_model(inputs, targets):\n    with tf.variable_scope(\'generator\'): # as scope\n        out_channels = int(targets.get_shape()[-1])\n        outputs = create_generator(inputs, out_channels)\n\n    return outputs\n\n\ndef convert(image):\n    return tf.image.convert_image_dtype(image, dtype=tf.uint8, saturate=True, name=\'output\')  # output tensor\n\n\ndef generate_output(x):\n    with tf.name_scope(\'generate_output\'):\n        test_inputs, test_targets = process_image(x)\n\n        # inputs and targets are [batch_size, height, width, channels]\n        model = create_model(test_inputs, test_targets)\n\n        # deprocess files\n        outputs = deprocess(model)\n\n        # reverse any processing on images so they can be written to disk or displayed to user\n        converted_outputs = convert(outputs)\n    return converted_outputs\n\n\nif __name__ == \'__main__\':\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\'--model-input\', dest=\'input_folder\', type=str, help=\'Model folder to import.\')\n    parser.add_argument(\'--model-output\', dest=\'output_folder\', type=str, help=\'Model (reduced) folder to export.\')\n    args = parser.parse_args()\n\n    x = tf.placeholder(tf.uint8, shape=(256, 512, 3), name=\'image_tensor\')  # input tensor\n    y = generate_output(x)\n\n    with tf.Session() as sess:\n        # Restore original model\n        saver = tf.train.Saver()\n        checkpoint = tf.train.latest_checkpoint(args.input_folder)\n        saver.restore(sess, checkpoint)\n\n        # Export reduced model used for prediction\n        saver = tf.train.Saver()\n        saver.save(sess, \'{}/reduced_model\'.format(args.output_folder))\n        print(""Model is exported to {}"".format(checkpoint))\n'"
run_webcam.py,5,"b'import argparse\nimport cv2\nimport dlib\nimport numpy as np\nimport tensorflow as tf\nfrom imutils import video\n\nCROP_SIZE = 256\nDOWNSAMPLE_RATIO = 4\n\n\ndef reshape_for_polyline(array):\n    """"""Reshape image so that it works with polyline.""""""\n    return np.array(array, np.int32).reshape((-1, 1, 2))\n\n\ndef resize(image):\n    """"""Crop and resize image for pix2pix.""""""\n    height, width, _ = image.shape\n    if height != width:\n        # crop to correct ratio\n        size = min(height, width)\n        oh = (height - size) // 2\n        ow = (width - size) // 2\n        cropped_image = image[oh:(oh + size), ow:(ow + size)]\n        image_resize = cv2.resize(cropped_image, (CROP_SIZE, CROP_SIZE))\n        return image_resize\n\n\ndef load_graph(frozen_graph_filename):\n    """"""Load a (frozen) Tensorflow model into memory.""""""\n    graph = tf.Graph()\n    with graph.as_default():\n        od_graph_def = tf.GraphDef()\n        with tf.gfile.GFile(frozen_graph_filename, \'rb\') as fid:\n            serialized_graph = fid.read()\n            od_graph_def.ParseFromString(serialized_graph)\n            tf.import_graph_def(od_graph_def, name=\'\')\n    return graph\n\n\ndef main():\n    # TensorFlow\n    graph = load_graph(args.frozen_model_file)\n    image_tensor = graph.get_tensor_by_name(\'image_tensor:0\')\n    output_tensor = graph.get_tensor_by_name(\'generate_output/output:0\')\n    sess = tf.Session(graph=graph)\n\n    # OpenCV\n    cap = cv2.VideoCapture(args.video_source)\n    fps = video.FPS().start()\n\n    while True:\n        ret, frame = cap.read()\n\n        # resize image and detect face\n        frame_resize = cv2.resize(frame, None, fx=1 / DOWNSAMPLE_RATIO, fy=1 / DOWNSAMPLE_RATIO)\n        gray = cv2.cvtColor(frame_resize, cv2.COLOR_BGR2GRAY)\n        faces = detector(gray, 1)\n        black_image = np.zeros(frame.shape, np.uint8)\n\n        for face in faces:\n            detected_landmarks = predictor(gray, face).parts()\n            landmarks = [[p.x * DOWNSAMPLE_RATIO, p.y * DOWNSAMPLE_RATIO] for p in detected_landmarks]\n\n            jaw = reshape_for_polyline(landmarks[0:17])\n            left_eyebrow = reshape_for_polyline(landmarks[22:27])\n            right_eyebrow = reshape_for_polyline(landmarks[17:22])\n            nose_bridge = reshape_for_polyline(landmarks[27:31])\n            lower_nose = reshape_for_polyline(landmarks[30:35])\n            left_eye = reshape_for_polyline(landmarks[42:48])\n            right_eye = reshape_for_polyline(landmarks[36:42])\n            outer_lip = reshape_for_polyline(landmarks[48:60])\n            inner_lip = reshape_for_polyline(landmarks[60:68])\n\n            color = (255, 255, 255)\n            thickness = 3\n\n            cv2.polylines(black_image, [jaw], False, color, thickness)\n            cv2.polylines(black_image, [left_eyebrow], False, color, thickness)\n            cv2.polylines(black_image, [right_eyebrow], False, color, thickness)\n            cv2.polylines(black_image, [nose_bridge], False, color, thickness)\n            cv2.polylines(black_image, [lower_nose], True, color, thickness)\n            cv2.polylines(black_image, [left_eye], True, color, thickness)\n            cv2.polylines(black_image, [right_eye], True, color, thickness)\n            cv2.polylines(black_image, [outer_lip], True, color, thickness)\n            cv2.polylines(black_image, [inner_lip], True, color, thickness)\n\n        # generate prediction\n        combined_image = np.concatenate([resize(black_image), resize(frame_resize)], axis=1)\n        image_rgb = cv2.cvtColor(combined_image, cv2.COLOR_BGR2RGB)  # OpenCV uses BGR instead of RGB\n        generated_image = sess.run(output_tensor, feed_dict={image_tensor: image_rgb})\n        image_bgr = cv2.cvtColor(np.squeeze(generated_image), cv2.COLOR_RGB2BGR)\n        image_normal = np.concatenate([resize(frame_resize), image_bgr], axis=1)\n        image_landmark = np.concatenate([resize(black_image), image_bgr], axis=1)\n\n        if args.display_landmark == 0:\n            cv2.imshow(\'frame\', image_normal)\n        else:\n            cv2.imshow(\'frame\', image_landmark)\n\n        fps.update()\n        if cv2.waitKey(1) & 0xFF == ord(\'q\'):\n            break\n\n    fps.stop()\n    print(\'[INFO] elapsed time (total): {:.2f}\'.format(fps.elapsed()))\n    print(\'[INFO] approx. FPS: {:.2f}\'.format(fps.fps()))\n\n    sess.close()\n    cap.release()\n    cv2.destroyAllWindows()\n\n\nif __name__ == \'__main__\':\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\'-src\', \'--source\', dest=\'video_source\', type=int,\n                        default=0, help=\'Device index of the camera.\')\n    parser.add_argument(\'--show\', dest=\'display_landmark\', type=int, default=0, choices=[0, 1],\n                        help=\'0 shows the normal input and 1 the facial landmark.\')\n    parser.add_argument(\'--landmark-model\', dest=\'face_landmark_shape_file\', type=str, help=\'Face landmark model file.\')\n    parser.add_argument(\'--tf-model\', dest=\'frozen_model_file\', type=str, help=\'Frozen TensorFlow model file.\')\n\n    args = parser.parse_args()\n\n    # Create the face predictor and landmark predictor\n    detector = dlib.get_frontal_face_detector()\n    predictor = dlib.shape_predictor(args.face_landmark_shape_file)\n\n    main()\n'"
