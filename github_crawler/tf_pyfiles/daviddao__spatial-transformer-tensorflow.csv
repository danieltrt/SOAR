file_path,api_count,code
cluttered_mnist.py,23,"b'# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# =============================================================================\nimport tensorflow as tf\nfrom spatial_transformer import transformer\nimport numpy as np\nfrom tf_utils import weight_variable, bias_variable, dense_to_one_hot\n\n# %% Load data\nmnist_cluttered = np.load(\'./data/mnist_sequence1_sample_5distortions5x5.npz\')\n\nX_train = mnist_cluttered[\'X_train\']\ny_train = mnist_cluttered[\'y_train\']\nX_valid = mnist_cluttered[\'X_valid\']\ny_valid = mnist_cluttered[\'y_valid\']\nX_test = mnist_cluttered[\'X_test\']\ny_test = mnist_cluttered[\'y_test\']\n\n# % turn from dense to one hot representation\nY_train = dense_to_one_hot(y_train, n_classes=10)\nY_valid = dense_to_one_hot(y_valid, n_classes=10)\nY_test = dense_to_one_hot(y_test, n_classes=10)\n\n# %% Graph representation of our network\n\n# %% Placeholders for 40x40 resolution\nx = tf.placeholder(tf.float32, [None, 1600])\ny = tf.placeholder(tf.float32, [None, 10])\n\n# %% Since x is currently [batch, height*width], we need to reshape to a\n# 4-D tensor to use it in a convolutional graph.  If one component of\n# `shape` is the special value -1, the size of that dimension is\n# computed so that the total size remains constant.  Since we haven\'t\n# defined the batch dimension\'s shape yet, we use -1 to denote this\n# dimension should not change size.\nx_tensor = tf.reshape(x, [-1, 40, 40, 1])\n\n# %% We\'ll setup the two-layer localisation network to figure out the\n# %% parameters for an affine transformation of the input\n# %% Create variables for fully connected layer\nW_fc_loc1 = weight_variable([1600, 20])\nb_fc_loc1 = bias_variable([20])\n\nW_fc_loc2 = weight_variable([20, 6])\n# Use identity transformation as starting point\ninitial = np.array([[1., 0, 0], [0, 1., 0]])\ninitial = initial.astype(\'float32\')\ninitial = initial.flatten()\nb_fc_loc2 = tf.Variable(initial_value=initial, name=\'b_fc_loc2\')\n\n# %% Define the two layer localisation network\nh_fc_loc1 = tf.nn.tanh(tf.matmul(x, W_fc_loc1) + b_fc_loc1)\n# %% We can add dropout for regularizing and to reduce overfitting like so:\nkeep_prob = tf.placeholder(tf.float32)\nh_fc_loc1_drop = tf.nn.dropout(h_fc_loc1, keep_prob)\n# %% Second layer\nh_fc_loc2 = tf.nn.tanh(tf.matmul(h_fc_loc1_drop, W_fc_loc2) + b_fc_loc2)\n\n# %% We\'ll create a spatial transformer module to identify discriminative\n# %% patches\nout_size = (40, 40)\nh_trans = transformer(x_tensor, h_fc_loc2, out_size)\n\n# %% We\'ll setup the first convolutional layer\n# Weight matrix is [height x width x input_channels x output_channels]\nfilter_size = 3\nn_filters_1 = 16\nW_conv1 = weight_variable([filter_size, filter_size, 1, n_filters_1])\n\n# %% Bias is [output_channels]\nb_conv1 = bias_variable([n_filters_1])\n\n# %% Now we can build a graph which does the first layer of convolution:\n# we define our stride as batch x height x width x channels\n# instead of pooling, we use strides of 2 and more layers\n# with smaller filters.\n\nh_conv1 = tf.nn.relu(\n    tf.nn.conv2d(input=h_trans,\n                 filter=W_conv1,\n                 strides=[1, 2, 2, 1],\n                 padding=\'SAME\') +\n    b_conv1)\n\n# %% And just like the first layer, add additional layers to create\n# a deep net\nn_filters_2 = 16\nW_conv2 = weight_variable([filter_size, filter_size, n_filters_1, n_filters_2])\nb_conv2 = bias_variable([n_filters_2])\nh_conv2 = tf.nn.relu(\n    tf.nn.conv2d(input=h_conv1,\n                 filter=W_conv2,\n                 strides=[1, 2, 2, 1],\n                 padding=\'SAME\') +\n    b_conv2)\n\n# %% We\'ll now reshape so we can connect to a fully-connected layer:\nh_conv2_flat = tf.reshape(h_conv2, [-1, 10 * 10 * n_filters_2])\n\n# %% Create a fully-connected layer:\nn_fc = 1024\nW_fc1 = weight_variable([10 * 10 * n_filters_2, n_fc])\nb_fc1 = bias_variable([n_fc])\nh_fc1 = tf.nn.relu(tf.matmul(h_conv2_flat, W_fc1) + b_fc1)\n\nh_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)\n\n# %% And finally our softmax layer:\nW_fc2 = weight_variable([n_fc, 10])\nb_fc2 = bias_variable([10])\ny_logits = tf.matmul(h_fc1_drop, W_fc2) + b_fc2\n\n# %% Define loss/eval/training functions\ncross_entropy = tf.reduce_mean(\n    tf.nn.softmax_cross_entropy_with_logits_v2(logits=y_logits, labels=y))\nopt = tf.train.AdamOptimizer()\noptimizer = opt.minimize(cross_entropy)\ngrads = opt.compute_gradients(cross_entropy, [b_fc_loc2])\n\n# %% Monitor accuracy\ncorrect_prediction = tf.equal(tf.argmax(y_logits, 1), tf.argmax(y, 1))\naccuracy = tf.reduce_mean(tf.cast(correct_prediction, \'float\'))\n\n# %% We now create a new session to actually perform the initialization the\n# variables:\nsess = tf.Session()\nsess.run(tf.global_variables_initializer())\n\n\n# %% We\'ll now train in minibatches and report accuracy, loss:\niter_per_epoch = 100\nn_epochs = 500\ntrain_size = 10000\n\nindices = np.linspace(0, 10000 - 1, iter_per_epoch)\nindices = indices.astype(\'int\')\n\nfor epoch_i in range(n_epochs):\n    for iter_i in range(iter_per_epoch - 1):\n        batch_xs = X_train[indices[iter_i]:indices[iter_i+1]]\n        batch_ys = Y_train[indices[iter_i]:indices[iter_i+1]]\n\n        if iter_i % 10 == 0:\n            loss = sess.run(cross_entropy,\n                            feed_dict={\n                                x: batch_xs,\n                                y: batch_ys,\n                                keep_prob: 1.0\n                            })\n            print(\'Iteration: \' + str(iter_i) + \' Loss: \' + str(loss))\n\n        sess.run(optimizer, feed_dict={\n            x: batch_xs, y: batch_ys, keep_prob: 0.8})\n\n    print(\'Accuracy (%d): \' % epoch_i + str(sess.run(accuracy,\n                                                     feed_dict={\n                                                         x: X_valid,\n                                                         y: Y_valid,\n                                                         keep_prob: 1.0\n                                                     })))\n    # theta = sess.run(h_fc_loc2, feed_dict={\n    #        x: batch_xs, keep_prob: 1.0})\n    # print(theta[0])\n'"
example.py,8,"b'# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\nfrom scipy import ndimage\nimport tensorflow as tf\nfrom spatial_transformer import transformer\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# %% Create a batch of three images (1600 x 1200)\n# %% Image retrieved from:\n# %% https://raw.githubusercontent.com/skaae/transformer_network/master/cat.jpg\nim = ndimage.imread(\'cat.jpg\')\nim = im / 255.\nim = im.reshape(1, 1200, 1600, 3)\nim = im.astype(\'float32\')\n\n# %% Let the output size of the transformer be half the image size.\nout_size = (600, 800)\n\n# %% Simulate batch\nbatch = np.append(im, im, axis=0)\nbatch = np.append(batch, im, axis=0)\nnum_batch = 3\n\nx = tf.placeholder(tf.float32, [None, 1200, 1600, 3])\nx = tf.cast(batch, \'float32\')\n\n# %% Create localisation network and convolutional layer\nwith tf.variable_scope(\'spatial_transformer_0\'):\n\n    # %% Create a fully-connected layer with 6 output nodes\n    n_fc = 6\n    W_fc1 = tf.Variable(tf.zeros([1200 * 1600 * 3, n_fc]), name=\'W_fc1\')\n\n    # %% Zoom into the image\n    initial = np.array([[0.5, 0, 0], [0, 0.5, 0]])\n    initial = initial.astype(\'float32\')\n    initial = initial.flatten()\n\n    b_fc1 = tf.Variable(initial_value=initial, name=\'b_fc1\')\n    h_fc1 = tf.matmul(tf.zeros([num_batch, 1200 * 1600 * 3]), W_fc1) + b_fc1\n    h_trans = transformer(x, h_fc1, out_size)\n\n# %% Run session\nsess = tf.Session()\nsess.run(tf.initialize_all_variables())\ny = sess.run(h_trans, feed_dict={x: batch})\n\n# plt.imshow(y[0])\n'"
spatial_transformer.py,73,"b'# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\nimport tensorflow as tf\n\n\ndef transformer(U, theta, out_size, name=\'SpatialTransformer\', **kwargs):\n    """"""Spatial Transformer Layer\n\n    Implements a spatial transformer layer as described in [1]_.\n    Based on [2]_ and edited by David Dao for Tensorflow.\n\n    Parameters\n    ----------\n    U : float\n        The output of a convolutional net should have the\n        shape [num_batch, height, width, num_channels].\n    theta: float\n        The output of the\n        localisation network should be [num_batch, 6].\n    out_size: tuple of two ints\n        The size of the output of the network (height, width)\n\n    References\n    ----------\n    .. [1]  Spatial Transformer Networks\n            Max Jaderberg, Karen Simonyan, Andrew Zisserman, Koray Kavukcuoglu\n            Submitted on 5 Jun 2015\n    .. [2]  https://github.com/skaae/transformer_network/blob/master/transformerlayer.py\n\n    Notes\n    -----\n    To initialize the network to the identity transform init\n    ``theta`` to :\n        identity = np.array([[1., 0., 0.],\n                             [0., 1., 0.]])\n        identity = identity.flatten()\n        theta = tf.Variable(initial_value=identity)\n\n    """"""\n\n    def _repeat(x, n_repeats):\n        with tf.variable_scope(\'_repeat\'):\n            rep = tf.transpose(\n                tf.expand_dims(tf.ones(shape=tf.stack([n_repeats, ])), 1), [1, 0])\n            rep = tf.cast(rep, \'int32\')\n            x = tf.matmul(tf.reshape(x, (-1, 1)), rep)\n            return tf.reshape(x, [-1])\n\n    def _interpolate(im, x, y, out_size):\n        with tf.variable_scope(\'_interpolate\'):\n            # constants\n            num_batch = tf.shape(im)[0]\n            height = tf.shape(im)[1]\n            width = tf.shape(im)[2]\n            channels = tf.shape(im)[3]\n\n            x = tf.cast(x, \'float32\')\n            y = tf.cast(y, \'float32\')\n            height_f = tf.cast(height, \'float32\')\n            width_f = tf.cast(width, \'float32\')\n            out_height = out_size[0]\n            out_width = out_size[1]\n            zero = tf.zeros([], dtype=\'int32\')\n            max_y = tf.cast(tf.shape(im)[1] - 1, \'int32\')\n            max_x = tf.cast(tf.shape(im)[2] - 1, \'int32\')\n\n            # scale indices from [-1, 1] to [0, width/height]\n            x = (x + 1.0)*(width_f) / 2.0\n            y = (y + 1.0)*(height_f) / 2.0\n\n            # do sampling\n            x0 = tf.cast(tf.floor(x), \'int32\')\n            x1 = x0 + 1\n            y0 = tf.cast(tf.floor(y), \'int32\')\n            y1 = y0 + 1\n\n            x0 = tf.clip_by_value(x0, zero, max_x)\n            x1 = tf.clip_by_value(x1, zero, max_x)\n            y0 = tf.clip_by_value(y0, zero, max_y)\n            y1 = tf.clip_by_value(y1, zero, max_y)\n            dim2 = width\n            dim1 = width*height\n            base = _repeat(tf.range(num_batch)*dim1, out_height*out_width)\n            base_y0 = base + y0*dim2\n            base_y1 = base + y1*dim2\n            idx_a = base_y0 + x0\n            idx_b = base_y1 + x0\n            idx_c = base_y0 + x1\n            idx_d = base_y1 + x1\n\n            # use indices to lookup pixels in the flat image and restore\n            # channels dim\n            im_flat = tf.reshape(im, tf.stack([-1, channels]))\n            im_flat = tf.cast(im_flat, \'float32\')\n            Ia = tf.gather(im_flat, idx_a)\n            Ib = tf.gather(im_flat, idx_b)\n            Ic = tf.gather(im_flat, idx_c)\n            Id = tf.gather(im_flat, idx_d)\n\n            # and finally calculate interpolated values\n            x0_f = tf.cast(x0, \'float32\')\n            x1_f = tf.cast(x1, \'float32\')\n            y0_f = tf.cast(y0, \'float32\')\n            y1_f = tf.cast(y1, \'float32\')\n            wa = tf.expand_dims(((x1_f-x) * (y1_f-y)), 1)\n            wb = tf.expand_dims(((x1_f-x) * (y-y0_f)), 1)\n            wc = tf.expand_dims(((x-x0_f) * (y1_f-y)), 1)\n            wd = tf.expand_dims(((x-x0_f) * (y-y0_f)), 1)\n            output = tf.add_n([wa*Ia, wb*Ib, wc*Ic, wd*Id])\n            return output\n\n    def _meshgrid(height, width):\n        with tf.variable_scope(\'_meshgrid\'):\n            # This should be equivalent to:\n            #  x_t, y_t = np.meshgrid(np.linspace(-1, 1, width),\n            #                         np.linspace(-1, 1, height))\n            #  ones = np.ones(np.prod(x_t.shape))\n            #  grid = np.vstack([x_t.flatten(), y_t.flatten(), ones])\n            x_t = tf.matmul(tf.ones(shape=tf.stack([height, 1])),\n                            tf.transpose(tf.expand_dims(tf.linspace(-1.0, 1.0, width), 1), [1, 0]))\n            y_t = tf.matmul(tf.expand_dims(tf.linspace(-1.0, 1.0, height), 1),\n                            tf.ones(shape=tf.stack([1, width])))\n\n            x_t_flat = tf.reshape(x_t, (1, -1))\n            y_t_flat = tf.reshape(y_t, (1, -1))\n\n            ones = tf.ones_like(x_t_flat)\n            grid = tf.concat([x_t_flat, y_t_flat, ones], 0)\n            return grid\n\n    def _transform(theta, input_dim, out_size):\n        with tf.variable_scope(\'_transform\'):\n            num_batch = tf.shape(input_dim)[0]\n            height = tf.shape(input_dim)[1]\n            width = tf.shape(input_dim)[2]\n            num_channels = tf.shape(input_dim)[3]\n            theta = tf.reshape(theta, (-1, 2, 3))\n            theta = tf.cast(theta, \'float32\')\n\n            # grid of (x_t, y_t, 1), eq (1) in ref [1]\n            height_f = tf.cast(height, \'float32\')\n            width_f = tf.cast(width, \'float32\')\n            out_height = out_size[0]\n            out_width = out_size[1]\n            grid = _meshgrid(out_height, out_width)\n            grid = tf.expand_dims(grid, 0)\n            grid = tf.reshape(grid, [-1])\n            grid = tf.tile(grid, tf.stack([num_batch]))\n            grid = tf.reshape(grid, tf.stack([num_batch, 3, -1]))\n\n            # Transform A x (x_t, y_t, 1)^T -> (x_s, y_s)\n            T_g = tf.matmul(theta, grid)\n            x_s = tf.slice(T_g, [0, 0, 0], [-1, 1, -1])\n            y_s = tf.slice(T_g, [0, 1, 0], [-1, 1, -1])\n            x_s_flat = tf.reshape(x_s, [-1])\n            y_s_flat = tf.reshape(y_s, [-1])\n\n            input_transformed = _interpolate(\n                input_dim, x_s_flat, y_s_flat,\n                out_size)\n\n            output = tf.reshape(\n                input_transformed, tf.stack([num_batch, out_height, out_width, num_channels]))\n            return output\n\n    with tf.variable_scope(name):\n        output = _transform(theta, U, out_size)\n        return output\n\n\ndef batch_transformer(U, thetas, out_size, name=\'BatchSpatialTransformer\'):\n    """"""Batch Spatial Transformer Layer\n\n    Parameters\n    ----------\n\n    U : float\n        tensor of inputs [num_batch,height,width,num_channels]\n    thetas : float\n        a set of transformations for each input [num_batch,num_transforms,6]\n    out_size : int\n        the size of the output [out_height,out_width]\n\n    Returns: float\n        Tensor of size [num_batch*num_transforms,out_height,out_width,num_channels]\n    """"""\n    with tf.variable_scope(name):\n        num_batch, num_transforms = map(int, thetas.get_shape().as_list()[:2])\n        indices = [[i]*num_transforms for i in xrange(num_batch)]\n        input_repeated = tf.gather(U, tf.reshape(indices, [-1]))\n        return transformer(input_repeated, thetas, out_size)\n'"
tf_utils.py,15,"b'# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n# %% Borrowed utils from here: https://github.com/pkmital/tensorflow_tutorials/\nimport tensorflow as tf\nimport numpy as np\n\ndef conv2d(x, n_filters,\n           k_h=5, k_w=5,\n           stride_h=2, stride_w=2,\n           stddev=0.02,\n           activation=lambda x: x,\n           bias=True,\n           padding=\'SAME\',\n           name=""Conv2D""):\n    """"""2D Convolution with options for kernel size, stride, and init deviation.\n    Parameters\n    ----------\n    x : Tensor\n        Input tensor to convolve.\n    n_filters : int\n        Number of filters to apply.\n    k_h : int, optional\n        Kernel height.\n    k_w : int, optional\n        Kernel width.\n    stride_h : int, optional\n        Stride in rows.\n    stride_w : int, optional\n        Stride in cols.\n    stddev : float, optional\n        Initialization\'s standard deviation.\n    activation : arguments, optional\n        Function which applies a nonlinearity\n    padding : str, optional\n        \'SAME\' or \'VALID\'\n    name : str, optional\n        Variable scope to use.\n    Returns\n    -------\n    x : Tensor\n        Convolved input.\n    """"""\n    with tf.variable_scope(name):\n        w = tf.get_variable(\n            \'w\', [k_h, k_w, x.get_shape()[-1], n_filters],\n            initializer=tf.truncated_normal_initializer(stddev=stddev))\n        conv = tf.nn.conv2d(\n            x, w, strides=[1, stride_h, stride_w, 1], padding=padding)\n        if bias:\n            b = tf.get_variable(\n                \'b\', [n_filters],\n                initializer=tf.truncated_normal_initializer(stddev=stddev))\n            conv = conv + b\n        return conv\n    \ndef linear(x, n_units, scope=None, stddev=0.02,\n           activation=lambda x: x):\n    """"""Fully-connected network.\n    Parameters\n    ----------\n    x : Tensor\n        Input tensor to the network.\n    n_units : int\n        Number of units to connect to.\n    scope : str, optional\n        Variable scope to use.\n    stddev : float, optional\n        Initialization\'s standard deviation.\n    activation : arguments, optional\n        Function which applies a nonlinearity\n    Returns\n    -------\n    x : Tensor\n        Fully-connected output.\n    """"""\n    shape = x.get_shape().as_list()\n\n    with tf.variable_scope(scope or ""Linear""):\n        matrix = tf.get_variable(""Matrix"", [shape[1], n_units], tf.float32,\n                                 tf.random_normal_initializer(stddev=stddev))\n        return activation(tf.matmul(x, matrix))\n    \n# %%\ndef weight_variable(shape):\n    \'\'\'Helper function to create a weight variable initialized with\n    a normal distribution\n    Parameters\n    ----------\n    shape : list\n        Size of weight variable\n    \'\'\'\n    #initial = tf.random_normal(shape, mean=0.0, stddev=0.01)\n    initial = tf.zeros(shape)\n    return tf.Variable(initial)\n\n# %%\ndef bias_variable(shape):\n    \'\'\'Helper function to create a bias variable initialized with\n    a constant value.\n    Parameters\n    ----------\n    shape : list\n        Size of weight variable\n    \'\'\'\n    initial = tf.random_normal(shape, mean=0.0, stddev=0.01)\n    return tf.Variable(initial)\n\n# %% \ndef dense_to_one_hot(labels, n_classes=2):\n    """"""Convert class labels from scalars to one-hot vectors.""""""\n    labels = np.array(labels)\n    n_labels = labels.shape[0]\n    index_offset = np.arange(n_labels) * n_classes\n    labels_one_hot = np.zeros((n_labels, n_classes), dtype=np.float32)\n    labels_one_hot.flat[index_offset + labels.ravel()] = 1\n    return labels_one_hot\n'"
