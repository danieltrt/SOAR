file_path,api_count,code
__init__.py,0,b''
contributed/__init__.py,0,b''
contributed/batch_represent.py,5,"b'#!/usr/bin/env python\n# coding=utf-8\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\n""""""\nAllows you to generate embeddings from a directory of images in the format:\n\nInstructions:\n\nImage data directory should look like the following figure:\nperson-1\n\xe2\x94\x9c\xe2\x94\x80\xe2\x94\x80 image-1.jpg\n\xe2\x94\x9c\xe2\x94\x80\xe2\x94\x80 image-2.png\n...\n\xe2\x94\x94\xe2\x94\x80\xe2\x94\x80 image-p.png\n\n...\n\nperson-m\n\xe2\x94\x9c\xe2\x94\x80\xe2\x94\x80 image-1.png\n\xe2\x94\x9c\xe2\x94\x80\xe2\x94\x80 image-2.jpg\n...\n\xe2\x94\x94\xe2\x94\x80\xe2\x94\x80 image-q.png\n\nTrained Model:\n- Both the trained model metagraph and the model parameters need to exist\nin the same directory, and the metagraph should have the extension \'.meta\'.\n\n####\nUSAGE:\n$ python batch_represent.py -d <YOUR IMAGE DATA DIRECTORY> -o <DIRECTORY TO STORE OUTPUT ARRAYS> --trained_model_dir <DIRECTORY CONTAINING PRETRAINED MODEL>\n###\n""""""\n\n""""""\nAttributions:\nThe code is heavily inspired by the code from by David Sandberg\'s ../src/validate_on_lfw.py\nThe concept is inspired by Brandon Amos\' github.com/cmusatyalab/openface/blob/master/batch-represent/batch-represent.lua\n""""""\n\n#----------------------------------------------------\n# MIT License\n#\n# Copyright (c) 2017 Rakshak Talwar\n#\n# Permission is hereby granted, free of charge, to any person obtaining a copy\n# of this software and associated documentation files (the ""Software""), to deal\n# in the Software without restriction, including without limitation the rights\n# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n# copies of the Software, and to permit persons to whom the Software is\n# furnished to do so, subject to the following conditions:\n#\n# The above copyright notice and this permission notice shall be included in all\n# copies or substantial portions of the Software.\n#\n# THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n# SOFTWARE.\n#----------------------------------------------------\n\nimport os\nimport sys\nimport argparse\nimport importlib\nimport time\n\nsys.path.insert(1, ""../src"")\nimport facenet\nimport numpy as np\nfrom sklearn.datasets import load_files\nimport tensorflow as tf\nfrom six.moves import xrange\n\ndef main(args):\n\n\twith tf.Graph().as_default():\n\n\t\twith tf.Session() as sess:\n\n\t\t\t# create output directory if it doesn\'t exist\n\t\t\toutput_dir = os.path.expanduser(args.output_dir)\n\t\t\tif not os.path.isdir(output_dir):\n\t\t\t\tos.makedirs(output_dir)\n\n\t\t\t# load the model\n\t\t\tprint(""Loading trained model...\\n"")\n\t\t\tmeta_file, ckpt_file = facenet.get_model_filenames(os.path.expanduser(args.trained_model_dir))\n\t\t\tfacenet.load_model(args.trained_model_dir, meta_file, ckpt_file)\n\n\t\t\t# grab all image paths and labels\n\t\t\tprint(""Finding image paths and targets...\\n"")\n\t\t\tdata = load_files(args.data_dir, load_content=False, shuffle=False)\n\t\t\tlabels_array = data[\'target\']\n\t\t\tpaths = data[\'filenames\']\n\n\t\t\t# Get input and output tensors\n\t\t\timages_placeholder = tf.get_default_graph().get_tensor_by_name(""input:0"")\n\t\t\tembeddings = tf.get_default_graph().get_tensor_by_name(""embeddings:0"")\n\t\t\tphase_train_placeholder = tf.get_default_graph().get_tensor_by_name(""phase_train:0"")\n\n\t\t\timage_size = images_placeholder.get_shape()[1]\n\t\t\tembedding_size = embeddings.get_shape()[1]\n\n\t\t\t# Run forward pass to calculate embeddings\n\t\t\tprint(\'Generating embeddings from images...\\n\')\n\t\t\tstart_time = time.time()\n\t\t\tbatch_size = args.batch_size\n\t\t\tnrof_images = len(paths)\n\t\t\tnrof_batches = int(np.ceil(1.0*nrof_images / batch_size))\n\t\t\temb_array = np.zeros((nrof_images, embedding_size))\n\t\t\tfor i in xrange(nrof_batches):\n\t\t\t\tstart_index = i*batch_size\n\t\t\t\tend_index = min((i+1)*batch_size, nrof_images)\n\t\t\t\tpaths_batch = paths[start_index:end_index]\n\t\t\t\timages = facenet.load_data(paths_batch, do_random_crop=False, do_random_flip=False, image_size=image_size, do_prewhiten=True)\n\t\t\t\tfeed_dict = { images_placeholder:images, phase_train_placeholder:False}\n\t\t\t\temb_array[start_index:end_index,:] = sess.run(embeddings, feed_dict=feed_dict)\n\n\t\t\ttime_avg_forward_pass = (time.time() - start_time) / float(nrof_images)\n\t\t\tprint(""Forward pass took avg of %.3f[seconds/image] for %d images\\n"" % (time_avg_forward_pass, nrof_images))\n\n\t\t\tprint(""Finally saving embeddings and gallery to: %s"" % (output_dir))\n\t\t\t# save the gallery and embeddings (signatures) as numpy arrays to disk\n\t\t\tnp.save(os.path.join(output_dir, ""gallery.npy""), labels_array)\n\t\t\tnp.save(os.path.join(output_dir, ""signatures.npy""), emb_array)\n\ndef parse_arguments(argv):\n\tparser = argparse.ArgumentParser(description=""Batch-represent face embeddings from a given data directory"")\n\tparser.add_argument(\'-d\', \'--data_dir\', type=str,\n\t\thelp=\'directory of images with structure as seen at the top of this file.\')\n\tparser.add_argument(\'-o\', \'--output_dir\', type=str,\n\t\thelp=\'directory containing aligned face patches with file structure as seen at the top of this file.\')\n\tparser.add_argument(\'--trained_model_dir\', type=str,\n        help=\'Load a trained model before training starts.\')\n\tparser.add_argument(\'--batch_size\', type=int, help=\'Number of images to process in a batch.\', default=50)\n\n\treturn parser.parse_args(argv)\n\n\nif __name__ == ""__main__"":\n\tmain(parse_arguments(sys.argv[1:]))\n'"
contributed/cluster.py,5,"b'# MIT License\n#\n# Copyright (c) 2017 PXL University College\n#\n# Permission is hereby granted, free of charge, to any person obtaining a copy\n# of this software and associated documentation files (the ""Software""), to deal\n# in the Software without restriction, including without limitation the rights\n# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n# copies of the Software, and to permit persons to whom the Software is\n# furnished to do so, subject to the following conditions:\n#\n# The above copyright notice and this permission notice shall be included in all\n# copies or substantial portions of the Software.\n#\n# THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n# SOFTWARE.\n\n# Clusters similar faces from input folder together in folders based on euclidean distance matrix\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom scipy import misc\nimport tensorflow as tf\nimport numpy as np\nimport os\nimport sys\nimport argparse\nimport facenet\nimport align.detect_face\nfrom sklearn.cluster import DBSCAN\n\n\ndef main(args):\n    pnet, rnet, onet = create_network_face_detection(args.gpu_memory_fraction)\n\n    with tf.Graph().as_default():\n\n        with tf.Session() as sess:\n            facenet.load_model(args.model)\n\n            image_list = load_images_from_folder(args.data_dir)\n            images = align_data(image_list, args.image_size, args.margin, pnet, rnet, onet)\n\n            images_placeholder = sess.graph.get_tensor_by_name(""input:0"")\n            embeddings = sess.graph.get_tensor_by_name(""embeddings:0"")\n            phase_train_placeholder = sess.graph.get_tensor_by_name(""phase_train:0"")\n            feed_dict = {images_placeholder: images, phase_train_placeholder: False}\n            emb = sess.run(embeddings, feed_dict=feed_dict)\n\n            nrof_images = len(images)\n\n            matrix = np.zeros((nrof_images, nrof_images))\n\n            print(\'\')\n            # Print distance matrix\n            print(\'Distance matrix\')\n            print(\'    \', end=\'\')\n            for i in range(nrof_images):\n                print(\'    %1d     \' % i, end=\'\')\n            print(\'\')\n            for i in range(nrof_images):\n                print(\'%1d  \' % i, end=\'\')\n                for j in range(nrof_images):\n                    dist = np.sqrt(np.sum(np.square(np.subtract(emb[i, :], emb[j, :]))))\n                    matrix[i][j] = dist\n                    print(\'  %1.4f  \' % dist, end=\'\')\n                print(\'\')\n\n            print(\'\')\n\n            # DBSCAN is the only algorithm that doesn\'t require the number of clusters to be defined.\n            db = DBSCAN(eps=args.cluster_threshold, min_samples=args.min_cluster_size, metric=\'precomputed\')\n            db.fit(matrix)\n            labels = db.labels_\n\n            # get number of clusters\n            no_clusters = len(set(labels)) - (1 if -1 in labels else 0)\n\n            print(\'No of clusters:\', no_clusters)\n\n            if no_clusters > 0:\n                if args.largest_cluster_only:\n                    largest_cluster = 0\n                    for i in range(no_clusters):\n                        print(\'Cluster {}: {}\'.format(i, np.nonzero(labels == i)[0]))\n                        if len(np.nonzero(labels == i)[0]) > len(np.nonzero(labels == largest_cluster)[0]):\n                            largest_cluster = i\n                    print(\'Saving largest cluster (Cluster: {})\'.format(largest_cluster))\n                    cnt = 1\n                    for i in np.nonzero(labels == largest_cluster)[0]:\n                        misc.imsave(os.path.join(args.out_dir, str(cnt) + \'.png\'), images[i])\n                        cnt += 1\n                else:\n                    print(\'Saving all clusters\')\n                    for i in range(no_clusters):\n                        cnt = 1\n                        print(\'Cluster {}: {}\'.format(i, np.nonzero(labels == i)[0]))\n                        path = os.path.join(args.out_dir, str(i))\n                        if not os.path.exists(path):\n                            os.makedirs(path)\n                            for j in np.nonzero(labels == i)[0]:\n                                misc.imsave(os.path.join(path, str(cnt) + \'.png\'), images[j])\n                                cnt += 1\n                        else:\n                            for j in np.nonzero(labels == i)[0]:\n                                misc.imsave(os.path.join(path, str(cnt) + \'.png\'), images[j])\n                                cnt += 1\n\n\ndef align_data(image_list, image_size, margin, pnet, rnet, onet):\n    minsize = 20  # minimum size of face\n    threshold = [0.6, 0.7, 0.7]  # three steps\'s threshold\n    factor = 0.709  # scale factor\n\n    img_list = []\n\n    for x in xrange(len(image_list)):\n        img_size = np.asarray(image_list[x].shape)[0:2]\n        bounding_boxes, _ = align.detect_face.detect_face(image_list[x], minsize, pnet, rnet, onet, threshold, factor)\n        nrof_samples = len(bounding_boxes)\n        if nrof_samples > 0:\n            for i in xrange(nrof_samples):\n                if bounding_boxes[i][4] > 0.95:\n                    det = np.squeeze(bounding_boxes[i, 0:4])\n                    bb = np.zeros(4, dtype=np.int32)\n                    bb[0] = np.maximum(det[0] - margin / 2, 0)\n                    bb[1] = np.maximum(det[1] - margin / 2, 0)\n                    bb[2] = np.minimum(det[2] + margin / 2, img_size[1])\n                    bb[3] = np.minimum(det[3] + margin / 2, img_size[0])\n                    cropped = image_list[x][bb[1]:bb[3], bb[0]:bb[2], :]\n                    aligned = misc.imresize(cropped, (image_size, image_size), interp=\'bilinear\')\n                    prewhitened = facenet.prewhiten(aligned)\n                    img_list.append(prewhitened)\n\n    if len(img_list) > 0:\n        images = np.stack(img_list)\n        return images\n    else:\n        return None\n\n\ndef create_network_face_detection(gpu_memory_fraction):\n    with tf.Graph().as_default():\n        gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=gpu_memory_fraction)\n        sess = tf.Session(config=tf.ConfigProto(gpu_options=gpu_options, log_device_placement=False))\n        with sess.as_default():\n            pnet, rnet, onet = align.detect_face.create_mtcnn(sess, None)\n    return pnet, rnet, onet\n\n\ndef load_images_from_folder(folder):\n    images = []\n    for filename in os.listdir(folder):\n        img = misc.imread(os.path.join(folder, filename))\n        if img is not None:\n            images.append(img)\n    return images\n\n\ndef parse_arguments(argv):\n    parser = argparse.ArgumentParser()\n\n    parser.add_argument(\'model\', type=str,\n                        help=\'Either a directory containing the meta_file and ckpt_file or a model protobuf (.pb) file\')\n    parser.add_argument(\'data_dir\', type=str,\n                        help=\'The directory containing the images to cluster into folders.\')\n    parser.add_argument(\'out_dir\', type=str,\n                        help=\'The output directory where the image clusters will be saved.\')\n    parser.add_argument(\'--image_size\', type=int,\n                        help=\'Image size (height, width) in pixels.\', default=160)\n    parser.add_argument(\'--margin\', type=int,\n                        help=\'Margin for the crop around the bounding box (height, width) in pixels.\', default=44)\n    parser.add_argument(\'--min_cluster_size\', type=int,\n                        help=\'The minimum amount of pictures required for a cluster.\', default=1)\n    parser.add_argument(\'--cluster_threshold\', type=float,\n                        help=\'The minimum distance for faces to be in the same cluster\', default=1.0)\n    parser.add_argument(\'--largest_cluster_only\', action=\'store_true\',\n                        help=\'This argument will make that only the biggest cluster is saved.\')\n    parser.add_argument(\'--gpu_memory_fraction\', type=float,\n                        help=\'Upper bound on the amount of GPU memory that will be used by the process.\', default=1.0)\n\n    return parser.parse_args(argv)\n\n\nif __name__ == \'__main__\':\n    main(parse_arguments(sys.argv[1:]))\n'"
contributed/clustering.py,7,"b'"""""" Face Cluster """"""\nimport tensorflow as tf\nimport numpy as np\nimport importlib\nimport argparse\nimport facenet\nimport os\nimport math\ndef face_distance(face_encodings, face_to_compare):\n    """"""\n    Given a list of face encodings, compare them to a known face encoding and get a euclidean distance\n    for each comparison face. The distance tells you how similar the faces are.\n    :param faces: List of face encodings to compare\n    :param face_to_compare: A face encoding to compare against\n    :return: A numpy ndarray with the distance for each face in the same order as the \'faces\' array\n    """"""\n    import numpy as np\n    if len(face_encodings) == 0:\n        return np.empty((0))\n\n    #return 1/np.linalg.norm(face_encodings - face_to_compare, axis=1)\n    return np.sum(face_encodings*face_to_compare,axis=1)\n\ndef load_model(model_dir, meta_file, ckpt_file):\n    model_dir_exp = os.path.expanduser(model_dir)\n    saver = tf.train.import_meta_graph(os.path.join(model_dir_exp, meta_file))\n    saver.restore(tf.get_default_session(), os.path.join(model_dir_exp, ckpt_file))\n\ndef _chinese_whispers(encoding_list, threshold=0.55, iterations=20):\n    """""" Chinese Whispers Algorithm\n\n    Modified from Alex Loveless\' implementation,\n    http://alexloveless.co.uk/data/chinese-whispers-graph-clustering-in-python/\n\n    Inputs:\n        encoding_list: a list of facial encodings from face_recognition\n        threshold: facial match threshold,default 0.6\n        iterations: since chinese whispers is an iterative algorithm, number of times to iterate\n\n    Outputs:\n        sorted_clusters: a list of clusters, a cluster being a list of imagepaths,\n            sorted by largest cluster to smallest\n    """"""\n\n    #from face_recognition.api import _face_distance\n    from random import shuffle\n    import networkx as nx\n    # Create graph\n    nodes = []\n    edges = []\n\n    image_paths, encodings = zip(*encoding_list)\n\n    if len(encodings) <= 1:\n        print (""No enough encodings to cluster!"")\n        return []\n\n    for idx, face_encoding_to_check in enumerate(encodings):\n        # Adding node of facial encoding\n        node_id = idx+1\n\n        # Initialize \'cluster\' to unique value (cluster of itself)\n        node = (node_id, {\'cluster\': image_paths[idx], \'path\': image_paths[idx]})\n        nodes.append(node)\n\n        # Facial encodings to compare\n        if (idx+1) >= len(encodings):\n            # Node is last element, don\'t create edge\n            break\n\n        compare_encodings = encodings[idx+1:]\n        distances = face_distance(compare_encodings, face_encoding_to_check)\n        encoding_edges = []\n        for i, distance in enumerate(distances):\n            if distance > threshold:\n                # Add edge if facial match\n                edge_id = idx+i+2\n                encoding_edges.append((node_id, edge_id, {\'weight\': distance}))\n\n        edges = edges + encoding_edges\n\n    G = nx.Graph()\n    G.add_nodes_from(nodes)\n    G.add_edges_from(edges)\n\n    # Iterate\n    for _ in range(0, iterations):\n        cluster_nodes = G.nodes()\n        shuffle(cluster_nodes)\n        for node in cluster_nodes:\n            neighbors = G[node]\n            clusters = {}\n\n            for ne in neighbors:\n                if isinstance(ne, int):\n                    if G.node[ne][\'cluster\'] in clusters:\n                        clusters[G.node[ne][\'cluster\']] += G[node][ne][\'weight\']\n                    else:\n                        clusters[G.node[ne][\'cluster\']] = G[node][ne][\'weight\']\n\n            # find the class with the highest edge weight sum\n            edge_weight_sum = 0\n            max_cluster = 0\n            #use the max sum of neighbor weights class as current node\'s class\n            for cluster in clusters:\n                if clusters[cluster] > edge_weight_sum:\n                    edge_weight_sum = clusters[cluster]\n                    max_cluster = cluster\n\n            # set the class of target node to the winning local class\n            G.node[node][\'cluster\'] = max_cluster\n\n    clusters = {}\n\n    # Prepare cluster output\n    for (_, data) in G.node.items():\n        cluster = data[\'cluster\']\n        path = data[\'path\']\n\n        if cluster:\n            if cluster not in clusters:\n                clusters[cluster] = []\n            clusters[cluster].append(path)\n\n    # Sort cluster output\n    sorted_clusters = sorted(clusters.values(), key=len, reverse=True)\n\n    return sorted_clusters\n\ndef cluster_facial_encodings(facial_encodings):\n    """""" Cluster facial encodings\n\n        Intended to be an optional switch for different clustering algorithms, as of right now\n        only chinese whispers is available.\n\n        Input:\n            facial_encodings: (image_path, facial_encoding) dictionary of facial encodings\n\n        Output:\n            sorted_clusters: a list of clusters, a cluster being a list of imagepaths,\n                sorted by largest cluster to smallest\n\n    """"""\n\n    if len(facial_encodings) <= 1:\n        print (""Number of facial encodings must be greater than one, can\'t cluster"")\n        return []\n\n    # Only use the chinese whispers algorithm for now\n    sorted_clusters = _chinese_whispers(facial_encodings.items())\n    return sorted_clusters\n\ndef compute_facial_encodings(sess,images_placeholder,embeddings,phase_train_placeholder,image_size,\n                    embedding_size,nrof_images,nrof_batches,emb_array,batch_size,paths):\n    """""" Compute Facial Encodings\n\n        Given a set of images, compute the facial encodings of each face detected in the images and\n        return them. If no faces, or more than one face found, return nothing for that image.\n\n        Inputs:\n            image_paths: a list of image paths\n\n        Outputs:\n            facial_encodings: (image_path, facial_encoding) dictionary of facial encodings\n\n    """"""\n\n    for i in range(nrof_batches):\n        start_index = i*batch_size\n        end_index = min((i+1)*batch_size, nrof_images)\n        paths_batch = paths[start_index:end_index]\n        images = facenet.load_data(paths_batch, False, False, image_size)\n        feed_dict = { images_placeholder:images, phase_train_placeholder:False }\n        emb_array[start_index:end_index,:] = sess.run(embeddings, feed_dict=feed_dict)\n\n    facial_encodings = {}\n    for x in range(nrof_images):\n        facial_encodings[paths[x]] = emb_array[x,:]\n\n\n    return facial_encodings\n\ndef get_onedir(paths):\n    dataset = []\n    path_exp = os.path.expanduser(paths)\n    if os.path.isdir(path_exp):\n        images = os.listdir(path_exp)\n        image_paths = [os.path.join(path_exp,img) for img in images]\n\n        for x in image_paths:\n            if os.path.getsize(x)>0:\n                dataset.append(x)\n        \n    return dataset \n\n\ndef main(args):\n    """""" Main\n\n    Given a list of images, save out facial encoding data files and copy\n    images into folders of face clusters.\n\n    """"""\n    from os.path import join, basename, exists\n    from os import makedirs\n    import numpy as np\n    import shutil\n    import sys\n\n    if not exists(args.output):\n        makedirs(args.output)\n\n    with tf.Graph().as_default():\n        with tf.Session() as sess:\n            image_paths = get_onedir(args.input)\n            #image_list, label_list = facenet.get_image_paths_and_labels(train_set)\n\n            meta_file, ckpt_file = facenet.get_model_filenames(os.path.expanduser(args.model_dir))\n            \n            print(\'Metagraph file: %s\' % meta_file)\n            print(\'Checkpoint file: %s\' % ckpt_file)\n            load_model(args.model_dir, meta_file, ckpt_file)\n            \n            # Get input and output tensors\n            images_placeholder = tf.get_default_graph().get_tensor_by_name(""input:0"")\n            embeddings = tf.get_default_graph().get_tensor_by_name(""embeddings:0"")\n            phase_train_placeholder = tf.get_default_graph().get_tensor_by_name(""phase_train:0"")\n            \n            image_size = images_placeholder.get_shape()[1]\n            print(""image_size:"",image_size)\n            embedding_size = embeddings.get_shape()[1]\n        \n            # Run forward pass to calculate embeddings\n            print(\'Runnning forward pass on images\') \n\n            nrof_images = len(image_paths)\n            nrof_batches = int(math.ceil(1.0*nrof_images / args.batch_size))\n            emb_array = np.zeros((nrof_images, embedding_size))\n            facial_encodings = compute_facial_encodings(sess,images_placeholder,embeddings,phase_train_placeholder,image_size,\n                embedding_size,nrof_images,nrof_batches,emb_array,args.batch_size,image_paths)\n            sorted_clusters = cluster_facial_encodings(facial_encodings)\n            num_cluster = len(sorted_clusters)\n                \n            # Copy image files to cluster folders\n            for idx, cluster in enumerate(sorted_clusters):\n                #save all the cluster\n                cluster_dir = join(args.output, str(idx))\n                if not exists(cluster_dir):\n                    makedirs(cluster_dir)\n                for path in cluster:\n                    shutil.copy(path, join(cluster_dir, basename(path)))\n\ndef parse_args():\n    """"""Parse input arguments.""""""\n    import argparse\n    parser = argparse.ArgumentParser(description=\'Get a shape mesh (t-pose)\')\n    parser.add_argument(\'--model_dir\', type=str, help=\'model dir\', required=True)\n    parser.add_argument(\'--batch_size\', type=int, help=\'batch size\', required=30)\n    parser.add_argument(\'--input\', type=str, help=\'Input dir of images\', required=True)\n    parser.add_argument(\'--output\', type=str, help=\'Output dir of clusters\', required=True)\n    args = parser.parse_args()\n\n    return args\n\nif __name__ == \'__main__\':\n    """""" Entry point """"""\n    main(parse_args())\n'"
contributed/export_embeddings.py,8,"b'""""""\nExports the embeddings and labels of a directory of images as numpy arrays.\n\nTypicall usage expect the image directory to be of the openface/facenet form and\nthe images to be aligned. Simply point to your model and your image directory:\n    python facenet/contributed/export_embeddings.py ~/models/facenet/20170216-091149/ ~/datasets/lfw/mylfw\n\nOutput:\nembeddings.npy -- Embeddings as np array, Use --embeddings_name to change name\nlabels.npy -- Integer labels as np array, Use --labels_name to change name\nlabel_strings.npy -- Strings from folders names, --labels_strings_name to change name\n\n\nUse --image_batch to dictacte how many images to load in memory at a time.\n\nIf your images aren\'t already pre-aligned, use --is_aligned False\n\nI started with compare.py from David Sandberg, and modified it to export\nthe embeddings. The image loading is done use the facenet library if the image\nis pre-aligned. If the image isn\'t pre-aligned, I use the compare.py function.\nI\'ve found working with the embeddings useful for classifications models.\n\nCharles Jekel 2017\n\n""""""\n\n# MIT License\n#\n# Copyright (c) 2016 David Sandberg\n#\n# Permission is hereby granted, free of charge, to any person obtaining a copy\n# of this software and associated documentation files (the ""Software""), to deal\n# in the Software without restriction, including without limitation the rights\n# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n# copies of the Software, and to permit persons to whom the Software is\n# furnished to do so, subject to the following conditions:\n#\n# The above copyright notice and this permission notice shall be included in all\n# copies or substantial portions of the Software.\n#\n# THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n# SOFTWARE.\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport time\nfrom scipy import misc\nimport tensorflow as tf\nimport numpy as np\nimport sys\nimport os\nimport argparse\nimport facenet\nimport align.detect_face\nimport glob\n\nfrom six.moves import xrange\n\ndef main(args):\n    train_set = facenet.get_dataset(args.data_dir)\n    image_list, label_list = facenet.get_image_paths_and_labels(train_set)\n    # fetch the classes (labels as strings) exactly as it\'s done in get_dataset\n    path_exp = os.path.expanduser(args.data_dir)\n    classes = [path for path in os.listdir(path_exp) \\\n               if os.path.isdir(os.path.join(path_exp, path))]\n    classes.sort()\n    # get the label strings\n    label_strings = [name for name in classes if \\\n       os.path.isdir(os.path.join(path_exp, name))]\n\n    with tf.Graph().as_default():\n\n        with tf.Session() as sess:\n\n            # Load the model\n            facenet.load_model(args.model_dir)\n\n            # Get input and output tensors\n            images_placeholder = tf.get_default_graph().get_tensor_by_name(""input:0"")\n            embeddings = tf.get_default_graph().get_tensor_by_name(""embeddings:0"")\n            phase_train_placeholder = tf.get_default_graph().get_tensor_by_name(""phase_train:0"")\n\n            # Run forward pass to calculate embeddings\n            nrof_images = len(image_list)\n            print(\'Number of images: \', nrof_images)\n            batch_size = args.image_batch\n            if nrof_images % batch_size == 0:\n                nrof_batches = nrof_images // batch_size\n            else:\n                nrof_batches = (nrof_images // batch_size) + 1\n            print(\'Number of batches: \', nrof_batches)\n            embedding_size = embeddings.get_shape()[1]\n            emb_array = np.zeros((nrof_images, embedding_size))\n            start_time = time.time()\n\n            for i in range(nrof_batches):\n                if i == nrof_batches -1:\n                    n = nrof_images\n                else:\n                    n = i*batch_size + batch_size\n                # Get images for the batch\n                if args.is_aligned is True:\n                    images = facenet.load_data(image_list[i*batch_size:n], False, False, args.image_size)\n                else:\n                    images = load_and_align_data(image_list[i*batch_size:n], args.image_size, args.margin, args.gpu_memory_fraction)\n                feed_dict = { images_placeholder: images, phase_train_placeholder:False }\n                # Use the facenet model to calcualte embeddings\n                embed = sess.run(embeddings, feed_dict=feed_dict)\n                emb_array[i*batch_size:n, :] = embed\n                print(\'Completed batch\', i+1, \'of\', nrof_batches)\n\n            run_time = time.time() - start_time\n            print(\'Run time: \', run_time)\n\n            #   export emedings and labels\n            label_list  = np.array(label_list)\n\n            np.save(args.embeddings_name, emb_array)\n            np.save(args.labels_name, label_list)\n            label_strings = np.array(label_strings)\n            np.save(args.labels_strings_name, label_strings[label_list])\n\n\ndef load_and_align_data(image_paths, image_size, margin, gpu_memory_fraction):\n\n    minsize = 20 # minimum size of face\n    threshold = [ 0.6, 0.7, 0.7 ]  # three steps\'s threshold\n    factor = 0.709 # scale factor\n\n    print(\'Creating networks and loading parameters\')\n    with tf.Graph().as_default():\n        gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=gpu_memory_fraction)\n        sess = tf.Session(config=tf.ConfigProto(gpu_options=gpu_options, log_device_placement=False))\n        with sess.as_default():\n            pnet, rnet, onet = align.detect_face.create_mtcnn(sess, None)\n\n    nrof_samples = len(image_paths)\n    img_list = [None] * nrof_samples\n    for i in xrange(nrof_samples):\n        print(image_paths[i])\n        img = misc.imread(os.path.expanduser(image_paths[i]))\n        img_size = np.asarray(img.shape)[0:2]\n        bounding_boxes, _ = align.detect_face.detect_face(img, minsize, pnet, rnet, onet, threshold, factor)\n        det = np.squeeze(bounding_boxes[0,0:4])\n        bb = np.zeros(4, dtype=np.int32)\n        bb[0] = np.maximum(det[0]-margin/2, 0)\n        bb[1] = np.maximum(det[1]-margin/2, 0)\n        bb[2] = np.minimum(det[2]+margin/2, img_size[1])\n        bb[3] = np.minimum(det[3]+margin/2, img_size[0])\n        cropped = img[bb[1]:bb[3],bb[0]:bb[2],:]\n        aligned = misc.imresize(cropped, (image_size, image_size), interp=\'bilinear\')\n        prewhitened = facenet.prewhiten(aligned)\n        img_list[i] = prewhitened\n    images = np.stack(img_list)\n    return images\n\ndef parse_arguments(argv):\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\'model_dir\', type=str,\n        help=\'Directory containing the meta_file and ckpt_file\')\n    parser.add_argument(\'data_dir\', type=str,\n        help=\'Directory containing images. If images are not already aligned and cropped include --is_aligned False.\')\n    parser.add_argument(\'--is_aligned\', type=str,\n        help=\'Is the data directory already aligned and cropped?\', default=True)\n    parser.add_argument(\'--image_size\', type=int,\n        help=\'Image size (height, width) in pixels.\', default=160)\n    parser.add_argument(\'--margin\', type=int,\n        help=\'Margin for the crop around the bounding box (height, width) in pixels.\',\n        default=44)\n    parser.add_argument(\'--gpu_memory_fraction\', type=float,\n        help=\'Upper bound on the amount of GPU memory that will be used by the process.\',\n        default=1.0)\n    parser.add_argument(\'--image_batch\', type=int,\n        help=\'Number of images stored in memory at a time. Default 500.\',\n        default=500)\n\n    #   numpy file Names\n    parser.add_argument(\'--embeddings_name\', type=str,\n        help=\'Enter string of which the embeddings numpy array is saved as.\',\n        default=\'embeddings.npy\')\n    parser.add_argument(\'--labels_name\', type=str,\n        help=\'Enter string of which the labels numpy array is saved as.\',\n        default=\'labels.npy\')\n    parser.add_argument(\'--labels_strings_name\', type=str,\n        help=\'Enter string of which the labels as strings numpy array is saved as.\',\n        default=\'label_strings.npy\')\n    return parser.parse_args(argv)\n\nif __name__ == \'__main__\':\n    main(parse_arguments(sys.argv[1:]))\n'"
contributed/face.py,7,"b'# coding=utf-8\n""""""Face Detection and Recognition""""""\n# MIT License\n#\n# Copyright (c) 2017 Fran\xc3\xa7ois Gervais\n#\n# This is the work of David Sandberg and shanren7 remodelled into a\n# high level container. It\'s an attempt to simplify the use of such\n# technology and provide an easy to use facial recognition package.\n#\n# https://github.com/davidsandberg/facenet\n# https://github.com/shanren7/real_time_face_recognition\n#\n# Permission is hereby granted, free of charge, to any person obtaining a copy\n# of this software and associated documentation files (the ""Software""), to deal\n# in the Software without restriction, including without limitation the rights\n# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n# copies of the Software, and to permit persons to whom the Software is\n# furnished to do so, subject to the following conditions:\n#\n# The above copyright notice and this permission notice shall be included in all\n# copies or substantial portions of the Software.\n#\n# THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n# SOFTWARE.\n\nimport pickle\nimport os\n\nimport cv2\nimport numpy as np\nimport tensorflow as tf\nfrom scipy import misc\n\nimport align.detect_face\nimport facenet\n\n\ngpu_memory_fraction = 0.3\nfacenet_model_checkpoint = os.path.dirname(__file__) + ""/../model_checkpoints/20170512-110547""\nclassifier_model = os.path.dirname(__file__) + ""/../model_checkpoints/my_classifier_1.pkl""\ndebug = False\n\n\nclass Face:\n    def __init__(self):\n        self.name = None\n        self.bounding_box = None\n        self.image = None\n        self.container_image = None\n        self.embedding = None\n\n\nclass Recognition:\n    def __init__(self):\n        self.detect = Detection()\n        self.encoder = Encoder()\n        self.identifier = Identifier()\n\n    def add_identity(self, image, person_name):\n        faces = self.detect.find_faces(image)\n\n        if len(faces) == 1:\n            face = faces[0]\n            face.name = person_name\n            face.embedding = self.encoder.generate_embedding(face)\n            return faces\n\n    def identify(self, image):\n        faces = self.detect.find_faces(image)\n\n        for i, face in enumerate(faces):\n            if debug:\n                cv2.imshow(""Face: "" + str(i), face.image)\n            face.embedding = self.encoder.generate_embedding(face)\n            face.name = self.identifier.identify(face)\n\n        return faces\n\n\nclass Identifier:\n    def __init__(self):\n        with open(classifier_model, \'rb\') as infile:\n            self.model, self.class_names = pickle.load(infile)\n\n    def identify(self, face):\n        if face.embedding is not None:\n            predictions = self.model.predict_proba([face.embedding])\n            best_class_indices = np.argmax(predictions, axis=1)\n            return self.class_names[best_class_indices[0]]\n\n\nclass Encoder:\n    def __init__(self):\n        self.sess = tf.Session()\n        with self.sess.as_default():\n            facenet.load_model(facenet_model_checkpoint)\n\n    def generate_embedding(self, face):\n        # Get input and output tensors\n        images_placeholder = tf.get_default_graph().get_tensor_by_name(""input:0"")\n        embeddings = tf.get_default_graph().get_tensor_by_name(""embeddings:0"")\n        phase_train_placeholder = tf.get_default_graph().get_tensor_by_name(""phase_train:0"")\n\n        prewhiten_face = facenet.prewhiten(face.image)\n\n        # Run forward pass to calculate embeddings\n        feed_dict = {images_placeholder: [prewhiten_face], phase_train_placeholder: False}\n        return self.sess.run(embeddings, feed_dict=feed_dict)[0]\n\n\nclass Detection:\n    # face detection parameters\n    minsize = 20  # minimum size of face\n    threshold = [0.6, 0.7, 0.7]  # three steps\'s threshold\n    factor = 0.709  # scale factor\n\n    def __init__(self, face_crop_size=160, face_crop_margin=32):\n        self.pnet, self.rnet, self.onet = self._setup_mtcnn()\n        self.face_crop_size = face_crop_size\n        self.face_crop_margin = face_crop_margin\n\n    def _setup_mtcnn(self):\n        with tf.Graph().as_default():\n            gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=gpu_memory_fraction)\n            sess = tf.Session(config=tf.ConfigProto(gpu_options=gpu_options, log_device_placement=False))\n            with sess.as_default():\n                return align.detect_face.create_mtcnn(sess, None)\n\n    def find_faces(self, image):\n        faces = []\n\n        bounding_boxes, _ = align.detect_face.detect_face(image, self.minsize,\n                                                          self.pnet, self.rnet, self.onet,\n                                                          self.threshold, self.factor)\n        for bb in bounding_boxes:\n            face = Face()\n            face.container_image = image\n            face.bounding_box = np.zeros(4, dtype=np.int32)\n\n            img_size = np.asarray(image.shape)[0:2]\n            face.bounding_box[0] = np.maximum(bb[0] - self.face_crop_margin / 2, 0)\n            face.bounding_box[1] = np.maximum(bb[1] - self.face_crop_margin / 2, 0)\n            face.bounding_box[2] = np.minimum(bb[2] + self.face_crop_margin / 2, img_size[1])\n            face.bounding_box[3] = np.minimum(bb[3] + self.face_crop_margin / 2, img_size[0])\n            cropped = image[face.bounding_box[1]:face.bounding_box[3], face.bounding_box[0]:face.bounding_box[2], :]\n            face.image = misc.imresize(cropped, (self.face_crop_size, self.face_crop_size), interp=\'bilinear\')\n\n            faces.append(face)\n\n        return faces\n'"
contributed/predict.py,8,"b'\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\n#----------------------------------------------------\n# MIT License\n#\n# Copyright (c) 2017 Rishi Rai\n#\n# Permission is hereby granted, free of charge, to any person obtaining a copy\n# of this software and associated documentation files (the ""Software""), to deal\n# in the Software without restriction, including without limitation the rights\n# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n# copies of the Software, and to permit persons to whom the Software is\n# furnished to do so, subject to the following conditions:\n#\n# The above copyright notice and this permission notice shall be included in all\n# copies or substantial portions of the Software.\n#\n# THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n# SOFTWARE.\n#----------------------------------------------------\n\n\nimport tensorflow as tf\nimport numpy as np\nimport argparse\nimport facenet\nimport os\nimport sys\nimport math\nimport pickle\nfrom sklearn.svm import SVC\nfrom scipy import misc\nimport align.detect_face\nfrom six.moves import xrange\n\ndef main(args):\n  \n    images, cout_per_image, nrof_samples = load_and_align_data(args.image_files,args.image_size, args.margin, args.gpu_memory_fraction)\n    with tf.Graph().as_default():\n\n       with tf.Session() as sess:\n      \n            # Load the model\n                facenet.load_model(args.model)\n            # Get input and output tensors\n                images_placeholder = tf.get_default_graph().get_tensor_by_name(""input:0"")\n                embeddings = tf.get_default_graph().get_tensor_by_name(""embeddings:0"")\n                phase_train_placeholder = tf.get_default_graph().get_tensor_by_name(""phase_train:0"")\n\n            # Run forward pass to calculate embeddings\n                feed_dict = { images_placeholder: images , phase_train_placeholder:False}\n                emb = sess.run(embeddings, feed_dict=feed_dict)\n                classifier_filename_exp = os.path.expanduser(args.classifier_filename)\n                with open(classifier_filename_exp, \'rb\') as infile:\n                    (model, class_names) = pickle.load(infile)\n                print(\'Loaded classifier model from file ""%s""\\n\' % classifier_filename_exp)\n                predictions = model.predict_proba(emb)\n                best_class_indices = np.argmax(predictions, axis=1)\n                best_class_probabilities = predictions[np.arange(len(best_class_indices)), best_class_indices]\n                k=0     \n\t    #print predictions       \n                for i in range(nrof_samples):\n                    print(""\\npeople in image %s :"" %(args.image_files[i]))\n                    for j in range(cout_per_image[i]):\n                        print(\'%s: %.3f\' % (class_names[best_class_indices[k]], best_class_probabilities[k]))\n                        k+=1\n                    \ndef load_and_align_data(image_paths, image_size, margin, gpu_memory_fraction):\n\n    minsize = 20 # minimum size of face\n    threshold = [ 0.6, 0.7, 0.7 ]  # three steps\'s threshold\n    factor = 0.709 # scale factor\n    \n    print(\'Creating networks and loading parameters\')\n    with tf.Graph().as_default():\n        gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=gpu_memory_fraction)\n        sess = tf.Session(config=tf.ConfigProto(gpu_options=gpu_options, log_device_placement=False))\n        with sess.as_default():\n            pnet, rnet, onet = align.detect_face.create_mtcnn(sess, None)\n  \n    nrof_samples = len(image_paths)\n    img_list = [] \n    count_per_image = []\n    for i in xrange(nrof_samples):\n        img = misc.imread(os.path.expanduser(image_paths[i]))\n        img_size = np.asarray(img.shape)[0:2]\n        bounding_boxes, _ = align.detect_face.detect_face(img, minsize, pnet, rnet, onet, threshold, factor)\n        count_per_image.append(len(bounding_boxes))\n        for j in range(len(bounding_boxes)):\t\n                det = np.squeeze(bounding_boxes[j,0:4])\n                bb = np.zeros(4, dtype=np.int32)\n                bb[0] = np.maximum(det[0]-margin/2, 0)\n                bb[1] = np.maximum(det[1]-margin/2, 0)\n                bb[2] = np.minimum(det[2]+margin/2, img_size[1])\n                bb[3] = np.minimum(det[3]+margin/2, img_size[0])\n                cropped = img[bb[1]:bb[3],bb[0]:bb[2],:]\n                aligned = misc.imresize(cropped, (image_size, image_size), interp=\'bilinear\')\n                prewhitened = facenet.prewhiten(aligned)\n                img_list.append(prewhitened)\t\t\n    images = np.stack(img_list)\n    return images, count_per_image, nrof_samples\n\ndef parse_arguments(argv):\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\'image_files\', type=str, nargs=\'+\', help=\'Path(s) of the image(s)\')\n    parser.add_argument(\'model\', type=str, \n        help=\'Could be either a directory containing the meta_file and ckpt_file or a model protobuf (.pb) file\')\n    parser.add_argument(\'classifier_filename\', \n        help=\'Classifier model file name as a pickle (.pkl) file. \' + \n        \'For training this is the output and for classification this is an input.\')\n    parser.add_argument(\'--image_size\', type=int,\n        help=\'Image size (height, width) in pixels.\', default=160)\n    parser.add_argument(\'--seed\', type=int,\n        help=\'Random seed.\', default=666)\n    parser.add_argument(\'--margin\', type=int,\n        help=\'Margin for the crop around the bounding box (height, width) in pixels.\', default=44)\n    parser.add_argument(\'--gpu_memory_fraction\', type=float,\n        help=\'Upper bound on the amount of GPU memory that will be used by the process.\', default=1.0)\n    return parser.parse_args(argv)\n\nif __name__ == \'__main__\':\n    main(parse_arguments(sys.argv[1:]))\n\n'"
contributed/real_time_face_recognition.py,0,"b'# coding=utf-8\n""""""Performs face detection in realtime.\n\nBased on code from https://github.com/shanren7/real_time_face_recognition\n""""""\n# MIT License\n#\n# Copyright (c) 2017 Fran\xc3\xa7ois Gervais\n#\n# Permission is hereby granted, free of charge, to any person obtaining a copy\n# of this software and associated documentation files (the ""Software""), to deal\n# in the Software without restriction, including without limitation the rights\n# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n# copies of the Software, and to permit persons to whom the Software is\n# furnished to do so, subject to the following conditions:\n#\n# The above copyright notice and this permission notice shall be included in all\n# copies or substantial portions of the Software.\n#\n# THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n# SOFTWARE.\nimport argparse\nimport sys\nimport time\n\nimport cv2\n\nimport face\n\n\ndef add_overlays(frame, faces, frame_rate):\n    if faces is not None:\n        for face in faces:\n            face_bb = face.bounding_box.astype(int)\n            cv2.rectangle(frame,\n                          (face_bb[0], face_bb[1]), (face_bb[2], face_bb[3]),\n                          (0, 255, 0), 2)\n            if face.name is not None:\n                cv2.putText(frame, face.name, (face_bb[0], face_bb[3]),\n                            cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0),\n                            thickness=2, lineType=2)\n\n    cv2.putText(frame, str(frame_rate) + "" fps"", (10, 30),\n                cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0),\n                thickness=2, lineType=2)\n\n\ndef main(args):\n    frame_interval = 3  # Number of frames after which to run face detection\n    fps_display_interval = 5  # seconds\n    frame_rate = 0\n    frame_count = 0\n\n    video_capture = cv2.VideoCapture(0)\n    face_recognition = face.Recognition()\n    start_time = time.time()\n\n    if args.debug:\n        print(""Debug enabled"")\n        face.debug = True\n\n    while True:\n        # Capture frame-by-frame\n        ret, frame = video_capture.read()\n\n        if (frame_count % frame_interval) == 0:\n            faces = face_recognition.identify(frame)\n\n            # Check our current fps\n            end_time = time.time()\n            if (end_time - start_time) > fps_display_interval:\n                frame_rate = int(frame_count / (end_time - start_time))\n                start_time = time.time()\n                frame_count = 0\n\n        add_overlays(frame, faces, frame_rate)\n\n        frame_count += 1\n        cv2.imshow(\'Video\', frame)\n\n        if cv2.waitKey(1) & 0xFF == ord(\'q\'):\n            break\n\n    # When everything is done, release the capture\n    video_capture.release()\n    cv2.destroyAllWindows()\n\n\ndef parse_arguments(argv):\n    parser = argparse.ArgumentParser()\n\n    parser.add_argument(\'--debug\', action=\'store_true\',\n                        help=\'Enable some debug outputs.\')\n    return parser.parse_args(argv)\n\n\nif __name__ == \'__main__\':\n    main(parse_arguments(sys.argv[1:]))\n'"
src/__init__.py,0,b'# flake8: noqa\n\n'
src/calculate_filtering_metrics.py,6,"b'""""""Calculate filtering metrics for a dataset and store in a .hdf file.\n""""""\n# MIT License\n# \n# Copyright (c) 2016 David Sandberg\n# \n# Permission is hereby granted, free of charge, to any person obtaining a copy\n# of this software and associated documentation files (the ""Software""), to deal\n# in the Software without restriction, including without limitation the rights\n# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n# copies of the Software, and to permit persons to whom the Software is\n# furnished to do so, subject to the following conditions:\n# \n# The above copyright notice and this permission notice shall be included in all\n# copies or substantial portions of the Software.\n# \n# THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n# SOFTWARE.\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\nimport numpy as np\nimport argparse\nimport facenet\nimport os\nimport sys\nimport time\nimport h5py\nimport math\nfrom tensorflow.python.platform import gfile\nfrom six import iteritems\n\ndef main(args):\n    dataset = facenet.get_dataset(args.dataset_dir)\n  \n    with tf.Graph().as_default():\n      \n        # Get a list of image paths and their labels\n        image_list, label_list = facenet.get_image_paths_and_labels(dataset)\n        nrof_images = len(image_list)\n        image_indices = range(nrof_images)\n\n        image_batch, label_batch = facenet.read_and_augment_data(image_list,\n            image_indices, args.image_size, args.batch_size, None, \n            False, False, False, nrof_preprocess_threads=4, shuffle=False)\n        \n        model_exp = os.path.expanduser(args.model_file)\n        with gfile.FastGFile(model_exp,\'rb\') as f:\n            graph_def = tf.GraphDef()\n            graph_def.ParseFromString(f.read())\n            input_map={\'input\':image_batch, \'phase_train\':False}\n            tf.import_graph_def(graph_def, input_map=input_map, name=\'net\')\n        \n        embeddings = tf.get_default_graph().get_tensor_by_name(""net/embeddings:0"")\n\n        with tf.Session() as sess:\n            tf.train.start_queue_runners(sess=sess)\n                \n            embedding_size = int(embeddings.get_shape()[1])\n            nrof_batches = int(math.ceil(nrof_images / args.batch_size))\n            nrof_classes = len(dataset)\n            label_array = np.array(label_list)\n            class_names = [cls.name for cls in dataset]\n            nrof_examples_per_class = [ len(cls.image_paths) for cls in dataset ]\n            class_variance = np.zeros((nrof_classes,))\n            class_center = np.zeros((nrof_classes,embedding_size))\n            distance_to_center = np.ones((len(label_list),))*np.NaN\n            emb_array = np.zeros((0,embedding_size))\n            idx_array = np.zeros((0,), dtype=np.int32)\n            lab_array = np.zeros((0,), dtype=np.int32)\n            index_arr = np.append(0, np.cumsum(nrof_examples_per_class))\n            for i in range(nrof_batches):\n                t = time.time()\n                emb, idx = sess.run([embeddings, label_batch])\n                emb_array = np.append(emb_array, emb, axis=0)\n                idx_array = np.append(idx_array, idx, axis=0)\n                lab_array = np.append(lab_array, label_array[idx], axis=0)\n                for cls in set(lab_array):\n                    cls_idx = np.where(lab_array==cls)[0]\n                    if cls_idx.shape[0]==nrof_examples_per_class[cls]:\n                        # We have calculated all the embeddings for this class\n                        i2 = np.argsort(idx_array[cls_idx])\n                        emb_class = emb_array[cls_idx,:]\n                        emb_sort = emb_class[i2,:]\n                        center = np.mean(emb_sort, axis=0)\n                        diffs = emb_sort - center\n                        dists_sqr = np.sum(np.square(diffs), axis=1)\n                        class_variance[cls] = np.mean(dists_sqr)\n                        class_center[cls,:] = center\n                        distance_to_center[index_arr[cls]:index_arr[cls+1]] = np.sqrt(dists_sqr)\n                        emb_array = np.delete(emb_array, cls_idx, axis=0)\n                        idx_array = np.delete(idx_array, cls_idx, axis=0)\n                        lab_array = np.delete(lab_array, cls_idx, axis=0)\n\n                        \n                print(\'Batch %d in %.3f seconds\' % (i, time.time()-t))\n                \n            print(\'Writing filtering data to %s\' % args.data_file_name)\n            mdict = {\'class_names\':class_names, \'image_list\':image_list, \'label_list\':label_list, \'distance_to_center\':distance_to_center }\n            with h5py.File(args.data_file_name, \'w\') as f:\n                for key, value in iteritems(mdict):\n                    f.create_dataset(key, data=value)\n                        \ndef parse_arguments(argv):\n    parser = argparse.ArgumentParser()\n    \n    parser.add_argument(\'dataset_dir\', type=str,\n        help=\'Path to the directory containing aligned dataset.\')\n    parser.add_argument(\'model_file\', type=str,\n        help=\'File containing the frozen model in protobuf (.pb) format to use for feature extraction.\')\n    parser.add_argument(\'data_file_name\', type=str,\n        help=\'The name of the file to store filtering data in.\')\n    parser.add_argument(\'--image_size\', type=int,\n        help=\'Image size.\', default=160)\n    parser.add_argument(\'--batch_size\', type=int,\n        help=\'Number of images to process in a batch.\', default=90)\n    return parser.parse_args(argv)\n\nif __name__ == \'__main__\':\n    main(parse_arguments(sys.argv[1:]))\n'"
src/classifier.py,5,"b'""""""An example of how to use your own dataset to train a classifier that recognizes people.\n""""""\n# MIT License\n# \n# Copyright (c) 2016 David Sandberg\n# \n# Permission is hereby granted, free of charge, to any person obtaining a copy\n# of this software and associated documentation files (the ""Software""), to deal\n# in the Software without restriction, including without limitation the rights\n# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n# copies of the Software, and to permit persons to whom the Software is\n# furnished to do so, subject to the following conditions:\n# \n# The above copyright notice and this permission notice shall be included in all\n# copies or substantial portions of the Software.\n# \n# THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n# SOFTWARE.\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\nimport numpy as np\nimport argparse\nimport facenet\nimport os\nimport sys\nimport math\nimport pickle\nfrom sklearn.svm import SVC\n\ndef main(args):\n  \n    with tf.Graph().as_default():\n      \n        with tf.Session() as sess:\n            \n            np.random.seed(seed=args.seed)\n            \n            if args.use_split_dataset:\n                dataset_tmp = facenet.get_dataset(args.data_dir)\n                train_set, test_set = split_dataset(dataset_tmp, args.min_nrof_images_per_class, args.nrof_train_images_per_class)\n                if (args.mode==\'TRAIN\'):\n                    dataset = train_set\n                elif (args.mode==\'CLASSIFY\'):\n                    dataset = test_set\n            else:\n                dataset = facenet.get_dataset(args.data_dir)\n\n            # Check that there are at least one training image per class\n            for cls in dataset:\n                assert(len(cls.image_paths)>0, \'There must be at least one image for each class in the dataset\')            \n\n                 \n            paths, labels = facenet.get_image_paths_and_labels(dataset)\n            \n            print(\'Number of classes: %d\' % len(dataset))\n            print(\'Number of images: %d\' % len(paths))\n            \n            # Load the model\n            print(\'Loading feature extraction model\')\n            facenet.load_model(args.model)\n            \n            # Get input and output tensors\n            images_placeholder = tf.get_default_graph().get_tensor_by_name(""input:0"")\n            embeddings = tf.get_default_graph().get_tensor_by_name(""embeddings:0"")\n            phase_train_placeholder = tf.get_default_graph().get_tensor_by_name(""phase_train:0"")\n            embedding_size = embeddings.get_shape()[1]\n            \n            # Run forward pass to calculate embeddings\n            print(\'Calculating features for images\')\n            nrof_images = len(paths)\n            nrof_batches_per_epoch = int(math.ceil(1.0*nrof_images / args.batch_size))\n            emb_array = np.zeros((nrof_images, embedding_size))\n            for i in range(nrof_batches_per_epoch):\n                start_index = i*args.batch_size\n                end_index = min((i+1)*args.batch_size, nrof_images)\n                paths_batch = paths[start_index:end_index]\n                images = facenet.load_data(paths_batch, False, False, args.image_size)\n                feed_dict = { images_placeholder:images, phase_train_placeholder:False }\n                emb_array[start_index:end_index,:] = sess.run(embeddings, feed_dict=feed_dict)\n            \n            classifier_filename_exp = os.path.expanduser(args.classifier_filename)\n\n            if (args.mode==\'TRAIN\'):\n                # Train classifier\n                print(\'Training classifier\')\n                model = SVC(kernel=\'linear\', probability=True)\n                model.fit(emb_array, labels)\n            \n                # Create a list of class names\n                class_names = [ cls.name.replace(\'_\', \' \') for cls in dataset]\n\n                # Saving classifier model\n                with open(classifier_filename_exp, \'wb\') as outfile:\n                    pickle.dump((model, class_names), outfile)\n                print(\'Saved classifier model to file ""%s""\' % classifier_filename_exp)\n                \n            elif (args.mode==\'CLASSIFY\'):\n                # Classify images\n                print(\'Testing classifier\')\n                with open(classifier_filename_exp, \'rb\') as infile:\n                    (model, class_names) = pickle.load(infile)\n\n                print(\'Loaded classifier model from file ""%s""\' % classifier_filename_exp)\n\n                predictions = model.predict_proba(emb_array)\n                best_class_indices = np.argmax(predictions, axis=1)\n                best_class_probabilities = predictions[np.arange(len(best_class_indices)), best_class_indices]\n                \n                for i in range(len(best_class_indices)):\n                    print(\'%4d  %s: %.3f\' % (i, class_names[best_class_indices[i]], best_class_probabilities[i]))\n                    \n                accuracy = np.mean(np.equal(best_class_indices, labels))\n                print(\'Accuracy: %.3f\' % accuracy)\n                \n            \ndef split_dataset(dataset, min_nrof_images_per_class, nrof_train_images_per_class):\n    train_set = []\n    test_set = []\n    for cls in dataset:\n        paths = cls.image_paths\n        # Remove classes with less than min_nrof_images_per_class\n        if len(paths)>=min_nrof_images_per_class:\n            np.random.shuffle(paths)\n            train_set.append(facenet.ImageClass(cls.name, paths[:nrof_train_images_per_class]))\n            test_set.append(facenet.ImageClass(cls.name, paths[nrof_train_images_per_class:]))\n    return train_set, test_set\n\n            \ndef parse_arguments(argv):\n    parser = argparse.ArgumentParser()\n    \n    parser.add_argument(\'mode\', type=str, choices=[\'TRAIN\', \'CLASSIFY\'],\n        help=\'Indicates if a new classifier should be trained or a classification \' + \n        \'model should be used for classification\', default=\'CLASSIFY\')\n    parser.add_argument(\'data_dir\', type=str,\n        help=\'Path to the data directory containing aligned LFW face patches.\')\n    parser.add_argument(\'model\', type=str, \n        help=\'Could be either a directory containing the meta_file and ckpt_file or a model protobuf (.pb) file\')\n    parser.add_argument(\'classifier_filename\', \n        help=\'Classifier model file name as a pickle (.pkl) file. \' + \n        \'For training this is the output and for classification this is an input.\')\n    parser.add_argument(\'--use_split_dataset\', \n        help=\'Indicates that the dataset specified by data_dir should be split into a training and test set. \' +  \n        \'Otherwise a separate test set can be specified using the test_data_dir option.\', action=\'store_true\')\n    parser.add_argument(\'--test_data_dir\', type=str,\n        help=\'Path to the test data directory containing aligned images used for testing.\')\n    parser.add_argument(\'--batch_size\', type=int,\n        help=\'Number of images to process in a batch.\', default=90)\n    parser.add_argument(\'--image_size\', type=int,\n        help=\'Image size (height, width) in pixels.\', default=160)\n    parser.add_argument(\'--seed\', type=int,\n        help=\'Random seed.\', default=666)\n    parser.add_argument(\'--min_nrof_images_per_class\', type=int,\n        help=\'Only include classes with at least this number of images in the dataset\', default=20)\n    parser.add_argument(\'--nrof_train_images_per_class\', type=int,\n        help=\'Use this number of images from each class for training and the rest for testing\', default=10)\n    \n    return parser.parse_args(argv)\n\nif __name__ == \'__main__\':\n    main(parse_arguments(sys.argv[1:]))\n'"
src/compare.py,8,"b'""""""Performs face alignment and calculates L2 distance between the embeddings of images.""""""\n\n# MIT License\n# \n# Copyright (c) 2016 David Sandberg\n# \n# Permission is hereby granted, free of charge, to any person obtaining a copy\n# of this software and associated documentation files (the ""Software""), to deal\n# in the Software without restriction, including without limitation the rights\n# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n# copies of the Software, and to permit persons to whom the Software is\n# furnished to do so, subject to the following conditions:\n# \n# The above copyright notice and this permission notice shall be included in all\n# copies or substantial portions of the Software.\n# \n# THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n# SOFTWARE.\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom scipy import misc\nimport tensorflow as tf\nimport numpy as np\nimport sys\nimport os\nimport copy\nimport argparse\nimport facenet\nimport align.detect_face\n\ndef main(args):\n\n    images = load_and_align_data(args.image_files, args.image_size, args.margin, args.gpu_memory_fraction)\n    with tf.Graph().as_default():\n\n        with tf.Session() as sess:\n      \n            # Load the model\n            facenet.load_model(args.model)\n    \n            # Get input and output tensors\n            images_placeholder = tf.get_default_graph().get_tensor_by_name(""input:0"")\n            embeddings = tf.get_default_graph().get_tensor_by_name(""embeddings:0"")\n            phase_train_placeholder = tf.get_default_graph().get_tensor_by_name(""phase_train:0"")\n\n            # Run forward pass to calculate embeddings\n            feed_dict = { images_placeholder: images, phase_train_placeholder:False }\n            emb = sess.run(embeddings, feed_dict=feed_dict)\n            \n            nrof_images = len(args.image_files)\n\n            print(\'Images:\')\n            for i in range(nrof_images):\n                print(\'%1d: %s\' % (i, args.image_files[i]))\n            print(\'\')\n            \n            # Print distance matrix\n            print(\'Distance matrix\')\n            print(\'    \', end=\'\')\n            for i in range(nrof_images):\n                print(\'    %1d     \' % i, end=\'\')\n            print(\'\')\n            for i in range(nrof_images):\n                print(\'%1d  \' % i, end=\'\')\n                for j in range(nrof_images):\n                    dist = np.sqrt(np.sum(np.square(np.subtract(emb[i,:], emb[j,:]))))\n                    print(\'  %1.4f  \' % dist, end=\'\')\n                print(\'\')\n            \n            \ndef load_and_align_data(image_paths, image_size, margin, gpu_memory_fraction):\n\n    minsize = 20 # minimum size of face\n    threshold = [ 0.6, 0.7, 0.7 ]  # three steps\'s threshold\n    factor = 0.709 # scale factor\n    \n    print(\'Creating networks and loading parameters\')\n    with tf.Graph().as_default():\n        gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=gpu_memory_fraction)\n        sess = tf.Session(config=tf.ConfigProto(gpu_options=gpu_options, log_device_placement=False))\n        with sess.as_default():\n            pnet, rnet, onet = align.detect_face.create_mtcnn(sess, None)\n  \n    tmp_image_paths=copy.copy(image_paths)\n    img_list = []\n    for image in tmp_image_paths:\n        img = misc.imread(os.path.expanduser(image), mode=\'RGB\')\n        img_size = np.asarray(img.shape)[0:2]\n        bounding_boxes, _ = align.detect_face.detect_face(img, minsize, pnet, rnet, onet, threshold, factor)\n        if len(bounding_boxes) < 1:\n          image_paths.remove(image)\n          print(""can\'t detect face, remove "", image)\n          continue\n        det = np.squeeze(bounding_boxes[0,0:4])\n        bb = np.zeros(4, dtype=np.int32)\n        bb[0] = np.maximum(det[0]-margin/2, 0)\n        bb[1] = np.maximum(det[1]-margin/2, 0)\n        bb[2] = np.minimum(det[2]+margin/2, img_size[1])\n        bb[3] = np.minimum(det[3]+margin/2, img_size[0])\n        cropped = img[bb[1]:bb[3],bb[0]:bb[2],:]\n        aligned = misc.imresize(cropped, (image_size, image_size), interp=\'bilinear\')\n        prewhitened = facenet.prewhiten(aligned)\n        img_list.append(prewhitened)\n    images = np.stack(img_list)\n    return images\n\ndef parse_arguments(argv):\n    parser = argparse.ArgumentParser()\n    \n    parser.add_argument(\'model\', type=str, \n        help=\'Could be either a directory containing the meta_file and ckpt_file or a model protobuf (.pb) file\')\n    parser.add_argument(\'image_files\', type=str, nargs=\'+\', help=\'Images to compare\')\n    parser.add_argument(\'--image_size\', type=int,\n        help=\'Image size (height, width) in pixels.\', default=160)\n    parser.add_argument(\'--margin\', type=int,\n        help=\'Margin for the crop around the bounding box (height, width) in pixels.\', default=44)\n    parser.add_argument(\'--gpu_memory_fraction\', type=float,\n        help=\'Upper bound on the amount of GPU memory that will be used by the process.\', default=1.0)\n    return parser.parse_args(argv)\n\nif __name__ == \'__main__\':\n    main(parse_arguments(sys.argv[1:]))\n'"
src/decode_msceleb_dataset.py,0,"b'""""""Decode the MsCelebV1 dataset in TSV (tab separated values) format downloaded from\nhttps://www.microsoft.com/en-us/research/project/ms-celeb-1m-challenge-recognizing-one-million-celebrities-real-world/\n""""""\n# MIT License\n# \n# Copyright (c) 2016 David Sandberg\n# \n# Permission is hereby granted, free of charge, to any person obtaining a copy\n# of this software and associated documentation files (the ""Software""), to deal\n# in the Software without restriction, including without limitation the rights\n# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n# copies of the Software, and to permit persons to whom the Software is\n# furnished to do so, subject to the following conditions:\n# \n# The above copyright notice and this permission notice shall be included in all\n# copies or substantial portions of the Software.\n# \n# THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n# SOFTWARE.\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom scipy import misc\nimport numpy as np\nimport base64\nimport sys\nimport os\nimport cv2\nimport argparse\nimport facenet\n\n\n# File format: text files, each line is an image record containing 6 columns, delimited by TAB.\n# Column1: Freebase MID\n# Column2: Query/Name\n# Column3: ImageSearchRank\n# Column4: ImageURL\n# Column5: PageURL\n# Column6: ImageData_Base64Encoded\n\ndef main(args):\n    output_dir = os.path.expanduser(args.output_dir)\n  \n    if not os.path.exists(output_dir):\n        os.mkdir(output_dir)\n  \n    # Store some git revision info in a text file in the output directory\n    src_path,_ = os.path.split(os.path.realpath(__file__))\n    facenet.store_revision_info(src_path, output_dir, \' \'.join(sys.argv))\n    \n    i = 0\n    for f in args.tsv_files:\n        for line in f:\n            fields = line.split(\'\\t\')\n            class_dir = fields[0]\n            img_name = fields[1] + \'-\' + fields[4] + \'.\' + args.output_format\n            img_string = fields[5]\n            img_dec_string = base64.b64decode(img_string)\n            img_data = np.fromstring(img_dec_string, dtype=np.uint8)\n            img = cv2.imdecode(img_data, cv2.IMREAD_COLOR) #pylint: disable=maybe-no-member\n            if args.size:\n                img = misc.imresize(img, (args.size, args.size), interp=\'bilinear\')\n            full_class_dir = os.path.join(output_dir, class_dir)\n            if not os.path.exists(full_class_dir):\n                os.mkdir(full_class_dir)\n            full_path = os.path.join(full_class_dir, img_name.replace(\'/\',\'_\'))\n            cv2.imwrite(full_path, img) #pylint: disable=maybe-no-member\n            print(\'%8d: %s\' % (i, full_path))\n            i += 1\n  \nif __name__ == \'__main__\':\n    parser = argparse.ArgumentParser()\n\n    parser.add_argument(\'output_dir\', type=str, help=\'Output base directory for the image dataset\')\n    parser.add_argument(\'tsv_files\', type=argparse.FileType(\'r\'), nargs=\'+\', help=\'Input TSV file name(s)\')\n    parser.add_argument(\'--size\', type=int, help=\'Images are resized to the given size\')\n    parser.add_argument(\'--output_format\', type=str, help=\'Format of the output images\', default=\'png\', choices=[\'png\', \'jpg\'])\n\n    main(parser.parse_args())\n\n'"
src/download_and_extract.py,0,"b'import requests\nimport zipfile\nimport os\n\nmodel_dict = {\n    \'lfw-subset\':      \'1B5BQUZuJO-paxdN8UclxeHAR1WnR_Tzi\', \n    \'20170131-234652\': \'0B5MzpY9kBtDVSGM0RmVET2EwVEk\',\n    \'20170216-091149\': \'0B5MzpY9kBtDVTGZjcWkzT3pldDA\',\n    \'20170512-110547\': \'0B5MzpY9kBtDVZ2RpVDYwWmxoSUk\',\n    \'20180402-114759\': \'1EXPBSXwTaqrSC0OhUdXNmKSh9qJUQ55-\'\n    }\n\ndef download_and_extract_file(model_name, data_dir):\n    file_id = model_dict[model_name]\n    destination = os.path.join(data_dir, model_name + \'.zip\')\n    if not os.path.exists(destination):\n        print(\'Downloading file to %s\' % destination)\n        download_file_from_google_drive(file_id, destination)\n        with zipfile.ZipFile(destination, \'r\') as zip_ref:\n            print(\'Extracting file to %s\' % data_dir)\n            zip_ref.extractall(data_dir)\n\ndef download_file_from_google_drive(file_id, destination):\n    \n        URL = ""https://drive.google.com/uc?export=download""\n    \n        session = requests.Session()\n    \n        response = session.get(URL, params = { \'id\' : file_id }, stream = True)\n        token = get_confirm_token(response)\n    \n        if token:\n            params = { \'id\' : file_id, \'confirm\' : token }\n            response = session.get(URL, params = params, stream = True)\n    \n        save_response_content(response, destination)    \n\ndef get_confirm_token(response):\n    for key, value in response.cookies.items():\n        if key.startswith(\'download_warning\'):\n            return value\n\n    return None\n\ndef save_response_content(response, destination):\n    CHUNK_SIZE = 32768\n\n    with open(destination, ""wb"") as f:\n        for chunk in response.iter_content(CHUNK_SIZE):\n            if chunk: # filter out keep-alive new chunks\n                f.write(chunk)\n'"
src/facenet.py,55,"b'""""""Functions for building the face recognition network.\n""""""\n# MIT License\n# \n# Copyright (c) 2016 David Sandberg\n# \n# Permission is hereby granted, free of charge, to any person obtaining a copy\n# of this software and associated documentation files (the ""Software""), to deal\n# in the Software without restriction, including without limitation the rights\n# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n# copies of the Software, and to permit persons to whom the Software is\n# furnished to do so, subject to the following conditions:\n# \n# The above copyright notice and this permission notice shall be included in all\n# copies or substantial portions of the Software.\n# \n# THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n# SOFTWARE.\n\n# pylint: disable=missing-docstring\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport os\nfrom subprocess import Popen, PIPE\nimport tensorflow as tf\nimport numpy as np\nfrom scipy import misc\nfrom sklearn.model_selection import KFold\nfrom scipy import interpolate\nfrom tensorflow.python.training import training\nimport random\nimport re\nfrom tensorflow.python.platform import gfile\nimport math\nfrom six import iteritems\n\ndef triplet_loss(anchor, positive, negative, alpha):\n    """"""Calculate the triplet loss according to the FaceNet paper\n    \n    Args:\n      anchor: the embeddings for the anchor images.\n      positive: the embeddings for the positive images.\n      negative: the embeddings for the negative images.\n  \n    Returns:\n      the triplet loss according to the FaceNet paper as a float tensor.\n    """"""\n    with tf.variable_scope(\'triplet_loss\'):\n        pos_dist = tf.reduce_sum(tf.square(tf.subtract(anchor, positive)), 1)\n        neg_dist = tf.reduce_sum(tf.square(tf.subtract(anchor, negative)), 1)\n        \n        basic_loss = tf.add(tf.subtract(pos_dist,neg_dist), alpha)\n        loss = tf.reduce_mean(tf.maximum(basic_loss, 0.0), 0)\n      \n    return loss\n  \ndef center_loss(features, label, alfa, nrof_classes):\n    """"""Center loss based on the paper ""A Discriminative Feature Learning Approach for Deep Face Recognition""\n       (http://ydwen.github.io/papers/WenECCV16.pdf)\n    """"""\n    nrof_features = features.get_shape()[1]\n    centers = tf.get_variable(\'centers\', [nrof_classes, nrof_features], dtype=tf.float32,\n        initializer=tf.constant_initializer(0), trainable=False)\n    label = tf.reshape(label, [-1])\n    centers_batch = tf.gather(centers, label)\n    diff = (1 - alfa) * (centers_batch - features)\n    centers = tf.scatter_sub(centers, label, diff)\n    with tf.control_dependencies([centers]):\n        loss = tf.reduce_mean(tf.square(features - centers_batch))\n    return loss, centers\n\ndef get_image_paths_and_labels(dataset):\n    image_paths_flat = []\n    labels_flat = []\n    for i in range(len(dataset)):\n        image_paths_flat += dataset[i].image_paths\n        labels_flat += [i] * len(dataset[i].image_paths)\n    return image_paths_flat, labels_flat\n\ndef shuffle_examples(image_paths, labels):\n    shuffle_list = list(zip(image_paths, labels))\n    random.shuffle(shuffle_list)\n    image_paths_shuff, labels_shuff = zip(*shuffle_list)\n    return image_paths_shuff, labels_shuff\n\ndef random_rotate_image(image):\n    angle = np.random.uniform(low=-10.0, high=10.0)\n    return misc.imrotate(image, angle, \'bicubic\')\n  \n# 1: Random rotate 2: Random crop  4: Random flip  8:  Fixed image standardization  16: Flip\nRANDOM_ROTATE = 1\nRANDOM_CROP = 2\nRANDOM_FLIP = 4\nFIXED_STANDARDIZATION = 8\nFLIP = 16\ndef create_input_pipeline(input_queue, image_size, nrof_preprocess_threads, batch_size_placeholder):\n    images_and_labels_list = []\n    for _ in range(nrof_preprocess_threads):\n        filenames, label, control = input_queue.dequeue()\n        images = []\n        for filename in tf.unstack(filenames):\n            file_contents = tf.read_file(filename)\n            image = tf.image.decode_image(file_contents, 3)\n            image = tf.cond(get_control_flag(control[0], RANDOM_ROTATE),\n                            lambda:tf.py_func(random_rotate_image, [image], tf.uint8), \n                            lambda:tf.identity(image))\n            image = tf.cond(get_control_flag(control[0], RANDOM_CROP), \n                            lambda:tf.random_crop(image, image_size + (3,)), \n                            lambda:tf.image.resize_image_with_crop_or_pad(image, image_size[0], image_size[1]))\n            image = tf.cond(get_control_flag(control[0], RANDOM_FLIP),\n                            lambda:tf.image.random_flip_left_right(image),\n                            lambda:tf.identity(image))\n            image = tf.cond(get_control_flag(control[0], FIXED_STANDARDIZATION),\n                            lambda:(tf.cast(image, tf.float32) - 127.5)/128.0,\n                            lambda:tf.image.per_image_standardization(image))\n            image = tf.cond(get_control_flag(control[0], FLIP),\n                            lambda:tf.image.flip_left_right(image),\n                            lambda:tf.identity(image))\n            #pylint: disable=no-member\n            image.set_shape(image_size + (3,))\n            images.append(image)\n        images_and_labels_list.append([images, label])\n\n    image_batch, label_batch = tf.train.batch_join(\n        images_and_labels_list, batch_size=batch_size_placeholder, \n        shapes=[image_size + (3,), ()], enqueue_many=True,\n        capacity=4 * nrof_preprocess_threads * 100,\n        allow_smaller_final_batch=True)\n    \n    return image_batch, label_batch\n\ndef get_control_flag(control, field):\n    return tf.equal(tf.mod(tf.floor_div(control, field), 2), 1)\n  \ndef _add_loss_summaries(total_loss):\n    """"""Add summaries for losses.\n  \n    Generates moving average for all losses and associated summaries for\n    visualizing the performance of the network.\n  \n    Args:\n      total_loss: Total loss from loss().\n    Returns:\n      loss_averages_op: op for generating moving averages of losses.\n    """"""\n    # Compute the moving average of all individual losses and the total loss.\n    loss_averages = tf.train.ExponentialMovingAverage(0.9, name=\'avg\')\n    losses = tf.get_collection(\'losses\')\n    loss_averages_op = loss_averages.apply(losses + [total_loss])\n  \n    # Attach a scalar summmary to all individual losses and the total loss; do the\n    # same for the averaged version of the losses.\n    for l in losses + [total_loss]:\n        # Name each loss as \'(raw)\' and name the moving average version of the loss\n        # as the original loss name.\n        tf.summary.scalar(l.op.name +\' (raw)\', l)\n        tf.summary.scalar(l.op.name, loss_averages.average(l))\n  \n    return loss_averages_op\n\ndef train(total_loss, global_step, optimizer, learning_rate, moving_average_decay, update_gradient_vars, log_histograms=True):\n    # Generate moving averages of all losses and associated summaries.\n    loss_averages_op = _add_loss_summaries(total_loss)\n\n    # Compute gradients.\n    with tf.control_dependencies([loss_averages_op]):\n        if optimizer==\'ADAGRAD\':\n            opt = tf.train.AdagradOptimizer(learning_rate)\n        elif optimizer==\'ADADELTA\':\n            opt = tf.train.AdadeltaOptimizer(learning_rate, rho=0.9, epsilon=1e-6)\n        elif optimizer==\'ADAM\':\n            opt = tf.train.AdamOptimizer(learning_rate, beta1=0.9, beta2=0.999, epsilon=0.1)\n        elif optimizer==\'RMSPROP\':\n            opt = tf.train.RMSPropOptimizer(learning_rate, decay=0.9, momentum=0.9, epsilon=1.0)\n        elif optimizer==\'MOM\':\n            opt = tf.train.MomentumOptimizer(learning_rate, 0.9, use_nesterov=True)\n        else:\n            raise ValueError(\'Invalid optimization algorithm\')\n    \n        grads = opt.compute_gradients(total_loss, update_gradient_vars)\n        \n    # Apply gradients.\n    apply_gradient_op = opt.apply_gradients(grads, global_step=global_step)\n  \n    # Add histograms for trainable variables.\n    if log_histograms:\n        for var in tf.trainable_variables():\n            tf.summary.histogram(var.op.name, var)\n   \n    # Add histograms for gradients.\n    if log_histograms:\n        for grad, var in grads:\n            if grad is not None:\n                tf.summary.histogram(var.op.name + \'/gradients\', grad)\n  \n    # Track the moving averages of all trainable variables.\n    variable_averages = tf.train.ExponentialMovingAverage(\n        moving_average_decay, global_step)\n    variables_averages_op = variable_averages.apply(tf.trainable_variables())\n  \n    with tf.control_dependencies([apply_gradient_op, variables_averages_op]):\n        train_op = tf.no_op(name=\'train\')\n  \n    return train_op\n\ndef prewhiten(x):\n    mean = np.mean(x)\n    std = np.std(x)\n    std_adj = np.maximum(std, 1.0/np.sqrt(x.size))\n    y = np.multiply(np.subtract(x, mean), 1/std_adj)\n    return y  \n\ndef crop(image, random_crop, image_size):\n    if image.shape[1]>image_size:\n        sz1 = int(image.shape[1]//2)\n        sz2 = int(image_size//2)\n        if random_crop:\n            diff = sz1-sz2\n            (h, v) = (np.random.randint(-diff, diff+1), np.random.randint(-diff, diff+1))\n        else:\n            (h, v) = (0,0)\n        image = image[(sz1-sz2+v):(sz1+sz2+v),(sz1-sz2+h):(sz1+sz2+h),:]\n    return image\n  \ndef flip(image, random_flip):\n    if random_flip and np.random.choice([True, False]):\n        image = np.fliplr(image)\n    return image\n\ndef to_rgb(img):\n    w, h = img.shape\n    ret = np.empty((w, h, 3), dtype=np.uint8)\n    ret[:, :, 0] = ret[:, :, 1] = ret[:, :, 2] = img\n    return ret\n  \ndef load_data(image_paths, do_random_crop, do_random_flip, image_size, do_prewhiten=True):\n    nrof_samples = len(image_paths)\n    images = np.zeros((nrof_samples, image_size, image_size, 3))\n    for i in range(nrof_samples):\n        img = misc.imread(image_paths[i])\n        if img.ndim == 2:\n            img = to_rgb(img)\n        if do_prewhiten:\n            img = prewhiten(img)\n        img = crop(img, do_random_crop, image_size)\n        img = flip(img, do_random_flip)\n        images[i,:,:,:] = img\n    return images\n\ndef get_label_batch(label_data, batch_size, batch_index):\n    nrof_examples = np.size(label_data, 0)\n    j = batch_index*batch_size % nrof_examples\n    if j+batch_size<=nrof_examples:\n        batch = label_data[j:j+batch_size]\n    else:\n        x1 = label_data[j:nrof_examples]\n        x2 = label_data[0:nrof_examples-j]\n        batch = np.vstack([x1,x2])\n    batch_int = batch.astype(np.int64)\n    return batch_int\n\ndef get_batch(image_data, batch_size, batch_index):\n    nrof_examples = np.size(image_data, 0)\n    j = batch_index*batch_size % nrof_examples\n    if j+batch_size<=nrof_examples:\n        batch = image_data[j:j+batch_size,:,:,:]\n    else:\n        x1 = image_data[j:nrof_examples,:,:,:]\n        x2 = image_data[0:nrof_examples-j,:,:,:]\n        batch = np.vstack([x1,x2])\n    batch_float = batch.astype(np.float32)\n    return batch_float\n\ndef get_triplet_batch(triplets, batch_index, batch_size):\n    ax, px, nx = triplets\n    a = get_batch(ax, int(batch_size/3), batch_index)\n    p = get_batch(px, int(batch_size/3), batch_index)\n    n = get_batch(nx, int(batch_size/3), batch_index)\n    batch = np.vstack([a, p, n])\n    return batch\n\ndef get_learning_rate_from_file(filename, epoch):\n    with open(filename, \'r\') as f:\n        for line in f.readlines():\n            line = line.split(\'#\', 1)[0]\n            if line:\n                par = line.strip().split(\':\')\n                e = int(par[0])\n                if par[1]==\'-\':\n                    lr = -1\n                else:\n                    lr = float(par[1])\n                if e <= epoch:\n                    learning_rate = lr\n                else:\n                    return learning_rate\n\nclass ImageClass():\n    ""Stores the paths to images for a given class""\n    def __init__(self, name, image_paths):\n        self.name = name\n        self.image_paths = image_paths\n  \n    def __str__(self):\n        return self.name + \', \' + str(len(self.image_paths)) + \' images\'\n  \n    def __len__(self):\n        return len(self.image_paths)\n  \ndef get_dataset(path, has_class_directories=True):\n    dataset = []\n    path_exp = os.path.expanduser(path)\n    classes = [path for path in os.listdir(path_exp) \\\n                    if os.path.isdir(os.path.join(path_exp, path))]\n    classes.sort()\n    nrof_classes = len(classes)\n    for i in range(nrof_classes):\n        class_name = classes[i]\n        facedir = os.path.join(path_exp, class_name)\n        image_paths = get_image_paths(facedir)\n        dataset.append(ImageClass(class_name, image_paths))\n  \n    return dataset\n\ndef get_image_paths(facedir):\n    image_paths = []\n    if os.path.isdir(facedir):\n        images = os.listdir(facedir)\n        image_paths = [os.path.join(facedir,img) for img in images]\n    return image_paths\n  \ndef split_dataset(dataset, split_ratio, min_nrof_images_per_class, mode):\n    if mode==\'SPLIT_CLASSES\':\n        nrof_classes = len(dataset)\n        class_indices = np.arange(nrof_classes)\n        np.random.shuffle(class_indices)\n        split = int(round(nrof_classes*(1-split_ratio)))\n        train_set = [dataset[i] for i in class_indices[0:split]]\n        test_set = [dataset[i] for i in class_indices[split:-1]]\n    elif mode==\'SPLIT_IMAGES\':\n        train_set = []\n        test_set = []\n        for cls in dataset:\n            paths = cls.image_paths\n            np.random.shuffle(paths)\n            nrof_images_in_class = len(paths)\n            split = int(math.floor(nrof_images_in_class*(1-split_ratio)))\n            if split==nrof_images_in_class:\n                split = nrof_images_in_class-1\n            if split>=min_nrof_images_per_class and nrof_images_in_class-split>=1:\n                train_set.append(ImageClass(cls.name, paths[:split]))\n                test_set.append(ImageClass(cls.name, paths[split:]))\n    else:\n        raise ValueError(\'Invalid train/test split mode ""%s""\' % mode)\n    return train_set, test_set\n\ndef load_model(model, input_map=None):\n    # Check if the model is a model directory (containing a metagraph and a checkpoint file)\n    #  or if it is a protobuf file with a frozen graph\n    model_exp = os.path.expanduser(model)\n    if (os.path.isfile(model_exp)):\n        print(\'Model filename: %s\' % model_exp)\n        with gfile.FastGFile(model_exp,\'rb\') as f:\n            graph_def = tf.GraphDef()\n            graph_def.ParseFromString(f.read())\n            tf.import_graph_def(graph_def, input_map=input_map, name=\'\')\n    else:\n        print(\'Model directory: %s\' % model_exp)\n        meta_file, ckpt_file = get_model_filenames(model_exp)\n        \n        print(\'Metagraph file: %s\' % meta_file)\n        print(\'Checkpoint file: %s\' % ckpt_file)\n      \n        saver = tf.train.import_meta_graph(os.path.join(model_exp, meta_file), input_map=input_map)\n        saver.restore(tf.get_default_session(), os.path.join(model_exp, ckpt_file))\n    \ndef get_model_filenames(model_dir):\n    files = os.listdir(model_dir)\n    meta_files = [s for s in files if s.endswith(\'.meta\')]\n    if len(meta_files)==0:\n        raise ValueError(\'No meta file found in the model directory (%s)\' % model_dir)\n    elif len(meta_files)>1:\n        raise ValueError(\'There should not be more than one meta file in the model directory (%s)\' % model_dir)\n    meta_file = meta_files[0]\n    ckpt = tf.train.get_checkpoint_state(model_dir)\n    if ckpt and ckpt.model_checkpoint_path:\n        ckpt_file = os.path.basename(ckpt.model_checkpoint_path)\n        return meta_file, ckpt_file\n\n    meta_files = [s for s in files if \'.ckpt\' in s]\n    max_step = -1\n    for f in files:\n        step_str = re.match(r\'(^model-[\\w\\- ]+.ckpt-(\\d+))\', f)\n        if step_str is not None and len(step_str.groups())>=2:\n            step = int(step_str.groups()[1])\n            if step > max_step:\n                max_step = step\n                ckpt_file = step_str.groups()[0]\n    return meta_file, ckpt_file\n  \ndef distance(embeddings1, embeddings2, distance_metric=0):\n    if distance_metric==0:\n        # Euclidian distance\n        diff = np.subtract(embeddings1, embeddings2)\n        dist = np.sum(np.square(diff),1)\n    elif distance_metric==1:\n        # Distance based on cosine similarity\n        dot = np.sum(np.multiply(embeddings1, embeddings2), axis=1)\n        norm = np.linalg.norm(embeddings1, axis=1) * np.linalg.norm(embeddings2, axis=1)\n        similarity = dot / norm\n        dist = np.arccos(similarity) / math.pi\n    else:\n        raise \'Undefined distance metric %d\' % distance_metric \n        \n    return dist\n\ndef calculate_roc(thresholds, embeddings1, embeddings2, actual_issame, nrof_folds=10, distance_metric=0, subtract_mean=False):\n    assert(embeddings1.shape[0] == embeddings2.shape[0])\n    assert(embeddings1.shape[1] == embeddings2.shape[1])\n    nrof_pairs = min(len(actual_issame), embeddings1.shape[0])\n    nrof_thresholds = len(thresholds)\n    k_fold = KFold(n_splits=nrof_folds, shuffle=False)\n    \n    tprs = np.zeros((nrof_folds,nrof_thresholds))\n    fprs = np.zeros((nrof_folds,nrof_thresholds))\n    accuracy = np.zeros((nrof_folds))\n    \n    indices = np.arange(nrof_pairs)\n    \n    for fold_idx, (train_set, test_set) in enumerate(k_fold.split(indices)):\n        if subtract_mean:\n            mean = np.mean(np.concatenate([embeddings1[train_set], embeddings2[train_set]]), axis=0)\n        else:\n          mean = 0.0\n        dist = distance(embeddings1-mean, embeddings2-mean, distance_metric)\n        \n        # Find the best threshold for the fold\n        acc_train = np.zeros((nrof_thresholds))\n        for threshold_idx, threshold in enumerate(thresholds):\n            _, _, acc_train[threshold_idx] = calculate_accuracy(threshold, dist[train_set], actual_issame[train_set])\n        best_threshold_index = np.argmax(acc_train)\n        for threshold_idx, threshold in enumerate(thresholds):\n            tprs[fold_idx,threshold_idx], fprs[fold_idx,threshold_idx], _ = calculate_accuracy(threshold, dist[test_set], actual_issame[test_set])\n        _, _, accuracy[fold_idx] = calculate_accuracy(thresholds[best_threshold_index], dist[test_set], actual_issame[test_set])\n          \n        tpr = np.mean(tprs,0)\n        fpr = np.mean(fprs,0)\n    return tpr, fpr, accuracy\n\ndef calculate_accuracy(threshold, dist, actual_issame):\n    predict_issame = np.less(dist, threshold)\n    tp = np.sum(np.logical_and(predict_issame, actual_issame))\n    fp = np.sum(np.logical_and(predict_issame, np.logical_not(actual_issame)))\n    tn = np.sum(np.logical_and(np.logical_not(predict_issame), np.logical_not(actual_issame)))\n    fn = np.sum(np.logical_and(np.logical_not(predict_issame), actual_issame))\n  \n    tpr = 0 if (tp+fn==0) else float(tp) / float(tp+fn)\n    fpr = 0 if (fp+tn==0) else float(fp) / float(fp+tn)\n    acc = float(tp+tn)/dist.size\n    return tpr, fpr, acc\n\n\n  \ndef calculate_val(thresholds, embeddings1, embeddings2, actual_issame, far_target, nrof_folds=10, distance_metric=0, subtract_mean=False):\n    assert(embeddings1.shape[0] == embeddings2.shape[0])\n    assert(embeddings1.shape[1] == embeddings2.shape[1])\n    nrof_pairs = min(len(actual_issame), embeddings1.shape[0])\n    nrof_thresholds = len(thresholds)\n    k_fold = KFold(n_splits=nrof_folds, shuffle=False)\n    \n    val = np.zeros(nrof_folds)\n    far = np.zeros(nrof_folds)\n    \n    indices = np.arange(nrof_pairs)\n    \n    for fold_idx, (train_set, test_set) in enumerate(k_fold.split(indices)):\n        if subtract_mean:\n            mean = np.mean(np.concatenate([embeddings1[train_set], embeddings2[train_set]]), axis=0)\n        else:\n          mean = 0.0\n        dist = distance(embeddings1-mean, embeddings2-mean, distance_metric)\n      \n        # Find the threshold that gives FAR = far_target\n        far_train = np.zeros(nrof_thresholds)\n        for threshold_idx, threshold in enumerate(thresholds):\n            _, far_train[threshold_idx] = calculate_val_far(threshold, dist[train_set], actual_issame[train_set])\n        if np.max(far_train)>=far_target:\n            f = interpolate.interp1d(far_train, thresholds, kind=\'slinear\')\n            threshold = f(far_target)\n        else:\n            threshold = 0.0\n    \n        val[fold_idx], far[fold_idx] = calculate_val_far(threshold, dist[test_set], actual_issame[test_set])\n  \n    val_mean = np.mean(val)\n    far_mean = np.mean(far)\n    val_std = np.std(val)\n    return val_mean, val_std, far_mean\n\n\ndef calculate_val_far(threshold, dist, actual_issame):\n    predict_issame = np.less(dist, threshold)\n    true_accept = np.sum(np.logical_and(predict_issame, actual_issame))\n    false_accept = np.sum(np.logical_and(predict_issame, np.logical_not(actual_issame)))\n    n_same = np.sum(actual_issame)\n    n_diff = np.sum(np.logical_not(actual_issame))\n    val = float(true_accept) / float(n_same)\n    far = float(false_accept) / float(n_diff)\n    return val, far\n\ndef store_revision_info(src_path, output_dir, arg_string):\n    try:\n        # Get git hash\n        cmd = [\'git\', \'rev-parse\', \'HEAD\']\n        gitproc = Popen(cmd, stdout = PIPE, cwd=src_path)\n        (stdout, _) = gitproc.communicate()\n        git_hash = stdout.strip()\n    except OSError as e:\n        git_hash = \' \'.join(cmd) + \': \' +  e.strerror\n  \n    try:\n        # Get local changes\n        cmd = [\'git\', \'diff\', \'HEAD\']\n        gitproc = Popen(cmd, stdout = PIPE, cwd=src_path)\n        (stdout, _) = gitproc.communicate()\n        git_diff = stdout.strip()\n    except OSError as e:\n        git_diff = \' \'.join(cmd) + \': \' +  e.strerror\n    \n    # Store a text file in the log directory\n    rev_info_filename = os.path.join(output_dir, \'revision_info.txt\')\n    with open(rev_info_filename, ""w"") as text_file:\n        text_file.write(\'arguments: %s\\n--------------------\\n\' % arg_string)\n        text_file.write(\'tensorflow version: %s\\n--------------------\\n\' % tf.__version__)  # @UndefinedVariable\n        text_file.write(\'git hash: %s\\n--------------------\\n\' % git_hash)\n        text_file.write(\'%s\' % git_diff)\n\ndef list_variables(filename):\n    reader = training.NewCheckpointReader(filename)\n    variable_map = reader.get_variable_to_shape_map()\n    names = sorted(variable_map.keys())\n    return names\n\ndef put_images_on_grid(images, shape=(16,8)):\n    nrof_images = images.shape[0]\n    img_size = images.shape[1]\n    bw = 3\n    img = np.zeros((shape[1]*(img_size+bw)+bw, shape[0]*(img_size+bw)+bw, 3), np.float32)\n    for i in range(shape[1]):\n        x_start = i*(img_size+bw)+bw\n        for j in range(shape[0]):\n            img_index = i*shape[0]+j\n            if img_index>=nrof_images:\n                break\n            y_start = j*(img_size+bw)+bw\n            img[x_start:x_start+img_size, y_start:y_start+img_size, :] = images[img_index, :, :, :]\n        if img_index>=nrof_images:\n            break\n    return img\n\ndef write_arguments_to_file(args, filename):\n    with open(filename, \'w\') as f:\n        for key, value in iteritems(vars(args)):\n            f.write(\'%s: %s\\n\' % (key, str(value)))\n'"
src/freeze_graph.py,7,"b'""""""Imports a model metagraph and checkpoint file, converts the variables to constants\nand exports the model as a graphdef protobuf\n""""""\n# MIT License\n# \n# Copyright (c) 2016 David Sandberg\n# \n# Permission is hereby granted, free of charge, to any person obtaining a copy\n# of this software and associated documentation files (the ""Software""), to deal\n# in the Software without restriction, including without limitation the rights\n# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n# copies of the Software, and to permit persons to whom the Software is\n# furnished to do so, subject to the following conditions:\n# \n# The above copyright notice and this permission notice shall be included in all\n# copies or substantial portions of the Software.\n# \n# THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n# SOFTWARE.\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom tensorflow.python.framework import graph_util\nimport tensorflow as tf\nimport argparse\nimport os\nimport sys\nimport facenet\nfrom six.moves import xrange  # @UnresolvedImport\n\ndef main(args):\n    with tf.Graph().as_default():\n        with tf.Session() as sess:\n            # Load the model metagraph and checkpoint\n            print(\'Model directory: %s\' % args.model_dir)\n            meta_file, ckpt_file = facenet.get_model_filenames(os.path.expanduser(args.model_dir))\n            \n            print(\'Metagraph file: %s\' % meta_file)\n            print(\'Checkpoint file: %s\' % ckpt_file)\n\n            model_dir_exp = os.path.expanduser(args.model_dir)\n            saver = tf.train.import_meta_graph(os.path.join(model_dir_exp, meta_file), clear_devices=True)\n            tf.get_default_session().run(tf.global_variables_initializer())\n            tf.get_default_session().run(tf.local_variables_initializer())\n            saver.restore(tf.get_default_session(), os.path.join(model_dir_exp, ckpt_file))\n            \n            # Retrieve the protobuf graph definition and fix the batch norm nodes\n            input_graph_def = sess.graph.as_graph_def()\n            \n            # Freeze the graph def\n            output_graph_def = freeze_graph_def(sess, input_graph_def, \'embeddings,label_batch\')\n\n        # Serialize and dump the output graph to the filesystem\n        with tf.gfile.GFile(args.output_file, \'wb\') as f:\n            f.write(output_graph_def.SerializeToString())\n        print(""%d ops in the final graph: %s"" % (len(output_graph_def.node), args.output_file))\n        \ndef freeze_graph_def(sess, input_graph_def, output_node_names):\n    for node in input_graph_def.node:\n        if node.op == \'RefSwitch\':\n            node.op = \'Switch\'\n            for index in xrange(len(node.input)):\n                if \'moving_\' in node.input[index]:\n                    node.input[index] = node.input[index] + \'/read\'\n        elif node.op == \'AssignSub\':\n            node.op = \'Sub\'\n            if \'use_locking\' in node.attr: del node.attr[\'use_locking\']\n        elif node.op == \'AssignAdd\':\n            node.op = \'Add\'\n            if \'use_locking\' in node.attr: del node.attr[\'use_locking\']\n    \n    # Get the list of important nodes\n    whitelist_names = []\n    for node in input_graph_def.node:\n        if (node.name.startswith(\'InceptionResnet\') or node.name.startswith(\'embeddings\') or \n                node.name.startswith(\'image_batch\') or node.name.startswith(\'label_batch\') or\n                node.name.startswith(\'phase_train\') or node.name.startswith(\'Logits\')):\n            whitelist_names.append(node.name)\n\n    # Replace all the variables in the graph with constants of the same values\n    output_graph_def = graph_util.convert_variables_to_constants(\n        sess, input_graph_def, output_node_names.split("",""),\n        variable_names_whitelist=whitelist_names)\n    return output_graph_def\n  \ndef parse_arguments(argv):\n    parser = argparse.ArgumentParser()\n    \n    parser.add_argument(\'model_dir\', type=str, \n        help=\'Directory containing the metagraph (.meta) file and the checkpoint (ckpt) file containing model parameters\')\n    parser.add_argument(\'output_file\', type=str, \n        help=\'Filename for the exported graphdef protobuf (.pb)\')\n    return parser.parse_args(argv)\n\nif __name__ == \'__main__\':\n    main(parse_arguments(sys.argv[1:]))\n'"
src/lfw.py,0,"b'""""""Helper for evaluation on the Labeled Faces in the Wild dataset \n""""""\n\n# MIT License\n# \n# Copyright (c) 2016 David Sandberg\n# \n# Permission is hereby granted, free of charge, to any person obtaining a copy\n# of this software and associated documentation files (the ""Software""), to deal\n# in the Software without restriction, including without limitation the rights\n# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n# copies of the Software, and to permit persons to whom the Software is\n# furnished to do so, subject to the following conditions:\n# \n# The above copyright notice and this permission notice shall be included in all\n# copies or substantial portions of the Software.\n# \n# THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n# SOFTWARE.\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport os\nimport numpy as np\nimport facenet\n\ndef evaluate(embeddings, actual_issame, nrof_folds=10, distance_metric=0, subtract_mean=False):\n    # Calculate evaluation metrics\n    thresholds = np.arange(0, 4, 0.01)\n    embeddings1 = embeddings[0::2]\n    embeddings2 = embeddings[1::2]\n    tpr, fpr, accuracy = facenet.calculate_roc(thresholds, embeddings1, embeddings2,\n        np.asarray(actual_issame), nrof_folds=nrof_folds, distance_metric=distance_metric, subtract_mean=subtract_mean)\n    thresholds = np.arange(0, 4, 0.001)\n    val, val_std, far = facenet.calculate_val(thresholds, embeddings1, embeddings2,\n        np.asarray(actual_issame), 1e-3, nrof_folds=nrof_folds, distance_metric=distance_metric, subtract_mean=subtract_mean)\n    return tpr, fpr, accuracy, val, val_std, far\n\ndef get_paths(lfw_dir, pairs):\n    nrof_skipped_pairs = 0\n    path_list = []\n    issame_list = []\n    for pair in pairs:\n        if len(pair) == 3:\n            path0 = add_extension(os.path.join(lfw_dir, pair[0], pair[0] + \'_\' + \'%04d\' % int(pair[1])))\n            path1 = add_extension(os.path.join(lfw_dir, pair[0], pair[0] + \'_\' + \'%04d\' % int(pair[2])))\n            issame = True\n        elif len(pair) == 4:\n            path0 = add_extension(os.path.join(lfw_dir, pair[0], pair[0] + \'_\' + \'%04d\' % int(pair[1])))\n            path1 = add_extension(os.path.join(lfw_dir, pair[2], pair[2] + \'_\' + \'%04d\' % int(pair[3])))\n            issame = False\n        if os.path.exists(path0) and os.path.exists(path1):    # Only add the pair if both paths exist\n            path_list += (path0,path1)\n            issame_list.append(issame)\n        else:\n            nrof_skipped_pairs += 1\n    if nrof_skipped_pairs>0:\n        print(\'Skipped %d image pairs\' % nrof_skipped_pairs)\n    \n    return path_list, issame_list\n  \ndef add_extension(path):\n    if os.path.exists(path+\'.jpg\'):\n        return path+\'.jpg\'\n    elif os.path.exists(path+\'.png\'):\n        return path+\'.png\'\n    else:\n        raise RuntimeError(\'No file ""%s"" with extension png or jpg.\' % path)\n\ndef read_pairs(pairs_filename):\n    pairs = []\n    with open(pairs_filename, \'r\') as f:\n        for line in f.readlines()[1:]:\n            pair = line.strip().split()\n            pairs.append(pair)\n    return np.array(pairs)\n\n\n\n'"
src/train_softmax.py,41,"b'""""""Training a face recognizer with TensorFlow using softmax cross entropy loss\n""""""\n# MIT License\n# \n# Copyright (c) 2016 David Sandberg\n# \n# Permission is hereby granted, free of charge, to any person obtaining a copy\n# of this software and associated documentation files (the ""Software""), to deal\n# in the Software without restriction, including without limitation the rights\n# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n# copies of the Software, and to permit persons to whom the Software is\n# furnished to do so, subject to the following conditions:\n# \n# The above copyright notice and this permission notice shall be included in all\n# copies or substantial portions of the Software.\n# \n# THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n# SOFTWARE.\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom datetime import datetime\nimport os.path\nimport time\nimport sys\nimport random\nimport tensorflow as tf\nimport numpy as np\nimport importlib\nimport argparse\nimport facenet\nimport lfw\nimport h5py\nimport math\nimport tensorflow.contrib.slim as slim\nfrom tensorflow.python.ops import data_flow_ops\nfrom tensorflow.python.framework import ops\nfrom tensorflow.python.ops import array_ops\n\ndef main(args):\n  \n    network = importlib.import_module(args.model_def)\n    image_size = (args.image_size, args.image_size)\n\n    subdir = datetime.strftime(datetime.now(), \'%Y%m%d-%H%M%S\')\n    log_dir = os.path.join(os.path.expanduser(args.logs_base_dir), subdir)\n    if not os.path.isdir(log_dir):  # Create the log directory if it doesn\'t exist\n        os.makedirs(log_dir)\n    model_dir = os.path.join(os.path.expanduser(args.models_base_dir), subdir)\n    if not os.path.isdir(model_dir):  # Create the model directory if it doesn\'t exist\n        os.makedirs(model_dir)\n\n    stat_file_name = os.path.join(log_dir, \'stat.h5\')\n\n    # Write arguments to a text file\n    facenet.write_arguments_to_file(args, os.path.join(log_dir, \'arguments.txt\'))\n        \n    # Store some git revision info in a text file in the log directory\n    src_path,_ = os.path.split(os.path.realpath(__file__))\n    facenet.store_revision_info(src_path, log_dir, \' \'.join(sys.argv))\n\n    np.random.seed(seed=args.seed)\n    random.seed(args.seed)\n    dataset = facenet.get_dataset(args.data_dir)\n    if args.filter_filename:\n        dataset = filter_dataset(dataset, os.path.expanduser(args.filter_filename), \n            args.filter_percentile, args.filter_min_nrof_images_per_class)\n        \n    if args.validation_set_split_ratio>0.0:\n        train_set, val_set = facenet.split_dataset(dataset, args.validation_set_split_ratio, args.min_nrof_val_images_per_class, \'SPLIT_IMAGES\')\n    else:\n        train_set, val_set = dataset, []\n        \n    nrof_classes = len(train_set)\n    \n    print(\'Model directory: %s\' % model_dir)\n    print(\'Log directory: %s\' % log_dir)\n    pretrained_model = None\n    if args.pretrained_model:\n        pretrained_model = os.path.expanduser(args.pretrained_model)\n        print(\'Pre-trained model: %s\' % pretrained_model)\n    \n    if args.lfw_dir:\n        print(\'LFW directory: %s\' % args.lfw_dir)\n        # Read the file containing the pairs used for testing\n        pairs = lfw.read_pairs(os.path.expanduser(args.lfw_pairs))\n        # Get the paths for the corresponding images\n        lfw_paths, actual_issame = lfw.get_paths(os.path.expanduser(args.lfw_dir), pairs)\n    \n    with tf.Graph().as_default():\n        tf.set_random_seed(args.seed)\n        global_step = tf.Variable(0, trainable=False)\n        \n        # Get a list of image paths and their labels\n        image_list, label_list = facenet.get_image_paths_and_labels(train_set)\n        assert len(image_list)>0, \'The training set should not be empty\'\n        \n        val_image_list, val_label_list = facenet.get_image_paths_and_labels(val_set)\n\n        # Create a queue that produces indices into the image_list and label_list \n        labels = ops.convert_to_tensor(label_list, dtype=tf.int32)\n        range_size = array_ops.shape(labels)[0]\n        index_queue = tf.train.range_input_producer(range_size, num_epochs=None,\n                             shuffle=True, seed=None, capacity=32)\n        \n        index_dequeue_op = index_queue.dequeue_many(args.batch_size*args.epoch_size, \'index_dequeue\')\n        \n        learning_rate_placeholder = tf.placeholder(tf.float32, name=\'learning_rate\')\n        batch_size_placeholder = tf.placeholder(tf.int32, name=\'batch_size\')\n        phase_train_placeholder = tf.placeholder(tf.bool, name=\'phase_train\')\n        image_paths_placeholder = tf.placeholder(tf.string, shape=(None,1), name=\'image_paths\')\n        labels_placeholder = tf.placeholder(tf.int32, shape=(None,1), name=\'labels\')\n        control_placeholder = tf.placeholder(tf.int32, shape=(None,1), name=\'control\')\n        \n        nrof_preprocess_threads = 4\n        input_queue = data_flow_ops.FIFOQueue(capacity=2000000,\n                                    dtypes=[tf.string, tf.int32, tf.int32],\n                                    shapes=[(1,), (1,), (1,)],\n                                    shared_name=None, name=None)\n        enqueue_op = input_queue.enqueue_many([image_paths_placeholder, labels_placeholder, control_placeholder], name=\'enqueue_op\')\n        image_batch, label_batch = facenet.create_input_pipeline(input_queue, image_size, nrof_preprocess_threads, batch_size_placeholder)\n\n        image_batch = tf.identity(image_batch, \'image_batch\')\n        image_batch = tf.identity(image_batch, \'input\')\n        label_batch = tf.identity(label_batch, \'label_batch\')\n        \n        print(\'Number of classes in training set: %d\' % nrof_classes)\n        print(\'Number of examples in training set: %d\' % len(image_list))\n\n        print(\'Number of classes in validation set: %d\' % len(val_set))\n        print(\'Number of examples in validation set: %d\' % len(val_image_list))\n        \n        print(\'Building training graph\')\n        \n        # Build the inference graph\n        prelogits, _ = network.inference(image_batch, args.keep_probability, \n            phase_train=phase_train_placeholder, bottleneck_layer_size=args.embedding_size, \n            weight_decay=args.weight_decay)\n        logits = slim.fully_connected(prelogits, len(train_set), activation_fn=None, \n                weights_initializer=slim.initializers.xavier_initializer(), \n                weights_regularizer=slim.l2_regularizer(args.weight_decay),\n                scope=\'Logits\', reuse=False)\n\n        embeddings = tf.nn.l2_normalize(prelogits, 1, 1e-10, name=\'embeddings\')\n\n        # Norm for the prelogits\n        eps = 1e-4\n        prelogits_norm = tf.reduce_mean(tf.norm(tf.abs(prelogits)+eps, ord=args.prelogits_norm_p, axis=1))\n        tf.add_to_collection(tf.GraphKeys.REGULARIZATION_LOSSES, prelogits_norm * args.prelogits_norm_loss_factor)\n\n        # Add center loss\n        prelogits_center_loss, _ = facenet.center_loss(prelogits, label_batch, args.center_loss_alfa, nrof_classes)\n        tf.add_to_collection(tf.GraphKeys.REGULARIZATION_LOSSES, prelogits_center_loss * args.center_loss_factor)\n\n        learning_rate = tf.train.exponential_decay(learning_rate_placeholder, global_step,\n            args.learning_rate_decay_epochs*args.epoch_size, args.learning_rate_decay_factor, staircase=True)\n        tf.summary.scalar(\'learning_rate\', learning_rate)\n\n        # Calculate the average cross entropy loss across the batch\n        cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(\n            labels=label_batch, logits=logits, name=\'cross_entropy_per_example\')\n        cross_entropy_mean = tf.reduce_mean(cross_entropy, name=\'cross_entropy\')\n        tf.add_to_collection(\'losses\', cross_entropy_mean)\n        \n        correct_prediction = tf.cast(tf.equal(tf.argmax(logits, 1), tf.cast(label_batch, tf.int64)), tf.float32)\n        accuracy = tf.reduce_mean(correct_prediction)\n        \n        # Calculate the total losses\n        regularization_losses = tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES)\n        total_loss = tf.add_n([cross_entropy_mean] + regularization_losses, name=\'total_loss\')\n\n        # Build a Graph that trains the model with one batch of examples and updates the model parameters\n        train_op = facenet.train(total_loss, global_step, args.optimizer, \n            learning_rate, args.moving_average_decay, tf.global_variables(), args.log_histograms)\n        \n        # Create a saver\n        saver = tf.train.Saver(tf.trainable_variables(), max_to_keep=3)\n\n        # Build the summary operation based on the TF collection of Summaries.\n        summary_op = tf.summary.merge_all()\n\n        # Start running operations on the Graph.\n        gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=args.gpu_memory_fraction)\n        sess = tf.Session(config=tf.ConfigProto(gpu_options=gpu_options, log_device_placement=False))\n        sess.run(tf.global_variables_initializer())\n        sess.run(tf.local_variables_initializer())\n        summary_writer = tf.summary.FileWriter(log_dir, sess.graph)\n        coord = tf.train.Coordinator()\n        tf.train.start_queue_runners(coord=coord, sess=sess)\n\n        with sess.as_default():\n\n            if pretrained_model:\n                print(\'Restoring pretrained model: %s\' % pretrained_model)\n                saver.restore(sess, pretrained_model)\n\n            # Training and validation loop\n            print(\'Running training\')\n            nrof_steps = args.max_nrof_epochs*args.epoch_size\n            nrof_val_samples = int(math.ceil(args.max_nrof_epochs / args.validate_every_n_epochs))   # Validate every validate_every_n_epochs as well as in the last epoch\n            stat = {\n                \'loss\': np.zeros((nrof_steps,), np.float32),\n                \'center_loss\': np.zeros((nrof_steps,), np.float32),\n                \'reg_loss\': np.zeros((nrof_steps,), np.float32),\n                \'xent_loss\': np.zeros((nrof_steps,), np.float32),\n                \'prelogits_norm\': np.zeros((nrof_steps,), np.float32),\n                \'accuracy\': np.zeros((nrof_steps,), np.float32),\n                \'val_loss\': np.zeros((nrof_val_samples,), np.float32),\n                \'val_xent_loss\': np.zeros((nrof_val_samples,), np.float32),\n                \'val_accuracy\': np.zeros((nrof_val_samples,), np.float32),\n                \'lfw_accuracy\': np.zeros((args.max_nrof_epochs,), np.float32),\n                \'lfw_valrate\': np.zeros((args.max_nrof_epochs,), np.float32),\n                \'learning_rate\': np.zeros((args.max_nrof_epochs,), np.float32),\n                \'time_train\': np.zeros((args.max_nrof_epochs,), np.float32),\n                \'time_validate\': np.zeros((args.max_nrof_epochs,), np.float32),\n                \'time_evaluate\': np.zeros((args.max_nrof_epochs,), np.float32),\n                \'prelogits_hist\': np.zeros((args.max_nrof_epochs, 1000), np.float32),\n              }\n            for epoch in range(1,args.max_nrof_epochs+1):\n                step = sess.run(global_step, feed_dict=None)\n                # Train for one epoch\n                t = time.time()\n                cont = train(args, sess, epoch, image_list, label_list, index_dequeue_op, enqueue_op, image_paths_placeholder, labels_placeholder,\n                    learning_rate_placeholder, phase_train_placeholder, batch_size_placeholder, control_placeholder, global_step, \n                    total_loss, train_op, summary_op, summary_writer, regularization_losses, args.learning_rate_schedule_file,\n                    stat, cross_entropy_mean, accuracy, learning_rate,\n                    prelogits, prelogits_center_loss, args.random_rotate, args.random_crop, args.random_flip, prelogits_norm, args.prelogits_hist_max, args.use_fixed_image_standardization)\n                stat[\'time_train\'][epoch-1] = time.time() - t\n                \n                if not cont:\n                    break\n                  \n                t = time.time()\n                if len(val_image_list)>0 and ((epoch-1) % args.validate_every_n_epochs == args.validate_every_n_epochs-1 or epoch==args.max_nrof_epochs):\n                    validate(args, sess, epoch, val_image_list, val_label_list, enqueue_op, image_paths_placeholder, labels_placeholder, control_placeholder,\n                        phase_train_placeholder, batch_size_placeholder, \n                        stat, total_loss, regularization_losses, cross_entropy_mean, accuracy, args.validate_every_n_epochs, args.use_fixed_image_standardization)\n                stat[\'time_validate\'][epoch-1] = time.time() - t\n\n                # Save variables and the metagraph if it doesn\'t exist already\n                save_variables_and_metagraph(sess, saver, summary_writer, model_dir, subdir, epoch)\n\n                # Evaluate on LFW\n                t = time.time()\n                if args.lfw_dir:\n                    evaluate(sess, enqueue_op, image_paths_placeholder, labels_placeholder, phase_train_placeholder, batch_size_placeholder, control_placeholder, \n                        embeddings, label_batch, lfw_paths, actual_issame, args.lfw_batch_size, args.lfw_nrof_folds, log_dir, step, summary_writer, stat, epoch, \n                        args.lfw_distance_metric, args.lfw_subtract_mean, args.lfw_use_flipped_images, args.use_fixed_image_standardization)\n                stat[\'time_evaluate\'][epoch-1] = time.time() - t\n\n                print(\'Saving statistics\')\n                with h5py.File(stat_file_name, \'w\') as f:\n                    for key, value in stat.iteritems():\n                        f.create_dataset(key, data=value)\n    \n    return model_dir\n  \ndef find_threshold(var, percentile):\n    hist, bin_edges = np.histogram(var, 100)\n    cdf = np.float32(np.cumsum(hist)) / np.sum(hist)\n    bin_centers = (bin_edges[:-1]+bin_edges[1:])/2\n    #plt.plot(bin_centers, cdf)\n    threshold = np.interp(percentile*0.01, cdf, bin_centers)\n    return threshold\n  \ndef filter_dataset(dataset, data_filename, percentile, min_nrof_images_per_class):\n    with h5py.File(data_filename,\'r\') as f:\n        distance_to_center = np.array(f.get(\'distance_to_center\'))\n        label_list = np.array(f.get(\'label_list\'))\n        image_list = np.array(f.get(\'image_list\'))\n        distance_to_center_threshold = find_threshold(distance_to_center, percentile)\n        indices = np.where(distance_to_center>=distance_to_center_threshold)[0]\n        filtered_dataset = dataset\n        removelist = []\n        for i in indices:\n            label = label_list[i]\n            image = image_list[i]\n            if image in filtered_dataset[label].image_paths:\n                filtered_dataset[label].image_paths.remove(image)\n            if len(filtered_dataset[label].image_paths)<min_nrof_images_per_class:\n                removelist.append(label)\n\n        ix = sorted(list(set(removelist)), reverse=True)\n        for i in ix:\n            del(filtered_dataset[i])\n\n    return filtered_dataset\n  \ndef train(args, sess, epoch, image_list, label_list, index_dequeue_op, enqueue_op, image_paths_placeholder, labels_placeholder, \n      learning_rate_placeholder, phase_train_placeholder, batch_size_placeholder, control_placeholder, step, \n      loss, train_op, summary_op, summary_writer, reg_losses, learning_rate_schedule_file, \n      stat, cross_entropy_mean, accuracy, \n      learning_rate, prelogits, prelogits_center_loss, random_rotate, random_crop, random_flip, prelogits_norm, prelogits_hist_max, use_fixed_image_standardization):\n    batch_number = 0\n    \n    if args.learning_rate>0.0:\n        lr = args.learning_rate\n    else:\n        lr = facenet.get_learning_rate_from_file(learning_rate_schedule_file, epoch)\n        \n    if lr<=0:\n        return False \n\n    index_epoch = sess.run(index_dequeue_op)\n    label_epoch = np.array(label_list)[index_epoch]\n    image_epoch = np.array(image_list)[index_epoch]\n    \n    # Enqueue one epoch of image paths and labels\n    labels_array = np.expand_dims(np.array(label_epoch),1)\n    image_paths_array = np.expand_dims(np.array(image_epoch),1)\n    control_value = facenet.RANDOM_ROTATE * random_rotate + facenet.RANDOM_CROP * random_crop + facenet.RANDOM_FLIP * random_flip + facenet.FIXED_STANDARDIZATION * use_fixed_image_standardization\n    control_array = np.ones_like(labels_array) * control_value\n    sess.run(enqueue_op, {image_paths_placeholder: image_paths_array, labels_placeholder: labels_array, control_placeholder: control_array})\n\n    # Training loop\n    train_time = 0\n    while batch_number < args.epoch_size:\n        start_time = time.time()\n        feed_dict = {learning_rate_placeholder: lr, phase_train_placeholder:True, batch_size_placeholder:args.batch_size}\n        tensor_list = [loss, train_op, step, reg_losses, prelogits, cross_entropy_mean, learning_rate, prelogits_norm, accuracy, prelogits_center_loss]\n        if batch_number % 100 == 0:\n            loss_, _, step_, reg_losses_, prelogits_, cross_entropy_mean_, lr_, prelogits_norm_, accuracy_, center_loss_, summary_str = sess.run(tensor_list + [summary_op], feed_dict=feed_dict)\n            summary_writer.add_summary(summary_str, global_step=step_)\n        else:\n            loss_, _, step_, reg_losses_, prelogits_, cross_entropy_mean_, lr_, prelogits_norm_, accuracy_, center_loss_ = sess.run(tensor_list, feed_dict=feed_dict)\n         \n        duration = time.time() - start_time\n        stat[\'loss\'][step_-1] = loss_\n        stat[\'center_loss\'][step_-1] = center_loss_\n        stat[\'reg_loss\'][step_-1] = np.sum(reg_losses_)\n        stat[\'xent_loss\'][step_-1] = cross_entropy_mean_\n        stat[\'prelogits_norm\'][step_-1] = prelogits_norm_\n        stat[\'learning_rate\'][epoch-1] = lr_\n        stat[\'accuracy\'][step_-1] = accuracy_\n        stat[\'prelogits_hist\'][epoch-1,:] += np.histogram(np.minimum(np.abs(prelogits_), prelogits_hist_max), bins=1000, range=(0.0, prelogits_hist_max))[0]\n        \n        duration = time.time() - start_time\n        print(\'Epoch: [%d][%d/%d]\\tTime %.3f\\tLoss %2.3f\\tXent %2.3f\\tRegLoss %2.3f\\tAccuracy %2.3f\\tLr %2.5f\\tCl %2.3f\' %\n              (epoch, batch_number+1, args.epoch_size, duration, loss_, cross_entropy_mean_, np.sum(reg_losses_), accuracy_, lr_, center_loss_))\n        batch_number += 1\n        train_time += duration\n    # Add validation loss and accuracy to summary\n    summary = tf.Summary()\n    #pylint: disable=maybe-no-member\n    summary.value.add(tag=\'time/total\', simple_value=train_time)\n    summary_writer.add_summary(summary, global_step=step_)\n    return True\n\ndef validate(args, sess, epoch, image_list, label_list, enqueue_op, image_paths_placeholder, labels_placeholder, control_placeholder,\n             phase_train_placeholder, batch_size_placeholder, \n             stat, loss, regularization_losses, cross_entropy_mean, accuracy, validate_every_n_epochs, use_fixed_image_standardization):\n  \n    print(\'Running forward pass on validation set\')\n\n    nrof_batches = len(label_list) // args.lfw_batch_size\n    nrof_images = nrof_batches * args.lfw_batch_size\n    \n    # Enqueue one epoch of image paths and labels\n    labels_array = np.expand_dims(np.array(label_list[:nrof_images]),1)\n    image_paths_array = np.expand_dims(np.array(image_list[:nrof_images]),1)\n    control_array = np.ones_like(labels_array, np.int32)*facenet.FIXED_STANDARDIZATION * use_fixed_image_standardization\n    sess.run(enqueue_op, {image_paths_placeholder: image_paths_array, labels_placeholder: labels_array, control_placeholder: control_array})\n\n    loss_array = np.zeros((nrof_batches,), np.float32)\n    xent_array = np.zeros((nrof_batches,), np.float32)\n    accuracy_array = np.zeros((nrof_batches,), np.float32)\n\n    # Training loop\n    start_time = time.time()\n    for i in range(nrof_batches):\n        feed_dict = {phase_train_placeholder:False, batch_size_placeholder:args.lfw_batch_size}\n        loss_, cross_entropy_mean_, accuracy_ = sess.run([loss, cross_entropy_mean, accuracy], feed_dict=feed_dict)\n        loss_array[i], xent_array[i], accuracy_array[i] = (loss_, cross_entropy_mean_, accuracy_)\n        if i % 10 == 9:\n            print(\'.\', end=\'\')\n            sys.stdout.flush()\n    print(\'\')\n\n    duration = time.time() - start_time\n\n    val_index = (epoch-1)//validate_every_n_epochs\n    stat[\'val_loss\'][val_index] = np.mean(loss_array)\n    stat[\'val_xent_loss\'][val_index] = np.mean(xent_array)\n    stat[\'val_accuracy\'][val_index] = np.mean(accuracy_array)\n\n    print(\'Validation Epoch: %d\\tTime %.3f\\tLoss %2.3f\\tXent %2.3f\\tAccuracy %2.3f\' %\n          (epoch, duration, np.mean(loss_array), np.mean(xent_array), np.mean(accuracy_array)))\n\n\ndef evaluate(sess, enqueue_op, image_paths_placeholder, labels_placeholder, phase_train_placeholder, batch_size_placeholder, control_placeholder, \n        embeddings, labels, image_paths, actual_issame, batch_size, nrof_folds, log_dir, step, summary_writer, stat, epoch, distance_metric, subtract_mean, use_flipped_images, use_fixed_image_standardization):\n    start_time = time.time()\n    # Run forward pass to calculate embeddings\n    print(\'Runnning forward pass on LFW images\')\n    \n    # Enqueue one epoch of image paths and labels\n    nrof_embeddings = len(actual_issame)*2  # nrof_pairs * nrof_images_per_pair\n    nrof_flips = 2 if use_flipped_images else 1\n    nrof_images = nrof_embeddings * nrof_flips\n    labels_array = np.expand_dims(np.arange(0,nrof_images),1)\n    image_paths_array = np.expand_dims(np.repeat(np.array(image_paths),nrof_flips),1)\n    control_array = np.zeros_like(labels_array, np.int32)\n    if use_fixed_image_standardization:\n        control_array += np.ones_like(labels_array)*facenet.FIXED_STANDARDIZATION\n    if use_flipped_images:\n        # Flip every second image\n        control_array += (labels_array % 2)*facenet.FLIP\n    sess.run(enqueue_op, {image_paths_placeholder: image_paths_array, labels_placeholder: labels_array, control_placeholder: control_array})\n    \n    embedding_size = int(embeddings.get_shape()[1])\n    assert nrof_images % batch_size == 0, \'The number of LFW images must be an integer multiple of the LFW batch size\'\n    nrof_batches = nrof_images // batch_size\n    emb_array = np.zeros((nrof_images, embedding_size))\n    lab_array = np.zeros((nrof_images,))\n    for i in range(nrof_batches):\n        feed_dict = {phase_train_placeholder:False, batch_size_placeholder:batch_size}\n        emb, lab = sess.run([embeddings, labels], feed_dict=feed_dict)\n        lab_array[lab] = lab\n        emb_array[lab, :] = emb\n        if i % 10 == 9:\n            print(\'.\', end=\'\')\n            sys.stdout.flush()\n    print(\'\')\n    embeddings = np.zeros((nrof_embeddings, embedding_size*nrof_flips))\n    if use_flipped_images:\n        # Concatenate embeddings for flipped and non flipped version of the images\n        embeddings[:,:embedding_size] = emb_array[0::2,:]\n        embeddings[:,embedding_size:] = emb_array[1::2,:]\n    else:\n        embeddings = emb_array\n\n    assert np.array_equal(lab_array, np.arange(nrof_images))==True, \'Wrong labels used for evaluation, possibly caused by training examples left in the input pipeline\'\n    _, _, accuracy, val, val_std, far = lfw.evaluate(embeddings, actual_issame, nrof_folds=nrof_folds, distance_metric=distance_metric, subtract_mean=subtract_mean)\n    \n    print(\'Accuracy: %2.5f+-%2.5f\' % (np.mean(accuracy), np.std(accuracy)))\n    print(\'Validation rate: %2.5f+-%2.5f @ FAR=%2.5f\' % (val, val_std, far))\n    lfw_time = time.time() - start_time\n    # Add validation loss and accuracy to summary\n    summary = tf.Summary()\n    #pylint: disable=maybe-no-member\n    summary.value.add(tag=\'lfw/accuracy\', simple_value=np.mean(accuracy))\n    summary.value.add(tag=\'lfw/val_rate\', simple_value=val)\n    summary.value.add(tag=\'time/lfw\', simple_value=lfw_time)\n    summary_writer.add_summary(summary, step)\n    with open(os.path.join(log_dir,\'lfw_result.txt\'),\'at\') as f:\n        f.write(\'%d\\t%.5f\\t%.5f\\n\' % (step, np.mean(accuracy), val))\n    stat[\'lfw_accuracy\'][epoch-1] = np.mean(accuracy)\n    stat[\'lfw_valrate\'][epoch-1] = val\n\ndef save_variables_and_metagraph(sess, saver, summary_writer, model_dir, model_name, step):\n    # Save the model checkpoint\n    print(\'Saving variables\')\n    start_time = time.time()\n    checkpoint_path = os.path.join(model_dir, \'model-%s.ckpt\' % model_name)\n    saver.save(sess, checkpoint_path, global_step=step, write_meta_graph=False)\n    save_time_variables = time.time() - start_time\n    print(\'Variables saved in %.2f seconds\' % save_time_variables)\n    metagraph_filename = os.path.join(model_dir, \'model-%s.meta\' % model_name)\n    save_time_metagraph = 0  \n    if not os.path.exists(metagraph_filename):\n        print(\'Saving metagraph\')\n        start_time = time.time()\n        saver.export_meta_graph(metagraph_filename)\n        save_time_metagraph = time.time() - start_time\n        print(\'Metagraph saved in %.2f seconds\' % save_time_metagraph)\n    summary = tf.Summary()\n    #pylint: disable=maybe-no-member\n    summary.value.add(tag=\'time/save_variables\', simple_value=save_time_variables)\n    summary.value.add(tag=\'time/save_metagraph\', simple_value=save_time_metagraph)\n    summary_writer.add_summary(summary, step)\n  \n\ndef parse_arguments(argv):\n    parser = argparse.ArgumentParser()\n    \n    parser.add_argument(\'--logs_base_dir\', type=str, \n        help=\'Directory where to write event logs.\', default=\'~/logs/facenet\')\n    parser.add_argument(\'--models_base_dir\', type=str,\n        help=\'Directory where to write trained models and checkpoints.\', default=\'~/models/facenet\')\n    parser.add_argument(\'--gpu_memory_fraction\', type=float,\n        help=\'Upper bound on the amount of GPU memory that will be used by the process.\', default=1.0)\n    parser.add_argument(\'--pretrained_model\', type=str,\n        help=\'Load a pretrained model before training starts.\')\n    parser.add_argument(\'--data_dir\', type=str,\n        help=\'Path to the data directory containing aligned face patches.\',\n        default=\'~/datasets/casia/casia_maxpy_mtcnnalign_182_160\')\n    parser.add_argument(\'--model_def\', type=str,\n        help=\'Model definition. Points to a module containing the definition of the inference graph.\', default=\'models.inception_resnet_v1\')\n    parser.add_argument(\'--max_nrof_epochs\', type=int,\n        help=\'Number of epochs to run.\', default=500)\n    parser.add_argument(\'--batch_size\', type=int,\n        help=\'Number of images to process in a batch.\', default=90)\n    parser.add_argument(\'--image_size\', type=int,\n        help=\'Image size (height, width) in pixels.\', default=160)\n    parser.add_argument(\'--epoch_size\', type=int,\n        help=\'Number of batches per epoch.\', default=1000)\n    parser.add_argument(\'--embedding_size\', type=int,\n        help=\'Dimensionality of the embedding.\', default=128)\n    parser.add_argument(\'--random_crop\', \n        help=\'Performs random cropping of training images. If false, the center image_size pixels from the training images are used. \' +\n         \'If the size of the images in the data directory is equal to image_size no cropping is performed\', action=\'store_true\')\n    parser.add_argument(\'--random_flip\', \n        help=\'Performs random horizontal flipping of training images.\', action=\'store_true\')\n    parser.add_argument(\'--random_rotate\', \n        help=\'Performs random rotations of training images.\', action=\'store_true\')\n    parser.add_argument(\'--use_fixed_image_standardization\', \n        help=\'Performs fixed standardization of images.\', action=\'store_true\')\n    parser.add_argument(\'--keep_probability\', type=float,\n        help=\'Keep probability of dropout for the fully connected layer(s).\', default=1.0)\n    parser.add_argument(\'--weight_decay\', type=float,\n        help=\'L2 weight regularization.\', default=0.0)\n    parser.add_argument(\'--center_loss_factor\', type=float,\n        help=\'Center loss factor.\', default=0.0)\n    parser.add_argument(\'--center_loss_alfa\', type=float,\n        help=\'Center update rate for center loss.\', default=0.95)\n    parser.add_argument(\'--prelogits_norm_loss_factor\', type=float,\n        help=\'Loss based on the norm of the activations in the prelogits layer.\', default=0.0)\n    parser.add_argument(\'--prelogits_norm_p\', type=float,\n        help=\'Norm to use for prelogits norm loss.\', default=1.0)\n    parser.add_argument(\'--prelogits_hist_max\', type=float,\n        help=\'The max value for the prelogits histogram.\', default=10.0)\n    parser.add_argument(\'--optimizer\', type=str, choices=[\'ADAGRAD\', \'ADADELTA\', \'ADAM\', \'RMSPROP\', \'MOM\'],\n        help=\'The optimization algorithm to use\', default=\'ADAGRAD\')\n    parser.add_argument(\'--learning_rate\', type=float,\n        help=\'Initial learning rate. If set to a negative value a learning rate \' +\n        \'schedule can be specified in the file ""learning_rate_schedule.txt""\', default=0.1)\n    parser.add_argument(\'--learning_rate_decay_epochs\', type=int,\n        help=\'Number of epochs between learning rate decay.\', default=100)\n    parser.add_argument(\'--learning_rate_decay_factor\', type=float,\n        help=\'Learning rate decay factor.\', default=1.0)\n    parser.add_argument(\'--moving_average_decay\', type=float,\n        help=\'Exponential decay for tracking of training parameters.\', default=0.9999)\n    parser.add_argument(\'--seed\', type=int,\n        help=\'Random seed.\', default=666)\n    parser.add_argument(\'--nrof_preprocess_threads\', type=int,\n        help=\'Number of preprocessing (data loading and augmentation) threads.\', default=4)\n    parser.add_argument(\'--log_histograms\', \n        help=\'Enables logging of weight/bias histograms in tensorboard.\', action=\'store_true\')\n    parser.add_argument(\'--learning_rate_schedule_file\', type=str,\n        help=\'File containing the learning rate schedule that is used when learning_rate is set to to -1.\', default=\'data/learning_rate_schedule.txt\')\n    parser.add_argument(\'--filter_filename\', type=str,\n        help=\'File containing image data used for dataset filtering\', default=\'\')\n    parser.add_argument(\'--filter_percentile\', type=float,\n        help=\'Keep only the percentile images closed to its class center\', default=100.0)\n    parser.add_argument(\'--filter_min_nrof_images_per_class\', type=int,\n        help=\'Keep only the classes with this number of examples or more\', default=0)\n    parser.add_argument(\'--validate_every_n_epochs\', type=int,\n        help=\'Number of epoch between validation\', default=5)\n    parser.add_argument(\'--validation_set_split_ratio\', type=float,\n        help=\'The ratio of the total dataset to use for validation\', default=0.0)\n    parser.add_argument(\'--min_nrof_val_images_per_class\', type=float,\n        help=\'Classes with fewer images will be removed from the validation set\', default=0)\n \n    # Parameters for validation on LFW\n    parser.add_argument(\'--lfw_pairs\', type=str,\n        help=\'The file containing the pairs to use for validation.\', default=\'data/pairs.txt\')\n    parser.add_argument(\'--lfw_dir\', type=str,\n        help=\'Path to the data directory containing aligned face patches.\', default=\'\')\n    parser.add_argument(\'--lfw_batch_size\', type=int,\n        help=\'Number of images to process in a batch in the LFW test set.\', default=100)\n    parser.add_argument(\'--lfw_nrof_folds\', type=int,\n        help=\'Number of folds to use for cross validation. Mainly used for testing.\', default=10)\n    parser.add_argument(\'--lfw_distance_metric\', type=int,\n        help=\'Type of distance metric to use. 0: Euclidian, 1:Cosine similarity distance.\', default=0)\n    parser.add_argument(\'--lfw_use_flipped_images\', \n        help=\'Concatenates embeddings for the image and its horizontally flipped counterpart.\', action=\'store_true\')\n    parser.add_argument(\'--lfw_subtract_mean\', \n        help=\'Subtract feature mean before calculating distance.\', action=\'store_true\')\n    return parser.parse_args(argv)\n  \n\nif __name__ == \'__main__\':\n    main(parse_arguments(sys.argv[1:]))\n'"
src/train_tripletloss.py,39,"b'""""""Training a face recognizer with TensorFlow based on the FaceNet paper\nFaceNet: A Unified Embedding for Face Recognition and Clustering: http://arxiv.org/abs/1503.03832\n""""""\n# MIT License\n# \n# Copyright (c) 2016 David Sandberg\n# \n# Permission is hereby granted, free of charge, to any person obtaining a copy\n# of this software and associated documentation files (the ""Software""), to deal\n# in the Software without restriction, including without limitation the rights\n# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n# copies of the Software, and to permit persons to whom the Software is\n# furnished to do so, subject to the following conditions:\n# \n# The above copyright notice and this permission notice shall be included in all\n# copies or substantial portions of the Software.\n# \n# THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n# SOFTWARE.\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom datetime import datetime\nimport os.path\nimport time\nimport sys\nimport tensorflow as tf\nimport numpy as np\nimport importlib\nimport itertools\nimport argparse\nimport facenet\nimport lfw\n\nfrom tensorflow.python.ops import data_flow_ops\n\nfrom six.moves import xrange  # @UnresolvedImport\n\ndef main(args):\n  \n    network = importlib.import_module(args.model_def)\n\n    subdir = datetime.strftime(datetime.now(), \'%Y%m%d-%H%M%S\')\n    log_dir = os.path.join(os.path.expanduser(args.logs_base_dir), subdir)\n    if not os.path.isdir(log_dir):  # Create the log directory if it doesn\'t exist\n        os.makedirs(log_dir)\n    model_dir = os.path.join(os.path.expanduser(args.models_base_dir), subdir)\n    if not os.path.isdir(model_dir):  # Create the model directory if it doesn\'t exist\n        os.makedirs(model_dir)\n\n    # Write arguments to a text file\n    facenet.write_arguments_to_file(args, os.path.join(log_dir, \'arguments.txt\'))\n        \n    # Store some git revision info in a text file in the log directory\n    src_path,_ = os.path.split(os.path.realpath(__file__))\n    facenet.store_revision_info(src_path, log_dir, \' \'.join(sys.argv))\n\n    np.random.seed(seed=args.seed)\n    train_set = facenet.get_dataset(args.data_dir)\n    \n    print(\'Model directory: %s\' % model_dir)\n    print(\'Log directory: %s\' % log_dir)\n    if args.pretrained_model:\n        print(\'Pre-trained model: %s\' % os.path.expanduser(args.pretrained_model))\n    \n    if args.lfw_dir:\n        print(\'LFW directory: %s\' % args.lfw_dir)\n        # Read the file containing the pairs used for testing\n        pairs = lfw.read_pairs(os.path.expanduser(args.lfw_pairs))\n        # Get the paths for the corresponding images\n        lfw_paths, actual_issame = lfw.get_paths(os.path.expanduser(args.lfw_dir), pairs)\n        \n    \n    with tf.Graph().as_default():\n        tf.set_random_seed(args.seed)\n        global_step = tf.Variable(0, trainable=False)\n\n        # Placeholder for the learning rate\n        learning_rate_placeholder = tf.placeholder(tf.float32, name=\'learning_rate\')\n        \n        batch_size_placeholder = tf.placeholder(tf.int32, name=\'batch_size\')\n        \n        phase_train_placeholder = tf.placeholder(tf.bool, name=\'phase_train\')\n        \n        image_paths_placeholder = tf.placeholder(tf.string, shape=(None,3), name=\'image_paths\')\n        labels_placeholder = tf.placeholder(tf.int64, shape=(None,3), name=\'labels\')\n        \n        input_queue = data_flow_ops.FIFOQueue(capacity=100000,\n                                    dtypes=[tf.string, tf.int64],\n                                    shapes=[(3,), (3,)],\n                                    shared_name=None, name=None)\n        enqueue_op = input_queue.enqueue_many([image_paths_placeholder, labels_placeholder])\n        \n        nrof_preprocess_threads = 4\n        images_and_labels = []\n        for _ in range(nrof_preprocess_threads):\n            filenames, label = input_queue.dequeue()\n            images = []\n            for filename in tf.unstack(filenames):\n                file_contents = tf.read_file(filename)\n                image = tf.image.decode_image(file_contents, channels=3)\n                \n                if args.random_crop:\n                    image = tf.random_crop(image, [args.image_size, args.image_size, 3])\n                else:\n                    image = tf.image.resize_image_with_crop_or_pad(image, args.image_size, args.image_size)\n                if args.random_flip:\n                    image = tf.image.random_flip_left_right(image)\n    \n                #pylint: disable=no-member\n                image.set_shape((args.image_size, args.image_size, 3))\n                images.append(tf.image.per_image_standardization(image))\n            images_and_labels.append([images, label])\n    \n        image_batch, labels_batch = tf.train.batch_join(\n            images_and_labels, batch_size=batch_size_placeholder, \n            shapes=[(args.image_size, args.image_size, 3), ()], enqueue_many=True,\n            capacity=4 * nrof_preprocess_threads * args.batch_size,\n            allow_smaller_final_batch=True)\n        image_batch = tf.identity(image_batch, \'image_batch\')\n        image_batch = tf.identity(image_batch, \'input\')\n        labels_batch = tf.identity(labels_batch, \'label_batch\')\n\n        # Build the inference graph\n        prelogits, _ = network.inference(image_batch, args.keep_probability, \n            phase_train=phase_train_placeholder, bottleneck_layer_size=args.embedding_size,\n            weight_decay=args.weight_decay)\n        \n        embeddings = tf.nn.l2_normalize(prelogits, 1, 1e-10, name=\'embeddings\')\n        # Split embeddings into anchor, positive and negative and calculate triplet loss\n        anchor, positive, negative = tf.unstack(tf.reshape(embeddings, [-1,3,args.embedding_size]), 3, 1)\n        triplet_loss = facenet.triplet_loss(anchor, positive, negative, args.alpha)\n        \n        learning_rate = tf.train.exponential_decay(learning_rate_placeholder, global_step,\n            args.learning_rate_decay_epochs*args.epoch_size, args.learning_rate_decay_factor, staircase=True)\n        tf.summary.scalar(\'learning_rate\', learning_rate)\n\n        # Calculate the total losses\n        regularization_losses = tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES)\n        total_loss = tf.add_n([triplet_loss] + regularization_losses, name=\'total_loss\')\n\n        # Build a Graph that trains the model with one batch of examples and updates the model parameters\n        train_op = facenet.train(total_loss, global_step, args.optimizer, \n            learning_rate, args.moving_average_decay, tf.global_variables())\n        \n        # Create a saver\n        saver = tf.train.Saver(tf.trainable_variables(), max_to_keep=3)\n\n        # Build the summary operation based on the TF collection of Summaries.\n        summary_op = tf.summary.merge_all()\n\n        # Start running operations on the Graph.\n        gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=args.gpu_memory_fraction)\n        sess = tf.Session(config=tf.ConfigProto(gpu_options=gpu_options))        \n\n        # Initialize variables\n        sess.run(tf.global_variables_initializer(), feed_dict={phase_train_placeholder:True})\n        sess.run(tf.local_variables_initializer(), feed_dict={phase_train_placeholder:True})\n\n        summary_writer = tf.summary.FileWriter(log_dir, sess.graph)\n        coord = tf.train.Coordinator()\n        tf.train.start_queue_runners(coord=coord, sess=sess)\n\n        with sess.as_default():\n\n            if args.pretrained_model:\n                print(\'Restoring pretrained model: %s\' % args.pretrained_model)\n                saver.restore(sess, os.path.expanduser(args.pretrained_model))\n\n            # Training and validation loop\n            epoch = 0\n            while epoch < args.max_nrof_epochs:\n                step = sess.run(global_step, feed_dict=None)\n                epoch = step // args.epoch_size\n                # Train for one epoch\n                train(args, sess, train_set, epoch, image_paths_placeholder, labels_placeholder, labels_batch,\n                    batch_size_placeholder, learning_rate_placeholder, phase_train_placeholder, enqueue_op, input_queue, global_step, \n                    embeddings, total_loss, train_op, summary_op, summary_writer, args.learning_rate_schedule_file,\n                    args.embedding_size, anchor, positive, negative, triplet_loss)\n\n                # Save variables and the metagraph if it doesn\'t exist already\n                save_variables_and_metagraph(sess, saver, summary_writer, model_dir, subdir, step)\n\n                # Evaluate on LFW\n                if args.lfw_dir:\n                    evaluate(sess, lfw_paths, embeddings, labels_batch, image_paths_placeholder, labels_placeholder, \n                            batch_size_placeholder, learning_rate_placeholder, phase_train_placeholder, enqueue_op, actual_issame, args.batch_size, \n                            args.lfw_nrof_folds, log_dir, step, summary_writer, args.embedding_size)\n\n    return model_dir\n\n\ndef train(args, sess, dataset, epoch, image_paths_placeholder, labels_placeholder, labels_batch,\n          batch_size_placeholder, learning_rate_placeholder, phase_train_placeholder, enqueue_op, input_queue, global_step, \n          embeddings, loss, train_op, summary_op, summary_writer, learning_rate_schedule_file,\n          embedding_size, anchor, positive, negative, triplet_loss):\n    batch_number = 0\n    \n    if args.learning_rate>0.0:\n        lr = args.learning_rate\n    else:\n        lr = facenet.get_learning_rate_from_file(learning_rate_schedule_file, epoch)\n    while batch_number < args.epoch_size:\n        # Sample people randomly from the dataset\n        image_paths, num_per_class = sample_people(dataset, args.people_per_batch, args.images_per_person)\n        \n        print(\'Running forward pass on sampled images: \', end=\'\')\n        start_time = time.time()\n        nrof_examples = args.people_per_batch * args.images_per_person\n        labels_array = np.reshape(np.arange(nrof_examples),(-1,3))\n        image_paths_array = np.reshape(np.expand_dims(np.array(image_paths),1), (-1,3))\n        sess.run(enqueue_op, {image_paths_placeholder: image_paths_array, labels_placeholder: labels_array})\n        emb_array = np.zeros((nrof_examples, embedding_size))\n        nrof_batches = int(np.ceil(nrof_examples / args.batch_size))\n        for i in range(nrof_batches):\n            batch_size = min(nrof_examples-i*args.batch_size, args.batch_size)\n            emb, lab = sess.run([embeddings, labels_batch], feed_dict={batch_size_placeholder: batch_size, \n                learning_rate_placeholder: lr, phase_train_placeholder: True})\n            emb_array[lab,:] = emb\n        print(\'%.3f\' % (time.time()-start_time))\n\n        # Select triplets based on the embeddings\n        print(\'Selecting suitable triplets for training\')\n        triplets, nrof_random_negs, nrof_triplets = select_triplets(emb_array, num_per_class, \n            image_paths, args.people_per_batch, args.alpha)\n        selection_time = time.time() - start_time\n        print(\'(nrof_random_negs, nrof_triplets) = (%d, %d): time=%.3f seconds\' % \n            (nrof_random_negs, nrof_triplets, selection_time))\n\n        # Perform training on the selected triplets\n        nrof_batches = int(np.ceil(nrof_triplets*3/args.batch_size))\n        triplet_paths = list(itertools.chain(*triplets))\n        labels_array = np.reshape(np.arange(len(triplet_paths)),(-1,3))\n        triplet_paths_array = np.reshape(np.expand_dims(np.array(triplet_paths),1), (-1,3))\n        sess.run(enqueue_op, {image_paths_placeholder: triplet_paths_array, labels_placeholder: labels_array})\n        nrof_examples = len(triplet_paths)\n        train_time = 0\n        i = 0\n        emb_array = np.zeros((nrof_examples, embedding_size))\n        loss_array = np.zeros((nrof_triplets,))\n        summary = tf.Summary()\n        step = 0\n        while i < nrof_batches:\n            start_time = time.time()\n            batch_size = min(nrof_examples-i*args.batch_size, args.batch_size)\n            feed_dict = {batch_size_placeholder: batch_size, learning_rate_placeholder: lr, phase_train_placeholder: True}\n            err, _, step, emb, lab = sess.run([loss, train_op, global_step, embeddings, labels_batch], feed_dict=feed_dict)\n            emb_array[lab,:] = emb\n            loss_array[i] = err\n            duration = time.time() - start_time\n            print(\'Epoch: [%d][%d/%d]\\tTime %.3f\\tLoss %2.3f\' %\n                  (epoch, batch_number+1, args.epoch_size, duration, err))\n            batch_number += 1\n            i += 1\n            train_time += duration\n            summary.value.add(tag=\'loss\', simple_value=err)\n            \n        # Add validation loss and accuracy to summary\n        #pylint: disable=maybe-no-member\n        summary.value.add(tag=\'time/selection\', simple_value=selection_time)\n        summary_writer.add_summary(summary, step)\n    return step\n  \ndef select_triplets(embeddings, nrof_images_per_class, image_paths, people_per_batch, alpha):\n    """""" Select the triplets for training\n    """"""\n    trip_idx = 0\n    emb_start_idx = 0\n    num_trips = 0\n    triplets = []\n    \n    # VGG Face: Choosing good triplets is crucial and should strike a balance between\n    #  selecting informative (i.e. challenging) examples and swamping training with examples that\n    #  are too hard. This is achieve by extending each pair (a, p) to a triplet (a, p, n) by sampling\n    #  the image n at random, but only between the ones that violate the triplet loss margin. The\n    #  latter is a form of hard-negative mining, but it is not as aggressive (and much cheaper) than\n    #  choosing the maximally violating example, as often done in structured output learning.\n\n    for i in xrange(people_per_batch):\n        nrof_images = int(nrof_images_per_class[i])\n        for j in xrange(1,nrof_images):\n            a_idx = emb_start_idx + j - 1\n            neg_dists_sqr = np.sum(np.square(embeddings[a_idx] - embeddings), 1)\n            for pair in xrange(j, nrof_images): # For every possible positive pair.\n                p_idx = emb_start_idx + pair\n                pos_dist_sqr = np.sum(np.square(embeddings[a_idx]-embeddings[p_idx]))\n                neg_dists_sqr[emb_start_idx:emb_start_idx+nrof_images] = np.NaN\n                #all_neg = np.where(np.logical_and(neg_dists_sqr-pos_dist_sqr<alpha, pos_dist_sqr<neg_dists_sqr))[0]  # FaceNet selection\n                all_neg = np.where(neg_dists_sqr-pos_dist_sqr<alpha)[0] # VGG Face selecction\n                nrof_random_negs = all_neg.shape[0]\n                if nrof_random_negs>0:\n                    rnd_idx = np.random.randint(nrof_random_negs)\n                    n_idx = all_neg[rnd_idx]\n                    triplets.append((image_paths[a_idx], image_paths[p_idx], image_paths[n_idx]))\n                    #print(\'Triplet %d: (%d, %d, %d), pos_dist=%2.6f, neg_dist=%2.6f (%d, %d, %d, %d, %d)\' % \n                    #    (trip_idx, a_idx, p_idx, n_idx, pos_dist_sqr, neg_dists_sqr[n_idx], nrof_random_negs, rnd_idx, i, j, emb_start_idx))\n                    trip_idx += 1\n\n                num_trips += 1\n\n        emb_start_idx += nrof_images\n\n    np.random.shuffle(triplets)\n    return triplets, num_trips, len(triplets)\n\ndef sample_people(dataset, people_per_batch, images_per_person):\n    nrof_images = people_per_batch * images_per_person\n  \n    # Sample classes from the dataset\n    nrof_classes = len(dataset)\n    class_indices = np.arange(nrof_classes)\n    np.random.shuffle(class_indices)\n    \n    i = 0\n    image_paths = []\n    num_per_class = []\n    sampled_class_indices = []\n    # Sample images from these classes until we have enough\n    while len(image_paths)<nrof_images:\n        class_index = class_indices[i]\n        nrof_images_in_class = len(dataset[class_index])\n        image_indices = np.arange(nrof_images_in_class)\n        np.random.shuffle(image_indices)\n        nrof_images_from_class = min(nrof_images_in_class, images_per_person, nrof_images-len(image_paths))\n        idx = image_indices[0:nrof_images_from_class]\n        image_paths_for_class = [dataset[class_index].image_paths[j] for j in idx]\n        sampled_class_indices += [class_index]*nrof_images_from_class\n        image_paths += image_paths_for_class\n        num_per_class.append(nrof_images_from_class)\n        i+=1\n  \n    return image_paths, num_per_class\n\ndef evaluate(sess, image_paths, embeddings, labels_batch, image_paths_placeholder, labels_placeholder, \n        batch_size_placeholder, learning_rate_placeholder, phase_train_placeholder, enqueue_op, actual_issame, batch_size, \n        nrof_folds, log_dir, step, summary_writer, embedding_size):\n    start_time = time.time()\n    # Run forward pass to calculate embeddings\n    print(\'Running forward pass on LFW images: \', end=\'\')\n    \n    nrof_images = len(actual_issame)*2\n    assert(len(image_paths)==nrof_images)\n    labels_array = np.reshape(np.arange(nrof_images),(-1,3))\n    image_paths_array = np.reshape(np.expand_dims(np.array(image_paths),1), (-1,3))\n    sess.run(enqueue_op, {image_paths_placeholder: image_paths_array, labels_placeholder: labels_array})\n    emb_array = np.zeros((nrof_images, embedding_size))\n    nrof_batches = int(np.ceil(nrof_images / batch_size))\n    label_check_array = np.zeros((nrof_images,))\n    for i in xrange(nrof_batches):\n        batch_size = min(nrof_images-i*batch_size, batch_size)\n        emb, lab = sess.run([embeddings, labels_batch], feed_dict={batch_size_placeholder: batch_size,\n            learning_rate_placeholder: 0.0, phase_train_placeholder: False})\n        emb_array[lab,:] = emb\n        label_check_array[lab] = 1\n    print(\'%.3f\' % (time.time()-start_time))\n    \n    assert(np.all(label_check_array==1))\n    \n    _, _, accuracy, val, val_std, far = lfw.evaluate(emb_array, actual_issame, nrof_folds=nrof_folds)\n    \n    print(\'Accuracy: %1.3f+-%1.3f\' % (np.mean(accuracy), np.std(accuracy)))\n    print(\'Validation rate: %2.5f+-%2.5f @ FAR=%2.5f\' % (val, val_std, far))\n    lfw_time = time.time() - start_time\n    # Add validation loss and accuracy to summary\n    summary = tf.Summary()\n    #pylint: disable=maybe-no-member\n    summary.value.add(tag=\'lfw/accuracy\', simple_value=np.mean(accuracy))\n    summary.value.add(tag=\'lfw/val_rate\', simple_value=val)\n    summary.value.add(tag=\'time/lfw\', simple_value=lfw_time)\n    summary_writer.add_summary(summary, step)\n    with open(os.path.join(log_dir,\'lfw_result.txt\'),\'at\') as f:\n        f.write(\'%d\\t%.5f\\t%.5f\\n\' % (step, np.mean(accuracy), val))\n\ndef save_variables_and_metagraph(sess, saver, summary_writer, model_dir, model_name, step):\n    # Save the model checkpoint\n    print(\'Saving variables\')\n    start_time = time.time()\n    checkpoint_path = os.path.join(model_dir, \'model-%s.ckpt\' % model_name)\n    saver.save(sess, checkpoint_path, global_step=step, write_meta_graph=False)\n    save_time_variables = time.time() - start_time\n    print(\'Variables saved in %.2f seconds\' % save_time_variables)\n    metagraph_filename = os.path.join(model_dir, \'model-%s.meta\' % model_name)\n    save_time_metagraph = 0  \n    if not os.path.exists(metagraph_filename):\n        print(\'Saving metagraph\')\n        start_time = time.time()\n        saver.export_meta_graph(metagraph_filename)\n        save_time_metagraph = time.time() - start_time\n        print(\'Metagraph saved in %.2f seconds\' % save_time_metagraph)\n    summary = tf.Summary()\n    #pylint: disable=maybe-no-member\n    summary.value.add(tag=\'time/save_variables\', simple_value=save_time_variables)\n    summary.value.add(tag=\'time/save_metagraph\', simple_value=save_time_metagraph)\n    summary_writer.add_summary(summary, step)\n  \n  \ndef get_learning_rate_from_file(filename, epoch):\n    with open(filename, \'r\') as f:\n        for line in f.readlines():\n            line = line.split(\'#\', 1)[0]\n            if line:\n                par = line.strip().split(\':\')\n                e = int(par[0])\n                lr = float(par[1])\n                if e <= epoch:\n                    learning_rate = lr\n                else:\n                    return learning_rate\n    \n\ndef parse_arguments(argv):\n    parser = argparse.ArgumentParser()\n    \n    parser.add_argument(\'--logs_base_dir\', type=str, \n        help=\'Directory where to write event logs.\', default=\'~/logs/facenet\')\n    parser.add_argument(\'--models_base_dir\', type=str,\n        help=\'Directory where to write trained models and checkpoints.\', default=\'~/models/facenet\')\n    parser.add_argument(\'--gpu_memory_fraction\', type=float,\n        help=\'Upper bound on the amount of GPU memory that will be used by the process.\', default=1.0)\n    parser.add_argument(\'--pretrained_model\', type=str,\n        help=\'Load a pretrained model before training starts.\')\n    parser.add_argument(\'--data_dir\', type=str,\n        help=\'Path to the data directory containing aligned face patches.\',\n        default=\'~/datasets/casia/casia_maxpy_mtcnnalign_182_160\')\n    parser.add_argument(\'--model_def\', type=str,\n        help=\'Model definition. Points to a module containing the definition of the inference graph.\', default=\'models.inception_resnet_v1\')\n    parser.add_argument(\'--max_nrof_epochs\', type=int,\n        help=\'Number of epochs to run.\', default=500)\n    parser.add_argument(\'--batch_size\', type=int,\n        help=\'Number of images to process in a batch.\', default=90)\n    parser.add_argument(\'--image_size\', type=int,\n        help=\'Image size (height, width) in pixels.\', default=160)\n    parser.add_argument(\'--people_per_batch\', type=int,\n        help=\'Number of people per batch.\', default=45)\n    parser.add_argument(\'--images_per_person\', type=int,\n        help=\'Number of images per person.\', default=40)\n    parser.add_argument(\'--epoch_size\', type=int,\n        help=\'Number of batches per epoch.\', default=1000)\n    parser.add_argument(\'--alpha\', type=float,\n        help=\'Positive to negative triplet distance margin.\', default=0.2)\n    parser.add_argument(\'--embedding_size\', type=int,\n        help=\'Dimensionality of the embedding.\', default=128)\n    parser.add_argument(\'--random_crop\', \n        help=\'Performs random cropping of training images. If false, the center image_size pixels from the training images are used. \' +\n         \'If the size of the images in the data directory is equal to image_size no cropping is performed\', action=\'store_true\')\n    parser.add_argument(\'--random_flip\', \n        help=\'Performs random horizontal flipping of training images.\', action=\'store_true\')\n    parser.add_argument(\'--keep_probability\', type=float,\n        help=\'Keep probability of dropout for the fully connected layer(s).\', default=1.0)\n    parser.add_argument(\'--weight_decay\', type=float,\n        help=\'L2 weight regularization.\', default=0.0)\n    parser.add_argument(\'--optimizer\', type=str, choices=[\'ADAGRAD\', \'ADADELTA\', \'ADAM\', \'RMSPROP\', \'MOM\'],\n        help=\'The optimization algorithm to use\', default=\'ADAGRAD\')\n    parser.add_argument(\'--learning_rate\', type=float,\n        help=\'Initial learning rate. If set to a negative value a learning rate \' +\n        \'schedule can be specified in the file ""learning_rate_schedule.txt""\', default=0.1)\n    parser.add_argument(\'--learning_rate_decay_epochs\', type=int,\n        help=\'Number of epochs between learning rate decay.\', default=100)\n    parser.add_argument(\'--learning_rate_decay_factor\', type=float,\n        help=\'Learning rate decay factor.\', default=1.0)\n    parser.add_argument(\'--moving_average_decay\', type=float,\n        help=\'Exponential decay for tracking of training parameters.\', default=0.9999)\n    parser.add_argument(\'--seed\', type=int,\n        help=\'Random seed.\', default=666)\n    parser.add_argument(\'--learning_rate_schedule_file\', type=str,\n        help=\'File containing the learning rate schedule that is used when learning_rate is set to to -1.\', default=\'data/learning_rate_schedule.txt\')\n\n    # Parameters for validation on LFW\n    parser.add_argument(\'--lfw_pairs\', type=str,\n        help=\'The file containing the pairs to use for validation.\', default=\'data/pairs.txt\')\n    parser.add_argument(\'--lfw_dir\', type=str,\n        help=\'Path to the data directory containing aligned face patches.\', default=\'\')\n    parser.add_argument(\'--lfw_nrof_folds\', type=int,\n        help=\'Number of folds to use for cross validation. Mainly used for testing.\', default=10)\n    return parser.parse_args(argv)\n  \n\nif __name__ == \'__main__\':\n    main(parse_arguments(sys.argv[1:]))\n'"
src/validate_on_lfw.py,11,"b'""""""Validate a face recognizer on the ""Labeled Faces in the Wild"" dataset (http://vis-www.cs.umass.edu/lfw/).\nEmbeddings are calculated using the pairs from http://vis-www.cs.umass.edu/lfw/pairs.txt and the ROC curve\nis calculated and plotted. Both the model metagraph and the model parameters need to exist\nin the same directory, and the metagraph should have the extension \'.meta\'.\n""""""\n# MIT License\n# \n# Copyright (c) 2016 David Sandberg\n# \n# Permission is hereby granted, free of charge, to any person obtaining a copy\n# of this software and associated documentation files (the ""Software""), to deal\n# in the Software without restriction, including without limitation the rights\n# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n# copies of the Software, and to permit persons to whom the Software is\n# furnished to do so, subject to the following conditions:\n# \n# The above copyright notice and this permission notice shall be included in all\n# copies or substantial portions of the Software.\n# \n# THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n# SOFTWARE.\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\nimport numpy as np\nimport argparse\nimport facenet\nimport lfw\nimport os\nimport sys\nfrom tensorflow.python.ops import data_flow_ops\nfrom sklearn import metrics\nfrom scipy.optimize import brentq\nfrom scipy import interpolate\n\ndef main(args):\n  \n    with tf.Graph().as_default():\n      \n        with tf.Session() as sess:\n            \n            # Read the file containing the pairs used for testing\n            pairs = lfw.read_pairs(os.path.expanduser(args.lfw_pairs))\n\n            # Get the paths for the corresponding images\n            paths, actual_issame = lfw.get_paths(os.path.expanduser(args.lfw_dir), pairs)\n            \n            image_paths_placeholder = tf.placeholder(tf.string, shape=(None,1), name=\'image_paths\')\n            labels_placeholder = tf.placeholder(tf.int32, shape=(None,1), name=\'labels\')\n            batch_size_placeholder = tf.placeholder(tf.int32, name=\'batch_size\')\n            control_placeholder = tf.placeholder(tf.int32, shape=(None,1), name=\'control\')\n            phase_train_placeholder = tf.placeholder(tf.bool, name=\'phase_train\')\n \n            nrof_preprocess_threads = 4\n            image_size = (args.image_size, args.image_size)\n            eval_input_queue = data_flow_ops.FIFOQueue(capacity=2000000,\n                                        dtypes=[tf.string, tf.int32, tf.int32],\n                                        shapes=[(1,), (1,), (1,)],\n                                        shared_name=None, name=None)\n            eval_enqueue_op = eval_input_queue.enqueue_many([image_paths_placeholder, labels_placeholder, control_placeholder], name=\'eval_enqueue_op\')\n            image_batch, label_batch = facenet.create_input_pipeline(eval_input_queue, image_size, nrof_preprocess_threads, batch_size_placeholder)\n     \n            # Load the model\n            input_map = {\'image_batch\': image_batch, \'label_batch\': label_batch, \'phase_train\': phase_train_placeholder}\n            facenet.load_model(args.model, input_map=input_map)\n\n            # Get output tensor\n            embeddings = tf.get_default_graph().get_tensor_by_name(""embeddings:0"")\n#              \n            coord = tf.train.Coordinator()\n            tf.train.start_queue_runners(coord=coord, sess=sess)\n\n            evaluate(sess, eval_enqueue_op, image_paths_placeholder, labels_placeholder, phase_train_placeholder, batch_size_placeholder, control_placeholder,\n                embeddings, label_batch, paths, actual_issame, args.lfw_batch_size, args.lfw_nrof_folds, args.distance_metric, args.subtract_mean,\n                args.use_flipped_images, args.use_fixed_image_standardization)\n\n              \ndef evaluate(sess, enqueue_op, image_paths_placeholder, labels_placeholder, phase_train_placeholder, batch_size_placeholder, control_placeholder,\n        embeddings, labels, image_paths, actual_issame, batch_size, nrof_folds, distance_metric, subtract_mean, use_flipped_images, use_fixed_image_standardization):\n    # Run forward pass to calculate embeddings\n    print(\'Runnning forward pass on LFW images\')\n    \n    # Enqueue one epoch of image paths and labels\n    nrof_embeddings = len(actual_issame)*2  # nrof_pairs * nrof_images_per_pair\n    nrof_flips = 2 if use_flipped_images else 1\n    nrof_images = nrof_embeddings * nrof_flips\n    labels_array = np.expand_dims(np.arange(0,nrof_images),1)\n    image_paths_array = np.expand_dims(np.repeat(np.array(image_paths),nrof_flips),1)\n    control_array = np.zeros_like(labels_array, np.int32)\n    if use_fixed_image_standardization:\n        control_array += np.ones_like(labels_array)*facenet.FIXED_STANDARDIZATION\n    if use_flipped_images:\n        # Flip every second image\n        control_array += (labels_array % 2)*facenet.FLIP\n    sess.run(enqueue_op, {image_paths_placeholder: image_paths_array, labels_placeholder: labels_array, control_placeholder: control_array})\n    \n    embedding_size = int(embeddings.get_shape()[1])\n    assert nrof_images % batch_size == 0, \'The number of LFW images must be an integer multiple of the LFW batch size\'\n    nrof_batches = nrof_images // batch_size\n    emb_array = np.zeros((nrof_images, embedding_size))\n    lab_array = np.zeros((nrof_images,))\n    for i in range(nrof_batches):\n        feed_dict = {phase_train_placeholder:False, batch_size_placeholder:batch_size}\n        emb, lab = sess.run([embeddings, labels], feed_dict=feed_dict)\n        lab_array[lab] = lab\n        emb_array[lab, :] = emb\n        if i % 10 == 9:\n            print(\'.\', end=\'\')\n            sys.stdout.flush()\n    print(\'\')\n    embeddings = np.zeros((nrof_embeddings, embedding_size*nrof_flips))\n    if use_flipped_images:\n        # Concatenate embeddings for flipped and non flipped version of the images\n        embeddings[:,:embedding_size] = emb_array[0::2,:]\n        embeddings[:,embedding_size:] = emb_array[1::2,:]\n    else:\n        embeddings = emb_array\n\n    assert np.array_equal(lab_array, np.arange(nrof_images))==True, \'Wrong labels used for evaluation, possibly caused by training examples left in the input pipeline\'\n    tpr, fpr, accuracy, val, val_std, far = lfw.evaluate(embeddings, actual_issame, nrof_folds=nrof_folds, distance_metric=distance_metric, subtract_mean=subtract_mean)\n    \n    print(\'Accuracy: %2.5f+-%2.5f\' % (np.mean(accuracy), np.std(accuracy)))\n    print(\'Validation rate: %2.5f+-%2.5f @ FAR=%2.5f\' % (val, val_std, far))\n    \n    auc = metrics.auc(fpr, tpr)\n    print(\'Area Under Curve (AUC): %1.3f\' % auc)\n    eer = brentq(lambda x: 1. - x - interpolate.interp1d(fpr, tpr)(x), 0., 1.)\n    print(\'Equal Error Rate (EER): %1.3f\' % eer)\n    \ndef parse_arguments(argv):\n    parser = argparse.ArgumentParser()\n    \n    parser.add_argument(\'lfw_dir\', type=str,\n        help=\'Path to the data directory containing aligned LFW face patches.\')\n    parser.add_argument(\'--lfw_batch_size\', type=int,\n        help=\'Number of images to process in a batch in the LFW test set.\', default=100)\n    parser.add_argument(\'model\', type=str, \n        help=\'Could be either a directory containing the meta_file and ckpt_file or a model protobuf (.pb) file\')\n    parser.add_argument(\'--image_size\', type=int,\n        help=\'Image size (height, width) in pixels.\', default=160)\n    parser.add_argument(\'--lfw_pairs\', type=str,\n        help=\'The file containing the pairs to use for validation.\', default=\'data/pairs.txt\')\n    parser.add_argument(\'--lfw_nrof_folds\', type=int,\n        help=\'Number of folds to use for cross validation. Mainly used for testing.\', default=10)\n    parser.add_argument(\'--distance_metric\', type=int,\n        help=\'Distance metric  0:euclidian, 1:cosine similarity.\', default=0)\n    parser.add_argument(\'--use_flipped_images\', \n        help=\'Concatenates embeddings for the image and its horizontally flipped counterpart.\', action=\'store_true\')\n    parser.add_argument(\'--subtract_mean\', \n        help=\'Subtract feature mean before calculating distance.\', action=\'store_true\')\n    parser.add_argument(\'--use_fixed_image_standardization\', \n        help=\'Performs fixed standardization of images.\', action=\'store_true\')\n    return parser.parse_args(argv)\n\nif __name__ == \'__main__\':\n    main(parse_arguments(sys.argv[1:]))\n'"
test/batch_norm_test.py,6,"b'# MIT License\n# \n# Copyright (c) 2016 David Sandberg\n# \n# Permission is hereby granted, free of charge, to any person obtaining a copy\n# of this software and associated documentation files (the ""Software""), to deal\n# in the Software without restriction, including without limitation the rights\n# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n# copies of the Software, and to permit persons to whom the Software is\n# furnished to do so, subject to the following conditions:\n# \n# The above copyright notice and this permission notice shall be included in all\n# copies or substantial portions of the Software.\n# \n# THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n# SOFTWARE.\n\nimport unittest\nimport tensorflow as tf\nimport models\nimport numpy as np\nimport numpy.testing as testing\n\nclass BatchNormTest(unittest.TestCase):\n\n\n    @unittest.skip(""Skip batch norm test case"")\n    def testBatchNorm(self):\n      \n        tf.set_random_seed(123)\n  \n        x = tf.placeholder(tf.float32, [None, 20, 20, 10], name=\'input\')\n        phase_train = tf.placeholder(tf.bool, name=\'phase_train\')\n        \n        # generate random noise to pass into batch norm\n        #x_gen = tf.random_normal([50,20,20,10])\n        \n        bn = models.network.batch_norm(x, phase_train)\n        \n        init = tf.global_variables_initializer()\n        sess = tf.Session(config=tf.ConfigProto())\n        sess.run(init)\n  \n        with sess.as_default():\n        \n            #generate a constant variable to pass into batch norm\n            y = np.random.normal(0, 1, size=(50,20,20,10))\n            \n            feed_dict = {x: y, phase_train: True}\n            sess.run(bn, feed_dict=feed_dict)\n            \n            feed_dict = {x: y, phase_train: False}\n            y1 = sess.run(bn, feed_dict=feed_dict)\n            y2 = sess.run(bn, feed_dict=feed_dict)\n            \n            testing.assert_almost_equal(y1, y2, 10, \'Output from two forward passes with phase_train==false should be equal\')\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n    '"
test/center_loss_test.py,5,"b'# MIT License\n# \n# Copyright (c) 2016 David Sandberg\n# \n# Permission is hereby granted, free of charge, to any person obtaining a copy\n# of this software and associated documentation files (the ""Software""), to deal\n# in the Software without restriction, including without limitation the rights\n# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n# copies of the Software, and to permit persons to whom the Software is\n# furnished to do so, subject to the following conditions:\n# \n# The above copyright notice and this permission notice shall be included in all\n# copies or substantial portions of the Software.\n# \n# THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n# SOFTWARE.\n\nimport unittest\nimport tensorflow as tf\nimport numpy as np\nimport facenet\n\nclass CenterLossTest(unittest.TestCase):\n  \n\n\n    def testCenterLoss(self):\n        batch_size = 16\n        nrof_features = 2\n        nrof_classes = 16\n        alfa = 0.5\n        \n        with tf.Graph().as_default():\n        \n            features = tf.placeholder(tf.float32, shape=(batch_size, nrof_features), name=\'features\')\n            labels = tf.placeholder(tf.int32, shape=(batch_size,), name=\'labels\')\n\n            # Define center loss\n            center_loss, centers = facenet.center_loss(features, labels, alfa, nrof_classes)\n            \n            label_to_center = np.array( [ \n                 [-3,-3],  [-3,-1],  [-3,1],  [-3,3],\n                 [-1,-3],  [-1,-1],  [-1,1],  [-1,3],\n                 [ 1,-3],  [ 1,-1],  [ 1,1],  [ 1,3],\n                 [ 3,-3],  [ 3,-1],  [ 3,1],  [ 3,3] \n                 ])\n                \n            sess = tf.Session()\n            with sess.as_default():\n                sess.run(tf.global_variables_initializer())\n                np.random.seed(seed=666)\n                \n                for _ in range(0,100):\n                    # Create array of random labels\n                    lbls = np.random.randint(low=0, high=nrof_classes, size=(batch_size,))\n                    feats = create_features(label_to_center, batch_size, nrof_features, lbls)\n\n                    center_loss_, centers_ = sess.run([center_loss, centers], feed_dict={features:feats, labels:lbls})\n                    \n                # After a large number of updates the estimated centers should be close to the true ones\n                np.testing.assert_almost_equal(centers_, label_to_center, decimal=5, err_msg=\'Incorrect estimated centers\')\n                np.testing.assert_almost_equal(center_loss_, 0.0, decimal=5, err_msg=\'Incorrect center loss\')\n                \n\ndef create_features(label_to_center, batch_size, nrof_features, labels):\n    # Map label to center\n#     label_to_center_dict = { \n#          0:(-3,-3),  1:(-3,-1),  2:(-3,1),  3:(-3,3),\n#          4:(-1,-3),  5:(-1,-1),  6:(-1,1),  7:(-1,3),\n#          8:( 1,-3),  9:( 1,-1), 10:( 1,1), 11:( 1,3),\n#         12:( 3,-3), 13:( 3,-1), 14:( 3,1), 15:( 3,3),\n#         }\n    # Create array of features corresponding to the labels\n    feats = np.zeros((batch_size, nrof_features))\n    for i in range(batch_size):\n        cntr =  label_to_center[labels[i]]\n        for j in range(nrof_features):\n            feats[i,j] = cntr[j]\n    return feats\n                      \nif __name__ == ""__main__"":\n    unittest.main()\n'"
test/restore_test.py,31,"b'# MIT License\n# \n# Copyright (c) 2016 David Sandberg\n# \n# Permission is hereby granted, free of charge, to any person obtaining a copy\n# of this software and associated documentation files (the ""Software""), to deal\n# in the Software without restriction, including without limitation the rights\n# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n# copies of the Software, and to permit persons to whom the Software is\n# furnished to do so, subject to the following conditions:\n# \n# The above copyright notice and this permission notice shall be included in all\n# copies or substantial portions of the Software.\n# \n# THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n# SOFTWARE.\n\nimport unittest\nimport tempfile\nimport os\nimport shutil\nimport tensorflow as tf\nimport numpy as np\n\nclass TrainTest(unittest.TestCase):\n  \n    @classmethod\n    def setUpClass(self):\n        self.tmp_dir = tempfile.mkdtemp()\n        \n    @classmethod\n    def tearDownClass(self):\n        # Recursively remove the temporary directory\n        shutil.rmtree(self.tmp_dir)\n\n    def test_restore_noema(self):\n        \n        # Create 100 phony x, y data points in NumPy, y = x * 0.1 + 0.3\n        x_data = np.random.rand(100).astype(np.float32)\n        y_data = x_data * 0.1 + 0.3\n        \n        # Try to find values for W and b that compute y_data = W * x_data + b\n        # (We know that W should be 0.1 and b 0.3, but TensorFlow will\n        # figure that out for us.)\n        W = tf.Variable(tf.random_uniform([1], -1.0, 1.0), name=\'W\')\n        b = tf.Variable(tf.zeros([1]), name=\'b\')\n        y = W * x_data + b\n        \n        # Minimize the mean squared errors.\n        loss = tf.reduce_mean(tf.square(y - y_data))\n        optimizer = tf.train.GradientDescentOptimizer(0.5)\n        train = optimizer.minimize(loss)\n        \n        # Before starting, initialize the variables.  We will \'run\' this first.\n        init = tf.global_variables_initializer()\n\n        saver = tf.train.Saver(tf.trainable_variables())\n        \n        # Launch the graph.\n        sess = tf.Session()\n        sess.run(init)\n        \n        # Fit the line.\n        for _ in range(201):\n            sess.run(train)\n        \n        w_reference = sess.run(\'W:0\')\n        b_reference = sess.run(\'b:0\')\n        \n        saver.save(sess, os.path.join(self.tmp_dir, ""model_ex1""))\n        \n        tf.reset_default_graph()\n\n        saver = tf.train.import_meta_graph(os.path.join(self.tmp_dir, ""model_ex1.meta""))\n        sess = tf.Session()\n        saver.restore(sess, os.path.join(self.tmp_dir, ""model_ex1""))\n        \n        w_restored = sess.run(\'W:0\')\n        b_restored = sess.run(\'b:0\')\n        \n        self.assertAlmostEqual(w_reference, w_restored, \'Restored model use different weight than the original model\')\n        self.assertAlmostEqual(b_reference, b_restored, \'Restored model use different weight than the original model\')\n\n\n    @unittest.skip(""Skip restore EMA test case for now"")\n    def test_restore_ema(self):\n        \n        # Create 100 phony x, y data points in NumPy, y = x * 0.1 + 0.3\n        x_data = np.random.rand(100).astype(np.float32)\n        y_data = x_data * 0.1 + 0.3\n        \n        # Try to find values for W and b that compute y_data = W * x_data + b\n        # (We know that W should be 0.1 and b 0.3, but TensorFlow will\n        # figure that out for us.)\n        W = tf.Variable(tf.random_uniform([1], -1.0, 1.0), name=\'W\')\n        b = tf.Variable(tf.zeros([1]), name=\'b\')\n        y = W * x_data + b\n        \n        # Minimize the mean squared errors.\n        loss = tf.reduce_mean(tf.square(y - y_data))\n        optimizer = tf.train.GradientDescentOptimizer(0.5)\n        opt_op = optimizer.minimize(loss)\n\n        # Track the moving averages of all trainable variables.\n        ema = tf.train.ExponentialMovingAverage(decay=0.9999)\n        averages_op = ema.apply(tf.trainable_variables())\n        with tf.control_dependencies([opt_op]):\n            train_op = tf.group(averages_op)\n  \n        # Before starting, initialize the variables.  We will \'run\' this first.\n        init = tf.global_variables_initializer()\n\n        saver = tf.train.Saver(tf.trainable_variables())\n        \n        # Launch the graph.\n        sess = tf.Session()\n        sess.run(init)\n        \n        # Fit the line.\n        for _ in range(201):\n            sess.run(train_op)\n        \n        w_reference = sess.run(\'W/ExponentialMovingAverage:0\')\n        b_reference = sess.run(\'b/ExponentialMovingAverage:0\')\n        \n        saver.save(sess, os.path.join(self.tmp_dir, ""model_ex1""))\n                \n        tf.reset_default_graph()\n\n        tf.train.import_meta_graph(os.path.join(self.tmp_dir, ""model_ex1.meta""))\n        sess = tf.Session()\n        \n        print(\'------------------------------------------------------\')\n        for var in tf.global_variables():\n            print(\'all variables: \' + var.op.name)\n        for var in tf.trainable_variables():\n            print(\'normal variable: \' + var.op.name)\n        for var in tf.moving_average_variables():\n            print(\'ema variable: \' + var.op.name)\n        print(\'------------------------------------------------------\')\n\n        mode = 1\n        restore_vars = {}\n        if mode == 0:\n            ema = tf.train.ExponentialMovingAverage(1.0)\n            for var in tf.trainable_variables():\n                print(\'%s: %s\' % (ema.average_name(var), var.op.name))\n                restore_vars[ema.average_name(var)] = var\n        elif mode == 1:\n            for var in tf.trainable_variables():\n                ema_name = var.op.name + \'/ExponentialMovingAverage\'\n                print(\'%s: %s\' % (ema_name, var.op.name))\n                restore_vars[ema_name] = var\n            \n        saver = tf.train.Saver(restore_vars, name=\'ema_restore\')\n        \n        saver.restore(sess, os.path.join(self.tmp_dir, ""model_ex1""))\n        \n        w_restored = sess.run(\'W:0\')\n        b_restored = sess.run(\'b:0\')\n        \n        self.assertAlmostEqual(w_reference, w_restored, \'Restored model modes not use the EMA filtered weight\')\n        self.assertAlmostEqual(b_reference, b_restored, \'Restored model modes not use the EMA filtered bias\')\n\n        \n# Create a checkpoint file pointing to the model\ndef create_checkpoint_file(model_dir, model_file):\n    checkpoint_filename = os.path.join(model_dir, \'checkpoint\')\n    full_model_filename = os.path.join(model_dir, model_file)\n    with open(checkpoint_filename, \'w\') as f:\n        f.write(\'model_checkpoint_path: ""%s""\\n\' % full_model_filename)\n        f.write(\'all_model_checkpoint_paths: ""%s""\\n\' % full_model_filename)\n        \nif __name__ == ""__main__"":\n    unittest.main()\n    '"
test/train_test.py,0,"b'# MIT License\n# \n# Copyright (c) 2016 David Sandberg\n# \n# Permission is hereby granted, free of charge, to any person obtaining a copy\n# of this software and associated documentation files (the ""Software""), to deal\n# in the Software without restriction, including without limitation the rights\n# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n# copies of the Software, and to permit persons to whom the Software is\n# furnished to do so, subject to the following conditions:\n# \n# The above copyright notice and this permission notice shall be included in all\n# copies or substantial portions of the Software.\n# \n# THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n# SOFTWARE.\n\nimport unittest\nimport tempfile\nimport numpy as np\nimport cv2\nimport os\nimport shutil\nimport download_and_extract  # @UnresolvedImport\nimport subprocess\n\ndef memory_usage_psutil():\n    # return the memory usage in MB\n    import psutil\n    process = psutil.Process(os.getpid())\n    mem = process.memory_info()[0] / float(2 ** 20)\n    return mem\n\ndef align_dataset_if_needed(self):\n    if not os.path.exists(\'data/lfw_aligned\'):\n        argv = [\'python\',\n                \'src/align/align_dataset_mtcnn.py\',\n                \'data/lfw\',\n                \'data/lfw_aligned\',\n                \'--image_size\', \'160\',\n                \'--margin\', \'32\' ]\n        subprocess.call(argv)\n        \n        \nclass TrainTest(unittest.TestCase):\n  \n    @classmethod\n    def setUpClass(self):\n        self.tmp_dir = tempfile.mkdtemp()\n        self.dataset_dir = os.path.join(self.tmp_dir, \'dataset\')\n        create_mock_dataset(self.dataset_dir, 160)\n        self.lfw_pairs_file = create_mock_lfw_pairs(self.tmp_dir)\n        print(self.lfw_pairs_file)\n        self.pretrained_model_name = \'20180402-114759\'\n        download_and_extract.download_and_extract_file(self.pretrained_model_name, \'data/\')\n        download_and_extract.download_and_extract_file(\'lfw-subset\', \'data/\')\n        self.model_file = os.path.join(\'data\', self.pretrained_model_name, \'model-%s.ckpt-275\' % self.pretrained_model_name)\n        self.pretrained_model = os.path.join(\'data\', self.pretrained_model_name)\n        self.frozen_graph_filename = os.path.join(\'data\', self.pretrained_model_name+\'.pb\')\n        print(\'Memory utilization (SetUpClass): %.3f MB\' % memory_usage_psutil())\n\n    @classmethod\n    def tearDownClass(self):\n        # Recursively remove the temporary directory\n        shutil.rmtree(self.tmp_dir)\n\n    def tearDown(self):\n        print(\'Memory utilization (TearDown): %.3f MB\' % memory_usage_psutil())\n\n    def test_training_classifier_inception_resnet_v1(self):\n        print(\'test_training_classifier_inception_resnet_v1\')\n        argv = [\'python\',\n                \'src/train_softmax.py\',\n                \'--logs_base_dir\', self.tmp_dir,\n                \'--models_base_dir\', self.tmp_dir,\n                \'--data_dir\', self.dataset_dir,\n                \'--model_def\', \'models.inception_resnet_v1\',\n                \'--epoch_size\', \'1\',\n                \'--max_nrof_epochs\', \'1\',\n                \'--batch_size\', \'1\',\n                \'--lfw_pairs\', self.lfw_pairs_file,\n                \'--lfw_dir\', self.dataset_dir,\n                \'--lfw_nrof_folds\', \'2\',\n                \'--lfw_batch_size\', \'1\',\n                \'--nrof_preprocess_threads\', \'1\' ]\n        subprocess.call(argv)\n\n    def test_training_classifier_inception_resnet_v2(self):\n        print(\'test_training_classifier_inception_resnet_v2\')\n        argv = [\'python\',\n                \'src/train_softmax.py\',\n                \'--logs_base_dir\', self.tmp_dir,\n                \'--models_base_dir\', self.tmp_dir,\n                \'--data_dir\', self.dataset_dir,\n                \'--model_def\', \'models.inception_resnet_v2\',\n                \'--epoch_size\', \'1\',\n                \'--max_nrof_epochs\', \'1\',\n                \'--batch_size\', \'1\',\n                \'--lfw_pairs\', self.lfw_pairs_file,\n                \'--lfw_dir\', self.dataset_dir,\n                \'--lfw_nrof_folds\', \'2\',\n                \'--lfw_batch_size\', \'1\' ]\n        subprocess.call(argv)\n  \n    def test_training_classifier_squeezenet(self):\n        print(\'test_training_classifier_squeezenet\')\n        argv = [\'python\',\n                \'src/train_softmax.py\',\n                \'--logs_base_dir\', self.tmp_dir,\n                \'--models_base_dir\', self.tmp_dir,\n                \'--data_dir\', self.dataset_dir,\n                \'--model_def\', \'models.squeezenet\',\n                \'--epoch_size\', \'1\',\n                \'--max_nrof_epochs\', \'1\',\n                \'--batch_size\', \'1\',\n                \'--lfw_pairs\', self.lfw_pairs_file,\n                \'--lfw_dir\', self.dataset_dir,\n                \'--lfw_nrof_folds\', \'2\',\n                \'--lfw_batch_size\', \'1\',\n                \'--nrof_preprocess_threads\', \'1\' ]\n        subprocess.call(argv)\n \n    def test_train_tripletloss_inception_resnet_v1(self):\n        print(\'test_train_tripletloss_inception_resnet_v1\')\n        argv = [\'python\',\n                \'src/train_tripletloss.py\',\n                \'--logs_base_dir\', self.tmp_dir,\n                \'--models_base_dir\', self.tmp_dir,\n                \'--data_dir\', self.dataset_dir,\n                \'--model_def\', \'models.inception_resnet_v1\',\n                \'--epoch_size\', \'1\',\n                \'--max_nrof_epochs\', \'1\',\n                \'--batch_size\', \'6\',\n                \'--people_per_batch\', \'2\',\n                \'--images_per_person\', \'3\',\n                \'--lfw_pairs\', self.lfw_pairs_file,\n                \'--lfw_dir\', self.dataset_dir,\n                \'--lfw_nrof_folds\', \'2\' ]\n        subprocess.call(argv)\n  \n    def test_finetune_tripletloss_inception_resnet_v1(self):\n        print(\'test_finetune_tripletloss_inception_resnet_v1\')\n        argv = [\'python\',\n                \'src/train_tripletloss.py\',\n                \'--logs_base_dir\', self.tmp_dir,\n                \'--models_base_dir\', self.tmp_dir,\n                \'--data_dir\', self.dataset_dir,\n                \'--model_def\', \'models.inception_resnet_v1\',\n                \'--pretrained_model\', self.model_file,\n                \'--embedding_size\', \'512\',\n                \'--epoch_size\', \'1\',\n                \'--max_nrof_epochs\', \'1\',\n                \'--batch_size\', \'6\',\n                \'--people_per_batch\', \'2\',\n                \'--images_per_person\', \'3\',\n                \'--lfw_pairs\', self.lfw_pairs_file,\n                \'--lfw_dir\', self.dataset_dir,\n                \'--lfw_nrof_folds\', \'2\' ]\n        subprocess.call(argv)\n  \n    def test_compare(self):\n        print(\'test_compare\')\n        argv = [\'python\',\n                \'src/compare.py\',\n                os.path.join(\'data/\', self.pretrained_model_name),\n                \'data/images/Anthony_Hopkins_0001.jpg\',\n                \'data/images/Anthony_Hopkins_0002.jpg\' ]\n        subprocess.call(argv)\n         \n    def test_validate_on_lfw(self):\n        print(\'test_validate_on_lfw\')\n        align_dataset_if_needed(self)\n        argv = [\'python\',\n                \'src/validate_on_lfw.py\', \n                \'data/lfw_aligned\',\n                self.pretrained_model,\n                \'--lfw_pairs\', \'data/lfw/pairs_small.txt\',\n                \'--lfw_nrof_folds\', \'2\',\n                \'--lfw_batch_size\', \'6\']\n        subprocess.call(argv)\n \n    def test_validate_on_lfw_frozen_graph(self):\n        print(\'test_validate_on_lfw_frozen_graph\')\n        self.pretrained_model = os.path.join(\'data\', self.pretrained_model_name)\n        frozen_model = os.path.join(self.pretrained_model, self.pretrained_model_name+\'.pb\')\n        argv = [\'python\',\n                \'src/validate_on_lfw.py\',\n                self.dataset_dir,\n                frozen_model,\n                \'--lfw_pairs\', self.lfw_pairs_file,\n                \'--lfw_nrof_folds\', \'2\',\n                \'--lfw_batch_size\', \'6\']\n        subprocess.call(argv)\n \n    def test_freeze_graph(self):\n        print(\'test_freeze_graph\')\n        argv = [\'python\',\n                \'src/freeze_graph.py\',\n                self.pretrained_model,\n                self.frozen_graph_filename ]\n        subprocess.call(argv)\n\n# Create a mock dataset with random pixel images\ndef create_mock_dataset(dataset_dir, image_size):\n   \n    nrof_persons = 3\n    nrof_images_per_person = 2\n    np.random.seed(seed=666)\n    os.mkdir(dataset_dir)\n    for i in range(nrof_persons):\n        class_name = \'%04d\' % (i+1)\n        class_dir = os.path.join(dataset_dir, class_name)\n        os.mkdir(class_dir)\n        for j in range(nrof_images_per_person):\n            img_name = \'%04d\' % (j+1)\n            img_path = os.path.join(class_dir, class_name+\'_\'+img_name + \'.png\')\n            img = np.random.uniform(low=0.0, high=255.0, size=(image_size,image_size,3))\n            cv2.imwrite(img_path, img) #@UndefinedVariable\n\n# Create a mock LFW pairs file\ndef create_mock_lfw_pairs(tmp_dir):\n    pairs_filename = os.path.join(tmp_dir, \'pairs_mock.txt\')\n    with open(pairs_filename, \'w\') as f:\n        f.write(\'10 300\\n\')\n        f.write(\'0001 1 2\\n\')\n        f.write(\'0001 1 0002 1\\n\')\n        f.write(\'0002 1 0003 1\\n\')\n        f.write(\'0001 1 0003 1\\n\')\n        f.write(\'0002 1 2\\n\')\n        f.write(\'0001 2 0002 2\\n\')\n        f.write(\'0002 2 0003 2\\n\')\n        f.write(\'0001 2 0003 2\\n\')\n        f.write(\'0003 1 2\\n\')\n        f.write(\'0001 1 0002 2\\n\')\n        f.write(\'0002 1 0003 2\\n\')\n        f.write(\'0001 1 0003 2\\n\')\n    return pairs_filename\n\nif __name__ == ""__main__"":\n    unittest.main()\n    '"
test/triplet_loss_test.py,4,"b'# MIT License\n# \n# Copyright (c) 2016 David Sandberg\n# \n# Permission is hereby granted, free of charge, to any person obtaining a copy\n# of this software and associated documentation files (the ""Software""), to deal\n# in the Software without restriction, including without limitation the rights\n# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n# copies of the Software, and to permit persons to whom the Software is\n# furnished to do so, subject to the following conditions:\n# \n# The above copyright notice and this permission notice shall be included in all\n# copies or substantial portions of the Software.\n# \n# THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n# SOFTWARE.\n\nimport unittest\nimport tensorflow as tf\nimport numpy as np\nimport facenet\n\nclass DemuxEmbeddingsTest(unittest.TestCase):\n  \n    def testDemuxEmbeddings(self):\n        batch_size = 3*12\n        embedding_size = 16\n        alpha = 0.2\n        \n        with tf.Graph().as_default():\n        \n            embeddings = tf.placeholder(tf.float64, shape=(batch_size, embedding_size), name=\'embeddings\')\n            anchor, positive, negative = tf.unstack(tf.reshape(embeddings, [-1,3,embedding_size]), 3, 1)\n            triplet_loss = facenet.triplet_loss(anchor, positive, negative, alpha)\n                \n            sess = tf.Session()\n            with sess.as_default():\n                np.random.seed(seed=666)\n                emb = np.random.uniform(size=(batch_size, embedding_size))\n                tf_triplet_loss = sess.run(triplet_loss, feed_dict={embeddings:emb})\n\n                pos_dist_sqr = np.sum(np.square(emb[0::3,:]-emb[1::3,:]),1)\n                neg_dist_sqr = np.sum(np.square(emb[0::3,:]-emb[2::3,:]),1)\n                np_triplet_loss = np.mean(np.maximum(0.0, pos_dist_sqr - neg_dist_sqr + alpha))\n                \n                np.testing.assert_almost_equal(tf_triplet_loss, np_triplet_loss, decimal=5, err_msg=\'Triplet loss is incorrect\')\n                      \nif __name__ == ""__main__"":\n    unittest.main()\n'"
tmp/__init__.py,0,b'# flake8: noqa\n\n'
tmp/align_dataset.py,0,"b'""""""Performs face alignment and stores face thumbnails in the output directory.""""""\n\n# MIT License\n# \n# Copyright (c) 2016 David Sandberg\n# \n# Permission is hereby granted, free of charge, to any person obtaining a copy\n# of this software and associated documentation files (the ""Software""), to deal\n# in the Software without restriction, including without limitation the rights\n# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n# copies of the Software, and to permit persons to whom the Software is\n# furnished to do so, subject to the following conditions:\n# \n# The above copyright notice and this permission notice shall be included in all\n# copies or substantial portions of the Software.\n# \n# THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n# SOFTWARE.\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom scipy import misc\nimport sys\nimport os\nimport argparse\nimport random\nimport align_dlib  # @UnresolvedImport\nimport facenet\n\ndef main(args):\n    align = align_dlib.AlignDlib(os.path.expanduser(args.dlib_face_predictor))\n    landmarkIndices = align_dlib.AlignDlib.OUTER_EYES_AND_NOSE\n    output_dir = os.path.expanduser(args.output_dir)\n    if not os.path.exists(output_dir):\n        os.makedirs(output_dir)\n    # Store some git revision info in a text file in the log directory\n    src_path,_ = os.path.split(os.path.realpath(__file__))\n    facenet.store_revision_info(src_path, output_dir, \' \'.join(sys.argv))\n    dataset = facenet.get_dataset(args.input_dir)\n    random.shuffle(dataset)\n    # Scale the image such that the face fills the frame when cropped to crop_size\n    scale = float(args.face_size) / args.image_size\n    nrof_images_total = 0\n    nrof_prealigned_images = 0\n    nrof_successfully_aligned = 0\n    for cls in dataset:\n        output_class_dir = os.path.join(output_dir, cls.name)\n        if not os.path.exists(output_class_dir):\n            os.makedirs(output_class_dir)\n        random.shuffle(cls.image_paths)\n        for image_path in cls.image_paths:\n            nrof_images_total += 1\n            filename = os.path.splitext(os.path.split(image_path)[1])[0]\n            output_filename = os.path.join(output_class_dir, filename+\'.png\')\n            if not os.path.exists(output_filename):\n                try:\n                    img = misc.imread(image_path)\n                except (IOError, ValueError, IndexError) as e:\n                    errorMessage = \'{}: {}\'.format(image_path, e)\n                    print(errorMessage)\n                else:\n                    if img.ndim == 2:\n                        img = facenet.to_rgb(img)\n                    if args.use_center_crop:\n                        scaled = misc.imresize(img, args.prealigned_scale, interp=\'bilinear\')\n                        sz1 = scaled.shape[1]/2\n                        sz2 = args.image_size/2\n                        aligned = scaled[(sz1-sz2):(sz1+sz2),(sz1-sz2):(sz1+sz2),:]\n                    else:\n                        aligned = align.align(args.image_size, img, landmarkIndices=landmarkIndices, \n                                              skipMulti=False, scale=scale)\n                    if aligned is not None:\n                        print(image_path)\n                        nrof_successfully_aligned += 1\n                        misc.imsave(output_filename, aligned)\n                    elif args.prealigned_dir:\n                        # Face detection failed. Use center crop from pre-aligned dataset\n                        class_name = os.path.split(output_class_dir)[1]\n                        image_path_without_ext = os.path.join(os.path.expanduser(args.prealigned_dir), \n                                                              class_name, filename)\n                        # Find the extension of the image\n                        exts = (\'jpg\', \'png\')\n                        for ext in exts:\n                            temp_path = image_path_without_ext + \'.\' + ext\n                            image_path = \'\'\n                            if os.path.exists(temp_path):\n                                image_path = temp_path\n                                break\n                        try:\n                            img = misc.imread(image_path)\n                        except (IOError, ValueError, IndexError) as e:\n                            errorMessage = \'{}: {}\'.format(image_path, e)\n                            print(errorMessage)\n                        else:\n                            scaled = misc.imresize(img, args.prealigned_scale, interp=\'bilinear\')\n                            sz1 = scaled.shape[1]/2\n                            sz2 = args.image_size/2\n                            cropped = scaled[(sz1-sz2):(sz1+sz2),(sz1-sz2):(sz1+sz2),:]\n                            print(image_path)\n                            nrof_prealigned_images += 1\n                            misc.imsave(output_filename, cropped)\n                    else:\n                        print(\'Unable to align ""%s""\' % image_path)\n                            \n    print(\'Total number of images: %d\' % nrof_images_total)\n    print(\'Number of successfully aligned images: %d\' % nrof_successfully_aligned)\n    print(\'Number of pre-aligned images: %d\' % nrof_prealigned_images)\n            \n\ndef parse_arguments(argv):\n    parser = argparse.ArgumentParser()\n    \n    parser.add_argument(\'input_dir\', type=str, help=\'Directory with unaligned images.\')\n    parser.add_argument(\'output_dir\', type=str, help=\'Directory with aligned face thumbnails.\')\n    parser.add_argument(\'--dlib_face_predictor\', type=str,\n        help=\'File containing the dlib face predictor.\', default=\'../data/shape_predictor_68_face_landmarks.dat\')\n    parser.add_argument(\'--image_size\', type=int,\n        help=\'Image size (height, width) in pixels.\', default=110)\n    parser.add_argument(\'--face_size\', type=int,\n        help=\'Size of the face thumbnail (height, width) in pixels.\', default=96)\n    parser.add_argument(\'--use_center_crop\', \n        help=\'Use the center crop of the original image after scaling the image using prealigned_scale.\', action=\'store_true\')\n    parser.add_argument(\'--prealigned_dir\', type=str,\n        help=\'Replace image with a pre-aligned version when face detection fails.\', default=\'\')\n    parser.add_argument(\'--prealigned_scale\', type=float,\n        help=\'The amount of scaling to apply to prealigned images before taking the center crop.\', default=0.87)\n    return parser.parse_args(argv)\n\nif __name__ == \'__main__\':\n    main(parse_arguments(sys.argv[1:]))\n'"
tmp/align_dlib.py,0,"b'# Copyright 2015-2016 Carnegie Mellon University\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""Module for dlib-based alignment.""""""\n\n# NOTE: This file has been copied from the openface project.\n#  https://github.com/cmusatyalab/openface/blob/master/openface/align_dlib.py\n\nimport cv2\nimport dlib\nimport numpy as np\n\nTEMPLATE = np.float32([\n    (0.0792396913815, 0.339223741112), (0.0829219487236, 0.456955367943),\n    (0.0967927109165, 0.575648016728), (0.122141515615, 0.691921601066),\n    (0.168687863544, 0.800341263616), (0.239789390707, 0.895732504778),\n    (0.325662452515, 0.977068762493), (0.422318282013, 1.04329000149),\n    (0.531777802068, 1.06080371126), (0.641296298053, 1.03981924107),\n    (0.738105872266, 0.972268833998), (0.824444363295, 0.889624082279),\n    (0.894792677532, 0.792494155836), (0.939395486253, 0.681546643421),\n    (0.96111933829, 0.562238253072), (0.970579841181, 0.441758925744),\n    (0.971193274221, 0.322118743967), (0.163846223133, 0.249151738053),\n    (0.21780354657, 0.204255863861), (0.291299351124, 0.192367318323),\n    (0.367460241458, 0.203582210627), (0.4392945113, 0.233135599851),\n    (0.586445962425, 0.228141644834), (0.660152671635, 0.195923841854),\n    (0.737466449096, 0.182360984545), (0.813236546239, 0.192828009114),\n    (0.8707571886, 0.235293377042), (0.51534533827, 0.31863546193),\n    (0.516221448289, 0.396200446263), (0.517118861835, 0.473797687758),\n    (0.51816430343, 0.553157797772), (0.433701156035, 0.604054457668),\n    (0.475501237769, 0.62076344024), (0.520712933176, 0.634268222208),\n    (0.565874114041, 0.618796581487), (0.607054002672, 0.60157671656),\n    (0.252418718401, 0.331052263829), (0.298663015648, 0.302646354002),\n    (0.355749724218, 0.303020650651), (0.403718978315, 0.33867711083),\n    (0.352507175597, 0.349987615384), (0.296791759886, 0.350478978225),\n    (0.631326076346, 0.334136672344), (0.679073381078, 0.29645404267),\n    (0.73597236153, 0.294721285802), (0.782865376271, 0.321305281656),\n    (0.740312274764, 0.341849376713), (0.68499850091, 0.343734332172),\n    (0.353167761422, 0.746189164237), (0.414587777921, 0.719053835073),\n    (0.477677654595, 0.706835892494), (0.522732900812, 0.717092275768),\n    (0.569832064287, 0.705414478982), (0.635195811927, 0.71565572516),\n    (0.69951672331, 0.739419187253), (0.639447159575, 0.805236879972),\n    (0.576410514055, 0.835436670169), (0.525398405766, 0.841706377792),\n    (0.47641545769, 0.837505914975), (0.41379548902, 0.810045601727),\n    (0.380084785646, 0.749979603086), (0.477955996282, 0.74513234612),\n    (0.523389793327, 0.748924302636), (0.571057789237, 0.74332894691),\n    (0.672409137852, 0.744177032192), (0.572539621444, 0.776609286626),\n    (0.5240106503, 0.783370783245), (0.477561227414, 0.778476346951)])\n\nINV_TEMPLATE = np.float32([\n                            (-0.04099179660567834, -0.008425234314031194, 2.575498465013183),\n                            (0.04062510634554352, -0.009678089746831375, -1.2534351452524177),\n                            (0.0003666902601348179, 0.01810332406086298, -0.32206331976076663)])\n\nTPL_MIN, TPL_MAX = np.min(TEMPLATE, axis=0), np.max(TEMPLATE, axis=0)\nMINMAX_TEMPLATE = (TEMPLATE - TPL_MIN) / (TPL_MAX - TPL_MIN)\n\n\nclass AlignDlib:\n    """"""\n    Use `dlib\'s landmark estimation <http://blog.dlib.net/2014/08/real-time-face-pose-estimation.html>`_ to align faces.\n\n    The alignment preprocess faces for input into a neural network.\n    Faces are resized to the same size (such as 96x96) and transformed\n    to make landmarks (such as the eyes and nose) appear at the same\n    location on every image.\n\n    Normalized landmarks:\n\n    .. image:: ../images/dlib-landmark-mean.png\n    """"""\n\n    #: Landmark indices corresponding to the inner eyes and bottom lip.\n    INNER_EYES_AND_BOTTOM_LIP = [39, 42, 57]\n\n    #: Landmark indices corresponding to the outer eyes and nose.\n    OUTER_EYES_AND_NOSE = [36, 45, 33]\n\n    def __init__(self, facePredictor):\n        """"""\n        Instantiate an \'AlignDlib\' object.\n\n        :param facePredictor: The path to dlib\'s\n        :type facePredictor: str\n        """"""\n        assert facePredictor is not None\n\n        #pylint: disable=no-member\n        self.detector = dlib.get_frontal_face_detector()\n        self.predictor = dlib.shape_predictor(facePredictor)\n\n    def getAllFaceBoundingBoxes(self, rgbImg):\n        """"""\n        Find all face bounding boxes in an image.\n\n        :param rgbImg: RGB image to process. Shape: (height, width, 3)\n        :type rgbImg: numpy.ndarray\n        :return: All face bounding boxes in an image.\n        :rtype: dlib.rectangles\n        """"""\n        assert rgbImg is not None\n\n        try:\n            return self.detector(rgbImg, 1)\n        except Exception as e: #pylint: disable=broad-except\n            print(""Warning: {}"".format(e))\n            # In rare cases, exceptions are thrown.\n            return []\n\n    def getLargestFaceBoundingBox(self, rgbImg, skipMulti=False):\n        """"""\n        Find the largest face bounding box in an image.\n\n        :param rgbImg: RGB image to process. Shape: (height, width, 3)\n        :type rgbImg: numpy.ndarray\n        :param skipMulti: Skip image if more than one face detected.\n        :type skipMulti: bool\n        :return: The largest face bounding box in an image, or None.\n        :rtype: dlib.rectangle\n        """"""\n        assert rgbImg is not None\n\n        faces = self.getAllFaceBoundingBoxes(rgbImg)\n        if (not skipMulti and len(faces) > 0) or len(faces) == 1:\n            return max(faces, key=lambda rect: rect.width() * rect.height())\n        else:\n            return None\n\n    def findLandmarks(self, rgbImg, bb):\n        """"""\n        Find the landmarks of a face.\n\n        :param rgbImg: RGB image to process. Shape: (height, width, 3)\n        :type rgbImg: numpy.ndarray\n        :param bb: Bounding box around the face to find landmarks for.\n        :type bb: dlib.rectangle\n        :return: Detected landmark locations.\n        :rtype: list of (x,y) tuples\n        """"""\n        assert rgbImg is not None\n        assert bb is not None\n\n        points = self.predictor(rgbImg, bb)\n        #return list(map(lambda p: (p.x, p.y), points.parts()))\n        return [(p.x, p.y) for p in points.parts()]\n\n    #pylint: disable=dangerous-default-value\n    def align(self, imgDim, rgbImg, bb=None,\n              landmarks=None, landmarkIndices=INNER_EYES_AND_BOTTOM_LIP,\n              skipMulti=False, scale=1.0):\n        r""""""align(imgDim, rgbImg, bb=None, landmarks=None, landmarkIndices=INNER_EYES_AND_BOTTOM_LIP)\n\n        Transform and align a face in an image.\n\n        :param imgDim: The edge length in pixels of the square the image is resized to.\n        :type imgDim: int\n        :param rgbImg: RGB image to process. Shape: (height, width, 3)\n        :type rgbImg: numpy.ndarray\n        :param bb: Bounding box around the face to align. \\\n                   Defaults to the largest face.\n        :type bb: dlib.rectangle\n        :param landmarks: Detected landmark locations. \\\n                          Landmarks found on `bb` if not provided.\n        :type landmarks: list of (x,y) tuples\n        :param landmarkIndices: The indices to transform to.\n        :type landmarkIndices: list of ints\n        :param skipMulti: Skip image if more than one face detected.\n        :type skipMulti: bool\n        :param scale: Scale image before cropping to the size given by imgDim.\n        :type scale: float\n        :return: The aligned RGB image. Shape: (imgDim, imgDim, 3)\n        :rtype: numpy.ndarray\n        """"""\n        assert imgDim is not None\n        assert rgbImg is not None\n        assert landmarkIndices is not None\n\n        if bb is None:\n            bb = self.getLargestFaceBoundingBox(rgbImg, skipMulti)\n            if bb is None:\n                return\n\n        if landmarks is None:\n            landmarks = self.findLandmarks(rgbImg, bb)\n\n        npLandmarks = np.float32(landmarks)\n        npLandmarkIndices = np.array(landmarkIndices)\n\n        #pylint: disable=maybe-no-member\n        H = cv2.getAffineTransform(npLandmarks[npLandmarkIndices],\n                                   imgDim * MINMAX_TEMPLATE[npLandmarkIndices]*scale + imgDim*(1-scale)/2)\n        thumbnail = cv2.warpAffine(rgbImg, H, (imgDim, imgDim))\n        \n        return thumbnail\n'"
tmp/cacd2000_split_identities.py,0,"b""import shutil\nimport argparse\nimport os\nimport sys\n\ndef main(args):\n    src_path_exp = os.path.expanduser(args.src_path)\n    dst_path_exp = os.path.expanduser(args.dst_path)\n    if not os.path.exists(dst_path_exp):\n        os.makedirs(dst_path_exp)\n    files = os.listdir(src_path_exp)\n    for f in files:\n        file_name = '.'.join(f.split('.')[0:-1])\n        x = file_name.split('_')\n        dir_name = '_'.join(x[1:-1])\n        class_dst_path = os.path.join(dst_path_exp, dir_name)\n        if not os.path.exists(class_dst_path):\n            os.makedirs(class_dst_path)\n        src_file_path = os.path.join(src_path_exp, f)\n        dst_file = os.path.join(class_dst_path, f)\n        print('%s -> %s' % (src_file_path, dst_file))\n        shutil.copyfile(src_file_path, dst_file)\n\ndef parse_arguments(argv):\n    parser = argparse.ArgumentParser()\n    \n    parser.add_argument('src_path', type=str, help='Path to the source directory.')\n    parser.add_argument('dst_path', type=str, help='Path to the destination directory.')\n    return parser.parse_args(argv)\n\nif __name__ == '__main__':\n    main(parse_arguments(sys.argv[1:]))\n"""
tmp/dataset_read_speed.py,0,"b""import facenet\nimport argparse\nimport sys\nimport time\nimport numpy as np\n\ndef main(args):\n\n    dataset = facenet.get_dataset(args.dir)\n    paths, _ = facenet.get_image_paths_and_labels(dataset)\n    t = np.zeros((len(paths)))\n    x = time.time()\n    for i, path in enumerate(paths):\n        start_time = time.time()\n        with open(path, mode='rb') as f:\n            _ = f.read()\n        duration = time.time() - start_time\n        t[i] = duration\n        if i % 1000 == 0 or i==len(paths)-1:\n            print('File %d/%d  Total time: %.2f  Avg: %.3f  Std: %.3f' % (i, len(paths), time.time()-x, np.mean(t[0:i])*1000, np.std(t[0:i])*1000))\n\n\ndef parse_arguments(argv):\n    parser = argparse.ArgumentParser()\n    parser.add_argument('dir', type=str, \n        help='Directory with dataset to test')\n    return parser.parse_args(argv)\n\n\nif __name__ == '__main__':\n    main(parse_arguments(sys.argv[1:]))\n"""
tmp/deepdream.py,30,"b'# boilerplate code\nimport numpy as np\nfrom functools import partial\nimport PIL.Image\n\nimport tensorflow as tf\nimport matplotlib.pyplot as plt\nimport urllib2\nimport os\nimport zipfile\n\ndef main():\n    # download pre-trained model by running the command below in a shell\n    #  wget https://storage.googleapis.com/download.tensorflow.org/models/inception5h.zip && unzip inception5h.zip\n    url = \'https://storage.googleapis.com/download.tensorflow.org/models/inception5h.zip\'\n    data_dir = \'../data/\'\n    model_name = os.path.split(url)[-1]\n    local_zip_file = os.path.join(data_dir, model_name)\n    if not os.path.exists(local_zip_file):\n        # Download\n        model_url = urllib2.urlopen(url)\n        with open(local_zip_file, \'wb\') as output:\n            output.write(model_url.read())\n        # Extract\n        with zipfile.ZipFile(local_zip_file, \'r\') as zip_ref:\n            zip_ref.extractall(data_dir)\n  \n    # start with a gray image with a little noise\n    img_noise = np.random.uniform(size=(224,224,3)) + 100.0\n  \n    model_fn = \'tensorflow_inception_graph.pb\'\n    \n    # creating TensorFlow session and loading the model\n    graph = tf.Graph()\n    sess = tf.InteractiveSession(graph=graph)\n    with tf.gfile.FastGFile(os.path.join(data_dir, model_fn), \'rb\') as f:\n        graph_def = tf.GraphDef()\n        graph_def.ParseFromString(f.read())\n    t_input = tf.placeholder(np.float32, name=\'input\') # define the input tensor\n    imagenet_mean = 117.0\n    t_preprocessed = tf.expand_dims(t_input-imagenet_mean, 0)\n    tf.import_graph_def(graph_def, {\'input\':t_preprocessed})\n    \n    layers = [op.name for op in graph.get_operations() if op.type==\'Conv2D\' and \'import/\' in op.name]\n    feature_nums = [int(graph.get_tensor_by_name(name+\':0\').get_shape()[-1]) for name in layers]\n    \n    print(\'Number of layers\', len(layers))\n    print(\'Total number of feature channels:\', sum(feature_nums))\n  \n  \n    # Helper functions for TF Graph visualization\n    #pylint: disable=unused-variable\n    def strip_consts(graph_def, max_const_size=32):\n        """"""Strip large constant values from graph_def.""""""\n        strip_def = tf.GraphDef()\n        for n0 in graph_def.node:\n            n = strip_def.node.add() #pylint: disable=maybe-no-member\n            n.MergeFrom(n0)\n            if n.op == \'Const\':\n                tensor = n.attr[\'value\'].tensor\n                size = len(tensor.tensor_content)\n                if size > max_const_size:\n                    tensor.tensor_content = ""<stripped %d bytes>""%size\n        return strip_def\n      \n    def rename_nodes(graph_def, rename_func):\n        res_def = tf.GraphDef()\n        for n0 in graph_def.node:\n            n = res_def.node.add() #pylint: disable=maybe-no-member\n            n.MergeFrom(n0)\n            n.name = rename_func(n.name)\n            for i, s in enumerate(n.input):\n                n.input[i] = rename_func(s) if s[0]!=\'^\' else \'^\'+rename_func(s[1:])\n        return res_def\n      \n    def showarray(a):\n        a = np.uint8(np.clip(a, 0, 1)*255)\n        plt.imshow(a)\n        plt.show()\n        \n    def visstd(a, s=0.1):\n        \'\'\'Normalize the image range for visualization\'\'\'\n        return (a-a.mean())/max(a.std(), 1e-4)*s + 0.5\n    \n    def T(layer):\n        \'\'\'Helper for getting layer output tensor\'\'\'\n        return graph.get_tensor_by_name(""import/%s:0""%layer)\n    \n    def render_naive(t_obj, img0=img_noise, iter_n=20, step=1.0):\n        t_score = tf.reduce_mean(t_obj) # defining the optimization objective\n        t_grad = tf.gradients(t_score, t_input)[0] # behold the power of automatic differentiation!\n        \n        img = img0.copy()\n        for _ in range(iter_n):\n            g, _ = sess.run([t_grad, t_score], {t_input:img})\n            # normalizing the gradient, so the same step size should work \n            g /= g.std()+1e-8         # for different layers and networks\n            img += g*step\n        showarray(visstd(img))\n        \n    def tffunc(*argtypes):\n        \'\'\'Helper that transforms TF-graph generating function into a regular one.\n        See ""resize"" function below.\n        \'\'\'\n        placeholders = list(map(tf.placeholder, argtypes))\n        def wrap(f):\n            out = f(*placeholders)\n            def wrapper(*args, **kw):\n                return out.eval(dict(zip(placeholders, args)), session=kw.get(\'session\'))\n            return wrapper\n        return wrap\n    \n    # Helper function that uses TF to resize an image\n    def resize(img, size):\n        img = tf.expand_dims(img, 0)\n        return tf.image.resize_bilinear(img, size)[0,:,:,:]\n    resize = tffunc(np.float32, np.int32)(resize)\n    \n    \n    def calc_grad_tiled(img, t_grad, tile_size=512):\n        \'\'\'Compute the value of tensor t_grad over the image in a tiled way.\n        Random shifts are applied to the image to blur tile boundaries over \n        multiple iterations.\'\'\'\n        sz = tile_size\n        h, w = img.shape[:2]\n        sx, sy = np.random.randint(sz, size=2)\n        img_shift = np.roll(np.roll(img, sx, 1), sy, 0)\n        grad = np.zeros_like(img)\n        for y in range(0, max(h-sz//2, sz),sz):\n            for x in range(0, max(w-sz//2, sz),sz):\n                sub = img_shift[y:y+sz,x:x+sz]\n                g = sess.run(t_grad, {t_input:sub})\n                grad[y:y+sz,x:x+sz] = g\n        return np.roll(np.roll(grad, -sx, 1), -sy, 0)    \n      \n    def render_multiscale(t_obj, img0=img_noise, iter_n=10, step=1.0, octave_n=3, octave_scale=1.4):\n        t_score = tf.reduce_mean(t_obj) # defining the optimization objective\n        t_grad = tf.gradients(t_score, t_input)[0] # behold the power of automatic differentiation!\n        \n        img = img0.copy()\n        for octave in range(octave_n):\n            if octave>0:\n                hw = np.float32(img.shape[:2])*octave_scale\n                img = resize(img, np.int32(hw))\n            for _ in range(iter_n):\n                g = calc_grad_tiled(img, t_grad)\n                # normalizing the gradient, so the same step size should work \n                g /= g.std()+1e-8         # for different layers and networks\n                img += g*step\n            showarray(visstd(img))\n            \n    def lap_split(img):\n        \'\'\'Split the image into lo and hi frequency components\'\'\'\n        with tf.name_scope(\'split\'):\n            lo = tf.nn.conv2d(img, k5x5, [1,2,2,1], \'SAME\')\n            lo2 = tf.nn.conv2d_transpose(lo, k5x5*4, tf.shape(img), [1,2,2,1])\n            hi = img-lo2\n        return lo, hi\n    \n    def lap_split_n(img, n):\n        \'\'\'Build Laplacian pyramid with n splits\'\'\'\n        levels = []\n        for _ in range(n):\n            img, hi = lap_split(img)\n            levels.append(hi)\n        levels.append(img)\n        return levels[::-1]\n    \n    def lap_merge(levels):\n        \'\'\'Merge Laplacian pyramid\'\'\'\n        img = levels[0]\n        for hi in levels[1:]:\n            with tf.name_scope(\'merge\'):\n                img = tf.nn.conv2d_transpose(img, k5x5*4, tf.shape(hi), [1,2,2,1]) + hi\n        return img\n    \n    def normalize_std(img, eps=1e-10):\n        \'\'\'Normalize image by making its standard deviation = 1.0\'\'\'\n        with tf.name_scope(\'normalize\'):\n            std = tf.sqrt(tf.reduce_mean(tf.square(img)))\n            return img/tf.maximum(std, eps)\n    \n    def lap_normalize(img, scale_n=4):\n        \'\'\'Perform the Laplacian pyramid normalization.\'\'\'\n        img = tf.expand_dims(img,0)\n        tlevels = lap_split_n(img, scale_n)\n        tlevels = list(map(normalize_std, tlevels))\n        out = lap_merge(tlevels)\n        return out[0,:,:,:]\n  \n    def render_lapnorm(t_obj, img0=img_noise, visfunc=visstd,\n                       iter_n=10, step=1.0, octave_n=3, octave_scale=1.4, lap_n=4):\n        t_score = tf.reduce_mean(t_obj) # defining the optimization objective\n        t_grad = tf.gradients(t_score, t_input)[0] # behold the power of automatic differentiation!\n        # build the laplacian normalization graph\n        lap_norm_func = tffunc(np.float32)(partial(lap_normalize, scale_n=lap_n))\n    \n        img = img0.copy()\n        for octave in range(octave_n):\n            if octave>0:\n                hw = np.float32(img.shape[:2])*octave_scale\n                img = resize(img, np.int32(hw))\n            for _ in range(iter_n):\n                g = calc_grad_tiled(img, t_grad)\n                g = lap_norm_func(g)\n                img += g*step\n            showarray(visfunc(img))\n  \n    def render_deepdream(t_obj, img0=img_noise,\n                         iter_n=10, step=1.5, octave_n=4, octave_scale=1.4):\n        t_score = tf.reduce_mean(t_obj) # defining the optimization objective\n        t_grad = tf.gradients(t_score, t_input)[0] # behold the power of automatic differentiation!\n    \n        # split the image into a number of octaves\n        img = img0\n        octaves = []\n        for _ in range(octave_n-1):\n            hw = img.shape[:2]\n            lo = resize(img, np.int32(np.float32(hw)/octave_scale))\n            hi = img-resize(lo, hw)\n            img = lo\n            octaves.append(hi)\n        \n        # generate details octave by octave\n        for octave in range(octave_n):\n            if octave>0:\n                hi = octaves[-octave]\n                img = resize(img, hi.shape[:2])+hi\n            for _ in range(iter_n):\n                g = calc_grad_tiled(img, t_grad)\n                img += g*(step / (np.abs(g).mean()+1e-7))\n            showarray(img/255.0)\n  \n    # Picking some internal layer. Note that we use outputs before applying the ReLU nonlinearity\n    # to have non-zero gradients for features with negative initial activations.\n    layer = \'mixed4d_3x3_bottleneck_pre_relu\'\n    channel = 139 # picking some feature channel to visualize\n    render_naive(T(layer)[:,:,:,channel])\n    \n    render_multiscale(T(layer)[:,:,:,channel])\n   \n    k = np.float32([1,4,6,4,1])\n    k = np.outer(k, k)\n    k5x5 = k[:,:,None,None]/k.sum()*np.eye(3, dtype=np.float32)\n     \n    render_lapnorm(T(layer)[:,:,:,channel])\n     \n    render_lapnorm(T(layer)[:,:,:,65])\n     \n    render_lapnorm(T(\'mixed3b_1x1_pre_relu\')[:,:,:,101])\n     \n    render_lapnorm(T(layer)[:,:,:,65]+T(layer)[:,:,:,139], octave_n=4)\n     \n     \n    img0 = PIL.Image.open(\'pilatus800.jpg\')\n    img0 = np.float32(img0)\n    showarray(img0/255.0)\n     \n    render_deepdream(tf.square(T(\'mixed4c\')), img0)\n     \n    render_deepdream(T(layer)[:,:,:,139], img0)\n    \n  \nif __name__ == \'__main__\':\n    main()\n'"
tmp/download_vgg_face_dataset.py,0,"b'""""""Download the VGG face dataset from URLs given by http://www.robots.ox.ac.uk/~vgg/data/vgg_face/vgg_face_dataset.tar.gz\n""""""\n# MIT License\n# \n# Copyright (c) 2016 David Sandberg\n# \n# Permission is hereby granted, free of charge, to any person obtaining a copy\n# of this software and associated documentation files (the ""Software""), to deal\n# in the Software without restriction, including without limitation the rights\n# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n# copies of the Software, and to permit persons to whom the Software is\n# furnished to do so, subject to the following conditions:\n# \n# The above copyright notice and this permission notice shall be included in all\n# copies or substantial portions of the Software.\n# \n# THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n# SOFTWARE.\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom scipy import misc\nimport numpy as np\nfrom skimage import io\nimport sys\nimport argparse\nimport os\nimport socket\nfrom urllib2 import HTTPError, URLError\nfrom httplib import HTTPException\n\ndef main(args):\n    socket.setdefaulttimeout(30)\n    textfile_names = os.listdir(args.dataset_descriptor)\n    for textfile_name in textfile_names:\n        if textfile_name.endswith(\'.txt\'):\n            with open(os.path.join(args.dataset_descriptor, textfile_name), \'rt\') as f:\n                lines = f.readlines()\n            dir_name = textfile_name.split(\'.\')[0]\n            class_path = os.path.join(args.dataset_descriptor, dir_name)\n            if not os.path.exists(class_path):\n                os.makedirs(class_path)\n            for line in lines:\n                x = line.split(\' \')\n                filename = x[0]\n                url = x[1]\n                box = np.rint(np.array(map(float, x[2:6])))  # x1,y1,x2,y2\n                image_path = os.path.join(args.dataset_descriptor, dir_name, filename+\'.\'+args.output_format)\n                error_path = os.path.join(args.dataset_descriptor, dir_name, filename+\'.err\')\n                if not os.path.exists(image_path) and not os.path.exists(error_path):\n                    try:\n                        img = io.imread(url, mode=\'RGB\')\n                    except (HTTPException, HTTPError, URLError, IOError, ValueError, IndexError, OSError) as e:\n                        error_message = \'{}: {}\'.format(url, e)\n                        save_error_message_file(error_path, error_message)\n                    else:\n                        try:\n                            if img.ndim == 2:\n                                img = to_rgb(img)\n                            if img.ndim != 3:\n                                raise ValueError(\'Wrong number of image dimensions\')\n                            hist = np.histogram(img, 255, density=True)\n                            if hist[0][0]>0.9 and hist[0][254]>0.9:\n                                raise ValueError(\'Image is mainly black or white\')\n                            else:\n                                # Crop image according to dataset descriptor\n                                img_cropped = img[int(box[1]):int(box[3]),int(box[0]):int(box[2]),:]\n                                # Scale to 256x256\n                                img_resized = misc.imresize(img_cropped, (args.image_size,args.image_size))\n                                # Save image as .png\n                                misc.imsave(image_path, img_resized)\n                        except ValueError as e:\n                            error_message = \'{}: {}\'.format(url, e)\n                            save_error_message_file(error_path, error_message)\n            \ndef save_error_message_file(filename, error_message):\n    print(error_message)\n    with open(filename, ""w"") as textfile:\n        textfile.write(error_message)\n          \ndef to_rgb(img):\n    w, h = img.shape\n    ret = np.empty((w, h, 3), dtype=np.uint8)\n    ret[:, :, 0] = ret[:, :, 1] = ret[:, :, 2] = img\n    return ret\n\ndef parse_arguments(argv):\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\'dataset_descriptor\', type=str, \n        help=\'Directory containing the text files with the image URLs. Image files will also be placed in this directory.\')\n    parser.add_argument(\'--output_format\', type=str, help=\'Format of the output images\', default=\'png\', choices=[\'png\', \'jpg\'])\n    parser.add_argument(\'--image_size\', type=int,\n        help=\'Image size (height, width) in pixels.\', default=256)\n    return parser.parse_args(argv)\n\nif __name__ == \'__main__\':\n    main(parse_arguments(sys.argv[1:]))\n'"
tmp/funnel_dataset.py,0,"b'""""""Performs face alignment and stores face thumbnails in the output directory.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom scipy import misc\nimport sys\nimport os\nimport argparse\nimport facenet\nimport subprocess\nfrom contextlib import contextmanager\nimport tempfile\nimport shutil\nimport numpy as np\n\n@contextmanager\ndef TemporaryDirectory():\n    name = tempfile.mkdtemp()\n    try:\n        yield name\n    finally:\n        shutil.rmtree(name)\n\n\ndef main(args):\n    funnel_cmd = \'funnelReal\'\n    funnel_model = \'people.train\'\n\n    output_dir = os.path.expanduser(args.output_dir)\n    if not os.path.exists(output_dir):\n        os.makedirs(output_dir)\n    # Store some git revision info in a text file in the output directory\n    src_path,_ = os.path.split(os.path.realpath(__file__))\n    facenet.store_revision_info(src_path, output_dir, \' \'.join(sys.argv))\n    dataset = facenet.get_dataset(args.input_dir)\n    np.random.shuffle(dataset)\n    # Scale the image such that the face fills the frame when cropped to crop_size\n    #scale = float(args.face_size) / args.image_size\n    with TemporaryDirectory() as tmp_dir:\n        for cls in dataset:\n            output_class_dir = os.path.join(output_dir, cls.name)\n            tmp_output_class_dir = os.path.join(tmp_dir, cls.name)\n            if not os.path.exists(output_class_dir) and not os.path.exists(tmp_output_class_dir):\n                print(\'Aligning class %s:\' % cls.name)\n                tmp_filenames = []\n                if not os.path.exists(tmp_output_class_dir):\n                    os.makedirs(tmp_output_class_dir)\n                input_list_filename = os.path.join(tmp_dir, \'input_list.txt\')\n                output_list_filename = os.path.join(tmp_dir, \'output_list.txt\')\n                input_file = open(input_list_filename, \'w\')\n                output_file = open(output_list_filename,\'w\')\n                for image_path in cls.image_paths:\n                    filename = os.path.split(image_path)[1]\n                    input_file.write(image_path+\'\\n\')\n                    output_filename = os.path.join(tmp_output_class_dir, filename)\n                    output_file.write(output_filename+\'\\n\')\n                    tmp_filenames.append(output_filename)\n                input_file.close()\n                output_file.close()\n                cmd = args.funnel_dir+funnel_cmd + \' \' + input_list_filename + \' \' + args.funnel_dir+funnel_model + \' \' + output_list_filename\n                subprocess.call(cmd, shell=True)\n                \n                # Resize and crop images\n                if not os.path.exists(output_class_dir):\n                    os.makedirs(output_class_dir)\n                scale = 1.0\n                for tmp_filename in tmp_filenames:\n                    img = misc.imread(tmp_filename)\n                    img_scale = misc.imresize(img, scale)\n                    sz1 = img.shape[1]/2\n                    sz2 = args.image_size/2\n                    img_crop = img_scale[int(sz1-sz2):int(sz1+sz2),int(sz1-sz2):int(sz1+sz2),:]\n                    filename = os.path.splitext(os.path.split(tmp_filename)[1])[0]\n                    output_filename = os.path.join(output_class_dir, filename+\'.png\')\n                    print(\'Saving image %s\' % output_filename)\n                    misc.imsave(output_filename, img_crop)\n                    \n                # Remove tmp directory with images\n                shutil.rmtree(tmp_output_class_dir)\n                \ndef parse_arguments(argv):\n    parser = argparse.ArgumentParser()\n    \n    parser.add_argument(\'input_dir\', type=str, help=\'Directory with unaligned images.\')\n    parser.add_argument(\'output_dir\', type=str, help=\'Directory with aligned face thumbnails.\')\n    parser.add_argument(\'funnel_dir\', type=str, help=\'Directory containing the funnelReal binary and the people.train model file\')\n    parser.add_argument(\'--image_size\', type=int,\n        help=\'Image size (height, width) in pixels.\', default=110)\n    parser.add_argument(\'--face_size\', type=int,\n        help=\'Size of the face thumbnail (height, width) in pixels.\', default=96)\n    return parser.parse_args(argv)\n\nif __name__ == \'__main__\':\n    main(parse_arguments(sys.argv[1:]))\n'"
tmp/mnist_center_loss.py,58,"b'# Copyright 2015 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n""""""Simple, end-to-end, LeNet-5-like convolutional MNIST model example.\nThis should achieve a test error of 0.7%. Please keep this model as simple and\nlinear as possible, it is meant as a tutorial for simple convolutional models.\nRun with --self_test on the command line to execute a short self-test.\n""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport gzip\nimport os\nimport sys\nimport time\n\nfrom six.moves import urllib  # @UnresolvedImport\nimport tensorflow as tf\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom tensorflow.python.ops import control_flow_ops\nimport facenet\nfrom six.moves import xrange\n\nSOURCE_URL = \'http://yann.lecun.com/exdb/mnist/\'\nWORK_DIRECTORY = \'data\'\nIMAGE_SIZE = 28\nNUM_CHANNELS = 1\nPIXEL_DEPTH = 255\nNUM_LABELS = 10\nVALIDATION_SIZE = 5000  # Size of the validation set.\nSEED = 66478  # Set to None for random seed.\nBATCH_SIZE = 64\nNUM_EPOCHS = 10\nEVAL_BATCH_SIZE = 64\nEVAL_FREQUENCY = 100  # Number of steps between evaluations.\n\n\ntf.app.flags.DEFINE_boolean(""self_test"", False, ""True if running a self test."")\ntf.app.flags.DEFINE_boolean(\'use_fp16\', False,\n                            ""Use half floats instead of full floats if True."")\nFLAGS = tf.app.flags.FLAGS\n\n\ndef data_type():\n    """"""Return the type of the activations, weights, and placeholder variables.""""""\n    if FLAGS.use_fp16:\n        return tf.float16\n    else:\n        return tf.float32\n\n\ndef maybe_download(filename):\n    """"""Download the data from Yann\'s website, unless it\'s already here.""""""\n    if not tf.gfile.Exists(WORK_DIRECTORY):\n        tf.gfile.MakeDirs(WORK_DIRECTORY)\n    filepath = os.path.join(WORK_DIRECTORY, filename)\n    if not tf.gfile.Exists(filepath):\n        filepath, _ = urllib.request.urlretrieve(SOURCE_URL + filename, filepath)\n        with tf.gfile.GFile(filepath) as f:\n            size = f.size()\n        print(\'Successfully downloaded\', filename, size, \'bytes.\')\n    return filepath\n\n\ndef extract_data(filename, num_images):\n    """"""Extract the images into a 4D tensor [image index, y, x, channels].\n    Values are rescaled from [0, 255] down to [-0.5, 0.5].\n    """"""\n    print(\'Extracting\', filename)\n    with gzip.open(filename) as bytestream:\n        bytestream.read(16)\n        buf = bytestream.read(IMAGE_SIZE * IMAGE_SIZE * num_images * NUM_CHANNELS)\n        data = np.frombuffer(buf, dtype=np.uint8).astype(np.float32)\n        data = (data - (PIXEL_DEPTH / 2.0)) / PIXEL_DEPTH\n        data = data.reshape(num_images, IMAGE_SIZE, IMAGE_SIZE, NUM_CHANNELS)\n        return data\n\n\ndef extract_labels(filename, num_images):\n    """"""Extract the labels into a vector of int64 label IDs.""""""\n    print(\'Extracting\', filename)\n    with gzip.open(filename) as bytestream:\n        bytestream.read(8)\n        buf = bytestream.read(1 * num_images)\n        labels = np.frombuffer(buf, dtype=np.uint8).astype(np.int64)\n    return labels\n\n\ndef fake_data(num_images):\n    """"""Generate a fake dataset that matches the dimensions of MNIST.""""""\n    data = np.ndarray(\n        shape=(num_images, IMAGE_SIZE, IMAGE_SIZE, NUM_CHANNELS),\n        dtype=np.float32)\n    labels = np.zeros(shape=(num_images,), dtype=np.int64)\n    for image in range(num_images):\n        label = image % 2\n        data[image, :, :, 0] = label - 0.5\n        labels[image] = label\n    return data, labels\n\n\ndef error_rate(predictions, labels):\n    """"""Return the error rate based on dense predictions and sparse labels.""""""\n    return 100.0 - (\n        100.0 *\n        np.sum(np.argmax(predictions, 1) == labels) /\n        predictions.shape[0])\n\n\ndef main(argv=None):  # pylint: disable=unused-argument\n    if FLAGS.self_test:\n        print(\'Running self-test.\')\n        train_data, train_labels = fake_data(256)\n        validation_data, validation_labels = fake_data(EVAL_BATCH_SIZE)\n        test_data, test_labels = fake_data(EVAL_BATCH_SIZE)\n        num_epochs = 1\n    else:\n        # Get the data.\n        train_data_filename = maybe_download(\'train-images-idx3-ubyte.gz\')\n        train_labels_filename = maybe_download(\'train-labels-idx1-ubyte.gz\')\n        test_data_filename = maybe_download(\'t10k-images-idx3-ubyte.gz\')\n        test_labels_filename = maybe_download(\'t10k-labels-idx1-ubyte.gz\')\n    \n        # Extract it into numpy arrays.\n        train_data = extract_data(train_data_filename, 60000)\n        train_labels = extract_labels(train_labels_filename, 60000)\n        test_data = extract_data(test_data_filename, 10000)\n        test_labels = extract_labels(test_labels_filename, 10000)\n    \n        # Generate a validation set.\n        validation_data = train_data[:VALIDATION_SIZE, ...]\n        validation_labels = train_labels[:VALIDATION_SIZE]\n        train_data = train_data[VALIDATION_SIZE:, ...]\n        train_labels = train_labels[VALIDATION_SIZE:]\n        num_epochs = NUM_EPOCHS\n    train_size = train_labels.shape[0]\n\n    # This is where training samples and labels are fed to the graph.\n    # These placeholder nodes will be fed a batch of training data at each\n    # training step using the {feed_dict} argument to the Run() call below.\n    train_data_node = tf.placeholder(\n        data_type(),\n        shape=(BATCH_SIZE, IMAGE_SIZE, IMAGE_SIZE, NUM_CHANNELS))\n    train_labels_node = tf.placeholder(tf.int64, shape=(BATCH_SIZE,))\n    eval_data = tf.placeholder(\n        data_type(),\n        shape=(EVAL_BATCH_SIZE, IMAGE_SIZE, IMAGE_SIZE, NUM_CHANNELS))\n\n    # The variables below hold all the trainable weights. They are passed an\n    # initial value which will be assigned when we call:\n    # {tf.global_variables_initializer().run()}\n    conv1_weights = tf.Variable(\n        tf.truncated_normal([5, 5, NUM_CHANNELS, 32],  # 5x5 filter, depth 32.\n                            stddev=0.1,\n                            seed=SEED, dtype=data_type()))\n    conv1_biases = tf.Variable(tf.zeros([32], dtype=data_type()))\n    conv2_weights = tf.Variable(tf.truncated_normal(\n        [5, 5, 32, 64], stddev=0.1,\n        seed=SEED, dtype=data_type()))\n    conv2_biases = tf.Variable(tf.constant(0.1, shape=[64], dtype=data_type()))\n    fc1_weights = tf.Variable(  # fully connected, depth 512.\n        tf.truncated_normal([IMAGE_SIZE // 4 * IMAGE_SIZE // 4 * 64, 512],\n                            stddev=0.1,\n                            seed=SEED,\n                            dtype=data_type()))\n    fc1_biases = tf.Variable(tf.constant(0.1, shape=[512], dtype=data_type()))\n    fc1p_weights = tf.Variable(  # fully connected, depth 512.\n        tf.truncated_normal([512, 2],\n                            stddev=0.1,\n                            seed=SEED,\n                            dtype=data_type()))\n    fc1p_biases = tf.Variable(tf.constant(0.1, shape=[2], dtype=data_type()))\n    fc2_weights = tf.Variable(tf.truncated_normal([2, NUM_LABELS],\n                                                  stddev=0.1,\n                                                  seed=SEED,\n                                                  dtype=data_type()))\n    fc2_biases = tf.Variable(tf.constant(\n        0.1, shape=[NUM_LABELS], dtype=data_type()))\n    \n    def batch_norm(x, phase_train):  #pylint: disable=unused-variable\n        """"""\n        Batch normalization on convolutional maps.\n        Args:\n            x:           Tensor, 4D BHWD input maps\n            n_out:       integer, depth of input maps\n            phase_train: boolean tf.Variable, true indicates training phase\n            scope:       string, variable scope\n            affn:      whether to affn-transform outputs\n        Return:\n            normed:      batch-normalized maps\n        Ref: http://stackoverflow.com/questions/33949786/how-could-i-use-batch-normalization-in-tensorflow/33950177\n        """"""\n        name = \'batch_norm\'\n        with tf.variable_scope(name):\n            phase_train = tf.convert_to_tensor(phase_train, dtype=tf.bool)\n            n_out = int(x.get_shape()[-1])\n            beta = tf.Variable(tf.constant(0.0, shape=[n_out], dtype=x.dtype),\n                               name=name+\'/beta\', trainable=True, dtype=x.dtype)\n            gamma = tf.Variable(tf.constant(1.0, shape=[n_out], dtype=x.dtype),\n                                name=name+\'/gamma\', trainable=True, dtype=x.dtype)\n          \n            batch_mean, batch_var = tf.nn.moments(x, [0], name=\'moments\')\n            ema = tf.train.ExponentialMovingAverage(decay=0.9)\n            def mean_var_with_update():\n                ema_apply_op = ema.apply([batch_mean, batch_var])\n                with tf.control_dependencies([ema_apply_op]):\n                    return tf.identity(batch_mean), tf.identity(batch_var)\n            mean, var = control_flow_ops.cond(phase_train,\n                                              mean_var_with_update,\n                                              lambda: (ema.average(batch_mean), ema.average(batch_var)))\n            normed = tf.nn.batch_normalization(x, mean, var, beta, gamma, 1e-3)\n        return normed\n    \n\n    # We will replicate the model structure for the training subgraph, as well\n    # as the evaluation subgraphs, while sharing the trainable parameters.\n    def model(data, train=False):\n        """"""The Model definition.""""""\n        # 2D convolution, with \'SAME\' padding (i.e. the output feature map has\n        # the same size as the input). Note that {strides} is a 4D array whose\n        # shape matches the data layout: [image index, y, x, depth].\n        conv = tf.nn.conv2d(data,\n                            conv1_weights,\n                            strides=[1, 1, 1, 1],\n                            padding=\'SAME\')\n        # Bias and rectified linear non-linearity.\n        relu = tf.nn.relu(tf.nn.bias_add(conv, conv1_biases))\n        # Max pooling. The kernel size spec {ksize} also follows the layout of\n        # the data. Here we have a pooling window of 2, and a stride of 2.\n        pool = tf.nn.max_pool(relu,\n                              ksize=[1, 2, 2, 1],\n                              strides=[1, 2, 2, 1],\n                              padding=\'SAME\')\n        conv = tf.nn.conv2d(pool,\n                            conv2_weights,\n                            strides=[1, 1, 1, 1],\n                            padding=\'SAME\')\n        relu = tf.nn.relu(tf.nn.bias_add(conv, conv2_biases))\n        pool = tf.nn.max_pool(relu,\n                              ksize=[1, 2, 2, 1],\n                              strides=[1, 2, 2, 1],\n                              padding=\'SAME\')\n        # Reshape the feature map cuboid into a 2D matrix to feed it to the\n        # fully connected layers.\n        pool_shape = pool.get_shape().as_list() #pylint: disable=no-member\n        reshape = tf.reshape(\n            pool,\n            [pool_shape[0], pool_shape[1] * pool_shape[2] * pool_shape[3]])\n        # Fully connected layer. Note that the \'+\' operation automatically\n        # broadcasts the biases.\n        hidden = tf.nn.relu(tf.matmul(reshape, fc1_weights) + fc1_biases)\n        # Add a 50% dropout during training only. Dropout also scales\n        # activations such that no rescaling is needed at evaluation time.\n        if train:\n            hidden = tf.nn.dropout(hidden, 0.5, seed=SEED)\n\n        hidden = tf.matmul(hidden, fc1p_weights) + fc1p_biases\n\n        return tf.nn.relu(tf.matmul(hidden, fc2_weights) + fc2_biases), hidden\n\n    # Training computation: logits + cross-entropy loss.\n    logits, hidden = model(train_data_node, True)\n    #logits = batch_norm(logits, True)\n    xent_loss = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(\n        logits, train_labels_node))\n    beta = 1e-3\n    #center_loss, update_centers = center_loss_op(hidden, train_labels_node)\n    center_loss, _ = facenet.center_loss(hidden, train_labels_node, 0.95, NUM_LABELS)\n    loss = xent_loss + beta * center_loss\n  \n    # L2 regularization for the fully connected parameters.\n    regularizers = (tf.nn.l2_loss(fc1_weights) + tf.nn.l2_loss(fc1_biases) +\n                    tf.nn.l2_loss(fc2_weights) + tf.nn.l2_loss(fc2_biases))\n    # Add the regularization term to the loss.\n    loss += 5e-4 * regularizers\n  \n    # Optimizer: set up a variable that\'s incremented once per batch and\n    # controls the learning rate decay.\n    batch = tf.Variable(0, dtype=data_type())\n    # Decay once per epoch, using an exponential schedule starting at 0.01.\n    learning_rate = tf.train.exponential_decay(\n        0.01,                # Base learning rate.\n        batch * BATCH_SIZE,  # Current index into the dataset.\n        train_size,          # Decay step.\n        0.95,                # Decay rate.\n        staircase=True)\n    # Use simple momentum for the optimization.\n    optimizer = tf.train.MomentumOptimizer(learning_rate,\n                                           0.9).minimize(loss,\n                                                         global_step=batch)\n  \n    # Predictions for the current training minibatch.\n    train_prediction = tf.nn.softmax(logits)\n  \n    # Predictions for the test and validation, which we\'ll compute less often.\n    eval_logits, eval_embeddings = model(eval_data)\n    eval_prediction = tf.nn.softmax(eval_logits)\n    \n    # Small utility function to evaluate a dataset by feeding batches of data to\n    # {eval_data} and pulling the results from {eval_predictions}.\n    # Saves memory and enables this to run on smaller GPUs.\n    def eval_in_batches(data, sess):\n        """"""Get all predictions for a dataset by running it in small batches.""""""\n        size = data.shape[0]\n        if size < EVAL_BATCH_SIZE:\n            raise ValueError(""batch size for evals larger than dataset: %d"" % size)\n        predictions = np.ndarray(shape=(size, NUM_LABELS), dtype=np.float32)\n        for begin in xrange(0, size, EVAL_BATCH_SIZE):\n            end = begin + EVAL_BATCH_SIZE\n            if end <= size:\n                predictions[begin:end, :] = sess.run(\n                    eval_prediction,\n                    feed_dict={eval_data: data[begin:end, ...]})\n            else:\n                batch_predictions = sess.run(\n                    eval_prediction,\n                    feed_dict={eval_data: data[-EVAL_BATCH_SIZE:, ...]})\n                predictions[begin:, :] = batch_predictions[begin - size:, :]\n        return predictions\n  \n    def calculate_embeddings(data, sess):\n        """"""Get all predictions for a dataset by running it in small batches.""""""\n        size = data.shape[0]\n        if size < EVAL_BATCH_SIZE:\n            raise ValueError(""batch size for evals larger than dataset: %d"" % size)\n        predictions = np.ndarray(shape=(size, 2), dtype=np.float32)\n        for begin in xrange(0, size, EVAL_BATCH_SIZE):\n            end = begin + EVAL_BATCH_SIZE\n            if end <= size:\n                predictions[begin:end, :] = sess.run(\n                    eval_embeddings,\n                    feed_dict={eval_data: data[begin:end, ...]})\n            else:\n                batch_predictions = sess.run(\n                    eval_embeddings,\n                    feed_dict={eval_data: data[-EVAL_BATCH_SIZE:, ...]})\n                predictions[begin:, :] = batch_predictions[begin - size:, :]\n        return predictions\n\n    # Create a local session to run the training.\n    start_time = time.time()\n    with tf.Session() as sess:\n        # Run all the initializers to prepare the trainable parameters.\n        tf.global_variables_initializer().run() #pylint: disable=no-member\n        print(\'Initialized!\')\n        # Loop through training steps.\n        for step in xrange(int(num_epochs * train_size) // BATCH_SIZE):\n            # Compute the offset of the current minibatch in the data.\n            # Note that we could use better randomization across epochs.\n            offset = (step * BATCH_SIZE) % (train_size - BATCH_SIZE)\n            batch_data = train_data[offset:(offset + BATCH_SIZE), ...]\n            batch_labels = train_labels[offset:(offset + BATCH_SIZE)]\n            # This dictionary maps the batch data (as a numpy array) to the\n            # node in the graph it should be fed to.\n            feed_dict = {train_data_node: batch_data,\n                         train_labels_node: batch_labels}\n            # Run the graph and fetch some of the nodes.\n            #_, l, lr, predictions = sess.run([optimizer, loss, learning_rate, train_prediction], feed_dict=feed_dict)\n            _, cl, l, lr, predictions = sess.run([optimizer, center_loss, loss, learning_rate, train_prediction], feed_dict=feed_dict)\n            if step % EVAL_FREQUENCY == 0:\n                elapsed_time = time.time() - start_time\n                start_time = time.time()\n                print(\'Step %d (epoch %.2f), %.1f ms\' %\n                      (step, float(step) * BATCH_SIZE / train_size,\n                       1000 * elapsed_time / EVAL_FREQUENCY))\n                print(\'Minibatch loss: %.3f  %.3f, learning rate: %.6f\' % (l, cl*beta, lr))\n                print(\'Minibatch error: %.1f%%\' % error_rate(predictions, batch_labels))\n                print(\'Validation error: %.1f%%\' % error_rate(\n                    eval_in_batches(validation_data, sess), validation_labels))\n                sys.stdout.flush()\n        # Finally print the result!\n        test_error = error_rate(eval_in_batches(test_data, sess), test_labels)\n        print(\'Test error: %.1f%%\' % test_error)\n        if FLAGS.self_test:\n            print(\'test_error\', test_error)\n            assert test_error == 0.0, \'expected 0.0 test_error, got %.2f\' % (\n                test_error,)\n            \n        train_embeddings = calculate_embeddings(train_data, sess)\n        \n        color_list = [\'b\', \'g\', \'r\', \'c\', \'m\', \'y\', \'k\', \'b\', \'g\', \'r\', \'c\' ]\n        plt.figure(1)\n        for n in range(0,10):\n            idx = np.where(train_labels[0:10000]==n)\n            plt.plot(train_embeddings[idx,0], train_embeddings[idx,1], color_list[n]+\'.\')\n        plt.show()\n\n\nif __name__ == \'__main__\':\n    tf.app.run()\n'"
tmp/mnist_noise_labels.py,51,"b'# Copyright 2015 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n""""""Simple, end-to-end, LeNet-5-like convolutional MNIST model example.\nThis should achieve a test error of 0.7%. Please keep this model as simple and\nlinear as possible, it is meant as a tutorial for simple convolutional models.\nRun with --self_test on the command line to execute a short self-test.\n""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport gzip\nimport os\nimport sys\nimport time\n\nfrom six.moves import urllib  # @UnresolvedImport\nimport tensorflow as tf\nimport numpy as np\nfrom six.moves import xrange\n\nSOURCE_URL = \'http://yann.lecun.com/exdb/mnist/\'\nWORK_DIRECTORY = \'data\'\nIMAGE_SIZE = 28\nNUM_CHANNELS = 1\nPIXEL_DEPTH = 255\nNUM_LABELS = 10\nVALIDATION_SIZE = 5000  # Size of the validation set.\nSEED = 66478  # Set to None for random seed.\nBATCH_SIZE = 64\nNUM_EPOCHS = 10\nEVAL_BATCH_SIZE = 64\nEVAL_FREQUENCY = 100  # Number of steps between evaluations.\nNOISE_FACTOR = 0.2\nBETA = 0.8\n\n\ntf.app.flags.DEFINE_boolean(""self_test"", False, ""True if running a self test."")\ntf.app.flags.DEFINE_boolean(\'use_fp16\', False,\n                            ""Use half floats instead of full floats if True."")\nFLAGS = tf.app.flags.FLAGS\n\n\ndef data_type():\n    """"""Return the type of the activations, weights, and placeholder variables.""""""\n    if FLAGS.use_fp16:\n        return tf.float16\n    else:\n        return tf.float32\n\n\ndef maybe_download(filename):\n    """"""Download the data from Yann\'s website, unless it\'s already here.""""""\n    if not tf.gfile.Exists(WORK_DIRECTORY):\n        tf.gfile.MakeDirs(WORK_DIRECTORY)\n    filepath = os.path.join(WORK_DIRECTORY, filename)\n    if not tf.gfile.Exists(filepath):\n        filepath, _ = urllib.request.urlretrieve(SOURCE_URL + filename, filepath)\n        with tf.gfile.GFile(filepath) as f:\n            size = f.size()\n        print(\'Successfully downloaded\', filename, size, \'bytes.\')\n    return filepath\n\n\ndef extract_data(filename, num_images):\n    """"""Extract the images into a 4D tensor [image index, y, x, channels].\n    Values are rescaled from [0, 255] down to [-0.5, 0.5].\n    """"""\n    print(\'Extracting\', filename)\n    with gzip.open(filename) as bytestream:\n        bytestream.read(16)\n        buf = bytestream.read(IMAGE_SIZE * IMAGE_SIZE * num_images * NUM_CHANNELS)\n        data = np.frombuffer(buf, dtype=np.uint8).astype(np.float32)\n        data = (data - (PIXEL_DEPTH / 2.0)) / PIXEL_DEPTH\n        data = data.reshape(num_images, IMAGE_SIZE, IMAGE_SIZE, NUM_CHANNELS)\n        return data\n\n\ndef extract_labels(filename, num_images):\n    """"""Extract the labels into a vector of int64 label IDs.""""""\n    print(\'Extracting\', filename)\n    with gzip.open(filename) as bytestream:\n        bytestream.read(8)\n        buf = bytestream.read(1 * num_images)\n        labels = np.frombuffer(buf, dtype=np.uint8).astype(np.int64)\n    return labels\n\n\ndef fake_data(num_images):\n    """"""Generate a fake dataset that matches the dimensions of MNIST.""""""\n    data = np.ndarray(\n        shape=(num_images, IMAGE_SIZE, IMAGE_SIZE, NUM_CHANNELS),\n        dtype=np.float32)\n    labels = np.zeros(shape=(num_images,), dtype=np.int64)\n    for image in range(num_images):\n        label = image % 2\n        data[image, :, :, 0] = label - 0.5\n        labels[image] = label\n    return data, labels\n\n\ndef error_rate(predictions, labels):\n    """"""Return the error rate based on dense predictions and sparse labels.""""""\n    return 100.0 - (\n        100.0 *\n        np.sum(np.argmax(predictions, 1) == labels) /\n        predictions.shape[0])\n\n\ndef main(argv=None):  # pylint: disable=unused-argument\n    if FLAGS.self_test:\n        print(\'Running self-test.\')\n        train_data, train_labels = fake_data(256)\n        validation_data, validation_labels = fake_data(EVAL_BATCH_SIZE)\n        test_data, test_labels = fake_data(EVAL_BATCH_SIZE)\n        num_epochs = 1\n    else:\n        # Get the data.\n        train_data_filename = maybe_download(\'train-images-idx3-ubyte.gz\')\n        train_labels_filename = maybe_download(\'train-labels-idx1-ubyte.gz\')\n        test_data_filename = maybe_download(\'t10k-images-idx3-ubyte.gz\')\n        test_labels_filename = maybe_download(\'t10k-labels-idx1-ubyte.gz\')\n    \n        # Extract it into numpy arrays.\n        train_data = extract_data(train_data_filename, 60000)\n        train_labels = extract_labels(train_labels_filename, 60000)\n        test_data = extract_data(test_data_filename, 10000)\n        test_labels = extract_labels(test_labels_filename, 10000)\n    \n        # Generate a validation set.\n        validation_data = train_data[:VALIDATION_SIZE, ...]\n        validation_labels = train_labels[:VALIDATION_SIZE]\n        train_data = train_data[VALIDATION_SIZE:, ...]\n        train_labels = train_labels[VALIDATION_SIZE:]\n        nrof_training_examples = train_labels.shape[0]\n        nrof_changed_labels = int(nrof_training_examples*NOISE_FACTOR)\n        shuf = np.arange(0,nrof_training_examples)\n        np.random.shuffle(shuf)\n        change_idx = shuf[0:nrof_changed_labels]\n        train_labels[change_idx] = (train_labels[change_idx] + np.random.randint(1,9,size=(nrof_changed_labels,))) % NUM_LABELS\n        num_epochs = NUM_EPOCHS\n    train_size = train_labels.shape[0]\n\n    # This is where training samples and labels are fed to the graph.\n    # These placeholder nodes will be fed a batch of training data at each\n    # training step using the {feed_dict} argument to the Run() call below.\n    train_data_node = tf.placeholder(\n        data_type(),\n        shape=(BATCH_SIZE, IMAGE_SIZE, IMAGE_SIZE, NUM_CHANNELS))\n    train_labels_node = tf.placeholder(tf.int64, shape=(BATCH_SIZE,))\n    eval_data = tf.placeholder(\n        data_type(),\n        shape=(EVAL_BATCH_SIZE, IMAGE_SIZE, IMAGE_SIZE, NUM_CHANNELS))\n\n    # The variables below hold all the trainable weights. They are passed an\n    # initial value which will be assigned when we call:\n    # {tf.global_variables_initializer().run()}\n    conv1_weights = tf.Variable(\n        tf.truncated_normal([5, 5, NUM_CHANNELS, 32],  # 5x5 filter, depth 32.\n                            stddev=0.1,\n                            seed=SEED, dtype=data_type()))\n    conv1_biases = tf.Variable(tf.zeros([32], dtype=data_type()))\n    conv2_weights = tf.Variable(tf.truncated_normal(\n        [5, 5, 32, 64], stddev=0.1,\n        seed=SEED, dtype=data_type()))\n    conv2_biases = tf.Variable(tf.constant(0.1, shape=[64], dtype=data_type()))\n    fc1_weights = tf.Variable(  # fully connected, depth 512.\n        tf.truncated_normal([IMAGE_SIZE // 4 * IMAGE_SIZE // 4 * 64, 512],\n                            stddev=0.1,\n                            seed=SEED,\n                            dtype=data_type()))\n    fc1_biases = tf.Variable(tf.constant(0.1, shape=[512], dtype=data_type()))\n    fc2_weights = tf.Variable(tf.truncated_normal([512, NUM_LABELS],\n                                                  stddev=0.1,\n                                                  seed=SEED,\n                                                  dtype=data_type()))\n    fc2_biases = tf.Variable(tf.constant(\n        0.1, shape=[NUM_LABELS], dtype=data_type()))\n\n    # We will replicate the model structure for the training subgraph, as well\n    # as the evaluation subgraphs, while sharing the trainable parameters.\n    def model(data, train=False):\n        """"""The Model definition.""""""\n        # 2D convolution, with \'SAME\' padding (i.e. the output feature map has\n        # the same size as the input). Note that {strides} is a 4D array whose\n        # shape matches the data layout: [image index, y, x, depth].\n        conv = tf.nn.conv2d(data,\n                            conv1_weights,\n                            strides=[1, 1, 1, 1],\n                            padding=\'SAME\')\n        # Bias and rectified linear non-linearity.\n        relu = tf.nn.relu(tf.nn.bias_add(conv, conv1_biases))\n        # Max pooling. The kernel size spec {ksize} also follows the layout of\n        # the data. Here we have a pooling window of 2, and a stride of 2.\n        pool = tf.nn.max_pool(relu,\n                              ksize=[1, 2, 2, 1],\n                              strides=[1, 2, 2, 1],\n                              padding=\'SAME\')\n        conv = tf.nn.conv2d(pool,\n                            conv2_weights,\n                            strides=[1, 1, 1, 1],\n                            padding=\'SAME\')\n        relu = tf.nn.relu(tf.nn.bias_add(conv, conv2_biases))\n        pool = tf.nn.max_pool(relu,\n                              ksize=[1, 2, 2, 1],\n                              strides=[1, 2, 2, 1],\n                              padding=\'SAME\')\n        # Reshape the feature map cuboid into a 2D matrix to feed it to the\n        # fully connected layers.\n        pool_shape = pool.get_shape().as_list() #pylint: disable=no-member\n        reshape = tf.reshape(\n            pool,\n            [pool_shape[0], pool_shape[1] * pool_shape[2] * pool_shape[3]])\n        # Fully connected layer. Note that the \'+\' operation automatically\n        # broadcasts the biases.\n        hidden = tf.nn.relu(tf.matmul(reshape, fc1_weights) + fc1_biases)\n\n        # Add a 50% dropout during training only. Dropout also scales\n        # activations such that no rescaling is needed at evaluation time.\n        if train:\n            hidden = tf.nn.dropout(hidden, 0.5, seed=SEED)\n        return tf.matmul(hidden, fc2_weights) + fc2_biases\n\n    # Training computation: logits + cross-entropy loss.\n    logits = model(train_data_node, True)\n    \n    # t: observed noisy labels\n    # q: estimated class probabilities (output from softmax)\n    # z: argmax of q\n\n    t = tf.one_hot(train_labels_node, NUM_LABELS)\n    q = tf.nn.softmax(logits)\n    qqq = tf.arg_max(q, dimension=1)\n    z = tf.one_hot(qqq, NUM_LABELS)\n    #cross_entropy = -tf.reduce_sum(t*tf.log(q),reduction_indices=1)\n    cross_entropy = -tf.reduce_sum((BETA*t+(1-BETA)*z)*tf.log(q),reduction_indices=1)\n    \n    loss = tf.reduce_mean(cross_entropy)\n    \n#     loss = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(\n#         logits, train_labels_node))\n  \n    # L2 regularization for the fully connected parameters.\n    regularizers = (tf.nn.l2_loss(fc1_weights) + tf.nn.l2_loss(fc1_biases) +\n                    tf.nn.l2_loss(fc2_weights) + tf.nn.l2_loss(fc2_biases))\n    # Add the regularization term to the loss.\n    loss += 5e-4 * regularizers\n  \n    # Optimizer: set up a variable that\'s incremented once per batch and\n    # controls the learning rate decay.\n    batch = tf.Variable(0, dtype=data_type())\n    # Decay once per epoch, using an exponential schedule starting at 0.01.\n    learning_rate = tf.train.exponential_decay(\n        0.01,                # Base learning rate.\n        batch * BATCH_SIZE,  # Current index into the dataset.\n        train_size,          # Decay step.\n        0.95,                # Decay rate.\n        staircase=True)\n    # Use simple momentum for the optimization.\n    optimizer = tf.train.MomentumOptimizer(learning_rate,\n                                           0.9).minimize(loss,\n                                                         global_step=batch)\n  \n    # Predictions for the current training minibatch.\n    train_prediction = tf.nn.softmax(logits)\n  \n    # Predictions for the test and validation, which we\'ll compute less often.\n    eval_prediction = tf.nn.softmax(model(eval_data))\n    \n    # Small utility function to evaluate a dataset by feeding batches of data to\n    # {eval_data} and pulling the results from {eval_predictions}.\n    # Saves memory and enables this to run on smaller GPUs.\n    def eval_in_batches(data, sess):\n        """"""Get all predictions for a dataset by running it in small batches.""""""\n        size = data.shape[0]\n        if size < EVAL_BATCH_SIZE:\n            raise ValueError(""batch size for evals larger than dataset: %d"" % size)\n        predictions = np.ndarray(shape=(size, NUM_LABELS), dtype=np.float32)\n        for begin in xrange(0, size, EVAL_BATCH_SIZE):\n            end = begin + EVAL_BATCH_SIZE\n            if end <= size:\n                predictions[begin:end, :] = sess.run(\n                    eval_prediction,\n                    feed_dict={eval_data: data[begin:end, ...]})\n            else:\n                batch_predictions = sess.run(\n                    eval_prediction,\n                    feed_dict={eval_data: data[-EVAL_BATCH_SIZE:, ...]})\n                predictions[begin:, :] = batch_predictions[begin - size:, :]\n        return predictions\n  \n    # Create a local session to run the training.\n    start_time = time.time()\n    with tf.Session() as sess:\n        # Run all the initializers to prepare the trainable parameters.\n        tf.global_variables_initializer().run() #pylint: disable=no-member\n        print(\'Initialized!\')\n        # Loop through training steps.\n        for step in xrange(int(num_epochs * train_size) // BATCH_SIZE):\n            # Compute the offset of the current minibatch in the data.\n            # Note that we could use better randomization across epochs.\n            offset = (step * BATCH_SIZE) % (train_size - BATCH_SIZE)\n            batch_data = train_data[offset:(offset + BATCH_SIZE), ...]\n            batch_labels = train_labels[offset:(offset + BATCH_SIZE)]\n            # This dictionary maps the batch data (as a numpy array) to the\n            # node in the graph it should be fed to.\n            feed_dict = {train_data_node: batch_data,\n                         train_labels_node: batch_labels}\n            # Run the graph and fetch some of the nodes.\n            _, l, lr, predictions = sess.run(\n                [optimizer, loss, learning_rate, train_prediction],\n                feed_dict=feed_dict)\n            if step % EVAL_FREQUENCY == 0:\n                elapsed_time = time.time() - start_time\n                start_time = time.time()\n                print(\'Step %d (epoch %.2f), %.1f ms\' %\n                      (step, float(step) * BATCH_SIZE / train_size,\n                       1000 * elapsed_time / EVAL_FREQUENCY))\n                print(\'Minibatch loss: %.3f, learning rate: %.6f\' % (l, lr))\n                print(\'Minibatch error: %.1f%%\' % error_rate(predictions, batch_labels))\n                print(\'Validation error: %.1f%%\' % error_rate(\n                    eval_in_batches(validation_data, sess), validation_labels))\n                sys.stdout.flush()\n        # Finally print the result!\n        test_error = error_rate(eval_in_batches(test_data, sess), test_labels)\n        print(\'Test error: %.1f%%\' % test_error)\n        if FLAGS.self_test:\n            print(\'test_error\', test_error)\n            assert test_error == 0.0, \'expected 0.0 test_error, got %.2f\' % (\n                test_error,)\n\n\nif __name__ == \'__main__\':\n    tf.app.run()\n'"
tmp/mtcnn.py,8,"b'# MIT License\n# \n# Copyright (c) 2016 David Sandberg\n# \n# Permission is hereby granted, free of charge, to any person obtaining a copy\n# of this software and associated documentation files (the ""Software""), to deal\n# in the Software without restriction, including without limitation the rights\n# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n# copies of the Software, and to permit persons to whom the Software is\n# furnished to do so, subject to the following conditions:\n# \n# The above copyright notice and this permission notice shall be included in all\n# copies or substantial portions of the Software.\n# \n# THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n# SOFTWARE.\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\nimport align.detect_face\nfrom scipy import misc\n\nwith tf.Graph().as_default():\n  \n    sess = tf.Session()\n    with sess.as_default():\n        with tf.variable_scope(\'pnet\'):\n            data = tf.placeholder(tf.float32, (None,None,None,3), \'input\')\n            pnet = align.detect_face.PNet({\'data\':data})\n            pnet.load(\'../../data/det1.npy\', sess)\n        with tf.variable_scope(\'rnet\'):\n            data = tf.placeholder(tf.float32, (None,24,24,3), \'input\')\n            rnet = align.detect_face.RNet({\'data\':data})\n            rnet.load(\'../../data/det2.npy\', sess)\n        with tf.variable_scope(\'onet\'):\n            data = tf.placeholder(tf.float32, (None,48,48,3), \'input\')\n            onet = align.detect_face.ONet({\'data\':data})\n            onet.load(\'../../data/det3.npy\', sess)\n            \n        pnet_fun = lambda img : sess.run((\'pnet/conv4-2/BiasAdd:0\', \'pnet/prob1:0\'), feed_dict={\'pnet/input:0\':img})\n        rnet_fun = lambda img : sess.run((\'rnet/conv5-2/conv5-2:0\', \'rnet/prob1:0\'), feed_dict={\'rnet/input:0\':img})\n        onet_fun = lambda img : sess.run((\'onet/conv6-2/conv6-2:0\', \'onet/conv6-3/conv6-3:0\', \'onet/prob1:0\'), feed_dict={\'onet/input:0\':img})\n\nminsize = 20 # minimum size of face\nthreshold = [ 0.6, 0.7, 0.7 ]  # three steps\'s threshold\nfactor = 0.709 # scale factor\n\nsource_path = \'/home/david/datasets/casia/CASIA-maxpy-clean/0000045/002.jpg\'\nimg = misc.imread(source_path)\n\nbounding_boxes, points = align.detect_face.detect_face(img, minsize, pnet_fun, rnet_fun, onet_fun, threshold, factor)\n\nprint(\'Bounding box: %s\' % bounding_boxes)\n\n\n'"
tmp/mtcnn_test.py,9,"b'# MIT License\n# \n# Copyright (c) 2016 David Sandberg\n# \n# Permission is hereby granted, free of charge, to any person obtaining a copy\n# of this software and associated documentation files (the ""Software""), to deal\n# in the Software without restriction, including without limitation the rights\n# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n# copies of the Software, and to permit persons to whom the Software is\n# furnished to do so, subject to the following conditions:\n# \n# The above copyright notice and this permission notice shall be included in all\n# copies or substantial portions of the Software.\n# \n# THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n# SOFTWARE.\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\nimport numpy as np\nimport align.detect_face\n\ng1 = tf.Graph()\nwith g1.as_default():\n    data = tf.placeholder(tf.float32, (None,None,None,3), \'input\')\n    pnet = align.detect_face.PNet({\'data\':data})\n    sess1 = tf.Session(graph=g1)\n    pnet.load(\'../../data/det1.npy\', sess1)\n    pnet_fun = lambda img : sess1.run((\'conv4-2/BiasAdd:0\', \'prob1:0\'), feed_dict={\'input:0\':img})\nnp.random.seed(666)\nimg = np.random.rand(1,3,150,150)\nimg = np.transpose(img, (0,2,3,1))\n      \nnp.set_printoptions(formatter={\'float\': \'{: 0.4f}\'.format})\n       \n# prob1=sess1.run(\'prob1:0\', feed_dict={data:img})\n# print(prob1[0,0,0,:])\n# conv42=sess1.run(\'conv4-2/BiasAdd:0\', feed_dict={data:img})\n# print(conv42[0,0,0,:])\n      \n# conv42, prob1 = pnet_fun(img)\n# print(prob1[0,0,0,:])\n# print(conv42[0,0,0,:])\n\n\n# [ 0.9929  0.0071] prob1, caffe\n# [ 0.9929  0.0071] prob1, tensorflow\n  \n# [ 0.1207 -0.0116 -0.1231 -0.0463] conv4-2, caffe\n# [ 0.1207 -0.0116 -0.1231 -0.0463] conv4-2, tensorflow\n      \n\ng2 = tf.Graph()\nwith g2.as_default():\n    data = tf.placeholder(tf.float32, (None,24,24,3), \'input\')\n    rnet = align.detect_face.RNet({\'data\':data})\n    sess2 = tf.Session(graph=g2)\n    rnet.load(\'../../data/det2.npy\', sess2)\n    rnet_fun = lambda img : sess2.run((\'conv5-2/conv5-2:0\', \'prob1:0\'), feed_dict={\'input:0\':img})\nnp.random.seed(666)\nimg = np.random.rand(73,3,24,24)\nimg = np.transpose(img, (0,2,3,1))\n \n# np.set_printoptions(formatter={\'float\': \'{: 0.4f}\'.format})\n#  \n# prob1=sess2.run(\'prob1:0\', feed_dict={data:img})\n# print(prob1[0,:])\n# \n# conv52=sess2.run(\'conv5-2/conv5-2:0\', feed_dict={data:img})\n# print(conv52[0,:])\n  \n# [ 0.9945  0.0055] prob1, caffe\n# [ 0.1108 -0.0038 -0.1631 -0.0890] conv5-2, caffe\n \n# [ 0.9945  0.0055] prob1, tensorflow\n# [ 0.1108 -0.0038 -0.1631 -0.0890] conv5-2, tensorflow\n\n    \ng3 = tf.Graph()\nwith g3.as_default():\n    data = tf.placeholder(tf.float32, (None,48,48,3), \'input\')\n    onet = align.detect_face.ONet({\'data\':data})\n    sess3 = tf.Session(graph=g3)\n    onet.load(\'../../data/det3.npy\', sess3)\n    onet_fun = lambda img : sess3.run((\'conv6-2/conv6-2:0\', \'conv6-3/conv6-3:0\', \'prob1:0\'), feed_dict={\'input:0\':img})\nnp.random.seed(666)\nimg = np.random.rand(11,3,48,48)\nimg = np.transpose(img, (0,2,3,1))\n \n# np.set_printoptions(formatter={\'float\': \'{: 0.4f}\'.format})\n#  \n# prob1=sess3.run(\'prob1:0\', feed_dict={data:img})\n# print(prob1[0,:])\n# print(\'prob1, tensorflow\')\n# \n# conv62=sess3.run(\'conv6-2/conv6-2:0\', feed_dict={data:img})\n# print(conv62[0,:])\n# print(\'conv6-2, tensorflow\')\n#  \n# conv63=sess3.run(\'conv6-3/conv6-3:0\', feed_dict={data:img})\n# print(conv63[0,:])\n# print(\'conv6-3, tensorflow\')\n\n# [ 0.9988  0.0012] prob1, caffe\n# [ 0.0446 -0.0968 -0.1091 -0.0212] conv6-2, caffe\n# [ 0.2429  0.6104  0.4074  0.3104  0.5939  0.2729  0.2132  0.5462  0.7863  0.7568] conv6-3, caffe\n  \n# [ 0.9988  0.0012] prob1, tensorflow\n# [ 0.0446 -0.0968 -0.1091 -0.0212] conv6-2, tensorflow\n# [ 0.2429  0.6104  0.4074  0.3104  0.5939  0.2729  0.2132  0.5462  0.7863  0.7568] conv6-3, tensorflow\n\n#pnet_fun = lambda img : sess1.run((\'conv4-2/BiasAdd:0\', \'prob1:0\'), feed_dict={\'input:0\':img})\n\n'"
tmp/mtcnn_test_pnet_dbg.py,15,"b""from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\nimport numpy as np\nimport scipy.io as io\nimport align.detect_face\n\n#ref = io.loadmat('pnet_dbg.mat')\nwith tf.Graph().as_default():\n    sess = tf.Session()\n    with sess.as_default():\n        with tf.variable_scope('pnet'):\n#            data = tf.placeholder(tf.float32, (None,None,None,3), 'input')\n            data = tf.placeholder(tf.float32, (1,1610, 1901,3), 'input')\n            pnet = align.detect_face.PNet({'data':data})\n            pnet.load('../../data/det1.npy', sess)\n#         with tf.variable_scope('rnet'):\n#             data = tf.placeholder(tf.float32, (None,24,24,3), 'input')\n#             rnet = align.detect_face.RNet({'data':data})\n#             rnet.load('../../data/det2.npy', sess)\n#         with tf.variable_scope('onet'):\n#             data = tf.placeholder(tf.float32, (None,48,48,3), 'input')\n#             onet = align.detect_face.ONet({'data':data})\n#             onet.load('../../data/det3.npy', sess)\n            \n        pnet_fun = lambda img : sess.run(('pnet/conv4-2/BiasAdd:0', 'pnet/prob1:0'), feed_dict={'pnet/input:0':img})\n#         rnet_fun = lambda img : sess.run(('rnet/conv5-2/conv5-2:0', 'rnet/prob1:0'), feed_dict={'rnet/input:0':img})\n#         onet_fun = lambda img : sess.run(('onet/conv6-2/conv6-2:0', 'onet/conv6-3/conv6-3:0', 'onet/prob1:0'), feed_dict={'onet/input:0':img})\n        \n        \nref = io.loadmat('pnet_dbg.mat')\n\nimg_x = np.expand_dims(ref['im_data'], 0)\nimg_y = np.transpose(img_x, (0,2,1,3))\nout = pnet_fun(img_y)\nout0 = np.transpose(out[0], (0,2,1,3))\nout1 = np.transpose(out[1], (0,2,1,3))\n\n#np.where(abs(out0[0,:,:,:]-ref['out0'])>1e-18)\nqqq3 = np.where(abs(out1[0,:,:,:]-ref['out1'])>1e-7) # 3390 diffs with softmax2\nprint(qqq3[0].shape)\n      \nnp.set_printoptions(formatter={'float': '{: 0.4f}'.format})\n       \n# prob1=sess1.run('prob1:0', feed_dict={data:img})\n# print(prob1[0,0,0,:])\n# conv42=sess1.run('conv4-2/BiasAdd:0', feed_dict={data:img})\n# print(conv42[0,0,0,:])\n      \n# conv42, prob1 = pnet_fun(img)\n# print(prob1[0,0,0,:])\n# print(conv42[0,0,0,:])\n\n\n# [ 0.9929  0.0071] prob1, caffe\n# [ 0.9929  0.0071] prob1, tensorflow\n  \n# [ 0.1207 -0.0116 -0.1231 -0.0463] conv4-2, caffe\n# [ 0.1207 -0.0116 -0.1231 -0.0463] conv4-2, tensorflow\n      \n\n# g2 = tf.Graph()\n# with g2.as_default():\n#     data = tf.placeholder(tf.float32, (None,24,24,3), 'input')\n#     rnet = align.detect_face.RNet({'data':data})\n#     sess2 = tf.Session(graph=g2)\n#     rnet.load('../../data/det2.npy', sess2)\n#     rnet_fun = lambda img : sess2.run(('conv5-2/conv5-2:0', 'prob1:0'), feed_dict={'input:0':img})\n# np.random.seed(666)\n# img = np.random.rand(73,3,24,24)\n# img = np.transpose(img, (0,2,3,1))\n \n# np.set_printoptions(formatter={'float': '{: 0.4f}'.format})\n#  \n# prob1=sess2.run('prob1:0', feed_dict={data:img})\n# print(prob1[0,:])\n# \n# conv52=sess2.run('conv5-2/conv5-2:0', feed_dict={data:img})\n# print(conv52[0,:])\n  \n# [ 0.9945  0.0055] prob1, caffe\n# [ 0.1108 -0.0038 -0.1631 -0.0890] conv5-2, caffe\n \n# [ 0.9945  0.0055] prob1, tensorflow\n# [ 0.1108 -0.0038 -0.1631 -0.0890] conv5-2, tensorflow\n\n    \n# g3 = tf.Graph()\n# with g3.as_default():\n#     data = tf.placeholder(tf.float32, (None,48,48,3), 'input')\n#     onet = align.detect_face.ONet({'data':data})\n#     sess3 = tf.Session(graph=g3)\n#     onet.load('../../data/det3.npy', sess3)\n#     onet_fun = lambda img : sess3.run(('conv6-2/conv6-2:0', 'conv6-3/conv6-3:0', 'prob1:0'), feed_dict={'input:0':img})\n# np.random.seed(666)\n# img = np.random.rand(11,3,48,48)\n# img = np.transpose(img, (0,2,3,1))\n \n# np.set_printoptions(formatter={'float': '{: 0.4f}'.format})\n#  \n# prob1=sess3.run('prob1:0', feed_dict={data:img})\n# print(prob1[0,:])\n# print('prob1, tensorflow')\n# \n# conv62=sess3.run('conv6-2/conv6-2:0', feed_dict={data:img})\n# print(conv62[0,:])\n# print('conv6-2, tensorflow')\n#  \n# conv63=sess3.run('conv6-3/conv6-3:0', feed_dict={data:img})\n# print(conv63[0,:])\n# print('conv6-3, tensorflow')\n\n# [ 0.9988  0.0012] prob1, caffe\n# [ 0.0446 -0.0968 -0.1091 -0.0212] conv6-2, caffe\n# [ 0.2429  0.6104  0.4074  0.3104  0.5939  0.2729  0.2132  0.5462  0.7863  0.7568] conv6-3, caffe\n  \n# [ 0.9988  0.0012] prob1, tensorflow\n# [ 0.0446 -0.0968 -0.1091 -0.0212] conv6-2, tensorflow\n# [ 0.2429  0.6104  0.4074  0.3104  0.5939  0.2729  0.2132  0.5462  0.7863  0.7568] conv6-3, tensorflow\n\n#pnet_fun = lambda img : sess1.run(('conv4-2/BiasAdd:0', 'prob1:0'), feed_dict={'input:0':img})\n\n"""
tmp/network.py,41,"b'""""""Functions for building the face recognition network.\n""""""\n# MIT License\n# \n# Copyright (c) 2016 David Sandberg\n# \n# Permission is hereby granted, free of charge, to any person obtaining a copy\n# of this software and associated documentation files (the ""Software""), to deal\n# in the Software without restriction, including without limitation the rights\n# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n# copies of the Software, and to permit persons to whom the Software is\n# furnished to do so, subject to the following conditions:\n# \n# The above copyright notice and this permission notice shall be included in all\n# copies or substantial portions of the Software.\n# \n# THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n# SOFTWARE.\n\n# pylint: disable=missing-docstring\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\nfrom tensorflow.python.ops import array_ops\nfrom tensorflow.python.ops import control_flow_ops\n\n\ndef conv(inpOp, nIn, nOut, kH, kW, dH, dW, padType, name, phase_train=True, use_batch_norm=True, weight_decay=0.0):\n    with tf.variable_scope(name):\n        l2_regularizer = lambda t: l2_loss(t, weight=weight_decay)\n        kernel = tf.get_variable(""weights"", [kH, kW, nIn, nOut],\n            initializer=tf.truncated_normal_initializer(stddev=1e-1),\n            regularizer=l2_regularizer, dtype=inpOp.dtype)\n        cnv = tf.nn.conv2d(inpOp, kernel, [1, dH, dW, 1], padding=padType)\n        \n        if use_batch_norm:\n            conv_bn = batch_norm(cnv, phase_train)\n        else:\n            conv_bn = cnv\n        biases = tf.get_variable(""biases"", [nOut], initializer=tf.constant_initializer(), dtype=inpOp.dtype)\n        bias = tf.nn.bias_add(conv_bn, biases)\n        conv1 = tf.nn.relu(bias)\n    return conv1\n\ndef affine(inpOp, nIn, nOut, name, weight_decay=0.0):\n    with tf.variable_scope(name):\n        l2_regularizer = lambda t: l2_loss(t, weight=weight_decay)\n        weights = tf.get_variable(""weights"", [nIn, nOut],\n            initializer=tf.truncated_normal_initializer(stddev=1e-1),\n            regularizer=l2_regularizer, dtype=inpOp.dtype)\n        biases = tf.get_variable(""biases"", [nOut], initializer=tf.constant_initializer(), dtype=inpOp.dtype)\n        affine1 = tf.nn.relu_layer(inpOp, weights, biases)\n    return affine1\n\ndef l2_loss(tensor, weight=1.0, scope=None):\n    """"""Define a L2Loss, useful for regularize, i.e. weight decay.\n    Args:\n      tensor: tensor to regularize.\n      weight: an optional weight to modulate the loss.\n      scope: Optional scope for op_scope.\n    Returns:\n      the L2 loss op.\n    """"""\n    with tf.name_scope(scope):\n        weight = tf.convert_to_tensor(weight,\n                                      dtype=tensor.dtype.base_dtype,\n                                      name=\'loss_weight\')\n        loss = tf.multiply(weight, tf.nn.l2_loss(tensor), name=\'value\')\n    return loss\n\ndef lppool(inpOp, pnorm, kH, kW, dH, dW, padding, name):\n    with tf.variable_scope(name):\n        if pnorm == 2:\n            pwr = tf.square(inpOp)\n        else:\n            pwr = tf.pow(inpOp, pnorm)\n          \n        subsamp = tf.nn.avg_pool(pwr,\n                              ksize=[1, kH, kW, 1],\n                              strides=[1, dH, dW, 1],\n                              padding=padding)\n        subsamp_sum = tf.multiply(subsamp, kH*kW)\n        \n        if pnorm == 2:\n            out = tf.sqrt(subsamp_sum)\n        else:\n            out = tf.pow(subsamp_sum, 1/pnorm)\n    \n    return out\n\ndef mpool(inpOp, kH, kW, dH, dW, padding, name):\n    with tf.variable_scope(name):\n        maxpool = tf.nn.max_pool(inpOp,\n                       ksize=[1, kH, kW, 1],\n                       strides=[1, dH, dW, 1],\n                       padding=padding)  \n    return maxpool\n\ndef apool(inpOp, kH, kW, dH, dW, padding, name):\n    with tf.variable_scope(name):\n        avgpool = tf.nn.avg_pool(inpOp,\n                              ksize=[1, kH, kW, 1],\n                              strides=[1, dH, dW, 1],\n                              padding=padding)\n    return avgpool\n\ndef batch_norm(x, phase_train):\n    """"""\n    Batch normalization on convolutional maps.\n    Args:\n        x:           Tensor, 4D BHWD input maps\n        n_out:       integer, depth of input maps\n        phase_train: boolean tf.Variable, true indicates training phase\n        scope:       string, variable scope\n        affn:      whether to affn-transform outputs\n    Return:\n        normed:      batch-normalized maps\n    Ref: http://stackoverflow.com/questions/33949786/how-could-i-use-batch-normalization-in-tensorflow/33950177\n    """"""\n    name = \'batch_norm\'\n    with tf.variable_scope(name):\n        phase_train = tf.convert_to_tensor(phase_train, dtype=tf.bool)\n        n_out = int(x.get_shape()[3])\n        beta = tf.Variable(tf.constant(0.0, shape=[n_out], dtype=x.dtype),\n                           name=name+\'/beta\', trainable=True, dtype=x.dtype)\n        gamma = tf.Variable(tf.constant(1.0, shape=[n_out], dtype=x.dtype),\n                            name=name+\'/gamma\', trainable=True, dtype=x.dtype)\n      \n        batch_mean, batch_var = tf.nn.moments(x, [0,1,2], name=\'moments\')\n        ema = tf.train.ExponentialMovingAverage(decay=0.9)\n        def mean_var_with_update():\n            ema_apply_op = ema.apply([batch_mean, batch_var])\n            with tf.control_dependencies([ema_apply_op]):\n                return tf.identity(batch_mean), tf.identity(batch_var)\n        mean, var = control_flow_ops.cond(phase_train,\n                                          mean_var_with_update,\n                                          lambda: (ema.average(batch_mean), ema.average(batch_var)))\n        normed = tf.nn.batch_normalization(x, mean, var, beta, gamma, 1e-3)\n    return normed\n\ndef inception(inp, inSize, ks, o1s, o2s1, o2s2, o3s1, o3s2, o4s1, o4s2, o4s3, poolType, name, \n              phase_train=True, use_batch_norm=True, weight_decay=0.0):\n  \n    print(\'name = \', name)\n    print(\'inputSize = \', inSize)\n    print(\'kernelSize = {3,5}\')\n    print(\'kernelStride = {%d,%d}\' % (ks,ks))\n    print(\'outputSize = {%d,%d}\' % (o2s2,o3s2))\n    print(\'reduceSize = {%d,%d,%d,%d}\' % (o2s1,o3s1,o4s2,o1s))\n    print(\'pooling = {%s, %d, %d, %d, %d}\' % (poolType, o4s1, o4s1, o4s3, o4s3))\n    if (o4s2>0):\n        o4 = o4s2\n    else:\n        o4 = inSize\n    print(\'outputSize = \', o1s+o2s2+o3s2+o4)\n    print()\n    \n    net = []\n    \n    with tf.variable_scope(name):\n        with tf.variable_scope(\'branch1_1x1\'):\n            if o1s>0:\n                conv1 = conv(inp, inSize, o1s, 1, 1, 1, 1, \'SAME\', \'conv1x1\', phase_train=phase_train, use_batch_norm=use_batch_norm, weight_decay=weight_decay)\n                net.append(conv1)\n      \n        with tf.variable_scope(\'branch2_3x3\'):\n            if o2s1>0:\n                conv3a = conv(inp, inSize, o2s1, 1, 1, 1, 1, \'SAME\', \'conv1x1\', phase_train=phase_train, use_batch_norm=use_batch_norm, weight_decay=weight_decay)\n                conv3 = conv(conv3a, o2s1, o2s2, 3, 3, ks, ks, \'SAME\', \'conv3x3\', phase_train=phase_train, use_batch_norm=use_batch_norm, weight_decay=weight_decay)\n                net.append(conv3)\n      \n        with tf.variable_scope(\'branch3_5x5\'):\n            if o3s1>0:\n                conv5a = conv(inp, inSize, o3s1, 1, 1, 1, 1, \'SAME\', \'conv1x1\', phase_train=phase_train, use_batch_norm=use_batch_norm, weight_decay=weight_decay)\n                conv5 = conv(conv5a, o3s1, o3s2, 5, 5, ks, ks, \'SAME\', \'conv5x5\', phase_train=phase_train, use_batch_norm=use_batch_norm, weight_decay=weight_decay)\n                net.append(conv5)\n      \n        with tf.variable_scope(\'branch4_pool\'):\n            if poolType==\'MAX\':\n                pool = mpool(inp, o4s1, o4s1, o4s3, o4s3, \'SAME\', \'pool\')\n            elif poolType==\'L2\':\n                pool = lppool(inp, 2, o4s1, o4s1, o4s3, o4s3, \'SAME\', \'pool\')\n            else:\n                raise ValueError(\'Invalid pooling type ""%s""\' % poolType)\n            \n            if o4s2>0:\n                pool_conv = conv(pool, inSize, o4s2, 1, 1, 1, 1, \'SAME\', \'conv1x1\', phase_train=phase_train, use_batch_norm=use_batch_norm, weight_decay=weight_decay)\n            else:\n                pool_conv = pool\n            net.append(pool_conv)\n      \n        incept = array_ops.concat(net, 3, name=name)\n    return incept\n'"
tmp/nn2.py,2,"b'# MIT License\n# \n# Copyright (c) 2016 David Sandberg\n# \n# Permission is hereby granted, free of charge, to any person obtaining a copy\n# of this software and associated documentation files (the ""Software""), to deal\n# in the Software without restriction, including without limitation the rights\n# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n# copies of the Software, and to permit persons to whom the Software is\n# furnished to do so, subject to the following conditions:\n# \n# The above copyright notice and this permission notice shall be included in all\n# copies or substantial portions of the Software.\n# \n# THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n# SOFTWARE.\n\n# pylint: disable=missing-docstring\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\nimport models.network as network\n\ndef inference(images, keep_probability, phase_train=True, weight_decay=0.0):\n    """""" Define an inference network for face recognition based \n           on inception modules using batch normalization\n    \n    Args:\n      images: The images to run inference on, dimensions batch_size x height x width x channels\n      phase_train: True if batch normalization should operate in training mode\n    """"""\n    endpoints = {}\n    net = network.conv(images, 3, 64, 7, 7, 2, 2, \'SAME\', \'conv1_7x7\', phase_train=phase_train, use_batch_norm=True, weight_decay=weight_decay)\n    endpoints[\'conv1\'] = net\n    net = network.mpool(net,  3, 3, 2, 2, \'SAME\', \'pool1\')\n    endpoints[\'pool1\'] = net\n    net = network.conv(net,  64, 64, 1, 1, 1, 1, \'SAME\', \'conv2_1x1\', phase_train=phase_train, use_batch_norm=True, weight_decay=weight_decay)\n    endpoints[\'conv2_1x1\'] = net\n    net = network.conv(net,  64, 192, 3, 3, 1, 1, \'SAME\', \'conv3_3x3\', phase_train=phase_train, use_batch_norm=True, weight_decay=weight_decay)\n    endpoints[\'conv3_3x3\'] = net\n    net = network.mpool(net,  3, 3, 2, 2, \'SAME\', \'pool3\')\n    endpoints[\'pool3\'] = net\n  \n    net = network.inception(net, 192, 1, 64, 96, 128, 16, 32, 3, 32, 1, \'MAX\', \'incept3a\', phase_train=phase_train, use_batch_norm=True, weight_decay=weight_decay)\n    endpoints[\'incept3a\'] = net\n    net = network.inception(net, 256, 1, 64, 96, 128, 32, 64, 3, 64, 1, \'MAX\', \'incept3b\', phase_train=phase_train, use_batch_norm=True, weight_decay=weight_decay)\n    endpoints[\'incept3b\'] = net\n    net = network.inception(net, 320, 2, 0, 128, 256, 32, 64, 3, 0, 2, \'MAX\', \'incept3c\', phase_train=phase_train, use_batch_norm=True, weight_decay=weight_decay)\n    endpoints[\'incept3c\'] = net\n    \n    net = network.inception(net, 640, 1, 256, 96, 192, 32, 64, 3, 128, 1, \'MAX\', \'incept4a\', phase_train=phase_train, use_batch_norm=True, weight_decay=weight_decay)\n    endpoints[\'incept4a\'] = net\n    net = network.inception(net, 640, 1, 224, 112, 224, 32, 64, 3, 128, 1, \'MAX\', \'incept4b\', phase_train=phase_train, use_batch_norm=True, weight_decay=weight_decay)\n    endpoints[\'incept4b\'] = net\n    net = network.inception(net, 640, 1, 192, 128, 256, 32, 64, 3, 128, 1, \'MAX\', \'incept4c\', phase_train=phase_train, use_batch_norm=True, weight_decay=weight_decay)\n    endpoints[\'incept4c\'] = net\n    net = network.inception(net, 640, 1, 160, 144, 288, 32, 64, 3, 128, 1, \'MAX\', \'incept4d\', phase_train=phase_train, use_batch_norm=True, weight_decay=weight_decay)\n    endpoints[\'incept4d\'] = net\n    net = network.inception(net, 640, 2, 0, 160, 256, 64, 128, 3, 0, 2, \'MAX\', \'incept4e\', phase_train=phase_train, use_batch_norm=True)\n    endpoints[\'incept4e\'] = net\n    \n    net = network.inception(net, 1024, 1, 384, 192, 384, 48, 128, 3, 128, 1, \'MAX\', \'incept5a\', phase_train=phase_train, use_batch_norm=True, weight_decay=weight_decay)\n    endpoints[\'incept5a\'] = net\n    net = network.inception(net, 1024, 1, 384, 192, 384, 48, 128, 3, 128, 1, \'MAX\', \'incept5b\', phase_train=phase_train, use_batch_norm=True, weight_decay=weight_decay)\n    endpoints[\'incept5b\'] = net\n    net = network.apool(net,  7, 7, 1, 1, \'VALID\', \'pool6\')\n    endpoints[\'pool6\'] = net\n    net = tf.reshape(net, [-1, 1024])\n    endpoints[\'prelogits\'] = net\n    net = tf.nn.dropout(net, keep_probability)\n    endpoints[\'dropout\'] = net\n    \n    return net, endpoints\n'"
tmp/nn3.py,2,"b'# MIT License\n# \n# Copyright (c) 2016 David Sandberg\n# \n# Permission is hereby granted, free of charge, to any person obtaining a copy\n# of this software and associated documentation files (the ""Software""), to deal\n# in the Software without restriction, including without limitation the rights\n# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n# copies of the Software, and to permit persons to whom the Software is\n# furnished to do so, subject to the following conditions:\n# \n# The above copyright notice and this permission notice shall be included in all\n# copies or substantial portions of the Software.\n# \n# THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n# SOFTWARE.\n\n# pylint: disable=missing-docstring\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\nimport models.network as network\n\ndef inference(images, keep_probability, phase_train=True, weight_decay=0.0):\n    """""" Define an inference network for face recognition based \n           on inception modules using batch normalization\n    \n    Args:\n      images: The images to run inference on, dimensions batch_size x height x width x channels\n      phase_train: True if batch normalization should operate in training mode\n    """"""\n    endpoints = {}\n    net = network.conv(images, 3, 64, 7, 7, 2, 2, \'SAME\', \'conv1_7x7\', phase_train=phase_train, use_batch_norm=True, weight_decay=weight_decay)\n    endpoints[\'conv1\'] = net\n    net = network.mpool(net,  3, 3, 2, 2, \'SAME\', \'pool1\')\n    endpoints[\'pool1\'] = net\n    net = network.conv(net,  64, 64, 1, 1, 1, 1, \'SAME\', \'conv2_1x1\', phase_train=phase_train, use_batch_norm=True, weight_decay=weight_decay)\n    endpoints[\'conv2_1x1\'] = net\n    net = network.conv(net,  64, 192, 3, 3, 1, 1, \'SAME\', \'conv3_3x3\', phase_train=phase_train, use_batch_norm=True, weight_decay=weight_decay)\n    endpoints[\'conv3_3x3\'] = net\n    net = network.mpool(net,  3, 3, 2, 2, \'SAME\', \'pool3\')\n    endpoints[\'pool3\'] = net\n  \n    net = network.inception(net, 192, 1, 64, 96, 128, 16, 32, 3, 32, 1, \'MAX\', \'incept3a\', phase_train=phase_train, use_batch_norm=True, weight_decay=weight_decay)\n    endpoints[\'incept3a\'] = net\n    net = network.inception(net, 256, 1, 64, 96, 128, 32, 64, 3, 64, 1, \'MAX\', \'incept3b\', phase_train=phase_train, use_batch_norm=True, weight_decay=weight_decay)\n    endpoints[\'incept3b\'] = net\n    net = network.inception(net, 320, 2, 0, 128, 256, 32, 64, 3, 0, 2, \'MAX\', \'incept3c\', phase_train=phase_train, use_batch_norm=True, weight_decay=weight_decay)\n    endpoints[\'incept3c\'] = net\n    \n    net = network.inception(net, 640, 1, 256, 96, 192, 32, 64, 3, 128, 1, \'MAX\', \'incept4a\', phase_train=phase_train, use_batch_norm=True, weight_decay=weight_decay)\n    endpoints[\'incept4a\'] = net\n    net = network.inception(net, 640, 1, 224, 112, 224, 32, 64, 3, 128, 1, \'MAX\', \'incept4b\', phase_train=phase_train, use_batch_norm=True, weight_decay=weight_decay)\n    endpoints[\'incept4b\'] = net\n    net = network.inception(net, 640, 1, 192, 128, 256, 32, 64, 3, 128, 1, \'MAX\', \'incept4c\', phase_train=phase_train, use_batch_norm=True, weight_decay=weight_decay)\n    endpoints[\'incept4c\'] = net\n    net = network.inception(net, 640, 1, 160, 144, 288, 32, 64, 3, 128, 1, \'MAX\', \'incept4d\', phase_train=phase_train, use_batch_norm=True, weight_decay=weight_decay)\n    endpoints[\'incept4d\'] = net\n    net = network.inception(net, 640, 2, 0, 160, 256, 64, 128, 3, 0, 2, \'MAX\', \'incept4e\', phase_train=phase_train, use_batch_norm=True)\n    endpoints[\'incept4e\'] = net\n    \n    net = network.inception(net, 1024, 1, 384, 192, 384, 48, 128, 3, 128, 1, \'MAX\', \'incept5a\', phase_train=phase_train, use_batch_norm=True, weight_decay=weight_decay)\n    endpoints[\'incept5a\'] = net\n    net = network.inception(net, 1024, 1, 384, 192, 384, 48, 128, 3, 128, 1, \'MAX\', \'incept5b\', phase_train=phase_train, use_batch_norm=True, weight_decay=weight_decay)\n    endpoints[\'incept5b\'] = net\n    net = network.apool(net,  5, 5, 1, 1, \'VALID\', \'pool6\')\n    endpoints[\'pool6\'] = net\n    net = tf.reshape(net, [-1, 1024])\n    endpoints[\'prelogits\'] = net\n    net = tf.nn.dropout(net, keep_probability)\n    endpoints[\'dropout\'] = net\n    \n    return net, endpoints\n'"
tmp/nn4.py,2,"b'# MIT License\n# \n# Copyright (c) 2016 David Sandberg\n# \n# Permission is hereby granted, free of charge, to any person obtaining a copy\n# of this software and associated documentation files (the ""Software""), to deal\n# in the Software without restriction, including without limitation the rights\n# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n# copies of the Software, and to permit persons to whom the Software is\n# furnished to do so, subject to the following conditions:\n# \n# The above copyright notice and this permission notice shall be included in all\n# copies or substantial portions of the Software.\n# \n# THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n# SOFTWARE.\n\n# pylint: disable=missing-docstring\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\nimport models.network as network\n\ndef inference(images, keep_probability, phase_train=True, weight_decay=0.0):\n    """""" Define an inference network for face recognition based \n           on inception modules using batch normalization\n    \n    Args:\n      images: The images to run inference on, dimensions batch_size x height x width x channels\n      phase_train: True if batch normalization should operate in training mode\n    """"""\n    endpoints = {}\n    net = network.conv(images, 3, 64, 7, 7, 2, 2, \'SAME\', \'conv1_7x7\', phase_train=phase_train, use_batch_norm=True, weight_decay=weight_decay)\n    endpoints[\'conv1\'] = net\n    net = network.mpool(net,  3, 3, 2, 2, \'SAME\', \'pool1\')\n    endpoints[\'pool1\'] = net\n    net = network.conv(net,  64, 64, 1, 1, 1, 1, \'SAME\', \'conv2_1x1\', phase_train=phase_train, use_batch_norm=True, weight_decay=weight_decay)\n    endpoints[\'conv2_1x1\'] = net\n    net = network.conv(net,  64, 192, 3, 3, 1, 1, \'SAME\', \'conv3_3x3\', phase_train=phase_train, use_batch_norm=True, weight_decay=weight_decay)\n    endpoints[\'conv3_3x3\'] = net\n    net = network.mpool(net,  3, 3, 2, 2, \'SAME\', \'pool3\')\n    endpoints[\'pool3\'] = net\n  \n    net = network.inception(net, 192, 1, 64, 96, 128, 16, 32, 3, 32, 1, \'MAX\', \'incept3a\', phase_train=phase_train, use_batch_norm=True, weight_decay=weight_decay)\n    endpoints[\'incept3a\'] = net\n    net = network.inception(net, 256, 1, 64, 96, 128, 32, 64, 3, 64, 1, \'MAX\', \'incept3b\', phase_train=phase_train, use_batch_norm=True, weight_decay=weight_decay)\n    endpoints[\'incept3b\'] = net\n    net = network.inception(net, 320, 2, 0, 128, 256, 32, 64, 3, 0, 2, \'MAX\', \'incept3c\', phase_train=phase_train, use_batch_norm=True, weight_decay=weight_decay)\n    endpoints[\'incept3c\'] = net\n    \n    net = network.inception(net, 640, 1, 256, 96, 192, 32, 64, 3, 128, 1, \'MAX\', \'incept4a\', phase_train=phase_train, use_batch_norm=True, weight_decay=weight_decay)\n    endpoints[\'incept4a\'] = net\n    net = network.inception(net, 640, 1, 224, 112, 224, 32, 64, 3, 128, 1, \'MAX\', \'incept4b\', phase_train=phase_train, use_batch_norm=True, weight_decay=weight_decay)\n    endpoints[\'incept4b\'] = net\n    net = network.inception(net, 640, 1, 192, 128, 256, 32, 64, 3, 128, 1, \'MAX\', \'incept4c\', phase_train=phase_train, use_batch_norm=True, weight_decay=weight_decay)\n    endpoints[\'incept4c\'] = net\n    net = network.inception(net, 640, 1, 160, 144, 288, 32, 64, 3, 128, 1, \'MAX\', \'incept4d\', phase_train=phase_train, use_batch_norm=True, weight_decay=weight_decay)\n    endpoints[\'incept4d\'] = net\n    net = network.inception(net, 640, 2, 0, 160, 256, 64, 128, 3, 0, 2, \'MAX\', \'incept4e\', phase_train=phase_train, use_batch_norm=True)\n    endpoints[\'incept4e\'] = net\n    \n    net = network.inception(net, 1024, 1, 384, 192, 384, 0, 0, 3, 128, 1, \'MAX\', \'incept5a\', phase_train=phase_train, use_batch_norm=True, weight_decay=weight_decay)\n    endpoints[\'incept5a\'] = net\n    net = network.inception(net, 896, 1, 384, 192, 384, 0, 0, 3, 128, 1, \'MAX\', \'incept5b\', phase_train=phase_train, use_batch_norm=True, weight_decay=weight_decay)\n    endpoints[\'incept5b\'] = net\n    net = network.apool(net,  3, 3, 1, 1, \'VALID\', \'pool6\')\n    endpoints[\'pool6\'] = net\n    net = tf.reshape(net, [-1, 896])\n    endpoints[\'prelogits\'] = net\n    net = tf.nn.dropout(net, keep_probability)\n    endpoints[\'dropout\'] = net\n    \n    return net, endpoints\n'"
tmp/nn4_small2_v1.py,2,"b'# MIT License\n# \n# Copyright (c) 2016 David Sandberg\n# \n# Permission is hereby granted, free of charge, to any person obtaining a copy\n# of this software and associated documentation files (the ""Software""), to deal\n# in the Software without restriction, including without limitation the rights\n# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n# copies of the Software, and to permit persons to whom the Software is\n# furnished to do so, subject to the following conditions:\n# \n# The above copyright notice and this permission notice shall be included in all\n# copies or substantial portions of the Software.\n# \n# THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n# SOFTWARE.\n\n# pylint: disable=missing-docstring\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\nimport models.network as network\n\ndef inference(images, keep_probability, phase_train=True, weight_decay=0.0):\n    """""" Define an inference network for face recognition based \n           on inception modules using batch normalization\n    \n    Args:\n      images: The images to run inference on, dimensions batch_size x height x width x channels\n      phase_train: True if batch normalization should operate in training mode\n    """"""\n    endpoints = {}\n    net = network.conv(images, 3, 64, 7, 7, 2, 2, \'SAME\', \'conv1_7x7\', phase_train=phase_train, use_batch_norm=True, weight_decay=weight_decay)\n    endpoints[\'conv1\'] = net\n    net = network.mpool(net,  3, 3, 2, 2, \'SAME\', \'pool1\')\n    endpoints[\'pool1\'] = net\n    net = network.conv(net,  64, 64, 1, 1, 1, 1, \'SAME\', \'conv2_1x1\', phase_train=phase_train, use_batch_norm=True, weight_decay=weight_decay)\n    endpoints[\'conv2_1x1\'] = net\n    net = network.conv(net,  64, 192, 3, 3, 1, 1, \'SAME\', \'conv3_3x3\', phase_train=phase_train, use_batch_norm=True, weight_decay=weight_decay)\n    endpoints[\'conv3_3x3\'] = net\n    net = network.mpool(net,  3, 3, 2, 2, \'SAME\', \'pool3\')\n    endpoints[\'pool3\'] = net\n  \n    net = network.inception(net, 192, 1, 64, 96, 128, 16, 32, 3, 32, 1, \'MAX\', \'incept3a\', phase_train=phase_train, use_batch_norm=True, weight_decay=weight_decay)\n    endpoints[\'incept3a\'] = net\n    net = network.inception(net, 256, 1, 64, 96, 128, 32, 64, 3, 64, 1, \'MAX\', \'incept3b\', phase_train=phase_train, use_batch_norm=True, weight_decay=weight_decay)\n    endpoints[\'incept3b\'] = net\n    net = network.inception(net, 320, 2, 0, 128, 256, 32, 64, 3, 0, 2, \'MAX\', \'incept3c\', phase_train=phase_train, use_batch_norm=True, weight_decay=weight_decay)\n    endpoints[\'incept3c\'] = net\n    \n    net = network.inception(net, 640, 1, 256, 96, 192, 32, 64, 3, 128, 1, \'MAX\', \'incept4a\', phase_train=phase_train, use_batch_norm=True, weight_decay=weight_decay)\n    endpoints[\'incept4a\'] = net\n    net = network.inception(net, 640, 2, 0, 160, 256, 64, 128, 3, 0, 2, \'MAX\', \'incept4e\', phase_train=phase_train, use_batch_norm=True, weight_decay=weight_decay)\n    endpoints[\'incept4e\'] = net\n    \n    net = network.inception(net, 1024, 1, 256, 96,  384, 0, 0, 3, 96,  1, \'MAX\', \'incept5a\', phase_train=phase_train, use_batch_norm=True, weight_decay=weight_decay)\n    endpoints[\'incept5a\'] = net\n    net = network.inception(net, 736, 1, 256, 96, 384, 0, 0, 3, 96, 1, \'MAX\', \'incept5b\', phase_train=phase_train, use_batch_norm=True, weight_decay=weight_decay)\n    endpoints[\'incept5b\'] = net\n    net = network.apool(net,  3, 3, 1, 1, \'VALID\', \'pool6\')\n    endpoints[\'pool6\'] = net\n    net = tf.reshape(net, [-1, 736])\n    endpoints[\'prelogits\'] = net\n    net = tf.nn.dropout(net, keep_probability)\n    endpoints[\'dropout\'] = net\n    \n    return net, endpoints\n  '"
tmp/random_test.py,15,"b'import tensorflow as tf\nimport numpy as np\nfrom six.moves import xrange\n\n\nwith tf.Graph().as_default():\n  tf.set_random_seed(666)\n\n\n  # Placeholder for input images\n  input_placeholder = tf.placeholder(tf.float32, shape=(9, 7), name=\'input\')\n  \n  # Split example embeddings into anchor, positive and negative\n  #anchor, positive, negative = tf.split(0, 3, input)\n  resh1 = tf.reshape(input_placeholder, [3,3,7])\n  anchor = resh1[0,:,:]\n  positive = resh1[1,:,:]\n  negative = resh1[2,:,:]\n  \n  # Build an initialization operation to run below.\n  init = tf.global_variables_initializer()\n\n  # Start running operations on the Graph.\n  sess = tf.Session(config=tf.ConfigProto(log_device_placement=False))\n  sess.run(init)\n  \n  with sess.as_default():\n    batch = np.zeros((9,7))\n    batch[0,:] = 1.1\n    batch[1,:] = 2.1\n    batch[2,:] = 3.1\n    batch[3,:] = 1.2\n    batch[4,:] = 2.2\n    batch[5,:] = 3.2\n    batch[6,:] = 1.3\n    batch[7,:] = 2.3\n    batch[8,:] = 3.3\n    feed_dict = {input_placeholder: batch }\n    print(batch)\n    print(sess.run([anchor, positive, negative], feed_dict=feed_dict))\n\n\n\n\n#feed_dict = { images_placeholder: np.zeros((90,96,96,3)), phase_train_placeholder: True }\n#vars_eval  = sess.run(tf.global_variables(), feed_dict=feed_dict)\n#for gt in vars_eval:\n  #print(\'%.20f\' % (np.sum(gt)))\n#for gt, gv in zip(grads_eval, grad_vars):\n  #print(\'%40s: %.20f\' % (gv.op.name, np.sum(gt)))\n\n  \n\n#import h5py\n#myFile = h5py.File(\'/home/david/repo/TensorFace/network.h5\', \'r\')\n\n## The \'...\' means retrieve the whole tensor\n#data = myFile[...]\n#print(data)\n\n\n#import h5py    # HDF5 support\n\n#fileName = ""/home/david/repo/TensorFace/network.h5""\n#f = h5py.File(fileName,  ""r"")\n##for item in f.keys():\n  ##print item\n#for item in f.values():\n  #print item\n\n\n#import tensorflow as tf\n#import numpy as np\n#import matplotlib.pyplot as plt\n#import math\n#import facenet\n#import os\n#import glob\n#from scipy import misc\n\n#def plot_triplet(apn, idx):\n    #plt.subplot(1,3,1)\n    #plt.imshow(np.multiply(apn[idx*3+0,:,:,:],1/256))\n    #plt.subplot(1,3,2)\n    #plt.imshow(np.multiply(apn[idx*3+1,:,:,:],1/256))\n    #plt.subplot(1,3,3)\n    #plt.imshow(np.multiply(apn[idx*3+2,:,:,:],1/256))\n\n\n#input_image = tf.placeholder(tf.float32, name=\'input_image\')\n#phase_train = tf.placeholder(tf.bool, name=\'phase_train\')\n\n#n_in, n_out = 3, 16\n#ksize = 3\n#stride = 1\n#kernel = tf.Variable(tf.truncated_normal([ksize, ksize, n_in, n_out],\n                                         #stddev=math.sqrt(2/(ksize*ksize*n_out))),\n                     #name=\'kernel\')\n#conv = tf.nn.conv2d(input_image, kernel, [1,stride,stride,1], padding=""SAME"")\n#conv_bn = facenet.batch_norm(conv, n_out, phase_train)\n#relu = tf.nn.relu(conv_bn)\n\n## Build an initialization operation to run below.\n#init = tf.global_variables_initializer()\n\n## Start running operations on the Graph.\n#sess = tf.Session()\n#sess.run(init)\n\n#path = \'/home/david/datasets/fs_aligned/Zooey_Deschanel/\'\n#files = glob.glob(os.path.join(path, \'*.png\'))\n#nrof_samples = 30\n#img_list = [None] * nrof_samples\n#for i in xrange(nrof_samples):\n    #img_list[i] = misc.imread(files[i])\n#images = np.stack(img_list)\n\n#feed_dict = {\n    #input_image: images.astype(np.float32),\n    #phase_train: True\n#}\n\n#out = sess.run([relu], feed_dict=feed_dict)\n#print(out[0].shape)\n\n##print(out)\n\n#plot_triplet(images, 0)\n\n\n\n#import matplotlib.pyplot as plt\n#import numpy as np\n\n#a=[3,4,5,6]\n#b = [1,a[1:3]]\n#print(b)\n\n## Generate some data...\n#x, y = np.meshgrid(np.linspace(-2,2,200), np.linspace(-2,2,200))\n#x, y = x - x.mean(), y - y.mean()\n#z = x * np.exp(-x**2 - y**2)\n#print(z.shape)\n\n## Plot the grid\n#plt.imshow(z)\n#plt.gray()\n#plt.show()\n\n#import numpy as np\n\n#np.random.seed(123)\n#rnd = 1.0*np.random.randint(1,2**32)/2**32\n#print(rnd)\n'"
tmp/rename_casia_directories.py,0,"b'import shutil\nimport argparse\nimport os\nimport sys\n\ndef main(args):\n  \n    identity_map = {}\n    with open(os.path.expanduser(args.map_file_name), ""r"") as f:\n        for line in f:\n            fields = line.split(\' \')\n            dir_name = fields[0]\n            class_name = fields[1].replace(\'\\n\', \'\').replace(\'\\r\', \'\')\n            if class_name not in identity_map.values():\n                identity_map[dir_name] = class_name\n            else:\n                print(\'Duplicate class names: %s\' % class_name)\n            \n    dataset_path_exp = os.path.expanduser(args.dataset_path)\n    dirs = os.listdir(dataset_path_exp)\n    for f in dirs:\n        old_path = os.path.join(dataset_path_exp, f)\n        if f in identity_map:\n            new_path = os.path.join(dataset_path_exp, identity_map[f])\n            if os.path.isdir(old_path):\n                print(\'Renaming %s to %s\' % (old_path, new_path))\n                shutil.move(old_path, new_path)\n\ndef parse_arguments(argv):\n    parser = argparse.ArgumentParser()\n    \n    parser.add_argument(\'map_file_name\', type=str, help=\'Name of the text file that contains the directory to class name mappings.\')\n    parser.add_argument(\'dataset_path\', type=str, help=\'Path to the dataset directory.\')\n    return parser.parse_args(argv)\n\nif __name__ == \'__main__\':\n    main(parse_arguments(sys.argv[1:]))\n'"
tmp/seed_test.py,32,"b'import tensorflow as tf\nimport numpy as np\nimport sys\nimport time\nsys.path.append(\'../src\')\nimport facenet\nfrom tensorflow.python.ops import control_flow_ops\nfrom tensorflow.python.ops import array_ops\n\nfrom six.moves import xrange\n\ntf.app.flags.DEFINE_integer(\'batch_size\', 90,\n                            """"""Number of images to process in a batch."""""")\ntf.app.flags.DEFINE_integer(\'image_size\', 96,\n                            """"""Image size (height, width) in pixels."""""")\ntf.app.flags.DEFINE_float(\'alpha\', 0.2,\n                          """"""Positive to negative triplet distance margin."""""")\ntf.app.flags.DEFINE_float(\'learning_rate\', 0.1,\n                          """"""Initial learning rate."""""")\ntf.app.flags.DEFINE_float(\'moving_average_decay\', 0.9999,\n                          """"""Expontential decay for tracking of training parameters."""""")\n\nFLAGS = tf.app.flags.FLAGS\n\ndef run_train():\n  \n  with tf.Graph().as_default():\n  \n    # Set the seed for the graph\n    tf.set_random_seed(666)\n\n    # Placeholder for input images\n    images_placeholder = tf.placeholder(tf.float32, shape=(FLAGS.batch_size, FLAGS.image_size, FLAGS.image_size, 3), name=\'input\')\n    \n    # Build the inference graph\n    embeddings = inference_conv_test(images_placeholder)\n    #embeddings = inference_affine_test(images_placeholder)\n    \n    # Split example embeddings into anchor, positive and negative\n    anchor, positive, negative = tf.split(0, 3, embeddings)\n\n    # Alternative implementation of the split operation\n    # This produces the same error\n    #resh1 = tf.reshape(embeddings, [3,int(FLAGS.batch_size/3), 128])\n    #anchor = resh1[0,:,:]\n    #positive = resh1[1,:,:]\n    #negative = resh1[2,:,:]\n    \n    # Calculate triplet loss\n    pos_dist = tf.reduce_sum(tf.square(tf.sub(anchor, positive)), 1)\n    neg_dist = tf.reduce_sum(tf.square(tf.sub(anchor, negative)), 1)\n    basic_loss = tf.add(tf.sub(pos_dist,neg_dist), FLAGS.alpha)\n    loss = tf.reduce_mean(tf.maximum(basic_loss, 0.0), 0)\n\n    # Build a Graph that trains the model with one batch of examples and updates the model parameters\n    opt = tf.train.GradientDescentOptimizer(FLAGS.learning_rate)\n    #opt = tf.train.AdagradOptimizer(FLAGS.learning_rate)  # Optimizer does not seem to matter\n    grads = opt.compute_gradients(loss)\n    train_op = opt.apply_gradients(grads)\n    \n    # Initialize the variables\n    init = tf.global_variables_initializer()\n    \n    # Launch the graph.\n    sess = tf.Session()\n    sess.run(init)\n\n    # Set the numpy seed\n    np.random.seed(666)\n    \n    with sess.as_default():\n      grads_eval = []\n      all_vars = []\n      for step in xrange(1):\n        # Generate some random input data\n        batch = np.random.random((FLAGS.batch_size, FLAGS.image_size, FLAGS.image_size, 3))\n        feed_dict = { images_placeholder: batch }\n        # Get the variables\n        var_names = tf.global_variables()\n        all_vars  += sess.run(var_names, feed_dict=feed_dict)\n        # Get the gradients\n        grad_tensors, grad_vars = zip(*grads)\n        grads_eval  += sess.run(grad_tensors, feed_dict=feed_dict)\n        # Run training\n        sess.run(train_op, feed_dict=feed_dict)\n    \n    sess.close()\n  return (var_names, all_vars, grad_vars, grads_eval)\n\ndef _conv(inpOp, nIn, nOut, kH, kW, dH, dW, padType):\n  kernel = tf.Variable(tf.truncated_normal([kH, kW, nIn, nOut],\n                                           dtype=tf.float32,\n                                           stddev=1e-1), name=\'weights\')\n  conv = tf.nn.conv2d(inpOp, kernel, [1, dH, dW, 1], padding=padType)\n  \n  biases = tf.Variable(tf.constant(0.0, shape=[nOut], dtype=tf.float32),\n                       trainable=True, name=\'biases\')\n  bias = tf.reshape(tf.nn.bias_add(conv, biases), conv.get_shape())\n  conv1 = tf.nn.relu(bias)\n  return conv1\n\ndef _affine(inpOp, nIn, nOut):\n  kernel = tf.Variable(tf.truncated_normal([nIn, nOut],\n                                           dtype=tf.float32,\n                                           stddev=1e-1), name=\'weights\')\n  biases = tf.Variable(tf.constant(0.0, shape=[nOut], dtype=tf.float32),\n                       trainable=True, name=\'biases\')\n  affine1 = tf.nn.relu_layer(inpOp, kernel, biases)\n  return affine1\n  \ndef inference_conv_test(images):\n  conv1 = _conv(images, 3, 64, 7, 7, 2, 2, \'SAME\')\n  resh1 = tf.reshape(conv1, [-1, 147456])\n  affn = _affine(resh1, 147456, 128)  # Affine layer not needed to reproduce the error\n  return affn\n\ndef inference_affine_test(images):\n  resh1 = tf.reshape(images, [-1, 27648])\n  affn1 = _affine(resh1, 27648, 1024)\n  affn2 = _affine(affn1, 1024, 1024)\n  affn3 = _affine(affn2, 1024, 1024)\n  affn4 = _affine(affn3, 1024, 128)\n  return affn4\n\n# Run two sessions with the same seed. These runs should produce the same result.\nvar_names1, all_vars1, grad_names1, all_grads1 = run_train()\nvar_names2, all_vars2, grad_names2, all_grads2 = run_train()\n\nall_vars_close = [None] * len(all_vars1)\nfor i in range(len(all_vars1)):\n  all_vars_close[i] = np.allclose(all_vars1[i], all_vars2[i], rtol=1.e-16)\n  print(\'%d var %s: %s\' % (i, var_names1[i].op.name, all_vars_close[i]))\n  \nall_grads_close = [None] * len(all_grads1)\nfor i in range(len(all_grads1)):\n  all_grads_close[i] = np.allclose(all_grads1[i], all_grads2[i], rtol=1.e-16)\n  print(\'%d grad %s: %s\' % (i, grad_names1[i].op.name, all_grads_close[i]))\n\nassert all(all_vars_close), \'Variable values differ between the two sessions (with the same seed)\'\nassert all(all_grads_close), \'Gradient values differ between the two sessions (with the same seed)\'\n'"
tmp/select_triplets_test.py,3,"b'import facenet\nimport numpy as np\nimport tensorflow as tf\n\nFLAGS = tf.app.flags.FLAGS\n\ntf.app.flags.DEFINE_integer(\'people_per_batch\', 45,\n                            """"""Number of people per batch."""""")\ntf.app.flags.DEFINE_integer(\'alpha\', 0.2,\n                            """"""Positive to negative triplet distance margin."""""")\n\n\nembeddings = np.zeros((1800,128))\n\nnp.random.seed(123)\nfor ix in range(embeddings.shape[0]):\n    for jx in range(embeddings.shape[1]):\n        rnd = 1.0*np.random.randint(1,2**32)/2**32\n        embeddings[ix][jx] = rnd\n\n\nemb_array = embeddings\nimage_data = np.zeros((1800,96,96,3))\n\n\nnum_per_class = [40 for i in range(45)]\n\n\nnp.random.seed(123)\napn, nrof_random_negs, nrof_triplets = facenet.select_triplets(emb_array, num_per_class, image_data)\n'"
tmp/test1.py,0,"b""print('Hello world')\n"""
tmp/test_align.py,0,"b""import facenet\nimport os\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n\ndef main():\n    image_size = 96\n    old_dataset = '/home/david/datasets/facescrub/fs_aligned_new_oean/'\n    new_dataset = '/home/david/datasets/facescrub/facescrub_110_96/'\n    eq = 0\n    num = 0\n    l = []\n    dataset = facenet.get_dataset(old_dataset)\n    for cls in dataset:\n        new_class_dir = os.path.join(new_dataset, cls.name)\n        for image_path in cls.image_paths:\n          try:\n            filename = os.path.splitext(os.path.split(image_path)[1])[0]\n            new_filename = os.path.join(new_class_dir, filename+'.png')\n            #print(image_path)\n            if os.path.exists(new_filename):\n                a = facenet.load_data([image_path, new_filename], False, False, image_size, do_prewhiten=False)\n                if np.array_equal(a[0], a[1]):\n                  eq+=1\n                num+=1\n                err = np.sum(np.square(np.subtract(a[0], a[1])))\n                #print(err)\n                l.append(err)\n                if err>2000:\n                  fig = plt.figure(1)\n                  p1 = fig.add_subplot(121)\n                  p1.imshow(a[0])\n                  p2 = fig.add_subplot(122)\n                  p2.imshow(a[1])\n                  print('%6.1f: %s\\n' % (err, new_filename))\n                  pass\n            else:\n                pass\n                #print('File not found: %s' % new_filename)\n          except:\n            pass\nif __name__ == '__main__':\n    main()\n"""
tmp/test_invariance_on_lfw.py,5,"b'""""""Test invariance to translation, scaling and rotation on the ""Labeled Faces in the Wild"" dataset (http://vis-www.cs.umass.edu/lfw/).\nThis requires test images to be cropped a bit wider than the normal to give some room for the transformations.\n""""""\n# MIT License\n# \n# Copyright (c) 2016 David Sandberg\n# \n# Permission is hereby granted, free of charge, to any person obtaining a copy\n# of this software and associated documentation files (the ""Software""), to deal\n# in the Software without restriction, including without limitation the rights\n# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n# copies of the Software, and to permit persons to whom the Software is\n# furnished to do so, subject to the following conditions:\n# \n# The above copyright notice and this permission notice shall be included in all\n# copies or substantial portions of the Software.\n# \n# THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n# SOFTWARE.\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\nimport numpy as np\nimport argparse\nimport facenet\nimport lfw\nimport matplotlib.pyplot as plt\nfrom scipy import misc\nimport os\nimport sys\nimport math\n\ndef main(args):\n    \n    pairs = lfw.read_pairs(os.path.expanduser(args.lfw_pairs))\n    paths, actual_issame = lfw.get_paths(os.path.expanduser(args.lfw_dir), pairs)\n    result_dir = \'../data/\'\n    plt.ioff()  # Disable interactive plotting mode\n    \n    with tf.Graph().as_default():\n\n        with tf.Session() as sess:\n    \n            # Load the model\n            print(\'Loading model ""%s""\' % args.model_file)\n            facenet.load_model(args.model_file)\n            \n            # Get input and output tensors\n            images_placeholder = tf.get_default_graph().get_tensor_by_name(""input:0"")\n            phase_train_placeholder = tf.get_default_graph().get_tensor_by_name(""phase_train:0"")\n            embeddings = tf.get_default_graph().get_tensor_by_name(""embeddings:0"")\n            image_size = int(images_placeholder.get_shape()[1])\n            \n            # Run test on LFW to check accuracy for different horizontal/vertical translations of input images\n            if args.nrof_offsets>0:\n                step = 3\n                offsets = np.asarray([x*step for x in range(-args.nrof_offsets//2+1, args.nrof_offsets//2+1)])\n                horizontal_offset_accuracy = [None] * len(offsets)\n                for idx, offset in enumerate(offsets):\n                    accuracy = evaluate_accuracy(sess, images_placeholder, phase_train_placeholder, image_size, embeddings, \n                        paths, actual_issame, translate_images, (offset,0), 60, args.orig_image_size, args.seed)\n                    print(\'Hoffset: %1.3f  Accuracy: %1.3f+-%1.3f\' % (offset, np.mean(accuracy), np.std(accuracy)))\n                    horizontal_offset_accuracy[idx] = np.mean(accuracy)\n                vertical_offset_accuracy = [None] * len(offsets)\n                for idx, offset in enumerate(offsets):\n                    accuracy = evaluate_accuracy(sess, images_placeholder, phase_train_placeholder, image_size, embeddings, \n                        paths, actual_issame, translate_images, (0,offset), 60, args.orig_image_size, args.seed)\n                    print(\'Voffset: %1.3f  Accuracy: %1.3f+-%1.3f\' % (offset, np.mean(accuracy), np.std(accuracy)))\n                    vertical_offset_accuracy[idx] = np.mean(accuracy)\n                fig = plt.figure(1)\n                plt.plot(offsets, horizontal_offset_accuracy, label=\'Horizontal\')\n                plt.plot(offsets, vertical_offset_accuracy, label=\'Vertical\')\n                plt.legend()\n                plt.grid(True)\n                plt.title(\'Translation invariance on LFW\')\n                plt.xlabel(\'Offset [pixels]\')\n                plt.ylabel(\'Accuracy\')\n#                plt.show()\n                print(\'Saving results in %s\' % result_dir)\n                fig.savefig(os.path.join(result_dir, \'invariance_translation.png\'))\n                save_result(offsets, horizontal_offset_accuracy, os.path.join(result_dir, \'invariance_translation_horizontal.txt\'))\n                save_result(offsets, vertical_offset_accuracy, os.path.join(result_dir, \'invariance_translation_vertical.txt\'))\n\n            # Run test on LFW to check accuracy for different rotation of input images\n            if args.nrof_angles>0:\n                step = 3\n                angles = np.asarray([x*step for x in range(-args.nrof_offsets//2+1, args.nrof_offsets//2+1)])\n                rotation_accuracy = [None] * len(angles)\n                for idx, angle in enumerate(angles):\n                    accuracy = evaluate_accuracy(sess, images_placeholder, phase_train_placeholder, image_size, embeddings, \n                        paths, actual_issame, rotate_images, angle, 60, args.orig_image_size, args.seed)\n                    print(\'Angle: %1.3f  Accuracy: %1.3f+-%1.3f\' % (angle, np.mean(accuracy), np.std(accuracy)))\n                    rotation_accuracy[idx] = np.mean(accuracy)\n                fig = plt.figure(2)\n                plt.plot(angles, rotation_accuracy)\n                plt.grid(True)\n                plt.title(\'Rotation invariance on LFW\')\n                plt.xlabel(\'Angle [deg]\')\n                plt.ylabel(\'Accuracy\')\n#                plt.show()\n                print(\'Saving results in %s\' % result_dir)\n                fig.savefig(os.path.join(result_dir, \'invariance_rotation.png\'))\n                save_result(angles, rotation_accuracy, os.path.join(result_dir, \'invariance_rotation.txt\'))\n\n            # Run test on LFW to check accuracy for different scaling of input images\n            if args.nrof_scales>0:\n                step = 0.05\n                scales = np.asarray([x*step+1 for x in range(-args.nrof_offsets//2+1, args.nrof_offsets//2+1)])\n                scale_accuracy = [None] * len(scales)\n                for scale_idx, scale in enumerate(scales):\n                    accuracy = evaluate_accuracy(sess, images_placeholder, phase_train_placeholder, image_size, embeddings, \n                        paths, actual_issame, scale_images, scale, 60, args.orig_image_size, args.seed)\n                    print(\'Scale: %1.3f  Accuracy: %1.3f+-%1.3f\' % (scale, np.mean(accuracy), np.std(accuracy)))\n                    scale_accuracy[scale_idx] = np.mean(accuracy)\n                fig = plt.figure(3)\n                plt.plot(scales, scale_accuracy)\n                plt.grid(True)\n                plt.title(\'Scale invariance on LFW\')\n                plt.xlabel(\'Scale\')\n                plt.ylabel(\'Accuracy\')\n#                plt.show()\n                print(\'Saving results in %s\' % result_dir)\n                fig.savefig(os.path.join(result_dir, \'invariance_scale.png\'))\n                save_result(scales, scale_accuracy, os.path.join(result_dir, \'invariance_scale.txt\'))\n                \ndef save_result(aug, acc, filename):\n    with open(filename, ""w"") as f:\n        for i in range(aug.size):\n            f.write(\'%6.4f %6.4f\\n\' % (aug[i], acc[i]))\n            \ndef evaluate_accuracy(sess, images_placeholder, phase_train_placeholder, image_size, embeddings, \n        paths, actual_issame, augment_images, aug_value, batch_size, orig_image_size, seed):\n    nrof_images = len(paths)\n    nrof_batches = int(math.ceil(1.0*nrof_images / batch_size))\n    emb_list = []\n    for i in range(nrof_batches):\n        start_index = i*batch_size\n        end_index = min((i+1)*batch_size, nrof_images)\n        paths_batch = paths[start_index:end_index]\n        images = facenet.load_data(paths_batch, False, False, orig_image_size)\n        images_aug = augment_images(images, aug_value, image_size)\n        feed_dict = { images_placeholder: images_aug, phase_train_placeholder: False }\n        emb_list += sess.run([embeddings], feed_dict=feed_dict)\n    emb_array = np.vstack(emb_list)  # Stack the embeddings to a nrof_examples_per_epoch x 128 matrix\n    \n    thresholds = np.arange(0, 4, 0.01)\n    embeddings1 = emb_array[0::2]\n    embeddings2 = emb_array[1::2]\n    _, _, accuracy = facenet.calculate_roc(thresholds, embeddings1, embeddings2, np.asarray(actual_issame), seed)\n    return accuracy\n\ndef scale_images(images, scale, image_size):\n    images_scale_list = [None] * images.shape[0]\n    for i in range(images.shape[0]):\n        images_scale_list[i] = misc.imresize(images[i,:,:,:], scale)\n    images_scale = np.stack(images_scale_list,axis=0)\n    sz1 = images_scale.shape[1]/2\n    sz2 = image_size/2\n    images_crop = images_scale[:,(sz1-sz2):(sz1+sz2),(sz1-sz2):(sz1+sz2),:]\n    return images_crop\n\ndef rotate_images(images, angle, image_size):\n    images_list = [None] * images.shape[0]\n    for i in range(images.shape[0]):\n        images_list[i] = misc.imrotate(images[i,:,:,:], angle)\n    images_rot = np.stack(images_list,axis=0)\n    sz1 = images_rot.shape[1]/2\n    sz2 = image_size/2\n    images_crop = images_rot[:,(sz1-sz2):(sz1+sz2),(sz1-sz2):(sz1+sz2),:]\n    return images_crop\n\ndef translate_images(images, offset, image_size):\n    h, v = offset\n    sz1 = images.shape[1]/2\n    sz2 = image_size/2\n    images_crop = images[:,(sz1-sz2+v):(sz1+sz2+v),(sz1-sz2+h):(sz1+sz2+h),:]\n    return images_crop\n\ndef parse_arguments(argv):\n    parser = argparse.ArgumentParser()\n    \n    parser.add_argument(\'--model_file\', type=str, \n        help=\'File containing the model parameters as well as the model metagraph (with extension "".meta"")\', \n        default=\'~/models/facenet/20160514-234418/model.ckpt-500000\')\n    parser.add_argument(\'--nrof_offsets\', type=int,\n        help=\'Number of horizontal and vertical offsets to evaluate.\', default=21)\n    parser.add_argument(\'--nrof_angles\', type=int,\n        help=\'Number of angles to evaluate.\', default=21)\n    parser.add_argument(\'--nrof_scales\', type=int,\n        help=\'Number of scales to evaluate.\', default=21)\n    parser.add_argument(\'--lfw_pairs\', type=str,\n        help=\'The file containing the pairs to use for validation.\', default=\'../data/pairs.txt\')\n    parser.add_argument(\'--lfw_dir\', type=str,\n        help=\'Path to the data directory containing aligned face patches.\', default=\'~/datasets/lfw/lfw_realigned/\')\n    parser.add_argument(\'--orig_image_size\', type=int,\n        help=\'Image size (height, width) in pixels of the original (uncropped/unscaled) images.\', default=224)\n    parser.add_argument(\'--lfw_nrof_folds\', type=int,\n        help=\'Number of folds to use for cross validation. Mainly used for testing.\', default=10)\n    parser.add_argument(\'--seed\', type=int,\n        help=\'Random seed.\', default=666)\n    return parser.parse_args(argv)\n\n\nif __name__ == \'__main__\':\n    main(parse_arguments(sys.argv[1:]))\n'"
tmp/vggface16.py,39,"b'""""""Load the VGG Face model into TensorFlow.\nDownload the model from http://www.robots.ox.ac.uk/~vgg/software/vgg_face/\nand point to the file \'vgg_face.mat\'\n""""""\nimport numpy as np\nfrom scipy import io\nimport tensorflow as tf\n\ndef load(filename, images):\n    #filename = \'../data/vgg_face_matconvnet/data/vgg_face.mat\'\n    vgg16 = io.loadmat(filename)\n    vgg16Layers = vgg16[\'net\'][0][0][\'layers\']\n    \n    # A function to get the weights of the VGG layers\n    def vbbWeights(layerNumber):\n        W = vgg16Layers[0][layerNumber][0][0][2][0][0]\n        W = tf.constant(W)\n        return W\n     \n    def vbbConstants(layerNumber):\n        b = vgg16Layers[0][layerNumber][0][0][2][0][1].T\n        b = tf.constant(np.reshape(b, (b.size)))\n        return b\n    \n    modelGraph = {}\n    modelGraph[\'input\'] = images\n    \n    modelGraph[\'conv1_1\'] = tf.nn.conv2d(modelGraph[\'input\'], filter = vbbWeights(0), strides = [1, 1, 1, 1], padding = \'SAME\')\n    modelGraph[\'relu1_1\'] = tf.nn.relu(modelGraph[\'conv1_1\'] + vbbConstants(0))\n    modelGraph[\'conv1_2\'] = tf.nn.conv2d(modelGraph[\'relu1_1\'], filter = vbbWeights(2), strides = [1, 1, 1, 1], padding = \'SAME\')\n    modelGraph[\'relu1_2\'] = tf.nn.relu(modelGraph[\'conv1_2\'] + vbbConstants(2))\n    modelGraph[\'pool1\'] = tf.nn.max_pool(modelGraph[\'relu1_2\'], ksize = [1, 2, 2, 1], strides = [1, 2, 2, 1], padding = \'SAME\')\n    \n    modelGraph[\'conv2_1\'] = tf.nn.conv2d(modelGraph[\'pool1\'], filter = vbbWeights(5), strides = [1, 1, 1, 1], padding = \'SAME\')\n    modelGraph[\'relu2_1\'] = tf.nn.relu(modelGraph[\'conv2_1\'] + vbbConstants(5))\n    modelGraph[\'conv2_2\'] = tf.nn.conv2d(modelGraph[\'relu2_1\'], filter = vbbWeights(7), strides = [1, 1, 1, 1], padding = \'SAME\')\n    modelGraph[\'relu2_2\'] = tf.nn.relu(modelGraph[\'conv2_2\'] + vbbConstants(7))\n    modelGraph[\'pool2\'] = tf.nn.max_pool(modelGraph[\'relu2_2\'], ksize = [1, 2, 2, 1], strides = [1, 2, 2, 1], padding = \'SAME\')\n    \n    modelGraph[\'conv3_1\'] = tf.nn.conv2d(modelGraph[\'pool2\'], filter = vbbWeights(10), strides = [1, 1, 1, 1], padding = \'SAME\')\n    modelGraph[\'relu3_1\'] = tf.nn.relu(modelGraph[\'conv3_1\'] + vbbConstants(10))\n    modelGraph[\'conv3_2\'] = tf.nn.conv2d(modelGraph[\'relu3_1\'], filter = vbbWeights(12), strides = [1, 1, 1, 1], padding = \'SAME\')\n    modelGraph[\'relu3_2\'] = tf.nn.relu(modelGraph[\'conv3_2\'] + vbbConstants(12))\n    modelGraph[\'conv3_3\'] = tf.nn.conv2d(modelGraph[\'relu3_2\'], filter = vbbWeights(14), strides = [1, 1, 1, 1], padding = \'SAME\')\n    modelGraph[\'relu3_3\'] = tf.nn.relu(modelGraph[\'conv3_3\'] + vbbConstants(14))\n    modelGraph[\'pool3\'] = tf.nn.max_pool(modelGraph[\'relu3_3\'], ksize = [1, 2, 2, 1], strides = [1, 2, 2, 1], padding = \'SAME\')\n    \n    modelGraph[\'conv4_1\'] = tf.nn.conv2d(modelGraph[\'pool3\'], filter = vbbWeights(17), strides = [1, 1, 1, 1], padding = \'SAME\')\n    modelGraph[\'relu4_1\'] = tf.nn.relu(modelGraph[\'conv4_1\'] + vbbConstants(17))\n    modelGraph[\'conv4_2\'] = tf.nn.conv2d(modelGraph[\'relu4_1\'], filter = vbbWeights(19), strides = [1, 1, 1, 1], padding = \'SAME\')\n    modelGraph[\'relu4_2\'] = tf.nn.relu(modelGraph[\'conv4_2\'] + vbbConstants(19))\n    modelGraph[\'conv4_3\'] = tf.nn.conv2d(modelGraph[\'relu4_2\'], filter = vbbWeights(21), strides = [1, 1, 1, 1], padding = \'SAME\')\n    modelGraph[\'relu4_3\'] = tf.nn.relu(modelGraph[\'conv4_3\'] + vbbConstants(21))\n    modelGraph[\'pool4\'] = tf.nn.max_pool(modelGraph[\'relu4_3\'], ksize = [1, 2, 2, 1], strides = [1, 2, 2, 1], padding = \'SAME\')\n    \n    modelGraph[\'conv5_1\'] = tf.nn.conv2d(modelGraph[\'pool4\'], filter = vbbWeights(24), strides = [1, 1, 1, 1], padding = \'SAME\')\n    modelGraph[\'relu5_1\'] = tf.nn.relu(modelGraph[\'conv5_1\'] + vbbConstants(24))\n    modelGraph[\'conv5_2\'] = tf.nn.conv2d(modelGraph[\'relu5_1\'], filter = vbbWeights(26), strides = [1, 1, 1, 1], padding = \'SAME\')\n    modelGraph[\'relu5_2\'] = tf.nn.relu(modelGraph[\'conv5_2\'] + vbbConstants(26))\n    modelGraph[\'conv5_3\'] = tf.nn.conv2d(modelGraph[\'relu5_2\'], filter = vbbWeights(28), strides = [1, 1, 1, 1], padding = \'SAME\')\n    modelGraph[\'relu5_3\'] = tf.nn.relu(modelGraph[\'conv5_3\'] + vbbConstants(28))\n    modelGraph[\'pool5\'] = tf.nn.max_pool(modelGraph[\'relu5_3\'], ksize = [1, 2, 2, 1], strides = [1, 2, 2, 1], padding = \'SAME\')\n    \n    modelGraph[\'resh1\'] = tf.reshape(modelGraph[\'pool5\'], [-1, 25088])\n    modelGraph[\'fc6\'] = tf.nn.relu_layer(modelGraph[\'resh1\'], tf.reshape(vbbWeights(31), [25088, 4096]), vbbConstants(31))\n    modelGraph[\'dropout1\'] = tf.nn.dropout(modelGraph[\'fc6\'], 0.5)\n    modelGraph[\'fc7\'] = tf.nn.relu_layer(modelGraph[\'dropout1\'], tf.squeeze(vbbWeights(34), [0, 1]), vbbConstants(34))\n    modelGraph[\'dropout2\'] = tf.nn.dropout(modelGraph[\'fc7\'], 0.5)\n    modelGraph[\'fc8\'] = tf.nn.relu_layer(modelGraph[\'dropout2\'], tf.squeeze(vbbWeights(37), [0, 1]), vbbConstants(37))\n\n    return modelGraph\n'"
tmp/vggverydeep19.py,23,"b'""""""Load the VGG imagenet model into TensorFlow.\nDownload the model from http://www.robots.ox.ac.uk/~vgg/research/very_deep/\nand point to the file \'imagenet-vgg-verydeep-19.mat\'\n""""""\nimport numpy as np\nfrom scipy import io\nimport tensorflow as tf\n\ndef load(filename, images):\n    vgg19 = io.loadmat(filename)\n    vgg19Layers = vgg19[\'layers\']\n    \n    # A function to get the weights of the VGG layers\n    def vbbWeights(layerNumber):\n        W = vgg19Layers[0][layerNumber][0][0][2][0][0]\n        W = tf.constant(W)\n        return W\n      \n    def vbbConstants(layerNumber):\n        b = vgg19Layers[0][layerNumber][0][0][2][0][1].T\n        b = tf.constant(np.reshape(b, (b.size)))\n        return b\n    \n    modelGraph = {}\n    modelGraph[\'input\'] = images\n    modelGraph[\'conv1_1\'] = tf.nn.relu(tf.nn.conv2d(modelGraph[\'input\'], filter = vbbWeights(0), strides = [1, 1, 1, 1], padding = \'SAME\') + vbbConstants(0))\n    modelGraph[\'conv1_2\'] = tf.nn.relu(tf.nn.conv2d(modelGraph[\'conv1_1\'], filter = vbbWeights(2), strides = [1, 1, 1, 1], padding = \'SAME\') + vbbConstants(2))\n    modelGraph[\'avgpool1\'] = tf.nn.avg_pool(modelGraph[\'conv1_2\'], ksize = [1, 2, 2, 1], strides = [1, 2, 2, 1], padding = \'SAME\')\n    modelGraph[\'conv2_1\'] = tf.nn.relu(tf.nn.conv2d(modelGraph[\'avgpool1\'], filter = vbbWeights(5), strides = [1, 1, 1, 1], padding = \'SAME\') + vbbConstants(5))\n    modelGraph[\'conv2_2\'] = tf.nn.relu(tf.nn.conv2d(modelGraph[\'conv2_1\'], filter = vbbWeights(7), strides = [1, 1, 1, 1], padding = \'SAME\') + vbbConstants(7))\n    modelGraph[\'avgpool2\'] = tf.nn.avg_pool(modelGraph[\'conv2_2\'], ksize = [1, 2, 2, 1], strides = [1, 2, 2, 1], padding = \'SAME\')\n    modelGraph[\'conv3_1\'] = tf.nn.relu(tf.nn.conv2d(modelGraph[\'avgpool2\'], filter = vbbWeights(10), strides = [1, 1, 1, 1], padding = \'SAME\') + vbbConstants(10))\n    modelGraph[\'conv3_2\'] = tf.nn.relu(tf.nn.conv2d(modelGraph[\'conv3_1\'], filter = vbbWeights(12), strides = [1, 1, 1, 1], padding = \'SAME\') + vbbConstants(12))\n    modelGraph[\'conv3_3\'] = tf.nn.relu(tf.nn.conv2d(modelGraph[\'conv3_2\'], filter = vbbWeights(14), strides = [1, 1, 1, 1], padding = \'SAME\') + vbbConstants(14))\n    modelGraph[\'conv3_4\'] = tf.nn.relu(tf.nn.conv2d(modelGraph[\'conv3_3\'], filter = vbbWeights(16), strides = [1, 1, 1, 1], padding = \'SAME\') + vbbConstants(16))\n    modelGraph[\'avgpool3\'] = tf.nn.avg_pool(modelGraph[\'conv3_4\'], ksize = [1, 2, 2, 1], strides = [1, 2, 2, 1], padding = \'SAME\')\n    modelGraph[\'conv4_1\'] = tf.nn.relu(tf.nn.conv2d(modelGraph[\'avgpool3\'], filter = vbbWeights(19), strides = [1, 1, 1, 1], padding = \'SAME\') + vbbConstants(19))\n    modelGraph[\'conv4_2\'] = tf.nn.relu(tf.nn.conv2d(modelGraph[\'conv4_1\'], filter = vbbWeights(21), strides = [1, 1, 1, 1], padding = \'SAME\') + vbbConstants(21))\n    modelGraph[\'conv4_3\'] = tf.nn.relu(tf.nn.conv2d(modelGraph[\'conv4_2\'], filter = vbbWeights(23), strides = [1, 1, 1, 1], padding = \'SAME\') + vbbConstants(23))\n    modelGraph[\'conv4_4\'] = tf.nn.relu(tf.nn.conv2d(modelGraph[\'conv4_3\'], filter = vbbWeights(25), strides = [1, 1, 1, 1], padding = \'SAME\') + vbbConstants(25))\n    modelGraph[\'avgpool4\'] = tf.nn.avg_pool(modelGraph[\'conv4_4\'], ksize = [1, 2, 2, 1], strides = [1, 2, 2, 1], padding = \'SAME\')\n    modelGraph[\'conv5_1\'] = tf.nn.relu(tf.nn.conv2d(modelGraph[\'avgpool4\'], filter = vbbWeights(28), strides = [1, 1, 1, 1], padding = \'SAME\') + vbbConstants(28))\n    modelGraph[\'conv5_2\'] = tf.nn.relu(tf.nn.conv2d(modelGraph[\'conv5_1\'], filter = vbbWeights(30), strides = [1, 1, 1, 1], padding = \'SAME\') + vbbConstants(30))\n    modelGraph[\'conv5_3\'] = tf.nn.relu(tf.nn.conv2d(modelGraph[\'conv5_2\'], filter = vbbWeights(32), strides = [1, 1, 1, 1], padding = \'SAME\') + vbbConstants(32))\n    modelGraph[\'conv5_4\'] = tf.nn.relu(tf.nn.conv2d(modelGraph[\'conv5_3\'], filter = vbbWeights(34), strides = [1, 1, 1, 1], padding = \'SAME\') + vbbConstants(34))\n    modelGraph[\'avgpool5\'] = tf.nn.avg_pool(modelGraph[\'conv5_4\'], ksize = [1, 2, 2, 1], strides = [1, 2, 2, 1], padding = \'SAME\')\n    \n    return modelGraph\n\n'"
tmp/visualize.py,8,"b'""""""Visualize individual feature channels and their combinations to explore the space of patterns learned by the neural network\nBased on http://nbviewer.jupyter.org/github/tensorflow/tensorflow/blob/master/tensorflow/examples/tutorials/deepdream/deepdream.ipynb\n""""""\n# MIT License\n# \n# Copyright (c) 2016 David Sandberg\n# \n# Permission is hereby granted, free of charge, to any person obtaining a copy\n# of this software and associated documentation files (the ""Software""), to deal\n# in the Software without restriction, including without limitation the rights\n# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n# copies of the Software, and to permit persons to whom the Software is\n# furnished to do so, subject to the following conditions:\n# \n# The above copyright notice and this permission notice shall be included in all\n# copies or substantial portions of the Software.\n# \n# THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n# SOFTWARE.\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport os\nimport numpy as np\nimport sys\nimport argparse\nimport tensorflow as tf\nimport importlib\nfrom scipy import misc\n\ndef main(args):\n  \n    network = importlib.import_module(args.model_def, \'inference\')\n\n    # Start with a gray image with a little noise\n    np.random.seed(seed=args.seed)\n    img_noise = np.random.uniform(size=(args.image_size,args.image_size,3)) + 100.0\n  \n    sess = tf.Session()\n  \n    t_input = tf.placeholder(np.float32, shape=(args.image_size,args.image_size,3), name=\'input\') # define the input tensor\n    image_mean = 117.0\n    t_preprocessed = tf.expand_dims(t_input-image_mean, 0)\n     \n    # Build the inference graph\n    network.inference(t_preprocessed, 1.0, \n            phase_train=True, weight_decay=0.0)\n      \n    # Create a saver for restoring variables\n    saver = tf.train.Saver(tf.global_variables())\n  \n    # Restore the parameters\n    saver.restore(sess, args.model_file)\n  \n    layers = [op.name for op in tf.get_default_graph().get_operations() if op.type==\'Conv2D\']\n    feature_nums = {layer: int(T(layer).get_shape()[-1]) for layer in layers}\n  \n    print(\'Number of layers: %d\' % len(layers))\n  \n    for layer in sorted(feature_nums.keys()):\n        print(\'%s%d\' % ((layer+\': \').ljust(40), feature_nums[layer]))\n  \n    # Picking some internal layer. Note that we use outputs before applying the ReLU nonlinearity\n    # to have non-zero gradients for features with negative initial activations.\n    layer = \'InceptionResnetV1/Repeat_2/block8_3/Conv2d_1x1/Conv2D\'\n    #layer = \'incept4b/in4_conv1x1_31/Conv2D\'\n    result_dir = \'../data/\'\n    print(\'Number of features in layer ""%s"": %d\' % (layer, feature_nums[layer]))\n    channels = range(feature_nums[layer])\n    np.random.shuffle(channels)\n    for i in range(32):\n        print(\'Rendering feature %d\' % channels[i])\n        channel = channels[i]\n        img = render_naive(sess, t_input, T(layer)[:,:,:,channel], img_noise)\n        filename = \'%s_%03d.png\' % (layer.replace(\'/\', \'_\'), channel)\n        misc.imsave(os.path.join(result_dir, filename), img)\n  \n\ndef T(layer):\n    \'\'\'Helper for getting layer output tensor\'\'\'\n    return tf.get_default_graph().get_tensor_by_name(\'%s:0\' % layer)\n\ndef visstd(a, s=0.1):\n    \'\'\'Normalize the image range for visualization\'\'\'\n    return (a-a.mean())/max(a.std(), 1e-4)*s + 0.5\n\ndef render_naive(sess, t_input, t_obj, img0, iter_n=20, step=1.0):\n    t_score = tf.reduce_mean(t_obj) # defining the optimization objective\n    t_grad = tf.gradients(t_score, t_input)[0] # behold the power of automatic differentiation!\n    \n    img = img0.copy()\n    for _ in range(iter_n):\n        g, _ = sess.run([t_grad, t_score], {t_input:img})\n        # normalizing the gradient, so the same step size should work \n        g /= g.std()+1e-8         # for different layers and networks\n        img += g*step\n    return visstd(img)\n\ndef parse_arguments(argv):\n    parser = argparse.ArgumentParser()\n    \n    parser.add_argument(\'model_file\', type=str, \n        help=\'Directory containing the graph definition and checkpoint files.\')\n    parser.add_argument(\'--model_def\', type=str, \n        help=\'Model definition. Points to a module containing the definition of the inference graph.\',\n        default=\'models.nn4\')\n    parser.add_argument(\'--image_size\', type=int,\n        help=\'Image size (height, width) in pixels.\', default=96)\n    parser.add_argument(\'--seed\', type=int,\n        help=\'Random seed.\', default=666)\n    return parser.parse_args(argv)\n\nif __name__ == \'__main__\':\n    main(parse_arguments(sys.argv[1:]))\n'"
tmp/visualize_vgg_model.py,9,"b'import numpy as np\nfrom scipy import misc\nimport tensorflow as tf\nfrom matplotlib import pyplot, image\nimport vggverydeep19\n\npaintingStyleImage = image.imread(""../data/schoolofathens.jpg"")\npyplot.imshow(paintingStyleImage)\n\ninputImage = image.imread(""../data/grandcentral.jpg"")\npyplot.imshow(inputImage)\n\noutputWidth = 800\noutputHeight = 600\n\n# Beta constant \nbeta = 5\n# Alpha constant\nalpha = 100\n# Noise ratio\nnoiseRatio = 0.6\n\nnodes = vggverydeep19.load(\'../data/imagenet-vgg-verydeep-19.mat\', (600, 800))\n\n# Mean VGG-19 image\nmeanImage19 = np.array([103.939, 116.779, 123.68]).reshape((1,1,1,3)) #pylint: disable=no-member\n\n\n\n# Squared-error loss of content between the two feature representations\ndef sqErrorLossContent(sess, modelGraph, layer):\n    p = session.run(modelGraph[layer])\n    #pylint: disable=maybe-no-member\n    N = p.shape[3]\n    M = p.shape[1] * p.shape[2]\n    return (1 / (4 * N * M)) * tf.reduce_sum(tf.pow(modelGraph[layer] - sess.run(modelGraph[layer]), 2))\n \n# Squared-error loss of style between the two feature representations\nstyleLayers = [\n    (\'conv1_1\', 0.2),\n    (\'conv2_1\', 0.2),\n    (\'conv3_1\', 0.2),\n    (\'conv4_1\', 0.2),\n    (\'conv5_1\', 0.2),\n]\ndef sqErrorLossStyle(sess, modelGraph):\n    def intermediateCalc(x, y):\n        N = x.shape[3]\n        M = x.shape[1] * x.shape[2]\n        A = tf.matmul(tf.transpose(tf.reshape(x, (M, N))), tf.reshape(x, (M, N)))\n        G = tf.matmul(tf.transpose(tf.reshape(y, (M, N))), tf.reshape(y, (M, N)))\n        return (1 / (4 * N**2 * M**2)) * tf.reduce_sum(tf.pow(G - A, 2))\n    E = [intermediateCalc(sess.run(modelGraph[layerName]), modelGraph[layerName]) for layerName, _ in styleLayers]\n    W = [w for _, w in styleLayers]\n    return sum([W[layerNumber] * E[layerNumber] for layerNumber in range(len(styleLayers))])\n\nsession = tf.InteractiveSession()\n \n# Addition of extra dimension to image\ninputImage = np.reshape(inputImage, ((1,) + inputImage.shape))\ninputImage = inputImage - meanImage19\n# Display image\npyplot.imshow(inputImage[0])\n\n# Addition of extra dimension to image\npaintingStyleImage = np.reshape(paintingStyleImage, ((1,) + paintingStyleImage.shape))\npaintingStyleImage = paintingStyleImage - meanImage19\n# Display image\npyplot.imshow(paintingStyleImage[0])\n\nimageNoise = np.random.uniform(-20, 20, (1, outputHeight, outputWidth, 3)).astype(\'float32\')\npyplot.imshow(imageNoise[0])\nmixedImage = imageNoise * noiseRatio + inputImage * (1 - noiseRatio)\npyplot.imshow(inputImage[0])\n\n\nsession.run(tf.global_variables_initializer())\nsession.run(nodes[\'input\'].assign(inputImage))\ncontentLoss = sqErrorLossContent(session, nodes, \'conv4_2\')\nsession.run(nodes[\'input\'].assign(paintingStyleImage))\nstyleLoss = sqErrorLossStyle(session, nodes)\ntotalLoss = beta * contentLoss + alpha * styleLoss\n\noptimizer = tf.train.AdamOptimizer(2.0)\ntrainStep = optimizer.minimize(totalLoss)\nsession.run(tf.global_variables_initializer())\nsession.run(nodes[\'input\'].assign(inputImage))\n# Number of iterations to run.\niterations = 2000\nsession.run(tf.global_variables_initializer())\nsession.run(nodes[\'input\'].assign(inputImage))\n \nfor iters in range(iterations):\n    session.run(trainStep)\n    if iters%50 == 0:\n        # Output every 50 iterations for animation       \n        filename = \'output%d.png\' % (iters)\n        im = mixedImage + meanImage19\n        im = im[0]\n        im = np.clip(im, 0, 255).astype(\'uint8\')\n        misc.imsave(filename, im)\n \nim = mixedImage + meanImage19\nim = im[0]\nim = np.clip(im, 0, 255).astype(\'uint8\')\nmisc.imsave(\'finalImage.png\', im)\n\n'"
tmp/visualize_vggface.py,5,"b""import numpy as np\nimport tensorflow as tf\nimport matplotlib.pyplot as plt\nimport tmp.vggface16\n\ndef main():\n  \n    sess = tf.Session()\n  \n    t_input = tf.placeholder(np.float32, name='input') # define the input tensor\n    image_mean = 117.0\n    t_preprocessed = tf.expand_dims(t_input-image_mean, 0)\n     \n    # Build the inference graph\n    nodes = tmp.vggface16.load('data/vgg_face.mat', t_preprocessed)\n        \n    img_noise = np.random.uniform(size=(224,224,3)) + 117.0\n\n    # Picking some internal layer. Note that we use outputs before applying the ReLU nonlinearity\n    # to have non-zero gradients for features with negative initial activations.\n    layer = 'conv5_3'\n    channel = 140 # picking some feature channel to visualize\n    img = render_naive(sess, t_input, nodes[layer][:,:,:,channel], img_noise)\n    showarray(img)\n\ndef showarray(a):\n    a = np.uint8(np.clip(a, 0, 1)*255)\n    plt.imshow(a)\n    plt.show()\n    \ndef visstd(a, s=0.1):\n    '''Normalize the image range for visualization'''\n    return (a-a.mean())/max(a.std(), 1e-4)*s + 0.5\n\ndef render_naive(sess, t_input, t_obj, img0, iter_n=20, step=1.0):\n    t_score = tf.reduce_mean(t_obj) # defining the optimization objective\n    t_grad = tf.gradients(t_score, t_input)[0] # behold the power of automatic differentiation!\n    \n    img = img0.copy()\n    for _ in range(iter_n):\n        g, _ = sess.run([t_grad, t_score], {t_input:img})\n        # normalizing the gradient, so the same step size should work \n        g /= g.std()+1e-8         # for different layers and networks\n        img += g*step\n    return visstd(img)\n\n  \nif __name__ == '__main__':\n    main()\n"""
src/align/__init__.py,0,b''
src/align/align_dataset_mtcnn.py,3,"b'""""""Performs face alignment and stores face thumbnails in the output directory.""""""\n# MIT License\n# \n# Copyright (c) 2016 David Sandberg\n# \n# Permission is hereby granted, free of charge, to any person obtaining a copy\n# of this software and associated documentation files (the ""Software""), to deal\n# in the Software without restriction, including without limitation the rights\n# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n# copies of the Software, and to permit persons to whom the Software is\n# furnished to do so, subject to the following conditions:\n# \n# The above copyright notice and this permission notice shall be included in all\n# copies or substantial portions of the Software.\n# \n# THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n# SOFTWARE.\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom scipy import misc\nimport sys\nimport os\nimport argparse\nimport tensorflow as tf\nimport numpy as np\nimport facenet\nimport align.detect_face\nimport random\nfrom time import sleep\n\ndef main(args):\n    sleep(random.random())\n    output_dir = os.path.expanduser(args.output_dir)\n    if not os.path.exists(output_dir):\n        os.makedirs(output_dir)\n    # Store some git revision info in a text file in the log directory\n    src_path,_ = os.path.split(os.path.realpath(__file__))\n    facenet.store_revision_info(src_path, output_dir, \' \'.join(sys.argv))\n    dataset = facenet.get_dataset(args.input_dir)\n    \n    print(\'Creating networks and loading parameters\')\n    \n    with tf.Graph().as_default():\n        gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=args.gpu_memory_fraction)\n        sess = tf.Session(config=tf.ConfigProto(gpu_options=gpu_options, log_device_placement=False))\n        with sess.as_default():\n            pnet, rnet, onet = align.detect_face.create_mtcnn(sess, None)\n    \n    minsize = 20 # minimum size of face\n    threshold = [ 0.6, 0.7, 0.7 ]  # three steps\'s threshold\n    factor = 0.709 # scale factor\n\n    # Add a random key to the filename to allow alignment using multiple processes\n    random_key = np.random.randint(0, high=99999)\n    bounding_boxes_filename = os.path.join(output_dir, \'bounding_boxes_%05d.txt\' % random_key)\n    \n    with open(bounding_boxes_filename, ""w"") as text_file:\n        nrof_images_total = 0\n        nrof_successfully_aligned = 0\n        if args.random_order:\n            random.shuffle(dataset)\n        for cls in dataset:\n            output_class_dir = os.path.join(output_dir, cls.name)\n            if not os.path.exists(output_class_dir):\n                os.makedirs(output_class_dir)\n                if args.random_order:\n                    random.shuffle(cls.image_paths)\n            for image_path in cls.image_paths:\n                nrof_images_total += 1\n                filename = os.path.splitext(os.path.split(image_path)[1])[0]\n                output_filename = os.path.join(output_class_dir, filename+\'.png\')\n                print(image_path)\n                if not os.path.exists(output_filename):\n                    try:\n                        img = misc.imread(image_path)\n                    except (IOError, ValueError, IndexError) as e:\n                        errorMessage = \'{}: {}\'.format(image_path, e)\n                        print(errorMessage)\n                    else:\n                        if img.ndim<2:\n                            print(\'Unable to align ""%s""\' % image_path)\n                            text_file.write(\'%s\\n\' % (output_filename))\n                            continue\n                        if img.ndim == 2:\n                            img = facenet.to_rgb(img)\n                        img = img[:,:,0:3]\n    \n                        bounding_boxes, _ = align.detect_face.detect_face(img, minsize, pnet, rnet, onet, threshold, factor)\n                        nrof_faces = bounding_boxes.shape[0]\n                        if nrof_faces>0:\n                            det = bounding_boxes[:,0:4]\n                            det_arr = []\n                            img_size = np.asarray(img.shape)[0:2]\n                            if nrof_faces>1:\n                                if args.detect_multiple_faces:\n                                    for i in range(nrof_faces):\n                                        det_arr.append(np.squeeze(det[i]))\n                                else:\n                                    bounding_box_size = (det[:,2]-det[:,0])*(det[:,3]-det[:,1])\n                                    img_center = img_size / 2\n                                    offsets = np.vstack([ (det[:,0]+det[:,2])/2-img_center[1], (det[:,1]+det[:,3])/2-img_center[0] ])\n                                    offset_dist_squared = np.sum(np.power(offsets,2.0),0)\n                                    index = np.argmax(bounding_box_size-offset_dist_squared*2.0) # some extra weight on the centering\n                                    det_arr.append(det[index,:])\n                            else:\n                                det_arr.append(np.squeeze(det))\n\n                            for i, det in enumerate(det_arr):\n                                det = np.squeeze(det)\n                                bb = np.zeros(4, dtype=np.int32)\n                                bb[0] = np.maximum(det[0]-args.margin/2, 0)\n                                bb[1] = np.maximum(det[1]-args.margin/2, 0)\n                                bb[2] = np.minimum(det[2]+args.margin/2, img_size[1])\n                                bb[3] = np.minimum(det[3]+args.margin/2, img_size[0])\n                                cropped = img[bb[1]:bb[3],bb[0]:bb[2],:]\n                                scaled = misc.imresize(cropped, (args.image_size, args.image_size), interp=\'bilinear\')\n                                nrof_successfully_aligned += 1\n                                filename_base, file_extension = os.path.splitext(output_filename)\n                                if args.detect_multiple_faces:\n                                    output_filename_n = ""{}_{}{}"".format(filename_base, i, file_extension)\n                                else:\n                                    output_filename_n = ""{}{}"".format(filename_base, file_extension)\n                                misc.imsave(output_filename_n, scaled)\n                                text_file.write(\'%s %d %d %d %d\\n\' % (output_filename_n, bb[0], bb[1], bb[2], bb[3]))\n                        else:\n                            print(\'Unable to align ""%s""\' % image_path)\n                            text_file.write(\'%s\\n\' % (output_filename))\n                            \n    print(\'Total number of images: %d\' % nrof_images_total)\n    print(\'Number of successfully aligned images: %d\' % nrof_successfully_aligned)\n            \n\ndef parse_arguments(argv):\n    parser = argparse.ArgumentParser()\n    \n    parser.add_argument(\'input_dir\', type=str, help=\'Directory with unaligned images.\')\n    parser.add_argument(\'output_dir\', type=str, help=\'Directory with aligned face thumbnails.\')\n    parser.add_argument(\'--image_size\', type=int,\n        help=\'Image size (height, width) in pixels.\', default=182)\n    parser.add_argument(\'--margin\', type=int,\n        help=\'Margin for the crop around the bounding box (height, width) in pixels.\', default=44)\n    parser.add_argument(\'--random_order\', \n        help=\'Shuffles the order of images to enable alignment using multiple processes.\', action=\'store_true\')\n    parser.add_argument(\'--gpu_memory_fraction\', type=float,\n        help=\'Upper bound on the amount of GPU memory that will be used by the process.\', default=1.0)\n    parser.add_argument(\'--detect_multiple_faces\', type=bool,\n                        help=\'Detect and align multiple faces per image.\', default=False)\n    return parser.parse_args(argv)\n\nif __name__ == \'__main__\':\n    main(parse_arguments(sys.argv[1:]))\n'"
src/align/detect_face.py,23,"b'"""""" Tensorflow implementation of the face detection / alignment algorithm found at\nhttps://github.com/kpzhang93/MTCNN_face_detection_alignment\n""""""\n# MIT License\n# \n# Copyright (c) 2016 David Sandberg\n# \n# Permission is hereby granted, free of charge, to any person obtaining a copy\n# of this software and associated documentation files (the ""Software""), to deal\n# in the Software without restriction, including without limitation the rights\n# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n# copies of the Software, and to permit persons to whom the Software is\n# furnished to do so, subject to the following conditions:\n# \n# The above copyright notice and this permission notice shall be included in all\n# copies or substantial portions of the Software.\n# \n# THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n# SOFTWARE.\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nfrom six import string_types, iteritems\n\nimport numpy as np\nimport tensorflow as tf\n#from math import floor\nimport cv2\nimport os\n\ndef layer(op):\n    """"""Decorator for composable network layers.""""""\n\n    def layer_decorated(self, *args, **kwargs):\n        # Automatically set a name if not provided.\n        name = kwargs.setdefault(\'name\', self.get_unique_name(op.__name__))\n        # Figure out the layer inputs.\n        if len(self.terminals) == 0:\n            raise RuntimeError(\'No input variables found for layer %s.\' % name)\n        elif len(self.terminals) == 1:\n            layer_input = self.terminals[0]\n        else:\n            layer_input = list(self.terminals)\n        # Perform the operation and get the output.\n        layer_output = op(self, layer_input, *args, **kwargs)\n        # Add to layer LUT.\n        self.layers[name] = layer_output\n        # This output is now the input for the next layer.\n        self.feed(layer_output)\n        # Return self for chained calls.\n        return self\n\n    return layer_decorated\n\nclass Network(object):\n\n    def __init__(self, inputs, trainable=True):\n        # The input nodes for this network\n        self.inputs = inputs\n        # The current list of terminal nodes\n        self.terminals = []\n        # Mapping from layer names to layers\n        self.layers = dict(inputs)\n        # If true, the resulting variables are set as trainable\n        self.trainable = trainable\n\n        self.setup()\n\n    def setup(self):\n        """"""Construct the network. """"""\n        raise NotImplementedError(\'Must be implemented by the subclass.\')\n\n    def load(self, data_path, session, ignore_missing=False):\n        """"""Load network weights.\n        data_path: The path to the numpy-serialized network weights\n        session: The current TensorFlow session\n        ignore_missing: If true, serialized weights for missing layers are ignored.\n        """"""\n        data_dict = np.load(data_path, encoding=\'latin1\').item() #pylint: disable=no-member\n\n        for op_name in data_dict:\n            with tf.variable_scope(op_name, reuse=True):\n                for param_name, data in iteritems(data_dict[op_name]):\n                    try:\n                        var = tf.get_variable(param_name)\n                        session.run(var.assign(data))\n                    except ValueError:\n                        if not ignore_missing:\n                            raise\n\n    def feed(self, *args):\n        """"""Set the input(s) for the next operation by replacing the terminal nodes.\n        The arguments can be either layer names or the actual layers.\n        """"""\n        assert len(args) != 0\n        self.terminals = []\n        for fed_layer in args:\n            if isinstance(fed_layer, string_types):\n                try:\n                    fed_layer = self.layers[fed_layer]\n                except KeyError:\n                    raise KeyError(\'Unknown layer name fed: %s\' % fed_layer)\n            self.terminals.append(fed_layer)\n        return self\n\n    def get_output(self):\n        """"""Returns the current network output.""""""\n        return self.terminals[-1]\n\n    def get_unique_name(self, prefix):\n        """"""Returns an index-suffixed unique name for the given prefix.\n        This is used for auto-generating layer names based on the type-prefix.\n        """"""\n        ident = sum(t.startswith(prefix) for t, _ in self.layers.items()) + 1\n        return \'%s_%d\' % (prefix, ident)\n\n    def make_var(self, name, shape):\n        """"""Creates a new TensorFlow variable.""""""\n        return tf.get_variable(name, shape, trainable=self.trainable)\n\n    def validate_padding(self, padding):\n        """"""Verifies that the padding is one of the supported ones.""""""\n        assert padding in (\'SAME\', \'VALID\')\n\n    @layer\n    def conv(self,\n             inp,\n             k_h,\n             k_w,\n             c_o,\n             s_h,\n             s_w,\n             name,\n             relu=True,\n             padding=\'SAME\',\n             group=1,\n             biased=True):\n        # Verify that the padding is acceptable\n        self.validate_padding(padding)\n        # Get the number of channels in the input\n        c_i = int(inp.get_shape()[-1])\n        # Verify that the grouping parameter is valid\n        assert c_i % group == 0\n        assert c_o % group == 0\n        # Convolution for a given input and kernel\n        convolve = lambda i, k: tf.nn.conv2d(i, k, [1, s_h, s_w, 1], padding=padding)\n        with tf.variable_scope(name) as scope:\n            kernel = self.make_var(\'weights\', shape=[k_h, k_w, c_i // group, c_o])\n            # This is the common-case. Convolve the input without any further complications.\n            output = convolve(inp, kernel)\n            # Add the biases\n            if biased:\n                biases = self.make_var(\'biases\', [c_o])\n                output = tf.nn.bias_add(output, biases)\n            if relu:\n                # ReLU non-linearity\n                output = tf.nn.relu(output, name=scope.name)\n            return output\n\n    @layer\n    def prelu(self, inp, name):\n        with tf.variable_scope(name):\n            i = int(inp.get_shape()[-1])\n            alpha = self.make_var(\'alpha\', shape=(i,))\n            output = tf.nn.relu(inp) + tf.multiply(alpha, -tf.nn.relu(-inp))\n        return output\n\n    @layer\n    def max_pool(self, inp, k_h, k_w, s_h, s_w, name, padding=\'SAME\'):\n        self.validate_padding(padding)\n        return tf.nn.max_pool(inp,\n                              ksize=[1, k_h, k_w, 1],\n                              strides=[1, s_h, s_w, 1],\n                              padding=padding,\n                              name=name)\n\n    @layer\n    def fc(self, inp, num_out, name, relu=True):\n        with tf.variable_scope(name):\n            input_shape = inp.get_shape()\n            if input_shape.ndims == 4:\n                # The input is spatial. Vectorize it first.\n                dim = 1\n                for d in input_shape[1:].as_list():\n                    dim *= int(d)\n                feed_in = tf.reshape(inp, [-1, dim])\n            else:\n                feed_in, dim = (inp, input_shape[-1].value)\n            weights = self.make_var(\'weights\', shape=[dim, num_out])\n            biases = self.make_var(\'biases\', [num_out])\n            op = tf.nn.relu_layer if relu else tf.nn.xw_plus_b\n            fc = op(feed_in, weights, biases, name=name)\n            return fc\n\n\n    """"""\n    Multi dimensional softmax,\n    refer to https://github.com/tensorflow/tensorflow/issues/210\n    compute softmax along the dimension of target\n    the native softmax only supports batch_size x dimension\n    """"""\n    @layer\n    def softmax(self, target, axis, name=None):\n        max_axis = tf.reduce_max(target, axis, keepdims=True)\n        target_exp = tf.exp(target-max_axis)\n        normalize = tf.reduce_sum(target_exp, axis, keepdims=True)\n        softmax = tf.div(target_exp, normalize, name)\n        return softmax\n    \nclass PNet(Network):\n    def setup(self):\n        (self.feed(\'data\') #pylint: disable=no-value-for-parameter, no-member\n             .conv(3, 3, 10, 1, 1, padding=\'VALID\', relu=False, name=\'conv1\')\n             .prelu(name=\'PReLU1\')\n             .max_pool(2, 2, 2, 2, name=\'pool1\')\n             .conv(3, 3, 16, 1, 1, padding=\'VALID\', relu=False, name=\'conv2\')\n             .prelu(name=\'PReLU2\')\n             .conv(3, 3, 32, 1, 1, padding=\'VALID\', relu=False, name=\'conv3\')\n             .prelu(name=\'PReLU3\')\n             .conv(1, 1, 2, 1, 1, relu=False, name=\'conv4-1\')\n             .softmax(3,name=\'prob1\'))\n\n        (self.feed(\'PReLU3\') #pylint: disable=no-value-for-parameter\n             .conv(1, 1, 4, 1, 1, relu=False, name=\'conv4-2\'))\n        \nclass RNet(Network):\n    def setup(self):\n        (self.feed(\'data\') #pylint: disable=no-value-for-parameter, no-member\n             .conv(3, 3, 28, 1, 1, padding=\'VALID\', relu=False, name=\'conv1\')\n             .prelu(name=\'prelu1\')\n             .max_pool(3, 3, 2, 2, name=\'pool1\')\n             .conv(3, 3, 48, 1, 1, padding=\'VALID\', relu=False, name=\'conv2\')\n             .prelu(name=\'prelu2\')\n             .max_pool(3, 3, 2, 2, padding=\'VALID\', name=\'pool2\')\n             .conv(2, 2, 64, 1, 1, padding=\'VALID\', relu=False, name=\'conv3\')\n             .prelu(name=\'prelu3\')\n             .fc(128, relu=False, name=\'conv4\')\n             .prelu(name=\'prelu4\')\n             .fc(2, relu=False, name=\'conv5-1\')\n             .softmax(1,name=\'prob1\'))\n\n        (self.feed(\'prelu4\') #pylint: disable=no-value-for-parameter\n             .fc(4, relu=False, name=\'conv5-2\'))\n\nclass ONet(Network):\n    def setup(self):\n        (self.feed(\'data\') #pylint: disable=no-value-for-parameter, no-member\n             .conv(3, 3, 32, 1, 1, padding=\'VALID\', relu=False, name=\'conv1\')\n             .prelu(name=\'prelu1\')\n             .max_pool(3, 3, 2, 2, name=\'pool1\')\n             .conv(3, 3, 64, 1, 1, padding=\'VALID\', relu=False, name=\'conv2\')\n             .prelu(name=\'prelu2\')\n             .max_pool(3, 3, 2, 2, padding=\'VALID\', name=\'pool2\')\n             .conv(3, 3, 64, 1, 1, padding=\'VALID\', relu=False, name=\'conv3\')\n             .prelu(name=\'prelu3\')\n             .max_pool(2, 2, 2, 2, name=\'pool3\')\n             .conv(2, 2, 128, 1, 1, padding=\'VALID\', relu=False, name=\'conv4\')\n             .prelu(name=\'prelu4\')\n             .fc(256, relu=False, name=\'conv5\')\n             .prelu(name=\'prelu5\')\n             .fc(2, relu=False, name=\'conv6-1\')\n             .softmax(1, name=\'prob1\'))\n\n        (self.feed(\'prelu5\') #pylint: disable=no-value-for-parameter\n             .fc(4, relu=False, name=\'conv6-2\'))\n\n        (self.feed(\'prelu5\') #pylint: disable=no-value-for-parameter\n             .fc(10, relu=False, name=\'conv6-3\'))\n\ndef create_mtcnn(sess, model_path):\n    if not model_path:\n        model_path,_ = os.path.split(os.path.realpath(__file__))\n\n    with tf.variable_scope(\'pnet\'):\n        data = tf.placeholder(tf.float32, (None,None,None,3), \'input\')\n        pnet = PNet({\'data\':data})\n        pnet.load(os.path.join(model_path, \'det1.npy\'), sess)\n    with tf.variable_scope(\'rnet\'):\n        data = tf.placeholder(tf.float32, (None,24,24,3), \'input\')\n        rnet = RNet({\'data\':data})\n        rnet.load(os.path.join(model_path, \'det2.npy\'), sess)\n    with tf.variable_scope(\'onet\'):\n        data = tf.placeholder(tf.float32, (None,48,48,3), \'input\')\n        onet = ONet({\'data\':data})\n        onet.load(os.path.join(model_path, \'det3.npy\'), sess)\n        \n    pnet_fun = lambda img : sess.run((\'pnet/conv4-2/BiasAdd:0\', \'pnet/prob1:0\'), feed_dict={\'pnet/input:0\':img})\n    rnet_fun = lambda img : sess.run((\'rnet/conv5-2/conv5-2:0\', \'rnet/prob1:0\'), feed_dict={\'rnet/input:0\':img})\n    onet_fun = lambda img : sess.run((\'onet/conv6-2/conv6-2:0\', \'onet/conv6-3/conv6-3:0\', \'onet/prob1:0\'), feed_dict={\'onet/input:0\':img})\n    return pnet_fun, rnet_fun, onet_fun\n\ndef detect_face(img, minsize, pnet, rnet, onet, threshold, factor):\n    """"""Detects faces in an image, and returns bounding boxes and points for them.\n    img: input image\n    minsize: minimum faces\' size\n    pnet, rnet, onet: caffemodel\n    threshold: threshold=[th1, th2, th3], th1-3 are three steps\'s threshold\n    factor: the factor used to create a scaling pyramid of face sizes to detect in the image.\n    """"""\n    factor_count=0\n    total_boxes=np.empty((0,9))\n    points=np.empty(0)\n    h=img.shape[0]\n    w=img.shape[1]\n    minl=np.amin([h, w])\n    m=12.0/minsize\n    minl=minl*m\n    # create scale pyramid\n    scales=[]\n    while minl>=12:\n        scales += [m*np.power(factor, factor_count)]\n        minl = minl*factor\n        factor_count += 1\n\n    # first stage\n    for scale in scales:\n        hs=int(np.ceil(h*scale))\n        ws=int(np.ceil(w*scale))\n        im_data = imresample(img, (hs, ws))\n        im_data = (im_data-127.5)*0.0078125\n        img_x = np.expand_dims(im_data, 0)\n        img_y = np.transpose(img_x, (0,2,1,3))\n        out = pnet(img_y)\n        out0 = np.transpose(out[0], (0,2,1,3))\n        out1 = np.transpose(out[1], (0,2,1,3))\n        \n        boxes, _ = generateBoundingBox(out1[0,:,:,1].copy(), out0[0,:,:,:].copy(), scale, threshold[0])\n        \n        # inter-scale nms\n        pick = nms(boxes.copy(), 0.5, \'Union\')\n        if boxes.size>0 and pick.size>0:\n            boxes = boxes[pick,:]\n            total_boxes = np.append(total_boxes, boxes, axis=0)\n\n    numbox = total_boxes.shape[0]\n    if numbox>0:\n        pick = nms(total_boxes.copy(), 0.7, \'Union\')\n        total_boxes = total_boxes[pick,:]\n        regw = total_boxes[:,2]-total_boxes[:,0]\n        regh = total_boxes[:,3]-total_boxes[:,1]\n        qq1 = total_boxes[:,0]+total_boxes[:,5]*regw\n        qq2 = total_boxes[:,1]+total_boxes[:,6]*regh\n        qq3 = total_boxes[:,2]+total_boxes[:,7]*regw\n        qq4 = total_boxes[:,3]+total_boxes[:,8]*regh\n        total_boxes = np.transpose(np.vstack([qq1, qq2, qq3, qq4, total_boxes[:,4]]))\n        total_boxes = rerec(total_boxes.copy())\n        total_boxes[:,0:4] = np.fix(total_boxes[:,0:4]).astype(np.int32)\n        dy, edy, dx, edx, y, ey, x, ex, tmpw, tmph = pad(total_boxes.copy(), w, h)\n\n    numbox = total_boxes.shape[0]\n    if numbox>0:\n        # second stage\n        tempimg = np.zeros((24,24,3,numbox))\n        for k in range(0,numbox):\n            tmp = np.zeros((int(tmph[k]),int(tmpw[k]),3))\n            tmp[dy[k]-1:edy[k],dx[k]-1:edx[k],:] = img[y[k]-1:ey[k],x[k]-1:ex[k],:]\n            if tmp.shape[0]>0 and tmp.shape[1]>0 or tmp.shape[0]==0 and tmp.shape[1]==0:\n                tempimg[:,:,:,k] = imresample(tmp, (24, 24))\n            else:\n                return np.empty()\n        tempimg = (tempimg-127.5)*0.0078125\n        tempimg1 = np.transpose(tempimg, (3,1,0,2))\n        out = rnet(tempimg1)\n        out0 = np.transpose(out[0])\n        out1 = np.transpose(out[1])\n        score = out1[1,:]\n        ipass = np.where(score>threshold[1])\n        total_boxes = np.hstack([total_boxes[ipass[0],0:4].copy(), np.expand_dims(score[ipass].copy(),1)])\n        mv = out0[:,ipass[0]]\n        if total_boxes.shape[0]>0:\n            pick = nms(total_boxes, 0.7, \'Union\')\n            total_boxes = total_boxes[pick,:]\n            total_boxes = bbreg(total_boxes.copy(), np.transpose(mv[:,pick]))\n            total_boxes = rerec(total_boxes.copy())\n\n    numbox = total_boxes.shape[0]\n    if numbox>0:\n        # third stage\n        total_boxes = np.fix(total_boxes).astype(np.int32)\n        dy, edy, dx, edx, y, ey, x, ex, tmpw, tmph = pad(total_boxes.copy(), w, h)\n        tempimg = np.zeros((48,48,3,numbox))\n        for k in range(0,numbox):\n            tmp = np.zeros((int(tmph[k]),int(tmpw[k]),3))\n            tmp[dy[k]-1:edy[k],dx[k]-1:edx[k],:] = img[y[k]-1:ey[k],x[k]-1:ex[k],:]\n            if tmp.shape[0]>0 and tmp.shape[1]>0 or tmp.shape[0]==0 and tmp.shape[1]==0:\n                tempimg[:,:,:,k] = imresample(tmp, (48, 48))\n            else:\n                return np.empty()\n        tempimg = (tempimg-127.5)*0.0078125\n        tempimg1 = np.transpose(tempimg, (3,1,0,2))\n        out = onet(tempimg1)\n        out0 = np.transpose(out[0])\n        out1 = np.transpose(out[1])\n        out2 = np.transpose(out[2])\n        score = out2[1,:]\n        points = out1\n        ipass = np.where(score>threshold[2])\n        points = points[:,ipass[0]]\n        total_boxes = np.hstack([total_boxes[ipass[0],0:4].copy(), np.expand_dims(score[ipass].copy(),1)])\n        mv = out0[:,ipass[0]]\n\n        w = total_boxes[:,2]-total_boxes[:,0]+1\n        h = total_boxes[:,3]-total_boxes[:,1]+1\n        points[0:5,:] = np.tile(w,(5, 1))*points[0:5,:] + np.tile(total_boxes[:,0],(5, 1))-1\n        points[5:10,:] = np.tile(h,(5, 1))*points[5:10,:] + np.tile(total_boxes[:,1],(5, 1))-1\n        if total_boxes.shape[0]>0:\n            total_boxes = bbreg(total_boxes.copy(), np.transpose(mv))\n            pick = nms(total_boxes.copy(), 0.7, \'Min\')\n            total_boxes = total_boxes[pick,:]\n            points = points[:,pick]\n                \n    return total_boxes, points\n\n\ndef bulk_detect_face(images, detection_window_size_ratio, pnet, rnet, onet, threshold, factor):\n    """"""Detects faces in a list of images\n    images: list containing input images\n    detection_window_size_ratio: ratio of minimum face size to smallest image dimension\n    pnet, rnet, onet: caffemodel\n    threshold: threshold=[th1 th2 th3], th1-3 are three steps\'s threshold [0-1]\n    factor: the factor used to create a scaling pyramid of face sizes to detect in the image.\n    """"""\n    all_scales = [None] * len(images)\n    images_with_boxes = [None] * len(images)\n\n    for i in range(len(images)):\n        images_with_boxes[i] = {\'total_boxes\': np.empty((0, 9))}\n\n    # create scale pyramid\n    for index, img in enumerate(images):\n        all_scales[index] = []\n        h = img.shape[0]\n        w = img.shape[1]\n        minsize = int(detection_window_size_ratio * np.minimum(w, h))\n        factor_count = 0\n        minl = np.amin([h, w])\n        if minsize <= 12:\n            minsize = 12\n\n        m = 12.0 / minsize\n        minl = minl * m\n        while minl >= 12:\n            all_scales[index].append(m * np.power(factor, factor_count))\n            minl = minl * factor\n            factor_count += 1\n\n    # # # # # # # # # # # # #\n    # first stage - fast proposal network (pnet) to obtain face candidates\n    # # # # # # # # # # # # #\n\n    images_obj_per_resolution = {}\n\n    # TODO: use some type of rounding to number module 8 to increase probability that pyramid images will have the same resolution across input images\n\n    for index, scales in enumerate(all_scales):\n        h = images[index].shape[0]\n        w = images[index].shape[1]\n\n        for scale in scales:\n            hs = int(np.ceil(h * scale))\n            ws = int(np.ceil(w * scale))\n\n            if (ws, hs) not in images_obj_per_resolution:\n                images_obj_per_resolution[(ws, hs)] = []\n\n            im_data = imresample(images[index], (hs, ws))\n            im_data = (im_data - 127.5) * 0.0078125\n            img_y = np.transpose(im_data, (1, 0, 2))  # caffe uses different dimensions ordering\n            images_obj_per_resolution[(ws, hs)].append({\'scale\': scale, \'image\': img_y, \'index\': index})\n\n    for resolution in images_obj_per_resolution:\n        images_per_resolution = [i[\'image\'] for i in images_obj_per_resolution[resolution]]\n        outs = pnet(images_per_resolution)\n\n        for index in range(len(outs[0])):\n            scale = images_obj_per_resolution[resolution][index][\'scale\']\n            image_index = images_obj_per_resolution[resolution][index][\'index\']\n            out0 = np.transpose(outs[0][index], (1, 0, 2))\n            out1 = np.transpose(outs[1][index], (1, 0, 2))\n\n            boxes, _ = generateBoundingBox(out1[:, :, 1].copy(), out0[:, :, :].copy(), scale, threshold[0])\n\n            # inter-scale nms\n            pick = nms(boxes.copy(), 0.5, \'Union\')\n            if boxes.size > 0 and pick.size > 0:\n                boxes = boxes[pick, :]\n                images_with_boxes[image_index][\'total_boxes\'] = np.append(images_with_boxes[image_index][\'total_boxes\'],\n                                                                          boxes,\n                                                                          axis=0)\n\n    for index, image_obj in enumerate(images_with_boxes):\n        numbox = image_obj[\'total_boxes\'].shape[0]\n        if numbox > 0:\n            h = images[index].shape[0]\n            w = images[index].shape[1]\n            pick = nms(image_obj[\'total_boxes\'].copy(), 0.7, \'Union\')\n            image_obj[\'total_boxes\'] = image_obj[\'total_boxes\'][pick, :]\n            regw = image_obj[\'total_boxes\'][:, 2] - image_obj[\'total_boxes\'][:, 0]\n            regh = image_obj[\'total_boxes\'][:, 3] - image_obj[\'total_boxes\'][:, 1]\n            qq1 = image_obj[\'total_boxes\'][:, 0] + image_obj[\'total_boxes\'][:, 5] * regw\n            qq2 = image_obj[\'total_boxes\'][:, 1] + image_obj[\'total_boxes\'][:, 6] * regh\n            qq3 = image_obj[\'total_boxes\'][:, 2] + image_obj[\'total_boxes\'][:, 7] * regw\n            qq4 = image_obj[\'total_boxes\'][:, 3] + image_obj[\'total_boxes\'][:, 8] * regh\n            image_obj[\'total_boxes\'] = np.transpose(np.vstack([qq1, qq2, qq3, qq4, image_obj[\'total_boxes\'][:, 4]]))\n            image_obj[\'total_boxes\'] = rerec(image_obj[\'total_boxes\'].copy())\n            image_obj[\'total_boxes\'][:, 0:4] = np.fix(image_obj[\'total_boxes\'][:, 0:4]).astype(np.int32)\n            dy, edy, dx, edx, y, ey, x, ex, tmpw, tmph = pad(image_obj[\'total_boxes\'].copy(), w, h)\n\n            numbox = image_obj[\'total_boxes\'].shape[0]\n            tempimg = np.zeros((24, 24, 3, numbox))\n\n            if numbox > 0:\n                for k in range(0, numbox):\n                    tmp = np.zeros((int(tmph[k]), int(tmpw[k]), 3))\n                    tmp[dy[k] - 1:edy[k], dx[k] - 1:edx[k], :] = images[index][y[k] - 1:ey[k], x[k] - 1:ex[k], :]\n                    if tmp.shape[0] > 0 and tmp.shape[1] > 0 or tmp.shape[0] == 0 and tmp.shape[1] == 0:\n                        tempimg[:, :, :, k] = imresample(tmp, (24, 24))\n                    else:\n                        return np.empty()\n\n                tempimg = (tempimg - 127.5) * 0.0078125\n                image_obj[\'rnet_input\'] = np.transpose(tempimg, (3, 1, 0, 2))\n\n    # # # # # # # # # # # # #\n    # second stage - refinement of face candidates with rnet\n    # # # # # # # # # # # # #\n\n    bulk_rnet_input = np.empty((0, 24, 24, 3))\n    for index, image_obj in enumerate(images_with_boxes):\n        if \'rnet_input\' in image_obj:\n            bulk_rnet_input = np.append(bulk_rnet_input, image_obj[\'rnet_input\'], axis=0)\n\n    out = rnet(bulk_rnet_input)\n    out0 = np.transpose(out[0])\n    out1 = np.transpose(out[1])\n    score = out1[1, :]\n\n    i = 0\n    for index, image_obj in enumerate(images_with_boxes):\n        if \'rnet_input\' not in image_obj:\n            continue\n\n        rnet_input_count = image_obj[\'rnet_input\'].shape[0]\n        score_per_image = score[i:i + rnet_input_count]\n        out0_per_image = out0[:, i:i + rnet_input_count]\n\n        ipass = np.where(score_per_image > threshold[1])\n        image_obj[\'total_boxes\'] = np.hstack([image_obj[\'total_boxes\'][ipass[0], 0:4].copy(),\n                                              np.expand_dims(score_per_image[ipass].copy(), 1)])\n\n        mv = out0_per_image[:, ipass[0]]\n\n        if image_obj[\'total_boxes\'].shape[0] > 0:\n            h = images[index].shape[0]\n            w = images[index].shape[1]\n            pick = nms(image_obj[\'total_boxes\'], 0.7, \'Union\')\n            image_obj[\'total_boxes\'] = image_obj[\'total_boxes\'][pick, :]\n            image_obj[\'total_boxes\'] = bbreg(image_obj[\'total_boxes\'].copy(), np.transpose(mv[:, pick]))\n            image_obj[\'total_boxes\'] = rerec(image_obj[\'total_boxes\'].copy())\n\n            numbox = image_obj[\'total_boxes\'].shape[0]\n\n            if numbox > 0:\n                tempimg = np.zeros((48, 48, 3, numbox))\n                image_obj[\'total_boxes\'] = np.fix(image_obj[\'total_boxes\']).astype(np.int32)\n                dy, edy, dx, edx, y, ey, x, ex, tmpw, tmph = pad(image_obj[\'total_boxes\'].copy(), w, h)\n\n                for k in range(0, numbox):\n                    tmp = np.zeros((int(tmph[k]), int(tmpw[k]), 3))\n                    tmp[dy[k] - 1:edy[k], dx[k] - 1:edx[k], :] = images[index][y[k] - 1:ey[k], x[k] - 1:ex[k], :]\n                    if tmp.shape[0] > 0 and tmp.shape[1] > 0 or tmp.shape[0] == 0 and tmp.shape[1] == 0:\n                        tempimg[:, :, :, k] = imresample(tmp, (48, 48))\n                    else:\n                        return np.empty()\n                tempimg = (tempimg - 127.5) * 0.0078125\n                image_obj[\'onet_input\'] = np.transpose(tempimg, (3, 1, 0, 2))\n\n        i += rnet_input_count\n\n    # # # # # # # # # # # # #\n    # third stage - further refinement and facial landmarks positions with onet\n    # # # # # # # # # # # # #\n\n    bulk_onet_input = np.empty((0, 48, 48, 3))\n    for index, image_obj in enumerate(images_with_boxes):\n        if \'onet_input\' in image_obj:\n            bulk_onet_input = np.append(bulk_onet_input, image_obj[\'onet_input\'], axis=0)\n\n    out = onet(bulk_onet_input)\n\n    out0 = np.transpose(out[0])\n    out1 = np.transpose(out[1])\n    out2 = np.transpose(out[2])\n    score = out2[1, :]\n    points = out1\n\n    i = 0\n    ret = []\n    for index, image_obj in enumerate(images_with_boxes):\n        if \'onet_input\' not in image_obj:\n            ret.append(None)\n            continue\n\n        onet_input_count = image_obj[\'onet_input\'].shape[0]\n\n        out0_per_image = out0[:, i:i + onet_input_count]\n        score_per_image = score[i:i + onet_input_count]\n        points_per_image = points[:, i:i + onet_input_count]\n\n        ipass = np.where(score_per_image > threshold[2])\n        points_per_image = points_per_image[:, ipass[0]]\n\n        image_obj[\'total_boxes\'] = np.hstack([image_obj[\'total_boxes\'][ipass[0], 0:4].copy(),\n                                              np.expand_dims(score_per_image[ipass].copy(), 1)])\n        mv = out0_per_image[:, ipass[0]]\n\n        w = image_obj[\'total_boxes\'][:, 2] - image_obj[\'total_boxes\'][:, 0] + 1\n        h = image_obj[\'total_boxes\'][:, 3] - image_obj[\'total_boxes\'][:, 1] + 1\n        points_per_image[0:5, :] = np.tile(w, (5, 1)) * points_per_image[0:5, :] + np.tile(\n            image_obj[\'total_boxes\'][:, 0], (5, 1)) - 1\n        points_per_image[5:10, :] = np.tile(h, (5, 1)) * points_per_image[5:10, :] + np.tile(\n            image_obj[\'total_boxes\'][:, 1], (5, 1)) - 1\n\n        if image_obj[\'total_boxes\'].shape[0] > 0:\n            image_obj[\'total_boxes\'] = bbreg(image_obj[\'total_boxes\'].copy(), np.transpose(mv))\n            pick = nms(image_obj[\'total_boxes\'].copy(), 0.7, \'Min\')\n            image_obj[\'total_boxes\'] = image_obj[\'total_boxes\'][pick, :]\n            points_per_image = points_per_image[:, pick]\n\n            ret.append((image_obj[\'total_boxes\'], points_per_image))\n        else:\n            ret.append(None)\n\n        i += onet_input_count\n\n    return ret\n\n\n# function [boundingbox] = bbreg(boundingbox,reg)\ndef bbreg(boundingbox,reg):\n    """"""Calibrate bounding boxes""""""\n    if reg.shape[1]==1:\n        reg = np.reshape(reg, (reg.shape[2], reg.shape[3]))\n\n    w = boundingbox[:,2]-boundingbox[:,0]+1\n    h = boundingbox[:,3]-boundingbox[:,1]+1\n    b1 = boundingbox[:,0]+reg[:,0]*w\n    b2 = boundingbox[:,1]+reg[:,1]*h\n    b3 = boundingbox[:,2]+reg[:,2]*w\n    b4 = boundingbox[:,3]+reg[:,3]*h\n    boundingbox[:,0:4] = np.transpose(np.vstack([b1, b2, b3, b4 ]))\n    return boundingbox\n \ndef generateBoundingBox(imap, reg, scale, t):\n    """"""Use heatmap to generate bounding boxes""""""\n    stride=2\n    cellsize=12\n\n    imap = np.transpose(imap)\n    dx1 = np.transpose(reg[:,:,0])\n    dy1 = np.transpose(reg[:,:,1])\n    dx2 = np.transpose(reg[:,:,2])\n    dy2 = np.transpose(reg[:,:,3])\n    y, x = np.where(imap >= t)\n    if y.shape[0]==1:\n        dx1 = np.flipud(dx1)\n        dy1 = np.flipud(dy1)\n        dx2 = np.flipud(dx2)\n        dy2 = np.flipud(dy2)\n    score = imap[(y,x)]\n    reg = np.transpose(np.vstack([ dx1[(y,x)], dy1[(y,x)], dx2[(y,x)], dy2[(y,x)] ]))\n    if reg.size==0:\n        reg = np.empty((0,3))\n    bb = np.transpose(np.vstack([y,x]))\n    q1 = np.fix((stride*bb+1)/scale)\n    q2 = np.fix((stride*bb+cellsize-1+1)/scale)\n    boundingbox = np.hstack([q1, q2, np.expand_dims(score,1), reg])\n    return boundingbox, reg\n \n# function pick = nms(boxes,threshold,type)\ndef nms(boxes, threshold, method):\n    if boxes.size==0:\n        return np.empty((0,3))\n    x1 = boxes[:,0]\n    y1 = boxes[:,1]\n    x2 = boxes[:,2]\n    y2 = boxes[:,3]\n    s = boxes[:,4]\n    area = (x2-x1+1) * (y2-y1+1)\n    I = np.argsort(s)\n    pick = np.zeros_like(s, dtype=np.int16)\n    counter = 0\n    while I.size>0:\n        i = I[-1]\n        pick[counter] = i\n        counter += 1\n        idx = I[0:-1]\n        xx1 = np.maximum(x1[i], x1[idx])\n        yy1 = np.maximum(y1[i], y1[idx])\n        xx2 = np.minimum(x2[i], x2[idx])\n        yy2 = np.minimum(y2[i], y2[idx])\n        w = np.maximum(0.0, xx2-xx1+1)\n        h = np.maximum(0.0, yy2-yy1+1)\n        inter = w * h\n        if method is \'Min\':\n            o = inter / np.minimum(area[i], area[idx])\n        else:\n            o = inter / (area[i] + area[idx] - inter)\n        I = I[np.where(o<=threshold)]\n    pick = pick[0:counter]\n    return pick\n\n# function [dy edy dx edx y ey x ex tmpw tmph] = pad(total_boxes,w,h)\ndef pad(total_boxes, w, h):\n    """"""Compute the padding coordinates (pad the bounding boxes to square)""""""\n    tmpw = (total_boxes[:,2]-total_boxes[:,0]+1).astype(np.int32)\n    tmph = (total_boxes[:,3]-total_boxes[:,1]+1).astype(np.int32)\n    numbox = total_boxes.shape[0]\n\n    dx = np.ones((numbox), dtype=np.int32)\n    dy = np.ones((numbox), dtype=np.int32)\n    edx = tmpw.copy().astype(np.int32)\n    edy = tmph.copy().astype(np.int32)\n\n    x = total_boxes[:,0].copy().astype(np.int32)\n    y = total_boxes[:,1].copy().astype(np.int32)\n    ex = total_boxes[:,2].copy().astype(np.int32)\n    ey = total_boxes[:,3].copy().astype(np.int32)\n\n    tmp = np.where(ex>w)\n    edx.flat[tmp] = np.expand_dims(-ex[tmp]+w+tmpw[tmp],1)\n    ex[tmp] = w\n    \n    tmp = np.where(ey>h)\n    edy.flat[tmp] = np.expand_dims(-ey[tmp]+h+tmph[tmp],1)\n    ey[tmp] = h\n\n    tmp = np.where(x<1)\n    dx.flat[tmp] = np.expand_dims(2-x[tmp],1)\n    x[tmp] = 1\n\n    tmp = np.where(y<1)\n    dy.flat[tmp] = np.expand_dims(2-y[tmp],1)\n    y[tmp] = 1\n    \n    return dy, edy, dx, edx, y, ey, x, ex, tmpw, tmph\n\n# function [bboxA] = rerec(bboxA)\ndef rerec(bboxA):\n    """"""Convert bboxA to square.""""""\n    h = bboxA[:,3]-bboxA[:,1]\n    w = bboxA[:,2]-bboxA[:,0]\n    l = np.maximum(w, h)\n    bboxA[:,0] = bboxA[:,0]+w*0.5-l*0.5\n    bboxA[:,1] = bboxA[:,1]+h*0.5-l*0.5\n    bboxA[:,2:4] = bboxA[:,0:2] + np.transpose(np.tile(l,(2,1)))\n    return bboxA\n\ndef imresample(img, sz):\n    im_data = cv2.resize(img, (sz[1], sz[0]), interpolation=cv2.INTER_AREA) #@UndefinedVariable\n    return im_data\n\n    # This method is kept for debugging purpose\n#     h=img.shape[0]\n#     w=img.shape[1]\n#     hs, ws = sz\n#     dx = float(w) / ws\n#     dy = float(h) / hs\n#     im_data = np.zeros((hs,ws,3))\n#     for a1 in range(0,hs):\n#         for a2 in range(0,ws):\n#             for a3 in range(0,3):\n#                 im_data[a1,a2,a3] = img[int(floor(a1*dy)),int(floor(a2*dx)),a3]\n#     return im_data\n\n'"
src/generative/__init__.py,0,b''
src/generative/calculate_attribute_vectors.py,19,"b'# MIT License\n# \n# Copyright (c) 2017 David Sandberg\n# \n# Permission is hereby granted, free of charge, to any person obtaining a copy\n# of this software and associated documentation files (the ""Software""), to deal\n# in the Software without restriction, including without limitation the rights\n# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n# copies of the Software, and to permit persons to whom the Software is\n# furnished to do so, subject to the following conditions:\n# \n# The above copyright notice and this permission notice shall be included in all\n# copies or substantial portions of the Software.\n# \n# THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n# SOFTWARE.\n\n""""""Calculate average latent variables (here called attribute vectors) \nfor the different attributes in CelebA\n""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\nimport sys\nimport argparse\nimport importlib\nimport facenet\nimport os\nimport numpy as np\nimport math\nimport time\nimport h5py\nfrom six import iteritems\n\ndef main(args):\n  \n    img_mean = np.array([134.10714722, 102.52040863, 87.15436554])\n    img_stddev = np.sqrt(np.array([3941.30175781, 2856.94287109, 2519.35791016]))\n    \n    vae_checkpoint = os.path.expanduser(args.vae_checkpoint)\n    \n    fields, attribs_dict = read_annotations(args.annotations_filename)\n    \n    vae_def = importlib.import_module(args.vae_def)\n    vae = vae_def.Vae(args.latent_var_size)\n    gen_image_size = vae.get_image_size()\n\n    with tf.Graph().as_default():\n        tf.set_random_seed(args.seed)\n        \n        image_list = facenet.get_image_paths(os.path.expanduser(args.data_dir))\n        \n        # Get attributes for images\n        nrof_attributes = len(fields)\n        attribs_list = []\n        for img in image_list:\n            key = os.path.split(img)[1].split(\'.\')[0]\n            attr = attribs_dict[key]\n            assert len(attr)==nrof_attributes\n            attribs_list.append(attr)\n            \n        # Create the input queue\n        index_list = range(len(image_list))\n        input_queue = tf.train.slice_input_producer([image_list, attribs_list, index_list], num_epochs=1, shuffle=False)        \n        \n        nrof_preprocess_threads = 4\n        image_per_thread = []\n        for _ in range(nrof_preprocess_threads):\n            filename = input_queue[0]\n            file_contents = tf.read_file(filename)\n            image = tf.image.decode_image(file_contents, channels=3)\n            image = tf.image.resize_image_with_crop_or_pad(image, 160, 160)\n            #image = tf.image.resize_images(image, (64,64))\n            image.set_shape((args.image_size, args.image_size, 3))\n            attrib = input_queue[1]\n            attrib.set_shape((nrof_attributes,))\n            image = tf.cast(image, tf.float32)\n            image_per_thread.append([image, attrib, input_queue[2]])\n    \n        images, attribs, indices = tf.train.batch_join(\n            image_per_thread, batch_size=args.batch_size, \n            shapes=[(args.image_size, args.image_size, 3), (nrof_attributes,), ()], enqueue_many=False,\n            capacity=4 * nrof_preprocess_threads * args.batch_size,\n            allow_smaller_final_batch=True)\n        \n        # Normalize\n        images_norm = (images-img_mean) / img_stddev\n\n        # Resize to appropriate size for the encoder \n        images_norm_resize = tf.image.resize_images(images_norm, (gen_image_size,gen_image_size))\n        \n        # Create encoder network\n        mean, log_variance = vae.encoder(images_norm_resize, True)\n        \n        epsilon = tf.random_normal((tf.shape(mean)[0], args.latent_var_size))\n        std = tf.exp(log_variance/2)\n        latent_var = mean + epsilon * std\n        \n        # Create a saver\n        saver = tf.train.Saver(tf.trainable_variables(), max_to_keep=3)\n        \n        # Start running operations on the Graph\n        gpu_memory_fraction = 1.0\n        gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=gpu_memory_fraction)\n        sess = tf.Session(config=tf.ConfigProto(gpu_options=gpu_options, log_device_placement=False))\n        sess.run(tf.global_variables_initializer())\n        sess.run(tf.local_variables_initializer())\n        coord = tf.train.Coordinator()\n        tf.train.start_queue_runners(coord=coord, sess=sess)\n        \n\n        with sess.as_default():\n          \n            if vae_checkpoint:\n                print(\'Restoring VAE checkpoint: %s\' % vae_checkpoint)\n                saver.restore(sess, vae_checkpoint)\n           \n            nrof_images = len(image_list)\n            nrof_batches = int(math.ceil(len(image_list) / args.batch_size))\n            latent_vars = np.zeros((nrof_images, args.latent_var_size))\n            attributes = np.zeros((nrof_images, nrof_attributes))\n            for i in range(nrof_batches):\n                start_time = time.time()\n                latent_var_, attribs_, indices_ = sess.run([latent_var, attribs, indices])\n                latent_vars[indices_,:] = latent_var_\n                attributes[indices_,:] = attribs_\n                duration = time.time() - start_time\n                print(\'Batch %d/%d: %.3f seconds\' % (i+1, nrof_batches, duration))\n            # NOTE: This will print the \'Out of range\' warning if the last batch is not full,\n            #  as described by https://github.com/tensorflow/tensorflow/issues/8330\n             \n            # Calculate average change in the latent variable when each attribute changes\n            attribute_vectors = np.zeros((nrof_attributes, args.latent_var_size), np.float32)\n            for i in range(nrof_attributes):\n                pos_idx = np.argwhere(attributes[:,i]==1)[:,0]\n                neg_idx = np.argwhere(attributes[:,i]==-1)[:,0]\n                pos_avg = np.mean(latent_vars[pos_idx,:], 0)\n                neg_avg = np.mean(latent_vars[neg_idx,:], 0)\n                attribute_vectors[i,:] = pos_avg - neg_avg\n            \n            filename = os.path.expanduser(args.output_filename)\n            print(\'Writing attribute vectors, latent variables and attributes to %s\' % filename)\n            mdict = {\'latent_vars\':latent_vars, \'attributes\':attributes, \n                     \'fields\':fields, \'attribute_vectors\':attribute_vectors }\n            with h5py.File(filename, \'w\') as f:\n                for key, value in iteritems(mdict):\n                    f.create_dataset(key, data=value)\n                    \n                    \ndef read_annotations(filename):\n    attribs = {}    \n    with open(filename, \'r\') as f:\n        for i, line in enumerate(f.readlines()):\n            if i==0:\n                continue  # First line is the number of entries in the file\n            elif i==1:\n                fields = line.strip().split() # Second line is the field names\n            else:\n                line = line.split()\n                img_name = line[0].split(\'.\')[0]\n                img_attribs = map(int, line[1:])\n                attribs[img_name] = img_attribs\n    return fields, attribs\n\ndef parse_arguments(argv):\n    parser = argparse.ArgumentParser()\n    \n    parser.add_argument(\'vae_def\', type=str,\n        help=\'Model definition for the variational autoencoder. Points to a module containing the definition.\', \n        default=\'src.generative.models.dfc_vae\')\n    parser.add_argument(\'vae_checkpoint\', type=str,\n        help=\'Checkpoint file of a pre-trained variational autoencoder.\')\n    parser.add_argument(\'data_dir\', type=str,\n        help=\'Path to the directory containing aligned face patches for the CelebA dataset.\')\n    parser.add_argument(\'annotations_filename\', type=str,\n        help=\'Path to the annotations file\',\n        default=\'/media/deep/datasets/CelebA/Anno/list_attr_celeba.txt\')\n    parser.add_argument(\'output_filename\', type=str,\n        help=\'Filename to use for the file containing the attribute vectors.\')\n    parser.add_argument(\'--batch_size\', type=int,\n        help=\'Number of images to process in a batch.\', default=128)\n    parser.add_argument(\'--image_size\', type=int,\n        help=\'Image size (height, width) in pixels.\', default=64)\n    parser.add_argument(\'--latent_var_size\', type=int,\n        help=\'Dimensionality of the latent variable.\', default=100)\n    parser.add_argument(\'--seed\', type=int,\n        help=\'Random seed.\', default=666)\n\n    return parser.parse_args(argv)\n  \n    \nif __name__ == \'__main__\':\n    main(parse_arguments(sys.argv[1:]))\n'"
src/generative/modify_attribute.py,13,"b'# MIT License\n# \n# Copyright (c) 2017 David Sandberg\n# \n# Permission is hereby granted, free of charge, to any person obtaining a copy\n# of this software and associated documentation files (the ""Software""), to deal\n# in the Software without restriction, including without limitation the rights\n# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n# copies of the Software, and to permit persons to whom the Software is\n# furnished to do so, subject to the following conditions:\n# \n# The above copyright notice and this permission notice shall be included in all\n# copies or substantial portions of the Software.\n# \n# THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n# SOFTWARE.\n\n""""""Modify attributes of images using attribute vectors calculated using\n\'calculate_attribute_vectors.py\'. Images are generated from latent variables of\nthe CelebA dataset.\n""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\nimport sys\nimport argparse\nimport importlib\nimport facenet\nimport os\nimport numpy as np\nimport h5py\nimport math\nfrom scipy import misc\n\ndef main(args):\n  \n    img_mean = np.array([134.10714722, 102.52040863, 87.15436554])\n    img_stddev = np.sqrt(np.array([3941.30175781, 2856.94287109, 2519.35791016]))\n    \n    vae_def = importlib.import_module(args.vae_def)\n    vae = vae_def.Vae(args.latent_var_size)\n    gen_image_size = vae.get_image_size()\n\n    with tf.Graph().as_default():\n        tf.set_random_seed(args.seed)\n        \n        images = tf.placeholder(tf.float32, shape=(None,gen_image_size,gen_image_size,3), name=\'input\')\n        \n        # Normalize\n        images_norm = (images-img_mean) / img_stddev\n\n        # Resize to appropriate size for the encoder \n        images_norm_resize = tf.image.resize_images(images_norm, (gen_image_size,gen_image_size))\n        \n        # Create encoder network\n        mean, log_variance = vae.encoder(images_norm_resize, True)\n        \n        epsilon = tf.random_normal((tf.shape(mean)[0], args.latent_var_size))\n        std = tf.exp(log_variance/2)\n        latent_var = mean + epsilon * std\n        \n        # Create decoder\n        reconstructed_norm = vae.decoder(latent_var, False)\n        \n        # Un-normalize\n        reconstructed = (reconstructed_norm*img_stddev) + img_mean\n\n        # Create a saver\n        saver = tf.train.Saver(tf.trainable_variables(), max_to_keep=3)\n        \n        # Start running operations on the Graph\n        gpu_memory_fraction = 1.0\n        gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=gpu_memory_fraction)\n        sess = tf.Session(config=tf.ConfigProto(gpu_options=gpu_options, log_device_placement=False))\n        sess.run(tf.global_variables_initializer())\n        sess.run(tf.local_variables_initializer())\n        coord = tf.train.Coordinator()\n        tf.train.start_queue_runners(coord=coord, sess=sess)\n        \n\n        with sess.as_default():\n          \n            vae_checkpoint = os.path.expanduser(args.vae_checkpoint)\n            print(\'Restoring VAE checkpoint: %s\' % vae_checkpoint)\n            saver.restore(sess, vae_checkpoint)\n           \n            filename = os.path.expanduser(args.attributes_filename)\n            with h5py.File(filename,\'r\') as f:\n                latent_vars = np.array(f.get(\'latent_vars\'))\n                attributes = np.array(f.get(\'attributes\'))\n                #fields = np.array(f.get(\'fields\'))\n                attribute_vectors = np.array(f.get(\'attribute_vectors\'))\n\n            # Reconstruct faces while adding varying amount of the selected attribute vector\n            attribute_index = 31 # 31: \'Smiling\'\n            image_indices = [8,11,13,18,19,26,31,39,47,54,56,57,58,59,60,73]\n            nrof_images = len(image_indices)\n            nrof_interp_steps = 10\n            sweep_latent_var = np.zeros((nrof_interp_steps*nrof_images, args.latent_var_size), np.float32)\n            for j in range(nrof_images):\n                image_index = image_indices[j]\n                idx = np.argwhere(attributes[:,attribute_index]==-1)[image_index,0]\n                for i in range(nrof_interp_steps):\n                    sweep_latent_var[i+nrof_interp_steps*j,:] = latent_vars[idx,:] + 5.0*i/nrof_interp_steps*attribute_vectors[attribute_index,:]\n                \n            recon = sess.run(reconstructed, feed_dict={latent_var:sweep_latent_var})\n            \n            img = facenet.put_images_on_grid(recon, shape=(nrof_interp_steps*2,int(math.ceil(nrof_images/2))))\n            \n            image_filename = os.path.expanduser(args.output_image_filename)\n            print(\'Writing generated image to %s\' % image_filename)\n            misc.imsave(image_filename, img)\n\n                    \ndef parse_arguments(argv):\n    parser = argparse.ArgumentParser()\n    \n    parser.add_argument(\'vae_def\', type=str,\n        help=\'Model definition for the variational autoencoder. Points to a module containing the definition.\')\n    parser.add_argument(\'vae_checkpoint\', type=str,\n        help=\'Checkpoint file of a pre-trained variational autoencoder.\')\n    parser.add_argument(\'attributes_filename\', type=str,\n        help=\'The file containing the attribute vectors, as generated by calculate_attribute_vectors.py.\')\n    parser.add_argument(\'output_image_filename\', type=str,\n        help=\'File to write the generated image to.\')\n    parser.add_argument(\'--latent_var_size\', type=int,\n        help=\'Dimensionality of the latent variable.\', default=100)\n    parser.add_argument(\'--seed\', type=int,\n        help=\'Random seed.\', default=666)\n\n    return parser.parse_args(argv)\n  \n    \nif __name__ == \'__main__\':\n    main(parse_arguments(sys.argv[1:]))\n'"
src/generative/train_vae.py,35,"b'# MIT License\n# \n# Copyright (c) 2017 David Sandberg\n# \n# Permission is hereby granted, free of charge, to any person obtaining a copy\n# of this software and associated documentation files (the ""Software""), to deal\n# in the Software without restriction, including without limitation the rights\n# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n# copies of the Software, and to permit persons to whom the Software is\n# furnished to do so, subject to the following conditions:\n# \n# The above copyright notice and this permission notice shall be included in all\n# copies or substantial portions of the Software.\n# \n# THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n# SOFTWARE.\n\n""""""Train a Variational Autoencoder\n""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\nimport tensorflow.contrib.slim as slim\nimport sys\nimport time\nimport importlib\nimport argparse\nimport facenet\nimport numpy as np\nimport h5py\nimport os\nfrom datetime import datetime\nfrom scipy import misc\nfrom six import iteritems\n\ndef main(args):\n  \n    img_mean = np.array([134.10714722, 102.52040863, 87.15436554])\n    img_stddev = np.sqrt(np.array([3941.30175781, 2856.94287109, 2519.35791016]))\n  \n    vae_def = importlib.import_module(args.vae_def)\n    vae = vae_def.Vae(args.latent_var_size)\n    gen_image_size = vae.get_image_size()\n\n    subdir = datetime.strftime(datetime.now(), \'%Y%m%d-%H%M%S\')\n    model_dir = os.path.join(os.path.expanduser(args.models_base_dir), subdir)\n    if not os.path.isdir(model_dir):  # Create the model directory if it doesn\'t exist\n        os.makedirs(model_dir)\n    log_file_name = os.path.join(model_dir, \'logs.h5\')\n    \n    # Write arguments to a text file\n    facenet.write_arguments_to_file(args, os.path.join(model_dir, \'arguments.txt\'))\n        \n    # Store some git revision info in a text file in the log directory\n    src_path,_ = os.path.split(os.path.realpath(__file__))\n    facenet.store_revision_info(src_path, model_dir, \' \'.join(sys.argv))\n    \n    with tf.Graph().as_default():\n        tf.set_random_seed(args.seed)\n        global_step = tf.Variable(0, trainable=False)\n        \n        train_set = facenet.get_dataset(args.data_dir)\n        image_list, _ = facenet.get_image_paths_and_labels(train_set)\n        \n        # Create the input queue\n        input_queue = tf.train.string_input_producer(image_list, shuffle=True)\n    \n        nrof_preprocess_threads = 4\n        image_per_thread = []\n        for _ in range(nrof_preprocess_threads):\n            file_contents = tf.read_file(input_queue.dequeue())\n            image = tf.image.decode_image(file_contents, channels=3)\n            image = tf.image.resize_image_with_crop_or_pad(image, args.input_image_size, args.input_image_size)\n            image.set_shape((args.input_image_size, args.input_image_size, 3))\n            image = tf.cast(image, tf.float32)\n            #pylint: disable=no-member\n            image_per_thread.append([image])\n    \n        images = tf.train.batch_join(\n            image_per_thread, batch_size=args.batch_size,\n            capacity=4 * nrof_preprocess_threads * args.batch_size,\n            allow_smaller_final_batch=False)\n        \n        # Normalize\n        images_norm = (images-img_mean) / img_stddev\n\n        # Resize to appropriate size for the encoder \n        images_norm_resize = tf.image.resize_images(images_norm, (gen_image_size,gen_image_size))\n        \n        # Create encoder network\n        mean, log_variance = vae.encoder(images_norm_resize, True)\n        \n        epsilon = tf.random_normal((tf.shape(mean)[0], args.latent_var_size))\n        std = tf.exp(log_variance/2)\n        latent_var = mean + epsilon * std\n        \n        # Create decoder network\n        reconstructed_norm = vae.decoder(latent_var, True)\n        \n        # Un-normalize\n        reconstructed = (reconstructed_norm*img_stddev) + img_mean\n        \n        # Create reconstruction loss\n        if args.reconstruction_loss_type==\'PLAIN\':\n            images_resize = tf.image.resize_images(images, (gen_image_size,gen_image_size))\n            reconstruction_loss = tf.reduce_mean(tf.reduce_sum(tf.pow(images_resize - reconstructed,2)))\n        elif args.reconstruction_loss_type==\'PERCEPTUAL\':\n            network = importlib.import_module(args.model_def)\n\n            reconstructed_norm_resize = tf.image.resize_images(reconstructed_norm, (args.input_image_size,args.input_image_size))\n\n            # Stack images from both the input batch and the reconstructed batch in a new tensor \n            shp = [-1] + images_norm.get_shape().as_list()[1:]\n            input_images = tf.reshape(tf.stack([images_norm, reconstructed_norm_resize], axis=0), shp)\n            _, end_points = network.inference(input_images, 1.0, \n                phase_train=False, bottleneck_layer_size=128, weight_decay=0.0)\n\n            # Get a list of feature names to use for loss terms\n            feature_names = args.loss_features.replace(\' \', \'\').split(\',\')\n\n            # Calculate L2 loss between original and reconstructed images in feature space\n            reconstruction_loss_list = []\n            for feature_name in feature_names:\n                feature_flat = slim.flatten(end_points[feature_name])\n                image_feature, reconstructed_feature = tf.unstack(tf.reshape(feature_flat, [2,args.batch_size,-1]), num=2, axis=0)\n                reconstruction_loss = tf.reduce_mean(tf.reduce_sum(tf.pow(image_feature-reconstructed_feature, 2)), name=feature_name+\'_loss\')\n                reconstruction_loss_list.append(reconstruction_loss)\n            # Sum up the losses in for the different features\n            reconstruction_loss = tf.add_n(reconstruction_loss_list, \'reconstruction_loss\')\n        else:\n            pass\n        \n        # Create KL divergence loss\n        kl_loss = kl_divergence_loss(mean, log_variance)\n        kl_loss_mean = tf.reduce_mean(kl_loss)\n        \n        total_loss = args.alfa*kl_loss_mean + args.beta*reconstruction_loss\n        \n        learning_rate = tf.train.exponential_decay(args.initial_learning_rate, global_step,\n            args.learning_rate_decay_steps, args.learning_rate_decay_factor, staircase=True)\n        \n        # Calculate gradients and make sure not to include parameters for the perceptual loss model\n        opt = tf.train.AdamOptimizer(learning_rate)\n        grads = opt.compute_gradients(total_loss, var_list=get_variables_to_train())\n        \n        # Apply gradients\n        apply_gradient_op = opt.apply_gradients(grads, global_step=global_step)\n        with tf.control_dependencies([apply_gradient_op]):\n            train_op = tf.no_op(name=\'train\')\n\n        # Create a saver\n        saver = tf.train.Saver(tf.trainable_variables(), max_to_keep=3)\n        \n        facenet_saver = tf.train.Saver(get_facenet_variables_to_restore())\n\n        # Start running operations on the Graph\n        gpu_memory_fraction = 1.0\n        gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=gpu_memory_fraction)\n        sess = tf.Session(config=tf.ConfigProto(gpu_options=gpu_options, log_device_placement=False))\n        sess.run(tf.global_variables_initializer())\n        sess.run(tf.local_variables_initializer())\n        coord = tf.train.Coordinator()\n        tf.train.start_queue_runners(coord=coord, sess=sess)\n\n        with sess.as_default():\n            \n            if args.reconstruction_loss_type==\'PERCEPTUAL\':\n                if not args.pretrained_model:\n                    raise ValueError(\'A pretrained model must be specified when using perceptual loss\')\n                pretrained_model_exp = os.path.expanduser(args.pretrained_model)\n                print(\'Restoring pretrained model: %s\' % pretrained_model_exp)\n                facenet_saver.restore(sess, pretrained_model_exp)\n          \n            log = {\n                \'total_loss\': np.zeros((0,), np.float),\n                \'reconstruction_loss\': np.zeros((0,), np.float),\n                \'kl_loss\': np.zeros((0,), np.float),\n                \'learning_rate\': np.zeros((0,), np.float),\n                }\n            \n            step = 0\n            print(\'Running training\')\n            while step < args.max_nrof_steps:\n                start_time = time.time()\n                step += 1\n                save_state = step>0 and (step % args.save_every_n_steps==0 or step==args.max_nrof_steps)\n                if save_state:\n                    _, reconstruction_loss_, kl_loss_mean_, total_loss_, learning_rate_, rec_ = sess.run(\n                          [train_op, reconstruction_loss, kl_loss_mean, total_loss, learning_rate, reconstructed])\n                    img = facenet.put_images_on_grid(rec_, shape=(16,8))\n                    misc.imsave(os.path.join(model_dir, \'reconstructed_%06d.png\' % step), img)\n                else:\n                    _, reconstruction_loss_, kl_loss_mean_, total_loss_, learning_rate_ = sess.run(\n                          [train_op, reconstruction_loss, kl_loss_mean, total_loss, learning_rate])\n                log[\'total_loss\'] = np.append(log[\'total_loss\'], total_loss_)\n                log[\'reconstruction_loss\'] = np.append(log[\'reconstruction_loss\'], reconstruction_loss_)\n                log[\'kl_loss\'] = np.append(log[\'kl_loss\'], kl_loss_mean_)\n                log[\'learning_rate\'] = np.append(log[\'learning_rate\'], learning_rate_)\n\n                duration = time.time() - start_time\n                print(\'Step: %d \\tTime: %.3f \\trec_loss: %.3f \\tkl_loss: %.3f \\ttotal_loss: %.3f\' % (step, duration, reconstruction_loss_, kl_loss_mean_, total_loss_))\n\n                if save_state:\n                    print(\'Saving checkpoint file\')\n                    checkpoint_path = os.path.join(model_dir, \'model.ckpt\')\n                    saver.save(sess, checkpoint_path, global_step=step, write_meta_graph=False)\n                    print(\'Saving log\')\n                    with h5py.File(log_file_name, \'w\') as f:\n                        for key, value in iteritems(log):\n                            f.create_dataset(key, data=value)\n\ndef get_variables_to_train():\n    train_variables = []\n    for var in tf.trainable_variables():\n        if \'Inception\' not in var.name:\n            train_variables.append(var)\n    return train_variables\n\ndef get_facenet_variables_to_restore():\n    facenet_variables = []\n    for var in tf.global_variables():\n        if var.name.startswith(\'Inception\'):\n            if \'Adam\' not in var.name:\n                facenet_variables.append(var)\n    return facenet_variables\n\ndef kl_divergence_loss(mean, log_variance):\n    kl = 0.5 * tf.reduce_sum( tf.exp(log_variance) + tf.square(mean) - 1.0 - log_variance, reduction_indices = 1)\n    return kl\n\ndef parse_arguments(argv):\n    parser = argparse.ArgumentParser()\n    \n    parser.add_argument(\'vae_def\', type=str,\n        help=\'Model definition for the variational autoencoder. Points to a module containing the definition.\')\n    parser.add_argument(\'data_dir\', type=str,\n        help=\'Path to the data directory containing aligned face patches.\')\n    parser.add_argument(\'model_def\', type=str,\n        help=\'Model definition. Points to a module containing the definition of the inference graph.\')\n    parser.add_argument(\'pretrained_model\', type=str,\n        help=\'Pretrained model to use to calculate features for perceptual loss.\')\n    parser.add_argument(\'--models_base_dir\', type=str,\n        help=\'Directory where to write trained models and checkpoints.\', default=\'~/vae\')\n    parser.add_argument(\'--loss_features\', type=str,\n        help=\'Comma separated list of features to use for perceptual loss. Features should be defined \' +\n          \'in the end_points dictionary.\', default=\'Conv2d_1a_3x3,Conv2d_2a_3x3, Conv2d_2b_3x3\')\n    parser.add_argument(\'--reconstruction_loss_type\', type=str, choices=[\'PLAIN\', \'PERCEPTUAL\'],\n        help=\'The type of reconstruction loss to use\', default=\'PERCEPTUAL\')\n    parser.add_argument(\'--max_nrof_steps\', type=int,\n        help=\'Number of steps to run.\', default=50000)\n    parser.add_argument(\'--save_every_n_steps\', type=int,\n        help=\'Number of steps between storing of model checkpoint and log files\', default=500)\n    parser.add_argument(\'--batch_size\', type=int,\n        help=\'Number of images to process in a batch.\', default=128)\n    parser.add_argument(\'--input_image_size\', type=int,\n        help=\'Image size of input images (height, width) in pixels. If perceptual loss is used this \' \n        + \'should be the input image size for the perceptual loss model\', default=160)\n    parser.add_argument(\'--latent_var_size\', type=int,\n        help=\'Dimensionality of the latent variable.\', default=100)\n    parser.add_argument(\'--initial_learning_rate\', type=float,\n        help=\'Initial learning rate.\', default=0.0005)\n    parser.add_argument(\'--learning_rate_decay_steps\', type=int,\n        help=\'Number of steps between learning rate decay.\', default=1)\n    parser.add_argument(\'--learning_rate_decay_factor\', type=float,\n        help=\'Learning rate decay factor.\', default=1.0)\n    parser.add_argument(\'--seed\', type=int,\n        help=\'Random seed.\', default=666)\n    parser.add_argument(\'--alfa\', type=float,\n        help=\'Kullback-Leibler divergence loss factor.\', default=1.0)\n    parser.add_argument(\'--beta\', type=float,\n        help=\'Reconstruction loss factor.\', default=0.5)\n    \n    return parser.parse_args(argv)\n  \n    \nif __name__ == \'__main__\':\n    main(parse_arguments(sys.argv[1:]))\n'"
src/models/__init__.py,0,b'# flake8: noqa\n\n'
src/models/dummy.py,3,"b'""""""Dummy model used only for testing\n""""""\n# MIT License\n# \n# Copyright (c) 2016 David Sandberg\n# \n# Permission is hereby granted, free of charge, to any person obtaining a copy\n# of this software and associated documentation files (the ""Software""), to deal\n# in the Software without restriction, including without limitation the rights\n# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n# copies of the Software, and to permit persons to whom the Software is\n# furnished to do so, subject to the following conditions:\n# \n# The above copyright notice and this permission notice shall be included in all\n# copies or substantial portions of the Software.\n# \n# THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n# SOFTWARE.\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\nimport tensorflow.contrib.slim as slim\nimport numpy as np\n  \ndef inference(images, keep_probability, phase_train=True,  # @UnusedVariable\n              bottleneck_layer_size=128, bottleneck_layer_activation=None, weight_decay=0.0, reuse=None):  # @UnusedVariable\n    batch_norm_params = {\n        # Decay for the moving averages.\n        \'decay\': 0.995,\n        # epsilon to prevent 0s in variance.\n        \'epsilon\': 0.001,\n        # force in-place updates of mean and variance estimates\n        \'updates_collections\': None,\n        # Moving averages ends up in the trainable variables collection\n        \'variables_collections\': [ tf.GraphKeys.TRAINABLE_VARIABLES ],\n    }\n    \n    with slim.arg_scope([slim.conv2d, slim.fully_connected],\n                        weights_initializer=tf.truncated_normal_initializer(stddev=0.1),\n                        weights_regularizer=slim.l2_regularizer(weight_decay),\n                        normalizer_fn=slim.batch_norm,\n                        normalizer_params=batch_norm_params):\n        size = np.prod(images.get_shape()[1:].as_list())\n        net = slim.fully_connected(tf.reshape(images, (-1,size)), bottleneck_layer_size, activation_fn=None, \n                scope=\'Bottleneck\', reuse=False)\n        return net, None\n'"
src/models/inception_resnet_v1.py,30,"b'# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n""""""Contains the definition of the Inception Resnet V1 architecture.\nAs described in http://arxiv.org/abs/1602.07261.\n  Inception-v4, Inception-ResNet and the Impact of Residual Connections\n    on Learning\n  Christian Szegedy, Sergey Ioffe, Vincent Vanhoucke, Alex Alemi\n""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\nimport tensorflow.contrib.slim as slim\n\n# Inception-Resnet-A\ndef block35(net, scale=1.0, activation_fn=tf.nn.relu, scope=None, reuse=None):\n    """"""Builds the 35x35 resnet block.""""""\n    with tf.variable_scope(scope, \'Block35\', [net], reuse=reuse):\n        with tf.variable_scope(\'Branch_0\'):\n            tower_conv = slim.conv2d(net, 32, 1, scope=\'Conv2d_1x1\')\n        with tf.variable_scope(\'Branch_1\'):\n            tower_conv1_0 = slim.conv2d(net, 32, 1, scope=\'Conv2d_0a_1x1\')\n            tower_conv1_1 = slim.conv2d(tower_conv1_0, 32, 3, scope=\'Conv2d_0b_3x3\')\n        with tf.variable_scope(\'Branch_2\'):\n            tower_conv2_0 = slim.conv2d(net, 32, 1, scope=\'Conv2d_0a_1x1\')\n            tower_conv2_1 = slim.conv2d(tower_conv2_0, 32, 3, scope=\'Conv2d_0b_3x3\')\n            tower_conv2_2 = slim.conv2d(tower_conv2_1, 32, 3, scope=\'Conv2d_0c_3x3\')\n        mixed = tf.concat([tower_conv, tower_conv1_1, tower_conv2_2], 3)\n        up = slim.conv2d(mixed, net.get_shape()[3], 1, normalizer_fn=None,\n                         activation_fn=None, scope=\'Conv2d_1x1\')\n        net += scale * up\n        if activation_fn:\n            net = activation_fn(net)\n    return net\n\n# Inception-Resnet-B\ndef block17(net, scale=1.0, activation_fn=tf.nn.relu, scope=None, reuse=None):\n    """"""Builds the 17x17 resnet block.""""""\n    with tf.variable_scope(scope, \'Block17\', [net], reuse=reuse):\n        with tf.variable_scope(\'Branch_0\'):\n            tower_conv = slim.conv2d(net, 128, 1, scope=\'Conv2d_1x1\')\n        with tf.variable_scope(\'Branch_1\'):\n            tower_conv1_0 = slim.conv2d(net, 128, 1, scope=\'Conv2d_0a_1x1\')\n            tower_conv1_1 = slim.conv2d(tower_conv1_0, 128, [1, 7],\n                                        scope=\'Conv2d_0b_1x7\')\n            tower_conv1_2 = slim.conv2d(tower_conv1_1, 128, [7, 1],\n                                        scope=\'Conv2d_0c_7x1\')\n        mixed = tf.concat([tower_conv, tower_conv1_2], 3)\n        up = slim.conv2d(mixed, net.get_shape()[3], 1, normalizer_fn=None,\n                         activation_fn=None, scope=\'Conv2d_1x1\')\n        net += scale * up\n        if activation_fn:\n            net = activation_fn(net)\n    return net\n\n\n# Inception-Resnet-C\ndef block8(net, scale=1.0, activation_fn=tf.nn.relu, scope=None, reuse=None):\n    """"""Builds the 8x8 resnet block.""""""\n    with tf.variable_scope(scope, \'Block8\', [net], reuse=reuse):\n        with tf.variable_scope(\'Branch_0\'):\n            tower_conv = slim.conv2d(net, 192, 1, scope=\'Conv2d_1x1\')\n        with tf.variable_scope(\'Branch_1\'):\n            tower_conv1_0 = slim.conv2d(net, 192, 1, scope=\'Conv2d_0a_1x1\')\n            tower_conv1_1 = slim.conv2d(tower_conv1_0, 192, [1, 3],\n                                        scope=\'Conv2d_0b_1x3\')\n            tower_conv1_2 = slim.conv2d(tower_conv1_1, 192, [3, 1],\n                                        scope=\'Conv2d_0c_3x1\')\n        mixed = tf.concat([tower_conv, tower_conv1_2], 3)\n        up = slim.conv2d(mixed, net.get_shape()[3], 1, normalizer_fn=None,\n                         activation_fn=None, scope=\'Conv2d_1x1\')\n        net += scale * up\n        if activation_fn:\n            net = activation_fn(net)\n    return net\n  \ndef reduction_a(net, k, l, m, n):\n    with tf.variable_scope(\'Branch_0\'):\n        tower_conv = slim.conv2d(net, n, 3, stride=2, padding=\'VALID\',\n                                 scope=\'Conv2d_1a_3x3\')\n    with tf.variable_scope(\'Branch_1\'):\n        tower_conv1_0 = slim.conv2d(net, k, 1, scope=\'Conv2d_0a_1x1\')\n        tower_conv1_1 = slim.conv2d(tower_conv1_0, l, 3,\n                                    scope=\'Conv2d_0b_3x3\')\n        tower_conv1_2 = slim.conv2d(tower_conv1_1, m, 3,\n                                    stride=2, padding=\'VALID\',\n                                    scope=\'Conv2d_1a_3x3\')\n    with tf.variable_scope(\'Branch_2\'):\n        tower_pool = slim.max_pool2d(net, 3, stride=2, padding=\'VALID\',\n                                     scope=\'MaxPool_1a_3x3\')\n    net = tf.concat([tower_conv, tower_conv1_2, tower_pool], 3)\n    return net\n\ndef reduction_b(net):\n    with tf.variable_scope(\'Branch_0\'):\n        tower_conv = slim.conv2d(net, 256, 1, scope=\'Conv2d_0a_1x1\')\n        tower_conv_1 = slim.conv2d(tower_conv, 384, 3, stride=2,\n                                   padding=\'VALID\', scope=\'Conv2d_1a_3x3\')\n    with tf.variable_scope(\'Branch_1\'):\n        tower_conv1 = slim.conv2d(net, 256, 1, scope=\'Conv2d_0a_1x1\')\n        tower_conv1_1 = slim.conv2d(tower_conv1, 256, 3, stride=2,\n                                    padding=\'VALID\', scope=\'Conv2d_1a_3x3\')\n    with tf.variable_scope(\'Branch_2\'):\n        tower_conv2 = slim.conv2d(net, 256, 1, scope=\'Conv2d_0a_1x1\')\n        tower_conv2_1 = slim.conv2d(tower_conv2, 256, 3,\n                                    scope=\'Conv2d_0b_3x3\')\n        tower_conv2_2 = slim.conv2d(tower_conv2_1, 256, 3, stride=2,\n                                    padding=\'VALID\', scope=\'Conv2d_1a_3x3\')\n    with tf.variable_scope(\'Branch_3\'):\n        tower_pool = slim.max_pool2d(net, 3, stride=2, padding=\'VALID\',\n                                     scope=\'MaxPool_1a_3x3\')\n    net = tf.concat([tower_conv_1, tower_conv1_1,\n                        tower_conv2_2, tower_pool], 3)\n    return net\n  \ndef inference(images, keep_probability, phase_train=True, \n              bottleneck_layer_size=128, weight_decay=0.0, reuse=None):\n    batch_norm_params = {\n        # Decay for the moving averages.\n        \'decay\': 0.995,\n        # epsilon to prevent 0s in variance.\n        \'epsilon\': 0.001,\n        # force in-place updates of mean and variance estimates\n        \'updates_collections\': None,\n        # Moving averages ends up in the trainable variables collection\n        \'variables_collections\': [ tf.GraphKeys.TRAINABLE_VARIABLES ],\n    }\n    \n    with slim.arg_scope([slim.conv2d, slim.fully_connected],\n                        weights_initializer=slim.initializers.xavier_initializer(), \n                        weights_regularizer=slim.l2_regularizer(weight_decay),\n                        normalizer_fn=slim.batch_norm,\n                        normalizer_params=batch_norm_params):\n        return inception_resnet_v1(images, is_training=phase_train,\n              dropout_keep_prob=keep_probability, bottleneck_layer_size=bottleneck_layer_size, reuse=reuse)\n\n\ndef inception_resnet_v1(inputs, is_training=True,\n                        dropout_keep_prob=0.8,\n                        bottleneck_layer_size=128,\n                        reuse=None, \n                        scope=\'InceptionResnetV1\'):\n    """"""Creates the Inception Resnet V1 model.\n    Args:\n      inputs: a 4-D tensor of size [batch_size, height, width, 3].\n      num_classes: number of predicted classes.\n      is_training: whether is training or not.\n      dropout_keep_prob: float, the fraction to keep before final layer.\n      reuse: whether or not the network and its variables should be reused. To be\n        able to reuse \'scope\' must be given.\n      scope: Optional variable_scope.\n    Returns:\n      logits: the logits outputs of the model.\n      end_points: the set of end_points from the inception model.\n    """"""\n    end_points = {}\n  \n    with tf.variable_scope(scope, \'InceptionResnetV1\', [inputs], reuse=reuse):\n        with slim.arg_scope([slim.batch_norm, slim.dropout],\n                            is_training=is_training):\n            with slim.arg_scope([slim.conv2d, slim.max_pool2d, slim.avg_pool2d],\n                                stride=1, padding=\'SAME\'):\n      \n                # 149 x 149 x 32\n                net = slim.conv2d(inputs, 32, 3, stride=2, padding=\'VALID\',\n                                  scope=\'Conv2d_1a_3x3\')\n                end_points[\'Conv2d_1a_3x3\'] = net\n                # 147 x 147 x 32\n                net = slim.conv2d(net, 32, 3, padding=\'VALID\',\n                                  scope=\'Conv2d_2a_3x3\')\n                end_points[\'Conv2d_2a_3x3\'] = net\n                # 147 x 147 x 64\n                net = slim.conv2d(net, 64, 3, scope=\'Conv2d_2b_3x3\')\n                end_points[\'Conv2d_2b_3x3\'] = net\n                # 73 x 73 x 64\n                net = slim.max_pool2d(net, 3, stride=2, padding=\'VALID\',\n                                      scope=\'MaxPool_3a_3x3\')\n                end_points[\'MaxPool_3a_3x3\'] = net\n                # 73 x 73 x 80\n                net = slim.conv2d(net, 80, 1, padding=\'VALID\',\n                                  scope=\'Conv2d_3b_1x1\')\n                end_points[\'Conv2d_3b_1x1\'] = net\n                # 71 x 71 x 192\n                net = slim.conv2d(net, 192, 3, padding=\'VALID\',\n                                  scope=\'Conv2d_4a_3x3\')\n                end_points[\'Conv2d_4a_3x3\'] = net\n                # 35 x 35 x 256\n                net = slim.conv2d(net, 256, 3, stride=2, padding=\'VALID\',\n                                  scope=\'Conv2d_4b_3x3\')\n                end_points[\'Conv2d_4b_3x3\'] = net\n                \n                # 5 x Inception-resnet-A\n                net = slim.repeat(net, 5, block35, scale=0.17)\n                end_points[\'Mixed_5a\'] = net\n        \n                # Reduction-A\n                with tf.variable_scope(\'Mixed_6a\'):\n                    net = reduction_a(net, 192, 192, 256, 384)\n                end_points[\'Mixed_6a\'] = net\n                \n                # 10 x Inception-Resnet-B\n                net = slim.repeat(net, 10, block17, scale=0.10)\n                end_points[\'Mixed_6b\'] = net\n                \n                # Reduction-B\n                with tf.variable_scope(\'Mixed_7a\'):\n                    net = reduction_b(net)\n                end_points[\'Mixed_7a\'] = net\n                \n                # 5 x Inception-Resnet-C\n                net = slim.repeat(net, 5, block8, scale=0.20)\n                end_points[\'Mixed_8a\'] = net\n                \n                net = block8(net, activation_fn=None)\n                end_points[\'Mixed_8b\'] = net\n                \n                with tf.variable_scope(\'Logits\'):\n                    end_points[\'PrePool\'] = net\n                    #pylint: disable=no-member\n                    net = slim.avg_pool2d(net, net.get_shape()[1:3], padding=\'VALID\',\n                                          scope=\'AvgPool_1a_8x8\')\n                    net = slim.flatten(net)\n          \n                    net = slim.dropout(net, dropout_keep_prob, is_training=is_training,\n                                       scope=\'Dropout\')\n          \n                    end_points[\'PreLogitsFlatten\'] = net\n                \n                net = slim.fully_connected(net, bottleneck_layer_size, activation_fn=None, \n                        scope=\'Bottleneck\', reuse=False)\n  \n    return net, end_points\n'"
src/models/inception_resnet_v2.py,36,"b'# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n""""""Contains the definition of the Inception Resnet V2 architecture.\nAs described in http://arxiv.org/abs/1602.07261.\n  Inception-v4, Inception-ResNet and the Impact of Residual Connections\n    on Learning\n  Christian Szegedy, Sergey Ioffe, Vincent Vanhoucke, Alex Alemi\n""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\nimport tensorflow.contrib.slim as slim\n\n# Inception-Resnet-A\ndef block35(net, scale=1.0, activation_fn=tf.nn.relu, scope=None, reuse=None):\n    """"""Builds the 35x35 resnet block.""""""\n    with tf.variable_scope(scope, \'Block35\', [net], reuse=reuse):\n        with tf.variable_scope(\'Branch_0\'):\n            tower_conv = slim.conv2d(net, 32, 1, scope=\'Conv2d_1x1\')\n        with tf.variable_scope(\'Branch_1\'):\n            tower_conv1_0 = slim.conv2d(net, 32, 1, scope=\'Conv2d_0a_1x1\')\n            tower_conv1_1 = slim.conv2d(tower_conv1_0, 32, 3, scope=\'Conv2d_0b_3x3\')\n        with tf.variable_scope(\'Branch_2\'):\n            tower_conv2_0 = slim.conv2d(net, 32, 1, scope=\'Conv2d_0a_1x1\')\n            tower_conv2_1 = slim.conv2d(tower_conv2_0, 48, 3, scope=\'Conv2d_0b_3x3\')\n            tower_conv2_2 = slim.conv2d(tower_conv2_1, 64, 3, scope=\'Conv2d_0c_3x3\')\n        mixed = tf.concat([tower_conv, tower_conv1_1, tower_conv2_2], 3)\n        up = slim.conv2d(mixed, net.get_shape()[3], 1, normalizer_fn=None,\n                         activation_fn=None, scope=\'Conv2d_1x1\')\n        net += scale * up\n        if activation_fn:\n            net = activation_fn(net)\n    return net\n\n# Inception-Resnet-B\ndef block17(net, scale=1.0, activation_fn=tf.nn.relu, scope=None, reuse=None):\n    """"""Builds the 17x17 resnet block.""""""\n    with tf.variable_scope(scope, \'Block17\', [net], reuse=reuse):\n        with tf.variable_scope(\'Branch_0\'):\n            tower_conv = slim.conv2d(net, 192, 1, scope=\'Conv2d_1x1\')\n        with tf.variable_scope(\'Branch_1\'):\n            tower_conv1_0 = slim.conv2d(net, 128, 1, scope=\'Conv2d_0a_1x1\')\n            tower_conv1_1 = slim.conv2d(tower_conv1_0, 160, [1, 7],\n                                        scope=\'Conv2d_0b_1x7\')\n            tower_conv1_2 = slim.conv2d(tower_conv1_1, 192, [7, 1],\n                                        scope=\'Conv2d_0c_7x1\')\n        mixed = tf.concat([tower_conv, tower_conv1_2], 3)\n        up = slim.conv2d(mixed, net.get_shape()[3], 1, normalizer_fn=None,\n                         activation_fn=None, scope=\'Conv2d_1x1\')\n        net += scale * up\n        if activation_fn:\n            net = activation_fn(net)\n    return net\n\n\n# Inception-Resnet-C\ndef block8(net, scale=1.0, activation_fn=tf.nn.relu, scope=None, reuse=None):\n    """"""Builds the 8x8 resnet block.""""""\n    with tf.variable_scope(scope, \'Block8\', [net], reuse=reuse):\n        with tf.variable_scope(\'Branch_0\'):\n            tower_conv = slim.conv2d(net, 192, 1, scope=\'Conv2d_1x1\')\n        with tf.variable_scope(\'Branch_1\'):\n            tower_conv1_0 = slim.conv2d(net, 192, 1, scope=\'Conv2d_0a_1x1\')\n            tower_conv1_1 = slim.conv2d(tower_conv1_0, 224, [1, 3],\n                                        scope=\'Conv2d_0b_1x3\')\n            tower_conv1_2 = slim.conv2d(tower_conv1_1, 256, [3, 1],\n                                        scope=\'Conv2d_0c_3x1\')\n        mixed = tf.concat([tower_conv, tower_conv1_2], 3)\n        up = slim.conv2d(mixed, net.get_shape()[3], 1, normalizer_fn=None,\n                         activation_fn=None, scope=\'Conv2d_1x1\')\n        net += scale * up\n        if activation_fn:\n            net = activation_fn(net)\n    return net\n  \ndef inference(images, keep_probability, phase_train=True, \n              bottleneck_layer_size=128, weight_decay=0.0, reuse=None):\n    batch_norm_params = {\n        # Decay for the moving averages.\n        \'decay\': 0.995,\n        # epsilon to prevent 0s in variance.\n        \'epsilon\': 0.001,\n        # force in-place updates of mean and variance estimates\n        \'updates_collections\': None,\n        # Moving averages ends up in the trainable variables collection\n        \'variables_collections\': [ tf.GraphKeys.TRAINABLE_VARIABLES ],\n}\n    with slim.arg_scope([slim.conv2d, slim.fully_connected],\n                        weights_initializer=slim.initializers.xavier_initializer(), \n                        weights_regularizer=slim.l2_regularizer(weight_decay),\n                        normalizer_fn=slim.batch_norm,\n                        normalizer_params=batch_norm_params):\n        return inception_resnet_v2(images, is_training=phase_train,\n              dropout_keep_prob=keep_probability, bottleneck_layer_size=bottleneck_layer_size, reuse=reuse)\n\n\ndef inception_resnet_v2(inputs, is_training=True,\n                        dropout_keep_prob=0.8,\n                        bottleneck_layer_size=128,\n                        reuse=None,\n                        scope=\'InceptionResnetV2\'):\n    """"""Creates the Inception Resnet V2 model.\n    Args:\n      inputs: a 4-D tensor of size [batch_size, height, width, 3].\n      num_classes: number of predicted classes.\n      is_training: whether is training or not.\n      dropout_keep_prob: float, the fraction to keep before final layer.\n      reuse: whether or not the network and its variables should be reused. To be\n        able to reuse \'scope\' must be given.\n      scope: Optional variable_scope.\n    Returns:\n      logits: the logits outputs of the model.\n      end_points: the set of end_points from the inception model.\n    """"""\n    end_points = {}\n  \n    with tf.variable_scope(scope, \'InceptionResnetV2\', [inputs], reuse=reuse):\n        with slim.arg_scope([slim.batch_norm, slim.dropout],\n                            is_training=is_training):\n            with slim.arg_scope([slim.conv2d, slim.max_pool2d, slim.avg_pool2d],\n                                stride=1, padding=\'SAME\'):\n      \n                # 149 x 149 x 32\n                net = slim.conv2d(inputs, 32, 3, stride=2, padding=\'VALID\',\n                                  scope=\'Conv2d_1a_3x3\')\n                end_points[\'Conv2d_1a_3x3\'] = net\n                # 147 x 147 x 32\n                net = slim.conv2d(net, 32, 3, padding=\'VALID\',\n                                  scope=\'Conv2d_2a_3x3\')\n                end_points[\'Conv2d_2a_3x3\'] = net\n                # 147 x 147 x 64\n                net = slim.conv2d(net, 64, 3, scope=\'Conv2d_2b_3x3\')\n                end_points[\'Conv2d_2b_3x3\'] = net\n                # 73 x 73 x 64\n                net = slim.max_pool2d(net, 3, stride=2, padding=\'VALID\',\n                                      scope=\'MaxPool_3a_3x3\')\n                end_points[\'MaxPool_3a_3x3\'] = net\n                # 73 x 73 x 80\n                net = slim.conv2d(net, 80, 1, padding=\'VALID\',\n                                  scope=\'Conv2d_3b_1x1\')\n                end_points[\'Conv2d_3b_1x1\'] = net\n                # 71 x 71 x 192\n                net = slim.conv2d(net, 192, 3, padding=\'VALID\',\n                                  scope=\'Conv2d_4a_3x3\')\n                end_points[\'Conv2d_4a_3x3\'] = net\n                # 35 x 35 x 192\n                net = slim.max_pool2d(net, 3, stride=2, padding=\'VALID\',\n                                      scope=\'MaxPool_5a_3x3\')\n                end_points[\'MaxPool_5a_3x3\'] = net\n        \n                # 35 x 35 x 320\n                with tf.variable_scope(\'Mixed_5b\'):\n                    with tf.variable_scope(\'Branch_0\'):\n                        tower_conv = slim.conv2d(net, 96, 1, scope=\'Conv2d_1x1\')\n                    with tf.variable_scope(\'Branch_1\'):\n                        tower_conv1_0 = slim.conv2d(net, 48, 1, scope=\'Conv2d_0a_1x1\')\n                        tower_conv1_1 = slim.conv2d(tower_conv1_0, 64, 5,\n                                                    scope=\'Conv2d_0b_5x5\')\n                    with tf.variable_scope(\'Branch_2\'):\n                        tower_conv2_0 = slim.conv2d(net, 64, 1, scope=\'Conv2d_0a_1x1\')\n                        tower_conv2_1 = slim.conv2d(tower_conv2_0, 96, 3,\n                                                    scope=\'Conv2d_0b_3x3\')\n                        tower_conv2_2 = slim.conv2d(tower_conv2_1, 96, 3,\n                                                    scope=\'Conv2d_0c_3x3\')\n                    with tf.variable_scope(\'Branch_3\'):\n                        tower_pool = slim.avg_pool2d(net, 3, stride=1, padding=\'SAME\',\n                                                     scope=\'AvgPool_0a_3x3\')\n                        tower_pool_1 = slim.conv2d(tower_pool, 64, 1,\n                                                   scope=\'Conv2d_0b_1x1\')\n                    net = tf.concat([tower_conv, tower_conv1_1,\n                                        tower_conv2_2, tower_pool_1], 3)\n        \n                end_points[\'Mixed_5b\'] = net\n                net = slim.repeat(net, 10, block35, scale=0.17)\n        \n                # 17 x 17 x 1024\n                with tf.variable_scope(\'Mixed_6a\'):\n                    with tf.variable_scope(\'Branch_0\'):\n                        tower_conv = slim.conv2d(net, 384, 3, stride=2, padding=\'VALID\',\n                                                 scope=\'Conv2d_1a_3x3\')\n                    with tf.variable_scope(\'Branch_1\'):\n                        tower_conv1_0 = slim.conv2d(net, 256, 1, scope=\'Conv2d_0a_1x1\')\n                        tower_conv1_1 = slim.conv2d(tower_conv1_0, 256, 3,\n                                                    scope=\'Conv2d_0b_3x3\')\n                        tower_conv1_2 = slim.conv2d(tower_conv1_1, 384, 3,\n                                                    stride=2, padding=\'VALID\',\n                                                    scope=\'Conv2d_1a_3x3\')\n                    with tf.variable_scope(\'Branch_2\'):\n                        tower_pool = slim.max_pool2d(net, 3, stride=2, padding=\'VALID\',\n                                                     scope=\'MaxPool_1a_3x3\')\n                    net = tf.concat([tower_conv, tower_conv1_2, tower_pool], 3)\n        \n                end_points[\'Mixed_6a\'] = net\n                net = slim.repeat(net, 20, block17, scale=0.10)\n        \n                with tf.variable_scope(\'Mixed_7a\'):\n                    with tf.variable_scope(\'Branch_0\'):\n                        tower_conv = slim.conv2d(net, 256, 1, scope=\'Conv2d_0a_1x1\')\n                        tower_conv_1 = slim.conv2d(tower_conv, 384, 3, stride=2,\n                                                   padding=\'VALID\', scope=\'Conv2d_1a_3x3\')\n                    with tf.variable_scope(\'Branch_1\'):\n                        tower_conv1 = slim.conv2d(net, 256, 1, scope=\'Conv2d_0a_1x1\')\n                        tower_conv1_1 = slim.conv2d(tower_conv1, 288, 3, stride=2,\n                                                    padding=\'VALID\', scope=\'Conv2d_1a_3x3\')\n                    with tf.variable_scope(\'Branch_2\'):\n                        tower_conv2 = slim.conv2d(net, 256, 1, scope=\'Conv2d_0a_1x1\')\n                        tower_conv2_1 = slim.conv2d(tower_conv2, 288, 3,\n                                                    scope=\'Conv2d_0b_3x3\')\n                        tower_conv2_2 = slim.conv2d(tower_conv2_1, 320, 3, stride=2,\n                                                    padding=\'VALID\', scope=\'Conv2d_1a_3x3\')\n                    with tf.variable_scope(\'Branch_3\'):\n                        tower_pool = slim.max_pool2d(net, 3, stride=2, padding=\'VALID\',\n                                                     scope=\'MaxPool_1a_3x3\')\n                    net = tf.concat([tower_conv_1, tower_conv1_1,\n                                        tower_conv2_2, tower_pool], 3)\n        \n                end_points[\'Mixed_7a\'] = net\n        \n                net = slim.repeat(net, 9, block8, scale=0.20)\n                net = block8(net, activation_fn=None)\n        \n                net = slim.conv2d(net, 1536, 1, scope=\'Conv2d_7b_1x1\')\n                end_points[\'Conv2d_7b_1x1\'] = net\n        \n                with tf.variable_scope(\'Logits\'):\n                    end_points[\'PrePool\'] = net\n                    #pylint: disable=no-member\n                    net = slim.avg_pool2d(net, net.get_shape()[1:3], padding=\'VALID\',\n                                          scope=\'AvgPool_1a_8x8\')\n                    net = slim.flatten(net)\n          \n                    net = slim.dropout(net, dropout_keep_prob, is_training=is_training,\n                                       scope=\'Dropout\')\n          \n                    end_points[\'PreLogitsFlatten\'] = net\n                \n                net = slim.fully_connected(net, bottleneck_layer_size, activation_fn=None, \n                        scope=\'Bottleneck\', reuse=False)\n  \n    return net, end_points\n'"
src/models/squeezenet.py,6,"b""from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\nimport tensorflow.contrib.slim as slim\n\ndef fire_module(inputs,\n                squeeze_depth,\n                expand_depth,\n                reuse=None,\n                scope=None,\n                outputs_collections=None):\n    with tf.variable_scope(scope, 'fire', [inputs], reuse=reuse):\n        with slim.arg_scope([slim.conv2d, slim.max_pool2d],\n                            outputs_collections=None):\n            net = squeeze(inputs, squeeze_depth)\n            outputs = expand(net, expand_depth)\n            return outputs\n\ndef squeeze(inputs, num_outputs):\n    return slim.conv2d(inputs, num_outputs, [1, 1], stride=1, scope='squeeze')\n\ndef expand(inputs, num_outputs):\n    with tf.variable_scope('expand'):\n        e1x1 = slim.conv2d(inputs, num_outputs, [1, 1], stride=1, scope='1x1')\n        e3x3 = slim.conv2d(inputs, num_outputs, [3, 3], scope='3x3')\n    return tf.concat([e1x1, e3x3], 3)\n\ndef inference(images, keep_probability, phase_train=True, bottleneck_layer_size=128, weight_decay=0.0, reuse=None):\n    batch_norm_params = {\n        # Decay for the moving averages.\n        'decay': 0.995,\n        # epsilon to prevent 0s in variance.\n        'epsilon': 0.001,\n        # force in-place updates of mean and variance estimates\n        'updates_collections': None,\n        # Moving averages ends up in the trainable variables collection\n        'variables_collections': [ tf.GraphKeys.TRAINABLE_VARIABLES ],\n    }\n    with slim.arg_scope([slim.conv2d, slim.fully_connected],\n                        weights_initializer=slim.xavier_initializer_conv2d(uniform=True),\n                        weights_regularizer=slim.l2_regularizer(weight_decay),\n                        normalizer_fn=slim.batch_norm,\n                        normalizer_params=batch_norm_params):\n        with tf.variable_scope('squeezenet', [images], reuse=reuse):\n            with slim.arg_scope([slim.batch_norm, slim.dropout],\n                                is_training=phase_train):\n                net = slim.conv2d(images, 96, [7, 7], stride=2, scope='conv1')\n                net = slim.max_pool2d(net, [3, 3], stride=2, scope='maxpool1')\n                net = fire_module(net, 16, 64, scope='fire2')\n                net = fire_module(net, 16, 64, scope='fire3')\n                net = fire_module(net, 32, 128, scope='fire4')\n                net = slim.max_pool2d(net, [2, 2], stride=2, scope='maxpool4')\n                net = fire_module(net, 32, 128, scope='fire5')\n                net = fire_module(net, 48, 192, scope='fire6')\n                net = fire_module(net, 48, 192, scope='fire7')\n                net = fire_module(net, 64, 256, scope='fire8')\n                net = slim.max_pool2d(net, [3, 3], stride=2, scope='maxpool8')\n                net = fire_module(net, 64, 256, scope='fire9')\n                net = slim.dropout(net, keep_probability)\n                net = slim.conv2d(net, 1000, [1, 1], activation_fn=None, normalizer_fn=None, scope='conv10')\n                net = slim.avg_pool2d(net, net.get_shape()[1:3], scope='avgpool10')\n                net = tf.squeeze(net, [1, 2], name='logits')\n                net = slim.fully_connected(net, bottleneck_layer_size, activation_fn=None, \n                        scope='Bottleneck', reuse=False)\n    return net, None\n"""
src/generative/models/__init__.py,0,b''
src/generative/models/dfc_vae.py,12,"b'# MIT License\n# \n# Copyright (c) 2017 David Sandberg\n# \n# Permission is hereby granted, free of charge, to any person obtaining a copy\n# of this software and associated documentation files (the ""Software""), to deal\n# in the Software without restriction, including without limitation the rights\n# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n# copies of the Software, and to permit persons to whom the Software is\n# furnished to do so, subject to the following conditions:\n# \n# The above copyright notice and this permission notice shall be included in all\n# copies or substantial portions of the Software.\n# \n# THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n# SOFTWARE.\n\n""""""Variational autoencoder based on the paper \n\'Deep Feature Consistent Variational Autoencoder\'\n(https://arxiv.org/pdf/1610.00291.pdf)\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\nimport tensorflow.contrib.slim as slim\nimport generative.models.vae_base  # @UnresolvedImport\n\n\nclass Vae(generative.models.vae_base.Vae):\n  \n    def __init__(self, latent_variable_dim):\n        super(Vae, self).__init__(latent_variable_dim, 64)\n  \n    def encoder(self, images, is_training):\n        activation_fn = leaky_relu  # tf.nn.relu\n        weight_decay = 0.0\n        with tf.variable_scope(\'encoder\'):\n            with slim.arg_scope([slim.batch_norm],\n                                is_training=is_training):\n                with slim.arg_scope([slim.conv2d, slim.fully_connected],\n                                    weights_initializer=tf.truncated_normal_initializer(stddev=0.1),\n                                    weights_regularizer=slim.l2_regularizer(weight_decay),\n                                    normalizer_fn=slim.batch_norm,\n                                    normalizer_params=self.batch_norm_params):\n                    net = slim.conv2d(images, 32, [4, 4], 2, activation_fn=activation_fn, scope=\'Conv2d_1\')\n                    net = slim.conv2d(net, 64, [4, 4], 2, activation_fn=activation_fn, scope=\'Conv2d_2\')\n                    net = slim.conv2d(net, 128, [4, 4], 2, activation_fn=activation_fn, scope=\'Conv2d_3\')\n                    net = slim.conv2d(net, 256, [4, 4], 2, activation_fn=activation_fn, scope=\'Conv2d_4\')\n                    net = slim.flatten(net)\n                    fc1 = slim.fully_connected(net, self.latent_variable_dim, activation_fn=None, normalizer_fn=None, scope=\'Fc_1\')\n                    fc2 = slim.fully_connected(net, self.latent_variable_dim, activation_fn=None, normalizer_fn=None, scope=\'Fc_2\')\n        return fc1, fc2\n      \n    def decoder(self, latent_var, is_training):\n        activation_fn = leaky_relu  # tf.nn.relu\n        weight_decay = 0.0 \n        with tf.variable_scope(\'decoder\'):\n            with slim.arg_scope([slim.batch_norm],\n                                is_training=is_training):\n                with slim.arg_scope([slim.conv2d, slim.fully_connected],\n                                    weights_initializer=tf.truncated_normal_initializer(stddev=0.1),\n                                    weights_regularizer=slim.l2_regularizer(weight_decay),\n                                    normalizer_fn=slim.batch_norm,\n                                    normalizer_params=self.batch_norm_params):\n                    net = slim.fully_connected(latent_var, 4096, activation_fn=None, normalizer_fn=None, scope=\'Fc_1\')\n                    net = tf.reshape(net, [-1,4,4,256], name=\'Reshape\')\n                    \n                    net = tf.image.resize_nearest_neighbor(net, size=(8,8), name=\'Upsample_1\')\n                    net = slim.conv2d(net, 128, [3, 3], 1, activation_fn=activation_fn, scope=\'Conv2d_1\')\n            \n                    net = tf.image.resize_nearest_neighbor(net, size=(16,16), name=\'Upsample_2\')\n                    net = slim.conv2d(net, 64, [3, 3], 1, activation_fn=activation_fn, scope=\'Conv2d_2\')\n            \n                    net = tf.image.resize_nearest_neighbor(net, size=(32,32), name=\'Upsample_3\')\n                    net = slim.conv2d(net, 32, [3, 3], 1, activation_fn=activation_fn, scope=\'Conv2d_3\')\n            \n                    net = tf.image.resize_nearest_neighbor(net, size=(64,64), name=\'Upsample_4\')\n                    net = slim.conv2d(net, 3, [3, 3], 1, activation_fn=None, scope=\'Conv2d_4\')\n                \n        return net\n      \ndef leaky_relu(x):\n    return tf.maximum(0.1*x,x)\n  '"
src/generative/models/dfc_vae_large.py,13,"b'# MIT License\n# \n# Copyright (c) 2017 David Sandberg\n# \n# Permission is hereby granted, free of charge, to any person obtaining a copy\n# of this software and associated documentation files (the ""Software""), to deal\n# in the Software without restriction, including without limitation the rights\n# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n# copies of the Software, and to permit persons to whom the Software is\n# furnished to do so, subject to the following conditions:\n# \n# The above copyright notice and this permission notice shall be included in all\n# copies or substantial portions of the Software.\n# \n# THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n# SOFTWARE.\n\n""""""Variational autoencoder based on the paper \n\'Deep Feature Consistent Variational Autoencoder\'\n(https://arxiv.org/pdf/1610.00291.pdf) but with a larger image size (128x128 pixels)\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\nimport tensorflow.contrib.slim as slim\nimport generative.models.vae_base  # @UnresolvedImport\n\n\nclass Vae(generative.models.vae_base.Vae):\n  \n    def __init__(self, latent_variable_dim):\n        super(Vae, self).__init__(latent_variable_dim, 128)\n        \n      \n    def encoder(self, images, is_training):\n        activation_fn = leaky_relu  # tf.nn.relu\n        weight_decay = 0.0\n        with tf.variable_scope(\'encoder\'):\n            with slim.arg_scope([slim.batch_norm],\n                                is_training=is_training):\n                with slim.arg_scope([slim.conv2d, slim.fully_connected],\n                                    weights_initializer=tf.truncated_normal_initializer(stddev=0.1),\n                                    weights_regularizer=slim.l2_regularizer(weight_decay),\n                                    normalizer_fn=slim.batch_norm,\n                                    normalizer_params=self.batch_norm_params):\n                    net = slim.conv2d(images, 32, [4, 4], 2, activation_fn=activation_fn, scope=\'Conv2d_1\')\n                    net = slim.conv2d(net, 64, [4, 4], 2, activation_fn=activation_fn, scope=\'Conv2d_2\')\n                    net = slim.conv2d(net, 128, [4, 4], 2, activation_fn=activation_fn, scope=\'Conv2d_3\')\n                    net = slim.conv2d(net, 256, [4, 4], 2, activation_fn=activation_fn, scope=\'Conv2d_4\')\n                    net = slim.conv2d(net, 512, [4, 4], 2, activation_fn=activation_fn, scope=\'Conv2d_5\')\n                    net = slim.flatten(net)\n                    fc1 = slim.fully_connected(net, self.latent_variable_dim, activation_fn=None, normalizer_fn=None, scope=\'Fc_1\')\n                    fc2 = slim.fully_connected(net, self.latent_variable_dim, activation_fn=None, normalizer_fn=None, scope=\'Fc_2\')\n        return fc1, fc2\n      \n    def decoder(self, latent_var, is_training):\n        activation_fn = leaky_relu  # tf.nn.relu\n        weight_decay = 0.0 \n        with tf.variable_scope(\'decoder\'):\n            with slim.arg_scope([slim.batch_norm],\n                                is_training=is_training):\n                with slim.arg_scope([slim.conv2d, slim.fully_connected],\n                                    weights_initializer=tf.truncated_normal_initializer(stddev=0.1),\n                                    weights_regularizer=slim.l2_regularizer(weight_decay),\n                                    normalizer_fn=slim.batch_norm,\n                                    normalizer_params=self.batch_norm_params):\n                    net = slim.fully_connected(latent_var, 4096, activation_fn=None, normalizer_fn=None, scope=\'Fc_1\')\n                    net = tf.reshape(net, [-1,4,4,256], name=\'Reshape\')\n                    \n                    net = tf.image.resize_nearest_neighbor(net, size=(8,8), name=\'Upsample_1\')\n                    net = slim.conv2d(net, 128, [3, 3], 1, activation_fn=activation_fn, scope=\'Conv2d_1\')\n            \n                    net = tf.image.resize_nearest_neighbor(net, size=(16,16), name=\'Upsample_2\')\n                    net = slim.conv2d(net, 64, [3, 3], 1, activation_fn=activation_fn, scope=\'Conv2d_2\')\n            \n                    net = tf.image.resize_nearest_neighbor(net, size=(32,32), name=\'Upsample_3\')\n                    net = slim.conv2d(net, 32, [3, 3], 1, activation_fn=activation_fn, scope=\'Conv2d_3\')\n            \n                    net = tf.image.resize_nearest_neighbor(net, size=(64,64), name=\'Upsample_4\')\n                    net = slim.conv2d(net, 3, [3, 3], 1, activation_fn=activation_fn, scope=\'Conv2d_4\')\n                \n                    net = tf.image.resize_nearest_neighbor(net, size=(128,128), name=\'Upsample_5\')\n                    net = slim.conv2d(net, 3, [3, 3], 1, activation_fn=None, scope=\'Conv2d_5\')\n        return net\n\ndef leaky_relu(x):\n    return tf.maximum(0.1*x,x)  \n'"
src/generative/models/dfc_vae_resnet.py,12,"b'# MIT License\n# \n# Copyright (c) 2017 David Sandberg\n# \n# Permission is hereby granted, free of charge, to any person obtaining a copy\n# of this software and associated documentation files (the ""Software""), to deal\n# in the Software without restriction, including without limitation the rights\n# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n# copies of the Software, and to permit persons to whom the Software is\n# furnished to do so, subject to the following conditions:\n# \n# The above copyright notice and this permission notice shall be included in all\n# copies or substantial portions of the Software.\n# \n# THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n# SOFTWARE.\n\n""""""Variational autoencoder based on the paper \n\'Deep Feature Consistent Variational Autoencoder\'\n(https://arxiv.org/pdf/1610.00291.pdf)\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\nimport tensorflow.contrib.slim as slim\nimport generative.models.vae_base  # @UnresolvedImport\n\n\nclass Vae(generative.models.vae_base.Vae):\n  \n    def __init__(self, latent_variable_dim):\n        super(Vae, self).__init__(latent_variable_dim, 64)\n  \n    def encoder(self, images, is_training):\n        activation_fn = leaky_relu  # tf.nn.relu\n        weight_decay = 0.0\n        with tf.variable_scope(\'encoder\'):\n            with slim.arg_scope([slim.batch_norm],\n                                is_training=is_training):\n                with slim.arg_scope([slim.conv2d, slim.fully_connected],\n                                    weights_initializer=tf.truncated_normal_initializer(stddev=0.1),\n                                    weights_regularizer=slim.l2_regularizer(weight_decay),\n                                    normalizer_fn=slim.batch_norm,\n                                    normalizer_params=self.batch_norm_params):\n                    net = images\n                    \n                    net = slim.conv2d(net, 32, [4, 4], 2, activation_fn=activation_fn, scope=\'Conv2d_1a\')\n                    net = slim.repeat(net, 3, conv2d_block, 0.1, 32, [4, 4], 1, activation_fn=activation_fn, scope=\'Conv2d_1b\')\n                    \n                    net = slim.conv2d(net, 64, [4, 4], 2, activation_fn=activation_fn, scope=\'Conv2d_2a\')\n                    net = slim.repeat(net, 3, conv2d_block, 0.1, 64, [4, 4], 1, activation_fn=activation_fn, scope=\'Conv2d_2b\')\n\n                    net = slim.conv2d(net, 128, [4, 4], 2, activation_fn=activation_fn, scope=\'Conv2d_3a\')\n                    net = slim.repeat(net, 3, conv2d_block, 0.1, 128, [4, 4], 1, activation_fn=activation_fn, scope=\'Conv2d_3b\')\n\n                    net = slim.conv2d(net, 256, [4, 4], 2, activation_fn=activation_fn, scope=\'Conv2d_4a\')\n                    net = slim.repeat(net, 3, conv2d_block, 0.1, 256, [4, 4], 1, activation_fn=activation_fn, scope=\'Conv2d_4b\')\n                    \n                    net = slim.flatten(net)\n                    fc1 = slim.fully_connected(net, self.latent_variable_dim, activation_fn=None, normalizer_fn=None, scope=\'Fc_1\')\n                    fc2 = slim.fully_connected(net, self.latent_variable_dim, activation_fn=None, normalizer_fn=None, scope=\'Fc_2\')\n        return fc1, fc2\n      \n    def decoder(self, latent_var, is_training):\n        activation_fn = leaky_relu  # tf.nn.relu\n        weight_decay = 0.0 \n        with tf.variable_scope(\'decoder\'):\n            with slim.arg_scope([slim.batch_norm],\n                                is_training=is_training):\n                with slim.arg_scope([slim.conv2d, slim.fully_connected],\n                                    weights_initializer=tf.truncated_normal_initializer(stddev=0.1),\n                                    weights_regularizer=slim.l2_regularizer(weight_decay),\n                                    normalizer_fn=slim.batch_norm,\n                                    normalizer_params=self.batch_norm_params):\n                    net = slim.fully_connected(latent_var, 4096, activation_fn=None, normalizer_fn=None, scope=\'Fc_1\')\n                    net = tf.reshape(net, [-1,4,4,256], name=\'Reshape\')\n                    \n                    net = tf.image.resize_nearest_neighbor(net, size=(8,8), name=\'Upsample_1\')\n                    net = slim.conv2d(net, 128, [3, 3], 1, activation_fn=activation_fn, scope=\'Conv2d_1a\')\n                    net = slim.repeat(net, 3, conv2d_block, 0.1, 128, [3, 3], 1, activation_fn=activation_fn, scope=\'Conv2d_1b\')\n            \n                    net = tf.image.resize_nearest_neighbor(net, size=(16,16), name=\'Upsample_2\')\n                    net = slim.conv2d(net, 64, [3, 3], 1, activation_fn=activation_fn, scope=\'Conv2d_2a\')\n                    net = slim.repeat(net, 3, conv2d_block, 0.1, 64, [3, 3], 1, activation_fn=activation_fn, scope=\'Conv2d_2b\')\n            \n                    net = tf.image.resize_nearest_neighbor(net, size=(32,32), name=\'Upsample_3\')\n                    net = slim.conv2d(net, 32, [3, 3], 1, activation_fn=activation_fn, scope=\'Conv2d_3a\')\n                    net = slim.repeat(net, 3, conv2d_block, 0.1, 32, [3, 3], 1, activation_fn=activation_fn, scope=\'Conv2d_3b\')\n            \n                    net = tf.image.resize_nearest_neighbor(net, size=(64,64), name=\'Upsample_4\')\n                    net = slim.conv2d(net, 3, [3, 3], 1, activation_fn=activation_fn, scope=\'Conv2d_4a\')\n                    net = slim.repeat(net, 3, conv2d_block, 0.1, 3, [3, 3], 1, activation_fn=activation_fn, scope=\'Conv2d_4b\')\n                    net = slim.conv2d(net, 3, [3, 3], 1, activation_fn=None, scope=\'Conv2d_4c\')\n                \n        return net\n      \ndef conv2d_block(inp, scale, *args, **kwargs):\n    return inp + slim.conv2d(inp, *args, **kwargs) * scale\n\ndef leaky_relu(x):\n    return tf.maximum(0.1*x,x)\n  '"
src/generative/models/vae_base.py,1,"b'# MIT License\n# \n# Copyright (c) 2017 David Sandberg\n# \n# Permission is hereby granted, free of charge, to any person obtaining a copy\n# of this software and associated documentation files (the ""Software""), to deal\n# in the Software without restriction, including without limitation the rights\n# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n# copies of the Software, and to permit persons to whom the Software is\n# furnished to do so, subject to the following conditions:\n# \n# The above copyright notice and this permission notice shall be included in all\n# copies or substantial portions of the Software.\n# \n# THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n# SOFTWARE.\n\n""""""Base class for variational autoencoders containing an encoder and a decoder\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\n\nclass Vae(object):\n  \n    def __init__(self, latent_variable_dim, image_size):\n        self.latent_variable_dim = latent_variable_dim\n        self.image_size = image_size\n        self.batch_norm_params = {\n        # Decay for the moving averages.\n        \'decay\': 0.995,\n        # epsilon to prevent 0s in variance.\n        \'epsilon\': 0.001,\n        # force in-place updates of mean and variance estimates\n        \'updates_collections\': None,\n        # Moving averages ends up in the trainable variables collection\n        \'variables_collections\': [ tf.GraphKeys.TRAINABLE_VARIABLES ],\n    }\n  \n    def encoder(self, images, is_training):\n        # Must be overridden in implementation classes\n        raise NotImplementedError\n      \n    def decoder(self, latent_var, is_training):\n        # Must be overridden in implementation classes\n        raise NotImplementedError\n\n    def get_image_size(self):\n        return self.image_size\n        '"
