file_path,api_count,code
driver/code/deep_pi_car.py,0,"b'import logging\nimport picar\nimport cv2\nimport datetime\nfrom hand_coded_lane_follower import HandCodedLaneFollower\nfrom objects_on_road_processor import ObjectsOnRoadProcessor\n\n_SHOW_IMAGE = True\n\n\nclass DeepPiCar(object):\n\n    __INITIAL_SPEED = 0\n    __SCREEN_WIDTH = 320\n    __SCREEN_HEIGHT = 240\n\n    def __init__(self):\n        """""" Init camera and wheels""""""\n        logging.info(\'Creating a DeepPiCar...\')\n\n        picar.setup()\n\n        logging.debug(\'Set up camera\')\n        self.camera = cv2.VideoCapture(-1)\n        self.camera.set(3, self.__SCREEN_WIDTH)\n        self.camera.set(4, self.__SCREEN_HEIGHT)\n\n        self.pan_servo = picar.Servo.Servo(1)\n        self.pan_servo.offset = -30  # calibrate servo to center\n        self.pan_servo.write(90)\n\n        self.tilt_servo = picar.Servo.Servo(2)\n        self.tilt_servo.offset = 20  # calibrate servo to center\n        self.tilt_servo.write(90)\n\n        logging.debug(\'Set up back wheels\')\n        self.back_wheels = picar.back_wheels.Back_Wheels()\n        self.back_wheels.speed = 0  # Speed Range is 0 (stop) - 100 (fastest)\n\n        logging.debug(\'Set up front wheels\')\n        self.front_wheels = picar.front_wheels.Front_Wheels()\n        self.front_wheels.turning_offset = -25  # calibrate servo to center\n        self.front_wheels.turn(90)  # Steering Range is 45 (left) - 90 (center) - 135 (right)\n\n        self.lane_follower = HandCodedLaneFollower(self)\n        self.traffic_sign_processor = ObjectsOnRoadProcessor(self)\n        # lane_follower = DeepLearningLaneFollower()\n\n        self.fourcc = cv2.VideoWriter_fourcc(*\'XVID\')\n        datestr = datetime.datetime.now().strftime(""%y%m%d_%H%M%S"")\n        self.video_orig = self.create_video_recorder(\'../data/tmp/car_video%s.avi\' % datestr)\n        self.video_lane = self.create_video_recorder(\'../data/tmp/car_video_lane%s.avi\' % datestr)\n        self.video_objs = self.create_video_recorder(\'../data/tmp/car_video_objs%s.avi\' % datestr)\n\n        logging.info(\'Created a DeepPiCar\')\n\n    def create_video_recorder(self, path):\n        return cv2.VideoWriter(path, self.fourcc, 20.0, (self.__SCREEN_WIDTH, self.__SCREEN_HEIGHT))\n\n    def __enter__(self):\n        """""" Entering a with statement """"""\n        return self\n\n    def __exit__(self, _type, value, traceback):\n        """""" Exit a with statement""""""\n        if traceback is not None:\n            # Exception occurred:\n            logging.error(\'Exiting with statement with exception %s\' % traceback)\n\n        self.cleanup()\n\n    def cleanup(self):\n        """""" Reset the hardware""""""\n        logging.info(\'Stopping the car, resetting hardware.\')\n        self.back_wheels.speed = 0\n        self.front_wheels.turn(90)\n        self.camera.release()\n        self.video_orig.release()\n        self.video_lane.release()\n        self.video_objs.release()\n        cv2.destroyAllWindows()\n\n    def drive(self, speed=__INITIAL_SPEED):\n        """""" Main entry point of the car, and put it in drive mode\n\n        Keyword arguments:\n        speed -- speed of back wheel, range is 0 (stop) - 100 (fastest)\n        """"""\n\n        logging.info(\'Starting to drive at speed %s...\' % speed)\n        self.back_wheels.speed = speed\n        i = 0\n        while self.camera.isOpened():\n            _, image_lane = self.camera.read()\n            image_objs = image_lane.copy()\n            i += 1\n            self.video_orig.write(image_lane)\n\n            image_objs = self.process_objects_on_road(image_objs)\n            self.video_objs.write(image_objs)\n            show_image(\'Detected Objects\', image_objs)\n\n            image_lane = self.follow_lane(image_lane)\n            self.video_lane.write(image_lane)\n            show_image(\'Lane Lines\', image_lane)\n\n            if cv2.waitKey(1) & 0xFF == ord(\'q\'):\n                self.cleanup()\n                break\n\n    def process_objects_on_road(self, image):\n        image = self.traffic_sign_processor.process_objects_on_road(image)\n        return image\n\n    def follow_lane(self, image):\n        image = self.lane_follower.follow_lane(image)\n        return image\n\n\n############################\n# Utility Functions\n############################\ndef show_image(title, frame, show=_SHOW_IMAGE):\n    if show:\n        cv2.imshow(title, frame)\n\n\ndef main():\n    with DeepPiCar() as car:\n        car.drive(40)\n\n\nif __name__ == \'__main__\':\n    logging.basicConfig(level=logging.DEBUG, format=\'%(levelname)-5s:%(asctime)s: %(message)s\')\n    \n    main()\n'"
driver/code/driver_main.py,0,"b""from deep_pi_car import DeepPiCar\nimport logging\nimport sys\n\ndef main():\n    # print system info\n    logging.info('Starting DeepPiCar, system info: ' + sys.version)\n    \n    with DeepPiCar() as car:\n        car.drive(40)\n    \nif __name__ == '__main__':\n    logging.basicConfig(level=logging.DEBUG)\n    main()\n"""
driver/code/end_to_end_lane_follower.py,0,"b'import cv2\nimport numpy as np\nimport logging\nimport math\nfrom keras.models import load_model\nfrom hand_coded_lane_follower import HandCodedLaneFollower\n\n_SHOW_IMAGE = False\n\n\nclass EndToEndLaneFollower(object):\n\n    def __init__(self,\n                 car=None,\n                 model_path=\'/home/pi/DeepPiCar/models/lane_navigation/data/model_result/lane_navigation.h5\'):\n        logging.info(\'Creating a EndToEndLaneFollower...\')\n\n        self.car = car\n        self.curr_steering_angle = 90\n        self.model = load_model(model_path)\n\n    def follow_lane(self, frame):\n        # Main entry point of the lane follower\n        show_image(""orig"", frame)\n\n        self.curr_steering_angle = self.compute_steering_angle(frame)\n        logging.debug(""curr_steering_angle = %d"" % self.curr_steering_angle)\n\n        if self.car is not None:\n            self.car.front_wheels.turn(self.curr_steering_angle)\n        final_frame = display_heading_line(frame, self.curr_steering_angle)\n\n        return final_frame\n\n    def compute_steering_angle(self, frame):\n        """""" Find the steering angle directly based on video frame\n            We assume that camera is calibrated to point to dead center\n        """"""\n        preprocessed = img_preprocess(frame)\n        X = np.asarray([preprocessed])\n        steering_angle = self.model.predict(X)[0]\n\n        logging.debug(\'new steering angle: %s\' % steering_angle)\n        return int(steering_angle + 0.5) # round the nearest integer\n\n\ndef img_preprocess(image):\n    height, _, _ = image.shape\n    image = image[int(height/2):,:,:]  # remove top half of the image, as it is not relevant for lane following\n    image = cv2.cvtColor(image, cv2.COLOR_BGR2YUV)  # Nvidia model said it is best to use YUV color space\n    image = cv2.GaussianBlur(image, (3,3), 0)\n    image = cv2.resize(image, (200,66)) # input image size (200,66) Nvidia model\n    image = image / 255 # normalizing, the processed image becomes black for some reason.  do we need this?\n    return image\n\ndef display_heading_line(frame, steering_angle, line_color=(0, 0, 255), line_width=5, ):\n    heading_image = np.zeros_like(frame)\n    height, width, _ = frame.shape\n\n    # figure out the heading line from steering angle\n    # heading line (x1,y1) is always center bottom of the screen\n    # (x2, y2) requires a bit of trigonometry\n\n    # Note: the steering angle of:\n    # 0-89 degree: turn left\n    # 90 degree: going straight\n    # 91-180 degree: turn right \n    steering_angle_radian = steering_angle / 180.0 * math.pi\n    x1 = int(width / 2)\n    y1 = height\n    x2 = int(x1 - height / 2 / math.tan(steering_angle_radian))\n    y2 = int(height / 2)\n\n    cv2.line(heading_image, (x1, y1), (x2, y2), line_color, line_width)\n    heading_image = cv2.addWeighted(frame, 0.8, heading_image, 1, 1)\n\n    return heading_image\n\n\ndef show_image(title, frame, show=_SHOW_IMAGE):\n    if show:\n        cv2.imshow(title, frame)\n\n\n############################\n# Test Functions\n############################\ndef test_photo(file):\n    lane_follower = EndToEndLaneFollower()\n    frame = cv2.imread(file)\n    combo_image = lane_follower.follow_lane(frame)\n    show_image(\'final\', combo_image, True)\n    logging.info(""filename=%s, model=%3d"" % (file, lane_follower.curr_steering_angle))\n    cv2.waitKey(0)\n    cv2.destroyAllWindows()\n\n\ndef test_video(video_file):\n    end_to_end_lane_follower = EndToEndLaneFollower()\n    hand_coded_lane_follower = HandCodedLaneFollower()\n    cap = cv2.VideoCapture(video_file + \'.avi\')\n\n    # skip first second of video.\n    for i in range(3):\n        _, frame = cap.read()\n\n    video_type = cv2.VideoWriter_fourcc(*\'XVID\')\n    video_overlay = cv2.VideoWriter(""%s_end_to_end.avi"" % video_file, video_type, 20.0, (320, 240))\n    try:\n        i = 0\n        while cap.isOpened():\n            _, frame = cap.read()\n            frame_copy = frame.copy()\n            logging.info(\'Frame %s\' % i)\n            combo_image1 = hand_coded_lane_follower.follow_lane(frame)\n            combo_image2 = end_to_end_lane_follower.follow_lane(frame_copy)\n\n            diff = end_to_end_lane_follower.curr_steering_angle - hand_coded_lane_follower.curr_steering_angle;\n            logging.info(""desired=%3d, model=%3d, diff=%3d"" %\n                          (hand_coded_lane_follower.curr_steering_angle,\n                          end_to_end_lane_follower.curr_steering_angle,\n                          diff))\n            video_overlay.write(combo_image2)\n            cv2.imshow(""Hand Coded"", combo_image1)\n            cv2.imshow(""Deep Learning"", combo_image2)\n\n            i += 1\n            if cv2.waitKey(1) & 0xFF == ord(\'q\'):\n                break\n    finally:\n        cap.release()\n        video_overlay.release()\n        cv2.destroyAllWindows()\n\n\nif __name__ == \'__main__\':\n    logging.basicConfig(level=logging.INFO)\n\n    test_video(\'/home/pi/DeepPiCar/models/lane_navigation/data/images/video01\')\n    #test_photo(\'/home/pi/DeepPiCar/models/lane_navigation/data/images/video01_100_084.png\')\n    # test_photo(sys.argv[1])\n    # test_video(sys.argv[1])\n'"
driver/code/hand_coded_lane_follower.py,0,"b'import cv2\nimport numpy as np\nimport logging\nimport math\nimport datetime\nimport sys\n\n_SHOW_IMAGE = False\n\n\nclass HandCodedLaneFollower(object):\n\n    def __init__(self, car=None):\n        logging.info(\'Creating a HandCodedLaneFollower...\')\n        self.car = car\n        self.curr_steering_angle = 90\n\n    def follow_lane(self, frame):\n        # Main entry point of the lane follower\n        show_image(""orig"", frame)\n\n        lane_lines, frame = detect_lane(frame)\n        final_frame = self.steer(frame, lane_lines)\n\n        return final_frame\n\n    def steer(self, frame, lane_lines):\n        logging.debug(\'steering...\')\n        if len(lane_lines) == 0:\n            logging.error(\'No lane lines detected, nothing to do.\')\n            return frame\n\n        new_steering_angle = compute_steering_angle(frame, lane_lines)\n        self.curr_steering_angle = stabilize_steering_angle(self.curr_steering_angle, new_steering_angle, len(lane_lines))\n\n        if self.car is not None:\n            self.car.front_wheels.turn(self.curr_steering_angle)\n        curr_heading_image = display_heading_line(frame, self.curr_steering_angle)\n        show_image(""heading"", curr_heading_image)\n\n        return curr_heading_image\n\n\n############################\n# Frame processing steps\n############################\ndef detect_lane(frame):\n    logging.debug(\'detecting lane lines...\')\n\n    edges = detect_edges(frame)\n    show_image(\'edges\', edges)\n\n    cropped_edges = region_of_interest(edges)\n    show_image(\'edges cropped\', cropped_edges)\n\n    line_segments = detect_line_segments(cropped_edges)\n    line_segment_image = display_lines(frame, line_segments)\n    show_image(""line segments"", line_segment_image)\n\n    lane_lines = average_slope_intercept(frame, line_segments)\n    lane_lines_image = display_lines(frame, lane_lines)\n    show_image(""lane lines"", lane_lines_image)\n\n    return lane_lines, lane_lines_image\n\n\ndef detect_edges(frame):\n    # filter for blue lane lines\n    hsv = cv2.cvtColor(frame, cv2.COLOR_BGR2HSV)\n    show_image(""hsv"", hsv)\n    lower_blue = np.array([30, 40, 0])\n    upper_blue = np.array([150, 255, 255])\n    mask = cv2.inRange(hsv, lower_blue, upper_blue)\n    show_image(""blue mask"", mask)\n\n    # detect edges\n    edges = cv2.Canny(mask, 200, 400)\n\n    return edges\n\ndef detect_edges_old(frame):\n    # filter for blue lane lines\n    hsv = cv2.cvtColor(frame, cv2.COLOR_BGR2HSV)\n    show_image(""hsv"", hsv)\n    for i in range(16):\n        lower_blue = np.array([30, 16 * i, 0])\n        upper_blue = np.array([150, 255, 255])\n        mask = cv2.inRange(hsv, lower_blue, upper_blue)\n        show_image(""blue mask Sat=%s"" % (16* i), mask)\n\n\n    #for i in range(16):\n        #lower_blue = np.array([16 * i, 40, 50])\n        #upper_blue = np.array([150, 255, 255])\n        #mask = cv2.inRange(hsv, lower_blue, upper_blue)\n       # show_image(""blue mask hue=%s"" % (16* i), mask)\n\n        # detect edges\n    edges = cv2.Canny(mask, 200, 400)\n\n    return edges\n\n\ndef region_of_interest(canny):\n    height, width = canny.shape\n    mask = np.zeros_like(canny)\n\n    # only focus bottom half of the screen\n\n    polygon = np.array([[\n        (0, height * 1 / 2),\n        (width, height * 1 / 2),\n        (width, height),\n        (0, height),\n    ]], np.int32)\n\n    cv2.fillPoly(mask, polygon, 255)\n    show_image(""mask"", mask)\n    masked_image = cv2.bitwise_and(canny, mask)\n    return masked_image\n\n\ndef detect_line_segments(cropped_edges):\n    # tuning min_threshold, minLineLength, maxLineGap is a trial and error process by hand\n    rho = 1  # precision in pixel, i.e. 1 pixel\n    angle = np.pi / 180  # degree in radian, i.e. 1 degree\n    min_threshold = 10  # minimal of votes\n    line_segments = cv2.HoughLinesP(cropped_edges, rho, angle, min_threshold, np.array([]), minLineLength=8,\n                                    maxLineGap=4)\n\n    if line_segments is not None:\n        for line_segment in line_segments:\n            logging.debug(\'detected line_segment:\')\n            logging.debug(""%s of length %s"" % (line_segment, length_of_line_segment(line_segment[0])))\n\n    return line_segments\n\n\ndef average_slope_intercept(frame, line_segments):\n    """"""\n    This function combines line segments into one or two lane lines\n    If all line slopes are < 0: then we only have detected left lane\n    If all line slopes are > 0: then we only have detected right lane\n    """"""\n    lane_lines = []\n    if line_segments is None:\n        logging.info(\'No line_segment segments detected\')\n        return lane_lines\n\n    height, width, _ = frame.shape\n    left_fit = []\n    right_fit = []\n\n    boundary = 1/3\n    left_region_boundary = width * (1 - boundary)  # left lane line segment should be on left 2/3 of the screen\n    right_region_boundary = width * boundary # right lane line segment should be on left 2/3 of the screen\n\n    for line_segment in line_segments:\n        for x1, y1, x2, y2 in line_segment:\n            if x1 == x2:\n                logging.info(\'skipping vertical line segment (slope=inf): %s\' % line_segment)\n                continue\n            fit = np.polyfit((x1, x2), (y1, y2), 1)\n            slope = fit[0]\n            intercept = fit[1]\n            if slope < 0:\n                if x1 < left_region_boundary and x2 < left_region_boundary:\n                    left_fit.append((slope, intercept))\n            else:\n                if x1 > right_region_boundary and x2 > right_region_boundary:\n                    right_fit.append((slope, intercept))\n\n    left_fit_average = np.average(left_fit, axis=0)\n    if len(left_fit) > 0:\n        lane_lines.append(make_points(frame, left_fit_average))\n\n    right_fit_average = np.average(right_fit, axis=0)\n    if len(right_fit) > 0:\n        lane_lines.append(make_points(frame, right_fit_average))\n\n    logging.debug(\'lane lines: %s\' % lane_lines)  # [[[316, 720, 484, 432]], [[1009, 720, 718, 432]]]\n\n    return lane_lines\n\n\ndef compute_steering_angle(frame, lane_lines):\n    """""" Find the steering angle based on lane line coordinate\n        We assume that camera is calibrated to point to dead center\n    """"""\n    if len(lane_lines) == 0:\n        logging.info(\'No lane lines detected, do nothing\')\n        return -90\n\n    height, width, _ = frame.shape\n    if len(lane_lines) == 1:\n        logging.debug(\'Only detected one lane line, just follow it. %s\' % lane_lines[0])\n        x1, _, x2, _ = lane_lines[0][0]\n        x_offset = x2 - x1\n    else:\n        _, _, left_x2, _ = lane_lines[0][0]\n        _, _, right_x2, _ = lane_lines[1][0]\n        camera_mid_offset_percent = 0.02 # 0.0 means car pointing to center, -0.03: car is centered to left, +0.03 means car pointing to right\n        mid = int(width / 2 * (1 + camera_mid_offset_percent))\n        x_offset = (left_x2 + right_x2) / 2 - mid\n\n    # find the steering angle, which is angle between navigation direction to end of center line\n    y_offset = int(height / 2)\n\n    angle_to_mid_radian = math.atan(x_offset / y_offset)  # angle (in radian) to center vertical line\n    angle_to_mid_deg = int(angle_to_mid_radian * 180.0 / math.pi)  # angle (in degrees) to center vertical line\n    steering_angle = angle_to_mid_deg + 90  # this is the steering angle needed by picar front wheel\n\n    logging.debug(\'new steering angle: %s\' % steering_angle)\n    return steering_angle\n\n\ndef stabilize_steering_angle(curr_steering_angle, new_steering_angle, num_of_lane_lines, max_angle_deviation_two_lines=5, max_angle_deviation_one_lane=1):\n    """"""\n    Using last steering angle to stabilize the steering angle\n    This can be improved to use last N angles, etc\n    if new angle is too different from current angle, only turn by max_angle_deviation degrees\n    """"""\n    if num_of_lane_lines == 2 :\n        # if both lane lines detected, then we can deviate more\n        max_angle_deviation = max_angle_deviation_two_lines\n    else :\n        # if only one lane detected, don\'t deviate too much\n        max_angle_deviation = max_angle_deviation_one_lane\n    \n    angle_deviation = new_steering_angle - curr_steering_angle\n    if abs(angle_deviation) > max_angle_deviation:\n        stabilized_steering_angle = int(curr_steering_angle\n                                        + max_angle_deviation * angle_deviation / abs(angle_deviation))\n    else:\n        stabilized_steering_angle = new_steering_angle\n    logging.info(\'Proposed angle: %s, stabilized angle: %s\' % (new_steering_angle, stabilized_steering_angle))\n    return stabilized_steering_angle\n\n\n############################\n# Utility Functions\n############################\ndef display_lines(frame, lines, line_color=(0, 255, 0), line_width=10):\n    line_image = np.zeros_like(frame)\n    if lines is not None:\n        for line in lines:\n            for x1, y1, x2, y2 in line:\n                cv2.line(line_image, (x1, y1), (x2, y2), line_color, line_width)\n    line_image = cv2.addWeighted(frame, 0.8, line_image, 1, 1)\n    return line_image\n\n\ndef display_heading_line(frame, steering_angle, line_color=(0, 0, 255), line_width=5, ):\n    heading_image = np.zeros_like(frame)\n    height, width, _ = frame.shape\n\n    # figure out the heading line from steering angle\n    # heading line (x1,y1) is always center bottom of the screen\n    # (x2, y2) requires a bit of trigonometry\n\n    # Note: the steering angle of:\n    # 0-89 degree: turn left\n    # 90 degree: going straight\n    # 91-180 degree: turn right \n    steering_angle_radian = steering_angle / 180.0 * math.pi\n    x1 = int(width / 2)\n    y1 = height\n    x2 = int(x1 - height / 2 / math.tan(steering_angle_radian))\n    y2 = int(height / 2)\n\n    cv2.line(heading_image, (x1, y1), (x2, y2), line_color, line_width)\n    heading_image = cv2.addWeighted(frame, 0.8, heading_image, 1, 1)\n\n    return heading_image\n\n\ndef length_of_line_segment(line):\n    x1, y1, x2, y2 = line\n    return math.sqrt((x2 - x1) ** 2 + (y2 - y1) ** 2)\n\n\ndef show_image(title, frame, show=_SHOW_IMAGE):\n    if show:\n        cv2.imshow(title, frame)\n\n\ndef make_points(frame, line):\n    height, width, _ = frame.shape\n    slope, intercept = line\n    y1 = height  # bottom of the frame\n    y2 = int(y1 * 1 / 2)  # make points from middle of the frame down\n\n    # bound the coordinates within the frame\n    x1 = max(-width, min(2 * width, int((y1 - intercept) / slope)))\n    x2 = max(-width, min(2 * width, int((y2 - intercept) / slope)))\n    return [[x1, y1, x2, y2]]\n\n\n############################\n# Test Functions\n############################\ndef test_photo(file):\n    land_follower = HandCodedLaneFollower()\n    frame = cv2.imread(file)\n    combo_image = land_follower.follow_lane(frame)\n    show_image(\'final\', combo_image, True)\n    cv2.waitKey(0)\n    cv2.destroyAllWindows()\n\n\ndef test_video(video_file):\n    lane_follower = HandCodedLaneFollower()\n    cap = cv2.VideoCapture(video_file + \'.avi\')\n\n    # skip first second of video.\n    for i in range(3):\n        _, frame = cap.read()\n\n    video_type = cv2.VideoWriter_fourcc(*\'XVID\')\n    video_overlay = cv2.VideoWriter(""%s_overlay.avi"" % (video_file), video_type, 20.0, (320, 240))\n    try:\n        i = 0\n        while cap.isOpened():\n            _, frame = cap.read()\n            print(\'frame %s\' % i )\n            combo_image= lane_follower.follow_lane(frame)\n            \n            cv2.imwrite(""%s_%03d_%03d.png"" % (video_file, i, lane_follower.curr_steering_angle), frame)\n            \n            cv2.imwrite(""%s_overlay_%03d.png"" % (video_file, i), combo_image)\n            video_overlay.write(combo_image)\n            cv2.imshow(""Road with Lane line"", combo_image)\n\n            i += 1\n            if cv2.waitKey(1) & 0xFF == ord(\'q\'):\n                break\n    finally:\n        cap.release()\n        video_overlay.release()\n        cv2.destroyAllWindows()\n\n\nif __name__ == \'__main__\':\n    logging.basicConfig(level=logging.INFO)\n\n    test_video(\'/home/pi/DeepPiCar/driver/data/tmp/video01\')\n    #test_photo(\'/home/pi/DeepPiCar/driver/data/video/car_video_190427_110320_073.png\')\n    #test_photo(sys.argv[1])\n    #test_video(sys.argv[1])\n'"
driver/code/objects_on_road_processor.py,0,"b'import cv2\nimport logging\nimport datetime\nimport time\nimport edgetpu.detection.engine\nfrom PIL import Image\nfrom traffic_objects import *\n\n_SHOW_IMAGE = False\n\n\nclass ObjectsOnRoadProcessor(object):\n    """"""\n    This class 1) detects what objects (namely traffic signs and people) are on the road\n    and 2) controls the car navigation (speed/steering) accordingly\n    """"""\n\n    def __init__(self,\n                 car=None,\n                 speed_limit=40,\n                 model=\'/home/pi/DeepPiCar/models/object_detection/data/model_result/road_signs_quantized_edgetpu.tflite\',\n                 label=\'/home/pi/DeepPiCar/models/object_detection/data/model_result/road_sign_labels.txt\',\n                 width=640,\n                 height=480):\n        # model: This MUST be a tflite model that was specifically compiled for Edge TPU.\n        # https://coral.withgoogle.com/web-compiler/\n        logging.info(\'Creating a ObjectsOnRoadProcessor...\')\n        self.width = width\n        self.height = height\n\n        # initialize car\n        self.car = car\n        self.speed_limit = speed_limit\n        self.speed = speed_limit\n\n        # initialize TensorFlow models\n        with open(label, \'r\') as f:\n            pairs = (l.strip().split(maxsplit=1) for l in f.readlines())\n            self.labels = dict((int(k), v) for k, v in pairs)\n\n        # initial edge TPU engine\n        logging.info(\'Initialize Edge TPU with model %s...\' % model)\n        self.engine = edgetpu.detection.engine.DetectionEngine(model)\n        self.min_confidence = 0.30\n        self.num_of_objects = 3\n        logging.info(\'Initialize Edge TPU with model done.\')\n\n        # initialize open cv for drawing boxes\n        self.font = cv2.FONT_HERSHEY_SIMPLEX\n        self.bottomLeftCornerOfText = (10, height - 10)\n        self.fontScale = 1\n        self.fontColor = (255, 255, 255)  # white\n        self.boxColor = (0, 0, 255)  # RED\n        self.boxLineWidth = 1\n        self.lineType = 2\n        self.annotate_text = """"\n        self.annotate_text_time = time.time()\n        self.time_to_show_prediction = 1.0  # ms\n\n        #\n        self.traffic_objects = {0: GreenTrafficLight(),\n                                1: Person(),\n                                2: RedTrafficLight(),\n                                3: SpeedLimit(25),\n                                4: SpeedLimit(40),\n                                5: StopSign()}\n\n    def process_objects_on_road(self, frame):\n        # Main entry point of the Road Object Handler\n        logging.debug(\'Processing objects.................................\')\n        objects, final_frame = self.detect_objects(frame)\n        self.control_car(objects)\n        logging.debug(\'Processing objects END..............................\')\n\n        return final_frame\n\n    def control_car(self, objects):\n        logging.debug(\'Control car...\')\n        car_state = {""speed"": self.speed_limit, ""speed_limit"": self.speed_limit}\n\n        if len(objects) == 0:\n            logging.debug(\'No objects detected, drive at speed limit of %s.\' % self.speed_limit)\n\n        contain_stop_sign = False\n        for obj in objects:\n            obj_label = self.labels[obj.label_id]\n            processor = self.traffic_objects[obj.label_id]\n            if processor.is_close_by(obj, self.height):\n                processor.set_car_state(car_state)\n            else:\n                logging.debug(""[%s] object detected, but it is too far, ignoring. "" % obj_label)\n            if obj_label == \'Stop\':\n                contain_stop_sign = True\n\n        if not contain_stop_sign:\n            self.traffic_objects[5].clear()\n\n        self.resume_driving(car_state)\n\n    def resume_driving(self, car_state):\n        old_speed = self.speed\n        self.speed_limit = car_state[\'speed_limit\']\n        self.speed = car_state[\'speed\']\n\n        if self.speed == 0:\n            self.set_speed(0)\n        else:\n            self.set_speed(self.speed_limit)\n        logging.debug(\'Current Speed = %d, New Speed = %d\' % (old_speed, self.speed))\n\n        if self.speed == 0:\n            logging.debug(\'full stop for 1 seconds\')\n            time.sleep(1)\n\n    def set_speed(self, speed):\n        # Use this setter, so we can test this class without a car attached\n        self.speed = speed\n        if self.car is not None:\n            logging.debug(""Actually setting car speed to %d"" % speed)\n            self.car.back_wheels.speed = speed\n\n\n\n    ############################\n    # Frame processing steps\n    ############################\n    def detect_objects(self, frame):\n        logging.debug(\'Detecting objects...\')\n\n        # call tpu for inference\n        start_ms = time.time()\n        frame_RGB = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n        img_pil = Image.fromarray(frame_RGB)\n        objects = self.engine.DetectWithImage(img_pil, threshold=self.min_confidence, keep_aspect_ratio=True,\n                                         relative_coord=False, top_k=self.num_of_objects)\n        if objects:\n            for obj in objects:\n                height = obj.bounding_box[1][1]-obj.bounding_box[0][1]\n                width = obj.bounding_box[1][0]-obj.bounding_box[0][0]\n                logging.debug(""%s, %.0f%% w=%.0f h=%.0f"" % (self.labels[obj.label_id], obj.score * 100, width, height))\n                box = obj.bounding_box\n                coord_top_left = (int(box[0][0]), int(box[0][1]))\n                coord_bottom_right = (int(box[1][0]), int(box[1][1]))\n                cv2.rectangle(frame, coord_top_left, coord_bottom_right, self.boxColor, self.boxLineWidth)\n                annotate_text = ""%s %.0f%%"" % (self.labels[obj.label_id], obj.score * 100)\n                coord_top_left = (coord_top_left[0], coord_top_left[1] + 15)\n                cv2.putText(frame, annotate_text, coord_top_left, self.font, self.fontScale, self.boxColor, self.lineType)\n        else:\n            logging.debug(\'No object detected\')\n\n        elapsed_ms = time.time() - start_ms\n\n        annotate_summary = ""%.1f FPS"" % (1.0/elapsed_ms)\n        logging.debug(annotate_summary)\n        cv2.putText(frame, annotate_summary, self.bottomLeftCornerOfText, self.font, self.fontScale, self.fontColor, self.lineType)\n        #cv2.imshow(\'Detected Objects\', frame)\n\n        return objects, frame\n\n\n############################\n# Utility Functions\n############################\ndef show_image(title, frame, show=_SHOW_IMAGE):\n    if show:\n        cv2.imshow(title, frame)\n\n\n############################\n# Test Functions\n############################\ndef test_photo(file):\n    object_processor = ObjectsOnRoadProcessor()\n    frame = cv2.imread(file)\n    combo_image = object_processor.process_objects_on_road(frame)\n    show_image(\'Detected Objects\', combo_image)\n\n    cv2.waitKey(0)\n    cv2.destroyAllWindows()\n\ndef test_stop_sign():\n    # this simulates a car at stop sign\n    object_processor = ObjectsOnRoadProcessor()\n    frame = cv2.imread(\'/home/pi/DeepPiCar/driver/data/objects/stop_sign.jpg\')\n    combo_image = object_processor.process_objects_on_road(frame)\n    show_image(\'Stop 1\', combo_image)\n    time.sleep(1)\n    frame = cv2.imread(\'/home/pi/DeepPiCar/driver/data/objects/stop_sign.jpg\')\n    combo_image = object_processor.process_objects_on_road(frame)\n    show_image(\'Stop 2\', combo_image)\n    time.sleep(2)\n    frame = cv2.imread(\'/home/pi/DeepPiCar/driver/data/objects/stop_sign.jpg\')\n    combo_image = object_processor.process_objects_on_road(frame)\n    show_image(\'Stop 3\', combo_image)\n    time.sleep(1)\n    frame = cv2.imread(\'/home/pi/DeepPiCar/driver/data/objects/green_light.jpg\')\n    combo_image = object_processor.process_objects_on_road(frame)\n    show_image(\'Stop 4\', combo_image)\n\n    cv2.waitKey(0)\n    cv2.destroyAllWindows()\n\ndef test_video(video_file):\n    object_processor = ObjectsOnRoadProcessor()\n    cap = cv2.VideoCapture(video_file + \'.avi\')\n\n    # skip first second of video.\n    for i in range(3):\n        _, frame = cap.read()\n\n    video_type = cv2.VideoWriter_fourcc(*\'XVID\')\n    date_str = datetime.datetime.now().strftime(""%y%m%d_%H%M%S"")\n    video_overlay = cv2.VideoWriter(""%s_overlay_%s.avi"" % (video_file, date_str), video_type, 20.0, (320, 240))\n    try:\n        i = 0\n        while cap.isOpened():\n            _, frame = cap.read()\n            cv2.imwrite(""%s_%03d.png"" % (video_file, i), frame)\n\n            combo_image = object_processor.process_objects_on_road(frame)\n            cv2.imwrite(""%s_overlay_%03d.png"" % (video_file, i), combo_image)\n            video_overlay.write(combo_image)\n\n            cv2.imshow(""Detected Objects"", combo_image)\n\n            i += 1\n            if cv2.waitKey(1) & 0xFF == ord(\'q\'):\n                break\n    finally:\n        cap.release()\n        video_overlay.release()\n        cv2.destroyAllWindows()\n\n\nif __name__ == \'__main__\':\n    logging.basicConfig(level=logging.DEBUG, format=\'%(levelname)-5s:%(asctime)s: %(message)s\')\n\n    # These processors contains no state\n    test_photo(\'/home/pi/DeepPiCar/driver/data/objects/red_light.jpg\')\n    test_photo(\'/home/pi/DeepPiCar/driver/data/objects/person.jpg\')\n    test_photo(\'/home/pi/DeepPiCar/driver/data/objects/limit_40.jpg\')\n    test_photo(\'/home/pi/DeepPiCar/driver/data/objects/limit_25.jpg\')\n    test_photo(\'/home/pi/DeepPiCar/driver/data/objects/green_light.jpg\')\n    test_photo(\'/home/pi/DeepPiCar/driver/data/objects/no_obj.jpg\')\n\n    # test stop sign, which carries state\n    test_stop_sign()'"
driver/code/opencv_test.py,0,"b""import cv2\n\ndef main():\n    camera = cv2.VideoCapture(-1)\n    camera.set(3, 640)\n    camera.set(4, 480)\n\n    while( camera.isOpened()):\n        _, image = camera.read()        \n        cv2.imshow('Original', image)\n        \n        b_w_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n        cv2.imshow('B/W', b_w_image)\n        \n        if cv2.waitKey(1) & 0xFF == ord('q') :\n            break\n            \n    cv2.destroyAllWindows()\n    \nif __name__ == '__main__':\n    main()"""
driver/code/save_training_data.py,0,"b'import cv2\nimport sys\nfrom hand_coded_lane_follower import HandCodedLaneFollower\n\n\ndef save_image_and_steering_angle(video_file):\n    lane_follower = HandCodedLaneFollower()\n    cap = cv2.VideoCapture(video_file + \'.avi\')\n\n    try:\n        i = 0\n        while cap.isOpened():\n            _, frame = cap.read()\n            lane_follower.follow_lane(frame)\n            cv2.imwrite(""%s_%03d_%03d.png"" % (video_file, i, lane_follower.curr_steering_angle), frame)\n            i += 1\n            if cv2.waitKey(1) & 0xFF == ord(\'q\'):\n                break\n    finally:\n        cap.release()\n        cv2.destroyAllWindows()\n\n\nif __name__ == \'__main__\':\n    save_image_and_steering_angle(sys.argv[1])'"
driver/code/traffic_objects.py,0,"b'from threading import Timer\nimport logging\n\n\nclass TrafficObject(object):\n\n    def set_car_state(self, car_state):\n        pass\n\n    @staticmethod\n    def is_close_by(obj, frame_height, min_height_pct=0.05):\n        # default: if a sign is 10% of the height of frame\n        obj_height = obj.bounding_box[1][1]-obj.bounding_box[0][1]\n        return obj_height / frame_height > min_height_pct\n\n\nclass RedTrafficLight(TrafficObject):\n\n    def set_car_state(self, car_state):\n        logging.debug(\'red light: stopping car\')\n        car_state[\'speed\'] = 0\n\n\nclass GreenTrafficLight(TrafficObject):\n\n    def set_car_state(self, car_state):\n        logging.debug(\'green light: make no changes\')\n\n\nclass Person(TrafficObject):\n\n    def set_car_state(self, car_state):\n        logging.debug(\'pedestrian: stopping car\')\n\n        car_state[\'speed\'] = 0\n\n\nclass SpeedLimit(TrafficObject):\n\n    def __init__(self, speed_limit):\n        self.speed_limit = speed_limit\n\n    def set_car_state(self, car_state):\n        logging.debug(\'speed limit: set limit to %d\' % self.speed_limit)\n        car_state[\'speed_limit\'] = self.speed_limit\n\n\nclass StopSign(TrafficObject):\n    """"""\n    Stop Sign object would wait\n    """"""\n\n    def __init__(self, wait_time_in_sec=3, min_no_stop_sign=20):\n        self.in_wait_mode = False\n        self.has_stopped = False\n        self.wait_time_in_sec = wait_time_in_sec\n        self.min_no_stop_sign = min_no_stop_sign\n        self.no_stop_count = min_no_stop_sign\n        self.timer = None\n\n    def set_car_state(self, car_state):\n        self.no_stop_count = self.min_no_stop_sign\n\n        if self.in_wait_mode:\n            logging.debug(\'stop sign: 2) still waiting\')\n            # wait for 2 second before proceeding\n            car_state[\'speed\'] = 0\n            return\n\n        if not self.has_stopped:\n            logging.debug(\'stop sign: 1) just detected\')\n\n            car_state[\'speed\'] = 0\n            self.in_wait_mode = True\n            self.has_stopped = True\n            self.timer = Timer(self.wait_time_in_sec, self.wait_done)\n            self.timer.start()\n            return\n\n    def wait_done(self):\n        logging.debug(\'stop sign: 3) finished waiting for %d seconds\' % self.wait_time_in_sec)\n        self.in_wait_mode = False\n\n    def clear(self):\n        if self.has_stopped:\n            # need this counter in case object detection has a glitch that one frame does not\n            # detect stop sign, make sure we see 20 consecutive no stop sign frames (about 1 sec)\n            # and then mark has_stopped = False\n            self.no_stop_count -= 1\n            if self.no_stop_count == 0:\n                logging.debug(""stop sign: 4) no more stop sign detected"")\n                self.has_stopped = False\n                self.in_wait_mode = False  # may not need to set this\n'"
models/object_detection/code/coco_object_detection.py,0,"b'""""""A demo to classify Raspberry Pi camera stream.""""""\nimport argparse\nimport time\n\nimport numpy as np\nimport os\nimport datetime\n\nimport edgetpu.detection.engine\nimport cv2\nfrom PIL import Image\n\ndef main():\n    os.chdir(\'/home/pi/DeepPiCar/models/object_detection\')\n    \n    parser = argparse.ArgumentParser()\n    parser.add_argument(\n      \'--model\', help=\'File path of Tflite model.\', required=False)\n    parser.add_argument(\n      \'--label\', help=\'File path of label file.\', required=False)\n    args = parser.parse_args()\n    \n    args.model = \'data/model_result/mobilenet_ssd_v2_coco_quant_postprocess_edgetpu.tflite\'\n    args.label = \'data/model_result/coco_labels.txt\'\n        \n    with open(args.label, \'r\') as f:\n        pairs = (l.strip().split(maxsplit=1) for l in f.readlines())\n        labels = dict((int(k), v) for k, v in pairs)\n\n    # initialize open cv\n    IM_WIDTH = 640\n    IM_HEIGHT = 480\n    camera = cv2.VideoCapture(0)\n    ret = camera.set(3,IM_WIDTH)\n    ret = camera.set(4,IM_HEIGHT)\n    \n    font = cv2.FONT_HERSHEY_SIMPLEX\n    bottomLeftCornerOfText = (10,IM_HEIGHT-10)\n    fontScale = 1\n    fontColor = (255,255,255)  # white\n    boxColor = (0,0,255)   # RED?\n    boxLineWidth = 1\n    lineType = 2\n    \n    annotate_text = """"\n    annotate_text_time = time.time()\n    time_to_show_prediction = 1.0 # ms\n    min_confidence = 0.20\n    \n    # initial classification engine\n    engine = edgetpu.detection.engine.DetectionEngine(args.model)\n    elapsed_ms = 0\n    \n    fourcc = cv2.VideoWriter_fourcc(*\'XVID\')\n    out = cv2.VideoWriter(\'output.avi\',fourcc, 20.0, (IM_WIDTH,IM_HEIGHT))\n    \n    \n    try:\n        while camera.isOpened():\n            try:\n                start_ms = time.time()\n                ret, frame = camera.read() # grab a frame from camera\n                if ret == False :\n                    print(\'can NOT read from camera\')\n                    break\n                \n                frame_expanded = np.expand_dims(frame, axis=0)\n                \n                ret, img = camera.read()\n                input = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)  # convert to RGB color space\n                img_pil = Image.fromarray(input)\n                #input = cv2.resize(input, (width,height))\n                start_tf_ms = time.time()\n                results = engine.DetectWithImage(img_pil, threshold=min_confidence, keep_aspect_ratio=True,\n                                   relative_coord=False, top_k=5)\n                end_tf_ms = time.time()\n                elapsed_tf_ms = end_tf_ms - start_ms\n                \n                if results :\n                    for obj in results:\n                        \n                        print(""%s, %.0f%% %s %.2fms"" % (labels[obj.label_id], obj.score *100, obj.bounding_box, elapsed_tf_ms * 1000))\n                        box = obj.bounding_box\n                        coord_top_left = (int(box[0][0]), int(box[0][1]))\n                        coord_bottom_right = (int(box[1][0]), int(box[1][1]))\n                        cv2.rectangle(img, coord_top_left, coord_bottom_right, boxColor, boxLineWidth)\n                        annotate_text = ""%s, %.0f%%"" % (labels[obj.label_id], obj.score * 100)\n                        coord_top_left = (coord_top_left[0],coord_top_left[1]+15)\n                        cv2.putText(img, annotate_text, coord_top_left, font, fontScale, boxColor, lineType )\n                    print(\'------\')\n                else:\n                    print(\'No object detected\')\n\n                # Print Frame rate info\n                elapsed_ms = time.time() - start_ms\n                annotate_text = ""%.2f FPS, %.2fms total, %.2fms in tf "" % (1.0 / elapsed_ms, elapsed_ms*1000, elapsed_tf_ms*1000)\n                print(\'%s: %s\' % (datetime.datetime.now(), annotate_text))\n                cv2.putText(img, annotate_text, bottomLeftCornerOfText, font, fontScale, fontColor, lineType)\n                \n                out.write(img)\n                    \n                cv2.imshow(\'Detected Objects\', img)\n                if cv2.waitKey(1) & 0xFF == ord(\'q\'):\n                    break\n            except:\n                # catch it and don\'t exit the while loop\n                print(\'In except\')\n                traceback.print_exc()\n\n    finally:\n        print(\'In Finally\')\n        camera.release()\n        out.release()\n        cv2.destroyAllWindows()\n\nif __name__ == \'__main__\':\n    main()\n'"
models/object_detection/code/generate_tfrecord.py,6,"b'""""""\nUsage:\n\n# Create train data:\npython generate_tfrecord.py --label=<LABEL> --csv_input=<PATH_TO_ANNOTATIONS_FOLDER>/train_labels.csv  --output_path=<PATH_TO_ANNOTATIONS_FOLDER>/train.record <PATH_TO_ANNOTATIONS_FOLDER>/label_map.pbtxt\n\n# Create test data:\npython generate_tfrecord.py --label=<LABEL> --csv_input=<PATH_TO_ANNOTATIONS_FOLDER>/test_labels.csv  --output_path=<PATH_TO_ANNOTATIONS_FOLDER>/test.record  --label_map <PATH_TO_ANNOTATIONS_FOLDER>/label_map.pbtxt\n""""""\n\nfrom __future__ import division\nfrom __future__ import print_function\nfrom __future__ import absolute_import\n\nimport os\nimport io\nimport pandas as pd\nimport tensorflow as tf\nimport sys\n\nsys.path.append(""../../models/research"")\n\nfrom PIL import Image\nfrom object_detection.utils import dataset_util\nfrom collections import namedtuple, OrderedDict\n\nflags = tf.app.flags\nflags.DEFINE_string(""csv_input"", """", ""Path to the CSV input"")\nflags.DEFINE_string(""output_path"", """", ""Path to output TFRecord"")\nflags.DEFINE_string(\n    ""label_map"",\n    """",\n    ""Path to the `label_map.pbtxt` contains the <class_name>:<class_index> pairs generated by `xml_to_csv.py` or manually."",\n)\n# if your image has more labels input them as\n# flags.DEFINE_string(\'label0\', \'\', \'Name of class[0] label\')\n# flags.DEFINE_string(\'label1\', \'\', \'Name of class[1] label\')\n# and so on.\nflags.DEFINE_string(""img_path"", """", ""Path to images"")\nFLAGS = flags.FLAGS\n\n\ndef split(df, group):\n    data = namedtuple(""data"", [""filename"", ""object""])\n    gb = df.groupby(group)\n    return [\n        data(filename, gb.get_group(x))\n        for filename, x in zip(gb.groups.keys(), gb.groups)\n    ]\n\n\ndef create_tf_example(group, path, label_map):\n    with tf.gfile.GFile(os.path.join(path, ""{}"".format(group.filename)), ""rb"") as fid:\n        encoded_jpg = fid.read()\n    encoded_jpg_io = io.BytesIO(encoded_jpg)\n    image = Image.open(encoded_jpg_io)\n    width, height = image.size\n\n    filename = group.filename.encode(""utf8"")\n    image_format = b""jpg""\n    # check if the image format is matching with your images.\n    xmins = []\n    xmaxs = []\n    ymins = []\n    ymaxs = []\n    classes_text = []\n    classes = []\n\n    for index, row in group.object.iterrows():\n        xmins.append(row[""xmin""] / width)\n        xmaxs.append(row[""xmax""] / width)\n        ymins.append(row[""ymin""] / height)\n        ymaxs.append(row[""ymax""] / height)\n        classes_text.append(row[""class""].encode(""utf8""))\n        class_index = label_map.get(row[""class""])\n        assert (\n            class_index is not None\n        ), ""class label: `{}` not found in label_map: {}"".format(\n            row[""class""], label_map\n        )\n        classes.append(class_index)\n\n    tf_example = tf.train.Example(\n        features=tf.train.Features(\n            feature={\n                ""image/height"": dataset_util.int64_feature(height),\n                ""image/width"": dataset_util.int64_feature(width),\n                ""image/filename"": dataset_util.bytes_feature(filename),\n                ""image/source_id"": dataset_util.bytes_feature(filename),\n                ""image/encoded"": dataset_util.bytes_feature(encoded_jpg),\n                ""image/format"": dataset_util.bytes_feature(image_format),\n                ""image/object/bbox/xmin"": dataset_util.float_list_feature(xmins),\n                ""image/object/bbox/xmax"": dataset_util.float_list_feature(xmaxs),\n                ""image/object/bbox/ymin"": dataset_util.float_list_feature(ymins),\n                ""image/object/bbox/ymax"": dataset_util.float_list_feature(ymaxs),\n                ""image/object/class/text"": dataset_util.bytes_list_feature(\n                    classes_text\n                ),\n                ""image/object/class/label"": dataset_util.int64_list_feature(classes),\n            }\n        )\n    )\n    return tf_example\n\n\ndef main(_):\n    writer = tf.python_io.TFRecordWriter(FLAGS.output_path)\n    path = os.path.join(os.getcwd(), FLAGS.img_path)\n    examples = pd.read_csv(FLAGS.csv_input)\n\n    # Load the `label_map` from pbtxt file.\n    from object_detection.utils import label_map_util\n\n    label_map = label_map_util.load_labelmap(FLAGS.label_map)\n    categories = label_map_util.convert_label_map_to_categories(\n        label_map, max_num_classes=90, use_display_name=True\n    )\n    category_index = label_map_util.create_category_index(categories)\n    label_map = {}\n    for k, v in category_index.items():\n        label_map[v.get(""name"")] = v.get(""id"")\n\n    grouped = split(examples, ""filename"")\n    for group in grouped:\n        tf_example = create_tf_example(group, path, label_map)\n        writer.write(tf_example.SerializeToString())\n\n    writer.close()\n    output_path = os.path.join(os.getcwd(), FLAGS.output_path)\n    print(""Successfully created the TFRecords: {}"".format(output_path))\n\n\nif __name__ == ""__main__"":\n    tf.app.run()\n'"
models/object_detection/code/object_detection_usb.py,0,"b'""""""A demo to classify Raspberry Pi camera stream.""""""\nimport argparse\nimport time\n\nimport numpy as np\nimport os\nimport datetime\n\nimport edgetpu.detection.engine\nimport cv2\nfrom PIL import Image\n\ndef main():\n    os.chdir(\'/home/pi/DeepPiCar/models/object_detection\')\n    \n    parser = argparse.ArgumentParser()\n    parser.add_argument(\n      \'--model\', help=\'File path of Tflite model.\', required=False)\n    parser.add_argument(\n      \'--label\', help=\'File path of label file.\', required=False)\n    args = parser.parse_args()\n    \n    #args.model = \'test_data/mobilenet_ssd_v2_coco_quant_postprocess.tflite\'\n    args.model = \'data/model_result/road_signs_quantized.tflite\'\n    args.label = \'data/model_result/road_sign_labels.txt\'\n        \n    with open(args.label, \'r\') as f:\n        pairs = (l.strip().split(maxsplit=1) for l in f.readlines())\n        labels = dict((int(k), v) for k, v in pairs)\n\n    # initialize open cv\n    IM_WIDTH = 640\n    IM_HEIGHT = 480\n    camera = cv2.VideoCapture(0)\n    ret = camera.set(3,IM_WIDTH)\n    ret = camera.set(4,IM_HEIGHT)\n    \n    font = cv2.FONT_HERSHEY_SIMPLEX\n    bottomLeftCornerOfText = (10,IM_HEIGHT-10)\n    fontScale = 1\n    fontColor = (255,255,255)  # white\n    boxColor = (0,0,255)   # RED?\n    boxLineWidth = 1\n    lineType = 2\n    \n    annotate_text = """"\n    annotate_text_time = time.time()\n    time_to_show_prediction = 1.0 # ms\n    min_confidence = 0.20\n    \n    # initial classification engine\n    engine = edgetpu.detection.engine.DetectionEngine(args.model)\n    elapsed_ms = 0\n    \n    fourcc = cv2.VideoWriter_fourcc(*\'XVID\')\n    out = cv2.VideoWriter(\'output.avi\',fourcc, 20.0, (IM_WIDTH,IM_HEIGHT))\n    \n    \n    try:\n        while camera.isOpened():\n            try:\n                start_ms = time.time()\n                ret, frame = camera.read() # grab a frame from camera\n                if ret == False :\n                    print(\'can NOT read from camera\')\n                    break\n\n                input = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)  # convert to RGB color space\n                img_pil = Image.fromarray(input)\n                #input = cv2.resize(input, (width,height))\n                start_tf_ms = time.time()\n                results = engine.DetectWithImage(img_pil, threshold=min_confidence, keep_aspect_ratio=True,\n                                   relative_coord=False, top_k=5)\n                end_tf_ms = time.time()\n                elapsed_tf_ms = end_tf_ms - start_ms\n                \n                if results :\n                    for obj in results:\n                        \n                        print(""%s, %.0f%% %s %.2fms"" % (labels[obj.label_id], obj.score *100, obj.bounding_box, elapsed_tf_ms * 1000))\n                        box = obj.bounding_box\n                        coord_top_left = (int(box[0][0]), int(box[0][1]))\n                        coord_bottom_right = (int(box[1][0]), int(box[1][1]))\n                        cv2.rectangle(img, coord_top_left, coord_bottom_right, boxColor, boxLineWidth)\n                        annotate_text = ""%s, %.0f%%"" % (labels[obj.label_id], obj.score * 100)\n                        coord_top_left = (coord_top_left[0],coord_top_left[1]+15)\n                        cv2.putText(img, annotate_text, coord_top_left, font, fontScale, boxColor, lineType )\n                    print(\'------\')\n                else:\n                    print(\'No object detected\')\n\n                # Print Frame rate info\n                elapsed_ms = time.time() - start_ms\n                annotate_text = ""%.2f FPS, %.2fms total, %.2fms in tf "" % (1.0 / elapsed_ms, elapsed_ms*1000, elapsed_tf_ms*1000)\n                print(\'%s: %s\' % (datetime.datetime.now(), annotate_text))\n                cv2.putText(img, annotate_text, bottomLeftCornerOfText, font, fontScale, fontColor, lineType)\n                \n                out.write(img)\n                    \n                cv2.imshow(\'Detected Objects\', img)\n                if cv2.waitKey(1) & 0xFF == ord(\'q\'):\n                    break\n            except:\n                # catch it and don\'t exit the while loop\n                print(\'In except\')\n                traceback.print_exc()\n\n    finally:\n        print(\'In Finally\')\n        camera.release()\n        out.release()\n        cv2.destroyAllWindows()\n\nif __name__ == \'__main__\':\n    main()\n'"
models/object_detection/code/xml_to_csv.py,0,"b'""""""\nUsage:\n# Create train data:\npython xml_to_csv.py -i [PATH_TO_IMAGES_FOLDER]/train -o [PATH_TO_ANNOTATIONS_FOLDER]/train_labels.csv\n\n# Create test data:\npython xml_to_csv.py -i [PATH_TO_IMAGES_FOLDER]/test -o [PATH_TO_ANNOTATIONS_FOLDER]/test_labels.csv\n""""""\n\nimport os\nimport glob\nimport pandas as pd\nimport argparse\nimport xml.etree.ElementTree as ET\n\n\ndef xml_to_csv(path):\n    """"""Iterates through all .xml files (generated by labelImg) in a given directory and combines them in a single Pandas datagrame.\n\n    Parameters:\n    ----------\n    path : {str}\n        The path containing the .xml files\n    Returns\n    -------\n    Pandas DataFrame\n        The produced dataframe\n    """"""\n    classes_names = []\n    xml_list = []\n    for xml_file in glob.glob(path + ""/*.xml""):\n        tree = ET.parse(xml_file)\n        root = tree.getroot()\n        for member in root.findall(""object""):\n            classes_names.append(member[0].text)\n            value = (\n                root.find(""filename"").text,\n                int(root.find(""size"")[0].text),\n                int(root.find(""size"")[1].text),\n                member[0].text,\n                int(member[4][0].text),\n                int(member[4][1].text),\n                int(member[4][2].text),\n                int(member[4][3].text),\n            )\n            xml_list.append(value)\n    column_name = [\n        ""filename"",\n        ""width"",\n        ""height"",\n        ""class"",\n        ""xmin"",\n        ""ymin"",\n        ""xmax"",\n        ""ymax"",\n    ]\n    xml_df = pd.DataFrame(xml_list, columns=column_name)\n    classes_names = list(set(classes_names))\n    classes_names.sort()\n    return xml_df, classes_names\n\n\ndef main():\n    # Initiate argument parser\n    parser = argparse.ArgumentParser(\n        description=""Sample TensorFlow XML-to-CSV converter""\n    )\n    parser.add_argument(\n        ""-i"",\n        ""--inputDir"",\n        help=""Path to the folder where the input .xml files are stored"",\n        type=str,\n    )\n    parser.add_argument(\n        ""-o"", ""--outputFile"", help=""Name of output .csv file (including path)"", type=str\n    )\n\n    parser.add_argument(\n        ""-l"",\n        ""--labelMapDir"",\n        help=""Directory path to save label_map.pbtxt file is specified."",\n        type=str,\n        default="""",\n    )\n\n    args = parser.parse_args()\n\n    if args.inputDir is None:\n        args.inputDir = os.getcwd()\n    if args.outputFile is None:\n        args.outputFile = args.inputDir + ""/labels.csv""\n\n    assert os.path.isdir(args.inputDir)\n    os.makedirs(os.path.dirname(args.outputFile), exist_ok=True)\n    xml_df, classes_names = xml_to_csv(args.inputDir)\n    xml_df.to_csv(args.outputFile, index=None)\n    print(""Successfully converted xml to csv."")\n    if args.labelMapDir:\n        os.makedirs(args.labelMapDir, exist_ok=True)\n        label_map_path = os.path.join(args.labelMapDir, ""label_map.pbtxt"")\n        print(""Generate `{}`"".format(label_map_path))\n\n        # Create the `label_map.pbtxt` file\n        pbtxt_content = """"\n        for i, class_name in enumerate(classes_names):\n            pbtxt_content = (\n                pbtxt_content\n                + ""item {{\\n    id: {0}\\n    name: \'{1}\'\\n}}\\n\\n"".format(\n                    i + 1, class_name\n                )\n            )\n        pbtxt_content = pbtxt_content.strip()\n        with open(label_map_path, ""w"") as f:\n            f.write(pbtxt_content)\n\n\nif __name__ == ""__main__"":\n    main()\n'"
