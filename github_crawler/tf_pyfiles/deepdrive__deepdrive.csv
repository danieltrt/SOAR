file_path,api_count,code
dashboard.py,0,"b'import json\nimport sys\nimport time\nfrom threading import Thread\n\n# TODO: Replace matplotlib UI with Unreal based HUD - and do comms via exsiting Python to Unreal channels\n\nis_py2 = sys.version[0] == \'2\'\nif is_py2:\n    import Queue as queue\nelse:\n    import queue as queue\nfrom collections import deque, OrderedDict\n\nimport numpy as np\nimport zmq\n\nimport logs\n\nZMQ_PREFIX = \'deepdrive-dashboard\'\nZMQ_CONN_STRING = ""tcp://127.0.0.1:5681""\n\nlog = logs.get_log(__name__, \'dashboard_log.txt\')\n\n\ndef dashboard_fn():\n    log.info(\'Starting dashboard\')\n\n    message_q = get_message_q()\n\n    import matplotlib.animation as animation\n    import matplotlib\n    try:\n        # noinspection PyUnresolvedReferences\n        import matplotlib.pyplot as plt\n    except ImportError as e:\n        log.error(\'\\n\\n\\n***** Error: Could not start dashboard: %s\\n\\n\', e)\n        return\n    plt.figure(0)\n\n    class Disp(object):\n        stats = {}\n        txt_values = {}\n        lines = {}\n        x_lists = {}\n        y_lists = {}\n\n    def get_next():\n        try:\n            message = message_q.pop()\n            message = message[len(ZMQ_PREFIX) + 1:]\n            q_next = OrderedDict(json.loads(message.decode(\'utf8\').replace(""\'"", \'""\')))\n            if q_next.get(\'should_stop\', False):\n                print(\'Stopping dashboard\')\n                try:\n                    anim._fig.canvas._tkcanvas.master.quit()  # Hack to avoid ""Exiting Abnormally""\n                finally:\n                    print(\'Dashboard stopped\')\n                    exit()\n            else:\n                Disp.stats = OrderedDict(q_next[\'display_stats\'])\n        except IndexError:\n            # Reuuse old stats\n            pass\n        except KeyboardInterrupt:\n            print(\'KeyboardInterrupt detected in dashboard\')\n\n    get_next()\n    while not Disp.stats.items():\n        get_next()\n        time.sleep(0.1)\n\n    font = {\'size\': 8}\n    matplotlib.rc(\'font\', **font)\n\n    log.debug(\'Populating graph parts with %r\', Disp.stats.items())\n    for i, (stat_name, stat) in enumerate(Disp.stats.items()):\n        stat = Disp.stats[stat_name]\n        stat_label_subplot = plt.subplot2grid((len(Disp.stats), 3), (i, 0))\n        stat_value_subplot = plt.subplot2grid((len(Disp.stats), 3), (i, 1))\n        stat_graph_subplot = plt.subplot2grid((len(Disp.stats), 3), (i, 2))\n        stat_label_subplot.text(0.5, 0.5, stat_name, fontsize=12, va=""center"", ha=""center"")\n        txt_value = stat_value_subplot.text(0.5, 0.5, \'\', fontsize=12, va=""center"", ha=""center"")\n        Disp.txt_values[stat_name] = txt_value\n        stat_graph_subplot.set_xlim([0, 200])\n        stat_graph_subplot.set_ylim([stat[\'ymin\'], stat[\'ymax\']])\n        Disp.lines[stat_name], = stat_graph_subplot.plot([], [])\n        stat_label_subplot.axis(\'off\')\n        stat_value_subplot.axis(\'off\')\n        Disp.x_lists[stat_name] = deque(np.linspace(200, 0, num=400))\n        Disp.y_lists[stat_name] = deque([-1] * 400)\n        frame1 = plt.gca()\n        frame1.axes.get_xaxis().set_visible(False)\n\n    plt.subplots_adjust(hspace=0.88)\n    fig = plt.gcf()\n    fig.set_size_inches(5.5, len(Disp.stats) * 0.5)\n    fig.canvas.set_window_title(\'Dashboard\')\n    anim = None\n\n    def init():\n        lines = []\n        for s_name in Disp.stats:\n            line = Disp.lines[s_name]\n            line.set_data([], [])\n            lines.append(line)\n        return lines\n\n    def animate(_i):\n        lines = []\n        try:\n            get_next()\n            for s_name in Disp.stats:\n                s = Disp.stats[s_name]\n                xs = Disp.x_lists[s_name]\n                ys = Disp.y_lists[s_name]\n                tv = Disp.txt_values[s_name]\n                line = Disp.lines[s_name]\n                val = s[\'value\']\n                total = s[\'total\']\n                tv.set_text(str(round(total, 2)) + s[\'units\'])\n                ys.pop()\n                ys.appendleft(val)\n                line.set_data(xs, ys)\n                lines.append(line)\n            plt.draw()\n        except KeyboardInterrupt:\n            print(\'KeyboardInterrupt detected in animate, exiting\')\n            exit()\n        return lines\n\n    # TODO: Add blit=True and deal with updating the text if performance becomes unacceptable\n    anim = animation.FuncAnimation(fig, animate, init_func=init, frames=200, interval=100)\n\n    try:\n        plt.show()\n    except KeyboardInterrupt:\n        print(\'KeyboardInterrupt detected in show dashboard, exiting\')\n        exit()\n\n\n# ZMQ stuff (multiprocessing queue was taking 80ms, so switched to faster zmq) -----------------------------------------\n\nclass DashboardPub(object):\n    def __init__(self):\n        # ZeroMQ Context\n        self.context = zmq.Context()\n\n        # Define the socket using the ""Context""\n        sock = self.context.socket(zmq.PUB)\n        sock.bind(ZMQ_CONN_STRING)\n        self.sock = sock\n\n    def put(self, display_stats):\n        # Message [prefix][message]\n        message = ""{prefix}-{msg}"".format(prefix=ZMQ_PREFIX, msg=json.dumps(list(display_stats.items())))\n        log.debug(\'dashpub put %s\', message)\n        start = time.time()\n        self.sock.send_string(message)\n        log.debug(\'took %fs\', time.time() - start)\n\n    def close(self):\n        log.info(\'Closing dashboard\')\n        self.sock.close()\n        self.context.term()\n\n\ndef get_message_q():\n    q = deque(maxlen=10)\n    zmq_socket = start_zmq_sub()\n    thread = Thread(target=poll_zmq, args=(zmq_socket, q))\n    thread.start()\n    return q\n\n\ndef poll_zmq(zmq_socket, q):\n    while True:\n        message = zmq_socket.recv()\n        log.debug(\'dashsub pull %s\', message)\n        q.appendleft(message)\n\n\ndef start_zmq_sub():\n    # ZeroMQ Context\n    context = zmq.Context()\n    # Define the socket using the ""Context""\n    sock = context.socket(zmq.SUB)\n    sock.setsockopt_string(zmq.SUBSCRIBE, ZMQ_PREFIX)\n    sock.connect(ZMQ_CONN_STRING)\n    return sock\n'"
example.py,0,"b""import sim\n\n\ndef main():\n    env = sim.start(\n\n        # map can be canyons, kevindale, kevindale_bare, or jamestown\n        map='kevindale_bare',\n\n        # scenario can be 0 => 5 - Scenario descriptions:\n        # https://gist.github.com/crizCraig/855a5cc4bc96fc2765cb9bf5d6f953b4\n        scenario_index=1\n    )\n    forward = sim.action(throttle=0.75, steering=0, brake=0)\n    done = False\n    while not done:\n        observation, reward, done, info = env.step(forward)\n    env.close()\n\nif __name__ == '__main__':\n    main()\n"""
example_remote.py,0,"b""import time\n\nimport sim\nfrom sim.action import Action\n\n\n# Usage: Please make sure to start a fresh api/server.py as connection life cycle is not properly implemented yet\n\ndef main():\n    env = sim.start(is_remote_client=True, render=True)\n    forward = Action(throttle=1)\n    done = False\n    while True:\n        while not done:\n            observation, reward, done, info = env.step(forward)\n        env.reset()\n        print('Episode finished')\n        done = False\n\n\nif __name__ == '__main__':\n    main()\n"""
example_sync.py,0,"b""import sim\nimport config\nfrom sim import DrivingStyle\n\n\ndef main():\n    env = sim.start(is_sync=True,\n                    render=True,\n                    enable_traffic=True,\n                    experiment='my_experiment',\n                    cameras=[config.DEFAULT_CAM],\n                    fps=config.DEFAULT_FPS,  # Agent steps per second\n                    sim_step_time=config.DEFAULT_SIM_STEP_TIME,\n                    is_discrete=False,  # Discretizes the action space\n                    driving_style=DrivingStyle.NORMAL,\n                    is_remote_client=False,\n                    max_steps=500,\n                    max_episodes=100,\n                    should_record=True,  # HDF5/numpy recordings\n                    recording_dir=config.RECORDING_DIR,\n                    randomize_view_mode=False,\n                    view_mode_period=None,  # Domain randomization\n                    randomize_sun_speed=False,\n                    randomize_shadow_level=False,\n                    randomize_month=False)\n\n    forward = sim.action(throttle=1, steering=0, brake=0)\n    done = False\n    while True:\n        while not done:\n            observation, reward, done, info = env.step(forward)\n        env.reset()\n        print('Episode finished')\n        done = False\n\n\nif __name__ == '__main__':\n    main()\n"""
install.py,5,"b'from __future__ import (absolute_import, division,\n                        print_function, unicode_literals)\nimport argparse\nimport os\nimport shutil\nimport tempfile\nfrom subprocess import Popen, PIPE\nimport sys\nimport platform\nfrom distutils.spawn import find_executable\nfrom distutils.version import LooseVersion as semvar\n\nDIR = os.path.dirname(os.path.realpath(__file__))\n\nIS_LINUX = sys.platform == \'linux\' or sys.platform == \'linux2\'\nIS_MAC = sys.platform == \'darwin\'\nIS_UNIX = IS_LINUX or IS_MAC or \'bsd\' in sys.platform.lower()\nIS_WINDOWS = sys.platform == \'win32\'\n\n\ndef run_command_with_sarge(cmd, throw=True):\n    from sarge import run, Capture\n    # TODO: p = run(..., stdout=Capture(buffer_size=-1), stderr=Capture(buffer_size=-1))\n    # TODO: Then log p.stdout. while process not complete in realtime and to file\n    p = run(cmd, async_=True)\n    # Allow streaming stdout and stderr to user while command executes\n    p.close()\n    if p.returncode != 0:\n        if throw:\n            raise RuntimeError(\'Command failed, see above\')\n\n\ndef run_command_no_deps(cmd, cwd=None, env=None, throw=True, verbose=False, print_errors=True):\n    def say(*args):\n        if verbose:\n            print(*args)\n\n    say(\'running command: \' + cmd)\n    if not isinstance(cmd, list):\n        cmd = cmd.split()\n    process = Popen(cmd, stdin=PIPE, stdout=PIPE, stderr=PIPE, cwd=cwd, env=env)\n    result, err = process.communicate()\n    if not isinstance(result, str):\n        result = \'\'.join(map(chr, result))\n    result = result.strip()\n    say(result)\n    if process.returncode != 0:\n        if not isinstance(err, str):\n            err = \'\'.join(map(chr, err))\n        err_msg = \' \'.join(cmd) + \' finished with error \' + err.strip()\n        if throw:\n            raise RuntimeError(err_msg)\n        elif print_errors:\n            say(err_msg)\n    return result, process.returncode\n\n\ndef check_py_version():\n    version = sys.version_info[:]\n    if version[0] == 3 and version[1] >= 5:\n        return sys.executable\n    else:\n        raise RuntimeError(\'Error: Python 3.5+ is required to run deepdrive\')\n\n\ndef get_latest_valid_bindings():\n    _, _, major_minor = get_version_info()\n    aws_bucket = \'deepdrive\'\n    aws_bucket_url = \'https://s3-us-west-1.amazonaws.com/\' + aws_bucket\n    from boto.s3.connection import S3Connection\n    prefix = f\'validated-bindings-versions/{major_minor}\'\n    conn = S3Connection(anon=True)\n    bucket = conn.get_bucket(\'deepdrive\')\n    bucket_search_str = prefix\n    bindings_versions = list(bucket.list(bucket_search_str))\n    if not bindings_versions:\n        raise RuntimeError(\'Could not find a bindings version matching %s \'\n                           \'in bucket %s\' % (bucket_search_str, aws_bucket_url))\n\n    bindings_versions = [b.name.split(\'/\')[1] for b in bindings_versions]\n    bindings_versions = sorted(bindings_versions)\n\n    ret = bindings_versions[-1]\n    return ret\n\n\ndef get_version_info():\n    version_str = open(os.path.join(DIR, \'VERSION\')).read().strip()\n    major_minor_version = semvar(version_str).version[:2]\n    major_minor_version_str = \'.\'.join(str(vx) for vx in major_minor_version)\n    return version_str, major_minor_version, major_minor_version_str\n\n\ndef main():\n    print(\'Checking python version...\', end=\'\')\n    py = check_py_version()\n    print(\'check!\')\n\n    if not is_docker():\n        # Docker does not build with nvidia runtime. We can do this if\n        # we want by setting the default docker runtime to nvidia, but\n        # we don\'t want to require that people do this.\n        check_tensorflow_gpu(is_install=True)\n\n    # Install sarge to nicely stream commands and wheel for precompiled packages\n    # Install requests and boto to get bindings version\n    run_command_no_deps(py + \' -m pip install sarge wheel requests boto\',\n                        verbose=True)\n\n\n    if \'ubuntu\' in platform.platform().lower() and not is_docker():\n        # Install tk for dashboard\n        run_command_with_sarge(\'sudo apt-get install -y python3-tk\', throw=False)\n\n    if os.name == \'nt\':\n        req_filename = \'requirements-windows.txt\'\n    else:\n        req_filename = \'requirements.txt\'\n\n    run_command_with_sarge(f\'{py} -m pip install -r {req_filename}\')\n\n    # Create deepdrive directory\n    import config as c\n\n    if is_docker():\n        pip_args = \'--no-cache-dir\'\n    else:\n        pip_args = \'\'\n\n    # Install correct version of the python bindings\n    # # TODO: Remove dev0 once 3.0 is stable\n    # run_command_with_sarge(py + \' -m pip install {pip_args} ""deepdrive > {major_minor_version}.*dev0""\'.format(\n\n    bindings_version = get_latest_valid_bindings()\n\n    print(f\'Installing latest valid sim-bindings {bindings_version}\')\n\n    run_command_with_sarge(\n        f\'{py} -m pip install {pip_args} ""deepdrive=={bindings_version}.dev0""\')\n\n    # noinspection PyUnresolvedReferences\n    import config.check_bindings\n\n    print(""""""\n   ___                  __    _            \n  / _ \\___ ___ ___  ___/ /___(_)  _____    \n / // / -_) -_) _ \\/ _  / __/ / |/ / -_)   \n/____/\\__/\\__/ .__/\\_,_/_/ /_/|___/\\__/    \n  _______ __/_/___/ /_ __                  \n / __/ -_) _ `/ _  / // /                  \n/_/  \\__/\\_,_/\\_,_/\\_, /                   \n                  /___/            \n    """""")\n    # Gen: https://bit.ly/2SrCVFO\n\n\ndef check_nvidia_docker():\n    if is_docker() and not has_nvidia_docker():\n        print(\'WARNING: No nvidia-docker runtime detected\', file=sys.stderr)\n        return False\n    else:\n        return True\n\n\ndef check_tensorflow_gpu(is_install=False):\n    error_msg = \\\n        \'\\n\\n*** Warning: %s, Tensorflow agents will not be available. \' \\\n        \'HINT: Install Tensorflow or use the python / virtualenv \' \\\n        \'you have it already installed to. \' \\\n        \'If you install, check out our Tensorflow install \' \\\n        \'tips on the README \' \\\n        \'\\n\\n\'\n    print(\'Checking for valid Tensorflow installation\')\n    # noinspection PyUnresolvedReferences\n    if not check_nvidia_docker():\n        print(error_msg % \'Using Docker but not nvidia-docker runtime\', file=sys.stderr)\n        ret = False\n    else:\n        if not is_install:\n            import h5py  # importing tensorflow later causes seg faults\n        try:\n            # noinspection PyUnresolvedReferences\n            import tensorflow as tf\n        except ImportError:\n            print(error_msg % \'Tensorflow not installed\', file=sys.stderr)\n            ret = False\n        else:\n            min_version = \'1.7\'\n            max_version = \'2.0\'\n            if semvar(tf.__version__) < semvar(min_version):\n                warn_msg = \'Tensorflow %s is less than the minimum \' \\\n                           \'required version (%s)\' \\\n                           % (tf.__version__, min_version)\n                print(error_msg % warn_msg, file=sys.stderr)\n                ret = False\n            elif semvar(tf.__version__) >= semvar(max_version):\n                warn_msg = \'Tensorflow %s is greater or equal to the maximum \' \\\n                           \'required version (%s)\' \\\n                           % (tf.__version__, min_version)\n                print(error_msg % warn_msg, file=sys.stderr)\n                ret = False\n            else:\n                print(\'Tensorflow %s detected - meets min version\'\n                      \' (%s)\' % (tf.__version__, min_version))\n                ret = True\n\n    return ret\n\n\ndef get_available_gpus():\n    from tensorflow.python.client import device_lib\n    local_device_protos = device_lib.list_local_devices()\n    return [x.name for x in local_device_protos if x.device_type == \'GPU\']\n\n\ndef is_docker():\n    path = \'/proc/self/cgroup\'\n    return (\n        os.path.exists(\'/.dockerenv\') or\n        os.path.isfile(path) and any(\'docker\' in line for line in open(path))\n    )\n\n\ndef has_nvidia_docker():\n    return \'NVIDIA_VISIBLE_DEVICES\' in os.environ\n\n\nif __name__ == \'__main__\':\n    if \'--test-get-bindings-version\' in sys.argv:\n        print(get_latest_valid_bindings())\n    else:\n        try:\n            main()\n        except Exception as e:\n            print(\'\\n \'\n                  \'* Install failed, fix issues below and rerun install.py \\n\')\n            raise e\n\n'"
logs.py,0,"b'import os\nimport logging\nfrom logging.handlers import RotatingFileHandler\nimport sys\n\nimport config as c\nfrom util.anonymize import anonymize_user_home\n\nos.makedirs(c.LOG_DIR, exist_ok=True)\nlog_format = logging.Formatter(\n    \'%(asctime)s - %(name)s - %(levelname)s - %(message)s\')\nlog_level = logging.INFO\nall_loggers = {}\nrotators = {}\n\n\ndef get_log(namespace, filename=\'log.txt\'):\n    """"""\n    Use separate filenames for separate processes to avoid rollover\n    errors on Windows\n    https://stackoverflow.com/questions/16712638\n    """"""\n    if all_loggers.get(namespace):\n        return all_loggers[namespace]\n    rotator = get_log_rotator(filename)\n    ret = logging.getLogger(namespace)\n    ret.addFilter(AnonymizeFilter())\n    if ret.parent != ret.root:\n        # Avoid duplicate log messages in multiprocessing scenarios\n        # where module is imported twice\n        print(\'Warning, using parent logger to avoid nested\'\n              \' loggers and duplicate errors messages\')\n        return ret.parent\n    ret.setLevel(log_level)\n    ch = logging.StreamHandler(sys.stdout)\n    ch.setFormatter(log_format)\n    ret.addHandler(ch)\n    ret.addHandler(rotator)\n    ret.propagate = False\n    all_loggers[namespace] = ret\n    return ret\n\n\ndef get_log_rotator(filename):\n    if filename in rotators:\n        return rotators[filename]\n    rotator = RotatingFileHandler(os.path.join(c.LOG_DIR, filename),\n                                  maxBytes=(1048576 * 5), backupCount=7)\n    rotator.setFormatter(log_format)\n    rotators[filename] = rotator\n    return rotator\n\n\ndef set_level(level):\n    global log_level\n    log_level = level\n    for l in all_loggers.values():\n        l.setLevel(level)\n\n\ndef log_manual():\n    test_log_rotator = RotatingFileHandler(os.path.join(c.LOG_DIR, \'test.txt\'), maxBytes=3, backupCount=7)\n    log1 = get_log(\'log1\', rotator=test_log_rotator)\n    log2 = get_log(\'log2\', rotator=test_log_rotator)\n    log1.info(\'asdf\')\n    log2.info(\'zxcv\')\n\n\nclass AnonymizeFilter(logging.Filter):\n    def filter(self, record):\n        anon = anonymize_user_home\n        record.msg = anonymize_user_home(record.msg)\n        record.args = tuple(anon(a) for a in record.args)\n        return True\n'"
main.py,0,"b'from __future__ import (absolute_import, division,\n                        print_function, unicode_literals)\n\nimport json\nimport sys\n\nimport h5py  # Needs to be imported before tensorflow to avoid seg faults\n\nimport glob\nimport logging\nimport os\nimport traceback\n\n# noinspection PyUnresolvedReferences\nimport config.check_bindings\n\nfrom config import camera_config, MOBILENET_V2_IMAGE_SHAPE\nimport config as c\nfrom sim import DrivingStyle, SimArgs\nfrom util.ensure_sim import get_sim_path\nimport sim\nimport logs\nfrom util.args import Args\nfrom util.ensure_sim import ensure_sim\nfrom utils import dbox\n\nlog = logs.get_log(__name__)\n\n\ndef add_standard_args(args:Args):\n    args.add(\n        \'-e\', \'--env-id\', nargs=\'?\', default=\'Deepdrive-v0\',\n        help=\'Select the environment to run\')\n    args.add(\n        \'-r\', \'--record\', action=\'store_true\', default=False,\n        help=\'Records game driving, including recovering from random actions\')\n    args.add(\n        \'--discrete-actions\', action=\'store_true\', default=False,\n        help=\'Use discrete, rather than continuous actions\')\n    args.add(\n        \'--recording-dir\', nargs=\'?\', default=c.RECORDING_DIR,\n        help=\'Where to store and read recorded environment data from\')\n    args.add(\n        \'--render\', action=\'store_true\', default=False,\n        help=\'Show the cameras as seen your agents in Python\')\n    args.add(\n        \'--sync\', action=\'store_true\', default=False,\n        help=\'Use synchronous stepping mode where the simulation advances only \'\n             \'when calling step\')\n    args.add(\n        \'--sim-step-time\', type=float,\n        default=c.DEFAULT_SIM_STEP_TIME,\n        help=\'Time to pause sim in synchronous stepping mode\')\n    args.add(\n        \'--enable-traffic\', action=\'store_true\', default=False,\n        help=\'Enable traffic within the simulator\')\n    args.add(\n        \'--randomize-sun-speed\', action=\'store_true\', default=False,\n        help=\'Whether to randomize the virtual speed of the earth\\\'s orbit \'\n             \'around the sun\')\n    args.add(\n        \'--randomize-view-mode\', action=\'store_true\', default=False,\n        help=\'Whether to randomize view mode on episode reset\')\n    args.add(\n        \'--randomize-shadow-level\', action=\'store_true\', default=False,\n        help=\'Whether to randomize virtual position of Earth around Sun via \'\n             \'month\')\n    args.add(\n        \'--randomize-month\', action=\'store_true\', default=False,\n        help=\'Whether to randomize shadow quality render levels\')\n    args.add(\n        \'--path-follower\', action=\'store_true\', default=False,\n        help=\'Whether to let the in-game path follower drive\')\n    args.add(\n        \'--eval-only\', action=\'store_true\', default=False,\n        help=\'Whether to just run evaluation, i.e. disable gradient updates\', )\n    args.add(\n        \'--driving-style\', nargs=\'?\',\n        default=DrivingStyle.NORMAL.as_string(),\n        help=\'Speed vs comfort prioritization, i.e. \' +\n             \', \'.join([level.name.lower() for level in\n                        DrivingStyle]))\n    args.add(\n        \'--remote\', action=\'store_true\', default=False,\n        help=\'Use API to connect to a remote environment\')\n    args.add(\n        \'-v\', \'--verbose\',\n        help=\'Increase output verbosity\', action=\'store_true\')\n    args.add(\n        \'--camera-rigs\', nargs=\'?\', default=None,\n        help=\'Name of camera rigs to use\')\n    args.add(\n        \'--experiment\', nargs=\'?\', default=None,\n        help=\'Name of your experiment\')\n    args.add(\n        \'--fps\', type=int, default=c.DEFAULT_FPS,\n        help=\'Frames or steps per second\')\n    args.add(\n        \'--ego-mph\', type=float, default=25,\n        help=\'Ego (i.e. main) agent vehicle miles per hour\')\n    args.add(\n        \'--view-mode-period\', type=int, default=None,\n        help=\'Number of steps between view mode switches\')\n    args.add(\n        \'--max-steps\', type=int, default=None,\n        help=\'Max number of steps to run per episode\')\n    args.add(\n        \'--max-episodes\', type=int, default=None,\n        help=\'Maximum number of episodes\')\n    args.add(\n        \'--server\', action=\'store_true\', default=False,\n        help=\'Run as an API server - serializes in pyarrow\', )\n    args.add(\n        \'--json-server\', action=\'store_true\', default=False,\n        help=\'Run as a JSON API server - serializes with JSON\', )\n    args.add(\n        \'--upload-gist\', action=\'store_true\', default=False,\n        help=\'Upload a private gist with driving performance\'\n             \'stats csv files\', )\n    args.add(\n        \'--public\', action=\'store_true\', default=False,\n        help=\'Results will be made public, i.e. artifacts like \'\n             \'https://gist.github.com/deepdrive-results/cce0a164498c17269ce2adea2a88ec95\', )\n\n    args.add(\n        \'--image-resize-dims\', nargs=\'?\',\n        default=json.dumps(MOBILENET_V2_IMAGE_SHAPE),\n        help=\'Resize the image coming from the cameras. This was added as \'\n             \'we trained MNET (224x224) on old AlexNet data (227x227), and\'\n             \'wanted to test using the same transformation.\')\n    args.add(\n        \'--update-sim\', action=\'store_true\', default=False,\n        help=\'Update sim to the latest version\', )\n\n    args.add(\n        \'--scenario\', type=int, default=c.DEFAULT_SCENARIO_INDEX,\n        help=\'Scenario index to run 0-5 are Kevindale scenarios\')\n\n    args.add(\'--map\', nargs=\'?\', default=\'\',\n        help=\'The Unreal Map to load - options: \' +\n             \', \'.join(c.MAP_LOOKUP.keys()))\n\n\ndef add_agent_args(args):\n    args.add_agent_arg(\n        \'--baseline\', action=\'store_true\', default=False,\n        help=\'Runs pretrained alexnet-based imitation learning based agent\')\n    args.add_agent_arg(\n        \'--mnet2-baseline\', action=\'store_true\', default=False,\n        help=\'Runs pretrained mnet2-based imitation learning based agent\')\n    args.add_agent_arg(\n        \'--ppo-baseline\', action=\'store_true\', default=False,\n        help=\'Runs pretrained ppo-based imitation learning based agent\')\n    args.add_agent_arg(\n        \'-t\', \'--train\', action=\'store_true\', default=False,\n        help=\'Trains tensorflow agent on stored driving data\')\n    args.add_agent_arg(\n        \'--hdf5-2-tfrecord\', action=\'store_true\', default=False,\n        help=\'Converts all recorded hdf5 files to a tfrecord dataset\')\n    args.add_agent_arg(\n        \'--use-latest-model\', action=\'store_true\', default=False,\n        help=\'Use most recently trained model\')\n    args.add_agent_arg(\n        \'--jitter-actions\', action=\'store_true\', default=False,\n        help=\'Whether to occasionally perform random actions and record recovery from them\')\n    args.add_agent_arg(\n        \'--overfit\', action=\'store_true\', default=False,\n        help=\'Whether or not to overfit to a small test set during training to sanity check \'\n             \'convergability\')\n    args.add_agent_arg(\n        \'--net-path\', nargs=\'?\', default=None,\n        help=\'Path to the tensorflow checkpoint you want to test drive. \'\n             \'i.e. /home/a/DeepDrive/tensorflow/2018-01-01__11-11-11AM_train/model.ckpt-98331\')\n    args.add_agent_arg(\n        \'--net-type\', nargs=\'?\', default=None,\n        help=\'Your model type - i.e. AlexNet or MobileNetV2\')\n    args.add_agent_arg(\n        \'--resume-train\', nargs=\'?\', default=None,\n        help=\'Name of the tensorflow training session you want \'\n             \'to resume within %s, \'\n             \'i.e. 2018-01-01__11-11-11AM_train\' %\n             c.TENSORFLOW_OUT_DIR)\n    args.add_agent_arg(\n        \'--tf-debug\', action=\'store_true\', default=False,\n        help=\'Run a tf_debug session\')\n    args.add_agent_arg(\n        \'--freeze-pretrained\', action=\'store_true\',\n        default=False,\n        help=\'Freeze pretrained layers during training\')\n    args.add_agent_arg(\n        \'--train-args-collection\', nargs=\'?\', default=None,\n        help=\'Name of the set of training args to use\')\n    args.add_agent_arg(\n        \'--agent\', nargs=\'?\', default=c.DAGGER_MNET2,\n        help=\'Agent type (%s, %s, %s)\' % (c.DAGGER,\n                                          c.DAGGER_MNET2,\n                                          c.BOOTSTRAPPED_PPO2))\n\n\ndef get_args():\n    args = Args()\n    add_standard_args(args)\n    add_agent_args(args)\n    all_args = args.all.parse_args()\n    agent_args = args.agent.parse_known_args()\n    return all_args, agent_args\n\n\ndef main():\n    args, agent_args = get_args()\n    c.MAIN_ARGS = vars(args)  # For documenting runs\n    if args.verbose:\n        logs.set_level(logging.DEBUG)\n\n    if args.update_sim:\n        ensure_sim(update=True)\n        return\n\n    if args.public and not c.PUBLIC:\n        answer = input(\'Please confirm you want to make the results \'\n                       \'of this evaluation public? \')\n        args.public = answer.lower() in [\'y\', \'yes\']\n        if not args.public:\n            print(\'Answer was not ""y"" or ""yes"", not making public\')\n\n    if args.recording_dir.startswith(\'~\'):\n        args.recording_dir = os.path.expanduser(args.recording_dir)\n\n    if args.hdf5_2_tfrecord:\n        from agents.dagger.train import hdf5_to_tfrecord\n        hdf5_to_tfrecord.encode(hdf5_path=args.recording_dir,\n                                experiment=args.experiment)\n        return\n    elif args.server or args.json_server:\n        from deepdrive_api import server\n        sim_args = None\n        log.info(\'Starting Deepdrive server\')\n        if len(sys.argv) > 2:\n            # More than just --server was passed,\n            # so sim will be configured purely on the server side,\n            # vs purely from the client in order to prevent\n            # cheating / clients that change their environment in evals.\n            sim_args = get_sim_args_from_command_args(args)\n        if sim_args is not None:\n            sim_args = sim_args.to_dict()\n        ensure_sim()\n        server.start(sim, json_mode=args.json_server,\n                     sim_path=get_sim_path(), sim_args=sim_args)\n        return\n    else:\n        camera_rigs = get_camera_rigs(args)\n        driving_style = DrivingStyle.from_str(args.driving_style)\n        from install import check_tensorflow_gpu\n\n        if args.path_follower:\n            run_path_follower(args, camera_rigs)\n        elif not check_tensorflow_gpu():\n            log.info(\'Tensorflow not installed, falling back to PID path \'\n                     \'follower agent as mnet2 baseline agent requires \'\n                     \'Tensorflow\')\n            run_path_follower(args, camera_rigs)\n        else:\n            run_tf_based_models(args, camera_rigs, driving_style)\n\n\ndef run_tf_based_models(args, camera_rigs, driving_style):\n    from install import check_tensorflow_gpu\n    if not check_tensorflow_gpu():\n        raise RuntimeError(\'Tensorflow not installed, cannot run or \'\n                           \'trained tensorflow agents\')\n    configure_net_args(args)\n    if args.train or args.agent == c.BOOTSTRAPPED_PPO2:\n        # Training and running are more coupled in RL in our\n        # implementation (and generally), so we\n        # call train_agent even for eval_only.\n        train_agent(args, driving_style)\n    else:\n        run_agent(args, camera_rigs)\n\n\ndef get_camera_rigs(args):\n    if args.camera_rigs:\n        camera_rigs = camera_config.rigs[args.camera_rigs]\n    else:\n        camera_rigs = camera_config.rigs[\'baseline_rigs\']\n    return camera_rigs\n\n\ndef configure_net_args(args):\n    if args.use_latest_model:\n        if args.net_path:\n            raise ValueError(\'--use-latest-model and \'\n                             \'--net-path cannot both be set\')\n        if args.train:\n            args.resume_train = get_latest_model()\n        else:\n            args.net_path = get_latest_model()\n    elif args.net_path:\n        if args.net_path.startswith(\'https://\'):\n            url = str(args.net_path)\n            import utils\n            args.net_path = utils.download_weights(url)\n\n        if args.net_path.startswith(\'~\'):\n            args.net_path = os.path.expanduser(""~"") + args.net_path[1:]\n\n        if not os.path.exists(args.net_path):\n            raise RuntimeError(f\'Net path does not exist: {args.net_path}\')\n        elif os.path.isdir(args.net_path):\n            args.net_path = get_latest_model_from_path(args.net_path)\n\n    from agents.dagger import net\n    if args.net_type is None:\n        args.net_type = c.MOBILENET_V2_NAME\n    if args.mnet2_baseline:\n        args.net_type = c.MOBILENET_V2_NAME\n\n    if args.ppo_baseline:\n        # args.agent / agent_name are use in training, but\n        # training and running are more tightly linked in RL, so we\n        # want to make sure agent is set here even for just running a baseline.\n        args.agent = c.BOOTSTRAPPED_PPO2\n        args.eval_only = True\n\n\ndef run_agent(args, camera_rigs):\n    """"""\n    Here we run an agent alongside an open simulator and either just benchmark\n    it\'s performance, as with agents trained offline (i.e. the current dagger\n    mnet and alexnet agents), or train an online agent (i.e. the PP02 agent).\n\n    :param args Command line args that configure agent and sim\n    :param camera_rigs: A collection of camera configs to cycle through, with\n    one rig used for the duration of an episode\n    """"""\n    from agents.dagger import agent\n    sim_args = get_sim_args_from_command_args(args)\n    agent.run(sim_args,\n              net_path=args.net_path,\n              run_baseline_agent=args.baseline,\n              run_mnet2_baseline_agent=args.mnet2_baseline,\n              run_ppo_baseline_agent=args.ppo_baseline,\n              camera_rigs=camera_rigs,\n              should_jitter_actions=args.jitter_actions,\n              net_name=args.net_type,\n              max_episodes=args.max_episodes,\n              agent_name=args.agent,)\n\n\ndef get_sim_args_from_command_args(args):\n    sim_args = SimArgs(\n        experiment=args.experiment,\n        env_id=args.env_id,\n        should_record=args.record,\n        render=args.render,\n        # cameras will be set in agent\n        fps=args.fps,\n        is_sync=args.sync,\n        driving_style=args.driving_style,\n        is_remote_client=args.remote,\n        recording_dir=args.recording_dir,\n        enable_traffic=args.enable_traffic,\n        view_mode_period=args.view_mode_period,\n        max_steps=args.max_steps,\n        max_episodes=args.max_episodes,\n        eval_only=args.eval_only,\n        upload_gist=args.upload_gist,\n        public=args.public,\n        sim_step_time=args.sim_step_time,\n        randomize_view_mode=args.randomize_view_mode,\n        randomize_sun_speed=args.randomize_sun_speed,\n        randomize_shadow_level=args.randomize_shadow_level,\n        randomize_month=args.randomize_month,\n        image_resize_dims=tuple(json.loads(args.image_resize_dims)),\n        scenario_index=args.scenario,\n        path_follower=args.path_follower,\n        map=args.map,\n    )\n    return sim_args\n\n\ndef run_path_follower(args, camera_rigs):\n    """"""\n    Runs the C++ PID-based path follower agent which uses a reference\n    spline in the center of the lane, and speed annotations on tight turns\n    to drive.\n\n    Or on new maps a behavior tree based agent is used with communication\n    between agents.\n\n    Refer to https://github.com/deepdrive/deepdrive-sim/tree/b21e0a0bf8cec60538425fa41b5fc5ee28142556/Plugins/DeepDrivePlugin/Source/DeepDrivePlugin/Private/Simulation/Agent\n    """"""\n    done = False\n    gym_env = None\n    try:\n        sim_args = get_sim_args_from_command_args(args)\n        if sim_args is not None:\n            sim_args = sim_args.to_dict()\n        else:\n            sim_args = {}\n        cameras = camera_rigs\n        if isinstance(camera_rigs[0], list):\n            cameras = cameras[0]\n        sim_args[\'cameras\'] = cameras\n        gym_env = sim.start(**sim_args)\n        log.info(\'Path follower drive mode\')\n        episode_num = 0\n        info = {}\n        def should_stop(index, step_info):\n            if dbox(step_info).should_close:\n                return True\n            elif args.max_episodes:\n                return index >= args.max_episodes\n            else:\n                return False\n\n        while not should_stop(episode_num, info):\n            episode_num += 1\n            done = False\n            while not done:\n                action = sim.action(has_control=False)\n                obz, reward, done, info = gym_env.step(action)\n                if done:\n                    gym_env.reset()\n    except KeyboardInterrupt:\n        log.info(\'keyboard interrupt detected, closing\')\n    except Exception as e:\n        log.error(\'Error running agent. %s\', e)\n        print(traceback.format_exc())\n    else:\n        log.info(\'Last episode complete, closing\')\n    finally:\n        if gym_env:\n            gym_env.close()\n\n\ndef train_agent(args, driving_style):\n    from agents.dagger.agent import ensure_mnet2_baseline_weights\n    if args.agent == c.DAGGER or args.agent == c.DAGGER_MNET2:\n        train_dagger(args)\n    elif args.agent == c.BOOTSTRAPPED_PPO2:\n        from agents.bootstrap_rl.train import train\n        net_path = args.net_path\n        if not net_path:\n            log.info(\'Bootstrapping from baseline agent\')\n            net_path = ensure_mnet2_baseline_weights(args.net_path)\n        if not args.sync and not args.eval_only:\n            args.sync = True\n            log.warning(\'Detected training RL in async mode which \'\n                        \'can cause unequal time deltas. \'\n                        \'Switching to synchronous mode. \'\n                        \'Use --sync to avoid this.\')\n        sim_args = get_sim_args_from_command_args(args)\n        train.run(args.env_id, resume_dir=args.resume_train,\n                  bootstrap_net_path=net_path, agent_name=args.agent,\n                  render=args.render, camera_rigs=[c.DEFAULT_CAM],\n                  is_sync=args.sync, driving_style=driving_style,\n                  is_remote_client=args.remote, eval_only=args.eval_only,\n                  sim_args=sim_args)\n    else:\n        raise Exception(\'Agent type not recognized\')\n\n\ndef train_dagger(args):\n    """"""\n    Run the first iteration of DAgger where our policy is random.\n    """"""\n    from agents.dagger.train import train\n    train.run(resume_dir=args.resume_train, data_dir=args.recording_dir,\n              agent_name=args.agent,\n              overfit=args.overfit, eval_only=args.eval_only,\n              tf_debug=args.tf_debug,\n              freeze_pretrained=args.freeze_pretrained,\n              train_args_collection_name=args.train_args_collection)\n\n\ndef get_latest_model():\n    # TODO: Get best performing model from n latest\n    return get_latest_model_from_path(\'%s/*\' % c.TENSORFLOW_OUT_DIR)\n\n\ndef get_latest_model_from_path(model_dir):\n    model = max(glob.glob(\n        \'%s/model.ckpt-*.meta\' % model_dir),\n        key=os.path.getmtime)\n    if not model:\n        raise RuntimeError(\'No tensorflow models found in %s\' % model_dir)\n    prefix = model[:-len(\'.meta\')]\n    log.info(\'Latest model is %s\', prefix)\n    return prefix\n\n\nif __name__ == \'__main__\':\n    main()\n'"
tf_utils.py,13,"b""import tensorflow as tf\nimport time\nfrom tensorflow.python.client import timeline\nimport numpy as np\nimport logs\n\nlog = logs.get_log(__name__)\n\n\nIMAGE = tf.placeholder(tf.float64)\nDEPTH = tf.placeholder(tf.float64)\n\n\ndef _image_op(x):\n    y = x ** 0.45  # gamma correction\n    y = tf.clip_by_value(y, 0, 1)\n    y = y * 255.\n    y = tf.cast(y, tf.uint8)\n    return y\n\n\ndef _depth_op(x):\n    x = x ** -(1 / 3.)\n    x = _normalize_op(x)\n    x = _heatmap_op(x)\n    return x\n\n\ndef _normalize_op(x):\n    amax = tf.reduce_max(x)\n    amin = tf.reduce_min(x)\n    arange = amax - amin\n    x = (x - amin) / arange\n    return x\n\n\ndef _heatmap_op(x):\n    red = x\n    green = 1.0 - tf.abs(0.5 - x) * 2.\n    blue = 1. - x\n    y = tf.stack([red, green, blue])\n    y = tf.transpose(y, (1, 2, 0))\n    y = tf.cast(y * 255, tf.uint8)\n    return y\n\n\nimage_op = _image_op(IMAGE)\ndepth_op = _depth_op(DEPTH)\n\n\ndef preprocess_image(image, sess, trace=False):\n    return _run_op(sess, image_op, IMAGE, image, trace, op_name='preprocess_image')\n\n\ndef preprocess_depth(depth, sess, trace=False):\n    return _run_op(sess, depth_op, DEPTH, depth, trace, op_name='preprocess_depth')\n\n\ndef _run_op(sess, op, X, x, trace=False, op_name='tf_op'):\n    start = time.time()\n    if trace:\n        run_options = tf.RunOptions(trace_level=tf.RunOptions.FULL_TRACE)\n        run_metadata = tf.RunMetadata()\n        ret = sess.run(op, feed_dict={X: x}, options=run_options, run_metadata=run_metadata)\n\n        # Create the Timeline object, and write it to a json\n        tl = timeline.Timeline(run_metadata.step_stats)\n        ctf = tl.generate_chrome_trace_format()\n        with open('timeline.json', 'w') as f:\n            f.write(ctf)\n    else:\n        ret = sess.run(op, feed_dict={X: x})\n    end = time.time()\n    log.debug('%r took %rms', op_name, (end - start) * 1000)\n    return ret\n\n\ndef _main():\n    h = w = 227\n    import sys\n    log.basicConfig(level=log.DEBUG, stream=sys.stdout, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n    with tf.Session() as sess:\n        preprocess_image(np.random.rand(h, w, 3), sess)\n        preprocess_image(np.random.rand(h, w, 3), sess)\n        preprocess_image(np.random.rand(h, w, 3), sess)\n        preprocess_depth(np.random.rand(h, w,), sess)\n        preprocess_depth(np.random.rand(h, w,), sess)\n        preprocess_depth(np.random.rand(h, w,), sess)\n\n\nif __name__ == '__main__':\n    _main()\n"""
upgrade.py,0,"b'""""""\nTODO: In main.py - check version of deepdrive against version in Pipfile\n  - if minor version in Pipfile is greater, upgrade sim and extension\n    - Delete sim, run python install.py which will install bumped version of deepdrive extension via the new Pipfile\n    - Missing sim will be detected and correct major_minor version downloaded\n  - else just run install.py which will update any packages via pip install\n""""""'"
utils.py,0,"b'from __future__ import (absolute_import, division,\n                        print_function, unicode_literals)\n\nimport math\nimport re\n\n# TODO: Bootstrap future module to enable Python 2 support of install which depends on this file to do below\n# from future.builtins import (ascii, bytes, chr, dict, filter, hex, input,\n#                              int, map, next, oct, open, pow, range, round,\n#                              str, super, zip)\n\nimport ctypes\nimport platform\nimport shutil\n\nimport glob\nimport inspect\nimport os\nimport sys\nimport threading\nimport time\nimport traceback\nfrom os.path import exists, expanduser, basename\nfrom typing import Tuple\n\nimport numpy as np\n\nimport h5py\nimport requests\nfrom box import Box\n\nimport config as c\nimport logs\nfrom util.anonymize import anonymize_user_home\nfrom util.download import download\nfrom util.ensure_sim import ensure_sim\nfrom util.run_command import run_command\n\nlog = logs.get_log(__name__)\n\n\ndef normalize(a):\n    amax = a.max()\n    amin = a.min()\n    arange = amax - amin\n    a = (a - amin) / arange\n    return a\n\n\ndef preprocess_image(image):\n    start = time.time()\n    image = (image.astype(np.float32, copy=False)\n             ** 0.45  # gamma correct\n             * 255.)\n    image = np.clip(image, a_min=0, a_max=255)\\\n        .astype(\'uint8\', copy=False)\n    end = time.time()\n    log.debug(\'preprocess_capture_image took %rms\', (end - start) * 1000.)\n    return image\n\n\ndef preprocess_depth(depth):\n    depth = depth.astype(\'float64\', copy=False)\n    # x = list(range(depth.size))\n    # y = depth.flatten()\n    # plt.scatter(x, y)\n    # plt.show()\n    depth = depth ** -(1 / 3.)\n    depth = normalize(depth)\n    return depth\n\n\ndef depth_heatmap(depth):\n    red = depth\n    green = 1.0 - np.abs(0.5 - depth) * 2.\n    blue = 1. - depth\n    ret = np.array([red, green, blue])\n    ret = np.transpose(ret, (1, 2, 0))\n    ret = (ret * 255).astype(\'uint8\', copy=False)\n    return ret\n\n\ndef obj2dict(obj, exclude=None):\n    """"""\n    Converts object properties to a dict.\n    This acts as a single level copy, i.e. it\'s NOT recursive.\n\n    @:param obj - The Object to convert\n    @:param exclude - A list of property names to omit from the returned object\n\n    """"""\n    ret = {}\n    exclude = exclude or []\n    for name in dir(obj):\n        if not name.startswith(\'__\') and name not in exclude:\n            value = getattr(obj, name)\n            if not callable(value):\n                ret[name] = value\n    return ret\n\n\ndef save_hdf5(out, filename, background=True):\n    assert_disk_space(os.path.dirname(filename))\n    if \'DEEPDRIVE_NO_THREAD_SAVE\' in os.environ or not background:\n        return save_hdf5_task(out, filename)\n    else:\n        thread = threading.Thread(target=save_hdf5_task, args=(out, filename))\n        thread.start()\n        return thread\n\n\ndef save_hdf5_task(out, filename):\n    os.makedirs(os.path.dirname(filename), exist_ok=True)\n    log.debug(\'Saving to %s\', filename)\n    opts = dict(compression=\'lzf\', fletcher32=True)\n    with h5py.File(filename, \'w\') as f:\n        for i, frame in enumerate(out):\n            frame_grp = f.create_group(\'frame_%s\' % str(i).zfill(10))\n            add_collision_to_hdf5(frame, frame_grp)\n            add_return_to_hdf5(frame, frame_grp)\n            add_world_to_hdf5(frame, frame_grp)\n            add_cams_to_hdf5(frame, frame_grp, opts)\n            del frame[\'cameras\']\n            for k, v in frame.items():\n                frame_grp.attrs[k] = v\n    log.info(\'Saved to %s\', filename)\n\n\ndef add_world_to_hdf5(frame, frame_grp):\n    parent_key = \'world\'\n    world = frame[parent_key]\n    world_grp = frame_grp.create_group(parent_key)\n    for k, v in world.items():\n        if k == \'vehicle_positions\':\n            v = np.array(v)\n        world_grp.attrs[k] = v\n    del frame[parent_key]\n\n\ndef add_cams_to_hdf5(frame, frame_grp, opts):\n    for j, camera in enumerate(frame[\'cameras\']):\n        camera_grp = frame_grp.create_group(\'camera_%s\' % str(j).zfill(5))\n        camera_grp.create_dataset(\'image\', data=camera[\'image\'], **opts)\n        camera_grp.create_dataset(\'depth\', data=camera[\'depth\'], **opts)\n        del camera[\'image_data\']\n        del camera[\'depth_data\']\n        del camera[\'image\']\n        if \'image_raw\' in camera:\n            del camera[\'image_raw\']\n        del camera[\'depth\']\n        for k, v in camera.items():\n            # TODO: Move this to a \'props\' dataset as attrs can only be 64kB\n            camera_grp.attrs[k] = v\n\n\ndef add_return_to_hdf5(frame, frame_grp):\n    from sim.return_aggregator import EpisodeReturn\n    episode_return = frame[\'episode_return\']\n    return_grp = frame_grp.create_group(\'episode_return\')\n    defaults = obj2dict(EpisodeReturn)\n    prop_names = defaults.keys()\n    for k in prop_names:\n        if \'sampler\' not in k.lower():\n            return_grp.attrs[k] = episode_return.get(k, defaults[k])\n    del frame[\'episode_return\']\n\n\ndef add_collision_to_hdf5(frame, frame_grp):\n    from box import Box\n    clsn_grp = frame_grp.create_group(\'last_collision\')\n    clsn = Box(frame[\'last_collision\'], box_it_up=True)\n    clsn_grp.attrs[\'collidee_velocity\'] = tuple(clsn.collidee_velocity)\n    collidee_location = getattr(clsn, \'collidee_location\', None)\n    clsn_grp.attrs[\'collidee_location\'] = \\\n        collidee_location if (clsn.time_utc and collidee_location) else \'\'\n    clsn_grp.attrs[\'collision_normal\'] = tuple(clsn.collision_normal)\n    clsn_grp.attrs[\'time_since_last_collision\'] = clsn.time_since_last_collision\n    clsn_grp.attrs[\'time_stamp\'] = clsn.time_stamp\n    clsn_grp.attrs[\'time_utc\'] = clsn.time_utc\n    del frame[\'last_collision\']\n\n\ndef read_hdf5(filename, save_png_dir=None, overfit=False, save_prefix=\'\'):\n    ret = []\n    with h5py.File(filename, \'r\') as file:\n        for i, frame_name in enumerate(file):\n            out_frame = read_frame(file, frame_name, i, save_png_dir,\n                                   save_prefix)\n            if out_frame is None:\n                log.error(\'Could not read frame, skipping\')\n            else:\n                ret.append(out_frame)\n                if overfit:\n                    log.info(\'overfitting to %r, image# %d\', filename, i)\n                    if i == 1:\n                        break\n    return ret\n\n\ndef read_frame(file, frame_name, frame_index, save_png_dir, save_prefix=\'\'):\n    try:\n        frame = file[frame_name]\n        out_frame = dict(frame.attrs)\n        out_cameras = []\n        for dataset_name in frame:\n            if dataset_name.startswith(\'camera_\'):\n                read_camera(dataset_name, frame, frame_index,\n                            out_cameras, save_png_dir, save_prefix)\n            elif dataset_name == \'last_collision\':\n                out_frame[\'last_collision\'] = dict(frame[dataset_name].attrs)\n        out_frame[\'cameras\'] = out_cameras\n    except Exception as e:\n        traceback.print_stack()\n        log.error(\'Exception reading frame %s\', str(e))\n        out_frame = None\n    return out_frame\n\n\ndef read_camera(dataset_name, frame, frame_index, out_cameras, save_png_dir,\n                save_prefix=\'\'):\n    camera = frame[dataset_name]\n    out_camera = dict(camera.attrs)\n    out_camera[\'image\'] = camera[\'image\'][()]\n    out_camera[\'depth\'] = camera[\'depth\'][()]\n    out_cameras.append(out_camera)\n    if save_png_dir is not None:\n        if not os.path.exists(save_png_dir):\n            os.makedirs(save_png_dir)\n        save_camera(out_camera[\'image\'], out_camera[\'depth\'],\n                    save_dir=save_png_dir, name=save_prefix + str(frame_index)\n                    .zfill(c.HDF5_FRAME_ZFILL))\n\n\ndef save_camera(image, depth, save_dir, name):\n    from scipy.misc import imsave\n    im_path = os.path.join(save_dir, \'i_\' + name + \'.png\')\n    dp_path = os.path.join(save_dir, \'z_\' + name + \'.png\')\n    imsave(im_path, image)\n    imsave(dp_path, depth)\n    log.debug(\'saved image and depth to %s and %s\', im_path, dp_path)\n\n\ndef show_camera(image, depth):\n    from scipy.misc import toimage\n    toimage(image).show()\n    toimage(depth).show()\n    input(\'Enter any key to continue\')\n\n\ndef hdf5_to_mp4(fps=c.DEFAULT_FPS, png_dir=None, combine_all=False, sess_dir=None):\n    if png_dir is None:\n        png_dir = save_hdf5_recordings_to_png(combine_all, sess_dir)\n    try:\n        file_path = pngs_to_mp4(combine_all, fps, png_dir)\n    finally:\n        guarded_rmtree(png_dir)\n    return file_path\n\n\ndef pngs_to_mp4(combine_all, fps, png_dir):\n    # TODO: Add FPS, frame number, run id, date str,\n    #  g-forces, episode #, hdf5 #, etc... to this\n    #  and rendered views for human interprettability\n    log.info(\'Saved png\\\'s to \' + png_dir)\n    file_path = None\n    import distutils.spawn\n    ffmpeg_path = distutils.spawn.find_executable(\'ffmpeg\')\n    if ffmpeg_path is None:\n        log.error(\'Could not find ffmpeg. Skipping hdf5=>mp4 conversion\')\n    else:\n        zfill_total = c.HDF5_DIR_ZFILL + c.HDF5_FRAME_ZFILL\n        pix_fmt = \'yuv420p\'  # The pix_fmt does not define resolution (i.e. this is totally different than 480p)\n        title = \'deepdrive\'\n        file_dir = c.RESULTS_DIR\n        if not combine_all:\n            title += \'_\' + c.DATE_STR\n        file_path = os.path.join(file_dir, \'%s.mp4\' % title)\n        ffmpeg_cmd = (\'ffmpeg\'\n                      \' -y \'\n                      \' -r {fps}\'\n                      \' -f image2\'\n                      \' -i {temp_png_dir}/i_hdf5_%0{zfill_total}d.png\'\n                      \' -vcodec libx264\'\n                      \' -crf 25\'\n                      \' -pix_fmt {pix_fmt}\'\n                      \' -vf ""pad=ceil(iw/2)*2:ceil(ih/2)*2""\'\n                      \' {file_path}\'.format(fps=fps, pix_fmt=pix_fmt,\n                                            zfill_total=zfill_total,\n                                            file_path=file_path,\n                                            temp_png_dir=png_dir))\n        log.info(\'PNG=>MP4: \' + ffmpeg_cmd)\n        ffmpeg_result = os.system(ffmpeg_cmd)\n        if ffmpeg_result == 0:\n            log.info(\'Wrote mp4 to: \' + anonymize_user_home(file_path))\n        else:\n            file_path = None\n    return file_path\n\n\ndef upload_to_gist(name: str, file_paths: list, public: bool):\n    files = \' \'.join(\'""%s""\' % f for f in file_paths)\n    gist_env = os.environ.copy()\n    gist_env[\'YOU_GET_MY_JIST\'] = requests.get(c.YOU_GET_MY_JIST_URL).text.strip()\n    if os.path.dirname(sys.executable) not in os.environ[\'PATH\']:\n        gist_env[\'PATH\'] = os.path.dirname(sys.executable) + \':\' + gist_env[\'PATH\']\n    opts = \'--public\' if public else \'\'\n    cmd = \'gist {opts} create {gist_name} {files}\'\n    cmd = cmd.format(gist_name=name, files=files, opts=opts)\n    output, ret_code = run_command(cmd, env=gist_env, verbose=True)\n    if ret_code != 0:\n        log.warn(\'Could not upload gist. \\n%s\' % (output,))\n    url = output if ret_code == 0 else None\n    return url\n\n\ndef in_home(name):\n    p = os.path\n    return p.exists(p.join(p.expanduser(\'~\'), name))\n\n\ndef upload_to_youtube(file_path):\n    youtube_creds_name = \'.youtube-upload-credentials.json\'\n    client_secrets_name = \'.client_secrets.json\'\n    youtube_creds_exists = in_home(youtube_creds_name)\n    client_secrets_exists = in_home(client_secrets_name)\n    if not youtube_creds_exists or not client_secrets_exists:\n        log.error(\'Need %s and %s in your home directory to upload to YouTube.\',\n                  youtube_creds_name, client_secrets_name)\n        return False\n\n    # python_path = os.environ[\'PYTHONPATH\']\n    # youtube_upload_dir = os.path.join(c.ROOT_DIR, \'vendor\', \'youtube_upload\')\n    # os.environ[\'PYTHONPATH\'] = \'%s:%s\' % (youtube_upload_dir, python_path)\n    import youtube_upload.main\n    _, options, _ = youtube_upload.main.get_options([])\n    options._update_careful(dict(\n        default_box=True,\n        title=basename(file_path), privacy=\'unlisted\', client_secrets=\'\',\n        credentials_file=\'\', auth_browser=None,\n        description=\'Deepdrive results for %s\' % c.MAIN_ARGS))\n\n    youtube = youtube_upload.main.get_youtube_handler(options)\n    video_id = youtube_upload.main.upload_youtube_video(youtube, options,\n                                                        file_path, 1, 0)\n    # TODO: Put link to s3 artifacts in description [hdf5, csv, diff,\n    #  eventually ue-recording]\n    # cmd = \'%s %s --title=test --privacy=unlisted %s\' % (\n    #     sys.executable,\n    #     os.path.join(youtube_upload_dir, \'bin\', \'youtube_upload\'),\n    #     file_path\n    # )\n    # os.environ[\'PYTHONPATH\'] = python_path\n\n\n    # TODO: Mount client_secret.json and credentials into a container somehow\n    # PYTHONPATH=. python vendor/youtube_upload/bin/youtube_upload --title=test --privacy=unlisted test.mp4\n    # TODO: Remove temp_dir if TEMP\n\n    return video_id\n\n\ndef save_hdf5_recordings_to_png(combine_all=False, sess_dir=None):\n    if combine_all:\n        hdf5_filenames = sorted(glob.glob(c.RECORDING_DIR + \'/**/*.hdf5\',\n                                          recursive=True))\n    else:\n        sess_dir = sess_dir or c.HDF5_SESSION_DIR\n        hdf5_filenames = sorted(glob.glob(sess_dir + \'/*.hdf5\', recursive=True))\n    save_dir = os.path.join(c.RECORDING_DIR, \'pngs\', c.DATE_STR)\n    os.makedirs(save_dir)\n    for i, f in enumerate(hdf5_filenames):\n        try:\n            read_hdf5(f,\n                      save_png_dir=save_dir,\n                      save_prefix=\'hdf5_%s\' % str(i).zfill(c.HDF5_DIR_ZFILL))\n        except OSError as e:\n            log.error(e)\n    return save_dir\n\n\ndef save_random_hdf5_to_png(recording_dir=c.RECORDING_DIR):\n    random_file = np.random.choice(glob.glob(recording_dir + \'/*/*.hdf5\'))\n    if not random_file:\n        raise RuntimeError(\'No hdf5 files found\')\n    else:\n        p = os.path\n        save_png_dir = os.path.join(p.join(recording_dir, \'random_hdf5_view\'),\n                                    p.basename(p.dirname(random_file)),\n                                    p.basename(random_file)[:-5])\n        log.info(\'Saving random files to \' + save_png_dir)\n        os.makedirs(save_png_dir, exist_ok=True)\n        read_hdf5(p.join(recording_dir, random_file),\n                  save_png_dir=save_png_dir)\n\n\ndef read_hdf5_manual(recording_dir=c.RECORDING_DIR):\n    save_png_dir = os.path.join(recording_dir, \'test_view\')\n    os.makedirs(save_png_dir, exist_ok=True)\n    read_hdf5(os.path.join(recording_dir, \'2018-01-18__05-14-48PM\',\n                           \'0000000001.hdf5\'), save_png_dir=save_png_dir)\n\n\ndef is_debugging():\n    for frame in inspect.stack():\n        if frame[1].endswith(""pydevd.py""):\n            return True\n    return False\n\n\ndef download_weights(url):\n    folder = url.split(\'/\')[-1].replace(\'.zip\', \'\')\n    dest = os.path.join(c.WEIGHTS_DIR, folder)\n    if not glob.glob(dest + \'/*\'):\n        log.info(\'Downloading weights %s\', folder)\n        download(url, dest)\n    else:\n        log.info(\'Found cached weights at %s\', dest)\n    return dest\n\n\ndef is_docker():\n    path = \'/proc/self/cgroup\'\n    return (\n        os.path.exists(\'/.dockerenv\') or\n        os.path.isfile(path) and any(\'docker\' in line for line in open(path))\n    )\n\n\ndef get_free_space_mb(filename):\n    """"""Return folder/drive free space (in megabytes).""""""\n    if platform.system() == \'Windows\':\n        drive, _path = os.path.splitdrive(filename)\n        free_bytes = ctypes.c_ulonglong(0)\n        ctypes.windll.kernel32.GetDiskFreeSpaceExW(\n            ctypes.c_wchar_p(drive), None, None, ctypes.pointer(free_bytes))\n        return free_bytes.value / 1024 / 1024\n    else:\n        path = filename\n        while not os.path.exists(path):\n            if not path or path == \'/\':\n                raise ValueError(\'Drive does not exist for filename %s\' % filename)\n            path = os.path.dirname(path)\n        st = os.statvfs(path)\n        return st.f_bavail * st.f_frsize / 1024 / 1024\n\n\ndef remotable(f):\n    def extract_args(*args, **kwargs):\n        return f((args, kwargs), *args, **kwargs)\n\n    return extract_args\n\n\ndef assert_disk_space(filename, mb=2000):\n    try:\n        if get_free_space_mb(filename) < mb:\n            raise Exception(\'Less than %dMB left on device, aborting\'\n                            \' save of %s\' % (mb, filename))\n    except Exception as e:\n        log.error(\'Could not get free space on the drive containing %s\' %\n                  filename)\n        raise e\n\n\ndef resize_images(input_image_shape, images, always=False):\n    import scipy.misc\n    for img_idx, img in enumerate(images):\n        img = images[img_idx]\n        if img.shape != input_image_shape or always:\n            # Interesting bug here. Since resize converts mean subtracted\n            # floats (~-120 to ~130) to 0-255 uint8,\n            # but we don\'t always resize since randomize_cameras does nothing\n            # to the size 5% of the time.\n            # This actually worked surprisingly well. Need to test whether\n            # this bug actually improves things or not.\n            log.debug(\'invalid image shape %s - resizing\', str(img.shape))\n            images[img_idx] = scipy.misc.imresize(img, (input_image_shape[0],\n                                                        input_image_shape[1]))\n    return images\n\n\ndef kill_process(process_to_kill):\n    try:\n        process_to_kill.terminate()\n        time.sleep(0.2)\n        i = 0\n        while process_to_kill and process_to_kill.poll() is None:\n            log.info(\'Waiting for process to die\')\n            time.sleep(0.1 * 2 ** i)\n            if i > 4:\n                # Die!\n                log.warn(\'Forcefully killing process\')\n                process_to_kill.kill()\n                return False\n            i += 1\n        return True\n\n    except Exception as e:\n        log.error(\'Error closing process\', str(e))\n        return False\n\n\ndef get_valid_filename(s):\n    s = str(s).strip().replace(\' \', \'_\')\n    return re.sub(r\'(?u)[^-\\w.]\', \'\', s)\n\n\ndef copy_dir_clean(src, dest):\n    if exists(dest):\n        log.info(\'Removing %s\', dest)\n        guarded_rmtree(dest)\n    log.info(\'Copying files to %s\', dest)\n    shutil.copytree(src, dest)\n\n\ndef guarded_rmtree(dest, allow_small_paths=False):\n    msg = \'Not letting you delete %s\' % dest\n    if dest in [\'/\', \'\\\\\', \'/root\', \'~\', expanduser(\'~\')]:\n        raise RuntimeError(msg)\n    elif len(dest.split(os.path.sep)) <= 2:\n        raise RuntimeError(msg)\n    elif in_home(dest) and len(dest) < 10 and not allow_small_paths:\n        raise RuntimeError(msg)\n    elif not in_home(dest) and len(dest) < 5 and not allow_small_paths:\n        raise RuntimeError(msg)\n    else:\n        return shutil.rmtree(dest)\n\n\ndef sizeof_fmt(num, suffix=\'B\'):\n    for unit in [\'\',\'Ki\',\'Mi\',\'Gi\',\'Ti\',\'Pi\',\'Ei\',\'Zi\']:\n        if abs(num) < 1024.0:\n            return ""%3.1f%s%s"" % (num, unit, suffix)\n        num /= 1024.0\n    return ""%.1f%s%s"" % (num, \'Yi\', suffix)\n\n\nclass timer:\n    def __init__(self, msg, fmt=""%0.3g""):\n        self.msg = msg\n        self.fmt = fmt\n\n    def __enter__(self):\n        self.start = time.process_time()\n        return self\n\n    def __exit__(self, *args):\n        t = time.process_time() - self.start\n        log.debug((""%s : "" + self.fmt + "" seconds"") % (self.msg, t))\n        self.time = t\n\n\ndef nearest_neighbor(me: np.array, them: np.array) -> Tuple[float, int]:\n    """"""\n    :param me: start point\n    :param them: other points\n    :return:\n    """"""\n    import scipy.spatial\n    if len(them) == 0:\n        distance, index = math.inf, -1\n    elif len(them) > 27:\n        # 27 derived from https://gist.github.com/crizCraig/fd3d04e28defa5d5cfc7f37757f81a26\n        with timer(\'kd tree total\'):\n            kd_tree = scipy.spatial.cKDTree(them)\n            distance, index = kd_tree.query(me)\n    else:\n        with timer(\'brute force\'):\n            min_dist = math.inf\n            min_index = -1\n            for index, point in enumerate(them):\n                dist = scipy.spatial.distance.cdist(\n                    np.array([point]), np.array([me]))[0][0]\n                if dist < min_dist:\n                    min_dist = dist\n                    min_index = index\n            distance = min_dist\n            index = min_index\n    return distance, index\n\n\ndef dbox(obj=None, **kwargs):\n    if kwargs:\n        obj = dict(kwargs)\n    else:\n        obj = obj or {}\n    return Box(obj, default_box=True)\n\n\ndef main():\n    # download(\'https://d1y4edi1yk5yok.cloudfront.net/sim/asdf.zip\', r\'C:\\Users\\a\\src\\beta\\deepdrive-agents-beta\\asdf\')\n    # read_hdf5_manual()\n    # ensure_sim()\n    # save_random_hdf5_to_png()\n    # assert_disk_space(r\'C:\\Users\\a\\DeepDrive\\recordings\\2018-11-03__12-29-33PM\\0000000143.hdf5\')\n    # assert_disk_space(\'/media/a/data-ext4/deepdrive-data/v2.1/asdf.hd5f\')\n    # print(get_sim_url())\n    # print(save_recordings_to_png_and_mp4(png_dir=\'/tmp/tmp30zl8ouq\'))\n    # print(save_hdf5_recordings_to_png())\n    # print(upload_to_gist(\'asdf\', [\'/home/c2/src/deepdrive/results/2018-05-30__02-40-01PM.csv\', \'/home/c2/src/deepdrive/results/2019-03-14__06-08-38PM.diff\']))\n    # log.info(\'testing %s\', os.path.expanduser(\'~\'))\n    # import traceback\n    #\n    # traceback.print_stack(file=sys.stdout)\n    # log.info(\'testing %d\', 1234)\n    # ensure_sim()\n    import scipy.spatial\n\n    # vehicle_positions = [[-24164.44140625, 29898.431640625, 19338.546875],\n    #                      [-21652.052734375, 32871.953125, 19180.44921875],\n    #                      [-18885.00390625, 35396.0, 19488.966796875],\n    #                      [-15975.9833984375, 37886.96875, 19873.97265625],\n    #                      [-6273.73681640625, 38792.83984375, 21070.3828125]]\n\n    vehicle_positions = np.random.normal(size=(27, 3))\n\n    with timer(\'kd tree total\'):\n        with timer(\'kd tree build\'):\n            kd_tree = scipy.spatial.cKDTree(vehicle_positions)\n\n        ego_position = [8000.45654297, 27470.125, 24762.65039062]\n\n        with timer(\'kd tree query\'):\n            distance, index = kd_tree.query(ego_position)\n\n    with timer(\'brute force\'):\n        min_dist = None\n        min_index = -1\n        for index, point in enumerate(vehicle_positions):\n            dist = scipy.spatial.distance.cdist(\n                np.array([point]), np.array([ego_position]))\n            if min_dist is None or dist < min_dist:\n                min_dist = dist\n                min_index = index\n        distance = min_dist\n        index = min_index\n\n\n    pass\n\n\nif __name__ == \'__main__\':\n    main()\n\n'"
.ci/build_botleague_containers.py,0,"b'import os\nimport sys\nfrom glob import glob\nfrom os.path import join\n\nDIR = os.path.dirname(os.path.realpath(__file__))\nROOT = os.path.dirname(DIR)\n\n\ndef main():\n    bot_dirs = glob(f\'{join(ROOT, ""botleague"")}/bots/*\')\n    problem_dirs = glob(f\'{join(ROOT, ""botleague"")}/problems/*\')\n\n    # Get names of docker files, build them\n    for pdir in problem_dirs + bot_dirs:\n        exit_code = os.system(f\'cd {pdir} && make && make push\')\n        if exit_code != 0:\n            raise RuntimeError(f\'Error building {pdir} container, check above\')\n\n\nif __name__ == \'__main__\':\n    main()\n'"
.ci/cloud_ci.py,0,"b'import json\nimport os\nimport sys\nfrom typing import List, Tuple\n\nimport time\nfrom datetime import datetime\nfrom os.path import dirname, realpath, join\n\nfrom botleague_helpers.ci import build_and_run_botleague_ci, run_botleague_ci, \\\n    dbox\nfrom box import Box, BoxList\nfrom loguru import logger as log\n\nfrom problem_constants.constants import SUPPORTED_PROBLEMS\n\nimport get_tag_build_id\n\n# from logs import log\n\n""""""\nHere we run the build and integration tests on our own infrastructure so that\nwe can have full control over how the build is run, but still keep track builds\nin a standard way with a nice, free, hosted, UI in Circle / Travis.\n""""""\n\nDIR = dirname(realpath(__file__))\n\n@log.catch(reraise=True)\ndef main():\n    commit = os.environ[\'CIRCLE_SHA1\']\n    branch = os.environ[\'CIRCLE_BRANCH\']\n    # Circle builds / pushes the candidate deepdrive and botleague containers\n    run_botleague_ci_for_deepdrive_build(branch, commit)\n\ndef run_botleague_ci_for_deepdrive_build(branch, commit):\n\n    def set_version(problem_def, version):\n        # Deepdrive sim sets the problem version, which is appropriate since\n        # it determines most of the differences between versions.\n        date_str = datetime.utcnow().strftime(\'%Y-%m-%d_%I-%M-%S%p\')\n        problem_def.rerun = f\'deepdrive-build-{date_str}\'\n\n    pr_message = \'Auto generated commit for deepdrive-build CI\'\n    container_postfix = get_tag_build_id.main()\n    passed_ci = run_botleague_ci(\n        branch=branch,\n        version=commit,\n        pr_message=pr_message,\n        set_version_fn=set_version,\n        supported_problems=SUPPORTED_PROBLEMS,\n        container_postfix=container_postfix)\n    if not passed_ci:\n        raise RuntimeError(\'Failed Botleague CI\')\n\nif __name__ == \'__main__\':\n    main()\n'"
.ci/get_tag_build_id.py,0,"b'import os\n\n\ndef main():\n    if \'CIRCLE_BUILD_NUM\' in os.environ:\n        return f\'_{os.environ[""CIRCLE_BUILD_NUM""]}\'\n    else:\n        return \'_local_build\'\n\n\nif __name__ == \'__main__\':\n    print(main())\n'"
agents/__init__.py,0,b''
agents/common.py,0,"b'from __future__ import (absolute_import, division,\n                        print_function, unicode_literals)\n\nimport glob\nimport os\n\nfrom future.builtins import (ascii, bytes, chr, dict, filter, hex, input,\n                             int, map, next, oct, open, pow, range, round,\n                             str, super, zip)\n\nimport sys\n\nimport logs\nimport config as c\n\n\nlog = logs.get_log(__name__)\n\n\ndef get_throttle(actual_speed, target_speed):\n    # TODO: Use a PID here\n    desired_throttle = abs(target_speed / max(actual_speed, 1e-3))\n    desired_throttle = min(max(desired_throttle, 0.), 1.)\n    return desired_throttle\n'"
config/__init__.py,0,b'from config.directories import *\nfrom config.version import *\nfrom config.runtime import *\n'
config/camera_config.py,0,"b""from __future__ import (absolute_import, division,\n                        print_function, unicode_literals)\n\n# TODO: Bootstrap future module to enable Python 2 support of install which depends on this file to do below\n# from future.builtins import (ascii, bytes, chr, dict, filter, hex, input,\n#                              int, map, next, oct, open, pow, range, round,\n#                              str, super, zip)\n\n\nfrom copy import deepcopy\n\nimport logs\nimport config as c\nfrom sim.view_mode import ViewMode\n\nlog = logs.get_log(__name__)\n\n# Rigs are two dimensional arrays where...\n# cameras in the first dimension are rotated through at the end of the episode during recording and...\n# cameras in the second dimension create multiple simultaneously rendering views from the vehicle.\nrigs = {\n    'default_rig': [[c.DEFAULT_CAM]],\n    'default_rig_1080p': [[c.DEFAULT_CAM_1080p]],\n    'baseline_rigs': [\n        [c.DEFAULT_CAM],\n        [dict(name='forward cam 90 FOV', field_of_view=90, capture_width=340, capture_height=227,\n              relative_position=[150, 1.0, 200],\n              relative_rotation=[0.0, 0.0, 0.0])],\n        [dict(name='semi-truck tall cam 110 FOV', field_of_view=110, capture_width=340, capture_height=227,\n              relative_position=[150, 1.0, 400],\n              relative_rotation=[0.0, -15.0, 0.0])],\n    ],\n    'three_cam_rig': [[\n        dict(name='forward cam', field_of_view=60, capture_width=512, capture_height=289,\n             relative_position=[150, 1.0, 200],\n             relative_rotation=[0.0, 0.0, 0.0]),\n        dict(name='left cam', field_of_view=60, capture_width=512, capture_height=289,\n             relative_position=[150, -150., 200],\n             relative_rotation=[0.0, 0.0, 0.0]),\n        dict(name='right cam', field_of_view=60, capture_width=512, capture_height=289,\n             relative_position=[150, 150., 200],\n             relative_rotation=[0.0, 0.0, 0.0])\n    ]]\n}\n\nDEFAULT_BASE_COLOR_CAM = deepcopy(c.DEFAULT_CAM)\nDEFAULT_BASE_COLOR_CAM['view_mode'] = ViewMode.BASE_COLOR.value\nrigs['default_base_color_rig'] = [[DEFAULT_BASE_COLOR_CAM]]\n"""
config/directories.py,0,b'from util.get_directories import *\n'
config/runtime.py,0,"b""from __future__ import (absolute_import, division,\n                        print_function, unicode_literals)\n\n# TODO: Bootstrap future module to enable Python 2 support of install\n#  which depends on this file to do below\n# from future.builtins import (ascii, bytes, chr, dict, filter, hex, input,\n#                              int, map, next, oct, open, pow, range, round,\n#                              str, super, zip)\n\nimport random\nimport os\nimport sys\nfrom glob import glob\n\nfrom config.directories import *\n\nimport numpy as np\n\ntry:\n    from gym.utils import seeding\n    # Seeded random number generator for reproducibility\n    RNG_SEED = 0\n    rng = seeding.np_random(RNG_SEED)[0]\nexcept Exception as e:\n    # noinspection PyUnresolvedReferences\n    import __main__\n    if getattr(__main__, '__file__', None) != 'install.py':\n        raise e\n    else:\n        print('Skipping rng seed - not needed for install')\n\n\nimport config.version\n\n# General\nCONTROL_NAMES = ['spin', 'direction', 'speed', 'speed_change', 'steering',\n                 'throttle']\n\n# Net\nNUM_TARGETS = len(CONTROL_NAMES)\n\n# Normalization\nSPIN_THRESHOLD = 1.0\nSPEED_NORMALIZATION_FACTOR = 2000.\nSPIN_NORMALIZATION_FACTOR = 10.\nMEAN_PIXEL = np.array([104., 117., 123.], np.float32)\n\n# HDF5\nFRAMES_PER_HDF5_FILE = int(os.environ.get('FRAMES_PER_HDF5_FILE', 1000))\nNUM_TRAIN_FRAMES_TO_QUEUE = 6000\nNUM_TRAIN_FILES_TO_QUEUE = NUM_TRAIN_FRAMES_TO_QUEUE // FRAMES_PER_HDF5_FILE\nHDF5_DIR_ZFILL = 7\nHDF5_FRAME_ZFILL = 10\n\n# OS \nIS_LINUX = sys.platform == 'linux' or sys.platform == 'linux2'\nIS_MAC = sys.platform == 'darwin'\nIS_UNIX = IS_LINUX or IS_MAC or 'bsd' in sys.platform.lower()\nIS_WINDOWS = sys.platform == 'win32'\nif IS_WINDOWS:\n    OS_NAME = 'windows'\nelif IS_LINUX:\n    OS_NAME = 'linux'\nelse:\n    raise RuntimeError('Unexpected OS')\n\n# AGENTS\nDAGGER = 'dagger'\nDAGGER_MNET2 = 'dagger_mobilenet_v2'\nBOOTSTRAPPED_PPO2 = 'bootstrapped_ppo2'\n\n\n# Weights\nALEXNET_BASELINE_WEIGHTS_DIR = os.path.join(WEIGHTS_DIR,\n                                            'baseline_agent_weights')\nALEXNET_BASELINE_WEIGHTS_VERSION = 'model.ckpt-143361'\nALEXNET_PRETRAINED_NAME = 'bvlc_alexnet.ckpt'\nALEXNET_PRETRAINED_PATH = os.path.join(WEIGHTS_DIR, ALEXNET_PRETRAINED_NAME)\n\nMNET2_BASELINE_WEIGHTS_DIR = os.path.join(WEIGHTS_DIR, 'mnet2_baseline_weights')\nMNET2_BASELINE_WEIGHTS_VERSION = 'model.ckpt-49147'\nMNET2_PRETRAINED_NAME = 'mobilenet_v2_1.0_224_checkpoint'\nMNET2_PRETRAINED_PATH = os.path.join(WEIGHTS_DIR, MNET2_PRETRAINED_NAME,\n                                     'mobilenet_v2_1.0_224.ckpt')\n\nPPO_BASELINE_WEIGHTS_DIR = os.path.join(WEIGHTS_DIR,\n                                        'ppo_baseline_agent_weights')\nPPO_BASELINE_WEIGHTS_VERSION = '03125'\n\n# Urls\nAWS_BUCKET = 'deepdrive'\nGCP_BUCKET = 'deepdriveio'\nAWS_BUCKET_URL = 'https://s3-us-west-1.amazonaws.com/' + AWS_BUCKET\nGCP_BUCKET_URL = 'https://storage.googleapis.com/' + GCP_BUCKET  # https://storage.googleapis.com/deepdriveio/filename.ext\nBASE_WEIGHTS_URL = AWS_BUCKET_URL + '/weights'\nALEXNET_BASELINE_WEIGHTS_URL = BASE_WEIGHTS_URL + '/baseline_agent_weights.zip'\nALEXNET_PRETRAINED_URL = '%s/%s.zip' % (BASE_WEIGHTS_URL, ALEXNET_PRETRAINED_NAME)\nMNET2_PRETRAINED_URL = '%s/%s.zip' % (BASE_WEIGHTS_URL, MNET2_PRETRAINED_NAME)\nMNET2_BASELINE_WEIGHTS_URL = BASE_WEIGHTS_URL + '/mnet2_baseline_weights.zip'\nPPO_BASELINE_WEIGHTS_URL = BASE_WEIGHTS_URL + '/ppo_baseline_agent_weights.zip'\nSIM_PREFIX = 'deepdrive-sim-' + OS_NAME\nYOU_GET_MY_JIST_URL = AWS_BUCKET_URL + '/yougetmyjist.json'\nSIM_URL = os.environ.get('SIM_URL')\nDEFAULT_SCENARIO_INDEX = -1\nKEVINDALE_BARE_MAP_PARAM = '/DeepDriveKevindalePlugin/DeepDrive/Maps/DeepDriveSim_Kevindale_Bare'\nKEVINDALE_FULL_MAP_PARAM = '/DeepDriveKevindalePlugin/DeepDrive/Maps/DeepDriveSim_Kevindale_Full'\nJAMESTOWN_MAP_PARAM = '/DeepDriveCityPlugin/DeepDrive/Maps/DeepDriveSim_CityMapTraffic_Demo'\nCANYONS_MAP_NAME = ''\nKEVINDALE_MAP_NAME = 'kevindale'\nKEVINDALE_BARE_MAP_NAME = 'kevindale_bare'\nJAMESTOWN_MAP_NAME = 'jamestown'\n\nMAP_LOOKUP = {\n    CANYONS_MAP_NAME:'',\n    KEVINDALE_MAP_NAME:KEVINDALE_FULL_MAP_PARAM,\n    KEVINDALE_BARE_MAP_NAME:KEVINDALE_BARE_MAP_PARAM,\n    JAMESTOWN_MAP_NAME:JAMESTOWN_MAP_PARAM\n}\n\nEMPTY_UEPY_OBZ = dict(success=True, result=dict(vehicle_positions=[]))\n\nif 'DEEPDRIVE_SIM_START_COMMAND' in os.environ:\n    # Can do something like\n    # `<your-unreal-path>\\Engine\\Binaries\\Win32\\UE4Editor.exe <your-deepdrive-sim-path>\\DeepDrive.uproject -game ResX=640 ResY=480`\n    SIM_START_COMMAND = os.environ['DEEPDRIVE_SIM_START_COMMAND']\nelse:\n    SIM_START_COMMAND = None\n\n\nREUSE_OPEN_SIM = 'DEEPDRIVE_REUSE_OPEN_SIM' in os.environ\n\nDEFAULT_CAM = dict(\n    name='forward cam 227x227 60 FOV',\n    field_of_view=60,\n    capture_width=227,\n    capture_height=227,\n    relative_position=[150, 1.0, 200],\n    relative_rotation=[0.0, 0.0, 0.0])\n\n\nDEFAULT_CAM_1080p = dict(\n    name='forward cam 1920x1080 90 FOV',\n    field_of_view=90,\n    capture_width=1920,\n    capture_height=1080,\n    relative_position=[150, 1.0, 200],\n    relative_rotation=[0.0, 0.0, 0.0])\n\nDEFAULT_FPS = 8\nDEFAULT_SIM_STEP_TIME = 1 / DEFAULT_FPS\n\ntry:\n    import tensorflow\nexcept ImportError:\n    TENSORFLOW_AVAILABLE = False\nelse:\n    TENSORFLOW_AVAILABLE = True\n\n\n# Not passing through main.py args yet, but better for reproducing to put here than in os.environ\nSIMPLE_PPO = False\n# PPO_RESUME_PATH = '/home/a/baselines_results/openai-2018-06-17-17-48-24-795338/checkpoints/03125'\n# PPO_RESUME_PATH = '/home/a/baselines_results/openai-2018-06-22-00-00-21-866205/checkpoints/03125'\nPPO_RESUME_PATH = None\n\n\n# API\nAPI_PORT = 5557\nAPI_TIMEOUT_MS = 5000\n\n# Stream\nSTREAM_PORT = 5558\n\n\n# Set via main\nMAIN_ARGS:dict = {}\n\n# Upload results to github\nSESS_RESULTS_CSV_FILENAME_TEMPLATE = '{RESULTS_DIR}{os_path_sep}{DATE_STR}_{prefix}_{name}.csv'\n\nSUMMARY_CSV_FILENAME = SESS_RESULTS_CSV_FILENAME_TEMPLATE.format(\n    RESULTS_DIR=RESULTS_DIR, os_path_sep=os.path.sep, prefix='r0',\n    name='summary', DATE_STR=DATE_STR)\nEPISODES_CSV_FILENAME = SESS_RESULTS_CSV_FILENAME_TEMPLATE.format(\n    RESULTS_DIR=RESULTS_DIR, os_path_sep=os.path.sep, prefix='r1',\n    name='episodes', DATE_STR=DATE_STR)\n\nBINDINGS_PACKAGE_NAME = 'deepdrive'\n\nPUBLIC = 'DEEPDRIVE_PUBLIC' in os.environ\n\nALEXNET_NAME = 'AlexNet'\nALEXNET_FC7 = 4096\nALEXNET_IMAGE_SHAPE = (227, 227, 3)\nMOBILENET_V2_NAME = 'MobileNetV2'\nMOBILENET_V2_SLIM_NAME = 'mobilenet_v2_deepdrive'\nMOBILENET_V2_IMAGE_SHAPE = (224, 224, 3)\n\n# Unit conversions\nCMPS_TO_KPH = 0.036\nKPH_2_MPH = 0.6213711922\nMPH_2_KPH = 1. / KPH_2_MPH\nCMPS_TO_MPH = CMPS_TO_KPH * KPH_2_MPH\n\nUPLOAD_RESULTS = os.environ.get('DEEPDRIVE_UPLOAD', '')\n\nIS_DEBUG_MODE = getattr(sys, 'gettrace', None)"""
config/version.py,0,"b""from distutils.version import LooseVersion as semvar\n\nfrom config.directories import *\n\n# Version\nVERSION_STR = open(os.path.join(ROOT_DIR, 'VERSION')).read()\nMAJOR_MINOR_VERSION = semvar(VERSION_STR).version[:2]\nMAJOR_MINOR_VERSION_STR = '.'.join(str(vx) for vx in MAJOR_MINOR_VERSION)\n"""
control/__init__.py,0,b''
control/pid.py,0,"b'#!/usr/bin/python\n#\n# This file is part of IvPID.\n# Copyright (C) 2015 Ivmech Mechatronics Ltd. <bilgi@ivmech.com>\n#\n# IvPID is free software: you can redistribute it and/or modify\n# it under the terms of the GNU General Public License as published by\n# the Free Software Foundation, either version 3 of the License, or\n# (at your option) any later version.\n#\n# IvPID is distributed in the hope that it will be useful,\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n# GNU General Public License for more details.\n#\n# You should have received a copy of the GNU General Public License\n# along with this program.  If not, see <http://www.gnu.org/licenses/>.\n\n# title           :PID.py\n# description     :python pid controller\n# author          :Caner Durmusoglu\n# date            :20151218\n# version         :0.1\n# notes           :\n# ==============================================================================\n\n""""""Ivmech PID Controller is simple implementation of a Proportional-Integral-Derivative (PID) Controller in the Python Programming Language.\nMore information about PID Controller: http://en.wikipedia.org/wiki/PID_controller\n""""""\nimport time\n\n\nclass PID:\n    """"""PID Controller\n    """"""\n\n    def __init__(self, P=0.2, I=0.0, D=0.0):\n\n        self.Kp = P\n        self.Ki = I\n        self.Kd = D\n\n        self.sample_time = 0.00\n        self.current_time = time.time()\n        self.last_time = self.current_time\n\n        self.clear()\n\n    def clear(self):\n        """"""Clears PID computations and coefficients""""""\n        self.SetPoint = 0.0\n\n        self.PTerm = 0.0\n        self.ITerm = 0.0\n        self.DTerm = 0.0\n        self.last_error = 0.0\n\n        # Windup Guard\n        self.int_error = 0.0\n        self.windup_guard = 20.0\n\n        self.output = 0.0\n\n    def update(self, feedback_value):\n        """"""Calculates PID value for given reference feedback\n\n        .. math::\n            u(t) = K_p e(t) + K_i \\int_{0}^{t} e(t)dt + K_d {de}/{dt}\n\n        .. figure:: images/pid_1.png\n           :align:   center\n\n           Test PID with Kp=1.2, Ki=1, Kd=0.001 (test_pid.py)\n\n        """"""\n        error = self.SetPoint - feedback_value\n\n        self.current_time = time.time()\n        delta_time = self.current_time - self.last_time\n        delta_error = error - self.last_error\n\n        if (delta_time >= self.sample_time):\n            self.PTerm = self.Kp * error\n            self.ITerm += error * delta_time\n\n            if (self.ITerm < -self.windup_guard):\n                self.ITerm = -self.windup_guard\n            elif (self.ITerm > self.windup_guard):\n                self.ITerm = self.windup_guard\n\n            self.DTerm = 0.0\n            if delta_time > 0:\n                self.DTerm = delta_error / delta_time\n\n            # Remember last time and last error for next calculation\n            self.last_time = self.current_time\n            self.last_error = error\n\n            self.output = self.PTerm + (self.Ki * self.ITerm) + (self.Kd * self.DTerm)\n\n    def setKp(self, proportional_gain):\n        """"""Determines how aggressively the PID reacts to the current error with setting Proportional Gain""""""\n        self.Kp = proportional_gain\n\n    def setKi(self, integral_gain):\n        """"""Determines how aggressively the PID reacts to the current error with setting Integral Gain""""""\n        self.Ki = integral_gain\n\n    def setKd(self, derivative_gain):\n        """"""Determines how aggressively the PID reacts to the current error with setting Derivative Gain""""""\n        self.Kd = derivative_gain\n\n    def setWindup(self, windup):\n        """"""Integral windup, also known as integrator windup or reset windup,\n        refers to the situation in a PID feedback controller where\n        a large change in setpoint occurs (say a positive change)\n        and the integral terms accumulates a significant error\n        during the rise (windup), thus overshooting and continuing\n        to increase as this accumulated error is unwound\n        (offset by errors in the other direction).\n        The specific problem is the excess overshooting.\n        """"""\n        self.windup_guard = windup\n\n    def setSampleTime(self, sample_time):\n        """"""PID that should be updated at a regular interval.\n        Based on a pre-determined sampe time, the PID decides if it should compute or return immediately.\n        """"""\n        self.sample_time = sample_time\n'"
recorder/__init__.py,0,b'\n'
recorder/recorder.py,0,"b'import shutil\nfrom typing import List, Tuple\n\nimport requests\nfrom box import Box\n\n\nimport os\nimport glob\nimport json\nfrom os.path import join, basename\n\nimport config as c\nimport logs\nimport utils\nfrom sim.return_aggregator import TotalReturn, EpisodeReturn\nfrom util.anonymize import anonymize_user_home\nfrom utils import copy_dir_clean\n\nlog = logs.get_log(__name__)\n\nTEST_SAVE_IMAGE = False\n\n\nclass Recorder(object):\n    """"""\n    Responsible for artifact creation and uploading, including:\n    * HDF5\'s (Optionally saved to S3)\n    * Results csv files (Uploaded to Gist)\n    * MP4\'s (Uploaded to YouTube)\n    """"""\n\n    def __init__(self, recording_dir, should_record_agent_actions=True,\n                 should_record=True, eval_only=False, should_upload_gist=False,\n                 public=False, main_args=None, is_botleague=False):\n        self.save_threads:list = []\n        self.record_agent_actions:bool = should_record_agent_actions\n        self.should_record:bool = should_record\n        self.hdf5_dir:str = c.HDF5_SESSION_DIR\n        self.obz_recording:list = []\n        self.skipped_first_agent_action:bool = False\n        self.was_agent_action:bool = True\n        self.recorded_obz_count:int = 0\n        self.num_saved_observations:int = 0\n        self.recording_dir:str = recording_dir\n        self.eval_only:bool = eval_only\n        self.should_upload_gist:bool = should_upload_gist\n        self.public:bool = public\n        self.main_args:dict = main_args\n        self.is_botleague:bool = is_botleague\n        if self.should_record:\n            log.info(\'Recording driving data to %s\', self.hdf5_dir)\n\n    def step(self, obz, done, reward, action, is_agent_action=True):\n        self.was_agent_action = is_agent_action\n        log.debug(\'obz_exists? %r should_record? %r\', obz is not None,\n                  self.should_record)\n        if self.should_record_obz(obz):\n            log.debug(\'Recording frame\')\n            obz[\'gym_done\'] = done\n            obz[\'gym_reward\'] = reward\n            obz[\'gym_action\'] = action.serialize()\n            self.obz_recording.append(obz)\n            if TEST_SAVE_IMAGE:\n                utils.save_camera(obz[\'cameras\'][0][\'image\'],\n                                  obz[\'cameras\'][0][\'depth\'],\n                                  self.hdf5_dir, \'screenshot_\' +\n                                  str(self.step).zfill(10))\n                input(\'continue?\')\n            self.recorded_obz_count += 1\n            if self.recorded_obz_count % 100 == 0:\n                log.info(\'%d recorded observations\', self.recorded_obz_count)\n\n        else:\n            log.debug(\'Not recording frame.\')\n        self.maybe_save()\n\n    def maybe_save(self):\n        if (\n                self.should_record and\n                self.recorded_obz_count != 0 and\n                self.recorded_obz_count % c.FRAMES_PER_HDF5_FILE == 0 and\n                self.num_saved_observations < self.recorded_obz_count\n        ):\n            self.save_recordings()\n\n    def close(self, total_return:TotalReturn, episode_returns:List[EpisodeReturn],\n              median_fps:float, ):\n        log.info(\'Closing recorder\')\n        if self.eval_only:\n            # Okay to have partial eval recordings\n            self.save_unsaved_observations()\n        else:\n            log.info(\'Discarding %d observations to keep even number of \'\n                     \'frames in recorded datasets. \'\n                     \'Pass --eval-only to save all observations.\' %\n                     self.num_saved_observations)\n\n        if self.recorded_obz_count > 0:\n            for save_thread in self.save_threads:\n                # Wait for HDF5 saves to complete\n                save_thread.join()\n            mp4_file = utils.hdf5_to_mp4()\n            local_public_run = self.should_upload_gist and self.public\n            server_public_run = bool(c.UPLOAD_RESULTS)\n            public_run = local_public_run or server_public_run\n\n            if public_run:\n                # Gists will be accessible via YOUTGETMYJIST token\n                # regardless of whether they are \'public\' gists.\n                gist_url = utils.upload_to_gist(\n                    \'deepdrive-results-\' + c.DATE_STR,\n                    [c.SUMMARY_CSV_FILENAME, c.EPISODES_CSV_FILENAME],\n                    public=True)\n                log.info(\'gist uploaded to %s\', gist_url)\n            else:\n                gist_url = None\n\n            hdf5_observations = glob.glob(c.HDF5_SESSION_DIR + \'/*.hdf5\')\n            self.create_artifacts_inventory(\n                hdf5_observations=hdf5_observations,\n                episodes_file=c.EPISODES_CSV_FILENAME,\n                summary_file=c.SUMMARY_CSV_FILENAME,\n                mp4_file=mp4_file)\n\n            create_botleague_results(total_return, episode_returns, gist_url,\n                                     hdf5_observations,\n                                     mp4_file,\n                                     episodes_file=c.EPISODES_CSV_FILENAME,\n                                     summary_file=c.SUMMARY_CSV_FILENAME,\n                                     median_fps=median_fps)\n\n            # TODO: Create a Botleague compatible results.json file with\n            #  - YouTube link\n            #  - HDF5 links\n            #  - artifacts.json stuff (gist_url) etc..\n\n            # TODO: Add YouTube description file with the episode score summary,\n            #  gist link, and s3 link\n\n    def save_recordings(self):\n        name = str(self.recorded_obz_count // c.FRAMES_PER_HDF5_FILE).zfill(10)\n        filepath = os.path.join(self.hdf5_dir, \'%s.hdf5\' % name)\n        thread = utils.save_hdf5(self.obz_recording, filename=filepath,\n                                 background=True)\n        self.save_threads.append(thread)\n        log.info(\'Flushing output data\')\n        self.obz_recording = []\n        self.num_saved_observations = self.recorded_obz_count\n\n    def save_unsaved_observations(self):\n        if self.should_record and self.num_unsaved_observations():\n            self.save_recordings()\n\n    def should_record_obz(self, obz):\n        if not obz:\n            return False\n        elif not self.should_record:\n            return False\n        elif self.record_agent_actions:\n            return self.should_record\n        else:\n            is_game_driving = self.get_is_game_driving(obz)\n            safe_action = is_game_driving and not self.was_agent_action\n            if safe_action:\n                # TODO: Test to see if skipped_first_agent_action guard\n                #  can be safely removed\n                if self.skipped_first_agent_action:\n                    return True\n                else:\n                    self.skipped_first_agent_action = True\n                    return False\n            else:\n                self.skipped_first_agent_action = False\n                return False\n\n    @staticmethod\n    def get_is_game_driving(obz):\n        if not obz:\n            log.warn(\n                \'Observation not set, assuming game not driving to \'\n                \'prevent recording bad actions\')\n            return False\n        return obz[\'is_game_driving\'] == 1\n\n    @staticmethod\n    def create_artifacts_inventory(hdf5_observations: list,\n                                   episodes_file: str,\n                                   summary_file: str,\n                                   mp4_file: str):\n        anon = anonymize_user_home\n        filename = join(c.RESULTS_DIR, \'artifacts.json\')\n        with open(filename, \'w\') as out_file:\n            data = {\'artifacts\': {\n                \'mp4\': anon(mp4_file),\n                \'performance_summary\': anon(summary_file),\n                \'episodes\': anon(episodes_file),\n                \'hdf5_observations\': [anon(o) for o in hdf5_observations],\n            }}\n            json.dump(data, out_file, indent=2)\n        log.info(\'Wrote artifacts inventory to %s\' % anon(filename))\n        latest_artifacts_filename = join(c.RESULTS_BASE_DIR,\n                                         \'latest-artifacts.json\')\n        shutil.copy2(filename, latest_artifacts_filename)\n        print(\'\\n****\\nARTIFACTS INVENTORY COPIED TO: ""%s""\' +\n              anon(latest_artifacts_filename))\n\n    def num_unsaved_observations(self):\n        return self.recorded_obz_count - self.num_saved_observations\n\n\ndef upload_artifacts(mp4_file:str, hdf5_observations: List[str]) \\\n        -> Tuple[str, str, str, List[str]]:\n    if \'UPLOAD_TO_YOUTUBE\' in os.environ:\n        youtube_id = utils.upload_to_youtube(mp4_file)\n        if youtube_id:\n            youtube_url = \'https://www.youtube.com/watch?v=%s\' % youtube_id\n            log.info(\'Successfully uploaded to YouTube! %s\', youtube_url)\n        else:\n            youtube_url = \'\'\n    else:\n        youtube_url = \'\'\n        youtube_id = \'\'\n    mp4_url = upload_artifacts_to_cloud([mp4_file], \'mp4\')[0]\n    hdf5_urls = upload_artifacts_to_cloud(hdf5_observations, \'hdf5\')\n    return youtube_id, youtube_url, mp4_url, hdf5_urls\n\n\ndef upload_artifacts_to_cloud(file_paths:List[str], directory:str,\n                              use_gcp=True) -> List[str]:\n    from ue4helpers import GCPUtils, AWSUtils\n    ret = []\n    for file_path in file_paths:\n        s3path = \'artifacts/\' + os.path.basename(c.RESULTS_DIR)\n        key = s3path + (\'/%s/\' % directory) + os.path.basename(file_path)\n        if use_gcp:\n            bucket = c.GCP_BUCKET\n            url = c.GCP_BUCKET_URL\n            storage_utils = GCPUtils\n        else:\n            bucket = c.AWS_BUCKET\n            url = c.AWS_BUCKET_URL\n            storage_utils = AWSUtils\n        storage_utils.upload_file(bucket, key=key, filename=file_path)\n        ret.append(\'%s/%s\' % (url, key))\n    return ret\n\n\ndef create_botleague_results(total_return: TotalReturn, episode_returns, gist_url,\n                             hdf5_observations, mp4_file, episodes_file,\n                             summary_file, median_fps):\n    ret = Box(default_box=True)\n    ts = total_return\n    if gist_url:\n        ret.gist = gist_url\n\n    def sum_over_episodes(key):\n        return sum(getattr(e, key) for e in episode_returns)\n\n    ret.sensorimotor_specific = get_sensorimotor_specific_results(\n        episode_returns, median_fps, sum_over_episodes, ts)\n    total_time = ret.sensorimotor_specific.total_episode_seconds\n    ret.driving_specific, ds_score = get_driving_specific_results(\n        episode_returns, sum_over_episodes, total_time, ts)\n    ret.score = ds_score\n    artifact_dir = c.BOTLEAGUE_RESULTS_DIR\n    os.makedirs(artifact_dir, exist_ok=True)\n    csv_relative_dir = \'csvs\'\n    if c.UPLOAD_RESULTS:\n        create_uploaded_artifacts(csv_relative_dir, episodes_file,\n                                  hdf5_observations, mp4_file, ret,\n                                  summary_file)\n    else:\n        use_local_artifacts(episodes_file, hdf5_observations, mp4_file, ret,\n                            summary_file)\n    store_results(artifact_dir, ret)\n\n    return ret\n\n\ndef store_results(artifact_dir, ret):\n    results_json_filename = join(artifact_dir, \'results.json\')\n    log.info(f\'Results:\\n{ret.to_json(indent=2)}\')\n    ret.to_json(filename=results_json_filename, indent=2)\n    log.info(\'Wrote botleague results to %s\' % results_json_filename)\n    copy_dir_clean(src=c.BOTLEAGUE_RESULTS_DIR,\n                   dest=c.LATEST_BOTLEAGUE_RESULTS)\n\n\ndef get_sensorimotor_specific_results(episode_returns, median_fps,\n                                      sum_over_episodes, ts) -> Box:\n    ret = Box()\n    ret.num_episodes = len(episode_returns)\n    ret.median_fps = median_fps\n    ret.num_steps = ts.num_steps\n    ret.total_episode_seconds = sum_over_episodes(\'episode_time\')\n    return ret\n\n\ndef use_local_artifacts(episodes_file, hdf5_observations, mp4_file, ret,\n                        summary_file):\n    ret.mp4 = mp4_file\n    ret.problem_specific.hdf5_observations = hdf5_observations\n    ret.problem_specific.summary = summary_file\n    ret.problem_specific.episodes = episodes_file\n\n\ndef create_uploaded_artifacts(csv_relative_dir, episodes_file,\n                              hdf5_observations, mp4_file, ret, summary_file):\n    summary_url, episodes_url = upload_artifacts_to_cloud(\n        [summary_file, episodes_file], csv_relative_dir)\n    ret.problem_specific.summary = summary_url\n    ret.problem_specific.episodes = episodes_url\n    youtube_id, youtube_url, mp4_url, hdf5_urls = \\\n        upload_artifacts(mp4_file, hdf5_observations)\n    ret.youtube = youtube_url\n    ret.mp4 = mp4_url\n    ret.problem_specific.hdf5_observations = hdf5_urls\n\n\ndef get_driving_specific_results(episode_returns, sum_over_episodes,\n                                 total_time, ts):\n    # TODO: Closest distance to pedestrians\n    # See https://docs.google.com/spreadsheets/d/1Nm7_3vUYM5pIs2zLWM2lO_TCoIlVpwX-YRLVo4S4-Cc/edit#gid=0\n    #   for balancing score coefficients\n\n    # We have a problem with reaching the destination not meaning anything.\n    # You should not get any trip speed bonus if you don\'t complete the trip.\n    # SPD   CMFT   JAR\n    # 58  - 700  - 600\n    # 116 - 4800 - 1500\n\n    ret = Box()\n    score = 0\n    ret.max_gforce = ts.max_gforce\n    ret.uncomfortable_gforce_seconds = sum_over_episodes(\n        \'uncomfortable_gforce_seconds\')\n    ret.jarring_gforce_seconds = \\\n        sum_over_episodes(\'jarring_gforce_seconds\')\n    ret.harmful_gforces = \\\n        any(e.harmful_gforces for e in episode_returns)\n    if total_time == 0:\n        ret.comfort_pct = 0\n        ret.jarring_pct = 0\n    else:\n        ret.comfort_pct = 100 - ret.uncomfortable_gforce_seconds / total_time * 100\n        ret.jarring_pct = ret.jarring_gforce_seconds / total_time * 100\n    score -= (100 - ret.comfort_pct) * 100\n    score -= ret.jarring_pct * 500\n    ret.max_gforce = ts.max_gforce\n    ret.max_kph = ts.max_kph\n    ret.trip_speed_kph = ts.trip_speed_kph\n    score += ts.trip_speed_kph * 10\n    ret.collided_with_vehicle = ts.collided_with_vehicle\n    ret.collided_with_non_actor = ts.collided_with_non_actor\n    if ret.harmful_gforces:\n        score -= 1e4\n    elif ts.collided_with_vehicle:\n        score -= 1e4\n    elif ts.collided_with_non_actor:\n        score -= 2.5e3\n    ret.closest_vehicle_meters = ts.closest_vehicle_cm / 100\n    ret.closest_vehicle_meters_while_at_least_4kph = \\\n        ts.closest_vehicle_cm_while_at_least_4kph / 100\n    ret.max_lane_deviation_meters = ts.max_lane_deviation_cm / 100\n    return ret, score\n\n\ndef make_needs_upload(base_dir:str, relative_dir:str, file:str,\n                      upload_to:str) -> Box:\n    ret = Box(default_box=True)\n    upload = ret.needs_upload\n    abs_dir = join(base_dir, relative_dir)\n    os.makedirs(abs_dir, exist_ok=True)\n    shutil.copy2(file, abs_dir)\n    if relative_dir:\n        upload.relative_path = \'/\'.join([relative_dir, basename(file)])\n    else:\n        upload.relative_path = basename(file)\n    upload.upload_to = upload_to\n    return ret\n'"
renderer/__init__.py,0,"b'from __future__ import (absolute_import, division,\n                        print_function, unicode_literals)\n\nfrom enum import Enum\n\n\nimport logs\n\nlog = logs.get_log(__name__)\n\n\nclass RendererType(Enum):\n    WEB = 1\n    PYGLET = 2\n\n\ndef renderer_factory(renderer_type=None, cameras=None):\n    if renderer_type is None:\n        try:\n            import cv2\n            renderer_type = RendererType.WEB\n        except ImportError as e:\n            log.warn(""""""\n            \n***************************************************             \n    Could not start web renderer. Need OpenCV.\n    HINT: Install with:\n        pip install opencv-python\n    Falling back to pyglet renderer.\n***************************************************\n"""""")\n            renderer_type = RendererType.PYGLET\n\n    if renderer_type is RendererType.WEB:\n        from renderer.web_renderer import get_web_renderer\n        return get_web_renderer()\n    elif renderer_type is RendererType.PYGLET:\n        from renderer.pyglet_renderer import PygletRenderer\n        return PygletRenderer(cameras)\n    else:\n        raise NotImplementedError(\'Renderer type not recognized\')\n'"
renderer/base_renderer.py,0,"b""from __future__ import (absolute_import, division,\n                        print_function, unicode_literals)\nfrom future.builtins import (ascii, bytes, chr, dict, filter, hex, input,\n                             int, map, next, oct, open, pow, range, round,\n                             str, super, zip)\n\n\nclass Renderer(object):\n    def render(self, obz):\n        raise NotImplementedError('Please define render in your child class')\n\n    def close(self):\n        raise NotImplementedError()\n\n\n"""
renderer/pyglet_renderer.py,0,"b""from __future__ import (absolute_import, division,\n                        print_function, unicode_literals)\nfrom future.builtins import (ascii, bytes, chr, dict, filter, hex, input,\n                             int, map, next, oct, open, pow, range, round,\n                             str, super, zip)\n\nimport time\n\nimport numpy as np\n\nfrom multiprocessing import Process, Queue\n\nimport utils\nimport logs\nfrom renderer.base_renderer import Renderer\n\nlog = logs.get_log(__name__)\n\nDRAW_FPS = True\n\n\nclass PygletRenderer(Renderer):\n    def __init__(self, cameras):\n        self.prev_render_time = None\n        # TODO: Use ZMQ/pyarrow instead of multiprocessing for faster transfer\n        q = Queue(maxsize=1)\n        p = Process(target=render_cameras, args=(q, cameras))\n        p.start()\n        self.pyglet_process = p\n        self.pyglet_queue = q\n\n    def render(self, obz):\n        now = time.time()\n        if self.prev_render_time:\n            log.debug('render time %r', now - self.prev_render_time)\n        self.prev_render_time = now\n        if obz is not None:\n            self.pyglet_queue.put(obz['cameras'])\n\n    def close(self):\n        pass\n        # TODO: close stuff? works well without for now\n        # self.pyglet_process.join()\n\n\ndef render_cameras(render_queue, cameras):\n    import pyglet\n    from pyglet.gl import GLubyte\n\n    widths = []\n    heights = []\n    for camera in cameras:\n        widths += [camera['capture_width']]\n        heights += [camera['capture_height']]\n\n    width = max(widths) * 2  # image and depths\n    height = sum(heights)\n    window = pyglet.window.Window(width, height)\n    fps_display = pyglet.clock.ClockDisplay() if DRAW_FPS else None\n\n    @window.event\n    def on_draw():\n        window.clear()\n        cams = render_queue.get(block=True)\n        channels = 3\n        bytes_per_channel = 1\n        for cam_idx, cam in enumerate(cams):\n            img_raw = cam['img_raw'] if 'img_raw' in cam else cam['image']\n            img_data = np.copy(img_raw)\n            depth_data = np.ascontiguousarray(utils.depth_heatmap(np.copy(cam['depth'])))\n            img_data.shape = -1\n            depth_data.shape = -1\n            img_texture = (GLubyte * img_data.size)(*img_data.astype('uint8'))\n            depth_texture = (GLubyte * depth_data.size)(*depth_data.astype('uint8'))\n            image = pyglet.image.ImageData(\n                cam['capture_width'],\n                cam['capture_height'],\n                'RGB',\n                img_texture,\n                pitch= -1 * cam['capture_width'] * channels * bytes_per_channel)\n            depth = pyglet.image.ImageData(\n                cam['capture_width'],\n                cam['capture_height'],\n                'RGB',\n                depth_texture,\n                pitch= -1 * cam['capture_width'] * channels * bytes_per_channel)\n            if image is not None:\n                image.blit(0, cam_idx * cam['capture_height'])\n            if depth is not None:\n                depth.blit(cam['capture_width'], cam_idx * cam['capture_height'])\n        if DRAW_FPS:\n            fps_display.draw()\n\n    while True:\n        pyglet.clock.tick()\n\n        for window in pyglet.app.windows:\n            window.switch_to()\n            window.dispatch_events()\n            window.dispatch_event('on_draw')\n            window.flip()\n"""
renderer/stream_server.py,0,"b'from __future__ import (absolute_import, division,\n                        print_function, unicode_literals)\n\nimport time\n\nimport pyarrow\n\nfrom future.builtins import (ascii, bytes, chr, dict, filter, hex, input,\n                             int, map, next, oct, open, pow, range, round,\n                             str, super, zip)\n\nimport threading\nimport cv2\nimport numpy as np\nfrom flask import render_template\nfrom werkzeug.wrappers import Response\n\nfrom collections import deque\n\nimport logs\nimport config as c\nimport utils\n\nlog = logs.get_log(__name__, \'renderer_log.txt\')\n\n\nclass StreamServer(object):\n    def __init__(self, app):\n        self.app = app\n        import zmq\n\n        log.info(\'Pairing to zmq image stream\')\n        context = zmq.Context()\n        socket = context.socket(zmq.PAIR)\n        conn_string = \'tcp://*:%s\' % c.STREAM_PORT\n        socket.bind(conn_string)\n        log.debug(\'Grabbing images over ZMQ from %s\', conn_string)\n        self.socket = socket\n        self.context = context\n        self.frame_queue = deque(maxlen=1)\n\n        gen = self.gen\n\n        @app.route(\'/\')\n        def index():\n            return render_template(\'index.html\')\n\n        @app.route(\'/video_feed\')\n        def video_feed():\n            return Response(gen(), mimetype=\'multipart/x-mixed-replace; boundary=frame\')\n\n        self.frame_worker = threading.Thread(target=frame_worker, args=(socket, self.frame_queue))\n        self.frame_worker.start()\n\n        self.start_server()\n\n    def start_server(self):\n        # To see various options explored, go here: https://github.com/crizCraig/mjpg_server_test\n        log.info(""""""\n************************************************\nWeb renderer available at  http://localhost:5000\n************************************************\n"""""")\n        use_cherrypy_wsgi = True\n        if use_cherrypy_wsgi:\n            import cherrypy\n            from paste.translogger import TransLogger\n\n            # Enable WSGI access logging via Paste\n            app_logged = TransLogger(self.app)\n\n            # Mount the WSGI callable object (app) on the root directory\n            cherrypy.tree.graft(app_logged, \'/\')\n\n            # Set the configuration of the web server\n            cherrypy.config.update({\n                \'engine.autoreload.on\': False,\n                \'checker.on\': False,\n                \'tools.log_headers.on\': False,\n                \'request.show_tracebacks\': False,\n                \'request.show_mismatched_params\': False,\n                \'log.screen\': False,\n                \'server.socket_port\': 5000,\n                \'server.socket_host\': \'0.0.0.0\',\n                \'server.thread_pool\': 30,\n                \'server.socket_queue_size\': 30,\n                \'server.accepted_queue_size\': -1,\n            })\n\n            # Start the CherryPy WSGI web server\n            cherrypy.engine.start()\n            cherrypy.engine.block()\n        else:\n            self.app.run(host=\'0.0.0.0\', port=5000)\n\n    def gen(self):\n        jpeg_bytes = None\n        while True:\n            if len(self.frame_queue) > 0:\n                if jpeg_bytes is not self.frame_queue[0]:\n                    yield self.frame_queue[0]\n                    jpeg_bytes = self.frame_queue[0]\n                time.sleep(0.001)\n\n    def __del__(self):\n        try:\n            if self.socket is not None:\n                # In background process\n                self.socket.close()\n                self.context.term()\n        except Exception as e:\n            print(e)\n\n\ndef frame_worker(socket, queue):\n    """"""Pull images from ZMQ and add them to the stream queue""""""\n    while True:\n        msg = socket.recv()\n        if msg:\n            cameras = pyarrow.deserialize(msg)\n            all_cams_image = None\n            if cameras is not None:\n                jpeg = get_image_and_depth(all_cams_image, cameras)\n                queue.append(b\'--frame\\r\\n\'\n                             b\'Content-Type: image/jpeg\\r\\n\\r\\n\' +\n                             jpeg.tobytes() + b\'\\r\\n\\r\\n\')\n\n\ndef get_image_and_depth(all_cams_image, cameras):\n    for cam_idx, cam in enumerate(cameras):\n        image = cam[\'image_raw\'] if \'image_raw\' in cam else cam[\'image\']\n        depth = np.ascontiguousarray(utils.depth_heatmap(np.copy(cam[\'depth\'])))\n        try:\n            image = np.concatenate((image, depth), axis=1)\n        except ValueError as e:\n            log.error(\'Could not concatenate image with shape %r \'\n                      \'and depth with shape %r %s\',\n                      image.shape, depth.shape, str(e))\n\n        image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n        if all_cams_image is None:\n            all_cams_image = image\n        else:\n            all_cams_image = np.concatenate((all_cams_image, image), axis=0)\n    ret, jpeg = cv2.imencode(\'.jpg\', all_cams_image)\n    return jpeg\n'"
renderer/web_renderer.py,0,"b'from __future__ import (absolute_import, division,\n                        print_function, unicode_literals)\n\nimport sys\n\nfrom future.builtins import (ascii, bytes, chr, dict, filter, hex, input,\n                             int, map, next, oct, open, pow, range, round,\n                             str, super, zip)\nimport time\nfrom multiprocessing import Process\nimport pyarrow\nfrom flask import Flask\n\nimport config as c\nimport logs\nimport utils\nfrom renderer.base_renderer import Renderer\n\nlog = logs.get_log(__name__)\napp = Flask(__name__)\n\nweb_renderer = None\n\n""""""\nUsage:\nCall main.py with --render and navigate to http://0.0.0.0:5000\n""""""\n\n# TODO: Support rendering saved HDF5 and tfrecord\'s\n\n\ndef get_web_renderer():\n    # Singleton is a hack around difficulties setting up multiple ZMQ contexts on same port in the same process\n    global web_renderer\n    if web_renderer is None:\n        web_renderer = WebRenderer()\n    return web_renderer\n\n\nclass WebRenderer(Renderer):\n\n    def __init__(self):\n\n        # TODO: Move source ZMQ to base renderer and replace\n        #  pyglet renderer\'s use of multiprocessing\n        import zmq\n\n        self.prev_render_time = None\n        self.context = zmq.Context()\n        self.socket = self.context.socket(zmq.PAIR)\n        conn_string = \'tcp://localhost:%s\' % c.STREAM_PORT\n        log.debug(\'Sending images over ZMQ to %s\', conn_string)\n\n        self.socket.connect(conn_string)\n        self.web_server_process = Process(target=background_server_process,\n                                          name=\'streaming server\', daemon=True)\n        self.web_server_process.start()\n\n    def close(self):\n        self.socket.close()\n        self.context.term()\n        try:\n            self.web_server_process.join(timeout=.25)\n        except Exception as e:\n            # TODO: Find out why we cannot web server process on close.\n            pass\n\n        # Print here as log object has been deleted by now\n        print(\'Closed web renderer\')\n\n    def render(self, obz):\n        now = time.time()\n        if self.prev_render_time:\n            delta = now - self.prev_render_time\n            log.debug(\'send image period %f\', delta)\n        self.prev_render_time = now\n        if obz is not None:\n            self.socket.send(pyarrow.serialize(obz[\'cameras\']).to_buffer())\n\n\ndef background_server_process():\n    from renderer.stream_server import StreamServer\n    StreamServer(app)\n\n'"
sim/__init__.py,0,"b'from __future__ import (absolute_import, division,\n                        print_function, unicode_literals)\n\nfrom future.builtins import (dict, input, str)\n\nfrom box import Box\nfrom gym.envs.registration import register\n\n# noinspection PyUnresolvedReferences\nimport gym\nimport logs\n\nimport config as c\nfrom deepdrive_api.client import Client\n\n# noinspection PyUnresolvedReferences\nfrom sim.action import gym_action as action\nfrom sim.driving_style import DrivingStyle\nfrom sim import driving_style\nfrom sim.sim_args import SimArgs\nfrom sim.view_mode import ViewMode, ViewModeController\nfrom sim import world\nfrom recorder.recorder import Recorder\n\nlog = logs.get_log(__name__)\n\n\ndef start(**kwargs):\n    """"""\n    Deepdrive gym environment factory.\n\n    This configures and creates a gym environment from parameters passed through main.py\n\n    The parameters are either used to start the environment in process,\n    or marshalled and passed to a remote instance of the environment,\n    in which case this method is called again on the remote server\n    with the same args, minus is_remote_client.\n\n    :param kwargs: Deepdrive gym env configuration\n    :return: environment object that implements the gym API (i.e. step, reset, close, render)\n    """"""\n    args = SimArgs(**kwargs)\n    if args.is_remote_client:\n        if isinstance(args.driving_style, DrivingStyle):\n            args.driving_style = args.driving_style.as_string()\n        args.client_main_args = c.MAIN_ARGS\n        env = Client(**(args.to_dict()))\n    else:\n        env = start_local_env(args)\n    return env\n\n\ndef start_local_env(args:SimArgs):\n    """"""\n    Acts as a constructor / factory for a local gym environment\n    and starts the associated Unreal process\n    :param args:\n    :return: gym environment\n    """"""\n    if isinstance(args.driving_style, str):\n        args.driving_style = DrivingStyle.from_str(args.driving_style)\n    env = gym.make(args.env_id)\n    env.seed(c.RNG_SEED)\n    if args.experiment is None:\n        args.experiment = \'\'\n    _env = env.unwrapped\n    _env.is_discrete = args.is_discrete\n    _env.preprocess_with_tensorflow = args.preprocess_with_tensorflow\n    _env.is_sync = args.is_sync\n    _env.reset_returns_zero = args.reset_returns_zero\n    _env.init_action_space()\n    _env.target_fps = args.fps\n    _env.experiment = args.experiment.replace(\' \', \'_\')\n    _env.sim_step_time = args.sim_step_time\n    _env.period = 1. / args.fps\n    _env.driving_style = args.driving_style\n    _env.should_render = args.render\n    _env.enable_traffic = args.enable_traffic\n    _env.ego_mph = args.ego_mph\n    _env.view_mode_controller = ViewModeController(period=args.view_mode_period)\n    _env.max_steps = args.max_steps\n    _env.max_episodes = args.max_episodes\n    _env.set_use_sim_start_command(args.use_sim_start_command)\n    _env.image_resize_dims = args.image_resize_dims\n    add_recorder(_env, args)\n    _env.should_normalize_image = args.should_normalize_image\n    _env.randomize_sun_speed = args.randomize_sun_speed\n    _env.randomize_shadow_level = args.randomize_shadow_level\n    _env.randomize_month = args.randomize_month\n    _env.is_botleague = args.is_botleague\n    _env.scenario_index = args.scenario_index\n    _env.unreal_map = c.MAP_LOOKUP[args.map]\n    if args.path_follower:\n        _env.has_control = False\n\n    connect_to_unreal(_env, args)\n    _env.set_step_mode()\n    if args.sess:\n        _env.set_tf_session(args.sess)\n    if args.start_dashboard:\n        _env.start_dashboard()\n    log.info(\'Will save results to %s\', c.RESULTS_DIR)\n    _env.init_benchmarking()\n\n    monkey_patch_env_api(env)\n\n    env.reset()\n    return env\n\n\ndef add_recorder(_env, args: SimArgs):\n    if args.is_remote_client:\n        main_args = args.client_main_args\n    else:\n        main_args = c.MAIN_ARGS\n    _env.recorder = Recorder(args.recording_dir,\n                             should_record=args.should_record,\n                             eval_only=args.eval_only,\n                             should_upload_gist=args.upload_gist,\n                             public=args.public,\n                             main_args=main_args,\n                             is_botleague=args.is_botleague)\n\n\ndef monkey_patch_env_api(env):\n    """"""\n    Monkey patch methods we want in the env API (c.f. api/client.py)\n    :param env: gym environment to patch\n    """"""\n    env.change_cameras = env.unwrapped.change_cameras\n\n\ndef connect_to_unreal(_env, args):\n    """"""\n    Start or connect to an existing instance of the Deepdrive\n    simulator running in Unreal Engine\n    """"""\n    if args.use_sim_start_command:\n        # TODO: Find a better way to do this. Waiting for the hwnd and focusing does not work in windows.\n        input(\'Press any key when the game has loaded\')\n    _env.connect(args.cameras)\n\n\n# Use start() to parameterize environment.\n# Parameterizing here leads to combinitorial splosion.\nregister(\n    id=\'Deepdrive-v0\',\n    entry_point=\'sim.gym_env:DeepDriveEnv\',\n    kwargs=dict(),\n)\n'"
sim/action.py,0,"b'from __future__ import (absolute_import, division,\n                        print_function, unicode_literals)\n\nfrom itertools import product\n\nfrom future.builtins import (int, open, round,\n                             str)\n\nimport math\n\nimport numpy as np\nimport logs\nlog = logs.get_log(__name__)\n\n\nclass Action(object):\n    STEERING_INDEX = 0\n    THROTTLE_INDEX = 1\n    BRAKE_INDEX = 2\n    HANDBRAKE_INDEX = 3\n    HAS_CONTROL_INDEX = 4\n\n    STEERING_MIN, STEERING_MAX = -1, 1\n    THROTTLE_MIN, THROTTLE_MAX = -1, 1\n    BRAKE_MIN, BRAKE_MAX = 0, 1\n    HANDBRAKE_MIN, HANDBRAKE_MAX = 0, 1\n\n    def __init__(self, steering=0, throttle=0, brake=0, handbrake=0,\n                 has_control=True):\n        self.steering = steering\n        self.throttle = throttle\n        self.brake = brake\n        self.handbrake = handbrake\n        self.has_control = has_control\n\n    def clip(self):\n        self.steering  = min(max(self.steering,  self.STEERING_MIN),  self.STEERING_MAX)\n        self.throttle  = min(max(self.throttle,  self.THROTTLE_MIN),  self.THROTTLE_MAX)\n        self.brake     = min(max(self.brake,     self.BRAKE_MIN),     self.BRAKE_MAX)\n        self.handbrake = min(max(self.handbrake, self.HANDBRAKE_MIN), self.HANDBRAKE_MAX)\n\n    def as_gym(self):\n        ret = gym_action(steering=self.steering, throttle=self.throttle,\n                         brake=self.brake, handbrake=self.handbrake,\n                         has_control=self.has_control)\n        return ret\n\n    def serialize(self):\n        ret = [self.steering, self.throttle, self.brake, self.handbrake,\n               self.has_control]\n        return ret\n\n\n    @classmethod\n    def from_gym(cls, action):\n        has_control = True\n        if len(action) > 4:\n            if isinstance(action[4], list):\n                has_control = action[4][0]\n            else:\n                has_control = action[cls.HAS_CONTROL_INDEX]\n        handbrake = action[cls.HANDBRAKE_INDEX][0]\n        if handbrake <= 0 or math.isnan(handbrake):\n            handbrake = 0\n        else:\n            handbrake = 1\n        ret = cls(steering=action[cls.STEERING_INDEX][0],\n                  throttle=action[cls.THROTTLE_INDEX][0],\n                  brake=action[cls.BRAKE_INDEX][0],\n                  handbrake=handbrake, has_control=has_control)\n        return ret\n\n\ndef gym_action(steering=0, throttle=0, brake=0, handbrake=0, has_control=True):\n    action = [np.array([steering]),\n              np.array([throttle]),\n              np.array([brake]),\n              np.array([handbrake]),\n              has_control]\n    return action\n\n\nclass DiscreteActions(object):\n    def __init__(self, steer, throttle, brake):\n        self.steer = steer\n        self.throttle = throttle\n        self.brake = brake\n\n        self.product = list(product(steer, throttle, brake))\n\n    def get_components(self, idx):\n        steer, throttle, brake = self.product[idx]\n        return steer, throttle, brake'"
sim/camera.py,0,"b'from __future__ import (absolute_import, division,\n                        print_function, unicode_literals)\nfrom future.builtins import (int, open, round,\n                             str)\n\nclass Camera(object):\n    def __init__(self, name, field_of_view, capture_width, capture_height, relative_position, relative_rotation):\n        self.name = name\n        self.field_of_view = field_of_view\n        self.capture_width = capture_width\n        self.capture_height = capture_height\n        self.relative_position = relative_position\n        self.relative_rotation = relative_rotation\n        self.connection_id = None\n'"
sim/driving_style.py,0,"b'from __future__ import (absolute_import, division,\n                        print_function, unicode_literals)\nfrom future.builtins import (int, open, round,\n                             str)\n\n\nfrom enum import Enum\n\nimport logs\nlog = logs.get_log(__name__)\n\n\nclass RewardWeighting(object):\n    def __init__(self, progress, gforce, lane_deviation, total_time, speed):\n        # Progress and time were used in DeepDrive-v0 (2.0) -\n        # keeping for now in case we want to use again\n        self.progress_weight = progress\n        self.gforce_weight = gforce\n        self.lane_deviation_weight = lane_deviation\n        self.time_weight = total_time\n        self.speed_weight = speed\n\n    @staticmethod\n    def combine(progress_reward, gforce_penalty, lane_deviation_penalty, time_penalty, speed):\n        return progress_reward \\\n               - gforce_penalty \\\n               - lane_deviation_penalty \\\n               - time_penalty \\\n               + speed\n\n\nclass DrivingStyle(Enum):\n    """"""Idea: Adjust these weights dynamically to produce a sort of curriculum where speed is learned first,\n    then lane, then gforce. Also, need to record unweighted score components in physical units (m, m/s^2, etc...)\n    so that scores can be compared across different weightings and environments.\n\n    To adjust dynamically, the reward weighting should be changed per episode, or a horizon based on discount factor,\n    in order to achieve a desired reward component balance.\n\n    So say we wanted speed to be the majority of the reward received, i.e. 50%. We would look at the share made up by\n    speed in the return for an episode (i.e. trip or lap for driving). If it\'s 25% of the absolute reward\n    (summing positive and abs(negative) rewards), then we double a ""curriculum coefficient"" or CC for speed. These curriculum\n    coefficients then get normalized so the final aggregate reward maintains the same scale as before.\n\n    Then, as speed gets closer to 50% of the reward, the smaller components of the reward will begin to get weighted\n    more heavily. If speed becomes more than 50% of the reward, then its CC will shrink and allow learning how to achieve\n    other objectives.\n\n    Why do this?\n    Optimization should find the best way to squeeze out all the juice from the reward, right? Well, maybe, but\n    I\'m finding the scale and *order* to be very important in practice. In particular, lane deviation grows like crazy\n    once you are out of the lane, regardless of the weight. So if speed is not learned first, our agent just decides\n    to not move. Also, g-force penalties counter initial acceleration required to get speed, so we end up needing to\n    weight g-force too small or too large with respect to speed over the long term.\n\n    The above curriculum approach aims to fix these things by targeting a certain balance of objectives over the\n    long-term, rather than the short-term, while adjusting short-term curriculum weights in order to get there. Yes,\n    it does feel like the model should take care of this, but it\'s only optimized for the expected aggregate reward\n    across all the objectives. Perhaps inputting the different components running averages or real-time values to\n    a recurrent part of the model would allow it to balance the objectives through SGD rather than the above\n    simple linear tweaking.\n\n    (looks like EPG is a nicer formulation of this https://blog.openai.com/evolved-policy-gradients/)\n    (Now looks like RUDDER is another principled step in this direction https://arxiv.org/abs/1806.07857)\n\n    - After some experimentation, seems like we may not need this yet. Observation normalization was causing the\n    motivating problem by learning too slow. Optimization does find a way. I think distributional RL may be helpful here\n    especially if we can get dimensions for all the components of the reward. Also a novelty bonus on\n    (observation,action) or (game-state,action) would be helpful most likely to avoid local minima.\n    """"""\n    __order__ = \'CRUISING NORMAL RL_1 LATE EMERGENCY CHASE STEER_ONLY\'\n    # TODO: Possibly assign function rather than just weights\n    CRUISING   = RewardWeighting(speed=0.5, progress=0.0, gforce=2.00, lane_deviation=1.50, total_time=0.0)\n    NORMAL     = RewardWeighting(speed=1.0, progress=0.0, gforce=1.00, lane_deviation=0.10, total_time=0.0)\n    RL_1       = RewardWeighting(speed=1.0, progress=0.0, gforce=0.00, lane_deviation=0.10, total_time=0.0)  # TODO: Use this to bootstrap PPO, otherwise will not gain speed.\n    LATE       = RewardWeighting(speed=2.0, progress=0.0, gforce=0.50, lane_deviation=0.50, total_time=0.0)\n    EMERGENCY  = RewardWeighting(speed=2.0, progress=0.0, gforce=0.75, lane_deviation=0.75, total_time=0.0)\n    CHASE      = RewardWeighting(speed=2.0, progress=0.0, gforce=0.00, lane_deviation=0.00, total_time=0.0)\n    STEER_ONLY = RewardWeighting(speed=1.0, progress=0.0, gforce=0.00, lane_deviation=0.00, total_time=0.0)\n\n    def as_string(self):\n        return self.name.lower()\n\n    @classmethod\n    def from_str(cls, name):\n        return cls[name.upper()]\n'"
sim/graphics.py,0,"b'from __future__ import (absolute_import, division,\n                        print_function, unicode_literals)\n\nimport numpy as np\nfrom future.builtins import (int, open, round,\n                             str)\n\nimport deepdrive_simulation\n\nimport config as c\nimport logs\nlog = logs.get_log(__name__)\n\n\nSHADOW_RANGE = list(range(0, 5))\n\n\ndef set_capture_graphics(shadow_level):\n    """"""Set shadow quality to level where level is an int between 0 and 4\n        Note: We can set texture level, ambient occlusion, etc... but it only affects\n        the display window, not the captured cameras.\n    """"""\n    if not isinstance(shadow_level, int) and np.issubdtype(shadow_level, int):\n        raise ValueError(\'Shadow level should be an integer\')\n    shadow_level = int(shadow_level)\n    if shadow_level not in SHADOW_RANGE:\n        raise ValueError(\'Shadow level should be between 0 and 4\')\n    settings = deepdrive_simulation.SimulationGraphicsSettings()\n    settings.shadow_quality = shadow_level\n    deepdrive_simulation.set_graphics_settings(settings)\n\n\ndef randomize_shadow_level():\n    level = c.rng.choice(SHADOW_RANGE)\n    log.info(\'Setting new shadow quality level (%r/%r)\', level, max(SHADOW_RANGE))\n    set_capture_graphics(shadow_level=level)\n'"
sim/gym_env.py,2,"b'import platform\nimport shutil\n\n# noinspection PyUnresolvedReferences\nimport traceback\nfrom typing import List\n\nimport config.check_bindings\n\n\nfrom future.builtins import (int, open, round,\n                             str)\nimport csv\nimport os\nimport random\nimport sys\nimport time\nfrom collections import OrderedDict\nfrom multiprocessing import Process\nfrom subprocess import Popen\nimport pkg_resources\nfrom distutils.version import LooseVersion as semvar\n\nimport arrow\nimport gym\nimport numpy as np\nfrom GPUtil import GPUtil\nfrom gym import spaces\nfrom gym.utils import seeding\nimport deepdrive_client\nimport deepdrive_capture\nimport deepdrive_simulation\n\nimport config as c\nimport logs\nimport sim\nimport util.ensure_sim\nimport util.run_command\nfrom recorder.recorder import Recorder\nimport utils\nfrom sim import world, graphics\nfrom sim.action import Action, DiscreteActions\nfrom sim.graphics import set_capture_graphics\nfrom sim.reward_calculator import RewardCalculator\nfrom sim.return_aggregator import EpisodeReturn, TotalReturn\nfrom sim.view_mode import ViewModeController\nfrom sim import DrivingStyle\nfrom renderer import renderer_factory, base_renderer\nfrom util import ensure_sim\nfrom util.anonymize import anonymize_user_home\nfrom util.sampler import Sampler\nfrom utils import obj2dict\nfrom dashboard import dashboard_fn, DashboardPub\n\nlog = logs.get_log(__name__)\nSPEED_LIMIT_KPH = 64.\nHEAD_START_TIME = 3\nLAP_LENGTH = 2736.7\n\n\n# noinspection PyMethodMayBeStatic\nclass DeepDriveEnv(gym.Env):\n    metadata = {\'render.modes\': [\'human\']}\n\n    def __init__(self):\n        # Whether to use discrete or continuous actions\n        self.is_discrete:bool = None\n\n        # Whether to pause the sim between steps\n        self.is_sync:bool = None\n\n        # The amount of time to pause Unreal\n        self.sim_step_time:float = None\n\n        self.discrete_actions:DiscreteActions = None\n        self.preprocess_with_tensorflow:bool = None\n        self.sess = None\n        self.start_time:float = time.time()\n        self.step_num:int = 0\n        self.prev_step_time:float = None\n        self.display_stats: OrderedDict = self.init_display_stats()\n        self.dashboard_process:Process = None\n        self.dashboard_pub:DashboardPub = None\n        self.should_exit:bool = False\n        self.sim_process = None\n        self.client_id:int = None\n        self.has_control:bool = None\n        self.cameras:List[dict] = None\n        self.use_sim_start_command:bool = None\n        self.connection_props:dict = None\n        self.should_render:bool = False\n        self.ep_time_balance_coeff:float = 10.\n        self.previous_action_time:time.time = None\n\n        # The FPS you wish to run the agent at, including sim + agent step time\n        self.target_fps:float = None\n\n        self.fps_tracker:Sampler = Sampler()\n\n        # Inverse of target_fps\n        self.period:float = None\n\n        self.experiment:str = None\n        self.driving_style:DrivingStyle = None\n        self.reset_returns_zero:bool = None\n        self.started_driving_wrong_way_time:bool = None\n        self.previous_distance_along_route:float = None\n        self.renderer:base_renderer.Renderer = None\n        self.np_random:tuple = None\n        self.last_obz:dict = None\n        self.view_mode_controller:ViewModeController = None\n        self.enable_traffic:bool = False\n        self.ego_mph:float = None\n        self.max_steps:int = None\n        self.max_episodes: int = None\n        self.should_close: bool = False\n        self.recorder:Recorder = None\n        self.image_resize_dims:np.ndarray = None\n        self.should_normalize_image:bool = False\n        self.tried_to_close:bool = False\n        self.scenario_index: int = c.DEFAULT_SCENARIO_INDEX\n        self.unreal_map: str = \'\'\n\n        if not c.REUSE_OPEN_SIM and not self._try_connect():\n            util.ensure_sim.ensure_sim()\n\n        self.client_version = pkg_resources.get_distribution(\n            c.BINDINGS_PACKAGE_NAME).version\n\n        # collision detection  # TODO: Remove in favor of in-game detection\n        self.set_forward_progress()\n\n        self.distance_along_route:float = 0\n        self.start_distance_along_route:float = None\n        self.previous_distance_along_route:float = 0\n\n        # reward\n        self.episode_return:EpisodeReturn = EpisodeReturn()\n        self.total_return:TotalReturn = TotalReturn()\n\n        # laps\n        self.lap_number = None\n        self.prev_lap_return = 0\n        self.total_laps = 0\n\n        # domain randomization\n        self.randomize_sun_speed: bool = False\n        self.randomize_shadow_level: bool = False\n        self.randomize_month: bool = False\n\n        self.is_botleague: bool = False\n\n        self.episode_returns:List[EpisodeReturn] = []\n\n        try:\n            self.git_commit = str(util.run_command.run_command(\n                \'git rev-parse --short HEAD\')[0])\n        except:\n            self.git_commit = \'n/a\'\n            log.warning(\'Could not get git commit for associating benchmark \'\n                        \'results with code state\')\n\n        try:\n            self.git_diff = util.run_command.run_command(\'git diff\')[0]\n        except:\n            self.git_diff = None\n            log.warning(\'Could not get git diff for associating benchmark \'\n                        \'results with code state\')\n\n        if c.TENSORFLOW_AVAILABLE:\n            import tensorflow as tf\n            self.tensorboard_writer = tf.summary.FileWriter(c.TF_ENV_EVENT_DIR)\n        else:\n            self.tensorboard_writer = None\n\n    def init_display_stats(self) -> OrderedDict:\n        disp_stats = OrderedDict()\n        disp_stats[\'g-forces\']                = {\'total\': 0, \'value\': 0, \'ymin\': 0,      \'ymax\': 3,    \'units\': \'g\'}\n        # disp_stats[\'gforce penalty\']          = {\'total\': 0, \'value\': 0, \'ymin\': -500,   \'ymax\': 0,    \'units\': \'\'}\n        # disp_stats[\'lane deviation penalty\']  = {\'total\': 0, \'value\': 0, \'ymin\': -100,   \'ymax\': 0,    \'units\': \'\'}\n        disp_stats[\'lap progress\']            = {\'total\': 0, \'value\': 0, \'ymin\': 0,      \'ymax\': 100,  \'units\': \'%\'}\n        # disp_stats[\'speed reward\']            = {\'total\': 0, \'value\': 0, \'ymin\': 0,      \'ymax\': 5000, \'units\': \'\'}\n        disp_stats[\'episode #\']               = {\'total\': 0, \'value\': 0, \'ymin\': 0,      \'ymax\': 5,    \'units\': \'\'}\n        disp_stats[\'time\']                    = {\'total\': 0, \'value\': 0, \'ymin\': 0,      \'ymax\': 500,  \'units\': \'s\'}\n        # disp_stats[\'episode score\']           = {\'total\': 0, \'value\': 0, \'ymin\': -500,   \'ymax\': 4000, \'units\': \'\'}\n        return disp_stats\n\n    def open_sim(self):\n        self._kill_competing_procs()\n        if c.REUSE_OPEN_SIM:\n            return\n        if self.use_sim_start_command:\n            self.sim_start_cmd_windows()\n        else:\n            sim_bin_path = util.ensure_sim.get_sim_bin_path()\n            parts = sim_bin_path.split(os.path.sep)\n            sim_version = [p for p in parts if p.startswith(c.SIM_PREFIX)]\n            if not sim_version:\n                raise RuntimeError(f\'{sim_bin_path} is too old, please run \'\n                                   f\'install.py to update your sim\')\n            sim_version = semvar(sim_version[0].split(\'-\')[-1])\n            cmd = [sim_bin_path, self.unreal_map]\n            if self.unreal_map != \'\' and self.has_control is None:\n                self.has_control = True\n                if self.scenario_index == -1:\n                    # Hack to work around issue where scenario needs to be set\n                    # for manual  mode\n                    self.scenario_index = 2\n                cmd.append(\'-remote_ai\')\n            if self.scenario_index != c.DEFAULT_SCENARIO_INDEX:\n                min_kevindale_version = semvar(\'3.0.20191103201659\')\n                if sim_version < min_kevindale_version:\n                    raise RuntimeError(\n                        f\'{sim_bin_path} is too old to run this map, please run \'\n                                   f\'install.py to update your sim\')\n                cmd += [\'-scenario_mode\',\n                        f\'-scenario_index={self.scenario_index}\']\n            if not c.IS_WINDOWS:\n                cmd.append(\'-opengl4\')\n            cmd += [\'-ResX=1280\', \'-ResY=720\', \'-WINDOWED\', \'-WinX=-1280\']\n            if log.getEffectiveLevel() < 20:  # More verbose than info (i.e. debug)\n                cmd += \' -LogCmds=""LogPython Verbose, LogSharedMemoryImpl_Linux VeryVerbose, LogDeepDriveAgent VeryVerbose""\'\n\n            self.sim_process = Popen(cmd)\n            log.info(\'Starting simulator with %s \'\n                     \'(takes a few seconds the first time).\', cmd)\n\n    def sim_start_cmd_windows(self):\n        log.info(\'Starting simulator with command %s - this will take a \'\n                 \'few seconds.\', c.SIM_START_COMMAND)\n        self.sim_process = Popen(c.SIM_START_COMMAND)\n        import win32gui\n        import win32process\n        def get_hwnds_for_pid(pid):\n            def callback(hwnd, _hwnds):\n                if win32gui.IsWindowVisible(hwnd) and win32gui.IsWindowEnabled(\n                        hwnd):\n                    _, found_pid = win32process.GetWindowThreadProcessId(hwnd)\n                    if found_pid == pid:\n                        _hwnds.append(hwnd)\n                return True\n\n            hwnds = []\n            win32gui.EnumWindows(callback, hwnds)\n            return hwnds\n\n        focused = False\n        while not focused:\n            time.sleep(1)\n            dd_hwnds = get_hwnds_for_pid(self.sim_process.pid)\n            if not dd_hwnds:\n                log.info(\'No windows found, waiting\')\n            else:\n                try:\n                    win32gui.SetForegroundWindow(dd_hwnds[0])\n                    focused = True\n                except:\n                    log.info(\'Window not ready, waiting\')\n\n    def close_sim(self):\n        log.info(\'Closing sim\')\n        self.connection_props = None\n        process_to_kill = self.sim_process\n        if process_to_kill is not None:\n            if utils.kill_process(process_to_kill):\n                log.info(\'Sim closed\')\n\n    def _kill_competing_procs(self):\n        # TODO: Allow for many environments on the same machine by using\n        #  registry DB for this and sharedmem\n        path = util.ensure_sim.get_sim_bin_path()\n        if path is None:\n            return\n        process_name = os.path.basename(util.ensure_sim.get_sim_bin_path())\n        if c.IS_WINDOWS:\n            cmd = \'taskkill /IM %s /F\' % process_name\n        elif c.IS_LINUX or c.IS_MAC:\n            cmd = \'pkill %s\' % process_name\n        else:\n            raise NotImplementedError(\'OS not supported\')\n        util.run_command.run_command(cmd, verbose=False, throw=False, print_errors=False)\n        time.sleep(1)\n        # TODO: Don\'t rely on time for shared mem to go away,\n        #  we should have a unique name on startup.\n\n    def set_use_sim_start_command(self, use_sim_start_command):\n        self.use_sim_start_command = use_sim_start_command\n\n    def init_benchmarking(self):\n        os.makedirs(c.RESULTS_DIR, exist_ok=True)\n\n    def start_dashboard(self):\n        if utils.is_debugging():\n            # TODO: Deal with plot UI not being in the main thread somehow -\n            #  (move to Unreal HUD)\n            log.warning(\'Dashboard not supported in debug mode\')\n            return\n        elif utils.is_docker():\n            # TODO: Move dashboard stats to Unreal / Tensorboard\n            #  where appropriate\n            log.warning(\'Dashboard not supported in docker\')\n            return\n\n        p = Process(target=dashboard_fn)\n        p.daemon = True\n        p.start()\n        self.dashboard_process = p\n        self.dashboard_pub = DashboardPub()\n\n    def set_tf_session(self, session):\n        self.sess = session\n\n    def step(self, action):\n        try:\n            ret = self.inner_step(action)\n        except Exception as e:\n            self.close()\n            raise e\n        return ret\n\n    def inner_step(self, action):\n        if self.surpassed_max_episodes():\n            ret = None, 0, True, {\'should_close\': True}\n        else:\n            dd_action = self.get_dd_action(action)\n            send_control_start = time.time()\n            self.send_control(dd_action)\n            log.debug(\'send_control took %fs\',\n                      time.time() - send_control_start)\n\n            obz = self.get_observation()\n            if not obz:\n                log.debug(\'Observation not available - step %r\', self.step_num)\n\n            self.last_obz = obz\n            if self.should_render:\n                self.render()\n\n            now = time.time()\n\n            done, reward = self.get_reward_timed(now, obz)\n            self.prev_step_time = now\n            self.publish_to_dashboard()\n            self.step_num += 1\n\n            # Info is just for OpenAI baselines.\n            # Info is not recorded in hdf5 logs, so\n            # don\'t put anything there that isn\'t in the observation if you want\n            # it to be in the drive logs.\n            info = self.init_step_info()\n            if done:\n                self.report_return(info)\n\n            if obz is not None:\n                obz[\'episode_return\'] = self.episode_return.serialize()\n\n            self.regulate_fps()\n            self.view_mode_controller.step(self.client_id)\n            obz = self.postprocess_obz(obz)\n\n            if self.recorder is not None:\n                self.recorder.step(obz, done, reward,\n                                   dd_action,\n                                   is_agent_action=dd_action.has_control)\n\n            # TODO: Remove things that we don\'t want botleague agents to be able\n            #   to use.\n\n            ret = obz, reward, done, info\n        return ret\n\n    def get_reward_timed(self, now, obz):\n        start_reward_stuff = time.time()\n        reward, done = self.get_reward(obz, now)\n        log.debug(\'reward stuff took %fs\', time.time() - start_reward_stuff)\n        return done, reward\n\n    def publish_to_dashboard(self):\n        if self.dashboard_pub is not None:\n            start_dashboard_put = time.time()\n            self.dashboard_pub.put(OrderedDict(\n                {\'display_stats\': list(self.display_stats.items()),\n                 \'should_stop\': False}))\n            log.debug(\n                \'dashboard put took %fs\', time.time() - start_dashboard_put)\n\n    def get_dd_action(self, action):\n        if self.is_discrete:\n            steer, throttle, brake = self.discrete_actions.get_components(\n                action)\n            dd_action = Action(steering=steer, throttle=throttle, brake=brake)\n        else:\n            dd_action = Action.from_gym(action)\n        if self.is_botleague and not dd_action.has_control:\n            raise RuntimeError(\'Cannot use built-in AI on Botleague\')\n        return dd_action\n\n    def init_step_info(self):\n        """"""From https://gym.openai.com/docs/\n        info (dict): diagnostic information useful for debugging.\n            It can sometimes be useful for learning (for example,\n            it might contain the raw probabilities behind the environment\xe2\x80\x99s\n            last state change). However, official evaluations of your agent\n            are not allowed to use this for learning.\n\n        \'episode_time\' was added here in order to comply with OpenAI baselines\n        PPO2.\n\n        Info is just for OpenAI baselines. Info is not recorded in hdf5 logs, so\n        don\'t put anything there that isn\'t in the observation if you want\n        it to be in the drive logs.\n\n        We can selectively remove items from the observation if we want to hide\n        them from agents but still log them. However, until driving is amazing\n        in the simulator, we can just let agent developers decide what info\n        is interesting to them.\n        """"""\n        info = {}\n        info[\'episode_return\'] = info.get(\'episode\', {})\n        info[\'episode_return\'][\'episode_time\'] = self.episode_return.episode_time\n\n        return info\n\n    def report_return(self, info):\n        self.prev_lap_return = self.episode_return.total\n\n        # For OpenAI baselines\n        info[\'episode\'] = episode_info = {}\n        episode_info[\'reward\'] = self.episode_return.total\n        episode_info[\'length\'] = self.step_num\n        episode_info[\'time\'] = self.episode_return.episode_time\n\n        self.finalize_reward()\n        if self.tensorboard_writer is not None:\n            import tensorflow as tf\n            summary = tf.Summary()\n            summary.value.add(tag=""reward/total"", simple_value=self.episode_return.total)\n            summary.value.add(tag=""reward/episode_length"", simple_value=self.step_num)\n            summary.value.add(tag=""reward/episode_time"", simple_value=self.episode_return.episode_time)\n            summary.value.add(tag=""reward/speed_reward"", simple_value=self.episode_return.speed_reward)\n\n            summary.value.add(tag=""reward/lane_deviation_penalty"", simple_value=self.episode_return.lane_deviation_penalty)\n            summary.value.add(tag=""reward/gforce_penalty"", simple_value=self.episode_return.gforce_penalty)\n            summary.value.add(tag=""reward/got_stuck"", simple_value=self.episode_return.got_stuck)\n            summary.value.add(tag=""reward/wrong_way"", simple_value=self.episode_return.wrong_way)\n            summary.value.add(tag=""reward/time_penalty"", simple_value=self.episode_return.time_penalty)\n\n            self.tensorboard_writer.add_summary(summary)\n            self.tensorboard_writer.flush()\n        return info\n\n    def regulate_fps(self):\n        now = time.time()\n        log.debug(\'in regulate_fps\')\n        if self.previous_action_time:\n            delta = now - self.previous_action_time\n            fps = 1. / max(delta, 1E-9)\n            self.fps_tracker.sample(fps)\n            log.debug(\'step duration delta actual %f desired %f\', delta, self.period)\n            if delta < self.period:\n                # Simulator step was quicker than we want\n                if not self.is_sync:\n                    sleep_time = max(0., self.period - delta - 0.001)\n                    log.debug(\'regulating fps by sleeping for %f\', sleep_time)\n                    time.sleep(sleep_time)\n                    # TODO: Change capture FPS within Unreal\n                    #  so that sleep is not needed here.\n            else:\n                log.debug(\'step longer than desired\')\n                if self.step_num > 5 and fps < self.target_fps / 2:\n                    log.warning(\'Step %r took %rs - target is %rs\',\n                                self.step_num, delta, 1 / self.target_fps)\n        self.previous_action_time = now\n\n    def compute_lap_statistics(self, obz):\n        start_compute_lap_stats = time.time()\n        lap_bonus = 0\n        done = False\n        if obz:\n            lap_number = obz.get(\'lap_number\')\n            lap_via_progress = self.episode_return.progress_pct > 99.9\n            if lap_via_progress:\n                median_meters_per_sec = self.episode_return.cm_per_second_sampler.mean() / 100\n                est_travel_cm = median_meters_per_sec * self.episode_return.episode_time * 100  # cm travelled\n                took_shortcut = self.get_took_shortcut(est_travel_cm, obz)\n                if took_shortcut:\n                    log.warn(\'Shortcut detected, not scoring lap\')\n                else:\n                    lap_bonus = 10\n                self.total_laps += 1\n                done = True  # One lap per episode\n                log.info(\'episode finished, trip complete\')\n                self.log_up_time()\n            self.lap_number = lap_number\n            log.debug(\'compute lap stats took %fs\',\n                      time.time() - start_compute_lap_stats)\n\n        return done, lap_bonus\n\n    def get_took_shortcut(self, est_travel_cm, obz):\n        if self.unreal_map == \'\':\n            # Route is one loop around track, so this is an easy cheat\n            return est_travel_cm < (obz[\'route_length\'] * 0.9)\n        else:\n            return False\n\n    def get_reward(self, obz, now):\n        start_get_reward = time.time()\n        done = False\n        lap_done, lap_bonus = self.compute_lap_statistics(obz)\n        reward = 0\n        gforce_done = False\n        episode_return = self.episode_return\n        if obz:\n            if self.is_sync:\n                step_time = self.period\n            elif self.prev_step_time is not None:\n                step_time = now - self.prev_step_time\n            else:\n                step_time = None\n\n            episode_return.episode_time += (step_time or 0)\n\n            if episode_return.episode_time < HEAD_START_TIME:\n                # Give time to get on track after spawn\n                reward = 0\n            else:\n                gforce_penalty, gforce_done = self.get_gforce_penalty(\n                    obz, step_time)\n                lane_deviation_penalty = self.get_lane_deviation_penalty(\n                    obz, step_time)\n                time_penalty = self.get_time_penalty(obz, step_time)\n                progress_reward, speed_reward = \\\n                    self.get_progress_and_speed_reward(\n                        obz, step_time, gforce_penalty, lane_deviation_penalty)\n                reward = self.combine_rewards(\n                    progress_reward, gforce_penalty, lane_deviation_penalty,\n                    time_penalty, speed_reward)\n\n            episode_return.wrong_way = self.driving_wrong_way()\n            if episode_return.wrong_way:\n                log.warn(\'episode finished, going the wrong way\')\n\n            if self.is_stuck(obz) or episode_return.wrong_way:\n                done = True\n                reward -= 10\n                # TODONT: Scale cost by collision momentum when speed is returned\n                #  We now use g-force as a better proxy for this.\n                # if obz[\'last_collision\'].time_utc:\n                #     reward *= obz[\'last_collision\'].speed\n\n            episode_return.cm_per_second_sampler.sample(obz[\'speed\'])\n            episode_return.total += reward + lap_bonus\n\n            last_collision = obz[\'last_collision\']\n            if last_collision[\'collidee_velocity\'].any():\n                episode_return.collided_with_vehicle = True\n            elif last_collision[\'time_stamp\']:\n                episode_return.collided_with_non_actor = True\n\n            # TODO: Add missing components from https://docs.google.com/document/d/1_M4d7KTTzWRV6NthQjINl68gfl_NdGKF41za91-7O7A/edit#\n            #   like running traffic lights, crossing double yellows, etc...\n            self.check_closest_vehicle(obz, episode_return)\n\n            self.display_stats[\'time\'][\'value\'] = episode_return.episode_time\n            self.display_stats[\'time\'][\'total\'] = episode_return.episode_time\n            # self.display_stats[\'episode reward\'][\'value\'] = episode_return.total\n            # self.display_stats[\'episode reward\'][\'total\'] = episode_return.total\n\n            log.debug(\'reward %r\', reward)\n            log.debug(\'return %r\', episode_return.total)\n            log.debug(\'progress %r\', episode_return.progress_pct)\n            log.debug(\'throttle %f\', obz[\'throttle\'])\n            log.debug(\'steering %f\', obz[\'steering\'])\n            log.debug(\'brake %f\', obz[\'brake\'])\n            log.debug(\'handbrake %f\', obz[\'handbrake\'])\n\n        log.debug(\'get reward took %fs\', time.time() - start_get_reward)\n\n        steps_done = self.step_num == self.max_steps\n\n        if steps_done:\n            log.info(\'Ending episode due to max steps (%r)\' % self.max_steps)\n\n        done = done or lap_done or gforce_done or steps_done\n\n        return reward, done\n\n    def check_closest_vehicle(self, obz, episode_return):\n        closest_vehicle_cm, _veh_index = \\\n            utils.nearest_neighbor(\n                np.array(obz[\'position\']),\n                np.array(obz[\'world\'][\'vehicle_positions\']),\n            )\n        episode_return.closest_vehicle_cm = \\\n            min(closest_vehicle_cm, episode_return.closest_vehicle_cm)\n        if (obz[\'speed\'] * c.CMPS_TO_KPH) >= 4:\n            # Why 4kph: http://sparebumper.com/federal-bumper-standards/\n            episode_return.closest_vehicle_cm_while_at_least_4kph = min(\n                closest_vehicle_cm,\n                episode_return.closest_vehicle_cm_while_at_least_4kph)\n\n    def log_up_time(self):\n        log.info(\'up for %r\' % arrow.get(time.time()).humanize(\n            other=arrow.get(self.start_time), only_distance=True))\n\n    def get_lane_deviation_penalty(self, obz, time_passed):\n        lane_deviation_penalty = 0.\n        if \'distance_to_center_of_lane\' in obz:\n            lane_deviation = obz[\'distance_to_center_of_lane\']\n            self.episode_return.max_lane_deviation_cm = max(\n                self.episode_return.max_lane_deviation_cm, lane_deviation)\n            lane_deviation_penalty = RewardCalculator.get_lane_deviation_penalty(\n                lane_deviation, time_passed)\n\n        lane_deviation_penalty *= self.driving_style.value.lane_deviation_weight\n        self.episode_return.lane_deviation_penalty += lane_deviation_penalty\n\n        # self.display_stats[\'lane deviation penalty\'][\'value\'] = -lane_deviation_penalty\n        # self.display_stats[\'lane deviation penalty\'][\'total\'] = -self.episode_return.lane_deviation_penalty\n        return lane_deviation_penalty\n\n    def get_gforce_penalty(self, obz, time_passed):\n        gforce_penalty = 0\n        done = False\n        episode_return = self.episode_return\n\n        if \'acceleration\' in obz:\n            if time_passed is not None:\n                a = obz[\'acceleration\']\n                gs = np.sqrt(a.dot(a)) / 980  # g = 980 cm/s**2\n                episode_return.max_gforce = \\\n                    max(gs, episode_return.max_gforce)\n                self.total_return.max_gforce = \\\n                    max(gs, self.total_return.max_gforce)\n                sampler = episode_return.gforce_sampler\n                sampler.sample(gs)\n                three_second_avg = self.average_gs(sampler, secs=3)\n                if gs > 1 or three_second_avg > 1:\n                    # https://www.quora.com/How-many-Gs-do-we-feel-driving-a-car\n                    episode_return.harmful_gforces = True\n                    # Game over\n                    log.warn(\'G-force limit exceeded, game over. \'\n                             \'Recent g\\\'s were: %r\',\n                             list(reversed(list(sampler.q)[-10:])))\n                    done = True\n                elif gs > 0.4:\n                    # https://www.quora.com/How-many-Gs-do-we-feel-driving-a-car\n                    episode_return.jarring_gforce_seconds += time_passed\n                if gs > 0.1:\n                    # Based on regression model on page 47 - can achieve 92/100 comfort with 0.15 x and y acceleration\n                    # http://www.diva-portal.org/smash/get/diva2:950643/FULLTEXT01.pdf\n                    # Perhaps we should combine x, 0.15 and y 0.15\n                    episode_return.uncomfortable_gforce_seconds += time_passed\n\n                self.display_stats[\'g-forces\'][\'value\'] = gs\n                self.display_stats[\'g-forces\'][\'total\'] = gs\n                gforce_penalty = RewardCalculator.get_gforce_penalty(\n                    gs, time_passed)\n\n        gforce_penalty *= self.driving_style.value.gforce_weight\n        episode_return.gforce_penalty += gforce_penalty\n\n        # self.display_stats[\'gforce penalty\'][\'value\'] = -episode_return.gforce_penalty\n        # self.display_stats[\'gforce penalty\'][\'total\'] = -episode_return.gforce_penalty\n        return gforce_penalty, done\n\n    def get_progress_and_speed_reward(self, obz, time_passed, gforce_penalty,\n                                      lane_deviation_penalty):\n        progress_reward = speed_reward = 0\n        if \'distance_along_route\' in obz:\n            if self.start_distance_along_route is None:\n                if self.unreal_map == \'\':\n                    self.start_distance_along_route = obz[\'distance_along_route\']\n                else:\n                    self.start_distance_along_route = 0\n            if obz[\'distance_along_route\'] < self.start_distance_along_route:\n                dist = (obz[\'route_length\'] - self.start_distance_along_route) +\\\n                       obz[\'distance_along_route\']\n            else:\n                dist = obz[\'distance_along_route\'] - self.start_distance_along_route\n            progress_cm = dist - self.distance_along_route\n            if self.distance_along_route:\n                self.previous_distance_along_route = self.distance_along_route\n            self.distance_along_route = dist\n            progress_reward, speed_reward, meters_per_second = \\\n                RewardCalculator.get_progress_and_speed_reward(\n                    progress_cm, time_passed)\n            kph = obz[\'speed\'] * c.CMPS_TO_KPH\n            self.episode_return.max_kph = max(kph, self.episode_return.max_kph)\n            self.episode_return.max_kph = max(kph, self.episode_return.max_kph)\n            self.episode_return.prev_progress_pct = self.episode_return.progress_pct\n            if obz[\'route_length\']:\n                self.episode_return.progress_pct = \\\n                    100 * self.distance_along_route / obz[\'route_length\']\n            else:\n                self.episode_return.progress_pct = 0\n            self.episode_return.cm_along_route = self.distance_along_route\n            self.episode_return.route_length_cm = obz[\'route_length\']\n\n        progress_reward *= self.driving_style.value.progress_weight\n        speed_reward *= self.driving_style.value.speed_weight\n\n        if self.episode_return.episode_time < 2:\n            # Speed reward is too big at reset due to small offset between\n            # origin and spawn, so clip it to avoid incenting resets\n            speed_reward = min(max(speed_reward, -1), 1)\n            progress_reward = min(max(progress_reward, -1), 1)\n\n        if gforce_penalty > 0 or lane_deviation_penalty > 0:\n            # Discourage making up for penalties by going faster\n            speed_reward /= 2\n            progress_reward /= 2\n\n        self.episode_return.progress_reward += progress_reward\n        self.episode_return.speed_reward += speed_reward\n\n        self.display_stats[\'lap progress\'][\'total\'] = self.episode_return.progress_pct or 0\n        self.display_stats[\'lap progress\'][\'value\'] = self.display_stats[\'lap progress\'][\'total\']\n        self.display_stats[\'episode #\'][\'total\'] = self.total_laps\n        self.display_stats[\'episode #\'][\'value\'] = self.total_laps\n        # self.display_stats[\'speed reward\'][\'total\'] = self.episode_return.speed_reward\n        # self.display_stats[\'speed reward\'][\'value\'] = self.episode_return.speed_reward\n\n        return progress_reward, speed_reward\n\n    def get_time_penalty(self, _obz, time_passed):\n        time_penalty = time_passed or 0\n        time_penalty *= self.driving_style.value.time_weight\n        self.episode_return.time_penalty += time_penalty\n        return time_penalty\n\n    def combine_rewards(self, progress_reward, gforce_penalty,\n                        lane_deviation_penalty, time_penalty, speed):\n        return self.driving_style.value.combine(progress_reward, gforce_penalty,\n                                                lane_deviation_penalty,\n                                                time_penalty, speed)\n\n    def is_stuck(self, obz):\n        start_is_stuck = time.time()\n\n        # TODO: Get this from the game instead\n        ret = False\n        if \'TEST_END_OF_EPISODE\' in os.environ and self.step_num >= 9:\n            log.warn(\'TEST_END_OF_EPISODE is set triggering end of episode\'\n                     \' via is_stuck!\')\n            ret = True\n        elif obz is None:\n            log.debug(\'obz is None, not checking if stuck\')\n            ret = False\n        elif obz[\'speed\'] < 100:  # cm/s\n            log.debug(\'speed less than 1m/s, checking if stuck\')\n            self.steps_crawling += 1\n            if obz[\'throttle\'] > 0 and obz[\'brake\'] == 0 and obz[\'handbrake\'] == 0:\n                self.steps_crawling_with_throttle_on += 1\n                log.debug(\'crawling detected num steps crawling is %d\', self.steps_crawling_with_throttle_on)\n            else:\n                log.debug(\'not stuck, throttle %f, brake %f, handbrake %f\',\n                          obz[\'throttle\'], obz[\'brake\'], obz[\'handbrake\'])\n\n            time_crawling = time.time() - self.last_not_stuck_time\n\n            # This was to detect legitimate stops, but we will have real\n            # collision detection before the need to stop\n            # portion_crawling = self.steps_crawling_with_throttle_on / max(1, self.steps_crawling)\n\n            if self.steps_crawling_with_throttle_on > 40 and time_crawling > 5:\n                log.warn(\'No progress made while throttle on - \'\n                         \'assuming stuck and ending episode. \'\n                         \'steps crawling: %r, \'\n                         \'steps crawling with throttle on: %r, \'\n                         \'time crawling: %r\',\n                         self.steps_crawling,\n                         self.steps_crawling_with_throttle_on, time_crawling)\n                self.set_forward_progress()\n                self.episode_return.got_stuck = True\n                ret = True\n        else:\n            log.debug(\'speed greater than 1m/s, not stuck\')\n            self.set_forward_progress()\n        log.debug(\'is stuck took %fs\', time.time() - start_is_stuck)\n\n        if ret:\n            log.info(\'episode finished, detected we were stuck\')\n\n        return ret\n\n    def finalize_reward(self):\n        self.episode_return.end_time = time.time()\n        self.episode_return.num_steps = self.step_num\n\n        self.total_return.num_steps += self.step_num\n        log.info(\'episode time %r\', self.episode_return.episode_time)\n        self.episode_returns.append(self.episode_return)\n        self.total_return.update(self.episode_returns)\n        log.info(\'benchmark lap #%d reward: %f - average: %f\',\n                 len(self.episode_returns), self.episode_return.total,\n                 self.total_return.average)\n        file_prefix = self.experiment + \'_\' if self.experiment else \'\'\n        diff_filename = \'%s%s.diff\' % (file_prefix, c.DATE_STR)\n        diff_filepath = os.path.join(c.RESULTS_DIR, diff_filename)\n\n        if self.git_diff is not None:\n            with open(diff_filepath, \'w\', encoding=\'utf-8\') as diff_file:\n                diff_file.write(self.git_diff)\n\n        self.write_result_csvs(self.total_return.average, diff_filename,\n                               self.total_return.high, self.total_return.low,\n                               self.total_return.median, self.total_return.std)\n        summary_file = anonymize_user_home(\n            os.path.normpath(c.EPISODES_CSV_FILENAME))\n        episodes_file = anonymize_user_home(\n            os.path.normpath(c.EPISODES_CSV_FILENAME))\n        log.info(\'median reward %r\', self.total_return.median)\n        log.info(\'avg reward %r\', self.total_return.average)\n        log.info(\'std %r\', self.total_return.std)\n        log.info(\'high reward %r\', self.total_return.high)\n        log.info(\'low reward %r\', self.total_return.low)\n        log.info(\'progress_reward %r\',\n                 self.episode_return.progress_reward)\n        log.info(\'speed_reward %r\',\n                 self.episode_return.speed_reward)\n        log.info(\'lane_deviation_penalty %r\',\n                 self.episode_return.lane_deviation_penalty)\n        log.info(\'time_penalty %r\',\n                 self.episode_return.time_penalty)\n        log.info(\'gforce_penalty %r\',\n                 self.episode_return.gforce_penalty)\n        log.info(\'episode_time %r\',\n                 self.episode_return.episode_time)\n        log.info(\'wrote results to %s and %s\', summary_file, episodes_file)\n\n    def write_result_csvs(self, average, diff_filename, high, low, median, std):\n        import io\n        episodes_io = io.StringIO()\n        summary_io = io.StringIO()\n\n        writer = csv.writer(episodes_io)\n        # Write headers\n\n        headers = [\'episode #\']\n        # Relying on dir() sorting alphabetically\n        headers += list(EpisodeReturn().serialize().keys())\n        writer.writerow(headers)\n        for i, episode_return in enumerate(self.episode_returns):\n            episode_row = [i+1]\n            episode_row += episode_return.serialize().values()\n            writer.writerow(episode_row)\n\n        if self.recorder.main_args:\n            main_args = str(self.recorder.main_args)\n        else:\n            main_args = \'\'\n        writer = csv.writer(summary_io)\n        writer.writerow([\'Stat\', \'Value\'])\n        writer.writerow([\'median reward\', median])\n        writer.writerow([\'avg reward\', average])\n        writer.writerow([\'std\', std])\n        writer.writerow([\'high reward\', high])\n        writer.writerow([\'low reward\', low])\n        writer.writerow([\'env\', self.spec.id])\n        writer.writerow([\'cmd args\', \', \'.join(sys.argv[1:])])\n        writer.writerow([\'main args\', main_args])\n        writer.writerow([\'git commit\', \'@\' + self.git_commit])\n        writer.writerow([\'git diff\', diff_filename])\n        writer.writerow([\'experiment name\', self.experiment or \'n/a\'])\n        writer.writerow([\'run id\', c.RUN_ID])\n\n        writer.writerow([\'os\', self.get_os_version()])\n        try:\n            gpus = \',\'.join([gpu.name for gpu in GPUtil.getGPUs()])\n        except:\n            gpus = \'n/a\'\n        writer.writerow([\'gpus\', gpus])\n\n        episodes_str = episodes_io.getvalue()\n        summary_str = summary_io.getvalue()\n\n        episodes_str = anonymize_user_home(episodes_str)\n        summary_str = anonymize_user_home(summary_str)\n        with open(c.EPISODES_CSV_FILENAME, \'w\', newline=\'\') as episodes_out:\n            episodes_out.write(episodes_str)\n        with open(c.SUMMARY_CSV_FILENAME, \'w\', newline=\'\') as summary_out:\n            summary_out.write(summary_str)\n\n    def get_os_version(self):\n        os_version = platform.platform()\n        if c.IS_LINUX:\n            try:\n                lsb_release = util.run_command.run_command(\'lsb_release -a\')[0].split()\n                os_version = \' \'.join(lsb_release + [os_version])\n            except:\n                log.debug(\'Could not get os version from lsb_release\')\n        return os_version\n\n    def release_agent_control(self):\n        res = deepdrive_client.release_agent_control(self.client_id)\n        self.has_control = res is not None\n\n    def request_agent_control(self):\n        res = deepdrive_client.request_agent_control(self.client_id)\n        self.has_control = res == 1\n\n    # noinspection PyAttributeOutsideInit\n    def set_forward_progress(self):\n        self.last_not_stuck_time = time.time()\n        self.steps_crawling_with_throttle_on = 0\n        self.steps_crawling = 0\n\n    def reset(self):\n        self.reset_agent()\n        self.step_num = 0\n        self.distance_along_route = 0\n        self.previous_distance_along_route = 0\n        self.start_distance_along_route = None\n        self.prev_step_time = None\n        self.episode_return = EpisodeReturn()\n        self.start_time = time.time()\n        self.started_driving_wrong_way_time = None\n        self.view_mode_controller.reset()\n\n        # Create domain randomization controller for these\n\n        if self.randomize_sun_speed:\n            world.randomize_sun_speed()\n        if self.randomize_shadow_level:\n            graphics.randomize_shadow_level()\n        if self.randomize_month:\n            world.randomize_sun_month()\n\n        log.info(\'Reset complete\')\n        if self.reset_returns_zero:\n            # TODO: Always return zero after testing that everything\n            #  works with dagger agents\n            return 0\n\n    def change_viewpoint(self, cameras, use_sim_start_command):\n        # TODO: Delete this method\n        self.use_sim_start_command = use_sim_start_command\n        deepdrive_capture.close()\n        deepdrive_client.close(self.client_id)\n        self.client_id = 0\n        self.close_sim()  # Need to restart process now to change cameras\n        self.open_sim()\n        self.connect(cameras)\n\n    def close(self):\n        # Only try to close things once, i.e. if __del__ is called after close()\n        if self.tried_to_close:\n            return\n        else:\n            self.tried_to_close = True\n\n        if self.recorder is not None:\n            self.recorder.close(total_return=self.total_return,\n                                episode_returns=self.episode_returns,\n                                median_fps=self.fps_tracker.median(), )\n        if self.dashboard_pub is not None:\n            try:\n                self.dashboard_pub.put({\'should_stop\': True})\n                time.sleep(0.25)  # Give time for message to be received\n            except Exception as e:\n                log.error(\'Error closing dashboard. %s\', e)\n                # print(traceback.format_exc())\n            self.dashboard_pub.close()\n        log.info(\'Closed dashboard\')\n        if self.dashboard_process is not None:\n            self.dashboard_process.join(timeout=.25)\n        if self.renderer is not None:\n            self.renderer.close()\n        if self.is_sync:\n            deepdrive_client.deactivate_synchronous_stepping(self.client_id)\n\n        if self.has_control:\n            deepdrive_client.release_agent_control(self.client_id)\n\n        deepdrive_capture.close()\n        deepdrive_client.close(self.client_id)\n        deepdrive_simulation.disconnect()\n        self.client_id = 0\n\n        # TODO: Test the we can re-enable session closing\n        #  Sessions were kept open as a hack to change cameras by reopening the sim\n        # if self.sess:\n        #     self.sess.close()\n\n        self.close_sim()\n\n    def render(self, mode=\'human\', close=False):\n        """"""\n        We pass the obz through an instance variable to comply with\n        the gym api where render() takes 0 arguments\n        """"""\n        if self.last_obz:\n            self.renderer.render(self.last_obz)\n\n    def seed(self, seed=None):\n        self.np_random = seeding.np_random(seed)\n        # TODO: Generate random actions with this seed\n\n    def preprocess_observation(self, obz):\n        def has_cams(_obs):\n            return getattr(_obs, \'cameras\', None) is not None\n        if obz and len(obz.cameras[0].image_data):\n            ret = obj2dict(obz, exclude=[\'cameras\'])\n            if obz.camera_count > 0 and has_cams(obz):\n                cameras = obz.cameras\n                ret[\'cameras\'] = self.preprocess_cameras(cameras)\n            else:\n                ret[\'cameras\'] = []\n            ret[\'view_mode\'] = self.view_mode_controller.current_mode_name()\n            if ret[\'last_collision\']:\n                ret[\'last_collision\'] = obj2dict(ret[\'last_collision\'])\n        else:\n            if obz and len(obz.cameras[0].image_data) == 0:\n                log.warn(\'No camera data received - nulling observation\')\n            ret = None\n        return ret\n\n    def postprocess_obz(self, obz):\n        """"""\n        Perform normalization and resizing for use with neural nets\n        """"""\n        if not self.should_normalize_image:\n            if self.image_resize_dims:\n                raise NotImplementedError(\'Resize without normalizing not \'\n                                          \'implemented\')\n        else:\n            if obz is None:\n                return\n            for camera in obz[\'cameras\']:\n                start = time.time()\n                image = camera[\'image\']\n                image = image.astype(np.float32)\n                image -= c.MEAN_PIXEL\n                if self.image_resize_dims is not None:\n                    resize_start = time.time()\n                    image = utils.resize_images(self.image_resize_dims,\n                                                [image], always=True)[0]\n                    log.debug(\'resize took %fs\', time.time() - resize_start)\n                camera[\'image\'] = image\n                log.debug(\'preprocess_obz took %fs\',  time.time() - start)\n        return obz\n\n    def preprocess_cameras(self, cameras):\n        ret = []\n        for camera in cameras:\n            image = camera.image_data.reshape(camera.capture_height,\n                                              camera.capture_width, 3)\n            depth = camera.depth_data.reshape(camera.capture_height,\n                                              camera.capture_width)\n            start_preprocess = time.time()\n            if self.preprocess_with_tensorflow:\n                import tf_utils  # avoid hard requirement on tensorflow\n                if self.sess is None:\n                    raise Exception(\n                        \'No tensorflow session. Did you call set_tf_session?\')\n                # This runs ~2x slower (18ms on a gtx 980) than\n                # CPU when we are not running a model due to\n                # transfer overhead, but we do it anyway to keep\n                # training and testing as similar as possible.\n                image = tf_utils.preprocess_image(image, self.sess)\n                depth = tf_utils.preprocess_depth(depth, self.sess)\n            else:\n                image = utils.preprocess_image(image)\n                depth = utils.preprocess_depth(depth)\n\n            end_preprocess = time.time()\n            log.debug(\'preprocess took %rms\',\n                      (end_preprocess - start_preprocess) * 1000.)\n            camera_out = obj2dict(camera, exclude=[\'image\', \'depth\'])\n            camera_out[\'image\'] = image\n            if self.should_render:\n                # Keep copy of image without mean subtraction etc\n                # that agent does through side effect on this mutable sensor\n                # data that gets passed around\n                camera_out[\'image_raw\'] = image\n            camera_out[\'depth\'] = depth\n            ret.append(camera_out)\n        return ret\n\n    def get_observation(self):\n        start_get_obz = time.time()\n        disable_uepy_obz = True\n        try:\n            shared_mem_obz = deepdrive_capture.step()\n            log.debug(\'get shared mem took %fs\', time.time() - start_get_obz)\n            if disable_uepy_obz:\n                # TODO: Find out why even no-op RPC\'s are so slow on larger maps\n                uepy_obz = c.EMPTY_UEPY_OBZ\n            else:\n                uepy_obz = sim.world.get_observation()\n            log.debug(\'get obz took %fs\', time.time() - start_get_obz)\n        except SystemError as e:\n            log.error(\'caught error during step\' + str(e))\n            ret = None\n        else:\n            ret = self.preprocess_observation(shared_mem_obz)\n            if ret is not None and self.unreal_map != \'\':\n                # New maps are offset by 2m as a buffer\n                if ret[\'route_length\'] and ret[\'route_length\'] > (10 * 100):\n                    ret[\'route_length\'] -= (2 * 100)\n            if ret is not None and uepy_obz[\'success\']:\n                ret[\'world\'] = uepy_obz[\'result\']\n\n        log.debug(\'completed capture step\')\n        return ret\n\n    def reset_agent(self):\n        # TODO: Scenario config here:\n        # if self.scenario:\n        #     world.configure(scenario)\n        # else:\n\n        if self.unreal_map == \'\':\n            # Not valid on new maps?\n            world.reset(enable_traffic=self.enable_traffic)\n\n        if self.ego_mph is not None and self.unreal_map == \'\':\n            # Not valid on new maps\n            world.set_ego_mph(self.ego_mph, self.ego_mph)\n\n    def send_control(self, action):\n        if self.has_control != action.has_control:\n            self.change_has_control(action.has_control)\n\n        if action.handbrake:\n            log.debug(\'Not expecting any handbraking right now! \'\n                      \'What\\\'s happening?! Disabling - hack :D\')\n            action.handbrake = False\n\n        action.clip()\n\n        if self.is_sync:\n            sync_start = time.time()\n            seq_number = deepdrive_client.advance_synchronous_stepping(\n                self.client_id, self.sim_step_time, action.steering,\n                action.throttle, action.brake, action.handbrake)\n            log.debug(\'sync step took %fs - sim_step_time set to %fs\',\n                      time.time() - sync_start, self.sim_step_time)\n        else:\n            if c.PPO_RESUME_PATH:\n                # Hack to deal with sync vs async\n                action.throttle = action.throttle * 0.90\n            deepdrive_client.set_control_values(\n                self.client_id, steering=action.steering,\n                throttle=action.throttle, brake=action.brake,\n                handbrake=action.handbrake)\n\n    def set_step_mode(self):\n        if self.is_sync:\n            ret = deepdrive_client.activate_synchronous_stepping(self.client_id)\n            if ret != 1:\n                raise RuntimeError(\n                    \'Could not activate synchronous mode. \'\n                    \'Is there another sim open? errno %r\' % ret)\n\n    def check_version(self):\n        self.client_id = self.connection_props[\'client_id\']\n        server_version_str = self.connection_props[\'server_protocol_version\']\n        if not server_version_str:\n            log.warn(\'Server version not reported. \'\n                     \'Can not check version compatibility.\')\n        else:\n            server_version = semvar(server_version_str).version\n            # TODO: For dev, store hash of .cpp and .h files on extension build inside VERSION_DEV, then when\n            #   connecting, compute same hash and compare. (Need to figure out what to do on dev packaged version as\n            #   files may change - maybe ignore as it\'s uncommon).\n            #   Currently, we timestamp the build, and set that as the version in the extension. This is fine unless\n            #   you change shared code and build the extension only, then the versions won\'t change, and you could\n            #   see incompatibilities.\n            if c.MAJOR_MINOR_VERSION != server_version[:2]:\n                self.deal_with_server_version_mismatch(server_version_str)\n                return False\n        return True\n\n    def deal_with_server_version_mismatch(self, server_version_str):\n        self.close_sim()\n        log.error(\n            \'Server client version mismatch server@%s client@%s - closed sim\' %\n            (server_version_str, self.client_version))\n        sim_url = util.ensure_sim.get_latest_sim_url()\n        if sim_url:\n            answer = input(\'We\\\'ve found a version of the sim which matches your \'\n                           \'client. Would you like to download it now? [y/n] \')\n            if answer.lower().strip() == \'y\':\n                backup_dir = os.path.join(\n                    c.DEEPDRIVE_DIR,\n                    \'%s-%s\' % (c.SIM_PREFIX, server_version_str))\n                log.warn(\'Backing up old sim to %s\', backup_dir)\n                shutil.move(util.ensure_sim.get_sim_path(), backup_dir)\n                util.ensure_sim.ensure_sim()\n                self.open_sim()\n\n    def connect(self, cameras=None):\n        self._connect_with_retries()\n\n        if cameras is None:\n            cameras = [c.DEFAULT_CAM]\n\n        if self.client_id and self.client_id > 0:\n            self.register_cameras(cameras)\n            shared_mem = deepdrive_client.get_shared_memory(self.client_id)\n            self.reset_capture(shared_mem[0], shared_mem[1])\n            if not deepdrive_simulation.connect(\'127.0.0.1\', 9009,\n                                                seed=c.rng.randint(1, 10**9)):\n                raise RuntimeError(\n                    \'Could not connect to Deepdrive simulation server\')\n            self._init_observation_space()\n        else:\n            log.error(\'Invalid client_id of ""%s"". Expected a value greater \'\n                      \'than zero. Aborting connection\',\n                      str(self.client_id))\n            self.raise_connect_fail()\n\n        if config.IS_DEBUG_MODE:\n            log.warning(\'Turning off render in debug mode - doesn\\\'t do well \'\n                        \'with multiple processes.\')\n            self.should_render = False\n        if self.should_render:\n            self.renderer = renderer_factory(cameras=cameras)\n\n        self._perform_first_step()\n\n    def register_cameras(self, cameras):\n        for cam in cameras:\n            cam[\'cxn_id\'] = deepdrive_client.register_camera(\n                self.client_id,\n                cam[\'field_of_view\'],\n                cam[\'capture_width\'],\n                cam[\'capture_height\'],\n                cam[\'relative_position\'],\n                cam[\'relative_rotation\'],\n                cam[\'name\'])\n\n            if \'view_mode\' in cam:\n                self.view_mode_controller.set_view_mode(cam[\'view_mode\'],\n                                                        cam_id=cam[\'cxn_id\'])\n\n            self.cameras = cameras\n\n    def _try_connect(self) -> bool:\n        try:\n            self.connection_props = deepdrive_client.create(\n                \'127.0.0.1\', 9876)\n            if isinstance(self.connection_props, int):\n                raise Exception(\n                    \'You have an old version of the deepdrive client - \'\n                    \'try uninstalling and reinstalling with pip\')\n            if (not self.connection_props or\n                    not self.connection_props[\'max_capture_resolution\']):\n                # Try again\n                return False\n            self.check_version()\n\n        except deepdrive_client.time_out:\n            self._try_connect()\n        return True\n\n    def _connect_with_retries(self):\n        if self.connection_props:\n            ensure_sim.check_pyarrow_compat()\n            log.info(\'Connecting to an already open sim\')\n            self.unregister_cameras()\n            for _client_id in range(1, self.connection_props[\'client_id\']):\n                try:\n                    deepdrive_client.deactivate_synchronous_stepping(_client_id)\n                except deepdrive_client.client_doesnt_exist:\n                    pass\n        cxn_attempts = 0\n        max_cxn_attempts = 10\n        while not self.connection_props:\n            cxn_attempts += 1\n            if cxn_attempts == 1:\n                log.info(\'No open sim detected\')\n                self.open_sim()\n            else:\n                # splay to avoid thundering herd\n                sleep = cxn_attempts + random.random() * 2\n                log.warning(\'Connection to environment failed, \'\n                            \'retry (%d/%d) in %d seconds\',\n                            cxn_attempts,\n                            max_cxn_attempts, round(sleep, 0))\n                time.sleep(sleep)\n                self._try_connect()\n            if cxn_attempts >= max_cxn_attempts:\n                raise RuntimeError(\'Could not connect to the environment\')\n\n    def _perform_first_step(self):\n        obz = None\n        read_obz_count = 0\n        while obz is None:\n            if read_obz_count > 10:\n                error_msg = \'Failed first step of environment\'\n                log.error(error_msg)\n                raise RuntimeError(error_msg)\n            try:\n                obz = deepdrive_capture.step()\n            except Exception as e:\n                traceback.print_stack()\n                log.error(\'caught error during step\' + str(e))\n\n            time.sleep(0.25 * read_obz_count)\n            read_obz_count += 1\n\n    def reset_capture(self, shared_mem_name, shared_mem_size):\n        n = 10\n        sleep = 0.1\n        log.debug(\'Connecting to deepdrive...\')\n        while n > 0:\n            # TODO: Establish some handshake so we don\'t hardcode size\n            #  here and in Unreal project\n            if deepdrive_capture.reset(shared_mem_name, shared_mem_size):\n                log.debug(\'Connected to deepdrive shared capture memory\')\n                return\n            n -= 1\n            sleep *= 2\n            log.debug(\'Sleeping %r\', sleep)\n            time.sleep(sleep)\n        log.error(\'Could not connect to deepdrive capture memory at %s\', shared_mem_name)\n        self.raise_connect_fail()\n\n    @staticmethod\n    def raise_connect_fail():\n        log.error(\'Environment connection failed\')\n        if c.SIM_START_COMMAND or c.REUSE_OPEN_SIM:\n            raise Exception(\n                \'Could not connect to environment. \'\n                \'You may need to close the Unreal Editor and/or turn off \'\n                \'saving CPU in background in the Editor preferences \'\n                \'(search for CPU).\')\n        else:\n            raise Exception(\'\\n\\n\\n\'\n                        \'**********************************************************************\\n\'\n                        \'**********************************************************************\\n\'\n                        \'****                                                              ****\\n\\n\'\n                        \'|               Could not connect to the environment.                |\\n\\n\'\n                        \'****                                                              ****\\n\'\n                        \'**********************************************************************\\n\'\n                        \'**********************************************************************\\n\\n\')\n\n    def init_action_space(self):\n        if self.is_discrete:\n            num_steer_steps = 30\n            steer_step = 1 / ((num_steer_steps - 1) / 2)\n\n            # Fermi estimate of a good discretization\n            steer = list(np.arange(-1, 1 + steer_step, steer_step))\n            throttle = [0, 0.01, 0.02, 0.04, 0.08, 0.16, 0.32, 0.64, 0.8, 1]\n            brake = [0, 0.1, 0.3, 0.7, 1]\n\n            self.discrete_actions = DiscreteActions(steer, throttle, brake)\n\n            learned_space = spaces.Discrete(len(self.discrete_actions.product))\n            is_game_driving_space = spaces.Discrete(2)\n\n            # action_space = spaces.Tuple(\n            #     (learned_space, is_game_driving_space))\n\n            action_space = learned_space\n\n        else:\n            # TODO(PPO)\n            # I think what we need to do here is normalize the brake and\n            # handbrake space to be between -1 and 1 so\n            # that all actions have the same dimension -\n            # Then create a single box space. The is_game_driving space\n            # can be ignored for now within the ppo agent.\n            steering_space = spaces.Box(\n                low=Action.STEERING_MIN, high=Action.STEERING_MAX, shape=(1,),\n                dtype=np.float32)\n            throttle_space = spaces.Box(\n                low=Action.THROTTLE_MIN, high=Action.THROTTLE_MAX, shape=(1,),\n                dtype=np.float32)\n            brake_space = spaces.Box(\n                low=Action.BRAKE_MIN, high=Action.BRAKE_MAX, shape=(1,),\n                dtype=np.float32)\n            handbrake_space = spaces.Box(\n                low=Action.HANDBRAKE_MIN, high=Action.HANDBRAKE_MAX, shape=(1,),\n                dtype=np.float32)\n            is_game_driving_space = spaces.Discrete(2)\n            action_space = spaces.Tuple(\n                (steering_space, throttle_space, brake_space, handbrake_space,\n                 is_game_driving_space))\n        self.action_space = action_space\n        return action_space\n\n    def _init_observation_space(self):\n        if len(self.cameras) > 1:\n            log.warning(\n                \'\\n\\n\\n MULTIPLE CAMERAS OBSERVATION SPACE RETURNS TUPLE - \'\n                \'YOU MAY WANT TO IMPLEMENT BETTER SUPPORT DEPENDING ON HOW YOUR \'\n                \'AGENT COMBINES CAMERA VIEWS \\n\\n\\n\')\n\n            obz_spaces = []\n            for camera in self.cameras:\n                obz_spaces.append(spaces.Box(\n                    low=0, high=255,\n                    shape=(camera[\'capture_width\'],\n                           camera[\'capture_height\']), dtype=np.uint8))\n            observation_space = spaces.Tuple(tuple(obz_spaces))\n            self.observation_space = observation_space\n            return observation_space\n        else:\n            camera = self.cameras[0]\n            self.observation_space = spaces.Box(\n                low=0, high=255,\n                shape=(camera[\'capture_width\'], camera[\'capture_height\']),\n                dtype=np.uint8)\n\n    def change_has_control(self, has_control):\n        if self.unreal_map != \'\':\n            self.has_control = has_control\n            return\n        elif has_control:\n            self.request_agent_control()\n        else:\n            self.release_agent_control()\n\n    def driving_wrong_way(self):\n        if None in [self.previous_distance_along_route,\n                    self.distance_along_route]:\n            return False\n\n        if self.distance_along_route < self.previous_distance_along_route:\n            now = time.time()\n            s = self.started_driving_wrong_way_time\n            if s is not None:\n                if (now - s) > 5:\n                    return True\n            else:\n                self.started_driving_wrong_way_time = now\n\n        else:\n            self.started_driving_wrong_way_time = None\n        return False\n\n    def change_cameras(self, cameras):\n        self.unregister_cameras()\n        self.register_cameras(cameras)\n\n    def unregister_cameras(self):\n        if not deepdrive_client.unregister_camera(self.client_id, 0):\n            # 0 => Unregister all\n            raise RuntimeError(\'Not able to unregister cameras\')\n        self.cameras = None\n\n    def average_gs(self, gforce_sampler, secs):\n        total = 0\n        steps = secs * self.target_fps\n        if len(gforce_sampler.q) < steps:\n            return 0\n        for i in range(steps):\n            total += gforce_sampler.q[-i]\n        avg = total / steps\n        return avg\n\n    def surpassed_max_episodes(self) -> bool:\n        if self.max_episodes is not None and \\\n                len(self.episode_returns) >= self.max_episodes:\n            log.info(\'Max episodes of %r reached.\', self.max_episodes)\n            self.should_close = True\n            return True\n        return False\n\n\n'"
sim/return_aggregator.py,0,"b""import math\nfrom typing import List\n\nimport arrow\nimport numpy as np\n\n\nimport time\n\nfrom util.sampler import Sampler\nimport utils\nimport config as c\n\n\nclass EpisodeReturn(object):\n    total = 0\n    gforce_penalty: float = 0\n    max_gforce: float = 0\n    max_kph: float = 0\n    avg_kph: float = 0\n    lane_deviation_penalty: float = 0\n    time_penalty: float = 0\n    progress_reward: float = 0\n    speed_reward: float = 0\n    progress_pct: float = 0\n    prev_progress_pct: float = 0\n    got_stuck = False\n    wrong_way = False\n    start_time: float = 0\n    end_time: float = 0\n    episode_time: float = 0  # seconds\n    num_steps: int = 0\n    cm_along_route: float = 0\n    route_length_cm: float = 0\n    collided_with_vehicle: bool = False\n    collided_with_non_actor: bool = False\n    closest_vehicle_cm: float = math.inf\n    closest_vehicle_cm_while_at_least_4kph: float = math.inf\n    max_lane_deviation_cm: float = 0\n    uncomfortable_gforce_seconds: float = 0\n    jarring_gforce_seconds: float = 0\n    harmful_gforces: bool = False\n\n    def __init__(self):\n        self.start_time = time.time()\n        self.end_time = 0\n        self.episode_time = 0\n        self.cm_per_second_sampler = Sampler()\n        self.gforce_sampler = Sampler()\n\n    def serialize(self):\n        defaults = utils.obj2dict(EpisodeReturn)\n        prop_names = defaults.keys()\n        ret = {}\n        for k in prop_names:\n            v = getattr(self, k, defaults[k])\n            if k in ['start_time', 'end_time']:\n                v = str(arrow.get(v).to('local'))\n            ret[k] = v\n        return ret\n\n\nclass TotalReturn(object):\n    median: float\n    average: float\n    high: float\n    low: float\n    std: float\n    num_episodes: int = 0\n    num_steps: int = 0\n    max_gforce: float = 0\n    max_kph: float = 0\n    avg_kph: float = 0\n    trip_speed_kph: float = 0\n    collided_with_vehicle: bool = False\n    collided_with_non_actor: bool = False\n    closest_vehicle_cm: float = math.inf\n    closest_vehicle_cm_while_at_least_4kph: float = math.inf\n    max_lane_deviation_cm: float = 0\n\n    def update(self, episode_returns:List[EpisodeReturn]):\n        totals = [e.total for e in episode_returns]\n        self.median = float(np.median(totals))\n        self.average = float(np.mean(totals))\n        self.high = float(max(totals))\n        self.low = float(min(totals))\n        self.std = float(np.std(totals))\n        self.num_episodes = len(episode_returns)\n        cm_per_second_means = \\\n            [e.cm_per_second_sampler.mean() for e in episode_returns]\n        cm_per_second_avg = float(np.mean(cm_per_second_means))\n        self.avg_kph = cm_per_second_avg * c.CMPS_TO_KPH\n        trip_cm_per_second = float(np.mean(\n            [e.cm_along_route / e.episode_time for e in episode_returns]))\n        self.trip_speed_kph = trip_cm_per_second * c.CMPS_TO_KPH\n        self.max_kph = max([e.max_kph for e in episode_returns])\n        self.collided_with_vehicle = any(e.collided_with_vehicle for e in\n                                         episode_returns)\n        self.collided_with_non_actor = any(e.collided_with_non_actor for e\n                                           in episode_returns)\n        self.closest_vehicle_cm = min(\n            [e.closest_vehicle_cm for e in episode_returns])\n        self.closest_vehicle_cm_while_at_least_4kph = min(\n            [e.closest_vehicle_cm_while_at_least_4kph for e in episode_returns])\n\n        self.max_lane_deviation_cm = max(\n            [e.max_lane_deviation_cm for e in episode_returns])\n\n\ndef main():\n    episode_return = EpisodeReturn()\n    from utils import obj2dict\n    now = time.time()\n    ser = obj2dict(episode_return)\n    took = time.time() - now\n    print('took %f s' % took)\n    print(ser)\n\n\nif __name__ == '__main__':\n    main()\n"""
sim/reward_calculator.py,0,"b""from __future__ import (absolute_import, division,\n                        print_function, unicode_literals)\n\nimport numpy as np\nimport logs\nlog = logs.get_log(__name__)\n\nclass RewardCalculator(object):\n    @staticmethod\n    def clip(reward):\n        # time_passed not parameter in order to set hard limits on reward magnitude\n        return min(max(reward, -1e2), 1e2)\n\n    @staticmethod\n    def get_lane_deviation_penalty(lane_deviation, time_passed):\n        lane_deviation_penalty = 0\n        if lane_deviation < 0:\n            raise ValueError('Lane deviation should be positive')\n        if time_passed is not None and lane_deviation > 200:  # Tuned for Canyons spline - change for future maps\n            lane_deviation_coeff = 0.1\n            lane_deviation_penalty = lane_deviation_coeff * time_passed * lane_deviation ** 2 / 100.\n        log.debug('distance_to_center_of_lane %r', lane_deviation)\n        lane_deviation_penalty = RewardCalculator.clip(lane_deviation_penalty)\n        return lane_deviation_penalty\n\n    @staticmethod\n    def get_gforce_penalty(gforces, time_passed):\n        log.debug('gforces %r', gforces)\n        gforce_penalty = 0\n        if gforces < 0:\n            raise ValueError('G-Force should be positive')\n        if gforces > 0.1:\n            # Based on regression model on page 47 - can achieve 92/100 comfort with 0.15 x and y acceleration\n            # http://www.diva-portal.org/smash/get/diva2:950643/FULLTEXT01.pdf\n            # Perhaps we should combine x, 0.15 and y 0.15,\n            time_weighted_gs = time_passed * gforces\n            time_weighted_gs = min(time_weighted_gs,\n                                   5)  # Don't allow a large frame skip to ruin the approximation\n            balance_coeff = 24  # 24 meters of reward every second you do this\n            gforce_penalty = time_weighted_gs * balance_coeff\n            log.debug('accumulated_gforce %r', time_weighted_gs)\n            log.debug('gforce_penalty %r', gforce_penalty)\n        gforce_penalty = RewardCalculator.clip(gforce_penalty)\n        return gforce_penalty\n\n    @staticmethod\n    def get_progress_and_speed_reward(progress_cm, time_passed):\n        if not time_passed:\n            progress_cm = speed_reward = meters_per_second = 0\n        else:\n            progress_cm = progress_cm / 100.  # cm=>meters\n\n            # TODO: meters per second can be off by around 1.5x - not sure why\n            #  yet. Possibly more time elapses in the sim than we are asking for\n            #  in sync mode.\n            meters_per_second = progress_cm / time_passed\n            if meters_per_second < -400:\n                # Trip completed\n                log.debug('assuming trip complete, progress zero')\n                progress_cm = meters_per_second = 0\n\n            # Square the speed to greatly outweigh the advantage\n            # of getting more rewards by going slower.\n            speed_reward = np.sign(meters_per_second) * meters_per_second ** 2 * time_passed\n\n        progress_balance_coeff = 1.0\n        progress_reward = progress_cm * progress_balance_coeff\n\n        speed_balance_coeff = 0.15\n        speed_reward *= speed_balance_coeff\n\n        progress_reward = RewardCalculator.clip(progress_reward)\n        speed_reward = RewardCalculator.clip(speed_reward)\n        return progress_reward, speed_reward, meters_per_second\n\n\n"""
sim/sim_args.py,0,"b""from typing import Tuple, List\n\nfrom sim import DrivingStyle\n\nimport config as c\n\n\nclass SimArgs:\n    experiment: str = None\n    env_id: str = 'Deepdrive-v0'\n    sess = None  # tensorflow Session\n    start_dashboard: bool = True\n    cameras: List[dict] = None\n    use_sim_start_command: bool = False\n    render: bool = False\n    fps: int = c.DEFAULT_FPS\n    combine_box_action_spaces: bool = False\n    is_discrete: bool = False\n    preprocess_with_tensorflow:bool = False\n    is_sync: bool = False\n    driving_style: str = DrivingStyle.NORMAL.as_string()\n    is_remote_client: bool = False\n    enable_traffic: bool = False\n    ego_mph: float = None\n    view_mode_period: int = None  # TODO: Change to view_mode_period_ms\n    max_steps: int = None\n    max_episodes: int = None\n    should_record: bool = False\n    recording_dir: str = c.RECORDING_DIR\n    image_resize_dims: Tuple[int] = None\n    should_normalize_image: bool = True\n    reset_returns_zero: bool = True  # TODO: Change once dagger agents confirmed working with True\n    eval_only: bool = False\n    upload_gist: bool = False\n    public: bool = False\n    client_main_args: dict = None\n    sim_step_time: float = c.DEFAULT_SIM_STEP_TIME\n    randomize_view_mode: bool = False\n    randomize_sun_speed: bool = False\n    randomize_shadow_level: bool = False\n    randomize_month: bool = False\n    is_botleague: bool = False\n    scenario_index: int = c.DEFAULT_SCENARIO_INDEX\n    map: str = c.CANYONS_MAP_NAME\n    path_follower: bool = False\n\n    def __init__(self, **kwargs):\n        for k in kwargs:\n            if not hasattr(self, k):\n                raise RuntimeError('Invalid sim start arg: %s' % k)\n            setattr(self, k, kwargs[k])\n\n    def get_vars(self):\n        for (k, v) in vars(self).items():\n            if not k.startswith('__'):\n                yield k, v\n\n    def to_dict(self):\n        ret = {k: v for k,v in self.get_vars()}\n        return ret\n\n\n"""
sim/uepy_client.py,0,"b'from __future__ import (absolute_import, division,\n                        print_function, unicode_literals)\n\nimport sys\nimport time\n\nimport zmq\nimport pyarrow\n\nimport logs\nfrom utils import sizeof_fmt\n\nlog = logs.get_log(__name__)\n\nAPI_PORT = 5657\n\n\nclass UEPyClient(object):\n    """"""\n    Call Unreal API via RPC  -\n    Deepdrive sim API: http://bit.ly/2I1XB5u\n    Unreal+Python integration: https://github.com/20tab/UnrealEnginePython\n    """"""\n\n    def __init__(self, **kwargs):\n        self.socket = None\n        self.last_obz = None\n        self.create_socket()\n        # self._send(m.START, kwargs=kwargs)\n\n    def call(self, method: str, *args, **kwargs):\n        """"""\n        Eval expressions against Unreal Python API\n        :param method API method to execute\n        :param args - tuple or list of args to pass\n        :param kwargs - dict of kwargs to pass\n        :return: dict of form {\'success\': <False iff exception thrown>,\n         \'result\': <eval(...) return value>}\n        """"""\n        ret = None\n        try:\n            start_serialize = time.time()\n            msg = pyarrow.serialize([method, args, kwargs]).to_buffer()\n\n            start_send = time.time()\n            self.socket.send(msg)\n            log.debug(\'send took %r\' % (time.time() - start_send))\n\n            start_receive = time.time()\n            resp = self.socket.recv()\n            log.debug(\'receive took %r\', (time.time() - start_receive))\n\n            size_formatted = sizeof_fmt(sys.getsizeof(resp))\n            log.debug(\'receive size was %s\', size_formatted)\n\n            start_deserialize = time.time()\n            ret = pyarrow.deserialize(resp)\n            log.debug(\'deserialize took %r\', (time.time() - start_deserialize))\n        except zmq.error.Again:\n            print(\'Waiting for uepy server\')\n            self.create_socket()\n            return None\n        finally:\n            if ret is None:\n                raise RuntimeError(\n                    \'Could not get response from uepy server. \'\n                    \'Ensure your Arrow/pyarrow versions are compatible, and/or \'\n                    \'try restarting sim or Unreal Editor. \')\n            if not ret[\'success\']:\n                log.error(ret[\'result\'])\n                raise RuntimeError(\n                    \'Error executing %s(%s, %s) in Unreal - \'\n                    \'Traceback above\' % (method, str(args), str(kwargs)))\n            return ret\n\n    def create_socket(self):\n        if self.socket:\n            self.socket.close()\n        context = zmq.Context()\n        socket = context.socket(zmq.PAIR)\n\n        # Creating a new socket on timeout is not working when other ZMQ\n        # connections are present in the process.\n        socket.RCVTIMEO = 5000\n        # socket.SNDTIMEO = c.API_TIMEOUT_MS\n\n        socket.connect(""tcp://localhost:%s"" % API_PORT)\n        self.socket = socket\n        return socket\n\n    def close(self):\n        self.socket.close()\n\n\n# TODO: Don\'t use a global singleton client\nCLIENT = None\n\n\ndef rpc(method_name, *args, **kwargs):\n    """"""Calls a method defined via api_methods.py in deepdrive-sim and run via\n    the UnrealEnginePython embedded python interpreter""""""\n    global CLIENT\n    if CLIENT is None:\n        CLIENT = UEPyClient()\n\n    start_call = time.time()\n    ret = CLIENT.call(method_name, *args, **kwargs)\n    log.debug(\'rpc %s call took %r\', method_name, (time.time() - start_call))\n    return ret\n\n\ndef main():\n    answer = rpc(\'get_42\')\n    print(\'UnrealEnginePython evaluated answer to \', answer)\n\n    answer = rpc(\'get_world\')\n    print(\'UnrealEnginePython evaluated answer to \', answer)\n\n\nif __name__ == \'__main__\':\n    main()\n'"
sim/view_mode.py,0,"b""from __future__ import (absolute_import, division,\n                        print_function, unicode_literals)\n\n\nfrom future.builtins import (int, open, round,\n                             str)\n\nfrom enum import Enum\n\nimport deepdrive_client\nimport config as c\n\n\nclass ViewMode(Enum):\n    NORMAL = ''\n    WORLD_NORMAL = 'WorldNormal'\n    BASE_COLOR = 'BaseColor'\n    ROUGHNESS = 'Roughness'\n    REFLECTIVITY = 'Reflectivity'\n    AMBIENT_OCCLUSION = 'AmbientOcclusion'\n    DEPTH_HEAT_MAP = 'HeatMap'\n    SPECULARITY = 'Specularity'\n\n\nclass ViewModeController(object):\n    def __init__(self, period=None, client_id=None):\n        self.client_id = None\n        self.period = period\n        self.client_id = client_id\n        self.steps_since_switch = None\n        self.modes = list(ViewMode.__members__.values())\n        self.num_modes = len(self.modes)\n        self.current = ViewMode.NORMAL\n        self.view_index = None\n        self.reset()\n\n    def step(self, client_id):\n        self.client_id = client_id\n        if self.should_switch():\n            self.view_index += 1\n            if self.view_index >= self.num_modes:\n                self.view_index = 0\n            self.set_view_mode(self.modes[self.view_index])\n            self.steps_since_switch = 0\n        else:\n            self.steps_since_switch += 1\n\n    def should_switch(self):\n        if self.period is not None:\n            return self.steps_since_switch == (self.period - 1)\n        else:\n            return False\n\n    def reset(self):\n        self.steps_since_switch = 0\n        self.view_index = 0\n\n    def current_mode_name(self):\n        return self.current.name.lower()\n\n    def set_view_mode(self, view_mode, cam_id=-1):\n        # Passing a cam_id of -1 sets all cameras with the same view mode\n        if self.client_id is None:\n            raise RuntimeError('Client id not set. HINT: Call env.step() '\n                               'at least once before setting view mode')\n        deepdrive_client.set_view_mode(self.client_id, cam_id, view_mode.value)\n        self.current = view_mode\n\n    def set_random(self):\n        self.set_view_mode(c.rng.choice(list(ViewMode.__members__.values())))\n\n"""
sim/world.py,0,"b'from __future__ import (absolute_import, division,\n                        print_function, unicode_literals)\n\nimport time\nfrom typing import List\n\nimport numpy as np\nfrom future.builtins import (int, open, round,\n                             str)\n\nimport deepdrive_simulation\n\nimport logs\nimport config as c\nfrom sim.uepy_client import rpc\n\nlog = logs.get_log(__name__)\n\n\ndef sun_speed(speed):\n    """"""Seconds of sun rotation per second of simulation time""""""\n    if not np.issubdtype(type(speed), np.integer):\n        raise ValueError(\'Expected integer sun speed, got %r\', type(speed))\n    if not (0 < speed <= 10 ** 6):\n        raise ValueError(\'Sun speed not between zero and 10M, was %r\', speed)\n\n    deepdrive_simulation.set_sun_simulation_speed(int(speed))\n\n\ndef randomize_sun_speed():\n    sun_speeds = np.array([[1,        0.5,  \'Earth (1 second per second)\'],\n                           [1000,     0.3,  \'Rolling shadows 1\'],\n                           [2000,     0.11, \'Rolling shadows 2\'],\n                           [10 ** 4,  0.05, \'Time lapse\'],\n                           [10 ** 5,  0.02, \'Rave\'],\n                           [10 ** 6,  0.02, \'Strobe\']])\n\n    probs = list(sun_speeds[:, 1].astype(np.float32))\n    speeds = sun_speeds[:, 0].astype(np.int)\n    rand_sun_speed = c.rng.choice(speeds, 1, probs)[0]\n    sun_speed(rand_sun_speed)\n\n\ndef randomize_sun_month():\n    deepdrive_simulation.set_date_and_time(month=c.rng.choice(list(range(1, 13))))\n\n\ndef reset(enable_traffic=False):\n    return rpc(\'reset\', enable_traffic=enable_traffic)\n\n\ndef set_ego_mph(min_mph, max_mph):\n    return rpc(\'set_ego_mph\', min_mph=min_mph, max_mph=max_mph)\n\n\ndef get_agent_positions() -> List[list]:\n    """"""\n    :return: Positions of non-ego agents\n    """"""\n    start = time.time()\n    ret = rpc(\'get_agent_positions\')\n    t = time.time() - start\n    log.debug(\'get_agent_positions() took %rs\' % t)\n    return ret\n\n\ndef get_agents() -> List[dict]:\n    """"""\n    :return: Detailed list of all agents in the scene\n    See: https://gist.github.com/crizCraig/b5f88911cae9dc346bf805498f31ec3f\n    """"""\n    start = time.time()\n    ret = rpc(\'get_agents\')\n    t = time.time() - start\n    log.debug(\'get_agents() took %rs\' % t)\n    return ret\n\n\ndef get_observation() -> dict:\n    """"""\n    This is a combined call to the UEPy server to get all the data needed\n    for the step. The UEPy API server can only return one response per frame,\n    so at 60FPS latency will be minimum 17ms\n    :return: Info appropriate to return every step. \n    """"""""""\n    ret = rpc(\'get_observation\')\n    return ret\n'"
tests/__init__.py,0,b''
util/__init__.py,0,b''
util/anonymize.py,0,"b'from __future__ import (absolute_import, division,\n                        print_function, unicode_literals)\n\nfrom future.builtins import (ascii, bytes, chr, dict, filter, hex, input,\n                             int, map, next, oct, open, pow, range, round,\n                             str, super, zip)\n\n\nimport os\n\n\ndef anonymize_user_home(data):\n    if not isinstance(data, str):\n        return data\n    else:\n        return data.replace(os.path.expanduser(""~""), \'~\')\n'"
util/args.py,0,"b'import argparse\n\n\nclass Args:\n    all: argparse.ArgumentParser\n    agent: argparse.ArgumentParser\n\n    def __init__(self):\n        self.all = argparse.ArgumentParser()\n        self.agent = argparse.ArgumentParser()\n\n    def add(self, *args, **kwargs):\n        self.all.add_argument(*args, **kwargs)\n\n    def add_agent_arg(self, *args, **kwargs):\n        self.agent.add_argument(*args, **kwargs)\n        self.all.add_argument(*args, **kwargs)'"
util/download.py,0,"b'from __future__ import (absolute_import, division,\n                        print_function, unicode_literals)\n\nimport os\nimport tempfile\nimport zipfile\n\nimport requests\nfrom clint.textui import progress\n\nfrom future.builtins import (ascii, bytes, chr, dict, filter, hex, input,\n                             int, map, next, oct, open, pow, range, round,\n                             str, super, zip)\n\nimport config as c\nimport logs\n\nlog = logs.get_log(__name__)\n\n\ndef download(url, directory, warn_existing=True, overwrite=False):\n    """"""\n    Download and unzip\n    """"""\n    if has_stuff(directory, warn_existing, overwrite):\n        return\n    else:\n        os.makedirs(directory, exist_ok=True)\n\n    log.info(\'Downloading %s to %s/ ...\', url, directory)\n\n    request = requests.get(url, stream=True)\n    filename = url.split(\'/\')[-1]\n    if \'?\' in filename:\n        filename = filename[:filename.index(\'?\')]\n    location = os.path.join(tempfile.gettempdir(), filename)\n    with open(location, \'wb\') as f:\n        if request.status_code == 404:\n            raise RuntimeError(\'Download URL not accessible %s\' % url)\n        total_length = int(request.headers.get(\'content-length\'))\n        for chunk in progress.bar(request.iter_content(chunk_size=1024), expected_size=(total_length / 1024) + 1):\n            if chunk:\n                f.write(chunk)\n                f.flush()\n\n    log.info(\'done.\')\n    zip_ref = zipfile.ZipFile(location, \'r\')\n    log.info(\'Unzipping temp file %s to %s...\', location, directory)\n    try:\n        zip_ref.extractall(directory)\n    except Exception:\n        print(\'You may want to close all programs that may have these files \'\n              \'open or delete existing \'\n              \'folders this is trying to overwrite\')\n        raise\n    finally:\n        zip_ref.close()\n        os.remove(location)\n        log.info(\'Removed temp file %s\', location)\n\n\ndef has_stuff(path, warn_existing=False, overwrite=False):\n    # TODO: Remove overwrite as a parameter, doesn\'t make sense here.\n    if os.path.exists(path) and (dir_has_stuff(path) or file_has_stuff(path)):\n        if warn_existing:\n            print(\'%s exists, do you want to re-download and overwrite any existing files (y/n)?\' % path, end=\' \')\n            overwrite = input()\n            if \'n\' in overwrite.lower():\n                print(\'USING EXISTING %s - Try rerunning and overwriting if you run into problems.\' % path)\n                return True\n        elif not overwrite:\n            return True\n    return False\n\n\ndef dir_has_stuff(path):\n    return os.path.isdir(path) and os.listdir(path)\n\n\ndef file_has_stuff(path):\n    return os.path.isfile(path) and os.path.getsize(path) > 0\n'"
util/ensure_sim.py,0,"b'from __future__ import (absolute_import, division,\n                        print_function, unicode_literals)\n\nimport glob\nimport os\nimport stat\n\nfrom boto.s3.connection import S3Connection\nfrom future.builtins import (ascii, bytes, chr, dict, filter, hex, input,\n                             int, map, next, oct, open, pow, range, round,\n                             str, super, zip)\n\nimport deepdrive_api.utils\nfrom deepdrive_api.utils import check_pyarrow_compatibility\nimport config as c\nimport logs\nfrom util.run_command import run_command\n\n\nfrom util.download import download\n\nlog = logs.get_log(__name__)\n\n\ndef ensure_sim(update=False):\n    actual_path, expected_path = get_sim_bin_path(return_expected_path=True)\n    if update or actual_path is None:\n        print(\'\\n--------- Updating to latest simulator ----------\')\n        if c.IS_LINUX or c.IS_WINDOWS:\n            if c.SIM_URL is None:\n                log.info(\'Downloading latest sim\')\n                url = c.AWS_BUCKET_URL + get_latest_sim_url()\n            else:\n                log.info(f\'Using configured SIM_URL {c.SIM_URL}\')\n                url = c.SIM_URL\n            sim_path = os.path.join(c.DEEPDRIVE_DIR, get_sim_name_from_url(url))\n            download(url, sim_path, warn_existing=False, overwrite=False)\n        else:\n            raise NotImplementedError(\n                \'Sim download not yet implemented for this OS\')\n    ensure_executable(get_sim_bin_path())\n    ensure_sim_python_binaries()\n\n\ndef get_sim_name_from_url(url, include_file_extension=False):\n    ret = url.split(\'/\')[-1]\n    if not include_file_extension:\n        ret = \'.\'.join(ret.split(\'.\')[:-1])\n    return ret\n\n\ndef ensure_sim_python_binaries():\n    base_url = c.AWS_BUCKET_URL + \'/embedded_python_for_unreal/\'\n    if c.IS_WINDOWS:\n        # These include Python and our requirements\n        lib_url = base_url + \'windows/python_bin_with_libs.zip\'\n        lib_path = os.path.join(get_sim_project_dir(), \'Binaries\', \'Win64\')\n        if not os.path.exists(lib_path) or not os.path.exists(\n                os.path.join(lib_path, \'python3.dll\')):\n            print(\'Unreal embedded Python not found. Downloading...\')\n            download(lib_url, lib_path, overwrite=True, warn_existing=False)\n    elif c.IS_LINUX:\n        # Python is already embedded, however ensure_requirements\n        # fails with pip-req-tracker errors\n        uepy = deepdrive_api.utils.ensure_uepy_executable(get_sim_path())\n        os.system(\'{uepy} -m pip install pyzmq pyarrow==0.12.1 requests\'.\n                  format(uepy=uepy))\n        log.info(\'Installed UEPy python dependencies\')\n    elif c.IS_MAC:\n        raise NotImplementedError(\n            \'Sim does not yet run on OSX, see FAQs /\'\n            \' running a remote agent in /api.\')\n\n\ndef ensure_executable(path):\n    if c.IS_UNIX:\n        st = os.stat(path)\n        os.chmod(path, st.st_mode | stat.S_IEXEC)\n\n\ndef get_sim_bin_path(return_expected_path=False):\n    expected_path = None\n\n    def get_from_glob(search_path):\n        paths = glob.glob(search_path) or [search_path]\n        paths = [p for p in paths\n                 if not (p.endswith(\'.debug\') or p.endswith(\'.sym\'))]\n        if len(paths) > 1:\n            log.warn(\'Found multiple sim binaries in search directory - \'\n                     \'picking the first from %r\', paths)\n        if not paths:\n            ret_path = None\n        else:\n            ret_path = paths[0]\n        return ret_path\n\n    sim_path = get_sim_path()\n    if c.REUSE_OPEN_SIM:\n        return None\n    elif sim_path is None:\n        expected_path = None\n    elif c.IS_LINUX:\n        if os.path.exists(sim_path + \'/LinuxNoEditor\'):\n            expected_path = sim_path + \'/LinuxNoEditor/DeepDrive/Binaries/Linux/DeepDrive*\'\n        else:\n            expected_path = sim_path + \'/DeepDrive/Binaries/Linux/DeepDrive\'\n    elif c.IS_MAC:\n        raise NotImplementedError(\'Sim does not yet run on OSX, see FAQs / \'\n                                  \'running a remote agent in /api.\')\n    elif c.IS_WINDOWS:\n        expected_path = os.path.join(\n            sim_path, \'WindowsNoEditor\', \'DeepDrive\', \'Binaries\', \'Win64\',\n            \'DeepDrive*.exe\')\n\n    if expected_path is not None:\n        path = get_from_glob(expected_path)\n    else:\n        path = None\n\n    if path and not os.path.exists(path):\n        ret = None\n    else:\n        ret = path\n\n    if return_expected_path:\n        return ret, expected_path\n    else:\n        return ret\n\n\ndef get_latest_sim_url():\n    sim_prefix = \'sim/\' + c.SIM_PREFIX\n    conn = S3Connection(anon=True)\n    bucket = conn.get_bucket(\'deepdrive\')\n    bucket_search_str = sim_prefix + \'-\' + c.MAJOR_MINOR_VERSION_STR\n    sim_versions = list(bucket.list(bucket_search_str))\n    if not sim_versions:\n        raise RuntimeError(\'Could not find a sim version matching %s \'\n                           \'in bucket %s\' % (bucket_search_str, c.AWS_BUCKET_URL))\n    latest_sim_file, path_version = \\\n        sorted([(x.name, x.name.split(\'.\')[-2]) for x in sim_versions],\n               key=lambda y: y[1])[-1]\n    return \'/\' + latest_sim_file\n\n\ndef get_sim_path() -> str:\n    if c.SIM_URL:\n        sim_dir = get_sim_name_from_url(c.SIM_URL)\n        ret = os.path.join(c.DEEPDRIVE_DIR, sim_dir)\n    else:\n        paths = glob.glob(\n            os.path.join(c.DEEPDRIVE_DIR, \'deepdrive-sim-*-%s.*\'\n                                          % c.version.MAJOR_MINOR_VERSION_STR))\n        paths = [p for p in paths if not p.endswith(\'.zip\')]\n        if paths:\n            ret = list(sorted(paths))[-1]\n        else:\n            ret = None\n    return ret\n\ndef get_sim_project_dir():\n    if c.REUSE_OPEN_SIM:\n        path = input(\'What is the path to your simulator project directory?\'\n                     \'\\n\\ti.e. for sources something like ~/src/deepdrive-sim \'\n                     \'\\n\\tor for packaged binaries, something like ~/Deepdrive/sim/LinuxNoEditor/DeepDrive\')\n    elif c.IS_LINUX:\n        path = os.path.join(get_sim_path(), \'LinuxNoEditor/DeepDrive\')\n    elif c.IS_MAC:\n        raise NotImplementedError(\'Sim does not yet run on OSX, see FAQs / running a remote agent in /api.\')\n    elif c.IS_WINDOWS:\n        path = os.path.join(get_sim_path(), \'WindowsNoEditor\', \'DeepDrive\')\n    else:\n        raise RuntimeError(\'OS not recognized\')\n\n    return path\n\n\ndef check_pyarrow_compat():\n    try:\n        import ue4cli\n        manager = ue4cli.UnrealManagerFactory.create()\n        try:\n            unreal_root = manager.getEngineRoot()\n        except ue4cli.UnrealManagerException.UnrealManagerException:\n            log.warning(""""""\nUnable to find Unreal Engine root directory. Please run\n\nue4 setroot <your-unreal-directory>\n\n"""""")\n            return\n\n        check_pyarrow_compatibility(unreal_root)\n    except:\n        log.exception(\'Could not check for pyarrow compatibility, if you see \'\n                      \'segfaults serializing UEPY data, ensure the \'\n                      \'UEPY embedded python pyarrow version matches the \'\n                      \'the pyarrow version being used by this python \'\n                      \'interpreter\')\n\nif __name__ == \'__main__\':\n    ensure_sim()\n'"
util/experience_buffer.py,0,"b""from collections import deque\n\n\nclass ExperienceBuffer:\n    def __init__(self, step_seconds=0.25, seconds_to_keep=2, fade_fn=None):\n        self.step_seconds: float = step_seconds\n        self.seconds_to_keep: int = seconds_to_keep\n\n        self.max_length = int(self.seconds_to_keep / self.step_seconds)\n        self.buffer = deque(maxlen=self.max_length)\n        self.last_capture_time = None\n        self.fade_fn = fade_fn or (lambda j: 2 * j)\n\n        self.fade_length = 0\n        while self.fade_fn(self.fade_length) < self.max_length:\n            self.fade_length += 1\n\n    def maybe_add(self, x, t):\n        if self.last_capture_time is None or t > (self.last_capture_time + self.step_seconds):\n            self.buffer.append((x, t))\n\n    def size(self):\n        return len(self.buffer)\n\n    def __len__(self):\n        return self.size()\n\n    def get_fading(self):\n        if self.size() > 0 and self.size() == self.max_length:\n            i = self.size() - 1\n            ret = []\n            while i >= 0:\n                ret.append(self.buffer[i])\n                i -= self.fade_fn(i)\n        else:\n            ret = None\n\n        if len(ret) < self.fade_length:\n            # Don't return a partial buffer\n            ret = None\n\n        return ret\n\n\n"""
util/get_directories.py,0,"b""from __future__ import (absolute_import, division,\n                        print_function, unicode_literals)\n\nimport os\nimport sys\nimport uuid\nfrom datetime import datetime\n\np = os.path\n\n\ndef makedirs(name, exist_ok=False):\n    if not p.exists(name) or not exist_ok:\n        os.makedirs(name)\n\n\ndef _get_deepdrive_dir():\n    dir_config_file = p.join(DEEPDRIVE_CONFIG_DIR, 'deepdrive_dir')\n    if p.exists(dir_config_file):\n        with open(dir_config_file) as dcf:\n            ret = dcf.read()\n    else:\n        default_dir = p.join(p.expanduser('~'), 'Deepdrive')\n        if 'DEEPDRIVE_DOCKER_HOST' in os.environ:\n            ret = default_dir\n        else:\n            ret = input('Where would you like to store Deepdrive files '\n                        '(i.e. sim binaries (1GB), checkpoints (200MB), '\n                        'recordings, and logs)? [Press Enter for %s] '\n                        % default_dir)\n            deepdrive_dir_set = False\n            while not deepdrive_dir_set:\n                ret = ret or default_dir\n                if 'deepdrive' not in ret.lower():\n                    ret = p.join(ret, 'Deepdrive')\n                if not p.isabs(ret):\n                    ret = input('Path: %s is not absolute, please specify a '\n                                'different path [Press Enter for %s] ' %\n                                (ret, default_dir))\n                if p.isfile(ret):\n                    ret = input('Path: %s is already a file, please specify a '\n                                'different path [Press Enter for %s] ' %\n                                (ret, default_dir))\n                else:\n                    deepdrive_dir_set = True\n        with open(dir_config_file, 'w') as dcf:\n            dcf.write(ret)\n            print('%s written to %s' % (ret, dir_config_file))\n    ret = ret.replace('\\r', '').replace('\\n', '')\n    if not p.exists(ret):\n        print('Creating Deepdrive directory at %s' % ret)\n        os.makedirs(ret)\n    return ret\n\n\ndef _ensure_python_bin_config():\n    if 'DEEPDRIVE_DOCKER_HOST' in os.environ:\n        return\n    else:\n        py_bin = p.join(DEEPDRIVE_CONFIG_DIR, 'python_bin')\n        with open(py_bin, 'w') as _dpbf:\n            _dpbf.write(sys.executable)\n\n\n# Directories\nROOT_DIR = p.dirname(p.dirname(p.realpath(__file__)))\nDEEPDRIVE_DIR = os.environ.get('DEEPDRIVE_DIR')\nDEEPDRIVE_CONFIG_DIR = p.expanduser('~') + '/.deepdrive'\nmakedirs(DEEPDRIVE_CONFIG_DIR, exist_ok=True)\nif DEEPDRIVE_DIR is None:\n    DEEPDRIVE_DIR = _get_deepdrive_dir()\n_ensure_python_bin_config()\n\n# Data and log directories\nRUN_ID = uuid.uuid4().hex[:4]\nDIR_DATE_FORMAT = '%Y-%m-%d__%I-%M-%S%p'\nDATE_STR = datetime.now().strftime(DIR_DATE_FORMAT)\nRECORDING_DIR = os.environ.get('DEEPDRIVE_RECORDING_DIR') or \\\n                p.join(DEEPDRIVE_DIR, 'recordings')\nHDF5_SESSION_DIR = p.join(RECORDING_DIR, DATE_STR)\nGYM_DIR = p.join(DEEPDRIVE_DIR, 'gym')\nLOG_DIR = p.join(DEEPDRIVE_DIR, 'log')\nRESULTS_BASE_DIR = p.join(DEEPDRIVE_DIR, 'results')\nRESULTS_DIR = p.join(RESULTS_BASE_DIR, DATE_STR + '_' + RUN_ID)\nBOTLEAGUE_RESULTS_DIR = p.join(RESULTS_DIR, 'botleague_results')\nLATEST_BOTLEAGUE_RESULTS = os.environ.get('BOTLEAGUE_RESULT_FILEPATH',\n                                          p.join(RESULTS_BASE_DIR,\n                                                 'latest_botleague_results'))\nTENSORFLOW_OUT_DIR = p.join(DEEPDRIVE_DIR, 'tensorflow')\nWEIGHTS_DIR = p.join(DEEPDRIVE_DIR, 'weights')\nBASELINES_DIR = p.join(DEEPDRIVE_DIR, 'baselines_results')\nTFRECORD_DIR_SUFFIX = '_tfrecords'\nTF_ENV_EVENT_DIR = p.join(TENSORFLOW_OUT_DIR, 'env', DATE_STR)\nmakedirs(TF_ENV_EVENT_DIR, exist_ok=True)\n"""
util/run_command.py,0,"b""from __future__ import (absolute_import, division,\n                        print_function, unicode_literals)\n\nfrom future.builtins import (ascii, bytes, chr, dict, filter, hex, input,\n                             int, map, next, oct, open, pow, range, round,\n                             str, super, zip)\n\nfrom subprocess import Popen, PIPE\n\n\ndef run_command(cmd, cwd=None, env=None, throw=True, verbose=False, print_errors=True):\n    def say(*args):\n        if verbose:\n            print(*args)\n    say(cmd)\n    if not isinstance(cmd, list):\n        cmd = cmd.split()\n    process = Popen(cmd, stdin=PIPE, stdout=PIPE, stderr=PIPE, cwd=cwd, env=env)\n    result, err = process.communicate()\n    if not isinstance(result, str):\n        result = ''.join(map(chr, result))\n    result = result.strip()\n    say(result)\n    if process.returncode != 0:\n        if not isinstance(err, str):\n            err = ''.join(map(chr, err))\n        err_msg = ' '.join(cmd) + ' finished with error ' + err.strip()\n        if throw:\n            raise RuntimeError(err_msg)\n        elif print_errors:\n            print(err_msg)\n    return result, process.returncode\n"""
util/sampler.py,0,"b'from random import randint\nfrom enum import Enum\nfrom collections import deque\n\nimport numpy as np\n\nimport config as c\nimport logs\nlog = logs.get_log(__name__)\n\n\nclass SamplerType(Enum):\n    ALL_TIME = 1\n    RECENT = 2\n\n\nclass Sampler(object):\n    """"""\n    Simple data structure that maintains a fixed sized random sample of an\n    input stream for computing a running median, etc...\n\n    i.e. reservoir sampling\n    """"""\n    def __init__(self, maxlen=10000, sampler_type=SamplerType.ALL_TIME):\n        """"""\n        maxlen: Number of samples to maintain\n        """"""\n        self.q = deque(maxlen=maxlen)\n        self.maxlen = maxlen\n        self.is_full = False\n        self.num_samples = 0\n        self.type = sampler_type\n        self.max:float = float(\'-inf\')\n        self.min:float = float(\'inf\')\n        self.total:float = 0\n        self._mean:float = 0\n\n    def sample(self, x):\n        self.num_samples += 1\n        if self.type is SamplerType.RECENT or not self.is_full:\n            self.q.append(x)\n        else:\n            if self.type is not SamplerType.ALL_TIME:\n                raise RuntimeError(\'Expected sampling type to be \'\n                                   \'across all time, but was %s\' % str(self.type))\n            # Once we have filled the buffer, try to keep an equal distribution of old and new samples by\n            # decaying the replacement probability.\n            should_replace = self.is_full and self.uniform_random_chance()\n            if should_replace:\n                index = randint(0, self.maxlen - 1)\n                del self.q[index]\n                self.q.append(x)\n        if not self.is_full:\n            self.is_full = len(self.q) == self.maxlen\n        self.max = max(self.max, x)\n        self.min = min(self.min, x)\n        self.total += x\n\n    def uniform_random_chance(self):\n        return c.rng.rand() < len(self.q) / self.num_samples\n\n    def mean(self):\n        if self.type is SamplerType.ALL_TIME:\n            return self.total / self.num_samples\n        else:\n            return np.mean(self.q)\n\n    def median(self):\n        return np.median(self.q)\n\n    def change(self, steps):\n        if self.type is SamplerType.ALL_TIME:\n            log.warn(\'Times between samples will grow in all time mode, are you sure you don\\\'t want recent mode?\')\n        if len(self.q) < steps:\n            return None\n        else:\n            return self.q[-1] - self.q[-steps]\n\n\ndef main():\n    test_all_time()\n    test_recent()\n\n\ndef test_recent():\n    s = Sampler(maxlen=50, sampler_type=SamplerType.RECENT)\n    for i in range(10000):\n        # s.sample(randint(0, 100))\n        s.sample(i)\n        mean = s.mean()\n        median = s.median()\n        change = s.change(steps=1)\n    print(i, \'mean\', mean, \'median\', median, \'change\',\n          change, \'len\', len(s.q), s.q, \'max\', s.max, \'min\', s.min)\n\n\ndef test_all_time():\n    s = Sampler(maxlen=25, sampler_type=SamplerType.ALL_TIME)\n    for i in range(10000):\n        # s.sample(randint(0, 100))\n        s.sample(i)\n        mean = s.mean()\n        median = s.median()\n    print(i, \'mean\', mean, \'median\', median, \'len\', len(s.q), s.q,\n          \'max\', s.max, \'min\', s.min)\n\n\nif __name__ == \'__main__\':\n    main()\n'"
vendor/__init__.py,0,b''
agents/dagger/__init__.py,0,b''
agents/dagger/action_jitterer.py,0,"b'from __future__ import (absolute_import, division,\n                        print_function, unicode_literals)\n\nimport glob\nimport os\n\nfrom future.builtins import (ascii, bytes, chr, dict, filter, hex, input,\n                             int, map, next, oct, open, pow, range, round,\n                             str, super, zip)\nfrom enum import Enum\n\nimport config as c\nimport logs\n\nlog = logs.get_log(__name__)\n\n\nclass JitterState(Enum):\n    MAINTAIN = 1\n    SWITCH_TO_NONRAND = 2\n    SWITCH_TO_RAND = 3\n\n\nclass ActionJitterer(object):\n    def __init__(self):\n        """"""\n        Jitters state between m random then n non-random steps -\n            where m and n are set to different values to increase sample diversity\n        """"""\n        self.rand_step = None\n        self.nonrand_step = None\n        self.rand_total = None\n        self.nonrand_total = None\n        self.seq_total = None\n        self.perform_random_actions = None\n        self.reset()\n\n    def advance(self):\n        if self.perform_random_actions:\n            if self.rand_step < self.rand_total:\n                self.rand_step += 1\n                ret = JitterState.MAINTAIN\n            else:\n                # Done with random actions, switch to non-random\n                self.perform_random_actions = False\n                ret = JitterState.SWITCH_TO_NONRAND\n        else:\n            if self.nonrand_step < self.nonrand_total:\n                self.nonrand_step += 1\n                ret = JitterState.MAINTAIN\n            else:\n                # We are done with the sequence\n                self.reset()\n                if self.perform_random_actions:\n                    ret = JitterState.SWITCH_TO_RAND\n                else:\n                    ret = JitterState.MAINTAIN  # Skipping random actions this sequence\n        return ret\n\n    def reset(self):\n        self.rand_step = 0\n        self.nonrand_step = 0\n        rand = c.rng.rand()\n        if rand < 0.50:\n            self.rand_total = 0\n            self.nonrand_total = 10\n        elif rand < 0.67:\n            self.rand_total = 4\n            self.nonrand_total = 5\n        elif rand < 0.85:\n            self.rand_total = 8\n            self.nonrand_total = 10\n        elif rand < 0.95:\n            self.rand_total = 12\n            self.nonrand_total = 15\n        else:\n            self.rand_total = 24\n            self.nonrand_total = 30\n        log.debug(\'random action total %r, non-random total %r\', self.rand_total,\n                  self.nonrand_total)\n        self.seq_total = self.rand_total + self.nonrand_total\n        self.perform_random_actions = self.rand_total != 0\n\n\n'"
agents/dagger/agent.py,8,"b'import copy\nimport json\nimport os\nimport time\nfrom datetime import datetime\nimport math\nimport glob\n\nimport tensorflow as tf\nimport numpy as np\nfrom simple_pid import PID\n\nimport config as c\nimport sim\nfrom agents.common import get_throttle\nfrom agents.dagger import net\nfrom agents.dagger.action_jitterer import ActionJitterer, JitterState\nfrom sim.driving_style import DrivingStyle\nfrom sim.action import Action\nfrom sim import world\nfrom agents.dagger.net import AlexNet, MobileNetV2\nfrom config import MOBILENET_V2_IMAGE_SHAPE\nfrom sim.sim_args import SimArgs\nfrom util.download import download\nimport logs\n\nlog = logs.get_log(__name__)\n\nTARGET_MPH = 25\nTARGET_MPS = TARGET_MPH / 2.237\nTARGET_MPS_TEST = 75 * TARGET_MPS\n\n\nclass Agent(object):\n    def __init__(self, tf_session, should_jitter_actions=True,\n                 net_path=None, use_frozen_net=False, path_follower=False,\n                 output_last_hidden=False, net_name=c.ALEXNET_NAME,\n                 driving_style: DrivingStyle = DrivingStyle.NORMAL):\n        np.random.seed(c.RNG_SEED)\n        self.previous_action = None\n        self.previous_net_out = None\n        self.step = 0\n        self.net_name = net_name\n        self.driving_style = driving_style\n\n        # State for toggling random actions\n        self.should_jitter_actions = should_jitter_actions\n        self.episode_action_count = 0\n        self.path_follower_mode = path_follower\n        self.output_last_hidden = output_last_hidden\n\n        if should_jitter_actions:\n            log.info(\'Mixing in random actions to increase data diversity \'\n                     \'(these are not recorded).\')\n\n        # Net\n        self.sess = tf_session\n        self.use_frozen_net = use_frozen_net\n        if net_path is not None:\n            if net_name == c.ALEXNET_NAME:\n                input_shape = c.ALEXNET_IMAGE_SHAPE\n            elif net_name == c.MOBILENET_V2_NAME:\n                input_shape = c.MOBILENET_V2_IMAGE_SHAPE\n            else:\n                raise NotImplementedError(net_name + \' not recognized\')\n            self.load_net(net_path, use_frozen_net, input_shape)\n        else:\n            self.net = None\n            self.sess = None\n\n        self.throttle_pid = PID(0.3, 0.05, 0.4)\n        self.throttle_pid.output_limits = (-1, 1)\n\n        self.jitterer = ActionJitterer()\n\n    def act(self, obz, reward, done):\n        net_out = None\n        if obz:\n            try:\n                log.debug(\'steering %r\', obz[\'steering\'])\n                log.debug(\'throttle %r\', obz[\'throttle\'])\n            except TypeError as e:\n                log.error(\'Could not parse this observation %r\', obz)\n                raise\n\n        if self.should_jitter_actions:\n            episode_time = obz.get(\'episode_return\', {}).get(\'episode_time\',\n                                                    None) if obz else None\n            if episode_time is None or episode_time < 10:\n                # Hold off a bit at start of episode\n                action = Action(has_control=False)\n            else:\n                if okay_to_jitter_actions(obz):\n                    action = self.jitter_action(obz)\n                else:\n                    action = Action(has_control=False)\n\n        elif self.net is not None:\n            if not obz or not obz[\'cameras\']:\n                net_out = None\n            else:\n                image = obz[\'cameras\'][0][\'image\']\n                net_out = self.get_net_out(image)\n            if net_out is not None and self.output_last_hidden:\n                y_hat = net_out[0]\n            else:\n                y_hat = net_out\n            action = self.get_next_action(obz, y_hat)\n        else:\n            action = Action(has_control=(not self.path_follower_mode))\n        self.previous_action = action\n        self.step += 1\n        action = action.as_gym()\n        return action, net_out\n\n    def get_next_action(self, obz, net_out):\n        log.debug(\'getting next action\')\n        if net_out is None:\n            log.debug(\'net out is None\')\n            return self.previous_action or Action()\n\n        net_out = net_out[0]  # We currently only have one environment\n\n        (desired_spin,\n         desired_direction,\n         desired_speed,\n         desired_speed_change,\n         desired_steering,\n         desired_throttle) = net_out\n\n        desired_spin = desired_spin * c.SPIN_NORMALIZATION_FACTOR\n        desired_speed = desired_speed * c.SPEED_NORMALIZATION_FACTOR\n        desired_speed_change = desired_speed_change * c.SPEED_NORMALIZATION_FACTOR\n\n        log.debug(\'desired_steering %f\', desired_steering)\n        log.debug(\'desired_throttle %f\', desired_throttle)\n\n        log.debug(\'desired_direction %f\', desired_direction)\n        log.debug(\'desired_speed %f\', desired_speed)\n        log.debug(\'desired_speed_change %f\', desired_speed_change)\n        log.debug(\'desired_throttle %f\', desired_throttle)\n        log.debug(\'desired_spin %f\', desired_spin)\n\n        actual_speed = obz[\'speed\']\n        log.debug(\'actual_speed %f\', actual_speed)\n        log.debug(\'desired_speed %f\', desired_speed)\n\n        # max_meters_per_sec = 8\n\n        if isinstance(self.net, MobileNetV2):\n            # target_speed = desired_speed\n            # desired_throttle = abs(target_speed / max(actual_speed, 1e-3))\n\n            # if actual_speed > 0.8 * (max_meters_per_sec * 100):\n            #     desired_speed *= 0.8\n\n            # TODO(post v3): Support different driving styles\n\n            # desired_throttle = get_throttle(actual_speed, desired_speed * 0.48)\n\n            pid_throttle = self.get_target_throttle(obz)\n\n            desired_throttle = pid_throttle  # min(max(desired_throttle, 0.), pid_throttle)\n\n            # if self.previous_net_out:\n            #     desired_throttle = 0.2 * self.previous_action.throttle + 0.8 * desired_throttle\n            # else:\n            #     desired_throttle = desired_throttle * 0.95\n            #\n            # desired_throttle *= 0.95\n        else:\n            # AlexNet\n\n            if self.driving_style == DrivingStyle.CRUISING:\n                target_speed = 8 * 100\n            elif self.driving_style == DrivingStyle.NORMAL:\n                target_speed = 9 * 100\n            elif self.driving_style == DrivingStyle.LATE:\n                target_speed = 10 * 100\n            else:\n                raise NotImplementedError(\'Driving style not supported\')\n\n            # AlexNet overfit on speed, plus it\'s nice to be able to change it,\n            # so we just ignore output speed of net\n            desired_throttle = get_throttle(actual_speed, target_speed)\n        log.debug(\'actual_speed %r\' % actual_speed)\n\n        # log.info(\'desired_steering %f\', desired_steering)\n        # log.info(\'desired_throttle %f\', desired_throttle)\n        if self.previous_action:\n            smoothed_steering = 0.2 * self.previous_action.steering + 0.5 * desired_steering\n        else:\n            smoothed_steering = desired_steering * 0.7\n\n        # desired_throttle = desired_throttle * 1.1\n\n        action = Action(smoothed_steering, desired_throttle)\n        return action\n\n    def jitter_action(self, obz):\n        """"""\n        Reduce sampling error by randomly exploring space around non-random agent\'s\n        trajectory with occasional random actions\n        """"""\n        state = self.jitterer.advance()\n        if state is JitterState.SWITCH_TO_RAND:\n            # Going too large here gets us stuck\n            steering = np.random.uniform(-0.5, 0.5, 1)[0]\n\n            # Slow down a bit so we don\'t crash before recovering\n            throttle = self.get_target_throttle(obz) * 0.5\n\n            has_control = True\n            log.debug(\'random steering %f\', steering)\n        elif state is JitterState.SWITCH_TO_NONRAND:\n            has_control = False\n            steering = throttle = 0  # Has no effect\n        elif state is JitterState.MAINTAIN:\n            if self.previous_action is None:\n                has_control = False\n                steering = throttle = 0  # Has no effect\n                log.warning(\'Previous action none\')\n            else:\n                steering = self.previous_action.steering\n                throttle = self.get_target_throttle(obz)\n                has_control = self.previous_action.has_control\n        else:\n            raise ValueError(\'Unexpected action jitter state\')\n        action = Action(steering, throttle, has_control=has_control)\n        if not has_control:\n            # TODO(post v3): Move setpoint to env\n            # We should not be calling single purpose\n            # UEPy API methods this frequently, i.e. every step as the server\n            # can only respond to one message per frame.\n            # This should be moved to a combined call in step instead.\n            world.set_ego_mph(TARGET_MPH, TARGET_MPH)\n        return action\n\n    def get_target_throttle(self, obz):\n        if obz and \'speed\' in obz:\n            actual_speed = obz[\'speed\']\n        else:\n            actual_speed = TARGET_MPS\n\n        pid = self.throttle_pid\n        target_cmps = TARGET_MPS * 100\n        if pid.setpoint != target_cmps:\n            pid.setpoint = target_cmps\n        throttle = pid(actual_speed)\n        if not pid.auto_mode:\n            pid.auto_mode = True\n        if throttle is None:\n            log.warn(\'PID output None, setting throttle to 0.\')\n            throttle = 0.\n        throttle = min(max(throttle, 0.), 1.)\n        return throttle\n\n    def load_net(self, net_path, is_frozen=False, image_shape=None):\n        """"""\n        Frozen nets can be generated with something like\n\n        `python freeze_graph.py --input_graph=""C:\\tmp\\deepdrive\\tensorflow_random_action\\train\\graph.pbtxt"" --input_checkpoint=""C:\\tmp\\deepdrive\\tensorflow_random_action\\train\\model.ckpt-273141"" --output_graph=""C:\\tmp\\deepdrive\\tensorflow_random_action\\frozen_graph.pb"" --output_node_names=""model/add_2""`\n\n        where model/add_2 is the auto-generated name for self.net.p\n        """"""\n        if image_shape is None:\n            raise RuntimeError(\'Image shape not defined\')\n        if is_frozen:\n            # TODO(post v3): Get frozen nets working\n\n            # We load the protobuf file from the disk and parse it to\n            # retrieve the unserialized graph_def\n            with tf.gfile.GFile(net_path, ""rb"") as f:\n                graph_def = tf.GraphDef()\n                graph_def.ParseFromString(f.read())\n\n            # Then, we can use again a convenient built-in function to \n            # import a graph_def into the current default Graph\n            with tf.Graph().as_default() as graph:\n                tf.import_graph_def(\n                    graph_def,\n                    input_map=None,\n                    return_elements=None,\n                    name=""prefix"",\n                    op_dict=None,\n                    producer_op_list=None\n                )\n            self.net = graph\n\n        else:\n            if self.net_name == c.MOBILENET_V2_NAME:\n                self.net = MobileNetV2(is_training=False)\n            else:\n                self.net = AlexNet(is_training=False)\n            saver = tf.train.Saver()\n            saver.restore(self.sess, net_path)\n\n    def close(self):\n        if self.sess is not None:\n            self.sess.close()\n\n    def get_net_out(self, image):\n        begin = time.time()\n        if self.use_frozen_net:\n            out_var = \'prefix/model/add_2\'\n        else:\n            out_var = self.net.out\n        if self.output_last_hidden:\n            out_var = [out_var, self.net.last_hidden]\n\n        image = image.reshape(1, *self.net.input_image_shape)\n        net_out = self.sess.run(out_var, feed_dict={\n            self.net.input: image, })\n\n        # print(net_out)\n        end = time.time()\n        log.debug(\'inference time %s\', end - begin)\n        return net_out\n\n    def reset(self):\n        self.jitterer.reset()\n        self.throttle_pid.auto_mode = False\n\n\ndef run(sim_args: SimArgs,\n        net_path=None,\n        run_baseline_agent=False,\n        run_mnet2_baseline_agent=False,\n        run_ppo_baseline_agent=False,\n        camera_rigs=None,\n        should_jitter_actions=False,\n        path_follower=False,\n        net_name=c.ALEXNET_NAME,\n        max_episodes=1000,\n        agent_name=None,\n\n        # Placeholder for rotating between Unreal Editor and packaged game\n        should_rotate_sim_types=False, ):\n    """"""\n    Run inference for agents\n    """"""\n    sim_args.max_episodes = max_episodes\n    if sim_args.should_record:\n        path_follower = True\n\n    if agent_name == c.DAGGER_MNET2:\n        image_resize_dims = MOBILENET_V2_IMAGE_SHAPE\n    else:\n        image_resize_dims = None\n\n    agent, env, should_rotate_camera_rigs, start_env = \\\n        setup(sim_args=sim_args,\n              camera_rigs=camera_rigs,\n              net_name=net_name,\n              net_path=net_path,\n              path_follower=path_follower,\n              run_baseline_agent=run_baseline_agent,\n              run_mnet2_baseline_agent=run_mnet2_baseline_agent,\n              run_ppo_baseline_agent=run_ppo_baseline_agent,\n              should_jitter_actions=should_jitter_actions,\n              image_resize_dims=image_resize_dims,\n              agent_name=agent_name)\n\n    reward = 0\n    episode_done = False\n\n    def close():\n        env.close()\n        agent.close()\n\n    session_done = False\n    episode = 0\n\n    try:\n        while not session_done:\n            if episode_done:\n                # TODO: Don\'t use reset return value. Should always be 0.\n                obz = env.reset()\n                episode_done = False\n            else:\n                obz = None\n            if max_episodes is not None and episode >= (max_episodes - 1):\n                session_done = True\n            episode_done, should_close = run_episode(\n                agent, env, episode_done, obz, reward)\n\n            if session_done:\n                log.info(\'Session done\')\n            elif should_close:\n                session_done = True\n                log.info(\'Server directed to close\')\n            else:\n                log.info(\'Episode done\')\n                episode += 1\n                agent.reset()\n                if should_rotate_camera_rigs:\n                    rotate_cameras(camera_rigs, env, episode)\n\n    except KeyboardInterrupt:\n        log.info(\'keyboard interrupt detected in agent, closing\')\n    except Exception as e:\n        raise e\n    finally:\n        close()\n\n\ndef rotate_cameras(camera_rigs, env, episode):\n    # TODO: Add this to domain_randomization()\n    cameras = copy.deepcopy(\n        camera_rigs[episode % len(camera_rigs)])\n    randomize_cameras(cameras)\n    env.change_cameras(cameras)\n\n\ndef run_episode(agent, env, episode_done, obz, reward):\n    should_close = False\n    while not episode_done:\n        act_start = time.time()\n        action, net_out = agent.act(obz, reward, episode_done)\n        log.debug(\'act took %fs\', time.time() - act_start)\n\n        env_step_start = time.time()\n        obz, reward, episode_done, info = env.step(action)\n        if \'should_close\' in info:\n            should_close = info[\'should_close\']\n        log.debug(\'env step took %fs\', time.time() - env_step_start)\n    return episode_done, should_close\n\n\ndef setup(sim_args, camera_rigs, net_name, net_path,\n          path_follower, run_baseline_agent,\n          run_mnet2_baseline_agent,\n          run_ppo_baseline_agent, should_jitter_actions, image_resize_dims,\n          agent_name):\n    if net_path and (run_baseline_agent or\n                     run_mnet2_baseline_agent or\n                     run_ppo_baseline_agent):\n        raise RuntimeError(\'Running a baseline agent does not require a \'\n                           \'net_path. Please remove net_path or baseline \'\n                           \'argument\')\n    if not net_path:\n        if agent_name == c.DAGGER:\n            log.info(\'Running alexnet dagger agent\')\n            run_baseline_agent = True\n        elif agent_name == c.DAGGER_MNET2:\n            log.info(\'Running mnet2 dagger agent\')\n            run_mnet2_baseline_agent = True\n        elif agent_name == c.BOOTSTRAPPED_PPO2:\n            log.info(\'Running baseline ppo2 agent\')\n            run_ppo_baseline_agent = True\n\n    if run_baseline_agent:\n        net_path = ensure_alexnet_baseline_weights(net_path)\n    elif run_mnet2_baseline_agent:\n        net_path = ensure_mnet2_baseline_weights(net_path)\n    elif run_ppo_baseline_agent:\n        net_path = ensure_ppo_baseline_weights(net_path)\n\n    sess = config_tensorflow_memory(net_name)\n\n    # TODO: Allow rotating and randomizing cameras rigs in env\n    if camera_rigs:\n        cameras = camera_rigs[0]\n    else:\n        cameras = None\n    if sim_args.should_record and camera_rigs is not None and \\\n            len(camera_rigs) >= 1:\n        should_rotate_camera_rigs = True\n        log.info(\n            \'Rotating cameras while recording to encourage visual robustness\')\n    else:\n        should_rotate_camera_rigs = False\n    if should_rotate_camera_rigs:\n        randomize_cameras(cameras)\n\n    use_sim_start_command_first_lap = c.SIM_START_COMMAND is not None\n\n    def start_env():\n        sim_args.cameras = cameras\n        sim_args.use_sim_start_command = use_sim_start_command_first_lap\n        sim_args.image_resize_dims = image_resize_dims\n        return sim.start(**(sim_args.to_dict()))\n\n    env = start_env()\n\n    try:\n        agent = Agent(sess, should_jitter_actions=should_jitter_actions,\n                      net_path=net_path, path_follower=path_follower,\n                      net_name=net_name,\n                      driving_style=DrivingStyle.from_str(sim_args.driving_style))\n    except Exception as e:\n        env.close()\n        raise e\n\n    if net_path:\n        log.info(\'Running tensorflow agent checkpoint: %s\', net_path)\n    return agent, env, should_rotate_camera_rigs, start_env\n\n\ndef config_tensorflow_memory(net_name):\n    """"""\n    Configure TensorFlow so that we leave GPU memory for the game to run\n    on cards with 4GB of VRAM\n\n    NOTE: Debugging python, i.e. with PyCharm can cause OOM errors,\n    where running will not\n\n    :param net_name: Name of the Neural Network we\'re using\n    :return: Tensorflow Session object\n    """"""\n\n    if net_name == c.ALEXNET_NAME:\n        per_process_gpu_memory_fraction = 0.8\n    else:\n        per_process_gpu_memory_fraction = 0.4\n    tf_config = tf.ConfigProto(\n        gpu_options=tf.GPUOptions(\n            per_process_gpu_memory_fraction=per_process_gpu_memory_fraction,\n            allow_growth=True\n        ),\n    )\n    sess = tf.Session(config=tf_config)\n    return sess\n\n\ndef okay_to_jitter_actions(obz):\n    if obz is None:\n        return False\n    else:\n        dnext = obz[\'distance_to_next_agent\']\n        dprev = obz[\'distance_to_prev_agent\']\n        if dnext == -1.0 and dprev == -1.0:\n            return True\n        if dnext < (100 * 100) or dprev < (50 * 100):\n            log.info(\n                \'Not okay to act randomly passing %r distance next %r distance prev %r\',\n                obz[\'is_passing\'], obz[\'distance_to_next_agent\'],\n                obz[\'distance_to_prev_agent\'])\n            return False\n        else:\n            return True\n\n\ndef randomize_cameras(cameras):\n    for cam in cameras:\n        # Add some randomness to the position (less than meter), rotation (less than degree), fov (less than degree),\n        # capture height (1%), and capture width (1%)\n        for i in range(len(cam[\'relative_rotation\'])):\n            cam[\'relative_rotation\'][i] += math.radians(np.random.random())\n        for i in range(len(cam[\'relative_position\'])):\n            if i == 0 or i == 2:\n                # x - forward or z - up\n                cam[\'relative_position\'][i] += (np.random.random() * 100)\n            elif i == 1:\n                # y - right\n                cam[\'relative_position\'][i] += (np.random.random() * 100 - 50)\n\n        cam[\'field_of_view\'] += (np.random.random() - 0.5)\n        cam[\'capture_height\'] += round(np.random.random() *\n                                       0.01 * cam[\'capture_height\'])\n        cam[\'capture_width\'] += round(np.random.random() *\n                                      0.01 * cam[\'capture_width\'])\n\n\ndef random_use_sim_start_command(should_rotate_sim_types):\n    use_sim_start_command = should_rotate_sim_types and np.random.random() < 0.5\n    return use_sim_start_command\n\n\ndef _ensure_baseline_weights(net_path, version, weights_dir, url):\n    if net_path is not None:\n        raise ValueError(\'Net path should not be set when running the \'\n                         \'baseline agent as it has its own weights.\')\n    net_path = os.path.join(weights_dir, version)\n    if not glob.glob(net_path + \'*\'):\n        print(\'\\n--------- Baseline weights not found, downloading ----------\')\n        download(url + \'?cache_bust=\' + version, c.WEIGHTS_DIR,\n                 warn_existing=False, overwrite=True)\n    return net_path\n\n\ndef ensure_alexnet_baseline_weights(net_path):\n    return _ensure_baseline_weights(net_path,\n                                    c.ALEXNET_BASELINE_WEIGHTS_VERSION,\n                                    c.ALEXNET_BASELINE_WEIGHTS_DIR,\n                                    c.ALEXNET_BASELINE_WEIGHTS_URL)\n\n\ndef ensure_mnet2_baseline_weights(net_path):\n    return _ensure_baseline_weights(net_path, c.MNET2_BASELINE_WEIGHTS_VERSION,\n                                    c.MNET2_BASELINE_WEIGHTS_DIR,\n                                    c.MNET2_BASELINE_WEIGHTS_URL)\n\n\ndef ensure_ppo_baseline_weights(net_path):\n    return _ensure_baseline_weights(net_path, c.PPO_BASELINE_WEIGHTS_VERSION,\n                                    c.PPO_BASELINE_WEIGHTS_DIR,\n                                    c.PPO_BASELINE_WEIGHTS_URL)\n\n\nif __name__ == \'__main__\':\n    ensure_mnet2_baseline_weights(None)\n'"
agents/dagger/layers.py,13,"b'from __future__ import division\nimport numpy as np\nimport tensorflow as tf\n\n\ndef conv(input, kernel, biases, k_h, k_w, c_o, s_h, s_w, padding=""VALID"", group=1):\n    \'\'\'From https://github.com/ethereon/caffe-tensorflow\n    \'\'\'\n    c_i = input.get_shape()[-1]\n    assert c_i % group == 0\n    assert c_o % group == 0\n\n    def convolve(i, k):\n        return tf.nn.conv2d(i, k, [1, s_h, s_w, 1], padding=padding)\n\n    if group == 1:\n        conv = convolve(input, kernel)\n    else:\n        input_groups = tf.split(input, group, 3)\n        kernel_groups = tf.split(kernel, group, 3)\n        output_groups = [convolve(i, k) for i, k in zip(input_groups, kernel_groups)]\n        conv = tf.concat(output_groups, 3)\n    return tf.reshape(tf.nn.bias_add(conv, biases), [-1] + conv.get_shape().as_list()[1:])\n\n\ndef conv2d(x, name, num_features, kernel_size, stride, group):\n    input_features = x.get_shape()[3]\n    w = tf.get_variable(name + ""_W"", [kernel_size, kernel_size, int(input_features) // group, num_features])\n    b = tf.get_variable(name + ""_b"", [num_features])\n    return conv(x, w, b, kernel_size, kernel_size, num_features, stride, stride, padding=""SAME"", group=group)\n\n\ndef linear(x, name, size):\n    input_size = np.prod(list(map(int, x.get_shape()[1:])))\n    x = tf.reshape(x, [-1, input_size])\n    w = tf.get_variable(name + ""_W"", [input_size, size], initializer=tf.random_normal_initializer(0.0, 0.005))\n    b = tf.get_variable(name + ""_b"", [size], initializer=tf.zeros_initializer)\n    return tf.matmul(x, w) + b\n\n\ndef max_pool_2x2(x):\n    return tf.nn.max_pool(x, ksize=[1, 3, 3, 1], strides=[1, 2, 2, 1], padding=\'VALID\')\n\n\ndef lrn(x):\n    return tf.nn.local_response_normalization(x, depth_radius=2, alpha=2e-05, beta=0.75, bias=1.0)\n\n'"
agents/dagger/net.py,32,"b'import tensorflow as tf\n\nfrom agents.dagger.layers import conv2d, max_pool_2x2, linear, lrn\nimport config as c\nimport logs\nfrom config import ALEXNET_NAME, ALEXNET_FC7, ALEXNET_IMAGE_SHAPE, \\\n    MOBILENET_V2_SLIM_NAME, MOBILENET_V2_IMAGE_SHAPE\nfrom util.download import download, has_stuff\nfrom vendor.tensorflow.models.research.slim.nets import nets_factory\nfrom vendor.tensorflow.models.research.slim.preprocessing import preprocessing_factory\n\nlog = logs.get_log(__name__)\n\n# A module is understood as instrumented for quantization with TF-Lite\n# if it contains any of these ops.\nFAKE_QUANT_OPS = (\'FakeQuantWithMinMaxVars\',\n                  \'FakeQuantWithMinMaxVarsPerChannel\')\n\n\nclass Net(object):\n    def __init__(self, global_step=None, num_targets=c.NUM_TARGETS, is_training=True,\n                 freeze_pretrained=False, overfit=False):\n        self.num_targets = num_targets\n        self.is_training = is_training\n        self.global_step = global_step\n        self.freeze_pretrained = freeze_pretrained  # TODO: Implement for AlexNet\n        self.overfit = overfit\n        self.batch_size = None\n        self.learning_rate = None\n        self.starter_learning_rate = None\n        self.weight_decay = None\n        self.input, self.last_hidden, self.out, self.eval_out = self._init_net()\n        self.num_last_hidden = self.last_hidden.shape[-1]\n\n    def get_tf_init_fn(self, init_op):\n        raise NotImplementedError(\'get_tf_init_fn not implemented\')\n\n    def preprocess(self, image):\n        return image\n\n    def _init_net(self):\n        raise NotImplementedError(\'init_net not implemented\')\n\n\nclass MobileNetV2(Net):\n    def __init__(self, *args, **kwargs):\n        self.input_image_shape = MOBILENET_V2_IMAGE_SHAPE\n        self.image_preprocessing_fn = preprocessing_factory.get_preprocessing(\n            MOBILENET_V2_SLIM_NAME, is_training=False)\n        super(MobileNetV2, self).__init__(*args, **kwargs)\n        if self.is_training:\n            if self.freeze_pretrained:\n                self.batch_size = 48\n                self.starter_learning_rate = 1e-3\n            else:\n                self.batch_size = 32\n                self.starter_learning_rate = 1e-3\n            self.learning_rate = tf.train.exponential_decay(self.starter_learning_rate, global_step=self.global_step,\n                                                            decay_steps=55000, decay_rate=0.5, staircase=True)\n            if self.overfit:\n                self.weight_decay = 0.\n            else:\n                self.weight_decay = 0.00004  # same as mobilenet2 paper https://arxiv.org/pdf/1801.04381.pdf\n            self.mute_spurious_targets = True\n\n    def get_tf_init_fn(self, init_op):\n        def ret(ses):\n            log.info(\'Initializing parameters.\')\n            ses.run(init_op)\n        return ret\n\n    def preprocess(self, image):\n        image = self.image_preprocessing_fn(image, self.input_image_shape[0], self.input_image_shape[1])\n        image = tf.expand_dims(image, 0)\n        return image\n\n    def _init_net(self):\n        in_tensor = tf.placeholder(\n            tf.uint8, [None] + list(MOBILENET_V2_IMAGE_SHAPE))\n        network_fn = nets_factory.get_network_fn(\n            MOBILENET_V2_SLIM_NAME,\n            num_classes=None,\n            num_targets=6,\n            is_training=False, )\n        log.info(\'Loading mobilenet v2\')\n        image = self.preprocess(in_tensor)\n        out, endpoints = network_fn(image)\n        last_hidden = endpoints[\'global_pool\']\n        eval_out = out\n\n        return in_tensor, last_hidden, out, eval_out\n\nclass AlexNet(Net):\n    def __init__(self, *args, **kwargs):\n        self.input_image_shape = ALEXNET_IMAGE_SHAPE\n        self.num_last_hidden = ALEXNET_FC7\n        self.net_name = ALEXNET_NAME\n        self.starter_learning_rate = 2e-6\n        super(AlexNet, self).__init__(*args, **kwargs)\n\n        if self.is_training:\n            # Decrease this to fit in your GPU\'s memory\n            # If you increase, remember that it decreases accuracy https://arxiv.org/abs/1711.00489\n            self.batch_size = 32\n\n            # TODO: add polyak averaging.\n            self.learning_rate = tf.train.exponential_decay(self.starter_learning_rate, global_step=self.global_step,\n                                                            decay_steps=73000, decay_rate=0.5, staircase=True)\n\n            if self.overfit:\n                self.weight_decay = 0.\n            else:\n                self.weight_decay = 0.0005\n\n            self.mute_spurious_targets = False\n\n    def _init_net(self):\n        in_tensor = tf.placeholder(tf.float32, (None,) + self.input_image_shape)\n\n        with tf.variable_scope(""model"") as variable_scope:\n            last_hidden, out = self._init_alexnet(in_tensor, self.is_training, self.num_targets,\n                                                  self.num_last_hidden)\n            if self.is_training:\n                variable_scope.reuse_variables()\n                _, eval_out = self._init_alexnet(in_tensor, False, self.num_targets, self.num_last_hidden)\n            else:\n                eval_out = None\n\n        return in_tensor, last_hidden, out, eval_out\n\n    def get_tf_init_fn(self, init_op_):\n        return self._load_alexnet_pretrained(init_op_)\n\n    @staticmethod\n    def _load_alexnet_pretrained(init_op):\n        pretrained_var_map = {}\n        for v in tf.trainable_variables():\n            found = False\n            for bad_layer in [""fc6"", ""fc7"", ""fc8""]:\n                if bad_layer in v.name:\n                    found = True\n            if found:\n                continue\n\n            pretrained_var_map[v.op.name[6:]] = v\n        alexnet_saver = tf.train.Saver(pretrained_var_map)\n\n        def init_fn(ses):\n            log.info(\'Initializing parameters.\')\n            if not has_stuff(c.ALEXNET_PRETRAINED_PATH):\n                print(\'\\n--------- ImageNet checkpoint not found, downloading ----------\')\n                download(c.ALEXNET_PRETRAINED_URL, c.WEIGHTS_DIR, warn_existing=False, overwrite=True)\n            ses.run(init_op)\n            alexnet_saver.restore(ses, c.ALEXNET_PRETRAINED_PATH)\n\n        return init_fn\n\n    @staticmethod\n    def _init_alexnet(in_tensor, is_training, num_targets, num_last_hidden):\n        """"""AlexNet architecture with modified final fully-connected layers regressed on driving control outputs (steering, throttle, etc...)""""""\n        # phase = tf.placeholder(tf.bool, name=\'phase\')  # Used for batch norm\n\n        conv1 = tf.nn.relu(conv2d(in_tensor, ""conv1"", 96, 11, 4, 1))\n        lrn1 = lrn(conv1)\n        maxpool1 = max_pool_2x2(lrn1)\n        conv2 = tf.nn.relu(conv2d(maxpool1, ""conv2"", 256, 5, 1, 2))\n        lrn2 = lrn(conv2)\n        maxpool2 = max_pool_2x2(lrn2)\n        conv3 = tf.nn.relu(conv2d(maxpool2, ""conv3"", 384, 3, 1,\n                                  1))  # Not sure why this isn\'t 2 groups, but pretrained net was trained like this so we\'re going with it.\n\n        # Avoid diverging from pretrained weights with things like batch norm for now.\n        # Perhaps try a modern small net like Inception V1, ResNet 18, or Resnet 50\n        # conv3 = tf.contrib.layers.batch_norm(conv3, scope=\'batchnorm3\', is_training=phase,\n        #     # fused=True,\n        #     # data_format=\'NCHW\',\n        #     # renorm=True\n        # )\n\n        conv4 = tf.nn.relu(conv2d(conv3, ""conv4"", 384, 3, 1, 2))\n        conv5 = tf.nn.relu(conv2d(conv4, ""conv5"", 256, 3, 1, 2))\n        maxpool5 = max_pool_2x2(conv5)\n        fc6 = tf.nn.relu(linear(maxpool5, ""fc6"", 4096))\n        if is_training:\n            fc6 = tf.nn.dropout(fc6, 0.5)\n        else:\n            fc6 = tf.nn.dropout(fc6, 1.0)\n\n        fc7 = tf.nn.relu(linear(fc6, ""fc7"", num_last_hidden))\n        # fc7 = tf.contrib.layers.batch_norm(fc7, scope=\'batchnorm7\', is_training=phase)\n        if is_training:\n            fc7 = tf.nn.dropout(fc7, 0.95)\n        else:\n            fc7 = tf.nn.dropout(fc7, 1.0)\n\n        fc8 = linear(fc7, ""fc8"", num_targets)\n\n        return fc7, fc8\n\n\ndef variable_summaries(var):\n    """"""Attach a lot of summaries to a Tensor (for TensorBoard visualization).""""""\n    with tf.name_scope(\'summaries\'):\n        mean = tf.reduce_mean(var)\n        tf.summary.scalar(\'mean\', mean)\n        with tf.name_scope(\'stddev\'):\n            stddev = tf.sqrt(tf.reduce_mean(tf.square(var - mean)))\n        tf.summary.scalar(\'stddev\', stddev)\n        tf.summary.scalar(\'max\', tf.reduce_max(var))\n        tf.summary.scalar(\'min\', tf.reduce_min(var))\n        tf.summary.histogram(\'histogram\', var)\n\n\n\n\n\n\n\n'"
config/check_bindings/__init__.py,0,"b'import pkg_resources\nfrom distutils.version import LooseVersion as semvar\n\nimport config as c\n\n\ndef check_bindings_version():\n    # TODO: Bindings name change\n    bindings_version = semvar(pkg_resources.get_distribution(\n        c.BINDINGS_PACKAGE_NAME).version).version[:2]\n    client_version = c.MAJOR_MINOR_VERSION\n    if bindings_version != client_version:\n        print(""""""ERROR: Python bindings version mismatch. \n\nExpected {client_version_str}, got {bindings_version_str}\n\nHINT:\n\nFor binary sim distributions, try:\npip install package=={client_version_str}.*\n\nFor source sim distributions, try:\ncd <your-sim-sources>/Plugins/DeepDrivePlugin/Source/DeepDrivePython\npython build/build.py --type dev\n\n"""""".format(client_version=client_version, bindings_version=bindings_version,\n           client_version_str=\'.\'.join(str(vx) for vx in client_version),\n           bindings_version_str=\'.\'.join(str(vx) for vx in bindings_version), ))\n        exit(1)\n\n\ncheck_bindings_version()\n\n\nif __name__ == \'__main__\':\n    print(c.MAJOR_MINOR_VERSION)\n\n'"
tests/integration_tests/reset.py,0,"b""import sim\n\n\ndef main():\n    # TODO: Add some asserts and get working on Jenkins\n    env = sim.start(is_sync=True)\n    forward = sim.action(throttle=1, steering=0, brake=0)\n    done = False\n    i = 0\n    while 1:\n        i += 1\n        observation, reward, done, info = env.step(forward)\n        if i % 10 == 0:\n            env.reset()\n\n\n\nif __name__ == '__main__':\n    main()"""
tests/integration_tests/view_mode_eval.py,0,"b""import sim\nfrom sim.action import Action\n\n\n# TODO: Use python docker\n\n# Usage: Please make sure to start a fresh api/server.py as\n# connection life cycle is not properly implemented yet\n\ndef main():\n    env = sim.start(is_remote_client=True, render=True)\n    forward = Action(throttle=1)\n    done = False\n    while True:\n        while not done:\n            observation, reward, done, info = env.step(forward)\n\n        print('Episode finished')\n        done = env.reset()\n\n\nif __name__ == '__main__':\n    main()\n"""
tests/unit_tests/__init__.py,0,b''
tests/unit_tests/test_sanity.py,1,"b""from __future__ import (absolute_import, division,\n                        print_function, unicode_literals)\n\nfrom future.builtins import (int, range)\nimport numpy as np\nimport pytest\nfrom numpy.random import RandomState\nimport tempfile\nimport os\n\nos.environ['DEEPDRIVE_DIR'] = os.path.join(tempfile.gettempdir(), 'testdeepdrive')\n\nimport utils\nfrom sim.reward_calculator import RewardCalculator\n\ntry:\n    import tensorflow as tf\n    import tf_utils\nexcept ImportError:\n    print('Tensorflow not found, skipping tests')\n    tf = None\n    tf_utils = None\n\n\n@pytest.fixture()\ndef tf_sess():\n    if tf:\n        with tf.Session() as sess:\n            yield sess\n    else:\n        yield None\n\n\ndef test_lane_deviation_penalty():\n    penalty = RewardCalculator.get_lane_deviation_penalty(lane_deviation=100, time_passed=0.1)\n    assert penalty == pytest.approx(0)\n    penalty = RewardCalculator.get_lane_deviation_penalty(lane_deviation=300, time_passed=0.1)\n    assert penalty == pytest.approx(9)\n    penalty = RewardCalculator.get_lane_deviation_penalty(lane_deviation=300, time_passed=1e8)\n    assert penalty == pytest.approx(100)\n    penalty = RewardCalculator.get_lane_deviation_penalty(lane_deviation=300, time_passed=1e-8)\n    assert penalty == pytest.approx(0, abs=1e-6)\n    penalty = RewardCalculator.get_lane_deviation_penalty(lane_deviation=0, time_passed=0.1)\n    assert penalty == pytest.approx(0)\n    penalty = RewardCalculator.get_lane_deviation_penalty(lane_deviation=1e8, time_passed=0.1)\n    assert penalty == pytest.approx(100)\n    with pytest.raises(ValueError):\n        RewardCalculator.get_lane_deviation_penalty(lane_deviation=-1, time_passed=0.1)\n\n\ndef test_gforce_penalty():\n    penalty = RewardCalculator.get_gforce_penalty(gforces=1, time_passed=0.1)\n    assert penalty == pytest.approx(2.4)\n    penalty = RewardCalculator.get_gforce_penalty(gforces=5, time_passed=1e8)\n    assert penalty == pytest.approx(100)\n    penalty = RewardCalculator.get_gforce_penalty(gforces=5, time_passed=1e-8)\n    assert penalty == pytest.approx(0, abs=1e-5)\n    penalty = RewardCalculator.get_gforce_penalty(gforces=0, time_passed=0.1)\n    assert penalty == pytest.approx(0)\n    penalty = RewardCalculator.get_gforce_penalty(gforces=1e8, time_passed=0.1)\n    assert penalty == pytest.approx(100)\n    with pytest.raises(ValueError):\n        RewardCalculator.get_gforce_penalty(gforces=-5, time_passed=1e8)\n\n\ndef test_progress_reward():\n    progress_reward, speed_reward, _ = RewardCalculator.get_progress_and_speed_reward(\n        progress_cm=100, time_passed=0.1)\n    assert progress_reward == pytest.approx(1.) and speed_reward == pytest.approx(1.5)\n    progress_reward, speed_reward, _ = RewardCalculator.get_progress_and_speed_reward(\n        progress_cm=100, time_passed=1)\n    assert progress_reward == pytest.approx(1.) and speed_reward == pytest.approx(0.15)\n    progress_reward, speed_reward, _ = RewardCalculator.get_progress_and_speed_reward(\n        progress_cm=3, time_passed=0.1)\n    assert progress_reward == pytest.approx(0.03) and speed_reward == pytest.approx(0.00135)\n    progress_reward, speed_reward, _ = RewardCalculator.get_progress_and_speed_reward(\n        progress_cm=3, time_passed=1e-8)\n    assert progress_reward == pytest.approx(0.03, abs=1e-6) and speed_reward == pytest.approx(100.0)  # Should clip\n    progress_reward, speed_reward, _ = RewardCalculator.get_progress_and_speed_reward(\n        progress_cm=0, time_passed=0.1)\n    assert progress_reward == pytest.approx(0.) and speed_reward == pytest.approx(0.)\n    progress_reward, speed_reward, _ = RewardCalculator.get_progress_and_speed_reward(\n        progress_cm=-10, time_passed=0.1)\n    assert progress_reward == pytest.approx(-0.1) and speed_reward == pytest.approx(-0.015)\n    progress_reward, speed_reward, _ = RewardCalculator.get_progress_and_speed_reward(\n        progress_cm=1e8, time_passed=0.1)\n    assert progress_reward == pytest.approx(100.) and speed_reward == pytest.approx(100.)  # Should clip\n    progress_reward, speed_reward, _ = RewardCalculator.get_progress_and_speed_reward(\n        progress_cm=-1e8, time_passed=0.1)\n    assert progress_reward == pytest.approx(0.) and speed_reward == pytest.approx(0.)  # trip complete, zero out\n\n    # Test invariance of sampling frequency\n    p1, r1 = episode_progress_reward(hz=1,   total_secs=10)\n    p2, r2 = episode_progress_reward(hz=2,   total_secs=10)\n    p3, r3 = episode_progress_reward(hz=0.5, total_secs=10)\n    assert p1 == pytest.approx(p2) and r1 == pytest.approx(r2) and p2 == pytest.approx(p3) and r2 == pytest.approx(r3)\n\n\ndef episode_progress_reward(hz, total_secs):\n    total_progress = total_speed_reward = 0\n    time_passed = 1 / hz\n    progress = 1000 / hz\n    for i in range(int(total_secs * hz)):\n        progress_reward, speed_reward, _ = RewardCalculator.get_progress_and_speed_reward(\n            progress_cm=progress,\n            time_passed=time_passed)\n        total_progress += progress_reward\n        total_speed_reward += speed_reward\n    return total_progress, total_speed_reward\n\n\ndef test_preprocess_image(tf_sess):\n    rng = RandomState(0)\n    img = rng.rand(1920, 1200)\n    p1 = utils.preprocess_image(img)\n    if tf_sess is not None:\n        p2 = tf_utils.preprocess_image(img, tf_sess)\n        assert np.max(p1 - p2) <= 1\n    assert p1[0][0] == 194\n    assert p1[-1][-1] == 193\n    assert p1[p1.shape[0] // 2][p1.shape[1] // 2] == 126\n    assert list(p1[p1.shape[0] // 3])[:100] == [179, 149, 229, 252, 225, 223, 243, 96, 211, 198, 212, 249, 190, 247,\n                                                197, 241, 163, 172, 247, 237, 101, 120, 41, 90, 206, 94, 185, 73, 172,\n                                                158, 225, 224, 207, 207, 248, 194, 150, 250, 242, 146, 211, 215, 237,\n                                                40, 244, 239, 202, 208, 233, 147, 123, 251, 199, 168, 250, 254, 144,\n                                                252, 224, 215, 227, 216, 199, 161, 115, 253, 135, 77, 105, 183, 176,\n                                                143, 117, 202, 203, 243, 122, 215, 219, 124, 187, 254, 247, 133, 76, 39,\n                                                242, 205, 236, 192, 192, 103, 193, 251, 35, 248, 146, 221, 46, 186]\n\n\ndef test_preprocess_depth(tf_sess):\n    rng = RandomState(0)\n    depth = rng.rand(1920, 1200)\n    p1 = utils.preprocess_depth(depth)\n    if tf_sess is not None:\n        p2 = tf_utils.preprocess_image(depth, tf_sess)\n        assert np.max(p1 - p2) <= 1\n    actual = list(p1.flatten()[p1.size // 3:p1.size // 3 + 100])\n    expected = [0.002514684773514457, 0.0041350987147154988, 0.00067722453287451874, 6.0382635299006502e-05,\n                0.00082243375692568252, 0.000881154928328925, 0.0003015844024133575, 0.0089413299711420015,\n                0.0012668913602365804, 0.0017493899057672484, 0.0012297302821309404, 0.00014536841866812092,\n                0.0020726431321461147, 0.00019011504751303418, 0.0017598160353674602, 0.00034701931169399186,\n                0.0033050015045214651, 0.0028537698444585334, 0.00018769877621070891, 0.00046990796258813728,\n                0.0082884580709213836, 0.0063293130559742986, 0.02410169598058122, 0.0098601405999210537,\n                0.0014280333911834955, 0.009190239693315394, 0.0022671623482907188, 0.01291206819927891,\n                0.0028541757203758389, 0.0035968311309652603, 0.00080925787214987009, 0.00084134980953235545,\n                0.0014126927712115813, 0.0013990039275166234, 0.00016829517477263774, 0.0018799619925731936,\n                0.0040850057101084773, 0.00010090817999749792, 0.00033563212584004454, 0.0043546132884055479,\n                0.0012573811077964459, 0.0011374194166438882, 0.00045884926209596846, 0.02479401838835903,\n                0.00027300163051291075, 0.00039739814613985009, 0.0015890552578129568, 0.0013716619839378632,\n                0.0005784882590990733, 0.0042358330268640679, 0.0060610271132032099, 9.6305884230573046e-05,\n                0.0016883902282087526, 0.0030552248204359788, 0.00011060120551770693, 9.7939084781005703e-07,\n                0.0044259935197632737, 5.5585308283770396e-05, 0.00085706376157983081, 0.0011429927648087563,\n                0.00073759259177592473, 0.0010953049908472632, 0.0016877924426287776, 0.0034122647675984574,\n                0.0068016515868866353, 4.6687312682328084e-05, 0.0050817640491964402, 0.01197324427120547,\n                0.0078595122255692915, 0.0023387198527116881, 0.0026548165841310317, 0.0045282857380305784,\n                0.0065714183871598449, 0.0015699930675901715, 0.0015339906295552164, 0.00030075231572516281,\n                0.0061212924601392986, 0.0011359644398976096, 0.0010003071142535968, 0.0059864809477887075,\n                0.0021943800369656633, 1.4161412765171874e-05, 0.00017803344629484447, 0.005255369976266736,\n                0.012265895735866821, 0.0251301315909821, 0.00032247668159115258, 0.0014601893690483472,\n                0.00049330646650431163, 0.0019625317886386704, 0.0019577928547728561, 0.0080632104203648101,\n                0.0019493655410045428, 8.9598907012599079e-05, 0.028066647603396212, 0.00017480272492269728,\n                0.0043565111781060008, 0.00094263459389571725, 0.021517855433459819, 0.002208296477107196]\n    assert np.max(actual - np.array(expected)) < 1e-7\n"""
vendor/tensorflow/__init__.py,0,b''
agents/bootstrap_rl/train/__init__.py,0,b''
agents/bootstrap_rl/train/train.py,6,"b'from __future__ import (absolute_import, division,\n                        print_function, unicode_literals)\n\nfrom future.builtins import (super)\n\nimport gym\nimport tensorflow as tf\nimport numpy as np\nfrom gym import spaces\n\nimport config as c\nimport sim\nfrom agents.common import get_throttle\nfrom agents.dagger.agent import Agent\nfrom config import MOBILENET_V2_NAME, MOBILENET_V2_IMAGE_SHAPE\nfrom sim.driving_style import DrivingStyle\nfrom sim.action import Action\nfrom util.experience_buffer import ExperienceBuffer\nfrom vendor.openai.baselines.ppo2.run_deepdrive import train\n\n\nclass BootstrapRLGymEnv(gym.Wrapper):\n    """"""\n    Bootstrap is probably a bad name here due to its overloaded use in RL\n    where bootstrapping historically refers to learning with value based or\n    TDD methods.\n    """"""\n    def __init__(self, env, dagger_agent, driving_style=DrivingStyle.STEER_ONLY):\n        """"""\n        Normalize the brake and\n        handbrake space to be between -1 and 1 so\n        that all actions have the same dimension -\n        Then create a single box space. The is_game_driving space\n        can be ignored for now within the ppo agent.\n        """"""\n        super(BootstrapRLGymEnv, self).__init__(env)\n\n        self.denormalizers = None\n        self.combine_action_spaces()\n\n        self.dagger_agent = dagger_agent\n        self.driving_style = driving_style\n        self.previous_obz = None\n        self.experience_buffer = ExperienceBuffer()\n\n        self.simple_test = c.SIMPLE_PPO\n\n        # One thing we need to do here is to make each action a bi-modal guassian to avoid averaging 50/50 decisions\n        # i.e. half the time we veer left, half the time veer right - but on average this is go straight and can run us\n        # into an obstacle. right now the DiagGaussianPd is just adding up errors which would not be the right\n        # thing to do for a bi-modal guassian. also, DiagGaussianPd assumes steering and throttle are\n        # independent which is not the case (steering at higher speeds causes more acceleration a = v**2/r),\n        # so that may be a problem as well.\n\n        if self.simple_test:\n            shape = (5,)\n        else:\n            # TODO(post v3): Add prior 200ms, 500ms, 1s and 2s mobilenet activations, along with speed, acceleration, and other stats we get from obz\n            speed_length = 1\n            acceleration_length = 3  # x,y,z\n            previous_output_length = 3  # steer,throttle,handbrake\n\n            # obz_length = dagger_agent.net.num_last_hidden + dagger_agent.net.num_targets\n            #\n            # shape = (obz_length * self.experience_buffer.fade_length,)\n\n            shape = (dagger_agent.net.num_last_hidden + dagger_agent.net.num_targets,)\n\n        self.observation_space = spaces.Box(low=np.finfo(np.float32).min,\n                                            high=np.finfo(np.float32).max,\n                                            # shape=(c.ALEXNET_FC7 + c.NUM_TARGETS,),\n                                            shape=shape,\n                                            dtype=np.float32)\n\n    def step(self, action):\n\n        # Denormalize the action into the original high and low for the space\n        action = [[denorm(action[i])] for i, denorm in\n                  enumerate(self.denormalizers)]\n\n        if self.driving_style == DrivingStyle.STEER_ONLY and self.previous_obz is not None:\n            # Simplifying by only controlling steering. Otherwise, we need to shape rewards so that initial acceleration\n            # is not disincentivized by gforce penalty.\n            action[Action.THROTTLE_INDEX] = [get_throttle(\n                actual_speed=self.previous_obz[\'speed\'],\n                target_speed=(8 * 100))]\n            action[Action.BRAKE_INDEX] = [0]\n            action[Action.HANDBRAKE_INDEX] = [0]\n\n        obz, reward, done, info = self.env.step(action)\n        if \'episode_return\' in info and \'episode_time\' in info[\'episode_return\']:\n            self.experience_buffer.maybe_add(obz, info[\'episode_return\'][\'episode_time\'])\n        self.previous_obz = obz\n        action, net_out = self.dagger_agent.act(obz, reward, done)\n        if net_out is None:\n            obz = None\n        else:\n            if self.simple_test:\n                obz = np.array([np.squeeze(a) for a in action])\n            else:\n                obz = np.concatenate((np.squeeze(net_out[0]), np.squeeze(net_out[1])))\n        return obz, reward, done, info\n\n    def reset(self):\n        return self.env.reset()\n\n    def combine_action_spaces(self):\n        """"""\n        Normalize the brake and handbrake space to be between -1 and 1 so\n        that all actions have the same dimension -\n        Then create a single box space. The is_game_driving space\n        can be ignored for now within the ppo agent.\n        """"""\n        ac_space = self.action_space\n        if isinstance(ac_space, gym.spaces.Tuple):\n            self.denormalizers = []\n            box_spaces = [s for s in ac_space.spaces if\n                          isinstance(s, gym.spaces.Box)]\n            total_dims = 0\n            for i, space in enumerate(box_spaces):\n                if len(space.shape) > 1 or space.shape[0] > 1:\n                    raise NotImplementedError(\n                        \'Multi-dimensional box spaces not yet supported - need to flatten / separate\')\n                else:\n                    total_dims += 1\n                self.denormalizers.append(\n                    self.get_denormalizer(space.high[0], space.low[0]))\n            self.action_space = gym.spaces.Box(-1, 1, shape=(total_dims,))\n\n    @staticmethod\n    def get_denormalizer(high, low):\n        def denormalizer(x):\n            ret = (x + 1) * (high - low) / 2 + low\n            return ret\n        return denormalizer\n\n\ndef run(env_id, bootstrap_net_path,\n        resume_dir=None, experiment=None, camera_rigs=None, render=False, fps=c.DEFAULT_FPS,\n        should_record=False, is_discrete=False, agent_name=MOBILENET_V2_NAME, is_sync=True,\n        driving_style=DrivingStyle.NORMAL, is_remote_client=False, eval_only=False,\n        sim_args=None):\n    tf_config = tf.ConfigProto(\n        allow_soft_placement=True,\n        intra_op_parallelism_threads=1,\n        inter_op_parallelism_threads=1,\n        gpu_options=tf.GPUOptions(\n            per_process_gpu_memory_fraction=0.4,\n            # leave room for the game,\n            # NOTE: debugging python, i.e. with PyCharm can cause OOM errors, where running will not\n            allow_growth=True\n        ),\n    )\n\n    g_1 = tf.Graph()\n    with g_1.as_default():\n        sess_1 = tf.Session(config=tf_config)\n\n        with sess_1.as_default():\n            sim_args_for_rl = dict(\n                experiment=experiment, env_id=env_id, cameras=camera_rigs,\n                render=render, fps=fps,\n                is_sync=is_sync, driving_style=driving_style,\n                is_remote_client=is_remote_client, should_record=should_record,\n                image_resize_dims=MOBILENET_V2_IMAGE_SHAPE,\n                should_normalize_image=True)\n\n            sim_args_dict = sim_args.to_dict()\n            sim_args_dict.update(sim_args_for_rl)\n\n            dagger_gym_env = sim.start(**sim_args_dict)\n            dagger_agent = Agent(\n                sess_1, should_jitter_actions=False,\n                net_path=bootstrap_net_path, output_last_hidden=True,\n                net_name=MOBILENET_V2_NAME)\n\n    g_2 = tf.Graph()\n    with g_2.as_default():\n        sess_2 = tf.Session(config=tf_config)\n\n        with sess_2.as_default():\n\n            # Wrap step so we get the pretrained layer activations rather than pixels for our observation\n            bootstrap_gym_env = BootstrapRLGymEnv(dagger_gym_env, dagger_agent, driving_style)\n\n            if c.SIMPLE_PPO:\n                minibatch_steps = 16\n                mlp_width = 5\n            else:\n                minibatch_steps = 80\n                mlp_width = 64\n            train(bootstrap_gym_env, seed=c.RNG_SEED, sess=sess_2, is_discrete=is_discrete,\n                  minibatch_steps=minibatch_steps, mlp_width=mlp_width, eval_only=eval_only)\n    #\n    # action = deepdrive.action()\n    # while not done:\n    #     observation, reward, done, info = gym_env.step(action)\n\n\n'"
agents/dagger/train/__init__.py,0,b''
agents/dagger/train/data_utils.py,0,"b'from __future__ import (absolute_import, division,\n                        print_function, unicode_literals)\n\nfrom future.builtins import (int, open, round,\n                             str)\n\nimport glob\nimport multiprocessing\nimport threading\nfrom collections import deque\nfrom multiprocessing import Pool\n\n\nimport numpy as np\n\nfrom utils import read_hdf5\nimport config as c\nimport logs\n\nlog = logs.get_log(__name__)\n\n\nclass BackgroundGenerator(threading.Thread):\n    def __init__(self, generator, should_shuffle=False):\n        threading.Thread.__init__(self)\n        self.queue = deque()\n        self.generator = generator\n        self.daemon = True\n        self.should_shuffle = should_shuffle\n        self.cv = threading.Condition()\n        self.start()\n\n    def run(self):\n        for item in self.generator:\n            self._insert_item(item)\n        log.debug(\'inserting none\')\n        self.queue.append(None)\n\n    def _insert_item(self, item):\n        with self.cv:\n            log.debug(\'queue length %r\', len(self.queue))\n            while len(self.queue) > c.NUM_TRAIN_FILES_TO_QUEUE:\n                log.debug(\'waiting for queue size to decrease\')\n                self.cv.wait()\n            if self.should_shuffle:\n                queue_index = c.rng.randint(0, len(self.queue) + 1)\n                log.debug(\'inserting randomly at\', queue_index)\n                self.queue.insert(queue_index, item)\n            else:\n                self.queue.append(item)\n            log.debug(\'inserted, queue length is %r item was None %r\',\n                      len(self.queue), item is None)\n            self.cv.notify()  # Tell consumer we have more\n\n    def __iter__(self):\n        return self\n\n    def __next__(self):\n        log.debug(\'getting item\')\n        with self.cv:\n            log.debug(\'in cv getting item\')\n            while len(self.queue) == 0:\n                log.debug(\'waiting for item\')\n                self.cv.wait()\n                log.debug(\'out of cv getting item\')\n            next_item = self.queue.popleft()\n            self.cv.notify()   # Tell producer we want more\n        if next_item is None:\n            log.debug(\'next item is None, ending!!!!\')\n            raise StopIteration\n        # print(\'returning item!\', next_item)\n\n        return next_item\n\n\ndef get_file_names(hdf5_path, train=True, overfit=False):\n    files = glob.glob(hdf5_path + \'/**/*.hdf5\', recursive=True) \\\n            or glob.glob(hdf5_path + \'*.hdf5\', recursive=True)\n    c.rng.shuffle(files)\n    if overfit:\n        files = files[-1:]\n    elif train and len(files) > 1:\n        files = files[1:]\n    elif not train:\n        # Eval\n        if len(files) == 1:\n            egregious_error_message()\n        files = files[0:1]\n    if len(files) == 0:\n        raise Exception(\'zero %s hdf5 files, aborting!\' %\n                        \'train\' if train else \'eval\')\n    return files\n\n\ndef egregious_error_message():\n    log.error(""""""\n .----------------. \n| .--------------. |\n| |              | |\n| |      _       | |\n| |     | |      | |\n| |     | |      | |\n| |     | |      | |\n| |     |_|      | |\n| |     (_)      | |\n| \'--------------\' |\n \'----------------\'             \n            \nOnly one file in the\n      dataset.\n                  \nWill eval on train!!!\n"""""")\n\n\ndef load_file(h5_filename, overfit=False, mute_spurious_targets=False):\n    log.info(\'loading %s\', h5_filename)\n    out_images = []\n    out_targets = []\n    try:\n        frames = read_hdf5(h5_filename, overfit=overfit)\n        c.rng.shuffle(frames)\n        for frame in frames:\n            # Just use one camera for now\n            out_images.append(frame[\'cameras\'][0][\'image\'])\n            out_targets.append([*normalize_frame(frame, mute_spurious_targets)])\n    except Exception as e:\n        log.error(\'Could not load %s - skipping - error was: %r\',\n                  h5_filename, e)\n\n    log.info(\'finished loading %s\', h5_filename)\n    return out_images, out_targets\n\n\ndef normalize_frame(frame, mute_spurious_targets=False):\n    spin = frame[\'angular_velocity\'][2]\n    if spin <= -c.SPIN_THRESHOLD:\n        direction = -1.0\n    elif spin >= c.SPIN_THRESHOLD:\n        direction = 1.0\n    else:\n        direction = 0.0\n    spin = spin / c.SPIN_NORMALIZATION_FACTOR\n    speed = frame[\'speed\'] / c.SPEED_NORMALIZATION_FACTOR\n    if mute_spurious_targets:\n        speed_change = 0.\n        direction = 0.\n        spin = 0.\n    else:\n        speed_change = np.dot(\n            frame[\'acceleration\'],\n            frame[\'forward_vector\']) / c.SPEED_NORMALIZATION_FACTOR\n    steering = frame[\'steering\']\n    throttle = frame[\'throttle\']\n    return spin, direction, speed, speed_change, steering, throttle\n\n\ndef file_loader(file_stream, overfit=False, mute_spurious_targets=False):\n    for h5_filename in file_stream:\n        log.info(\'loading %s\', h5_filename)\n        yield load_file(h5_filename, overfit, mute_spurious_targets)\n    log.info(\'finished training files\')\n\n\ndef batch_gen(file_stream, batch_size, overfit=False,\n              mute_spurious_targets=False):\n    gen = BackgroundGenerator(\n        file_loader(file_stream, overfit, mute_spurious_targets),\n        should_shuffle=False)\n    for images, targets in gen:\n        if overfit:\n            images, targets = images[0:batch_size], targets[0:batch_size]\n            num_repeats, remainder = divmod(batch_size, len(images))\n            if num_repeats > 1:\n                images = images * num_repeats\n                targets = targets * num_repeats\n                if remainder > 0:\n                    images = images + images[:remainder]\n                    targets = targets + targets[:remainder]\n            yield images, targets\n        else:\n            num_iters = len(images) // batch_size\n        # print(\'num iters\', num_iters)\n        # print(\'images\', images)\n\n            for i in range(num_iters):\n                yield images[i * batch_size:(i+1) * batch_size], \\\n                      targets[i * batch_size:(i+1) * batch_size]\n\n    print(\'finished batch gen\')\n\n\nclass Dataset(object):\n    def __init__(self, files, overfit=False, mute_spurious_targets=False):\n        self._files = files\n        self.overfit = overfit\n        self.mute_spurious_targets = mute_spurious_targets\n\n    def iterate_once(self, batch_size):\n        def file_stream():\n            for file_name in self._files:\n                log.info(\'queueing data from %s for iterate once\', file_name)\n                yield file_name\n        yield from batch_gen(file_stream(), batch_size,\n                             mute_spurious_targets=self.mute_spurious_targets)\n\n    def iterate_forever(self, batch_size):\n        def file_stream():\n            while True:\n                # File order will be the same every epoch\n                c.rng.shuffle(self._files)\n                for file_name in self._files:\n                    log.info(\'queueing data from %s for iterate forever\',\n                             file_name)\n                    yield file_name\n        yield from batch_gen(file_stream(), batch_size, self.overfit,\n                             self.mute_spurious_targets)\n\n    def iterate_parallel_once(self, get_callback):\n        with Pool(max(multiprocessing.cpu_count() // 2, 1)) as p:\n            for i, file in enumerate(self._files):\n                # get_callback(i)(load_file(file, self.overfit,\n                # self.mute_spurious_targets))\n                p.apply_async(load_file,\n                              (file, self.overfit, self.mute_spurious_targets),\n                              callback=get_callback(i))\n            p.close()\n            p.join()\n\n\ndef get_dataset(hdf5_path, train=True, overfit=False,\n                mute_spurious_targets=False):\n    file_names = get_file_names(hdf5_path, train=train, overfit=overfit)\n    return Dataset(file_names, overfit, mute_spurious_targets)\n\n\ndef run():\n    hdf5_path = c.RECORDING_DIR\n    log.info(get_file_names(hdf5_path, train=True))\n    dataset = get_dataset(hdf5_path, train=True)\n    log.info(dataset)\n    log.info(\'Iterating through recordings\')\n    for images, targets in dataset.iterate_once(64):\n        for image in images:\n            if image.shape != (227,227,3):\n                log.info(\'FOUND %r\', image.shape)\n\n\nif __name__ == ""__main__"":\n    run()\n'"
agents/dagger/train/hdf5_to_tfrecord.py,27,"b'from __future__ import (absolute_import, division,\n                        print_function, unicode_literals)\n\nfrom future.builtins import (ascii, bytes, chr, dict, filter, hex, input,\n                             int, map, next, oct, open, pow, range, round,\n                             str, super, zip)\n\nimport glob\nimport os\nimport sys\n\nimport tensorflow as tf\n\nimport utils\nfrom agents.dagger.train.data_utils import get_dataset\nimport logs\nimport config as c\n\n\nlog = logs.get_log(__name__)\n\nINPUT_IMAGE_SHAPE = (224, 224, 3)\n\n\ndef _int64_feature(value):\n    """"""Wrapper for inserting int64 features into Example proto.""""""\n    if not isinstance(value, list):\n        value = [value]\n    return tf.train.Feature(int64_list=tf.train.Int64List(value=value))\n\n\ndef _float_feature(value):\n    """"""Wrapper for inserting float features into Example proto.""""""\n    if not isinstance(value, list):\n        value = [value]\n    return tf.train.Feature(float_list=tf.train.FloatList(value=value))\n\n\ndef _bytes_feature(value):\n    """"""Wrapper for inserting bytes features into Example proto.""""""\n    return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))\n\n\ndef _floats_feature(value):\n    """"""\n    Wrapper for inserting numpy 32 bit float arrays features\n    into Example proto.\n    """"""\n    return tf.train.Feature(\n        float_list=tf.train.FloatList(value=value.reshape(-1)))\n\n\ndef add_total_to_tfrecord_files(directory, filename_prefix):\n    all_files = glob.glob(os.path.join(directory, filename_prefix) +\n                          \'_\' + \'[0-9]\' * 5 + \'*.tfrecord\')\n    files_to_rename = []\n    for file in all_files:\n        if os.path.getsize(file) == 0:\n            log.warn(\'Encountered an empty tfrecord file! Deleting %s\', file)\n            os.remove(file)\n        else:\n            files_to_rename.append(file)\n    mid_fix = \'-of-%s\' % str(len(files_to_rename) - 1).zfill(5)\n    for i, file in enumerate(sorted(files_to_rename)):\n        fname = os.path.basename(file)\n        os.rename(file, os.path.join(directory, fname[:16] + str(i).zfill(5)\n                                     + mid_fix + \'.tfrecord\'))\n\n\ndef save_dataset(dataset, buffer_size, filename, out_path, parallelize=True):\n    if parallelize:\n        def get_callback(_file_idx):\n            log.debug(\'getting callback for %d\', _file_idx)\n            \n            def callback(result):\n                log.debug(\'inside callback for %r\', _file_idx)\n                imgs, tgts = result\n                save_tfrecord_file(_file_idx, filename, imgs, tgts)\n            return callback\n\n        dataset.iterate_parallel_once(get_callback)\n    else:\n        file_idx = 0\n        for images, targets in dataset.iterate_once(buffer_size):\n            log.info(\'starting file %d\', file_idx)\n            save_tfrecord_file(file_idx, filename, images, targets)\n            file_idx += 1\n\n    add_total_to_tfrecord_files(out_path, filename)\n\n\ndef save_tfrecord_file(file_idx, filename, images, targets):\n    utils.assert_disk_space(filename)\n    colorspace = b\'RGB\'\n    channels = 3\n    image_format = b\'RAW\'\n    out_filename = filename + \'_\' + str(file_idx).zfill(5) + \'.tfrecord\'\n    writer = tf.python_io.TFRecordWriter(out_filename)\n    utils.resize_images(INPUT_IMAGE_SHAPE, images)\n    for image_idx in range(len(images)):\n\n        if not image_idx % 500:\n            log.info(\'{}/{} examples saved to {}\'.format(\n                image_idx, len(images), out_filename))\n\n        image = images[image_idx]\n\n        # Undo initial subtraction of mean pixel during recording\n        image += c.MEAN_PIXEL.astype(image.dtype)\n\n        if image.min() < 0:\n            raise ValueError(\'Min image less than zero\')\n\n        target = targets[image_idx]\n\n        feature_dict = {\n            \'image/width\': _int64_feature(image.shape[0]),\n            \'image/height\': _int64_feature(image.shape[1]),\n            \'image/colorspace\': _bytes_feature(colorspace),\n            \'image/channels\': _int64_feature(channels),\n            \'image/format\': _bytes_feature(image_format),\n            \'image/encoded\': _bytes_feature(\n                tf.compat.as_bytes(image.tostring()))}\n\n        for name_idx, name in enumerate(c.CONTROL_NAMES):\n            feature_dict[\'image/control/\' + name] = \\\n                _float_feature(target[name_idx])\n\n        example = tf.train.Example(\n            features=tf.train.Features(feature=feature_dict))\n\n        # Serialize to string and write on the file\n        writer.write(example.SerializeToString())\n    writer.close()\n    log.info(\'Wrote %r examples to %s\', len(images), out_filename)\n\n\ndef encode(parallelize=True, hdf5_path=c.RECORDING_DIR, experiment=None):\n    # TODO(post v3): Get a couple separate hdf5 files from different\n    #  situations / view modes for eval\n    hdf5_to_convert_path = get_hdf5_path_to_convert(hdf5_path)\n    train_dataset = get_dataset(hdf5_to_convert_path, train=True)\n    eval_dataset = get_dataset(hdf5_to_convert_path, train=False)\n    buffer_size = 1000\n    utils.assert_disk_space(hdf5_path)\n    out_path = None\n    while experiment is None:\n        experiment = utils.get_valid_filename(\n            input(\'Enter a name for your dataset: \'))\n        out_path = os.path.join(hdf5_path, experiment + c.TFRECORD_DIR_SUFFIX)\n        if os.path.exists(out_path):\n            print(\'The path %s exists, please choose a new name.\' % out_path)\n            experiment = None\n    if out_path is None:\n        raise RuntimeError(\'tfrecord output path not set\')\n    os.makedirs(out_path)\n    save_dataset(train_dataset, buffer_size,\n                 filename=os.path.join(out_path, \'deepdrive_train\'),\n                 parallelize=parallelize, out_path=out_path)\n    save_dataset(eval_dataset, buffer_size,\n                 filename=os.path.join(out_path, \'deepdrive_eval\'),\n                 parallelize=parallelize, out_path=out_path)\n\n\ndef get_hdf5_path_to_convert(hdf5_path):\n    hdf5_dirs = list(get_hdf5_dirs(hdf5_path))\n    if not hdf5_dirs:\n        raise RuntimeError(\'No directories with hdf5 files found in %s\' %\n                           hdf5_path)\n    convert_all = 1\n    convert_latest = 2\n    convert_path = 3\n    options = {\n        convert_all: \'Convert all HDF5 recording to tfrecords\',\n        convert_latest: \'Convert latest (%s)\' % hdf5_dirs[0],\n        convert_path: \'Enter a path to convert\', }\n    option = None\n    if \'DEEPDRIVE_CONVERT_ALL_HDF5\' in os.environ:\n        option = convert_all\n    else:\n        done = False\n        print(\'Please choose an option:\')\n        msg = \'\\n\'.join([\'%d) %s\' % o for o in options.items()])\n        answer = input(msg + \'\\nOption #: \')\n        while not done:\n            try:\n                option = int(answer)\n                _ = options[option]\n            except (ValueError, KeyError):\n                answer = input(\'Invalid option. Please try again: \')\n            else:\n                done = True\n    if option is None:\n        raise RuntimeError(\'No option chosen for HDF5 conversion\')\n    if option == convert_path:\n        hdf5_path = input(\n            \'Please input the absolute path to the directory \'\n            \'containing hdf5 files you want to convert: \').strip()\n    elif option == convert_latest:\n        hdf5_path = os.path.join(hdf5_path, hdf5_dirs[0])\n    return hdf5_path\n\n\ndef get_hdf5_dirs(hdf5_path):\n    for d in sorted(next(os.walk(hdf5_path))[1], reverse=True):\n        has_hdf5 = glob.glob(os.path.join(hdf5_path, d, \'*.hdf5\'))\n        sensible_year = d[0:4].isdigit() and int(d[0:4]) > 2000\n        if has_hdf5 and sensible_year:\n            yield d\n\n\ndef test_decode():\n    data_path = os.path.join(c.RECORDING_DIR,\n                             \'deepdrive_train_00000-of-00162.tfrecord\')\n\n    with tf.Session() as sess:\n        feature = {\n            \'image/width\': tf.FixedLenFeature([], tf.int64),\n            \'image/height\': tf.FixedLenFeature([], tf.int64),\n            \'image/colorspace\': tf.FixedLenFeature([], tf.string),\n            \'image/channels\': tf.FixedLenFeature([], tf.int64),\n            \'image/format\': tf.FixedLenFeature([], tf.string),\n            \'image/encoded\': tf.FixedLenFeature([], tf.string)}\n\n        # Create a list of filenames and pass it to a queue\n        filename_queue = tf.train.string_input_producer([data_path],\n                                                        num_epochs=1)\n        # Define a reader and read the next record\n        reader = tf.TFRecordReader()\n        _, serialized_example = reader.read(filename_queue)\n        # Decode the record read by the reader\n        features = tf.parse_single_example(serialized_example, features=feature)\n        # Convert the image data from string back to the numbers\n        image = tf.decode_raw(features[\'image/encoded\'], tf.uint8)\n\n        sess.run(tf.local_variables_initializer())\n        sess.run(tf.global_variables_initializer())\n\n        coord = tf.train.Coordinator()\n        threads = tf.train.start_queue_runners(coord=coord)\n        for i in range(1000):\n            example = sess.run(image)\n            print(example)\n        coord.request_stop()\n        coord.join(threads)\n        # image_np = sess.run(image)\n\n        # # Cast label data into int32\n        # label = tf.cast(features[\'train/label\'], tf.int32)\n        # # Reshape image data into the original shape\n        # image = tf.reshape(image, [224, 224, 3])\n        #\n        # # Any preprocessing here ...\n        #\n        # # Creates batches by randomly shuffling tensors\n        # images, labels = tf.train.shuffle_batch([image, label],\n        #     batch_size=10, capacity=30, num_threads=1, min_after_dequeue=10)\n\n\nif __name__ == \'__main__\':\n    log.info(\'Converting HDF5 to tf record\')\n    if \'--decode\' in sys.argv:\n        test_decode()\n    elif \'--rename-only\' in sys.argv:\n        add_total_to_tfrecord_files(c.RECORDING_DIR, \'deepdrive_train\')\n        add_total_to_tfrecord_files(c.RECORDING_DIR, \'deepdrive_eval\')\n    else:\n        encode(parallelize=(\'sync\' not in sys.argv))\n'"
agents/dagger/train/train.py,37,"b'from __future__ import (absolute_import, division,\n                        print_function, unicode_literals)\n\nimport glob\n\nfrom future.builtins import (ascii, bytes, chr, dict, filter, hex, input,\n                             int, map, next, oct, open, pow, range, round,\n                             str, super, zip)\n\nimport os\n\nimport numpy as np\nimport tensorflow as tf\n\nimport config as c\nimport utils\nfrom agents.dagger import net\nfrom agents.dagger.net import Net, AlexNet, MobileNetV2\nfrom agents.dagger.train.data_utils import get_dataset\nfrom agents.dagger.train.train_mobilenet_v2 import train_mobile_net\n\nfrom agents.dagger.train.train_mobilenet_v2 import eval_mobile_net\nfrom util.download import download, has_stuff\nimport logs\n\nlog = logs.get_log(__name__)\n\n\ndef visualize_model(model_in, model_out, y):\n    for i in range(len(c.CONTROL_NAMES)):\n        p = tf.reduce_mean(model_out[:, i])\n        tf.summary.scalar(""losses/{}/p"".format(c.CONTROL_NAMES[i]), tf.reduce_mean(p))\n        err = 0.5 * tf.reduce_mean(tf.square(model_out[:, i] - y[:, i]))\n        tf.summary.scalar(""losses/{}/error"".format(c.CONTROL_NAMES[i]), err)\n    tf.summary.image(""model/x"", model_in, max_outputs=10)\n\n\ndef visualize_gradients(grads_and_vars):\n    grads = [g for g, v in grads_and_vars]\n    var_list = [v for g, v in grads_and_vars]\n    for g, v in grads_and_vars:\n        if g is None:\n            continue\n        tf.summary.histogram(v.name, v)\n        tf.summary.histogram(v.name + ""/grad"", g)\n        tf.summary.scalar(""norms/"" + v.name, tf.global_norm([v]))\n        tf.summary.scalar(""norms/"" + v.name + ""/grad"", tf.global_norm([g]))\n    grad_norm = tf.global_norm(grads)\n    tf.summary.scalar(""model/grad_global_norm"", grad_norm)\n    tf.summary.scalar(""model/var_global_norm"", tf.global_norm(var_list))\n\n\ndef run(resume_dir=None, data_dir=c.RECORDING_DIR, agent_name=None,\n        overfit=False, eval_only=False, tf_debug=False,\n        freeze_pretrained=False, train_args_collection_name=None):\n    show_tfboard_hint()\n    if agent_name == c.DAGGER_MNET2:\n        if not glob.glob(data_dir + \'/*.tfrecord\'):\n            tfrecord_dirs = glob.glob(data_dir + \'/*\' + c.TFRECORD_DIR_SUFFIX)\n            if len(tfrecord_dirs) == 0:\n                raise RuntimeError(\'No tfrecord directories found\')\n            elif len(tfrecord_dirs) > 1:\n                dir_num = None\n                while dir_num is None:\n                    question = \'Which tfrecord directory would you like to train on?\'\n                    for i, tfrecord_dir in enumerate(tfrecord_dirs):\n                        question += \'\\n%d) %s\' % (i+1, tfrecord_dir)\n                    dir_num = int(input(question + \'\\nEnter index above: \').strip())\n                    if dir_num > len(tfrecord_dirs):\n                        dir_num = None\n                        print(\'Invalid directory number\')\n                data_dir = tfrecord_dirs[dir_num-1]\n        # Use tf-slim training\n        if eval_only:\n            eval_mobile_net(data_dir)\n        else:\n            train_mobile_net(data_dir, resume_dir, train_args_collection_name)\n    else:\n        custom_train_loop(agent_name, data_dir, eval_only,\n                          freeze_pretrained, overfit, resume_dir, tf_debug)\n\n\ndef custom_train_loop(agent_name, data_dir, eval_only,\n                      freeze_pretrained, overfit, resume_dir, tf_debug):\n    # TODO(post v3): Don\'t use generic word like \'model\' here that other projects often use.\n    # Need to rename/retrain saved models tho...\n    with tf.variable_scope(""model""):\n        global_step = tf.get_variable(""global_step"", [], tf.int32,\n                                      initializer=tf.zeros_initializer, trainable=False)\n    agent_net = get_agent_net(agent_name, global_step,\n                              eval_only=eval_only, freeze_pretrained=freeze_pretrained)\n    log.info(\'starter learning rate is %f\', agent_net.starter_learning_rate)\n    sess_eval_dir, sess_train_dir = get_dirs(resume_dir)\n    targets_tensor = tf.placeholder(tf.float32, (None, agent_net.num_targets))\n    total_loss = setup_loss(agent_net, targets_tensor)\n    opt = tf.train.AdamOptimizer(agent_net.learning_rate)\n    tf.summary.scalar(""model/learning_rate"", agent_net.learning_rate)\n    summary_op, sv, train_op = get_train_ops(agent_net, global_step, opt,\n                                             sess_train_dir, targets_tensor, total_loss)\n    eval_sw = tf.summary.FileWriter(sess_eval_dir)\n    train_dataset = get_dataset(data_dir, overfit=overfit,\n                                mute_spurious_targets=agent_net.mute_spurious_targets)\n    eval_dataset = get_dataset(data_dir, train=False,\n                               mute_spurious_targets=agent_net.mute_spurious_targets)\n    config = tf.ConfigProto(allow_soft_placement=True)\n    with sv.managed_session(config=config) as sess, sess.as_default():\n        if tf_debug:\n            from tensorflow.python import debug as tf_debug\n            sess = tf_debug.TensorBoardDebugWrapperSession(sess, \'localhost:6064\')\n        train_data_provider = train_dataset.iterate_forever(agent_net.batch_size)\n        while True:\n            if not eval_only:\n                for i in range(100):  # Change this to eval less or more often\n                    loss = train_batch(agent_net, i, sess, summary_op, sv, targets_tensor, train_data_provider,\n                                       train_op, total_loss)\n                    step = global_step.eval()\n\n                    log.info(\'step %d loss %f\', step, loss)\n\n            step = global_step.eval()\n            perform_eval(step, agent_net, agent_net.batch_size, eval_dataset, eval_sw, sess)\n\n\ndef show_tfboard_hint():\n    log.info(\'\\n\\n*********************************************************************\\n\'\n             \'Start tensorboard with \\n\\n\\ttensorboard --logdir=""\' + c.TENSORFLOW_OUT_DIR +\n             \'""\\n\\n(In Windows tensorboard will be in your python env\\\'s Scripts folder, \'\n             \'i.e. C:\\\\Users\\\\<YOU>\\\\Miniconda3\\\\envs\\\\tensorflow\\\\Scripts) but this should already be in your path \\n\'\n             \'Then navigate to http://localhost:6006 - You may see errors if Tensorboard was already \'\n             \'started / has tabs open. If so, shut down Tenosrboard first and close all Tensorboard tabs. \'\n             \'Sometimes you may just need to restart training if you get CUDA device errors.\'\n             \'\\n*********************************************************************\\n\\n\')\n\n\ndef get_dirs(resume_dir):\n    os.makedirs(c.TENSORFLOW_OUT_DIR, exist_ok=True)\n    date_str = c.DATE_STR\n\n    if resume_dir is not None:\n        sess_train_dir = resume_dir\n        sess_eval_dir = \'%s/%s_eval\' % (resume_dir, date_str)\n    else:\n        sess_train_dir = \'%s/%s_train\' % (c.TENSORFLOW_OUT_DIR, date_str)\n        sess_eval_dir = \'%s/%s_eval\' % (c.TENSORFLOW_OUT_DIR, date_str)\n    os.makedirs(sess_train_dir, exist_ok=True)\n    os.makedirs(sess_eval_dir, exist_ok=True)\n    return sess_eval_dir, sess_train_dir\n\n\ndef get_agent_net(agent_name, global_step, overfit=False, eval_only=False, freeze_pretrained=False):\n    if agent_name is None or agent_name == c.DAGGER:\n        agent_net_fn = AlexNet\n    elif agent_name == c.DAGGER_MNET2:\n        agent_net_fn = MobileNetV2\n    else:\n        raise NotImplementedError(\'%r agent_name not supported\' % agent_name)\n    agent_net = agent_net_fn(global_step, overfit=overfit, is_training=(not eval_only),\n                             freeze_pretrained=freeze_pretrained)\n    return agent_net\n\n\ndef get_train_ops(agent_net, global_step, opt, sess_train_dir, targets_tensor, total_loss):\n    grads_and_vars = opt.compute_gradients(total_loss)\n    visualize_model(agent_net.input, agent_net.out, targets_tensor)\n    visualize_gradients(grads_and_vars)\n    summary_op = tf.summary.merge_all()\n    update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n    with tf.control_dependencies(update_ops):\n        train_op = opt.apply_gradients(grads_and_vars, global_step)\n    init_fn = agent_net.get_tf_init_fn(tf.global_variables_initializer())\n    saver = tf.train.Saver()\n    sv = tf.train.Supervisor(is_chief=True,\n                             logdir=sess_train_dir,\n                             summary_op=None,  # Automatic summaries don\'t work with placeholders.\n                             saver=saver,\n                             global_step=global_step,\n                             save_summaries_secs=30,\n                             save_model_secs=60,\n                             init_op=None,\n                             init_fn=init_fn)\n    return summary_op, sv, train_op\n\n\ndef setup_loss(agent_net, targets_tensor):\n    l2_norm = tf.global_norm(tf.trainable_variables())\n    # steering_error = tf.reduce_mean(tf.abs(agent_net.out[:, 4] - targets_tensor[:, 4]))\n    # throttle_error = tf.reduce_mean(tf.abs(agent_net.out[:, 5] - targets_tensor[:, 5]))\n    # tf.summary.scalar(""steering_error/train"", steering_error)\n    # tf.summary.scalar(""throttle_error/train"", throttle_error)\n    # l2_norm = tf.Print(l2_norm, [steering_error], \'TRAIN STEERING ERROR IS!!!!!!!!! \', summarize=100)\n    loss = 0.5 * tf.reduce_sum(tf.square(agent_net.out - targets_tensor)) / tf.to_float(tf.shape(agent_net.input)[0])\n    tf.summary.scalar(""model/loss"", loss)\n    tf.summary.scalar(""model/l2_norm"", l2_norm)\n    total_loss = loss + agent_net.weight_decay * l2_norm\n    tf.summary.scalar(""model/total_loss"", total_loss)\n    return total_loss\n\n\ndef train_batch(agent_net, i, sess, summary_op, sv, targets_tensor, train_data_provider, train_op, total_loss):\n    images, targets = next(train_data_provider)\n    log.debug(\'num images %r\', len(images))\n    log.debug(\'num targets %r\', len(targets))\n    valid_target_shape = True\n    utils.resize_images(agent_net.input_image_shape, images)\n    loss = None\n    for tgt in targets:\n        if len(tgt) != 6:\n            log.error(\'invalid target shape %r skipping\' % len(tgt))\n            valid_target_shape = False\n    if valid_target_shape:\n        feed_dict = {agent_net.input: images, targets_tensor: targets}  # , \'phase:0\': 1}\n        if i % 10 == 0 and i > 0:\n            # Summarize: Do this less frequently to speed up training time, more frequently to debug issues\n            try:\n                _, summ, loss = sess.run([train_op, summary_op, total_loss], feed_dict)\n            except ValueError as e:\n                print(\'Error processing batch, skipping - error was %r\' % e)\n            else:\n                sv.summary_computed(sess, summ)\n                sv.summary_writer.flush()\n        else:\n            # print(\'evaluating %r\' % feed_dict)\n            try:\n                _, loss = sess.run([train_op, total_loss], feed_dict)\n            except ValueError as e:\n                print(\'Error processing batch, skipping - error was %r\' % e)\n    return loss\n\n\ndef perform_eval(step, agent_net, batch_size, eval_dataset, eval_sw, sess):\n    log.info(\'performing evaluation\')\n    losses = []\n    for images, targets in eval_dataset.iterate_once(batch_size):\n        utils.resize_images(agent_net.input_image_shape, images)\n        predictions = sess.run(agent_net.eval_out, {agent_net.input: images})\n        losses += [np.square(targets - predictions)]\n    losses = np.concatenate(losses)\n    summary = tf.Summary()\n    eval_loss = float(0.5 * losses.sum() / losses.shape[0])\n    log.info(\'eval loss %f\', eval_loss)\n    summary.value.add(tag=""eval/loss"", simple_value=eval_loss)\n    for i in range(len(c.CONTROL_NAMES)):\n        loss_component = float(0.5 * losses[:, i].mean())\n        loss_name = c.CONTROL_NAMES[i]\n        summary.value.add(tag=""eval/{}"".format(loss_name), simple_value=loss_component)\n        log.info(\'%s loss %f\', loss_name, loss_component)\n    eval_sw.add_summary(summary, step)\n    eval_sw.flush()\n\n\nif __name__ == ""__main__"":\n    run()\n'"
agents/dagger/train/train_mobilenet_v2.py,0,"b'from __future__ import (absolute_import, division,\n                        print_function, unicode_literals)\n\nfrom future.builtins import (ascii, bytes, chr, dict, filter, hex, input,\n                             int, map, next, oct, open, pow, range, round,\n                             str, super, zip)\n\nfrom datetime import datetime\nimport glob\nimport os\nimport sys\nfrom multiprocessing import Process\n\nimport config as c\nimport util.download\nimport utils\nfrom config import MOBILENET_V2_SLIM_NAME\nfrom agents.dagger.train import hdf5_to_tfrecord\nfrom install import check_tensorflow_gpu\nfrom vendor.tensorflow.models.research.slim.eval_image_nn import \\\n    slim_eval_image_nn\nfrom vendor.tensorflow.models.research.slim.train_image_nn import \\\n    slim_train_image_nn\nimport logs\n\nlog = logs.get_log(__name__)\n\nIMG_SIZE = 224\n\nTRAIN_ARG_COLLECTIONS = dict(\n    viewmode_simple=dict(\n        fine_tune_all_layers=dict(\n            max_number_of_steps=10 ** 5,\n        )\n    )\n)\n\n\ndef train_mobile_net(data_dir, resume_dir=None,\n                     train_args_collection_name=None):\n    """"""\n    Should see eval steering error of about 0.1135\n    Original Deepdrive 2.0 baseline steering error eval was ~0.2,\n    train steering error: ~0.08\n    """"""\n\n    if not check_tensorflow_gpu():\n        raise RuntimeError(\n            \'Invalid Tensorflow version detected. See above for details.\')\n\n    train_args = TRAIN_ARG_COLLECTIONS.get(train_args_collection_name, {})\n\n    if not os.path.exists(c.MNET2_PRETRAINED_PATH + \'.meta\'):\n        util.download.download(c.MNET2_PRETRAINED_URL + \'?cache_bust=1\', c.WEIGHTS_DIR,\n                               warn_existing=False, overwrite=True)\n\n    if not glob.glob(data_dir + \'/*.tfrecord\'):\n        if glob.glob(data_dir + \'/*/*.hdf5\'):\n            raise RuntimeError(\n                \'No tfrecords in %s - \'\n                \'Run main.py --hdf5-2-tfrecord --recording-dir=""%s"" \'\n                \'to convert hdf5 records\' %\n                (data_dir, data_dir))\n        else:\n            raise RuntimeError(\'No tfrecords found in %s - aborting\' % data_dir)\n\n    # Execute sessions in separate processes to ensure Tensorflow\n    # cleans up nicely.\n    # Without this, fine_tune_all_layers would crash towards the end with\n    #  Error polling for event status: failed to query event:\n    #  CUDA_ERROR_LAUNCH_FAILED:\n\n    if resume_dir is None:\n        train_dir = datetime.now().strftime(\n            os.path.join(c.TENSORFLOW_OUT_DIR, \'%Y-%m-%d__%I-%M-%S%p\'))\n        print(\'train_dir is \', train_dir)\n        isolate_in_process(fine_tune_new_layers,\n                           args=(data_dir, train_dir, train_args.get(\n                                     \'fine_tune_new_layers\', None)))\n        isolate_in_process(eval_mobile_net, args=(data_dir,))\n    else:\n        # TODO(post v3): Fix MNET2/tf-slim issue resuming with train_dir\n        train_dir = resume_dir\n        print(\'resume_dir is \', resume_dir)\n\n    isolate_in_process(fine_tune_all_layers, args=(\n    data_dir, train_dir, train_args.get(\'fine_tune_all_layers\', None)))\n    isolate_in_process(eval_mobile_net, args=(data_dir,))\n    log.info(\'Finished training\')\n\n\ndef isolate_in_process(target, args):\n    p = Process(target=target, args=args)\n    p.start()\n    p.join()\n    if p.exitcode != 0:\n        raise RuntimeError(\n""""""\nProcess finished with error. See above for details. HINTS: \n\n1) If you see CUDA errors like:\n\n        Error polling for event status: failed to query event: CUDA_ERROR_LAUNCH_FAILED\n\n    try restarting (esp. Windows), then running with \n    \n    --use-latest-model to resume training from the last checkpoint. \n\n2) Running training outside of IDE\'s like PyCharm seems to be more stable."""""")\n\n\ndef fine_tune_new_layers(data_dir, train_dir, train_args=None):\n    args = dict(\n        dataset_name=\'deepdrive\',\n        dataset_split_name=\'train\',\n        train_dir=train_dir,\n        dataset_dir=data_dir,\n        model_name=MOBILENET_V2_SLIM_NAME,\n        train_image_size=IMG_SIZE,\n        checkpoint_path=c.MNET2_PRETRAINED_PATH,\n        checkpoint_exclude_scopes=\'MobilenetV2/Logits,MobilenetV2/Predictions,MobilenetV2/predics\',\n        trainable_scopes=\'MobilenetV2/Logits,MobilenetV2/Predictions,MobilenetV2/predics\',\n        max_number_of_steps=2000,\n        batch_size=32,\n        learning_rate=0.0001,\n        learning_rate_decay_type=\'fixed\',\n        save_interval_secs=10,\n        save_summaries_secs=60,\n        log_every_n_steps=20,\n        optimizer=\'rmsprop\',\n        weight_decay=0.00004)\n    if train_args is not None:\n        args.update(train_args)\n    slim_train_image_nn(**args)\n\n\ndef fine_tune_all_layers(data_dir, train_dir, train_args=None):\n    args = dict(\n        dataset_name=\'deepdrive\',\n        checkpoint_path=train_dir,\n        dataset_split_name=\'train\',\n        dataset_dir=data_dir,\n        model_name=MOBILENET_V2_SLIM_NAME,\n        train_image_size=IMG_SIZE,\n        max_number_of_steps=9707,\n        # Performance degrades by 20e3 despite eval still dropping.\n        # TODO: Find out why\n        #  (perhaps https://github.com/felipecode/coiltraine will say more) -\n        #  basically we must do early stopping before loss indicates it to get\n        #  optimum driving performance\n        batch_size=16,\n        learning_rate=0.00004,\n        learning_rate_decay_type=\'fixed\',\n        save_interval_secs=180,\n        save_summaries_secs=60,\n        log_every_n_steps=20,\n        optimizer=\'rmsprop\',\n        weight_decay=0.00004)\n    if train_args is not None:\n        args.update(train_args)\n    slim_train_image_nn(**args)\n\n\ndef eval_mobile_net(data_dir):\n    slim_eval_image_nn(dataset_name=\'deepdrive\', dataset_split_name=\'eval\',\n                       dataset_dir=data_dir,\n                       model_name=MOBILENET_V2_SLIM_NAME,\n                       eval_image_size=IMG_SIZE)\n'"
vendor/openai/baselines/__init__.py,0,b''
vendor/openai/baselines/logger.py,3,"b'import os\nimport sys\nimport shutil\nimport os.path as osp\nimport json\nimport time\nimport datetime\nimport tempfile\nfrom collections import defaultdict\n\nimport config as c\n\nLOG_OUTPUT_FORMATS = [\'stdout\', \'log\', \'csv\']\n# Also valid: json, tensorboard\n\nDEBUG = 10\nINFO = 20\nWARN = 30\nERROR = 40\n\nDISABLED = 50\n\nclass KVWriter(object):\n    def writekvs(self, kvs):\n        raise NotImplementedError\n\nclass SeqWriter(object):\n    def writeseq(self, seq):\n        raise NotImplementedError\n\nclass HumanOutputFormat(KVWriter, SeqWriter):\n    def __init__(self, filename_or_file):\n        if isinstance(filename_or_file, str):\n            self.file = open(filename_or_file, \'wt\')\n            self.own_file = True\n        else:\n            assert hasattr(filename_or_file, \'read\'), \'expected file or str, got %s\'%filename_or_file\n            self.file = filename_or_file\n            self.own_file = False\n\n    def writekvs(self, kvs):\n        # Create strings for printing\n        key2str = {}\n        for (key, val) in sorted(kvs.items()):\n            if isinstance(val, float):\n                valstr = \'%-8.3g\' % (val,)\n            else:\n                valstr = str(val)\n            key2str[self._truncate(key)] = self._truncate(valstr)\n\n        # Find max widths\n        if len(key2str) == 0:\n            print(\'WARNING: tried to write empty key-value dict\')\n            return\n        else:\n            keywidth = max(map(len, key2str.keys()))\n            valwidth = max(map(len, key2str.values()))\n\n        # Write out the data\n        dashes = \'-\' * (keywidth + valwidth + 7)\n        lines = [dashes]\n        for (key, val) in sorted(key2str.items()):\n            lines.append(\'| %s%s | %s%s |\' % (\n                key,\n                \' \' * (keywidth - len(key)),\n                val,\n                \' \' * (valwidth - len(val)),\n            ))\n        lines.append(dashes)\n        self.file.write(\'\\n\'.join(lines) + \'\\n\')\n\n        # Flush the output to the file\n        self.file.flush()\n\n    def _truncate(self, s):\n        return s[:20] + \'...\' if len(s) > 23 else s\n\n    def writeseq(self, seq):\n        for arg in seq:\n            self.file.write(arg)\n        self.file.write(\'\\n\')\n        self.file.flush()\n\n    def close(self):\n        if self.own_file:\n            self.file.close()\n\nclass JSONOutputFormat(KVWriter):\n    def __init__(self, filename):\n        self.file = open(filename, \'wt\')\n\n    def writekvs(self, kvs):\n        for k, v in sorted(kvs.items()):\n            if hasattr(v, \'dtype\'):\n                v = v.tolist()\n                kvs[k] = float(v)\n        self.file.write(json.dumps(kvs) + \'\\n\')\n        self.file.flush()\n\n    def close(self):\n        self.file.close()\n\nclass CSVOutputFormat(KVWriter):\n    def __init__(self, filename):\n        self.file = open(filename, \'w+t\')\n        self.keys = []\n        self.sep = \',\'\n\n    def writekvs(self, kvs):\n        # Add our current row to the history\n        extra_keys = kvs.keys() - self.keys\n        if extra_keys:\n            self.keys.extend(extra_keys)\n            self.file.seek(0)\n            lines = self.file.readlines()\n            self.file.seek(0)\n            for (i, k) in enumerate(self.keys):\n                if i > 0:\n                    self.file.write(\',\')\n                self.file.write(k)\n            self.file.write(\'\\n\')\n            for line in lines[1:]:\n                self.file.write(line[:-1])\n                self.file.write(self.sep * len(extra_keys))\n                self.file.write(\'\\n\')\n        for (i, k) in enumerate(self.keys):\n            if i > 0:\n                self.file.write(\',\')\n            v = kvs.get(k)\n            if v is not None:\n                self.file.write(str(v))\n        self.file.write(\'\\n\')\n        self.file.flush()\n\n    def close(self):\n        self.file.close()\n\n\nclass TensorBoardOutputFormat(KVWriter):\n    """"""\n    Dumps key/value pairs into TensorBoard\'s numeric format.\n    """"""\n    def __init__(self, dir):\n        os.makedirs(dir, exist_ok=True)\n        self.dir = dir\n        self.step = 1\n        prefix = \'events\'\n        path = osp.join(osp.abspath(dir), prefix)\n        import tensorflow as tf\n        from tensorflow.python import pywrap_tensorflow\n        from tensorflow.core.util import event_pb2\n        from tensorflow.python.util import compat\n        self.tf = tf\n        self.event_pb2 = event_pb2\n        self.pywrap_tensorflow = pywrap_tensorflow\n        self.writer = pywrap_tensorflow.EventsWriter(compat.as_bytes(path))\n\n    def writekvs(self, kvs):\n        def summary_val(k, v):\n            kwargs = {\'tag\': k, \'simple_value\': float(v)}\n            return self.tf.Summary.Value(**kwargs)\n        summary = self.tf.Summary(value=[summary_val(k, v) for k, v in kvs.items()])\n        event = self.event_pb2.Event(wall_time=time.time(), summary=summary)\n        event.step = self.step # is there any reason why you\'d want to specify the step?\n        self.writer.WriteEvent(event)\n        self.writer.Flush()\n        self.step += 1\n\n    def close(self):\n        if self.writer:\n            self.writer.Close()\n            self.writer = None\n\ndef make_output_format(format, ev_dir, log_suffix=\'\'):\n    os.makedirs(ev_dir, exist_ok=True)\n    if format == \'stdout\':\n        return HumanOutputFormat(sys.stdout)\n    elif format == \'log\':\n        return HumanOutputFormat(osp.join(ev_dir, \'log%s.txt\' % log_suffix))\n    elif format == \'json\':\n        return JSONOutputFormat(osp.join(ev_dir, \'progress%s.json\' % log_suffix))\n    elif format == \'csv\':\n        return CSVOutputFormat(osp.join(ev_dir, \'progress%s.csv\' % log_suffix))\n    elif format == \'tensorboard\':\n        return TensorBoardOutputFormat(osp.join(ev_dir, \'tb%s\' % log_suffix))\n    else:\n        raise ValueError(\'Unknown format specified: %s\' % (format,))\n\n# ================================================================\n# API\n# ================================================================\n\ndef logkv(key, val):\n    """"""\n    Log a value of some diagnostic\n    Call this once for each diagnostic quantity, each iteration\n    If called many times, last value will be used.\n    """"""\n    Logger.CURRENT.logkv(key, val)\n\ndef logkv_mean(key, val):\n    """"""\n    The same as logkv(), but if called many times, values averaged.\n    """"""\n    Logger.CURRENT.logkv_mean(key, val)\n\ndef logkvs(d):\n    """"""\n    Log a dictionary of key-value pairs\n    """"""\n    for (k, v) in d.items():\n        logkv(k, v)\n\ndef dumpkvs():\n    """"""\n    Write all of the diagnostics from the current iteration\n\n    level: int. (see logger.py docs) If the global logger level is higher than\n                the level argument here, don\'t print to stdout.\n    """"""\n    Logger.CURRENT.dumpkvs()\n\ndef getkvs():\n    return Logger.CURRENT.name2val\n\n\ndef log(*args, level=INFO):\n    """"""\n    Write the sequence of args, with no separators, to the console and output files (if you\'ve configured an output file).\n    """"""\n    Logger.CURRENT.log(*args, level=level)\n\ndef debug(*args):\n    log(*args, level=DEBUG)\n\ndef info(*args):\n    log(*args, level=INFO)\n\ndef warn(*args):\n    log(*args, level=WARN)\n\ndef error(*args):\n    log(*args, level=ERROR)\n\n\ndef set_level(level):\n    """"""\n    Set logging threshold on current logger.\n    """"""\n    Logger.CURRENT.set_level(level)\n\ndef get_dir():\n    """"""\n    Get directory that log files are being written to.\n    will be None if there is no output directory (i.e., if you didn\'t call start)\n    """"""\n    return Logger.CURRENT.get_dir()\n\nrecord_tabular = logkv\ndump_tabular = dumpkvs\n\nclass ProfileKV:\n    """"""\n    Usage:\n    with logger.ProfileKV(""interesting_scope""):\n        code\n    """"""\n    def __init__(self, n):\n        self.n = ""wait_"" + n\n    def __enter__(self):\n        self.t1 = time.time()\n    def __exit__(self ,type, value, traceback):\n        Logger.CURRENT.name2val[self.n] += time.time() - self.t1\n\ndef profile(n):\n    """"""\n    Usage:\n    @profile(""my_func"")\n    def my_func(): code\n    """"""\n    def decorator_with_name(func):\n        def func_wrapper(*args, **kwargs):\n            with ProfileKV(n):\n                return func(*args, **kwargs)\n        return func_wrapper\n    return decorator_with_name\n\n\n# ================================================================\n# Backend\n# ================================================================\n\nclass Logger(object):\n    DEFAULT = None  # A logger with no output files. (See right below class definition)\n                    # So that you can still log to the terminal without setting up any output files\n    CURRENT = None  # Current logger being used by the free functions above\n\n    def __init__(self, dir, output_formats):\n        self.name2val = defaultdict(float)  # values this iteration\n        self.name2cnt = defaultdict(int)\n        self.level = INFO\n        self.dir = dir\n        self.output_formats = output_formats\n\n    # Logging API, forwarded\n    # ----------------------------------------\n    def logkv(self, key, val):\n        self.name2val[key] = val\n\n    def logkv_mean(self, key, val):\n        if val is None:\n            self.name2val[key] = None\n            return\n        oldval, cnt = self.name2val[key], self.name2cnt[key]\n        self.name2val[key] = oldval*cnt/(cnt+1) + val/(cnt+1)\n        self.name2cnt[key] = cnt + 1\n\n    def dumpkvs(self):\n        if self.level == DISABLED: return\n        for fmt in self.output_formats:\n            if isinstance(fmt, KVWriter):\n                fmt.writekvs(self.name2val)\n        self.name2val.clear()\n        self.name2cnt.clear()\n\n    def log(self, *args, level=INFO):\n        if self.level <= level:\n            self._do_log(args)\n\n    # Configuration\n    # ----------------------------------------\n    def set_level(self, level):\n        self.level = level\n\n    def get_dir(self):\n        return self.dir\n\n    def close(self):\n        for fmt in self.output_formats:\n            fmt.close()\n\n    # Misc\n    # ----------------------------------------\n    def _do_log(self, args):\n        for fmt in self.output_formats:\n            if isinstance(fmt, SeqWriter):\n                fmt.writeseq(map(str, args))\n\nLogger.DEFAULT = Logger.CURRENT = Logger(dir=None, output_formats=[HumanOutputFormat(sys.stdout)])\n\ndef configure(log_dir=None, format_strs=None):\n    if log_dir is None:\n        log_dir = os.getenv(\'OPENAI_LOGDIR\')\n    if log_dir is None:\n        log_dir = osp.join(c.BASELINES_DIR,\n                           datetime.datetime.now().strftime(""openai-%Y-%m-%d-%H-%M-%S-%f""))\n    assert isinstance(log_dir, str)\n    os.makedirs(log_dir, exist_ok=True)\n\n    if format_strs is None:\n        strs = os.getenv(\'OPENAI_LOG_FORMAT\')\n        format_strs = strs.split(\',\') if strs else LOG_OUTPUT_FORMATS\n    output_formats = [make_output_format(f, log_dir) for f in format_strs]\n\n    Logger.CURRENT = Logger(dir=log_dir, output_formats=output_formats)\n    log(\'Logging to %s\' % log_dir)\n\ndef reset():\n    if Logger.CURRENT is not Logger.DEFAULT:\n        Logger.CURRENT.close()\n        Logger.CURRENT = Logger.DEFAULT\n        log(\'Reset logger\')\n\nclass scoped_configure(object):\n    def __init__(self, dir=None, format_strs=None):\n        self.dir = dir\n        self.format_strs = format_strs\n        self.prevlogger = None\n    def __enter__(self):\n        self.prevlogger = Logger.CURRENT\n        configure(dir=self.dir, format_strs=self.format_strs)\n    def __exit__(self, *args):\n        Logger.CURRENT.close()\n        Logger.CURRENT = self.prevlogger\n\n# ================================================================\n\ndef _demo():\n    info(""hi"")\n    debug(""shouldn\'t appear"")\n    set_level(DEBUG)\n    debug(""should appear"")\n    dir = ""/tmp/testlogging""\n    if os.path.exists(dir):\n        shutil.rmtree(dir)\n    configure(dir=dir)\n    logkv(""a"", 3)\n    logkv(""b"", 2.5)\n    dumpkvs()\n    logkv(""b"", -2.5)\n    logkv(""a"", 5.5)\n    dumpkvs()\n    info(""^^^ should see a = 5.5"")\n    logkv_mean(""b"", -22.5)\n    logkv_mean(""b"", -44.4)\n    logkv(""a"", 5.5)\n    dumpkvs()\n    info(""^^^ should see b = 33.3"")\n\n    logkv(""b"", -2.5)\n    dumpkvs()\n\n    logkv(""a"", ""longasslongasslongasslongasslongasslongassvalue"")\n    dumpkvs()\n\n\n# ================================================================\n# Readers\n# ================================================================\n\ndef read_json(fname):\n    import pandas\n    ds = []\n    with open(fname, \'rt\') as fh:\n        for line in fh:\n            ds.append(json.loads(line))\n    return pandas.DataFrame(ds)\n\ndef read_csv(fname):\n    import pandas\n    return pandas.read_csv(fname, index_col=None, comment=\'#\')\n\ndef read_tb(path):\n    """"""\n    path : a tensorboard file OR a directory, where we will find all TB files\n           of the form events.*\n    """"""\n    import pandas\n    import numpy as np\n    from glob import glob\n    from collections import defaultdict\n    import tensorflow as tf\n    if osp.isdir(path):\n        fnames = glob(osp.join(path, ""events.*""))\n    elif osp.basename(path).startswith(""events.""):\n        fnames = [path]\n    else:\n        raise NotImplementedError(""Expected tensorboard file or directory containing them. Got %s""%path)\n    tag2pairs = defaultdict(list)\n    maxstep = 0\n    for fname in fnames:\n        for summary in tf.train.summary_iterator(fname):\n            if summary.step > 0:\n                for v in summary.summary.value:\n                    pair = (summary.step, v.simple_value)\n                    tag2pairs[v.tag].append(pair)\n                maxstep = max(summary.step, maxstep)\n    data = np.empty((maxstep, len(tag2pairs)))\n    data[:] = np.nan\n    tags = sorted(tag2pairs.keys())\n    for (colidx,tag) in enumerate(tags):\n        pairs = tag2pairs[tag]\n        for (step, value) in pairs:\n            data[step-1, colidx] = value\n    return pandas.DataFrame(data, columns=tags)\n\nif __name__ == ""__main__"":\n    _demo()\n'"
vendor/tensorflow/models/__init__.py,0,b''
vendor/openai/baselines/a2c/__init__.py,0,b''
vendor/openai/baselines/a2c/a2c.py,13,"b'import os.path as osp\nimport time\n\nimport joblib\nimport numpy as np\nimport tensorflow as tf\nfrom vendor.openai.baselines import logger\nfrom vendor.openai.baselines.a2c.utils import Scheduler, make_path, find_trainable_variables\nfrom vendor.openai.baselines.a2c.utils import cat_entropy, mse\nfrom vendor.openai.baselines.a2c.utils import discount_with_dones\nfrom vendor.openai.baselines.common import set_global_seeds, explained_variance\nfrom vendor.openai.baselines.common import tf_util\n\n\nclass Model(object):\n\n    def __init__(self, policy, ob_space, ac_space, nenvs, nsteps,\n            ent_coef=0.01, vf_coef=0.5, max_grad_norm=0.5, lr=7e-4,\n            alpha=0.99, epsilon=1e-5, total_timesteps=int(80e6), lrschedule=\'linear\'):\n\n        sess = tf_util.make_session()\n        nact = ac_space.n\n        nbatch = nenvs*nsteps\n\n        A = tf.placeholder(tf.int32, [nbatch])\n        ADV = tf.placeholder(tf.float32, [nbatch])\n        R = tf.placeholder(tf.float32, [nbatch])\n        LR = tf.placeholder(tf.float32, [])\n\n        step_model = policy(sess, ob_space, ac_space, nenvs, 1, reuse=False)\n        train_model = policy(sess, ob_space, ac_space, nenvs*nsteps, nsteps, reuse=True)\n\n        neglogpac = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=train_model.pi, labels=A)\n        pg_loss = tf.reduce_mean(ADV * neglogpac)\n        vf_loss = tf.reduce_mean(mse(tf.squeeze(train_model.vf), R))\n        entropy = tf.reduce_mean(cat_entropy(train_model.pi))\n        loss = pg_loss - entropy*ent_coef + vf_loss * vf_coef\n\n        params = find_trainable_variables(""model"")\n        grads = tf.gradients(loss, params)\n        if max_grad_norm is not None:\n            grads, grad_norm = tf.clip_by_global_norm(grads, max_grad_norm)\n        grads = list(zip(grads, params))\n        trainer = tf.train.RMSPropOptimizer(learning_rate=LR, decay=alpha, epsilon=epsilon)\n        _train = trainer.apply_gradients(grads)\n\n        lr = Scheduler(v=lr, nvalues=total_timesteps, schedule=lrschedule)\n\n        def train(obs, states, rewards, masks, actions, values):\n            advs = rewards - values\n            for step in range(len(obs)):\n                cur_lr = lr.value()\n            td_map = {train_model.X:obs, A:actions, ADV:advs, R:rewards, LR:cur_lr}\n            if states is not None:\n                td_map[train_model.S] = states\n                td_map[train_model.M] = masks\n            policy_loss, value_loss, policy_entropy, _ = sess.run(\n                [pg_loss, vf_loss, entropy, _train],\n                td_map\n            )\n            return policy_loss, value_loss, policy_entropy\n\n        def save(save_path):\n            ps = sess.run(params)\n            make_path(osp.dirname(save_path))\n            joblib.dump(ps, save_path)\n\n        def load(load_path):\n            loaded_params = joblib.load(load_path)\n            restores = []\n            for p, loaded_p in zip(params, loaded_params):\n                restores.append(p.assign(loaded_p))\n            ps = sess.run(restores)\n\n        self.train = train\n        self.train_model = train_model\n        self.step_model = step_model\n        self.step = step_model.step\n        self.value = step_model.value\n        self.initial_state = step_model.initial_state\n        self.save = save\n        self.load = load\n        tf.global_variables_initializer().run(session=sess)\n\nclass Runner(object):\n\n    def __init__(self, env, model, nsteps=5, gamma=0.99):\n        self.env = env\n        self.model = model\n        nh, nw, nc = env.observation_space.shape\n        nenv = env.num_envs\n        self.batch_ob_shape = (nenv*nsteps, nh, nw, nc)\n        self.obs = np.zeros((nenv, nh, nw, nc), dtype=np.uint8)\n        self.nc = nc\n        obs = env.reset()\n        self.gamma = gamma\n        self.nsteps = nsteps\n        self.states = model.initial_state\n        self.dones = [False for _ in range(nenv)]\n\n    def run(self):\n        mb_obs, mb_rewards, mb_actions, mb_values, mb_dones = [],[],[],[],[]\n        mb_states = self.states\n        for n in range(self.nsteps):\n            actions, values, states, _ = self.model.step(self.obs, self.states, self.dones)\n            mb_obs.append(np.copy(self.obs))\n            mb_actions.append(actions)\n            mb_values.append(values)\n            mb_dones.append(self.dones)\n            obs, rewards, dones, _ = self.env.step(actions)\n            self.states = states\n            self.dones = dones\n            for n, done in enumerate(dones):\n                if done:\n                    self.obs[n] = self.obs[n]*0\n            self.obs = obs\n            mb_rewards.append(rewards)\n        mb_dones.append(self.dones)\n        #batch of steps to batch of rollouts\n        mb_obs = np.asarray(mb_obs, dtype=np.uint8).swapaxes(1, 0).reshape(self.batch_ob_shape)\n        mb_rewards = np.asarray(mb_rewards, dtype=np.float32).swapaxes(1, 0)\n        mb_actions = np.asarray(mb_actions, dtype=np.int32).swapaxes(1, 0)\n        mb_values = np.asarray(mb_values, dtype=np.float32).swapaxes(1, 0)\n        mb_dones = np.asarray(mb_dones, dtype=np.bool).swapaxes(1, 0)\n        mb_masks = mb_dones[:, :-1]\n        mb_dones = mb_dones[:, 1:]\n        last_values = self.model.value(self.obs, self.states, self.dones).tolist()\n        #discount/bootstrap off value fn\n        for n, (rewards, dones, value) in enumerate(zip(mb_rewards, mb_dones, last_values)):\n            rewards = rewards.tolist()\n            dones = dones.tolist()\n            if dones[-1] == 0:\n                rewards = discount_with_dones(rewards+[value], dones+[0], self.gamma)[:-1]\n            else:\n                rewards = discount_with_dones(rewards, dones, self.gamma)\n            mb_rewards[n] = rewards\n        mb_rewards = mb_rewards.flatten()\n        mb_actions = mb_actions.flatten()\n        mb_values = mb_values.flatten()\n        mb_masks = mb_masks.flatten()\n        return mb_obs, mb_states, mb_rewards, mb_masks, mb_actions, mb_values\n\ndef learn(policy, env, seed, nsteps=5, total_timesteps=int(80e6), vf_coef=0.5, ent_coef=0.01, max_grad_norm=0.5, lr=7e-4, lrschedule=\'linear\', epsilon=1e-5, alpha=0.99, gamma=0.99, log_interval=100):\n    tf.reset_default_graph()\n    set_global_seeds(seed)\n\n    nenvs = env.num_envs\n    ob_space = env.observation_space\n    ac_space = env.action_space\n    model = Model(policy=policy, ob_space=ob_space, ac_space=ac_space, nenvs=nenvs, nsteps=nsteps, ent_coef=ent_coef, vf_coef=vf_coef,\n        max_grad_norm=max_grad_norm, lr=lr, alpha=alpha, epsilon=epsilon, total_timesteps=total_timesteps, lrschedule=lrschedule)\n    runner = Runner(env, model, nsteps=nsteps, gamma=gamma)\n\n    nbatch = nenvs*nsteps\n    tstart = time.time()\n    for update in range(1, total_timesteps//nbatch+1):\n        obs, states, rewards, masks, actions, values = runner.run()\n        policy_loss, value_loss, policy_entropy = model.train(obs, states, rewards, masks, actions, values)\n        nseconds = time.time()-tstart\n        fps = int((update*nbatch)/nseconds)\n        if update % log_interval == 0 or update == 1:\n            ev = explained_variance(values, rewards)\n            logger.record_tabular(""nupdates"", update)\n            logger.record_tabular(""total_timesteps"", update*nbatch)\n            logger.record_tabular(""fps"", fps)\n            logger.record_tabular(""policy_entropy"", float(policy_entropy))\n            logger.record_tabular(""value_loss"", float(value_loss))\n            logger.record_tabular(""explained_variance"", float(ev))\n            logger.dump_tabular()\n    env.close()\n'"
vendor/openai/baselines/a2c/policies.py,18,"b'import numpy as np\nimport tensorflow as tf\n\nfrom vendor.openai.baselines.common.distributions import make_pdtype\nfrom vendor.openai.baselines.a2c import conv, fc, conv_to_fc, batch_to_seq, seq_to_batch, lstm, lnlstm\n\n\ndef nature_cnn(unscaled_images):\n    """"""\n    CNN from Nature paper.\n    """"""\n    scaled_images = tf.cast(unscaled_images, tf.float32) / 255.\n    activ = tf.nn.relu\n    h = activ(conv(scaled_images, \'c1\', nf=32, rf=8, stride=4, init_scale=np.sqrt(2)))\n    h2 = activ(conv(h, \'c2\', nf=64, rf=4, stride=2, init_scale=np.sqrt(2)))\n    h3 = activ(conv(h2, \'c3\', nf=64, rf=3, stride=1, init_scale=np.sqrt(2)))\n    h3 = conv_to_fc(h3)\n    return activ(fc(h3, \'fc1\', nh=512, init_scale=np.sqrt(2)))\n\nclass LnLstmPolicy(object):\n    def __init__(self, sess, ob_space, ac_space, nbatch, nsteps, nlstm=256, reuse=False):\n        nenv = nbatch // nsteps\n        nh, nw, nc = ob_space.shape\n        ob_shape = (nbatch, nh, nw, nc)\n        nact = ac_space.n\n        X = tf.placeholder(tf.uint8, ob_shape) #obs\n        M = tf.placeholder(tf.float32, [nbatch]) #mask (done t-1)\n        S = tf.placeholder(tf.float32, [nenv, nlstm*2]) #states\n        with tf.variable_scope(""model"", reuse=reuse):\n            h = nature_cnn(X)\n            xs = batch_to_seq(h, nenv, nsteps)\n            ms = batch_to_seq(M, nenv, nsteps)\n            h5, snew = lnlstm(xs, ms, S, \'lstm1\', nh=nlstm)\n            h5 = seq_to_batch(h5)\n            pi = fc(h5, \'pi\', nact)\n            vf = fc(h5, \'v\', 1)\n\n        self.pdtype = make_pdtype(ac_space)\n        self.pd = self.pdtype.pdfromflat(pi)\n\n        v0 = vf[:, 0]\n        a0 = self.pd.sample()\n        neglogp0 = self.pd.neglogp(a0)\n        self.initial_state = np.zeros((nenv, nlstm*2), dtype=np.float32)\n\n        def step(ob, state, mask):\n            return sess.run([a0, v0, snew, neglogp0], {X:ob, S:state, M:mask})\n\n        def value(ob, state, mask):\n            return sess.run(v0, {X:ob, S:state, M:mask})\n\n        self.X = X\n        self.M = M\n        self.S = S\n        self.pi = pi\n        self.vf = vf\n        self.step = step\n        self.value = value\n\nclass LstmPolicy(object):\n\n    def __init__(self, sess, ob_space, ac_space, nbatch, nsteps, nlstm=256, reuse=False):\n        nenv = nbatch // nsteps\n\n        nh, nw, nc = ob_space.shape\n        ob_shape = (nbatch, nh, nw, nc)\n        nact = ac_space.n\n        X = tf.placeholder(tf.uint8, ob_shape) #obs\n        M = tf.placeholder(tf.float32, [nbatch]) #mask (done t-1)\n        S = tf.placeholder(tf.float32, [nenv, nlstm*2]) #states\n        with tf.variable_scope(""model"", reuse=reuse):\n            h = nature_cnn(X)\n            xs = batch_to_seq(h, nenv, nsteps)\n            ms = batch_to_seq(M, nenv, nsteps)\n            h5, snew = lstm(xs, ms, S, \'lstm1\', nh=nlstm)\n            h5 = seq_to_batch(h5)\n            pi = fc(h5, \'pi\', nact)\n            vf = fc(h5, \'v\', 1)\n\n        self.pdtype = make_pdtype(ac_space)\n        self.pd = self.pdtype.pdfromflat(pi)\n\n        v0 = vf[:, 0]\n        a0 = self.pd.sample()\n        neglogp0 = self.pd.neglogp(a0)\n        self.initial_state = np.zeros((nenv, nlstm*2), dtype=np.float32)\n\n        def step(ob, state, mask):\n            return sess.run([a0, v0, snew, neglogp0], {X:ob, S:state, M:mask})\n\n        def value(ob, state, mask):\n            return sess.run(v0, {X:ob, S:state, M:mask})\n\n        self.X = X\n        self.M = M\n        self.S = S\n        self.pi = pi\n        self.vf = vf\n        self.step = step\n        self.value = value\n\nclass CnnPolicy(object):\n\n    def __init__(self, sess, ob_space, ac_space, nbatch, nsteps, reuse=False): #pylint: disable=W0613\n        nh, nw, nc = ob_space.shape\n        ob_shape = (nbatch, nh, nw, nc)\n        nact = ac_space.n\n        X = tf.placeholder(tf.uint8, ob_shape) #obs\n        with tf.variable_scope(""model"", reuse=reuse):\n            h = nature_cnn(X)\n            pi = fc(h, \'pi\', nact, init_scale=0.01)\n            vf = fc(h, \'v\', 1)[:,0]\n\n        self.pdtype = make_pdtype(ac_space)\n        self.pd = self.pdtype.pdfromflat(pi)\n\n        a0 = self.pd.sample()\n        neglogp0 = self.pd.neglogp(a0)\n        self.initial_state = None\n\n        def step(ob, *_args, **_kwargs):\n            a, v, neglogp = sess.run([a0, vf, neglogp0], {X:ob})\n            return a, v, self.initial_state, neglogp\n\n        def value(ob, *_args, **_kwargs):\n            return sess.run(vf, {X:ob})\n\n        self.X = X\n        self.pi = pi\n        self.vf = vf\n        self.step = step\n        self.value = value\n\nclass MlpPolicy(object):\n    def __init__(self, sess, ob_space, ac_space, nbatch, nsteps, reuse=False): #pylint: disable=W0613\n        ob_shape = (nbatch,) + ob_space.shape\n        actdim = ac_space.shape[0]\n        X = tf.placeholder(tf.float32, ob_shape, name=\'Ob\') #obs\n        with tf.variable_scope(""model"", reuse=reuse):\n            activ = tf.tanh\n            h1 = activ(fc(X, \'pi_fc1\', nh=64, init_scale=np.sqrt(2)))\n            h2 = activ(fc(h1, \'pi_fc2\', nh=64, init_scale=np.sqrt(2)))\n            pi = fc(h2, \'pi\', actdim, init_scale=0.01)\n            h1 = activ(fc(X, \'vf_fc1\', nh=64, init_scale=np.sqrt(2)))\n            h2 = activ(fc(h1, \'vf_fc2\', nh=64, init_scale=np.sqrt(2)))\n            vf = fc(h2, \'vf\', 1)[:,0]\n            logstd = tf.get_variable(name=""logstd"", shape=[1, actdim],\n                initializer=tf.zeros_initializer())\n\n        pdparam = tf.concat([pi, pi * 0.0 + logstd], axis=1)\n\n        self.pdtype = make_pdtype(ac_space)\n        self.pd = self.pdtype.pdfromflat(pdparam)\n\n        a0 = self.pd.sample()\n        neglogp0 = self.pd.neglogp(a0)\n        self.initial_state = None\n\n        def step(ob, *_args, **_kwargs):\n            a, v, neglogp = sess.run([a0, vf, neglogp0], {X:ob})\n            return a, v, self.initial_state, neglogp\n\n        def value(ob, *_args, **_kwargs):\n            return sess.run(vf, {X:ob})\n\n        self.X = X\n        self.pi = pi\n        self.vf = vf\n        self.step = step\n        self.value = value\n'"
vendor/openai/baselines/a2c/run_atari.py,0,"b""#!/usr/bin/env python3\n\nfrom vendor.openai.baselines import logger\nfrom vendor.openai.baselines.a2c.a2c import learn\nfrom vendor.openai.baselines.common.vec_env.vec_frame_stack import VecFrameStack\nfrom vendor.openai.baselines.ppo2.policies import CnnPolicy, LstmPolicy, LnLstmPolicy\n\nfrom vendor.openai.baselines.common.cmd_util import make_atari_env, atari_arg_parser\n\n\ndef train(env_id, num_timesteps, seed, policy, lrschedule, num_env):\n    if policy == 'cnn':\n        policy_fn = CnnPolicy\n    elif policy == 'lstm':\n        policy_fn = LstmPolicy\n    elif policy == 'lnlstm':\n        policy_fn = LnLstmPolicy\n    env = VecFrameStack(make_atari_env(env_id, num_env, seed), 4)\n    learn(policy_fn, env, seed, total_timesteps=int(num_timesteps * 1.1), lrschedule=lrschedule)\n    env.close()\n\ndef main():\n    parser = atari_arg_parser()\n    parser.add_argument('--policy', help='Policy architecture', choices=['cnn', 'lstm', 'lnlstm'], default='cnn')\n    parser.add_argument('--lrschedule', help='Learning rate schedule', choices=['constant', 'linear'], default='constant')\n    args = parser.parse_args()\n    logger.configure()\n    train(args.env, num_timesteps=args.num_timesteps, seed=args.seed,\n        policy=args.policy, lrschedule=args.lrschedule, num_env=16)\n\nif __name__ == '__main__':\n    main()\n"""
vendor/openai/baselines/a2c/utils.py,66,"b'import os\nimport gym\nimport numpy as np\nimport tensorflow as tf\nfrom gym import spaces\nfrom collections import deque\n\ndef sample(logits):\n    noise = tf.random_uniform(tf.shape(logits))\n    return tf.argmax(logits - tf.log(-tf.log(noise)), 1)\n\ndef cat_entropy(logits):\n    a0 = logits - tf.reduce_max(logits, 1, keep_dims=True)\n    ea0 = tf.exp(a0)\n    z0 = tf.reduce_sum(ea0, 1, keep_dims=True)\n    p0 = ea0 / z0\n    return tf.reduce_sum(p0 * (tf.log(z0) - a0), 1)\n\ndef cat_entropy_softmax(p0):\n    return - tf.reduce_sum(p0 * tf.log(p0 + 1e-6), axis = 1)\n\ndef mse(pred, target):\n    return tf.square(pred-target)/2.\n\ndef ortho_init(scale=1.0):\n    def _ortho_init(shape, dtype, partition_info=None):\n        #lasagne ortho init for tf\n        shape = tuple(shape)\n        if len(shape) == 2:\n            flat_shape = shape\n        elif len(shape) == 4: # assumes NHWC\n            flat_shape = (np.prod(shape[:-1]), shape[-1])\n        else:\n            raise NotImplementedError\n        a = np.random.normal(0.0, 1.0, flat_shape)\n        u, _, v = np.linalg.svd(a, full_matrices=False)\n        q = u if u.shape == flat_shape else v # pick the one with the correct shape\n        q = q.reshape(shape)\n        return (scale * q[:shape[0], :shape[1]]).astype(np.float32)\n    return _ortho_init\n\ndef conv(x, scope, *, nf, rf, stride, pad=\'VALID\', init_scale=1.0, data_format=\'NHWC\'):\n    if data_format == \'NHWC\':\n        channel_ax = 3\n        strides = [1, stride, stride, 1]\n        bshape = [1, 1, 1, nf]\n    elif data_format == \'NCHW\':\n        channel_ax = 1\n        strides = [1, 1, stride, stride]\n        bshape = [1, nf, 1, 1]\n    else:\n        raise NotImplementedError\n    nin = x.get_shape()[channel_ax].value\n    wshape = [rf, rf, nin, nf]\n    with tf.variable_scope(scope):\n        w = tf.get_variable(""w"", wshape, initializer=ortho_init(init_scale))\n        b = tf.get_variable(""b"", [1, nf, 1, 1], initializer=tf.constant_initializer(0.0))\n        if data_format == \'NHWC\': b = tf.reshape(b, bshape)\n        return b + tf.nn.conv2d(x, w, strides=strides, padding=pad, data_format=data_format)\n\ndef fc(x, scope, nh, *, init_scale=1.0, init_bias=0.0):\n    with tf.variable_scope(scope):\n        nin = x.get_shape()[1].value\n        w = tf.get_variable(""w"", [nin, nh], initializer=ortho_init(init_scale))\n        # w = tf.Print(w, [\'w \', scope, w], summarize=100)\n        b = tf.get_variable(""b"", [nh], initializer=tf.constant_initializer(init_bias))\n        # b = tf.Print(b, [\'b \', scope, w], summarize=100)\n\n        return tf.matmul(x, w)+b\n\ndef batch_to_seq(h, nbatch, nsteps, flat=False):\n    if flat:\n        h = tf.reshape(h, [nbatch, nsteps])\n    else:\n        h = tf.reshape(h, [nbatch, nsteps, -1])\n    return [tf.squeeze(v, [1]) for v in tf.split(axis=1, num_or_size_splits=nsteps, value=h)]\n\ndef seq_to_batch(h, flat = False):\n    shape = h[0].get_shape().as_list()\n    if not flat:\n        assert(len(shape) > 1)\n        nh = h[0].get_shape()[-1].value\n        return tf.reshape(tf.concat(axis=1, values=h), [-1, nh])\n    else:\n        return tf.reshape(tf.stack(values=h, axis=1), [-1])\n\ndef lstm(xs, ms, s, scope, nh, init_scale=1.0):\n    nbatch, nin = [v.value for v in xs[0].get_shape()]\n    nsteps = len(xs)\n    with tf.variable_scope(scope):\n        wx = tf.get_variable(""wx"", [nin, nh*4], initializer=ortho_init(init_scale))\n        wh = tf.get_variable(""wh"", [nh, nh*4], initializer=ortho_init(init_scale))\n        b = tf.get_variable(""b"", [nh*4], initializer=tf.constant_initializer(0.0))\n\n    c, h = tf.split(axis=1, num_or_size_splits=2, value=s)\n    for idx, (x, m) in enumerate(zip(xs, ms)):\n        c = c*(1-m)\n        h = h*(1-m)\n        z = tf.matmul(x, wx) + tf.matmul(h, wh) + b\n        i, f, o, u = tf.split(axis=1, num_or_size_splits=4, value=z)\n        i = tf.nn.sigmoid(i)\n        f = tf.nn.sigmoid(f)\n        o = tf.nn.sigmoid(o)\n        u = tf.tanh(u)\n        c = f*c + i*u\n        h = o*tf.tanh(c)\n        xs[idx] = h\n    s = tf.concat(axis=1, values=[c, h])\n    return xs, s\n\ndef _ln(x, g, b, e=1e-5, axes=[1]):\n    u, s = tf.nn.moments(x, axes=axes, keep_dims=True)\n    x = (x-u)/tf.sqrt(s+e)\n    x = x*g+b\n    return x\n\ndef lnlstm(xs, ms, s, scope, nh, init_scale=1.0):\n    nbatch, nin = [v.value for v in xs[0].get_shape()]\n    nsteps = len(xs)\n    with tf.variable_scope(scope):\n        wx = tf.get_variable(""wx"", [nin, nh*4], initializer=ortho_init(init_scale))\n        gx = tf.get_variable(""gx"", [nh*4], initializer=tf.constant_initializer(1.0))\n        bx = tf.get_variable(""bx"", [nh*4], initializer=tf.constant_initializer(0.0))\n\n        wh = tf.get_variable(""wh"", [nh, nh*4], initializer=ortho_init(init_scale))\n        gh = tf.get_variable(""gh"", [nh*4], initializer=tf.constant_initializer(1.0))\n        bh = tf.get_variable(""bh"", [nh*4], initializer=tf.constant_initializer(0.0))\n\n        b = tf.get_variable(""b"", [nh*4], initializer=tf.constant_initializer(0.0))\n\n        gc = tf.get_variable(""gc"", [nh], initializer=tf.constant_initializer(1.0))\n        bc = tf.get_variable(""bc"", [nh], initializer=tf.constant_initializer(0.0))\n\n    c, h = tf.split(axis=1, num_or_size_splits=2, value=s)\n    for idx, (x, m) in enumerate(zip(xs, ms)):\n        c = c*(1-m)\n        h = h*(1-m)\n        z = _ln(tf.matmul(x, wx), gx, bx) + _ln(tf.matmul(h, wh), gh, bh) + b\n        i, f, o, u = tf.split(axis=1, num_or_size_splits=4, value=z)\n        i = tf.nn.sigmoid(i)\n        f = tf.nn.sigmoid(f)\n        o = tf.nn.sigmoid(o)\n        u = tf.tanh(u)\n        c = f*c + i*u\n        h = o*tf.tanh(_ln(c, gc, bc))\n        xs[idx] = h\n    s = tf.concat(axis=1, values=[c, h])\n    return xs, s\n\ndef conv_to_fc(x):\n    nh = np.prod([v.value for v in x.get_shape()[1:]])\n    x = tf.reshape(x, [-1, nh])\n    return x\n\ndef discount_with_dones(rewards, dones, gamma):\n    discounted = []\n    r = 0\n    for reward, done in zip(rewards[::-1], dones[::-1]):\n        r = reward + gamma*r*(1.-done) # fixed off by one bug\n        discounted.append(r)\n    return discounted[::-1]\n\ndef find_trainable_variables(key):\n    with tf.variable_scope(key):\n        return tf.trainable_variables()\n\ndef make_path(f):\n    return os.makedirs(f, exist_ok=True)\n\ndef constant(p):\n    return 1\n\ndef linear(p):\n    return 1-p\n\ndef middle_drop(p):\n    eps = 0.75\n    if 1-p<eps:\n        return eps*0.1\n    return 1-p\n\ndef double_linear_con(p):\n    p *= 2\n    eps = 0.125\n    if 1-p<eps:\n        return eps\n    return 1-p\n\ndef double_middle_drop(p):\n    eps1 = 0.75\n    eps2 = 0.25\n    if 1-p<eps1:\n        if 1-p<eps2:\n            return eps2*0.5\n        return eps1*0.1\n    return 1-p\n\nschedules = {\n    \'linear\':linear,\n    \'constant\':constant,\n    \'double_linear_con\': double_linear_con,\n    \'middle_drop\': middle_drop,\n    \'double_middle_drop\': double_middle_drop\n}\n\nclass Scheduler(object):\n\n    def __init__(self, v, nvalues, schedule):\n        self.n = 0.\n        self.v = v\n        self.nvalues = nvalues\n        self.schedule = schedules[schedule]\n\n    def value(self):\n        current_value = self.v*self.schedule(self.n/self.nvalues)\n        self.n += 1.\n        return current_value\n\n    def value_steps(self, steps):\n        return self.v*self.schedule(steps/self.nvalues)\n\n\nclass EpisodeStats:\n    def __init__(self, nsteps, nenvs):\n        self.episode_rewards = []\n        for i in range(nenvs):\n            self.episode_rewards.append([])\n        self.lenbuffer = deque(maxlen=40)  # rolling buffer for episode lengths\n        self.rewbuffer = deque(maxlen=40)  # rolling buffer for episode rewards\n        self.nsteps = nsteps\n        self.nenvs = nenvs\n\n    def feed(self, rewards, masks):\n        rewards = np.reshape(rewards, [self.nenvs, self.nsteps])\n        masks = np.reshape(masks, [self.nenvs, self.nsteps])\n        for i in range(0, self.nenvs):\n            for j in range(0, self.nsteps):\n                self.episode_rewards[i].append(rewards[i][j])\n                if masks[i][j]:\n                    l = len(self.episode_rewards[i])\n                    s = sum(self.episode_rewards[i])\n                    self.lenbuffer.append(l)\n                    self.rewbuffer.append(s)\n                    self.episode_rewards[i] = []\n\n    def mean_length(self):\n        if self.lenbuffer:\n            return np.mean(self.lenbuffer)\n        else:\n            return 0  # on the first params dump, no episodes are finished\n\n    def mean_reward(self):\n        if self.rewbuffer:\n            return np.mean(self.rewbuffer)\n        else:\n            return 0\n\n\n# For ACER\ndef get_by_index(x, idx):\n    assert(len(x.get_shape()) == 2)\n    assert(len(idx.get_shape()) == 1)\n    idx_flattened = tf.range(0, x.shape[0]) * x.shape[1] + idx\n    y = tf.gather(tf.reshape(x, [-1]),  # flatten input\n                  idx_flattened)  # use flattened indices\n    return y\n\ndef check_shape(ts,shapes):\n    i = 0\n    for (t,shape) in zip(ts,shapes):\n        assert t.get_shape().as_list()==shape, ""id "" + str(i) + "" shape "" + str(t.get_shape()) + str(shape)\n        i += 1\n\ndef avg_norm(t):\n    return tf.reduce_mean(tf.sqrt(tf.reduce_sum(tf.square(t), axis=-1)))\n\ndef gradient_add(g1, g2, param):\n    print([g1, g2, param.name])\n    assert (not (g1 is None and g2 is None)), param.name\n    if g1 is None:\n        return g2\n    elif g2 is None:\n        return g1\n    else:\n        return g1 + g2\n\ndef q_explained_variance(qpred, q):\n    _, vary = tf.nn.moments(q, axes=[0, 1])\n    _, varpred = tf.nn.moments(q - qpred, axes=[0, 1])\n    check_shape([vary, varpred], [[]] * 2)\n    return 1.0 - (varpred / vary)\n'"
vendor/openai/baselines/bench/__init__.py,0,b'from vendor.openai.baselines.bench.benchmarks import *\nfrom vendor.openai.baselines.bench.monitor import *\n'
vendor/openai/baselines/bench/benchmarks.py,0,"b'import re\nimport os.path as osp\nimport os\nSCRIPT_DIR = os.path.dirname(os.path.abspath(__file__))\n\n_atari7 = [\'BeamRider\', \'Breakout\', \'Enduro\', \'Pong\', \'Qbert\', \'Seaquest\', \'SpaceInvaders\']\n_atariexpl7 = [\'Freeway\', \'Gravitar\', \'MontezumaRevenge\', \'Pitfall\', \'PrivateEye\', \'Solaris\', \'Venture\']\n\n_BENCHMARKS = []\n\nremove_version_re = re.compile(r\'-v\\d+$\')\ndef register_benchmark(benchmark):\n    for b in _BENCHMARKS:\n        if b[\'name\'] == benchmark[\'name\']:\n            raise ValueError(\'Benchmark with name %s already registered!\' % b[\'name\'])\n\n    # automatically add a description if it is not present\n    if \'tasks\' in benchmark:\n        for t in benchmark[\'tasks\']:\n            if \'desc\' not in t:\n                t[\'desc\'] = remove_version_re.sub(\'\', t[\'env_id\'])\n    _BENCHMARKS.append(benchmark)\n\n\ndef list_benchmarks():\n    return [b[\'name\'] for b in _BENCHMARKS]\n\n\ndef get_benchmark(benchmark_name):\n    for b in _BENCHMARKS:\n        if b[\'name\'] == benchmark_name:\n            return b\n    raise ValueError(\'%s not found! Known benchmarks: %s\' % (benchmark_name, list_benchmarks()))\n\n\ndef get_task(benchmark, env_id):\n    """"""Get a task by env_id. Return None if the benchmark doesn\'t have the env""""""\n    return next(filter(lambda task: task[\'env_id\'] == env_id, benchmark[\'tasks\']), None)\n\n\ndef find_task_for_env_id_in_any_benchmark(env_id):\n    for bm in _BENCHMARKS:\n        for task in bm[""tasks""]:\n            if task[""env_id""] == env_id:\n                return bm, task\n    return None, None\n\n\n_ATARI_SUFFIX = \'NoFrameskip-v4\'\n\nregister_benchmark({\n    \'name\': \'Atari50M\',\n    \'description\': \'7 Atari games from Mnih et al. (2013), with pixel observations, 50M timesteps\',\n    \'tasks\': [{\'desc\': _game, \'env_id\': _game + _ATARI_SUFFIX, \'trials\': 2, \'num_timesteps\': int(50e6)} for _game in _atari7]\n})\n\nregister_benchmark({\n    \'name\': \'Atari10M\',\n    \'description\': \'7 Atari games from Mnih et al. (2013), with pixel observations, 10M timesteps\',\n    \'tasks\': [{\'desc\': _game, \'env_id\': _game + _ATARI_SUFFIX, \'trials\': 2, \'num_timesteps\': int(10e6)} for _game in _atari7]\n})\n\nregister_benchmark({\n    \'name\': \'Atari1Hr\',\n    \'description\': \'7 Atari games from Mnih et al. (2013), with pixel observations, 1 hour of walltime\',\n    \'tasks\': [{\'desc\': _game, \'env_id\': _game + _ATARI_SUFFIX, \'trials\': 2, \'num_seconds\': 60 * 60} for _game in _atari7]\n})\n\nregister_benchmark({\n    \'name\': \'AtariExploration10M\',\n    \'description\': \'7 Atari games emphasizing exploration, with pixel observations, 10M timesteps\',\n    \'tasks\': [{\'desc\': _game, \'env_id\': _game + _ATARI_SUFFIX, \'trials\': 2, \'num_timesteps\': int(10e6)} for _game in _atariexpl7]\n})\n\n\n# MuJoCo\n\n_mujocosmall = [\n    \'InvertedDoublePendulum-v2\', \'InvertedPendulum-v2\',\n    \'HalfCheetah-v2\', \'Hopper-v2\', \'Walker2d-v2\',\n    \'Reacher-v2\', \'Swimmer-v2\']\nregister_benchmark({\n    \'name\': \'Mujoco1M\',\n    \'description\': \'Some small 2D MuJoCo tasks, run for 1M timesteps\',\n    \'tasks\': [{\'env_id\': _envid, \'trials\': 3, \'num_timesteps\': int(1e6)} for _envid in _mujocosmall]\n})\nregister_benchmark({\n    \'name\': \'MujocoWalkers\',\n    \'description\': \'MuJoCo forward walkers, run for 8M, humanoid 100M\',\n    \'tasks\': [\n        {\'env_id\': ""Hopper-v1"", \'trials\': 4, \'num_timesteps\': 8 * 1000000},\n        {\'env_id\': ""Walker2d-v1"", \'trials\': 4, \'num_timesteps\': 8 * 1000000},\n        {\'env_id\': ""Humanoid-v1"", \'trials\': 4, \'num_timesteps\': 100 * 1000000},\n    ]\n})\n\n# Roboschool\n\nregister_benchmark({\n    \'name\': \'Roboschool8M\',\n    \'description\': \'Small 2D tasks, up to 30 minutes to complete on 8 cores\',\n    \'tasks\': [\n        {\'env_id\': ""RoboschoolReacher-v1"", \'trials\': 4, \'num_timesteps\': 2 * 1000000},\n        {\'env_id\': ""RoboschoolAnt-v1"", \'trials\': 4, \'num_timesteps\': 8 * 1000000},\n        {\'env_id\': ""RoboschoolHalfCheetah-v1"", \'trials\': 4, \'num_timesteps\': 8 * 1000000},\n        {\'env_id\': ""RoboschoolHopper-v1"", \'trials\': 4, \'num_timesteps\': 8 * 1000000},\n        {\'env_id\': ""RoboschoolWalker2d-v1"", \'trials\': 4, \'num_timesteps\': 8 * 1000000},\n    ]\n})\nregister_benchmark({\n    \'name\': \'RoboschoolHarder\',\n    \'description\': \'Test your might!!! Up to 12 hours on 32 cores\',\n    \'tasks\': [\n        {\'env_id\': ""RoboschoolHumanoid-v1"", \'trials\': 4, \'num_timesteps\': 100 * 1000000},\n        {\'env_id\': ""RoboschoolHumanoidFlagrun-v1"", \'trials\': 4, \'num_timesteps\': 200 * 1000000},\n        {\'env_id\': ""RoboschoolHumanoidFlagrunHarder-v1"", \'trials\': 4, \'num_timesteps\': 400 * 1000000},\n    ]\n})\n\n# Other\n\n_atari50 = [  # actually 47\n    \'Alien\', \'Amidar\', \'Assault\', \'Asterix\', \'Asteroids\',\n    \'Atlantis\', \'BankHeist\', \'BattleZone\', \'BeamRider\', \'Bowling\',\n    \'Breakout\', \'Centipede\', \'ChopperCommand\', \'CrazyClimber\',\n    \'DemonAttack\', \'DoubleDunk\', \'Enduro\', \'FishingDerby\', \'Freeway\',\n    \'Frostbite\', \'Gopher\', \'Gravitar\', \'IceHockey\', \'Jamesbond\',\n    \'Kangaroo\', \'Krull\', \'KungFuMaster\', \'MontezumaRevenge\', \'MsPacman\',\n    \'NameThisGame\', \'Pitfall\', \'Pong\', \'PrivateEye\', \'Qbert\',\n    \'RoadRunner\', \'Robotank\', \'Seaquest\', \'SpaceInvaders\', \'StarGunner\',\n    \'Tennis\', \'TimePilot\', \'Tutankham\', \'UpNDown\', \'Venture\',\n    \'VideoPinball\', \'WizardOfWor\', \'Zaxxon\',\n]\n\nregister_benchmark({\n    \'name\': \'Atari50_10M\',\n    \'description\': \'47 Atari games from Mnih et al. (2013), with pixel observations, 10M timesteps\',\n    \'tasks\': [{\'desc\': _game, \'env_id\': _game + _ATARI_SUFFIX, \'trials\': 2, \'num_timesteps\': int(10e6)} for _game in _atari50]\n})\n\n'"
vendor/openai/baselines/bench/monitor.py,0,"b'__all__ = [\'Monitor\', \'get_monitor_files\', \'load_results\']\n\nimport gym\nfrom gym.core import Wrapper\nimport time\nfrom glob import glob\nimport csv\nimport os.path as osp\nimport json\nimport numpy as np\n\nclass Monitor(Wrapper):\n    EXT = ""monitor.csv""\n    f = None\n\n    def __init__(self, env, filename, allow_early_resets=False, reset_keywords=(), info_keywords=()):\n        Wrapper.__init__(self, env=env)\n        self.tstart = time.time()\n        if filename is None:\n            self.f = None\n            self.logger = None\n        else:\n            if not filename.endswith(Monitor.EXT):\n                if osp.isdir(filename):\n                    filename = osp.join(filename, Monitor.EXT)\n                else:\n                    filename = filename + ""."" + Monitor.EXT\n            self.f = open(filename, ""wt"")\n            self.f.write(\'#%s\\n\'%json.dumps({""t_start"": self.tstart, \'env_id\' : env.spec and env.spec.id}))\n            self.logger = csv.DictWriter(self.f, fieldnames=(\'r\', \'l\', \'t\')+reset_keywords+info_keywords)\n            self.logger.writeheader()\n            self.f.flush()\n\n        self.reset_keywords = reset_keywords\n        self.info_keywords = info_keywords\n        self.allow_early_resets = allow_early_resets\n        self.rewards = None\n        self.needs_reset = True\n        self.episode_rewards = []\n        self.episode_lengths = []\n        self.episode_times = []\n        self.total_steps = 0\n        self.current_reset_info = {} # extra info about the current episode, that was passed in during reset()\n\n    def reset(self, **kwargs):\n        if not self.allow_early_resets and not self.needs_reset:\n            raise RuntimeError(""Tried to reset an environment before done. If you want to allow early resets, wrap your env with Monitor(env, path, allow_early_resets=True)"")\n        self.rewards = []\n        self.needs_reset = False\n        for k in self.reset_keywords:\n            v = kwargs.get(k)\n            if v is None:\n                raise ValueError(\'Expected you to pass kwarg %s into reset\'%k)\n            self.current_reset_info[k] = v\n        return self.env.reset(**kwargs)\n\n    def step(self, action):\n        if self.needs_reset:\n            raise RuntimeError(""Tried to step environment that needs reset"")\n        ob, rew, done, info = self.env.step(action)\n        self.rewards.append(rew)\n        if done:\n            self.needs_reset = True\n            eprew = sum(self.rewards)\n            eplen = len(self.rewards)\n            epinfo = {""r"": round(eprew, 6), ""l"": eplen, ""t"": round(time.time() - self.tstart, 6)}\n            for k in self.info_keywords:\n                epinfo[k] = info[k]\n            self.episode_rewards.append(eprew)\n            self.episode_lengths.append(eplen)\n            self.episode_times.append(time.time() - self.tstart)\n            epinfo.update(self.current_reset_info)\n            if self.logger:\n                self.logger.writerow(epinfo)\n                self.f.flush()\n            info[\'episode\'] = epinfo\n        self.total_steps += 1\n        return (ob, rew, done, info)\n\n    def close(self):\n        if self.f is not None:\n            self.f.close()\n\n    def get_total_steps(self):\n        return self.total_steps\n\n    def get_episode_rewards(self):\n        return self.episode_rewards\n\n    def get_episode_lengths(self):\n        return self.episode_lengths\n\n    def get_episode_times(self):\n        return self.episode_times\n\nclass LoadMonitorResultsError(Exception):\n    pass\n\ndef get_monitor_files(dir):\n    return glob(osp.join(dir, ""*"" + Monitor.EXT))\n\ndef load_results(dir):\n    import pandas\n    monitor_files = (\n        glob(osp.join(dir, ""*monitor.json"")) + \n        glob(osp.join(dir, ""*monitor.csv""))) # get both csv and (old) json files\n    if not monitor_files:\n        raise LoadMonitorResultsError(""no monitor files of the form *%s found in %s"" % (Monitor.EXT, dir))\n    dfs = []\n    headers = []\n    for fname in monitor_files:\n        with open(fname, \'rt\') as fh:\n            if fname.endswith(\'csv\'):\n                firstline = fh.readline()\n                assert firstline[0] == \'#\'\n                header = json.loads(firstline[1:])\n                df = pandas.read_csv(fh, index_col=None)\n                headers.append(header)\n            elif fname.endswith(\'json\'): # Deprecated json format\n                episodes = []\n                lines = fh.readlines()\n                header = json.loads(lines[0])\n                headers.append(header)\n                for line in lines[1:]:\n                    episode = json.loads(line)\n                    episodes.append(episode)\n                df = pandas.DataFrame(episodes)\n            else:\n                assert 0, \'unreachable\'\n            df[\'t\'] += header[\'t_start\']\n        dfs.append(df)\n    df = pandas.concat(dfs)\n    df.sort_values(\'t\', inplace=True)\n    df.reset_index(inplace=True)\n    df[\'t\'] -= min(header[\'t_start\'] for header in headers)\n    df.headers = headers # HACK to preserve backwards compatibility\n    return df\n\ndef test_monitor():\n    env = gym.make(""CartPole-v1"")\n    env.seed(0)\n    mon_file = ""/tmp/baselines-test-%s.monitor.csv"" % uuid.uuid4()\n    menv = Monitor(env, mon_file)\n    menv.reset()\n    for _ in range(1000):\n        _, _, done, _ = menv.step(0)\n        if done:\n            menv.reset()\n\n    f = open(mon_file, \'rt\')\n\n    firstline = f.readline()\n    assert firstline.startswith(\'#\')\n    metadata = json.loads(firstline[1:])\n    assert metadata[\'env_id\'] == ""CartPole-v1""\n    assert set(metadata.keys()) == {\'env_id\', \'gym_version\', \'t_start\'},  ""Incorrect keys in monitor metadata""\n\n    last_logline = pandas.read_csv(f, index_col=None)\n    assert set(last_logline.keys()) == {\'l\', \'t\', \'r\'}, ""Incorrect keys in monitor logline""\n    f.close()\n    os.remove(mon_file)'"
vendor/openai/baselines/common/__init__.py,0,b'from vendor.openai.baselines.common.console_util import *\nfrom vendor.openai.baselines.common.dataset import Dataset\nfrom vendor.openai.baselines.common.math_util import *\nfrom vendor.openai.baselines.common.misc_util import *\n'
vendor/openai/baselines/common/atari_wrappers.py,0,"b'import numpy as np\nfrom collections import deque\nimport gym\nfrom gym import spaces\ntry:\n    import cv2\n    cv2.ocl.setUseOpenCL(False)\nexcept ImportError:\n    print(\'Error importing opencv, atari wrappers will not work\')\n\nclass NoopResetEnv(gym.Wrapper):\n    def __init__(self, env, noop_max=30):\n        """"""Sample initial states by taking random number of no-ops on reset.\n        No-op is assumed to be action 0.\n        """"""\n        gym.Wrapper.__init__(self, env)\n        self.noop_max = noop_max\n        self.override_num_noops = None\n        self.noop_action = 0\n        assert env.unwrapped.get_action_meanings()[0] == \'NOOP\'\n\n    def reset(self, **kwargs):\n        """""" Do no-op action for a number of steps in [1, noop_max].""""""\n        self.env.reset(**kwargs)\n        if self.override_num_noops is not None:\n            noops = self.override_num_noops\n        else:\n            noops = self.unwrapped.np_random.randint(1, self.noop_max + 1) #pylint: disable=E1101\n        assert noops > 0\n        obs = None\n        for _ in range(noops):\n            obs, _, done, _ = self.env.step(self.noop_action)\n            if done:\n                obs = self.env.reset(**kwargs)\n        return obs\n\n    def step(self, ac):\n        return self.env.step(ac)\n\nclass FireResetEnv(gym.Wrapper):\n    def __init__(self, env):\n        """"""Take action on reset for environments that are fixed until firing.""""""\n        gym.Wrapper.__init__(self, env)\n        assert env.unwrapped.get_action_meanings()[1] == \'FIRE\'\n        assert len(env.unwrapped.get_action_meanings()) >= 3\n\n    def reset(self, **kwargs):\n        self.env.reset(**kwargs)\n        obs, _, done, _ = self.env.step(1)\n        if done:\n            self.env.reset(**kwargs)\n        obs, _, done, _ = self.env.step(2)\n        if done:\n            self.env.reset(**kwargs)\n        return obs\n\n    def step(self, ac):\n        return self.env.step(ac)\n\nclass EpisodicLifeEnv(gym.Wrapper):\n    def __init__(self, env):\n        """"""Make end-of-life == end-of-episode, but only reset on true game over.\n        Done by DeepMind for the DQN and co. since it helps value estimation.\n        """"""\n        gym.Wrapper.__init__(self, env)\n        self.lives = 0\n        self.was_real_done  = True\n\n    def step(self, action):\n        obs, reward, done, info = self.env.step(action)\n        self.was_real_done = done\n        # check current lives, make loss of life terminal,\n        # then update lives to handle bonus lives\n        lives = self.env.unwrapped.ale.lives()\n        if lives < self.lives and lives > 0:\n            # for Qbert sometimes we stay in lives == 0 condtion for a few frames\n            # so its important to keep lives > 0, so that we only reset once\n            # the environment advertises done.\n            done = True\n        self.lives = lives\n        return obs, reward, done, info\n\n    def reset(self, **kwargs):\n        """"""Reset only when lives are exhausted.\n        This way all states are still reachable even though lives are episodic,\n        and the learner need not know about any of this behind-the-scenes.\n        """"""\n        if self.was_real_done:\n            obs = self.env.reset(**kwargs)\n        else:\n            # no-op step to advance from terminal/lost life state\n            obs, _, _, _ = self.env.step(0)\n        self.lives = self.env.unwrapped.ale.lives()\n        return obs\n\nclass MaxAndSkipEnv(gym.Wrapper):\n    def __init__(self, env, skip=4):\n        """"""Return only every `skip`-th frame""""""\n        gym.Wrapper.__init__(self, env)\n        # most recent raw observations (for max pooling across time steps)\n        self._obs_buffer = np.zeros((2,)+env.observation_space.shape, dtype=np.uint8)\n        self._skip       = skip\n\n    def reset(self):\n        return self.env.reset()\n\n    def step(self, action):\n        """"""Repeat action, sum reward, and max over last observations.""""""\n        total_reward = 0.0\n        done = None\n        for i in range(self._skip):\n            obs, reward, done, info = self.env.step(action)\n            if i == self._skip - 2: self._obs_buffer[0] = obs\n            if i == self._skip - 1: self._obs_buffer[1] = obs\n            total_reward += reward\n            if done:\n                break\n        # Note that the observation on the done=True frame\n        # doesn\'t matter\n        max_frame = self._obs_buffer.max(axis=0)\n\n        return max_frame, total_reward, done, info\n\n    def reset(self, **kwargs):\n        return self.env.reset(**kwargs)\n\nclass ClipRewardEnv(gym.RewardWrapper):\n    def __init__(self, env):\n        gym.RewardWrapper.__init__(self, env)\n\n    def reward(self, reward):\n        """"""Bin reward to {+1, 0, -1} by its sign.""""""\n        return np.sign(reward)\n\nclass WarpFrame(gym.ObservationWrapper):\n    def __init__(self, env):\n        """"""Warp frames to 84x84 as done in the Nature paper and later work.""""""\n        gym.ObservationWrapper.__init__(self, env)\n        self.width = 84\n        self.height = 84\n        self.observation_space = spaces.Box(low=0, high=255,\n            shape=(self.height, self.width, 1), dtype=np.uint8)\n\n    def observation(self, frame):\n        frame = cv2.cvtColor(frame, cv2.COLOR_RGB2GRAY)\n        frame = cv2.resize(frame, (self.width, self.height), interpolation=cv2.INTER_AREA)\n        return frame[:, :, None]\n\nclass FrameStack(gym.Wrapper):\n    def __init__(self, env, k):\n        """"""Stack k last frames.\n\n        Returns lazy array, which is much more memory efficient.\n\n        See Also\n        --------\n        baselines.common.atari_wrappers.LazyFrames\n        """"""\n        gym.Wrapper.__init__(self, env)\n        self.k = k\n        self.frames = deque([], maxlen=k)\n        shp = env.observation_space.shape\n        self.observation_space = spaces.Box(low=0, high=255, shape=(shp[0], shp[1], shp[2] * k), dtype=np.uint8)\n\n    def reset(self):\n        ob = self.env.reset()\n        for _ in range(self.k):\n            self.frames.append(ob)\n        return self._get_ob()\n\n    def step(self, action):\n        ob, reward, done, info = self.env.step(action)\n        self.frames.append(ob)\n        return self._get_ob(), reward, done, info\n\n    def _get_ob(self):\n        assert len(self.frames) == self.k\n        return LazyFrames(list(self.frames))\n\nclass ScaledFloatFrame(gym.ObservationWrapper):\n    def __init__(self, env):\n        gym.ObservationWrapper.__init__(self, env)\n\n    def observation(self, observation):\n        # careful! This undoes the memory optimization, use\n        # with smaller replay buffers only.\n        return np.array(observation).astype(np.float32) / 255.0\n\nclass LazyFrames(object):\n    def __init__(self, frames):\n        """"""This object ensures that common frames between the observations are only stored once.\n        It exists purely to optimize memory usage which can be huge for DQN\'s 1M frames replay\n        buffers.\n\n        This object should only be converted to numpy array before being passed to the model.\n\n        You\'d not believe how complex the previous solution was.""""""\n        self._frames = frames\n        self._out = None\n\n    def _force(self):\n        if self._out is None:\n            self._out = np.concatenate(self._frames, axis=2)\n            self._frames = None\n        return self._out\n\n    def __array__(self, dtype=None):\n        out = self._force()\n        if dtype is not None:\n            out = out.astype(dtype)\n        return out\n\n    def __len__(self):\n        return len(self._force())\n\n    def __getitem__(self, i):\n        return self._force()[i]\n\ndef make_atari(env_id):\n    env = gym.make(env_id)\n    assert \'NoFrameskip\' in env.spec.id\n    env = NoopResetEnv(env, noop_max=30)\n    env = MaxAndSkipEnv(env, skip=4)\n    return env\n\ndef wrap_deepmind(env, episode_life=True, clip_rewards=True, frame_stack=False, scale=False):\n    """"""Configure environment for DeepMind-style Atari.\n    """"""\n    if episode_life:\n        env = EpisodicLifeEnv(env)\n    if \'FIRE\' in env.unwrapped.get_action_meanings():\n        env = FireResetEnv(env)\n    env = WarpFrame(env)\n    if scale:\n        env = ScaledFloatFrame(env)\n    if clip_rewards:\n        env = ClipRewardEnv(env)\n    if frame_stack:\n        env = FrameStack(env, 4)\n    return env\n\n'"
vendor/openai/baselines/common/cg.py,0,"b'import numpy as np\ndef cg(f_Ax, b, cg_iters=10, callback=None, verbose=False, residual_tol=1e-10):\n    """"""\n    Demmel p 312\n    """"""\n    p = b.copy()\n    r = b.copy()\n    x = np.zeros_like(b)\n    rdotr = r.dot(r)\n\n    fmtstr =  ""%10i %10.3g %10.3g""\n    titlestr =  ""%10s %10s %10s""\n    if verbose: print(titlestr % (""iter"", ""residual norm"", ""soln norm""))\n\n    for i in range(cg_iters):\n        if callback is not None:\n            callback(x)\n        if verbose: print(fmtstr % (i, rdotr, np.linalg.norm(x)))\n        z = f_Ax(p)\n        v = rdotr / p.dot(z)\n        x += v*p\n        r -= v*z\n        newrdotr = r.dot(r)\n        mu = newrdotr/rdotr\n        p = r + mu*p\n\n        rdotr = newrdotr\n        if rdotr < residual_tol:\n            break\n\n    if callback is not None:\n        callback(x)\n    if verbose: print(fmtstr % (i+1, rdotr, np.linalg.norm(x)))  # pylint: disable=W0631\n    return x'"
vendor/openai/baselines/common/cmd_util.py,0,"b'""""""\nHelpers for scripts like run_atari.py.\n""""""\n\nimport os\nimport gym\nfrom gym.wrappers import FlattenDictWrapper\nfrom vendor.openai.baselines import logger\nfrom vendor.openai.baselines.bench import Monitor\nfrom vendor.openai.baselines.common import set_global_seeds\nfrom vendor.openai.baselines.common.atari_wrappers import make_atari, wrap_deepmind\nfrom vendor.openai.baselines.common.vec_env.subproc_vec_env import SubprocVecEnv\n# from mpi4py import MPI\n\ndef make_atari_env(env_id, num_env, seed, wrapper_kwargs=None, start_index=0):\n    """"""\n    Create a wrapped, monitored SubprocVecEnv for Atari.\n    """"""\n    if wrapper_kwargs is None: wrapper_kwargs = {}\n    def make_env(rank): # pylint: disable=C0111\n        def _thunk():\n            env = make_atari(env_id)\n            env.seed(seed + rank)\n            env = Monitor(env, logger.get_dir() and os.path.join(logger.get_dir(), str(rank)))\n            env.__rank = rank\n            return wrap_deepmind(env, **wrapper_kwargs)\n        return _thunk\n    set_global_seeds(seed)\n    return SubprocVecEnv([make_env(i + start_index) for i in range(num_env)])\n\ndef make_mujoco_env(env_id, seed):\n    """"""\n    Create a wrapped, monitored gym.Env for MuJoCo.\n    """"""\n    set_global_seeds(seed)\n    env = gym.make(env_id)\n    env = Monitor(env, logger.get_dir())\n    env.seed(seed)\n    return env\n\ndef make_robotics_env(env_id, seed, rank=0):\n    """"""\n    Create a wrapped, monitored gym.Env for MuJoCo.\n    """"""\n    set_global_seeds(seed)\n    env = gym.make(env_id)\n    env = FlattenDictWrapper(env, [\'observation\', \'desired_goal\'])\n    env = Monitor(\n        env, logger.get_dir() and os.path.join(logger.get_dir(), str(rank)),\n        info_keywords=(\'is_success\',))\n    env.seed(seed)\n    return env\n\ndef arg_parser():\n    """"""\n    Create an empty argparse.ArgumentParser.\n    """"""\n    import argparse\n    return argparse.ArgumentParser(formatter_class=argparse.ArgumentDefaultsHelpFormatter)\n\ndef atari_arg_parser():\n    """"""\n    Create an argparse.ArgumentParser for run_atari.py.\n    """"""\n    parser = arg_parser()\n    parser.add_argument(\'--env\', help=\'environment ID\', default=\'BreakoutNoFrameskip-v4\')\n    parser.add_argument(\'--seed\', help=\'RNG seed\', type=int, default=0)\n    parser.add_argument(\'--num-timesteps\', type=int, default=int(10e6))\n    return parser\n\ndef mujoco_arg_parser():\n    """"""\n    Create an argparse.ArgumentParser for run_mujoco.py.\n    """"""\n    parser = arg_parser()\n    parser.add_argument(\'--env\', help=\'environment ID\', type=str, default=\'Reacher-v2\')\n    parser.add_argument(\'--seed\', help=\'RNG seed\', type=int, default=0)\n    parser.add_argument(\'--num-timesteps\', type=int, default=int(1e6))\n    return parser\n\ndef robotics_arg_parser():\n    """"""\n    Create an argparse.ArgumentParser for run_mujoco.py.\n    """"""\n    parser = arg_parser()\n    parser.add_argument(\'--env\', help=\'environment ID\', type=str, default=\'FetchReach-v0\')\n    parser.add_argument(\'--seed\', help=\'RNG seed\', type=int, default=0)\n    parser.add_argument(\'--num-timesteps\', type=int, default=int(1e6))\n    return parser\n\ndef continuous_mountain_car_arg_parser():\n    """"""\n    Create an argparse.ArgumentParser for run_mujoco.py.\n    """"""\n    parser = arg_parser()\n    parser.add_argument(\'--env\', help=\'environment ID\', type=str, default=\'MountainCarContinuous-v0\')\n    parser.add_argument(\'--seed\', help=\'RNG seed\', type=int, default=0)\n    parser.add_argument(\'--num-timesteps\', type=int, default=int(10e6))\n    return parser'"
vendor/openai/baselines/common/console_util.py,0,"b'from __future__ import print_function\nfrom contextlib import contextmanager\nimport numpy as np\nimport time\n\n# ================================================================\n# Misc\n# ================================================================\n\ndef fmt_row(width, row, header=False):\n    out = "" | "".join(fmt_item(x, width) for x in row)\n    if header: out = out + ""\\n"" + ""-""*len(out)\n    return out\n\ndef fmt_item(x, l):\n    if isinstance(x, np.ndarray):\n        assert x.ndim==0\n        x = x.item()\n    if isinstance(x, (float, np.float32, np.float64)):\n        v = abs(x)\n        if (v < 1e-4 or v > 1e+4) and v > 0:\n            rep = ""%7.2e"" % x\n        else:\n            rep = ""%7.5f"" % x\n    else: rep = str(x)\n    return "" ""*(l - len(rep)) + rep\n\ncolor2num = dict(\n    gray=30,\n    red=31,\n    green=32,\n    yellow=33,\n    blue=34,\n    magenta=35,\n    cyan=36,\n    white=37,\n    crimson=38\n)\n\ndef colorize(string, color, bold=False, highlight=False):\n    attr = []\n    num = color2num[color]\n    if highlight: num += 10\n    attr.append(str(num))\n    if bold: attr.append(\'1\')\n    return \'\\x1b[%sm%s\\x1b[0m\' % (\';\'.join(attr), string)\n\n\nMESSAGE_DEPTH = 0\n\n@contextmanager\ndef timed(msg):\n    global MESSAGE_DEPTH #pylint: disable=W0603\n    print(colorize(\'\\t\'*MESSAGE_DEPTH + \'=: \' + msg, color=\'magenta\'))\n    tstart = time.time()\n    MESSAGE_DEPTH += 1\n    yield\n    MESSAGE_DEPTH -= 1\n    print(colorize(\'\\t\'*MESSAGE_DEPTH + ""done in %.3f seconds""%(time.time() - tstart), color=\'magenta\'))\n'"
vendor/openai/baselines/common/dataset.py,0,"b""import numpy as np\n\nclass Dataset(object):\n    def __init__(self, data_map, deterministic=False, shuffle=True):\n        self.data_map = data_map\n        self.deterministic = deterministic\n        self.enable_shuffle = shuffle\n        self.n = next(iter(data_map.values())).shape[0]\n        self._next_id = 0\n        self.shuffle()\n\n    def shuffle(self):\n        if self.deterministic:\n            return\n        perm = np.arange(self.n)\n        np.random.shuffle(perm)\n\n        for key in self.data_map:\n            self.data_map[key] = self.data_map[key][perm]\n\n        self._next_id = 0\n\n    def next_batch(self, batch_size):\n        if self._next_id >= self.n and self.enable_shuffle:\n            self.shuffle()\n\n        cur_id = self._next_id\n        cur_batch_size = min(batch_size, self.n - self._next_id)\n        self._next_id += cur_batch_size\n\n        data_map = dict()\n        for key in self.data_map:\n            data_map[key] = self.data_map[key][cur_id:cur_id+cur_batch_size]\n        return data_map\n\n    def iterate_once(self, batch_size):\n        if self.enable_shuffle: self.shuffle()\n\n        while self._next_id <= self.n - batch_size:\n            yield self.next_batch(batch_size)\n        self._next_id = 0\n\n    def subset(self, num_elements, deterministic=True):\n        data_map = dict()\n        for key in self.data_map:\n            data_map[key] = self.data_map[key][:num_elements]\n        return Dataset(data_map, deterministic)\n\n\ndef iterbatches(arrays, *, num_batches=None, batch_size=None, shuffle=True, include_final_partial_batch=True):\n    assert (num_batches is None) != (batch_size is None), 'Provide num_batches or batch_size, but not both'\n    arrays = tuple(map(np.asarray, arrays))\n    n = arrays[0].shape[0]\n    assert all(a.shape[0] == n for a in arrays[1:])\n    inds = np.arange(n)\n    if shuffle: np.random.shuffle(inds)\n    sections = np.arange(0, n, batch_size)[1:] if num_batches is None else num_batches\n    for batch_inds in np.array_split(inds, sections):\n        if include_final_partial_batch or len(batch_inds) == batch_size:\n            yield tuple(a[batch_inds] for a in arrays)\n"""
vendor/openai/baselines/common/distributions.py,53,"b'import tensorflow as tf\nimport numpy as np\nimport vendor.openai.baselines.common.tf_util as U\nfrom tensorflow.python.ops import math_ops\n\nclass Pd(object):\n    """"""\n    A particular probability distribution\n    """"""\n    def flatparam(self):\n        raise NotImplementedError\n    def mode(self):\n        raise NotImplementedError\n    def neglogp(self, x):\n        # Usually it\'s easier to define the negative logprob\n        raise NotImplementedError\n    def kl(self, other):\n        raise NotImplementedError\n    def entropy(self):\n        raise NotImplementedError\n    def sample(self):\n        raise NotImplementedError\n    def logp(self, x):\n        return - self.neglogp(x)\n\nclass PdType(object):\n    """"""\n    Parametrized family of probability distributions\n    """"""\n    def pdclass(self):\n        raise NotImplementedError\n    def pdfromflat(self, flat):\n        return self.pdclass()(flat)\n    def param_shape(self):\n        raise NotImplementedError\n    def sample_shape(self):\n        raise NotImplementedError\n    def sample_dtype(self):\n        raise NotImplementedError\n\n    def param_placeholder(self, prepend_shape, name=None):\n        return tf.placeholder(dtype=tf.float32, shape=prepend_shape+self.param_shape(), name=name)\n    def sample_placeholder(self, prepend_shape, name=None):\n        return tf.placeholder(dtype=self.sample_dtype(), shape=prepend_shape+self.sample_shape(), name=name)\n\nclass CategoricalPdType(PdType):\n    def __init__(self, ncat):\n        self.ncat = ncat\n    def pdclass(self):\n        return CategoricalPd\n    def param_shape(self):\n        return [self.ncat]\n    def sample_shape(self):\n        return []\n    def sample_dtype(self):\n        return tf.int32\n\n\nclass MultiCategoricalPdType(PdType):\n    def __init__(self, nvec):\n        self.ncats = nvec\n    def pdclass(self):\n        return MultiCategoricalPd\n    def pdfromflat(self, flat):\n        return MultiCategoricalPd(self.ncats, flat)\n    def param_shape(self):\n        return [sum(self.ncats)]\n    def sample_shape(self):\n        return [len(self.ncats)]\n    def sample_dtype(self):\n        return tf.int32\n\nclass DiagGaussianPdType(PdType):\n    def __init__(self, size):\n        self.size = size\n    def pdclass(self):\n        return DiagGaussianPd\n    def param_shape(self):\n        return [2*self.size]\n    def sample_shape(self):\n        return [self.size]\n    def sample_dtype(self):\n        return tf.float32\n\nclass BernoulliPdType(PdType):\n    def __init__(self, size):\n        self.size = size\n    def pdclass(self):\n        return BernoulliPd\n    def param_shape(self):\n        return [self.size]\n    def sample_shape(self):\n        return [self.size]\n    def sample_dtype(self):\n        return tf.int32\n\n# WRONG SECOND DERIVATIVES\n# class CategoricalPd(Pd):\n#     def __init__(self, logits):\n#         self.logits = logits\n#         self.ps = tf.nn.softmax(logits)\n#     @classmethod\n#     def fromflat(cls, flat):\n#         return cls(flat)\n#     def flatparam(self):\n#         return self.logits\n#     def mode(self):\n#         return U.argmax(self.logits, axis=-1)\n#     def logp(self, x):\n#         return -tf.nn.sparse_softmax_cross_entropy_with_logits(self.logits, x)\n#     def kl(self, other):\n#         return tf.nn.softmax_cross_entropy_with_logits(other.logits, self.ps) \\\n#                 - tf.nn.softmax_cross_entropy_with_logits(self.logits, self.ps)\n#     def entropy(self):\n#         return tf.nn.softmax_cross_entropy_with_logits(self.logits, self.ps)\n#     def sample(self):\n#         u = tf.random_uniform(tf.shape(self.logits))\n#         return U.argmax(self.logits - tf.log(-tf.log(u)), axis=-1)\n\nclass CategoricalPd(Pd):\n    def __init__(self, logits):\n        self.logits = logits\n    def flatparam(self):\n        return self.logits\n    def mode(self):\n        return tf.argmax(self.logits, axis=-1)\n    def neglogp(self, x):\n        # return tf.nn.sparse_softmax_cross_entropy_with_logits(logits=self.logits, labels=x)\n        # Note: we can\'t use sparse_softmax_cross_entropy_with_logits because\n        #       the implementation does not allow second-order derivatives...\n        one_hot_actions = tf.one_hot(x, self.logits.get_shape().as_list()[-1])\n        return tf.nn.softmax_cross_entropy_with_logits(\n            logits=self.logits,\n            labels=one_hot_actions)\n    def kl(self, other):\n        a0 = self.logits - tf.reduce_max(self.logits, axis=-1, keep_dims=True)\n        a1 = other.logits - tf.reduce_max(other.logits, axis=-1, keep_dims=True)\n        ea0 = tf.exp(a0)\n        ea1 = tf.exp(a1)\n        z0 = tf.reduce_sum(ea0, axis=-1, keep_dims=True)\n        z1 = tf.reduce_sum(ea1, axis=-1, keep_dims=True)\n        p0 = ea0 / z0\n        return tf.reduce_sum(p0 * (a0 - tf.log(z0) - a1 + tf.log(z1)), axis=-1)\n    def entropy(self):\n        a0 = self.logits - tf.reduce_max(self.logits, axis=-1, keep_dims=True)\n        ea0 = tf.exp(a0)\n        z0 = tf.reduce_sum(ea0, axis=-1, keep_dims=True)\n        p0 = ea0 / z0\n        return tf.reduce_sum(p0 * (tf.log(z0) - a0), axis=-1)\n    def sample(self):\n        u = tf.random_uniform(tf.shape(self.logits))\n        return tf.argmax(self.logits - tf.log(-tf.log(u)), axis=-1)\n    @classmethod\n    def fromflat(cls, flat):\n        return cls(flat)\n\nclass MultiCategoricalPd(Pd):\n    def __init__(self, nvec, flat):\n        self.flat = flat\n        self.categoricals = list(map(CategoricalPd, tf.split(flat, nvec, axis=-1)))\n    def flatparam(self):\n        return self.flat\n    def mode(self):\n        return tf.cast(tf.stack([p.mode() for p in self.categoricals], axis=-1), tf.int32)\n    def neglogp(self, x):\n        return tf.add_n([p.neglogp(px) for p, px in zip(self.categoricals, tf.unstack(x, axis=-1))])\n    def kl(self, other):\n        return tf.add_n([p.kl(q) for p, q in zip(self.categoricals, other.categoricals)])\n    def entropy(self):\n        return tf.add_n([p.entropy() for p in self.categoricals])\n    def sample(self):\n        return tf.cast(tf.stack([p.sample() for p in self.categoricals], axis=-1), tf.int32)\n    @classmethod\n    def fromflat(cls, flat):\n        raise NotImplementedError\n\nclass DiagGaussianPd(Pd):\n    def __init__(self, flat):\n        self.flat = flat\n        mean, logstd = tf.split(axis=len(flat.shape)-1, num_or_size_splits=2, value=flat)\n        self.mean = mean\n        self.logstd = logstd\n        self.std = tf.exp(logstd)\n    def flatparam(self):\n        return self.flat\n    def mode(self):\n        return self.mean\n    def neglogp(self, x):\n        return 0.5 * tf.reduce_sum(tf.square((x - self.mean) / self.std), axis=-1) \\\n               + 0.5 * np.log(2.0 * np.pi) * tf.to_float(tf.shape(x)[-1]) \\\n               + tf.reduce_sum(self.logstd, axis=-1)\n    def kl(self, other):\n        assert isinstance(other, DiagGaussianPd)\n        return tf.reduce_sum(other.logstd - self.logstd + (tf.square(self.std) + tf.square(self.mean - other.mean)) / (2.0 * tf.square(other.std)) - 0.5, axis=-1)\n    def entropy(self):\n        return tf.reduce_sum(self.logstd + .5 * np.log(2.0 * np.pi * np.e), axis=-1)\n    def sample(self):\n        return self.mean + self.std * tf.random_normal(tf.shape(self.mean))\n    @classmethod\n    def fromflat(cls, flat):\n        return cls(flat)\n\nclass BernoulliPd(Pd):\n    def __init__(self, logits):\n        self.logits = logits\n        self.ps = tf.sigmoid(logits)\n    def flatparam(self):\n        return self.logits\n    def mode(self):\n        return tf.round(self.ps)\n    def neglogp(self, x):\n        return tf.reduce_sum(tf.nn.sigmoid_cross_entropy_with_logits(logits=self.logits, labels=tf.to_float(x)), axis=-1)\n    def kl(self, other):\n        return tf.reduce_sum(tf.nn.sigmoid_cross_entropy_with_logits(logits=other.logits, labels=self.ps), axis=-1) - tf.reduce_sum(tf.nn.sigmoid_cross_entropy_with_logits(logits=self.logits, labels=self.ps), axis=-1)\n    def entropy(self):\n        return tf.reduce_sum(tf.nn.sigmoid_cross_entropy_with_logits(logits=self.logits, labels=self.ps), axis=-1)\n    def sample(self):\n        u = tf.random_uniform(tf.shape(self.ps))\n        return tf.to_float(math_ops.less(u, self.ps))\n    @classmethod\n    def fromflat(cls, flat):\n        return cls(flat)\n\ndef make_pdtype(ac_space):\n    from gym import spaces\n    if isinstance(ac_space, spaces.Box):\n        assert len(ac_space.shape) == 1\n        return DiagGaussianPdType(ac_space.shape[0])\n    elif isinstance(ac_space, spaces.Discrete):\n        return CategoricalPdType(ac_space.n)\n    elif isinstance(ac_space, spaces.MultiDiscrete):\n        return MultiCategoricalPdType(ac_space.nvec)\n    elif isinstance(ac_space, spaces.MultiBinary):\n        return BernoulliPdType(ac_space.n)\n    else:\n        raise NotImplementedError\n\ndef shape_el(v, i):\n    maybe = v.get_shape()[i]\n    if maybe is not None:\n        return maybe\n    else:\n        return tf.shape(v)[i]\n\n@U.in_session\ndef test_probtypes():\n    np.random.seed(0)\n\n    pdparam_diag_gauss = np.array([-.2, .3, .4, -.5, .1, -.5, .1, 0.8])\n    diag_gauss = DiagGaussianPdType(pdparam_diag_gauss.size // 2) #pylint: disable=E1101\n    validate_probtype(diag_gauss, pdparam_diag_gauss)\n\n    pdparam_categorical = np.array([-.2, .3, .5])\n    categorical = CategoricalPdType(pdparam_categorical.size) #pylint: disable=E1101\n    validate_probtype(categorical, pdparam_categorical)\n\n    nvec = [1,2,3]\n    pdparam_multicategorical = np.array([-.2, .3, .5, .1, 1, -.1])\n    multicategorical = MultiCategoricalPdType(nvec) #pylint: disable=E1101\n    validate_probtype(multicategorical, pdparam_multicategorical)\n\n    pdparam_bernoulli = np.array([-.2, .3, .5])\n    bernoulli = BernoulliPdType(pdparam_bernoulli.size) #pylint: disable=E1101\n    validate_probtype(bernoulli, pdparam_bernoulli)\n\n\ndef validate_probtype(probtype, pdparam):\n    N = 100000\n    # Check to see if mean negative log likelihood == differential entropy\n    Mval = np.repeat(pdparam[None, :], N, axis=0)\n    M = probtype.param_placeholder([N])\n    X = probtype.sample_placeholder([N])\n    pd = probtype.pdfromflat(M)\n    calcloglik = U.function([X, M], pd.logp(X))\n    calcent = U.function([M], pd.entropy())\n    Xval = tf.get_default_session().run(pd.sample(), feed_dict={M:Mval})\n    logliks = calcloglik(Xval, Mval)\n    entval_ll = - logliks.mean() #pylint: disable=E1101\n    entval_ll_stderr = logliks.std() / np.sqrt(N) #pylint: disable=E1101\n    entval = calcent(Mval).mean() #pylint: disable=E1101\n    assert np.abs(entval - entval_ll) < 3 * entval_ll_stderr # within 3 sigmas\n\n    # Check to see if kldiv[p,q] = - ent[p] - E_p[log q]\n    M2 = probtype.param_placeholder([N])\n    pd2 = probtype.pdfromflat(M2)\n    q = pdparam + np.random.randn(pdparam.size) * 0.1\n    Mval2 = np.repeat(q[None, :], N, axis=0)\n    calckl = U.function([M, M2], pd.kl(pd2))\n    klval = calckl(Mval, Mval2).mean() #pylint: disable=E1101\n    logliks = calcloglik(Xval, Mval2)\n    klval_ll = - entval - logliks.mean() #pylint: disable=E1101\n    klval_ll_stderr = logliks.std() / np.sqrt(N) #pylint: disable=E1101\n    assert np.abs(klval - klval_ll) < 3 * klval_ll_stderr # within 3 sigmas\n    print(\'ok on\', probtype, pdparam)\n\n'"
vendor/openai/baselines/common/math_util.py,0,"b'import numpy as np\nimport scipy.signal\n\n\ndef discount(x, gamma):\n    """"""\n    computes discounted sums along 0th dimension of x.\n\n    inputs\n    ------\n    x: ndarray\n    gamma: float\n\n    outputs\n    -------\n    y: ndarray with same shape as x, satisfying\n\n        y[t] = x[t] + gamma*x[t+1] + gamma^2*x[t+2] + ... + gamma^k x[t+k],\n                where k = len(x) - t - 1\n\n    """"""\n    assert x.ndim >= 1\n    return scipy.signal.lfilter([1],[1,-gamma],x[::-1], axis=0)[::-1]\n\ndef explained_variance(ypred,y):\n    """"""\n    Computes fraction of variance that ypred explains about y.\n    Returns 1 - Var[y-ypred] / Var[y]\n\n    interpretation:\n        ev=0  =>  might as well have predicted zero\n        ev=1  =>  perfect prediction\n        ev<0  =>  worse than just predicting zero\n\n    """"""\n    assert y.ndim == 1 and ypred.ndim == 1\n    vary = np.var(y)\n    return np.nan if vary==0 else 1 - np.var(y-ypred)/vary\n\ndef explained_variance_2d(ypred, y):\n    assert y.ndim == 2 and ypred.ndim == 2\n    vary = np.var(y, axis=0)\n    out = 1 - np.var(y-ypred)/vary\n    out[vary < 1e-10] = 0\n    return out\n\ndef ncc(ypred, y):\n    return np.corrcoef(ypred, y)[1,0]\n\ndef flatten_arrays(arrs):\n    return np.concatenate([arr.flat for arr in arrs])\n\ndef unflatten_vector(vec, shapes):\n    i=0\n    arrs = []\n    for shape in shapes:\n        size = np.prod(shape)\n        arr = vec[i:i+size].reshape(shape)\n        arrs.append(arr)\n        i += size\n    return arrs\n\ndef discount_with_boundaries(X, New, gamma):\n    """"""\n    X: 2d array of floats, time x features\n    New: 2d array of bools, indicating when a new episode has started\n    """"""\n    Y = np.zeros_like(X)\n    T = X.shape[0]\n    Y[T-1] = X[T-1]\n    for t in range(T-2, -1, -1):\n        Y[t] = X[t] + gamma * Y[t+1] * (1 - New[t+1])\n    return Y\n\ndef test_discount_with_boundaries():\n    gamma=0.9\n    x = np.array([1.0, 2.0, 3.0, 4.0], \'float32\')\n    starts = [1.0, 0.0, 0.0, 1.0]\n    y = discount_with_boundaries(x, starts, gamma)\n    assert np.allclose(y, [\n        1 + gamma * 2 + gamma**2 * 3,\n        2 + gamma * 3,\n        3,\n        4\n    ])'"
vendor/openai/baselines/common/misc_util.py,1,"b'import gym\nimport numpy as np\nimport os\nimport pickle\nimport random\nimport tempfile\nimport zipfile\n\n\ndef zipsame(*seqs):\n    L = len(seqs[0])\n    assert all(len(seq) == L for seq in seqs[1:])\n    return zip(*seqs)\n\n\ndef unpack(seq, sizes):\n    """"""\n    Unpack \'seq\' into a sequence of lists, with lengths specified by \'sizes\'.\n    None = just one bare element, not a list\n\n    Example:\n    unpack([1,2,3,4,5,6], [3,None,2]) -> ([1,2,3], 4, [5,6])\n    """"""\n    seq = list(seq)\n    it = iter(seq)\n    assert sum(1 if s is None else s for s in sizes) == len(seq), ""Trying to unpack %s into %s"" % (seq, sizes)\n    for size in sizes:\n        if size is None:\n            yield it.__next__()\n        else:\n            li = []\n            for _ in range(size):\n                li.append(it.__next__())\n            yield li\n\n\nclass EzPickle(object):\n    """"""Objects that are pickled and unpickled via their constructor\n    arguments.\n\n    Example usage:\n\n        class Dog(Animal, EzPickle):\n            def __init__(self, furcolor, tailkind=""bushy""):\n                Animal.__init__()\n                EzPickle.__init__(furcolor, tailkind)\n                ...\n\n    When this object is unpickled, a new Dog will be constructed by passing the provided\n    furcolor and tailkind into the constructor. However, philosophers are still not sure\n    whether it is still the same dog.\n\n    This is generally needed only for environments which wrap C/C++ code, such as MuJoCo\n    and Atari.\n    """"""\n\n    def __init__(self, *args, **kwargs):\n        self._ezpickle_args = args\n        self._ezpickle_kwargs = kwargs\n\n    def __getstate__(self):\n        return {""_ezpickle_args"": self._ezpickle_args, ""_ezpickle_kwargs"": self._ezpickle_kwargs}\n\n    def __setstate__(self, d):\n        out = type(self)(*d[""_ezpickle_args""], **d[""_ezpickle_kwargs""])\n        self.__dict__.update(out.__dict__)\n\n\ndef set_global_seeds(i):\n    try:\n        import tensorflow as tf\n    except ImportError:\n        pass\n    else:\n        tf.set_random_seed(i)\n    np.random.seed(i)\n    random.seed(i)\n\n\ndef pretty_eta(seconds_left):\n    """"""Print the number of seconds in human readable format.\n\n    Examples:\n    2 days\n    2 hours and 37 minutes\n    less than a minute\n\n    Paramters\n    ---------\n    seconds_left: int\n        Number of seconds to be converted to the ETA\n    Returns\n    -------\n    eta: str\n        String representing the pretty ETA.\n    """"""\n    minutes_left = seconds_left // 60\n    seconds_left %= 60\n    hours_left = minutes_left // 60\n    minutes_left %= 60\n    days_left = hours_left // 24\n    hours_left %= 24\n\n    def helper(cnt, name):\n        return ""{} {}{}"".format(str(cnt), name, (\'s\' if cnt > 1 else \'\'))\n\n    if days_left > 0:\n        msg = helper(days_left, \'day\')\n        if hours_left > 0:\n            msg += \' and \' + helper(hours_left, \'hour\')\n        return msg\n    if hours_left > 0:\n        msg = helper(hours_left, \'hour\')\n        if minutes_left > 0:\n            msg += \' and \' + helper(minutes_left, \'minute\')\n        return msg\n    if minutes_left > 0:\n        return helper(minutes_left, \'minute\')\n    return \'less than a minute\'\n\n\nclass RunningAvg(object):\n    def __init__(self, gamma, init_value=None):\n        """"""Keep a running estimate of a quantity. This is a bit like mean\n        but more sensitive to recent changes.\n\n        Parameters\n        ----------\n        gamma: float\n            Must be between 0 and 1, where 0 is the most sensitive to recent\n            changes.\n        init_value: float or None\n            Initial value of the estimate. If None, it will be set on the first update.\n        """"""\n        self._value = init_value\n        self._gamma = gamma\n\n    def update(self, new_val):\n        """"""Update the estimate.\n\n        Parameters\n        ----------\n        new_val: float\n            new observated value of estimated quantity.\n        """"""\n        if self._value is None:\n            self._value = new_val\n        else:\n            self._value = self._gamma * self._value + (1.0 - self._gamma) * new_val\n\n    def __float__(self):\n        """"""Get the current estimate""""""\n        return self._value\n\ndef boolean_flag(parser, name, default=False, help=None):\n    """"""Add a boolean flag to argparse parser.\n\n    Parameters\n    ----------\n    parser: argparse.Parser\n        parser to add the flag to\n    name: str\n        --<name> will enable the flag, while --no-<name> will disable it\n    default: bool or None\n        default value of the flag\n    help: str\n        help string for the flag\n    """"""\n    dest = name.replace(\'-\', \'_\')\n    parser.add_argument(""--"" + name, action=""store_true"", default=default, dest=dest, help=help)\n    parser.add_argument(""--no-"" + name, action=""store_false"", dest=dest)\n\n\ndef get_wrapper_by_name(env, classname):\n    """"""Given an a gym environment possibly wrapped multiple times, returns a wrapper\n    of class named classname or raises ValueError if no such wrapper was applied\n\n    Parameters\n    ----------\n    env: gym.Env of gym.Wrapper\n        gym environment\n    classname: str\n        name of the wrapper\n\n    Returns\n    -------\n    wrapper: gym.Wrapper\n        wrapper named classname\n    """"""\n    currentenv = env\n    while True:\n        if classname == currentenv.class_name():\n            return currentenv\n        elif isinstance(currentenv, gym.Wrapper):\n            currentenv = currentenv.env\n        else:\n            raise ValueError(""Couldn\'t find wrapper named %s"" % classname)\n\n\ndef relatively_safe_pickle_dump(obj, path, compression=False):\n    """"""This is just like regular pickle dump, except from the fact that failure cases are\n    different:\n\n        - It\'s never possible that we end up with a pickle in corrupted state.\n        - If a there was a different file at the path, that file will remain unchanged in the\n          even of failure (provided that filesystem rename is atomic).\n        - it is sometimes possible that we end up with useless temp file which needs to be\n          deleted manually (it will be removed automatically on the next function call)\n\n    The indended use case is periodic checkpoints of experiment state, such that we never\n    corrupt previous checkpoints if the current one fails.\n\n    Parameters\n    ----------\n    obj: object\n        object to pickle\n    path: str\n        path to the output file\n    compression: bool\n        if true pickle will be compressed\n    """"""\n    temp_storage = path + "".relatively_safe""\n    if compression:\n        # Using gzip here would be simpler, but the size is limited to 2GB\n        with tempfile.NamedTemporaryFile() as uncompressed_file:\n            pickle.dump(obj, uncompressed_file)\n            uncompressed_file.file.flush()\n            with zipfile.ZipFile(temp_storage, ""w"", compression=zipfile.ZIP_DEFLATED) as myzip:\n                myzip.write(uncompressed_file.name, ""data"")\n    else:\n        with open(temp_storage, ""wb"") as f:\n            pickle.dump(obj, f)\n    os.rename(temp_storage, path)\n\n\ndef pickle_load(path, compression=False):\n    """"""Unpickle a possible compressed pickle.\n\n    Parameters\n    ----------\n    path: str\n        path to the output file\n    compression: bool\n        if true assumes that pickle was compressed when created and attempts decompression.\n\n    Returns\n    -------\n    obj: object\n        the unpickled object\n    """"""\n\n    if compression:\n        with zipfile.ZipFile(path, ""r"", compression=zipfile.ZIP_DEFLATED) as myzip:\n            with myzip.open(""data"") as f:\n                return pickle.load(f)\n    else:\n        with open(path, ""rb"") as f:\n            return pickle.load(f)\n'"
vendor/openai/baselines/common/mpi_adam.py,8,"b""from mpi4py import MPI\nimport vendor.openai.baselines.common.tf_util as U\nimport tensorflow as tf\nimport numpy as np\n\nclass MpiAdam(object):\n    def __init__(self, var_list, *, beta1=0.9, beta2=0.999, epsilon=1e-08, scale_grad_by_procs=True, comm=None):\n        self.var_list = var_list\n        self.beta1 = beta1\n        self.beta2 = beta2\n        self.epsilon = epsilon\n        self.scale_grad_by_procs = scale_grad_by_procs\n        size = sum(U.numel(v) for v in var_list)\n        self.m = np.zeros(size, 'float32')\n        self.v = np.zeros(size, 'float32')\n        self.t = 0\n        self.setfromflat = U.SetFromFlat(var_list)\n        self.getflat = U.GetFlat(var_list)\n        self.comm = MPI.COMM_WORLD if comm is None else comm\n\n    def update(self, localg, stepsize):\n        if self.t % 100 == 0:\n            self.check_synced()\n        localg = localg.astype('float32')\n        globalg = np.zeros_like(localg)\n        self.comm.Allreduce(localg, globalg, op=MPI.SUM)\n        if self.scale_grad_by_procs:\n            globalg /= self.comm.Get_size()\n\n        self.t += 1\n        a = stepsize * np.sqrt(1 - self.beta2**self.t)/(1 - self.beta1**self.t)\n        self.m = self.beta1 * self.m + (1 - self.beta1) * globalg\n        self.v = self.beta2 * self.v + (1 - self.beta2) * (globalg * globalg)\n        step = (- a) * self.m / (np.sqrt(self.v) + self.epsilon)\n        self.setfromflat(self.getflat() + step)\n\n    def sync(self):\n        theta = self.getflat()\n        self.comm.Bcast(theta, root=0)\n        self.setfromflat(theta)\n\n    def check_synced(self):\n        if self.comm.Get_rank() == 0: # this is root\n            theta = self.getflat()\n            self.comm.Bcast(theta, root=0)\n        else:\n            thetalocal = self.getflat()\n            thetaroot = np.empty_like(thetalocal)\n            self.comm.Bcast(thetaroot, root=0)\n            assert (thetaroot == thetalocal).all(), (thetaroot, thetalocal)\n\n@U.in_session\ndef test_MpiAdam():\n    np.random.seed(0)\n    tf.set_random_seed(0)\n\n    a = tf.Variable(np.random.randn(3).astype('float32'))\n    b = tf.Variable(np.random.randn(2,5).astype('float32'))\n    loss = tf.reduce_sum(tf.square(a)) + tf.reduce_sum(tf.sin(b))\n\n    stepsize = 1e-2\n    update_op = tf.train.AdamOptimizer(stepsize).minimize(loss)\n    do_update = U.function([], loss, updates=[update_op])\n\n    tf.get_default_session().run(tf.global_variables_initializer())\n    for i in range(10):\n        print(i,do_update())\n\n    tf.set_random_seed(0)\n    tf.get_default_session().run(tf.global_variables_initializer())\n\n    var_list = [a,b]\n    lossandgrad = U.function([], [loss, U.flatgrad(loss, var_list)], updates=[update_op])\n    adam = MpiAdam(var_list)\n\n    for i in range(10):\n        l,g = lossandgrad()\n        adam.update(g, stepsize)\n        print(i,l)"""
vendor/openai/baselines/common/mpi_fork.py,0,"b'import os, subprocess, sys\n\ndef mpi_fork(n, bind_to_core=False):\n    """"""Re-launches the current script with workers\n    Returns ""parent"" for original parent, ""child"" for MPI children\n    """"""\n    if n<=1: \n        return ""child""\n    if os.getenv(""IN_MPI"") is None:\n        env = os.environ.copy()\n        env.update(\n            MKL_NUM_THREADS=""1"",\n            OMP_NUM_THREADS=""1"",\n            IN_MPI=""1""\n        )\n        args = [""mpirun"", ""-np"", str(n)]\n        if bind_to_core:\n            args += [""-bind-to"", ""core""]\n        args += [sys.executable] + sys.argv\n        subprocess.check_call(args, env=env)\n        return ""parent""\n    else:\n        return ""child""\n'"
vendor/openai/baselines/common/mpi_moments.py,0,"b'from mpi4py import MPI\nimport numpy as np\nfrom vendor.openai.baselines.common import zipsame\n\ndef mpi_mean(x, axis=0, comm=None, keepdims=False):\n    x = np.asarray(x)\n    assert x.ndim > 0\n    if comm is None: comm = MPI.COMM_WORLD\n    xsum = x.sum(axis=axis, keepdims=keepdims)\n    n = xsum.size\n    localsum = np.zeros(n+1, x.dtype)\n    localsum[:n] = xsum.ravel()\n    localsum[n] = x.shape[axis]\n    globalsum = np.zeros_like(localsum)\n    comm.Allreduce(localsum, globalsum, op=MPI.SUM)\n    return globalsum[:n].reshape(xsum.shape) / globalsum[n], globalsum[n]\n\ndef mpi_moments(x, axis=0, comm=None, keepdims=False):\n    x = np.asarray(x)\n    assert x.ndim > 0\n    mean, count = mpi_mean(x, axis=axis, comm=comm, keepdims=True)\n    sqdiffs = np.square(x - mean)\n    meansqdiff, count1 = mpi_mean(sqdiffs, axis=axis, comm=comm, keepdims=True)\n    assert count1 == count\n    std = np.sqrt(meansqdiff)\n    if not keepdims:\n        newshape = mean.shape[:axis] + mean.shape[axis+1:]\n        mean = mean.reshape(newshape)\n        std = std.reshape(newshape)\n    return mean, std, count\n\n\ndef test_runningmeanstd():\n    import subprocess\n    subprocess.check_call([\'mpirun\', \'-np\', \'3\', \n        \'python\',\'-c\', \n        \'from vendor.openai.baselines.common.mpi_moments import _helper_runningmeanstd; _helper_runningmeanstd()\'])\n\ndef _helper_runningmeanstd():\n    comm = MPI.COMM_WORLD\n    np.random.seed(0)\n    for (triple,axis) in [\n        ((np.random.randn(3), np.random.randn(4), np.random.randn(5)),0),\n        ((np.random.randn(3,2), np.random.randn(4,2), np.random.randn(5,2)),0),\n        ((np.random.randn(2,3), np.random.randn(2,4), np.random.randn(2,4)),1),\n        ]:\n\n\n        x = np.concatenate(triple, axis=axis)\n        ms1 = [x.mean(axis=axis), x.std(axis=axis), x.shape[axis]]\n\n\n        ms2 = mpi_moments(triple[comm.Get_rank()],axis=axis)\n\n        for (a1,a2) in zipsame(ms1, ms2):\n            print(a1, a2)\n            assert np.allclose(a1, a2)\n            print(""ok!"")\n\n'"
vendor/openai/baselines/common/mpi_running_mean_std.py,17,"b'from mpi4py import MPI\nimport tensorflow as tf, baselines.common.tf_util as U, numpy as np\n\nclass RunningMeanStd(object):\n    # https://en.wikipedia.org/wiki/Algorithms_for_calculating_variance#Parallel_algorithm\n    def __init__(self, epsilon=1e-2, shape=()):\n\n        self._sum = tf.get_variable(\n            dtype=tf.float64,\n            shape=shape,\n            initializer=tf.constant_initializer(0.0),\n            name=""runningsum"", trainable=False)\n        self._sumsq = tf.get_variable(\n            dtype=tf.float64,\n            shape=shape,\n            initializer=tf.constant_initializer(epsilon),\n            name=""runningsumsq"", trainable=False)\n        self._count = tf.get_variable(\n            dtype=tf.float64,\n            shape=(),\n            initializer=tf.constant_initializer(epsilon),\n            name=""count"", trainable=False)\n        self.shape = shape\n\n        self.mean = tf.to_float(self._sum / self._count)\n        self.std = tf.sqrt( tf.maximum( tf.to_float(self._sumsq / self._count) - tf.square(self.mean) , 1e-2 ))\n\n        newsum = tf.placeholder(shape=self.shape, dtype=tf.float64, name=\'sum\')\n        newsumsq = tf.placeholder(shape=self.shape, dtype=tf.float64, name=\'var\')\n        newcount = tf.placeholder(shape=[], dtype=tf.float64, name=\'count\')\n        self.incfiltparams = U.function([newsum, newsumsq, newcount], [],\n            updates=[tf.assign_add(self._sum, newsum),\n                     tf.assign_add(self._sumsq, newsumsq),\n                     tf.assign_add(self._count, newcount)])\n\n\n    def update(self, x):\n        x = x.astype(\'float64\')\n        n = int(np.prod(self.shape))\n        totalvec = np.zeros(n*2+1, \'float64\')\n        addvec = np.concatenate([x.sum(axis=0).ravel(), np.square(x).sum(axis=0).ravel(), np.array([len(x)],dtype=\'float64\')])\n        MPI.COMM_WORLD.Allreduce(addvec, totalvec, op=MPI.SUM)\n        self.incfiltparams(totalvec[0:n].reshape(self.shape), totalvec[n:2*n].reshape(self.shape), totalvec[2*n])\n\n@U.in_session\ndef test_runningmeanstd():\n    for (x1, x2, x3) in [\n        (np.random.randn(3), np.random.randn(4), np.random.randn(5)),\n        (np.random.randn(3,2), np.random.randn(4,2), np.random.randn(5,2)),\n        ]:\n\n        rms = RunningMeanStd(epsilon=0.0, shape=x1.shape[1:])\n        U.initialize()\n\n        x = np.concatenate([x1, x2, x3], axis=0)\n        ms1 = [x.mean(axis=0), x.std(axis=0)]\n        rms.update(x1)\n        rms.update(x2)\n        rms.update(x3)\n        ms2 = [rms.mean.eval(), rms.std.eval()]\n\n        assert np.allclose(ms1, ms2)\n\n@U.in_session\ndef test_dist():\n    np.random.seed(0)\n    p1,p2,p3=(np.random.randn(3,1), np.random.randn(4,1), np.random.randn(5,1))\n    q1,q2,q3=(np.random.randn(6,1), np.random.randn(7,1), np.random.randn(8,1))\n\n    # p1,p2,p3=(np.random.randn(3), np.random.randn(4), np.random.randn(5))\n    # q1,q2,q3=(np.random.randn(6), np.random.randn(7), np.random.randn(8))\n\n    comm = MPI.COMM_WORLD\n    assert comm.Get_size()==2\n    if comm.Get_rank()==0:\n        x1,x2,x3 = p1,p2,p3\n    elif comm.Get_rank()==1:\n        x1,x2,x3 = q1,q2,q3\n    else:\n        assert False\n\n    rms = RunningMeanStd(epsilon=0.0, shape=(1,))\n    U.initialize()\n\n    rms.update(x1)\n    rms.update(x2)\n    rms.update(x3)\n\n    bigvec = np.concatenate([p1,p2,p3,q1,q2,q3])\n\n    def checkallclose(x,y):\n        print(x,y)\n        return np.allclose(x,y)\n\n    assert checkallclose(\n        bigvec.mean(axis=0),\n        rms.mean.eval(),\n    )\n    assert checkallclose(\n        bigvec.std(axis=0),\n        rms.std.eval(),\n    )\n\n\nif __name__ == ""__main__"":\n    # Run with mpirun -np 2 python <filename>\n    test_dist()\n'"
vendor/openai/baselines/common/running_mean_std.py,0,"b""import numpy as np\nclass RunningMeanStd(object):\n    # https://en.wikipedia.org/wiki/Algorithms_for_calculating_variance#Parallel_algorithm\n    def __init__(self, epsilon=1e-4, shape=()):\n        self.mean = np.zeros(shape, 'float64')\n        self.var = np.ones(shape, 'float64')\n        self.count = epsilon\n\n    def update(self, x):\n        batch_mean = np.mean(x, axis=0)\n        batch_var = np.var(x, axis=0)\n        batch_count = x.shape[0]\n        self.update_from_moments(batch_mean, batch_var, batch_count)\n\n    def update_from_moments(self, batch_mean, batch_var, batch_count):\n        delta = batch_mean - self.mean\n        tot_count = self.count + batch_count\n\n        new_mean = self.mean + delta * batch_count / tot_count        \n        m_a = self.var * (self.count)\n        m_b = batch_var * (batch_count)\n        M2 = m_a + m_b + np.square(delta) * self.count * batch_count / (self.count + batch_count)\n        new_var = M2 / (self.count + batch_count)\n\n        new_count = batch_count + self.count\n\n        self.mean = new_mean\n        self.var = new_var\n        self.count = new_count    \n\ndef test_runningmeanstd():\n    for (x1, x2, x3) in [\n        (np.random.randn(3), np.random.randn(4), np.random.randn(5)),\n        (np.random.randn(3,2), np.random.randn(4,2), np.random.randn(5,2)),\n        ]:\n\n        rms = RunningMeanStd(epsilon=0.0, shape=x1.shape[1:])\n\n        x = np.concatenate([x1, x2, x3], axis=0)\n        ms1 = [x.mean(axis=0), x.var(axis=0)]\n        rms.update(x1)\n        rms.update(x2)\n        rms.update(x3)\n        ms2 = [rms.mean, rms.var]\n\n        assert np.allclose(ms1, ms2)\n"""
vendor/openai/baselines/common/schedules.py,0,"b'""""""This file is used for specifying various schedules that evolve over\ntime throughout the execution of the algorithm, such as:\n - learning rate for the optimizer\n - exploration epsilon for the epsilon greedy exploration strategy\n - beta parameter for beta parameter in prioritized replay\n\nEach schedule has a function `value(t)` which returns the current value\nof the parameter given the timestep t of the optimization procedure.\n""""""\n\n\nclass Schedule(object):\n    def value(self, t):\n        """"""Value of the schedule at time t""""""\n        raise NotImplementedError()\n\n\nclass ConstantSchedule(object):\n    def __init__(self, value):\n        """"""Value remains constant over time.\n\n        Parameters\n        ----------\n        value: float\n            Constant value of the schedule\n        """"""\n        self._v = value\n\n    def value(self, t):\n        """"""See Schedule.value""""""\n        return self._v\n\n\ndef linear_interpolation(l, r, alpha):\n    return l + alpha * (r - l)\n\n\nclass PiecewiseSchedule(object):\n    def __init__(self, endpoints, interpolation=linear_interpolation, outside_value=None):\n        """"""Piecewise schedule.\n\n        endpoints: [(int, int)]\n            list of pairs `(time, value)` meanining that schedule should output\n            `value` when `t==time`. All the values for time must be sorted in\n            an increasing order. When t is between two times, e.g. `(time_a, value_a)`\n            and `(time_b, value_b)`, such that `time_a <= t < time_b` then value outputs\n            `interpolation(value_a, value_b, alpha)` where alpha is a fraction of\n            time passed between `time_a` and `time_b` for time `t`.\n        interpolation: lambda float, float, float: float\n            a function that takes value to the left and to the right of t according\n            to the `endpoints`. Alpha is the fraction of distance from left endpoint to\n            right endpoint that t has covered. See linear_interpolation for example.\n        outside_value: float\n            if the value is requested outside of all the intervals sepecified in\n            `endpoints` this value is returned. If None then AssertionError is\n            raised when outside value is requested.\n        """"""\n        idxes = [e[0] for e in endpoints]\n        assert idxes == sorted(idxes)\n        self._interpolation = interpolation\n        self._outside_value = outside_value\n        self._endpoints = endpoints\n\n    def value(self, t):\n        """"""See Schedule.value""""""\n        for (l_t, l), (r_t, r) in zip(self._endpoints[:-1], self._endpoints[1:]):\n            if l_t <= t and t < r_t:\n                alpha = float(t - l_t) / (r_t - l_t)\n                return self._interpolation(l, r, alpha)\n\n        # t does not belong to any of the pieces, so doom.\n        assert self._outside_value is not None\n        return self._outside_value\n\n\nclass LinearSchedule(object):\n    def __init__(self, schedule_timesteps, final_p, initial_p=1.0):\n        """"""Linear interpolation between initial_p and final_p over\n        schedule_timesteps. After this many timesteps pass final_p is\n        returned.\n\n        Parameters\n        ----------\n        schedule_timesteps: int\n            Number of timesteps for which to linearly anneal initial_p\n            to final_p\n        initial_p: float\n            initial output value\n        final_p: float\n            final output value\n        """"""\n        self.schedule_timesteps = schedule_timesteps\n        self.final_p = final_p\n        self.initial_p = initial_p\n\n    def value(self, t):\n        """"""See Schedule.value""""""\n        fraction = min(float(t) / self.schedule_timesteps, 1.0)\n        return self.initial_p + fraction * (self.final_p - self.initial_p)\n'"
vendor/openai/baselines/common/segment_tree.py,0,"b'import operator\n\n\nclass SegmentTree(object):\n    def __init__(self, capacity, operation, neutral_element):\n        """"""Build a Segment Tree data structure.\n\n        https://en.wikipedia.org/wiki/Segment_tree\n\n        Can be used as regular array, but with two\n        important differences:\n\n            a) setting item\'s value is slightly slower.\n               It is O(lg capacity) instead of O(1).\n            b) user has access to an efficient `reduce`\n               operation which reduces `operation` over\n               a contiguous subsequence of items in the\n               array.\n\n        Paramters\n        ---------\n        capacity: int\n            Total size of the array - must be a power of two.\n        operation: lambda obj, obj -> obj\n            and operation for combining elements (eg. sum, max)\n            must for a mathematical group together with the set of\n            possible values for array elements.\n        neutral_element: obj\n            neutral element for the operation above. eg. float(\'-inf\')\n            for max and 0 for sum.\n        """"""\n        assert capacity > 0 and capacity & (capacity - 1) == 0, ""capacity must be positive and a power of 2.""\n        self._capacity = capacity\n        self._value = [neutral_element for _ in range(2 * capacity)]\n        self._operation = operation\n\n    def _reduce_helper(self, start, end, node, node_start, node_end):\n        if start == node_start and end == node_end:\n            return self._value[node]\n        mid = (node_start + node_end) // 2\n        if end <= mid:\n            return self._reduce_helper(start, end, 2 * node, node_start, mid)\n        else:\n            if mid + 1 <= start:\n                return self._reduce_helper(start, end, 2 * node + 1, mid + 1, node_end)\n            else:\n                return self._operation(\n                    self._reduce_helper(start, mid, 2 * node, node_start, mid),\n                    self._reduce_helper(mid + 1, end, 2 * node + 1, mid + 1, node_end)\n                )\n\n    def reduce(self, start=0, end=None):\n        """"""Returns result of applying `self.operation`\n        to a contiguous subsequence of the array.\n\n            self.operation(arr[start], operation(arr[start+1], operation(... arr[end])))\n\n        Parameters\n        ----------\n        start: int\n            beginning of the subsequence\n        end: int\n            end of the subsequences\n\n        Returns\n        -------\n        reduced: obj\n            result of reducing self.operation over the specified range of array elements.\n        """"""\n        if end is None:\n            end = self._capacity\n        if end < 0:\n            end += self._capacity\n        end -= 1\n        return self._reduce_helper(start, end, 1, 0, self._capacity - 1)\n\n    def __setitem__(self, idx, val):\n        # index of the leaf\n        idx += self._capacity\n        self._value[idx] = val\n        idx //= 2\n        while idx >= 1:\n            self._value[idx] = self._operation(\n                self._value[2 * idx],\n                self._value[2 * idx + 1]\n            )\n            idx //= 2\n\n    def __getitem__(self, idx):\n        assert 0 <= idx < self._capacity\n        return self._value[self._capacity + idx]\n\n\nclass SumSegmentTree(SegmentTree):\n    def __init__(self, capacity):\n        super(SumSegmentTree, self).__init__(\n            capacity=capacity,\n            operation=operator.add,\n            neutral_element=0.0\n        )\n\n    def sum(self, start=0, end=None):\n        """"""Returns arr[start] + ... + arr[end]""""""\n        return super(SumSegmentTree, self).reduce(start, end)\n\n    def find_prefixsum_idx(self, prefixsum):\n        """"""Find the highest index `i` in the array such that\n            sum(arr[0] + arr[1] + ... + arr[i - i]) <= prefixsum\n\n        if array values are probabilities, this function\n        allows to sample indexes according to the discrete\n        probability efficiently.\n\n        Parameters\n        ----------\n        perfixsum: float\n            upperbound on the sum of array prefix\n\n        Returns\n        -------\n        idx: int\n            highest index satisfying the prefixsum constraint\n        """"""\n        assert 0 <= prefixsum <= self.sum() + 1e-5\n        idx = 1\n        while idx < self._capacity:  # while non-leaf\n            if self._value[2 * idx] > prefixsum:\n                idx = 2 * idx\n            else:\n                prefixsum -= self._value[2 * idx]\n                idx = 2 * idx + 1\n        return idx - self._capacity\n\n\nclass MinSegmentTree(SegmentTree):\n    def __init__(self, capacity):\n        super(MinSegmentTree, self).__init__(\n            capacity=capacity,\n            operation=min,\n            neutral_element=float(\'inf\')\n        )\n\n    def min(self, start=0, end=None):\n        """"""Returns min(arr[start], ...,  arr[end])""""""\n\n        return super(MinSegmentTree, self).reduce(start, end)\n'"
vendor/openai/baselines/common/tf_util.py,39,"b'import numpy as np\nimport tensorflow as tf  # pylint: ignore-module\nimport copy\nimport os\nimport functools\nimport collections\nimport multiprocessing\n\ndef switch(condition, then_expression, else_expression):\n    """"""Switches between two operations depending on a scalar value (int or bool).\n    Note that both `then_expression` and `else_expression`\n    should be symbolic tensors of the *same shape*.\n\n    # Arguments\n        condition: scalar tensor.\n        then_expression: TensorFlow operation.\n        else_expression: TensorFlow operation.\n    """"""\n    x_shape = copy.copy(then_expression.get_shape())\n    x = tf.cond(tf.cast(condition, \'bool\'),\n                lambda: then_expression,\n                lambda: else_expression)\n    x.set_shape(x_shape)\n    return x\n\n# ================================================================\n# Extras\n# ================================================================\n\ndef lrelu(x, leak=0.2):\n    f1 = 0.5 * (1 + leak)\n    f2 = 0.5 * (1 - leak)\n    return f1 * x + f2 * abs(x)\n\n# ================================================================\n# Mathematical utils\n# ================================================================\n\ndef huber_loss(x, delta=1.0):\n    """"""Reference: https://en.wikipedia.org/wiki/Huber_loss""""""\n    return tf.where(\n        tf.abs(x) < delta,\n        tf.square(x) * 0.5,\n        delta * (tf.abs(x) - 0.5 * delta)\n    )\n\n# ================================================================\n# Global session\n# ================================================================\n\ndef make_session(num_cpu=None, make_default=False):\n    """"""Returns a session that will use <num_cpu> CPU\'s only""""""\n    if num_cpu is None:\n        num_cpu = int(os.getenv(\'RCALL_NUM_CPU\', multiprocessing.cpu_count()))\n    tf_config = tf.ConfigProto(\n        inter_op_parallelism_threads=num_cpu,\n        intra_op_parallelism_threads=num_cpu)\n    tf_config.gpu_options.allocator_type = \'BFC\'\n    if make_default:\n        return tf.InteractiveSession(config=tf_config)\n    else:\n        return tf.Session(config=tf_config)\n\ndef single_threaded_session():\n    """"""Returns a session which will only use a single CPU""""""\n    return make_session(num_cpu=1)\n\ndef in_session(f):\n    @functools.wraps(f)\n    def newfunc(*args, **kwargs):\n        with tf.Session():\n            f(*args, **kwargs)\n    return newfunc\n\nALREADY_INITIALIZED = set()\n\ndef initialize():\n    """"""Initialize all the uninitialized variables in the global scope.""""""\n    new_variables = set(tf.global_variables()) - ALREADY_INITIALIZED\n    tf.get_default_session().run(tf.variables_initializer(new_variables))\n    ALREADY_INITIALIZED.update(new_variables)\n\n# ================================================================\n# Model components\n# ================================================================\n\ndef normc_initializer(std=1.0):\n    def _initializer(shape, dtype=None, partition_info=None):  # pylint: disable=W0613\n        out = np.random.randn(*shape).astype(np.float32)\n        out *= std / np.sqrt(np.square(out).sum(axis=0, keepdims=True))\n        return tf.constant(out)\n    return _initializer\n\ndef conv2d(x, num_filters, name, filter_size=(3, 3), stride=(1, 1), pad=""SAME"", dtype=tf.float32, collections=None,\n           summary_tag=None):\n    with tf.variable_scope(name):\n        stride_shape = [1, stride[0], stride[1], 1]\n        filter_shape = [filter_size[0], filter_size[1], int(x.get_shape()[3]), num_filters]\n\n        # there are ""num input feature maps * filter height * filter width""\n        # inputs to each hidden unit\n        fan_in = intprod(filter_shape[:3])\n        # each unit in the lower layer receives a gradient from:\n        # ""num output feature maps * filter height * filter width"" /\n        #   pooling size\n        fan_out = intprod(filter_shape[:2]) * num_filters\n        # initialize weights with random weights\n        w_bound = np.sqrt(6. / (fan_in + fan_out))\n\n        w = tf.get_variable(""W"", filter_shape, dtype, tf.random_uniform_initializer(-w_bound, w_bound),\n                            collections=collections)\n        b = tf.get_variable(""b"", [1, 1, 1, num_filters], initializer=tf.zeros_initializer(),\n                            collections=collections)\n\n        if summary_tag is not None:\n            tf.summary.image(summary_tag,\n                             tf.transpose(tf.reshape(w, [filter_size[0], filter_size[1], -1, 1]),\n                                          [2, 0, 1, 3]),\n                             max_images=10)\n\n        return tf.nn.conv2d(x, w, stride_shape, pad) + b\n\n# ================================================================\n# Theano-like Function\n# ================================================================\n\ndef function(inputs, outputs, updates=None, givens=None):\n    """"""Just like Theano function. Take a bunch of tensorflow placeholders and expressions\n    computed based on those placeholders and produces f(inputs) -> outputs. Function f takes\n    values to be fed to the input\'s placeholders and produces the values of the expressions\n    in outputs.\n\n    Input values can be passed in the same order as inputs or can be provided as kwargs based\n    on placeholder name (passed to constructor or accessible via placeholder.op.name).\n\n    Example:\n        x = tf.placeholder(tf.int32, (), name=""x"")\n        y = tf.placeholder(tf.int32, (), name=""y"")\n        z = 3 * x + 2 * y\n        lin = function([x, y], z, givens={y: 0})\n\n        with single_threaded_session():\n            initialize()\n\n            assert lin(2) == 6\n            assert lin(x=3) == 9\n            assert lin(2, 2) == 10\n            assert lin(x=2, y=3) == 12\n\n    Parameters\n    ----------\n    inputs: [tf.placeholder, tf.constant, or object with make_feed_dict method]\n        list of input arguments\n    outputs: [tf.Variable] or tf.Variable\n        list of outputs or a single output to be returned from function. Returned\n        value will also have the same shape.\n    """"""\n    if isinstance(outputs, list):\n        return _Function(inputs, outputs, updates, givens=givens)\n    elif isinstance(outputs, (dict, collections.OrderedDict)):\n        f = _Function(inputs, outputs.values(), updates, givens=givens)\n        return lambda *args, **kwargs: type(outputs)(zip(outputs.keys(),\n                                                         f(*args, **kwargs)))\n    else:\n        f = _Function(inputs, [outputs], updates, givens=givens)\n        return lambda *args, **kwargs: f(*args, **kwargs)[0]\n\n\nclass _Function(object):\n    def __init__(self, inputs, outputs, updates, givens):\n        for inpt in inputs:\n            if not hasattr(inpt, \'make_feed_dict\') and not (type(inpt) is tf.Tensor and len(inpt.op.inputs) == 0):\n                assert False, ""inputs should all be placeholders, constants, or have a make_feed_dict method""\n        self.inputs = inputs\n        updates = updates or []\n        self.update_group = tf.group(*updates)\n        self.outputs_update = list(outputs) + [self.update_group]\n        self.givens = {} if givens is None else givens\n\n    def _feed_input(self, feed_dict, inpt, value):\n        if hasattr(inpt, \'make_feed_dict\'):\n            feed_dict.update(inpt.make_feed_dict(value))\n        else:\n            feed_dict[inpt] = value\n\n    def __call__(self, *args):\n        assert len(args) <= len(self.inputs), ""Too many arguments provided""\n        feed_dict = {}\n        # Update the args\n        for inpt, value in zip(self.inputs, args):\n            self._feed_input(feed_dict, inpt, value)\n        # Update feed dict with givens.\n        for inpt in self.givens:\n            feed_dict[inpt] = feed_dict.get(inpt, self.givens[inpt])\n        results = tf.get_default_session().run(self.outputs_update, feed_dict=feed_dict)[:-1]\n        return results\n\n# ================================================================\n# Flat vectors\n# ================================================================\n\ndef var_shape(x):\n    out = x.get_shape().as_list()\n    assert all(isinstance(a, int) for a in out), \\\n        ""shape function assumes that shape is fully known""\n    return out\n\ndef numel(x):\n    return intprod(var_shape(x))\n\ndef intprod(x):\n    return int(np.prod(x))\n\ndef flatgrad(loss, var_list, clip_norm=None):\n    grads = tf.gradients(loss, var_list)\n    if clip_norm is not None:\n        grads = [tf.clip_by_norm(grad, clip_norm=clip_norm) for grad in grads]\n    return tf.concat(axis=0, values=[\n        tf.reshape(grad if grad is not None else tf.zeros_like(v), [numel(v)])\n        for (v, grad) in zip(var_list, grads)\n    ])\n\nclass SetFromFlat(object):\n    def __init__(self, var_list, dtype=tf.float32):\n        assigns = []\n        shapes = list(map(var_shape, var_list))\n        total_size = np.sum([intprod(shape) for shape in shapes])\n\n        self.theta = theta = tf.placeholder(dtype, [total_size])\n        start = 0\n        assigns = []\n        for (shape, v) in zip(shapes, var_list):\n            size = intprod(shape)\n            assigns.append(tf.assign(v, tf.reshape(theta[start:start + size], shape)))\n            start += size\n        self.op = tf.group(*assigns)\n\n    def __call__(self, theta):\n        tf.get_default_session().run(self.op, feed_dict={self.theta: theta})\n\nclass GetFlat(object):\n    def __init__(self, var_list):\n        self.op = tf.concat(axis=0, values=[tf.reshape(v, [numel(v)]) for v in var_list])\n\n    def __call__(self):\n        return tf.get_default_session().run(self.op)\n\n_PLACEHOLDER_CACHE = {}  # name -> (placeholder, dtype, shape)\n\ndef get_placeholder(name, dtype, shape):\n    if name in _PLACEHOLDER_CACHE:\n        out, dtype1, shape1 = _PLACEHOLDER_CACHE[name]\n        assert dtype1 == dtype and shape1 == shape\n        return out\n    else:\n        out = tf.placeholder(dtype=dtype, shape=shape, name=name)\n        _PLACEHOLDER_CACHE[name] = (out, dtype, shape)\n        return out\n\ndef get_placeholder_cached(name):\n    return _PLACEHOLDER_CACHE[name][0]\n\ndef flattenallbut0(x):\n    return tf.reshape(x, [-1, intprod(x.get_shape().as_list()[1:])])\n\n\n# ================================================================\n# Diagnostics \n# ================================================================\n\ndef display_var_info(vars):\n    from vendor.openai.baselines import logger\n    count_params = 0\n    for v in vars:\n        name = v.name\n        if ""/Adam"" in name or ""beta1_power"" in name or ""beta2_power"" in name: continue\n        count_params += np.prod(v.shape.as_list())\n        if ""/b:"" in name: continue    # Wx+b, bias is not interesting to look at => count params, but not print\n        logger.info(""    %s%s%s"" % (name, "" ""*(55-len(name)), str(v.shape)))\n    logger.info(""Total model parameters: %0.1f million"" % (count_params*1e-6))\n\n'"
vendor/openai/baselines/ppo2/__init__.py,0,b''
vendor/openai/baselines/ppo2/policies.py,29,"b'import numpy as np\nimport tensorflow as tf\nfrom vendor.openai.baselines.common.distributions import make_pdtype\nfrom gym import spaces\nimport config as c\n\nfrom vendor.openai.baselines.a2c.utils import conv, fc, conv_to_fc, batch_to_seq, seq_to_batch, lstm, lnlstm\nfrom vendor.openai.baselines.ppo2.ppo2 import TF_VAR_SCOPE\n\n\ndef nature_cnn(unscaled_images):\n    """"""\n    CNN from Nature paper.\n    """"""\n    scaled_images = tf.cast(unscaled_images, tf.float32) / 255.\n    activ = tf.nn.relu\n    h = activ(conv(scaled_images, \'c1\', nf=32, rf=8, stride=4, init_scale=np.sqrt(2)))\n    h2 = activ(conv(h, \'c2\', nf=64, rf=4, stride=2, init_scale=np.sqrt(2)))\n    h3 = activ(conv(h2, \'c3\', nf=64, rf=3, stride=1, init_scale=np.sqrt(2)))\n    h3 = conv_to_fc(h3)\n    return activ(fc(h3, \'fc1\', nh=512, init_scale=np.sqrt(2)))\n\nclass LnLstmPolicy(object):\n    def __init__(self, sess, ob_space, ac_space, nbatch, nsteps, nlstm=256, reuse=False):\n        nenv = nbatch // nsteps\n        nh, nw, nc = ob_space.shape\n        ob_shape = (nbatch, nh, nw, nc)\n        nact = ac_space.n\n        X = tf.placeholder(tf.uint8, ob_shape) #obs\n        M = tf.placeholder(tf.float32, [nbatch]) #mask (done t-1)\n        S = tf.placeholder(tf.float32, [nenv, nlstm*2]) #states\n        with tf.variable_scope(TF_VAR_SCOPE, reuse=reuse):\n            h = nature_cnn(X)\n            xs = batch_to_seq(h, nenv, nsteps)\n            ms = batch_to_seq(M, nenv, nsteps)\n            h5, snew = lnlstm(xs, ms, S, \'lstm1\', nh=nlstm)\n            h5 = seq_to_batch(h5)\n            pi = fc(h5, \'pi\', nact)\n            vf = fc(h5, \'v\', 1)\n\n        self.pdtype = make_pdtype(ac_space)\n        self.pd = self.pdtype.pdfromflat(pi)\n\n        v0 = vf[:, 0]\n        a0 = self.pd.sample()\n        neglogp0 = self.pd.neglogp(a0)\n        self.initial_state = np.zeros((nenv, nlstm*2), dtype=np.float32)\n\n        def step(ob, state, mask):\n            return sess.run([a0, v0, snew, neglogp0], {X:ob, S:state, M:mask})\n\n        def value(ob, state, mask):\n            return sess.run(v0, {X:ob, S:state, M:mask})\n\n        self.X = X\n        self.M = M\n        self.S = S\n        self.pi = pi\n        self.vf = vf\n        self.step = step\n        self.value = value\n\n\nclass LstmPolicy(object):\n\n    def __init__(self, sess, ob_space, ac_space, nbatch, nsteps, nlstm=256, reuse=False):\n        nenv = nbatch // nsteps\n\n        nh, nw, nc = ob_space.shape\n        ob_shape = (nbatch, nh, nw, nc)\n        nact = ac_space.n\n        X = tf.placeholder(tf.uint8, ob_shape) #obs\n        M = tf.placeholder(tf.float32, [nbatch]) #mask (done t-1)\n        S = tf.placeholder(tf.float32, [nenv, nlstm*2]) #states\n        with tf.variable_scope(TF_VAR_SCOPE, reuse=reuse):\n            h = nature_cnn(X)\n            xs = batch_to_seq(h, nenv, nsteps)\n            ms = batch_to_seq(M, nenv, nsteps)\n            h5, snew = lstm(xs, ms, S, \'lstm1\', nh=nlstm)\n            h5 = seq_to_batch(h5)\n            pi = fc(h5, \'pi\', nact)\n            vf = fc(h5, \'v\', 1)\n\n        self.pdtype = make_pdtype(ac_space)\n        self.pd = self.pdtype.pdfromflat(pi)\n\n        v0 = vf[:, 0]\n        a0 = self.pd.sample()\n        neglogp0 = self.pd.neglogp(a0)\n        self.initial_state = np.zeros((nenv, nlstm*2), dtype=np.float32)\n\n        def step(ob, state, mask):\n            return sess.run([a0, v0, snew, neglogp0], {X:ob, S:state, M:mask})\n\n        def value(ob, state, mask):\n            return sess.run(v0, {X:ob, S:state, M:mask})\n\n        self.X = X\n        self.M = M\n        self.S = S\n        self.pi = pi\n        self.vf = vf\n        self.step = step\n        self.value = value\n\n\nclass LstmPolicyFlat(object):\n\n    def __init__(self, sess, ob_space, ac_space, nbatch, nsteps, nlstm=256, reuse=False):\n        nenv = nbatch // nsteps\n        ob_shape = (nbatch, ob_space.shape[0])\n        if type(ac_space) is spaces.Box:\n            nact = ac_space.shape[0]\n        else:\n            nact = ac_space.n\n        X = tf.placeholder(tf.uint8, ob_shape) #obs\n        M = tf.placeholder(tf.float32, [nbatch]) #mask (done t-1)\n        S = tf.placeholder(tf.float32, [nenv, nlstm*2]) #states\n        with tf.variable_scope(TF_VAR_SCOPE, reuse=reuse):\n            h = tf.cast(X, tf.float32)\n            xs = batch_to_seq(h, nenv, nsteps)\n            ms = batch_to_seq(M, nenv, nsteps)\n            h5, snew = lstm(xs, ms, S, \'lstm1\', nh=nlstm)\n            h5 = seq_to_batch(h5)\n            pi = fc(h5, \'pi\', nact)\n            vf = fc(h5, \'v\', 1)\n            if type(ac_space) is spaces.Box:\n                logstd = tf.get_variable(name=""logstd"", shape=[1, nact],\n                                         initializer=tf.zeros_initializer())\n            else:\n                logstd = None\n\n        if logstd is not None:\n            pdparam = tf.concat([pi, pi * 0.0 + logstd], axis=1)\n        else:\n            pdparam = pi\n        self.pdtype = make_pdtype(ac_space)\n        self.pd = self.pdtype.pdfromflat(pdparam)\n\n        v0 = vf[:, 0]\n        a0 = self.pd.sample()\n        neglogp0 = self.pd.neglogp(a0)\n        self.initial_state = np.zeros((nenv, nlstm*2), dtype=np.float32)\n\n        def step(ob, state, mask):\n            return sess.run([a0, v0, snew, neglogp0], {X:ob, S:state, M:mask})\n\n        def value(ob, state, mask):\n            return sess.run(v0, {X:ob, S:state, M:mask})\n\n        self.X = X\n        self.M = M\n        self.S = S\n        self.pi = pi\n        self.vf = vf\n        self.step = step\n        self.value = value\n\n\nclass CnnPolicy(object):\n\n    def __init__(self, sess, ob_space, ac_space, nbatch, nsteps, reuse=False): #pylint: disable=W0613\n        nh, nw, nc = ob_space.shape\n        ob_shape = (nbatch, nh, nw, nc)\n        nact = ac_space.n\n        X = tf.placeholder(tf.uint8, ob_shape) #obs\n        with tf.variable_scope(TF_VAR_SCOPE, reuse=reuse):\n            h = nature_cnn(X)\n            pi = fc(h, \'pi\', nact, init_scale=0.01)\n            vf = fc(h, \'v\', 1)[:,0]\n\n        self.pdtype = make_pdtype(ac_space)\n        self.pd = self.pdtype.pdfromflat(pi)\n\n        a0 = self.pd.sample()\n        neglogp0 = self.pd.neglogp(a0)\n        self.initial_state = None\n\n        def step(ob, *_args, **_kwargs):\n            a, v, neglogp = sess.run([a0, vf, neglogp0], {X:ob})\n            return a, v, self.initial_state, neglogp\n\n        def value(ob, *_args, **_kwargs):\n            return sess.run(vf, {X:ob})\n\n        self.X = X\n        self.pi = pi\n        self.vf = vf\n        self.step = step\n        self.value = value\n\n\nclass MlpPolicy(object):\n\n    def __init__(self, sess, ob_space, ac_space, nbatch, nsteps, reuse=False, **kwargs): #pylint: disable=W0613\n        self.width = kwargs[\'mlp_width\']\n        ob_shape = (nbatch,) + ob_space.shape\n        actdim = ac_space.shape[0]\n        X = tf.placeholder(tf.float32, ob_shape, name=\'Ob\') #obs\n        with tf.variable_scope(TF_VAR_SCOPE, reuse=reuse):\n            activ = tf.tanh  # Diverges even at super low learning rates\n            # activ = tf.nn.relu\n            # activ = tf.nn.leaky_relu  # Diverges\n            p_h1 = activ(fc(X, \'pi_fc1\', nh=self.width, init_scale=np.sqrt(2)))\n            p_h2 = activ(fc(p_h1, \'pi_fc2\', nh=self.width, init_scale=np.sqrt(2)))\n            pi = fc(p_h2, \'pi\', actdim, init_scale=0.01)\n            v_h1 = activ(fc(X, \'vf_fc1\', nh=self.width, init_scale=np.sqrt(2)))\n            v_h2 = activ(fc(v_h1, \'vf_fc2\', nh=self.width, init_scale=np.sqrt(2)))\n            vf = fc(v_h2, \'vf\', 1)[:, 0]\n            logstd = tf.get_variable(name=""logstd"", shape=[1, actdim],\n                                     initializer=tf.zeros_initializer())\n\n        pdparam = tf.concat([pi, pi * 0.0 + logstd], axis=1)\n\n        self.p_h1 = p_h1\n        self.pdtype = make_pdtype(ac_space)\n        self.pd = self.pdtype.pdfromflat(pdparam)\n\n\n        # One idea to benefit from SL learned actions would be to average the SL action with the one drawn here, where\n        # the weight of the SL action decays over time.\n        # That way you are still optimizing with respect to the parameters, but there is a nudge towards taking the\n        # correct action which helps narrow down the exploration space. As you decay the SL weight, you offer a chance\n        # to learn actions better than SL. This needs to be balanced with decaying other hyperparams like lr, etc... tho.\n        a0 = self.pd.sample()\n\n        neglogp0 = self.pd.neglogp(a0)\n        action_probs0 = tf.exp(-neglogp0)\n\n        self.initial_state = None\n\n        def step(ob, *_args, **_kwargs):\n            a, v, neglogp, action_probs = sess.run([a0, vf, neglogp0, action_probs0], {X: ob})\n\n            # For deepdrive we expect outputs to be between -1 and 1\n            # a = np.tanh(a)  # Doesn\'t work very well, clipping and letting the network learn the valid range is prob better\n\n            return a, v, self.initial_state, neglogp, action_probs\n\n        def value(ob, *_args, **_kwargs):\n            return sess.run(vf, {X: ob})\n\n        self.X = X\n        self.pi = pi\n        self.action_probs0 = action_probs0\n        self.vf = vf\n        self.step = step\n        self.value = value\n'"
vendor/openai/baselines/ppo2/ppo2.py,23,"b'import math\nimport os\nimport os.path as osp\nimport time\nfrom collections import deque\n\nimport joblib\nimport numpy as np\n# noinspection PyPackageRequirements\nimport tensorflow as tf\n\nfrom agents.common import get_throttle\nfrom sim.action import Action\nfrom vendor.openai.baselines import logger\n\nfrom vendor.openai.baselines.common.math_util import explained_variance\n\nimport config as c\n\nTF_VAR_SCOPE = \'ppo2model\'\n\n\nclass Model(object):\n    def __init__(self, *, policy, ob_space, ac_space, nbatch_act, nbatch_train,\n                nsteps, ent_coef, vf_coef, max_grad_norm, **kwargs):\n        sess = tf.get_default_session()\n\n        act_model = policy(sess, ob_space, ac_space, nbatch_act, 1, reuse=False, **kwargs)\n        train_model = policy(sess, ob_space, ac_space, nbatch_train, nsteps, reuse=True, **kwargs)\n\n        A = train_model.pdtype.sample_placeholder([None])\n        ADV = tf.placeholder(tf.float32, [None])\n        R = tf.placeholder(tf.float32, [None])\n        OLDNEGLOGPAC = tf.placeholder(tf.float32, [None])\n        OLDVPRED = tf.placeholder(tf.float32, [None])\n        LR = tf.placeholder(tf.float32, [])\n        CLIPRANGE = tf.placeholder(tf.float32, [])\n\n        neglogpac = train_model.pd.neglogp(A)\n        entropy = tf.reduce_mean(train_model.pd.entropy())\n\n        vpred = train_model.vf\n        vpredclipped = OLDVPRED + tf.clip_by_value(train_model.vf - OLDVPRED, - CLIPRANGE, CLIPRANGE)\n        vf_losses1 = tf.square(vpred - R)\n        vf_losses2 = tf.square(vpredclipped - R)\n        vf_loss = .5 * tf.reduce_mean(tf.maximum(vf_losses1, vf_losses2))\n        ratio = tf.exp(OLDNEGLOGPAC - neglogpac)\n        pg_losses = -ADV * ratio\n        pg_losses2 = -ADV * tf.clip_by_value(ratio, 1.0 - CLIPRANGE, 1.0 + CLIPRANGE)\n        pg_loss = tf.reduce_mean(tf.maximum(pg_losses, pg_losses2))\n        approxkl = .5 * tf.reduce_mean(tf.square(neglogpac - OLDNEGLOGPAC))\n        clipfrac = tf.reduce_mean(tf.to_float(tf.greater(tf.abs(ratio - 1.0), CLIPRANGE)))\n        loss = pg_loss - entropy * ent_coef + vf_loss * vf_coef\n        with tf.variable_scope(TF_VAR_SCOPE):\n            params = tf.trainable_variables()\n        grads = tf.gradients(loss, params)\n        if max_grad_norm is not None:\n            grads, _grad_norm = tf.clip_by_global_norm(grads, max_grad_norm)\n        grads = list(zip(grads, params))\n        trainer = tf.train.AdamOptimizer(learning_rate=LR, epsilon=1e-5)\n        _train = trainer.apply_gradients(grads)\n\n        def train(lr, cliprange, obs, returns, masks, actions, values, neglogpacs, states=None):\n            advs = returns - values\n            if len(advs) > 1:\n                advs = (advs - advs.mean()) / (advs.std() + 1e-8)\n            for _adv in advs:\n                if math.isnan(_adv):\n                    print(\'huh oh nan time\')\n            td_map = {train_model.X:obs, A:actions, ADV:advs, R:returns, LR:lr,\n                    CLIPRANGE:cliprange, OLDNEGLOGPAC:neglogpacs, OLDVPRED:values}\n            if states is not None:\n                td_map[train_model.S] = states\n                td_map[train_model.M] = masks\n            # print(\'running backprop\')\n            ret = sess.run(\n                [pg_loss, vf_loss, entropy, approxkl, clipfrac, _train],\n                td_map\n            )[:-1]\n            return ret\n        self.loss_names = [\'policy_loss\', \'value_loss\', \'policy_entropy\', \'approxkl\', \'clipfrac\']\n\n        def save(save_path):\n            print(\'saving model to %s\' % save_path)\n            ps = sess.run(params)\n            joblib.dump(ps, save_path)\n\n        def load(load_path):\n            print(\'loading weights from %s\' % load_path)\n            loaded_params = joblib.load(load_path)\n            restores = []\n            for p, loaded_p in zip(params, loaded_params):\n                restores.append(p.assign(loaded_p))\n            sess.run(restores)\n            # If you want to load weights, also save/load observation scaling inside VecNormalize\n\n        self.train = train\n        self.train_model = train_model\n        self.act_model = act_model\n        self.step = act_model.step\n        self.value = act_model.value\n        self.initial_state = act_model.initial_state\n        self.save = save\n        self.load = load\n        tf.global_variables_initializer().run(session=sess) #pylint: disable=E1101\n\n\ndef mis(action_probs, rewards):\n    """""" Mistake importance scaling\n    It seems that taking the log probability in Policy Gradient reverses the amount of learning you would want for\n    negative rewards. i.e. We learn much more from unlikely bad actions, than we do likely ones. Whereas this is what\n    we want for positive rewards - to learn more from unlikely good actions, we would want the opposite for negative\n    rewards - learn more from likely bad actions because our goal is for bad actions and states to be unlikely.\n    I\'ve tested these ideas a bit in baselines and the results seem to be good.\n    Although I\'m sort of duct-taping on the idea by scaling negative rewards inversely to their odds to reverse\n    the effect of taking the log. I also notice that DQN, which does not scale the gradient by log likelihood,\n    does better than PG methods on Atari games with mostly negative rewards, i.e. DoubleDunk, ice hockey, and surround,\n    with skiing being an exception to this rule - but the score for skiing is weird.""""""\n    mis_rewards = []\n    for i, reward in enumerate(rewards):\n        if \'SCALE_ALL_REWARDS\' in os.environ:\n            mis_rewards.append(reward * 1.8)  # Works (in pong), but not as well as scaling by odds\n        else:\n            if reward < 0:\n                scale = 1 + action_probs[i] / (1 - action_probs[i])\n                scale = min(scale, 3)\n                mis_rewards.append(reward * scale)\n            else:\n                mis_rewards.append(reward)\n    return mis_rewards\n\n\nclass Runner(object):\n\n    def __init__(self, *, env, model, nsteps, gamma, lam):\n        self.env = env\n        self.model = model\n        nenv = env.num_envs\n        self.obs = np.zeros((nenv,) + env.observation_space.shape, dtype=model.train_model.X.dtype.name)\n        self.obs[:] = env.reset()\n        self.gamma = gamma\n        self.lam = lam\n        self.nsteps = nsteps\n        self.states = model.initial_state\n        self.dones = [False for _ in range(nenv)]\n\n    def run(self):\n        mb_obs, mb_rewards, mb_actions, mb_values, mb_dones, mb_neglogpacs = [],[],[],[],[],[]\n        mb_states = self.states\n        epinfos = []\n        for _ in range(self.nsteps):\n            actions, values, self.states, neglogpacs, action_probs = self.model.step(self.obs, self.states, self.dones)\n\n            mb_obs.append(self.obs.copy())\n            mb_actions.append(actions)\n            mb_values.append(values)\n            mb_neglogpacs.append(neglogpacs)\n            mb_dones.append(self.dones)\n\n            self.obs[:], rewards, self.dones, infos = self.env.step(actions)\n\n            rewards = mis(action_probs, rewards)\n\n            for info in infos:\n                maybe_episode_info = info.get(\'episode\') if info else None\n                if maybe_episode_info: epinfos.append(maybe_episode_info)\n\n            mb_rewards.append(rewards)\n        #batch of steps to batch of rollouts\n        mb_obs = np.asarray(mb_obs, dtype=self.obs.dtype)\n        mb_rewards = np.asarray(mb_rewards, dtype=np.float32)\n        mb_actions = np.asarray(mb_actions)\n        mb_values = np.asarray(mb_values, dtype=np.float32)\n        mb_neglogpacs = np.asarray(mb_neglogpacs, dtype=np.float32)\n        mb_dones = np.asarray(mb_dones, dtype=np.bool)\n        last_values = self.model.value(self.obs, self.states, self.dones)\n        #discount/bootstrap off value fn\n        mb_returns = np.zeros_like(mb_rewards)\n        mb_advs = np.zeros_like(mb_rewards)\n        lastgaelam = 0\n        for t in reversed(range(self.nsteps)):\n            if t == self.nsteps - 1:\n                nextnonterminal = 1.0 - self.dones\n                nextvalues = last_values\n            else:\n                nextnonterminal = 1.0 - mb_dones[t+1]\n                nextvalues = mb_values[t+1]\n            delta = mb_rewards[t] + self.gamma * nextvalues * nextnonterminal - mb_values[t]\n            mb_advs[t] = lastgaelam = delta + self.gamma * self.lam * nextnonterminal * lastgaelam\n        mb_returns = mb_advs + mb_values\n\n        # TODO(py27): Python versions < 3.5 do not support starred expressions in tuples, lists, and sets\n        return (*map(sf01, (mb_obs, mb_returns, mb_dones, mb_actions, mb_values, mb_neglogpacs)),\n            mb_states, epinfos)\n\n    def process_actions(self, actions):\n        action = Action.from_gym(actions)\n        action.throttle = get_throttle(actual_speed=self.obs[\'speed\'], target_speed=(8 * 100))\n        actions = action.as_gym()\n        return actions\n\n\n# obs, returns, masks, actions, values, neglogpacs, states = runner.run()\n\n\ndef sf01(arr):\n    """"""\n    swap and then flatten axes 0 and 1\n    """"""\n    s = arr.shape\n    return arr.swapaxes(0, 1).reshape(s[0] * s[1], *s[2:])\n\n\ndef constfn(val):\n    def f(_):\n        return val\n    return f\n\n\ndef learn(*, policy, env, nsteps, total_timesteps, ent_coef, lr,\n            vf_coef=0.5,  max_grad_norm=0.5, gamma=0.99, lam=0.95,\n            log_interval=10, nminibatches=4, noptepochs=4, cliprange=0.2,\n            save_interval=0, eval_only=False, **kwargs):\n\n    if isinstance(lr, float): lr = constfn(lr)\n    else: assert callable(lr)\n    if isinstance(cliprange, float): cliprange = constfn(cliprange)\n    else: assert callable(cliprange)\n    total_timesteps = int(total_timesteps)\n\n    nenvs = env.num_envs\n    ob_space = env.observation_space\n\n    ac_space = env.action_space\n    nbatch = nenvs * nsteps\n\n    if nenvs < nminibatches and \'lstm\' in policy.__name__.lower():\n        # We aren\'t running enough environments to split our observations across\n        nbatch_train = nbatch\n    else:\n        nbatch_train = nbatch // nminibatches\n\n    make_model = lambda : Model(policy=policy, ob_space=ob_space, ac_space=ac_space, nbatch_act=nenvs, nbatch_train=nbatch_train,\n                    nsteps=nsteps, ent_coef=ent_coef, vf_coef=vf_coef,\n                    max_grad_norm=max_grad_norm, **kwargs)\n    if save_interval and logger.get_dir():\n        import cloudpickle\n        with open(osp.join(logger.get_dir(), \'make_model.pkl\'), \'wb\') as fh:\n            fh.write(cloudpickle.dumps(make_model))\n    model = make_model()\n    if c.PPO_RESUME_PATH is not None:\n        model.load(c.PPO_RESUME_PATH)\n\n    runner = Runner(env=env, model=model, nsteps=nsteps, gamma=gamma, lam=lam)\n\n    epinfobuf = deque(maxlen=100)\n    tfirststart = time.time()\n\n    nupdates = total_timesteps//nbatch\n    for update in range(1, nupdates + 1):\n        assert nbatch % nminibatches == 0\n        nbatch_train = nbatch // nminibatches\n        tstart = time.time()\n        frac = 1.0 - (update - 1.0) / nupdates\n        lrnow = lr(frac)\n        cliprangenow = cliprange(frac)\n\n        obs, returns, masks, actions, values, neglogpacs, states, epinfos = runner.run() #pylint: disable=E0632\n\n        if eval_only:\n            continue\n\n        epinfobuf.extend(epinfos)\n        mblossvals = []\n        if states is None: # nonrecurrent version\n            inds = np.arange(nbatch)\n            for _ in range(noptepochs):\n                np.random.shuffle(inds)\n                for start in range(0, nbatch, nbatch_train):\n                    end = start + nbatch_train\n                    minibatch_indxs = inds[start:end]\n                    slices = (arr[minibatch_indxs] for arr in (obs, returns, masks, actions, values, neglogpacs))\n                    mblossvals.append(model.train(lrnow, cliprangenow, *slices))\n        else: # recurrent version\n            # assert nenvs % nminibatches == 0\n            # envsperbatch = nenvs // nminibatches\n            envinds = np.arange(nenvs)\n            flatinds = np.arange(nenvs * nsteps).reshape(nenvs, nsteps)\n            envsperbatch = nbatch_train // nsteps  # ((nevns * nsteps) // nminibatches)  // nsteps\n            envsperbatch = max(envsperbatch, 1)\n            for _ in range(noptepochs):\n                np.random.shuffle(envinds)\n                for start in range(0, nenvs, envsperbatch):\n                    end = start + envsperbatch\n                    mbenvinds = envinds[start:end]\n                    mbflatinds = flatinds[mbenvinds].ravel()\n                    slices = (arr[mbflatinds] for arr in (obs, returns, masks, actions, values, neglogpacs))\n                    mbstates = states[mbenvinds]\n\n                    # TODO(py27): Python versions < 3.5 do not allow positional arguments after *expression\n                    mblossvals.append(model.train(lrnow, cliprangenow, *slices, mbstates))\n\n        lossvals = np.mean(mblossvals, axis=0)\n        tnow = time.time()\n        fps = int(nbatch / (tnow - tstart))\n        if update % log_interval == 0 or update == 1:\n            ev = explained_variance(values, returns)\n            logger.logkv(""serial_timesteps"", update * nsteps)\n            logger.logkv(""nupdates"", update)\n            logger.logkv(""total_timesteps"", update * nbatch)\n            logger.logkv(""fps"", fps)\n            logger.logkv(""explained_variance"", float(ev))\n            logger.logkv(\'eprewmean\', safemean([epinfo[\'reward\'] for epinfo in epinfobuf]))\n            logger.logkv(\'eplenmean\', safemean([epinfo[\'length\'] for epinfo in epinfobuf]))\n            logger.logkv(\'time_elapsed\', tnow - tfirststart)\n            for (lossval, lossname) in zip(lossvals, model.loss_names):\n                logger.logkv(lossname, lossval)\n            logger.dumpkvs()\n            # input(\'continue?\')\n        if save_interval and (update % save_interval == 0 or update == 1) and logger.get_dir():\n            checkdir = osp.join(logger.get_dir(), \'checkpoints\')\n            os.makedirs(checkdir, exist_ok=True)\n            savepath = osp.join(checkdir, \'%.5i\'%update)\n            print(\'Saving to\', savepath)\n            model.save(savepath)\n    env.close()\n\n\ndef safemean(xs):\n    return np.nan if len(xs) == 0 else np.mean(xs)\n'"
vendor/openai/baselines/ppo2/run_atari.py,2,"b""#!/usr/bin/env python3\nimport sys\nfrom vendor.openai.baselines import logger\nfrom vendor.openai.baselines.common.cmd_util import make_atari_env, atari_arg_parser\nfrom vendor.openai.baselines.common.vec_env.vec_frame_stack import VecFrameStack\nfrom vendor.openai.baselines.ppo2 import ppo2\nfrom vendor.openai.baselines.ppo2.policies import CnnPolicy, LstmPolicy, LnLstmPolicy\nimport multiprocessing\nimport tensorflow as tf\n\n\ndef train(env_id, num_timesteps, seed, policy):\n\n    ncpu = multiprocessing.cpu_count()\n    if sys.platform == 'darwin': ncpu //= 2\n    config = tf.ConfigProto(allow_soft_placement=True,\n                            intra_op_parallelism_threads=ncpu,\n                            inter_op_parallelism_threads=ncpu)\n    config.gpu_options.allow_growth = True #pylint: disable=E1101\n    tf.Session(config=config).__enter__()\n\n    env = VecFrameStack(make_atari_env(env_id, 8, seed), 4)\n    policy = {'cnn' : CnnPolicy, 'lstm' : LstmPolicy, 'lnlstm' : LnLstmPolicy}[policy]\n    ppo2.learn(policy=policy,\n               env=env,\n               nsteps=128,\n               nminibatches=4,\n               lam=0.95,\n               gamma=0.99,\n               noptepochs=4, log_interval=1,\n               ent_coef=.01,\n               lr=lambda f: f * 2.5e-4,\n               cliprange=lambda f: f * 0.1,\n               total_timesteps=int(num_timesteps * 1.1))\n\ndef main():\n    parser = atari_arg_parser()\n    parser.add_argument('--policy', help='Policy architecture', choices=['cnn', 'lstm', 'lnlstm'], default='cnn')\n    args = parser.parse_args()\n    logger.configure()\n    train(args.env, num_timesteps=args.num_timesteps, seed=args.seed,\n        policy=args.policy)\n\nif __name__ == '__main__':\n    main()\n"""
vendor/openai/baselines/ppo2/run_continuous_mountain_car.py,2,"b""#!/usr/bin/env python3\n\nimport os\n\nfrom vendor.openai.baselines import bench, logger\n\nfrom vendor.openai.baselines.common.cmd_util import continuous_mountain_car_arg_parser\n\n\ndef train(env_id, num_timesteps, seed):\n    from vendor.openai.baselines.common.misc_util import set_global_seeds\n    from vendor.openai.baselines.common.vec_env import VecNormalize\n    from vendor.openai.baselines.ppo2 import ppo2\n    from vendor.openai.baselines.ppo2 import MlpPolicy, LstmPolicyFlat\n    import gym\n    import tensorflow as tf\n    from vendor.openai.baselines.common.vec_env import DummyVecEnv\n    ncpu = 1\n    config = tf.ConfigProto(allow_soft_placement=True,\n                            intra_op_parallelism_threads=ncpu,\n                            inter_op_parallelism_threads=ncpu)\n    tf.Session(config=config).__enter__()\n\n    def make_env():\n        env = gym.make(env_id)\n        env = bench.Monitor(env, logger.get_dir())\n        pstep = env.step\n\n        def step(action):\n            observation, reward, done, info = pstep(action)\n            env.render()\n            return observation, reward, done, info\n        env.step = step\n        return env\n\n    env = DummyVecEnv([make_env])\n    env = VecNormalize(env)\n\n    set_global_seeds(seed)\n    if 'LSTM_FLAT' in os.environ:\n        policy = LstmPolicyFlat\n    else:\n        policy = MlpPolicy\n    ppo2.learn(policy=policy,\n               env=env,\n               nsteps=256,\n               nminibatches=32,  # Sweet spot is between 16 and 64\n               lam=0.95,\n               gamma=0.99,\n               noptepochs=10,\n               log_interval=1,\n               ent_coef=0.0,\n               lr=lambda f: f * 2.5e-4,\n               cliprange=lambda f: f * 0.1,\n               total_timesteps=num_timesteps)\n\n\ndef main():\n    args = continuous_mountain_car_arg_parser().parse_args()\n    logger.configure()\n    train(args.env, num_timesteps=args.num_timesteps, seed=args.seed)\n\n\nif __name__ == '__main__':\n    main()\n"""
vendor/openai/baselines/ppo2/run_deepdrive.py,2,"b'#!/usr/bin/env python3\n\nimport os\n\nimport config as c\n\nfrom vendor.openai.baselines import bench, logger\n\nfrom vendor.openai.baselines.common.cmd_util import continuous_mountain_car_arg_parser\n\n\ndef train(env, seed, sess=None, is_discrete=True, minibatch_steps=None, mlp_width=None, eval_only=False):\n    from vendor.openai.baselines.common.misc_util import set_global_seeds\n    from vendor.openai.baselines.common.vec_env.vec_normalize import VecNormalize\n    from vendor.openai.baselines.ppo2 import ppo2\n    from vendor.openai.baselines.ppo2.policies import CnnPolicy, LstmPolicyFlat, MlpPolicy\n    import gym\n    import tensorflow as tf\n    from vendor.openai.baselines.common.vec_env.dummy_vec_env import DummyVecEnv\n    ncpu = 1\n\n    logger.configure()\n\n    if sess is None:\n        config = tf.ConfigProto(allow_soft_placement=True,\n                                intra_op_parallelism_threads=ncpu,\n                                inter_op_parallelism_threads=ncpu)\n        tf.Session(config=config).__enter__()\n\n    env = DummyVecEnv(envs=[env])\n    env = VecNormalize(env, ob=False)\n\n    set_global_seeds(seed)\n    if is_discrete:\n        policy = LstmPolicyFlat\n    else:\n        # continuous\n        policy = MlpPolicy\n\n\n    # TODO: Stack 8 (1 second) input frames as is done for atari environments\n\n    # TODO: Simplify by just outputting right, left, straight discrete actions, no bootstrap, Enduro setup. manual throttle - reward center of lane only\n\n    ppo2.learn(policy=policy,\n               env=env,\n               nsteps=minibatch_steps,\n               nminibatches=1,  # Sweet spot is between 16 and 64 for continuous mountain car @55fps\n               lam=0.95,\n               gamma=0.99,\n               save_interval=5,\n               noptepochs=3,\n               log_interval=1,\n               ent_coef=0.0,\n               lr=lambda f: f * 2.5e-3,\n               cliprange=lambda f: f * 0.1,\n               total_timesteps=int(5e5),\n               mlp_width=mlp_width,\n               eval_only=eval_only)\n\n    # Long training with lots of epochs\n    # ppo2.learn(policy=policy,\n    #            env=env,\n    #            nsteps=minibatch_steps,\n    #            nminibatches=1,  # Sweet spot is between 16 and 64 for continuous mountain car @55fps\n    #            lam=0.95,\n    #            gamma=0.99,\n    #            save_interval=5,\n    #            noptepochs=30,\n    #            log_interval=1,\n    #            ent_coef=0.0,\n    #            lr=lambda f: f * 2.5e-3,\n    #            cliprange=lambda f: f * 0.1,\n    #            total_timesteps=int(2.5e5),\n    #            mlp_width=mlp_width)\n\n'"
vendor/tensorflow/models/research/__init__.py,0,b''
vendor/tensorflow/models/research/setup.py,0,"b'""""""Setup script for object_detection.""""""\n\nfrom setuptools import find_packages\nfrom setuptools import setup\n\n\nREQUIRED_PACKAGES = [\'Pillow>=1.0\']\n\nsetup(\n    name=\'object_detection\',\n    version=\'0.1\',\n    install_requires=REQUIRED_PACKAGES,\n    include_package_data=True,\n    packages=[p for p in find_packages() if p.startswith(\'object_detection\')],\n    description=\'Tensorflow Object Detection Library\',\n)\n'"
vendor/openai/baselines/common/tests/__init__.py,0,b''
vendor/openai/baselines/common/tests/test_schedules.py,0,"b'import numpy as np\n\nfrom vendor.openai.baselines.common.schedules import ConstantSchedule, PiecewiseSchedule\n\n\ndef test_piecewise_schedule():\n    ps = PiecewiseSchedule([(-5, 100), (5, 200), (10, 50), (100, 50), (200, -50)], outside_value=500)\n\n    assert np.isclose(ps.value(-10), 500)\n    assert np.isclose(ps.value(0), 150)\n    assert np.isclose(ps.value(5), 200)\n    assert np.isclose(ps.value(9), 80)\n    assert np.isclose(ps.value(50), 50)\n    assert np.isclose(ps.value(80), 50)\n    assert np.isclose(ps.value(150), 0)\n    assert np.isclose(ps.value(175), -25)\n    assert np.isclose(ps.value(201), 500)\n    assert np.isclose(ps.value(500), 500)\n\n    assert np.isclose(ps.value(200 - 1e-10), -50)\n\n\ndef test_constant_schedule():\n    cs = ConstantSchedule(5)\n    for i in range(-100, 100):\n        assert np.isclose(cs.value(i), 5)\n'"
vendor/openai/baselines/common/tests/test_segment_tree.py,0,"b""import numpy as np\n\nfrom vendor.openai.baselines.common.segment_tree import SumSegmentTree, MinSegmentTree\n\n\ndef test_tree_set():\n    tree = SumSegmentTree(4)\n\n    tree[2] = 1.0\n    tree[3] = 3.0\n\n    assert np.isclose(tree.sum(), 4.0)\n    assert np.isclose(tree.sum(0, 2), 0.0)\n    assert np.isclose(tree.sum(0, 3), 1.0)\n    assert np.isclose(tree.sum(2, 3), 1.0)\n    assert np.isclose(tree.sum(2, -1), 1.0)\n    assert np.isclose(tree.sum(2, 4), 4.0)\n\n\ndef test_tree_set_overlap():\n    tree = SumSegmentTree(4)\n\n    tree[2] = 1.0\n    tree[2] = 3.0\n\n    assert np.isclose(tree.sum(), 3.0)\n    assert np.isclose(tree.sum(2, 3), 3.0)\n    assert np.isclose(tree.sum(2, -1), 3.0)\n    assert np.isclose(tree.sum(2, 4), 3.0)\n    assert np.isclose(tree.sum(1, 2), 0.0)\n\n\ndef test_prefixsum_idx():\n    tree = SumSegmentTree(4)\n\n    tree[2] = 1.0\n    tree[3] = 3.0\n\n    assert tree.find_prefixsum_idx(0.0) == 2\n    assert tree.find_prefixsum_idx(0.5) == 2\n    assert tree.find_prefixsum_idx(0.99) == 2\n    assert tree.find_prefixsum_idx(1.01) == 3\n    assert tree.find_prefixsum_idx(3.00) == 3\n    assert tree.find_prefixsum_idx(4.00) == 3\n\n\ndef test_prefixsum_idx2():\n    tree = SumSegmentTree(4)\n\n    tree[0] = 0.5\n    tree[1] = 1.0\n    tree[2] = 1.0\n    tree[3] = 3.0\n\n    assert tree.find_prefixsum_idx(0.00) == 0\n    assert tree.find_prefixsum_idx(0.55) == 1\n    assert tree.find_prefixsum_idx(0.99) == 1\n    assert tree.find_prefixsum_idx(1.51) == 2\n    assert tree.find_prefixsum_idx(3.00) == 3\n    assert tree.find_prefixsum_idx(5.50) == 3\n\n\ndef test_max_interval_tree():\n    tree = MinSegmentTree(4)\n\n    tree[0] = 1.0\n    tree[2] = 0.5\n    tree[3] = 3.0\n\n    assert np.isclose(tree.min(), 0.5)\n    assert np.isclose(tree.min(0, 2), 1.0)\n    assert np.isclose(tree.min(0, 3), 0.5)\n    assert np.isclose(tree.min(0, -1), 0.5)\n    assert np.isclose(tree.min(2, 4), 0.5)\n    assert np.isclose(tree.min(3, 4), 3.0)\n\n    tree[2] = 0.7\n\n    assert np.isclose(tree.min(), 0.7)\n    assert np.isclose(tree.min(0, 2), 1.0)\n    assert np.isclose(tree.min(0, 3), 0.7)\n    assert np.isclose(tree.min(0, -1), 0.7)\n    assert np.isclose(tree.min(2, 4), 0.7)\n    assert np.isclose(tree.min(3, 4), 3.0)\n\n    tree[2] = 4.0\n\n    assert np.isclose(tree.min(), 1.0)\n    assert np.isclose(tree.min(0, 2), 1.0)\n    assert np.isclose(tree.min(0, 3), 1.0)\n    assert np.isclose(tree.min(0, -1), 1.0)\n    assert np.isclose(tree.min(2, 4), 3.0)\n    assert np.isclose(tree.min(2, 3), 4.0)\n    assert np.isclose(tree.min(2, -1), 4.0)\n    assert np.isclose(tree.min(3, 4), 3.0)\n\n\nif __name__ == '__main__':\n    test_tree_set()\n    test_tree_set_overlap()\n    test_prefixsum_idx()\n    test_prefixsum_idx2()\n    test_max_interval_tree()\n"""
vendor/openai/baselines/common/tests/test_tf_util.py,7,"b'# tests for tf_util\nimport tensorflow as tf\nfrom vendor.openai.baselines.common.tf_util import (\n    function,\n    initialize,\n    single_threaded_session\n)\n\n\ndef test_function():\n    with tf.Graph().as_default():\n        x = tf.placeholder(tf.int32, (), name=""x"")\n        y = tf.placeholder(tf.int32, (), name=""y"")\n        z = 3 * x + 2 * y\n        lin = function([x, y], z, givens={y: 0})\n\n        with single_threaded_session():\n            initialize()\n\n            assert lin(2) == 6\n            assert lin(2, 2) == 10\n\n\ndef test_multikwargs():\n    with tf.Graph().as_default():\n        x = tf.placeholder(tf.int32, (), name=""x"")\n        with tf.variable_scope(""other""):\n            x2 = tf.placeholder(tf.int32, (), name=""x"")\n        z = 3 * x + 2 * x2\n\n        lin = function([x, x2], z, givens={x2: 0})\n        with single_threaded_session():\n            initialize()\n            assert lin(2) == 6\n            assert lin(2, 2) == 10\n            expt_caught = False\n\n\nif __name__ == \'__main__\':\n    test_function()\n    test_multikwargs()\n'"
vendor/openai/baselines/common/vec_env/__init__.py,0,"b'from abc import ABC, abstractmethod\nfrom vendor.openai.baselines import logger\n\nclass AlreadySteppingError(Exception):\n    """"""\n    Raised when an asynchronous step is running while\n    step_async() is called again.\n    """"""\n    def __init__(self):\n        msg = \'already running an async step\'\n        Exception.__init__(self, msg)\n\nclass NotSteppingError(Exception):\n    """"""\n    Raised when an asynchronous step is not running but\n    step_wait() is called.\n    """"""\n    def __init__(self):\n        msg = \'not running an async step\'\n        Exception.__init__(self, msg)\n\nclass VecEnv(ABC):\n    """"""\n    An abstract asynchronous, vectorized environment.\n    """"""\n    def __init__(self, num_envs, observation_space, action_space):\n        self.num_envs = num_envs\n        self.observation_space = observation_space\n        self.action_space = action_space\n\n    @abstractmethod\n    def reset(self):\n        """"""\n        Reset all the environments and return an array of\n        observations, or a tuple of observation arrays.\n\n        If step_async is still doing work, that work will\n        be cancelled and step_wait() should not be called\n        until step_async() is invoked again.\n        """"""\n        pass\n\n    @abstractmethod\n    def step_async(self, actions):\n        """"""\n        Tell all the environments to start taking a step\n        with the given actions.\n        Call step_wait() to get the results of the step.\n\n        You should not call this if a step_async run is\n        already pending.\n        """"""\n        pass\n\n    @abstractmethod\n    def step_wait(self):\n        """"""\n        Wait for the step taken with step_async().\n\n        Returns (obs, rews, dones, infos):\n         - obs: an array of observations, or a tuple of\n                arrays of observations.\n         - rews: an array of rewards\n         - dones: an array of ""episode done"" booleans\n         - infos: a sequence of info objects\n        """"""\n        pass\n\n    @abstractmethod\n    def close(self):\n        """"""\n        Clean up the environments\' resources.\n        """"""\n        pass\n\n    def step(self, actions):\n        self.step_async(actions)\n        return self.step_wait()\n\n    def render(self):\n        logger.warn(\'Render not defined for %s\'%self)\n\nclass VecEnvWrapper(VecEnv):\n    def __init__(self, venv, observation_space=None, action_space=None):\n        self.venv = venv\n        VecEnv.__init__(self, \n            num_envs=venv.num_envs,\n            observation_space=observation_space or venv.observation_space, \n            action_space=action_space or venv.action_space)\n\n    def step_async(self, actions):\n        self.venv.step_async(actions)\n\n    @abstractmethod\n    def reset(self):\n        pass\n\n    @abstractmethod\n    def step_wait(self):\n        pass\n\n    def close(self):\n        return self.venv.close()\n\n    def render(self):\n        self.venv.render()\n\nclass CloudpickleWrapper(object):\n    """"""\n    Uses cloudpickle to serialize contents (otherwise multiprocessing tries to use pickle)\n    """"""\n    def __init__(self, x):\n        self.x = x\n    def __getstate__(self):\n        import cloudpickle\n        return cloudpickle.dumps(self.x)\n    def __setstate__(self, ob):\n        import pickle\n        self.x = pickle.loads(ob)\n'"
vendor/openai/baselines/common/vec_env/dummy_vec_env.py,0,"b'import numpy as np\nimport gym\nimport gym.spaces\nfrom . import VecEnv\n\nclass DummyVecEnv(VecEnv):\n    def __init__(self, env_fns=None, envs=None):\n        if envs is None:\n            self.envs = [fn() for fn in env_fns]\n        else:\n            self.envs = envs\n        env = self.envs[0]\n        VecEnv.__init__(self, len(envs), env.observation_space, env.action_space)\n\n        obs_spaces = self.observation_space.spaces if isinstance(self.observation_space, gym.spaces.Tuple) else (self.observation_space,)\n        self.buf_obs = [np.zeros((self.num_envs,) + tuple(s.shape), s.dtype) for s in obs_spaces]\n        self.buf_dones = np.zeros((self.num_envs,), dtype=np.bool)\n        self.buf_rews  = np.zeros((self.num_envs,), dtype=np.float32)\n        self.buf_infos = [{} for _ in range(self.num_envs)]\n        self.actions = None\n\n    def step_async(self, actions):\n        self.actions = actions\n\n    def step_wait(self):\n        for i in range(self.num_envs):\n            obs_tuple, self.buf_rews[i], self.buf_dones[i], self.buf_infos[i] = self.envs[i].step(self.actions[i])\n            if self.buf_dones[i]:\n                obs_tuple = self.envs[i].reset()\n            if obs_tuple is None:\n                obs_tuple = 0.\n            if isinstance(obs_tuple, (tuple, list)):\n                for t, x in enumerate(obs_tuple):\n                    self.buf_obs[t][i] = x\n            else:\n                self.buf_obs[0][i] = obs_tuple\n        return (self._obs_from_buf(), np.copy(self.buf_rews), np.copy(self.buf_dones),\n                self.buf_infos.copy())\n\n    def reset(self):\n        for i in range(self.num_envs):\n            obs_tuple = self.envs[i].reset()\n            if obs_tuple is None:\n                obs_tuple = 0.\n            if isinstance(obs_tuple, (tuple, list)):\n                for t, x in enumerate(obs_tuple):\n                    self.buf_obs[t][i] = x\n            else:\n                self.buf_obs[0][i] = obs_tuple\n        return self._obs_from_buf()\n\n    def close(self):\n        return\n\n    def _obs_from_buf(self):\n        if len(self.buf_obs) == 1:\n            return np.copy(self.buf_obs[0])\n        else:\n            return tuple(np.copy(x) for x in self.buf_obs)\n'"
vendor/openai/baselines/common/vec_env/subproc_vec_env.py,0,"b'import numpy as np\nfrom multiprocessing import Process, Pipe\nfrom vendor.openai.baselines.common.vec_env import VecEnv, CloudpickleWrapper\n\n\ndef worker(remote, parent_remote, env_fn_wrapper):\n    parent_remote.close()\n    env = env_fn_wrapper.x()\n    while True:\n        cmd, data = remote.recv()\n        if cmd == \'step\':\n            ob, reward, done, info = env.step(data)\n            if env.env.env.env.env.__rank == 0:\n                env.render()\n            if done:\n                ob = env.reset()\n            remote.send((ob, reward, done, info))\n        elif cmd == \'reset\':\n            ob = env.reset()\n            remote.send(ob)\n        elif cmd == \'reset_task\':\n            ob = env.reset_task()\n            remote.send(ob)\n        elif cmd == \'close\':\n            remote.close()\n            break\n        elif cmd == \'get_spaces\':\n            remote.send((env.observation_space, env.action_space))\n        else:\n            raise NotImplementedError\n\n\nclass SubprocVecEnv(VecEnv):\n    def __init__(self, env_fns, spaces=None):\n        """"""\n        envs: list of gym environments to run in subprocesses\n        """"""\n        self.waiting = False\n        self.closed = False\n        nenvs = len(env_fns)\n        self.remotes, self.work_remotes = zip(*[Pipe() for _ in range(nenvs)])\n        self.ps = [Process(target=worker, args=(work_remote, remote, CloudpickleWrapper(env_fn)))\n            for (work_remote, remote, env_fn) in zip(self.work_remotes, self.remotes, env_fns)]\n        for p in self.ps:\n            p.daemon = True # if the main process crashes, we should not cause things to hang\n            p.start()\n        for remote in self.work_remotes:\n            remote.close()\n\n        self.remotes[0].send((\'get_spaces\', None))\n        observation_space, action_space = self.remotes[0].recv()\n        VecEnv.__init__(self, len(env_fns), observation_space, action_space)\n\n    def step_async(self, actions):\n        for remote, action in zip(self.remotes, actions):\n            remote.send((\'step\', action))\n        self.waiting = True\n\n    def step_wait(self):\n        results = [remote.recv() for remote in self.remotes]\n        self.waiting = False\n        obs, rews, dones, infos = zip(*results)\n        return np.stack(obs), np.stack(rews), np.stack(dones), infos\n\n    def reset(self):\n        for remote in self.remotes:\n            remote.send((\'reset\', None))\n        return np.stack([remote.recv() for remote in self.remotes])\n\n    def reset_task(self):\n        for remote in self.remotes:\n            remote.send((\'reset_task\', None))\n        return np.stack([remote.recv() for remote in self.remotes])\n\n    def close(self):\n        if self.closed:\n            return\n        if self.waiting:\n            for remote in self.remotes:            \n                remote.recv()\n        for remote in self.remotes:\n            remote.send((\'close\', None))\n        for p in self.ps:\n            p.join()\n        self.closed = True\n'"
vendor/openai/baselines/common/vec_env/vec_frame_stack.py,0,"b'from vendor.openai.baselines.common.vec_env import VecEnvWrapper\nimport numpy as np\nfrom gym import spaces\n\nclass VecFrameStack(VecEnvWrapper):\n    """"""\n    Vectorized environment base class\n    """"""\n    def __init__(self, venv, nstack):\n        self.venv = venv\n        self.nstack = nstack\n        wos = venv.observation_space # wrapped ob space\n        low = np.repeat(wos.low, self.nstack, axis=-1)\n        high = np.repeat(wos.high, self.nstack, axis=-1)\n        self.stackedobs = np.zeros((venv.num_envs,)+low.shape, low.dtype)\n        observation_space = spaces.Box(low=low, high=high, dtype=venv.observation_space.dtype)\n        VecEnvWrapper.__init__(self, venv, observation_space=observation_space)\n\n    def step_wait(self):\n        obs, rews, news, infos = self.venv.step_wait()\n        self.stackedobs = np.roll(self.stackedobs, shift=-1, axis=-1)\n        for (i, new) in enumerate(news):\n            if new:\n                self.stackedobs[i] = 0\n        self.stackedobs[..., -obs.shape[-1]:] = obs\n        return self.stackedobs, rews, news, infos\n\n    def reset(self):\n        """"""\n        Reset all environments\n        """"""\n        obs = self.venv.reset()\n        self.stackedobs[...] = 0\n        self.stackedobs[..., -obs.shape[-1]:] = obs\n        return self.stackedobs\n\n    def close(self):\n        self.venv.close()\n'"
vendor/openai/baselines/common/vec_env/vec_normalize.py,0,"b'from vendor.openai.baselines.common.vec_env import VecEnvWrapper\nfrom vendor.openai.baselines.common.running_mean_std import RunningMeanStd\nimport numpy as np\n\n\nclass VecNormalize(VecEnvWrapper):\n    """"""\n    Vectorized environment base class\n    """"""\n    def __init__(self, venv, ob=True, ret=True, clipob=10., cliprew=10., gamma=0.99, epsilon=1e-8):\n        VecEnvWrapper.__init__(self, venv)\n        self.ob_rms = RunningMeanStd(shape=self.observation_space.shape) if ob else None\n        self.ret_rms = RunningMeanStd(shape=()) if ret else None\n        self.clipob = clipob\n        self.cliprew = cliprew\n        self.ret = np.zeros(self.num_envs)\n        self.gamma = gamma\n        self.epsilon = epsilon\n\n    def step_wait(self):\n        """"""\n        Apply sequence of actions to sequence of environments\n        actions -> (observations, rewards, news)\n\n        where \'news\' is a boolean vector indicating whether each element is new.\n        """"""\n        obs, rews, news, infos = self.venv.step_wait()\n        self.ret = self.ret * self.gamma + rews\n        obs = self._obfilt(obs)\n        if self.ret_rms:\n            self.ret_rms.update(self.ret)\n            rews = np.clip(rews / np.sqrt(self.ret_rms.var + self.epsilon), -self.cliprew, self.cliprew)\n        return obs, rews, news, infos\n\n    def _obfilt(self, obs):\n        if self.ob_rms:\n            self.ob_rms.update(obs)\n            obs = np.clip((obs - self.ob_rms.mean) / np.sqrt(self.ob_rms.var + self.epsilon), -self.clipob, self.clipob)\n            return obs\n        else:\n            return obs\n\n    def reset(self):\n        """"""\n        Reset all environments\n        """"""\n        obs = self.venv.reset()\n        return self._obfilt(obs)\n'"
vendor/tensorflow/models/research/slim/__init__.py,0,b''
vendor/tensorflow/models/research/slim/eval_image_nn.py,46,"b'# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Generic evaluation script that evaluates a model using a given dataset.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport glob\nimport math\nimport os\n\nimport tensorflow as tf\n\nfrom config import TENSORFLOW_OUT_DIR, CONTROL_NAMES, MOBILENET_V2_SLIM_NAME\nfrom vendor.tensorflow.models.research.slim.datasets import dataset_factory\nfrom vendor.tensorflow.models.research.slim.nets import nets_factory\nfrom vendor.tensorflow.models.research.slim.preprocessing import preprocessing_factory\n\nslim = tf.contrib.slim\n\n\ndef create_flags():\n    tf.app.flags.DEFINE_integer(\n        \'batch_size\', 100, \'The number of samples in each batch.\')\n\n    tf.app.flags.DEFINE_integer(\n        \'max_num_batches\', None,\n        \'Max number of batches to evaluate by default use all.\')\n\n    tf.app.flags.DEFINE_string(\n        \'master\', \'\', \'The address of the TensorFlow master to use.\')\n\n    tf.app.flags.DEFINE_string(\n        \'checkpoint_path\', None,\n        \'The directory where the model was written to or an absolute path to a \'\n        \'checkpoint file.\')\n\n    tf.app.flags.DEFINE_string(\n        \'eval_dir\', None, \'Directory where the results are saved to.\')\n\n    tf.app.flags.DEFINE_integer(\n        \'num_preprocessing_threads\', 4,\n        \'The number of threads used to create the batches.\')\n\n    tf.app.flags.DEFINE_string(\n        \'dataset_name\', \'imagenet\', \'The name of the dataset to load.\')\n\n    tf.app.flags.DEFINE_string(\n        \'dataset_split_name\', \'test\', \'The name of the train/test split.\')\n\n    tf.app.flags.DEFINE_string(\n        \'dataset_dir\', None, \'The directory where the dataset files are stored.\')\n\n    tf.app.flags.DEFINE_integer(\n        \'labels_offset\', 0,\n        \'An offset for the labels in the dataset. This flag is primarily used to \'\n        \'evaluate the VGG and ResNet architectures which do not use a background \'\n        \'class for the ImageNet dataset.\')\n\n    tf.app.flags.DEFINE_string(\n        \'model_name\', \'inception_v3\', \'The name of the architecture to evaluate.\')\n\n    tf.app.flags.DEFINE_string(\n        \'preprocessing_name\', None, \'The name of the preprocessing to use. If left \'\n                                    \'as `None`, then the model_name flag is used.\')\n\n    tf.app.flags.DEFINE_float(\n        \'moving_average_decay\', None,\n        \'The decay to use for the moving average.\'\n        \'If left as None, then moving averages are not used.\')\n\n    tf.app.flags.DEFINE_integer(\n        \'eval_image_size\', None, \'Eval image size\')\n\n\ndef main(_):\n    create_flags()\n    FLAGS = tf.app.flags.FLAGS\n    slim_eval_image_nn(FLAGS.eval_dir, FLAGS.dataset_dir, FLAGS.dataset_name, FLAGS.dataset_split_name, FLAGS.model_name,\n                       FLAGS.master, FLAGS.checkpoint_path, FLAGS.max_num_batches, FLAGS.labels_offset, FLAGS.moving_average_decay,\n                       FLAGS.num_preprocessing_threads, FLAGS.batch_size, FLAGS.preprocessing_name, FLAGS.eval_image_size)\n\n\ndef slim_eval_image_nn(eval_dir=None, dataset_dir=None, dataset_name=\'imagenet\', dataset_split_name=\'test\',\n                       model_name=\'inception_v3\', master=\'\', checkpoint_path=None, max_num_batches=None,\n                       labels_offset=0, moving_average_decay=None, num_preprocessing_threads=4, batch_size=100,\n                       preprocessing_name=None, eval_image_size=None):\n\n    if eval_dir is None:\n        eval_dir = max(glob.glob(TENSORFLOW_OUT_DIR + \'/*\'), key=os.path.getmtime)\n\n    if not dataset_dir:\n        raise ValueError(\'You must supply the dataset directory with --dataset_dir\')\n\n    tf.logging.set_verbosity(tf.logging.INFO)\n    with tf.Graph().as_default():\n        tf_global_step = slim.get_or_create_global_step()\n\n        ######################\n        # Select the dataset #\n        ######################\n        dataset = dataset_factory.get_dataset(\n            dataset_name, dataset_split_name, dataset_dir)\n\n        ####################\n        # Select the model #\n        ####################\n        ######################\n        # Select the network #\n        ######################\n        if model_name == MOBILENET_V2_SLIM_NAME:\n            network_fn = nets_factory.get_network_fn(\n                model_name,\n                num_classes=None,\n                num_targets=6,\n                is_training=False, )\n\n        else:\n            network_fn = nets_factory.get_network_fn(\n                model_name,\n                num_classes=(dataset.num_classes - labels_offset),\n                is_training=False)\n\n        #####################################\n        # Select the preprocessing function #\n        #####################################\n        preprocessing_name = preprocessing_name or model_name\n        image_preprocessing_fn = preprocessing_factory.get_preprocessing(\n            preprocessing_name,\n            is_training=False)\n\n        eval_image_size = eval_image_size or network_fn.default_image_size\n\n        ##############################################################\n        # Create a dataset provider that loads data from the dataset #\n        ##############################################################\n        provider = slim.dataset_data_provider.DatasetDataProvider(\n            dataset,\n            shuffle=False,\n            common_queue_capacity=2 * batch_size,\n            common_queue_min=batch_size)\n        if model_name == MOBILENET_V2_SLIM_NAME:\n            [image, spin, direction, speed, speed_change, steering, throttle] = provider.get(\n                [\'image\', \'spin\', \'direction\', \'speed\', \'speed_change\', \'steering\', \'throttle\'])\n\n            image = image_preprocessing_fn(image, eval_image_size, eval_image_size)\n\n            images, targets = tf.train.batch(\n                [image, [spin, direction, speed, speed_change, steering, throttle]],\n                batch_size=batch_size,\n                num_threads=num_preprocessing_threads,\n                capacity=5 * batch_size)\n        else:\n            [image, label] = provider.get([\'image\', \'label\'])\n            label -= labels_offset\n\n            image = image_preprocessing_fn(image, eval_image_size, eval_image_size)\n\n            images, labels = tf.train.batch(\n                [image, label],\n                batch_size=batch_size,\n                num_threads=num_preprocessing_threads,\n                capacity=5 * batch_size)\n\n        ####################\n        # Define the model #\n        ####################\n        logits, _ = network_fn(images)\n\n        if moving_average_decay:\n            variable_averages = tf.train.ExponentialMovingAverage(\n                moving_average_decay, tf_global_step)\n            variables_to_restore = variable_averages.variables_to_restore(\n                slim.get_model_variables())\n            variables_to_restore[tf_global_step.op.name] = tf_global_step\n        else:\n            variables_to_restore = slim.get_variables_to_restore()\n\n        if model_name == MOBILENET_V2_SLIM_NAME:\n            # targets = tf.Print(targets, [targets[0][0], logits[0][0]], \'epxpected and actual spin \')\n            # targets = tf.Print(targets, [targets[0][1], logits[0][1]], \'epxpected and actual direction \')\n            # targets = tf.Print(targets, [targets[0][2], logits[0][2]], \'epxpected and actual speed \')\n            # targets = tf.Print(targets, [targets[0][3], logits[0][3]], \'epxpected and actual speed_change \')\n            # targets = tf.Print(targets, [targets[:, 4]], \'expected steering \', summarize=600)\n            # targets = tf.Print(targets, [logits[:, 4]],  \'actual steering   \', summarize=600)\n            # targets = tf.Print(targets, [targets[0][5], logits[0][5]], \'epxpected and actual throttle \')\n\n            target_delta = logits - targets\n\n            for net_out_i, net_out_name in enumerate(CONTROL_NAMES):\n                delta = target_delta[:, net_out_i]\n                target_delta = tf.Print(target_delta, [delta], net_out_name + \'_delta \', summarize=1000)\n                mean_delta = tf.reduce_mean(tf.abs(delta))\n                tf.summary.scalar(\'deepdrive_error/%s_eval\' % net_out_name, mean_delta)\n                targets = tf.Print(targets, [mean_delta], \'eval %s error \' % net_out_name)\n\n            sq_root_normalized_target_delta = target_delta / targets.shape[1].value ** .5\n            # sq_root_normalized_target_delta = tf.Print(sq_root_normalized_target_delta, [sq_root_normalized_target_delta], \'sq_root_normalized_target_delta \')\n\n            dd_loss = tf.nn.l2_loss(sq_root_normalized_target_delta)\n\n            targets = tf.Print(targets, [dd_loss], \'eval loss \')\n            # targets = tf.Print(targets, [targets, logits], \'targets and logits \', summarize=100)\n            # targets = tf.Print(targets, [targets, logits], \'target_delta \', summarize=100)\n\n            # Define the metrics:\n            names_to_values, names_to_updates = slim.metrics.aggregate_metric_map({\n                \'Accuracy\': slim.metrics.streaming_accuracy(logits, targets)\n            })\n\n        else:\n            predictions = tf.argmax(logits, 1)\n            labels = tf.squeeze(labels)\n\n            # Define the metrics:\n            names_to_values, names_to_updates = slim.metrics.aggregate_metric_map({\n                \'Accuracy\': slim.metrics.streaming_accuracy(predictions, labels),\n                \'Recall_5\': slim.metrics.streaming_recall_at_k(\n                    logits, labels, 5),\n            })\n\n        # Print the summaries to screen.\n        for name, value in names_to_values.items():\n            summary_name = \'eval/%s\' % name\n            op = tf.summary.scalar(summary_name, value, collections=[])\n            op = tf.Print(op, [value], summary_name)\n            tf.add_to_collection(tf.GraphKeys.SUMMARIES, op)\n\n        # TODO(sguada) use num_epochs=1\n        if max_num_batches:\n            num_batches = max_num_batches\n        else:\n            # This ensures that we make a single pass over all of the data.\n            num_batches = math.ceil(dataset.num_samples / float(batch_size))\n\n        if model_name == MOBILENET_V2_SLIM_NAME:\n            if not checkpoint_path:\n                checkpoint_path = max(glob.glob(TENSORFLOW_OUT_DIR + \'/*\'), key=os.path.getmtime)\n\n        if tf.gfile.IsDirectory(checkpoint_path):\n            checkpoint_path = tf.train.latest_checkpoint(checkpoint_path)\n        else:\n            checkpoint_path = checkpoint_path\n\n        tf.logging.info(\'Evaluating %s\' % checkpoint_path)\n\n        slim.evaluation.evaluate_once(\n            master=master,\n            checkpoint_path=checkpoint_path,\n            logdir=eval_dir,\n            num_evals=num_batches,\n            eval_op=list(names_to_updates.values()),\n            variables_to_restore=variables_to_restore)\n\n\nif __name__ == \'__main__\':\n    tf.app.run()'"
vendor/tensorflow/models/research/slim/train_image_nn.py,106,"b'# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Generic training script that trains a model using a given dataset.""""""\n\n# DONE: Export dataset to tf records resized to 224x224\n# - Get features (img, speed, steer, ...), put into example / split train, test, ...\n# DONE: Change prepro not to flip\n# DONE: Change loss to Euclidean\n# DONE: Read tfrecords and get size\n# DONE: See if we need to break tfrecords into more manageably sized chunks\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport glob\nimport os\nfrom datetime import datetime\n\nimport tensorflow as tf\n\nfrom config import TENSORFLOW_OUT_DIR, CONTROL_NAMES, MOBILENET_V2_SLIM_NAME\nfrom vendor.tensorflow.models.research.slim.datasets import dataset_factory\nfrom vendor.tensorflow.models.research.slim.deployment import model_deploy\nfrom vendor.tensorflow.models.research.slim.nets import nets_factory\nfrom vendor.tensorflow.models.research.slim.preprocessing import preprocessing_factory\n\n\nslim = tf.contrib.slim\n\n\ndef create_flags():\n    tf.app.flags.DEFINE_string(\n        \'master\', \'\', \'The address of the TensorFlow master to use.\')\n\n    tf.app.flags.DEFINE_boolean(\'resume_deepdrive\', False,\n                                \'Resume the previous training session in deepdrive.\')\n\n    tf.app.flags.DEFINE_string(\n        \'train_dir\', None,\n        \'Directory where checkpoints and event logs are written to.\')\n\n    tf.app.flags.DEFINE_integer(\'num_clones\', 1,\n                                \'Number of model clones to deploy.\')\n\n    tf.app.flags.DEFINE_boolean(\'clone_on_cpu\', False,\n                                \'Use CPUs to deploy clones.\')\n\n    tf.app.flags.DEFINE_integer(\'worker_replicas\', 1, \'Number of worker replicas.\')\n\n    tf.app.flags.DEFINE_integer(\n        \'num_ps_tasks\', 0,\n        \'The number of parameter servers. If the value is 0, then the parameters \'\n        \'are handled locally by the worker.\')\n\n    tf.app.flags.DEFINE_integer(\n        \'num_readers\', 4,\n        \'The number of parallel readers that read data from the dataset.\')\n\n    tf.app.flags.DEFINE_integer(\n        \'num_preprocessing_threads\', 4,\n        \'The number of threads used to create the batches.\')\n\n    tf.app.flags.DEFINE_integer(\n        \'log_every_n_steps\', 10,\n        \'The frequency with which logs are print.\')\n\n    tf.app.flags.DEFINE_integer(\n        \'save_summaries_secs\', 600,\n        \'The frequency with which summaries are saved, in seconds.\')\n\n    tf.app.flags.DEFINE_integer(\n        \'save_interval_secs\', 600,\n        \'The frequency with which the model is saved, in seconds.\')\n\n    tf.app.flags.DEFINE_integer(\n        \'task\', 0, \'Task id of the replica running the training.\')\n\n    ######################\n    # Optimization Flags #\n    ######################\n\n    tf.app.flags.DEFINE_float(\n        \'weight_decay\', 0.00004, \'The weight decay on the model weights.\')\n\n    tf.app.flags.DEFINE_string(\n        \'optimizer\', \'rmsprop\',\n        \'The name of the optimizer, one of ""adadelta"", ""adagrad"", ""adam"",\'\n        \'""ftrl"", ""momentum"", ""sgd"" or ""rmsprop"".\')\n\n    tf.app.flags.DEFINE_float(\n        \'adadelta_rho\', 0.95,\n        \'The decay rate for adadelta.\')\n\n    tf.app.flags.DEFINE_float(\n        \'adagrad_initial_accumulator_value\', 0.1,\n        \'Starting value for the AdaGrad accumulators.\')\n\n    tf.app.flags.DEFINE_float(\n        \'adam_beta1\', 0.9,\n        \'The exponential decay rate for the 1st moment estimates.\')\n\n    tf.app.flags.DEFINE_float(\n        \'adam_beta2\', 0.999,\n        \'The exponential decay rate for the 2nd moment estimates.\')\n\n    tf.app.flags.DEFINE_float(\'opt_epsilon\', 1.0, \'Epsilon term for the optimizer.\')\n\n    tf.app.flags.DEFINE_float(\'ftrl_learning_rate_power\', -0.5,\n                              \'The learning rate power.\')\n\n    tf.app.flags.DEFINE_float(\n        \'ftrl_initial_accumulator_value\', 0.1,\n        \'Starting value for the FTRL accumulators.\')\n\n    tf.app.flags.DEFINE_float(\n        \'ftrl_l1\', 0.0, \'The FTRL l1 regularization strength.\')\n\n    tf.app.flags.DEFINE_float(\n        \'ftrl_l2\', 0.0, \'The FTRL l2 regularization strength.\')\n\n    tf.app.flags.DEFINE_float(\n        \'momentum\', 0.9,\n        \'The momentum for the MomentumOptimizer and RMSPropOptimizer.\')\n\n    tf.app.flags.DEFINE_float(\'rmsprop_momentum\', 0.9, \'Momentum.\')\n\n    tf.app.flags.DEFINE_float(\'rmsprop_decay\', 0.9, \'Decay term for RMSProp.\')\n\n    #######################\n    # Learning Rate Flags #\n    #######################\n\n    tf.app.flags.DEFINE_string(\n        \'learning_rate_decay_type\',\n        \'exponential\',\n        \'Specifies how the learning rate is decayed. One of ""fixed"", ""exponential"",\'\n        \' or ""polynomial""\')\n\n    tf.app.flags.DEFINE_float(\'learning_rate\', 0.01, \'Initial learning rate.\')\n\n    tf.app.flags.DEFINE_float(\n        \'end_learning_rate\', 0.0001,\n        \'The minimal end learning rate used by a polynomial decay learning rate.\')\n\n    tf.app.flags.DEFINE_float(\n        \'label_smoothing\', 0.0, \'The amount of label smoothing.\')\n\n    tf.app.flags.DEFINE_float(\n        \'learning_rate_decay_factor\', 0.94, \'Learning rate decay factor.\')\n\n    tf.app.flags.DEFINE_float(\n        \'num_epochs_per_decay\', 2.0,\n        \'Number of epochs after which learning rate decays.\')\n\n    tf.app.flags.DEFINE_bool(\n        \'sync_replicas\', False,\n        \'Whether or not to synchronize the replicas during training.\')\n\n    tf.app.flags.DEFINE_integer(\n        \'replicas_to_aggregate\', 1,\n        \'The Number of gradients to collect before updating params.\')\n\n    tf.app.flags.DEFINE_float(\n        \'moving_average_decay\', None,\n        \'The decay to use for the moving average.\'\n        \'If left as None, then moving averages are not used.\')\n\n    #######################\n    # Dataset Flags #\n    #######################\n\n    tf.app.flags.DEFINE_string(\n        \'dataset_name\', \'imagenet\', \'The name of the dataset to load.\')\n\n    tf.app.flags.DEFINE_string(\n        \'dataset_split_name\', \'train\', \'The name of the train/test split.\')\n\n    tf.app.flags.DEFINE_string(\n        \'dataset_dir\', None, \'The directory where the dataset files are stored.\')\n\n    tf.app.flags.DEFINE_integer(\n        \'labels_offset\', 0,\n        \'An offset for the labels in the dataset. This flag is primarily used to \'\n        \'evaluate the VGG and ResNet architectures which do not use a background \'\n        \'class for the ImageNet dataset.\')\n\n    tf.app.flags.DEFINE_string(\n        \'model_name\', \'inception_v3\', \'The name of the architecture to train.\')\n\n    tf.app.flags.DEFINE_string(\n        \'preprocessing_name\', None, \'The name of the preprocessing to use. If left \'\n                                    \'as `None`, then the model_name flag is used.\')\n\n    tf.app.flags.DEFINE_integer(\n        \'batch_size\', 32, \'The number of samples in each batch.\')\n\n    tf.app.flags.DEFINE_integer(\n        \'train_image_size\', None, \'Train image size\')\n\n    tf.app.flags.DEFINE_integer(\'max_number_of_steps\', None,\n                                \'The maximum number of training steps.\')\n\n    #####################\n    # Fine-Tuning Flags #\n    #####################\n\n    tf.app.flags.DEFINE_string(\n        \'checkpoint_path\', None,\n        \'The path to a checkpoint from which to fine-tune.\')\n\n    tf.app.flags.DEFINE_string(\n        \'checkpoint_exclude_scopes\', None,\n        \'Comma-separated list of scopes of variables to exclude when restoring \'\n        \'from a checkpoint.\')\n\n    tf.app.flags.DEFINE_string(\n        \'trainable_scopes\', None,\n        \'Comma-separated list of scopes to filter the set of variables to train.\'\n        \'By default, None would train all the variables.\')\n\n    tf.app.flags.DEFINE_boolean(\n        \'ignore_missing_vars\', False,\n        \'When restoring a checkpoint would ignore missing variables.\')\n\n\ndef _configure_learning_rate(num_samples_per_epoch, global_step, replicas_to_aggregate, batch_size, learning_rate_decay_factor, learning_rate,\n                             end_learning_rate, learning_rate_decay_type, num_epochs_per_decay, sync_replicas):\n    """"""Configures the learning rate.\n\n    Args:\n      num_samples_per_epoch: The number of samples in each epoch of training.\n      global_step: The global_step tensor.\n\n    Returns:\n      A `Tensor` representing the learning rate.\n\n    Raises:\n      ValueError: if\n    """"""\n    decay_steps = int(num_samples_per_epoch / batch_size *\n                      num_epochs_per_decay)\n    if sync_replicas:\n        decay_steps /= replicas_to_aggregate\n\n    if learning_rate_decay_type == \'exponential\':\n        return tf.train.exponential_decay(learning_rate,\n                                          global_step,\n                                          decay_steps,\n                                          learning_rate_decay_factor,\n                                          staircase=True,\n                                          name=\'exponential_decay_learning_rate\')\n    elif learning_rate_decay_type == \'fixed\':\n        return tf.constant(learning_rate, name=\'fixed_learning_rate\')\n    elif learning_rate_decay_type == \'polynomial\':\n        return tf.train.polynomial_decay(learning_rate,\n                                         global_step,\n                                         decay_steps,\n                                         end_learning_rate,\n                                         power=1.0,\n                                         cycle=False,\n                                         name=\'polynomial_decay_learning_rate\')\n    else:\n        raise ValueError(\'learning_rate_decay_type [%s] was not recognized\',\n                         learning_rate_decay_type)\n\n\ndef _configure_optimizer(learning_rate, optimizer, adadelta_rho, adam_beta1, adam_beta2, ftrl_learning_rate_power,\n                         ftrl_initial_accumulator_value, ftrl_l1, ftrl_l2, momentum, rmsprop_decay, rmsprop_momentum,\n                         opt_epsilon, adagrad_initial_accumulator_value):\n    """"""Configures the optimizer used for training.\n\n    Args:\n      learning_rate: A scalar or `Tensor` learning rate.\n\n    Returns:\n      An instance of an optimizer.\n\n    Raises:\n      ValueError: if optimizer is not recognized.\n    """"""\n    if optimizer == \'adadelta\':\n        optimizer = tf.train.AdadeltaOptimizer(\n            learning_rate,\n            rho=adadelta_rho,\n            epsilon=opt_epsilon)\n    elif optimizer == \'adagrad\':\n        optimizer = tf.train.AdagradOptimizer(\n            learning_rate,\n            initial_accumulator_value=adagrad_initial_accumulator_value)\n    elif optimizer == \'adam\':\n        optimizer = tf.train.AdamOptimizer(\n            learning_rate,\n            beta1=adam_beta1,\n            beta2=adam_beta2,\n            epsilon=opt_epsilon)\n    elif optimizer == \'ftrl\':\n        optimizer = tf.train.FtrlOptimizer(\n            learning_rate,\n            learning_rate_power=ftrl_learning_rate_power,\n            initial_accumulator_value=ftrl_initial_accumulator_value,\n            l1_regularization_strength=ftrl_l1,\n            l2_regularization_strength=ftrl_l2)\n    elif optimizer == \'momentum\':\n        optimizer = tf.train.MomentumOptimizer(\n            learning_rate,\n            momentum=momentum,\n            name=\'Momentum\')\n    elif optimizer == \'rmsprop\':\n        optimizer = tf.train.RMSPropOptimizer(\n            learning_rate,\n            decay=rmsprop_decay,\n            momentum=rmsprop_momentum,\n            epsilon=opt_epsilon)\n    elif optimizer == \'sgd\':\n        optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n    else:\n        raise ValueError(\'Optimizer [%s] was not recognized\', optimizer)\n    return optimizer\n\n\ndef _get_init_fn(checkpoint_path, checkpoint_exclude_scopes, ignore_missing_vars, train_dir):\n    """"""Returns a function run by the chief worker to warm-start the training.\n\n    Note that the init_fn is only run when initializing the model during the very\n    first global step.\n\n    Returns:\n      An init function run by the supervisor.\n    """"""\n    if checkpoint_path is None:\n        return None\n\n    # Warn the user if a checkpoint exists in the train_dir. Then we\'ll be\n    # ignoring the checkpoint anyway.\n    if tf.train.latest_checkpoint(train_dir):\n        tf.logging.info(\n            \'Ignoring --checkpoint_path because a checkpoint already exists in %s\'\n            % train_dir)\n        return None\n\n    exclusions = []\n    if checkpoint_exclude_scopes:\n        exclusions = [scope.strip()\n                      for scope in checkpoint_exclude_scopes.split(\',\')]\n\n    # TODO(sguada) variables.filter_variables()\n    variables_to_restore = []\n    for var in slim.get_model_variables():\n        excluded = False\n        for exclusion in exclusions:\n            if var.op.name.startswith(exclusion):\n                excluded = True\n                break\n        if not excluded:\n            variables_to_restore.append(var)\n\n    if tf.gfile.IsDirectory(checkpoint_path):\n        checkpoint_path = tf.train.latest_checkpoint(checkpoint_path)\n    else:\n        checkpoint_path = checkpoint_path\n\n    tf.logging.info(\'Fine-tuning from %s\' % checkpoint_path)\n\n    return slim.assign_from_checkpoint_fn(\n        checkpoint_path,\n        variables_to_restore,\n        ignore_missing_vars=ignore_missing_vars)\n\n\ndef _get_variables_to_train(trainable_scopes):\n    """"""Returns a list of variables to train.\n\n    Returns:\n      A list of variables to train by the optimizer.\n    """"""\n    if trainable_scopes is None:\n        return tf.trainable_variables()\n    else:\n        scopes = [scope.strip() for scope in trainable_scopes.split(\',\')]\n\n    variables_to_train = []\n    for scope in scopes:\n        variables = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope)\n        variables_to_train.extend(variables)\n    return variables_to_train\n\n\ndef main(_):\n    create_flags()\n    FLAGS = tf.app.flags.FLAGS\n    slim_train_image_nn(FLAGS.resume_deepdrive, FLAGS.dataset_name, FLAGS.dataset_split_name, FLAGS.dataset_dir,\n                        FLAGS.model_name, FLAGS.train_image_size, FLAGS.checkpoint_path,\n                        FLAGS.checkpoint_exclude_scopes, FLAGS.trainable_scopes, FLAGS.max_number_of_steps,\n                        FLAGS.batch_size, FLAGS.learning_rate, FLAGS.learning_rate_decay_type, FLAGS.save_interval_secs,\n                        FLAGS.save_summaries_secs, FLAGS.log_every_n_steps, FLAGS.optimizer, FLAGS.weight_decay,\n                        FLAGS.worker_replicas, FLAGS.num_clones, FLAGS.train_dir, FLAGS.clone_on_cpu, FLAGS.task,\n                        FLAGS.num_ps_tasks, FLAGS.sync_replicas, FLAGS.replicas_to_aggregate, FLAGS.label_smoothing,\n                        FLAGS.labels_offset, FLAGS.num_readers, FLAGS.num_preprocessing_threads,\n                        FLAGS.preprocessing_name, FLAGS.master, FLAGS.moving_average_decay, FLAGS.ignore_missing_vars)\n\n\n# TODO: Store these and FLAGS defaults in one place as constants (problem with FLAGS is the convention to mutate them)\ndef slim_train_image_nn(resume_deepdrive=False, dataset_name=\'imagenet\', dataset_split_name=\'train\', dataset_dir=None,\n                        model_name=\'inception_v3\', train_image_size=None, checkpoint_path=None,\n                        checkpoint_exclude_scopes=None, trainable_scopes=None, max_number_of_steps=None, batch_size=32,\n                        learning_rate=0.01, learning_rate_decay_type=\'exponential\', save_interval_secs=600,\n                        save_summaries_secs=600, log_every_n_steps=10, optimizer=\'rmsprop\', weight_decay=0.00004,\n                        worker_replicas=1, num_clones=1, train_dir=None, clone_on_cpu=False,\n                        task=0, num_ps_tasks=0, sync_replicas=False, replicas_to_aggregate=1, label_smoothing=0.0,\n                        labels_offset=0, num_readers=4, num_preprocessing_threads=4, preprocessing_name=None, master=\'\',\n                        moving_average_decay=None, ignore_missing_vars=False, adadelta_rho=0.95, adam_beta1=0.9,\n                        adam_beta2=0.999, ftrl_learning_rate_power=-0.5, ftrl_initial_accumulator_value=0.1,\n                        ftrl_l1=0.0, ftrl_l2=0.0, momentum=0.9, rmsprop_decay=0.9, rmsprop_momentum=0.9,\n                        opt_epsilon=1.0, adagrad_initial_accumulator_value=0.1, learning_rate_decay_factor=0.94,\n                        end_learning_rate=0.0001, num_epochs_per_decay=2.0):\n    if resume_deepdrive:\n        # Mysteriously can not do this due to RMSProp restore error - we resume via checkpoint dir instead\n        # train_dir = max(glob.glob(DEEPDRIVE_TRAIN_PARENT_DIR + \'/*\'), key=os.path.getmtime)\n\n        checkpoint_path = max(glob.glob(TENSORFLOW_OUT_DIR + \'/*\'), key=os.path.getmtime)\n\n    if train_dir is None:\n        train_dir = datetime.now().strftime(os.path.join(TENSORFLOW_OUT_DIR, \'%Y-%m-%d__%I-%M-%S%p\'))\n    else:\n        print(""WARNING: Setting the train_dir currently results in RMSProp restore error - fix by correctly""\n              ""saving graph or restore with `resume_deepdrive` instead"")\n\n    print(\'train dir is\', train_dir)\n\n    if not dataset_dir:\n        raise ValueError(\'You must supply the dataset directory with --dataset_dir\')\n\n    tf.logging.set_verbosity(tf.logging.INFO)\n    with tf.Graph().as_default():\n        #######################\n        # Config model_deploy #\n        #######################\n        deploy_config = model_deploy.DeploymentConfig(\n            num_clones=num_clones,\n            clone_on_cpu=clone_on_cpu,\n            replica_id=task,\n            num_replicas=worker_replicas,\n            num_ps_tasks=num_ps_tasks)\n\n        # Create global_step\n        with tf.device(deploy_config.variables_device()):\n            global_step = slim.create_global_step()\n\n        ######################\n        # Select the dataset #\n        ######################\n        dataset = dataset_factory.get_dataset(\n            dataset_name, dataset_split_name, dataset_dir)\n\n        ######################\n        # Select the network #\n        ######################\n        if model_name == \'mobilenet_v2_deepdrive\':\n            network_fn = nets_factory.get_network_fn(\n                model_name,\n                weight_decay=weight_decay,\n                num_classes=None,\n                num_targets=6,\n                is_training=True, )\n\n        else:\n            network_fn = nets_factory.get_network_fn(\n                model_name,\n                num_classes=(dataset.num_classes - labels_offset),\n                weight_decay=weight_decay,\n                is_training=True)\n\n        #####################################\n        # Select the preprocessing function #\n        #####################################\n        preprocessing_name = preprocessing_name or model_name\n        image_preprocessing_fn = preprocessing_factory.get_preprocessing(\n            preprocessing_name,\n            is_training=True)\n\n        ##############################################################\n        # Create a dataset provider that loads data from the dataset #\n        ##############################################################\n        with tf.device(deploy_config.inputs_device()):\n            provider = slim.dataset_data_provider.DatasetDataProvider(\n                dataset,\n                num_readers=num_readers,\n                common_queue_capacity=20 * batch_size,\n                common_queue_min=10 * batch_size)\n\n            if model_name == \'mobilenet_v2_deepdrive\':\n                [image, spin, direction, speed, speed_change, steering, throttle] = provider.get(\n                    [\'image\', \'spin\', \'direction\', \'speed\', \'speed_change\', \'steering\', \'throttle\'])\n\n                train_image_size = train_image_size or network_fn.default_image_size\n\n                image = image_preprocessing_fn(image, train_image_size, train_image_size)\n\n                images, targets = tf.train.batch(\n                    [image, [spin, direction, speed, speed_change, steering, throttle]],\n                    batch_size=batch_size,\n                    num_threads=num_preprocessing_threads,\n                    capacity=5 * batch_size)\n\n                batch_queue = slim.prefetch_queue.prefetch_queue(\n                    [images, targets], capacity=2 * deploy_config.num_clones)\n            else:\n                [image, label] = provider.get([\'image\', \'label\'])\n                label -= labels_offset\n\n                train_image_size = train_image_size or network_fn.default_image_size\n\n                image = image_preprocessing_fn(image, train_image_size, train_image_size)\n\n                images, labels = tf.train.batch(\n                    [image, label],\n                    batch_size=batch_size,\n                    num_threads=num_preprocessing_threads,\n                    capacity=5 * batch_size)\n                labels = slim.one_hot_encoding(\n                    labels, dataset.num_classes - labels_offset)\n                batch_queue = slim.prefetch_queue.prefetch_queue(\n                    [images, labels], capacity=2 * deploy_config.num_clones)\n\n        ####################\n        # Define the model #\n        ####################\n        def clone_fn(batch_queue):\n            """"""Allows data parallelism by creating multiple clones of network_fn.""""""\n            #############################\n            # Specify the loss function #\n            #############################\n            if model_name == MOBILENET_V2_SLIM_NAME:\n                images, targets = batch_queue.dequeue()\n                logits, end_points = network_fn(images)\n                # targets = tf.Print(targets, [targets[0][0], logits[0][0]], \'epxpected and actual spin \')\n                # targets = tf.Print(targets, [targets[0][1], logits[0][1]], \'epxpected and actual direction \')\n                # targets = tf.Print(targets, [targets[0][2], logits[0][2]], \'epxpected and actual speed \')\n                # targets = tf.Print(targets, [targets[0][3], logits[0][3]], \'epxpected and actual speed_change \')\n                # targets = tf.Print(targets, [targets[0][4], logits[0][4]], \'expected and actual steering \')\n                # targets = tf.Print(targets, [targets[0][5], logits[0][5]], \'epxpected and actual throttle \')\n\n                target_delta = logits - targets\n                # target_delta = tf.Print(target_delta, [target_delta], \'target_delta \')\n\n                for net_out_i, net_out_name in enumerate(CONTROL_NAMES):\n                    delta = target_delta[:, net_out_i]\n                    mean_delta = tf.reduce_mean(tf.abs(delta))\n                    # target_delta = tf.Print(target_delta, [mean_delta], net_out_name + \' error \')\n                    tf.summary.scalar(\'deepdrive_error/%s_train\' % net_out_name, mean_delta)\n\n                sq_root_normalized_target_delta = target_delta / targets.shape[1].value ** .5\n                # sq_root_normalized_target_delta = tf.Print(sq_root_normalized_target_delta, [sq_root_normalized_target_delta], \'sq_root_normalized_target_delta \')\n\n                tf.losses.add_loss(tf.nn.l2_loss(sq_root_normalized_target_delta))\n            else:\n                images, labels = batch_queue.dequeue()\n                logits, end_points = network_fn(images)\n                if \'AuxLogits\' in end_points:\n                    slim.losses.softmax_cross_entropy(\n                        end_points[\'AuxLogits\'], labels,\n                        label_smoothing=label_smoothing, weights=0.4,\n                        scope=\'aux_loss\')\n                slim.losses.softmax_cross_entropy(\n                    logits, labels, label_smoothing=label_smoothing, weights=1.0)\n            return end_points\n\n        # Gather initial summaries.\n        summaries = set(tf.get_collection(tf.GraphKeys.SUMMARIES))\n\n        clones = model_deploy.create_clones(deploy_config, clone_fn, [batch_queue])\n        first_clone_scope = deploy_config.clone_scope(0)\n        # Gather update_ops from the first clone. These contain, for example,\n        # the updates for the batch_norm variables created by network_fn.\n        update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS, first_clone_scope)\n\n        # Add summaries for end_points.\n        end_points = clones[0].outputs\n        for end_point in end_points:\n            x = end_points[end_point]\n            summaries.add(tf.summary.histogram(\'activations/\' + end_point, x))\n            summaries.add(tf.summary.scalar(\'sparsity/\' + end_point,\n                                            tf.nn.zero_fraction(x)))\n\n        # Add summaries for losses.\n        for loss in tf.get_collection(tf.GraphKeys.LOSSES, first_clone_scope):\n            summaries.add(tf.summary.scalar(\'losses/%s\' % loss.op.name, loss))\n\n        # Add summaries for variables.\n        for variable in slim.get_model_variables():\n            summaries.add(tf.summary.histogram(variable.op.name, variable))\n\n        #################################\n        # Configure the moving averages #\n        #################################\n        if moving_average_decay:\n            moving_average_variables = slim.get_model_variables()\n            variable_averages = tf.train.ExponentialMovingAverage(\n                moving_average_decay, global_step)\n        else:\n            moving_average_variables, variable_averages = None, None\n\n        #########################################\n        # Configure the optimization procedure. #\n        #########################################\n        with tf.device(deploy_config.optimizer_device()):\n            learning_rate = _configure_learning_rate(dataset.num_samples, global_step, replicas_to_aggregate,\n                                                     batch_size, learning_rate_decay_factor, learning_rate,\n                                                     end_learning_rate, learning_rate_decay_type, num_epochs_per_decay,\n                                                     sync_replicas)\n            # noinspection PyPep8\n            optimizer = _configure_optimizer(learning_rate, optimizer, adadelta_rho, adam_beta1, adam_beta2, ftrl_learning_rate_power,\n                         ftrl_initial_accumulator_value, ftrl_l1, ftrl_l2, momentum, rmsprop_decay, rmsprop_momentum,\n                         opt_epsilon, adagrad_initial_accumulator_value)\n            summaries.add(tf.summary.scalar(\'learning_rate\', learning_rate))\n\n        if sync_replicas:\n            # If sync_replicas is enabled, the averaging will be done in the chief\n            # queue runner.\n            optimizer = tf.train.SyncReplicasOptimizer(\n                opt=optimizer,\n                replicas_to_aggregate=replicas_to_aggregate,\n                total_num_replicas=worker_replicas,\n                variable_averages=variable_averages,\n                variables_to_average=moving_average_variables)\n        elif moving_average_decay:\n            # Update ops executed locally by trainer.\n            update_ops.append(variable_averages.apply(moving_average_variables))\n\n        # Variables to train.\n        variables_to_train = _get_variables_to_train(trainable_scopes)\n\n        #  and returns a train_tensor and summary_op\n        total_loss, clones_gradients = model_deploy.optimize_clones(\n            clones,\n            optimizer,\n            var_list=variables_to_train)\n        # Add total_loss to summary.\n        summaries.add(tf.summary.scalar(\'total_loss\', total_loss))\n\n        # Create gradient updates.\n        grad_updates = optimizer.apply_gradients(clones_gradients,\n                                                 global_step=global_step)\n        update_ops.append(grad_updates)\n\n        update_op = tf.group(*update_ops)\n        with tf.control_dependencies([update_op]):\n            train_tensor = tf.identity(total_loss, name=\'train_op\')\n\n        # Add the summaries from the first clone. These contain the summaries\n        # created by model_fn and either optimize_clones() or _gather_clone_loss().\n        summaries |= set(tf.get_collection(tf.GraphKeys.SUMMARIES,\n                                           first_clone_scope))\n\n        # Merge all summaries together.\n        summary_op = tf.summary.merge(list(summaries), name=\'summary_op\')\n\n        # Add config to avoid \'could not satisfy explicit device\' problem\n        sess_config = tf.ConfigProto(allow_soft_placement=True)\n\n        ###########################\n        # Kicks off the training. #\n        ###########################\n        slim.learning.train(\n            train_tensor,\n            logdir=train_dir,\n            master=master,\n            is_chief=(task == 0),\n            init_fn=_get_init_fn(checkpoint_path, checkpoint_exclude_scopes, ignore_missing_vars, train_dir),\n            summary_op=summary_op,\n            number_of_steps=max_number_of_steps,\n            log_every_n_steps=log_every_n_steps,\n            save_summaries_secs=save_summaries_secs,\n            save_interval_secs=save_interval_secs,\n            sync_optimizer=optimizer if sync_replicas else None,\n            session_config=sess_config)\n\n        return train_dir\n\n\nif __name__ == \'__main__\':\n    tf.app.run()'"
vendor/tensorflow/models/research/slim/utils.py,2,"b""import os\nimport pkgutil\nimport sys\n\nimport absl\nfrom absl.flags import FlagValues\n\nimport tensorflow as tf\nfrom six.moves import reload_module\n\n\ndef commandeer_tf_flags(create_flags, kwargs):\n    pkgpath = os.path.dirname(absl.flags.__file__)\n    for _, name, _ in pkgutil.walk_packages([pkgpath]):\n        reload_module(sys.modules['absl.flags.' + name])\n\n    # Hack to reset global flags\n    reload_module(absl.flags._flagvalues)\n    reload_module(absl.flags._defines)\n    reload_module(absl.flags)\n    reload_module(tf.flags)\n\n    tf.app.flags.DEFINE_integer\n\n    flags = create_flags()\n    flags.__dict__['__wrapped']([sys.argv[0]])  # Hack to avoid parsing sys.argv in flags\n    for k in kwargs:\n        setattr(flags, k, kwargs[k])\n"""
vendor/tensorflow/models/research/tensorrt/tensorrt.py,24,"b'# Copyright 2018 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Methods for running the Official Models with TensorRT.\n\nPlease note that all of these methods are in development, and subject to\nrapid change.\n""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport argparse\nimport imghdr\nimport json\nimport os\nimport sys\nimport time\n\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow.contrib.saved_model.python.saved_model import reader\nimport tensorflow.contrib.tensorrt as trt\n\nfrom official.resnet import imagenet_preprocessing  # pylint: disable=g-bad-import-order\n\n_GPU_MEM_FRACTION = 0.50\n_WARMUP_NUM_LOOPS = 5\n_LOG_FILE = ""log.txt""\n_LABELS_FILE = ""labellist.json""\n_GRAPH_FILE = ""frozen_graph.pb""\n\n\n################################################################################\n# Prep the image input to the graph.\n################################################################################\ndef preprocess_image(file_name, output_height=224, output_width=224,\n                     num_channels=3):\n  """"""Run standard ImageNet preprocessing on the passed image file.\n\n  Args:\n    file_name: string, path to file containing a JPEG image\n    output_height: int, final height of image\n    output_width: int, final width of image\n    num_channels: int, depth of input image\n\n  Returns:\n    Float array representing processed image with shape\n      [output_height, output_width, num_channels]\n\n  Raises:\n    ValueError: if image is not a JPEG.\n  """"""\n  if imghdr.what(file_name) != ""jpeg"":\n    raise ValueError(""At this time, only JPEG images are supported. ""\n                     ""Please try another image."")\n\n  image_buffer = tf.read_file(file_name)\n  normalized = imagenet_preprocessing.preprocess_image(\n      image_buffer=image_buffer,\n      bbox=None,\n      output_height=output_height,\n      output_width=output_width,\n      num_channels=num_channels,\n      is_training=False)\n\n  with tf.Session(config=get_gpu_config()) as sess:\n    result = sess.run([normalized])\n\n  return result[0]\n\n\ndef batch_from_image(file_name, batch_size, output_height=224, output_width=224,\n                     num_channels=3):\n  """"""Produce a batch of data from the passed image file.\n\n  Args:\n    file_name: string, path to file containing a JPEG image\n    batch_size: int, the size of the desired batch of data\n    output_height: int, final height of data\n    output_width: int, final width of data\n    num_channels: int, depth of input data\n\n  Returns:\n    Float array representing copies of the image with shape\n      [batch_size, output_height, output_width, num_channels]\n  """"""\n  image_array = preprocess_image(\n      file_name, output_height, output_width, num_channels)\n\n  tiled_array = np.tile(image_array, [batch_size, 1, 1, 1])\n  return tiled_array\n\n\ndef batch_from_random(batch_size, output_height=224, output_width=224,\n                      num_channels=3):\n  """"""Produce a batch of random data.\n\n  Args:\n    batch_size: int, the size of the desired batch of data\n    output_height: int, final height of data\n    output_width: int, final width of data\n    num_channels: int, depth of output data\n\n  Returns:\n    Float array containing random numbers with shape\n      [batch_size, output_height, output_width, num_channels]\n  """"""\n  shape = [batch_size, output_height, output_width, num_channels]\n  # Make sure we return float32, as float64 will not get cast automatically.\n  return np.random.random_sample(shape).astype(np.float32)\n\n\n################################################################################\n# Utils for handling Frozen Graphs.\n################################################################################\ndef get_serving_meta_graph_def(savedmodel_dir):\n  """"""Extract the SERVING MetaGraphDef from a SavedModel directory.\n\n  Args:\n    savedmodel_dir: the string path to the directory containing the .pb\n      and variables for a SavedModel. This is equivalent to the subdirectory\n      that is created under the directory specified by --export_dir when\n      running an Official Model.\n\n  Returns:\n    MetaGraphDef that should be used for tag_constants.SERVING mode.\n\n  Raises:\n    ValueError: if a MetaGraphDef matching tag_constants.SERVING is not found.\n  """"""\n  # We only care about the serving graph def\n  tag_set = set([tf.saved_model.tag_constants.SERVING])\n  serving_graph_def = None\n  saved_model = reader.read_saved_model(savedmodel_dir)\n  for meta_graph_def in saved_model.meta_graphs:\n    if set(meta_graph_def.meta_info_def.tags) == tag_set:\n      serving_graph_def = meta_graph_def\n  if not serving_graph_def:\n    raise ValueError(""No MetaGraphDef found for tag_constants.SERVING. ""\n                     ""Please make sure the SavedModel includes a SERVING def."")\n\n  return serving_graph_def\n\n\ndef write_graph_to_file(graph_name, graph_def, output_dir):\n  """"""Write Frozen Graph file to disk.""""""\n  output_path = os.path.join(output_dir, graph_name)\n  with tf.gfile.GFile(output_path, ""wb"") as f:\n    f.write(graph_def.SerializeToString())\n\n\ndef convert_savedmodel_to_frozen_graph(savedmodel_dir, output_dir):\n  """"""Convert a SavedModel to a Frozen Graph.\n\n  A SavedModel includes a `variables` directory with variable values,\n  and a specification of the MetaGraph in a ProtoBuffer file. A Frozen Graph\n  takes the variable values and inserts them into the graph, such that the\n  SavedModel is all bundled into a single file. TensorRT and TFLite both\n  leverage Frozen Graphs. Here, we provide a simple utility for converting\n  a SavedModel into a frozen graph for use with these other tools.\n\n  Args:\n    savedmodel_dir: the string path to the directory containing the .pb\n      and variables for a SavedModel. This is equivalent to the subdirectory\n      that is created under the directory specified by --export_dir when\n      running an Official Model.\n    output_dir: string representing path to the output directory for saving\n      the frozen graph.\n\n  Returns:\n    Frozen Graph definition for use.\n  """"""\n  meta_graph = get_serving_meta_graph_def(savedmodel_dir)\n  signature_def = tf.contrib.saved_model.get_signature_def_by_key(\n      meta_graph,\n      tf.saved_model.signature_constants.DEFAULT_SERVING_SIGNATURE_DEF_KEY)\n\n  # TODO(py27): dict.iterkeys(), dict.iteritems() and dict.itervalues() methods are not available in py3\n  outputs = [v.name for v in signature_def.outputs.itervalues()]\n  output_names = [node.split("":"")[0] for node in outputs]\n\n  graph = tf.Graph()\n  with tf.Session(graph=graph) as sess:\n    tf.saved_model.loader.load(\n        sess, meta_graph.meta_info_def.tags, savedmodel_dir)\n    frozen_graph_def = tf.graph_util.convert_variables_to_constants(\n        sess, graph.as_graph_def(), output_names)\n\n  write_graph_to_file(_GRAPH_FILE, frozen_graph_def, output_dir)\n\n  return frozen_graph_def\n\n\ndef get_frozen_graph(graph_file):\n  """"""Read Frozen Graph file from disk.""""""\n  with tf.gfile.FastGFile(graph_file, ""rb"") as f:\n    graph_def = tf.GraphDef()\n    graph_def.ParseFromString(f.read())\n  return graph_def\n\n\ndef get_tftrt_name(graph_name, precision_string):\n  return ""tftrt_{}_{}"".format(precision_string.lower(), graph_name)\n\n\ndef get_trt_graph(graph_name, graph_def, precision_mode, output_dir,\n                  output_node, batch_size=128, workspace_size=1<<30):\n  """"""Create and save inference graph using the TensorRT library.\n\n  Args:\n    graph_name: string, name of the graph to be used for saving.\n    graph_def: GraphDef, the Frozen Graph to be converted.\n    precision_mode: string, the precision that TensorRT should convert into.\n      Options- FP32, FP16, INT8.\n    output_dir: string, the path to where files should be written.\n    output_node: string, the names of the output node that will\n      be returned during inference.\n    batch_size: int, the number of examples that will be predicted at a time.\n    workspace_size: long, size in bytes that can be used during conversion.\n\n  Returns:\n    GraphDef for the TensorRT inference graph.\n  """"""\n  trt_graph = trt.create_inference_graph(\n      graph_def, [output_node], max_batch_size=batch_size,\n      max_workspace_size_bytes=workspace_size,\n      precision_mode=precision_mode)\n\n  write_graph_to_file(graph_name, trt_graph, output_dir)\n\n  return trt_graph\n\n\ndef get_trt_graph_from_calib(graph_name, calib_graph_def, output_dir):\n  """"""Convert a TensorRT graph used for calibration to an inference graph.""""""\n  trt_graph = trt.calib_graph_to_infer_graph(calib_graph_def)\n  write_graph_to_file(graph_name, trt_graph, output_dir)\n  return trt_graph\n\n\n################################################################################\n# Run the graph in various precision modes.\n################################################################################\ndef get_gpu_config():\n  """"""Share GPU memory between image preprocessing and inference.""""""\n  gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=_GPU_MEM_FRACTION)\n  return tf.ConfigProto(gpu_options=gpu_options)\n\n\ndef get_iterator(data):\n  """"""Wrap numpy data in a dataset.""""""\n  dataset = tf.data.Dataset.from_tensors(data).repeat()\n  return dataset.make_one_shot_iterator()\n\n\ndef time_graph(graph_def, data, input_node, output_node, num_loops=100):\n  """"""Run and time the inference graph.\n\n  This function sets up the input and outputs for inference, warms up by\n  running inference for _WARMUP_NUM_LOOPS, then times inference for num_loops\n  loops.\n\n  Args:\n    graph_def: GraphDef, the graph to be timed.\n    data: ndarray of shape [batch_size, height, width, depth], data to be\n      predicted.\n    input_node: string, the label of the input node where data will enter the\n      graph.\n    output_node: string, the names of the output node that will\n      be returned during inference.\n    num_loops: int, number of batches that should run through for timing.\n\n  Returns:\n    A tuple consisting of a list of num_loops inference times, and the\n    predictions that were output for the batch.\n  """"""\n  tf.logging.info(""Starting execution"")\n\n  tf.reset_default_graph()\n  g = tf.Graph()\n\n  with g.as_default():\n    iterator = get_iterator(data)\n    return_tensors = tf.import_graph_def(\n        graph_def=graph_def,\n        input_map={input_node: iterator.get_next()},\n        return_elements=[output_node]\n    )\n    # Unwrap the returned output node. For now, we assume we only\n    # want the tensor with index `:0`, which is the 0th element of the\n    # `.outputs` list.\n    output = return_tensors[0].outputs[0]\n\n  timings = []\n  with tf.Session(graph=g, config=get_gpu_config()) as sess:\n    tf.logging.info(""Starting Warmup cycle"")\n\n    for _ in range(_WARMUP_NUM_LOOPS):\n      sess.run([output])\n\n    tf.logging.info(""Starting timing."")\n\n    for _ in range(num_loops):\n      tstart = time.time()\n      val = sess.run([output])\n      timings.append(time.time() - tstart)\n\n    tf.logging.info(""Timing loop done!"")\n\n  return timings, val[0]\n\n\ndef log_stats(graph_name, log_buffer, timings, batch_size):\n  """"""Write stats to the passed log_buffer.\n\n  Args:\n    graph_name: string, name of the graph to be used for reporting.\n    log_buffer: filehandle, log file opened for appending.\n    timings: list of floats, times produced for multiple runs that will be\n      used for statistic calculation\n    batch_size: int, number of examples per batch\n  """"""\n  times = np.array(timings)\n  steps = len(times)\n  speeds = batch_size / times\n  time_mean = np.mean(times)\n  time_med = np.median(times)\n  time_99th = np.percentile(times, 99)\n  time_99th_uncertainty = np.abs(np.percentile(times[0::2], 99) -\n                                 np.percentile(times[1::2], 99))\n  speed_mean = np.mean(speeds)\n  speed_med = np.median(speeds)\n  speed_uncertainty = np.std(speeds, ddof=1) / np.sqrt(float(steps))\n  speed_jitter = 1.4826 * np.median(np.abs(speeds - np.median(speeds)))\n\n  msg = (""\\n==========================\\n""\n         ""network: %s,\\t batchsize %d, steps %d\\n""\n         ""  fps \\tmedian: %.1f, \\tmean: %.1f, \\tuncertainty: %.1f, \\tjitter: %.1f\\n""  # pylint: disable=line-too-long\n         ""  latency \\tmedian: %.5f, \\tmean: %.5f, \\t99th_p: %.5f, \\t99th_uncertainty: %.5f\\n""  # pylint: disable=line-too-long\n        ) % (graph_name, batch_size, steps,\n             speed_med, speed_mean, speed_uncertainty, speed_jitter,\n             time_med, time_mean, time_99th, time_99th_uncertainty)\n\n  log_buffer.write(msg)\n\n\ndef time_and_log_graph(graph_name, graph_def, data, log_buffer, flags):\n  timings, result = time_graph(\n      graph_def, data, flags.input_node, flags.output_node, flags.num_loops)\n  log_stats(graph_name, log_buffer, timings, flags.batch_size)\n\n  return result\n\n\ndef run_trt_graph_for_mode(\n    graph_name, graph_def, mode, data, log_buffer, flags):\n  """"""Convert, time, and log the graph at `mode` precision using TensorRT.""""""\n  g_name = get_tftrt_name(graph_name, mode)\n  graph = get_trt_graph(\n      g_name, graph_def, mode, flags.output_dir, flags.output_node,\n      flags.batch_size, flags.workspace_size)\n  result = time_and_log_graph(g_name, graph, data, log_buffer, flags)\n  return result\n\n\n################################################################################\n# Parse predictions\n################################################################################\ndef get_labels():\n  """"""Get the set of possible labels for classification.""""""\n  with open(_LABELS_FILE, ""r"") as labels_file:\n    labels = json.load(labels_file)\n\n  return labels\n\n\ndef top_predictions(result, n):\n  """"""Get the top n predictions given the array of softmax results.""""""\n  # We only care about the first example.\n  probabilities = result[0]\n  # Get the ids of most probable labels. Reverse order to get greatest first.\n  ids = np.argsort(probabilities)[::-1]\n  return ids[:n]\n\n\ndef get_labels_for_ids(labels, ids, ids_are_one_indexed=False):\n  """"""Get the human-readable labels for given ids.\n\n  Args:\n    labels: dict, string-ID to label mapping from ImageNet.\n    ids: list of ints, IDs to return labels for.\n    ids_are_one_indexed: whether to increment passed IDs by 1 to account for\n      the background category. See ArgParser `--ids_are_one_indexed`\n      for details.\n\n  Returns:\n    list of category labels\n  """"""\n  return [labels[str(x + int(ids_are_one_indexed))] for x in ids]\n\n\ndef print_predictions(results, ids_are_one_indexed=False, preds_to_print=5):\n  """"""Given an array of mode, graph_name, predicted_ID, print labels.""""""\n  labels = get_labels()\n\n  print(""Predictions:"")\n  for mode, result in results:\n    pred_ids = top_predictions(result, preds_to_print)\n    pred_labels = get_labels_for_ids(labels, pred_ids, ids_are_one_indexed)\n    print(""Precision: "", mode, pred_labels)\n\n\n################################################################################\n# Run this script\n################################################################################\ndef main(argv):\n  parser = TensorRTParser()\n  flags = parser.parse_args(args=argv[1:])\n\n  # Load the data.\n  if flags.image_file:\n    data = batch_from_image(flags.image_file, flags.batch_size)\n  else:\n    data = batch_from_random(flags.batch_size)\n\n  # Load the graph def\n  if flags.frozen_graph:\n    frozen_graph_def = get_frozen_graph(flags.frozen_graph)\n  elif flags.savedmodel_dir:\n    frozen_graph_def = convert_savedmodel_to_frozen_graph(\n        flags.savedmodel_dir, flags.output_dir)\n  else:\n    raise ValueError(\n        ""Either a Frozen Graph file or a SavedModel must be provided."")\n\n  # Get a name for saving TensorRT versions of the graph.\n  graph_name = os.path.basename(flags.frozen_graph or _GRAPH_FILE)\n\n  # Write to a single file for all tests, continuing from previous logs.\n  log_buffer = open(os.path.join(flags.output_dir, _LOG_FILE), ""a"")\n\n  # Run inference in all desired modes.\n  results = []\n  if flags.native:\n    mode = ""native""\n    print(""Running {} graph"".format(mode))\n    g_name = ""{}_{}"".format(mode, graph_name)\n    result = time_and_log_graph(\n        g_name, frozen_graph_def, data, log_buffer, flags)\n    results.append((mode, result))\n\n  if flags.fp32:\n    mode = ""FP32""\n    print(""Running {} graph"".format(mode))\n    result = run_trt_graph_for_mode(\n        graph_name, frozen_graph_def, mode, data, log_buffer, flags)\n    results.append((mode, result))\n\n  if flags.fp16:\n    mode = ""FP16""\n    print(""Running {} graph"".format(mode))\n    result = run_trt_graph_for_mode(\n        graph_name, frozen_graph_def, mode, data, log_buffer, flags)\n    results.append((mode, result))\n\n  if flags.int8:\n    mode = ""INT8""\n    print(""Running {} graph"".format(mode))\n    save_name = get_tftrt_name(graph_name, ""INT8_calib"")\n    calib_graph = get_trt_graph(\n        save_name, frozen_graph_def, mode, flags.output_dir, flags.output_node,\n        flags.batch_size, flags.workspace_size)\n    time_graph(calib_graph, data, flags.input_node, flags.output_node,\n               num_loops=1)\n\n    g_name = get_tftrt_name(graph_name, mode)\n    int8_graph = get_trt_graph_from_calib(g_name, calib_graph, flags.output_dir)\n    result = time_and_log_graph(g_name, int8_graph, data, log_buffer, flags)\n    results.append((mode, result))\n\n  # Print prediction results to the command line.\n  print_predictions(\n      results, flags.ids_are_one_indexed, flags.predictions_to_print)\n\n\nclass TensorRTParser(argparse.ArgumentParser):\n  """"""Parser to contain flags for running the TensorRT timers.""""""\n\n  def __init__(self):\n    super(TensorRTParser, self).__init__()\n\n    self.add_argument(\n        ""--frozen_graph"", ""-fg"", default=None,\n        help=""[default: %(default)s] The location of a Frozen Graph ""\n        ""protobuf file that will be used for inference. Note that either ""\n        ""savedmodel_dir or frozen_graph should be passed in, and ""\n        ""frozen_graph will take precedence."",\n        metavar=""<FG>"",\n    )\n\n    self.add_argument(\n        ""--savedmodel_dir"", ""-sd"", default=None,\n        help=""[default: %(default)s] The location of a SavedModel directory ""\n        ""to be converted into a Frozen Graph. This is equivalent to the ""\n        ""subdirectory that is created under the directory specified by ""\n        ""--export_dir when running an Official Model. Note that either ""\n        ""savedmodel_dir or frozen_graph should be passed in, and ""\n        ""frozen_graph will take precedence."",\n        metavar=""<SD>"",\n    )\n\n    self.add_argument(\n        ""--output_dir"", ""-od"", default=""/tmp"",\n        help=""[default: %(default)s] The location where output files will ""\n        ""be saved."",\n        metavar=""<OD>"",\n    )\n\n    self.add_argument(\n        ""--output_node"", ""-on"", default=""softmax_tensor"",\n        help=""[default: %(default)s] The names of the graph output node ""\n        ""that should be used when retrieving results. Assumed to be a softmax."",\n        metavar=""<ON>"",\n    )\n\n    self.add_argument(\n        ""--input_node"", ""-in"", default=""input_tensor"",\n        help=""[default: %(default)s] The name of the graph input node where ""\n        ""the float image array should be fed for prediction."",\n        metavar=""<ON>"",\n    )\n\n    self.add_argument(\n        ""--batch_size"", ""-bs"", type=int, default=128,\n        help=""[default: %(default)s] Batch size for inference. If an ""\n        ""image file is passed, it will be copied batch_size times to ""\n        ""imitate a batch."",\n        metavar=""<BS>""\n    )\n\n    self.add_argument(\n        ""--image_file"", ""-if"", default=None,\n        help=""[default: %(default)s] The location of a JPEG image that will ""\n        ""be passed in for inference. This will be copied batch_size times to ""\n        ""imitate a batch. If not passed, random data will be used."",\n        metavar=""<IF>"",\n    )\n\n    self.add_argument(\n        ""--native"", action=""store_true"",\n        help=""[default: %(default)s] If set, benchmark the model ""\n        ""with it\'s native precision and without TensorRT.""\n    )\n\n    self.add_argument(\n        ""--fp32"", action=""store_true"",\n        help=""[default: %(default)s] If set, benchmark the model with TensorRT ""\n        ""using fp32 precision.""\n    )\n\n    self.add_argument(\n        ""--fp16"", action=""store_true"",\n        help=""[default: %(default)s] If set, benchmark the model with TensorRT ""\n        ""using fp16 precision.""\n    )\n\n    self.add_argument(\n        ""--int8"", action=""store_true"",\n        help=""[default: %(default)s] If set, benchmark the model with TensorRT ""\n        ""using int8 precision.""\n    )\n\n    self.add_argument(\n        ""--num_loops"", ""-nl"", type=int, default=100,\n        help=""[default: %(default)s] Number of inferences to time per ""\n        ""benchmarked model."",\n        metavar=""<NL>""\n    )\n\n    self.add_argument(\n        ""--workspace_size"", ""-ws"", type=long, default=2<<30,\n        help=""[default: %(default)s] Workspace size in bytes."",\n        metavar=""<WS>""\n    )\n\n    self.add_argument(\n        ""--ids_are_one_indexed"", action=""store_true"",\n        help=""[default: %(default)s] Some ResNet models include a `background` ""\n        ""category, and others do not. If the model used includes `background` ""\n        ""at index 0 in the output and represents all 1001 categories, ""\n        ""this should be False. If the model used omits the `background` label ""\n        ""and has only 1000 categories, this should be True.""\n    )\n\n    self.add_argument(\n        ""--predictions_to_print"", ""-pp"", type=int, default=5,\n        help=""[default: %(default)s] Number of predicted labels to predict."",\n        metavar=""<PP>""\n    )\n\n\nif __name__ == ""__main__"":\n  tf.logging.set_verbosity(tf.logging.INFO)\n  main(argv=sys.argv)\n'"
vendor/tensorflow/models/research/slim/datasets/__init__.py,0,b'\n'
vendor/tensorflow/models/research/slim/datasets/build_imagenet_data.py,31,"b'# Copyright 2016 Google Inc. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Converts ImageNet data to TFRecords file format with Example protos.\n\nThe raw ImageNet data set is expected to reside in JPEG files located in the\nfollowing directory structure.\n\n  data_dir/n01440764/ILSVRC2012_val_00000293.JPEG\n  data_dir/n01440764/ILSVRC2012_val_00000543.JPEG\n  ...\n\nwhere \'n01440764\' is the unique synset label associated with\nthese images.\n\nThe training data set consists of 1000 sub-directories (i.e. labels)\neach containing 1200 JPEG images for a total of 1.2M JPEG images.\n\nThe evaluation data set consists of 1000 sub-directories (i.e. labels)\neach containing 50 JPEG images for a total of 50K JPEG images.\n\nThis TensorFlow script converts the training and evaluation data into\na sharded data set consisting of 1024 and 128 TFRecord files, respectively.\n\n  train_directory/train-00000-of-01024\n  train_directory/train-00001-of-01024\n  ...\n  train_directory/train-00127-of-01024\n\nand\n\n  validation_directory/validation-00000-of-00128\n  validation_directory/validation-00001-of-00128\n  ...\n  validation_directory/validation-00127-of-00128\n\nEach validation TFRecord file contains ~390 records. Each training TFREcord\nfile contains ~1250 records. Each record within the TFRecord file is a\nserialized Example proto. The Example proto contains the following fields:\n\n  image/encoded: string containing JPEG encoded image in RGB colorspace\n  image/height: integer, image height in pixels\n  image/width: integer, image width in pixels\n  image/colorspace: string, specifying the colorspace, always \'RGB\'\n  image/channels: integer, specifying the number of channels, always 3\n  image/format: string, specifying the format, always\'JPEG\'\n\n  image/filename: string containing the basename of the image file\n            e.g. \'n01440764_10026.JPEG\' or \'ILSVRC2012_val_00000293.JPEG\'\n  image/class/label: integer specifying the index in a classification layer.\n    The label ranges from [1, 1000] where 0 is not used.\n  image/class/synset: string specifying the unique ID of the label,\n    e.g. \'n01440764\'\n  image/class/text: string specifying the human-readable version of the label\n    e.g. \'red fox, Vulpes vulpes\'\n\n  image/object/bbox/xmin: list of integers specifying the 0+ human annotated\n    bounding boxes\n  image/object/bbox/xmax: list of integers specifying the 0+ human annotated\n    bounding boxes\n  image/object/bbox/ymin: list of integers specifying the 0+ human annotated\n    bounding boxes\n  image/object/bbox/ymax: list of integers specifying the 0+ human annotated\n    bounding boxes\n  image/object/bbox/label: integer specifying the index in a classification\n    layer. The label ranges from [1, 1000] where 0 is not used. Note this is\n    always identical to the image label.\n\nNote that the length of xmin is identical to the length of xmax, ymin and ymax\nfor each example.\n\nRunning this script using 16 threads may take around ~2.5 hours on a HP Z420.\n""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom datetime import datetime\nimport os\nimport random\nimport sys\nimport threading\n\nimport numpy as np\nfrom six.moves import xrange  # pylint: disable=redefined-builtin\nimport tensorflow as tf\n\n\ntf.app.flags.DEFINE_string(\'train_directory\', \'/tmp/\',\n                           \'Training data directory\')\ntf.app.flags.DEFINE_string(\'validation_directory\', \'/tmp/\',\n                           \'Validation data directory\')\ntf.app.flags.DEFINE_string(\'output_directory\', \'/tmp/\',\n                           \'Output data directory\')\n\ntf.app.flags.DEFINE_integer(\'train_shards\', 1024,\n                            \'Number of shards in training TFRecord files.\')\ntf.app.flags.DEFINE_integer(\'validation_shards\', 128,\n                            \'Number of shards in validation TFRecord files.\')\n\ntf.app.flags.DEFINE_integer(\'num_threads\', 8,\n                            \'Number of threads to preprocess the images.\')\n\n# The labels file contains a list of valid labels are held in this file.\n# Assumes that the file contains entries as such:\n#   n01440764\n#   n01443537\n#   n01484850\n# where each line corresponds to a label expressed as a synset. We map\n# each synset contained in the file to an integer (based on the alphabetical\n# ordering). See below for details.\ntf.app.flags.DEFINE_string(\'labels_file\',\n                           \'imagenet_lsvrc_2015_synsets.txt\',\n                           \'Labels file\')\n\n# This file containing mapping from synset to human-readable label.\n# Assumes each line of the file looks like:\n#\n#   n02119247    black fox\n#   n02119359    silver fox\n#   n02119477    red fox, Vulpes fulva\n#\n# where each line corresponds to a unique mapping. Note that each line is\n# formatted as <synset>\\t<human readable label>.\ntf.app.flags.DEFINE_string(\'imagenet_metadata_file\',\n                           \'imagenet_metadata.txt\',\n                           \'ImageNet metadata file\')\n\n# This file is the output of process_bounding_box.py\n# Assumes each line of the file looks like:\n#\n#   n00007846_64193.JPEG,0.0060,0.2620,0.7545,0.9940\n#\n# where each line corresponds to one bounding box annotation associated\n# with an image. Each line can be parsed as:\n#\n#   <JPEG file name>, <xmin>, <ymin>, <xmax>, <ymax>\n#\n# Note that there might exist mulitple bounding box annotations associated\n# with an image file.\ntf.app.flags.DEFINE_string(\'bounding_box_file\',\n                           \'./imagenet_2012_bounding_boxes.csv\',\n                           \'Bounding box file\')\n\nFLAGS = tf.app.flags.FLAGS\n\n\ndef _int64_feature(value):\n  """"""Wrapper for inserting int64 features into Example proto.""""""\n  if not isinstance(value, list):\n    value = [value]\n  return tf.train.Feature(int64_list=tf.train.Int64List(value=value))\n\n\ndef _float_feature(value):\n  """"""Wrapper for inserting float features into Example proto.""""""\n  if not isinstance(value, list):\n    value = [value]\n  return tf.train.Feature(float_list=tf.train.FloatList(value=value))\n\n\ndef _bytes_feature(value):\n  """"""Wrapper for inserting bytes features into Example proto.""""""\n  return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))\n\n\ndef _convert_to_example(filename, image_buffer, label, synset, human, bbox,\n                        height, width):\n  """"""Build an Example proto for an example.\n\n  Args:\n    filename: string, path to an image file, e.g., \'/path/to/example.JPG\'\n    image_buffer: string, JPEG encoding of RGB image\n    label: integer, identifier for the ground truth for the network\n    synset: string, unique WordNet ID specifying the label, e.g., \'n02323233\'\n    human: string, human-readable label, e.g., \'red fox, Vulpes vulpes\'\n    bbox: list of bounding boxes; each box is a list of integers\n      specifying [xmin, ymin, xmax, ymax]. All boxes are assumed to belong to\n      the same label as the image label.\n    height: integer, image height in pixels\n    width: integer, image width in pixels\n  Returns:\n    Example proto\n  """"""\n  xmin = []\n  ymin = []\n  xmax = []\n  ymax = []\n  for b in bbox:\n    assert len(b) == 4\n    # pylint: disable=expression-not-assigned\n    [l.append(point) for l, point in zip([xmin, ymin, xmax, ymax], b)]\n    # pylint: enable=expression-not-assigned\n\n  colorspace = \'RGB\'\n  channels = 3\n  image_format = \'JPEG\'\n\n  example = tf.train.Example(features=tf.train.Features(feature={\n      \'image/height\': _int64_feature(height),\n      \'image/width\': _int64_feature(width),\n      \'image/colorspace\': _bytes_feature(colorspace),\n      \'image/channels\': _int64_feature(channels),\n      \'image/class/label\': _int64_feature(label),\n      \'image/class/synset\': _bytes_feature(synset),\n      \'image/class/text\': _bytes_feature(human),\n      \'image/object/bbox/xmin\': _float_feature(xmin),\n      \'image/object/bbox/xmax\': _float_feature(xmax),\n      \'image/object/bbox/ymin\': _float_feature(ymin),\n      \'image/object/bbox/ymax\': _float_feature(ymax),\n      \'image/object/bbox/label\': _int64_feature([label] * len(xmin)),\n      \'image/format\': _bytes_feature(image_format),\n      \'image/filename\': _bytes_feature(os.path.basename(filename)),\n      \'image/encoded\': _bytes_feature(image_buffer)}))\n  return example\n\n\nclass ImageCoder(object):\n  """"""Helper class that provides TensorFlow image coding utilities.""""""\n\n  def __init__(self):\n    # Create a single Session to run all image coding calls.\n    self._sess = tf.Session()\n\n    # Initializes function that converts PNG to JPEG data.\n    self._png_data = tf.placeholder(dtype=tf.string)\n    image = tf.image.decode_png(self._png_data, channels=3)\n    self._png_to_jpeg = tf.image.encode_jpeg(image, format=\'rgb\', quality=100)\n\n    # Initializes function that converts CMYK JPEG data to RGB JPEG data.\n    self._cmyk_data = tf.placeholder(dtype=tf.string)\n    image = tf.image.decode_jpeg(self._cmyk_data, channels=0)\n    self._cmyk_to_rgb = tf.image.encode_jpeg(image, format=\'rgb\', quality=100)\n\n    # Initializes function that decodes RGB JPEG data.\n    self._decode_jpeg_data = tf.placeholder(dtype=tf.string)\n    self._decode_jpeg = tf.image.decode_jpeg(self._decode_jpeg_data, channels=3)\n\n  def png_to_jpeg(self, image_data):\n    return self._sess.run(self._png_to_jpeg,\n                          feed_dict={self._png_data: image_data})\n\n  def cmyk_to_rgb(self, image_data):\n    return self._sess.run(self._cmyk_to_rgb,\n                          feed_dict={self._cmyk_data: image_data})\n\n  def decode_jpeg(self, image_data):\n    image = self._sess.run(self._decode_jpeg,\n                           feed_dict={self._decode_jpeg_data: image_data})\n    assert len(image.shape) == 3\n    assert image.shape[2] == 3\n    return image\n\n\ndef _is_png(filename):\n  """"""Determine if a file contains a PNG format image.\n\n  Args:\n    filename: string, path of the image file.\n\n  Returns:\n    boolean indicating if the image is a PNG.\n  """"""\n  # File list from:\n  # https://groups.google.com/forum/embed/?place=forum/torch7#!topic/torch7/fOSTXHIESSU\n  return \'n02105855_2933.JPEG\' in filename\n\n\ndef _is_cmyk(filename):\n  """"""Determine if file contains a CMYK JPEG format image.\n\n  Args:\n    filename: string, path of the image file.\n\n  Returns:\n    boolean indicating if the image is a JPEG encoded with CMYK color space.\n  """"""\n  # File list from:\n  # https://github.com/cytsai/ilsvrc-cmyk-image-list\n  blacklist = [\'n01739381_1309.JPEG\', \'n02077923_14822.JPEG\',\n               \'n02447366_23489.JPEG\', \'n02492035_15739.JPEG\',\n               \'n02747177_10752.JPEG\', \'n03018349_4028.JPEG\',\n               \'n03062245_4620.JPEG\', \'n03347037_9675.JPEG\',\n               \'n03467068_12171.JPEG\', \'n03529860_11437.JPEG\',\n               \'n03544143_17228.JPEG\', \'n03633091_5218.JPEG\',\n               \'n03710637_5125.JPEG\', \'n03961711_5286.JPEG\',\n               \'n04033995_2932.JPEG\', \'n04258138_17003.JPEG\',\n               \'n04264628_27969.JPEG\', \'n04336792_7448.JPEG\',\n               \'n04371774_5854.JPEG\', \'n04596742_4225.JPEG\',\n               \'n07583066_647.JPEG\', \'n13037406_4650.JPEG\']\n  return filename.split(\'/\')[-1] in blacklist\n\n\ndef _process_image(filename, coder):\n  """"""Process a single image file.\n\n  Args:\n    filename: string, path to an image file e.g., \'/path/to/example.JPG\'.\n    coder: instance of ImageCoder to provide TensorFlow image coding utils.\n  Returns:\n    image_buffer: string, JPEG encoding of RGB image.\n    height: integer, image height in pixels.\n    width: integer, image width in pixels.\n  """"""\n  # Read the image file.\n  image_data = tf.gfile.FastGFile(filename, \'r\').read()\n\n  # Clean the dirty data.\n  if _is_png(filename):\n    # 1 image is a PNG.\n    print(\'Converting PNG to JPEG for %s\' % filename)\n    image_data = coder.png_to_jpeg(image_data)\n  elif _is_cmyk(filename):\n    # 22 JPEG images are in CMYK colorspace.\n    print(\'Converting CMYK to RGB for %s\' % filename)\n    image_data = coder.cmyk_to_rgb(image_data)\n\n  # Decode the RGB JPEG.\n  image = coder.decode_jpeg(image_data)\n\n  # Check that image converted to RGB\n  assert len(image.shape) == 3\n  height = image.shape[0]\n  width = image.shape[1]\n  assert image.shape[2] == 3\n\n  return image_data, height, width\n\n\ndef _process_image_files_batch(coder, thread_index, ranges, name, filenames,\n                               synsets, labels, humans, bboxes, num_shards):\n  """"""Processes and saves list of images as TFRecord in 1 thread.\n\n  Args:\n    coder: instance of ImageCoder to provide TensorFlow image coding utils.\n    thread_index: integer, unique batch to run index is within [0, len(ranges)).\n    ranges: list of pairs of integers specifying ranges of each batches to\n      analyze in parallel.\n    name: string, unique identifier specifying the data set\n    filenames: list of strings; each string is a path to an image file\n    synsets: list of strings; each string is a unique WordNet ID\n    labels: list of integer; each integer identifies the ground truth\n    humans: list of strings; each string is a human-readable label\n    bboxes: list of bounding boxes for each image. Note that each entry in this\n      list might contain from 0+ entries corresponding to the number of bounding\n      box annotations for the image.\n    num_shards: integer number of shards for this data set.\n  """"""\n  # Each thread produces N shards where N = int(num_shards / num_threads).\n  # For instance, if num_shards = 128, and the num_threads = 2, then the first\n  # thread would produce shards [0, 64).\n  num_threads = len(ranges)\n  assert not num_shards % num_threads\n  num_shards_per_batch = int(num_shards / num_threads)\n\n  shard_ranges = np.linspace(ranges[thread_index][0],\n                             ranges[thread_index][1],\n                             num_shards_per_batch + 1).astype(int)\n  num_files_in_thread = ranges[thread_index][1] - ranges[thread_index][0]\n\n  counter = 0\n  for s in xrange(num_shards_per_batch):\n    # Generate a sharded version of the file name, e.g. \'train-00002-of-00010\'\n    shard = thread_index * num_shards_per_batch + s\n    output_filename = \'%s-%.5d-of-%.5d\' % (name, shard, num_shards)\n    output_file = os.path.join(FLAGS.output_directory, output_filename)\n    writer = tf.python_io.TFRecordWriter(output_file)\n\n    shard_counter = 0\n    files_in_shard = np.arange(shard_ranges[s], shard_ranges[s + 1], dtype=int)\n    for i in files_in_shard:\n      filename = filenames[i]\n      label = labels[i]\n      synset = synsets[i]\n      human = humans[i]\n      bbox = bboxes[i]\n\n      image_buffer, height, width = _process_image(filename, coder)\n\n      example = _convert_to_example(filename, image_buffer, label,\n                                    synset, human, bbox,\n                                    height, width)\n      writer.write(example.SerializeToString())\n      shard_counter += 1\n      counter += 1\n\n      if not counter % 1000:\n        print(\'%s [thread %d]: Processed %d of %d images in thread batch.\' %\n              (datetime.now(), thread_index, counter, num_files_in_thread))\n        sys.stdout.flush()\n\n    writer.close()\n    print(\'%s [thread %d]: Wrote %d images to %s\' %\n          (datetime.now(), thread_index, shard_counter, output_file))\n    sys.stdout.flush()\n    shard_counter = 0\n  print(\'%s [thread %d]: Wrote %d images to %d shards.\' %\n        (datetime.now(), thread_index, counter, num_files_in_thread))\n  sys.stdout.flush()\n\n\ndef _process_image_files(name, filenames, synsets, labels, humans,\n                         bboxes, num_shards):\n  """"""Process and save list of images as TFRecord of Example protos.\n\n  Args:\n    name: string, unique identifier specifying the data set\n    filenames: list of strings; each string is a path to an image file\n    synsets: list of strings; each string is a unique WordNet ID\n    labels: list of integer; each integer identifies the ground truth\n    humans: list of strings; each string is a human-readable label\n    bboxes: list of bounding boxes for each image. Note that each entry in this\n      list might contain from 0+ entries corresponding to the number of bounding\n      box annotations for the image.\n    num_shards: integer number of shards for this data set.\n  """"""\n  assert len(filenames) == len(synsets)\n  assert len(filenames) == len(labels)\n  assert len(filenames) == len(humans)\n  assert len(filenames) == len(bboxes)\n\n  # Break all images into batches with a [ranges[i][0], ranges[i][1]].\n  spacing = np.linspace(0, len(filenames), FLAGS.num_threads + 1).astype(np.int)\n  ranges = []\n  threads = []\n  for i in xrange(len(spacing) - 1):\n    ranges.append([spacing[i], spacing[i+1]])\n\n  # Launch a thread for each batch.\n  print(\'Launching %d threads for spacings: %s\' % (FLAGS.num_threads, ranges))\n  sys.stdout.flush()\n\n  # Create a mechanism for monitoring when all threads are finished.\n  coord = tf.train.Coordinator()\n\n  # Create a generic TensorFlow-based utility for converting all image codings.\n  coder = ImageCoder()\n\n  threads = []\n  for thread_index in xrange(len(ranges)):\n    args = (coder, thread_index, ranges, name, filenames,\n            synsets, labels, humans, bboxes, num_shards)\n    t = threading.Thread(target=_process_image_files_batch, args=args)\n    t.start()\n    threads.append(t)\n\n  # Wait for all the threads to terminate.\n  coord.join(threads)\n  print(\'%s: Finished writing all %d images in data set.\' %\n        (datetime.now(), len(filenames)))\n  sys.stdout.flush()\n\n\ndef _find_image_files(data_dir, labels_file):\n  """"""Build a list of all images files and labels in the data set.\n\n  Args:\n    data_dir: string, path to the root directory of images.\n\n      Assumes that the ImageNet data set resides in JPEG files located in\n      the following directory structure.\n\n        data_dir/n01440764/ILSVRC2012_val_00000293.JPEG\n        data_dir/n01440764/ILSVRC2012_val_00000543.JPEG\n\n      where \'n01440764\' is the unique synset label associated with these images.\n\n    labels_file: string, path to the labels file.\n\n      The list of valid labels are held in this file. Assumes that the file\n      contains entries as such:\n        n01440764\n        n01443537\n        n01484850\n      where each line corresponds to a label expressed as a synset. We map\n      each synset contained in the file to an integer (based on the alphabetical\n      ordering) starting with the integer 1 corresponding to the synset\n      contained in the first line.\n\n      The reason we start the integer labels at 1 is to reserve label 0 as an\n      unused background class.\n\n  Returns:\n    filenames: list of strings; each string is a path to an image file.\n    synsets: list of strings; each string is a unique WordNet ID.\n    labels: list of integer; each integer identifies the ground truth.\n  """"""\n  print(\'Determining list of input files and labels from %s.\' % data_dir)\n  challenge_synsets = [l.strip() for l in\n                       tf.gfile.FastGFile(labels_file, \'r\').readlines()]\n\n  labels = []\n  filenames = []\n  synsets = []\n\n  # Leave label index 0 empty as a background class.\n  label_index = 1\n\n  # Construct the list of JPEG files and labels.\n  for synset in challenge_synsets:\n    jpeg_file_path = \'%s/%s/*.JPEG\' % (data_dir, synset)\n    matching_files = tf.gfile.Glob(jpeg_file_path)\n\n    labels.extend([label_index] * len(matching_files))\n    synsets.extend([synset] * len(matching_files))\n    filenames.extend(matching_files)\n\n    if not label_index % 100:\n      print(\'Finished finding files in %d of %d classes.\' % (\n          label_index, len(challenge_synsets)))\n    label_index += 1\n\n  # Shuffle the ordering of all image files in order to guarantee\n  # random ordering of the images with respect to label in the\n  # saved TFRecord files. Make the randomization repeatable.\n  shuffled_index = range(len(filenames))\n  random.seed(12345)\n  random.shuffle(shuffled_index)\n\n  filenames = [filenames[i] for i in shuffled_index]\n  synsets = [synsets[i] for i in shuffled_index]\n  labels = [labels[i] for i in shuffled_index]\n\n  print(\'Found %d JPEG files across %d labels inside %s.\' %\n        (len(filenames), len(challenge_synsets), data_dir))\n  return filenames, synsets, labels\n\n\ndef _find_human_readable_labels(synsets, synset_to_human):\n  """"""Build a list of human-readable labels.\n\n  Args:\n    synsets: list of strings; each string is a unique WordNet ID.\n    synset_to_human: dict of synset to human labels, e.g.,\n      \'n02119022\' --> \'red fox, Vulpes vulpes\'\n\n  Returns:\n    List of human-readable strings corresponding to each synset.\n  """"""\n  humans = []\n  for s in synsets:\n    assert s in synset_to_human, (\'Failed to find: %s\' % s)\n    humans.append(synset_to_human[s])\n  return humans\n\n\ndef _find_image_bounding_boxes(filenames, image_to_bboxes):\n  """"""Find the bounding boxes for a given image file.\n\n  Args:\n    filenames: list of strings; each string is a path to an image file.\n    image_to_bboxes: dictionary mapping image file names to a list of\n      bounding boxes. This list contains 0+ bounding boxes.\n  Returns:\n    List of bounding boxes for each image. Note that each entry in this\n    list might contain from 0+ entries corresponding to the number of bounding\n    box annotations for the image.\n  """"""\n  num_image_bbox = 0\n  bboxes = []\n  for f in filenames:\n    basename = os.path.basename(f)\n    if basename in image_to_bboxes:\n      bboxes.append(image_to_bboxes[basename])\n      num_image_bbox += 1\n    else:\n      bboxes.append([])\n  print(\'Found %d images with bboxes out of %d images\' % (\n      num_image_bbox, len(filenames)))\n  return bboxes\n\n\ndef _process_dataset(name, directory, num_shards, synset_to_human,\n                     image_to_bboxes):\n  """"""Process a complete data set and save it as a TFRecord.\n\n  Args:\n    name: string, unique identifier specifying the data set.\n    directory: string, root path to the data set.\n    num_shards: integer number of shards for this data set.\n    synset_to_human: dict of synset to human labels, e.g.,\n      \'n02119022\' --> \'red fox, Vulpes vulpes\'\n    image_to_bboxes: dictionary mapping image file names to a list of\n      bounding boxes. This list contains 0+ bounding boxes.\n  """"""\n  filenames, synsets, labels = _find_image_files(directory, FLAGS.labels_file)\n  humans = _find_human_readable_labels(synsets, synset_to_human)\n  bboxes = _find_image_bounding_boxes(filenames, image_to_bboxes)\n  _process_image_files(name, filenames, synsets, labels,\n                       humans, bboxes, num_shards)\n\n\ndef _build_synset_lookup(imagenet_metadata_file):\n  """"""Build lookup for synset to human-readable label.\n\n  Args:\n    imagenet_metadata_file: string, path to file containing mapping from\n      synset to human-readable label.\n\n      Assumes each line of the file looks like:\n\n        n02119247    black fox\n        n02119359    silver fox\n        n02119477    red fox, Vulpes fulva\n\n      where each line corresponds to a unique mapping. Note that each line is\n      formatted as <synset>\\t<human readable label>.\n\n  Returns:\n    Dictionary of synset to human labels, such as:\n      \'n02119022\' --> \'red fox, Vulpes vulpes\'\n  """"""\n  lines = tf.gfile.FastGFile(imagenet_metadata_file, \'r\').readlines()\n  synset_to_human = {}\n  for l in lines:\n    if l:\n      parts = l.strip().split(\'\\t\')\n      assert len(parts) == 2\n      synset = parts[0]\n      human = parts[1]\n      synset_to_human[synset] = human\n  return synset_to_human\n\n\ndef _build_bounding_box_lookup(bounding_box_file):\n  """"""Build a lookup from image file to bounding boxes.\n\n  Args:\n    bounding_box_file: string, path to file with bounding boxes annotations.\n\n      Assumes each line of the file looks like:\n\n        n00007846_64193.JPEG,0.0060,0.2620,0.7545,0.9940\n\n      where each line corresponds to one bounding box annotation associated\n      with an image. Each line can be parsed as:\n\n        <JPEG file name>, <xmin>, <ymin>, <xmax>, <ymax>\n\n      Note that there might exist mulitple bounding box annotations associated\n      with an image file. This file is the output of process_bounding_boxes.py.\n\n  Returns:\n    Dictionary mapping image file names to a list of bounding boxes. This list\n    contains 0+ bounding boxes.\n  """"""\n  lines = tf.gfile.FastGFile(bounding_box_file, \'r\').readlines()\n  images_to_bboxes = {}\n  num_bbox = 0\n  num_image = 0\n  for l in lines:\n    if l:\n      parts = l.split(\',\')\n      assert len(parts) == 5, (\'Failed to parse: %s\' % l)\n      filename = parts[0]\n      xmin = float(parts[1])\n      ymin = float(parts[2])\n      xmax = float(parts[3])\n      ymax = float(parts[4])\n      box = [xmin, ymin, xmax, ymax]\n\n      if filename not in images_to_bboxes:\n        images_to_bboxes[filename] = []\n        num_image += 1\n      images_to_bboxes[filename].append(box)\n      num_bbox += 1\n\n  print(\'Successfully read %d bounding boxes \'\n        \'across %d images.\' % (num_bbox, num_image))\n  return images_to_bboxes\n\n\ndef main(unused_argv):\n  assert not FLAGS.train_shards % FLAGS.num_threads, (\n      \'Please make the FLAGS.num_threads commensurate with FLAGS.train_shards\')\n  assert not FLAGS.validation_shards % FLAGS.num_threads, (\n      \'Please make the FLAGS.num_threads commensurate with \'\n      \'FLAGS.validation_shards\')\n  print(\'Saving results to %s\' % FLAGS.output_directory)\n\n  # Build a map from synset to human-readable label.\n  synset_to_human = _build_synset_lookup(FLAGS.imagenet_metadata_file)\n  image_to_bboxes = _build_bounding_box_lookup(FLAGS.bounding_box_file)\n\n  # Run it!\n  _process_dataset(\'validation\', FLAGS.validation_directory,\n                   FLAGS.validation_shards, synset_to_human, image_to_bboxes)\n  _process_dataset(\'train\', FLAGS.train_directory, FLAGS.train_shards,\n                   synset_to_human, image_to_bboxes)\n\n\nif __name__ == \'__main__\':\n  tf.app.run()\n'"
vendor/tensorflow/models/research/slim/datasets/cifar10.py,6,"b'# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Provides data for the Cifar10 dataset.\n\nThe dataset scripts used to create the dataset can be found at:\ntensorflow/models/research/slim/datasets/download_and_convert_cifar10.py\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport os\nimport tensorflow as tf\n\nfrom vendor.tensorflow.models.research.slim.datasets import dataset_utils\n\nslim = tf.contrib.slim\n\n_FILE_PATTERN = \'cifar10_%s.tfrecord\'\n\nSPLITS_TO_SIZES = {\'train\': 50000, \'test\': 10000}\n\n_NUM_CLASSES = 10\n\n_ITEMS_TO_DESCRIPTIONS = {\n    \'image\': \'A [32 x 32 x 3] color image.\',\n    \'label\': \'A single integer between 0 and 9\',\n}\n\n\ndef get_split(split_name, dataset_dir, file_pattern=None, reader=None):\n  """"""Gets a dataset tuple with instructions for reading cifar10.\n\n  Args:\n    split_name: A train/test split name.\n    dataset_dir: The base directory of the dataset sources.\n    file_pattern: The file pattern to use when matching the dataset sources.\n      It is assumed that the pattern contains a \'%s\' string so that the split\n      name can be inserted.\n    reader: The TensorFlow reader type.\n\n  Returns:\n    A `Dataset` namedtuple.\n\n  Raises:\n    ValueError: if `split_name` is not a valid train/test split.\n  """"""\n  if split_name not in SPLITS_TO_SIZES:\n    raise ValueError(\'split name %s was not recognized.\' % split_name)\n\n  if not file_pattern:\n    file_pattern = _FILE_PATTERN\n  file_pattern = os.path.join(dataset_dir, file_pattern % split_name)\n\n  # Allowing None in the signature so that dataset_factory can use the default.\n  if not reader:\n    reader = tf.TFRecordReader\n\n  keys_to_features = {\n      \'image/encoded\': tf.FixedLenFeature((), tf.string, default_value=\'\'),\n      \'image/format\': tf.FixedLenFeature((), tf.string, default_value=\'png\'),\n      \'image/class/label\': tf.FixedLenFeature(\n          [], tf.int64, default_value=tf.zeros([], dtype=tf.int64)),\n  }\n\n  items_to_handlers = {\n      \'image\': slim.tfexample_decoder.Image(shape=[32, 32, 3]),\n      \'label\': slim.tfexample_decoder.Tensor(\'image/class/label\'),\n  }\n\n  decoder = slim.tfexample_decoder.TFExampleDecoder(\n      keys_to_features, items_to_handlers)\n\n  labels_to_names = None\n  if dataset_utils.has_labels(dataset_dir):\n    labels_to_names = dataset_utils.read_label_file(dataset_dir)\n\n  return slim.dataset.Dataset(\n      data_sources=file_pattern,\n      reader=reader,\n      decoder=decoder,\n      num_samples=SPLITS_TO_SIZES[split_name],\n      items_to_descriptions=_ITEMS_TO_DESCRIPTIONS,\n      num_classes=_NUM_CLASSES,\n      labels_to_names=labels_to_names)\n'"
vendor/tensorflow/models/research/slim/datasets/dataset_factory.py,1,"b'# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""A factory-pattern class which returns classification image/label pairs.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom vendor.tensorflow.models.research.slim.datasets import cifar10\nfrom vendor.tensorflow.models.research.slim.datasets import flowers\nfrom vendor.tensorflow.models.research.slim.datasets import imagenet\nfrom vendor.tensorflow.models.research.slim.datasets import mnist\nfrom vendor.tensorflow.models.research.slim.datasets import deepdrive\n\ndatasets_map = {\n    \'cifar10\': cifar10,\n    \'flowers\': flowers,\n    \'imagenet\': imagenet,\n    \'mnist\': mnist,\n    \'deepdrive\': deepdrive,\n}\n\n\ndef get_dataset(name, split_name, dataset_dir, file_pattern=None, reader=None):\n  """"""Given a dataset name and a split_name returns a Dataset.\n\n  Args:\n    name: String, the name of the dataset.\n    split_name: A train/test split name.\n    dataset_dir: The directory where the dataset files are stored.\n    file_pattern: The file pattern to use for matching the dataset source files.\n    reader: The subclass of tf.ReaderBase. If left as `None`, then the default\n      reader defined by each dataset is used.\n\n  Returns:\n    A `Dataset` class.\n\n  Raises:\n    ValueError: If the dataset `name` is unknown.\n  """"""\n  if name not in datasets_map:\n    raise ValueError(\'Name of dataset unknown %s\' % name)\n  return datasets_map[name].get_split(\n      split_name,\n      dataset_dir,\n      file_pattern,\n      reader)\n'"
vendor/tensorflow/models/research/slim/datasets/dataset_utils.py,7,"b'# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Contains utilities for downloading and converting datasets.""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport os\nimport sys\nimport tarfile\n\nfrom six.moves import urllib\nimport tensorflow as tf\n\nLABELS_FILENAME = \'labels.txt\'\n\n\ndef int64_feature(values):\n  """"""Returns a TF-Feature of int64s.\n\n  Args:\n    values: A scalar or list of values.\n\n  Returns:\n    A TF-Feature.\n  """"""\n  if not isinstance(values, (tuple, list)):\n    values = [values]\n  return tf.train.Feature(int64_list=tf.train.Int64List(value=values))\n\n\ndef bytes_feature(values):\n  """"""Returns a TF-Feature of bytes.\n\n  Args:\n    values: A string.\n\n  Returns:\n    A TF-Feature.\n  """"""\n  return tf.train.Feature(bytes_list=tf.train.BytesList(value=[values]))\n\n\ndef float_feature(values):\n  """"""Returns a TF-Feature of floats.\n\n  Args:\n    values: A scalar of list of values.\n\n  Returns:\n    A TF-Feature.\n  """"""\n  if not isinstance(values, (tuple, list)):\n    values = [values]\n  return tf.train.Feature(float_list=tf.train.FloatList(value=values))\n\n\ndef image_to_tfexample(image_data, image_format, height, width, class_id):\n  return tf.train.Example(features=tf.train.Features(feature={\n      \'image/encoded\': bytes_feature(image_data),\n      \'image/format\': bytes_feature(image_format),\n      \'image/class/label\': int64_feature(class_id),\n      \'image/height\': int64_feature(height),\n      \'image/width\': int64_feature(width),\n  }))\n\n\ndef download_and_uncompress_tarball(tarball_url, dataset_dir):\n  """"""Downloads the `tarball_url` and uncompresses it locally.\n\n  Args:\n    tarball_url: The URL of a tarball file.\n    dataset_dir: The directory where the temporary files are stored.\n  """"""\n  filename = tarball_url.split(\'/\')[-1]\n  filepath = os.path.join(dataset_dir, filename)\n\n  def _progress(count, block_size, total_size):\n    sys.stdout.write(\'\\r>> Downloading %s %.1f%%\' % (\n        filename, float(count * block_size) / float(total_size) * 100.0))\n    sys.stdout.flush()\n  filepath, _ = urllib.request.urlretrieve(tarball_url, filepath, _progress)\n  print()\n  statinfo = os.stat(filepath)\n  print(\'Successfully downloaded\', filename, statinfo.st_size, \'bytes.\')\n  tarfile.open(filepath, \'r:gz\').extractall(dataset_dir)\n\n\ndef write_label_file(labels_to_class_names, dataset_dir,\n                     filename=LABELS_FILENAME):\n  """"""Writes a file with the list of class names.\n\n  Args:\n    labels_to_class_names: A map of (integer) labels to class names.\n    dataset_dir: The directory in which the labels file should be written.\n    filename: The filename where the class names are written.\n  """"""\n  labels_filename = os.path.join(dataset_dir, filename)\n  with tf.gfile.Open(labels_filename, \'w\') as f:\n    for label in labels_to_class_names:\n      class_name = labels_to_class_names[label]\n      f.write(\'%d:%s\\n\' % (label, class_name))\n\n\ndef has_labels(dataset_dir, filename=LABELS_FILENAME):\n  """"""Specifies whether or not the dataset directory contains a label map file.\n\n  Args:\n    dataset_dir: The directory in which the labels file is found.\n    filename: The filename where the class names are written.\n\n  Returns:\n    `True` if the labels file exists and `False` otherwise.\n  """"""\n  return tf.gfile.Exists(os.path.join(dataset_dir, filename))\n\n\ndef read_label_file(dataset_dir, filename=LABELS_FILENAME):\n  """"""Reads the labels file and returns a mapping from ID to class name.\n\n  Args:\n    dataset_dir: The directory in which the labels file is found.\n    filename: The filename where the class names are written.\n\n  Returns:\n    A map from a label (integer) to class name.\n  """"""\n  labels_filename = os.path.join(dataset_dir, filename)\n  with tf.gfile.Open(labels_filename, \'rb\') as f:\n    lines = f.read().decode()\n  lines = lines.split(\'\\n\')\n  lines = filter(None, lines)\n\n  labels_to_class_names = {}\n  for line in lines:\n    index = line.index(\':\')\n    labels_to_class_names[int(line[:index])] = line[index+1:]\n  return labels_to_class_names\n'"
vendor/tensorflow/models/research/slim/datasets/deepdrive.py,10,"b'# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Provides data for the flowers dataset.\n\nThe dataset scripts used to create the dataset can be found at:\ntensorflow/models/research/slim/datasets/download_and_convert_flowers.py\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport os\nimport tensorflow as tf\n\nslim = tf.contrib.slim\n\n_FILE_PATTERN = \'deepdrive_%s*.tfrecord\'\n\nSPLITS_TO_SIZES = {\'train\': 1000 * 164, \'eval\': 1000}\n\n_ITEMS_TO_DESCRIPTIONS = {\n    \'image\': \'A color image of varying size.\',\n    \'spin\':         \'deepdrive control value\',\n    \'direction\':    \'deepdrive control value\',\n    \'speed\':        \'deepdrive control value\',\n    \'speed_change\': \'deepdrive control value\',\n    \'steering\':     \'deepdrive control value\',\n    \'throttle\':     \'deepdrive control value\',\n}\n\n\ndef get_split(split_name, dataset_dir, file_pattern=None, reader=None):\n  """"""Gets a dataset tuple with instructions for reading flowers.\n\n  Args:\n    split_name: A train/validation split name.\n    dataset_dir: The b`ase directory of the dataset sources.\n    file_pattern: The file pattern to use when matching the dataset sources.\n      It is assumed that the pattern contains a \'%s\' string so that the split\n      name can be inserted.\n    reader: The TensorFlow reader type.\n\n  Returns:\n    A `Dataset` namedtuple.\n\n  Raises:\n    ValueError: if `split_name` is not a valid train/validation split.\n  """"""\n  if split_name not in SPLITS_TO_SIZES:\n    raise ValueError(\'split name %s was not recognized.\' % split_name)\n\n  if not file_pattern:\n    file_pattern = _FILE_PATTERN\n  file_pattern = os.path.join(dataset_dir, file_pattern % split_name)\n\n  # Allowing None in the signature so that dataset_factory can use the default.\n  if reader is None:\n    reader = tf.TFRecordReader\n\n  keys_to_features = {\n      \'image/encoded\': tf.FixedLenFeature((), tf.string, default_value=\'\'),\n      \'image/format\': tf.FixedLenFeature((), tf.string, default_value=\'raw\'),\n      \'image/control/spin\': tf.FixedLenFeature([], tf.float32, default_value=tf.zeros([], dtype=tf.float32)),\n      \'image/control/direction\': tf.FixedLenFeature([], tf.float32, default_value=tf.zeros([], dtype=tf.float32)),\n      \'image/control/speed\': tf.FixedLenFeature([], tf.float32, default_value=tf.zeros([], dtype=tf.float32)),\n      \'image/control/speed_change\': tf.FixedLenFeature([], tf.float32, default_value=tf.zeros([], dtype=tf.float32)),\n      \'image/control/steering\': tf.FixedLenFeature([], tf.float32, default_value=tf.zeros([], dtype=tf.float32)),\n      \'image/control/throttle\': tf.FixedLenFeature([], tf.float32, default_value=tf.zeros([], dtype=tf.float32)),\n  }\n\n  items_to_handlers = {\n      \'image\': slim.tfexample_decoder.Image(),\n      \'spin\': slim.tfexample_decoder.Tensor(\'image/control/spin\'),\n      \'direction\': slim.tfexample_decoder.Tensor(\'image/control/direction\'),\n      \'speed\': slim.tfexample_decoder.Tensor(\'image/control/speed\'),\n      \'speed_change\': slim.tfexample_decoder.Tensor(\'image/control/speed_change\'),\n      \'steering\': slim.tfexample_decoder.Tensor(\'image/control/steering\'),\n      \'throttle\': slim.tfexample_decoder.Tensor(\'image/control/throttle\'),\n  }\n\n  decoder = slim.tfexample_decoder.TFExampleDecoder(\n      keys_to_features, items_to_handlers)\n\n\n  # labels_to_names = None\n  # if dataset_utils.has_labels(dataset_dir):\n  #   labels_to_names = dataset_utils.read_label_file(dataset_dir)\n\n  return slim.dataset.Dataset(\n      data_sources=file_pattern,\n      reader=reader,\n      decoder=decoder,\n      num_samples=SPLITS_TO_SIZES[split_name],\n      items_to_descriptions=_ITEMS_TO_DESCRIPTIONS,)\n'"
vendor/tensorflow/models/research/slim/datasets/download_and_convert_cifar10.py,12,"b'# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\nr""""""Downloads and converts cifar10 data to TFRecords of TF-Example protos.\n\nThis module downloads the cifar10 data, uncompresses it, reads the files\nthat make up the cifar10 data and creates two TFRecord datasets: one for train\nand one for test. Each TFRecord dataset is comprised of a set of TF-Example\nprotocol buffers, each of which contain a single image and label.\n\nThe script should take several minutes to run.\n\n""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport os\nimport sys\nimport tarfile\n\nimport numpy as np\nfrom six.moves import cPickle\nfrom six.moves import urllib\nimport tensorflow as tf\n\nfrom vendor.tensorflow.models.research.slim.datasets import dataset_utils\n\n# The URL where the CIFAR data can be downloaded.\n_DATA_URL = \'https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\'\n\n# The number of training files.\n_NUM_TRAIN_FILES = 5\n\n# The height and width of each image.\n_IMAGE_SIZE = 32\n\n# The names of the classes.\n_CLASS_NAMES = [\n    \'airplane\',\n    \'automobile\',\n    \'bird\',\n    \'cat\',\n    \'deer\',\n    \'dog\',\n    \'frog\',\n    \'horse\',\n    \'ship\',\n    \'truck\',\n]\n\n\ndef _add_to_tfrecord(filename, tfrecord_writer, offset=0):\n  """"""Loads data from the cifar10 pickle files and writes files to a TFRecord.\n\n  Args:\n    filename: The filename of the cifar10 pickle file.\n    tfrecord_writer: The TFRecord writer to use for writing.\n    offset: An offset into the absolute number of images previously written.\n\n  Returns:\n    The new offset.\n  """"""\n  with tf.gfile.Open(filename, \'rb\') as f:\n    if sys.version_info < (3,):\n      data = cPickle.load(f)\n    else:\n      data = cPickle.load(f, encoding=\'bytes\')\n\n  images = data[b\'data\']\n  num_images = images.shape[0]\n\n  images = images.reshape((num_images, 3, 32, 32))\n  labels = data[b\'labels\']\n\n  with tf.Graph().as_default():\n    image_placeholder = tf.placeholder(dtype=tf.uint8)\n    encoded_image = tf.image.encode_png(image_placeholder)\n\n    with tf.Session(\'\') as sess:\n\n      for j in range(num_images):\n        sys.stdout.write(\'\\r>> Reading file [%s] image %d/%d\' % (\n            filename, offset + j + 1, offset + num_images))\n        sys.stdout.flush()\n\n        image = np.squeeze(images[j]).transpose((1, 2, 0))\n        label = labels[j]\n\n        png_string = sess.run(encoded_image,\n                              feed_dict={image_placeholder: image})\n\n        example = dataset_utils.image_to_tfexample(\n            png_string, b\'png\', _IMAGE_SIZE, _IMAGE_SIZE, label)\n        tfrecord_writer.write(example.SerializeToString())\n\n  return offset + num_images\n\n\ndef _get_output_filename(dataset_dir, split_name):\n  """"""Creates the output filename.\n\n  Args:\n    dataset_dir: The dataset directory where the dataset is stored.\n    split_name: The name of the train/test split.\n\n  Returns:\n    An absolute file path.\n  """"""\n  return \'%s/cifar10_%s.tfrecord\' % (dataset_dir, split_name)\n\n\ndef _download_and_uncompress_dataset(dataset_dir):\n  """"""Downloads cifar10 and uncompresses it locally.\n\n  Args:\n    dataset_dir: The directory where the temporary files are stored.\n  """"""\n  filename = _DATA_URL.split(\'/\')[-1]\n  filepath = os.path.join(dataset_dir, filename)\n\n  if not os.path.exists(filepath):\n    def _progress(count, block_size, total_size):\n      sys.stdout.write(\'\\r>> Downloading %s %.1f%%\' % (\n          filename, float(count * block_size) / float(total_size) * 100.0))\n      sys.stdout.flush()\n    filepath, _ = urllib.request.urlretrieve(_DATA_URL, filepath, _progress)\n    print()\n    statinfo = os.stat(filepath)\n    print(\'Successfully downloaded\', filename, statinfo.st_size, \'bytes.\')\n    tarfile.open(filepath, \'r:gz\').extractall(dataset_dir)\n\n\ndef _clean_up_temporary_files(dataset_dir):\n  """"""Removes temporary files used to create the dataset.\n\n  Args:\n    dataset_dir: The directory where the temporary files are stored.\n  """"""\n  filename = _DATA_URL.split(\'/\')[-1]\n  filepath = os.path.join(dataset_dir, filename)\n  tf.gfile.Remove(filepath)\n\n  tmp_dir = os.path.join(dataset_dir, \'cifar-10-batches-py\')\n  tf.gfile.DeleteRecursively(tmp_dir)\n\n\ndef run(dataset_dir):\n  """"""Runs the download and conversion operation.\n\n  Args:\n    dataset_dir: The dataset directory where the dataset is stored.\n  """"""\n  if not tf.gfile.Exists(dataset_dir):\n    tf.gfile.MakeDirs(dataset_dir)\n\n  training_filename = _get_output_filename(dataset_dir, \'train\')\n  testing_filename = _get_output_filename(dataset_dir, \'test\')\n\n  if tf.gfile.Exists(training_filename) and tf.gfile.Exists(testing_filename):\n    print(\'Dataset files already exist. Exiting without re-creating them.\')\n    return\n\n  dataset_utils.download_and_uncompress_tarball(_DATA_URL, dataset_dir)\n\n  # First, process the training data:\n  with tf.python_io.TFRecordWriter(training_filename) as tfrecord_writer:\n    offset = 0\n    for i in range(_NUM_TRAIN_FILES):\n      filename = os.path.join(dataset_dir,\n                              \'cifar-10-batches-py\',\n                              \'data_batch_%d\' % (i + 1))  # 1-indexed.\n      offset = _add_to_tfrecord(filename, tfrecord_writer, offset)\n\n  # Next, process the testing data:\n  with tf.python_io.TFRecordWriter(testing_filename) as tfrecord_writer:\n    filename = os.path.join(dataset_dir,\n                            \'cifar-10-batches-py\',\n                            \'test_batch\')\n    _add_to_tfrecord(filename, tfrecord_writer)\n\n  # Finally, write the labels file:\n  labels_to_class_names = dict(zip(range(len(_CLASS_NAMES)), _CLASS_NAMES))\n  dataset_utils.write_label_file(labels_to_class_names, dataset_dir)\n\n  _clean_up_temporary_files(dataset_dir)\n  print(\'\\nFinished converting the Cifar10 dataset!\')\n'"
vendor/tensorflow/models/research/slim/datasets/download_and_convert_flowers.py,11,"b'# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\nr""""""Downloads and converts Flowers data to TFRecords of TF-Example protos.\n\nThis module downloads the Flowers data, uncompresses it, reads the files\nthat make up the Flowers data and creates two TFRecord datasets: one for train\nand one for test. Each TFRecord dataset is comprised of a set of TF-Example\nprotocol buffers, each of which contain a single image and label.\n\nThe script should take about a minute to run.\n\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport math\nimport os\nimport random\nimport sys\n\nimport tensorflow as tf\n\nfrom vendor.tensorflow.models.research.slim.datasets import dataset_utils\n\n# The URL where the Flowers data can be downloaded.\n_DATA_URL = \'http://download.tensorflow.org/example_images/flower_photos.tgz\'\n\n# The number of images in the validation set.\n_NUM_VALIDATION = 350\n\n# Seed for repeatability.\n_RANDOM_SEED = 0\n\n# The number of shards per dataset split.\n_NUM_SHARDS = 5\n\n\nclass ImageReader(object):\n  """"""Helper class that provides TensorFlow image coding utilities.""""""\n\n  def __init__(self):\n    # Initializes function that decodes RGB JPEG data.\n    self._decode_jpeg_data = tf.placeholder(dtype=tf.string)\n    self._decode_jpeg = tf.image.decode_jpeg(self._decode_jpeg_data, channels=3)\n\n  def read_image_dims(self, sess, image_data):\n    image = self.decode_jpeg(sess, image_data)\n    return image.shape[0], image.shape[1]\n\n  def decode_jpeg(self, sess, image_data):\n    image = sess.run(self._decode_jpeg,\n                     feed_dict={self._decode_jpeg_data: image_data})\n    assert len(image.shape) == 3\n    assert image.shape[2] == 3\n    return image\n\n\ndef _get_filenames_and_classes(dataset_dir):\n  """"""Returns a list of filenames and inferred class names.\n\n  Args:\n    dataset_dir: A directory containing a set of subdirectories representing\n      class names. Each subdirectory should contain PNG or JPG encoded images.\n\n  Returns:\n    A list of image file paths, relative to `dataset_dir` and the list of\n    subdirectories, representing class names.\n  """"""\n  flower_root = os.path.join(dataset_dir, \'flower_photos\')\n  directories = []\n  class_names = []\n  for filename in os.listdir(flower_root):\n    path = os.path.join(flower_root, filename)\n    if os.path.isdir(path):\n      directories.append(path)\n      class_names.append(filename)\n\n  photo_filenames = []\n  for directory in directories:\n    for filename in os.listdir(directory):\n      path = os.path.join(directory, filename)\n      photo_filenames.append(path)\n\n  return photo_filenames, sorted(class_names)\n\n\ndef _get_dataset_filename(dataset_dir, split_name, shard_id):\n  output_filename = \'flowers_%s_%05d-of-%05d.tfrecord\' % (\n      split_name, shard_id, _NUM_SHARDS)\n  return os.path.join(dataset_dir, output_filename)\n\n\ndef _convert_dataset(split_name, filenames, class_names_to_ids, dataset_dir):\n  """"""Converts the given filenames to a TFRecord dataset.\n\n  Args:\n    split_name: The name of the dataset, either \'train\' or \'validation\'.\n    filenames: A list of absolute paths to png or jpg images.\n    class_names_to_ids: A dictionary from class names (strings) to ids\n      (integers).\n    dataset_dir: The directory where the converted datasets are stored.\n  """"""\n  assert split_name in [\'train\', \'validation\']\n\n  num_per_shard = int(math.ceil(len(filenames) / float(_NUM_SHARDS)))\n\n  with tf.Graph().as_default():\n    image_reader = ImageReader()\n\n    with tf.Session(\'\') as sess:\n\n      for shard_id in range(_NUM_SHARDS):\n        output_filename = _get_dataset_filename(\n            dataset_dir, split_name, shard_id)\n\n        with tf.python_io.TFRecordWriter(output_filename) as tfrecord_writer:\n          start_ndx = shard_id * num_per_shard\n          end_ndx = min((shard_id+1) * num_per_shard, len(filenames))\n          for i in range(start_ndx, end_ndx):\n            sys.stdout.write(\'\\r>> Converting image %d/%d shard %d\' % (\n                i+1, len(filenames), shard_id))\n            sys.stdout.flush()\n\n            # Read the filename:\n            image_data = tf.gfile.FastGFile(filenames[i], \'rb\').read()\n            height, width = image_reader.read_image_dims(sess, image_data)\n\n            class_name = os.path.basename(os.path.dirname(filenames[i]))\n            class_id = class_names_to_ids[class_name]\n\n            example = dataset_utils.image_to_tfexample(\n                image_data, b\'jpg\', height, width, class_id)\n            tfrecord_writer.write(example.SerializeToString())\n\n  sys.stdout.write(\'\\n\')\n  sys.stdout.flush()\n\n\ndef _clean_up_temporary_files(dataset_dir):\n  """"""Removes temporary files used to create the dataset.\n\n  Args:\n    dataset_dir: The directory where the temporary files are stored.\n  """"""\n  filename = _DATA_URL.split(\'/\')[-1]\n  filepath = os.path.join(dataset_dir, filename)\n  tf.gfile.Remove(filepath)\n\n  tmp_dir = os.path.join(dataset_dir, \'flower_photos\')\n  tf.gfile.DeleteRecursively(tmp_dir)\n\n\ndef _dataset_exists(dataset_dir):\n  for split_name in [\'train\', \'validation\']:\n    for shard_id in range(_NUM_SHARDS):\n      output_filename = _get_dataset_filename(\n          dataset_dir, split_name, shard_id)\n      if not tf.gfile.Exists(output_filename):\n        return False\n  return True\n\n\ndef run(dataset_dir):\n  """"""Runs the download and conversion operation.\n\n  Args:\n    dataset_dir: The dataset directory where the dataset is stored.\n  """"""\n  if not tf.gfile.Exists(dataset_dir):\n    tf.gfile.MakeDirs(dataset_dir)\n\n  if _dataset_exists(dataset_dir):\n    print(\'Dataset files already exist. Exiting without re-creating them.\')\n    return\n\n  dataset_utils.download_and_uncompress_tarball(_DATA_URL, dataset_dir)\n  photo_filenames, class_names = _get_filenames_and_classes(dataset_dir)\n  class_names_to_ids = dict(zip(class_names, range(len(class_names))))\n\n  # Divide into train and test:\n  random.seed(_RANDOM_SEED)\n  random.shuffle(photo_filenames)\n  training_filenames = photo_filenames[_NUM_VALIDATION:]\n  validation_filenames = photo_filenames[:_NUM_VALIDATION]\n\n  # First, convert the training and validation sets.\n  _convert_dataset(\'train\', training_filenames, class_names_to_ids,\n                   dataset_dir)\n  _convert_dataset(\'validation\', validation_filenames, class_names_to_ids,\n                   dataset_dir)\n\n  # Finally, write the labels file:\n  labels_to_class_names = dict(zip(range(len(class_names)), class_names))\n  dataset_utils.write_label_file(labels_to_class_names, dataset_dir)\n\n  _clean_up_temporary_files(dataset_dir)\n  print(\'\\nFinished converting the Flowers dataset!\')\n'"
vendor/tensorflow/models/research/slim/datasets/download_and_convert_mnist.py,11,"b'# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\nr""""""Downloads and converts MNIST data to TFRecords of TF-Example protos.\n\nThis module downloads the MNIST data, uncompresses it, reads the files\nthat make up the MNIST data and creates two TFRecord datasets: one for train\nand one for test. Each TFRecord dataset is comprised of a set of TF-Example\nprotocol buffers, each of which contain a single image and label.\n\nThe script should take about a minute to run.\n\n""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport gzip\nimport os\nimport sys\n\nimport numpy as np\nfrom six.moves import urllib\nimport tensorflow as tf\n\nfrom vendor.tensorflow.models.research.slim.datasets import dataset_utils\n\n# The URLs where the MNIST data can be downloaded.\n_DATA_URL = \'http://yann.lecun.com/exdb/mnist/\'\n_TRAIN_DATA_FILENAME = \'train-images-idx3-ubyte.gz\'\n_TRAIN_LABELS_FILENAME = \'train-labels-idx1-ubyte.gz\'\n_TEST_DATA_FILENAME = \'t10k-images-idx3-ubyte.gz\'\n_TEST_LABELS_FILENAME = \'t10k-labels-idx1-ubyte.gz\'\n\n_IMAGE_SIZE = 28\n_NUM_CHANNELS = 1\n\n# The names of the classes.\n_CLASS_NAMES = [\n    \'zero\',\n    \'one\',\n    \'two\',\n    \'three\',\n    \'four\',\n    \'five\',\n    \'size\',\n    \'seven\',\n    \'eight\',\n    \'nine\',\n]\n\n\ndef _extract_images(filename, num_images):\n  """"""Extract the images into a numpy array.\n\n  Args:\n    filename: The path to an MNIST images file.\n    num_images: The number of images in the file.\n\n  Returns:\n    A numpy array of shape [number_of_images, height, width, channels].\n  """"""\n  print(\'Extracting images from: \', filename)\n  with gzip.open(filename) as bytestream:\n    bytestream.read(16)\n    buf = bytestream.read(\n        _IMAGE_SIZE * _IMAGE_SIZE * num_images * _NUM_CHANNELS)\n    data = np.frombuffer(buf, dtype=np.uint8)\n    data = data.reshape(num_images, _IMAGE_SIZE, _IMAGE_SIZE, _NUM_CHANNELS)\n  return data\n\n\ndef _extract_labels(filename, num_labels):\n  """"""Extract the labels into a vector of int64 label IDs.\n\n  Args:\n    filename: The path to an MNIST labels file.\n    num_labels: The number of labels in the file.\n\n  Returns:\n    A numpy array of shape [number_of_labels]\n  """"""\n  print(\'Extracting labels from: \', filename)\n  with gzip.open(filename) as bytestream:\n    bytestream.read(8)\n    buf = bytestream.read(1 * num_labels)\n    labels = np.frombuffer(buf, dtype=np.uint8).astype(np.int64)\n  return labels\n\n\ndef _add_to_tfrecord(data_filename, labels_filename, num_images,\n                     tfrecord_writer):\n  """"""Loads data from the binary MNIST files and writes files to a TFRecord.\n\n  Args:\n    data_filename: The filename of the MNIST images.\n    labels_filename: The filename of the MNIST labels.\n    num_images: The number of images in the dataset.\n    tfrecord_writer: The TFRecord writer to use for writing.\n  """"""\n  images = _extract_images(data_filename, num_images)\n  labels = _extract_labels(labels_filename, num_images)\n\n  shape = (_IMAGE_SIZE, _IMAGE_SIZE, _NUM_CHANNELS)\n  with tf.Graph().as_default():\n    image = tf.placeholder(dtype=tf.uint8, shape=shape)\n    encoded_png = tf.image.encode_png(image)\n\n    with tf.Session(\'\') as sess:\n      for j in range(num_images):\n        sys.stdout.write(\'\\r>> Converting image %d/%d\' % (j + 1, num_images))\n        sys.stdout.flush()\n\n        png_string = sess.run(encoded_png, feed_dict={image: images[j]})\n\n        example = dataset_utils.image_to_tfexample(\n            png_string, \'png\'.encode(), _IMAGE_SIZE, _IMAGE_SIZE, labels[j])\n        tfrecord_writer.write(example.SerializeToString())\n\n\ndef _get_output_filename(dataset_dir, split_name):\n  """"""Creates the output filename.\n\n  Args:\n    dataset_dir: The directory where the temporary files are stored.\n    split_name: The name of the train/test split.\n\n  Returns:\n    An absolute file path.\n  """"""\n  return \'%s/mnist_%s.tfrecord\' % (dataset_dir, split_name)\n\n\ndef _download_dataset(dataset_dir):\n  """"""Downloads MNIST locally.\n\n  Args:\n    dataset_dir: The directory where the temporary files are stored.\n  """"""\n  for filename in [_TRAIN_DATA_FILENAME,\n                   _TRAIN_LABELS_FILENAME,\n                   _TEST_DATA_FILENAME,\n                   _TEST_LABELS_FILENAME]:\n    filepath = os.path.join(dataset_dir, filename)\n\n    if not os.path.exists(filepath):\n      print(\'Downloading file %s...\' % filename)\n      def _progress(count, block_size, total_size):\n        sys.stdout.write(\'\\r>> Downloading %.1f%%\' % (\n            float(count * block_size) / float(total_size) * 100.0))\n        sys.stdout.flush()\n      filepath, _ = urllib.request.urlretrieve(_DATA_URL + filename,\n                                               filepath,\n                                               _progress)\n      print()\n      with tf.gfile.GFile(filepath) as f:\n        size = f.size()\n      print(\'Successfully downloaded\', filename, size, \'bytes.\')\n\n\ndef _clean_up_temporary_files(dataset_dir):\n  """"""Removes temporary files used to create the dataset.\n\n  Args:\n    dataset_dir: The directory where the temporary files are stored.\n  """"""\n  for filename in [_TRAIN_DATA_FILENAME,\n                   _TRAIN_LABELS_FILENAME,\n                   _TEST_DATA_FILENAME,\n                   _TEST_LABELS_FILENAME]:\n    filepath = os.path.join(dataset_dir, filename)\n    tf.gfile.Remove(filepath)\n\n\ndef run(dataset_dir):\n  """"""Runs the download and conversion operation.\n\n  Args:\n    dataset_dir: The dataset directory where the dataset is stored.\n  """"""\n  if not tf.gfile.Exists(dataset_dir):\n    tf.gfile.MakeDirs(dataset_dir)\n\n  training_filename = _get_output_filename(dataset_dir, \'train\')\n  testing_filename = _get_output_filename(dataset_dir, \'test\')\n\n  if tf.gfile.Exists(training_filename) and tf.gfile.Exists(testing_filename):\n    print(\'Dataset files already exist. Exiting without re-creating them.\')\n    return\n\n  _download_dataset(dataset_dir)\n\n  # First, process the training data:\n  with tf.python_io.TFRecordWriter(training_filename) as tfrecord_writer:\n    data_filename = os.path.join(dataset_dir, _TRAIN_DATA_FILENAME)\n    labels_filename = os.path.join(dataset_dir, _TRAIN_LABELS_FILENAME)\n    _add_to_tfrecord(data_filename, labels_filename, 60000, tfrecord_writer)\n\n  # Next, process the testing data:\n  with tf.python_io.TFRecordWriter(testing_filename) as tfrecord_writer:\n    data_filename = os.path.join(dataset_dir, _TEST_DATA_FILENAME)\n    labels_filename = os.path.join(dataset_dir, _TEST_LABELS_FILENAME)\n    _add_to_tfrecord(data_filename, labels_filename, 10000, tfrecord_writer)\n\n  # Finally, write the labels file:\n  labels_to_class_names = dict(zip(range(len(_CLASS_NAMES)), _CLASS_NAMES))\n  dataset_utils.write_label_file(labels_to_class_names, dataset_dir)\n\n  _clean_up_temporary_files(dataset_dir)\n  print(\'\\nFinished converting the MNIST dataset!\')\n'"
vendor/tensorflow/models/research/slim/datasets/flowers.py,6,"b'# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Provides data for the flowers dataset.\n\nThe dataset scripts used to create the dataset can be found at:\ntensorflow/models/research/slim/datasets/download_and_convert_flowers.py\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport os\nimport tensorflow as tf\n\nfrom vendor.tensorflow.models.research.slim.datasets import dataset_utils\n\nslim = tf.contrib.slim\n\n_FILE_PATTERN = \'flowers_%s_*.tfrecord\'\n\nSPLITS_TO_SIZES = {\'train\': 3320, \'validation\': 350}\n\n_NUM_CLASSES = 5\n\n_ITEMS_TO_DESCRIPTIONS = {\n    \'image\': \'A color image of varying size.\',\n    \'label\': \'A single integer between 0 and 4\',\n}\n\n\ndef get_split(split_name, dataset_dir, file_pattern=None, reader=None):\n  """"""Gets a dataset tuple with instructions for reading flowers.\n\n  Args:\n    split_name: A train/validation split name.\n    dataset_dir: The base directory of the dataset sources.\n    file_pattern: The file pattern to use when matching the dataset sources.\n      It is assumed that the pattern contains a \'%s\' string so that the split\n      name can be inserted.\n    reader: The TensorFlow reader type.\n\n  Returns:\n    A `Dataset` namedtuple.\n\n  Raises:\n    ValueError: if `split_name` is not a valid train/validation split.\n  """"""\n  if split_name not in SPLITS_TO_SIZES:\n    raise ValueError(\'split name %s was not recognized.\' % split_name)\n\n  if not file_pattern:\n    file_pattern = _FILE_PATTERN\n  file_pattern = os.path.join(dataset_dir, file_pattern % split_name)\n\n  # Allowing None in the signature so that dataset_factory can use the default.\n  if reader is None:\n    reader = tf.TFRecordReader\n\n  keys_to_features = {\n      \'image/encoded\': tf.FixedLenFeature((), tf.string, default_value=\'\'),\n      \'image/format\': tf.FixedLenFeature((), tf.string, default_value=\'png\'),\n      \'image/class/label\': tf.FixedLenFeature(\n          [], tf.int64, default_value=tf.zeros([], dtype=tf.int64)),\n  }\n\n  items_to_handlers = {\n      \'image\': slim.tfexample_decoder.Image(),\n      \'label\': slim.tfexample_decoder.Tensor(\'image/class/label\'),\n  }\n\n  decoder = slim.tfexample_decoder.TFExampleDecoder(\n      keys_to_features, items_to_handlers)\n\n  labels_to_names = None\n  if dataset_utils.has_labels(dataset_dir):\n    labels_to_names = dataset_utils.read_label_file(dataset_dir)\n\n  return slim.dataset.Dataset(\n      data_sources=file_pattern,\n      reader=reader,\n      decoder=decoder,\n      num_samples=SPLITS_TO_SIZES[split_name],\n      items_to_descriptions=_ITEMS_TO_DESCRIPTIONS,\n      num_classes=_NUM_CLASSES,\n      labels_to_names=labels_to_names)\n'"
vendor/tensorflow/models/research/slim/datasets/imagenet.py,20,"b'# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Provides data for the ImageNet ILSVRC 2012 Dataset plus some bounding boxes.\n\nSome images have one or more bounding boxes associated with the label of the\nimage. See details here: http://image-net.org/download-bboxes\n\nImageNet is based upon WordNet 3.0. To uniquely identify a synset, we use\n""WordNet ID"" (wnid), which is a concatenation of POS ( i.e. part of speech )\nand SYNSET OFFSET of WordNet. For more information, please refer to the\nWordNet documentation[http://wordnet.princeton.edu/wordnet/documentation/].\n\n""There are bounding boxes for over 3000 popular synsets available.\nFor each synset, there are on average 150 images with bounding boxes.""\n\nWARNING: Don\'t use for object detection, in this case all the bounding boxes\nof the image belong to just one class.\n""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport os\nfrom six.moves import urllib\nimport tensorflow as tf\n\nfrom vendor.tensorflow.models.research.slim.datasets import dataset_utils\n\nslim = tf.contrib.slim\n\n# TODO(nsilberman): Add tfrecord file type once the script is updated.\n_FILE_PATTERN = \'%s-*\'\n\n_SPLITS_TO_SIZES = {\n    \'train\': 1281167,\n    \'validation\': 50000,\n}\n\n_ITEMS_TO_DESCRIPTIONS = {\n    \'image\': \'A color image of varying height and width.\',\n    \'label\': \'The label id of the image, integer between 0 and 999\',\n    \'label_text\': \'The text of the label.\',\n    \'object/bbox\': \'A list of bounding boxes.\',\n    \'object/label\': \'A list of labels, one per each object.\',\n}\n\n_NUM_CLASSES = 1001\n\n\ndef create_readable_names_for_imagenet_labels():\n  """"""Create a dict mapping label id to human readable string.\n\n  Returns:\n      labels_to_names: dictionary where keys are integers from to 1000\n      and values are human-readable names.\n\n  We retrieve a synset file, which contains a list of valid synset labels used\n  by ILSVRC competition. There is one synset one per line, eg.\n          #   n01440764\n          #   n01443537\n  We also retrieve a synset_to_human_file, which contains a mapping from synsets\n  to human-readable names for every synset in Imagenet. These are stored in a\n  tsv format, as follows:\n          #   n02119247    black fox\n          #   n02119359    silver fox\n  We assign each synset (in alphabetical order) an integer, starting from 1\n  (since 0 is reserved for the background class).\n\n  Code is based on\n  https://github.com/tensorflow/models/blob/master/research/inception/inception/data/build_imagenet_data.py#L463\n  """"""\n\n  # pylint: disable=g-line-too-long\n  base_url = \'https://raw.githubusercontent.com/tensorflow/models/master/research/inception/inception/data/\'\n  synset_url = \'{}/imagenet_lsvrc_2015_synsets.txt\'.format(base_url)\n  synset_to_human_url = \'{}/imagenet_metadata.txt\'.format(base_url)\n\n  filename, _ = urllib.request.urlretrieve(synset_url)\n  synset_list = [s.strip() for s in open(filename).readlines()]\n  num_synsets_in_ilsvrc = len(synset_list)\n  assert num_synsets_in_ilsvrc == 1000\n\n  filename, _ = urllib.request.urlretrieve(synset_to_human_url)\n  synset_to_human_list = open(filename).readlines()\n  num_synsets_in_all_imagenet = len(synset_to_human_list)\n  assert num_synsets_in_all_imagenet == 21842\n\n  synset_to_human = {}\n  for s in synset_to_human_list:\n    parts = s.strip().split(\'\\t\')\n    assert len(parts) == 2\n    synset = parts[0]\n    human = parts[1]\n    synset_to_human[synset] = human\n\n  label_index = 1\n  labels_to_names = {0: \'background\'}\n  for synset in synset_list:\n    name = synset_to_human[synset]\n    labels_to_names[label_index] = name\n    label_index += 1\n\n  return labels_to_names\n\n\ndef get_split(split_name, dataset_dir, file_pattern=None, reader=None):\n  """"""Gets a dataset tuple with instructions for reading ImageNet.\n\n  Args:\n    split_name: A train/test split name.\n    dataset_dir: The base directory of the dataset sources.\n    file_pattern: The file pattern to use when matching the dataset sources.\n      It is assumed that the pattern contains a \'%s\' string so that the split\n      name can be inserted.\n    reader: The TensorFlow reader type.\n\n  Returns:\n    A `Dataset` namedtuple.\n\n  Raises:\n    ValueError: if `split_name` is not a valid train/test split.\n  """"""\n  if split_name not in _SPLITS_TO_SIZES:\n    raise ValueError(\'split name %s was not recognized.\' % split_name)\n\n  if not file_pattern:\n    file_pattern = _FILE_PATTERN\n  file_pattern = os.path.join(dataset_dir, file_pattern % split_name)\n\n  # Allowing None in the signature so that dataset_factory can use the default.\n  if reader is None:\n    reader = tf.TFRecordReader\n\n  keys_to_features = {\n      \'image/encoded\': tf.FixedLenFeature(\n          (), tf.string, default_value=\'\'),\n      \'image/format\': tf.FixedLenFeature(\n          (), tf.string, default_value=\'jpeg\'),\n      \'image/class/label\': tf.FixedLenFeature(\n          [], dtype=tf.int64, default_value=-1),\n      \'image/class/text\': tf.FixedLenFeature(\n          [], dtype=tf.string, default_value=\'\'),\n      \'image/object/bbox/xmin\': tf.VarLenFeature(\n          dtype=tf.float32),\n      \'image/object/bbox/ymin\': tf.VarLenFeature(\n          dtype=tf.float32),\n      \'image/object/bbox/xmax\': tf.VarLenFeature(\n          dtype=tf.float32),\n      \'image/object/bbox/ymax\': tf.VarLenFeature(\n          dtype=tf.float32),\n      \'image/object/class/label\': tf.VarLenFeature(\n          dtype=tf.int64),\n  }\n\n  items_to_handlers = {\n      \'image\': slim.tfexample_decoder.Image(\'image/encoded\', \'image/format\'),\n      \'label\': slim.tfexample_decoder.Tensor(\'image/class/label\'),\n      \'label_text\': slim.tfexample_decoder.Tensor(\'image/class/text\'),\n      \'object/bbox\': slim.tfexample_decoder.BoundingBox(\n          [\'ymin\', \'xmin\', \'ymax\', \'xmax\'], \'image/object/bbox/\'),\n      \'object/label\': slim.tfexample_decoder.Tensor(\'image/object/class/label\'),\n  }\n\n  decoder = slim.tfexample_decoder.TFExampleDecoder(\n      keys_to_features, items_to_handlers)\n\n  labels_to_names = None\n  if dataset_utils.has_labels(dataset_dir):\n    labels_to_names = dataset_utils.read_label_file(dataset_dir)\n  else:\n    labels_to_names = create_readable_names_for_imagenet_labels()\n    dataset_utils.write_label_file(labels_to_names, dataset_dir)\n\n  return slim.dataset.Dataset(\n      data_sources=file_pattern,\n      reader=reader,\n      decoder=decoder,\n      num_samples=_SPLITS_TO_SIZES[split_name],\n      items_to_descriptions=_ITEMS_TO_DESCRIPTIONS,\n      num_classes=_NUM_CLASSES,\n      labels_to_names=labels_to_names)\n'"
vendor/tensorflow/models/research/slim/datasets/mnist.py,6,"b'# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Provides data for the MNIST dataset.\n\nThe dataset scripts used to create the dataset can be found at:\ntensorflow/models/research/slim/datasets/download_and_convert_mnist.py\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport os\nimport tensorflow as tf\n\nfrom vendor.tensorflow.models.research.slim.datasets import dataset_utils\n\nslim = tf.contrib.slim\n\n_FILE_PATTERN = \'mnist_%s.tfrecord\'\n\n_SPLITS_TO_SIZES = {\'train\': 60000, \'test\': 10000}\n\n_NUM_CLASSES = 10\n\n_ITEMS_TO_DESCRIPTIONS = {\n    \'image\': \'A [28 x 28 x 1] grayscale image.\',\n    \'label\': \'A single integer between 0 and 9\',\n}\n\n\ndef get_split(split_name, dataset_dir, file_pattern=None, reader=None):\n  """"""Gets a dataset tuple with instructions for reading MNIST.\n\n  Args:\n    split_name: A train/test split name.\n    dataset_dir: The base directory of the dataset sources.\n    file_pattern: The file pattern to use when matching the dataset sources.\n      It is assumed that the pattern contains a \'%s\' string so that the split\n      name can be inserted.\n    reader: The TensorFlow reader type.\n\n  Returns:\n    A `Dataset` namedtuple.\n\n  Raises:\n    ValueError: if `split_name` is not a valid train/test split.\n  """"""\n  if split_name not in _SPLITS_TO_SIZES:\n    raise ValueError(\'split name %s was not recognized.\' % split_name)\n\n  if not file_pattern:\n    file_pattern = _FILE_PATTERN\n  file_pattern = os.path.join(dataset_dir, file_pattern % split_name)\n\n  # Allowing None in the signature so that dataset_factory can use the default.\n  if reader is None:\n    reader = tf.TFRecordReader\n\n  keys_to_features = {\n      \'image/encoded\': tf.FixedLenFeature((), tf.string, default_value=\'\'),\n      \'image/format\': tf.FixedLenFeature((), tf.string, default_value=\'raw\'),\n      \'image/class/label\': tf.FixedLenFeature(\n          [1], tf.int64, default_value=tf.zeros([1], dtype=tf.int64)),\n  }\n\n  items_to_handlers = {\n      \'image\': slim.tfexample_decoder.Image(shape=[28, 28, 1], channels=1),\n      \'label\': slim.tfexample_decoder.Tensor(\'image/class/label\', shape=[]),\n  }\n\n  decoder = slim.tfexample_decoder.TFExampleDecoder(\n      keys_to_features, items_to_handlers)\n\n  labels_to_names = None\n  if dataset_utils.has_labels(dataset_dir):\n    labels_to_names = dataset_utils.read_label_file(dataset_dir)\n\n  return slim.dataset.Dataset(\n      data_sources=file_pattern,\n      reader=reader,\n      decoder=decoder,\n      num_samples=_SPLITS_TO_SIZES[split_name],\n      num_classes=_NUM_CLASSES,\n      items_to_descriptions=_ITEMS_TO_DESCRIPTIONS,\n      labels_to_names=labels_to_names)\n'"
vendor/tensorflow/models/research/slim/datasets/preprocess_imagenet_validation_data.py,0,"b'#!/usr/bin/python\n# Copyright 2016 Google Inc. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\nr""""""Process the ImageNet Challenge bounding boxes for TensorFlow model training.\n\nAssociate the ImageNet 2012 Challenge validation data set with labels.\n\nThe raw ImageNet validation data set is expected to reside in JPEG files\nlocated in the following directory structure.\n\n data_dir/ILSVRC2012_val_00000001.JPEG\n data_dir/ILSVRC2012_val_00000002.JPEG\n ...\n data_dir/ILSVRC2012_val_00050000.JPEG\n\nThis script moves the files into a directory structure like such:\n data_dir/n01440764/ILSVRC2012_val_00000293.JPEG\n data_dir/n01440764/ILSVRC2012_val_00000543.JPEG\n ...\nwhere \'n01440764\' is the unique synset label associated with\nthese images.\n\nThis directory reorganization requires a mapping from validation image\nnumber (i.e. suffix of the original file) to the associated label. This\nis provided in the ImageNet development kit via a Matlab file.\n\nIn order to make life easier and divorce ourselves from Matlab, we instead\nsupply a custom text file that provides this mapping for us.\n\nSample usage:\n  ./preprocess_imagenet_validation_data.py ILSVRC2012_img_val \\\n  imagenet_2012_validation_synset_labels.txt\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport os\nimport sys\n\nfrom six.moves import xrange  # pylint: disable=redefined-builtin\n\n\nif __name__ == \'__main__\':\n  if len(sys.argv) < 3:\n    print(\'Invalid usage\\n\'\n          \'usage: preprocess_imagenet_validation_data.py \'\n          \'<validation data dir> <validation labels file>\')\n    sys.exit(-1)\n  data_dir = sys.argv[1]\n  validation_labels_file = sys.argv[2]\n\n  # Read in the 50000 synsets associated with the validation data set.\n  labels = [l.strip() for l in open(validation_labels_file).readlines()]\n  unique_labels = set(labels)\n\n  # Make all sub-directories in the validation data dir.\n  for label in unique_labels:\n    labeled_data_dir = os.path.join(data_dir, label)\n    os.makedirs(labeled_data_dir)\n\n  # Move all of the image to the appropriate sub-directory.\n  for i in xrange(len(labels)):\n    basename = \'ILSVRC2012_val_000%.5d.JPEG\' % (i + 1)\n    original_filename = os.path.join(data_dir, basename)\n    if not os.path.exists(original_filename):\n      print(\'Failed to find: \' % original_filename)\n      sys.exit(-1)\n    new_filename = os.path.join(data_dir, labels[i], basename)\n    os.rename(original_filename, new_filename)\n'"
vendor/tensorflow/models/research/slim/datasets/process_bounding_boxes.py,0,"b'#!/usr/bin/python\n# Copyright 2016 Google Inc. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Process the ImageNet Challenge bounding boxes for TensorFlow model training.\n\nThis script is called as\n\nprocess_bounding_boxes.py <dir> [synsets-file]\n\nWhere <dir> is a directory containing the downloaded and unpacked bounding box\ndata. If [synsets-file] is supplied, then only the bounding boxes whose\nsynstes are contained within this file are returned. Note that the\n[synsets-file] file contains synset ids, one per line.\n\nThe script dumps out a CSV text file in which each line contains an entry.\n  n00007846_64193.JPEG,0.0060,0.2620,0.7545,0.9940\n\nThe entry can be read as:\n  <JPEG file name>, <xmin>, <ymin>, <xmax>, <ymax>\n\nThe bounding box for <JPEG file name> contains two points (xmin, ymin) and\n(xmax, ymax) specifying the lower-left corner and upper-right corner of a\nbounding box in *relative* coordinates.\n\nThe user supplies a directory where the XML files reside. The directory\nstructure in the directory <dir> is assumed to look like this:\n\n<dir>/nXXXXXXXX/nXXXXXXXX_YYYY.xml\n\nEach XML file contains a bounding box annotation. The script:\n\n (1) Parses the XML file and extracts the filename, label and bounding box info.\n\n (2) The bounding box is specified in the XML files as integer (xmin, ymin) and\n    (xmax, ymax) *relative* to image size displayed to the human annotator. The\n    size of the image displayed to the human annotator is stored in the XML file\n    as integer (height, width).\n\n    Note that the displayed size will differ from the actual size of the image\n    downloaded from image-net.org. To make the bounding box annotation useable,\n    we convert bounding box to floating point numbers relative to displayed\n    height and width of the image.\n\n    Note that each XML file might contain N bounding box annotations.\n\n    Note that the points are all clamped at a range of [0.0, 1.0] because some\n    human annotations extend outside the range of the supplied image.\n\n    See details here: http://image-net.org/download-bboxes\n\n(3) By default, the script outputs all valid bounding boxes. If a\n    [synsets-file] is supplied, only the subset of bounding boxes associated\n    with those synsets are outputted. Importantly, one can supply a list of\n    synsets in the ImageNet Challenge and output the list of bounding boxes\n    associated with the training images of the ILSVRC.\n\n    We use these bounding boxes to inform the random distortion of images\n    supplied to the network.\n\nIf you run this script successfully, you will see the following output\nto stderr:\n> Finished processing 544546 XML files.\n> Skipped 0 XML files not in ImageNet Challenge.\n> Skipped 0 bounding boxes not in ImageNet Challenge.\n> Wrote 615299 bounding boxes from 544546 annotated images.\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport glob\nimport os.path\nimport sys\nimport xml.etree.ElementTree as ET\nfrom six.moves import xrange  # pylint: disable=redefined-builtin\n\n\nclass BoundingBox(object):\n  pass\n\n\ndef GetItem(name, root, index=0):\n  count = 0\n  for item in root.iter(name):\n    if count == index:\n      return item.text\n    count += 1\n  # Failed to find ""index"" occurrence of item.\n  return -1\n\n\ndef GetInt(name, root, index=0):\n  return int(GetItem(name, root, index))\n\n\ndef FindNumberBoundingBoxes(root):\n  index = 0\n  while True:\n    if GetInt(\'xmin\', root, index) == -1:\n      break\n    index += 1\n  return index\n\n\ndef ProcessXMLAnnotation(xml_file):\n  """"""Process a single XML file containing a bounding box.""""""\n  # pylint: disable=broad-except\n  try:\n    tree = ET.parse(xml_file)\n  except Exception:\n    print(\'Failed to parse: \' + xml_file, file=sys.stderr)\n    return None\n  # pylint: enable=broad-except\n  root = tree.getroot()\n\n  num_boxes = FindNumberBoundingBoxes(root)\n  boxes = []\n\n  for index in xrange(num_boxes):\n    box = BoundingBox()\n    # Grab the \'index\' annotation.\n    box.xmin = GetInt(\'xmin\', root, index)\n    box.ymin = GetInt(\'ymin\', root, index)\n    box.xmax = GetInt(\'xmax\', root, index)\n    box.ymax = GetInt(\'ymax\', root, index)\n\n    box.width = GetInt(\'width\', root)\n    box.height = GetInt(\'height\', root)\n    box.filename = GetItem(\'filename\', root) + \'.JPEG\'\n    box.label = GetItem(\'name\', root)\n\n    xmin = float(box.xmin) / float(box.width)\n    xmax = float(box.xmax) / float(box.width)\n    ymin = float(box.ymin) / float(box.height)\n    ymax = float(box.ymax) / float(box.height)\n\n    # Some images contain bounding box annotations that\n    # extend outside of the supplied image. See, e.g.\n    # n03127925/n03127925_147.xml\n    # Additionally, for some bounding boxes, the min > max\n    # or the box is entirely outside of the image.\n    min_x = min(xmin, xmax)\n    max_x = max(xmin, xmax)\n    box.xmin_scaled = min(max(min_x, 0.0), 1.0)\n    box.xmax_scaled = min(max(max_x, 0.0), 1.0)\n\n    min_y = min(ymin, ymax)\n    max_y = max(ymin, ymax)\n    box.ymin_scaled = min(max(min_y, 0.0), 1.0)\n    box.ymax_scaled = min(max(max_y, 0.0), 1.0)\n\n    boxes.append(box)\n\n  return boxes\n\nif __name__ == \'__main__\':\n  if len(sys.argv) < 2 or len(sys.argv) > 3:\n    print(\'Invalid usage\\n\'\n          \'usage: process_bounding_boxes.py <dir> [synsets-file]\',\n          file=sys.stderr)\n    sys.exit(-1)\n\n  xml_files = glob.glob(sys.argv[1] + \'/*/*.xml\')\n  print(\'Identified %d XML files in %s\' % (len(xml_files), sys.argv[1]),\n        file=sys.stderr)\n\n  if len(sys.argv) == 3:\n    labels = set([l.strip() for l in open(sys.argv[2]).readlines()])\n    print(\'Identified %d synset IDs in %s\' % (len(labels), sys.argv[2]),\n          file=sys.stderr)\n  else:\n    labels = None\n\n  skipped_boxes = 0\n  skipped_files = 0\n  saved_boxes = 0\n  saved_files = 0\n  for file_index, one_file in enumerate(xml_files):\n    # Example: <...>/n06470073/n00141669_6790.xml\n    label = os.path.basename(os.path.dirname(one_file))\n\n    # Determine if the annotation is from an ImageNet Challenge label.\n    if labels is not None and label not in labels:\n      skipped_files += 1\n      continue\n\n    bboxes = ProcessXMLAnnotation(one_file)\n    assert bboxes is not None, \'No bounding boxes found in \' + one_file\n\n    found_box = False\n    for bbox in bboxes:\n      if labels is not None:\n        if bbox.label != label:\n          # Note: There is a slight bug in the bounding box annotation data.\n          # Many of the dog labels have the human label \'Scottish_deerhound\'\n          # instead of the synset ID \'n02092002\' in the bbox.label field. As a\n          # simple hack to overcome this issue, we only exclude bbox labels\n          # *which are synset ID\'s* that do not match original synset label for\n          # the XML file.\n          if bbox.label in labels:\n            skipped_boxes += 1\n            continue\n\n      # Guard against improperly specified boxes.\n      if (bbox.xmin_scaled >= bbox.xmax_scaled or\n          bbox.ymin_scaled >= bbox.ymax_scaled):\n        skipped_boxes += 1\n        continue\n\n      # Note bbox.filename occasionally contains \'%s\' in the name. This is\n      # data set noise that is fixed by just using the basename of the XML file.\n      image_filename = os.path.splitext(os.path.basename(one_file))[0]\n      print(\'%s.JPEG,%.4f,%.4f,%.4f,%.4f\' %\n            (image_filename,\n             bbox.xmin_scaled, bbox.ymin_scaled,\n             bbox.xmax_scaled, bbox.ymax_scaled))\n\n      saved_boxes += 1\n      found_box = True\n    if found_box:\n      saved_files += 1\n    else:\n      skipped_files += 1\n\n    if not file_index % 5000:\n      print(\'--> processed %d of %d XML files.\' %\n            (file_index + 1, len(xml_files)),\n            file=sys.stderr)\n      print(\'--> skipped %d boxes and %d XML files.\' %\n            (skipped_boxes, skipped_files), file=sys.stderr)\n\n  print(\'Finished processing %d XML files.\' % len(xml_files), file=sys.stderr)\n  print(\'Skipped %d XML files not in ImageNet Challenge.\' % skipped_files,\n        file=sys.stderr)\n  print(\'Skipped %d bounding boxes not in ImageNet Challenge.\' % skipped_boxes,\n        file=sys.stderr)\n  print(\'Wrote %d bounding boxes from %d annotated images.\' %\n        (saved_boxes, saved_files),\n        file=sys.stderr)\n  print(\'Finished.\', file=sys.stderr)\n'"
vendor/tensorflow/models/research/slim/deployment/__init__.py,0,b'\n'
vendor/tensorflow/models/research/slim/deployment/model_deploy.py,54,"b'# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Deploy Slim models across multiple clones and replicas.\n\n# TODO(sguada) docstring paragraph by (a) motivating the need for the file and\n# (b) defining clones.\n\n# TODO(sguada) describe the high-level components of model deployment.\n# E.g. ""each model deployment is composed of several parts: a DeploymentConfig,\n# which captures A, B and C, an input_fn which loads data.. etc\n\nTo easily train a model on multiple GPUs or across multiple machines this\nmodule provides a set of helper functions: `create_clones`,\n`optimize_clones` and `deploy`.\n\nUsage:\n\n  g = tf.Graph()\n\n  # Set up DeploymentConfig\n  config = model_deploy.DeploymentConfig(num_clones=2, clone_on_cpu=True)\n\n  # Create the global step on the device storing the variables.\n  with tf.device(config.variables_device()):\n    global_step = slim.create_global_step()\n\n  # Define the inputs\n  with tf.device(config.inputs_device()):\n    images, labels = LoadData(...)\n    inputs_queue = slim.data.prefetch_queue((images, labels))\n\n  # Define the optimizer.\n  with tf.device(config.optimizer_device()):\n    optimizer = tf.train.MomentumOptimizer(FLAGS.learning_rate, FLAGS.momentum)\n\n  # Define the model including the loss.\n  def model_fn(inputs_queue):\n    images, labels = inputs_queue.dequeue()\n    predictions = CreateNetwork(images)\n    slim.losses.log_loss(predictions, labels)\n\n  model_dp = model_deploy.deploy(config, model_fn, [inputs_queue],\n                                 optimizer=optimizer)\n\n  # Run training.\n  slim.learning.train(model_dp.train_op, my_log_dir,\n                      summary_op=model_dp.summary_op)\n\nThe Clone namedtuple holds together the values associated with each call to\nmodel_fn:\n  * outputs: The return values of the calls to `model_fn()`.\n  * scope: The scope used to create the clone.\n  * device: The device used to create the clone.\n\nDeployedModel namedtuple, holds together the values needed to train multiple\nclones:\n  * train_op: An operation that run the optimizer training op and include\n    all the update ops created by `model_fn`. Present only if an optimizer\n    was specified.\n  * summary_op: An operation that run the summaries created by `model_fn`\n    and process_gradients.\n  * total_loss: A `Tensor` that contains the sum of all losses created by\n    `model_fn` plus the regularization losses.\n  * clones: List of `Clone` tuples returned by `create_clones()`.\n\nDeploymentConfig parameters:\n  * num_clones: Number of model clones to deploy in each replica.\n  * clone_on_cpu: True if clones should be placed on CPU.\n  * replica_id: Integer.  Index of the replica for which the model is\n      deployed.  Usually 0 for the chief replica.\n  * num_replicas: Number of replicas to use.\n  * num_ps_tasks: Number of tasks for the `ps` job. 0 to not use replicas.\n  * worker_job_name: A name for the worker job.\n  * ps_job_name: A name for the parameter server job.\n\nTODO(sguada):\n  - describe side effect to the graph.\n  - what happens to summaries and update_ops.\n  - which graph collections are altered.\n  - write a tutorial on how to use this.\n  - analyze the possibility of calling deploy more than once.\n\n\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport collections\n\nimport tensorflow as tf\n\nslim = tf.contrib.slim\n\n__all__ = [\'create_clones\',\n           \'deploy\',\n           \'optimize_clones\',\n           \'DeployedModel\',\n           \'DeploymentConfig\',\n           \'Clone\',\n           ]\n\n# Namedtuple used to represent a clone during deployment.\nClone = collections.namedtuple(\'Clone\',\n                               [\'outputs\',  # Whatever model_fn() returned.\n                                \'scope\',  # The scope used to create it.\n                                \'device\',  # The device used to create.\n                                ])\n\n# Namedtuple used to represent a DeployedModel, returned by deploy().\nDeployedModel = collections.namedtuple(\'DeployedModel\',\n                                       [\'train_op\',  # The `train_op`\n                                        \'summary_op\',  # The `summary_op`\n                                        \'total_loss\',  # The loss `Tensor`\n                                        \'clones\',  # A list of `Clones` tuples.\n                                        ])\n\n# Default parameters for DeploymentConfig\n_deployment_params = {\'num_clones\': 1,\n                      \'clone_on_cpu\': False,\n                      \'replica_id\': 0,\n                      \'num_replicas\': 1,\n                      \'num_ps_tasks\': 0,\n                      \'worker_job_name\': \'worker\',\n                      \'ps_job_name\': \'ps\'}\n\n\ndef create_clones(config, model_fn, args=None, kwargs=None):\n    """"""Creates multiple clones according to config using a `model_fn`.\n\n    The returned values of `model_fn(*args, **kwargs)` are collected along with\n    the scope and device used to created it in a namedtuple\n    `Clone(outputs, scope, device)`\n\n    Note: it is assumed that any loss created by `model_fn` is collected at\n    the tf.GraphKeys.LOSSES collection.\n\n    To recover the losses, summaries or update_ops created by the clone use:\n    ```python\n      losses = tf.get_collection(tf.GraphKeys.LOSSES, clone.scope)\n      summaries = tf.get_collection(tf.GraphKeys.SUMMARIES, clone.scope)\n      update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS, clone.scope)\n    ```\n\n    The deployment options are specified by the config object and support\n    deploying one or several clones on different GPUs and one or several replicas\n    of such clones.\n\n    The argument `model_fn` is called `config.num_clones` times to create the\n    model clones as `model_fn(*args, **kwargs)`.\n\n    If `config` specifies deployment on multiple replicas then the default\n    tensorflow device is set appropriatly for each call to `model_fn` and for the\n    slim variable creation functions: model and global variables will be created\n    on the `ps` device, the clone operations will be on the `worker` device.\n\n    Args:\n      config: A DeploymentConfig object.\n      model_fn: A callable. Called as `model_fn(*args, **kwargs)`\n      args: Optional list of arguments to pass to `model_fn`.\n      kwargs: Optional list of keyword arguments to pass to `model_fn`.\n\n    Returns:\n      A list of namedtuples `Clone`.\n    """"""\n    clones = []\n    args = args or []\n    kwargs = kwargs or {}\n    with slim.arg_scope([slim.model_variable, slim.variable],\n                        device=config.variables_device()):\n        # Create clones.\n        for i in range(0, config.num_clones):\n            with tf.name_scope(config.clone_scope(i)) as clone_scope:\n                clone_device = config.clone_device(i)\n                with tf.device(clone_device):\n                    with tf.variable_scope(tf.get_variable_scope(),\n                                           reuse=True if i > 0 else None):\n                        outputs = model_fn(*args, **kwargs)\n                    clones.append(Clone(outputs, clone_scope, clone_device))\n    return clones\n\n\ndef _gather_clone_loss(clone, num_clones, regularization_losses):\n    """"""Gather the loss for a single clone.\n\n    Args:\n      clone: A Clone namedtuple.\n      num_clones: The number of clones being deployed.\n      regularization_losses: Possibly empty list of regularization_losses\n        to add to the clone losses.\n\n    Returns:\n      A tensor for the total loss for the clone.  Can be None.\n    """"""\n    # The return value.\n    sum_loss = None\n    # Individual components of the loss that will need summaries.\n    clone_loss = None\n    regularization_loss = None\n    # Compute and aggregate losses on the clone device.\n    with tf.device(clone.device):\n        all_losses = []\n        clone_losses = tf.get_collection(tf.GraphKeys.LOSSES, clone.scope)\n        if clone_losses:\n            clone_loss = tf.add_n(clone_losses, name=\'clone_loss\')\n            if num_clones > 1:\n                clone_loss = tf.div(clone_loss, 1.0 * num_clones,\n                                    name=\'scaled_clone_loss\')\n            all_losses.append(clone_loss)\n        if regularization_losses:\n            regularization_loss = tf.add_n(regularization_losses,\n                                           name=\'regularization_loss\')\n            all_losses.append(regularization_loss)\n        if all_losses:\n            sum_loss = tf.add_n(all_losses)\n    # Add the summaries out of the clone device block.\n    if clone_loss is not None:\n        tf.summary.scalar(\'/\'.join(filter(None,\n                                          [\'Losses\', clone.scope, \'clone_loss\'])),\n                          clone_loss)\n    if regularization_loss is not None:\n        tf.summary.scalar(\'Losses/regularization_loss\', regularization_loss)\n    return sum_loss\n\n\ndef _optimize_clone(optimizer, clone, num_clones, regularization_losses,\n                    **kwargs):\n    """"""Compute losses and gradients for a single clone.\n\n    Args:\n      optimizer: A tf.Optimizer  object.\n      clone: A Clone namedtuple.\n      num_clones: The number of clones being deployed.\n      regularization_losses: Possibly empty list of regularization_losses\n        to add to the clone losses.\n      **kwargs: Dict of kwarg to pass to compute_gradients().\n\n    Returns:\n      A tuple (clone_loss, clone_grads_and_vars).\n        - clone_loss: A tensor for the total loss for the clone.  Can be None.\n        - clone_grads_and_vars: List of (gradient, variable) for the clone.\n          Can be empty.\n    """"""\n    sum_loss = _gather_clone_loss(clone, num_clones, regularization_losses)\n    clone_grad = None\n    if sum_loss is not None:\n        with tf.device(clone.device):\n            clone_grad = optimizer.compute_gradients(sum_loss, **kwargs)\n    return sum_loss, clone_grad\n\n\ndef optimize_clones(clones, optimizer,\n                    regularization_losses=None,\n                    **kwargs):\n    """"""Compute clone losses and gradients for the given list of `Clones`.\n\n    Note: The regularization_losses are added to the first clone losses.\n\n    Args:\n     clones: List of `Clones` created by `create_clones()`.\n     optimizer: An `Optimizer` object.\n     regularization_losses: Optional list of regularization losses. If None it\n       will gather them from tf.GraphKeys.REGULARIZATION_LOSSES. Pass `[]` to\n       exclude them.\n     **kwargs: Optional list of keyword arguments to pass to `compute_gradients`.\n\n    Returns:\n     A tuple (total_loss, grads_and_vars).\n       - total_loss: A Tensor containing the average of the clone losses including\n         the regularization loss.\n       - grads_and_vars: A List of tuples (gradient, variable) containing the sum\n         of the gradients for each variable.\n\n    """"""\n    grads_and_vars = []\n    clones_losses = []\n    num_clones = len(clones)\n    if regularization_losses is None:\n        regularization_losses = tf.get_collection(\n            tf.GraphKeys.REGULARIZATION_LOSSES)\n    for clone in clones:\n        with tf.name_scope(clone.scope):\n            clone_loss, clone_grad = _optimize_clone(\n                optimizer, clone, num_clones, regularization_losses, **kwargs)\n            if clone_loss is not None:\n                clones_losses.append(clone_loss)\n                grads_and_vars.append(clone_grad)\n            # Only use regularization_losses for the first clone\n            regularization_losses = None\n    # Compute the total_loss summing all the clones_losses.\n    total_loss = tf.add_n(clones_losses, name=\'total_loss\')\n    # Sum the gradients across clones.\n    grads_and_vars = _sum_clones_gradients(grads_and_vars)\n    return total_loss, grads_and_vars\n\n\ndef deploy(config,\n           model_fn,\n           args=None,\n           kwargs=None,\n           optimizer=None,\n           summarize_gradients=False):\n    """"""Deploys a Slim-constructed model across multiple clones.\n\n    The deployment options are specified by the config object and support\n    deploying one or several clones on different GPUs and one or several replicas\n    of such clones.\n\n    The argument `model_fn` is called `config.num_clones` times to create the\n    model clones as `model_fn(*args, **kwargs)`.\n\n    The optional argument `optimizer` is an `Optimizer` object.  If not `None`,\n    the deployed model is configured for training with that optimizer.\n\n    If `config` specifies deployment on multiple replicas then the default\n    tensorflow device is set appropriatly for each call to `model_fn` and for the\n    slim variable creation functions: model and global variables will be created\n    on the `ps` device, the clone operations will be on the `worker` device.\n\n    Args:\n      config: A `DeploymentConfig` object.\n      model_fn: A callable. Called as `model_fn(*args, **kwargs)`\n      args: Optional list of arguments to pass to `model_fn`.\n      kwargs: Optional list of keyword arguments to pass to `model_fn`.\n      optimizer: Optional `Optimizer` object.  If passed the model is deployed\n        for training with that optimizer.\n      summarize_gradients: Whether or not add summaries to the gradients.\n\n    Returns:\n      A `DeployedModel` namedtuple.\n\n    """"""\n    # Gather initial summaries.\n    summaries = set(tf.get_collection(tf.GraphKeys.SUMMARIES))\n\n    # Create Clones.\n    clones = create_clones(config, model_fn, args, kwargs)\n    first_clone = clones[0]\n\n    # Gather update_ops from the first clone. These contain, for example,\n    # the updates for the batch_norm variables created by model_fn.\n    update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS, first_clone.scope)\n\n    train_op = None\n    total_loss = None\n    with tf.device(config.optimizer_device()):\n        if optimizer:\n            # Place the global step on the device storing the variables.\n            with tf.device(config.variables_device()):\n                global_step = slim.get_or_create_global_step()\n\n            # Compute the gradients for the clones.\n            total_loss, clones_gradients = optimize_clones(clones, optimizer)\n\n            if clones_gradients:\n                if summarize_gradients:\n                    # Add summaries to the gradients.\n                    summaries |= set(_add_gradients_summaries(clones_gradients))\n\n                # Create gradient updates.\n                grad_updates = optimizer.apply_gradients(clones_gradients,\n                                                         global_step=global_step)\n                update_ops.append(grad_updates)\n\n                update_op = tf.group(*update_ops)\n                with tf.control_dependencies([update_op]):\n                    train_op = tf.identity(total_loss, name=\'train_op\')\n        else:\n            clones_losses = []\n            regularization_losses = tf.get_collection(\n                tf.GraphKeys.REGULARIZATION_LOSSES)\n            for clone in clones:\n                with tf.name_scope(clone.scope):\n                    clone_loss = _gather_clone_loss(clone, len(clones),\n                                                    regularization_losses)\n                    if clone_loss is not None:\n                        clones_losses.append(clone_loss)\n                    # Only use regularization_losses for the first clone\n                    regularization_losses = None\n            if clones_losses:\n                total_loss = tf.add_n(clones_losses, name=\'total_loss\')\n\n        # Add the summaries from the first clone. These contain the summaries\n        # created by model_fn and either optimize_clones() or _gather_clone_loss().\n        summaries |= set(tf.get_collection(tf.GraphKeys.SUMMARIES,\n                                           first_clone.scope))\n\n        if total_loss is not None:\n            # Add total_loss to summary.\n            summaries.add(tf.summary.scalar(\'total_loss\', total_loss))\n\n        if summaries:\n            # Merge all summaries together.\n            summary_op = tf.summary.merge(list(summaries), name=\'summary_op\')\n        else:\n            summary_op = None\n\n    return DeployedModel(train_op, summary_op, total_loss, clones)\n\n\ndef _sum_clones_gradients(clone_grads):\n    """"""Calculate the sum gradient for each shared variable across all clones.\n\n    This function assumes that the clone_grads has been scaled appropriately by\n    1 / num_clones.\n\n    Args:\n      clone_grads: A List of List of tuples (gradient, variable), one list per\n      `Clone`.\n\n    Returns:\n       List of tuples of (gradient, variable) where the gradient has been summed\n       across all clones.\n    """"""\n    sum_grads = []\n    for grad_and_vars in zip(*clone_grads):\n        # Note that each grad_and_vars looks like the following:\n        #   ((grad_var0_clone0, var0), ... (grad_varN_cloneN, varN))\n        grads = []\n        var = grad_and_vars[0][1]\n        for g, v in grad_and_vars:\n            assert v == var\n            if g is not None:\n                grads.append(g)\n        if grads:\n            if len(grads) > 1:\n                sum_grad = tf.add_n(grads, name=var.op.name + \'/sum_grads\')\n            else:\n                sum_grad = grads[0]\n            sum_grads.append((sum_grad, var))\n    return sum_grads\n\n\ndef _add_gradients_summaries(grads_and_vars):\n    """"""Add histogram summaries to gradients.\n\n    Note: The summaries are also added to the SUMMARIES collection.\n\n    Args:\n      grads_and_vars: A list of gradient to variable pairs (tuples).\n\n    Returns:\n      The _list_ of the added summaries for grads_and_vars.\n    """"""\n    summaries = []\n    for grad, var in grads_and_vars:\n        if grad is not None:\n            if isinstance(grad, tf.IndexedSlices):\n                grad_values = grad.values\n            else:\n                grad_values = grad\n            summaries.append(tf.summary.histogram(var.op.name + \':gradient\',\n                                                  grad_values))\n            summaries.append(tf.summary.histogram(var.op.name + \':gradient_norm\',\n                                                  tf.global_norm([grad_values])))\n        else:\n            tf.logging.info(\'Var %s has no gradient\', var.op.name)\n    return summaries\n\n\nclass DeploymentConfig(object):\n    """"""Configuration for deploying a model with `deploy()`.\n\n    You can pass an instance of this class to `deploy()` to specify exactly\n    how to deploy the model to build.  If you do not pass one, an instance built\n    from the default deployment_hparams will be used.\n    """"""\n\n    def __init__(self,\n                 num_clones=1,\n                 clone_on_cpu=False,\n                 replica_id=0,\n                 num_replicas=1,\n                 num_ps_tasks=0,\n                 worker_job_name=\'worker\',\n                 ps_job_name=\'ps\'):\n        """"""Create a DeploymentConfig.\n\n        The config describes how to deploy a model across multiple clones and\n        replicas.  The model will be replicated `num_clones` times in each replica.\n        If `clone_on_cpu` is True, each clone will placed on CPU.\n\n        If `num_replicas` is 1, the model is deployed via a single process.  In that\n        case `worker_device`, `num_ps_tasks`, and `ps_device` are ignored.\n\n        If `num_replicas` is greater than 1, then `worker_device` and `ps_device`\n        must specify TensorFlow devices for the `worker` and `ps` jobs and\n        `num_ps_tasks` must be positive.\n\n        Args:\n          num_clones: Number of model clones to deploy in each replica.\n          clone_on_cpu: If True clones would be placed on CPU.\n          replica_id: Integer.  Index of the replica for which the model is\n            deployed.  Usually 0 for the chief replica.\n          num_replicas: Number of replicas to use.\n          num_ps_tasks: Number of tasks for the `ps` job. 0 to not use replicas.\n          worker_job_name: A name for the worker job.\n          ps_job_name: A name for the parameter server job.\n\n        Raises:\n          ValueError: If the arguments are invalid.\n        """"""\n        if num_replicas > 1:\n            if num_ps_tasks < 1:\n                raise ValueError(\'When using replicas num_ps_tasks must be positive\')\n        if num_replicas > 1 or num_ps_tasks > 0:\n            if not worker_job_name:\n                raise ValueError(\'Must specify worker_job_name when using replicas\')\n            if not ps_job_name:\n                raise ValueError(\'Must specify ps_job_name when using parameter server\')\n        if replica_id >= num_replicas:\n            raise ValueError(\'replica_id must be less than num_replicas\')\n        self._num_clones = num_clones\n        self._clone_on_cpu = clone_on_cpu\n        self._replica_id = replica_id\n        self._num_replicas = num_replicas\n        self._num_ps_tasks = num_ps_tasks\n        self._ps_device = \'/job:\' + ps_job_name if num_ps_tasks > 0 else \'\'\n        self._worker_device = \'/job:\' + worker_job_name if num_ps_tasks > 0 else \'\'\n\n    @property\n    def num_clones(self):\n        return self._num_clones\n\n    @property\n    def clone_on_cpu(self):\n        return self._clone_on_cpu\n\n    @property\n    def replica_id(self):\n        return self._replica_id\n\n    @property\n    def num_replicas(self):\n        return self._num_replicas\n\n    @property\n    def num_ps_tasks(self):\n        return self._num_ps_tasks\n\n    @property\n    def ps_device(self):\n        return self._ps_device\n\n    @property\n    def worker_device(self):\n        return self._worker_device\n\n    def caching_device(self):\n        """"""Returns the device to use for caching variables.\n\n        Variables are cached on the worker CPU when using replicas.\n\n        Returns:\n          A device string or None if the variables do not need to be cached.\n        """"""\n        if self._num_ps_tasks > 0:\n            return lambda op: op.device\n        else:\n            return None\n\n    def clone_device(self, clone_index):\n        """"""Device used to create the clone and all the ops inside the clone.\n\n        Args:\n          clone_index: Int, representing the clone_index.\n\n        Returns:\n          A value suitable for `tf.device()`.\n\n        Raises:\n          ValueError: if `clone_index` is greater or equal to the number of clones"".\n        """"""\n        if clone_index >= self._num_clones:\n            raise ValueError(\'clone_index must be less than num_clones\')\n        device = \'\'\n        if self._num_ps_tasks > 0:\n            device += self._worker_device\n        if self._clone_on_cpu:\n            device += \'/device:CPU:0\'\n        else:\n            device += \'/device:GPU:%d\' % clone_index\n        return device\n\n    def clone_scope(self, clone_index):\n        """"""Name scope to create the clone.\n\n        Args:\n          clone_index: Int, representing the clone_index.\n\n        Returns:\n          A name_scope suitable for `tf.name_scope()`.\n\n        Raises:\n          ValueError: if `clone_index` is greater or equal to the number of clones"".\n        """"""\n        if clone_index >= self._num_clones:\n            raise ValueError(\'clone_index must be less than num_clones\')\n        scope = \'\'\n        if self._num_clones > 1:\n            scope = \'clone_%d\' % clone_index\n        return scope\n\n    def optimizer_device(self):\n        """"""Device to use with the optimizer.\n\n        Returns:\n          A value suitable for `tf.device()`.\n        """"""\n        if self._num_ps_tasks > 0 or self._num_clones > 0:\n            return self._worker_device + \'/device:CPU:0\'\n        else:\n            return \'\'\n\n    def inputs_device(self):\n        """"""Device to use to build the inputs.\n\n        Returns:\n          A value suitable for `tf.device()`.\n        """"""\n        device = \'\'\n        if self._num_ps_tasks > 0:\n            device += self._worker_device\n        device += \'/device:CPU:0\'\n        return device\n\n    def variables_device(self):\n        """"""Returns the device to use for variables created inside the clone.\n\n        Returns:\n          A value suitable for `tf.device()`.\n        """"""\n        device = \'\'\n        if self._num_ps_tasks > 0:\n            device += self._ps_device\n        device += \'/device:CPU:0\'\n\n        class _PSDeviceChooser(object):\n            """"""Slim device chooser for variables when using PS.""""""\n\n            def __init__(self, device, tasks):\n                self._device = device\n                self._tasks = tasks\n                self._task = 0\n\n            def choose(self, op):\n                if op.device:\n                    return op.device\n                node_def = op if isinstance(op, tf.NodeDef) else op.node_def\n                if node_def.op.startswith(\'Variable\'):\n                    t = self._task\n                    self._task = (self._task + 1) % self._tasks\n                    d = \'%s/task:%d\' % (self._device, t)\n                    return d\n                else:\n                    return op.device\n\n        if not self._num_ps_tasks:\n            return device\n        else:\n            chooser = _PSDeviceChooser(device, self._num_ps_tasks)\n            return chooser.choose\n'"
vendor/tensorflow/models/research/slim/deployment/model_deploy_test.py,96,"b'# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Tests for model_deploy.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport numpy as np\nimport tensorflow as tf\n\nfrom deployment import model_deploy\n\nslim = tf.contrib.slim\n\n\nclass DeploymentConfigTest(tf.test.TestCase):\n\n  def testDefaults(self):\n    deploy_config = model_deploy.DeploymentConfig()\n\n    self.assertEqual(slim.get_variables(), [])\n    self.assertEqual(deploy_config.caching_device(), None)\n    self.assertDeviceEqual(deploy_config.clone_device(0), \'GPU:0\')\n    self.assertEqual(deploy_config.clone_scope(0), \'\')\n    self.assertDeviceEqual(deploy_config.optimizer_device(), \'CPU:0\')\n    self.assertDeviceEqual(deploy_config.inputs_device(), \'CPU:0\')\n    self.assertDeviceEqual(deploy_config.variables_device(), \'CPU:0\')\n\n  def testCPUonly(self):\n    deploy_config = model_deploy.DeploymentConfig(clone_on_cpu=True)\n\n    self.assertEqual(deploy_config.caching_device(), None)\n    self.assertDeviceEqual(deploy_config.clone_device(0), \'CPU:0\')\n    self.assertEqual(deploy_config.clone_scope(0), \'\')\n    self.assertDeviceEqual(deploy_config.optimizer_device(), \'CPU:0\')\n    self.assertDeviceEqual(deploy_config.inputs_device(), \'CPU:0\')\n    self.assertDeviceEqual(deploy_config.variables_device(), \'CPU:0\')\n\n  def testMultiGPU(self):\n    deploy_config = model_deploy.DeploymentConfig(num_clones=2)\n\n    self.assertEqual(deploy_config.caching_device(), None)\n    self.assertDeviceEqual(deploy_config.clone_device(0), \'GPU:0\')\n    self.assertDeviceEqual(deploy_config.clone_device(1), \'GPU:1\')\n    self.assertEqual(deploy_config.clone_scope(0), \'clone_0\')\n    self.assertEqual(deploy_config.clone_scope(1), \'clone_1\')\n    self.assertDeviceEqual(deploy_config.optimizer_device(), \'CPU:0\')\n    self.assertDeviceEqual(deploy_config.inputs_device(), \'CPU:0\')\n    self.assertDeviceEqual(deploy_config.variables_device(), \'CPU:0\')\n\n  def testPS(self):\n    deploy_config = model_deploy.DeploymentConfig(num_clones=1, num_ps_tasks=1)\n\n    self.assertDeviceEqual(deploy_config.clone_device(0),\n                           \'/job:worker/device:GPU:0\')\n    self.assertEqual(deploy_config.clone_scope(0), \'\')\n    self.assertDeviceEqual(deploy_config.optimizer_device(),\n                           \'/job:worker/device:CPU:0\')\n    self.assertDeviceEqual(deploy_config.inputs_device(),\n                           \'/job:worker/device:CPU:0\')\n    with tf.device(deploy_config.variables_device()):\n      a = tf.Variable(0)\n      b = tf.Variable(0)\n      c = tf.no_op()\n      d = slim.variable(\'a\', [],\n                        caching_device=deploy_config.caching_device())\n    self.assertDeviceEqual(a.device, \'/job:ps/task:0/device:CPU:0\')\n    self.assertDeviceEqual(a.device, a.value().device)\n    self.assertDeviceEqual(b.device, \'/job:ps/task:0/device:CPU:0\')\n    self.assertDeviceEqual(b.device, b.value().device)\n    self.assertDeviceEqual(c.device, \'\')\n    self.assertDeviceEqual(d.device, \'/job:ps/task:0/device:CPU:0\')\n    self.assertDeviceEqual(d.value().device, \'\')\n\n  def testMultiGPUPS(self):\n    deploy_config = model_deploy.DeploymentConfig(num_clones=2, num_ps_tasks=1)\n\n    self.assertEqual(deploy_config.caching_device()(tf.no_op()), \'\')\n    self.assertDeviceEqual(deploy_config.clone_device(0),\n                           \'/job:worker/device:GPU:0\')\n    self.assertDeviceEqual(deploy_config.clone_device(1),\n                           \'/job:worker/device:GPU:1\')\n    self.assertEqual(deploy_config.clone_scope(0), \'clone_0\')\n    self.assertEqual(deploy_config.clone_scope(1), \'clone_1\')\n    self.assertDeviceEqual(deploy_config.optimizer_device(),\n                           \'/job:worker/device:CPU:0\')\n    self.assertDeviceEqual(deploy_config.inputs_device(),\n                           \'/job:worker/device:CPU:0\')\n\n  def testReplicasPS(self):\n    deploy_config = model_deploy.DeploymentConfig(num_replicas=2,\n                                                  num_ps_tasks=2)\n\n    self.assertDeviceEqual(deploy_config.clone_device(0),\n                           \'/job:worker/device:GPU:0\')\n    self.assertEqual(deploy_config.clone_scope(0), \'\')\n    self.assertDeviceEqual(deploy_config.optimizer_device(),\n                           \'/job:worker/device:CPU:0\')\n    self.assertDeviceEqual(deploy_config.inputs_device(),\n                           \'/job:worker/device:CPU:0\')\n\n  def testReplicasMultiGPUPS(self):\n    deploy_config = model_deploy.DeploymentConfig(num_replicas=2,\n                                                  num_clones=2,\n                                                  num_ps_tasks=2)\n    self.assertDeviceEqual(deploy_config.clone_device(0),\n                           \'/job:worker/device:GPU:0\')\n    self.assertDeviceEqual(deploy_config.clone_device(1),\n                           \'/job:worker/device:GPU:1\')\n    self.assertEqual(deploy_config.clone_scope(0), \'clone_0\')\n    self.assertEqual(deploy_config.clone_scope(1), \'clone_1\')\n    self.assertDeviceEqual(deploy_config.optimizer_device(),\n                           \'/job:worker/device:CPU:0\')\n    self.assertDeviceEqual(deploy_config.inputs_device(),\n                           \'/job:worker/device:CPU:0\')\n\n  def testVariablesPS(self):\n    deploy_config = model_deploy.DeploymentConfig(num_ps_tasks=2)\n\n    with tf.device(deploy_config.variables_device()):\n      a = tf.Variable(0)\n      b = tf.Variable(0)\n      c = tf.no_op()\n      d = slim.variable(\'a\', [],\n                        caching_device=deploy_config.caching_device())\n\n    self.assertDeviceEqual(a.device, \'/job:ps/task:0/device:CPU:0\')\n    self.assertDeviceEqual(a.device, a.value().device)\n    self.assertDeviceEqual(b.device, \'/job:ps/task:1/device:CPU:0\')\n    self.assertDeviceEqual(b.device, b.value().device)\n    self.assertDeviceEqual(c.device, \'\')\n    self.assertDeviceEqual(d.device, \'/job:ps/task:0/device:CPU:0\')\n    self.assertDeviceEqual(d.value().device, \'\')\n\n\ndef LogisticClassifier(inputs, labels, scope=None, reuse=None):\n  with tf.variable_scope(scope, \'LogisticClassifier\', [inputs, labels],\n                         reuse=reuse):\n    predictions = slim.fully_connected(inputs, 1, activation_fn=tf.sigmoid,\n                                       scope=\'fully_connected\')\n    slim.losses.log_loss(predictions, labels)\n    return predictions\n\n\ndef BatchNormClassifier(inputs, labels, scope=None, reuse=None):\n  with tf.variable_scope(scope, \'BatchNormClassifier\', [inputs, labels],\n                         reuse=reuse):\n    inputs = slim.batch_norm(inputs, decay=0.1, fused=True)\n    predictions = slim.fully_connected(inputs, 1,\n                                       activation_fn=tf.sigmoid,\n                                       scope=\'fully_connected\')\n    slim.losses.log_loss(predictions, labels)\n    return predictions\n\n\nclass CreatecloneTest(tf.test.TestCase):\n\n  def setUp(self):\n    # Create an easy training set:\n    np.random.seed(0)\n\n    self._inputs = np.zeros((16, 4))\n    self._labels = np.random.randint(0, 2, size=(16, 1)).astype(np.float32)\n    self._logdir = self.get_temp_dir()\n\n    for i in range(16):\n      j = int(2 * self._labels[i] + np.random.randint(0, 2))\n      self._inputs[i, j] = 1\n\n  def testCreateLogisticClassifier(self):\n    g = tf.Graph()\n    with g.as_default():\n      tf.set_random_seed(0)\n      tf_inputs = tf.constant(self._inputs, dtype=tf.float32)\n      tf_labels = tf.constant(self._labels, dtype=tf.float32)\n\n      model_fn = LogisticClassifier\n      clone_args = (tf_inputs, tf_labels)\n      deploy_config = model_deploy.DeploymentConfig(num_clones=1)\n\n      self.assertEqual(slim.get_variables(), [])\n      clones = model_deploy.create_clones(deploy_config, model_fn, clone_args)\n      clone = clones[0]\n      self.assertEqual(len(slim.get_variables()), 2)\n      for v in slim.get_variables():\n        self.assertDeviceEqual(v.device, \'CPU:0\')\n        self.assertDeviceEqual(v.value().device, \'CPU:0\')\n      self.assertEqual(clone.outputs.op.name,\n                       \'LogisticClassifier/fully_connected/Sigmoid\')\n      self.assertEqual(clone.scope, \'\')\n      self.assertDeviceEqual(clone.device, \'GPU:0\')\n      self.assertEqual(len(slim.losses.get_losses()), 1)\n      update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n      self.assertEqual(update_ops, [])\n\n  def testCreateSingleclone(self):\n    g = tf.Graph()\n    with g.as_default():\n      tf.set_random_seed(0)\n      tf_inputs = tf.constant(self._inputs, dtype=tf.float32)\n      tf_labels = tf.constant(self._labels, dtype=tf.float32)\n\n      model_fn = BatchNormClassifier\n      clone_args = (tf_inputs, tf_labels)\n      deploy_config = model_deploy.DeploymentConfig(num_clones=1)\n\n      self.assertEqual(slim.get_variables(), [])\n      clones = model_deploy.create_clones(deploy_config, model_fn, clone_args)\n      clone = clones[0]\n      self.assertEqual(len(slim.get_variables()), 5)\n      for v in slim.get_variables():\n        self.assertDeviceEqual(v.device, \'CPU:0\')\n        self.assertDeviceEqual(v.value().device, \'CPU:0\')\n      self.assertEqual(clone.outputs.op.name,\n                       \'BatchNormClassifier/fully_connected/Sigmoid\')\n      self.assertEqual(clone.scope, \'\')\n      self.assertDeviceEqual(clone.device, \'GPU:0\')\n      self.assertEqual(len(slim.losses.get_losses()), 1)\n      update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n      self.assertEqual(len(update_ops), 2)\n\n  def testCreateMulticlone(self):\n    g = tf.Graph()\n    with g.as_default():\n      tf.set_random_seed(0)\n      tf_inputs = tf.constant(self._inputs, dtype=tf.float32)\n      tf_labels = tf.constant(self._labels, dtype=tf.float32)\n\n      model_fn = BatchNormClassifier\n      clone_args = (tf_inputs, tf_labels)\n      num_clones = 4\n      deploy_config = model_deploy.DeploymentConfig(num_clones=num_clones)\n\n      self.assertEqual(slim.get_variables(), [])\n      clones = model_deploy.create_clones(deploy_config, model_fn, clone_args)\n      self.assertEqual(len(slim.get_variables()), 5)\n      for v in slim.get_variables():\n        self.assertDeviceEqual(v.device, \'CPU:0\')\n        self.assertDeviceEqual(v.value().device, \'CPU:0\')\n      self.assertEqual(len(clones), num_clones)\n      for i, clone in enumerate(clones):\n        self.assertEqual(\n            clone.outputs.op.name,\n            \'clone_%d/BatchNormClassifier/fully_connected/Sigmoid\' % i)\n        update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS, clone.scope)\n        self.assertEqual(len(update_ops), 2)\n        self.assertEqual(clone.scope, \'clone_%d/\' % i)\n        self.assertDeviceEqual(clone.device, \'GPU:%d\' % i)\n\n  def testCreateOnecloneWithPS(self):\n    g = tf.Graph()\n    with g.as_default():\n      tf.set_random_seed(0)\n      tf_inputs = tf.constant(self._inputs, dtype=tf.float32)\n      tf_labels = tf.constant(self._labels, dtype=tf.float32)\n\n      model_fn = BatchNormClassifier\n      clone_args = (tf_inputs, tf_labels)\n      deploy_config = model_deploy.DeploymentConfig(num_clones=1,\n                                                    num_ps_tasks=1)\n\n      self.assertEqual(slim.get_variables(), [])\n      clones = model_deploy.create_clones(deploy_config, model_fn, clone_args)\n      self.assertEqual(len(clones), 1)\n      clone = clones[0]\n      self.assertEqual(clone.outputs.op.name,\n                       \'BatchNormClassifier/fully_connected/Sigmoid\')\n      self.assertDeviceEqual(clone.device, \'/job:worker/device:GPU:0\')\n      self.assertEqual(clone.scope, \'\')\n      self.assertEqual(len(slim.get_variables()), 5)\n      for v in slim.get_variables():\n        self.assertDeviceEqual(v.device, \'/job:ps/task:0/CPU:0\')\n        self.assertDeviceEqual(v.device, v.value().device)\n\n  def testCreateMulticloneWithPS(self):\n    g = tf.Graph()\n    with g.as_default():\n      tf.set_random_seed(0)\n      tf_inputs = tf.constant(self._inputs, dtype=tf.float32)\n      tf_labels = tf.constant(self._labels, dtype=tf.float32)\n\n      model_fn = BatchNormClassifier\n      clone_args = (tf_inputs, tf_labels)\n      deploy_config = model_deploy.DeploymentConfig(num_clones=2,\n                                                    num_ps_tasks=2)\n\n      self.assertEqual(slim.get_variables(), [])\n      clones = model_deploy.create_clones(deploy_config, model_fn, clone_args)\n      self.assertEqual(len(slim.get_variables()), 5)\n      for i, v in enumerate(slim.get_variables()):\n        t = i % 2\n        self.assertDeviceEqual(v.device, \'/job:ps/task:%d/device:CPU:0\' % t)\n        self.assertDeviceEqual(v.device, v.value().device)\n      self.assertEqual(len(clones), 2)\n      for i, clone in enumerate(clones):\n        self.assertEqual(\n            clone.outputs.op.name,\n            \'clone_%d/BatchNormClassifier/fully_connected/Sigmoid\' % i)\n        self.assertEqual(clone.scope, \'clone_%d/\' % i)\n        self.assertDeviceEqual(clone.device, \'/job:worker/device:GPU:%d\' % i)\n\n\nclass OptimizeclonesTest(tf.test.TestCase):\n\n  def setUp(self):\n    # Create an easy training set:\n    np.random.seed(0)\n\n    self._inputs = np.zeros((16, 4))\n    self._labels = np.random.randint(0, 2, size=(16, 1)).astype(np.float32)\n    self._logdir = self.get_temp_dir()\n\n    for i in range(16):\n      j = int(2 * self._labels[i] + np.random.randint(0, 2))\n      self._inputs[i, j] = 1\n\n  def testCreateLogisticClassifier(self):\n    g = tf.Graph()\n    with g.as_default():\n      tf.set_random_seed(0)\n      tf_inputs = tf.constant(self._inputs, dtype=tf.float32)\n      tf_labels = tf.constant(self._labels, dtype=tf.float32)\n\n      model_fn = LogisticClassifier\n      clone_args = (tf_inputs, tf_labels)\n      deploy_config = model_deploy.DeploymentConfig(num_clones=1)\n\n      self.assertEqual(slim.get_variables(), [])\n      clones = model_deploy.create_clones(deploy_config, model_fn, clone_args)\n      self.assertEqual(len(slim.get_variables()), 2)\n      update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n      self.assertEqual(update_ops, [])\n\n      optimizer = tf.train.GradientDescentOptimizer(learning_rate=1.0)\n      total_loss, grads_and_vars = model_deploy.optimize_clones(clones,\n                                                                optimizer)\n      self.assertEqual(len(grads_and_vars), len(tf.trainable_variables()))\n      self.assertEqual(total_loss.op.name, \'total_loss\')\n      for g, v in grads_and_vars:\n        self.assertDeviceEqual(g.device, \'GPU:0\')\n        self.assertDeviceEqual(v.device, \'CPU:0\')\n\n  def testCreateSingleclone(self):\n    g = tf.Graph()\n    with g.as_default():\n      tf.set_random_seed(0)\n      tf_inputs = tf.constant(self._inputs, dtype=tf.float32)\n      tf_labels = tf.constant(self._labels, dtype=tf.float32)\n\n      model_fn = BatchNormClassifier\n      clone_args = (tf_inputs, tf_labels)\n      deploy_config = model_deploy.DeploymentConfig(num_clones=1)\n\n      self.assertEqual(slim.get_variables(), [])\n      clones = model_deploy.create_clones(deploy_config, model_fn, clone_args)\n      self.assertEqual(len(slim.get_variables()), 5)\n      update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n      self.assertEqual(len(update_ops), 2)\n\n      optimizer = tf.train.GradientDescentOptimizer(learning_rate=1.0)\n      total_loss, grads_and_vars = model_deploy.optimize_clones(clones,\n                                                                optimizer)\n      self.assertEqual(len(grads_and_vars), len(tf.trainable_variables()))\n      self.assertEqual(total_loss.op.name, \'total_loss\')\n      for g, v in grads_and_vars:\n        self.assertDeviceEqual(g.device, \'GPU:0\')\n        self.assertDeviceEqual(v.device, \'CPU:0\')\n\n  def testCreateMulticlone(self):\n    g = tf.Graph()\n    with g.as_default():\n      tf.set_random_seed(0)\n      tf_inputs = tf.constant(self._inputs, dtype=tf.float32)\n      tf_labels = tf.constant(self._labels, dtype=tf.float32)\n\n      model_fn = BatchNormClassifier\n      clone_args = (tf_inputs, tf_labels)\n      num_clones = 4\n      deploy_config = model_deploy.DeploymentConfig(num_clones=num_clones)\n\n      self.assertEqual(slim.get_variables(), [])\n      clones = model_deploy.create_clones(deploy_config, model_fn, clone_args)\n      self.assertEqual(len(slim.get_variables()), 5)\n      update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n      self.assertEqual(len(update_ops), num_clones * 2)\n\n      optimizer = tf.train.GradientDescentOptimizer(learning_rate=1.0)\n      total_loss, grads_and_vars = model_deploy.optimize_clones(clones,\n                                                                optimizer)\n      self.assertEqual(len(grads_and_vars), len(tf.trainable_variables()))\n      self.assertEqual(total_loss.op.name, \'total_loss\')\n      for g, v in grads_and_vars:\n        self.assertDeviceEqual(g.device, \'\')\n        self.assertDeviceEqual(v.device, \'CPU:0\')\n\n  def testCreateMulticloneCPU(self):\n    g = tf.Graph()\n    with g.as_default():\n      tf.set_random_seed(0)\n      tf_inputs = tf.constant(self._inputs, dtype=tf.float32)\n      tf_labels = tf.constant(self._labels, dtype=tf.float32)\n\n      model_fn = BatchNormClassifier\n      model_args = (tf_inputs, tf_labels)\n      num_clones = 4\n      deploy_config = model_deploy.DeploymentConfig(num_clones=num_clones,\n                                                    clone_on_cpu=True)\n\n      self.assertEqual(slim.get_variables(), [])\n      clones = model_deploy.create_clones(deploy_config, model_fn, model_args)\n      self.assertEqual(len(slim.get_variables()), 5)\n      update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n      self.assertEqual(len(update_ops), num_clones * 2)\n\n      optimizer = tf.train.GradientDescentOptimizer(learning_rate=1.0)\n      total_loss, grads_and_vars = model_deploy.optimize_clones(clones,\n                                                                optimizer)\n      self.assertEqual(len(grads_and_vars), len(tf.trainable_variables()))\n      self.assertEqual(total_loss.op.name, \'total_loss\')\n      for g, v in grads_and_vars:\n        self.assertDeviceEqual(g.device, \'\')\n        self.assertDeviceEqual(v.device, \'CPU:0\')\n\n  def testCreateOnecloneWithPS(self):\n    g = tf.Graph()\n    with g.as_default():\n      tf.set_random_seed(0)\n      tf_inputs = tf.constant(self._inputs, dtype=tf.float32)\n      tf_labels = tf.constant(self._labels, dtype=tf.float32)\n\n      model_fn = BatchNormClassifier\n      model_args = (tf_inputs, tf_labels)\n      deploy_config = model_deploy.DeploymentConfig(num_clones=1,\n                                                    num_ps_tasks=1)\n\n      self.assertEqual(slim.get_variables(), [])\n      clones = model_deploy.create_clones(deploy_config, model_fn, model_args)\n      self.assertEqual(len(slim.get_variables()), 5)\n      update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n      self.assertEqual(len(update_ops), 2)\n\n      optimizer = tf.train.GradientDescentOptimizer(learning_rate=1.0)\n      total_loss, grads_and_vars = model_deploy.optimize_clones(clones,\n                                                                optimizer)\n      self.assertEqual(len(grads_and_vars), len(tf.trainable_variables()))\n      self.assertEqual(total_loss.op.name, \'total_loss\')\n      for g, v in grads_and_vars:\n        self.assertDeviceEqual(g.device, \'/job:worker/device:GPU:0\')\n        self.assertDeviceEqual(v.device, \'/job:ps/task:0/CPU:0\')\n\n\nclass DeployTest(tf.test.TestCase):\n\n  def setUp(self):\n    # Create an easy training set:\n    np.random.seed(0)\n\n    self._inputs = np.zeros((16, 4))\n    self._labels = np.random.randint(0, 2, size=(16, 1)).astype(np.float32)\n    self._logdir = self.get_temp_dir()\n\n    for i in range(16):\n      j = int(2 * self._labels[i] + np.random.randint(0, 2))\n      self._inputs[i, j] = 1\n\n  def _addBesselsCorrection(self, sample_size, expected_var):\n    correction_factor = sample_size / (sample_size - 1)\n    expected_var *= correction_factor\n    return expected_var\n\n  def testLocalTrainOp(self):\n    g = tf.Graph()\n    with g.as_default():\n      tf.set_random_seed(0)\n      tf_inputs = tf.constant(self._inputs, dtype=tf.float32)\n      tf_labels = tf.constant(self._labels, dtype=tf.float32)\n\n      model_fn = BatchNormClassifier\n      model_args = (tf_inputs, tf_labels)\n      deploy_config = model_deploy.DeploymentConfig(num_clones=2,\n                                                    clone_on_cpu=True)\n\n      optimizer = tf.train.GradientDescentOptimizer(learning_rate=1.0)\n\n      self.assertEqual(slim.get_variables(), [])\n      model = model_deploy.deploy(deploy_config, model_fn, model_args,\n                                  optimizer=optimizer)\n\n      update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n      self.assertEqual(len(update_ops), 4)\n      self.assertEqual(len(model.clones), 2)\n      self.assertEqual(model.total_loss.op.name, \'total_loss\')\n      self.assertEqual(model.summary_op.op.name, \'summary_op/summary_op\')\n      self.assertEqual(model.train_op.op.name, \'train_op\')\n\n      with tf.Session() as sess:\n        sess.run(tf.global_variables_initializer())\n        moving_mean = tf.contrib.framework.get_variables_by_name(\n            \'moving_mean\')[0]\n        moving_variance = tf.contrib.framework.get_variables_by_name(\n            \'moving_variance\')[0]\n        initial_loss = sess.run(model.total_loss)\n        initial_mean, initial_variance = sess.run([moving_mean,\n                                                   moving_variance])\n        self.assertAllClose(initial_mean, [0.0, 0.0, 0.0, 0.0])\n        self.assertAllClose(initial_variance, [1.0, 1.0, 1.0, 1.0])\n        for _ in range(10):\n          sess.run(model.train_op)\n        final_loss = sess.run(model.total_loss)\n        self.assertLess(final_loss, initial_loss / 5.0)\n\n        final_mean, final_variance = sess.run([moving_mean,\n                                               moving_variance])\n        expected_mean = np.array([0.125, 0.25, 0.375, 0.25])\n        expected_var = np.array([0.109375, 0.1875, 0.234375, 0.1875])\n        expected_var = self._addBesselsCorrection(16, expected_var)\n        self.assertAllClose(final_mean, expected_mean)\n        self.assertAllClose(final_variance, expected_var)\n\n  def testNoSummariesOnGPU(self):\n    with tf.Graph().as_default():\n      deploy_config = model_deploy.DeploymentConfig(num_clones=2)\n\n      # clone function creates a fully_connected layer with a regularizer loss.\n      def ModelFn():\n        inputs = tf.constant(1.0, shape=(10, 20), dtype=tf.float32)\n        reg = tf.contrib.layers.l2_regularizer(0.001)\n        tf.contrib.layers.fully_connected(inputs, 30, weights_regularizer=reg)\n\n      model = model_deploy.deploy(\n          deploy_config, ModelFn,\n          optimizer=tf.train.GradientDescentOptimizer(1.0))\n      # The model summary op should have a few summary inputs and all of them\n      # should be on the CPU.\n      self.assertTrue(model.summary_op.op.inputs)\n      for inp in  model.summary_op.op.inputs:\n        self.assertEqual(\'/device:CPU:0\', inp.device)\n\n  def testNoSummariesOnGPUForEvals(self):\n    with tf.Graph().as_default():\n      deploy_config = model_deploy.DeploymentConfig(num_clones=2)\n\n      # clone function creates a fully_connected layer with a regularizer loss.\n      def ModelFn():\n        inputs = tf.constant(1.0, shape=(10, 20), dtype=tf.float32)\n        reg = tf.contrib.layers.l2_regularizer(0.001)\n        tf.contrib.layers.fully_connected(inputs, 30, weights_regularizer=reg)\n\n      # No optimizer here, it\'s an eval.\n      model = model_deploy.deploy(deploy_config, ModelFn)\n      # The model summary op should have a few summary inputs and all of them\n      # should be on the CPU.\n      self.assertTrue(model.summary_op.op.inputs)\n      for inp in  model.summary_op.op.inputs:\n        self.assertEqual(\'/device:CPU:0\', inp.device)\n\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
vendor/tensorflow/models/research/slim/nets/__init__.py,0,b'\n'
vendor/tensorflow/models/research/slim/nets/nets_factory.py,1,"b'# Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Contains a factory for building various models.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nimport functools\n\nimport tensorflow as tf\n\n# from nets import alexnet\n# from nets import cifarnet\n# from nets import inception\n# from nets import lenet\n# from nets import mobilenet_v1\n# from nets import overfeat\n# from nets import resnet_v1\n# from nets import resnet_v2\n# from nets import vgg\n# from nets.mobilenet import mobilenet_v2\n\n# from nets.nasnet import nasnet\n# from nets.nasnet import pnasnet\nfrom vendor.tensorflow.models.research.slim.nets.mobilenet import mobilenet_v2\n\nslim = tf.contrib.slim\n\nnetworks_map = {\n    # \'alexnet_v2\': alexnet.alexnet_v2,\n    # \'cifarnet\': cifarnet.cifarnet,\n    # \'overfeat\': overfeat.overfeat,\n    # \'vgg_a\': vgg.vgg_a,\n    # \'vgg_16\': vgg.vgg_16,\n    # \'vgg_19\': vgg.vgg_19,\n    # \'inception_v1\': inception.inception_v1,\n    # \'inception_v2\': inception.inception_v2,\n    # \'inception_v3\': inception.inception_v3,\n    # \'inception_v4\': inception.inception_v4,\n    # \'inception_resnet_v2\': inception.inception_resnet_v2,\n    # \'lenet\': lenet.lenet,\n    # \'resnet_v1_50\': resnet_v1.resnet_v1_50,\n    # \'resnet_v1_101\': resnet_v1.resnet_v1_101,\n    # \'resnet_v1_152\': resnet_v1.resnet_v1_152,\n    # \'resnet_v1_200\': resnet_v1.resnet_v1_200,\n    # \'resnet_v2_50\': resnet_v2.resnet_v2_50,\n    # \'resnet_v2_101\': resnet_v2.resnet_v2_101,\n    # \'resnet_v2_152\': resnet_v2.resnet_v2_152,\n    # \'resnet_v2_200\': resnet_v2.resnet_v2_200,\n    # \'mobilenet_v1\': mobilenet_v1.mobilenet_v1,\n    # \'mobilenet_v1_075\': mobilenet_v1.mobilenet_v1_075,\n    # \'mobilenet_v1_050\': mobilenet_v1.mobilenet_v1_050,\n    # \'mobilenet_v1_025\': mobilenet_v1.mobilenet_v1_025,\n    # \'mobilenet_v2\': mobilenet_v2.mobilenet,\n    \'mobilenet_v2_deepdrive\': mobilenet_v2.mobilenet_deepdrive,\n    # \'nasnet_cifar\': nasnet.build_nasnet_cifar,\n    # \'nasnet_mobile\': nasnet.build_nasnet_mobile,\n    # \'nasnet_large\': nasnet.build_nasnet_large,\n    # \'pnasnet_large\': pnasnet.build_pnasnet_large,\n}\n\narg_scopes_map = {\n    # \'alexnet_v2\': alexnet.alexnet_v2_arg_scope,\n    # \'cifarnet\': cifarnet.cifarnet_arg_scope,\n    # \'overfeat\': overfeat.overfeat_arg_scope,\n    # \'vgg_a\': vgg.vgg_arg_scope,\n    # \'vgg_16\': vgg.vgg_arg_scope,\n    # \'vgg_19\': vgg.vgg_arg_scope,\n    # \'inception_v1\': inception.inception_v3_arg_scope,\n    # \'inception_v2\': inception.inception_v3_arg_scope,\n    # \'inception_v3\': inception.inception_v3_arg_scope,\n    # \'inception_v4\': inception.inception_v4_arg_scope,\n    # \'inception_resnet_v2\':\n    # inception.inception_resnet_v2_arg_scope,\n    # \'lenet\': lenet.lenet_arg_scope,\n    # \'resnet_v1_50\': resnet_v1.resnet_arg_scope,\n    # \'resnet_v1_101\': resnet_v1.resnet_arg_scope,\n    # \'resnet_v1_152\': resnet_v1.resnet_arg_scope,\n    # \'resnet_v1_200\': resnet_v1.resnet_arg_scope,\n    # \'resnet_v2_50\': resnet_v2.resnet_arg_scope,\n    # \'resnet_v2_101\': resnet_v2.resnet_arg_scope,\n    # \'resnet_v2_152\': resnet_v2.resnet_arg_scope,\n    # \'resnet_v2_200\': resnet_v2.resnet_arg_scope,\n    # \'mobilenet_v1\': mobilenet_v1.mobilenet_v1_arg_scope,\n    # \'mobilenet_v1_075\': mobilenet_v1.mobilenet_v1_arg_scope,\n    # \'mobilenet_v1_050\': mobilenet_v1.mobilenet_v1_arg_scope,\n    # \'mobilenet_v1_025\': mobilenet_v1.mobilenet_v1_arg_scope,\n    # \'mobilenet_v2\': mobilenet_v2.training_scope,\n    \'mobilenet_v2_deepdrive\': mobilenet_v2.training_scope,\n    # \'nasnet_cifar\': nasnet.nasnet_cifar_arg_scope,\n    # \'nasnet_mobile\': nasnet.nasnet_mobile_arg_scope,\n    # \'nasnet_large\': nasnet.nasnet_large_arg_scope,\n    # \'pnasnet_large\': pnasnet.pnasnet_large_arg_scope,\n}\n\n\ndef get_network_fn(name, num_classes, weight_decay=0.0, is_training=False, num_targets=None, preprocess=None):\n    """"""Returns a network_fn such as `logits, end_points = network_fn(images)`.\n\n    Args:\n      name: The name of the network.\n      num_classes: The number of classes to use for classification. If 0 or None,\n        the logits layer is omitted and its input features are returned instead.\n      weight_decay: The l2 coefficient for the model weights.\n      is_training: `True` if the model is being used for training and `False`\n        otherwise.\n\n    Returns:\n      network_fn: A function that applies the model to a batch of images. It has\n        the following signature:\n            net, end_points = network_fn(images)\n        The `images` input is a tensor of shape [batch_size, height, width, 3]\n        with height = width = network_fn.default_image_size. (The permissibility\n        and treatment of other sizes depends on the network_fn.)\n        The returned `end_points` are a dictionary of intermediate activations.\n        The returned `net` is the topmost layer, depending on `num_classes`:\n        If `num_classes` was a non-zero integer, `net` is a logits tensor\n        of shape [batch_size, num_classes].\n        If `num_classes` was 0 or `None`, `net` is a tensor with the input\n        to the logits layer of shape [batch_size, 1, 1, num_features] or\n        [batch_size, num_features]. Dropout has not been applied to this\n        (even if the network\'s original classification does); it remains for\n        the caller to do this or not.\n\n    Raises:\n      ValueError: If network `name` is not recognized.\n    """"""\n    if name not in networks_map:\n        raise ValueError(\'Name of network unknown %s\' % name)\n    func = networks_map[name]\n\n    @functools.wraps(func)\n    def network_fn(images, **kwargs):\n        arg_scope = arg_scopes_map[name](weight_decay=weight_decay)\n        with slim.arg_scope(arg_scope):\n            out_dim = num_targets if num_targets is not None else num_classes\n            if preprocess is not None:\n                images = preprocess(images)\n            return func(images, out_dim, is_training=is_training, **kwargs)\n\n    if hasattr(func, \'default_image_size\'):\n        network_fn.default_image_size = func.default_image_size\n\n    return network_fn\n'"
vendor/tensorflow/models/research/slim/nets/nets_factory_test.py,8,"b'# Copyright 2016 Google Inc. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n""""""Tests for slim.inception.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\n\nimport tensorflow as tf\n\nfrom nets import nets_factory\n\n\nclass NetworksTest(tf.test.TestCase):\n\n  def testGetNetworkFnFirstHalf(self):\n    batch_size = 5\n    num_classes = 1000\n    for net in list(nets_factory.networks_map.keys())[:10]:\n      with tf.Graph().as_default() as g, self.test_session(g):\n        net_fn = nets_factory.get_network_fn(net, num_classes)\n        # Most networks use 224 as their default_image_size\n        image_size = getattr(net_fn, \'default_image_size\', 224)\n        inputs = tf.random_uniform((batch_size, image_size, image_size, 3))\n        logits, end_points = net_fn(inputs)\n        self.assertTrue(isinstance(logits, tf.Tensor))\n        self.assertTrue(isinstance(end_points, dict))\n        self.assertEqual(logits.get_shape().as_list()[0], batch_size)\n        self.assertEqual(logits.get_shape().as_list()[-1], num_classes)\n\n  def testGetNetworkFnSecondHalf(self):\n    batch_size = 5\n    num_classes = 1000\n    for net in list(nets_factory.networks_map.keys())[10:]:\n      with tf.Graph().as_default() as g, self.test_session(g):\n        net_fn = nets_factory.get_network_fn(net, num_classes)\n        # Most networks use 224 as their default_image_size\n        image_size = getattr(net_fn, \'default_image_size\', 224)\n        inputs = tf.random_uniform((batch_size, image_size, image_size, 3))\n        logits, end_points = net_fn(inputs)\n        self.assertTrue(isinstance(logits, tf.Tensor))\n        self.assertTrue(isinstance(end_points, dict))\n        self.assertEqual(logits.get_shape().as_list()[0], batch_size)\n        self.assertEqual(logits.get_shape().as_list()[-1], num_classes)\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
vendor/tensorflow/models/research/slim/preprocessing/__init__.py,0,b'\n'
vendor/tensorflow/models/research/slim/preprocessing/deepdrive_preprocessing.py,93,"b'# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Provides utilities to preprocess images for the Inception networks.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\n\nfrom tensorflow.python.ops import control_flow_ops\n\n\ndef apply_with_random_selector(x, func, num_cases):\n    """"""Computes func(x, sel), with sel sampled from [0...num_cases-1].\n\n    Args:\n      x: input Tensor.\n      func: Python function to apply.\n      num_cases: Python int32, number of cases to sample sel from.\n\n    Returns:\n      The result of func(x, sel), where func receives the value of the\n      selector as a python integer, but sel is sampled dynamically.\n    """"""\n    sel = tf.random_uniform([], maxval=num_cases, dtype=tf.int32)\n    # Pass the real x only to one of the func calls.\n    return control_flow_ops.merge([\n        func(control_flow_ops.switch(x, tf.equal(sel, case))[1], case)\n        for case in range(num_cases)])[0]\n\n\ndef distort_color(image, color_ordering=0, fast_mode=True, scope=None):\n    """"""Distort the color of a Tensor image.\n\n    Each color distortion is non-commutative and thus ordering of the color ops\n    matters. Ideally we would randomly permute the ordering of the color ops.\n    Rather then adding that level of complication, we select a distinct ordering\n    of color ops for each preprocessing thread.\n\n    Args:\n      image: 3-D Tensor containing single image in [0, 1].\n      color_ordering: Python int, a type of distortion (valid values: 0-3).\n      fast_mode: Avoids slower ops (random_hue and random_contrast)\n      scope: Optional scope for name_scope.\n    Returns:\n      3-D Tensor color-distorted image on range [0, 1]\n    Raises:\n      ValueError: if color_ordering not in [0, 3]\n    """"""\n    with tf.name_scope(scope, \'distort_color\', [image]):\n        if fast_mode:\n            if color_ordering == 0:\n                image = tf.image.random_brightness(image, max_delta=32. / 255.)\n                image = tf.image.random_saturation(image, lower=0.5, upper=1.5)\n            else:\n                image = tf.image.random_saturation(image, lower=0.5, upper=1.5)\n                image = tf.image.random_brightness(image, max_delta=32. / 255.)\n        else:\n            if color_ordering == 0:\n                image = tf.image.random_brightness(image, max_delta=32. / 255.)\n                image = tf.image.random_saturation(image, lower=0.5, upper=1.5)\n                image = tf.image.random_hue(image, max_delta=0.2)\n                image = tf.image.random_contrast(image, lower=0.5, upper=1.5)\n            elif color_ordering == 1:\n                image = tf.image.random_saturation(image, lower=0.5, upper=1.5)\n                image = tf.image.random_brightness(image, max_delta=32. / 255.)\n                image = tf.image.random_contrast(image, lower=0.5, upper=1.5)\n                image = tf.image.random_hue(image, max_delta=0.2)\n            elif color_ordering == 2:\n                image = tf.image.random_contrast(image, lower=0.5, upper=1.5)\n                image = tf.image.random_hue(image, max_delta=0.2)\n                image = tf.image.random_brightness(image, max_delta=32. / 255.)\n                image = tf.image.random_saturation(image, lower=0.5, upper=1.5)\n            elif color_ordering == 3:\n                image = tf.image.random_hue(image, max_delta=0.2)\n                image = tf.image.random_saturation(image, lower=0.5, upper=1.5)\n                image = tf.image.random_contrast(image, lower=0.5, upper=1.5)\n                image = tf.image.random_brightness(image, max_delta=32. / 255.)\n            else:\n                raise ValueError(\'color_ordering must be in [0, 3]\')\n\n        # The random_* ops do not necessarily clamp.\n        return tf.clip_by_value(image, 0.0, 1.0)\n\n\ndef distorted_bounding_box_crop(image,\n                                bbox,\n                                min_object_covered=0.1,\n                                aspect_ratio_range=(0.75, 1.33),\n                                area_range=(0.05, 1.0),\n                                max_attempts=100,\n                                scope=None):\n    """"""Generates cropped_image using a one of the bboxes randomly distorted.\n\n    See `tf.image.sample_distorted_bounding_box` for more documentation.\n\n    Args:\n      image: 3-D Tensor of image (it will be converted to floats in [0, 1]).\n      bbox: 3-D float Tensor of bounding boxes arranged [1, num_boxes, coords]\n        where each coordinate is [0, 1) and the coordinates are arranged\n        as [ymin, xmin, ymax, xmax]. If num_boxes is 0 then it would use the whole\n        image.\n      min_object_covered: An optional `float`. Defaults to `0.1`. The cropped\n        area of the image must contain at least this fraction of any bounding box\n        supplied.\n      aspect_ratio_range: An optional list of `floats`. The cropped area of the\n        image must have an aspect ratio = width / height within this range.\n      area_range: An optional list of `floats`. The cropped area of the image\n        must contain a fraction of the supplied image within in this range.\n      max_attempts: An optional `int`. Number of attempts at generating a cropped\n        region of the image of the specified constraints. After `max_attempts`\n        failures, return the entire image.\n      scope: Optional scope for name_scope.\n    Returns:\n      A tuple, a 3-D Tensor cropped_image and the distorted bbox\n    """"""\n    with tf.name_scope(scope, \'distorted_bounding_box_crop\', [image, bbox]):\n        # Each bounding box has shape [1, num_boxes, box coords] and\n        # the coordinates are ordered [ymin, xmin, ymax, xmax].\n\n        # A large fraction of image datasets contain a human-annotated bounding\n        # box delineating the region of the image containing the object of interest.\n        # We choose to create a new bounding box for the object which is a randomly\n        # distorted version of the human-annotated bounding box that obeys an\n        # allowed range of aspect ratios, sizes and overlap with the human-annotated\n        # bounding box. If no box is supplied, then we assume the bounding box is\n        # the entire image.\n        sample_distorted_bounding_box = tf.image.sample_distorted_bounding_box(\n            tf.shape(image),\n            bounding_boxes=bbox,\n            min_object_covered=min_object_covered,\n            aspect_ratio_range=aspect_ratio_range,\n            area_range=area_range,\n            max_attempts=max_attempts,\n            use_image_if_no_bounding_boxes=True)\n        bbox_begin, bbox_size, distort_bbox = sample_distorted_bounding_box\n\n        # Crop the image to the specified bounding box.\n        cropped_image = tf.slice(image, bbox_begin, bbox_size)\n        return cropped_image, distort_bbox\n\n\ndef preprocess_for_train(image, height, width, bbox,\n                         fast_mode=True,\n                         scope=None,\n                         add_image_summaries=True):\n    """"""Distort one image for training a network.\n\n    Distorting images provides a useful technique for augmenting the data\n    set during training in order to make the network invariant to aspects\n    of the image that do not effect the label.\n\n    Additionally it would create image_summaries to display the different\n    transformations applied to the image.\n\n    Args:\n      image: 3-D Tensor of image. If dtype is tf.float32 then the range should be\n        [0, 1], otherwise it would converted to tf.float32 assuming that the range\n        is [0, MAX], where MAX is largest positive representable number for\n        int(8/16/32) data type (see `tf.image.convert_image_dtype` for details).\n      height: integer\n      width: integer\n      bbox: 3-D float Tensor of bounding boxes arranged [1, num_boxes, coords]\n        where each coordinate is [0, 1) and the coordinates are arranged\n        as [ymin, xmin, ymax, xmax].\n      fast_mode: Optional boolean, if True avoids slower transformations (i.e.\n        bi-cubic resizing, random_hue or random_contrast).\n      scope: Optional scope for name_scope.\n      add_image_summaries: Enable image summaries.\n    Returns:\n      3-D float Tensor of distorted image used for training with range [-1, 1].\n    """"""\n    with tf.name_scope(scope, \'distort_image\', [image, height, width, bbox]):\n        if bbox is None:\n            bbox = tf.constant([0.0, 0.0, 1.0, 1.0],\n                               dtype=tf.float32,\n                               shape=[1, 1, 4])\n\n        # image = tf.Print(image, [\'min \', tf.reduce_min(tf.reshape(image, shape=[-1])), \'max \',\n        #                          tf.reduce_max(tf.reshape(image, shape=[-1])), image], \'image in train prepro \')\n\n        image = tf.reshape(image, [height, width, 3])\n\n        if image.dtype != tf.float32:\n            image = tf.image.convert_image_dtype(image, dtype=tf.float32)\n\n        # image = tf.Print(image,\n        #                  [\'min \', tf.reduce_min(tf.reshape(image, shape=[-1])),\n        #                   \'max \', tf.reduce_max(tf.reshape(image, shape=[-1])), image],\n        #                  \'image in train prepro after convert float\')\n\n        augment_image = True\n\n        if augment_image:\n            # Each bounding box has shape [1, num_boxes, box coords] and\n            # the coordinates are ordered [ymin, xmin, ymax, xmax].\n            image_with_box = tf.image.draw_bounding_boxes(tf.expand_dims(image, 0),\n                                                          bbox)\n\n            # image = tf.Print(image,\n            #                  [\'min \', tf.reduce_min(tf.reshape(image, shape=[-1])),\n            #                   \'max \', tf.reduce_max(tf.reshape(image, shape=[-1])), image],\n            #                  \'image in train prepro draw box\')\n\n            if add_image_summaries:\n                tf.summary.image(\'image_with_bounding_boxes\', image_with_box)\n\n            distorted_image, distorted_bbox = distorted_bounding_box_crop(image, bbox, area_range=(0.75, 1.0))\n\n            # distorted_image = tf.Print(distorted_image,\n            #                  [\'min \', tf.reduce_min(tf.reshape(distorted_image, shape=[-1])),\n            #                   \'max \', tf.reduce_max(tf.reshape(distorted_image, shape=[-1])), image_with_box],\n            #                  \'image in train prepro 3\')\n\n            # Restore the shape since the dynamic slice based upon the bbox_size loses\n            # the third dimension.\n            distorted_image.set_shape([None, None, 3])\n            image_with_distorted_box = tf.image.draw_bounding_boxes(\n                tf.expand_dims(image, 0), distorted_bbox)\n\n            # distorted_image = tf.Print(distorted_image,\n            #                  [\'min \', tf.reduce_min(tf.reshape(distorted_image, shape=[-1])),\n            #                   \'max \', tf.reduce_max(tf.reshape(distorted_image, shape=[-1])), image_with_box],\n            #                  \'image in train prepro 4\')\n\n            if add_image_summaries:\n                tf.summary.image(\'images_with_distorted_bounding_box\',\n                                 image_with_distorted_box)\n\n            # This resizing operation may distort the images because the aspect\n            # ratio is not respected. We select a resize method in a round robin\n            # fashion based on the thread number.\n            # Note that ResizeMethod contains 4 enumerated resizing methods.\n\n            # We select only 1 case for fast_mode bilinear.\n            num_resize_cases = 1 if fast_mode else 4\n            distorted_image = apply_with_random_selector(\n                distorted_image,\n                lambda x, method: tf.image.resize_images(x, [height, width], method),\n                num_cases=num_resize_cases)\n\n            # distorted_image = tf.Print(distorted_image,\n            #                  [\'min \', tf.reduce_min(tf.reshape(distorted_image, shape=[-1])),\n            #                   \'max \', tf.reduce_max(tf.reshape(distorted_image, shape=[-1])), image_with_box],\n            #                  \'image in train prepro 5\')\n\n            if add_image_summaries:\n                tf.summary.image(\'cropped_resized_image\',\n                                 tf.expand_dims(distorted_image, 0))\n\n            # If we flip image horizontally, we need to flip steering, direction, and spin as well\n            # Also - this puts us on the opposite side of the lane - which may be nice left-hand-traffic\n            # robustness, but this still seems sketchy on many levels. Signs and words will be flipped...\n            # There will be more left handed people. Navigating by the stars will be all messed up, IDK!\n\n            # Randomly flip the image horizontally.\n            # distorted_image = tf.image.random_flip_left_right(distorted_image)\n\n            # Randomly distort the colors. There are 1 or 4 ways to do it.\n            num_distort_cases = 1 if fast_mode else 4\n            distorted_image = apply_with_random_selector(\n                distorted_image,\n                lambda x, ordering: distort_color(x, ordering, fast_mode),\n                num_cases=num_distort_cases)\n\n            # distorted_image = tf.Print(distorted_image,\n            #                  [\'min \', tf.reduce_min(tf.reshape(distorted_image, shape=[-1])),\n            #                   \'max \', tf.reduce_max(tf.reshape(distorted_image, shape=[-1])), image_with_box],\n            #                  \'image in train prepro 6\')\n\n\n        else:\n            distorted_image = image\n\n        distorted_image = tf.subtract(distorted_image, 0.5)\n        distorted_image = tf.multiply(distorted_image, 2.0)\n\n        if add_image_summaries:\n            tf.summary.image(\'final_distorted_image\',\n                             tf.expand_dims(distorted_image, 0))\n\n        # distorted_image = tf.Print(distorted_image,\n        #                  [\'min \', tf.reduce_min(tf.reshape(distorted_image, shape=[-1])),\n        #                   \'max \', tf.reduce_max(tf.reshape(distorted_image, shape=[-1])), distorted_image],\n        #                  \'image in train prepro 7\')\n\n        return distorted_image\n\n\ndef preprocess_for_eval(image, height, width,\n                        central_fraction=0.875, scope=None):\n    """"""Prepare one image for evaluation.\n\n    If height and width are specified it would output an image with that size by\n    applying resize_bilinear.\n\n    If central_fraction is specified it would crop the central fraction of the\n    input image.\n\n    Args:\n      image: 3-D Tensor of image. If dtype is tf.float32 then the range should be\n        [0, 1], otherwise it would converted to tf.float32 assuming that the range\n        is [0, MAX], where MAX is largest positive representable number for\n        int(8/16/32) data type (see `tf.image.convert_image_dtype` for details).\n      height: integer\n      width: integer\n      central_fraction: Optional Float, fraction of the image to crop.\n      scope: Optional scope for name_scope.\n    Returns:\n      3-D float Tensor of prepared image.\n    """"""\n    with tf.name_scope(scope, \'eval_image\', [image, height, width]):\n        if image.dtype != tf.float32:\n            # Warning this convert normalizes as well!\n            image = tf.image.convert_image_dtype(image, dtype=tf.float32)\n        # Crop the central region of the image with an area containing 87.5% of\n        # the original image.\n        # if central_fraction:\n        #   image = tf.image.central_crop(image, central_fraction=central_fraction)\n\n        # image = tf.Print(image, [image], \'image in eval prepro is \', summarize=100)\n        image = tf.reshape(image, [height, width, 3])\n\n        # if height and width:\n        #   # Resize the image to the specified height and width.\n        #   image = tf.expand_dims(image, 0)\n        #   image = tf.image.resize_bilinear(image, [height, width],\n        #                                    align_corners=False)\n        #   image = tf.squeeze(image, [0])\n        image = tf.subtract(image, 0.5)\n        image = tf.multiply(image, 2.0)\n        return image\n\n\ndef preprocess_image(image, height, width,\n                     is_training=False,\n                     bbox=None,\n                     fast_mode=True,\n                     add_image_summaries=True):\n    """"""Pre-process one image for training or evaluation.\n\n    Args:\n      image: 3-D Tensor [height, width, channels] with the image. If dtype is\n        tf.float32 then the range should be [0, 1], otherwise it would converted\n        to tf.float32 assuming that the range is [0, MAX], where MAX is largest\n        positive representable number for int(8/16/32) data type (see\n        `tf.image.convert_image_dtype` for details).\n      height: integer, image expected height.\n      width: integer, image expected width.\n      is_training: Boolean. If true it would transform an image for train,\n        otherwise it would transform it for evaluation.\n      bbox: 3-D float Tensor of bounding boxes arranged [1, num_boxes, coords]\n        where each coordinate is [0, 1) and the coordinates are arranged as\n        [ymin, xmin, ymax, xmax].\n      fast_mode: Optional boolean, if True avoids slower transformations.\n      add_image_summaries: Enable image summaries.\n\n    Returns:\n      3-D float Tensor containing an appropriately scaled image\n\n    Raises:\n      ValueError: if user does not provide bounding box\n    """"""\n    if is_training:\n        return preprocess_for_train(image, height, width, bbox, fast_mode,\n                                    add_image_summaries=add_image_summaries)\n    else:\n        # image = tf.Print(image, [\'min \', tf.reduce_min(tf.reshape(image, shape=[-1])), \'max \', tf.reduce_max(tf.reshape(image, shape=[-1])), image], \'image befor prepro \')\n        image = preprocess_for_eval(image, height, width)\n        # image = tf.Print(image, [\'min \', tf.reduce_min(tf.reshape(image, shape=[-1])), \'max \', tf.reduce_max(tf.reshape(image, shape=[-1])), image], \'image after prepro \')\n        return image\n'"
vendor/tensorflow/models/research/slim/preprocessing/preprocessing_factory.py,1,"b'# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Contains a factory for building various models.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\n\n# from preprocessing import cifarnet_preprocessing\n# from preprocessing import inception_preprocessing\n# from preprocessing import lenet_preprocessing\n# from preprocessing import vgg_preprocessing\n# from preprocessing import deepdrive_preprocessing\nfrom vendor.tensorflow.models.research.slim.preprocessing import deepdrive_preprocessing\n\nslim = tf.contrib.slim\n\n\ndef get_preprocessing(name, is_training=False):\n    """"""Returns preprocessing_fn(image, height, width, **kwargs).\n\n    Args:\n      name: The name of the preprocessing function.\n      is_training: `True` if the model is being used for training and `False`\n        otherwise.\n\n    Returns:\n      preprocessing_fn: A function that preprocessing a single image (pre-batch).\n        It has the following signature:\n          image = preprocessing_fn(image, output_height, output_width, ...).\n\n    Raises:\n      ValueError: If Preprocessing `name` is not recognized.\n    """"""\n    preprocessing_fn_map = {\n        # \'cifarnet\': cifarnet_preprocessing,\n        # \'inception\': inception_preprocessing,\n        # \'inception_v1\': inception_preprocessing,\n        # \'inception_v2\': inception_preprocessing,\n        # \'inception_v3\': inception_preprocessing,\n        # \'inception_v4\': inception_preprocessing,\n        # \'inception_resnet_v2\': inception_preprocessing,\n        # \'lenet\': lenet_preprocessing,\n        # \'mobilenet_v1\': inception_preprocessing,\n        # \'mobilenet_v2\': inception_preprocessing,\n        \'mobilenet_v2_deepdrive\': deepdrive_preprocessing,\n        # \'nasnet_mobile\': inception_preprocessing,\n        # \'nasnet_large\': inception_preprocessing,\n        # \'pnasnet_large\': inception_preprocessing,\n        # \'resnet_v1_50\': vgg_preprocessing,\n        # \'resnet_v1_101\': vgg_preprocessing,\n        # \'resnet_v1_152\': vgg_preprocessing,\n        # \'resnet_v1_200\': vgg_preprocessing,\n        # \'resnet_v2_50\': vgg_preprocessing,\n        # \'resnet_v2_101\': vgg_preprocessing,\n        # \'resnet_v2_152\': vgg_preprocessing,\n        # \'resnet_v2_200\': vgg_preprocessing,\n        # \'vgg\': vgg_preprocessing,\n        # \'vgg_a\': vgg_preprocessing,\n        # \'vgg_16\': vgg_preprocessing,\n        # \'vgg_19\': vgg_preprocessing,\n    }\n\n    if name not in preprocessing_fn_map:\n        raise ValueError(\'Preprocessing name [%s] was not recognized\' % name)\n\n    def preprocessing_fn(image, output_height, output_width, **kwargs):\n        return preprocessing_fn_map[name].preprocess_image(\n            image, output_height, output_width, is_training=is_training, **kwargs)\n\n    return preprocessing_fn\n'"
vendor/tensorflow/models/research/slim/nets/mobilenet/__init__.py,0,b''
vendor/tensorflow/models/research/slim/nets/mobilenet/conv_blocks.py,14,"b'# Copyright 2018 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Convolution blocks for mobilenet.""""""\nimport contextlib\nimport functools\n\nimport tensorflow as tf\n\nslim = tf.contrib.slim\n\n\ndef _fixed_padding(inputs, kernel_size, rate=1):\n  """"""Pads the input along the spatial dimensions independently of input size.\n\n  Pads the input such that if it was used in a convolution with \'VALID\' padding,\n  the output would have the same dimensions as if the unpadded input was used\n  in a convolution with \'SAME\' padding.\n\n  Args:\n    inputs: A tensor of size [batch, height_in, width_in, channels].\n    kernel_size: The kernel to be used in the conv2d or max_pool2d operation.\n    rate: An integer, rate for atrous convolution.\n\n  Returns:\n    output: A tensor of size [batch, height_out, width_out, channels] with the\n      input, either intact (if kernel_size == 1) or padded (if kernel_size > 1).\n  """"""\n  kernel_size_effective = [kernel_size[0] + (kernel_size[0] - 1) * (rate - 1),\n                           kernel_size[0] + (kernel_size[0] - 1) * (rate - 1)]\n  pad_total = [kernel_size_effective[0] - 1, kernel_size_effective[1] - 1]\n  pad_beg = [pad_total[0] // 2, pad_total[1] // 2]\n  pad_end = [pad_total[0] - pad_beg[0], pad_total[1] - pad_beg[1]]\n  padded_inputs = tf.pad(inputs, [[0, 0], [pad_beg[0], pad_end[0]],\n                                  [pad_beg[1], pad_end[1]], [0, 0]])\n  return padded_inputs\n\n\ndef _make_divisible(v, divisor, min_value=None):\n  if min_value is None:\n    min_value = divisor\n  new_v = max(min_value, int(v + divisor / 2) // divisor * divisor)\n  # Make sure that round down does not go down by more than 10%.\n  if new_v < 0.9 * v:\n    new_v += divisor\n  return new_v\n\n\ndef _split_divisible(num, num_ways, divisible_by=8):\n  """"""Evenly splits num, num_ways so each piece is a multiple of divisible_by.""""""\n  assert num % divisible_by == 0\n  assert num / num_ways >= divisible_by\n  # Note: want to round down, we adjust each split to match the total.\n  base = num // num_ways // divisible_by * divisible_by\n  result = []\n  accumulated = 0\n  for i in range(num_ways):\n    r = base\n    while accumulated + r < num * (i + 1) / num_ways:\n      r += divisible_by\n    result.append(r)\n    accumulated += r\n  assert accumulated == num\n  return result\n\n\n@contextlib.contextmanager\ndef _v1_compatible_scope_naming(scope):\n  if scope is None:  # Create uniqified separable blocks.\n    with tf.variable_scope(None, default_name=\'separable\') as s, \\\n         tf.name_scope(s.original_name_scope):\n      yield \'\'\n  else:\n    # We use scope_depthwise, scope_pointwise for compatibility with V1 ckpts.\n    # which provide numbered scopes.\n    scope += \'_\'\n    yield scope\n\n\n@slim.add_arg_scope\ndef split_separable_conv2d(input_tensor,\n                           num_outputs,\n                           scope=None,\n                           normalizer_fn=None,\n                           stride=1,\n                           rate=1,\n                           endpoints=None,\n                           use_explicit_padding=False):\n  """"""Separable mobilenet V1 style convolution.\n\n  Depthwise convolution, with default non-linearity,\n  followed by 1x1 depthwise convolution.  This is similar to\n  slim.separable_conv2d, but differs in tha it applies batch\n  normalization and non-linearity to depthwise. This  matches\n  the basic building of Mobilenet Paper\n  (https://arxiv.org/abs/1704.04861)\n\n  Args:\n    input_tensor: input\n    num_outputs: number of outputs\n    scope: optional name of the scope. Note if provided it will use\n    scope_depthwise for deptwhise, and scope_pointwise for pointwise.\n    normalizer_fn: which normalizer function to use for depthwise/pointwise\n    stride: stride\n    rate: output rate (also known as dilation rate)\n    endpoints: optional, if provided, will export additional tensors to it.\n    use_explicit_padding: Use \'VALID\' padding for convolutions, but prepad\n      inputs so that the output dimensions are the same as if \'SAME\' padding\n      were used.\n\n  Returns:\n    output tesnor\n  """"""\n\n  with _v1_compatible_scope_naming(scope) as scope:\n    dw_scope = scope + \'depthwise\'\n    endpoints = endpoints if endpoints is not None else {}\n    kernel_size = [3, 3]\n    padding = \'SAME\'\n    if use_explicit_padding:\n      padding = \'VALID\'\n      input_tensor = _fixed_padding(input_tensor, kernel_size, rate)\n    net = slim.separable_conv2d(\n        input_tensor,\n        None,\n        kernel_size,\n        depth_multiplier=1,\n        stride=stride,\n        rate=rate,\n        normalizer_fn=normalizer_fn,\n        padding=padding,\n        scope=dw_scope)\n\n    endpoints[dw_scope] = net\n\n    pw_scope = scope + \'pointwise\'\n    net = slim.conv2d(\n        net,\n        num_outputs, [1, 1],\n        stride=1,\n        normalizer_fn=normalizer_fn,\n        scope=pw_scope)\n    endpoints[pw_scope] = net\n  return net\n\n\ndef expand_input_by_factor(n, divisible_by=8):\n  return lambda num_inputs, **_: _make_divisible(num_inputs * n, divisible_by)\n\n\n@slim.add_arg_scope\ndef expanded_conv(input_tensor,\n                  num_outputs,\n                  expansion_size=expand_input_by_factor(6),\n                  stride=1,\n                  rate=1,\n                  kernel_size=(3, 3),\n                  residual=True,\n                  normalizer_fn=None,\n                  split_projection=1,\n                  split_expansion=1,\n                  expansion_transform=None,\n                  depthwise_location=\'expansion\',\n                  depthwise_channel_multiplier=1,\n                  endpoints=None,\n                  use_explicit_padding=False,\n                  scope=None):\n  """"""Depthwise Convolution Block with expansion.\n\n  Builds a composite convolution that has the following structure\n  expansion (1x1) -> depthwise (kernel_size) -> projection (1x1)\n\n  Args:\n    input_tensor: input\n    num_outputs: number of outputs in the final layer.\n    expansion_size: the size of expansion, could be a constant or a callable.\n      If latter it will be provided \'num_inputs\' as an input. For forward\n      compatibility it should accept arbitrary keyword arguments.\n      Default will expand the input by factor of 6.\n    stride: depthwise stride\n    rate: depthwise rate\n    kernel_size: depthwise kernel\n    residual: whether to include residual connection between input\n      and output.\n    normalizer_fn: batchnorm or otherwise\n    split_projection: how many ways to split projection operator\n      (that is conv expansion->bottleneck)\n    split_expansion: how many ways to split expansion op\n      (that is conv bottleneck->expansion) ops will keep depth divisible\n      by this value.\n    expansion_transform: Optional function that takes expansion\n      as a single input and returns output.\n    depthwise_location: where to put depthwise covnvolutions supported\n      values None, \'input\', \'output\', \'expansion\'\n    depthwise_channel_multiplier: depthwise channel multiplier:\n    each input will replicated (with different filters)\n    that many times. So if input had c channels,\n    output will have c x depthwise_channel_multpilier.\n    endpoints: An optional dictionary into which intermediate endpoints are\n      placed. The keys ""expansion_output"", ""depthwise_output"",\n      ""projection_output"" and ""expansion_transform"" are always populated, even\n      if the corresponding functions are not invoked.\n    use_explicit_padding: Use \'VALID\' padding for convolutions, but prepad\n      inputs so that the output dimensions are the same as if \'SAME\' padding\n      were used.\n    scope: optional scope.\n\n  Returns:\n    Tensor of depth num_outputs\n\n  Raises:\n    TypeError: on inval\n  """"""\n  with tf.variable_scope(scope, default_name=\'expanded_conv\') as s, \\\n       tf.name_scope(s.original_name_scope):\n    prev_depth = input_tensor.get_shape().as_list()[3]\n    if  depthwise_location not in [None, \'input\', \'output\', \'expansion\']:\n      raise TypeError(\'%r is unknown value for depthwise_location\' %\n                      depthwise_location)\n    padding = \'SAME\'\n    if use_explicit_padding:\n      padding = \'VALID\'\n    depthwise_func = functools.partial(\n        slim.separable_conv2d,\n        num_outputs=None,\n        kernel_size=kernel_size,\n        depth_multiplier=depthwise_channel_multiplier,\n        stride=stride,\n        rate=rate,\n        normalizer_fn=normalizer_fn,\n        padding=padding,\n        scope=\'depthwise\')\n    # b1 -> b2 * r -> b2\n    #   i -> (o * r) (bottleneck) -> o\n    input_tensor = tf.identity(input_tensor, \'input\')\n    net = input_tensor\n\n    if depthwise_location == \'input\':\n      if use_explicit_padding:\n        net = _fixed_padding(net, kernel_size, rate)\n      net = depthwise_func(net, activation_fn=None)\n\n    if callable(expansion_size):\n      inner_size = expansion_size(num_inputs=prev_depth)\n    else:\n      inner_size = expansion_size\n\n    if inner_size > net.shape[3]:\n      net = split_conv(\n          net,\n          inner_size,\n          num_ways=split_expansion,\n          scope=\'expand\',\n          stride=1,\n          normalizer_fn=normalizer_fn)\n      net = tf.identity(net, \'expansion_output\')\n    if endpoints is not None:\n      endpoints[\'expansion_output\'] = net\n\n    if depthwise_location == \'expansion\':\n      if use_explicit_padding:\n        net = _fixed_padding(net, kernel_size, rate)\n      net = depthwise_func(net)\n\n    net = tf.identity(net, name=\'depthwise_output\')\n    if endpoints is not None:\n      endpoints[\'depthwise_output\'] = net\n    if expansion_transform:\n      net = expansion_transform(expansion_tensor=net, input_tensor=input_tensor)\n    # Note in contrast with expansion, we always have\n    # projection to produce the desired output size.\n    net = split_conv(\n        net,\n        num_outputs,\n        num_ways=split_projection,\n        stride=1,\n        scope=\'project\',\n        normalizer_fn=normalizer_fn,\n        activation_fn=tf.identity)\n    if endpoints is not None:\n      endpoints[\'projection_output\'] = net\n    if depthwise_location == \'output\':\n      if use_explicit_padding:\n        net = _fixed_padding(net, kernel_size, rate)\n      net = depthwise_func(net, activation_fn=None)\n\n    if callable(residual):  # custom residual\n      net = residual(input_tensor=input_tensor, output_tensor=net)\n    elif (residual and\n          # stride check enforces that we don\'t add residuals when spatial\n          # dimensions are None\n          stride == 1 and\n          # Depth matches\n          net.get_shape().as_list()[3] ==\n          input_tensor.get_shape().as_list()[3]):\n      net += input_tensor\n    return tf.identity(net, name=\'output\')\n\n\ndef split_conv(input_tensor,\n               num_outputs,\n               num_ways,\n               scope,\n               divisible_by=8,\n               **kwargs):\n  """"""Creates a split convolution.\n\n  Split convolution splits the input and output into\n  \'num_blocks\' blocks of approximately the same size each,\n  and only connects $i$-th input to $i$ output.\n\n  Args:\n    input_tensor: input tensor\n    num_outputs: number of output filters\n    num_ways: num blocks to split by.\n    scope: scope for all the operators.\n    divisible_by: make sure that every part is divisiable by this.\n    **kwargs: will be passed directly into conv2d operator\n  Returns:\n    tensor\n  """"""\n  b = input_tensor.get_shape().as_list()[3]\n\n  if num_ways == 1 or min(b // num_ways,\n                          num_outputs // num_ways) < divisible_by:\n    # Don\'t do any splitting if we end up with less than 8 filters\n    # on either side.\n    return slim.conv2d(input_tensor, num_outputs, [1, 1], scope=scope, **kwargs)\n\n  outs = []\n  input_splits = _split_divisible(b, num_ways, divisible_by=divisible_by)\n  output_splits = _split_divisible(\n      num_outputs, num_ways, divisible_by=divisible_by)\n  inputs = tf.split(input_tensor, input_splits, axis=3, name=\'split_\' + scope)\n  base = scope\n  for i, (input_tensor, out_size) in enumerate(zip(inputs, output_splits)):\n    scope = base + \'_part_%d\' % (i,)\n    n = slim.conv2d(input_tensor, out_size, [1, 1], scope=scope, **kwargs)\n    n = tf.identity(n, scope + \'_output\')\n    outs.append(n)\n  return tf.concat(outs, 3, name=scope + \'_concat\')\n'"
vendor/tensorflow/models/research/slim/nets/mobilenet/mobilenet.py,24,"b'# Copyright 2018 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Mobilenet Base Class.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nimport collections\nimport contextlib\nimport copy\nimport os\n\nimport tensorflow as tf\n\n\nslim = tf.contrib.slim\n\n\n@slim.add_arg_scope\ndef apply_activation(x, name=None, activation_fn=None):\n  return activation_fn(x, name=name) if activation_fn else x\n\n\ndef _fixed_padding(inputs, kernel_size, rate=1):\n  """"""Pads the input along the spatial dimensions independently of input size.\n\n  Pads the input such that if it was used in a convolution with \'VALID\' padding,\n  the output would have the same dimensions as if the unpadded input was used\n  in a convolution with \'SAME\' padding.\n\n  Args:\n    inputs: A tensor of size [batch, height_in, width_in, channels].\n    kernel_size: The kernel to be used in the conv2d or max_pool2d operation.\n    rate: An integer, rate for atrous convolution.\n\n  Returns:\n    output: A tensor of size [batch, height_out, width_out, channels] with the\n      input, either intact (if kernel_size == 1) or padded (if kernel_size > 1).\n  """"""\n  kernel_size_effective = [kernel_size[0] + (kernel_size[0] - 1) * (rate - 1),\n                           kernel_size[0] + (kernel_size[0] - 1) * (rate - 1)]\n  pad_total = [kernel_size_effective[0] - 1, kernel_size_effective[1] - 1]\n  pad_beg = [pad_total[0] // 2, pad_total[1] // 2]\n  pad_end = [pad_total[0] - pad_beg[0], pad_total[1] - pad_beg[1]]\n  padded_inputs = tf.pad(inputs, [[0, 0], [pad_beg[0], pad_end[0]],\n                                  [pad_beg[1], pad_end[1]], [0, 0]])\n  return padded_inputs\n\n\ndef _make_divisible(v, divisor, min_value=None):\n  if min_value is None:\n    min_value = divisor\n  new_v = max(min_value, int(v + divisor / 2) // divisor * divisor)\n  # Make sure that round down does not go down by more than 10%.\n  if new_v < 0.9 * v:\n    new_v += divisor\n  return new_v\n\n\n@contextlib.contextmanager\ndef _set_arg_scope_defaults(defaults):\n  """"""Sets arg scope defaults for all items present in defaults.\n\n  Args:\n    defaults: dictionary/list of pairs, containing a mapping from\n    function to a dictionary of default args.\n\n  Yields:\n    context manager where all defaults are set.\n  """"""\n  if hasattr(defaults, \'items\'):\n    items = list(defaults.items())\n  else:\n    items = defaults\n  if not items:\n    yield\n  else:\n    func, default_arg = items[0]\n    with slim.arg_scope(func, **default_arg):\n      with _set_arg_scope_defaults(items[1:]):\n        yield\n\n\n@slim.add_arg_scope\ndef depth_multiplier(output_params,\n                     multiplier,\n                     divisible_by=8,\n                     min_depth=8,\n                     **unused_kwargs):\n  if \'num_outputs\' not in output_params:\n    return\n  d = output_params[\'num_outputs\']\n  output_params[\'num_outputs\'] = _make_divisible(d * multiplier, divisible_by,\n                                                 min_depth)\n\n\n_Op = collections.namedtuple(\'Op\', [\'op\', \'params\', \'multiplier_func\'])\n\n\ndef op(opfunc, **params):\n  multiplier = params.pop(\'multiplier_transorm\', depth_multiplier)\n  return _Op(opfunc, params=params, multiplier_func=multiplier)\n\n\n@slim.add_arg_scope\ndef mobilenet_base(  # pylint: disable=invalid-name\n    inputs,\n    conv_defs,\n    multiplier=1.0,\n    final_endpoint=None,\n    output_stride=None,\n    use_explicit_padding=False,\n    scope=None,\n    is_training=False):\n  """"""Mobilenet base network.\n\n  Constructs a network from inputs to the given final endpoint. By default\n  the network is constructed in inference mode. To create network\n  in training mode use:\n\n  with slim.arg_scope(mobilenet.training_scope()):\n     logits, endpoints = mobilenet_base(...)\n\n  Args:\n    inputs: a tensor of shape [batch_size, height, width, channels].\n    conv_defs: A list of op(...) layers specifying the net architecture.\n    multiplier: Float multiplier for the depth (number of channels)\n      for all convolution ops. The value must be greater than zero. Typical\n      usage will be to set this value in (0, 1) to reduce the number of\n      parameters or computation cost of the model.\n    final_endpoint: The name of last layer, for early termination for\n    for V1-based networks: last layer is ""layer_14"", for V2: ""layer_20""\n    output_stride: An integer that specifies the requested ratio of input to\n      output spatial resolution. If not None, then we invoke atrous convolution\n      if necessary to prevent the network from reducing the spatial resolution\n      of the activation maps. Allowed values are 1 or any even number, excluding\n      zero. Typical values are 8 (accurate fully convolutional mode), 16\n      (fast fully convolutional mode), and 32 (classification mode).\n\n      NOTE- output_stride relies on all consequent operators to support dilated\n      operators via ""rate"" parameter. This might require wrapping non-conv\n      operators to operate properly.\n\n    use_explicit_padding: Use \'VALID\' padding for convolutions, but prepad\n      inputs so that the output dimensions are the same as if \'SAME\' padding\n      were used.\n    scope: optional variable scope.\n    is_training: How to setup batch_norm and other ops. Note: most of the time\n      this does not need be set directly. Use mobilenet.training_scope() to set\n      up training instead. This parameter is here for backward compatibility\n      only. It is safe to set it to the value matching\n      training_scope(is_training=...). It is also safe to explicitly set\n      it to False, even if there is outer training_scope set to to training.\n      (The network will be built in inference mode).\n  Returns:\n    tensor_out: output tensor.\n    end_points: a set of activations for external use, for example summaries or\n                losses.\n\n  Raises:\n    ValueError: depth_multiplier <= 0, or the target output_stride is not\n                allowed.\n  """"""\n  if multiplier <= 0:\n    raise ValueError(\'multiplier is not greater than zero.\')\n\n  # Set conv defs defaults and overrides.\n  conv_defs_defaults = conv_defs.get(\'defaults\', {})\n  conv_defs_overrides = conv_defs.get(\'overrides\', {})\n  if use_explicit_padding:\n    conv_defs_overrides = copy.deepcopy(conv_defs_overrides)\n    conv_defs_overrides[\n        (slim.conv2d, slim.separable_conv2d)] = {\'padding\': \'VALID\'}\n\n  if output_stride is not None:\n    if output_stride == 0 or (output_stride > 1 and output_stride % 2):\n      raise ValueError(\'Output stride must be None, 1 or a multiple of 2.\')\n\n  # a) Set the tensorflow scope\n  # b) set padding to default: note we might consider removing this\n  # since it is also set by mobilenet_scope\n  # c) set all defaults\n  # d) set all extra overrides.\n  with _scope_all(scope, default_scope=\'Mobilenet\'), \\\n      slim.arg_scope([slim.batch_norm], is_training=is_training), \\\n      _set_arg_scope_defaults(conv_defs_defaults), \\\n      _set_arg_scope_defaults(conv_defs_overrides):\n    # The current_stride variable keeps track of the output stride of the\n    # activations, i.e., the running product of convolution strides up to the\n    # current network layer. This allows us to invoke atrous convolution\n    # whenever applying the next convolution would result in the activations\n    # having output stride larger than the target output_stride.\n    current_stride = 1\n\n    # The atrous convolution rate parameter.\n    rate = 1\n\n    net = inputs\n    # Insert default parameters before the base scope which includes\n    # any custom overrides set in mobilenet.\n    end_points = {}\n    scopes = {}\n    for i, opdef in enumerate(conv_defs[\'spec\']):\n      params = dict(opdef.params)\n      opdef.multiplier_func(params, multiplier)\n      stride = params.get(\'stride\', 1)\n      if output_stride is not None and current_stride == output_stride:\n        # If we have reached the target output_stride, then we need to employ\n        # atrous convolution with stride=1 and multiply the atrous rate by the\n        # current unit\'s stride for use in subsequent layers.\n        layer_stride = 1\n        layer_rate = rate\n        rate *= stride\n      else:\n        layer_stride = stride\n        layer_rate = 1\n        current_stride *= stride\n      # Update params.\n      params[\'stride\'] = layer_stride\n      # Only insert rate to params if rate > 1.\n      if layer_rate > 1:\n        params[\'rate\'] = layer_rate\n      # Set padding\n      if use_explicit_padding:\n        if \'kernel_size\' in params:\n          net = _fixed_padding(net, params[\'kernel_size\'], layer_rate)\n        else:\n          params[\'use_explicit_padding\'] = True\n\n      end_point = \'layer_%d\' % (i + 1)\n      try:\n        net = opdef.op(net, **params)\n      except Exception:\n        print(\'Failed to create op %i: %r params: %r\' % (i, opdef, params))\n        raise\n      end_points[end_point] = net\n      scope = os.path.dirname(net.name)\n      scopes[scope] = end_point\n      if final_endpoint is not None and end_point == final_endpoint:\n        break\n\n    # Add all tensors that end with \'output\' to\n    # endpoints\n    for t in net.graph.get_operations():\n      scope = os.path.dirname(t.name)\n      bn = os.path.basename(t.name)\n      if scope in scopes and t.name.endswith(\'output\'):\n        end_points[scopes[scope] + \'/\' + bn] = t.outputs[0]\n    return net, end_points\n\n\n@contextlib.contextmanager\ndef _scope_all(scope, default_scope=None):\n  with tf.variable_scope(scope, default_name=default_scope) as s,\\\n       tf.name_scope(s.original_name_scope):\n    yield s\n\n\n@slim.add_arg_scope\ndef mobilenet_deepdrive(inputs,\n                        num_targets=6,\n                        prediction_fn=None,\n                        reuse=None,\n                        scope=\'Mobilenet\',\n                        base_only=False,\n                        **mobilenet_args):\n  """"""Mobilenet model for classification, supports both V1 and V2.\n\n  Note: default mode is inference, use mobilenet.training_scope to create\n  training network.\n\n\n  Args:\n    inputs: a tensor of shape [batch_size, height, width, channels].\n    num_targets: number of targets. If 0 or None, the logits layer\n      is omitted and the input features to the logits layer (before dropout)\n      are returned instead.\n    prediction_fn: a function to get predictions out of logits\n      (default softmax).\n    reuse: whether or not the network and its variables should be reused. To be\n      able to reuse \'scope\' must be given.\n    scope: Optional variable_scope.\n    base_only: if True will only create the base of the network (no pooling\n    and no logits).\n    **mobilenet_args: passed to mobilenet_base verbatim.\n      - conv_defs: list of conv defs\n      - multiplier: Float multiplier for the depth (number of channels)\n      for all convolution ops. The value must be greater than zero. Typical\n      usage will be to set this value in (0, 1) to reduce the number of\n      parameters or computation cost of the model.\n      - output_stride: will ensure that the last layer has at most total stride.\n      If the architecture calls for more stride than that provided\n      (e.g. output_stride=16, but the architecture has 5 stride=2 operators),\n      it will replace output_stride with fractional convolutions using Atrous\n      Convolutions.\n\n  Returns:\n    logits: the pre-softmax activations, a tensor of size\n      [batch_size, num_classes]\n    end_points: a dictionary from components of the network to the corresponding\n      activation tensor.\n\n  Raises:\n    ValueError: Input rank is invalid.\n  """"""\n  is_training = mobilenet_args.get(\'is_training\', False)\n  input_shape = inputs.get_shape().as_list()\n  if len(input_shape) != 4:\n    raise ValueError(\'Expected rank 4 input, was: %d\' % len(input_shape))\n\n  with tf.variable_scope(scope, \'Mobilenet\', reuse=reuse) as scope:\n    inputs = tf.identity(inputs, \'input\')\n    net, end_points = mobilenet_base(inputs, scope=scope, **mobilenet_args)\n    if base_only:\n      return net, end_points\n\n    net = tf.identity(net, name=\'embedding\')\n\n    with tf.variable_scope(\'Logits\'):\n      net = global_pool(net)\n      end_points[\'global_pool\'] = net\n      if not num_targets:\n        return net, end_points\n      net = slim.dropout(net, scope=\'Dropout\', is_training=is_training)\n      # 1 x 1 x num_targets\n      # Note: legacy scope name.\n      logits = slim.conv2d(\n          net,\n          num_targets, [1, 1],\n          activation_fn=None,\n          normalizer_fn=None,\n          biases_initializer=tf.zeros_initializer(),\n          scope=\'Conv2d_1c_1x1\')\n\n      logits = tf.squeeze(logits, [1, 2])\n\n      logits = tf.identity(logits, name=\'output\')\n    end_points[\'Logits\'] = logits\n    if prediction_fn:\n      end_points[\'Predictions\'] = prediction_fn(logits, \'Predictions\')\n  return logits, end_points\n\n@slim.add_arg_scope\ndef mobilenet(inputs,\n              num_classes=1001,\n              prediction_fn=slim.softmax,\n              reuse=None,\n              scope=\'Mobilenet\',\n              base_only=False,\n              **mobilenet_args):\n  """"""Mobilenet model for classification, supports both V1 and V2.\n\n  Note: default mode is inference, use mobilenet.training_scope to create\n  training network.\n\n\n  Args:\n    inputs: a tensor of shape [batch_size, height, width, channels].\n    num_classes: number of predicted classes. If 0 or None, the logits layer\n      is omitted and the input features to the logits layer (before dropout)\n      are returned instead.\n    prediction_fn: a function to get predictions out of logits\n      (default softmax).\n    reuse: whether or not the network and its variables should be reused. To be\n      able to reuse \'scope\' must be given.\n    scope: Optional variable_scope.\n    base_only: if True will only create the base of the network (no pooling\n    and no logits).\n    **mobilenet_args: passed to mobilenet_base verbatim.\n      - conv_defs: list of conv defs\n      - multiplier: Float multiplier for the depth (number of channels)\n      for all convolution ops. The value must be greater than zero. Typical\n      usage will be to set this value in (0, 1) to reduce the number of\n      parameters or computation cost of the model.\n      - output_stride: will ensure that the last layer has at most total stride.\n      If the architecture calls for more stride than that provided\n      (e.g. output_stride=16, but the architecture has 5 stride=2 operators),\n      it will replace output_stride with fractional convolutions using Atrous\n      Convolutions.\n\n  Returns:\n    logits: the pre-softmax activations, a tensor of size\n      [batch_size, num_classes]\n    end_points: a dictionary from components of the network to the corresponding\n      activation tensor.\n\n  Raises:\n    ValueError: Input rank is invalid.\n  """"""\n  is_training = mobilenet_args.get(\'is_training\', False)\n  input_shape = inputs.get_shape().as_list()\n  if len(input_shape) != 4:\n    raise ValueError(\'Expected rank 4 input, was: %d\' % len(input_shape))\n\n  with tf.variable_scope(scope, \'Mobilenet\', reuse=reuse) as scope:\n    inputs = tf.identity(inputs, \'input\')\n    net, end_points = mobilenet_base(inputs, scope=scope, **mobilenet_args)\n    if base_only:\n      return net, end_points\n\n    net = tf.identity(net, name=\'embedding\')\n\n    with tf.variable_scope(\'Logits\'):\n      net = global_pool(net)\n      end_points[\'global_pool\'] = net\n      if not num_classes:\n        return net, end_points\n      net = slim.dropout(net, scope=\'Dropout\', is_training=is_training)\n      # 1 x 1 x num_classes\n      # Note: legacy scope name.\n      logits = slim.conv2d(\n          net,\n          num_classes, [1, 1],\n          activation_fn=None,\n          normalizer_fn=None,\n          biases_initializer=tf.zeros_initializer(),\n          scope=\'Conv2d_1c_1x1\')\n\n      logits = tf.squeeze(logits, [1, 2])\n\n      logits = tf.identity(logits, name=\'output\')\n    end_points[\'Logits\'] = logits\n    if prediction_fn:\n      end_points[\'Predictions\'] = prediction_fn(logits, \'Predictions\')\n  return logits, end_points\n\n\ndef global_pool(input_tensor, pool_op=tf.nn.avg_pool):\n  """"""Applies avg pool to produce 1x1 output.\n\n  NOTE: This function is funcitonally equivalenet to reduce_mean, but it has\n  baked in average pool which has better support across hardware.\n\n  Args:\n    input_tensor: input tensor\n    pool_op: pooling op (avg pool is default)\n  Returns:\n    a tensor batch_size x 1 x 1 x depth.\n  """"""\n  shape = input_tensor.get_shape().as_list()\n  if shape[1] is None or shape[2] is None:\n    kernel_size = tf.convert_to_tensor(\n        [1, tf.shape(input_tensor)[1],\n         tf.shape(input_tensor)[2], 1])\n  else:\n    kernel_size = [1, shape[1], shape[2], 1]\n  output = pool_op(\n      input_tensor, ksize=kernel_size, strides=[1, 1, 1, 1], padding=\'VALID\')\n  # Recover output shape, for unknown shape.\n  output.set_shape([None, 1, 1, None])\n  return output\n\n\ndef training_scope(is_training=True,\n                   weight_decay=0.00004,\n                   stddev=0.09,\n                   dropout_keep_prob=0.8,\n                   bn_decay=0.997):\n  """"""Defines Mobilenet training scope.\n\n  Usage:\n     with tf.contrib.slim.arg_scope(mobilenet.training_scope()):\n       logits, endpoints = mobilenet_v2.mobilenet(input_tensor)\n\n     # the network created will be trainble with dropout/batch norm\n     # initialized appropriately.\n  Args:\n    is_training: if set to False this will ensure that all customizations are\n    set to non-training mode. This might be helpful for code that is reused\n    across both training/evaluation, but most of the time training_scope with\n    value False is not needed.\n\n    weight_decay: The weight decay to use for regularizing the model.\n    stddev: Standard deviation for initialization, if negative uses xavier.\n    dropout_keep_prob: dropout keep probability\n    bn_decay: decay for the batch norm moving averages.\n\n  Returns:\n    An argument scope to use via arg_scope.\n  """"""\n  # Note: do not introduce parameters that would change the inference\n  # model here (for example whether to use bias), modify conv_def instead.\n  batch_norm_params = {\n      \'is_training\': is_training,\n      \'decay\': bn_decay,\n  }\n\n  if stddev < 0:\n    weight_intitializer = slim.initializers.xavier_initializer()\n  else:\n    weight_intitializer = tf.truncated_normal_initializer(stddev=stddev)\n\n  # Set weight_decay for weights in Conv and FC layers.\n  with slim.arg_scope(\n      [slim.conv2d, slim.fully_connected, slim.separable_conv2d],\n      weights_initializer=weight_intitializer,\n      normalizer_fn=slim.batch_norm), \\\n      slim.arg_scope([mobilenet_base, mobilenet], is_training=is_training),\\\n      slim.arg_scope([slim.batch_norm], **batch_norm_params), \\\n      slim.arg_scope([slim.dropout], is_training=is_training,\n                     keep_prob=dropout_keep_prob), \\\n      slim.arg_scope([slim.conv2d], \\\n                     weights_regularizer=slim.l2_regularizer(weight_decay)), \\\n      slim.arg_scope([slim.separable_conv2d], weights_regularizer=None) as s:\n    return s\n'"
vendor/tensorflow/models/research/slim/nets/mobilenet/mobilenet_v2.py,5,"b'# Copyright 2018 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Implementation of Mobilenet V2.\n\nArchitecture: https://arxiv.org/abs/1801.04381\n\nThe base model gives 72.2% accuracy on ImageNet, with 300MMadds,\n3.4 M parameters.\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport copy\n\nimport tensorflow as tf\n\nfrom vendor.tensorflow.models.research.slim.nets.mobilenet import conv_blocks as ops\nfrom vendor.tensorflow.models.research.slim.nets.mobilenet import mobilenet as lib\n\nslim = tf.contrib.slim\nop = lib.op\n\nexpand_input = ops.expand_input_by_factor\n\n# pyformat: disable\n# Architecture: https://arxiv.org/abs/1801.04381\nV2_DEF = dict(\n    defaults={\n        # Note: these parameters of batch norm affect the architecture\n        # that\'s why they are here and not in training_scope.\n        (slim.batch_norm,): {\'center\': True, \'scale\': True},\n        (slim.conv2d, slim.fully_connected, slim.separable_conv2d): {\n            \'normalizer_fn\': slim.batch_norm, \'activation_fn\': tf.nn.relu6\n        },\n        (ops.expanded_conv,): {\n            \'expansion_size\': expand_input(6),\n            \'split_expansion\': 1,\n            \'normalizer_fn\': slim.batch_norm,\n            \'residual\': True\n        },\n        (slim.conv2d, slim.separable_conv2d): {\'padding\': \'SAME\'}\n    },\n    spec=[\n        op(slim.conv2d, stride=2, num_outputs=32, kernel_size=[3, 3]),\n        op(ops.expanded_conv,\n           expansion_size=expand_input(1, divisible_by=1),\n           num_outputs=16),\n        op(ops.expanded_conv, stride=2, num_outputs=24),\n        op(ops.expanded_conv, stride=1, num_outputs=24),\n        op(ops.expanded_conv, stride=2, num_outputs=32),\n        op(ops.expanded_conv, stride=1, num_outputs=32),\n        op(ops.expanded_conv, stride=1, num_outputs=32),\n        op(ops.expanded_conv, stride=2, num_outputs=64),\n        op(ops.expanded_conv, stride=1, num_outputs=64),\n        op(ops.expanded_conv, stride=1, num_outputs=64),\n        op(ops.expanded_conv, stride=1, num_outputs=64),\n        op(ops.expanded_conv, stride=1, num_outputs=96),\n        op(ops.expanded_conv, stride=1, num_outputs=96),\n        op(ops.expanded_conv, stride=1, num_outputs=96),\n        op(ops.expanded_conv, stride=2, num_outputs=160),\n        op(ops.expanded_conv, stride=1, num_outputs=160),\n        op(ops.expanded_conv, stride=1, num_outputs=160),\n        op(ops.expanded_conv, stride=1, num_outputs=320),\n        op(slim.conv2d, stride=1, kernel_size=[1, 1], num_outputs=1280)\n    ],\n)\n# pyformat: enable\n\n\n@slim.add_arg_scope\ndef mobilenet(input_tensor,\n              num_classes=1001,\n              depth_multiplier=1.0,\n              scope=\'MobilenetV2\',\n              conv_defs=None,\n              finegrain_classification_mode=False,\n              min_depth=None,\n              divisible_by=None,\n              **kwargs):\n  """"""Creates mobilenet V2 network.\n\n  Inference mode is created by default. To create training use training_scope\n  below.\n\n  with tf.contrib.slim.arg_scope(mobilenet_v2.training_scope()):\n     logits, endpoints = mobilenet_v2.mobilenet(input_tensor)\n\n  Args:\n    input_tensor: The input tensor\n    num_classes: number of classes\n    depth_multiplier: The multiplier applied to scale number of\n    channels in each layer. Note: this is called depth multiplier in the\n    paper but the name is kept for consistency with slim\'s model builder.\n    scope: Scope of the operator\n    conv_defs: Allows to override default conv def.\n    finegrain_classification_mode: When set to True, the model\n    will keep the last layer large even for small multipliers. Following\n    https://arxiv.org/abs/1801.04381\n    suggests that it improves performance for ImageNet-type of problems.\n      *Note* ignored if final_endpoint makes the builder exit earlier.\n    min_depth: If provided, will ensure that all layers will have that\n    many channels after application of depth multiplier.\n    divisible_by: If provided will ensure that all layers # channels\n    will be divisible by this number.\n    **kwargs: passed directly to mobilenet.mobilenet:\n      prediction_fn- what prediction function to use.\n      reuse-: whether to reuse variables (if reuse set to true, scope\n      must be given).\n  Returns:\n    logits/endpoints pair\n\n  Raises:\n    ValueError: On invalid arguments\n  """"""\n  if conv_defs is None:\n    conv_defs = V2_DEF\n  if \'multiplier\' in kwargs:\n    raise ValueError(\'mobilenetv2 doesn\\\'t support generic \'\n                     \'multiplier parameter use ""depth_multiplier"" instead.\')\n  if finegrain_classification_mode:\n    conv_defs = copy.deepcopy(conv_defs)\n    if depth_multiplier < 1:\n      conv_defs[\'spec\'][-1].params[\'num_outputs\'] /= depth_multiplier\n\n  depth_args = {}\n  # NB: do not set depth_args unless they are provided to avoid overriding\n  # whatever default depth_multiplier might have thanks to arg_scope.\n  if min_depth is not None:\n    depth_args[\'min_depth\'] = min_depth\n  if divisible_by is not None:\n    depth_args[\'divisible_by\'] = divisible_by\n\n  with slim.arg_scope((lib.depth_multiplier,), **depth_args):\n    return lib.mobilenet(\n        input_tensor,\n        num_targets=num_classes,\n        conv_defs=conv_defs,\n        scope=scope,\n        multiplier=depth_multiplier,\n        **kwargs)\n\n@slim.add_arg_scope\ndef mobilenet_deepdrive(input_tensor,\n              num_targets=6,\n              depth_multiplier=1.0,\n              scope=\'MobilenetV2\',\n              conv_defs=None,\n              finegrain_classification_mode=False,\n              min_depth=None,\n              divisible_by=None,\n              **kwargs):\n  """"""Creates mobilenet V2 network.\n\n  Inference mode is created by default. To create training use training_scope\n  below.\n\n  with tf.contrib.slim.arg_scope(mobilenet_v2.training_scope()):\n     logits, endpoints = mobilenet_v2.mobilenet(input_tensor)\n\n  Args:\n    input_tensor: The input tensor\n    num_classes: number of classes\n    depth_multiplier: The multiplier applied to scale number of\n    channels in each layer. Note: this is called depth multiplier in the\n    paper but the name is kept for consistency with slim\'s model builder.\n    scope: Scope of the operator\n    conv_defs: Allows to override default conv def.\n    finegrain_classification_mode: When set to True, the model\n    will keep the last layer large even for small multipliers. Following\n    https://arxiv.org/abs/1801.04381\n    suggests that it improves performance for ImageNet-type of problems.\n      *Note* ignored if final_endpoint makes the builder exit earlier.\n    min_depth: If provided, will ensure that all layers will have that\n    many channels after application of depth multiplier.\n    divisible_by: If provided will ensure that all layers # channels\n    will be divisible by this number.\n    **kwargs: passed directly to mobilenet.mobilenet:\n      prediction_fn- what prediction function to use.\n      reuse-: whether to reuse variables (if reuse set to true, scope\n      must be given).\n  Returns:\n    logits/endpoints pair\n\n  Raises:\n    ValueError: On invalid arguments\n  """"""\n  if conv_defs is None:\n    conv_defs = V2_DEF\n  depth_args = {}\n\n  with slim.arg_scope((lib.depth_multiplier,), **depth_args):\n    return lib.mobilenet_deepdrive(\n        input_tensor,\n        num_targets=num_targets,\n        conv_defs=conv_defs,\n        scope=scope,\n        multiplier=depth_multiplier,\n        **kwargs)\n\n\n@slim.add_arg_scope\ndef mobilenet_base(input_tensor, depth_multiplier=1.0, **kwargs):\n  """"""Creates base of the mobilenet (no pooling and no logits) .""""""\n  return mobilenet(input_tensor,\n                   depth_multiplier=depth_multiplier,\n                   base_only=True, **kwargs)\n\n\ndef training_scope(**kwargs):\n  """"""Defines MobilenetV2 training scope.\n\n  Usage:\n     with tf.contrib.slim.arg_scope(mobilenet_v2.training_scope()):\n       logits, endpoints = mobilenet_v2.mobilenet(input_tensor)\n\n  with slim.\n\n  Args:\n    **kwargs: Passed to mobilenet.training_scope. The following parameters\n    are supported:\n      weight_decay- The weight decay to use for regularizing the model.\n      stddev-  Standard deviation for initialization, if negative uses xavier.\n      dropout_keep_prob- dropout keep probability\n      bn_decay- decay for the batch norm moving averages.\n\n  Returns:\n    An `arg_scope` to use for the mobilenet v2 model.\n  """"""\n  return lib.training_scope(**kwargs)\n\n\n__all__ = [\'training_scope\', \'mobilenet_base\', \'mobilenet\', \'V2_DEF\']\n'"
vendor/tensorflow/models/research/slim/nets/mobilenet/mobilenet_v2_test.py,25,"b'# Copyright 2018 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Tests for mobilenet_v2.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nimport copy\nimport tensorflow as tf\nfrom nets.mobilenet import conv_blocks as ops\nfrom nets.mobilenet import mobilenet\nfrom nets.mobilenet import mobilenet_v2\n\n\nslim = tf.contrib.slim\n\n\ndef find_ops(optype):\n  """"""Find ops of a given type in graphdef or a graph.\n\n  Args:\n    optype: operation type (e.g. Conv2D)\n  Returns:\n     List of operations.\n  """"""\n  gd = tf.get_default_graph()\n  return [var for var in gd.get_operations() if var.type == optype]\n\n\nclass MobilenetV2Test(tf.test.TestCase):\n\n  def setUp(self):\n    tf.reset_default_graph()\n\n  def testCreation(self):\n    spec = dict(mobilenet_v2.V2_DEF)\n    _, ep = mobilenet.mobilenet(\n        tf.placeholder(tf.float32, (10, 224, 224, 16)), conv_defs=spec)\n    num_convs = len(find_ops(\'Conv2D\'))\n\n    # This is mostly a sanity test. No deep reason for these particular\n    # constants.\n    #\n    # All but first 2 and last one have  two convolutions, and there is one\n    # extra conv that is not in the spec. (logits)\n    self.assertEqual(num_convs, len(spec[\'spec\']) * 2 - 2)\n    # Check that depthwise are exposed.\n    for i in range(2, 17):\n      self.assertIn(\'layer_%d/depthwise_output\' % i, ep)\n\n  def testCreationNoClasses(self):\n    spec = copy.deepcopy(mobilenet_v2.V2_DEF)\n    net, ep = mobilenet.mobilenet(\n        tf.placeholder(tf.float32, (10, 224, 224, 16)), conv_defs=spec,\n        num_classes=None)\n    self.assertIs(net, ep[\'global_pool\'])\n\n  def testImageSizes(self):\n    for input_size, output_size in [(224, 7), (192, 6), (160, 5),\n                                    (128, 4), (96, 3)]:\n      tf.reset_default_graph()\n      _, ep = mobilenet_v2.mobilenet(\n          tf.placeholder(tf.float32, (10, input_size, input_size, 3)))\n\n      self.assertEqual(ep[\'layer_18/output\'].get_shape().as_list()[1:3],\n                       [output_size] * 2)\n\n  def testWithSplits(self):\n    spec = copy.deepcopy(mobilenet_v2.V2_DEF)\n    spec[\'overrides\'] = {\n        (ops.expanded_conv,): dict(split_expansion=2),\n    }\n    _, _ = mobilenet.mobilenet(\n        tf.placeholder(tf.float32, (10, 224, 224, 16)), conv_defs=spec)\n    num_convs = len(find_ops(\'Conv2D\'))\n    # All but 3 op has 3 conv operatore, the remainign 3 have one\n    # and there is one unaccounted.\n    self.assertEqual(num_convs, len(spec[\'spec\']) * 3 - 5)\n\n  def testWithOutputStride8(self):\n    out, _ = mobilenet.mobilenet_base(\n        tf.placeholder(tf.float32, (10, 224, 224, 16)),\n        conv_defs=mobilenet_v2.V2_DEF,\n        output_stride=8,\n        scope=\'MobilenetV2\')\n    self.assertEqual(out.get_shape().as_list()[1:3], [28, 28])\n\n  def testDivisibleBy(self):\n    tf.reset_default_graph()\n    mobilenet_v2.mobilenet(\n        tf.placeholder(tf.float32, (10, 224, 224, 16)),\n        conv_defs=mobilenet_v2.V2_DEF,\n        divisible_by=16,\n        min_depth=32)\n    s = [op.outputs[0].get_shape().as_list()[-1] for op in find_ops(\'Conv2D\')]\n    s = set(s)\n    self.assertSameElements([32, 64, 96, 160, 192, 320, 384, 576, 960, 1280,\n                             1001], s)\n\n  def testDivisibleByWithArgScope(self):\n    tf.reset_default_graph()\n    # Verifies that depth_multiplier arg scope actually works\n    # if no default min_depth is provided.\n    with slim.arg_scope((mobilenet.depth_multiplier,), min_depth=32):\n      mobilenet_v2.mobilenet(\n          tf.placeholder(tf.float32, (10, 224, 224, 2)),\n          conv_defs=mobilenet_v2.V2_DEF, depth_multiplier=0.1)\n      s = [op.outputs[0].get_shape().as_list()[-1] for op in find_ops(\'Conv2D\')]\n      s = set(s)\n      self.assertSameElements(s, [32, 192, 128, 1001])\n\n  def testFineGrained(self):\n    tf.reset_default_graph()\n    # Verifies that depth_multiplier arg scope actually works\n    # if no default min_depth is provided.\n\n    mobilenet_v2.mobilenet(\n        tf.placeholder(tf.float32, (10, 224, 224, 2)),\n        conv_defs=mobilenet_v2.V2_DEF, depth_multiplier=0.01,\n        finegrain_classification_mode=True)\n    s = [op.outputs[0].get_shape().as_list()[-1] for op in find_ops(\'Conv2D\')]\n    s = set(s)\n    # All convolutions will be 8->48, except for the last one.\n    self.assertSameElements(s, [8, 48, 1001, 1280])\n\n  def testMobilenetBase(self):\n    tf.reset_default_graph()\n    # Verifies that mobilenet_base returns pre-pooling layer.\n    with slim.arg_scope((mobilenet.depth_multiplier,), min_depth=32):\n      net, _ = mobilenet_v2.mobilenet_base(\n          tf.placeholder(tf.float32, (10, 224, 224, 16)),\n          conv_defs=mobilenet_v2.V2_DEF, depth_multiplier=0.1)\n      self.assertEqual(net.get_shape().as_list(), [10, 7, 7, 128])\n\n  def testWithOutputStride16(self):\n    tf.reset_default_graph()\n    out, _ = mobilenet.mobilenet_base(\n        tf.placeholder(tf.float32, (10, 224, 224, 16)),\n        conv_defs=mobilenet_v2.V2_DEF,\n        output_stride=16)\n    self.assertEqual(out.get_shape().as_list()[1:3], [14, 14])\n\n  def testWithOutputStride8AndExplicitPadding(self):\n    tf.reset_default_graph()\n    out, _ = mobilenet.mobilenet_base(\n        tf.placeholder(tf.float32, (10, 224, 224, 16)),\n        conv_defs=mobilenet_v2.V2_DEF,\n        output_stride=8,\n        use_explicit_padding=True,\n        scope=\'MobilenetV2\')\n    self.assertEqual(out.get_shape().as_list()[1:3], [28, 28])\n\n  def testWithOutputStride16AndExplicitPadding(self):\n    tf.reset_default_graph()\n    out, _ = mobilenet.mobilenet_base(\n        tf.placeholder(tf.float32, (10, 224, 224, 16)),\n        conv_defs=mobilenet_v2.V2_DEF,\n        output_stride=16,\n        use_explicit_padding=True)\n    self.assertEqual(out.get_shape().as_list()[1:3], [14, 14])\n\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
